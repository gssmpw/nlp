\appendix

\section{Bloom Cognitive Model}\label{bloom_model}
Bloom Cognitive Model~\cite{krathwohl1973taxonomy} is an educational theoretical framework that outlines six hierarchical levels of cognitive processes, ranging from lower-order to higher-order thinking skills in the learning process as follows:
\begin{itemize}[leftmargin=*]
\item\textbf{Remember}: The capacity to recall, identify, and reproduce information.

\item\textbf{Understand}: The ability to interpret, summarize, and make sense of information.

\item\textbf{Apply}: The skill to use learned knowledge in new contexts.

\item\textbf{Analyze}: The capability to deconstruct information and examine relationships between components.

\item\textbf{Evaluate}: The proficiency in making informed judgments based on specific criteria, involving critical thinking.

\item\textbf{Create}: The ability to synthesize elements into novel patterns or generate original work.
\end{itemize}

Motivated by this framework, we aim to develop the model that goes beyond merely memorizing surface patterns and demonstrates higher-order capabilities. To test whether the model truly understands commonsense knowledge, we create seven variants of each question. It is our view that if the model genuinely understands commonsense knowledge, it should be able to correctly respond to the same knowledge expressed in different reasoning forms. Here's how our seven variants map onto these cognitive levels:
\begin{itemize}[leftmargin=*]
\item\textbf{Understanding} is demonstrated through \textit{Problem Restatement} and \textit{Causal Inference}.

\item\textbf{Application} skills are tested via \textit{Reverse Conversion}, \textit{Scenario Refinement}, and \textit{Negative Transformation}.

\item\textbf{Analysis} capabilities are assessed through \textit{Sentence Ordering}.

\item\textbf{Evaluation} competency is measured by \textit{Critical Testing}.
\end{itemize}


\section{Human Annotation}\label{cn_hellaswag_annotation}


\subsection{Annotator Qualification and Compensation}
We maintained strict control over annotator qualification, data quality, and annotation procedure. Specifically, we recruited 34 professional annotators specializing in NLP tasks totally. All annotators hold at least a bachelor's degree, have passed the College English Test Level 4 of China, and possess extensive annotation experience of NLP tasks. We compensated them at a rate of 23 RMB per hour (significantly higher than the average hourly wage in China), with an average payment of 1.98 RMB per question. We promptly addressed any concerns during the annotation process and allowed sufficient time for each question to prevent unnecessary pressure on annotators.

\subsection{Data Quality and Consistency}
31 out of 34 annotators were involved in data filtering. We enforced the strict annotation guidelines. For Chinese HellaSwag construction, in Stage 1 (initial dataset generation), annotators labeled 12,960 entries in total and filtered down to 12,287 entries. The authors randomly sampled 100 filtered entries and verified them against annotation guidelines, achieving a 98\% compliance rate. In Stage 2 (difficult sample replacement), annotators labeled 5,209 entries in total and filtered down to 2,451 entries. A similar 100-question sample check by authors showed a 96\% compliance rate. For HellaSwag-Pro construction, annotators labeled 24,260 entries, filtering down to 11,200. The authors randomly checked 100 question variants against variant annotation guidelines, achieving a 95\% compliance rate. These measures ensured high quality of our dataset.

\subsection{Human Performance}
To evaluate human performance, we sampled a subset of 400 questions by randomly selecting 25 original questions in both Chinese and English, along with their variants. Three additional crowd workers, who were not involved in the original annotation process, were tested on this subset. We calculated their average accuracy as human performance.

\subsection{Detailed Annotation Guidelines}
We provided rich examples for the annotation tasks to ensure annotators understood the tasks at hand. We maintained close contact with the annotators to clarify any misunderstandings in time. Our annotation tasks were divided into four parts:

\subsubsection{Chinese HellaSwag Annotation for Stage One}
Annotators were given the context, six choices filtered by the model, label, broad type, and detailed type. They scored based on three dimensions: the possibility to select 4 out of 6 choices, and whether they conform to the two category definitions. The annotation requirements for annotators were as follows:
\begin{itemize}[leftmargin=*]
\item\textbf{Possibility to select 4 out of 6}: Using the model's scoring of the 6 choices as a reference, determine if it's possible to select 4 choices, with only one correct answer and the other three being as confusing as possible (i.e., conforming to commonsense but not suitable for the context, or judged by how much modification is needed to make them correct - the less modification needed, the more confusing). Ensure the uniqueness of the answer and avoid controversy. Score 1 if possible, and note the corresponding option numbers, with the first being the correct option and the next three being incorrect options. If not possible, score 0 and select the appropriate reason: A. No correct option or B. Unable to select 3 incorrect options, e.g., more than 4 correct options.

\item\textbf{Broad type}: Score it conforms to the definition, otherwise 0.

\item\textbf{Detailed type}: Score 1 if it conforms to the definition, otherwise 0.
\end{itemize}

The following are the definitions for broad and detailed types. 

\begin{itemize}[leftmargin=*]
\item \textbf{Family}

Household chores: Labor activities to maintain a clean and tidy home environment, including but not limited to cleaning, laundry, and preparing traditional Chinese cuisine.

Personal hygiene: Daily personal cleaning habits such as bathing, brushing teeth, and maintaining good living habits to ensure physical health.

Family entertainment: Leisure activities shared by family members, such as playing family games, pet care, watching TV shows, or reading books together.

Holiday celebrations: Celebrating family members' birthdays, traditional festivals, or special occasions like wedding anniversaries.

Family affairs: Daily life management, emotional communication, and responsibility allocation among family members, including household shopping, financial management, and handling potential disagreements or conflicts.

Family transitions: Changes in family structure or living environment, such as home renovation, moving, marriage, or welcoming a newborn.

Emergency handling: Measures for potential emergencies like fires or natural disasters.

\item \textbf{Education}

School education: Formal education received in school settings, including classroom learning, extracurricular activities, and exam preparation.
Family education: Education provided by parents or other family members, including homework assistance, shared reading, and cultivation of interests and moral qualities.

Online learning: Learning through internet resources, including self-study tools, remote tutoring, and interactive learning platforms.

Community education: Educational activities within the community, such as lectures, interest groups, and practical activities.

Vocational training: Professional training aimed at improving occupational skills, including obtaining professional qualifications and on-the-job continuing education.

Lifelong learning: Continuous learning activities for adults to improve themselves, such as adult education or senior university courses.

International exchange: Consultation for studying abroad, language skill improvement, and other forms of cross-cultural exchange.


\item \textbf{Work}

Work Meetings: Various meetings held in the workplace, including team meetings, departmental reports, and project evaluations.

Project Management: The entire process of managing a project from initiation to completion, including strategy formulation, progress tracking, and problem-solving.

Customer Service: Services provided to meet customer needs, including customer inquiries, complaint handling, sales negotiations, and after-sales support.

Teamwork: Effective collaborative work patterns within a team, including team building, task allocation, conflict resolution, and incentive measures.

Personal Development: The process of individual career growth, covering skill learning, career planning, financial management, and maintaining mental and physical health.

Administrative Management: Daily management activities within a company, including attendance records, performance evaluations, travel expense reimbursements, employee benefits distribution, and company policy communication.

Technological Innovation: Activities driving technological advancement in a company, including new product development, technology application, technical training, and technology exchange.


\item \textbf{Sociality}

Daily Interactions: Everyday social interactions with family, friends, and neighbors.

School Interactions: Communication between students, between teachers and students, and between parents and teachers.

Workplace Interactions: Interactions with colleagues, superiors, or subordinates in the workplace, as well as formal business dinners.

Community Interactions: Participating in community-organized activities or providing volunteer services.

Public Space Interactions: Interactions with others in public spaces such as public transportation, shopping malls, restaurants, and lecture halls.

Online Social Networking: Social activities using online platforms, including social media, online gaming, internet forums, and video live streaming.

Special Occasion Interactions: Interpersonal interactions at weddings, funerals, award ceremonies, and other celebratory events.

\item \textbf{ Shopping}

In-store Shopping: Shopping activities in physical retail stores, such as supermarkets, department stores, and specialty shops.

Online Shopping: Online purchasing behavior through e-commerce platforms, live streaming sales, or social commerce.

Food and Dining Purchases: Buying food products, including dining out, ordering takeout, and home cooking.

Service Purchases: Buying various service products, such as travel services, beauty and fitness, and educational training.

Overseas Shopping: Purchasing foreign goods through cross-border e-commerce or personal shopping agents.

Special Occasion Shopping: Shopping in specific situations, such as promotional events, group buying, auctions, and second-hand transactions.

Returns and After-sales Service: Consumer behavior in seeking refunds, exchanges, and after-sales service when issues arise with products.


\item \textbf{Transportation}

Public Transportation: Using public transit systems, such as buses and subways.

Private Transportation: Using private vehicles, bicycles, etc., for travel.

Long-distance Travel: Travel methods covering longer distances, such as trains, planes, or long-distance buses.

Emergency Travel: Choosing emergency transportation in response to sudden situations, such as travel during severe weather conditions.

Tourist Transportation: Using sightseeing vehicles, boats, or cable cars for tourism purposes.

International Travel: Visa applications, international flight bookings, and entry procedures required for traveling abroad.

Special Occasion Transportation: Transportation services provided for specific situations, such as wedding cars or conference shuttles.

\item \textbf{Health}

Preventive Healthcare: Measures taken to prevent diseases, including health check-ups, vaccinations, and health education.

Outpatient Care: Receiving non-hospitalized treatment at hospitals or clinics, including appointment scheduling, initial diagnosis, follow-up visits, and specialist consultations.

Inpatient Treatment: Hospital admission for treatment, including admission procedures, ward life, surgery arrangements, and discharge preparation.

Rehabilitation Care: Treatment during the recovery period, including rehabilitation training, long-term care, and psychological counseling.

Medication Management: Guidance on medication use and storage methods.

Health Insurance: Purchasing medical insurance products, claim procedures, and health consultation services.

Epidemic Prevention and Control: Measures such as epidemic monitoring, isolation observation, and health code management.


\item \textbf{Leisure}

Outdoor Activities: Recreational activities in natural settings, such as hiking, picnicking, and gardening.

Cultural Experiences: Engaging in cultural activities like visiting museums, watching theatrical performances, or attending film screenings.

Travel Experiences: Domestic or international tourism activities.

Sporting Events: Watching or participating in sports competitions, including esports.

Artistic Pursuits: Engaging in artistic activities such as painting, calligraphy, playing musical instruments, or creating handicrafts.

Digital Entertainment: Leisure activities using digital devices, such as online gaming, live streaming interactions, or virtual reality experiences.

Recreational Fitness: Maintaining physical and mental health through activities like gym workouts or practicing yoga and meditation.

\end{itemize}

\subsubsection{Chinese HellaSwag Annotation for Stage Two}
In order to increase the number of difficult samples, the annotators were given a context and four replaced options regenerated by models to judge whether the label of the question was correct and whether it had a unique correct option. If both are true, the replaced options were retained.


\subsubsection{Hellaswag-Pro Annotation} \label{appe:hellaswag_pro_annotation}
Annotators are provided with the original context, original choices, original label, transformed context, transformed choices, transformed label, and perturbation type for annotation according to different variant definitions. The variant definitions are as follows:

\begin{itemize}[leftmargin=*]
\item\textbf{Problem restatement}:
Restate the original context and the original label corresponding to the original choices in a different way, ensuring the semantics remain unchanged. Other options of the original choices should remain unchanged without restatement. Pay special attention to ensuring that the connection between the context and the choice corresponding to the label is smooth.

\item\textbf{Reverse conversion}:
Combine the original choices corresponding to the original label with the original context into a complete passage. Then, make the last sentence of this passage the context, and transform the remaining sentences into the correct choice. A slight modification is allowed for smoothness. Also, generate five other incorrect options that do not fit the context, modeled on the format and length of the correct option. Place the correct option in the first position and label it as 0. To ensure the context is complete, append "Which is the possible context for this action?" This conversion process aims to infer the potential background through the results. The generated incorrect options should not include supernatural elements and should have a similar word count to the correct option.

\item\textbf{Causal inference}:
Combine the original choices corresponding to the original label with the original context to form a complete passage and turn it into the context. Then, generate the reason for such choices that contain commonsense as the correct option in the choices. The correct choice should be as concise as possible while generating five other evidently incorrect options modeled on the format and length of the correct choice. Put the correct choice in the first position and label it as 0. To ensure the context is complete, append "Which is the possible reason for this action?" This conversion process aims to infer the potential reason through the context and options.

\item\textbf{Negative transformation}:
Modify the original context to end with a negation word as the context, retaining one most unreasonable option and the original choice corresponding to the original label. Then, generate two other reasonable options as choices. Generated options should be similar in length and format to the original options. Place this most unreasonable option as the first element in the choices and label the index of this option in choices as 0. This conversion process aims to transform the original task into a negation prediction, containing one unreasonable option and three other reasonable options.

\item\textbf{Scenario refinement}:
First, select a relatively reasonable option from the incorrect options in the original choices, then modify the original context as the context to allow the selection of this option as the correct choice. The value of choices is equal to the original choices. The label value corresponds to the value of the selected incorrect option. This conversion process aims to refine the context, thereby altering the correct choice.

\item\textbf{Sentence ordering}:

1) Sentence ordering - Short:
First, combine the original choices corresponding to the original label with the original context into a complete sentence. Then, predict the development of subsequent events, continuing to write a few more sentences to form a paragraph. Pay attention to the sequence and completeness of continued sentences, ensuring the uniqueness of the answer. Then, disorder each sentence of this passage and number them. The correct option is the original order of the paragraph, and three other incorrect options are generated based on the correct option by disordering the numbers. Place the correct option in the first position and label it as 0. To ensure the context is complete, append "The correct order is." This conversion process aims to infer the correct order of sentences.

2) Sentence ordering - Long:
Combine the original choices corresponding to the original label with the original context into a complete passage. Then, disorder each sentence of this passage and number them. The correct option is the original order of the paragraph, and three other incorrect options are generated by disordering the numbers. Place the correct option in the first position and label it as 0. To ensure the context is complete, append "The correct order is." This conversion process aims to infer the correct order of sentences.

\item\textbf{Critical testing}:
Modify the original context so that none of the options can be chosen as the context, then add an option of 'None of the above four options are appropriate' to the original choices as choices. The label value corresponds to the index of 'None of the above four options are appropriate'. Note that the modified context should still present a question, ideally with an ending word identical to the original context. This conversion process aims to test the model's critical thinking.
\end{itemize}

\subsubsection{Hellaswag-Pro Human Evaluation}
% 标注员身份/学历 是否有培训 筛选 是否有一致性检验 
Annotators were provided with the context and choices from the Hellaswag-Pro and made selections. We then compared the selections made by annotators with the labels to calculate accuracy.

\section{Prompt Strategy}
The prompting strategies we designed, including Direct, CN-CoT, EN-CoT, CN-XLT and EN-XLT, are as shown Figure \ref{prompt1}, \ref{prompt2} and \ref{prompt3}.

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{CJK*}{UTF8}{gbsn}
\begin{table*}[t]
 \centering
 \scalebox{0.9}{
\begin{tabularx}{\textwidth}{lXX}
  \toprule
  
  \textbf{Problem:} & Question: 周老师负责学校的英语培训工作，常为学生提供实用的语言学习建议。他建议多读原版英文报纸提高阅读水平。可能的原因是 \newline
Choices: \newline 
A.主张减少课外阅读时间增加听力训练。 \newline
B.阅读原版材料能够增强语感并扩大词汇量。 \newline
C.觉得看漫画书更能激发学生的兴趣。\newline
D.相信通过电影比读书更容易学好语言。 & Question: The mother instructs them on how to brush their teeth while laughing. The boy helps his younger sister brush his teeth. She gets them some water to gargle in their mouths. Which could be the possible reason for this action?
Choices:  \newline
A. She wants to ensure they rinse out any remaining toothpaste and clean their mouth thoroughly. \newline
B. She is conducting an experiment to see if water tastes different after brushing. \newline
C. She is using the water as a distraction so they won't notice her leaving the room. \newline
D. She is training them to be synchronized swimmers by coordinating their gargles. \\
  \cdashline{1-3}[3pt/2.5pt]\noalign{\vskip 0.5ex}
Prompt& Chinese Dataset  & English Dataset   \\ 
 \cdashline{1-3}[3pt/2.5pt]\noalign{\vskip 0.5ex}
Direct& Answer: B & Answer: A\\ \cdashline{1-3}[3pt/2.5pt]\noalign{\vskip 0.5ex}
CN-   CoT & 答案：让我们一步步思考。首先，我们需要分析周老师作为英语培训教师的建议 - 多读原版英文报纸。这建议的目的是为了提高学生的阅读水平。查看选项：A提到减少阅读增加听力，与题目表述矛盾；C提到漫画书和D提到电影都偏离了原版英文报纸这一重点；只有B选项提到阅读原版材料可以增强语感并扩大词汇量，这与读原版英文报纸的效果是一致的。原版英文报纸使用地道的英语表达，包含丰富的词汇和表达方式，长期阅读确实能帮助学习者提高语感，扩大词汇量，提升整体语言水平。答案是B。  & 答案：让我们一步步思考。这是一个关于刷牙后漱口的场景，母亲在指导孩子们刷牙，并让他们用水漱口。从日常生活经验来看，刷牙后漱口的主要目的是清除口腔中残留的牙膏和清洁口腔。漱口这个动作不太可能是为了做实验测试水的味道（选项B），也不是为了分散注意力（选项C），更不可能是在训练同步游泳（选项D）。选项A提到让孩子们漱口是为了冲洗掉剩余的牙膏并彻底清洁口腔，这符合我们日常刷牙后漱口的目的，也是最合理的解释。因此答案是A。  \\  \cdashline{1-3}[3pt/2.5pt]\noalign{\vskip 0.5ex}
EN-CoT& Answer: Let's think step by step. First, we know that Teacher   Zhou is responsible for English training at school and often provides   practical language learning advice to students. Second, he specifically   recommends reading original English newspapers to improve reading skills.   Third, reading original English materials has several benefits: it exposes   students to authentic language usage, helps them understand how native   speakers express ideas, enhances language intuition, and naturally expands   their vocabulary through context. Fourth, looking at the options, option B   directly aligns with these benefits by mentioning both improved language   sense and vocabulary expansion. The other options either suggest different   learning methods (like movies or comics) or propose reducing reading time,   which contradicts the teacher's recommendation. Since Teacher Zhou   specifically advocates for reading original English newspapers, option B is   the most logical explanation for his recommendation. The answer is B.& Answer: Let's think step by step. Gargling with water after brushing teeth is   a common and important part of proper dental hygiene. The mother in this   scenario is teaching her children good oral care habits. When we brush our   teeth, toothpaste helps clean them, but some toothpaste residue remains in   our mouth. This residue, if not rinsed out, can leave a lingering taste and   might contain fluoride that shouldn't be swallowed in large amounts. By   giving the children water to gargle, the mother is helping them complete   their tooth-brushing routine correctly. The gargling action helps remove any   loose debris, leftover toothpaste, and bacteria from the mouth, leading to   better oral hygiene. Looking at the other options, they are either illogical   or unrelated to dental care - testing water taste, using it as a distraction,   or swimming practice are not reasonable explanations for gargling after   brushing teeth. The most sensible and practical reason for the mother's   action is to ensure her children properly rinse their mouths after brushing.   The answer is A. \\  

  
  \bottomrule
   
\end{tabularx}
 }

 \caption{Examples of Direct, CN-CoT and EN-CoT Prompt.}
 \label{prompt1}
\end{table*}
\end{CJK*}
\begin{CJK*}{UTF8}{gbsn}
\begin{table*}[t]
 \centering
 \scalebox{0.9}{
\begin{tabularx}{\textwidth}{lXX}
  \toprule
  
  \textbf{Problem:} & Question: 周老师负责学校的英语培训工作，常为学生提供实用的语言学习建议。他建议多读原版英文报纸提高阅读水平。可能的原因是 \newline
Choices: \newline 
A.主张减少课外阅读时间增加听力训练。 \newline
B.阅读原版材料能够增强语感并扩大词汇量。 \newline
C.觉得看漫画书更能激发学生的兴趣。\newline
D.相信通过电影比读书更容易学好语言。 & Question: The mother instructs them on how to brush their teeth while laughing. The boy helps his younger sister brush his teeth. She gets them some water to gargle in their mouths. Which could be the possible reason for this action?
Choices:  \newline
A. She wants to ensure they rinse out any remaining toothpaste and clean their mouth thoroughly. \newline
B. She is conducting an experiment to see if water tastes different after brushing. \newline
C. She is using the water as a distraction so they won't notice her leaving the room. \newline
D. She is training them to be synchronized swimmers by coordinating their gargles. \\
  \cdashline{1-3}[3pt/2.5pt]\noalign{\vskip 0.5ex}
Prompt& Chinese Dataset  & English Dataset   \\ 
 \cdashline{1-3}[3pt/2.5pt]\noalign{\vskip 0.5ex}
CN-   XLT & - & 我希望你扮演一个英文常识推理专家。\newline 你应该用中文复述问题和选项。\newline 你应该一步一步地回答问题，选出正确的答案。\newline 问题：母亲一边笑一边指导他们如何刷牙。男孩帮助他的小妹妹刷牙。她给他们一些水让他们漱口。对此行为，可能的原因是什么？\newline 选项：\newline A.   她想确保他们漱掉任何剩余的牙膏并清洁嘴巴。\newline B.   她正在进行实验，以查看刷牙后水的味道是否不同。\newline C.   她用水作为分散注意力的方法，以便他们不会注意到她离开房间。\newline D. 她通过协调漱口来训练他们成为同步游泳运动员。\newline 分步解答：\newline 1.   母亲给他们水漱口，通常是为了卫生目的。\newline 2.   漱口可以帮助去除残留的牙膏，确保口腔清洁。\newline 3.   选项A是一个常见且合理的原因，符合情境。\newline 4. 选项B、C和D不符合通常情况下的刷牙步骤或目的。\newline 所以答案是A。   \\

  
  \bottomrule
   
\end{tabularx}
 }

 \caption{Examples of CN-XLT Prompt.}
 \label{prompt2}
\end{table*}
\end{CJK*}


\begin{CJK*}{UTF8}{gbsn}
\begin{table*}[t]
 \centering
 \scalebox{0.85}{
\begin{tabularx}{\textwidth}{lXX}
  \toprule
  
  \textbf{Problem:} & Question: 周老师负责学校的英语培训工作，常为学生提供实用的语言学习建议。他建议多读原版英文报纸提高阅读水平。可能的原因是 \newline
Choices: \newline 
A.主张减少课外阅读时间增加听力训练。 \newline
B.阅读原版材料能够增强语感并扩大词汇量。 \newline
C.觉得看漫画书更能激发学生的兴趣。\newline
D.相信通过电影比读书更容易学好语言。 & Question: The mother instructs them on how to brush their teeth while laughing. The boy helps his younger sister brush his teeth. She gets them some water to gargle in their mouths. Which could be the possible reason for this action?
Choices:  \newline
A. She wants to ensure they rinse out any remaining toothpaste and clean their mouth thoroughly. \newline
B. She is conducting an experiment to see if water tastes different after brushing. \newline
C. She is using the water as a distraction so they won't notice her leaving the room. \newline
D. She is training them to be synchronized swimmers by coordinating their gargles. \\
  \cdashline{1-3}[3pt/2.5pt]\noalign{\vskip 0.5ex}
Prompt& Chinese Dataset  & English Dataset   \\ 
 \cdashline{1-3}[3pt/2.5pt]\noalign{\vskip 0.5ex}

EN-XLT& I want you to act as a commonsense reasoning expert for   Chinese.\newline You should retell the question and choices in English.\newline You should   do the answer step by step to choose the right answer.\newline You should tell me   the answer in this format 'So the answer is'.\newline Question: Teacher Zhou is in   charge of the English training at school and often provides students with   practical language learning advice. He suggests reading original English   newspapers to improve reading skills. The possible reason is:\newline Choices:\newline A. Advocates   reducing extracurricular reading time to increase listening practice.\newline B.   Reading original materials can enhance language sense and expand   vocabulary.\newline C. Believes that reading comic books can better stimulate   students' interest.\newline D. Believes it is easier to learn a language well   through movies than through reading.\newline \newline Step-by-step answer:\newline 1. The   statement suggests reading original English newspapers as a way to improve   reading skills.\newline 2. Option B suggests that reading original materials can enhance   language sense and expand vocabulary, which aligns with the goal of improving   reading skills.\newline 3. Option A talks about reducing reading time, which   contradicts the suggestion to read newspapers.\newline 4. Option C talks about comic   books, which is not related to reading original newspapers.\newline 5. Option D   suggests movies are more effective, which again, does not align with the   focus on reading newspapers.\newline So the answer is B. & - \\  

  
  \bottomrule
   
\end{tabularx}
 }

 \caption{Examples of EN-XLT Prompt.}
 \label{prompt3}
\end{table*}
\end{CJK*}



\section{Case Study}
Figure \ref{case} shows an example of the Chinese hellaswag generation process, from which we can see that our wrong options are becoming more and more challenging.

\begin{CJK*}{UTF8}{gbsn}
\begin{table*}[]
 \centering
 \scalebox{0.79}{
\begin{tabular}{l|l}
\hline
Step& Gererated or Filtered Content\\ \hline
\begin{tabular}[c]{@{}l@{}}\textbf{Step   1}\\Over-generate\\context with broad\\type and detailed\\type with Qwen- Max\end{tabular}& \begin{tabular}[c]{@{}l@{}}Context：丽丽报名参加了日本京都的一趟文化之旅，深度体验了传统艺伎表演。她\\ Broad Type: 休闲娱乐\\ Detailed Type: 旅游体验\\ Context:   Lili signed up for a cultural tour in Kyoto, Japan, and experienced a  traditional \\ geisha performance. She\\ Broad Type: Leisure\\ Detailed Type: Travel Experience\end{tabular}\\ \hline
\begin{tabular}[c]{@{}l@{}}\textbf{Step   2}\\Over-generate\\candidate choices\\with Qwen-Max\end{tabular} & \begin{tabular}[c]{@{}l@{}}Choices:\\1. 深深地被茶道的精致仪式所吸引。(Score:10)\\2. 抱怨为什么所有的寺庙都没有钟声。(Score:3)\\3. 抱怨旅馆里的榻榻米太过柔软。(Score:3)\\4. 要求在京都找到寿司制作的机器人。(Score:2)\\5. 质疑为什么艺伎穿的是拖鞋而不是高跟鞋。(Score:5)\\6. 坚持要在神社祈祷时使用英文。(Score:2)\\7. 对亲自动手参与和服穿着过程表现出浓厚的兴趣。(Score:9)\\8. 询问为什么没人听说过她喜欢的动漫角色。(Score:4)\\9. 希望能在旅途中看到更多现代化的建筑。(Score:3)\\10. 感到遗憾没能在旅途中参与一场本地的相扑比赛。(Score:4)\\ Choices:\\1. was deeply attracted by the exquisite ritual of the tea ceremony.   (Score:10)\\2. complained why there were no bells in all temples. (Score:3)\\3. complained that the tatami in the hotel was too soft. (Score:3)\\4. requested to find a sushi-making robot in Kyoto. (Score:2)\\5. questioned why geishas wore slippers instead of high heels.   (Score:5)\\6. insisted on using English when praying at the shrine. (Score:2)\\7. showed great interest in participating in the process of wearing a   kimono. (Score:9)\\8. asked why no one has heard of her favorite anime character.   (Score:4)\\9. hoped to see more modern buildings during the trip. (Score:3)\\10. regreted not being able to attend a local sumo match during the trip.   (Score:4)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}\textbf{Step   3-1}\\Find filtered choices\\with Qwen-Max\end{tabular}& \begin{tabular}[c]{@{}l@{}}Choices:\\1. 深深地被茶道的精致仪式所吸引。(Score:10)\\5. 质疑为什么艺伎穿的是拖鞋而不是高跟鞋。(Score:5)\\8. 询问为什么没人听说过她喜欢的动漫角色。(Score:4)\\10. 感到遗憾没能在旅途中参与一场本地的相扑比赛。(Score:4)\\2. 抱怨为什么所有的寺庙都没有钟声。(Score:3)\\9. 希望能在旅途中看到更多现代化的建筑。(Score:3) \\ Choices:\\1. was deeply attracted by the exquisite ritual of the tea ceremony.   (Score:10)\\5. questioned why geishas wore slippers instead of high heels. (Score:5)\\8. asked why no one has heard of her favorite anime character.   (Score:4)\\10. regreted not being able to attend a local sumo match during the trip.   (Score:4)\\2. complained why there were no bells in all temples. (Score:3)\\9. hoped to see more modern buildings during the trip. (Score:3)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}\textbf{Step   3-2}\\Find filtered choices\\with human annotators\end{tabular}  & \begin{tabular}[c]{@{}l@{}}Choices:\\1. 深深地被茶道的精致仪式所吸引。(Score:10)\\5. 质疑为什么艺伎穿的是拖鞋而不是高跟鞋。(Score:5)\\8. 询问为什么没人听说过她喜欢的动漫角色。(Score:4)\\9. 希望能在旅途中看到更多现代化的建筑。(Score:3)\\ Choices:\\1. was deeply attracted by the exquisite ritual of the tea ceremony.   (Score:10)\\5. questioned why geishas wore slippers instead of high heels.   (Score:5)\\8. asked why no one has heard of her favorite anime character.   (Score:4)\\9. hoped to see more modern buildings during the trip. (Score:3)\end{tabular}  \\ \hline
\begin{tabular}[c]{@{}l@{}}\textbf{Step   4}\\Replace easily-\\identifiable false\\choices with\\adversarial ones\\through human-in-\\the-loop alternating\\adversarial filtering\end{tabular} & \begin{tabular}[c]{@{}l@{}}Choices:\\1.深深地被茶道的精致仪式所吸引。\\2.学习了传统的日式剑道和弓道技巧。\\3.欣赏了京都著名的樱花季和红叶景观。\\4.品尝了正宗的关西风味章鱼烧和大阪烧。\\ Choices:\\1. was deeply attracted by the exquisite ritual of the tea ceremony. \\2. learned traditional Japanese kendo and archery techniques.\\3. enjoyed Kyoto's famous cherry blossom season and red leaves.\\4. tasted authentic Kansai-style takoyaki and okonomiyaki.\end{tabular}   \\ \hline
\end{tabular}
}
 \caption{An example of Chinese HellaSwag Gereration. Step 3-1 filters the top 5 wrong options with scores below 9 to prevent multiple correct options, and Step 3-2 select the most confusing wrong options by human annotators.}
 \label{case}
\end{table*}
\end{CJK*}

\section{Chinese HellaSwag Evaluation}\label{cn_eval}
We also evaluate the overall results of Chinese Hellaswag using both open-source and closed-source models, analyzing them from the perspectives of broad categories and length categories. As shown in Table \ref{cn_type}, within all categories of Chinese Hellaswag, \textit{Traffic} is the most challenging, with an average accuracy of only 58.56\%, while the \textit{Education} category is the easiest, achieving an average accuracy of 77.64\%. Additionally, as the context length increases, the difficulty of the problems generally decreases, with average accuracy of long types at 72\%, medium types at 70\%, and short types at 64\% as shown in Table \ref{cn_length}. Overall, the closed-source models outperform open-source models. Among the closed-source models, Claude-3.5 performs the best, reaching an accuracy of 94\%, whereas among open-source models, Qwen2.5-72B shows the highest performance, achieving 71\%.

\begin{table*}[h]
\centering
 \scalebox{0.9}{

\begin{tabular}{l|llllllll|l}
\hline
Model                 & Education & Health & Famliy & Leisure & Shopping & Sociality & Traffic & Work & AVG  \\ \hline
Baichuan2-7B-Base     & 0.76      & 0.71   & 0.66   & 0.69    & 0.62     & 0.68   & 0.55    & 0.70 & 0.67 \\
Baichuan2-13B-Base    & 0.78      & 0.70   & 0.68   & 0.70    & 0.64     & 0.69   & 0.57    & 0.71 & 0.68 \\
Meta-Llama-3-8B       & 0.74      & 0.59   & 0.55   & 0.57    & 0.54     & 0.56   & 0.46    & 0.61 & 0.58 \\
Meta-Llama-3-70B      & 0.76      & 0.65   & 0.63   & 0.66    & 0.63     & 0.67   & 0.54    & 0.65 & 0.65 \\
Mistral-7B-v0.1       & 0.70      & 0.59   & 0.52   & 0.56    & 0.57     & 0.57   & 0.50    & 0.61 & 0.58 \\
Qwen2.5-0.5B          & 0.72      & 0.66   & 0.53   & 0.60    & 0.53     & 0.58   & 0.47    & 0.66 & 0.59 \\
Qwen2.5-1.5B          & 0.75      & 0.66   & 0.60   & 0.62    & 0.59     & 0.64   & 0.51    & 0.67 & 0.63 \\
Qwen2.5-3B            & 0.75      & 0.67   & 0.63   & 0.66    & 0.61     & 0.66   & 0.55    & 0.68 & 0.65 \\
Qwen2.5-7B            & 0.76      & 0.68   & 0.66   & 0.68    & 0.63     & 0.69   & 0.58    & 0.70 & 0.67 \\
Qwen2.5-14B           & 0.78      & 0.68   & 0.68   & 0.69    & 0.65     & 0.69   & 0.58    & 0.71 & 0.68 \\
Qwen2.5-32B           & 0.77      & 0.69   & 0.68   & 0.69    & 0.66     & 0.69   & 0.58    & 0.69 & 0.68 \\
Qwen2.5-72B           & 0.78      & 0.70   & 0.70   & 0.72    & 0.69     & 0.73   & 0.60    & 0.73 & 0.71 \\
Yi-1.5-6B             & 0.78      & 0.69   & 0.66   & 0.68    & 0.63     & 0.69   & 0.56    & 0.72 & 0.68 \\
Yi-1.5-9B             & 0.78      & 0.70   & 0.67   & 0.70    & 0.64     & 0.69   & 0.57    & 0.72 & 0.68 \\
deepseek-llm-7b-base  & 0.79      & 0.70   & 0.67   & 0.69    & 0.64     & 0.69   & 0.57    & 0.73 & 0.68 \\
deepseek-llm-67b-base & 0.80      & 0.72   & 0.70   & 0.72    & 0.67     & 0.70   & 0.58    & 0.74 & 0.70 \\
gemma-2-2b            & 0.73      & 0.62   & 0.57   & 0.60    & 0.60     & 0.60   & 0.50    & 0.66 & 0.61 \\
gemma-2-9b            & 0.78      & 0.68   & 0.64   & 0.67    & 0.65     & 0.69   & 0.55    & 0.74 & 0.67 \\
gemma-2-27b           & 0.72      & 0.66   & 0.64   & 0.62    & 0.62     & 0.58   & 0.50    & 0.67 & 0.63 \\
internlm2\_5-1\_8b    & 0.73      & 0.64   & 0.58   & 0.64    & 0.54     & 0.60   & 0.49    & 0.65 & 0.61 \\
internlm2\_5-7b       & 0.76      & 0.68   & 0.67   & 0.70    & 0.63     & 0.67   & 0.60    & 0.69 & 0.67 \\
internlm2\_5-20b      & 0.76      & 0.67   & 0.68   & 0.70    & 0.64     & 0.68   & 0.59    & 0.69 & 0.68 \\
GPT-4o                & 0.91      & 0.92   & 0.88   & 0.92    & 0.90     & 0.90   & 0.86    & 0.91 & 0.90 \\
Claude-3-5            & 0.94      & 0.96   & 0.94   & 0.94    & 0.95     & 0.95   & 0.91    & 0.96 & 0.94 \\
Gemini-1.5-pro        & 0.88      & 0.91   & 0.88   & 0.90    & 0.90     & 0.91   & 0.85    & 0.91 & 0.89 \\
Qwen-Max              & 0.91      & 0.95   & 0.91   & 0.92    & 0.93     & 0.94   & 0.88    & 0.94 & 0.92 \\ \hline
AVG                   & 0.78      & 0.71   & 0.68   & 0.70    & 0.67     & 0.70   & 0.60    & 0.73 & 0.69 \\ \hline
\end{tabular}

}
  \caption{Model Performance on Chinese HellaSwag based on broad category under Direct Prompt.}
 \label{cn_type}
\end{table*}



\begin{table*}[]
\centering
 \scalebox{1.1}{

\begin{tabular}{l|lll|l}
\hline
\multicolumn{1}{l|}{Model} & Long & Medium & Short & AVG  \\ \hline
Baichuan2-7B-Base           & 0.70 & 0.70   & 0.62  & 0.67 \\
Baichuan2-13B-Base          & 0.72 & 0.71   & 0.62  & 0.68 \\
Meta-Llama-3-8B             & 0.64 & 0.59   & 0.51  & 0.58 \\
Meta-Llama-3-70B            & 0.70 & 0.67   & 0.58  & 0.65 \\
Mistral-7B-v0.1             & 0.63 & 0.58   & 0.52  & 0.58 \\
Qwen2.5-0.5B                & 0.63 & 0.61   & 0.54  & 0.59 \\
Qwen2.5-1.5B                & 0.67 & 0.64   & 0.58  & 0.63 \\
Qwen2.5-3B                  & 0.68 & 0.67   & 0.60  & 0.65 \\
Qwen2.5-7B                  & 0.71 & 0.69   & 0.62  & 0.67 \\
Qwen2.5-14B                 & 0.72 & 0.70   & 0.63  & 0.68 \\
Qwen2.5-32B                 & 0.72 & 0.69   & 0.62  & 0.68 \\
Qwen2.5-72B                 & 0.74 & 0.73   & 0.65  & 0.71 \\
Yi-1.5-6B                   & 0.72 & 0.70   & 0.62  & 0.68 \\
Yi-1.5-9B                   & 0.73 & 0.70   & 0.62  & 0.68 \\
deepseek-llm-7b-base        & 0.73 & 0.71   & 0.61  & 0.68 \\
deepseek-llm-67b-base       & 0.76 & 0.72   & 0.62  & 0.70 \\
gemma-2-2b                  & 0.65 & 0.61   & 0.56  & 0.61 \\
gemma-2-9b                  & 0.72 & 0.69   & 0.61  & 0.67 \\
gemma-2-27b                 & 0.68 & 0.64   & 0.56  & 0.63 \\
internlm2\_5-1\_8b          & 0.65 & 0.63   & 0.55  & 0.61 \\
internlm2\_5-7b             & 0.72 & 0.69   & 0.61  & 0.67 \\
internlm2\_5-20b            & 0.73 & 0.69   & 0.61  & 0.68 \\
GPT-4o                      & 0.87 & 0.90   & 0.93  & 0.90 \\
Claude-3-5                  & 0.92 & 0.95   & 0.97  & 0.94 \\
Gemini-1.5-pro              & 0.86 & 0.91   & 0.92  & 0.89 \\
Qwen-Max                    & 0.89 & 0.93   & 0.95  & 0.92 \\ \hline
AVG                         & 0.73 & 0.71   & 0.65  & 0.69 \\ \hline
\end{tabular}


}
  \caption{Model Performance on Chinese HellaSwag based on length category under Direct Prompt.}
 \label{cn_length}
\end{table*}


\section{Experiment Detailed Result}\label{detailed}
Figures \ref{qwen1} to \ref{internlm} show the detailed results of the open-source models on the 9 prompt strategies and 7 variants we design.

In addition, we also use direct prompt to evaluate the performance of the Qwen, LLaMA, and Mixtral series of intruct models on our robustness task, as shown in Table \ref{instruct-tab} and Figure \ref{instruct}. We can observe that the intruct models have improved performance in robustness task compared to the base models.

\begin{table*}[]
\centering
 \scalebox{0.9}{
\begin{tabular}{l|llllllll}
\hline
\textbf{Model\_\_Prompt\_Language}& CI   & CT   & NT   & PR   & RC   & SR   &SO  & \textbf{ARA} \\ \hline
Qwen2.5-0.5B\_\_direct\_cn& 0.66 & 0.36 & 0.06 & 0.64 & 0.50 & 0.36 & 0.58 & 0.45   \\
Qwen2.5-1.5B\_\_direct\_cn& 0.70 & 0.35 & 0.07 & 0.65 & 0.55 & 0.38 & 0.54 & 0.46   \\
Qwen2.5-3B\_\_direct\_cn& 0.66 & 0.37 & 0.06 & 0.66 & 0.57 & 0.42 & 0.68 & 0.49   \\
Qwen2.5-0.5B\_\_direct\_cn& 0.52 & 0.34 & 0.07 & 0.67 & 0.36 & 0.35 & 0.37 & 0.38   \\
Qwen2.5-1.5B\_\_direct\_cn& 0.56 & 0.38 & 0.06 & 0.75 & 0.32 & 0.35 & 0.36 & 0.40   \\
Qwen2.5-3B\_\_direct\_cn& 0.59 & 0.40 & 0.05 & 0.78 & 0.29 & 0.34 & 0.35 & 0.40   \\ \hline
   
Qwen2.5-0.5B\_\_few\_shot\_en\_cot\_cn  & 0.75 & 0.48 & 0.14 & 0.62 & 0.43 & 0.38 & 0.23 & 0.43   \\
Qwen2.5-0.5B\_\_few\_shot\_en\_cot\_en  & 0.80 & 0.29 & 0.42 & 0.47 & 0.41 & 0.32 & 0.74 & 0.49   \\
Qwen2.5-0.5B\_\_few\_shot\_en\_xlt\_cn  & 0.40 & 0.11 & 0.12 & 0.35 & 0.34 & 0.18 & 0.09 & 0.23   \\
Qwen2.5-0.5B\_\_few\_shot\_cn\_cot\_cn  & 0.73 & 0.51 & 0.17 & 0.62 & 0.36 & 0.29 & 0.30 & 0.42   \\
Qwen2.5-0.5B\_\_few\_shot\_cn\_cot\_en  & 0.81 & 0.37 & 0.64 & 0.35 & 0.43 & 0.26 & 0.89 & 0.54   \\
Qwen2.5-0.5B\_\_few\_shot\_cn\_xlt\_en  & 0.73 & 0.28 & 0.15 & 0.29 & 0.36 & 0.18 & 0.88 & 0.41   \\ \hline
   
Qwen2.5-1.5B\_\_few\_shot\_en\_cot\_cn  & 0.91 & 0.40 & 0.75 & 0.82 & 0.79 & 0.43 & 0.40 & 0.64   \\
Qwen2.5-1.5B\_\_few\_shot\_en\_cot\_en  & 0.82 & 0.26 & 0.50 & 0.66 & 0.42 & 0.47 & 0.82 & 0.56   \\
Qwen2.5-1.5B\_\_few\_shot\_en\_xlt\_cn  & 0.33 & 0.18 & 0.41 & 0.50 & 0.45 & 0.22 & 0.23 & 0.33   \\
Qwen2.5-1.5B\_\_few\_shot\_cn\_cot\_cn  & 0.89 & 0.48 & 0.82 & 0.84 & 0.74 & 0.42 & 0.62 & 0.68   \\
Qwen2.5-1.5B\_\_few\_shot\_cn\_cot\_en  & 0.85 & 0.41 & 0.37 & 0.63 & 0.39 & 0.41 & 0.70 & 0.54   \\
Qwen2.5-1.5B\_\_few\_shot\_cn\_xlt\_en  & 0.45 & 0.17 & 0.15 & 0.54 & 0.26 & 0.23 & 0.64 & 0.35   \\ \hline
   
Qwen2.5-3B\_\_few\_shot\_en\_cot\_cn& 0.94 & 0.50 & 0.83 & 0.89 & 0.86 & 0.47 & 0.70 & 0.74   \\
Qwen2.5-3B\_\_few\_shot\_en\_cot\_en& 0.89 & 0.39 & 0.52 & 0.72 & 0.48 & 0.49 & 0.68 & 0.59   \\
Qwen2.5-3B\_\_few\_shot\_en\_xlt\_cn& 0.41 & 0.27 & 0.45 & 0.51 & 0.51 & 0.32 & 0.10 & 0.37   \\
Qwen2.5-3B\_\_few\_shot\_cn\_cot\_cn& 0.92 & 0.49 & 0.90 & 0.89 & 0.81 & 0.46 & 0.80 & 0.75   \\
Qwen2.5-3B\_\_few\_shot\_cn\_cot\_en& 0.89 & 0.35 & 0.57 & 0.70 & 0.48 & 0.44 & 0.73 & 0.59   \\
Qwen2.5-3B\_\_few\_shot\_cn\_xlt\_en& 0.72 & 0.19 & 0.64 & 0.58 & 0.40 & 0.28 & 0.51 & 0.47   \\ \hline
   
Qwen2.5-0.5B\_\_zero\_shot\_en\_cot\_cn & 0.54 & 0.18 & 0.07 & 0.54 & 0.34 & 0.23 & 0.18 & 0.30   \\
Qwen2.5-0.5B\_\_zero\_shot\_en\_cot\_en & 0.53 & 0.17 & 0.29 & 0.36 & 0.30 & 0.29 & 0.27 & 0.32   \\
Qwen2.5-0.5B\_\_zero\_shot\_en\_xlt\_cn & 0.14 & 0.11 & 0.02 & 0.09 & 0.07 & 0.08 & 0.00 & 0.07   \\
Qwen2.5-0.5B\_\_zero\_shot\_cn\_cot\_cn & 0.59 & 0.33 & 0.10 & 0.49 & 0.26 & 0.00 & 0.06 & 0.26   \\
Qwen2.5-0.5B\_\_zero\_shot\_cn\_cot\_en & 0.42 & 0.18 & 0.28 & 0.27 & 0.24 & 0.20 & 0.43 & 0.29   \\
Qwen2.5-0.5B\_\_zero\_shot\_cn\_xlt\_en & 0.01 & 0.01 & 0.02 & 0.10 & 0.01 & 0.06 & 0.01 & 0.03   \\ \hline
   
Qwen2.5-1.5B\_\_zero\_shot\_en\_cot\_cn & 0.95 & 0.57 & 0.23 & 0.86 & 0.74 & 0.49 & 0.39 & 0.61   \\
Qwen2.5-1.5B\_\_zero\_shot\_en\_cot\_en & 0.69 & 0.32 & 0.54 & 0.47 & 0.48 & 0.40 & 0.41 & 0.47   \\
Qwen2.5-1.5B\_\_zero\_shot\_en\_xlt\_cn & 0.03 & 0.03 & 0.01 & 0.01 & 0.00 & 0.04 & 0.01 & 0.02   \\
Qwen2.5-1.5B\_\_zero\_shot\_cn\_cot\_cn & 0.70 & 0.45 & 0.35 & 0.72 & 0.55 & 0.00 & 0.38 & 0.45   \\
Qwen2.5-1.5B\_\_zero\_shot\_cn\_cot\_en & 0.48 & 0.26 & 0.05 & 0.53 & 0.28 & 0.36 & 0.40 & 0.34   \\
Qwen2.5-1.5B\_\_zero\_shot\_cn\_xlt\_en & 0.02 & 0.02 & 0.03 & 0.04 & 0.00 & 0.03 & 0.00 & 0.02   \\ \hline
   
Qwen2.5-3B\_\_zero\_shot\_en\_cot\_cn   & 0.92 & 0.44 & 0.50 & 0.88 & 0.78 & 0.46 & 0.73 & 0.67   \\
Qwen2.5-3B\_\_zero\_shot\_en\_cot\_en   & 0.81 & 0.27 & 0.33 & 0.66 & 0.43 & 0.45 & 0.56 & 0.50   \\
Qwen2.5-3B\_\_zero\_shot\_en\_xlt\_cn   & 0.52 & 0.51 & 0.48 & 0.54 & 0.50 & 0.43 & 0.53 & 0.50   \\
Qwen2.5-3B\_\_zero\_shot\_cn\_cot\_cn   & 0.75 & 0.43 & 0.28 & 0.72 & 0.55 & 0.00 & 0.58 & 0.47   \\
Qwen2.5-3B\_\_zero\_shot\_cn\_cot\_en   & 0.51 & 0.20 & 0.07 & 0.53 & 0.37 & 0.32 & 0.39 & 0.34   \\
Qwen2.5-3B\_\_zero\_shot\_cn\_xlt\_en   & 0.82 & 0.25 & 0.03 & 0.73 & 0.37 & 0.47 & 0.13 & 0.40   \\ \hline
\end{tabular}
}
  \caption{Performance of Qwen Series (0.5B-3B). And NT, CT, CI, SO, SR, RC, PR are the abbreviations for the variant names of Negation Transformation, Critical Testing, Causal Inference, Sentence Ordering, Scenario Refinement, Reverse Conversion and Problem Restatement.}
 \label{qwen1}
\end{table*}

\begin{table*}[]
\centering
 \scalebox{0.8}{
 % Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{tabular}{l|llllllll}
\hline
\textbf{Model\_\_Prompt\_Language}& CI   & CT   & NT   & PR   & RC   & SR   &SO  & \textbf{ARA} \\ \hline
Qwen2.5-7B\_\_direct\_cn& 0.66 & 0.34 & 0.07 & 0.67 & 0.62 & 0.41 & 0.77 & 0.51   \\
Qwen2.5-14B\_\_direct\_cn   & 0.67 & 0.35 & 0.07 & 0.69 & 0.63 & 0.43 & 0.75 & 0.51   \\
Qwen2.5-32B\_\_direct\_cn   & 0.68 & 0.36 & 0.06 & 0.68 & 0.63 & 0.43 & 0.87 & 0.53   \\
Qwen2.5-72B\_\_direct\_cn   & 0.67 & 0.39 & 0.08 & 0.69 & 0.65 & 0.44 & 0.92 & 0.55   \\
Qwen2.5-7B\_\_direct\_en& 0.64 & 0.41 & 0.05 & 0.82 & 0.27 & 0.33 & 0.56 & 0.44   \\
Qwen2.5-14B\_\_direct\_en   & 0.66 & 0.42 & 0.05 & 0.83 & 0.28 & 0.35 & 0.58 & 0.45   \\
Qwen2.5-32B\_\_direct\_en   & 0.65 & 0.42 & 0.05 & 0.83 & 0.29 & 0.34 & 0.65 & 0.46   \\
Qwen2.5-72B\_\_direct\_en   & 0.67 & 0.43 & 0.05 & 0.86 & 0.30 & 0.34 & 0.71 & 0.48   \\ \hline
 
 
 
Qwen2.5-7B\_\_few\_shot\_en\_cot\_cn  & 0.95 & 0.57 & 0.92 & 0.90 & 0.89 & 0.53 & 0.60 & 0.77   \\
Qwen2.5-7B\_\_few\_shot\_en\_cot\_en  & 0.88 & 0.50 & 0.78 & 0.84 & 0.54 & 0.54 & 0.81 & 0.70   \\
Qwen2.5-7B\_\_few\_shot\_en\_xlt\_cn  & 0.51 & 0.27 & 0.50 & 0.48 & 0.53 & 0.41 & 0.44 & 0.45   \\
Qwen2.5-7B\_\_few\_shot\_cn\_cot\_cn  & 0.96 & 0.55 & 0.91 & 0.92 & 0.85 & 0.59 & 0.93 & 0.81   \\
Qwen2.5-7B\_\_few\_shot\_cn\_cot\_en  & 0.93 & 0.53 & 0.79 & 0.75 & 0.56 & 0.54 & 0.81 & 0.70   \\
Qwen2.5-7B\_\_few\_shot\_cn\_xlt\_en  & 0.76 & 0.30 & 0.77 & 0.69 & 0.43 & 0.27 & 0.65 & 0.55   \\ \hline
 
Qwen2.5-14B\_\_few\_shot\_en\_cot\_cn & 0.97 & 0.58 & 0.93 & 0.93 & 0.88 & 0.66 & 0.94 & 0.84   \\
Qwen2.5-14B\_\_few\_shot\_en\_cot\_en & 0.93 & 0.50 & 0.75 & 0.88 & 0.57 & 0.55 & 0.81 & 0.71   \\
Qwen2.5-14B\_\_few\_shot\_en\_xlt\_cn & 0.63 & 0.40 & 0.65 & 0.69 & 0.58 & 0.49 & 0.69 & 0.59   \\
Qwen2.5-14B\_\_few\_shot\_cn\_cot\_cn & 0.97 & 0.60 & 0.94 & 0.91 & 0.87 & 0.66 & 0.92 & 0.84   \\
Qwen2.5-14B\_\_few\_shot\_cn\_cot\_en & 0.93 & 0.56 & 0.77 & 0.83 & 0.56 & 0.55 & 0.82 & 0.72   \\
Qwen2.5-14B\_\_few\_shot\_cn\_xlt\_en & 0.82 & 0.38 & 0.74 & 0.71 & 0.42 & 0.36 & 0.57 & 0.57   \\ \hline
 
Qwen2.5-32B\_\_few\_shot\_en\_cot\_cn & 0.98 & 0.63 & 0.91 & 0.94 & 0.92 & 0.71 & 0.95 & 0.86   \\
Qwen2.5-32B\_\_few\_shot\_en\_cot\_en & 0.93 & 0.59 & 0.84 & 0.88 & 0.64 & 0.58 & 0.81 & 0.75   \\
Qwen2.5-32B\_\_few\_shot\_en\_xlt\_cn & 0.68 & 0.46 & 0.75 & 0.66 & 0.60 & 0.49 & 0.80 & 0.64   \\
Qwen2.5-32B\_\_few\_shot\_cn\_cot\_cn & 0.98 & 0.61 & 0.94 & 0.93 & 0.90 & 0.68 & 0.95 & 0.85   \\
Qwen2.5-32B\_\_few\_shot\_cn\_cot\_en & 0.94 & 0.66 & 0.84 & 0.90 & 0.62 & 0.59 & 0.83 & 0.77   \\
Qwen2.5-32B\_\_few\_shot\_cn\_xlt\_en & 0.82 & 0.46 & 0.82 & 0.82 & 0.51 & 0.42 & 0.59 & 0.63   \\ \hline
 
Qwen2.5-72B\_\_few\_shot\_en\_cot\_cn & 0.98 & 0.66 & 0.91 & 0.94 & 0.92 & 0.73 & 0.92 & 0.87   \\
Qwen2.5-72B\_\_few\_shot\_en\_cot\_en & 0.91 & 0.59 & 0.80 & 0.92 & 0.67 & 0.58 & 0.81 & 0.75   \\
Qwen2.5-72B\_\_few\_shot\_en\_xlt\_cn & 0.50 & 0.30 & 0.68 & 0.62 & 0.55 & 0.54 & 0.76 & 0.56   \\
Qwen2.5-72B\_\_few\_shot\_cn\_cot\_cn & 0.97 & 0.62 & 0.95 & 0.93 & 0.91 & 0.74 & 0.92 & 0.86   \\
Qwen2.5-72B\_\_few\_shot\_cn\_cot\_en & 0.94 & 0.62 & 0.80 & 0.90 & 0.69 & 0.59 & 0.83 & 0.77   \\
Qwen2.5-72B\_\_few\_shot\_cn\_xlt\_en & 0.59 & 0.44 & 0.81 & 0.70 & 0.53 & 0.33 & 0.57 & 0.57   \\ \hline
 
 
Qwen2.5-7B\_\_zero\_shot\_en\_cot\_cn & 0.82 & 0.56 & 0.73 & 0.85 & 0.70 & 0.54 & 0.78 & 0.71   \\
Qwen2.5-7B\_\_zero\_shot\_en\_cot\_en & 0.82 & 0.40 & 0.53 & 0.70 & 0.38 & 0.53 & 0.59 & 0.57   \\
Qwen2.5-7B\_\_zero\_shot\_en\_xlt\_cn & 0.83 & 0.55 & 0.62 & 0.80 & 0.73 & 0.53 & 0.83 & 0.70   \\
Qwen2.5-7B\_\_zero\_shot\_cn\_cot\_cn & 0.74 & 0.42 & 0.30 & 0.76 & 0.65 & 0.00 & 0.50 & 0.48   \\
Qwen2.5-7B\_\_zero\_shot\_cn\_cot\_en & 0.70 & 0.27 & 0.09 & 0.57 & 0.33 & 0.37 & 0.54 & 0.41   \\
Qwen2.5-7B\_\_zero\_shot\_cn\_xlt\_en & 0.02 & 0.00 & 0.00 & 0.00 & 0.01 & 0.00 & 0.01 & 0.01   \\ \hline
 
 
Qwen2.5-14B\_\_zero\_shot\_en\_cot\_cn& 0.62 & 0.46 & 0.81 & 0.78 & 0.66 & 0.54 & 0.86 & 0.68   \\
Qwen2.5-14B\_\_zero\_shot\_en\_cot\_en& 0.88 & 0.44 & 0.41 & 0.70 & 0.46 & 0.53 & 0.72 & 0.59   \\
Qwen2.5-14B\_\_zero\_shot\_en\_xlt\_cn& 0.93 & 0.63 & 0.85 & 0.93 & 0.79 & 0.70 & 0.92 & 0.82   \\
Qwen2.5-14B\_\_zero\_shot\_cn\_cot\_cn& 0.79 & 0.56 & 0.74 & 0.81 & 0.73 & 0.51 & 0.82 & 0.71   \\
Qwen2.5-14B\_\_zero\_shot\_cn\_cot\_en& 0.72 & 0.37 & 0.24 & 0.64 & 0.44 & 0.38 & 0.59 & 0.48   \\
Qwen2.5-14B\_\_zero\_shot\_cn\_xlt\_en& 0.01 & 0.00 & 0.17 & 0.09 & 0.22 & 0.02 & 0.01 & 0.07   \\ \hline
 
Qwen2.5-32B\_\_zero\_shot\_en\_cot\_cn& 0.80 & 0.43 & 0.83 & 0.81 & 0.68 & 0.57 & 0.86 & 0.71   \\
Qwen2.5-32B\_\_zero\_shot\_en\_cot\_en& 0.86 & 0.52 & 0.56 & 0.82 & 0.53 & 0.54 & 0.75 & 0.65   \\
Qwen2.5-32B\_\_zero\_shot\_en\_xlt\_cn& 0.78 & 0.58 & 0.83 & 0.75 & 0.49 & 0.52 & 0.49 & 0.63   \\
Qwen2.5-32B\_\_zero\_shot\_cn\_cot\_cn& 0.87 & 0.60 & 0.81 & 0.87 & 0.76 & 0.00 & 0.91 & 0.69   \\
Qwen2.5-32B\_\_zero\_shot\_cn\_cot\_en& 0.82 & 0.47 & 0.30 & 0.79 & 0.51 & 0.48 & 0.61 & 0.57   \\
Qwen2.5-32B\_\_zero\_shot\_cn\_xlt\_en& 0.64 & 0.45 & 0.36 & 0.62 & 0.55 & 0.27 & 0.62 & 0.50   \\ \hline
 
Qwen2.5-72B\_\_zero\_shot\_en\_cot\_cn& 0.84 & 0.48 & 0.82 & 0.84 & 0.72 & 0.60 & 0.73 & 0.72   \\
Qwen2.5-72B\_\_zero\_shot\_en\_cot\_en& 0.78 & 0.50 & 0.44 & 0.79 & 0.49 & 0.51 & 0.75 & 0.61   \\
Qwen2.5-72B\_\_zero\_shot\_en\_xlt\_cn& 0.06 & 0.06 & 0.18 & 0.21 & 0.19 & 0.05 & 0.17 & 0.13   \\
Qwen2.5-72B\_\_zero\_shot\_cn\_cot\_cn& 0.79 & 0.59 & 0.73 & 0.86 & 0.73 & 0.56 & 0.82 & 0.73   \\
Qwen2.5-72B\_\_zero\_shot\_cn\_cot\_en& 0.70 & 0.31 & 0.15 & 0.76 & 0.55 & 0.51 & 0.48 & 0.49   \\
Qwen2.5-72B\_\_zero\_shot\_cn\_xlt\_en& 0.03 & 0.01 & 0.14 & 0.20 & 0.39 & 0.07 & 0.04 & 0.12   \\ \hline
\end{tabular}
}
 \caption{Performance of Qwen Series (7B-72B). And NT, CT, CI, SO, SR, RC, PR are the abbreviations for the variant names of Negation Transformation, Critical Testing, Causal Inference, Sentence Ordering, Scenario Refinement, Reverse Conversion and Problem Restatement.}
 \label{qwen2}
\end{table*}

\begin{table*}[]
\centering
 \scalebox{0.9}{
\begin{tabular}{l|llllllll}
\hline
\textbf{Model\_\_Prompt\_Language}  & CI   & CT   & NT   & PR   & RC   & SR   &SO  & \textbf{ARA} \\ \hline
deepseek-llm-7b-base\_\_direct\_cn & 0.61 & 0.35 & 0.07 & 0.67 & 0.59 & 0.43 & 0.43 & 0.48\\
deepseek-llm-67b-base\_\_direct\_cn& 0.65 & 0.38 & 0.08 & 0.71 & 0.63 & 0.46 & 0.55 & 0.49\\
deepseek-llm-7b-base\_\_direct\_en & 0.53 & 0.41 & 0.05 & 0.81 & 0.33 & 0.34 & 0.37 & 0.40\\
deepseek-llm-67b-base\_\_direct\_en& 0.57 & 0.42 & 0.05 & 0.85 & 0.25 & 0.34 & 0.37 & 0.41\\ \hline

deepseek-llm-7b-base\_\_few\_shot\_en\_cot\_cn   & 0.85 & 0.49 & 0.35 & 0.81 & 0.62 & 0.43 & 0.40 & 0.56\\
deepseek-llm-7b-base\_\_few\_shot\_en\_cot\_en   & 0.85 & 0.20 & 0.45 & 0.53 & 0.26 & 0.40 & 0.90 & 0.51\\
deepseek-llm-7b-base\_\_few\_shot\_en\_xlt\_cn   & 0.27 & 0.06 & 0.10 & 0.33 & 0.42 & 0.28 & 0.28 & 0.25\\
deepseek-llm-7b-base\_\_few\_shot\_cn\_cot\_cn   & 0.88 & 0.55 & 0.66 & 0.74 & 0.63 & 0.49 & 0.56 & 0.64\\
deepseek-llm-7b-base\_\_few\_shot\_cn\_cot\_en   & 0.82 & 0.27 & 0.62 & 0.44 & 0.38 & 0.34 & 0.82 & 0.52\\
deepseek-llm-7b-base\_\_few\_shot\_cn\_xlt\_en   & 0.34 & 0.16 & 0.51 & 0.46 & 0.45 & 0.29 & 0.81 & 0.43\\ \hline

deepseek-llm-67b-base\_\_few\_shot\_en\_cot\_cn  & 0.96 & 0.51 & 0.89 & 0.91 & 0.84 & 0.68 & 0.81 & 0.80\\
deepseek-llm-67b-base\_\_few\_shot\_en\_cot\_en  & 0.92 & 0.39 & 0.88 & 0.85 & 0.53 & 0.49 & 0.88 & 0.71\\
deepseek-llm-67b-base\_\_few\_shot\_en\_xlt\_cn  & 0.42 & 0.10 & 0.70 & 0.63 & 0.51 & 0.50 & 0.36 & 0.46\\
deepseek-llm-67b-base\_\_few\_shot\_cn\_cot\_cn  & 0.97 & 0.61 & 0.91 & 0.89 & 0.82 & 0.71 & 0.90 & 0.83\\
deepseek-llm-67b-base\_\_few\_shot\_cn\_cot\_en  & 0.92 & 0.43 & 0.84 & 0.79 & 0.45 & 0.44 & 0.90 & 0.68\\
deepseek-llm-67b-base\_\_few\_shot\_cn\_xlt\_en  & 0.52 & 0.24 & 0.18 & 0.74 & 0.46 & 0.34 & 0.65 & 0.45\\ \hline

deepseek-llm-7b-base\_\_zero\_shot\_en\_cot\_cn  & 0.18 & 0.03 & 0.03 & 0.15 & 0.05 & 0.05 & 0.12 & 0.09\\
deepseek-llm-7b-base\_\_zero\_shot\_en\_cot\_en  & 0.11 & 0.14 & 0.01 & 0.16 & 0.09 & 0.19 & 0.01 & 0.10\\
deepseek-llm-7b-base\_\_zero\_shot\_en\_xlt\_cn  & 0.03 & 0.01 & 0.01 & 0.08 & 0.02 & 0.02 & 0.01 & 0.02\\
deepseek-llm-7b-base\_\_zero\_shot\_cn\_cot\_cn  & 0.40 & 0.18 & 0.07 & 0.23 & 0.18 & 0.00 & 0.09 & 0.16\\
deepseek-llm-7b-base\_\_zero\_shot\_cn\_cot\_en  & 0.27 & 0.12 & 0.07 & 0.17 & 0.13 & 0.11 & 0.00 & 0.12\\
deepseek-llm-7b-base\_\_zero\_shot\_cn\_xlt\_en  & 0.01 & 0.00 & 0.00 & 0.02 & 0.01 & 0.01 & 0.00 & 0.01\\ \hline

deepseek-llm-67b-base\_\_zero\_shot\_en\_cot\_cn & 0.03 & 0.17 & 0.01 & 0.14 & 0.12 & 0.12 & 0.40 & 0.14\\
deepseek-llm-67b-base\_\_zero\_shot\_en\_cot\_en & 0.08 & 0.12 & 0.01 & 0.11 & 0.02 & 0.08 & 0.33 & 0.11\\
deepseek-llm-67b-base\_\_zero\_shot\_en\_xlt\_cn & 0.64 & 0.34 & 0.02 & 0.29 & 0.29 & 0.30 & 0.03 & 0.27\\
deepseek-llm-67b-base\_\_zero\_shot\_cn\_cot\_cn & 0.36 & 0.14 & 0.03 & 0.36 & 0.18 & 0.20 & 0.34 & 0.23\\
deepseek-llm-67b-base\_\_zero\_shot\_cn\_cot\_en & 0.20 & 0.08 & 0.01 & 0.08 & 0.02 & 0.07 & 0.16 & 0.09\\
deepseek-llm-67b-base\_\_zero\_shot\_cn\_xlt\_en & 0.49 & 0.06 & 0.00 & 0.29 & 0.14 & 0.11 & 0.07 & 0.17\\ \hline
\end{tabular}
}
 \caption{Performance of DeepSeek Series. And NT, CT, CI, SO, SR, RC, PR are the abbreviations for the variant names of Negation Transformation, Critical Testing, Causal Inference, Sentence Ordering, Scenario Refinement, Reverse Conversion and Problem Restatement.}
 \label{deepseek}
\end{table*}


\begin{table*}[]
\centering
 \scalebox{0.9}{

\begin{tabular}{l|llllllll}
\hline
\textbf{Model\_\_Prompt\_Language} & CI   & CT   & NT   & PR   & RC   & SR   &SO  & \textbf{ARA} \\ \hline
Yi-6B\_\_direct\_cn & 0.68 & 0.37 & 0.07 & 0.67 & 0.60 & 0.42 & 0.67 & 0.50\\
Yi-9B\_\_direct\_cn & 0.70 & 0.37 & 0.08 & 0.66 & 0.64 & 0.44 & 0.62 & 0.50\\
Yi-34B\_\_direct\_cn& 0.69 & 0.38 & 0.09 & 0.67 & 0.67 & 0.44 & 0.72 & 0.52\\
Yi-6B\_\_direct\_en & 0.58 & 0.39 & 0.05 & 0.77 & 0.27 & 0.34 & 0.36 & 0.39\\
Yi-9B\_\_direct\_en & 0.56 & 0.41 & 0.06 & 0.80 & 0.26 & 0.35 & 0.35 & 0.40\\
Yi-34B\_\_direct\_en& 0.62 & 0.41 & 0.05 & 0.81 & 0.27 & 0.34 & 0.36 & 0.41\\ \hline
   
Yi-1.5-6B\_\_few\_shot\_en\_cot\_cn   & 0.91 & 0.60 & 0.78 & 0.84 & 0.82 & 0.56 & 0.68 & 0.74\\
Yi-1.5-6B\_\_few\_shot\_en\_cot\_en   & 0.89 & 0.36 & 0.45 & 0.68 & 0.33 & 0.51 & 0.65 & 0.55\\
Yi-1.5-6B\_\_few\_shot\_en\_xlt\_cn   & 0.41 & 0.10 & 0.47 & 0.48 & 0.44 & 0.35 & 0.23 & 0.35\\
Yi-1.5-6B\_\_few\_shot\_cn\_cot\_cn   & 0.94 & 0.58 & 0.85 & 0.86 & 0.76 & 0.57 & 0.72 & 0.75\\
Yi-1.5-6B\_\_few\_shot\_cn\_cot\_en   & 0.83 & 0.46 & 0.60 & 0.57 & 0.33 & 0.40 & 0.86 & 0.58\\
Yi-1.5-6B\_\_few\_shot\_cn\_xlt\_en   & 0.72 & 0.23 & 0.15 & 0.55 & 0.41 & 0.27 & 0.41 & 0.39\\ \hline
   
Yi-1.5-9B\_\_few\_shot\_en\_cot\_cn   & 0.97 & 0.60 & 0.85 & 0.88 & 0.88 & 0.64 & 0.78 & 0.80\\
Yi-1.5-9B\_\_few\_shot\_en\_cot\_en   & 0.93 & 0.45 & 0.78 & 0.77 & 0.55 & 0.50 & 0.77 & 0.68\\
Yi-1.5-9B\_\_few\_shot\_en\_xlt\_cn   & 0.48 & 0.24 & 0.68 & 0.50 & 0.45 & 0.39 & 0.41 & 0.45\\
Yi-1.5-9B\_\_few\_shot\_cn\_cot\_cn   & 0.96 & 0.61 & 0.90 & 0.89 & 0.86 & 0.65 & 0.87 & 0.82\\
Yi-1.5-9B\_\_few\_shot\_cn\_cot\_en   & 0.92 & 0.50 & 0.67 & 0.70 & 0.52 & 0.47 & 0.77 & 0.65\\
Yi-1.5-9B\_\_few\_shot\_cn\_xlt\_en   & 0.84 & 0.25 & 0.53 & 0.69 & 0.46 & 0.29 & 0.45 & 0.50\\ \hline
   
Yi-1.5-34B\_\_few\_shot\_en\_cot\_cn  & 0.96 & 0.54 & 0.91 & 0.91 & 0.88 & 0.70 & 0.93 & 0.83\\
Yi-1.5-34B\_\_few\_shot\_en\_cot\_en  & 0.92 & 0.59 & 0.79 & 0.92 & 0.60 & 0.54 & 0.82 & 0.74\\
Yi-1.5-34B\_\_few\_shot\_en\_xlt\_cn  & 0.27 & 0.12 & 0.58 & 0.36 & 0.44 & 0.40 & 0.43 & 0.37\\
Yi-1.5-34B\_\_few\_shot\_cn\_cot\_cn  & 0.96 & 0.57 & 0.92 & 0.89 & 0.86 & 0.68 & 0.93 & 0.83\\
Yi-1.5-34B\_\_few\_shot\_cn\_cot\_en  & 0.93 & 0.57 & 0.79 & 0.87 & 0.57 & 0.52 & 0.78 & 0.72\\
Yi-1.5-34B\_\_few\_shot\_cn\_xlt\_en  & 0.62 & 0.33 & 0.71 & 0.74 & 0.49 & 0.30 & 0.57 & 0.54\\ \hline
   
Yi-1.5-6B\_\_zero\_shot\_en\_cot\_cn  & 0.77 & 0.52 & 0.12 & 0.67 & 0.64 & 0.34 & 0.48 & 0.51\\
Yi-1.5-6B\_\_zero\_shot\_en\_cot\_en  & 0.83 & 0.43 & 0.50 & 0.59 & 0.55 & 0.54 & 0.21 & 0.52\\
Yi-1.5-6B\_\_zero\_shot\_en\_xlt\_cn  & 0.20 & 0.17 & 0.02 & 0.19 & 0.15 & 0.06 & 0.30 & 0.16\\
Yi-1.5-6B\_\_zero\_shot\_cn\_cot\_cn  & 0.69 & 0.27 & 0.05 & 0.63 & 0.57 & 0.00 & 0.24 & 0.35\\
Yi-1.5-6B\_\_zero\_shot\_cn\_cot\_en  & 0.58 & 0.26 & 0.04 & 0.48 & 0.25 & 0.37 & 0.17 & 0.31\\
Yi-1.5-6B\_\_zero\_shot\_cn\_xlt\_en  & 0.05 & 0.05 & 0.02 & 0.02 & 0.01 & 0.03 & 0.07 & 0.04\\ \hline
   
Yi-1.5-9B\_\_zero\_shot\_en\_cot\_cn  & 0.60 & 0.47 & 0.14 & 0.67 & 0.48 & 0.39 & 0.70 & 0.49\\
Yi-1.5-9B\_\_zero\_shot\_en\_cot\_en  & 0.87 & 0.38 & 0.26 & 0.68 & 0.47 & 0.51 & 0.48 & 0.52\\
Yi-1.5-9B\_\_zero\_shot\_en\_xlt\_cn  & 0.94 & 0.51 & 0.27 & 0.85 & 0.76 & 0.56 & 0.70 & 0.65\\
Yi-1.5-9B\_\_zero\_shot\_cn\_cot\_cn  & 0.79 & 0.36 & 0.12 & 0.72 & 0.70 & 0.50 & 0.14 & 0.47\\
Yi-1.5-9B\_\_zero\_shot\_cn\_cot\_en  & 0.61 & 0.18 & 0.10 & 0.50 & 0.35 & 0.36 & 0.29 & 0.34\\
Yi-1.5-9B\_\_zero\_shot\_cn\_xlt\_en  & 0.91 & 0.64 & 0.03 & 0.84 & 0.45 & 0.58 & 0.59 & 0.58\\ \hline
   
Yi-1.5-34B\_\_zero\_shot\_en\_cot\_cn & 0.28 & 0.37 & 0.18 & 0.24 & 0.28 & 0.17 & 0.58 & 0.30\\
Yi-1.5-34B\_\_zero\_shot\_en\_cot\_en & 0.85 & 0.48 & 0.35 & 0.75 & 0.50 & 0.47 & 0.66 & 0.58\\
Yi-1.5-34B\_\_zero\_shot\_en\_xlt\_cn & 0.09 & 0.03 & 0.08 & 0.08 & 0.07 & 0.05 & 0.09 & 0.07\\
Yi-1.5-34B\_\_zero\_shot\_cn\_cot\_cn & 0.82 & 0.36 & 0.29 & 0.72 & 0.69 & 0.00 & 0.77 & 0.52\\
Yi-1.5-34B\_\_zero\_shot\_cn\_cot\_en & 0.77 & 0.31 & 0.05 & 0.64 & 0.50 & 0.43 & 0.64 & 0.48\\
Yi-1.5-34B\_\_zero\_shot\_cn\_xlt\_en & 0.78 & 0.25 & 0.02 & 0.26 & 0.43 & 0.16 & 0.34 & 0.32\\ \hline
\end{tabular}

}
 \caption{Performance of Yi Series. And NT, CT, CI, SO, SR, RC, PR are the abbreviations for the variant names of Negation Transformation, Critical Testing, Causal Inference, Sentence Ordering, Scenario Refinement, Reverse Conversion and Problem Restatement.}
 \label{baichuan}
\end{table*}

\begin{table*}[]
\centering
 \scalebox{0.9}{
\begin{tabular}{l|llllllll}
\hline
\textbf{Model\_\_Prompt\_Language}  & CI   & CT   & NT   & PR   & RC   & SR   &SO  & \textbf{ARA} \\ \hline
Meta-Llama-3-8B\_\_direct\_cn   & 0.57 & 0.37 & 0.09 & 0.63 & 0.51 & 0.46 & 0.63 & 0.47\\
Meta-Llama-3-70B\_\_direct\_cn  & 0.63 & 0.40 & 0.08 & 0.67 & 0.60 & 0.46 & 0.57 & 0.49\\
Meta-Llama-3-8B\_\_direct\_en   & 0.56 & 0.41 & 0.05 & 0.82 & 0.27 & 0.35 & 0.36 & 0.40\\
Meta-Llama-3-70B\_\_direct\_cn  & 0.57 & 0.43 & 0.04 & 0.86 & 0.29 & 0.34 & 0.36 & 0.41\\
 \hline
Meta-Llama-3-8B\_\_few\_shot\_en\_cot\_cn   & 0.90 & 0.45 & 0.68 & 0.81 & 0.67 & 0.57 & 0.58 & 0.66\\
Meta-Llama-3-8B\_\_few\_shot\_en\_cot\_en   & 0.79 & 0.39 & 0.79 & 0.64 & 0.51 & 0.45 & 0.94 & 0.64\\
Meta-Llama-3-8B\_\_few\_shot\_en\_xlt\_cn   & 0.36 & 0.21 & 0.39 & 0.48 & 0.54 & 0.39 & 0.23 & 0.37\\
Meta-Llama-3-8B\_\_few\_shot\_cn\_cot\_cn   & 0.87 & 0.45 & 0.67 & 0.73 & 0.54 & 0.60 & 0.63 & 0.64\\
Meta-Llama-3-8B\_\_few\_shot\_cn\_cot\_en   & 0.76 & 0.46 & 0.56 & 0.59 & 0.39 & 0.38 & 0.82 & 0.56\\
Meta-Llama-3-8B\_\_few\_shot\_cn\_xlt\_en   & 0.10 & 0.10 & 0.06 & 0.27 & 0.13 & 0.05 & 0.32 & 0.15\\ \hline


Meta-Llama-3-70B\_\_few\_shot\_en\_cot\_cn  & 0.97 & 0.58 & 0.87 & 0.91 & 0.89 & 0.70 & 0.91 & 0.83\\
Meta-Llama-3-70B\_\_few\_shot\_en\_cot\_en  & 0.92 & 0.42 & 0.77 & 0.84 & 0.57 & 0.50 & 0.88 & 0.70\\
Meta-Llama-3-70B\_\_few\_shot\_en\_xlt\_cn  & 0.46 & 0.22 & 0.65 & 0.62 & 0.56 & 0.52 & 0.52 & 0.51\\
Meta-Llama-3-70B\_\_few\_shot\_cn\_cot\_cn  & 0.97 & 0.36 & 0.87 & 0.84 & 0.69 & 0.63 & 0.92 & 0.75\\
Meta-Llama-3-70B\_\_few\_shot\_cn\_cot\_en  & 0.93 & 0.45 & 0.73 & 0.86 & 0.56 & 0.44 & 0.85 & 0.69\\
Meta-Llama-3-70B\_\_few\_shot\_cn\_xlt\_en  & 0.12 & 0.09 & 0.31 & 0.50 & 0.25 & 0.14 & 0.28 & 0.24\\ \hline

Meta-Llama-3-8B\_\_zero\_shot\_en\_cot\_cn  & 0.59 & 0.24 & 0.07 & 0.40 & 0.32 & 0.29 & 0.40 & 0.33\\
Meta-Llama-3-8B\_\_zero\_shot\_en\_cot\_en  & 0.52 & 0.18 & 0.11 & 0.38 & 0.19 & 0.33 & 0.38 & 0.30\\
Meta-Llama-3-8B\_\_zero\_shot\_en\_xlt\_cn  & 0.39 & 0.17 & 0.01 & 0.42 & 0.16 & 0.19 & 0.16 & 0.22\\
Meta-Llama-3-8B\_\_zero\_shot\_cn\_cot\_cn  & 0.53 & 0.21 & 0.09 & 0.40 & 0.34 & 0.50 & 0.12 & 0.31\\
Meta-Llama-3-8B\_\_zero\_shot\_cn\_cot\_en  & 0.50 & 0.16 & 0.04 & 0.30 & 0.22 & 0.24 & 0.12 & 0.23\\
Meta-Llama-3-8B\_\_zero\_shot\_cn\_xlt\_en  & 0.43 & 0.08 & 0.02 & 0.22 & 0.23 & 0.15 & 0.03 & 0.17\\ \hline


Meta-Llama-3-70B\_\_zero\_shot\_en\_cot\_cn & 0.78 & 0.43 & 0.06 & 0.64 & 0.63 & 0.37 & 0.56 & 0.50\\
Meta-Llama-3-70B\_\_zero\_shot\_en\_cot\_en & 0.78 & 0.35 & 0.04 & 0.64 & 0.45 & 0.41 & 0.57 & 0.46\\
Meta-Llama-3-70B\_\_zero\_shot\_en\_xlt\_cn & 0.79 & 0.55 & 0.05 & 0.77 & 0.49 & 0.36 & 0.22 & 0.46\\
Meta-Llama-3-70B\_\_zero\_shot\_cn\_cot\_cn & 0.63 & 0.35 & 0.13 & 0.56 & 0.48 & 0.34 & 0.63 & 0.44\\
Meta-Llama-3-70B\_\_zero\_shot\_cn\_cot\_en & 0.58 & 0.30 & 0.02 & 0.50 & 0.32 & 0.31 & 0.38 & 0.34\\
Meta-Llama-3-70B\_\_zero\_shot\_cn\_xlt\_en & 0.90 & 0.39 & 0.01 & 0.86 & 0.52 & 0.50 & 0.62 & 0.54\\ \hline
\end{tabular}
}
 \caption{Performance of LLaMA Series. And NT, CT, CI, SO, SR, RC, PR are the abbreviations for the variant names of Negation Transformation, Critical Testing, Causal Inference, Sentence Ordering, Scenario Refinement, Reverse Conversion and Problem Restatement.}
 \label{llama}
\end{table*}


\begin{table*}[]
\centering
 \scalebox{0.9}{

\begin{tabular}{l|llllllll}
\hline
\textbf{Model\_\_Prompt\_Language}& CI   & CT   & NT   & PR   & RC   & SR   &SO  & \textbf{ARA} \\ \hline
Mistral-7B-v0.1\_\_direct\_cn& 0.58 & 0.37 & 0.10 & 0.64 & 0.50 & 0.44 & 0.62 & 0.46 \\
Mixtral-8x7B-v0.1\_\_direct\_cn  & 0.59 & 0.36 & 0.09 & 0.65 & 0.54 & 0.46 & 0.58 & 0.47 \\
Mixtral-8x22B-v0.1\_\_direct\_cn & 0.66 & 0.39 & 0.07 & 0.69 & 0.57 & 0.43 & 0.74 & 0.51 \\
Mistral-7B-v0.1\_\_direct\_en& 0.57 & 0.41 & 0.05 & 0.82 & 0.31 & 0.34 & 0.40 & 0.42 \\
Mixtral-8x7B-v0.1\_\_direct\_en  & 0.56 & 0.42 & 0.06 & 0.85 & 0.31 & 0.33 & 0.36 & 0.41 \\
Mixtral-8x22B-v0.1\_\_direct\_en & 0.57 & 0.42 & 0.04 & 0.85 & 0.31 & 0.34 & 0.36 & 0.41 \\ \hline
  
Mistral-7B-v0.1\_\_few\_shot\_en\_cot\_cn & 0.90 & 0.51 & 0.62 & 0.83 & 0.71 & 0.56 & 0.57 & 0.67 \\
Mistral-7B-v0.1\_\_few\_shot\_en\_cot\_en & 0.86 & 0.34 & 0.84 & 0.63 & 0.53 & 0.47 & 0.90 & 0.65 \\
Mistral-7B-v0.1\_\_few\_shot\_en\_xlt\_cn & 0.13 & 0.05 & 0.17 & 0.20 & 0.30 & 0.25 & 0.44 & 0.22 \\
Mistral-7B-v0.1\_\_few\_shot\_cn\_cot\_cn & 0.56 & 0.06 & 0.34 & 0.49 & 0.41 & 0.43 & 0.51 & 0.40 \\
Mistral-7B-v0.1\_\_few\_shot\_cn\_cot\_en & 0.61 & 0.28 & 0.35 & 0.45 & 0.17 & 0.16 & 0.67 & 0.39 \\
Mistral-7B-v0.1\_\_few\_shot\_cn\_xlt\_en & 0.00 & 0.00 & 0.01 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 \\ \hline
Mixtral-8x7B-v0.1\_\_few\_shot\_en\_cot\_cn   & 0.94 & 0.60 & 0.80 & 0.90 & 0.81 & 0.66 & 0.65 & 0.77 \\
Mixtral-8x7B-v0.1\_\_few\_shot\_en\_cot\_en   & 0.91 & 0.45 & 0.59 & 0.82 & 0.56 & 0.54 & 0.87 & 0.68 \\
Mixtral-8x7B-v0.1\_\_few\_shot\_en\_xlt\_cn   & 0.15 & 0.04 & 0.18 & 0.18 & 0.34 & 0.23 & 0.24 & 0.19 \\
Mixtral-8x7B-v0.1\_\_few\_shot\_cn\_cot\_cn   & 0.75 & 0.13 & 0.35 & 0.47 & 0.52 & 0.42 & 0.76 & 0.48 \\
Mixtral-8x7B-v0.1\_\_few\_shot\_cn\_cot\_en   & 0.61 & 0.33 & 0.38 & 0.56 & 0.28 & 0.13 & 0.64 & 0.42 \\
Mixtral-8x7B-v0.1\_\_few\_shot\_cn\_xlt\_en   & 0.00 & 0.00 & 0.02 & 0.02 & 0.01 & 0.00 & 0.00 & 0.01 \\ \hline
  
Mixtral-8x22B-v0.1\_\_few\_shot\_en\_cot\_cn  & 0.96 & 0.63 & 0.90 & 0.92 & 0.86 & 0.69 & 0.87 & 0.83 \\
Mixtral-8x22B-v0.1\_\_few\_shot\_en\_cot\_en  & 0.92 & 0.59 & 0.69 & 0.88 & 0.63 & 0.57 & 0.78 & 0.72 \\
Mixtral-8x22B-v0.1\_\_few\_shot\_en\_xlt\_cn  & 0.26 & 0.08 & 0.36 & 0.43 & 0.36 & 0.31 & 0.33 & 0.30 \\
Mixtral-8x22B-v0.1\_\_few\_shot\_cn\_cot\_cn  & 0.60 & 0.08 & 0.36 & 0.37 & 0.57 & 0.32 & 0.82 & 0.45 \\
Mixtral-8x22B-v0.1\_\_few\_shot\_cn\_cot\_en  & 0.62 & 0.44 & 0.34 & 0.65 & 0.26 & 0.21 & 0.76 & 0.47 \\
Mixtral-8x22B-v0.1\_\_few\_shot\_cn\_xlt\_en  & 0.01 & 0.00 & 0.01 & 0.01 & 0.01 & 0.00 & 0.01 & 0.01 \\ \hline
Mistral-7B-v0.1\_\_zero\_shot\_en\_cot\_cn& 0.05 & 0.06 & 0.01 & 0.06 & 0.03 & 0.06 & 0.11 & 0.05 \\
Mistral-7B-v0.1\_\_zero\_shot\_en\_cot\_en& 0.27 & 0.31 & 0.02 & 0.19 & 0.08 & 0.19 & 0.11 & 0.17 \\
Mistral-7B-v0.1\_\_zero\_shot\_en\_xlt\_cn& 0.01 & 0.00 & 0.00 & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 \\
Mistral-7B-v0.1\_\_zero\_shot\_cn\_cot\_cn& 0.21 & 0.08 & 0.01 & 0.17 & 0.08 & 0.00 & 0.00 & 0.08 \\
Mistral-7B-v0.1\_\_zero\_shot\_cn\_cot\_en& 0.11 & 0.04 & 0.02 & 0.08 & 0.04 & 0.05 & 0.08 & 0.06 \\
Mistral-7B-v0.1\_\_zero\_shot\_cn\_xlt\_en& 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ \hline
Mixtral-8x7B-v0.1\_\_zero\_shot\_en\_cot\_en  & 0.66 & 0.25 & 0.07 & 0.47 & 0.24 & 0.40 & 0.29 & 0.34 \\
Mixtral-8x7B-v0.1\_\_zero\_shot\_en\_xlt\_cn  & 0.00 & 0.01 & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 \\
Mixtral-8x7B-v0.1\_\_zero\_shot\_cn\_cot\_cn  & 0.52 & 0.20 & 0.04 & 0.38 & 0.27 & 0.28 & 0.14 & 0.26 \\
Mixtral-8x7B-v0.1\_\_zero\_shot\_cn\_cot\_en  & 0.48 & 0.17 & 0.01 & 0.31 & 0.15 & 0.23 & 0.09 & 0.21 \\
Mixtral-8x7B-v0.1\_\_zero\_shot\_cn\_xlt\_en  & 0.06 & 0.00 & 0.00 & 0.04 & 0.02 & 0.02 & 0.00 & 0.02 \\ \hline
  
  
Mixtral-8x22B-v0.1\_\_zero\_shot\_en\_cot\_cn & 0.89 & 0.57 & 0.05 & 0.83 & 0.60 & 0.51 & 0.57 & 0.57 \\
Mixtral-8x22B-v0.1\_\_zero\_shot\_en\_cot\_en & 0.82 & 0.33 & 0.13 & 0.52 & 0.48 & 0.36 & 0.58 & 0.46 \\
Mixtral-8x22B-v0.1\_\_zero\_shot\_en\_xlt\_cn & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
Mixtral-8x22B-v0.1\_\_zero\_shot\_cn\_cot\_cn & 0.45 & 0.27 & 0.15 & 0.39 & 0.33 & 0.26 & 0.21 & 0.29 \\
Mixtral-8x22B-v0.1\_\_zero\_shot\_cn\_cot\_en & 0.44 & 0.22 & 0.04 & 0.35 & 0.22 & 0.21 & 0.35 & 0.26 \\
Mixtral-8x22B-v0.1\_\_zero\_shot\_cn\_xlt\_en & 0.22 & 0.02 & 0.00 & 0.07 & 0.07 & 0.04 & 0.10 & 0.07 \\ \hline
\end{tabular}

}
 \caption{Performance of Mixtral Series. And NT, CT, CI, SO, SR, RC, PR are the abbreviations for the variant names of Negation Transformation, Critical Testing, Causal Inference, Sentence Ordering, Scenario Refinement, Reverse Conversion and Problem Restatement.}
 \label{mixtral}
\end{table*}


\begin{table*}[]
\centering
 \scalebox{0.9}{

\begin{tabular}{l|llllllll}
\hline
\textbf{Model\_\_Prompt\_Language}     & CI   & CT   & NT   & PR   & RC   & SR   &SO  & \textbf{ARA} \\ \hline
gemma-2-2b\_\_direct\_cn     & 0.58 & 0.33 & 0.08 & 0.64 & 0.55 & 0.41 & 0.59 & 0.45   \\
gemma-2-9b\_\_direct\_cn     & 0.59 & 0.33 & 0.06 & 0.68 & 0.59 & 0.47 & 0.55 & 0.47   \\
gemma-2-27\_\_direct\_cn     & 0.61 & 0.33 & 0.09 & 0.64 & 0.64 & 0.44 & 0.64 & 0.49   \\
gemma-2-2b\_\_direct\_en     & 0.49 & 0.38 & 0.05 & 0.74 & 0.35 & 0.35 & 0.39 & 0.39   \\
gemma-2-9b\_\_direct\_en     & 0.56 & 0.39 & 0.04 & 0.77 & 0.30 & 0.36 & 0.36 & 0.40   \\
gemma-2-27\_\_direct\_en     & 0.57 & 0.42 & 0.05 & 0.85 & 0.28 & 0.34 & 0.36 & 0.41   \\ \hline
   
gemma-2-2b\_\_few\_shot\_en\_cot\_cn   & 0.86 & 0.47 & 0.24 & 0.75 & 0.55 & 0.43 & 0.30 & 0.52   \\
gemma-2-2b\_\_few\_shot\_en\_cot\_en   & 0.69 & 0.25 & 0.48 & 0.59 & 0.37 & 0.40 & 0.80 & 0.51   \\
gemma-2-2b\_\_few\_shot\_en\_xlt\_cn   & 0.28 & 0.11 & 0.22 & 0.40 & 0.43 & 0.27 & 0.15 & 0.26   \\
gemma-2-2b\_\_few\_shot\_cn\_cot\_cn   & 0.70 & 0.45 & 0.31 & 0.71 & 0.50 & 0.33 & 0.46 & 0.49   \\
gemma-2-2b\_\_few\_shot\_cn\_cot\_en   & 0.85 & 0.41 & 0.33 & 0.27 & 0.31 & 0.26 & 0.55 & 0.43   \\
gemma-2-2b\_\_few\_shot\_cn\_xlt\_en   & 0.58 & 0.14 & 0.67 & 0.35 & 0.39 & 0.24 & 0.55 & 0.42   \\ \hline
   
gemma-2-9b\_\_few\_shot\_en\_cot\_cn   & 0.96 & 0.56 & 0.80 & 0.89 & 0.86 & 0.62 & 0.79 & 0.78   \\
gemma-2-9b\_\_few\_shot\_en\_cot\_en   & 0.88 & 0.50 & 0.79 & 0.84 & 0.51 & 0.55 & 0.80 & 0.69   \\
gemma-2-9b\_\_few\_shot\_en\_xlt\_cn   & 0.33 & 0.16 & 0.34 & 0.51 & 0.49 & 0.41 & 0.40 & 0.38   \\
gemma-2-9b\_\_few\_shot\_cn\_cot\_cn   & 0.93 & 0.47 & 0.77 & 0.85 & 0.80 & 0.54 & 0.87 & 0.75   \\
gemma-2-9b\_\_few\_shot\_cn\_cot\_en   & 0.91 & 0.46 & 0.71 & 0.67 & 0.50 & 0.44 & 0.76 & 0.63   \\
gemma-2-9b\_\_few\_shot\_cn\_xlt\_en   & 0.73 & 0.25 & 0.42 & 0.58 & 0.41 & 0.30 & 0.61 & 0.47   \\ \hline
   
gemma-2-27b\_\_few\_shot\_en\_cot\_cn  & 0.43 & 0.33 & 0.33 & 0.43 & 0.42 & 0.32 & 0.14 & 0.34   \\
gemma-2-27b\_\_few\_shot\_en\_cot\_en  & 0.34 & 0.15 & 0.24 & 0.27 & 0.29 & 0.21 & 0.11 & 0.23   \\
gemma-2-27b\_\_few\_shot\_en\_xlt\_cn  & 0.09 & 0.04 & 0.07 & 0.08 & 0.08 & 0.04 & 0.04 & 0.06   \\
gemma-2-27b\_\_few\_shot\_cn\_cot\_cn  & 0.30 & 0.06 & 0.15 & 0.21 & 0.23 & 0.16 & 0.22 & 0.19   \\
gemma-2-27b\_\_few\_shot\_cn\_cot\_en  & 0.34 & 0.10 & 0.18 & 0.23 & 0.21 & 0.13 & 0.21 & 0.20   \\
gemma-2-27b\_\_few\_shot\_cn\_xlt\_en  & 0.14 & 0.01 & 0.03 & 0.02 & 0.03 & 0.02 & 0.01 & 0.04   \\ \hline
   
gemma-2-2b\_\_zero\_shot\_en\_cot\_cn  & 0.31 & 0.19 & 0.08 & 0.15 & 0.19 & 0.09 & 0.15 & 0.17   \\
gemma-2-2b\_\_zero\_shot\_en\_cot\_en  & 0.30 & 0.28 & 0.04 & 0.17 & 0.20 & 0.16 & 0.43 & 0.22   \\
gemma-2-2b\_\_zero\_shot\_en\_xlt\_cn  & 0.43 & 0.17 & 0.11 & 0.25 & 0.25 & 0.15 & 0.07 & 0.20   \\
gemma-2-2b\_\_zero\_shot\_cn\_cot\_cn  & 0.10 & 0.13 & 0.06 & 0.08 & 0.05 & 0.00 & 0.03 & 0.06   \\
gemma-2-2b\_\_zero\_shot\_cn\_cot\_en  & 0.07 & 0.04 & 0.09 & 0.05 & 0.07 & 0.03 & 0.01 & 0.05   \\
gemma-2-2b\_\_zero\_shot\_cn\_xlt\_en  & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00   \\ \hline
   
gemma-2-9b\_\_zero\_shot\_en\_cot\_cn  & 0.66 & 0.42 & 0.06 & 0.62 & 0.53 & 0.34 & 0.40 & 0.43   \\
gemma-2-9b\_\_zero\_shot\_en\_cot\_en  & 0.71 & 0.37 & 0.04 & 0.52 & 0.27 & 0.37 & 0.53 & 0.40   \\
gemma-2-9b\_\_zero\_shot\_en\_xlt\_cn  & 0.04 & 0.02 & 0.02 & 0.05 & 0.11 & 0.01 & 0.00 & 0.04   \\
gemma-2-9b\_\_zero\_shot\_cn\_cot\_cn  & 0.51 & 0.36 & 0.09 & 0.54 & 0.45 & 0.29 & 0.39 & 0.37   \\
gemma-2-9b\_\_zero\_shot\_cn\_cot\_en  & 0.64 & 0.17 & 0.03 & 0.36 & 0.23 & 0.23 & 0.12 & 0.25   \\
gemma-2-9b\_\_zero\_shot\_cn\_xlt\_en  & 0.33 & 0.01 & 0.00 & 0.04 & 0.12 & 0.02 & 0.04 & 0.08   \\ \hline
   
gemma-2-27b\_\_zero\_shot\_en\_cot\_cn & 0.37 & 0.18 & 0.08 & 0.31 & 0.29 & 0.24 & 0.15 & 0.23   \\
gemma-2-27b\_\_zero\_shot\_en\_cot\_en & 0.21 & 0.07 & 0.05 & 0.17 & 0.14 & 0.13 & 0.12 & 0.13   \\
gemma-2-27b\_\_zero\_shot\_en\_xlt\_cn & 0.22 & 0.13 & 0.05 & 0.13 & 0.14 & 0.09 & 0.03 & 0.11   \\
gemma-2-27b\_\_zero\_shot\_cn\_cot\_cn & 0.15 & 0.08 & 0.05 & 0.11 & 0.13 & 0.00 & 0.07 & 0.08   \\
gemma-2-27b\_\_zero\_shot\_cn\_cot\_en & 0.15 & 0.04 & 0.04 & 0.05 & 0.06 & 0.04 & 0.05 & 0.06   \\
gemma-2-27b\_\_zero\_shot\_cn\_xlt\_en & 0.03 & 0.01 & 0.02 & 0.01 & 0.03 & 0.01 & 0.01 & 0.02   \\ \hline
\end{tabular}

}
 \caption{Performance of Gemma Series. And NT, CT, CI, SO, SR, RC, PR are the abbreviations for the variant names of Negation Transformation, Critical Testing, Causal Inference, Sentence Ordering, Scenario Refinement, Reverse Conversion and Problem Restatement.}
 \label{gemma}
\end{table*}

\begin{table*}[]
\centering
 \scalebox{0.9}{
\begin{tabular}{l|llllllll}
\hline
\multicolumn{1}{l|}{\textbf{Model\_\_Prompt\_Language}}   & CI   & CT   & NT   & PR   & RC   & SR   &SO  & \textbf{ARA} \\ \hline
Baichuan2-7B-Base\_\_\_direct\_cn & 0.59 & 0.34 & 0.09 & 0.65 & 0.60 & 0.41 & 0.55 & 0.46 \\
Baichuan2-13B-Base\_\_\_direct\_cn& 0.62 & 0.37 & 0.08 & 0.66 & 0.60 & 0.43 & 0.54 & 0.47 \\
Baichuan2-7B-Base\_\_\_direct\_en & 0.51 & 0.38 & 0.05 & 0.76 & 0.31 & 0.34 & 0.38 & 0.39 \\
Baichuan2-13B-Base\_\_\_direct\_en& 0.47 & 0.40 & 0.05 & 0.80 & 0.30 & 0.34 & 0.35 & 0.39 \\ \hline
Baichuan2-7B-Base\_\_few\_shot\_en\_cot\_cn   & 0.82 & 0.49 & 0.32 & 0.72 & 0.62 & 0.44 & 0.55 & 0.56 \\
Baichuan2-7B-Base\_\_few\_shot\_en\_cot\_en   & 0.67 & 0.22 & 0.70 & 0.56 & 0.24 & 0.41 & 0.68 & 0.50 \\
Baichuan2-7B-Base\_\_few\_shot\_en\_xlt\_cn   & 0.15 & 0.01 & 0.23 & 0.25 & 0.39 & 0.23 & 0.30 & 0.22 \\
Baichuan2-7B-Base\_\_few\_shot\_cn\_cot\_cn   & 0.89 & 0.50 & 0.54 & 0.75 & 0.70 & 0.53 & 0.51 & 0.63 \\
Baichuan2-7B-Base\_\_few\_shot\_cn\_cot\_en   & 0.83 & 0.28 & 0.75 & 0.50 & 0.50 & 0.33 & 0.70 & 0.56 \\
Baichuan2-7B-Base\_\_few\_shot\_cn\_xlt\_en   & 0.48 & 0.20 & 0.22 & 0.56 & 0.49 & 0.33 & 0.41 & 0.38 \\ \hline

Baichuan2-13B-Base\_\_few\_shot\_en\_cot\_cn  & 0.94 & 0.56 & 0.71 & 0.84 & 0.78 & 0.60 & 0.58 & 0.72 \\
Baichuan2-13B-Base\_\_few\_shot\_en\_cot\_en  & 0.78 & 0.31 & 0.36 & 0.56 & 0.48 & 0.47 & 0.81 & 0.54 \\
Baichuan2-13B-Base\_\_few\_shot\_en\_xlt\_cn  & 0.22 & 0.13 & 0.39 & 0.53 & 0.47 & 0.30 & 0.41 & 0.35 \\
Baichuan2-13B-Base\_\_few\_shot\_cn\_cot\_cn  & 0.89 & 0.53 & 0.76 & 0.86 & 0.72 & 0.53 & 0.61 & 0.70 \\
Baichuan2-13B-Base\_\_few\_shot\_cn\_cot\_en  & 0.86 & 0.42 & 0.52 & 0.57 & 0.47 & 0.35 & 0.87 & 0.58 \\
Baichuan2-13B-Base\_\_few\_shot\_cn\_xlt\_en  & 0.83 & 0.26 & 0.25 & 0.49 & 0.44 & 0.34 & 0.74 & 0.48 \\ \hline

Baichuan2-7B-Base\_\_zero\_shot\_en\_cot\_cn  & 0.31 & 0.16 & 0.01 & 0.24 & 0.21 & 0.13 & 0.03 & 0.16 \\
Baichuan2-7B-Base\_\_zero\_shot\_en\_cot\_en  & 0.29 & 0.20 & 0.01 & 0.35 & 0.08 & 0.22 & 0.01 & 0.16 \\
Baichuan2-7B-Base\_\_zero\_shot\_en\_xlt\_cn  & 0.19 & 0.21 & 0.02 & 0.17 & 0.10 & 0.06 & 0.01 & 0.11 \\
Baichuan2-7B-Base\_\_zero\_shot\_cn\_cot\_cn  & 0.42 & 0.23 & 0.07 & 0.38 & 0.22 & 0.00 & 0.02 & 0.19 \\
Baichuan2-7B-Base\_\_zero\_shot\_cn\_cot\_en  & 0.22 & 0.11 & 0.11 & 0.29 & 0.13 & 0.16 & 0.03 & 0.15 \\
Baichuan2-7B-Base\_\_zero\_shot\_cn\_xlt\_en  & 0.02 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ \hline

Baichuan2-13B-Base\_\_zero\_shot\_en\_cot\_cn & 0.41 & 0.21 & 0.02 & 0.25 & 0.54 & 0.11 & 0.21 & 0.25 \\
Baichuan2-13B-Base\_\_zero\_shot\_en\_cot\_en & 0.31 & 0.38 & 0.05 & 0.45 & 0.10 & 0.35 & 0.13 & 0.25 \\
Baichuan2-13B-Base\_\_zero\_shot\_en\_xlt\_cn & 0.23 & 0.19 & 0.01 & 0.28 & 0.20 & 0.05 & 0.10 & 0.15 \\
Baichuan2-13B-Base\_\_zero\_shot\_cn\_cot\_cn & 0.41 & 0.17 & 0.07 & 0.32 & 0.30 & 0.16 & 0.01 & 0.21 \\
Baichuan2-13B-Base\_\_zero\_shot\_cn\_cot\_en & 0.13 & 0.10 & 0.06 & 0.10 & 0.14 & 0.07 & 0.04 & 0.09 \\
Baichuan2-13B-Base\_\_zero\_shot\_cn\_xlt\_en & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ \hline
\end{tabular}

}
 \caption{Performance of Baichuan Series. And NT, CT, CI, SO, SR, RC, PR are the abbreviations for the variant names of Negation Transformation, Critical Testing, Causal Inference, Sentence Ordering, Scenario Refinement, Reverse Conversion and Problem Restatement.}
 \label{baichuan}
\end{table*}

\begin{table*}[]
\centering
\begin{tabular}{l|llllllll}
\hline
\textbf{Model\_\_Prompt\_Language}& CI   & CT   & NT   & PR   & RC   & SR   &SO  & \textbf{ARA} \\ \hline
internlm2\_5-1\_8b\_\_direct\_cn& 0.62 & 0.33 & 0.09 & 0.63 & 0.52 & 0.37 & 0.38 & 0.42   \\
internlm2\_5-7b\_\_direct\_cn & 0.71 & 0.34 & 0.08 & 0.66 & 0.60 & 0.43 & 0.67 & 0.50   \\
internlm2\_5-20b\_\_direct\_cn& 0.69 & 0.36 & 0.08 & 0.70 & 0.66 & 0.42 & 0.45 & 0.48   \\
internlm2\_5-1\_8b\_\_direct\_en& 0.55 & 0.33 & 0.08 & 0.63 & 0.41 & 0.35 & 0.35 & 0.38   \\
internlm2\_5-7b\_\_direct\_en & 0.59 & 0.39 & 0.06 & 0.76 & 0.36 & 0.32 & 0.37 & 0.41   \\
internlm2\_5-20b\_\_direct\_en& 0.62 & 0.41 & 0.05 & 0.78 & 0.35 & 0.31 & 0.37 & 0.41   \\ \hline
   
internlm2\_5-1\_8b\_\_few\_shot\_en\_cot\_cn  & 0.89 & 0.47 & 0.39 & 0.83 & 0.73 & 0.45 & 0.40 & 0.60   \\
internlm2\_5-1\_8b\_\_few\_shot\_en\_cot\_en  & 0.84 & 0.32 & 0.37 & 0.56 & 0.39 & 0.45 & 0.42 & 0.48   \\
internlm2\_5-1\_8b\_\_few\_shot\_en\_xlt\_cn  & 0.44 & 0.27 & 0.47 & 0.48 & 0.53 & 0.35 & 0.40 & 0.42   \\
internlm2\_5-1\_8b\_\_few\_shot\_cn\_cot\_cn  & 0.86 & 0.43 & 0.55 & 0.80 & 0.67 & 0.44 & 0.50 & 0.61   \\
internlm2\_5-1\_8b\_\_few\_shot\_cn\_cot\_en  & 0.80 & 0.44 & 0.20 & 0.49 & 0.41 & 0.45 & 0.67 & 0.49   \\
internlm2\_5-1\_8b\_\_few\_shot\_cn\_xlt\_en  & 0.62 & 0.20 & 0.17 & 0.48 & 0.47 & 0.35 & 0.42 & 0.39   \\ \hline
   
internlm2\_5-7b\_\_few\_shot\_en\_cot\_cn& 0.78 & 0.65 & 0.88 & 0.84 & 0.87 & 0.63 & 0.77 & 0.77   \\
internlm2\_5-7b\_\_few\_shot\_en\_cot\_en& 0.93 & 0.52 & 0.85 & 0.84 & 0.53 & 0.53 & 0.88 & 0.72   \\
internlm2\_5-7b\_\_few\_shot\_en\_xlt\_cn& 0.62 & 0.41 & 0.61 & 0.57 & 0.59 & 0.51 & 0.42 & 0.53   \\
internlm2\_5-7b\_\_few\_shot\_cn\_cot\_cn& 0.95 & 0.52 & 0.90 & 0.86 & 0.85 & 0.61 & 0.84 & 0.79   \\
internlm2\_5-7b\_\_few\_shot\_cn\_cot\_en& 0.91 & 0.60 & 0.75 & 0.77 & 0.47 & 0.54 & 0.82 & 0.69   \\
internlm2\_5-7b\_\_few\_shot\_cn\_xlt\_en& 0.78 & 0.31 & 0.48 & 0.77 & 0.40 & 0.38 & 0.62 & 0.54   \\ \hline
   
internlm2\_5-20b\_\_few\_shot\_en\_cot\_cn& 0.62 & 0.58 & 0.92 & 0.28 & 0.00 & 0.48 & 0.79 & 0.52   \\
internlm2\_5-20b\_\_few\_shot\_en\_cot\_en& 0.91 & 0.63 & 0.85 & 0.88 & 0.53 & 0.55 & 0.82 & 0.74   \\
internlm2\_5-20b\_\_few\_shot\_en\_xlt\_cn& 0.66 & 0.38 & 0.63 & 0.42 & 0.54 & 0.59 & 0.18 & 0.48   \\
internlm2\_5-20b\_\_few\_shot\_cn\_cot\_cn& 0.00 & 0.27 & 0.91 & 0.01 & 0.00 & 0.00 & 0.57 & 0.25   \\
internlm2\_5-20b\_\_few\_shot\_cn\_cot\_en& 0.91 & 0.65 & 0.79 & 0.86 & 0.51 & 0.56 & 0.84 & 0.73   \\
internlm2\_5-20b\_\_few\_shot\_cn\_xlt\_en& 0.74 & 0.46 & 0.90 & 0.83 & 0.53 & 0.44 & 0.69 & 0.65   \\ \hline
   
internlm2\_5-1\_8b\_\_zero\_shot\_en\_cot\_cn & 0.51 & 0.22 & 0.03 & 0.42 & 0.31 & 0.26 & 0.17 & 0.27   \\
internlm2\_5-1\_8b\_\_zero\_shot\_en\_cot\_en & 0.15 & 0.06 & 0.06 & 0.16 & 0.14 & 0.13 & 0.28 & 0.14   \\
internlm2\_5-1\_8b\_\_zero\_shot\_en\_xlt\_cn & 0.06 & 0.02 & 0.00 & 0.02 & 0.05 & 0.03 & 0.00 & 0.03   \\
internlm2\_5-1\_8b\_\_zero\_shot\_cn\_cot\_cn & 0.67 & 0.42 & 0.17 & 0.51 & 0.40 & 0.50 & 0.34 & 0.43   \\
internlm2\_5-1\_8b\_\_zero\_shot\_cn\_cot\_en & 0.45 & 0.32 & 0.20 & 0.32 & 0.32 & 0.28 & 0.47 & 0.34   \\
internlm2\_5-1\_8b\_\_zero\_shot\_cn\_xlt\_en & 0.07 & 0.04 & 0.08 & 0.07 & 0.03 & 0.06 & 0.01 & 0.05   \\ \hline
   
internlm2\_5-7b\_\_zero\_shot\_en\_cot\_cn& 0.78 & 0.65 & 0.88 & 0.84 & 0.87 & 0.63 & 0.77 & 0.77   \\
internlm2\_5-7b\_\_zero\_shot\_en\_cot\_en& 0.93 & 0.52 & 0.85 & 0.84 & 0.53 & 0.53 & 0.88 & 0.72   \\
internlm2\_5-7b\_\_zero\_shot\_en\_xlt\_cn& 0.15 & 0.08 & 0.12 & 0.12 & 0.18 & 0.11 & 0.14 & 0.13   \\
internlm2\_5-7b\_\_zero\_shot\_cn\_cot\_cn& 0.95 & 0.52 & 0.90 & 0.86 & 0.85 & 0.61 & 0.84 & 0.79   \\
internlm2\_5-7b\_\_zero\_shot\_cn\_cot\_en& 0.91 & 0.60 & 0.75 & 0.77 & 0.47 & 0.54 & 0.82 & 0.69   \\
internlm2\_5-7b\_\_zero\_shot\_cn\_xlt\_en& 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00   \\ \hline
   
internlm2\_5-20b\_\_zero\_shot\_en\_cot\_cn   & 0.39 & 0.36 & 0.15 & 0.48 & 0.55 & 0.31 & 0.22 & 0.35   \\
internlm2\_5-20b\_\_zero\_shot\_en\_cot\_en   & 0.48 & 0.28 & 0.11 & 0.48 & 0.41 & 0.25 & 0.75 & 0.39   \\
internlm2\_5-20b\_\_zero\_shot\_en\_xlt\_cn   & 0.27 & 0.33 & 0.33 & 0.54 & 0.42 & 0.26 & 0.59 & 0.39   \\
internlm2\_5-20b\_\_zero\_shot\_cn\_cot\_cn   & 0.55 & 0.53 & 0.27 & 0.71 & 0.56 & 0.00 & 0.71 & 0.48   \\
internlm2\_5-20b\_\_zero\_shot\_cn\_cot\_en   & 0.54 & 0.47 & 0.03 & 0.69 & 0.43 & 0.42 & 0.30 & 0.41   \\
internlm2\_5-20b\_\_zero\_shot\_cn\_xlt\_en   & 0.80 & 0.18 & 0.02 & 0.64 & 0.43 & 0.32 & 0.89 & 0.47   \\ \hline
\end{tabular}
 \caption{Performance of InternLM Series. And NT, CT, CI, SO, SR, RC, PR are the abbreviations for the variant names of Negation Transformation, Critical Testing, Causal Inference, Sentence Ordering, Scenario Refinement, Reverse Conversion and Problem Restatement.}
 \label{internlm}
\end{table*}


\begin{table*}[]
\centering
 \scalebox{0.9}{
\begin{tabular}{l|llllllll}
\hline
\textbf{Instruct-Model} & CI   & CT   & NT   & PR   & RC   & SR   &SO  & \textbf{ARA} \\ \hline
Qwen2.5-0.5B            & 0.58 & 0.36 & 0.07 & 0.65 & 0.41 & 0.36 & 0.58 & 0.43         \\
Qwen2.5-1.5B            & 0.64 & 0.36 & 0.07 & 0.70 & 0.41 & 0.37 & 0.49 & 0.43         \\
Qwen2.5-3B              & 0.66 & 0.40 & 0.05 & 0.75 & 0.41 & 0.40 & 0.53 & 0.46         \\
Qwen2.5-7B              & 0.74 & 0.39 & 0.06 & 0.76 & 0.44 & 0.39 & 0.64 & 0.49         \\
Qwen2.5-14B             & 0.78 & 0.40 & 0.06 & 0.77 & 0.44 & 0.40 & 0.74 & 0.51         \\
Qwen2.5-32B             & 0.75 & 0.40 & 0.06 & 0.78 & 0.45 & 0.40 & 0.79 & 0.52         \\
Qwen2.5-72B             & 0.78 & 0.41 & 0.06 & 0.79 & 0.48 & 0.41 & 0.80 & 0.53         \\ \hline
Meta-Llama-3-8B            & 0.65 & 0.38 & 0.07 & 0.71 & 0.34 & 0.40 & 0.52 & 0.44         \\
Meta-Llama-3-70B             & 0.68 & 0.40 & 0.06 & 0.73 & 0.37 & 0.41 & 0.57 & 0.46         \\ \hline
Mistral-7B-v0.2              & 0.67 & 0.41 & 0.07 & 0.76 & 0.35 & 0.38 & 0.58 & 0.46         \\
Mixtral-8x7B-v0.1            & 0.65 & 0.41 & 0.07 & 0.76 & 0.41 & 0.40 & 0.52 & 0.46         \\
Mixtral-8x22B-v0.1           & 0.70 & 0.43 & 0.05 & 0.78 & 0.42 & 0.39 & 0.57 & 0.48         \\ \hline
\end{tabular}
}
 \caption{Performance of intruct models under Direct Prompt. And NT, CT, CI, SO, SR, RC, PR are the abbreviations for the variant names of Negation Transformation, Critical Testing, Causal Inference, Sentence Ordering, Scenario Refinement, Reverse Conversion and Problem Restatement.}
 \label{instruct-tab}
\end{table*}


\begin{figure}[h]
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=0.85\linewidth,scale=0.50]{images/instruct.pdf}
\caption{Comparasion between base and their instruct models.}
\label{instruct}
\vspace{-15pt}
\end{figure}

\clearpage

\begin{figure}[H]
\centering
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=\linewidth,scale=1.00]{images/robust_data.pdf}
\caption{An example of HellaSwag-Pro.}
\label{robust_data}
\vspace{-15pt}
\end{figure}

\begin{figure}[H]
\centering
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=\linewidth,scale=1.00]{images/cn_data.pdf}
\caption{An example of Chinese HellaSwag.}
\label{cn_data}
\vspace{-15pt}
\end{figure}

\begin{figure}[H]
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=\linewidth,scale=1.00]{images/pro_prompt.pdf}
\caption{Prompt for HellaSwag-Pro construction.}
\label{pro_prompt}
\vspace{-15pt}
\end{figure}

\begin{figure}[H]
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\includegraphics[width=\linewidth,scale=1.00]{images/cn_prompt.pdf}
\caption{Prompt for Chinese HellaSwag construction.}
\label{cn_prompt}
\vspace{-15pt}
\end{figure}
