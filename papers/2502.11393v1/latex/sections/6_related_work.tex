\section{Related Work}

\paragraph{Commonsense Reasoning Evaluation} 
There are numerous benchmarks and datasets for commonsense reasoning, most of which are in English. 
%Some work focused on evaluating general commonsense knowledge, such as HellaSwag \cite{zellers2019hellaswag}, CommonsenseQA \cite{talmor2019commonsenseqa}, OpenBookQA \cite{OpenBookQA2018}, and WSC \cite{levesque2012winograd}. 
Some studies focus on evaluating general commonsense knowledge \cite{zellers2019hellaswag,talmor2019commonsenseqa,OpenBookQA2018}. 
%Others target specific aspects of commonsense reasoning, including temporal commonsense with MCTACO \cite{zhou2019going}, physical commonsense with PIQA \cite{bisk2020piqa}, social commonsense with SocialIQA \cite{sap2019socialiqa}, numerical commonsense with NumerSense \cite{lin2020birds}, and scientific commonsense with ARC \cite{clark2018think} and QASC \cite{khot2020qasc}. Notably, most of these datasets are in English. 
Others target specific aspects of commonsense reasoning\cite{zhou2019going,bisk2020piqa,sap2019socialiqa,lin2020birds,clark2018think,khot2020qasc}.
There are some Chinese datasets for commonsense reasoning \cite{sun2024benchmarking,shi2024corecode}. 
For instance, CHARM \cite{sun2024benchmarking} distinguishes between global commonsense and Chinese-specific commonsense but includes only a limited number of everyday commonsense cases. 
However, evaluations aimed at assessing the robustness of commonsense reasoning are still understudied. 

\paragraph{Datasets on Different Reasoning Forms}
There are several datasets relevant to our variant design. For reverse reasoning, ART \cite{DBLP:conf/iclr/BhagavatulaBMSH20}, $\delta$-NLI \cite{DBLP:conf/emnlp/RudingerSHBFBSC20}, and CLUTRR \cite{DBLP:conf/emnlp/SinhaSDPH19} explore different reasoning directions. FCR \cite{DBLP:journals/corr/abs-2204-07408} and NatQuest \cite{ceraolo2024analyzinghumanquestioningbehavior} evaluate causal reasoning, while TimeTravel \cite{DBLP:conf/emnlp/QinBHBCC19} focuses on counterfactual scenario refinement. Additionally, PoE \cite{balepur2024s} assesses reasoning involving negation. 
However, not all these datasets focus on commonsense reasoning, nor are they structured by original questions and their variants. Furthermore, they typically target limited reasoning types. Lastly, our dataset is large-scale and covers diverse commonsense knowledge. 

\paragraph{Robustness and Consistency in LLMs} 
Early work focuses on adversarial attacks, with developing evaluation methods for reading comprehension systems \cite{jia2017adversarial}, followed by universal adversarial triggers \cite{wallace2019universal}. The field then expands to examine spurious correlations, with revealing how models often exploit superficial patterns rather than engaging in genuine reasoning \cite{branco2021shortcutted,geirhos2020shortcut}. And \citealp{ross2022does} investigates whether self-explanation can mitigate these spurious correlations. Coherence and consistency evaluation advances through classifier assessment methods \cite{storks2021beyond} and analysis of accuracy-consistency trade-offs \cite{johnson2023much}. While these studies primarily address model robustness against adversarial attacks or spurious correlations, our work takes a novel approach by examining robustness in reasoning forms.
%, specifically focusing on how models maintain consistent reasoning when presented with different reasoning forms of the same commonsense knowledge.
%\paragraph{Dataset Construction by LLM} 
%Research indicates that when LLMs are utilized for dataset generation, the resulting datasets are more accurate and fluent \cite{lu2022fantastically, min-etal-2022-rethinking} than those created by crowd-sourced annotators. Furthermore, generating datasets with LLMs is significantly more cost-effective than using crowd-sourced annotations \cite{liu2022wanli, wiegreffe2022reframing, west2022symbolic}. Hence, we generate our benchmark by LLM in-context learning.

% \paragraph{In-Context Learning} 
% As LLMs become more widely used, in-context learning (\citealp{brown2020language}; \citealp{ouyang2022training}; \citealp{min-etal-2022-rethinking}) has emerged as the primary approach for executing various tasks. This method involves supplying LLMs with textual instructions and examples and removes the necessity for parameter modifications. Research indicates that when LLMs are utilized for dataset generation, the resulting datasets are more accurate and fluent (\citealp{lu2022fantastically}; \citealp{min-etal-2022-rethinking}) than those created by crowd-sourced annotators. Furthermore, generating datasets with LLMs is significantly more cost-effective than using crowd-sourced annotations (\citealp{liu2022wanli}; \citealp{wiegreffe2022reframing}; \citealp{west2022symbolic}). Hence, we have decided to construct our benchmark by over-generating data using in-context learning and employing human annotators for filtering to ensure high efficiency and high quality.
% Moxin: 这部分应该改成用LLM 生成dataset的工作？
% 
