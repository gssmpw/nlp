%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2025}
% \acmDOI{XXXXXXX.XXXXXXX}

% %% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
% %%
% %%  Uncomment \acmBooktitle if the title of the proceedings is different
% %%  from ``Proceedings of ...''!
% %%
% %%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
% %%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/18/06}

\copyrightyear{2025} 
\acmYear{2025} 
\setcopyright{cc}
\setcctype{by}
\acmConference[WWW Companion '25]{Companion Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
\acmBooktitle{Companion Proceedings of the ACM Web Conference 2025 (WWW Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia}
\acmDOI{10.1145/3701716.3715504}
\acmISBN{979-8-4007-1331-6/25/04}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Pranav Bhandari}
\authornote{Network Analysis and Social Influence Modeling (NASIM) Lab at UWA.}

\orcid{https://orcid.org/0000-0002-5638-5339}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{School of Physics, Mathematics and Computing}
  \city{University of Western Australia}
  % \state{Western Australia}
  \country{Australia}
}\email{pranav.bhandari@research.uwa.edu.au}

\author{Usman Naseem}
\affiliation{%
   \institution{School of Computing}
  \city{Macquaire University, Australia}
  % \state{NSW}
  \country{Australia}
}\email{usman.naseem@mq.edu.au}



\author{Amitava Datta}
\affiliation{%
\institution{School of Physics, Mathematics and Computing}
  \city{University of Western Australia}
  % \state{Western Australia}
  \country{Australia}
}\email{amitava.datta@uwa.edu.au}

\author{Nicolas Fay}
\affiliation{%
   \institution{School of Psychological Sciences}
  \city{University of Western Australia}
  % \state{Western Australia}
  \country{Australia}
}\email{nicolas.fay@uwa.edu.au}

\author{Mehwish Nasim}
\authornotemark[1]
\authornote{Flinders University, South Australia.}
\authornote{University of Adelaide, South Australia.}
\affiliation{%
   \institution{School of Physics, Mathematics and Computing}
  \city{University of Western Australia}
  % \state{Western Australia}
  \country{Australia}
  }\email{mehwish.nasim@uwa.edu.au}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Bhandari et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Psychological assessment tools have long helped humans understand behavioural patterns. While Large Language Models (LLMs) can generate content comparable to that of humans, we explore whether they exhibit personality traits. To this end, this work applies psychological tools to LLMs in diverse scenarios to generate personality profiles. Using established trait-based questionnaires such as the Big Five Inventory and by addressing the possibility of training data contamination, we examine the dimensional variability and dominance of LLMs across five core personality dimensions: \emph{Openness}, \emph{Conscientiousness}, \emph{Extraversion}, \emph{Agreeableness}, and  \emph{Neuroticism}. Our findings reveal that LLMs exhibit unique dominant traits, varying characteristics, and distinct personality profiles even within the same family of models.
 \end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010179.10003352</concept_id>
       <concept_desc>Computing methodologies~Information extraction</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010179.10010186</concept_id>
       <concept_desc>Computing methodologies~Language resources</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Computing methodologies~Information extraction}
\ccsdesc[300]{Computing methodologies~Language resources}
% \ccsdesc[500]{Computing methodologies~Natural language generation}
% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{LLMs, Psychometrics, Personality traits, Role playing agents  }
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
% LLMs and psychology??
Understanding the behaviour of LLMs is essential as they are increasingly used in diverse fields such as education, law, business and medicine\cite{miotto-etal-2022-gpt} where they significantly influence human interactions and decision-making processes. These models can generate coherent and insightful content, allowing personal recommendation and solving complex problems\cite{probingpnas}. However, concern for ethical considerations, inherent bias and the potential for misuse still exist\cite{miotto-etal-2022-gpt} which must be addressed by exploring the underlying patterns through systematic approaches such as psychological assessments\cite{hagendorff2024machinepsychology}. Explaining specific behaviours exhibited by LLMs comes with significant challenges, particularly when these LLMs generate specific outputs with no insights into the underlying reasoning\cite{hagendorff2024machinepsychology}. This challenge forms a strong motivation to systematically apply psychological tools to analyse and interpret the personality traits exhibited by LLMs. 

% how psychology comes into play and what parts of psychology
% Various assessments in the form of questionnaires have been used to understand the psychological standing of humans and provide a possible explanation for certain behaviours. Hence, we explore the possibility that these diagnostic tools can be used to understand the psychological nuances of LLMs given that they generate coherent and contextual information in the same pattern as humans.

 % what psychometrics? their validity? and why is it justifiable to use in the research? 
Personality plays a critical role in shaping conversations in many ways, such as exchanging empathy, creating openness, and mitigating harm that is measured through well-established psychometric tools in humans. Tests such as the IPIP-NEO-60\cite{ipipneo60}, HEXACO\cite{hexaco}, and TIPI\cite{tipi} are validated through long-standing rigorous processes such as internal consistency, test-retest methods, and convergence validity with similar tests. 


Since LLMs are trained on vast amounts of human-produced data, we argue that even if they lack behaviour in a \emph{true psychological sense}, their outputs may still reflect psychological traits. Furthermore, psychological assessments have proven to be relevant, reliable and valid for large-scale language models\cite{serapiogarcía2023personalitytraitslargelanguage} in the literature. We use five different tools such as the Big Five Inventory (BFI) \cite{thebfi} that measure various dimensions of personality in multiple LLMs to understand their personality traits. In contrast to existing literature\cite{miotto-etal-2022-gpt, jiang2023evaluating}, where tests are directly administered to LLMs introducing potential bias, as these models may have encountered the questions during training, we restructure the tests to create their closest representations and validate them prior to administration. Additionally, the questionnaires are randomised to eliminate dependency bias between them.

We present results from different dimensions namely \emph{Openness}, \emph{Conscientiousness}, \emph{Extraversion}, \emph{Agreeableness}, and \emph{Neuroticism} for different State-Of-The-Art large language models and compare them. Our results indicate that LLMs likely have distinct personality traits. Each of these dimensions is divided into several traits\cite{jiang2023evaluating} that contribute to positive or negative behaviour. 
%Misalignment of one of these dimensions can easily cause a potential threat to users. 
\begin{figure}[h]
  \centering
  \includegraphics[trim = 20 0 20 0, clip, width=\linewidth]{methodology.pdf}
  \caption{Methodology involves combining system and user prompts with questionnaires to administer to LLMs, extracting, processing and plotting the responses respectively. }
  \Description{Methodology of the paper}
  \label{fig:methodology}
\end{figure}


\subsection{Contributions:} Our contributions are as follows: %for LLMs about psychology:
1).~Firstly, we systematically assess LLMs personality traits using restructured personality questionnaires to resolve training data contamination and reduce dependency bias; 2).~We then present the distinct personality profiles of LLMs to provide insights into their personality traits; 3).~Finally, we conduct the dimensional analysis for dominance and variability across each trait of the personality questionnaires for all the LLMs used. 

We find that LLMs often score higher in traits like Agreeableness, Openness, and Conscientiousness, reflecting their cooperative, creative, and organised behaviour. Furthermore, we show that LLMs exhibit varying consistency across dimensions of dominance and variability when tested across different models. Overall, we provide a systematic approach to understanding LLM personality traits using established questionnaires. 
%Some words about the findings. 

\subsection{Literature Review: } Notable connections of LLM with psychology began with the advent of models such as GPT-3 \cite{miotto-etal-2022-gpt}. Various perspectives of psychology are infused to understand behaviours from multiple dimensions such as emotion, cognition\cite{probingpnas}, Theory-of-Mind, and morality \cite{li2024quantifying}. Identifying personality traits has been one of the main emphasises in the field of study \cite{sorokovikova2024llmssimulatebigpersonality, probingpnas, jiang2023evaluating, serapiogarcía2023personalitytraitslargelanguage, personallm}. Psychometric tools such as IPIP-NEO-120\cite{serapiogarcía2023personalitytraitslargelanguage}, Big Five Inventory (BFI) \cite{thebfi,li2024quantifying}, are predominantly used in the literature to assess the five dimensions of personality mentioned above.

Variations in prompts and the use of role-playing agents have been impersonated in several studies to study different personality behaviours in LLMs\cite{miotto-etal-2022-gpt}. The major aim behind these studies was to assess whether the behaviour is consistent or changing across different simulated situations. Context-sensitive variations were observed for several scenarios\cite{miotto-etal-2022-gpt, jiang2023evaluating}. Jiang et al. \cite{personallm} used personality prompting methods to induce and tailor the personality of LLMs according to the dynamic needs of the tests. This also draws attention to the need to enforce ethical factors for LLMs to create a safe and moderated environment for users. 

A limited number of LLMs and personality traits sharing the same domain in the literature restrict the scope of analysis. The small sample size at this exploratory stage could impact the validity of results across models. A significant concern is the use of these tests in their original form, raising the risk of training data contamination and potential bias due to the sequential nature of questions within the same Big Five Personality dimension \cite{hagendorff2024machinepsychology}.

\begin{table}[h!]
\centering
\caption{Personality Trait Inventories used  denoting the number of questions, Strengths and Weaknesses.}
\small
\begin{tabular}{@{}llp{4.5cm}@{}}
\toprule
\textbf{Test Name} &  \textbf{Questions }& \textbf{Strengths ($\uparrow$) and Weaknesses($\downarrow$)} \\
\midrule
BFI\cite{thebfi} & 44(8-10 each) & \textbf{$\uparrow$:} Reliable, widely validated \newline \textbf{$\downarrow$:} Depth, time-consuming \\
\hline
HEXACO\cite{hexaco} & 100(10 each) & \textbf{$\uparrow$:} cross-cultural validity. \newline \textbf{$\downarrow$:} Lengthy, complex  \\
\hline
TIPI\cite{tipi} & 10(2 each) & \textbf{$\uparrow$:}  quick, easy to administer. \newline \textbf{$\downarrow$:} medium reliability \\
\hline
MINI-IPIP\cite{miniipip} & 20(4 each) & \textbf{$\uparrow$}  balances brevity and validity \newline \textbf{$\downarrow$:} Limited depth \\
\hline
NEO-PI-R\cite{ipipneo60} & 60(12 each) & \textbf{$\uparrow$} More depth, reliable. \newline \textbf{$\downarrow$:} No facet analysis. \\
\bottomrule
\end{tabular}
\label{tab:personality_tests}
\end{table}



\section{Methodology}

The basic methodology consists of administering various personality tests from psychology to LLMs. Five personality traits questionnaires are used, all containing five dimensions of personality i.e., \emph{Openness}, \emph{Conscientiousness}, \emph{Extraversion}, \emph{Agreeableness}, and  \emph{Neuroticism}. Table \ref{tab:personality_tests} presents the five tests used in the experiments. Each questionnaire has varying questions with strengths ($\uparrow$) and weaknesses ($\downarrow$). Despite variations in the depth of judgment, question types, and scoring methods, all these tests consistently measure the five common dimensions of personality traits. Each test is administered to LLMs using system and user prompts (Figure \ref{fig:methodology}) at least three times per LLM to ensure score consistency and avoid mix-ups across combinations.


Likert scale scores obtained from querying LLMs are processed based on each questionnaire's scoring rules and visualised in various formats. These methods aim to address gaps in the literature regarding the administration of such questionnaires to LLMs.




\begin{table*}[t]
\centering
\caption{ Average Likert scores denoting personality traits for all the LLMs across five personality questionnaire inventories are presented. Ex: Extraversion, Ag: Agreeableness, Co: Conscientiousness, Ne: Neuroticism, Op: Openness.}
\label{tab:personality_scores}
\small
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lccccccccccccccccccccccccc@{}}
\toprule
{\textbf{LLM}} & \multicolumn{5}{c}{\centering \textbf{BFI}} & \multicolumn{5}{c}{\centering \textbf{HEXACO}}  & \multicolumn{5}{c}{\centering \textbf{MINI-IPIP}} & \multicolumn{5}{c}{\centering \textbf{IPIP NEO}}& \multicolumn{5}{c}{\centering \textbf{TIPI}} \\
\cmidrule(lr){2-6} \cmidrule(lr){7-11} \cmidrule(lr){12-16} \cmidrule(lr){17-21} \cmidrule(lr){22-26}
& Ex & Ag & Co & Ne & Op & Ex & Ag & Co & Ne & Op & Ex & Ag & Co & Ne & Op & Ex & Ag & Co & Ne & Op & Ex & Ag & Co & Ne & Op\\
\midrule
 GPT-4 & 3.5 & 4.0 & 4.3 & 3.3 & 4.4 & 3.3 & 3.3 & 3.5 & 3.6 & 3.3 & 3.5 & 4.5 & 3.4 & 2.4 & 4.0 & 3.9 & 4.5 & 4.2 & 2.2 & 3.6 & 4.4 & 5.3 & 5.9 & 2.9 & 5.8\\
GPT-4o-m & 3.8 & 3.9 & 4.1 & 3.0 & 4.6 & 3.5 & 3.6 & 3.6 & 3.7 & 3.3 & 3.8 & 4.5 & 2.9 & 2.6 & 4.1 & 4.0 & 4.4 & 4.0 & 2.5 & 4.0 & 5.0 & 5.6 & 5.9 & 2.8 & 5.9\\
Llama 3 & 3.5 & 3.8 & 4.8 & 3.4 & 3.9 & 3.1 & 2.9 & 3.0 & 3.1 & 3.2 & 3.2 & 4.0 & 3.1 & 2.1 & 4.0 & 3.8 & 4.2 & 4.0 & 2.2 & 3.9 & 4.6 & 5.8 & 6.1 & 2.0 & 5.8\\
Llama 3.1 & 3.9 & 4.0 & 4.4 & 3.4 & 4.3 & 3.3 & 3.2 & 3.2 & 3.6 & 3.2 & 3.0 & 4.4 & 3.7 & 1.6 & 4.5 & 4.1 & 4.4 & 4.4 & 1.9 & 4.2 & 5.5 & 6.6 & 6.7 & 1.5 & 6.5\\
Llama 3.2 & 3.0 & 3.9 & 4.0 & 3.1 & 4.3 & 3.4 & 3.5 & 3.7 & 3.5 & 4.0 & 2.0 & 3.9 & 2.7 & 2.5 & 3.2 & 4.18 & 4.4 & 4.4 & 1.8 & 4.3 & 4.0 & 5.5 & 6.0 & 2.6 & 4.9\\

\bottomrule
\Description{This table consists the results of all the llm scores with different scores}
\label{tab:results_table}
\end{tabular}
}
% \end{adjustbox}
\end{table*}


\subsection{Removing Training Data Contamination }

To remove the training data contamination as stated in the literature \cite{hagendorff2024machinepsychology}, a 2-step procedure was implemented. Initially, each question in the questionnaire was reworded to contain a different structure while the purpose of the question was intact. We used high-end models such as GPT-4o\footnote{\url{https://chatgpt.com/}} to reconstruct the questions. These were validated using sentence-similarity models from Hugging Face\footnote{\url{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}}, which calculated cosine similarity scores. A threshold of 0.7, indicating moderate to high similarity, was applied. Questions scoring below 0.7 were reconstructed under human supervision as needed. For each LLM, temperature parameters were set to minimum values: $0$ for OpenAI models and $0.01$ for Llama models to ensure consistent and deterministic scores across multiple runs.




\subsection{Systematic Sequence of Prompts}
Once the prompts and questions were finalised, they were administered to LLMs in a randomised order to mitigate dependency bias from specific personality traits. Questions were presented in batches of $10$ to preserve contextual understanding, with answers remapped upon completing each iteration. Across all models, $100$ iterations were conducted using a minimum temperature setting and the responses were mapped accordingly.

\subsection{Calculate Coefficient of Variation}
While various tests are administered across different LLMs, it is important to analyse the consistency of the scores for the reliability and robustness of the tests. This analysis examines the variability of scores in the five different personality traits, across multiple runs ($n=100$). To analyse this variability, we calculate the Coefficient of Variation (CV) for the scores of a specific personality dimension (e.g., openness, neuroticism) across all personality questionnaires for a given model. The CV is computed using the Mean (\( \mathbf{\text{M}} \)) and Standard Deviation (\( \mathbf{\text{SD}} \)) as: \( \mathbf{\text{CV}} = \mathbf{\left( \frac{\text{SD}}{\text{M}} \right)} \times 100\% \).


\section{Results}
% make sure to include what do these scores mean. higher scores significance? lower scores? 

Three models from the Llama family i.e., Llama-3-8B-Instruct, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct and two from the OpenAI family i.e., GPT-4 and GPT-4o-mini are used in the experiments. The reason to choose different models from the same family is to effectively compare the results as models from the same family share similar structures with some changes in the methodology allowing us to understand the impact of minor changes concerning personality traits. Table \ref{tab:results_table} represents the average scores for each domain of all the personality trait questionnaires used. 

% \begin{table*}[h!]
% \centering
% \caption{Personality Trait Scores Across LLMs and Questionnaires. The table indicates the average scores of Likert scales for all the LLMs across five personality questionnaire inventory.}
% \label{tab:personality_scores}
% \small
% \resizebox{\textwidth}{!}{
% \begin{tabular}{@{}lccccccccccccccccccccccccc@{}}
% \toprule
% \textbf{LLM} & \multicolumn{5}{c}{\centering \textbf{BFI}} & \multicolumn{5}{c}{\centering \textbf{HEXACO}}  & \multicolumn{5}{c}{\centering \textbf{MINI-IPIP}} & \multicolumn{5}{c}{\centering \textbf{IPIP NEO}}& \multicolumn{5}{c}{\centering \textbf{TIPI}} \\
% \\
% \cmidrule(lr){2-6} \cmidrule(lr){7-11} \cmidrule(lr){12-16} \cmidrule(lr){17-21} \cmidrule(lr){22-26}
% & Ex & Ag & Co & Ne & Op & Ex & Ag & Co & Ne & Op & Ex & Ag & Co & Ne & Op & Ex & Ag & Co & Ne & Op & Ex & Ag & Co & Ne & Op\\
% \midrule
%  GPT-4 & 3.5 & 4.0 & 4.3 & 3.3 & 4.4 & 3.3 & 3.3 & 3.5 & 3.6 & 3.3 & 3.5 & 4.5 & 3.4 & 2.4 & 4.0 & 3.9 & 4.5 & 4.2 & 2.2 & 3.6 & 4.4 & 5.3 & 5.9 & 2.9 & 5.8\\
% GPT-4o-m & 3.8 & 3.9 & 4.1 & 3.0 & 4.6 & 3.5 & 3.6 & 3.6 & 3.7 & 3.3 & 3.8 & 4.5 & 2.9 & 2.6 & 4.1 & 4.0 & 4.4 & 4.0 & 2.5 & 4.0 & 5.0 & 5.6 & 5.9 & 2.8 & 5.9\\
% Llama 3 & 3.5 & 3.8 & 4.8 & 3.4 & 3.9 & 3.1 & 2.9 & 3.0 & 3.1 & 3.2 & 3.2 & 4.0 & 3.1 & 2.1 & 4.0 & 3.8 & 4.2 & 4.0 & 2.2 & 3.9 & 4.6 & 5.8 & 6.1 & 2.0 & 5.8\\
% Llama 3.1 & \textbf{3.9} & 4.0 & 4.4 & 3.4 & 4.3 & 3.3 & 3.2 & 3.2 & 3.6 & 3.2 & 3.0 & 4.4 & 3.7 & 1.6 & 4.5 & 4.1 & 4.4 & 4.4 & 1.9 & 4.2 & 5.5 & 6.6 & 6.7 & 1.5 & 6.5\\
% Llama 3.2 & 3.0 & 3.9 & 4.0 & 3.1 & 4.3 & 3.4 & 3.5 & 3.7 & 3.5 & 4.0 & 2.0 & 3.9 & 2.7 & 2.5 & 3.2 & 4.18 & 4.4 & 4.4 & 1.8 & 4.3 & 4.0 & 5.5 & 6.0 & 2.6 & 4.9\\

% \bottomrule
% \Description{This table consists the results of all the llm scores with different scores}
% \label{tab:results_table}
% \end{tabular}
% }
% % \end{adjustbox}
% \end{table*}


% All personality trait questionnaires, except the Ten Item Personality Inventory (TIPI), are scored on a Likert scale from 1 to 5, where 1 indicates the lowest level of agreement and 5 indicates the highest. In contrast, the TIPI is scored on a 1 to 7 Likert scale.

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{radar_revisit_again}
  \caption{ Radar plots to visualise all five personality questionnaires across their dimensions, comparing results from all LLMs. While scores are generally similar, notable gaps in specific personality traits are evident.}
  \label{fig:Radar_plots}
  \Description{Figures representing the results of psychology}
\end{figure*}

\subsection{Personality Profiles of LLMs}
Table \ref{tab:results_table} provides an insight into the personality scores of LLMs. Figure \ref{fig:Radar_plots} illustrates the personality profiles of LLMs across five tests. Agreeableness and Neuroticism scores from the BFI questionnaire are comparable across models, whereas other tests reveal gaps, suggesting a more uniform alignment with these traits when evaluated using the BFI. 

Agreeableness consistently shows higher average scores across all LLMs compared to other dimensions, while Neuroticism demonstrates lower average scores across all tests. Lower scores in Neuroticism indicate that LLM exhibit calm, emotionally stable, and resilient behaviour in their responses. These scores highlight the tendency of LLMs to maintain balanced and neutral behaviour, avoiding extreme or negative emotional expressions. Moderate levels of Extraversion are observed across all LLMs in all tests, except for Llama-3.2-3B-Instruct, which shows lower sociable or enthusiastic tendencies in the mini-IPIP and TIPI questionnaires. This indicates that its behaviour may vary in specific contexts as captured by these tests. These deviations highlight the influence of the model-specific design and the sensitivity of the questionnaire in assessing personality traits. Openness and Conscientiousness are generally higher for all LLMs across all personality questionnaires. This suggests that LLMs consistently exhibit traits of curiosity, creativity (Openness), reliability, organisation, and attention to detail (Conscientiousness). These higher scores likely reflect the models' design to provide thoughtful, adaptable, and structured responses.

\begin{table}[h!]
\centering
\small % Increase text size for better readability
\caption{Summary of CV and Dimensional Dominance. }
\begin{tabular}{l@{\hspace{2mm}}l@{\hspace{2mm}}c@{\hspace{2mm}}c}
\hline
\textbf{Model} & \textbf{CV (High(\%), Low(\%))} & \textbf{Dominant Dim.} & \textbf{Mean} \\
\hline
GPT-4          & Ne (20.49), Ex (6.14)  & Ag                     & 4.32 \\
GPT-4o-mini    & Ne (15.99), Ex (4.73)  & Ag                     & 4.40 \\
Llama 3        & Ne (20.79), Ex (8.05)  & Co                     & 4.20 \\
Llama 3.1      & Ne (33.69), Op (12.41) & Op                     & 4.54 \\
Llama 3.2      & Ex (21.93), Ag (7.25)  & Ag                     & 4.22 \\
\hline
\end{tabular}
\label{tab:compact_cv_dominance}
\Description{table combining CV and dimensional dominance for LLMs.}
\end{table}
 % While major personality differences were expected given different models with different training data and methodologies, our study explains LLMs share similar personality traits with minor distinctions.
\subsection{Dimensional Variability Across  Models}

As mentioned in the methodology Section 3.3 we calculate the CV for each model for each of the five dimensions across all the personality questionnaires except for the TIPI because they are measured in the Likert scale of 1-7 as opposed to all other personality inventories which are measured from 1-5. Table \ref{tab:compact_cv_dominance} presents the result for the highest and lowest CV for each of the models. 


Neuroticism demonstrates the highest variability across the majority of the models (GPT-4, GPT-4o-mini, Llama-3-8B-Instruct and Llama-3.1-8B-Instruct), suggesting the representation of emotional stability fluctuates significantly across personality questionnaires. When analysed individually, Llama-3.1-8B-Instruct represents an exceptionally high value of Neuroticism (33.69\%). This substantial variability highlights challenges in both the definition and evaluation of Neuroticism in different personality inventory questionnaires for different LLMs. While Neuroticism exhibited the highest CV across the models, the lowest variance was with Extraversion, Openness and Agreeableness for different models. OpenAI models have the lowest CV across Extraversion, suggesting a consistent measurement of sociability across the tests. Conclusively, lower variability in these dimensions indicates better alignment across personality questionnaires and consistent LLM responses. 

\subsection{Dimensional dominance by Model Type}
LLMs exhibit varying strengths across personality traits, reflecting their training strategies and alignment methods. Our study explores whether LLMs trained with distinct data and methods demonstrate different dominating dimensions. To evaluate dimensional dominance, the mean score for each dimension is calculated across all questionnaires for each LLM, identifying the dimension with the highest mean. These results are summarised in Table \ref{tab:compact_cv_dominance}. For the OpenAI modelS, the dominance is around the Agreeableness domain. The implications are these models emphasize traits that support the domain such as cooperation, friendliness, and trustworthiness.


% \begin{table}[h!]
% \centering
% \caption{Dimensional Dominance for LLMs}
% \begin{tabular}{lcc}
% \hline
% \textbf{Model} & \textbf{Dominant Dimension} & \textbf{Mean Score} \\
% \hline
% GPT-4          & Agreeableness          & 4.32 \\
% GPT-4o-mini      & Agreeableness          & 4.40 \\
% Llama 3        & Conscientiousness     & 4.20 \\
% Llama 3.1      & Openness            & 4.54 \\
% Llama 3.2      & Agreeableness         & 4.22 \\
% \hline
% \end{tabular}
% \label{tab:compact_cv_dominance}
% \Description{Sorting LLMs by dimensional dominance}
% \end{table}

The Llama models however have dimensional dominance across varying dimensions. For Llama-3-8B-Instruct, Conscientiousness is the dominant dimension meaning it is efficient and organised, and has strong alignment with traits such as reliability and attention to detail. Llama-3.1-8B-Instruct however, dominates the Openness dimension, which adheres toward having strong traits such as creativity, insight and originality with wide ideas. Lastly, Llama-3.2-3B-Instruct dominates Agreeableness, representing traits such as sympathetic and user-friendly interactions. Although these numbers indicate dimensional dominance for certain traits, visuals from Figure \ref{fig:Radar_plots} show a contiguous nature across most dimensions. There might be various reasons behind this such as the interconnection of personality traits like Agreeableness and Conscientiousness overlapping in their behavioural expressions. Furthermore, Fine-Tunings can contribute towards not demonstrating extreme behaviours in a specific personality trait to maintain the balance.  

% how do the tests affect the result??
% \subsection{Questionniare size vs the variability of results}


% \begin{table*}[h!]
% \centering
% \caption{Personality Trait Scores Across LLMs and Questionnaires. The table indicates the average scores of Likert scales for all the LLMs across five personality questionnaire inventory.}
% \label{tab:personality_scores}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{@{}lccccccccccccccccccccccccc@{}}
% \toprule
% \textbf{LLM} & \multicolumn{5}{c}{\centering \textbf{BFI}} & \multicolumn{5}{c}{\centering \textbf{HEXACO}}  & \multicolumn{5}{c}{\centering \textbf{MINI-IPIP}} & \multicolumn{5}{c}{\centering \textbf{IPIP NEO}}& \multicolumn{5}{c}{\centering \textbf{TIPI}} \\
% \\
% \cmidrule(lr){2-6} \cmidrule(lr){7-11} \cmidrule(lr){12-16} \cmidrule(lr){17-21} \cmidrule(lr){22-26}
% & Ex & Ag & Co & Ne & Op & Ex & Ag & Co & Ne & Op & Ex & Ag & Co & Ne & Op & Ex & Ag & Co & Ne & Op & Ex & Ag & Co & Ne & Op\\
% \midrule
%  GPT-4 & 3.5 & 4.0 & 4.3 & 3.3 & 4.4 & 3.3 & 3.3 & 3.5 & 3.6 & 3.3 & 3.5 & 4.5 & 3.4 & 2.4 & 4.0 & 3.9 & 4.5 & 4.2 & 2.2 & 3.6 & 4.4 & 5.3 & 5.9 & 2.9 & 5.8\\
% GPT-4o-m & 3.8 & 3.9 & 4.1 & 3.0 & 4.6 & 3.5 & 3.6 & 3.6 & 3.7 & 3.3 & 3.8 & 4.5 & 2.9 & 2.6 & 4.1 & 4.0 & 4.4 & 4.0 & 2.5 & 4.0 & 5.0 & 5.6 & 5.9 & 2.8 & 5.9\\
% Llama 3 & 3.5 & 3.8 & 4.8 & 3.4 & 3.9 & 3.1 & 2.9 & 3.0 & 3.1 & 3.2 & 3.2 & 4.0 & 3.1 & 2.1 & 4.0 & 3.8 & 4.2 & 4.0 & 2.2 & 3.9 & 4.6 & 5.8 & 6.1 & 2.0 & 5.8\\
% Llama 3.1 & \textbf{3.9} & 4.0 & 4.4 & 3.4 & 4.3 & 3.3 & 3.2 & 3.2 & 3.6 & 3.2 & 3.0 & 4.4 & 3.7 & 1.6 & 4.5 & 4.1 & 4.4 & 4.4 & 1.9 & 4.2 & 5.5 & 6.6 & 6.7 & 1.5 & 6.5\\
% Llama 3.2 & 3.0 & 3.9 & 4.0 & 3.1 & 4.3 & 3.4 & 3.5 & 3.7 & 3.5 & 4.0 & 2.0 & 3.9 & 2.7 & 2.5 & 3.2 & 4.18 & 4.4 & 4.4 & 1.8 & 4.3 & 4.0 & 5.5 & 6.0 & 2.6 & 4.9\\

% \bottomrule
% \Description{This table consists the results of all the llm scores with different scores}
% \label{tab:results_table}
% \end{tabular}
% }
% % \end{adjustbox}
% \end{table*}

\section{Conclusion}
This study systematically evaluates the personality traits of LLMs using established psychological questionnaires. By administering diverse tests varying in depth and length, we analyse the strengths and limitations of widely used LLMs from the Llama and OpenAI families. The findings reveal that models, whether from the same or different families, exhibit distinct personality traits and differences in dimensional dominance. For instance, GPT-4 models emphasise Agreeableness, while Llama models highlight Conscientiousness or Openness, reflecting variations in fine-tuning objectives and design goals. Additionally, traits like Extraversion and Agreeableness show high consistency, whereas Neuroticism yields more uncertain results, underscoring the need for careful questionnaire design to enhance test validity. 
%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
JTSI/Defence Science Centre's grant 2223R5CRG002, awarded to Dr Mehwish Nasim in 2023. 
Office of National Intelligence and Australian Research Council grant NI210100224 awarded to Nicolas Fay.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\appendix



\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
