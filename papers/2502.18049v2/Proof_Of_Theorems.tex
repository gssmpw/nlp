\section{Proofs of Theorems}
\textbf{Proof of Theorem \ref{Thm1:Uni}.} The proof of Theorem \ref{Thm1:Uni} follows directly as a special case of Theorem \ref{Thm1:Mul} by setting $p=1$.\qed \\

\noindent
\textbf{Proof of Theorem \ref{Thm2:Uni_var}.} The proof of Theorem \ref{Thm2:Uni_var} is structured into four steps, as outlined below.

\noindent
\textbf{Step 1: Characterizing the limiting distribution of $\sigma_t^2(w,k)$}. By Lemma \ref{independent_lemma_var}, we obtain 
\[
\sigma_t^2(w,k) = w\widehat{S}_t + (1-w)\widetilde{S}_t = w\widehat{S}_t + (1-w)\sigma_{t-1}^2(w,k) M_t,
\]
where the pairs \(\{(M_t, \widehat{S}_t)\}_{t=1}^{T}\) form an i.i.d. sequence, and \(M_t\) and \(\widehat{S}_t\) are mutually independent.




In this step, we show that \(\sigma_t^2(w,k)\) converges in distribution to a stationary random variable \(\sigma_\infty^2(w,k)\), which is characterized by the following stochastic equation for all \(0 \leq w < 1\):
\begin{align}
\label{solution_uni_variance_expression}
\sigma_\infty^2(w,k) = (1-w)M\sigma_\infty^2(w,k) +wQ,
\end{align}
where \((M, Q)\) follows the same distribution as \((M_t, \widehat{S}_t)\), and \(\sigma_\infty^2(w,k)\) is independent of \((M, Q)\).

To prove (\ref{solution_uni_variance_expression}), we proceed to verify the conditions for applying Theorem 1.5 of \cite{vervaat1979stochastic}. First, we have to prove the existence of solutions to (\ref{solution_uni_variance_expression}). Observe that
\[
\mathbb{E}\bigl[\log \abs{(1-w)M_t}\bigr] 
< 
\log \big(\mathbb{E}(M_t)\big) + \log (1-w) 
\leq
\log 1 
=
0,
\]
where the first inequality follows from the Jensen's inequality and the second inequality follows from the facts that $\log \bigl(\mathbb{E}(M_t)\bigr)=1$ and $M_t >0$.

Next, we compute
\[
\mathbb{E}[\log \abs{(1-w)M_t}] = \log \abs{1-w} + \mathbb{E}[\log M_t] = \log (1-w) + \psi\!\bigl(\tfrac{m-1}{2}\bigr)
-
\ln\!\bigl(\tfrac{m-1}{2}\bigr),
\]
where \(\psi(\cdot)\) denotes the digamma function. Since \(\log (1-w) + \psi\bigl(\tfrac{m-1}{2}\bigr) - \ln\bigl(\tfrac{m-1}{2}\bigr) > -\infty\), the expectation is finite \citep{lee1989bayesian}. Moreover,
\[
\mathbb{E}\Bigl[\log^{+}\bigl(|w\widehat{S}_t|\bigr)\Bigr]
\;\le\;
\mathbb{E}\Bigl[\log\bigl(\widehat{S}_t + 1\bigr)\Bigr]
\;\le\;
\mathbb{E}\bigl[\widehat{S}_t\bigr]<\infty,
\]
where $\log^+(x)=\max\{\log(x),1\}$. Therefore, by Theorem 1.6 (b) of \citet{vervaat1979stochastic}, we conclude that the solution to \eqref{solution_uni_variance_expression} indeed exists, where \((M,Q)\) is an independent copy of \((M_t,\widehat{S}_t)\) and \(\sigma_\infty^2(w,k)\) is independent of \((M, Q)\).

\bigskip

By the Law of Large Numbers, we have
\[
\frac{1}{n}\sum_{k=1}^{n} \log \abs{(1-w)M_k}
\;\xrightarrow{a.s.}\;
\mathbb{E}\bigl[\log \abs{(1-w)M}\bigr]
\;<\;
0.
\]
It follows that
\[
\sum_{k=1}^{n} \log \abs{(1-w)M_k}
\;\xrightarrow{a.s.}\;
-\infty,
\]
and consequently,
\[
\sum_{k=1}^{n}\log \abs{(1-w)M_k}
\;\xrightarrow{d}\;
-\infty.
\]
Thus, by Theorem 1.5 of \citep{vervaat1979stochastic}, the solution to \eqref{solution_uni_variance_expression} is unique and also serves as the limit in distribution for \(\sigma_t^2(w,k)\) as $t$ approaches infinity.

\noindent
\textbf{Step 2: Analyze the moments of \(\sigma_\infty^2(w,k)\) and establish moment convergence.} In this step, we aim to analyze the behavior of $\mathbb{E}(\sigma_\infty^2(w,k))$ and $\mathbb{E}[(\sigma_\infty^2(w,k))^2]$.


\textbf{Proving $\mathbb{E}[\sigma_{\infty}^2(w,k)] = \sigma^2$}. Since \(w > 0\), we have
\begin{align}
\mathbb{E}[(1-w)M] = 1-w < 1,
\end{align}
and, in addition,
\begin{align}
\mathbb{E}[wQ] = w\sigma^2 \leq \sigma^2 < \infty.
\end{align}
Thus, by Lemma \ref{converge_moment_theorem_vervaat}, we know 
\begin{align}
\mathbb{E}[\sigma_{\infty}^2(w,k)] < \infty.
\end{align}
Taking expectations on both sides of \eqref{solution_uni_variance_expression}, we conclude that 
\begin{align*}
\mathbb{E}[\sigma_{\infty}^2(w,k)] &= \mathbb{E}[(1-w)M\sigma_{\infty}^2(w,k) + wQ] \\&= (1-w)\mathbb{E}[M]\mathbb{E}[\sigma_{\infty}^2(w,k)] + w\mathbb{E}[Q] = (1-w)\mathbb{E}[\sigma_{\infty}^2(w,k)] + w\sigma^2.
\end{align*}
This implies $\mathbb{E}[\sigma_{\infty}^2(w,k)] = \sigma^2,
$ for any $w \in [0,1)$.


\textbf{Analyzing $\mathbb{E}(\sigma_\infty^2(w,k))$.} Next, we analyze the value of $\mathbb{E}(\sigma_\infty^2(w,k))$. If \(w > 1 - \sqrt{\frac{1}{1+\frac{2}{m-1}}}\), we obtain
\begin{align*}
\mathbb{E}[(1-w)M]^2 = (1-w)^2\mathbb{E}[M^2] = (1-w)^2\bigl(1 + \frac{2}{m-1}\bigr) < 1.
\end{align*}
Note that $\mathbb{E}[(wQ)^2] = w^2\bigl(1 + \frac{2}{n-1}\bigr)\sigma^4 < \infty$. Then, applying Lemma \ref{converge_moment_theorem_vervaat} yields that
\begin{align*}
\mathbb{E}[(\sigma_{\infty}^2(w,k))^2] < \infty, \quad \lim_{t \to \infty} \mathbb{E}[(\sigma_t^2(w,k))^2] \to \mathbb{E}[(\sigma_\infty^2(w,k))^2].
\end{align*}
Moreover, by Lemma \ref{converge_moment_theorem_vervaat}
\begin{align*}
\mathbb{E}[(\sigma_{\infty}^2(w,k))^2] 
= \bigl\{w^2\bigl(1 + \frac{2}{n-1}\bigr) + 2w(1-w)\bigr\}\sigma^4 + (1-w)^2\mathbb{E}[(\sigma_{\infty}^2(w,k))^2]\bigl(1 + \frac{2}{m-1}\bigr).
\end{align*}
This then implies
\begin{align}
\label{square_secondmoment}
\mathbb{E}[(\sigma_{\infty}^2(w,k))^2] = \frac{\sigma^4}{1 - (1-w)^2\bigl(1+\frac{2}{m-1}\bigr)}\bigl\{w^2\bigl(1 + \frac{2}{n-1}\bigr) + 2w(1-w)\bigr\}.
\end{align}
Next, we consider the case when \(0 < w \leq 1 - \sqrt{\frac{1}{1+\frac{2}{m-1}}}\). By Lemma \ref{infinity_var_lemma_uni}, we now $\mathrm{Err}_{\infty}(\sigma^2(w,k)) = \infty$ in this case.
% Next, we consider the case when \(0 < w \leq 1 - \sqrt{\frac{1}{1+\frac{2}{m-1}}}\). We first prove that \(\mathbb{E}[(\sigma_t^2)^2] = \infty\) via contradiction. Assume that \(\mathbb{E}[(\sigma_t^2)^2] < \infty\). By taking the second moment on both sides of \eqref{solution_uni_variance_expression}, we obtain that \eqref{square_secondmoment} holds. This implies
% \begin{align}
% \mathbb{E}[(\sigma_{\infty}^2)^2] = \frac{\sigma^4}{1 - (1-w)^2\bigl(1+\frac{2}{m-1}\bigr)}\bigl\{w^2\bigl(1 + \frac{2}{n-1}\bigr) + 2w(1-w)\bigr\} < 0.
% \end{align}
% This contradicts with the non-negativity of $\mathbb{E}[(\sigma_{\infty}^2)^2]$. Thus, we have \(\mathbb{E}[(\sigma_{\infty}^2)^2] = \infty\).

% To prove that \(\mathbb{E}[(\sigma_t^2)^2] \to \infty\) as \(t \to \infty\), define
% \begin{align}
% \sigma^2_{t,k} = \min(\sigma^2_{t}, k),
% \end{align}
% and 
% \begin{align}
% \sigma_{\infty,k}^2 = \min(\sigma_{\infty}^2, k).
% \end{align}
% Since
% \begin{align}
% \sigma^2_{t} \xrightarrow{d} \sigma_{\infty}^2,
% \end{align}
% and the function
% \begin{align}
% f_k(x) = \min(x^2, k)
% \end{align}
% is a bounded continuous function for any given positive integer \(k\), by Lemma \ref{helly-bray}, we have
% \begin{align}
% \mathbb{E}[f_k(\sigma_t^2)] \to \mathbb{E}[f_k(\sigma_{\infty}^2)],
% \end{align}
% which implies
% \begin{align}
% \mathbb{E}[\min((\sigma_t^2)^2, k)] \to \mathbb{E}[\min((\sigma_{\infty}^2)^2, k)],
% \end{align}
% for all \(k \in \{1, 2, \ldots\}\). By the monotone convergence theorem, we have
% \begin{align}
% \lim_{k \to \infty} \mathbb{E}[\min((\sigma_{\infty}^2)^2, k)] = \mathbb{E}[(\sigma_{\infty}^2)^2] = \infty.
% \end{align}
% Thus,
% \begin{align}
% \sup_{k} \lim_{t \to \infty} \mathbb{E}[\min((\sigma_t^2)^2, k)] = \infty.
% \end{align}
% Noting that
% \((\sigma_t^2)^2 \geq \min((\sigma_t^2)^2, k)\) for all \(k \geq 1\), we deduce
% \begin{align}
% \lim_{t \to \infty} \mathbb{E}[(\sigma_t^2)^2] \geq \sup_{k} \lim_{t \to \infty} \mathbb{E}[\min((\sigma_t^2)^2, k)] = \infty.
% \end{align}
% This proves that 
% \(\mathbb{E}[(\sigma_t^2)^2] \to \infty\), and consequently, \(\mathbb{E}[(\sigma_t^2)^2] \to \mathbb{E}[(\sigma_{\infty}^2)^2]\) also holds when \(0 < w \leq 1 - \sqrt{\frac{1}{1+\frac{2}{m-1}}}\).

% \begin{align*}
% & \mathbb{E}(\sigma_\infty^2) = \sigma^2, \quad \forall w >0,  \\
% & \mathbb{E}[(\sigma_\infty^2)^2] = \frac{2w(1-w) + (1-w)^2\bigl(1+\frac{2}{m-1}\bigr)}{1 - (1-w)^2\bigl(1+\frac{2}{n-1}\bigr)} \sigma^4, \quad \forall w > 1 - \sqrt{\frac{1}{1+\frac{2}{m-1}}},\\
% & \mathbb{E}[(\sigma_\infty^2)^2] = \infty, \quad \forall 0 < w \leq 1 - \sqrt{\frac{1}{1+\frac{2}{m-1}}}, \\
% & \lim_{t \to \infty}\mathbb{E}[(\sigma_t^2)^2] = \mathbb{E}[(\sigma_\infty^2)^2].
% \end{align*}

\noindent
\textbf{Step 3: Analyze $\mathrm{Err}_{\infty}(\sigma^2(w,k))$.}
We now know
\begin{align}
\label{var_cal_step3}
\begin{split}
\mathrm{Err}_{\infty}(\sigma^2(w,k))&=\lim_{T \to \infty}\{\mathbb{E}[\sigma^2_T(w,k)]^2-2\mathbb{E}\sigma^2_T(w,k)\sigma^2+\sigma^4\}=\lim_{T \to \infty}\mathbb{E}
[\sigma^2_T(w,k)]^2-\sigma^4 \\&=\begin{cases} 
\infty, & 0 < w \leq 1 - \sqrt{\frac{1}{1+\frac{2}{m-1}}}, \\[10pt]
\left[\frac{w^2\left(1 + \frac{2}{n-1}\right) + 2w(1-w)}{1 - (1-w)^2\left(1+\frac{2}{m-1}\right)}-1\right]\sigma^4, & 1 - \sqrt{\frac{1}{1+\frac{2}{m-1}}} < w \leq 1,
\end{cases}
\end{split}
\end{align}
which implies 
$\mathrm{Err}_{\infty}(\sigma^2(w,k)) = {\widehat{C}(w,m,n)\sigma^4}$ with 
\begin{align*}
C_1(w,m,n) = \begin{cases} 
\infty, & 0 < w \leq 1 - \sqrt{\dfrac{1}{1+\dfrac{2}{m-1}}}, \\[10pt]
\left[\dfrac{w^2\left(1 + \dfrac{2}{n-1}\right) + 2w(1-w)}{1 - (1-w)^2\left(1+\dfrac{2}{m-1}\right)} - 1 \right] , & 1 - \sqrt{\dfrac{1}{1+\dfrac{2}{m-1}}} < w \leq 1.
\end{cases}
\end{align*}
Thus 
\begin{align}
\begin{split}
\label{w_1_inequality_orignal}
&\frac{\mathrm{Err}\big(\sigma_{\infty}^2(w,k)\big)}{\mathrm{Err}(\sigma_{\infty}^2(1,k))}=\frac{C_1(w,m,n)}{C_1(1,m,n)}=\left[\dfrac{w^2\left(1 + \dfrac{2}{n-1}\right) + 2w(1-w)}{1 - (1-w)^2\left(1+\dfrac{2}{m-1}\right)} - 1 \right]\times \frac{1}{1+\frac{2}{n-1}}, \\& 1 - \sqrt{\dfrac{1}{1+\dfrac{2}{m-1}}} < w \leq 1
\end{split}
\end{align}
Solving \eqref{w_1_inequality_orignal} $<1$, we have
$\frac{n+1}{n+2m-1}<w<1$. Thus 
\begin{align*}
\frac{\mathrm{Err}\big(\sigma_{\infty}^2(w,k)\big)}{\mathrm{Err}(\sigma_{\infty}^2(1,k))}<1, \text{ for any } w 
\in \left(\frac{n+1}{n+2m-1},1\right).
\end{align*}
By \eqref{w_1_inequality_orignal},
\begin{align*}
\lim_{m\rightarrow \infty}&
    \frac{\mathrm{Err}\big(\sigma_{\infty}^2(w,k)\big)}{\mathrm{Err}(\sigma_{\infty}^2(1,k))}=\left[\dfrac{w^2\left(1 + \dfrac{2}{n-1}\right) + 2w(1-w)}{1 - (1-w)^2\left(1+\dfrac{2}{m-1}\right)} - 1 \right]\times \frac{1}{1+\frac{2}{n-1}}=
    \frac{w^2}{2w-w^2}<1,
    \\
    \text{ for} & \ \text{any } w \in \left(1 - \sqrt{\frac{m-1}{m+1}},1\right].
\end{align*}
Now we calculate the minimum of $C_1(w,m,n)$. The derivative of \( C_1(w,m,n) \) with respect to \( w \) is given as
\begin{align*}
\frac{d C_1(w,m,n)}{dw} = \frac{4 (m - 1) \left[ (m + 1) w^{2} + (n - 3) w - n + 1 \right]}{(n - 1) \left[ (m + 1) w^{2} + (-2m - 2) w + 2 \right]^{2}}.
\end{align*}
Setting the derivative equal to zero, we obtain
\begin{align*}
\frac{4 (m - 1) \left[ (m + 1) w^{2} + (n - 3) w - n + 1 \right]}{(n - 1) \left[ (m + 1) w^{2} + (-2m - 2) w + 2 \right]^{2}} = 0.
\end{align*}
Simplifying further yields
\begin{align*}
(m + 1) w^{2} + (n - 3) w - n + 1 = 0.
\end{align*}
The solutions to the above equations are given as
\begin{align*}
w^{\star}_+ &= \frac{\sqrt{n^{2} + (4m - 2) n - 4m + 5} - n + 3}{2m + 2} = \frac{2(n-1)}{\sqrt{n^2 + (4m - 2)(n-1) + 3} + n - 3} \in (0,1), \\
w^{\star}_- &= -\frac{\sqrt{n^{2} + (4m - 2) n - 4m + 5} + n - 3}{2m + 2} < 0.
\end{align*}
Let $c_0 = \left(1+\frac{2}{m-1}\right)^{-\frac{1}{2}}$. Note that
\begin{align*}
\lim_{w \rightarrow 1 - c_0} C_1(w,m,n) = \infty,
\end{align*}
Therefore, we should choose $w>1 -c_0$. Furthermore, it can be verified that
\begin{align*}
w^{\star}_+ > 1 - c_0.
\end{align*}
Thus, \( C_1(w,m,n) \) is decreasing for \( 1 - c_0 \leq w \leq w^{\star}_+ \) and increasing for \( w \geq w^{\star}_+ \). Therefore, \( w^{\star}_+ \) is indeed the optimal ratio. Note that 
\begin{align}
\label{lim_optimal_ratio_uni_var}
\lim_{m\to \infty, n\to \infty ,\frac
{n}{m}\to k} w^{\star}_{+} 
= \frac{\sqrt{k^2+4k}-k}{2},
\end{align}
and \begin{align}
\label{limit_loss_function_uni_var}
\widehat{C}(w,m,n)=&\left[\dfrac{w^2\left(1 + \dfrac{2}{n-1}\right) + 2w(1-w)}{1 - (1-w)^2\left(1+\dfrac{2}{m-1}\right)} - 1 \right] =\frac{w^2 \frac{2}{n-1}+{(1-w)^2}{\frac{2}{m-1}} }{1-(1-w)^2(1+\frac{2}{m-1})} \notag \\
=&\frac{2}{n-1}\big(C (w,k) + o(1) \big),
\end{align}
as $m \xrightarrow{} \infty, n\xrightarrow{} \infty, \frac{n}{m}\xrightarrow{}k$ with $C(w,k)$ being defined in Theorem \ref{Thm1:Uni}.

Also, we could calculate that
\begin{align}
\label{fixed_point_result_from_thm1}
C(\frac{\sqrt{k^2+4k}-k}{2},k)=\frac{\sqrt{k^2+4k}-k}{2}.
\end{align}
Combining \eqref{lim_optimal_ratio_uni_var},\eqref{limit_loss_function_uni_var} and \eqref{fixed_point_result_from_thm1}, we know 
\begin{align}
\mathrm{Err}_{\infty}\big(\sigma^2(w^{\star},k)\big)=\frac{2}{n-1}( \frac{\sqrt{k^2+4k}-k}{2}+o(1)  )\sigma^4,
\end{align}
as $m \xrightarrow{} \infty, n\xrightarrow{} \infty, \frac{n}{m}\xrightarrow{}k$. Particularly, if $w=1$, we have
\begin{align*}
\mathrm{Err}_{\infty}\big(\sigma^2(1,k)\big)=\frac{2}{n-1}\sigma^4,
\end{align*}
To sup up, we have
\begin{align*}
    \lim_{m\to \infty, n\to \infty, \frac{n}{m}\to k}\frac{\mathrm{Err}_{\infty}(\sigma^2(w^{\star},k))}{\mathrm{Err}_{\infty}(\sigma^2(1,k))} = \frac{\sqrt{k^{2} + 4k} - k}{2},
\end{align*}
for any $k>0$. This completes the proof. \qed \\

\noindent
\textbf{Proof of Theorem \ref{Thm1:Mul}.} The proof of Theorem \ref{Thm1:Mul} is structured into four steps, as outlined below.

\noindent
\textbf{Step 1: Representation of $\bm{\mu}_{t}(w,k)$}. By the definition of $\bm{\mu}_{t}(w,k)$ for $t\geq 1$, we have
\begin{align}
\label{initial_representation_mu_one_dimension}
&\bm{\mu}_{t}(w,k)=w \bm{\widehat{\mu}}_t + (1-w)\bm{\widetilde{\mu}}_{t} \notag
\\& =w(\bm{\widehat{\mu}}_t-\bm{\mu})+w\bm{\mu}+(1-w)(\bm{\widetilde{\mu}}_t-\bm{\mu}_{t-1}(w,k))+(1-w)\bm{\mu}_{t-1}(w,k)
\notag \\& =w\bm{\Sigma}^{\frac{1}{2}}\bm{\Sigma}^{-\frac{1}{2}}(
{\widehat{\bm{\mu}}_t-\bm{\mu}})+w\bm{\mu}+
(1-w)\bm{\Sigma}^{\frac{1}{2}
}_{t-1}(w,k)\bm{\Sigma}^{-\frac{1}{2}
}_{t-1}(w,k)(\widetilde{\bm{\mu}}_t-\bm{\mu_{t-1}}(w,k))\notag\\
&+(1-w)\bm{\mu}_{t-1}(w,k).
\end{align}
Next, we define $ \widetilde{\bm{z}}_t = \sqrt{m}\bm{\Sigma}^{-\frac{1}{2}
}_{t-1}(w,k)(\widetilde{\bm{\mu}}_t-\bm{\mu}_{t-1}(w,k))$ and $ \widehat{\bm{z}}_t = \sqrt{n} \bm{\Sigma}^{-\frac{1}{2}}(
{\widehat{\bm{\mu}}_t-\bm{\mu}}) $. Then according to Lemma \ref{indepedent_lemma_multivariable}, both $ \widetilde{\bm{z}}_t $ and $ \widehat{\bm{z}}_t $ are random variables following $ N(\bm{0},\bm{I}) $ and are independent of $ \mathcal{F}_{t-1} $. Furthermore, $ \widehat{\bm{z}}_t $ is independent of $ \widetilde{\bm{z}}_t $, since $ \{\bm{x}_{t,i}\}_{i=1}^n $ are generated from the real distribution. Substituting $ \widetilde{\bm{z}}_t $ and $ \widehat{\bm{z}}_t $ into \eqref{initial_representation_mu_one_dimension}, we obtain
\begin{align}
\label{representaion_one_dimension_mu}
\bm{\mu}_{t}(w,k)=w\bm{\Sigma}^{\frac{1}{2}
} \frac{1}{\sqrt{n}}\widehat{\bm{z}}_t+w\bm{\mu}+(1-w)\bm{\Sigma}_{t-1}^{\frac{1}{2}}(w,k)\frac{1}{\sqrt{m}}\widetilde{\bm z}_t+(1-w)\bm{\mu}_{t-1}(w,k).
\end{align}
Taking the conditional expectation of $\bm{\mu}_{t}(w,k)$, we have
\begin{align*}
    \mathbb{E}[\bm{\mu}_{t}(w,k)|\mathcal{F}_{t-1}] =w \bm{\mu} + (1-w) \bm{\mu}_{t-1}(w,k).
\end{align*}

\noindent
\textbf{Step 2: Calculate $\mathrm{Err}(\bm{\mu}_{t}(w,k) )$.} Substitute \eqref{representaion_one_dimension_mu} into $\mathrm{Err}(\bm{\mu}_{t}(w,k) )$, we have
\begin{align}
\label{Uni_Eq2}
&\text{Err}(\bm{\mu}_{t}(w,k)) 
= \mathbb{E}\left\{\left \|  
w\bm{\Sigma}^{\frac{1}{2}} \frac{1}{\sqrt{n}}\widehat{\bm{z}}_t + w\bm{\mu} + (1-w)\bm{\Sigma}^{\frac{1}{2}}_{t-1}(w,k)\frac{1}{\sqrt{m}}\widetilde{\bm{z}}_t + (1-w)\bm{\mu}_{t-1}(w,k) - \bm{\mu}\right \|_2^2\right\}\notag  \\
&= \mathbb{E}\left\{\left\|
w \bm{\Sigma}^{\frac{1}{2}} \frac{1}{\sqrt{n}}\widehat{\bm{z}}_t + (1-w)\bm{\Sigma}_{t-1}^{\frac{1}{2}}(w,k)\frac{1}{\sqrt{m}}\widetilde{\bm{z}}_t + (1-w)(\bm{\mu}_{t-1}(w,k)-\bm{\mu})\right\|_2^2\right\}\notag \\
& = \mathbb{E}\left[\left\| w \bm{\Sigma}^{\frac{1}{2}} \frac{1}{\sqrt{n}}\widehat{\bm{z}}_t + (1-w)\bm{\Sigma}_{t-1}^{\frac{1}{2}}(w,k)\frac{1}{\sqrt{m}}\widetilde{\bm{z}}_t\right\|^2\right]+
(1-w)^2\mathbb{E}\left[\left\|\bm{\mu}_{t-1}(w,k)-\bm{\mu}\right\|_2^2\right]\notag  \\
&+2(1-w)\mathbb{E}\left[\left(w \bm{\Sigma}^{\frac{1}{2}} \frac{1}{\sqrt{n}}\widehat{\bm{z}}_t + (1-w)\bm{\Sigma}_{t-1}^{\frac{1}{2}}(w,k)\frac{1}{\sqrt{m}}\widetilde{\bm{z}}_t\right)^T
\Big(\bm{\mu}_{t-1}(w,k)-\bm{\mu}\Big)\right]\notag 
\\
&\triangleq A_1 +A_2 +A_3.
\end{align}
Next, we proceed to calculate $A_1,A_2$ and $A_3$, separately.
\begin{align}
&A_1 = \mathbb{E}\left[\left\| w \bm{\Sigma}^{\frac{1}{2}} \frac{1}{\sqrt{n}}\widehat{\bm{z}}_t + (1-w)\bm{\Sigma}_{t-1}^{\frac{1}{2}}(w,k)\frac{1}{\sqrt{m}}\widetilde{\bm{z}}_t\right\|_2^2\right] \notag \\
=&w^2\mathbb{E}\left[\left\|  \bm{\Sigma}^{\frac{1}{2}} \frac{\widehat{\bm{z}}_t}{\sqrt{n}}\right\|_2^2   \right]
+{(1-w)^2}\mathbb{E}\left[\left\|
\bm{\Sigma}_{t-1}^{\frac{1}{2}}(w,k)\frac{\widetilde{\bm{z}}_t}{\sqrt{m}}
\right\|_2^2\right] 
+\frac{2w(1-w)}{\sqrt{n}\sqrt{m}} \mathbb{E}\left[  \left< \bm{\Sigma}^{\frac{1}{2}} \widehat{\bm{z}}_t, \bm{\Sigma}_{t-1}^{\frac{1}{2}}(w,k)\widetilde{\bm{z}}_t\right>  \right]
\notag \\
=&w^2\mathrm{tr}(\bm{\Sigma}) \frac{1}{n}+\frac{(1-w)^2}{m}\mathbb{E}\left(\mathrm{tr}\left( 
\bm{\Sigma}_{t-1}(w,k)\widetilde{\bm{z}}_t \widetilde{\bm{z}}_t^T
\right)
\right)+\frac{2w(1-w)}{\sqrt{n}\sqrt{m}}\left[  \left<\mathbb{E} \bm{\Sigma}^{\frac{1}{2}} \widehat{\bm{z}}_t, \mathbb{E}\bm{\Sigma}_{t-1}^{\frac{1}{2}}(w,k)\widetilde{\bm{z}}_t\right>  \right]
\notag 
\\=&w^2\mathrm{tr}(\bm{\Sigma}) \frac{1}{n}+\frac{(1-w)^2}{m}\mathrm{tr}\left(\mathbb{E}\left[ 
\bm{\Sigma}_{t-1}(w,k)\right]\mathbb{E}\left[\widetilde{\bm{z}}_t \widetilde{\bm{z}}_t^T\right]
\right)
+0 \notag
\\=&w^2 \mathrm{tr}(\bm{\Sigma}) \frac{1}{n} +\frac{(1-w)^2}{m} \mathrm{tr}(\bm{\Sigma}),
\end{align}
where the third and  the forth equalities uses the fact that $\bm{\Sigma}_{t-1}(w,k),\widehat{z}_t,\widetilde{z}_t$ are independent to each other and the last equality uses the fact that $\mathbb{E}[\bm{\Sigma}_{t-1}(w,k)]=\bm{\Sigma}$, which is shown in the Step 1 of the proof of Lemma \ref{lemma_tr_cal}.

For $A_2$, we can verify that $A_2=(1-w)^2\mathbb{E}\left[\left\|\bm{\mu}_{t-1}(w,k)-\bm{\mu}\right\|^2\right]$ is exactly $(1-w)^2 \text{Err} (\bm{\mu}_{t-1} (w,k) ) $. For $A_3$, we have 
\begin{align*}
A_3=&2(1-w)\mathbb{E}\left[\left(w \bm{\Sigma}^{\frac{1}{2}} \frac{1}{\sqrt{n}}\widehat{\bm{z}}_t + (1-w)\bm{\Sigma}_{t-1}^{\frac{1}{2}}(w,k)\frac{1}{\sqrt{m}}\widetilde{\bm{z}}_t\right)^T
\Big(\bm{\mu}_{t-1}(w,k)-\bm{\mu}\Big)\right]\\
=&2(1-w)\mathbb{E}_{\mathcal{F}_{t-1}}\mathbb{E}\left[    \left(w \bm{\Sigma}^{\frac{1}{2}} \frac{1}{\sqrt{n}}\widehat{\bm{z}}_t + (1-w)\bm{\Sigma}_{t-1}^{\frac{1}{2}}(w,k)\frac{1}{\sqrt{m}}\widetilde{\bm{z}}_t\right)^T
\Big(\bm{\mu}_{t-1}(w,k)-\bm{\mu}\Big)     \Big| \mathcal{F}_{t-1}  \right]\\
=&2\mathbb{E}_{\mathcal{F}_{t-1}}(0)=0.
\end{align*}

By combining the results of $A_1$-$A_3$, and substituting them into \eqref{Uni_Eq2}, we obtain the following result.
\begin{align*}
\text{Err}(\bm{\mu}_t (w,k) )=w^2 \mathrm{tr}(\bm{\Sigma}) \frac{1}{n} +\frac{(1-w)^2}{m} \mathrm{tr}(\bm{\Sigma})+(1-w)^2\text{Err}( \bm{\mu}_{t-1}(w,k) ),
\end{align*}
which implies
\begin{align}
\label{explicti_error_mu}
\text{Err}(\bm{\mu}_t (w,k)) =(1-w)^{2t}\text{Err}( \bm{\mu}_{0}(w,k)  )
+\frac{
[w^2\mathrm{tr}(\bm{\Sigma})\frac{1}{n}+\frac{(1-w)^2}{m}\mathrm{tr}(\bm{\Sigma})](1- (1-w)^{2t} )
}{1-(1-w)^2}.
\end{align}
\eqref{explicti_error_mu} implies that 
\begin{align}
\lim_{t \to \infty} \text{Err}(\bm{\mu}_t (w,k) )=\frac{w^2 \mathrm{tr}(\bm{\Sigma}) \frac{1}{n}+\frac{(1-w)^2 \mathrm{tr}(\bm{\Sigma}) }{m}   }{1 - (1-w)^2}=\frac{C(w,k)\mathrm{tr}(\bm{\Sigma})}{n},
\end{align}
where $C(w,k)$ is defined as
\begin{align*}
    C(w,k) = \frac{w^2+(1-w)^2k}{2w-w^2}.
\end{align*}

\noindent
\textbf{Step 3: Find the minimum of $C(w,k)$} \textbf{in} [0,1]. Let $C(w,k)$ take the derivative with respect to $w$, we have
\begin{align}
\label{derivate_Cw}
\frac{\mathrm{d}C(w,k)}{\mathrm{d}w}=\frac{2 \left(w^{2} + kw - k\right)}{\left(w - 2\right)^{2} w^{2}}.
\end{align}
Note that when $0 < w < \frac{\sqrt{k^{2} + 4k} - k}{2}$, $\frac{\mathrm{d}C(w,k)}{\mathrm{d}w} < 0$, meaning $C(w,k)$ decreases as $w$ increases. Conversely, when $\frac{\sqrt{k^{2} + 4k} - k}{2} < w < 1$, $\frac{\mathrm{d}C(w,k)}{\mathrm{d}w} > 0$, so $C(w,k)$ increases as $w$ increases. Therefore $w^{\star}=\frac{\sqrt{k^{2} + 4k} - k}{2}$ is the optimal weight, and 
\begin{align}
\frac{\mathrm{Err}(\bm{\mu}_{\infty}(w^\star,k))}{\mathrm{Err}(\bm{\mu}_{\infty}(1,k))} = \frac{C(w^{\star}, k)}{C(1,k)}=\frac{C\left(\frac{\sqrt{k^{2} + 4k} - k}{2},k\right)}{1}=\frac{\sqrt{k^2+4k}-k}{2}=\frac{2k}{\sqrt{k^2+4k}+k}<\frac{2k}{2k}=1.
\end{align}
This completes the proof. \qed \\
\noindent
\textbf{Proof of Theorem \ref{thm3:multi_variance}.} When $w\leq 1-\sqrt{\frac{1}{1+\frac{2}{m-1}}}$, by Lemma \ref{multi_critical_point_lemma}, we have
\begin{align*}
\lim_{t\to \infty}\mathrm{Err}(\mathbf{\Sigma}_{t}(w,k)) = \infty.
\end{align*}
When $w >1-\sqrt{\frac{1}{1+\frac{2}{m-1}}}$, by Lemma \ref{multi_critical_point_lemma}, we have
\begin{align*}
\lim_{t \to \infty} \bm{v}_t(w,k) \ \textit{exists}.
\end{align*}
Define $\bm{v}_{\infty}(w,k)=\lim_{t \to \infty} \bm{v}_t(w,k)$, then taking the limit in both sides of \eqref{tr_vector_original}, we have
\begin{align}
\bm{v}_{\infty}(w,k) = \mathbf{A} \bm{v}_{\infty}(w,k) + \bm{b}.
\end{align}
Namely
\begin{align*}
\begin{bmatrix}
\lim_{t \to \infty}\mathbb{E}[\mathrm{tr}^2(\bm \Sigma_t(w,k))] \\
\lim_{t \to \infty}\mathbb{E}[\mathrm{tr}(\bm \Sigma_t^2(w,k))]
\end{bmatrix}&=\begin{bmatrix}
(w - 1)^2 & \frac{2(w - 1)^2}{m - 1} \\
\frac{(w - 1)^2}{m - 1} & \frac{(m )(w - 1)^2}{m - 1}
\end{bmatrix}\begin{bmatrix}
\lim_{t \to \infty}\mathbb{E}[\mathrm{tr}^2(\bm \Sigma_{t-1}(w,k))] \\
\lim_{t \to \infty}\mathbb{E}[\mathrm{tr}(\bm \Sigma_{t-1}^2(w,k))]
\end{bmatrix}
\\ &+
\begin{bmatrix}
\mathrm{tr}^2(\bm{\Sigma})(2w-w^2)+\frac{2w^2}{n-1}\mathrm{tr}(\bm{\Sigma}^2) \\
\mathrm{tr}^2(\bm{\Sigma})\frac{w^2}{n-1}+(2w-w^2+w^2\frac{1}{n-1})\mathrm{tr}(\bm{\Sigma}^2)
\end{bmatrix}.
\end{align*}
Solving this, we have
\begin{align*}
\lim_{t \to \infty}\mathbb{E}[\mathrm{tr}(\bm \Sigma_t^2(w,k))]=\frac{N}{D},
\end{align*}
where $N$ and $D$ are defined as
\begin{align*}
N & = 
\frac{(w-1)^2}{(m-1)(1-(w-1)^2)}
\left[
\mathrm{tr}^2(\mathbf{\Sigma})(2w-w^2)
+ \frac{2w^2}{n-1}\mathrm{tr}(\mathbf{\Sigma}^2)
\right]
\\ & + \mathrm{tr}^2(\mathbf{\Sigma})\frac{w^2}{n-1}
+ \left(2w-w^2+w^2\frac{1}{n-1}\right)\mathrm{tr}(\mathbf{\Sigma}^2), \\
D& = 
1 
- \frac{(w-1)^2}{(m-1)[1-(w-1)^2]}
\frac{2(w-1)^2}{m-1} 
- \frac{m}{m-1}(w-1)^2.
\end{align*}
Then plugging these two results into $\lim_{t \to \infty} \mathrm{Err}(\mathbf{\Sigma}_t(w,k))=\lim_{t \to \infty}\mathbb{E}\left[\mathrm{tr}(\bm{\Sigma}^2_t(w,k))\right]-\mathrm{tr}(\mathbf{\Sigma}^2)$ as shown in the proof of Lemma \ref{multi_critical_point_lemma}, and this completes the proof. \qed \\

\noindent
\textbf{Proof of Theorem \ref{linear_regression_fixed_design}.} Conditioned on $\mathbf{X}$, solving \eqref{Eqn:BetaUpdate}, we have
\begin{align}
\begin{split}
\widehat{\boldsymbol{\beta}}_t(w)&=(\mathbf{X}^T\mathbf{X})^{-1}[w \mathbf{X}^T\mathbf{X}\boldsymbol{\beta}+w\mathbf{X}^T\boldsymbol{\epsilon}_t+(1-w)\mathbf{X}^T\mathbf{X}\widehat{\boldsymbol{\beta}}_{t-1}(w)+(1-w)\mathbf{X}^T\widetilde{\boldsymbol{\epsilon}_t}]
\\&=w(\boldsymbol{\beta}+(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \boldsymbol{\epsilon}_t) +(1-w)(\widehat{\boldsymbol{\beta}}_{t-1}(w)+(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \widetilde{\boldsymbol{\epsilon}}_t),
\end{split}
\end{align}
then we could see Case \ref{alg:linear_regression_fixed} is essentially Case \ref{alg:Multivariable_gaussian_estimation}, with $n=m=1$, the underlying ground truth Gaussian distribution being $N(\boldsymbol{\beta}, \sigma^2(\mathbf{X^T X})^{-1})$. The result then follows by taking the expectation with regard to $\mathbf{X}$ and utilizing the result from Theorem \ref{Thm1:Mul}. \qed \\



\section{Proofs of Corollaries}
\textbf{Proof of Corollary \ref{Th1_Coro1}.} The proof of Corollary \ref{Th1_Coro1} follows directly as a special case of Corollary \ref{Coro:Multi_Mean} by setting $p=1$.\qed \\

\noindent
\textbf{Proof of Corollary \ref{Thm2:Coro2}.} By Theorem \ref{Thm2:Uni_var}, $\mathrm{Err}_{\infty}(\sigma^2(w,k)) = {\widehat{C}(w,m,n)\sigma^4}$ with 
\begin{align*}
C_1(w,m,n) = \begin{cases} 
\infty, & 0 < w \leq 1 - \sqrt{\dfrac{1}{1+\dfrac{2}{m-1}}}, \\[10pt]
\left[\dfrac{w^2\left(1 + \dfrac{2}{n-1}\right) + 2w(1-w)}{1 - (1-w)^2\left(1+\dfrac{2}{m-1}\right)} - 1 \right] , & 1 - \sqrt{\dfrac{1}{1+\dfrac{2}{m-1}}} < w \leq 1,
\end{cases}
\end{align*}
where the optimal weight $w^\star=\argmin_{w \in (0,1]}C_1(w, m, n)$ is given as
$$
w^\star = \frac{\sqrt{n^{2} + \left(4m - 2\right) n - 4m + 5} - n + 3}{2m + 2}.
$$
Since $\lim_{w \to 0^{+}}C_1(w,m,n)=\infty$, we have $\lim_{w \rightarrow 0^{+}} \mathrm{Err}(\sigma_{\infty}^2(w,k)) =\infty$. \\
We have already shown \begin{align*}
\mathrm{Err}_{\infty}\big(\sigma^2(w^{\star},k)\big)=\frac{2}{n-1}( \frac{\sqrt{k^2+4k}-k}{2}+o(1)  )\sigma^4=\frac{2}{n-1}\left( \frac{\sqrt{k^2+4k}-k}{2}  \right)\sigma^4+o(n^{-1}),
\end{align*}
as $m \xrightarrow{} \infty, n\xrightarrow{} \infty, \frac{n}{m}\xrightarrow{}k$ and if $w=1$,
\begin{align*}
\mathrm{Err}_{\infty}\big(\sigma^2(1,k)\big)=\frac{2}{n-1}\sigma^4
\end{align*}
in the proof of Theorem \ref{Thm2:Uni_var}. This completes the proof.
\qed \\

\noindent
\textbf{Proof of Corollary \ref{Coro:Multi_Mean}.} By Theorem \ref{Thm1:Mul}, it holds that 
$$\mathrm{Err}(\bm{\mu}_{\infty}(w,k)) = \lim_{t \rightarrow \infty} \mathrm{Err}(\bm{\mu}_{t}(w,k) ) 
=\frac{C(w,k)\mathrm{tr}(\bm{\Sigma})}{n},$$
where $C(w,k) =\frac{w^2+(1-w)^2k}{2w-w^2}$ and $\mathrm{tr}(\cdot)$ is the trace of a matrix. 
Additionally, we have the following limits and values
$$
\lim_{w \to 0^+} C(w,k) = \infty, \quad \lim_{w \to 1^-} C(w,k) = C(1,k) = 1, \quad \text{and} \quad C(w^{\star},k) = \frac{\sqrt{k^{2} + 4k} - k}{2},
$$
where $w^\star = \frac{\sqrt{k^{2} + 4k} - k}{2}$ is the optimal weight as defined in Theorem \ref{Thm1:Mul}. This completes the proof.\qed \\

\noindent
\textbf{Proof of Corollary \ref{multi_var_corollary}.} 
We use the same notations as in Theorem \ref{thm3:multi_variance}. Using first order taylor expansion, we have
\begin{align}
\label{1/D}
\begin{split}
\frac{1}{D}=&\frac{1}{1 
- \frac{(w-1)^2}{(m-1)[1-(w-1)^2]}
\frac{2(w-1)^2}{m-1} 
- \frac{m}{m-1}(w-1)^2}=\frac{1}{1-(w-1)^2-\frac{(w-1)^2}{m-1}-O(\frac{1}{m^2})}\\&=\frac{1}{1-(w-1)^2}-\frac{1}{[1-(w-1)^2]^2}(-\frac{(w-1)^2}{m-1}-O(\frac{1}{m^2}))+o(-\frac{(w-1)^2}{m-1}-O(\frac{1}{m^2}))\\=&
\frac{1}{1-(w-1)^2}+\frac{(w-1)^2}{[1-(w-1)^2]^2}\frac{1}{m}+o(\frac{1}{m}).
\end{split}
\end{align}
We also have
\begin{align}
\label{N}
\begin{split}
N & = 
\frac{(w-1)^2}{(m-1)(1-(w-1)^2)}
\left[
\mathrm{tr}^2(\mathbf{\Sigma})(2w-w^2)
+ \frac{2w^2}{n-1}\mathrm{tr}(\mathbf{\Sigma}^2)
\right]
\\ & + \mathrm{tr}^2(\mathbf{\Sigma})\frac{w^2}{n-1}
+ \left(2w-w^2+w^2\frac{1}{n-1}\right)\mathrm{tr}(\mathbf{\Sigma}^2)\\ &=\mathrm{tr}^2(\mathbf{\Sigma})[\frac{(w-1)^2}{m-1}+\frac{w^2}{n-1}]+(2w-w^2+\frac{w^2}{n-1})\mathrm{tr}(\mathbf{\Sigma}^2)+O(\frac{1}{mn})
\end{split}
\end{align}
Combining \eqref{N} and \eqref{1/D} together, we could calculate that
\begin{align}
\begin{split}
\frac{N}{D}&=\mathrm{tr}(\mathbf{\Sigma}^2)[1+\frac{1}{m}\frac{(w-1)^2}{1-(1-w)^2}+\frac{1}{n-1}\frac{w^2}{1-(w-1)^2} ]+\mathrm{tr}^2(\mathbf{\Sigma})[\frac{(w-1)^2}{m-1}+\frac{w^2}{n-1}]\frac{1}{1-(w-1)^2}\\&+o(\max{(\frac{1}{m},\frac{1}{n})}) \\&=[\mathrm{tr}(\mathbf{\Sigma}^2)+\mathrm{tr}^2(\mathbf{\Sigma})][\frac{1}{m}\frac{(w-1)^2}{1-(1-w)^2}+\frac{1}{n}\frac{w^2}{1-(w-1)^2} ]+\mathrm{tr}(\mathbf{\Sigma^2})+o(\max(\frac{1}{m},\frac{1}{n})). 
\end{split}
\end{align}
Therefore, when $w >1-\sqrt{\frac{1}{1+\frac{2}{m-1}}}$
\begin{align}
\begin{split}
&\lim_{t \to \infty} \mathrm{Err}(\mathbf{\Sigma}_t(w,k))=
\frac{N}{D}-\mathrm{tr}(\mathbf{\Sigma}^2)\\&=
[\mathrm{tr}(\mathbf{\Sigma}^2)+\mathrm{tr}^2(\mathbf{\Sigma})][\frac{1}{m}\frac{(w-1)^2}{1-(1-w)^2}+\frac{1}{n}\frac{w^2}{1-(w-1)^2} ]+o(\max(\frac{1}{m},\frac{1}{n})) \\
&=[\mathrm{tr}(\mathbf{\Sigma}^2)+\mathrm{tr}^2(\mathbf{\Sigma})][\frac{k}{n}\frac{(w-1)^2}{1-(1-w)^2}+\frac{1}{n}\frac{w^2}{1-(w-1)^2}]+o(\max(\frac{1}{m},\frac{1}{n}))\\&=[\mathrm{tr}(\mathbf{\Sigma}^2)+\mathrm{tr}^2(\mathbf{\Sigma})]\frac{1}{n}\frac{k(w-1)^2+w^2}{1-(1-w)^2}+o(\max{(\frac{1}{m},\frac{1}{n})})\\
&=[\mathrm{tr}(\mathbf{\Sigma}^2)+\mathrm{tr}^2(\mathbf{\Sigma})]\frac{C(w,k)+o(1)}{n}.
\end{split}
\end{align}
When $w\leq 1-\sqrt{\frac{1}{1+\frac{2}{m-1}}}$, from Theorem \ref{thm3:multi_variance}, we know 
\begin{align}
\lim_{t \to \infty}\mathrm{Err}(\mathbf{\Sigma}_t(w,k))=\infty.
\end{align}
The remainder of the proof follows by analyzing the function \( C(w, k) \), as detailed in the proofs of Theorem \ref{Thm1:Mul} and Corollary \ref{Coro:Multi_Mean}. \qed

\noindent
\section{Proofs of Proportions}
\textbf{Proof of Proportion \ref{corollary_global_optimum_uni}}
The proof of Proportion \ref{corollary_global_optimum_uni} follows directly as a special case of Proportion \ref{corollary_global_optimum} by setting $p=1$.\qed \\

\noindent
\textbf{Proof of Proportion \ref{corollary_global_optimum}.}
Define
\begin{equation*}
    \boldsymbol{z}_{t,i} = \boldsymbol{x}_{t,i} - \boldsymbol{\mu},
\end{equation*}
and
\begin{equation*}
    \widetilde{\boldsymbol{z}}_{t,i} = \widetilde{\boldsymbol{x}}_{t,i} - \boldsymbol{\mu}_{t}(\{w_i,\widetilde{w}_i\},k).
\end{equation*}
Under the data generation mechanism specified in Case \ref{alg:Multivariable_gaussian_estimation}, $
    \boldsymbol{z}_{t,i} \sim N(0, \mathbf{\Sigma})$
$,
    \widetilde{\boldsymbol{z}}_{t,i} \sim N(0, \mathbf{\Sigma}_{t}(w,k)),
$
independently conditioned on $\mathcal{F}_t$.
Solving \eqref{Eqn:MULMeanUpdate_General}, we obtain
\begin{align}
\label{general_analytic_form}
\boldsymbol{\mu}_{t+1}(\{w_i, \widetilde{w}_i\}, k) = \sum_{i=1}^{n} \frac{w_i}{n} \boldsymbol{x}_{t,i} + \sum_{j=1}^{m} \frac{\widetilde{w}_j}{m} \widetilde{\boldsymbol{x}}_{t,j}.
\end{align}

Taking the expectation on both sides of \eqref{general_analytic_form} and utilizing the assumption that the estimation is unbiased at each step, we derive
\begin{align*}
\boldsymbol{\mu} = \sum_{i=1}^{n} \frac{w_i}{n} \boldsymbol{\mu} + \sum_{j=1}^{m} \frac{\widetilde{w}_j}{m} \boldsymbol{\mu}=(\sum_{i=1}^{n}\frac{w_i}{n}+\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m})\boldsymbol{\mu},
\end{align*}
which implies 
\begin{align}
\label{unbiased_corollary}
\sum_{i=1}^{n}\frac{w_i}{n}+\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m}=1
\end{align}.
Then 
\begin{align}
\label{general_stepbystep_cal}
\begin{split}
& \mathbb{E}[(\bm{\mu}_{t+1}(\{{w}_i, \widetilde{w}_i\}, k)-\bm{\mu})^2|\mathcal{F}_t]=\mathbb{E}[(\sum_{i=1}^{n}\frac{w_i}{n}\boldsymbol{x}_{t,i}+\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m}\widetilde{\boldsymbol{x}}_{t,j}-\bm{\mu})^2|\mathcal{F}_t] \\ 
=& \mathbb{E}[(\sum_{i=1}^{n}\frac{w_i}{n}\boldsymbol{\mu}+\sum_{i=1}^{n}\frac{w_i}{n}\boldsymbol{z}_i+\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m}{\boldsymbol{\mu}}_{t}(\{w_i,\widetilde{w}_i\},k)+\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m}\widetilde{z}_j-\bm{\mu})^2|\mathcal{F}_t] \\
=&\mathbb{E}[(\sum_{i=1}^{n}\frac{w_i}{n}\boldsymbol{z}_i+\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m}{\boldsymbol{\mu}}_{t}(\{w_i,\widetilde{w}_i\},k)+\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m}\widetilde{z}_j-\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m}\bm{\mu})^2|\mathcal{F}_t] \\=& \mathbb{E}[(\sum_{i=1}^{n}\frac{w_i}{n}\boldsymbol{z}_i+\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m}\widetilde{z}_j)^2|\mathcal{F}_t]+\mathbb{E}[(\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m}{\boldsymbol{\mu}}_{t}(\{w_i,\widetilde{w}_i\},k)-\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m}{\boldsymbol{\mu}})^2|\mathcal{F}_t]\\+&2\mathbb{E}[(\sum_{i=1}^{n}\frac{w_i}{n}\boldsymbol{z}_i+\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m}\widetilde{z}_j)(\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m}{\boldsymbol{\mu}}_{t}(\{w_i,\widetilde{w}_i\},k)-\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m}{\boldsymbol{\mu}})|\mathcal{F}_t] \\=&\frac{1}{n^2}\sum_{i=1}^{n}w_i^2\mathrm{tr}(\mathbf{\Sigma})+\frac{1}{m^2}\sum_{j=1}^{m}\widetilde{w}_j^2\mathbb{E}[\mathrm{tr}(\mathbf{\Sigma}_t(w,k))|\mathcal{F}_t]+\frac{1}{m^2}(\sum_{j=1}^{m}\widetilde{w}_j)^2\mathbb{E}[(\bm{\mu}_{t}(\{{w}_i, \widetilde{w}_i\}, k)-\bm{\mu})^2|\mathcal{F}_t],
\end{split}
\end{align}
where in the second equation, we use 
\begin{align*}
\sum_{i=1}^{n}\frac{w_i}{n}\boldsymbol{\mu}-\boldsymbol{\mu}=\sum_{i=1}^{n}\frac{w_i}{n}\boldsymbol{\mu}-(\sum_{i=1}^{n}\frac{w_i}{n}+\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m})\boldsymbol{\mu}=-\sum_{j=1}^{m}\frac{\widetilde{w}_j}{m}\boldsymbol{\mu}.
\end{align*}
Take expectation in both sides of \eqref{general_stepbystep_cal}, and note that 
\begin{align*}
\mathbb{E}\left\{\mathbb{E}[(\bm{\mu}_{t}(\{{w}_i, \widetilde{w}_i\}, k)-\bm{\mu})^2|\mathcal{F}_t]\right\}=\mathbb{E}[(\bm{\mu}_{t}(\{{w}_i, \widetilde{w}_i\}, k)-\bm{\mu})^2]=\mathrm{Err}(\bm{\mu}_{t}(\{w_i, \widetilde{w}_i\}, k)),
\end{align*}
and 
\begin{align*}
\mathbb{E}\left\{\mathbb{E}[\mathrm{tr}(\mathbf{\Sigma}_t(w,k))|\mathcal{F}_t]\right\}=\mathrm{tr}\left(\mathbb{E}[\mathbf{\Sigma}_t(w,k)]=\mathrm{tr}(\mathbf{\Sigma} \right),
\end{align*}
since the estimation of the covariance matrix is also unbiased by Step 1 in the proof of Lemma \ref{lemma_tr_cal}, we have
\begin{align*}
\mathrm{Err}(\bm{\mu}_{t+1}(\{w_i, \widetilde{w}_i\}, k)) = \left(\sum_{i=1}^{m} \widetilde{w}_i \right)^2 \mathrm{Err}(\bm{\mu}_t (\{w_i, \widetilde{w}_i\}, k) ) + \left(\frac{1}{n^2}\sum_{i=1}^{n} w_i^2 + \frac{1}{m^2}\sum_{i=1}^{m} \widetilde{w}_i^2\right)\mathrm{tr}(\bm{\Sigma}).
\end{align*}

Applying the Cauchy-Schwarz inequality, we have:
\begin{equation}
\sum_{i=1}^{m} \widetilde{w}_i^2 \geq m \left(\frac{1}{m}\sum_{i=1}^{m} \widetilde{w}_i\right)^2, \quad \sum_{i=1}^{n} w_i^2 \geq n \left(\frac{1}{n}\sum_{i=1}^{n} w_i\right)^2,
\end{equation}
which gives the lower bound:
\begin{equation*}
\mathrm{Err}(\bm{\mu}_{t+1}(\{w_i, \widetilde{w}_i\}, k)) \geq \left(\sum_{i=1}^{m} \widetilde{w}_i \right)^2 \mathrm{Err}(\bm{\mu}_t (\{w_i, \widetilde{w}_i\}, k) ) + \left\{ \frac{1}{n} \left(\frac{1}{n}\sum_{i=1}^{n} w_i \right)^2 + \frac{1}{m} \left(\frac{1}{m}\sum_{i=1}^{m} \widetilde{w}_i \right)^2 \right\}\mathrm{tr}(\bm{\Sigma}).
\end{equation*}

By \eqref{unbiased_corollary},
\begin{equation*}
\frac{1}{n}\sum_{i=1}^{n}w_i+\frac{1}{m}\sum_{i=1}^{m}\widetilde{w}_i=1.
\end{equation*}
Setting 
\begin{equation*}
\frac{1}{n}\sum_{i=1}^{n}w_i=w, \quad \frac{1}{m}\sum_{i=1}^{m}\widetilde{w}_i=1-w,
\end{equation*}
we obtain:
\begin{equation}
\label{key_lowerbound}
\mathrm{Err}(\bm{\mu}_{t+1}(\{w_i, \widetilde{w}_i\}, k)) \geq \left(1-w \right)^2 \mathrm{Err}(\bm{\mu}_t (\{w_i, \widetilde{w}_i\}, k) ) + \left\{ \frac{1}{n} \left(w \right)^2 + \frac{1}{m} \left(1-w \right)^2 \right\}\mathrm{tr}(\bm{\Sigma}).
\end{equation}
When $w \leq 0 \ \text{or} \  w \geq 2$, $(1-w)^2 \geq 1$, and \eqref{key_lowerbound} implies that $\lim_{t \to \infty} \mathrm{Err}(\bm{\mu}_{t}(\{w_i, \widetilde{w}_i\}, k))=\infty$. When $w \in (0,2)$, \eqref{key_lowerbound} implies 
\begin{align}
\mathrm{Err}(\bm{\mu}_{\infty}(\{w_i, \widetilde{w}_i\}, k)) = \lim_{t \rightarrow \infty} \mathrm{Err}(\bm{\mu}_{t}(\{w_i, \widetilde{w}_i\}, k) ) 
\geq \frac{C(w,k)\mathrm{tr}(\bm{\Sigma})}{n},
\end{align}
where $C(w,k)$ is defined in Theorem \ref{Thm1:Mul}, and the equality holds if and only if $w_i=w_j, \forall i\neq j$ and $\widetilde{w}_i=\widetilde{w}_j, \forall i\neq j$, which is the original Case \ref{alg:Multivariable_gaussian_estimation}.
From the gradient analysis for $C(w,k)$, we see that $w^{\star}$ is in fact the global minimum of $C(w,k)$ in $(0,2)$. Then we know $\frac{C(w^{\star},k)\mathrm{tr}(\bm{\Sigma})}{n}$ is in fact the lower bound for $\mathrm{Err}(\bm{\mu}_{\infty}(\{w_i, \widetilde{w}_i\}, k))$. This confirms that the mixture using the optimal weight $w^{\star}$ remains the BLUE estimator in a more general case. \qed