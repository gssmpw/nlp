
\section{Supporting Lemmas}

\begin{lemma}
[Lemma 1.1 of \cite{vervaat1979stochastic}]\label{characterize_solution} Consider the stochastic difference equation $Y_n = A_n Y_{n-1} + B_n$ for $n \geq 1$, where the pairs $(A_n, B_n)$ are independent and identically distributed (i.i.d.) random variables. If $Y_n \xrightarrow{d} Y$ for some $\mathbb{R}$-valued random variable $Y$, then the distribution of $Y$ satisfies the following stochastic fixed-point equation: $Y = A Y + B$, where $Y$ and $(A, B)$ are independent, and $(A, B)$ follows the same distribution as $(A_n, B_n)$.
\end{lemma}

%\begin{lemma}
%[Theorem 1.5 of \cite{vervaat1979stochastic}]
%\label{convergence_to_solution}
%Assume the solution of $\eqref{stochastic_solution}$ exists and 
%\begin{align}
%\sum_{k=1}^{n}\log |A_k| \xrightarrow{d}-\infty,
%\end{align}
%then \eqref{stochastic_solution}'s solution is unique. Denoting the solution of \eqref{stochastic_solution} by $Y$, then we have
%\begin{align}
%Y_n \xrightarrow{d} Y
%\end{align}
%no matter what the $Y_0$ is.
%\end{lemma}
%One key assumption in \eqref{convergence_to_solution} is the existence of the solution, which could be guaranteed with the assumptions in the following lemma:
%\begin{lemma}[Cf. Theorem 1.6(b) in \cite{vervaat1979stochastic}]
%\label{existence_solution}
%If \begin{align}-\infty < \mathbb{E}\log (|A_n|)<0,
%\end{align}
%and 
%\begin{align}
%\mathbb{E}\log^+(|B_n|)<\infty,
%\end{align}
%where $log^+$ is defined by $\log^+(x)=max(\log(x),0)$, then \eqref{stochastic_solution} has a solution.
%\end{lemma}

\begin{lemma}
[Theorem 5.1 of \cite{vervaat1979stochastic}]\label{converge_moment_theorem_vervaat}
Consider the stochastic difference equation $Y_n = A_n Y_{n-1}+B_n$, where $(A_n, B_n)$ are i.i.d. and $(A_n, B_n) \sim (A,B)$. If $\mathbb{E} |A|^p<1$ and $\mathbb{E}|B|^p < \infty$, then $Y=AY+B$ has a unique solution $Y$ with $\mathbb{E}|Y|^p<\infty$. The moments $\mathbb{E}Y^j$ for $j \in [p]$ are uniquely determined by the equations
\begin{align*}
\mathbb{E} Y^j=\sum_{k=0}^j\binom{j}{k} \mathbb{E}\left(A^k B^{j-k}\right) \mathbb{E} Y^k \quad \text { for } \quad j \in [p].
\end{align*}
If $\mathbb{E} |Y_0|^p <\infty$, then $\mathbb{E}Y_n^p \to \mathbb{E}Y^p$ as $n \to \infty$.
\end{lemma}

\begin{lemma}[Monotone Convergence Theorem; \citealp{durrett2019probability}]
\label{monotone_convergence}
Assume we have a series of non-negative random variables $\{X_1, X_2,\cdots, X_n\}$ and $X$ satisfying $X_n \uparrow and 
 X$ a.s. Then $ \mathbb{E}X_n \to \mathbb{E} X$.
 \end{lemma}
\begin{lemma}[Theorem 2.1 (ii) of \cite{billingsley2013convergence}]
\label{helly-bray}
Assume $\{X_n\}$ is a series of random variables and $X_n \xrightarrow{d}X$, then for a bounded continuous function $f$, we have $\mathbb{E}f(X_n) \to \mathbb{E}f(X)$.
\end{lemma}

\begin{lemma}[(9.3.6.l) of \cite{holgersson2020collection}]
\label{tr(2)_cal_lemma}
Assume $\bm{W} \sim W\left(\bm{\Sigma}, n\right)$, then 
\begin{align}
\mathbb{E}\left[\mathrm{tr}\left\{\bm{W}^2\right\}\right]=\left(n+n^2\right) \mathrm{tr}\left\{\Sigma^2\right\}+n(\mathrm{tr}\{\Sigma\})^2
\end{align}
\end{lemma}
\begin{lemma}[(9.3.6.j) of \cite{holgersson2020collection}]
\label{tr^2_cal_lemma}
If $\bm{W} \sim W\left(\bm{\Sigma}, n\right)$, then
\begin{align*}
\mathbb{E}[\mathrm{tr}(\bm A \bm{W}) \mathrm{tr}(\bm B \bm{W})]=n \mathrm{tr}\left(\bm A \bm \Sigma \bm B \bm \Sigma\right)+n \mathrm{tr}(\bm A^{\prime} \bm \Sigma \bm B\bm \Sigma)  +n^2 \mathrm{tr}(\bm{A} \bm \Sigma) \mathrm{tr}(\bm B \bm \Sigma) 
\end{align*}
for any matrices $\bm{A}$ and $\bm{B}$, where $\mathrm{tr}(\cdot)$ denotes the trace of matrices.
\end{lemma}

\section{Our Lemmas and Their Proofs}

\noindent
\begin{lemma}
\label{indepedent_lemma}
Define $ \widetilde{z}_t = \sqrt{m} \frac{\widetilde{\mu}_t - \mu_{t-1}(w,k)}{\sigma_{t-1}(w,k)} $ and $ \widehat{z}_t = \sqrt{n} \frac{\widehat{\mu}_t - \mu}{\sigma} $ in Fresh Data Augmentation Case \ref{alg:univariable_gaussian_estimation}, then both $\widetilde{z}_t$ and $\widehat{z}_t$ are independent of $\mathcal{F}_{t-1}$ and follow $\mathcal{N}(0,1)$.
\end{lemma}

\noindent
\textbf{Proof of Lemma \ref{indepedent_lemma}:}
    Notice that according to the Change of Variables Formula for the probability density function,
\begin{align*}
\mathbb{P}&(\widetilde{z}_t=a|\mathcal{F}_{t-1})=\frac{\sigma_{t-1}(w,k)}{\sqrt{m}}\mathbb{P}(\widetilde{\mu}_t =\frac{1}{\sqrt{m}}\sigma_{t-1}(w,k)a+\mu_{t-1}(w,k)|\mathcal{F}_{t-1})\\&= \frac{\sigma_{t-1}(w,k)}{\sqrt
m} \mathbb{P}(
\widetilde{\mu}_t =\frac{1}{\sqrt{m}}\sigma_{t-1}(w,k)a+\mu_{t-1}(w,k)|\mu_{t-1}(w,k),\sigma_{t-1}(w,k)
) \\&=\frac{\sigma_{t-1}(w,k)}{\sqrt{m}}
\frac{1}{\sqrt
{2\pi \frac{\sigma^{2}_{t-1}(w,k)}{m} }
}e^{-\frac{1}{2 \frac{\sigma_{t-1}^2(w,k)}{m}    }   ( \frac{1}{\sqrt{m}}\sigma_{t-1}(w,k)a+\mu_{t-1}(w,k)   -\mu_{t-1}(w,k)   )^2      } \\ & =\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}a^2},
\end{align*}
we then know 
\begin{align}
\mathbb{P}(\widetilde{z}_t=a)=\mathbb{E}_{\mathcal{F}_{t-1}}\mathbb{P}(\widetilde{z}_t=a | \mathcal{F}_{t-1})=\mathbb{E}_{\mathcal{F}_{t-1}}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}a^2}=\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}a^2},
\end{align}
which implies $\mathbb{P}(\widetilde{z}_t=a)=\mathbb{P}(\widetilde{z}_t=a|\mathcal{F}_{t-1}),$ $\forall a \in \mathbb{R}$. This shows that $\widetilde{z}_t$ is independent of $\mathcal{F}_{t-1}$ and follows $\mathcal{N}(0,1)$. Similary, we could show that $\widehat{z}_t$ is also independent of $\mathcal{F}_{t-1}$ and follows $\mathcal{N}(0,1)$. \qed 

\begin{lemma}
\label{indepedent_lemma_multivariable}
Define $ \widetilde{\bm{z}}_t = \sqrt{m}\bm{\Sigma}^{-\frac{1}{2}
}_{t-1}(w,k)(\widetilde{\bm{\mu}}_t-\bm{\mu_{t-1}}(w,k)) $ and $\widehat{\bm{z}}_t = \sqrt{n} \bm{\Sigma}^{-\frac{1}{2}}(
{\widehat{\bm{\mu}}_t-\bm{\mu}})$ in Fresh Data Augmentation Case \ref{alg:Multivariable_gaussian_estimation}, then both $\widetilde{\bm z}_t$ and $\widehat{\bm z}_t$ are independent of $\mathcal{F}_{t-1}$ and follow $N(0,\mathbf{I})$.
\end{lemma}

\noindent
\textbf{Proof of Lemma \ref{indepedent_lemma_multivariable}:}
    Notice that according to the Change of Variables Formula for the probability density function,
\begin{align*}
\mathbb{P}&(\widetilde{\bm{z}}_t=\bm{a}|\mathcal{F}_{t-1})=\text{det}\left(\frac{\bm{\Sigma}^{\frac{1}{2}}_{t-1}(w,k)}{\sqrt{m}}\right)\mathbb{P}\left(\widetilde{\bm{\mu}}_t =\frac{1}{\sqrt{m}}\bm{\Sigma}^{\frac{1}{2}}_{t-1}(w,k)a+\bm{\mu}_{t-1}(w,k)|\mathcal{F}_{t-1}\right)\\&= \text{det}\left(\frac{\bm{\Sigma}^{\frac{1}{2}}_{t-1}(w,k)}{\sqrt{m}}\right) \mathbb{P}\left(
\widetilde{\bm{\mu}}_t =\frac{1}{\sqrt{m}}\bm{\Sigma}^{\frac{1}{2}}_{t-1}(w,k)\bm a+\bm{\mu}_{t-1}(w,k)|\bm{\mu}_{t-1}(w,k),\bm{\Sigma}_{t-1}(w,k)
\right) \\&=\text{det}\left(\frac{\bm{\Sigma}^{\frac{1}{2}}_{t-1}(w,k)}{\sqrt{m}}\right)
\frac{1}{\sqrt
{{(2\pi)}^p \text{det}\left(\frac{\bm{\Sigma}_{t-1}(w,k)}{m}\right) }
} \\& \times e^{-\frac{1}{2}   ( \frac{1}{\sqrt{m}}\bm{\Sigma}^{\frac{1}{2}}_{t-1}(w,k)\bm a+\bm{\mu}_{t-1}(w,k)   -\bm{\mu}_{t-1}(w,k)   )^T {(\frac{\bm{\Sigma}_{t-1}(w,k)}{m})}^{-1} ( \frac{1}{\sqrt{m}}\bm{\Sigma}^{\frac{1}{2}}_{t-1}(w,k)\bm{a}+\bm{\mu}_{t-1}(w,k)   -\bm{\mu}_{t-1}(w,k)   )    } \\ & =\frac{1}{\sqrt{{(2\pi)}^p}}e^{-\frac{1}{2}\|\bm a\|^2},
\end{align*}
we then know 
\begin{align}
\mathbb{P}(\widetilde{\bm z}_t=\bm{a})=\mathbb{E}_{\mathcal{F}_{t-1}}\left[\mathbb{P}(\widetilde{\bm z}_t=\bm{a} | \mathcal{F}_{t-1})\right]=\mathbb{E}_{\mathcal{F}_{t-1}}\left[\frac{1}{\sqrt{{(2\pi)}^p}}e^{-\frac{1}{2}\|\bm{a}\|^2}\right]=\frac{1}{\sqrt{{(2\pi)}^p}}e^{-\frac{1}{2}\|\bm{a}\|^2},
\end{align}
which implies $\mathbb{P}(\widetilde{\bm z}_t=\bm a)=\mathbb{P}(\widetilde{\bm z}_t=\bm a|\mathcal{F}_{t-1}),$ $\forall a \in \mathbb{R}$. This shows that $\widetilde{\bm z}_t$ is independent of $\mathcal{F}_{t-1}$ and follows $N(\bm{0},\mathbf{I})$. Similarly, we can show that $\widehat{\bm z}_t$ is also independent of $\mathcal{F}_{t-1}$ and follows $N(\bm 0,\mathbf{I})$. \qed 

\begin{lemma}
\label{independent_lemma_var}
Recall that $\widehat{S}_t =  \frac{1}{n-1}\sum_{i=1}^n (x_{t,i}-\widehat{\mu}_t)^2$ and $\widetilde{S}_t =  \frac{1}{m-1}\sum_{i=1}^m (\widetilde{x}_{t,i}-\widetilde{\mu}_t)^2$. We define $M_t = \frac{\widetilde{S}_t}{\sigma_{t-1}^2}$. Then, \(\{(M_t, \widehat{S}_t)\}_{t=1}^{\infty}\) are i.i.d. \(\mathbb{R}^2_{>0}\)-valued random variables. Furthermore, \(M_t\) and \(\widehat{S}_t\) are independent, following the distributions
\[
M_t \sim \frac{\chi^2(m-1)}{m-1} \mbox{ and } \widehat{S}_t \sim \sigma^2 \frac{\chi^2(n-1)}{n-1}.
\]
\end{lemma}
\noindent
\textbf{Proof of Lemma \ref{independent_lemma_var}:} 
The proof of Lemma \ref{independent_lemma_var} is structured into two steps, as outlined below.
\noindent
\textbf{Step 1: Independence between $\mathcal{F}_{t-1}$ and $(M_t,\widehat{S}_t)$}.
The proof approach in this step closely follows that of Lemma \ref{indepedent_lemma}. Observe that in the Fresh Data Augmentation Case \ref{alg:univariable_gaussian_estimation}, conditional on \(\mathcal{F}_{t-1}\), we have:
\[
\mathbb{P}\big((\widetilde{S}_t, \widehat{S}_t) \in A \mid \mathcal{F}_{t-1}\big) = \mathbb{P}\Bigg(  \frac{\sigma_{t-1}^2}{m-1}\chi^2(m-1) \otimes \frac{\sigma^2}{n-1}\chi^2(n-1) \in A \Bigg),
\]
for any event \(A \subset \mathbb{R}_{>0}^2\). Applying the linear transformation \((x, y) \mapsto \big(\frac{x}{\sigma_{t-1}^2}, {y}\big)\) to the expression above, we obtain:
\[
\mathbb{P}\big((M_t, \widehat{S}_t) \in B \mid \mathcal{F}_{t-1}\big) = \mathbb{P}\Bigg(\Big(\frac{\widetilde{S}_t}{\sigma_{t-1}^2}, {\widehat{S}_t}\Big) \in B \mid \mathcal{F}_{t-1}\Bigg) = \mathbb{P}\Bigg(\Big(\frac{\chi^2(m-1)}{m-1}, \sigma^2\frac{\chi^2(n-1)}{n-1}\Big) \in B \Bigg),
\]
for any event $B \subset \mathbb{R}_{}>0^2)$. This implies that
\begin{align*}
      \mathbb{P}\big((M_t, \widehat{S}_t) \in B\big) 
    &= \mathbb{E}_{\mathcal{F}_{t-1}}\Big[\mathbb{P}\big((M_t, \widehat{S}_t) \in B \mid \mathcal{F}_{t-1}\big)\Big] \\
    &= \mathbb{E}_{\mathcal{F}_{t-1}}\Big[\mathbb{P}\Big(\Big(\frac{\chi^2(m-1)}{m-1}, \sigma^2 \frac{\chi^2(n-1)}{n-1}\Big) \in B\Big)\Big] \\
    &= \mathbb{P}\Big(\Big(\frac{\chi^2(m-1)}{m-1}, \sigma^2\frac{\chi^2(n-1)}{n-1}\Big) \in B\Big).  
\end{align*}
Thus, we have
\[
\mathbb{P}\big((M_t, \widehat{S}_t) \in B\big) = \mathbb{P}\big((M_t, \widehat{S}_t) \in B \mid \mathcal{F}_{t-1}\big),
\]
which implies that \((M_t, \widehat{S}_t)\) is independent of \(\mathcal{F}_{t-1}\). Moreover, the distribution of $(M_t, \widehat{S}_t)$ is given as
\[
(M_t, \widehat{S}_t) \sim \left(\frac{\chi^2(m-1)}{m-1}, \sigma^2\frac{\chi^2(n-1)}{n-1}\right).
\]
\textbf{Step 2: Show $\{(M_t,\widehat{S}_t)\}_{t=1}^{\infty}$ are i.i.d. random variables}.
We proceed to establish that the sequence \(\{(M_t, \widehat{S}_t)\}\) consists of i.i.d. random variables. 

In the first step, we have already shown that 
\[
(M_t, \widehat{S}_t) \sim \left(\frac{\chi^2(m-1)}{m-1}, \sigma^2\frac{\chi^2(n-1)}{n-1}\right).
\]
Thus, by Kolmogorov’s Consistency Theorem \citep{khoshnevisan2006multiparameter}, it suffices to prove that for any \(k \geq 1\) and any indices \(i_1, i_2, \ldots, i_k\), the random variables 
\[
(M_{i_1}, Q_{i_1}), (M_{i_2}, Q_{i_2}), \ldots, (M_{i_k}, Q_{i_k})
\]
are mutually independent. 


To this end, we use mathematical induction. When \(k = 1\), the conclusion is trivially true. Assume the conclusion holds for \(k = m\), i.e., for any \(m\) indices \(i_1, i_2, \ldots, i_m\), the random variables 
\[
(M_{i_1}, Q_{i_1}), (M_{i_2}, Q_{i_2}), \ldots, (M_{i_m}, Q_{i_m})
\]
are mutually independent. 


We now prove that the conclusion also holds for \(k = m+1\). Consider \(m+1\) indices \(i_1, i_2, \ldots, i_m, i_{m+1}\). Without loss of generality, assume \(i_1 < i_2 < \cdots < i_m < i_{m+1}\). By the inductive hypothesis, the random variables 
\[
(M_{i_1}, Q_{i_1}), (M_{i_2}, Q_{i_2}), \ldots, (M_{i_m}, Q_{i_m})
\]
are mutually independent. Moreover, note that 
\[
\{(M_{i_1}, Q_{i_1}), (M_{i_2}, Q_{i_2}), \ldots, (M_{i_m}, Q_{i_m})\} \in \mathcal{F}_{i_m},
\]
where \(\mathcal{F}_{i_m} \subseteq \mathcal{F}_{i_{m+1}-1}\), and \((M_{i_{m+1}}, Q_{i_{m+1}})\) is independent of \(\mathcal{F}_{i_{m+1}-1}\). Therefore, \((M_{i_{m+1}}, Q_{i_{m+1}})\) is independent of 
\[
\{(M_{i_1}, Q_{i_1}), (M_{i_2}, Q_{i_2}), \ldots, (M_{i_m}, Q_{i_m})\}.
\]
This completes the proof of independence for \(k = m+1\). By the principle of mathematical induction, the sequence \(\{(M_t, \widehat{S}_t)\}\) is i.i.d. This completes the proof. \qed \\

\begin{lemma}
\label{infinity_var_lemma_uni}
For any $k>0$, it holds that 
\begin{equation*}
\mathrm{Err}(\sigma_{\infty}^2(w,k)) = \infty,
\end{equation*}
provided that $w \leq 1 - \sqrt{\frac{1}{1 + \frac{2}{m-1}}}$.
\end{lemma}


\noindent
\textbf{Proof of Lemma \ref{infinity_var_lemma_uni}:} According to Lemma \ref{independent_lemma_var}, we have
\begin{align*}
\sigma_t^2(w,k)=w \widehat{S}_t+(1-w)\sigma_{t-1}^2(w,k) M_t.
\end{align*}
Taking the square of both sides and then the expectation, we obtain
\begin{align*}
& \mathbb{E}\sigma_t^4(w,k)=\mathbb{E}\{w^2 {\widehat{S}_t}^2+(1-w)^2\sigma_{t-1}^4(w,k) M_t^2+2w(1-w)\widehat{S}_t\sigma_{t-1}^2(w,k) M_t\}\\ &=w^2\sigma^4 \left(1+\frac{2}{n-1}\right)+(1-w)^2\mathbb{E}\sigma_{t-1}^4(w,k)\mathbb{E}M_t^2+2w(1-w)\mathbb{E}\widehat{S}_t\mathbb{E}\sigma_{t-1}^2(w,k)\mathbb{E}M_t \\
&=w^2\sigma^4 \left(1+\frac{2}{n-1}\right)+(1-w)^2\left(1+\frac{2}{m-1}\right)\mathbb{E}\sigma_{t-1}^4(w,k)+2w(1-w)\sigma^4.
\end{align*}
Here we use a fact that the second order moment of a $\chi^2_k$ distribution is $k^2+2k$ and the fact that $\widehat{S}_t,\sigma_{t-1}^2(w,k),M_t$ are independent from Lemma \ref{independent_lemma_var}.

If $w\leq 1-\sqrt{\frac{1}{1+\frac{2}{m-1}}}$, we have $(1-w)^2(1+\frac{2}{m-1})\geq 1$. This then implies that
\begin{align}
\begin{split}
\label{uni_var_infintiy_proof}
    \mathbb{E}\sigma_t^4(w,k) \geq & \mathbb{E}\sigma_{t-1}^4(w,k)+w^2\sigma^4 \left(1+\frac{2}{n-1}\right)+2w(1-w)\sigma^4 \\
    \geq & \mathbb{E}\sigma_{0}^4+t\left[w^2\sigma^4 \left(1+\frac{2}{n-1}\right)+2w(1-w)\sigma^4\right].
\end{split}
\end{align}
Let $t$ go to infinity, we have $\lim_{t \to \infty}\mathbb{E}\sigma_t^4(w,k)=\infty$. Therefore, if $w\leq 1-\sqrt{\frac{1}{1+\frac{2}{m-1}}}$, it holds that
\begin{align}
\mathrm{Err}(\sigma_{\infty}^2(w,k)) =\lim_{t\to \infty}\mathbb{E}[\sigma_t^2(w,k)-\sigma^2]^2=\infty,
\end{align}
for any $k >0$. This completes the proof. \qed

\begin{lemma}
\label{lemma_tr_cal}
Let $\bm{\Sigma}_t(w,k)$ be the covariance matrix defined in \textbf{Fresh Data Augmentation Case} \ref{alg:Multivariable_gaussian_estimation} and define a 2-dimensional vector $\bm{v}_t(w,k) = \left(\mathbb{E}[\mathrm{tr}^2(\bm{\Sigma}_t(w,k))],\mathbb{E}[\mathrm{tr}(\bm{\Sigma}_t^2(w,k))]\right)^T$. Then we have
\begin{align}
\label{tr_vector_original}
\bm{v}_t(w,k) = \bm{A} \bm{v}_{t-1}(w,k) + \bm{b},
\end{align}
where the coefficient matrix $\bm{A}$ and the constant vector $\bm{b}$ are defined respectively as follows:
\begin{align*}
\bm{A} =
\begin{bmatrix}
(w - 1)^2 & \frac{2(w - 1)^2}{m - 1} \\
\frac{(w - 1)^2}{m - 1} & \frac{(m )(w - 1)^2}{m - 1}
\end{bmatrix} \mbox{ and }    \bm{b} =
\begin{bmatrix}
\mathrm{tr}^2(\bm{\Sigma})(2w-w^2)+\frac{2w^2}{n-1}\mathrm{tr}(\bm{\Sigma}^2) \\
\mathrm{tr}^2(\bm{\Sigma})\frac{w^2}{n-1}+(2w-w^2+w^2\frac{1}{n-1})\mathrm{tr}(\bm{\Sigma}^2)
\end{bmatrix}.
\end{align*}

\end{lemma}
\noindent
\textbf{Proof of Lemma \ref{lemma_tr_cal}} The proof of Lemma \ref{lemma_tr_cal} is structured into four steps, as outlined below.

\noindent
\textbf{Step 1: Calculating $\mathbb{E}\bigl[\bm{\Sigma}_t(w,k)\bigr]$}. Similar to the step 2 in the proof of Theorem \ref{Thm2:Uni_var}, we can prove $\mathbb{E}[\bm \Sigma_t(w,k)]=\bm \Sigma$ for values of $(w,k,t)$ by mathematical induction. According to Case \ref{alg:Multivariable_gaussian_estimation}, $\mathbb{E}[\bm{\Sigma}_0] = \mathbb{E}[\frac{1}{n-1}\sum_{i=1}^n (\bm{x}_{0,i}-\bm{\mu}_0)(\bm{x}_{0,i}-\bm{\mu}_0)^T]=\bm \Sigma$. We first suppose that $\mathbb{E}[\bm \Sigma_{t}(w,k)]=\bm \Sigma$ for any value of $(w,k)$, then we have
\begin{align*}
\mathbb{E}[\bm\Sigma_{t+1}(w,k)]&=\mathbb{E}[w\widehat{\bm S}_{t+1}+  (1-w) \widetilde{\bm S}_{t+1}]=w \mathbb{E}[\widehat{\bm S}_{t+1}]+(1-w)\mathbb{E}[\widetilde{\bm S}_{t+1}]\\
&=w \bm\Sigma +(1-w)\mathbb{E}[\widetilde{\bm S}_{t+1}] =w \bm\Sigma +(1-w)\mathbb{E}_{\mathcal{F}_{t}}[\mathbb{E}[\widetilde{\bm S}_{t+1}| \mathcal{F}_{t}]]\\
&=w\bm\Sigma +(1-w)\mathbb{E}_{\mathcal{F}_{t}}[\bm\Sigma_{t}(w,k) ]=w\bm\Sigma+(1-w)\bm \Sigma=\bm \Sigma.
\end{align*}
Then by mathematical induction, we know $\mathbb{E}[\bm \Sigma_t(w,k)]=\bm \Sigma$ indeed holds for any values of $k$, $w$, and $t \geq 0$.

\noindent
\textbf{Step 2: Calculating $\mathbb{E}\bigl[\mathrm{tr}^2(\bm{\Sigma}_t(w,k))\bigr]$.} Recall that
\begin{align}
\bm{\Sigma}_t(w,k) = w\widehat{\bm S}_t+(1-w)\widetilde{\bm S}_t.
\end{align}
With this, we have
\begin{align}
\begin{split}
\label{tr^2_first_calculation}
\mathbb{E}&\bigl[\mathrm{tr}^2(\bm{\Sigma}_t(w,k))\bigr]=\mathbb{E}[\mathrm{tr}(w\widehat{\bm S}_t+(1-w)\widetilde{\bm S}_t)\mathrm{tr}(w\widehat{\bm S}_t+(1-w)\widetilde{\bm S}_t)]  \\&=w^2\mathbb{E}[\mathrm{tr}^2(\widehat{\bm{S}_t})]+(1-w)^2\mathbb{E}[\mathrm{tr}^2(\widetilde{\bm{S}_t})]+2w(1-w)\mathbb{E}[\mathrm{tr}(\widehat{\bm S}_t)\mathrm{tr}(\widetilde{\bm S}_t)] \\ &=w^2\mathbb{E}[\mathrm{tr}^2(\widehat{\bm{S}_t})]+(1-w)^2\mathbb{E}[\mathrm{tr}^2(\widetilde{\bm{S}_t})]+2w(1-w)\mathbb{E}[\mathrm{tr}(\widehat{\bm S}_t)]\mathbb{E}[\mathrm{tr}(\widetilde{\bm{S}}_t)] \\&=w^2\mathbb{E}[\mathrm{tr}^2(\widehat{\bm{S}_t})]+(1-w)^2\mathbb{E}[\mathrm{tr}^2(\widetilde{\bm{S}_t})]+2w(1-w)\mathbb{E}[\mathrm{tr}(\widehat{\bm S}_t)]\mathbb{E}[\mathrm{tr}(\widetilde{\bm S}_t)]  \\ &= w^2\mathbb{E}[\mathrm{tr}^2(\widehat{\bm{S}_t})]+(1-w)^2\mathbb{E}_{\mathcal{F}_{t-1}}\mathbb{E}[\mathrm{tr}^2(\widetilde{\bm{S}_t})|\mathcal{F}_{t-1}]+2w(1-w)\mathbb{E}[\mathrm{tr}(\widehat{\bm S}_t)]\mathbb{E}[\mathrm{tr}(\widetilde{\bm S}_t)]. 
%\\ &= w^2 \frac{2}{(n-1)}\mathrm{tr}\bigl(\bm{\Sigma}^2\bigr) + {w^2}\mathrm{tr}^2(\bm{\Sigma}) \notag\\&\quad + (1-w)^2 \frac{2}{(m-1)}\mathbb{E}\Bigl[\mathrm{tr}\bigl(\bm{\Sigma}_{t-1}^2(w,k)\bigr)\Bigr] \notag\\&\quad + (1-w)^2 \mathbb{E}\bigl[\mathrm{tr}^2(\bm{\Sigma}_{t-1}(w,k))\bigr] \\&\quad + 2w(1-w)\mathrm{tr}^2\bigl(\bm{\Sigma}\bigr)
\end{split}
\end{align}
where the last equality follows from the fact that ${\widehat{\bm S}_t}$ and $\widetilde{\bm{S}}_t$ are independent. 

Next, conditioned on $\mathcal{F}_{t-1}$, we have
$$
(n-1)\widehat{\bm{S}}_t \sim W(\bm{\Sigma},n-1)
\mbox{ and }
(m-1)\widetilde{\bm{S}}_t \sim W(\bm{\Sigma}_{t-1},m-1),
$$
where $W(\cdot,\cdot)$ represents the Wishart distribution. Therefore, using the properties of Wishart distributions, we further have
\begin{align*}
    \mathbb{E}[\mathrm{tr}(\widehat{\bm S}_t)] = \mathrm{tr}(\mathbb{E}[\widehat{\bm S}_t]) = \mathrm{tr}(\bm{\Sigma}) \mbox{ and }
    \mathbb{E}[\mathrm{tr}(\widetilde{\bm S}_t)] 
    = \mathrm{tr}(\mathbb{E}_{\mathcal{F}_{t-1}}[\mathbb{E}[\widetilde{\bm S}_t] \mid \mathcal{F}_{t-1}]) 
 = \mathrm{tr}(\bm{\Sigma}).
\end{align*}
Using Lemma \ref{tr^2_cal_lemma} with \(\bm{A} = \bm{B} = \bm{I}\) (the identity matrices), we obtain
\begin{align*}
\mathbb{E}[\mathrm{tr}^2(\widehat{\bm{S}_t})|\mathcal{F}_{t-1}]=\frac{2}{(n-1)}\mathrm{tr}\bigl(\bm{\Sigma}^2\bigr)
   + \mathrm{tr}^2(\bm{\Sigma}).
\end{align*}
Applying a similar argument yields that
\begin{align*}
\mathbb{E}[\mathrm{tr}^2(\widetilde{\bm{S}_t})|\mathcal{F}_{t-1}]=\frac{2}{(m-1)}\mathrm{tr}\bigl(\bm{\Sigma}_{t-1}^2(w,k)\bigr)
   + \mathrm{tr}^2(\bm{\Sigma}_{t-1}(w,k)).
\end{align*}
Therefore, we can express $\mathbb{E}\bigl[\mathrm{tr}^2(\bm{\Sigma}_t(w,k))\bigr]$ as 
\begin{align}
\label{MulB1}
    \mathbb{E}&\bigl[\mathrm{tr}^2(\bm{\Sigma}_t(w,k))\bigr]
    = w^2\mathbb{E}[\mathrm{tr}^2(\widehat{\bm{S}_t})]+(1-w)^2\mathbb{E}_{\mathcal{F}_{t-1}}\mathbb{E}[\mathrm{tr}^2(\widetilde{\bm{S}_t})|\mathcal{F}_{t-1}]+2w(1-w)\mathbb{E}[\mathrm{tr}(\widehat{\bm S}_t)]\mathbb{E}[\mathrm{tr}(\widetilde{\bm S}_t)] \notag \\
    =& \frac{2w^2}{(n-1)}\mathrm{tr}\bigl(\bm{\Sigma}^2\bigr) + {w^2}\mathrm{tr}^2(\bm{\Sigma})  + (1-w)^2 \frac{2}{(m-1)} \mathbb{E}\Bigl[\mathrm{tr}\bigl(\bm{\Sigma}_{t-1}^2(w,k)\bigr)\Bigr] \notag\\& + (1-w)^2 \mathbb{E}\bigl[\mathrm{tr}^2(\bm{\Sigma}_{t-1}(w,k))\bigr]  +2w(1-w) \mathrm{tr}^2\bigl(\bm{\Sigma}\bigr).
\end{align}

\noindent
\textbf{Step 3: Calculating $\mathbb{E} \bigl[ \mathrm{tr}\bigl(\bm{\Sigma}_t^2(w, k)\bigr) \bigr]$}. Next, by Lemma \ref{tr(2)_cal_lemma}, we have
\begin{align*}
\mathbb{E}\bigl[ \mathrm{tr}(\widehat{\bm{S}}_t^2)  \bigr]= \frac{(n-1) + (n-1)^2}{(n-1)^2}\mathrm{tr}\bigl(\bm{\Sigma}^2\bigr)
   + \frac{1}{n-1}\mathrm{tr}^2(\bm{\Sigma}), 
\end{align*}
and
\begin{align*}
\mathbb{E}[\mathrm{tr}(\widetilde{\bm{S}}_t^2)|\mathcal{F}_{t-1}]=\frac{(m-1)+(m-1)^2}{(m-1)^2}\mathrm{tr}\bigl(\bm{\Sigma}_{t-1}^2(w, k)\bigr)+\frac{1}{m-1}\mathrm{tr}^2(\bm{\Sigma}_{t-1}(w, k)).
\end{align*}
Therefore, we can express $\mathbb{E} \bigl[ \mathrm{tr}\bigl(\bm{\Sigma}_t^2(w, k)\bigr) \bigr]$ as
\begin{align}
\label{tr_2_first_calculation}
\mathbb{E}& \bigl[ \mathrm{tr}\bigl(\bm{\Sigma}_t^2(w, k)\bigr) \bigr] =w^2 \mathbb{E}\bigl[ \mathrm{tr}(\widehat{\bm{S}}_t^2)  \bigr]+(1-w)^2\mathbb{E}_{\mathcal{F}_{t-1}}\mathbb{E}[\mathrm{tr}(\widetilde{\bm{S}}_t^2)|\mathcal{F}_{t-1}]
\notag \\ & =w^2 \frac{(n-1) + (n-1)^2}{(n-1)^2}\mathrm{tr}\bigl(\bm{\Sigma}^2\bigr)
   + \frac{w^2}{n-1}\mathrm{tr}^2(\bm{\Sigma})  \notag +
(1-w)^2 \frac{(m-1)+(m-1)^2}{(m-1)^2}\mathbb{E}[\mathrm{tr}\bigl(\bm{\Sigma}_{t-1}^2(w, k)\bigr)]\notag \\ 
&+\frac{(1-w)^2}{m-1}\mathbb{E} [\mathrm{tr}^2(\bm{\Sigma}_{t-1}(w, k))] +2w(1-w)\mathrm{tr}(\bm{\Sigma}^2).
\end{align}

\noindent
\textbf{Step 4: Expressing the result in vector form}. Combining \eqref{MulB1} and \eqref{tr_2_first_calculation} together, we have
\begin{align*}
\begin{bmatrix}
\mathbb{E}[\mathrm{tr}^2(\bm \Sigma_t(w,k))] \\
\mathbb{E}[\mathrm{tr}(\bm \Sigma_t^2(w,k))]
\end{bmatrix}&=\bm{A}
\begin{bmatrix}
\mathbb{E}[\mathrm{tr}^2(\bm \Sigma_{t-1}(w,k))] \\
\mathbb{E}[\mathrm{tr}(\bm \Sigma_{t-1}^2(w,k))]
\end{bmatrix}
+
\begin{bmatrix}
\mathrm{tr}^2(\bm{\Sigma})(2w-w^2)+\frac{2w^2}{n-1}\mathrm{tr}(\bm{\Sigma}^2) \\
\mathrm{tr}^2(\bm{\Sigma})\frac{w^2}{n-1}+(2w-w^2+\frac{w^2}{n-1})\mathrm{tr}(\bm{\Sigma}^2)
\end{bmatrix},
\end{align*}
where $\bm{A}$ is given as 
\begin{align*}
    \bm{A}=\begin{bmatrix}
(w - 1)^2 & \frac{2(w - 1)^2}{m - 1} \\
\frac{(w - 1)^2}{m - 1} & \frac{m(w - 1)^2}{m - 1}
\end{bmatrix}
\end{align*}
This completes the proof. \qed \\

\begin{lemma}
\label{Lemma_temp_for_Thm4}
The value of $\mathrm{Err}_{\infty}(\bm{\Sigma}(w,k))$ can be analyzed in the following two cases:
\label{multi_critical_point_lemma}
\begin{align}
\lim_{t\to \infty}\mathrm{Err}(\bm{\Sigma}_{t}(w,k)) = \lim_{t \to \infty}\mathbb{E}\big[\left\|\bm{\Sigma}_{t}(w,k) - \bm{\Sigma}\right \|^2_F\big]=\infty,
\end{align}
if and only if $w\leq 1-\sqrt{\frac{1}{1+\frac{2}{m-1}}}$.
\begin{align}
\lim_{t\to \infty}\mathrm{Err}(\bm{\Sigma}_{t}(w,k)) = \lim_{t \to \infty}\mathbb{E}\big[\left\|\bm{\Sigma}_{t}(w,k) - \bm{\Sigma}\right\|_F^2\big] \ \textit{exists and}<\infty,
\end{align}
if and only if $w>1-\sqrt{\frac{1}{1+\frac{2}{m-1}}}$.
\end{lemma}

\noindent
\textbf{Proof of Lemma \ref{Lemma_temp_for_Thm4}.} Consider solving the characteristic equation  
\begin{align}
|\bm{A} - \lambda \bm{I}| = 0,
\end{align}
where $\bm{A}$ is defined in Lemma \ref{lemma_tr_cal},
which yields the following two eigenvalues:
\begin{align*}
\lambda_1 = (1-w)^2 \frac{m-2}{m-1} \mbox{ and }
\lambda_2 = (1-w)^2 \frac{m+1}{m-1}.
\end{align*}
and the corresponding eigenvectors:
\begin{align*}
\bm{v}_1 = 
\begin{bmatrix}
2 \\ 
-1
\end{bmatrix} \mbox{ and }
\bm{v}_2 = 
\begin{bmatrix}
1 \\ 
1
\end{bmatrix}.
\end{align*}
Then $\bm{A}$ admits the following decomposition:
\begin{align}
\label{Def_A}
\bm{A}=\bm{C}^{}\begin{bmatrix}
\lambda_1 & 0\\
0 & \lambda_2
\end{bmatrix} \bm{C}^{-1} \mbox{ with }
\bm{C} = 
\begin{bmatrix}
  2 & 1 \\ 
  -1 & 1
\end{bmatrix}.
\end{align}
Plugging (\ref{Def_A}) into \eqref{tr_vector_original}, we have
\begin{align*}
\bm{v}_t(w,k) = \bm{C}^{}\begin{bmatrix}
\lambda_1 & 0\\
0 & \lambda_2
\end{bmatrix} \bm{C}^{-1} \bm{v}_{t-1}(w,k) + \bm{b},
\end{align*}
which implies
\begin{align}
\label{inverse_form_orignal}
\bm{C}^{-1} \bm{v}_t(w,k) =\begin{bmatrix}
\lambda_1 & 0\\
0 & \lambda_2
\end{bmatrix} \bm{C}^{-1} \bm{v}_{t-1}(w,k) + \bm{C}^{-1}\bm{b}.
\end{align}
Define $\begin{pmatrix}
    \bm{r}_t(w,k) \\ \bm{h}_t(w,k)
\end{pmatrix}=\bm{C}^{-1} \bm{v}_t(w,k)$, then \eqref{inverse_form_orignal} can be re-written as
\begin{align}
\bm{r}_t(w,k)=\lambda_1  \bm{r}_{t-1}(w,k)+\frac{1}{3}\bm{b}_1-\frac{1}{3}\bm{b}_2,  \label{r_t_iteration}\\
\bm{h}_t(w,k)=\lambda_2  \bm{h}_{t-1}(w,k)+\frac{1}{3}\bm{b}_1+\frac{2}{3}\bm{b}_2, \label{h_t_iteration}
\end{align}
where $\bm{b}_1=\mathrm{tr}^2(\bm{\Sigma})(2w-w^2)+\frac{2w^2}{n-1}\mathrm{tr}(\bm{\Sigma}^2)$ and $\bm{b}_2=\mathrm{tr}^2(\bm{\Sigma})\frac{w^2}{n-1}+(2w-w^2+w^2\frac{1}{n-1})\mathrm{tr}(\bm{\Sigma}^2)$.
Besides, we have $\bm{h}_t(w,k)=(1/3,2/3)\bm{v}_t(w,k)\geq 0$ for any values of $(w,k,t)$ since the elements of $\bm{v}_t(w,k)$ are non-negative.


\noindent
\textbf{Case 1.} If $w\leq 1-\sqrt{\frac{1}{1+\frac{2}{m-1}}}$, we have $\lambda_2=(1-w)^2 \frac{m+1}{m-1}\geq 1$. This implies that
\begin{align*}
\bm{h}_t(w,k)=\lambda_2  \bm{h}_{t-1}(w,k)+\frac{1}{3}\bm{b}_1+\frac{2}{3}\bm{b}_2\geq \bm{h}_{1}(w,k)+\left(\frac{1}{3}\bm{b}_1+\frac{2}{3}\bm{b}_2\right)(t-1),
\end{align*}
which implies $\bm{h}_t(w,k) \to \infty$ as $t \to \infty$. 

Then under this circumstance, we know $\left\|
\begin{pmatrix}
    \bm{r}_t(w, k) \\
    \bm{h}_t(w, k)
\end{pmatrix}
\right\|_2 \to \infty$ as $t \to \infty$. Similarly, we have
\begin{align*}
  \|\bm{v}_t(w,k)\|_2=\left\|\bm{C}\begin{pmatrix}
    \bm{r}_t(w,k) \\ \bm{h}_t(w,k)
\end{pmatrix}\right\|_2\geq \sigma_{\min}(\bm{C})\left\| \begin{pmatrix}
    \bm{r}_t(w,k) \\ \bm{h}_t(w,k)
\end{pmatrix}\right\|_2 \to \infty,
\end{align*}
where $\sigma_{\min}(\bm{C})$ is the minimum singular value of $\bm{C}$, which is greater than 0 since $\bm{C}$ is invertible.

By Lemma \ref{lemma_tr_cal}, 
\begin{align*}
\mathbb{E}\bigl[\mathrm{tr}(\bm{\Sigma}_t^2(w,k))\bigr]&=\left[\frac{(w - 1)^2}{m-1}, \frac{m(w - 1)^2}{m - 1}\right]\bm{v}_{t-1}(w,k)+ \mathrm{tr}^2(\bm{\Sigma})\frac{w^2}{n-1}+(2w-w^2+w^2\frac{1}{n-1})\mathrm{tr}(\bm{\Sigma}^2) \\ &\geq \frac{(w-1)^2}{m-1} \| \bm{v}_{t-1}(w,k)\|_2 \to \infty \mbox{ as } t\rightarrow \infty,
\end{align*}
where last inequality follows from the fact that that $a+b \geq \sqrt{a^2+b^2} $ for two non negative values $a$ and $b$. Then under this circumstance,
\begin{align*}
&\lim_{t\to \infty}\mathrm{Err}(\bm{\Sigma}_{t}(w,k)) \\
=& \lim_{t \to \infty}\mathbb{E}\big[\left\|\bm{\Sigma}_{t}(w,k) - \bm{\Sigma}\right \|^2_F\big]=\lim_{t \to \infty}\mathbb{E}\left[\mathrm{tr}(\bm{\Sigma}_{t}(w,k) - \bm{\Sigma})(\bm{\Sigma}_{t}(w,k) - \bm{\Sigma})\right] \\ 
=&\lim_{t \to \infty}\mathbb{E} \left\{ 
\mathrm{tr}(\bm{\Sigma}^2_t(w,k))-\mathrm{tr}(\bm{\Sigma}_{t}(w,k)\bm{\Sigma})-\mathrm{tr}(\bm{\Sigma}\bm{\Sigma}_t(w,k))+\bm{\Sigma}^2
\right\} 
\\  =&\lim_{t \to \infty}\mathbb{E}\left[\mathrm{tr}(\bm{\Sigma}^2_t(w,k))\right]-\lim_{t \to \infty}\mathrm{tr}(\mathbb{E}\left[\bm{\Sigma}_{t}(w,k)\right]\bm{\Sigma})-\lim_{t\to\infty}\mathrm{tr}(\bm{\Sigma}\mathbb{E}\left[
\bm{\Sigma}_t(w,k)
\right]) +\mathrm{tr}(\bm{\Sigma}^2)
\\  =&\lim_{t \to \infty}\mathbb{E}\left[\mathrm{tr}(\bm{\Sigma}^2_t(w,k))\right]-\mathrm{tr}(\bm{\Sigma}^2)=\infty,
\end{align*}
where the third line follows from the fact that $\mathbb{E}\left[\bm{\Sigma}_t(w,k) \right]=\bm{\Sigma}$.

\noindent
\textbf{Case 2.} If $w>1-\sqrt{\frac{1}{1+\frac{2}{m-1}}}$, we have
\begin{align*}
\lambda_1 = (1-w)^2 \frac{m-2}{m-1}<1 \mbox{ and }
\lambda_2=(1-w)^2 \frac{m+1}{m-1}< 1.
\end{align*}
Then using the similar arguments as in \eqref{explicti_error_mu}, we can show that $(\bm{r}_t(w,k),\bm{h}_t(w,k))$ converge. This then implies the convergence of $\bm{v}_t(w,k)=(\bm{r}_t(w,k),\bm{h}_t(w,k))^T$. It then follows that $\mathbb{E}\left[\mathrm{tr}(\bm{\Sigma}^2_t(w,k))\right]$ converges as $t$ approaches infinity. Therefore, we have
\begin{align*}
\lim_{t\to \infty}\mathrm{Err}(\bm{\Sigma}_{t}(w,k))=\lim_{t \to \infty}\mathbb{E}\left[\mathrm{tr}(\bm{\Sigma}^2_t(w,k))\right]-\mathrm{tr}(\bm{\Sigma}^2) \ \textit{exists and} <\infty.
\end{align*}
This completes the proof. \qed \\

