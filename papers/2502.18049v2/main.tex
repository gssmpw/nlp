\documentclass[12pt]{article}
\usepackage{amsmath,amsthm,amssymb,bm,mathrsfs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[ruled,linesnumbered]{algorithm2e}
\let\listofalgorithms\relax
\usepackage{caption,enumerate,listings}
\usepackage{setspace}
\usepackage[OT1]{fontenc}
\usepackage{natbib}
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=blue]{hyperref}
\usepackage{pifont}
\usepackage[margin=1in]{geometry}
\usepackage[protrusion=false,expansion=true]{microtype}
\usepackage[title]{appendix}
\usepackage{bbm}
\usepackage{subcaption}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{multirow}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\minimize}{\mathop{\mathrm{minimize}}}
\newcommand{\sign}{\mathop{\mathrm{sign}}}
\newcommand{\wx}{\widetilde{\bm{x}}}
\newcommand{\wy}{\widetilde{y}}
\newcommand{\we}{\widetilde{\eta}}
\newcommand{\abs}[1]{\left\lvert #1\right\rvert}
\newtheorem{proposition}{{\bf Proposition}}
%\numberwithin{proposition}{section}
\newtheorem{lemma}{{\bf Lemma}}
%\numberwithin{lemma}{section}
\newtheorem{corollary}{{\bf Corollary}}
%\numberwithin{corollary}{section}
\newtheorem{theorem}{{\bf Theorem}}
%\numberwithin{theorem}{section}
\newtheorem{assumption}{{\bf Assumption}}
%\numberwithin{assumption}{section}
\newtheorem{definition}{{\bf Definition}}
%\numberwithin{definition}{section}
\newtheorem{remark}{{\bf Remark}}
%\numberwithin{remark}{section}
\newtheorem{example}{{\bf Example}}
\usepackage{setspace}
\usepackage{booktabs}
\theoremstyle{plain}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{titlesec}
\titleformat{\section}[block]{\normalfont\large\bfseries}{\thesection}{1em}{}


\floatname{algorithm}{Fresh Data Augmentation Case}

\title{\Large \bf Golden Ratio Weighting Prevents Model Collapse}
\date{ }
\author{
Hengzhi He$^\dag$ \quad
Shirong Xu$^\dag$\footnote{Address for correspondence: Shirong Xu, Department of Statistics and Data Science
     University of California, Los Angeles, 580 Portola Plaza, Los Angeles, CA 90095. Email: shirongxu56@ucla.edu
} \quad 
Guang Cheng$^\dag$ \\
 $^\dag$Department of Statistics and Data Science\\
     University of California, Los Angeles
}



\begin{document}

\maketitle


\begin{abstract}
Recent studies identified an intriguing phenomenon in recursive generative model training known as model collapse, where models trained on data generated by previous models exhibit severe performance degradation. Addressing this issue and developing more effective training strategies have become central challenges in generative model research. In this paper, we investigate this phenomenon theoretically within a novel framework, where generative models are iteratively trained on a combination of newly collected real data and synthetic data from the previous training step. To develop an optimal training strategy for integrating real and synthetic data, we evaluate the performance of a weighted training scheme in various scenarios, including Gaussian distribution estimation and linear regression. We theoretically characterize the impact of the mixing proportion and weighting scheme of synthetic data on the final model's performance. Our key finding is that, across different settings, the optimal weighting scheme under different proportions of synthetic data asymptotically follows a unified expression, revealing a fundamental trade-off between leveraging synthetic data and generative model performance. Notably, in some cases, the optimal weight assigned to real data corresponds to the reciprocal of the golden ratio. Finally, we validate our theoretical results on extensive simulated datasets and a real tabular dataset.
\end{abstract}
\textbf{Keywords:} Optimal Mixing, Generative Model, Golden Ratio, Model Collapse, Synthetic Data




%\setstretch{1}
\newpage
\setstretch{1.7}
\section{Introduction}


In recent years, synthetic data have been widely used to train generative models, especially in the domain of large language models \citep{meng2022generating,shumailov2023curse,dohmatob2024tale} and computer vision \citep{man2022review,mishra2022task2sim}. This trend is mainly motivated by the limited availability of data to train larger models due to neural scaling laws \citep{bansal2022systematic,villalobos2024will}. However, several critical issues arise regarding the utility of synthetic data for training purposes. For instance, because synthetic data do not perfectly align with the real data distribution, models trained on synthetic data are prone to have degraded performance. This phenomenon has been extensively validated in the literature \citep{xu2023utility,wong2016understanding,dohmatob2024strong}.


A recent study by \citet{shumailov2024ai} reveals that AI models may collapse when trained on recursively generated data. The recursive training framework is illustrated in Figure \ref{fig:ModelCollapse_1}, where generative models are trained on synthetic data produced by earlier generative models. Over successive training iterations, these models gradually lose information about the real data distribution, a phenomenon known as \textit{model collapse}. For example, \citet{shumailov2024ai} use Gaussian distribution estimation to demonstrate how repeated estimation over iterations causes the estimated covariance matrix to almost surely collapse to zero, while the sample mean diverges. Similar results in a linear regression context are validated by \citet{dohmatob2024model}. In this paper, we refer to this framework (illustrated in Figure~\ref{fig:ModelCollapse_1}) as the \textit{fully synthetic} framework.
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.26]{MC_Frame_1.png}
    \caption{The general framework of model collapse phenomenon during recursive training \citep{shumailov2024ai}. In this framework, $\mathcal{D}_0$ denotes the initial real dataset and $\widetilde{\mathcal{D}}_{t}$ denotes the synthetic dataset generated by the $(t-1)$-th generative model, which is then used to train the $t$-th generative model.}
    \label{fig:ModelCollapse_1}
\end{figure}

Subsequently, \cite{gerstgrasser2024model} and \cite{kazdan2024collapse} explore model collapse in a new scenario, illustrated in Figure~\ref{fig:ModelCollapse_2}, in which a model is trained iteratively, with each training step leveraging the original real training dataset $\mathcal{D}_0$ and all synthetic data in previous steps. Theoretically, they demonstrate that in both regression and Gaussian distribution estimation settings, the phenomenon of model collapse is avoided, showing that the expected test error remains bounded even as the number of iterations increases. In other words, model collapse is prevented by integrating all the data from previous steps into the training process. Furthermore, they demonstrate through experiments that data accumulation effectively prevents model collapse across various models, including LLMs and diffusion models. Later, such a data accumulation scheme is proved to be effective when the underlying generative models belong to exponential families \citep{dey2024universality}. In this paper, we refer to this framework (illustrated in Figure~\ref{fig:ModelCollapse_2}) as the \textit{synthetic accumulation} framework.

%\cite{dey2024universality} extends the theoretical results of \cite{gerstgrasser2024model} and \cite{kazdan2024collapse}, showing that model collapse can be avoided by \color{blue}data accumulation\color{black}\ during training when the underlying generative models belong to exponential families. In this paper, we refer to this framework (illustrated in Figure~\ref{fig:ModelCollapse_2}) as the \textit{synthetic accumulation} framework.

%For instance, Model 2 is trained on a dataset that augments $\mathcal{D}_0$ with synthetic datasets $\widetilde{\mathcal{D}}_{1}$ and $\widetilde{\mathcal{D}}_{2}$ generated by Model 0 and Model 1, respectively. 


%argue that the fully synthetic setting depicted in Figure~\ref{fig:ModelCollapse_1}—where the training data for each iteration is entirely generated by the previous iteration—is unrealistic. Instead, they consider 
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.26]{MC_Frame_2.png}
    \caption{The general framework for mitigating the model collapse phenomenon during recursive training involves accumulating data \citep{gerstgrasser2024model, kazdan2024collapse}. In this framework, the training dataset is progressively expanded at each step $t$ by incorporating synthetic data from last generative model.}
    \label{fig:ModelCollapse_2}
\end{figure}

%Their theoretical findings are empirically validated on image datasets using different diffusion models, demonstrating that the final generative model tends to exhibit greater stability when trained with a smaller proportion of synthetic data. 

A key result of the synthetic accumulation framework is that it incorporates {\em excessive} synthetic data during training, leading to poorer performance in the final model compared to one trained purely on real data \citep{kazdan2024collapse}. In contrast, \citet{bertrand2024on} consider a data augmentation approach that utilizes only synthetic data generated by the most recent generative model, as shown in Figure~\ref{fig:ModelCollapse_3}. In this approach, the $t$-th generative model is trained on a combination of a fixed real dataset $\mathcal{D}_0$ and the synthetic dataset $\widetilde{\mathcal{D}}_{t-1}$ produced by the $(t-1)$-th generative model. \citet{bertrand2024on} show that this framework (Figure \ref{fig:ModelCollapse_3}) can effectively avoid the model collapse phenomenon if the proportion of synthetic data is small. In this paper, we refer to this framework (illustrated in Figure~\ref{fig:ModelCollapse_3}) as the \textit{synthetic augmentation} framework.
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.26]{MC_Frame_3.png}
    \caption{The general framework for avoiding the model collapse phenomenon during recursive training by augmenting only real data \citep{bertrand2024on}. In this framework, at the $t$-th training step, the generative model is trained based on $\widetilde{\mathcal{D}}_{t} \cup \mathcal{D}_0$.
}
    \label{fig:ModelCollapse_3}
\end{figure}



The frameworks illustrated in Figures \ref{fig:ModelCollapse_1}–\ref{fig:ModelCollapse_3} all highlight a frustrating result: synthetic data fail to improve the performance of the final generative model, also empirically validated in this paper. Recently, \citet{alemohammad2024selfconsuming} investigate the phenomenon of model collapse in a \textit{new scenario}, where real data is continuously collected at each step for model estimation, as shown in Figure \ref{Fig:Framework}. In this work, the authors present empirical evidence demonstrating that a modest amount of synthetic data can enhance the performance of the final generative model. However, model performance declines when the amount of synthetic data exceeds a critical threshold. While \citet{alemohammad2024selfconsuming} provide empirical evidence for the effectiveness of synthetic data within the framework illustrated in Figure \ref{Fig:Framework}, the underlying theoretical foundation remains unexplored. In this paper, we refer to this framework (illustrated in Figure~\ref{fig:ModelCollapse_3}) as the \textit{fresh data augmentation} framework.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.26]{MC_Frame_4.png}
    \caption{The general framework considered in this paper involves mixing newly collected real and synthetic data to address model collapse. In this framework, at the $t$-th training step, a newly collected dataset $\mathcal{D}_t$ is augmented with $\widetilde{\mathcal{D}}_{t}$ to train the $t$-th generative model.}
    \label{Fig:Framework}
\end{figure}


In this paper, we theoretically investigate the phenomenon of model collapse within the new framework considered by \citet{alemohammad2024selfconsuming}. There are two primary practical reasons for studying the framework depicted in Figure \ref{Fig:Framework}. The first relates to privacy concerns. For instance, the California Consumer Privacy Act (CCPA)\footnote{\url{https://oag.ca.gov/privacy/ccpa}} grants users the right to request that platforms retain their personal information only for a limited time. Consequently, in practical applications, the real dataset $\mathcal{D}_0$ may not always be available for future training as shown in the frameworks of Figures \ref{fig:ModelCollapse_1}-\ref{fig:ModelCollapse_3}. Second, in a realistic setting, real data is continuously collected, enabling the training of new models using both synthetic data generated by prior models and newly collected real data. Despite its importance, the practice of training a model on a combination of newly collected real data and synthetic data from previous models remains theoretically underexplored. This naturally raises several critical questions:
\begin{itemize}[leftmargin=1cm] 
\item[\textbf{Q1}]: Can mixing newly collected real data with synthetic data during training help prevent model collapse?
\item[\textbf{Q2}]: What is the optimal ratio of real to synthetic data when training a new generative model with data from the previous model?
\item[\textbf{Q3}]: To what extent can synthetic data enhance estimation efficiency?
\end{itemize}

To address these questions, we first introduce our definition of model collapse in parametric generative modeling (see Definition~\ref{Def:MC}):  statistical inconsistency of parameter estimation caused by the recursive training process. To mitigate this issue, we propose a weighted training scheme for generative models that integrates newly collected real data with varying amounts of synthetic data. We explore several scenarios involving recursive parameter estimation for Gaussian distributions and linear regression, building on previous research \citep{gerstgrasser2024model,kazdan2024collapse,bertrand2024on}. Our analysis demonstrates that the fresh data augmentation framework not only effectively prevents model collapse but also improves estimation efficiency. A comparison of different frameworks is provided in Table \ref{tab:simple_table}.
\begin{table}[h!]
\centering
\caption{The utility of synthetic data in different frameworks illustrated in Figures \ref{fig:ModelCollapse_1}-\ref{Fig:Framework}}
\begin{tabular}{l|c|c}
\toprule
\textbf{Framework} & \textbf{Model collapse?} & \textbf{Synthetic data enhance estimation?} \\ \hline
Fully Synthetic     & \checkmark      & \ding{55}      \\ \hline
Synthetic Accumulation      & \ding{55}     & \ding{55}   \\ \hline
Synthetic Augmentation    & \ding{55}     & \ding{55}     \\ 
\hline
Fresh Data Augmentation    & \ding{55}     & \checkmark  \\
\bottomrule
\end{tabular}
\label{tab:simple_table}
\end{table}
In this paper, we summarize our contributions to the study of the model collapse phenomenon under the framework illustrated in Figure \ref{Fig:Framework} as follows:
 \begin{itemize}[leftmargin=0.2cm] 

     \item \textbf{Optimal Weight of Real and Synthetic Data:} We explicate the optimal weights for real and synthetic data when training a new generative model using different amount of synthetic data across different regimes. Our findings reveal that while different regimes exhibit distinct model behaviors, the optimal weight asymptotically converges to a unified expression across all regimes. Interestingly, we find that regardless of the amount of real and synthetic data available in each round, simply mixing them without proper weighting is always suboptimal. In certain scenarios, we identify critical thresholds for the proportion of real and synthetic data in the mixture. As long as the weight of newly collected real data exceeds this threshold, model collapse can be effectively prevented.

    \item \textbf{Surprising Connection to the Golden Ratio:} When the amounts of real and synthetic data remain the same in each training iteration, we make a striking discovery: the optimal weight of real data is \textit{the reciprocal of the golden ratio}.


    \item \textbf{Threshold for Beneficial Synthetic Data:} We demonstrate that as long as the weight of synthetic data in each iteration remains below a certain threshold, incorporating synthetic data enhances estimation efficiency in our fresh data augmentation setting compared to using only real data, highlighting the effectiveness of incorporating synthetic data for training.
\end{itemize}

The rest of this paper is organized as follows. In Section \ref{Sec:Back_Pre}, we introduce the necessary notations and provide background on model collapse within a parametric setting. In Section \ref{Sec:GauEsti}, we explore the model collapse phenomenon within the fresh data augmentation framework, focusing on Gaussian estimation, and develop optimal training schemes to avoid model collapse and enhance estimation efficiency. In Section \ref{sec:linear}, we extend the analysis in Section \ref{Sec:GauEsti} to linear regression. In Section \ref{Sec:Diss}, we discuss the selection of the optimal ratio between real and synthetic data from a practical perspective, taking into account the cost of data acquisition. In Section \ref{Sec:Exp}, we conduct extensive simulations and a real application to validate our theoretical findings. In Section \ref{Sec:Discs}, we explore potential future extensions of this work. Additional contents and all proofs are deferred to the Appendix.

\section{Preliminaries}
\label{Sec:Back_Pre}

In this section, we first introduce the necessary notations in Section \ref{Sec:Not}. To formalize our analysis, we then provide a mathematical definition of model collapse within the framework of parametric model estimation in Section \ref{Sec:MCD}. While model collapse was originally observed in the training of AI models \citep{shumailov2024ai}, a formal mathematical definition within the broader context of estimating parametric generative models remains absent.
\subsection{Notations}
\label{Sec:Not}
In this paper, we use bold notation to represent multivariate quantities and unbolded notation for univariate quantities. For any positive integer \( M \), we define \( [M] \) as the set $[M] = \{1,2,\ldots,M\}$. For example, $\bm{x} \in \mathbb{R}^p$ represents a $p$-dimensional vector, while $x$ represents a real value. For a vector $\bm{x} \in \mathbb{R}^p$, we denote its %$l_1$-norm and 
$l_2$-norm as  
%$\Vert \bm{x} \Vert_1=\sum_{i=1}^p |x_i|$ and 
$\Vert \bm{x} \Vert_2=\big(\sum_{i=1}^p |x_i|^2\big)^{1/2}$. 
%For two given sequences $\{A_n\}_{n \in \mathbb{N}}$ and $\{ B_n\}_{n\in \mathbb{N}}$, we write $A_n \gtrsim B_n$ if there exists a constant $C>0$ such that $A_n \geq C B_n$ for all $n \in \mathbb{N}$. 
%Additionally, we write $A_n \asymp B_n$ if both $A_n \gtrsim B_n$ and $A_n \lesssim B_n$ hold. 
For a continuous random variable $X$, we let $P_X(x)$ denote its probability density function at $x$ and $\mathbb{P}_{X}$ denote the associated probability measure. We use $\mathbb{E}_{X}$ to denote the expectation taken with respect to the randomness of $X$. For a sequence of random variables $\{X_n\}_{n\in \mathbb{N}}$, we write $X_n =o_p(1)$ to indicate that $X_n$ converges to zero in probability. For a matrix $\bm{A}=(A_{ij})_{i,j \in [n]}$, we define its trace as $\text{tr}(\bm{A})=\sum_{i=1}^n A_{ii}$. Finally, we use $\mathcal{F}_t$ to represent the $\sigma$-algebra generated by all events occurring up to the $t$-th training step.



\subsection{Model Collapse in Parametric Generative Models}
\label{Sec:MCD}
In this section, we present a general definition of \textit{model collapse} for parametric generative models within the framework of recursive model estimation as illustrated in Figure \ref{fig:UF}. Consider a family of generative models parameterized by $\bm{\theta} \in \bm{\Theta}$, denoted as $\{\mathbb{P}(\cdot \mid \bm{\theta}) \mid \bm{\theta} \in \bm{\Theta}\}$. Let $\bm{\theta}^\star$ be the ground truth parameter. The goal is to estimate $\bm{\theta}^\star$ after successive iterations.


\begin{figure}[h]
    \centering
    \includegraphics[scale=0.25]{UniFrame.png}
    \caption{The general framework of recursive model estimation.}
    \label{fig:UF}
\end{figure}




In the framework of Figure \ref{fig:UF}, an estimation scheme $\mathcal{M}: \mathcal{X}^{\mathbb{N}} \to \bm{\Theta}$ is applied consistently at each estimation step, i.e., $\widehat{\bm{\theta}}_t = \mathcal{M}(D_t)$ for $t \geq 0$, where $\mathcal{X}$ represents the data support, $\bm{\Theta}$ denotes the parameter space, and $D_t = \{\bm{x}_{t,i}\}_{i=1}^{N_t}$ is the dataset used at step $t$. Aligning with the frameworks in Figure \ref{fig:ModelCollapse_1}-\ref{Fig:Framework}, we define $D_0 = \{\bm{x}_{0,i}\}_{i=1}^{N_0}$ as a real dataset drawn from $\mathbb{P}(\bm{x} \mid \bm{\theta}^\star)$. For $t \geq 1$, $D_t$ may consist of either real data or synthetic data generated by previous generative models $\{\mathbb{P}(\bm{x} \mid \widehat{\bm{\theta}}_{i}) : 0 \leq i \leq t-1\}$. Without loss of generality, we assume that the sample sizes $\{N_t\}_{t=0}^{\infty}$ follow the pattern $N_t = N_0 \times c_t$ for $t \geq 1$. In other words, given a fixed initial real sample size $N_0$, the sequence $\{c_t\}_{t=1}^{\infty}$ determines the trend of the training datasets throughout the training process.  





We denote by $\mathbb{P}_{t}(\bm{x})$ the underlying distribution of $D_t = \{\bm{x}_{t,i}\}_{i=1}^{N_t}$. Since $D_t$ consists of both real data and synthetic data generated by previous models, $\mathbb{P}_{t}(\bm{x})$ is a mixture of the real data distribution $\mathbb{P}(\bm{x} \mid \bm{\theta}^\star)$ and the synthetic data distributions $\{\mathbb{P}(\bm{x} \mid \widehat{\bm{\theta}}_{i}) : 0 \leq i \leq t-1\}$ prior to the $t$-th step. In other words, the dependence of $\mathbb{P}_{t}(\bm{x})$ on $\mathbb{P}(\bm{x} \mid \bm{\theta}^\star)$ and $\{\mathbb{P}(\bm{x} \mid \widehat{\bm{\theta}}_{i}) : 0 \leq i \leq t-1\}$ is determined by the strategy used to integrate synthetic and real data for training. For example, in a fully synthetic framework, the distribution simplifies to $\mathbb{P}_{t}(\bm{x}) = \mathbb{P}(\bm{x} \mid \widehat{\bm{\theta}}_{t-1})$. In contrast, under a synthetic data augmentation framework, it follows  
$\mathbb{P}_{t}(\bm{x}) = \frac{1}{2} \mathbb{P}(\bm{x} \mid \widehat{\bm{\theta}}_{t-1}) + \frac{1}{2} \mathbb{P}(\bm{x} \mid \bm{\theta}^\star)$.  



Notably, the framework in Figure \ref{fig:UF} generalizes those in Figures \ref{fig:ModelCollapse_1}-\ref{Fig:Framework} as special cases. Particularly, the framework in Figure \ref{fig:UF} simplifies to the \textit{fully-synthetic case} in Figure \ref{fig:ModelCollapse_1} when $N_i = N_j$ for all $i, j \geq 0$, and $\{\bm{x}_{t,i}\}_{i=1}^{N_t}$ is entirely sampled from $\mathbb{P}(\bm{x} \mid\widehat{\bm{\theta}}_{t-1})$, i.e., $\mathbb{P}_{t}(\bm{x})=\mathbb{P}(\bm{x} \mid \widehat{\bm{\theta}}_{t-1})$ for $t \geq 1$.






The key challenge in Figure \ref{fig:UF} is to assess the performance of the final generative model $\mathbb{P}(\cdot \mid \widehat{\bm{\theta}}_T)$ when $T$ is large, which can be quantified by the estimation error $\mathbb{E} \|\widehat{\bm{\theta}}_T - \bm{\theta}^\star\|_2^2$. Here, the expectation is taken with respect to the randomness of all previous training datasets. If $N_0$ is fixed, the performance of $\widehat{\bm{\theta}}_T$ in estimating $\bm{\theta}^\star$ as $T \to \infty$ is influenced by three key factors: (1) the estimation scheme $\mathcal{M}(\cdot)$, (2) the pattern of sample sizes $\{c_t\}_{t=1}^\infty$, and (3) the underlying distribution $\mathbb{P}_t$ at each step. In other words, once $(\mathcal{M}, \{c_t\}_{t=1}^{\infty}, \{\mathbb{P}_t\}_{t=1}^{\infty})$ is specified, the training strategy for recursive estimation is uniquely determined.  





In statistical learning theory, if $(\mathcal{M}, \{c_t\}_{t=1}^{\infty}, \{\mathbb{P}_t\}_{t=1}^{\infty})$ is fixed with an appropriate estimation scheme $\mathcal{M}$, the error $\mathbb{E} \|\widehat{\bm{\theta}}_T - \bm{\theta}^\star\|_2^2$ for a fixed $T$ should converge to zero as $N_0$ increases to infinity. However, existing literature \citep{shumailov2024ai,kazdan2024collapse} shows that, as $T$ increases, $\widehat{\bm \theta}_T$ gradually loses information about $\mathbb{P}(\cdot|\bm{\theta}^\star)$ and finally fails to converge to $\bm{\theta}^\star$ regardless of the size of data during recursive training process. Therefore, preventing model collapse should focus on designing appropriate data integration and training schemes to ensure that the estimation error \( \mathbb{E} \|\widehat{\bm{\theta}}_T - \bm{\theta}^\star\|_2^2 \) remains stable as \( T \) grows to infinity and decreases as more data is used for training. Based on this rationale, we formally define model collapse in the context of parametric generative models in Definition \ref{Def:MC}.





\begin{definition}[Model Collapse in Parametric Generative Models] 
\label{Def:MC}
For a recursive training scheme $(\mathcal{M}, \{c_t\}_{t=1}^{\infty}, \{\mathbb{P}_t\}_{t=1}^{\infty})$, we say that model collapse occurs if there exists a constant \(\delta > 0\) such that $\lim \limits_{T \to \infty} \mathbb{E} \|\widehat{\bm{\theta}}_T - \bm{\theta}^\star\|_2^2 \geq \delta$ for any value of $N_0$.
\end{definition}


In Definition \ref{Def:MC}, model collapse refers to the phenomenon where the estimator \(\widehat{\bm{\theta}}\) progressively loses information about the true data distribution \(\mathbb{P}(\cdot|\bm{\theta}^\star)\) as \(T\) increases, causing \(\widehat{\bm{\theta}}\) to fail to converge to \(\bm{\theta}^\star\) regardless of the samples used throughout the training process. We characterize this failure by the estimator’s inconsistency, independent of the sample sizes used at each estimation step. Specifically, the limiting error \(\lim_{T\rightarrow \infty} \mathbb{E} \|\widehat{\bm{\theta}}_T - \bm{\theta}^\star\|_2^2\) remains bounded away from zero, no matter how large the sample sizes are at any step. 




\section{Gaussian Distribution Estimation}
\label{Sec:GauEsti}

In this section, we investigate the phenomenon of model collapse within the fresh data augmentation framework, using recursive Gaussian distribution estimation as a case study. We analyze the univariate case in this section, which is subsequently extended to the multivariate case in Appendix \ref{SubSec:Gaus2}.

Within the fresh data augmentation framework, the process begins with a real dataset \(\mathcal{D}_{0} = \{x_{0,i}\}_{i=1}^n\) sampled from \(N(\mu, \sigma^2)\). At the \(t\)-th training step for \(t \geq 1\), a newly collected real dataset \(\mathcal{D}_{t} = \{x_{t,i}\}_{i=1}^n\) is drawn from \(N(\mu, \sigma^2)\), and a synthetic dataset \(\widetilde{\mathcal{D}}_{t} = \{\widetilde{x}_{t,i}\}_{i=1}^m\) is generated based on the \((t-1)\)-th Gaussian distribution. As a result, the training dataset at step \(t\) is \(D_t = \mathcal{D}_t \cup \widetilde{\mathcal{D}}_t\). As illustrated in Figure~\ref{Fig:Framework}, $\mathcal{D}_{t}$ and $\widetilde{\mathcal{D}}_{t}$ are used to estimate the parameters of a univariate Gaussian distribution at the $t$-th step. Although the most straightforward approach might be to simply combine the two datasets for training,
we show in Remark \ref{advantage_than_direct_mix} that at least in the case of mean estimation, this method is strictly suboptimal. Instead, we adopt a weighted estimation scheme as follows:
\begin{align}
\label{Eqn:MeanUpdate}
  \text{Mean Estimation: }   &   \mu_{t}(w,k) =\argmin_{\mu \in \mathbb{R}}
    \frac{w}{n} \sum_{i=1}^n (\mu-x_{t,i})^2 + \frac{1-w}{m}\sum_{i=1}^m (\mu-\widetilde{x}_{t,i})^2, \\
             \label{Eqn:VarUpdate}   \text{Variance Estimation: } &    \sigma_t^2(w,k) = \argmin_{\sigma^2 \in \mathbb{R}}
    w (\sigma^2-\widehat{S}_t)^2+(1-w)(\sigma^2-\widetilde{S}_t)^2,
\end{align}
where $k = \frac{n}{m}$ represents the ratio of real data size to synthetic data size, and $\widehat{S}_t = \frac{1}{n-1} \sum_{i=1}^n (x_{t,i} - \widehat{\mu}_t)^2$ and $\widetilde{S}_t = \frac{1}{m-1} \sum_{i=1}^m (\widetilde{x}_{t,i} - \widetilde{\mu}_t)^2$ denote the sample variances of the real and synthetic data at the $t$-th step, respectively. Here, \( \widehat{\mu}_t = \frac{1}{n} \sum_{i=1}^n x_{t,i} \) and \( \widetilde{\mu}_t = \frac{1}{m} \sum_{i=1}^m \widetilde{x}_{t,i} \) are the sample means of the real and synthetic data, respectively. 

The overall recursive estimation process for a univariate Gaussian distribution, obtained by solving \eqref{Eqn:MeanUpdate}, is summarized in \textbf{Fresh Data Augmentation Case} \ref{alg:univariable_gaussian_estimation}. In this process, $w$ is a weighting parameter that controls the emphasis placed on the real data in the optimization process. We assume that $w$ is fixed throughout the entire recursive training process. Specifically, $w = 0$ corresponds to the fully synthetic framework depicted in Figure~\ref{fig:ModelCollapse_1}, while $w = 1$ refers to the scenario that only utilizes real data at the $t$-th step. Additionally, $k$ is a parameter controlling the size of synthetic data used for estimation. In practice, $k$ can be extremely small, indicating that a large amount of synthetic data is used for training. From a statistical viewpoint, assuming $n$ is fixed, the resulting estimators in the case of $k=0$ are optimal in terms of estimation efficiency. However, the choice of $k$ usually depends on the tradeoff between the cost of computational resources and estimation improvement in some real generative modeling scenarios.








\begin{algorithm}[h]
\caption{- Univariate Gaussian Estimation}
\label{alg:univariable_gaussian_estimation}
\begin{algorithmic}
\STATE \textbf{Initialization:} Compute $\mu_0 = \frac{1}{n}\sum_{i=1}^n x_{0,i}$ and $\sigma_0^2 = \frac{1}{n-1}\sum_{i=1}^n (x_{0,i}-\mu_0)^2$ based on $\mathcal{D}_0=\{x_{0,i}\}_{i=1}^n$ from $N(\mu,\sigma^2)$;
\FOR{$t = 1,2,\ldots,T$}
    \STATE \textbf{1. Data Collection}: Sample $\widetilde{\mathcal{D}}_{t}=\{\widetilde{x}_{t,i}\}_{i=1}^m$ from $N(\mu_{t-1},\sigma_{t-1}^2)$ and collect a new real dataset $\mathcal{D}_{t}=\{x_{t,i}\}_{i=1}^n$ from $N(\mu,\sigma^2)$;
    \STATE \textbf{2. Mean Estimation}: Compute the mean of the Gaussian distribution at the $t$-th training step:
    \begin{equation*}
    %\label{Eqn:MeanUpdate}
    \mu_{t}(w,k) =  w \widehat{\mu}_t + (1-w)\widetilde{\mu}_{t},
    \end{equation*}
    where $\widehat{\mu}_t=\frac{1}{n}\sum_{i=1}^n x_{t,i}$ and $\widetilde{\mu}_{t}=\frac{1}{m}\sum_{i=1}^m \widetilde{x}_{t,i}$.
    \STATE \textbf{3. Variance Estimation}: Compute the variance of the Gaussian distribution at the $t$-th training step:
            \begin{equation*}
            %\label{Eqn:VarUpdate}
    \sigma_t^2(w,k) = w\widehat{S}_t+(1-w)\widetilde{S}_t,
    \end{equation*}
      where $\widehat{S}_t =  \frac{1}{n-1}\sum_{i=1}^n (x_{t,i}-\widehat{\mu}_t)^2$ and $\widetilde{S}_t =  \frac{1}{m-1}\sum_{i=1}^m (\widetilde{x}_{t,i}-\widetilde{\mu}_t)^2$.
\ENDFOR
\RETURN $\mu_T$ and $\sigma_T^2$.
\end{algorithmic}
\end{algorithm}


To analyze the behavior of $\mu_{t}(w,k)$ and $\sigma_t^2(w,k)$ as $t \to \infty$ with $k \triangleq n/m$ being the mixing ratio, we define the following metrics: 
\begin{align*}
    \text{Err}(\mu_{t}(w,k)) = \mathbb{E}\big[(\mu_{t}(w,k) - \mu)^2\big] \mbox{ and }
    \text{Err}(\sigma_{t}^2(w,k)) = \mathbb{E}\big[(\sigma_{t}^2(w,k) - \sigma^2)^2\big],
\end{align*}  
where $\text{Err}(\mu_{t}(w,k))$ and $\text{Err}(\sigma_{t}^2(w,k))$ represent the expected squared errors of $\mu_{t}(w,k)$ and $\sigma_{t}^2(w,k)$, measuring the estimation errors for $\mu_{t}(w,k)$ and $\sigma_t^2(w,k)$, respectively. It is important to note that these expectations are taken with respect to the randomness inherent in all real and synthetic datasets generated during the first $t$ steps. Essentially, $\{\mu_{t}(w,k)\}_{t=1}^T$ and $\{\sigma_{t}^2(w,k)\}_{t=1}^T$ can be regarded as two time series, where the randomness of $(t-1)$-th element is contained within that of $t$-th element. A key problem is analyzing the behavior of $\mu_t(w,k)$ and $\sigma_t^2(w,k)$ as $t$ approaches infinity. Therefore, we consider the following metrics for the case where \( t = \infty \):
\begin{align*}
    \mathrm{Err}(\mu_{\infty}(w,k)) \triangleq \lim_{t \rightarrow \infty} \mathrm{Err} \left( \mu_{t}(w,k) \right) \mbox{ and } \mathrm{Err}(\sigma_{\infty}^2(w,k)) \triangleq \lim_{t \rightarrow \infty} \mathrm{Err} \left( \sigma_t^2(w,k) \right).
\end{align*}

In the following, we investigate the behavior of $\mathrm{Err}(\mu_{\infty}(w,k))$ and $\mathrm{Err}(\sigma_{\infty}^2(w,k))$ in a finite-sample setting, aiming to shed light on the impact of $w$ and $k$.




\begin{theorem}
    \label{Thm1:Uni}
For any $w \in (0,1]$ and $n,m \geq 1$ with $k=\frac{n}{m}$.
It holds that 
$$\mathrm{Err}(\mu_{\infty}(w,k)) = \lim_{t \rightarrow \infty} \mathrm{Err}(\mu_{t}(w,k) ) 
=\frac{C(w,k)\sigma^2}{n},$$
where $C(w,k) =\frac{w^2+(1-w)^2k}{2w-w^2}$. Using the optimal weighting parameter $w^\star = \frac{\sqrt{k^{2} + 4k} - k}{2}$ with $w^\star =\argmin\limits_{w \in (0,1]}C(w,k)$, the estimation improvement for estimating $\mu$ is given by 
\begin{align*}
    \frac{\mathrm{Err}(\mu_{\infty}(w^\star,k))}{\mathrm{Err}(\mu_{\infty}(1,k))} = w^\star=\frac{\sqrt{k^2+4k}-k}{2}<1.
\end{align*}
Particularly, when $k=1$ (n=m), we have $\mathrm{Err}(\mu_{\infty}(w,1))<\mathrm{Err}(\mu_{\infty}(1,1))$ for any $w \in (\frac{1}{3}, 1)$.
\end{theorem}


In Theorem \ref{Thm1:Uni}, we derive explicit expressions for \(\mathrm{Err}(\mu_{\infty}(w,k))\) under different values of the weighting parameter \(w\) and the mixing ratio $k$. Specifically, we show that the finite sample \(l_2\)-error of \(\mu_{t}(w,k)\) in estimating \(\mu\) is given by \(\frac{C(w, k)\sigma^2}{n}\) when $t$ approaches infinity, where \(C(w, k)\) depends on \(w\) and \(k \triangleq \frac{n}{m}\), the ratio of real to synthetic data generated in each round. Furthermore, we compare the estimation errors of our proposed estimator with optimal weight $w^\star$ and the estimator based solely on newly emerging real samples in each round. Interestingly, we find that the ratio of their estimation errors is exactly $w^\star$, which is always less than 1, as shown in Figure \ref{fig:OW}. Clearly, as more synthetic data is integrated during the estimation process (i.e., smaller values of $k$), the estimation accuracy improves. In particular, as $k$ approaches 0 (representing an infinite number of synthetic samples), $\mathrm{Err}(\mu_{\infty}(w^\star,k))$ converges to zero, implying that the proposed estimator converges to $\mu$ after recursive estimation. Additionally, we demonstrate the existence of a phase transition phenomenon in $\mathrm{Err}(\mu_{\infty}(w,k))$ depending on the choice of $w$. In particular, when $n = m$ (see Left Panel of Figure \ref{fig:OW}), $\mathrm{Err}(\mu_{\infty}(w,k))$ is smaller than the estimation error $\mathrm{Err}(\mu_{\infty}(1,1))$ that relies solely on newly collected real data, provided that $w$ is chosen to be greater than $1/3$. Conversely, when $w$ is smaller than $1/3$, $\mu_{\infty}(w,1)$ performs worse than $\mu_{\infty}(1,1)$ in estimating $\mu$.












\begin{figure}[h]
    \centering
        \begin{subfigure}[b]{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Temp_1.png}
        \caption{Phase Transition of $\mathrm{Err}(\mu_{\infty}(w,1))$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Optimal_W.png}
        \caption{The Optimal Weighting $w^\star$ for Varying $k$}
    \end{subfigure}
\caption{The left panel illustrates the quantitative behavior of $\mathrm{Err}(\mu_{\infty}(w,k))$ with $n = m = 4$ across three phases: (1) the model collapse phase, (2) the phase with no estimation improvement, and (3) the phase with estimation improvement. The right panel demonstrates the quantitative relationship between the optimal weight $w^\star$ and the mixing ratio $k$ is presented. Particularly, when $n = m$ (i.e., $k = 1$), $w^\star$ is the reciprocal of the golden ratio.}

    \label{fig:OW}
\end{figure}


\begin{corollary}
    \label{Th1_Coro1}
     $\mathrm{Err}(\mu_{\infty}(1,k))$ exhibits the following special cases:
    \begin{itemize}
        \item[(1)] \textbf{Fully Synthetic - Model Collapse}: $\lim_{w \rightarrow 0^{+}} \mathrm{Err}(\mu_{\infty}(w,k)) = \infty$ \text{ for any } $k>0$,
        \item[(2)] \textbf{Real Data Only}: $\mathrm{Err}(\mu_{\infty}(1,k)) = \frac{\sigma^2}{n}$ \text{ for any } $k>0$,
        \item[(3)] \textbf{Optimal Weight}: $\mathrm{Err}(\mu_{\infty}(w^{\star},k)) = \frac{w^\star \sigma^2}{n}$ \text{ for any } $k>0$,
    \end{itemize}
where $w^\star = \frac{\sqrt{k^{2} + 4k} - k}{2}$ is the optimal weight as defined in Theorem \ref{Thm1:Uni}.
\end{corollary}


In Corollary \ref{Th1_Coro1}, we present several special cases of Theorem \ref{Thm1:Uni}. Notably, when $w = 0$ (corresponding to the fully synthetic case), the estimation error $\mathrm{Err}(\mu_{\infty}(w, k))$ diverges to infinity. This finding aligns with existing literature on model collapse in Gaussian mean estimation \citep{kazdan2024collapse, shumailov2024ai}. Interestingly, when $m = n$, the minimal estimation error occurs at $w = w^\star = \frac{-1 + \sqrt{5}}{2}$, the reciprocal of the golden ratio $\frac{1 + \sqrt{5}}{2}$. Using this optimal weight, the estimation error $\mathrm{Err}(\mu_{\infty}(w, k))$ improves compared to using only real data, as evidenced by $\frac{\sqrt{k^2 + 4k} - k}{2} < 1$ for any $k > 0$. Specifically, when $k = 1$ ($n = m$), the improvement is given by $\frac{\mathrm{Err}(\mu_{\infty}(w^\star, 1))}{\mathrm{Err}(\mu_{\infty}(1, 1))} = w^\star = \frac{\sqrt{5} - 1}{2}$.

In Theorem \ref{Thm1:Uni}, we consider a special case where each synthetic sample is assigned the same weight internally, so does each real sample. However, in a more general setting, where each synthetic sample $\widetilde{ x}_{t,i}$ has an associated weight $\widetilde{w}_i$ and each real sample ${x}_{t,i}$ has a weight $w_i$, we can extend the proof technique of Theorem \ref{Thm1:Uni} to show that the proposed weighted combination in Theorem \ref{Thm1:Uni} remains the Best Linear Unbiased Estimator (BLUE), as stated in the following proposition.

\begin{proposition}
\label{corollary_global_optimum_uni}
Consider a more general update rule for mean estimation given by:\begin{equation}
    \label{Eqn:MULMeanUpdate_General}
    {\mu}_{t}(\{w_i\}_{i=1}^n,\{\widetilde{w}_j\}_{j=1}^m,k) = \frac{1}{n}\sum_{i=1}^n w_i {x}_{t,i}+\frac{1}{m}\sum_{j=1}^m \widetilde{w}_j{x}_{t,i},
\end{equation}
where \( w_i \) and \( \widetilde{w}_j \) are the weights assigned to the \( i \)-th real and the $j$-th synthetic data points, respectively, satisfying that $\sum_{i=1}^n w_i/n + \sum_{j=1}^m \widetilde{w}_j/m = 1$. Consider the limiting estimation error defined as
$$\mathrm{Err}({\mu}_{\infty}(\{w_i\}_{i=1}^n,\{\widetilde{w}_j\}_{j=1}^m,k)) = \lim_{t\rightarrow \infty}\mathbb{E}\big[\Vert {\mu}_{t}(\{w_i\}_{i=1}^n,\{\widetilde{w}_j\}_{j=1}^m,k) - {\mu}\Vert_2^2\big].$$ It holds true that $\mathrm{Err}(\bm{\mu}_{\infty}(\{w_i\}_{i=1}^n,\{\widetilde{w}_j\}_{j=1}^m,k))$ is minimized by $w_i = w^{\star}$ for $i \in [n]$ and $\widetilde{w}_j = 1 - w^{\star}$ for $j \in [m]$, where \( w^{\star}= \frac{\sqrt{k^{2} + 4k} - k}{2} \) as defined in Theorem \ref{Thm1:Uni}.

\end{proposition}

\begin{remark}[The Advantage of Weighted Training Over Direct Mixing]
\label{advantage_than_direct_mix}
The primary reason for adopting a weighted training scheme over direct mixing is that simply combining the data without explicit weighting can be strictly suboptimal. For example, consider the mean estimation step. If we directly merge the datasets \(\mathcal{D}_t\) and \(\widetilde{\mathcal{D}}_t\) and minimize the empirical loss on them:
\begin{align*}
{\mu}_{t}(w,k) = \argmin_{{\mu} \in \mathbb{R}^p}  \frac{1}{n+m} \left\{ \sum_{i=1}^n \| {\mu} - {x}_{t,i} \|_2^2 +\sum_{i=1}^m \|{\mu} - \widetilde{{x}}_{t,i} \|_2^2 \right\},
\end{align*}
then the resulting estimate can be written as ${\mu}_{t}(w_0, k) =  w_0 \widehat{{\mu}}_t + (1-w_0)\widetilde{{\mu}}_{t}$, where the implicit weight \(w_0\) is given by:
\begin{equation*}
w_0 = \frac{n}{n+m} =  \frac{2k}{k+2+k} = \frac{2k}{\sqrt{k^2+4k+4} +k }
<w^{\star} = \frac{\sqrt{k^2+4k}-k}{2} = \frac{2k}{ \sqrt{k^2+4k }+k  }.
\end{equation*}
A detailed derivation is provided in Appendix.
\ref{explain_advantage}.
This inequality shows that directly mixing the data results in a strictly suboptimal weight for mean estimation for any value of $k$. The proposed weighted training scheme achieves a better balance between real and synthetic data.
\end{remark}

\begin{theorem}
  \label{Thm2:Uni_var}
For any $w \in (0, 1]$, $n, m \geq 2$, and $k > 0$, it holds that  
\[
\mathrm{Err}(\sigma_{\infty}^2(w, k)) = 
\begin{cases} 
\infty, & 0 < w \leq 1 - \sqrt{\frac{m-1}{m+1}}, \\[10pt]
C_1(w, m, n)\sigma^4, & 1 - \sqrt{\frac{m-1}{m+1}} < w \leq 1,
\end{cases}
\]
where $C_1(w, m, n)=\frac{w^2\left(1 + \frac{2}{n-1}\right) + 2w(1-w)}{1 - (1-w)^2\left(1+\frac{2}{m-1}\right)}-1$. The optimal weight $w^\star=\argmin_{w \in (0,1]}C_1(w, m, n)$ is given as
$$
w^\star = \frac{\sqrt{n^{2} + \left(4m - 2\right) n - 4m + 5} - n + 3}{2m + 2}.
$$
Particularly, when $m,n\rightarrow \infty$ with $\frac{n}{m}=k$, $w^\star \rightarrow \frac{\sqrt{k^2+4k}-k}{2}$. The estimation improvement exists in the following cases: 
\begin{align*}
&\frac{\mathrm{Err}\big(\sigma_{\infty}^2(w,k)\big)}{\mathrm{Err}(\sigma_{\infty}^2(1,k))}<1, \text{ for any } w 
\in \left(\frac{n+1}{n+2m-1},1\right), \\
    \lim_{n,m\rightarrow \infty,\frac{n}{m}\rightarrow k}&
    \frac{\mathrm{Err}\big(\sigma_{\infty}^2(w^{\star},k)\big)}{\mathrm{Err}(\sigma_{\infty}^2(1,k))}=
    \frac{\sqrt{k^{2} + 4k} - k}{2}<1, \text{ for any } k \in \mathbb{R}_{\geq 0},  \\
        \lim_{m\rightarrow \infty}&
    \frac{\mathrm{Err}\big(\sigma_{\infty}^2(w,k)\big)}{\mathrm{Err}(\sigma_{\infty}^2(1,k))}=
    \frac{w^2}{2w-w^2}<1, \text{ for any } w \in \left(1 - \sqrt{\frac{m-1}{m+1}},1\right]
    .
\end{align*}
\end{theorem}

In Theorem \ref{Thm2:Uni_var}, we derive the explicit form of $\mathrm{Err}(\sigma_{\infty}^2(w,k))$ for different values of the weighting parameter $w$ and the mixing ratio $k$. Notably, $\mathrm{Err}(\sigma_{\infty}^2(w,k))$ exhibits distinct behavior compared to $\mathrm{Err}(\mu_{\infty}(w,k))$. Specifically, our proposed estimator $\sigma_t^2(w,k)$ undergoes a phase transition in estimating $\sigma^2$. When $w \leq 1 - \sqrt{\frac{m-1}{m+1}}$, the estimation error $\mathrm{Err}(\sigma_{\infty}^2(w,k))$ diverges to infinity, indicating that the estimator is inconsistent, regardless of the number of real and synthetic samples used. This result suggests that model collapse persists when $w$ is too small, demonstrating that it can occur even when real data is incorporated into training. In contrast, when $w > 1 - \sqrt{\frac{m-1}{m+1}}$, the estimation error remains finite and depends on the real and synthetic sample sizes. To achieve better estimation performance than using only newly emerging real data, $w$ should be set higher than $\frac{n+1}{n+2m-1}$. Furthermore, as $n$ and $m$ approach infinity with $n/m = k$, the optimal weight $w^\star$ aligns with that for $\mathrm{Err}(\mu_{\infty}(w,k))$, and $\mathrm{Err}(\sigma_{\infty}^2(w,k))$ achieves the same estimation improvement as $\mathrm{Err}(\mu_{\infty}(w,k))$ compared to the estimator based solely on newly emerging real samples.
  



\begin{figure}[h!]
    \centering
    % First row
    \begin{subfigure}[b]{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figure_sigma.png}
        \caption{Phase Transition of $\mathrm{Err}(\sigma_{\infty}^2(w,k))$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figure_Bound.png}
        \caption{Phase Vanishing of $\mathrm{Err}(\sigma_{\infty}^2(w,k))$}
    \end{subfigure}
    \caption{The left panel illustrates the quantitative behavior of $\mathrm{Err}(\sigma_{\infty}^2(w,k))$ with $n = m = 4$ across three phases: (1) the model collapse phase, (2) the phase with no estimation improvement, and (3) the phase with estimation improvement. The right panel shows that the first two phases vanish as more synthetic samples are used for estimation with $n = 4$.}
    \label{fig:Sigma}
\end{figure}


An intriguing finding from Theorem \ref{Thm2:Uni_var} is the presence of a phase transition in the estimation error $\mathrm{Err}(\sigma_{\infty}^2(w,k))$ as shown in Figure \ref{fig:Sigma}. Specifically, for finite values of $m$ and $n$, the error $\mathrm{Err}(\sigma_{\infty}^2(w,k))$ diverges to infinity whenever $w \leq 1-\sqrt{\frac{m-1}{m+1}}$, a phenomenon we refer to as the \textbf{model collapse phase}. In contrast, when $w$ falls within the interval $\left(1-\sqrt{\frac{m-1}{m+1}}, \frac{n+1}{2m+n-1} \right]$, the estimation error remains finite but is still worse than $\mathrm{Err}(\sigma_{\infty}^2(1,k))$. This indicates that incorporating synthetic data in recursive training does not outperform estimation based solely on newly emerging data. We term this region the \textbf{phase of estimation without improvement}. Finally, when $w > \frac{n+1}{2m+n-1}$, we obtain $ \frac{\mathrm{Err}(\sigma_{\infty}^2(w,k))}{\mathrm{Err}(\sigma_{\infty}^2(1,k))} < 1 $, signifying the existence of estimation improvement. In this regime, optimal estimation efficiency is achieved at $w^\star$, as defined in Theorem \ref{Thm2:Uni_var}. 

Building on the analysis above, we now examine an asymptotic regime where $n = m \to \infty$. In this setting, the threshold simplifies to $w = \frac{1}{3}$, providing a clear criterion for assessing the impact of synthetic data incorporation. Specifically, synthetic data remains beneficial for estimation as long as its weight does not exceed $1-\frac{1}{3}=\frac{2}{3}$. This conclusion also holds for the mean estimation. Specifically, by Theorem~\ref{Thm1:Uni}, when $n=m$, the error ratio satisfies
\[
\frac{\mathrm{Err}(\mu_{\infty}(w^\star, k))}{\mathrm{Err}(\mu_{\infty}(1, k))}
= \frac{w^2 + (1 - w)^2}{2w - w^2} < 1 
\quad \text{if and only if} \quad 
\tfrac{1}{3} < w < 1.
\]
Hence, when \(w\) exceeds \(\tfrac{1}{3}\), or equivalently, the synthetic-data weight 
is below \(\tfrac{2}{3}\), the combined estimator outperforms the estimator based solely on real data.


 \begin{corollary}
\label{Thm2:Coro2}
    $\mathrm{Err}(\sigma_{\infty}^2(w, k))$ exhibits the following special cases
    \begin{itemize}
        \item[(1)] \textbf{Fully Synthetic - Model Collapse}: $\lim_{w \rightarrow 0^{+}} \mathrm{Err}(\sigma_{\infty}^2(w,k)) =\infty$,
        \item[(2)] \textbf{Real Data Only}: $\mathrm{Err}(\sigma_{\infty}^2(1,k)) =\frac{2\sigma^4}{n-1}$,
        \item[(3)] \textbf{Asymptotic Optimal Weight}: $\mathrm{Err}\big(\sigma_{\infty}^2(w^{\star},k)\big)=\frac{2}{n-1}\left( \frac{\sqrt{k^2+4k}-k}{2}  \right)\sigma^4+o(n^{-1})$ as $m\to \infty, n\to \infty, \frac{n}{m}\to k$, for any $k\in \mathbb{R}_{\geq 0}$,
    \end{itemize}
where $w^\star = \frac{\sqrt{n^{2} + \left(4m - 2\right) n - 4m + 5} - n + 3}{2m + 2}$ is the optimal weight as defined in Theorem \ref{Thm2:Uni_var}, which converges to $\frac{\sqrt{5}-1}{2}$, the reciprocal of golden ratio, as $n=m\to\infty$.
\end{corollary}

In Corollary \ref{Thm2:Coro2}, we present several special cases of Theorem \ref{Thm2:Uni_var}. As $w$ approaches zero, the estimation error $\mathrm{Err}(\sigma_{\infty}^2(w,k))$ diverges to infinity. However, as noted earlier, model collapse does not occur solely in the fully synthetic case. For instance, when $m = 3$, the estimation error $\mathrm{Err}(\sigma_{\infty}^2(w,k))$ always diverges for any $w \leq 1 - \sqrt{2}/2$. If estimation relies exclusively on newly emerging real data, the error is given by $\mathrm{Err}(\sigma_{\infty}^2(1,k)) = \frac{2\sigma^4}{n-1}$. By using the optimal weight $w^\star$ defined in Theorem \ref{Thm2:Uni_var}, the proposed estimator $\mathrm{Err}\big(\sigma_{\infty}^2(w^{\star},k)\big)$ achieves a faster convergence rate compared to estimation based solely on real data, demonstrating the utility of synthetic data.  

The results of univariate Gaussian estimation are extended to the multivariate setting in Appendix \ref{SubSec:Gaus2}. Compared to the univariate case, new complexities arise due to the nontrivial analysis of correlations among the variables. In particular, while the results for mean estimation remain similar to those in the univariate case, variance estimation becomes substantially more complicated. Nevertheless, it is surprising that the multivariate case exhibits the same phase transition and, asymptotically, the same optimal weight as the univariate case.
%\begin{remark}
%By applying the result from \cite{vervaat1979stochastic} in the proof of Theorem \ref{Thm2:Uni_var}, we observe that $\sigma_t^2$ will in fact converge to a stationary distribution, denoted as $\sigma_{\infty}^2$. The estimation error of this stationary distribution can be expressed as

%\[
%\mathrm{Err}(\sigma_{\infty}^2(w,k)) = \lim_{t \to \infty} \mathrm{Err}%(\sigma_t^2(w,k)).
%\]
%\end{remark}

\section{Linear Regression Model}
\label{sec:linear}
In this section, we explore the phenomenon of model collapse under the fresh data augmentation framework in linear regression. Specifically, we follow the setting in \cite{gerstgrasser2024model}, where $\bm{X} = (\bm{X}_1, \ldots, \bm{X}_n)^T \in \mathbb{R}^{n\times p}$ is fixed at every step. In the initial stage, a real dataset $\mathcal{D}_0 = \{(\bm{X}_i, Y_{0,i})\}_{i=1}^n$ is generated as follows:  
\begin{align*}
    Y_{0,i} = \bm{X}_i^T\bm{\beta} + \epsilon_{0,i}, \quad \text{for } i=1,\ldots,n,
\end{align*}
where $\bm{\beta}$ is the true parameter, and $\epsilon_{0,i} \sim N(0,\sigma^2)$. Subsequently, at the $t$-th step, a synthetic dataset $\mathcal{D}_{t} = \{(\bm{X}_i, \widetilde{Y}_{t,i})\}_{i=1}^n$ and a new real dataset $\mathcal{D}_t = \{(\bm{X}_i, Y_{t,i})\}_{i=1}^n$ emerge and are generated as follows:  
\begin{align*}
    \text{Newly Emerging Real Data at the $t$-th Step:} \quad & Y_{t,i} = \bm{X}_i^T\bm{\beta} + \epsilon_{t,i}, \quad \text{for } i=1,\ldots,n, \\
    \text{Synthetic Data at the $t$-th Step:} \quad & \widetilde{Y}_{t,i} = \bm{X}_i^T \widehat{\bm{\beta}}_t(w)+ \widetilde{\epsilon}_{t,i}, \quad \text{for } i=1,\ldots,n,
\end{align*}
where $\widehat{\bm{\beta}}(w)$ represents the fitted regression coefficient in the $t$-th generative model and $w$ represents the weighting parameter. 


The estimation scheme for $\widehat{\bm{\beta}}_t(w)$ follows a similar approach as before, and the overall procedure is summarized in \textbf{Fresh Data Augmentation Framework}~\ref{alg:linear_regression_fixed}. Specifically, we propose minimizing a weighted squared error at each step of the recursive estimation procedure. Specifically, at the $t$-th step, $\widehat{\bm{\beta}}_t(w)$ is obtained by solving the following optimization task:
    \begin{equation}
        \label{Eqn:BetaUpdate}
        \widehat{\bm{\beta}}_t(w) = \arg\min_{\bm{\beta}} \left\{ w \|\bm{Y}_t - \bm{X} \bm{\beta}\|_2^2 + (1-w) \|\widetilde{\bm{Y}}_t - \bm{X} \bm{\beta}\|_2^2 \right\},
    \end{equation}
    where $\bm{Y}_t = (Y_{t,1},\ldots,Y_{t,n})^T$ and $\widetilde{\bm{Y}}_t = (\widetilde{Y}_{t,1},\ldots,\widetilde{Y}_{t,n})^T$. The weighting parameter $w$ determines the emphasis placed on the real data emerging at $t$-th step, controlling the balance between real and synthetic data in the optimization process.


\begin{algorithm}
\caption{- Linear Regression with Fixed Design}
\label{alg:linear_regression_fixed}
\begin{algorithmic}
\STATE \textbf{Initialization:} Compute the initial estimate of \( \widehat{\bm{\beta}}_0(w) \) based on the dataset \( \mathcal{D}_0 = \{(\bm{X}_i, Y_{0,i})\}_{i=1}^n \), where $Y_{0,i}=\bm{X}_i \bm{\beta}+\epsilon_{0,i}$ and $\epsilon_{0,i}\sim N(0,\sigma^2)$:
\begin{equation}
    \widehat{\bm{\beta}}_0(w) = (\bm{X}^\top \bm{X})^{-1} \bm{X}^\top \bm{Y}_0.
\end{equation}

\FOR{$t = 1,2,\ldots,T$}
    \STATE \textbf{1. Data Collection:} Collect two datasets:
    \begin{itemize}
        \item A new synthetic dataset \( \widetilde{\mathcal{D}}_t = \{(\bm{X}_i, \widetilde{Y}_{t,i})\}_{i=1}^n \), where \( \widetilde{Y}_{t,i} = \bm{X}_i^T \widehat{\bm{\beta}}_{t-1}(w) + \widetilde{\epsilon}_{t,i} \) and \( \widetilde{\epsilon}_{t,i} \sim N(0, \sigma^2) \).
        \item A new real dataset \( \mathcal{D}_t = \{(\bm{X}_i, Y_{t,i})\}_{i=1}^n \), where \( Y_{t,i} = \bm{X}_i^T \bm{\beta} + \epsilon_{t,i} \) and \( \epsilon_{t,i} \sim N(0, \sigma^2) \).
    \end{itemize}
    \STATE \textbf{2. Parameter Estimation:} Compute the regression coefficients at the \( t \)-th step by the following closed-form solution:
    \begin{equation*}
        \widehat{\bm{\beta}}_t(w) = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\left(w\bm{Y}_t+(1-w)\widetilde{\bm{Y}}_t\right).
    \end{equation*}
    
\ENDFOR
\RETURN \( \widehat{\bm{\beta}}_T(w) \).
\end{algorithmic}
\end{algorithm}



Similarly, we examine the testing performance of the resulting estimator \(\widehat{\bm{\beta}}_T(w)\). Following the framework of \cite{gerstgrasser2024model}, we assess the quality of \(\widehat{\bm{\beta}}_T(w)\) using the metric:  
\begin{align*}  
    \mathrm{Err}(\widehat{\bm{\beta}}_T(w)) = \mathbb{E}\left[ \Vert \widehat{\bm{\beta}}_T(w)-\bm{\beta} \Vert_{2}^2\right],  
\end{align*}  
where the expectation is taken over the design matrix \(\bm{X}\) and the noise terms \(\bm{\epsilon}_{t} = (\epsilon_{t,1},\ldots,\epsilon_{t,n})\) and \(\widetilde{\bm{\epsilon}}_{t} = (\widetilde{\epsilon}_{t,1},\ldots,\widetilde{\epsilon}_{t,n})\) for $t \geq 1$.


Similar to the previous results, we are interested in the behavior of $\mathrm{Err}_T(w)$ as $T$ approaches infinity. Under the normality assumption of $\bm{\epsilon}_t$ and $\widetilde{\bm{\epsilon}}_t$, the expression for $\mathrm{Err}(\widehat{\bm{\beta}}_T(w))$ can be readily derived using the results from Theorem \ref{Thm1:Mul} in Appendix \ref{SubSec:Gaus2}. A key distinction in the \textbf{Fresh Data Augmentation Framework}~\ref{alg:linear_regression_fixed}, compared to the previous results, is the assumption that $n = m$. This assumption stems from the fact that $\bm{X}$ remains fixed throughout the recursive training process, with only the labels being synthetic.









\begin{theorem}
   \label{linear_regression_fixed_design}
For any $n \geq 2$, it holds that
\begin{align}
\label{LG}
\mathrm{Err}(\widehat{\bm{\beta}}_{\infty}(w)) \triangleq
\lim_{T \to \infty}\mathrm{Err}(\widehat{\bm{\beta}}_T(w))=\sigma^2 \frac{2w^2-2w+1}{2w-w^2}\mathbb{E}_{\bm{X}}\left[\mathrm{tr}\left\{(\bm{X^T X})^{-1}\right\}\right],
\end{align}
and the optimal $w^{\star}=\frac{\sqrt{5}-1}{2}$, which is the reciprocal of the golden ratio. The estimation error improves compared to using only real data, i.e. $
    \mathrm{Err}(\widehat{\bm{\beta}}_{\infty}(w)) < \mathrm{Err}(\widehat{\bm{\beta}}_{\infty}(1))$, if and only if $w \in (\frac{1}{3},1)$.
\end{theorem}


In Theorem \ref{linear_regression_fixed_design}, we analyze the limiting estimation error $\mathrm{Err}(\widehat{\bm{\beta}}_{\infty}(w))$ in the context of linear regression. Specifically, we derive the exact form of $\mathrm{Err}(\widehat{\bm{\beta}}_{\infty}(w))$ in (\ref{LG}), which allows us to identify several interesting scenarios. Notably, to achieves the fastest convergence rate, the optimal weight $w^\star$ for our proposed estimator corresponds to the reciprocal of the golden ratio. Additionally, our result also aligns with existing results regarding the model collapse in the context of linear regression \citep{gerstgrasser2024model}. Specifically, by letting $w$ approach zero, the estimation error $\mathrm{Err}(\widehat{\bm{\beta}}_{\infty}(w))$ diverges to infinity, leading to model collapse.

\section{Discussion on the Optimal Ratio of Real to Synthetic Data}
\label{Sec:Diss}
In this section, we discuss the selection of the mixing proportion between real and synthetic data from a practical perspective. Through Theorems \ref{Thm1:Uni}-\ref{linear_regression_fixed_design}, we observe that, within the fresh data augmentation framework, different generative modeling scenarios exhibit varying estimation errors in the finite sample setting. However, their asymptotic estimation errors conform to a unified mathematical form:
\begin{align}
\label{orignal_loss_key}
\text{Estimation Error} \propto \frac{C(w,k)}{n} + o\left(\frac{1}{n}\right),
\end{align}
where \( C(w,k) = \frac{w^2 + (1 - w)^2 k}{2w - w^2} \) and \( k = n/m \). For a fixed \( k \), \( C(w, k) \) is minimized at \( w^\star = \frac{\sqrt{k^2 + 4k} - k}{2} \), and the corresponding minimum value is
\begin{align}
\label{optimal_loss_key}
C(w^\star, k) = \frac{\sqrt{k^2 + 4k} - k}{2} = w^\star.
\end{align}
Interestingly, the minimizer is also a fixed point of the function.

\textbf{Synthetic Data Improves Estimation.} The result in (\ref{optimal_loss_key}) offers practical insights for selecting the weight $w$ and the proportion $k$. If the amount of real data $n$ is fixed, we observe that choosing the optimal weight $w^\star$ results in a smaller value of  
$C(w^{}, k)$. Note that $C(w^{\star},k)= \frac{\sqrt{k^2+4k}-k}{2}$ will decrease as $k$ decreases.
This implies that, if the number of real data points per iteration is fixed, increasing the number of synthetic data points improves performance during iterative training. In fact, through further verification, we can prove that this result holds not only when $T \to \infty$, but also for any finite number of iterations, and even for an arbitrary fixed weight $w$. Intuitively, a larger amount of synthetic data ensures that the information inherited from previous iterations remains cleaner, leading to better overall performance.


\textbf{Cost of Using Synthetic Data.} Even though utilizing more synthetic data can always improve estimation efficiency in our setting, in practice, obtaining a large amount of synthetic data may incur some cost. Therefore, one must consider the tradeoff between cost and estimation error. Consider a specific scenario in which the total budget per iteration constrains the combined amount of real and synthetic data. Specifically, we assume that acquiring one unit of synthetic data costs a fraction $r$ (where $0 < r < 1$) of the cost of collecting one unit of real data. This assumption implies that obtaining a synthetic data point is cheaper than obtaining a real data point for training. Under this budgeted setting, the total data per iteration must satisfy $n + r m = S$, where \( n \) and \( m \) denote the size of real and synthetic data, respectively, and \( S \) represents the fixed total budget allocated for data collection in each iteration. Then we can rewrite the estimation error in \eqref{orignal_loss_key} as follows
\begin{align}
\label{Res:FInal}
\text{Estimation Error} \propto \underbrace{\frac{1}{S}\frac{k+r}{k} \frac{\sqrt{k^2+4k}-k}{2}}_{\text{Dominant Term}}+o(S^{-1}).
\end{align}
Here, it can be proved that the dominant term is minimized at $k=\frac{r^{\frac{3}{2}} + r}{1-r}$. A detailed derivation is provided in Appendix \ref{additional_discussion}.




The result in (\ref{Res:FInal}) indicates that, given a fixed budget $S$ each step and the relative cost $r$ of synthetic data compared to real data, the optimal data ratio of real to synthetic data should be $k = \frac{r^{\frac{3}{2}} + r}{1 - r}$. From this result, we can derive several special cases that provide insights into how synthetic data should be used in a cost-effective way:
\begin{itemize}
    \item When \( r = \left(\frac{\sqrt{5} - 1}{2}\right)^2 \) (the squared inverse golden ratio): The optimal proportion simplifies to \( k = \frac{n}{m} = 1 \), meaning real and synthetic data should be used in equal proportions. Interestingly, at this point, the optimal weight \( w^\star \) aligns with the reciprocal of the golden ratio, as discussed earlier.
    
    \item When \( r = 0 \) (i.e., synthetic data is free): The optimal strategy is to use as much synthetic data as possible, as increasing its quantity improves performance without additional cost. This suggests that in cases where generating synthetic data incurs no expense, it should be leveraged extensively to enhance model training.
    
    \item As \( r \to 1 \) (i.e., the cost of synthetic data approaches that of real data): The optimal ratio \( k \to \infty \), meaning synthetic data becomes less cost-effective. In this case, relying on synthetic data is no longer advantageous in terms of budgeting, and real data should be prioritized.
\end{itemize}

\section{Experiments}
\label{Sec:Exp}
In this section, we present a series of simulations and a real-world application to validate our theoretical findings. Our objectives are three-fold. First, we demonstrate the existence of an optimal weight for weighted training, particularly when real and synthetic data are present in equal amounts. Second, we illustrate that the fresh data augmentation framework improves estimation efficiency compared to other frameworks. Third, we highlight the phase transition phenomenon in the usefulness of synthetic data, which depends on variations in synthetic sample size and weighting.










\subsection{Simulations}

\noindent
\textbf{Scenario 1 (Model Collapse in Fully Synthetic Case):} In this scenario, we aim to validate our results on Gaussian distribution estimation and linear regression. For the experimental setup, we consider the following configurations: (1) In Gaussian estimation, the parameters are set as \(\bm{\mu} = (0, 0, 0, 0)\) and \(\bm{\Sigma} = \bm{I}_4\). (2) In linear regression, we use \(\bm{\beta} = (1, 1, 1, 1)\), with noise modeled by a standard normal distribution. The sample size is fixed at 100 for all experiments, with each experiment involving \(1,000\) recursive estimation steps. To estimate the values of \(\mathrm{Err}(\bm{\mu}_{t}(w,k))\), \(\mathrm{Err}(\bm{\Sigma}_{t}(w,k))\), and\(\mathrm{Err}(\widehat{\bm{\beta}}_t(w))\) for \(t \in \{1,2,\ldots,10^3\}\), each experimental setup is replicated \(1,000\) times.






\begin{figure}[h!]
    \centering
    % First row
    \begin{subfigure}[b]{0.322\textwidth}
        \centering
        \includegraphics[width=\textwidth]{S1_Mean.png}
        \caption{Gaussian Mean}
    \end{subfigure}
    % Second row
    \begin{subfigure}[b]{0.322\textwidth}
        \centering
        \includegraphics[width=\textwidth]{S1_Var.png}
        \caption{Gaussian Variance}
    \end{subfigure}
    \begin{subfigure}[b]{0.322\textwidth}
        \centering
        \includegraphics[width=\textwidth]{S1_Beta.png}
        \caption{Linear Regression}
    \end{subfigure}
    \caption{The averaged values of estimation errors $\mathrm{Err}(\bm{\mu}_{t}(w,k))$, $\mathrm{Err}(\bm{\Sigma}_{t}(w,k))$, and $\mathrm{Err}(\widehat{\bm{\beta}}_t(w))$ over $1,000$ replications during the first $1,000$ steps in the fully synthetic case $(w=0)$ are reported for Scenario I (Fully Synthetic Case).
    }
    \label{fig:S_FS}
\end{figure}

Figure \ref{fig:S_FS} presents the averaged estimation errors in Gaussian estimation and linear regression, estimated over $1,000$ replications for each $t \in \{1,2,\ldots,10^3\}$. For the multivariate Gaussian distribution, $\mathrm{Err}(\bm{\mu}_{t}(0,k))$ increases gradually with $t$, and the 95\% confidence intervals for $\Vert \bm{\mu}_t(0,k)-\bm{\mu}\Vert_2^2$ widen as $t$ grows. Consistently, as $t$ approaches infinity, $\mathrm{Err}(\bm{\mu}_t(w,k))$ diverges to infinity, corroborating our result in Corollary \ref{Coro:Multi_Mean}. Moreover, for covariance matrix estimation, the estimation error $\mathrm{Err}(\bm{\Sigma}_{t}(w,k))$ diverges more significantly as $t$ increases, indicating that the estimation of the true covariance matrix $\bm{\Sigma}$ becomes highly inaccurate. This result also aligns with Corollary \ref{multi_var_corollary}. For linear regression, $\mathrm{Err}(\widehat{\bm{\beta}}_t(w))$ increases linearly with $t$ when $w=0$. As expected, $\mathrm{Err}(\widehat{\bm{\beta}}_{\infty}(w))$ becomes infinite, in line with Theorem \ref{linear_regression_fixed_design}. 


\noindent
\textbf{Scenario 2 (Golden Ratio Verification):} In this scenario, we aim to validate our theoretical findings, which suggest that when $n = m$, the optimal proportion of synthetic data corresponds to the reciprocal of the golden ratio. To achieve this, we adopt the same experimental setup for simulated data generation as outlined in Scenario 1, with $w \in \{0.2 + 0.02 \times i \mid i \in [30]\}$ and $(n, T, k) = (100, 10^3, 1)$. To estimate the limiting estimation errors, we use the average of the last 100 estimation errors as an approximation. For instance, in Gaussian mean estimation, we approximate $\mathrm{Err}(\bm{\mu}_{\infty}(w,k))$ using  
\begin{align}
\label{LE_esti}
    \mathrm{Err}(\bm{\mu}_{\infty}(w,k)) \approx
\frac{1}{100} \sum_{t=900}^{1,000} \mathrm{Err}(\bm{\mu}_t(w,k)).
\end{align} 
Each case is repeated 10,000 times, and the average estimated limiting errors, along with their 95\% confidence intervals, are reported in Figure \ref{fig:S2_comparison}.  



\begin{figure}[h!]
    \centering
    % First row
    \begin{subfigure}[b]{0.322\textwidth}
        \centering
        \includegraphics[width=\textwidth]{S2loss_mean.png}
        \caption{Gaussian Mean}
    \end{subfigure}
    % Second row
    \begin{subfigure}[b]{0.322\textwidth}
        \centering
        \includegraphics[width=\textwidth]{S2loss_var.png}
        \caption{Gaussian Variance}
    \end{subfigure}
    \begin{subfigure}[b]{0.321\textwidth}
        \centering
        \includegraphics[width=\textwidth]{S2loss_beta.png}
        \caption{Linear Regression}
    \end{subfigure}


    \caption{The averaged values of estimation errors under different values of $w$ with $(n,T,k) = (100,10^3,1)$ over 10,000 replications. The minimum values of estimated $\mathrm{Err}(\bm{\mu}_\infty(w,k))$, $\mathrm{Err}(\bm{\Sigma}_\infty^2(w,k))$, and $\mathrm{Err}(\widehat{\bm{\beta}}_\infty(w))$ are highlighted}
    \label{fig:S2_comparison}
\end{figure}

The results in Figure \ref{fig:S2_comparison} align with our theoretical findings. Specifically, when \( n = m \) (\( k = 1 \)), the optimal weighting parameter that minimizes all estimation errors is approximately 0.62, as shown in Figure \ref{fig:S2_comparison}. Notably, this value is the closest, within the experimental range \( w \in \{0.2 + 0.02 \times i \mid i \in [30]\} \), to the reciprocal of the golden ratio, i.e., \( \frac{\sqrt{5}-1}{2} \).




\noindent
\textbf{Scenario 3 (Synthetic Data Helps Reduce Error using Optimal Weighting):} In this scenario, we aim to demonstrate that synthetic data can effectively reduce estimation errors using the optimal mixing proportion within the fresh data augmentation framework. To illustrate this, we consider Gaussian distribution estimation and fix \( (n,T) = (100,10^3) \), following the same experimental setup for simulated data generation as described in Scenario 1. We vary the number of synthetic data points per round, considering \( m \in \{100 \times i \mid i \in [10]\} \).

For our method, we set \( w \) as the optimal weight, given by \( w^\star = \frac{\sqrt{k^2+4k}-k}{2} \), where \( k = n/m \). For comparison, we also report the estimation errors of the data accumulation framework (Figure \ref{fig:ModelCollapse_2}) and the data augmentation framework (Figure \ref{fig:ModelCollapse_3}). As a baseline, we include the estimation error of the estimator based solely on first-round real data, \( \mathcal{D}_0 \).



\begin{figure}[h!]
    \centering
    % First row
    \begin{subfigure}[b]{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{S3_m.png}
        \caption{Gaussian Mean Estimation}
    \end{subfigure}
    % Second row
    \begin{subfigure}[b]{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{S3_v.png}
        \caption{Gaussian Variance Matrix Estimation}
    \end{subfigure}
\caption{The averaged estimation errors for estimating the Gaussian mean and covariance matrix under the synthetic accumulation, synthetic augmentation, and fresh data augmentation frameworks, with varying synthetic sample sizes \( m \in \{100, 200, \dots, 10^3\} \) and \( n = 100 \).}

    \label{fig:S3}
\end{figure}

Figure \ref{fig:S3} shows that, under the synthetic accumulation and synthetic augmentation frameworks, the estimation errors for the Gaussian mean and covariance matrix are higher than those obtained using only real data. This result suggests that synthetic data in these two frameworks degrades estimation performance and conveys little useful information for learning the true distribution. In contrast, under the fresh data augmentation framework, synthetic data effectively reduces estimation errors compared to the real data-only case, regardless of synthetic sample size. This indicates that synthetic data provides useful information for learning the true distribution. As expected, as \( m \) increases, estimation errors decrease. This result aligns with our theoretical results in Theorems \ref{Thm1:Mul} and \ref{thm3:multi_variance}.



\noindent
\textbf{Scenario 4 (Phase Transition Phenomenon Induced by Weighting):}In this scenario, we aim to demonstrate the existence of a phase transition phenomenon in the helpfulness of synthetic data, which depends on the value of $w$, as illustrated in Figure \ref{fig:Sigma}. To illustrate this, we consider the case where $n = m = 100$ and $T = 1,000$. As demonstrated by Theorems \ref{Thm1:Uni} and \ref{Thm2:Uni_var}, synthetic data contributes to reducing estimation errors for both the mean and the variance matrix in a Gaussian distribution when $w > 1/3$, which indicates that the weight assigned to real data in the weighted training scheme should be higher than $1/3$. To this end, we consider $w \in \{0.1 + 0.01 \times i \mid i \in [90]\}$ and replicate each experimental setting 1,000 times. The corresponding limiting errors are estimated via (\ref{LE_esti}).




\begin{figure}[h!]
    \centering
    % First row
    \begin{subfigure}[b]{0.323\textwidth}
        \centering
        \includegraphics[width=\textwidth]{S4_m1.png}
        \caption{Mean Estimation: $p=1$}
    \end{subfigure}
    % Second row
    \begin{subfigure}[b]{0.323\textwidth}
        \centering
        \includegraphics[width=\textwidth]{S4_v1.png}
        \caption{Variance Estimation: $p=1$}
    \end{subfigure}
        \begin{subfigure}[b]{0.323\textwidth}
        \centering
        \includegraphics[width=\textwidth]{S4_lr1.png}
        \caption{Linear Regression: $p=1$}
    \end{subfigure}
        \begin{subfigure}[b]{0.323\textwidth}
        \centering
        \includegraphics[width=\textwidth]{S4_m10.png}
        \caption{Mean Estimation: $p=10$}
    \end{subfigure}
    % Second row
    \begin{subfigure}[b]{0.323\textwidth}
        \centering
        \includegraphics[width=\textwidth]{S4_v10.png}
        \caption{Variance Estimation: $p=10$}
    \end{subfigure}
        \begin{subfigure}[b]{0.323\textwidth}
        \centering
        \includegraphics[width=\textwidth]{S4_lr10.png}
        \caption{Linear Regression: $p=10$}
    \end{subfigure}
\caption{The averaged limiting estimation errors for Gaussian mean and variance estimation in cases \( p=1 \) (Top) and \( p=10 \) (Bottom) under varying values of \( w \).}

    \label{fig:S4}
\end{figure}





\subsection{Real Application-Adult Dataset}
In this section, we use the Adult dataset \citep{adult_2} to empirically validate our theoretical findings on model collapse. The dataset consists of 48,842 instances, each with 14 features used to predict whether an individual’s annual income exceeds $50K$. These features include continuous variables such as capital-gain, age, and hours-per-week, which, although rounded to integers, maintain their continuous nature. Additionally, the dataset contains categorical variables, including education level, race, and others. 


In this application, we pursue two main objectives. First, we demonstrate that the fresh data augmentation framework achieves superior estimation efficiency compared to other framework. Second, we seek to validate our theoretical findings on the optimal choice of weighting parameters. Our analysis primarily focuses on estimating the means and variances of age and hours-per-week. 


\begin{figure}[h!]
    \centering
    % First row
    \begin{subfigure}[b]{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{R0.png}
        \caption{Logarithm of Age}
    \end{subfigure}
    % Second row
    \begin{subfigure}[b]{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{R1.png}
        \caption{Logarithm of Hours Per-week}
    \end{subfigure}
    \caption{The empirical distributions and densities of the logarithms of age (left) and weekly working hours (right).}
    \label{fig:R_con_density}
\end{figure}



We begin by applying a log transformation to both variables and visualizing their histograms along with their density estimates in Figure \ref{fig:R_con_density}. This transformation makes the distributions of both variables more closely resemble normal distributions. In the first experiment, we consider the same experimental setting as in Scenario 3, with a fixed sample size of $n = 200$ and a varying synthetic sample size each round, given by $m \in \{100 \times i \mid i \in [10]\}$. For our framework, we utilize the optimal weight, defined as $w^\star = \frac{\sqrt{k^2+4k}-k}{2}$, where $k = n/m$. In the second experiment, we aim to demonstrate that synthetic data can help reduce estimation errors under appropriate choices of weights. Specifically, we consider $n = m = 200$ and vary the weights as $w \in \{0.1 + 0.01 \times i \mid i \in [90]\}$. Each case is replicated 1,000 times and the averaged limiting estimation errors of all cases are reported in Figure \ref{fig:R_con}.






\begin{figure}[h!]
    \centering
    % First row
    \begin{subfigure}[b]{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{R2_M.png}
        \caption{Varying $m$: Mean Estimation}
    \end{subfigure}
    % Second row
    \begin{subfigure}[b]{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{R2_V.png}
        \caption{Varying $m$: Variance Estimation}
    \end{subfigure}
        \begin{subfigure}[b]{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{R1_m.png}
        \caption{Varying $w$: Mean Estimation}
    \end{subfigure}
    % Second row
    \begin{subfigure}[b]{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{R1_v.png}
        \caption{Varying $m$: Variance Estimation}
    \end{subfigure}
    \caption{The averaged limiting estimation errors over 1,000 replications for comparing different frameworks (Top) and varying weight \( w \) under the proposed framework (Bottom). }
    \label{fig:R_con}
\end{figure}


The experimental results presented in Figure \ref{fig:R_con} convey several key messages. First, compared to other frameworks, the fresh data augmentation framework highlights the utility of synthetic data in reducing estimation errors. Notably, as the size of the synthetic dataset increases, the estimation of both the mean and variance matrix improves beyond what is achievable using real data alone. Second, in mean estimation, we observe that when the real and synthetic datasets have equal sample sizes in each round, the optimal weight for the mean estimator aligns with the reciporal of the golden ratio. However, this phenomenon does not extend to variance estimation. The underlying discrepancy primarily to model misspecification. In other words, the real data is not normally distributed. In our theoretical results, we assume a normal distribution for the generative model, which provides an unbiased estimate of the second moment of the true distribution. This property leads to the emergence of the golden ratio in mean estimation. However, the generative model fails to accurately capture the fourth moment of the real data, preventing a similar result for variance estimation. The key message is that to achieve a golden-ratio outcome for a general generative model, the model must be able to approximate the real distribution. Since this condition is not met in variance estimation when applied to real data analysis, the golden ratio does not emerge as the optimal weight.

\section{Discussion and Future Work}
\label{Sec:Discs}
In this paper, we investigate the phenomenon of model collapse within a data augmentation framework, where each training round incorporates both newly collected real data and synthetic data generated by the model from the previous iteration. Our theoretical analysis considers a fixed mixing proportion and weighting scheme throughout the recursive training process, examining how these factors influence the final model’s performance. Notably, in the setting where the ratio of synthetic to real data is 1:1, we uncover a striking connection to the golden ratio: in the large-sample regime, the optimal training weight for real data is given by the reciprocal of the golden ratio. These findings underscore a fundamental trade-off between leveraging synthetic data for efficiency gains and maintaining model performance across multiple iterations.

Despite these insights, several open questions remain. A key assumption in our analysis is that the weights assigned to real and synthetic data remain fixed throughout training. A natural extension is to explore an adaptive weighting scheme, where these weights evolve dynamically at each iteration. Investigating such adaptive strategies could provide deeper insights into optimal training dynamics and further clarify the role of synthetic data in long-term learning stability. Another critical direction is the statistical properties of our estimation method. In this paper, we establish that our approach yields the Best Linear Unbiased Estimator (BLUE) under the assumption that the estimation scheme remains fixed in each round. However, an important open question is whether it remains the best unbiased estimator in a more general sense. Conventional techniques, such as the Cramér-Rao Lower Bound, do not directly apply to iterative estimation procedures, making it challenging to derive general optimality results. Developing novel theoretical tools to analyze model collapse from an information-theoretic perspective will offer deeper insights into the statistical behavior of iterative learning processes.



\bibliographystyle{apalike}
\bibliography{ref}

\appendix

\newpage
\baselineskip=24pt
\setcounter{page}{1}
\setcounter{equation}{0}
\setcounter{section}{0}
\renewcommand{\thesection}{A.\arabic{section}}
\renewcommand{\thelemma}{A\arabic{lemma}}
\begin{center}
{\Large\bf Supplementary Materials} \\
\medskip
{\Large\bf ``Golden Ratio Mixing of Real and Synthetic Data for
Stabilizing Generative Model Training"}  \\
\bigskip
\vspace{0.2in} % comment out this line when unblind the author line
\end{center}
\bigskip

\section{Additional Results on Multivariate Gaussian Estimation}
\label{SubSec:Gaus2}

In this part, we extend our results from Section \ref{Sec:GauEsti} to a multivariate setting. Specifically, we assume the real data distribution follows \( N(\bm{\mu}, \bm{\Sigma}) \), where \( \bm{\mu} \in \mathbb{R}^p \) is the mean vector and \( \bm{\Sigma} \in \mathbb{R}^{p \times p} \) is the covariance matrix. The goal of recursive estimation is to estimate \( \bm{\mu} \) and \( \bm{\Sigma} \) simultaneously. Compared to the univariate case, new complexities arise due to the nontrivial analysis in the correlation among variables.

Let \( \mathcal{D}_{t} = \{\bm{x}_{t,i}\}_{i=1}^n \) denote the real dataset collected at the \( t \)-th training step, and let \( \widetilde{\mathcal{D}}_{t} = \{\widetilde{\bm{x}}_{t,i}\}_{i=1}^n \) represent the synthetic dataset generated at the \((t-1)\)-th generation step. The estimation scheme follows a similar approach to the univariate case, and the overall procedure is summarized in \textbf{Fresh Data Augmentation Case}~\ref{alg:Multivariable_gaussian_estimation}. Specifically, we consider the following weighted estimation scheme at the $t$-th training step:
\begin{align}
     \text{Mean Estimation: } &   \boldsymbol{\mu}_{t}(w,k) = \argmin_{\boldsymbol{\mu} \in \mathbb{R}^p}  \frac{w}{n} \sum_{i=1}^n \| \boldsymbol{\mu} - \boldsymbol{x}_{t,i} \|_2^2 + \frac{1-w}{m} \sum_{i=1}^m \| \boldsymbol{\mu} - \widetilde{\boldsymbol{x}}_{t,i} \|_2^2 , \\
     \text{Variance Estimation: } &  \boldsymbol{\Sigma}_t(w,k) = \argmin_{\boldsymbol{\Sigma} \in \mathbb{R}^{p \times p}}  w \| \boldsymbol{\Sigma} - \widehat{\boldsymbol{S}}_t \|_F^2 + (1-w) \| \boldsymbol{\Sigma} - \widetilde{\boldsymbol{S}}_t \|_F^2 ,
\end{align}
where \( \widehat{\boldsymbol{S}}_t =  \frac{1}{n-1}\sum_{i=1}^n (\boldsymbol{x}_{t,i}-\widehat{\boldsymbol{\mu}}_t)(\boldsymbol{x}_{t,i}-\widehat{\boldsymbol{\mu}}_t)^T \) and \( \widetilde{\boldsymbol{S}}_t =  \frac{1}{m-1}\sum_{i=1}^m (\widetilde{\boldsymbol{x}}_{t,i}-\widetilde{\boldsymbol{\mu}}_t)(\widetilde{\boldsymbol{x}}_{t,i}-\widetilde{\boldsymbol{\mu}}_t)^T \) with with \( \widehat{\boldsymbol{\mu}}_t=\frac{1}{n}\sum_{i=1}^n \boldsymbol{x}_{t,i} \) and \( \widetilde{\boldsymbol{\mu}}_{t}=\frac{1}{m}\sum_{i=1}^m \widetilde{\boldsymbol{x}}_{t,i} \).




Similarly, we define the following metrics to analyze the quantitative behavior of $\bm{\mu}_t(w,k)$ and $\bm{\Sigma}_t(w,k)$ especially when $t$ increases to infinity:
\begin{align*}
    \text{Err}(\bm{\mu}_t(w,k)) = \mathbb{E}\big[\Vert \bm{\mu}_t(w,k) - \bm{\mu}\Vert_2^2\big] \mbox{ and }
    \text{Err}(\bm{\Sigma}_t(w,k)) = \mathbb{E}\big[\Vert \bm{\Sigma}_t(w,k) - \bm{\Sigma}\Vert_F^2\big],
\end{align*}  
where $\Vert \cdot \Vert_2$ is the $l_2$-norm and $\Vert \cdot \Vert_F$ is the Frobenius norm. Here, $\text{Err}(\bm{\mu}_{t}(w,k))$ and $\text{Err}(\bm{\Sigma}_t(w,k))$ quantify the estimation errors of $\bm{\mu}_{t}(w,k)$ and $\bm{\Sigma}_t(w,k)$, respectively. The expectation is taken with respect to the randomness inherent in all real and synthetic datasets generated during the first $t$ steps.


\begin{algorithm}[h]
\caption{- Multivariate Gaussian Estimation}
\label{alg:Multivariable_gaussian_estimation}
\begin{algorithmic}
\STATE \textbf{Initialization:} Compute \( \boldsymbol{\mu}_0 = \frac{1}{n}\sum_{i=1}^n \boldsymbol{x}_{0,i} \) and \( \boldsymbol{\Sigma}_0 = \frac{1}{n-1}\sum_{i=1}^n (\boldsymbol{x}_{0,i}-\boldsymbol{\mu}_0)(\boldsymbol{x}_{0,i}-\boldsymbol{\mu}_0)^T \) based on \( \mathcal{D}_0=\{\boldsymbol{x}_{0,i}\}_{i=1}^n \) from \( N(\boldsymbol{\mu},\boldsymbol{\Sigma}) \);
\FOR{$t = 1,2,\ldots,T$}
    \STATE \textbf{1. Data Collection}: Sample \( \widetilde{\mathcal{D}}_{t}=\{\widetilde{\boldsymbol{x}}_{t,i}\}_{i=1}^m \) from \( N(\boldsymbol{\mu}_{t-1},\boldsymbol{\Sigma}_{t-1}) \) and collect a new real dataset \( \mathcal{D}_{t}=\{\boldsymbol{x}_{t,i}\}_{i=1}^n \) from \( N(\boldsymbol{\mu},\boldsymbol{\Sigma}) \);
    \STATE \textbf{2. Mean Estimation}: Compute the mean of the Gaussian distribution at the $t$-th training step:
    \begin{equation}
        \label{Eqn:MULMeanUpdate}
         \boldsymbol{\mu}_{t}(w,k) =  w \widehat{\boldsymbol{\mu}}_t + (1-w)\widetilde{\boldsymbol{\mu}}_{t},
    \end{equation}
   where \( \widehat{\boldsymbol{\mu}}_t=\frac{1}{n}\sum_{i=1}^n \boldsymbol{x}_{t,i} \), \( \widetilde{\boldsymbol{\mu}}_{t}=\frac{1}{m}\sum_{i=1}^m \widetilde{\boldsymbol{x}}_{t,i} \), and \( k = n/m \).

    \STATE \textbf{3. Covariance Estimation}: Compute the covariance of the Gaussian distribution at the $t$-th training step:
        \begin{equation}
        \label{Eqn:MULVarUpdate}
\boldsymbol{\Sigma}_t(w,k) = w\widehat{\boldsymbol{S}}_t + (1-w)\widetilde{\boldsymbol{S}}_t.,
    \end{equation}
    where \( \widehat{\boldsymbol{S}}_t =  \frac{1}{n-1}\sum_{i=1}^n (\boldsymbol{x}_{t,i}-\widehat{\boldsymbol{\mu}}_t)(\boldsymbol{x}_{t,i}-\widehat{\boldsymbol{\mu}}_t)^T \) and \( \widetilde{\boldsymbol{S}}_t =  \frac{1}{m-1}\sum_{i=1}^m (\widetilde{\boldsymbol{x}}_{t,i}-\widetilde{\boldsymbol{\mu}}_t)(\widetilde{\boldsymbol{x}}_{t,i}-\widetilde{\boldsymbol{\mu}}_t)^T \).
\ENDFOR
\RETURN $\boldsymbol{\mu}_T(w,k)$ and $\boldsymbol{\Sigma}_T(w,k)$.
\end{algorithmic}
\end{algorithm}



The central problem is analyzing the behavior of $\bm{\mu}_t(w,k)$ and $\bm{\Sigma}_t(w,k)$ as $t$ approaches infinity. Therefore, we also consider the following metrics for the case where \( t = \infty \):
\begin{align*}
    \mathrm{Err}(\bm{\mu}_{\infty}(w,k)) \triangleq \lim_{t \rightarrow \infty} \mathrm{Err} \left( \bm{\mu}_{t}(w,k) \right) \mbox{ and } \mathrm{Err}(\bm{\Sigma}_{\infty}(w,k)) \triangleq \lim_{t \rightarrow \infty} \mathrm{Err} \left( \bm{\Sigma}_t(w,k) \right).
\end{align*}
In the following, we present the theoretical results on the quantitative behavior of $\mathrm{Err}(\bm{\mu}_{\infty}(w,k))$ and $\mathrm{Err}(\bm{\Sigma}_{\infty}(w,k))$ in a finite-sample setting, aiming to elucidate the impact of $w$ and $k$. Particularly, we show that while the analysis of the mean estimation closely parallels the univariate case, the analysis for variance matrix estimation introduces new complexities. Specifically, the potential correlations among variables of a multivariate normal distribution, cause the variance estimation errors across different time points to no longer follow a straightforward iterative relationship, in contrast with the one-dimensional case. Despite these technical hurdles, we find that $\mathrm{Err}(\bm{\Sigma}_{\infty}(w,k))$ can be succinctly characterized by two key quantities: \(\mathrm{tr}(\mathbf{\Sigma}^2)\) and \(\left[\mathrm{tr}(\mathbf{\Sigma})\right]^2\), as these two terms, especially \(\left[\mathrm{tr}(\mathbf{\Sigma})\right]^2\), can increase with dimensionality, further highlighting the necessity of appropriately choosing $w$ to reduce error.







\begin{theorem}
    \label{Thm1:Mul}
For any $w \in (0,1]$ and $n,m \geq 1$ with $k=\frac{n}{m}$.
It holds that 
$$\mathrm{Err}(\bm{\mu}_{\infty}(w,k)) = \lim_{t \rightarrow \infty} \mathrm{Err}(\bm{\mu}_{t}(w,k) ) 
=\frac{C(w,k)\mathrm{tr}(\bm{\Sigma})}{n},$$
where $C(w,k) =\frac{w^2+(1-w)^2k}{2w-w^2}$ and $\mathrm{tr}(\cdot)$ is the trace of a matrix. Using the optimal weight $w^\star = \frac{\sqrt{k^{2} + 4k} - k}{2}$, the estimation improvement for estimating $\bm{\mu}$ is given by  
\begin{align*}
    \frac{\mathrm{Err}(\bm{\mu}_{\infty}(w^\star,k))}{\mathrm{Err}(\bm{\mu}_{\infty}(1,k))} = w^\star=\frac{\sqrt{k^2+4k}-k}{2}<1.
\end{align*}
\end{theorem}

In Theorem \ref{Thm1:Mul}, we derive the exact expression for \(\mathrm{Err}(\bm{\mu}_{\infty}(w,k))\) under different values of the weighting parameter \(w\) and the mixing ratio \(k\). Notably, the result in Theorem \ref{Thm1:Mul} simplifies to that in Theorem \ref{Thm1:Uni} when the data is one-dimensional. Thus, Theorem \ref{Thm1:Mul} can be regarded as the multivariate extension of Theorem \ref{Thm1:Uni}.

Theorem \ref{Thm1:Mul} shows that $\mathrm{Err}(\bm{\mu}_{\infty}(w,k)) = \frac{C(w,k) \, \mathrm{tr}(\bm{\Sigma})}{n}$ for any values of $(n,m,w)$, where $C(w,k)$ is the same as in Theorem \ref{Thm1:Uni}. Consequently, we draw the same conclusions as in Theorem \ref{Thm1:Uni} regarding the choice of the optimal weight and the improvement in estimation compared to the estimator based solely on newly emerging real data. For any values of $(m,n)$ with $n/m = k$, the optimal weight is given by $w^\star = \frac{\sqrt{k^{2} + 4k} - k}{2}$. In particular, when $k=1$ (i.e., $m = n$), the optimal weight for multivariate mean estimation is $\frac{\sqrt{5}-1}{2}$, which is the reciprocal of the golden ratio.

Similar to Proposition \ref{corollary_global_optimum_uni}, the proposed weighted combination in Theorem \ref{Thm1:Mul} remains the Best Linear Unbiased Estimator (BLUE), as stated in the following proposition.

\begin{proposition}
\label{corollary_global_optimum}
Consider a more general update rule for mean estimation given by:\begin{equation}
    \label{Eqn:MULMeanUpdate_General_multi}
    \bm{\mu}_{t}(\{w_i\}_{i=1}^n,\{\widetilde{w}_j\}_{j=1}^m,k) = \frac{1}{n}\sum_{i=1}^n w_i \bm{x}_{t,i}+\frac{1}{m}\sum_{j=1}^m \widetilde{w}_j\widetilde{\bm{x}}_{t,i},
\end{equation}
where \( w_i \) and \( \widetilde{w}_j \) are the weights assigned to the \( i \)-th real and the $j$-th synthetic data points, respectively, satisfying that $\sum_{i=1}^n w_i/n + \sum_{j=1}^m \widetilde{w}_j/m = 1$. Consider the limiting estimation error defined as
$$\mathrm{Err}(\bm{\mu}_{\infty}(\{w_i\}_{i=1}^n,\{\widetilde{w}_j\}_{j=1}^m,k)) = \lim_{t\rightarrow \infty}\mathbb{E}\big[\Vert \bm{\mu}_{t}(\{w_i\}_{i=1}^n,\{\widetilde{w}_j\}_{j=1}^m,k) - \bm{\mu}\Vert_2^2\big].$$ It holds true that $\mathrm{Err}(\bm{\mu}_{\infty}(\{w_i\}_{i=1}^n,\{\widetilde{w}_j\}_{j=1}^m,k))$ is minimized by $w_i = w^{\star}$ for $i \in [n]$ and $\widetilde{w}_j = 1 - w^{\star}$ for $j \in [m]$, where \( w^{\star}= \frac{\sqrt{k^{2} + 4k} - k}{2} \) as defined in Theorem \ref{Thm1:Mul}.

\end{proposition}

\begin{remark}[The Advantage of Weighted Training Over Direct Mixing]
\label{advantage_than_direct_mix_multi}
Similar to Remark \ref{advantage_than_direct_mix}, if we directly merge the datasets \(\mathcal{D}_t\) and \(\widetilde{\mathcal{D}}_t\) and minimize the empirical loss on them:
\begin{align*}
\boldsymbol{\mu}_{t}(w,k) = \argmin_{\boldsymbol{\mu} \in \mathbb{R}^p}  \frac{1}{n+m} \left\{ \sum_{i=1}^n \| \boldsymbol{\mu} - \boldsymbol{x}_{t,i} \|_2^2 +\sum_{i=1}^m \| \boldsymbol{\mu} - \widetilde{\boldsymbol{x}}_{t,i} \|_2^2 \right\},
\end{align*}
then the resulting estimate can be written as $\bm{\mu}_{t}(w_0, k) =  w_0 \widehat{\bm{\mu}}_t + (1-w_0)\widetilde{\bm{\mu}}_{t}$, where the implicit weight \(w_0\) is given by:
\begin{equation*}
w_0 = \frac{n}{n+m} = \frac{\frac{n}{m}}{\frac{n}{m}+1} = \frac{k}{k+1} =  \frac{2k}{\sqrt{k^2+4k+4} +k }
<w^{\star} = \frac{\sqrt{k^2+4k}-k}{2} = \frac{2k}{ \sqrt{k^2+4k }+k  }.
\end{equation*}
A detailed derivation is provided in Appendix \ref{explain_advantage}.
This inequality demonstrates that, similar to Remark \ref{advantage_than_direct_mix}, directly mixing the data again results in a strictly suboptimal weight for mean estimation for any value of $k$ in the context of multivariate Gaussian estimation.
\end{remark}


\begin{corollary}
    \label{Coro:Multi_Mean}
    $\mathrm{Err}(\bm \mu_{\infty}(w,k))$ exhibits the following special cases:
        \begin{itemize}
        \item[(1)] \textbf{Fully Synthetic - Model Collapse}:  $\lim_{w \rightarrow 0^{+}} \mathrm{Err}(\bm{\mu}_{\infty}(w,k)) = \infty$, 
        \item[(2)] \textbf{Real Data Only}: $\mathrm{Err}(\bm{\mu}_{\infty}(1,k)) = \frac{\mathrm{tr}(\bm{\Sigma})}{n}$ \text{ for any } $k>0$,
        \item[(3)] \textbf{Optimal Weight}: $\mathrm{Err}(\bm{\mu}_{\infty}(w^{\star},k)) = \frac{w^\star \mathrm{tr}(\bm{\Sigma})}{n}$ \text{ for any } $k>0$,
    \end{itemize}
where $w^\star = \frac{\sqrt{k^{2} + 4k} - k}{2}$ is the optimal weight as defined in Theorem \ref{Thm1:Mul}.
\end{corollary}

In Corollary \ref{Coro:Multi_Mean}, we present several special cases of Theorem \ref{Thm1:Mul}. These three special cases are the multivariate extensions of those in Corollary \ref{Th1_Coro1}. Notably, when $w = 0$ (corresponding to the fully synthetic case), the estimation error $\mathrm{Err}(\bm{\mu}_{\infty}(w, k))$ diverges to infinity, which mirrors the result in the univariate case. When $m = n$, the minimal estimation error occurs at $w = w^\star = \frac{\sqrt{5}-1}{2}$. Using this optimal mixing weight, the estimation error $\mathrm{Err}(\bm{\mu}_{\infty}(w, k))$ improves compared to using only real data, as evidenced by the inequality $\frac{\sqrt{k^2 + 4k} - k}{2} < 1$ for any $k > 0$. Specifically, when $k = 1$ ($n = m$), the improvement is also given by $\frac{\mathrm{Err}(\bm{\mu}_{\infty}(w^\star, 1))}{\mathrm{Err}(\bm{\mu}_{\infty}(1, 1))} = w^\star = \frac{\sqrt{5} - 1}{2}$ as in Corollary \ref{Th1_Coro1}. 



\begin{theorem}
\label{thm3:multi_variance}
For any $w \in (0, 1]$, $n, m \geq 2$, and $k > 0$, it holds that  
\begin{align*}
  \mathrm{Err}(\bm{\Sigma}_{\infty}(w,k))=
  \begin{cases} 
\infty, & 0 < w \leq 1 - \sqrt{\frac{m-1}{m+1}}, \\
\frac{N}{D}-\mathrm{tr}(\bm{\Sigma}^2), & 1 - \sqrt{\frac{m-1}{m+1}} < w \leq 1,
\end{cases}
\end{align*}
where \( N \) and \( D \) are defined as
\begin{align*}
&N  = (2w-w^2)\mathrm{tr}(\bm{\Sigma}^2)+
\underbrace{\frac{(w-1)^2\left[
\mathrm{tr}^2(\bm{\Sigma})(2w-w^2)
+ \frac{2w^2\mathrm{tr}(\bm{\Sigma}^2)}{n-1}
\right] }{(m-1)(2w-w^2)}}_{\text{Convergence to 0 as } m \rightarrow \infty}
+ \frac{w^2[\mathrm{tr}^2(\bm{\Sigma})+\mathrm{tr}(\bm{\Sigma}^2)]}{n-1},\\
&D = 
2w-w^2
- \underbrace{\frac{2(w-1)^4}{(m-1)^2[2w-w^2]} - \frac{(w-1)^2}{m-1}}_{\text{Convergence to 0 as } m \rightarrow \infty}.
\end{align*}
\end{theorem}

In Theorem \ref{thm3:multi_variance}, we derive the exact expression for \(\mathrm{Err}(\bm{\Sigma}_{\infty}(w,k))\) under various values of the weighting parameter \(w\) and the mixing ratio \(k\). Similar to Theorem \ref{Thm2:Uni_var}, we demonstrate that \(\mathrm{Err}(\bm{\Sigma}_{\infty}(w,k))\) exhibits two main situations. Specifically, when \( w \leq 1 - \sqrt{\frac{m-1}{m+1}} \), meaning that the estimator based on real samples is assigned a small weight, the estimation error \(\mathrm{Err}(\bm{\Sigma}_{\infty}(w,k))\) diverges to infinity, leading to model collapse. When \( w > 1 - \sqrt{\frac{m-1}{m+1}} \), \(\mathrm{Err}(\bm{\Sigma}_{\infty}(w,k))\) remains finite for any values of \( (n,m) \). As \( n, m \) approach infinity, \(\mathrm{Err}(\bm{\Sigma}_{\infty}(w,k))\) converges to zero. This result indicates that the proposed estimator effectively utilizes all the information from previous real data contained in the synthetic data.







\begin{corollary}
\label{multi_var_corollary}
For any \( k \in \mathbb{R}_{\geq 0} \), assume that \( n, m \to \infty \) while maintaining the ratio \( \frac{n}{m} = k \). Then, it holds that
\begin{align*}
\mathrm{Err}(\bm{\Sigma}_{\infty}(w,k)) = 
\begin{cases} 
\infty, & \text{if } w \leq 1-\sqrt{\frac{m-1}{m+1}},\\
[\mathrm{tr}(\bm{\Sigma}^2)+\mathrm{tr}^2(\bm{\Sigma})]\frac{C(w,k)}{n-1}+o(n^{-1}), & \text{if } w > 1-\sqrt{\frac{m-1}{m+1}}, 
\end{cases}
\end{align*}
Furthermore, $\mathrm{Err}(\bm{\Sigma}_{\infty}(w,k))$ exhibits the following special cases:
\begin{itemize}
        \item[(1)] \textbf{Fully Synthetic - Model Collapse}:  $\lim_{w \rightarrow 0^{+}}\mathrm{Err}(\bm{\Sigma}_{\infty}(w,k)) = \infty$ for any $k>0$,
        \item[(2)] \textbf{Real Data Only}: $\mathrm{Err}(\bm{\Sigma}_{\infty}(1,k)) = \frac{\mathrm{tr}(\bm{\Sigma}^2)+\mathrm{tr}^2(\bm{\Sigma})}{n-1}$ for any $k>0$,
        \item[(3)] \textbf{Asymptotic Optimal Weight}: $\mathrm{Err}(\bm{\Sigma}_{\infty}(w^\star,k)) = \frac{w^\star [\mathrm{tr}(\bm{\Sigma}^2)+\mathrm{tr}^2(\bm{\Sigma})]}{n-1}+o(n^{-1})$ as $m\to \infty, n\to \infty, \frac{n}{m}\to k$ for any $k>0$,
\end{itemize}
where $w^\star = \frac{\sqrt{k^{2} + 4k} - k}{2}$ is the asymptotic optimal weight for estimating $\bm{\Sigma}$. Particularly, when $k=1$, $w^{\star}$ becomes the reciprocal of the golden ratio, given by $\frac{\sqrt{5}-1}{2}$. Using $w=w^\star$, for any $k \in \mathbb{R}_{\geq 0}$, we have 
\begin{align*}
\lim_{m,n\rightarrow \infty,\frac{n}{m}\rightarrow k}
    \frac{\mathrm{Err}(\bm{\Sigma}_{\infty}(w^\star,k))}{\mathrm{Err}(\bm{\Sigma}_{\infty}(1,k))} = \frac{\sqrt{k^2+4k}-k}{2}<1.
\end{align*}
\end{corollary}
In Corollary \ref{multi_var_corollary}, we analyze the asymptotic behavior of $\mathrm{Err}(\bm{\Sigma}_{\infty}(w,k))$ as $m$ and $n$ diverge while maintaining the ratio $\frac{n}{m} = k$. Notably, in the asymptotic regime, $\mathrm{Err}(\bm{\Sigma}_{\infty}(w,k))$ is primarily determined by  
$ \left[\mathrm{tr}(\bm{\Sigma}^2) + \mathrm{tr}^2(\bm{\Sigma})\right] \frac{C(w,k)}{n-1}$, where the optimal weight for minimizing $C(w,k)$ is given by  
$ w^\star = \frac{\sqrt{k^4 + 4k} - k}{2}$. Furthermore, consistent with previous results, we present several special cases of $\mathrm{Err}(\bm{\Sigma}_{\infty}(w,k))$. In particular, when $w = 0$, the estimation error diverges to infinity, leading to model collapse. When $w = 1$, the estimation error simplifies to $\frac{\mathrm{tr}(\bm{\Sigma}^2) + \mathrm{tr}^2(\bm{\Sigma})}{n-1}$. Clearly, in the asymptotic regime where $m, n$ approach infinity with $k = \frac{n}{m}$, the proposed estimator outperforms one that relies solely on newly emerging real data whenever $w$ is chosen such that $\mathrm{Err}(\bm{\Sigma}_{\infty}(w,k)) < \mathrm{Err}(\bm{\Sigma}_{\infty}(1,k))$.


\section{Additional Discussion and Results for Remark \ref{advantage_than_direct_mix} and \ref{advantage_than_direct_mix_multi}}
\label{explain_advantage}
If we directly merge the datasets \(\mathcal{D}_t\) and \(\widetilde{\mathcal{D}}_t\) and minimize the empirical loss on them:
\begin{align*}
\boldsymbol{\mu}_{t}(w,k) = \argmin_{\boldsymbol{\mu} \in \mathbb{R}^p}  \frac{1}{n+m} \left\{ \sum_{i=1}^n \| \boldsymbol{\mu} - \boldsymbol{x}_{t,i} \|_2^2 +\sum_{i=1}^m \| \boldsymbol{\mu} - \widetilde{\boldsymbol{x}}_{t,i} \|_2^2 \right\},
\end{align*}
by taking the derivative and setting it equal to zero, we obtain
\begin{align*}
\boldsymbol{\mu}_{t}(w,k)&=\frac{1}{m+n}\left(\sum_{i=1}^{n}\boldsymbol{x}_{t,i}+\sum_{i=1}^{m}\widetilde{\boldsymbol{x}}_{t,i} \right) \\
&=\frac{n}{m+n}\frac{\sum_{i=1}^{n}\boldsymbol{x}_{t,i}}{n}+\frac{m}{m+n}\frac{\sum_{i=1}^{m}\widetilde{\boldsymbol{x}}_{t,i}}{m} \\ 
&=\argmin_{\boldsymbol{\mu} \in \mathbb{R}^p}  \frac{n}{m+n}\frac{\sum_{i=1}^n \| \boldsymbol{\mu} - \boldsymbol{x}_{t,i} \|_2^2}{n} + (1-\frac{n}{m+n})\frac{\sum_{i=1}^m \| \boldsymbol{\mu} - \widetilde{\boldsymbol{x}}_{t,i} \|_2^2}{m},
\end{align*}
where the last equality is obtained by taking the derivative and setting it to zero again.
\section{Additional Discussion and Results for Section \ref{Sec:Diss}}
\label{additional_discussion}
Consider the dominant term in Equation \(\eqref{Res:FInal}\):
\begin{equation*}
    g(k)=\frac{1}{S}\frac{k+r}{k}\frac{\sqrt{k^2+4k}-k}{2}.
\end{equation*}
Taking the derivative with respect to \(k\) gives
\begin{equation}
    \frac{\partial g(k)}{\partial k} = -\frac{k \sqrt{k^{2} + 4k} - k^{2} - 2k + 2r}{2Sk \sqrt{k^{2} + 4k}},
\end{equation}
so that the sign of \(\frac{\partial g(k)}{\partial k}\) is determined by
\begin{equation}
    h(k) = -\Bigl(k \sqrt{k^{2} + 4k} - k^{2} - 2k + 2r\Bigr).
\end{equation}

Next, we analyze the condition \(h(k) \ge 0\):
\begin{align}
    h(k) \ge 0 &\iff k\sqrt{k^2+4k} \le k^2+2k-2r \nonumber\\ 
    &\iff k^2 (k^2+4k) \le k^4+4k^3+4k^2-4r(k^2+2k)+4r^2, \quad \text{with } k^2+2k-2r \ge 0 \nonumber\\ 
    &\iff (1-r)k^2-2rk+r^2 \ge 0,\quad k^2+2k-2r \ge 0.
\end{align}

\bigskip

\textbf{Case 1: \(r > 1\): } When \(r>1\), the inequality \(k^2+2k-2r \ge 0\) is equivalent to
\[
k\ge \sqrt{2r+1}-1.
\]
On the other hand, the inequality
\[
(1-r)k^2-2rk+r^2 \ge 0
\]
is equivalent to
\[
k\le \frac{r^{1.5}-r}{r-1}.
\]
However, since for all \(r>0\) it holds that
\[
\sqrt{2r+1}-1\ge \frac{r^{1.5}-r}{r-1},
\]
there is no \(k\) that satisfies both conditions simultaneously. A similar verification for \(r=1\) also yields no solution. In other words, for these cases, we have \(h(k) < 0\) for all \(k\ge0\), meaning that \(g(k)\) is a decreasing function on \(k\ge0\). Therefore, the optimal (i.e., minimizing) value is obtained as \(k\to\infty\).

\bigskip

\textbf{Case 2:} When \(r<1\), the conditions
\[
(1-r)k^2-2rk+r^2 \ge 0,\quad k^2+2k-2r \ge 0
\]
are equivalent to
\[
k\in\Bigl(0, \frac{r^{\frac{3}{2}} - r}{r - 1}\Bigr)\cap \Bigl(-\frac{r^{\frac{3}{2}} + r}{r - 1},\infty\Bigr)\cap \Bigl(\sqrt{2r+1}-1,\infty\Bigr).
\]
After simplification, it can be shown that the above conditions reduce to
\[
k> -\frac{r^{\frac{3}{2}} + r}{r - 1}.
\]
Thus, in this case, the optimal value that minimizes \(g(k)\) is given by
\[
k = -\frac{r^{\frac{3}{2}} + r}{r - 1}.
\]


\input{Proof_Of_Lemmas}

\input{Proof_Of_Theorems}

\end{document}
