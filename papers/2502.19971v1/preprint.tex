% science_template.tex
% See accompanying readme.txt for copyright statement, change log etc.

% Any modification of this template, including writing a paper using it,
% MUST rename the file i.e. use a different file name.

%%%%%%%%%%%%%%%% START OF PREAMBLE %%%%%%%%%%%%%%%

% Basic setup. Authors shouldn't need to adjust these commands.
% It's annoying, but please do NOT strip these into a separate file.
% They need to be included in this .tex for our production software to work.

% Use the basic LaTeX article class, 12pt text
\documentclass[12pt]{article}

% Science uses Times font. If you don't have this installed (most LaTeX installations will be
% fine) or prefer the old Computer Modern fonts, comment out the following line
\usepackage{newtxtext,newtxmath}
% Depending on your LaTeX fonts installation, you might get better results with one or both of these:
%\usepackage{mathptmx}
%\usepackage{txfonts}

% \usepackage{lineno}
% \linenumbers
\usepackage{hyperref}

% Allow external graphics files
\usepackage{graphicx}

% Use US letter sized paper with 1 inch margins
\usepackage[letterpaper,margin=1in]{geometry}


% Double line spacing, including in captions
\linespread{1.5} % For some reason double spacing is 1.5, not 2.0!

% One space after each sentence
\frenchspacing

% Abstract formatting and spacing - no heading
\renewenvironment{abstract}
	{\quotation}
	{\endquotation}

% No date in the title section
\date{}

% Reference section heading
\renewcommand\refname{References and Notes}

% Figure and Table labels in bold
\makeatletter
\renewcommand{\fnum@figure}{\textbf{Figure \thefigure}}
\renewcommand{\fnum@table}{\textbf{Table \thetable}}
\makeatother

\newcommand{\qcode}[3]{$[\![ #1, #2, #3 ]\!]$}

% Call the accompanying scicite.sty package.
% This formats citation numbers in Science style.
\usepackage{scicite}
% Provides the \url command, and fixes a crash if URLs or DOIs contain underscores
\usepackage{url}

%%%%%%%%%%%% CUSTOM COMMANDS AND PACKAGES %%%%%%%%%%%%

% Authors can define simple custom commands e.g. as shortcuts to save on typing
% Use \newcommand (not \def) to avoid overwriting existing commands.
% Keep them as simple as possible and note the warning in the text below.
% Example:
\newcommand{\pcc}{\,cm$^{-3}$}	% per cm-cubed

% Please DO NOT import additional external packages or .sty files.
% Those are unlikely to work with our conversion software and will cause problems later.
% Don't add any more \usepackage{} commands.


%%%%%%%%%%%%%%%% TITLE AND AUTHORS %%%%%%%%%%%%%%%%

% Title of the paper.
% Keep it short and understandable by any reader of Science.
% Avoid acronyms or jargon. Use sentence case.
\def\scititle{
    Efficient and Universal Neural-Network Decoder for Stabilizer-Based Quantum Error Correction
}
% Store the title in a variable for reuse in the supplement (otherwise \maketitle deletes it)
\title{\bfseries \boldmath \scititle}

% Author and institution list.
% Institution numbers etc. should be hard-coded, do *not* use the \footnote command.
\author{
	% You can write out first names or use initials - either way is acceptable, but be consistent
	Gengyuan Hu$^\text{1}$,
        Wanli Ouyang$^{\text{1,2}\ast}$,
        Chao-Yang Lu$^\text{3,4,5,6}$,
	Chen Lin$^{7\ast}$,
        Han-Sen Zhong$^{\text{1,8}\ast}$\and
	% Additional lines of authors should be inserted using the \and command (not \\)
	% Institution list, in a slightly smaller font
        \small$^{1}$Shanghai Artificial Intelligence Laboratory, Shanghai \& 200030, China.\and
        \small$^{2}$Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong SAR \& HKG, China.\and
        \small$^{3}$Hefei National Research Center for Physical Sciences at the Microscale and School of \and
        \small Physical Sciences, University of Science and Technology of China, Hefei \& 230026, China.\and
        \small$^{4}$CAS Center for Excellence in Quantum Information and Quantum Physics, University of \and
        \small Science and Technology of China, Shanghai \& 201315, China.\and
        \small$^{5}$Hefei National Laboratory, University of Science and Technology of China, Hefei \& 230088, China.\and
        \small$^{6}$Shanghai Research Center for Quantum Sciences, Shanghai \& 201315, China.\and
        \small$^{7}$Department of Engineering, University of Oxford, Oxford \& OX1 4BH, UK.\and
        \small$^{8}$Shanghai Innovation Institute, Shanghai \& 200231, China.\and
	% Identify at least one corresponding author, with contact email address
	\small$^\ast$Corresponding author. Email: wlouyang@ie.cuhk.edu.hk \and
	\small$^\ast$Corresponding author. Email: chen.lin@eng.ox.ac.uk \and
	\small$^\ast$Corresponding author. Email: zhonghansen@pjlab.org.cn \and
}
%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%% START OF MAIN TEXT %%%%%%%%%%%%%%%
\begin{document} 

% Insert the title and author list
\maketitle

% Abstract, in bold
% There are strict length limits, and not all formats have abstracts.
% Consult the journal instructions to authors for details.
% Do not cite any references in the abstract.
\begin{abstract} \bfseries \boldmath
% Start with one or two sentences of background
Quantum error correction is crucial for large-scale quantum computing, but the absence of efficient decoders for new codes like quantum low-density parity-check (QLDPC) codes has hindered progress.
% Then summarise the results of your observations, experiments, simulations etc.
Here we introduce a universal decoder based on linear attention sequence modeling and graph neural network that operates directly on any stabilizer code's graph structure. 
% End with a statement of your main conclusions
Our numerical experiments demonstrate that this decoder outperforms specialized algorithms in both accuracy and speed across diverse stabilizer codes, including surface codes, color codes, and QLDPC codes. The decoder maintains linear time scaling with syndrome measurements and requires no structural modifications between different codes. For the Bivariate Bicycle code with distance 12, our approach achieves a 39.4\% lower logical error rate than previous best decoders while requiring only ~1\% of the decoding time. These results provide a practical, universal solution for quantum error correction, eliminating the need for code-specific decoders.
\end{abstract}

% The first paragraph of any Science paper does NOT have a heading
% Nor is it indented
\noindent
\section{Main}
Quantum computing systems are highly susceptible to environmental noise, making quantum error correction (QEC)\cite{Shor1995QEC,Nielsen2010Quantum} essential for practical implementations. QEC protects quantum information by encoding logical qubits across multiple physical qubits, analogous to classical error correction. However, quantum error correction is fundamentally more challenging due to constraints imposed by quantum mechanics, including the no-cloning theorem\cite{Wootters1982Noclone} and measurement-induced wavefunction collapse.

The stabilizer formalism\cite{Gottesman1997Stabilizer} has emerged as the leading theoretical framework for quantum error correction, providing a systematic approach to encode quantum information. While the encoding process is straightforward in the stabilizer formalism, the decoding problem presents a significant challenge, having been proven NP-complete\cite{Iyer2015StabilizerNP} to achieve optimal accuracy. As a result, a practical decoder which allows real-time error suppressing on a quantum computer requires a well designed trade off between accuracy and decoding speed. Although efficient specialized decoders exist for certain codes such as the surface code\cite{higgott2023sparse,Dennis2002MWPM}, many advantageous schemes like quantum low-density parity-check (QLDPC) codes\cite{gottesman2013fault} and color code\cite{bombin2006topological} still suffers from either low decoding accuracy or long decoding latency.

Two primary approaches have emerged for achieving efficient decoding beyond the surface code. The matching-based approach, which has demonstrated both high accuracy and computational efficiency for surface codes, focuses on decomposing complex encoding schemes into surface code-like sub-blocks for decoding\cite{delfosse2014decoding}. The belief propagation (BP) based approach adapts classical BP algorithms to quantum contexts, with the Belief Propagation with Ordered Statistics Decoding (BP-OSD)\cite{panteleev2021degenerate} serving as a widely-adopted universal stabilizer code decoder, despite several claimed improvements in recent variants\cite{hillmann2024localized,wolanski2024ambiguity}.

In parallel, deep neural networks have shown promising results in quantum error correction, though primarily focusing on surface codes thus far\cite{krastanov2017deep,torlai2017neural,chamberland2018deep,meinerz2022scalable,varbanov2023neural}. The application of neural decoders has evolved significantly from initial proof-of-concept studies to more sophisticated implementations, with recent research demonstrating superior performance over conventional methods when fine-tuned with experimental data\cite{bausch2024learning}. While most neural decoding efforts concentrate on surface codes, there have been initial attempts to extend these techniques to other schemes such as color codes\cite{maskara2019advantages,baireuther2019neural}, suggesting the potential for neural approaches to address broader classes of quantum error correction codes.

In this work, we introduce a universal quantum error decoder based on graph neural networks (GNNs) that addresses the limitations of existing approaches. Our decoder represents syndrome measurements in a code-specific graph structure that naturally captures the stabilizer dependencies of the quantum code, and processes the temporal correlations through a recurrent architecture, enabling efficient decoding across different code families.

We demonstrate the universality and efficiency of our approach through comprehensive circuit-level noise decoding experiments on three distinct families of stabilizer codes: the color code, the Bivariate Bicycle (BB) code\cite{bravyi2024high}, and the surface code. On the color code and BB code, our decoder achieves lower logical error rate compared to BP-OSD under realistic circuit-level noise conditions, suppressed the error rate from $15.7\%$ to $39.4\%$ on different code variants, while maintaining linear time complexity with respect to the number of error correction cycles. In surface code experiments, we surpass the performance of the Alpha-Qubit\cite{bausch2024learning} neural decoder on the Sycamore dataset\cite{google_quantum_ai_team_2022_6804040} while using significantly fewer model parameters. These results establish our approach as a promising universal solution for quantum error correction, potentially accelerating the development and practical implementation of diverse quantum codes for fault-tolerant quantum computing.


% Research Articles and Reviews split the text into sections using headings
% Use a short (up 6 words) descriptive phrase, not generic 'Results' or 'Conclusions'
% Most other formats do not have headings, see the journal instructions to authors for details

\subsection{Decode on the Tanner graph}
In the quantum circuit model, quantum computations are executed through sequences of elementary quantum gates acting on qubits, analogous to classical logic gates in digital circuits. However, physical implementations are susceptible to errors during all operations, including gate operations, qubit initialization, and measurements. This realistic scenario is characterized by the circuit-level noise model.

To protect quantum information against such noise, stabilizer codes are employed. A \qcode{n}{k}{d} stabilizer code is defined by a set of $n-k$ stabilizer generators, represented by a $2n\times(n-k)$ binary symplectic matrix $H \equiv \left[\cdot|\cdot\right]$. Each row in $H$ represents a stabilizer generator, and each column of each half corresponds to a qubit. The matrix elements specify Pauli operators through the mapping:
\begin{equation}\label{eq:sympletic representation}
    (0|0) \rightarrow I, (0|1) \rightarrow Z, (1|0) \rightarrow X, (1|1) \rightarrow Y
\end{equation}
such that each stabilizer generator is a product of Pauli operators on the encoded qubits.

Our approach begins by representing this stabilizer structure as a Tanner graph (FIG.\ref{fig:WorkingFlow}). In this representation, stabilizer generators become check nodes and qubits become data nodes, with edges indicating non-zero matrix elements. The Tanner graph serves not only as a mathematical equivalent representation for a stabilizer code but also as an abstracted description of the code's hardware topology. Signals from the quantum device implementing the described code can be naturally embedded into the graph for feature extraction.

During quantum error correction, measurements from each stabilizer generator are collected in successive cycles until the logical qubit measurement is required. The collected measurement results from previous cycles, called syndromes, are used to correct the logical measurement results through decoding. To connect the decoding problem with the syndrome, we extend the basic Tanner graph by incorporating nodes and edges representing logical observables, which are also defined as products of Pauli operators. We then embed all syndrome data into the extended Tanner graph $\mathcal{G}(V_{\text{data}},V_{\text{check}},V_{\text{logical}},E_{\text{stabilizer}},E_{\text{logical}})$, treating each time as a slice of a temporal dynamic graph. This formulation transforms the decoding problem into a temporal graph classification task, seeking a mapping $f$:
\begin{equation}
    f: \mathcal{G}_t\left(\{S_t\}\right) \rightarrow \mathcal{C}, S_t \in \mathbb{F}_2^{n_s}, \mathcal{C} \in \mathbb{F}_2^k
\end{equation}
where $\{S_t\}$ represents the syndrome sequence, and $\mathcal{C}$ is a binary vector indicating logical qubit flips, with $n_s$ being the number of syndromes and $k$ the number of encoded logical qubits. This binary nature of inputs and outputs reflects the discrete measurement outcomes in quantum experiments.

The graph formulation's expressiveness distinguishes it from existing neural decoders constrained by topological rigidity. Conventional neural approaches, such as Convolutional Neural Networks (CNNs) tailored for surface codes, require redesigning network architectures when applied to codes spanning different geometries (e.g., 3D toric codes) or non-local structures (e.g., QLDPC codes). In contrast, our extended Tanner graph $\mathcal{G}$ offers a topology-agnostic interface: its nodes and edges dynamically adapt to any stabilizer code's connectivity, whether geometric or algebraic. By grounding the neural architecture directly in $\mathcal{G}$'s graph isomorphisms, the decoder permits a single neural model to learn decoding strategies across code families, where prior neural decoders required per-code specialization.  

The neural network decoder, whose architecture mirrors the extended Tanner graph structure, can be trained using data from either quantum simulators or real quantum processors. This approach enables the decoder to learn both the underlying noise patterns and circuit-specific error correlations directly from the data, without requiring explicit noise process modeling.

\subsection{Efficient universal decoding across different code families}

Our previous section established the theoretical framework for universal neural decoders, yet significant challenges persist in optimizing two critical dimensions: modeling the topological structure of quantum codes and capturing temporal dependencies in syndrome sequences. To address these challenges, we propose a three-phase neural decoding architecture :

\begin{enumerate}
    \item \textit{Spatial Feature Extraction:} Embedded multiplicative message passing (MMP) operators process syndrome patterns on Tanner graph slices, explicitly leveraging parity-check constraints.
    \item \textit{Temporal Context Integration:} Historical syndrome correlations are captured through linear attention mechanisms, preserving linear inference complexity ($\mathcal{O}(T)$) while enabling parallel training.
    \item \textit{Logical Error Prediction:} Final decoding decisions are generated via topology-grounded graph pooling, incorporating code-specific boundary conditions.
\end{enumerate}

The computational complexity constraint fundamentally shapes our architecture design. To enable real-time decoding for quantum devices, the decoder must maintain linear scaling in the time dimension. While recurrent neural networks (RNNs) naturally satisfy this requirement, their known limitations in long-sequence modeling and training instability make them suboptimal. Instead, we adopt modern linear attention layers \cite{yang2024gated,yang2024parallelizing} - a technique matured through large language model development - which combine parallelizable training with efficient recurrent inference through matrix decomposition techniques.

Our spatial processing employs multiplicative rather than additive message passing, driven by the algebraic structure of stabilizer measurements: simultaneous violations of a stabilizer generator cancel modulo 2. This property is natively captured by first normalizing node features to (-1,1), then applying element-wise multiplication during message aggregation. The resulting product terms directly reflect parity conservation laws.

To rigorously evaluate the performance of our decoder, we conduct quantum memory experiments using diverse quantum error correction (QEC) codes, including two major code families: triangular color codes with code distances ranging from $d=3$ to $d=11$, and BB codes represented by two distinct configurations, \qcode{72}{12}{6} and \qcode{144}{12}{12}, where \qcode{n}{k}{d} denotes an encoding scheme that uses $n$ physical qubits to encode $k$ logical qubits with a code distance of $d$. Both families belong to the CSS code subclass (Calderbank-Shor-Steane codes), which allows for independent X- and Z-basis decoding. This structural property simplifies the temporal decoding graph $\mathcal{G}_t$ into a static graph $\mathcal{G}$ containing only half the stabilizer check nodes, thereby significantly reducing decoding complexity.  The simulation employs a uniform depolarizing noise model with an error parameter $p=0.005$, reflecting realistic near-term quantum computing conditions \cite{acharya2024quantum, evered2023high}
 For comprehensive benchmarking, we compare our neural network decoder against BP-OSD. Across all test cases, we maintain consistent neural network architectures while strategically adapting graph connectivity configurations and optimizing hyperparameters for each specific code structure. Further details on the noise model can be found in the Supplementary Material.

As shown in Fig. \ref{fig:UniversalDecodingAcc}, we quantify decoder performance through two critical fault-tolerance metrics: logical error rate (y-axis) and encoding rate (x-axis). Our neural network decoder demonstrates universal superiority over BP-OSD, achieving a 27.0\% average reduction in logical error rates. The performance gap widens substantially for larger codes: In the \qcode{144}{12}{12} BB code implementation, our decoder attains a 39.4\% error rate reduction .

We also evaluate the decoding speed of the decoders, with the results presented in Fig. \ref{fig:UniversalDecodingTime}. The neural decoder demonstrates significantly faster performance compared to the BP-OSD decoder, particularly for large codes with a high number of cycles. This speed advantage stems from the decoder's linear computational complexity along the time dimension and its parallelizable structure across the qubit dimension. For instance, on the BB\qcode{144}{12}{12} code, the neural decoder achieves a two-orders-of-magnitude speedup at 12 cycles, and this advantage grows to nearly three orders of magnitude at 36 cycles. Although we have not yet fully optimized the decoder for inference, the per-cycle latency is already approximately 1 ms/cycle. Furthermore, we demonstrate the potential throughput when leveraging the full computational power of a GTX4090 device. This scalability underscores the practicality of our method for fault-tolerant quantum architectures.

To benchmark the performance of our decoder against more existing approaches, we also conducted comparative evaluations using the surface code — the most widely researched quantum error correction architecture. Fig. \ref{fig:CompareGoogle} presents analysis results from the Sycamore surface code dataset\cite{google_quantum_ai_team_2022_6804040}, which encompasses quantum memory experiments for rotated surface codes with distances $d=3$ and $d=5$ across 25 syndrome extraction cycles. The surface code variant under investigation is the rotated XZZX variant which is not CSS, it has a different number of syndrome measurement at the initializing cycle that establishes it as an prototype for evaluating our framework's capability to handle dynamic temporal graph structures.
Following the experimental protocol established in prior work\cite{bausch2024learning}, we partitioned the dataset into validation and test sets based on index parity, with final performance metrics obtained by averaging across measurement bases, qubit configurations, and parity selections. Adopting the pretrain-finetune paradigm, we first trained our model on simulated data before performing experimental data adaptation. The proposed decoder demonstrates superior error suppression capability, achieving lower logical error rates while maintaining enhanced parameter efficiency compared to existing solutions.

\subsection{Robustness to noise strength}

Since our neural network decoder learns an implicit noise model from training data rather than relying on explicit noise parameters, adapting it to new noise models is challenging. Therefore, it is important to evaluate its performance across different physical error rates $p$.

Fig. \ref{fig:SubThreshold} compares our neural network decoder with the BP-OSD decoder on both BB code and Color code under varying error rates. The neural network decoder, trained at $p=0.005$, is applied directly across all error rates without fine-tuning. In contrast, the BP-OSD decoder incorporates the actual error rates as prior information during decoding. Our decoder outperforms BP-OSD across all test cases, even with the performance gap widening at lower error rates - far from the training condition of $p=0.005$.

To quantify the performance scaling, we analyze the sub-threshold behavior using the established scaling formula\cite{Wang2003confinement,xu2024constant}:
\begin{equation}
    \text{LER} = A(p/p_{\text{th}})^{\alpha n^\beta / 2}
\end{equation}\label{eq:sub_threshold}
where $p$ is the physical error rate, $n$ is the block length, representing the number of physical qubits needed in a minimum encoding unit, and LER is the logical error rate. Among the fitting parameters, two are particularly significant: the threshold physical error rate $p_\text{th}$ and the scaling factor $\beta$. The threshold $p_\text{th}$ represents the maximum physical error rate at which increasing code distance reduces logical error rate, while $\beta$ quantifies the effectiveness of increasing block length.

Our experiments show that the neural decoder improves the color code parameters from $p_\text{th}=0.006056$, $\beta = 0.4108$ to $p_\text{th}=0.006610$, $\beta = 0.5060$, and the BB code parameters from $p_\text{th}=0.007282$, $\beta = 0.8458$ to $p_\text{th}=0.006472$, $\beta = 1.250$. These results reveal two distinct patterns: For the color code, the significant higher threshold and similar scaling factor suggest consistent improvement across all code sizes. For the BB code, the significantly higher scaling factor indicates that larger block lengths benefit more from the neural decoder in small $p$ region, highlighting the limitations of BP-OSD for large-scale codes.
% TODO: add conclusion with 288 results

\section{Discussion}

In this work, we have developed a universal framework for quantum error correction decoding using graph neural networks. Our decoder demonstrates superior performance in both accuracy and speed compared to existing approaches. While the framework's architecture is conceptually straightforward, its consistent success across diverse test cases validates the potential of neural network-based approaches.

The challenges in developing neural network decoders differ fundamentally from those of traditional decoders. Rather than algorithmic design, the primary obstacles lie in network training. Our work, along with other studies, has shown that the required training samples increase rapidly with block length. These challenges help explain why it has taken considerable time for neural decoders to surpass traditional approaches since their initial conception. Our successful implementation on the \qcode{144}{12}{12} BB code, which sets a new scale record for neural network decoders, was enabled not only by improved neural architecture design but also by recent advances in AI infrastructure.

Despite the substantial training requirements, we contend that neural network approaches represent the most promising path toward real-time decoding for complex quantum error correction schemes. While traditional decoders like BP-OSD and maximum weight perfect matching(MWPM) become increasingly slower and inaccurate with code complexity, neural network decoders encapsulate this complexity in the training phase while maintaining constant inference time. This characteristic is crucial for fault-tolerant quantum devices, where real-time decoding efficiency is paramount and extended pre-deployment training is acceptable.

Several important limitations remain in our current work. Our evaluation focuses solely on quantum memory experiments, which in our temporal graph framework corresponds to simple graphs. Many practical quantum operations, such as lattice surgery, involve diverse dynamic graph structures as the Tanner graph evolves. Furthermore, the question of model generalization across different graphs remains open - a capability that could significantly reduce training costs for larger codes.

As a graph-based machine learning approach, our decoder's key strength lies in its direct operation on the mathematical structure of quantum codes. This enables a unique combination of universality and efficiency - the decoder automatically learns optimal strategies from any stabilizer code's graph representation while maintaining constant inference time regardless of code complexity. Our success with large QLDPC codes like the \qcode{144}{12}{12} BB code, which has long challenged traditional decoders, demonstrates how modern machine learning can overcome established bottlenecks in quantum error correction. The decoder's ability to significantly outperform specialized algorithms across diverse code families, while requiring only a single training framework, points to a fundamental shift in how we approach quantum error correction. While the field continues to develop novel quantum codes and hardware architectures, our results suggest that graph neural networks could serve as a universal computational layer bridging theoretical code design and practical quantum computing implementations.


\section{Acknowledgments}

This work is partially supported by the National Key R\&D Program of China(NO.2022ZD0160101) the Shanghai Municipal Science and Technology Commission (Grant No. 24DP2600300), and Shanghai Artificial Intelligence Laboratory. 

We thanks Yi-Ming Zhang for his helpful suggestions on quantum circuit simulation. All technical details will be made publicly available upon the publication of this paper, and all research code will be made open-sourced as well.

% If your text is very short you might need to uncomment the following line to avoid
% layout problems with the figures and tables.
%\newpage


%%%%%%%%%%%%%%%% MAIN TEXT FIGURES %%%%%%%%%%%%%%%


\begin{figure}
\centering
\includegraphics[width=\textwidth]{myres/WorkingFlow.pdf}
\caption{\textbf{Overview of the stabilizer code decoding framework}. A) The stabilizer generator matrix that defines the stabilizer group. For CSS codes, this matrix adopts a block diagonal form, separating X-type and Z-type stabilizers. B) The logical Pauli operators, expressed as tensor products of single-qubit Pauli operators, which define the encoded logical qubits. C) The extended Tanner graph construction derived from the stabilizer generator matrix and logical operators. D) The decoding workflow: The syndrome extraction circuit, where check qubits ($Z_i$) interact with data qubits through CNOT gates. After each cycle, check qubits are measured in the Z basis, reset to $\left| 0 \right>$, and their measurement outcomes $S_i$ are mapped to corresponding nodes in the extended Tanner graph. The neural decoder then corrects each logical qubit based on the input sequence of the graph.}\label{fig:WorkingFlow}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{myres/UniversalDecodingAcc.pdf}
\caption{\textbf{Decoding Performance on Simulated Quantum Memory Experiments.} 
Logical error rates for various decoders across different code families under uniform depolarizing noise with $p=0.005$. The x-axis represents the encoding rate $k/n$, while the y-axis shows the corresponding logical error rate after decoding. The red dashed line (break-even line) indicates the threshold where the logical error rate equals the physical error rate. Below this line, the logical error rate is lower than the physical error rate.}
\label{fig:UniversalDecodingAcc}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{myres/UniversalDecodingTime.pdf}
\caption{
\textbf{Decoding Speed Comparison Between BP-OSD and Neural Decoders.} 
All tests were conducted on a system equipped with $2\times$ AMD EPYC 7H12 64-Core Processors and a single NVIDIA GeForce RTX 4090 GPU. 
(a) Decoding time as a function of the number of error correction cycles. Each code was tested using cycle lengths that are multiples of its respective code distance $d$, under uniform depolarizing noise with $p=0.005$. Both the BP-OSD decoder and the neural decoder were run with a batch size of 1. 
(b) Per-cycle latency measured with a batch size of 1. 
(c) Throughput measured with a batch size of 1000, allowing the decoders to fully utilize the computational power of the hardware.
}
\label{fig:UniversalDecodingTime}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{myres/CompareGoogle.pdf}
\caption{\textbf{The decoder performance on Sycamore surface code data.} We compare our network with other neural network decoders in terms of both accuracy and model size. The performance of traditional decoders is shown as dashed lines for reference.}\label{fig:CompareGoogle}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{myres/ThresholdFit.pdf}
\caption{\textbf{The sub-threshold scaling of decoder performance on different code.} The diamond points represents the BP-OSD results, and the square points represents the neural network results, our decoder performs better than BP-OSD in all case without retraining. The dash-lines is the fitting curve, where fitting parameters are decided separately for each decoder and each code family, and is shared inside each group. The $d=3$ color code is excluded from the fitting to reduce the influence of finite size effect.}\label{fig:SubThreshold}
\end{figure}

% \begin{figure}
% \centering
% % \includegraphics[width=\textwidth]{myres/sub_threshold.pdf}
% \caption{The time scaling of nerual decoder.}\label{TimeScaling}
% \end{figure}

%%%%%%%%%%%%%%%% MAIN TEXT TABLES %%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%% REFERENCES %%%%%%%%%%%%%%%

\clearpage % Clear all remaining figures and tables then start a new page

% The list of references goes after the main text and before the acknowledgements
% When preparing an initial submission, we recommend you use BibTeX, like this:
%
\bibliography{main} % for a file named science_template.bib
\bibliographystyle{sciencemag}

%%%%%%%%%%%%%%%% ACKNOWLEDGEMENTS %%%%%%%%%%%%%%%


\end{document}