\clearpage
\appendix 
\section{Supplementary Illustration}
\subsection{\textit{Space Syntax}}
\label{apdx:space_syntax}
\textit{Space Syntax} was proposed and developed by British architectural scholars Bill Hillier and Julienne Hanson at University College London (UCL) in the 1970s. Hillier et al. analyzed London's street network through spatial syntax, revealing the relationship between street structure and crime rate, and found that streets with high integration are more easily monitored and have lower crime rates. 

In general, \textit{Space Syntax} provides a unique perspective to understand and optimize spatial structure, reveals the profound impact of spatial configuration on human behavior and social interaction through quantitative analysis, and provides a scientific basis for urban and architectural design.

Four types of \textit{Space Syntax} features are applied in our model, and Figure ~\ref{fig:space_syntax} provides a vivid explanation of them.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.40\textwidth] 
    {figure/space_syntax.pdf}
    \caption{
        Four types of \textit{Space Syntax} concepts: (a) node $i$ has a larger Total Depth than node $j$; (b) node $i$ is in the center of network, with larger Integration than node $j$; (c) Connectivity is only related with neighborhood nodes, node $i$ 's Connectivity is $5$; (d) node $i$ is a transportation hub in this network, so it has larger Choice than other nodes.       
    }
    \label{fig:space_syntax}
\end{figure}

\subsection{Max Entropy Inverse Reinforcement Learning}
\label{apdx:irl}
Below, we derive the preference learning process from the perspective of maximum entropy inverse reinforcement learning. 
We treat the negative value of road preference weights as rewards. The maximum entropy learning assumes that the reward function is set in such a way that the entropy of the probability of expert trajectories is maximized. Let $P(\tau|\omega)$ denote the probability of trajectory $\tau$ given the reward function parameters $\omega$. Then, maximum entropy inverse reinforcement learning solves the following problem under the feature matching constraint,
\begin{equation}
\begin{aligned}
    &\omega^{*} = \arg \max _\omega \sum_\tau -P(\tau|\omega)\log P(\tau | \omega) \\
    &\text{s.t.} \begin{cases}
    \sum_\tau P(\tau | \omega) \sum_i p(\tau_i) = \sum_{\tau\in \mathcal{T}} P(\tau | \omega) \sum_i p(\tau_i)  \\
    \sum_\tau P(\tau | \omega) = 1
    \end{cases}
\end{aligned}
\end{equation}
where $\mathcal{T}$ is the expert trajectory dataset, and $p(\tau_i)$ is the preference for $\tau_i$.

According to the assumptions in ~\cite{max_ent}, 
the above optimization problem is equivalent to solving the following maximum likelihood problem
\begin{equation}
\begin{gathered}
    \arg \max_\omega \sum_{\tau\in \mathcal{T}} \log P(\tau|\omega), \\ 
    P(\tau|\omega) =\frac{1}{Z(\omega)} \exp\left(-\sum_i p(\tau_i) \right), \\
    Z(\omega) =\sum_\tau \exp\left(-\sum_i p(\tau_i)\right). 
\end{gathered}
\end{equation}
The partition function $Z(\omega)$ represents the sum of the trajectory logit of the best strategies under the given $\omega$ setting.
The gradient of the likelihood function is calculated as follows
\begin{equation}
\begin{split}
    &\nabla_\omega \sum_{\tau\in \mathcal{T}} \log P(\tau|\omega) \\
    &=\nabla_\omega\left( -\sum_{\tau\in \mathcal{T}}\sum_i p(\tau_i)\right)  - |\mathcal{T}|\nabla_\omega\log Z(\omega),     
\end{split}
\end{equation}
Where the gradient of the second term is derived as follows
\begin{equation}
\begin{split}
    \nabla_\omega \log Z(\omega) &= \sum_\tau \frac{\exp \left( \sum_i -p(\tau_i) \right)}{Z(\omega)} \left(-\sum \nabla_\omega p(\tau_i, \omega) \right) \\ 
    &=-\sum_\tau P(\tau|\omega)\sum \nabla_\omega p(\tau_i)
\end{split}
\end{equation}
Since computing the entire space of possible trajectories is computationally infeasible, we use Monte Carlo sampling to approximate this gradient term. Specifically, in each iteration, we generate the shortest path trajectory $\hat{\tau}$, which corresponds to the real trajectory $\tau$. We then compute the rewards for both under the current preference settings and calculate the gradient.
\begin{equation}
    \nabla_\omega \log P(\tau|\omega) = \nabla_\omega \left( -\sum_i p(\tau_i) + \sum_{i} p(\hat \tau_i) \right)
\end{equation}
Therefore, maximizing the above likelihood function is equivalent to minimizing our loss function
\begin{equation}
    \mathcal{L}^{(pref)} = \frac{1}{\left| \mathcal{T}^{(src)} \right|} \sum_{\tau_{ij} \in \mathcal{T}^{(src)}} \left[ p \left( \tau_{ij} \right) - \hat{p} \left( \hat{\tau}_{ij} \right)\right].
\end{equation}

\section{Datasets}
\subsection{Datasets Description}
\label{apdx:data_desc}
The BJ dataset contains real GPS trajectory data of Beijing taxis from November 1 to 30, 2015 within Beijingâ€™s Fourth Ring Roads, which is sampled every minute. The XA and CD datasets are originally released by DIDI Chuxing. The detailed information of the three datasets is shown in the Table~\ref{tab:dataset_statistic}. These datasets were chosen due to the number of trajectories and the time intensive collection.
\begin{table}[t]
  \centering
  \captionsetup{skip=5pt}
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{c|ccc}
        \toprule
          Datastatistics & BJ & XA & CD \\
        \midrule
          Time Period & Nov. 2015 & Nov. 2018 & Nov. 2018 \\
          Trajectories & 2344762 & 804302 & 1252233 \\
          Road Segments & 14685 & 4147 & 3514 \\
          Average Hops & 24.0 & 13.4 & 14.4 \\
          Average Travel Distance(km) & 4.10 & 2.38 & 2.14 \\
        \bottomrule
        \end{tabular}
    }
    \caption{Datasets Statistics}
    \label{tab:dataset_statistic}
\end{table}

\subsection{Data Processing}
\label{apdx:data_proc}
To eliminate abnormal trajectories, we remove trajectories with lengths of less than 3 steps and trajectories with loops. We partitioned the trajectory datasets of each city into training and testing sets. The supervised training label, including travel time and travel speed for each road segment, are generated from the training set, while the testing set was used to evaluate the quality of the generated trajectories.

Our model needs to predict travel costs, therefore, we need to compute travel cost labels for each road segment based on trajectory dataset. The map-matching algorithm can output the travel time on each road segment for each trajectory. Because the travel cost of roads varies dynamically over time, we divide a day into 24 time slots and calculate the average travel time of roads within each hour. For each road segment, we collect all travel time samples within a time slot and filter out outliers exceeding $3 \sigma $. Finally, we calculate the average of the remaining samples as the label for the road segment's travel cost. After completing the data preprocessing, the basic information of the three datasets is shown in Table ~\ref{tab:dataset_statistic}. 

\section{Implementation Details}
\begin{table*}[t]
    \centering
    \resizebox{1.0\textwidth}{!}{
      \begin{tabular}{c|c|ccccccc|ccccccc}
      \toprule
        \multirow{2}{*}{Target} & \multirow{2}{*}{\makecell{\#Traj \\ ($\times 10^3$)}} & \multicolumn{7}{c|}{Source City XA for BJ, BJ for CD and CD for XA} & \multicolumn{7}{c}{Source City BJ for XA, XA for CD and CD for BJ} \\
        \cmidrule{3-16}
         &  & \makecell{Distance \\ ($\times 10^{-3}$)} & \makecell{Radius \\ ($\times 10^{-3}$) } & LocFreq & Hausdorff & DTW  & EDT & EDR & \makecell{Distance \\ ($\times 10^{-3}$)} & \makecell{Radius \\ ($\times 10^{-3}$)} & LocFreq & Hausdorff & DTW  & EDT & EDR \\
      \midrule

    \multirow{6}{*}{BJ} & 0.0 & 0.636 & 0.111 & 0.041 & 0.292 & 4.89 & 8.73 & 0.190 & 0.643 & 0.182 & 0.043 & 0.315 & 5.27 & 9.17 & 0.205 \\ 
        & 0.1 & 0.612 & 0.122 & 0.036 & 0.289 & 4.88 & 8.48 & 0.186 & 0.455 & 0.339 & 0.040 & 0.311 & 5.08 & 9.01 & 0.203 \\ 
        & 0.4 & 0.533 & 0.189 & 0.035 & 0.283 & 4.63 & 8.48 & 0.184 & 0.461 & 0.352 & 0.041 & 0.308 & 4.96 & 9.04 & 0.202 \\ 
        & 1.6 & 0.561 & 0.150 & 0.033 & 0.274 & 4.47 & 8.30 & 0.179 & 0.474 & 0.275 & 0.038 & 0.300 & 4.86 & 8.85 & 0.197 \\ 
        & 6.4 & 0.689 & 0.095 & 0.034 & 0.262 & 4.19 & 8.32 & 0.172 & 0.582 & 0.121 & 0.035 & 0.264 & 4.20 & 8.18 & 0.174 \\ 
        & 12.8 & 0.620 & 0.114 & 0.028 & 0.250 & 4.02 & 8.00 & 0.164 & 0.574 & 0.124 & 0.031 & 0.254 & 4.05 & 7.96 & 0.167 \\ 
    \midrule
    \multirow{6}{*}{XA} & 0.0 & 4.375 & 0.206 & 0.042 & 0.188 & 2.32 & 4.29 & 0.142 & 4.039 & 0.228 & 0.040 & 0.187 & 2.34 & 4.23 & 0.135 \\ 
        & 0.1 & 4.468 & 0.345 & 0.050 & 0.192 & 2.45 & 4.67 & 0.148 & 4.679 & 0.825 & 0.040 & 0.194 & 2.52 & 4.36 & 0.143 \\ 
        & 0.4 & 5.951 & 0.464 & 0.067 & 0.207 & 2.72 & 5.05 & 0.159 & 4.875 & 0.306 & 0.044 & 0.188 & 2.30 & 4.38 & 0.147 \\ 
        & 1.6 & 4.285 & 0.175 & 0.043 & 0.181 & 2.13 & 4.38 & 0.139 & 4.233 & 0.334 & 0.036 & 0.184 & 2.21 & 4.17 & 0.139 \\ 
        & 6.4 & 4.103 & 0.219 & 0.033 & 0.171 & 1.95 & 4.09 & 0.130 & 3.976 & 0.139 & 0.029 & 0.174 & 2.07 & 4.00 & 0.129 \\ 
        & 12.8 & 4.128 & 0.253 & 0.029 & 0.170 & 1.93 & 4.01 & 0.128 & 4.034 & 0.104 & 0.027 & 0.170 & 1.95 & 3.96 & 0.125 \\ 
    \midrule
    \multirow{6}{*}{CD} & 0.0 & 5.109 & 0.233 & 0.027 & 0.117 & 1.19 & 3.59 & 0.109 & 4.902 & 0.248 & 0.026 & 0.125 & 1.34 & 3.63 & 0.117 \\ 
        & 0.1 & 6.124 & 0.205 & 0.028 & 0.126 & 1.30 & 3.74 & 0.119 & 4.616 & 0.284 & 0.028 & 0.129 & 1.35 & 3.68 & 0.119 \\ 
        & 0.4 & 4.673 & 0.233 & 0.022 & 0.121 & 1.26 & 3.52 & 0.112 & 4.441 & 0.187 & 0.025 & 0.124 & 1.31 & 3.58 & 0.116 \\ 
        & 1.6 & 4.219 & 0.192 & 0.022 & 0.116 & 1.18 & 3.37 & 0.104 & 4.355 & 0.217 & 0.023 & 0.120 & 1.25 & 3.47 & 0.110 \\ 
        & 6.4 & 4.473 & 0.207 & 0.019 & 0.112 & 1.12 & 3.24 & 0.102 & 4.291 & 0.208 & 0.023 & 0.121 & 1.26 & 3.49 & 0.111 \\ 
        & 12.8 & 4.407 & 0.117 & 0.018 & 0.113 & 1.12 & 3.25 & 0.102 & 4.639 & 0.221 & 0.024 & 0.123 & 1.25 & 3.54 & 0.113 \\ 
      \bottomrule
      \end{tabular}%
    }
    \caption{Target City Fine-tune Experiment Results}
    \label{tab:finetune_total}%
\end{table*}%

\begin{table*}[t]
    \centering
    \resizebox{1.0\textwidth}{!}{
      \begin{tabular}{c|c|ccccccc|ccccccc}
      \toprule
        \multirow{2}{*}{Target} & \multirow{2}{*}{Method} & \multicolumn{7}{c|}{Source City XA for BJ, BJ for CD and CD for XA} & \multicolumn{7}{c}{Source City BJ for XA, XA for CD and CD for BJ} \\
        % \cline{3-16}
        \cmidrule{3-16}
         &  & \makecell{Distance \\ ($\times 10^{-3}$)} & \makecell{Radius \\ ($\times 10^{-3}$) } & LocFreq & Hausdorff & DTW  & EDT & EDR & \makecell{Distance \\ ($\times 10^{-3}$)} & \makecell{Radius \\ ($\times 10^{-3}$)} & LocFreq & Hausdorff & DTW  & EDT & EDR \\
      \midrule

      \multirow{4}{*}{BJ} & w/o Cost & 1.505 & 1.190 & 0.082 & 0.373 & 6.99 & 12.47 & 0.248 & 1.704 & 2.969 & 0.120 & 0.444 & 9.99 & 15.62 & 0.290 \\ 
        & w/o Pref & 0.444 & 0.564 & 0.039 & 0.303 & 5.07 & 8.97 & 0.199 & 0.448 & 0.627 & 0.040 & 0.308 & 5.08 & 9.01 & 0.201 \\ 
        & w/o SS & 0.439 & 0.363 & 0.042 & 0.297 & 4.95 & 9.08 & 0.194 & 0.511 & 0.346 & 0.056 & 0.370 & 6.30 & 10.12 & 0.238 \\ 
        & \name & 0.636 & 0.111 & 0.041 & 0.292 & 4.89 & 8.73 & 0.190 & 0.643 & 0.182 & 0.043 & 0.315 & 5.27 & 9.17 & 0.205 \\ 
      \midrule
      \multirow{4}{*}{XA} & w/o Cost & 17.745 & 3.763 & 0.118 & 0.274 & 3.52 & 6.98 & 0.227 & 19.171 & 5.066 & 0.119 & 0.264 & 3.78 & 8.51 & 0.260 \\ 
        & w/o Pref & 4.641 & 1.052 & 0.062 & 0.219 & 3.00 & 5.03 & 0.164 & 4.356 & 0.293 & 0.047 & 0.198 & 2.54 & 4.61 & 0.146 \\ 
        & w/o SS & 5.261 & 2.817 & 0.059 & 0.228 & 3.15 & 4.95 & 0.163 & 3.685 & 0.673 & 0.041 & 0.199 & 2.57 & 4.49 & 0.144 \\ 
        & \name & 4.375 & 0.206 & 0.042 & 0.188 & 2.32 & 4.29 & 0.142 & 4.039 & 0.228 & 0.040 & 0.187 & 2.34 & 4.23 & 0.135 \\ 
      \midrule
      \multirow{4}{*}{CD} & w/o Cost & 6.403 & 2.483 & 0.074 & 0.198 & 2.47 & 6.77 & 0.195 & 4.957 & 0.217 & 0.037 & 0.140 & 1.61 & 3.94 & 0.127 \\ 
        & w/o Pref & 10.607 & 0.496 & 0.035 & 0.136 & 1.41 & 4.09 & 0.128 & 7.616 & 0.257 & 0.029 & 0.131 & 1.37 & 3.90 & 0.120 \\ 
        & w/o SS & 5.061 & 0.348 & 0.033 & 0.133 & 1.37 & 3.95 & 0.123 & 5.095 & 0.362 & 0.035 & 0.137 & 1.42 & 4.09 & 0.129 \\ 
        & \name & 5.109 & 0.233 & 0.027 & 0.117 & 1.19 & 3.59 & 0.109 & 4.902 & 0.248 & 0.026 & 0.125 & 1.34 & 3.63 & 0.117 \\ 
      \bottomrule
      \end{tabular}%
      }
    \caption{Ablation Study Results}
    \label{tab:ablation_total}%
\end{table*}%

\subsection{Details of Baselines}
To evaluate our method, the following trajectory generation models are selected as baselines.
\begin{itemize}
    \item Random Walk~\cite{node2vec}(RW): This method, as utilized in Node2Vec~\cite{node2vec}, involves simulating city mobility by performing random walks on the graph. We analyzed the trajectory length and the distribution characteristics of starting nodes in the dataset and then executed random walks on the graph to simulate city mobility.

    \item EPR Models: Including Density-EPR~\cite{epr_1}(DE) and Spatial-EPR~\cite{epr_2}(SE). These models characterize human behavior into two patterns: Explore and Preferential Return. They introduce gravity models to simulate the impact of group mobility on individuals. Empirical parameters are set to accomplish sampling.
    
    \item TrajGen~\cite{trajgen}(TG): This method utilizes a CNN-based GAN to generate the synthetic trajectory image. Then, it extracts locations from the image and uses a Seq2Seq model to infer the real trajectory sequence. 

    \item SeqGAN~\cite{SeqGAN}(SG): This method is the classical sequence generation method, which combines policy gradient with GAN to solve sequence generation problem.

    \item SVAE~\cite{SVAE}(SV): This method is the first to combine the variational autoencoder with the Seq2Seq model to generate mobility trajectory data.
    
    \item MoveSim~\cite{MoveSim}(MS): This method builds a self-attention-based trajectory generator and designs a mobility regularity-aware discriminator to train the generator in the reinforcement learning paradiagm.
    
    \item TS-TrajGen~\cite{ts_trajgen}(TT): This method combines the A* search algorithm with neural networks to model agent policy, and proposes a two-stage adversarial generative network to efficiently generate trajectory data.
    
    \item DiffTraj ~\cite{difftraj}(DT): This method utilize the generative abilities of diffusion model to reconstruct and synthesize geographic trajectories from white noise through a reverse trajectory denoising process.
    
    \item VOLUNTEER ~\cite{volunteer}(VO): This method employs Variational Autoencoder (VAE) to model the complex spatio-temporal distribution of users and obtain accurate trajectory simulations.
\end{itemize}
In the deeplearning methods, DiffTraj and VOLUNTEER do not incorporate travel demand information as input to the model. SeqGAN, SVAE, and MoveSim only take the starting road segment as input to the model. In contrast, TS-TrajGen and our model take both the starting and destination road segments from the travel demand as input, making them more suitable for simulating trajectory data for given travel demands. 

\subsection{Evaluation Metrics}
\label{apdx:metrics}
In the macro-similarity perspective, what we focus on is the overall statistical distribution of the trajectory dataset.
We evaluate the quality of the generated data by comparing the similarity of mobility patterns and urban traffic state metrics between the generated and real data. To obtain quantitative results, we use Jensen-Shannon Divergence(JS-Divergence) to calculate the similarity of real trajectory dataset $\mathcal{T}$ and generated trajectory dataset $\mathcal{\hat{T}}$, as follows
\begin{equation}
    Sim^{(mac)} = \text{JSD}(P(\mathcal{T}), P(\mathcal{\hat{T}})).
\end{equation}
In detail, we calculate the JS-Divergence in the following aspects.
\begin{itemize}
    \item Distance: Travel distance, which represents the spatial length of a trajectory.
    \item Radius: Radius of gyration, which represents the spatial travel range of a trajectory.
    \item LocFreq: Visit frequency distribution of each single road segment, which indicates the popularity of roads.
\end{itemize}

In the micro-similarity perspective, we focus on measuring the
sequence distance between the real trajectories and the generated trajectories with the same travel demand, as follows
\begin{equation}
    Sim^{(mic)} = \frac{1}{N} \sum_{\tau \in \mathcal{T}, \hat{\tau} \in \hat{\mathcal{T}}} \text{SeqDist}(\tau, \hat{\tau}). 
\end{equation}
In practice, we use four widely used trajectory distance metrics to
measure the micro similarity.
\begin{itemize}
    \item Hausdorff distance: The Hausdorff distance quantifies the maximum distance from any point in one set to the nearest point in another set.
    \item DTW: It finds an optimal alignment between sequences by warping the time axis to minimize the cumulative distance between corresponding points.
    \item EDT: It calculates the minimum edit distance between two sequence. 
    \item EDR: Edit distance on real sequence, a variant of editing distance.
\end{itemize}

\subsection{Model Settings}
All experiments were conducted on a machine equipped with an NVIDIA GeForce 3090 GPU, running the Ubuntu 20.04 operating system. The model was implemented using Pytorch 1.12.1. The number of SAGAT layer is $6$. During 
training, the number of clusters $K$ varies according to the size of the dataset; a batch of data consists of $k=3$ clusters; the learning rate is set to $10^{-5}$; and the training epochs is $600$. The balancing weights in the loss functions are: $\lambda_r = 50$, $\lambda_d = 100$ and $\lambda_g = 5$.

\section{Experiment Result}
\label{apdx:result}
We completed six groups of experiments on three datasets, with each of the three cities serving as both the source and target city in different combinations, to evaluate the effectiveness of fine-tune in the target cities and the impact after ablating key submodules. The complete results of the target city fine-tune experiment are shown in the Table~\ref{tab:finetune_total} and the complete results of ablation study are shown in the Table~\ref{tab:ablation_total}.
