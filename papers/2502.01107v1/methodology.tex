\section{Methodology}
\label{sec:methodology}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.46\textwidth] 
    {figure/framework.pdf}
    \caption{
        Overview of the framework
    }
    \label{fig:framework}
\end{figure}

\subsection{Framework}

As is shown in Figure~\ref{fig:framework}, our trajectory generation framework consists of three modules: Topological Feature Extraction, Travel Cost Prediction and Preference Learning. 
The training is performed using trajectory data in the source city and road network of both source city and target city. Trajectories are generated by inferring the preference values of each road segment and searching the shortest path.

\subsection{Topological Feature Extraction}
\subsubsection{\textit{Space Syntax} Feature Extraction}
\textit{Space Syntax}~\cite{hillier1976space} is a theory for analyzing and understanding spatial structure of cities and buildings. 
We use four types of concepts in \textit{Space Syntax} to describe the topological features of the road network.

\textbf{Total Depth} refers to the sum of the step depth (SD) from a given road segment to all other road segments within a certain range. SD is the minimum number of steps required to reach other target segment from a starting road segment.
\begin{equation}
    x_i^{(td)} = \sum_{j\neq i} \text{Len}(\text{ShortestPath}(r_i, r_j)).
\end{equation}
    
\textbf{Integration} measures the centrality of a road segment within the entire road network. The integration for road segment $r_i$ can be formulated as 
\begin{equation}
    x_i^{(in)} = \frac{\text{NC}(r_i)^2}{x_i^{(td)}}, 
\end{equation}
where $\text{NC}(r_i)$ refers to the total number of road segments that need to be traversed to reach all other road segments starting from $r_i$.

\textbf{Connectivity} indicates the number of directly connected neighbors to a road segment, which can be calculated as
\begin{equation}
    x_i^{(co)} = \text{Degree}(r_i) = \sum_j a_{ij}.
\end{equation}
If $r_i$ is adjacent to $r_j$, $a_{ij}$ equals $1$; otherwise, it equals $0$.

\textbf{Choice}, also known as Betweenness, reflects how often a road segment is likely to be encountered when moving through the space. The Choice for road segment $r_i$ is calculated by counting how many times it lies on the shortest paths between all pairs of segments $r_j$ and $r_k$, as follows
\begin{equation}
    x_i^{(ch)} = \sum_{j,k}{\delta_{ijk}}, 
\end{equation}
where $\delta_{ijk}$ indicates whether the shortest path from $r_j$ to $r_k$ passes $r_i$, formulated as 
\begin{equation}
    \delta_{ijk} = 
    \begin{cases}
        1, & \text{if} \  r_i \in \text{ShortestPath}(r_j, r_k), \\
        0, & \text{other}.
    \end{cases}
\end{equation}

Finally, The \textit{Space Syntax} features and the basic features of road segments, including length $x_i^{(le)}$, type $x_i^{(tp)}$ and direction $x_i^{(dr)}$, are concatenated as
\begin{equation}
    \bm{x}_{i} = x_i^{(td)} \| x_i^{(in)} \| x_i^{(co)} \| x_i^{(ch)} \| x_i^{(le)} \| x_i^{(tp)} \| x_i^{(dr)} \| t,
\end{equation}
where time slices index $t$ is incorporated to account for the varying travel costs of roads over time. Discrete variables are encoded through the embedding layer.

\subsubsection{Topological Feature Aggregation}
The raw road segment features are aggregated by GNNs to obtain representations with richer topological information.

Inductive GCN ~\cite{inductive_gcn} methods address generalization performance issues through subgraph learning. Sampling a variety of sub-graph samples can help further improve performance, but it is not possible to sample the entire graph due to computational efficiency. A compromise is to use the Metis algorithm, which partitions the entire road network into $K$ subgraphs, and $k$ of them are randomly sampled to form a training batch~\cite{ClusterGCN}, as follows
\begin{equation}
\begin{gathered}
    \left\{ \mathcal{\widetilde{G}}_1,  \mathcal{\widetilde{G}}_2, \cdots, \mathcal{\widetilde{G}}_K \right\} = \text{METIS} \left( \mathcal{G}, K \right), \\
    \mathcal{G}_k = \text{RandomSample} \left( \left\{ \mathcal{\widetilde{G}}_1,  \mathcal{\widetilde{G}}_2, \cdots, \mathcal{\widetilde{G}}_K \right\}, k \right).
\end{gathered}
\end{equation}

Next, Spatial Aware Graph Attention Networks (SAGAT) is designed to process the subgraph samples and obtain the aggregated representations. 
The SAGAT is a GATv2 ~\cite{GATv2} network in which we integrate spatial relationships between road segments to enhance the model's spatial awareness. This process could be formulated as 
\begin{equation}
    \left\{ \bm{h}_i \Big| r_i \in \mathcal{G}_k \right\} = \text{SAGAT} \left( \mathcal{G}_k \right).
\end{equation}

SAGAT is composed of multiple GAT layers stacked together, where the input of the first layer is a linear transformation of the features.
\begin{equation}
    \bm{h}_{i}^{(0)} = \text{MLP}(\bm{x}_{i}).
\end{equation} 
For the layer $l + 1$, the attention weights are calculated with the output of last layer $\bm{h}_{i}^{(l)}$, as follows
\begin{equation}
\begin{gathered}
    \alpha_{ij}^{(l+1)} = \frac{\exp{ \left( e_{ij}^{(l)} \right)}}{\sum_{j'\in \mathcal{N}_i}\exp \left( e_{ij'}^{(l)} \right)}, \\
    e_{ij}^{(l)} 
    = \bm{a}^\top \sigma \left( \bm{W}_s \bm{h}_i^{(l)} + \bm{W}_t \bm{h}_j^{(l)} + \bm{W}_e  \bm{s}_{ij} \right),
\end{gathered}
\end{equation}
where $\bm{a}$, $\bm{W}_s$, $\bm{W}_t$, $\bm{W}_e$ are learnable parameters, $\mathcal{N}_i$ is the neighbors of the road segment $r_i$ and $\sigma$ is LeaklyReLU function.
$\bm{s}_{ij}$ represents the spatial relationships between road segment $r_i$ and $r_j$, formulated as
\begin{equation}
    \bm{s}_{ij} =  \text{Bet}(r_i, r_j)\| \text{Angle}(r_i, r_j)\| \text{Dist}(r_i, r_j),
\end{equation}
where $\text{Bet}(r_i, r_j)$ is the Betweenness of road segment pair $r_i$ and $r_j$, which is defined as the ratio of the number of shortest paths that traverse $r_i$ and $r_j$ to the total number of shortest paths in the entire network. $\text{Angle}(r_i, r_j)$ represents the turning angle, and $\text{Dist}(r_i, r_j)$ represents the travel distance from the center of road segment.

The output of layer $l + 1$ is the weighted aggregation of the representations of neighboring nodes, as follows
\begin{equation}
    \bm{h}^{(l+1)}_{i} = \sigma \left(\sum_{j \in \mathcal{N}_i} \alpha_{ij}^{(l+1)} \bm{h}^{(l)}_{j}\right) + \bm{h}_{i}^{(l)},
\end{equation}
where $\sigma$ is the ReLU function. 

\subsection{Travel Cost Prediction with Disentangled Representation}

After aggregating the topological features, we aim to predict the travel cost based on the representation of the road segments while ensuring good generalization to the target city. 


The difference in representation distribution between source and target cities reducing the model’s generalization ability. To address this, disentangled learning and adversarial domain adaptation are used to create city-invariant representations. Building on ~\cite{disent_ijcai}, the method assumes road segment information is determined by two independent latent variables: a semantic latent variable $\bm{z}^{(s)}$ and a domain latent variable $\bm{z}^{(d)}$. These are extracted using a semantic and a domain encoder, respectively. $\bm{z}^{(s)}$ captures semantic information for predicting trajectory costs, while $\bm{z}^{(d)}$ contains city-specific domain information, as follows
\begin{equation}
    \label{equ:dis_encoder}
    \begin{split}
    \bm{z}_i^{(s)} &= \text{SemEncoder} \left( \bm{h}_i \right), \\
    \bm{z}_i^{(d)} &= \text{DomEncoder} \left( \bm{h}_i \right).
    \end{split}
\end{equation}

The adversarial domain adaptation technique is used to train these representations, incorporating a predictor for travel cost estimation and a discriminator for city identification.


\subsubsection{Travel Cost Prediction}

The travel cost, represented by the average travel time and speed of road segments during specific periods, is denoted as
\begin{equation}
    \bm{y}_i = \left\{ y_i^{(m)} \Big| m \in \{\text{time, speed}\} \right\},
\end{equation}
 where $m$ denotes the type of cost. Using the disentangled latent variable $\bm{z}_i$ as input (with superscripts omitted for simplicity), the travel cost prediction network is formulated as
\begin{equation}
\hat{\bm{y}}_i = \log \left( 1 + \exp \left( \text{MLP} \left(\bm{z}_i \right) \right) \right),
\end{equation}
The prediction loss function comprises a MSE loss and a Rank loss. The MSE loss is 
\begin{equation}
    \mathcal{L}_{mse} = \frac{1}{N_s} \sum_{i} \left \Vert \hat{\bm{y}}_i - \bm{y}_i \right \Vert ^2,
\end{equation}
where $N_s$ is the size of source city dataset.

\begin{figure}[t]
    % \centering
    \subfloat{\includegraphics[width=0.22\textwidth]{figure/speed_density.pdf}}
    \subfloat{\includegraphics[width=0.22\textwidth]{figure/time_density.pdf}}
    \caption{Travel cost (speed and time) distribution in Xi'an and Beijing.}
    \label{fig:cost_distribution}
\end{figure}

Rank loss~\cite{ranknet} is motivated by the challenge of directly predicting absolute travel costs in the cross-city task, where potential biases across different cities can complicate predictions (see Figure \ref{fig:cost_distribution}). This approach emphasizes predicting relative rankings, which is generally less complex than estimating absolute values.

The probability that \(r_i\) has a higher travel cost than \(r_j\) is
\begin{equation}
\hat{q}_{ij}^{(m)} = \text{Sigmoid} \left( \hat{y}_i^{(m)} - \hat{y}_j^{(m)} \right).
\end{equation}
The actual ranking label of sample pair ($r_i$, $r_j$) is 
\begin{equation}
    q_{ij}^{(m)} = 
    \begin{cases}
        1, & \ y_i^{(m)} > y_j^{(m)}, \\
        0, & y_i^{(m)} < y_j^{(m)}.
    \end{cases}
\end{equation}
The binary cross-entropy loss is calculated as follows
\begin{equation}
\begin{gathered}
    l \left( \hat{q}, q \right) = - \left( q \log (\hat{q}) + (1 - q) \log (1 - \hat{q}) \right), \\
    \mathcal{L}_{rank} = \frac{1}{N_s^2} \sum_{i} \sum_{j} \sum_{m} l \left( \hat{q}_{ij}^{(m)}, q_{ij}^{(m)}\right).
\end{gathered}
\end{equation}
The overall loss for the travel cost prediction is
\begin{equation}
    \mathcal{L}_{pred} = \mathcal{L}_{mse} + \lambda_r \mathcal{L}_{rank},
\end{equation}
where $\lambda_r$ is a balance weight between the above two loss.

\subsubsection{Domain Discrimination}
To separate domain information from semantic information, we introduce a discriminator to predict the domain label of the road segment.
We extract subgraph sample from source city or target city and assign a domain label to each road segment, as follows
\begin{equation}
    d_{i} = 
    \begin{cases}
        1, & \ r_i \in \mathcal{G}^{(src)}, \\
        0, & \ r_i \in \mathcal{G}^{(tgt)}.
    \end{cases}
\end{equation}
Given the latent variable $\bm{z}_i$ as input, the domain discriminator is formulated as
\begin{equation}
    \hat{d}_i = \text{Sigmoid} \left( \text{MLP} \left( \bm{z}_i \right) \right).
\end{equation}
Binary cross entropy loss is used for this domain discrimination task, as follows
\begin{equation}
    \mathcal{L}_{dis} = - \frac{1}{N_s + N_t} \sum_i \left( d_i \log \hat{d}_i + ( 1 - d_i) \log ( 1 - \hat{d}_i) \right),
\end{equation}
where $N_s$ and $N_t$ represent the sizes of the source city and target city datasets, respectively.

\subsubsection{Disentangled Adversarial Training} Adversarial training is used to promote the information decoupling of $z_i^{(s)}$ and $z_i^{(s)}$. The ultimate objective for semantic latent variables is to minimize travel cost prediction loss while maximizing domain discrimination loss, whereas for domain latent variables, the goal is the opposite. By using different representations as input, the loss functions are calculated as follows
\begin{equation}
\begin{split}
\mathcal{L}_{total}^{(s)} = \mathcal{L}_{pred}\left(z_i^{(s)}\right) - \lambda_d \mathcal{L}_{dis}\left(z_i^{(s)}\right), \\
\mathcal{L}_{total}^{(d)} = \lambda_d \mathcal{L}_{dis}\left(z_i^{(d)}\right) - \mathcal{L}_{pred}\left(z_i^{(d)}\right).    
\end{split}
\end{equation}

As shown in Figure~\ref{fig:framework}, the GRL layer is used to negate the gradient to achieve efficient adversarial training, which performs an identity transformation in the forward propagation and negates the gradient in the back propagation.
In addition, the orthogonal loss is introduced to further reduce information coupling, calculated as follows
\begin{equation}
\mathcal{L}_{og} = \frac{1}{N_s + N_t}\sum_i \left( \frac{\bm{z}_i^{(s)} \cdot \bm{z}_i^{(d)}}{ \left\| \bm{z}_i^{(s)} \right\| \cdot \left\| \bm{z}_i^{(d)} \right\| } \right)^2.
\end{equation}
The total loss function for disentangled domain adaptation combines these losses
\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{total}^{(s)} + \mathcal{L}_{total}^{(d)} + \lambda_g \mathcal{L}_{og}.
\end{equation}
\subsection{Travel Preference Learning}

After completing the travel cost prediction, we can use the shortest path search algorithm to generate trajectories for the target city.

Shortest path search algorithms rely on fixed road cost factors, such as travel speed or time, for route planning. However, focusing on a single cost factor often fails to fully capture users’ actual travel preferences, which are influenced by more complex factors ~\cite{rideshare_chen2018price}. 

To address this, we propose modeling travel preferences as a combination of observable costs and hidden costs. Hidden costs, which account for harder-to-explain factors influencing human choices, are generated using a multi-layer perceptron (MLP) as follows
\begin{equation}
y_i^{(hid)} = \log \left( 1 + \exp \left( \text{MLP} (\bm{z}_i) \right) \right),
\end{equation}
where \(\bm{z}_i\) is a semantic latent variable.
We then estimate overall travel preference using a weighted combination of observable and hidden costs
\begin{equation}
\label{equ:combined_cost}
p(r_i) = \sum_{m} w^{(m)} y_i^{(m)} + y_i^{(hid)},
\end{equation}
where \(w^{(m)}\) are learnable weights. The smaller the value of \(p(r_i)\), the higher the preference for the road segment \(r_i\). We believe this method of combining preferences remains consistent across different cities.

The preference is learned by an unsupervised training. During training, we first randomly initialize the parameters and search for the shortest path. The shortest path found from $r_i$ to $r_j$ is denoted as $\hat{\tau}_{ij} = (r_i, \cdots, r_j)$, whose preference sum is 
\begin{equation}
    \hat{p} \left( \hat{\tau}_{ij} \right) = \sum_{r_k \in \hat{\tau}_{ij}} p(r_k).
\end{equation}
The sum of the preference values of the real trajectory ${\tau}_{ij}$ is
\begin{equation}
    p \left( \tau_{ij} \right) = \sum_{r_k \in \tau_{ij}} p(r_k).
\end{equation}
And then the loss function is formulated as 
\begin{equation}
    \mathcal{L}_{pref} = \frac{1}{\left| \mathcal{T}^{(src)} \right|} \sum_{\tau_{ij} \in \mathcal{T}^{(src)}} \left( p \left( \tau_{ij} \right) - \hat{p} \left( \hat{\tau}_{ij} \right)\right).
\end{equation}
Through iterative training, the model learns the invariant mapping relationship between travel preferences and various travel costs, which can then be applied to the target city to generate trajectory data. It is worth mentioning that assigning a cost to each road segment is similar to the MaxEnt IRL~\cite{max_ent}. Theoretical analysis can be found in the code repository.
