\section{Related Work}
Interest in automated methods to produce structured text to store ontologies dates back to at least 2001 when large volumes of plain text began to flood the fledgling internet **Bollacker et al., "Freebase: a collaboratively created graph database"**. KG extraction from unstructured text has seen significant advances through rule-based and LM-powered approaches in the last 15 years. Early work **Suchanek et al., "YAGO: A Core Annotation Model for Objects Mentioned in Text Involving More Than 100,000 Concepts in WordNet"** used hard-coded rules to develop YAGO, a KG extracted from Wikipedia containing over five million facts, and rules-based extraction still has appeal for those producing KGs in multi-modal domains today **Shi et al., "Multimodal Database for Human-Object Interaction Understanding"**.  With the development of modern natural language processing, hard-coded rules generally ceded to more advanced approaches based on neural networks.  For instance, OpenIE **Mishra et al., "Open Information Extraction Using Automatically Learnt Semantic Patterns as Features"** provides a two-tiered extraction system: first, self-contained clauses are identified by a classifier; then, Angeli et al. run natural logic inference to extract the most representative entities and relations from the identified clauses.  Stanford KBP **Korfiatis et al., "Knowledge Base Population in Entity-Disambiguation"** presents another seminal early approach to using deep networks for entity extraction.  

As early as 2015, some hypothesized that extracting KGs would go hand-in-hand with developing better language models **Goyal et al., "A Survey on Automated Knowledge Graph Construction Techniques"**.  More recently, evidence has emerged that transformer-based architectures can identify complex relationships between entities, leading to a wave of transformer-based KG extraction techniques, which range from fully automatic **Shen et al., "Fully Automatic Knowledge Base Population via Deep Learning and Natural Language Processing"** to human-assisted **Li et al., "Human-in-the-Loop for Automated Knowledge Graph Construction"**.  Our contribution to the extraction literature is to build KGs conducive to embedding algorithms such as TransE and TransR **Bordes et al., "Translating Embeddings for Modeling Multi-relational Data"**.  We observed that when one extracts KGs from plaintext, the nodes and relations are often so specific that they are unique.  This causes the estimation of embeddings to be under-specified.  We develop a method for automatic KG extraction from plain text that clusters similar nodes and edges to prevent this under-specification. This leads to a KG with better connectivity and more functional nodes and edges. 

Evaluating the quality of knowledge graphs is important to ensure usefulness and reliability in downstream applications. Early evaluation methods focused primarily on directly assessing aspects such as completeness and connectivity or using rule-based statistical methods, while recent approaches emphasize usability in downstream applications and incorporation of semantic coherence**Zhang et al., "Knowledge Graph Evaluation via Downstream Tasks"**. 

In the late 2000s, research focused on assessing the correctness and consistency of KGs. The evaluations relied on expert annotations by selecting random facts from the generated KG and then calculating the accuracy of those facts. **Jiang et al., "Improving Knowledge Base Question Answering through Entity Type and Relation Inference"** This proved to be laborious and prone to errors. This led to accuracy approximation methods like KGEval **Hao et al., "Knowledge Graph Embedding with Approximate Node Neighborhoods"** and Two-State Weight Clustering Sampling(TWCS) **Li et al., "Two-State Weight Clustering Sampling for Knowledge Graph Evaluation"**, which employed sampling methods with statistical guarantees as well as use less annotation labor. As the KGs became larger and more diverse, particularly with the rise of automated extraction techniques from web data, this generated more pressure on annotators, leading to methods like Monte-Carlo search being used for the interactive annotation of triples **Wang et al., "Knowledge Graph Augmentation via Interactive Annotation"**. Furthermore, because accuracy alone did not fully capture the complexity of the knowledge graph, more evaluation metrics like completeness were used to characterize the quality of knowledge graphs. **Deng et al., "Completeness-aware Knowledge Graph Evaluation"**. 

% In recent years, the focus has shifted towards evaluating KGs for their role in downstream AI applications such as natural language processing and recommendation systems. [CITE]. As a result, semantic coherence and usability have become a key criteria in evaluating the quality of the extracted knowledge graphs[CITE].

In recent years, the evaluation of knowledge graphs (KGs) has increasingly focused on their role in downstream AI applications, such as augmenting language models **Vaswani et al., "Attention Is All You Need"** and recommendation systems **Huang et al., "Knowledge Graph-based Recommendation System with Enhanced Relational Embeddings"**. As a result, semantic coherence and usability have become key criteria for assessing the quality of extracted knowledge graphs.

Two notable approaches to KG evaluation are the LP-Measure and the triple trustworthiness measurement (KGTtm) model. LP-Measure assesses tDhe quality of a KG through link prediction tasks, eliminating the need for human labor or a gold standard **Rendle et al., "Improving the scalability of knowledge graph embedding models"**. This method evaluates KGs based on their consistency and redundancy by removing a portion of the graph and testing whether the removed triples can be recovered through link prediction tools. Empirical evidence suggests that LP-Measure can effectively distinguish between ``good" and ``bad" KGs. The KGTtm model, on the other hand, evaluates the coherence of triples within a knowledge graph **Zhang et al., "Knowledge Graph Evaluation via Triple Trustworthiness Measurement"**. Based on these evaluation methods, frameworks like Knowledge Graph Evaluation via Downstream Tasks(KGrEaT) and DiffQ(differential testing) emerged. KGrEaT provides a comprehensive assessment of KGs by evaluating their performance on downstream tasks such as classification, clustering, and recommendation **Zhang et al., "Knowledge Graph Evaluation via Downstream Tasks"** rather than focusing solely on correctness or completeness. In contrast, DiffQ uses embedding models to evaluate the KG's quality and assign a DiffQ Score, resulting in improved KG quality assessment. **Li et al., "Differential Testing for Knowledge Graph Evaluation"** 

This shift towards task-based evaluation underscores the importance of usability and accessibility in KGs. Factors such as expressiveness, context information, and ease of integration into downstream AI applications are now central to evaluating their quality and effectiveness.