\section{Related Work}
Interest in automated methods to produce structured text to store ontologies dates back to at least 2001 when large volumes of plain text began to flood the fledgling internet \citep{early_ontologies}. KG extraction from unstructured text has seen significant advances through rule-based and LM-powered approaches in the last 15 years.  Early work \citep{yago} used hard-coded rules to develop YAGO, a KG extracted from Wikipedia containing over five million facts, and rules-based extraction still has appeal for those producing KGs in multi-modal domains today \citep{Norabid2022RulebasedTE, rules_music}.  With the development of modern natural language processing, hard-coded rules generally ceded to more advanced approaches based on neural networks.  For instance, OpenIE \citep{angeli-etal-2015-leveraging} provides a two-tiered extraction system: first, self-contained clauses are identified by a classifier; then, Angeli et al. run natural logic inference to extract the most representative entities and relations from the identified clauses.  Stanford KBP \citep{Angeli2013Stanfords2K} presents another seminal early approach to using deep networks for entity extraction.  

As early as 2015, some hypothesized that extracting KGs would go hand-in-hand with developing better language models \citep{Domeniconi}.  More recently, evidence has emerged that transformer-based architectures can identify complex relationships between entities, leading to a wave of transformer-based KG extraction techniques, which range from fully automatic \citep{qiao2022joint, Arsenyan2023LargeLM, Zhang2024ExtractDC} to human-assisted \citep{Kommineni2024FromHE}.  Our contribution to the extraction literature is to build KGs conducive to embedding algorithms such as TransE and TransR \citep{TransE, TransR}.  We observed that when one extracts KGs from plaintext, the nodes and relations are often so specific that they are unique.  This causes the estimation of embeddings to be under-specified.  We develop a method for automatic KG extraction from plain text that clusters similar nodes and edges to prevent this under-specification. This leads to a KG with better connectivity and more functional nodes and edges. 

Evaluating the quality of knowledge graphs is important to ensure usefulness and reliability in downstream applications. Early evaluation methods focused primarily on directly assessing aspects such as completeness and connectivity or using rule-based statistical methods, while recent approaches emphasize usability in downstream applications and incorporation of semantic coherence\citep{xue2023knowledge}. 

In the late 2000s, research focused on assessing the correctness and consistency of KGs. The evaluations relied on expert annotations by selecting random facts from the generated KG and then calculating the accuracy of those facts. \citep{yago} This proved to be laborious and prone to errors. This led to accuracy approximation methods like KGEval \citep{ojha-talukdar-2017-kgeval} and Two-State Weight Clustering Sampling(TWCS) \citep{gao2018efficientKGeval}, which employed sampling methods with statistical guarantees as well as use less annotation labor. As the KGs became larger and more diverse, particularly with the rise of automated extraction techniques from web data, this generated more pressure on annotators, leading to methods like Monte-Carlo search being used for the interactive annotation of triples \citep{qi2022optimizedhumancollab}. Furthermore, because accuracy alone did not fully capture the complexity of the knowledge graph, more evaluation metrics like completeness were used to characterize the quality of knowledge graphs. \citep{SubhiIssaKnowledgeGraphCompleteness}. 

% In recent years, the focus has shifted towards evaluating KGs for their role in downstream AI applications such as natural language processing and recommendation systems. [CITE]. As a result, semantic coherence and usability have become a key criteria in evaluating the quality of the extracted knowledge graphs[CITE].

In recent years, the evaluation of knowledge graphs (KGs) has increasingly focused on their role in downstream AI applications, such as augmenting language models \citep{decadeofkginnlp} and recommendation systems \citep{he2020lightgcn}. As a result, semantic coherence and usability have become key criteria for assessing the quality of extracted knowledge graphs.

Two notable approaches to KG evaluation are the LP-Measure and the triple trustworthiness measurement (KGTtm) model. LP-Measure assesses tDhe quality of a KG through link prediction tasks, eliminating the need for human labor or a gold standard \citep{zhu2023assessing}. This method evaluates KGs based on their consistency and redundancy by removing a portion of the graph and testing whether the removed triples can be recovered through link prediction tools. Empirical evidence suggests that LP-Measure can effectively distinguish between ``good" and ``bad" KGs. The KGTtm model, on the other hand, evaluates the coherence of triples within a knowledge graph \cite{jia2019triple}. Based on these evaluation methods, frameworks like Knowledge Graph Evaluation via Downstream Tasks(KGrEaT) and DiffQ(differential testing) emerged. KGrEaT provides a comprehensive assessment of KGs by evaluating their performance on downstream tasks such as classification, clustering, and recommendation \citep{heist2023kgreat} rather than focusing solely on correctness or completeness. In contrast, DiffQ uses embedding models to evaluate the KG's quality and assign a DiffQ Score, resulting in improved KG quality assessment. \cite{tan2024diffq} 

This shift towards task-based evaluation underscores the importance of usability and accessibility in KGs. Factors such as expressiveness, context information, and ease of integration into downstream AI applications are now central to evaluating their quality and effectiveness.