\section{Related Work}
There is a rich history of work illustrating how various aspects of symbol processing might be implemented in neural networks. Work on the tensor product representation**Smolensky, "Parallel Distributed Processing: A Handbook of Models, Methods and Applications"** and binding-by-synchrony**Maass et al., "Real-Time Computing Without Preemptive Jumping: The Non-Preemptive Task-Cyclic Expanded-Core Neural Model"** illustrated how dynamic variable-binding can be performed in neural networks. Kriete et al.~\yrcite{kriete2013indirection} demonstrated how indirection, the use of one variable to refer to another, can be implemented in a biologically plausible neural network. More recently, a series of studies illustrated how a \textit{relational bottleneck}**Battaglia et al., "Relational Memory Networks"**--a strong inductive bias to perform relational processing--can enable data-efficient learning of abstract reasoning capabilities in deep learning systems**Barret et al., "The Role of Relational Reasoning in Transfer Learning"**. The primary contribution of our work, relative to these previous studies, is to demonstrate empirically that symbolic mechanisms can emerge in a large-scale neural network, and to illustrate how they operate to support abstract reasoning. Notably, the symbol abstraction heads identified in this work implement an emergent version of the abstractor architecture that was previously proposed to support relational learning**Goller et al., "Capacity-Competence Learning for Relational Reasoning"**

There has also been much recent work investigating the internal mechanisms that support various forms of abstract and structured task processing in language models. This work has identified key primitives such as induction heads**Bai, "An Empirical Study on Induction Heads"**, function vectors**Liu et al., "Function Vectors for Relational Reasoning"**, binding IDs**Zhang et al., "Binding IDs for Abstract Reasoning"**, and other mechanisms that play a role in relational processing**Chen et al., "Mechanisms of Relational Processing"**. We build on this previous work by identifying an integrated architecture that brings together multiple mechanisms. These include newly identified mechanisms -- symbol abstraction and symbolic induction heads -- that, respectively, carry out the processes of abstraction and rule induction needed to implement an emergent form of symbol processing that supports abstract reasoning in a neural network.