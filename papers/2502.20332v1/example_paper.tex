%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[preprint]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{comment}

\usepackage{color-edits}%suppress
% \usepackage{bbm}
% \usepackage{dsfont}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Emergent Symbolic Mechanisms Support Reasoning in LLMs}

\begin{document}

\twocolumn[
\icmltitle{Emergent Symbolic Mechanisms Support\\Abstract Reasoning in Large Language Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yukang Yang}{princeton-ece}
\icmlauthor{Declan Campbell}{pni}
\icmlauthor{Kaixuan Huang}{princeton-ece}
\icmlauthor{Mengdi Wang}{princeton-ece}
\icmlauthor{Jonathan Cohen}{pni}
\icmlauthor{Taylor Webb}{msr}
\end{icmlauthorlist}

\icmlaffiliation{princeton-ece}{Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ}
\icmlaffiliation{pni}{Princeton Neuroscience Institute, Princeton University, Princeton, NJ}
\icmlaffiliation{msr}{Microsoft Research, New York, NY}

\icmlcorrespondingauthor{Taylor Webb}{taylor.w.webb@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Many recent studies have found evidence for emergent reasoning capabilities in large language models, but debate persists concerning the robustness of these capabilities, and the extent to which they depend on structured reasoning mechanisms. To shed light on these issues, we perform a comprehensive study of the internal mechanisms that support abstract rule induction in an open-source language model (Llama3-70B). We identify an emergent symbolic architecture that implements abstract reasoning via a series of three computations. In early layers, \textit{symbol abstraction heads} convert input tokens to abstract variables based on the relations between those tokens. In intermediate layers, \textit{symbolic induction heads} perform sequence induction over these abstract variables. Finally, in later layers, \textit{retrieval heads} predict the next token by retrieving the value associated with the predicted abstract variable. These results point toward a resolution of the longstanding debate between symbolic and neural network approaches, suggesting that emergent reasoning in neural networks depends on the emergence of symbolic mechanisms.
\end{abstract}

\section{Introduction}

Large language models have become the dominant paradigm in artificial intelligence, but there is significant ongoing debate concerning the limits and reliability of their capabilities. One major focus of this debate has been the question of whether they can reason systematically in a abstract or human-like manner. Many studies have documented impressive performance on various reasoning tasks~\cite{wei2022emergent,mirchandani2023large}, even rivaling human performance in some cases~\cite{webb2023emergent,musker2024semantic,webb2024evidence}, but other studies have questioned these conclusions~\cite{wu2023reasoning,mccoy2023embers,lewis2024evaluating}. In particular, language models appear to perform more poorly in some reasoning domains, such as mathematical reasoning~\cite{dziri2024faith} or planning~\cite{momennejad2024evaluating}; and, even in domains in which they have shown strong performance such as analogical reasoning~\cite{webb2023emergent}, some studies have questioned the robustness of these capabilities~\cite{lewis2024evaluating}.

These conflicting findings raise the question: are language models genuinely capable of abstractly structured human-like reasoning, or are they merely mimicking this capacity by statistically approximating their training data? One way to answer this question is to look at the internal mechanisms that support this capacity. It has long been hypothesized that innate symbol-processing mechanisms are required to support human-like abstraction~\cite{marcus2001algebraic,dehaene2022symbols,wong2023word}. It has also been demonstrated that neural networks are capable, at least in principle, of implementing some of the key properties of symbolic systems~\cite{smolensky1990tensor,hummel2003symbolic,kriete2013indirection}, and that the incorporation of these properties as architectural inductive biases can support data-efficient acquisition of abstract symbolic reasoning~\cite{webb2020emergent,altabaa2023abstractors,webb2024relational}. It remains unclear, however, in the case of transformer language models that do not obviously possess such strong inductive biases, what mechanisms support their emergent capability for abstraction.

\begin{figure*}[h!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.6\columnwidth]{emergent_architecture_v3.pdf}}
\caption{\textbf{Emergent Symbolic Architecture}. Schematic depiction of the proposed three-stage architecture for abstract reasoning in language models. Inputs are identity rule problems, each instance of which is constructed from arbitrarily chosen tokens (example shown is an ABA rule). Symbol abstraction heads identify relations between input tokens and, based on these relations, represent the tokens using a consistent set of abstract variables aligned with their role in the relations. Symbolic induction heads perform sequence induction over abstract variables (i.e., they predict the next variable based on the sequence observed in the previous in-context examples). Retrieval heads predict the next token by retrieving the value associated with the predicted abstract variable.}
\label{emergent_architecture}
\end{center}
\vskip -0.2in
\end{figure*}

Here, we report evidence for a set of emergent symbolic mechanisms that support abstract reasoning in large language models. We focus on a simple but paradigmatic abstract reasoning task--induction of algebraic rules--and identify an emergent three-stage architecture that supports performance of this task in the open-source language model Llama3-70B~\cite{dubey2024llama}. Notably, this architecture can be straightforwardly interpreted as performing a form of symbol processing. Specifically, we find evidence to support a three-stage procedure in which: 1) input tokens are converted to abstract variables (i.e., symbols) based on the relations between those tokens, 2) sequence induction is then performed over these variables, and 3) next-token prediction is performed by retrieving the value associated with the predicted variable. We refer to the attention heads that perform these computations as \textit{symbol abstraction heads}, \textit{symbolic induction heads}, and \textit{retrieval heads} respectively. Through a series of ablation and causal mediation experiments, we show that this architecture is both necessary and sufficient to perform abstract rule induction. We also carry out representational similarity analyses and analyze attention patterns to better understand these mechanisms, finding evidence that they operate over symbol-like representations. Taken together, these results suggest that emergent reasoning in large language models depends on structured, abstract reasoning mechanisms, rather than less abstract forms of statistical approximation. More broadly, these results suggest a resolution to the longstanding debate between symbolic and neural network approaches, illustrating how neural networks can learn to perform abstract reasoning via the development of emergent symbol processing mechanisms.

\section{Approach}

Figure~\ref{emergent_architecture} depicts the proposed architecture and rule induction task. In this work, we focused on an algebraic rule induction task involving sequences governed by one of two identity rules, ABA or ABB. For each problem, two in-context examples were presented, followed by an incomplete third example. The model was expected to generate the token that completes this third example. We instantiated rules using tokens randomly sampled from Llama3's vocabulary (ensuring that in-context examples within the same problem instance did not share tokens). We found that Llama3-70B displayed a 2-shot accuracy of 95\% on this task.

Although this task is relatively simple, especially when compared with some of the tasks that have been featured in recent debates over LLM reasoning (e.g., matrix reasoning tasks~\cite{webb2023emergent}), it nevertheless offers a paradigmatic case of relational abstraction. In particular, the use of completely arbitrary tokens ensures that the task cannot be solved by relying on statistical patterns specific to the tokens or associations among them, and for this reason it has previously been used to argue for the presence of symbol-processing mechanisms in human cognition~\cite{marcus1999rule}, and to evaluate systematic generalization of abstract rules in neural networks~\cite{webb2020emergent}. Accordingly, the ability to reliably solve this task is already strongly suggestive of the presence of some form of symbol-processing. In the following sections, we describe a specific mechanistic hypothesis for how symbol-processing might be carried out in this model.

\subsection{Symbol Abstraction Heads}

Our hypothesis consists of three stages. In the first stage, input tokens are converted to symbolic representations. The inspiration for this hypothesis comes from the \textit{abstractor} architecture~\cite{altabaa2023abstractors}, a variant of the transformer that implements a strong relational inductive bias~\cite{webb2024relational}. In that architecture, a modified form of attention (termed \textit{relational cross-attention}) is employed in which the values consist of a standalone set of learned embeddings, rather than being conditioned on the input tokens as in standard self attention. As a result, the output of this attention operation is completely abstracted away from the identity of the input tokens, and instead only reflects the pattern of relations among those tokens (as encoded by the pattern of inner products between query and key embeddings). These outputs can therefore be viewed as a form of learned, distributed symbolic representations.

Here, we hypothesize that an emergent form of this relational attention operation is implemented by attention heads in early layers of the model. We refer to these heads as \textit{symbol abstraction heads}. Concretely, the keys and query embeddings in these heads represent the input tokens, and the inner product between keys and queries represents the relations between these tokens. It is natural to interpret this operation as representing similarity relations (and this is the relevant type of relations in our task), but it is also possible for this operation to represent a broader class of relations~\cite{altabaa2024approximation}. Importantly, we hypothesize that the value embeddings in these heads do \textit{not} carry information about the specific identity of the input tokens, but instead represent only their position. More precisely, we hypothesize that the value embeddings represent the relative position of a token within an in-context example, as this is precisely the information that's needed to compute the abstract variable associated with that token (e.g., the fact that the first token and the third token are the same in an ABA rule is precisely what determines that they share the same variable). Given that these conditions are met, the self-attention operation is equivalent to relational cross-attention~\cite{altabaa2023abstractors}, and the output of such an attention head will be analogous to an abstract variable. 

% We hypothesize that information about the abstract variable or `role' (computed by the symbol abstraction heads) and information about the concrete token or `filler' are maintained together in the residual stream of the model at the position where the token appeared in the input. This can be viewed as a form of variable-binding, in which the abstract variable (i.e., role) its associated token (i.e., filler) are bound via their position in the sequence. This can also be viewed as a form of the `referencing' operation that is central to symbolic computing, where the abstract variable is an address that can be used to refer to the token later on (as described in the section on `retrieval heads' below).

\subsection{Symbolic Induction Heads}

In the second stage, we hypothesize that sequence induction is performed over the abstract variables computed in the first stage. This hypothesis is inspired by previous work on \textit{induction heads}, an emergent circuit that supports in-context learning in transformers~\cite{elhage2021mathematical,olsson2022context}. As originally formulated, this circuit performs a simple sequence induction mechanism: given a sequence that ends with a particular token, an induction head will look for previous instances of that token, and retrieve the token that succeeded it. Although this mechanism performs induction based only on in-context bigram statistics, subsequent work has identified heads that also compute higher-order n-gram statistics \cite{akyurek2024context}. Here, we use the term `induction' to refer to the more general process of predicting the next token based on in-context transition probabilities (i.e., beyond bigram statistics). 

We hypothesize that a symbolic variant of this mechanism is responsible for performing induction over sequences of symbols rather than literal tokens. We refer to the attention heads that carry out this mechanism as \textit{symbolic induction heads}. Unlike standard induction heads, which operate over direct representations of the input tokens, symbolic induction heads operate over representations of abstract variables (computed by symbol abstraction heads in previous layers). The output of symbolic induction heads is a prediction of the abstract variable associated with the next token. Empirically, we find that symbolic induction heads are distinct from standard induction heads (section~\ref{induction_head_section}). 

\subsection{Retrieval Heads}

Finally, in the third stage, we hypothesize that a separate mechanism is used to convert the abstract variables (symbols) to their associated tokens (values), by performing a simple form of retrieval. We refer to the attention heads that perform this retrieval operation as \textit{retrieval heads}. The key and query embeddings in these heads represent abstract variables, and the value embeddings represent the corresponding input tokens. Retrieval heads perform the inverse of the relational attention operation performed by symbol abstraction heads. Given an input embedding representing an abstract variable (the prediction computed by symbolic induction heads in previous layers), this variable is matched with previous instances, and the associated token is retrieved. This can be viewed as a form of the `dereferencing' operation that is central to symbolic computing, wherein a variable (i.e., a pointer to a particular location in memory) is used to retrieve the value associated with it (i.e., the data stored at that location). 

\section{Results}

\subsection{Causal Mediation Analyses}
\label{causal_mediation_section}

We performed causal mediation analysis~\cite{pearl2022direct,meng2022locating,wang2022interpretability,todd2023function} to isolate the hypothesized attention heads. In this analysis, embeddings from one context are patched into another context. This approach can be used to estimate the causal effect of an embedding at a particular layer, position, or attention head. 

\begin{figure*}[ht] % Regular figure environment for a single-column layout
    % First row
        \centering
    \subfigure[Abstract Causal Mediation]{
        \includegraphics[width=5.0cm]{figures/causal_mediation_results_updated/layer_positon_abs_symb_Logit_Difference_heatmap_exclude_bos.pdf}
        \label{abstract_CM}
    } 
    \subfigure[Token Causal Mediation]{ 
    \centering
    \includegraphics[width=5.0cm]{figures/causal_mediation_results_updated/layer_position_retrieve_Logit_Difference_heatmap_exclude_bos.pdf}
    \label{token_CM}
    }  
    \\
    \subfigure[Symbol Abstraction Heads]{
        \includegraphics[width=5.0cm]{figures/causal_mediation_results/abstraction_head_Logit_Difference_heatmap.pdf}
        \label{abstraction_heads}
    }
    \subfigure[Symbolic Induction Heads]{
        \centering
        \includegraphics[width=5.0cm]{figures/causal_mediation_results/symbolic_induction_head_Logit_Difference_heatmap.pdf}
        \label{symb_ind_heads}
    }
    \subfigure[Retrieval Heads]{
        \includegraphics[width=5.0cm]{figures/causal_mediation_results/retrieval_head_Logit_Difference_heatmap.pdf}
        \label{retrieval_heads}
        \vspace{3pt}
    }
\caption{\textbf{Causal Mediation Results.} \textbf{(a)} Abstract causal mediation effects at each layer and sequence position. X-axis shows the two contexts ($c_{1}^{abstract}$ and $c_{2}^{abstract}$) used for this analysis, aligned with each sequence position. \textbf{(b)} Token causal mediation effects at each layer and sequence position, with corresponding $c_{1}^{token}$ and $c_{2}^{token}$ contexts shown along X-axis. \textbf{(c)} Identification of symbol abstraction heads: abstract causal mediation effects for each attention head, averaged across positions corresponding to the last item in each of the two in-context examples. \textbf{(d)} Identification of symbolic induction heads: abstract causal mediation effects for each attention head, for the final token in the sequence. \textbf{(e)} Identification of retrieval heads: token causal mediation effects for each attention head, for the final token in the sequence. Note that all causal mediation scores are clipped at zero.}
\end{figure*}

Our analysis had two conditions. In one condition, intended to isolate representations of abstract variables (i.e., symbols), we created two contexts in which the same token is associated with two different variables. Given a set of tokens $A_{1}, B_{1},...A_{N}, B_{N}$ (where $N-1$ represents the number of in-context examples), we created one context $c_{1}^{abstract}$ that instantiated an ABA rule, and another context $c_{2}^{abstract}$ that instantiated an ABB rule:

\begin{equation}
    c_{1}^{abstract} = A_{1}, B_{1}, A_{1}, ..., A_{N}, B_{N}
    \label{eq:c1_abs}
\end{equation}
\begin{equation}
    c_{2}^{abstract} = B_{1}, A_{1}, A_{1}, ..., B_{N}, A_{N}
    \label{eq:c2_abs}
\end{equation}

Importantly, in this analysis, the final token in each in-context example, $A_{n}$, is identical for both contexts, but it is associated with a different variable, based on its relations to the other tokens within the same in-context example. 

We contrast this with another condition that is intended to isolate representations of literal tokens. Given the same set of tokens used for the previous condition, we created the following two contexts:

\begin{equation}
    c_{1}^{token} = A_{1}, B_{1}, A_{1}, ..., A_{N}, B_{N}
    \label{eq:c1_tok}
\end{equation}
\begin{equation}
    c_{2}^{token} = A_{1}, B_{1}, A_{1}, ..., B_{N}, A_{N}
    \label{eq:c2_tok}
\end{equation}

In this condition, both contexts involve the same abstract rule, but the tokens used in the final example ($A_{n}$ vs. $B_{n}$) are swapped. We also perform a version of both analyses using the ABB rule and average the results for the two rules. 

Together, these analyses allow for a double dissociation between representations of abstract variables (abstract causal mediation) vs. the tokens associated with those variables (token causal mediation). For each analysis, given two contexts $c_{1}$ and $c_{2}$, and a pretrained lanuage model $f(c)$ that outputs logits for all possible next tokens,  we computed the causal mediation score developed by Wang et al. \yrcite{wang2022interpretability}:

\begin{equation}
    s = (f(c_{1}^{*})[y_{c_{1}^{*}}] - f(c_{1}^{*})[y_{c_{1}}]) - (f({c_{1}})[y_{c_{1}^{*}}] - f({c_{1}})[y_{c_{1}}]),
\end{equation}

where $f(c_{1})[y]$ is the logit for answer $y$ in context $c_{1}$; $f(c_{1}^{*})[y]$ is the logit for answer $y$ in the patched context $c_{1}^{*}$, for which activations are patched from $c_{2}$ to $c_{1}$ at specific layers, positions, and/or attention heads; $y_{c_{1}}$ is the correct answer for context $c_{1}$; and $y_{c_{1}^{*}}$ is the expected answer for the patched context $c_{1}^{*}$. We averaged over different sets of tokens. The procedure for causal mediation analyses is shown in Algorithm \ref{alg:causal_mediation}. Intuitively, this score reflects the extent to which patching activations from context $c_{2}$ to $c_{1}$ has the expected effects on the model's outputs.

\begin{figure*}[ht] % Regular figure environment for a single-column layout
    % First row
    \centering
    \subfigure[ Symbol Abstraction Heads]{
    \begin{minipage}[c]{0.3\linewidth}
    \centering
        \includegraphics[width=5.0cm]{figures/attention_maps/ABA/Abstraction_Head_Attention_heatmap.pdf}
        \newline
        \includegraphics[width=5.0cm]{figures/attention_maps/ABB/Abstraction_Head_Attention_heatmap.pdf}
        \label{abstraction_heads_attention}
    \end{minipage}
    }
    \subfigure[Symbolic Induction Heads]{
    \begin{minipage}[c]{0.3\linewidth}
    \centering
        \includegraphics[width=5.0cm]{figures/attention_maps/ABA/Symbolic_Induction_Head_Attention_heatmap.pdf}
        \newline
        \includegraphics[width=5.0cm]{figures/attention_maps/ABB/Symbolic_Induction_Head_Attention_heatmap.pdf} 
        \label{symb_ind_heads_attention}
    \end{minipage}
    
    }
    \subfigure[Retrieval Heads]{
    \begin{minipage}[c]{0.3\linewidth}
         \centering
        \includegraphics[width=5.0cm]{figures/attention_maps/ABA/Retrieval_Head_Attention_heatmap.pdf}
        \newline
        \includegraphics[width=5.0cm]{figures/attention_maps/ABB/Retrieval_Head_Attention_heatmap.pdf}
        \label{retrieval_heads_attention}
    \end{minipage}
    }
\caption{\textbf{Attention Analysis.} Analysis of attention patterns for \textbf{(a)} symbol abstraction heads, \textbf{(b)} symbolic induction heads, and \textbf{(c)} retrieval heads. For each type of attention head, the 10 heads with the highest causal mediation score were selected, and a weighted average of their attention patterns was computed using the causal mediation scores as weights. X-axis corresponds to keys (positions that are attended to), Y-axis corresponds to queries (positions from which attention is directed). Top row depicts attention pattern for ABA problems, bottom row depicts attention pattern for ABB problems. Prompt templates are shown along each axis, with tokens aligned to their corresponding positions. Red dashed lines highlight positions of interest (discussed in text). Note that beginning-of-sequence token is omitted.}
\end{figure*}

Figures \ref{abstract_CM} and \ref{token_CM} show the results of these causal mediation analyses when they were performed on both the aggregated attention head outputs and the MLP outputs (leaving the residual stream intact) at each sequence position and layer within the model. Consistent with our hypothesized architecture, the abstract causal mediation analysis (Figure \ref{abstract_CM}) revealed two distinct stages of processing, one in early layers of the model, with an effect that was largely concentrated at the positions of the final item in each in-context example, and one in intermediate layers, with an effect that was concentrated at the final position in the sequence (i.e., the position at which the model must generate a completion to the query). These results are consistent with the hypothesized behavior (both in terms of specific positions within the sequence, and relative order across layers) of symbol abstraction heads and symbolic induction heads respectively. The token causal mediation analysis (Figure \ref{token_CM}) revealed a later stage of processing that was also concentrated at the final position in the sequence, consistent with the hypothesized behavior of retrieval heads.

Next, we performed causal mediation analysis on the output of individual attention heads. To isolate symbol abstraction heads, we performed the abstract causal mediation analysis at the positions corresponding to the final item in each in-context example. To isolate symbolic induction heads, we performed the abstract causal mediation analysis at the final position in the sequence. To isolate retrieval heads, we performed the token causal mediation analysis at the final position in the sequence. The results (Figures \ref{abstraction_heads}-\ref{retrieval_heads}) revealed a relatively sparse selection of attention heads, again conforming to the hypothesized three-stage structure. 

\subsection{Attention Analyses}

We analyzed attention patterns to better understand the behavior of the identified attention heads. Figure~\ref{abstraction_heads_attention} shows the attention patterns for symbol abstraction heads. Our hypothesis predicts that attention should be directed from the third item in each in-context example to the first item for ABA rules (top), and should be directed from the third item to the second item for ABB rules (bottom). The results largely confirmed this hypothesis (the attention patterns for these positions are highlighted with red dashed lines). Interestingly, the pattern became more focused for the second in-context example, suggesting that the symbol abstraction heads benefit from in-context learning.

Figure~\ref{symb_ind_heads_attention} shows the results for symbolic induction heads. Our hypothesis predicts that attention should be directed from the final position in the entire sequence (the separation token at the end of the incomplete example) to positions of the final items in each in-context example, as these are the positions where the previous instances of the to-be-predicted abstract variable are located. The results confirmed this hypothesis. Again, the pattern was stronger for the second in-context example, consistent with a general effect of in-context learning, and consistent with the more focused pattern of attention observed for the symbol abstraction heads in the second in-context example.

\begin{figure*}[ht] % Regular figure environment for a single-column layout
    % \centering
    \hspace{0.15\linewidth}
    \subfigure[Abstract Similarity Matrix]{
    \begin{minipage}[c]{0.3\linewidth}
        \includegraphics[width=\linewidth]{figures/rsa/hand_code_abstraction_head_11_RSA_heatmap.pdf}
    \end{minipage}
    \label{abstract_similarity}
    }
    \subfigure[Token Similarity Matrix]{
    \begin{minipage}[c]{0.3\linewidth}
        \includegraphics[width=\linewidth]{figures/rsa/hand_code_retrieval_head_RSA_heatmap.pdf} 
    \end{minipage}
    \label{token_similarity}
    }
    \newline
    \centering
    \subfigure[Symbol Abstraction Heads]{
    \begin{minipage}[c]{0.3\linewidth}
            \includegraphics[width=\linewidth]{figures/rsa_updated/abstraction_head_5_11_avg_RSA_heatmap.pdf}
    \end{minipage}
    \label{abstraction_head_similarity}
    }
    \subfigure[Symbolic Induction Heads]{
    \begin{minipage}[c]{0.3\linewidth}
        \includegraphics[width=\linewidth]{figures/rsa_updated/symbolic_induction_head_RSA_heatmap.pdf} 
    \end{minipage}
    \label{symb_ind_head_similarity}
    }
    \subfigure[Retrieval Heads]{
    \begin{minipage}[c]{0.3\linewidth}
        \includegraphics[width=\linewidth]{figures/rsa_updated/retrieval_head_RSA_heatmap.pdf} 
    \end{minipage}
    \label{retrieval_head_similarity}
    }
\caption{\textbf{Representational Similarity Analysis.} \textbf{(a)} Predicted pattern of pairwise similarity for representations of abstract variables. \textbf{(b)} Predicted pattern for representations of tokens. \textbf{(c)} Pairwise similarity for symbol abstraction head outputs, averaged across the third position in the two in-context examples. \textbf{(d)} Pairwise similarity for symbolic induction head outputs at the final sequence position. \textbf{(e)} Pairwise similarity for retrieval heads at the final sequence position. For each type of attention head, the 10 heads with the highest causal mediation score were selected, and a weighted average of their similarity matrices was computed using the causal mediation scores as weights.}
\end{figure*}

Figure~\ref{retrieval_heads_attention}) shows the results for retrieval heads. Our hypothesis predicts that attention should be directed from the final position in the sequence to the positions corresponding to the tokens that will appear next (i.e., within the incomplete example). For ABA rules (top), we predict that attention should be directed to the first item in the example (corresponding to the variable A), and for ABB rules (bottom) we predict that attention should be directed to the second item in each example (corresponding to the variable B). This prediction too was confirmed by our analyses. 

\subsection{Representational Similarity Analyses}

We also performed representational similarity analyses~\cite{kriegeskorte2008representational} to better understand the representations that were produced by each type of attention head. In this analysis, representations are modeled in terms of their similarity with one another. For each set of tokens $A_{1}, B_{1},...A_{N}, A_{N}$, we created four prompts, intended to dissociate representations of abstract variables (i.e., symbols) vs. literal tokens. The first two prompts were the same as the $c_{1}^{abstract}$ and $c_{2}^{abstract}$ contexts used for causal mediation analysis, in which the same token plays different abstract roles. The other two prompts were based on $c_{1}^{abstract}$ and $c_{2}^{abstract}$, but the final instances of $A_{N}$ and $B_{N}$ were swapped. The resulting set of prompts predict one pattern of pairwise similarity for abstract variables, and a different pattern of similarity for literal tokens. 

Figure \ref{abstract_similarity} shows the predicted pattern of pairwise similarities for representations of abstract variables, with all pairs involving two instances of the variable $A$ forming one block of high similarity, and all pairs involving two instances of $B$ forming another block. By contrast, Figure \ref{token_similarity} shows the predicted pattern of pairwise similarities for representations of literal tokens. This pattern displays 3 diagonal bands, corresponding to pairs of the same token (regardless of whether this token plays the same role). 

\begin{figure*}[ht] % Regular figure environment for a single-column layout
    \centering
    \subfigure[Symbol Abstraction Heads]{
    \begin{minipage}[c]{0.3\linewidth}
       \includegraphics[width=\linewidth]{figures/ablation_updated/abstraction_head.pdf}
    \end{minipage}
    \label{abstraction_head_ablation}
    }
    \subfigure[Symbolic Induction Heads]{
    \begin{minipage}[c]{0.3\linewidth} 
        \includegraphics[width=\linewidth]{figures/ablation_updated/symbolic_induction_head.pdf}
    \end{minipage}
    \label{symb_ind_head_ablation}
    }
    \subfigure[Retrieval Heads]{
    \begin{minipage}[c]{0.3\linewidth}
        \includegraphics[width=\linewidth]{figures/ablation_updated/retrieval_head.pdf} 
    \end{minipage}
    \label{retrieval_head_ablation}
    }
\caption{\textbf{Ablation Analyses.} \textbf{(a)} Cumulative ablation of symbol abstraction heads. Control condition involves ablation of heads in the same layers but with the lowest causal mediation scores. \textbf{(b)} Cumulative ablation of symbolic induction heads. \textbf{(c)} Cumulative ablation of retrieval heads.}
\label{ablation}
\end{figure*}

Figure \ref{abstraction_head_similarity} shows the pattern of similarity observed for the output of symbol abstraction heads at the third position in both in-context examples (averaged across the two examples). The pattern closely resembles the abstract variable similarity matrix, indicating that the output of symbol abstraction heads have a representational structure similar to abstract variables. Figure \ref{symb_ind_head_similarity} shows the pattern observed for symbolic induction heads at the final sequence position. This pattern also resembles the pattern predicted for abstract variables. Figure \ref{retrieval_head_similarity} shows the patten observed for retrieval heads at the final sequence position. This pattern closely resembles the token similarity matrix. 

Interestingly, although symbol abstraction heads and symbolic induction heads primarily resemble the pattern predicted for abstract variables, they also show the diagonal bands predicted by the token similarity matrix, suggesting that they preserve some degree of token identity, and thus do not represent perfectly abstract variables. Nevertheless, the patterns of similarity displayed by these three distinct types of attention heads have a strikingly close correspondence to the predictions of the hypothesized architecture, with symbol abstraction heads and symbolic induction heads both generating outputs that primarily resemble abstract variables, and retrieval heads generating outputs that primarily resemble literal tokens.

\subsection{Ablation Analyses}

The causal mediation analyses in section~\ref{causal_mediation_section} demonstrate that the identified attention heads are causally sufficient, in the sense that perturbing their outputs alters the model's responses in a predictable manner. We also performed ablation analyses to test whether these heads are \textit{necessary} for the model to perform the task. 

For each type of attention head, we performed a cumulative ablation analysis in which the heads with the top $h$ causal mediation scores were ablated. This was performed for the full range of $h=1...H$ heads in the entire model. We also performed a control experiment in which each head ablated in the previous experiment was replaced by the head in the same layer with the \textit{lowest} causal mediation score. We measured the effects of these ablations in terms of the probability assigned to the correct answer.

We found that these ablation experiments had a dramatic effect for all three types of heads (Figures \ref{abstraction_head_ablation}-\ref{retrieval_head_ablation}). In the ablation condition, the probability assigned to the correct answer rapidly fell to zero as more heads were ablated, whereas in the control condition it was necessary to ablate almost all attention heads to have such an effect. These experiments confirmed that all three types of attention heads were both sufficient and necessary to perform the rule induction task.

\subsection{Comparison with Induction Heads}
\label{induction_head_section}

We investigated the relationship between symbolic induction heads and the standard induction heads identified in previous work \cite{olsson2022context}. In that work, it was proposed that induction heads not only perform literal sequence induction, but may also perform a fuzzy or abstract form of induction. This raises the question of whether symbolic induction heads are merely standard induction heads.

For each attention head, we computed the prefix matching score previously used to identify induction heads \cite{olsson2022context}, and compared this with the causal mediation score for symbolic induction heads. We found that these scores were very weakly correlated ($r=0.13$), thus appearing largely orthogonal (Figure~\ref{induction_head_scatterplot}). These results suggest that, despite the conceptual similarity between these two mechanisms, they are implemented by disjoint sets of attention heads.

\subsection{Comparison with Function Vectors}

We also investigated the relationship between symbolic induction heads and function vectors \cite{todd2023function}, representations of an in-context task that are generated by a subset of attention heads. The symbolic induction heads identified in the present work have many similarities to the attention heads that generate function vectors, including: 1) they are found in intermediate layers of the model, 2) they primarily attend to the last item in each in-context example, and 3) they are causally implicated in in-context learning for relational tasks.

To address this, we computed the average indirect effect for each attention head, which represents a measure of the extent to which its outputs constitute function vectors~\cite{todd2023function}, and compared this with the causal mediation score for symbolic induction heads. This analysis revealed that these scores are indeed highly correlated (Figure~\ref{function_vector_scatterplot}; $r=0.82$), suggesting that they are essentially the same set of attention heads. That is, the output of symbolic induction heads can be thought of as function vectors. This result provides a novel perspective on function vectors, suggesting that, where relevant, they can be used to implement symbolic forms of computation. This result also provides insight into the mechanism that computes function vectors, suggesting that it may be conceptualized as a form of sequence induction over abstract variables.

\section{Related Work}

There is a rich history of work illustrating how various aspects of symbol processing might be implemented in neural networks. Work on the tensor product representation~\cite{smolensky1990tensor} and binding-by-synchrony~\cite{hummel2003symbolic} illustrated how dynamic variable-binding can be performed in neural networks. Kriete et al.~\yrcite{kriete2013indirection} demonstrated how indirection, the use of one variable to refer to another, can be implemented in a biologically plausible neural network. More recently, a series of studies illustrated how a \textit{relational bottleneck}~\cite{webb2024relational}--a strong inductive bias to perform relational processing--can enable data-efficient learning of abstract reasoning capabilities in deep learning systems~\cite{webb2020emergent,kerg2022neural,altabaa2023abstractors}. The primary contribution of our work, relative to these previous studies, is to demonstrate empirically that symbolic mechanisms can emerge in a large-scale neural network, and to illustrate how they operate to support abstract reasoning. Notably, the symbol abstraction heads identified in this work implement an emergent version of the abstractor architecture that was previously proposed to support relational learning~\cite{altabaa2023abstractors}

There has also been much recent work investigating the internal mechanisms that support various forms of abstract and structured task processing in language models. This work has identified key primitives such as induction heads~\cite{olsson2022context}, function vectors~\cite{todd2023function}, binding IDs~\cite{feng2023language}, and other mechanisms that play a role in relational processing~\cite{merullo2023mechanism}. We build on this previous work by identifying an integrated architecture that brings together multiple mechanisms. These include newly identified mechanisms -- symbol abstraction and symbolic induction heads -- that, respectively, carry out the processes of abstraction and rule induction needed to implement an emergent form of symbol processing that supports abstract reasoning in a neural network.

\section{Discussion}

In this work, we have identified an emergent architecture consisting of several newly identified mechanistic primitives, and illustrated how these mechanisms work together to implement a form of symbol processing. These results have major implications both for the debate over whether language models are capable of genuine reasoning, and for the broader debate between traditional symbolic and neural network approaches in artificial intelligence and cognitive science. 

On the one hand, the emergent architecture identified here, that supports abstract reasoning via an intermediate layer of symbol processing, is strikingly at odds with characterizations of language models as mere stochastic parrots~\cite{bender2021dangers} or `approximate retrieval' engines~\cite{wu2023reasoning}. These results are also at odds with claims that neural networks will need innately configured symbol processing mechanisms in order to perform human-like abstract reasoning~\cite{marcus2001algebraic,dehaene2022symbols,wong2023word}. On the other hand, these results can be viewed as a vindication of longstanding claims that symbol-processing mechanisms of some form (whether they be innate or learned) are a necessary component supporting human cognitive abilities~\cite{fodor1988connectionism}, insofar as they suggest that neural networks can acquire these abilities by developing their own form of symbol processing.

It is interesting to consider the extent to which the identified mechanisms are truly emergent vs. dependent on innate aspects of the model. The transformer architecture~\cite{vaswani2017attention} does not obviously possess the strong relational inductive biases that characterize the abstractor~\cite{altabaa2023abstractors} or other architectures designed to perform relational abstraction~\cite{webb2024relational}. However, transformers do have some inductive biases that seem relevant, including: 1) an innate mechanism for computing in-context similarity via the inner product between keys and queries, and 2) a form of indirection, in the sense that the keys and queries that are used to select information for retrieval are distinct from the values that are retrieved. In future work, it would be interesting to investigate the extent to which these or other inductive biases contribute to the development of emergent symbol processing mechanisms.

Finally, it should be noted that our results do not suggest the language model precisely implements the hypothesized architecture, but more likely that it implements an approximate version of these computations. For instance, the symbol abstraction head and symbolic induction head outputs do not appear to represent \emph{perfectly} abstract variables, but rather contain some information about the specific tokens that are used in each problem. This might explain some of the `content effects' that have been observed in language models~\cite{dasgupta2022language}, in which reasoning performance is not entirely abstract, but depends on the specific content over which reasoning is performed. These effects are also well documented in human psychology~\cite{wason1968reasoning}, which suggests that a similar form of approximate symbol processing may be implemented by the human brain. Whether and how these systems can account for the reasoning mechanisms employed by the brain is an interesting and important question to explore in future work.

% \section*{Impact Statement}

% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
