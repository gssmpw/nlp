\documentclass[conference]{IEEEtran}
%\usepackage[style=ieee,backend=biber]{biblatex}
%\usepackage[style=ieee]{biblatex}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{verbatim}
\usepackage{flushend} % Makes columns balanced
\usepackage{url}
\usepackage{hyperref}

\usepackage[pscoord]{eso-pic} 
\newcommand{\placetextbox}[3]{ 
    \setbox0=\hbox{#3} 
    \AddToShipoutPictureFG*{ 
    \put(\LenToUnit{#1\paperwidth},\LenToUnit{#2\paperheight}){ 
    \vtop{{\null}\makebox[0pt][c]{#3}}} 
    } 
}
\placetextbox{.2}{0.055}{978-3-903176-54-6~\copyright~2023 IFIP}
\bibliographystyle{ieeetran}



%\addbibresource{references.bib}

\usepackage[pscoord]{eso-pic}
\newcommand{\customplacetextbox}[3]{
    \setbox0=\hbox{#3}
    \AddToShipoutPictureFG*{
    \put(\LenToUnit{#1\paperwidth},\LenToUnit{#2\paperheight}){
    \vtop{{\null}\makebox[0pt][c]{#3}}}
    }
}
\customplacetextbox{.2}{0.055}{978-3-903176-54-6~\copyright~2023 IFIP}

\newcommand{\abeg}{\textcolor{blue}}
\begin{document}


\title{
Reinforcement Learning with Graph Attention for Routing and Wavelength Assignment with Lightpath Reuse \\

\thanks{Financial support from  EPSRC Centre for Doctoral Training in Connected Electronic and Photonic Systems (CEPS CDT) and EPSRC Programme Grant TRANSNET (EP/R035342/1) is gratefully acknowledged.}
}

\author{\IEEEauthorblockN{Michael Doherty}
\IEEEauthorblockA{Optical Networks Group\\
Electronic \& Electrical Eng. Dept.\\
\textit{University College London}\\
London, UK \\
michael.doherty.21@ucl.ac.uk}
\and
\IEEEauthorblockN{Alejandra Beghelli}
\IEEEauthorblockA{Optical Networks Group\\
Electronic \& Electrical Eng. Dept.\\
\textit{University College London}\\
London, UK \\
alejandra.beghelli@ucl.ac.uk}
}

\maketitle


\begin{abstract}
Many works have investigated reinforcement learning (RL) for routing and spectrum assignment on flex-grid networks but only one work has examined RL for fixed-grid with flex-rate transponders, despite production systems using this paradigm. Flex-rate transponders allow existing lightpaths to accommodate new services, a task we term routing and wavelength assignment with lightpath reuse (RWA-LR). We re-examine this problem case and present a thorough benchmarking of heuristic algorithms for RWA-LR, which are shown to have 6\% increased throughput when candidate paths are ordered by number of hops, rather than total length. We train an RL agent for RWA-LR with graph attention networks for the policy and value functions to exploit the graph-structured data. We provide details of our methodology and open source all of our code for reproduction. We outperform the previous state-of-the-art RL approach by 2.5\% (17.4~Tbps mean additional throughput) and the best heuristic by 1.2\% (8.5~Tbps mean additional throughput). This marginal gain highlights the difficulty in learning effective RL policies on long horizon resource allocation tasks. 
\end{abstract}

%Cisco NCS 2000 Flex Spectrum Single Module ROADM and it's integrated pre-amplifier has a nominal noise figure of 5.5dB (for 24dB gain) or 11.7dB (for 12dB gain), 
% We assume ROADMs require per wavelength receive power of 0dBm https://www.infinera.com/blog/is-your-roadm-ready-for-next-generation-coherent/tag/optical/
% Assume using Infinera ICE7 as transmitter, which supports up to 9dBm per channel and 144GBd symbol rate.



\begin{IEEEkeywords}
fixed-grid optical network (EON), flex-rate transponders, Gaussian noise (GN) model, deep reinforcement learning (DRL)
\end{IEEEkeywords}





\section{Introduction and Motivation}
\label{sec:intro}

Reinforcement learning (RL) has been considered a promising solution method for resource allocation tasks on optical networks due to its potential to learn novel strategies. RL solutions have been shown to compute allocations in comparable time to heuristic algorithms while approaching the solution quality of exact methods such as integer linear programming (ILP) \cite{di_cicco_deep_2022}. This combination of fast allocation with potentially superior performance to heuristics has made RL the subject of much research for traffic allocation on dynamic networks \cite{chen_deeprmsa_2019,shimoda_mask_2021,tang_heuristic_2022,xu_deep_2022,cheng_ptrnet-rsa_2024}. Elastic optical networks have been the main focus of this research due to their potential for efficient spectrum utilization if the problem of spectral fragmentation can be avoided \cite{gerstel_elastic_2012}. 

However, flex-rate networks \cite{lord_flexible_2022} with a fixed 50 or 100GHz grid offer efficient use of spectrum without fragmentation and it is reported that many current production systems use this paradigm \cite{nevin_techniques_2022}. Flex-rate transponders allow additional data services to be accommodated on existing lightpaths if they have spare capacity, a task termed routing and wavelength assignment with lightpath reuse (RWA-LR). Additionally, incremental loading of the network with non-expiring services is considered a more realistic traffic model than dynamic traffic \cite{nevin_techniques_2022}. The unknown nature of future requests in the incremental loading setting, with the constraint that active services cannot be disrupted, make ILP impractical, therefore RL is of interest. 

So far only one paper, Nevin et al. from 2022 \cite{nevin_techniques_2022}, has investigated RWA-LR and used a simple feed-forward neural network with only 10M training samples. We therefore aim to investigate how much we can improve on this work by employing a sophisticated graph attention network (GAT) architecture, to take advantage of the graph-structure of the problem, and 200M training samples for the final training run. This new scale of training is enabled by our GPU-based simulation framework, XLRON \cite{doherty_xlron_2024}, which we make openly available \cite{doherty_xlron_2023}. %\hyperlink{https://github.com/micdoh/XLRON}{https://github.com/micdoh/XLRON}. 

To establish the strongest benchmark to compare our RL results against, we evaluate K-Shortest Path First-Fit (KSP-FF) and First-Fit K-Shortest Paths (FF-KSP) heuristics and, crucially, compare their results when the candidate paths are ordered according to different sort criteria: 1) number of hops or 2) total length in km. Our trained RL agent improves on the strongest benchmark (KSP-FF with hops-ordering) and Nevin et al.'s RL solution.

Our work makes three novel contributions:

\begin{itemize}
    \item[1.] Thorough benchmarking of KSP-FF and FF-KSP with optimized path ordering, which shows hops-ordering to improve performance, with up to 6\% mean throughput increase in our case of study.
    \item[2.] Details of a new methodology for GAT-based RL applied to the RWA-LR problem with incremental traffic, giving new state-of-the-art results for network throughput, with 8.5 Tbps additional capacity compared to best benchmark.
    \item[3.] Release of all training and evaluation code from GPU-based framework, which enables RL training at a much larger scale than CPU-bound approaches.
\end{itemize}





\section{Previous Work}
Many works have examined RL for resource allocation in optical networks, but only a few have employed graph-based neural network architectures. Almasan et al. \cite{almasan_deep_2022} were the first to explore a message passing neural network (MPNN) with RL for routing in an optical network, which demonstrated improved performance compared to a theoretical fluid model and some capability to generalize to new topologies (with degraded performance). Xu et al. \cite{xu_deep_2022} used a graph convolutional neural network (GCN) as part of the policy and value functions in their actor-critic architecture for dynamic RMSA, and demonstrated 28\% improvement in blocking probability over KSP-FF and 22\% better than their implementation of DeepRMSA \cite{chen_deeprmsa_2019}. The GAT architecture was used in a recent work by Xiong et al. \cite{xiong_graph_2024} which demonstrated BP reduced from 2\% to 1\% compared to KSP-FF with K=5 on NSFNET for dynamic RMSA. The trend toward more advanced graph-based architectures is continued by Cheng et al. with their work PtrNet-RSA \cite{cheng_ptrnet-rsa_2024}, which employs a pointer network \cite{vinyals_pointer_2015} to select the nodes of the path and overcome the limitation of considering only k-shortest paths. Most of these works consider very similar problem settings (topologies and traffic models) and use the same benchmarks without investigating if they are the best benchmarks available. In this work, we establish the strongest heuristic benchmarks available (by testing different sort criteria for the candidate paths) to ensure our RL results are not trivial.

As discussed, only one previous work has studied the RWA-LR problem. Nevin et al. \cite{nevin_techniques_2022} made major contributions to the field by introducing RAW-LR and providing a detailed description of the techniques necessary to improve on their benchmarks, including invalid action masking and shortened training episodes. They demonstrate 18.4 Tbps increase in network throughput compared to their best benchmark (FF-KSP with K=5 and paths ordered by length). In this work we build on their recommendations and incorporate two improvements: we include the GAT architecture in our learning algorithm to exploit graph structure and we use our GPU-based simulation and training framework to enable training in a high-data regime.

%Shortcomings are they fail to investigate different sort criteria for the candidate paths. Also 10,000 service request episodes is an arbitrary number. Services accepted until first blocking event is a less arbitrary measure.





\section{Network Model}
\label{sec:method}

We recreate the problem setting from Nevin et al. exactly, including the physical layer model. The paper uses a regular incoherent nonlinear interference Gaussian noise (GN) model for the physical layer, assuming transmission at Shannon rate and rectangular spectrum pulses with channel bandwidth equal to symbol rate. The point-to-point throughput is calculated at optimum launch power, with nonlinear interference treated as white Gaussian noise, and is for total modulated bandwidth up to combined C+L band \cite{shevchenko_maximizing_2022}. The Shannon capacity is calculated according to equation \ref{eq:c_path}, assuming Nyquist rate transmission. $R_s$ is the symbol rate, $NSR_i$ is the noise-to-signal ratio of the $i^{th}$ link of the path.

\begin{equation}
C_{path} = 2R_S \cdot \log_2\left(1 + \frac{1}{\sum_i NSR_i}\right)
\label{eq:c_path}
\end{equation}


For brevity, we do not include full details of the physical layer model here and instead direct readers to Nevin et al. \cite{nevin_techniques_2022}. For each path between node pairs, we calculate the path capacity. When a lightpath is established on that path, it is designated the pre-calculated capacity, which is incrementally reduced with each service that is added to the lightpath. Each lightpath can therefore accommodate multiple services until the capacity is exhausted.

We consider the NSFNET topology with bi-directional links\footnotemark, 100GHz WDM channel spacing, and non-expiring service requests of 100~Gbps that arrive sequentially (we term this "incremental loading"). We define an episode as 10,000 service requests to align with Nevin et al. but we note this is an arbitrary length. We consider a uniform traffic matrix (each node pair has equal chance of being selected for a request).


\footnotetext{We note real-world networks are likely to have dual fiber pairs with dedicated directions of propagation, but we consider bi-directional links to align with previous work.}



\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{IMAGES/xlron_rwalr.png}
    \caption{Outline of our RL training and optical network simulation framework, XLRON. The hierarchy of Device$_{LEARN}$, Learner, Device$_{ENV}$ and Environment computational abstractions are shown left. Right shows details of the training loop for a single Learner (set of neural network parameters) acting over parallel environments. The topology shown is illustrative; we use the NSFNET topology for our studies.}
    \label{fig:xlron}
\end{figure*}


We use the XLRON framework \cite{doherty_xlron_2023} to implement the RWA-LR environment. XLRON uses the JAX array computing framework \cite{bradbury_jax_2018} to allow compilation of programs to accelerator hardware, such as GPU or TPU. JAX imposes the constraint of functional programming to enable this compilation and requires all data structures used in XLRON are arrays. We track the spectrum occupancy, capacity of each lightpath, and the ID of each lightpath on every link. We verify that our recreation of Nevin et al.'s network model is accurate by comparison of our results for KSP-FF and FF-KSP, which reproduce Nevin et al.'s published results.











\section{Reinforcement Learning Agent Architecture}
\label{sec:rl}


We use the widely adopted Proximal Policy Optimization (PPO) \cite{schulman_proximal_2017} as our RL algorithm. PPO uses an actor-critic architecture, with separate learned policy and value functions. The value function provides a baseline estimate of the expected returns from a given state, which is subtracted from the calculated returns from a given rollout (sequence of state-action-reward tuples) to reduce the variance in the policy gradient, and give more stable updates to the parameters of the policy and value functions.

In our implementation, we use GAT's for the policy and value functions. GAT's use the message-passing framework of conventional graph neural networks with an additional learned attention mechanism to weight the contributions of messages from neighbors \cite{velickovic_graph_2018}. GAT's take graph-structured data as input and output. The input graphs for our GATs have the current service request as a global feature and the spectrum occupancy of links as edge features. The input graph features are processed by repeating the following steps for each round of message-passing: 1) Concatenate edge features with neighboring node features and global features, 2) transform concatenated features with a multi-layer perceptron (MLP), 3) apply a learned attention matrix to weight the transformed edge features, 4) aggregate the weighted edge features at nodes, 5) concatenate the node, global, and aggregated edge features, 6) transform the concatenated features with a MLP. 

The output graph features form the basis of our action selection distribution. For each candidate path, we element-wise sum the processed features of the path links, normalize the resulting array by the path length, and concatenate the aggregated arrays from each path to form a $K \times S$ array, where $K$ is the number of candidate paths and $S$ is the number of WDM channels. The array is passed to a softmax function to select the action, which is interpreted as a path and WDM channel. This method has the advantage of being scalable to an arbitrary number of paths. We also use the invalid action masking technique \cite{shimoda_mask_2021}. We make all code available on Github \cite{doherty_xlron_2023}.

Figure \ref{fig:xlron} provides an overview of the XLRON framework we use to implement our agent and environment. To enable scaling to hundreds of parallel environments on a single device, XLRON maintains a logical hierarchy of Device, Learner, and Environment, shown on the left of Figure \ref{fig:xlron}. A Learner represents a single learning agent, which can exist on one or more Devices. Environments can exist on only one Device and belong to one Learner, but the Learner may be replicated across Devices, to enable multi-device training. The right side of Figure \ref{fig:xlron} shows the RL training loop for a single Learner. The purple region indicates the internals of the agent with GAT for the policy and value functions, and rollout data stored in trajectory buffers for parameter updates. The batched parallel environments are illustrated in green, with each learner providing actions for the environments in parallel. The reward function for a single step is simply +1/-1 for success/failure in the service allocation.



\section{Training}
\label{sec:results}


The advantage of implementing both the agent and environment in JAX is the entire training loop can be compiled as a single program for greater efficiency, which is what enables us to scale training our computationally expensive GAT architecture to 200M samples. We perform all our experiments on a Nvidia A100 80GB. We performed a sweep of learning rate, discount factor, the generalized advantage estimation $\lambda$ parameter, GNN parameters of message passing steps, latent space dimensions, and learning rate schedule. We summarize the final values of the parameters in the following command, which we used to initiate the final training run:

{\footnotesize
\begin{samepage}
\begin{verbatim}
train.py --env_type rwa_lightpath_reuse
--incremental_loading --k 5 --link_resources 100
--topology_name nsfnet_deeprmsa_undirected  
--max_requests 10000 --values_bw 100 
--TOTAL_TIMESTEPS 200000000 --UPDATE_EPOCHS 10 
--ROLLOUT_LENGTH 150 --ACTION_MASKING 
--NUM_ENVS 100 --LR_SCHEDULE warmup_cosine 
--WARMUP_END_FRACTION 0.1 --scale_factor 0.2
--WARMUP_STEPS_FRACTION 0.1 --USE_GNN
--WARMUP_PEAK_MULTIPLIER 2 --gnn_mlp_layers 2
--message_passing_steps 3  --GAE_LAMBDA 0.984 
--GAMMA 0.919 --LR 1.943e-05 --gnn_latent 128 
\end{verbatim}
\end{samepage}
}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{IMAGES/rwalr_training.png}
    \caption{Training of our agent, compared with published RL results from Nevin et al. and our strongest heuristic benchmark (5-SP-FF with paths ordered by hops). Shaded areas indicate standard deviations. We used 100 parallel environments, with mean and standard deviation of accepted services at end of each episode calculated across environments.}
    \label{fig:training}
\end{figure}

The training run took 10h 50m for 200M environment transitions across 100 parallel environments and a rollout length of 150, which equates to 13,333 parameter updates. Each parallel environment completed 1000 episodes of 2000 steps, giving 200M steps total. Training required 10GB of RAM. We could have scaled to more parallel environments to provide further speed-up but were limited by other users of the device.

As recommended by Nevin et al., we use a scaling factor of 0.2 to reduce the length of our training episodes compared to our evaluation episodes. We train on episodes of 2,000 service requests and evaluate on 10,000. Each parallel environment therefore steps through 100,000 training episodes. This shorter training episode length reduces the horizon over which the RL agent needs to optimize, and reduces the difficulty of credit assignment \cite{pignatelli_survey_2023}, but is demonstrated to generalize to longer episodes in this case.

Figure \ref{fig:training} shows the progression of training the XLRON agent in comparison to Nevin et al.'s published results for training, and the performance of the KSP-FF heuristic with paths ordered by hops. The agent exceeds the benchmark performance after training on 60M environment steps, after which it improves gradually and stabilizes.



\section{Evaluation}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{IMAGES/boxplots.png}
    \caption{Each heuristic and RL solution was evaluated on 100 episodes fo 10,000 service requests. Boxplots show the mean, median, standard deviation and 1.5*interquartile range for accepted services from those episodes. \#hops indicates heuristics with candidate paths ordered by number of hops.}
    \label{fig:boxplots}
\end{figure}

To evaluate the trained model, we compare it to the best-performing heuristics from Nevin et al.: KSP-FF and FF-KSP \cite{vincent_scalable_2019}. We consider K=5 for our case to align with previous work. We include hops-ordering and length-ordering of paths in our analysis. Figure \ref{fig:boxplots} shows the distribution of total accepted services at the end of a 10,000 request evaluation episode, for 100 episodes in each case. Ordering by number of hops causes more requests to be accepted for both heuristics, with 6\% increased throughput for KSP-FF\footnotemark. This increase causes KSP-FF$_{hops}$ to exceed the RL results from Nevin et al., therefore highlighting the importance of benchmarking against diverse heuristics. XLRON improves on all cases, with 1.8\% increased throughput (8.5~Tbps) over KSP-FF$_{hops}$.


\footnotetext{We note that a more realistic physical layer model that accounts for ROADM losses at nodes is likely to show an even greater improvement in hops-ordering compared to length-ordering.}




\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{IMAGES/waterfall.png}
    \caption{Comparison of services accepted by XLRON vs. KSP-FF$_{hops}$ for the same evaluation episodes (same sequences of service requests). Green bars indicate additional services accepted by XLRON, red indicates more services accepted by KSP-FF$_{hops}$.}
    \label{fig:waterfall}
\end{figure}


To thoroughly evaluate the performance of our agent, we apply both the trained agent and KSP-FF$_{hops}$ to the exact same sequence of traffic requests for 100 episodes. The difference in the total accepted services between the agent and heuristic in each episode is shown in Figure \ref{fig:waterfall}. In 91/100 episodes, our agent allows more services to be accepted, with a mean of 85 more. That the agent does not succeed in every episode shows the highly stochastic nature of the environment, for which it may be very difficult to learn a policy that outperforms in all cases.



\section{Conclusions}
\label{sec:conclusions}

Our work shows that it is possible to train an RL agent to outperform the best heuristic algorithms for RWA-LR. Thorough benchmarking is required to ensure that sophisticated RL solutions aren't outperformed by simple heuristics, and the ordering of shortest paths in heuristic algorithms has a large impact on throughput. In our case, training an agent to outperform the best heuristic required considerable computational resources, engineering effort, and graph-based learning algorithms to take advantage of the graph structure of the data. We could not have achieved this without our GPU-based simulation framework.

The less than 2\% improvement we achieve over the best heuristic highlights the difficulty of improving on the best heuristics and of learning effective RL policies on long horizon resource allocation tasks. We note that the projected annual increases in data traffic \cite{ericsson_ab_ericsson_2024} are likely to render such marginal gains as insignificant and might not be worth the additional operational complexity.

%The XLRON project is easily extensible with additional functionality, which will include more environments, such as SDM network simulations, and calculations of physical layer impairments using closed form approximations of the ISRS GN model \cite{buglia_closed-form_2022}. 
%More sophisticated deep learning model architectures for improved feature extraction and representation learning, such as graph neural networks (GNN), can be integrated with XLRON to derive maximal benefit of the new training regime and the extensive models available in the Deepmind JAX ecosystem \cite{babuschkin_deepmind_2020}.


\bibliography{paper.bib}
%\printbibliography


\end{document}




