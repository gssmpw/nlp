\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{geometry}
\geometry{margin=25mm}
\usepackage{amsmath, amssymb}
\usepackage{bm}
\usepackage{xcolor}
\usepackage{hyperref}

\usepackage{cmbright} % chooses the font
\usepackage[T1]{fontenc} % font encoding output 
\usepackage[utf8]{inputenc} % font encoding input

\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}

%\usepackage{showlabels}

\usepackage{algorithm}
\usepackage{algpseudocode}


%\usepackage{biblatex}
%\addbibresource{main.bib}
 \usepackage[numbers]{natbib}
 \bibliographystyle{unsrtnat}
% \bibliography{tensor-train}


% \AtEveryCitekey{
%     \clearfield{month}
%     \clearfield{urldate}
%     \clearfield{url}
%     \clearfield{issn}
%     } 
% \AtEveryBibitem{
%     \clearfield{url}\clearfield{issn}\clearfield{isbn}\clearfield{month}\clearfield{eprint}\clearfield{eventtitle}
%     } 

\graphicspath{{pictures/}}


\newcommand{\bfA}{ {\bf A} }
\newcommand{\bfu}{ {\bf u} }
\newcommand{\bfx}{ {\bf x} }
\newcommand{\bfj}{ {\bf j} }
\newcommand{\ibfj}{ {\boldsymbol{j}  } }
\newcommand{\bfxi}{ {\bm \xi} }
\newcommand{\bff}{ {\bf f} }
\newcommand{\bfv}{ {\bf v} }
\newcommand{\bfH}{ {\bf H} }
\newcommand{\bfzero}{ {\bm 0} }
\newcommand{\bfomega}{ {\bm \omega} }
\newcommand{\R}{ \mathbb R }
\newcommand{\E}{ \mathbb E }
\newcommand{\diff}{ \; \text{d} }
\newcommand{\var}{ \text{Var} }
\newcommand{\minmod}{ \text{minmod} }
\newcommand{\xiQuad}[1]{ \xi_{j_{q_{#1} }} }
\newcommand{\flux}[4]{ {\mathbfcal F}_{ {#1},{#2} }^{ {#3} {#4}} }
\newcommand{\fluxE}[4]{ \bar{\mathbfcal F}_{ {#1},{#2} }^{ {#3} {#4}} }


\newcommand{\mh}[1]{{\color{red} {#1}}}
\newcommand{\sm}[1]{{\color{blue} {#1}}}
\newcommand{\jd}[1]{{\color{violet} {#1}}}


\newtheorem{remark}{Remark}

\title{High-dimensional stochastic finite volumes using the tensor train format}
\author{Juliette Dubois, Michael Herty, Siegfried M\"uller}
\date{2024}

\begin{document}

\maketitle

% \begin{itemize}
% 	%\item Intro: look for "stochastic finite volume method" to see what has been done
%     %\item Look for ref on the cross approximation performances
%     %\item Make a scheme for the mixed format 
%     %\item Look for reference for the convergence of central Euler?
%     %\item Check whether the scheme is conservative: for a shock, there should be the same 'shape' on either side of the chock. 
%     %\item Make a comment on the precision (should be in $\mathcal O (\Delta x^2 \Delta \xi^2)$)
%     %\item References on large nb of parameters for hyperbolic problems: see Christophe Chalons, Markus Bachmayr, Wolfgang Dahm, Hesthaven (for high-dim elliptic/parabolic problems), Martin Frank 
% \end{itemize}
%Code 
%\begin{itemize}
    %\item Recompute the expectations -> doesn't seem to change the results a lot, I can rerun everything later
    %\item Add more points for the "Ntt = f(dim)" figure 
%\end{itemize}


\paragraph{Abstract. }
We propose a method for the uncertainty quantification of nonlinear hyperbolic equations with many uncertain parameters. 
The method combines the stochastic finite volume method and tensor trains in a novel way: 
the physical space and time dimensions are kept as full tensors, while all stochastic dimensions are compressed together into a tensor train. 
The resulting hybrid format has  one tensor train for each spatial cell and each time step. 
We adapt a MUSCL scheme to this hybrid format and show the feasibility of the approach using several classical test cases. 
A convergence study is done on the Burgers' equation with three stochastic parameters.
We also solve the Burgers' equation for an increasing number of stochastic dimensions and show an example with the Euler equations. 
The presented method opens new avenues for combining uncertainty quantification with well-known numerical schemes for conservation law.  


\section{Introduction}
This work focuses on uncertainty quantification for nonlinear hyperbolic equations with many parameters. 
Various methods have been developed to study how uncertainties affect partial differential equations.
Two main categories are Monte Carlo-type methods and the use of an orthogonal basis, for example with the generalized polynomial chaos expansion (gPC). 
The gPC method can be either intrusive, with e.g. stochastic Galerkin methods, or non-intrusive, e.g. stochastic collocation. 
The Monte Carlo method is non-intrusive and its convergence rate is independent of the problem dimension, but it converges very slowly. 
The gPC method has an exponential convergence rate when the solution of the equation depends smoothly on the stochastic parameters.
This makes it well-suited for elliptic and parabolic equations, but the smooth dependency is in general not observed for hyperbolic problems. 

Barth \cite{barth_propagation_2012} introduced the stochastic finite volume method (SFV) to study hyperbolic equations with uncertainties. 
The SFV is a deterministic formulation of the equations that keeps some properties of the original hyperbolic problem, such as well-posedness \cite{mishra_sparse_2012}. 
However, since a new dimension is added for each uncertain parameter, solving numerically the equations quickly becomes impossible. 

When adapting methods that were developed for low dimensions to high dimensions, the number of parameters at hand increases exponentially. 
This phenomenon is called the curse of dimensionality. 
Low-rank tensor formats appear as a possible solution to keep the storage and operations manageable. 
Low-rank tensors can be seen as a generalization to tensors of the well-known low-rank matrix decomposition. 
Several tensor formats have been developed, among them the canonical, the Tucker and the hierarchical formats \cite{bachmayr_low-rank_2023}. 
Here,  we focus on one hierarchical format, namely the tensor train format. 
Tensor trains have been successfully used for solving elliptic, parabolic and linear hyperbolic problems \cite{oseledets_tensor-train_2011, kornev_numerical_2023, zhong_fast_2018}. 
The non-linear hyperbolic case presents new difficulties because the structure is much less preserved over space and time: for example, shocks can appear. 
In \cite{walton_tensor-train_2024}, the authors propose to combine SFV with tensor trains and obtain promising results. 

In this paper, we propose a mixed formulation where --- in contrast to  \cite{walton_tensor-train_2024} ---  the time and physical space are kept in the full format while the stochastic space is compressed in the tensor train format.
Section \ref{sec:SFV} introduces the problem and presents the principle of the stochastic finite volume method.
In Section \ref{sec:TT} we recall the main ideas of the tensor train format.
The new mixed format is presented in Section \ref{sec:Mixed}.
A MUSCL-type algorithm adapted to the mixed format is then described. 
It highlights the few modifications needed to use tensor trains compared to the classical scheme.
In Section \ref{sec:Numerics}, we show numerical experiments that prove the feasibility and efficiency of our approach. 


\section{The stochastic finite volume method} \label{sec:SFV}
We are interested in conservation laws with uncertain initial data. 
The problem is presented in the one-dimensional space, but the method is the same for higher dimensions. 
Let $T>0$, $\Omega_x\subset\R$, $\Omega_\bfxi\subset\R^m$, 
and let $\bfu:(0,T)\times\Omega_x\times\Omega_\bfxi\to\mathbb{R}^p$ be the solution to 
\begin{align}  
    \label{eq:PDE}
    \frac{\partial \bfu }{\partial t}(t,x; \omega_\bfxi)
    + \frac{\partial \bff}{\partial x} (\bfu (t,x; \omega_\bfxi))
    = \bfzero, \quad x \in \Omega_x, \ \omega_\bfxi \in \Omega_\bfxi, \ t \in (0,T),
    \\
    \label{eq:PDE_IC}
    \bfu (0, x; \omega_\bfxi) = \bfu_0(x; \omega_\bfxi)
    , \quad x \in \Omega_x, \ \omega_\bfxi \in \Omega_\bfxi.
\end{align}
Here, $\bff = (f_1, \dots f_p)$ is the flux field. 
The random input $\omega_\bfxi$ is parametrized by a random variable $\bfxi:\Omega\to\Omega_\bfxi$ defined on the probability space 
$(\Omega, \mathcal F, \mathbb P)$.
We assume that there exists a probability density 
$p:\R^m\to[0,\infty)$ such that the expectation and the variance for $\bfu$ can be expressed as 
\begin{equation}
\E[\bfu(t,x)] = \int_{\Omega_\bfxi} \bfu(t,x ; \omega_\bfxi) p (\bfxi) \diff \bfxi, \quad
\var [\bfu(t,x)] = \E[(\bfu(t,x)-\E[\bfu(t,x)])^2].  
\end{equation}


\subsection{General description of the method}
The idea is to consider the space $\Omega_\bfxi\subset\R^m$ as new "spatial directions" of the problem \cite{jin_stochastic_2017}. 
%The new variable $\bfxi$ is not transported by a flux.
The stochastic problem \eqref{eq:PDE}-\eqref{eq:PDE_IC} is reformulated as a deterministic problem for the unknown 
$\bfu(t,x,\bfxi)$. 

%Following the description from \cite{jin_stochastic_2017}:
The physical space $\Omega_x$ and the parametrized probability space $\Omega_\bfxi$
are discretized as Cartesian grids, respectively denoted by $\mathcal{C}_x$ and $\mathcal{C}_\bfxi$. 
The time interval $[0,T]$ is discretized with a time step $\Delta t$, 
\[
\mathcal C_x   = \cup_i K_x^i, 
\quad
\mathcal C_\bfxi = \cup_\ibfj K_\bfxi^\ibfj, 
\quad
[0,t]= \cup_k [t_k, t_k + \Delta t], 
\]
where $\bfj=(j_1, \dots j_m)$ is a multi-index.
We introduce a constant mesh size in the spatial direction $\Delta x$,
and in each stochastic dimension $\Delta \xi_1,\dots,\Delta \xi_m$.  
The cells $K_x^i$ and $K_\bfxi^\ibfj$ are defined as 
\begin{align}
    K_x^i = [x_{i-1/2}, x_{i+1/2} ], 
    \quad x_{i \pm 1/2} = x_{i} \pm \frac{\Delta x}{2}, 
    \\
    K_\bfxi^\ibfj = \Pi_{\ell = 1}^m [\xi_{j_\ell-1/2}, \xi_{j_\ell + 1/2} ],
    \quad 
    \xi_{j_\ell\pm 1/2} = \xi_{j_\ell } \pm \frac{\Delta \xi_\ell}{2} .
\end{align}
Let $N_x$ and $N_{\xi_1},\dots,N_{\xi_m}$ denote the number of 1D cells in each spatial and stochastic direction, respectively. 

We introduce the cell average in space
\begin{equation}
    \bfu_{i}^n(\bfxi) =
    \frac{1}{|K_x^i|}
    \int_{K_x^i}  \bfu (t^n,x,\bfxi) \diff x,
\end{equation}
and the cell average in space and in expectation over a cell $\bfj$
\begin{equation}
    \bar \bfu_{i,\ibfj}^n =
    \frac{1}{|K_\bfxi^\ibfj|}
    \E_\ibfj [\bfu_{i,\ibfj}^n]
    =
    \frac{1}{|K_x^i||K_\bfxi^\ibfj|}
    \int_{K_\bfxi^\ibfj}  \int_{K_\bfx^i}  \bfu (t^n ,x,\bfxi)
    \diff x \, p(\bfxi) \diff \bfxi.
\end{equation}
Here the cell volumes are 
\begin{equation}
    |K_x^i| = \int_{K_x^i} \diff x = \Delta x, 
    \qquad 
    |K_\bfxi^\ibfj| = \int_{K_\bfxi^\ibfj} p(\bfxi) \diff \bfxi.
\end{equation}
\\

Integrating Equation \eqref{eq:PDE} in space and taking the expectation over a cell yields   
\begin{equation} \label{eq:PDE_integral}
    \int_{K_\bfxi^\bfj} \int_{K_x^i}
    \frac{\partial \bfu }{\partial t}(t^n ,x,\bfxi)
     \diff x \, p(\bfxi) \diff \bfxi 
    + 
    \int_{K_\bfxi^\bfj} \int_{K_\bfx^i} 
    \frac{\partial \bff}{\partial x} (\bfu (t^n ,x,\bfxi))
     \diff x  \, p(\bfxi) \diff \bfxi 
    = \bfzero. 
\end{equation}
After integration by parts, Equation \eqref{eq:PDE_integral} reads 
\begin{equation}\label{eq:PDE_integral-2}
    \Delta x |K_\bfxi^\ibfj| \frac{d \bar \bfu_{i,\ibfj}^n}{d t}
    + \E_\ibfj [ {\bf f}(\bfu (t,x_{i+1/2},\bfxi) ) - {\bf f}(\bfu (t,x_{i-1/2},\bfxi)) ]
    = \bfzero.
\end{equation}

The exact flux $\bff$ is replaced by a numerical flux $\mathbfcal F$: 
the value of the solution at the interface $x_{i+1/2}$ is 
approximated using the values of the averages in the surrounding cells 
$\bar \bfu_{i-1,\ibfj}^n , \bar \bfu_{i,\ibfj}^n , \bar \bfu_{i+1,\ibfj}^n $. 
Let $\flux{i}{\ibfj}{n}{+}$ and $\flux{i}{\ibfj}{n}{-}$ denote the numerical flux at the interface 
$x_{i+1/2}$ and $x_{i-1/2}$, respectively. 
The approximation for their expectation over each cell is denoted by $\fluxE{i}{\ibfj}{n}{+}$ and $\fluxE{i}{\ibfj}{n}{-}$. Then Equation \eqref{eq:PDE_integral-2} is approximated by
\begin{equation} \label{eq:PDE_numFlux}
    \Delta x |K_\bfxi^\ibfj| \frac{d \bar \bfu_{i,\ibfj}^n }{d t}
    + \fluxE{i}{\ibfj}{n}{+} - \fluxE{i}{\ibfj}{n}{-}
    = 0, 
    \qquad \fluxE{i}{\ibfj}{n}{+}
    = \E_\ibfj [ \mathcal F^+(\bfu_{i,\ibfj}^n ) ] , 
    \quad \fluxE{i}{\ibfj}{n}{-}
    = \E_\ibfj [ \mathcal F^-(\bfu_{i,\ibfj}^n ) ].
\end{equation}

The integrals are approximated using the midpoint rule. 
We introduce the approximated volume for the stochastic cells $K_\ibfj=\Delta\xi_1\dots\Delta\xi_m\,p(\xi_\ibfj)$.
The approximated average over a spatial cell is 
\begin{equation} \label{eq:quad_space}
\bar \bfu_{i,\ibfj}^n  \approx 
u(t^n , x_i, \bfxi_\ibfj) \, p(\bfxi).
\end{equation}
By abuse of notation, let $\bar \bfu_{i,\ibfj}^n$ and $\fluxE{i}{\ibfj}{n}{\pm}$ denote the approximated expectation of the solution and the numerical fluxes, respectively.
The semi-discrete equation reads then
\begin{equation} \label{eq:PDE_semiDiscrete}
    K_\ibfj \Delta x \frac{d \bar \bfu_{i,\ibfj}^n }{d t}
    + K_\ibfj \fluxE{i}{\ibfj}{n}{+}
    - K_\ibfj \fluxE{i}{\ibfj}{n}{-}
    = \bfzero.
\end{equation}
After simplifications ($K_\ibfj\neq 0$), the semi-discrete scheme reads 
\begin{equation}
    \frac{d \bar \bfu_{i,\ibfj}^n }{d t}
    + \frac{1}{\Delta x} \left(
        \fluxE{i}{\ibfj}{n}{+} - \fluxE{i}{\ibfj}{n}{-}
    \right)
    = \bfzero.
\end{equation}

\subsection{Numerical flux}
The numerical flux is computed for a MUSCL scheme \cite{kurganov_new_2000}.
%For simplicity, the scheme is described for 
%one physical dimension ($d=1, \bfx = x$) and 
%two stochastic dimensions 
%($m=2, \bfxi=(\xi_1, \xi_2), \bfj=(j_1,j_2)$). 
The expectation of the numerical flux at the interface $i+1/2$ is defined by
\begin{equation}
\bar{\bfH}_{i+1/2,\ibfj}^n  = 
\frac{ \bff \left( \bfu_{i+1/2,\ibfj}^{n +} \right) 
     + \bff \left( \bfu_{i+1/2,\ibfj}^{n -} \right) }{2}
- \frac{a_{i+1/2,\ibfj}^n}{2} \left( \bfu_{i+1/2,\ibfj}^{n +}  - \bfu_{i+1/2,\ibfj}^{n -}  \right),
\end{equation}
where intermediate values $\bfu_{i+1/2,\ibfj}^{n +}, \bfu_{i+1/2,\ibfj}^{n -}$ are linear reconstructions of the solution using the approximate derivative,
and $a_{i+1/2,\ibfj}^n$ is the local speed. 
Since the midpoint rule is used to compute the expectation, all quantities should be understood as evaluated at the point $\bfxi_\ibfj$. 
The intermediate values are computed at the interface between two cells in space: 
\begin{align} \label{eq:SFV_interpolation}
    \bfu_{i+1/2,\ibfj}^{n +} = \bar \bfu_{i+1,\ibfj}^n  - \frac{\Delta x}{2} (\bfu_x)_{i+1,\ibfj}^n  
    \\
    \bfu_{i+1/2,\ibfj}^{n -} = \bar \bfu_{i,\ibfj}^n  + \frac{\Delta x}{2} (\bfu_x)_{i,\ibfj}^n  
\end{align}
The approximation of the derivative $ (\bfu_x)_{i,\ibfj}^n $ is computed with a limiter (here the $\minmod$ function), 
\begin{equation}
    (\bfu_x)_{i,\ibfj}^n  = \minmod \left(
        \frac{\bar{\bfu}_{i,\ibfj}^n - \bar{\bfu}_{i-1,\ibfj}^n }{\Delta x}, \frac{\bar{\bfu}_{i+1,\ibfj}^n - \bar{\bfu}_{i,\ibfj}^n }{\Delta x}
    \right). 
\end{equation}
The local speed is given by 
\begin{equation}
    a_{i+1/2,\ibfj}^n  = \max\left\{
        \rho(\bff'(\bfu_{i+1/2,\ibfj}^{n +})), \rho(\bff'(\bfu_{i+1/2,\ibfj}^{n +}))
    \right\},
\end{equation}
where $\rho(\bfA)$ denotes the spectral radius of the matrix $\bfA$. 
We recall that, if $\lambda_i(\bfA)$ are the eigenvalues of  $\bfA$, the spectral radius is defined by
$\rho(\bfA) = \max_i |\lambda_i(\bfA)|$. 

% An approximation of the expectation of the numerical flux is obtained by evaluating all the above quantities at the point $\bfxi_\ibfj$. 
% \begin{equation}
% \bfH_{i+1/2,\ibfj}^n  = 
% \frac{ \bff \left( \bfu_{i+1/2,\ibfj}^{n +} \right) 
%      + \bff \left( \bfu_{i+1/2,\ibfj}^{n -} \right) }{2}
% - \frac{a_{i+1/2,\ibfj}^n}{2} \left( \bfu_{i+1/2,\ibfj}^{n +}  - \bfu_{i+1/2,\ibfj}^{n -}  \right),
% \end{equation}



% \subsection{Averaging}
% We describe now how the cell average over a spatial cell is computed.
% %We make the additional assumption that the random variables are independent. 
% %Let $p_1, p_2$ be the distribution associated to $\xi_1$ and $\xi_2$ respectively. 
% The exact average over a cell $(i,j_1,j_2)$ for the solution at time $t$ is given by
% \begin{align} \label{eq:average_exact}
%     \bar \bfu_{i, j_1, j_2}^n  = 
%     \frac{1}{\Delta x |K_{\xi_1}^{j_1}||K_{\xi_2}^{j_2}|}
%     \int_{\xi_{j_1-1/2}}^{\xi_{j_1+1/2}} 
%     \int_{\xi_{j_2-1/2}}^{\xi_{j_2+1/2}} 
%     \int_{x_{i-1/2}}^{x_{i+1/2}}
%         \bfu(t^n, x, \xi_1, \xi_2) 
%     \diff x \, 
%     p_1(\xi_1) p_2(\xi_2) \diff \xi_1 \diff \xi_2, 
%     \\ 
%     |K_{\xi_1}^{j_1}| = \int_{\xi_{j_1-1/2}}^{\xi_{j_1+1/2}} p_1(\xi_1) \diff \xi_1, 
%     \quad 
%     |K_{\xi_2}^{j_2}| = \int_{\xi_{j_2-1/2}}^{\xi_{j_2+1/2}} p_1(\xi_2) \diff \xi_2.
% \end{align}




\subsection{Fully-discrete formulation and computational cost}
We conclude the presentation of the stochastic finite volume method with the time discretization. 
A simple forward Euler scheme yields
\begin{equation} \label{eq:Full_discrete}
    \bar \bfu_{i,\ibfj}^{n +1} 
    = \bar \bfu_{i,\ibfj}^n  
    - \frac{\Delta t}{\Delta x} (\bar \bfH_{i+1/2,\ibfj}^n  - \bar \bfH_{i-1/2,\ibfj}^n ).
\end{equation}
Note that in the case of a forward Euler discretization in time, the cell average in space is computed only for the initial condition. 
The cell average of the next time steps is directly given by \eqref{eq:Full_discrete}.
%However interpolation and integration over the stochastic space is required at each time step. 

%question{Write a summary here of the method?} 

Theoretically, the stochastic finite volume method has the advantage to give a complete description of the solution dependency to the random variables. 
However, the problem becomes computationally intractable when the dimension of the stochastic space $\Omega_\bfxi$ increases. 
Indeed, in the general case 
%$d\geq~1, m\geq~1$ 
$m\geq 1$ with
%$N_{x_1},\dots,N_{x_d}$ spatial cells, 
$N_{\xi_1},\dots,N_{\xi_m}$ stochastic cells and $N_t$ time steps, 
the discrete solution $\bar \bfu_{i,\ibfj}^n $ is a tensor with a number of entries growing exponentially with the number of directions: 
\begin{equation}
    \left(\bar \bfu_{i,\ibfj}^n \right)_{
        \begin{subarray}{l}n=1,\dots,N_t \\
            i=1,\dots,N_x \\
            \ibfj = (1,\dots,1),\dots,(N_{\xi_1},\dots,N_{\xi_j})
            % j_1=1,\dots,N_{\xi_1} \dots
            % j_m=1,\dots,N_{\xi_m}
            \end{subarray}
        }
    \in \R^{p N_t} \times 
    \R^{p N_{x_1}}   \dots \times \R^{p N_{x_d}} \times 
    \R^{p N_{\xi_1}} \dots \times \R^{p N_{\xi_m}}.
\end{equation}
For the considered problem, 
%$d$ is usually small (from 1 to 3 spatial dimensions), but 
$m$ can become very large, making the tensor $\bar \bfu_{i,\ibfj}^n$ too large for computation or storage. 
To circumvent this problem, we propose to use a low-rank tensor approximation. 
Following \cite{walton_tensor-train_2024} we choose to use the tensor train format. 
Tensor trains are briefly described in the next section. 

\section{The tensor train format} \label{sec:TT}
Various low-rank formats for tensors have been proposed \cite{bachmayr_low-rank_2023}. 
We focus on the tensor train format, which was introduced by \cite{oseledets_tensor-train_2011} for numerical analysis.  

\subsection{Definition of a tensor train}
For $m \in \mathbb N$, an $m$th-order tensor $T~\in~\R^{N_1 \times \dots \times N_m}$ is a tensor train if it can be written as a sum of products of third-order tensors,
\begin{equation}
    T(i_1,\dots,i_m) 
    = \sum_{\alpha_1 \dots \alpha_m =1 }^{r_1 \dots r_m}
    G_1(i_1,\alpha_1) G_2(\alpha_1, i_2, \alpha_2) \cdots G_m(\alpha_m, i_m).
\end{equation}
The third-order tensors $G_\ell \in \R^{r_{\ell-1} \times N_\ell \times r_\ell}$ are called the cores of the tensor train, and the values $r_1,\dots,r_m$ are called the ranks.
For the first and the last cores, the ranks are set to $r_0=r_{m+1}=1$.
The tensor train rank (TT-rank) is the largest rank of the tensor train. We denote it by $r=\max_{\ell=1 \dots m}(r_\ell)$. 

The tensor train format has two properties that make it well suited for our objective: it breaks the curse of dimensionality, and the associated approximation problem can be solved. 
Breaking the curse of dimensionality is a common goal of low-rank formats. 
For an $m$th-order tensor with $N$ entries in each dimension, a total of $N^m$ entries is required to describe the tensor. 
Adding new dimensions to the tensor increases exponentially the number of entries. 
For a tensor train, only $\mathcal O((m-2)Nr^2+2Nr)$ entries are necessary: adding new dimensions to a tensor train increases only polynomially its number of entries. 
Not all tensors have a low-rank representation, hence, for practical applications it is necessary to have some way of approximating a given tensor by a tensor in the low-rank format of choice.
In this regard, the tensor train format is more convenient than other low-rank formats, the canonical decomposition for example.
It was proved in  \cite{oseledets_tensor-train_2011} that the problem of finding a quasi-optimal tensor train approximation of a tensor for a given rank is well-posed. 
The proof is constructive, and the paper provides an algorithm to construct the quasi-optimal approximation.
In contrast, the canonical decomposition has a higher compression rate, but its approximation problem is ill-posed \cite{de_silva_tensor_2008}.

\subsection{Operations on tensor trains}

Tensor trains allow for a range of algorithms for many common tensor operations. 
We list here the ones that will be used in the rest of the paper. 
Algebraic operations on tensors such as addition, element-wise multiplication, and tensor contraction can be done directly in the tensor train format by applying the appropriate operations on the cores. 
However, the resulting tensors have generally larger ranks.
A rounding algorithm was introduced in \cite{oseledets_tensor-train_2011}. 
The rounding of a tensor train consists in approximating it with a tensor train of a lower rank. This operation is again done directly in the tensor train format, and requires $\mathcal O(mNr^2+mr^4)$ operations. 
Hence, after each algebraic operation, the result should be rounded to keep the rank low.

Applying a non-algebraic function to a tensor train is also possible thanks to the cross approximation algorithm. The cross approximation was first introduced in \cite{oseledets_tt-cross_2010}, and a version with adaptive ranks was developed in \cite{savostyanov_fast_2011}.
Cross approximation consists in evaluating the function at some entries of the tensor. 
%It is an iterative algorithm: starting from a first guess, the cores are updated by an algorithm inspired from 
%the Density Matrix Renormalization Group (DMRG) scheme.
%The entries used to apply the DMRG scheme are selected by the maxvol algorithm. 
%To our knowledge, mathematical estimates for the maxvol algorithm for the tensor train format are yet to be developed. 
%In practice, the cross-interpolation has shown good approximation results with a reasonable speed . 
Error estimates for the cross approximation algorithm have been developed in \cite{savostyanov_quasioptimality_2014,osinsky_tensor_2019} for element wise errors and in \cite{qin_error_2022} for errors in the Frobenius norm. 
The main results of those papers is that the approximation error does not grow exponentially with the number of dimensions. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{A mixed format for the stochastic finite volume method} \label{sec:Mixed}
We propose here a method to solve hyperbolic equations with a high number of uncertain parameters. 
Our method takes advantage of the low-rank approximation while keeping the structure of deterministic solvers. 
The idea is to use the tensor train format only for the stochastic space, and to keep the full format in space and in time. 
We choose to keep a full format in time because we do not expect the solution to have a low-rank approximation with respect to time. 
The solution of a nonlinear hyperbolic problem can indeed significantly differ from the initial condition as time passes due to the steepening of gradients and, thus, discontinuities may develop.
A comparison between low-rank approximation and full format in time was done in \cite{zhong_fast_2018} for linear and quasi-linear hyperbolic problems. 
The authors observed that the full format in time was more accurate than its low-rank approximation, and 
we can expect the effect to be even more noticeable for non-linear problems. 
Moreover, keeping the full format in space and in time allows us to adapt more easily well-known deterministic solvers.
Indeed, this way most steps are the same as a classical algorithm. The operations on scalars are simply replaced by operations on tensor trains, as we will describe in more detail later.  

The low-rank format is used in the stochastic directions, where the number of dimensions is the largest. 
There is no theoretical argument ensuring that a problem starting with a low-rank structure keeps a low-rank structure.
In previous works \cite{pares_higher-dimensional_2024,Kolb:2024}, it was even observed that shocks can appear in the stochastic direction, even if there is no flux in this direction. 
However, previous results \cite{walton_tensor-train_2024} give experimental evidence that low-rank formats can well approximate the solution.  

We now describe our method. 
For conciseness, the method is given for a scalar equation with one spatial dimension 
and with the same discretization in each stochastic dimension: 
$\Delta\xi_1=\Delta\xi_2=\dots=\Delta\xi_m=:\Delta\xi$, and 
$N_{\xi_1}=N_{\xi_2}=\dots=N_{\xi_m}=:N_\xi$. 
In this case, the discrete solution $\bar u$ has $N_t~N_x~N_\xi^m$ entries. 
We fix one time step $n$ and one spatial cell $i$, and consider the array of the numerical solution for all cells in the stochastic space. 
This $m$-dimensional array is approximated as a tensor train, denoted by $\bar U_i^n $. 
\begin{equation}
\bar U_i^n  \approx  \left( \bar u_{i,\ibfj}^n  \right)_{\ibfj \in \{1,\dots,N_\xi\}^m}.
\end{equation}
By doing this for all spatial cells, we get a one-dimensional array (a vector) of tensor trains, one for each spatial cell. 
Hence, at the time step $n$ we consider the vector of tensor trains $\bar{U}^n=(\bar{U}_1^n,\dots,\bar{U}_{N_x}^n)^\top$.
The same approximation is done for each reconstructed solution at the interface $i+1/2$. 
The resulting tensor train is denoted by $U_{i+1/2}^{n \pm}$.
\begin{equation}
U_{i+1/2}^{n \pm} \approx \left( u_{i+1/2, \ibfj}^{n \pm} \right)_{\ibfj \in \{1, \dots N_\xi\}^m},
\end{equation}
and we consider the vector of tensor trains 
$U^{n\pm}=(U_{3/2}^{n\pm},\dots,U_{N_x+1/2}^{n\pm})^\top$.
% With similar notations, let $H^n=(H^n_{1/2},\dots,H^n_{N_x+1/2})$ be
% the vector of tensor trains representing the numerical flux at the interfaces, 
% and by $\bar{H}^n=(\bar{H}^n_{1/2},\dots,\bar{H}^n_{N_x+1/2})$ its expectation over stochastic cells. 

% With similar notations, let $H^n=(H^n_{1/2},\dots,H^n_{N_x+1/2})$ be
% the vector of tensor trains representing the numerical flux at the interfaces, 
% and by $\bar{H}^n=(\bar{H}^n_{1/2},\dots,\bar{H}^n_{N_x+1/2})$ its expectation over stochastic cells. 

With similar notations, let  $\bar{H}^n=(\bar{H}^n_{1/2},\dots,\bar{H}^n_{N_x+1/2})$ be
the vector of tensor trains representing the numerical flux expectation at the interfaces. 

A practical example of our approach is now presented for the MUSCL scheme with forward Euler in time. 
Note that the idea is very general and can in principle be applied to any type of finite volume solver. 
After presenting the scheme, we describe how to compute the expectation and variance of the solution over the whole stochastic space.


\subsection{The MUSCL scheme in the mixed format}
We adapt the scheme from Section \ref{sec:SFV} to apply it to arrays of tensor trains.
Assume that, for a fixed cell $i$ and time step $n$, we know how to compute the tensor train approximations
$\bar U^n _i$ and  $U_{i+1/2}^{n +}$,  $U_{i+1/2}^{n -}$. 
Then the scheme reads:
\begin{align}
    \label{eq:FV-TT1}
    \bar U_i^{n +1} 
    & = \bar U_i^n  - \frac{\Delta t}{\Delta x} t (\bar H_{i+1/2}^n  - \bar H_{i-1/2}^n ),
    \\[5pt]
    H_{i+1/2}^n  
    & = \frac{ f\left( U_{i+1/2}^{n +} \right) + f\left( U_{i+1/2}^{n -} \right) }{2}
    - \frac{a_{i+1/2}}{2} \left(  U_{i+1/2}^{n +}  -  U_{i+1/2}^{n -}  \right),
    \\[5pt] 
    a_{i+1/2}^n
    & = \max \left(
        |f'(U_{i+1/2}^{n +})|, |f'(U_{i+1/2}^{n -})|
    \right), 
    \\
    U_{i+1/2}^{n +} 
    & = \bar U_{i+1}^n  - \frac{\Delta x}{2} (U_x)_{i+1}^n, 
    \\
    U_{i+1/2}^{n -} 
    & = \bar U_{i}^n  + \frac{\Delta x}{2} (U_x)_{i}^n, 
    \\
    (U_x)_{i}^n &= \minmod\left(
        \frac{\bar U_i^n - \bar U_{i-1}^n}{\Delta x},
        \frac{\bar U_{i+1}^n - \bar U_i^n}{\Delta x}
    \right) .
    \label{eq:FV-TT5}
\end{align}
Equations \eqref{eq:FV-TT1}-\eqref{eq:FV-TT5} are similar to a classical MUSCL scheme with no stochastic variables. 
The only difference is that the objects 
$\bar U_i^n , U_{i+1/2}^{n \pm}, (U_x)_i^n $ and $\bar H_{i+1/2}^n $ 
are scalars in the classical case, and tensor trains in our mixed formulation. 
Hence, all operations on these objects must be adapted to the tensor train format. 
As explained in Section \ref{sec:TT}, linear and polynomial functions can be applied directly to tensor trains, whereas the cross-interpolation algorithm must be used for non-polynomial functions. 
In our case, the operations $(T_1, T_2) \mapsto \minmod(T_1, T_2)$ and 
the function $(T_1, T_2) \mapsto \max(|T_1|, |T_2|)$ are non-polynomial in the variables $T_1, T_2$. 
%The function $(T_1, T_2) \mapsto \minmod(T_1, T_2)$, which will be used in the interpolation, is also non-polynomial.  
Depending on the problem at hand, this can also be the case for the initial condition $u_0$, the flux $f$ and its derivative $f'$. 

The first step of the algorithm is to compute the cell average of the initial condition in the mixed format $\bar U^0$ using \eqref{eq:quad_space}. 
%Then, the fixed time step $\Delta t$ is computed to satisfy the CFL condition 
For scalar equations, we make use of the maximum principle $|u(t,x,\bfxi)| \leq \max_{x,\bfxi}|u_0(x,\bfxi)|$ to use the fixed time step
\begin{equation} \label{eq:CFL}
    \Delta t < \frac{1}{2} \frac{\Delta x}{\max|f'(\bar U^0)|}. 
\end{equation}
%The maximum of a tensor can be computed in the tensor train format. 
%In the library we use \cite{usvyatsov_tntorch_2022}, the minimum of a tensor is computed by minimizing the objective function $x \mapsto \tan(\pi/2 - x)$, where the objective function is evaluated with cross approximation. 
The case for systems of equations will be described in Section \ref{sec:Euler}.
The solution is then propagated in time following the scheme \eqref{eq:FV-TT1}-\eqref{eq:FV-TT5}.
%An algorithm to evaluate the maximum of a tensor train was proposed in \cite{chertkov}, but it is not implemented in the library we use.     



The algorithms are summarized in Appendix \ref{sec:Algo}.




\subsection{Computing expectation and variance} \label{sec:Exp_var}
Once the solution in the mixed format $\bar U$ is known, we compute its expectation and variance. 
For simplicity, we describe the computation for two stochastic variables $\xi_1, \xi_2$. 
The expectation and variance of the solution $u$ at time $t$ and point $x$ are 
\begin{align} \label{eq:exp_var}
    \E[u(t,x)] &= \int_{\Omega_\bfxi} u(t,x,\xi_1,\xi_2) p_\bfxi (\bfxi) \diff \bfxi, 
    \\
    \var [u(t,x)] &= \E[(u(t,x)-\E[u(t,x)])^2] = \E[u(t,x)^2] - \E[u(t,x)]^2.  
\end{align}
We make the additional assumption that the random variables are independent and denote by $p_1, p_2$ the probability distributions associated to $\xi_1$ and $\xi_2$, respectively. 
The integral over the whole space $\Omega_\bfxi$ is approximated as a sum 
%\question{Could I discretize the stochastic intervals so that the points correspond to quadrature points? Not sure because it's average over a cell.}
\begin{equation}
    \E[u_i^n] \approx \sum_{j_1=1}^{N_\bfxi} \sum_{j_2=1}^{N_\bfxi} 
    \bar u_{i,j_1,j_2}^n p_1(\xi_{j_1}) p_2(\xi_{j_2}) (\Delta \xi)^2.
\end{equation}
This operation can be seen as a contraction of the tensor 
$\bar u_{i,j_1,j_2}^n$ with the tensor $p(\xi_{j_1})p(\xi_{j_2})$. 
Moreover, the tensor $p(\xi_{j_1})p(\xi_{j_2})$ is a tensor train with rank 1 and cores $p(\xi_{j_1})$ and $p(\xi_{j_2})$. 
With the mixed-format approximation $\bar U_i^n$, the tensor contraction can be computed in the tensor train format.
For the variance, we first compute the squared solution in the mixed format by operation on tensor trains, then we compute its expectation as above and finally obtain the variance from Equation \eqref{eq:exp_var}. 



\section{Numerical results} \label{sec:Numerics}
In this section, we apply the method to two hyperbolic problems:
%The objective of this paper is to give a proof of concept: we show that the method is able to compute a solution in cases where computations in the full format would be intractable. 
%For this reason, we consider only one spatial dimension.
%We solve two classical hyperbolic problems: 
the Riemann problem associated to Burgers' equation, and the Sod problem for the Euler equations. 
The first problem is used to investigate some properties of the method. 
We study two performance indicators: the convergence order in the expectation and variance, and the number of tensor entries. 
We also evaluate the influence of some parameters on the performance of the algorithm, namely the TT-rank and the tolerance for the cross approximation algorithm. 
Finally, we show that the method can scale up for a larger number of stochastic dimensions. 
The second problem showcases the method's applicability to a system of equations, and its ability to capture effects such as shock, rarefaction, and contact waves. 

The method was implemented in python with the tntorch library \cite{usvyatsov_tntorch_2022}. 

\subsection{Stochastic Burgers' equation}
We consider the Burgers' equation with uncertain initial conditions,
\begin{align}
    \frac{\partial u}{\partial t} + \frac{1}{2}\frac{\partial u^2}{\partial x} = 0, \label{eq:Burgers}
    \quad
    u_0(x,\bfxi) = \begin{cases}
         1 + \sum_{\ell=1}^m v_{L,\ell} \, \xi_\ell, & \quad x < 0
        \\
        - 1 + \sum_{\ell=1}^m v_{R,\ell} \, \xi_\ell, & \quad x > 0
    \end{cases}  %\label{eq:Burgers_IC}
    .
\end{align}
Let $\bfv_L=(v_{L,1},\dots,v_{L,m})^\top$ and $\bfv_R=(v_{R,1},\dots,v_{R,m})^\top$.
For now, the number $m$ of random variables is arbitrary. The value of $m$ and of the coefficients $\bfv_L, \bfv_R$ will be given in each subsection. 
%The random variables are assumed independent and uniformly distributed. 
We focus on the case where the solution is a shock wave, hence, we assume that for all realizations of $\bfxi=(\xi_1,\dots,\xi_m)$ it holds 
\begin{equation} 
    1 + \sum_{\ell=1}^m v_{L,\ell} \, \xi_\ell > - 1 + \sum_{\ell=1}^m v_{R,\ell} \, \xi_\ell. 
\end{equation}
%This is satisfied e.g for $v_{L,\ell}\geq0,v_{R,\ell}\leq0\,\FORALL\,\ell\in\{1,\dots~m\}$. 
Under this assumption, the exact solution to \eqref{eq:Burgers} is a shock
\begin{equation}
    u(t,x,\bfxi) = \begin{cases}
        u_L(\bfxi),  & \quad x < s(\bfxi)t
        \\
        u_R(\bfxi), & \quad x > s(\bfxi)t
    \end{cases}
    ,\quad s(\bfxi) = \frac{u_L(\bfxi)+u_R(\bfxi)}{2}.
\end{equation}
%
In the following, we focus on the case $m=3$ and study the algorithmic performance for an increasing number of (spatial and stochastic) cells. 
Then, we consider the scale-up possibility by increasing $m$. 
%  and study the performance of the algorithm when there is the same number of cell in each (spatial and stochastic) direction. 
% The convergence of the 
% We then consider the case of a low number of stochastic cells and 

\subsubsection{Convergence study and influence of algorithmic parameters}
The random variables are assumed independent and $\xi_\ell \sim \mathcal U(0,1)$, for all $\ell \in \{1,\dots,m\}$. 
We fix $m=3$, $(v_{L,1}, v_{L,2}, v_{L,3}) = (0.1, 0, -0.1)$, $(v_{R,1}, v_{R,2}, v_{R,3}) = (0.1, -0.1, 0)$. 
The initial condition then reads
\begin{equation}
    u_0(x,\bfxi) = \begin{cases}
        1 + 0.1 \xi_1 - 0.1 \xi_3,  & \quad x < 0
        \\
        - 1 + 0.1 \xi_1 - 0.1 \xi_2, & \quad x > 0
    \end{cases}
    .
\end{equation}
We investigate the influence of some parameters on the  performance of the algorithm. 
Various performance indicators can be used, here we focus on three values: the $L^1$-error between the approximated and exact solution expectation, 
the $L^1$-error between the approximated and exact solution variance, and the compression ratio for the solution at the final time.
The compression ratio is defined as the number of entries for the mixed format divided by the number of entries for the full tensor.  
The relative $L^1$-error in expectation at final time $t^n$ is defined by 
\begin{align}
    |\mathbb E|_{L^1, \text{rel}}
    = \frac{|\mathbb E_\text{TT} - \mathbb E_\text{ex}|_{L^1}}{|\mathbb E_\text{ex}|_{L^1}}, 
    \quad
    |\mathbb E|_{L^1} 
    = \frac{1}{N_x} \sum_{i=1}^{N_x} \mathbb E[\bar u_i^n].
\end{align}
Here the expectations for the exact and approximated solution are computed with
\begin{equation}
    |\mathbb E_{TT} - \mathbb E_{ex}|_{L^1}
    = \frac{1}{N_x} \sum_{i=1}^{N_x} \left( 
        \mathbb E[U_i^n] 
        - \int_{x_{i-1/2}}^{x_{i+1/2}} \mathbb E[u(x,t)]
    \right).
\end{equation}
For the integration of the exact expectation along $x$, we use the midpoint rule. 
%\question{Is it enough?}
The expectation for the tensor train $\mathbb E[U_i^n]$ is computed as described in Section \ref{sec:Exp_var}.  
The expectation and the variance of the exact solution are given in Appendix \ref{sec:Exact}. 
Since their expression requires nontrivial integration, the integrals are numerically approximated using the scipy.integrate package \cite{virtanen_scipy_2020}. 

The investigated parameters are the number of stochastic and spatial cells, the TT-rank, and the tolerance for the cross approximation function.
We give here some explanation for each parameter. 
As before, the number of cells in the spatial direction is denoted by $N_x$. 
Each stochastic direction has the same discretization, and we denote by $N_\xi$ the number of cells for each stochastic direction.  
For example, in the case $N_x = N_\xi = 100$, the spatial-stochastic grid is made of $100^4$ cells. 
The TT-rank $r$ corresponds to the maximal rank allowed for each rounding operation. %We compute the numerical approximation for two cases: $r=1$ and $r=5$. 
The tolerance $\varepsilon$ is a parameter related to the cross approximation algorithm. 
The iterative algorithm terminates once the residual on the selected entries is less than $\varepsilon$. 
It should be noted that this tolerance does not give a global error on the tensor approximation, so its influence on the total error of the algorithm is not known.
%We test the values $\varepsilon = 0.1$ and $\varepsilon = 0.01$. 

Figure \ref{fig:Exp1} shows the case $N_x=N_\xi\in\{40,\dots,100\}$, and Figure \ref{fig:Exp2} shows the case $N_\xi = 20$ and $N_x \in \{40, \dots 100\}$. 
In addition to the results shown in these figures, we also tried other values for the TT-rank and the tolerance. 
For the TT-rank, we tried with $r=10, r=100$ and setting no maximal ranks, but this led to an excess of memory for high $N_x$. 
For the tolerance, we tried with $\varepsilon = 10^{-6}$, as it is the default value in the library tntorch. 
Then the algorithm became much slower, and the cross approximation failed to converge in many cases. 
As we see in the results, it is not clear whether a smaller tolerance improves the  accuracy of the solution. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Exp1.pdf}
    \caption{$L^1$-error of the expectation and variance, and number of parameters for a shock wave for $N_x=N_\xi$.
    The curves for the same $r$ and different $\varepsilon$ are superposed.}
    \label{fig:Exp1}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=\textwidth]{Exp2.pdf}
\caption{$L^1$-error of the expectation and variance, and a number of parameters for a shock wave for $N_\xi=20$. The curves for the same $r$ and different $\varepsilon$ are superposed.}
\label{fig:Exp2}
\end{figure}

From Figures \ref{fig:Exp1} and \ref{fig:Exp2} we make the following observations:
\begin{itemize}
    \item The $L^1$-errors are higher for odd $N_x$ than for even $N_x$. 
    This might be caused by the location of shock relative  to the cell boundary. It should also be noted that for small $N_x$, i.e.~$N_x<60$, similar oscillations can be seen in the exact solution.
    \item The convergence order is approximately 1. 
    For a non-stochastic problem, a MUSCL scheme with a midpoint integration and a central Euler is expected to converge of second order.
    However, to our knowledge, no convergence order has been proved for the solution expectation of a stochastic problem. 
    It would also be interesting to investigate how tensor approximation alters the convergence order.  
    \item Changing the tolerance $\varepsilon$ does not seem to have a noticeable effect.
    \item A larger TT-rank $r$ yields a lower $L^1$-error for the variance. The $L^1$-error for the expectations is not strictly smaller, but its oscillation in $N_x$ is smaller.  
    \item The compression ratio is globally very small, even for $r=5$. %It also decreases for larger $N_\xi$ \question{is it because NTT decrease a lot, of because Nfull increase a lot?}.
    \item For a fixed $N_x$, the version with higher $N_\xi$ is slightly more accurate. 
    Increasing $N_\xi$ does not seem to have an impact on the convergence order in $x$.
    We can assume that here, the loss of convergence order in $x$ is not related to the discretization in $N_\xi$. 
\end{itemize}





\subsubsection{Scaling study}
The random variables are assumed independent and $\xi_\ell \sim \mathcal U(0,1)$, for all $\ell \in \{1,\dots,m\}$. 
We fix $N_x=N_\xi=20, r=5, \varepsilon=0.1$ and vary the number of stochastic dimensions $m$.
Here the coefficients $\bfv_R, \bfv_L$ are $v_{R,\ell}=0.1,~v_{L,\ell}=-0.1$ for $\ell\in\{1,\dots,m\}$. 
This ensures that the solution is a shock wave, no matter the number of stochastic dimensions. 
Thus, the expectation will exhibit a discontinuity at around $x=0$, see Figure \ref{fig:Exp3_E}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Exp3.pdf}
    \caption{Number of entries for the solution at the final time, compared to the number of entries for the full tensor.}
    \label{fig:Exp3}
\end{figure}
Figure \ref{fig:Exp3} shows the number of entries for the solution at the final time and compare it with the number of entries for the full tensor. 
The number of entries for the mixed format is polynomial with respect to the number of stochastic dimensions, whereas it is exponential for the full format. 
When running the experiment, we noticed that the cross approximation algorithm did not always converge. 
The cross approximation failed to converge more frequently with increasing dimensions. 
To check how much the convergence of the cross approximation alters the solution, we show in Figure \ref{fig:Exp3_E} the expectation of the solution. 
The numerical solution seems not to be  affected by the failed convergence: a shock is still visible at the expected location.
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Exp3_E.pdf}
    \caption{Expectation of the solution as a function of $x$, for various number of stochastic dimensions $m$.}
    \label{fig:Exp3_E}
\end{figure}





\subsection{Euler equations}\label{sec:Euler}
The last experiment is the Sod problem for the Euler equations. 
We use this example to show that the algorithm can be extended to systems, and that the approximated solution can capture features such as  shocks and contact waves typical for nonlinear hyperbolic problems.
We also investigate the influence of the mesh size and the rank on the solution. 

\subsubsection{Problem description}
We consider the Euler equations in one spatial dimension,
\begin{equation}
    \frac{\partial}{\partial t}
    \begin{pmatrix}
        \rho \\ \rho u \\ \rho E
    \end{pmatrix}
    +
    \frac{\partial}{\partial x}
    \begin{pmatrix}
        \rho u \\ \rho u^2 + p \\ (\rho E+p)u
    \end{pmatrix}
    = 0.
\end{equation}
Here $\rho$ is the density, $u$ the velocity, $E$ the total energy, $p$ the pressure. 
For an ideal fluid with a specific heat ration $\gamma=1.4$, the pressure is given by 
\begin{equation}
    p = (\gamma-1) (\rho E-\frac{\rho u^2}{2}).
\end{equation}
Note that compared to the Burgers' equation, the flux of the Euler equations consists in non-polynomial functions. Hence, cross approximation is required for the flux evaluation. 
We consider the Sod shock tube problem for $x \in [0,1]$ with free boundary conditions and with $m=3$ stochastic directions, 
\begin{equation}
    \begin{pmatrix}
        \rho(0,x,\bfxi), 
        \\
        u (0,x,\bfxi), 
        \\
        p(0,x,\bfxi)
    \end{pmatrix}
    = 
    \begin{cases}
        \begin{pmatrix}
            1 + 0.1 \, \xi_1 + 0.1 \, \xi_2 + 0.05 \, \xi_3
            \\ 
            \quad -0.01 \, \xi_1 + 0.05 \, \xi_2 + 0.01 \, \xi_3
            \\ 
            1 + 0.1 \, \xi_1 - 0.01 \, \xi_2 + 0.01 \, \xi_3
        \end{pmatrix} , & 0<x<0.5
        \\[20pt]
        \begin{pmatrix}
            0.125 + 0.05 \, \xi_1 - 0.05 \, \xi_2 + 0.01 \, \xi_3
            \\ 
            0.05 \, \xi_1 - 0.01 \, \xi_2
            \\ 
            0.1 + 0.01 \, \xi_1 + 0.05 \, \xi_2 - 0.01 \, \xi_3
        \end{pmatrix} , & 0.5<x<1
    \end{cases}
    .
\end{equation}
The random variables are assumed independent and $\xi_\ell \sim \mathcal U(0,1)$, for all $\ell \in \{1,\dots,m\}$. 
Here, the time step is updated at each iteration. 
For a hyperbolic system with eigenvalues $\lambda_1,\dots,\lambda_p$, the CFL condition has the form 
\begin{equation} 
    \frac{\Delta t}{\Delta x}
    \max_p |\lambda_p| < \alpha,
\end{equation}
and we use the CFL number $\alpha=0.4$. The eigenvalues of the Euler equations are 
\begin{equation}
    \lambda_1 = u-c, \ \lambda_2 = u, \ \lambda_3 = u+c,
    \quad c = \sqrt{\frac{\gamma p}{\rho}}. 
\end{equation}
%The solution at final time $t=0.2$ is computed with $N_x=30,N_\xi=10$, $r=10$, and $\varepsilon=0.1$. The expectation and standard deviation for the density, pressure and velocity are shown in Figure \ref{fig:Euler}. 
% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\textwidth]{Euler.png}
%     \caption{Expectation and variance to the Sod problem.}
%     \label{fig:Euler}
% \end{figure}

\subsubsection{Parameter study}

The Sod problem is used to investigate the influence of some parameters on the solution. 
We focus here on the mesh size in the spatial and stochastic directions, and on the maximal rank. 
The tolerance for cross approximation, that was varied in the Burgers case, is  fixed here at $\varepsilon=0.1$. 
Since there is no analytical solution known for the Sod problem, we focus here on the qualitative results.
We plot the expectation and variance of the primitive variables (density, velocity and pressure) at final time $t=0.2$.
For comparison, the expectation and variance are also computed using a Monte-Carlo method with the library MultiWave 
\cite{GerhardIaconoMayMuellerSchaefer:2015}: an adaptive discontinuous Galerkin solver was used with $L=6$ refinement levels corresponding 
to a uniformly refined grid with 192 cells and polynomial elements of degree 2, i.e., third-order scheme; the expectation and variance were computed with 125,000 samples.
Details can be found in \cite{herty_multiresolution_2024,pares_higher-dimensional_2024,Kolb:2024} where similar Monte Carlo Simulations were performed.

Figures \ref{fig:Euler_Nx_E} and \ref{fig:Euler_Nx_var} respectively show the expectation and variance of the primitive variable
for $r=5,N_\xi=20$ and for various mesh refinement in the physical space $N_x$. 
For the expectation, there are little differences between $N_x=80$ and $N_x=160$, so the scheme seems to converge. 
Moreover, the general shape is in globally in good agreement with the MC results. 
However, in transition between the contact wave and the shock wave at $x \in [0.65,0.85]$, there are non-physical oscillations, which will be further investigated in the next paragraph. 
We also note that the results with MC are sharper, which is probably due to the fact that a third-order DG solver was used.
For the variance, in some areas convergence is slower and a local refinement could be beneficial. 
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Euler_Nx_E.pdf}
    \caption{Expectation of $\rho, u,p$ for $r=5, N_\xi=20$ and $N_x\in \{20,40,80,160\}$. 
    The expectation obtained with a Monte-Carlo ('MC') is in dotted line.}
    \label{fig:Euler_Nx_E}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Euler_Nx_var.pdf}
    \caption{Variance of $\rho, u, p$ for $r=5, N_\xi=20$ and $N_x\in \{20,40,80,160\}$. 
    The variance obtained with a Monte-Carlo ('MC') is in dotted line.}
    \label{fig:Euler_Nx_var}
\end{figure}


Figure \ref{fig:Euler_r} shows the expectation of the primitive variables for $N_x=160, N_\xi=20$ and for various values of the rank $r$.
All the tested ranks yield mostly the same result, which supports the idea that the solution can be represented in most cells with low-rank tensors. 
Only the zone $x \in [0.65, 0.85]$, between the contact wave and the shock, shows differences for various ranks, see Figure \ref{fig:Euler_r_zoom}. Increasing the rank is here necessary to reduce the oscillations. 
Here, we see that increasing locally the rank would improve the solution while keeping computational cost low. 
We also tried ranks lower than 5, but this yielded non-physical solutions. 
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Euler_r.pdf}
    \caption{Expectation of $\rho, u, p$ for $N_x=160, N_\xi=20$, and $r\in \{5,15\}$ }
    \label{fig:Euler_r}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Euler_r_zoom.pdf}
    \caption{Zoom on expectation of $u$ for $N_x=160, N_\xi=20$, and $r\in \{5,15\}$}
    \label{fig:Euler_r_zoom}
\end{figure}

Figure \ref{fig:Euler_Nxi} and \ref{fig:Euler_Nxi_var} respectively show the expectation and variance of the primitive variable
for $N_x=160,r=5$ and for various mesh refinement in the stochastic space $N_\xi$.
Here, the number of stochastic cells does not seem to have a significant influence, which is probably due to the choice of a uniform probability distributions for each random variable. 
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Euler_Nxi_E.pdf}
    \caption{Expectation of $\rho, u, p$ for $N_x=160, r=5$ and $N_\xi\in \{20,40,80\}$.}
    \label{fig:Euler_Nxi}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Euler_Nxi_var.pdf}
    \caption{Variance of $\rho, u, p$ for $N_x=160, r=5$ and $N_\xi \in \{20,40,80\}$.}
    \label{fig:Euler_Nxi_var}
\end{figure}


In summary, we showed that for the Sod problem, a low rank representation is sufficient in most places, but higher rank are necessary in some local parts. 
This example illustrates how we can use the mixed format to gain more precise information on the problem at hand and tune the parameters to improve the solution. 
For simplicity, we chose here to keep the same rank for all spatial cells, but the algorithm could easily be changed to have a higher rank only in some cells. 

% Comments on the parameter study:
% \begin{itemize}
%     \item There seem to be problems between the contact wave and the shock. From Burgers' test case: the shock seem resolved, so the problem is maybe in the contact. 
%     \item The rest seems to be well resolved.
%     \item Tests with one variable show that there is no oscillation, so the oscillations seem to come from the TT approximation. 
%     \item Increasing the rank reduces the oscillations. This motivates further the idea of having a varying rank for each spatial cell. 
% \end{itemize}


\section{Conclusion}
In this paper, we have introduced a new method to use the stochastic finite volume method with many stochastic parameters.
The numerical experiments show the feasibility of the approach. 
They also underline the importance of correctly choosing the TT-rank for the tensor approximation: there is a trade-off between the accuracy and the need to keep computational cost tractable. 

This work opens new avenues for combining the tensor train format with an accurate numerical method for hyperbolic equations. 
Several aspects of the proposed method could be further investigated:
\begin{itemize}
	\item The choice of correct algorithm parameters -- such as the TT-rank or the tolerance for the cross approximation -- is very likely to be problem-dependent. 
    Hence, the algorithm should be tested on other representative problems and for various probability densities. 
	
	\item The convergence order of deterministic methods is altered by the introduction of random parameters. 
    This aspect should be further investigated to provide convergence estimates for this recent class of algorithms. 
	
    \item The method proposed in this paper keeps each spatial cell separate, which should allow some analysis. 
    In particular, it would be interesting to look for correlations between the features in the physical space and the TT-ranks in stochastic space. 
    From the Sod example, we assume that imposing higher ranks only on few cells should improve the result while keeping computational cost low. Further investigations are required to validate this hypothesis.
\end{itemize}
A comparison with the fully compressed approach \cite{walton_tensor-train_2024} would also be of interest. 
One could for example compare the number of parameters needed by both approaches to obtain the same accuracy, when using the same finite volume method and the same propagation in time.

% Perspectives
The proposed algorithm is a proof of concept and can easily be combined with various high-performance computing techniques. 
A mesh adaptation in the physical space could take into account the TT-rank. 
For example, the method introduced in \cite{herty_multiresolution_2024,HertyKolbMueller:2024,Kolb:2024} for mesh adaptation in physical and stochastic space could be adapted to our setting. 
Moreover, the algorithm is easily parallelizable, since the numerical flux is computed independently on each spatial cell. 


\section*{Aknowledgements}
The authors thank Adrian Kolb for the Monte-Carlo results.
The work has been supported by the  DFG (German Research Foundation) through 320021702/GRK2326: Energy, Entropy, and Dissipative Dynamics (EDDy) and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - SPP 2410 Hyperbolic Balance Laws in Fluid Mechanics: Complexity, Scales, Randomness (CoScaRa) within the Projects 525842915, 52584291 (Numerische Verfahren für gekoppelte Mehrskalenprobleme) and 
 525853336 (Zufällige kompressible Euler Gleichungen: Numerik und ihre Analysis). The authors thank the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) for the financial support through 333849990/GRK2379
(IRTG Hierarchical and Hybrid Approaches in Modern Inverse Problems).
Also, support received funding from the European Union’s Horizon Europe research and innovation program under the Marie Sklodowska-Curie Doctoral Network Datahyking (Grant No. 101072546).



%\printbibliography
\bibliography{main}



\appendix
\section{Algorithms.} \label{sec:Algo}
We describe the method in pseudocode. 
The general algorithm is presented in Algorithm \ref{algo:euler} and the flux computation is presented in Algorithm \ref{algo:muscl}.
Every time cross interpolation is used, it is made visible in the pseudocode with the syntax '$\Call{cross}{f(T1, T2, \dots)}$'. 
Here, 'f' is the non-polynomial function and 'T1, T2, ...' are its arguments in the tensor train format.

\begin{remark}
    In Algorithm \ref{algo:euler}, the maximum of $|f'(\bar U^0)|$ can be computed directly in the tensor train format. 
    In tntorch -- the tensor train library used for this work -- the minimum of a tensor train is computed by minimizing the objective function $x \mapsto \tan(\pi/2 - x)$, where the objective function is evaluated with cross approximation. 
\end{remark}

\begin{algorithm}
\caption{Algorithm for a forward Euler scheme in time} \label{algo:euler}
\begin{algorithmic}
    % \State $U^{0\pm} \gets \Call{\minmod}{u_0(X,\xi)}$  
    % \Comment{Compute the initial condition at interfaces and at quadrature point, 
    % in the mixed format $U^0$, see Algorithm \ref{algo:cell_avg}}
    \ForAll{$i \in \{1, \dots, N_x N_q\}$}
        \State $U^0_{i_q} \gets \Call{cross}{u_0(x_{i_q},\xi_1,\dots,\xi_m)}$
        \Comment{Evaluate $u_0$ on the quadrature grid with cross approximation}
    \EndFor
    \State $\bar U^0 \gets \Call{Average}{U^0}$
    \Comment{Compute the cell average of the initial condition $\bar U^0$} %with Algorithm \ref{algo:cell_expect}}
    \State Compute $\Delta t < \frac{1}{2} \dfrac{\Delta x}{\max |f'(\bar U^0)|}$. 
    \ForAll{$k \in 0, \dots, N_t$} 
        \ForAll{$i \in \{1, \dots, N_x\}$}
            \State $U^{n  \pm}_{i \pm 1/2} \gets \Call{Interpolate}{\bar U_i^n } $ 
            \Comment{Interpolate values at interfaces, see Algorithm \ref{algo:interpolate} }
            %
            \State $H_{i\pm 1/2} \gets \Call{NumFlux}{U^\pm_{i\pm1/2}}$ 
            \Comment{Compute the numerical flux at interfaces, see Algorithm \ref{algo:muscl}}
            %
            %\State $\bar H_{i\pm1/2} \gets \Call{Expectation}{H_{i\pm 1/2}}$ 
            %\Comment{compute the flux expectation over stochastic cells, see Algorithm \ref{algo:cell_expect}}
            %
            \State $\bar U_i^{n +1} \gets \bar U_i^n  - \frac{\Delta t }{\Delta x}(H_{i+1/2}-H_{i-1/2})$
            \Comment{Update the solution}
        \EndFor
    \EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Numerical flux computation for a MUSCL-type scheme} \label{algo:muscl}
\begin{algorithmic}
    %\ForAll{$i \in \{1, \dots N_x\}$}
	    % \State $u_{i-1/2}^-, \ u_{i+1/2}^- \gets u^-_{i-1}, \ u^-_i$ 
	    % \State $u_{i-1/2}^+ , \ u_{i+1/2}^+ \gets u^+_{i}, \ u^+_{i+1}$  
        % \State
	    \State $a^+ \gets \Call{cross}{\max(|f'(U_{i+1/2}^+)|, |f'(U_{i+1/2}^-)|)}$
        \State
	    % \State $a^- \gets \Call{cross}{\max(|f'(U_{i-1/2}^+)|, |f'(U_{i-1/2}^-)|)}$
	    \State $H_{i+1/2} \gets \dfrac{f(U_{i+1/2}^+) + f(U_{i+1/2}^-)}{2 \Delta x}
            - \dfrac{a^+ (U_{i+1/2}^+ - U_{i+1/2}^-)}{2 \Delta x}$
        \State \Return $H_{i+1/2}$
    %\EndFor 
    %\State \Return $H$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Interpolation} \label{algo:interpolate} 
\begin{algorithmic}
    \ForAll {$i \in \{1, \dots, N_x\}$}
	\State $(U_x)_i^n  \gets \Call{cross}{\minmod (U_i - U_{i-1} , U_{i+1} - U_i)}$
    \State $U^{n +}_{i+1/2} \gets \bar U_{i+1}^n - \frac{1}{2} (U_x)_{i+1}^n$  
    \State $U^{n -}_{i+1/2} \gets \bar U_i^n + \frac{1}{2} (U_x)_i^n$  
	\EndFor
\State \Return $U^{n +}_i, U^{n -}_i$
\end{algorithmic}
\end{algorithm}


\section{Expectation and variance for the exact solution} \label{sec:Exact}
We give here the expression for the expectation and variance of the exact solution to the Burgers' equation, for the shock case and for $m \geq 2$.  

%%%%%%%%%%%%%%% Case m=2
% Let ${\bf v}_L=(v_{L,1},\dots,v_{L,m})$, ${\bf v}_R=(v_{R,1},\dots,v_{R,m})$ and ${\bf v}=({\bf v}_L+{\bf v}_R)/2$ 
% \paragraph{Expectation.} The expectation reads
% \begin{align}
%     \E[u] & = \int_0^1 \int_0^1 \int_0^1 u(t,x,\xi_1,\xi_2,\xi_3) p_1(\xi_1) p_2(\xi_2) p_3(\xi_3)  \diff \xi_1 \diff \xi_2 \diff \xi_3
%     \\
%     & = \int_0^1 \int_0^1 \int_0^1 \left( 
%           u_L(\xi_1,\xi_2,\xi_3) 1_{x < s t}
%         + u_R(\xi_1,\xi_2,\xi_3) 1_{x > s t}
%     \right) p_1(\xi_1) p_2(\xi_2) p_3(\xi_3)  \diff \xi_1 \diff \xi_2 \diff \xi_3
% \end{align}
% It holds, for $\xi_3 \in [0,1]$ and assuming $v_3 \neq 0$
% \begin{align}
%     x < s(\xi_1,\xi_2,\xi_3) t 
%     & \Leftrightarrow 
%     \xi_3 > \frac{1}{v_3} \left(\frac{x}{t} - v_1 \xi_1 - v_2 \xi_2 \right), \xi_3 \in [0,1]
% \end{align}
% Let $c(\xi_1,\xi_2,x,t) = (x/t - v_1 \xi_1 - v_2 \xi_2)/v_3$ and define 
% \begin{equation}
%     a = \begin{cases}
%         1,      & c(\xi_1,\xi_2,x,t) > 1,
%         \\
%         c(\xi_1,\xi_2,x,t),  & 0 < c(\xi_1,\xi_2,x,t) < 1,
%         \\
%         0,  &  c(\xi_1,\xi_2,x,t) < 0
%     \end{cases}
%     = \min\left(1, \max\left(0, c(\xi_1,\xi_2,x,t)\right)\right).
% \end{equation}
% The expectation is 
% \begin{multline}
%     \E[u] = \int_0^1 \int_0^1 \int_{\xi_3 = 0}^{a(\xi_1, \xi_2)} 
%           u_R(\xi_1,\xi_2,\xi_3) p_1(\xi_1) p_2(\xi_2) p_3(\xi_3)  
%     \diff \xi_1 \diff \xi_2 \diff \xi_3
%     %%% 
%     \\ 
%     + \int_0^1 \int_0^1 \int_{\xi_3 = a(\xi_1,\xi_2)}^{1} 
%         u_L(\xi_1,\xi_2,\xi_3) p_1(\xi_1) p_2(\xi_2) p_3(\xi_3)
%     \diff \xi_1 \diff \xi_2 \diff \xi_3
% \end{multline}
% For a uniform probability density over $[0,1]$, that is $p_1 = p_2 = p_3 = 1$ and using the expression of ${\bf v}_L, {\bf v}_R$,
% \begin{multline}
%     \E[u]
%     = \int_0^1 \int_0^1 
%         (-1+v_{R,1} \xi_1 + v_{R,2} \xi_2) a + v_{R,3} \frac{a^2}{2}
%     \notag
%         + (1 + v_{L,1} \xi_1 + v_{L,2} \xi_2) + \frac{v_{L,3}}{2} 
%     \\ 
%         - (1 + v_{L,1} \xi_1 + v_{L,2} \xi_2) a - v_{L,3} \frac{a^2}{2} 
%     \diff \xi_1 \diff \xi_2
% \end{multline}
%
% \paragraph{Variance.}
% For the variance we start by computing
% \begin{align}
%     \E[u^2] 
%     & = \int_0^1 \int_0^1 \int_0^1 
%     u^2(t,x,\xi_1,\xi_2,\xi_3) p_1(\xi_1) p_2(\xi_2) p_3(\xi_3) 
%     \diff \xi_1 \diff \xi_2 \diff \xi_3
%     \\
%     & = \int_0^1 \int_0^1 \int_0^1 \left( 
%           u_L^2(\xi_1,\xi_2,\xi_3) 1_{x < s t}
%         + u_R^2(\xi_1,\xi_2,\xi_3) 1_{x > s t}
%     \right) 
%     p_1(\xi_1) p_2(\xi_2) p_3(\xi_3)  \diff \xi_1 \diff \xi_2 \diff \xi_3
% \end{align}
% Again for uniform probability densities over $[0,1]$ we get
% \begin{align}
%     \E[u^2] \notag
%     & = \int_0^1 \int_0^1 
%     \left( 
%       (-1 + v_{R,1} \xi_1 + v_{R,2} \xi_2)^2 a
%     + (-1 + v_{R,1} \xi_1 + v_{R,2} \xi_2) v_{R,3} a^2
%     + \frac{v_{R,3}^2}{3} a^3
%     \right)
%     \\ & \notag
%     -
%     \left( 
%       (1 + v_{L,1} \xi_1 + v_{L,2} \xi_2)^2 a
%     + (1 + v_{L,1} \xi_1 + v_{L,2} \xi_2) v_{L,3} a^2
%     + \frac{v_{L,3}^2}{3} a^3
%     \right) 
%     \\ & 
%     +
%     \left( 
%       (1 + v_{L,1} \xi_1 + v_{L,2} \xi_2)^2 
%     + (1 + v_{L,1} \xi_1 + v_{L,2} \xi_2) v_{L,3} 
%     + \frac{v_{L,3}^2}{3} 
%     \right)   
%     \diff \xi_1 \diff \xi_2 
% \end{align}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%For $m \geq 2$, with a uniform probability density and letting 
We assume that $v_m=(v_{L,m}+v_{R,m})/2\neq0$ and define 
\begin{align}
    \hat\bfxi=(\xi_1,\dots,\xi_{m-1}), 
    \quad
    \hat{\bf v}_L=(v_{L,1},\dots,v_{L,m-1}),
    \quad
    \hat{\bf v}_R=(v_{R,1},\dots,v_{R,m-1}),
    \quad
    \hat{\bf v}=(\hat{\bf v}_L+\hat{\bf v}_R)/2.
\end{align}
Let $c(t,x,\hat\bfxi)=(x/t-\hat{\bf{v}}\cdot\hat\bfxi)/v_m$ and
\begin{equation}
    a(t,x,\hat \bfxi) = \begin{cases}
        1,      & c(\hat \bfxi,x,t) > 1,
        \\
        c(t,x,\hat \bfxi),  & 0 < c(t,x,\hat \bfxi) < 1,
        \\
        0,  &  c(t,x,\hat \bfxi) < 0
    \end{cases}
    = \min\left(1, \max\left(0, c(t,x,\hat \bfxi)\right)\right).
\end{equation}


The expectation is 
\begin{multline}
    \E[u] = \int_{[0,1]^{m-1}} \int_{\xi_m = 0}^{a(\hat \bfxi)} 
          u_R(\bfxi) p_1(\xi_1) \dots p_m(\xi_m)  
    \diff \xi_m \diff \hat \bfxi 
    %%%
    +  \int_{[0,1]^{m-1}} \int_{\xi_m = a(\hat \bfxi)}^{1} 
        u_L(\bfxi) p_1(\xi_1) \dots p_m(\xi_m)       
    \diff \xi_m \diff \hat \bfxi .
\end{multline}
For a uniform probability density over $[0,1]$, that is $p_1 = \dots = p_m = 1$ and using the expression of $u_L,u_R$ and ${\bf v}_L, {\bf v}_R$,
\begin{equation}
    \E[u]
    = \int_{[0,1]^{m-1}}
          (-1+ \hat {\bf v}_R \cdot \hat \bfxi) a + v_{R,m} \frac{a^2}{2}
    \notag
        + (1 + \hat {\bf v}_L \cdot \hat \bfxi) + \frac{v_{L,m}}{2} 
        - (1 + \hat {\bf v}_L \cdot \hat \bfxi) a - v_{L,m} \frac{a^2}{2} 
    \diff \hat \bfxi .
\end{equation}
For the variance we start by computing
\begin{multline}
    \E[u^2] 
    = \int_{[0,1]^{m-1}}
    \left( 
      (-1 + \hat {\bf v}_R \cdot \hat \bfxi)^2 a
    + (-1 + \hat {\bf v}_R \cdot \hat \bfxi) v_{R,m} a^2
    + \frac{v_{R,m}^2}{3} a^3
    \right)
    \\ 
    -
    \left( 
      (1 + \hat {\bf v}_L \cdot \hat \bfxi)^2 a
    + (1 + \hat {\bf v}_L \cdot \hat \bfxi) v_{L,m} a^2
    + \frac{v_{L,m}^2}{3} a^3
    \right) 
    \\ 
    +
    \left( 
      (1 + \hat {\bf v}_L \cdot \hat \bfxi)^2 
    + (1 + \hat {\bf v}_L \cdot \hat \bfxi) v_{L,m} 
    + \frac{v_{L,m}^2}{3} 
    \right)   
    \diff \hat \bfxi.
\end{multline}
The variance is then obtained with $\var [u(t,x)] = \E[u(t,x)^2]-\E[u(t,x)]^2$. 


\end{document}





% \paragraph{Cell-averaging in the mixed format.}
% The averaged value $\bar U_{i}^{n }$ in the mixed format is obtained by integration over the cell $(i,j)$. 
% We recall that the quadrature in full format is given by Equation \eqref{eq:quad_space}. 
% Averaging amounts then to summing the tensor trains. After summing, the resulting tensor train is rounded to avoir large ranks. 
% \question{If I use the midpoint rule also in space, I don't need this part}

% In the rest of this section, we explain how the cell average $\bar U^n _i$, 
% the interpolation $U_{i+1/2}^{n \pm}$ and the expectation $\bar H_{i+1/2}^n $ are computed.  
%
% \paragraph{Cell-averaging in the mixed format.}
% The averaged value $\bar U_{i}^{n }$ in the mixed format is obtained by integration over the cell $(i,j)$. 
% We recall that the quadrature in full format is given by Equation \eqref{eq:average_quad}.
% The stochastic directions are coupled in the term 
% $u(t^n , x_{i_m}, \xiQuad{1}, \xiQuad{2})$, and computing this term in the full format would require $\mathcal O(N_q^3)$ operations. 
% Instead, we take advantage of the mixed format. 
% We define the array $u_{i_m,j_{q_1},j_{q_2}}^n  = u(t^n , x_{i_m}, \xiQuad{1}, \xiQuad{2})$. 
% For a fixed index $i_m$ and timesetp $t^n $, this array is approximated by a tensor train with rank $r$:
% \begin{equation} \label{eq:TT}
%     u_{i_m,j_{q_1},j_{q_2}}^n  
%     \approx \sum_{\alpha=1}^{r}
%     G_{i_m,1}^n (1, j_{q_1}, \alpha) \,  
%     G_{i_m,2}^n (\alpha, j_{q_2}, 1).    
% \end{equation}
% The array $u_{i_m,j_{q_1},j_{q_2}}^n $ is replaced in Equation \eqref{eq:average_quad} by its tensor train approximation \eqref{eq:TT},
% \question{change accordingly for a generic quadrature rule}
% \begin{multline}
%     \frac{1}{2 K_{j_1} K_{j_2}}
%     \sum_{m = 1}^{N_q}
%     \sum_{q_1, q_2 = 1}^{N_q}
%     u_{i_m,j_{q_1},j_{q_2}}^n  \, p(\xiQuad{1}, \xiQuad{2}) \, w_{n_1} \, w_{n_2} \ w_m 
%     \\
%     \approx
%     \frac{1}{2 K_{j_1} K_{j_2}}
%     \sum_{m = 1}^{N_q}
%     \sum_{q_1, q_2 = 1}^{N_q}
%     \left( \sum_{\alpha=1}^r
%     G_{i_m,1}^n (1, j_{q_1}, \alpha) \,  G_{i_m,2}^n (\alpha, j_{q_2}, 1)
%     \right) 
%     p_1(\xiQuad{1}) p_2(\xiQuad{2})
%     \, w_{q_1} w_{q_2} w_m.
% \end{multline}
% %\question{Problem with this formulation: the volumes $|K_{\xi_1}^{j_1}|$ can be very small, probable numerical errors. }
% The average over the cell $(j_1, j_2)$ for a fixed $i_m$ has hence a tensor train representation with cores
% \begin{align} %\label{eq:integration_core}
%     \bar G_{i_m,1}^n (1, j_1, \alpha)
%     = \frac{1}{K_{j_1}} \sum_{q_1 = 1}^{N_q}
%     G_{i_m,1}^n (1, j_{q_1}, \alpha) p_1(\xiQuad{1}) \, w_{q_1},
%     \\ %\label{eq:integration_core2}
%     \bar G_{i_m,2}(\alpha,j_2,1)
%     = \frac{1}{K_{j_2}} \sum_{q_2 = 1}^{N_q} 
%     G_{i_m,2}^n (\alpha, j_{q_2}, 1)
%     p_2(\xiQuad{2}) \, w_{q_2}.
% \end{align}
% To conclude, the mixed representation of the cell-averaged solution $\bar U^n _i$ is computed with
% \begin{equation}
%     \bar U^n _i = \frac{1}{2} \sum_{m = 1}^{N_q}
%     \left(
%         \sum_{\alpha=1}^{r}
%         \bar  G_{i_m,1}^n (1, j_1, \alpha) \bar G_{i_m,2}^n (\alpha,j_2,1)
%     \right) 
%     w_n.        
% \end{equation}
% After the summation over $m$, the resulting tensor train is rounded. 
% By abuse of notation, the result is written 
% \begin{equation} \label{eq:TT_average}
%     \bar U^n _i = 
%     \sum_{\alpha=1}^{r}
%     \bar  G_{i,1}^n (1, j_1, \alpha) \bar G_{i,2}^n (\alpha,j_2,1)  
% \end{equation}
%
%
% \change{
% \paragraph{Interpolation in the mixed format.}
% We explain now how the interpolated value $U_{i+1/2}^{n +}$ is computed in the mixed format. 
% The same method is used for  $U_{i+1/2}^{n -}$. 
% We assume that we have the averaged values $\bar U_{i}^{n }$ in the mixed format, whose expression is given by Equation \eqref{eq:TT_average}. 
% We use 
% \begin{align}
%     U_{i+1/2}^{n +} &= \bar U_{i+1}^n  - \frac{\Delta x}{2} (U_x)_{i+1}^n  + A_i^n ,  
%     \\
%     A_i^n  &\approx \left( (j_{1,q} - j_1) (u_{\xi_1})_{i,j_q}^n  + (j_{2,q} - j_2) (u_{\xi_2})_{i,j_q}^n  \right)_{j_q},
% \end{align}
% where $A_i^n (q_1,q_2)$ -- the tensor train approximation of the interpolation in the stochastic direction -- 
% must be given explicitly. 
% To give the expression of  $A_i^n (q_1,q_2)$, we describe how the approximated velocities are computed 
% from the averages values.
% We recall the definition of the approximate derivative in the full format
% \begin{align}
%     (u_{\xi_1})_{i,j}^n  = \minmod \left(
%         \frac{\bar u_{i,j_1, j_2}^n  - \bar u_{i,j_1-1, j_2}^n }{\Delta \xi}, 
%         \frac{\bar u_{i,j_1+1, j_2}^n  - \bar u_{i,j_1, j_2}^n }{\Delta \xi}
%     \right).
% \end{align}
% In the mixed format, the operation $\bar u_{i,j_1} \mapsto \bar u_{i,j_1\pm 1}$ can be done separately on each core: 
% \begin{align}
%     \bar u_{i,j_1,j_2}^n  - \bar u_{i,j_1-1,j_2}^n 
%     & = \sum_\alpha G_{i,1}(1,j_1,\alpha) G_{i,2}(\alpha,j_2,1) 
%     - \sum_\alpha G_{i,1}(1,j_1-1,\alpha) G_{i,2}(\alpha,j_2,1) 
% \end{align}
% By defining the core 
% \begin{equation} \label{eq:shift_core}
%     G_{i,1}^{n -}(1,j_1,\alpha) = G_{i,1}(1,j_1,\alpha) - G_{i,1}(1,j_1-1,\alpha), 
%     %& =: U_{i,j_1-1/2,\dots j_m}^n .
% \end{equation}
% we have
% \begin{equation}
%     \bar u_{i,j_1,j_2}^n  - \bar u_{i,j_1-1,j_2}^n  =  \sum_\alpha G_{i,1}^{n -}(1,j_1,\alpha) G_{i,2}(\alpha,j_2,1). 
% \end{equation}
% The core $G_{i,1}(1,j_1,\alpha)^{n +}$ is defined similarly. 
% The minmod operation is then applied via cross-interpolation and the result is denoted by $(U_{\xi_1})_i^n $. 
% The approximated derivative $(U_{\xi_2})_i^n $ is defined in the same way. 
% Finally, the approximate derivatives are duplicated so that they are defined for the iterpolation points $j_q$. 
% Interpolating in each dimension amounts then to modifying the corresponding core: 
% \begin{align} \label{eq:interpolation_core}
%     (j_{q_1} - j_1) (u_{\xi_1})_i^n (j_{q_1} , j_{q_2} ) 
%     = \sum_\alpha \left[ (j_{q_1} - j_1) G_{1}(1,j_{q_1},\alpha) \right] G_{2}(\alpha,j_{q_2}1).
% \end{align}
% We denote by $(U_{\text{int},1})_i^n $ the tensor train defined with the right-hand side of the above equation. 
% % \question{V1}
% % %For one cell $i$ and one time step $k$ there are 2 tensor trains representing the derivatives in each  (spatial and stochastic) dimension. 
% % We go back to the interpolation in the stochastic dimensions. The term 
% % \begin{align}
% %     \left( (j_{q_1} - j_1) (u_{\xi_1})_i^n  + (j_{q_2} - j_2) (u_{\xi_2})_i^n  \right)_{j_q}
% % \end{align}
% % can be seen as the product of rank-1 tensors with $m=2$ dimensions and $N_\bfxi \times N_q$ entries at each dimension, 
% % \begin{equation}
% %     \left((j_{q_1}-j_1) \otimes (j_{q_2}-j_2) \right) \cdot ((u_{\xi_1})_i^n  \otimes (u_{\xi_2})_i^n )  
% % \end{equation}
% % We denote by $\Delta \bfxi_q$ the tensor 
% % $ \left((j_{q_1}-j_1) \otimes (j_{q_2}-j_2) \right)_{j_q} $. 
% % and by $\nabla_\bfxi u$ the tensor $(u_{\xi_1})_i^n  \otimes (u_{\xi_2})_i^n $. 
% % \\
% Finally, the interpolation reads
% \begin{equation} \label{eq:interpolation-TT}
%     U_{i+1/2}^{n +} 
%     = \bar U_{i+1}^n  - \frac{\Delta x}{2} (U_x)_{i+1}^n  
%     + (U_{\text{int},1})_i^n  + (U_{\text{int},2})_i^n .
% \end{equation}
% \\
% %The pseudocode for the interpolation is described in Algorithm \ref{algo:interpolate}.
% }
%
%
% \paragraph{Expectation. }
% Once the flux is computed with \eqref{eq:FV-TT3},
% it remains to compute the expectation $\bar H_{i+1/2}^n $ from the flux $H_{i+1/2}^n $ at the quadrature points in the mixed format. 
% This is the same method as for the cell averaging. 
% If $G_{i+1/2,1}^n , G_{i+1/2,2}^n $ denote the cores of the tensor train $H_{i+1/2}^n $, 
% the tensor train $\bar H_{i+1/2}^n $ has the cores 
% \begin{align} \label{eq:integration_core}
%     \bar G_{i+1/2,1}^n  (1,j_1,\alpha)^n 
%     & = \frac{1}{K_{j_1}} \sum_{q_1 = 1}^{N_q}
%     G_{i+1/2,1}^n  (1,j_{q_1},\alpha) p_1(\xiQuad{1}) \, w_{q_1},
%     \\ 
%     \bar G_{i+1/2,2}^n  (\alpha, j_2, 1)^n 
%     & = \frac{1}{K_{j_2}} \sum_{q_2 = 1}^{N_q}
%     G_{i+1/2,2}^n  (1,j_{q_2},\alpha) p_2(\xiQuad{2}) \, w_{q_1}. 
% \end{align}


%Remark for Algorithm \ref{algo:interpolate}: the operations on cores can be done on all points at once by applying some tensor. 

% \begin{algorithm}
%     \caption{Cell average} \label{algo:cell_avg} 
% \begin{algorithmic}
%     \ForAll {$i \in \{1, \dots N_x\}$}
%         %%%%%%%
%         \ForAll {$n \in \{1, N_q\}$}
%             \State $G_{i_n,1}^n , \dots G_{i_n,m}^n  \gets \text{cores}(U_{i_n}^n )$  
%             %%%%%%%
%             \ForAll {$ \ell \in \{1, \dots m\}$}
%                 %%%%%%%
%                 \ForAll {$ j \in \{1, \dots N_\xi \}$}
%                     \State $K_{j_\ell} \gets \sum_{q=1}^{N_q} w_{q} p_\ell(\xi_{j_{q}})$
%                     \State $\bar G_{i_n,\ell}^n (:,j,:)
%                     \gets \frac{1}{K_{j}} \sum_{q=1}^{N_q}
%                     G_{i_n  ,\ell}^n (:, j_{q},:) 
%                     p_\ell(\xi_{j_{q}})  
%                     w_{q}$
%                     \Comment{integrate each core}
%                 \EndFor 
%                 %%%%%%%
%                 \State $U_{i_n}^n  \gets TT(\bar G_{i_n, 1}^n , \dots \bar G_{i_n, m}^n )$
%             \EndFor
%             %%%%%%%
%         \EndFor
%         \State $\bar U_i^n  \gets 
%         \frac{1}{2} \sum_{n=1}^{N_q} U_{i_n}^n  w_{n}$. 
%     \EndFor
% \State \Return $\bar U^n $
% \end{algorithmic}
% \end{algorithm}
%Remark for Algorithm \ref{algo:cell_avg}: the operation on the cores can be done for all points at once using operations on tensors (e.g. 'einsum' in the Torch package for Python).
%
%
% \begin{algorithm}
% \caption{Cell expectation} \label{algo:cell_expect} 
% \begin{algorithmic}
%     \ForAll {$i \in \{1, \dots N_x\}$}
%         %%%%%%%
%         \State $G_{i+1/2,1}^n , \dots G_{i+1/2,m}^n  \gets \text{cores}(H_{i+1/2}^n )$  
%         %%%%%%%
%         \ForAll {$ \ell \in \{1, \dots m\}$}
%             %%%%%%%
%             \ForAll {$ j \in \{1, \dots N_\xi \}$}
%                 \State $K_{j} \gets \sum_{q=1}^{N_q} w_{q} p_\ell(\xi_{j_q})$
%                 \State $\bar G_{i+1/2,\ell}^n (:,j,:)
%                 \gets \frac{1}{K_{j}} \sum_{q=1}^{N_q}
%                 G_{i+1/2,\ell}^n (:, j_{q},:) 
%                 p_\ell(\xi_{j_{q}})  
%                 w_{q}$ 
%                 \Comment{integrate each core}
%             \EndFor 
%             %%%%%%%
%             \State $\bar H_{i+1/2}^n  \gets TT(\bar G_{i+1/2,1}^n , \dots \bar G_{i+1/2,m}^n )$
%         \EndFor
%         %%%%%%%
%     \EndFor
% \State \Return $\bar H^n $
% \end{algorithmic}
% \end{algorithm}
