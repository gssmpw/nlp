\section{Evaluations}
\label{sec:experiment}

In this section, we demonstrate that \sassha can indeed improve upon existing second-order methods available for standard deep learning tasks.
We also show that \sassha performs competitively to the first-order baseline methods.
Specifically, \sassha is compared to AdaHessian \citep{adahessian}, Sophia-H \citep{sophia}, Shampoo \cite{gupta2018shampoo}, SGD, AdamW \citep{loshchilov2018decoupled}, and SAM \citep{sam} on a diverse set of both vision and language tasks.
We emphasize that we perform an \emph{extensive} hyperparameter search to rigorously tune all optimizers and ensure fair comparisons.
We provide the details of experiment settings to reproduce our results in \cref{app:hypersearch}.
The code to reproduce all results reported in this work is made available for download at \url{https://github.com/LOG-postech/Sassha}.

\subsection{Image Classification}
\begin{table*}[t!]
    \vspace{-0.5em}
    \centering
    \caption{Image classification results of various optimization methods in terms of final validation accuracy (mean$\pm$std).
    \sassha consistently outperforms the other methods for all workloads.
    * means \emph{omitted} due to excessive computational requirements.}
    
    \vskip 0.1in
    \resizebox{0.8\linewidth}{!}{
        \begin{tabular}{clcccccc}
        \toprule
         & 
         & \multicolumn{2}{c}{CIFAR-10} 
         & \multicolumn{2}{c}{CIFAR-100} 
         & \multicolumn{2}{c}{ImageNet} \\
         \cmidrule(l{3pt}r{3pt}){3-4} \cmidrule(l{3pt}r{3pt}){5-6} \cmidrule(l{3pt}r{3pt}){7-8}
         \multicolumn{1}{c}{ Category }
         & \multicolumn{1}{c}{ Method }
         & \multicolumn{1}{c}{ ResNet-20 } 
         & \multicolumn{1}{c}{ ResNet-32 } 
         & \multicolumn{1}{c}{ ResNet-32 }  
         & \multicolumn{1}{c}{ WRN-28-10} 
         & \multicolumn{1}{c}{ ResNet-50 } 
         & \multicolumn{1}{c}{ ViT-s-32} \\ \midrule

        
       \multirow{4}{*}{First-order}  
       & SGD       & 
         $ 92.03 _{ \textcolor{black!60}{\pm 0.32} } $    &
         $ 92.69 _{\textcolor{black!60}{\pm 0.06} }  $    &
         $ 69.32 _{\textcolor{black!60}{\pm 0.19} }  $    &
         $ 80.06 _{\textcolor{black!60}{\pm 0.15} }  $    &
         $ 75.58 _{\textcolor{black!60}{\pm 0.05} }  $    &
         $ 62.90 _{\textcolor{black!60}{\pm 0.36} }  $   \\

        & AdamW      & 
        $ 92.04 _{\textcolor{black!60}{\pm 0.11} }  $     &
        $ 92.42 _{\textcolor{black!60}{\pm 0.13} }  $     &
        $ 68.78 _{\textcolor{black!60}{\pm 0.22} }  $     &
        $ 79.09 _{\textcolor{black!60}{\pm 0.35} }  $     &
        $ 75.38 _{\textcolor{black!60}{\pm 0.08} }  $     &
        $ 66.46 _{\textcolor{black!60}{\pm 0.15} }  $    \\
        
        & SAM $_{\text{SGD}}$  &
        $ 92.85 _{\textcolor{black!60}{\pm 0.07} }  $    &
        $ 93.89 _{\textcolor{black!60}{\pm 0.13} }  $    &
        $ 71.99 _{\textcolor{black!60}{\pm 0.20} }  $    &
        $ 83.14 _{\textcolor{black!60}{\pm 0.13} }  $    &
        $ 76.36 _{\textcolor{black!60}{\pm 0.16} }  $    &
        $ 64.54 _{\textcolor{black!60}{\pm 0.63} }  $    \\
        
        & SAM $_{\text{AdamW}}$  &
        $ 92.77 _{\textcolor{black!60}{\pm 0.29} }  $    &
        $ 93.45 _{\textcolor{black!60}{\pm 0.24} }  $    &
        $ 71.15 _{\textcolor{black!60}{\pm 0.37} }  $    &
        $ 82.88 _{\textcolor{black!60}{\pm 0.31} }  $    &
        $ 76.35 _{\textcolor{black!60}{\pm 0.16} }  $    &
        $ 68.31 _{\textcolor{black!60}{\pm 0.17} }  $    \\

        \midrule
        
        \multirow{4}{*}{Second-order} &
        AdaHessian &
        $ 92.00 _{\textcolor{black!60}{\pm 0.17} } $  &
        $ 92.48 _{\textcolor{black!60}{\pm 0.15} } $  &
        $ 68.06 _{\textcolor{black!60}{\pm 0.22} } $  &
        $ 76.92 _{\textcolor{black!60}{\pm 0.26} } $  &
        $ 73.64 _{\textcolor{black!60}{\pm 0.16} } $  &
        $ 66.42 _{\textcolor{black!60}{\pm 0.23} } $  \\
        
        & Sophia-H   & 
        $ 91.81 _{\textcolor{black!60}{\pm 0.27} } $  &
        $ 91.99 _{\textcolor{black!60}{\pm 0.08} } $  &
        $ 67.76 _{\textcolor{black!60}{\pm 0.37} } $  & 
        $ 79.35 _{\textcolor{black!60}{\pm 0.24} } $  & 
        $ 72.06 _{\textcolor{black!60}{\pm 0.49} } $  &
        $ 62.44 _{\textcolor{black!60}{\pm 0.36} } $  \\
        
        & Shampoo    & 
        $ 88.55 _ {\textcolor{black!60}{\pm 0.83}}$  &
        $ 90.23 _{\textcolor{black!60}{\pm 0.24}} $  &
        $ 64.08 _{\textcolor{black!60}{\pm 0.46}} $  &
        $ 74.06 _{\textcolor{black!60}{\pm 1.28}} $  &
        $*$                                          &
        $*$  \\
        
        \cmidrule(l{3pt}r{3pt}){2-8}
        
        \rowcolor{green!20} &
        \sassha    &
        $ \textbf{92.98} _{\textcolor{black!60}{\pm 0.05} }  $ &
        $ \textbf{94.09} _{\textcolor{black!60}{\pm 0.24} }  $ &
        $ \textbf{72.14} _{\textcolor{black!60}{\pm 0.16} }  $ & 
        $ \textbf{83.54} _{\textcolor{black!60}{\pm 0.08} }  $ &
        $ \textbf{76.43} _{\textcolor{black!60}{\pm 0.18} }  $ &
        $ \textbf{69.20} _{\textcolor{black!60}{\pm 0.30} }  $ \\
        
        \bottomrule
        \end{tabular}
    }
    \vskip 0.1in
    \label{tab:im_cls_results}
\end{table*}

\begin{figure*}[t!]
    \vspace{-0.5em}
    \centering
    \resizebox{0.8\linewidth}{!}{
    \includegraphics[width=0.325\linewidth]{figures/validation/Res32-CIFAR10-Acc.pdf}
    \includegraphics[width=0.325\linewidth]{figures/validation/WRN28-CIFAR100-Acc.pdf}
    \includegraphics[width=0.325\linewidth]{figures/validation/Res50-ImageNet-Acc.pdf}
    }
    \vspace{-0.5em}
    \caption{
    Validation accuracy curves along the training trajectory.
    We also provide loss curves in \cref{app:valloss}.
    }
    \label{fig:im_cls_results}
    \vspace{-0.7em}
\end{figure*}

\begin{table*}[ht!]
    \centering
    \caption{
    Language finetuning and pertraining results for various optimizers. For finetuning, \sassha achieves better results than AdamW and AdaHessian and compares competitively with Sophia-H. For pretraining, \sassha achieves the lowest perplexity among all optimizers.
    }
    \vskip 0.1in
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lc}
            \toprule
             & \multicolumn{1}{c}{$\textbf{Pretrain} / $ GPT1-mini} \\
             \cmidrule(l{3pt}r{3pt}){2-2}
             & Wikitext-2 \\
             & \texttt{Perplexity}\\
            \midrule
            
            AdamW & $ 175.06 $ \\
            SAM $_{\text{AdamW}}$ & $ 158.06 $ \\
            AdaHessian & $ 407.69 $ \\
            Sophia-H & $ 157.60 $ \\
            
            \midrule 
            
            \rowcolor{green!20}
            \sassha &
            $ \textbf{122.40} $ \\
            
            \bottomrule
        \end{tabular}
        
        \begin{tabular}{|ccccccc}
            \toprule
                         \multicolumn{7}{|c}{ \textbf{Finetune} /  SqeezeBERT } \\
                         \cmidrule(l{3pt}r{3pt}){1-7}
                         SST-2 &  MRPC & STS-B & QQP & MNLI & QNLI & RTE \\
             \texttt{Acc} &  \texttt{Acc / F1}  & \texttt{S/P corr.} & \texttt{F1 / Acc} & \texttt{mat/m.mat} &  \texttt{Acc} &  \texttt{Acc} \\
            \midrule
            
            %AdamW         & 
            $ 90.29 _{\textcolor{black!60}{\pm 0.52}} $ 
            & $ 84.56 _{ \textcolor{black!60}{\pm 0.25} } $ / $ 88.99 _{\textcolor{black!60}{\pm 0.11}} $ 
            & $ 88.34 _{\textcolor{black!60}{\pm 0.15}} $ / $ 88.48 _{\textcolor{black!60}{\pm 0.20}} $ 
            & $ 89.92 _{\textcolor{black!60}{\pm 0.05}} $ / $ 86.58 _{\textcolor{black!60}{\pm 0.11}} $ 
            & $ 81.22 _{\textcolor{black!60}{\pm 0.07}} $ / $ 82.26 _{\textcolor{black!60}{\pm 0.05}} $ 
            & $ 89.93 _{\textcolor{black!60}{\pm 0.14}} $ 
            & $ 68.95 _{\textcolor{black!60}{\pm 0.72}} $  \\
    
            %SAM _{\text{AdamW}}   &
            $ \textbf{90.52} _{\textcolor{black!60}{\pm 0.27}} $ 
            & $ 83.25 _{\textcolor{black!60}{\pm 2.79}} $ / $ 87.90 _{\textcolor{black!60}{\pm 2.21}} $ 
            & $ 88.38 _{\textcolor{black!60}{\pm 0.01}} $ / $ 88.79 _{\textcolor{black!60}{\pm 0.99}} $ 
            & $ 90.26 _{\textcolor{black!60}{\pm 0.28}} $ / $ 86.99 _{\textcolor{black!60}{\pm 0.31}} $ 
            & $ 81.56 _{\textcolor{black!60}{\pm 0.18}} $ / $ \textbf{82.46} _{\textcolor{black!60}{\pm 0.19}} $ 
            & $ \textbf{90.38} _{\textcolor{black!60}{\pm 0.05}} $ 
            & $ 68.83 _{\textcolor{black!60}{\pm 1.46}} $  \\
    
            %AdaHessian    & 
            $ 89.64 _{\textcolor{black!60}{\pm 0.13}} $ 
            & $ 79.74 _{\textcolor{black!60}{\pm 4.00}} $ / $ 85.26 _{\textcolor{black!60}{\pm 3.50}} $ 
            & $ 86.08 _{\textcolor{black!60}{\pm 4.04}} $ / $ 86.46 _{\textcolor{black!60}{\pm 4.06}} $ 
            & $ 90.37 _{\textcolor{black!60}{\pm 0.05}} $ / $ 87.07 _{\textcolor{black!60}{\pm 0.05}} $ 
            & $ 81.33 _{\textcolor{black!60}{\pm 0.17}} $ / $ 82.08 _{\textcolor{black!60}{\pm 0.02}} $ 
            & $ 89.94 _{\textcolor{black!60}{\pm 0.12}} $ 
            & $ 71.00 _{\textcolor{black!60}{\pm 1.04}} $ \\
            
            % Sophia-H  &
            $ 90.44 _{\textcolor{black!60}{\pm 0.46}} $ 
            & $ 85.78 _{\textcolor{black!60}{\pm 1.07}} $ / $ 89.90 _{\textcolor{black!60}{\pm 0.82}} $ 
            & $ 88.17 _{\textcolor{black!60}{\pm 1.07}} $ / $ 88.53 _{\textcolor{black!60}{\pm 1.13}} $ 
            & $ 90.70 _{\textcolor{black!60}{\pm 0.04}} $ / $ 87.60 _{\textcolor{black!60}{\pm 0.06}} $ 
            & $ \textbf{81.77} _{\textcolor{black!60}{\pm 0.18}} $ / $ 82.36 _{\textcolor{black!60}{\pm 0.22}} $ 
            & $ 90.12_{\textcolor{black!60}{\pm 0.14}} $ 
            & $ 70.76 _{\textcolor{black!60}{\pm 1.44}} $  \\
            
            \midrule
            
            \rowcolor{green!20} 
            $ 90.44 _{\textcolor{black!60}{\pm 0.98}} $    &
            $ \textbf{86.28} _{\textcolor{black!60}{\pm 0.28}} $ / $ \textbf{90.13} _{\textcolor{black!60}{\pm 0.161}} $     &
            $ \textbf{88.72} _{\textcolor{black!60}{\pm 0.75}} $ / $ \textbf{89.10} _{\textcolor{black!60}{\pm 0.70}}  $     &
            $ \textbf{90.91} _{\textcolor{black!60}{\pm 0.06}} $ / $ \textbf{87.85}  _{\textcolor{black!60}{\pm 0.09}} $     &
            $ 81.61 _{\textcolor{black!60}{\pm 0.25}} $ / $ 81.71 _{\textcolor{black!60}{\pm 0.11}} $     &
            $ 89.85_{\textcolor{black!60}{\pm 0.20}} $    &
            $ \textbf{72.08} _{\textcolor{black!60}{\pm 0.55}} $  \\
            
            \bottomrule
        \end{tabular}
    }
    \vspace{-0.5em}
    \label{tab:language}
\end{table*}

We first evaluate \sassha for image classification on CIFAR-10, CIFAR-100, and ImageNet.
We train various models of the ResNet family \citep{he2016deep,zagoruyko2016wide} and an efficient variant of Vision Transformer \citep{beyer2022better}.
We adhere to standard inception-style data augmentations during training instead of making use of advanced data augmentation techniques \citep{devries2017improved} or regularization methods \citep{gastaldi2017shake}.
Results are presented in \cref{tab:im_cls_results} and \cref{fig:im_cls_results}.

We begin by comparing the generalization performance of adaptive second-order methods to that of first-order methods.
Across all settings, adaptive second-order methods consistently exhibit lower accuracy than their first-order counterparts.
This observation aligns with previous studies indicating that second-order optimization often result in poorer generalization compared to first-order approaches.
In contrast, \sassha, benefiting from sharpness minimization, consistently demonstrates superior generalization performance, outperforming both first-order and second-order methods in every setting.
Particularly, \sassha is up to 4\% more effective than the best-performing adaptive or second-order methods (\eg, WRN-28-10, ViT-s-32).
Moreover, \sassha continually surpasses SGD and AdamW, even when they are trained for twice as many epochs, achieving a performance margin of about 0.3\% to 3\%. 
Further details are provided in \cref{app:comp_fo_fair}.

Interestingly, \sassha also outperforms SAM.
Since first-order methods typically exhibit superior generalization performance compared to second-order methods, it might be intuitive to expect SAM to surpass \sassha if the two are viewed merely as the outcomes of applying sharpness minimization to first-order and second-order methods, respectively.
However, the results conflict with this intuition.
We attribute this to the careful design choices made in \sassha, stabilizing Hessian approximation under sharpness minimization, so as to unleash the potential of the second-order method, leading to its outstanding performance.
As a support, we show that naively incorporating SAM into other second-order methods does not yield these favorable results in \cref{app:samsophia}.
We also make more comparisons with SAM in \cref{sec:sassha_vs_sam}.

\subsection{Language Modeling}

Recent studies have shown the potential of second-order methods for pretraining language models.
Here, we first evaluate how \sassha performs on this task.
Specifically, we train GPT1-mini, a scaled-down variant of GPT1 \citep{radford2019language}, on Wikitext-2 dataset \citep{merity2022pointer} using various methods including \sassha and compare their results (see the left of \cref{tab:language}).
Our results show that \sassha achieves the lowest perplexity among all methods including Sophia-H \citep{sophia}, a recent method that is designed specifically for language modeling tasks and sets state of the art, which highlights generality in addition to the numerical advantage of \sassha.

We also extend our evaluation to finetuning tasks.
Specifically, we finetune SqueezeBERT \citep{iandola2020squeezebert} for diverse tasks in the GLUE benchmark \citep{wang2018glue}.
The results are on the right side of \cref{tab:language}.
It shows that \sassha compares competitively to other second-order methods.
Notably, it also outperforms AdamW---often the method of choice for training language models---on nearly all tasks.

\subsection{Comparison to SAM}\label{sec:sassha_vs_sam}

So far, we have seen that \sassha outperforms second-order methods quite consistently on both vision and language tasks.
Interestingly, we also find that \sassha often improves upon SAM.
In particular, it appears that the gain is larger for the Transformer-based architectures, \ie, ViT results in \cref{tab:im_cls_results} or GPT/BERT results in \cref{tab:language}.

We posit that this is potentially due to the robustness of \sassha to the block heterogeneity inherent in Transformer architectures, where the Hessian spectrum varies significantly across different blocks.
This characteristic is known to make SGD perform worse than adaptive methods like Adam on Transformer-based models \citep{zhang2024why}.
Since \sassha leverages second-order information via preconditioning gradients, it has the potential to address the ill-conditioned nature of Transformers more effectively than SAM with first-order methods.

To push further, we conducted additional experiments.
First, we allocate more training budgets to SAM to see whether it compares to \sassha.
% additionally compare \sassha to SAM with more training budgets.
The results are presented in \cref{tab:sam}.
We find that SAM still underperforms \sassha, even though it is given more budgets of training iterations over data or wall-clock time.
Furthermore, we also compare \sassha to more advanced variants of SAM including ASAM \citep{asam} and GSAM \citep{gsam}, showing that \sassha performs competitively even to these methods (\cref{app:samvariants_vs_sassha}).
Notably, however, these variants of SAM require a lot more hyperparameter tuning to be compared.

