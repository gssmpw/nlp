In this section, we provide preliminary convergence analysis results.
Based on the well-established analyses of \citet{li2023convergence, khanh2024fundamental}, we further investigate the complexities arising from preconditioned perturbed gradients.\\

\begin{assumption}
    \label{assumption:convex_smooth_bounded_nonzero}
    The function $ f : \mathbb{R}^d \to \mathbb{R} $ is convex, $\beta$-smooth, and bounded from below, i.e., $ f^* := \inf_x f(x) > -\infty $. Additionally, the gradient $ \nabla f(x_t) $  is non-zero for a finite number of iterations, i.e., $ \nabla f(x_t) \neq 0 $ for all $ t \in \{1, 2, \dots, n\} $.
\end{assumption}
\vspace{5mm}
\begin{assumption}
    \label{assumption:learning_rate_and_perturbation_radius}
    Step sizes $\eta_t$ and perturbation radii $\rho_t$ are assumed to satisfy the following conditions:
    \begin{align*}
        \sum_{t=1}^{\infty} \eta_t = \infty, \quad \sum_{t=1}^{\infty} \eta_t^2 < \infty, \quad \sum_{t=1}^{\infty} \rho_t^2 \eta_t < \infty.
    \end{align*}
\end{assumption}
\vspace{3mm}
\begin{remark}
    The following notations will be used throughout
    \begin{enumerate}
        \item $ g_t := \nabla f(x_t) $ denotes the gradient of $ f $ at iteration $t$.
        \item The intermediate points and the difference between the gradients are defined as
        \begin{align*}
            x_{t+\frac{1}{2}} := x_t + \rho_t \frac{g_t}{\|g_t\|}, \quad g_{t+\frac{1}{2}} := \nabla f(x_{t+\frac{1}{2}}), \quad \delta_t := g_{t+\frac{1}{2}} - g_t.
        \end{align*}
        \item For $ u, v \in \mathbb{R}^d $, operations such as $ \sqrt{v}, |v| $ and $ \frac{v}{u} $, as well as the symbols $ \preceq $ and $ \succeq $, are applied element-wise.
    \end{enumerate}
\end{remark}
\vspace{3mm}
\begin{remark}
    The update rule for the iterates is given by
    \begin{align}\label{remark:sassha_iteration_adamlike}
        x_{t+1} = x_t - \frac{\eta_t}{\sqrt{|\operatorname{diag}(\nabla^2 f(x_{t+\frac{1}{2}}))|} + \epsilon} \odot g_{t+\frac{1}{2}},
    \end{align}
    where $ \operatorname{diag} $ extracts the diagonal elements of a matrix as a vector, or constructs a diagonal matrix from a vector, and  $\epsilon$  is a damping constant. Define  $h_t$  as
    \begin{align*}
        h_t = \frac{\eta_t}{\sqrt{|\operatorname{diag}(\nabla^2 f(x_{t+\frac{1}{2}}))|} + \epsilon},
    \end{align*}
    then the following hold
    \begin{enumerate}
        \item From the convexity and $\beta$-smoothness of $ f $, the diagonal elements of $ \nabla^2 f(x) $ are bounded within the interval $[0, \beta]$, i.e.,
        \begin{align*}
            0 \leq \left[\nabla^2 f(x)\right]_{(i,i)} = e_i^\top \nabla^2 f(x) e_i \leq \beta,        
        \end{align*}
        where $ e_i $ is the $ i $-th standard basis vector in $ \mathbb{R}^d $.
        \item The term $h_t$ is bounded as 
        \begin{align*}
            \frac{\eta_t}{\sqrt{\beta} + \epsilon} \preceq h_t \preceq \frac{\eta_t}{\epsilon}.
        \end{align*}
    \end{enumerate}
\end{remark}

\begin{remark}
    For the matrix representation
    \begin{enumerate}
        \item Denoting $ H_t := \operatorname{diag}(h_t) $, the matrix bounds for $H_t$  are given by
        \begin{align}\label{remark:matrix_bound}
            \frac{\eta_t}{\sqrt{\beta} + \epsilon} I \preceq H_t \preceq \frac{\eta_t}{\epsilon} I,
        \end{align}
        where  $I$  is the identity matrix.
        \item Using the matrix notation $ H_t $, the update for the iterates is expressed as
        \begin{align*}
            x_{t+1} = x_t - H_t g_{t+\frac{1}{2}}.    
        \end{align*}
        \end{enumerate}
\end{remark}

\begin{remark}
    From the $\beta$-smoothness of $f$, $\delta_t$ is bounded by
    \begin{align}\label{remark:delta_bound}
        \|\delta_t\| \leq \beta \| x_t + \rho_t \frac{\nabla f(x_t)}{\|\nabla f(x_t)\|} - x_t \| = \beta \rho_t.
    \end{align}
\end{remark}

\begin{lemma}[Descent Lemma]\label{lemma:descent_lemma}
    Under \cref{assumption:convex_smooth_bounded_nonzero} and \cref{assumption:learning_rate_and_perturbation_radius}, for given $\beta$ and $\epsilon$, there exists a $T \in \mathbb{N}$ such that for $\forall t \geq T$, $\eta_t$ satisfies $\eta_t \leq \min\left\{ \frac{\epsilon^2}{6\beta(\sqrt{\beta} + \epsilon)}, \frac{\epsilon}{4\beta} \right\}$.
    For such $t\geq T$, the following inequality holds
    \begin{align}
        f(x_{t+1}) \leq f(x_t) - \frac{\eta_t}{2(\sqrt{\beta} + \epsilon)} \|g_t\|^2 + \frac{\eta_t}{\epsilon} \|\delta_t\|^2.
    \end{align}
\end{lemma}

\begin{proof}
    We begin by applying the $\beta$-smoothness of $f$, 
    \begin{align*} 
        f(x_{t+1}) & \leq f(x_t) + \left\langle g_t, x_{t+1} - x_t \right\rangle + \frac{\beta}{2} \|x_{t+1} - x_t\|^2\\
        & =f\left(x_t\right)-\left\langle g_t, H_t (g_t+\delta_t)\right\rangle+\frac{\beta}{2}\left\|H_t (g_t+\delta_t)\right\|^2 \\
        & \leq f\left(x_t\right)- g_t^\top H_tg_t 
        + \frac{1}{2\alpha} g_t^\top H_t g_t + \frac{\alpha}{2} \delta_t^\top H_t \delta_t
        +\frac{\beta}{2}\left\|H_t (g_t+\delta_t)\right\|^2\\
        % &\hspace{13mm} (\because)\ \text{Young's inequality, } g_t^\top H_t \delta_t \leq \frac{1}{2\alpha}g_t^\top H_t g_t+\frac{\alpha}{2}\delta_t^\top H_t \delta_t\notag\\
        & \leq f\left(x_t\right)-(1-\frac{1}{2\alpha})  \frac{\eta_t}{\sqrt{\beta}+\epsilon} \|g_t\|^2  +\frac{\alpha}{2} \frac{\eta_t}{\epsilon} \|\delta_t\|^2 +\frac{\beta}{2}\frac{\eta_t^2}{\epsilon^2}\left\|g_t+\delta_t\right\|^2 \\
        % &\hspace{59mm} (\because)\ \frac{\eta_t}{\sqrt{\beta}+\epsilon} I \preceq H_t \preceq \frac{\eta_t}{\epsilon} I\notag\\
        & \leq f\left(x_t\right)-(1-\frac{1}{2\alpha})  \frac{\eta_t}{\sqrt{\beta}+\epsilon} \|g_t\|^2  +\frac{\alpha}{2} \frac{\eta_t}{\epsilon} \|\delta_t\|^2 +\beta\frac{\eta_t^2}{\epsilon^2}(\left\|g_t\|^2+\|\delta_t\right\|^2) \\
        % &\hspace{60mm} (\because)\ \|a+b\|^2 \leq 2\|a\|^2+2\|b\|^2\notag\\
        & = f\left(x_t\right)-\eta_t((1-\frac{1}{2\alpha})\frac{1}{\sqrt{\beta}+\epsilon}-\beta\frac{\eta_t}{\epsilon^2}) \|g_t\|^2  +\eta_t(\frac{\alpha}{2\epsilon} + \beta\frac{\eta_t}{\epsilon^2})\|\delta_t\|^2.
\end{align*}
The second inequality follows from Young's inequality, the third inequality is obtained from \cref{remark:matrix_bound}, and the last inequality is simplified using the property \( \|a + b\|^2 \leq 2\|a\|^2 + 2\|b\|^2 \). By setting \( \alpha = \frac{3}{2} \), we get
\begin{align*}
        \hspace{3.5mm}= f\left(x_t\right)-\eta_t(\frac{2}{3}\left(\frac{1}{\sqrt{\beta}+\epsilon}\right)-\beta\frac{\eta_t}{\epsilon^2}) \|g_t\|^2  +\eta_t(\frac{3}{4\epsilon} + \beta\frac{\eta_t}{\epsilon^2})\|\delta_t\|^2.
    \end{align*}    
    Since $\eta_t \to 0,\ \exists T\in\mathbb{N}$ such that $\eta_t\leq\min\{\frac{\epsilon^2}{6\beta(\sqrt{\beta}+\epsilon)}, \frac{\epsilon}{4\beta}\}$,
    this gives $\frac{2}{3}\left(\frac{1}{\sqrt{\beta}+\epsilon}\right)-\beta\frac{\eta_t}{\epsilon^2}\geq \frac{1}{2(\sqrt{\beta}+\epsilon)}$ and $\frac{3}{4\epsilon} + \beta\frac{\eta_t}{\epsilon^2} \leq \frac{1}{\epsilon}$, which implies
    \begin{align*}
    \hspace{-34mm} \leq f\left(x_t\right)-\frac{\eta_t}{2(\sqrt{\beta}+\epsilon)} \|g_t\|^2 +\frac{\eta_t}{\epsilon} \|\delta_t\|^2     
    \end{align*}
\end{proof}
\begin{theorem}
    Under \cref{assumption:convex_smooth_bounded_nonzero} and \cref{assumption:learning_rate_and_perturbation_radius}, given any initial point $x_0 \in \mathbb{R}^d$, let $\{x_t\}$ be generated by \cref{remark:sassha_iteration_adamlike}. Then, it holds that $\liminf_{t \to \infty} \|g_t\| = 0$.
\end{theorem}
\begin{proof}
    From \cref{lemma:descent_lemma} and \cref{remark:delta_bound}, we have the bound
    \begin{align*}
        f(x_{t+1}) &\leq f(x_t) - \frac{\eta_t}{2(\sqrt{\beta} + \epsilon)} \|g_t\|^2 + \frac{\eta_t}{\epsilon} \|\delta_t\|^2 \\
        &\leq f(x_t) - \frac{\eta_t}{2(\sqrt{\beta} + \epsilon)} \|g_t\|^2 + \frac{\eta_t}{\epsilon} \beta^2 \rho_t^2.
    \end{align*}
    
    By rearranging the terms, we obtain the following
    \begin{align*}
        \frac{\eta_t}{2(\sqrt{\beta} + \epsilon)} \|g_t\|^2 \leq f(x_t) - f(x_{t+1}) + \frac{\eta_t}{\epsilon} \beta^2 \rho_t^2.
    \end{align*}
    
    For any $M > T$, we have
    \begin{align*}
    \frac{1}{2(\sqrt{\beta} + \epsilon)} \sum_{t=T}^M \eta_t \|g_t\|^2 &\leq \sum_{t=T}^M \left(f(x_t) - f(x_{t+1})\right) + \frac{\beta^2}{\epsilon} \sum_{t=T}^M \rho_t^2 \eta_t \\
    &= f(x_T) - f(x_{M+1}) + \frac{\beta^2}{\epsilon} \sum_{t=T}^{M} \rho_t^2 \eta_t \\
    &\leq f(x_T) - \inf_{t \in \mathbb{N}} f(x_t) + \frac{\beta^2}{\epsilon} \sum_{t=T}^{M} \rho_t^2 \eta_t.
    \end{align*}
    As \( M \to \infty \), the series \( \sum_{t=T}^{\infty} \eta_t \|g_t\|^2 \) converges. Now, assume for contradiction that \( \liminf_{t \to \infty} \|g_t\| \neq 0 \). This means there exists some \( \xi > 0 \) and \( N \geq T \) such that \( \|g_t\| \geq \xi \) for all \( t \geq N \). Consequently, we have
    \begin{align*}
        \infty > \sum_{t=N}^{\infty} \eta_t \|g_t\|^2 \geq \xi^2 \sum_{t=N}^{\infty} \eta_t = \infty,    
    \end{align*}
    which is a contradiction. Therefore, \( \liminf_{t \to \infty} \|g_t\| = 0 \).
\end{proof}
