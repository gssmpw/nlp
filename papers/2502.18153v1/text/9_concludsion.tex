% \vspace{-0.5em}
\section{Conclusion}
In this work, we focus on addressing the issue of poor generalization in approximate second-order methods.
To this end, we propose a new method called \sassha that stably minimizes sharpness within the framework of second-order optimization.
\sassha converges to flat solutions and achieves state-of-the-art performance within this class.
\sassha also performs competitively to widely-used first-order, adaptive, and sharpness-aware methods.
\sassha achieves this in robust, stable, and efficient ways without incurring much cost or requiring extra hyperparameter tuning.
All of these are rigorously assessed with extensive experiments.

% Moreover, stable Hessian approximation in \sassha effectively mitigates instability issues during the sharpness minimization without extra hyperparameters, and allows lazy Hessian updates, significantly reducing the cost of Hessian computation.
% Through this process, we also discover that sharpness minimization can aid lazy Hessian updates.
Nonetheless, there are many limitations to be addressed in this work for more improvements which may include, but are not limited to, 
extending experiments to extreme scales of various models and different data, and consolidating theoretical foundations.
Seeing it as an exciting opportunity, we plan to investigate further in future work.

% evaluating on a more extreme scale or other domains,
% and developing theoretical properties such as convergence rate and implicit bias, all to more rigorously confirm the value of \sassha.
% Seeing it as an exciting opportunity, we are planning to investigate further in future work.

% In this work, we have addressed the poor generalization issue of approximate second-order methods by proposing \sassha, which explicitly minimizes sharpness within approximate second-order optimization, achieving competitive performance for various standard deep learning tasks.
% Nonetheless, there are many remaining possibilities for further improvements which may include, but are not limited to, 
% evaluating on a more extreme scale and other data distributions in different domains,
% and developing theoretical properties such as convergence rate and implicit bias, all to more rigorously confirm the value of \sassha.
% Seeing it as an exciting opportunity, we are planning to investigate further in future work.

% In this work, we have attended to the poor generalization issue of approximate second-order methods.
% Inspired by the concept of sharpness minimization and their potential for generalization improvement, we have made a collection of effective alterations to existing methods and presented a new method called \sassha.
% We emphasize that \sassha has been extensively evaluated and achieved state-of-the-art performance for various standard deep learning tasks.
% Nonetheless, there are many remaining possibilities for further improvements.
% Some examples may include, but not limited to, 
% evaluating on more extreme scale and other data distributions in different domains,
% and developing theoretical properties such as convergence and implicit bias, all to more rigorously confirm the value of \sassha.
% Seeing it as an exciting opportunity, we are planning to investigate further in future work.