
\begin{figure*}[t!]
    \centering
    % \resizebox{\linewidth}{!}{
        % \begin{subfigure}[b]{0.32\textwidth}
        %     \centering
        %     \includegraphics[width=\textwidth, trim={0 2em 0 4em}, clip]{figures/landscape/landscape_sgd.pdf}
        %     \caption{SGD}
        % \end{subfigure}
        \begin{subfigure}[b]{0.245\textwidth}
            \centering
            \includegraphics[width=\textwidth, trim={0 2em 4em 4em}, clip]{figures/landscape/landscape_adahessian.pdf}
            \caption{AdaHessian}
        \end{subfigure}
        \begin{subfigure}[b]{0.245\textwidth}
            \centering
            \includegraphics[width=\textwidth, trim={0 2em 4em 4em}, clip]{figures/landscape/landscape_sophia.pdf}
            \caption{Sophia-H}
        \end{subfigure}
     \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim={0 2em 4.6em 4em}, clip]{figures/landscape/landscape_shampoo.pdf}
        \caption{Shampoo}
     \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=\textwidth, trim={0 2em 4em 4em}, clip]{figures/landscape/landscape_sassha.pdf}
        \caption{\sassha}
    \end{subfigure}
    \caption{Visualization of the found solutions along the directions of the dominant eigenvectors.}
    \label{fig:landscape}
    %
\end{figure*}

\section{Related Works} \label{sec:related_works}
\paragraph{Second-order optimization for deep learning.}
First-order methods such as SGD are popular optimization methods for deep learning due to their low per-iteration cost and good generalization performance \citep{hardt2016train}.
However, these methods have two major drawbacks: slow convergence under ill-conditioned landscapes and high sensitivity to hyperparameter choices such as learning rate \citep{doi:10.1137/1.9781611976236}.
Adaptive methods \citep{duchi2011adaptive, hinton2012neural, kingma2014adam} propose using empirical Fisher-type preconditioning to alleviate these issues, though recent studies suggest their insufficiency to do so \citep{kunstner2019limitations}.
This has led to recent interest in developing approximate second-order methods such as Hessian-Free Inexact Newton methods \citep{martens2010deep, kiros2013training}, stochastic quasi-Newton methods \citep{byrd2016stochastic, gower2016stochastic}, Gauss-Newton methods \citep{schraudolph2002fast, botev2017practical}, natural gradient methods \citep{amari2000adaptive}, and Kronecker-factored approximations \citep{martens2015optimizing, goldfarb2020practical}.
However, these approaches still incur non-trivial memory and computational costs, or are difficult to parallelize, limiting their applicability to large-scale problems such as deep learning.
This has driven growing interest in developing more scalable and efficient second-order approaches, particularly through diagonal scaling methods \citep{bottou, adahessian, sophia}, to better accommodate large-scale deep learning scenarios.
\vspace{-1em}
\paragraph{Sharpness minimization for generalization.}
The relationship between the geometry of the loss landscape and the generalization ability of neural networks was first discussed in the work of \citet{NIPS1994_Hochreiter}, and the interest in this subject has persisted over time.
Expanding on this foundation, subsequent studies have explored the impact of flat regions on generalization both empirically and theoretically \citep{Hochreiter1997, keskar2016large, DR17, NIPS2017_Neyshabur, dinh17b, 2020Fantastic}. 
Motivated by this, various approaches have been proposed to achieve flat minima such as regularizing local entropy \citep{chaudhari2017entropy}, averaging model weights \citep{izmailov2018averaging}, explicitly regularizing sharpness by solving a min-max problem \citep{sam}, and injecting anti-correlated noise \citep{antipgd_orvieto22a}, to name a few.
In particular, the sharpness-aware minimization (SAM) \citep{sam} has attracted significant attention for its strong generalization performance across various domains \citep{chenvision, bahri2022sharpness, qu2022generalized} and its robustness to label noise \citep{baek2024why}.
Nevertheless, to our knowledge, the sharpness minimization scheme has not been studied to enable second-order methods to find flat minima as of yet.

