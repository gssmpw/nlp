\section{Method}

In the previous section, we observe that the generalization performance of approximate second-order algorithms anti-correlates with the sharpness of their solutions.
Based on this, we introduce \sassha---a novel adaptive second-order method designed to improve generalization by reducing sharpness without adversely impacting the Hessian.


\subsection{Sharpness-aware Second-order Optimization}\label{sec:psol}

We consider a min-max problem, similar to \citet{keskar2016large, sam}, to minimize sharpness. 
This is defined as minimizing the objective $f$ within the entire $\rho$-ball neighborhood:
\begin{equation} \label{eq:sam_optimization}
    \min_{x\in \R^d} \max_{\|\epsilon\|_2\leq \rho} f(x+\epsilon),
\end{equation}

Based on this, we construct our sharpness minimization technique for second-order optimization as follows.
We first follow a similar procedure as \citet{sam} by solving for $\epsilon$ on the first-order approximation of the objective, which exactly solves the dual norm problem as follows:
\begin{align}
    \epsilon_t^\star = \argmax_{\|\epsilon\|_2\leq \rho} f(x_t) + \epsilon^\top\nabla f(x_t) 
    %&= \argmax_{\|\epsilon\|_2\leq \rho} \epsilon^\top\nabla f(x_t) \nonumber \\
    &= \rho\frac{\nabla f(x_t)}{\|\nabla f(x_t)\|_2}.
\end{align}
We plug this back to yield the following perturbed objective function:
\begin{equation*}
    \Tilde{f}_{t}(x) \coloneqq f\left(x+\rho\frac{\nabla f(x_t)}{\|\nabla f(x_t)\|_2}\right),
\end{equation*}
which shifts the point of the approximately highest function value within the neighborhood to the current iterate.

With this sharpness-penalized objective, we proceed to make a second-order Taylor approximation: 
\begin{align*}
    x_{t+1} = \argmin_x~ & \Tilde{f}_{t}\left(x_t\right) + \nabla \Tilde{f}_{t}\left(x_t\right)^\top (x-x_t) \nonumber \\ 
    &+ (x-x_t)^\top \Tilde{H}_{t}\left(x_t\right) (x-x_t),
\end{align*}
where $\Tilde{H}_{t}$ denotes the Hessian of $\Tilde{f}_{t}$.
Using the first-order optimality condition, we derive the basis update rule for our sharpness-aware second-order optimization: 
\begin{align*}
    x_{t+1} &= x_t - \Tilde{H}_{t}\left(x_t\right)^{-1} \nabla \Tilde{f}_{t}\left(x_t\right) \nonumber \\
            &= x_t - H \left(x_t+ \epsilon_t^\star \right)^{-1} \nabla f \left(x_t+ \epsilon_t^\star \right),
\label{eq: update}
\end{align*}
where $H$ denotes the Hessian of the original objective $f$.

Practical second-order methods must rely on approximately estimated Hessians (\ie, $H \rightarrow \widehat{H}$) since the exact computation is prohibitively expensive for large-scale problems.
We choose to employ the diagonal approximation via Hutchinson's method.
However, as we will show in our analysis (\cref{sec:sqrt_ablation}), we find that these estimates can become numerically unstable during the sharpness reduction process, as it penalizes Hessian entries close to zero.
This can lead to fatal underestimation of the diagonal Hessian compared to scenarios without sharpness minimization, significantly disrupting training.
We propose a stable Hessian approximation to address these issue in the following sections.


\subsection{Improving Stability} \label{sec:method_stability}

\paragraph{Alleviating divergence.} \label{sec: alleviating divergence}
Approximate second-order methods can yield overly large steps when their diagonal Hessian estimations underestimate the curvature \citep{dauphin2015equilibrated}.
However, this instability seems to be more present under sharpness minimization, presumably due to smaller top Hessian eigenvalue $\lambda_1$ \citep{agarwala2023sam, shin2024critical} yielding smaller estimated diagonal entries on average: 
\begin{equation*}
    \E\left[\frac{1}{d}\sum_{i=1}^d{\widehat{H}_{ii}}\right] 
    = \frac{1}{d}\sum_{i=1}^d{\E[\widehat{H}]_{ii}} 
    = \frac{\operatorname{tr}(H)}{d} 
    = \frac{1}{d}\sum_{i=1}^d{\lambda_i} 
    \leq \lambda_1.
\end{equation*}

This tendency toward zero intensifies numerical instability during Hessian inversion, increasing the risk of training failures.

Conventional techniques such as damping or clipping can be employed to mitigate this, although their additional hyperparameters require careful tuning.
Instead, we propose square rooting the Hessian (\ie, $|\widehat{H}|^{1/2}$), which effectively mitigates instability, allowing improved generalization performance over other alternatives without additional hyperparameters. 
We present empirical validation of this in \cref{sec:sqrt_ablation} and \cref{app:sqrt_alternatives}.

Its benefits can be understood from two perspectives.
First, the square root smoothly increase the magnitude of the near-zero diagonal Hessian entries in the denominator (\ie, $h<\sqrt{h}$ if $0<h<1$) while damping and clipping either shift the entire Hessian estimate or abruptly replace its certain entries to a predefined constant, potentially leading to performance degradation without careful tuning.
Alternatively, it can be interpreted as a geometric interpolation between the identity matrix and the preconditioning matrix $ H^{\alpha}I^{1-\alpha}$, which has been demonstrated to allow balancing between the bias and the variance of the population risk, thereby improving generalization \citep{amari2021when}.
We specifically adopt $\alpha = 1 / 2$ (i.e., square root), as it has consistently demonstrated robust performance across various scenarios \citep{amari2021when, kingma2014adam}.

\paragraph{Absolute Hessian scaling}
In neural network training, the computed Hessian often contains negative entries.
However, since the square-root operation we introduced applies only to positive values, these entries must first be transformed.
To tackle this, we attend to the prior works of \citet{becker1988improving, adahessian} and employ the absolute function to adjust the negative entries of the diagonal Hessian to be positive, \ie
\begin{equation}
    \lvert \widehat{H} \rvert := \sum_{i=1}^d \lvert \widehat{H}_{ii} \rvert \mathbf{e}_i\mathbf{e}_i^\top
\end{equation}
where $\widehat{H}_{ii}$ and $\mathbf{e}_i$ are the $i^{\text{th}}$ diagonal entry of the approximate diagonal Hessian and the $i^{\text{th}}$ standard basis vector, respectively.
Importantly, this also preserve the same optimal rescaling as Newtonâ€™s method, which can provide a more effective second-order step compared to alternatives like clipping \citep{nocedal1999numerical, murry2010newton, dauphin2014identifying, wang2013eigenvalue}. 
Additionally, this transformation mitigates the risk of convergence to critical points such as saddle or local maxima.
We empirically validate the effectiveness of this approach in \cref{app:samsophia} and \cref{sec:abs_ablation}.

\subsection{Improving Efficiency via Lazy Hessian Update} \label{sec:lazy_hessian}
While the diagonal Hessian approximation can significantly reduce computations, it still requires at least twice as much backpropagation compared to first-order methods.
Here we attempt to further alleviate this by lazily computing the Hessian every $k$ steps:
{\small
\begin{equation*}\label{eq:lazy_update}
    D_t = \begin{cases}
    \beta_2 D_{t-1} + (1-\beta_2) \lvert \widehat{H} (x_t+\epsilon^\star_t) \rvert & \texttt{if } t \  \operatorname{mod} \  k = 1 \\ 
    D_{t-1} & \texttt{otherwise} 
    \end{cases},
\end{equation*}
}
where $D_t$ and $\beta_2$ are the moving average of the Hessian and its hyperparameter, respectively.
This reduces the overhead from additional Hessian computation by $1/k$.
We set $k=10$ for all experiments in this work unless stated otherwise.

However, extensive Hessian reusing will lead to significant performance degradation since it would no longer accurately reflect the current curvature \citep{lazyhessian}. 
Interestingly, \sassha is quite resilient against prolonged reusing, keeping its performance relatively high over longer Hessian reusing compared to other approximate second-order methods.
Our investigation reveals that along the trajectory of \sassha, the Hessian tends to change less frequently than existing alternatives.
We hypothesize that the introduction of sharpness minimization plays an integral role in this phenomenon by biasing the optimization path toward regions with lower curvature change, allowing the prior Hessian to remain relevant over more extended steps. 
We provide a detailed analysis of the lazy Hessian updates in \cref{sec:emp_lazy_hess}.

\subsection{Algorithm}
The exact steps of \sassha is outlined in \cref{algo:sassha}.
We also compare \sassha with other adaptive and second-order methods in detail in \cref{app:comparison}, where one can see the exact differences between these sophisticated methods.

\begin{algorithm}[t!]
    \small
    \caption{\sassha algorithm}
    \begin{algorithmic}[1]
          \STATE {\bf Input:} Initial parameter $x_0$, learning rate $\{\eta_\mathnormal{t}\}$,  moving average parameters $\beta_{1},\beta_{2}$, Hessian update interval $k$, weight decay parameter $\lambda$
          \STATE Set $\mathnormal{m}_{-1} = 0$, $D_{-1} = 0$
          \FOR{$\mathnormal{t}=1$ {\bf to} $T$}
            %\STATE Compute minibatch loss $f_\mathcal{B}(x_t)$
            %\IF{\sassha}
            \STATE $\mathnormal{g}_\mathnormal{t} = \nabla f_\mathcal{B}(x_t)$
            \STATE $\epsilon^\star_t=\rho g_t/\|g_t\|_2$  
            % \ELSIF{\msassha}
            %     \STATE $\epsilon^\star_t = \rho m_{t-1}/\|m_{t-1}\|_2$  
            %\ENDIF            
            \STATE $\Tilde{g}_{t} = \nabla f_\mathcal{B}(x_t + \epsilon^\star_t)$
            \STATE  $m_t = \beta_{1} m_{t-1} + (1 - \beta_{1}) \Tilde{g}_{t}$ 
            \STATE $\overline{m}_t = m_t / (1 - \beta_1^t)$ 
            
            %\STATE
            \IF{$t \operatorname{mod} k = 1$}
                \STATE $\Tilde{H}_t = \widehat{H}(x_t + \epsilon^\star_t)$ \hfill$\triangleright$ \cref{sec:psol} 
                \STATE $D_\mathnormal{t} = \beta_2 D_{t-1} + (1 - \beta_2) |\Tilde{H}_t| $ 
                \STATE $\overline{D}_{t} = \sqrt{D_t/(1 - \beta_2^t)}$ \hfill $\triangleright$ 
                \cref{sec:method_stability}
            \ELSE
                \STATE $\overline{D}_t=\overline{D}_{t-1}$ \hfill$\triangleright$ \cref{sec:lazy_hessian}
            \ENDIF
            \STATE $x_{t+1} = x_t - \eta_{t} \overline{D}_{t}^{-1} \overline{m}_t - \eta_t \lambda x_t$
        \ENDFOR
    \end{algorithmic}
    \label{algo:sassha}
\end{algorithm}

\subsection{Convergence Analysis}
In this section, we present a standard convergence analysis of \sassha under the following assumptions.
\begin{assumption} \label{method_assumption:lowbound}
    The function $ f $ is bounded from below, i.e., \( f^* := \inf_x f(x) > -\infty \).
\end{assumption}
\begin{assumption} \label{method_assumption:smooth}
    The function $ f $ is twice differentiable, convex, and $\beta$-smooth. That is, $0\preceq \nabla^2f \preceq \beta.$
\end{assumption}
\begin{assumption}    
    \label{method_assumption:bounded_steps}
    \textit{The gradient \( \nabla f(x_t) \) is nonzero for a finite number of iterations, i.e., \( \nabla f(x_t) \neq 0 \) for all \( t \in \{1, 2, \dots, n\} \).}
\end{assumption}
Under these assumptions, we derive a descent inequality for $f(x_t)$ by leveraging Adam-like proof techniques from \citet{li2023convergence} to handle the diagonal Hessian and employing smoothness-based bounds to account for the perturbation step based on analyses of \citet{khanh2024fundamental}.
Now we give the convergence results as follows:
\begin{theorem}
    \label{thm:method_theorem}
    Under Assumptions \ref{method_assumption:lowbound}-\ref{method_assumption:bounded_steps}, given any initial point $x_0 \in \mathbb{R}^d$, let $\{x_t\}$ be generated by the update rule \sassha \cref{remark:sassha_iteration_adamlike} with step sizes \( \eta_t \) and perturbation radii \( \rho_t \) satisfying  
    $
        \sum_{t=1}^{\infty} \eta_t = \infty, \quad \sum_{t=1}^{\infty} \eta_t^2 < \infty, \quad \sum_{t=1}^{\infty} \rho_t^2 \eta_t < \infty
    $. 
    Then, we have $\liminf_{t \to \infty} \|\nabla f(x_t)\| = 0$.
\end{theorem}
This preliminary result indicates that any limit point of \sassha is a stationary point of $f$, ensuring progress towards optimal solutions.
We refer to \cref{app:convergence} for the full proof details.
