\section{Further Analysis}

\label{sec:ablation}

\begin{table}[t!]
    \centering
    %\vspace{-0.5em}
    \bgroup
    \def\arraystretch{1.2}
    \caption{
    Comparison between \sassha and SAM with more training budgets for the ViT-s-32 / ImageNet workload.
    }
    \label{tab:sam}
    \vskip 0.1in
    \resizebox{0.7\linewidth}{!}{
        \centering
        \begin{tabular}{lccc}  % 5 more
        \toprule
        &\multicolumn{1}{c}{ Epoch } & Time (\texttt{s}) &\multicolumn{1}{c}{ Accuracy (\%) } \\ \midrule
        SAM$_\text{ SGD}$      &  180  & 220,852 & $  65.403 _{\textcolor{black!60}{\pm 0.63}}$ \\
        SAM$_\text{ AdamW}$    &  180 & 234,374 & $68.706 _{\textcolor{black!60}{\pm 0.16}}$ \\
        \midrule
        \rowcolor{green!20}\sassha        &  \textbf{90}  & \textbf{123,948} &  $\textbf{69.195} _{\textcolor{black!60}{\pm 0.30} } $  \\
        \bottomrule
        \end{tabular}
    }
    \egroup
    \vspace{-1.5em}
\end{table}

\subsection{Robustness}
\label{sec:robustness}

Noisily labeled training data can critically degrade generalization performance \citep{natarajan2013learning}.
To evaluate how \sassha generalizes under these practical conditions, we randomly corrupt certain fractions of the training data and compare the validation performances between different methods.
The results show that \sassha outperforms other methods across all noise levels with minimal accuracy degradation (\cref{tab:noise_label}).
Additionally, we also observe the same trend on CIFAR-10 (\cref{tab:noise_label_sassha}).

Interestingly, \sassha surpasses SAM \citep{sam}, which is known to be one of the most robust techniques against label noise \citep{baek2024why}. 
We hypothesize that its robustness stems from the complementary benefits of the sharpness-minimization scheme and second-order methods.
Specifically, SAM enhances robustness by adversarially perturbing the parameters and giving more importance to clean data during optimization, making the model more resistant to label noise \citep{sam, baek2024why}.
Also, recent research indicates that second-order methods are robust to label noise due to preconditioning that reduces the variance in the population risk \citep{amari2021when}.

\begin{table}[t!]
    \centering
    %\vspace{-0.5em}
    \caption{
    Validation accuracy measured for ResNet-32/CIFAR-100 at different levels of noise.
    \sassha shows the best robustness.
    }
    \label{tab:noise_label}
    \vskip 0.1in
    \resizebox{0.92\linewidth}{!}{%
    \begin{tabular}{lccccc}
        \toprule
        & \multicolumn{4}{c}{Noise level} \\ 
        \cmidrule(l{3pt}r{3pt}){2-5}  
        Method & {0\%} & {20\%} & {40\%} & {60\%} \\ 
        \midrule
        SGD                 &
        $69.32_{\textcolor{black!60}{\pm 0.19}}$
        & $62.18_{\textcolor{black!60}{\pm 0.06}}$ 
        & $55.78_{\textcolor{black!60}{\pm 0.55}}$  
        & $45.53_{\textcolor{black!60}{\pm 0.78}}$ \\ 
        
        SAM $_{\text{SGD}}$ & 
        $71.99_{\textcolor{black!60}{\pm 0.20}}$
        & $65.53_{\textcolor{black!60}{\pm 0.11}}$  
        & $ 61.20_{\textcolor{black!60}{\pm 0.17}}$  
        & $ 51.93_{\textcolor{black!60}{\pm 0.47}}$ \\ 
        
        AdaHessian         &
        $68.06_{\textcolor{black!60}{\pm 0.22}}$
        & $63.06_{\textcolor{black!60}{\pm 0.25}}$  
        & $58.37_{\textcolor{black!60}{\pm 0.13}}$  
        & $46.02_{\textcolor{black!60}{\pm 1.96}}$  \\

        Sophia-H           &
        $67.76_{\textcolor{black!60}{\pm 0.37}}$
        & $62.34_{\textcolor{black!60}{\pm 0.47}}$  
        & $56.54_{\textcolor{black!60}{\pm 0.28}}$  
        & $45.37_{\textcolor{black!60}{\pm 0.27}}$  \\
        
        Shampoo           & 
        $64.08_{\textcolor{black!60}{\pm 0.46}}$
        & $58.85_{\textcolor{black!60}{\pm 0.66}}$ & $ 53.82 _{\textcolor{black!60}{\pm 0.71}}$  
        & $ 42.91_{\textcolor{black!60}{\pm 0.99}}$ \\
        
        \midrule
        
        \rowcolor{green!20} \sassha         & 
        $ \textbf{72.14}_{\textcolor{black!60}{\pm 0.16}}    $
        & $\textbf{66.78}_{\textcolor{black!60}{\pm 0.47}}   $  
        & $ \textbf{ 61.97}_{\textcolor{black!60}{\pm 0.27}} $  
        & $\textbf{ 53.98}_{\textcolor{black!60}{\pm 0.57}}  $ \\
        
        % \rowcolor{green!20} \msassha        &  
        % $ 70.93 _{\textcolor{black!60}{\pm 0.26}}     $
        % $ 66.10 _{\textcolor{black!60}{\pm 0.26}}     $  
        % &  $ 61.13 _{\textcolor{black!60}{\pm 0.28}}  $  
        % &  $ 52.45 _{\textcolor{black!60}{\pm 0.34}}  $ \\   
        \bottomrule  
    \end{tabular}}
    %\vspace{-1em}
\end{table}

\begin{figure}[t!]
%\vspace{-2em}
\resizebox{\linewidth}{!}{%
    \centering
    \begin{subfigure}{0.4\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ablation/loss.pdf}
        \caption{Train loss}
        \label{fig:sqrt_ablation_train_loss}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \centering
        \includegraphics[width=\linewidth,trim={0.5em 0.5em 1em 0},clip]{figures/ablation/Update_size.pdf}
        \caption{Update size}
        \label{fig:sqrt_ablation_update-size}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.4\linewidth}
        \centering
        \includegraphics[width=\linewidth,trim={0 1em 0 1.2em},clip]{figures/ablation/sqrt_ridgeline.pdf}\\
        
        \caption{$D$ distribution}
        \label{fig:sqrt_ablation_ridgeline-plot}
    \end{subfigure}
    }
    \caption{
    Effects of square-root measured for ResNet-32/CIFAR-100;
    $D$ is set to be either $|\widehat{H}|^{1/2}$ for \sassha or $|\widehat{H}|$ for \texttt{No-Sqrt}.
    Sharpness minimization drives the diagonal Hessian entries move towards zero, causing divergence.
    The square-root in \sassha helps counteract this effect, stabilizing the training process.
    }
    \label{fig:sqrt_ablation}
    %\vspace{-1em}
\end{figure}

\subsection{Stability} \label{sec:sqrt_ablation}

To show the effect of the square-root function on stabilizing the training process, we run \sassha without the square-root (\texttt{No-Sqrt}), repeatedly for multiple times with different random seeds.
As a result, we find that the training diverges most of the time.
A failure case is depicted in \cref{fig:sqrt_ablation}.

At first, we find that the level of training loss for \texttt{No-Sqrt} is much higher than that of \sassha, and also, it spikes up around step $200$ (\cref{fig:sqrt_ablation_train_loss}).
To look into it further, we also measure the update sizes along the trajectory (\cref{fig:sqrt_ablation_update-size}).
The results show that it matches well with the loss curves, suggesting that the training failure is somehow due to taking too large steps.

It turns out that this problem stems from the preconditioning matrix $D$ being too small;
\ie, the distribution of diagonal entries in the preconditioning matrix gradually shifts toward zero values (\cref{fig:sqrt_ablation_ridgeline-plot});
as a result, $D^{-1}$ becomes too large, creating large steps.
This progressive increase in near-zero diagonal Hessian entries is precisely due to the sharpness minimization scheme that we introduced; it penalizes the Hessian eigenspectrum to yield flat solutions, yet it could also make training unstable if taken naively.
By including square-root, the preconditioner are less situated near zero, effectively suppressing the risk of large updates, thereby stabilizing the training process.
We validate this further by showing its superiority to other alternatives including damping and clipping in \cref{app:sqrt_alternatives}.

We also provide an ablation analysis for the absolute-value function in \cref{sec:abs_ablation}, which demonstrates that it increases the stability of \sassha in tandem with square-root.

\begin{figure}[t!] 
    \centering
    \begin{minipage}{\linewidth}
        \centering
        \hspace{1.2em}
        \includegraphics[width=0.9\linewidth, trim={0 0 0 0},clip]{figures/ablation/legend_only.pdf} % Path to your legend image
        \vspace{-0.6em}
    \end{minipage}

    \resizebox{\linewidth}{!}{
        \begin{subfigure}{0.288\linewidth}
        \centering%
            \includegraphics[width=\textwidth,trim={0 -1em 1.2em 0.5em},clip]{figures/ablation/hessian_update_frequency.pdf}
            \caption{Lazy Hessian}
            \label{fig:lazy_results}
        \end{subfigure}
        % \hspace{2.3em} 
        \begin{subfigure}{0.305\linewidth}
            \centering
            \includegraphics[width=\textwidth,trim={2em 1.4em 0 0.1em},clip]{figures/ablation/hess_diff.pdf}
            \caption{$\widehat{H}$ change}
            \label{fig:lazy_hess_diff}
        \end{subfigure}%
        \begin{subfigure}{0.32\linewidth}
            \centering
            \includegraphics[width=\linewidth,trim={0 -0.7em 0 0.3em},clip]{figures/ablation/perturbed.pdf}
            \caption{Local sensitivity}
            \label{fig:lazy_perturbed}
        \end{subfigure}
    }

    \caption{
    Effect of lazy Hessian for ResNet-32/CIFAR-100.
    \sassha stays within the region where the Hessian varies small.
    }
    \label{fig:diagonal_hessian_comp} %
    %\vspace{-1em}
\end{figure}

\subsection{Efficiency} \label{sec:emp_lazy_hess}

Here we show the effectiveness of lazy Hessian updates in \sassha.
The results are shown in \cref{fig:diagonal_hessian_comp}.
At first, we see that \sassha maintains its performance even at $k=100$, indicating that it is extremely robust to lazy Hessian updates (\cref{fig:lazy_results}).
We also measure the difference between the current and previous Hessians to validate lazy Hessian updates more directly (\cref{fig:lazy_hess_diff}).
The result shows that \sassha keeps the changes in Hessian to be small, and much smaller than other methods, indicating its advantage of robust reuse, and hence, computational efficiency.

We attribute this robustness to the sharpness minimization scheme incorporated in \sassha, which can potentially bias optimization toward the region of low curvature sensitivity.
To verify, we define local Hessian sensitivity as follows:
\begin{equation}\label{eq:diff_hessian(2)}
    \max_{\delta \sim \mathcal{N}(0, 1)}\left\|\widehat{H}\left(x+\rho\frac{\delta}{\|\delta\|_2}\right) - \widehat{H}(x)\right\|_F
\end{equation}
\ie, it measures the maximum change in Hessian induced from normalized random perturbations.
A smaller Hessian sensitivity would suggest reduced variability in the loss curvature, leading to greater relevance of the current Hessian for subsequent optimization steps.
We find that \sassha is far less sensitive compared to other methods (\cref{fig:lazy_perturbed}).

\subsection{Cost}
\label{sec:cost}

Second-order methods can be highly costly.
In this section, we discuss the computational cost of \sassha and reveal its competitiveness to other methods.

\sassha requires one gradient computation (\texttt{GC}) in the sharpness minimization step, one Hessian-vector product (\texttt{HVP}) for diagonal Hessian computation, and an additional \texttt{GC} in the descent step.
That is, a total of $2$\texttt{GC}s and $1$\texttt{HVP} are required.
However, with lazy Hessian updates, the number of \texttt{HVP}s reduces drastically to $ 1 / k $.
With $ k = 10 $ as the default value used in this work, this scales down to $0.1$\texttt{HVP}s.

It turns out that this is critical to the utility of \sassha, because $1$\texttt{HVP} is known to take about $ \times 3 $ the computation time of $1$\texttt{GC} in practice \citep{dagrou2024how}.
Compared to conventional second-order methods ($1$\texttt{GC} $+$ $1$\texttt{HVP} $\simeq$ $4$\texttt{GC}s), the cost of \sassha can roughly be a half of that ($2.3$\texttt{GC}s).
It is also comparable to standard SAM variants ($2$\texttt{GC}s).

Furthermore, we can leverage a momentum of gradients in the perturbation step to reduce the cost.
This variant \msassha requires only $1.3$\texttt{GC}s with minimal decrease in performance.
Notably, \msassha still outperforms standard first-order methods like SGD and AdamW (\cref{app:msassha}).

To verify, we measure the average wall-clock times and present the results in \cref{tab:costs}.
First, one can see that the theoretical cost is reflected well on the actual cost;
\ie, the time measurements scales proportionally roughly well with respect to the total cost.
More importantly, this result indicates the potential of \sassha for performance-critical applications.
Considering its well-balanced cost, and that it has been challenging to employ second-order methods efficiently for large-scale tasks without sacrificing performance, \sassha can be a reasonable addition to the lineup.

\begin{table}[t!]
    \vspace{-0.5em}
    \centering
    \caption{
    Average wall-clock time per epoch (\texttt{s}) and the theoretical cost of different methods.
    \sassha can be an effective alternative to existing methods for its enhanced generalization performance.
    } 
    \vskip 0.1in
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{l|r|r|r|r|r|r|r}
    \toprule
    \multirow{2.5}{*}{Method}
    & \multicolumn{4}{c|}{Cost}
    & \multicolumn{1}{c|}{CIFAR10} 
    & \multicolumn{1}{c|}{CIFAR100} 
    & \multicolumn{1}{c}{ImageNet} \\ 
    \cmidrule(l{3pt}r{3pt}){2-5}
    \cmidrule(l{3pt}r{3pt}){6-8} 
     & \multicolumn{1}{c|}{Descent}
     & \multicolumn{1}{c|}{Sharpness}
     & \multicolumn{1}{c|}{Hessian}
     & \multicolumn{1}{c|}{Total}
     & \multicolumn{1}{c|}{ResNet32} 
     & \multicolumn{1}{c|}{WRN28-10}  
     & \multicolumn{1}{c}{ViT-small} \\ 
    \midrule
    AdamW  & 1 \texttt{GC}
    & 0 \texttt{GC}
    & 0 \texttt{HVP}
    & 1 \texttt{GC} & 5.03 & 59.29 & 976.56 \\
    
    SAM    & 1 \texttt{GC}
    & 1 \texttt{GC}
    & 0 \texttt{HVP}
    & 2 \texttt{GC} & 9.16 & 118.46  &  1302.08\\ 
    
    AdaHessian &  1 \texttt{GC}
    & 0 \texttt{GC}
    & 1 \texttt{HVP}
    & 4 \texttt{GC} & 33.75 & 296.63  & 2489.07 \\ \midrule
    
    \rowcolor{green!20} \sassha & 1 \texttt{GC}
    & 1 \texttt{GC}
    & 0.1 \texttt{HVP}
    & 2.3 \texttt{GC} & 12.00   & 142.06   & 1377.20 \\ 
    
    \rowcolor{green!20} \msassha & 1 \texttt{GC}
    & 0 \texttt{GC}
    & 0.1 \texttt{HVP}
    & 1.3 \texttt{GC}  & 8.91   & 84.12 & 1065.40 \\ 
    \bottomrule
    \end{tabular}
    }
    \vspace{-1em}
    \label{tab:costs}
\end{table}