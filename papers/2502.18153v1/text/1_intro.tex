\section{Introduction}\label{sec:intro}

Approximate second-order methods have recently gained a surge of interest due to their potential to accelerate the large-scale training process with minimal computational and memory overhead \citep{adahessian, sophia, gupta2018shampoo}.
However, studies also suggest that these methods may undermine generalization, trying to identify underlying factors behind this loss \citep{wilson2017marginal, zhou2020towards, zou2022understanding}.
For instance, \citet{amari2021when} shows that preconditioning hinders achieving the optimal bias for population risk, and \citet{wadia2021whitening} points to negative effect of whitening data.

While the precise understanding is still under investigation, many studies have suggested a strong correlation between the flatness of minima and their generalization capabilities \citep{keskar2016large}, spurring the development of optimization techniques aimed at inducing flat minima \citep{chaudhari2017entropy, izmailov2018averaging, sam, antipgd_orvieto22a}.
Inspired by this, we raise an important question in this work:
what type of minima do second-order methods converge to, and is there any potential for improving their generalization performance based on that?

\begin{figure}
    \centering
    \resizebox{0.6\linewidth}{!}{
        \hspace{-2em}
        \includegraphics[width=1.3\linewidth,trim={0 6em 0 0},clip]{figures/ablation/motivate.pdf}
    }
    \caption{
        Motivating toy example (a mixture of bivariate Gaussian densities).
        \sassha converges to a flat minimum unlike others.
    }
    \label{fig:motivate}
    %\vspace{-1em}
    
\end{figure}

To answer these questions, we first measure the sharpness of different second-order methods using diverse metrics, suggesting that they converge to significantly sharper minima compared to stochastic gradient descent (SGD).
Then, we propose \sassha---\underline{\textbf{S}}harpness-aware \underline{\textbf{A}}daptive \underline{\textbf{S}}econd-order optimization with \underline{\textbf{S}}table \underline{\textbf{H}}essian \underline{\textbf{A}}pproximation---designed to enhance the generalization of approximate second-order methods by explicitly reducing sharpness (see \cref{fig:motivate} for the basic results).

\sassha incorporates a sharpness minimization scheme similar to SAM \citep{sam} into the second-order optimization framework, in which the Hessian diagonal is estimated.
Such estimates, however, can become numerically unstable when enforcing the sharpness reduction process.
To increase stability while preserving the benefits of reduced sharpness, we make a series of well-engineered design choices based on principles studied in the literature.
This not only smoothly adjusts underestimated curvature, but also enables efficient reuse of previously computed Hessians, resulting in a stable and efficient algorithm.

We extensively evaluate the effectiveness of \sassha across diverse vision and natural language tasks.
Our results reveal that \sassha consistently achieves flatter minima and attains stronger generalization performance, all compared to existing practical second-order methods, and interestingly, to first-order methods including SGD, AdamW, and SAM.
Furthermore, we provide an array of additional analyses to comprehensively study \sassha including convergence, robustness, stability, efficiency, and cost.

\bgroup
% \def\arraystretch{1.1}
\begin{table*}[!t]
    %\vspace{-0.5em}
    \centering
    \caption{
    Sharpness measurements of the solutions found by different optimizers and their generalization for ResNet-32 on CIFAR-100.
    Approximate second-order methods tend to yield highly sharp solutions and poor generalization compared to \sassha.
    We provide more results for other workloads in \cref{app:sharp} where the same trend holds.}
    %
    \vskip 0.1in
    \resizebox{0.85\linewidth}{!}{
        \begin{tabular}{lcccccccc}%{cc*{2}{S[table-format=1]}cccccc}
            \toprule
             &  \multicolumn{4}{c}{Sharpness} & \multicolumn{2}{c}{Generalization} \\
             \cmidrule(l{3pt}r{3pt}){2-5} \cmidrule(l{3pt}r{3pt}){6-7}
             & {$\lambda_{max}(H)$}              & {$\operatorname{tr}(H)_{\times 10^3}$} & $\delta L_\text{grad}$
             &  $\delta L_{\text{avg}\times 10^{-3}}$ & $L_{\text{val}}$ & $\text{Acc}_\text{val}$ (\%) \\ \midrule
             SGD        &
             $ 265 _{\textcolor{black!60}{\pm 25} } $           & 
             $ 7.29 _{\textcolor{black!60}{\pm 0.30} } $    &
             $ 0.703 _{\textcolor{black!60}{\pm 0.132} } $ &
             $ 1.31 _{\textcolor{black!60}{\pm 1.03} } $ &
             $ 1.260 _{\textcolor{black!60}{\pm 0.001} } $ &
             $ 69.32 _{\textcolor{black!60}{\pm 0.19} } $ \\
             
             Sophia-H    & 
             $ 22797 _{\textcolor{black!60}{\pm 10857} } $      & 
             $ 68.15 _{\textcolor{black!60}{\pm 20.19} } $ &
             $ 8.130 _{\textcolor{black!60}{\pm 3.082} } $ &
             $ 19.19 _{\textcolor{black!60}{\pm 6.38} } $ &
             $ 1.463 _{\textcolor{black!60}{\pm 0.022} } $ & $ 67.76 _{\textcolor{black!60}{\pm 0.37} } $ \\

             AdaHessian  & 
             $ 11992 _{\textcolor{black!60}{\pm 5779} } $       &
             $ 46.94 _{\textcolor{black!60}{\pm 17.60} } $ &
             $ 4.119 _{\textcolor{black!60}{\pm 1.136} } $ &
             $ 12.50 _{\textcolor{black!60}{\pm 6.08} } $ &
             $ 1.377 _{\textcolor{black!60}{\pm 0.070} } $ &
             $ 68.06 _{\textcolor{black!60}{\pm 0.22} } $ \\
             
             Shampoo & 
             $ 436374 _{\textcolor{black!60}{\pm 9017}} $ &
             $ 6823.34 _{ \textcolor{black!60}{\pm 664.65}}$ &
             $73.27 _{\textcolor{black!60}{\pm 12.51}} $ &
             $ 49307489 _{\textcolor{black!60}{\pm 56979794}} $ &
             $1.386 _{\textcolor{black!60}{\pm 0.010}}$ &
             $ 64.08 _{\textcolor{black!60}{\pm 0.46}} $ \\
             
             \midrule
             
            \rowcolor{green!20} \sassha                            &
            $ \textbf{107} _{\textcolor{black!60}{\pm 40} } $      &
            $ \textbf{1.87} _{\textcolor{black!60}{\pm 0.65} } $   &
            $ \textbf{0.238} _{\textcolor{black!60}{\pm 0.088} } $ & 
            $ \textbf{0.65} _{\textcolor{black!60}{\pm 0.86} } $   &
            $ \textbf{0.961} _{\textcolor{black!60}{\pm 0.005} }$  &
            $ \textbf{72.14} _{\textcolor{black!60}{\pm 0.16} } $\\ 
            \bottomrule
        \end{tabular}
    }
    \vskip 0.1in
    \label{tab:sharp}
\end{table*}
\egroup
