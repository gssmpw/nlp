\section{Practical Second-order Optimizers Converge to Sharp Minima}

In this section, we investigate the sharpness of minima obtained by approximate second-order methods and their generalization properties.
We posit that poor generalization of second-order methods reported in the literature \citep{amari2021when, wadia2021whitening} can potentially be attributed to sharpness of their solutions.

We employ four metrics frequently used in the literature: maximum eigenvalue of the Hessian, the trace of Hessian, gradient-direction sharpness, and average sharpness \citep{Hochreiter1997, jastrzkebski2018relation, xie2020diffusion, saf, chenvision}.
The first two, denoted as $\lambda_{\max}(H)$ and $\operatorname{tr}(H)$, are often used as standard mathematical measures for the worst-case and the average curvature computed using the power iteration method and the Hutchinson trace estimation, respectively.
The other two measures, $\delta L_\text{grad}$ and $\delta L_\text{avg}$, assess sharpness based on the loss difference under perturbations.
$\delta L_\text{grad}$ evaluates sharpness in the gradient direction and is computed as $L(x^\star+\rho\nabla L(x^\star)/\|\nabla L(x^\star)\|)-L(x^\star)$.
$\delta L_\text{avg}$ computes the average loss difference over Gaussian random perturbations, expressed as
$\E_{z\sim \mathcal{N}(0, 1)}[L(x^\star+\rho z/\|z\|)-L(x^\star)]$.
Here we choose $\rho=0.1$ for the scale of the perturbation.

With these, we measure the sharpness of the minima found by three approximate second-order methods designed for deep learning; Sophia-H \citep{sophia}, AdaHessian \citep{adahessian}, and Shampoo \citep{gupta2018shampoo}, and compare them with \sassha as well as SGD for reference.
We also compute the validation loss and accuracy to see any correlation between sharpness and generalization of these solutions.
The results are presented in \cref{tab:sharp}.

We observe that existing second-order optimizers produce solutions with significantly higher sharpness compared to \sassha in all sharpness metrics, which also correlates well with their generalization.
We also provide a visualization of the loss landscape for the found solutions, where we find that the solutions obtained by second-order methods are indeed much sharper than that of \sassha (\cref{fig:landscape}).
