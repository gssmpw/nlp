\section{Sharpness Measurements for Other Settings} \label{app:sharp}

\bgroup
% \def\arraystretch{1.1}
\begin{table}[ht!]
    \centering
    \caption{Sharpness measurements of the solutions found by seven different optimizers and their generalization on CIFAR-10/100 and Wikitext-2. Approximate second-order methods tend to yield highly sharp solutions and poor generalization compared to SGD; \sassha and \msassha effectively recover this. Here, we measure sharpness in terms of maximum Hessian eigenvalue $\lambda_{max}(H)$, trace of Hessian $\operatorname{tr}(H)$, worst-case sharpness $\delta L_{\text{worst}}$, and average sharpness $\delta L_{\text{avg}}$, along with generalization using validation loss $L_{val}$ and accuracy $\text{Acc}_{val}$.}
    %
    \vskip 0.1in
    \resizebox{\linewidth}{!}{
        \begin{tabular}{clcccccccc}%{cc*{2}{S[table-format=1]}cccccc}
            \toprule
             & &  \multicolumn{4}{c}{Sharpness} & \multicolumn{2}{c}{Generalization} \\
             \cmidrule(l{3pt}r{3pt}){3-6} \cmidrule(l{3pt}r{3pt}){7-8}    &    &
             {$\lambda_{max}(H)$}                              &
             {$\operatorname{tr}(H)_{\times 10^3}$}            &
             $\delta L_{\text{worst}}$                         &
             $\delta L_{\text{avg} \times 10^{-3}}$            &
             $ L_\text{val} $                                  &
             $\text{Acc}_\text{val}$                          \\ 
             
             \midrule
             
             \rowcolor{lgray} \multicolumn{8}{c}{CIFAR-10} 
             
             \\ \midrule
             
             \multirow{7}{*}{ResNet20}
             
             & SGD        &
             $ 107_{\textcolor{black!60}{\pm 4.37} } $          &
             $ 1.38_{\textcolor{black!60}{\pm 0.01} } $         &
             $ 0.840_{\textcolor{black!60}{\pm 0.304}} $        & 
             $ 0.690_{\textcolor{black!60}{\pm 0.39}} $         &
             $ 0.295 _{\textcolor{black!60}{\pm 0.008}} $       &
             $ 92.03_{\textcolor{black!60}{\pm 0.32} } $        \\
             
             & SAM         &  
             $ 58_{\textcolor{black!60}{\pm 2.98}}  $ &
             $ 0.73_{\textcolor{black!60}{\pm 0.04}}$    &
             $0.171 _{\textcolor{black!60}{\pm 0.038}}$  & 
             $ 0.461 _{\textcolor{black!60}{\pm 0.24}} $  & 
             $ 0.119_{\textcolor{black!60}{\pm 0.002}} $     & 
             $ 92.85_{\textcolor{black!60}{\pm 0.07} } $ \\
             
             & Sophia-H    & 
             $ 3606 _{\textcolor{black!60}{\pm 303} } $        &
             $ 31.24 _{\textcolor{black!60}{\pm 2.628} } $               &
             $ 6.120 _{\textcolor{black!60}{\pm 1.634}} $ &
             $ 18.11 _{\textcolor{black!60}{\pm 1}} $ &
             $  0.316 _{\textcolor{black!60}{\pm 0.002}} $ &
             $ 91.81_{\textcolor{black!60}{\pm 0.27} } $ \\
             
             & AdaHessian  & 
             $ 23048_{\textcolor{black!60}{\pm 29932}} $      &
             $  189.48_{\textcolor{black!60}{\pm 240.55} } $            &
             $  4.538 _{\textcolor{black!60}{\pm 1.634}}$  &
             $ 198.66 _{\textcolor{black!60}{\pm 266}}$ & 
             $ 0.260 _{\textcolor{black!60}{\pm 0.006}}  $ & 
             $ 92.00_{\textcolor{black!60}{\pm 0.17} } $ \\
             
             & Shampoo     & 
             $ 647066 _{\textcolor{black!60}{\pm 419964}} $ & 
             $ 3899.5_{\textcolor{black!60}{\pm 1825}}$  & 
             $ 166.3 _{\textcolor{black!60}{\pm 48.00}} $ &
             $ 2177189 _{\textcolor{black!60}{\pm 1628993}} $ &
             $ 0.381 _{\textcolor{black!60}{\pm 0.028}} $ &
             $ 88.55 _{\textcolor{black!60}{\pm 0.83}} $ \\
             
             \rowcolor{green!20}\cellcolor{white} &  
             \msassha &
             $  129_{\textcolor{black!60}{\pm 17} } $         &
             $  1.58 _{\textcolor{black!60}{\pm 0.08} } $               &
             $ 1.551 _{\textcolor{black!60}{\pm 0.684}} $  &
             $  1.025 _{\textcolor{black!60}{\pm 0.36}} $ &
             $ 0.234 _{\textcolor{black!60}{\pm 0.003}} $ &
             $ 92.36_{\textcolor{black!60}{\pm 0.23} } $ \\


            \rowcolor{green!20}\cellcolor{white} &
             \sassha  &
             $  78_{\textcolor{black!60}{\pm 5.09}} $          &
             $  0.86 _{\textcolor{black!60}{\pm 0.03} } $                 & 
             $ 0.184 _{\textcolor{black!60}{\pm 0.053}}$ &
             $0.388 _{\textcolor{black!60}{\pm 0.704}}  $  &
             $ 0.209 _{\textcolor{black!60}{\pm 0.001}}  $ &
             $ 92.98_{\textcolor{black!60}{\pm 0.05} } $ \\ 
             
             \midrule
             
             \multirow{7}{*}{ResNet32}
             & SGD        &
             $56 _{\textcolor{black!60}{\pm 5.10}}  $           &
             $ 0.80 _{\textcolor{black!60}{\pm 0.04}}  $    &
             $  0.560_ {\textcolor{black!60}{\pm 0.219}} $ &
             $  0.196 _{\textcolor{black!60}{\pm 0.146}}  $ &
             $ 0.309 _{\textcolor{black!60}{\pm 0.002}}  $ &
             $ 92.69_{\textcolor{black!60}{\pm 0.06} } $ \\
             
             & SAM        & 
             $ 45 _{\textcolor{black!60}{\pm 2.67}}  $           & 
             $ 0.58 _{\textcolor{black!60}{\pm 0.02}} $           &
             $ 0.107 _{\textcolor{black!60}{\pm 0.005}}  $    &  
             $ 0.753 _{\textcolor{black!60}{\pm 0.351}}  $   & 
             $0.128 _{\textcolor{black!60}{\pm 0.001} }$   & 
             $ 93.89_{\textcolor{black!60}{\pm 0.13} } $ \\
             
             & Sophia-H    &
             $ 7167 _{\textcolor{black!60}{\pm 2755}}  $      & 
             $ 18.82 _{\textcolor{black!60}{\pm 5.50}} $ &
             $  9.399 _{\textcolor{black!60}{\pm 2.283}}   $ & 
             $ 7.915 _{\textcolor{black!60}{\pm 3.397}}  $ & 
             $ 0.394 _{\textcolor{black!60}{\pm 0.010}} $ & 
             $ 91.99_{\textcolor{black!60}{\pm 0.08} } $ \\

             & AdaHessian  &
             $ 1746 _{\textcolor{black!60}{\pm 1018}}  $       & 
             $ 17.06 _{\textcolor{black!60}{\pm 10.24}} $ & 
             $ 4.599 _{\textcolor{black!60}{\pm 1.710}}  $ & 
             $ 5.518 _{\textcolor{black!60}{\pm 3.623}}  $ & 
             $ 0.278 _{\textcolor{black!60}{\pm 0.006}}  $ & 
             $  92.48_{\textcolor{black!60}{\pm 0.15} } $ \\
             
             & Shampoo &
             $ 717553 _{\textcolor{black!60}{\pm 93129} } $ &
             $ 4523 _{\textcolor{black!60}{\pm 629.7}}$ &
             $ 162.1 _{\textcolor{black!60}{\pm 123.2}} $ & 
             $ 105322 _{\textcolor{black!60}{\pm 82246}} $ & 
             $ 0.348 _{\textcolor{black!60}{\pm 0.008}} $ &
             $ 90.23 _{\textcolor{black!60}{\pm 0.24}} $ \\

             \rowcolor{green!20}\cellcolor{white} &
             \msassha & 
             $ 283 _{\textcolor{black!60}{\pm 10}}  $           &
             $ 3.96 _{\textcolor{black!60}{\pm 0.10}}  $    & 
             $ 2.986 _{\textcolor{black!60}{\pm 1.133}}  $ & 
             $ 1.300 _{\textcolor{black!60}{\pm 0.969}}  $ &
             $ 0.211 _{\textcolor{black!60}{\pm 0.010}} $ & 
             $ 93.18_{\textcolor{black!60}{\pm 0.30} } $ \\

            \rowcolor{green!20}\cellcolor{white} &
            \sassha   &
            $ 47 _{\textcolor{black!60}{\pm 1.88}} $  &
            $ 0.59 _{\textcolor{black!60}{\pm 0.02}} $ &
            $ 0.136 _{\textcolor{black!60}{\pm 0.019}} $ &
            $ 0.714 _{\textcolor{black!60}{\pm 0.090}}  $ &
            $ 0.177 _{\textcolor{black!60}{\pm 0.002}}  $ &
            $ 94.09_{\textcolor{black!60}{\pm 0.24} } $  \\ 
            
            \midrule
            
            \rowcolor{lgray} 
            \multicolumn{8}{c}{CIFAR-100} \\ \midrule
            \multirow{7}{*}{ResNet32} &
             
             SGD        &
             $ 265 _{\textcolor{black!60}{\pm 25} } $           & 
             $ 7.29 _{\textcolor{black!60}{\pm 0.30} } $    & 
             $ 0.703 _{\textcolor{black!60}{\pm 0.132} } $ &
             $ 1.31 _{\textcolor{black!60}{\pm 1.03} } $ & 
             $ 1.260 _{\textcolor{black!60}{\pm 0.001} } $ & 
             $ 69.32 _{\textcolor{black!60}{\pm 0.19} } $ \\
             
             &SAM        & 
             $123_{\textcolor{black!60}{\pm 11}}$           & 
             $2.63 _{\textcolor{black!60}{\pm 0.09}}$           &  
             $0.266 _{\textcolor{black!60}{\pm 0.025}} $    &  
             $-0.619 _{\textcolor{black!60}{\pm 0.594}} $   &  
             $0.512 _{\textcolor{black!60}{\pm 0.016}}$    &  
             $71.99 _{\textcolor{black!60}{\pm 0.20}}$ \\
             
             &Sophia-H    & 
             $ 22797 _{\textcolor{black!60}{\pm 10857} } $      &
             $ 68.15 _{\textcolor{black!60}{\pm 20.19} } $ & 
             $ 8.130 _{\textcolor{black!60}{\pm 3.082} } $ &
             $ 19.19 _{\textcolor{black!60}{\pm 6.38} } $ & 
             $ 1.463 _{\textcolor{black!60}{\pm 0.022} } $ & 
             $ 67.76 _{\textcolor{black!60}{\pm 0.37} } $ \\
             
             &AdaHessian  &
             $ 11992 _{\textcolor{black!60}{\pm 5779} } $       & 
             $ 46.94 _{\textcolor{black!60}{\pm 17.60} } $ & 
             $ 4.119 _{\textcolor{black!60}{\pm 1.136} } $ & 
             $ 12.50 _{\textcolor{black!60}{\pm 6.08} } $ & 
             $ 1.377 _{\textcolor{black!60}{\pm 0.070} } $ & 
             $ 68.06 _{\textcolor{black!60}{\pm 0.22} } $ \\
             
             &Shampoo &
             $ 436374 _{\textcolor{black!60}{\pm 9017}} $ &
             $ 6823.34 _{ \textcolor{black!60}{\pm 664.65}}$ &
             $ 73.27 _{\textcolor{black!60}{\pm 12.51}} $ & 
             $ 49307489 _{\textcolor{black!60}{\pm 56979794}} $ & 
             $1.386 _{\textcolor{black!60}{\pm 0.010}}$ & 
             $ 64.08 _{\textcolor{black!60}{\pm 0.46}} $ \\
             
             \rowcolor{green!20}\cellcolor{white} &
             \msassha & 
             $ 382 _{\textcolor{black!60}{\pm 65} } $           & 
             $ 8.75 _{\textcolor{black!60}{\pm 0.31} } $    & 
             $ 2.391 _{\textcolor{black!60}{\pm 0.425} } $ & 
             $ 2.26 _{\textcolor{black!60}{\pm 1.66} } $ & 
             $ 1.067 _{\textcolor{black!60}{\pm 0.001} } $ & 
             $ 70.93 _{\textcolor{black!60}{\pm 0.21} } $ \\
             
            \rowcolor{green!20}\cellcolor{white} &
            \sassha   &
            $ 107 _{\textcolor{black!60}{\pm 40} } $  &
            $ 1.87 _{\textcolor{black!60}{\pm 0.65} } $ &
            $ 0.238 _{\textcolor{black!60}{\pm 0.088} } $ 
            & $ 0.65 _{\textcolor{black!60}{\pm 0.86} } $ &
            $ 0.961 _{\textcolor{black!60}{\pm 0.005} }$ & 
            $ 72.14 _{\textcolor{black!60}{\pm 0.16} } $\\ 
            
            \midrule
            
            \multirow{7}{*}{WRN28-10}
             & SGD        &
             $ 18_{\textcolor{black!60}{\pm 1.17} } $ & 
             $ 0.66 _{\textcolor{black!60}{\pm 0.04} } $        & 
             $  1.984_{\textcolor{black!60}{\pm 0.506} } $ & 
             $ -0.007 _{\textcolor{black!60}{\pm 0.028}} $ & 
             $ 0.820 _{\textcolor{black!60}{\pm 0.005} } $ & 
             $ 80.06 _{\textcolor{black!60}{\pm 0.15} } $ \\
             
             & SAM         &  
             $9_{\textcolor{black!60}{\pm 0.866}}  $ & 
             $0.23 _{\textcolor{black!60}{\pm 0.01}}$   &
             $0.841_{\textcolor{black!60}{\pm 0.084}}$  &
             $0.024_{\textcolor{black!60}{\pm 0.041}} $  &  
             $0.648 _{\textcolor{black!60}{\pm 0.006}}$  & 
             $82.56 _{\textcolor{black!60}{\pm 0.13}}$ \\
             
             & Sophia-H    & 
             $ 3419_{\textcolor{black!60}{\pm 3240} } $        &
             $ 13.57 _{\textcolor{black!60}{\pm 3.30} } $               &
             $ 5.073 _{\textcolor{black!60}{\pm 0.268} } $ & 
             $ 0.067 _{  \pm 0.054}$  &
             $ 0.866 _{\textcolor{black!60}{\pm 0.003} } $ & 
             $ 79.35 _{\textcolor{black!60}{\pm 0.24} } $ \\
             
             & AdaHessian  & 
             $ 35119_{\textcolor{black!60}{\pm 46936} } $      &
             $ 139.53 _{\textcolor{black!60}{\pm 190.98} } $            &
             $ 6.745 _{\textcolor{black!60}{\pm 1.932} } $  & 
             $ 19.727 _{\textcolor{black!60}{\pm 27.866}}$ & 
             $ 1.005 _{\textcolor{black!60}{\pm 0.008} } $ & 
             $ 76.92 _{\textcolor{black!60}{\pm 0.26} } $ \\

             
             & Shampoo     &
             $102129 _{\textcolor{black!60}{\pm 60722}} $   & 
             $1459.09 _{\textcolor{black!60}{\pm 709.42}}$  &
             $483.0 _{\textcolor{black!60}{\pm 172.0}} $ &
             $ 98.558 _{\textcolor{black!60}{\pm 123.082}}$ & 
             $ 1.173 _{\textcolor{black!60}{\pm 0.088} } $ & 
             $ 74.06 _{\textcolor{black!60}{\pm 1.28} } $ \\
             
             \rowcolor{green!20}\cellcolor{white} &
             \msassha &
             $ 2257 _{\textcolor{black!60}{\pm 248} } $         & 
             $ 30.40 _{\textcolor{black!60}{\pm 4.78} } $               & 
             $ 4.599 _{\textcolor{black!60}{\pm 0.003} } $  & 
             $0.301 _{\textcolor{black!60}{\pm 0.047}}$ &
             $ 0.757 _{\textcolor{black!60}{\pm 0.011} } $ & 
             $ 81.53 _{\textcolor{black!60}{\pm  0.27}} $ \\
             
             \rowcolor{green!20}\cellcolor{white} & 
             \sassha  & 
             $ 84 _{\textcolor{black!60}{\pm 3.15} } $          & 
             $ 2.03 _{\textcolor{black!60}{\pm 0.11} } $                 & 
             $ 4.540 _{\textcolor{black!60}{\pm 0.122} } $ &  
             $ 0.007 _{\textcolor{black!60}{\pm 0.129}}$  &
             $ 0.625 _{\textcolor{black!60}{\pm 0.002} } $ & 
             $ 83.54 _{\textcolor{black!60}{\pm  0.08} } $ \\
             
             \midrule
             
             \rowcolor{lgray} \multicolumn{8}{c}{Wikitext-2} \\ \midrule
            \multirow{5}{*}{Mini-GPT1} 
            & AdamW       & $ 836_{\textcolor{black!60}{\pm 13} } $           
                          & $ 31.61_{\textcolor{black!60}{\pm 0.433} } $    
                          & $ 1.642_{\textcolor{black!60}{\pm 1.036} } $ 
                          & $ 7_{\textcolor{black!60}{\pm 0} } $ 
                          & $  5.072_{\textcolor{black!60}{\pm 0.013} } $ 
                          & $ 175.06 _{\textcolor{black!60}{\pm 0.19} } $ 
                          \\
             & AdaHessian & $ 13141_{\textcolor{black!60}{\pm 14432}}$           
                          & $ 46.36_{\textcolor{black!60}{\pm 26.85}}$           
                          & $ 0.289_{\textcolor{black!60}{\pm 0.187}} $    
                          & $ 9_{\textcolor{black!60}{\pm 5}} $   
                          & $ 7.231_{\textcolor{black!60}{\pm 0.043}}$    
                          & $ 407.69_{\textcolor{black!60}{\pm 0.20}}$ \\
             & Sophia-H   & $ 319_{\textcolor{black!60}{\pm 14} } $      
                          & $ 55.17_{\textcolor{black!60}{\pm 1.100} } $ 
                          & $ 0.824_{\textcolor{black!60}{\pm 0.089} } $ 
                          & $ 13_{\textcolor{black!60}{\pm 1} } $ 
                          & $ 5.077_{\textcolor{black!60}{\pm 0.014} } $ 
                          & $ 157.60_{\textcolor{black!60}{\pm 0.37} } $ \\
            \rowcolor{green!20}\cellcolor{white} &
                          \msassha 
                          & $ 145_{\textcolor{black!60}{\pm 125} } $           
                          & $ 13.23_{\textcolor{black!60}{\pm 17.19} } $    
                          & $ 0.379_{\textcolor{black!60}{\pm 0.275} } $ 
                          & $ 3_{\textcolor{black!60}{\pm 1} } $ 
                          & $ 5.259_{\textcolor{black!60}{\pm 0.010} } $ 
                          & $ 125.01_{\textcolor{black!60}{\pm 0.21} } $ \\
            \rowcolor{green!20}\cellcolor{white} & 
                          \sassha   
                          & $ 79_{\textcolor{black!60}{\pm 2} } $  
                          & $ 14.50_{\textcolor{black!60}{\pm 0.325} } $ 
                          & $ 0.221_{\textcolor{black!60}{\pm 0.023} } $ 
                          & $ 3_{\textcolor{black!60}{\pm 0} } $ 
                          & $ 4.808_{\textcolor{black!60}{\pm 0.001} }$ 
                          & $ 122.40_{\textcolor{black!60}{\pm 0.16} } $\\
            \bottomrule

        \end{tabular}
    }
    \label{tab:add_sharp}
\end{table}
\egroup

\newpage

\section{Algorithm Comparison} \label{app:comparison}
\begin{table}[ht!]
\renewcommand{\arraystretch}{1.8}
%\centering
\caption{Comparison of various optimization algorithms in terms of gradient momentum $m_t$, diagonal preconditioning matrix $D_t$, and method-specific operations $\mathbf{U}(z)$. Here $g_t, \widehat{H}_t$ are the stochastic gradient and the Hessian estimation respectively, and $\beta_1, \beta_2$ denotes the momentum hyperparameters for the gradient and estimated Hessian. Bias correction \texttt{bc}$(\cdot)$ compensates for initialization biases in the gradient and Hessian momentum variables due to zero initialization.} 

\vskip 0.1in
\resizebox{\linewidth}{!}{
    \begin{tabular}{lllll}
        \toprule
        \rowcolor{lgray} \multicolumn{4}{c}{\large $x_{t+1} = x_t - \eta_{t} \mathbf{U}(D_{t}^{-1}m_{t})$}\\[0.3em] \midrule
         & $m_t$ & $D_t$ & $ \mathbf{U}(z) $ \\ \midrule
        SGD with momentum      &
        $\beta_1 m_{t-1} + (1 - \beta_1)g_t $ &
        $I$ &
        $z$ \\ 
        
        Stochastic Newton &
        $g_t$                                 &
        $H_t(x_t)$ &
        $z$ \\ 
        
        Adam \citep{kingma2014adam}             &
        $\beta_1 m_{t-1} + (1 - \beta_1)g_t $ &
        $ \sqrt{ \beta_2 v_{t-1} + (1-\beta_2) \colorbox{red!20}{$\operatorname{diag}(g_tg_t^\top)$} }$ &
        $ \texttt{bc}(z) $ \\ 
        
        % AdamW \citep{loshchilov2018decoupled}            &
        % \large "                              &
        % \large "                                                                        &
        % $z +$ \colorbox{red!20}{$ \lambda _{t}$} \\
        
        AdaHessian \citep{adahessian}       &
        \large "                              &
        $ \sqrt{ \beta_2 v_{t-1} + (1-\beta_2) \colorbox{red!20}{$\widehat{H}_t^{(s)}(x_t)^2$} }$ &
        $ \texttt{bc}(z) $ \\ 
        
        Sophia-H \citep{sophia}         &
        \large "                              &
        \hspace{1em} $ \beta_2 v_{t-1} + (1-\beta_2) \colorbox{red!20}{$\widehat{H}^{(c)}_t(x_t)$}$ every $k$ steps &
        \colorbox{red!20}{$\operatorname{clip}(z)$}  \\
        
        \midrule

        \sassha (Ours)       &
        $ \beta_1 m_{t-1} + (1-\beta_1) \colorbox{red!20}{$g_{t}(x_t+\boldsymbol{\epsilon_t^\star})$} $ &
        $ \sqrt{ \beta_2 v_{t-1} + (1-\beta_2) \colorbox{red!20}{$|\widehat{H}_t(x_t+\boldsymbol{\epsilon_t^\star})|$} }$ every $k$ steps & 
        $ \texttt{bc}(z) $ \\
        
        \bottomrule
    \end{tabular}
}
\label{tab:comp_algo}
\end{table}
In this section, we compare our algorithm with other adaptive and approximate second-order methods designed for deep learning to better illustrate our contributions within concurrent literature.
We present a detailed comparison of each methods in \cref{tab:comp_algo}.

Adam \citep{kingma2014adam} is an adaptive method popular among practitioners, which rescales the learning rate for each parameter dimension by dividing by the square root of the moving average of squared gradients.
This adaptive learning rate effectively adjusts the gradient (momentum) at each descent step, accelerating convergence and improving update stability.
Although Adam is not explicitly a second-order method, its process is related to second-order methods as it can be viewed as preconditioning via a diagonal approximation of the empirical Fisher information matrix.
AdamW \citep{loshchilov2018decoupled} proposes to improve Adam by decoupling the weight decay from the update rule for better generalization.
This is also shown to be effective in most approximate second-order methods, thus employed in all subsequently mentioned algorithms.

AdaHessian \citep{adahessian} is one of the earliest approximate second-order optimization methods tailored for deep learning.
To reduce the prohibitive cost of computing the Hessian, it uses Hutchinson's method \citep{hutchinson, hutchinson2} to estimate a diagonal Hessian approximation $\widehat{H}_{t}$ and applies a moving average to reduce variance in the estimation.
The authors also propose spatial averaging of the Hessian estimate, denoted as ($\widehat{H}_{t}^{(s)}$), which involves averaging the diagonal element within a filter of a convolution layer for filter-wise gradient scaling.
Sophia \citep{sophia} is an approximate second-order method specifically designed for language model pretraining.
Its primary feature is the use of the clipping mechanism $\operatorname{clip}(z) = \max\{\min\{z, \rho\}, -\rho\}$ with a predefined threshold $\rho$ to control the worst-case update size resulting from errorneous diagonal Hessian estimates in preconditioning.
Additionally, a hard adjustment is applied to each Hessian entry, substituting negative and very small values with a constant $\epsilon$, such as $\widehat{H}^{(c)}_t = \max \{ \widehat{h}_t , \epsilon \}$ to prevent convergence to saddle points and mitigate numerical instability.
Furthermore, Sophia also incorporates lazy Hessian updates to enhance computational efficiency. 
This works without significant performance degradation as the clipping technique and hard adjustment prevent a rapid change of the Hessian, keeping the previous Hessian relevant over more extended steps.

Our method \sassha{} adds perturbation $\epsilon^\star_t$ before computing the gradient and Hessian estimation to penalize sharpness during the training process for improved generalization--an approach not previously explored in the literature.
For stability, we additionally introduce two techniques: an absolute function and a square root to the Hessian estimates.
The absolute function enforces Hessian estimates to be semi-positive definite while preserving their magnitude.
Also, the square root smoothly adjusts underestimated curvature, stabilizing the Hessian estimates.
Consequently, the blend of sharpness reduction and Hessian stabilization enables the efficient reuse of previously computed Hessians, resulting in a stable and efficient algorithm.


\section{Convergence Analysis of \sassha} \label{app:convergence}
\input{text/appendix_convergence}


\section{Experiment Setting} \label{app:hypersearch}
Here, we describe our experiment settings in detail. 
We evaluate \sassha against AdaHessian \citep{adahessian}, Sophia-H \citep{sophia}, Shampoo \citep{gupta2018shampoo}, SGD, AdamW \citep{loshchilov2018decoupled}, and SAM \citep{sam} across a diverse set of vision and language tasks.
Across all evaluations except for language finetuning, we set lazy Hessian update interval to $k = 10$ for \sassha. 
In fact, Sophia-H also supports lazy Hessian updates, but \citet{sophia} reports that it achieves the best performance when $k = 1$, without lazy updating.
Since our goal is to demonstrate that \sassha{} exhibits better generalization than existing approximate second-order methods, we compare it with Sophia-H without lazy Hessian updating $k = 1$, ensuring that the algorithm is assessed under its optimal configuration.


\subsection{Image Classification}
\paragraph{CIFAR}
We trained ResNet-20 and ResNet-32 on the CIFAR datasets for $160$ epochs and Wide-ResNet28-10 for $200$ epochs. 
Only standard inception-style data augmentations, such as random cropping and horizontal flipping, were applied, without any additional regularization techniques or extra augmentations.
We used standard cross-entropy without label smoothing as a loss function.
Also, we adopted a multi-step decay learning rate schedule. 
Specifically, for ResNet-20 and ResNet-32, the learning rate was decayed by a factor of $0.1$ at epochs $80$ and $120$. 
For Wide-ResNet28-10, the learning rate was decayed by a factor of $0.2$ at epochs $60$, $120$ and $160$. 
The exponential moving average hyperparameters were set to $\beta_1 = 0.9$ and $\beta_2 = 0.999$.
All experiments were conducted with a batch size of 256.
The hyperparameter search space for each method is detailed in \cref{tab:tuning_cifar}.

\begin{table}[ht!]
\renewcommand{\arraystretch}{2}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcc|cccc|c}
    \toprule
    \rowcolor{lgray} 
    Method        
    & \sassha
    & \msassha
    & AdaHessian 
    & Sophia-H 
    & AdamW / SGD
    & SAM
    & shampoo \\
    
    \midrule
    
    Learning Rate    & 
    \multicolumn{2}{c|}{ $\Bigl\{0.3, 0.15, 0.03, 0.015 \Bigr\}$ } 
    & \multicolumn{4}{c|}{ $\Bigl\{ 0.3, 0.15, 0.1, 0.03, 0.015, 0.01, 0.003, 0.001, 0.0003, 0.0001  \Bigr\}$ } 
    & $\Bigl\{ \substack{1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.9, 0.8, 0.7, 0.6, \\ 0.5, 0.4, 0.3, 0.2, 0.1, 0.01, 0.04, 0.004} \Bigr\} $ \\ 
    
    \midrule
    Weight Decay     & \multicolumn{7}{c}{$\Bigl\{ \text{1e-3}, \text{5e-4}, \text{1e-4}, \text{5e-5}, \text{1e-5}, \text{5e-6}, \text{1e-6} \Bigr\}$}   \\ 
    
    \midrule
    
    Perturbation radius $\rho$      & $\Bigl\{0.1, 0.15, 0.2, 0.25 \Bigr\}$ 
    & $\Bigl\{ 0.1, 0.2, 0.3, 0.6, 0.8 \Bigr\}$ 
    & - 
    & - 
    & - 
    & $\Bigl\{ \substack{0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3,\\ 0.35, 0.4, 0.45, 0.5, 0.55, 0.6} \Bigr\}$
    & - \\ 
    
    \midrule
    
    Clipping-threshold   & - 
    & - 
    & - 
    & $\Bigl\{ \substack{0.1, 0.05, 0.01, 0.005,\\ 0.001, 0.0005, 0.0001} \Bigr\}$ 
    & -   
    & - 
    & - \\ 
    
    \midrule
    
    Damping   & - 
    & - 
    & -  
    & - 
    & -
    & -
    & 1e-$\Bigl\{2, 3, 4, 6, 8 \Bigr\}$ \\ 
    
    \midrule
    
    Hessian Update Interval $k$ & 10 
    & 10 
    & 1 
    & 1 
    & - 
    & -
    &  1\\
    
    \midrule
    
    learning rate schedule & \multicolumn{7}{c}{\text{Multi-step decay}} \\
    
    \bottomrule
    \end{tabular}
    }
    \caption{Hyperparameter search space for CIFAR datasets}
    \label{tab:tuning_cifar}
    
\end{table}


\paragraph{ImageNet}

We trained ResNet-50 and \textit{plain Vision Transformer} (plain ViT) \citep{beyer2022better} for 90 epochs.
Remarkably, plain ViT converges in just 90 epochs on ImageNet, attaining performance comparable to the original ViT trained for 300 epochs \citep{beyer2022better}.
This faster convergence allows us to efficiently assess whether \sassha can enhance the generalization in ViT architectures. 
Consistent with our CIFAR training settings, we applied only standard inception-style data augmentations and used standard cross-entropy as a loss function. 
For ResNet-50, we adopted a multi-step decay learning rate schedule, reducing the learning rate by a factor of 0.1 at epochs 30 and 60. 
However, AdaHessian could not be trained with a multi-step decay schedule; therefore, as recommended by \citet{adahessian}, we employed a plateau decay schedule instead. 
For Vision Transformer training, following \citet{chenvision}, we used a cosine learning rate schedule with an 8-epoch warm-up phase. 
Additionally, the exponential moving average hyperparameters $ \beta_1 $ and $ \beta_2 $ were set to $0.9$ and $0.999$ respectively. 
We used a batch size of 256 for ResNet50 and 1024 for ViT. 
The hyperparameter search spaces for each methods used during training on the ImageNet dataset are detailed in \cref{tab:tuning_imag}.


\begin{table}[ht!]
    \renewcommand{\arraystretch}{2}
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lccc|cccc}
    \toprule
    \rowcolor{lgray} 
    methods     
    & \sassha
    & \msassha
    & AdaHessian 
    & Sophia-H
    & AdamW / SGD 
    & SAM \\
    \midrule
    Learning Rate    &  $\Bigl\{0.6, 0.3, 0.15 \Bigr\}$
    &  $\Bigl\{0.6, 0.3, 0.15 \Bigr\}$
    &  $\Bigl\{0.6 0.3, 0.15 \Bigr\}$
    &  \multicolumn{3}{c}{$ \Bigl\{0.4, 0.2, 0.1, 0.04, 0.02, 0.01, 0.001 \Bigr\} $}  \\ 
    
    \midrule
    
    Weight Decay     & \multicolumn{6}{c}{$\Bigl\{ \text{1e-3}, \text{5e-4}, \text{1e-4}, \text{5e-5}, \text{1e-5} \Bigr\}$}   \\ 
    
    \midrule
    
    Perturbation radius $\rho$       & $\Bigl\{ 0.1, 0.15, 0.2, 0.25 \Bigr\}$ 
    & $\Bigl\{ 0.1, 0.2, 0.4, 0.8 \Bigr\}$ 
    & -
    & -
    & -
    & $\Bigl\{ 0.1, 0.15, 0.2, 0.25, 0.3 \Bigr\} $ \\ 
    
    \midrule
    
    Clipping-threshold   & - 
    & - 
    & - 
    & $\Bigl\{ 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001 \Bigr\}$  
    & -
    & - \\ 
    
    \midrule
    
    Hessian Update Interval $k$ & 10 
    & 10
    & 1
    & 1
    & - 
    & - \\ 
    
    \bottomrule
    
    \end{tabular}
    }
    \caption{Hyperparameter search space for ImageNet}
    \label{tab:tuning_imag}
    
\end{table}

\subsection{Language}
\paragraph{Language Pretraining}
Following the training settings introduced in \citet{gomes2024adafisher}, we conducted experiments on a mini GPT-1 model using the Wikitext-2 dataset. 
This scaled-down version of GPT-1 maintains essential modeling capabilities while reducing computational demands. 
We trained the model with three methods: \sassha{}, \msassha{}, and Sophia-H. 
The hyperparameter tuning spaces for these methods are summarized in \cref{tab:tuning_pretraining}. 
For other methods not listed in the table, we directly reported the results from \citet{gomes2024adafisher}.

\begin{table}[ht!]
    \renewcommand{\arraystretch}{2}
    \centering
    \resizebox{0.8\textwidth}{!}{%
    \begin{tabular}{lccc}
    \toprule
    \rowcolor{lgray} 
    methods        & \sassha{} / \msassha{}     
    & Sophia-H 
    & SAM        \\
    
    \midrule
    
    Learning Rate     & $\Bigl\{0.15, 0.1, 0.03, 0.01, 0.003, 0.0015 \Bigr\}$      
    &   $\Bigl\{ \text{1e-2}, \text{5e-3}, \text{1e-3}, \text{5e-4}, \text{1e-4}, \text{5e-5}, \text{1e-5} \Bigr\}$ 
    &   $\Bigl\{ \text{1e-2}, \text{1e-3}, \text{1e-4}, \text{1e-5}, \text{1e-6} \Bigr\}$                      \\ 
    
    \midrule
    
    Weight Decay      & \multicolumn{3}{c}{ $ \text{1e-}\{1, 2, 3, 4, 5, 6, 7, 8\}$} \\ 
    
    \midrule
    
    Perturbation radius $\rho$    
    &  $ \text{1e-}\{1, 2, 3, 4, 5\} $ 
    & - 
    &  $ \text{1e-}\{1, 2, 3, 4, 5, 6, 7, 8 \} $ \\ 
    
    \midrule
    
    Clipping-threshold   
    & - 
    & $\Bigl\{ \text{1e-1}, \text{5e-2}, \text{1e-2}, \text{5e-3}, \text{1e-3}, \text{5e-4}, \text{1e-4} \Bigr\}$ 
    & - \\ 
    
    \midrule
    
    Hessian Update Interval $k$  & 10  
    & 1 
    & - \\ 
    
    \midrule
    
    Epochs        & \multicolumn{2}{c}{$50$} 
    & 55 \\ 
    
    \bottomrule
    \end{tabular}
    }
    \caption{Hyperparameter search space for language pretraining}
    \label{tab:tuning_pretraining}
    
\end{table}


\paragraph{Language Finetuning} 
We utilized a pretrained SqueezeBERT \citep{iandola2020squeezebert} from the HuggingFace Hub \citep{wolf2020huggingfaces}.
We set the batch size to 16, the maximum sequence length to 512, and the dropout rate to 0.
The number of training epochs varied depending on the specific GLUE task: 5 epochs for MNLI, QQP, QNLI, and SST-2; 10 epochs for STS-B, MRPC, and RTE.
Additionally, We adopted a polynomial learning rate decay scheduler. 
The detailed hyperparameter search spaces are presented in \cref{tab:tuning_fine}.

\begin{table}[ht]
    \renewcommand{\arraystretch}{2}
    \centering
    \resizebox{0.8\linewidth}{!}{%
    \begin{tabular}{lccccc}
    \toprule
    \rowcolor{lgray} 
    methods        & \sassha{} / \msassha{} 
    & Sophia-H 
    & AdaHessian 
    & AdamW
    & SAM          \\
    \midrule
    Learning Rate         &   \multicolumn{5}{c}{ $ \text{1e-}\{1, 2, 3, 4, 5, 6, 7, 8\}$} \\ 
    
    \midrule
    
    Weight Decay          &   \multicolumn{5}{c}{ $ \text{1e-}\{1, 2, 3, 4, 5, 6, 7, 8\}$}  \\ 
    
    \midrule
    
    Perturbation radius $\rho$       &  $ \text{1e-}\{2, 3, 4, 5\}$  
    & - 
    & - 
    & -
    & $ \text{1e-}\{1, 2, 3, 4, 5, 6, 7, 8 \}$ \\ 
    
    \midrule
    
    Clipping-threshold  & - 
    & $\Bigl\{ \substack{0.1, 0.05, 0.01, 0.005,\\ 0.001, 0.0005, 0.0001} \Bigr\}$ 
    & - 
    & - \\ 
    
    \midrule
    
    Hessian Update Interval $k$ & 1 
    & 1 
    & 1 
    & -
    & - \\ 
    
    \bottomrule
    
    \end{tabular}
    }
    \caption{Hyperparameter search space for language finetuning}
    %\vspace{-2.5em}
    \label{tab:tuning_fine}
    
\end{table}


\subsection{Label Noise}
We introduced label noise by randomly corrupting a fraction of the training data at rates of 20\%, 40\%, and 60\%.
Using this setup, we trained ResNet-32 for 160 epochs with a batch size of 256.
We adopted a multi-step decay learning rate schedule, reducing the learning rate by a factor of 0.1 at epochs 80 and 120.
The specific hyperparameters explored during these experiments are detailed in \cref{tab:tuning_noise}.

\begin{table}[ht!]
\renewcommand{\arraystretch}{2}
\centering
\resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccccc}
    \toprule
    \rowcolor{lgray} 
    Methods        & \sassha{} & \msassha{} 
    & Sophia-H 
    & AdaHessian 
    & SAM
    & SGD \\
    \midrule
    Learning Rate    & \multicolumn{6}{c}{$ \Bigl\{ 0.3, 0.15, 0.1, 0.03, 0.015, 0.01, 0.003, 0.0015, 0.001 \Bigr\} $}   \\ 
    
    \midrule
    
    Weight Decay     & \multicolumn{6}{c}{$\Bigl\{ \text{1e-3}, \text{5e-4}, \text{5e-5}, \text{1e-5}, \text{5e-6}, \text{1e-6} \Bigr\}$}   \\ 
    
    \midrule
    
    Perturbation radius $\rho$       & $\Bigl\{ 0.25, 0.2, 0.15, 0.1  \Bigr\}$ & $\Bigl\{ 0.8, 0.6, 0.3, 0.2, 0.1 \Bigr\}$ 
    & - 
    & - 
    & $\Bigl\{ 0.3, 0.25, 0.2, 0.15, 0.1, 0.05, 0.02, 0.01, 0.002, 0.001      \Bigr\}$ 
    & - \\ 
    
    \midrule
    
    Clipping-threshold   & - 
    & - 
    & $\Bigl\{ \substack{0.1, 0.05, 0.01, 0.005,\\ 0.001, 0.0005, 0.0001} \Bigr\}$  & - & - & - \\ 
    
    \midrule
    
    Hessian Update Interval $k$ & 10 
    & 10 
    & 1 
    & 1 
    & - 
    & - \\ \bottomrule
    \end{tabular}
    }
    \caption{Hyperparameter search space for label noise experiments}
    \label{tab:tuning_noise}
\end{table}


\section{More Ablations} \label{app:add_ablation}

\subsection{Square Root Function}\label{app:sqrt_alternatives}

\begin{table}[ht!]
    \centering
    \caption{
    Comparison of square-root against damping and clipping.
    }
    \vskip 0.1in
    \resizebox{0.5\linewidth}{!}{
        \begin{tabular}{lccc}
        \toprule
         & \multicolumn{2}{c}{CIFAR-10} 
         & \multicolumn{1}{c}{CIFAR-100}  \\
         
         \cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-4} 
         
         & \multicolumn{1}{c}{ ResNet-20 } & \multicolumn{1}{c}{ ResNet-32 }   & \multicolumn{1}{c}{ ResNet-32 } \\  
         
         \midrule
           
        Clipping   & $ 92.78 _{\textcolor{black!60}{\pm 0.18} } $ 
        & $ 93.80 _{\textcolor{black!60}{\pm 0.16} } $ 
        & $ 69.47 _{\textcolor{black!60}{\pm 0.20} } $ \\
              
        Damping   & $  92.74 _{\textcolor{black!60}{\pm 0.06} } $ 
        & $  93.68 _{\textcolor{black!60}{\pm 0.29} } $ 
        & $ 71.27 _{\textcolor{black!60}{\pm 0.43} } $ \\
        
        \midrule
        
        \rowcolor{green!20} Square root (\sassha)    & $ \textbf{92.98} _{\textcolor{black!60}{\pm 0.05}} $ 
        & $ \textbf{94.09} _{\textcolor{black!60}{\pm 0.24} } $ 
        & $ \textbf{72.14} _{\textcolor{black!60}{\pm 0.16} } $ \\
        \bottomrule
        
        \end{tabular}
    }
    \label{tab:sqrt generalization}    
\end{table}

We conduct an ablation study to support our use of the square-rooted preconditioner in \sassha, comparing it to other alternatives to stabilize the preconditioner such as damping or clipping.
We search damping and clipping hyperparameters over $\{10^{-4}, 10^{-6}, 10^{-8}, 10^{-12}\}$ and $\{0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001\}$, respectively.
We note that the square-root employed in \sassha does not require such extensive hyperparamter search.
The results are presented in \cref{tab:sqrt generalization}.

Our experiments demonstrate that the square-rooted preconditioner achieves higher validation accuracy than those with damping or clipping, even with a three times smaller hyperparameter search budget.
We provide two possible explanations for this observation.
First, clipping and damping rigidly transform Hessian estimates.
Specifically, damping shifts the Hessian estimate by a fixed damping factor, and clipping replaces certain Hessian entries with a predefined constant.
Without careful tuning, these inflexible modification may lead to incorrect updates in specific directions, degrading performance.
In contrast, the square root smoothly adjusts Hessian estimates, preserving their structural integrity while mitigating extreme values. 
Second, square-rooted preconditioner can be interpreted as the result of a geometric interpolation between the identity matrix $I$ and $H^\alpha$. 
This interpolation has been demonstrated to enable selecting an optimal preconditioner that balances the bias and the variance of the population risk, thereby minimizing generalization error \citep{amari2021when}.
In general, $\alpha=1/2$ (i.e., square root) has consistently shown moderate performance across various scenarios \citep{{amari2021when, duchi2011adaptive, kingma2014adam}}.

\subsection{Absolute Value Function} \label{sec:abs_ablation}
\begin{wrapfigure}{r}{0.49\linewidth}
    \centering
    \vspace{-2em}
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ablation/abs_train.pdf}
        \caption{Train loss}
        \label{fig:abs_ablation_val}
    \end{subfigure}
    \begin{subfigure}{0.52\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ablation/abs_dist.pdf}
        \caption{Hess. eigenspectrum}
        \label{fig:abs_ablation_dist}
    \end{subfigure}
    % 
    \caption{Effect of the absolute function on the training loss and the Hessian eigenspectrum of the found solution of \sassha on ResNet-32/CIFAR-10. Without the absolute function, \sassha converges to sub-optimal saddle point.}
    \label{fig:abs_ablation}
    
\end{wrapfigure}

We observe how the absolute function influences the training process to avoid convergence to a critical solution that could result in sub-optimal performance.
We train ResNet-32 on CIFAR-100 using \sassha without the absolute function (\texttt{No-Abs}) and compare the resulting training loss to that of the original \sassha.
We also plot the Hessian eigenspectrum of the found solution via the Lanczos algorithm \citep{yao2020pyhessian} to determine whether the found solution corresponds to a minimum or a saddle point.
The results are illustrated in \cref{fig:abs_ablation}.
We can see that without the absolute function, the training loss converges to a sub-optimal solution, where the prevalent negative values in the diagonal Hessian distribution indicate it as a saddle point.
This shows the necessity of the absolute function for preventing convergence to these critical regions.

\section{Validation Loss Curve for Vision Task} \label{app:valloss}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.27\linewidth]{figures/validation/Res20-CIFAR10-Loss.pdf}
    \includegraphics[width=0.27\linewidth]{figures/validation/Res32-CIFAR10-Loss.pdf}
    \includegraphics[width=0.27\linewidth]{figures/validation/Res32-CIFAR100-Loss.pdf}
    \includegraphics[width=0.27\linewidth]{figures/validation/WRN28-CIFAR100-Loss.pdf}
    \includegraphics[width=0.27\linewidth]{figures/validation/Res50-ImageNet-Loss.pdf}
    \includegraphics[width=0.27\linewidth]{figures/validation/ViT-ImageNet-Loss.pdf}
    \caption{Validation loss curve of \sassha, SGD, AdaHessian, AdamW, and Sophia-H on various image classification models and tasks. \sassha outperforms all first-order and second-order baseline methods.}
    \label{fig:valloss}
\end{figure}

The experimental results in \cref{fig:valloss} demonstrate better generalization capability of \sassha over the related methods. 
Across all datasets and model architectures, our method consistently achieves the lowest validation loss, indicative of its enhanced ability to generalize from training to validation data effectively.
This robust performance of \sassha underscores its potential as a leading optimization method for various deep learning applications, particularly in image classification.


\section{Comparison with First-order Baselines with Given More Training Budget than \sassha }\label{app:comp_fo_fair}

We train SGD and AdamW for twice as many epochs as \sassha and compare their final validation accuracies.
The results are presented in \cref{tab:comp_fo_fair}.
Despite this extended training budget, these first-order methods fall short of the performance attained with \sassha, demonstrating their limited effectiveness compared to \sassha. 
We attribute this outcome to \sassha reaching a flatter and better generalizing solution along with stable preconditioning, which together enables consistent outperformance over first-order baselines.

\begin{table}[ht!]
    \centering
    \caption{
    Performance comparison of \sassha against SGD and AdamW with twice the epoch allocation. \sassha achieves better results with significantly fewer epochs.}
    \vskip 0.1in
    
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lcccccc}
        \toprule
         & RN20 - CIFAR-10 & RN32 - CIFAR-10 & RN32 - CIFAR-100 & WRN28 - CIFAR-100 & RN50 - ImageNet & ViT\_s - ImageNet \\
         & \texttt{Acc (epoch)} & \texttt{Acc (epoch)} & \texttt{Acc (epoch)} & \texttt{Acc (epoch)} & \texttt{Acc (epoch)} & \texttt{Acc (epoch)} \\
        \midrule
        SGD        & $ 92.62 $ (320e) & $ 93.43 $ (320e) & $ 69.93 $ (320e) & $ 80.50 $ (400e) & $ 75.90 $ (180e) & $ 63.64 $ (180e) \\
        AdamW      & $ 92.55 $ (320e) & $ 92.97 $ (320e) & $ 69.50 $ (320e) & $ 79.46 $ (400e) & $ 75.57 $ (180e) & $ 66.97 $ (180e) \\
        \midrule
        \rowcolor{green!20} \sassha  & $ \textbf{92.98} $ (160e) & $ \textbf{94.09} $ (160e) & $ \textbf{72.14} $ (160e) & $ \textbf{83.54} $ (200e) & $ \textbf{76.43} $ (90e) & $ \textbf{69.20} $ (90e) \\
        \bottomrule
        \end{tabular}
    }
    \label{tab:comp_fo_fair}
\end{table}




\section{Effectiveness of Stable Hessian Approximations in \sassha } 
\label{app:samsophia}
%\vspace{-1.2em}

\begin{table}[ht!]
    \centering
    \caption{
    Results of Sophia-H with sharpness minimization.
    }
    \vskip 0.1in
    \resizebox{0.6\linewidth}{!}{
        \begin{tabular}{lcccc}
        \toprule
         & \multicolumn{2}{c}{CIFAR-10} 
         & \multicolumn{2}{c}{CIFAR-100}  \\
         \cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5} 
         & \multicolumn{1}{c}{ ResNet-20 } & \multicolumn{1}{c}{ ResNet-32 }   & \multicolumn{1}{c}{ ResNet-32 }   & \multicolumn{1}{c}{ WRN-28-10}    \\
         
         \midrule
           
         SAM   & $ 92.85 _{\textcolor{black!60}{\pm 0.07} } $ 
         & $ 93.89 _{\textcolor{black!60}{\pm 0.13} } $ 
         & $ 71.99 _{\textcolor{black!60}{\pm 0.20} } $ 
         & $ 83.14 _{\textcolor{black!60}{\pm 0.13} } $ \\
              
         Sophia-H (with SAM) $_{\text{}}$ & $ 92.53 _{\textcolor{black!60}{\pm 0.39} } $ 
         & $  93.59 _{\textcolor{black!60}{\pm 0.31} } $ 
         & $ 71.31 _{\textcolor{black!60}{\pm 0.43} } $ 
         & $ 80.15 _{\textcolor{black!60}{\pm 0.35} } $  \\
        
         \midrule
        
         \rowcolor{green!20} \sassha    & $ \textbf{92.98} _{\textcolor{black!60}{\pm 0.05}} $ 
         & $ \textbf{94.09} _{\textcolor{black!60}{\pm 0.24} } $ 
         & $ \textbf{72.14} _{\textcolor{black!60}{\pm 0.16} } $ 
         & $\textbf{83.54} _{\textcolor{black!60}{\pm 0.08}}$ 
         \\
        
        \bottomrule
        \end{tabular}
    }
    \label{tab:im_cls_samsophia}
    
\end{table}

We demonstrate limited benefit from naively combining SAM with existing approximate second-order methods without the carefully designed stabilization strategies of \sassha.  
Precisely, we compare the validation accuracy of \sassha with a simple combination of SAM and Sophia, denoted as Sophia-H (with SAM).
We provide results in \cref{tab:im_cls_samsophia}.

We observe that Sophia-H (with SAM) performs worse than SAM, whereas \sassha outperforms both methods, validating the effectiveness of the design choices made in \sassha.
We attribute this to the reduced compatibility of Sophia-H with SAM compared to \sassha.
First, when curvature is reduced due to SAM, clipping may cause a significant increase in the number of Hessian entries replaced by a very small constant.
This situation raises the sensitivity to hyperparameters like the clipping threshold and makes the optimization process more dependent on careful tuning.
Conversely, the stable Hessian approximation in \sassha, incorporating the absolute function and square rooting, avoids enforcing hard adjustments to the magnitude or direction of the update vector or to Hessian entries. 
Instead, it smoothly adjusts Hessian, preserving its structural integrity while mitigating extreme values.

In addition, the use of clipping results in Sophia partially performing signSGD over a subset of parameters \citep{sophia}, which may lead to suboptimal convergence in typical situations \cite{karimireddy2019error}.

\section{Comparison with Advanced SAM Variants} \label{app:samvariants_vs_sassha}

Thus far, our primary focus has centered on validating the effectiveness of \sassha in the context of approximate second-order optimization.
While this remains the principal objective of our study, here we additionally compare \sassha with advanced SAM variants (\ie ASAM \citep{asam}, GSAM \citep{gsam}) to prove that \sassha is a sensible approach.
We also evaluate \gsassha (\sassha with surrogate gap guided sharpness from \citep{gsam}) for fair comparison.
The results are represented in \cref{tab:im_cls_samvariants}.

\begin{table}[ht!]
    \centering
    \caption{\sassha v.s. advanced SAM variants in Image classification.}
    \vskip 0.1in
    
    \resizebox{0.7\linewidth}{!}{
        \begin{tabular}{lcccccc}
        \toprule
         & \multicolumn{2}{c}{CIFAR-10} 
         & \multicolumn{2}{c}{CIFAR-100} 
         & \multicolumn{2}{c}{ImageNet} \\
         \cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5} \cmidrule(l{3pt}r{3pt}){6-7}
           & \multicolumn{1}{c}{ ResNet-20 } & \multicolumn{1}{c}{ ResNet-32 } & \multicolumn{1}{c}{ ResNet-32 }  & \multicolumn{1}{c}{ WRN-28-10} & \multicolumn{1}{c}{ ResNet-50 } & \multicolumn{1}{c}{ ViT-s-32} \\ \midrule
           
        ASAM       & $ 92.96 _{\textcolor{black!60}{\pm 0.25}} $ 
        & $ 93.85 _{\textcolor{black!60}{\pm 0.15} } $ 
        & $ 72.02 _{\textcolor{black!60}{\pm 0.28} } $ 
        & $ 83.39_{\textcolor{black!60}{\pm 0.06} } $ 
        & $ 76.54 _{\textcolor{black!60}{\pm 0.15} } $ 
        & $ 68.26 _{\textcolor{black!60}{\pm 0.36}}$  \\
        
        GSAM       & $ 92.72 _{\textcolor{black!60}{\pm 0.39} } $ 
        & $ 93.76 _{\textcolor{black!60}{\pm 0.31} } $ 
        & $ 72.10 _{\textcolor{black!60}{\pm 0.43} } $ 
        & $ 83.21 _{\textcolor{black!60}{\pm 0.39} } $ 
        & $ 76.45 _{\textcolor{black!60}{\pm 0.22} } $ 
        & $ 69.60 _{\textcolor{black!60}{\pm 0.16} } $  \\
        
        \midrule
        
        \rowcolor{green!20} \sassha    & $ \textbf{92.98} _{\textcolor{black!60}{\pm 0.05}} $ 
        & $ 94.09 _{\textcolor{black!60}{\pm 0.24} } $ 
        & $ 72.14 _{\textcolor{black!60}{\pm 0.16} } $ 
        & $ 83.54 _{\textcolor{black!60}{\pm 0.08}}$ 
        & $ 76.43 _{\textcolor{black!60}{\pm 0.18} }$ 
        & $ 69.20 _{\textcolor{black!60}{\pm 0.30} }$ \\
        
        \rowcolor{green!20} \gsassha   & $ 92.94 _{\textcolor{black!60}{\pm 0.18}} $   
        & $ \textbf{94.15} _{\textcolor{black!60}{\pm 0.12} } $ 
        & $ \textbf{72.18} _{\textcolor{black!60}{\pm 0.52} } $ 
        & $ \textbf{83.56} _{\textcolor{black!60}{\pm 0.27} } $ 
        & $\textbf{76.66} _{\textcolor{black!60}{\pm 0.23}}$  
        & $\textbf{69.67} _{\textcolor{black!60}{\pm 0.14}}$ \\
        \bottomrule
        \end{tabular}
    }
    \label{tab:im_cls_samvariants}
    
\end{table}


We find that \sassha is competitive with these advanced SAM variants.
However, we note clearly that those SAM variants require considerably more hyperparameter tuning to achieve generalization performance comparable to \sassha. For example, GSAM introduces an additional hyperparameter $\alpha$, demanding as much tuning effort as tuning $\rho$. 
Similarly, ASAM, as noted by its authors, typically necessitates exploring a broader $\rho$ range, as its appropriate value is approximately 10 times larger than that of SAM.
In our setup, tuning GSAM and ASAM involved $4.5\times \sim 15.75 \times$ and $3\times \sim 8 \times$ larger search grids compared to \sassha, respectively.
We provide detailed setup and hyperparameter search space below.

\textbf{Setup and Search space.} For ResNet, we use SGD as the base methods for ASAM and GSAM, while for ViT, AdamW with gradient clipping set to 1.0 serves as the base methods. 
For all models, typical cross entropy loss is used (not label-smoothing cross entropy), and the best learning rate and weight decay of the base methods are selected in experiments with ASAM and GSAM.
All algorithms are evaluated with constant $\rho$ (without scheduling). 
For learning rate schedule, we apply multi-step decay with a decay rate of 0.1 for ResNet on CIFAR, and use cosine learning rate decay with 8 warm-up epochs for ViTs.

\begin{table}[ht!]
\renewcommand{\arraystretch}{2}
\centering
\resizebox{0.8\textwidth}{!}{%
    \begin{tabular}{lccc}
    \toprule
    \rowcolor{lgray} 
      
    & ResNet/CIFAR
    & ResNet/ImageNet 
    & ViT/ImageNet \\
    \midrule
    \hspace{1em} $\rho$ \hspace{1em}  
    & $\{0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4 \} $
    & $\{0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3\}$
    & $\{0.1, 0.2, 0.3, 0.4, 0.5, 0.6 \}$\\
    \midrule
    \hspace{1em} $\alpha$ \hspace{1em}
    & $\{0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3 \} $
    & $ \{0.01, 0.05, 0.1, 0.15, 0.2 \}$
    & $\{0.1, 0.2, 0.3 \}$\\ \bottomrule
    \end{tabular}
    }
    \caption{Hyperparameter search space for GSAM and G-\sassha}
    \label{tab:hyper_gsam}
\end{table}

\begin{table}[ht!]
\renewcommand{\arraystretch}{2}
\centering
\resizebox{0.8\textwidth}{!}{%
    \begin{tabular}{lcc}
    \toprule
    \rowcolor{lgray} 
      
    & ResNet/CIFAR
    & ImageNet  \\
    \midrule
    \hspace{1em} $\rho$ \hspace{1em}
    & $\Bigl\{ \substack{0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, \\ 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6} \Bigr\} $
    & $\{0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 2 \}$\\ \bottomrule
    \end{tabular}
    }
    \caption{Hyperparameter search space for ASAM}
    \label{tab:hyper_gsam2}
\end{table}




\section{Additional Label Noise Experiments}
\label{app:add_label_noise}

\begin{table}[ht!]
    \centering
    \caption{Robustness to label noise. Here we measure the validation accuracy under various levels of label noise using ResNet-32 trained on CIFAR-100 and CIFAR-10. \sassha shows much robust performance under label noise.}
    \label{tab:noise_label_sassha}
    \resizebox{0.85\linewidth}{!}{%
    \begin{tabular}{lccccccccc}
        \toprule
        & \multicolumn{4}{c}{CIFAR-10} & \multicolumn{4}{c}{CIFAR-100} \\ 
        \cmidrule(l{3pt}r{3pt}){2-5} \cmidrule(l{3pt}r{3pt}){6-9} 
        Noise level & {0\%} & {20\%} & {40\%} & {60\%} & {0\%} & {20\%} & {40\%} & {60\%} \\ 
        \midrule
        SGD                 & 
        $ 92.69 _{\textcolor{black!60}{\pm 0.06}}$     &
        $ 89.91 _{\textcolor{black!60}{\pm 0.87}}$     &
        $ 87.26 _{\textcolor{black!60}{\pm 0.40}}$     &
        $ 82.72 _{\textcolor{black!60}{\pm 1.59}}$     &
        $ 69.32 _{\textcolor{black!60}{\pm 0.19}}$     &
        $ 62.18 _{\textcolor{black!60}{\pm 0.06}}$     & 
        $ 55.78 _{\textcolor{black!60}{\pm 0.55}}$     &  
        $ 45.53 _{\textcolor{black!60}{\pm 0.78}}$     \\
        
        
        SAM $_{\text{SGD}}$ & 
        $ 93.89 _{\textcolor{black!60}{\pm 0.13}}$     &
        $ 92.27 _{\textcolor{black!60}{\pm 0.14}}$     &
        $ 90.11 _{\textcolor{black!60}{\pm 0.25}}$     &
        $ 85.79 _{\textcolor{black!60}{\pm 0.30}}$     &
        $ 71.99 _{\textcolor{black!60}{\pm 0.20}}$     &
        $ 65.53 _{\textcolor{black!60}{\pm 0.11}}$     & 
        $ 61.20 _{\textcolor{black!60}{\pm 0.17}}$     &
        $ 51.93 _{\textcolor{black!60}{\pm 0.47}}$     \\ 
        
        
        AdaHessian         & 
        $ 92.48 _{\textcolor{black!60}{\pm 0.15}}$     &
        $ 90.11 _{\textcolor{black!60}{\pm 0.01}}$     & 
        $ 86.88 _{\textcolor{black!60}{\pm 0.04}}$     &
        $ 83.25 _{\textcolor{black!60}{\pm 0.01}}$     &
        $ 68.06 _{\textcolor{black!60}{\pm 0.22}}$     &
        $ 63.06 _{\textcolor{black!60}{\pm 0.25}}$     & 
        $ 58.37 _{\textcolor{black!60}{\pm 0.13}}$     & 
        $ 46.02 _{\textcolor{black!60}{\pm 1.96}}$     \\


        Sophia-H           & 
        $ 91.99 _{\textcolor{black!60}{\pm 0.08}}$     &
        $ 89.93 _{\textcolor{black!60}{\pm 0.01}}$     &
        $ 87.30 _{\textcolor{black!60}{\pm 0.51}}$     & 
        $ 82.78 _{\textcolor{black!60}{\pm 1.43}}$     &
        $ 67.76 _{\textcolor{black!60}{\pm 0.37}}$     &
        $ 62.34 _{\textcolor{black!60}{\pm 0.47}}$     & 
        $ 56.54 _{\textcolor{black!60}{\pm 0.28}}$     & 
        $ 45.37 _{\textcolor{black!60}{\pm 0.27}}$     \\
        
        
        Shampoo           &  
        $ 90.23 _{\textcolor{black!60}{\pm 0.83}}$     &
        $ 88.14 _{\textcolor{black!60}{\pm 0.29}}$     & 
        $ 85.15 _{\textcolor{black!60}{\pm 0.61}}$     & 
        $ 81.16 _{\textcolor{black!60}{\pm 0.30}}$     &
        $ 64.08 _{\textcolor{black!60}{\pm 0.46}}$     &
        $ 58.85 _{\textcolor{black!60}{\pm 0.66}}$     & 
        $ 53.82 _{\textcolor{black!60}{\pm 0.71}}$     & 
        $ 42.91 _{\textcolor{black!60}{\pm 0.99}}$    \\
        
        \midrule
        
        \rowcolor{green!20} \sassha         & 
        $ \textbf{94.09} _{\textcolor{black!60}{\pm 0.24}}$    &
        $ \textbf{92.49} _{\textcolor{black!60}{\pm 0.11}}$    &
        $ \textbf{90.29} _{\textcolor{black!60}{\pm 0.11}}$    &
        $ \textbf{86.50} _{\textcolor{black!60}{\pm 0.08}}$    &
        $ \textbf{72.14} _{\textcolor{black!60}{\pm 0.16}}$    &
        $ \textbf{66.78} _{\textcolor{black!60}{\pm 0.47}}$    &
        $ \textbf{61.97} _{\textcolor{black!60}{\pm 0.27}}$    &
        $ \textbf{53.98} _{\textcolor{black!60}{\pm 0.57}}$    \\ 
        
        
        % \rowcolor{green!20} \msassha        &  
        % $ \textbf{93.18} _{\textcolor{black!60}{\pm 0.23}}$    &
        % $ \textbf{91.27} _{\textcolor{black!60}{\pm 0.31}}$    &
        % $ \textbf{88.85} _{\textcolor{black!60}{\pm 0.31}}$    &
        % $ \textbf{85.17} _{\textcolor{black!60}{\pm 0.24}}$    &
        % $ \textbf{70.93} _{\textcolor{black!60}{\pm 0.21}}$    &
        % $ \textbf{66.10} _{\textcolor{black!60}{\pm 0.26}}$    &
        % $ \textbf{61.13} _{\textcolor{black!60}{\pm 0.28}}$    &
        % $ \textbf{52.45} _{\textcolor{black!60}{\pm 0.34}}$    \\
        \bottomrule
    \end{tabular}}
\end{table}



\section{\msassha: Efficient Perturbation} \label{app:msassha}
Having explored techniques to reduce the computational cost of second-order methods, here we consider employing techniques to alleviate the additional gradient computation in sharpness-minimization.
Prior works have suggested different ways to reduce this computational overhead including infrequent computations \citep{looksam}, use of sparse perturbations \citep{mi2022make}, or computing with selective weight and data \citep{esam}.
In particular, we employ the approaches of \citet{becker2024momentum}, which uses the normalized negative momentum as the perturbation:
\begin{equation}\label{eq:lsam-perturb}
    \epsilon^\star_t = \rho\frac{m_{t-1}}{\|m_{t-1}\|_2},
\end{equation}
which entirely eliminates the need for additional gradient computation with similar generalization improvement as the original SAM.
We call this low-computation alternative as \msassha and evaluate this across vision, language, and label noise tasks, as we did in the main sections. 
The results are presented in \cref{tab:im_cls_results_msassha,tab:language_msassha,tab:noise_label_msassha}, respectively. 

Despite having a computational cost comparable to first-order methods like SGD and Adam, and significantly lower than approximate second-order methods, \msassha demonstrates superior performance over both first-order and second-order approaches.
In image classification, \msassha proves more effective than the best-performing approximate second-order methods by 2\% on CIFAR-100 with ResNet-32 and by 2.5\% with ResNet-50, while also exceeding AdamW by approximately 1.6\% on ViT.
For language pretraining, it attains a test perplexity that is 22 points lower than the second-best performinig Sophia-H  and outperforms AdamW in nearly all language tasks.
Lastly, \msassha surpasses other methods across all noise levels, proving highly resilient in the presence of extreme label noise.
These results reaffirm the effectiveness and consistency of our well-engineered design choices, which enable the stable integration of efficient sharpness minimization into second-order optimization while retaining its benefits.

\begin{table*}[ht!]
    \vspace{-1em}
    \centering
    \caption{\msassha $\text{v.s.}$ baselines in image classification. \msassha shows superior performance.}
    \vskip 0.1in
    \resizebox{0.8\linewidth}{!}{
        \begin{tabular}{clcccccc}
        \toprule
         & 
         & \multicolumn{2}{c}{CIFAR-10} 
         & \multicolumn{2}{c}{CIFAR-100} 
         & \multicolumn{2}{c}{ImageNet} \\
         \cmidrule(l{3pt}r{3pt}){3-4} \cmidrule(l{3pt}r{3pt}){5-6} \cmidrule(l{3pt}r{3pt}){7-8}
         \multicolumn{1}{c}{ Category }
         & \multicolumn{1}{c}{ Method }
         & \multicolumn{1}{c}{ ResNet-20 } 
         & \multicolumn{1}{c}{ ResNet-32 } 
         & \multicolumn{1}{c}{ ResNet-32 }  
         & \multicolumn{1}{c}{ WRN-28-10} 
         & \multicolumn{1}{c}{ ResNet-50 } 
         & \multicolumn{1}{c}{ ViT-s-32} \\ \midrule

        
       \multirow{2}{*}{First-order}  & SGD       & $ 92.03 _{ \textcolor{black!60}{\pm 0.32} } $ 
        & $ 92.69 _{\textcolor{black!60}{\pm 0.06} } $ 
        & $ 69.32 _{\textcolor{black!60}{\pm 0.19} } $ 
        & $ 80.06 _{\textcolor{black!60}{\pm 0.15} } $ 
        & $ 75.58 _{\textcolor{black!60}{\pm 0.05} } $ 
        & $62.90 _{\textcolor{black!60}{\pm 0.36}}$  \\

        & AdamW      & $ 92.04 _{\textcolor{black!60}{\pm 0.11} } $ & $ 92.42 _{\textcolor{black!60}{\pm 0.13} } $ & $ 68.78 _{\textcolor{black!60}{\pm 0.22} } $  & $ 79.09 _{\textcolor{black!60}{\pm 0.35} } $ & $ 75.38 _{\textcolor{black!60}{\pm 0.08} } $ & $ 66.46 _{\textcolor{black!60}{\pm 0.15} } $ \\
        
        % & SAM $_{\text{SGD}}$ & $ 92.847 _{\textcolor{black!60}{\pm 0.07} } $ 
        % & $ 93.893 _{\textcolor{black!60}{\pm 0.13} } $ 
        % &  $ 71.993 _{\textcolor{black!60}{\pm 0.20} } $ 
        % & $ 83.136 _{\textcolor{black!60}{\pm 0.13} } $ 
        % & $ 76.355 _{\textcolor{black!60}{\pm 0.16} } $ 
        % & $ 64.543_{\textcolor{black!60}{\pm 0.63} } $  \\
        
        % & SAM $_{\text{AdamW}}$ &  $92.767 _{\textcolor{black!60}{\pm 0.29} } $ 
        % & $ 93.450 _{\textcolor{black!60}{\pm 0.24} } $ 
        % &  $ 71.153 _{\textcolor{black!60}{\pm 0.37} } $ 
        % & $ 82.880 _{\textcolor{black!60}{\pm 0.31} } $ 
        % & $  _{\textcolor{black!60}{\pm 0.16} } $ 
        % & $ 68.307 _{\textcolor{black!60}{\pm 0.17} } $  \\

        \midrule
        
        \multirow{4.5}{*}{Second-order}                   &
        AdaHessian                                      &
        $ 92.00 _{\textcolor{black!60}{\pm 0.17} } $    &
        $ 92.48 _{\textcolor{black!60}{\pm 0.15} } $    &
        $ 68.06 _{\textcolor{black!60}{\pm 0.22} } $    &
        $ 76.92 _{\textcolor{black!60}{\pm 0.26} } $    &
        $ 73.64 _{\textcolor{black!60}{\pm 0.16} } $    &
        $ 66.42 _{\textcolor{black!60}{\pm 0.23} } $   \\
        
        & Sophia-H   &
        $ 91.81 _{\textcolor{black!60}{\pm 0.27} } $    &
        $ 91.99 _{\textcolor{black!60}{\pm 0.08} } $    &
        $ 67.76 _{\textcolor{black!60}{\pm 0.37} } $    &
        $ 79.35 _{\textcolor{black!60}{\pm 0.24} } $    &
        $ 72.06 _{\textcolor{black!60}{\pm 0.49} } $    &
        $62.44 _{\textcolor{black!60}{\pm 0.36}}$      \\
        
        & Shampoo    &
        $88.55 _ {\textcolor{black!60}{\pm 0.83}}$   &
        $90.23 _{\textcolor{black!60}{\pm 0.24}}$    &
        $64.08 _{\textcolor{black!60}{\pm 0.46}}$    &
        $74.06 _{\textcolor{black!60}{\pm 1.28}}$    &
        $*$                                          &
        $*$  \\
        
        \cmidrule(l{3pt}r{3pt}){2-8}
        
        \rowcolor{green!20} \cellcolor{white} &  
        \msassha   &
        $ \textbf{92.36} _{\textcolor{black!60}{\pm 0.23} } $   
        & $ \textbf{93.18} _{\textcolor{black!60}{\pm 0.30} } $ 
        & $ \textbf{70.93} _{\textcolor{black!60}{\pm 0.21} } $ 
        & $ \textbf{81.53} _{\textcolor{black!60}{\pm 0.27} } $ 
        & $ \textbf{76.00} _{\textcolor{black!60}{\pm 0.04}}$  
        & $ \textbf{68.04} _{\textcolor{black!60}{\pm 0.14}}$ \\
        
        \bottomrule
        \end{tabular}
    }
    \vspace{-1em}
    \label{tab:im_cls_results_msassha}
    %
\end{table*}

\begin{table}[ht!]
    \centering
    \caption{
    \msassha $\text{v.s.}$ baselines in language tasks. For pretraining, \msassha achieves the lowest perplexity among all methods. For finetuning, \msassha performs better than AdamW and compares competitively with Sophia-H.
    }
    \vskip 0.1in
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lc}
            \toprule
             & \multicolumn{1}{c}{\textbf{Pretrain} / GPT1-mini} \\
             \cmidrule(l{3pt}r{3pt}){2-2}
             & Wikitext-2 \\
             & \texttt{Perplexity}\\
            \midrule
            
            AdamW & $ 175.06 $ \\
            % SAM _{\text{AdamW}} & $ 158.06 $ \\
            AdaHessian & $ 407.69 $ \\
            Sophia-H & $ 157.60 $ \\
            \midrule \rowcolor{green!20} \msassha &
            $ \textbf{125.01} $ \\
            
            \bottomrule
        \end{tabular}
        \begin{tabular}{|ccccccc}
            \toprule
                         \multicolumn{7}{|c}{\textbf{Finetune} / SqeezeBERT} \\
                         \cmidrule(l{3pt}r{3pt}){1-7}
                         SST-2 &  MRPC & STS-B & QQP & MNLI & QNLI & RTE \\
             \texttt{Acc} &  \texttt{Acc / F1}  & \texttt{S/P corr.} & \texttt{F1 / Acc} & \texttt{mat/m.mat} &  \texttt{Acc} &  \texttt{Acc} \\
            \midrule
            
            %AdamW         & 
            $ 90.29 _{\textcolor{black!60}{\pm 0.52}} $ 
            & $ 84.56 _{ \textcolor{black!60}{\pm 0.25} } $ / $ 88.99 _{\textcolor{black!60}{\pm 0.11}} $ 
            & $ 88.34 _{\textcolor{black!60}{\pm 0.15}} $ / $ 88.48 _{\textcolor{black!60}{\pm 0.20}} $ 
            & $ 89.92 _{\textcolor{black!60}{\pm 0.05}} $ / $ 86.58 _{\textcolor{black!60}{\pm 0.11}} $ 
            & $ 81.22 _{\textcolor{black!60}{\pm 0.07}} $ / $ 82.26 _{\textcolor{black!60}{\pm 0.05}} $ 
            & $ 89.93 _{\textcolor{black!60}{\pm 0.14}} $ 
            & $ 68.95 _{\textcolor{black!60}{\pm 0.72}} $  \\
    
            %SAM _{\text{AdamW}}   &
            % $ \textbf{90.52} _{\textcolor{black!60}{\pm 0.27}} $ 
            % & $ 83.25 _{\textcolor{black!60}{\pm 2.79}} $ / $ 87.90 _{\textcolor{black!60}{\pm 2.21}} $ 
            % & $ 88.38 _{\textcolor{black!60}{\pm 0.01}} $ / $ 88.79 _{\textcolor{black!60}{\pm 0.99}} $ 
            % & $ 90.26 _{\textcolor{black!60}{\pm 0.28}} $ / $ 86.99 _{\textcolor{black!60}{\pm 0.31}} $ 
            % & $ 81.56 _{\textcolor{black!60}{\pm 0.18}} $ / $ \textbf{82.46} _{\textcolor{black!60}{\pm 0.19}} $ 
            % & $ \textbf{90.38} _{\textcolor{black!60}{\pm 0.05}} $ 
            % & $ 68.83 _{\textcolor{black!60}{\pm 1.46}} $  \\
    
            %AdaHessian    & 
            $ 89.64 _{\textcolor{black!60}{\pm 0.13}} $ 
            & $ 79.74 _{\textcolor{black!60}{\pm 4.00}} $ / $ 85.26 _{\textcolor{black!60}{\pm 3.50}} $ 
            & $ 86.08 _{\textcolor{black!60}{\pm 4.04}} $ / $ 86.46 _{\textcolor{black!60}{\pm 4.06}} $ 
            & $ 90.37 _{\textcolor{black!60}{\pm 0.05}} $ / $ 87.07 _{\textcolor{black!60}{\pm 0.05}} $ 
            & $ 81.33 _{\textcolor{black!60}{\pm 0.17}} $ / $ 82.08 _{\textcolor{black!60}{\pm 0.02}} $ 
            & $ 89.94 _{\textcolor{black!60}{\pm 0.12}} $ 
            & $ \textbf{71.00} _{\textcolor{black!60}{\pm 1.04}} $ \\
            
            % Sophia-H  &
            $ \textbf{90.44} _{\textcolor{black!60}{\pm 0.46}} $ 
            & $ 85.78 _{\textcolor{black!60}{\pm 1.07}} $ / $ 89.90 _{\textcolor{black!60}{\pm 0.82}} $ 
            & $ 88.17 _{\textcolor{black!60}{\pm 1.07}} $ / $ \textbf{88.53} _{\textcolor{black!60}{\pm 1.13}} $ 
            & $ 90.70 _{\textcolor{black!60}{\pm 0.04}} $ / $ 87.60 _{\textcolor{black!60}{\pm 0.06}} $ 
            & $ \textbf{81.77} _{\textcolor{black!60}{\pm 0.18}} $ / $ \textbf{82.36} _{\textcolor{black!60}{\pm 0.22}} $ 
            & $ \textbf{90.12}_{\textcolor{black!60}{\pm 0.14}} $ 
            & $ 70.76 _{\textcolor{black!60}{\pm 1.44}} $  \\
            
            \midrule
            \rowcolor{green!20} 
             $ 90.332 _{\pm 0.88} $ 
             & $ \textbf{87.092} _{\pm 1.98} $ / $ \textbf{90.599} _{\pm 1.51} $ 
             & $ \textbf{88.37} _{\pm 0.04} $ / $  88.46 _{\pm 0.07} $ 
             & $ \textbf{90.78} _{\pm 0.05} $ / $ \textbf{87.61} _{\pm 0.07} $ 
             & $ 81.42 _{\pm 0.19} $ / $ 81.94 _{\pm 0.09} $ 
             & $ 89.84 _{\pm 0.22} $ 
             & $ 70.40 _{\pm 0.96} $ \\
            
            
            %\sassha & 
            % $ 90.44 _{\textcolor{black!60}{\pm 0.98}} $ 
            % & $ \textbf{86.28} _{\textcolor{black!60}{\pm 0.28}} $ / $ \textbf{90.13} _{\textcolor{black!60}{\pm 0.161}} $ 
            % & $ \textbf{88.72} _{\textcolor{black!60}{\pm 0.75}} $ / $ \textbf{89.10} _{\textcolor{black!60}{\pm 0.70}} $ 
            % & $ \textbf{90.91} _{\textcolor{black!60}{\pm 0.06}} $ / $ \textbf{87.85} _{\textcolor{black!60}{\pm 0.09}} $ 
            % & $ \underline{81.61} _{\textcolor{black!60}{\pm 0.25}} $ / $ 81.71 _{\textcolor{black!60}{\pm 0.11}} $ 
            % & $ 89.85_{\textcolor{black!60}{\pm 0.20}} $ 
            % & $ \textbf{72.08} _{\textcolor{black!60}{\pm 0.55}} $  \\
            
            \bottomrule
        \end{tabular}
        \hspace{0.1em}
    }
    \label{tab:language_msassha}
\end{table}

\begin{table}[ht!]
    \vspace{-1em}
    \centering
    \caption{Robustness to label noise. Here we measure the validation accuracy under various levels of label noise using ResNet-32 trained on CIFAR-100 and CIFAR-10. \msassha shows much robust performance under label noise.}
    \label{tab:noise_label_msassha}
    \resizebox{0.9\linewidth}{!}{%
    \begin{tabular}{lccccccccc}
        \toprule
        & \multicolumn{4}{c}{CIFAR-10} & \multicolumn{4}{c}{CIFAR-100} \\ 
        \cmidrule(l{3pt}r{3pt}){2-5} \cmidrule(l{3pt}r{3pt}){6-9} 
        Noise level & {0\%} & {20\%} & {40\%} & {60\%} & {0\%} & {20\%} & {40\%} & {60\%} \\ 
        \midrule
        SGD                 & 
        $ 92.69 _{\textcolor{black!60}{\pm 0.06}}$     &
        $ 89.91 _{\textcolor{black!60}{\pm 0.87}}$     &
        $ 87.26 _{\textcolor{black!60}{\pm 0.40}}$     &
        $ 82.72 _{\textcolor{black!60}{\pm 1.59}}$     &
        $ 69.32 _{\textcolor{black!60}{\pm 0.19}}$     &
        $ 62.18 _{\textcolor{black!60}{\pm 0.06}}$     & 
        $ 55.78 _{\textcolor{black!60}{\pm 0.55}}$     &  
        $ 45.53 _{\textcolor{black!60}{\pm 0.78}}$     \\
        
        % SAM $_{\text{SGD}}$ & 
        % $ 93.89 _{\textcolor{black!60}{\pm 0.13}}$     &
        % $ 92.27 _{\textcolor{black!60}{\pm 0.14}}$     &
        % $ 90.11 _{\textcolor{black!60}{\pm 0.25}}$     &
        % $ 85.79 _{\textcolor{black!60}{\pm 0.30}}$     &
        % $ 71.99 _{\textcolor{black!60}{\pm 0.20}}$     &
        % $ 65.53 _{\textcolor{black!60}{\pm 0.11}}$     & 
        % $ 61.20 _{\textcolor{black!60}{\pm 0.17}}$     &
        % $ 51.93 _{\textcolor{black!60}{\pm 0.47}}$     \\ 
        
        
        AdaHessian         & 
        $ 92.48 _{\textcolor{black!60}{\pm 0.15}}$     &
        $ 90.11 _{\textcolor{black!60}{\pm 0.01}}$     & 
        $ 86.88 _{\textcolor{black!60}{\pm 0.04}}$     &
        $ 83.25 _{\textcolor{black!60}{\pm 0.01}}$     &
        $ 68.06 _{\textcolor{black!60}{\pm 0.22}}$     &
        $ 63.06 _{\textcolor{black!60}{\pm 0.25}}$     & 
        $ 58.37 _{\textcolor{black!60}{\pm 0.13}}$     & 
        $ 46.02 _{\textcolor{black!60}{\pm 1.96}}$     \\


        Sophia-H           & 
        $ 91.99 _{\textcolor{black!60}{\pm 0.08}}$     &
        $ 89.93 _{\textcolor{black!60}{\pm 0.01}}$     &
        $ 87.30 _{\textcolor{black!60}{\pm 0.51}}$     & 
        $ 82.78 _{\textcolor{black!60}{\pm 1.43}}$     &
        $ 67.76 _{\textcolor{black!60}{\pm 0.37}}$     &
        $ 62.34 _{\textcolor{black!60}{\pm 0.47}}$     & 
        $ 56.54 _{\textcolor{black!60}{\pm 0.28}}$     & 
        $ 45.37 _{\textcolor{black!60}{\pm 0.27}}$     \\
        
        
        Shampoo           &  
        $ 90.23 _{\textcolor{black!60}{\pm 0.83}}$     &
        $ 88.14 _{\textcolor{black!60}{\pm 0.29}}$     & 
        $ 85.15 _{\textcolor{black!60}{\pm 0.61}}$     & 
        $ 81.16 _{\textcolor{black!60}{\pm 0.30}}$     &
        $ 64.08 _{\textcolor{black!60}{\pm 0.46}}$     &
        $ 58.85 _{\textcolor{black!60}{\pm 0.66}}$     & 
        $ 53.82 _{\textcolor{black!60}{\pm 0.71}}$     & 
        $ 42.91 _{\textcolor{black!60}{\pm 0.99}}$    \\
        
        \midrule
        
        % \rowcolor{green!20} \sassha         & 
        % $ \textbf{94.09} _{\textcolor{black!60}{\pm 0.24}}$    &
        % $ \textbf{92.49} _{\textcolor{black!60}{\pm 0.11}}$    &
        % $ \textbf{90.29} _{\textcolor{black!60}{\pm 0.11}}$    &
        % $ \textbf{86.50} _{\textcolor{black!60}{\pm 0.08}}$    &
        % $ \textbf{72.14} _{\textcolor{black!60}{\pm 0.16}}$    &
        % $ \textbf{66.78} _{\textcolor{black!60}{\pm 0.47}}$    &
        % $ \textbf{61.97} _{\textcolor{black!60}{\pm 0.27}}$    &
        % $ \textbf{53.98} _{\textcolor{black!60}{\pm 0.57}}$    \\ 
        
        
        \rowcolor{green!20} \msassha        &  
        $ \textbf{93.18} _{\textcolor{black!60}{\pm 0.23}}$    &
        $ \textbf{91.27} _{\textcolor{black!60}{\pm 0.31}}$    &
        $ \textbf{88.85} _{\textcolor{black!60}{\pm 0.31}}$    &
        $ \textbf{85.17} _{\textcolor{black!60}{\pm 0.24}}$    &
        $ \textbf{70.93} _{\textcolor{black!60}{\pm 0.21}}$    &
        $ \textbf{66.10} _{\textcolor{black!60}{\pm 0.26}}$    &
        $ \textbf{61.13} _{\textcolor{black!60}{\pm 0.28}}$    &
        $ \textbf{52.45} _{\textcolor{black!60}{\pm 0.34}}$    \\
        
        \bottomrule
    \end{tabular}}
\end{table}
