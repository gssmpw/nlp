\section{Regret bounds with sequential complexities in classical online learning}\label{rademacher}

Notions of complexity for a given hypothesis class have traditionally been studied within the batch learning framework and are often characterized by the Rademacher complexity \citep{Rademacher}.

\begin{definition}[Rademacher complexity]
    Let $\mathcal{X}$ be a sample space with an associated distribution $\mathcal D$ and $\mathcal{H}$ be the hypothesis class. Let $(x_j)_{j\in[m]} \sim \mathcal D^m$ be a sequence of samples, sampled i.i.d. from $\mathcal{X}$. The Rademacher complexity can then be defined as:
    \begin{equation*}
\label{eq:rad_comp}
    \mathcal{R}_m (\mathcal{H}) = \mathop{\mathbb{E}}_{(x_j)\sim \mathcal D^m} \Big[\frac{1}{m} \mathop{\mathbb{E}}_{\boldsymbol{\epsilon}} \Big[ \sup_{h \in \mathcal{H}} \sum_{j=1}^m \epsilon_j h(x_j) \Big]
 \Big],
\end{equation*}
where $\boldsymbol{\epsilon} = (\epsilon_1, \cdots, \epsilon_m)$ are called Rademacher variables, that satisfy $P(\epsilon = +1) = P(\epsilon = -1) = 1/2$.
\end{definition}
Perhaps not surprisingly, in addition to being an indicator for expressivity of a given hypothesis class, Rademacher complexity also upper bounds generalization error in the setting of batch learning \citep{Rademacher}.

Rademacher complexity generalises to sequential Rademacher complexity in the online setting \citep{rakhlin2015online}. In order to define sequential Rademacher complexity let us first define a $\mathcal{X}$-valued complete binary tree. 

\begin{definition}[Sequential Rademacher complexity]
    Let $\mathbf{x}$ be a $\mathcal{X}$-valued complete binary tree of depth $T$. The sequential Rademacher complexity of a hypothesis class $\mathcal{H}$ on the tree $\mathbf{x}$ is then given as:
    \begin{equation*}
    \label{eq:seq_rad_comp}
        \mathfrak{R}_T (\mathcal{H}, \mathbf{x}) = \Big[\frac{1}{T} \mathop{\mathbb{E}}_{\boldsymbol{\epsilon}} \Big[ \sup_{h \in \mathcal{H}} \sum_{t=1}^T \epsilon_t h(\mathbf{x}_t(\boldsymbol{\epsilon})) \Big]
    \Big].
    \end{equation*}
\end{definition}

The $\mathbf{x}$ dependence of the sequential Rademacher complexity can be subsequently removed by considering the supremum over all $\mathcal{X}$-valued trees of depth $T$: $\mathfrak{R}_T (\mathcal{H}) = \sup_{\mathbf{x}} \mathfrak{R}_T(\mathcal{H}, \mathbf{x})$. Similar to how Rademacher complexity upper bounds the generalization error, the sequential Rademacher complexity was shown to upper bound the minimax regret \citep{rakhlin2015online}. For the case of supervised learning, the following relation holds: 
\begin{equation}
\label{eq:reg_rad}
    \mathcal{V}_T \leq 2 L T \mathfrak{R}_T(\mathcal{H}).
\end{equation}
Here, $L$ comes from the fact that the loss function considered is $L$-Lipschitz. 

The growth of sequential Rademacher complexities has been shown to be influenced by other related notions of sequential complexities. One prominent example is the sequential fat-shattering dimension \citep{rakhlin2015online}. It was shown in \citet{rakhlin2015online} that this dimension serves as an upper bound to the sequential Rademacher complexity, which subsequently provides a bound on the minimax regret as per \cref{eq:reg_rad}. Similarly, recall that the minimax regret can be lower bounded by the sequential fat-shattering dimension (\cref{eq:Regret_LB}), provided that $\ell_t (h_t(x_t), y_t) = \vert h_t(x_t) - y_t \vert$ and that $\mathcal{P}$ is taken to be the whole set of all distributions on $\mathcal{X}$. In fact, it is also lower bounded by the Rademacher complexity \citep{rakhlin2015online, rakhlin2015sequential}. 
\begin{eqnarray}
\label{eq:Regret_LB_bis}
    \mathcal{V}_T &\geq& \Big< \sup_{\mathcal{D}_t \in \mathcal{P}} \ \mathop{\mathbb{E}}_{x_t \sim \mathcal{D}_t} \Big>_{t=1}^T  \ \mathop{\mathbb{E}}_{\boldsymbol{\epsilon}} \Big[ \sup_{h \in \mathcal{H}} \sum_{t=1}^T \epsilon_t h(\mathbf{x}_t(\boldsymbol{\epsilon})) \Big] \nonumber \\
    &\geq& \frac{1}{4 \sqrt{2}} \sup_{\delta > 0} \Big\{ \sqrt{\delta^2 T \min\{ \text{sfat}_\delta (\mathcal{H}, \mathcal{X}), T\}} \Big\}.
\end{eqnarray}
where ${\boldsymbol{\epsilon}} = (\epsilon_1, \epsilon_2, \cdots, \epsilon_T)$ are Rademacher variables. 