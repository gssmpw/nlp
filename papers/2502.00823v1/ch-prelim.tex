
\subsection{Online Learning}

Online learning, or the sequential prediction model, is a $T$ round repeated two-player game \citep{online_book}. In each round $t \in [T]$ of the game, the learner is presented with an input from the sample space $x_t \in \mathcal{X}$. Without any loss of generality, we can assume that $x_t$ is sampled from a distribution $\mathcal{D}_t(\mathcal{X})$ where $\mathcal{D}_t$ may be chosen adversarially. The learner's goal is to learn an unknown function $f: \mathcal{X} \rightarrow \mathcal{Y}$ from the data they receive. Here, $\mathcal{Y}$ denotes the space of possible labels for each input $x_t$. The learning proceeds by designing an algorithm that outputs a sequence of functions $h_t:\mathcal{X} \rightarrow \mathcal{Y}$ chosen from a hypothesis class $\mathcal H$. After each round, the learner incurs a loss and aims to minimize the cumulative regret at the end of all $T$ rounds of the game \citep{cesa1997_expert, arora2012multiplicative}:

\begin{definition}[Regret]
\label{def:regret}
    Let $\mathcal{X}$ denote the sample space, $\mathcal{Y}$ its associated label space, and $\mathcal{H}$ the hypothesis class. In each round $t \in [T]$ of the online learning process, the learner incurs a loss $\ell_t (h_t(x_t), y_t)$, where $y_t \in \mathcal{Y}$ is the true label associated to $x_t$. The regret is then defined as:
    \begin{equation*}
    R_T = \sum_{t=1}^T \ell_t (h_t (x_t), y_t) - \inf_{h \in \mathcal{H}} \sum_{t=1}^T \ell_t(h(x_t), y_t).
    \end{equation*}
\end{definition}

Note here that the hypothesis class $\mathcal{H}$ may or may not contain the target function $f$. For a given pair $(\mathcal{H}, \mathcal{X})$, if the regret grows sublinearly in T, we have
\begin{equation}
    \lim_{T \rightarrow \infty} \  \frac{1}{T}R_T = 0,
\end{equation}
{\it i.e.} the online decision is as good as the offline decision asymptotically.
We say a problem is online learnable for such cases. An alternate figure of merit that is often considered in the literature is the minimax regret \citep{rakhlin2015online}.

\begin{definition}[Minimax regret]
    Consider the online learning setting described in \cref{def:regret}. Let $\mathcal{P}$ and $\mathcal{Q}$ be sets of probability measures defined on $\mathcal{X}$ and $\mathcal{H}$ respectively. The minimax regret is then defined as:
\begin{equation*}
    \label{eq:minmax_reg}
    \begin{aligned}
     \mathcal{V}_T = &\Big< \inf_{\mathcal{Q} \in \Delta(\mathcal{H})} \ \sup_{y_t} \  \sup_{\mathcal{D}_t \in \mathcal{P}} \  \mathop{\mathbb{E}}_{h_t \sim \mathcal{Q}} \  \mathop{\mathbb{E}}_{x_t \sim \mathcal{D}_t} \Big>_{t=1}^T \\&\Big[ \sum_{t=1}^T \ell_t (h_t (x_t), y_t) - \inf_{h \in \mathcal{H}} \sum_{t=1}^T \ell_t(h(x_t), y_t) \Big],
     \end{aligned}
\end{equation*}
    where $\big< \cdot \big>_{t=1}^T$ denotes iterated application of the enclosed operators. 
\end{definition}
The question of learnability of an online learning problem can then be reduced to the study of $\mathcal{V}_T$. Given a pair $(\mathcal{H}, \mathcal{X})$, a problem is said to be online learnable if and only if $\lim_{T \rightarrow \infty} \mathcal{V}_T/T = 0$.

\subsection{Online Learning of Quantum States}

An $n$-qubit quantum state can be written as a density matrix $\rho \in \{\omega \in\mathrm{Herm}_{\mathbb{C}} (2^n); \ \operatorname{Tr}(\omega) = 1, \ \omega \succeq 0\}$. In the context of quantum state learning, the sample space $\mathcal{X}$ is composed of two-outcome quantum measurements, represented by two-element positive operator-valued measure (POVM) $\{E, \mathbf{1} - E\}$, where $E\in\mathrm{Herm}_{\mathbb{C}}(2^n)$ and $\operatorname{Spec}(E) \subset[0, 1]$. Since the second element of the POVM is uniquely determined by the first, a two-outcome measurement can effectively be represented by a single operator $E$. Accordingly, we define the sample space as $\mathcal{X} \subset \{E\in\mathrm{Herm}_{\mathbb{C}}(2^n), \operatorname{Spec}(E) \subset[0, 1]\}$.

A measurement $E$  is said to {\it accept} a quantum state $\rho$ with probability $\operatorname{Tr}(E\rho)$ and {\it reject} it with probability $1 - \operatorname{Tr}(E\rho)$. For a given quantum state $\rho$, predicting its acceptance probabilities for all measurements $E$ is tantamount to characterizing it completely. Hence learning a quantum state $\rho$ is equivalent to learning the function $\operatorname{Tr}_\rho \colon \mathcal X \rightarrow [0,1]$, defined as $\operatorname{Tr}_\rho (E) = \operatorname{Tr}(E \rho)$. Therefore, denoting $\mathcal C_n$ as the set of all $n$-qubit quantum states, we  set the hypothesis class to be $\mathcal{H}_n=\{\operatorname{Tr}_\omega, \omega\in\mathcal C_n\}$, and $\mathcal Q$ can be seen as a distribution over $\mathcal C_n$. Here, the learner receives a sequence of measurements  $(E_t)_{t \in [T]}$, each drawn from a distribution $\mathcal{D}_t$ (chosen adversarially) one at a time. Upon receiving each measurement, the learner selects a hypothesis $\omega_t \in \mathcal{C}_n$ and thereby incurs a loss of $\ell_t (\operatorname{Tr}_{\omega_t} (E_t), \operatorname{Tr}_{\rho} (E_t)) = \ell_t (\operatorname{Tr} (E_t\omega_t ), \operatorname{Tr}(E_t\rho ))$. Drawing parallels to \cref{sec:related}, $\operatorname{Tr}(E_t\rho )$ can be seen as the label associated to each measurement $E_t$, leading to the label space $\mathcal Y =[0,1]$. Note that for a fixed state $\rho$, the label $\operatorname{Tr} (E_t \rho)$ is completely determined by $E_t$, assuming ideal situations where $\operatorname{Tr} (E_t \rho)$ can be determined perfectly. Under this scenario, we introduce a slightly modified version of the minimax regret than the one considered in \cref{eq:minmax_reg} as the figure of merit:
\begin{equation}
    \label{eq:minmax_reg_no_label}
    \begin{aligned}
         \Bar{\mathcal{V}}_T = &\Big< \inf_{\mathcal{Q} \in \Delta(\mathcal{C}_n)} \ \sup_{\mathcal{D}_t \in \mathcal{P}} \   \mathop{\mathbb{E}}_{{\omega_t} \sim \mathcal{Q}} \  \mathop{\mathbb{E}}_{E_t \sim \mathcal{D}_t} \Big>_{t=1}^T \\
         &\Big[ \sum_{t=1}^T \ell_t (\operatorname{Tr} (E_t\omega_t ), \operatorname{Tr}(E_t\rho )) \\&- \inf_{\omega \in \mathcal{C}_n} \sum_{t=1}^T \ell_t (\operatorname{Tr} (E_t\omega ), \operatorname{Tr}(E_t\rho )) \Big].
    \end{aligned}
\end{equation}
The key difference here is that the label associated to each $E_t$ is not chosen adversarially as was the case in \cref{eq:minmax_reg}. Nevertheless, if estimating $\operatorname{Tr} (E_t \rho)$ is only approximate (due to the presence of noise or finite number of measurement shots), one can still use $\mathcal{V}_T$ as the figure of merit with $y_t \in [0,1]$. It is easy to see that $\Bar{\mathcal{V}}_T \leq \mathcal{V}_T$. We say that the problem is online learnable if, for any adversarially chosen sequence of measurements  $(E_t)_{t\in [T]}$, there exists a strategy $(\mathcal{Q}_t)_{t \in [T]}$ for which the minimax regret grows sub-linearly with respect to $T$.

\subsection{Sequential fat-shattering dimension}

The main notion we will be studying in this paper is that of sequential fat-shattering dimension. 

\begin{definition}[Sequential fat-shattering dimension]

A $\mathcal{X}$-valued complete binary tree $\mathbf{x}$ of depth $T$ is deemed to be $\delta$-shattered by a hypothesis class $\mathcal{H}$ if there exists a $\mathbb{R}$-valued complete binary tree $\mathbf{v}$ of same depth $T$ such that for all paths $\ \boldsymbol{\epsilon} \in \{\pm 1\}^{T-1}$,
\begin{equation*}
     \exists \ h \in \mathcal{H} \ : \ \forall t \in [T] \ \ \epsilon_t [h(\mathbf{x}_t(\boldsymbol{\epsilon})) - \mathbf{v}_t (\boldsymbol{\epsilon})] \geq \frac{\delta}{2}.
\end{equation*}
The sequential fat-shattering dimension at scale $\delta$, $\text{sfat}_\delta (\mathcal{H}, \mathcal{X})$, is defined to be the largest $T$ for which $\mathcal{H}$ $\delta$-shatters a $\mathcal{X}$-valued tree of depth $T$.
\end{definition}
Recall that a $\mathcal{X}$-valued complete binary tree of depth $T$, $\mathbf{x}$, is defined as a sequence of $T$ mappings $(\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_T)$, where $\mathbf{x}_t: \{\pm 1\}^{t-1} \rightarrow \mathcal{X}$, with a constant function $\mathbf{x}_1 \in \mathcal{X}$ as the root. In simpler terms the tree can be seen as a collection of $T$ length paths $\boldsymbol{\epsilon} = (\epsilon_1, \epsilon_2, \cdots, \epsilon_{T-1}) \in \{\pm1\}^{T-1}$ ($+1$ indicating right and $-1$ indicating left from any given node) and $\mathbf{x}_t(\boldsymbol{\epsilon}) \equiv \mathbf{x}_t (\epsilon_1, \cdots, \epsilon_{t-1}) \in \mathcal{X}$ denoting the label of the $t$-th node on the corresponding path $\mathbf{\epsilon}$. 

This dimension is a fundamental property in online learning, as it both upper and lower bounds regret \citep{rakhlin2015online, rakhlin2015sequential}, as shown in \cref{eq:fin_class_bnd,eq:Regret_LB}.
\begin{equation}
\label{eq:fin_class_bnd}
\begin{aligned}
    \mathcal{V}_T \leq & \inf_{\alpha > 0} \Big\{ 4 \alpha T L - \\ & 12 L \sqrt{T} \int_{\alpha}^1 \sqrt{\text{sfat}_\delta (\mathcal{H}, \mathcal{X}) \log \left(\frac{2 e T}{\delta} \right)} d\delta \Big\}.
\end{aligned}
\end{equation}
Note that this upper bound also holds for $\Bar{\mathcal{V}}_T$. The bound in \cref{eq:fin_class_bnd} was used in \citet{aaronson2019online} to derive the regret upper bounds for online quantum state learning. Similarly, the minimax regret can also be lower bounded by the sequential fat-shattering dimension, provided that $\ell_t (h_t(x_t), y_t) = \vert h_t(x_t) - y_t \vert$ and that $\mathcal{P}$ is taken to be the whole set of all distributions on $\mathcal{X}$. 
\begin{equation}
\label{eq:Regret_LB}
    \mathcal{V}_T \geq \frac{1}{4 \sqrt{2}} \sup_{\delta > 0} \Big\{ \sqrt{\delta^2 T \min\{ \text{sfat}_\delta (\mathcal{H}, \mathcal{X}), T\}} \Big\}.
\end{equation}
