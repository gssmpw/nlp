
\textbf{Lower bounds on fat-shattering dimension:} In this work, we established lower bounds on sequential fat-shattering dimension of various subproblems within the quantum state learning framework. Crucially, we showed that pure and mixed states almost share the same asymptotical dimension. Note that, although our construction directly implies that the regular $\delta$-fat-shattering dimension of pure states scales as $\Omega(\frac 1{\delta^2})$, whether we can recover $\Omega(\frac n{\delta^2})$ for pure $n$-qubits in the offline setting remains an open question.  

\textbf{Consequences on regret:} This lower bound on sequential fat-shattering dimension has several implications, including the key result that learning pure and mixed states in the online setting will incur the same asymptotical regret for the $L_1$-loss. However, there are no known tight lower bounds on regret for more general loss functions. Additionally, sequential fat-shattering dimension may serve as a fundamental tool for deriving bounds on other key complexity measures in quantum state learning.

\textbf{Smooth online learning:} Finally, we extend our analysis from standard online learning of quantum states to the smooth online learning setting. To our knowledge, this work represents the first application of smooth analysis to quantum state learning. In this setting, we establish an upper bound on the regret. However a key open question is whether this bound is tight.