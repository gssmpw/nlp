

The online learning framework discussed so far is fully adversarial, since the adversary is free to select any measurement at each round $t$. However, as seen in \cref{sec:intro}, the learner often aims to learn specific properties of $\rho$ rather than reconstructing it entirely in practical scenarios. In the PAC learning framework, such properties are captured by a fixed distribution $\mathcal D$ over the set of two-outcome measurements. We apply smooth analysis to extend these restrictions to the online setting, imposing the condition that the distributions $\mathcal{D}_t$ chosen by the adversary at every round must remain close to the original distribution $\mathcal D$.


\begin{definition}[Smooth distributions]
    A distribution $\mu$ is said to be $\sigma$-smooth with respect to a fixed distribution $\mathcal{D}$ for a $\sigma \in (0,1]$ if and only if \citep{smooth_distrib_def}:
\begin{enumerate}
    \item $\mu$ is absolutely continuous with respect to $\mathcal{D}$, {\it i.e.} every measurable set $A$ such that $\mathcal{D}(A) = 0$ satisfies $\mu(A) = 0$

    \item The Radon Nikodym derivative $d\mu/d\mathcal{D}$ satisfies the following relation:
\begin{equation}
    \text{ess} \sup \frac{d \mu}{d \mathcal{D}} \leq \frac{1}{\sigma}.
\end{equation}
\end{enumerate}
\end{definition}








Let $\mathcal{B}(\sigma, \mathcal{D})$ be the set of all $\sigma$-smooth distributions with respect to $\mathcal{D}$. In smoothed online learning, the adversary is restricted by the condition $\mathcal{D}_t \in \mathcal{B} (\sigma, \mathcal{D})$. Note that in this setting we recover the case of an oblivious adversary for $\sigma = 1$, while we get the completely adversarial case for $\sigma \rightarrow 0$. To establish regret bounds in smoothed online learning, \citet{Haghtalab2024, block2022smoothed} introduced the concept of coupling. The key idea here is that if the distributions $(\mathcal{D}_t)_{t=1}^T$ are $\sigma$-smooth with respect to $\mathcal{D}$, we may pretend that in expectation the data is sampled i.i.d from $\mathcal{D}$ instead of $(\mathcal{D}_t)_{t=1}^T$. For a more formal description, define $\mathcal{B}_T (\sigma, \mathcal{D})$ to be the set of joint distributions $\mathcal{D}_\wedge$ on $\mathcal{X}^T$, where each marginal distribution $\mathcal{D}_t(\cdot \vert x_1,...,x_{t-1})$ is conditioned on the previous draws. 

\begin{definition}[Coupling]
\label{def:coupling}
    A distribution $\mathcal{D}_\wedge \in \mathcal{B}_T (\sigma, \mathcal{D})$ is said to be coupled to independent random variables drawn according to $\mathcal{D}$ if there exists a probability measure $\Pi$ with random variables $(x_t, Z_t^j)_{t \in [T], j \in [k]}\sim\Pi$ satisfying the following conditions:
    \begin{enumerate}
        \item $x_t \sim \mathcal{D}_t (\cdot \vert x_1. x_2, \cdots, x_{t-1})$,
    
        \item $\{ Z_t^j \}_{t \in [T], j \in [k]} \sim \mathcal{D}^{\otimes kT}$,
    
        \item With probability at least $1 - T e^{-\sigma k}$, we have $x_t \in \{ Z_t^j \}_{ j \in [k]} \ \forall t \in [T]$. 
    \end{enumerate}
\end{definition}


The last relation is particularly interesting and is used to derive the regret bounds for smoothed online quantum state learning.


\subsection{Smooth online learning of quantum states}

Recall the expression of minimax regret in \cref{eq:minmax_reg}. In the smoothed setting, the expression gets slightly modified accounting for the restriction imposed on the adversary:
\begin{equation}
\begin{aligned}
    \label{eq:minmax_reg_smooth}
     \mathcal{V}_T =& \Big< \inf_{\mathcal{Q} \in \Delta(\mathcal{H})} \  \sup_{\mathcal{D}_t \in \mathcal{B}(\sigma, \mathcal{D})} \  \mathop{\mathbb{E}}_{h_t \sim \mathcal{Q}} \  \mathop{\mathbb{E}}_{x_t \sim \mathcal{D}_t} \Big>_{t=1}^T \\&\Big[ \sum_{t=1}^T \ell_t (h_t (x_t)) - \inf_{h \in \mathcal{H}} \sum_{t=1}^T \ell_t(h(x_t)) \Big].
     \end{aligned}
\end{equation}
Here, the key difference with \cref{eq:minmax_reg} is that $\mathcal{D}_t$ is now restricted to the set of all $\sigma$-smooth distributions with respect to $\mathcal{D}$ instead of all possible distributions on $\mathcal{X}$. We recall that for quantum state learning, we have $\mathcal{X} \subset \{E\in\mathrm{Herm}_{\mathbb{C}}(2^n), \operatorname{Spec}(E)\subset[0,1]\}$, $\mathcal{H}_n=\{\operatorname{Tr}_\omega, \omega\in\mathcal C_n\}$ and a target state $\rho$. For the sake of brevity, we will continue using the notation $x_t$ to indicate input data and $h$ to indicate the hypothesis. The derivation here closely follows the approach in \citet{block2022smoothed} (which derives the regret bounds for classical smoothed online supervised learning)  with one important difference; in the original derivation, for a given input $x_t \in \mathcal{X}$, the authors distinguish between a predicted label $\hat{y}_t \in \mathcal{Y}$ and $h_t(x_t) \in \mathcal{Y}$ where $\mathcal{Y}$ is the label space. We do not make this distinction as our labels are always related to our inputs via the hypothesis. 

 

\begin{theorem}
\label{theo:smooth_ub}
 Let $\mathcal{X} = \{E\in\mathrm{Herm}_{\mathbb{C}}(2^n), \operatorname{Spec}(E)\subset[0,1]\}$ be the sample space. Define $\mathcal{H}=\{\operatorname{Tr}_\omega, \omega\in\mathcal C_n, \mathrm{Tr}[\omega^2]=1\}$ as the hypothesis class, where $\mathcal{C}_n$ is the set of all $n$-qubit quantum states.  Furthermore let $\sigma \in (0,1]$ be the smoothness parameter. Then we have $\mathcal{V}_T = O\Bigg(\sqrt{\frac{nT \log T}{\sigma}} \ \Bigg).$
\end{theorem}

We prove this result in \cref{pf:smooth}.