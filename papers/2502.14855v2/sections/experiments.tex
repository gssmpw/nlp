This section contains a suite of experiments that validate the P2L method and demonstrate its utility.
In Section~\ref{sec:validation}, we show that P2L leads to gains in human preference prediction that scale with model size and data.
In Section~\ref{sec:validation}, we show direct predictive performance on pairwise human preferences, as well as scaling behavior with data size and parameter count.
In Section~\ref{sec:routing-experiments}, we show P2L allows for optimal cost-efficient routing via the algorithm developed previously in Section~\ref{sec:routing}.
In Section~\ref{sec:strengths-weaknesses}, we use P2L to automatically identify strengths and weaknesses for different models.
In Section~\ref{sec:aggregation-scaling}, we explore our aggregation technique against ground truth categories leaderboards, and observe data scaling trends.
Finally, in Section~\ref{sec:out-of-distribution}, we show that the P2L has reasonable performance on out-of-distribution data.

\subsection{Training setup}

To train a P2L model, we follow this three-step procedure:
\begin{enumerate}
    \item Begin with a pre-trained, instruction-tuned LLM.
    \item Remove the existing language model head and replace it with a randomly initialized \emph{coefficient head}. In the BT case, the coefficient head is a linear layer producing $M$ outputs, one per model.
    \item Train the model by running stochastic gradient descent to minimize the negative log-likelihood:
    \begin{equation}
        \mathcal{L}(\theta) = -\sum\limits_{i=1}^n \log \left( g_{\theta(Z_i)}(Y_i;X_i) \right).
    \end{equation}
\end{enumerate}
The result of this procedure is the trained model
\begin{equation}
    \hat\theta = \argmin_{\theta \in \Theta} \mathcal{L}(\theta),
\end{equation}
which is a direct generalization of~\eqref{eq:p2l-bt}.
We train on up to $n=1.5$ million crowdsourced human preference pairs from Chatbot Arena, containing $M=130$ unique models. Note that we find minimal left/right positional bias from voters.
We always train for 1 epoch. 
In order to study the scaling laws of P2L as a function of model size, we used the following models as the initializations: \texttt{SmolLM2-\{135, 360\}M-Instruct} and \texttt{Qwen2.5-\{0.5, 1.5, 3, 7\}B-Instruct}~\citep{allal2024SmolLM2, qwen2.5}.
We refer to our post-trained versions of these models as \texttt{P2L-\{135,360\}M} and \texttt{P2L-\{0.5,1.5,3,7\}B}, respectively.

\subsection{Feedback prediction}
\label{sec:validation}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/loss_metrics.pdf}
    \caption{\textbf{Loss metrics.} The line plot shows the validation loss as a function of the number of data points seen during training. The P2L models all substantially outperform the baselines, and performance scales with dataset and model size. The bar plots show the validation loss and mean squared error of the models trained on all 1.5M training points.}
    \label{fig:loss-metrics}
    \vspace{-5.0pt}
\end{figure}

We begin by evaluating P2L on its ability to predict human feedback on a prompt-by-prompt basis.
In other words, given two models and a prompt, we ask how effectively P2L can predict which model will win on that prompt.
These experiments measure the ability of P2L to accurately assess relative model quality on a prompt-by-prompt basis.

In this section, we evaluate the ability of P2L to predict human preferences on Chatbot Arena.
We construct a holdout validation set containing 41,507 annotated pairwise comparisons across 34 well-used models.
We then measure the negative log-likelihood (validation loss) on this dataset; a lower validation loss indicates better preference prediction performance.

Figure~\ref{fig:loss-metrics} shows the results of our procedure against two baselines.
First, we include the constant predictor that gives an equal probability of all preference outcomes; this is an extremely weak baseline akin to flipping a coin to decide the winner.
Second, we include the average (``marginal'') leaderboard.
For P2L, we show a ladder of increasing model and dataset sizes.
The more data is used to train P2L, the better the preference predictions become.
Notably, the gap between the best P2L leaderboard and the marginal model is several times the gap between the marginal leaderboard and the constant predictor.
This indicates that by capturing the prompt-dependent differences in model performance, P2L is able to produce much better predictions of human preference.

\subsection{Optimal routing}
\label{sec:routing-experiments}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/elo_bar.pdf}
    \caption{\textbf{P2L router performance} on Chatbot Arena. The left barplot shows the overall score of the router after it was deployed prospectively on Chatbot Arena. The right barplot shows the worst-case category score on Chatbot Arena. Overall, larger models lead to higher Arena scores, i.e., better routers. The exception is \texttt{P2L-1.5B}, which has a large bump in overall performance. However, the confidence intervals indicate that this bump is explainable by statistical variations in its BT coefficient estimate.}
    \label{fig:scores-scaling}
    \vspace{-5.0pt}
\end{figure}

Next, we evaluate the performance of the optimal router based on P2L as derived in Section~\ref{sec:routing}.
Our evaluations are based on prospective deployments of our router to Chatbot Arena.
We treat the router as a separate model.

\subsubsection{Unconstrained routing}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/routing_heatmap.pdf}
    \caption{\textbf{Router model choice distribution} in each prompt category. The rows are different models, and the columns are different categories. Each cell represents the probability that the model was selected within that category (i.e., columns sum to 1). Models with an average selection rate below 1\% are not shown.}
    \label{fig:routing_heatmap}
\end{figure}

We deployed the grounded Rao-Kupper versions of \texttt{P2L-0.5B}, \texttt{P2L-1.5B}, \texttt{P2L-3B}, and \texttt{P2L-7B} onto Chatbot Arena, crowdsourcing a total of 8,616 pairwise comparisons between P2L models and public models hosted on Chatbot Arena.
The P2L models routed between 34 models, including top models such as \texttt{Gemini-exp-1206}, \texttt{o1-2024-12-17}, and \texttt{ChatGPT-4o-20241120} as well as other models.
(See Appendix~\ref{app:model-list} for a full model list.)

Because there is no cost-constraint, the P2L router always picks the highest-ranked model conditionally on the prompt, i.e., the highest entry in $\hat\theta(z)$. 
Marginally, the strongest singular candidate model in the P2L router was \texttt{Gemini-exp-1206}, with a score of 1364. 

As shown in the top plot in Figure~\ref{fig:scores-scaling}, all P2L routers, regardless of parameter count, outperformed \texttt{Gemini-exp-1206}.
The best model, \texttt{P2L-1.5B}, reached \#1 on Chatbot Arena during our testing period with a score of 1389. 
This shows the utility of P2L: differences in model performance on a prompt-by-prompt basis allow P2L to outperform all individual LLMs.

Next, we discuss scaling performance with respect to the Arena score of the router.
We see a general trend in Figure~\ref{fig:scores-scaling} that bigger models do better overall.
The exception is \texttt{P2L-1.5B}, whose performance was unexplainably strong; otherwise, the trend holds.
We also tested other metrics, such as worst-case performance  (bottom of Figure~\ref{fig:scores-scaling}).
The worst-case performance of P2L scales with parameter count as expected, and is uniformly much better than that of the marginal leaderboard.

We also observe that the gap between the P2L routers and static models is large.
The P2L routers are able to avoid per-prompt model weaknesses and route elsewhere. In fact, the gap between the best P2L router and the best non-routed static model in the overall comparison was 25 points, while this gap grew to 51 points in the minimum category performance case. Figure~\ref{fig:routing_heatmap} shows \texttt{P2L-7B}'s routing distribution conditioned on each Chatbot Arena category. Notably, we see relatively diverse routing patterns, even within a single category. We also observe intuitive behavior patterns, such that heavily routing to \texttt{o1-2024-12-17} for math prompts and \texttt{Gemini-exp-1206} for creative prompts.


\subsubsection{Cost-optimal routing}
\label{sec:cost_routing}
We show results of the optimal routing procedure detailed in Theorem~\ref{thm:optimal-router} with a \texttt{P2L-7B} model on Chatbot Arena.
Here, we use P2L to route between \texttt{o1-mini}, \texttt{gpt-4o-2025-05-13}, \texttt{claude-3-5-sonnet-20240620}, \texttt{gemini-1.5-pro-001}, \texttt{mistral-large-2407}, \texttt{claude-3-5-haiku-20241022}, and \texttt{gemini-1.5-flash-001} and with budgets of \texttt{\{0.00218, 0.0044, 0.00675, 0.00945, 0.0123, $\infty$\}}. To get reasonable cost estimates, we calculate the expected cost per query with $c_i = O_i*\mathbb{E}[T_i]$ for all models $i\in[M]$, where $O_i$ is the output cost per token of model $i$, and $T_i$ is a random variable representing the number of tokens in a response from model $i$. We estimate $\mathbb{E}[T_i]$ as the response token length mean overall responses from model $i$ in Chatbot Arena. Additionally, we estimate $q$ in Theorem~\ref{thm:optimal-router} according to the Chatbot Arena model sampling distribution. We find the P2L router performs well, with Pareto frontier Arena score versus cost. Furthermore, on the right plot in Figure~\ref{fig:routing-metrics} we find the P2L router continues to show dominant performance in Chatbot Arena's creative category despite large shifts in individual model performances.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/arena_score_cost.pdf}
    \caption{\textbf{Arena score versus cost}. Both plots show routing performance as a function of average cost. The left plot shows the averaged performance across all categories, and the right plot shows the performance in the creative writing category. The black open circles give the raw performance and cost of the models used by the router. Each gold dot represents the Arena score of the  \texttt{P2L-7B} router as a function of the cost constraint in~\eqref{problem:optimal-routing-master}. The plots show that the P2L router dominates and substantially improves the cost-performance Pareto frontier. All confidence intervals are 95\%.}
    \label{fig:routing-metrics}
\end{figure}




% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth]{figures/mocks/cost-sensitive-routing.pdf}
%     \caption{\textbf{Cost-sensitive routing metrics.}}
%     \label{fig:cost-routing-metrics}
% \end{figure}


\subsection{Testing for regression and strength/weakness analysis}
\label{sec:strengths-weaknesses}
An important question when developing models is to understand their category-level performance, along with strengths and weaknesses.
Imagine, for example, a business seeking to upgrade their workflow to a cheaper or newer (and presumably more advanced) model.
In such a business, testing for regression of the model to a worse performance may be important. 
For example, they might ask the question: if I switch from \texttt{GPT-4o} to \texttt{GPT-4o-mini}, can I do so safely, and will my performance get worse on my customers?

This is a challenging question to answer because it requires knowledge of the enterprise's customer distribution which may require lengthy instrumentation and data collection procedures.
However, P2L provides a partial solution to this problem.
Given a large unlabeled dataset of prompts (e.g., customer use-cases), we seek to: (1) Categorize these prompts automatically using an LLM. (2) Produce a preference leaderboard within each category, and (3) On a per-model basis, analyze for which categories it is weak and strong (relative to itself or its competition).

For this, we can use a hierarchical clustering approach. Assume access to a multilevel hierarchical categorization of prompts (this can be obtained from an LLM). 
That is, we have a function $\mathsf{categorize}$ that takes in a prompt $z$ and an integer level $l$ and outputs a category in $\{1, \ldots, k_l\}$, for some integer $k_l$.
Given a set of prompts, $\mathcal{Z}^{\rm category}$, we can compute a per-category leaderboard using $\tilde{\theta}(\mathrm{unif}(\mathcal{Z}))$ as in~\eqref{eq:efficient-aggregation-function}.
Note that the finest-grained categories may have very little data, motivating the need for P2L.

Figure~\ref{fig:analysis} shows an example analysis of five different OpenAI models. Here, the percentages are calculated as the win rate against \texttt{GPT-4o-2024-05-13} under the BT model.
According to \texttt{P2L-7B}, OpenAI models' performance varies across different categories and topic clusters. 
While \texttt{o1} might be a better model on average, it is essentially the same compared to \texttt{GPT-4o-mini} on certain creativity tasks. In math flavored tasks, the gap widens significantly.
See Figures~\ref{fig:llama-regression-1} and~\ref{fig:llama-regression-2} for similar and more detailed plots on Llama 3 fine-tunes.
We also include a variant of our regression analysis under the grounded RK model from ~\eqref{eq:grounded-rk}; this provides guidance as to the absolute reliability of the model, not just preference over alternative models; see Figure~\ref{fig:analysis-grounded-rk}.



\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/bt_4o_tree.pdf}
    \caption{\textbf{Regression test.} We show the strengths of different OpenAI models on various topic clusters based on their win rate against \texttt{GPT-4o-2024-05-13} as predicted by \texttt{P2L-7B}. For each category, we show the probability a given model wins against \texttt{GPT-4o-2024-05-13} under the BT model. The results show strong category-specific variability in performance; for example, \texttt{o1-mini} is substantially better than \texttt{GPT-4o-2024-05-13} in ``Arithmetic Operations and Calculations'' but substantially worse when asked to write a ``Suspenseful Horror Story''.}
    \label{fig:analysis}
\end{figure}

% \subsection{Verifiable Benchmarks}
% \label{sec:benchmarks}

% 1) achieve best score on verifiable benchmarks

% 2) reproduce benchmark results without annotations.

% Benchmarks---datasets of prompts and fixed reference labels---have always been important for assessing AI systems, and they are becoming increasingly more so.
% Simultaneously, they are becoming increasingly expensive.
% GPQA, for example, is a 500-question dataset that cost about \$120,000 to collect.
% The cost per-datapoint is only going up, as we expect LLMs to excel at ever-narrower expert tasks.

% What if we could bypass the need to label these benchmarks altogether?
% That is, from a large corpus of unlabeled prompts, can we estimate what the ranking of our models would have been on the resulting benchmark, without ever collecting labels?

\subsection{Aggregation scaling}
\label{sec:aggregation-scaling}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/aggr_scaling.pdf}
    \caption{\textbf{Aggregation scaling.} The L1 distance between the aggregated leaderboard and the marginal BT regression as a function of the number of randomly sampled and aggregated datapoints in two categories: Chinese (left) and Math (right). The L1 distance plateaus at the optimal performance, which is around 0.025/0.015.
    A nonzero optimal distance is expected because the empirical BT coefficients are derived from a finite validation sample, and so these coefficients have their own irreducible statistical error.
    Thus, the P2L estimate converges to a near-optimal solution with increased data.}
    \label{fig:aggr_scale}
    \vspace{-5.0pt}
\end{figure}

Given a distribution of prompts, we aim to evaluate how P2L behaves using the aggregation technique described in \ref{sec:aggregating}. Specifically, we analyze how P2L’s aggregated leaderboards compare to ground truth category leaderboards as well as how this relationship scales with data. First, we calculate ground truth leaderboards over a large category from the validation set with marginal regression. We then aggregate P2L over increasing subsets of this category's prompts. Lastly, we plot the L1 function distance between the aggregated leaderboard’s predicted probabilities and the ground truth leaderboard’s predicted probabilities as subset size increases. Since both the train and validation set are drawn from the same distribution, we denote the optimal value to be the L1 function distance between the ground truth category leaderboard and the category leaderboard derived from marginal regression on the train set. 

In contrast to marginal regression, which requires thousands of prompts for a stable leaderboard, P2L converges near this optimal value within 100-250 prompts (Figure~\ref{fig:aggr_scale}). Here, we see P2L’s potential to create accurate aggregated leaderboards efficiently, while also reinforcing the validity of its per prompt outputs. Furthermore, as we scale the amount of training data seen, P2L’s predictions over singular prompts differ more drastically from category leaderboards while still converging with more prompts (Figure~\ref{fig:aggr_scale}). A clear scaling law ensues, as increased data allows P2L to make more distinguished individual leaderboards while still maintaining its aggregation ability at the category level. 


 \subsection{Performance on out-of-distribution prompts}
\label{sec:out-of-distribution}

To assess how P2L generalizes to unseen prompts, we evaluate it on LiveBench  \citep{white2024livebench}, a verifiable, contamination-free benchmark with 1{,}000 questions covering diverse categories (e.g., math, coding, reasoning). 
Unlike Chatbot Arena, it utilizes objective success metrics.
We restrict our evaluation to a smaller pool of models. Among these models, P2L selects its candidate models for each question based on the predicted prompt-specific performance and then uses the output of the chosen model as the final answer. Table~\ref{tab:livebench_table} shows that \texttt{P2L-7B} surpasses every static baseline among the model subset, achieving an overall LiveBench score of 59.275. Even far smaller versions (e.g., 1.5B) match or exceed top static models, demonstrating that preference-trained routing generalizes well to an out-of-distribution, ground-truth benchmark.

Many real-world deployments require balancing model performance against inference costs. To examine this trade-off, we apply Prompt2Leaderboard to LiveBench at various cost thresholds (e.g., \$2, \$5, \$10, \$15 per million tokens) using the cost-optimal routing method discussed in Section~\ref{sec:cost_routing}.
Figure~\ref{fig:livebench_cost_fig} (in the appendix) shows that, in all budgets tested, the P2L cost-aware router consistently scores higher or comparable LiveBench scores to the best-performing model within that specific cost threshold. These gains are most pronounced when the budget permits occasional routing to a more expensive (and often stronger) model for prompts that particularly benefit from it. Thus, even under strict monetary constraints, P2L’s flexible prompt-level routing remains a powerful approach to maximizing performance on challenging out-of-distribution tasks.
