Evaluating the real-world performance of large language models is an unresolved challenge.
A growing suite of benchmarks, including MMLU~\citep{hendrycks2020measuring}, MMLU-Pro~\citep{wang2024mmlu}, and GPQA~\citep{rein2023gpqa}, seek to address the challenge by reporting task-specific performance metrics, such as multiple-choice question-answering ability.
These highly-curated benchmarks focus on domain-specific performance measures but do not capture the general and subjective nature of organic human preferences.
Live evaluations, such as Chatbot Arena~\citep{chiang2024chatbot}, assess real-world performance by collecting millions of organic human preferences from users who visit the site and vote between pairs of model responses. 
These pairwise comparisons are aggregated using Bradley-Terry (BT) regression~\cite{bradley1952rank} to form a leaderboard.
This leaderboard averages over many users and prompts, only providing a coarse understanding of performance.

For example, if we want to identify the best model for SQL queries, the overall Chatbot Arena leaderboard may not be useful since SQL queries make up only 0.6\% of organic submissions and thus have little influence in the ranking. 
A natural solution is to stratify the data and run a separate BT regression for SQL queries. 
However, collecting the 3{,}000-5{,}000 SQL votes needed for a stable ranking would require around a million total votes---taking months to collect. 
Finer-grained categories, for example SQL table joins, would demand even more data, making stratified regression impractical and slow.
And the finest-grained analyses---for example, producing leaderboards for a \emph{specific} prompt or use-case---are rendered impossible.

This manuscript proposes a solution to this problem via a method called Prompt-to-Leaderboard (P2L).
P2L takes a prompt as input and outputs a leaderboard quantifying LLM performance \emph{on that specific prompt}.
Thus, P2L can be used to assess which models are best for a specific use-case, as opposed to on average.
Per-prompt leaderboards can also be aggregated over a group of prompts to form personalized leaderboards, showing which model is best for an individual or enterprise based on their prompt history.

The system works by training a P2L model, which is an LLM trained on human preference feedback to output a Bradley-Terry (BT) coefficient for every model in question; see Section~\ref{sec:core-method}. 
Because P2L characterizes the prompt-conditional win rate of any two models, it enables several downstream applications.
These include optimally routing prompts to LLMs  (Section~\ref{sec:routing}), personalized evaluations based on a user's prompt history (Section~\ref{sec:aggregating}), automated strength and weakness analysis of models (Section~\ref{sec:strengths-weaknesses}), and more.
Thus, we view P2L as a general-purpose tool for highly granular evaluations extracted from large corpuses of preference data.
As a demonstration of P2L's utility, we tested our prompt routing strategy on Chatbot Arena between the dates 01/19/2025---01/27/2025, and it achieved the \#1 spot with a score increase of 25 points over the previous top model, \texttt{Gemini-exp-1206} (see ``P2L router performance'' in Figure~\ref{fig:teaser}).

\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/main_figure.pdf}
    \caption{\textbf{Pipeline of P2L.} P2L takes a prompt or a set of prompts and outputs an $M$-dimensional vector that we call a leaderboard. Once we have a leaderboard, we can build better data products, like routers and automatic analyses (see right).}
    \label{fig:teaser}
\end{figure*}

More broadly, P2L is a subclass of a more general methodology we call Prompt-to-Regression (P2R) for training LLMs to output coefficients of parametric statistical regressions (see Section~\ref{sec:p2r}).
A canonical example that we will develop throughout this paper is a model taking prompts as input and outputting Bradley-Terry coefficients, as mentioned earlier.
However, the method also accommodates other feedback models (ties, real values, etc.) via other parametric models.
We describe this method and derive the optimal routing strategy in Section~\ref{sec:methods}.
We show experiments and other applications in Section~\ref{sec:experiments}.