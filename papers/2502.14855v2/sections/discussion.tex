
This work develops fundamental tools for granular and query-specific evaluations in all evaluation tasks. 
Although our experiments are largely based on Chatbot Arena, this is not the only evaluation that could benefit from P2L.
As discussed in Section~\ref{sec:methods}, any feedback signal can be accommodated.
Thus, our techniques would equally work well for other evaluations~\cite{hendrycks2020measuring, zellers2019hellaswag, cobbe2021training, srivastava2023beyond, zhong2023agieval, chen2021evaluating, lin2023toxicchat, liang2022helm} as well as cost and latency prediction.

\textbf{Modeling human preference.} During Reinforcement Learning from Human Feedback (RLHF), a reward model is often trained as a proxy to human preference. Similar to P2L, reward model training may use a contrastive pairwise or $K$-wise loss, for example using the BT model \citep{christiano2023deep, bai2022training, ouyang2022training, zhu2023principled}. However, reward models are agnostic to model identity, requiring a prompt and response to return a single score for the response. P2L, which is aware of model identities, instead seeks to output expected model response quality, conditioned on input prompt, instantly generating a full leaderboard over all models without requiring model responses to be generated. This yields efficient leaderboard creation over arbitrary prompt sets.

\textbf{Meta-learning.} P2L is related to meta learning~\cite{schmidhuber1987evolutionary, santoro2016meta, finn2017model} insofar as we are training a model to output models. For example, we have discussed training an LLM (the meta-learner) to output coefficients of a BT regression (the learner). However, the meta-learning literature primarily focuses on learners that are deep neural networks. Instead, we let the learner be an extremely simple statistical model that is used for inference.

\textbf{Routing.} Prior work on routing LLM queries optimizes trade-offs between cost and performance, typically through classifiers or gating mechanisms. RouteLLM \citep{ong2024routellm} and AutoMix \citep{madaan2023automix} train binary classifiers to decide between a strong and weak model, while LLM-Blender \citep{jiang2023llm} ranks candidate responses and blends them. Hybrid LLM \citep{ding2024hybrid} selects between cloud and edge models based on predicted query difficulty. RouterDC~\cite{chen2024routerdcquerybasedrouterdual} uses contrastive losses to train a query-based router. Unlike these approaches, which operate over a small fixed set of models, P2L learns a parametric function mapping prompts to full model leaderboards, enabling flexible selection across large model pools. Its statistical structure supports efficient cost-aware routing, outperforming static models in live crowdsourced settings while scaling to personalized and task-specific selections.
An interesting extension of P2L would be to minimize the cost subject to a performance constraint, instead of maximizing performance subject to a cost constraint as we do herein.

\textbf{Parametric statistical models.} Our work builds on classic log-linear models and GLMs, like those of~\citet{bradley1952rank, rao1967ties}; see~\cite{mccullagh2019generalized} for a review, and~\cite{ameli2024statistical} for further extensions that enrich this model class for better LLM ranking.
The closest piece of work to ours is~\citet{hastie1993varying}, which proposes varying-coefficient models.
P2L can be seen as a subclass of varying-coefficient models.
To our knowledge, ours is the first work to parameterize such a model via a foundation model and backpropagate it end-to-end, while the techniques in~\citet{hastie1993varying} use bespoke fitting procedures and simpler statistical models than LLMs.
