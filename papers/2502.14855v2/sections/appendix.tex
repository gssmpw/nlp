\section{Proofs}
\label{app:proofs}

\begin{proof}[Proof of Theorem~\ref{thm:optimal-router}]
    The equivalence of~\eqref{problem:optimal-routing} and~\eqref{problem:optimal-routing-master} is immediate.
    Proving the equivalence of~\eqref{problem:optimal-routing-bt} and~\eqref{problem:optimal-routing-master} is more challenging, and we focus there.
    
    We begin by simplifying the expressions in~\eqref{problem:optimal-routing-bt}.
    The cost constraint can be succinctly written as $\tilde\pi^\top c \leq C$.
    Regarding the objective, because the binary cross-entropy loss is linear in the response,
    \begin{multline}
        \E_{B \sim \tilde\pi, A \sim q, Y' \sim \mathrm{Bern}(\sigma(\theta^*(z)_B - \theta^*(z)_A))}\left[ \ell(\sigma(\theta - \theta^*(z)_A), Y') \mid Z = z\right] \\
        = \E_{B \sim \tilde\pi, A \sim q}\left[ \ell(\sigma(\theta - \theta^*(z)_A), \sigma(\theta^*(z)_B - \theta^*(z)_A)) \mid Z = z\right] \\
        = \E_{A \sim q}\left[ \ell\left(\sigma(\theta - \theta^*(z)_A), \left(\tilde\pi^\top \mathbf{W}^*\right)_A\right) \Bigg\vert Z = z\right],
    \end{multline}
    where again $\mathbf{W^*}$ represents the population win matrix, with entries $\mathbf{W}^*_{ba} = \sigma(\theta^*(z)_b - \theta^*(z)_a)$.
    Thus, the optimization problem in~\eqref{problem:optimal-routing-bt} can be equivalently rewritten as
    \begin{equation}
    \label{eq:optimal-routing-nested}
    \begin{aligned}
        \maximize_{\substack{ \tilde\pi \in \Delta^M }} \quad 
            & \theta'(\tilde\pi) 
            \quad \text{subject to}\quad
            \tilde\pi^\top c \le C,
    \end{aligned}
    \end{equation}
    where 
    \begin{equation}
    \theta'(\tilde\pi) 
    = 
    \argmin_{\theta \in \mathbb{R}}
    \;
    \E_{A \sim q}\Bigl[
      \ell\Bigl(\sigma(\theta - \theta^*(z)_A),\,
        (\tilde\pi^\top \mathbf{W}^*)_A
      \Bigr)
    \Bigr].
    \end{equation}
    Examining the first-order conditions of the inner optimization problem for $\theta'(\tilde\pi)$ shows that the solution satisfies
    \begin{equation}
        \label{eq:router-first-order-condition}
    \sum_A q_A\,\sigma\bigl(\theta'(\tilde\pi)-\theta^*(z)_A\bigr)
    = \tilde\pi^\top \mathbf{W}^*q.
    \end{equation}
    Define
    \begin{equation}
    R(\tilde\pi) = \tilde\pi^\top \mathbf{W}^*q, \qquad G(\theta) = \sum_A q_A\,\sigma(\theta - \theta^*(z)_A).
    \end{equation}
    Then $\theta'(\tilde\pi) = G^{-1}(R(\tilde\pi))$.
    Since $G^{-1}$ is strictly increasing,
    \begin{equation}
    \maximize_{\tilde\pi} \theta'(\tilde\pi) \quad \Longleftrightarrow \quad \maximize_{\tilde\pi} R(\tilde\pi).
    \end{equation}
    Thus, the problem reduces to:
    \begin{equation}
    \maximize_{\tilde\pi \in \Delta^M, \; \tilde\pi^\top c \le C} \tilde\pi^\top \mathbf{W}^*q,
    \end{equation}
    which is exactly the problem in~\eqref{problem:optimal-routing-master}.
\end{proof}


\section{Additional theory}
\label{app:theory}

\subsection{Aggregating leaderboards via averaging}
\label{app:aggregating-averaging}

The BT model tells us that for all $z \in \Z$,
\begin{equation}
    \log\left(\frac{\P(Y=1 \mid X=x, Z=z)}{1-\P(Y=1 \mid X=x, Z=z)}\right) = x^\top \theta^*(z).
\end{equation}
Thus,
\begin{equation}
    \E_{Z \sim Q}\left[\log\left(\frac{\P(Y=1 \mid X=x, Z)}{1-\P(Y=1 \mid X=x, Z)}\right)\right] = x^{\top}\left(\underbrace{\int_{z \in \cZ} \theta^*(z) dQ(z) }_{\tilde\theta(Q)}\right).
\end{equation}
That is, taking a (weighted) average of the values of $\theta^*(z)$ leads to a predictor of the expected log-odds.

This method has two downsides: firstly, increasing the $m$th coordinate of $\tilde{\theta}(Q)$ does not mean that model $m$ is more likely to win against other models on average.
Secondly, the function $\tilde\theta(Q)$ does not have a simple relationship with the win rate.
This motivates the need for the aggregation metric from Section~\ref{sec:aggregating}.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/tree_llama_full_1.pdf}
%     \caption{\textbf{Regression Test.} Llama 3 Fine-tunes Part 1. \ana{Horizontal version}}
%     \label{fig:llama-regression-1}
% \end{figure}

\section{Additional regression tests}
\label{app:more-strength-weakness}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figures/bt_llama_tree_1.pdf}
    \caption{\textbf{Regression test} on Llama models with creative writing and math prompts.  The percentages shown signify win rates against \texttt{Llama-3-70B} under the BT coefficients predicted from \texttt{P2L-7B}.}
    \label{fig:llama-regression-1}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{figures/bt_llama_tree_2.pdf}
    \caption{\textbf{Regression test} on Llama models with instruction following and coding prompts. The percentages shown signify win rates against \texttt{Llama-3-70B} under the BT coefficients predicted from \texttt{P2L-7B}.}
    \label{fig:llama-regression-2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/bag_openai.pdf}
    \caption{\textbf{Regression test using grounded Rao-Kupper.} We show the strengths of different OpenAI models on various topic clusters based on \texttt{P2L-7B} with a grounded RK regression head (see Section~\ref{sec:p2r}) and a dataset of unlabeled prompts. The percentage represents the sigmoid of the model coefficient.
    Because the RK model is grounded, this corresponds roughly to a signal of the model's reliability, i.e., its tendency to produce an answer that exceeds the voter's minimum bar of quality. The results show strong category-specific variability in performance; for example, \texttt{GPT-4o-mini} and \texttt{o1} have roughly the same reliability in the category ``Suspenseful Horror Story'', but not ``Arithmetic Operations and Calculations''. We can also see that some categories are more difficult in general for LLMs to answer reliably, and thus we see larger performance improvements from test-time compute models like \texttt{o1} and \texttt{o1-mini}.}
    \label{fig:analysis-grounded-rk}
\end{figure}

\begin{figure}[p]
    \vspace{-0.5cm}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/cost_plot_livebench.pdf}
    \caption{\textbf{LiveBench cost routing.} Comparison of the P2L cost-aware router and static models on LiveBench under various inference-cost constraints. The left plots show each model’s overall LiveBench performance at different maximum cost thresholds, while the right plots display models’ relative rankings across multiple categories at the specific cost limit. By adaptively allocating prompts to cheaper or more expensive models when advantageous, the P2L router consistently matches or surpasses the best single model within each budget.}
    \label{fig:livebench_cost_fig}
\end{figure}

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{lccccccc}
\toprule
Model & LiveBench & Math & Coding & Reasoning & Language & Instruction& Data \\
      & Score     &        &  &  &          &    Following  &  Analysis         \\

\midrule
\texttt{P2L-7B} & $\mathbf{59.3}$ & 51.9 & 65.2 & 50.0 & 56.5 & $\mathbf{75.8}$ & $\underline{56.3}$ \\
\texttt{claude-3-5-sonnet-20240620} & $\underline{59.2}$ & 51.3 & 63.5 & $\mathbf{54.7}$ & $\underline{56.8}$ & 72.3 & $\mathbf{56.7}$ \\
\texttt{claude-3-5-sonnet-20241022} & 59.0 & 51.3 & $\underline{66.8}$ & 50.0 & $\mathbf{57.0}$ & 74.1 & 54.9 \\
\texttt{P2L-1.5B} & 58.4 & $\mathbf{55.3}$ & $\mathbf{67.5}$ & 48.0 & 51.4 & 71.9 & $\mathbf{56.7}$ \\
\texttt{P2L-3B} & 57.8 & 49.6 & $\underline{66.8}$ & 50.7 & 53.3 & 70.4 & 56.2 \\
\texttt{P2L-0.5B} & 57.0 & 51.9 & 59.6 & 50.7 & 51.7 & 73.4 & 54.8 \\
\texttt{P2L-135M} & 56.2 & 48.9 & 63.5 & 50.0 & 47.1 & 74.1 & 54.0 \\
\texttt{P2L-360M} & 54.9 & 52.4 & 58.1 & 44.0 & 44.1 & 74.4 & $\mathbf{56.7}$ \\
\texttt{athene-v2-chat} & 53.4 & $\underline{53.4}$ & 56.9 & 48.0 & 37.5 & $\underline{74.6}$ & 50.2 \\
\texttt{gpt-4o-2024-05-13} & 52.8 & 42.7 & 50.4 & 47.3 & 49.3 & 72.4 & 54.4 \\
\texttt{qwen2.5-72b-instruct} & 52.6 & 52.3 & 55.6 & 47.3 & 36.0 & 73.3 & 51.1 \\
\texttt{gpt-4-turbo-2024-04-09} & 51.2 & 40.3 & 45.8 & $\underline{52.7}$ & 45.3 & 68.4 & 54.5 \\
\texttt{mistral-large-2407} & 50.4 & 48.4 & 45.8 & 44.0 & 40.5 & 73.1 & 50.4 \\
\texttt{chatgpt-4o-latest-20241120} & 49.4 & 37.7 & 44.4 & 44.7 & 43.7 & 74.1 & 51.7 \\
\texttt{gemini-1.5-pro-001} & 44.2 & 36.2 & 33.7 & 34.0 & 37.6 & 68.9 & 54.8 \\
\texttt{llama-3.1-70b-instruct} & 42.4 & 34.4 & 32.9 & 34.7 & 36.4 & 68.9 & 47.3 \\
\texttt{llama-3-70b-instruct} & 41.7 & 26.3 & 28.7 & 40.0 & 36.3 & 68.5 & 50.7 \\
\texttt{mixtral-8x22b-instruct-v0.1} & 37.5 & 28.0 & 32.3 & 36.0 & 27.9 & 65.5 & 35.5 \\
\texttt{llama-3.1-8b-instruct} & 26.3 & 19.5 & 14.5 & 18.7 & 17.8 & 53.9 & 33.3 \\
\texttt{mixtral-8x7b-instruct-v0.1} & 22.1 & 12.4 & 10.6 & 23.3 & 12.8 & 46.1 & 27.4 \\
\bottomrule
\end{tabular}
\caption{
\footnotesize
\textbf{LiveBench performance comparison}. Comprehensive evaluation of language models across seven capability categories: overall LiveBench score, mathematics, coding, reasoning, language understanding, instruction following, and data analysis. Results show performance comparison between p2l models at different parameter scales (135M to 7B), Claude-3.5 Sonnet versions, and other leading language models including GPT-4, Gemini, and LLaMA variants. All models were evaluated using identical inference settings as those employed in Chatbot Arena to ensure fair comparison. Scores are presented as percentages, with the highest score in each category shown in \textbf{bold} and second-highest \underline{underlined}. 
\texttt{P2L-7B} achieves top performance in LiveBench Score (59.3) and Instruction Following (75.8), while maintaining competitive performance across other categories.}
\label{tab:livebench_table}
\end{table}

\section{Additional information}
\subsection{Model list}
\label{app:model-list}
The full list of models is: \texttt{athene-v2-chat} \citep{frickathene}, \texttt{chatgpt-4o-latest-20241120}, \texttt{claude-3-5-haiku-20241022}, \texttt{claude-3-5-sonnet-20240620}, \texttt{claude-3-5-sonnet-20241022} \citep{claude32024family}, \texttt{deepseek-v3} \citep{liu2024deepseek}, \texttt{gemini-1.5-flash-001}, \texttt{gemini-1.5-flash-002}, \texttt{gemini-1.5-pro-001}, \texttt{gemini-1.5-pro-002} \citep{team2024gemini}, \texttt{gemini-2.0-flash-exp}, \texttt{gemini-2.0-flash-thinking-exp-1219}, \texttt{gemini-exp-1206}, \texttt{gemma-2-27b-it}, \texttt{gemma-2-9b-it} \citep{team2024gemma}, \texttt{glm-4-plus}, \texttt{gpt-4-1106-preview}, \texttt{gpt-4-turbo-2024-04-09} \citep{openai2023gpt4turbo}, \texttt{gpt-4o-2024-05-13}, \texttt{gpt-4o-2024-08-06}, \texttt{gpt-4o-mini-2024-07-18} \citep{openai2024gpt4o}, \texttt{llama-3-70b-instruct}, \texttt{llama-3.1-405b-instruct-fp8}, \texttt{llama-3.1-70b-instruct}, \texttt{llama-3.1-8b-instruct}, \texttt{llama-3.3-70b-instruct} \citep{llama3modelcard}, \texttt{mistral-large-2407}, \texttt{mixtral-8x22b-instruct-v0.1}, \texttt{mixtral-8x7b-instruct-v0.1} \citep{jiang2024mixtral}, \texttt{o1-2024-12-17}, \texttt{o1-mini}, \texttt{o1-preview} \citep{jaech2024openai}, \texttt{qwen2.5-72b-instruct} \citep{qwen2.5}, and \texttt{yi-lightning} \citep{ai2024yi}.