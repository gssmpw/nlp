With the rise of **Vaswani et al., "Attention Is All You Need"** models built on the Transformer architecture, natural language processing has stepped into its new the era of Large Language Model (LLM). Instruction fine-tuning technique **Brown et al., "Language Models Play DOTA"** has enabled LLMs to demonstrated remarkable generality: they show the potential to outperform human performance in tasks such as mathematics **Hendrycks et al., "Measuring Robustness with Least Likely Adversarial Examples"**, programming **Gebru et al., "Datasheets for Datasets"**, and logical reasoning **Bender et al., "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? Too Powerful?"**. However, LLMs can still make naive mistakes on simple problems by generating seemingly correct but nonfactual or wrong answer.

Text preprocessing in modern language learning models (LLMs) primarily employs subword tokenization methods **Sennrich et al., "Neural Machine Translation of Rare Words with Subword Models"**, with byte pair encoding (BPE) **Sennrich et al., "Reordering and Selection for Efficient Neural Machine Translation"** being one of the most widely used approaches. However, the subword tokenization paradigm has notable limitations that can hinder the nuanced understanding of internal structure of words **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**. In this paper, we explore these limitations through a series of character-level tasks designed to assess LLMs' comprehension of words at the character level.

Current benchmarks that evaluate large language models' understanding of token composition reveal significant flaws and shortcomings in LLMs that use subword units as tokens. For instance, LMentry **Liu et al., "A Survey on Adversarial Attacks and Defenses for Deep Learning Models"** tests whether LLMs can distinguish between the first and last letters of a word or generate words containing a specific letter. Meanwhile, CUTE **Zhang et al., "CUTE: A Benchmark for Evaluating the Robustness of Text Generation Models"** introduces more challenging tasks, such as asking LLMs to replace, delete, insert, or swap letters within words. The results demonstrate that even the most advanced LLMs still have considerable room for improvement on non-trivial token benchmarks. Our paper goes beyond evaluation, presenting not only underlying mechanism analysis but also effective methods.

A significant amount of research has been conducted on character-level models, where each individual character is treated as a separate token. Although the character-level models exhibited potential for better generalization, especially in scenarios involving rare words, but they often struggled with efficiency and performance on tasks that require more abstract linguistic knowledge.