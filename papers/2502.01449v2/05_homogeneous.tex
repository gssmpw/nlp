\section{\name~for Homogeneous Chiplet Shapes}
\label{sec:homo}

\ps{Introduce assumptions specific to the homogeneous setting}

We discuss our placement representation and optimization results for chiplets with homogeneous shapes.
In this setting, we assume that there are two possible configurations of \gls{phys} for a chiplet:
A chiplet either has four \gls{phys}, one on each side (we use this for compute-chiplets), or a single PHY on one side (we use this for memory- and IO-chiplets).

\subsection{Placement Representation}
\label{ssec:homo-repr}

\ps{Introduce the key idea of the placement representation for homogeneous chiplets}

We represent a placement of homogeneously shaped chiplets as an $R \times C$ grid such that $R \cdot C \geq N_\text{C} + N_\text{M} + N_\text{I}$.
A grid cell can contain a compute-, memory-,  or IO-chiplet, or it can be empty (see \Cref{fig:homo-plac-init}).
Chiplets with a single PHY can be rotated, but chiplets with four PHYs cannot, since this would result in an isomorphic placement with identical cost.
Furthermore, chiplets with a single PHY are always placed in a way s.t. the PHY faces another chiplet and not the "outside".
We provide the following functions for homogeneous placements:

\ps{Explain details of the placement representation for homogeneous chiplets}

\begin{itemize}
	\item \texttt{random\_placement()}:
	Randomly place all chiplets in the $R \times C$ grid (see \Cref{fig:homo-plac-init} for an example).
	\item	\texttt{mutate(X)}
	We provide four different mutation modes.
	\begin{itemize}
		\item \emph{any-both}: swap two chiplets \textbf{and} rotate one.
		\item \emph{any-one}: swap two chiplets \textbf{or} rotate one.
		\item \emph{neighbor-both}: swap two adjacent chiplets \textbf{and} rotate one.
		\item \emph{neighbor-one}: swap two adjacent chiplets \textbf{or} rotate one.
	\end{itemize}
	Only chiplets of different types can be swapped, since swapping two chiplets of the same type would result in an isomorphic placement with identical cost.
	\Cref{fig:homo-plac-mutate} visualizes an \emph{any-one} and a \emph{neighbor-one} mutation.

	\item	\texttt{merge(X,Y)}
	If chiplet types and/or chiplet rotations match between placements X and Y, then, those types and/or rotations are carried over to the merged placement.
	The remaining, unplaced chiplets are placed and rotated randomly.
	\Cref{fig:homo-plac-merge-1,fig:homo-plac-merge-2} visualize the merge process.

	\item	\texttt{get\_network()}
	The placement-based \gls{ici} topology is created by connecting any two opposing \gls{phys} of adjacent chiplets (see \Cref{fig:homo-plac-network}).
	This might result in an unconnected placement (as in \Cref{fig:homo-plac-network}).
	In those cases, the operation that created the unconnected placement is repeated.

	\item	\texttt{get\_area()}
	The area of a homogeneous placement is chiplet\_size $\cdot R \cdot C$.
	Consequently, in the homogeneous case, all placements of the same architecture have the same area.
\end{itemize}

\input{fig_homo_placement}

\subsection{Optimization Results}
\label{ssec:homo-opt}

\ps{Provide the experiment configuration}

We use \name~to optimize two architectures, one with $32$ compute-, $4$ memory-, and $4$ IO-chiplets (called \textit{32 cores homogeneous}) and one with $64$ compute-, $8$ memory-, and $8$ IO-chiplets (called \textit{64 cores homogeneous}).
Both architectures use chiplets of size $3$mm$ \times $3$mm$.
We perform our experiments on a single core of an Intel Xeon X7550 running Debian 11.

To set the weights of \gls{c2c}, \gls{c2m}, \gls{c2i}, and \gls{m2i} latency and throughput in the cost function accordingly, we analyze multiple cache-coherency traffic traces from the Netraces v1.0 \cite{netrace-traces} trace collection.
We observe that $0\%$-$5\%$ of the messages are \gls{c2c} traffic, $80\%$-$95\%$ are \gls{c2m} traffic, and $3\%$-$16\%$ are \gls{m2i} traffic.
Therefore, we set the cost function weights for the area as well as \gls{c2m} and \gls{m2i} latency and throughput to $2$ while we set the weights for \gls{c2c} and \gls{c2i} latency and throughput to $0.1$.
\Cref{tab:homo-params} shows the remaining parameters. 

\input{tab_homo_params}

\ps{Direct comparison between algorithms}

\Cref{fig:homo-results} shows our results for the two architectures.
For both architectures, all three optimization algorithms are able to outperform the baseline architecture which is based on a 2D mesh (see \Cref{fig:eval-placements}).
This is not surprising, as such an architecture is not ideal for \gls{c2m} and \gls{m2i} traffic, which was the highest weighted traffic in our cost function.
Furthermore, we observe that both the \gls{ga} and \gls{sa} significantly outperform \gls{br} which shows that the two more complex optimization algorithms work as intended. 

\input{fig_homo_results}

\ps{Discuss convergence}

We observe that the \gls{ga} is able to quickly converge to a good solution. 
For the 32 core architecture with a solution space of approximately $10^{14}$ solutions, the \gls{ga} converges after five minutes.
For the 64 core architecture with a solution space of approximately $10^{30}$ solutions, the convergence takes about 30 minutes.
\gls{sa} is able to continuously increase the quality of the solution, however, the allocated compute budget was not sufficient for \gls{sa} to reach convergence.
This shows the advantage of maintaining multiple good solutions and combining them (\gls{ga}) over keeping a single solution (\gls{sa}).
