\section{Details on Optimization Algorithms}
\label{app:opt}


In this section, we provide details on the optimization algorithms used in this work.
To illustrate these algorithms, we apply them to the following example problem:

\begin{center}
\textit{Given a blackbox cost function $f:[0,1]^3 \to \mathbb{R}$, find\\values for $a,b,c \in [0,1]$ such that $f(a,b,c)$ is minimized.}
\end{center}

\ps{Introduce "Best Random" as a baseline}

\subsection{Best Random}
\label{app:opt-br}

A na\"ive optimization algorithm is to generate many sets of random numbers $a,b,c \in [0,1]$ and to evaluate the cost function $f(a,b,c)$ for all of them.
We keep track of the values of $a,b,$ and $c$ that yield the lowest cost $f(a,b,c)$.
We use this na\"ive algorithm to verify that our more elaborate optimization algorithm perform better than random.
\Cref{alg:best-random} shows the pseudocode of the \gls{br} algorithm.

\input{alg_best_random.tex}

\ps{Explain the Genetic Algorithm}

\subsection{The Genetic Algorithm}
\label{app:opt-ga}

The \gls{ga} mimics biological evolution to solve an optimization problem.
We start with a randomly generated population of $P$ individuals.
An individual is one solution to our optimization problem, in our example, an individual is a set of numbers $a,b,c \in [0,1]$.
The algorithm iterates through generations of individuals, where each generation is formed based on the previous generation.
There are many strategies to form the next generation, in \name, we use the following policy:
The $E$ individuals of the current population that exhibit the lowest cost proceed to the next generation (elitism selection \cite{ga-elite}).

\input{tab_background_ga}

The remaining $(P-E)$ free spots in the next generation are filled with newly generated individuals called children.
A child is created by merging two parent-individuals of the current generation (crossover).
In our example, a crossover could mean that for each of the child's variables $a,b,$ and $c$, we randomly decide whether to adopt the value from the first or from the second parent.
Many methods of selecting parents exist \cite{ga-1}.
In our work, we use the tournament selection method \cite{ga-tourn} where for each child, we repeat the following process two times:
Select $T$ individuals of the current population u.a.r. 
Out of these $T$ individuals, select the one with the lowest cost function as a parent.
In the unlikely case that the same individual is selected as both parents, repeat the process until two unique parents are found.
When a child is created, there is a probability $p_m$ that it experiences a mutation.
In our example, a mutation could mean that we increase/decrease the value of one of the three variables by a random amount.
In the end, we report the lowest-cost individual as our solution.
\Cref{tab:back-ga} lists the parameters of the \gls{ga} and \Cref{alg:genetic-algorithm} provides its pseudocode.

\input{alg_genetic_algorithm.tex}

\ps{Explain Simulated Annealing}

\subsection{Simulated Annealing}
\label{app:opt-sa}

Simulated annealing is an optimization algorithm that is based on the annealing process in which a material is heated up and cooled down slowly in order to alter its properties.
At the beginning, we generate a random solution to our problem.
In our example, this corresponds to assigning random values to the variables $a,b,$ and $c$.
In each iteration of the algorithm, we create a slight modification of the current solution.
In our example, we could increase or decrease one of the three variable by a random amount.
If this new solution has a lower cost than our current solution, then we accept it, i.e., we take it as our current solution.
If the new solution has a higher cost than our current solution, then, we only accept it with a certain probability $p$. 
This probability changes over the course of the execution, which is where the magic of simulated annealing lies.
The probability $p$ is computed based on the Metropolis criterion \cite{metropolis}:


\begin{equation} \label{eq:back-opt-metro}
	p = e^{-\Delta / T}
\end{equation}

Here, $\Delta$ is the difference between the cost of the new solution and the cost of the current solution, and $T$ is the current temperature of the system.
At the beginning of the execution, $T$ is equal to the initial temperature $T_0$ and over the course of the execution, the temperature $T$ decreases.
The way in which $T$ decreases is called the cooling schedule.
Many cooling schedules exist \cite{sa-1, sa-2}.
Most of them operate in rounds of $L$ iterations.
We use $k$ to denote the index of the current round.
$\alpha$ is an additional parameter to steer the speed of the cooling process.
Examples of cooling schedules are:

\begin{itemize}
	\item	Exponential multiplicative cooling: $T = T_0 \cdot \alpha^k$
	\item	Logarithmic multiplicative cooling: $T = \frac{T_0}{1 + \alpha \cdot \log(1+k)}$
	\item	Linear multiplicative cooling: $T = \frac{T_0}{1 + \alpha \cdot k}$
	\item	Quadratic multiplicative cooling: $T = \frac{T_0}{1 + \alpha \cdot k^2}$
\end{itemize}

\input{alg_simulated_annealing.tex}

In our work, we also apply non-monotonic adaptive cooling \cite{sa-nma-cooling} where the temperature obtained by any of the aforementioned cooling schedules is multiplied with a factor $\mu$:

\begin{equation} \label{eq:}
	\mu = \left ( 1 + \frac{C_\text{cur} - C^*}{C_\text{cur}} \right )^\beta
\end{equation}

Here, $C_\text{cur}$ is the cost of the current solution and $C^*$ is the lowest cost that was found over the course of the execution.
$\beta$ is an additional parameter steering the magnitude of the adaptiveness.
The goal of non-monotonic adaptive cooling is to increase the probability of accepting higher-cost solutions in situations where the current solution is far from the best known one (i.e., to escape local optima). 
\Cref{tab:back-sa} summarizes the variables and parameters of the simulated annealing algorithm and \Cref{alg:simulated-annealing} provides its pseudocode.


\input{tab_background_sa}


