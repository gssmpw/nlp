
\documentclass{article} %
\usepackage{iclr2025_delta,times}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{booktabs} %

\usepackage{hyperref}
\usepackage{url}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}

\usepackage{pifont}%
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand*{\R}{\mathbb{R}}
\newcommand*{\X}{\mathcal{X}}
\newcommand*{\Y}{\mathcal{Y}}
\newcommand*{\B}{\mathcal{B}}
\newcommand*{\T}{\mathbb{T}}
\newcommand*{\N}{\mathbb{N}}
\newcommand*{\Z}{\mathcal{Z}}
\newcommand*{\C}{\mathcal{C}}
\newcommand*{\F}{\mathcal{F}}
\newcommand*{\Q}{\mathbb{Q}}
\newcommand*{\pr}{\mathbb{P}}
\newcommand*{\bfx}{{\bm{x}}}
\newcommand*{\bfX}{{\bm{X}}}
\newcommand*{\bfw}{{\bm{w}}}
\newcommand*{\bfW}{{\bm{W}}}
\newcommand*{\bfy}{{\bm{y}}}
\newcommand*{\bfz}{{\bm{z}}}
\newcommand*{\bfa}{{\bm{a}}}
\newcommand*{\bsu}{{\bm{u}}}
\newcommand*{\bfu}{{\bm{u}}}
\newcommand*{\bfA}{{\bm{A}}}
\newcommand*{\bfV}{{\bm{V}}}
\newcommand*{\bsf}{{\bm{f}}}
\newcommand*{\bsg}{{\bm{g}}}
\newcommand*{\bseps}{{\bm{\epsilon}}}
\newcommand*{\rmd}{\mathrm{d}}
\newcommand*{\var}{\textrm{Var}}
\newcommand*{\ex}{\mathbb{E}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareFontFamily{U}{mathx}{}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\widecheck}{0}{mathx}{"71}
\DeclareMathAccent{\widebar}{0}{mathx}{"73}

\numberwithin{equation}{section}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usetikzlibrary{positioning}

\newcommand{\distro}[4][40]{
  \begin{tikzpicture}[thick]
    \draw[dashed, dash pattern={on 2.3 off 2}] (0, .4) circle (12mm);
    \draw[blue!60!black, very thick] plot[variable=\t, domain=-1:1, samples=#1] ({\t}, {#2 * exp(-10*(\t)^2) + #3 * exp(-60*(\t-0.6)^2 - \t) + #3 * exp(-60*(\t+0.7)^2 - 0.2) + #4 * 0.5 * exp(-50*(\t+0.3)^2) + #4 * exp(-50*(\t-0.2)^2 + 0.1)});
    \draw[solid, <->] (-1, 0)--(1, 0);
  \end{tikzpicture}
}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\NB{\emph{N.B}\onedot}
\def\cf{\emph{cf}\onedot} \def\Cf{\emph{Cf}\onedot}
\def\etc{\emph{\&c}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\makeatother


\hypersetup{
    colorlinks,
    linkcolor={orange!70!black},
    citecolor={blue!80!black},
    urlcolor={magenta!80!black}
}
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage[many]{tcolorbox}

\newtcolorbox{theorembox}{
    enhanced,
    sharp corners,
    breakable,
    borderline west={2pt}{0pt}{blue},
    colback=blue!10,
    colframe=blue!10
}


\newtcolorbox{standoutbox}{
    enhanced,
    sharp corners,
    breakable,
    borderline west={2pt}{0pt}{orange},
    colback=orange!10,
    colframe=orange!10
}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\title{A Reversible Solver for Diffusion SDEs}


\author{Zander W. Blasingame\\
Clarkson University\\
\texttt{blasinzw@clarkson.edu}
\And Chen Liu\\
Clarkson University\\
\texttt{cliu@clarkson.edu}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy %
\begin{document}


\maketitle

\begin{abstract}
    Diffusion models have quickly become the state-of-the-art for generation tasks across many different data modalities.
    An important ability of diffusion models is the ability to encode samples from the data distribution back into the sampling prior distribution.
    This is useful for performing alterations to real data samples along with guided generation via the continuous adjoint equations.
    We propose an \textit{algebraically reversible} solver for diffusion SDEs that can exactly invert real data samples into the prior distribution.
    
\end{abstract}

\section{Introduction}
Diffusion models have quickly become the state-of-the-art in many different modalities in generation, \eg, audio~\citep{liu2023audioldm}, images~\citep{rombach2022high}, video~\citep{blattmann2023align}, protein generation~\citep{skreta2024superposition}, \etc.
The sampling process of diffusion models is done through numerically solving an It\^o Stochastic Differential Equation (SDE) or related Ordinary Differential Equation (ODE) which describes the evolution of a sample drawn for some prior noise distribution to the data distribution.

Inversion of the sampling procedure, \ie, an encoding from the data distribution back to the prior distribution, is invaluable for many downstream applications.
\Eg, image editing~\citep{hertz2023prompttoprompt,su2023dual,meng2022sdedit,nie2024blessing} and image interpolation~\citep{song2021denoising,blasingame2024adjointdeis,blasingame2024greedydim} with natural extensions to other data modalities.
Existing work on diffusion inversion has focused on the ODE formulation~\citep{wallace2023edict,zhang2023bdia,wang2024belm}; however, recent work~\citep{nie2024blessing} has shown that the SDE formulation is particularly useful for applications when the latent representation is edited.
Motivated by this finding we propose a novel \textit{algebraically reversible} solver for diffusion SDEs which makes use of the Brownian interval~\citep{kidger2021efficient} to perform \textit{exact} inversion with diffusion models without storing the entire Wiener process in memory.
To the best of our knowledge this work is the \textit{first} to propose a technique for exactly inverting diffusion SDEs \textit{without} storing the sampled noise from every timestep in memory.

\section{Preliminaries}
Diffusion models aim to learn a mapping from some simple prior distribution of Gaussian noise $p(\bfx)$ to the data distribution $q(\bfx)$.
The namesake for this class of models comes from the forward diffusion process, an It\^o SDE given by:
\begin{equation}
    \label{eq:forward_ito}
    \rmd \bfX_t = f(t)\bfX_t + g(t)\;\rmd \bfW_t,
\end{equation}
where $f, g \in \mathcal{C}^{1}([0,T])$
form the drift and diffusion coefficients of the SDE, and where $\{\bfW_t\}_{t \in [0,T]}$ is the standard Wiener process on the time interval $[0, T]$.
The \textit{reverse-time} SDE~\citep{anderson1982reverse} of~\cref{eq:forward_ito} is found to be:
\begin{equation}
    \label{eq:backward_ito}
    \rmd \bfX_t = [f(t)\bfX_t + g^2(t) \nabla_\bfx \log p_t(\bfX_t)] \; \rmd t + g(t) \; \rmd \widebar\bfW_t,
\end{equation}
where $\rmd t$ is a \textit{negative} timestep and $\{\widebar\bfW_t\}_{t \in [0,T]}$ is the standard Wiener process in reverse-time.
The aim of diffusion models, then, is to learn the score function $\bm{s}_\theta(\bfx, t) = \nabla_\bfx \log p_t(\bfx)$~\citep{song2021scorebased} or a closely related quantity, \eg, noise prediction~\citep{song2021denoising,ho2020denoising} or data prediction~\citep{kingma2021variational}.
Once the score function or another equivalent parameterization is learned, we can use it to sample $q(\bfx)$ by first sampling some $\bfx_T \sim p(\bfx)$ and then numerically solving~\cref{eq:backward_ito} in \textit{reverse-time} with our model of the score function with some kind of SDE solver, \eg, Euler-Maruyama.




\section{Motivation}
A common task with diffusion models is to encode samples from the data distribution $q(\bfx)$ back into the noise distribution $p(\bfx)$. This can be for tasks such as image editing~\citep{meng2022sdedit,nie2024blessing}, or for performing the backward pass in solving the continuous adjoint equations~\citep{blasingame2024adjointdeis,pan2024adjointdpm}.
However, simply solving the diffusion ODE forwards in time\footnote{\NB, due to the time conventions with diffusion models, sampling is performed backwards in time and encoding is forwards in time.} can raise some issues due to truncation errors and stability concerns.
Furthermore, integrating the diffusion SDE in forwards time requires a bit more care due to the It\^o stochastic integral.

\textbf{Truncation errors.}
Suppose we have some numerical scheme for sampling the diffusion model,
\eg, DDIM~\citep{song2021denoising}, DEIS~\citep{zhang2023fast}, or DPM-Solver~\citep{lu2022dpm,lu2022dpm++}; such that we can sample the solution trajectory $\{\tilde\bfx_{t_n}\}_{n=0}^N$ with timesteps $\{t_N = T > t_{N-1} \cdots > t_0 = 0\}$ and where $\tilde\bfx_{t_N} \sim q(\bfx)$ and the rest of the trajectory is found via the numerical scheme.
Now, suppose we apply the same scheme forwards in time with initial condition $\widecheck\bfx_{t_0} = \tilde\bfx_{t_0}$ to construct the encoding trajectory $\{\widecheck\bfx_{t_n}\}_{n=1}^N$, we have no guarantee that encoding trajectory will equal the sampling trajectory for $n > 0$, \ie, $\widecheck\bfx_{t_n} = \tilde\bfx_{t_n}$ does not \textit{necessarily} hold for $n > 0$.
This has a few implications: a) a scheme which is used for encoding and then sampling is not guaranteed to have exact inversion and b) the samples generated in the encoding trajectory differ from the samples in the sampling trajectory, causing inaccurate gradients when using the continuous adjoint equations.

\textbf{Stability.} Similarly, there are concerns about the numerical stability of the numerical scheme solved in both directions of time.
Consider the test ODE $\dot y(t) = \lambda y(t)$ with $\lambda < 0$ defined in the interval $[0, T]$ with the initial condition $y_0$.
An ODE solver with a nontrivial region of convergence~\citep[see][]{hairer2002stiff_odes} will be able to solve this ODE without much trouble, as the magnitude of the errors decreases exponentially since $\lambda$ is negative.
However, the backwards in time solve from $y(T)$ will suffer numerical instability as the errors will \textit{grow exponentially}.
\NB, this problem is simply reversed if the solver has good stability in reverse-time, with the solve in forward-time now suffering.
Furthermore, the poor stability in the backward solve is an issue for diffusion guidance techniques which use the continuous adjoint equations.

As such we desire a numerical solver for diffusion models which has \textit{alignment} in truncation errors in both directions of time along with reasonable numerical stability in both directions.

\section{Reversible Solvers for Diffusion SDEs}
Now as we alluded earlier, there are some difficulties with solving~\cref{eq:backward_ito} in forward-time as the It\^o integral is adapted to the backward filtration induced by $\{\widebar\bfW_t\}$.
It is much easier to instead use the \textit{Fisk-Stratonovich symmetric integral}~\citep[see][]{kunita2019stochastic}, which has the nice property of symmetry in time.
Now as the It\^o integral term in~\cref{eq:backward_ito} is only additive noise, we can freely switch to the Stratonovich integral without consequence, thereby rewriting~\cref{eq:backward_ito} as an integral equation of the form:
\begin{equation}
    \label{eq:backward_stratonovich}
    \bfX_t = \int_T^t f(\tau)\bfX_\tau + g^2(\tau)\nabla_\bfx \log p_\tau(\bfX_\tau)\;\rmd \tau + \int_T^t g(\tau) \circ \rmd \bfW_\tau.
\end{equation}
\NB, the Wiener process $\{\bfW_t\}$ is not the same process as the one used in~\cref{eq:forward_ito}, but we adopt it for a simpler notation, that we ask the reader to keep this in mind.\footnote{More technically, we define the Stratonovich integral \wrt to the natural \textit{two-sided} filtration induced by $\{\bfW_t\}$ defined in forward-time. For more details we refer to~\citet{kunita2019stochastic} and for an application in a modern ML context we recommend~\citet{li2020scalable}.}
The drift and diffusion coefficients, $(f, g)$, are defined via the schedule $(\alpha_t, \sigma_t)$ in the Variance Preserving (VP) formulation~\citep{song2021scorebased} with
    $f(t) = \frac{\rmd \log \alpha_t}{\rmd t}, g^2(t) = \frac{\rmd \sigma_t^2}{\rmd t} - 2 \frac{\rmd \log \alpha_t}{\rmd t}\sigma_t^2$.

Let $\bfx_{0|t}(\bfx) \coloneq \ex[\bfX_0|\bfX_t=\bfx]$ be the data prediction model.
Following~\citet{lu2022dpm} we let $\lambda \coloneq \log \alpha_t / \sigma_t$ denote one-half the log-SNR (Signal to Noise Ratio).
Since $\lambda_t$ is a strictly decreasing function of $t$, there exists an inverse function $t_\lambda(\cdot)$ such that $t = t_\lambda(\lambda_t)$ and, with abuse of notation, we let $\bfx_\lambda \coloneq \bfx_{t_\lambda(\lambda)}$ and $\bfx_{\lambda_0|\lambda}(\bfx_\lambda) \coloneq \bfx_{0|t_\lambda(\lambda)}(\bfx_{t_\lambda(\lambda)})$.
We can then simplify the integral equation in~\cref{eq:backward_stratonovich} using \textit{exponential integrators}~\citep{hochbruck2010exponential}---a common technique for diffusion models~\citep[see][]{lu2022dpm,zhang2023fast,gonzalez2023seeds,blasingame2024adjointdeis}---to simplify the solution of the Stratonovich integral equation.
This result is presented in~\cref{prop:exact_sol_sde} with the full proof in~\cref{proof:exact_sol_sde}.
\begin{theorembox}
\begin{proposition}[Exact solution of diffusion SDEs]
    \label{prop:exact_sol_sde}
    Given an initial value $\bfX_s(\omega) = \bfx_s$ at time $s \in [0,T]$  the exact solution of~\cref{eq:backward_stratonovich} can be expressed as:
    \begin{equation}
        \label{eq:exact_sol_sde}
        \bfX_t =
        \underbrace{\vphantom{\int_{\lambda_s}}\frac{\sigma_t}{\sigma_s}e^{\lambda_s - \lambda_t}\bfX_s}_{\substack{\textrm{\bfseries Linear term}\\\textrm{No truncation errors}}}
        + \underbrace{2\alpha_t\int_{\lambda_s}^{\lambda_t} e^{2(\lambda - \lambda_t)} \bfx_{\lambda_0 | \lambda}(\bfX_\lambda)\;\rmd\lambda}_{\substack{\textrm{\bfseries Approximated term}\\\textrm{Truncation errors}}}
        + \underbrace{\vphantom{\int_{\lambda_s}}\sqrt 2\sigma_t e^{-\lambda_t} \bfW_{\varsigma_t, \varsigma_s}}_{\substack{\textrm{\bfseries Brownian bridge}\\\textrm{No truncation errors}}},
    \end{equation} 
    where $\varsigma_t = \frac 12 (e^{2\lambda_t} - e^{2\lambda_T})$.
\end{proposition}
\end{theorembox}
\textit{Proof sketch.}
First we simplify the integral equation using the method\textit{variation-of-parameters} to get an \textit{exponential integrator}~\citep{hochbruck2010exponential}.
The we express the Stratonovich integral as a continuous local martingale in backwards time enabling us to use the Dubins-Schwarz theorem~\citep{dubins1965continuous} to rewrite the integral as time-changed Brownian motion.

The exponential integral term in~\cref{eq:exact_sol_sde} can be approximated via a truncated Stratonovich Taylor expansion~\citep{kloeden1991stratonovich} and the time-changed Brownian bridge can be efficiently calculated by using the Brownian interval~\citep[Algorithm 3]{kidger2021efficient} in \textbf{both} reverse-time and forward-time.


\subsection{Construction of the Reversible Solver}
Now equipped with a simplified form of the diffusion SDE we develop a reversible solver based on approximations of the exponential integral in~\cref{eq:exact_sol_sde}.
Taking inspiration from the recent work of~\citet{mccallum2024efficient}, wherein they design reversible solvers for neural ODEs with a non-trivial region of convergence, we apply their insight of using a coupling parameter to construct an \textit{algebraically reversible} solver to~\cref{eq:exact_sol_sde}.
Note, we assume that our data prediction model is trained to zero loss, \ie, $\hat\bfx_{0|t} = \bfx_{0|t}$.

\textbf{Forward pass.}
Suppose that we have a single-step solver for the exponential integral term in~\cref{eq:exact_sol_sde} given by $\Psi_h : \R \times \R^d \to \R^d$ where $h$ denotes the step size $h \coloneq \lambda_t - \lambda_s$ and timesteps $\{t_n\}_{n=1}^N$ which is defined in \textit{reverse-time}.
Let $\zeta \in (0, 1)$ be a coupling parameter that determines the stability of the forward and backward passes and let $\hat\bfx$ be an augmented state for algebraic reversibility.
For notational simplicity let $\bfx_n \coloneq \bfx_{t_n}$ and likewise for other variables.
We then define the forward pass as
\begin{standoutbox}
    \begin{equation}
    \begin{aligned}
        \bfx_{n+1} &= \zeta \bfx_n + (1 -\zeta)\hat\bfx_n + \frac{\sigma_{n+1}}{\sigma_n}e^{-h}\hat\bfx_n + 2\alpha_{n+1} \Psi_h(t_n, \hat\bfx_n)\\ &+ \sqrt2\sigma_{n+1}e^{-\lambda_{n+1}}\bfW_{\varsigma_{n+1}, \varsigma_n},\\
        \hat\bfx_{n+1} &= \hat\bfx_n - \frac{\sigma_n}{\sigma_{n+1}}e^{h}\bfx_{n+1} - 2\alpha_n \Psi_{-h}(t_{n+1}, \bfx_{n+1}) + \sqrt2\sigma_ne^{-\lambda_n}\bfW_{\varsigma_{n+1}, \varsigma_n}.
    \end{aligned}
    \label{eq:forward_pass}
    \end{equation}
\end{standoutbox}

\textbf{Backward pass.} The backward solve can then be compute algebraically from~\cref{eq:forward_pass} as
\begin{standoutbox}
    \begin{equation}
    \begin{aligned}
        \hat\bfx_n &= \hat\bfx_{n+1} + \frac{\sigma_n}{\sigma_{n+1}}e^{h}\bfx_{n+1} + 2\alpha_n \Psi_{-h}(t_{n+1}, \bfx_{n+1}) - \sqrt2\sigma_ne^{-\lambda_n}\bfW_{\varsigma_{n+1}, \varsigma_n},\\
        \bfx_n &= \zeta^{-1}\bfx_{n+1} + (1 -\zeta^{-1})\hat\bfx_n - \frac{\sigma_{n+1}}{\sigma_n}e^{-h}\zeta^{-1}\hat\bfx_n + 2\alpha_{n+1}\zeta^{-1} \Psi_h(t_n, \hat\bfx_n)\\& - \sqrt2\sigma_{n+1}e^{-\lambda_{n+1}}\zeta^{-1}\bfW_{\varsigma_{n+1}, \varsigma_n}.
    \end{aligned}
    \label{eq:backward_pass}
    \end{equation}
\end{standoutbox}

\begin{figure}[t]
    \centering
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{figures/ddim20.png}
        \caption{DDIM inversion with 20 steps.}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{figures/reversible_dpm20.png}
        \caption{Reversible DDIM with 20 steps.}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{figures/reversible_sde20.png}
        \caption{Reversible diffusion SDE with 20 steps.}
    \end{subfigure}
    \caption{
    Comparison of different solvers for diffusion models on an image interpolation task with interpolation values $0, 0.1, 0.25, 0.5, 0.75, 0.9, 1$ (from left to right).
    The leftmost and rightmost images are the original images: $\bfx_0^{(a)}$ and $\bfx_0^{(b)}$.
    The same number of steps are used for both the encoding and sampling procedure.
    For the reversible methods $\zeta = 0.999$. Original faces from FRLL~\citep{frll}.
    }
    \label{fig:interp_plot}
\end{figure}

\section{Interpolation Experiment}
Following~\citet{song2021denoising} we test our method with a small experiment of image interpolation, \ie, given two real images $\bfx_0^{(a)}$ and $\bfx_0^{(b)}$ they are inverted to find $\bfx_T^{(a)}$ and $\bfx_{T}^{(b)}$.
These representations are then interpolated via spherical linear interpolation~\citep{shoemake1985animating} to obtain an interpolated latent representation.
For the reversible solvers we also interpolate $\hat\bfx$.
For this experiment we used a Latent Diffusion Model (LDM)~\citep{rombach2022high} trained on the CelebA-HQ dataset~\citep{karras2018progressive} at a $256 \times 256$ resolution.
Further details can be found in~\cref{app:experimental_details}.

In~\cref{fig:interp_plot} we plot an example interpolation with three different solvers: a) the standard DDIM inversion which serves as the baseline, b) McCallum and Foster's method applied to DDIM, and c) the reversible solver described in~\cref{eq:forward_pass,eq:backward_pass} with first-order truncated Stratonovich-Taylor expansion.
Notice how the DDIM solver struggles to accurately invert the original images; whereas the same solver when used in conjunction with McCallum and Foster's method yields \textit{exact} inversion.
The poor performance of DDIM inversion is unsurprising as we used a low number of discretization steps on purpose to stress test the solvers.
The SDE solver with only 20 sampling steps performs better than the ODE-based inversion with less visible distortions while still achieving exact inversion.

\section{Conclusion}
In this work we propose a novel \textit{algebraically reversible} solver for diffusion SDEs enabling the exact inversion of samples into the latent space with diffusion SDEs.
To the best of our knowledge we are the first to propose a method for exactly inverting diffusion SDEs which \textit{does not} store the entire realization of the Wiener process in memory.
We illustrate the utility of our solver on the experiment of image interpolation.
This work has many potential applications in the editing of samples with diffusion SDEs and for eliminating truncation errors in guided generation methods which use the continuous adjoint equations.

\textbf{Limitations.} As preliminary work we have only explored a proof of concept experiments with image interpolation to illustrate the stability under perturbations and exact inversion of our method.
In the future we plan to explore using this solver for guided generation via the continuous adjoint equations and perform a more detailed analysis of the stability.




\bibliography{bib}
\bibliographystyle{iclr2025_delta}

\newpage
\appendix

\section{Related Works}
In this section we provide a small discussion on related works.

\textbf{Reversible solvers.} The asynchronous leapfrog method~\citep{zhuang2021mali} and the reversible Heun~\citep{kidger2021efficient} were the standard reversible solvers until recently~\citep[\cf][]{mccallum2024efficient}, with the former applicable to general neural ODEs and the latter applicable to neural ODEs, CDEs, and SDEs.
Recent work by~\citet{mccallum2024efficient} has improved upon these older solvers for neural ODEs by showing it is possible to construct reversible solvers with a non-trivial region of stability.

\textbf{Exact inversion with diffusion models.}
The work of~\citet{wallace2023edict} proposes a reversible solver for diffusion ODEs by solving a dual auxiliary state of the model and then interpolating between the two states.
Later work by~\citet{zhang2023bdia} explores a reversible solver by using bidirectional integration approximation scheme as a sort of leapfrog method.
More recent work by~\citet{wang2024belm} has explored the exact inversion of diffusion ODEs via bidirectional linear multi-step methods.
However, linear multi-step methods and leapfrog methods often suffer from poor stability~\citep{shampine2009stability}.


\textbf{Inversion with diffusion SDEs.}
More closely related to our work~\citet{wu2022unifying} propose a method for the exact inversion of diffusion SDEs.
Given a particular realization of the Wiener process that admits $\bfx_t \sim \mathcal{N}(\alpha_t \bfx_0 \mid \sigma_t^2\mathbf{I})$, then given $\bfx_s$ and noise $\bseps_s \sim \mathcal{N}(\mathbf0, \mathbf I)$ we can calculate
\begin{equation}
    \bfx_t = \frac{\alpha_t}{\alpha_s} \bfx_s + 2\sigma_t(e^h-1)\hat\bfx_{T|s}(\bfx_s) + \sigma_t\sqrt{e^{2h}-1}\bseps_s.
\end{equation}
\citet{wu2022unifying} propose to invert this by first calculating for two samples $\bfx_t$ and $\bfx_s$ the noise $\bseps_s$ can be calculated by rearranging the previous equation to find
\begin{equation}
    \bseps_s = \frac{\bfx_t - \frac{\alpha_t}{\alpha_s}\bfx_s + 2 \sigma_t (e^h - 1) \bseps_\theta(\bfx_s, \bfz, s)}{\sigma_t \sqrt{e^{2h} - 1}}
\end{equation}
With this the sequence $\{\bseps_{t_i}\}_{i=1}^N$ of added noises can be calculated which can be used to reconstruct the original input from the initial realization of the Wiener process.
However, unlike our approach, this process requires storing the entire realization in memory.


\section{Proof of \texorpdfstring{~\cref{prop:exact_sol_sde}}{Exact Solution of Diffusion SDEs}}
\label{proof:exact_sol_sde}
We restate~\cref{prop:exact_sol_sde} here:
\begin{proposition}[Exact solution of diffusion SDEs]
    Given an initial value $\bfX_s(\omega) = \bfx_s$ at time $s \in [0,T]$  the exact solution of~\cref{eq:backward_stratonovich} can be expressed as:
    \begin{equation}
        \bfX_t =
        \underbrace{\vphantom{\int_{\lambda_s}}\frac{\sigma_t}{\sigma_s}e^{\lambda_s - \lambda_t}\bfX_s}_{\substack{\textrm{\bfseries Linear term}\\\textrm{No truncation errors}}}
        + \underbrace{2\alpha_t\int_{\lambda_s}^{\lambda_t} e^{2(\lambda - \lambda_t)} \bfx_{\lambda_0 | \lambda}(\bfX_\lambda)\;\rmd\lambda}_{\substack{\textrm{\bfseries Approximated term}\\\textrm{Truncation errors}}}
        + \underbrace{\vphantom{\int_{\lambda_s}}\sqrt 2\sigma_t e^{-\lambda_t} \bfW_{\varsigma_t, \varsigma_s}}_{\substack{\textrm{\bfseries Brownian bridge}\\\textrm{No truncation errors}}},
    \end{equation} 
    where $\varsigma_t = \frac 12 (e^{2\lambda_t} - e^{2\lambda_T})$.
\end{proposition}

We also restate the Dubins-Schwarz representation theorem~\citep{dubins1965continuous} (sometimes referred to as the Dambis representation theorem) below:
\begin{theorem}[Dubins-Schwarz representation theorem]
    \label{thm:dubin_schwarz}
    Let $M$ be a continuous local martingale adapted to a filtration $\{\F_t\}_{t \in [0,\infty)}$ vanishing at zero that satisfies $\langle M \rangle_\infty = \infty$ almost surely. Define the stopping times $\{\tau_t\}_{t \in [0, \infty)}$ by
    \begin{equation*}
        \tau_t = \inf\;\{s \in [0, \infty) : \langle M \rangle_s > t\} = \sup\;\{s \in [0, \infty) : \langle M \rangle_s = t\}.
    \end{equation*}
    Then, $\{M_{\tau_t}\}_{t \in [0, \infty)}$ is a standard Brownian motion $\{B_t\}_{t \in [0,\infty)}$ and, for every $t \in [0, \infty)$,
    \begin{equation*}
        M_t = B_{\langle M \rangle_t}.
    \end{equation*}
\end{theorem}

Now we state our proof for~\cref{prop:exact_sol_sde}.
\begin{proof}
    First we restate our Stratonovich integral equation from~\cref{eq:backward_stratonovich}:
    \begin{equation}
        \bfX_t = \bfX_s + \int_s^t f(\tau)\bfX_\tau + g^2(\tau)\nabla_\bfx \log p_\tau(\bfX_\tau)\;\rmd \tau + \int_s^t g(\tau) \circ \rmd \bfW_\tau,
    \end{equation}
    where
    \begin{align}
        f(t) &= \frac{\rmd \log \alpha_t}{\rmd t},\\
        g^2(t) &= \frac{\rmd \sigma_t^2}{\rmd t} - 2\frac{\rmd \log \alpha_t}{\rmd t}\sigma_t^2 = -2\sigma_t^2\frac{\rmd \lambda_t}{\rmd t}.
    \end{align}
    We can express the score function in terms of the data prediction model $\bfx_{0|t}(\bfx) \coloneq \ex[\bfX_0 | \bfX_t = \bfx]$:    
    \begin{align}
        \nabla_\bfx \log p_t(\bfx) &= \frac{1}{\sigma_t^2}\bfx - \frac{\alpha_t}{\sigma_t^2}\bfx_{0|t}(\bfx).\label{eq:app:score_as_data_pred}
    \end{align}
    Using~\cref{eq:app:score_as_data_pred} we can rewrite~\cref{eq:backward_stratonovich} as
    \begin{align}
        \bfX_t &= \bfX_s + \int_s^t \bigg(f(\tau) + \frac{g^2(\tau)}{\sigma_\tau^2}\bigg ) \bfX_\tau - \frac{\alpha_\tau g^2(\tau)}{\sigma_\tau^2}\bfx_{0|t}(\bfX_\tau)\;\rmd\tau + \int_s^t g(\tau) \circ \rmd \bfW_\tau.
    \end{align}
    Then we can write the Stratonovich integral equation as
    \begin{equation}
        \bfX_t = \bfX_s + \int_s^t a(\tau)\bfX_\tau + b(\tau) \bfx_{0|\tau}(\bfX_\tau)\;\rmd\tau + \int_s^t g(\tau) \circ \rmd \bfW_\tau,
    \end{equation}
    where
    \begin{align}
        a(t) &= \frac{\rmd \log \alpha_t}{\rmd t} + \frac{g^2(t)}{\sigma_t^2} = \frac{1}{\sigma_t^2} \frac{\rmd \sigma_t^2}{\rmd t} - \frac{\rmd \log \alpha_t}{\rmd t},\\
        b(t) &= - \frac{\alpha_tg^2(t)}{\sigma_t^2} = 2\alpha_t \frac{\rmd \lambda_t}{\rmd t}.
    \end{align}
    Then we can use the \textit{variation-of-parameters} formula to find
    \begin{equation}
        \label{eq:app:if_strat_eq1}
        \bfX_t = \Phi_a(s, t)\bfX_s + \int_s^t \Phi_a(\tau, t)b(\tau)\bfx_{0|\tau}(\bfX_\tau)\;\rmd\tau + \int_s^t \Phi_a(\tau, t)g(\tau) \circ \rmd \bfW_\tau,
    \end{equation}
    where $\Phi_a(s, t) \coloneq \exp \int_s^t a(\tau)\;\rmd \tau$ is the integrating factor.
    This technique has been employed by other works to separate the linear and non-linear component of diffusion models~\citep{lu2022dpm,gonzalez2023seeds,blasingame2024adjointdeis}.
    We then simplify $\Phi_a(s, t)$ such that
    \begin{align}
        \Phi_a(s,t) &= \exp \bigg(\int_s^t \frac{1}{\sigma_\tau^2} \frac{\rmd \sigma_\tau^2}{\rmd \tau}\; \rmd \tau - \int_s^t \frac{\rmd \log \alpha_\tau}{\rmd \tau}\;\rmd \tau\bigg),\nonumber\\
        &= \exp \bigg(\int_{\sigma_s}^{\sigma_t} \frac{1}{\sigma^2} \; \rmd \sigma^2 - \int_s^t \rmd \log \alpha_\tau\bigg),\nonumber\\
        &= \exp \bigg(\log \sigma_t^2 - \log \sigma_s^2 - (\log \alpha_t - \log \alpha_s)\bigg ),\nonumber\\
        &= \exp \bigg(\log \frac{\sigma_t^2}{\sigma_s^2} - \log \frac{\alpha_t}{\alpha_s}\bigg ),\nonumber\\
        &= \exp \bigg(\log \frac{\sigma_t^2\alpha_s}{\sigma_s^2\alpha_t}\bigg ),\nonumber\\
        &= \frac{\sigma_t^2\alpha_s}{\sigma_s^2\alpha_t}.
    \end{align}
    Another useful form to express the integrating factor in, is in terms of $\lambda_t$, which we find:
    \begin{align}
        \Phi_a(s,t) &= \frac{\sigma_t^2\alpha_s}{\sigma_s^2\alpha_t},\nonumber\\
        &= \frac{\sigma_t}{\sigma_s}\exp \bigg(\log \frac{\sigma_t\alpha_s}{\sigma_s\alpha_t}\bigg),\nonumber\\
        &= \frac{\sigma_t}{\sigma_s}\exp \bigg(\log \frac{\alpha_s}{\sigma_s} - \log \frac{\alpha_t}{\sigma_t}\bigg),\nonumber\\
        &= \frac{\sigma_t}{\sigma_s}e^{\lambda_s - \lambda_t}.
    \end{align}
    Using this expression for $\Phi_{a}(s,t)$ we can rewrite~\cref{eq:app:if_strat_eq1} as:
    \begin{equation}
        \label{eq:app:if_strat_eq2}
        \bfX_t = \frac{\sigma_t}{\sigma_s}e^{\lambda_s - \lambda_t}\bfX_s + 2\sigma_t\int_s^t e^{\lambda_\tau - \lambda_t}\frac{\alpha_\tau}{\sigma_\tau} \frac{\rmd \lambda_\tau}{\rmd \tau} \bfx_{0|\tau}(\bfX_\tau)\;\rmd\tau + \sqrt 2\sigma_t\int_s^t e^{\lambda_\tau - \lambda_t}\sqrt{-\frac{\rmd \lambda_\tau}{\rmd \tau}} \circ \rmd \bfW_\tau.
    \end{equation}
    We first simplify the integral term
    \begin{align}
        2\sigma_t\int_s^t e^{\lambda_\tau - \lambda_t}\frac{\alpha_\tau}{\sigma_\tau} \frac{\rmd \lambda_\tau}{\rmd \tau} \bfx_{0|\tau}(\bfX_\tau)\;\rmd\tau &= 2\sigma_t\int_s^t e^{\lambda_\tau - \lambda_t} e^{\lambda_\tau} \frac{\rmd \lambda_\tau}{\rmd \tau} \bfx_{0|\tau}(\bfX_\tau)\;\rmd\tau,\nonumber\\
        &= 2\sigma_t\int_s^t e^{2\lambda_\tau - \lambda_t} \frac{\rmd \lambda_\tau}{\rmd \tau} \bfx_{0|\tau}(\bfX_\tau)\;\rmd\tau,\nonumber\\
        &= 2\sigma_t\int_{\lambda_s}^{\lambda_t} e^{2\lambda - \lambda_t} \bfx_{\lambda_0 | \lambda}(\bfX_\lambda)\;\rmd\lambda,\nonumber\\
        &= 2\alpha_t\int_{\lambda_s}^{\lambda_t} e^{2(\lambda - \lambda_t)} \bfx_{\lambda_0 | \lambda}(\bfX_\lambda)\;\rmd\lambda.\\
        &= 2\frac{\sigma_t^2}{\alpha_t}\int_{\lambda_s}^{\lambda_t} e^{2\lambda} \bfx_{\lambda_0 | \lambda}(\bfX_\lambda)\;\rmd\lambda.
    \end{align}
    To simplify the stochastic integral term we first define a continuous martingale $\boldsymbol{M}_t$ via the stochastic integral:
    \begin{equation}
        \boldsymbol{M}_t \coloneq -\int_T^t e^{\lambda_\tau}\sqrt{-\frac{\rmd \lambda_\tau}{\rmd \tau}} \circ \rmd \bfW_\tau.
    \end{equation}
    We choose time $T$ as our starting point for the martingale rather than 0 and then integrate in \textit{reverse-time}, hence the negative sign.
    We can then express our stochastic integral in~\cref{eq:app:if_strat_eq2} as
    \begin{equation}
        \int_s^t e^{\lambda_\tau}\sqrt{-\frac{\rmd \lambda_\tau}{\rmd \tau}} \circ \rmd \bfW_\tau = \boldsymbol{M}_t - \boldsymbol{M}_s.
    \end{equation}
    Next we establish a few properties of this martingale.
    First, $\boldsymbol{M}_0 = 0$ by construction.
    Second, the quadratic variation of $\boldsymbol{M}_t$ is found to be
    \begin{align}
        \langle\boldsymbol{M}\rangle_t &= -\int_T^t \bigg(e^{\lambda_\tau} \sqrt{-\frac{\rmd \lambda_\tau}{\rmd \tau}}\bigg)^2 \; \rmd \tau,\nonumber\\
        &= \int_T^t e^{2\lambda_\tau} \frac{\rmd \lambda_\tau}{\rmd \tau} \; \rmd \tau,\nonumber\\
        &= \int_{\lambda_T}^{\lambda_t} e^{2\lambda} \; \rmd \lambda,\nonumber\\
        \varsigma_t &= \frac{1}{2}\Big(e^{2\lambda_t} - e^{2\lambda_T}\Big),
    \end{align}
    where we let $\varsigma_t$ denote our new time variable.
    Now we have a deterministic mapping from the original time to our new time via:
    \begin{equation}
        \begin{aligned}
            \varsigma_t: \;&[0, T] \to [0, \infty),\\
            &t \mapsto \langle \boldsymbol{M} \rangle_t.
        \end{aligned}
    \end{equation}
    Notice that as $t \to 0$ and $\sigma_t \to 0$ we have $\langle \boldsymbol{M} \rangle_t \to \infty$.
    Such a martingale can be expressed as time-changed Brownian motion, see~\cref{thm:dubin_schwarz}, such that $\boldsymbol{M}_t = \bfW_{\langle\boldsymbol{M}\rangle_t}$.
    For notational simplicity we define the Brownian bridge in this time-changed as
    \begin{equation}
        \bfW_{\varsigma_t, \varsigma_s} \coloneq \bfW_{\varsigma_t} - \bfW_{\varsigma_s}.
    \end{equation}
    Thus we have
    \begin{equation}
       \bfX_t = \frac{\sigma_t}{\sigma_s}e^{\lambda_s - \lambda_t}\bfX_s + 2\alpha_t\int_{\lambda_s}^{\lambda_t} e^{2(\lambda - \lambda_t)} \bfx_{\lambda_0 | \lambda}(\bfX_\lambda)\;\rmd\lambda + \sqrt 2\sigma_t e^{-\lambda_t} \bfW_{\varsigma_t, \varsigma_s},
    \end{equation}
    thereby finishing the proof.
    
\end{proof}


\section{Experimental Details}
\label{app:experimental_details}
All experiments were run on a single NVIDIA Telsa V100 32GB.
We used the Brownian interval~\citep{kidger2021efficient} from the \href{https://github.com/google-research/torchsde}{\texttt{torch-sde}} package.
The faces are from the Face Research Lab London (FRLL) dataset~\citep{frll}.
We used the pre-trained LDM model from~\citet{rombach2022high} which can be found here: \url{https://huggingface.co/CompVis/ldm-celebahq-256}.


\end{document}
