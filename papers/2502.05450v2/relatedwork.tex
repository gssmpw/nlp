\section{Related Work}
\subsection{Reinforced Fine-tuning for Large Models}

RL has been widely adopted for fine-tuning LLMs and VLMs. Early works have primarily focused on RL incorporating human feedback \cite{christiano2017deep, ouyang2022training, luong2024reft, casper2023open, zhai2024fine} by learning from human preferences or by integrating task-specific rewards without explicit human preference \cite{ramamurthy2022reinforcement, bai2024digirl, carta2023grounding, kimin2021pebbla}. While many of these approaches employ on-policy algorithms (e.g., PPO \cite{schulman2017proximal}) to fine-tune pre-trained policies \cite{bai2024digirl, gupta2019relay, shao2024deepseekmath}, they typically demand large amounts of interaction data to achieve desirable performance \cite{ball2023efficient, li2024selu}. While RL has demonstrated success in many domains, it typically learns within self-generated synthetic environments rather than real-world environments. This gap prevents direct transfer for VLA models, which require real-world interaction. Our work addresses this discrepancy by developing RL frameworks tailored for efficient real-world VLA fine-tuning.
% While RL has demonstrated remarkable success across various domains, a critical distinction warrants attention when applying RL to fine-tune LLMs and VLMs. Those RL paradigms typically involve agents interacting with self-generated synthetic environments rather than authentic real-world settings. This fundamental discrepancy precludes direct experience transfer when adapting VLA models, as their fine-tuning inherently requires physical environment interactions. Our research primarily addresses this paradigm shift by developing principled RL frameworks that enable efficient VLA model fine-tuning in real-world deployment scenarios.
% In contrast to LLM or VLM fine-tuning, whose learning objectives primarily involve text or image-based tasks, VLA fine-tuning operates on physical robots, where interactions with real-world environments can be expensive and potentially unsafe. Consequently, our work focus on off-policy actor-critic RL methods \cite{chen2023boosting} that offer higher sample efficiency, making them more practical in real-world scenarios with strict safety and resource constraints. We aim to address the fine-tuning challenges of adapting VLA models to complex, safety-critical tasks involving direct robotic manipulation.

\subsection{Real-world RL Systems}

Real-world robotic RL systems require algorithms that are both sample-efficient in handling high-dimensional inputs and flexible enough to accommodate practical considerations like reward specification and environment resets \cite{luo2024precise}. Several previous methods have effectively demonstrated policy learning directly in physical environments \cite{riedmiller2009reinforcement, johannink2019residual, luo2024serl, luo2024precise}, using both off-policy \cite{tony2022offline, luo2023rlif, hu2023reboot, russell2024continuously}, on-policy \cite{zhu2019dexterous, zhuang2023robot} methods, or posing "RL as supervised learning" \cite{mark2024policy, jan2007reinforcement}. Despite this progress, many real-world RL systems still demand prolonged training sessions or require large amounts of interaction data \cite{henery2020ingredients}, which can be impractical and risk-prone in contact-rich tasks. In contrast to previous methods that train from scratch, our work focuses on utilizing pre-trained VLA models to provide high-quality policy initialization. This approach effectively mitigates unnecessary exploratory behaviors in early RL phases, thereby optimizing both policy learning efficiency and operational safety in the training process.
% In contrast to these real-world RL setups that often train smaller models for quick policy convergence, our framework builds upon a pre-trained VLA model, providing an effective initialization to shorten the online training time. By integrating both offline and online RFT, we balance data efficiency with policy optimization while ensuring safe exploration under human interventions in real-world environments. 

\subsection{Offline-to-online Methods}

Offline-to-online RL aims to leverage offline datasets to initialize a policy, which is then fine-tuned via online interactions for improved sample efficiency \cite{lee2022offline}. Existing works commonly adopt an offline pre-training stage followed by an online fine-tuning stage \cite{lee2022offline, agarwal2022reincarnating, rafailov2023moto, nakamoto2024cal}, mixing offline and online data as training proceeds. This offline-to-online pipeline is similar to our proposed two-stage fine-tuning approach that exploits pre-collected data to bootstrap policy training and then fine-tunes the policy in the real-world tasks \cite{tony2022offline}. Most offline-to-online methods assume the availability of large-scale, diverse datasets with sufficient state coverage \cite{rajeswaran2017learning, nair2018overcoming}, a condition rarely met in real-world deployments. We explore leveraging pre-trained VLA models as the base policy to enable sample-efficient policy refinement, achieving superior fine-tuning performance even under stringent demonstration data constraints.
% Most offline-to-online methods typically presume the availability of large-scale, diverse datasets to ensure sufficient state coverage \cite{rajeswaran2017learning, nair2018overcoming}, which is a condition rarely satisfied in practical deployments. Our work addresses this fundamental mismatch. We explore leveraging pre-trained VLA models as the base policy to enable sample-efficient policy refinement, achieving superior fine-tuning performance even under stringent demonstration data constraints.
% However, while most Offline2Online methods assume large-scale offline datasets or well-aligned state distributions \cite{rajeswaran2017learning, nair2018overcoming} which is often collected in simulation, our real-world scenario is far more constrained: we have only a handful of demonstrations with insufficient state coverage for conventional offline RL approaches.