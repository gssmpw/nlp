\section{Reduced Model}
\label{sec:reduced_model}

\subsection{In-context multi-hop reasoning learning}

\subsection{Reduced Model}

We consider a simplified three-layer single-head attention-only transformer model. (For $k$-hop reasoning, we might want to consider a $(k+1)$-layer transformer.)

We consider the following simplified attentions. 

\begin{definition}[Simplified attentions]
\label{defn:simplified_attn} 
A (simplified) single-head causal self-attention $\attn(\mH;\mA,\mY)$ parameterized by $\mA, \mY \in \real^{d \times d}$ maps an input matrix $\mH \in \real^{d \times T}$ to
\begin{align*}
\attn(\mH;\mA, \mY) = \mY\mH\softmax(\mask(\mH^\transpose\mA\mH)) \in \real^{d\times T}.
\end{align*}
\end{definition}

Note that there is no layer normalization in the simplified attention mechanism and we reparameterize $\mA = \mK^\transpose\mQ$ and $\mY = \mO\mV$ for notation convenience.

Now, we consider the following attention-only multi-layer single-head transformer.

\begin{definition}[Attention-only single-head transformers]
\label{defn:simplified_transformer} 
    An $L$-layer (simplified) attention-only single-head transformer $\transformer(\cdot): \vocab^T \to \Delta(\vocab)$ parameterized by $\mP$, $\mU$, $\{(\mA^{(l)}, \mY^{(l)})\}_{l\in[L]}$, and $\Wreadout \in \real^{V \times d}$ receives a sequence of tokens $s_{1:T}$ as input and processes it to obtain $\mH$ as in \Cref{defn:transformer}.
    For each layer $l \in [L]$, we have
    \begin{align*}
        \mH^{(l)} =  \mH^{(l-1)} + \attn(\mH^{(l-1)}; \mA^{(l)},\mY^{(l)}), 
    \end{align*}
    and $\mH^{(0)} = \mH$. The output is still obtained by 
    \begin{align*}
        \transformer(s_{1:T}) = \softmax(\Wreadout\vh_T^{(L)})
    \end{align*}
    where $\vh_T^{(L)}$ is the $T$-th column of $\mH^{(L)}$.
\end{definition}


\subsection{Reparameterization}

There are two possible mechanisms by which a transformer performs in-context multi-hop reasoning.

\paragraph{Mechanism 1:  Binary lifting preprocessing.} Find the $k$-th ancestor using $O(\log k)$ layers for every node and do one query in the final layer. \todo{Adding a more rigorous description.}

\paragraph{Mechanism 2: Linear search.} At each layer, the current query node will find and copy its successor. \todo{Adding a more rigorous description.}

From the perspective of expressivity, Mechanism 1 requires exponentially fewer layers to perform the task. However, in practice, we observed that Mechanism 2 is preferred during the training dynamics.

\paragraph{Reparameterization for the lowest layer.} Under both mechanisms, the first layer serves as an induction head, where each token will attend to its previous token and copy its content. According to the data format of our studied problem, particularly, each token at position $(2i+1)$ for $i \geq 1$ will attend to the token at position $2i$. (Note that the first token is a $\bos$ token.) Therefore, we can assume for the first layer that
\begin{equation}
\label{eq:reparam_layer_1}
\begin{aligned}
    \vh_i^\transpose\mA^{(1)}\vh_j = \alpha \cdot \indicator\{ j=i+1, j \text{ is odd} \}.
\end{aligned}    
\end{equation}

After the first layer, the input of the second layer has the form of either $\vh_i$ or $\vh_i + \mY^{(1)}\vh_{i-1}$. One can think of them as two (almost) orthogonal subspaces. 

\todo{We might also want to ignore the factors such as $\frac{N}{N+\exp(\alpha)}$. This assumption should be reasonable due to the attention sink and value state drain~\citep{guo2024active}. Ideally, we would keep four parameters for theoretical analysis: the induction head effect $\alpha$ in Layer 1, the query to mid-token attention $\beta_1$ in Layer 2, the target token to the mid-token attention $\beta_2$ in Layer 2, and the query (which is also the start token) to target token attention $\gamma$ in Layer 3.  }


\paragraph{Reparameterization for middle layers.} The subsequent layers perform a linear search algorithm where the token at position $T$, $s_T$, will search the previous token whose parent matches $s_T$ and use that token to replace $s_T$. Mathematically, we assume that
\begin{equation}
\label{eq:reparam_layer_2_component}
\begin{aligned}
    & \vh_i^\transpose\mA^{(l)}\vh_j = \beta_0 \cdot \indicator\{i \text{ is even, } j \text{ is odd, } i < j \}, \\
    & (\mY^{(1)}\vh_i)^\transpose\mA^{(l)}\vh_j = \beta_1 \cdot \indicator\{s_i=s_j, i <j=T\},
    \\  & \vh_i^\transpose\mA^{(l)}(\mY^{(1)}\vh_j) = \beta_2 \cdot \indicator\{s_i=s_j, i<j\}, 
    \\  & (\mY^{(1)}\vh_i)^\transpose\mA^{(l)}(\mY^{(1)}\vh_j) = 0.
\end{aligned}
\end{equation}

$\beta_0$ represents the strength of the interim mechanism, which randomly predicts a conclusion that appears in the prompt. $\beta_1$ represents the strength of our observed sequence query mechanism while $\beta_2$ represents the strength of the binary lifting mechanism posited by previous literature.
% and 
% \begin{equation}
% \label{eq:reparam_layer_2_component_2}
% \begin{aligned}
%     &(\mY^{(1)}\vh_i)^\transpose\mA^{(l)}(\mY^{(1)}\vh_j) \\ =& \beta_0 \cdot \indicator\{i \text{ is even, } j \text{ is odd, } i < j \}
%     \\ +& \beta_1 \cdot \indicator\{s_i=s_j, j = T \}
%     \\ +& \beta_2 \cdot \indicator\{i \text{ is even, } j \text{ is odd, } j = T \}
% \end{aligned}
% \end{equation}

\paragraph{Underlying mechanisms.} According to our experiments, there is an interesting interim mechanism that drives the transformer to randomly pick an end node as the prediction of the next token. This interim mechanism provides a bias away from the binary lifting mechanism, resulting in the linear search mechanism in the later training phase. 

Specifically, in the lowest layer, each token pays attention to the preceding token (i.e., induction head); in the middle layer, each child token will pay attention to all preceding parent tokens and will add a negative version of their value states to the residual stream; in the last layer, the query token pays attention to all child tokens and aggregates their value tokens (note that the value of each end token is a superposition of themselves and the negative of their corresponding mid-token, which will cancel out with the value state of that mid-token.) \todo{Need to double-check and write more rigorously and intuitively}

% \paragraph{Two settings.} Setting 1: several chains intertwine. Setting 2: with noise: e.g., in two-hop reasoning settings, we have length-one and length-two chains. 

% \paragraph{Preliminary experimental results.} Setting 1: with two-hop and one-hop: 3-layer slow (model A); 4-layer fast (model B); linear algorithm (model A') instead of the log one (model C: idealized model); In 4-layer, the layer 1 and 2 are similar (with layer 2 being more important).

% Setting 2: only k-hop: 3-layer fast (model A'); linear algorithm.

% \paragraph{Our story:} Starting from A' in setting 2. The setting is more clean and the algorithm is straightforward. Second step: compare with model C (why the training dynamics prefer the mechanism of A instead of C).

\paragraph{Two-hop Observations}

\textbf{ProbSumFig:}
\begin{enumerate}
    \item 0-50: Learn to predict a random token among all ending tokens in two-hop reasonings.
    \item 50-850: Probability on mid tokens decreases, still uniformly picks one ending token.
    \item 850+: Phase transition occurs.
    \item 0-50: Learn to only focus on children tokens.
\end{enumerate}

\textbf{AttentionLayer0:}
\begin{enumerate}
    \item 700+: Attention from all children to parents increases; 800+: Fast increase observed.
    \item Target is similar to noise.
\end{enumerate}

\textbf{AttentionLayer1:}
\begin{enumerate}
    \item 800+: Query to mid1 attention increases; 850+: Fast increase observed.
    \item Mid1 to start (or all children to parents) undergoes phase transition, then gradually decreases.
    \item 300+: Attention from end to mid1 has been restricted.
    \item Query learns to distinguish between target and noise.
\end{enumerate}

\textbf{AttentionLayer2:}
\begin{enumerate}
    \item 850+: Query to end attention increases.
    \item Query learns to distinguish between target and noise.
\end{enumerate}

\textbf{Value State Meaning:}
\begin{enumerate}
    \item 0-50: Learn to predict itself.
\end{enumerate}