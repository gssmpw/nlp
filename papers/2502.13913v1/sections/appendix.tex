\section{Additional Training Details}
\label{app:sec_add_training_detail}

The transformers are trained with positional embeddings, pre-layer normalization, \(\softmax\) activation in $\attn$, and ReLU activation in $\mlp$. Optimization is performed using Adam with a fixed learning rate of 0.0003, $\beta_1=0.9$, $\beta_2=0.99$, $\epsilon=10^{-8}$, and a weight decay of 0.01. For SGD in the simulation of the 3-parameter model, we use a learning rate of 0.1. At each training step, data is resampled from the BB task with a batch size of  $B=512$  and sequence length  $N=23$. Unless stated otherwise, the model is trained for $10,000$ steps, with results remaining consistent across different random seeds.

When fine-tuning Llama2-7B on the two-hop reasoning task, we use Adam with a learning rate of $0.0001$, $\beta_1=0.9$, $\beta_2=0.99$, $\epsilon=10^{-8}$, weight decay$=0.01$, and train for $1,000$ steps. The model is evaluated on the validation set containing $1000$ samples. We use a batch size of $B=4$.

% \begin{figure*}[h]  
%     \centering
%     \includegraphics[width=\textwidth]{figs/illustration_tmp.pdf}  
%     \caption{Illustration of the internal mechanism of a three-layer transformer to perform in-context two-hop reasoning. Due to space limitations, we provide the details of the above mechanism in \Cref{sec:illustration_mechanism}.} 
%     \label{fig:mechanism_illustration}
% \end{figure*}

\section{Dataset Details}
\label{app:dataset_detail}
We construct a diverse set of six two-hop reasoning templates that span different semantic domains. To ensure controlled experimentation, we carefully curate the vocabulary: for personal names, we select common English given names with balanced gender representation; for locations and biological taxonomies, we use synthetic names generated via prompting deepseek-R1 \citep{guo2025deepseek} to reduce the influence of real-world knowledge. The templates and associated vocabulary are as follows:

\begin{tcbraster}[raster columns=2,raster equal height]
\begin{templatebox}
Relations: "[A] is the mother of [B]. [B] is the mother of [C]. Therefore, [A] is the grandmother of",
"[A] is the father of [B]. [B] is the father of [C]. Therefore, [A] is the grandfather of",

Geography: "[A] is a city in the state of [B]. The state of [B] is part of the country [C]. Therefore, [A] is located in",
"[A] lives in [B]. People in [B] speak [C]. Therefore, [A] speaks",

Biology: "[A] is a species in the genus [B]. The genus [B] belongs to the family [C]. Therefore, [A] is classified under the family",

Arithmetic: "[A] follows the time zone of [B]. [B] is three hours ahead of [C]. Therefore, [A] is three hours ahead of",
\end{templatebox}
\begin{templatebox}
locations: "Zorvath", "Tyseria", "Kryo", "Vynora", "Quellion", "Dras", "Luminax", "Vesperon", "Noctari", "Xyphodon", "Glacidae", "Ophirion", "Eryndor", "Solmyra", "Umbrithis", "Balthorien", "Ytheris", "Fendrel", "Havroth", "Marendor"

biology: "Fluxilus", "Varnex", "Dranthidae", "Zynthor", "Gryvus", "Myralin", "Thalorium", "Zephyra", "Aerinth", "Xyphodon", "Kryostis", "Glacidae", "Borithis", "Chrysalix", "Noctilura", "Phorvian", "Seraphid", "Uthrelin", "Eldrinth", "Yvorith"

languages: "English", "Spanish", "Mandarin", "Hindi", "Arabic", "French", "German", "Japanese", "Portuguese", "Russian", "Korean", "Italian", "Turkish", "Dutch", "Swedish", "Polish", "Hebrew", "Greek", "Bengali", "Thai"

names: "Ben", "Jack", "Luke", "Mark", "Paul", "John", "Tom", "Sam", "Joe", "Max", "Amy", "Emma", "Anna", "Grace",  "Kate", "Lucy", "Sarah", "Alice", "Alex", "Ruby"
\end{templatebox}
\end{tcbraster}






\section{Additional Details on Mechanisms}
\label{app:sec_DI_mechanism}

\subsection{Real attention maps}
The attention maps at step 400 (\Cref{fig:attn_step_400}) illustrate the random guessing mechanism during the slow learning phase. In layer 0, we observe uniform attention where each token attends to all previous tokens, establishing the initial information gathering phase. Moving to layer 1, the attention pattern shows child tokens attending equally to all previous parent tokens, effectively copying their information into a buffer space for subsequent processing. Finally, in layer 2, the query entity (the last token in the sequence) demonstrates uniform attention to all child tokens. This final attention pattern is crucial as it enables the model to distinguish between end and bridge tokens through the aggregation of value states, where the positive values of bridge tokens from child positions are canceled out by their negative values from parent positions. This mechanism ultimately allows the model to randomly select an end token as its prediction.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/step400attn_layer0_head0.pdf}
        \caption{Attention maps of layer 0, step 400}
        \label{fig:step400attn_layer0_head0}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/step400attn_layer1_head0.pdf}
        \caption{Attention maps of layer 1, step 400}
        \label{fig:step400attn_layer1_head0}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/step400attn_layer2_head0.pdf}
        \caption{Attention maps of layer 2, step 400}
        \label{fig:step400attn_layer2_head0}
    \end{subfigure}
    \caption{Attention maps of Step 400.}
    \label{fig:attn_step_400}
\end{figure}

The attention maps at step 10000 (\Cref{fig:attn_step_10000}) demonstrate the sequential query mechanism that emerges after the phase transition. In layer 0, each child token shows attention concentrated to its parent token, implementing a copying mechanism that stores parent information. In layer 1, we observe a highly selective attention pattern where the query entity attends only to the bridge entity whose corresponding source entity matches the query entity. This targeted attention allows the model to copy the relevant bridge entity to the query entity's buffer space. Finally, in layer 2, the query entity uses the bridge entity information collected from the previous layer to attend precisely to the corresponding end entity, which represents the correct answer. This sequential querying process, performing one hop per layer, enables the model to achieve perfect accuracy in the two-hop reasoning task.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/step10000attn_layer0_head0.pdf}
        \caption{Attention maps of layer 0, step 10000}
        \label{fig:step10000attn_layer0_head0}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/step10000attn_layer1_head0.pdf}
        \caption{Attention maps of layer 1, step 10000}
        \label{fig:step400attn_layer1_head0}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/step10000attn_layer2_head0.pdf}
        \caption{Attention maps of layer 2, step 10000}
        \label{fig:step10000attn_layer2_head0}
    \end{subfigure}
    \caption{Attention maps of Step 10000.}
    \label{fig:attn_step_10000}
\end{figure}

\subsection{The evidence for the meaning of value states and buffer spaces}
\paragraph{Value states through logit lens.} We analyze the semantic content of value states across different layers using the logit lens technique. To quantify how well value states encode token identity, we develop a measure of self-prediction strength. Our analysis begins by generating a batch of 512 input sequences. For each token $t$ and layer $l$, we aggregate the corresponding value states across all sequences and compute their average. This averaged representation is then passed through the final layer normalization and readout matrix to obtain logits. By applying softmax, we calculate the probability of the value state predicting its corresponding token $t$. This probability serves as a measure of how strongly the value state encodes information about its token identity. We average these probabilities across all tokens and track their dynamics during training. As shown in \Cref{fig:value_logit_lens}, the value states quickly develop a strong capacity for self-prediction, indicating they quickly learn to preserve token identity information.

\paragraph{Buffer spaces: value states are in the orthogonal space with respect to embeddings.} We further investigate the relationship between value states and embeddings to provide evidence for the buffer spaces. We compute the cosine similarity between the value states and embeddings of each token at different layers. Since the we use hidden dimension $D=256$, the baselin similarity between two random vectors is $1/\sqrt{D}$. As shown in \Cref{fig:buffer_space_similarity}, the similarity between value states and embeddings is lower than the baseline, indicating that the value states reside in an orthogonal space from the embeddings. This suggests that the value states serve as buffer spaces that store and process information independently from the embeddings. The buffer spaces enable the network to copy and manipulate information across different tokens.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \centering
        \caption{Dynamics of token self-prediction probabilities derived from value states using the logit lens technique.}
        \includegraphics[width=\linewidth]{figs/post-icml-figures/self_probs.pdf}
        \label{fig:value_logit_lens}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \centering
        \caption{Cosine similarity between value states and embeddings}
        \includegraphics[width=\linewidth]{figs/post-icml-figures/similarities.pdf}
        \label{fig:buffer_space_similarity}
    \end{subfigure}
    \caption{\textit{Left (a):} The dynamics of token self-prediction probabilities derived from value states using the logit lens technique. \textit{Right (b):} Cosine similarity between value states and embeddings.}
    \label{fig:value_states}
\end{figure}



\subsection{The (hypothetical) double induction head mechanism.}
Contrary to our observation, previous literature~\citep{sanford2024transformers} studies in-context multi-hop reasoning theoretically and constructed a hypothetical mechanism, which we call the double induction head mechanism when specialized to our two-hop reasoning settings. After copying the parent node in the first layer (\Cref{fig:mech:L1_copy}), the second layer performs another copy operation, where each end token pays attention to its corresponding $\brga$ token by using the $\brgb$ token in its buffer space as the query, and copies the corresponding source token from the buffer space of $\brga$ to its own buffer space (\Cref{fig:mech:L2_double}). Finally, in the last layer, the query entity manages to pick the target end entity by querying its buffer space and matching the target source entity in the buffer (\Cref{fig:mech:L3_double}). Note that when generalizing to $k$-hop reasoning, the target end entity can copy the target source entity to its buffer space using $O(\log k)$ layers by repeatedly applying induction heads for $O(\log k)$ times, each per layer.


\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figs/mechanism/L2_double.pdf}
        \caption{Double induction head mechanism in the second layer. The end entity pays attention to its corresponding \brga entity.}
        \label{fig:mech:L2_double}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.4\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/mechanism/L3_double.pdf}
        \caption{Double induction head mechanism in the third layer.}
        \label{fig:mech:L3_double}
    \end{subfigure}
    \caption{Illustration of the double induction head mechanism.}
    \label{fig:mech:double_induction}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figs/mechanism/L2_double.pdf}
%     \caption{Double induction head mechanism in the second layer. The end entity pays attention to its corresponding \brga entity.}
%     \label{fig:mech:L2_double}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figs/mechanism/L3_double.pdf}
%     \caption{Double induction head mechanism in the third layer.}
%     \label{fig:mech:L3_double}
% \end{figure}

\section{Experiment Results on More Models}
\label{app:exp-more-model}

We present the evaluation results for more LLMs. We use Llama3.1-8B \citep{dubey2024llama}, OLMo-7B \citep{Groeneveld2023OLMo}, and Qwen2.5-7B \citep{yang2024qwen2} as base models and evaluate them on $1000$ synthetic two-hop reasoning samples. The results are shown in Figures~\ref{fig:llama3-8b-probabilities}, \ref{fig:olmo-probabilities}, and \ref{fig:qwen-probabilities}. We present the prediction probabilities for the target and distraction when there are two reasoning chains (This is different from Figure~\ref{fig:llm-onehop}, where we compare between one and two reasoning chains). We observe that the prediciton probabilities of these models are similar to those of Llama2-7B when there is one distraction. However, Llama3.1-8B and Qwen2.5-7B show a higher probability for the target when there are more distractions, while OLMo-7B shows a lower probability for the target when there are more distractions. This suggests that the stronger models Llama3.1-8B and Qwen2.5-7B are close to the phase transition phase in the training dynamics.


\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \centering
        \caption{\small Llama3.1-8B predictions in categories}
        \includegraphics[width=\linewidth]{figs/post-icml-figures/llama3-8b_cat_prob.pdf}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \centering
        \caption{\small Llama3.1-8B predictions with increasing number of distractions}
        \includegraphics[width=\linewidth]{figs/post-icml-figures/llama3-8b_k_prob.pdf}
    \end{subfigure}
    \caption{Llama3.1-8B probability distributions. \textit{Left (a):} The probability for the target and distraction when there is one distraction. \textit{Right (b):} The probability for the target and distraction when there are increasing number of distractions.}
    \label{fig:llama3-8b-probabilities}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \centering
        \caption{\small OLMo predictions in categories}
        \includegraphics[width=\linewidth]{figs/post-icml-figures/olmo_cat_prob.pdf}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \centering
        \caption{\small OLMo predictions with increasing number of distractions}
        \includegraphics[width=\linewidth]{figs/post-icml-figures/olmo_k_prob.pdf}
    \end{subfigure}
    \caption{OLMo probability distributions. \textit{Left (a):} The probability for the target and distraction when there is one distraction. \textit{Right (b):} The probability for the target and distraction when there are increasing number of distractions.}
    \label{fig:olmo-probabilities}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \centering
        \caption{\small Qwen predictions in categories}
        \includegraphics[width=\linewidth]{figs/post-icml-figures/qwen_cat_prob.pdf}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \centering
        \caption{\small Qwen predictions with increasing number of distractions}
        \includegraphics[width=\linewidth]{figs/post-icml-figures/qwen_k_prob.pdf}
    \end{subfigure}
    \caption{Qwen probability distributions. \textit{Left (a):} The probability for the target and distraction when there is one distraction. \textit{Right (b):} The probability for the target and distraction when there are increasing number of distractions.}
    \label{fig:qwen-probabilities}
\end{figure}


