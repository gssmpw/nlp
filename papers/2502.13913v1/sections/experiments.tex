\section{Experiment Setup}
\paragraph{Abstracting reasoning as a graph} The reasoning process can often be realized through a graph. Let $\graph = (V, E)$ be a graph, each node $u\in V$ corresponds to an abstract entity or concept. In the two-hop example ``Mike is an American, and American can speak English'', ``Mike'', ``American'', and ``English'' are three entities. Each edge $e \in E$ corresponds to a possible reasoning hop. In this example, the context implies two valid reasoning hops ``Mike $\to$ American'' and ``American $\to$ English''. A two-hop reasoning can be understood as having a length-two path along the graph. In this example, ``Mike can speak English'' is a two-hop reasoning based on the context information.

\paragraph{Input format}
Motivated by the graph realization of reasoning, we consider input with the format of $<s>, \text{graph information}, x_\q, ?$. A transformer model is trained to predict the correct two-hop target given the query $x_\q$. Given a graph $\graph=(V,E)$, its information is encoded as a sequence comprises of $|E|$ pairs $u_1, u_2, \ldots $, with each pair indicating an edge in the graph. The query $x_\q$ belong to the graph and is assumed to be a starting point of a valid two-hop path. The answer $x_{\text{answer}}$ is unique given the graph $\graph$.


\paragraph{Graph generation} We generate graphs that consist of disjoint paths with randomly assigned entities on nodes. The number of two-hop paths $k$ follows a prior $\Unif[1, 4]$. Given $k$, we iteratively add $k$ disjoint two-hops in the graph. In each iteration, we sample three entities with equal probability and without replacement from the vocabulary. Then, we we iteratively add $N-2k$ disjoint one-hops in the graph witht he same manner. This ensures that the total edges in the graph equals $N$, leading to same sequence length. We sample the query token uniformly from one of the starting nodes in $k$ two-hops. Since the all two-hop paths are disjoint, the answer is guaranteed to be unique.



% \paragraph{Input format}
% 1. <s> context query answer 2. The idea behind <s> 2. how to format graph to context 3. query answer

% The task combines two-hop reasoning and in-context learning. Consider a vocabulary $\vocab$ consists of $\vocabsize$ entities $\ent_1,\ldots,\ent_\vocabsize$. The model is trained to predict the two-hop target given the query entity $\ent_\q$ and the context $G$ that gives the reasoning structure among a subset of entities $\ent_1,\ldots,\ent$. The reasoning structure $G$ can be realized by a graph. 



The two-hop reasoning can be reformulated as finding a path with length $3$ 






