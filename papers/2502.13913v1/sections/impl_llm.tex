\section{Implications for Large Language Models}
\label{sec:impl_llm}
In this section, we discuss the implications of the mechanisms discovered from three-layer transformers in real-world large language models. We use Llama2-7B-base, a widely used pre-trained language model, for all the experiments in this section.

\subsection{Experimental setting}
\label{sec:llama-initial}
We evaluate the in-context two-hop reasoning performance of LLaMA2-7B. To this end, we design fixed templates for in-context reasoning chains that cover various topics. For example, a template may take the form: ”[A] is a species in the genus [B]. The genus [B] belongs to the family [C].” Given multiple two-hop chains, we split each template into two premises and randomly order them according to the data generation procedure illustrated in \Cref{fig:data_illustration}.
We then append a query sentence, such as “Therefore, [A] is classified under the family,” after the context. Finally, we randomly sample from a set of artificial names to replace placeholders [A], [B], and so on. We perform a forward pass on the entire sequence and compute the next-token probability for the last token—in this example, “family.” Further details on the dataset are provided in Appendix\ref{app:dataset_detail}.

We set the number of two-hop chains to  K=2  and compute the next-token probability. We track the probability corresponding to the target \Ed, which is the correct answer. When \Ed~consists of multiple tokens, we consider only the first token. \Cref{fig:llm-onehop} compares the probability of LLaMA2-7B predicting the correct answer with and without distracting information. The results indicate a consistent reduction in probability by half across different template categories.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/llama_onehop.pdf}
    \caption{\textbf{Two-hop reasoning with distractions} When exposed to a single distraction, the probability of LLaMA2-7B predicting the correct target \Ed~drops by half. This decrease is consistent across different categories of the two-hop reasoning format.}
    \label{fig:llm-onehop}
\end{figure}

\subsection{Length generalization}
\label{subsec:length_gen}

According to our analysis of the training dynamics of three-layer transformers in the previous section, we observed two mechanisms: the random guessing mechanism during the early stage and the sequential query mechanism after the sharp phase transition. Below, we study which mechanism Llama2-7B uses to solve the two-hop reasoning task with distracting reasoning chains. 

\Cref{fig:llm-length-gen} shows the model's performance on in-context two-hop reasoning with different numbers of distracting chains in the prompt, both before and after fine-tuning. 

For the original pre-trained Llama2-7B model without fine-tuning, although it can make correct predictions when there is no distracting chain, its performance drops drastically as long as there exist distracting chains. Moreover, the probability of the model predicting the target end is close to the average probability of predicting any specific distracting end, which shows that the model adopts nearly uniform random guessing.  

In contrast, after fine-tuning, the model robustly makes the correct prediction. The most remarkable phenomenon is that although the model is fine-tuned on data with only two two-hop reasoning chains (one target chain and one distracting chain), the model still makes correct predictions with high probability, even if the number of chains in the prompt is as large as five. Compared to the original model, it is salient that the original model performs random guessing while the fine-tuned model learns the correct mechanism to solve the in-context two-hop reasoning problem according to its ability on length generalization.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/llama_length_gen.pdf}
    \caption{The plot shows Llama's performance on in-context two-hop reasoning with various numbers of distracting reasoning chains in the prompt both before and after fine-tuning. Each point in the plot is averaged over 1000 samples. The $x$-axis represents the number of two-hop reasoning chains in the prompt. The $y$-axis represents either the probability of the model predicting the correct answer, i.e., the target end entity, or the average probability of predicting any specific distracting end entity. The fine-tuned model is trained on data with only two reasoning chains in the prompt (one target chain and one distracting chain).}
    \label{fig:llm-length-gen}
\end{figure}