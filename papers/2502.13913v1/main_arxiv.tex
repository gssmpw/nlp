\documentclass{article}
\input{notations}
\usepackage[accepted]{icml2025}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage[most]{tcolorbox}  % Load tcolorbox package
\newtcblisting{templatebox}{colback=white,colframe=black, listing only, listing options={
    breakautoindent=false, 
    breaklines=true,
    columns=fullflexible,
    breakindent=0pt,
    breakatwhitespace=true,
    basicstyle=\small\rmfamily,
    language=
}}
\newcommand{\theHalgorithm}{\arabic{algorithm}}


% For theorems and such
\usepackage{amsmath,amssymb,amsfonts,amsthm,mathtools,mathrsfs}  % Consolidated math packages
% \usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{hypothesis}{Hypothesis}[section]

\def\shownotes{0}  %set 1 to show author notes
\ifnum\shownotes=1
\newcommand{\authnote}[2]{{\scriptsize $\ll$\textsf{#1 notes: #2}$\gg$}}
\else
\newcommand{\authnote}[2]{}
\fi
\newcommand\RZ[1]{\textcolor{red}{\authnote{RZ}{#1}}}
\newcommand\hanlin[1]{\textcolor{blue}{\authnote{Hanlin}{#1}}}
\newcommand\sm[1]{\textcolor{red}{\authnote{Song}{#1}}}
\newcommand\tianyu[1]{\textcolor{brown}{\authnote{Tianyu}{#1}}}
% \usepackage[textsize=tiny]{todonotes}




\icmltitlerunning{How Do LLMs Perform Two-Hop Reasoning in Context?}

\newcommand{\Wup}{\bW_{\sf up}}
\newcommand{\Wgate}{\bW_{\sf gate}}
\newcommand{\Wdown}{\bW_{\sf down}}
\newcommand{\bup}{\mathbf{b}_{\sf up}}
\newcommand{\bgate}{\mathbf{b}_{\sf gate}}
\newcommand{\bdown}{\mathbf{b}_{\sf down}}
\newcommand{\silu}{\sigma_{\sf silu}}
% \newcommand{\bos}{\texttt{$\langle \texttt{s}\rangle$}}


\icmltitlerunning{How Do LLMs Perform Two-Hop Reasoning in Context?}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\begin{document}

\twocolumn[
\icmltitle{How Do LLMs Perform Two-Hop Reasoning in Context?}

\begin{icmlauthorlist}
\icmlauthor{Tianyu Guo$^*$}{berkeley}
\icmlauthor{Hanlin Zhu$^*$}{berkeley}
\icmlauthor{Ruiqi Zhang}{berkeley}
\icmlauthor{Jiantao Jiao}{berkeley}
\icmlauthor{Song Mei}{berkeley}
\icmlauthor{Michael I. Jordan}{berkeley}
\icmlauthor{Stuart Russell}{berkeley}
\end{icmlauthorlist}

\icmlaffiliation{berkeley}{UC Berkeley, Berkeley, CA, USA}

\icmlcorrespondingauthor{Tianyu Guo}{tianyu_guo@berkeley.edu}
\icmlcorrespondingauthor{Hanlin Zhu}{hanlinzhu@berkeley.edu}

\icmlkeywords{Machine Learning, Large Language Models, Reasoning, Training Dynamics}

\vskip 0.3in
]
\printAffiliationsAndNotice{\icmlEqualContribution} % Use this to indicate equal contribution

\begin{abstract}
\tianyu{I have slighted revised the abstract}
``Socrates is human. All humans are mortal. Therefore, Socrates is mortal.'' This classical example demonstrates two-hop reasoning, where a conclusion logically follows from two connected premises. \RZ{'This illustrates XXXX' looks strange. I would say 'The classical example "Socrates is human. All humans are mortal. Therefore, Socrates is mortal" demonstrates a fundamental two-hop reasoning process, where a conclusion follows from two premises.'} While transformer-based Large Language Models (LLMs) can make two-hop reasoning \RZ{'at such reasoning' seems weird. I am not sure whether using 'in context' here is good since people may confuse it with 'in-context' in ICL. Maybe 'within a textual context'?}, they tend to collapse to random guessing when faced with distracting premises \RZ{I would not say 'they may'. The tone is weak.}. To understand the underlying mechanism, we train a three-layer transformer on synthetic two-hop reasoning tasks. \RZ{I would say 'We train a three-layer XXXXX on XXX and analyze the dynamics of XXXXX'}The training dynamics show two stages: a slow learning phase, where the 3-layer transformer performs random guessing like LLMs, followed by an abrupt phase transitions, where the 3-layer transformer suddenly reaches $100\%$ accuracy. \RZ{I think we should remove the 'Initially' in this sentence. It makes the sentence less fluent.} Through reverse engineering, we explain the inner mechanisms for how models learn to randomly guess between distractions initially, and how they learn to ignore distractions eventually. We further propose a three-parameter model that supports the causal claims for the mechanisms to the training dynamics of the transformer. Finally, experiments on LLMs \RZ{Llama?} suggest that the discovered mechanisms generalize across scales. Our methodologies provide new perspectives for scientific understandings of LLMs and our findings provide new insights into how reasoning emerges during training. \RZ{The last sentence is weird. The paper is not on 'optimizing complex models' since we do not propose any techniques on optimization side.}
\end{abstract}

\input{sections_arxiv/intro}
\input{sections_arxiv/prelim}
% \input{sections_arxiv/theory}
\input{sections_arxiv/restricted_attention}
\input{sections_arxiv/impl_llm}
\input{sections_arxiv/conclusion}


\bibliographystyle{plainnat}
\bibliography{main_arxiv}

\newpage
\appendix
\onecolumn
\input{sections/appendix}


\end{document}
