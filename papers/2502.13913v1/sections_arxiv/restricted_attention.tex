\section{Toy models} \hanlin{Is there a better name? e.g., ``Mechanisms for Two-hop Reasoning ...''}
\label{sec:restricted_attention}


% The input of each layer $\ell$ is a sequence of embedding vectors $\mH^{(\ell-1)} = [\vh^{(\ell-1)}_1, \vh^{(\ell-1)}_2, \ldots, \vh^{(\ell-1)}_T ] \in \real^{d\times T}$, and the output is also a sequence of embeddings $\mH^{(\ell)} = [\vh^{(\ell)}_1, \vh^{(\ell)}_2, \ldots, \vh^{(\ell)}_T ] \in \real^{d\times T}$. The final output is $\softmax(\Wreadout\vh_T^{(L)}) \in \Delta(\vocab)$ where $\Wreadout \in \real^{V \times d}$ is the readout matrix.

% For each layer $\ell$, we have
% \begin{align*}
% \vh_i^{(\ell)} &= \sum_{j \leq i} q_{ij}^{(\ell)} \mV^{(\ell)} \vh_j^{(\ell-1)}, \\
% \text{ where } q_{ij}^{(\ell)} &= \frac{\exp(S_{ij}^{(\ell)})}{\sum_{k\leq i} \exp(S_{ik}^{(\ell)}) + \xi^{(\ell)}}. 
% \end{align*}

% Note that $\xi^{(\ell)} > 0$ is the attention logit for attention sink in layer $\ell$, where a query token can dump attention logits when there are no key tokens requiring attention~\citep{guo2024active}.

% $S_{ij}^{(\ell)}$ denotes the attention logit from token $i$ to token $j$ in layer $\ell$. Note that since we use causal attention,  $S_{ij}^{(\ell)}$ is non-zero only if $i \geq j$. Below, we provide the value of all non-zero terms among $\{S^{(\ell)}_{ij}\}$.

% \begin{align*}
% S_{ij}^{(1)} = \alpha, \quad i = \idx(\child), \, j = i-1
% \end{align*}

% \begin{align*}
% S_{ij}^{(2)} &=
% \begin{cases}
%     \beta_1 \langle \vh_i^{(1)}, \vh_j^{(1)} \rangle & i = \idx(\query),  j = \idx(\target  \brg) \\
%     \beta_2 \langle \vh_i^{(1)}, \vh_j^{(1)} \rangle & i \in \idx(\target \ed),  j = \idx(\brg \, \of \, i) \\
%     \lambda & i \in \idx(\ed),  j \in \idx(\parent)
% \end{cases}
% \end{align*}

% \begin{align*}
% S_{ij}^{(3)} &=
% \begin{cases}
%     \gamma \langle \vh_i^{(2)}, \vh_j^{(2)} \rangle + \eta & i = \idx(\query), \, j = \idx(\target \, \ed) \\
%     \eta & i = \idx(\query), \, j \in \idx(\child)
% \end{cases}
% \end{align*}


\subsection{Details of the mechanism of in-context two-hop reasoning}
\label{sec:illustration_mechanism}

% \Cref{fig:mechanism_illustration} shows the internal mechanism of a three-layer transformer to perform in-context two-hop reasoning.

In this section, we discuss the internal mechanism of a three-layer transformer to perform two-hop reasoning. For better visualization, we use illustrative attention maps and defer the empirical evidence to Appendix~\ref{app:sec_DI_mechanism}.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/mlp_zero_out.pdf}
    \caption{Accuracies of the full model and the ablated models with skipped MLPs in different training stages.}
    \label{fig:mlp_zero_out}
\end{figure}

\paragraph{MLP layers are negligible.} We conducted ablation studies on the effect of MLP layers. \Cref{fig:mlp_zero_out} shows that even if we skip all MLP layers during inference, the model's performance on the validation set during different training stages remains nearly the same.  This suggests that the MLP layers did not develop meaningful functionality during training, effectively rendering them redundant. Therefore, we focus our analysis on attention heads.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/acc_dynamics.pdf}
    \caption{Training Dynamics of the three-layer transformer. Since the correct answer is the \target-\ed~token. The average prediction probability for the \target-\ed~token is the accuracy. It goes though a slow learning phase (steps 0-400), and an abrupt phase transition (around steps 800). We take the average prediction probability for $4$ non-target-\ed~tokens. Therefore, approximately similar prediciton probabilities for target and non-target tokens indicate random guessing between target and distractions.}
    \label{fig:acc_dynamics}
\end{figure}

\paragraph{The slow learning phase and abrupt learning during training.} According to \Cref{fig:acc_dynamics}, the model mainly experienced two different phases during training. In the slow learning phase (e.g., training step $0$ to around $400$), the model has a \textit{slow learning phase}. It learned to predict an end token randomly (in \Cref{fig:acc_dynamics}, we take average prediction probabilities over all the end tokens in distracting chains, so in the slow learning phase, the prediction probability for the target end and average non-target end are roughly equal). After an abrupt phase transition after more than $800$ training steps, the model learns to predict the correct answer perfectly. We call the interim mechanism that the model learned during the slow learning phase \emph{random guessing mechanism}, and observed that the final mechanism the model learned to make the perfect prediction is a \emph{sequential query mechanism}. Besides, previous literature~\citep{sanford2024transformers} posited that the transformer performs in-context two-hop reasoning by a mechanism they theoretically constructed, which we called \emph{double induction head mechanism}. Below, we explain the random guessing mechanism and the sequential query mechanism layer by layer in detail, along with experiments supporting the existence of such mechanisms. The details of the double induction head mechanism are deferred to \Cref{app:sec_DI_mechanism}.

\paragraph{The first layer.} For both the \emph{random guessing mechanism} and the \emph{sequential query mechanism}, the first layer is a copy layer. In the attention map, each child token pays all attention to its parent token by positional encoding and then copies its parent token to its buffer space to be used in subsequent layers. This is pictorially illustrated in \Cref{fig:mech:L1_copy}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/mechanism/L1_copy.pdf}
    \caption{An illustration of the mechanism of the first layer. Each child token pays attention to its parent token and copies it to its buffer space.}
    \label{fig:mech:L1_copy}
\end{figure}

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1} % Adjust row height for better spacing
    \begin{tabular}{lccc}
        \toprule
        \textbf{Step} & \textit{Pre}-$\Brg$ & \textit{Self}-$\Ed$ & \textit{Post}-$\Brg$ \\
        \midrule
        $\mathbf{0}$ & 0.00 & 0.01 & 0.02 \\
        $\mathbf{400}$ & -1.55 & 7.14 & -0.02 \\
        $\mathbf{2000}$ & -4.80 & 11.50 & -0.05 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{The logit lens shows effect of \Ed~tokens' value states in layer 3 on the final output.} Through the logit lens, the value states become a logit score for the next token prediction. We track three groups of logits in the logit lens: The ``\textit{Pre}-\brg'' denotes the logits correspond to \brg~tokens precede the \Ed~token. The increasing logit score indicates the increasing suppression effect of the value states for \Ed~tokens to the prediction of \brg~tokens; The``\textit{Self}-\Ed'' denotes the logit corresponds to the \Ed~token itself, The increasing score indicates that \Ed's value states have increasing propensity to predict itself; The ``\textit{Post}-\brg'' denotes the \brg~tokens succeed the \Ed~token. The zero scores indicate that the suppression effect is formed in-context, not from memorization.}
    \label{tab:logit_lens_value}
\end{table}


\paragraph{The interim mechanism in slow learning phase: random guessing.} During the slow learning phase (e.g., around 400 steps), the model learns to randomly pick an end token by distinguishing between the end and bridge tokens among all child tokens. The underlying mechanism we observed is as follows. In the second layer, as shown in \Cref{fig:mech:L2_guess}, each child token will attend equally to all previous parent tokens and copy (the superposition of) them to its buffer.  The buffer space will later be used in the last layer, where the query entity (i.e., the last token in the input sequence) will attend equally to all child tokens as shown in \Cref{fig:mech:L3_guess}.  Thanks to the information on the buffer space collected in the second layer, the value state of each copied child token through the logit lens contains not only itself but also all parent tokens before that child token with a negative sign in the corresponding coordinate (\Cref{fig:mech:logit_lens_guess}). By aggregating the value states of all child tokens in the last layer, the query entity can distinguish the end token from the bridge token since the positive value of the bridge token due to $\brga$, a child token, is canceled out by the negative value caused by $\brgb$, a parent token, in the same corresponding coordinate. As a result, this interim mechanism learned in the slow learning phase can randomly guess an end entity as its prediction.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/mechanism/L2_guess.pdf}
    \caption{The random guessing mechanism in the second layer. Each child token pays attention to all the previous parent tokens.}
    \label{fig:mech:L2_guess}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/mechanism/L3_guess.pdf}
    \caption{The random guessing mechanism in the third layer. The query entity pays attention to all the child tokens.}
    \label{fig:mech:L3_guess}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/mechanism/logit_lens_guess.pdf}
    \caption{Value states of the last layer through logit lens for the random guessing mechanism. The value of the coordinate of bridge tokens will cancel out after aggregating, which enables the query token to distinguish between the bridge entity and the end entity.}
    \label{fig:mech:logit_lens_guess}
\end{figure}

\paragraph{The (observed) sequential query mechanism after phase transition.} After the phase transition during training, the model achieves nearly perfect accuracy, and we observed a sequential query mechanism during that stage. After copying parent entities in the first layer (\Cref{fig:mech:L1_copy}), in the second layer, the query entity will pay attention to the bridge entity whose corresponding source entity in the buffer space matches the query entity. Then, it copies this bridge entity to the query entity's buffer space (\Cref{fig:mech:L2_sequential}). In the last layer, the query entity uses the collected bridge entity from the last layer to do the query again and obtains the corresponding end entity, which is exactly the expected answer (\Cref{fig:mech:L3_sequential}). Note that in a $k$-hop reasoning setting, the above sequential query mechanism performs one hop per layer and thus requires $(k+1)$ layers.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/mechanism/L2_sequential.pdf}
    \caption{Sequential query mechanism in the second layer. The query entity pays attention to the target \brga  entity.}
    \label{fig:mech:L2_sequential}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/mechanism/L3_sequential.pdf}
    \caption{Sequential query mechanism in the third layer.}
    \label{fig:mech:L3_sequential}
\end{figure}

\paragraph{The model is prone to learn the sequential query mechanism.} Although the double induction head mechanism is theoretically optimal in terms of the number of layers required to perform multi-hop reasoning tasks (see \Cref{app:sec_DI_mechanism} for details), even for the simplest two-hop reasoning, the model tends to learn the sequential query mechanism according to our observations. 
% To further understand why the model prefers the seemingly less efficient sequential query mechanism, as well as why the model abruptly learned how to solve the task during the rapid phase transition, we study a three-parameter model that fully captures the training dynamics of the three-layer transformers on our two-hop reasoning settings in \Cref{sec:three_param_model}.

\subsection{Building causal relationships between mechanisms and training dynamics using a three-parameter model}
\label{sec:three_param_model}

\paragraph{``Causal'' hypotheses based on observations.} In the last section, we observed two stages along the training dynamics of the three-layer transformer. In this section, we aim to build a ``causal'' relationship between the observed mechanisms and the training dynamics. The two ``causal'' hypotheses are:
\begin{hypothesis}\label{hyp:random-guess}
The formation of the \textit{random guessing mechanism} causes the slow learning phase (0-400 steps).
\end{hypothesis}
\begin{hypothesis}\label{hyp:sequential-query}
The formation of the \textit{sequential query mechanism} causes the abrupt phase transition (800 steps).
\end{hypothesis}

We need to implement ``causal interventions'' to validate the hypotheses. Following the approach of \citet{reddy2023mechanistic}, we propose studying a \textit{three-parameter dynamical system}, which simulates only the dynamics of the sequential query mechanism, removing the random guessing mechanism.

\paragraph{Comparing the training dynamics of the three-parameter model with the training dynamics of transformers to validate the causal hypotheses}
Since the random guessing mechanism is removed and the sequential query mechanism is kept, we anticipate that
\begin{enumerate}
    \item \Cref{hyp:random-guess} holds if the three-parameter model loses the slow learning phase in the training dynamics.
    \item \Cref{hyp:sequential-query} holds if the three-parameter model preserves the abrupt phase transition in the training dynamics.
\end{enumerate}

\paragraph{Approximate SoftMax operator and notations for content and buffer spaces.} For convenience, we define the approximate SoftMax operator as:
\[
\pseudosoftmax(u, M) = \frac{\exp(u)}{\exp(u)+M}.
\]
Intuitively, the approximate SoftMax gives the probability of an item with logit $u$ where the remaining $M$ logits are all zero. 
Given a residual state $u$, we use $\content(u)$, $\buffer_1(u)$, $\buffer_2(u)$ to denote the original content (i.e., the token embedding and the positional embedding) of token $u$, the buffer space of $u$ in the first layer, and the buffer space of $u$ in the second layer, respectively. Since the tokens are not semantically related, we assume that $\langle\content(a),\content(b)\rangle = \mathbbm{1}\{a=b\}$ for any two tokens $a$ and $b$, which means that $\{\content(\cdot)\}$ is an orthonormal basis. 

\paragraph{The meaning of three parameters.} The sequential query mechanism consists of a copy layer in the first layer (\cref{fig:mech:L1_copy}), \query~to \brg~in the second layer (\cref{fig:mech:L2_sequential}), and \query~to \ed~in the third layer (\cref{fig:mech:L3_sequential}).
We use parameters $\alpha$, $\beta$, and $\gamma$ to represent the progressive measure of their functionalities.
\begin{align}
&\buffer_1(\brga)=w_1\content(\src), \label{eqn:L1_copy_brg}\\
&\buffer_1(\Ed)=w_1\content(\brg), \label{eqn:L1_copy_end}\\
&\buffer_2(\query)=w_2\content(\brg),  \label{eqn:L2_seq_qry}\\
&\texttt{Output}=w_3 \content(\Ed), \label{eqn:L3_output}\\
&\text{Loss} = -\log[\pseudosoftmax(\xi w_3, V)].\label{eqn:loss}
\end{align}
where
\begin{align*}
w_1&=\pseudosoftmax(\alpha,N), \\
w_2&=\pseudosoftmax(\beta\langle\content(\query),\buffer_1(\brga)\rangle,2N),\\
w_3&=\pseudosoftmax(\gamma\langle\buffer_2(\query),\buffer_1(\Ed)\rangle,2N).
\end{align*}
Note that when we set $\alpha\to\infty$, $\beta\to\infty$, and $\gamma\to\infty$, $\text{Loss}\to 0$, corresponding to the three-layer transformer trained after $10000$ steps. When we set $\alpha=\beta=\gamma=0$, the loss is close to a uniform guess in the vocabulary, corresponding to an untrained three-layer transformer. 

\paragraph{The derivation of Equations~\eqref{eqn:L1_copy_brg} and \eqref{eqn:L1_copy_end}.} We present that how we simplify a full transoformer block to get Equations~\eqref{eqn:L1_copy_brg} and \eqref{eqn:L1_copy_end}. As previously discussed, we can ignore the MLP block and focus on the attention block.
As illustrated in \Cref{fig:mech:L1_copy}, the first attention block relies on the positional information to copy parent tokens to the buffer spaces of child tokens. The attention logits are given by
\begin{align*}
~&\attlogit(\child\to\parent)\\
=~&\text{Pos}_i^\top Q^{(1)\top}K^{(1)} \text{Pos}_{i-1}, 
\end{align*}
where $Q^{(1)}$, $K^{(1)}$ are weight matrices in the first layer. We assume that $\text{Pos}_i^\top Q^{(1)\top}K^{(1)} \text{Pos}_{i-1}=\alpha$ for any $i$. Since we reshuffle the positions for $\brga$ and $\ed$ for each sequence, following \citet{reddy2023mechanistic}, we approximate the attention weights to parent tokens by $\pseudosoftmax(\alpha, N)$, where $N$ comes from taking the average from  $2N$ positions. This gives Equations~\eqref{eqn:L1_copy_brg} and \eqref{eqn:L1_copy_end}.

% We assume that the \attlogit~is only affected by the scale of the $Q^{(1)}K^{(1)}$. Therefore, for any $\child$ and $\parent$, we assume
% \begin{equation}\label{eqn:alpha}
% \attlogit(\child\to\parent)=\alpha. 
% \end{equation}

\paragraph{The derivation of Equation~\eqref{eqn:L2_seq_qry}.} 
Similarly, as illustrated in \Cref{fig:mech:L2_sequential}, the $\buffer_2(\query)$ is proportional to the attention from the \query~token to $\brga$ in the second layer. The \query~token uses its $\content(\query)$ to fit the $\buffer_1(\brga)$, copying $\content(\brg)$ to the residual stream. Therefore, 
\begin{align*} 
~&\attlogit(\query\to\brga)\\
=~&\content(\query)^\top Q^{(2)\top} K^{(2)}\buffer_1(\brga)\\
=~&\beta \cdot \langle \buffer_1(\brga), \content(\query) \rangle,
\end{align*}
where the last line could be viewed as a re-parametrization of $Q^{(2)\top} K^{(2)}$, with $\beta \propto \|Q^{(2)\top} K^{(2)}\|_2$. Moreover, we fix the attention logits from $\query$ token to all other tokens to be zero, removing  mechanisms other than the sequential query. The attention weight from the \query~token to the \brga~becomes $\pseudosoftmax(\attlogit(\query\to\brga), 2N)$. This gives Equation~\eqref{eqn:L2_seq_qry}.

\paragraph{The derivation of Equation~\eqref{eqn:L3_output} and \eqref{eqn:loss}.}
As illustrated in \Cref{fig:mech:L3_sequential}, the $\query$ token increasingly concentrates on the $\target$-\Ed~token along the training dynamics. With the same manner of Equation~\eqref{eqn:L2_seq_qry}, we set that
\begin{align*}
~& \attlogit(\query\to\target-\Ed)  \\
=~&\buffer_2(\query) Q^{(3)\top} K^{(3)}\buffer_1(\target-\Ed)\\
= ~& \gamma \cdot \langle \buffer_2(\query), \buffer_1(\target-\Ed)\rangle.
\end{align*}
We focus on  $\attlogit(\query\to\target-\Ed)$ and set all other \attlogit~to be zero.
The attention weight from the query to the \target-\Ed~becomes $\pseudosoftmax(\attlogit(\query\to\target-\Ed), 2N)$. We first consider the output \logit~on the query token. Through the logit lens, as illustrated in \Cref{tab:logit_lens_value}, the value states of \Ed~tokens have large logits on itself. Therefore, we assume that $\readout[\Val(\target-\Ed)] = \xi \cdot \bm{e}_{\Ed} \in \R^V$, with $\xi>0$ and $\bm{e}_{\Ed}$ being a one-hot vector in $\R^V$ that is non-zero on the index of $\Ed$. In our simulation, we fix $\xi=30$.
The loss can therefore be approximated through Equation~\eqref{eqn:loss}.

% Following the mechanism illustrated in \Cref{fig:mech:L3_sequential}, $\attlogit(\query\to\target-\Ed)$ is given by the inner product between $\buffer_2(\query)$ and $\buffer_1(\target-\Ed)$, scaled by $\gamma$. We therefore set that
% \begin{align}
% ~& \attlogit(\query\to\target-\Ed) \nonumber \nonumber \\
% = ~& \gamma \cdot \langle \buffer_2(\query), \buffer_1(\target-\Ed)\rangle.\label{eqn:gamma}
% \end{align}


% The $\buffer_1(\cdot)$ is merely given by the copying head in the first layer, which purely relies on the positional information. We assume that the \attlogit~is only affected by the scale of the $Q^{(1)}K^{(1)}$. Therefore, for any $\child$ and $\parent$, we assume
% \begin{equation}\label{eqn:alpha}
% \attlogit(\child\to\parent)=\alpha. 
% \end{equation}
% Since the child tokens randomly have $1$ to $2N-1$ tokens ($N$ being the number of premises) in front of them, but each of them has the same attention logit $\alpha$ with their parent, we can use $\pseudosoftmax(\alpha, N)$ to approximate the average attention weights for copying. We therefore set $\buffer_1(\brga)=\pseudosoftmax(\alpha, N)\content(\Src)$ and $\buffer_1(\Ed)=\pseudosoftmax(\alpha, N)\content(\brg)$. By combining Equations~\eqref{eqn:gamma}, \eqref{eqn:beta}, and \eqref{eqn:alpha}, and \eqref{eqn:loss}, we get a model with three parameters $\alpha$, $\beta$, and $\gamma$ that simulates the sequential query mechanism. We optimize the loss function in Eq.~\eqref{eqn:loss} by updating $\alpha$, $\beta$, and $\gamma$ through gradient descent. 

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.23\textwidth}
        \caption{\small Loss of 3-layer transformer}
        \includegraphics[width=\textwidth]{figs/post-icml-figures/log_acc_dynamics.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.23\textwidth}
        \centering 
        \caption{\small Loss of 3-parameter model}
\includegraphics[width=\textwidth]{figs/param_dynamics_loss.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.23\textwidth}
        \caption{\small 3-layer transformer}
        \includegraphics[width=\textwidth]{figs/post-icml-figures/attention_patterns.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.23\textwidth}
    \centering
        \subcaption{\small 3-parameter model}
\includegraphics[width=\textwidth]{figs/param_dynamics_params.pdf}
    \end{subfigure}
    \caption{\textbf{The comparison between 3-layer transformer and 3-parameter model validates our causal hypotheses.} \textit{Top left (a)}: The loss dynamics of the 3-layer transformer shows a slow learning phase (0-400 steps) followed by an abrupt phase transition (around 800 steps). \textit{Top right (b)}: The 3-parameter model, which only simulates the sequential query mechanism, skips the slow learning phase but preserves the abrupt phase transition, validating both hypotheses. \textit{Bottom left (c)}: The dynamics of important attention weights for the sequential query mechanism. \textit{Bottom right (d)}: The parameter dynamics of the 3-parameter model shows synchronized phase transitions in all three parameters ($\alpha$, $\beta$, $\gamma$), indicating the formation of the sequential query mechanism.}

    \label{fig:3_param_dnamics}
\end{figure}


\paragraph{Simulations on three-parameter model validate Hypotheses~\ref{hyp:random-guess} and~\ref{hyp:sequential-query}.} We optimize the loss function~\cref{eqn:loss} by running gradient descent with learning rate $0.1$.  
\Cref{fig:3_param_dnamics} presents the training dynamics of the 3-layer transoformer and the 3-parameter model. Since the model does not incorporate the random guessing mechanism, the loss remains unchanged during the first $1000$ steps, \hanlin{For the 3-param model, it's not 1000 epochs.} validating \Cref{hyp:random-guess}. 
Both the parameters and the loss function go through a sudden phase transition around step $1000$, suggesting that the emergence of the sequential query mechanism is the driving force behind the abrupt drop in loss. This validates \Cref{hyp:sequential-query}. 

% \subsection{Experimental results}

% \hanlin{The following part might be deleted, and we will simply use the above formula.}

% The input of each layer $l$ is a sequence of embedding vectors $\mH^{(l-1)} = [\vh^{(l-1)}_1, \vh^{(l-1)}_2, \ldots, \vh^{(l-1)}_T ] \in \real^{d\times T}$, and the output is also a sequence of embeddings $\mH^{(l)} = [\vh^{(l)}_1, \vh^{(l)}_2, \ldots, \vh^{(l)}_T ] \in \real^{d\times T}$. The final output is $\softmax(\Wreadout\vh_T^{(L)}) \in \Delta(\vocab)$ where $\Wreadout \in \real^{V \times d}$ is the readout matrix. \todo{might want to use a restricted readout function to reduce the number of parameters}

% \paragraph{The first layer.} In the first layer, each child node pays attention to its parent node and copies its value state. As a result, we assume that
% \begin{equation}
% \label{eq:restricted_attn_layer_1}
% \begin{aligned}
%     \vh^{(1)}_i = \vh^{(0)}_i + \indicator\{ i > 1, i \text{ is odd} \} \cdot \Val^{(1)}(\vh^{(0)}_{i-1}). 
% \end{aligned}
% \end{equation}

% \paragraph{The second layer.} The second layer is the most interesting part and is the focus of our analysis. There is an interim mechanism appeared in the early training stage ($\beta_0$), and two candidate mechanisms after phase transition: one is the sequential query mechanism observed in our experiments ($\beta_1$), and the other was posited by previous literature ($\beta_2$). Mathematically, we have
% \begin{equation}
% \label{eq:restricted_attn_layer_2}
% \begin{aligned}
% &\Key^{(2)}(\vh^{(1)}_i) \cdot \Qry^{(2)}(\vh^{(1)}_j)  \\ 
%      = & \beta_0 \cdot \indicator\{i \text{ is even, } j \text{ is odd, } i < j \} \\
%     & + \beta_1 \cdot \indicator\{s_{i-1}=s_j, i+1 <j=T \text{ and } i \text{ is odd } \}
%     \\  & + \beta_2 \cdot \indicator\{s_{i-1}=s_j, i<j \text{ and } i, j \text{ are odd }\}
% \end{aligned}
% \end{equation}

% \paragraph{The last layer.} In the last layer, we only need to keep track of $\vh^{(3)}_T$. In either the sequential query mechanism or binary lifting mechanism, 
% \begin{equation}
% \label{eq:restricted_attn_layer_3}
% \begin{aligned}
% &\Key^{(3)}(\Val^{(2)}(\Val^{(1)}(\vh^{(0)}_i)) \cdot \Qry^{(3)}(\vh^{(0)}_T)   =  \indicator\{s_i = s_T\} \\ 
%      & \beta_0 \cdot \indicator\{i \text{ is even, } j \text{ is odd, } i < j \} \\
%     & + \beta_1 \cdot \indicator\{s_{i-1}=s_j, i+1 <j=T \text{ and } i \text{ is odd } \}
%     \\  & + \beta_2 \cdot \indicator\{s_{i-1}=s_j, i<j \text{ and } i, j \text{ are odd }\}
% \end{aligned}
% \end{equation}
