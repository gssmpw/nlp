\section{Reasoning}

Possible ideas for understanding transformer reasoning: The task in Physics of Language Models 2.1 \citep{ye2024physics} can actually be decomposed into two parts: learning the underlying graph structure (DAG) in context and arithmetic calculation (mainly addition and multiplication module 23 (or any other number). Both of these two tasks are very basic and important for reasoning. The former can be viewed as graph theory for transformer, and the latter can be viewed as elementary arithmetic for transformer.

\begin{enumerate}
    \item Graph theory: The starting point can be the task of topologic sort. The input can be formatted sentences such as ``$A \to B$'', which means A has a directed edge to B. The output can be topologic sort or anything related:
    \begin{enumerate}
        \item The question can be a node $B$, and the output can be the shortest path to solve $B$.
        \item The question can be a node $B$, and the output can be the depth of $B$.
        \item The question can be a node $B$ and a node $A$, and the output can be whether $A$ is necessary to solve $B$.
    \end{enumerate}
    Note that a deep graph needs a deep transformer, so we don't necessarily need to stick to training dynamics: for a deep graph, we can also focus on expressivity (both upper bound and lower bound). However, we can work on training dynamics for a shallow graph, which is still interesting and challenging. 

    
    \item Elementary arithmetic: We can start with addition and then multiplication. Is embedding learning important for this transformer task? How many samples do the transformer need to learn the task? Some related work \citep{zhong2024clock,tian2024composing} might be useful.
\end{enumerate}


\emph{Possible experiments:} Learn the DAG in-context. 
\begin{itemize}
    \item Data format: <s>, a to b, b to c, $\ldots$; <Q>: c; <COT>: a b c; <ANS>: 
    \item Data configurations: follow the physics of LLMs, 
\end{itemize}



\emph{Given transformers pre-trained on the graph-sorting task:} 
\begin{itemize}
    \item Check attention maps;
    \item Probe whether the transformer can judge whether A precedes B;
    \item Understanding the mechanism
    \item The relationship between the graph structure (number of nodes, the longest path) and the minimal number of layers;
\end{itemize}

\emph{Possible theory:}
\begin{itemize}
    \item Construction;
    \item Prove the dynamics if there exist circuits similar to the induction head.
\end{itemize}

\emph{Experiment setup update:}
\begin{itemize}
    \item Set the upper bound for the depth of the DAG; (Set the depth$=2$ first and see how many layers can solve the problem.)
    \item Add 0-5 bos token at the beginning to avoid shortcuts;
    \item Fix the output of the topological sorting in a dictionary order.
    \item remove the loss computed on the padding tokens;
\end{itemize}

\subsection{Theoretical Results (plan)}

\begin{enumerate}
    \item For a depth-two graph, what is the training dynamics of a two-layer transformer?
    \item The difference between depth-two and depth-three graphs:
    \begin{enumerate}
        \item Maybe a lower bound that a constant depth (like two or three) transformer cannot solve a deep (or depth-three) graph.
        \item Training dynamics of (three or four?) layers of transformers for solving depth-three graphs
    \end{enumerate}
    \item For a deep graph (the depth is larger than three), how to explicitly construct parameters that can implement topological sorts. 
    \item Probing: How to learn $\dep(\tokenA,\tokenB)$ from pertaining on topological sort outputs?
\end{enumerate}


