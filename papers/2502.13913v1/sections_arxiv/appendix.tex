\section{Additional Training Details}
\label{app:sec_add_training_detail}

The transformers are trained with positional embeddings, pre-layer normalization, \(\softmax\) activation in $\attn$, and ReLU activation in $\mlp$. Optimization is performed using Adam with a fixed learning rate of 0.0003, $\beta_1=0.9$, $\beta_2=0.99$, $\epsilon=10^{-8}$, and a weight decay of 0.01. For SGD in the simulation of the 3-parameter model, we use a learning rate of 0.1. At each training step, data is resampled from the BB task with a batch size of  $B=512$  and sequence length  $N=23$. Unless stated otherwise, the model is trained for $10,000$ steps, with results remaining consistent across different random seeds.

% \begin{figure*}[h]  
%     \centering
%     \includegraphics[width=\textwidth]{figs/illustration_tmp.pdf}  
%     \caption{Illustration of the internal mechanism of a three-layer transformer to perform in-context two-hop reasoning. Due to space limitations, we provide the details of the above mechanism in \Cref{sec:illustration_mechanism}.} 
%     \label{fig:mechanism_illustration}
% \end{figure*}

\section{Dataset Details}
\label{app:dataset_detail}
We design six two-hop reasoning templates with the aid of deepseek-R1. We choose common male and female names. To avoid interference by prior knowledge, we prompt deepseek-R1 to generate fake locations and biological categories. Here are they:

\begin{tcbraster}[raster columns=2,raster equal height]
\begin{templatebox}
Relations: "[A] is the mother of [B]. [B] is the mother of [C]. Therefore, [A] is the grandmother of",
"[A] is the father of [B]. [B] is the father of [C]. Therefore, [A] is the grandfather of",

Geography: "[A] is a city in the state of [B]. The state of [B] is part of the country [C]. Therefore, [A] is located in",
"[A] lives in [B]. People in [B] speak [C]. Therefore, [A] speaks",

Biology: "[A] is a species in the genus [B]. The genus [B] belongs to the family [C]. Therefore, [A] is classified under the family",

Arithmetic: "[A] follows the time zone of [B]. [B] is three hours ahead of [C]. Therefore, [A] is three hours ahead of",
\end{templatebox}
\begin{templatebox}
locations: "Zorvath", "Tyseria", "Kryo", "Vynora", "Quellion", "Dras", "Luminax", "Vesperon", "Noctari", "Xyphodon", "Glacidae", "Ophirion", "Eryndor", "Solmyra", "Umbrithis", "Balthorien", "Ytheris", "Fendrel", "Havroth", "Marendor"

biology: "Fluxilus", "Varnex", "Dranthidae", "Zynthor", "Gryvus", "Myralin", "Thalorium", "Zephyra", "Aerinth", "Xyphodon", "Kryostis", "Glacidae", "Borithis", "Chrysalix", "Noctilura", "Phorvian", "Seraphid", "Uthrelin", "Eldrinth", "Yvorith"

languages: "English", "Spanish", "Mandarin", "Hindi", "Arabic", "French", "German", "Japanese", "Portuguese", "Russian", "Korean", "Italian", "Turkish", "Dutch", "Swedish", "Polish", "Hebrew", "Greek", "Bengali", "Thai"

names: "Ben", "Jack", "Luke", "Mark", "Paul", "John", "Tom", "Sam", "Joe", "Max", "Amy", "Emma", "Anna", "Grace",  "Kate", "Lucy", "Sarah", "Alice", "Alex", "Ruby"
\end{templatebox}
\end{tcbraster}






\section{Additional Details on Mechanisms}
\label{app:sec_DI_mechanism}

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/step400attn_layer0_head0.pdf}
        \caption{Attention maps of layer 0, step 400}
        \label{fig:step400attn_layer0_head0}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/step400attn_layer1_head0.pdf}
        \caption{Attention maps of layer 1, step 400}
        \label{fig:step400attn_layer1_head0}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/step400attn_layer2_head0.pdf}
        \caption{Attention maps of layer 2, step 400}
        \label{fig:step400attn_layer2_head0}
    \end{subfigure}
    \caption{Attention maps of Step 400.}
    \label{fig:attn_step_400}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/step10000attn_layer0_head0.pdf}
        \caption{Attention maps of layer 0, step 10000}
        \label{fig:step10000attn_layer0_head0}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/step10000attn_layer1_head0.pdf}
        \caption{Attention maps of layer 1, step 10000}
        \label{fig:step400attn_layer1_head0}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/step10000attn_layer2_head0.pdf}
        \caption{Attention maps of layer 2, step 10000}
        \label{fig:step10000attn_layer2_head0}
    \end{subfigure}
    \caption{Attention maps of Step 10000.}
    \label{fig:attn_step_10000}
\end{figure}

\paragraph{The (hypothetical) double induction head mechanism.} Contrary to our observation, previous literature~\citep{sanford2024transformers} studies in-context multi-hop reasoning theoretically and constructed a hypothetical mechanism, which we call the double induction head mechanism when specialized to our two-hop reasoning settings. After copying the parent node in the first layer (\Cref{fig:mech:L1_copy}), the second layer performs another copy operation, where each end token pays attention to its corresponding $\brga$ token by using the $\brgb$ token in its buffer space as the query, and copies the corresponding source token from the buffer space of $\brga$ to its own buffer space (\Cref{fig:mech:L2_double}). Finally, in the last layer, the query entity manages to pick the target end entity by querying its buffer space and matching the target source entity in the buffer (\Cref{fig:mech:L3_double}). Note that when generalizing to $k$-hop reasoning, the target end entity can copy the target source entity to its buffer space using $O(\log k)$ layers by repeatedly applying induction heads for $O(\log k)$ times, each per layer.


\begin{figure}[h]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figs/mechanism/L2_double.pdf}
        \caption{Double induction head mechanism in the second layer. The end entity pays attention to its corresponding \brga entity.}
        \label{fig:mech:L2_double}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.4\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/mechanism/L3_double.pdf}
        \caption{Double induction head mechanism in the third layer.}
        \label{fig:mech:L3_double}
    \end{subfigure}
    \caption{Illustration of the double induction head mechanism.}
    \label{fig:mech:double_induction}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figs/mechanism/L2_double.pdf}
%     \caption{Double induction head mechanism in the second layer. The end entity pays attention to its corresponding \brga entity.}
%     \label{fig:mech:L2_double}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figs/mechanism/L3_double.pdf}
%     \caption{Double induction head mechanism in the third layer.}
%     \label{fig:mech:L3_double}
% \end{figure}