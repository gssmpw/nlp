\section{Implications for Large Language Models}
\label{sec:impl_llm}
In this section, we discuss the implications of the mechanisms discovered from three-layer transformers in large language models. We use Llama2-7B-base, a widely used pre-trained language model, for all the experiments in this section. We relegate the details of experiments on Llama3.1-8B, Qwen2.5-7B to Appendix \ref{app:exp-more-model}.

\subsection{Evaluation on two-hop reasoning with distraction}
\label{sec:llama-initial}
\paragraph{Evaluation dataset.}
We evaluate the in-context two-hop reasoning performance of LLaMA2-7B when there are distractions. To this end, we design fixed templates for in-context reasoning chains that cover various topics. For example, a template related to biology: ”[A] is a species in the genus [B]. The genus [B] belongs to the family [C].” Given multiple two-hop chains, we randomly order premises of two-hop chains according to the data generation procedure illustrated in \Cref{fig:data_illustration}.
We then append a query sentence, such as “Therefore, [A] is classified under the family,” after the context. Finally, we randomly sample from a set of artificial names to replace placeholders [A], [B], and so on. We perform a forward pass on the entire sequence and compute the next-token probability for the last token—in this example, “family.” Further details on the dataset are provided in Appendix\ref{app:dataset_detail}.

\paragraph{Distraction significantly reduces the two-hop reasoning accuracy of LLMs.}
We set the number of two-hop chains to  $K=2$  and compute the next-token probability. We track the probability corresponding to the target \Ed, which is the correct answer. When \Ed~consists of multiple tokens, we consider only the first token. \Cref{fig:llm-onehop} compares the probability of LLaMA2-7B predicting the correct answer with and without distracting information. The results indicate a consistent reduction in probability by half across different template topics of the two-hop reasoning.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/llama_onehop.pdf}
    \caption{\textbf{Two-hop reasoning with distractions} When exposed to a single distraction, the probability of LLaMA2-7B predicting the correct target \Ed~drops by half. This decrease is consistent across different categories of the two-hop reasoning format.}
    \label{fig:llm-onehop}
\end{figure}

\subsection{Length generalization}
\label{subsec:length_gen}
The evaluation results indicate that Llama2-7B employs a random guessing mechanism for two-hop reasoning, which is affected by distractions significantly.
According to the training dynamics of three-layer transformers, Llama2-7B stays at a stage right before the abrupt phase transition. Therefore, we predict that LLMs will switch to sequential query mechanism through an abrupt phase transition when finetuned on two-hop reasoning task. Moreover, since the sequential query mechanism can generalize to arbitrary number of distractions. As a result, we have the following verifiable prediction for LLMs:
\begin{center}
\textit{Even finetuned on two-hop reasoning with one distraction, LLMs can generalize to two-hop reasoning with multiple distractions.}
\end{center}

\paragraph{Experimental details.} We finetune Llama2-7B on $1000$ prompts, each consist of one target two-hop reasoning chain and one distraction chain. The details are relegate to Appendix~\ref{app:dataset_detail}.
\Cref{fig:llm-length-gen} shows the model's performance on in-context two-hop reasoning with different numbers of distracting chains in the prompt, both before and after fine-tuning. \tianyu{I will add more details about the finetuning.}

\paragraph{Finetuning on prompts with one distraction generalizes to multiple distractions.}
For the original pre-trained Llama2-7B model without fine-tuning, although it can make correct predictions when there is no distracting chain, its performance drops drastically as long as there exist distracting chains. Moreover, the probability of the model predicting the target end is close to the average probability of predicting any specific distracting end, which shows that the model adopts nearly uniform random guessing.
In contrast, after fine-tuning, the model robustly makes the correct prediction. The most remarkable phenomenon is that although the model is fine-tuned on data with only two two-hop reasoning chains (one target chain and one distracting chain), the model still makes correct predictions with high probability, even if the number of chains in the prompt is as large as five. Compared to the original model, it is salient that the original model performs random guessing while the fine-tuned model learns the correct mechanism to solve the in-context two-hop reasoning problem according to its ability on length generalization. This validates the conclusion in \Cref{sec:restricted_attention}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/llama_length_gen.pdf}
    \caption{The plot shows Llama's performance on in-context two-hop reasoning with various numbers of distracting reasoning chains in the prompt both before and after fine-tuning. Each point in the plot is averaged over 1000 samples. The $x$-axis represents the number of two-hop reasoning chains in the prompt. The $y$-axis represents either the probability of the model predicting the correct answer, i.e., the target end entity, or the average probability of predicting any specific distracting end entity. The fine-tuned model is trained on data with only two reasoning chains in the prompt (one target chain and one distracting chain).}
    \label{fig:llm-length-gen}
\end{figure}