\section{Introduction}
\label{sec:intro}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/distraction_full.pdf}
    \caption{LLMs collapse to random guessing between two-hop chains when exposed to distracting input information.}
    \label{fig:llm-distraction}
\end{figure}


Transformer-based large language models (LLMs) show great capability in solving complex reasoning tasks~\citep{reynolds2021prompt,kojima2022large,brown2020language,wei2022chain,wang2022self,nye2021show,cobbe2021training,zelikman2022star}. Arguably, the in-context two-hop reasoning, which requires a model to derive a conclusion (e.g., \emph{[A] is the grandfather of [C]}) from two assumed premises (e.g., \emph{[A] is the father of [B]} and \emph{[B] is the father of [C]}) given in the prompt, is one of the most fundamental components of many reasoning problems. However, this seemingly the simplest reasoning task can fail real-world large language models, e.g., as shown in \Cref{fig:llm-distraction}, although LLMs can solve an in-context two-hop reasoning task correctly without distraction in the prompt, it simply collapses to random guessing when there is additional distracting information (i.e., \emph{[D] is the father of [E]} and \emph{[E] is the father of [F]}) in the context (see more details in \Cref{sec:llama-initial}). Similar phenomena have been discovered in the GSM-IC benchmark \citep{shi_chen_misra_scales_dohan_chi_sch√§rli_zhou}. This failure mode suggests that LLMs might adopt some undesired internal mechanisms to perform reasoning tasks. 


 To investigate why LLMs would struggle with the two-hop reasoning in the presence of distracting information, a common approach is circuit discovery (e.g., \citet{conmy2023towards}), which aims to identify the underlying mechanisms at the level of attention heads. However, the validity of circuits remains a topic of debate (e.g., \citet{shi2024hypothesis,hase2024does}). Irrespective of this concern, circuits related to two-hop reasoning would involve a large number of attention heads interconnected through complex computational graphs, making it challenging to extract meaningful insights.



To circumvent these difficulties, we adopt an alternative approach.
Specifically, we design a synthetic task which replicates key structures in two-hop reasoning with distractions, and train a small transformer on this task. At the beginning of the training, the small model is affected by distractions, recapulating the behaviour of LLMs observed in \Cref{fig:llm-distraction}. As the training continues, the small model goes through a phase transition and learns to ignore the distractions. 
By fully reverse-engineering the smaller model, we unveil the mechanism for random guessing. By further reducing the small model to a 3-parameter model (\Cref{sec:three_param_model}), we unveil that a sudden realization of the sequential query mechanism drives the transformer to solve the two-hop reasoning with distractions. 
Leveraging the aware of sequential query mechanism, we predict that LLMs finetuned on two-hop reasoning task with single distraction have length generalization and subsequently test them on larger models. 
The consistency of our results across different scales provides strong empirical evidence supporting the validity and generalizability of our findings (\Cref{sec:impl_llm}).
% Our methodology is close to induction heads~\citep{elhage2021mathematical}, physics

\Cref{fig:enter-label} provides an overview of the workflow of our paper. We summarize our main results and the workflow as follows:

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/workflow.pdf}
    \caption{An overview of the workflow of our paper.}
    \label{fig:enter-label}
\end{figure}

\begin{itemize}
    \item We build a dataset covering two-hop reasonings under various real-world contexts.     
    We show that LLMs (including Llama2-7B-Base \citep{touvron2023llama}, Llama3.1-8B-Base \citep{dubey2024llama}, Qwen2.5-7B-Base \citep{yang2024qwen2}) fail to solve the in-context two-hop reasoning task when exposed to distracting information in the prompt (\Cref{sec:llama-initial}).
    % \RZ{For this point, can we slightly change the narratives? We can say 'we formulate a practical two-hop reasoning task with distraction in the context. We showed that current large language models (i.e., Llama2-7b-base) fail to solve the task with distraction.' This emphasizes that our contribution is two-sided: we create a task and we show the model performs badly.}
    \item To better understand the underlying mechanism, we design synthetic tasks with symbolic prompts (\Cref{fig:data_illustration}). The trained three-layer transformer exhibits similar behavior to LLMs when presented with distracting information.  (\Cref{sec:illustration_mechanism}). 
    % \RZ{not 'train a TF that exhibit XXXX'}
    By analyzing the training dynamics and fully reverse-engineering the three-layer transformer, we identified two mechanisms that the three-layer transformer used for prediction on the two-hop reasoning task: a uniform guessing mechanism learned in the early stage and a sequential query mechanism after a sharp phase transition. \RZ{We can also slightly change this narrative.} 
    \item We conducted a more careful analysis of a three-parameter model (\Cref{sec:three_param_model}) to study why the transformer tends to learn the sequential query mechanism instead of a theoretically more efficient mechanism posited by previous literature. \RZ{TBA.} \hanlin{remove the last sentence if we did not compare sequential query and double induction head when analyzing 3-parameter models}
    \item To test whether the mechanism discovered from small-scale three-layer transformers is also adopted in large language models, we conduct further experiments on LLMs (\Cref{sec:impl_llm}), which provides strong evidence that the original pretrained model performs a uniform guessing mechanism, while very few steps of fine-tuning enable the model to learn a correct mechanism to solve two-hop reasoning tasks in the presence of distracting information.  
\end{itemize}


\begin{figure*}[h]  
    \centering
    \includegraphics[width=\textwidth]{figs/dgp.pdf}  
    \caption{An illustration of our synthetic data. Each shape represents an entity. A two-hop reasoning chain consists of two separate premises $\tokenA_i\to\tokenB_i$, the first hop, and $\tokenB_i\to\tokenC_i$, the second hop, where $\tokenA_i$ is the source entity (denoted as \src), $\tokenB_i$ is the bridge entity (denoted as \brg) and $\tokenC_i$  is the end entity (denoted as \ed). We study the case where the first hop always appears before the second hop.  In particular, to differentiate the two occurrences of the bridge entity, we use $\brga$ to denote the first occurrence of a bridge entity and $\brgb$ to denote its second occurrence.  For each premise, we call the entity before the arrow parent entity (denoted as $\parent$) and the other child entity (denoted as $\child$). In our synthetic data, we use different tokens to represent different entities and omit the arrow for simplicity. The last entity in the input is the query entity (denoted as $\query$), which matches one of the source tokens in the input, and the expected output is the corresponding end token. We define the source/bridge/end tokens corresponding to the query token as the target (denoted as \target) source/bridge/end tokens and the corresponding reasoning chain as the target chain. } 
    \label{fig:data_illustration}
\end{figure*}


\subsection{Related works}
\label{subsec:related_work}


Multi-hop reasoning is a common evaluation task for LLMs that requires composing several factual associations together \citep{zhong2023mquake}. Mechanistic interpretability studies have found that LLMs perform multi-hop reasoning by serially recalling intermediate hops \citep{biran2024hopping, yang2024latent, wang2024grokked, feng2024extractive}. This work studies an in-context multi-hop reasoning task, where the knowledge is extracted from context, in contrast to these in-weight multi-hop reasoning studies.

Earlier work introduced the induction-head mechanism for in-context learning in LLMs \citep{elhage2021mathematical, olsson2022context}. Recent theoretical and empirical analyses have extensively studied induction-head mechanisms in small transformers \citep{bietti2023birth, nichani2024transformers, wang2024transformers, chen2024unveiling}, demonstrating that a two-layer transformer is required to perform induction-head tasks \citep{sanford2024one}. The in-context two-hop reasoning task generalizes the induction-head task, as first proposed in \citet{sanford2024transformers}, where it was shown that a $\log k$-layers transformer is necessary and sufficient for performing in-context $k$-hop reasoning tasks. 


Mechanistic interpretability is a growing field focused on investigating the internal mechanisms of language models \citep{elhage2021mathematical, geva2023dissecting, meng2022locating, nanda2023progress, olsson2022context, bietti2024birth, wang2022interpretability, feng2023language, todd2023function}. Examples include mechanisms such as the induction head and function vector for in-context learning \citep{elhage2021mathematical, olsson2022context, todd2023function, bietti2024birth}, the binding ID mechanism for binding tasks \citep{feng2023language}, association-storage mechanisms for factual identification tasks \citep{meng2022locating}, and a complete circuit for indirect object identification \citet{wang2022interpretability}. Additionally, several studies have leveraged synthetic tasks to investigate neural network mechanisms \citet{charton2022my, liu2022towards, nanda2023progress, allen2023physics, zhu2023physics, guo2023transformers, zhang2022unveiling, lin2023transformers, guo2024active}. 

The dynamics of transformers have been explored under various simplifications, including linear attention structures \citep{zhang2024trained,ahn2024transformers}, reparametrizations \citep{tian2023joma,zhu2024towards}, and NTK \citep{deora2023optimization}. Much of this work focuses on in-context linear regression \citep{ahn2023linear,wu2023many,zhang2024context} and structured sequence learning \citep{bietti2024birth,nichani2024transformers,tian2023scan, guo2024active}. Notably, \citet{zhang2024trained, huang2023context, kim2024transformers} show that a one-layer attention head trained via gradient descent converges to a model capable of performing in-context regression. \citet{bietti2024birth} highlights the rapid emergence of bigram memorization and the slower development of in-context learning abilities. Meanwhile, \citet{reddy2023mechanistic} simplifies the structure of the induction head, revealing how sharp transitions in in-context learning dynamics arise from the nested nonlinearities of multi-layer operations. 

% In addition to the previously mentioned models, other toy models for training dynamics and interpretability include in-context learning in linear models \citep{zhang2024trained, guo2023transformers}, attention sink \citep{guo2024active}, the reversal curse~\citep{zhu2024towards}, etc. 


% Earlier work proposed the induction-head mechanism for in-context learning in real-world LLMs \cite{elhage2021mathematical, olsson2022context}. Theoretical and empirical analysis of induction head mechanisms for small transformers are extensively studied recently \cite{bietti2023birth}, \cite{nichani2024transformers}, \cite{wang2024transformers}, \cite{chen2024unveiling} where its has been proved that a two-layer transformer is required to perform induction-head task \cite{sanford2024one}. The in-context two-hop reasoning task is a generalization of the induction head task, first proposed in \cite{sanford2024transformers}. 
% Other toy model for dynamics and interoperability include: ICL linear regression \cite{zhang2024trained, guo2023transformers}, CoT XOR \cite{wen2024sparse}, Attention sink \cite{guo2024active}. 
% \begin{itemize}
%     \item Study expressivity or training dynamics of one or two-layer transformers with one or multiple heads per layer. 
%     \item The task can have the following structure: 
%     \begin{itemize}
%         \item Input: $x_1, x_2, x_3, \ldots, x_T \in \vocab^T$.  In particular, $x_1 = T_k$ for some $k \in [K]$, which denotes Task $k$.  For each task, there is a corresponding transition kernel $P_k(x'|x)$. Assume $P_k(\cdot|\cdot)$ has a unique stationary distribution $\pi_k(\cdot) \in \Delta(\vocab)$. Then the input distribution follows $x_2 \sim \pi_k(\cdot)$, $x_{t+1} \sim P_k(\cdot|x_t)$ for $2 \leq t < T$.
%         \item Output: $x_{T+1}$. Here, $x_{T+1} \sim P_k(\cdot|x_T)$.
%         \item The issue here is that if $T$ is long enough, then the transformer does not need the information of $x_1 = T_k$. Instead, the transformer can directly calculate the empirical transition kernel from the input.
%     \end{itemize}
%     \item Another choice:
%     \begin{itemize}
%         \item Input: $x_1, x_2 \in \vocab^T$.  In particular, $x_1 = T_k$ for some $k \in [K]$, which denotes Task $k$.  For each task, there is a corresponding transition kernel $P_k(x'|x)$. Assume $P_k(\cdot|\cdot)$ has a unique stationary distribution $\pi_k(\cdot) \in \Delta(\vocab)$. Then the input distribution follows $x_2 \sim \pi_k(\cdot)$.
%         \item Output: $x_3$. Here, $x_3 \sim P_k(\cdot|x_2)$.
%         \item The issue here is that the above formulation is equivalent to a two-order Markov chain $P(\cdot| x_1, x_2)$.
%         \item However, our task seems to be more difficult since there is no in-context learning information (i.e., the transformer needs to learn/memorize the transition kernel for each task instead of relying on the long input). 
%     \end{itemize}
%     \item The third choice:
%     \begin{itemize}
%         \item Consider the setting in \citet{nichani2024transformers}. The task in \citet{nichani2024transformers} is to learn a causal graph $\graph = ([T], \edge)$, and the transition kernel $\pi$ is learned in context. When $\graph$ is a tree, a single-head two-layer transformer can learn the causal graph. \citet{nichani2024transformers} also constructs a $k$-head attention for the case where each node in the graph can have either 0 or $k$ parents. The above result is still for the $k$-th order Markov chain. In our paper, we can study the case where there are $K$ graphs, i.e., $\graph_k = ([T], \edge_k), \forall k \in [K]$.
%         \item Input: the same as \citet{nichani2024transformers}, except that we have a task token $x_1 = T_k$. 
%         \item This looks more technically challenging than the second setting. 
%     \end{itemize}
% \end{itemize}

% (possible main message):
% \begin{itemize}
%     \item The decomposition of different heads on different tasks is at an entity level. For a certain entity, it might be true that different heads deal with different tasks. However, different entities might rely on different heads for the same task.
%     \item What's the core difference between a single head with dimension $d_{qkv} H$ and multi-heads ($H$ heads with dimension $d_{qkv} H$)? Why do we need multi-heads, and what are their benefits?
%     \begin{itemize}
%         \item For a single head with dimension $d_{qkv} H$ and $H$ heads with dimension $d_{qkv}$, there is no big difference for the output matrix $W_O\in \mathbb{R}^{d_{qkv}H \times d}$ and value matrix $W_V\in \mathbb{R}^{d_{qkv}H \times d}$, where 
%         \[
%         W_O = 
%         \begin{bmatrix}
%         W_O^{(1)} \\ 
%         W_O^{(2)} \\ 
%         \vdots \\ 
%         W_O^{(H)}
%         \end{bmatrix}
%         \quad \text{and} \quad
%         W_V = 
%         \begin{bmatrix}
%         W_V^{(1)} \\ 
%         W_V^{(2)} \\ 
%         \vdots \\ 
%         W_V^{(H)}
%         \end{bmatrix}.
%         \]
%         The most important feature of MHA is that it can produce $H$ distributions over the input sequences by key and query matrices.
%     \end{itemize}
% \end{itemize}
