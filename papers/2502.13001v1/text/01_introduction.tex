Meetings underlie collaboration and decision-making in corporations, academia, and government.
Meeting summaries help record key discussion points, update absentees, and capture to-dos \cite{ZhongYYZ21h, HuGDD23a, LaskarFCB23}.
While AI-based summaries are available on platforms such as Zoom\footnote{\href{https://www.zoom.com/en/ai-assistant}{https://www.zoom.com/en/ai-assistant}}, Microsoft Teams\footnote{\href{https://copilot.cloud.microsoft}{https://copilot.cloud.microsoft}}, they typically build on limited English data that does not represent the diversity of real meetings, e.g., multilingual sessions, specialized discussions \cite{KirsteinWGR25}.
Data scarcity for training and testing meeting summarization systems persists due to privacy and intellectual property concerns, along with expensive manual annotation \cite{AbachaYFL23}.
Existing corpora, such as AMI \cite{MccowanCKA05}, ICSI \cite{JaninBEE03}, and MeetingBank \cite{HuGDD23a}, offer only a narrow range of scenarios, which primarily revolve around staged business, academic, or parliamentary meetings.
Non-English resources like FREDSum \cite{RennardSGH23a}, containing manually transcribed and annotated political debates, remain sporadic and underscore the lack of linguistic diversity.

Researchers have explored synthetic transcripts to address this scarcity, but many methods are sub-optimal in scalability and realism.
Single-model, omniscient continuation \cite{QiuP24, ZhouSEK24} can produce dialogues lacking actual knowledge and speaker interplay, while crowdsourced role-plays are expensive \cite{MccowanCKA05,ThulkeGJD24}. 
Automated heuristics (e.g., noising, swapping) often yield disjointed conversations \cite{ChenY21, ParkSL22, LiuMSN22}. 
These approaches struggle to balance large-scale generation with authentic group dynamics and credible topic evolution, even though they can process and generate thousands of tokens.




We introduce \textbf{\pipeline{}} (\textbf{M}ulti-agent \textbf{IMI}tation of \textbf{C}onversations, see \Cref{fig:main_figure}), a movie-production-inspired framework based on multi-agent debate \cite{LiangHJW24a,DuLTT24}.
\pipeline{} summarizes a knowledge source, expands the summary into an agenda, and orchestrates psychologically grounded agents debating turn-by-turn, allowing interruptions (e.g., phone calls).
A modular refinement step mitigates repetitions or overly formal speech, ensuring plausible and coherent discourse at scale.
With \pipeline{}, we generate \textbf{\dataset{}} (\textbf{FA}ke \textbf{ME}etings), a corpus of 800 meetings (500 English, 300 German parallel in input) covering 14 meeting types (e.g., project updates, brainstorming) and 300 Wikipedia articles as a knowledge source.



We evaluate the synthetic meetings on quality criteria, e.g., naturalness, coherence, \cite{ChenPTK23}, and transcript difficulties, e.g., scattered information \cite{KirsteinWRG24a}, and propose a new psychology-based measure of behavioral authenticity \cite{ChoiAVQ20}.
Human ratings confirm that \dataset{} achieves near-real spontaneity (4.5/5  in naturalness), preserves speaker-related challenges (3/5 in spoken language), and intensifies low-information-density difficulties (4/5).
Comparisons with real transcripts and 100 crowdsourced meeting experiences show a similar behavioral pattern.
Evaluations of GPT-4o \cite{OpenAIAAA24}, Gemini 1.5 pro \cite{GeminiTeamRST24}, DeepSeek-R1 Distill Llama 70B \cite{DeepSeekAIGYZ25a}, Llama 3.3 70B\footnote{We will refer to these as GPT, Gemini, DeepSeek, and Llama throughout the paper.} \cite{GrattafioriDJP24} reveal persistent context-handling issues. 
Ablation studies show that \pipeline{} reliably generates good transcripts using different inputs or LLMs.

\paragraph{Contributions.}
\begin{itemize}[noitemsep, topsep=0pt, leftmargin=*]
    \item \textbf{\pipeline{}}: A multi-agent simulation method that captures realistic group dynamics.
    \item \textbf{\dataset{}}: A corpus of 800 meetings in English (500) and German (300) on diverse topics and meeting formats, including quality annotations.
    \item A \textbf{psychology-based evaluation framework} for behavioral authenticity, addressing a gap in measuring agent interactions. 
\end{itemize}