\label{sec:data_insights}

In the following, we detail how \dataset{} was generated, present its overall statistics, and analyze the authenticity of its simulated meetings and how challenging they are compared to real transcripts.
All data and annotations performed here are available per \Cref{sec:Data_Availability}.


\subsection{Setup to generate \dataset{}}

\paragraph{Knowledge source.} 
Unlike synthetic datasets built on minimal or artificial context, \dataset{} builds on grounded text from Wikipedia.
We select 300 articles from 28 broad domains (e.g., Global Issues, Technological Innovations, Cultural Diversity, Philosophy, Environment \& Ecology), each meeting three criteria: (1) have at least five subsections, (2) no negative flags regarding article quality, and (3) no reference deficiencies (to avoid contradictory claims).
For the German subset, we choose 150 German-language articles from the same pool, ensuring a high content overlap with their English counterparts via BERTScore \cite{ZhangKWW20} and an empirically chosen threshold of 0.7.
For each article, we randomly assign meeting types, participant roles, and the number of participants, yielding 500 English and 300 German meetings.


\paragraph{Backbone model.}
We use GPT \cite{OpenAIAAA24} for all stages of \pipeline{}, taking advantage of its 128k-token context window and robust role-playing capabilities \cite{KirsteinRKG24a}.
In \Cref{sec:ablation}, we show that models with fewer parameters (Gemini, DeepSeek, Llama) can generate high-quality transcripts with minor drops in naturalness.



\paragraph{Baseline.}
We compare \dataset{} to QMSum \cite{ZhongYYZ21h}, an established dataset combining academic (ICSI \cite{JaninBEE03}), product (AMI \cite{MccowanCKA05}), and parliament (Welsh/Canadian, WPCP) meetings.
While other corpora exist (e.g., MeetingBank \cite{HuGDD23a}, ELITR \cite{NedoluzhkoSHG22}), they closely resemble QMSum’s formal institutional settings.
We also contrast our non-omniscient multi-LLM pipeline with a single omniscient GPT approach \cite{ZhouSEK24} that generates entire meetings in one shot given the same article, target summary, speaker count, and meeting type used for \dataset{}.
To match in meeting length, we prompt GPT multiple times to circumvent its 16k-token output limit and produce transcripts of $\sim$20k tokens.



\subsection{General Statistics}
\noindent
\textbf{Subsection key finding.}
\dataset{}’s diversity enables large-scale benchmarking of summarizers, mimicking the varied conditions seen in real meetings.


\paragraph{Comparison.}

\Cref{tab:corpora_metrics} summarizes statistics for \dataset{}.
Our average meeting length ($\sim$6,200 words), turn count ($\sim$400 turns), and participant count ($\sim$4) closely mirrors real corpora (e.g., AMI), though \dataset{} has higher variance (e.g., larger standard deviation for turn counts).




\paragraph{Unique features.}
In addition to standard business, academic, and parliamentary contexts, \dataset{} spans 14 new meeting types and 28 Wikipedia-based domains.
\dataset{}’s dialogues primarily rephrase rather than copy source passages (average token overlap of English: 0.081, German: 0.096).
Over 3,000 participanth adopt unique speech patterns and undergo up to four behavioral shifts, introducing dynamic role changes beyond the fixed-character setups typical of existing corpora.
One-third of the meetings have interruptions, with 20\% of those featuring three or more disruptions. 
Existing corpora typically lack these unpredictabilities.



\subsection{Authenticity Evaluation}
\label{sec:authenticity_eval}


\noindent
\textbf{Subsection key finding.}
\dataset{} meetings exhibit near-real conversation flow, and participants behave in ways that closely match human expectations.

\paragraph{Approach.}

We evaluate meeting authenticity along two axes: overall authenticity, i.e., coherence, consistency, interestingness, naturalness \cite{ChenPTK23}, and participant behavior authenticity, e.g., conflicts and power dynamics, defined in consultation with psychology and sociology experts.
Therefore, eight overarching behavior categories, i.e., knowledge, power, conflict, status, trust, support, similarity, and fun \cite{bales2009interaction,ChoiAVQ20}, were divided into 18 items complete list given in \Cref{tab:appendix_behavior_questions} in \Cref{sec:appendix_E}).

Six annotators (students to PhD candidates in Psychology, Computer Science, Communication; aged 23–28, at least C1 English/German) rate 30 English and 30 German meetings from \dataset{}, as well as 30 meetings from QMSum, using a 1–5 Likert scales Krippendorff's $\alpha = 0.83$\footnote{
To support broader community use, we extend annotations across the entire \dataset{}, we used GPT with the three-step approach of \citet{KirsteinLG25}, aligned automatic scores through the self-training concept to match human ratings until the average discrepancy was below 0.8 points.
}.
Because existing real-meeting corpora may exhibit a behavior bias due to the formal or staged nature, we also surveyed 100 crowdworkers (age 24–63, balanced by gender, diverse professional backgrounds such as law, engineering, and management) who frequently attend meetings to collect experiences with real meetings.
Additional details on annotators, crowdworkers, and reliability are in \Cref{sec:appendix_F}.


\input{tables/common_dimensions_eval}

\paragraph{Quantitative analysis.}
\Cref{tab:common_dimensions_evaluation} compares overall authenticity scores for \dataset{} and QMSum.
The English/German \dataset{} subsets match QMSum in coherence, while both subsets score higher in conciseness and interestingness (4.5/5 in \dataset{} vs.\ 4/5 in QMSum). 
ICSI meetings receive the lowest interestingness rating (3/5) due to slower topic evolution, whereas WPCP leads in naturalness (5/5). 
The German \dataset{} subset lags the English one by 0.5 points in naturalness (English: 4.5/5), which is similar to most QMSum data.
Notably, transcripts generated by the single omniscient model rank lowest overall due to shallow, repetitive content.

As shown in \Cref{fig:psychology_grounded_evaluation}, social behavior patterns in \dataset{} align closely with both QMSum (within 0.2 points) and the crowdsourced baseline (within 1.0 points). 
Modest differences likely reflect gaps between self-assessment and observed performance \cite{kruger1999unskilled},  often linked to social desirability or recency biases.
The synthetic meetings’ mild overperformance in some categories suggests a slight dramatization of behaviors.
We show the behavior pattern in single-LLM meetings in \Cref{sec:appendix_A}, which just minimal overlaps with the \dataset{} patterns.
In sum, \pipeline{} speakers reliably replicate real behaviors.



\paragraph{Qualitative insights.}
Annotators highlight the realistic interplay of participant knowledge, consistent role portrayal, and dynamic sub-topic exploration.
Occasionally, overuse of transitional phrases (e.g., “This sounds great, but...”) or overly cordial tones reveal the synthetic origin (noted in 57 out of 800 meetings).
Despite these artifacts, \dataset{} remains a near-realistic meeting proxy, offering a robust environment to develop and benchmark summarization methods.
In contrast, the omniscient approach often yields shallow dialogues, word/passage repetition, or even recitations from well-known sources (e.g., the Bible).




\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/storage/Psychology_Figure.png}
    \caption{Evaluation of the social behavior in the meetings. All questions are detailed in \Cref{sec:appendix_A}. EN and GER are the English and German subsets of \dataset{}, and CS indicates the crowdsourced experiences.}
    \label{fig:psychology_grounded_evaluation}
\end{figure}


\subsection{Transcript Challenge Assessment}
\label{sec:challenges}
\noindent
\textbf{Subsection key finding.} 
\dataset{} preserves speaker-related complexities and extends information-centric challenges closer to real meetings.

\paragraph{Analysis.}
We adopt the seven challenges from \citet{KirsteinWRG24a}, i.e., spoken language, speaker dynamics, coreference, discourse structure, contextual turn-taking, implicit context, and low information density, to gauge the difficulty of summarizing \dataset{} transcripts (\Cref{tab:challenge_scores}).
We compare \dataset{}’s median 1–5 Likert ratings against human-annotated QMSum data \cite{KirsteinWRG24a}.
Overall, \dataset{} matches QMSum in spoken language (3/5) and coreference (2/5, German subset increases to 3/5), though it exhibits calmer speaker dynamics (2/5 vs.\ $\sim$3/5 in QMSum).
Both \dataset{} subsets surpass QMSum in implicit context (\dataset{}: 4/5 vs.\ QMSum: 0/5) and low information density (\dataset{}: 4/5 vs.\ QMSum: 2.5/5), indicating that participants rely on prior exchanges instead of explicitly reiterating every detail.
By contrast, many existing corpora emphasize exhaustive information sharing (ICSI, WPCP) or feature staged, side-discussion-free scenarios (AMI).


\input{tables/challenges_eval}
