\label{sec:ablation}
Our ablations analyze \pipeline{} under different settings and assess its role consistency.
Below, we briefly summarize the experiments; full details, tables, and figures appear in \Cref{sec:appendix_H}.
All evaluations use the same model settings, metrics, and annotation guidelines from \Cref{sec:data_insights,sec:experiments}.


\paragraph{Knowledge source shapes discussion depth and structure.} 
Beyond the semi-structured Wikipedia articles used for \dataset{}, we test \pipeline{} on 10 research papers from arXiv \cite{CohanDKB18} (clearly sectioned, e.g., abstract, methods, results) and 10 human-written stories from WritingPrompts \cite{FanLD18a} (unstructured).
Using GPT as the backbone, we generate one synthetic meeting per source (20 total) and apply our evaluation framework (\Cref{sec:data_insights}).
As shown in \Cref{tab:app_knowledge_source} (overall authenticity) and \Cref{fig:app_psychology1} (behavior authenticity) in \Cref{sec:app_knowledge_source}, quantitative metrics remain stable, suggesting meeting naturalness is not strongly tied to input structure. 
Research papers lead to deeper discussions, while short stories produce briefer, shallow meetings.



\paragraph{Other models yield shorter meetings.} 
To see if other LLMs can drive \pipeline{}, we replace GPT with DeepSeek and Llama, generating 25 meetings per model using the \dataset{} Wikipedia article pool.
We assess quality (overall authenticity, behavior authenticity, challenges) and compare outputs using identical knowledge sources.
The results are given in \Cref{tab:app_mid_size_models} (overall authenticity) and \Cref{fig:app_psychology2} (behavior authenticity) in \Cref{sec:app_mid_size_models}.
Although these models produce transcripts with about 50 fewer turns than \dataset{}, their naturalness remains high (4/5), and they replicate participant roles and behaviors almost as consistently as GPT, close to human expectations.
Occasionally, transcripts feature fewer back-and-forth exchanges but remain high-quality in qualitative reviews, thereby broadening \pipeline{}’s real-world applicability.

\paragraph{Editing is model-dependent but rarely critical.}
Stage 7 (editing) addresses issues such as formal phrasing or repeated filler words.
To measure its influence, we review 75 transcripts (25 each from GPT, DeepSeek, and Llama), examining chain-of-thought logs of stage 7 and final transcripts.
Only \emph{1 in 25} GPT transcripts require major edits to mask synthetic traits, rising to \emph{2 in 25} for the Llama-based models\footnote{\dataset{} contains a flag if refinement is due to a pipeline issue or a model-specific artifact along the feedback from the editorial stage.}. 
Minor refinements correct model-specific wording or repeated transitions, indicating that \pipeline{} already yields coherent discussions while stage 7 polishes for full realism.
% <-- maybe some related work here too? -->




\paragraph{Roles and behaviors are reliably enacted.}
Drawing on \citet{SerapioGarciaSCS23a}, we evaluate whether participants preserve assigned roles and behaviors.
We select 30 participants from the newly generated meetings (10 from GPT, Gemini, DeepSeek, and Llama) and ask our six annotators to judge each turn’s alignment with predefined behavior (e.g., blocker).
We cover 100 scenes and 400 simulated participants in total.
More than 95\% of GPT turns match their roles, dropping slightly to 92\% for the other two\footnote{Annotation will be extended and made available later.}.
To cross-check, we sample 50 participant profiles and prompt GPT enacting these profiles to answer the TREO questionnaire \cite{MathieuTKD15} containing 48 1--5 Likert-scored questions to determine someone's role in a group.
We find 48 of the profiles consistent with the assigned behaviors\footnote{All responses are provided as per \Cref{sec:Data_Availability}.}.
We conclude that \pipeline{} enforces coherent persona dynamics for social simulation \cite{ZhouZMZ24}.
