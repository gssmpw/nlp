\label{sec:experiments}

\input{tables/summary_evaluation}
We evaluate current LLMs on abstractive meeting summarization for real and synthetic transcripts, sampling 30 meetings from each \dataset{} (English, German) and QMSum (90 total).


\subsection{Experimental Setup}
\paragraph{Summarization approaches.} 
We benchmark two closed-weight models (GPT, Gemini) and two open-weight models (Llama, DeepSeek), excluding refinement-based methods \cite{KirsteinLG25a}, as these would produce self-refined GPT summaries.
We use a simple zero-shot prompt requesting an abstract summary of up to 250 tokens, reflecting standard practices in meeting summarization \cite{KirsteinLG25a}.
Full prompt details are provided in \Cref{sec:appendix_A}.


\paragraph{Evaluation metrics.}
We compare system outputs against reference summaries using the established ROUGE (R-1/R-2/R-L) \cite{Lin04} and BERTScore (rescaled F1) \cite{ZhangKWW20} metrics along MESA \cite{KirsteinLG25}, an LLM-based metric for the error types of meeting summarization (e.g., structure, irrelevance).
These metrics enable direct comparisons with prior work and provide detailed insights into model weaknesses.





\subsection{Results}
\label{subsec:results_summ}

\Cref{tab:summary_performance} contains the evaluation scores.

\paragraph{Reasoning boosts performance.}
Although Llama tops the ROUGE/BERTScore metrics on both QMSum and \dataset{}, MESA shows that DeepSeek consistently matches or improves over other models by minimizing common error types ($\sim$1 point lower per category), especially on QMSum.
DeepSeek also outperforms on the German \dataset{} subset, while Gemini generally trails behind.
All models perform comparably on the English subset, with the least language and coreference issues ($\sim$ 1/5).


\paragraph{\dataset{} reveals LLM struggles.} 
MESA scores for categories like incoherence, structure, and repetition remain similar to those on QMSum ($\sim$ 4/5) but with lower deviation for the \dataset{} subsets.
We conclude that the varied topics and meeting formats in \dataset{} add to the overall difficulty and negatively influence summary quality.


\paragraph{Contextualization deficits persist.}
The omission and irrelevance rows show that all models struggle with \dataset{}'s more difficult cross-turn information and information scarcity observed in \Cref{sec:challenges}.
Omission rises from 3/5 on QMSum to 4/5 on \dataset{}, and irrelevance from 2/5 to 3/5 for the English subset (German remains 2/5).
We derive that reliable content understanding \cite{KirsteinRKG24a,KirsteinLG25a} can be tested with our \dataset{} and that current LLMs struggle with this.
