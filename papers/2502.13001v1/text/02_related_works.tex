\noindent
\paragraph{Meeting summarization datasets.}
Most meeting summarization research relies on a few standard English-only corpora \cite{KirsteinWGR25}, notably AMI \cite{MccowanCKA05}, i.e., staged business, and ICSI \cite{JaninBEE03}, i.e., academic.
While QMSum \cite{ZhongYYZ21h} and MeetingBank \cite{HuGDD23a} broaden coverage to parliamentary sessions and city councils, non-English data remains sparse (e.g., FREDSum \cite{RennardSGH23a} for French and ELITR \cite{NedoluzhkoSHG22} for Czech).
We address this gaps by releasing \dataset{}, a corpus of 800 synthetic English and German meetings spanning diverse topics, meeting formats, and speaking styles.


\paragraph{Synthetic dialogue generation.}
Existing synthetic meeting data generation approaches often rely on relatively simple text continuation by LLMs \cite{QiuP24} or heuristics, e.g., noising, swapping \cite{ChenY21, ParkSL22, LiuMSN22}, risking superficial turn-taking and unrealistic participant behavior \cite{KirsteinWGR25}.
Small-scale manual simulations \cite{ThulkeGJD24} offer greater realism but are costly and hard to scale.
Recent tools like Google’s NotebookLM \cite{google-2024} and Nvidia’s PDF-to-Podcast \cite{nvidia-2025} transform documents into two-speaker podcasts but lack multi-participant group dynamics.
In contrast, \pipeline{} simulates turn-by-turn interactions among psychology-based agents with their own memory, allowing spontaneous debates and evolving stances. 
A post-processing module addresses common LLM flaws (e.g., repetition, vocabulary), ensuring high-quality, naturally flowing conversations.

