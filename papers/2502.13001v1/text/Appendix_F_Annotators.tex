\subsection{Complete annotation process}
\paragraph{Annotator selection:}
Our annotation team consisted of six graduate students, officially employed as interns or doctoral candidates through standardized contracts.
We selected them from a pool of volunteers based on their availability to complete the task without time pressure and their proficiency in English and German (native speakers or C1-C2 certified).
By that, we ensured they could comprehend meeting transcripts and summaries.
We aimed for gender balance (three males, three females) and diverse backgrounds, resulting in a team of two computer science students, three psychology students, and one communication science student aged 23-28.
The annotators consent that their annotations will be used anonymously in this work.
The annotation process has been internally reviewed by an ethics committee.

\paragraph{Preparation:}
We prepared a comprehensive handbook for our annotators, detailing the project context and defining the criteria (a short version as presented in \Cref{sec:appendix_E} and an extended version with more details).
Each definition included two examples: one with minimal impact on quality and one with high impact.
The handbook explained the 1--5 Likert rating for the individual questionnaires.
The handbook did not specify an order for processing the items.
We provided the handbook in English and the annotators' native languages, using professional translations.

We further elaborated a three-week timeline for the annotation process, preceded by a one-week onboarding period.
The first week featured twice-weekly check-ins with annotators, which were reduced to weekly meetings for the following two weeks. Separate quality checks without the annotators were scheduled weekly.
(Note: week refers to a regular working week)

\paragraph{Onboarding:}
The onboarding week was dedicated to getting to know the project and familiarization with the definitions and data.
We began with a kick-off meeting to introduce the project and explain the handbook, particularly focusing on each definition.
We noted initial questions to potentially revise the handbook. 
Annotators received ten transcripts and summaries generated by GPT \cite{OpenAIAAA24} using \pipeline{}.
After the first five samples, we held individual meetings to clarify any confusion and updated the guidelines accordingly.
The remaining five samples were then annotated using these updated guidelines. 
A second group meeting this week addressed any new issues with definitions.
After the group meeting, we met individually with the annotators to review their work, ensuring their quality and understanding of the task and samples. 
All six annotators demonstrated reliable performance and good comprehension of the task and definitions, judging from the reasoning they provided for each decision and annotation.
We computed an inter-annotator agreement score using Krippendorff's alpha, achieving 0.86, indicating sufficiently high overlap.

\paragraph{Annotation Process:}
Each week, we distribute all samples generated by one model/source (on average 27 samples) to one of the annotators. 
Consequently, one annotator worked through all samples of one model/source in one week.
On average, one annotator processes summaries from three models/sources (depending on other commitments, some annotators could only annotate two datasets, and others four or more).
Three annotators annotate each sample. 
The annotators were unaware of the origin of the meeting transcript and summary.
They were given a week to complete their set at their own pace and with their own break times. 
Quiet working rooms were provided if needed for concentration.
To mitigate position bias, the sample order was randomized for each annotator. 
Annotators could choose their annotation order for each sample and were allowed to revisit previous samples.

Regular meetings were held to address any emerging issues or questions on definitions. 
During the quality checks performed by the authors, we looked for incomplete annotations, missing explanations, and signs of misunderstanding judging from the provided reasoning. 
If the authors had found such a lack of quality, the respective annotator would have been notified to re-do the annotation.
After three weeks, we computed inter-annotator agreement scores on the error types (overall Krippendorff's alpha = 0.79). 
If we observed a significant difference among annotators, we planned a dedicated meeting with all annotators and a senior annotator to discuss such cases. 
On average, annotators spent 44 minutes per sample, completing about six samples daily.

\paragraph{Handling of unexpected cases:}
Given that our annotators had other commitments, we anticipated potential scheduling conflicts. 
We allowed flexibility for annotators to complete their samples beyond the week limit if needed, reserving a fourth week as a buffer. 
Despite these provisions, all annotators completed their assigned samples within the original weekly timeframes. 
We further allowed faster annotators to continue with an additional sample set.
This additional work was voluntary.


\subsection{Crowdworkers}
The crowdworkers comprised 100 employees (officially employed through standardized contracts) with diverse backgrounds, including psychology, law, business administration, physics, and design.
We selected them from a pool of volunteers and ensured that the crowdworkers did not overlap with the annotators.
We aimed for gender balance (57 male, 43 female), covering ages 24 to 63.
The crowdworkers consent that their answers (Likert scores) will be used anonymously in this work.
The crowdsourcing has been internally reviewed by an ethics committee.
The crowdworkers were given the behavior questionnaire in \Cref{tab:appendix_behavior_questions} (\Cref{sec:appendix_E}) in their native language with the task of answering the statements according to their general experience with meetings.
The 1--5 Likert scale rating scheme was initially explained to them with an example.
In case of unclarities, these were directly resolved.
The items were answered on a Laptop with an average time of 12 minutes.