We introduced \pipeline{}, a seven-stage multi-agent framework that uses psychologically grounded, non-omniscient LLMs to generate source-grounded meeting transcripts.
\pipeline{} generated \dataset{}, a multilingual corpus of 500 English and 300 German meetings on 28 domains and 14 meeting types.
Human assessments showed that \dataset{} closely mirrors real meetings (4.5/5 in naturalness) and amplified low-information density (4/5).
Comparisons with real meetings suggest that \dataset{} captures authentic group dynamics, while our ablation studies highlighted how varying knowledge sources and backbone models shape transcript quality and demonstrate the GPT's reliable role enactment.

\dataset{} and its detailed annotations open new directions for meeting summarization, from multilingual model development to re-introducing fine-tuning for LLMs and reinforcement learning to address persistent shortcomings (\Cref{sec:experiments}).
By releasing \pipeline{} as open-source, we provide a powerful toolkit that researchers can adapt to low-resource languages, diverse domains, and a range of conversational styles. 
The frameworkâ€™s psychology-based behavior definitions and evaluation methodology bring a higher level of realism into synthetic conversations, enabling deeper investigations of social dynamics.
Our work bridges a data gap through human-like meeting simulations that foster advances in summarization, conversational AI, social simulation, and beyond.
