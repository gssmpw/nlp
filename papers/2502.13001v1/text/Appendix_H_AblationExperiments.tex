\label{sec:appendix_H}

This appendix provides additional information on our ablation experiments (\Cref{sec:ablation}), elaborating on the models, evaluation procedures, annotator settings, and preliminary results. Unless otherwise stated, all evaluations follow the methodologies described in \Cref{sec:data_insights} and \Cref{sec:experiments}.

\subsection{Experimental Setup and Models}

\paragraph{Framework.}
We use the same seven-stage \pipeline{} approach (pre-production, production, post-production) outlined in the main paper to generate synthetic meetings.

\paragraph{Models.}
We primarily employ the GPT-4o model as a reference. In addition, we evaluate three LLMs, i.e., Gemini 1.5 pro, DeepSeek-R1 Llama Distill 70B, Llama 3.3 70B.
Each model receives identical prompts at every \pipeline{} stage. Hyperparameters (e.g., temperature, top-p) remain at the defaults recommended in each model’s documentation to maintain comparability.

\paragraph{Data Sources.}
In addition to the semi-structured Wikipedia articles that power \dataset{}, we also experiment with:
\begin{itemize}[leftmargin=1em]
    \item \textbf{Research Papers:} 10 arXiv papers \cite{CohanDKB18} with conventional sections (abstract, methods, results, conclusion).  
    \item \textbf{Stories:} 10 human-written short stories from WritingPrompts, each at least 500 tokens and lacking formal structure \cite{FanLD18a}.
\end{itemize}

\subsection{Evaluation Approach}

We use the same metrics and annotation protocols described in \Cref{sec:data_insights} and \Cref{sec:experiments}, summarized below:

\begin{itemize}[leftmargin=1em]
    \item \textbf{Overall Authenticity:} Following \citet{ChenPTK23}, we measure coherence, consistency, interestingness, and naturalness (1--5 Likert scale).
    \item \textbf{Behavior Authenticity:} Building on psychology/sociology literature \cite{ChoiAVQ20,bales2009interaction} and \Cref{sec:data_insights}, we consider eight overarching categories (e.g., knowledge sharing, conflict, power) subdivided into 18 specific items.
    \item \textbf{Challenge Scores:} Adopted from \citet{KirsteinWRG24a}, focusing on complexities like spoken language, speaker dynamics, coreference, implicit context, and low information density.
    \item \textbf{Manual Qualitative Checks:} For each experiment, we sample transcripts to review dialogue flow, persona behavior, chain-of-thought traces from Stages 5 (Quality Assuring) and 7 (Editing), plus any repetitive patterns or artifacts that might reveal synthetic origins.
\end{itemize}

All annotation tasks were performed by six raters (backgrounds in psychology, computer science, communication). They used the labeling guidelines introduced in \Cref{sec:experiments}, holding weekly calibration sessions to maintain high inter-annotator agreement (Krippendorff’s $\alpha > 0.80$). Additional annotator details are in \Cref{sec:appendix_F}.

\subsection{Knowledge Source Shapes Discussion Depth and Structure}
\label{sec:app_knowledge_source}

\paragraph{Goal.}
We aim to discover whether the level of structure in source texts (structured vs.\ unstructured) significantly impacts the generated meetings’ authenticity and behavioral patterns.

\paragraph{Setup.}
Using GPT as the backbone, we generated 20 synthetic meetings, one for each knowledge source (10 research papers, 10 short stories). Each transcript was configured to one of three meeting types (e.g., decision-making, brainstorming, innovation forum) to ensure diversity. We then applied our evaluation framework (\Cref{sec:data_insights}), measuring overall authenticity, behavior authenticity, and challenge scores, supplemented by a qualitative analysis of discussion depth.

\paragraph{Results.}
\Cref{tab:app_knowledge_source} and \Cref{fig:app_psychology1} show that meetings derived from research papers often feature deeper discussions and smoother scene transitions, attributed to the documents’ well-defined sections. 
In contrast, short-story inputs yield shorter and more tangentially structured meetings. Quantitative metrics (e.g., overall authenticity) remain close to those for Wikipedia-based transcripts, implying that meeting \emph{naturalness} does not heavily depend on the input’s structural clarity.









\subsection{Mid-Size Backbone Models for \pipeline{}}
\label{sec:app_mid_size_models}

\paragraph{Goal.}
We investigate whether \pipeline{} maintains comparable transcript quality when using other LLMs rather than GPT.

\paragraph{Setup.}
We replaced GPT with DeepSeek and Llama and randomly picked 25 Wikipedia articles from the \dataset{} pool. Each article was used to generate one synthetic meeting per model (75 total). We then ran the same evaluation process, collecting both quantitative metrics (authenticity, behavior, challenge scores) and qualitative feedback on dialogue flow and realism.

\paragraph{Results.}
\Cref{tab:app_mid_size_models} shows that other models produce an average of 50 fewer turns per transcript compared to GPT, yet maintain near-equal naturalness (4/5 vs.\ 4.5/5). Annotators report that roles and psychological behaviors (\Cref{fig:app_psychology2}) are faithfully preserved, though about one-third of the transcripts have fewer back-and-forth exchanges. Despite this, reviewers found the outputs coherent and realistic.



\begin{table}[t]
    \centering
    \renewcommand{\arraystretch}{1.2} % Increase vertical spacing by 20%
    \scriptsize
    \setlength{\tabcolsep}{3.8pt} % Adjust spacing if needed

    % Example color definition; place this in your preamble or near the top of your document
    \definecolor{highlightGreen}{HTML}{D4F4E9} 

    \begin{tabular}{lccc}
        \toprule
        \rowcolor{gray!20} % Light gray background for header
        & \textbf{\dataset{} EN} & \textbf{arXiv} & \textbf{Stories}\\
        \midrule
        COH 
            & \cellcolor{highlightGreen}{$\textbf{4.5}_{\textit{0.00}}$}
            & $4.0_{\textit{0.38}}$
            & $4.0_{\textit{0.46}}$ \\
        CON 
            & $4.5_{\textit{0.07}}$
            & $4.5_{\textit{0.05}}$
            & $4.5_{\textit{0.08}}$ \\
        INT 
            & \cellcolor{highlightGreen}{$\textbf{4.5}_{\textit{0.13}}$}
            & \cellcolor{highlightGreen}{$\textbf{4.5}_{\textit{0.54}}$}
            & $4_{\textit{0.86}}$ \\
        NAT 
            & \cellcolor{highlightGreen}{$\textbf{4.5}_{\textit{0.12}}$}
            & $4_{\textit{0.97}}$
            & \cellcolor{highlightGreen}{$\textbf{4.5}_{\textit{0.73}}$} \\
        \bottomrule
    \end{tabular}
    \caption{Average authenticity scores for structured (research papers) vs.\ unstructured (short stories). \dataset{} values taken from \Cref{sec:data_insights}. Values are Median$_{Std}$. \textbf{Higher} is better.}
    \label{tab:app_knowledge_source}
\end{table}





\begin{figure}[t!]
    \centering
    \includegraphics[width=0.6\linewidth]{figure/storage/Psychology_figure_KS.png}
    \caption{Behavior authenticity comparison for different knowledge sources.}
    \label{fig:app_psychology1}
\end{figure}


\subsection{Analysis of Editing Stage (Stage 7) Influence}
\label{sec:app_refinement}

\paragraph{Goal.}
We evaluate the necessity of Stage 7 (editing/refinement) in removing synthetic cues or repetitive language to achieve realistic transcripts.

\paragraph{Setup.}
We manually audited 75 transcripts generated during the mid-size model experiment: 25 each from GPT, DeepSeek, and Llama. Reviewers compared chain-of-thought logs at Stage 7 with final transcripts to identify whether refinements corrected pipeline-wide issues (e.g., missing subtopics) or model-specific artifacts (e.g., repeated filler phrases).

\paragraph{Results.}
We observed that only \emph{1 in 25} GPT transcripts needed major edits to hide synthetic traits, rising to \emph{2 in 25} for mid-scale models. Most refinements were minor vocabulary adjustments or removal of repetitive transition phrases. Thus, \pipeline{} inherently produces coherent multi-agent dialogues, with Stage 7 serving primarily as a final polish to further mask synthetic cues.


\begin{table}[t]
    \centering
    \renewcommand{\arraystretch}{1.2} % Increase vertical spacing by 20%
    \scriptsize
    \setlength{\tabcolsep}{3.8pt} % Adjust spacing if needed

    % Example color definition; place this in your preamble or near the top of your document
    \definecolor{highlightGreen}{HTML}{D4F4E9} 

    \begin{tabular}{lccc}
        \toprule
        \rowcolor{gray!20} % Light gray background for header
        & \textbf{GPT} & \textbf{DeepSeek} & \textbf{Llama}\\
        \midrule
        COH 
            & $4.5_{\textit{0.00}}$
            & $4.5_{\textit{0.15}}$
            & $4.5_{\textit{0.18}}$ \\
        CON 
            & $4.5_{\textit{0.07}}$
            & $4.5_{\textit{0.04}}$
            & $4.5_{\textit{0.03}}$ \\
        INT 
            & \cellcolor{highlightGreen}{$\textbf{4.5}_{\textit{0.13}}$}
            & \cellcolor{highlightGreen}{$\textbf{4.5}_{\textit{0.94}}$}
            & $4_{\textit{0.37}}$ \\
        NAT 
            & \cellcolor{highlightGreen}{$\textbf{4.5}_{\textit{0.12}}$}
            & \cellcolor{highlightGreen}{$\textbf{4.5}_{\textit{0.84}}$}
            & $4_{\textit{0.71}}$\\
        \bottomrule
    \end{tabular}
    \caption{Average authenticity scores for DeepSeek and Llama vs. \ GPT-4o. GPT-4o values taken from \Cref{sec:data_insights}. Values are Median$_{Std}$. \textbf{Higher} is better.}
    \label{tab:app_mid_size_models}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.6\linewidth]{figure/storage/Psychology_Figure_models.png}
    \caption{Behavior authenticity for GPT, DeepSeek (DS) and Llama (L) backbone models.}
    \label{fig:app_psychology2}
\end{figure}


\subsection{Roles and Behaviors Are Reliably Enacted}
\label{sec:app_roles_behaviors}

\paragraph{Goal.}
We test whether participants adhere to their assigned roles (e.g., project manager) and psychological behaviors (e.g., conflict aversion, leadership) throughout a meeting.

\paragraph{Setup.}
We sampled 30 newly generated meetings, 10 each from GPT, Gemini, DeepSeek, and Llama, and had six annotators evaluate each turn for alignment with assigned persona traits (e.g., status consciousness, creative thinking). This encompassed 100 scenes and 400 participants. Inspired by \citet{SerapioGarciaSCS23a}, we further deployed the TREO questionnaire \cite{MathieuTKD15}, containing 48 Likert-scored questions, to see whether a prompted model persona would self-report consistently with its designated role.

\paragraph{Results.}
Over 90\% of GPT’s turns (and 87\% from DeepSeek/Llama) aligned with the assigned persona. In 48 of 50 TREO questionnaires,\footnote{All participant responses are provided as per \Cref{sec:Data_Availability}.} the questionnaire results matched the participant’s predefined behavior. One notable exception is the “Blocker” role, which GPT partially avoided due to its tendency toward supportive language, causing a small mismatch in self-reported behaviors. Overall, these findings confirm that \pipeline{} enforces coherent persona dynamics even when smaller LLMs serve as the backbone.


\subsection{Summary of Ablation Findings}
\label{sec:app_summary_findings}

In summary, the ablation experiments demonstrate that:

\begin{itemize}[leftmargin=1em]
    \item \textbf{Knowledge Source:} Meeting \emph{naturalness} remains stable across structured (research papers) and unstructured (stories) inputs, though structured texts yield deeper, more cohesive scene discussions.
    \item \textbf{Backbone Models:} Mid-scale LLMs (Gemini, DeepSeek, Llama) produce slightly shorter transcripts but maintain strong authenticity and consistent role behaviors, indicating \pipeline{}’s flexibility across model scales.
    \item \textbf{Editing Stage:} Stage 7 is rarely essential for correctness but provides a valuable final polish, mitigating repeated language and formal tones that might expose synthetic origins.
    \item \textbf{Role Consistency:} Participants consistently adhere to their assigned personas and behaviors, as evidenced by both manual turn-based checks and the TREO questionnaire alignment.
\end{itemize}

Collectively, these findings underscore \pipeline{}’s robustness: it adapts to diverse inputs, model scales, and persona assignments while preserving conversational quality. Further details, including complete transcripts, chain-of-thought logs, and extended annotations, are available as per \Cref{sec:Data_Availability}.




\subsection{Summary of Findings}
\label{sec:app_summary_findings}
Overall, these ablation experiments demonstrate that:
\begin{itemize}[leftmargin=1em]
    \item \pipeline{} is robust to various knowledge sources, producing consistent authenticity scores yet deeper or shallower discussions based on how structured the source text is.
    \item Even mid-size LLMs can drive \pipeline{} effectively, though transcripts may be shorter and slightly less interactive than with GPT-4o.
    \item Stage 7 (editing) is seldom essential but serves as a valuable final polish, particularly for smaller models prone to repetitive filler or overly formal language.
    \item Role and behavior assignments remain coherent, evidenced by both manual annotation and psychological questionnaire alignment.
\end{itemize}

Collectively, these results underscore \pipeline{}’s flexibility: it adapts to different input structures and model scales while preserving role consistency and achieving high conversational naturalness. 
Additional data and logs are provided as per \Cref{sec:Data_Availability}.

