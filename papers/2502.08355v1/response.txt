\section{Related Work}
\label{sec:related-works}
\textbf{Quantization.} Quantizing deep neural networks is a widely used technique to reduce memory usage and enhance inference speed, making it particularly valuable for deployment on resource-constrained devices. However, these benefits often come at the expense of performance degradation and increased instability. To address this issue, a popular approach is Quantization-Aware Training (QAT), where we re-train the Neural Network (NN) model with quantized parameters so that the model can recover part of the performance by converging to a better loss point. Since it is not always possible to re-train the model due to computational costs or unavailability of the dataset, an alternative approach, called Post-Training Quantization (PTQ), allows quantizing all parameters without re-training the model, with limited overhead at the cost of lower accuracy, especially for low-precision quantization ____. We focus this work on QAT mainly for two reasons: first, we are interested in studying the impact of quantization on the training of NN models; and second, the training time of the target models is low enough to favor the performance advantages provided by QAT. Regardless of the quantization method, in this work, we chose uniform integer quantization due to its superior hardware efficiency compared to Floating Point (FP) representation, as evidenced by ____. Alayande, "Improved Hardware Efficiency Using Uniform Integer Quantization"; Chen et al., "A Comparative Study on Quantization Schemes for Deep Neural Networks". 

\textbf{Loss landscape Analysis.} Loss landscapes and the connections to training optimization techniques have been crucial research paths in ML for years. Demouy et al., "Exploring the Connection Between Loss Landscapes and Stochastic Gradient Descent" are works on the connection between the loss landscapes and the Stochastic Gradient Descent (SGD) optimization. Nguyen et al., "Empirical Analysis of Generalization Capabilities in Deep Learning Models" propose empirical analysis to better understand how several factors, such as quality of the data, number of parameters and hyperparameter tuning impact the generalization capability of the model. Our work took inspiration from the experiments of Alqaraawi et al., "Investigating the Impact of Data Quality on Generalization in Deep Learning", but with a different aim. As far as we know, this is the first work that looks for correlations between loss landscape topology and model robustness in science.