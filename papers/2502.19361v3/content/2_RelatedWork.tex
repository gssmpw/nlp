
\section{Related Works}




\noindent\textbf{Test-time Scaling.}
Recently, many works have begun to explore the test-time scaling techniques~\citep{openai-o1,snell2024scaling},
% Recent research has shifted away from merely expanding model parameters and training datasets towards exploring test-time scaling techniques. This approach
can greatly enhance performance by increasing the number of generated tokens. Several  methods~\citep{Guan2025rStarMathSL, chen2024alphamath, hao2023reasoninglanguagemodelplanning, yao2023treethoughtsdeliberateproblem}  have developed the tree search methods to improve reasoning capabilities. 
% Recently, 
In addition,
o1-like models (e.g., o1, R1)~\citep{openai-o1, guo2025deepseek, Team2025KimiKS} have investigated the reinforcement learning methods to generate long CoTs and enhance the model performance. Some methods have also begun exploring the intrinsic mechanisms of generating long CoTs~\citep{Yeo2025DemystifyingLC, Wu2024ACS}.
% These often incorporate human-like problem-solving strategies such as self-reflection, verification, and backtracking. This novel training paradigm not only introduces a new form of training data but also enhances the LLMs' reasoning abilities. Our study demonstrates that these extended CoT sequences effectively elicit the innate reasoning capabilities of LLMs.

% Instead of focusing on scaling model parameters and training data~\citep{kaplan2020scalinglawsneurallanguage}, 
% recent work has shifted to exploring test-time scaling~\citep{openai-o1,snell2024scaling}, i.e., increasing the number of tokens to improve performance. This can be achieved by augmenting LLMs with methods such as parallel sampling~\citep{brown2024largelanguagemonkeysscaling, wang2022self, Li_2022} or symbolic tree search~\citep{hao2023reasoninglanguagemodelplanning, chen2024alphamath, yao2023treethoughtsdeliberateproblem} to enhance reasoning ability. Furthermore,~\citet{openai-o1, guo2025deepseek} explore training LLMs using reinforcement learning to generate long CoT, which often include self-reflection, verification, and backtrackingâ€”processes commonly employed by humans when solving complex problems. This approach not only innovates the training paradigm for LLMs but also provides a new form of training data to augment their reasoning ability. Our work demonstrates that this long CoT exhibits high-quality characteristics in eliciting the inherent reasoning abilities of LLMs.

\noindent\textbf{Process Reward Modeling.} 
PRMs demonstrate a significant advantage over traditional outcome-level reward models (ORMs) in enhancing the accuracy of process reasoning~\citep{Lightman2023LetsVS}. The development of an increasing number of PRMs~\citep{Zhang2025TheLO, Xia2024EvaluatingMR, skyworkopeno12024} offers valuable contributions to multi-step reasoning. In addition, the introduction of numerous human-annotated process-level datasets~\citep{cobbe2021gsm8k, Wu2023FineGrainedHF, Li20242DDPOSD} provides essential resources for research. These advancements have collectively spurred exploration in the research direction of generating long CoT.


\noindent\textbf{LLM Critic.}
 % As many works have begun to explore the test-time scaling~\citep{openai-o1,snell2024scaling},
 The critique capabilities of LLMs have drawn great interests~\citep{Zhang2025CodeCriticBenchAH,liu2025air}. For example, CriticBench \citep{luo2023critique,zheng2024critic} uses LLMs to generate critiques and binary verdicts for input solutions, measuring accuracy by comparing these verdicts to ground truth labels.
 CriticEval \citep{lan2024criticeval} evaluates both feedback and correction qualities.
 % through prediction scores and the effectiveness of suggested revisions.
 Additionally, many works have explored the LLMs' self-critique for improving reasoning~\citep{tyen2023llms,stechly2024self}. 
 For example,
 \citet{huang2023large} have highlighted the challenges in self-correction without external feedback.
 ProCo~\citep{wu2024large} facilitates self-correction and iterative verification processes for better critical abilities. 
 % Interestingly, \citet{tyen2023llms} uncovered that while LLMs may struggle to independently identify reasoning errors, they excel at correcting them when the error locations are explicitly pointed out.
% These advancements in benchmarking and understanding LLMs' critique abilities contribute significantly to the ongoing development and refinement of AI systems capable of sophisticated reasoning and self-improvement.

% Several efforts have been made to advance the benchmarking of LLMs' critique abilities \citep{luo2023critique, lin2024criticbench, lan2024criticeval}. 
% Significant efforts have been made to benchmark the critique capabilities of LLMs. For instance, CriticBench \citep{luo2023critique,zheng2024critic} requires LLMs to generate critiques accompanied by binary verdicts (C+V) for input solutions, where the verdicts predict whether the solutions are correct or incorrect. The quality of the critiques is then evaluated by comparing these verdicts to ground truth labels, providing a measure of accuracy.  CriticEval \citep{lan2024criticeval} conducts a comprehensive evaluation of LLMs' critique abilities, introducing two key dimensions: feedback and correction. The feedback dimension assesses the quality of the analysis within the critique by assigning a prediction score, while the correction dimension evaluates the quality of revisions based on the critique's analysis. 
% Understanding LLMs' ability to self-critique and improve their reasoning has emerged as a critical research direction. \citet{huang2023large} demonstrated that LLMs often struggle with self-correction without external feedback, while \citet{stechly2024self} found that external verification from independent reasoners can significantly improve reasoning accuracy. \citet{wu2024large}'s ProCo framework showed promise in enabling self-correction through minimal prompting and iterative verification. \citet{tyen2023llms} revealed an interesting dynamic: while LLMs may struggle to detect reasoning errors independently, they can effectively correct them when error locations are explicitly identified.

