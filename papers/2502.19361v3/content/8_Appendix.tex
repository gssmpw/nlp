\clearpage
\onecolumn
\appendix
\onecolumn

\section{Details of Dataset Construction}
\label{app: dataset}

\subsection{Data Sources}
\label{app: data_source}

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{c|l}
        % \hline
        \toprule
        \textbf{Domain} & \textbf{Source} \\ 
        % \hline
        \midrule
        Math & \begin{tabular}[c]{@{}l@{}} 
                   MATH-500~\citep{Lightman2023LetsVS}, OlympiadBench~\citep{He2024OlympiadBenchAC},\\ Omni-MATH~\citep{Gao2024OmniMATHAU}, AIME, AMC23, CollegeMath
                   % \footnote{\usepackage{https://github.com/QwenLM/Qwen2.5-Math}}
               \end{tabular} \\ 
               % \hline
               \midrule
        Programming & \begin{tabular}[c]{@{}l@{}} 
                          CodeForce, BigCodeBench~\citep{zhuo2024bigcodebench}, LiveCodeBench~\citep{Jain2024LiveCodeBenchHA}, \\ 
                          FullStackBench~\citep{Cheng2024FullStackBE}, NaturalCodeBench~\citep{Zhang2024NaturalCodeBenchEC},\\ USACO~\citep{Shi2024CanLM} 
                      \end{tabular} \\
                      % \hline
                      \midrule
        PCB & \begin{tabular}[c]{@{}l@{}} 
                  GPQA~\citep{Rein2023GPQAAG}, OlympiadBench~\citep{He2024OlympiadBenchAC}, \\ AGIEval~\citep{Zhong2023AGIEvalAH}
              \end{tabular} \\ 
              % \hline
              \midrule
        General Reason & \begin{tabular}[c]{@{}l@{}} 
                             ZebraLogicBench~\citep{Lin2025ZebraLogicOT}, KOR-Bench~\citep{Ma2024KORBenchBL}, \\
                             Geeksforgeeks Puzzles, CS Interview Questions, China Civil Service Exam Questions 
                         \end{tabular} \\ 
                         \bottomrule
                         % \hline
    \end{tabular}
    \vspace{+3mm}
    \caption{Query-related data source statistics. Parts without citations come from open internet resources.}
    \label{tab: query_source}
\end{table}
Table \ref{tab: query_source} shows the original data sources from which we extracted high-quality queries and their corresponding solutions, along with additional information (for code data, test cases were extracted).

% \begin{figure*}[!htbp]
% \centering
% % \vspace{-10mm}
% \includegraphics[width=\linewidth]{figures/appendix/crop_div_sections.pdf}
% % \vspace{-0.6cm}
% \caption{An example of dividing sections.}
% \label{fig: section_div}
% % \vspace{-4mm}
% % \vspace{-0.6cm}
% \end{figure*}

\subsection{Details on Dataset Annotation}
\label{app:anno}
\label{app: profile}

The average cost for annotating each data unit was approximately \textit{\$15}. 
The annotation process is organized into three phases, each with specific goals and criteria:

\paragraph{Initial Assessment.} 
Annotators initially verify the quality of the question and subsequently evaluate the correctness of the modelâ€™s final response.

\paragraph{Section-Level Evaluation.}
The long CoT is divided into sections, each corresponding to a specific sub-task, such as problem analysis, verification of calculation results, and summarization. This phase requires the annotator to check and annotate each section individually.
The annotation process and examples are shown in Figure \ref{fig: label_case}
% \begin{itemize}[leftmargin=4mm]
% \item Whether the current section introduces a new method or strategy attempt? 
%     Annotators identified whether a section attempted a new reasoning approach. If a new approach was attempted, the specific steps of the attempt were documented.
% \item Reasoning Usefulness: The usefulness of the reasoning approach was assessed. If the reasoning contributed meaningfully to the solution, it was marked as useful even if minor errors were present. Conversely, sections that were redundant or detrimental to correct reasoning were marked as useless.
% \item Error Detection: Annotators meticulously examined each section for errors. If errors were identified, the specific step(s) where they occurred were documented, along with a brief explanation of the error and the correct reasoning or calculation.

% \item Reflective Corrections: Sections, where the model exhibited reflective corrections, were identified, and the initial step of reflection was recorded to assess the model's self-corrective capabilities.

% \end{itemize}

\paragraph{Quality Assurance and Validation.}
We have established a strict quality control process to ensure the high quality and consistency of annotations. Each data is assigned three initial annotators, two junior reviewers, and an additional five people who are responsible for overall spot checks. Outsourced personnel and external contractors responsible for annotations receive unified training. We regularly check the consistency and quality between annotators and repeatedly discuss and improve annotation protocols during the annotation process to make the standards more perfect. This process ensures the generation of high-quality annotations and minimizes subjective bias.


\paragraph{Profile of Annotation Persons.}

In this dataset annotation project, we engaged a diverse group of annotators through three distinct sources. We employed a set of external contractors directly recruited by us and collaborated with two additional suppliers to provide annotation services. The dataset was divided into three parts, with certain sections deliberately overlapping to facilitate cross-validation. This overlap allows us to compute the annotation consistency rate, and if the results do not meet the required standards, revisions are necessitated.

Our annotator pool is composed of highly qualified individuals: 23 Master's degree holders and 6 Ph.D. holders in Mathematics, 18 Master's graduates in Computer Science, 7 Master's and 2 Ph.D. graduates in Physics, 7 Master's and 3 Ph.D. graduates in Chemistry, and 6 Master's and 2 Ph.D. degree holders in Biology. We employ a rotational system to randomly assign two individuals from each academic field to serve as reviewers. Additionally, 5 algorithm specialists are tasked with conducting spot checks on the annotations. This meticulous selection and review process ensures superior data quality and reliability for subsequent analyses.


\subsection{Details on Assessment}
\label{app:asses}

\paragraph{Math} For mathematical queries, we employ a combination of rules and LLMs to evaluate the correctness of the provided solutions. Rule-based systems verify the validity of numerical calculations, while LLMs ensure that reasoning steps adhere to established mathematical principles. This dual approach guarantees high accuracy in error detection within the solutions.
\paragraph{Programming} For programming tasks, we utilize sandbox testing environments alongside LLM-based evaluations.  Specifically, we utilize SandboxFusion~\citep{Cheng2024FullStackBE} as our testing environment. The solution is initially executed in the sandbox environment. Subsequently, the test case, the sandbox environment's feedback output, and the code are provided to the LLM to determine the correctness of the answer.
\paragraph{PCB} Due to the straightforward nature of answers in these domains, we exclusively rely on LLM judgments, which offer high accuracy in assessing correctness.
\paragraph{General Reasoning} Similarly, for general reasoning questions, LLM judgments are employed to effectively and accurately assess solution validity.


\subsection{Findings in Data Preprocessing}
\label{app: data_preprocess}
During the data preprocessing stage, we identified several issues with the data collected from open-source datasets. These issues included incomplete queries, incorrect solutions, and excessively high query similarity. To address these problems, we applied a combination of manual review and LLM (Large Language Model) validation to filter out low-quality data. Additionally, for code data specifically, we observed that different sources and types of data sometimes included test cases, while others did not, and the formats of these test cases were inconsistent. To tackle these inconsistencies, we used GPT-4 to filter the data for quality and to extract test cases, standardizing them into executable code for SandboxFusion. This allowed us to conduct uniform sandbox verification to ensure data accuracy.

\subsection{Sections Division}
\label{app: section_div}


\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/longcot_steps_dist.pdf}
        \caption{Distribution of the number of steps in long CoT(divided by "\texttt{\textbackslash n\textbackslash n}").}
        
        \label{fig:longcot_origin_steps_dist}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/longcot_steps_per_section_dist.pdf}
        \caption{Distribution of the number of steps contained in each divided section.}
        \label{fig:section_step_dist}
    \end{subfigure}
    \caption{Statistical distribution of steps in long CoT.}
    \label{fig:longcot_steps_dist}
\end{figure}


Figure \ref{fig: section_div} illustrates the prompt for dividing sections along with examples of the resulting divisions. Several steps involved in addressing an atomic problem or exploring an idea are grouped into the same section. The specific outcome of the division is influenced by various factors, such as the task domain. However, compared to a purely long CoT, this approach is more user-friendly for human annotation.

Furthermore, to prevent sections from becoming overloaded with too many steps, which would increase the complexity of the annotation process, we iteratively divide sections that exceed $50$ steps. Figure \ref{fig:longcot_steps_dist} displays the distribution of steps in the original long CoT (subfigure \ref{fig:longcot_origin_steps_dist}) and the distribution of steps in each divided section (subfigure \ref{fig:section_step_dist}). Before sectioning, annotators are required to review each individual step, which can be exceedingly challenging for long CoTs with numerous steps. By dividing the sections, annotators can proceed on a section-by-section basis, making the process more comprehensible and significantly reducing the difficulty of annotation.


\subsection{Validation of Long CoT Correctness}
\label{app: correct_assess}

\begin{table}[!ht]
    \centering
    \begin{tabular}{c|ccc}
    \hline
        Domain & Filtered Total Num & Correct Num & Accuracy(\%) \\ \hline
        Math & 7534 & 5413 & 71.84 \\ 
        Programming & 2103 & 1276 & 60.67 \\ 
        PCB & 4517 & 2626 & 58.13 \\
        General Reasoning & 1981 & 1160 & 58.56 \\ \hline
    \end{tabular}
    \vspace{+3mm}
    \caption{Accuracy statistics for generated long CoT responses for filtered high-quality queries.}
    \label{tab: longcot_acc}
\end{table}

For the filtered high-quality queries, we use a mix of various o1-like models, including QwQ-32B-Preview, DeepSeek-R1, and  Gemini 2.0 Flash Thinking, to generate the corresponding long CoT. We then use LLM-as-a-judge and a sandbox testing environment to validate the accuracy of the long CoT generated by these o1-like models, obtaining the native erroneous long CoT for subsequent human annotation.

Table \ref{tab: longcot_acc} shows the accuracy of the generated long CoT. It can be seen that o1-like models enhanced with reinforcement learning in math and programming perform slightly better in these two areas compared to general reasoning and PCB.


\subsection{Statistics on Category Distribution}
\label{app: category_distribution}


\begin{table}[!ht]
    \centering 
    \begin{tabular}{llc}
        % \hline
        \toprule
        \textbf{Domain} & \textbf{Subcategory} & \textbf{Number} \\ 
        % \hline
        \midrule
        \multirow{7}{*}{Math} 
        & Discrete Mathematics & 144 \\ 
        & Number Theory & 104 \\ 
        & Geometry & 101 \\ 
        & Others & 74 \\ 
        & Calculus and Analysis & 58 \\ 
        & Statistics and Other Decision Science & 45 \\ 
        & Algebra & 36 \\
        % & total & 562 \\
        % \hline
        \midrule
        \multirow{7}{*}{Programming} 
        & Basic Programming & 133 \\ 
        & Mathematics & 86 \\ 
        & Advanced Programming & 48 \\ 
        & Data Analysis & 41 \\ 
        & Desktop and Web Development & 27 \\ 
        & Others & 24 \\ 
        & Software Engineering & 14 \\ 
        % & total & 373 \\
        % \hline
        \midrule
        \multirow{3}{*}{PCB} 
        & Chemistry & 64 \\ 
        & Physics & 63 \\ 
        & Biology & 27 \\ 
        % & total & 154 \\
        % \hline
        \midrule
        \multirow{7}{*}{General Reasoning} 
        & Logical Reasoning & 56 \\ 
        & Symbolic Reasoning & 28 \\ 
        & Quantitative Reasoning & 24 \\ 
        & Strategic Reasoning & 12 \\ 
        & Common Sense Reasoning & 9 \\ 
        & Spatio-temporal Reasoning & 9 \\ 
        & Others & 9 \\
        % \hline
        \midrule
        % & total & 147 \\ \hline
        & Total & 1236 \\
        % \hline
        \bottomrule
    \end{tabular}
    \vspace{+3mm}
    \caption{Detailed categories of DeltaBench and corresponding data volume statistics.}
    \label{tab: data_category}
\end{table}

Table \ref{tab: data_category} shows the subcategories and corresponding data volumes of DeltaBench across various domains. In obtaining queries and annotations, we strive to ensure balance across categories while also balancing annotation difficulty and accuracy.

% \subsection{Analysis of Long CoT Lengths}
% \label{app: long_CoT_length}


% \begin{figure*}[!htbp]
%     \centering
%     \includegraphics[width=0.85\textwidth]{figures/longcot_length_dist.pdf}
%     % \vspace{-10pt}
%     \caption{The distribution of LongCOT Length.}
%     \label{fig: longcot_length_dist}
% \end{figure*}

% Figure \ref{fig: longcot_length_dist} shows the length distribution of long CoT in DeltaBench, with an overall average length of approximately $4.4k$ tokens. The distribution is relatively uniform between $0$ and $12k$ tokens, allowing for a comprehensive evaluation of the performance of PRMs or critic models across different lengths.


% \subsection{Error Frequency}
% \label{app: error_freq}



\subsection{Analysis of Other Evaluation Metrics}
\label{app: other_evaluation}

\FloatBarrier
\begin{table}[!h]
\centering
\resizebox{0.8\textwidth}{!}{ 
\begin{tabular}{cccc}
\toprule
Model & F1-Score & First Error Acc. & Any Error Acc. \\
\midrule
GPT-4-turbo-128k & 40.76 & 57.04 & 69.17 \\
GPT-4o & 30.85 & 36.89 & 50.89 \\
DeepSeek-V3 & 27.33 & 31.72 & 42.39 \\
Qwen2.5-32B-Instruct & 26.73 & 30.58 & 42.23 \\
DeepSeek-R1 & 28.43 & 29.94 & 40.78 \\
Qwen2.5-7B-Instruct & 18.63 & 22.25 & 30.74 \\
GPT-3.5 & 7.98 & 6.15 & 11.65 \\
\bottomrule
\end{tabular}
}
 \vspace{+3mm}
\caption{The table compares different accuracy metrics for each model. 'First Error Acc.' is the accuracy in identifying the first error, and 'Any Error Acc.' is the accuracy in detecting any error.}
\label{tab:accuracy_comparison}
\end{table}
\FloatBarrier

% \paragraph{Analysis of other evaluation metrics.}
In Table ~\ref{tab:accuracy_comparison}, we present the performance of several models across different accuracy metrics: F1-Score, First Error Accuracy, and Any Error Accuracy. These metrics evaluate the models' ability to identify the first error and detect any error within a given sequence. A key observation is that the relative rankings of the models across the First Error Accuracy and Any Error Accuracy metrics closely align with their F1-Score. This consistency across different evaluation measures highlights the robustness of the F1-Score as a comprehensive indicator of model performance and suggests a strong correlation between the ability to detect the first error and the ability to identify any error in the sequence. Additionally, GPT-4-turbo consistently outperforms the other models, regardless of the evaluation metric used. Its Any Error Accuracy reaches 69\%, significantly higher than the other models in the comparison. This finding underscores the modelâ€™s superior performance in error recognition, yet it also points to the limitations that remain in current LLMs. 


\begin{table}[!ht]
    \centering
    % \scriptsize
    \small
    \begin{tabular}{cccccc}
    % \hline
    \bottomrule
        Model & Quantile & Threshold & prec & recall & F1 \\
        % \hline
        \midrule
        % Qwen/Qwen2.5-Math-PRM-7B & 5\% & 0.2168 & 26.91 & 9.38 & 13.91 \\
        % Qwen/Qwen2.5-Math-PRM-72B & 5\% & 0.2119 & 29.57 & 10.3 & 15.28 \\ 
        % RLHFlow/Llama3.1-8B-PRM-Deepseek-Data & 5\% & 0.2021 & 7.21 & 2.33 & 3.52 \\ 
        % RLHFlow/Llama3.1-8B-PRM-Mistral-Data & 5\% & 0.2949 & 5.68 & 1.98 & 2.94 \\
        % Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B & 5\% & 0.0303 & 21.05 & 7.36 & 10.91 \\
        % Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B & 5\% & 0.0278 & 24.65 & 8.62 & 12.77 \\ \hline
        Qwen/Qwen2.5-Math-PRM-7B & 5\% & 0.2168 & 39.81 & 73.86 & 46.48 \\ 
        Qwen/Qwen2.5-Math-PRM-72B & 5\% & 0.2119 & 33.51 & 65.11 & 40.44 \\ 
        RLHFlow/Llama3.1-8B-PRM-Deepseek-Data & 5\% & 0.2021 & 24.19 & 56.1 & 30.88 \\ 
        RLHFlow/Llama3.1-8B-PRM-Mistral-Data & 5\% & 0.2949 & 23.18 & 51.46 & 29.68 \\ 
        Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B & 5\% & 0.0303 & 19.48 & 46.76 & 24.45 \\ 
        Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B & 5\% & 0.0278 & 18.68 & 46.14 & 23.46 \\ 
        % \hline
        \bottomrule
    \end{tabular}
     \vspace{+3mm}
    \caption{Performance of PRMs using the overall reward quantile as the threshold.}
\label{tab:prm_quantile}
\end{table}

For PRMs, aside from outlier detection, we also experimented with evaluating using a fixed threshold based on quantiles. Specifically, we used the ascending 5\% quantile of all rewards on DeltaBench as the threshold, considering sections below this value as incorrect. The evaluation results are shown in table \ref{tab:prm_quantile}. However, compared to outlier detection, we found that this approach overestimates the performance of PRMs. This is because using a quantile as a threshold effectively forces PRMs to consider a fixed proportion of sections as incorrect.

% ------


% \subsection{More Related Works}
\section{Error Classification}
\label{app: error_classification}

\begin{figure*}[t]
\centering
\vspace{-10mm}
\includegraphics[width=0.95\linewidth]{figures/error_category.pdf}
% \vspace{-0.6cm}
\caption{Categories of Errors in o1-like Models.}
\label{fig: error_category}
\vspace{-4mm}
% \vspace{-0.6cm}
\end{figure*}

In Figure \ref{fig: error_category}, we conclude a detailed error classification based on human annotations of the errors contained in the modelâ€™s answers.

% \begin{table}[h]
%     \centering
%     \begin{tabular}{p{3cm}|p{3cm}|p{9cm}}
%         \hline
%         \textbf{Aspect} & \textbf{Category} & \textbf{Description} \\
%         \hline
%         Understanding Error & Problem Misunderstanding & Incorrect interpretation of the problem requirements or context at the initial comprehension stage \\
%         \cline{2-3}
%          & Conceptual Misunderstanding & Misunderstanding of basic principles or theoretical foundations \\
%         \hline
%         Knowledge Errors & Factual Error & Incorrect recall or statement of established facts or constants \\
%         \cline{2-3}
%                       & Theorem Error & Incorrect recall or application of mathematical theorems or scientific laws \\
%         \cline{2-3}
%                       & Definition Error & Incorrect understanding or application of standard definitions or terminology \\
%         \hline
%         Logical Errors & Strategy Error & Inappropriate selection of problem-solving approaches or methodologies \\
%         \cline{2-3}
%                      & Reasoning Error & Flaws in the logical flow of problem-solving steps \\
%         \cline{2-3}
%                      & Premise Error & Invalid or incorrect assumptions in the reasoning process \\
%         \cline{2-3}
%                      & Consistency Error & Contradictions or inconsistencies in the logical framework \\
%         \hline
%         Calculation Errors & Numerical Error & Inaccuracies in arithmetic operations or numerical computations \\
%         \cline{2-3}
%                            & Formula Error & Incorrect application or manipulation of mathematical formulas \\
%         \cline{2-3}
%                            & Parameter Error & Mistakes in handling variables or parameters \\
%         \cline{2-3}
%                            & Unit Error & Incorrect use or conversion of measurement units \\
%         \hline
%         Programming Errors & Syntax Error & Violations of programming language rules and conventions. Including incorrect code structure, missing delimiters, or improper statement construction \\
%         \cline{2-3}
%                            & Function Error & Deficiencies in function implementation, usage, or behavior. Including improper function definitions, incorrect function calls, inappropriate return values, or unexpected function behaviors \\
%         \cline{2-3}
%                            & Data Type Error & Incorrect handling of data types and type conversions. Including type mismatches, improper type conversions, or inappropriate data structure usage \\
%         \hline
%         Formal Errors & Symbol Error & Incorrect use of mathematical or scientific symbols \\
%         \cline{2-3}
%                       & Formatting Error & Improper presentation or organization of solution structure \\
%         \hline
%         Completeness Errors & Boundary Omission & Failure to consider edge cases or limiting conditions \\
%         \hline
%         Special Cases & Reflection Error & Insufficient analysis of solution validity or limitations \\
%         \cline{2-3}
%                       & Summary Error & Inadequate synthesis or conclusion of findings \\
%         \cline{2-3}
%                       & Hallucination & Generation of false or unsupported information \\
%         \cline{2-3}
%                       & Redundancy & Unnecessary repetition or superfluous information in solutions \\
%         \hline
%     \end{tabular}
%     \caption{Classification of Errors in Problem Solving}
% \end{table}






% \FloatBarrier
% \begin{figure}[!htbp]
% \centering
% %\vspace{-4mm}
% \begin{subfigure}[b]{0.48\linewidth}
%   \includegraphics[width=\linewidth, height=4cm]{figures/action_role.pdf}
%   \caption{Distribution of Action Roles in the depth degree of long CoT.}
%   \label{fig: section_role}
% \end{subfigure}
% \hfill
% \begin{subfigure}[b]{0.48\linewidth}
%   \includegraphics[width=\linewidth, height=4cm]{figures/wrong_types.pdf}
%   \caption{Distribution of Error Causes for Each Model.}
%   \label{fig: wrong_types}
% \end{subfigure}
% %\vspace{-4mm}
% \label{fig: combined_figures}
% \end{figure}
% % \caption{Distribution of error causes for each model.}
% \FloatBarrier
% \FloatBarrier
% \begin{figure*}[!h]
% \centering
% \includegraphics[width=0.6\linewidth]{figures/action_role.pdf}
% % \vspace{-0.6cm}
% \caption{Distribution of Action Roles throughout the Long CoT process.}
% \label{fig: apps}
% % \vspace{-0.6cm}
% \end{figure*}
% \FloatBarrier



% \section{Error Analysis of o1-like Models}
% \label{app: error_analysis}


% In this analysis, we find the following common errors across domains:

% \begin{itemize}[leftmargin=4mm]
% \item \textbf{Math:} The most frequent error type is \textit{Reasoning Error}, followed by \textit{Understanding Error} and \textit{Calculation Error}. This indicates that while the models often struggle with logical reasoning and problem comprehension, low-level computational mistakes also remain a significant issue. Additionally, \textit{Strategy Error} suggests difficulties in formulating effective problem-solving approaches.
% % \begin{tcolorbox}[colback=yellow!5!white, colframe=yellow!75!black, title=Key Finding for Math]
% % Models significantly struggle with reasoning and comprehension, with low-level computational errors also prevalent.
% % \end{tcolorbox}

% \item \textbf{Programming}: 
% \textit{Reasoning Error} (21.5\%) is the most common, followed by \textit{Formal Error} (16.7\%) and \textit{Understanding Error} (12.6\%). The high frequency of \textit{Formal Error} (e.g., syntax or data type mistakes) and \textit{Programming Error} (11.8\%) underscores the models' struggles with code-specific details and implementation. 

% % \begin{tcolorbox}[colback=yellow!5!white, colframe=yellow!75!black,  title=Key Finding for Programming]
% % Errors in programming and formal specifics indicate difficulties with detailed implementation.
% % \end{tcolorbox}


% \item \textbf{PCB}: 
% The dominant error types are \textit{Understanding Error} (20.4\%) and \textit{Knowledge Error} (17.3\%), closely followed by \textit{Reasoning Error} (17.3\%). This suggests that the main challenge for current models in the fields of physics, chemistry and biology is to understand field-specific concepts and accurately apply relevant knowledge.
% % \begin{tcolorbox}[colback=yellow!5!white, colframe=yellow!75!black,  title=Key Finding for PCB]
% % Understanding domain-specific concepts and applying knowledge accurately are the major hurdles.
% % \end{tcolorbox}


% \item \textbf{General Reasoning}: Similar to Mathematics, \textit{Reasoning Error} is the most prevalent, accounting for 43\%, followed by comprehension errors, accounting for 19\%. It indicates that the challenges in this domain are more abstract and centered around logical reasoning.

% % \begin{tcolorbox}[colback=yellow!5!white, colframe=yellow!75!black, title=Key Finding for General Reasoning]
% % Reasoning errors account for nearly half of all errors, revealing that abstract logical reasoning is the primary bottleneck in this domain.
% % \end{tcolorbox}

% \end{itemize}


% We also analyzed errors specific to individual models, providing further insights into model weaknesses:

% \begin{itemize}[leftmargin=4mm]

% \item \textbf{DeepSeek-R1} exhibits its most pronounced weakness in \textit{Reasoning Errors} (22.7\%), indicating challenges in constructing coherent and accurate logical reasoning paths. However, DeepSeek-R1 demonstrates relative strength in handling fundamental tasks, with minimal \textit{Calculation Errors} (3.1\%) and \textit{Programming Errors} (4.4\%), reflecting its ability to maintain precision in detailed computations.

% % \begin{tcolorbox}[colback=blue!5, colframe=blue!50!black, title=Key Finding for DeepSeek-R1]
% %   DeepSeek-R1 exhibits strength in precise calculations but struggles significantly with reasoning.
% % \end{tcolorbox}


% \item \textbf{QwQ-32B-Preview} demonstrates a relatively balanced performance but is notably weak in \textit{Calculation Errors} (17.9\%), indicating a significant limitation in numerical precision. It also shows a moderate frequency of \textit{Understanding Errors} (17.1\%), suggesting occasional difficulties in problem interpretation. 

% % \begin{tcolorbox}[colback=blue!5, colframe=blue!50!black, title=Key Finding for QwQ-32B-Preview]
% %   QwQ-32B-Preview has a marked limitation in numerical precision.
% % \end{tcolorbox}

% \end{itemize}


\section{Analysis of Underperforming Model}
\label{app: underperforming}

\FloatBarrier
\begin{figure*}[!htbp]
\centering
\vspace{-10mm}
\includegraphics[width=0.6\linewidth]{figures/wrong_types.pdf}
% \vspace{-0.6cm}
\caption{Distribution of error causes for each model.}
\label{fig: wrong_types}
% \vspace{-0.6cm}
\end{figure*}
\FloatBarrier

We classify the errors produced by underperforming models into three categories: (1) Not following instructions, where models fail to follow the critique instructions; (2) Overconfidence in Correctness, where models incorrectly assume the sample contains no errors; (3) Other errors, such as false negatives, where models incorrectly flag accurate reasoning as erroneous, and error misidentification, where models inaccurately determine the type or location of errors.

As illustrated in Figure ~\ref{fig: wrong_types}, the following observations can be made: (1) DeepSeek-R1-Distill series exhibit significant instruction-following deficiencies. These models tend to directly answer questions rather than critically evaluate the correctness of responses, indicating a potential overfitting problem. (2) o1-like models like DeepSeek-R1 and o1-preview-0912 also demonstrate notable instruction-following challenges, albeit to a lesser extent than GPT-4o. (3) A notable trend among the o1-preview and  Gemini 2.0 Flash Thinking models was their tendency to assume that samples were correct without thorough evaluation. This overconfidence in correctness was a significant contributor to their error rates. In contrast, GPT-4o demonstrates superior performance, with no instances of "Instructions not followed" errors and a relatively low number of "Overconfidence in Correctness" errors.