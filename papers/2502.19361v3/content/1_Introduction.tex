
\section{Introduction}

Large Language Models (LLMs) have witnessed remarkable progress in recent years~\citep{dubey2024llama3, he2024chinese, li2024graphreader,zhang2024mapneo}. Among these advancements, o1-like LLMs have emerged to greatly improve reasoning capabilities by generating long Chain-of-Thought (CoT) reasoning steps~\citep{openai-o1,guo2025deepseek}.

% The concept of Chain-of-Thought reasoning has garnered considerable attention in the AI community due to its potential to improve the problem-solving and logical deduction abilities of language models. By breaking down complex problems into smaller, more manageable steps, CoT reasoning allows models to tackle intricate tasks with greater accuracy and transparency.

However, despite the growing popularity of o1-like models and their long CoT reasoning approaches, \textit{systematic evaluation of the quality and effectiveness of the generated reasoning chains is not well investigated},
which poses a significant challenge in understanding the capabilities and limitations of these models~\citep{Chen2024DoNT,Yeo2025DemystifyingLC}.
Besides, as LLMs continue to evolve, further improving their performance has become increasingly challenging. One of the key areas of focus in this regard is the development of critique ability,
which measures the capability to provide detailed analysis, constructive suggestions, and refinement feedback suggestions for solutions generated by other models or even themselves~\citep{lan2024criticeval,lin2024criticbench,judgebench,prmbench,Zheng2024ProcessBenchIP}. However, \textit{evaluating the critique abilities of existing LLMs (e.g., critic models or process reward models) on the long CoT reasoning steps has not been explored}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/intro_longcot.pdf}
    \caption{Illustration of the evaluation process for critic models and Process Reward Models (PRMs) for DeltaBench.}
    \label{fig: intro}
\end{figure*}
Therefore, to address the aforementioned challenges, we introduce \textbf{DeltaBench}, 
the first dataset to analyze the qualities of the long CoTs generated by o1-like models and evaluate the critique abilities to \textbf{D}etect \textbf{E}rror in \textbf{L}ong Co\textbf{T} Re\textbf{A}soning of existing critic models and PRMs in Figure \ref{fig: intro}.
Specifically,
in DeltaBench, we first collect a diverse collection of long CoTs generated by various o1-like models (i.e., QwQ, DeepSeek-R1, and Gemini 2.0 Flash Thinking) across different reasoning tasks such as \textbf{Math}, \textbf{Programming}, \textbf{PCB} (physics, chemistry and biology), and \textbf{General Reasoning}.
Then, we divide each long CoT into different sections, where each section denotes an independent subtask. After that, each section is annotated with corresponding labels (e.g., reasoning usefulness, reasoning correctness, and reflection).

% Based on DeltaBench, we 

% Specifically, in our DeltaBench, we 

% and measure the critique ability to Detect Errors in Long CoT Reasoning for existing LLMs. 
% Specifically,

Based on DeltaBench,
% The primary objectives of our research are threefold:
we first conduct a fine-grained analysis on the efficiency of the generated long CoTs from different o1-like models, and have the following interesting findings:

\begin{itemize}[left=1em]

\item \textbf{Fundamental errors (e.g.,{calculation errors, syntax errors and format errors}) are usually existed in existing o1-like models.}
For example, such errors account for approximately 25\% and 23\% in QwQ-32B-Preview and Gemini 2.0 Flash Thinking,
respectively.

\item \textbf{The proportion of effective reflection is still very low}. Note that the ``effective reflection'' denotes this reflection leads to the right answer.
For example, on average, approximately 67.8\% of the reflections in the collected long CoT responses are useless.
% For the reflection mechanisms in o1-like models, despite frequent reflection, the proportion of effective reflection remains low.

\item \textbf{The long CoT reasoning process is very redundant for existing o1-like models.}
For example, on average, 27\% of the reasoning sections in the collected long CoT response are redundant.

% 20%, 30%

% For the reasoning efficiency of o1-like models, redundancy is also a significant issue.

\end{itemize}


After that, we evaluate the critique abilities of LLMs prompted as critic models and PRMs and draw the following insightful observations:

\begin{itemize}[left=1em]

\item \textbf{For existing LLMs and PRMs, the ability to effectively identify errors in long CoT reasoning is {very limited}}. For example, the top-performing model in DeltaBench, GPT-4-turbo-128k, achieves an F1-score of only 40.8\%.

\item \textbf{o1-like models (e.g., o1-mini, o1-preview) do not show any advantage over non-o1-like models on the critique abilities.} For example, the results of o1-preview are lower than the results of GPT-4o-mini.

\item \textbf{The self-critique abilities of existing o1-like models are generally weaker than their critique abilities on other o1-like models.}
For example, DeepSeek-R1 exhibits a 36\% reduction in self-critique performance compared to the critique performance of other o1-like models.
% especially in DeepSeek-R1.

\end{itemize}

% providing insights into the effectiveness and efficiency of different o1-like models.

% To evaluate existing process reward models (PRMs) and critic models in their ability to detect errors within each annotated reasoning process, thereby exploring the boundaries and limitations of current error detection mechanisms.

% To establish DeltaBench as a valuable resource for developers and researchers, enabling them to better understand the long CoT reasoning abilities of their models and contribute to the ongoing improvement of LLMs.

% By offering a comprehensive framework for assessing and comparing the reasoning capabilities of o1-like models, DeltaBench aims to bridge the gap between theoretical advancements and practical applications in the field of AI reasoning. This research not only contributes to the academic understanding of CoT reasoning but also has significant implications for the development and refinement of more robust and reliable AI systems.

% In the following sections, we will delve into the methodology behind DeltaBench, present our findings from the analysis of long CoTs, and discuss the implications of our research for the future of AI reasoning and language model development.