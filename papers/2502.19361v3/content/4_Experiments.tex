% \section{Experiments}

\section{Analysis}

\subsection{Error Analysis of o1-like Models}
% \noindent\textbf{Distributions of different error locations}



\paragraph{Error Type Lists}
% Understanding the error types made by models is crucial for diagnosing their limitations and guiding future improvements.
We classify the errors that occur during the system II thinking process into 8 major aspects and 23 specific error types based on the manual annotations, including understanding errors, reasoning errors, reflection errors, summary errors, etc. For detailed information about the error categories, see Appendix \ref{app: error_classification}.

\paragraph{What Are the Most Common Errors Across Domains?}

\begin{figure}[t]
    \centering
    \resizebox{1.0\textwidth}{!}
    {\includegraphics{figures/error_type_distribution.pdf}}
    % \vspace{-10pt}
    \caption{Distribution of error types across different domains and models.}
    % \vspace{-3mm}
    \label{fig: error_type}
\end{figure}

To analyze the characteristics of error distribution in different domains, we performed a uniform sampling of the data based on the model, the domain, and the query difficulty. Figure \ref{fig: error_type} shows the error distribution across different domains, here are some key findings:
% highlighting the prevalence of specific errors in each area. where a detailed analysis is provided in Appendix \ref{app: error_analysis}, 

\begin{itemize}[left=1em]
\item \textbf{Math:} The most frequent error type is \textit{Reasoning Error}(25.3\%), followed by \textit{Understanding Error}(15.7\%) and \textit{Calculation Error}(15.4\%). This indicates that while the models often struggle with logical reasoning and problem understanding, low-level computational mistakes also remain a significant issue.

\item \textbf{Programming}: 
\textit{Reasoning Error} (21.5\%) is the most common, followed by \textit{Formal Error} (16.7\%) and \textit{Understanding Error} (12.6\%). The high frequency of \textit{Formal Error} and \textit{Programming Error} (11.8\%) underscores the models' struggles with code-specific details and implementation. 

\item \textbf{PCB}: 
The dominant error types are \textit{Understanding Error} (20.4\%) and \textit{Knowledge Error} (17.3\%), closely followed by \textit{Reasoning Error} (17.3\%). This suggests that the main challenge for current models in the fields of physics, chemistry and biology is to understand field-specific concepts and accurately apply relevant knowledge.

\item \textbf{General Reasoning}: \textit{Reasoning Error} is the most prevalent, accounting for 43\%, followed by comprehension errors, accounting for 19\%, showing that logical reasoning is the primary bottleneck.

\end{itemize}

\paragraph{What Are the Model-Specific Error Patterns?}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/error_type_model.pdf}
%     % \vspace{-3mm}
%     \caption{Distribution of Error Types Across Models.}
%     % \vspace{-3mm}
%     \label{fig: error_type_model}
% \end{figure}

We also analyzed errors specific to individual models, providing further insights into model weaknesses, as illustrated in Figure \ref{fig: error_type_model}. The error distributions reveal distinct patterns for each model, highlighting their unique strengths and areas for improvement. Here are some key findings:
%Due to space constraints, we focus here on the key findings from the most commonly used models, with a comprehensive analysis of all models provided in Appendix \ref{app: error_analysis}.

\begin{itemize}[leftmargin=4mm]

\item \textbf{DeepSeek-R1} exhibits its most pronounced weakness in \textit{Reasoning Errors} (22.7\%), indicating challenges in constructing coherent and accurate logical reasoning paths. However, it demonstrates relative strength in handling fundamental tasks, with minimal \textit{Calculation Errors} (3.1\%) and \textit{Programming Errors} (4.4\%).

%achieves strong performance in detail-oriented tasks such as formula computation and code syntax. Its primary limitation lies in reasoning and comprehension capabilities.

\item \textbf{QwQ-32B-Preview} excels at identifying correct problem-solving approaches. However, its effectiveness is significantly hindered by deficiencies in handling finer details, particularly in \textit{Calculation Errors} (17.9\%)

%but its effectiveness is often undermined by deficiencies in handling finer details.

% {QwQ-32B-Preview} demonstrates a relatively balanced performance but is notably weak in \textit{Calculation Errors} (17.9\%), indicating a significant limitation in numerical precision. It also shows a moderate frequency of \textit{Understanding Errors} (17.1\%), suggesting occasional difficulties in problem interpretation. 

\end{itemize}

\begin{tcolorbox}[colback=white!95!gray, colframe=gray!70!black,  title=Key Finding for Error Type]
The primary bottleneck of current models remains reasoning ability. However, detailed errors like calculation and formal mistakes also contribute significantly.
\end{tcolorbox}


\subsection{Reflection Analysis of o1-like Models}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/reflection.pdf}
    \caption{Distribution of effective reflection times by models and domains on a sample level. The segments within each pie chart represent how many times effective reflection occurs in one sample, with segment `0' indicating there is no effective reflection.}
    \label{fig: error_type_model}
\end{figure}

\paragraph{Statistics.}
We also conduct a analysis of the total number of reflections and the proportion of effective reflections in the long CoT output of all questions (including questions answered correctly and incorrectly by the model). 
% On average, 
%We observe that the long CoT contains \textit{five} times reflections, indicating that current o1-like models tend to reflect frequently. 

\paragraph{How Effective Are Model Reflections Across Different Models and Domains?}
We classify samples with reflections based on the number of valid reflections to evaluate the ability to produce valid reflections. Specifically, we label samples as \texttt{0} if no valid reflections occur, and \texttt{1}, \texttt{2}, or \texttt{>=3} for samples with one, two, or three and more valid reflections, respectively(all statistical analyses were performed under strictly controlled conditions, ensuring uniform sampling and balanced tasks for a fair comparison). In Figure \ref{fig: error_type_model}, {DeepSeek-R1} exhibits the highest proportion of effective reflections, and the models show a notably higher rate of effective reflections in the {math} domain. However, the overall proportion of valid reflections across all models remains relatively low, ranging between 30\% and 40\%. This suggests that the reflection capabilities of current models require further improvement.
%Detailed statistical data can be found in Appendix D.

\begin{tcolorbox}[colback=white!95!gray, colframe=gray!70!black,  title=Key Finding for Reflection]
Despite frequent reflection attempts, the proportion of effective reflections remains low across models, and  DeepSeek-R1 achieves the highest rate of valid reflections.
\end{tcolorbox}

\subsection{Effective Reasoning of o1-like Models}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\textwidth]{figures/effetive_reasoning.pdf}
    \caption{Distribution of effective reasoning ratios.}
    
    \label{fig: effetive_reasoning}
\end{figure}

\paragraph{Statistics.} 
% As previously mentioned, 
Human annotators evaluate the usefulness of the reasoning in each section, enabling us to calculate the proportion of valid reasoning in each response. As illustrated in Figure \ref{fig: effetive_reasoning}, each graph shows the distribution of effective reasoning ratios for a particular model. The red dashed line in each graph indicates the average effective reasoning ratio.

\paragraph{What Proportion of Reasoning in Long CoT Responses is Effective?}
On average, only 73\% of the reasoning in the collected long CoT responses is useful, highlighting significant redundancy issues. Among the models analyzed, \textit{QwQ-32B-Preview} exhibited the lowest proportion of effective reasoning at 70\%, while \textit{DeepSeek-R1} achieved a notably higher proportion compared to the others, demonstrating superior reasoning efficiency.


\begin{tcolorbox}[colback=white!95!gray, colframe=gray!70!black,  title=Key Finding for Reasoning Efficiency]
On average, 27\% of reasoning in long CoT responses we collected is redundant, and DeepSeek-R1 outperforms others in reasoning efficiency.
\end{tcolorbox}
\vspace{-3mm}

\subsection{Reasoning Process Analysis}

Figure ~\ref{fig: action_roles} shows the distribution of each section's action roles in the system II thinking process of the o1-like models. Initially, problem analysis dominates, indicating that the model initially focuses on understanding the requirements and constraints of the problem. As the solution progresses, cognitive activities diversify significantly, with reflection and validation becoming more prominent. In the later part of the reasoning, the distribution of conclusion and summarization gradually increases. 
%As the model progresses from problem analysis, solution implementation and conclusion, it demonstrates the common reasoning template of o1-like models.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/action_role.pdf}
    \caption{Distribution of different task types throughout the progress of a long CoT response.}
    \vspace{-3mm}
    
    \label{fig: action_roles}
\end{figure}
\subsection{Results on DeltaBench}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[!t]
\centering
\resizebox{1.0\textwidth}{!}{%
    \begin{tabular}{cccccccccccccccc}
    \toprule
    \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Overall}} & \textbf{Math} & \textbf{Code} & \textbf{PCB} & \textbf{General} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8}
     & \textbf{\textit{Recall}} & \textbf{\textit{Precision}} & \textbf{\textit{F1}} & \textbf{\textit{F1}} & \textbf{\textit{F1}} & \textbf{\textit{F1}} & \textbf{\textit{F1}} \\
    \midrule
    \multicolumn{8}{c}{\textbf{\textit{Process Reward Models (PRMs)}}} \\
    \midrule
    \rowcolor[rgb]{ .988,  .949,  .8} Qwen2.5-Math-PRM-7B & \textbf{30.30} & \textbf{34.96} & \textbf{29.22}  &  \textbf{29.64} & \textbf{23.76} & \underline{31.09} & \underline{34.19}   \\
    \rowcolor[rgb]{ .988,  .949,  .8} Qwen2.5-Math-PRM-72B & \underline{28.16} & \underline{29.37} & \underline{26.38}  & \underline{24.16} & \underline{22.02} & \textbf{31.14} & \textbf{35.83}  \\
    \rowcolor[rgb]{ .988,  .949,  .8} Llama3.1-8B-PRM-Deepseek-Data & 11.7 & 15.59 & 12.02 &  12.28 & 10.95 & 16.76 & 12.59  \\
    \rowcolor[rgb]{ .988,  .949,  .8} Llama3.1-8B-PRM-Mistral-Data & 9.64 & 11.21 & 9.45 & 9.40 & 10.72 & 13.43 & 12.40  \\
    \rowcolor[rgb]{ .988,  .949,  .8} Skywork-o1-Qwen-2.5-1.5B & 3.32 & 3.84 & 3.07 & 1.30 & 6.66 & 5.43 & 7.87  \\
    \rowcolor[rgb]{ .988,  .949,  .8} Skywork-o1-Qwen-2.5-7B & 2.49 & 2.22 & 2.17 & 0.78 & 6.28 & 6.02 & 3.11  \\
    \midrule
     \multicolumn{8}{c}{\textbf{\textit{LLM as Critic Models}}} \\
    \midrule
    \rowcolor[rgb]{ .922,  .89,  .988} GPT-4-turbo-128k & \textbf{57.19} & \textbf{37.35} & \textbf{40.76} & \textbf{37.56} & \textbf{43.06} & \underline{45.54} & \underline{42.17} \\
    \rowcolor[rgb]{ .922,  .89,  .988} GPT-4o-mini & \underline{49.88} & 35.37 & \underline{37.82} & \underline{33.26} & 37.95 & \textbf{45.98} & \textbf{46.39} \\
    \rowcolor[rgb]{ .922,  .89,  .988} Doubao-1.5-Pro & 39.68 & \underline{37.02} & 35.25 & 32.46 & \underline{39.47} & 33.53 & 37.00 \\
    \rowcolor[rgb]{ .922,  .89,  .988} GPT-4o & 36.52 & 32.48 & 30.85 & 28.61 & 28.53 & 39.25 & 36.50 \\
    \rowcolor[rgb]{ .922,  .89,  .988} Qwen2.5-Max & 36.11 & 30.82 & 30.49 & 26.73 & 32.81 & 39.49 & 29.54 \\
    \rowcolor[rgb]{ .922,  .89,  .988} Gemini-1.5-pro & 35.51 & 30.32 & 29.59 & 26.56 & 28.20 & 40.13 & 33.66 \\
    \rowcolor[rgb]{ .922,  .89,  .988} DeepSeek-V3 & 32.33 & 28.13 & 27.33 & 27.04 & 27.73 & 27.35 & 27.45 \\
    \rowcolor[rgb]{ .922,  .89,  .988} Llama-3.1-70B-Instruct & 32.22 & 28.85 & 27.67 & 21.49 & 32.13 & 28.45 & 39.18 \\
    \rowcolor[rgb]{ .922,  .89,  .988} Qwen2.5-32B-Instruct & 30.12 & 28.63 & 26.73 & 22.34 & 31.37 & 33.78 & 24.37 \\
    \rowcolor[rgb]{ .882,  .949,  .89} DeepSeek-R1 & 29.20 & 32.66 & 28.43 & 24.17 & 29.28 & 34.78 & 35.87 \\
    \rowcolor[rgb]{ .882,  .949,  .89} o1-preview & 27.92 & 30.59 & 26.97 & 22.19 & 28.09 & 33.11 & 35.94 \\
    % Gemini-2.0-flash-thinking & 14.02 & 17.36 & 14.56 & 14.79 & 11.97 & 19.34 & 15.26 \\
    \rowcolor[rgb]{ .922,  .89,  .988} Qwen2.5-14B-Instruct & 26.64 & 27.27 & 24.73 & 21.51 & 29.05 & 29.98 & 20.59 \\
    \rowcolor[rgb]{ .922,  .89,  .988} Llama-3.1-8B-Instruct & 25.71 & 28.01 & 24.91 & 18.12 & 32.17 & 27.30 & 29.93 \\
    \rowcolor[rgb]{ .882,  .949,  .89} o1-mini & 22.90 & 22.90 & 19.89 & 16.71 & 21.70 & 20.37 & 26.94 \\
    \rowcolor[rgb]{ .922,  .89,  .988} Qwen2.5-7B-Instruct & 21.99 & 19.61 & 18.63 & 11.61 & 25.92 & 29.85 & 15.18 \\
    \rowcolor[rgb]{ .882,  .949,  .89} DeepSeek-R1-Distill-Qwen-32B & 17.19 & 18.65 & 16.28 & 13.02 & 23.55 & 15.05 & 11.56 \\
    % Gemini-2.0-flash-thinking & 14.02 & 17.36 & 14.56 & 14.79 & 11.97 & 19.34 & 15.26 \\
    \rowcolor[rgb]{ .882,  .949,  .89} DeepSeek-R1-Distill-Qwen-14B & 12.81 & 14.54 & 12.55 & 9.40 & 18.36 & 10.44 & 12.01 \\
    % \rowcolor[rgb]{ .882,  .949,  .89} QwQ-32B-Preview & 10.20 & 10.17 & 9.07 & 7.38 & 8.60 & 14.97 & 10.54 \\
    \bottomrule
    \end{tabular}
}
\caption{Experimental results of PRMs and critic models on DeltaBench. \textbf{Bold} indicates the best results within the same group of models, while \underline{ underline} indicates the second best.}
% \vspace{-4mm}
\label{tab: main}
\end{table*}

% \noindent\textbf{Evaluation Metrics.}
% % To accurately assess the performance of the PRM and critic models on DeltaBench, 
% We employ \textbf{recall}, \textbf{precision}, and \textbf{macro-F1 score} for error sections as evaluation metrics. For the PRMs, we utilize an outlier detection technique based on the Z-Score to make predictions. This method was chosen because threshold-based prediction methods determined from other step-level datasets, such as those used in ProcessBench~\citep{Zheng2024ProcessBenchIP}, may not be reliable due to significant differences in dataset distributions, particularly as DeltaBench focuses on long CoT. Outlier detection helps to avoid this bias. The threshold $t$ for determining the correctness of a section is defined as:
% % \begin{align}
% $t = \mu - \sigma$,
% % \nonumber
% % \label{eq: prm_threshold}
% % \end{align}
% where $\mu$ is the mean of the rewards distribution across the dataset, and $\sigma$ is the standard deviation. Sections falling below $t$ are predicted as error sections. For critic models, all erroneous sections within a long CoT are prompted to be identified. Given that error sections constitute a smaller proportion than correct sections across the dataset, we use macro-F1 to mitigate the potential impact of the imbalance between positive and negative sections. Macro-F1 independently calculates the F1 score for each sample
% % (for our metric, each case) 
% and then takes the average, providing a more balanced evaluation metric when dealing with class imbalance.

\noindent\textbf{Baseline Models.}
% 开源（中英模型，llama3）和闭源模型
% To comprehensively evaluate the performance of current PRMs and critic models, we extensively selected and evaluated a wide range of both open-source and closed-source models on DeltaBench.
% \paragraph{Process Reward Models}
For the \textbf{PRMs}, we select the following models: Qwen2.5-Math-PRM-7B\footnote{\href{https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B}{Qwen/Qwen2.5-Math-PRM-7B}}, Qwen2.5-Math-PRM-72B\footnote{\href{https://huggingface.co/Qwen/Qwen2.5-Math-PRM-72B}{Qwen/Qwen2.5-Math-PRM-72B}}, Llama3.1-8B-PRM-Deepseek-Data\footnote{\href{https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Deepseek-Data}{RLHFlow/Llama3.1-8B-PRM-Deepseek-Data}}, Llama3.1 -8B-PRM-Mistral-Data\footnote{\href{https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Mistral-Data}{RLHFlow/Llama3.1-8B-PRM-Mistral-Data}}, Skywork-o1-Open-PRM- Qwen-2.5-1.5B\footnote{\href{https://huggingface.co/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B}{Skywork/Skywork-o1-Open-PRM-Qwen-2.5-1.5B}}, and Skywork-o1-Open-PRM-Qwen-2.5-7B\footnote{\href{https://huggingface.co/Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B}{Skywork/Skywork-o1-Open-PRM-Qwen-2.5-7B}}. 
% These represent some of the best open-source PRMs currently available.
% \paragraph{Critic Models}
We select a group of the most advanced open-source and closed-source LLMs to serve as \textbf{critic models} for evaluation, which includes various GPT-4~\citep{gpt4} variants (such as GPT-4-turbo-128K, GPT-4o-mini, GPT-4o), the Gemini model~\citep{Reid2024Gemini1U}(Gemini-1.5-pro), several Qwen models~\citep{qwen2.5} (such as Qwen2.5-32B-Instruct and Qwen2.5-14B-Instruct), Doubao-1.5-Pro~\citep{doubao2025}
and o1 models~\citep{openai-o1} (o1-preview-0912, o1-mini-0912).
% , and a GPT-3.5 variant (gpt-3.5-16K).



\subsubsection{Main Results}
In Table \ref{tab: main},
we provide the results of different LLMs on DeltaBench. 
For PRMs, we have the following observations: (1). Existing PRMs usually achieve low performance, which indicates that existing PRMs cannot identify the errors in long CoTs effectively and it is necessary to improve the performance of PRMs. (2). Larger PRMs
do not lead to better performance. For example, the Qwen2.5-Math-PRM-72B is inferior to wen2.5-Math-PRM-7B.
For critic models, we have the following findings: (1)
GPT-4-turbo-128k archives the best critique results, which is better than other models (e.g., GPT-4o) a lot in DeltaBench. (2) For o1-like models (e.g., DeepSeek-R1, o1-mini, o1-preview), we observe that the results of these models are not superior to non-o1-like models, with the performance of o1-preview is even lower than Qwen2.5-32B-Instruct.
%Additionally, we observe that the QWQ and DeepSeek-R1-Distill series models exhibit weaknesses in following instructions. 
A detailed analysis of underperforming models is provided in Appendix \ref{app: underperforming}.

% model size
% domains
% o1模型跟普通模型critic能力对比分析


\subsubsection{Further Analysis}

\paragraph{Effect of Long CoT Length.}
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/4.5.1/length2.pdf}
    \caption{The effect of long CoT length.}
    \label{fig: crtic1}
\end{figure}
In Figure \ref{fig: crtic1}, we compare the average F1-Score performance of critic models and PRMs across varying LongCoT token lengths. 
For critic models, the performance notably declines as token length increases. Initially, models like Deepseek-R1 and GPT-4o exhibit strong performance with shorter sequences (1-3k tokens). However, as token length increases to mid-ranges (4-7k tokens), there is a marked decrease in performance across all models. This trend highlights the growing difficulty for critic models to maintain precision and recall as long CoT response become longer and more complex, likely due to the challenge of evaluating lengthy model outputs. In contrast, PRMs demonstrate greater stability across token lengths, as they evaluate sections sequentially rather than processing the entire output at once. Despite this advantage, PRMs achieve lower overall scores compared to critic models on our evaluation set.

\begin{tcolorbox}[colback=white!95!gray, colframe=gray!70!black, title=Key Finding]
  Critic models exhibit significant performance degradation with longer contexts, while PRMs demonstrate consistent evaluation capability across varying lengths.
\end{tcolorbox}


\paragraph{Performance Analysis Across Different Error Types.}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/4.5.2/top_models_per_task.pdf}
    \caption{Results of different LLMs on top-5 errors.}
    \label{fig: top_models_per_task}
\end{figure}
Figure \ref{fig: top_models_per_task} shows the performance of different models on the five most common error types. In terms of error types, most models demonstrate the highest accuracy in recognizing calculation errors. Conversely, the recognition of strategy errors is generally the weakest. In terms of models, there is significant variation in the ability of individual models to recognize different error types. For instance, DeepSeek-V3 achieves an F1 of 36\% on calculation errors but only 23\% on strategy errors. Meanwhile, Llama3.1-8B-PRM-Deepseek performs poorly, with an F1 score of 22\% on calculation errors, and shows a significant decline in performance across the other four error types. This highlights the limited generalization capabilities of most models when recognizing various error types.

\begin{tcolorbox}[colback=white!95!gray, colframe=gray!70!black, title=Key Finding]
  Models exhibit strong performance on calculation errors but struggle with strategy errors, revealing limited generalization across error types.
\end{tcolorbox}

\begin{table}[!ht]
    \centering
    % \scriptsize
    % \footnotesize
    \begin{tabular}{cccc}
    \toprule
        \multirow{2}{*}{Model} & \multicolumn{3}{c}{HitRate@$k$ - Avg(\%)} \\ \cline{2-4}
                           & $k=1$ & $k=3$ & $k=5$ \\ 
                           % \hline
                           \midrule
        Qwen2.5-Math-PRM-7B & \textbf{49.15} & \textbf{69.14} & \textbf{83.14} \\
        Qwen2.5-Math-PRM-72B & \underline{41.13} & \underline{62.70} & \underline{75.73} \\ 
        Llama3.1-8B-PRM-Deepseek-Data & 12.63 & 48.62 & 69.78 \\
        Llama3.1-8B-PRM-Mistral-Data & 8.99 & 42.97 & 65.33 \\
        Skywork-o1-Open-PRM-Qwen-2.5-1.5B & 31.90 & 53.82 & 69.23 \\
        Skywork-o1-Open-PRM-Qwen-2.5-7B & 31.58 & 52.59 & 69.16 \\
        % \hline
        \bottomrule
    \end{tabular}
    \vspace{+3mm}
    \caption{Results of HitRate@$k$. Bold and underlined results indicate the best and the second best.}
    % \vspace{-4mm}
\label{tab: hitrate}
\end{table}

\paragraph{Analysis on HitRate evaluation for PRMs.}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/prm_rank.pdf}
    % \vspace{-10pt}
    \caption{Ranking of rewards for the first incorrect section for different PRMs.}
    % \vspace{-3mm}
    \label{fig: prm_rank}
\end{figure}

To better measure the ability of PRMs to identify erroneous sections in long CoTs, we use HitRate@$k$ to evaluate PRMs. Specifically, within a sample, we rank the sections in ascending order based on the rewards given by the PRM, select the smallest $k$ sections, and calculate the recall rate for the erroneous sections among them. Specifically, we define the sorted sections as $S = \{s_1, s_2, \ldots, s_n\}$, with $E$ being the set of erroneous sections. We select the top $k$ sections, denoted as $S_k = \{s_1, s_2, \ldots, s_k\}$. The HitRate@$k$ is  calculated as:
\begin{align}
\text{HitRate@}k = \frac{|S_k \cap E|}{\min(k, |E|)}
% \nonumber
\label{eq: hitrate}
\end{align}
In this formula, $|S_k \cap E|$ indicates the number of erroneous sections identified among the top $k$ sections. This metric reflects the ability of PRMs to effectively identify erroneous sections within the top $k$ candidate sections. In Table \ref{tab: hitrate}, the relative performance rankings among different PRMs are quite similar to the results in Table \ref{tab: main}. Additionally, we observe that for $k=3$ and $k=5$, the performance differences between various PRMs are not particularly significant. However, when $k=1$, the Qwen2.5-Math-PRM-7B shows a clear performance advantage. Figure \ref{fig: prm_rank} illustrates the ranking ability of different PRMs for the first incorrect section within the sample, which is generally consistent with the performance evaluation results of HitRate@k.
% This is because a smaller $k$ value imposes stricter requirements on the PRM's ability to identify errors.

% HitRate@$k$ evaluates the performance of PRMs from the perspective of reward ranking, providing additional evidence for the experimental results and conclusions in Table \ref{tab: main} from a different angle.

\begin{tcolorbox}[colback=white!95!gray, colframe=gray!70!black, title=Key Finding]
  HitRate@k evaluation aligns with the main results, with Qwen2.5-Math-PRM-7B demonstrating superior performance in identifying the first incorrect section.
\end{tcolorbox}


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/4.5.4/self-critic.pdf}
    % \vspace{-10pt}
    \caption{F1-score comparison of self-critique and cross-model critique abilities for different models.}
    % \vspace{-5mm}
    \label{fig: self-critic}
\end{figure}

\paragraph{Comparative Analysis of Self-Critique Capabilities of LLMs.} We randomly sample queries based on domains and models that generate the long CoT output, followed by a statistical analysis of the model's performance in evaluating its own outputs as well as those of other models. In Figure \ref{fig: self-critic},  Gemini 2.0 Flash Thinking, DeepSeek-R1, and QwQ-32B-Preview show lower self-critique scores compared to their cross-model critique scores, indicating a prevalent deficiency in self-critic abilities. Notably, DeepSeek-R1 exhibits the largest discrepancy, with a 36\% decrease in self-evaluation compared to evaluations of other models. This suggests models' self-critic abilities remain underdeveloped.
% signaling an area that requires improvement.

\begin{tcolorbox}[colback=white!95!gray, colframe=gray!70!black, title=Key Finding]
  LLMs demonstrate weaker self-critique performance compared to cross-model critique, highlighting a fundamental limitation in self-critic capabilities.
\end{tcolorbox}



%%%

% \noindent\textbf{Performance Analysis Across Different Categories}

% \begin{figure}[htbp]
% \centering
% \includegraphics[width=\linewidth]{figures/prm_task_comparison.pdf}
% \caption{Performance of PRMs across different categories (outlier detection).}
% \label{fig: prm_task}
% % \vspace{-0.6cm}
% % \vspace{-4mm}
% \end{figure}


% \noindent\textbf{Performance Variation in Different Lengths of Long CoT}

% \noindent\textbf{Performance Analysis Across Different Error Types}

% \noindent\textbf{Analysis of In-Sample Reward Ranking}


% % \subsection{Evaluation Metrics}

% % \subsection{Main Results}

% % \subsection{Further Analysis}
% \subsection{Analysis on LLM Critics}
%  \textbf{error location}



% \subsubsection{The Performance across different domains}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/critic6.pdf}
%     \caption{The score distributions across different domains.}
%     \label{fig: crtic2}
% \end{figure}

% In Figure \ref{fig: crtic2}, we illustrate the F1-score distribution of various large language models (LLMs) across different domains. Analyzing model performance across domains reveals that most models demonstrate stronger critiquing abilities in Physics, Chemistry, Biology, and General Reasoning compared to Mathematics and Programming, indicating higher proficiency in scientific and general reasoning tasks. Meanwhile, the performance of each model varies significantly depending on the domain, reflecting inherent strengths and weaknesses in handling different tasks. For instance, the Gemini-1.5-Pro model achieves an F1-score of 40.1\% in PCB, yet only 26.6\% in Mathematics. These discrepancies underscore challenges in the models' generalization capabilities.





