%\vspace{-0.2cm}
\section{DeltaBench}
%\vspace{-0.2cm}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{figures/main5.pdf}
% \vspace{-0.6cm}
\caption{Left: Overview of DeltaBench. These pie charts show the distribution of questions in Math, General Reasoning, PCB (Physics, Chemistry and Biology), and Programming. Right:  Statistics of DeltaBench.}
% including the total number of questions, LongCOT token statistics, and section number statistics.}
\label{fig: category}
\end{figure*}
% TODO：recheck the number of the dataset
% In this section, we describe the construction of the Deltabench dataset, designed to evaluate a model's ability to identify and localize errors within a long CoT reasoning process. Deltabench consists of 1,000 samples across a range of domains, including mathematics, programming, physics, chemistry, biology, and general reasoning. Each sample includes a problem, its corresponding solution, and detailed annotations of each section. Each section is annotated with the following labels: whether the reasoning is useful, whether it contains errors, and whether it includes reflections. For sections with errors, a detailed description of the error is provided along with the location where the error occurs. If reflections are present, the accuracy of the reflections is evaluated, along with the location where the reflection occurs.

In this section, we detail the construction of the DeltaBench dataset, developed to assess a model's capacity to identify and locate errors in long CoT reasoning processes. DeltaBench comprises 1,236 samples across diverse domains, including \textbf{Math}, \textbf{Programming}, \textbf{PCB} (physics, chemistry and biology), and \textbf{General Reasoning}. Each sample encompasses a problem, its corresponding long CoT solution, and comprehensive human annotations. Specifically, the long CoT is divided into sections, and each section includes the following tags: 
\begin{itemize}[left=1em]

\item \textbf{Strategy Shift:} whether this section introduces a new method or strategy attempt. If a new strategy is introduced, the specific step is annotated.

\item \textbf{Reasoning Usefulness:} whether the reasoning in this section is useful. If the process of section can help to lead to the right answer, it considered as useful.

\item \textbf{Reasoning Correctness:} whether this section contains any errors. If an error is present, additional error-related fields are annotated, including the first step number at which the error occurs, explanation and correction.

\item \textbf{Reflection Efficiency:} whether this section contains reflection and whether the reflection is correct. If reflection is present, the step at which the reflection begins is annotated.

\end{itemize}





%whether the reasoning is useful, if it contains errors, and if reflections are included. For sections containing errors, a detailed description and the precise step where the error occurs are provided. If reflections are present, their accuracy is evaluated, and the step at which the reflection occurs is indicated.


\subsection{Dataset Construction}

\paragraph{Query collection.}
We extract queries from diverse open-source datasets. Detailed data sources are listed in Appendix \ref{app: data_source}. The domains include math, programming, physics, chemistry, biology, and general reasoning, which comprise 48 subcategories. To ensure the dataset's diversity and balance, we employ a multi-step process:

\begin{itemize}[left=1em]
\item \textbf{Clustering and Deduplication}: Queries are first converted into vector representations using the NV-Embed-v2\footnote{https://huggingface.co/nvidia/NV-Embed-v2} embedding model.  Then, similarity scores are computed between each pair of queries to identify and eliminate duplicates using a predefined threshold. The non-duplicate queries are clustered using DBSCAN~\citep{DBSCN}, resulting in 17,510 unique queries.

\item \textbf{Difficulty Filtering}: For each query, multiple models\footnote{GPT-4o~\citep{gpt4}, Meta-Llama-3-70B-Instruct~\citep{dubey2024llama3}, Meta-Llama-3-8B-Instruct~\citep{dubey2024llama3}, Qwen2.5-72B-Instruct~\citep{qwen2.5}, Qwen2.5-32B-Instruct~\citep{qwen2.5}, and DeepSeek-67B-chat~\citep{deepseek-llm}.} were employed to generate solutions, and difficulty labels were assigned based on the accuracy of the answers produced by these models. Following this, uniform sampling was carried out according to these difficulty labels to ensure a balanced distribution of difficulties.

\item \textbf{Subcategory Sampling}: For each query, GPT-4o is used to classify it into a subcategory. The queries are then uniformly sampled based on these subcategories to ensure diversity.
% . 
%The classification details are provided in Appendix \ref{}.


\end{itemize}


\paragraph{Data Preprocessing.}
We observe low-quality queries exist in open-source datasets. To address this issue, we employed GPT-4 and rule-based filtering strategies to identify and remove these low-quality queries. We have recorded encountered issues. Specific details are shown in Appendix \ref{app: data_preprocess}.

\paragraph{Long CoT Generation.}

We generate long CoT solutions using several open-source o1-like models, such as QwQ-32B-Preview, DeepSeek-R1, and Gemini 2.0 Flash Thinking, with random sampling to enhance diversity. This method ensures a wide range of reasoning processes and captures potential errors models may produce in real-world scenarios, enabling a robust evaluation of error detection capabilities in long CoT reasoning.

\paragraph{Section Split.}

\begin{figure*}[!tbp]
\centering
% \vspace{-10mm}
\includegraphics[width=\linewidth]{figures/appendix/crop_div_sections.pdf}
% \vspace{-0.6cm}
\caption{An example of section division for long CoT reasoning process.}
\label{fig: section_div}
% \vspace{-4mm}
% \vspace{-0.6cm}
\end{figure*}

% Previous approaches typically divided solutions into steps; however, long CoT responses often contain numerous steps, which significantly increases the difficulty of manual annotation without providing substantial benefits. To address this issue, we divided each model’s solution into multiple sections, with each section representing an independent sub-task, following a structure more aligned with human understanding patterns. Specifically, we first used the delimiter "\verb|\n\n|" to break the model’s response into steps. Then, we employ GPT-4 to identify the start and end steps of each section and generate a brief summary of the content within each section. This approach not only facilitated manual annotation but also enhanced the accuracy of the model's sectioning process.

Previous approaches typically divide solutions into steps. However, long CoT responses often contain numerous steps, which significantly increases the difficulty of human annotation, and many of which are either overly granular or lack meaningful contribution to the overall reasoning process. To address this issue, we segment each long CoT response into multiple sections, each representing an independent sub-task, aligning more closely with human cognitive patterns. Specifically, we use the delimiter "\verb|\n\n|" to partition the model’s response into steps first. Then, we employ GPT-4 to identify the start and end steps of each section and generate a brief summary of the content within each section. This approach not only facilitates manual annotation but also enhances the accuracy of the model's segmentation process. The details are provided in Appendix \ref{app: section_div}. 

% In addition, we visualize the changing distribution of section types in the model's reasoning process.
% % which can help us better understand system II thinking. 
% See Appendix \ref{app: action_role} for detailed analysis.


\subsection{Correctness Assessment}
Before manual annotation, we employ automated methods to assess the correctness of the long CoT results. Domain-specific techniques are used to identify potentially erroneous outputs in Appendix \ref{app:asses} and the evaluation accuracy of each domain and the details are shown in Appendix \ref{app: correct_assess}. 
This process ensures that the data provided for manual annotation are likely to contain errors, which enhances the annotation efficiency.
% of manual annotation.


\subsection{Human Annotation}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/human_annotation.pdf}
    \caption{An example of human annotation applied to a mathematical problem-solving process. Annotators are required to annotate each section individually.}
    \label{fig: label_case}
\end{figure}

The data annotation process aims to evaluate the reasoning process of each long CoT response systematically. Each section is assessed for \textbf{strategy shift}, \textbf{reasoning usefulness}, \textbf{reasoning correctness}, and \textbf{reflection efficiency}, as shown in Figure \ref{fig: label_case}. The annotation of whether strategy shift and reflection occurred is to help analyze o1's thinking pattern. The annotation of Reasoning usefulness and error identification is to better analyze and evaluate the performance of system II thinking and further evaluate the critique ability of other models for these problems. 

To ensure high-quality annotations, we recruit Master's and Ph.D. graduates from various disciplines and collaborate with specialized suppliers (See Appendix \ref{app:anno} for more details on the annotation and quality control processes).


\subsection{Dataset Statistics}

% Deltabench contains 1,000 carefully curated samples. These samples are distributed across five major domains and 48 subcategories. The dataset's structure ensures a balance of question types and difficulty levels, testing both simple and complex reasoning tasks. Key statistics on the distribution of samples and difficulty levels are provided in Appendix A. The average length of solutions, error frequency, and other relevant metrics will be detailed in Appendix B.
\textbf{DeltaBench} contains 1,236 carefully curated samples. These samples are distributed across five major domains and 48 subcategories. The dataset ensures a balance of question types and difficulty levels, incorporating a rich set of long CoT responses. 

% \begin{figure}[!htbp]
%     \centering
%     \begin{subfigure}{\columnwidth}
%         \centering
%         \includegraphics[width=0.85\textwidth]{figures/longcot_length_dist.pdf}
%         % \vspace{-10pt}
%         % \vspace{}
%         \caption{Distribution of long CoT Length.}
%         \label{fig: longcot_length_dist}
%     \end{subfigure}
%     \\
%     \begin{subfigure}{\columnwidth}
%         \centering
%         \includegraphics[width=0.85\textwidth]{figures/longcot_sections_dist.pdf}
%         % \vspace{-10pt}
%         \caption{Distribution of the number of long CoT sections.}
%         \label{fig: longcot_section_dist}
%     \end{subfigure}
%     \caption{Distribution of long CoT characteristics.}
%     \label{fig: longcot_dist}
% \end{figure}

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/longcot_length_dist.pdf}
        % \vspace{-10pt}
        \caption{Distribution of long CoT Length.}
        \label{fig: longcot_length_dist}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/longcot_sections_dist.pdf}
        % \vspace{-10pt}
        \caption{Distribution of the number of long CoT sections.}
        \label{fig: longcot_section_dist}
    \end{subfigure}
    \caption{Distribution of long CoT characteristics.}
    \label{fig: longcot_dist}
\end{figure}


Figure \ref{fig: longcot_dist} shows the distribution of both the length and the number of sections of long CoTs. The distribution is relatively balanced overall, enabling a comprehensive evaluation of the performance of PRMs or critic models across a range of different lengths. Additionally, detailed statistics on the category distribution are provided in Appendix \ref{app: category_distribution}.
% Figure \ref{fig: longcot_length_dist} shows the length distribution of long CoT in DeltaBench, with an overall average length of approximately $4.4k$ tokens. The distribution is relatively uniform between $0$ and $12k$ tokens, allowing for a comprehensive evaluation of the performance of PRMs or critic models across different lengths.

%[todo] 添加附录的引用
\subsection{Evaluation Metrics}
% To accurately assess the performance of the PRM and critic models on DeltaBench, 
We employ \textbf{recall}, \textbf{precision}, and \textbf{macro-F1 score} for error sections as evaluation metrics. For the PRMs, we utilize an outlier detection technique based on the Z-Score to make predictions. This method was chosen because threshold-based prediction methods determined from other step-level datasets, such as those used in ProcessBench~\citep{Zheng2024ProcessBenchIP}, may not be reliable due to significant differences in dataset distributions, particularly as DeltaBench focuses on long CoT (The details are provided in Appendix \ref{app: other_evaluation}). Outlier detection helps to avoid this bias. The threshold $t$ for determining the correctness of a section is defined as:
% \begin{align}
$t = \mu - \sigma$,
% \nonumber
% \label{eq: prm_threshold}
% \end{align}
where $\mu$ is the mean of the rewards distribution across the dataset, and $\sigma$ is the standard deviation. Sections falling below $t$ are predicted as error sections. For critic models, all erroneous sections within a long CoT are prompted to be identified. Given that error sections constitute a smaller proportion than correct sections across the dataset, we use macro-F1 to mitigate the potential impact of the imbalance between positive and negative sections. Macro-F1 independently calculates the F1 score for each sample
% (for our metric, each case) 
and then takes the average, providing a more balanced evaluation metric when dealing with class imbalance.


\subsection{Comparison to Other Benchmarks}


\begin{table}[t]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{llcc}
        \toprule
        \textbf{Benchmark} & \textbf{Source} & \textbf{Long CoT} & \textbf{Granularity} \\ 
        \midrule
        JudgeBench&
        \begin{tabular}[c]{@{}l@{}}
            MMLU-Pro~\citep{Wang2024MMLUProAM},
            LiveBench~\citep{White2024LiveBenchAC},\\ 
            LiveCodeBench~\citep{Jain2024LiveCodeBenchHA}
        \end{tabular} & 
        \texttimes & Sample-Level \\ \hline
        % MMLU-Pro~\citep{Wang2024MMLUProAM}, LiveBench~\citep{White2024LiveBenchAC}, LiveCodeBench~\citep{Jain2024LiveCodeBenchHA}&\texttimes&Sample-Level\\
        CriticBench&
        \begin{tabular}[c]{@{}l@{}}
            GSM8K~\citep{cobbe2021gsm8k},
            CSQA~\citep{Talmor2019CommonsenseQAAQ},\\
            BIGBench~\citep{Srivastava2022BeyondTI}, HumanEval~\citep{Chen2021EvaluatingLL}, etc
        \end{tabular} & 
        \texttimes & Sample-Level \\ \hline
        
        % GSM8K~\citep{cobbe2021gsm8k}, CSQA, BIGBench,HumanEval,etc&\texttimes&Sample-Level\\
        CriticEval & 
        \begin{tabular}[c]{@{}l@{}}
            GSM8K~\citep{cobbe2021gsm8k}, HumanEval~\citep{Chen2021EvaluatingLL}, \\ ChatArena~\citep{Chiang2024ChatbotAA}, etc.
        \end{tabular} & 
        \texttimes & Sample-Level \\\hline
        
        % GSM8K,HumanEval,ChatArena, etc.& \texttimes &  Sample-Level\\
        ProcessBench&
        \begin{tabular}[c]{@{}l@{}}
            GSM8K~\citep{cobbe2021gsm8k}, MATH~\citep{Lightman2023LetsVS}, \\
            OlympiadBench~\citep{He2024OlympiadBenchAC},
            Omni-MATH~\citep{Gao2024OmniMATHAU}
        \end{tabular} & 
        \texttimes & Step-Level \\
        
        % GSM8K,MATH, OlympiadBench, Omni-MATH&\texttimes&Step-Level\\
        %PRMBench&PRM800K&\texttimes&Step-Level Error Identification\\
        \midrule
        \textbf{DeltaBench}&
        \begin{tabular}[c]{@{}l@{}}
        AIME, BigCodeBench~\citep{zhuo2024bigcodebench}, 
        KOR-Bench~\citep{Ma2024KORBenchBL}, \\ 
        GPQA~\citep{Rein2023GPQAAG}, etc
        \end{tabular} & 
        \checkmark & Section-Level \\
        % &\checkmark&Section-Level\\
        % CodeCriticBench & 4,300 & 3,200 & 1,100 & \checkmark & \texttimes \\
        \bottomrule
    \end{tabular}

    }
    \vspace{0.3cm}
            \caption{Comparisons between different benchmarks. Sample-level evaluation classifies the entire model response as correct or incorrect. Step-level evaluation assesses the correctness of individual reasoning steps. Section-level evaluation evaluates the correctness of reasoning sections which is a more appropriate granularity for long CoT response.}
                \label{table:benchmark_compare}

\end{table}
In Table \ref{table:benchmark_compare}, 
% we compare DeltaBench with several existing critic benchmarks.
DeltaBench has the following features:
% itself in three key aspects: 
(1) We focus on difficult questions, providing a more challenging critical evaluation; (2) We utilize long CoT responses, enabling the assessment of a model's ability to identify errors within complex reasoning processes; (3) We evaluate the model's capability to identify all errors in the reasoning process, rather than just the first error or a binary classification of correctness on sample level,
which can provide a fine-grained analysis of long CoTs.
% This comprehensive approach allows for a more thorough evaluation of a model's critic ability of reasoning process, particularly in scenarios involving reflection and iterative reasoning. Additionally, DeltaBench exclusively uses naturally occurring errors from model responses, ensuring a realistic and representative assessment of errors that o1-like models might encounter.