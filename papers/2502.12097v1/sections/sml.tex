
We formally refer to \textsl{solution manifold} as the set of time-dependent velocity and pressure fields which are solutions of the Navier--Stokes equations~\eqref{eq:3dnse}
for different computational domains and boundary conditions~\eqref{eq:3dnse-bc}. 
The goal of solution manifold learning is to accurately and efficiently approximate the solution manifold using the available training data. 
%
Widely used approaches consider linear global reduced bases of the discrete finite element spaces 
(see e.g. ~\cite{benner_model_2017,hesthaven2016certified,rozza2022advanced}), time- or geometry-dependent linear bases, as well as 
non-linear approximants, including autoencoders~\cite{Fresca2020, romor2023nonlinear, ROMOR2025113729} or other non-linear dimension reduction techniques from machine learning. 

This section presents two applications of the shape registration method from section \ref{sec:registration} to solution manifold learning. 
First, in subsection~\ref{subsec:sml_correlations}, we propose different metrics to study the correlation between geometries and solutions (pressure/velocity), as well 
as between velocity and pressure, based on mapping the dataset of patient-specific flow data on the same reference shape.
The results are used in the context of data assimilation problems,  where correlations are particularly relevant when considering the estimation of pressure-related quantities from velocity measurements (see sections \ref{sec:da} and \ref{sec:prec}).  
Next, in section~\ref{subsec:sml_rec}, we investigate the accuracy of the global and local rSVD bases constructed registering the snapshots of pressure and velocity solutions from different shapes onto the same reference.

\subsection{Analysis of physics-based and geometric-based correlations}
\label{subsec:sml_correlations}
We evaluate the correlation between dissimilarity matrices computed from the database of shapes and corresponding numerical solutions, exploiting
the fact that all relevant fields can be encoded in the same discrete space, i.e., the finite element space on the computational domain of the reference shape.
%
In what follows, let us denote with $\mathcal{S}$ the fixed shape, and with $n_{p, \mathcal{S}}$, $\dofu$, and $\dofp$ the number of vertices of the
corresponding finite element mesh and the degrees of freedom of the velocity and pressure solutions, respectively ($n_{p, \mathcal{S}}=110676$, $\dofu = 332028$, $\dofp = 110676$ for the particular selection of the reference mesh considered in this study).
The remaining \textit{target} geometries, that can be registered to $\mathcal{S}$ via the registration maps
$\phi_1^i$, will be denoted
as $\shape{i}$, $i=1,\hdots,n_{\rm geo}$, with $n_{\rm geo} = 724 + 52 = 776$ (both the test and training sets used in section  \ref{sec:registration}).
%
Each target shape $\shape{i}$ can be encoded using the three spatial coordinates of the reference mesh vertices mapped to the target domain i.e. through the displacement fields
\begin{equation*}
  Y_{\shape{i}} = \phi^i_1(\XS)-\XS, \qquad Y_{\shape{i}}\in\mathbb{R}^{\nvertref \times 3}, 
\end{equation*}
and a further coordinate based on the distance from the centerline $l_{\shape{i}}$ computed on the mapped reference vertices
\begin{equation}
  \label{eq:enc_geo}
  Z_{\shape{i}} = d\left(\phi^i_1\left(\XS\right),l_{\shape{i}} \right),\qquad Z_{\shape{i}} \in \mathbb{R}^{\nvertref}\,,
\end{equation}
where $d$ stands for the Euclidean distance.

% then physics
For each $\shape{i}$, let us now define the matrices $X^{\mathbf u}_{\shape{i}} \in\mathbb{R}^{\dofu \times n_T}$  and 
$X^{p}_{\shape{i}}\in\mathbb{R}^{\dofp \times n_T}$ containing $n_T=80$ equally spaced snapshots of the velocity and pressure fields in the time interval $[0.05s, 0.25 s]$ (see remark~\ref{rmk:timewindow}) registered on $\mathcal{S}$ and evaluated on the reference finite element mesh.
%

We then introduce two metrics. Let $A$ and $B$ be two matrices, $A, B \in O(D, r)$  with $r>0$ orthonormal columns of dimension $D>0$. 
Following ~\cite{galarce2022state}, we define the Hausdorff distance as
\begin{equation}\label{eq:d_hausdorff}
 d_H^2(A, B) := \max\left(\max_{a\in \text{col}(A)} \frac{\lVert a-P_{B}a\rVert^2}{\lVert a\rVert^2};\max_{b\in \text{col}(B)} \frac{\lVert b-P_{A}b\rVert^2}{\lVert b\rVert^2}\right),
\end{equation}
where $\text{col}(A), \text{col}(B)$ are the set of columns of the matrices $A$ and $B$, respectively, 
and $P_C$, ($C=A,B$), is the finite-dimensional orthogonal projector onto the space spanned by the columns of $C$.
%
We also define the Grassmann distance (see e.g.~\cite{daniel2020model}) as
\begin{equation}\label{eq:d_grassman}
  d_{\text{Gr}}^2(A, B) = \sum^{r}_{i=1}\arccos^2(\sigma_i),\qquad A^TB = %U\Sigma V^T = 
  U\begin{bmatrix}
    \sigma_{1} & & \\
    & \ddots & \\
    & & \sigma_{r}
  \end{bmatrix}V^T,
\end{equation}
where $U\Sigma V^T$ stands for the singular value decomposition of $A^TB$. 


We consider then the following \textit{dissimilarity} matrices of dimension $\mathbb{R}^{n_{\text{geo}}\times n_{\text{geo}}}$:
\begin{equation}\label{eq:geo_matrices}
\begin{aligned}
(K^{\text{enc}})_{ij} & = d\left(Z_{\shape{i}}, Z_{\shape{j}}\right), \\
(K^{\phi})_{ij} & = d\left(\phi(1, X_{\mathcal S_i})-X_{\mathcal S_i}, \phi(1, X_{\shape{j}})-X_{\shape{j}}\right) = d\left(Y_{\shape{i}}, Y_{\shape{j}}\right), \\
\end{aligned}
\end{equation}
based on the Euclidean distance between geometric encodings, and 
\begin{equation}\label{eq:up_matrices}
\begin{aligned}
(K_{H}^{\mathbf u})_{ij} & = d_H(X_{\shape{i}}^{\mathbf u}, X_{\shape{j}}^{\mathbf u}),
\quad(K_{{\rm Gr}}^{\mathbf u})_{ij} = d_{\rm Gr}(X_{\shape{i}}^{\mathbf u}, X_{\shape{j}}^{\mathbf u}),\\
(K_{H}^{p})_{ij} & = d_H(X^p_{\shape{i}}, X^p_{\shape{j}}),
\quad(K_{{\rm Gr}}^{p})_{ij} = d_{\rm Gr}(X^p_{\shape{i}}, X^p_{\shape{j}}),
\end{aligned}
\end{equation}
based on the distances between the solutions introduced in \eqref{eq:d_hausdorff} and \eqref{eq:d_grassman}.
%
These matrices are used to evaluate the correlation between the geometry and the solution, as well as the velocity and the pressure fields using a Mantel test with 
Pearsonâ€™s product-moment correlation coefficient $r_m\in[-1, 1]$ and $999$ permutations.

Table~\ref{tab:mantel1} shows the results for the correlation between geometry and velocity/pressure fields, whilst Table~\ref{tab:mantel2} shows the results for the correlation 
between velocity and pressure.
Since different metrics are used, the dissimilarity matrices are centered, before the correlation coefficient is computed. 
%
For a qualitative comparison, figure~\ref{fig:mantel} shows the dissimilarity matrices entries omitting the diagonal ones. %and plotting one entry every $100$.

\begin{table}[htp!]
  \centering
  \footnotesize
  \caption{Mantel test results for the correlation between geometry dissimilarity matrices \eqref{eq:geo_matrices} and velocity/pressure dissimilarity matrices \eqref{eq:up_matrices}. Correlation coefficients with less statistical significance ($p$-value$>0.05$) have been omitted. }
  \begin{tabular}{
      l 
      |>{\centering\arraybackslash}p{3.cm} 
      |>{\centering\arraybackslash}p{3.cm} 
      |>{\centering\arraybackslash}p{3.2cm} 
      |>{\centering\arraybackslash}p{3.cm} }
      \textbf{} &$K_{H}^{\mathbf u}$ & $K^{\mathbf u}_{\text{Gr}}$ & $K^p_{d_H}$ &$K^p_{d_{\text{Gr}}}$ \\[3pt]
      \hline
      \hline 
     $K^{\text{enc}}$ & $r_m=0.172, p=0.001$ & $r_m=0.156, p=0.001$ & - & $r_m=0.110, p=0.001$ \\
      \hline
    $K^{\phi}$ & - & $r_m=0.267, p=0.001$ & $r_m=-0.094, p=0.001$ & $r_m=0.217, p=0.001$ \\
      \hline
  \end{tabular}
  \label{tab:mantel1}
\end{table}
%
\begin{table}[htp!]
  \caption{Mantel test results for the correlation between the velocity and pressure dissimilarity matrices \eqref{eq:up_matrices}. Correlation coefficients with less statistical significance ($p$-value$>0.05$) have been omitted. }
  \centering
    \footnotesize
  \begin{tabular}{
      l 
      |>{\centering\arraybackslash}p{3.5cm} 
      |>{\centering\arraybackslash}p{3.5cm} }
     & $K^p_{H}$ & $K^p_{\text{Gr}}$\\[3pt]
    \hline
    \hline
    $K_{H}^{\mathbf u}$ & $r_m=0.433, p=0.033$ & -\\[2pt]
    \hline
    $K^{\mathbf u}_{\text{Gr}}$ & $r_m=-0.169, p=0.001$ & $r_m=0.943, p=0.001$ \\[2pt]
    \hline
\end{tabular}
  \label{tab:mantel2}
\end{table}
%
The results suggests that the Grassmann metric is more granular than the Hausdorff metrics proposed in~\cite{galarce2022state}. 
In particular, the correlation plots $K^{\text{enc}}$ \textit{vs.} $K^{\mathbf{u}}_{H}$, $K^{\phi}$ \textit{vs.} $K^{\mathbf{u}}_{H}$, 
$K^{\mathbf{u}}_{H}$ \textit{vs.} $K^{p}_{H}$, and $K^{\mathbf{u}}_{H}$ \textit{vs.} $K^{p}_{\text{Gr}}$ show two clusters of more and less correlated pairs. 
%
We can also observe that there exists a choice of metrics ($K^{\mathbf{u}}_{\text{Gr}}$ \textit{vs.} $K^{p}_{\text{Gr}}$) for which velocity and pressure fields 
are highly correlated, suggesting that in this setting the data assimilation of pressure from velocity measurements may be more feasible. 
At the same time, based on the correlation study, inferring the velocity or the pressure solution solely from the geometry seems to be a more challenging task.
%
\begin{figure}[!htp]
  \centering
  \includegraphics[width=0.59\textwidth]{img/corr.pdf}
  \includegraphics[width=.335\textwidth]{img/corr2.pdf}
  \caption{Correlation among dissimilarity matrices: each dot corresponds to an entry of the matrices indicated in the $x$- and $y$-axis. 
Diagonal entries have been omitted.  Only one every $100$ entries among the $n_{\text{geo}}^2\text{-}n_{\text{geo}}$ off-diagonal entries are shown. 
\textbf{Left}: correlation between geometry encoding and velocity/pressure fields. \textbf{Right}: Correlation between velocity and pressure solutions.}
  \label{fig:mantel}
\end{figure}

In figure~\ref{fig:cluster_v}, we show the clustering of the available training and test geometries with MDS and dissimilarity matrix $K^{u}_{d_{G_r}}$ from equation~\eqref{eq:up_matrices}. In particular, we can detect the test geometries with the best ($n=12$) and worst ($n=42$) approximable velocity field.

\begin{figure}[!htp]
 \centering
 \includegraphics[width=0.9\textwidth]{img/cluster_v.pdf}
 \caption{\textbf{Left: } Clustering with MDS of the $724$ training and $52$ test geomeries. \textbf{Right: }  Test case $12$ and $42$ represent the closest and furthest geometries to the training set with respect to the Grassmann distance on the velocity field.}
 \label{fig:cluster_v}
\end{figure}


\subsection{Approximation properties of  global and local rSVD bases}
\label{subsec:sml_rec}
The shape dataset has been split into a  training ($n_{\text{train}}=724$) and a test  ($n_{\text{test}}=52$) set. To obtain a global linear reduced basis, we apply randomized SVD~\cite{halko2011finding} with a given rank $r>0$ to the matrices of velocity and pressure fields on the template geometry ordered column-wise $X^{\mathbf u}_{\text{train}}\in\mathbb{R}^{\dofu\times(n_{\text{train}}n_T)}$ and $X^{p}_{\text{train}}\in\mathbb{R}^{\dofp\times(n_{\text{train}}n_T)}$, respectively: 
\begin{equation*}
\begin{aligned}
X^{\mathbf u}_{\text{train}} \ &\rsvd\  \Phi_{\mathbf u}\Sigma^{r}_{\mathbf u}\Psi_{\mathbf u}
,\quad \Phi_{\mathbf u} \in\mathbb{R}^{\dofu\times r},\ \Sigma^{r}_{\mathbf u}\in\mathbb{R}^{r\times r},\ \Psi_{\mathbf u}\in\mathbb{R}^{r\times (n_{\text{train}}n_T)},\\
%
X^{p}_{\text{train}} \ &\rsvd\  \Phi_p\Sigma^{r}_p\Psi_p,\quad \Phi_p\in\mathbb{R}^{\dofp\times r},\ \Sigma^{r}_p\in\mathbb{R}^{r\times r},\ \Psi_p\in\mathbb{R}^{r\times (n_{\text{train}}n_T)}.
\end{aligned}
\end{equation*}
The columns of the matrices $\Phi_{\mathbf u}$ and $\Phi_p$ define the orthonormal global rSVD basis. For a $r$-dimensional (reduced) representation of a velocity field
$z_{\mathbf u}^{r} \in \mathbb R^r$ (resp. of a pressure field $z_p^{r}\in\mathbb{R}^r$ ), the corresponding approximation in the full finite element space will be defined by $\Phi_{\mathbf u} z_{\mathbf u}^{r}$ (resp. $\Phi_p z_p^{r}$).


\begin{rmk}[Partitioned \textit{vs.}  monolithic rSVD]
 We considered a partitioned rSVD global basis, i.e., computing the rSVD modes for velocity and pressure from two snapshot matrices. An alternative monolithic approach consists in 
computing the rSVD on a single snapshot matrix of dimension $(\dofu+\dofp) \times n_{\rm train} n_T$ where velocity and pressure solutions are stacked row-wise.
%
In our case, the choice was dictated by the better performance in terms of accuracy of the partitioned rSVD basis.
%
However, especially in the context of data assimilation for inferring pressure fields from velocity observations, a monolithic rSVD might have the advantage of handling the coupled latent representation in a single $r$-dimensional variable, which allows to automatically obtain the pressure field from the same reduced variable~\cite{galarce2023displacement}. 
\end{rmk}

Depending on the reduced dimension $r$, we consider the relative $L^2$-reconstruction errors to evaluate the accuracy of the reduced approximation 
\begin{equation}
  \label{eq:rec}
  \epsilon^{r}_{u} (\mathbf u_i(t)) := \frac{\lVert \mathbf u_i(t) -\Phi_{\mathbf u}\Phi_{\mathbf u}^T \mathbf u_i(t)\rVert_2}{\lVert \mathbf u_i(t) \rVert_2},\quad 
  \epsilon^{r}_p (p_i(t)) := \frac{\lVert p_i(t)-\Phi_p\Phi_p^T p_i(t)\rVert_2}{\lVert p_i(t)-\bar{p}_i \rVert_2},
\end{equation}
varying $u_i$ and $p_i$ among the numerical solutions of the training and test geometries, for $i=1,\hdots,724 + 52$, and time instances $t\in \{0.05s+n\cdot\Delta t\ | n\in\{0,\dots, n_T=80\}\}$.
In \eqref{eq:rec}, $\bar{p}_i\in\mathbb{R}$ stands for the average of the considered pressure solution.

The relative $L^2$-reconstruction errors \eqref{eq:rec} are shown in figure~\ref{fig:recerr} for $r\in\{500, 1000, 2000, 4000\}$, showing that a very high number of modes
is required to obtain approximation errors of the order of 10\% for the velocity field. The error for the pressure field is of the order of 1-5\% for all considered dimensions.
%
\begin{rmk}[Time window]
  \label{rmk:timewindow}
  Outside the considered time window $t \in [0.05s,0.25s]$, the velocity was poorly approximated. This might be due to the additional complexity in the flow patterns during flow deceleration, such as arise of vortices, which depend very strongly on the geometrical details and are not accurately reprodicible with a linear basis such as $\Phi_{\mathbf u}$. This is the reason why we restrict our data assimilation studies to the time window $t \in [0.05s,0.25s]$.
\end{rmk}
  %

\begin{figure}[!htp]
  \centering
  \includegraphics[width=0.85\textwidth]{img/rec_err.pdf}
  \caption{Average among the training or test datasets, of the relative $L^2$-reconstruction errors \eqref{eq:rec} of the velocity 
  and pressure fields in the time interval $[0.05s,0.25]$ for different rSVD ranks.}
  \label{fig:recerr}
\end{figure}
The current results suggest that the size of the reduced space might not be suitable for designing reduced order models, restricting the finite element spaces of the variational formulation to the linear space spanned by the rSVD basis.
Although this approach has been proposed and applied in related contexts using much lower reduced space dimensions~\cite{guibert2014group,PEGOLOTTI2021113762}, the additional complexity
considered in the current setting (general geometries, high variability of outlet boundary dimension, variable Windkessel parameters depending on flow split and measured inlet flow rate, presence of 
stenosis which leads to more complex patterns, and the incorporation of turbulence modelling) leads to the need of a much larger space for satisfactory approximations.

Handling shape variability in the context of reduced-order modeling and data assimilation has been also recently discussed in~\cite{galarce2022state} in the context of parametric domains, 
proposing to employ  a \textit{local} rSVD basis, i.e., first clustering the different shapes using a multidimensional scaling (MDS) clustering algorithm, and then assembling the reduced-order model only considering the closest instances.
%
%, but in our case the global rSVD basis performs best, possibly due to the high geometric variability and our limited computational budget to increase the training dataset. 
To test an analogous approach, we clustered the training geometries based on the MDS using the dissimilarity matrices $K^{\mathbf u}_{\text{Gr}}$ and $K^{\mathbf u}_{d_{H}}$. Then, given a test geometry
and a fixed value or $r$, we collect the snapshots of the closest $n_{\text{local}}:=\left\lceil r/n_T \right \rceil$ training shapes 
\begin{equation}
  \label{eq:trainsnap}
X^{\mathbf u}_{\text{train}, H}\in\mathbb{R}^{\dofu \times(n_{\text{local}}\,n_T)},\quad X^p_{\text{train}, H}\in\mathbb{R}^{\dofp\times(n_{\text{local}}\,n_T)} ,
\end{equation}
and
\begin{equation}
X^{\mathbf u}_{\text{train}, {\rm Gr}}\in\mathbb{R}^{\dofu \times(n_{\text{local}}\,n_T)},\quad X^p_{\text{train}, {\rm Gr}}\in\mathbb{R}^{\dofp\times(n_{\text{local}}\,n_T)} ,
\end{equation}
depending on the considered dissimilarity metric for the clustering, and compute the corresponding local rSVD bases.
Figure~\ref{fig:reclocal} shows the $L^2$-reconstruction errors \eqref{eq:rec} for the global rSVD basis and for the local ones
%the resulting local rSVD modes $\Phi_{u, H}$, $\Phi_{p, H}$ and $\Phi_{u, G_r}$, $\Phi_{p, G_r}$ are employed to compute the relative $L^2$-reconstruction error for that specific test geometry. We show the performance of a global SVD basis against local basis found with MDS clustering from $K^u_{d_{\text{Gr}}}, K^u_{d_{H}}$, in figure~\ref{fig:reclocal}. 
In the considered range of dimensions, the global rSVD achieves a better accuracy than the local approaches, using either the Hausdorff or the Grassmann metric. 
As observed above, this might reflect the high geometrical variability of the considered dataset, for which a larger amount of local geometries are required. 

%
\begin{figure}[!htp]
  \centering
  \includegraphics[width=0.7\textwidth]{img/rec_err_local.pdf}
  \caption{Relative $L^2$-reconstruction errors \eqref{eq:rec} for velocity and pressure averaged over the computational domain and over the considered
  time interval $[0.05s,0.25]$ varying the rSVD rank $r$ and considering a global rSVD and local Hausdorff and Grassman rSVD bases.
	The $25\%$ and $75\%$ percentile are also shown.}
  \label{fig:reclocal}
\end{figure}

Figure~\ref{fig:recerr_ntrain} shows the dependency of the reconstruction error on the number of training data employed. We observe that the global rSVD basis 
is not an efficient approximant of the solution manifold, yielding a decay of the velocity reconstruciton error of the order of $n_{\text{train}}^{-1/2}$. 
Increasing the training dataset with additional geometries is expected to improve the local rSVD error, and local rSVD basis and non-linear dimension reduction methods should be preferred. 

\begin{figure}[!htp]
  \centering
  \includegraphics[width=1\textwidth]{img/ml_studies.pdf}
  \caption{Study of the asymptotic behaviour of the relative $L^2$ reconstruction errors \eqref{eq:rec} increasing the size of the training dataset.
  A nonlinear fit with the function $n_{\text{train}}^{-a}+b$ and multiple initial values for the parameters $a>0,b$ is also shown: red crosses represent the extrapolated values. 
}
  \label{fig:recerr_ntrain}
\end{figure}


\section{EPD-GNN trained with registered solutions}\label{ssec:pres-gnn}
Building on the shape registration algorithm, we propose a new framework for inference with neural networks on different meshes. 
The dataset is represented by the collection of registered velocity and pressure fields supported on the reference shape.

We employ encode-process-decode graph neural networks (EPD-GNN), introduced in~\cite{pfaff2020learning}, that 
represent the state of the art GNN architectures to perform inference on computational meshes. 
To reduce the computational cost, the reference mesh has been coarsened using TetGen~\cite{Si2015}, reducing the number of vertices from $n_{p, \mathcal{S}}=110676$ to $n_{\text{vertices}}=5181$.
%
The velocity and pressure fields are transported from the fine to the coarse template meshes and back through RBF interpolation. The metrics (equation~\eqref{eq:l2relerr}) used to validate the results are always evaluated on the fine target meshes. We employ the nearest-neighbours algorithm to enrich each vertex with $e\in\{6, 9, 12\}$ edges to the closest vertices, for a total of $n_{\text{edges}}\in\{36648, 53748, 66135\}$ edges, respectively. Only undirected graphs will be employed.
%
We consider two inference problems.
\paragraph*{Geometry to velocity (\textit{gnn-gv}) and geometry to pressure (\textit{gnn-gp}) inference}
The input represents a geometrical encoding of the target computational domains with additional velocity b.c.. For each target domain, we evaluate the scalar field that represents the distance from the centerline $Z_{\shape{i}}\in\mathbb{R}^{n_{\text{vertices}}}$, from equation~\eqref{eq:enc_geo}. The pullback of this scalar field to the reference geometry through the registration map together with the pushforwarded coordinates of the vertices of the template geometry $\phi^i_1(\XS)\in\mathbb{R}^{n_{\text{vertices}\times 3}}$ is our $4$-dimensional geometrical encoding. This geometrical encoding is then embedded with a Fourier positional encoding~\cite{sutherland2015error} with $10$ features, through the maps $\{\cos(2^{i}z_j), \sin(2^{i}z_j)\}_{i=0, j=0}^{9, 3}$, where $\mathbf{z}=\{z_i\}_{i=0}^3\in\mathbb{R}^4$ is an arbitrary input vector, for a total of $n_{\text{feat}}=80=10\cdot 2\cdot 4$ geometrical input features. To these it is added the velocity field at $n_{t,\text{GNN}}=8$ times
\begin{equation}\label{eq:int}
  t\in\{0.05s, 0.075s, 0.1s, 0.125s, 0.15s, 0.2s, 0.225s\}=I_t,
\end{equation}
restricted at the boundaries $\Gamma_{\text{in}}$ and $\Gamma_i$ with $i\in\{1, 2, 3, 4\}$ of the target domains and then pulled back to the reference geometry. The value of the velocity boundary field is zero inside the computational domain $\overline{\Omega}\backslash \left(\Gamma_{\text{in}}\cup \left(\cup_{i=1}^{4}\Gamma_i\right)\right)$. The total dimension of the inputs is thus $n_{\text{fnodes}} = 104 =n_{\text{feat}}+n_{t,\text{GNN}}\cdot 3$, where $3$ refers to the number of components of the velocity field. Each edge between the vertices $\mathbf{x}_i$ and $\mathbf{x}_j$ has as features, the vector $\mathbf{x}_i-\mathbf{x}_j$, its $L^2$-norm $\lVert\mathbf{x}\rVert_2$, and the difference between the values of the input vector at the nodes $\mathbf{x}_i$ and $\mathbf{x}_j$, for a total of $n_{\text{fedges}}=4+n_{\text{fnodes}}$ edge features. The output of the GNNs for the problems \textit{gnn-gv} or \textit{gnn-gp} are the velocity field $u\in\mathbb{R}^{n_{\text{vertices}}\times 3n_{t,\text{GNN}}}$ or the pressure field $p\in\mathbb{R}^{n_{\text{vertices}}\times n_{t,\text{GNN}}}$ respectively, evaluated at $n_{t,\text{GNN}}=8$ time instants $t\in I_t$ and supported on the coarse reference mesh.

\paragraph*{Velocity to pressure (\textit{gnn-vp}) inference}
The input is the velocity field $u\in\mathbb{R}^{n_{\text{vertices}}\times 3n_{t,\text{GNN}}}$ at $n_{t,\text{GNN}}=8$ times $t\in I_t$ supported on the coarse reference mesh with $n_{\text{vertices}}=5181$ vertices and $n_{\text{edges}}\in\{36648, 53748, 66135\}$ edges, depending on the number of adjacent nodes $e\in\{6, 9, 12\}$. Each edge between the vertices $\mathbf{x}_i$ and $\mathbf{x}_j$ has, as features, the vector $\mathbf{x}_i-\mathbf{x}_j$, its $L^2$-norm $\lVert\mathbf{x}\rVert_2$, and the difference between the values of the velocity field at $n_{t,\text{GNN}}=8$ time instances at the nodes $\mathbf{x}_i$ and $\mathbf{x}_j$, for a total of $n_{\text{fedges}}=28=4+n_{t,\text{GNN}}\cdot 3$ edge features, $3$ stands for the components of the velocity field. The output is the pressure field $p\in\mathbb{R}^{n_{\text{vertices}}\times n_{t,\text{GNN}}}$ at $n_{t,\text{GNN}}=8$ times $t\in I_t$ supported on the coarse reference mesh. \newline

As our EPD-GNN model we choose the \textit{MeshGraphNet} architecture implemented in NVIDIA-Modulus~\cite{modulus}, based on \texttt{pytorch}~\cite{NEURIPS2019_9015}. The hyperparameters are the width of the network $w$, i.e., the number of consecutive EPD layers, the common hidden dimension $h$ of the node encoder and decoders and the edge encoder, and also the number of edges $e$ of each node. The loss is the relative mean squared error. We apply the ADAM stochastic optimization method~\cite{kingma2014adam} to train the EPD-GNNs with a scheduler used to halve the learning rate when the validation error does not decrease after $200$ epochs. The initial learning rate value is $0.001$. The $52$ test geometries are not employed during the training. We perform the optimization on a single GPU NVIDIA A100-SXM4 with 40GB of graphics RAM size.

We perform two hyperparameter studies.  Firstly, we consider the values $$(w,h)\in\{(10, 64), (15, 128), (20, 256)\}$$ and fix $e=9$, with $n_{\text{train}}=720$ training geometries and $n_{\text{val}}=4$ validation geometries, and $n_{\text{epochs}}=500$. We then select the best model as the one with lowest validation error. 
Secondly, we fix $(w,h)=(30, 256)$ and choose $e\in\{6, 9, 12\}$, with $n_{\text{train}}=680$ and $n_{\text{val}}=44$, and $n_{\text{epochs}}=1000$. We then select the best model as the one at the last epoch.

\begin{figure}[!htp]
  \centering
  \includegraphics[width=.85\textwidth]{img/gnn_train.pdf}
  \caption{First hyperparameter study: $e=9$, $(w,h)\in\{(10, 64), (15, 128), (20, 256)\}$, $n_{\text{train}}=720$, $n_{\text{val}}=4$, and $n_{\text{epoch}}=500$.}
  \label{fig:overfitting}
\end{figure}

\begin{figure}[!htp]
  \centering
  \includegraphics[width=.85\textwidth]{img/gnn_train_edges.pdf}
  \caption{Second hyperparameter study: $e\in\{6, 9, 12\}$, $(w,h)=(30, 256)$, $n_{\text{train}}=680$, $n_{\text{val}}=44$, and $n_{\text{epoch}}=1000$.}
  \label{fig:overfitting_edges}
\end{figure}

The values of the loss during the training for the first and second hyperparameter studies are reported in figures~\ref{fig:overfitting} and~\ref{fig:overfitting_edges}, respectively. A comparison of the training and validation mean squared loss, computed on the coarse mesh with $n_{\text{vertices}}=5181$ vertices, highlights a clear overfitting phenomenon in our limited data regime, with a high generalization error compared to the training error. From the convergence behavior of the training error, we are hopeful that increasing the training dataset would bring better results. The optimal way to increase the training dataset is a future direction of research.

To evaluate the prediction errors we will consider the $L^2$-relative errors for the velocity $\epsilon_{\widehat{\mathbf{u}}}$ and pressure $\epsilon_{\widehat{p}}$ evaluated on target shapes $\mathcal{T}$:
\begin{equation}
  \label{eq:l2relerr}
  \epsilon_{\widehat{\mathbf{u}}} = \frac{\lVert \widehat{\mathbf{u}}_{\text{true}}-\widehat{\mathbf{u}}\rVert_2}{\lVert \widehat{\mathbf{u}}_{\text{true}}\rVert_2},\qquad\epsilon_{\widehat{p}} = \frac{\lVert \widehat{p}_{\text{true}}-\overline{\widehat{p}}_{\text{true}} -(\widehat{p}-\overline{\widehat{p}})\rVert_2}{\lVert \widehat{p}_{\text{true}}-\overline{{\widehat{p}}}_{\text{true}} \rVert_2},
\end{equation}
where $\widehat{\mathbf{u}}_{\text{true}}$ and $\widehat{p}_{\text{true}}$ are the high-fidelity velocity and pressure fields obtained from the solution of the Navier--Stokes equation \eqref{eq:3dnse} on the target domain, $\widehat{\mathbf{u}}$ and $\widehat{p}$ are the predicted velocity and pressure fields, and
$\overline{\widehat{p}}_{\text{true}}\in\mathbb{R}$ and $\overline{\widehat{p}}\in\mathbb{R}$ denote the averages of the pressure fields. Hat symbols denote quantities defined on the target geometries. The minimum, maximum and median relative $L^2$-errors for the problems $\textit{gnn-gv}$, $\textit{gnn-gp}$, and $\textit{gnn-vp}$, are reported in Table~\ref{tab:gnns}: we evaluated these errors from the best models selected from the hyperparameter studies. The fields associated to the minimum, maximum and median values are shown in figure~\ref{fig:gv} for \textit{gnn-gv}, figure~\ref{fig:gp} for \textit{gnn-vp}, and figure~\ref{fig:vp} for \textit{gnn-vp}. In the following sections we will compare these results with PBDW (section~\ref{sec:da}) and pressure estimators (section~\ref{sec:prec}), using only the best model selected from the first hyperparameter study.
\begin{table}[H]
  \centering
  \begin{tabular}{l|ccc|ccc|ccc}
 & \multicolumn{3}{c}{$\textit{gnn-gv}~(\epsilon_{\widehat{\mathbf{u}}})$} & \multicolumn{3}{|c}{$\textit{gnn-gv}~(\epsilon_{\widehat{\mathbf{u}}})$} & \multicolumn{3}{|c}{$\textit{gnn-gv}~(\epsilon_{\widehat{\mathbf{u}}})$}\\ \hline
& \textbf{min} & \textbf{max} & \textbf{median} & \textbf{min} & \textbf{max} & \textbf{median} & \textbf{min} & \textbf{max} & \textbf{median}\\
       \hline
      First hp study  & $0.26$ & $0.44$ & $0.31$ & $0.11$ & $0.83$ & $0.27$ & $0.11$ & $0.54$ & $0.22$ \\
      \hline
      Second hp study & $0.25$ & $0.45$ & $0.31$ & $0.11$ & $0.93$ & $0.30$ & $0.11$ & $0.67$ & $0.24$ \\
      \hline
  \end{tabular}\hspace{5mm}
  \caption{Relative $L^2$-errors of the velocity and pressure predictions corresponding the best architectures from the first and second hyperparameter (hp) studies.}
  \label{tab:gnns}
\end{table}
%\begin{table}[H]
%  \centering
%  \begin{tabular}{lccc}
%      $\textit{gnn-gv}~(\epsilon_{\widehat{\mathbf{u}}})$ & \textbf{min} & \textbf{max} & \textbf{median} \\
%       \hline
%      first hp study  & $0.26$ & $0.44$ & $0.31$\\
%      \hline
%      second hp study & $0.25$ & $0.45$ & $0.31$\\
%      \hline
%  \end{tabular}\hspace{5mm}
%  \begin{tabular}{lccc}
%    $\textit{gnn-gp} ~(\epsilon_{\widehat{p}})$ & \textbf{min} & \textbf{max} & \textbf{median} \\
%    \hline
%    first hp study & $0.11$ & $0.83$ & $0.27$\\
%   \hline
%   second hp study & $0.11$ & $0.93$ & $0.30$\\
%   \hline
%\end{tabular}\vspace{3mm}
%\begin{tabular}{lccc}
%  $\textit{gnn-vp} ~(\epsilon_{\widehat{p}})$ & \textbf{min} & \textbf{max} & \textbf{median} \\
%  \hline
%  first hp study & $0.11$ & $0.54$ & $0.22$\\
% \hline
% second hp study & $0.11$ & $0.67$ & $0.24$\\
% \hline
%\end{tabular}
%  \caption{Relative $L^2$-errors of the velocity and pressure predictions corresponding the best architectures from the first and second hyperparameter (hp) studies.}
%  \label{tab:gnns}
%\end{table}

\begin{figure}[!htp]
  \centering
  \includegraphics[width=1\textwidth]{img/gv.pdf}
  \caption{Results on the test dataset of the EPD-GNNs for the problem \textit{gnn-gv} at systolic peak $t=0.125s$: true velocity field magnitude, predicted velocity field magnitude and difference between the two scalar fields. \textbf{Left: }Minimum test $L^2$-relative error $\boldsymbol{\epsilon}_{\widehat{\mathbf u}}=\textbf{0.26}$. \textbf{Right: } Maximum test $L^2$-relative error $\boldsymbol{\epsilon}_{\widehat{\mathbf u}}=\textbf{0.44}$. \textbf{Bottom: } Median test $L^2$-relative error $\boldsymbol{\epsilon}_{\widehat{\mathbf u}}=\textbf{0.31}$.}
  \label{fig:gv}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{img/gp.pdf}
  \caption{Results on the test dataset of the EPD-GNNs for the problem \textit{gnn-gp} at systolic peak $t=0.125s$: true pressure field, predicted pressure field and difference between the two scalar fields. The values of the pressure fields are rescaled to the same average. \textbf{Left: }Minimum test $L^2$-relative error $\boldsymbol{\epsilon}_{\widehat{p}}=\textbf{0.11}$. \textbf{Right: } Maximum test $L^2$-relative error $\boldsymbol{\epsilon}_{\widehat{p}}=\textbf{0.83}$. \textbf{Bottom: } Median test $L^2$-relative error $\boldsymbol{\epsilon}_{\widehat{p}}=\textbf{0.27}$.}
  \label{fig:gp}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=1\textwidth]{img/vp.pdf}
  \caption{Results on the test dataset of the EPD-GNNs for the problem \textit{gnn-vp} at systolic peak $t=0.125s$: true pressure field, predicted pressure field and difference between the two scalar fields. The values of the pressure fields are rescaled to the same average. \textbf{Left: }Minimum test $L^2$-relative error $\boldsymbol{\epsilon}_{\widehat{\mathbf p}}=\textbf{0.11}$. \textbf{Right: } Maximum test $L^2$-relative error $\boldsymbol{\epsilon}_{\widehat{\mathbf p}}=\textbf{0.54}$. \textbf{Bottom: } Median test $L^2$-relative error $\boldsymbol{\epsilon}_{\widehat{\mathbf p}}=\textbf{0.22}$.}
  \label{fig:vp}
\end{figure}

EPD-GNNs could be seen as a potential approach to reduce the effort and the time required for accurate experimental acquisition of 4DMRI data. However, in our case, the results suggest that the training of EPD-GNNs requires more data to achieve a better accuracy. 
Similar problems have been recently addressed considering GNNs or a combination of NNs and SVD as data-driven surrogate models in simpler settings, i.e., 
considering only healthy geometries, neglecting the secondary branches (LBCA, LCCA, LSA), employing a simplified physical model~\cite{pajaziti2023shape}(where $20$ and $57$ SVD modes for pressure and velocity are sufficient in their case to achieve a good reconstruction error with a different registration method), or employing 1D graphs instead of full 3D geometries~\cite{iacovelli2023novel,pegolotti2024learning}.

The overall low level of accuracy of the EPD-GNNs predictions is confirmed in figures~\ref{fig:gv},~\ref{fig:gp}, and~\ref{fig:vp}, showing the minimum, the maximum and the median $L^2$-relative error for the EPD-GNNs predictions.
These results might suggest that the variability of stenotic aortic geometries requires necessarily a larger training dataset
than only the $724$ training geometries used in the present study. 
This causes noticeable overfitting, see, e.g.,  the value of the loss on the validation set in figure~\ref{fig:overfitting}, and a still high training error.

In figure~\ref{fig:epdgnn_reg_no_reg}, we compare the mean $L^2$-relative error of the new framework over the $52$ test target geometries against the results of an EPD-GNN architecture trained on the original target geometries without registration, and coarsening them with TetGen~\cite{Si2015} as it has been done previously for the template mesh. The definition of the loss, inputs, outputs, and optimization are the same for registered and non-registered datasets. The difference is that the datasets are not supported on the same graph anymore. 
We can observe that the registration represents an efficient encoding of the geometrical and physical features of the problem: for each inference problem \textit{gnn-vp}, \textit{gnn-gp}, and \textit{gnn-gv}, the EPD-GNNs trained on registered data perform better than their alternatives on non-registered data \textit{gnn-gv-no-reg}, \textit{gnn-gp-no-reg}, and \textit{gnn-vp-no-reg}.


%mean $L^2$-relative error over the $52$ test target geometries for the predictions of the EPD-GNNs architectures trained with datasets registered on the template or supported on the original target geometries. Also in the case, we will consider only coarsened meshes obtained with Tetgen~\cite{Si2015}, with the same coarsening parameters, for computational budget limits. 
\begin{figure}[!htp]
  \centering
  \includegraphics[width=0.7\textwidth]{img/gnn_reg.pdf}
  \caption{$L^2$-relative errors $\epsilon_{\widehat{\mathbf{u}}}$ and $\epsilon_{\widehat{p}}$ computed on the target computational domains of the predicted velocity (\textit{gnn-gv},\textit{gnn-gv-no-reg}) and pressure fields (\textit{gnn-gp},\textit{gnn-gp-no-reg}, \textit{gnn-vp},\textit{gnn-vp-no-reg}) using EPD-GNN models with and without registration of the datasets.}
  \label{fig:epdgnn_reg_no_reg}
\end{figure}