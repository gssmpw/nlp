\section{Numerical solution of the 3D-0D blood flow model}
\label{appendix:weak}
At each time step $t_{n+1}\in[2\Delta t,0.7s]$ we solve the following linear system for $(\mathbf{u}_{n+1}, p_{n+1}, \{\pi_{i,n+1}\}_{i=1}^{4})$, given the solution for these quantities at time
steps $t_{n}$ and $t_{n-1}$. 

\begin{problem}[Weak formulation]
  \label{def:weak_ins_w}
  Find $(\mathbf{u}_{n+1}, p_{n+1}, \{\pi_{i,n+1}\}_{i=1}^{4})\in[\mathbb{P}^1(\mathcal{T}_h)]^3\times\mathbb{P}^1(\mathcal{T}_h)\times\mathbb{R}^4$ such that $\forall(\mathbf{v},q)\in [\mathbb{P}^1(\mathcal{T}_h)]^3\times\mathbb{P}^1(\mathcal{T}_h)$:
  \begin{subequations}\begin{align*}
    \rho\left(\frac{\tfrac{3}{2}\mathbf{u}_{n+1}-\mathbf{u}_{n}+\tfrac{1}{2}\mathbf{u}_{n-1}}{\Delta t},\mathbf{v}\right)_{\Omega}&+\left(2\mu\nabla^S\mathbf{u}_{n+1}, \nabla^S\mathbf{v}\right)_{\Omega} + \rho\left((2\mathbf{u}_n-\mathbf{u}_{n-1})\cdot\nabla\mathbf{u}_{n+1},\mathbf{v}\right)_{\Omega}+\\
    &-(p_{n+1},\nabla\cdot\mathbf{v})_{\Omega}+(\nabla\cdot\mathbf{u}_{n+1}, q)_{\Omega}+\\
    &+\mathcal{S}(\mathbf{u}_{n+1}, \mathbf{u}_{n}, \mathbf{u}_{n-1},p_{n+1}, p_{n}, p_{n-1},\mathbf{v}, q) =\\
    &= \sum_{i=1}^4\left(R_{p,i}\int_{\Gamma_i}\mathbf{u}_{n}\cdot\mathbf{n}+\pi_{i,n+1},\mathbf{v}\right)_{\Gamma_i}+\tfrac{\rho}{2}\left(\min\{(2\mathbf{u}_{n}-\mathbf{n}_{n-1})\cdot\mathbf{n},0\},\mathbf{v}\right)_{\Gamma_i},\\
    \text{for}\quad i\in\{1, 2,3,4\},\qquad &\int_{\Gamma_i}\mathbf{u}_{n}\cdot\mathbf{n}=C_{d,i}\frac{\pi_{i, n+1}-\pi_{i, n}}{\Delta t}+\frac{\pi_{i,n}}{R_{d,i}},
  \end{align*}\end{subequations}
  where $\rho,\mu, \{C_{d, i}\}_{i=1}^4, \{R_{d, i}\}_{i=1}^4, \{R_{p, i}\}_{i=1}^4$ are positive constants. The stabilization $\mathcal{S}$ includes the SUPG-PSPG and VMS-LES terms:
  \begin{subequations}\begin{align*}
    &\mathcal{S}(\mathbf{u}_{n+1}, \mathbf{u}_{n}, \mathbf{u}_{n-1},p_{n+1}, p_{n}, p_{n-1};\mathbf{v}, q) = \sum_{K\in\mathcal{T}_h}(\mathcal{S}_{\text{SUPG-PSPG}}^K+\mathcal{S}_{\text{VMS}}^K+\mathcal{S}_{\text{LES}}^K),\\
    &\mathcal{S}_{\text{SUPG-PSPG}}^K=(\tau_M\cdot \mathbf{r}_M(\mathbf{u}_{n+1}, p_{n+1}),\rho \mathbf{u}_{n+1}^{*}\cdot\nabla\mathbf{v}+\nabla q)_K+(\tau_C\cdot r_C,\nabla\cdot\mathbf{v})_K,\\
    &\mathcal{S}^K_{\text{VMS}} = (\tau_M\cdot \mathbf{r}_M(\mathbf{u}_{n+1}, p_{n+1}),\rho \mathbf{u}_{n+1}^{*}\cdot\nabla^T\mathbf{v})_K,\\
    &\mathcal{S}^K_{\text{LES}} = -(\tau_M \mathbf{r}_M(\mathbf{u}^{\text{EXT}}_{n+1}, p^{\text{EXT}}_{n+1})\otimes\tau_M \mathbf{r}^{\text{LHS}}_M(\mathbf{u}_{n+1}, p_{n+1}),\rho\nabla\mathbf{v})_K+(\tau_M \mathbf{r}_M(\mathbf{u}_{n+1}, p_{n+1})\otimes \tau_M \mathbf{r}^{\text{RHS}}_M, \rho\nabla\mathbf{v})_K,\\
      \end{align*}\end{subequations}
    with
\begin{subequations}\begin{align*}    
    &\mathbf{r}_M(\mathbf{u}, p) = \mathbf{r}^{\text{LHS}}_M(\mathbf{u}, p)-\mathbf{r}^{\text{RHS}}_M,\\
    &\mathbf{r}^{\text{LHS}}_M(\mathbf{u}, p) =\rho\left(\frac{\tfrac{3}{2}\mathbf{u}}{\Delta t}+\mathbf{u}^{*}_{n+1}\cdot\nabla\mathbf{u}\right)+\nabla p-\mu\Delta\mathbf{u},\\
    &\mathbf{r}^{\text{RHS}}_M=\rho\frac{\mathbf{u}_{n}-\tfrac{1}{2}\mathbf{u}_{n-1}}{\Delta t},\\
    &r_C = \nabla\cdot\mathbf{u}_{n+1},\\
    &\tau_M = \left(\frac{4\rho^2}{\Delta t^2}+\rho^2\mathbf{u}^{*}_{n+1}\cdot G\mathbf{u}^{*}_{n+1}+30\mu^2 G:G\right)^{\tfrac{1}{2}},\\
    &\tau_C = (\tau_M\mathbf{g}\cdot\mathbf{g})^{-1},\\
  \end{align*}\end{subequations}
  with $\mathbf{u}^{*}_{n+1} = 2\mathbf{u}_{n}-\mathbf{u}_{n-1}$, $p^{*}_{n+1}=2p_{n}-p_{n-1}$, $\mathbf{u}^{\text{EXT}}_{n+1}=2\mathbf{u}_{n}-\mathbf{u}_{n-1}$,$p^{\text{EXT}}_{n+1}=2p_{n}-p_{n-1}$, and where $G=J^{-T}J^{-1}$ and $\mathbf{g}=J^{-T}\mathbf{1}$ denote the metric tensor and vector obtained from the Jacobian $J$ of the map from reference element to current finite element.
\end{problem}
The first time step $t=\Delta t$ is solved with an explicit Euler scheme. We use an uniform time step with $\Delta t=\SI{2.5e-03}{\second}$.

Additional information can be found in the documentation of \texttt{lifex-cfd}~\cite{AFRICA2024109039} and references~\cite{Fumagalli2020, Fedele2017, bazilevs2007variational, forti2015semi}. To solve the linear system, we employ GMRES with the SIMPLE preconditioner, see the documentation \texttt{lifex-cfd}~\cite{AFRICA2024109039} and references~\cite{Deparis2014}: the AMG preconditioner is used to invert the Shur complement and the $(\mathbf{u}_{n+1},\mathbf{v})$ matrix block in the discretized block matrix obtain from definition~\ref{def:weak_ins_w}, neglecting the Windkessel model.

The meshes are generated with VMTK~\cite{antiga2008image} from surface representations obtained with the ZIB Amira software~\cite{stalling2005amira}. Through VMTK one boundary layer with thickness factor $0.3$ is added. The number of vertices of each mesh, the number of tetrahedra of each mesh, the maximum diameter across all tetrahedra of each mesh and the maximum volume across all tetrahedra of each mesh is reported in figure~\ref{fig:mesh_info}: the distribution of these quantities is reported over all train and test cases for a total of $776$ geometries. Some velocity snapshots are shown in figure~\ref{fig:snapsu}.

The CFD simulations were run in parallel on $64$ cores in a distributed-memory fashion. Multiple compute servers at WeierstraÃŸ Institute for Applied Analysis and Stochastic (WIAS, Berlin) were employed to run in serial the CFD simulations for each training and test geometry: HPE Synergy 660 Gen10 with four Intel Xeon Gold CPUs each with 18 cores and HPE Synergy 480 Gen10 with two Intel Xeon Gold CPUs each with 18 cores. The models of the Intel Xeon Gold CPUs vary.

\begin{figure}[!htp]
  \centering
  \includegraphics[width=1\textwidth,trim=0cm 0cm 0cm 1cm, clip=true]{img/mesh_info.pdf}
  \caption{Mesh information across the $776$ train and test geometries.}
  \label{fig:mesh_info}
\end{figure}

\begin{figure}[!htp]
  \centering
  \includegraphics[width=0.85\textwidth]{img/usnapstest-compressed.pdf}
  \caption{High-fidelity velocity fields at systolik peak $t=0.125s$ on the $52$ test geometries.}
  \label{fig:snapsu}
\end{figure}

\section{Convergence of the discrete registration problem}
\label{appendix:convergence}
Proof of theorem~\ref{theo:existreg}.
\begin{proof}
For all $0\leq r\leq s$, it holds
  \begin{align*}
    \lVert\phi_\epsilon(t)-\phi(t)\rVert_{H^r}&\leq \int_0^t \lVert f_\epsilon\circ\phi_\epsilon-f\circ\phi_\epsilon\rVert_{H^r}+\lVert f\circ\phi_\epsilon-f\circ \phi\rVert_{H^r}\ d\tau \\
    &\lesssim \int_{0}^t\lVert f_\epsilon-f\rVert_{H^r}+\lVert f\rVert_{H^r}\lVert\phi_\epsilon-\phi\rVert_{H^r}\ d\tau,
  \end{align*}
  thanks to the fact that $\phi_\epsilon$ is a diffeomorphism.
  By Gronwall's inequality it also holds
  \begin{equation}
    \label{eq:phif}
    \lVert \phi_\epsilon - \phi\rVert_{L^2(I, H^{r})}\rVert \leq C_1\lVert f_\epsilon - f\rVert_{L^2(I, H^{r})},  \;\forall r,\  0\leq r\leq s.
  \end{equation}
  The first thesis follows for the universal approximation theorem of ReLU neural networks. The second estimate follows from theorem 4.1~\cite{doi:10.1142/S0219530522500014}.
\end{proof}
We study the convergence of the discrete registration problem. We define the functional $F:L^2(I, H^s)\rightarrow\mathbb{R}$,
\begin{align}
  \label{eq:funccont}
   F(f) &= \int_{0}^{1}\lVert f(t, \cdot)\rVert^2_{H^s}\ dt + \int_G |S(\phi(1, \x))-T(\x)|^2\ d\x,\\\
    \phi(t, \x)&=\x + \int_{0}^{t}f(s, \x)\,ds,\qquad (t,\x)\in I\times G
\end{align}
with $S, T$ as in theorem~\ref{def:regpb}, and the approximants $F_N:L^2(I, H^s)\rightarrow\mathbb{R}$, without loss of generality with the same discretization parameter $N\in\mathbb{N}$ in time and space,
\begin{align}
  \label{eq:funcdis}
  F_N(f) &=\begin{cases}
    \frac{1}{N}\sum_{i=1}^{N}\lVert f(\tfrac{1}{N}, \cdot)\rVert^2_{H^s} + \frac{1}{N}\sum_{j=1}^{N} |S(\phi_N(1, \x_j))-T(\x_j)|^2,\qquad &f\in \mathcal{C}^{0, 1}(I, \mathcal{C}^{0, 1})\\\
    +\infty,\qquad &f\notin \mathcal{C}^{0, 1}(I, \mathcal{C}^{0, 1})
  \end{cases}\\
  \phi_N(t, \x)&=\x + \int_{0}^{t}f(s, \x)\,ds,\qquad (t,\x)\in I\times G,
\end{align}
where $(\mathbf{x}_j)_{j=1}^N$ are sampled in $G$ such that $\sum_{j=1}^{N} |S(\phi_N(1, \x_j))-T(\x_j)|^2\chi_{A_j}$ are simple functions converging to $|S(\phi_N(1, \x))-T(\x)|^2$ with supports $(A_j)_{j=1}^{N}\subset G$ of Lebesgue measure $\tfrac{1}{N}=\lambda_{\mathcal{L}}(A_j),\ \forall j$.

The next result states the $\Gamma$-convergence of $F_N$ to $F$.
\begin{theorem} Let $F_N$ and $F$ be defined as in equations~\eqref{eq:funccont} and~\eqref{eq:funcdis}, then
 \begin{equation*}
  F = \Gamma{\text -}\!\lim_{N\to\infty} F_N
 \end{equation*}
\end{theorem}
\begin{proof}
  To obtain the $\Gamma$-convergence it is sufficient to prove the limsup and liminf inequalities:
  \begin{align*}
    \forall f\in L^2(I, H^s),\ \exists (f_N)_N: f_N\to f,\quad \text{s.t.}\quad &F(f)\geq \limsup_{N\to\infty}F_N(f_N),\\
    \forall f\in L^2(I, H^s),\ \text{for every}\ (f_N)_N,\ f_N\to f,\qquad &F(f)\leq\liminf_{N\to\infty} F_N(f_N).
  \end{align*}
  The function $S\circ\phi(1, \cdot)$ is measurable thanks to the fact that $\phi$ is a diffeomorphism, so the second term of the continuous functional in equation~\eqref{eq:funccont} is well-defined. Let us prove the limsup inequality with a recovery sequence first. Due to the density of $\mathcal{C}^{0, 1}(I, \mathcal{C}^{0, 1})$ in $L^2(I, H^s)$ for every $f\in L^2(I, H^s)$ there exists $(f_N)_N\subset\mathcal{C}^{0, 1}(I, \mathcal{C}^{0, 1})$ s.t. $f_N\to f$. Thanks to the convergence of the norms, we have that the first term in equations~\eqref{eq:funccont} and~\eqref{eq:funcdis} converges. To prove that the second term converges, we need that $\phi_N\rightarrow \phi$ thanks to equation~\eqref{eq:phif}. Up to passing to subsequences, since $S$ is continuous a.e., we have the pointwise convergence a.e. of $|S(\phi_N(1, \x))-T(\x)|^2$ to $|S(\phi(1, \x))-T(\x)|^2$. Since $S, T$ and $G$ are bounded, by the dominated convergence theorem we obtain also the convergence of the second term of the functionals in~\eqref{eq:funccont} and~\eqref{eq:funcdis}.
  
  We now prove the liminf. Thanks to the convergence of the norms, we have the convergence to the first terms of the functionals in equation~\eqref{eq:funccont} and~\eqref{eq:funcdis}. The convergence $f_N\to f$ in $L^2(I, H^s)$ implies, through equation~\eqref{eq:phif}, $\phi_N\to \phi$ in $L^2(I, H^s)$ which implies, by Sobolev embedding for $s>d/2 + 1$, $\phi_N\to \phi$ in $\mathcal{C}^{1, \alpha}(I, \mathcal{C}^{1, \alpha})$ with $\alpha=s - \tfrac{d}{2} - 1>0$. Using that $S$ is continuous a.e., by Fatou's lemma,
\begin{align*}
  F(f)&=\lVert f\rVert^2_{L^2(I, H^s)}+\int_G |S(\phi(1, \x))-T(\x)|^2\ d\x \\
  &= \lVert f\rVert^2_{L^2(I, H^s)}+\int_G \liminf_{N\to\infty}|S(\phi_N(1, \x))-T(\x)|^2\ d\x\\
   &\leq\lim_{N\to\infty}\left(\int_G |S(\phi_N(1, \x))-T(\x)|^2\ d\x-\frac{1}{N}\sum_{j=1}^{N} |S(\phi_N(1, \x_j))-T(\x_j)|^2\right) + \liminf_{N\to\infty} F_N(f_N),
\end{align*}
by dominated convergence the lim of the right-hand side converges.
\end{proof}
  
  \begin{corol} [Corollary 7.20, \cite{dal2012introduction}]
    For every $N\in\mathbb{N}$, let $f_N$ be a minimizer (or $\epsilon_N$-minimizer) of $F_N$ in $L^2(I, H^s)$. If $f$ is a cluster point of $(f_N)_N$, then $f$ is a minimizer of $f$ and $F(f)=\limsup_{N\to\infty}F_N(f_N)$. If $(f_N)_N$ converges to $f$ in $L^2(I, H^s)$, then $f$ is a minimizer of $F$ and $F(f)=\lim_{N\to\infty}F_N(f_N)$.
  \end{corol}

\section{PBDW with heteroscedastic noise}
\label{appendix:pbdw}
\begin{proof}{Theorem~\ref{theo:pbdw}}
  For a fixed $z\in\mathbb{R}^{r_{\mathbf u}}$ in~\eqref{eq:pbdw_hetero}, the minimum with respect to $\eta$ is given by
  \begin{equation}\label{eq:pbdw_eta_sub}
\left(R^{-1}+KS^{-1}K\right)\pbdw{\eta} + KS^{-1}(Lz-y) = 0,
    \end{equation}
    i.e., the minimizer of \eqref{eq:pbdw_sub2}.
From \eqref{eq:pbdw_eta_sub}, using $R=SK^{-1}$, it also follows 
  \begin{equation}\label{eq:pbdw_eta_sub3}
     \pbdw{\eta} = \left[ SK^{-1}\left(R^{-1}+KS^{-1}K\right) \right]^{-1}(y-Lz) = \left( \text{Id} + K \right)(y-Lz) = W (y-Lz)\,.
    \end{equation}
Inserting \eqref{eq:pbdw_eta_sub3} into~\eqref{eq:pbdw_hetero}, it follows that $\pbdw{z}$ minimizes the function
  \begin{equation*}
  \begin{aligned}
    & (y-Lz)^TW^{-1}(R^{-1}+(SK^{-1}R^{-1})^TS^{-1}(SK^{-1}R^{-1}))W^{-1}(y-Lz) \\
    & = (y-Lz)^TW^{-1}\underbrace{\left(KS^{-1}+S^{-1}\right)}_{W S^{-1}}W^{-1}(y-Lz) = \lVert Lz-y\rVert^2_{S^{-1}W^{-1}},
  \end{aligned}
    \end{equation*}
    which completes the proof.
\end{proof}

\begin{rmk}
  The linear estimator $H_{\widehat{\mathbf{u}}}$ is biased:
  \begin{equation*}
    \mathbb{E}[H_{\widehat{\mathbf{u}}}(y+\epsilon_{y})-\Tilde{H}_{\widehat{\mathbf{u}}_{\text{PBDW}}}y] = [\Phi_r (H_{z_{\text{PBDW}}}-\Tilde{H}_{\Tilde{z}}) + \mathcal{Z}_{\mathbf u} (H_{\eta_{\text{PBDW}}}-\Tilde{H}_{\Tilde{\eta}}) + \mathcal{Z}_{\mathbf u} (H_{\eta_{\text{PBDW}}}LH_{z_{\text{PBDW}}}-\Tilde{H}_{\Tilde{\eta}}L\Tilde{H}_{\Tilde{z}})]y,
  \end{equation*}
  where 
  \begin{align*}
    &\Tilde{H}_{\widehat{\mathbf{u}}_{\text{PBDW}}} = \Phi_r \Tilde{H}_{\Tilde{z}}+\mathcal{Z}_{\mathbf u} \Tilde{H}_{\Tilde{\eta}}-\mathcal{Z}_{\mathbf u} \Tilde{H}_{\Tilde{\eta}}L\Tilde{H}_{\Tilde{z}}\\
    &\Tilde{H}_{\Tilde{z}}=H_{\Tilde{z}}y = (L^T K^{-1} L)^{-1} L^T K^{-1}\\
    &\Tilde{H}_{\Tilde{\eta}}= K^{-1}
  \end{align*}
\end{rmk}
\begin{proof}{Theorem~\ref{theo:pbdwmsq}}
  From the definition of covariance (equation~\eqref{eq:cov}) of the PBDW prediction $\widehat{\mathbf{u}}_{\text{PBDW}}$:
  \begin{align*}
    \mathbb{E}[\lVert \widehat{\mathbf{u}}^{\text{true}}-\widehat{\mathbf{u}}_{\text{PBDW}}\rVert^2_2]&\leq \mathbb{E}[\lVert \widehat{\mathbf{u}}^{\text{true}}-\mathbb{E}[\widehat{\mathbf{u}}_{\text{PBDW}}]\rVert^2_2]+\mathbb{E}[\lVert \mathbb{E}[\widehat{\mathbf{u}}_{\text{PBDW}}]-\widehat{\mathbf{u}}_{\text{PBDW}}\rVert^2_2]\\
    &= \lVert \widehat{\mathbf{u}}^{\text{true}}-\mathbb{E}[\widehat{\mathbf{u}}_{\text{PBDW}}]\rVert^2_2+\text{trace}(H_{\widehat{\mathbf{u}}_{\text{PBDW}}}SH_{\widehat{\mathbf{u}}_{\text{PBDW}}}^T).
  \end{align*}
  We consider the operator $H_l = H_{\widehat{\mathbf{u}}^{\text{PBDW}}}\circ l:\mathbb{R}^{\widehat{d}_{\mathbf u}}\rightarrow\mathbb{R}^{\widehat{d}_{\mathbf u}}$,
  \begin{align*}
    \lVert \widehat{\mathbf{u}}^{\text{true}}-\mathbb{E}[\widehat{\mathbf{u}}_{\text{PBDW}}]\rVert&=\lVert \widehat{\mathbf{u}}^{\text{true}}-H_{\widehat{\mathbf{u}}^{\text{PBDW}}}(l(\widehat{\mathbf{u}}^{\text{true}}))\rVert \\
    &= \lVert \left((\text{Id}-H_l)\circ P_{\text{Im}(H_l)}\right)(\widehat{\mathbf{u}}^{\text{true}})\rVert_2+\lVert (\text{Id}-H_l)(P_{\text{Im}(H_l)^{\perp}}\widehat{\mathbf{u}}^{\text{true}})\rVert_2\\
    &\leq \lVert(\text{Id}-H_l)\circ P_{\text{Im}(H_l)}\rVert_2\lVert \widehat{\mathbf{u}}^{\text{true}}\rVert_2+\lVert\text{Id}-H_l\rVert_2 \lVert P_{\text{Im}(H_l)^{\perp}}\widehat{\mathbf{u}}^{\text{true}}\rVert_2
  \end{align*}
  Summing and subtracting the terms $(\phi_{\text{RBF}})^{\#}({\mathbf u}^{\text{best}})$, $P_{{\text{Im}(H_l)}}(\phi_{\text{RBF}})^{\#}({\mathbf u}^{\text{best}})$, and $(\phi_{\text{RBF}})^{\#}(P_{\text{Im}(\Phi_{\mathbf u})} {\mathbf u}^{\text{best}})$:
  \begin{align*}
    \lVert P_{\text{Im}(H_l)^{\perp}}\widehat{\mathbf{u}}^{\text{true}}\rVert_2 &\leq\lVert(\phi_{\text{RBF}})^{\#}({\mathbf u}^{\text{best}})-\widehat{\mathbf{u}}^{\text{true}}-P_{{\text{Im}(H_l)}}((\phi_{\text{RBF}})^{\#}({\mathbf u}^{\text{best}})-\widehat{\mathbf{u}}^{\text{true}})\rVert_2\\
    &+\lVert (\phi_{\text{RBF}})^{\#}({\mathbf u}^{\text{best}})-(\phi_{\text{RBF}})^{\#}(P_{\text{Im}(\widehat{\Phi}_{u})} {\mathbf u}^{\text{best}})\rVert_2\\
    &+\lVert P_{\text{Im}(H_l)}(\phi_{\text{RBF}})^{\#}({\mathbf u}^{\text{best}})-(\phi_{\text{RBF}})^{\#}(P_{\text{Im}(\Phi_{\mathbf u})}{\mathbf u}^{\text{best}})\rVert_2,
  \end{align*}
  we remark that $\text{Im}(H_l) = \text{Im}(\widehat{\Phi}_{u})+\left(\text{Im}(\widehat{\Phi}_{u})^{\perp}\cap\text{Im}(\mathcal{Z}_{\mathbf u})\right)$. The RBF interpolation operator with thin-plate basis $I_{\text{RBF}}:\mathbb{R}^3\rightarrow\mathbb{R}^3$ is Lipshitz with constant $C_{\text{RBF}}$. Setting $C=C_{\text{RBF}}\sup_{\mathbf{x}\in \Omega_{S}} \lVert\text{det}(\nabla \phi_1(\mathbf{x}))\rVert_2$:
  \begin{align*}
    \lVert (I_{\text{RBF}}\circ\phi_1)^{\#}({\mathbf u}^{\text{best}})-(I_{\text{RBF}}\circ\phi_1)^{\#}(P_{\text{Im}(\widehat{\Phi}_{u})} {\mathbf u}^{\text{best}})\rVert_2\leq C\lVert {\mathbf u}^{\text{best}}-P_{\text{Im}(\widehat{\Phi}_{u})} {\mathbf u}^{\text{best}}\rVert_2,
  \end{align*}
  applying the change of variables formula.
\end{proof}

\begin{rmk}[Sources of error]
  We will describe the different sources of error in the right hand side of theorem~\ref{theo:pbdwmsq}. The fist term $\text{trace}(H_{\widehat{\mathbf{u}}_{\text{PBDW}}}SH_{\widehat{\mathbf{u}}_{\text{PBDW}}}^T)$ accounts for the uncertainty $\epsilon_y$ in the observations $y$: for exact observations it is zero.
  The second term $\lVert(\text{Id}-H_l)\circ P_{\text{Im}(H_l)}\rVert_2$ accounts for the additional term $\lVert \eta\rVert_{R^{-1}}$ in equation~\ref{eq:pbdw_hetero}: if this term would be removed than we would have $\lVert(\text{Id}-H_l)\circ P_{\text{Im}(H_l)}\rVert_2=0$ as $H_l$ would be the exact linear projection into $\text{Im}(H_l) = \text{Im}(\widehat{\Phi}_{u})+\left(\text{Im}(\widehat{\Phi}_{u})^{\perp}\cap\text{Im}(\mathcal{Z}_{\mathbf u})\right)$, see also~\cite{gong2019pbdw}.
  The PBDW stability constant $\lVert\text{Id}-H_l\rVert_2$ reduces to $\beta^{-1}(\Phi_{\widehat{\mathbf{u}}},  \mathcal{Z}_{\mathbf u})$ with $\beta(\Phi_{\widehat{\mathbf{u}}},  \mathcal{Z}_{\mathbf u})= \text{inf}_{z\in\text{Im}(\widehat{\Phi}_{u})}\sup_{\eta\in\text{Im}(\mathcal{Z}_{\mathbf u})} \frac{(z,\eta)}{\lVert z\rVert_2\lVert \eta\rVert_2}$, when we remove the regularization term $\lVert \eta\rVert_{R^{-1}}$ in equation~\ref{eq:pbdw_hetero}, obtaining the original noiseless PBDW formulation, see also~\cite{gong2019pbdw}.
  The fourth term:
  \begin{equation*}
    \lVert(\phi_{\text{RBF}})^{\#}({\mathbf u}^{\text{best}})-\widehat{\mathbf{u}}^{\text{true}}-P_{{\text{Im}(H_l)}}((\phi_{\text{RBF}})^{\#}({\mathbf u}^{\text{best}})-\widehat{\mathbf{u}}^{\text{true}})\rVert_2
  \end{equation*}
  represents the approximation error of the velocity fields in the new geometry $\widehat{\mathbf{u}}^{\text{true}}$ with the transported template manifold $(\phi_{\text{RBF}})^{\#}(X^{\text{train}}_{ut})$: it is zero if either $(\phi_{\text{RBF}})^{\#}({\mathbf u}^{\text{best}})=\widehat{\mathbf{u}}^{\text{true}}$, that is the velocity field $\widehat{\mathbf{u}}^{\text{true}}$ is contained in the transported solution manifold, or $\widehat{\mathbf{u}}^{\text{true}}=(\phi_{\text{RBF}})^{\#}({\mathbf u}^{\text{best}})+x$, with $x\in\text{Im}(\widehat{\Phi}_{u})$, if the error committed in approximating $\widehat{\mathbf{u}}^{\text{true}}$ with the transported solution manifold does not affect PBDW.
  The fifth term $C\cdot\lVert {\mathbf u}^{\text{best}}-P_{\text{Im}(\widehat{\Phi}_{u})} {\mathbf u}^{\text{best}}\rVert_2$ includes a multiplicative constant $C>0$, that accounts for the volume deformation from the template to the target geometry, and the reconstruction rSVD error in the template geometry studied in section~\ref{subsec:sml_rec}. The last term
  \begin{equation*}
    P_{\text{Im}(\widehat{\Phi}_{u})^T\cap\text{Im}(\mathcal{Z}_{\mathbf u})}(\phi_{\text{RBF}})^{\#}({\mathbf u}^{\text{best}})+P_{\text{Im}(\widehat{\Phi}_{u})}(\phi_{\text{RBF}})^{\#}({\mathbf u}^{\text{best}})-(\phi_{\text{RBF}})^{\#}(P_{\text{Im}(\Phi_{\mathbf u})}{\mathbf u}^{\text{best}}),
  \end{equation*}
  is a compatibility condition related to the commutativity of the diagram:
  \[ \begin{tikzcd}
    \mathbb{R}^{d_{\mathbf u}} \arrow{r}{P_{\text{Im}(\Phi_{\mathbf u})}} \arrow[swap]{d}{\phi_{\text{RBF}}} & \text{Im}(\Phi_{\mathbf u})\subset\mathbb{R}^{d_{\mathbf u}} \arrow{d}{\phi_{\text{RBF}}} \\%
    \mathbb{R}^{\widehat{d}_{\mathbf u}} \arrow{r}{P_{\text{Im}(\widehat{\Phi}_{u})}}& \text{Im}(\widehat{\Phi}_{u})\subset\mathbb{R}^{\widehat{d}_{\mathbf u}}
  \end{tikzcd}
  \]
\end{rmk}

\begin{rmk}[Registration map error]
  In theorem~\ref{theo:pbdwmsq}, we have not included the RBF interpolation error and the error in the approximation of the exact registration map $\phi_{1}$ with $\phi_{1,N}$, with the notations of theorem~\ref{theo:existreg}: let $u:\Omega_S\subset\mathbb{R}^3\rightarrow\mathbb{R}^3$ be a velocity field on the template computational domain $\Omega_S$:
  \begin{align*}
    \lVert &(I_{\text{RBF}}\circ \phi_{1,N})^{\#}(u)-(\phi_{1})^{\#}(u)\rVert_{L^2(\phi_1(\Omega_S))}\leq\\
    &\lVert (I_{\text{RBF}}\circ \phi_{1,N})^{\#}(u)-(\phi_{1,N})^{\#}(u)\rVert_{L^2(\phi_1(\Omega_S))}+
    \lVert(\phi_{1,N})^{\#}(u)-(\phi_{1})^{\#}(u)\rVert_{L^2(\phi_1(\Omega_S))}\\
    &\leq C h^2_{\phi_1(X_S), \Omega_T}\rho_{\phi_1(X_S), \Omega_T}^{1/2}\lVert(\phi_{1})^{\#}(u)\rVert_{L^2(\phi_1(\Omega_S))} + \sup_{\mathbf{x}\in\Omega_S}\lVert \nabla u(\mathbf{x})\rVert_{L^2(\phi_1(\Omega_S))}\cdot \lVert \phi_{1, N}^{-1}-\phi_1^{-1}\rVert_{L^2(\phi_1(\Omega_S))}\\
    &\leq C h^2_{\phi_1(X_S), \Omega_T}\rho_{\phi_1(X_S), \Omega_T}^{1/2}\lVert(\phi_{1})^{\#}(u)\rVert_{L^2(\phi_1(\Omega_S))} + \sup_{\mathbf{x}\in\Omega_S}\lVert \nabla u(\mathbf{x})\rVert_{L^2(\phi_1(\Omega_S))}\cdot C_3\cdot N^{-\frac{m}{2(d+1)}},\qquad \forall m\geq 1,
  \end{align*}
  where we have applied theorem~\ref{theo:existreg} with $n=0$, and to bound the RBF interpolation error we have applied theorem 4.2~\cite{narcowich2006sobolev} with $\beta=2$, since $(\phi_{1,N})^{\#}(u)\in\mathcal{C}^{2, 1}(I, \mathcal{C}^{2, 1})\subset W^{2,2}(\phi_1(\Omega_S))$, and $\tau=5/2$, since the Fourier transform of thin-plate spline basis~\cite{schaback1999improved} decays as $\lVert\omega\rVert_2^{-5}$ in frequency space $\omega\in\mathbb{R}^3$. The constants introduced are defined as:
  \begin{equation*}
    h_{\phi_1(X_S), \Omega_T} = \sup_{\mathbf{x}\in\Omega_T}\inf_{\mathbf{x}_j\in \phi_1(X_S)}\lVert\mathbf{x}-\mathbf{x}_j\rVert_2,\qquad \rho_{\phi_1(X_S), \Omega_T}=\frac{h_{\phi_1(X_S), \Omega_T}}{\frac{1}{2}\min_{j\neq k}\lVert\mathbf{x}_i-\mathbf{x}_j\rVert_2},
  \end{equation*}
  following the notation in~\cite{narcowich2006sobolev}.
\end{rmk}

% \section{EPD-GNNs on registered datasets}
% \label{appendix_epd-gnns}

% We test a new framework, that involves registration, for inference with neural networks on different meshes: our dataset, instead of consisting of velocity and pressure fields supported on their original meshes, is represented by the collection of registered velocity and pressure fields supported on the reference geometry. The registration procedure is explained in section~\ref{sec:registration}.

% We employ encode-process-decode graph neural networks (EPD-GNN) implemented in NVIDIA-Modulus~\cite{modulus} open-source library, and introduced in~\cite{pfaff2020learning}: they represent the state of the art GNN architectures to perform inference on computational meshes. Due to the size of the meshes we are considering and the limited computational budget we had at disposal, we coarsened with Tetgen~\cite{Si2015} the size of the reference mesh from $110676$ vertices to $n_{\text{vertices}}=5181$. Despite representing a relevant reduction of the resolution of the pressure and velocity field, the errors on the test dataset are mainly due to the extremely scarce data regime. Future directions of research involve increasing the computational budget and the training dataset. Additional solutions to reduce the computational costs of training a EPD-GNN include multi-resolution GNNs\TODO{refs}. The velocity and pressure fields are transported from fine to coarse meshes and back through RBF interpolation. The errors are always evaluated on the fine target geometries. We employ the nearest-neighbours algorithm to enrich each vertex with $9$ edges to the closest $9$ vertices, for a total of $n_{\text{edges}}=53748$ edges. We will consider only undirected graphs.

% We consider two inference problems.

% \paragraph*{Geometry to velocity (\textit{gnn-gv}) and pressure fields (\textit{gnn-gp}) inference}
% The input represents a geometrical encoding of the target computational domains with additional velocity b.c.. For each target domain, we evaluate the scalar field that represents the distance from the centerline. The \textit{pllback} of this scalar field to the reference geometry through the registration map together with the \textit{pllback} of the coordinates of the vertices of the target geometry is our $4$-dimensional geometrical encoding. This geometrical encoding is then embedded with $10$ fourier features~\cite{sutherland2015error} through the maps $\{\cos(2^{i}W\cdot\mathbf{z}), \sin(2^{i}W\cdot\mathbf{z})\}_{i=0}^{9}$, where $W\in\mathbb{R}^{4\times 4}$ columns are sampled independently from a multivariate normal distribution and $\mathbf{z}\in\mathbb{R}^4$ is an arbitrary input vector, for a total of $10\cdot 2\cdot 4=80$ geometrical input features. To these it is added the velocity field at $n_{t,\text{GNN}}=8$ times $t\in\{0.05s, 0.075s, 0.1s, 0.125s, 0.15s, 0.2s, 0.225s\}$ restricted at the boundaries $\Gamma_i$ $i\in\{1, 2, 3, 4, 5\}$ of the target domains and then pulled back to the reference geometry. The value of the velocity boundary field is zero inside the computational domain $\overline{\Omega}\backslash \cup_{i=1}^{5}\Gamma_i$. The total dimension of the inputs is thus $80+n_{t,\text{GNN}}\cdot 3=104$, where $3$ refers to the number of components of the velocity field. Each edge between the vertices $\mathbf{x}_i$ and $\mathbf{x}_j$ has as features, the vector $\mathbf{x}_i-\mathbf{x}_j$, its $L^2$-norm $\lVert\mathbf{x}\rVert_2$, and the difference between the values of the geometrical encoding and the boundary pressure field at $n_{t,\text{GNN}}=8$ time instances at the nodes $\mathbf{x}_i$ and $\mathbf{x}_j$, for a total of $4+80+8\cdot 3=108$ edge features, $3$ stands for the components of the velocity field. The output is the pressure field $p\in\mathbb{R}^{n_{\text{vertices}}\times n_{t,\text{GNN}}}$ at $n_{t,\text{GNN}}=8$ times $t\in\{0.05s, 0.075s, 0.1s, 0.125s, 0.15s, 0.2s, 0.225s\}$ supported on the coarse reference mesh. 

% \paragraph*{Velocity to pressure (\textit{gnn-vp}) field inference}
% The input is the velocity field $u\in\mathbb{R}^{n_{\text{vertices}}\times 3n_{t,\text{GNN}}}$ at $n_{t,\text{GNN}}=8$ times $t\in\{0.05s, 0.075s, 0.1s, 0.125s, 0.15s, 0.2s, 0.225s\}$ supported on the coarse reference mesh with $n_{\text{vertices}}=5181$ vertices and $n_{\text{edges}}=53748$ edges. Each edge between the vertices $\mathbf{x}_i$ and $\mathbf{x}_j$ has as features, the vector $\mathbf{x}_i-\mathbf{x}_j$, its $L^2$-norm $\lVert\mathbf{x}\rVert_2$, and the difference between the values of the velocity field at $n_{t,\text{GNN}}=8$ time instances at the nodes $\mathbf{x}_i$ and $\mathbf{x}_j$, for a total of $4+8\cdot 3=28$ edge features, $3$ stands for the components of the velocity field. The output is the pressure field $p\in\mathbb{R}^{n_{\text{vertices}}\times n_{t,\text{GNN}}}$ at $n_{t,\text{GNN}}=8$ times $t\in\{0.05s, 0.075s, 0.1s, 0.125s, 0.15s, 0.2s, 0.225s\}$ supported on the coarse reference mesh. \newline

% As our EPD-GNN model we choose the \textit{MeshGraphNet} architecture implemented in NVIDIA-Modulus~\cite{modulus}. The hyperparameters are the width of the network $w$, i.e. the number of consecutive EPD layers, and the common hidden dimension $h$ of the node encoder and decoders and the edge encoder. We will consider the following values $(w,h)\in\{(10, 64), (15, 128), (20, 256)\}$ in our hyperparameter studies.

% The loss is the relative mean squared error. We apply the ADAM sthochastic optimization method~\cite{kingma2014adam} to train the EPD-GNNs with a scheduler used to half the learning rate when the validation error does not decrease after $100$ epochs. The number of epochs is $500$ and the initial learning rate value is $0.001$. We split the $724$ available geometries in $720$ training and $4$ validation data. The EPD-GNN weights associated to the best validation error are chosen at the end of the optimization process with $500$ epochs. The $52$ test geometries are not employed during the training. We perform the optimization on a single GPU NVIDIA A100-SXM4 with 40GB of graphics RAM size.

% In figure~\ref{fig:overfitting}, a comparison of the training and validation mean squared loss, computed on the coarse mesh with $n_{\text{vertices}}=5181$ vertices, highlights a clear overfitting phenomenon in our limited data regime of $720$ training geometries. From the convergence behavior of the training error, we are hopeful that increasing the training dataset would bring better results. The optimal way to increase the training dataset is a future direction of research. 

% \begin{figure}[!htp]
%   \centering
%   \includegraphics[width=1\textwidth]{img/gnn_train.pdf}
%   \caption{Hyperparameter studies with $(w,h)\in\{(10, 64), (15, 128), (20, 256)\}$, where $w$ is the number of EPD layers and $h$ is the hidden dimension. The loss of the training dataset of $720$ geomeries and validation dataset of $4$ is shown over $500$ epochs.}
%   \label{fig:overfitting-app}
% \end{figure}

% In figure~\ref{fig:epdgnn_reg_no_reg}, we compare the mean $L^2$-relative error over the $52$ test target geometries for the predictions of the EPD-GNNs architectures trained with datasets registered on the template or supported on the original target geometries. Also in the case, we will consider only coarsened meshes obtained with Tetgen~\cite{Si2015}, with the same coarsening parameters, for computational budget limits. The definition of the loss, input and outputs, optimization are the same for registered and non-registered datasets. We can observe that registrations represents an efficient encoding of the geometrical and physical features of the problem: for each inference problem \textit{gnn-vp}, \textit{gnn-gp}, and \textit{gnn-gv}, the EPD-GNNs trained on registered data perform better than their alternatives on non-registered data \textit{gnn-vp-no-reg}, \textit{gnn-gp-no-reg}, and \textit{gnn-gv-no-reg}.

% \begin{figure}[!htp]
%   \centering
%   \includegraphics[width=0.8\textwidth]{img/gnn_reg.pdf}
%   \caption{$L^2$-relative errors $\epsilon_{\widehat{\mathbf{u}}}$ and $\epsilon_{\widehat{p}}$ computed in the target computational domains of the predicted velocity (\textit{gnn-gv},\textit{gnn-gv-no-reg}) and pressure fields (\textit{gnn-gp},\textit{gnn-gp-no-reg}, \textit{gnn-vp},\textit{gnn-vp-no-reg}) using EPD-GNN models with and without registration of the datasets.}
%   \label{fig:epdgnn_reg_no_reg-app}
% \end{figure}

\section{Reduced order PPE and STE}
\label{apendix:rom}

The reduced PPE and STE can be defined from their matrix forms in Equations~\eqref{eq:ppe} and~\eqref{eq:ste} respectively:
\begin{align*}
  \widehat{\Phi}_{p}^T A_{\text{PPE}}\,\widehat{\Phi}_{p}z_{\widehat{p}^{n+1/2}_{\text{PPE}}} = \widehat{\Phi}_{p}^T M_{\text{PPE}}^{n+1}\widehat{\ub}_{n+1} - \widehat{\Phi}_{p}^T M_{\text{PPE}}^{n}\widehat{\ub}_{n} + \widehat{\Phi}_{p}^T Q_{\text{PPE}}(\widehat{\ub}^{n+1/2}, \widehat{\ub}^{n+1/2}),
\end{align*}
\begin{align*}
  \widehat{\Phi}_{u}^T A_{\text{STE}}\,\widehat{\Phi}_{u}\mathbf{z}_{\wb} + \widehat{\Phi}_{u}^T B \widehat{\Phi}_{p}z_{\widehat{p}^{n+1/2}_{\text{STE}}} &= \widehat{\Phi}_{u}^T M_{\text{STE}}^{n+1}\widehat{\ub}_{n+1} - \widehat{\Phi}_{u}^T M_{\text{STE}}\widehat{\ub}_{n} + \widehat{\Phi}_{u}^T Q_{\text{STE}}(\widehat{\ub}^{n+1/2}, \widehat{\ub}^{n+1/2}) + \widehat{\Phi}_{u}^T M_{\text{STE}}\widehat{\ub}_{n+1/2},\\
     \widehat{\Phi}_{p}^T B^T\widehat{\Phi}_{u}\mathbf{z}_{\wb} &= 0,
\end{align*}
after left multiplication with the corresponding rSVD transported basis $\widehat{\Phi}_{u}\in\mathbb{R}^{r_{\mathbf u}\times \widehat{d}_{\mathbf u}}$ and $\widehat{\Phi}_{p}\in\mathbb{R}^{r_p\times \widehat{d}_p}$ and subsitution of $\widehat{p}^{n+1/2}_{\text{PPE}}\in\mathbb{R}^{\widehat{d}_p},\widehat{p}^{n+1/2}_{\text{STE}}\in\mathbb{R}^{\widehat{d}_p}, \wb\in\mathbb{R}^{\widehat{d}_{\mathbf u}}$ with the respective reduced variables $z_{\widehat{p}^{n+1/2}_{\text{PPE}}}\in\mathbb{R}^{r_p},z_{\widehat{p}^{n+1/2}_{\text{STE}}}\in\mathbb{R}^{r_p}, z_{\wb}\in\mathbb{R}^{r_{\mathbf u}}$.
% The assembly cannot be performed offline, since to transport the rSVD modes from the reference to the target geometry the definition of the registration map is needed. However, once the registration map is evaluated and the rSVD transposrted to the target geometry, the assembly of the reduced systems can be performed in parallel and solved with less computational costs thanks to the lower dimensionality of the reduced systems, related to the reduced dimensions $r_{\mathbf u}, r_p$.

The velocity rSVD modes $\widehat{\Phi}_{u}$ are enriched with the supremizer technique~\cite{ballarin2015supremizer} with additional $r_p$ modes computed from the pressure rSVD modes $\widehat{\Phi}_{p}$, for a total of $r_{u,\text{sup}}=r_{\mathbf u}+r_p$ velocity modes $\Phi_{\widehat{\mathbf{u}},\text{sup}}\in\mathbb{R}^{r_{u,\text{sup}}\times \widehat{d}_{\mathbf u}}$:
\begin{align*}
  \Phi_{\widehat{\mathbf{u}},\text{sup}}^T A_{\text{STE}}\,\Phi_{\widehat{\mathbf{u}},\text{sup}}\mathbf{z}_{\wb} &+ \Phi_{\widehat{\mathbf{u}},\text{sup}}^T B \widehat{\Phi}_{p}z_{\widehat{p}^{n+1/2}_{\text{STE}}} = \\
  &=\Phi_{\widehat{\mathbf{u}},\text{sup}}^T M_{\text{STE}}\widehat{\ub}_{n+1} - \Phi_{\widehat{\mathbf{u}},\text{sup}}^T M_{\text{STE}}\widehat{\ub}_{n} + \Phi_{\widehat{\mathbf{u}},\text{sup}}^T Q_{\text{STE}}(\widehat{\ub}^{n+1/2}, \widehat{\ub}^{n+1/2}) + \Phi_{\widehat{\mathbf{u}},\text{sup}}^T M_{\text{STE}}\widehat{\ub}_{n+1/2},\\
    &\qquad\,\,\widehat{\Phi}_{p}^T B^T\Phi_{\widehat{\mathbf{u}},\text{sup}}\mathbf{z}_{\wb} = 0.
\end{align*}
In principle, the supremizer enrichment could be performed offline on the template geometry and then transported with the registration map on the new patient geometry. It may occur that the method is yet not stable and additional corrections to the supremizers must be implemented. This is a future research direction. For the moment, we evaluted the supremizers directly on the test geometries.

\section{Computational costs}
\label{appendix:costs}

In Table~\ref{tab:costs} are reported the computational costs of all the numerical procedures introduced. The average cost of high-fidelity simulations with respect to the total number of training and test geometries is reported. To be efficient, a data assimilation method/surrogate model must achieve a considerable speed-up with respect to high-fidelity simulations, possibly employing less computational resources. This is possible because, in the case of surrogate models, the accuracy of the predictions is reduced; in the data assimilation case, the accuracy is inherently dependent on the noise level. Moreover, the offline costs such as rSVD evaluation, \textit{gnn-gp}, \textit{gnn-gv}, \textit{gnn-vp-pbdw} training, and the supremizer enrichment (SUP), are amortized over possibly infinite online queries.

In principle, rSVD computations are performed on 1 CPU but could be implemented in parallel. Data-parallelism on multiple GPUs could be easily applied to the \textit{gnn-gp}, \textit{gnn-gv}, and \textit{gnn-vp-pbdw} training. Model-parallelism could be useful to extend the EPD-GNN architecture to the whole mesh: we remark that we employ a coarser mesh as support of the EPD-GNN architecture because of memory constraints on the single $40GB$ RAM GPU we employ. The timings for a single time instance prediction with \textit{gnn-gp}, \textit{gnn-gv}, and \textit{gnn-vp-pbdw} are reported. They could also be efficiently parallelized for multiple time instances, as well as RBF interpolation.

Our implementation of RBF interpolation (RBFv, RBFp, RBF) between template and target aorta meshes (or coarse and fine meshes in the EPD-GNNs cases) relies on \texttt{SciPy}~\cite{2020SciPy-NMeth}: we employ a fixed number of $30$ neighbours for each evaluation point. However, a distributed-memory parallel implementation of RBF interpolation could be implemented, partitioning the target aorta mesh. The velocity and pressure modes transported from the template to the target geometries are not orthonormal anymore, so an efficient QR orthonormalization step (QRv, QRp) must be applied online. This step could also be parallelized.

The reduced-order models of the PPE and STE pressure estimators need to load and assemble online the right-hand sides (RHS) coming from the velocity field PBDW predictions. To do this efficiently, we have implemented a parallel version of PPE-ROM and STE-ROM. We have reported the costs for the assembly (ASS) of the RHS along with the costs for a single time instance solve (SOL) of the reduced systems: PPE-ROM reduced systems are of size $r_p\times r_p$, while STE-ROM reduced systems are of size $(2r_p+r_{\mathbf u})\times (2r_p+r_{\mathbf u})$ due to the employment of supremizers. As mentioned in appendix~\ref{apendix:rom}, we compute the supremimzers online, but in principle they could be computed offline.

The PBDW predictions for a single time instance are performed on a single CPU. In our context, the dimension of the observations $(M_{\text{voxels}}+1)\sim30000$ is greater than the number of velocity modes employed $(M_{\text{voxels}}+1)\gg r_{\mathbf u}\in\{500, 1000, 2000\}$: we solve for a generalized least-squares problem of dimension $(M_{\text{voxels}}+1)\times r_{\mathbf u}$ in equation~\eqref{eq:1pbdw} and a linear correction of dimension $(M_{\text{voxels}}+1)\times(M_{\text{voxels}}+1)$, in equation~\eqref{eq:2pbdw}. The PBDW predictions could be sped up reducing the resolution of the observations or with distributed-memory parallelism, partitioning the mesh supported on the voxels centroids. Moreover, the employment of a heteroscedastic noise model with respect to a homoscedastic one increases significantly the computational burden.

\begin{table}[hbtp!]
  \centering
  \caption{Computational costs of the numerical procedures introduced along with computational resources employed. Elapsed real times are shown. Offline stage costs are reported in blue, online stage costs in red. ResNet-LDDMM registration is performed both offline and online.}
  \footnotesize
  \begin{tabular}{|c|}
    \hline
    ResNet-LDDMM registration (1 GPU)\\
    \hline
    \hline
    $\sim$ 10 [min] \\
    \hline
\end{tabular}
\vspace{0.25cm}
\begin{tabular}{|>{\columncolor{blue!20}}c|}
    \hline
    Single high-fidelity simulations (72 CPUs)\\
    \hline
    \hline
    2.64 [h] \\
    \hline
\end{tabular}
\begin{tabular}{| c| >{\columncolor{blue!20}}c| >{\columncolor{blue!20}}c |}
  \hline
  & rSVD $u$ (1 CPU)& rSVD $p$ (1 CPU)\\
  \hline
  \hline
  $r=500$ & 34 [min] & 10 [min]\\
  \hline
  $r=1000$ & 36 [min] & 12 [min]\\
  \hline
  $r=2000$ & 38 [min] & 13 [min]\\
  \hline
  $r=4000$ & 51 [min] & 18 [min]\\
  \hline
\end{tabular}

\vspace{0.25cm}
\begin{tabular}{| c | >{\columncolor{blue!20}}c | >{\columncolor{red!20}}c |>{\columncolor{blue!20}}c | >{\columncolor{red!20}}c |}
  \hline
    & \textit{gnn-gp} train (1 GPU)  & \textit{gnn-gp} predict (1 GPU) & \textit{gnn-gv} train (1 GPU)  & \textit{gnn-gv} predict (1 GPU)\\
  \hline
  \hline
  $e=9,w=10,h=64$ &35 [h]& 0.65 [s] + RBF 5.5 [s] &37 [h]& 0.67 [s] + RBF 5.5 [s]\\
  \hline
  $e=9,w=15,h=128$ &39 [h]& 0.89 [s] + RBF 5.5 [s] &34 [h]& 0.92 [s] + RBF 5.5 [s] \\
  \hline
  $e=9,w=20,h=256$ &43 [h]& 1.21 [s] + RBF 5.5 [s] &43 [h]& 1.29 [s] + RBF 5.5 [s]\\
  \hline
\end{tabular}

\vspace{0.25cm}
\begin{tabular}{| c |>{\columncolor{blue!20}}c | >{\columncolor{red!20}}c |}
  \hline
    &\textit{gnn-vp} train (1 GPU) & \textit{gnn-vp} predict (1 GPU)\\
  \hline
  \hline
  $e=9,w=10,h=64$ &21 [h]& 0.8 [s] + RBF 5.5 [s] \\
  \hline
  $e=9,w=15,h=128$ &40 [h]& 1.08 [s] + RBF 5.5 [s] \\
  \hline
  $e=9,w=20,h=256$ &40 [h]& 1.42 [s] + RBF 5.5 [s]\\
  \hline
\end{tabular}

\vspace{0.25cm}
\begin{tabular}{| >{\columncolor{red!20}}c | >{\columncolor{red!20}}c |}
  \hline
   PPE-FOM  (32 CPUs) & STE-FOM (32 CPUs) \\
   \hline
   \hline
    SOL 1.61 [s] & SOL 6.33 [s]\\
   \hline
\end{tabular}

\vspace{0.25cm}
\begin{tabular}{| c | >{\columncolor{red!20}}c | >{\columncolor{red!20}}c |}
  \hline
   & PPE-ROM  ASS (32 CPUs) & PPE-ROM  SOL+RBF+QR (1 CPU) \\
  \hline
  \hline
  $r_p=500$ & ASS 100 [ms] & SOL 0.006 [s] + RBFp 28 [s] + QRp 4.4 [s]\\
  \hline
  $r_p=1000$ & ASS 112 [ms] & SOL 0.03 [s] + RBFp 38 [s] + QRp 17 [s]\\
  \hline
  $r_p=2000$ & ASS 105 [ms] & SOL 0.34 [s] + RBFp 50 [s] + QRp 46 [s]\\
  \hline
\end{tabular}

\vspace{0.25cm}
\begin{tabular}{| c | >{\columncolor{red!20}}c | >{\columncolor{red!20}}c |}
  \hline
  $r_{\mathbf u}=1000$ Fixed & STE-ROM  ASS+SUP (32 CPUs) & STE-ROM SOL+RBF+QR (1 CPU) \\
  \hline
  \hline
  $r_p=500$ & ASS 111 [ms]+SUP 461 [s] & SOL 0.31 [s]+RBFv 32 [s]+ QRv 14 [s]+ RBFp 28 [s] + QRp 4.4 [s]\\
  \hline
  $r_p=1000$ & ASS 130 [ms]+SUP 1125 [s] & SOL 1.02 [s]+RBFv 63 [s]+ QRv 44 [s]+ RBFp 38 [s] + QRp 17 [s]\\
  \hline
  $r_p=2000$ & ASS 123 [ms]+ SUP 2186 [s] & SOL 3.1 [s]+RBFv 112 [s]+ QRv 143 [s]+ RBFp 50 [s] + QRp 46 [s]\\
  \hline
\end{tabular}

\vspace{0.25cm}
\begin{tabular}{| c | >{\columncolor{red!20}}c |}
  \hline
   & PBDW predict single time instant (1 CPU)\\
  \hline
  \hline
  $r_{\mathbf u}=500$ & PBDW 32.4 [s] + RBFv 32 [s] + QRv 14 [s]\\
  \hline
  $r_{\mathbf u}=1000$ & PBDW 74.2 [s] + RBFv 63 [s] + QRv 44 [s]\\
  \hline
  $r_{\mathbf u}=2000$ & PBDW 74.9 [s] + RBFv 112 [s] + QRv 143 [s]\\
  \hline
\end{tabular}
  \label{tab:costs}
\end{table}