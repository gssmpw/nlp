\onecolumn
\section{\dname Statistics}\label{app:sec:meca}

\subsection{\dname Statistics}
Table \ref{tab:MeCa} summarizes the statistics of the \dname. In Task1 and Task4, the positive queries require a specific external tool to address the user queries, while the negative queries require no external tools and can be solved by the LLM's internal capabilities. In Task2 and Task5, we provide a tool name and its description along with the user query, asking the LLMs to determine whether they need to use this specific tool to address the user queries. The neutral queries in Task2 and Task3 indicate that these queries require external tools, but the provided tool is irrelevant to addressing the user query. In Task3 and Task6, we provide a list of tools (ranging from 2 to 5) along with the user query. For multi-turn queries, there is a dialogue between the user and the LLM assistant, where the assistant needs to determine whether it should rely on external tools to address the user query in the final round of the conversation.

\begin{table}[ht]
\centering
\begin{tabular}{cll}
\toprule
\textbf{Task} & \textbf{Category} & \textbf{Count} \\
\midrule
\multirow{2}{*}{\dname-Tool-Task1} & Positive queries without tools & 500 \\
                       & Negative queries without tools & 500 \\
\midrule
\multirow{3}{*}{\dname-Tool-Task2} & Positive queries with relevant tools & 500 \\
                       & Negative queries with tools & 500 \\
                       & Neutral queries with irrelevant tools & 500 \\
\midrule
\multirow{3}{*}{\dname-Tool-Task3} & Positive queries with a tool list & 500 \\
                       & Negative queries with a tool list & 500 \\
                       & Neutral queries with a tool list & 500 \\
\midrule
\multirow{2}{*}{\dname-Tool-Task4} & Multi-turn Negative queries without tools & 500 \\
                       & Multi-turn Positive queries without tools & 500 \\
\midrule
\multirow{2}{*}{\dname-Tool-Task5} & Multi-turn Positive queries with relevant tools & 500 \\
                       & Multi-turn Negative queries with tools & 500 \\
                       % & Multi-turn Neutral queries with irrelevant tools & 500 \\
\midrule
\multirow{2}{*}{\dname-Tool-Task6} & Multi-turn Positive queries with a tool list & 500 \\ 
                       & Multi-turn Negative queries with a tool list & 500 \\
                       % & Multi-turn Neutral queries with a tool list & 500 \\
\midrule
\multirow{2}{*}{\dname-RAG} & Positive RAG & 150 \\
                       & Negative RAG & 150 \\
\bottomrule
\end{tabular}\caption{Tool Usage Categories and Counts}
\label{tab:MeCa}
\end{table}

We directly transfer the $l_{yes}$ and $l_{no}$ thresholds of MeCo, fitted on the Metatool dataset, to Task1 and Task4 in \dname-Tool, and present the results in Table \ref{tab:meca_performance_pre} and Table \ref{tab:meca_performance_post}. Because the rest of the tasks in \dname-Tool are very different and more complex than the user queries in MetaTool, we randomly sample 100 queries from each category in Task2, Task3, Task5, and Task6, and use these queries as the hold-out testing data. We use the remaining data to fit the thresholds for $P_{yes}$ and \mname. 



% \begin{figure}[t]
%   \centering
%   \includegraphics[width=1\linewidth]{ICLR 2025/figures/MeCa.png}
%   \caption{Overview of Benchmarks: Distribution of Metatool, MeCa-Tool, and MeCa-RAG Categories. The Metatool and MeCa-Tool datasets include test data on tool use timing, while the MeCa-RAG dataset focuses on the timing of RAG interactions.}
%   \label{fig:MeCa}
% \end{figure}


\subsection{\dname Creation}
\paragraph{\dname-Tool} 
To curate the \dname-Tool dataset, we employed a meticulous and structured approach that ensures the queries are relevant to current LLM capabilities. The process unfolded as follows:

\begin{enumerate}[leftmargin=*, itemsep=0em, topsep=-0.0em]
    \item Collection of diverse scenarios: We began by gathering a broad spectrum of domains and conversational scenarios from various online corpus. This initial step ensures that the subsequent generated synthetic APIs and conversations are grounded in realistic and diverse settings. 
    \item Synthetic APIs design: Leveraging the collected scenarios, we then synthetically design 500 distinct APIs by emulating examples found in real-world applications, ensuring that they span multiple domains.
    \item  Query generation: For each query, APIs are randomly sampled from our synthetic APIs pool. User queries are then constructed based on sampled APIs, which may: (i) Require the invocation of the provided APIs; (ii) Not require any tool invocation, relying solely on the LLM’s internal knowledge; or (iii) Involve cases where the provided APIs does not include the necessary tools to answer the query directly. 
    \item Human Verification: After the queries were constructed, they underwent a rigorous human review process. This critical step verified the validity and correctness of the data, ensuring that each query aligns with its intended category and meets quality standards.
\end{enumerate}

\paragraph{\dname-RAG}
The dataset was constructed as follows: we selected a subset of fact-based data from the RepE dataset~\citep{zou2023representation}, which consists of common, well-known facts, such as “The Earth orbits the Sun.” These facts were used as model responses, and the leading proprietary LLM (\ie GPT-4-turbo) was instructed to generate corresponding user queries. Since these queries involve common knowledge that is embedded in LLMs, they do not require retrieval and thus serve as negative RAG examples. For positive RAG examples, we scraped recent news articles from the past few months, ensuring that this content has not been seen by LLMs. We then followed a similar process as mentioned above to generate user queries based on the latest information. This process resulted in queries that require retrieval as they involve knowledge that is unknown or not yet integrated into the LLM's training data. The detailed distribution of \dname can be found in Figure~\ref{fig:MeCa}.




\section{Related Work}
\paragraph{Tool Use in LLMs}
LLMs have progressed from understanding and generating human-like text to utilizing external tools based on natural language instructions. This evolution expands their application beyond basic conversational tasks to enable dynamic interactions across diverse functional domains, such as facility management and professional services~\citep{patil2023gorilla, liu2023bolaa,qin2023toolllm,chen2023agentverse}. For example, Toolformer~\citep{schick2024toolformer} enables LLMs to use external tools via simple APIs through a supervised fine-tuning (SFT) model. \cite{liuapigen} demonstrate strong executable functional API calls across different domains. ToolACE~\citep{liu2024toolace} trained on synthesized data, achieves state-of-the-art results on the Berkeley Function-Calling Leaderboard~\citep{berkeley-function-calling-leaderboard}, even with a relatively small model size of 8B parameters. Despite their growing popularity and capabilities, tool use in LLMs often depends on strategies like verbal feedback, which are hampered by the quality of the datasets used for fine-tuning. Several benchmarks/datasets have been developed to support tool use in a data-centric way, such as API-Bank~\citep{li2023comprehensive}, which provides a set of tool-use dialogues with various APIs to assess the LLM's tool use capabilities, Toolalpaca~\citep{tang2023toolalpaca} constructs a comprehensive tool-use corpus derived from collected real-world APIs, designed specifically to fine-tune LLMs for better tool utilization. ToolBench~\citep{qin2023toolllm} focuses on creating a synthetic instruction-tuning dataset for tool use. However, these methods rely solely on superficial textual information, without probing deeper into the LLM’s internal states to explain or justify when and why a tool should be called, resulting in an inability to accurately determine the optimal timing for tool invocation.

\paragraph{Adaptive RAG}
RAG has shown success in supporting AI systems that require up-to-date information or access domain-specific knowledge, particularly where the scope of queries is not seen in the training data of LLMs~\citep{lewis2020retrieval, ren2023investigating,vu2023freshllms,izacard2023atlas}. This paper is also consistent with the trend of towards adaptive RAG paradigm, which is designed to assess whether a query can be directly answered by the LLMs or requires external data retrieval~\citep{asai2023self, jiang2023active}. Specifically, a simple query within the LLM's knowledge should be directly answered by the LLMs themselves. On the other hand, for complex queries or questions about data they have not been trained on, RAG intervenes to prevent incorrect out-of-date answers or hallucination~\citep {JiSurvey}. This mechanism allows RAG to dynamically adjust operational strategies of retrieval-augmented LLMs by assessing the boundary of LLM's self-knowledge and the complexity of the query, thereby minimizing unnecessary computational overhead when the queries are answerable by LLMs themselves. 
Similar to the LLMs function-calling, the decision of retrieval timing typically hinges on three primary methods: (i) explicit verbal feedback from LLMs~\citep{ding2024retrieve}, (ii) enhancements through fine-tuning~\citep{asai2023self}, or (iii) probability-based metrics~\citep{kadavath2022language,jiang2023active}. Specifically, \cite{he2021efficient} proposed enhancing the retrieval time efficiency by computing the probability of the next token via interpolating an LLM with a distribution calculated from the $k$ nearest context-token pairs. \cite{drozdov2022you} further extend $k$NN-LM to the adaptive paradigm by assigning the interpolation coefficient according to the retrieval quality measured by semantic similarity. \cite{asai2023self} introduce Self-RAG to improve generation quality and factuality by enabling adaptive retrieval and self-reflection. In contrast, this paper conceptualizes RAG as an external tool and highlights the importance of understanding the internal states of an LLM when developing the retrieval policy. 

\paragraph{Explainability of LLMs}
However, there is a considerable discrepancy between LLM's decision mechanisms (often based on verbalized responses) and their internal cognition~\citep{zou2023representation}. The internal workings of LLMs are usually unclear, and this lack of transparency poses unwanted risks in downstream decision-making. Therefore, understanding and interpreting LLMs is crucial for elucidating their behaviors and limitations. To address this challenge, various explanations that provide insights into the inner workings of LLMs have been proposed~\citep{zhao2024explainability}: 
(i) Probing-based explanations: Probing uses vector representations to measure embedded knowledge~\citep{peters2018dissecting, jawahar} or examines specific knowledge during the LLM's generation process~\citep{li2022probing}, (ii) Neuron-level explanation: neuron analysis identifies critical neurons that are essential for model's performance~\citep{antverg2021pitfalls,bills2023language}, (iii) representation engineering (RepE): RepE leverages techniques inspired by cognitive neuroscience to identify and enhance the transparency of LLMs by uncovering their internal cognitive states~\citep{zou2023representation}. In this paper, we aim to detect the internal cognition of LLMs, and intervene LLM's decisions, \ie ensuring more precise decisions on tool use and retrieval timing.



\section{Extended Results}\label{app:sec:extend_results}
\subsection{Prompting Strategies}

% TODO: need a different base model. E.g., mistral
To determine the best prompting strategy for tool use, we explore five prompting strategies with multiple base models. The results are summarized in Table \ref{tab:prompting_strategies}.
\begin{enumerate} 
\item Yes/No + Explanation: The model first answers with "Yes" or "No" and then provides a brief explanation for its decision. 
\item Yes/No: The model answers solely with "Yes" or "No," without providing any explanation. 
\item No/Yes + Explanation: The model first answers with "No" or "Yes" and then provides a brief explanation for its decision. 
\item No/Yes: The model answers solely with "No" or "Yes," without providing any explanation. 
\item CoT (Chain of Thought): The model is instructed to think step-by-step, reasoning why it does or does not need external tools to address the user query, and finally concludes its decision with ``Yes" or ``No." 
\end{enumerate}


\begin{tcolorbox}[colback=yellow!20, colframe=black, title=Chain of Thought Prompting.]
You are an intelligent agent, and you need to constantly be aware of your own limitations. I will provide you with a user's query, and you should assess, based on your own capabilities, whether you need to use external tools to better address the user's query. Typically, there are four reasons why you might need to use external tools:

\begin{itemize}
    \item A. Solving issues with real-time or external data, databases, or APIs
    \item B. Handling specialized inputs/outputs
    \item C. Enhancing domain tasks beyond LLM's capabilities
    \item D. User customization, personalization, and interaction
\end{itemize}

Please think step by step, and provide a brief explanation for your decision at first. At last, please conclude with ``Yes" if you need to use external tools, or ``No" if you do not need external tools. \\

$\left\{ \text{Few-shot Examples} \right\}$\\

User query: $\left\{ \text{query} \right\}$\\

Answer: 
\end{tcolorbox}


Note that there are no results for the CoT prompting strategy for the \texttt{Mistral-7b-instruct-v0.3} model. Regardless of the prompts used, the model consistently responds with ``Yes/No" at the beginning, followed by an explanation of its decision. This behavior effectively mirrors the Yes/No+Explanation prompting strategy. Based on Table \ref{tab:prompting_strategies}, we make the following observations and provide corresponding analysis:
\begin{enumerate} 
\item Yes/No + Explanation generally performs the best out of the five prompting strategies. This approach provides a clear decision followed by reasoning, enhancing the model's reliability and user trust. 
\item CoT is not performing as well as expected. Through close human examination, we found that CoT results in long, complex answers where the model might ultimately conclude with a decision that contrasts with its prior reasoning process. This phenomenon is referred to as reasoning inconsistency, a challenge also reported in the literature\citep{wei2022chain,lyu2023faithful}. Specifically, LLMs sometimes generate the correct answer following an invalid reasoning path or produce a wrong answer after a correct reasoning process, leading to inconsistency between the derived answer and the reasoning process. In contrast, the "Yes/No-Explanation" prompting strategy does not suffer from this reasoning inconsistency in our experiments, thereby achieving better performance compared to CoT.
% \item CoT is not performing as well as expected. Analysis of responses shows that CoT results in long, detailed answers where the model might ultimately conclude with a decision that contrasts with its prior reasoning process. This inconsistency can reduce its effectiveness. 
\item Yes/No prompting strategy works better than No/Yes prompting. We hypothesize this phenomenon is due to the data format in the pre-training data. For example, there are likely many more Yes/No answers and reasoning processes in the training data compared to No/Yes answers, influencing the model's performance. 
\end{enumerate}

We adopt \texttt{Llama-3-8b-instruct} and \texttt{Mistral-7b-instruct-v0.3} as our backbone models because they exhibit strong performance in adaptive tool use. We exclude \texttt{Llama-2-7b-chat} due to its poor performance and lack of discernment regarding the necessity of external tools. Additionally, we exclude \texttt{Llama-3.1-8b-instruct} as its performance is almost identical to that of \texttt{Llama-3-8b-instruct}.
\begin{table}[ht]
    \centering
    \caption{Performance Comparison of Different Prompting Strategies}
    \label{tab:prompting_strategies}
    \begin{tabular}{clcccc}
        \toprule
        Model &Prompting Strategies & Accuracy & Precision & Recall & F1 Score \\
        \midrule
        \multirow{5}{*}{Llama-2-7b-chat} 
        & Yes/No+Explanation & 0.51 & 0.51 & 1.0 & 0.67 \\
        & Yes/No & 0.51 & 0.5 & 1.0 & 0.67 \\
        & No/Yes+Explanation & \textbf{0.52} & 0.51 & 1.0 & 0.67 \\
        & No/Yes & 0.51 & 0.5 & 1.0 & 0.67 \\
        & CoT & 0.51 & 0.5 & 0.99 & 0.67 \\
        \midrule
        \multirow{4}{*}{Llama-3-8b-instruct} 
        & Yes/No+Explanation & \textbf{0.72} & 0.82 & 0.57 & 0.67 \\
        & Yes/No & 0.63 & 0.61 & 0.72 & 0.66 \\
        & No/Yes+Explanation & 0.52 & 0.51 & 0.99 & 0.67 \\
        & No/Yes & 0.5 & 0.5 & 1.0 & 0.67 \\
        & CoT & 0.62 & 0.59 & 0.84 & 0.69 \\
        \midrule
        \multirow{4}{*}{Llama-3.1-8b-instruct} 
        & Yes/No+Explanation & \textbf{0.71} & 0.66 & 0.87 & 0.75 \\
        & Yes/No & 0.64 & 0.59 & 0.95 & 0.73 \\
        & No/Yes+Explanation & 0.57 & 0.54 & 0.97 & 0.69 \\
        & No/Yes & 0.53 & 0.51 & 0.99 & 0.68 \\
        & CoT & 0.63 & 0.62 & 0.91 & 0.71 \\
        \midrule
        \multirow{4}{*}{Mistral-7b-instruct-v0.3} 
        & Yes/No+Explanation & \textbf{0.74} & 0.68 & 0.89 & 0.77 \\
        & Yes/No & 0.70 & 0.64 & 0.92 & 0.75 \\
        & No/Yes+Explanation & 0.70 & 0.64 & 0.88 & 0.74 \\
        & No/Yes & 0.71 & 0.57 & 0.82 & 0.74 \\
        \bottomrule
    \end{tabular}
\end{table}


\subsection{P(Yes) Approach}\label{app:subsec:p_yes}
The $P_{\text{Yes}}$ baseline introduces a \textit{Yes-score}, as defined in Section \ref{sec:setup}. This score provides a nuanced measure of the model’s confidence, refining the binary approach taken by the Naive baseline. The \textit{Yes-score} spans from $0$ to $1$, where a score of $0$ signifies a definite "No" and a score of $1$ signifies a definite ``Yes". Scores close to $0.5$ reflect lower certainty in the model's response, signifying ambiguity in decision-making. By adjusting the model's output in cases where the \textit{Yes-score} is near $0.5$ to always ``Yes/No" answer, we aim to enhance the accuracy of both tool use and RAG timing. We employ \ref{eq:p_yes_threshold} to determine the optimal threshold $l$ for the \textit{Yes-score} based on training data, which is then applied to the test data.
\begin{equation}
\text{Decision} = 
\begin{cases} 
\text{Yes} & \text{if } \textit{Yes-score} > l \\
\text{No} & \text{if } \textit{Yes-score} \leq l 
\end{cases}
\label{eq:p_yes_threshold}
\end{equation}


\subsection{Distribution of P(Yes) and Meta-Cognition Scores}
Before delving into the analysis, we provide some background on the concept of calibration in the context of Large Language Models (LLMs). Calibration refers to the alignment between a model's predicted probabilities and the actual likelihood of those predictions being correct. A well-calibrated model generates probability scores that accurately reflect the true probability of its predictions.

In Figure \ref{fig:p_yes_distribution}, we present the distribution of P(Yes) scores for both correct and incorrect Yes/No decisions. Our key observations are as follows:

\begin{enumerate} 
\item When the model is given detailed instructions and few-shot examples, it demonstrates poor calibration. As illustrated in Figure \ref{fig:p_yes_distribution}(a), the distributions of P(Yes) scores for correct and incorrect decisions do not show a clear distinction. 
\item Conversely, when the model lacks detailed context and must rely on its internal beliefs to make decisions, it exhibits improved calibration. In Figure \ref{fig:p_yes_distribution}(b), the peak of the distribution for correct scores clearly deviates from that of incorrect scores. 
\item After fine-tuning, the model displays significantly better calibration, as shown in Figures \ref{fig:p_yes_distribution}(c) and (d). Most correct decisions have P(Yes) scores of either 1 (indicating "Yes") or 0 (indicating "No"), while the P(Yes) scores for incorrect decisions vary between 0 and 1. 
\end{enumerate}





\begin{figure}[ht]
    \centering
    \subfigure[Llama-3-8b with context]{
        \includegraphics[width=0.45\textwidth]{figures/Task1_train_ww_PTrue_llama3.png}
    }
    \hfill
    \subfigure[Llama-3-8b without context]{
        \includegraphics[width=0.45\textwidth]{figures/Task1_train_wowo_PTrue_llama3.png}
    }
    \vskip\baselineskip
    \subfigure[Llama-3-8b-sft with context]{
        \includegraphics[width=0.45\textwidth]{figures/Task1_train_ww_PTrue_llama3-sft.png}
    }
    \hfill
    \subfigure[Llama-3-8b-sft without context]{
        \includegraphics[width=0.45\textwidth]{figures/Task1_train_wowo_PTrue_llama3-sft.png}
    }
    \caption{Distribution of the P(Yes) scores of the correct Yes/No and incorrect Yes/No. Llama-3-8b is the model pre-fine-tuning and Llama-3-8b-sft is the model post-fine-tuning. Note that the scores are collected on the training data in the Metatool benchmark.}
    \label{fig:p_yes_distribution}
\end{figure}



\subsection{Meta-Cognition Scores at Different Layers} 
We examine the meta-cognition scores at various layers in the model and visualize the results in Figure \ref{fig:metacog_different_layer}. We focus on the meta-cognition scores at layers -2, -5, -8, -11, and -15 because these layers exhibit the highest classification accuracy, where layer -1 refers to the last layer before the output. Notably, the meta-cognition scores at different layers have distinct value ranges and slightly different distributions. Therefore, it is not reasonable to simply average the scores from different layers as the final score for a token, which has been a common approach in other research works based on RepE. In this study, we use the meta-cognition score from the second-to-last layer as the final score, as this layer demonstrates the highest classification accuracy and effectively differentiates between correct and incorrect responses.

\begin{figure}[ht]
  \centering
  % \includegraphics[width=1.0\linewidth]{figures/metacog_different_layers.png}
  \includegraphics[width=1.0\linewidth]{figures/ICLR2025/figures/metacog_different_layers_new.png}
  \caption{Distribution of meta-cognition scores for the first token at different layers. The results are collected using the Llama-3-8b model on the training data from the Metatool benchmark.}
  \label{fig:metacog_different_layer}
\end{figure}






\clearpage
\section{Probe Training}\label{app:sec:probe}
\subsection{Different Training Strategies}
Although it increases the length of the instructions and thus may degrade the signal we are detecting, we found that it is much better to provide the model with the query in the instruction than solely instruct the model to follow the ground truth explanations. Therefore, we include the queries in the contrastive instructions below. 
\begin{figure}[ht]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/ICLR2025/figures/accuracy_llama3_probes_compareDept.png}
  \caption{The classification accuracy of different probes trained with the query in the instruction and without the query in the instruction. Training data size is fixed as 2048 in this experiment.}
  \label{fig:probes_accuracy_dept_vs_indept}
\end{figure}


\subsection{Different Size of Training Data}
We further examine how the size of the training data affects the outcomes of the meta-cognition probe. Specifically, we analyze the performance of the trained probes with varying sizes of training data, as illustrated in Figure \ref{fig:probe_n_train} and Figure \ref{fig:probe_n_train_rag}. According to Equation \ref{eq:RepE}, a sentence with 10 tokens can be used to create 10 training data pairs of experimental prompts and reference prompts. Typically, a brief explanation of why or why not to use external tools/RAG corresponds to around 30 to 50 tokens. Thus, a training data size of 256 requires fewer than 10 queries and their associated explanations.

Although different backbone models exhibit significantly varying classification accuracies—with Llama-3-8b achieving the highest and Llama-3-70b the lowest—we found that only a small amount of training data is sufficient to train a probe with near-optimal performance. We hypothesize that this is due to the linear nature of the PCA methods adopted in RepE.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/rebuttal/different_models.png}
  \caption{Training data size vs classification accuracy of meta-cognition probe in adaptive tool use.}
  \label{fig:probe_n_train}
\end{figure}

% \begin{figure}[ht]
%     \centering
%     \subfigure[Llama-3-8b-instruct]{
%         \includegraphics[width=0.33\textwidth]{figures/accuracy_llama3_n_train_.png}
%     }
%     \hfill
%     \subfigure[Mistral-7b-instruct-v0.3]{
%         \includegraphics[width=0.33\textwidth]{figures/accuracy_mistral_n_train_.png}
%     }
%     \hfill
%     \subfigure[Llama-3-70b-instruct]{
%         \includegraphics[width=0.33\textwidth]{figures/rebuttal/accuracy_llama3-70b_n_train.png}
%     }
%     \caption{\textcolor{blue}{Training} data size vs classification accuracy of meta-cognition probe in adaptive tool use.}
%     \label{fig:probe_n_train}
% \end{figure}

\begin{figure}[ht]
    \centering
    \subfigure[Llama-3-8b-instruct]{
        \includegraphics[width=0.48\textwidth]{figures/ICLR2025/figures/accuracy_llama3_n_train_rag.png}
    }
    \hfill
    \subfigure[Mistral-7b-instruct-v0.3]{
        \includegraphics[width=0.48\textwidth]{figures/ICLR2025/figures/accuracy_mistral_n_train_rag.png}
    }
    \caption{Train data size vs classification accuracy of meta-cognition probe in adaptive RAG.}
    \label{fig:probe_n_train_rag}
\end{figure}



\subsection{Contrastive Instructions for Various Probes}
% honest probe
We used the following instruction pair to collect contrastive data and train the honesty probe. Specifically, we instructed the model to be both honest and untruthful when explaining its reasoning for the necessity of tool use tes
\begin{tcolorbox}[colback=gray!10, colframe=black, title=Contrastive Instructions for training Honesty Probe]
\begin{tcolorbox}[colback=yellow!20, colframe=black, title=Honest Instruction]
Imagine you are an honest intelligent assistant explaining why you need or do not need to use an external tool to respond to the following user query.\\

User Query: $\left\{ \text{query} \right\}$\\

Answer: 
\end{tcolorbox}

\begin{tcolorbox}[colback=green!20, colframe=black, title=Untruthful Instruction]
Imagine you are an untruthful intelligent assistant explaining why you need or do not need to use an external tool to respond to the following user query.\\

User Query: $\left\{ \text{query} \right\}$\\

Answer: 
\end{tcolorbox}
\end{tcolorbox}


% confident probe
Similarly, we instructed the model to exhibit both confidence and unconfidence when we trained the confidence probe.
\begin{tcolorbox}[colback=gray!10, colframe=black, title=Contrastive Instructions for training Confidence Probe]
\begin{tcolorbox}[colback=yellow!20, colframe=black, title=Confident Instruction]
Imagine you are a confident intelligent assistant explaining why you need or do not need to use an external tool to respond to the following user query.\\

User Query: $\left\{ \text{query} \right\}$\\

Answer: 
\end{tcolorbox}

\begin{tcolorbox}[colback=green!20, colframe=black, title=Unconfident Instruction]
Imagine you are an unconfident intelligent assistant explaining why you need or do not need to use an external tool to respond to the following user query.\\

User Query: $\left\{ \text{query} \right\}$\\

Answer: 
\end{tcolorbox}
\end{tcolorbox}


% metacog probe in adaptive tool use
For the meta-cognition probe, we instruct the model to exhibit strong meta-cognition by being constantly aware of its own limitations and capabilities and accurately assessing whether an external tool is necessary. Conversely, with weak meta-cognition, the model is often unaware of its own limitations and capabilities and struggles to assess the necessity of tool use.
\begin{tcolorbox}[colback=gray!10, colframe=black, title=Contrastive Instructions for training Meta-Cognition Probe]
\begin{tcolorbox}[colback=yellow!20, colframe=black, title=Strong Meta-Cognition Instruction in Adaptive Tool Use]
Imagine you are an intelligent assistant with strong meta-cognition, constantly aware of your own limitations and capabilities. You can accurately assess and explain whether you need to use an external tool to respond to the following user query.\\

User Query: $\left\{ \text{query} \right\}$\\

Answer: 
\end{tcolorbox}

\begin{tcolorbox}[colback=green!20, colframe=black, title=Weak Meta-Cognition Instruction]
Imagine you are an assistant with weak meta-cognition, often unaware of your own limitations and capabilities. You struggle to assess and explain why you need or do not need to use an external tool to respond to the following user query.\\

User Query: $\left\{ \text{query} \right\}$\\

Answer: 
\end{tcolorbox}
\end{tcolorbox}


% metacog probe in adaptive RAG
The meta-cognition instruction for Adaptive RAG is similar to that in the adaptive tool use setting, with the only difference being that we replace the necessity of tool use with the necessity of RAG.
\begin{tcolorbox}[colback=gray!10, colframe=black, title=Contrastive Instructions for training Meta-Cognition Probe in Adaptive RAG]
\begin{tcolorbox}[colback=yellow!20, colframe=black, title=Strong Meta-Cognition Instruction]
Imagine you are an intelligent assistant with strong meta-cognition, constantly aware of your own limitations and capabilities. You can accurately assess and explain whether you need to perform Retrieval Augmented Generation (RAG) to respond to the following user query.\\

User Query: $\left\{ \text{query} \right\}$\\

Answer: 
\end{tcolorbox}

\begin{tcolorbox}[colback=green!20, colframe=black, title=Weak Meta-Cognition Instruction]
Imagine you are an assistant with weak meta-cognition, often unaware of your own limitations and capabilities. You struggle to assess and explain why you need or do not need to perform Retrieval Augmented Generation (RAG) to respond to the following user query.\\

User Query: $\left\{ \text{query} \right\}$\\

Answer: 
\end{tcolorbox}
\end{tcolorbox}









\clearpage
\section{Prompts}
\subsection{Prompts in Adaptive Tool Use}
We employ two types of prompts in our experiments: 1) prompts with context, which provide specific reasons for why LLMs may require external tools to complete user tasks. These prompts also include five randomly sampled examples to assist the model in making decisions; and 2) prompts without context, which are more concise and contain only the instruction and query. The exact prompts are provided below. Note that the example queries are randomly sampled in the Metatool benchmark and we follow their setup and don't change the examples associated with queries. 

\begin{tcolorbox}[colback=yellow!20, colframe=black, title=Prompt with context.]
You are an intelligent agent, and you need to constantly be aware of your own limitations. I will provide you with a user's query, and you should assess, based on your own capabilities, whether you need to use external tools to better address the user's query. Typically, there are four reasons why you might need to use external tools:

\begin{itemize}
    \item A. Solving issues with real-time or external data, databases, or APIs
    \item B. Handling specialized inputs/outputs
    \item C. Enhancing domain tasks beyond LLM's capabilities
    \item D. User customization, personalization, and interaction
\end{itemize}

If you think it's necessary to use external tools, please respond with ``Yes"; otherwise, respond with ``No". Additionally, you should provide a very brief explanation for your answer. Here are some examples:

\begin{itemize}
    \item Query: ``Write an opinion piece about why diversity and inclusion is super important for the tech industry. The essay should be targeted at 'tech bros', and should avoid alienating them, but instead appeal to their logic; it should explain how diversity and inclusion of women, immigrants, etc. could benefit them specifically." Answer: No
    \item Query: ``Are there any loopholes that hackers can exploit on my website?" Answer: Yes
    \item Query: ``Plan a weekly lunch menu for a school. Write down a main dish, a carbohydrate side dish, a vegetable side dish, and a dessert for each day." Answer: No
    \item Query: ``Can you break down the main points of this TED talk for me? Here's the YouTube link." Answer: Yes
    \item Query: ``How's the weather in London right now?" Answer: No\\
\end{itemize}

User query: $\left\{ \text{query} \right\}$\\

Answer: 
\end{tcolorbox}

\begin{tcolorbox}[colback=green!20, colframe=black, title=Prompt without context.]
You are an intelligent agent, and you need to constantly be aware of your own limitations. I will provide you with a user's query, and you should assess, based on your own capabilities, whether you need to use external tools to better address the user's query. If you think it's necessary to use external tools, please respond with ``Yes"; otherwise, respond with ``No". Additionally, you should provide a very brief explanation for your answer.\\

User Query: $\left\{ \text{query} \right\}$\\

Answer: 
\end{tcolorbox}




\subsection{Prompts in Adaptive RAG}
In adaptive RAG task, LLMs are typically not provided with any reasons or examples to help them make a decision. Following this setting, we conduct the experiments in adaptive RAG without providing context in the prompts as shown below.

% \begin{tcolorbox}[colback=yellow!20, colframe=black, title=Prompt with context.]
% Imagine you are an intelligent assistant with strong meta-cognition, constantly aware of your own limitations and capabilities. Your task is to accurately assess and explain whether you need to perform Retrieval Augmented Generation (RAG) to respond to the following user query. Typically, there are four reasons why you might need to perform RAG:

% \begin{itemize}
%     \item A. Accessing real-time and up-to-date information
%     \item B. Fetching specialized domain-specific knowledge
%     \item C. Verifying facts and ensuring accuracy
%     \item D. Providing comprehensive and tailored responses
% \end{itemize}

% If you determine that performing RAG is necessary, please respond with ``Yes"; otherwise, respond with ``No". Additionally, provide a very brief explanation for your decision.:

% \begin{itemize}
%     \item Query: ``What does the moon orbit around?" Answer: No
%     \item Query: ``What does fire need to burn?" Answer: No
%     \item Query: ``What does the Earth's atmosphere protect us from?" Answer: No
%     \item Query: ``What are the Top 10 Emerging Technologies of 2024 according to the World Economic Forum's report?" Answer: Yes
%     \item Query: ``Which regions are leading in tech innovation according to global innovation indices?" Answer: Yes\\
%     \item Query: ``What did the ITIF report say about China's AI innovation?" Answer: Yes\\
% \end{itemize}

% User Query: $\left\{ \text{query} \right\}$\\

% Answer: 
% \end{tcolorbox}

\begin{tcolorbox}[colback=green!20, colframe=black, title=Prompt without context.]
Imagine you are an intelligent assistant with strong meta-cognition, constantly aware of your own limitations and capabilities. Your task is to accurately assess and explain whether you need to perform Retrieval Augmented Generation (RAG) to respond to the following user query. If you determine that performing RAG is necessary, please respond with ``Yes"; otherwise, respond with ``No". Additionally, provide a very brief explanation for your decision.\\

User Query: $\left\{ \text{query} \right\}$\\

Answer: 
\end{tcolorbox}


