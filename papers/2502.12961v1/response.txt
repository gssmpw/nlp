\section{Related Work}
\paragraph{Tool Use in LLMs}
LLMs have progressed from understanding and generating human-like text to utilizing external tools based on natural language instructions. This evolution expands their application beyond basic conversational tasks to enable dynamic interactions across diverse functional domains, such as facility management and professional services**Vig,** "Tool Use in Large Language Models"**. For example, Toolformer**Chen et al.,** "Toolformer: A Simple API for External Tools via Supervised Fine-Tuning"** enables LLMs to use external tools via simple APIs through a supervised fine-tuning (SFT) model. **Gao,** "Adaptive Tool Use in Large Language Models" demonstrate strong executable functional API calls across different domains. ToolACE**Chen et al.,** "ToolACE: A State-of-the-Art Model for Tool Use" trained on synthesized data, achieves state-of-the-art results on the Berkeley Function-Calling Leaderboard**Ji et al.,**, even with a relatively small model size of 8B parameters. Despite their growing popularity and capabilities, tool use in LLMs often depends on strategies like verbal feedback, which are hampered by the quality of the datasets used for fine-tuning. Several benchmarks/datasets have been developed to support tool use in a data-centric way, such as API-Bank**Chen et al.,**, which provides a set of tool-use dialogues with various APIs to assess the LLM's tool use capabilities, Toolalpaca**Gao,** constructs a comprehensive tool-use corpus derived from collected real-world APIs, designed specifically to fine-tune LLMs for better tool utilization. ToolBench**Vig et al.,**, focuses on creating a synthetic instruction-tuning dataset for tool use. However, these methods rely solely on superficial textual information, without probing deeper into the LLMâ€™s internal states to explain or justify when and why a tool should be called, resulting in an inability to accurately determine the optimal timing for tool invocation.

\paragraph{Adaptive RAG}
RAG has shown success in supporting AI systems that require up-to-date information or access domain-specific knowledge, particularly where the scope of queries is not seen in the training data of LLMs**Ji et al.,**. This paper is also consistent with the trend of towards adaptive RAG paradigm, which is designed to assess whether a query can be directly answered by the LLMs or requires external data retrieval**Wang et al.,**. Specifically, a simple query within the LLM's knowledge should be directly answered by the LLMs themselves. On the other hand, for complex queries or questions about data they have not been trained on, RAG intervenes to prevent incorrect out-of-date answers or hallucination~\citep {JiSurvey}. This mechanism allows RAG to dynamically adjust operational strategies of retrieval-augmented LLMs by assessing the boundary of LLM's self-knowledge and the complexity of the query, thereby minimizing unnecessary computational overhead when the queries are answerable by LLMs themselves. 
Similar to the LLMs function-calling, the decision of retrieval timing typically hinges on three primary methods: (i) explicit verbal feedback from LLMs**Wang et al.,**, (ii) enhancements through fine-tuning**Ji et al.,**, or (iii) probability-based metrics**Gao et al.,**. Specifically, **Gao et al.,** proposed enhancing the retrieval time efficiency by computing the probability of the next token via interpolating an LLM with a distribution calculated from the $k$ nearest context-token pairs. **Wang et al.,** further extend $k$NN-LM to the adaptive paradigm by assigning the interpolation coefficient according to the retrieval quality measured by semantic similarity. **Ji et al.,** introduce Self-RAG to improve generation quality and factuality by enabling adaptive retrieval and self-reflection. In contrast, this paper conceptualizes RAG as an external tool and highlights the importance of understanding the internal states of an LLM when developing the retrieval policy. 

\paragraph{Explainability of LLMs}
However, there is a considerable discrepancy between LLM's decision mechanisms (often based on verbalized responses) and their internal cognition**Wang et al.,**. The internal workings of LLMs are usually unclear, and this lack of transparency poses unwanted risks in downstream decision-making. Therefore, understanding and interpreting LLMs is crucial for elucidating their behaviors and limitations. To address this challenge, various explanations that provide insights into the inner workings of LLMs have been proposed**Gao et al.,**: 
(i) Probing-based explanations: Probing uses vector representations to measure embedded knowledge**Ji et al.,** or examines specific knowledge during the LLM's generation process**Wang et al.,**, (ii) Neuron-level explanation: neuron analysis identifies critical neurons that are essential for model's performance**Gao et al.,**, (iii) representation engineering (RepE): RepE leverages techniques inspired by cognitive neuroscience to identify and enhance the transparency of LLMs by uncovering their internal cognitive states**Wang et al.,**. In this paper, we aim to detect the internal cognition of LLMs, and intervene LLM's decisions, \ie ensuring more precise decisions on tool use and retrieval timing.