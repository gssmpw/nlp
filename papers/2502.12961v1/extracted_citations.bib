@article{antverg2021pitfalls,
  title={On the pitfalls of analyzing individual neurons in language models},
  author={Antverg, Omer and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:2110.07483},
  year={2021}
}

@article{asai2023self,
  title={Self-rag: Learning to retrieve, generate, and critique through self-reflection},
  author={Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2310.11511},
  year={2023}
}

@misc{berkeley-function-calling-leaderboard,
    title={Berkeley Function Calling Leaderboard}, 
    author={Fanjia Yan and Huanzhi Mao and Charlie Cheng-Jie Ji
    and Tianjun Zhang and Shishir G. Patil and Ion Stoica and Joseph E.
    Gonzalez},
    howpublished={\url{https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html}},
    year={2024}
}

@article{bills2023language,
  title={Language models can explain neurons in language models},
  author={Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William},
  journal={URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023)},
  volume={2},
  year={2023}
}

@inproceedings{chen2023agentverse,
  title={Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors},
  author={Chen, Weize and Su, Yusheng and Zuo, Jingwei and Yang, Cheng and Yuan, Chenfei and Chan, Chi-Min and Yu, Heyang and Lu, Yaxi and Hung, Yi-Hsin and Qian, Chen and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{ding2024retrieve,
  title={Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models},
  author={Ding, Hanxing and Pang, Liang and Wei, Zihao and Shen, Huawei and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2402.10612},
  year={2024}
}

@inproceedings{drozdov2022you,
  title={You canâ€™t pick your neighbors, or can you? When and How to Rely on Retrieval in the kNN-LM},
  author={Drozdov, Andrew and Wang, Shufan and Rahimi, Razieh and Mccallum, Andrew and Zamani, Hamed and Iyyer, Mohit},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={2997--3007},
  year={2022}
}

@inproceedings{he2021efficient,
  title={Efficient Nearest Neighbor Language Models},
  author={He, Junxian and Neubig, Graham and Berg-Kirkpatrick, Taylor},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={5703--5714},
  year={2021}
}

@article{izacard2023atlas,
  title={Atlas: Few-shot learning with retrieval augmented language models},
  author={Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={251},
  pages={1--43},
  year={2023}
}

@inproceedings{jawahar,
  TITLE = {{What does BERT learn about the structure of language?}},
  AUTHOR = {Jawahar, Ganesh and Sagot, Beno{\^i}t and Seddah, Djam{\'e}},
  URL = {https://inria.hal.science/hal-02131630},
  BOOKTITLE = {{ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics}},
  ADDRESS = {Florence, Italy},
  YEAR = {2019},
  MONTH = Jul,
  PDF = {https://inria.hal.science/hal-02131630v1/file/intbert_acl19paper-3.pdf},
  HAL_ID = {hal-02131630},
  HAL_VERSION = {v1},
}

@article{jiang2023active,
  title={Active retrieval augmented generation},
  author={Jiang, Zhengbao and Xu, Frank F and Gao, Luyu and Sun, Zhiqing and Liu, Qian and Dwivedi-Yu, Jane and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  journal={arXiv preprint arXiv:2305.06983},
  year={2023}
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{li2022probing,
  title={Probing via prompting},
  author={Li, Jiaoda and Cotterell, Ryan and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2207.01736},
  year={2022}
}

@inproceedings{li2023comprehensive,
  title={A comprehensive benchmark for tool-augmented LLMs},
  author={Li, Minghao and Zhao, Yingxiu and Yu, Bowen and Song, Feifan and Li, Hangyu and Yu, Haiyang and Li, Zhoujun and Huang, Fei and API-bank, Yongbin Li},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={3102--3116},
  year={2023}
}

@article{liu2023bolaa,
  title={Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents},
  author={Liu, Zhiwei and Yao, Weiran and Zhang, Jianguo and Xue, Le and Heinecke, Shelby and Murthy, Rithesh and Feng, Yihao and Chen, Zeyuan and Niebles, Juan Carlos and Arpit, Devansh and others},
  journal={arXiv preprint arXiv:2308.05960},
  year={2023}
}

@article{liu2024toolace,
  title={ToolACE: Winning the Points of LLM Function Calling},
  author={Liu, Weiwen and Huang, Xu and Zeng, Xingshan and Hao, Xinlong and Yu, Shuai and Li, Dexun and Wang, Shuai and Gan, Weinan and Liu, Zhengying and Yu, Yuanqing and others},
  journal={arXiv preprint arXiv:2409.00920},
  year={2024}
}

@inproceedings{liuapigen,
  title={APIGen: Automated PIpeline for Generating Verifiable and Diverse Function-Calling Datasets},
  author={Liu, Zuxin and Hoang, Thai Quoc and Zhang, Jianguo and Zhu, Ming and Lan, Tian and Kokane, Shirley and Tan, Juntao and Yao, Weiran and Liu, Zhiwei and Feng, Yihao and others},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024}
}

@article{patil2023gorilla,
  title={Gorilla: Large language model connected with massive apis},
  author={Patil, Shishir G and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2305.15334},
  year={2023}
}

@article{peters2018dissecting,
  title={Dissecting contextual word embeddings: Architecture and representation},
  author={Peters, Matthew E and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:1808.08949},
  year={2018}
}

@article{qin2023toolllm,
  title={Toolllm: Facilitating large language models to master 16000+ real-world apis},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  journal={arXiv preprint arXiv:2307.16789},
  year={2023}
}

@article{ren2023investigating,
  title={Investigating the factual knowledge boundary of large language models with retrieval augmentation},
  author={Ren, Ruiyang and Wang, Yuhao and Qu, Yingqi and Zhao, Wayne Xin and Liu, Jing and Tian, Hao and Wu, Hua and Wen, Ji-Rong and Wang, Haifeng},
  journal={arXiv preprint arXiv:2307.11019},
  year={2023}
}

@article{schick2024toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{tang2023toolalpaca,
  title={Toolalpaca: Generalized tool learning for language models with 3000 simulated cases},
  author={Tang, Qiaoyu and Deng, Ziliang and Lin, Hongyu and Han, Xianpei and Liang, Qiao and Cao, Boxi and Sun, Le},
  journal={arXiv preprint arXiv:2306.05301},
  year={2023}
}

@article{vu2023freshllms,
  title={Freshllms: Refreshing large language models with search engine augmentation},
  author={Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jerry and Wei, Jason and Tar, Chris and Sung, Yun-Hsuan and Zhou, Denny and Le, Quoc and others},
  journal={arXiv preprint arXiv:2310.03214},
  year={2023}
}

@article{zhao2024explainability,
  title={Explainability for large language models: A survey},
  author={Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={2},
  pages={1--38},
  year={2024},
  publisher={ACM New York, NY}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

