% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}


%%%%%%%%%% added packages %%%%%%%%%%
% \newcommand{\dx}[1]{{\textcolor{red}{#1}}}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{array}

\usepackage{color, xspace}
\usepackage{wrapfig}
\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\mname}{\textbf{MeCo}\xspace}
\newcommand{\dname}{\textbf{MeCa}\xspace}

\newcommand{\zh}[1]{\textcolor{orange}{[ZH: #1]}}
\newcommand{\kc}[1]{\textcolor{purple}{#1}}
\newcommand{\dx}[1]{\textcolor{blue}{#1}}
\usepackage[normalem]{ulem}
\usepackage{enumitem}
\newtheorem{definition}{Definition}

\title{Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger}

\author{
 \textbf{Wenjun Li\textsuperscript{*}},
 \textbf{Dexun Li\textsuperscript{*}},
 \textbf{Kuicai Dong\textsuperscript{}},
 \textbf{Cong Zhang\textsuperscript{}},
 \textbf{Hao Zhang\textsuperscript{}},
 \textbf{Weiwen Liu\textsuperscript{}},\\
 \textbf{Yasheng Wang\textsuperscript{}},
 \textbf{Ruiming Tang\textsuperscript{}},
 and \textbf{Yong Liu\textsuperscript{}}
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
% \\
\\
 \textsuperscript{}Huawei Noah's Ark Lab
  % \textsuperscript{1}Huawei Noah's Ark Lab, Singapore
 % \textsuperscript{2}Affiliation 2,
 % \textsuperscript{3}Affiliation 3,
 % \textsuperscript{4}Affiliation 4,
 % \textsuperscript{5}Affiliation 5
% \\
 % \small{
 %   \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
 % }
 \\
 \texttt{\{li.wenjun, lidexun, zhang.hao3, liuweiwen8, wangyasheng,} \\\texttt{tangruiming, liu.yong6@huawei.com\}}
 % \texttt{cong.zhang92@gmail.com, goh.xin.deik@huawei.com, lidexun@huawei.com, \\  
 % zhang.hao3@huawei.com, liu.yong6@huawei.com}
  % \texttt{cong.zhang92@gmail.com\{zhangcong92, goh.xin.deik, lidexun, zhang.hao3, liu.yong6\}@huawei.com}
% \texttt{cong.zhang92@gmail.com, goh.xin.deik@huawei.com, lidexun@huawei.com,} \\
% \texttt{zhang.hao3@huawei.com, liu.yong6@huawei.com}
}
\begin{document}
\maketitle
\def\thefootnote{*}\footnotetext{Both authors contributed equally to this work.}
\begin{abstract}
% ICLR Version
% Large language models (LLMs) have demonstrated remarkable emergent capabilities, reshaping the landscape of functional tasks by leveraging external tools to tackle complex problems, such as those requiring real-time data or specialized input/output processing. Existing research primarily focuses on equipping LLMs with a broader array of diverse external tools (\eg program interpreters, search engines, weather/map applications) but overlooks the necessity of tool usage, invoking external tools indiscriminately without assessing their actual need. This naive strategy leads to two significant issues: 1) increased latency due to prolonged processing times, and 2) potential errors arising from communication between LLMs and external tools, resulting in faulty outputs. In this paper, we introduce a concept we term meta-cognition as a proxy for LLM self-capability, and we propose an adaptive decision-making strategy for invoking external tools, referred to as {\em MeCo}. Specifically, MeCo focuses on representation space to capture emergent representations of high-level cognitive phenomena that quantify the LLM's meta-cognitive scores, thereby guiding decisions on when to use external tools. Notably, MeCo is fine-tuning-free, incurring minimal cost, and our experiments demonstrate that MeCo accurately detects the model's internal cognitive signals. More importantly, our approach significantly enhances decision-making accuracy in tool use for multiple base models across various benchmarks.



% 写作故事主线；围绕构建模型的自我能力认知能力，这个能力体现在自我决定是否已工具调用（其中包括是否进行RAG）。感觉看下来文章不够紧密。有大片留白。

%TODO List
% 1. Adjust the two tables: 1) the big table containing meca results; 2) the meca stats table could be moved into the main paper;
% 2. Always relying on external tools will lead to degraded performance. 


% ARR revised Version
Large language models (LLMs) have shown remarkable emergent capabilities, transforming the execution of functional tasks by leveraging external tools for complex problems that require specialized processing or real-time data. While existing research expands LLMs access to diverse tools (\eg program interpreters, search engines, weather/map apps), the necessity of using these tools is often overlooked, leading to indiscriminate tool invocation. This naive approach raises two key issues:(1) increased delays due to unnecessary tool calls, and (2) potential errors resulting from faulty interactions with external tools. In this paper, we introduce meta-cognition as a proxy for LLMs self-assessment of their capabilities, representing the model's awareness of its own limitations. Based on this, we propose {\em MeCo}, an adaptive decision-making strategy for external tool use. MeCo quantifies metacognitive scores by capturing high-level cognitive signals in the representation space, guiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal cost. Our experiments show that MeCo accurately detects LLMs’ internal cognitive signals and significantly improves tool-use decision-making across multiple base models and benchmarks.

\end{abstract}


\section{Introduction}
% ICLR Version
% Equipping Large language models (LLMs) with tool learning capabilities represents a promising paradigm to address complex tasks relying on external/real-time sources~\citep{komeili2021internet,tang2023toolalpaca}, specialized format/schema~\citep{yang2023foundation,gao2023pal,lu2024chameleon}, domain-specific knowledge~\citep{he2023solving,schick2024toolformer}, and so on. Although existing research has focused on increasing the number and types of tools available within this paradigm~\citep{qin2023toolllm,hao2024toolkengpt} and optimizing their usage of these tools~\citep{patil2023gorilla,shen2024hugginggpt}, the decision-making process regarding when tools are necessary remains underexplored.

% Equipping large language models (LLMs) with tool-use capabilities enables them to tackle complex tasks requiring external/real-time sources~\citep{komeili2021internet,tang2023toolalpaca}, specialized format/schema~\citep{yang2023foundation,gao2023pal,lu2024chameleon}, and domain-specific knowledge~\citep{he2023solving,schick2024toolformer}. 
Equipping large language models (LLMs) with tool-use capabilities allows them to overcome their limitations by accessing external/real-time data~\citep{komeili2021internet,tang2023toolalpaca}, domain-specific knowledge~\citep{he2023solving,schick2024toolformer}, and advanced specialized functionalities~\citep{yang2023foundation,gao2023pal,lu2024chameleon}, thereby enabling them to handle more complex tasks beyond their inherent capabilities.
While prior research has focused on expanding tool diversity~\citep{qin2023toolllm,hao2024toolkengpt} and optimizing their use~\citep{patil2023gorilla,shen2024hugginggpt}, the decision-making process for determining when tools are necessary remains underexplored.

% ICLR Version
% The prevailing strategy adopted by existing paradigms, which involves enhancing tool usage by fine-tuning LLMs on carefully crafted datasets, is often hampered by the quality of these datasets. On the other hand, if an LLM always relies on external tools to respond to user queries, it encounters two notable limitations. Primarily, it leads to increased latency~\citep{qu2024tool,wang2024survey}, as using an external tool (\eg search engine) can take significantly longer than leveraging the model's internal knowledge. Furthermore, the heavy reliance on external APIs and tools poses risks of robustness and integration issues. It potentially introduces incorrect or inconsistent outputs when tools malfunction~\citep{qin2023toolllm} or are unnecessarily used\textcolor{blue}{~\citep{lu2024chameleon,wurepoformer}}.

Indiscriminate invocation of external tools leads to two major issues: (1) increased latency~\citep{qu2024tool,wang2024survey}, as external tools, such as search engine, typically operates slower compared to relying on internal knowledge retrieval, and (2) robustness risks, where dependence on external APIs increases the likelihood of errors due to tools malfunction or unnecessary tool-use~\citep{qin2023toolllm,lu2024chameleon,wurepoformer}.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figures/rebuttal/MeCo_algo_revised.pdf}
  \caption{Overview of \mname: Learned Meta-Cognition determines the necessity for tool use or retrieval by using a trained meta-cognition probe to detect the internal state of an LLM.}
  \label{fig:algo_overview}
\end{figure*}

% ICLR Version
% To address aforementioned limitations by indiscriminate use of external tools, we propose an adaptive tool use strategy aimed at improving decision-making in LLMs concerning tool utilization. We introduce a novel approach, termed \mname, a \textbf{Me}ta-\textbf{Co}gnition-oriented trigger that facilitates more judicious use of external tools. We define meta-cognition as the model's ability to self-assess its own capabilities and limitations, discerning whether it can address a user's query independently or if it needs to utilize external tools. Overall, \mname integrates the following key principles:

To address these limitations, we propose an adaptive tool-use strategy that improves decision-making in LLMs. Our approach, \mname (\textbf{Me}ta-\textbf{Co}gnition-oriented trigger), enables LLMs to self-assess their capabilities and determine whether external tools are needed to address a user's query. \mname incorporates three key principles:

% ICLR Version
% \begin{enumerate}[leftmargin=*, itemsep=0em, topsep=-0.0em]
%     \item \textbf{Meta-Cognition-Oriented Trigger Mechanism:} This core component ensures that LLMs maintain an ongoing assessment of their capabilities and limitations, enabling them to determine the necessity of external tools. This self-awareness is crucial for minimizing unnecessary tool invocation and optimizing the model's performance. 
%     \item \textbf{Effective Policy Utilization:} Leveraging the meta-cognition evaluation, we implement a policy that governs tool use based on quantified meta-cognition feedback. Our experimental results demonstrate this policy's superiority over existing methods in decision accuracy.
%     \item \textbf{Generability:} The ability of \mname to generalize across various scenarios is validated empirically, ensuring its effectiveness and robustness in diverse operational environments. Moreover, we treat adaptive Retrieval-Augmented Generation (RAG) as a specific instance of tool utilization and validate the effectiveness of \mname on adaptive RAG against baseline methods. 
% \end{enumerate} 

\begin{itemize}[leftmargin=*, itemsep=0em, topsep=-0.0em]
    \item \textbf{Meta-Cognition:} We propose the concept of meta-cognition, which involves evaluating an LLM’s understanding of its capabilities based on its representation space. This self-awareness is crucial for minimizing unnecessary tool use.
    % LLMs continuously evaluate their confidence and limitations, minimizing unnecessary tool use.
    \item \textbf{Effective Strategy Utilization:} We design an efficient strategy that leverages the quantified meta-cognition feedback to dynamically adjust tool use decisions, significantly improving decision accuracy compared to existing methods.
    \item \textbf{Generability:} We empirically validate \mname's ability to generalize across a wide range of diverse scenarios. Additionally, we treat adaptive Retrieval-Augmented Generation (RAG) as a special case of tool use and demonstrate the superior effectiveness of \mname in adaptive RAG compared to baseline methods.
\end{itemize} 

% ICLR Version
% Building on our initial proposal of an adaptive tool use strategy for LLMs, we detail the development of a computationally efficient plug-in module designed to assess the meta-cognitive states of LLMs. Leveraging the Representation Engineering (RepE) framework~\citep{zou2023representation}, known for its effectiveness in identifying internal concepts such as honesty and confidence within LLMs, we applied this methodology to detect signals associated with meta-cognition. Our analysis indicates that meta-cognition can generate a strong signal which can be further used to enhance the interpretability of decisions made by LLMs. As illustrated in Figure~\ref{fig:algo_overview}, our strategy dictates that LLMs should engage external tools only when the complexity of a user query surpasses the model's inherent capabilities; otherwise, they should rely on their internal knowledge. Specifically, we establish two thresholds for a given task: one discriminates between strong and weak meta-cognition signals for affirmative responses ("Yes"), and another differentiates these signals for negative responses ("No"). This dual-threshold approach allows us to refine the model's decision-making process, particularly when meta-cognitive signals are weak, suggesting uncertainty or insufficient knowledge.

Building on the Representation Engineering (RepE) framework~\citep{zou2023representation}, we develop a computationally efficient plug-in module to assess LLM meta-cognition. Our analysis reveals that meta-cognition produces strong signals, which can enhance interpretability and guide tool-use decisions. As shown in Figure~\ref{fig:algo_overview}, \mname employs a dual-threshold policy to distinguish between strong and weak meta-cognitive signals, refining decision-making when uncertainty arises.

In summary, our contributions are four-fold: 1) We introduce the concept of adaptive tool use, which enhances both the efficiency and robustness of existing tool use paradigms in LLMs. 2) We integrate adaptive tool use and adaptive RAG within a unified framework, with their activation driven by a shared strategy based on meta-cognition detection. 3) We build a new benchmark, \dname, to evaluate the effectiveness of our method. 4) We empirically demonstrate that \mname significantly enhances the model's awareness in tool utilization and RAG processes.

\begin{figure*}[th]
  \centering
  \includegraphics[width=1\linewidth]{figures/ICLR2025/figures/framework.png}
  \caption{Pipeline for training the meta-cognition probe.}
  \label{fig:pipeline_probe_training}
\end{figure*}


\section{Background} \label{sec:background}
% ICLR Version
% Recent research has delved into the internal representations of LLMs to gain insights into their beliefs and interpretability\textcolor{blue}{~\citep{bricken2023monosemanticity,Levinstein_2024}}. Studies such as those by \textcolor{blue}{\cite{zou2023representation} and \cite{liu2023aligning}} have demonstrated that specific features and signals (\eg happiness, honesty, and confidence) align with distinct directions within the LLM's representation space, making them linearly separable. \textcolor{blue}{Figure \ref{fig:pipeline_probe_training} illustrates the main steps in the pipeline for training various types of probes.} To effectively capture and detect these signals, we can utilize contrastive instruction pairs to implicitly induce the emergence of these contrastive signals.

Recent studies have explored LLMs’ internal representations to improve interpretability and understand their implicit beliefs~\citep{bricken2023monosemanticity,Levinstein_2024}. Research~\citep{zou2023representation, liu2023aligning} shows that features like happiness, honesty, and confidence correspond to distinct, linearly separable directions in the representation space. Figure \ref{fig:pipeline_probe_training} illustrates the pipeline for training the corresponding feature probe. To detect these signals, contrastive instruction pairs can be used to induce their emergence.

% ICLR Version
% Building on these insights, we enable the capturing and controlling of high-level functions $f$ such as honesty in the model responses. We follow \citet{zou2023representation} to design an \textit{experimental prompt} $T^{+}_{f}$ that necessitates the execution of the function and a corresponding \textit{reference prompt} $T^{-}_{f}$ that does not require the function's execution. An example instruction template might resemble the following:

Building on these insights, we can capture and control high-level functions $f$ (\eg honesty) in model responses. We follow \citet{zou2023representation} to design an \textit{experimental prompt} $T^{+}_{f}$ that requires function execution and a \textit{reference prompt} $T^{-}_{f}$ that does not. The instruction template is:

\begin{tcolorbox}[colback=white!95!white, colframe=black, width=\columnwidth, arc=0mm, outer arc=0mm]
% \textbf{USER:} \textcolor{blue}{$\langle$ instruction $\rangle$} \textcolor{red}{$\langle$ experimental/reference prompt $\rangle$} \\
\textbf{USER:} \textcolor{blue}{$\langle$ instruction $\rangle$} \textcolor{red}{$\langle$ experimental/reference prompt $\rangle$} \\
\textbf{ASSISTANT:} \textcolor{blue}{$\langle$ output $\rangle$}
\end{tcolorbox}

For a function $f$ and model $M$, given instruction-response pairs $(q_i, a_i)$ in set $S$ and denoting a response truncated after token $k$ as $a^k_i$, we collect internal representations for the experimental and reference sets:
\begin{equation}
    % A^{\pm}_f = \left\{\text{Rep}(M, T^{\pm}_{f}(q_i, a^k_i))[-1] \text{ }|\text{ } (q_i, a_i) \in S, \text{for } 0 < k \le |a_i| \right\} \label{eq:RepE}
    A^{\pm}_f = \left\{\text{Rep}(M, T^{\pm}_{f}(q_i, a^k_i))[-1] \text{ }|\text{ } (q_i, a_i) \in S \right\} \label{eq:RepE}
\end{equation}
where Rep represents the representation obtaining operation, $[-1]$ denotes the last token representation of $a^k$, and $A^{\pm}_f$ are the resulted activations consist of individual vectors. 

Our goal is to learn a linear model to predict the direction of the function $A^{\pm}_f$ based on internal representations. Specifically, we apply PCA~\citep{mackiewicz1993principal} in an unsupervised manner to pairwise difference vectors, deriving the first principal component $v_f$ (referred to as the probe) to identify function direction in the model's responses. Equation \ref{eq:RepE} is applied at each layer of $M$ to derive layer-wise probes which are then used to interact with the LLM's representations to monitor and control its behavior.


\section{Approach}
We define meta-cognition in LLMs as follows:
\begin{definition} 
Meta-cognition refers to an LLM's ability to assess and regulate its own knowledge and limitations, enabling informed decision-making about task execution, including when to rely on external tools or resources.
\end{definition}
In the context of tool use, this involves assessing the model’s capabilities and limitations to determine whether a query can be answered independently or requires external tools, based on the query’s complexity and the sufficiency of the model’s internal knowledge.

% In the context of tool use, this means evaluating whether a query can be answered independently or requires external tools, based on its complexity and the model’s internal knowledge.


To quantify meta-cognition, we train a probe that detects the model's level of meta-cognitive awareness. This probe evaluates the rationale behind the model's decision-making process, providing a score that reflects the model's self-assessment accuracy.
For instance, when the model receives a complex mathematical query, the meta-cognition probe assesses whether it correctly decides to solve the problem itself or delegate it to a calculator. A high meta-cognition score indicates accurate self-assessment, while a low score suggests either an incorrect attempt to solve it independently or unnecessary reliance on the tool.


\subsection{Meta-Cognition Probe Extraction}
Training a meta-cognition probe significantly differs from training probes for concepts like honesty or confidence. The latter concepts typically involve true-false statements about facts, such as "fire needs oxygen to burn" and "oxygen is harmful to human breathing". These statements are independent of user queries, meaning the model produces the same statements regardless of the user query.

In contrast, detecting the model's internal cognition regarding tool use requires query-dependent responses. To achieve this, we employ leading proprietary LLM to generate user queries related to tool use and their corresponding responses (\ie Yes/No responses with brief explanations). We then construct the training dataset following the procedures outlined in Section \ref{sec:background}. 

Notably, only a small dataset (of query-response pairs) suffices for strong probe performance. The analysis of the relationship between probe performance and the size of the training data is provided in Appendix~\ref{app:sec:probe}. Specifically, after collecting the instruction response pairs $(q_i,a_i)$, where $i$ denotes the index of the queries. We gather the sets of internal representations from the paired data and compute $A^{\pm}_f$ according to Eq.~\ref{eq:RepE}, and then apply PCA to the input $\{(-1)^i(A_{f,i}^+-A_{f,i}^-)\}$ to obtain the first principal component $\nu_f$ as the meta-cognition probe. This principal component vector identifies the direction that represents the underlying meta-cognition concept.






After the training procedure described above, we obtain the meta-cognition probes across all model layers (\eg 32 probes for \texttt{Llama-3-8B}). We compare our meta-cognition probe to existing proposed honesty \cite{zou2023representation} and confidence probes \cite{liu2024ctrla} by evaluating the intermediate classification accuracy on held-out examples where the model is instructed to exhibit either honest/confident/strong meta-cognition or dishonest/lacking confidence/weak meta-cognition. The results of the probes are shown in Figure \ref{fig:probes_comparison}. As illustrated in  Figure~\ref{fig:probes_comparison}, our meta-cognition probe achieves near-optimal accuracy, significantly outperforming prior work. 

\begin{figure}[h
]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/ICLR2025/figures/accuracy_llama3_probes.png}
  \caption{Comparison between different probes. Note that -1 means the last layer in the LLMs.}
  \label{fig:probes_comparison}
\end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/ICLR2025/figures/MeCa_new.png}
  % \includegraphics[width=1\linewidth]{ICLR2025/figures/MeCa.png}
  \caption{Overview of benchmarks: Distribution of Metatool, \dname-Tool, and \dname-RAG categories. Metatool and \dname-Tool assess the necessity of tool use, while \dname-RAG evaluates the necessity of RAG interactions.}
  \label{fig:MeCa}
\end{figure*}

    
\subsection{Decision-Making Strategy based on Meta-Cognition}
% ICLR Version
% After developing accurate probes to detect the model's internal meta-cognition, we design a decision-making strategy utilizing these detection results. Given a user query, the LLM generates a response consisting of $m$ tokens, each associated with a meta-cognition score for every layer in the LLM. This yields a meta-cognition detection array with dimensions $(m, n)$, where $n$ represents the number of layers in the LLM. Our objective is to make a final decision-"Yes", indicating the need to use external tools or RAG, and "No", indicating that LLMs can respond directly without external tools or RAG)-based on this result array.

With an accurate meta-cognition probe, we design a decision-making strategy based on detection results. For a given query, the LLM generates a response of $m$ tokens, each assigned a meta-cognition score across $n$ layers, forming a detection array of size $(m, n)$. The final decision—"Yes" (use external tools/RAG) or "No" (respond independently)—is derived from this array.

\paragraph{Reducing $m$ to 1.}
We examine various prompting strategies (detailed in {Appendix~\ref{app:sec:extend_results}) and find that the Yes/No+Explanation strategy, where the model answers with "Yes" or "No" followed by a brief explanation, yields the best performance. Therefore, we focus on the first token of the model's response as it provides a clear signal of whether the model decides to rely on external tools. Extracting the meta-cognition score of the first token to represent the whole response simplifies our decision-making process, as calculating an overall meta-cognition score for the entire response is challenging due to varying response lengths and content across different queries. Since the model always responds with "Yes" or "No" as the first token, basing the trigger mechanism on the first token's meta-cognition score is both reasonable and practical.

\paragraph{Reducing $n$ to 1.}
In \citet{zou2023representation} and \citet{liu2024ctrla}, a mean score from multiple probes' results is usually used to represent the token's final quantification. However, our experiments show that scores predicted by different probes vary significantly, and simply averaging multiple scores does not yield accurate results. We found that probes in shallower layers (\eg layer -5 to -2) tend to be more effective, with appropriate score distributions, ranges, and lower variances. Therefore, we select a single probe with the highest classification accuracy in the layer -5 to -2 (as shown in Figure \ref{fig:probes_comparison}) as the final predictor.

With meta-cognition scores distilled into a single scalar value, we apply the dual-thresholding strategy shown in Figure \ref{fig:algo_overview} to determine the optimal thresholds, $l_{yes}$ and $l_{no}$, using validation data. These thresholds are then applied to the test data.



\begin{figure*}[th]
    \centering
    \subfigure[Correct Yes/No]{
        \includegraphics[width=0.31\textwidth]{figures/rebuttal/Task1_train_wowo_first_token_correct_llama3_layer-5.png}
    }
    \hfill
    \subfigure[Correct/Incorrect Yes]{
        \includegraphics[width=0.31\textwidth]{figures/rebuttal/Task1_train_wowo_first_token_yes_llama3_layer-5.png}
    }
    \hfill
    \subfigure[Correct/Incorrect No]{
        \includegraphics[width=0.31\textwidth]{figures/rebuttal/Task1_train_wowo_first_token_no_llama3_layer-5.png}
    }
    \vskip\baselineskip
    \subfigure[Correct Yes/No]{
        \includegraphics[width=0.31\textwidth]{figures/rebuttal/Task1_train_wowo_first_token_correct_llama3-sft.png}
    }
    \hfill
    \subfigure[Correct/Incorrect Yes]{
        \includegraphics[width=0.31\textwidth]{figures/rebuttal/Task1_train_wowo_first_token_yes_llama3-sft.png}
    }
    \hfill
    \subfigure[Correct/Incorrect No]{
        \includegraphics[width=0.31\textwidth]{figures/rebuttal/Task1_train_wowo_first_token_no_llama3-sft.png}
    }
    \caption{Distribution of meta-cognition scores of the first token in model responses. (a), (b), and (c) are from LM3-8B, while (d), (e), and (f) are from LM3-8B-sft (post-fine-tuning). The scores are derived from the train data in Metatool, using prompts without context.}
    \label{fig:metacog_distribution}
\end{figure*}





% \begin{table*}[ht]
% \small
%     \centering
%     \begin{tabular}{clcccc}
%         \toprule
%          \multirow{2}{*}{\textbf{LLM}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Pre Fine-tuning}} & \multicolumn{2}{c}{\textbf{Post Fine-tuning}} \\
%         \cmidrule(lr){3-4} \cmidrule(lr){5-6}
%         & & With Context & Without Context & With Context & Without Context \\ 
%         \midrule
%         \multirow{3}{*}{\textbf{Llama3-8b}} & Naive & 61.9\% & 58.3\% & 82.1\% & 80.8\% \\
%         & $P_{\text{Yes}}$ & 63.5\% & 62.7\% & 81.7\% & 80.8\% \\
%         & \mname & \textbf{65.0\%} & \textbf{74.0\%} & \textbf{84.3\%} & \textbf{82.3\%} \\
%         \midrule
%         \multirow{3}{*}{\textbf{Llama3-70b}} & Naive & 84.6\% & 68.8\% & 86.0\% & 77.7\% \\
%         & $P_{\text{Yes}}$ & 84.8\% & 73.7\% & 86.2\% & 77.1\% \\
%         & \mname & \textbf{85.4\%} & \textbf{79.6\%} & \textbf{87.3\%} & \textbf{81.2\%} \\
%         \midrule
%         \multirow{3}{*}{\textbf{Mistral-7b}} & Naive & 69.0\% & 68.5\% & 89.2\% & 86.0\% \\
%         & $P_{\text{Yes}}$ & 71.2\% & 73.1\% & 89.2\% & 85.0\% \\
%         & \mname & \textbf{75.4\%} & \textbf{74.7\%} & \textbf{90.2\%} & \textbf{86.5\%} \\
%         \midrule
%         \textbf{GPT-4-turbo} & - &84.4\% &61.3\% & - & - \\
%         \bottomrule
%     \end{tabular}
%     \caption{Performance Comparison between Naive, $P_{\text{Yes}}$ and \mname on Metatool. Note that we are unable to calculate $P_{\text{Yes}}$ or detect the internal states of proprietary LLMs such as GPT-4-turbo.}
%     \label{tab:restuls_on_metatool}
% \end{table*}



% % result summary on MeCa
% \begin{table*}[ht]
% % \setlength\tabcolsep{4.5pt} % Reduced padding
%     \small
%     \centering
%     \begin{tabular}{cclcccc}
%         \toprule
%         \multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Pre Fine-tuning}} & \multicolumn{2}{c}{\textbf{Post Fine-tuning}} \\
%         \cmidrule(lr){4-5} \cmidrule(lr){6-7}
%         & & & with context & without context & with context & without context \\ 
%         \midrule

%         % task-1
%         \multirow{6}{*}{\textbf{Task1}} & \multirow{3}{*}{\textbf{Llama-3-8b}} & Naive & 70.0\% & 65.0\% & 69.0\% & 80.0\% \\
%         & & $P_{\text{Yes}}$ & 74.0\% & 67.0\% & \textbf{70.0\%} & 78.0\% \\
%         & & \mname & \textbf{79.0\%} & \textbf{72.0\%} & 69.0\% & \textbf{80.0\%} \\
%         \cmidrule(lr){2-7}
%         & \multirow{3}{*}{\textbf{Mistral-7b}} & Naive & 54.0\% & 63.0\% & 68.0\% & 64.0\% \\
%         & & $P_{\text{Yes}}$ & 54.0\% & 63.0\% & 69.0\% & 63.0\% \\
%         & & \mname & \textbf{58.0\%} & \textbf{67.0\%} & \textbf{71.0\%} & \textbf{66.0\%} \\
%         \cmidrule(lr){1-7}
        
%         % task-2
%         \multirow{6}{*}{\textbf{Task2}} & \multirow{3}{*}{\textbf{Llama-3-8b}} & Naive & 62.3\% & 80.3\% & 53.3\% & 61.0\% \\
%         & & $P_{\text{Yes}}$ & 78.0\% & 80.7\% & 58.3\% & 68.7\% \\
%         & & \mname & \textbf{80.1\%} & \textbf{81.3\%} & \textbf{59.9\%} & \textbf{70.3\%} \\
%         \cmidrule(lr){2-7}
%         & \multirow{3}{*}{\textbf{Mistral-7b}} & Naive & 42.3\% & 55.7\% & 52.3\% & 53.0\% \\
%         & & $P_{\text{Yes}}$ & 45.0\% & 60.0\% & 55.3\% & 62.3\% \\
%         & & \mname & \textbf{66.7\%} & \textbf{66.0\%} & \textbf{60.7\%} & \textbf{66.3\%} \\
%         \cmidrule(lr){1-7}
        
%         % task-3
%         \multirow{6}{*}{\textbf{Task3}} & \multirow{3}{*}{\textbf{Llama-3-8b}} & Naive & 54.3\% & 78.7\% & 59.0\% & 68.7\% \\
%         & & $P_{\text{Yes}}$ & 66.0\% & \textbf{81.3\%} & 57.7\% & 70.0\% \\
%         & & \mname & \textbf{73.3\%} & 79.5\% & \textbf{60.0\%} & \textbf{73.4\%} \\
%         \cmidrule(lr){2-7}
%         & \multirow{3}{*}{\textbf{Mistral-7b}} & Naive & 55.7\% & 67.3\% & 58.3\% & 73.7\% \\
%         & & $P_{\text{Yes}}$ & 56.7\% & 70.7\% & 61.0\% & 75.0\% \\
%         & & \mname & \textbf{74.8\%} & \textbf{78.3\%} & \textbf{65.7\%} & \textbf{82.0\%} \\
%         \cmidrule(lr){1-7}

%         % task-4
%         \multirow{6}{*}{\textbf{Task4}} & \multirow{3}{*}{\textbf{Llama-3-8b}} & Naive & 66.0\% & 50.0\% & 74.0\% & 77.0\% \\
%         & & $P_{\text{Yes}}$ & 66.0\% & 62.0\% & 75.0\% & 77.0\% \\
%         & & \mname & \textbf{69.0\%} & \textbf{69.0\%} & \textbf{75.0\%} & \textbf{84.5\%} \\
%         \cmidrule(lr){2-7}
%         & \multirow{3}{*}{\textbf{Mistral-7b}} & Naive & 60.5\% & 70.0\% & 92.5\% & 77.5\% \\
%         & & $P_{\text{Yes}}$ & 66.5\% & 71.0\% & 92.5\% & 80.5\% \\
%         & & \mname & \textbf{69.0\%} & \textbf{78.5\%} & \textbf{95.0\%} & \textbf{87.0\%} \\
%         \cmidrule(lr){1-7}

%         % task-5
%         \multirow{6}{*}{\textbf{Task5}} & \multirow{3}{*}{\textbf{Llama-3-8b}} & Naive & 70.5\% & 54.0\% & 71.0\% & 78.5\% \\
%         & & $P_{\text{Yes}}$ & 72.0\% & 71.5\% & \textbf{80.5\%} & \textbf{84.0\%} \\
%         & & \mname & \textbf{74.0\%} & \textbf{78.5\%} & 79.5\% & 82.0\% \\
%         \cmidrule(lr){2-7}
%         & \multirow{3}{*}{\textbf{Mistral-7b}} & Naive & 73.5\% & 76.0\% & 87.5\% & 82.0\% \\
%         & & $P_{\text{Yes}}$ & 73.0\% & 76.0\% & 87.5\% & \textbf{83.0\%} \\
%         & & \mname & \textbf{76.2\%} & \textbf{80.0\%} & \textbf{88.0\%} & 82.0\% \\
%         \cmidrule(lr){1-7}

%         % task-6
%         \multirow{6}{*}{\textbf{Task6}} & \multirow{3}{*}{\textbf{Llama-3-8b}} & Naive & 60.5\% & 53.5\% & 78.5\% & 83.0\% \\
%         & & $P_{\text{Yes}}$ & 62.0\% & 64.5\% & \textbf{81.5\%} & 82.0\% \\
%         & & \mname & \textbf{63.5\%} & \textbf{67.0\%} & 80.0\% & \textbf{86.5\%} \\
%         \cmidrule(lr){2-7}
%         & \multirow{3}{*}{\textbf{Mistral-7b}} & Naive & 73.0\% & 62.5\% & 85.0\% & 70.5\% \\
%         & & $P_{\text{Yes}}$ & 73.5\% & 63.0\% & 86.5\% & 78.0\% \\
%         & & \mname & \textbf{74.0\%} & \textbf{65.5\%} & \textbf{88.0\%} & \textbf{80.5\%} \\
        
%         \bottomrule
%     \end{tabular}
%     \caption{Performance Comparison between Naive, $P_{\text{Yes}}$ and \mname on \textbf{MeCa-Tool}.}
%     \label{tab:tooluse_on_meca}
% \end{table*}








\section{Benchmark-\dname}
%%%% ICLR Version %%%%
% We evaluate \mname using a public benchmark: Metatool~\citep{huang2023metatool}. In addition, we introduce a new benchmark, named \textbf{Me}ta-\textbf{C}ognitive Tool \textbf{A}ssessment (\dname), where each query underwent a thorough human review. \dname expands on Metatool by incorporating a broader range of scenarios for assessing tool usage. We also include tasks to evaluate adaptive RAG in \dname. Below are the details of these two benchmarks.

% \textbf{Metatool}: Metatool consists of 1,040 queries designed to assess whether LLMs can recognize when to rely on external tools to solve user queries that they cannot address directly. The tool names and descriptions in Metatool are retrieved from OpenAI's plugin list \citep{openai_chatgpt_plugins}, and there are 166 distinct tools in the benchmark. Metatool primarily focuses on evaluating the model's awareness of tool usage for individual tools, where LLMs are provided only with a user query and must independently decide whether or not to resort to external tools without any tool names or descriptions.

% Metatool has the following limitations: 1) the user queries do not have any supplementary information or tool provisions, real-world tasks typically involve more complex intents and a diverse array of requirements; 2) the testing scenarios in Metatool are monotonous, only focusing on single-round user query regarding tool use. To more accurately reflect these multifaceted scenarios and make the evaluation of \mname more robust and comprehensive, we developed a new benchmark named \dname. \dname is divided into two main components: Tool and RAG. Each query is rigorously reviewed by humans to ensure data quality and relevance. \dname provides several significant improvements over Metatool:
%%%% ICLR Version %%%%
% result summary on the metatool benchmark




We evaluate \mname using Metatool~\citep{huang2023metatool} and introduce a new benchmark, Meta-Cognitive Tool Assessment (\dname), where each query undergoes human review. \dname extends Metatool by incorporating a broader range of scenarios to assess tool usage and adaptive RAG.

Metatool comprises 1,040 queries designed to evaluate whether LLMs recognize when to use external tools. It includes 166 tools sourced from OpenAI’s plugin list \citep{openai_chatgpt_plugins}. In Metatool, LLMs must decide on tool usage based solely on user queries, without tool names or descriptions. 
However, Metatool has limitations: 1) queries lack supplementary information or explicit tool provisions, whereas real-world tasks involve more complex intents and diverse requirements; 2) it primarily focuses on single-turn tool usage decisions. To address these gaps and ensure a more robust evaluation of \mname, we developed \dname, including two main components, \dname-Tool and \dname-RAG. 

\dname-Tool enhances Metatool by expanding tool-related assessments into three key categories:
\begin{itemize}[leftmargin=*] 
\item \textbf{Tool Usage Assessment}: Evaluates whether an LLM should invoke external tools. 
\begin{itemize} 
\item Queries solvable by the LLM without tools.
\item Queries requiring one or more tools due to insufficient internal capabilities.
\end{itemize}
\item \textbf{Provided Tool Evaluation}: Tests the LLM's ability to determine tool usage when provided with a predefined set of tools. 
\begin{itemize} \item Cases where external tools are unnecessary.
\item Cases where essential tools are available and should be used. 
\item Cases where required tools are missing.
\end{itemize}
\item \textbf{Multi-turn Interaction}: Assesses tool usage decisions in multi-turn dialogues, requiring adaptation to evolving contexts.
\begin{itemize} \item Cases where external tools are unnecessary.
\item Cases where essential tools are available and should be used. 
\item Cases where required tools are missing.
\end{itemize}
\end{itemize}

Specifically, we create six evaluation tasks in \dname-Tool to systematically assess an LLM’s ability to make tool-related decisions across different scenarios. Tasks 1 and 4 evaluate whether an external tool is necessary to solve a query. Tasks 2 and 5 test the LLM’s ability to determine the relevance of a provided tool for a given query, including cases where the tool is irrelevant. Tasks 3 and 6 further extend this evaluation by presenting multiple tools (ranging from 2 to 5) and requiring the LLM to select the appropriate one. Notably, Tasks 1–3 focus on single-turn scenarios, while Tasks 4–6 incorporate multi-turn dialogues, assessing how well the LLM adapts its tool-use decisions in evolving conversational contexts. \dname-Tool significantly expands Metatool by covering six tasks with 7,000 queries, providing a more diverse and comprehensive evaluation framework. The query composition of Metatool and \dname-Tool is illustrated in Figure \ref{fig:MeCa}. Table~\ref{tab:MeCa} in Appendix~\ref{app:sec:meca} provides detailed statistics for each task.



%%%% ICLR Version %%%%
% \begin{itemize}[leftmargin=*] 
% \item \textbf{Tool Usage Assessment}. This category expands the Metatool to evaluate the decision-making capability of the LLM regarding tool usage in more comprehensive scenarios, specifically whether to invoke any external tools. It includes: 
% \begin{itemize} 
% \item Queries that can be handled by the LLM's internal capabilities without external tools. 
% \item Queries that necessitate the use of one or more external tools, indicating tasks beyond the LLM's standalone capacity. 
% \end{itemize}
% \item \textbf{Provided Tool Evaluation}. In this category, the LLM assistant is provided with available tools alongside the user query, with the task of determining tool usage based on the tools' relevance and necessity: \begin{itemize} \item Cases where the external tools are unnecessary and the LLM assistant can successfully resolve the queries independently. 
% \item Situations where external tools are essential and are provided, enabling the LLM assistant to effectively address the user queries. 
% \item Instances where the required tools to solve the queries are absent from the provided list. 
% \end{itemize}
% \item \textbf{Multi-turn Interaction}. This category evaluates the LLM's decision-making regarding tool usage in multi-turn dialogues, involving extended interactions and long context accumulation. This setup tests the LLM's adaptability and decision-making in complex, evolving scenarios that also encompass the aforementioned categories
% \end{itemize} 
% Based on the main categories outlined, \dname-Tool offers a broader and more varied test set compared to Metatool by integrating diverse scenarios through combinations of these main categories. Specifically, the final configuration covers 6 tasks with 7,000 queries. Please refer to Table~\ref{tab:MeCa} in \textcolor{blue}{Appendix~\ref{app:sec:meca}} for more details about \dname. 

% Furthermore, as noted in our paper, this work aligns with the emerging trend of adopting an adaptive RAG (Retrieval-Augmented Generation) paradigm, which aims to determine whether a query can be answered directly by the LLM or necessitates external data retrieval. This is because RAG can be viewed as a special case of tool usage, where the LLM’s internal knowledge or capabilities are insufficient to address the query, requiring access to external datasets through retrieval tools.
% \paragraph{\dname-RAG} The “RAG” component \textcolor{blue}{was} specifically designed to evaluate whether and when retrieval is necessary.
% \begin{itemize}[leftmargin=*]
%     \item Positive RAG: cases where the LLM assistant needs to perform retrieval to answer complex queries or queries involving the latest information that LLMs do not have.
%     \item Negative RAG: cases where the LLM assistant can directly respond to simple queries using its internal knowledge, without the need for retrieval.
% \end{itemize}
%%%% ICLR Version %%%%

\begin{table}[t]
\small
\renewcommand{\arraystretch}{1.1}
\resizebox{\linewidth}{!}{
    \centering
    \begin{tabular}{cl|cc|cc}
        \toprule
         \multicolumn{2}{c|}{ \multirow{3}{*}{\textbf{Method}}} & \multicolumn{2}{c|}{\textbf{Pre Fine-tuning}} & \multicolumn{2}{c}{\textbf{Post Fine-tuning}} \\
        \cmidrule(lr){3-4} \cmidrule(lr){5-6}
        & & w/ ctx & w/o ctx & w/ ctx & w/o ctx \\ 
        \midrule

        \parbox[t]{2.0mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\textit{\textcolor{gray}{\textbf{LM3-8B}}}}} } 
        & Naive & 61.9 & 58.3 & 82.1 & 80.8 \\
        & $P_{\text{Yes}}$ & 63.5 & 62.7 & 81.7 & 80.8 \\
        & \mname & \textbf{65.0} & \textbf{74.0} & \textbf{84.3} & \textbf{82.3} \\
        \midrule
        \parbox[t]{2.0mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\textit{\textcolor{gray}{\textbf{LM3-70B}}}}} }
        & Naive & 84.6 & 68.8 & 86.0 & 77.7 \\
        & $P_{\text{Yes}}$ & 84.8 & 73.7 & 86.2 & 77.1 \\
        & \mname & \textbf{85.4} & \textbf{79.6} & \textbf{87.3} & \textbf{81.2} \\
        \midrule
        \parbox[t]{2.0mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\textit{\textcolor{gray}{\textbf{Mist-7B}}}}} }
        
        % \multirow{3}{*}{\textbf{Mistral-7b}} 
        & Naive & 69.0 & 68.5 & 89.2 & 86.0 \\
        & $P_{\text{Yes}}$ & 71.2 & 73.1 & 89.2 & 85.0 \\
        & \mname & \textbf{75.4} & \textbf{74.7} & \textbf{90.2} & \textbf{86.5} \\
        \midrule
        \multicolumn{2}{c|}{\textbf{GPT-4-turbo}} &84.4 &61.3 & - & - \\
        \bottomrule
    \end{tabular}}
    \caption{Performance Comparison between Naive, $P_{\text{Yes}}$ and \mname on Metatool. Note that we are unable to calculate $P_{\text{Yes}}$ or detect the internal states of proprietary LLMs such as GPT-4-turbo.}
    \label{tab:restuls_on_metatool}
\end{table}

\begin{table*}[t]
    \centering
    \small
    \setlength{\tabcolsep}{3.8pt} % 调整列间距
    \renewcommand{\arraystretch}{1.1} % 调整行距
    \begin{tabular}{cc|cc|cc|cc|cc|cc|cc}
        \toprule
        \multicolumn{2}{c|}{ \multirow{3}{*}{\textbf{Method}} }
        & \multicolumn{2}{c|}{\textbf{Task 1}} 
        & \multicolumn{2}{c|}{\textbf{Task 2}} 
        & \multicolumn{2}{c|}{\textbf{Task 3}} 
        & \multicolumn{2}{c|}{\textbf{Task 4}} 
        & \multicolumn{2}{c|}{\textbf{Task 5}} 
        & \multicolumn{2}{c}{\textbf{Task 6}} \\ 
        \cmidrule(lr){3-14} 
        & & w/ ctx & w/o ctx
        & w/ ctx& w/o ctx
        & w/ ctx& w/o ctx
        & w/ ctx& w/o ctx
        & w/ ctx& w/o ctx
        & w/ ctx& w/o ctx\\ 
        \midrule
        \parbox[t]{2.0mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\textit{\textcolor{gray}{\textbf{LM3-8B}}}}} } 
        & Naive 
          & 70.0 & 65.0  % Task1
          & 62.3 & 80.3  % Task2
          & 54.3 & 78.7  % Task3
          & 66.0 & 50.0  % Task4
          & 70.5 & 54.0  % Task5
          & 60.5 & 53.5  % Task6
          \\
        & $P_{Yes}$     
          & 74.0 & 67.0  % Task1
          & 78.0 & 80.7  % Task2
          & 66.0 & \textbf{81.3}  % Task3
          & 66.0 & 62.0  % Task4
          & 72.0 & 71.5  % Task5
          & 62.0 & 64.5  % Task6
          \\
        & \textbf{MeCo}  
          & \textbf{79.0} & \textbf{72.0}  % Task1
          & \textbf{80.1} & \textbf{81.3}  % Task2
          & \textbf{73.3} & {79.5}  % Task3
          & \textbf{69.0} & \textbf{69.0}  % Task4
          & \textbf{74.0} & \textbf{78.5}  % Task5
          & \textbf{63.5} & \textbf{67.0}  % Task6
          \\
        \midrule
        \parbox[t]{2.0mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\textit{\textcolor{gray}{\textbf{Mist-7B}}}}} }  
        & Naive 
          & 54.0 & 63.0  % Task1
          & 42.3 & 55.7  % Task2
          & 55.7 & 67.3  % Task3
          & 60.5 & 70.0  % Task4
          & 73.5 & 76.0  % Task5
          & 73.0 & 62.5  % Task6
          \\
        & $P_{Yes}$     
          & 54.0 & 63.0  % Task1
          & 45.0 & 60.0  % Task2
          & 56.7 & 70.7  % Task3
          & 66.5 & 71.0  % Task4
          & 73.0 & 76.0  % Task5
          & 73.5 & 63.0  % Task6
          \\
        & \textbf{MeCo}
          & \textbf{58.0} & \textbf{67.0}  % Task1
          & \textbf{66.7} & \textbf{66.0}  % Task2
          & \textbf{74.8} & \textbf{78.3}  % Task3
          & \textbf{69.0} & \textbf{78.5}  % Task4
          & \textbf{76.2} & \textbf{80.0}  % Task5
          & \textbf{74.0} & \textbf{65.5}  % Task6
          \\
        \bottomrule
    \end{tabular}
    \caption{Performance Comparison between Naive, $P_{\text{Yes}}$ and \mname on \textbf{MeCa-Tool} before fine-tuning. "With context" and "without context" are abbreviated as w/ ctx and w/o ctx for brevity.}
    \label{tab:meca_performance_pre}
\end{table*}


\begin{table*}[th]
    \centering
    \small
    \setlength{\tabcolsep}{3.8pt} % 调整列间距
    \renewcommand{\arraystretch}{1.3} % 调整行距
    \begin{tabular}{cc|cc|cc|cc|cc|cc|cc}
        \toprule
        \multicolumn{2}{c|}{ \multirow{3}{*}{\textbf{Method}} }
        & \multicolumn{2}{c|}{\textbf{Task 1}} 
        & \multicolumn{2}{c|}{\textbf{Task 2}} 
        & \multicolumn{2}{c|}{\textbf{Task 3}} 
        & \multicolumn{2}{c|}{\textbf{Task 4}} 
        & \multicolumn{2}{c|}{\textbf{Task 5}} 
        & \multicolumn{2}{c}{\textbf{Task 6}} \\ 
        \cmidrule(lr){3-14} 
        & & w/ ctx & w/o ctx
        & w/ ctx& w/o ctx
        & w/ ctx& w/o ctx
        & w/ ctx& w/o ctx
        & w/ ctx& w/o ctx
        & w/ ctx& w/o ctx\\ 
        \midrule
        \parbox[t]{2.0mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\textit{\textcolor{gray}{\textbf{LM3-8B-sft}}}}} } 
        & Naive & 69.0 & 80.0  % Task1
          & 53.3 & 61.0  % Task2
          & 59.0 & 68.7  % Task3
          & 74.0 & 77.0  % Task4
          & 71.0 & 78.5  % Task5
          & 78.5 & 83.0  % Task6
          \\
        & $P_{Yes}$     & \textbf{70.0} & 78.0  % Task1
          & 58.3 & 68.7  % Task2
          & 57.7 & 70.0  % Task3
          & 75.0 & 77.0  % Task4
          & \textbf{80.5} & \textbf{84.0}  % Task5
          & \textbf{81.5} & 82.0  % Task6
          \\
        & \textbf{MeCo}  & 69.0 & \textbf{80.0}  % Task1
          & \textbf{59.9} & \textbf{70.3}  % Task2
          & \textbf{60.0} & \textbf{73.4}  % Task3
          & \textbf{75.0} & \textbf{84.5}  % Task4
          & 79.5 & 82.0  % Task5
          & 80.0 & \textbf{86.5}  % Task6 
          \\
        \midrule
        \parbox[t]{2.0mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\textit{\textcolor{gray}{\textbf{Mist-7B-sft}}}}} }  
        & Naive & 68.0 & 64.0  % Task1
          & 52.3 & 53.0  % Task2
          & 58.3 & 73.7  % Task3
          & 92.5 & 77.5  % Task4
          & 87.5 & 82.0  % Task5
          & 85.0 & 70.5  % Task6
          \\
        & $P_{Yes}$     & 69.0 & 63.0  % Task1
          & 55.3 & 62.3  % Task2
          & 61.0 & 75.0  % Task3
          & 92.5 & 80.5  % Task4
          & 87.5 & \textbf{83.0}  % Task5
          & 86.5 & 78.0  % Task6
          \\
        & \textbf{MeCo}  & \textbf{71.0} & \textbf{66.0}  % Task1
          & \textbf{60.7} & \textbf{66.3}  % Task2
          & \textbf{65.7} & \textbf{82.0}  % Task3
          & \textbf{95.0} & \textbf{87.0}  % Task4
          & \textbf{88.0} & 82.0  % Task5
          & \textbf{88.0} & \textbf{80.5}  % Task6
          \\
        \bottomrule
    \end{tabular}
    \caption{Performance Comparison between Naive, $P_{\text{Yes}}$ and \mname on \textbf{MeCa-Tool} after fine-tuning on the crafted SFT data. "With context" and "without context" are abbreviated as w/ ctx and w/o ctx for brevity.}
    \label{tab:meca_performance_post}
\end{table*}




Beyond tool use, \dname-RAG also evaluates adaptive RAG—determining whether a query can be answered directly by the LLM or requires external retrieval. RAG can be seen as a special case of tool use, where the LLM’s internal knowledge is insufficient and necessitates using a search engine to access external data. \dname-RAG consists of:
\begin{itemize}[leftmargin=*] 
\item \textbf{Positive RAG}: Cases where retrieval is essential for answering complex queries or those requiring up-to-date information.
\item \textbf{Negative RAG}: Cases where LLMs can answer using its internal knowledge without retrieval.
\end{itemize}


To curate the \dname-Tool dataset, we employ a meticulous approach that began with collecting diverse scenarios from various online corpora, ensuring our synthetic APIs and conversations were grounded in realistic contexts. Based on these scenarios, we designed 500 distinct synthetic APIs emulating real-world applications across various domains and generated queries by randomly sampling from this API pool—queries that either require tool invocation, rely on the LLM’s internal knowledge, or expose cases where the provided APIs are insufficient—followed by rigorous human verification for accuracy. For the \dname-RAG dataset, we selected common fact-based data from the RepE~\citep{zou2023representation} dataset (e.g., ``The Earth orbits the Sun”) to generate negative RAG queries that do not require retrieval, while positive RAG queries were generated from crawled recent news unseen by LLMs, necessitating retrieval of new information. 

By incorporating both tool usage and adaptive RAG, \dname serves as a robust benchmark for assessing LLM decision-making in complex scenarios.  Detailed benchmark statistics and creation methodology for \dname-RAG can be found in Appendix~\ref{app:sec:meca}.




\section{Experiment Setup}\label{sec:setup}
% ICLR Version
% \textbf{Baselines:} We evaluate the proposed \mname against two baselines: Naive and $P_{\text{Yes}}$. The Naive baseline determines a "Yes" or "No" based solely on the first token generated by the LLM, where "Yes" represents a positive indication, \ie requiring external tools, and vice versa. In the $P_{\text{Yes}}$ baseline, we compute a \textit{Yes-score}, as outlined in Equation~\ref{eq:p_yes}, which offers a more refined measure of the model's confidence compared to the binary Naive approach. Note that the \textit{Yes-score} ranges from $0$ to $1$, where $0$ represents full "No" and $1$ denotes full "Yes". The proximity of the \textit{Yes-score} to $0.5$ indicates a lower certainty in the model's response, as scores around this midpoint reflect ambiguity in decision-making. $P_{\text{Yes}}$ can adjust the model's output in cases where the \textit{Yes-score} is near $0.5$ to enhance the accuracy of both tool use and retrieval timing. Refer to Section~\ref{app:subsec:p_yes} for more details.

\textbf{Baselines}: We evaluate \mname against two baselines: Naive and $P_{\text{Yes}}$. The Naive baseline determines "Yes" or "No" based solely on the first token generated by the LLM, where "Yes" represents a positive indication, \ie requiring external tools, and vice versa. The $P_{\text{Yes}}$ baseline refines this approach by computing a {\em Yes-score}, 
$$
\text{Yes-score} = \frac{P(\text{Yes }|\text{ Prompt})}{P(\text{Yes }|\text{ Prompt}) + P(\text{No }|\text{ Prompt})} 
$$
which ranges from 0 (full "No") to 1 (full "Yes"), with values near 0.5 indicating uncertainty. Instead of relying solely on the first token, $P_{\text{Yes}}$ learns an optimal threshold: scores above this threshold are classified as "Yes," while those below are classified as "No." Further details are in Section~\ref{app:subsec:p_yes}.


% ICLR Version
% \textbf{Backbone LLMs}: We evaluate two widely-used LLMs, \ie \texttt{Llama-3-8b-Instruct} and \texttt{Mistral-7b-Instruct-v0.3}. \textcolor{blue}{Additionally, to assess the effectiveness of MeCo on larger LLMs, we conducted experiments on \texttt{Llama-3-70b-Instruct}}. For conciseness, we refer to them as Llama-3-8b, \textcolor{blue}{Llama-3-70b}, and Mistral-7b throughout the paper, respectively. We also fine-tune these models with data generated by leading proprietary LLM, and the fine-tuned models are denoted as Llama-3-8b-sft, \textcolor{blue}{Llama-3-70b-sft}, and Mistral-7b-v0.3-sft, respectively.

\textbf{Backbone LLMs}: We employ \textit{Llama-3-8B-Instruct}, \textit{Mistral-7B-Instruct-v0.3}, and \textit{Llama-3-70B-Instruct} as backbone models to evaluate \mname. For brevity, we refer to them as LM3-8B, Mist-7B, and LM3-70B. Additionally, we fine-tune these models on a dataset of 4,000 tool-use query-response pairs generated by GPT-4-turbo, denoting the fine-tuned versions as LM3-8B-sft, Mist-7B-sft, and LM3-70B-sft.


% ICLR Version
% \textbf{Evaluation:} Our experiments primarily focus on the overall accuracy of decisions regarding the necessity of tool use. A tool use decision is considered correct if the query genuinely requires external tools and incorrect otherwise. An analysis of additional performance metrics (including precision, recall, etc.) is provided in \textcolor{blue}{Appendix~\ref{app:sec:extend_results}}.

\textbf{Evaluation}: The primary evaluation metric is decision accuracy—whether the model correctly identifies when external tools/RAG are genuinely needed. Additional metrics (precision, recall, etc.) are analyzed in Appendix~\ref{app:sec:extend_results}.

% ICLR Version
% \textbf{Prompt}: We explored various prompting strategies, including "Yes/No" responses with or without explanation and the Chain of Thought (\textcolor{blue}{CoT;~\cite{wei2022chain}}) approach. Our findings indicate that instructing the model first to provide a "Yes" or "No" response followed by an explanation yields better results than other strategies, including the CoT approach. Detailed results for different prompting strategies are available in \textcolor{blue}{Appendix~\ref{app:sec:extend_results}}. Consequently, all experiments in this paper utilize the "Yes/No + Explanation" prompting strategy.

\textbf{Prompting Strategies}: We experimented with various prompting strategies, including "Yes/No" with or without explanations and Chain-of-Thought (CoT; \cite{wei2022chain}). The best performance was achieved using the "Yes/No + Explanation" strategy, which is used throughout this paper. Detailed results for different prompting strategies are available in Appendix~\ref{app:sec:extend_results}. 

Moreover, we employ two types of prompts in our experiments: 1) prompts with context, which include specific reasons why LLMs may require external tools to complete user tasks, plus five randomly sampled examples to assist the model in making decisions; and 2) prompts without context, a concise version containing only the instruction and query. Exact prompts and settings are detailed in Appendix~\ref{app:sec:probe}. 



\section{Experiments}
We conduct extensive experiments to empirically reveal the effectiveness of \mname on two benchmarks: Metatool and \dname. Specifically, we evaluate \mname in adaptive tool use on both Metatool and \dname and in adaptive RAG on \dname. 

\subsection{\mname in Adaptive Tool Use}
First, we present the distribution of meta-cognition scores collected from the pre- and post-fine-tuning models in Figure \ref{fig:metacog_distribution}. We compare the meta-cognition scores for correct and incorrect responses and visualize how these scores differentiate between correct and incorrect Yes/No answers. Our key observations and interpretations are as follows: 
\begin{itemize}[leftmargin=*]
    \item \textbf{Clear Gap in Meta-Cognition Scores:} In both pre- and post-fine-tuning experiments, there is a noticeable gap between the meta-cognition scores of correct and incorrect responses. Our decision-making strategy can identify and leverage this gap to distinguish between correct and incorrect Yes/No answers. 
    \item \textbf{Higher Scores for Correct Yes, Lower for Correct No}: Correct Yes responses generally have higher meta-cognition scores than incorrect Yes responses, while correct No responses have lower scores than incorrect No responses. This occurs because the meta-cognition score for Yes/No tokens depends on the token embedding. Therefore, the meta-cognition scores of different tokens are not directly comparable; the score of Yes should only be compared to other Yes scores, and the score of No should only be compared to other No scores. 
\end{itemize}


In our experiment, we sampled a subset of queries from the Metatool benchmark to create training data for determining the optimal thresholds for $P_{\text{Yes}}$ and \mname. We then applied these thresholds to the test queries in both Metatool and \dname-Tool (Task1 and Task4). Due to the fundamental difference between the queries in Metatool and those in Task 2, Task 3, Task 5, and Task 6 in \dname-Tool, we randomly sample 100 queries from each of these categories to serve as hold-out testing data. We fit the thresholds for both $P_{\text{Yes}}$ and \mname using the remaining data. The complete evaluation results are summarized in Table \ref{tab:restuls_on_metatool}, Table \ref{tab:meca_performance_pre}, and Table \ref{tab:meca_performance_post}. We make two key observations:

\textbf{1. Superiority of \mname:} On both benchmarks, \mname significantly enhances the model's naive decision accuracy regarding tool use, outperforming $P_{\text{Yes}}$ by a considerable margin, indicating the effectiveness of the meta-cognition-based trigger mechanism. Notably, \mname's superiority is consistent across multiple backbone models and various evaluation settings, including both with and without context, as well as pre- and post-fine-tuning. 

Importantly, the improvement achieved with \mname incurs minimal costs, as it involves a fine-tuning-free and easy-to-integrate module. Note that fine-tuning and MeCo are two orthogonal approaches, and MeCo can provide additional benefits to fine-tuned models. Moreover, fine-tuned models do not transfer well to "out-of-distribution" testing scenarios. For instance, we observed performance degradation in the fine-tuned LM3-8B on Task 2 and Task 3 of MeCa-Tool. In contrast, the improvement brought by MeCo is consistent and robust across various testing scenarios.

\mname's superiority on \dname is particularly promising and significant. \dname contains more complex and realistic queries and user-assistant interactions, closely mimicking real-world scenarios. This underscores MeCo's applicability to real-world LLMs, highlighting its potential for practical deployment and effectiveness in diverse and realistic scenarios.

\textbf{2. Transferability:} The results of Task1 and Task4 in Table \ref{tab:meca_performance_pre} and Table \ref{tab:meca_performance_post} indicate that $P_{\text{Yes}}$ and \mname, when fitted on one benchmark, can effectively transfer to other benchmarks. It's worth noting that Metatool and \dname feature different tool sources and styles of queries. We hypothesize that the model's internal cognition is model-dependent, and once fitted on one benchmark, the decision strategy (\ie the thresholds) can be transferred to other testing datasets. Although it is always better to align the decision strategy with real testing data, \mname demonstrates satisfactory performance even when directly transferred, highlighting its robustness and adaptability.

\begin{table}[ht]
\small
    \centering
    \begin{tabular}{clc}
        \toprule
        {\textbf{Model}} & \textbf{Method} & \textbf{Accuracy} \\
        \midrule
        \multirow{3}{*}{\textbf{LM3-8B}} & Naive & 63.0 \\
        & $P_{\text{Yes}}$ & 75.0 \\
        & \mname & \textbf{76.0} \\
        \midrule
        \multirow{3}{*}{\textbf{Mist-7B}} & Naive & 84.0 \\
        & $P_{\text{Yes}}$ & 84.0 \\
        & \mname & \textbf{86.0} \\
        \midrule
        \textbf{GPT-3.5-Turbo} & - &86.0 \\
        \midrule
        \textbf{GPT-4-Turbo} & - &84.0 \\
        \bottomrule
    \end{tabular}
    \caption{Performance Comparison between Naive, $P_{\text{Yes}}$ and \mname on \textbf{MeCa-RAG}.}
    \label{tab:rag_on_meca}
\end{table}

\subsection{\mname in Adaptive RAG}
We further evaluate the effectiveness of \mname in the adaptive RAG task, where the LLMs need to determine whether or not to retrieve external information to address the user query. Typically, no reasons or examples are provided to the LLMs in adaptive RAG, and we follow this setting by providing no context in the prompts. The results in Table \ref{tab:rag_on_meca} further validate the effectiveness of \mname in the adaptive RAG task, demonstrating its robustness as a trigger mechanism across various applications. Note that GPT-4-turbo has more up-to-date information and thus does not perform RAG as often as GPT-3.5-turbo and results in a lower accuracy on our benchmark.


\section{Conclusion}
This paper introduces the concept of adaptive tool use to advance existing tool learning paradigms, which typically rely on external tools without discrimination to address user queries. We present \mname, a computationally efficient plug-in module that assesses LLM's meta-cognitive states, using a probe to detect relevant signals and inform more accurate tool-use decisions. To support evaluation, we introduce a new benchmark, \dname, specifically designed to evaluate LLMs' awareness of tool use as well as the necessity for retrieval. We empirically validate the effectiveness of \mname using both the Metatool and \dname, demonstrating significant improvements in the model's decision-making accuracy regarding the necessity for tool use and retrieval. Our findings suggest that by integrating meta-cognition into the tool usage framework, we can enhance the operational efficiency and decision-making capabilities of LLMs across diverse contexts.

% This paper introduces adaptive tool use to improve existing tool learning paradigms, which often rely on indiscriminate use of external tools. We present \mname, a computationally efficient module that assesses LLMs’ meta-cognitive states, using a probe to detect relevant signals and inform more accurate tool-use decisions. We also introduce the \dname benchmark to evaluate LLMs’ tool-use awareness and retrieval necessity. Our experiments on Metatool and \dname show significant improvements in decision-making accuracy, suggesting that integrating meta-cognition enhances LLMs' efficiency and decision-making across contexts.

\section{Limitations}
% We have omitted a fine-grained evaluation in the MeCa benchmark, which assesses the model's end-to-end performance in successfully using the correct tool. This includes tasks such as determining the necessity of tool use, template filling, answer retrieval and generation, etc. These tasks demand capabilities beyond tool-use decision-making and would involve significant human effort to evaluate final outputs. Our focus in this work is to introduce adaptive tool use and leverage meta-cognition for self-assessment in determining when tool use is necessary. We consider a comprehensive benchmark that assesses the full spectrum of tool-use capabilities in LLMs as a direction for future work.

We have omitted a fine-grained evaluation in the MeCa benchmark that measures the model's end-to-end performance in correctly using tools with appropriate parameters. This includes determining whether tool use is necessary, parameter filling, and answer generation, etc. These tasks require capabilities beyond tool-use decision-making and would involve significant human effort to evaluate the final outputs. Our work mainly focuses on introducing adaptive tool use and leveraging meta-cognition for self-assessment in deciding when tool use is necessary. Although applying \mname to parameter filling for tool use in LLMs is feasible, it also introduces additional latency. How to better apply \mname to improve the parameter filling accuracy and the end-to-end evaluation remains an important direction for future work.
% \begin{enumerate}
    % \item End-to-end performance evaluation is absent, because there is no such benchmark. Tool use benchmarks are all fake API call for now.
    % \item template-filling requires more decision-making, which is much more complex than yes-no tool invocation. This is our future work. 
% \end{enumerate}



% \clearpage

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{main}

\clearpage
\appendix
\input{appendix}

\end{document}
