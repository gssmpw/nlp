\pdfoutput=1
\documentclass[runningheads]{llncs}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[misc]{ifsym}
\newcommand{\corr}{(\Letter)}
% N.B.: do not change anything above this line. If you require additional packages, please load them directly after this line.

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting   
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{xcolor}         % colors
\usepackage{amsmath} 
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[inkscapelatex=false]{svg}

\newcommand{\jacob}[1]{\todo[inline]{\textbf{Jacob: }#1}}
\newcommand{\peter}[1]{\todo[inline]{\textbf{Peter: }#1}}
\newcommand{\mogens}[1]{\todo[inline]{\textbf{Mogens: }#1}}
\usepackage[colorinlistoftodos,bordercolor=orange,backgroundcolor=orange!20,linecolor=orange,textsize=scriptsize]{todonotes}

\begin{document}

\title{FlexDeMo: Decoupled Momentum Optimization for Hybrid Sharded Data Parallel Training}

\titlerunning{FlexDeMo}

\author{Mogens Henrik From \and Jacob Nielsen \and Lukas Galke \and Peter Schneider-Kamp}

\authorrunning{From, Nielsen, Galke, \& Schneider-Kamp}


\institute{Department of Mathematics and Computer Science, University of Southern Denmark, Odense, Denmark\\
\email{\{from, jacn, galke, petersk\}@imada.sdu.dk}}

\maketitle              % typeset the header of the contribution

\begin{abstract}
Training large neural network models requires extensive computational resources, often distributed across several nodes and accelerators. Recent findings suggest that it may be sufficient to only exchange the fast moving components of the gradients, while accumulating momentum locally (Decoupled Momentum, or DeMo). However, when considering larger models that do not fit on a single accelerator, the exchange of gradient information and the integration of DeMo needs to be reconsidered. Here, we propose employing a hybrid sharded data parallel training strategy, FlexDeMo, whereby nodes fully shard model parameters locally between different accelerators, while inter-node communication bandwidth requirements are reduced by synchronizing only fast-moving components instead of the full gradients. This effectively combines previous hybrid sharded strategies with the advantages of decoupled momentum. Our experimental results show that FlexDeMo is on par with hybrid sharded data parallel training employing AdamW and full gradient synchronization in terms of validation loss, demonstrating its viability. Furthermore, FlexDeMo achieves improved training speed compared to full gradient synchronization across nodes. In a bandwidth-constrained 2-node setup, FlexDeMo allows reaching desired levels of validation loss faster than hybrid sharded data parallel training with full gradient synchronization.

\keywords{deep learning \and distributed training \and network-aware training \and green machine learning \and large language models}
\end{abstract}

\input{sections/introduction.tex}
\input{sections/rw.tex}
\input{sections/method.tex}
\input{sections/results.tex}
\input{sections/discussion}
\input{sections/conclusion}

\section{Limitations}
Due to computational constraints, we conducted our experiments on a subset of the datasets training and validation sets. The smaller dataset might have made the model overfit to a certain degree for some of the configurations. We acknowledge that this could be an artifact in our results, yet this does not invalidate them. A further limitation is that the proposed training strategy with nodes as accelerator groups requires the model to fit within one node, which, while it relaxes the requirements from one single accelerator, still poses a limitation for very large models.
f the models (maybe an discussion thing)

\section{Ethical Considerations}
Our work aims to improve training efficiency when employing a distributed training strategy. It may contribute to the democratization of training large language models and to reducing the environmental footprint of LLMs. In particular, our findings suggest that LLMs potentially can be trained on a lower budget. We acknowledge that this may make training such models more accessible. However, this work is purely scientific and does not promote easier access to systems for training such models. Using our proposed method, it might be possible to reduce compute budgets, making larger models more financially accessible. 

\bibliographystyle{splncs04}
\bibliography{sample.bib}
\end{document}
