\section{Method}\label{sec:method}
Here, we present our hybrid sharded FSDP decoupled momentum optimizer FlexDeMo, which allows divergent optimizer states across accelerator nodes\footnote{Sharding to the accelerators of a node and replicating between the nodes is a reasonable strategy. FlexDeMo can be used to shard and replicate between any sets of accelerators, though, allowing both replicating model shards to multiple subsets of accelerators on one node and distributing different shards across multiple nodes.}, guided by the fast moving components of the momentum, computed via the discrete cosine transform (DCT-II). This transform can be implemented for efficient computing during each optimization step, allowing the results to be shared efficiently with relatively low network requirements. This enables the efficient training of large language models on multiple nodes, e.g., in HPCs where the interconnects become the bottleneck, or even using geographically-dispersed nodes. We will first provide a background on DeMO~\cite{peng2024demodecoupledmomentumoptimization}, which we build on, before describing our extension, FlexDeMO.

\subsection{Background on DeMO}
The key idea of DeMO~\cite{peng2024demodecoupledmomentumoptimization} is to reduce communication overhead by keeping optimizer states local and only communicating fast moving components between training instances, as calculated via a discrete cosine transform (DCT) operation.
DeMo introduces two hyperparameters to control the extraction of fast moving components. First, the optimizer states are chunked, i.e., split into smaller parts of a pre-defined size (chunk size). Then, the top few  fast moving components are extracted (TopK). The fast moving components are exchanged between the processes using an \texttt{all\_gather} operation, followed by local decompression. We provide a description of the algorithms steps in Algorithm \ref{alg:demo}, following the method proposed by~\cite{peng2024demodecoupledmomentumoptimization}.

One of the main practical limitations of DeMo is that it does not support FSDP or other sharding strategies at all and relies on an \texttt{all\_gather} operation across multiple nodes, which is inherently slow, as it requires every training process to exchange and receive gradients from every other instance (see Figure~\ref{fig:DeMo_communication_pattern_2_nodes}).

This raises the new research question regarding whether DeMo would be suitable in an sharded setting such as FSDP, as well as investigating if and how its hyperparameters need to be adjusted -- in particular, how many fast-moving components to extract. Here, we set out to overcome these technical hurdles and investigate the efficiency characteristics of such distributed training mechanism.

\subsection{FlexDeMo}
DeMo has the limitation that the model being trained needs to fit within the memory limitations of each accelerator. By relaxing the memory-constraint by sharding the model and optimizer states across multiple accelerators, typically within one node, it becomes possible to train significantly larger models. 
During training, the optimizer states are communicated between the accelerators within the node. Then fast moving components are extracted following the method from DeMo~\cite{peng2024demodecoupledmomentumoptimization}. These components are compressed and exchanged between groups of accelerators, typically nodes. This method allows for training larger models, keeping the expensive communication within the groups of accelerators, and minimizing the expensive communication between groups.

The flexible decoupled momentum training strategy we propose is powered by a Discrete Cosine Transform (DCT) of the momenta, revealing fast and slow moving components, allowing us to select only the fast moving momentum contributions. This limited but significant selection of the momenta is what is replicated between the nodes, minimizing the communication. The replication of the optimizer states is done between the groups of accelerators instead of between all accelerators and by only communicating the fast-moving momentum components. This is shown in Figure \ref{fig:flexDeMo_communication_pattern}, where accelerator 0 of node 0 is replicating momentum components to accelerator 0 of node 1, and so on. With this method, there is no replication of data between accelerator 0 in one node and accelerator 1 in another, effectively drastically limiting the amount of data shared between nodes.

While we only share the fast moving momentum components between the nodes, and even only between accelerators with corresponding shards, we operate on the full gradients within each group, where the transfer speeds are usually significantly higher. This takes advantage of the typically high bandwidth within nodes (e.g.,  NV-Linked accelerators), while acknowledging the slower bandwidth between nodes.

To achieve this, we reimplement the stochastic gradient descent (SGD) with momentum optimizer from \cite{peng2024demodecoupledmomentumoptimization}, introducing a series of changes to support decoupled optimization in FSDP employing intra-node hybrid-sharding. We assume that the model is wrapped as an FSDP object. Both the forward and backward passes must be wrapped in a \texttt{no\_sync} context manager, disabling automatic synchronization of gradients $\theta_t$ %\todo{notation} 
affecting the default \texttt{all\_reduce}-functionality, consequently decoupling the momentum $m$ across accelerator-nodes. PyTorch's \texttt{Autograd} produces the gradient for the whole unsharded grad-parameter accessible in \texttt{p.grad}.

Figure \ref{fig:flexDeMo_communication_pattern} shows a schematic representation of the communication. We employ the reduce-scatter operation, averaging and then sharding the computed gradients back to their respective ranks (hence sharded parameter-size) in the sharding-parallel-group. This allow us to work only on the shards in the subsequent operations. 
We denote the two communication-groups as $S$ and $R$ referring to the sharding-group and replication-group, respectively. Gradients are scattered intra-node locally within $S$ and the fast components, $q$, are communicated inter-node in $R$. We describe the operation of FlexDeMo in Algorithm~\ref{alg:flexdemo}.

Figure \ref{fig:DeMo_communication_pattern_1_node} shows the communication pattern for standard DeMo within a single node, and Figure \ref{fig:DeMo_communication_pattern_2_nodes} shows the communication pattern for DeMo when extended to two nodes, with inter-node communication. DeMo does not employ any sharding, and is thus using an \texttt{all\_gather} operation across the nodes, in the case of multiple nodes. This high amount of inter-node communication, is clearly evident in the figure.


\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}
\caption{DeMo}\label{alg:demo}
    \begin{algorithmic}
        \Require learning rate $\eta$, decay $\beta \in (0,1)$, parameters $\theta_t$, momentum $m_t$, 
        \Require hyperparameters $s, k$
        \State $\Delta_t^i \leftarrow LocalSGD(\theta_t^i )$ \Comment{Get local gradient $\Delta_t$}
        \State $m_t \leftarrow \beta m_t + \Delta_t$ \Comment{Accumulate gradient in momentum $m$}
        \State $q_t \leftarrow  ExtractFastComponents(m_t, s, k)$  \Comment{Extract fast components $q$ from $m$}
        \State $m_{t+1} \leftarrow m_t - q_t$ \Comment{Remove q from m}
        \State $Q_t^i \leftarrow Synchronize(q_t)$ \Comment{Synchronize $q$ across all nodes.}
        \State $\theta_{t+1}^i \leftarrow \theta_t^i - \eta Q_t$ \Comment{Parameter update step}
    \end{algorithmic}
    \label{alg:demo}
\end{algorithm}

% Change the names of commands
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}
\caption{FlexDeMo}\label{alg:denotation}
    \begin{algorithmic}
        \Require learning rate $\eta$, decay $\beta \in (0,1)$, parameters $\theta_t$, momentum $m_t$,
        \Require sharding-set $S$, replication-set $R$, hyperparameters $s, k$
        \State $\theta_t^i \leftarrow GradReduceScatter(\theta_t, S)$ \Comment{Get local parameter shard. Intra-Node.}
        \State $\Delta_t^i \leftarrow LocalSGD(\theta_t^i )$ \Comment{Local SGD gradient}
        \State $m_t \leftarrow \beta m_t + \Delta_t$ \Comment{Accumulate gradient in momentum $m$}
        \State $q_t \leftarrow  ExtractFastComponents(m_t, s, k)$  \Comment{Extract fast components $q$ from $m$}
        \State $m_{t+1} \leftarrow m_t - q_t$ \Comment{Remove q from m}
        \State $Q_t^i \leftarrow Synchronize(q_t, R)$ \Comment{Synchronize across all nodes. Inter-Node.}
        \State $\theta_{t+1}^i \leftarrow \theta_t^i - \eta Q_t$ \Comment{Parameter update step}
    \end{algorithmic}
    \label{alg:flexdemo}
\end{algorithm}



FlexDeMo degrades gracefully to pure FSDP and pure DDP settings for trivial sharding and replication groups: In the edge-case of only sharding ($\left|R\right|$ = 1), the behaviour of FlexDeMo corresponds to FSDP. In the edge-case of only replication ($\left|S\right|$ = 1), the behaviour of FlexDeMo corresponds to DDP with DeMo-style replication. Without sharding and replication, the behaviour of FlexDeMo collapses to single-accelerator training with the underlying SGD optimizer.


\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
         \includegraphics[width=\textwidth]{Figures/demo_ddp_comm.png}
        \caption{DeMo - 1 Node}
        \label{fig:DeMo_communication_pattern_1_node}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.64\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/demo_2_nodes_comv_v2.png}
        \caption{DeMo - 2 Nodes}
        \label{fig:DeMo_communication_pattern_2_nodes}
    \end{subfigure}
    \vspace{2em}
    % Center the third figure
    \centering
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/flex_demo_comv.png}
        \caption{FlexDeMo - 2 Nodes}
        \label{fig:flexDeMo_communication_pattern}
    \end{subfigure}
    \caption{
    Communication Patterns of DeMo and FlexDeMo, with their respective intra-node and inter-node communication.
    }
\end{figure*}

\section{Experimental Setup}
The purpose of our experiments is to show that FlexDeMo reduces communication overhead in comparison with the original DeMo optimizer and FSDP with a standard AdamW optimizer~\cite{kingma2014adam,adamw}. Thus, we design a minimal experimental setting with two nodes, each with two accelerators. In this setting, we control the network bandwidth in order to simulate network congestion in real-world settings. All configurations optimize a FSDP model employing a hybrid sharding strategy over 2 nodes each with 2 accelerators unless otherwise indicated. 


\paragraph{Dataset.} 
The dataset we use for training and evaluation is based on a cleaned version of the WikiHow Dataset~\cite{koupaee2018wikihow}.
We conduct an 80-20 split and use $15{,}000$ examples for training and $3{,}000$ for validation, again facilitating a large number of experiments. 

\paragraph{Model.} The model we consider is a T5 encoder-decoder language model~\cite{raffel2020exploring}, T5-Small, which is sufficiently large to study the communication overhead effects we are interested in but also allows us to carry out a large number of experiments.



\paragraph{Hardware.}
The experiments are conducted over two nodes, each with 2 accelerators . Our experiments are carried out on a distributed setup consisting of 2 nodes  with a dedicated 10 GigaBit/s Ethernet interconnect. Each node comprises two Nvidia RTX 6000 accelerators (48 GB memory each). Within a node, the accelerators are interconnected via NV-Link.

\paragraph{Baselines.}
We consider three main baselines:
First, \texttt{Deto-Full}, which denotes synchronizing the full gradients with an SGD optimizer. Second, \texttt{AdamW}, by which we refer to the default Pytorch FSDP implementation with an AdamW optimizer. 
Our third baseline is the decoupled momentum optimizer DeMo~\cite{peng2024demodecoupledmomentumoptimization}.
We test our proposed flexible decoupled momentum optimizer (\texttt{FlexDeMo}) against these baselines.

\paragraph{Hyperparameters.}  We employ a learning rate of 0.001, a batch size of 8, a chunk size of 128, an input sequence length of 512 tokens, and an output sequence length of 150 tokens. 
We train all models in all configurations of the optimizers for 10 epochs. The TopK hyperparameter for FlexDeMo (and DeMo) is varied and subject to analysis.

\paragraph{Conditions.}
We analyze three different bandwidth limits: $100$Mbps, $500$Mbps, and $1$Gbps. These bandwidth limits are selected to simulate network congestion in high-performance computing clusters and because $100$Mbps and $1$Gbps also correspond to common limits when one would perform distributed training over standard internet connections. The bandwidth limits are imposed on the network interface of each node, and confirmed by measuring the transferred bandwidth through the network switch connecting the nodes in the experiment.

\paragraph{Measures.}
Our main evaluation criterion is runtime, which is measured by saving the device timestamps for each training step and each epoch with Aim~\cite{Arakelyan_Aim_2020}.
In addition, we measure validation loss on the held-out data after each epoch.