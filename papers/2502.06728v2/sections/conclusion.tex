\section{Conclusion}
In conclusion, we have shown that FlexDeMo successfully extends DeMo optimization to the setting of hybrid sharded FSDP. Our small-scale experiments indicate results comparable with AdamW, without any delay of convergence, and the potential to reduce the bandwidth needed for communication between nodes. This enables FlexDeMo to be used to optimize LLMs that do not fit within the memory of one accelerator and further support training LLMs in low-resource network settings, ultimately lowering the entry point for both practitioners and researchers to participate in the development of future models and use-cases.

