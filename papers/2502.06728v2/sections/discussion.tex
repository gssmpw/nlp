\section{Discussion}
Through a thorough set of experiments with full control over the network bandwidth between the nodes, we have demonstrated that FlexDeMo is not only faster, but also delivers results comparable to those of AdamW. We conducted experiments on 100Mbps, 500Mpbs and 1Gbps to probe the experiments at typical network bandwidths in practice when training LLMs on HPC systems or geographically distributed nodes. 

We show that FlexDeMo yields losses comparable to those of full synchronisation, as well as FSDP. Furthermore, we observe that FlexDeMo achieves better loss in shorter time, and especially at lower bandwidths, which are comparable to what is achievable on HPCs and geographically distributed nodes. We also show that hybrid sharding is a feasible strategy, as the loss obtained is similar to the baseline, while it would allow for training larger models that do not fit within the memory limits of a single accelerator.

While AdamW obtains slightly better performance than the FlexDeMo runner-up configurations, it is slower, and significantly so at the low 100Mbps bandwidth, by being $369.364$ minutes delayed compared to Top1. Furthermore, we show that the validation losses across 1Gbps and 100Mbps are comparable and that the training-behavior is stable.
Our results further hints that employing a TopK larger than one leads to worse results than Top1. At first, this might occur as a very interesting fact, but we are training on a rather small dataset, which might turn these results into an artifact of overfitting.

For an optimization perspective our experiments are conducted in a fair setup, where all samples are seen the same number of times. However, for an efficiency perspective FlexDeMo is able to run considerably more optimization steps given the same wall-time constraint, which is an important insight as most HPC systems work on node or GPU hours. It is not clear from all of our figures that the training has converged, hinting that running DeMo for longer might yield even better performance.

% optimization time step
By controlling the bandwidth, we conducted experiments on the optimization step time across $100$, $500$ and $1000$Mpbs connections in Section \ref{results:optimization_step_time}. These bandwidths provide a broad insight into real-world connection-speeds from HPCs to local clusters. Our results show that FlexDeMo is faster with TopKs $1$, $8$, and $16$, while being generally slower than AdamW and full synchronization when communicating larger amounts of data with Top$32$. This is expected as computing the DCT induces a computational overhead of fast moving components, explaining the improvement in full gradient synchronization. AdamW furthermore has advantages in complex communication strategies for \texttt{all\_gather()} and \texttt{all\_reduce()}, which DeMo can not employ as we need to decompress the communicated data. 

% hybrid sharding
We further demonstrate the effectiveness of employing the hybrid-sharding strategy for training LLMs that fit into memory of accelerators across one node. Clearly, it is beneficial to use \texttt{all\_gather}, collecting gradients intra-node, and only sharing the fast moving DCT components in inter-node communication. Thus, with the hybrid-sharding strategy, the \texttt{all\_gather} is done only once per node, instead of once per accelerator with the original decoupled momentum implementation, as was also shown in Figure \ref{fig:DeMo_communication_pattern_2_nodes}. At larger scales, this proves a significant factor for the inter-node communication, underlinining the effectiveness of employing the hybrid-sharding strategy, opening an avenue of future research of decoupled training-techniques.

Our experiments were conducted on smaller scale models in a network without congestion through connection-topologies. We thus expect even more significant gains on larger models in HPC environments. Initial experiments with up to $64$ nodes and $256$ accelerators on the EuroHPC Leonardo BOOSTER so far conform with these expectations, yielding significant real-world advantages for large-scale training even with nominally 100Gbps interconnects.


\subsection{Future Work}
In this work we introduce FlexDeMo, which employs a hybrid training strategy of intra-node sharding and inter-node DeMo replication, demonstrating competitive results with original AdamW, while being considerably faster in realistic real-world network speeds. This opens an avenue for future research. First, it is not obvious that we need to synchronize the separately training processes during each optimization step. Second, the processes do not need to wait until the gather-communication finalizes, if it was possible to implement the sharing of components asynchronously. Last, future work investigating which information could and should be exchanged between the individual training processes would be valuable for future optimization research, in general, and for variants of decoupled distributed training strategies, particularly.
