\section{Introduction}
Training large deep neural networks (DNNs) induces large amounts of network traffic in the form of gradients that are transmitted between accelerators, typically requiring expensive localized high-throughput network setups on high-performance computing clusters. The network throughput increasingly becomes a bottleneck, as the number of accelerator nodes participating in the training increases and the general network congestion increases. Recent work demonstrates that synchronizing the full optimizer state is not always necessary for state-of-the-art results through decoupling momentum updates and allowing controlled divergence within each rank by carefully controlled inter-accelerator communication termed Decoupled Momentum optimization (DeMo)~\cite{peng2024demodecoupledmomentumoptimization}.

DeMo presents a viable strategy for distributed training with reduced gradient communication and, thus, enables distributed data parallel (DDP) training of DNNs with relatively low network bandwidth requirements. However, this strategy comes with several caveats. First, relying on DDP implies the constraint that the DNN model and optimizer states must fit within the memory capacity of each accelerator. This severely limits the applicability for training large models such as state-of-the-art large language models, which commonly do not fit within the memory of a single accelerator. Second, DeMo inherently relies on a distributed gathering operation whose bandwidth requirements scale linearly with the number of accelerators.

In this work, we introduce the Flexible Decoupled Momentum optimizer (FlexDeMo) for combining fully sharded data parallel (FSDP) training with decoupled momentum updates. This optimizer employs a hybrid-sharding strategy, where the model and optimizer states are typically sharded intra-node and replicated between nodes.
Instead of synchronizing the full gradients between the nodes as in extant hybrid sharding strategies, we compress, gather, and decompress selected relevant fast-moving momentum components following the approach that the DeMo optimizer has introduced for DDP training \cite{peng2024demodecoupledmomentumoptimization}.
This relaxes constraints regarding the accelerator memory requirements while simultaneously reducing inter-node bandwidth requirements.
This addresses the two caveats of DeMo mentioned above. First, FlexDeMo allows for decoupled momentum training of models that do not fit into the memory of a single accelerator but fit into the combined memory of the accelerators of one node. Second, FlexDeMo replicates between nodes rather than accelerators, effectively reducing the bandwidth requirements of the distributed gathering operation, which now scales linearly with the number of nodes rather than the number of accelerators. 

Our results show that FlexDeMo is faster than, and  comparable in terms of validation loss, to both DeMo and extant hybrid sharding strategies that combine intra-node FSDP with full inter-node gradient synchronization. In essence, FlexDeMo enables training of larger DNNs more efficiently across multiple nodes of a cluster or even across geographically-dispersed clusters.
Our implementation is available on GitHub\footnote{\url{https://github.com/schneiderkamplab/DeToNATION}} and the Python Package Index\footnote{\url{https://pypi.org/project/detonation/}}.

In summary, our contributions are as follows:
\begin{itemize}
    \item The first implementation of a hybrid sharded training strategy combining intra-node FSDP with decoupled momentum optimization (FlexDeMo). 
    \item Increased efficiency in bandwidth-limited settings, both compared to FSDP with full gradient synchronization and previous work (DeMo).
    \item Comparable model performance compared to extant hybrid sharded training strategies using the AdamW optimizer. 
    \item An analysis of the critical hyperparameter TopK providing guidance on its effects on efficiency and model performance.
\end{itemize}