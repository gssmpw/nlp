\section{Related Work}
Strategies to both scale and accelerate the training of deep neural networks have been active research areas \cite{shoeybi2019megatron,rajbhandari2020zero} for quite some years. The importance of these methods effectiveness is increasing as we train more and more large language models both in and across growing HPC capacities. We will first clarify the terminology and then provide a brief overview of the most important advances in distributed training.

\emph{Distributed data parallel (DDP)} replicates the model and optimizer states across multiple individual processes, which each handle a subset of the training data. The gradients are averaged from all processes to keep the model weights synchronized.  
In \emph{model parallelism}, the model is split across accelerators, consequently increasing the communication overhead as each device computes only the forward and backward passes for its assigned model-parts, requiring the communication of the intermediate activation between devices.
\emph{Tensor parallelism} works similarly as model parallelism with the difference that even individual tensors are split across devices. Another common strategy is to optimize locally for multiple steps before synchronization~\cite{DBLP:conf/iclr/Stich19}.

The zero redundancy optimizer (ZeRO)~\cite{rajbhandari2020zero}, tackling data and model parallelism, has introduced a variety of strategies to train large models, including partitioning optimizer states, gradients, and parameters -- all of which are being pulled dynamically to a single accelerator on demand. ZeRO has been integrated in the DeepSpeed library~\cite{rasley2020deepspeed}. The DeepSpeed team has since iterated on ZeRO with releases of Zero-2~\cite{zero2} and Zero-3~\cite{zero3}.

While ZeRO aims to achieve memory-efficiency to enable training of very large models, it also comes with a severe communication overhead. Both DeMo~\cite{peng2024demodecoupledmomentumoptimization} and our proposed FlexDeMo aim to alleviate this communication overhead.

FSDP~\cite{zhao2023fsdp} takes inspiration from ZeRO, but makes a few modifications to the exchange of gradients: ZeRO uses a reduce-scatter operation to distribute the gradients and an all-gather operation for the updated parameters. FSDP instead uses an all-gather operation to recover unsharded parameters and a reduce-scatter operation for the gradients.
FSDP also introduces a \emph{hybrid sharding} strategy, enabling the user to define the set of sharding accelerators and corresponding replicates to allow for a trade-off between memory and throughput by controlling the sharding groups and allowing for gradient accumulation with or without communication.


Notably, there are several other advances in accelerating large scale neural network training, such as compressing the gradient by only considering its sign~\cite{bernstein2018signsgd,DBLP:conf/nips/JiangYYZ24} or through low-rank projection of the gradients~\cite{DBLP:conf/icml/Zhao0CWAT24}. Although tackling similar challenges, we consider these directions orthogonal to our work, as we are interested in improving the communication overhead regarding optimizer states.