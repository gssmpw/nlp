\section{Related Work}
\paragraph{\bf{Image Stylization}} aims to transfer artistic styles to images while preserving structural content.
Early CNN-based methods~\cite{nst2016gatys, huang2017adain, dumoulin2017adversarially} laid the groundwork by capturing style and content representations.
With the advent of diffusion models~\cite{ho2020ddpm, rombach2022latentdiffusion}, recent approaches leverage pretrained architectures and textual guidance for high-quality stylization~\cite{chung2024styleid, subrtova2023diffusionimageanalogies, li2023stylediffusion, everaert2023diffusioninstyle, yang2023zero, li2024diffstyler, zhang2023inst}.
InST~\cite{zhang2023inst} employs textual inversion to encode styles in dedicated text embeddings, achieving flexible transfer.
StyleDiffusion~\cite{li2023stylediffusion} further refines style-content separation through a CLIP-based disentanglement loss applied during fine-tuning.
StyleID~\cite{chung2024styleid} adapts self-attention in pretrained diffusion models to incorporate artistic styles without additional training.
While these methods produce compelling results, they focus on overall style transfer without explicitly modeling semantic correspondences.
In contrast, we attempt to inject explicit semantic matching in stylization, thereby enabling precise style transfer according to semantically matching regions.



\paragraph{\bf{Semantic Correspondence.}}
Foundational works and recent innovations have shaped the evolution of semantic correspondence.
SIFT-Flow~\cite{liu2011siftflow} pioneered dense image alignment with handcrafted SIFT descriptors~\cite{lowe2004sift}.
Self-supervised vision transformers like DINO~\cite{caron2021dino} and DINO-V2~\cite{oquab2023dinov2, darcet2023vitneedreg} improved feature representation for semantic matching without labeled data~\cite{tumanyan2023disentangling,tumanyan2022splicing}.
Recent methods, such as ~\cite{zhang2023tale,hedlin2023unsupervised}, DIFT~\cite{tang2023dift}, cross-image-attention~\cite{alaluf2024cross} and~\cite{go2024eyeforaneye}, integrate diffusion models with these transformers, achieving superior zero-shot correspondence.
Techniques like Deep Functional Maps~\cite{cheng2024zeroshot} further refine correspondences by enforcing global structural consistency, demonstrating the potential of advanced representations in addressing correspondence challenges.
The development of these techniques enables the extraction of semantic correspondences using intermediate representations. 

\paragraph{\bf{Attention-based Control in Diffusion Models.}}
The attention modules in pretrained diffusion models are essential in controlling the generated content, allowing various image editing tasks through attention mask manipulation.
Prompt-to-Prompt~\cite{hertz2022p2p} pioneered text-based local editing by manipulating cross-attention between text prompts and image regions.
Similarly, Plug-and-play~\cite{tumanyan2023plugandplay} leverages the original image's spatial features and self-attention maps to preserve spatial layout while generating text-guided edited images.
Epstein et al.~\cite{epstein2023diffusionselfguidance} introduced Diffusion Self-Guidance, a zero-shot approach that leverages internal representations for fine-grained control over object attributes.
While these methods focus on text-to-image attention control, recent works like Generative Rendering~\cite{cai2023genren} explore cross-image attention by injecting 4D correspondences from meshes into attention for stylized video generation.
In contrast, we propose a direct image-to-image semantic attention mechanism that transfers appearances across all semantic categories simultaneously through explicit correspondence masks, enabling efficient and accurate scene-level stylization without text prompts or 3D priors.

\paragraph{\bf{Diffusion-based Novel-View Synthesis}}
of 3D scenes typically requires inferring and synthesizing new regions that are either unobserved or occluded in the original viewpoint.
A paradigm in prior work~\cite{wiles2020synsin, rockwell2021pixelsynth, liu2021infinitenature, koh2022simple} is the warp-and-refine approach: estimate a depth map from the input image, warp the image to the desired viewpoint, and then fill in occluded or missing areas through a learned refinement stage.
More recent research~\cite{rombach2021geometryfree, yu2023photoconsistentnvs, jin2024lvsm} avoids explicit depth-based warping by directly training generative models that handle view synthesis in a single feed-forward pass.
Another line of work~\cite{hao2023text2immersion, tang2023mvdiffusion, chung2023luciddreamer, shriram2024realmdreamer, Muller_2024_CVPR, cai2022diffdreamer, tseng2023poseguideddiffusion, yu2024viewcrafter, sun2024dimensionx, deng2024streetscapes, seo2024genwarp} integrates diffusion models such as StableDiffusion~\cite{rombach2022latentdiffusion}, making it possible to extrapolate plausible new views that are far from the input image for in-the-wild contents.
ReconX~\cite{liu2024reconx} and ViewCrafter~\cite{yu2024viewcrafter} both harness powerful video diffusion models combined with coarse 3D structure guidance to mitigate sparse-view ambiguities, achieving improved 3D consistency for novel-view synthesis.
Motivated by recent success in the warp-and-refine paradigm~\cite{seo2024genwarp}, we adopt a similar strategy but with a focus on style lifting, incorporating historical frames through adaptive blending to consistently propagate our style transfers across multiple views.


