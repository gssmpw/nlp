\section{ReStyle3D}
We present \ours, a framework for fine-grained \textit{appearance transfer} from a style image $\img_{style}\in \mathbb{R}^{H\times W \times3}$, to a 3D scene captured by \textit{unposed} multi-view images or video $\mathcal{X}_{src}:=\{\img^i_{src} \in \mathbb{R}^{H\times W \times3}\}_{i=1}^N$. Specifically, \ours aims to transfer the appearance of each region in $\img_{style}$ to its semantically corresponding region in $\mathcal{X}_{src}$, while maintaining multi-view consistency across all images. We assume spatial overlap between two consecutive frames in $\mathcal{X}_{src}$.



\subsection{Preliminaries}
\paragraph{Diffusion models} progressively add noise to an image $\img_0$ sampled from a data distribution $p_{\mathrm{data}}(\img)$, transforming it into Gaussian noise $\img_T$ over $T$ steps, following a variance schedule $\{\alpha_t\}_{t=1}^T$:
\begin{equation}
p(~\img_t~|~\img_0~) = \mathcal{N}(~\img_t;~\sqrt{\alpha_t}~\img_0, 1-\alpha_t\mathbf{I}~),
\end{equation}
where $\img_t$ represents the noisy image at timestep $t$. 
The \textit{reverse} process is performed by a denoising model $\epsilon_\theta(\cdot)$ that gradually removes noise from $\img_t$ to obtain cleaner $\img_{t-1}$. Here $\theta$ is the learnable parameters of the denoising model. During training, the denoising model is trained to remove noise following the objective function~\cite{ho2020ddpm}:
\begin{equation}
\mathcal{L} = \mathbb{E}_{\img_0,t \sim \mathcal{U}(T),\epsilon \sim \mathcal{N}(0, I)}||\hat{\epsilon}_{\theta} - \epsilon||_2^2,
\label{eq:2}
\end{equation}
where $\hat{\epsilon}_{\theta}=\hat{\epsilon}_\theta(\img_t, t, c)$, and $c$ is an optional input condition such as text, image mask, or depth information.
At inference stage, a clean image $\img:=\img_0$ is reconstructed
from a randomly sampled Gaussian noise $\img_T\sim \mathcal{N}(0, I)$ through an iterative noise-removal process. The cornerstone of modern image-based diffusion models is the latent diffusion model~\cite{rombach2022latentdiffusion} (LDM), where the diffusion process is brought to the latent space~\cite{vqgan_2021_CVPR} of a variational autoencoder (VAE). This approach is significantly more efficient compared to working directly in the pixel space. 

\input{figures/tex/method_2dtransfer}
\paragraph{\bf{Attention layers}} are fundamental building blocks in LDM. Given an intermediate feature map $F \in \mathbb{R}^{L \times d_h}$, where $L$ denotes the feature length and $d_h$ represents the feature dimension, the attention layer captures the interactions between all pairs of features through query-key-value operations:
\begin{equation}
\begin{split}
\phi &= \text{softmax}\left(\frac{Q'\cdot K'^T}{\sqrt{d_h}}\right)\cdot V' \\
Q' &= Q \cdot W_q, \quad K' = K \cdot W_k, \quad V' = V \cdot W_v,
\end{split}
\end{equation}
where $\phi$ is the updated feature map, $Q', K'$, and $V'$ are linearly projected representations of the inputs via $W_q$, $W_k$, and $W_v$, respectively. In self-attention, the key, query, and value originate from the same feature map, enabling context exchange within the same domain. For cross attention, the key and value come from a different source, facilitating information exchange across domains. In \ours, we tailor the self-attention layers specifically for semantic appearance transfer and keep the cross-attention unchanged.


\subsection{Appearance Transfer via Semantic Matching}
\label{sec:attention_appearance_transfer}
To transfer the appearance of $\img_{style}$ to $\img_{src}$, prior attempts also employing diffusion models~\cite{alaluf2024cross,zhang2023tale,cheng2024zeroshot} have primarily focused on single objects, and struggle with scene-level transfer involving multiple instances. Our key observation is that the implicit semantic correspondences in foundation models~\cite{rombach2022latentdiffusion,oquab2023dinov2} are insufficient for more complex multi-instance semantic matching. To address this limitation, \ours explicitly establishes and leverages semantic correspondences throughout the transfer process.



\paragraph{Open-vocabulary Semantic Matching.}
We leverage the open-vocabulary panoptic segmentation model ODISE~\cite{xu2023open} for semantic matching. For a given input image, ODISE generates segmentation maps $\mathcal{M} \in \{1, \ldots, C\}^{H \times W}$, assigning each pixel to one of $C$ semantic categories. These maps enable semantic correspondences between the style and source images (detailed below). By matching open-vocabulary semantic predictions, \ours is not limited by predefined semantic categories in a scene. The correspondences are injected into the diffusion process to guide appearance transfer between matched regions.



\paragraph{Injecting Semantic Correspondences in Self-attention.}
\ours enables training-free style transfer by extending the self-attention layer of a pretrained diffusion model (Fig.~\ref{fig:method_2d}). This approach injects style information from $\img_{style}$ into $\img_{src}$ while preserving its structure. Specifically, we first encode both the style and source images into the latent space of Stable Diffusion~\cite{rombach2022latentdiffusion}, producing $\latent^{style}_0$ and $\latent^{src}_0$. These latent representations are then inverted to Gaussian noise, $\latent^{style}_T$ and $\latent^{src}_T$, using edit-friendly DDPM inversion~\cite{huberman2024edit}.
To enhance structural preservation and mitigate LDM’s over-saturation artifacts, we incorporate monocular depth estimates~\cite{yang2024depth} of the input images through a depth-conditioned ControlNet~\cite{zhang2023adding} during the inversion process. The stylized image latent is then initialized as $\latent^{out}_T = \latent^{src}_T$.

Next, we transfer the style from $\latent^{style}_T$ to $\latent^{out}_T$ by de-noising them along parallel paths~\cite{alaluf2024cross}. At each de-noising step $t$, we extract style features $(K_{style}, V_{style})$ and query features $Q_{out}$ from individual self-attention layers. The semantic-guided attention for the output feature $\phi_{out}$ is computed by combining the attention features with the attention mask $M$ as follows:
\begin{equation} 
\phi_{out} = \text{softmax}\left(\frac{Q_{out} \cdot K_{style}^T}{\sqrt{d_h}} \odot M \right) \cdot V_{style}, 
\end{equation}
where $\odot$ denotes element-wise multiplication and $\phi_{out} \in \mathbb{R}^{d^2 \times d_h} $ is passed to the next layer after self-attention.


To obtain the attention mask $M \in \mathbb{R}^{d^2 \times d^2}$, we flatten and bilinearly downsample the semantic masks $\mathcal{M}_{style}$ and $\mathcal{M}_{src}$ to match the resolution of attention feature maps, which is $d\times d$. The attention mask is defined as $M(i,j)=1$ if the $i$-th region in the source and the $j$-th region in the style image share the same semantic class; otherwise, $M(i,j)=0$. 
This formulation ensures that each region in the output image samples its appearance solely from semantically corresponding regions in the style image.
For example, a chair in the source image is only cross-attended to its counterpart in the style image, inheriting its appearance.
If multiple instances in the style image share the same semantic class, attention is distributed across them based on sampling weights determined by softmax attention scores.
This mechanism naturally extends to support user-specified correspondences.
Regions without semantic matches attend to the entire style image to preserve global harmony.
Although semantic attention effectively transfers appearance, it may compromise the realism and structure of the stylized output, requiring further refinement.


\paragraph{Guidance and Refinement.} 
We draw inspiration from~\cite{ho2021cfg,alaluf2024cross} and incorporate classifier-free guidance~(CFG) combined with semantic and depth-conditioned generation. At each denoising step $t$, we compute three noise predictions: $\epsilon_t$, $\epsilon_t^{d}$ and $\epsilon_t^{s}$. Here, $\epsilon_t^{s}$ represents the predicted noise from the semantic attention path, $\epsilon_t^{d}$ is obtained from the depth-conditioned ControlNet~\cite{zhang2023adding}, and $\epsilon_t$ is the unconditional noise prediction. The final noise prediction is then calculated as follows: 
\begin{equation}
\hat{\epsilon}_t = (1 - \alpha) \epsilon_t + \alpha (\lambda_s\epsilon_t^s + \lambda_d\epsilon_t^d),
\end{equation}
where $\lambda_s$ and $\lambda_d$ are the respective guidance weights ($\lambda_s+\lambda_d=1$) for semantic and depth guidance. $(1 - \alpha)$ is the classifier-free guidance scale, which balances conditional and unconditional predictions, improving image realism.

To enhance image quality, we employ a two-stage refinement process. First, we upscale the initial stylized image from 512×512 to 1024×1024 resolution. Then, following SDEdit~\cite{meng2022sdedit}, we add high-frequency noise to this upscaled image and denoise it for 100 steps with SDXL~\cite{podell2024sdxl}. This refinement process enhances local details while maintaining the overall style, producing our final output $\hat{\img}_{src}$.

\input{figures/tex/independent_stylization}

\input{figures/tex/method_lifting}

\subsection{Multi-view Consistent Appearance Transfer}\label{sec:multiview_style_propagation}
Although our semantic attention module effectively transfers appearance for a single view, independently applying it to each view may cause inconsistent artifacts (see Fig. \ref{fig:inconsistency}). Therefore, we develop an approach to transfer the appearance from the stylized image $\hat{\img}_{src}^i$ to all remaining views while maintaining multi-view consistency.

\paragraph{Flow-guided Style Warping.}
Given a pair of source images $(\img_{src}^i, \img_{src}^j)$, we first leverage a stereo matching method DUSt3R~\cite{wang2024dust3r} to extract the dense point correspondence and the camera intrinsics. Using these, the optical flow $\mathbf{W}_{i\rightarrow j}\in \mathbb{R}^{H \times W \times 2}$ is calculated by projecting the pointmaps of $i$-th image to the $j$-th image. Next, given the optical flow and the stylized $i$-th image $\hat{\img}_{src}^i$, we employ softmax splatting~\cite{softslatting_CVPR_2020} to obtain the initial stylized image $\hat{\img}_{w}^j$ and its warping mask $\mathbf{M}_{w}^j$, which indicates missing pixels in the $j$-th frame after forward warping. 

\input{tables/comparison_2d}
\paragraph{Learning View-to-View Style Transfer.} 
Given the source image $\img_{src}^j$ and its initial stylized version $\hat{\img}_{w}^j$, we train a 2-view warp-and-refine model $\hat{\epsilon}_{\theta}=\hat{\epsilon}_\theta(\latent_t, t, c)$ to generate a complete and consistent stylized image following conditions $c$: the initial stylized image, the inpainting mask, and the monocular depth map $\mathbf{D}^j$ of the source image $\img_{src}^j$ (Fig.~\ref{fig:method_lifting}). The final condition $c=\mathrm{concat}(\latent^{(\hat{\img}_{w}^j)}, \latent^{(\mathbf{M}_{w}^j)}, \latent^{(\mathbf{D}^j)})$, $\latent^*$ denotes individual latent representations. To harness the power of a pretrained diffusion model~\cite{podell2024sdxl}, like Marigold~\cite{ke2023marigold}, we modify the input channels of its initial convolution layer to accommodate additional conditions and zero-initializing the additional weights. Following Eq.~\eqref{eq:2}, we train the model using quadruplets of the warped and incomplete image, depth map, mask, and the clean and complete image. The model simultaneously learns to complete missing pixels and refine all pixels to address warping artifacts.

\paragraph{Auto-regressive Multi-view Stylization.}
We propose an auto-regressive approach to extend two-view stylization to handle multiple views or even videos, ensuring global coherence across the scene (Fig.~\ref{fig:method_lifting}). Stylizing the $j$-th frame using only the previous frame $(j-1)$ can lead to inconsistencies with earlier frames while warping all historical frames could produce blurry outputs. Instead, we warp the stylized frame $(j-1)$ along with two randomly selected historical frames. In overlapping regions, where multiple pixels are warped to the same location, we adopt an exponential weighted averaging to blend pixels, prioritizing pixels from frame $(j-1)$. This adaptive weighting maintains temporal consistency and preserves sharp details in the resulting warped image $\hat{\img}_{w}^{1:j-1}$. Finally, our model refines the output, producing a fully stylized frame.
