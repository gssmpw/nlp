\section{Introduction}
{\let\thefootnote\relax\footnote{{$^*$These authors contributed equally to this work.}}}
Generative diffusion models have recently spurred significant advances in image stylization and broader generative applications, enabling the seamless synthesis or editing of images with remarkable visual fidelity.
While existing image stylization approaches~\cite{chung2024styleid,li2024diffstyler} often excel at transferring well-known artistic styles~(\eg, Van Gogh paintings) onto photographs, they fall short when it comes to practical and realistic style applications, such as virtual staging or professional interior decoration, where transferring the style of one image (\textit{style} image) to another (\textit{source} image) entails transferring the individual appearance of objects (Fig. \ref{fig:teaser}).

These methods tend to treat the style image globally, ignoring the semantic correspondence between individual objects or regions in the images.
This coarsely aligned stylization not only misrepresents object appearances but also fails to adapt fine-grained textures to semantically matched regions~(\eg, transferring \emph{couch} textures only to \emph{couches}). 
This is crucial for real-world use cases where style is defined by the unique characteristics (\eg, color, material, shape) of design elements (\ie, furniture, decor, lighting, and accessories) that give it its signature look \cite{park2022analysis}. 
Another line of work pursues \textit{semantic correspondence} for transferring object appearances~\cite{cheng2024zeroshot,zhang2023tale}.
While these methods show promise in aligning single objects or small regions via deep feature matching, they typically operate at low spatial resolutions~(often $64\times64$) and therefore struggle to handle complex scenes with strong perspective and multiple object instances.
Extending them to scene-level stylization remains a challenging problem due to both semantic and geometric complexity.

Moreover, when a scene is represented by multiple images (\eg, for larger coverage), ensuring multi-view \emph{consistency} in scene-level appearance transfer further complicates the task.
Existing multi-view image editing methods~\cite{or24mvedit,styleGaussians,fujiwara2024sn2n,vicanerf2023} commonly require known camera poses and an existing 3D scene representation (\eg, a neural radiance field~\cite{mildenhall2021nerf} or 3D Gaussian splatting~\cite{kerbl20233d}), which needs a dense set of input views and considerable compute time. 
These methods struggle with sparse or casually captured views, and their specialized 3D pipelines hinder plug-and-play use.
A pixel-space approach preserving geometric cues without heavy 3D modeling is preferable but remains under explored. 
We propose \textbf{\ours}, a novel framework for scene-level appearance transfer that cobines semantic correspondence and multi-view consistency, addressing limitations of 2D stylization and 3D-based editing methods.
\emph{Our key insight} is that the inherent but implicit semantic correspondences from pretrained diffusion models or vision transformers~(\eg, StableDiffusion~\cite{rombach2022latentdiffusion} and DINO~\cite{caron2021dino, oquab2023dinov2}) are insufficient for fine-grained, scene-level appearance transfer, especially when different objects or viewpoints are involved.
We tackle this by explicitly matching open-vocabulary panoptic segmentation predictions between the style and source images, while ensuring that unmatched parts of the scene still receive a global style harmonization.
This open-vocabulary labeling~(with no predefined semantic categories) helps us robustly align semantically corresponding regions even in cluttered indoor scenes.
By integrating these explicit correspondences into the attention mechanism of a diffusion process, we achieve more accurate and flexible stylization of multi-object scenes.

To further ensure \emph{3D awareness} and view-to-view consistency, we adopt a two-stage pipeline.
First, we achieve \emph{training-free} semantic appearance transfer in a single view by injecting our correspondence-informed attention into a pretrained diffusion model.
Second, a warp-and-refine diffusion network that efficiently propagates the stylized appearance to additional views in an auto-regressive manner, guided by monocular depth and pixel-level optical flows.
Our method does not require explicit pose or 3D modeling, and we show that the final stylized frames are fully compatible with off-the-shelf 3D reconstruction tools, enabling complete 3D visualizations and consistent multi-view stylization with minimal overhead.
In summary, our contributions are as follows:
\begin{itemize}
    \item We introduce \emph{SceneTransfer}, a new task of transferring multi-object appearance from a single style image to a 3D scene captured in multi-view images.
    \item We propose \ours, a two-stage pipeline that (\emph{i}) adapts a pretrained diffusion model with \emph{semantic attention} for instance-level stylization, and (\emph{ii}) trains a warp-and-refine novel-view synthesis module to propagate the style across all views, maintaining global consistency.
    \item We create the \dataset benchmark with 25 interior design images and 31 indoor scenes~(243 style-scene pairs) from different categories (\eg bedroom, living room, and kitchen). Our results show strong improvements in structure preservation, style fidelity, and cross-view coherence.
\end{itemize}
