\clearpage
\appendix


\section{Qualitative Ablation Study}
\label{sec:qual_ablation}
\input{figures/tex/qualitative_ablation}
We further validate our design choices with additional qualitative results in Fig.~\ref{fig:ar_ablation}, comparing frame selection strategies for our warp-and-refine model.
Warping only the last frame can cause inconsistencies, \eg, the painting on the wall appears and disappears due to occlusion and incorrect correspondence estimates and occlusions (Fig.~\ref{fig:ar_ablation} (a)).
Warping all past frames improves consistency but introduces conflicts, as overlapping pixel projections in the same location can result in smeared pixels and confuse the model about whether to refine or preserve them(Fig.~\ref{fig:ar_ablation} (b)).
Our strategy can mitigate the disadvantages of both approaches, yielding more coherent multi-view stylization (Fig.~\ref{fig:ar_ablation} (c)). 

We also validate the need for monocular depth condition to fully leverage two-view geometry in multi-view appearance transfer. As the stereo estimate from DUSt3R~\cite{wang2024dust3r} implicitly contains geometry information, the additional monocular depth condition could be deemed unnecessary. To this end, we train a model without monocular depth estimation (MDE) that is solely conditioned on warped images, to ablate its effect.
Fig.~\ref{fig:qualitative_ablation} shows that without MDE, the model still effectively learns the task, but loses fidelity in local details.
With pixel-aligned MDE control, the model corrects wrong projections from two-view geometry based on the high-resolution depth map improving accuracy in areas like the door handle, chairs, and wine bottles.

\section{Qualitative Video Results}
\label{sec:more_quals}
We present additional results that demonstrate our method's versatility in handling diverse styles and scene contents. Our supplemental video best illustrates this versatility, showing how a single style image can transform multiple indoor scenes into a cohesive appearance, as well as how one scene can be re-imagined across various styles. The video format particularly highlights our method's ability to maintain visual consistency across multiple viewpoints, a key advantage that is difficult to convey through static images.


\section{Data Curation}
\label{sec:train_data}
We curated the training data for our warp-and-refine model using 57 hourly-long 2K house tour videos. We first extract keyframes from all the videos at every 10$^{th}$ frame to get 57K sparse multi-view images. Then we divide the keyframes into 10-image chunks to emulate different overlap ratios in real-world scenarios. In each window, we use DUSt3R~\cite{wang2024dust3r} to compute the dense stereo between the first frame in the window and all the rest frames and get 9 warped images and their warping masks in each window. We also use Depth Anything V2~\cite{yang2024depth} to extract the monocular depth for the 9$^{th}$ image in the window. Then each training sample is generated as (warped image, monocular depth, mask, and clean target image) and used for training. 

We would like to note that both the warp-and-refine training data and the \dataset benchmark data are derived from a collection of house tour videos, which are part of a separate submission to another conference. These videos will be made publicly available through that paper, along with all relevant metadata (e.g., video IDs, start-end timestamps) to enable full replication and encourage future research and comparisons. The selection of this dataset was intentional. Unlike existing indoor scene datasets~\cite{dai2017scannet,yeshwanth2023scannet++}, which are typically designed for 3D reconstruction and feature imagery that remains close to surfaces and lacks spatial context, our house tour videos were collected for real-estate purposes. They offer stable, smooth trajectories with views that are further from surfaces, making them more suitable for stylization tasks.

The style images in the \dataset benchmark were manually collected from \href{https://www.pexels.com/search/interior%20design/}{\textit{Pexels}}, which provides copyright-free interior images.


\section{Style Images and Segmentation}
\label{sec:design_style}
\input{figures/tex/style_examples}
Fig~\ref{fig:style_examples} displays several style images from our \dataset benchmark alongside their open-vocabulary segmentation overlays.
Each row pairs a real indoor photograph~(kitchen, living room, or bedroom) with its corresponding color-coded semantic masks, as predicted by ODISE~\cite{xu2023open}.
The segmentation assigns each pixel to a semantic category~(e.g., ``wall'', ``sofa'', ``cabinet'', ``table'')\footnote{The open vocabulary method was queried with a list of common semantic labels. In our experiments we used the semantic list of the ODISE model.}, enabling instance-level alignment between the target scene and the reference style images.
In the \ours pipeline, these masks form the basis  for establishing precise semantic correspondences.
By ensuring that each object region in the target image only “borrows” style cues from its from its corresponding instance in the reference image, our framework preserves the scene layout while selectively transferring appearance.
This capability is particularly valuable in multi-object, complex scenes like the ones shown here, where various furniture and architectural elements must be stylized coherently while maintaining their original spatial relationships.


\section{Limitations and Future Work}
\label{sec:limitation}
In this work we focused exclusively on indoor scenes for interior design applications, without exploring other types of scenes, such as outdoor or dynamic environments.
While our method effectively transfers appearances, it lacks strong disentanglement between color, texture, and material properties, and struggles with significant lighting changes.
Future work could address these limitations by improving the transfer of appearances for smaller objects in the scene---currently overlooked due to downsampling of semantic masks--- and developing finer control mechanism for appearance transfer, such as material or texture.
