\vfill
\section{Conclusion}
We presented \ours, a framework for semantic appearance transfer from a design image to multi-view scenes. Our two-stage approach combines training-free semantic attention in diffusion models with a warp-and-refine network to ensure geometric consistency across views. Experiments and user studies on our \dataset benchmark confirm that \ours surpasses existing methods in semantic fidelity, structure preservation, and multi-view coherence. By using open-vocabulary segmentation and off-the-shelf reconstruction models, \ours avoids assumptions about scene semantics or geometry, making it suitable for real-world interior design and virtual staging. While focused on indoor scenes, its principles could apply to other domains. Limitations and more implementation details are discussed in the supplementary material.
