
\section{Experiments}

\input{figures/tex/2d_transfer}
\paragraph{\bf{Implementation Details.}}
We base our semantic attention module on Stable Diffusion 1.5~\cite{rombach2022latentdiffusion} and the refinement and 2-view warp-and-refine model on SDXL~\cite{podell2024sdxl}. To train our two-view warp-and-refine model~(Sec.~\ref{sec:multiview_style_propagation}), we use 4 NVIDIA A100 40GB GPUs with an effective batch size of 256 for 20K iterations, using the AdamW optimizer~\cite{loshchilov2018adamw} with learning rate $10^{-4}$. We randomly drop out half of the text prompt during training to make our model agnostic to text conditions. The model is trained on a dataset with 57K house tour images featuring 57 different houses/apartments.

\subsection{Evaluation Setting}
\paragraph{\bf{Dataset.}} Our \dataset benchmark comprises 31 distinct indoor scenes captured as short video clips, totaling 15,778 frames across multiple room categories, including living rooms, kitchens, and bedrooms, all disjoint from our training data. To evaluate stylization capabilities, we curated a set of 25 interior design reference images, enabling 243 unique style-scene combinations. Evaluation is performed on 1,109 keyframes sampled from these clips. For more details on data, please refer to the supplementary material (Supp.).


\paragraph{\bf{Evaluation Metrics.}}
We evaluate multiple different aspects of our pipeline.
First, we assess the appearance transfer performance using source images on two aspects: structure preservation and style transfer quality.
For structure preservation, we compare depth maps predicted by DepthAnythingV2~\cite{yang2024depth} between stylized and original images using standard metrics: Absolute Relative Error~(AbsRel), $\delta1$ accuracy, and Squared Relative Error~(SqRel), following established protocols~\cite{ke2023marigold, yang2024depth}.
For style transfer quality, we measure perceptual similarity between the stylized output and the style image using DINOv2~\cite{oquab2023dinov2}, CLIP, and DreamSim~\cite{fu2023dreamsim} scores. We evaluate this task on the stylized source images of each scene.
Next, we evaluate our two-view lifting model~(Sec.~\ref{sec:multiview_style_propagation}).
We assess its warp-and-refine quality using PSNR, SSIM~\cite{zhou2004ssim}, and LPIPS~\cite{zhang2018lpips} while also reporting FID~\cite{martin2017fid} to quantify the realism of generated frames under challenging viewpoint extrapolation. We evaluate using pairs of the source images per scene and their warped projections on the rest of the frames in each scene---we exclude pairs without correspondences. We do not use any stylization to train or evaluate since there is no ground truth.
To evaluate global consistency, we leverage DUSt3R~\cite{wang2024dust3r} to extract poses by aligning point maps from stylized sequences and compute cumulative error curve~(AUC) by comparing recovered camera poses against those from original images.





\subsection{Results}
\paragraph{\bf{Image Appearance Transfer.}}
We compare with three state-of-the-art methods on image-conditioned stylization and appearance transfer: Cross Image Attention~\cite{alaluf2024cross}, IP-Adapter~\cite{ye2023ip}, and StyleID~\cite{chung2024styleid}. For a fair comparison, we add depth ControlNet~\cite{zhang2023adding} to SDXL IP-Adapter~\cite{ye2023ip} and use the style image as the image prompt. As shown in Tab.~\ref{tab:2d_transfer}, our method achieves superior performance on both structure preservation and style transfer metrics. Notably, our explicit semantic attention mechanism in the diffusion UNet enhances the perceptual similarity between stylized outputs and style images, as evidenced by better DINO, CLIP, and DreamSim scores. The refinement step further improves structure preservation, reducing AbsRel from 11.30 to 8.34 and SqRel from 2.65 to 1.67.
Qualitative comparisons (Figs.~\ref{fig:2d_transfer} and~\ref{fig:2d_transfer_more}) reveal the limitations of existing approaches. Cross Image Attention effectively captures style textures but fails to maintain scene structure due to the lack of semantic guidance. IP-Adapter SDXL preserves overall structure but struggles with local detail transfer, as it compresses style information into a global feature vector. Although StyleID achieves the second-best performance, its results tend to preserve high-frequency details from the source image while applying style changes more globally, demonstrating limited capability in fine-grained appearance transfer.

We conduct a user study with 27 participants who were shown examples of a source and style image with outputs from four methods.
Participants selected the result that best preserved the structure while faithfully transferring the style.
Out of 252 evaluations (Tab.~\ref{tab:user_study}), \ours was the most preferred (42.4\%), demonstrating its effectiveness in balancing structure preservation and appearance transfer under human perception.

\input{tables/user_study}

\input{tables/pairwise_nvs}
\input{figures/tex/multiview_transfer}

\paragraph{\bf{Two-view NVS.}}
We compare our approach to: \textit{i)}~SDXL inpainting model~\cite{podell2024sdxl} with depth-conditioned ControlNet~\cite{zhang2023adding}, \textit{ii)} GenWarp~\cite{seo2024genwarp}, an image-based diffusion model for single view NVS, and \textit{iii)} ViewCrafter~\cite{yu2024viewcrafter}, a video-diffusion model for NVS. Note that the proposed task differs from traditional NVS as it leverages geometry information from the \textit{novel view} itself. We employ DUSt3R~\cite{wang2024dust3r} to extract the correspondences and provide the initial warped image as input to all methods. \ours outperforms  across all metrics, achieving a superior reconstruction ability as evidenced by the best PSNR, SSIM, and LPIPS metrics (\cf Tab.~\ref{tab:2viewnvs}, ). Additionally, it exhibits strong capability in extending style to unseen regions, evidenced by the lowest FID score (Fig.~\ref{fig:scannet_recon}). Notably, the second best method ViewCrafter~\cite{yu2024viewcrafter}, requires a predefined camera trajectory as input to video diffusion and runs 10$\times$ slower than ours. 


\input{tables/pose_eval}
\paragraph{\bf{Multi-view Consistency Evaluation.}}
We further evaluate the multi-view consistency of the stylized results through a proxy task. Specifically, we input the original and stylized images to DUSt3R~\cite{wang2024dust3r} and estimate the camera poses, separately. By evaluating the agreement with the poses from the original images, we analyze whether the geometry is preserved in the stylized images. The results are presented in Tab.~\ref{tab:corres_select}. Enabled by our adaptive auto-regressive approach, which effectively mitigates inconsistencies while preserving image sharpness, our method significantly outperforms the baselines on both rotation and translation metrics.
Figs.~\ref{fig:scannet_recon} and~\ref{fig:recon_extra} show multi-view transfer results, including the 3D reconstruction of stylized outputs with estimated camera poses, demonstrating both geometric and stylistic consistency despite camera motion and multiple objects.


\paragraph{\bf{Ablation Study.}} 
In Tab. \ref{tab:ablation}(a), we run \ours without our guidance strategy and observe significant degradation in structure preservation (AbsRelâ†‘ from 8.34 to 16.72). In (b), removing semantic attention hurts performance on perceptual similarity \wrt style image, showing that both components are crucial for semantic-accurate style transfer while maintaining structural integrity.

\input{tables/ablation}
