% \vspace{-1em}
\section{Background and Motivation}
\label{sec:background}
In this section, we first introduces LLM training in AI datacenters (DCs) (\S\ref{sec:background:llm_training}). Then, we examine existing High-Bandwidth Domain (HBD) architectures and discuss their limitations (\S\ref{sec:background:hbd}). Finally, we summarize key design principles of HBD for LLM training (\S\ref{sec:background:workload}).

% \vspace{-1em}
\subsection{LLM Training in AI DC}
\label{sec:background:llm_training}

\begin{figure*}[!t]
\centering
\begin{subfigure}[b]{0.25\textwidth}
    \centering
    \includegraphics[height=16ex]{figs/motivation/nvl36.drawio.pdf}
    \caption{Switch-centric: NVL36}
    \label{fig:hbd-archs:nvl36}
\end{subfigure}
\hspace{-1ex}\hfil\hspace{-1ex}
\begin{subfigure}[b]{0.24\textwidth}
    \centering
    \includegraphics[height=16ex]{figs/motivation/sip-ring.drawio.pdf}
    \caption{GPU-centric: SiP-Ring}
    \label{fig:hbd-archs:sip-ring}
\end{subfigure}
\hspace{-1ex}\hfil\hspace{-1ex}
\begin{subfigure}[b]{0.24\textwidth}
    \centering
    \includegraphics[height=16ex]{figs/motivation/dojo.drawio.pdf}
    \caption{GPU-centric: Dojo}
    \label{fig:hbd-archs:dojo}
\end{subfigure}
\hspace{-1ex}\hfil\hspace{-1ex}
\begin{subfigure}[b]{0.25\textwidth}
    \centering
    \includegraphics[height=16ex]{figs/motivation/tpuv4.drawio.pdf}
    \caption{Hybrid: TPUv4}
    \label{fig:hbd-archs:tpuv4}
\end{subfigure}
\vspace{-2ex}
\caption{Illustrative examples of HBD architectures. N represents Node, and S represents Switch. Red (with cross hatch) represents fault device and yellow (with dots) represents unavailable or downgraded GPU.}
\label{fig:hbd-archs}
\vspace{-1ex}
\end{figure*}

\begin{table*}[!htbp]\scriptsize
\centering
\begin{tabular}{llllllll}
\toprule
\multirow{2}{*}{\textbf{Architecture}}  & \multirow{2}{*}{\textbf{Type}}  & \multirow{2}{*}{\textbf{Scalability}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{Collective} \\ \textbf{Primitives}\end{tabular}} & \multicolumn{2}{l}{\textbf{Fault Explosion Radius}} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{Interconnect} \\ \textbf{Cost}\end{tabular}} & \multirow{2}{*}{\textbf{Fragmentation}} \\
                              &                                                &                              &                                                                                   & \textbf{Node-Side}           & \textbf{Switch-Side}          &                                                                               &                                \\
\midrule
NVL                           & Switch-centric                       & Low                          & Full CCL                                                                          & Node-level          & Switch-level         & High                                                                          & Many                           \\
\makecell{Dojo, TPUv3, SiP-Ring}         & GPU-centric                       & High                         & Ring-Allreduce                                                                    & HBD-level           & \ding{55}            & Low                                                                           & Few                            \\
TPUv4, TPUv5p                         & Switch-GPU Hybrid                   & Moderate                     & Ring-Allreduce                                                                    & Cube-level          & Switch-level         & Moderate                                                                      & Few                            \\
\sys{}                   & Transceiver-centric & High                         & Ring-Allreduce                                                                    & Node-level          & \ding{55}            & Low                                                                           & Few  \\
\bottomrule
\end{tabular}
\caption{Comparative analysis of HBD architectures.}
\label{tab:hbd-compare}
\vspace{-6ex}
\end{table*}

\para{LLM training parallelism and communication.} LLM training jobs employ various parallelism strategies to efficiently utilize GPUs distributed across AI DCs~\cite{megatron-lm, zero}. Based on communication loads, parallelism can be categorized into two types. The first type is \textit{communication-intensive  parallelism} which involves high communication load. Tensor Parallelism (TP) splits the model across multiple GPUs and synchronizes via AllReduce. The ring algorithm for AllReduce is theoretically optimal~\cite{patarasuk2009bandwidth}, making ring-based topologies ideal for TP. Expert Parallelism (EP), designed for Mixture of Experts (MoE) models~\cite{hunyuanlarge,deepseekv3,mixtralexperts}, assigns experts to different GPUs and relies on AlltoAll communication, requiring topologies with high bisection bandwidth (e.g., Full-Mesh). In contrast, parallelism strategies such as Data Parallelism (DP), Pipeline Parallelism (PP), Context Parallelism (CP), and Sequence Parallelism (SP) introduce lower communication overhead, placing less  demands on network performance.



\para{Compute fabric. } Compute fabric in AI DC interconnects GPUs to efficiently transmit model gradients and parameters. It consists of two primary components: Datacenter Network (DCN) and High-Bandwidth Domain (HBD). 
DCN provides communication across the entire AI DC via Ethernet or Infiniband, the bandwidth is around $200\sim 800Gbps$. Widely used DCN architectures include Fat-Tree~\cite{sigcomm2008fattree} and Rail-Optimized~\cite{rail-optimized}. In comparison, HBD offers Tbps-level throughput, and is more suitable for TP/EP. However, its scale is typically constrained by interconnection costs and fault tolerance considerations. For example, NVL-72~\cite{nvl72} only interconnects 72 GPUs per HBD.



\para{Faults and fault explosion radius. }As revealed by current advances of AI DCs~\cite{sigcomm2024hpn, sigcomm2024rdmameta}, training jobs experience a variety of faults, such as GPU faults, optical transceiver faults, switch faults, and link faults. We quantify the fault impact using the \textit{fault explosion radius}, defined as \textit{the number of GPUs degraded by a single fault event}.
The fault explosion radius varies depending on both the system architecture and the fault component.
For example, if a switch fails, the bandwidth of all devices connected to it will degrade, illustrating the switch-level fault explosion radius.


\para{HBD fragmentation.} When the number of GPUs in the HBD cannot be evenly divided by the size of the parallel group (i.e., TP size), the remaining GPUs become unusable, leading to resource waste.
The GPU waste ratio for each HBD can be expressed by the formula $\{(HBD_{size} - N_{fault}) \mod TP_{size}\}/{HBD_{size}}$.
In AI DCs with small-scale HBDs, GPU waste due to fragmentation is significant because each HBD experiences independent fragmentation.
This issue worsens as the TP group size increases with model scale. For example, for NVL-36 shown in \figref{fig:hbd-archs:nvl36}, running TP-16 causes $\geq$11\% GPU waste ratio.



\subsection{Limitations of Existing HBDs} 
\label{sec:background:hbd}


Existing HBD architectures for LLM training can be categorized into three types, based on the key components that provide connectivity. A summary is shown in Table~\ref{tab:hbd-compare}.

\para{Switch-centric HBD.}
This type architecture leverages switch chips to interconnect GPUs, as shown in \figref{fig:hbd-archs:nvl36}.
A prominent example is NVIDIA, which utilizes NVLink and NVLink Switch ~\cite{nvlink,nvswitch}, e.g. DGX H100~\cite{dgx} with 8-GPU and GB200 NVL-36, NVL-72, and NVL-576~\cite{nvl72}. 
These architectures offer high-performance any-to-any communication.
However, switch-centric HBDs have several drawbacks: i) They require a large number of switch chips due to their limited per-chip throughput; ii) They are vulnerable to a switch-level fault explosion radius—when a switch chip fails, all connected nodes experience bandwidth degradation; iii) High interconnect costs constrain the scale of HBDs, leading to significant fragmentation when serving large models.

\para{GPU-centric HBD.}
GPU-centric HBD architectures construct the HBD using direct GPU-to-GPU connections, eliminating the need for switch chips. As a result, cost scales linearly with HBD size.
A representative example is SiP-Ring~\cite{sip-ml}, shown in \figref{fig:hbd-archs:sip-ring}, where GPUs are organized into fixed-size rings. However, this design imposes a strict limitation: the TP group size must remain fixed. 
To enable communication at dynamic scales and support a wider range of workloads, more complex topologies are adopted (e.g., Dojo~\cite{dojo}, NVIDIA V100~\cite{v100},  TPUv3~\cite{cacm2020tpuv3}, and AWS Trainium ~\cite{aws-trainium} ), which support dynamic scaling by allowing jobs to execute on topology subsets of varying sizes. As shown in \figref{fig:hbd-archs:dojo}, Dojo~\cite{dojo} connects GPUs via mesh-like topologies and employ GPUs to forward traffic. While GPU-centric architectures mitigate cost explosion and can support various scales, they suffer from a large fault explosion radius. A single GPU failure can disrupt the entire HBD by altering its connectivity, degrading communication performance even for healthy GPUs—such as the yellow GPUs in \figref{fig:hbd-archs:dojo}.

\para{Switch-GPU Hybrid HBD.}
This architecture interconnects GPUs via a combination of direct GPU-to-GPU connections and switch links. A typical example is TPUv4~\cite{isca2023tpu}, which organizes TPUs into $4^3$ TPU cubes and connect them via centralized OCS-based switches (\figref{fig:hbd-archs:tpuv4}). TPUv4 scales up to 4,096 TPUs, with its expansion primarily limited by the port count of the OCS-based switch. Furthermore, it suffers from a cube-level fault explosion radius—a failure in any single TPU affects the entire 64-TPU cube, leading to significant performance degradation. Furthermore, OCS-based switches face challenges of high costs and manufacturing complexity, which undermines the cost-effectiveness of TPUv4. TPUv5p cluster~\cite{tpuv5} is similar to TPUv4 but can scale out to 8,960 TPUs.

\begin{figure*}[!tp]
    \centering
    \includegraphics[width=\linewidth]{figs/overview.drawio.pdf}
    \vspace{-5ex}
    \caption{\sys{} overview.}
    \label{fig:overview}
    \vspace{-2ex}
\end{figure*}

% \vspace{-2ex}
\subsection{Key Attributes of An Ideal HBD}
\label{sec:background:workload}


\vspace{-1em}
\begin{table}[h!t] \small
    \centering
    \begin{tabular}{cllllll}
    \toprule
    \textbf{GPU} & \textbf{TP} & \textbf{PP} & \textbf{DP} & \textbf{MFU} & \textbf{$\textbf{MFU}_{TP-8}$} & \textbf{Improve}\\
    \midrule
    1024    & 16 & 4  & 16  & 0.5236 & 0.5217   & 1.0036      \\
    4096    & 16 & 8  & 32  & 0.4668 & 0.4282   & 1.0901      \\
    8192    & 32 & 8  & 32  & 0.4247 & 0.3512   & 1.2093      \\
    16384   & 32 & 16 & 32  & 0.3756 & 0.2584   & 1.4536      \\
    32768   & 32 & 16 & 64  & 0.3090 & 0.1690   & 1.8284      \\
    65536   & 64 & 16 & 64  & 0.2493 & 0.0999   & 2.4955      \\
    131072  & 64 & 16 & 128 & 0.1851 & 0.0550   & 3.3655      \\
    \bottomrule
    \end{tabular}
    \caption{Optimal parallelism strategy for maximum MFU of Llama 3.1-405b, compared to the baseline MFU for TP-8 (e.g., in widely-deployed NVLink architectures), when GPU number varies.}
    \label{tab:eval:llama3-optimal}
    \vspace{-2em}
\end{table}

Existing HBD architectures face fundamental limitations in interconnection cost, resource utilization, and failure resiliency when scaling. To guide a better design, we analyze existing training workloads and explore two key questions without the limitations imposed by current HBD: i) What is the optimal group size that HBD should support? ii) What traffic patterns should HBD accommodate?


\para{Large and adaptable TP size is critical for dense models.}
The optimal LLM training parallelism depends on model architectures and cluster configurations. For example, as illustrated by previous work~\cite{disttrain, nsdi2025_rlhfuse}. 
We evaluate the Model FLOPs Utilization (MFU) for Llama 3.1-405B~\cite{llama3.1-405b} using our in-house LLM training simulator (\S\ref{sec:simulation:end2end}) and report the results in Table~\ref{tab:eval:llama3-optimal}. MFU and TP/PP/DP columns denote the optimal MFU when TP size is unconstrained and the corresponding parallelism strategies respectively. $MFU_{TP-8}$ column denotes the optimal MFU when TP size is limited to 8. As we increase the number of GPUs, the optimal TP size grows from 16 to 64, a trend we observe across other large dense models. 
In this case, the HBD scale restricts the maximum size of TP, which affects training performance as a result. 

\begin{table}[h!t] \small
\vspace{-2ex}
\centering
\begin{tabular}{cccc}
\toprule
\multicolumn{2}{c}{\textbf{Parallelism}} & \textbf{Operation}  & \textbf{Traffic Load}  \\
\midrule

\multicolumn{2}{c}{TP}            & AllReduce     &$2bsh\cdot\frac{n-1}{n}$ \\ 
\multicolumn{2}{c}{EP}            & AllToAll     &$2bsh\cdot\frac{n-1}{n}\cdot\frac{k}{n}$\\
\bottomrule
\end{tabular}
\caption{Communication load of TP and EP on a single MoE layer. $b$: batch size; $s$: sequence length; $h$: hidden dim; $k$: topK of MoE router; $n$: parallel size. Assume each expert is assigned equal number of tokens.}
\label{tab:workload}
\vspace{-5ex}
\end{table}

\para{MoE can also be efficient with large-size TP.}
Beyond widely used dense models, we also examine sparse MoE models, which are trending toward larger scales (e.g., 1T parameters~\cite{switch_transformer}). The distributed training for MoE can be achieved through TP or EP (or a combination of them)\footnote{For TP, each expert is equally sharded to GPUs. For EP, each expert is indivisible and allocated to one GPU in the EP group.}~\cite{sigcomm2023_janus}, both TP and EP are communication-intensive~\cite{atc2023_lina}, making them heavily reliant on HBD. 







\begin{table}[h!t]
    \vspace{-1ex}
    \centering
    \begin{tabular}{cccccc}
        \toprule
        & TP & \multicolumn{4}{c}{EP} \\
        \hline
        imbalance coef & - & 0\% & 10\% & 20\% & 30\% \\
        % \hline
        MFU (\%) & 31.2 & 31.5 & 30.5 & 29.8 & 28.8 \\
        \bottomrule
    \end{tabular}
    \caption{Performance comparison of TP and EP when training GPT-MoE.}
    \label{tab:ep-imbalance}
    \vspace{-2em}
\end{table}


Our production training experience on a 1T MoE model in production brings the following insights into the pros and cons of TP and EP.
On the one hand, EP is more communication-efficient than TP. Table~\ref{tab:workload} compares the communication volume of TP and EP. Clearly, EP is better if $k<n$, which is common~\cite{deepseek_v3} because existing models often choose small $k$ for higher computation sparsity.
On the other hand, EP suffers from the well-known expert imbalance problem~\cite{sigcomm2023_janus}, especially when the MoE routers use the no-token-left-behind algorithm~\cite{deepseek_v3, megablocks, glam}. This will result in non-equivalent number of tokens that each expert will receive, which hence causes straggler nodes that waste GPU cycles of other nodes. 
\tabref{tab:ep-imbalance} shows the simulated result of training GPT-MoE with 1.1T parameters (details in Appendix~\S\ref{appendix:gpt-moe}) under different expert imbalance coefficients\footnote{Calculated as $\frac{max - min}{max}$, where $max$ and $min$ represent the maximum and minimum tokens allocated to each expert respectively.}. When $coef=0$, EP is better than TP due to smaller communication overhead. As $coef$ increases, the MFU drops because of the straggler issue.


\para{Key findings}. These experiments provide us two key findings for HBD design. First, larger HBD size is increasingly needed for rapidly scaling LLMs (i.e., more than 1T parameters). Second, with larger HBD enabled, using TP is more favorable than EP to train MoE, because TP shards the computation equally across GPUs and hence bypasses the expert imbalance problem. 

These findings reveal two key design principles for HBD:  i) HBD must inherently support large and adaptable TP sizes, which fundamentally requires the scalability of HBD architecture; ii) the HBD designs need to ensure the effective support for the Ring-AllReduce communication. Given the demonstrated efficiency of TP in MoE training, ensuring support for Ring-AllReduce support is sufficient for mainstream LLM training scenarios; iii) small fault explosion radius. Thus, \textit{\textbf{we propose designing a large and adaptable HBD architecture tailored for ring-based TP communication to optimize LLM parallelism strategies.}}



