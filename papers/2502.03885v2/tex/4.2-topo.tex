
% \vspace{-10pt}
\subsection{\sys Topology}
\label{section:design:topology}


In this section, we present the \sys{} topology design (\figref{fig:overview}) integrating \docs{} that allows all GPUs within datacenter to be connected in a \textit{reconfigurable K-Hop Ring topology}, while supporting dynamic ring construction and high fault tolerance.  

\para{Intra-node Topology.} The intra-node topology is designed for \textit{dynamic ring construction} and compiles with the OCP UBB 2.0 standard~\cite{UBB2.0}. As shown in \figref{fig:degin:topo:ubb}, one node equipped with $R$ GPUs can support $R$ bundles of \ocstrx. Each \docs{} bundle is connected to a pair of GPUs, with one GPU linking to the upper-half SerDes and the other to the lower-half.
For one group of nodes connected as one line, the two GPU paris at each end can interconnect with the \ocstrx \xspace internal loopback path, forming a GPU-level ring. 
As shown in \figref{fig:overview}, nodes $N_1$ and $N_3$ are connected in a line, where $OCSTrx_1(N_1)$ and $OCSTrx_2(N_3)$ activate the cross-lane loopback path, creating a ring between the 8 GPUs of $N_1$ and $N_3$.
During ring construction, only two \docs{} bundles per node are utilized, while the remaining \docs{} operate in loopback mode. These idle \docs{} can be replaced with direct connections, such as DAC links, offering a trade-off between cost and reliability. \fig{fig:inner-topo}(a,b) shows a 4-GPU node with varying numbers of \textit{\docs{}} bundles. Note that the topology design in this section utilizes a 4-GPU node as an example, it can be easily scaled for 8-GPU nodes.

\begin{figure}[h!t]
   \centering
   \includegraphics[width=0.8\linewidth]{figs/design/ubb-standard.pdf}
   \vspace{-2ex}
   \caption{\ocstrx{} connection within nodes. Each block contains multiple \ocstrx \xspace as one bundle, .e.g, $8\times 800Gbps$ \ocstrx \xspace for a 6.4Tbps GPU. }

   \label{fig:degin:topo:ubb}
   \vspace{-2ex}
\end{figure}

\para{Inter-node Topology.}
We construct the inter-node topology by pruning the full-mesh design, based on two key observations:
i) \textit{Traffic locality}: TP Ring-AllReduce in HBD exhibits neighbor communication patterns, eliminating the need for distant connections; 
ii) \textit{Fault non-locality}: node-side failures typically occur independently at the node level, meaning consecutive multi-node failures follow an exponentially decaying probability.
Each node provides up to $2R$ external paths, allowing us to construct a DC-scale reconfigurable $K$-Hop Ring topology ($K\leq R$) by connecting them to nodes at $\pm 1,...,\pm K, K\leq R$. For AllReduce communication, only two out of the $2K$ links are activated once, with the others serving as backup links for fault isolation.
For example (\figref{fig:overview}), if $N_2$ fails, $OCSTrx_2(N_1)$ and $OCSTrx_1(N_3)$ can switch to backup links, maintaining connectivity between $N_1$ and $N_3$ while isolating $N_2$'s fault. As $K$ increases, the probability of encountering an unbypassed failure rapidly decreases, which is nearly negligible for $K=3$ (detailed analysis in Appendix~\S\ref{appendix:ft-anay}). Thus, this architecture typically achieves a node-level explosion radius.
Moreover, the K-Hop Ring can be broken into the K-Hop line topology, with the trade-off of reduced fault tolerance of $2K$ nodes at two ends.


\begin{figure}[h!t]
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[height=70pt]{figs/design/inner-topo/4gpu-2port.pdf}
        \caption{2 bundles of \ocstrx}
        \label{fig:4g3d}
    \end{subfigure}
    \hspace{10pt}
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[height=70pt]{figs/design/inner-topo/4gpu-3port.pdf}
        \caption{3 bundles of \ocstrx}
        \label{fig:4g2d}
    \end{subfigure}
    \vspace{-2ex}
    \caption{4-GPU node with \docs.}
    \label{fig:inner-topo}
    \vspace{-2ex}

\end{figure}





