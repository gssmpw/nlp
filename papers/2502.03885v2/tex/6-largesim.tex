% \vspace{-2ex}
\section{Large-Scale Simulation}
\label{sec:simulation}

We begin by outlining the experimental methodology and setup (\S\ref{sec:simulation:setup}). Next, we assess fault tolerance across different HBD architectures (\S\ref{sec:simulation:fault}), followed by end-to-end simulations to evaluate training performance under varying parallelism and GPU resource allocations (\S\ref{sec:simulation:end2end}). We then examine the improvements in communication efficiency achieved by our orchestration algorithm (\S\ref{sec:simulation:efficiency}). Finally, we present a comparative cost and power analysis of different HBD architectures (\S\ref{sec:simulation:cost-power}). The simulations demonstrate that \sys{} outperforms other architectures across all metrics. 

 

\vspace{-1ex}
\subsection{Methodology and Setup}
\label{sec:simulation:setup}


An in-house simulator dedicated for LLM training is used to evaluate \sys comprehensively. The simulator supports end-to-end simulations of both model training performance and hardware faults, with the HBD-DCN orchestration algorithm seamlessly integrated into the system.

\begin{figure}[h!t]
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/evaluation/fault_trace_based/cdf_trace_waste_tp16_gr4.pdf}
        \vspace{-1em}
        \caption{TP-16.}
        \label{fig:simulation:waste-cdf:tp16-gr8}
    \end{subfigure}
    \hspace{2pt}
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/evaluation/fault_trace_based/cdf_trace_waste_tp32_gr4.pdf}
        \vspace{-1em}
        \caption{TP-32.}
        \label{fig:simulation:waste-cdf:tp32-gr8}
    \end{subfigure}
    \vspace{-2em}
    \caption{CDF of GPU waste ratio over 4-GPU node based on production fault trace.}
    \vspace{-1em}
    \label{fig:simulation:waste-cdf:gr4}
\end{figure}




\para{GPU and network specification.}
The NVIDIA H100~\cite{h100} (989 TFLOPS, 80GiB) is used for the configuration of GPU in simulation. And the HBD bandwidth of GPU is set as $6.4Tbps$, which is the sum of 8 QSFP-DD \ocstrx. The DCN bandwidth is configured the same as NVIDIA ConnectX-7 ($400Gbps$). 
Since the simulation primarily focuses on HBD, the DCN is configured as a Fat-Tree topology~\cite{sigcomm2008fattree}. Several HBD architectures are then evaluated, including:

\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt, leftmargin=2ex]
    \item \textbf{Big-Switch}: The ideal HBD design, featuring a large centralized switch with no forwarding latency that connects all nodes, as the theoretical upper limit of communication performance and fault resilience.
    \item \textbf{\sys{}}: Two configurations are evaluated: the \ocstrx{} bundle is set to either $K = 2$ or $K = 3$ (\S\ref{section:design:topology}), constructing 2/3-Hop Ring respectively.
    \item \textbf{NVL-36, NVL-72, NVL-576}~\cite{nvl72}: HBDs with 36, 72, or 576 GPUs, GPU are interconnected via NVLink Switches.
    \item \textbf{TPUv4}~\cite{isca2023tpu}: Centralized OCS capable of scheduling with a $4^3$ TPU cube granularity.
    \item \textbf{SiP-Ring}~\cite{sip-ml}: All nodes are connected in a series of static rings with fix sizes equal to the TP sizes.
\end{itemize}


\para{GPU count per node. }The simulation aligns with both  4-GPU node (.e.g. NVIDIA GB200 NVL-36/72/576~\cite{nvl72} and TPUv4~\cite{isca2023tpu}) and 8-GPU node design  (NVIDIA H100, AMD MI300X~\cite{amdmi300}, Intel Gaudi3~\cite{intelgaudi3}, and UBB 2.0 standard\cite{UBB2.0}). 


\begin{figure}[h!t]
\vspace{-1em}
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/evaluation/fault_model_based/frag_ratio_tp16_gr4.pdf}
        \caption{TP-16.}
        \label{fig:simulation:model:wasted-overview:tp16}
    \end{subfigure}
    \hspace{2pt}
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/evaluation/fault_model_based/frag_ratio_tp32_gr4.pdf}
        \caption{TP-32.}
        \label{fig:simulation:model:wasted-overview:tp32}
    \end{subfigure}
    \vspace{-2em}
    
    \caption{GPU wastes ratio over the 4-GPU node with different GPU fault ratio based on fault model.}
    \vspace{-1em}
    \label{fig:simulation:model:wasted-overview}
\end{figure}


\para{Parallelism strategy. } 
Since \sys is primarily designed for TP, the key variable is the TP size. TP-8, TP-16, TP-32, and TP-64 are tested to evaluate the fault resilience of various HBD architectures (\S\ref{sec:simulation:fault}).
Additionally, other parallelism strategies, such as PP and DP, are used to simulate cross-ToR traffic and evaluate the orchestration algorithm (\S\ref{sec:simulation:efficiency}).


\para{Fault patterns. } The fault trace used in the simulation was collected from an 8-GPU node cluster with approximately 3,000 GPUs over a span of 160 days.
On average, the ratio of faulty 8-GPU nodes is $3.83\%$, with the P99 value as $7.22\%$, more details in Appendix~\S\ref{appendix:production-fault-trace}. In some simulations, fault traces generated based on this trace statistics are also derived. 

\subsection{HBD Fault Resilience}
\label{sec:simulation:fault}

This section evaluates the fault resilience of various HBD architectures, focusing on GPU waste ratio, job fault-waiting time, and the maximum job scale supported by the cluster. The main text presents the key results, with more detailed results provided in Appendix~\S\ref{appendix:wasted-GPUs-ratio}.



\para{GPU waste.} 
Apart from faulty GPUs, issues such as fragmentation, topology disconnections, and bandwidth degradation can render healthy GPUs wasted.
The GPU waste ratio quantifies the number of wasted GPUs under different fault scenarios. \figref{fig:simulation:waste-cdf:gr4} illustrates GPU waste ratios over production trace, while \figref{fig:simulation:model:wasted-overview} depicts the GPU waste ratio as node fault ratio vary.

\begin{table}[h!t] \footnotesize
    \vspace{-1ex}
    \centering
    \begin{tabular}{llllll}
    \toprule
    \textbf{GPU Num} & \textbf{TP} & \textbf{DP} & \textbf{PP} & \textbf{EP} & \textbf{MFU} \\
    \midrule
    1024    & 16       & 16      & 4       & 1       & 0.4276         \\
    2048    & 16      & 16      & 8        & 1       & 0.4140        \\
    4096    & 32      & 16      & 8        & 1       & 0.3894        \\
    8192    & 32      & 16      & 16      & 1       & 0.3656       \\
    16384  &  64     & 16       & 16      & 1      & 0.3116       \\
    \bottomrule
    \end{tabular}
    \caption{Optimal parallelism strategies for maximize MFU of GPT-MoE under varying GPU numbers.}
    \vspace{-3em}
    \label{tab:eval:gpt-moe-optimal}
\end{table}


In these scenarios, \sys{} ($K=3$) achieves near-zero GPU waste ratio, and outperforming all other architectures. Especially, the waste ratio for \sys ($K=2$) remains almost identical to \sys{} ($K=3$), allowing one bundle of \docs{} to be saved for clusters with low fault rates.   
NVL-36 and NVL-72 typically experience an 11\% waste ratio for TP sizes of 16 or larger, as $1/9$ of GPUs are reserved for redundant backups. NVL-576 has less fragmentation, benefiting from its larger size. TPUv4 performs well at low fault ratios and small TP sizes, but significantly degrades with larger TP sizes due to its coarse $4^3$ cube-based resource management, which amplifies the fault explosion radius. To sum up, \sys{} demonstrates the strongest fault resilience among all architectures.   



\para{Maximum job supported. } 
In fixed-size clusters, large job must pause when the available GPUs drop below the required count. Faced with same fault rate, cluster with lower GPU waste ratio can support larger job scales. \figref{fig:simulation:job_scale} shows the maximum job scale supported for various HBD architectures cluster with 2880-GPU, simulated with the fault traces normalized for 4-GPU nodes. \sys{} ($K=2$ or $K=3$) and NVL-576 lead in performance, and SiP-Ring exhibits declining efficiency as TP size increases.


\begin{figure}[h!t]
    \centering
    % \subfigure[No Fault-Waiting.]{
        \includegraphics[width=0.7\linewidth]{figs/evaluation/fault_trace_based/no_breakdown_maxjobscale_gr4.pdf}
    \vspace{-2ex}
    \caption{Maximal job scale supported by 2880 GPUs.}
    \label{fig:simulation:job_scale}
    \vspace{-1em}
\end{figure}


\para{Job fault-waiting time.} Large job must wait for the repairing when GPU availability falls below the required threshold. This simulations assume the average recovery time in the fault trace as a fixed repair duration. The total wasted time during 160 days is evaluated (\figref{fig:simulation:breakdown-duration}). For smaller TP sizes (TP-8/TP-16), NVL-36/NVL-72 exhibit the weakest resilience due to their 11\% backup overhead. For larger TP sizes (TP-32/TP-64), SiP-Ring and TPUv4 perform worst. 


\vspace{-1ex}
\subsection{Training Performance}
\label{sec:simulation:end2end}

This section analyzes the training performance of two representative large models, {LLama 3.1-405B}~\cite{llama3herdmodels} and {GPT-MoE} (configuration detailed in Appendix~\S\ref{appendix:gpt-moe}), under various GPU resource configurations and parallelism strategies. The simulation results validate the practical applicability of the \sys{} architecture. In simulations, we model practical TP and EP behaviors: For TP, increasing parallelism splits GEMMs into smaller, less efficient tasks, reducing hardware efficiency~\cite{gemm-eff}; for EP, we practically set expert imbalance coefficient at 20\%.


\begin{figure}[h!t]
    \vspace{-1em}
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/evaluation/fault_trace_based/breakdown_ratio_tp16_gr4.pdf}
        \vspace{-1em}
        \caption{TP-16.}
        \label{fig:simulation:breakdown-duration:tp16-8gpu}
    \end{subfigure}
    \hspace{2pt}
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/evaluation/fault_trace_based/breakdown_ratio_tp32_gr4.pdf}
        \vspace{-1em}
        \caption{TP-32.}
        \label{fig:simulation:breakdown-duration:tp32-8gpu}
    \end{subfigure}
    \vspace{-2em}
    \caption{Job fault-waiting time over the 4-GPU node with different levels of job-scale.}
    \vspace{-1em}
    \label{fig:simulation:breakdown-duration}
\end{figure}

\para{LLama 3.1-405B\footnote{To support larger-scale TP parallelism, we simplified the GQA~\cite{GQA} architecture of LLama 3.1-405B to a traditional MHA architecture.}. }The model adopts a classical decoder-only Transformer architecture. The simulation employs the conventional 3D parallelism strategy\footnote{$TP \in \{1,2,4,8,...,128\}$, $DP \in \{1,2,4,8,...,1024\}$, $PP \in \{1,2,4,8,16\}$, $bsz=2048$}, which combines TP, DP, and PP for performance analysis. 
\tabref{tab:eval:llama3-optimal} presents the optimal parallelism strategies and their corresponding MFU for LLama 3.1-405B under varying GPU resources. As GPU resources increase, the optimal TP size also increases. When the number of GPUs exceeds 8192, the traditional 8-GPU HBD architecture within a single node begins to limit training efficiency. As the cluster size expands, larger TP sizes become increasingly optimal.





\para{GPT-MoE.} The model utilizes the Mixture-of-Experts (MoE) architecture, with $EP \in \{1,2,4,8\}$ introduced in the simulation. \tabref{tab:eval:gpt-moe-optimal} shows the optimal parallelism strategy and the corresponding MFU for GPT-MoE under various GPU resources. The optimal EP value is 1, suggesting that MoE can also achieve high efficiency with TP.


\vspace{-1ex}
\subsection{Communication Efficiency}
\label{sec:simulation:efficiency}

\begin{figure*}[!t]
    \centering
    \hfill{}
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/evaluation/orch_unchange.pdf}
        \caption{Sensitivity to cluster size.}
        \label{fig:simulation:orch:cluster}
    \end{subfigure}
    \hfill{}
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/evaluation/job_scale_orch.pdf}
        \caption{Impact of job-scale ratio.}
        \label{fig:simulation:orch:job}
    \end{subfigure}
    \hfill{}
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/evaluation/ill_rate_orch.pdf}
        \caption{Sensitivity to fault ratio.}
        \label{fig:simulation:orch:fault}
    \end{subfigure}
    \hfill{}
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/evaluation/cost/aggregate-cost.pdf}
        \caption{Aggregate cost.}
        \label{fig:eval:aggregate-cost}
    \end{subfigure}
    \vspace{-2ex}
    \caption{DCN traffic optimization analysis and aggregate normalized cost varies across different architectures under different fault ratios.}
    \label{fig:simulation:job_scale:orch}
    \vspace{-3ex}
\end{figure*}

This section examines the impact of orchestration algorithms on DCN communication efficiency. Experiments were performed on a Fat-Tree architecture, like the setup in ~\cite{sigcomm2024rdmameta}. As shown in \figref{fig:simulation:orch:cluster}, the algorithm is not sensitive to cluster size. Therefore, the evaluation is based on TP-32 operations on \sys{} with 8192 GPUs. 

\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt, leftmargin=2ex]
    \item \textbf{Baseline:} A greedy algorithms, which randomly select nodes from the cluster and use the first permutation that meets the requirements.
    \item \textbf{Optimized:} The HBD-DCN orchestration algorithm proposed in \secref{sec:design:orch}.
\end{itemize}  

\figref{fig:simulation:orch:job} illustrates the impact of job-scale ratios (job size/total cluster GPUs) on cross-ToR traffic, where node fault ratio is 5\%. Baseline consistently results in approximately 10\% cross-ToR traffic. In contrast, the Optimized algorithm significantly outperforms the Baseline, reducing cross-ToR traffic to just 1.72\% even at a 90\% job-scale ratio.
\figref{fig:simulation:orch:fault} explores the sensitivity to node faults, with the job scale ratio fixed at 85\%. The Baseline shows a linear increase of cross-ToR traffic, while the Optimized algorithm sustains near-zero cross-ToR traffic for fault ratios under 7\%. 


\vspace{-2ex}
\subsection{Cost and Power Analysis}
\label{sec:simulation:cost-power}



To evaluate the interconnect costs of HBD architectures, we gather the cost and power information with the following methodologies:

\begin{itemize}[itemsep=2pt,topsep=0pt,parsep=0pt, leftmargin=2ex]
    \item For standard components (DAC cables, optical transceivers, fibers), pricing is sourced from official retailer websites~\cite{FS_COM, FIBER_MALL, NADDOD} with a 60\% wholesale discount validated against internal data.
    \item For components with scarce public pricing information, such as Google Palomar OCS, NVIDIA NVLink Switch, 1.6 Tbps ACC cables/optical transceivers, the data is amalgamated from multiple sources~\cite{SEMIANALYSIS_GB200, SEMIANALYSIS_OCS, SEMIANALYSIS_Power} to enhance accuracy.
    \item Public power consumption data is available for most components, though for NVLink Switch, multiple sources are combined to estimate a reasonable value.
\end{itemize}


The breakdown analysis of each architecture is provided in the Appendix~\S\ref{appendix:cost}. Based on this, the cost and power consumption are normalized according to GPU count and per-GPU bandwidth. As depicted in \tabref{tab:eval:cost-power}, \sys{} exhibits the lowest interconnect cost per GPU per GBps. Under the $K =2$ configuration, its cost is only 62.84\% of Google TPUv4 and 30.86\% of the NVIDIA GB200 NVL-36/72, with minimal power consumption.
This efficiency is primarily attributed to the avoidance of centralized switches. TPUv4 ranks second in interconnect cost and lowest in power consumption, achieved by reducing optical module use and per-port OCS costs. The NVL series has higher interconnect costs and power consumption due to its fully-connected topology and high-cost NVLink Switches. Notably, NVL-576 incurs the highest cost and power consumption due to its multilayer nonconvergent topology, which increases optical module expenses and requires more NVLink Switches.

\begin{table}[h!t] \footnotesize
    \vspace{-3ex}
    \centering
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Architecture}  & \multicolumn{2}{c}{\textbf{Per-GPU}}  & \multicolumn{2}{c}{\textbf{Per-GPU Per-GBps}} \\
 &  Cost & Watts & Cost & Watts \\
    \midrule
    TPUv4  & 1567.20  & 19.39 & 5.22& 0.06 \\
    NVL-36  & 9563.20  & 75.95 & 10.63& 0.08 \\
    NVL-72  & 9563.20  & 75.95 & 10.63 & 0.08 \\
    NVL-36x2  & 17924.00  & 150.33 & 19.92  & 0.17\\
    NVL-576   & 30417.60  & 413.45 & 33.80  & 0.46\\
    \midrule
    \SYS{} ($K=2$) &  2626.80 &  48.10 & 3.28  & 0.06\\\
    \SYS{} ($K=3$) &  3740.60 &  72.05  & 4.68  & 0.09\\
    \bottomrule
    \end{tabular}
    \caption{Interconnect cost (\$) and power (watts).}
    \label{tab:eval:cost-power}
    \vspace{-6ex}
\end{table}


Beyond interconnect costs, fault resilience variations also affect aggregate costs. The aggregate cost is defined as:

\vspace{-1em}
$$Cost_{GPU} \times (N_{Wasted-GPU} + N_{Faulty-GPU}) + Cost_{Interconnect}$$


Simulations on a 11,520-GPU cluster using the TP-32 configuration evaluate GPU availability under varying fault ratios across different architectures.
The variation in aggregate cost for different HBD architectures under varying node fault ratios is illustrated in \figref{fig:eval:aggregate-cost}. \sys{} consistently exhibits the lowest aggregate cost. Furthermore, when the fault ratio is below 12.1\%, the aggregate cost of \sys{} ($K=2$) is less than that of \sys{} ($K = 3$), suggesting that ($K = 2$) is the optimal design for most scenarios.







