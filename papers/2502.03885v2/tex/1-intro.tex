\section{Introduction}


Large-scale Large Language Models (LLMs) training rely on various parallelism strategies~\cite{megatrontrain3DP,zero}, such as Tensor Parallelism (TP), Expert Parallelism (EP), Data Parallelism (DP), Pipeline Parallelism (PP), Context Parallelism (CP) and Sequence Parallelism (SP). These strategies communicate over two types of AI datacenter compute fabrics, each with distinct bandwidth requirements. First, Datacenter Networks (DCNs) provide hundreds of Gbps per GPU and primarily handle DP, PP, CP, and SP traffic, which has lower communication demands. Second, High-Bandwidth Domains (HBDs) offer Tbps-level bandwidth, which is crucial for communication-intensive TP and EP. Efficient HBD design can reduce communication overhead, thereby improving Model FLOPs Utilization (MFU)-a key performance metric for LLM training.



The community has made significant advancements in designing DCNs for LLM training~\cite{wang2024railonly, sigcomm2024hpn,rail-optimized,sigcomm2024meta}. However, scaling HBD to optimize MFU in LLM training remains a challenging problem. Existing HBD architectures~\cite{nvl72,isca2023tpu,dojo,cacm2020tpuv3,aws-trainium} take important steps but still suffer from fundamental limitations in scalability, cost, and fault resiliency. 
\begin{packeditemize}
    \item \textbf{Switch-centric HBDs}, such as NVIDIA NVL-72~\cite{nvl72}, build multilayer nonconvergent networks for HBD with switch chips. However, the switch fabric incurs superlinear cost growth as it scales, constraining the number of GPUs per HBD. This limitation prevents optimal large TP and EP and causes severe \textit{resource fragmentation} when the size of TP/EP group increases. For instance, with 2 HBDs (32 GPUs each), 30 GPUs are wasted for TP-16 jobs if each HBD has a single GPU failure. This waste reduces to 14 GPUs if the two HBDs are combined into a 64-GPU unit. 
    \item \textbf{GPU-centric HBDs}, such as Dojo~\cite{dojo}, NVIDIA V100~\cite{v100}, TPUv3~\cite{cacm2020tpuv3}, and SiP-Ring~\cite{sip-ml}, adopts low-cost GPU-to-GPU links to construct large-scale ring or mesh topologies, forwarding traffic directly through GPUs. However, these architectures suffer from a large \textit{fault explosion radius}, where a single GPU failure degrades bandwidth for a group of adjacent GPUs, compromising the entire topology. For example, in SiP-Ring, one single GPU failure breaks the ring and reforms the topology a line. 
    \item \textbf{Switch-GPU Hybrid HBDs}, TPUv4~\cite{isca2023tpu} alleviates the limitation of large fault explosion radius via OCS-based switch\footnote{In this paper, "OCS" specifically denotes \textit{optical circuit switching capability}, while OCS-based switch denotes \textit{optical circuit switch}.}: each set of 64 TPUs is connected as a cube, with these cubes connected to multiple OCS-based switches to isolate faults within the cubes. However, it cannot resolve it fundamentally, as the fault explosion radius remains large at the cube level (64 TPUs).
\end{packeditemize}




In this paper, we take a first-principles approach to redesigning HBD for LLM training workloads.
Through a top-down analysis of parallelism strategies (\S\ref{sec:background:workload})  for maximizing MFU, we show that increasing TP size yields the most significant MFU gains for both dense and sparse LLM models. For large dense models~\cite{llama3herdmodels}, the optimal TP size scales from 16 to 64 as the number of GPUs increases. For sparse MoE models~\cite{hunyuanlarge,deepseekv3,mixtralexperts}, enlarging TP improves MFU more effectively than EP, particularly when considering the expert imbalance problem~\cite{sigcomm2023_janus}.

These findings lead to two key design principles: i) HBD should be optimized exclusively for TP Ring-Allreduce communication with large message sizes, which communicates with only logical neighboring nodes, eliminating the need for EP and the associated any-to-any communication; ii) Supporting large and adaptable TP is essential, as different GPU numbers and model sizes require varying TP configurations to maximize MFU.


Based on these principles, we propose \textbf{\sys}, a scalable and fault-resilient HBD architecture designed for optimizing TP communication. Our key insight is \textit{unifying connectivity and dynamic switching at the transceiver level} using OCS. By embedding OCS in each transceiver, we achieve reconfigurable point-to-multipoint connectivity. This marks a departure from traditional designs, where transceivers support only point-to-point connections and rely on high-radix switches for routing. We call this new design \textbf{transceiver-centric HBD architecture}. This transceiver-centric architecture offers two key benefits: i) It enables the flexible construction of arbitrarily large ring topologies by intra-node loopback mechanism. This can support optimal TP group sizes for different models, while effectively minimizing resource fragmentation; ii) When one node fails, its neighboring transceivers dynamically reconfigure connections to reroute traffic, significantly reducing the fault explosion radius and improving system resilience.

We realize the transceiver-centric HBD architecture in production by combining the following key ideas: 
\begin{packeditemize}
    \item \emph{Silicon Photonics based OCS transceiver (OCSTrx):} To design a cost-effective low-power transceiver with OCS support, we leverage the current advances of Silicon Photonics (SiPh) technology. Compared to MEMS~\cite{urata2022missionapollo, mem-optical-switches} technology which has been widly used to realize OCS, SiPh offers simpler structures, lower cost and power consumption. We build OCS with Mach-Zehnder interferometer (MZI) matrix~\cite{mzi}, taped out with $65nm$ CMOS processes. The chip size is less than $136.5mm^2$ while the chip power consumption is $3.2Watts$, which can be integrated into commercial QSFP-DD $800Gbps$ transceiver~\cite{QSFP-DD} with sub-1ms path reconfiguration latency. 
    \item \emph{Reconfigurable K-Hop Ring Topology:} While OCSTrx offers reconfigurable connections at the transceiver level, constructing adaptive-size rings that maximize GPU utilization remains a challenge. For example, a naive full-mesh topology built with OCSTrx would impose strict limits on TP size, while also resulting in significant bandwidth waste and fragmentation. To address this, we propose a reconfigurable K-Hop Ring, where each node connects to all other nodes within $\le K$ hops via \textit{OCSTrx}. The intra-node \textit{loopback} mechanism enables dynamic ring construction, while the inter-node \textit{backup link} bypasses faulty nodes, ensuring high fault tolerance.
    
    \item \emph{HBD-DCN Orchestration Algorithm:} While an optimal HBD topology is critical, end-to-end training performance also depends on efficient HBD-DCN coordination. For example, the orchestration of TP groups in HBD directly determines DP traffic distribution, which impacts congestion in DCN, ultimately governing training performance. Unfortunately, existing approaches lack mechanisms to jointly coordinate DCN and HBD to mitigate congestion and optimize communication efficiency. To address this, we propose a new orchestration algorithm that minimizes cross-ToR traffic, thereby minimizing congested traffic.
\end{packeditemize}


To the best of our knowledge, \sys is the first HBD design capable of scaling to datacenter scale while avoiding cost explosion and increased failure-induced waste. We evaluated \sys \xspace with the real 160-day fault trace from our 3K GPU cluster\footnote{Details in Appendix \secref{appendix:production-fault-trace}. We will open-source the trace later.}. When executing TP32 jobs with the trace, it demonstrates 0.44\% GPU waste ratio - 22x and 23x lower than NVL-72 (9.67\%) and TPUv4 (10.30\%). It achieves 3.24x and 1.59x cost reductions compared to NVIDIA NVL-72 and Google TPUv4 respectively. Through the orchestration algorithm, it maintains near-zero cross-ToR traffic under 7\% node failure rates. Its dynamic ring formation capability enables 3.37x higher MFU than NVIDIA DGX systems~\cite{dgx} (8 GPUs/node). 
