



\vspace{-1ex}
\subsection{HBD-DCN Orchestration Algorithm}  
\label{sec:design:orch}  

\sys{} is designed to work with arbitrary DCN, including Rail-Optimized~\cite{rail-optimized, sigcomm2024hpn} and Fat-Tree~\cite{sigcomm2008fattree}. This section co-optimizes communication performance for both HBD and DCN in \sys{}.  


\para{Problem Statement.} In \sys{}, GPUs communicate without routing traffic, preventing congestion at any scale. In contrast, DCNs experience inevitable congestion, leading to performance degradation. To mitigate this, we leverage traffic locality to orchestrate nodes, minimizing cross-ToR traffic. Given a job $J$ requiring $N$ nodes from an available pool of $M$ ($M \geq N$), we must select and order $N$ nodes to satisfy two requirements: (1) nodes in the same TP group should communicate via \sys{}, and (2) other parallel traffic should minimize congestion. Ideally, communication remains within the same ToR, confining congestion to switch-to-node links.  

\begin{figure}[h!t]
\centering
\begin{subfigure}[b]{0.23\textwidth}
    \centering
    \includegraphics[width=0.92\textwidth]{figs/problem-1.png}
    \caption{Orchestration scheme 1.}
    \label{figure:orchstration:problem-1}
\end{subfigure}
\hspace{2pt}
\begin{subfigure}[b]{0.23\textwidth}
    \centering
    \includegraphics[width=0.92\textwidth]{figs/problem-2.png}
    \caption{Orchestration scheme 2.}
    \label{figure:orchstration:problem-2}
\end{subfigure}
\vspace{-6ex}
\caption{Illustration for problem statement of node orchestration.}
\vspace{-1em}
\end{figure}

A naive approach is sorting nodes based on deployment order in \sys{}, fulfilling the first requirement but not the second. As shown in \figref{figure:orchstration:problem-1}, this method places ($N_1$, $N_2$) in the same TP group and ($N_1$, $N_3$) in the same DP group, forcing DP traffic across ToRs. A better scheme (\figref{figure:orchstration:problem-2}) eliminates cross-ToR traffic and congestion. However, considering failures and multiple parallel dimensions complicates orchestration, necessitating an efficient method.  



Our key insight is to arrange nodes in \sys{} based on DCN traffic locality, prioritizing appropriate network distances over minimal ones. For example, in \figref{figure:orchstration:problem-2}, $N_1$'s \sys{} neighbor is $N_3$, despite a 3-hop network distance in DCN. We propose a two-phase solution: (1) a deployment phase defining physical connections in DCN and \sys{}, and (2) a runtime phase using an algorithm to orchestrate nodes for arbitrary-scale jobs. 

\vspace{-1em}
\begin{algorithm}[h!t]
\small
\caption{Orchestration For Fat-Tree}
\label{alg:orchestration-fat-tree-overview}
\SetAlgoNlRelativeSize{-1}
\SetAlgoNlRelativeSize{1}
 \KwIn{
    Topology of DCN and HBD $G$, Faulty Node Set $F$, Job Information $J$.}
 \KwOut{Placement scheme that satisfies job scale and minimizes cross-ToR traffic.}
 Create graph $G_{deploy}=<S_{deploy},E_{deploy}>=\text{Deployment-Strategy}(G)$\;
 Initialize $high=n_{allconstraints}$, $low=0$, $placement =\{\}$\;
\While{ $low \leq high$}
{
     $mid=\lfloor \frac{low+high}{2} \rfloor$\;
     $placement=\text{Placement-Fat-Tree}(G_{deploy},mid,F,J)$\;
    \eIf {$placement$ satisfies job $J$}
    {
         $low=mid+1$\;
    }
    {
         $high=mid-1$\;
    }
}
\KwRet {$placement$}
\end{algorithm}
\vspace{-1em}

\begin{figure}[h!t]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/design/deployment.pdf}
    \vspace{-1em}
    \caption{Illustration of the deployment phase, showing only two backup links for simplicity.}
    \label{fig:fat-tree-topo}
    \vspace{-8pt}
\end{figure}


\para{Deployment Phase.} \figref{fig:fat-tree-topo} shows node deployment in HBD and DCN. \sys{} connects nodes at a network distance of 3 (i.e., cross-ToR). In a DCN with $r$ nodes per ToR, node $N_n$ connects to $N_{n\pm r}$ as main links, while backup links connect to $N_{n\pm 2r}$. For $1 < n \le r$, $N_n$ connects to $N_{D + n - r - 1}$, where $D$ is the total node count (e.g., $N_3$ connects to $N_{14}$). Additionally, $N_1$ may link to the last node, forming a ring.




\para{Runtime Phase.} Without considering DCN topology, \sys{} orchestrates nodes in three steps: (1) identifying cluster faults and modeling healthy nodes as a graph, (2) using Depth-First Search to find connected components, and (3) sequentially placing TP groups within these components. Due to \sys{}â€™s topology, each TP group forms a ring.  


For real-world DCNs, topology constraints refine step (2) and (3). In Fat-Tree networks, congestion arises when (1) a TP group spans multiple Aggregation-Switch domains, or (2) GPUs within a ToR have mismatched TP group ranks, forcing DP, CP, PP, SP traffic across ToRs. Thus, we aim to localize TP groups within the same Aggregation-Switch domain and align ranks within each ToR. Our scheduling algorithm minimizes cross-ToR traffic while meeting job scale requirements via a binary search over constraint variables. \algref{alg:orchestration-fat-tree-overview} outlines the approach, with full details in Appendix~\S\ref{appendix:orch-algo}.  
