

% \vspace{-1em}
\section{Design Overview}
\label{sec:overview}
% \vspace{-0.2em}

In this section, we first present our new HBD architecture \sys{} guided by the design principles outlined above. We then provide an overview of its key components.


\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[height=80pt]{figs/design/transceiver.pdf}
        \caption{Components of OCS transceivers.}
        \label{figure:design:transceiver:component}
    \end{subfigure}
    \hspace{10pt}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[height=80pt]{figs/design/phase-shifter-v2.pdf}
        \caption{Zoom into OCS MZI switch matrix.}
        \label{figure:design:transceiver:ocs}
    \end{subfigure}
    \vspace{-10pt}
    \caption{Design of OCS Transceivers. The core component is OCS integrated in transceivers.}
    \label{figure:design:transceiver}
    \vspace{-15pt}
\end{figure*}

\para{Transceiver-centric HBD architecture}.
As discussed in \S\ref{sec:background:hbd} and summarized in \tabref{tab:hbd-compare}, existing architectures face a fundamental tradeoff among scalability, cost, and fault isolation. The GPU-centric architecture offers high scalability and low cost connectivity but suffers from a large fault explosion radius. In contrast, the switch-centric architecture improves fault isolation by leveraging centralized switches to confine failures to the node level. However, this comes at the cost of reduced scalability and higher connection overhead. The GPU-switch hybrid architecture takes a middle-ground approach but still suffers from significant fault propagation. As a result, no existing architecture fully meets all requirements.

Our key insight is that \textit{connectivity and dynamic switching can be unified at the transceiver level} using Optical Circuit Switching (OCS). By embedding OCS within each transceiver, we can enable reconfigurable point-to-multipoint connectivity, effectively combining both connectivity and switching at the optical layer. This represents a fundamental departure from conventional designs, where transceivers are limited to static point-to-point links and rely on high-radix switches for dynamic switching. We refer to this novel design as the \textit{transceiver-centric HBD architecture}. 

We realize this design with \sys{}, which has three key components as shown in \figref{fig:overview}.


\para{Design 1: Silicon Photonics based OCS transceiver (OCSTrx) (\secref{sec:design:docs}).} 
To enable large-scale deployment, we require a low-cost, low-power transceiver with Optical Circuit Switching (OCS) support. Unlike prior high-radix switches solutions that rely on MEMS-based switching~\cite{urata2022missionapollo, mem-optical-switches}, we leverage advances in Silicon Photonics (SiPh), which offer a simpler structure, lower cost, and reduced power consumption—making them well-suited for commercial transceivers.

Our SiPh-based OCS transceiver (OCSTrx), shown on the left of \figref{fig:overview}, provides two types of communication paths: i) \textit{Cross-lane loopback path (path 3)}, enabling direct GPU-to-GPU communication within the node, which can be used to construct dynamic size topologies; ii) \textit{Dual external paths (path 1\&2)}, connecting to external nodes. All these paths utilize time-division bandwidth allocation, featuring sub-1ms switching latency. With this capability, our \ocstrx \xspace allows dynamic reallocation of full GPU bandwidth to an active external path rather than splitting bandwidth across multiple paths. This eliminates redundant link waste—for instance, activating one external path completely disables the other, ensuring efficient bandwidth utilization.


\para{Design 2: Reconfigurable K-Hop Ring topology (\secref{section:design:topology}).}
With \ocstrx~ that provides reconfigurable connections at the transceiver, the next challenge is designing the topology. A naive starting point is the the full-mesh topology~\cite{fullmesh} which can provide full connectivity among all nodes using \ocstrx~. However, full-mesh design requires $O(N^2)$ links, inducing prohibitive complexity and cost. To reduce costs while maintaining near-ideal fault tolerance and performance, we prune the full-mesh topology into a K-Hop Ring topology based on traffic locality and fault non-locality (Details in~\S\ref{section:design:topology}). Combining the reconfigurability of \ocstrx{}, we propose a \textit{reconfigurable K-Hop Ring topology}, shown in the middle of \figref{fig:overview}, which consists of two key parts:

i) \textit{Intra-node topology:} dynamic GPU-granular ring construction is enabled by activating loopback paths. For example, while $N_1$-$N_3$ physically form a line topology, activating loopback paths creates a ring between $N_1$'s GPUs (1–4) and $N_3$'s GPUs (1–4). This mechanism allows for the construction of arbitrary-sized rings at any location, supporting optimal TP group sizes for different models while effectively minimizing resource fragmentation.

ii) \textit{Inter-node fault isolation: } dual external paths connect to primary and secondary neighbors (e.g., 2-Hop Ring). When a node fails (e.g., $N_2$), its neighbor ($N_1$) activates the backup path ($N_1$-$N_3$) to bypass the fault while maintaining full bandwidth, approaching node-level fault explosion radius. \S\ref{section:design:topology} generalizes this design to $K>2$.


\para{Design 3: HBD-DCN Orchestration Algorithm (\secref{sec:design:orch}).}
Designing an optimal HBD topology is crucial, but end-to-end training performance also depends on the efficient coordination between HBD and DCN. For instance, improper orchestration of TP groups can cause DP traffic to span across ToRs, resulting in DCN congestion. However, existing methods lack the ability to jointly optimize HBD and DCN coordination to alleviate congestion and enhance communication efficiency.
To address this, we propose the HBD-DCN Orchestrator, as shown on the right side of \figref{fig:overview}. The orchestrator takes three inputs: the user-defined job scale and parallelism strategy, the DCN topology and traffic pattern, and the real-time HBD fault pattern. It then generates the TP placement scheme, which maximizes GPU utilization and minimizes cross-ToR communication within the DCN.



