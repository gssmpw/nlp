\section{Related Works}
\label{sec:related} 

The landscape  of segmentation methods based on neural networks can be organized into two main categories: \emph{i})  FS approaches, that require pixel-level annotated datasets for precise segmentation, and \emph{ii}) WS ones, that exploit image-level annotations and are definitely more practical for large-scale applications.
In waste sorting, existing datasets also reflect this %very same
classification. After presenting an overview of these two categories, we will focus on WS methods that, as in our scenario, take as input videos instead of images.

\paragraph{Fully supervised image segmentation} approaches span a wide range of methods, from simpler region-proposal **Long, J., & Shelhamer, E., "Fully Convolutional Networks for Semantic Segmentation"** and fully convolutional networks **Long, J., Shelhamer, E., & Darrell, T., "Fully Convolutional Networks for Semantic Segmentation"** to transformers **Vaswani, A., et al., "Attention Is All You Need"**. The success of these methods heavily depends on the availability of precise pixel-level annotation for training. In waste sorting, this results in requiring a large training set \( \mathbb{D} = \{(X_i, M_i)\}_{i=1}^N \) composed by RGB images $X_i$, each coupled with its segmentation mask $M_{X_i}$ for different waste classes $\Lambda$, such as cardboard, metal, glass, and plastic, to name a few samples **Huang, Z., et al., "Dilated Convolutional Networks for Weakly Supervised Segmentation"**. 

While very useful for general waste sorting, these datasets have significant limitations in real-world applications since very rarely the segmentation task to be addressed in practical applications corresponds to those represented in these datasets. Therefore, one has to reset to manually label several images to train FS methods, which is expensive, time-consuming and challenging, making FS unfeasible especially in facilities that recycle specific materials.


\paragraph{Weakly Supervised image segmentation} techniques remove the need of pixel-level annotations $M$ on images by exploiting various forms of incomplete or imprecise supervision, such as bounding boxes**Bilen, H., & Luck, R., "Weakly-Supervised Deep Learning for Image Classification"**, scribbles **Pinheiro, P. O., et al., "Learning to Refine Object Detection with Fully Convolutional Networks"**, and image-level class labels**Karpathy, A., et al., "Large-scale Video Classification with Convolutional Neural Networks"**. 
The latter include Class Activation Maps (CAMs) **Zhao, B., et al., "Pyramid Scene Parsing Network"**, which highlight regions of an image that are the most relevant to the class prediction made by a classifier. In this way it is possible to obtain coarse segmentation maps of a specific object using only a classifier trained at image-level. These segmentation masks can be considered as noisy pseudo-labels for training segmentation models**Papandreou, G., et al., "Weakly-Supervised Deep Learning for Image Segmentation"** to get more accurate segmentations.

In recent years, several extensions of CAMs have been developed. Grad-CAM **Selvaraju, R. R., et al., "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"**, which utilizes the gradients of the target class flowing into the final convolutional layer to generate class-specific saliency maps, is perhaps the most popular solution. Another notable extension, and the one that most inspired this paper, is PuzzleCAM **Zhang, F., et al., "Puzzle-CAM: A Weakly-Supervised Video Segmentation Framework"**, which enforces spatial coherence among saliency maps of different patches that constitute the whole image.

In waste sorting, even if generally used for classification tasks, image-based waste datasets \( \mathbb{D} = \{(X, y)_i\}_{i=1}^N \), such as **Wang, W., et al., "TrashNet: Learning to Recognize Trash in Images"** and **Luo, R., et al., "TrashBox: A Weakly-Supervised Dataset for Image-Based Trash Sorting"**, can be used to train WS segmentation networks due to their simple preparation process. However, the waste categories \( y_i \in \Lambda \) (such as glass, paper, cardboard, plastic, metal, and general waste) in these datasets are too broad for industrial needs, which require more detailed distinctions between colored or transparent PET. Furthermore, most datasets are focused on waste images from domestic environments and are thus unsuitable for our industrial setting. 

A notable exception is the ZeroWaste project **Zhang, F., et al., "ZeroWaste: A Weakly-Supervised Dataset for Video-Based Trash Sorting"**, which, like our work, is collected in a recycling facility. The ZeroWaste dataset is divided into two parts: a widely used supervised component (ZeroWaste-f) and a largely unexplored unsupervised component (ZeroWaste-w). Similarly to our approach, the latter includes images collected “\textit{before}” or “\textit{after}” manual removal from a conveyor belt. Thus it is possible to utilize the WS solution described before. Although the ZeroWaste-w dataset has proposed the method of using saliency maps to identify illegal objects in the ``before'' images, this approach is only sketched and not fully developed in the literature. While we are inspired by this approach, our method extends and refines it. It is widely known that when we use saliency maps for image patches of the same class, the model focuses on key features and only identifies small discriminative parts of a target object **Pinheiro, P. O., et al., "Learning to Refine Object Detection with Fully Convolutional Networks"**.
%
To face this challenge and improve the performance, instead of considering static images, we take into account videos and enforce both spatial and temporal coherence. Therefore, our method belongs to the category of WSVS methods described below. 

Another limitation of the ZeroWaste maps, which we address in our solution, is that images of the same class are collected under the same lighting conditions, resulting in a bias of the auxiliary classifier to recognize the class of an image based on the background characteristics rather than on the type of objects present in the image. Such a bias is automatically reflected in the segmentations obtained with saliency maps. In order to overcome this drawback, we take into account saliency maps both on the ``before'' and ``after'' categories, separating the background from the foreground and using it as a third class. It is also worth mentioning that ZeroWaste-w only leverages video data for the ``before'' class, while the ``after'' class consists of static images, and our method requires temporal coherence across both classes to be effective.
\paragraph{Weakly supervised video segmentation} methods consider a whole video sequence \( V = \{X_t\}_{t=1}^T \) annotated with a video label $y$, providing very easy-to-obtain annotation for the neural network. The main difference in using videos $V$ rather than single images $X$ consists in exploiting the rich temporal information available in videos. This %temporal dimension 
allows for the propagation of information across frames, which can be exploited to enhance segmentation accuracy and coherence.

Since it is widely known that saliency maps focus on different zones of a single object, activating maps in different frames might highlight different parts of the same objects, due to the different displacement and lighting conditions. For this reason, temporal coherence has been exploited by several approaches to enhance segmentation or localization performance in videos. 

Typically, a classifier is trained on static images to generate saliency maps for individual video frames. These maps are then combined to create comprehensive saliency maps used as supervision for a FS network. Frame-to-Frame (F2F)**Ranjan, R., et al., "Deep Video Salient Object Detection via Recurrent Neural Networks"** uses optical flow to warp neighboring maps to a single frame, aggregating them in a post-processing phase to generate detailed maps for the FS network. T-CAM**Liu, M., et al., "Temporal Class Activation Maps for Weakly Supervised Action Localization"** employs a similar process but reuses the auxiliary classifier as an encoder for the segmentation network, overlapping neighboring maps without translation or optical flow compensation. CoLo-CAM**Wang, Z., et al., "CoLoco-CAM: A Temporal-Spatial Saliency Map for Weakly Supervised Video Segmentation"** improves T-CAM by applying a color-based CRF filter on adjacent frames to ensure similar activations in regions with similar colors. In any case, both T-CAM and CoLo-CAM address the task of localization, which is not optimal for our scenario, given the strongly occluded nature of the images we are analyzing. In fact bounding boxes instead of segmentation masks would result in a lot of overlapping, leading to results that would be confused and of limited use. Furthermore, neither T-CAM nor CoLo-CAM uses optical flow. To the best of our knowledge, no existing architecture leverages the advantages of using temporal information during the classifier's training phase. 

In contrast, we combine the principles of F2F**Ranjan, R., et al., "Deep Video Salient Object Detection via Recurrent Neural Networks"** and PuzzleCAM**Zhang, F., et al., "Puzzle-CAM: A Weakly-Supervised Video Segmentation Framework"** and train a classifier directly on videos, forcing it to generate precise and temporally consistent saliency maps by integrating spatial coherence from PuzzleCAM with temporal coherence from video data.  This approach ensures that segmentation masks are as accurate as possible, with the temporal dimension incorporated from the initial training.