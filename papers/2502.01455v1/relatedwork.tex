\section{Related Works}
\label{sec:related} 

The landscape  of segmentation methods based on neural networks can be organized into two main categories: \emph{i})  FS approaches, that require pixel-level annotated datasets for precise segmentation, and \emph{ii}) WS ones, that exploit image-level annotations and are definitely more practical for large-scale applications.
In waste sorting, existing datasets also reflect this %very same
classification. After presenting an overview of these two categories, we will focus on WS methods that, as in our scenario, take as input videos instead of images.

\paragraph{Fully supervised image segmentation} approaches span a wide range of methods, from simpler region-proposal \cite{girshick2014rich, hariharan2014simultaneous, dai2015convolutional, caesar2016region, shen2020ranet} and fully convolutional networks \cite{noh2015learning, chen2018encoder, adeyinka2019deep, zhang2018exfuse} to transformers \cite{xie2021segformer}. The success of these methods heavily depends on the availability of precise pixel-level annotation for training. In waste sorting, this results in requiring a large training set \( \mathbb{D} = \{(X_i, M_i)\}_{i=1}^N \) composed by RGB images $X_i$, each coupled with its segmentation mask $M_{X_i}$ for different waste classes $\Lambda$, such as cardboard, metal, glass, and plastic, to name a few samples \cite{bashkirova2022zerowaste, 9395690, hong2020trashcan, sanchez2022cleansea, wang2020multi, proencca2020taco}. 

While very useful for general waste sorting, these datasets have significant limitations in real-world applications since very rarely the segmentation task to be addressed in practical applications corresponds to those represented in these datasets. Therefore, one has to reset to manually label several images to train FS methods, which is expensive, time-consuming and challenging, making FS unfeasible especially in facilities that recycle specific materials.


\paragraph{Weakly Supervised image segmentation} techniques remove the need of pixel-level annotations $M$ on images by exploiting various forms of incomplete or imprecise supervision, such as bounding boxes\cite{dai2015boxsup, papandreou2015weakly, khoreva2017simple, song2019box}, scribbles\cite{lin2016scribblesup, vernaza2017learning, tang2018normalized}, and image-level class labels\cite{kolesnikov2016seed, jin2017webly, hou2018self, wei2018revisiting, huang2018weakly, lee2019ficklenet, shimoda2019self, sun2020mining}. 
The latter include Class Activation Maps (CAMs) \cite{zhou2016learning}, which highlight regions of an image that are the most relevant to the class prediction made by a classifier. In this way it is possible to obtain coarse segmentation maps of a specific object using only a classifier trained at image-level. These segmentation masks can be considered as noisy pseudo-labels for training segmentation models\cite{sun2020mining, shimoda2019self, kolesnikov2016seed,huang2018weakly} to get more accurate segmentations.

In recent years, several extensions of CAMs have been developed. Grad-CAM \cite{selvaraju2017grad}, which utilizes the gradients of the target class flowing into the final convolutional layer to generate class-specific saliency maps, is perhaps the most popular solution. Another notable extension, and the one that most inspired this paper, is PuzzleCAM \cite{jo2021puzzle}, which enforces spatial coherence among saliency maps of different patches that constitute the whole image.

In waste sorting, even if generally used for classification tasks, image-based waste datasets \( \mathbb{D} = \{(X, y)_i\}_{i=1}^N \), such as \texttt{TrashNet}\cite{yang2016classification} and \texttt{TrashBox}\cite{kumsetty2022trashbox}, can be used to train WS segmentation networks due to their simple preparation process. However, the waste categories \( y_i \in \Lambda \) (such as glass, paper, cardboard, plastic, metal, and general waste) in these datasets are too broad for industrial needs, which require more detailed distinctions between colored or transparent PET. Furthermore, most datasets are focused on waste images from domestic environments and are thus unsuitable for our industrial setting. 

A notable exception is the ZeroWaste project \cite{bashkirova2022zerowaste}, which, like our work, is collected in a recycling facility. The ZeroWaste dataset is divided into two parts: a widely used supervised component (ZeroWaste-f) and a largely unexplored unsupervised component (ZeroWaste-w). Similarly to our approach, the latter includes images collected “\textit{before}” or “\textit{after}” manual removal from a conveyor belt. Thus it is possible to utilize the WS solution described before. Although the ZeroWaste-w dataset has proposed the method of using saliency maps to identify illegal objects in the ``before'' images, this approach is only sketched and not fully developed in the literature. While we are inspired by this approach, our method extends and refines it. It is widely known that when we use saliency maps for image patches of the same class, the model focuses on key features and only identifies small discriminative parts of a target object \cite{huang2018weakly, wei2017object, zhang2018adversarial}.
%
To face this challenge and improve the performance, instead of considering static images, we take into account videos and enforce both spatial and temporal coherence. Therefore, our method belongs to the category of WSVS methods described below. 

Another limitation of the ZeroWaste maps, which we address in our solution, is that images of the same class are collected under the same lighting conditions, resulting in a bias of the auxiliary classifier to recognize the class of an image based on the background characteristics rather than on the type of objects present in the image. Such a bias is automatically reflected in the segmentations obtained with saliency maps. In order to overcome this drawback, we take into account saliency maps both on the ``before'' and ``after'' categories, separating the background from the foreground and using it as a third class. It is also worth mentioning that ZeroWaste-w only leverages video data for the ``before'' class, while the ``after'' class consists of static images, and our method requires temporal coherence across both classes to be effective.
\paragraph{Weakly supervised video segmentation} methods consider a whole video sequence \( V = \{X_t\}_{t=1}^T \) annotated with a video label $y$, providing very easy-to-obtain annotation for the neural network. The main difference in using videos $V$ rather than single images $X$ consists in exploiting the rich temporal information available in videos. This %temporal dimension 
allows for the propagation of information across frames, which can be exploited to enhance segmentation accuracy and coherence.

Since it is widely known that saliency maps focus on different zones of a single object, activating maps in different frames might highlight different parts of the same objects, due to the different displacement and lighting conditions. For this reason, temporal coherence has been exploited by several approaches to enhance segmentation or localization performance in videos. 

Typically, a classifier is trained on static images to generate saliency maps for individual video frames. These maps are then combined to create comprehensive saliency maps used as supervision for a FS network. Frame-to-Frame (F2F)~\cite{lee2019frame} uses optical flow to warp neighboring maps to a single frame, aggregating them in a post-processing phase to generate detailed maps for the FS network. T-CAM~\cite{belharbi2023tcam} employs a similar process but reuses the auxiliary classifier as an encoder for the segmentation network, overlapping neighboring maps without translation or optical flow compensation. CoLo-CAM~\cite{belharbi2023colo} improves T-CAM by applying a color-based CRF filter on adjacent frames to ensure similar activations in regions with similar colors. In any case, both T-CAM and CoLo-CAM address the task of localization, which is not optimal for our scenario, given the strongly occluded nature of the images we are analyzing. In fact bounding boxes instead of segmentation masks would result in a lot of overlapping, leading to results that would be confused and of limited use. Furthermore, neither T-CAM nor CoLo-CAM uses optical flow. To the best of our knowledge, no existing architecture leverages the advantages of using temporal information during the classifier's training phase. 

In contrast, we combine the principles of F2F\cite{lee2019frame} and PuzzleCAM\cite{jo2021puzzle} and train a classifier directly on videos, forcing it to generate precise and temporally consistent saliency maps by integrating spatial coherence from PuzzleCAM with temporal coherence from video data.  This approach ensures that segmentation masks are as accurate as possible, with the temporal dimension incorporated from the initial training.