%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{makecell}
%\usepackage[table]{xcolor}
\usepackage[dvipsnames]{xcolor}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Complex Wavelet Mutual Information Loss: A Multi-Scale Loss Function for Semantic Segmentation}

\begin{document}

\twocolumn[
\icmltitle{Complex Wavelet Mutual Information Loss: A Multi-Scale Loss Function for Semantic Segmentation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Renhao Lu}{cornelBME}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{cornelBME}{Meinig School of Biomedical Engineering, Cornell University, Ithaca, NY, USA}

\icmlcorrespondingauthor{Renhao Lu}{rl839@cornell.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Semantic segmentation, wavelet transform, steerable pyramid, mutual information}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Recent advancements in deep neural networks have significantly enhanced the performance of semantic segmentation. However, class imbalance and instance imbalance remain persistent challenges, where smaller instances and thin boundaries are often overshadowed by larger structures. To address the multiscale nature of segmented objects, various models have incorporated mechanisms such as spatial attention and feature pyramid networks. Despite these advancements, most loss functions are still primarily pixel-wise, while regional and boundary-focused loss functions often incur high computational costs or are restricted to small-scale regions. To address this limitation, we propose complex wavelet mutual information (CWMI) loss, a novel loss function that leverages mutual information from subband images decomposed by a complex steerable pyramid. The complex steerable pyramid captures features across multiple orientations and preserves structural similarity across scales. Meanwhile, mutual information is well-suited for capturing high-dimensional directional features and exhibits greater noise robustness. Extensive experiments on diverse segmentation datasets demonstrate that CWMI loss achieves significant improvements in both pixel-wise accuracy and topological metrics compared to state-of-the-art methods, while introducing minimal computational overhead. The code is available at \href{https://anonymous.4open.science/r/CWMI-83B7/}{https://anonymous.4open.science/r/CWMI-83B7/}
\end{abstract}

\section{Introduction}
\label{Introduction}

Semantic segmentation, the process of partitioning an image into regions associated with semantic labels, plays a crucial role in applications ranging from autonomous driving to biomedical imaging. Despite significant progress driven by deep neural networks such as U-Net \cite{ronneberger2015u} and fully convolutional networks \cite{long2015fully}, challenges persist. Class imbalance, where dominant classes overshadow smaller ones, and instance imbalance, where small-scale structures are frequently ignored, remain major obstacles \cite{jiang2024multi,kofler2023blob}. Addressing these imbalances requires not only pixel-wise accuracy, but also the preservation of structural similarity, a property vital for ensuring spatial coherence and topological integrity in segmented outputs \cite{wang2004image}.

Although the advancements in feature extraction, such as feature pyramid networks \cite{lin2017feature} and attention mechanisms \cite{woo2018cbam,chen2021transunet,islam2020brain}, have enabled models to capture multiscale contextual information, most loss functions still focus on pixel-wise optimization \cite{azad2023loss}. Region-based loss functions, such as Dice loss \cite{milletari2016v} and Tversky loss \cite{salehi2017tversky}, are not subject to class imbalance, but smaller instances within the same class are still easy to be overshadowed, which can be critical for preserving regional and boundary details. Zhao et al. developed Regional Mutual Information (RMI) loss, which captures statistical relationships over regions. However, RMI is constrained within a relatively small region ($3\times3$ pixels) to avoid high computational overhead \cite{zhao2019region}. Thus, a loss function that balances computational efficiency with the ability to model structural and regional dependencies at larger scales is highly desirable.

Addressing these challenges from a frequency-domain perspective provides an innovative pathway. Patterns of varying scales in images are inherently tied to their frequency components: large-scale structures correspond to low frequencies, while finer details correspond to high frequencies. Wavelet transforms are uniquely suited for this multiscale decomposition, as they preserve both spatial and frequency information \cite{mallat1989theory}. Among these, the steerable pyramid, proposed by \cite{simoncelli1992shiftable}, leverages steerable filters for redundant wavelet decomposition, enabling multiscale and multi-orientation feature extraction. Its extension, the complex steerable pyramid, further enhances this framework by using complex numbers to explicitly represent local phase information, allowing for robust extraction of structural details across scales and orientations \cite{portilla2000parametric}. These properties make it a powerful tool for segmentation tasks where structural similarity is paramount.

In this paper, we introduce Complex Wavelet Mutual Information (CWMI) loss, a novel loss function that leverages the complex steerable pyramid for efficient multiscale structural information extraction. By combining the robust multiscale decomposition capabilities of the complex steerable pyramid with the statistical power of mutual information, CWMI loss explicitly incorporates local phase, orientation, and structural features into the loss calculation. This approach ensures structural coherence and boundary preservation while maintaining computational efficiency, making it well-suited for segmentation tasks with significant class and instance imbalances.

Our contributions are summarized as follows:
\begin{itemize}
    \item We propose CWMI loss, which can maximize the mutual information in the domain of complex steerable pyramid decompositions. Such a strategy can enhance multiscale structural features for semantic segmentation, especially for tasks with significant class and instance imbalances.
    \item We demonstrated the superiority of CWMI with extensive experiments on four public segmentation datasets: SNEMI3D (neurite segmentation in electron microscopy slices), GlaS (gland segmentation in H\&E slices), DRIVE (retinal vessel segmentation in fundus images), and MASS ROAD (road segmentation from aerial imagery). Compared with 11 state-of-art (SOTA) loss functions, CWMI showed better performance on both pixel-wise metrics and topological metrics, while introducing minimal computational overhead. 
\end{itemize}


\section{Related Work}

Semantic segmentation has seen tremendous advancements through deep learning architectures, with U-Net and its variants becoming a cornerstone of this field. The U-Net model, proposed by \cite{ronneberger2015u}, utilizes a symmetric encoder-decoder architecture with skip connections to preserve spatial details while capturing global context. Its success has inspired numerous adaptations in its convolutional blocks \cite{huang2019fixed,diakogiannis2020resunet} and skip connections \cite{zhou2018unet++,chen2021transunet}. Inspired by the transformer model, the attention mechanism has also been incorporated into the U-Net structure, which has shown significant performance enhancement, as in Attention U-Net \cite{islam2020brain}, TransUNet \cite{chen2021transunet}, etc. In this study, we compare our proposed CWMI loss using U-Net and Attention U-Net to test the generalization and superiority of CWMI, while the general idea is adaptable to other architectures as well. 

\subsection{Loss Functions for Semantic Segmentation}
While the architectural advancements in segmentation models have been significant, the performance of these networks is highly influenced by the design of the loss functions. Pixel-wise cross entropy loss (CE) minimizes the log likelihood of the prediction error but is significantly prone to class imbalance. To address this issue, class balanced cross entropy (BCE), proposed by \cite{long2015fully}, employs higher weights for classes with fewer pixels. Focal loss assigns higher weights to misclassified pixels with high probabilities \cite{ross2017focal}. Region-based losses, including Dice loss \cite{milletari2016v}, Tversky loss \cite{salehi2017tversky}, and Jaccard loss \cite{rahman2016optimizing}, inherently handle class imbalance but fail to address instance imbalance within the same class, such as thin boundaries and small objects.

To tackle instance imbalance, weighted loss functions have been proposed. In the original U-Net paper \cite{ronneberger2015u}, the weighted cross entropy (WCE) was introduced, employing a distance-based weight map to emphasize thin boundaries between objects. However, WCE assigns weights only to boundary pixels, neglecting object pixels. The adaptive boundary weighted (ABW) loss \cite{liu2022boundary} extends this approach by applying distance-based weights to both boundary and object pixels, while the Skea-Topo loss further improved the weight map based on boundary and object skeletons \cite{liu2024enhancing}. Despite their contributions, weighted losses suffer from two major limitations: (1) the weight maps are precomputed and fixed, failing to adapt to errors during training, and (2) they often generate thicker boundaries, which preserve topology but compromise metrics like Dice score and mIoU, as observed in our qualitative results.

Several methods address instance imbalance dynamically during training but at the cost of computational efficiency. Topology-based approaches, such as persistent homology methods \cite{stucki2023topologically,oner2023persistent}, describe image topologies and identify critical pixels but are computationally expensive, with cubic complexity to image size. The clDice loss \cite{shit2021cldice} employs a soft skeletonization algorithm to detect topological errors, primarily focusing on thin-boundary objects like retinal blood vessels. Similarly, Boundary Loss \cite{kervadec2019boundary} and Hausdorff Distance Loss \cite{karimi2019reducing} refine boundaries but incur significant computational overhead. Region Mutual Information (RMI) loss \cite{zhao2019region} captures pixel interdependencies over regions, but struggles with scalability for large-scale regional analysis due to the high computation cost. These losses either prioritize small regions at the expense of global accuracy or require extensive computational resources, necessitating more efficient and balanced approaches.


\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{Figure_1.PNG}
    \vspace{-10pt}
    \caption{Illustration of the proposed Complex Wavelet Mutual Information (CWMI) Loss. The prediction and label images are decomposed using a complex steerable pyramid, which generates subbands at different scales and orientations. Mutual information is calculated for each corresponding pair of subbands, and the CWMI is computed as the sum of these mutual information values. \(\mathbf{Y}_{B_n},\mathbf{P}_{B_n}\): complex steerable decomposition of label and prediction image at level \(n\); \(I(\mathbf{Y}_{B_n}, \mathbf{Y}_{B_n})\): mutual information between \(\mathbf{Y}_{B_n}\) and \(\mathbf{P}_{B_n}\)}
    \label{fig:schematic}
\end{figure*}

\subsection{Wavelet-Based Loss Functions}

Wavelet-based metrics were first introduced as Complex Wavelet Structural Similarity (CW-SSIM) \cite{sampat2009complex}, known for their robustness to small translations and rotations. In the deep learning era, wavelet-based methods have been employed in loss functions, leveraging their ability to analyze multiscale and multiresolution features. These methods have shown promise in tasks like sketch-to-image translation \cite{kim2023whfl}, image super-resolution \cite{korkmaz2024training}, image dehazing \cite{yang2020net}, and material analysis \cite{prantl2022wavelet}. However, to the best of our knowledge, wavelet-based loss functions have yet to be explored in semantic segmentation.

Existing wavelet-based loss functions typically rely on $L_1$ \cite{zhu2021wavelet,korkmaz2024training,prantl2022wavelet} or $L_2$ distances \cite{kim2023whfl}, or structural similarity (SSIM) \cite{yang2020net} in the decomposed domain. While effective, these methods are less suited for handling high-dimensional data with complex directional features, as in wavelet transforms, and may be vulnerable to noise. The proposed CWMI loss leverages mutual information between wavelet-based subband images, effectively capturing multiscale dependencies. As demonstrated in later ablation tests, CWMI outperforms traditional metrics like $L_1$, $L_2$, and SSIM, offering superior segmentation performance and robustness.


\section{Methods}

To compute the proposed Complex Wavelet Mutual Information (CWMI) loss, the prediction and ground truth label matrices are first decomposed into subbands using the complex steerable pyramid. Mutual information (MI) is then calculated for each subband. The CWMI loss is defined as the sum of the MI values across all subbands, as illustrated in Figure \ref{fig:schematic}.

\subsection{Complex Wavelet Decomposition}

\paragraph{Wavelet Transform}
Wavelet transforms are widely used for multiscale analysis, enabling decomposition of an image into frequency subbands while preserving spatial information. Unlike traditional Fourier transforms, which analyze global frequency components, wavelet transforms provide a localized frequency representation, making them well-suited for tasks involving spatially-varying structures such as semantic segmentation. For an image \(I(x, y)\), the wavelet transform is defined as:
\begin{equation}
W_{\psi}(s, t) = \int \int I(x, y) \psi_{s, t}(x, y) \,dx\,dy
\end{equation}
where \( \psi_{s, t}(x, y) \) is a scaled and translated version of the mother wavelet function \( \psi(x, y) \), with \( s \) controlling the scale and \( t \) the translation.

For discrete signals, the Discrete Wavelet Transform (DWT) decomposes an image into progressively lower resolution subbands using filter banks. However, traditional wavelet decompositions suffer from limited orientation selectivity, capturing only fixed horizontal, vertical, and diagonal directions.

\paragraph{Steerable Pyramid}
To overcome these limitations, steerable pyramid extends the wavelet framework by introducing orientation-sensitive band-pass filters, significantly enhancing orientation selectivity. Unlike DWT, which provides a non-redundant representation, the steerable pyramid offers a flexible, redundant image representation, facilitating improved multiscale analysis. This decomposition is achieved through the iterative application of steerable band-pass filters followed by downsampling. In the frequency domain, the band-pass filter for the \(k_{th}\) orientation is expressed in polar coordinates \((r, \theta)\) as:
\begin{equation}
B_k(r, \theta) = H(r) G_k(\theta), \quad k \in [1, K],
\end{equation}
where \(H(r)\) and \(G_k(\theta)\) represent the radial and angular components, respectively:
\begin{equation}
H(r) =
\begin{cases}
    \cos\left(\frac{\pi}{2} \log_2 \left(\frac{2r}{\pi}\right)\right), & \frac{\pi}{4} < r < \frac{\pi}{2}, \\
    1, & r \leq \frac{\pi}{2}, \\
    0, & r \geq \frac{\pi}{4}.
\end{cases}
\end{equation}
\begin{equation}\label{G real}
G_k(\theta) =
    \alpha_k \left|\cos\left(\theta - \frac{\pi k}{K}\right)\right|^{K-1},
\end{equation}
where \(K\) is the number of orientations and \(\alpha_k = 2^{k-1} \frac{(K-1)!}{\sqrt{K[2(K-1)]!}}\). Figure \ref{fig:2}A provides an illustration of the band filter with \(K=4\).

With total recursive levels \(N\), an image \(I\) can be decomposed as:
\begin{equation}\label{eq:7}
\mathbf{I} \rightarrow
\left|
\begin{array}{ll}
    \mathbf{I}_{H_0} \in \mathbf{R}^{H_0 \times W_0}, \\
    \mathbf{I}_{B_1} \in \mathbf{R}^{K \times H_0 \times W_0}, \\
    \mathbf{I}_{B_2} \in \mathbf{R}^{K \times H_1 \times W_1}, \\
    \dots \\
    \mathbf{I}_{B_N} \in \mathbf{R}^{K \times H_{N-1} \times W_{N-1}}, \\
    \mathbf{I}_{L_0} \in \mathbf{R}^{H_{N-1} \times W_{N-1}},
\end{array}
\right.
\end{equation}
where \(\mathbf{I}_{H_0}\) and \(\mathbf{I}_{L_0}\) are high-frequency and low-frequency residues, and \(\mathbf{I}_{B_n}\) represents subband images at level \(n\) with \(k\)-th direction concatenated in the first dimension. Figure \ref{fig:2}B shows an example of the decomposed output of an input image \(\mathbf{I}\).

\paragraph{Complex Steerable Pyramid}
Although the steerable pyramid effectively captures amplitude information across multiple orientations, it lacks the ability to extract local phase information, which is crucial for encoding structural features such as edges and corners \cite{canny1986computational}. To address this limitation, \cite{portilla2000parametric} introduced the complex steerable pyramid, which extends the conventional steerable pyramid by converting its decomposed images into their analytic signal representation. In this formulation, the real part remains unchanged, while the imaginary part is obtained via the Hilbert transform of the real component. In the Fourier domain, this transformation is equivalent to discarding negative frequency components, as illustrated in Figure \ref{fig:2}C.

For the complex steerable pyramid, the angular component \(G_k(\theta)\) is modified as:
\begin{equation}\label{G complex}
\tilde{G_k}(\theta) =
\begin{cases}
    2\alpha_k \left[\cos\left(\theta - \frac{\pi k}{K}\right)\right]^{K-1}, & \left| \theta - \frac{\pi k}{K} \right| < \frac{\pi}{2}, \\
    0, & \text{otherwise}.
\end{cases}
\end{equation}
This modification enables image decomposition into complex subbands, where phase information encodes critical structural features such as edges and corners (Figure \ref{fig:2}D), while amplitude represents feature strength.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Figure_2.PNG}
    \vspace{-10pt}
    \caption{Steerable pyramid and complex steerable pyramid. (A) Orientation-selective band-pass filters of the steerable pyramid. (B) Example decomposition using a steerable pyramid with N=3, K=4. (C) Band-pass filters of the complex steerable pyramid, where negative frequency components are discarded. (D) Phase representation of the complex steerable pyramid output, with the real part identical to that of the steerable pyramid.}
    \label{fig:2}
\end{figure}

\subsection{Mutual Information in Complex Wavelet Domain}

According to Equation \ref{eq:7}, the ground truth \(\mathbf{Y}\) and the prediction \(\mathbf{P}\) are decomposed into subbands \(\mathbf{Y}_{B_n}\) and \(\mathbf{P}_{B_n} \in \mathbf{R}^{K \times H_{n-1} \times W_{n-1}}\) for each level \(n \in [1, N]\). For each pixel \((x, y)\) at level \(n\), the \(K\)-directional features are treated as \(K\)-dimensional random variables. The mutual information between \(\mathbf{Y}_{B_n}\) and \(\mathbf{P}_{B_n}\) is:
\begin{equation}
I(\mathbf{Y}_{B_n}; \mathbf{P}_{B_n}) = H(\mathbf{Y}_{B_n}) - H(\mathbf{Y}_{B_n}|\mathbf{P}_{B_n}),
\end{equation}
where \(H(\mathbf{Y}_{B_n})\) is constant. Following \cite{zhao2019region}, we estimate the mutual information with:
\begin{equation}
I_l(\mathbf{Y}_{B_n}; \mathbf{P}_{B_n}) \approx -\frac{1}{2}\log\det(\mathbf{M}_n),
\end{equation}
\begin{equation}\label{eq:9}
\mathbf{M}_n = \mathbf{\Sigma}_{\mathbf{Y}_{B_n}} - \text{Cov}(\mathbf{Y}_{B_n}, \mathbf{P}_{B_n}) (\mathbf{\Sigma}_{\mathbf{P}_{B_n}}^{-1})^T \text{Cov}(\mathbf{Y}_{B_n}, \mathbf{P}_{B_n})^T
\end{equation}
where \(\mathbf{\Sigma}_{\mathbf{Y}_{B_n}}\) and \(\mathbf{\Sigma}_{\mathbf{P}_{B_n}}\) are covariance matrices, and \(\text{Cov}(\mathbf{Y}_{B_n}, \mathbf{P}_{B_n})\) is the cross-covariance. For the complex field, this equation is extended with Hermitian transpose \(H\):
\begin{equation}\label{eq:10}
\mathbf{\tilde{M}}_n = \mathbf{\Sigma}_{\mathbf{Y}_{B_n}} - \text{Cov}(\mathbf{Y}_{B_n}, \mathbf{P}_{B_n}) (\mathbf{\Sigma}_{\mathbf{P}_{B_n}}^{-1})^H \text{Cov}(\mathbf{Y}_{B_n}, \mathbf{P}_{B_n})^H
\end{equation}

Finally, the CWMI loss is computed as the sum of MI across all levels, which combines with cross entropy loss to integrate pixel-wise loss:
\begin{equation}
CWMI(\mathbf{Y}, \mathbf{P}) = (1-\lambda)\sum_{n=1}^N -I_l(\mathbf{Y}_{B_n}; \mathbf{P}_{B_n})+\lambda L_{ce}(\mathbf{Y}, \mathbf{P}).
\end{equation}

\begin{table*}\label{Table 1}
\scriptsize
    \centering
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{lccccc |ccccc}
        \Xhline{1pt}
        \multicolumn{11}{c}{\textbf{SENMI3D}}\\
        \hline
        ~&~&~&UNet&~&~&
        ~&~&AttenUNet&~ &~\\
Methods  & 
mIoU$\uparrow$ & mDice$\uparrow$ & VI$\downarrow$ & ARI$\uparrow$ & HD$\downarrow$ & 
mIoU$\uparrow$ & mDice$\uparrow$ & VI$\downarrow$ & ARI$\uparrow$ & HD$\downarrow$\\

\hline

CE & \(0.751_{\pm0.005}\) & \(0.850_{\pm0.004}\) & \(2.23_{\pm0.27}\) & \(0.477_{\pm0.041}\) & \(1.21_{\pm0.21}\) & \(0.754_{\pm0.006}\) & \(0.852_{\pm0.004}\) & \(1.69_{\pm0.15}\) & \(0.558_{\pm0.030}\) & \(1.23_{\pm0.36}\) \\
BCE & \(0.736_{\pm0.007}\) & \(0.841_{\pm0.005}\) & \(1.80_{\pm0.21}\) & \(0.558_{\pm0.029}\) & \(1.45_{\pm0.03}\) & \(0.736_{\pm0.014}\) & \(0.841_{\pm0.010}\) & \(1.79_{\pm0.03}\) & \(0.561_{\pm0.011}\) & \(1.40_{\pm0.12}\) \\
Dice & \(0.767_{\pm0.005}\) & \(0.861_{\pm0.004}\) & \(1.38_{\pm0.06}\) & \(0.605_{\pm0.016}\) & \(0.91_{\pm0.20}\) & \(0.767_{\pm0.007}\) & \(0.861_{\pm0.005}\) & \(1.53_{\pm0.11}\) & \(0.580_{\pm0.021}\) & \(0.81_{\pm0.05}\) \\
Focal & \(0.729_{\pm0.006}\) & \(0.836_{\pm0.005}\) & \(1.72_{\pm0.06}\) & \(0.562_{\pm0.014}\) & \(1.72_{\pm0.23}\) & \(0.724_{\pm0.004}\) & \(0.832_{\pm0.003}\) & \(2.01_{\pm0.19}\) & \(0.529_{\pm0.009}\) & \(1.81_{\pm0.14}\) \\
Jaccard & \(0.766_{\pm0.003}\) & \(0.861_{\pm0.003}\) & \(1.36_{\pm0.11}\) & \(0.615_{\pm0.019}\) & \(0.89_{\pm0.06}\) & \(0.764_{\pm0.001}\) & \(0.860_{\pm0.000}\) & \(1.36_{\pm0.10}\) & \(0.609_{\pm0.009}\) & \(0.90_{\pm0.04}\) \\
Tversky & \(0.764_{\pm0.005}\) & \(0.860_{\pm0.003}\) & \(1.30_{\pm0.01}\) & \(0.622_{\pm0.004}\) & \(1.07_{\pm0.19}\) & \(0.765_{\pm0.004}\) & \(0.860_{\pm0.003}\) & \(1.28_{\pm0.01}\) & \(0.623_{\pm0.004}\) & \(1.01_{\pm0.12}\) \\
WCE & \(0.715_{\pm0.006}\) & \(0.826_{\pm0.003}\) & \(2.18_{\pm0.53}\) & \(0.505_{\pm0.055}\) & \(1.50_{\pm0.23}\) & \(0.713_{\pm0.006}\) & \(0.826_{\pm0.005}\) & \(1.80_{\pm0.04}\) & \(0.546_{\pm0.014}\) & \(1.59_{\pm0.15}\) \\
ABW & \(0.637_{\pm0.051}\) & \(0.769_{\pm0.039}\) & \(2.56_{\pm0.59}\) & \(0.415_{\pm0.085}\) & \(2.83_{\pm1.51}\) & \(0.639_{\pm0.036}\) & \(0.769_{\pm0.028}\) & \(2.62_{\pm0.45}\) & \(0.410_{\pm0.065}\) & \(2.88_{\pm0.80}\) \\
Skea-topo & \(0.658_{\pm0.002}\) & \(0.785_{\pm0.002}\) & \(2.34_{\pm0.00}\) & \(0.449_{\pm0.000}\) & \(1.62_{\pm0.21}\) & \(0.657_{\pm0.005}\) & \(0.784_{\pm0.004}\) & \(2.42_{\pm0.04}\) & \(0.446_{\pm0.007}\) & \(1.47_{\pm0.00}\) \\
RMI & \(0.767_{\pm0.009}\) & \(0.861_{\pm0.007}\) & \(1.46_{\pm0.17}\) & \(0.587_{\pm0.038}\) & \(\underline{0.82_{\pm0.06}}\) & \(0.763_{\pm0.009}\) & \(0.858_{\pm0.007}\) & \(1.46_{\pm0.17}\) & \(0.577_{\pm0.031}\) & \(0.94_{\pm0.09}\) \\
clDice & \(0.706_{\pm0.023}\) & \(0.819_{\pm0.018}\) & \(2.26_{\pm0.41}\) & \(0.476_{\pm0.068}\) & \(1.56_{\pm0.28}\) & \(0.690_{\pm0.014}\) & \(0.807_{\pm0.010}\) & \(1.97_{\pm0.15}\) & \(0.498_{\pm0.023}\) & \(1.82_{\pm0.15}\) \\

\hline

CWMI-Real & \(\underline{0.774_{\pm0.005}}\) & \(\underline{0.866_{\pm0.004}}\) & \(\underline{1.19_{\pm0.10}}\) & \(\underline{0.634_{\pm0.018}}\) & \(0.91_{\pm0.20}\) & \(\underline{0.775_{\pm0.005}}\) & \(\underline{0.867_{\pm0.004}}\) & \(\mathbf{1.16_{\pm0.07}}\) & \(\mathbf{0.638_{\pm0.010}}\) & \(\underline{0.80_{\pm0.09}}\) \\
CWMI & \(\mathbf{0.779_{\pm0.004}}\) & \(\mathbf{0.869_{\pm0.003}}\) & \(\mathbf{1.16_{\pm0.09}}\) & \(\mathbf{0.640_{\pm0.005}}\) & \(\mathbf{0.69_{\pm0.04}}\) & \(\mathbf{0.778_{\pm0.009}}\) & \(\mathbf{0.869_{\pm0.007}}\) & \(\underline{1.19_{\pm0.15}}\) & \(\underline{0.637_{\pm0.034}}\) & \(\mathbf{0.78_{\pm0.08}}\) \\

        \Xhline{1pt}
        \multicolumn{11}{c}{\textbf{GlaS}}\\
        \hline
        ~&~&~&UNet&~&~&
        ~&~&AttenUNet&~ &~\\
Methods  & 
mIoU$\uparrow$ & mDice$\uparrow$ & VI$\downarrow$ & ARI$\uparrow$ & HD$\downarrow$ & 
mIoU$\uparrow$ & mDice$\uparrow$ & VI$\downarrow$ & ARI$\uparrow$ & HD$\downarrow$\\
        \hline
CE & \(0.808_{\pm0.009}\) & \(0.889_{\pm0.007}\) & \(0.99_{\pm0.02}\) & \(0.686_{\pm0.026}\) & \(3.13_{\pm0.65}\) & \(0.811_{\pm0.009}\) & \(0.889_{\pm0.005}\) & \(1.01_{\pm0.10}\) & \(0.690_{\pm0.028}\) & \(3.44_{\pm0.03}\) \\
BCE & \(0.682_{\pm0.038}\) & \(0.798_{\pm0.031}\) & \(1.82_{\pm0.57}\) & \(0.309_{\pm0.154}\) & \(10.87_{\pm5.26}\) & \(0.699_{\pm0.045}\) & \(0.814_{\pm0.032}\) & \(1.48_{\pm0.55}\) & \(0.398_{\pm0.137}\) & \(9.77_{\pm5.23}\) \\
Dice & \(0.816_{\pm0.019}\) & \(0.894_{\pm0.014}\) & \(0.94_{\pm0.06}\) & \(0.699_{\pm0.019}\) & \(3.01_{\pm0.87}\) & \(0.817_{\pm0.019}\) & \(0.895_{\pm0.013}\) & \(0.91_{\pm0.15}\) & \(0.704_{\pm0.031}\) & \(2.70_{\pm0.59}\) \\
Focal & \(0.583_{\pm0.080}\) & \(0.715_{\pm0.075}\) & \(2.31_{\pm0.20}\) & \(0.137_{\pm0.105}\) & \(16.50_{\pm9.43}\) & \(0.588_{\pm0.032}\) & \(0.719_{\pm0.028}\) & \(2.15_{\pm0.25}\) & \(0.151_{\pm0.037}\) & \(23.18_{\pm5.86}\) \\
Jaccard & \(0.803_{\pm0.024}\) & \(0.886_{\pm0.016}\) & \(1.02_{\pm0.12}\) & \(0.675_{\pm0.050}\) & \(3.30_{\pm0.63}\) & \(0.812_{\pm0.010}\) & \(0.892_{\pm0.006}\) & \(0.92_{\pm0.03}\) & \(0.693_{\pm0.019}\) & \(3.10_{\pm0.54}\) \\
Tversky & \(0.804_{\pm0.024}\) & \(0.887_{\pm0.016}\) & \(0.97_{\pm0.13}\) & \(0.670_{\pm0.039}\) & \(3.18_{\pm0.70}\) & \(0.814_{\pm0.024}\) & \(0.892_{\pm0.016}\) & \(0.93_{\pm0.07}\) & \(0.698_{\pm0.047}\) & \(3.34_{\pm1.26}\) \\
WCE & \(0.829_{\pm0.010}\) & \(0.902_{\pm0.007}\) & \(0.83_{\pm0.07}\) & \(0.721_{\pm0.008}\) & \(\mathbf{2.26_{\pm0.21}}\) & \(0.823_{\pm0.020}\) & \(0.898_{\pm0.014}\) & \(0.87_{\pm0.13}\) & \(0.705_{\pm0.026}\) & \(\mathbf{2.47_{\pm0.61}}\) \\
ABW & \(0.765_{\pm0.011}\) & \(0.860_{\pm0.008}\) & \(1.43_{\pm0.03}\) & \(0.621_{\pm0.033}\) & \(3.65_{\pm0.72}\) & \(0.766_{\pm0.011}\) & \(0.862_{\pm0.008}\) & \(1.49_{\pm0.11}\) & \(0.614_{\pm0.018}\) & \(3.20_{\pm0.58}\) \\
Skea-topo & \(0.788_{\pm0.008}\) & \(0.876_{\pm0.005}\) & \(1.26_{\pm0.04}\) & \(0.658_{\pm0.002}\) & \(2.99_{\pm0.76}\) & \(0.784_{\pm0.016}\) & \(0.873_{\pm0.010}\) & \(1.30_{\pm0.09}\) & \(0.645_{\pm0.022}\) & \(3.36_{\pm0.61}\) \\
RMI & \(\underline{0.839_{\pm0.003}}\) & \(\underline{0.908_{\pm0.002}}\) & \(0.82_{\pm0.06}\) & \(\underline{0.733_{\pm0.003}}\) & \(2.50_{\pm0.13}\) & \(\underline{0.842_{\pm0.001}}\) & \(\underline{0.910_{\pm0.001}}\) & \(0.83_{\pm0.04}\) & \(\underline{0.732_{\pm0.006}}\) & \(2.58_{\pm0.61}\) \\
clDice & \(0.818_{\pm0.010}\) & \(0.895_{\pm0.007}\) & \(0.89_{\pm0.05}\) & \(0.696_{\pm0.020}\) & \(2.70_{\pm0.17}\) & \(0.803_{\pm0.015}\) & \(0.886_{\pm0.011}\) & \(0.96_{\pm0.07}\) & \(0.687_{\pm0.025}\) & \(3.10_{\pm0.42}\) \\

\hline

CWMI-Real & \(0.835_{\pm0.008}\) & \(0.905_{\pm0.004}\) & \(\underline{0.80_{\pm0.03}}\) & \(0.724_{\pm0.010}\) & \(2.74_{\pm0.60}\) & \(0.842_{\pm0.022}\) & \(0.909_{\pm0.015}\) & \(\underline{0.79_{\pm0.06}}\) & \(0.724_{\pm0.038}\) & \(2.76_{\pm0.76}\) \\
CWMI & \(\mathbf{0.845_{\pm0.013}}\) & \(\mathbf{0.911_{\pm0.010}}\) & \(\mathbf{0.73_{\pm0.04}}\) & \(\mathbf{0.745_{\pm0.017}}\) & \(\underline{2.47_{\pm0.42}}\) & \(\mathbf{0.844_{\pm0.007}}\) & \(\mathbf{0.911_{\pm0.005}}\) & \(\mathbf{0.75_{\pm0.11}}\) & \(\mathbf{0.737_{\pm0.015}}\) & \(\underline{2.57_{\pm0.55}}\) \\


        \Xhline{1pt}
        \multicolumn{11}{c}{\textbf{DRIVE}}\\
        \hline
        ~&~&~&UNet&~&~&
        ~&~&AttenUNet&~ &~\\
Methods  & 
mIoU$\uparrow$ & mDice$\uparrow$ & VI$\downarrow$ & ARI$\uparrow$ & HD$\downarrow$ & 
mIoU$\uparrow$ & mDice$\uparrow$ & VI$\downarrow$ & ARI$\uparrow$ & HD$\downarrow$\\
        \hline
CE & \(0.770_{\pm0.020}\) & \(0.856_{\pm0.017}\) & \(1.39_{\pm0.19}\) & \(0.397_{\pm0.091}\) & \(2.37_{\pm0.51}\) & \(0.754_{\pm0.020}\) & \(0.843_{\pm0.017}\) & \(1.43_{\pm0.19}\) & \(0.381_{\pm0.067}\) & \(2.90_{\pm1.10}\) \\
BCE & \(0.742_{\pm0.004}\) & \(0.835_{\pm0.003}\) & \(1.53_{\pm0.19}\) & \(0.372_{\pm0.064}\) & \(2.50_{\pm0.36}\) & \(0.750_{\pm0.016}\) & \(0.841_{\pm0.013}\) & \(1.44_{\pm0.13}\) & \(0.417_{\pm0.050}\) & \(2.21_{\pm0.14}\) \\
Dice & \(0.775_{\pm0.027}\) & \(0.860_{\pm0.020}\) & \(1.32_{\pm0.17}\) & \(0.458_{\pm0.055}\) & \(1.85_{\pm0.14}\) & \(0.766_{\pm0.009}\) & \(0.852_{\pm0.006}\) & \(1.38_{\pm0.19}\) & \(0.421_{\pm0.117}\) & \(2.70_{\pm0.89}\) \\
Focal & \(0.734_{\pm0.004}\) & \(0.829_{\pm0.002}\) & \(1.40_{\pm0.12}\) & \(0.475_{\pm0.122}\) & \(2.20_{\pm0.47}\) & \(0.740_{\pm0.015}\) & \(0.833_{\pm0.012}\) & \(1.39_{\pm0.17}\) & \(0.480_{\pm0.079}\) & \(2.02_{\pm0.47}\) \\
Jaccard & \(0.760_{\pm0.011}\) & \(0.849_{\pm0.010}\) & \(1.29_{\pm0.27}\) & \(0.492_{\pm0.138}\) & \(2.20_{\pm0.94}\) & \(0.768_{\pm0.013}\) & \(0.856_{\pm0.010}\) & \(1.25_{\pm0.15}\) & \(0.533_{\pm0.069}\) & \(\underline{1.55_{\pm0.25}}\) \\
Tversky & \(0.754_{\pm0.028}\) & \(0.845_{\pm0.022}\) & \(1.29_{\pm0.07}\) & \(0.526_{\pm0.066}\) & \(1.54_{\pm0.47}\) & \(0.772_{\pm0.017}\) & \(0.859_{\pm0.013}\) & \(1.26_{\pm0.07}\) & \(0.523_{\pm0.015}\) & \(1.65_{\pm0.36}\) \\
WCE & \(0.735_{\pm0.014}\) & \(0.829_{\pm0.012}\) & \(1.44_{\pm0.10}\) & \(0.454_{\pm0.019}\) & \(2.30_{\pm0.70}\) & \(0.732_{\pm0.012}\) & \(0.828_{\pm0.010}\) & \(1.45_{\pm0.03}\) & \(0.455_{\pm0.045}\) & \(2.04_{\pm0.51}\) \\
ABW & - & - & - & - & - & - & - & - & - & - \\
Skea-topo & - & - & - & - & - & \(0.669_{\pm0.008}\) & \(0.772_{\pm0.014}\) & \(1.57_{\pm0.14}\) & \(0.397_{\pm0.140}\) & \(5.79_{\pm4.49}\) \\
RMI & \(\underline{0.786_{\pm0.015}}\) & \(\underline{0.868_{\pm0.011}}\) & \(1.31_{\pm0.14}\) & \(0.436_{\pm0.073}\) & \(1.83_{\pm0.27}\) & \(\mathbf{0.786_{\pm0.002}}\) & \(\mathbf{0.868_{\pm0.002}}\) & \(1.25_{\pm0.08}\) & \(0.467_{\pm0.024}\) & \(1.75_{\pm0.20}\) \\
clDice & \(0.466_{\pm0.072}\) & \(0.575_{\pm0.055}\) & \(2.05_{\pm0.13}\) & \(0.111_{\pm0.089}\) & \(14.28_{\pm10.08}\) & \(0.468_{\pm0.048}\) & \(0.577_{\pm0.052}\) & \(1.97_{\pm0.10}\) & \(0.181_{\pm0.145}\) & \(17.52_{\pm12.82}\) \\
\hline
CWMI-Real & \(0.774_{\pm0.018}\) & \(0.861_{\pm0.013}\) & \(\underline{1.12_{\pm0.18}}\) & \(\underline{0.590_{\pm0.115}}\) & \(\underline{1.45_{\pm0.31}}\) & \(\underline{0.782_{\pm0.050}}\) & \(\underline{0.865_{\pm0.038}}\) & \(\underline{1.13_{\pm0.16}}\) & \(\underline{0.566_{\pm0.038}}\) & \(1.59_{\pm0.91}\) \\
CWMI & \(\mathbf{0.801_{\pm0.011}}\) & \(\mathbf{0.880_{\pm0.008}}\) & \(\mathbf{1.06_{\pm0.08}}\) & \(\mathbf{0.596_{\pm0.032}}\) & \(\mathbf{1.07_{\pm0.13}}\) & \(0.778_{\pm0.026}\) & \(0.863_{\pm0.019}\) & \(\mathbf{1.01_{\pm0.05}}\) & \(\mathbf{0.641_{\pm0.024}}\) & \(\mathbf{1.48_{\pm0.47}}\) \\

\Xhline{1pt}
\multicolumn{11}{c}{\textbf{MASS ROAD}}\\
        \hline
        ~&~&~&UNet&~&~&
        ~&~&AttenUNet&~ &~\\
Methods  & 
mIoU$\uparrow$ & mDice$\uparrow$ & VI$\downarrow$ & ARI$\uparrow$ & HD$\downarrow$ & 
mIoU$\uparrow$ & mDice$\uparrow$ & VI$\downarrow$ & ARI$\uparrow$ & HD$\downarrow$\\
        \hline
CE & \(0.730_{\pm0.020}\) & \(0.823_{\pm0.018}\) & \(3.80_{\pm0.63}\) & \(0.148_{\pm0.087}\) & \(11.32_{\pm4.41}\) & \(0.739_{\pm0.021}\) & \(0.831_{\pm0.016}\) & \(2.94_{\pm0.92}\) & \(0.280_{\pm0.155}\) & \(10.64_{\pm0.98}\) \\
BCE & \(0.684_{\pm0.008}\) & \(0.790_{\pm0.008}\) & \(1.66_{\pm0.30}\) & \(0.575_{\pm0.058}\) & \(13.01_{\pm3.64}\) & \(0.691_{\pm0.002}\) & \(0.796_{\pm0.002}\) & \(1.63_{\pm0.19}\) & \(0.577_{\pm0.049}\) & \(13.00_{\pm4.04}\) \\
Dice & \(0.762_{\pm0.013}\) & \(0.850_{\pm0.011}\) & \(1.58_{\pm0.15}\) & \(0.560_{\pm0.049}\) & \(11.39_{\pm6.13}\) & \(0.760_{\pm0.010}\) & \(0.848_{\pm0.009}\) & \(1.64_{\pm0.20}\) & \(0.550_{\pm0.038}\) & \(11.61_{\pm5.15}\) \\
Focal & \(0.677_{\pm0.013}\) & \(0.785_{\pm0.011}\) & \(1.80_{\pm0.18}\) & \(0.540_{\pm0.055}\) & \(11.80_{\pm2.09}\) & \(0.678_{\pm0.017}\) & \(0.786_{\pm0.014}\) & \(1.74_{\pm0.13}\) & \(0.550_{\pm0.043}\) & \(12.76_{\pm2.50}\) \\
Jaccard & \(0.759_{\pm0.009}\) & \(0.848_{\pm0.007}\) & \(1.34_{\pm0.03}\) & \(0.629_{\pm0.005}\) & \(11.22_{\pm4.14}\) & \(0.753_{\pm0.007}\) & \(0.844_{\pm0.006}\) & \(1.36_{\pm0.12}\) & \(0.626_{\pm0.016}\) & \(11.65_{\pm7.52}\) \\
Tversky & \(0.753_{\pm0.010}\) & \(0.844_{\pm0.008}\) & \(1.35_{\pm0.25}\) & \(0.628_{\pm0.078}\) & \(11.61_{\pm5.02}\) & \(0.749_{\pm0.003}\) & \(0.841_{\pm0.003}\) & \(1.56_{\pm0.43}\) & \(0.581_{\pm0.101}\) & \(11.76_{\pm2.12}\) \\
WCE & \(0.651_{\pm0.011}\) & \(0.763_{\pm0.010}\) & \(1.71_{\pm0.08}\) & \(0.556_{\pm0.003}\) & \(\underline{10.94_{\pm0.37}}\) & \(0.649_{\pm0.008}\) & \(0.761_{\pm0.007}\) & \(1.82_{\pm0.14}\) & \(0.527_{\pm0.046}\) & \(12.37_{\pm2.06}\) \\
ABW & \(0.688_{\pm0.011}\) & \(0.794_{\pm0.009}\) & \(1.61_{\pm0.11}\) & \(0.580_{\pm0.034}\) & \(11.80_{\pm3.22}\) & \(0.680_{\pm0.005}\) & \(0.788_{\pm0.004}\) & \(1.66_{\pm0.10}\) & \(0.568_{\pm0.035}\) & \(\underline{10.47_{\pm2.00}}\) \\
Skea-topo & \(0.620_{\pm0.095}\) & \(0.734_{\pm0.085}\) & \(2.24_{\pm0.82}\) & \(0.463_{\pm0.159}\) & \(11.65_{\pm5.62}\) & \(0.673_{\pm0.005}\) & \(0.780_{\pm0.004}\) & \(2.02_{\pm0.26}\) & \(0.500_{\pm0.058}\) & \(11.28_{\pm1.14}\) \\
RMI & \(\underline{0.764_{\pm0.002}}\) & \(\underline{0.851_{\pm0.001}}\) & \(1.66_{\pm0.14}\) & \(0.541_{\pm0.024}\) & \(11.57_{\pm2.94}\) & \(\underline{0.762_{\pm0.006}}\) & \(\underline{0.849_{\pm0.003}}\) & \(1.92_{\pm0.45}\) & \(0.474_{\pm0.093}\) & \(11.68_{\pm4.24}\) \\
clDice & \(0.653_{\pm0.060}\) & \(0.762_{\pm0.053}\) & \(2.28_{\pm0.63}\) & \(0.459_{\pm0.103}\) & \(15.55_{\pm4.59}\) & \(0.683_{\pm0.038}\) & \(0.788_{\pm0.031}\) & \(1.74_{\pm0.21}\) & \(0.559_{\pm0.034}\) & \(13.54_{\pm1.88}\) \\
\hline
CWMI-Real & \(0.761_{\pm0.012}\) & \(0.850_{\pm0.010}\) & \(\underline{1.25_{\pm0.29}}\) & \(\underline{0.653_{\pm0.069}}\) & \(\mathbf{10.76_{\pm2.17}}\) & \(0.759_{\pm0.012}\) & \(0.849_{\pm0.009}\) & \(\underline{1.25_{\pm0.10}}\) & \(\underline{0.647_{\pm0.027}}\) & \(10.90_{\pm3.70}\) \\
CWMI & \(\mathbf{0.773_{\pm0.007}}\) & \(\mathbf{0.859_{\pm0.005}}\) & \(\mathbf{1.15_{\pm0.23}}\) & \(\mathbf{0.664_{\pm0.071}}\) & \(11.17_{\pm7.44}\) & \(\mathbf{0.770_{\pm0.011}}\) & \(\mathbf{0.856_{\pm0.009}}\) & \(\mathbf{1.18_{\pm0.10}}\) & \(\mathbf{0.657_{\pm0.029}}\) & \(\mathbf{10.37_{\pm7.02}}\) \\


         \Xhline{1pt}
    \end{tabular}
    \caption{Quantitative results of different loss functions across the four datasets and two neural network models. The \textbf{bold} numbers indicate the best performance for each metric, while the \underline{underlined} numbers denote the second-best performance. A hyphen ("-") indicates cases where the model did not converge.}
    \label{tab:1}
\end{table*}

\vspace{-10pt}
\section{Experiment}

\subsection{Experimental Setup}
\paragraph{Base Models}
To evaluate CWMI loss, we employed U-Net \cite{ronneberger2015u} and Attention U-Net \cite{oktay2018attention} as base models. This choice allows us to assess CWMI's generalization across fully convolutional models and attention-based models.
\vspace{-10pt}
\paragraph{Datasets}
We tested CWMI on three public segmentation datasets, all characterized by class and instance imbalance: (1) SNEMI3D, a neurite segmentation dataset containing 100 \(1024 \times 1024\) grayscale images from electron microscopy slices \cite{arganda2013snemi3d}; (2) GlaS, a gland segmentation dataset with 165 RGB images of varying sizes from histological images of colorectal cancer samples \cite{sirinukunwattana2017gland}; (3) DRIVE, a retinal vessel segmentation dataset comprising 40 \(584 \times 565\) RGB images from fundus photographs \cite{staal2004ridge}; and (4) the Massachusetts Roads dataset (MASS ROAD), a road segmentation dataset with 1171 \(1500\times 1500\) RGB images from aerial imagery \cite{MnihThesis}. We choose a subset of 120 images (ignoring images without a network of roads). For all datasets, three-fold cross-validation was used to ensure robust evaluation.
\vspace{-10pt}
\paragraph{Baselines and Implementation Details}
We compared CWMI against 11 state-of-the-art (SOTA) loss functions, including pixel-wise loss functions (e.g., cross entropy, BCE \cite{long2015fully}, Focal loss \cite{ross2017focal}), region-based loss functions (e.g., Dice loss \cite{milletari2016v}, Tversky loss \cite{salehi2017tversky}, Jaccard loss \cite{rahman2016optimizing}), and structural/topological loss functions (e.g., WCE \cite{ronneberger2015u}, ABW loss \cite{liu2022boundary}, Skea-topo loss \cite{liu2024enhancing}, RMI loss \cite{zhao2019region}, clDice loss \cite{shit2021cldice}). Hyperparameters for each baseline were tuned via grid search, with Tversky loss (\(\alpha = 0.5, \beta = 0.5\)) and Focal loss (\(\gamma = 2.5\)) as examples. 

We utilized steerable pyramids with four decomposition levels and four orientations in all experiments. A regularization parameter of \(\lambda=0.1\) was applied in all CWMI experiments. For the implementation of Equation \ref{eq:10}, although PyTorch supports complex matrix calculations, our experiments indicated that its efficiency remains suboptimal. Consequently, we computed using their real representations, which are mathematically equivalent \cite{golub2013matrix}. To assess the significance of the complex steerable pyramid, we compared CWMI with a real-number-only variant (CWMI-Real), implemented according to Equations \ref{G real} and \ref{eq:9}. Adam optimizer with a StepLR scheduler (initial learning rate \(1 \times 10^{-4}\), decay rate 0.8, step size 10) was used, and models were trained for 50 epochs with a batch size of 10. Early stopping based on mIoU was employed to select the best model. Training was conducted on an NVIDIA A100 GPU using the Google Colab runtime.
\vspace{-10pt}
\paragraph{Data Augmentation and Evaluation Metrics}
Random flips and rotations were applied to all datasets to improve generalization. For SNEMI3D and MASS ROAD, images were randomly cropped to \(512 \times 512\), while for GlaS, images were cropped to \(448 \times 576\) to standardize input sizes. No cropping was performed for DRIVE due to its uniform image dimensions.

\vspace{-5pt}

Performance was evaluated using five metrics: mIoU and mDice for regional precision, variation of information (VI) \cite{nunez2013machine} and adjusted Rand index (ARI) \cite{vinh2009information} for clustering precision, and Hausdorff distance (HD) for boundary and topological accuracy. These metrics provide a comprehensive assessment of both regional overlap and structural fidelity.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{Figure_3.PNG}
    \vspace{-20pt}
    \caption{Qualitative results of different loss functions on the SNEMI3D dataset. \textcolor{red}{Red: false positive regions;} \textcolor{blue}{Blue: false negative regions.} \textcolor{ForestGreen}{Green arrow: challenging false positive} and \textcolor{orange}{Organge arrow: challenging false negative} that are successfully addressed by CWMI.}
    \label{fig:3}
\end{figure}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{Figure_4.PNG}
    \vspace{-20pt}
    \caption{Qualitative results of different loss functions on the GlaS dataset. }
    \label{fig:4}
\end{figure}
\vspace{-5pt}
\subsection{Quantitative and qualitative results}
As shown in Table \ref{tab:1}, the proposed CWMI loss outperforms other loss functions in the majority of metrics for all datasets across U-Net and Attention U-Net models. Furthermore, CWMI consistently outperformed the real CWMI variant, highlighting the importance of incorporating phase information into the steerable pyramid decomposition. Qualitative results from SNEMI3D (Figure \ref{fig:3}), GlaS (Figure \ref{fig:4}), DRIVE (Figure \ref{fig:5}), and MASS ROAD (Figrue \ref{fig:6}) further demonstrate the effectiveness of complex CWMI in addressing challenging false positive and false negative segmentation issues that remain unresolved by other state-of-the-art loss functions.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figure_5.PNG}
    \vspace{-20pt}
    \caption{Qualitative results of different loss functions on the DRIVE dataset.}
    \label{fig:5}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{Figure_6.PNG}
    \vspace{-20pt}
    \caption{Qualitative results of different loss functions on the MASS ROAD dataset.}
    \label{fig:6}
\end{figure}

\subsection{Ablation studies}
In this section, all ablation experiments were performed on the SNEMI3D dataset with U-Net model.
\vspace{-10pt}
\paragraph{Mutual Information (MI) vs \(\mathbf{L_1}\), \(\mathbf{L_2}\) Distance and Structural Similarity (SSIM)} As previously discussed, various metrics based on wavelet transforms have been developed for evaluating image similarity and guiding loss functions. To assess the advantages of mutual information (MI) over commonly used metrics such as \(L_1\), \(L_2\), and SSIM, we conducted a comparative analysis. The experimental results, summarized in Table \ref{tab:2}, demonstrate that MI consistently outperforms \(L_1\), \(L_2\) distance, and SSIM across multiple evaluation metrics. Unlike \(L_1\) and \(L_2\), which focus on pixel-wise intensity differences, and SSIM, which emphasizes structural similarity, MI captures joint statistical dependencies between features in each direction. This ability provides a more robust representation of structural differences between predictions and labels, particularly in complex segmentation tasks.

\begin{table}[h]
\scriptsize
    \centering
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{lccccc}
        \Xhline{1pt}
Methods  & 
mIoU$\uparrow$ & mDice$\uparrow$ & VI$\downarrow$ & ARI$\uparrow$ & HD$\downarrow$ \\
        \hline
L1 & \(0.773_{\pm0.002}\) & \(0.866_{\pm0.001}\) & \(1.22_{\pm0.05}\) & \(0.632_{\pm0.007}\) & \(0.70_{\pm0.08}\) \\
L2 & \(0.774_{\pm0.003}\) & \(0.867_{\pm0.002}\) & \(1.23_{\pm0.06}\) & \(0.633_{\pm0.011}\) & \(0.76_{\pm0.13}\) \\
SSIM & \(0.468_{\pm0.032}\) & \(0.613_{\pm0.020}\) & \(5.96_{\pm0.96}\) & \(0.081_{\pm0.030}\) & \(9.31_{\pm1.64}\) \\
CWMI & \(\mathbf{0.779_{\pm0.004}}\) & \(\mathbf{0.870_{\pm0.003}}\) & \(\mathbf{1.16_{\pm0.09}}\) & \(\mathbf{0.640_{\pm0.005}}\) & \(\mathbf{0.69_{\pm0.04}}\) \\
\Xhline{1pt}
\end{tabular}
    \caption{Quantitative results comparing mutual information (MI), \(L_1\), \(L_2\) distance, and structural similarity (SSIM) based on U-Net with SNEMI3D. The \textbf{bold} numbers indicate the best performance for each metric.}
    \label{tab:2}
\end{table}

\vspace{-20pt}
\paragraph{Impact of Decomposition Level \(N\) and Number of Orientations \(K\)} 
Theoretically, the steerable pyramid can decompose an image into very high levels with an infinite number of orientations, provided the input image is sufficiently large. However, does deeper decomposition or a higher number of orientations improve feature extraction and loss computation? To address this, we analyzed the impact of \(N\) (decomposition level) and \(K\) (number of orientations) on the performance of CWMI, as shown in Table \ref{tab:3}. Interestingly, both \(N\) and \(K\) achieved optimal performance at relatively low values, suggesting that the critical information for segmentation is concentrated in relatively high-frequency regions. This observation supports the idea that emphasizing high-frequency subbands refines small instances and narrow boundaries, while lower-frequency components from deeper decompositions contribute less informative features compared to higher-frequency ones.

Knowing that the first four layers of decomposition are crucial, we further performed a layer ablation experiment (Table \ref{tab:3}, Layer Ablation), where only selected layers were used to compute mutual information while discarding the remaining layers. Our results show that the third layer outperforms other layers in region-based (mIoU, mDice) and cluster-based metrics (VI, ARI), while the fourth layer achieves the best performance in topological metrics (HD). These findings suggest that features extracted from multiple layers are essential for achieving high performance across all evaluation metrics, highlighting the importance of incorporating information from both mid- and high-frequency subbands in the segmentation task.

\begin{table}[h]
\scriptsize
    \centering
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{lccccc}
        \Xhline{1pt}
        \multicolumn{6}{c}{N=4}\\
        \hline
~  & 
mIoU$\uparrow$ & mDice$\uparrow$ & VI$\downarrow$ & ARI$\uparrow$ & HD$\downarrow$ \\
\hline
K=2 & \(0.777_{\pm0.006}\) & \(0.868_{\pm0.004}\) & \(1.18_{\pm0.10}\) & \(0.637_{\pm0.028}\) & \(0.79_{\pm0.02}\) \\
K=4 & \(\mathbf{0.779_{\pm0.004}}\) & \(\mathbf{0.870_{\pm0.003}}\) & \(\mathbf{1.16_{\pm0.09}}\) & \(\mathbf{0.640_{\pm0.005}}\) & \(\mathbf{0.69_{\pm0.04}}\) \\
K=8 & \(0.776_{\pm0.003}\) & \(0.868_{\pm0.002}\) & \(1.20_{\pm0.06}\) & \(0.635_{\pm0.009}\) & \(0.76_{\pm0.08}\) \\
K=12 & \(0.772_{\pm0.005}\) & \(0.865_{\pm0.003}\) & \(1.33_{\pm0.05}\) & \(0.621_{\pm0.004}\) & \(0.81_{\pm0.08}\) \\
\Xhline{1pt}
        \multicolumn{6}{c}{K=4}\\
        \hline
~  & 
mIoU$\uparrow$ & mDice$\uparrow$ & VI$\downarrow$ & ARI$\uparrow$ & HD$\downarrow$ \\
\hline
N=2 & \(0.760_{\pm0.002}\) & \(0.857_{\pm0.001}\) & \(1.30_{\pm0.04}\) & \(0.618_{\pm0.009}\) & \(1.20_{\pm0.12}\) \\
N=4 & \(\mathbf{0.777_{\pm0.002}}\) & \(\mathbf{0.869_{\pm0.001}}\) & \(\mathbf{1.21_{\pm0.10}}\) & \(\mathbf{0.634_{\pm0.017}}\) & \(\mathbf{0.79_{\pm0.190}}\) \\
N=6 & \(0.768_{\pm0.003}\) & \(0.862_{\pm0.002}\) & \(1.29_{\pm0.05}\) & \(0.618_{\pm0.004}\) & \(0.88_{\pm0.18}\) \\
\Xhline{1pt}
        \multicolumn{6}{c}{Layer ablation test}\\
        \hline
~  & 
mIoU$\uparrow$ & mDice$\uparrow$ & VI$\downarrow$ & ARI$\uparrow$ & HD$\downarrow$ \\
\hline
1st layer & \(0.738_{\pm0.008}\) & \(0.843_{\pm0.006}\) & \(1.61_{\pm0.07}\) & \(0.576_{\pm0.015}\) & \(1.38_{\pm0.14}\) \\
2nd layer & \(0.762_{\pm0.008}\) & \(0.858_{\pm0.006}\) & \(1.25_{\pm0.08}\) & \(0.624_{\pm0.016}\) & \(1.12_{\pm0.19}\) \\
3rd layer & \(\mathbf{0.774_{\pm0.003}}\) & \(\mathbf{0.867_{\pm0.002}}\) & \(\mathbf{1.23_{\pm0.10}}\) & \(\mathbf{0.638_{\pm0.020}}\) & \(0.82_{\pm0.12}\) \\
4th layer & \(0.766_{\pm0.004}\) & \(0.861_{\pm0.003}\) & \(1.39_{\pm0.09}\) & \(0.614_{\pm0.012}\) & \(\mathbf{0.76_{\pm0.04}}\) \\

\Xhline{1pt}
\end{tabular}
    \caption{The impact of the decomposition level N and orientation number K on CWMI performance on U-Net with SNEMI3D. In the layer ablation test, only the selected layer is computed for mutual information and all other layers are discard. The \textbf{bold} numbers indicate the best performance for each metric.}
    \label{tab:3}
\end{table}

\vspace{-10pt}
\paragraph{Computational complexity analysis} 
For an input image of size \( H \times W \) with \( K \) orientation decompositions, the computational complexity of the CWMI loss function is analyzed as follows.

First, the forward Fourier transform has a time complexity of \(O(HW \log(HW))\). For the first layer of decomposition, the operations include:
Band-pass filtering: \( O(HW K) \);
Inverse Fourier transform: \( O(HW \log(HW) K) \);
and mutual information computation: \( O(HW K^2) \).
Summing these terms, the total complexity for the first decomposition layer is:
\(
O(HW (K \log(HW) + K^2))
\)
For subsequent decomposition layers, the image size reduces by a factor of four at each step, meaning the second layer processes an image of size \( HW / 4 \), and the third layer processes \( HW / 16 \), and so on. Thus, the total computational complexity follows a geometric series with a superior bound 
\(
O\left(\frac{4}{3} HW (K \log(HW) + K^2) \right)
\)

From our ablation experiments, the optimal number of orientations is \( K = 4 \), which is relatively small. Hence the complexity of CWMI is linear to 
\(
O(HW \log(HW))
\), which remains scalable for high-resolution images. Compared to topology-aware losses such as RMI, clDice, or Hausdorff Distance Loss, CWMI achieves better structural and boundary preservation with lower computational overhead, as shown in Table \ref{tab:4}.

\begin{table}[h]
\scriptsize
    \centering
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{lcc}
        \Xhline{1pt}
        ~ & Epoch Time (s) & \(\Delta t\) to CE (s) \\
        \hline
        CE & 2.04 & 0.00 \\
BCE & 2.02 & -0.02 \\
Dice & 2.01 & -0.03 \\
Focal & 2.01 & -0.03 \\
Jaccard & 2.02 & -0.02 \\
Tversky & 2.01 & -0.03 \\
WCE & 2.15 & 0.12 \\
ABW & 2.33 & 0.30 \\
Skea-topo & 2.72 & 0.69 \\
RMI & 2.28 & 0.25 \\
clDice & 2.37 & 0.34 \\
\hline
CWMI-Real & 2.18 & 0.15 \\
CWMI & 2.27 & 0.23 \\
        \Xhline{1pt}
    \end{tabular}
    \caption{Training time per epoch and relative change to CE baseline for various loss functions on U-Net model with SNEMI3D dataset.}
    \label{tab:4}
\end{table}

\vspace{-20pt}
\section{Conclusion}

In this study, we introduced Complex Wavelet Mutual Information (CWMI) loss, a novel loss function for semantic segmentation that leverages the multiscale, multi-orientation decomposition capabilities of the complex steerable pyramid. By integrating mutual information across wavelet subbands, CWMI effectively captures high-dimensional dependencies and local structural features, including critical phase information, which are essential for accurate segmentation. Extensive experiments on four challenging datasets (SNEMI3D, GlaS, DRIVE, and MASS ROAD) demonstrate that CWMI consistently outperforms state-of-the-art loss functions across most evaluation metrics, particularly in segmenting small instances and narrow boundaries, while introducing minimal computational overhead. These results highlight CWMI as a robust and versatile loss function that effectively addresses key challenges in segmentation, such as class and instance imbalance, boundary precision, and topological consistency.

Beyond semantic segmentation, the core principles of CWMI—multiscale feature extraction and structural consistency—suggest its potential applicability to a broader range of computer vision and machine learning tasks, such as image-to-image translation and super-resolution, which we leave for future exploration.
\vspace{-10pt}
\paragraph{Limitations and Future Work} 
Although extensively studied on 2D images, extending it to multi-class and 3D segmentation is theoretically feasible but necessitates further validation in future research. 



\section*{Impact Statement}

The proposed Complex Wavelet Mutual Information (CWMI) loss introduces a novel approach to structural-aware learning in deep neural networks. By leveraging multiscale decomposition through the complex steerable pyramid and mutual information across frequency subbands, CWMI enables improved segmentation performance, particularly for small-scale structures and thin boundaries. Our empirical results demonstrate significant improvements in both pixel-wise and topological accuracy across multiple datasets. Beyond segmentation, CWMI has the potential to generalize to a wide range of real-world applications, including medical imaging, autonomous driving, satellite-based environmental monitoring, and industrial defect detection. 

Overall, CWMI provides a \textbf{computationally efficient}, \textbf{structure-aware} loss function that enhances segmentation performance while maintaining practical scalability. By extending its application beyond segmentation, we aim to contribute to the broader field of generative modeling, object detection, and self-supervised learning in deep neural networks.



\bibliography{main_bib}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage
%\appendix
%\onecolumn
%\section{You \emph{can} have an appendix here.}

%You can have as much text here as you want. The main body must be at most $8$ pages long.
%For the final version, one more page can be added.
%If you want, you can use an appendix like this one.  

%The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
