\section{Introduction}

Accurately predicting 3D structures/dynamics of physical systems remains a fundamental challenge in physics and biology. Typical tasks such as molecular dynamics simulation~\citep{hollingsworth2018molecular} and antibody design~\citep{tiller2015advances} require not only a deep understanding of complex spatial geometry but also the preservation of $\mathrm{E}(3)$-equivariance --- ensuring predictions transform correspondingly with input rotations, reflections and translations~\citep{batzner20223, huang2022equivariant}. From the machine learning perspective, $\mathrm{E}(3)$-equivariant models are more powerful than their non-equivariant counterparts, as they are inherently generalizable across arbitrary coordinate systems when modeling physical systems. To achieve equivariance, current approaches primarily rely on geometric Graph Neural Networks (GNNs)~\citep{wu2024equivariant, kong2022conditional}. Despite their fruitful progress, these models often lack the ability to leverage external domain knowledge and broader contextual information, such as task-specific instructions and expert-curated guidance, hindering further performance enhancement.

Recently, Large Language Models (LLMs) have demonstrated remarkable success across a wide range of applications, owing to their large-scale pretraining on extensive datasets and their substantial model size. It is well known that LLMs can not only understand and generate text but also excel at integrating and leveraging scientific knowledge~\citep{liu2025integrating, jablonka2024leveraging, wang2023scibench}. For instance, LLMs can comprehend fundamental chemical concepts and molecular structural characteristics~\citep{guo2023can}. More significantly, LLMs' flexibility in prompt engineering enables the development of tailored instructions that better leverage their capabilities, producing outputs more precisely suited to the task.

A natural idea is to directly employ LLMs for modeling 3D physical systems. However, this approach \emph{fails to} yield satisfactory results in practice. A key limitation is that LLMs are trained to process ordered and discrete text tokens, restricting their ability to directly comprehend unordered and continuous data in 3D space. One possible solution is to adapt existing multimodal LLM architectures, such as LLaVA~\citep{liu2024visual}, by treating 3D structures as a separate modality and simply replacing the image encoder with a geometric GNN. However, this naive adaptation fails to satisfy the $\mathrm{E}(3)$-equivariance requirement. Since geometric GNNs produce both invariant features and equivariant coordinates, passing these outputs through an LLM inevitably compromises equivariance. \emph{Therefore, it is non-trivial to integrate the strengths of both LLMs and geometric GNNs while maintaining essential geometric properties}.

To this end, this paper introduces EquiLLM, a novel framework for representing 3D physical systems that seamlessly integrates $\mathrm{E}(3)$-equivariance with LLM capabilities. 
EquiLLM is carefully designed and comprises four core modules (see Figure \ref{fig:architecture}): geometry-aware prompting, an equivariant encoder, an LLM, and an equivariant adapter. 
A key insight of EquiLLM in maintaining equivariance lies in its innovative design: the LLM guided by the instructive prompt serves as a sophisticated invariant feature processor, while 3D directional information is exclusively handled by the equivariant encoder and adaptor modules.
Specifically, to fully activate the spatial reasoning capabilities of LLMs, we introduce the geometry-aware prompts containing invariant geometric information, including task description, input
feature description and statistical information. Then, EquiLLM employs a geometric GNN as a domain-specific encoder to effectively model and extract 3D representations of input systems. Subsequently, to satisfy the equivariance constraint, the input to the LLM are strictly limited to the prompt and invariant features derived from the equivariant encoder's output. The LLM-generated outputs are subsequently combined with the 3D equivariant vectors produced by the equivariant encoder and fed into an equivariant adaptor for information fusion. EquiLLM ultimately produces both invariant labels and equivariant coordinates required by downstream applications.



To sum up, our main contributions are threefold:
\begin{itemize}
    \item To the best of our knowledge, we present the first investigation into modeling 3D physical systems by integrating LLMs with geometric GNNs, aiming to combine the strengths of both approaches.
    \item We present EquiLLM, a novel framework that is meticulously designed to permit E(3)-equivariance and instill 3D spatial reasoning into LLMs' powerful capabilities. 
    \item We conduct extensive experiments on diverse tasks of molecular dynamics simulation, human motion simulation and antibody design. The results show that our method achieves superior performance, attaining state-of-the-art results in nearly all metrics. 
\end{itemize}
