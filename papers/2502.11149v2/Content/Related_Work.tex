\section{Related Work}
\textbf{Geometric GNN.} Due to its thorough consideration of physical symmetries in 3D space, Geometric GNN~\citep{satorras2021n, fuchs2020se} has been widely applied in various scientific tasks, such as dynamics simulation~\citep{xu2024equivariant} in physics and antibody design~\citep{lin2024geoab} in biology. ESTAG~\citep{wu2024equivariant} enhances the model's simulation capability for object dynamics trajectories in both spatial and temporal dimensions through interleaved Equivariant Spatial Module and Equivariant Temporal Mod   ule. SEGNO~\citep{liu2024segno} incorporates the second-order graph neural ODE with equivariant property to reduce the roll-out error of long-term physical simulation. MEAN~\citep{kong2022conditional} not only leverages an Internal context encoder to model spatial correlations within antibody chains but also introduces an External attentive encoder with an attention mechanism to better capture inter-chain relationships. GeoAB~\citep{lin2024geoab} extends the equivariant model GMN~\citep{huang2022equivariant} to handle binary, ternary, and quaternary interactions and applies it in atom-level updates. Despite significant progress these methods have made, they fail to explore how to harness the capabilities of LLMs to augment model performance.

\textbf{LLM + GNN.} Large Language Models (LLMs) with rich knowledge are being widely transferred and applied across multiple domains to enhance model capabilities~\citep{singhal2023large, singhal2025toward}. Numerous excellent works have emerged in combining GNNs with LLMs for scientific applications. ChemLLMBench~\citep{guo2023can} tests LLM's understanding, reasoning, and explaining capabilities on various chemical tasks using in-context learning. Prot2Text~\citep{abdine2024prot2text} integrates protein sequence, structure, and textual annotations into an encoder-decoder framework composed of GNN and LLM to predict protein functions. MoleculeSTM~\citep{liu2023multi} uses a contrastive learning paradigm to align molecular graphs and textual descriptions in the semantic space, thereby learning better feature representations. MolCA~\citep{liu2023molca} employs Q-Former~\citep{li2023blip} as a cross-modal projector to align the feature spaces of graph encoder and language encoder, enhancing performance in molecule captioning tasks. Although the aforementioned methods promote interactions between GNNs and LLMs through various paradigms and yield promising results, they have yet to explore tasks involving 3D structural data, such as 3D structure generation and dynamic trajectory simulation in 3D space. The EquiLLM framework we propose in this paper is the first to integrate LLMs with Geometric GNNs that embed spatial symmetry constraints and has been effectively validated across datasets from both physical and biological domains.
