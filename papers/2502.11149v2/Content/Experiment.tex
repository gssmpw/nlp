\section{Experiments}
We validate the effectiveness of the proposed EquiLLM framework on two tasks from different domains: the dynamic simulation in physics (\cref{sec:4.1}) and the antibody design in biology (\cref{sec:4.2}). Furthermore, in \cref{sec:4.3}, we conduct ablation studies, explore potential design variations of the EquiLLM framework, and discuss why the current framework is superior.

\subsection{Dynamic Simulation}
\label{sec:4.1}
In the dynamic simulation task, to demonstrate the broad applicability of our model across varying scales, we conduct experiments on two distinct datasets: the molecular-level MD17~\citep{chmiela2017machine} dataset and the macro-level Human Motion Capture~\citep{de2009guide} dataset. In order to expedite the dynamics simulations, we implement a sampling strategy based on previous research~\citep{huang2022equivariant} to extract a subset of trajectories for the purposes of training, validation, and testing. This approach involves randomly selecting an initial point and then sampling $2 \times T$ timestamps. The first $T$ timestamps are utilized as input for the models, while the remaining $T$ timestamps represent the future states that the models need to predict.

\textbf{Baselines and metrics.}
We evaluate EquiLLM with several baseline models, including traditional GNNs: ST\_GNN~\citep{gilmer2017neural} and STGCN~\citep{yu2017spatio}, equivariant GNNs: ST\_TFN~\citep{thomas2018tensor}, ST\_SE(3)-Tr.~\citep{fuchs2020se}, ST\_EGNN~\citep{satorras2021n}, and ESTAG~\citep{wu2024equivariant}. We also compare EquiLLM with existing LLMs, including GPT-4o-mini~\citep{openai20244o}, Gemini-1.5-flash-latest~\citep{team2024gemini}, and DeepSeek-V3~\citep{liu2024deepseek}. The models marked with ``ST'' are those we modified to handle multi-frame inputs by adding basic spatio-temporal aggregation, as done in ~\citep{wu2024equivariant}. For evaluation, we calculate the Mean Squared Errors (MSEs) averaged across all predicted frames as the metric.

\subsubsection{Molecular Dynamics}
\input{Table/MSE_on_MD17_R10}

\textbf{Implementation details.}
MD17 consists of time-evolving paths produced through molecular dynamics simulation for eight different small compounds (such as aspirin, benzene, and others). To ensure a fair comparison, all hyperparameters (\emph{e.g.} learning rate, number of training epochs) are kept consistent across our model and all other baselines. We utilize the ESTAG~\citep{wu2024equivariant} as the Equivariant Encoder and GPT-2~\citep{radford2019language}~\footnote{A more powerful LLM may lead to a superior performance. Here we use GPT-2 for concept validation.} as the language model within our EquiLLM framework.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{Figure/vis.pdf} 
  \vspace{-1cm}
  \caption{The visualization of the predicted structures across various methods on Toluene of the MD17 dataset, where the pink represents the ground-truth structure.}
  \label{fig:rna_pdf}
\end{figure}

\textbf{Results.}
\cref{table:md17_r10} presents the performance of all models on MD17 dataset under the setting of predicting 10 frames from an input of 10 frames. From the table, the following conclusions can be drawn: \textbf{1.} The proposed EquiLLM framework achieves state-of-the-art (SOTA) performance on all eight molecules, demonstrating its superiority; \textbf{2.} Compared to our Equivariant Encoder model ESTAG, EquiLLM achieves a performance improvement of 5.41\% to 42.76\% on six molecules, indicating that EquiLLM effectively leverages knowledge from LLMs to enhance the prediction of molecular dynamics trajectories; \textbf{3.} We also tested the prediction capability of several leading LLMs, including GPT-4o-mini, Gemini-1.5-flash-latest, and DeepSeek-V3. DeepSeek-V3$^{\star}$ indicates that due to frequent API timeouts, several molecules do not complete the full testing dataset. Using the same prompt as EquiLLM, we provide the 3D coordinates of all atoms from the past 10 frames to these LLMs, allowing it to predict the coordinates of all atoms in the following 10 frames. The result shows that these LLMs significantly underperforms most baseline methods in prediction accuracy, indicating its weaker capability in directly predicting 3D coordinates. In contrast, our EquiLLM framework, by providing structured molecular descriptions and statistical constraints, enables the LLM to combine its pretrained knowledge with the specific task, thus significantly reduce the predicted MSE.

\subsubsection{Human Motion Simulation}
\input{Table/MotionAntiBody}
\input{Table/Ablation_Study_on_MD17}

\textbf{Implementation details.}
The Human Motion Capture dataset contains human motion trajectory data across multiple scenes. We focus primarily on two sub-datasets: Subject \#35 (\texttt{Walk}) and Subject \#102 (\texttt{Basketball}). To ensure a fair comparison, all hyperparameters (\emph{e.g.} learning rate, number of training epochs) are kept consistent across our model and all other baselines. We utilize the ESTAG as the Equivariant Encoder and GPT-2 as the language model within our EquiLLM framework.

\textbf{Results.}
\cref{table:motion} presents a performance comparison of all models on the \texttt{Walk} and \texttt{Basketball} datasets under settings requiring the prediction of 10, 15, and 20 frames, respectively. From the table, it is evident that EquiLLM framework achieves SOTA performance across all six settings, with a performance improvement ranging from 5.63\% to 36.11\%. This demonstrates that EquiLLM effectively handles predictions over varying prediction lengths, exhibiting excellent robustness and generalization ability

\subsection{Antibody Design}
\label{sec:4.2}
Following previous study MEAN~\citep{kong2022conditional}, we selected complete antibody-antigen complexes from the SAbDab~\citep{dunbar2014sabdab} dataset to construct the training and validation sets. First, we performed clustering based on CDRs, grouping complexes with CDR sequence identity above 40\% into the same cluster. Then, the training and validation sets were partitioned in the same manner as in MEAN. For test set, we selected 60 diverse complexes from the RAbD~\citep{adolf2018rosettaantibodydesign} dataset to evaluate the performance of different methods. Before starting the experiments, we remove samples from the training and validation sets that belong to the same cluster as the test set to prevent data leakage.


\textbf{Baselines and metrics.}
We compared our EquiLLM with seven methods, including RosettaAD~\citep{adolf2018rosettaantibodydesign}, LSTM~\citep{saka2021antibody, akbar2022silico}, RefineGNN~\citep{jin2022iterative}, MEAN~\citep{kong2022conditional}, GeoAB~\citep{lin2024geoab}, and two variants of LSTM and RefineGNN, C-LSTM and C-RefineGNN, which utilize the full contextual information. We use AAR and RMSD to reflect the recovery ratio of the CDR-H3 amino acid sequence and the accuracy of the corresponding 3D structure prediction. Additionally, we employ the TM-score~\citep{zhang2004scoring, xu2010significant} to measure the global similarity between two protein structures. We utilize the MEAN~\citep{kong2022conditional} as the Equivariant Encoder and GPT-2 as the language model within our EquiLLM framework. 


\textbf{Results.}
\cref{table:rabd} presents the performance of all models on the RAbD dataset. It can be concluded from the table: \textbf{1.} The proposed EquiLLM framework achieves the best performance in both AAR and RMSD metrics, with comparable results in the TM-score metric. The SOTA method GeoAB achieves superior performance in TM-score by incorporating more detailed geometric constraints, such as bond lengths, bond angles, and torsion angles, into the model, which enhances its overall structural prediction capabilities; \textbf{2.} Compared to the Equivariant Encoder model MEAN, EquiLLM shows significant improvement across all metrics, demonstrating that EquiLLM successfully leverages LLM’s knowledge integration and constraint-handling abilities while effectively utilizing LLM’s capacity for understanding and reasoning 1D sequences.

\subsection{Ablation studies}
\label{sec:4.3}
In this section, we delve into the design of the EquiLLM framework, analyzing the impact of different architectural designs and prompt configurations on model performance. The experimental results are shown in \cref{table:md17_ablation}, where EE represents the Equivariant Encoder.

\textbf{Architecture Design.} \textbf{1.} The results in the second row indicate that processing raw features through the LLM before feeding them into the Equivariant Encoder, while omitting the Equivariant Adapter for a simpler architecture, yields a performance improvement. This finding validates that the LLM has a fundamental capability to process and integrate structured information effectively. \textbf{2.} However, to fully exploit the potential of the LLM model, it is necessary to leverage prompts to capitalize on its strengths in text understanding. Building upon the second-row model, we perform experiments by adding prompts, as the results shown in the third row. The results indicate a significant performance drop. We speculate that this is due to the large semantic space difference between the unprocessed raw feature $\mH$ and the text features, which hampers the model's prediction capabilities. This suggests that LLM requires an appropriate interface to harness its advantages. This led to the design of the current EquiLLM framework.

\textbf{Prompt Design.} \textbf{3.} From \cref{table:md17_ablation}, it is evident that either completely removing the prompt or reducing its content (such as the Object Feature or Statistics) leads to a decline in performance. This observation reinforces our design philosophy: LLMs necessitate comprehensive information, including molecular descriptions and statistical constraints, to fully utilize their knowledge integration and constraint reasoning capabilities. This, in turn, facilitates more accurate predictive guidance.
