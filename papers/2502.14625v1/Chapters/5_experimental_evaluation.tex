\section{Experimental evaluation}
In this chapter we describe our methods of solving the problem and show the results on our dataset.
\subsection{Parallel pipeline}
The first architecture we tested was ``Parallel pipeline``. Visual scheme of this architecture described in \autoref{fig:parallel_pipeline}.
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.45\textwidth]{attach/parallel_pipeline.jpg}
    \caption{Parallel pipeline scheme}
    \label{fig:parallel_pipeline}
\end{figure}
\begin{itemize}
    \item \textbf{Segmentation} At this stage, we find records boundaries that help us split html into records and find corresponding attributes of each record.
    \item \textbf{Classification} At this stage, we give a corresponding label for each node on html. It is important that the segmentation stage and the classification stage are two independent stages.The results of neither are not shared with the other.
    \item \textbf{Matching of results} At this stage, the final result of the method is formed. Based on the results of segmentation and classification, the records are matched with their attributes and provided in a structured view.
\end{itemize}

\subsection{Sequential pipeline}
We will also test sequential pipeline as described at \autoref{fig:sequential_pipeline}. In the following we will compare the qualities of both architectures. Quality should vary due to different approaches to the classification stage.
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.45\textwidth]{attach/sequential_pipeline.jpg}
    \caption{Sequential pipeline scheme}
    \label{fig:sequential_pipeline}
\end{figure}
\begin{itemize}
    \item \textbf{Segmentation} In sequential architecture this stage is similar to parallel scheme.
    \item \textbf{Classification} At this stage, the final result of the method is formed. Based on the segmentation results, the stage of searching for attributes only in the selected area is carried out. Thus, the matching stage is not required.
\end{itemize}


\subsection{Segmentation subtask} \label{segmentation}
We formulate the task of web-pages segmentation as a task of searching for information boundaries, which are first nodes containing text of the DOM subtree and belonging to it. This formulation is the same as that used in the \cite{san_plate_2023}.

We test two methods of solving this task:
\begin{enumerate}
    \item Heuristic method based on classical MDR
    \item Neural method based on MarkupLM, which is state-of-the-art model in information extraction from HTML pages 
\end{enumerate}

\textbf{MDR} We tested MDR because it has open-source code. Since this method proposes several ``candidates`` for segmentation, ordered by their probability. We choose the segmentation with the highest probability.

\textbf{MarkupLM} We train the MarkupLM model on this task, having the data previously prepared: for each record on the page, we have marked its first node which contains text with the “BEGIN” label. All other nodes on the page have been marked with the label “OUT“. Thus, the solution of the problem is to predict the corresponding label for each DOM tree node by the model.

\subsubsection*{Segmentation metrics}
The result of the segmentation method can be evaluated by page-weighted metrics: $Precision_{avg}$, $Recall_{avg}$, $F1_{avg}$

To calculate them, the reference segmentation of the given page on the record is compared with the one obtained by the proposed methods. Based on this comparison, for each segment it is possible to calculate $TP_{page K}$ -- the number of DOM nodes correctly marked as an information boundary, $FP_{page K}$ - the number of DOM tree nodes that are not an information boundary, but marked with it, $FN_{page K}$ -- the number of DOM tree nodes that are an information boundary, but not marked as it:

$$
Precision_{page K} = \frac{TP_{page K}}{TP_{page K} + FP_{page K}}, 
$$

$$
Recall_{page K} = \frac{TP_{page K}}{TP_{page K} + FN_{page K}},
$$

$$
F\textit{1}_{page K} = 2 \cdot \frac{Precision_{page K} \cdot Recall_{page K}}{Precision_{page K} + Recall_{page K}},
$$

$$
Precision_{avg} = \frac{\sum_{i = 1}^{K}Precision_{page\,i}}{K},
$$

$$
Recall_{avg} = \frac{\sum_{i = 1}^{K}Recall_{page\,i}}{K},
$$

$$
F\textit{1}_{avg} = \frac{\sum_{i = 1}^{K}F\textit{1}_{page\,i}}{K}
$$

Also we will calculate classical $NMI$\cite{Strehl2002} and $ARI$\cite{Hubert1985} for segmentation task.

\begin{table}[!htb]
    \caption{Results of segmentation experiment}
    \centering
        \begin{tabular}{c|ccc|cc}
        \toprule
        Method & $Recall_{avg}$ & $Precision_{avg}$ & $F\textit{1}_{avg}$ & $ARI$ & $NMI$\\
        \midrule
         MDR & 0.473 & 0.486 & 0.465 & 0.437 & 0.517\\
         MarkupLM & \textbf{0.908} & \textbf{0.941} & \textbf{0.925} & \textbf{0.802} & \textbf{0.869}\\
         \bottomrule
    \end{tabular}
    \label{tab:segmentation}
\end{table}

The test results are shown in the \autoref{tab:segmentation}. The MarkupLM model shows superior results in all metrics in comparison with the other tool.

\subsection{Classification subtask}
This subtask includes searching for all potential attributes in the area of interest on the html page (for parallel pipeline - it is the whole page, and for sequential - a part of it).

\begin{table}[htbp]
\centering
\caption{Results of classification experiment}
\begin{tabular}{c|c|*3c}
    \toprule
        Type                                    &     Metric            & title    & tag      &  date \\
        
        \midrule
\multirow{3}{*}{Record context}                & $Precision_{avg}$     & 0.98     & 0.79     & 0.86  \\
                                                & $Recall_{avg}$        & 0.99     & 0.85     & 0.91  \\
                                                & $F\textit{1}_{avg}$   & \textbf{0.99}     & 0.82      & \textbf{0.88}  \\
        
        \midrule
\multirow{3}{*}{Page context}                   & $Precision_{avg}$     & 0.68     & 0.61       & 0.64      \\
                                                & $Recall_{avg}$        & 0.95     & 0.76       & 0.86      \\
                                                & $F\textit{1}_{avg}$   &  0.79    & \textbf{0.89}       & 0.73      \\
    \bottomrule
\end{tabular}
\label{tab:markuplm_classification}
\end{table}

The MarkupLM model is also chosen as a classification method. We train it on the task of predicting a node’s label. Thus, the model predicts the labels: ``title``, ``tag``, ``date``. 


\subsubsection*{Classification metrics}
Results are evaluated by classical page-weighted classification metrics: $Precision_{avg}$, $Recall_{avg}$ and $F\textit{1}_{avg}$.

We trained two models: one for the pipeline with full page context (in such conditions, classification is used in the parallel pipeline), and one with just record context (passing the part of the html related to a specific record directly to the input, so it is used in a sequential pipeline). The comparison of their results is presented in the table \autoref{tab:markuplm_classification}. So, MarkupLM with record context shows rather high results in this subtask.

\subsection{Matching subtask}
We propose matching algorithm:
\begin{itemize}
    \item Let $G$ -- set of xpaths\footnote{We are considering positional xpath expressions consisting of tags and indices in the DOM tree path from root to the given node.} to all DOM-nodes of htmls marked as ``information boundary`` at the segmentation stage.
    \item Let's make $\widehat{G}$: for each xpath from $G$ find minimal (by length) node prefix. Each prefix should belong to only one xpath from $G$.
    \item Let's enumerate $\widehat{G}$.
    \item  Let's refer Xpath of some attribute determined at classification stage as $xpath_{attr}$. Each attribute is matched with record with number $N$ if $\widehat{G}[N]$ is prefix for $xpath_{attr}$.
\end{itemize}


\subsection{Final method evaluation metrics}
We evaluated the final method using metrics similar to detailed page information extraction task's metrics. We define ``predicted record`` as a set of predicted attributes matched to the same record at the Matching stage. Also we define ``reference record`` as a set of ground truth attributes values from a sought ``information boundary``. There are three cases possible when the method is run:
\begin{enumerate}
    \item \textit{We can match the predicted record with the reference record.}\\ $TP$ is the number of correctly extracted attributes (for example if a record contains 4 tags and they are extracted correctly, then $TP$ is 4). $FP$ is the number of extracted attributes but none of them should have been. $FN$ is the number of not extracted attributes but all of them should have been.
    \item \textit{We cannot match any of the predicted records with the reference record.}\\ Only $FN$ increases for all attributes of the reference record.
    \item \textit{We cannot match any of the reference records with the predicted record.}\\ In this case $FP$ increases for all attributes of the predicted record.
\end{enumerate}

\subsection{Experiments results}
\begin{table}
\centering
\caption{Results of final method}
\begin{tabular}{c|c|*3c}
    \toprule
        Method                  &     Metric     & title    & tag       &  date        \\
        
        \midrule
\multirow{6}{*}{Parallel Pipeline}             & TP & 56008    & 29202     & 60570        \\
                                                & FP & 7021     & 10804     & 12728       \\
                                                & FN & 8083     & 4484      & 12361       \\
                                                \cmidrule(lr){2-5}
                                                & Precision & \textbf{0.874}     & \textbf{0.867}     & \textbf{0.831}  \\
                                                & Recall    & 0.889     & 0.73      & 0.826  \\
                                                & F1        & 0.881     & 0.793     & 0.828  \\
        
        \midrule
\multirow{6}{*}{Sequential Pipeline}            & TP & 55891    & 28643     & 58645           \\
                                                & FP & 1492     & 7641      & 7056            \\
                                                & FN & 8801     & 4899      & 13659           \\
                                                \cmidrule(lr){2-5}
                                                & Precision & 0.864     & 0.854     & 0.811  \\
                                                & Recall    & \textbf{0.974}     & \textbf{0.789}     & \textbf{0.893}  \\
                                                & F1        & \textbf{0.916}     & \textbf{0.82}      & \textbf{0.85}   \\
    \bottomrule
\end{tabular}
\label{tab:final_experiments_results}
\end{table}

We tested both proposed methods: parallel pipeline and sequential pipeline. For these methods, we used the corresponding trained versions of MarkupLM, as we described them in the previous chapters. Comparative results are shown in \autoref{tab:final_experiments_results}, while \autoref{tab:markuplm_classification} presents the performance of classification model in different contexts—record and page.

In \autoref{tab:markuplm_classification}, the model using the ``record context`` outperforms model using the ``page context``. The tag attribute in the page context performs relatively well, but the overall trend shows that the model is more effective in the record context. We assume that the advantage of page context for tag-attribute extraction is related to the peculiarity of this attribute. Most often, each record has several tags. Information about neighboring records allows making less noisy predictions.

The parallel pipeline demonstrates solid precision, particularly for title. However, it shows a decline in recall. In contrast, the sequential pipeline outperforms the parallel pipeline in recall. The sequential pipeline achieves a balanced performance, making it the preferred method due to its robustness and consistency.

In conclusion, the sequential pipeline is superior to the parallel pipeline. The record context further enhances classification performance, making it the optimal setting for method deployment. The high results of the obtained methods prove their applicability in real life.
