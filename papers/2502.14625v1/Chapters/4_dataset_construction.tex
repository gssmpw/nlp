\section{Dataset construction}
We decided to collect our own dataset for the task of extraction information from multi-record web-pages and make it publicly available. In this chapter we describe how the data was collected and annotated, also we provide the final dataset's main characteristics.

\subsection{Data collecting}
Our dataset contains news web pages collected from Russian-language media. News resources were selected according to the MediaMetrics\footnote{https://mediametrics.ru/rating/ru/online.html} latest news quotations system, which provides rankings based on the popularity of news resources. Web pages were downloaded between 12/28/2023 and 04/28/2024. Pages were downloaded using a special Python3 script (based on the Scrapy library\cite{scrapy_library}), which was run daily through prepared sitemaps.

\subsection{Sitemaps development}
Sitemaps were prepared using the WebScraper\footnote{https://github.com/ispras/web-scraper-chrome-extension} browser extension for Chrome. Using sitemaps in special web crawlers allows to download an html page, as well as an answer for this page. Thus, each site requires the development of a unique sitemap. On each page, the boundaries of each record and its attributes were annotated, if they existed. Only the following attributes were noted: \textit{date}, \textit{title}, \textit{tag}, \textit{short\_title}, \textit{author}, \textit{time}. The annotation was done manually.
We chose this set of attributes because they are the most popular in the news websites domains. 
For each site several categories were annotated, for example, politics, economics, and sports. In this way 312 sitemaps were prepared.

\subsection{Dataset preparing}
For further use of data in our dataset, it was necessary to preprocess the raw data. The following actions were carried out:
\begin{enumerate}
    \item Filtering duplicate pages. Some downloaded pages contained many records scraped before on other pages. Such pages, with more than a quarter repeated records, were filtered.
    \item Cleaning HTML. At this stage blocks, such as blocks of code in JavaScript (i.e. all nodes with the \verb|<script>| tag), that do not affect the algorithm's performance were removed from the HTML code of the page.
    \item Translating HTML. Since our dataset contains pages in Russian, it was necessary to translate them into English to be able to use pre-trained models.
    \item Division into training and test parts. The distribution of attributes and domains of web pages was taken into account. Each domain was placed either to the training or to the test parts (see attributes split in the \autoref{fig:attribute_dist}). The final distribution is shown in the \autoref{fig:amount_of_attributes_test} and \autoref{fig:amount_of_attributes_train}. Ratio of parts after splitting was the following: 75\% - training, 25\% - testing.
\end{enumerate}

\subsection{Dataset Statistics}
Since the maps were based on CSS selectors, there was a problem with downloading sites that dynamically change the names of the styles on the pages. In other words, when the website changed the name of the CSS style class, the selector specifying the class name stopped working correctly. Therefore, we were able to download only 278 sites.

\begin{table}[!htb]
    \caption{Attribute frequency}
    \centering
    \begin{tabular}{c|c|c|c}
        \toprule
        Name & Pages & Records & Sites\\
        \midrule
        title & 12679 & 247262 & 275\\
        date & 12296 & 241634 & 251\\
        tag & 6165 & 108400 & 140\\
        \cdashline{1-4}
        short\_text & 6855 & 115983 & 138\\
        short\_title & 105 & 1289 & 4\\
        author & 87 & 957 & 1\\
        time & 730 & 15809 & 8 \\
        \bottomrule
    \end{tabular}
    \label{tab:attribute_statistic}
\end{table}
Thus, a dataset that contained 13120 pages from 278 Internet media was prepared (distribution between pages and entries on them, shown in the \autoref{fig:page_records_dist}). On each page, the corresponding attributes were annotated,  and their frequency was presented in the \autoref{tab:attribute_statistic}. All pages presented in the table have UTF-8 encoding. 

We researched extraction of 3 attributes from our dataset: title, author and tag. Because they are the most frequent in our dataset.