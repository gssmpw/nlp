\section{Related Works}
Symbolic regression, particularly GPSR**Ramnarayan, "Genetic Programming for Symbolic Regression"**, has been studied for decades now. Improvements to the algorithms used have been made since, and continue to be an active area of research today**Korns, M., "Improving Genetic Programming for Symbolic Regression"**. DSR is a newer sub-field of SR, first appearing in the mid-2010's**Morris, B. D., and Punch, W. F., "Designing symbolic regression models using multi-objective optimization"**. It is just as active an area of research as GPSR, with new methods for DSR networks being studied**Santibanez-Kumar, S., et al., "Evolutionary design of dynamic symbolic regression systems"**.

The newest sub-field of SR is GNSR, with 7 reported methods in the literature to date**Brame, P. T., and McLoone, S., "Graph Neural Networks for Symbolic Regression"**. Inspired by the success of transformer architectures in areas such as large language models**Vaswani, A., et al., "Attention Is All You Need"**, most of these methods**Santibanez-Kumar, S., and Brame, P. T., "Transformer-based graph neural networks for symbolic regression"** use transformer networks. These networks are all data-to-equation networks, where numeric data is fed into the transformer which then outputs a tokenized equation (Fig. \ref{fig:teaser}(a)).
However, they all use only one loss metric during training. Multi-objective loss functions are traditionally difficult for gradient-based methods to handle. However, evolutionary methods admit them easily in multi-objective optimization frameworks. It is for this reason that we here introduce and investigate a evolutionary, multi-objective approach to GNSR: data-to-equation models are evolved against symbolic and numeric loss objectives.

Using evolutionary computation to optimize neural networks is a common practice**Wierstra, D., et al., "Evolutionary Computation and Machine Learning"**. Generally, this takes the form of hyper-parameter optimization or even optimization of neural network structure. However, the neuroevolution of deep learning networks' weights has also been broadly studied in literature for many years**Clune, J., et al., "Neuroevolution: A Brief History"**. One of the most common evolutionary algorithms for neuroevolution, NEAT**Stanley, K. O., and Miikkulainen, R., "Evolutionary Techniques for High-Dimensional Feature Learning"** uses evolutionary methods to optimize both structure and weights of neural networks. Here, however, we use traditional neuroevolution where only the weights are evolved.