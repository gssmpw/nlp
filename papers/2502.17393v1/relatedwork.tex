\section{Related Works}
Symbolic regression, particularly GPSR\cite{Koza1992}, has been studied for decades now. Improvements to the algorithms used have been made since, and continue to be an active area of research today~\cite{LaCava2021,Zojaji2022,Haider2023,Yang2023,Fleck2024}. DSR is a newer sub-field of SR, first appearing in the mid-2010's~\cite{Martius2016}. It is just as active an area of research as GPSR, with new methods for DSR networks being studied~\cite{Petersen2019,Zhang2023,Boddupalli2023}.

The newest sub-field of SR is GNSR, with 7 reported methods in the literature to date~\cite{Biggio2020,Biggio2021,Valipour2021,Kamienny2022,Vastl2022,Li2022,Bendinelli2023}. Inspired by the success of transformer architectures in areas such as large language models~\cite{Vaswani2017,Keskar2019,Wang2019language}, most of these methods~\cite{Biggio2021,Valipour2021,Kamienny2022,Vastl2022,Li2022,Bendinelli2023} use transformer networks. These networks are all data-to-equation networks, where numeric data is fed into the transformer which then outputs a tokenized equation (Fig. \ref{fig:teaser}(a)).
However, they all use only one loss metric during training. Multi-objective loss functions are traditionally difficult for gradient-based methods to handle. However, evolutionary methods admit them easily in multi-objective optimization frameworks. It is for this reason that we here introduce and investigate a evolutionary, multi-objective approach to GNSR: data-to-equation models are evolved against symbolic and numeric loss objectives.

Using evolutionary computation to optimize neural networks is a common practice\cite{Jin2009,Feurer2019,Biswas2021,Tani2021}. Generally, this takes the form of hyper-parameter optimization or even optimization of neural network structure. However, the neuroevolution of deep learning networks' weights has also been broadly studied in literature for many years\cite{Montana1989,Rocha2007,Floreano2008,Ding2013,Stanley2019,Galvan2021}. One of the most common evolutionary algorithms for neuroevolution, NEAT~\cite{Stanley2002} uses evolutionary methods to optimize both structure and weights of neural networks. Here, however, we use traditional neuroevolution where only the weights are evolved.