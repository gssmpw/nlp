\section{Context}
\label{sec:context}

Historically, the word ``algorithm" comes from the name of Muhammad ibn Musa al-Khwarizmi, 
and really consisted of decomposing complex tasks into elementary trivial subtasks.
This intuitive approach was further formalized by the pioneers of computer science, 
in particular Alonzo Church~\cite{church1936note} and Alan Turing~\cite{turing1936computable}.
Both argued that any computable function can be written as a composition of elementary ultra-specialised hard-wired mechanical operations,
typically on a Turing machine (or in lambda calculus).
In today's computing machines, 
such operations are the (now numerous) logical circuits engraved in processing units like CPUs.
The assertion that these fixed circuits suffice to perform 
\emph{any} computable information processing function is now known as the Church-Turing thesis.
This thesis arguably thus asserts that, in principle, 
any general capability is a composition of (a very large number of) specialised operations.

However, in practice, when software consumers use an algorithm,
the compositions of such specialised operations is abstracted away,
so that the consumer ends up interacting with a complex system,
whose range of capabilities can be narrow or wide.
Lately, the rise of language models, but also that of fully integrated and connected cloud services like Microsoft 365, has led to the commercialisation of ``general systems". 
% But what does this intuitive ``generality" actually entail? At least to the extent that 
Such systems (are said to) provide a large number of information processing services.

In fact, many software developers might not view their software solutions as a composition of specialised operations, especially if their solutions leverage external libraries. 
While some of these codes simply translate the developers' programming language codes into elementary binary codes (these are \emph{compilers}), it has become extremely common for developers to use complex ``general-purpose" libraries, especially when importing, e.g., large language models. 
Such systems could be argued to ``empower" developers, 
as they can now effortlessly program solutions to many more problems.
However, they can also be argued to reduce their capability to understand their own systems,
especially in terms of capability, safe usage and emergent risks.


What we might call ``specialisation blindness'' can be found in other activities or professions. A fast-food franchisee, for example, considers that their business is selling hamburgers or other products sold in this type of restaurant. However, upstream, a franchisor has to call on several specialist trades to develop all the building blocks for a successful franchise: the right furniture (interior designers, furniture salesmen...), the right recipes (chefs, nutritionists, etc.), the right ingredients (farms, bakers, drinks wholesalers...), the right business model (bankers, accountants...), the right location (estate agents, notaries...), etc. 


Specialisation blindness is arguably an unavoidable consequence of collaborating to perform complex tasks.
However, the question we raise in this paper, is how to organise collaboration to make it an asset
rather than a risk.
Essentially, we will argue that the key lies in well-specified specialisation.


\subsection{Defining generality}

% \len{Un peu long. À discuter :}

In the context of the AI Act, generality is often associated with the \emph{purpose} of a system. The greater the diversity of use cases, the more ``general-purpose" the system. This definition of generality-by-purpose can be also found in Model Cards~\cite{mitchell2019model}, which asks AI system developers to specify the ``intended use cases" of the developed systems.

We note however that there may be ambiguity behind the definition of a use case.
Assume for instance that a language model is used to translate messages on a social media platform.
Is the use case ``social media"? 
Or merely ``translation"?
Or even more simply ``next-token predictions", given an original text and a translation request prompt?
Clearly, if we consider the first answer (``social media"),
then the system may appear very ``general-purpose";
but more basic algorithms like json serializers or symmetric encryption should then be regarded as more ``general purpose".
Conversely, the more we dig into the very precise use of a system (``next-token prediction"), the less ``general-purpose" it will appear to be.

A leaked document~\cite{Maxwell2024leaked} from OpenAI and Microsoft revealed their use of a more financial approach to defining AGI.
Namely, they (privately) regarded as ``general" a system that generates hundreds of billions of dollars in revenues.
In a sense, this approach is similar to counting use cases of a system,
but it proposes to weigh the use cases by their financial added values.
It is however noteworthy that, given this definition, Blackrock's trading algorithm Aladdin \footnote{In 2020, Aladdin managed 21.6 trillion U.S. dollars in assets\cite{Ungarino}.}, and probably also the algorithms of Google's AdSense \footnote{Google's ad revenue amounted to 237.86 billion U.S. dollars in 2023\cite{Statista}.} should be regarded as much more ``general" than language models.

Another approach that may be considered to assess the generality of a software system
is the list of \emph{application program interface} (API) calls it defines.
% and through which it may be called.
Indeed, each API function proposed by the system may be viewed as a task that the system can offer to solve.
Of course, in practice, some API functions can be regarded as themselves more ``general" than others,
e.g. if they involve multi-modal inputs rather than text-only.
While still not fully rigorous, in the sequel,
we lean towards this definition.
We will regard a system as general, 
if the number of tasks that external users can ask the system to perform is large.

Note that a very specialised system in this sense may nevertheless be extremely complex.
For instance, even though a language model is only accessible through a ``next-token prediction" API,
or if its only API yields content recommendations,
it may be itself composed of trillions of parameters \cite{DBLP:conf/kdd/LianYZWHWSLLDLL22}.
In fact, any large human organization that is specialised to deliver a very specific task
must perform a large number of internal tasks,
such as accounting, sales or human resource management.
Nevertheless, we will regard it as specialised if the number of services 
it offers to \emph{external users} is small (and well-specified).


\subsection{Defining the notion of task and its granularity}
%\begin{itemize

Providing a single definition of what a task is can be tedious. We can start by trying to understand its centrality in automation and in the more general context of work (which inspires our thinking on specialisation), in division of labour.

In a work context, a task can be considered as an action associated with goals, means and conditions of execution. Within the context of prescribed work, a task corresponds to all the goals and procedures defined in advance, and meets codes, performance requirements, and quality standards. The elements of prescription - even though they may be \emph{underdeveloped} - are generally found in all work activities. Various institutions, professional hierarchies, public authorities or professional groups set tasks, objectives, procedures, directives, rules, and decrees defining what can be done or must be done.
Social sciences of labour have repeatedly shown by means of empirical studies that what workers, managers and even executives do is not simply the execution or application of the task prescription. Everything they do systematically goes beyond the divide between ``task'' (what is prescribed) and ``action'' (what is done). In the language of computer science, these may be translated as ``specification" and ``code execution".

When we look at the history of the organisation and rationalisation of work, particularly through the emblematic examples of Taylorism \cite{littler1978understanding}, Fordism \cite{watson2019fordism} and Toyotism \cite{dohse1985fordism}, all of these paradigms have involved defining and sorting out specific tasks, selecting workers to carry them out, and more generally (over)specialising work and then automating it. For the past two centuries, automation has been the hallmark of work transformation. Broadly speaking, it starts with an agent's craft know-how. Automation then consists of extracting the skills from the agent, and of encoding them into a procedure,
i.e. a series of simple, structured and repetitive tasks be they physical or cognitive.
In debates about the ``future of work''~\cite{brynjolfsson2017can}, jobs that are identified as least likely to be automated are those whose tasks are unstructured, non-routine, and whose performance is associated with critical thinking, long chains of reasoning or complex planning, a certain level of creativity, etc.
Arguably, this is because the resolutions of such tasks are hard to encode into procedures, though the rise of machine learning opens the door to the encoding of procedures that are humanly hard to describe, e.g. in the Kolomogorov-Solomonoff sense (see Section~\ref{sec:solomonoff}).

Automation - the creation of machines and algorithms to carry out tasks previously performed by humans - is a constant reconsideration of division of labour. To quote Karl Marx (himself echoing Charles Babbage), machines emerge as a synthesis of the division of labour~\cite{marx2019misere}. So the notion of task is at the heart of automation. An important question is to know what level of definition and operations to adopt in order to automate tasks and, ultimately, specialisation. There are limitations that make this exercise complicated. 
A prominent challenge is to select the granularity of a task (and thus its level of specialisation). There are informational and computational hurdles that make this exercise complicated. For example, there are many tasks that we understand tacitly and carry out without being able to state their explicit rules and procedures. In other words, formulated by Polanyi's paradox \cite{autor2014polanyi}, ``we can know more than we can tell’'~\cite{polanyi2009tacit}.
Moreover, automating certain tasks and not others also means renouncing possible ways of working and other possible uses~\cite{simondon1958mode}. 



Defining a task can also refer to expressions such as granularity or modularity. We can define granularity and modularity using the definitions proposed by Benkler \cite{Benkler} which are the basis of his socio-economic production model ``Commons-based peer production''. Modularity is a way of dividing a project into smaller components (modules) that can be developed independently before being assembled to form a whole, in order to maximise the flexibility and autonomy of the contributors. Granularity refers to the size of the modules, defined in terms of the time and energy required by the participants to produce them. It is important to specify the modules and their quantity in a project, because the interest and investment of the participants depend on it. As we will see, the two concepts are also central in computer science and system engineering.



