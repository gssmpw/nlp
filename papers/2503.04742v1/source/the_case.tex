\section{Why specialise?}
\label{sec:prospecialisation}

Now that we have highlighted the limits of the arguments \emph{against} specialisation,
we turn our attention to our case \emph{for} specialisation.

\subsection{Large models are more vulnerable}

In the context of artificial intelligence,
generality is obtained by training large models on massive web data crawls.
Indeed, ever larger models and datasets have been empirically observed to
yield ever more spectacular performances~\cite{DBLP:conf/nips/BrownMRSKDNSSAA20,DBLP:journals/corr/abs-2001-08361,DBLP:journals/corr/abs-2407-21783}.
However, these high performances seem inevitably entangled with a number of security issues~\cite{el2022impossible,oprea2023adversarial},
like privacy violations, jailbreaking and data poisoning.
Below we detail the reasons of this entanglement.

\subsubsection{The more parameters, the more vulnerabilities}

There is now a large literature that shows in particular that
the number of parameters of a learned model increases its vulnerability,
especially when it comes to privacy~\cite{DBLP:conf/compgeom/KattisN17} 
and poisoning~\cite{DBLP:conf/icml/MhamdiGR18,hoang2024poison}.
Intuitively, this is because the leading solution for privacy,
namely \emph{differential privacy}~\cite{DBLP:conf/tcc/DworkMNS06},
requires adding a bit of noise to all parameters when updating the model,
in order to correctly blur any trace of the data that led to this updating.

Similarly, to defend the model against malicious training data injections,
the leading solutions essentially boil down to outlier removal.
However, in high dimension (i.e. high number of parameters),
random (honest) data are scattered away,
which give poisoners a lot of room to bias the training 
without appearing to be out of distribution.

In the case of jailbreaking~\cite{DBLP:conf/icml/GuoYZQ024},
a key variable is the input size of the model,
also known as the \emph{context window} for language models.
Intuitively, the larger this input size, 
the more the model can be tuned for specific applications via input injection 
(prompting in the case of language models).
But then, the greater the \emph{attack surface} for jailbreaking attacks~\cite{anil2024many}.


\subsubsection{The more generality, the more data heterogeneity, the more vulnerability}

There is an additional subtler way through which the quest of generality harms resilience to attacks.
Namely, to gain capabilities in a wide variety of use cases, 
the model needs to be trained on a highly heterogeneous set of training data.
Such data are typically obtained through web crawls~\cite{baack2024critical},
which include dubious sources~\cite{schaul2023inside}.

In particular, under such high data heterogeneity,
sensitive and malicious data are significantly harder to identify and remove.
More rigorously, many papers have drawn a clear connection
between machine learning vulnerability and data heterogeneity~\cite{el2021collaborative}.



\subsection{Arguments from complex system engineering}

A fundamental and ubiquitous principle of complex system engineering is \emph{abstraction} \cite{DBLP:conf/iwssd/Shaw89}.
This corresponds to hiding the complexity of the system,
and to highlight instead the key features of the system
as well as the limited number of ways through which it can be used.
Perhaps, the most important example of such an abstraction is the creation of programming languages.
Such languages allow programmers to define how they may get machines to do what they want,
without having to care about the precise ways their programs will lead to
transistor operations on the machines.

Abstraction is especially instrumental to implement 
the principles of \emph{modularisation}
and the ``separation of concerns"~\cite{parnas1972criteria,tarr1999n}.
Modularisation consists of dividing a complex systems 
into interacting \emph{specialised} components, called \emph{modules}.
By carefully defining the features of the modules,
which amounts to defining the abstraction they must comply with,
the development of each module can then be performed independently from other modules.
Similarly, each module can be evaluated, stress-tested and even correctness-proved
independently from the rest of the complex system.

\emph{Separation of concerns} consists of segmenting a computer program into several parts. Each of these parts is isolated and takes charge of a piece of concern or information from the general problem being dealt with. This practice simplifies the development and maintenance of computer programs. A good, strict separation of concerns means that different parts of the code can be reused or modified independently, or that work can be done on one part of the code without having to know the other parts.

Modularisation is a fundamental technique of computer science
that strongly highlights the value of \emph{specialisation}.
For instance, the hashing function SHA-256~\cite{penard2008secure} is highly specialised.
However, it is this amount of specialisation that has facilitated its thorough study for decades,
and that has made it a core module of virtually all modern complex software systems.

Better yet, clearly defining the features of each module of a complex system
helps identify the module's \emph{least required privileges}~\cite{saltzer1975protection}.
Thereby, optimized security constraints can be enforced to the complex system's modules.
Typically, an AI model used in inference mode should not be given access to the Internet,
nor to the file systems, the webcam or the keyboard.
It should only be given the input data, and the required computing hardware to perform its task.
Thereby, if a given module is flawed, hacked or backdoored, 
then the scale of the harm to the complex system will then be limited to the privileges that were given to the module. This safety oriented mindset resembles two social organisation principles: \emph{separation of powers} and \emph{Subsidiarity}. Separation of powers is the foundation of constitutional states and democracies \cite{locke1690}, \cite{de1989montesquieu}, and prevents the concentration of legislative, executive, and judicial powers in the hands of a single individual or political group. The separation of powers is not applied in the same way in all countries \cite{bellamy2017rule}. The stricter it is, the more distinct, specialised, and separate the powers are, while retaining reciprocal means of action. The more flexible, the more the different powers work together. Subsidiarity is associated with decentralisation. Inspired by the social doctrine of the Catholic Church, subsidiarity \cite{evans2014global} is a political principle that assigns responsibility for a problem to the lowest competent level of public authority. This means looking for the level that is most relevant and closest to the people affected by a decision. In this way, the higher level is only called upon if the problem to be dealt with exceeds the lower level, and what can be done with the same efficiency at a lower level must be done without a higher level.

Even with limited privileges, a module may still be very dangerous.
This is typically the case if its output is not carefully sanitized 
by the subsequent algorithmic (or human) modules.
For instance, if the output of a language model is used directly for email response,
then the language model can be exploited to spread a spam worm~\cite{DBLP:journals/corr/abs-2403-02817}.
Likewise, a recommendation algorithm can threaten democracies by merely suggesting content,
if human populations enact upon the hate speech that the algorithm amplifies.
Tricking other modules can thus yield \emph{privilege escalation}~\cite{ozdemir2024privilege}.
But crucially, by carefully specialising the modules,
it will be much easier to then reason about the risks of their usage.

Finally, the careful modularisation of a complex system can increase its availability
by implementing the interoperable redundancy of each module,
with diverse implementations of the module.
The simplest example of this is the case of storage replicas.
However, more inspiring systems with interoperable modules have been designed for complex tasks.
Recently, the AT Protocol proposes to segment the tasks of a social media,
with interoperable modules such as data storage, 
published message collection (know as ``relays"),
message labeling,
feed generators
and client-side applications (known as ``AppViews")~\cite{kleppmann2024bluesky}.
By encouraging and facilitating the creation of interoperable modules managed by other entities,
the protocol helps reduce the risks of social media network effects~\cite{dou2013engineering}.
Similarly, \cite{solidago} proposes a modularisation of collaborative scoring,
as a way to facilitate the construction of numerous interoperable subtasks.

All of these elements naturally bring to mind the notion of \emph{decentralisation}, studied in various fields (computer science, political sciences, management science, law, public administration, economics, etc.) with different connotations. For example, in social sciences and particularly political sciences \cite{treisman2007architecture}, it usually refers to delegation of power, transfer of skills and resources from a central power to authorities/local communities distinct from it. 



Most of the notions we have mentioned (subsidiarity, decentralisation, separation of concerns and separation of powers) are sometimes used interchangeably with specialisation, and are involved in positive feedback loops with specialisation and while not delving into the interdependence between these notions and specialisation in this work, we highlight this noteworthy relationship that makes many of the arguments made in our paper at least partially valid for subsidiarity, separation of concerns, separation of powers and their alike, with maybe the notable exception of decentralisation. %which we discuss in the next subsection.
Decentralisation, and generally delegation of power, also offers some level of generality. For instance, components of a decentralised learning system can be learning all the same tasks, just as states in a federal system all have prerogatives on most administrative and daily life needs. Decentralisation and delegation offer advantages of their own, such as isolation of faults or confinement of corruption to preserve the overall system.


\subsection{Arguments from economics}

\subsubsection{Specialisation increased competitiveness}

The notion of specialisation is often associated with \textit{division of labour}, which has even been a major theme in philosophy and economics for centuries (\cite{marx1867}). In particular, division of labour should not be reduced to its technical dimension. Namely, the interdependence it generates between individuals within a society or on the scale of several has led to considering the division of labour as a important brick in the foundation of societies. 

While the division and specialisation of labour within human societies can be dated back to the Neolithic period, they rather started being discussed in the 16th century. Their formalization in a systematic thinking is often associated with the writings of \cite{smith1776}, who explored the logic behind the fragmentation of work and the specialisation of tasks, as well as the links between division of labour and market competition. Smith used the example of a pin factory to illustrate the division of labour, with a work decomposed into eighteen operations required to produce one unit. He notes that each step of work fragmentation increases productivity. Unlike craftsmanship, where the craftsman controls the entire production process, manufacturing is superior in terms of productivity, due in particular to the simplification of tasks, the reduction of downtime, and control over the pace of work. 
He does, however, point out one limit to the division of labour: the size of the market. If the market is not big enough, there will be no outlet for the surplus production resulting from an ever-increasing division of labour.

While Smith focused on the specialisation of workers on a single production line, still within classical political economy, \cite{ricardo1817} examined the division of labour on a global scale. 
With his theory of comparative advantage, Ricardo analysed the specialisation of national economies as they opened up to trade, using the famous example of England and Portugal and their production of wine and cloth. Ricardo's theory has later been further developed in numerous works on the international division of labour and international specialisation (\cite{samuelson1948international, HeckscherOhlin}). 

Note that, while Smith focused on human agents, Ricardo and others have already stressed the value of specialisation for non-human agents,
by arguing that it increases economic competitiveness and global production. Indeed, the theory of comparative advantage asserts that,
perhaps surprisingly, a less effective agent could still be helpful to a more effective agent, if the less effective agent specializes in tasks that they perform best, even in cases where the more effective agent performs better at these tasks.

\subsection{Arguments from the sociology of work, occupations, and organisations}

\subsubsection{Specialisation facilitates intermediate bodies}
    
Beyond the considerations of economic competitiveness,
% Focusing more on the human factor, 
Durkheim put forward a thesis\cite{durkheim1893division} asserting that the division of labour is the source of the social bond in industrial societies. He even made the study of changes in the division of labour one of the foundations of sociology. The division of labour is, he says, a source of “organic solidarity” characterized by differentiation, cooperation and strong interdependencies between individuals, as opposed to mechanical forms of social organization - individuals grouped together in communities on the basis of proximity, authority or faith. Aware of working-class misery and social conflicts, Durkheim advocated in the preface to the second edition of his book (1902) for a greater application of the division of labour to social organization, through the creation of intermediate bodies constituted on a professional basis, so that they could play the role of moral authorities arbitrating social conflicts.

Such arguments apply not only to humans, but also to human organisations. By constituting intermediate bodies, such non-human agents can better defend their cause, define industry norms and share good practices to self improve. Specialisation thus helps similar non-human agents build more resilient networks.

In software development too, intermediate bodies have emerged and have been essential to define industry norms,
and to increase the interoperability of various public and/or private software solutions.
Some prominent examples include the \emph{World Wide Web Consortium (W3C)}, 
the \textit{Internet Engineering Task Force (IETF)}
or the \emph{Web Hypertext Application Technology Working Group (WHATWG)}, 
which have shaped Internet and Web protocol standards,
thereby facilitating and securing the work of all companies 
that develop, exploit and depend on the Internet.


\subsubsection{The (positive) social chain reaction of specialisation}

By its psychological and moral dimensions \cite{hughes1951mistakes}, specialisation involves social interactions. Thus, specialised tasks arising from division of labour are part of extended and complex ensembles involving professionals, as well as non-professionals \cite{hughes1956social}. Organized or not, they develop and defend (opposing) visions of what work should be. In this way, specialisation becomes part of social interactions during which legitimacy, monopolies, competition, and everything that delimits professional territories, are discussed. Specialisation is not fixed. It depends on interactions between stakeholders, and its limitations fluctuate. In the context of general AI, particularly generative AI promising to replace a considerable amount of professions and create new ones, adopting this vision of specialisation and division of labour which is not only technical, but also social, moral and psychological, would make it possible to anticipate and support emerging specialities.

\subsubsection{A simplified lesson from company towns}

In an era where AGI is increasingly becoming the proclaimed goal of AI development, and when the most powerful billionaire are talking about the ``everything app''\cite{ArsTechnica}, it might be worth considering the 19th and 20th century use cases of company towns~\cite{garner1992company}. As defined in Wikipedia, ``a company town is a place where all or most of the stores and housing in the town are owned by the same company that is also the main employer. Company towns are often planned with a suite of amenities such as stores, houses of worship, schools, markets, and recreation facilities''. As a non-specialised, generalist, non-human entity; or as a general intelligence to stay in the parlance of AI, company towns can be seen as dealing with needs which are not limited to the sphere of work (in this case, postal, school, health, food, etc. services). Linked to what is known as ``industrial paternalism"~\cite{noiriel1988patronage}, this particular type of work management is associated with organisational problems. For example, the inability to provide all the services promised to workers, given that in fact, this system designed for blue-collar workers, mainly offered its advantages to white-collar workers. Another example is the difficulty of supervising workers, because it is impossible to be efficient by playing the roles of employer, priest, carer, policeman, etc., all at the same time, and therefore to bypass the division of labour and specialisation. Over and above the untenable nature of this strict social control of workers for reasons of labour profitability, this model of work organisation, based on task and power concentration, has not lasted. The reasons for the collapse of this system arguably include the replacement of the State by the management of the company town, the replacement of national law by the company practices, and most importantly, the replacement of the centuries-long process of specialisation of tasks and activities, by the one-company-do-it-all mindset of company-towns.

%\subsection{Specialisation and its friends}

%Various notions can be associated or even defined as causes or consequences of specialisation. We list some of them below. 

%\begin{itemize}
    %\item Separation of concerns. Derived from the field of software engineering, this principle consists of segmenting a computer program into several parts. Each of these parts is isolated and takes charge of a piece of concern or information from the general problem being dealt with. This practice simplifies the development and maintenance of computer programs. A good, strict separation of concerns means that different parts of the code can be reused or modified independently, or that work can be done on one part of the code without having to know the other parts.
    
    %\item Decentralisation. Decentralisation is studied in various fields (computer science, political sciences, management science, law, public administration, economics, etc.) with different connotations. In social sciences and particularly political sciences \cite{treisman2007architecture}, it usually refers to delegation of power, transfer of skills and resources from a central power to authorities/local communities distinct from it.
    
    %\item Separation of powers. Specialisation is also to be found in the principle of separation of powers, the foundation of constitutional states and democracies. This principle, formulated by John Locke \cite{locke1690} and theorised by Montesquieu (1748) \cite{de1989montesquieu}, prevents the concentration of legislative, executive, and judicial powers in the hands of a single individual or political group. The separation of powers is not applied in the same way in all countries \cite{bellamy2017rule}. The stricter it is, the more distinct, specialised, and separate the powers are, while retaining reciprocal means of action. The more flexible, the more the different powers work together.

    %\item Subsidiarity. In terms of political organisation, subsidiarity is associated with decentralisation. Inspired by the social doctrine of the Catholic Church, subsidiarity \cite{evans2014global} is a political principle that assigns responsibility for a problem to the lowest competent level of public authority. This means looking for the level that is most relevant and closest to the people affected by a decision. In this way, the higher level is only called upon if the problem to be dealt with exceeds the lower level, and what can be done with the same efficiency at a lower level must be done without a higher level.

%\end{itemize}

%The common thread between all of these notions (and other similar notions) is specialisation. The latter both produces, reinforces and is produced and reinforced in the presence of subsidiarity, decentralisation, separation of concerns and separation of powers. Naturally, most of these notions are sometimes mentioned interchangeably with specialisation, and are involved in positive feedback loops with specialisation and while not delving into the interdependence between these notions and specialisation in this work, we highlight this noteworthy relationship that makes many of the arguments made in our paper at least partially valid for subsidiarity, separation of concerns, separation of powers and their alike, with maybe the notable exception of decentralisation. %which we discuss in the next subsection.

%\paragraph{Scaling down without specialisation.}

%Decentralisation, and generally delegation of power, also offers some level of generality. For instance, components of a decentralised learning system can be learning all the same tasks, just as states in a federal system all have prerogatives on most administrative and daily life needs. Decentralisation and delegation offer advantages of their own, such as isolation of faults or confinement of corruption to preserve the overall system.