\section{A Case for Specification}
\label{sec:specification}

Closely related to specialisation,
\emph{specification} consists of precisely defining what a module excels at
(and what it cannot perform safely).
We dedicate a section to specification, 
as the value of specialisation is strongly tied to that of specification.


\subsection{Documentations}

The most straightforward step towards specification is \emph{documentation}.
To guarantee that any module or organisation is doing what it is supposed to be doing,
and more importantly that it is used as it is meant to be used,
it is essential to provide documents that describe the key features of the module or organisation.
Most products sold in developed countries must in fact be accompanied with such a documentation.

In practice, documentation may be in conflict with the quest for ready-to-play products.
In particular, viral adoption is often dependent on the ease with 
which the products can be used without relying on documentation.
What fraction of the population has read ChatGPT's documentation?
The flip side of this ease of use is unfortunately that such products are more likely to be misused,
e.g. to be deployed in applications where they are ill-suited.

In the context of machine learning,
increased calls for documentation have arisen, 
especially in the case of machine learning models through model cards~\cite{mitchell2019model} 
and in the case of datasets through data sheets~\cite{gebru2021datasheets}.


\subsection{Type systems and proofs of correctness}

However, careful specification can yield much more significant improvements,
especially with regards to the resilience and security of complex systems.
In particular, modern programming languages provide 
sophisticated type systems~\cite{cardelli1996type,matsakis2014rust,gaher2024refinedrust},
which allow to directly encode the specifications of a module,
and mathematically prove that the module correctly implements the specifications
at compilation time.
Crucially, this allows to catch bugs \emph{before} deployment.

Moreover, type systems prevent misuses of a module,
for instance by specifying the nature of the inputs that the module allows.
Better yet, rich type systems can determine the guarantees that a module can provide,
given some properties of the inputs that are fed to it.

Unfortunately, learning systems do not lend themselves well to such specifications,
which hinders a thorough bug-catching procedure ahead of deployment.
Arguably, this might be why bugs in learning systems have often been rebranded as \emph{hallucinations}
in the context of generative algorithms.
But fundamentally, their ubiquity could be traced to the lack of specifications and of means 
to construct solutions that (provably) verify the specifications.

Now, it is noteworthy that type systems allow verification at compilation time, given the source code.
In practice, an additional challenge is to verify the correctness of a program,
given its compiled binary code.
To meet this challenge, the fascinating domain of \emph{verifiable computing}~\cite{ahmad2018primitives}.
In addition to multi-party computations~\cite{garg2022two},
where carefully specified operations and communications enable more secure information processing,
verifiable computing leverages powerful cryptographic primitives
like \emph{Succinct Non-Interactive Arguments of Knowledge} (SNARK)~\cite{thaler2022proofs},
which enable a powerful computer to prove the soundness of its output to a weak verifier.

Recently, there has in fact been much progress towards verifiable machine-learning computing,
at inference time~\cite{fan2024validcnn} (after deployment) 
and at training time~\cite{fan2024vericnn} (before deployment).
In particular, these solutions allow to envisage 
the effortless enforcement of some existing laws (like EU's AI Act),
e.g. by demanding that all commercialized AI systems prove 
that they were trained on legally obtained data 
(i.e. excluding copyrighted, sensitive or error-prone data).
In fact, remarkably, the proofs may be constructed in \emph{zero-knowledge},
i.e. without disclosing non-legally-binding proprietary data.

Nevertheless, verifiable computing for machine learning will remain necessarily restricted
to the characteristics of a learning systems that can be humanly \emph{specified}.
While this includes important aspects (e.g. training or inference integrity),
this inevitably excludes other considerations 
(e.g. ``correct" hate speech moderation).


\subsection{Hypertelia and the pitfalls of automated task specification}   

In biology, hypertelia \cite{wenn1874154} designates an exaggerated growth of certain organs in relation to their function, to the point of making them annoying for the animal and its entire species (cf. canines of the saber-toothed tiger, too heavy antlers of the deer or disproportionate tail of the peacock). Taken up in philosophy by Simondon \cite{simondon1958mode,simondon1980mode} for his analysis of technical objects, the notion expresses in this context the idea of an exaggerated specialisation, and a functional over-adaptation of technical objects. Adapting an object too much to a particular purpose and context can result in its inability to function well. Tools adjusted to very specialized circumstances can lose autonomy outside of their specific technical environment. 
Specialisation must be guided by proven mechanisms and not be an end in itself. If guided by human choices, these can be inspired by the specialisation that has taken place in the professions. The challenge of good specification will arise more intensely when the granularity of specialisations is finer, which is enabled by automation. This is where a risk of hypertelia can occur, without being easily relieved. The challenge here is to determine who or what meta-algorithm defines the speciality of each sub-algorithm.

\subsection{Two limits of specifications}
\label{sec:solomonoff}

Unfortunately, many critical information processing tasks 
(e.g. content recommendation, email drafting, language translation)
are extremely hard to specify.
We highlight more generally two reasons why not all systems can be fully specified.

First, some tasks are too complex to specify, 
in the sense of the Kolomogorov-Solomonoff complexity~\cite{solomonoff1960preliminary,kolmogorov1963tables}.
Formally, a formalized specification is a program that, given any program,
returns whether the program verifies the specification.
The Kolomogorov-Solomonoff complexity is defined as the shortest program 
that implements this formalized specification.
Unfortunately, it is conjectured that many tasks require extremely complex specifications,
in the sense that they cannot be formally described in less than a million lines of codes.
As an example, EU's AI Act is 144 pages long.
Yet, it is clearly far from formalized and has been argued to be very incomplete~\cite{laux2024trustworthy}.
Another example is today's web standards, 
which correspond to very long documents that specify languages like HTML, CSS and ECMAScript, among others.

Second, some specifications are extremely hard to agree on.
This is typically the case of content moderation,
but it also holds for text autocompletion, biasless image generation or content recommendation.
Given additionally the lack of knowledge 
about the human population's distribution of specification preferences,
and perhaps even about one's own preferred specification,
a choice of specification may seem premature and inappropriate.
This is one heart of the challenges surrounding ``AI alignment"~\cite{hoang2019fabuleux}.

In the absence of concise and clearly consensual specifications,
should systems still aim to be specified?


\subsection{Specifying unspecifiable tasks through specified governance}

We stress that the specification challenge is an old problem.
Throughout centuries, the correct punishment of a convicted felon has been extremely hard to specify.
Nevertheless, this does not mean that we should give up on the specification effort and, e.g.,
abandon the choice of punishment to an all-powerful judge or dictator.

Instead of specifying the correct punishment, 
our societies have worked hard on specifying \emph{how} to specify the correct punishment.
This may be called the problem of the specification of the \emph{governance} of specifications.
Democracies have typically solved it by writing constitutions.
which specified how any modification of the law could be enforced.
Similar governance structures are arguably needed for organizations and algorithms
whose goals are partially open-ended (and thus under-specified).

Remarkably, a growing line of research has provided new solutions for \emph{algorithmic governance}.
In particular, WeBuildAI~\cite{lee2019webuildai} proposed a software 
which allow a number of stakeholders to collaboratively select
the recipients of food donations.
This software was itself subject to both informal and formal specifications,
e.g. both donors, recipients, volunteers and the organising nonprofit association
should all have some voting power on the decision,
or they may either use the software's learning-based system to construct a model of their preferences,
or write themselves a model of their preferences.
This system has subsequently inspired other algorithmic governance systems,
e.g. for trolley dilemmas~\cite{noothigattu2018voting},
kidney donation~\cite{freedman2020adapting}
content recommendation~\cite{hoang2021tournesol},
proposal prioritization~\cite{small2021polis}
and contextual note selection~\cite{righes2023community}.

In fact, \cite{solidago} aims to clarify the challenges of the specification
of the algorithmic collaborative governance of the scoring of any set of alternatives.
The list of subtasks to better specify includes 
participant verification,
trust propagation through a web of trust,
preference generalization,
and secure preference aggregation,
among others.

