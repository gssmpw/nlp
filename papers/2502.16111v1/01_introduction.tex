\section{Introduction}
\label{sec:introduction}

Effective planning is a crucial component for systems designed to solve complex real-world problems \citep{hao-etal-2023-reasoning, zhao-etal-2023-explicit, wang2024sibyl, jiao-etal-2024-learning, wang2025planning}. Traditional planning approaches, which rely heavily on template-based methods \citep{guan2023leveraging, valmeekam2024planbench, wang2024promptagent}, often lack generalizability and fail to capture the nuances of real-world tasks. In contrast, natural planning with LLMs offers a more promising direction, aligning better with real-world planning scenarios such as a trip or meeting planning \citep{zheng2024natural}. Furthermore, \citet{wang2025planning} shows that planning in natural language helps solve practical problems such as code generation. Thus, we aim to enhance LLMs' ability to generate effective natural plans and demonstrate their usefulness in solving downstream reasoning tasks within the scientific and financial domains. For the scope of this study, ``planning'' refers to the ability to decompose tasks and reason strategically to achieve solutions.

%In recent years, reasoning
In recent years, LLM agents have shown impressive abilities to solve complex reasoning problems \citep{yao2023react, xiao2024chainofexperts, wang2024survey}. Orthogonal to this exploration, scaling a search space during inference-time (i.e., test-time scaling) \citep{snell2024scaling, welleck2024decoding} has gained popularity in tackling difficult problems such as mathematical reasoning \citep{zhang2024accessing} and code generation \citep{wang2025planning}. Despite the success of these frameworks, we hypothesize that they often struggle with complex planning problems due to the lack of better verification module, and a failure to account for instance-level complexity across single-task. Furthermore, although some initial explorations exist \citep{bohnet2024exploring, lee2025evolving}, effectiveness of these frameworks for natural planning is under-explored (extended related work is presented in App. \ref{sec:related_works}). Motivated by these, we proposed \plangen{}, a model-agnostic, easily scalable, multi-agent framework for effective natural plan generation. 

\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{images/teaser_up_2.pdf}
    \caption{Schematic representation of \plangen{} (Mixture of Algorithms). An initial plan and constraints guide iterative plan refinement. The verification agent provides reward scores for plan quality, and the selection agent chooses inference algorithms until the highest-reward plan is found and used for downstream reasoning (if needed). UCB: Upper Confidence Bound, BoN: Best of $\mathcal{N}$, ToT: Tree-of-Thought, RS: REBASE.}
    \label{fig:teaser}
\end{figure*}

\plangen{} consists of three specialized agents: \textit{constraint agent}, \textit{verification agent}, and \textit{selection agent}. The constraint agent extracts instance-specific constraints (e.g., budget, concepts, rules, etc.); the verification agent evaluates plan quality and provides a reward score considering the constraints; and the selection agent dynamically chooses the best inference algorithm using an improved Upper Confidence Bound (UCB) formula \citep{han2024ucb} for instance of different complexity. We explore popular and widely used three inference algorithms within \plangen{}: Best of $\mathcal{N}$ \citep{brown2024large}, Tree-of-Thought (ToT) \citep{yao2024tree}, and REward-BAlanced SEarch (REBASE) \citep{wu2024empirical}. We combine our agents with these algorithms, yielding four frameworks: (1) \plangen{} (Best of $\mathcal{N}$), (2) \plangen{} (ToT), (3) \plangen{} (REBASE), and (4) \plangen{} (Mixture of Algorithms).  In \plangen{}, we use ``Multi-Agent'' approach which signifies using the constraint and verification agents for the first three approaches, and all three agents for the ``Mixture of Algorithms'' (Figure \ref{fig:teaser}). Figure \ref{fig:teaser} shows example from NATURAL PLAN (Calendar scheduling), and App. \ref{app:examples} provides more examples.

To evaluate \plangen{}, we perform all experiments using Gemini-1.5-Pro \citep{team2024gemini} as underlying model. We further present case-study on Gemini-2.0-Flash, and GPT-4o \citep{hurst2024gpt} to show the model-agnostic nature. We evaluate natural language planning ability on NATURAL PLAN \citep{zheng2024natural}, scientific/mathematical reasoning on GPQA \citep{rein2024gpqa} and OlympiadBench \citep{he-etal-2024-olympiadbench}, and financial reasoning on DocFinQA \citep{reddy-etal-2024-docfinqa}. Performance is compared against Zero-shot Chain-of-Thought (CoT) and a vanilla multi-agent baselines. We achieve state-of-the-art results on NATURAL PLAN ($\sim$8\%$\uparrow$ average across all categories), OlympiadBench (text-only) ($\sim$5\%$\uparrow$ on MATH, $\sim$4\%$\uparrow$ on PHYSICS), and DocFinQA ($\sim$7\%$\uparrow$). On GPQA, we outperform Gemini-1.5-Pro ($\sim$13\%$\uparrow$), GPT-4o ($\sim$12\%$\uparrow$), and Claude-3.5-Opus ($\sim$9\%$\uparrow$), while achieving competitive performance compared to the vanilla multi-agent baseline ($\sim$1\%$\uparrow$). Further analysis reveals that the simplest method (i.e., \plangen{} (Best of $\mathcal{N}$)) achieves the best performance on NATURAL PLAN (Figure \ref{fig:main_results}). \plangen{} (Mixture of Algorithms) achieves the best performance for complex problems (Figure \ref{fig:np_cal_analysis}) including GPQA, and OlympiadBench(MATH). We further conduct a thorough analysis of the results which reveals several important findings. In summary, our contributions are: (1) \plangen{}, a novel, model-agnostic, and scalable multi-agent framework for enhancing LLM natural planning; (2) SOTA results on several complex planning and reasoning benchmarks; and (3) a novel approach to constraint-based verification and instance-level complexity-based inference algorithm selection. 

%Our experimental results show significant improvements over strong baselines, particularly in complex reasoning tasks, demonstrating the \plangen{}'s ability to enhance planning and downstream reasoning across various domains. 

% Additionally, the framework is scalable and easily adaptable to various off-the-shelf LLMs. We believe this approach offers valuable insights for building improved LLM agents through inference-time scaling for complex planning and reasoning tasks.

%In this paper, we propose a scalable, and easy-to-integrate framework with any base LLM which contains various LLM agents, and inference-time algorithms. In this framework, we propose three different agents, i.e., \textit{constraint agent}, \textit{verifier agent}, and \textit{selection agent}. Along with this, we explore three widely used inference-time algorithms, Best of $\mathcal{N}$, Tree of Thought (ToT) \citep{yao2024tree}, and REward BAlanced SEarch (REBASE) \citep{wu2024empirical} with our feedback-based refinement. In this work, we not only adapt inference-time algorithms but also improve their capabilities using our LLM agents. Our framework consists of two key components: (1) modified versions of the inference-time algorithms that integrate the constraint and verifier agents, and (2) a dynamic selection mechanism where the Selection Agent iteratively chooses the best algorithm for each task instance based on complexity, using a combination of LLM-based reasoning and Upper Confidence Bound (UCB) selection. The overall pipeline is illustrated in Figure \ref{fig:teaser} where each inference-time algorithm is proposed variant. 

%For natural language planning, we use all three categories from the NATURAL PLAN benchmark \citep{zheng2024natural}. To evaluate scientific and mathematical reasoning, we leverage GPQA \citep{rein2024gpqa} and OlympiadBench \citep{he-etal-2024-olympiadbench}. Finally, for financial reasoning, we utilize DocFinQA \citep{reddy-etal-2024-docfinqa}. The performance of each method is compared against two baselines: Zero-shot Chain-of-Thought (CoT) and a Vanilla Multi-Agent System. The evaluation metrics for each datasets are adapted from their respective original publications. NATURAL PLAN is evaluated using Exact Match (EM) \citep{zheng2024natural}, OlympiadBench using micro-average accuracy \citep{he-etal-2024-olympiadbench}, GPQA and DocFinQA using accuracy \citep{rein2024gpqa, reddy-etal-2024-docfinqa}. 

% The results in Table 1 demonstrate the superiority of the proposed multi-agent framework over baseline methods across various benchmarks. In NaturalPlan, the Best of $\mathcal{N}$ approach achieved the highest scores in calendar (60.70) and trip planning (41.63), while the Mixture approach excelled in meeting planning (42.20). For complex reasoning tasks in OlympiadBench and GPQA, the Best of $\mathcal{N}$ method also led in MATH (54.90) and PHYSICS (31.36). In the DocFinQA financial reasoning benchmark, it achieved the highest accuracy (31.16). These findings highlight the framework's effectiveness in enhancing both planning and reasoning tasks through iterative refinement and optimal algorithm selection, outperforming traditional methods across diverse domains. \mihir{Add more concrete contributions and findings}


% With the emergence of reasoning models such as o1 \citep{zhong2024evaluation}, enabling human-like thinking in large language models (LLMs) to solve complex problems has gained significant attention from the research community. As scaling LLM parameters becomes increasingly challenging, the exploration of test-time scaling (i.e., scaling up the search space during inference) has gained popularity for tackling difficult problems like code generation and reasoning \citep{snell2024scaling}. As LLM capabilities progress, LLM-empowered agents have become a key component in many frameworks for solving complex tasks such as planning and reasoning \citep{xie2024revealing}. While there have been some efforts to explore inference-time scaling and LLM-based agents for these applications, the impact of test-time scaling in LLM-driven agent frameworks for solving planning and reasoning problems remains under-explored. To address this limitation, we propose a multi-agent framework that utilizes multiple inference-time scaling algorithms to enhance planning and downstream reasoning (shown in Figure \ref{fig:teaser}). 

% As illustrated in Figure \ref{fig:teaser}, the proposed method leverages multiple agents, each with specialized roles, to iteratively refine and optimize plans for complex tasks. The framework incorporates three key inference-time algorithms: Best of $\mathcal{N}$, Tree of Thought (ToT), and Reward-based Search (REBASE). These algorithms are integrated into a modular pipeline that begins with an initial plan generation, followed by iterative refinement driven by a suite of agents, including a Constraint Agent, Verifier Agent, and Selection Meta-Agent. The Constraint Agent extracts task-specific constraints to ensure feasibility, while the Verifier Agent evaluates the quality of generated plans based on predefined criteria. The Selection Meta-Agent dynamically selects the most suitable inference algorithm for each iteration, balancing exploration and exploitation to optimize plan quality. This iterative process continues until the plan meets a predefined quality threshold, ensuring robustness and adaptability across diverse planning problems.

%There are two major components in our proposed pipeline: (i) each inference-time algorithms have been modified to utilize constraint, and verifier agents giving us three different variants of these algorithms, and (ii) in our fourth variant, iterative update happens using selection agent where it dynamically selects the most suitable inference algorithm for each iteration, based on task-instance complexity. To measure the complexity and utility of various inference-time algorithms, selection agent consists of LLM-based and Upper Confidence Bound (UCB)-based selection. The later pipeline is shown in Figure \ref{fig:teaser}.

% To evaluate the effectiveness of the proposed framework, experiments were conducted on four distinct benchmarks: NaturalPlan, GPQA, OlympiadBench, and DocFinQA. These datasets were chosen to demonstrate the system's ability to handle a variety of planning and reasoning tasks, from natural language planning to domain-specific financial reasoning. The performance of the framework was compared against two baselines: Zero-shot Chain-of-Thought (CoT) and a Vanilla Multi-Agent System. The evaluation metrics varied by dataset, with NaturalPlan and OlympiadBench assessed using Exact Match (EM), while GPQA and DocFinQA were evaluated based on the accuracy of the final answer. The results showed significant improvements over the baselines, particularly in complex reasoning tasks, highlighting the framework's ability to enhance both planning and downstream reasoning capabilities across different domains.