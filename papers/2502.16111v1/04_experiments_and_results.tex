\section{Experiments and Results}
\label{sec:exp_and_res}

\subsection{Experimental Setup}

%We evaluate our system using four distinct benchmarks.

\paragraph{Datasets} 
To demonstrate improvement in natural planning abilities, we utilize the NATURAL PLAN \citep{zheng2024natural}. After improving the planning, we show that this significantly enhances the reasoning capabilities of LLMs on two benchmarks: GPQA \citep{rein2024gpqa} and OlympiadBench (text-only) \citep{he-etal-2024-olympiadbench}. Additionally, we show that \plangen{} improves performance on a domain-specific dataset, DocFinQA \citep{reddy-etal-2024-docfinqa}. Further details are presented in App. \ref{app:experiments}.

% Four figures in a 2x2 grid
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/np_results.pdf} % Replace with your image file
        \caption{\centering NATURAL PLAN}
        \label{subfig:natural_plan}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/olympiad_results.pdf} % Replace with your image file
        \caption{\centering OlympiadBench}
        \label{subfig:olympiad}
    \end{subfigure}
    
    \vspace{\baselineskip} % Add some vertical space between rows
    
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/gpqa_results.pdf} % Replace with your image file
        \caption{\centering GPQA}
        \label{subfig:gpqa}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/docfinqa_results.pdf} % Replace with your image file
        \caption{\centering DocFinQA}
        \label{subfig:docfinqa}
    \end{subfigure}
    
    \caption{Performance comparison of the proposed multi-agent frameworks against baselines across four benchmarks. All experiments are conducted using Gemini-1.5-Pro. Algo: Algorithms, MATH: Mathematics, PHY: Physics.}
    \label{fig:main_results}
\end{figure}


\paragraph{Baselines and Our Frameworks} 
We develop two baselines for comparison with our frameworks: (i) Zero-shot CoT \citep{zero_shot_cot} and (ii) a Vanilla Multi-Agent Baseline. In the Zero-shot CoT, we provide an input prompt to the model, which generates outputs in the form of <CoT reasoning, Answer>. For the ``Multi-Agent Baseline'', the same model is called iteratively across multiple iterations. The system repeatedly refines its outputs through feedback loops, where the feedback is generated based on a self-reflective prompt (App. \ref{app:experiments}) designed to improve reasoning. We evaluate all proposed frameworks (\textsection \ref{subsec:frameworks}) on all benchmarks. For reasoning tasks, we use a two-stage approach: (1) generating an optimized plan using our frameworks, and (2) executing the plan to produce the final answer (Figure \ref{fig:teaser}). App. \ref{app:experiments} presents further details on model selection, metrics, and experiment hyper-parameters including the hyper-parameter choices for inference-time algorithms.

% To evaluate our proposed method, we design two main experiments: (i) an evaluation of individual inference-time algorithms using only the proposed verification agent, and (ii) an evaluation of a multi-agent system incorporating both verification and selection agents.

\subsection{Main Results}

Figure \ref{fig:main_results} compares performance of multi-agent frameworks across various single-agent and multi-agent baselines (varies across benchmarks - some single-agent baselines for GPQA are obtained from \url{https://klu.ai/glossary/gpqa-eval}). From the results, it is evident that the multi-agent frameworks are consistently outperforming the baselines.

\paragraph{Performance on NATURAL PLAN}
From Figure \ref{subfig:natural_plan}, \plangen{} (Best of $\mathcal{N}$) achieves the highest EM scores across all tasks: $60.70$ (Calendar), $43.80$ (Meeting), and $41.63$ (Trip). In calendar scheduling, all four frameworks surpass the strongest baseline (Multi-Agent Baseline) by $\sim10\%$. For meeting and trip planning, all except ToT outperform the best baseline (Gemini-1.5-Pro) by $\sim6\%$ and $\sim7\%$, respectively. \plangen{} (Mixture of Algo.) achieves the second-highest performance in meeting and trip planning while remains competitive in calendar scheduling. These results demonstrate the effectiveness of our frameworks in handling diverse natural language planning tasks and establishing SOTA for all three categories of NATURAL PLAN.

\paragraph{Performance on OlympiadBench}
From Figure \ref{subfig:olympiad}, \plangen{} (Mixture of Algo.) achieves the highest accuracy in the MATH category ($55.94\%$), outperforming the strongest Multi-Agent Baseline ($50.68\%$) by $\sim5\%$. Notably, the superior performance of the \plangen{} (Mixture of Algo.) in MATH highlights its effectiveness in complex mathematical reasoning, setting a SOTA for the MATH. In the PHY category, all multi-agent frameworks surpass Gemini-1.5-Flash (strongest baseline), with \plangen{} (Best of $\mathcal{N}$) achieving the highest accuracy ($31.78\%$), setting a SOTA for the PHY.

\paragraph{Performance on GPQA}
From Figure \ref{subfig:gpqa}, the \plangen{} (Mixture of Algo.) achieves the highest accuracy ($59.6\%$). The individual inference-time algorithms achieve a lower performance, indicating the usefulness of selection. All proposed frameworks outperform Gemini-1.5-Pro ($46.2\%$), GPT models ($\sim48\%$), and Claude-3-Opus ($50.4\%$) by a large margin. While Claude-3.5-Sonnet, and Multi-Agent Baseline perform competitively ($\sim59\%$) compared to \plangen{} (Mixture of Algo.).

\paragraph{Performance on DocFinQA}
From Figure \ref{subfig:docfinqa}, our frameworks significantly improve performance on DocFinQA, with \plangen{} (Best of $\mathcal{N}$) achieving the highest accuracy ($31.16\%$) and F1-Score ($29.45\%$), setting SOTA for the task. All our frameworks outperform the Gemini-1.5-Pro (strongest baseline) by a large margin ($\sim7\%$). These results highlight the effectiveness of multi-agent frameworks in financial document understanding, and performing reasoning over them.

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    % \vspace{-7mm}
    \includegraphics[width=\linewidth]{images/analysis_1.pdf}
    % \vspace{-5mm}
    \caption{Performance comparison of inference-time algorithms across different complexity levels for calendar scheduling from NATURAL PLAN.}
    \label{fig:np_cal_analysis}
\end{wrapfigure}

\paragraph{Performance of our frameworks w.r.t. different complexity}
As shown in Figure \ref{fig:np_cal_analysis}, we conduct a case study on calendar scheduling task from NATURAL PLAN to analyze the impact of varying complexity levels on the performance of different frameworks. For the calendar scheduling, we observe that \plangen{} (ToT) performs best for simple problems, while \plangen{} (Best of $\mathcal{N}$) is more effective for intermediate problems. As complexity increases, a \plangen{} (Mixture of Algo.) proves to be the most effective approach. We further conduct a similar analysis for meeting and trip planning from NATURAL PLAN presented in App. \ref{app:analysis}.


%While certain methods work well for simpler tasks, more complex problems often require hybrid approaches or reveal the limitations of existing algorithms.

\paragraph{Main Findings} 
Compared to single-agent systems, multi-agent frameworks consistently outperform in generating optimized planning trajectories (Figure \ref{fig:main_results}). Furthermore, Multi-Agent (Baseline) is not always the strongest benchmark, as self-correction can introduce challenges as shown in \citet{huang2024large}. Thus, different agents within the system require distinct handling strategies similar to our \plangen{}. Additionally, even in multi-agent frameworks for \plangen{}, relying on a single inference-time algorithm proves insufficient for more complex problems (Figure \ref{fig:np_cal_analysis}). A \plangen{} (Mixture of Algo.) approach offers substantial advantages for solving complex planning problems, highlighting the importance of algorithm selection based on instance-specific complexity (Figure \ref{fig:teaser}). Given that our frameworks are multi-agent, we provide further discussion on $\#$ of LLM calls \textit{vs.} their performance in subsequent section.

%The NATURAL PLAN benchmark compares against Gemini-1.5-Flash, and Gemini-1.5-Pro. Among the proposed methods, the Best of $\mathcal{N}$ method achieves the highest scores in calendar planning (60.70 EM) and trip planning (41.63 EM), while the Mixture approach excels in meeting planning (42.20 EM). In contrast, the baselines fall short, with Gemini-1.5-Pro scoring 48.90 EM in calendar planning, while Gemini-1.5-Flash only reaches 34.30 EM.

% In OlympiadBench, which includes MATH and PHYSICS reasoning tasks, the baselines comprise Gemini-1.5-Pro and GPT-4-Turbo. The Best of N method outperforms these baselines, achieving 54.90 EM in MATH and 31.36 EM in PHYSICS, compared to GPT-4-Turbo's 50.68 EM in MATH and 23.54 EM in PHYSICS. Our propose frameworks establish new SOTA results across MATH and PHYSICS category. For the GPQA, baseline methods include Gemini-1.5-Pro, Claude-3-Opus, Claude-3.5-Sonnet, GPT-4o, and GPT-4-Turbo. Here, the proposed Mixture approach achieves the highest accuracy (59.60), outperforming Claude-3.5-Sonnet (56.66 accuracy) and GPT-4-Turbo (50.40 accuracy). This underscores the framework's adaptability and efficacy in reasoning tasks requiring multi-agent collaboration.

% In the domain-specific DocFinQA benchmark, the baselines include Gemini-1.5-Pro and Multi-Agent (Baseline). The Best of $\mathcal{N}$ method achieves the highest accuracy (31.16) and F1-score (29.35). This is a significant improvement over the baselines, with Gemini-1.5-Pro achieving 23.75 accuracy and 22.26 F1-score.

% The proposed methods consistently outperform the Multi-Agent (Baseline) across all benchmarks. In NaturalPlan, the Best of N method improves calendar planning (60.70 EM vs. 50.99) and trip planning (41.63 EM vs. 34.75), while the Mixture approach excels in meetings (42.20 EM vs. 38.64). For OlympiadBench, it achieves higher scores in MATH (54.90 EM vs. 50.68) and PHYSICS (31.36 EM vs. 28.35). In GPQA, the Mixture method leads with 59.60 accuracy (vs. 56.66), and for DocFinQA, Best of N achieves 31.16 accuracy and 39.29 F1-score (vs. 29.08 and 37.97).