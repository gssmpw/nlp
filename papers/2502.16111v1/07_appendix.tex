\section{Further Details on LLM Agents}
\label{app:llm_agents}

In this section, we provide additional details about each specialized agent in \plangen{}. We present the prompts used for each agent, highlighting their roles in the framework. The prompt for the constraint agent includes task-specific parameters that can be adjusted to extract relevant constraints for different tasks. In contrast, the prompts for the verification agent and selection agent are entirely task-agnostic, ensuring generalizability and adaptability across various problem domains.

\paragraph{Prompts for Constraint Agent}
The constraint agent is responsible for extracting problem-specific constraints that guide the planning process. To enable systematic extraction of constraints, we design a task-specific prompt for the constraint agent:

% enhanced,
% boxrule=0pt,frame hidden,
% borderline west={4pt}{0pt}{green!75!black},
% top=0pt,bottom=0pt,
% colback=green!10!white,
% sharp corners

\begin{tcolorbox}[boxrule=0pt, frame hidden, title=Prompt, breakable, sharp corners, borderline west={0pt}{0pt}{black!50}, title style={
        colback=black!50, % Example: Light gold title background
        colframe=black!50, % Match title frame to background
        coltitle=black % Black title text
    }]
You are an expert in understanding an input problem and generating set of constraints. Analyze the input problem and extract all relevant instance-specific constraints and contextual details necessary for accurate and feasible planning. 
\\
\\
(\hl{Optional}) These constraints may include:
\\

<You may provide any specific type of constraints>
\\

<You may provide any formatting instruction>
\\

\textbf{Input Problem:} <problem statement>
\end{tcolorbox}

Based on the above prompts, we define the types of constraints used in the NATURAL PLAN benchmark for different planning tasks: calendar scheduling, meeting planning, and trip planning. For DocFinQA, we provide a set of formatting instructions to ensure structured constraint generation. For GPQA and OlympiadBench, the constraint extraction follows the general prompt outlined above.

\paragraph{Prompts for Verification Agent}

The prompt for the verification agent is designed to be task-agnostic, meaning it can be applied across different problem domains without modification. By enforcing strict evaluation criteria, this agent enhances the reliability of \plangen{}, making it robust for various planning and reasoning tasks. In this prompt, list of constraints are generated using constraint agent. Notably, the list of constraints used in the verification prompt is dynamically generated by the constraint agent. This ensures that the verification process is based on instance-specific constraints rather than relying on predefined, static rules.

\begin{tcolorbox}[boxrule=0pt, frame hidden, title=Prompt, breakable, sharp corners, borderline west={2pt}{0pt}{black!50}, title style={
        colback=black!50, % Example: Light gold title background
        colframe=black!50, % Match title frame to background
        coltitle=black % Black title text
    }]

Provide a reward score between -100 and 100 for the quality of the provided plan steps, using strict evaluation standards. Ensure the reward reflects how effectively the plan contributes to progressing toward the correct solution.

\textbf{Problem Statement:}

\{problem\}

\textbf{Plan:}

\{plan\}

\textbf{Consider the following constraints while evaluating:}

- [Constraint 1]

- [Constraint 2]

- [Constraint 3]

\textbf{Provide feedback in the following format:}

[Step-by-step reasoning for the reward score]

\textbf{Score:} [Strictly provide an integer reward score between -100 and 100]
\end{tcolorbox}

\paragraph{Prompts for Selection Agent}

The prompt for the Selection Agent is task-agnostic, allowing it to be applied across various domains without modification. It processes feedback from the verification agent and contextual information from the problem statement to assign suitability scores to different inference-time algorithms.

\begin{tcolorbox}[boxrule=0pt, frame hidden, title=Prompt, breakable, sharp corners, borderline west={0pt}{0pt}{black!50}, title style={
        colback=black!50, % Example: Light gold title background
        colframe=black!50, % Match title frame to background
        coltitle=black % Black title text
    }]

Analyze the following planning problem and explain your reasoning for assigning priority scores to the algorithms based on their suitability. Scores should be between 0 and 1, where 1 represents the most suitable algorithm for the given problem.

\textbf{Problem Statement:} <problem statement>

\textbf{Requirements:} <feedback>

\textbf{Context:} <context if context else `None provided'>

Start by providing a brief reasoning for each algorithm's suitability based on problem complexity. Then, \textbf{ONLY output your response strictly as a list} with the exact format below:

\textbf{Reasoning:}
\begin{itemize}
    \item \textbf{Best of N:} [Explain why this algorithm is or isn’t suitable]
    \item \textbf{Rebase:} [Explain why this algorithm is or isn’t suitable]
    \item \textbf{ToT:} [Explain why this algorithm is or isn’t suitable]
\end{itemize}

\textbf{Scores:}
\begin{equation*}
\begin{aligned}
&[\text{("Best of N", float)}, \\
&\text{("Rebase", float)}, \\
&\text{("ToT", float)}]
\end{aligned}
\end{equation*}

\end{tcolorbox}

\paragraph{Algorithm for Selection using UCB}

The algorithm (Algorithm \ref{algo:selection}) presented is a modified UCB selection strategy that incorporates additional factors for exploration, diversity, and recovery. It initializes each algorithm with basic statistics like reward ($R(a)$), count of trials ($C(a)$), and recovery score ($Rec(a)$). The algorithm computes a normalized reward $\bar{R}_{\text{norm}}(a)$ for each option, balancing the reward with exploration ($E(a)$), which encourages trying less-used algorithms. A diversity bonus $D(a)$ penalizes overused algorithms, while a recovery bonus $RecB(a)$ rewards algorithms that perform well after prior failures. LLM-guided priors ($LLM\_prior$) are used to influence the selection process based on prior knowledge. The final selection is made by maximizing the UCB score, which combines these factors to balance exploitation and exploration.

\paragraph{Ablation Study on UCB Modifications}
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \vspace{-9mm}
    \includegraphics[width=\linewidth]{images/analysis_ucb.pdf}
    % \vspace{-5mm}
    \caption{Ablation Study of UCB Modifications on Selection Agent and its impact on Multi-Agent Mixture of Algorithms framework. div.: diversity bonus, rec: recovery term.}
    \label{fig:ucb_analysis}
\end{wrapfigure}
To design our selection agent, we conducted an ablation study evaluating modifications to the UCB formula, shown in Figure \ref{fig:ucb_analysis}.  Initially, we replaced the selection agent with a simple sequential strategy, termed ``Multi-Agent (Sequential)'', where algorithms execute in sequence, and the verification agent selects the highest-scoring plan.  Next, we implemented a UCB selection agent, but excluded the `diversity bonus' and `recovery term' introduced in our proposed formulation in the main paper, denoted as ``Multi-Agent (UCB w/o div. and rec.)''. Finally, we implemented the complete selection agent incorporating our proposed UCB, labeled ``Multi-Agent (UCB)''.  As shown in Figure \ref{fig:ucb_analysis}, the inclusion of the diversity bonus and recovery terms in the UCB formula ("Multi-Agent (UCB)") resulted in $\sim3.5\%$ performance gain compared to the UCB variant without these terms, further enhancing overall results. Note that the LLM-guided priors are still the part of Multi-Agent (UCB w/o div. and rec.) and Multi-Agent (UCB).

\section{Prompts for Proposed Frameworks}
\label{app:frameworks}

We provide further details in this section regarding the prompts used for \plangen{} (ToT) and \plangen{} (REBASE), as well as the specific algorithms used to execute these inference-time methods.

% \paragraph{Algorithms for Multi-Agent ToT and REBASE} 

% Here, Algorithms 2 and 3 detail the execution pipelines for Multi-Agent ToT and REBASE, respectively.

\paragraph{Prompts used for ToT and REBASE}

\plangen{} (ToT) and \plangen{} (REBASE) employ three prompt types: (1) step prompt, (2) step reward prompt, and (3) completion prompt. Step prompt guide the model to generate subsequent steps based on the problem statement and previously generated steps. Step reward prompt evaluate each intermediate step against the problem statement and constraints, similar to the prompts used by a verification agent. Completion prompt check for a complete solution after each step. If a solution is found, exploration terminates; otherwise, the process continues until a solution is reached.

\begin{tcolorbox}[boxrule=0pt, frame hidden, title=Step Prompt, breakable, sharp corners, borderline west={0pt}{0pt}{black!50}, title style={
        colback=black!50, % Example: Light gold title background
        colframe=black!50, % Match title frame to background
        coltitle=black % Black title text
    }]

You are an expert assistant for generating step-by-step plan to solve a given question using specified tools. Given the problem and any intermediate steps, output only the next step in the plan. Ensure that the next action helps in moving toward the correct plan to solve the given question. Do not provide the full plan. Keep responses concise, focusing solely on the immediate next step that is most effective in progressing toward the correct plan.
\\
\\
<problem>

\{Add a problem statement here\}

</problem>
\\
\\
<intermediate\_step>

\{Append previously generated steps\}

</intermediate\_step>
\end{tcolorbox}

\begin{tcolorbox}[boxrule=0pt, frame hidden, title=Completion Prompt, breakable, sharp corners, borderline west={0pt}{0pt}{black!50}, title style={
        colback=black!50, % Example: Light gold title background
        colframe=black!50, % Match title frame to background
        coltitle=black % Black title text
    }]

You are an assistant tasked with verifying if the final, complete plan to solve the given question has been achieved within the intermediate steps. Output only `1' if the intermediate steps contain the full solution needed to solve the question. If the full plan has not yet been reached, output only `0'. Provide no additional commentary—return exclusively `1' or `0'.
\\
\\
<problem>

\{Add a problem statement here\}

</problem>
\\
\\
<intermediate\_step>

\{Append previously generated steps\}

</intermediate\_step>
\end{tcolorbox}


\section{Details on Benchmarks and Experiments}
\label{app:experiments}

\paragraph{Statistics of Benchmarks}

For evaluation, we utilize evaluation sets of all four benchmarks. For NATURAL PLAN, we employed the provided evaluation sets, consisting of 1000 instances each for Calendar Scheduling and Meeting Planning, and 1600 instances for Trip Planning.  The GPQA evaluation was conducted using the Diamond set, which comprises 198 highly challenging instances.  From OlympiadBench, we selected the text-only problems, excluding those requiring a theorem prover, resulting in 674 instances for the MATH category and 236 for the PHY category. We also used 922 instances from the DocFinQA evaluation set.

\paragraph{Models}
Our primary evaluations use Gemini-1.5-Pro for all the experiments.  We also present a case study with Gemini-2.0-Flash and GPT-4o to showcase the model-agnostic nature of \plangen{}.

\paragraph{Metrics}
We use task-specific metrics for all evaluations. Specifically, we use Exact Match (EM) for NATURAL PLAN similar to \citet{zheng2024natural}, micro-average accuracy for OlympiadBench similar to \citet{he-etal-2024-olympiadbench}, and accuracy for GPQA and DocFinQA (along with F1-Score for DocFinQA).

\paragraph{Feedback prompt for Multi-Agent Baseline} In the multi-agent baseline, we employ a feedback prompt to iteratively generate improved and refined outputs. The prompt is provided below:

\begin{tcolorbox}[boxrule=0pt, frame hidden, title=Feedback Prompt, breakable, sharp corners, borderline west={0pt}{0pt}{black!50}, title style={
        colback=black!50, % Example: Light gold title background
        colframe=black!50, % Match title frame to background
        coltitle=black % Black title text
    }]

Analyze the following planning problem and explain your reasoning for assigning priority scores You are an intelligent assistant capable of self-reflection and refinement. I will provide you with your last response, and your task is to improve it, if necessary.

Here is your previous response:

\{previous\_response\}

Analyze and refine your response step-by-step:

1. Reflect on your reasoning process. Where might it be unclear or incorrect? Improve it.

2. Revise the explanation to address any identified issues and make it more logical and comprehensive.

3. Ensure the final answer is correct, supported by clear reasoning.
\end{tcolorbox}

\paragraph{Hyper-parameters for Experiments}
To ensure deterministic behavior, we set the temperature of all models to 0 for each agent.  For the inference-time algorithms, we used the following settings: \plangen{} (Best of $\mathcal{N}$) with five samples at a temperature of 0.7; Tree of Thoughts (ToT) with three children per root node, generated at a temperature of 0.7; and REBASE, initialized with width $10$ at temperature of 0.7, decremented by 1 after each call to expand.

\section{Additional Analysis}
\label{app:analysis}

\paragraph{Importance of Verification Agent}
The kernel density estimation (KDE) plot visualizes the distribution of reward values assigned to two distinct outcomes: ``Success'' (green) and ``Failure'' (red).  The plot reveals a clear separation between the reward distributions, with ``Success'' outcomes strongly associated with high reward values (around 80-100) and ``Failure'' outcomes primarily associated with low reward values (around 20-40). The sharply peaked green curve suggests consistent and high rewards for successful outcomes, while the broader red curve reflects more variability in rewards assigned to failures.  However, a small bump in the red curve at high reward values (around 80-90) suggests a few instances where failures received unexpectedly high rewards, warranting further investigation. 
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \vspace{2mm}
    \includegraphics[width=\linewidth]{images/kde_plot.pdf}
    % \vspace{2mm}
    \caption{KDE plot illustrating relationship between reward value and outcome (success/failure).}
    \label{fig:kde_plot}
\end{wrapfigure}This observation is further supported by a statistically significant difference between the reward distributions, a Mann-Whitney U test ($U = 116128.0$, $p < 0.0001$). The low p-value (3.42e-09) provides evidence that the difference in reward distributions is statistically significant.

\paragraph{Performance of our frameworks w.r.t. different complexity}

From Figure \ref{fig:np_meet_trip_analysis}, in the meeting planning, \plangen{} (Best of $\mathcal{N}$) excels in both simple and intermediate problems, whereas a \plangen{} (Mixture of Algo.) performs better for complex problems. The trip planning presents a different trend, where \plangen{} (Best of $\mathcal{N}$) and a \plangen{} (Mixture of Algo.) consistently outperform other approaches across all complexity levels. Nonetheless, in very complex problems for both meeting and trip planning, all algorithms exhibit poor performance.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/analysis_2.pdf}
    \caption{Performance comparison of inference-time algorithms across different complexity levels for meeting and trip planning from NATURAL PLAN}
    \label{fig:np_meet_trip_analysis}
\end{figure}

\input{tables/hyperparameters}

\paragraph{Different hyper-parameters of inference-time algorithms vs. their performance}

We conduct a case study on OlympiadBench, where we analyze the impact of varying hyper-parameters on the performance of different inference-time algorithms. The results (Table \ref{tab:hyperparameters}) indicate that while increasing the number of samples (Best of $\mathcal{N}$), steps (ToT), or refinements (REBASE) lead to marginal improvements, the overall differences remain relatively small. Given this, we opted for lower hyper-parameter values across all inference-time algorithms to balance efficiency and performance.

\paragraph{Frequency of inference-time algorithm selection across datasets}

For the \plangen{} (Mixture of Algo.) method, we analyze how frequently each inference-time algorithm (Best of $\mathcal{N}$, ToT, and REBASE) is selected across different datasets. The results (shown in Table \ref{tab:algo_selection}) show that \plangen{} (ToT) is the most frequently chosen algorithm in NATURAL PLAN, OlympiadBench, and GPQA, indicating its effectiveness in these domains. In contrast, for DocFinQA, \plangen{} (Best of $\mathcal{N}$) is the dominant choice, suggesting that its strategy aligns better with financial reasoning tasks. \plangen{} (REBASE) is selected the least across all datasets, implying that its refinements are less favored by the selection mechanism. These findings highlight the dataset-dependent nature of inference-time algorithm effectiveness and the adaptability of the mixture approach in dynamically choosing the most suitable method.


\begin{algorithm*}
% \footnotesize
\caption{Selection using Modified UCB with LLM-Guided Priors}
\begin{algorithmic}[1]
\State \textbf{Initialize:} $R(a) \gets 0$, $C(a) \gets 1$, $Rec(a) \gets 0$, $F(a) \gets 0$, $D(a) \gets 1$, $T \gets 0$
\State Set $\lambda_{\text{prior}}$, $\alpha_{\text{diversity}}$, $\alpha_{\text{recovery}}$
\State Load LLM-guided priors

\Procedure{SelectAlgorithm}{args}
    \State Compute prior decay: $\lambda_{\text{prior}} \gets \frac{\lambda_{\text{prior}}}{1 + T}$ \Comment{Reduces as trials increase}
    \State Set max exploration term $M \gets 5$
    \State Obtain LLM prior scores: $LLM\_prior \gets \text{LLM\_Guided\_Prior}(args)$
    \State Compute max reward: $R_{\max} \gets \max(R(a))$ (set to 1 if all rewards are 0)

    \For{each algorithm $a$}
        \State Compute normalized reward:
        \[
        \bar{R}_{\text{norm}}(a) \gets \frac{R(a)}{C(a) R_{\max}}
        \]
        \Comment{Scales rewards between 0 and 1 for comparability}

        \State Compute exploration term:
        \[
        E(a) \gets \min\left(\sqrt{\frac{2 \log(T+1)}{C(a)}}, M\right)
        \]
        \Comment{Encourages trying less-used algorithms, capped at $M$}

        \State Compute diversity bonus:
        \[
        D(a) \gets \frac{\alpha_{\text{diversity}}}{C(a) + 1}
        \]
        \Comment{Penalizes frequently used algorithms to encourage variety}

        \State Compute recovery bonus:
        \[
        RecB(a) \gets \alpha_{\text{recovery}} \cdot Rec(a)
        \]
        \Comment{Rewards algorithms that perform well after failures}

        \State Compute final UCB score:
        \[
        UCB(a) \gets \bar{R}_{\text{norm}}(a) + E(a) + \lambda_{\text{prior}} LLM\_prior(a) + D(a) + RecB(a)
        \]
        \Comment{Balances exploitation, exploration, diversity, and recovery}
    \EndFor
    
    \State Select best algorithm:
    \[
    a^* \gets \arg\max_{a} UCB(a)
    \]
    \State \Return $(a^*, UCB(a^*))$
\EndProcedure

\end{algorithmic}
\label{algo:selection}
\end{algorithm*}


\section{Various Examples for Different Components of \plangen{}}
\label{app:examples}

\paragraph{Examples for Constraint Agent}
To illustrate the output of our constraint agent, Table \ref{tab:np_constraints_examples}, Table \ref{tab:gpqa_constraints_examples}, and Table \ref{tab:olympiad_math_constraints} present representative examples of generated constraints. These tables highlight the diverse constraints generated for problem instances of different tasks.

\input{tables/algo_selection}

\paragraph{Example for Verification Agent}
To illustrate the output of our verification agent, Table \ref{tab:np_cal_verification_example} presents representative examples of verification process for NATURAL PLAN (calendar scheduling). This table highlights the how the verification agent verifies the generated plan using constraints.

\paragraph{Examples of Generated Plans}
To demonstrate the plan generation process, Table \ref{tab:np_examples}, Table \ref{tab:gpqa_example}, Table \ref{tab:docfinqa_example}, and Table \ref{tab:olympiad_math_example} present example plans for NATURAL PLAN, GPQA, DocFinQA, and OlympiadBench.  Generated using \plangen{} (Best of $\mathcal{N}$), these tables highlight the varied nature of plans produced across different task types. For GPQA, DocFinQA, and OlympiadBench (i.e., downstream reasoning tasks), the examples additionally illustrate how generated plans are executed to derive final answer. 

%More examples for agents and frameworks within \plangen{} are provided at \url{https://anonymous.4open.science/r/plangen-0C99}

\input{tables/np_constraint_examples}

\input{tables/gpqa_constraints_example}

\input{tables/olympiad_constraints_example}

\input{tables/np_calendar_example}

\input{tables/np_examples}

\input{tables/gpqa_examples}

\input{tables/docfinqa_examples}

\input{tables/olympiad_examples}