\section{\plangen{}}
\label{sec:method}

% This section provides detailed discussion on each proposed agents as well as proposed frameworks using those agents and three popular inference-time algorithms, i.e., Best of $\mathcal{N}$, ToT, and REBASE.

\subsection{Proposed LLM Agents}

\plangen{} comprises three specialized LLM agents: a \textit{constraint agent}, a \textit{verification agent}, and a \textit{selection agent}. Each agent utilizes an off-the-shelf LLM (e.g., Gemini, GPT) which is equipped with task-specific prompts for efficient performance.

\subsubsection{Constraint Agent}
\label{subsec:constraint}

We define ``constraints'' as the criteria necessary for verifying solutions to planning problems. These criteria are inherently instance-specific. For instance, in the calendar scheduling from NATURAL PLAN, relevant constraints include `individual schedules', `availabilities', and `preferences'. In a scientific reasoning problems from GPQA, constraints might be the `concepts used', `calculation correctness', and `formula selection'. We argue that careful extraction of instance-specific constraints is critical for successful verification. The constraint agent serves as a preprocessing component in the framework, designed to extract instance-specific constraints from the problem description. By analyzing the input problem, this agent identifies all possible critical constraints that are required for generated plan verification. The extracted constraints provide a foundation for verifying plans to improve the overall relevance and quality of the planning process. The prompt used by the constraint agent enables it to systematically identify constraints by asking the underlying LLM to focus on specific aspects of the problem description. This ensures that no critical information is overlooked and that the resulting constraints are comprehensive. Prompts used by the constraint agent and examples of generated constraints are provided in App. \ref{app:llm_agents} and App. \ref{app:examples}, respectively.

\subsubsection{Verification Agent}
\label{subsec:verification}

The verification agent plays a critical role in the framework by assessing the quality of generated plans based on constraints generated by the constraint agent. This agent ensures that plans are aligned with task objectives, adhere to constraints, and progress logically toward a correct and complete solution. The verification agent has two key components: (i) feedback generation, and (ii) numerical reward score generation based on feedback. Verification prompts and examples of verification are provided in App. \ref{app:llm_agents} and App. \ref{app:examples}, respectively.

\paragraph{Feedback Generation}
While verifying each generated plan against different constraints, the verification agent generates detailed natural language reasoning regarding plan quality. We consider this explanation as ``feedback'', offering interpretability and actionable next step towards improvement.

\paragraph{Numerical Reward Generation}
Motivated by \citet{zhang2024accessing}, we instruct the agent to evaluate the plan against various constraints and assign a reward score on a scale of $-100$ to $100$. The scoring mechanism is designed to enforce strict quality standards, with a threshold (e.g., a score of $95$ or higher) indicating a verified, high-quality plan.

\subsubsection{Selection Agent}
\label{subsec:selection}

The selection agent dynamically determines the most suitable inference algorithm for solving a given problem instance based on its complexity. It leverages a combination of historical performance; diversity, and recovery scores; and guidance from a LLM to adaptively select the best algorithm for the current instance. To create the selection agent, we utilize a modified Upper Confidence Bound (UCB) policy. The policy combines multiple factors, including normalized rewards, exploration bonuses, diversity adjustments, and recovery scores. Additionally, the agent incorporates LLM-guided priors, which provide algorithm suitability scores based on the problem statement, task requirements, and previous plan (if available). These priors enable the agent to align its selections with the input instance complexity and corresponding constraints, improving the relevance of the chosen algorithm.

% \begin{figure*}
%     \centering
%     \begin{minipage}{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/framework_bon.pdf}
%         \subcaption{Multi-Agent Best of $\mathcal{N}$}
%     \end{minipage}
%     \begin{minipage}{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth, height=0.855\textwidth]{images/framework_tot.pdf}
%         \subcaption{Multi-Agent ToT}
%     \end{minipage}
%     \begin{minipage}{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth, height=0.855\textwidth]{images/framework_rs.pdf}
%         \subcaption{Multi-Agent REBASE}
%     \end{minipage}
%     \caption{Schematic representation of proposed frameworks for inference-time algorithms}
%     \label{fig:frameworks}
% \end{figure*}

\paragraph{Modified UCB Policy} equation combines several terms to balance exploitation, and exploration when selecting the best algorithm for given task instance. To modify UCB, we first conducted a preliminary ablation study, presented in App. \ref{app:llm_agents}.

\begin{align*}
\mathrm{UCB}(a) = \frac{R(a)}{N(a)} + \sqrt{\frac{2 \log(T + 1)}{N(a)}} + \lambda_{\mathrm{prior}} \cdot \mathrm{Prior}(a) + \frac{\alpha_{\mathrm{diversity}}}{N(a) + 1} + \alpha_{\mathrm{recovery}} \cdot S_{\mathrm{recovery}}(a)
\end{align*}

All terms in equation given above are calculated across one evaluation run. Here, the cost of calculation is negligible since it only utilizes reward values from previous runs, but only one LLM call require to get score for \text{Prior}(a). The first term, \(\frac{R(a)}{N(a)}\), represents the average reward for algorithm \(a\), where \(R(a)\) is the total reward accumulated by the algorithm, and \(N(a)\) is the number of times the algorithm has been selected. This term ensures that algorithms with higher historical performance are prioritized. The second term, \(\sqrt{\frac{2 \log(T + 1)}{N(a)}}\), serves as the exploration component, encouraging the selection of algorithms with fewer trials, denoted as $T$. This term ensures that under-explored options are adequately evaluated. Furthermore, \(\lambda_{\text{prior}} \cdot \text{Prior}(a)\), which leverages LLM-guided priors to align algorithm selection with the instance-specific complexity. Here, \(\lambda_{\text{prior}}\) is a dynamically decaying weight defined as \(\frac{\lambda_{\text{prior}}}{1 + T}\), where \(T\) represents the total number of trials. This decay gradually shifts the focus from initial priors to historical performance as trials progress. The diversity bonus, \(\frac{\alpha_{\text{diversity}}}{N(a) + 1}\), penalizes overused algorithms, ensuring balanced exploration across all options. Finally, the recovery term, \(\alpha_{\text{recovery}} \cdot S_{\text{recovery}}(a)\), rewards algorithms that recover effectively from failures, with \(S_{\text{recovery}}(a)\) representing the recovery score for algorithm \(a\).

\paragraph{Selection Process}
This process begins by initializing algorithm-specific variables, such as accumulated rewards, selection counts, and failure counts. Further details on the algorithm can be found in Algorithm \ref{algo:selection} (App. \ref{app:llm_agents}). The agent then incorporates LLM-guided priors to generate suitability scores for the algorithms based on the problem statement and any provided feedback. These priors are derived from a LLM (prompt for this given in App. \ref{app:llm_agents}), and serve as initial estimates to adjust the UCB \citep{han2024ucb} values.

%These variables provide a foundational understanding of the performance history and usage patterns of each algorithm.

%Together, these terms form a comprehensive decision-making strategy that dynamically adapts to task requirements, balances exploration and exploitation, and promotes fairness and efficiency in algorithm selection.

%Next, the agent computes UCB values for all algorithms using a modified UCB policy that balances historical performance, exploration, and contextual adjustments. The policy incorporates normalized rewards, exploration terms, diversity penalties to avoid overusing algorithms, and recovery bonuses to favor algorithms that demonstrate resilience after failures. Finally, the algorithm with the highest UCB value is selected for execution. The agent tracks the performance of the chosen algorithm and updates the associated metrics, enabling continual refinement of the selection process in subsequent iterations. This dynamic approach ensures efficient and context-aware algorithm selection for solving diverse planning problems.

\subsection{Proposed Frameworks}
\label{subsec:frameworks}

Within \plangen{}, we propose four different frameworks: (1) \plangen{} (Best of $\mathcal{N}$) (Figure \ref{fig:bon}), (2) \plangen{} (ToT) (Figure \ref{fig:tot}), and (3) \plangen{} (REBASE) (Figure \ref{fig:rebase}), and (4) \plangen{} (Mixture of Algorithms) (Figure \ref{fig:teaser}). 

%Here, we present detailed description of each framework. 

\subsubsection{\plangen{} (Best of $\mathcal{N}$)}

\begin{wrapfigure}{r}{0.45\textwidth}
    \centering
    \vspace{-20mm}
    \includegraphics[width=0.8\linewidth]{images/framework_bon_2.pdf}
    \vspace{-3mm}
    \caption{Schematic representation of \plangen{} (Best of $\mathcal{N}$) (BoN).}
    \label{fig:bon}
\end{wrapfigure}Motivated by \citet{brown2024large}, we adapted the Best of $\mathcal{N}$ algorithm and modified it using our constraint and verification agents as illustrated in Figure \ref{fig:bon}.  The framework generates $\mathcal{N}$ candidate plans (Plan 1, Plan 2, ..., Plan n), and each plan is assessed by a verification agent based on a set of constraints. Then, a corresponding reward (Reward 1, Reward 2, ..., Reward n) gets assigned by the verification agent. Finally, the plan with the maximum reward is chosen, guaranteeing an optimal solution that best satisfies the problem constraints.

\subsubsection{\plangen{} (ToT)}

\begin{wrapfigure}{r}{0.45\textwidth}
    \centering
    \vspace{-12mm}
    \includegraphics[width=0.8\linewidth]{images/framework_tot_2.pdf}
    \vspace{-3mm}
    \caption{Schematic representation of \plangen{} (ToT). Highest-reward steps are highlighted in green.}
    \label{fig:tot}
\end{wrapfigure}ToT algorithm has been studied in detail for solving many complex problems \citep{yao2024tree}. As shown in Figure \ref{fig:tot}, we modify the ToT algorithm with our constraint and verification agents. The method begins by initializing a root node that represents the problem and generating multiple potential next steps, creating a tree-like structure. The generated steps are verified using a verification agent which assigns reward scores based on a set of constraints. The iterative process involves evaluating all possible steps at a given depth, selecting the most promising path based on reward scores, and expanding it further by generating new steps. This process continues until a valid solution is identified or a pre-defined limit on iterations is reached. Further details on various prompts for the ToT are presented in App. \ref{app:frameworks}.

%By combining structured generation with a robust evaluation mechanism, the framework ensures a balance between exploration of new possibilities and exploitation of promising directions. Additionally, the approach maintains a transparent reasoning trail, making it interpretable for tasks requiring step-by-step problem-solving using constraints.

\subsubsection{\plangen{} (REBASE)}

The REBASE tree search method inherits the exploitation and pruning properties of tree search and is well-studied for mathematical reasoning \citep{wu2024empirical}. As shown in Figure \ref{fig:rebase}, the framework incorporates a dynamic selection and expansion strategy to iteratively refine solutions. At each depth of the tree, candidate nodes are ranked based on their assigned reward scores (obtained using a verification agent), ensuring that the most promising candidates are explored first. Even steps with lower rewards are considered but with a reducing number of children, meaning that their exploration depth is limited. This hierarchical pruning helps maintain efficiency, thereby reducing unnecessary exploration of weaker nodes. This process continues until either a valid, complete solution is found or a predefined depth or width limit is reached. Also, there is a completion check similar to ToT which identifies nodes that represent complete solutions, enabling REBASE to terminate early once a satisfactory outcome is identified. App. \ref{app:frameworks} provides further details on prompts for the REBASE.

%By combining structured tree exploration with reward-based guidance, REBASE provides a scalable and interpretable approach for solving complex, multi-step problems under strict quality constraints. Its emphasis on balancing exploration and exploitation ensures robust and efficient solution discovery, making it particularly suited for domains where precision and adaptability are paramount.

\subsubsection{\plangen{} (Mixture of Algorithms)}

\begin{wrapfigure}{r}{0.45\textwidth}
    \centering
    \vspace{-12mm}
    \includegraphics[width=0.8\linewidth]{images/framework_rs_2.pdf}
    \vspace{-3mm}
    \caption{Schematic representation of \plangen{} (REBASE). Green shading indicates step reward (darker $=$ higher).  Darker steps prioritized for exploration.}
    \label{fig:rebase}
\end{wrapfigure}The Mixture of Algorithms framework (Figure \ref{fig:teaser}) introduces a selection agent (\textsection \ref{subsec:selection}) which dynamically selects the best possible inference-time algorithms proposed in the above sections based on instance-level complexity. The framework operates in a modular and iterative manner, ensuring adaptability in addressing planning and reasoning problems with different complexity effectively.

%This proposed framework introduces an agentic approach for iterative planning, designed to tackle complex task descriptions and generate high-quality plans by integrating multiple inference algorithms and feedback-driven refinement. 

\paragraph{Orchestration} 
The process begins with generating an initial plan using LLM based on the task description and problem statement. Along with this, the constraint agent (\textsection \ref{subsec:constraint}) is employed to generate an instance-specific set of constraints. Based on the constraints, the verification agent (\textsection \ref{subsec:verification}) evaluates the quality of the initial plan and provides a reward score (indicated as `Score' in Figure \ref{fig:teaser}). If the initial plan meets the required threshold (denoted $T_h$), it is acceptable as the ``Final Plan''. Otherwise, the iterative refinement process begins.

\paragraph{Iterative Refinement}
The refinement loop is driven by a suite of inference algorithms as shown in Figure \ref{fig:teaser}. During this iterative refinement, the selection agent (\textsection \ref{subsec:selection}) determines the most suitable algorithm based on the instance-specific complexity and historical UCB values. The selected algorithm produces an updated plan, which is then re-evaluated by the verification agent. To ensure continual improvement, the framework incorporates feedback generated by a verification agent that provides guidance, and this feedback loop enables the system to refine the plan incrementally.


%By leveraging a structured evaluation prompt and scoring mechanism, the Verifier Agent provides both qualitative feedback and a numerical reward score, which guide further refinement of the plan. The Verifier Agent operates using a prompt-based evaluation strategy. The prompt includes the problem statement, the current plan, and any specific verification constraints. It instructs the model to evaluate the plan against these constraints and assign a reward score on a scale of -100 to 100. The scoring mechanism is designed to enforce strict quality standards, with a threshold (e.g., a score of 95 or higher) indicating a verified, high-quality plan. Detailed reasoning for the score is included in the output, offering interpretability and actionable feedback. The agent's implementation incorporates robust error handling and clamping of reward scores to ensure consistent outputs. Additionally, the Verifier Agent employs a lightweight caching mechanism to optimize prompt reuse. This process ensures the verification step is both computationally efficient and methodologically rigorous, making it a reliable component for iterative plan refinement.

%REBASE is a reward-guided tree-based exploration framework designed to systematically search for solutions by leveraging structured expansion and evaluation. The method begins by initializing a tree where each node represents a potential solution state. Starting from a root node defined by the problem, REBASE employs a language model to generate and evaluate new solution steps iteratively. Each generated step is assessed using a verification mechanism that calculates a reward score based on its quality, relevance, and completeness, ensuring that the exploration is both rigorous and aligned with task objectives. Nodes with higher rewards are prioritized for expansion, enabling the framework to focus computational efforts on promising directions while balancing breadth and depth of exploration.