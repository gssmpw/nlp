\section{Analysis and Discussion}
\label{sec:discussion}

Here, we discuss detailed analysis over importance of our agents and model-agnostic nature of our frameworks. Additionally, we also present more analysis on results in App. \ref{app:analysis}.

\paragraph{Importance of Verification Agent}

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    % \vspace{-7mm}
    \includegraphics[width=\linewidth]{images/lr_plot.pdf}
    % \vspace{-5mm}
    \caption{Logistic regression plot showing verification agent's positive performance impact. P(Successful Outcome) $=$ probability of prediction being correct.}
    \label{fig:imp_verification}
\end{wrapfigure}Figure \ref{fig:imp_verification} demonstrates the verification agent's crucial role in \plangen{} by showing a strong correlation between assigned reward values and prediction correctness (1 for correct, 0 for incorrect).  The plotted points represent the average correctness rate for data buckets of varying reward values, each bucket containing hundreds of samples. A logistic regression model trained on DocFinQA and GPQA data ($\sim1100$ total samples) reveals a sigmoidal trend: higher rewards correlate with increased success probability, highlighting the agent's effectiveness.  This reinforces the importance of constraint-guided verification for improving inference-time algorithms (see App. \ref{app:analysis} for further details).

% The plot (Figure \ref{fig:imp_verification}) highlights the critical role of the verification agent in \plangen{}. Using data from DocFinQA and GPQA, we mapped assigned reward values to the correctness of final predictions ($1$ for correct, $0$ for incorrect) and trained a logistic regression model. The resulting fit demonstrates how higher reward values correlate with increased success probabilities, directly reflecting the verification agent’s impact on plan refinement. The plot indicates that, as the reward value increases, the likelihood of predicting correct answer increases. The observed sigmoidal trend highlights the agent’s effectiveness, and reinforces the importance of constraint-guided verification in improving inference-time algorithms. Further details on this is presented in App. \ref{app:analysis}.

\paragraph{Importance of Selection Agent}

Figure \ref{fig:imp_selection} illustrates the importance of the selection agent by comparing the performance on the NATURAL PLAN. Here, Multi-Agent (Ver.) includes only the verification agent, while Multi-Agent (Ver. + Selection) further includes a selection agent. The results highlight the progressive impact of these components. 

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    % \vspace{-7mm}
    \includegraphics[width=\linewidth]{images/analysis_ver.pdf}
    % \vspace{-5mm}
    \caption{Case study on NATURAL PLAN, showing the impact of selection agent. Ver.: Verification.}
    \label{fig:imp_selection}
\end{wrapfigure}For example, in calendar scheduling, Multi-Agent (Ver.) improves performance to 56.1 EM compared to Multi-Agent (Baseline). However, Multi-Agent (Ver. + Selection) achieves 59.3 EM, demonstrating the additional benefit of algorithm selection. A similar trend is observed in trip planning where Multi-Agent (Ver. + Selection) outperforms Multi-Agent (Ver.) (41.17 EM vs. 35.44 EM) and the Multi-Agent (Baseline). For meeting planning, Multi-Agent (Ver.) achieves 43.1 EM compared to 36.8 EM of Multi-Agent (Baseline), whereas, Multi-Agent (Ver. + Selection) achieves competitive performance. Together, verification and selection agents drive significant improvements over single-agent and multi-agent baselines.

% These results indicate that the ver. and selection agents are critical for improving performance. 

%of different systems
%to refine outputs
%to dynamically choose inference-time algorithms based on task needs.
%Together, these components drive significant improvements over single system such as Gemini-1.5-Pro and pre-defined self-improvement system such as the Multi-Agent (Baseline).

\input{tables/gpqa_model_agno}

\paragraph{Model-Agnostic Nature}

% The results from Table \ref{tab:model_agnostic} demonstrate the model-agnostic nature of our proposed multi-agent frameworks. While the primary experiments were conducted using Gemini-1.5-Pro, the framework's effectiveness holds across different underlying models, such as Gemini-2.0-Flash and GPT-4o. For instance, Multi-Agent Best of $\mathcal{N}$ significantly improves upon Gemini-2.0-Flash on NATURAL PLAN (reaching 68.90 EM). Similarly, Multi-Agent REBASE outperforms Gemini-2.0-Flash on OlympiadBench (MATH: 60.98, PHY: 36.02 accuracy), and Multi-Agent Mixture of Algorithms surpasses GPT-4o on GPQA (49.40 accuracy). These results demonstrate the consistent performance enhancement from our multi-agent frameworks across various underlying models, ensuring their robustness. Results marked with `*' in Table \ref{tab:model_agnostic} are based on partial data, but this does not affect the overall observations.

The results from Table \ref{tab:model_agnostic} demonstrate the model-agnostic nature of our proposed multi-agent frameworks. While the primary experiments were conducted using Gemini-1.5-Pro, the framework's effectiveness holds across different underlying models, such as Gemini-2.0-Flash and GPT-4o. For instance, in the NATURAL PLAN (calendar scheduling), the \plangen{} (Best of $\mathcal{N}$) framework achieves a significant improvement, reaching 68.90 EM, outperforming Gemini-2.0-Flash (61.10 EM). Similarly, in OlympiadBench, the \plangen{} (Mixture of Algo.) achieves the highest scores in MATH (64.10) and PHY (37.29), surpassing Gemini-2.0-Flash (52.13 MATH, 27.54 PHY). Note that, the Mixture of Algo. outperforms other three frameworks, showing effectiveness of selection agent. On GPQA, Mixture of Algo. (49.40) and \plangen{} (REBASE) (64.14) outperform GPT-4o (47.98) and Gemini-2.0-Flash (60.10), respectively. These results highlight that regardless of the underlying model, our frameworks consistently enhance performance by leveraging multi-agent collaboration, reinforcing their flexibility and robustness across models. 

% In the Table \ref{tab:model_agnostic}, `*' indicates results on partial data, but it does not change overall observation.

% \paragraph{Why do ToT and REBASE show lower performance on the Trip and Meeting categories of NaturalPlan?}

% Plans with errors receive lower scores, triggering further refinement, while high-quality plans achieve greater success probabilities. The observed sigmoidal trend highlights the agent’s effectiveness—initial improvements lead to gradual gains, but once a plan meets essential constraints and achive higher reward values, success likelihood increases. This reinforces the importance of constraint-guided verification in improving inference-time algorithms. Further details on this is presented in App. \ref{app:analysis}.

%Gemini-1.5-Pro and Multi-Agent (Baseline) serve as references, as they do not utilize any agents, relying instead on predefined decision-making approaches.

\paragraph{Discussion on LLM calls \textit{vs.} Performance (\%)}

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \vspace{-10mm}
    \includegraphics[width=\linewidth]{images/llm_calls_vs_performance.pdf}
    % \vspace{-5mm}
    \caption{Comparison of baselines and our frameworks, showing the trade-off between LLM calls and performance (\%) for OlympiadBench (MATH).}
    \label{fig:llm_calls_vs_performance}
\end{wrapfigure}Figure \ref{fig:llm_calls_vs_performance} shows the relationship between the number of LLM calls and task performance across baselines (single-agent and multi-agent) and proposed frameworks, using OlympiadBench (MATH category). The single-agent system, zero-shot CoT, requires only one LLM call. The multi-agent baseline requires the same number of calls as \plangen{} (Best of $\mathcal{N}$), but our framework outperforms the multi-agent baseline. For \plangen{} (ToT) and \plangen{} (REBASE), we focus on LLM calls during the tree expansion phase. \plangen{} (ToT) involves dynamic exploration, where each explored path requires three LLM calls: step generation, reward evaluation, and completion verification. The total cost is the per-path cost multiplied by the number of paths explored, constrained by either the number of steps generated for each problem or a predefined iteration budget (i.e., 20). For \plangen{} (REBASE), the number of LLM calls depends on the search width (i.e., 10). Each solution path expansion involves three calls: step generation, quality evaluation, and completion verification, thus, giving maximum 30 LLM calls for single problem. For \plangen{} (Mixture of Algo.), we estimate the average LLM calls by summing the estimated calls for each selected algorithm per problem, then dividing by the total number of problems. As shown in Figure \ref{fig:llm_calls_vs_performance}, the single-agent system exhibits the lowest performance despite requiring just one LLM call. Multi-agent approaches show improved performance, with \plangen{} (ToT) and \plangen{} (REBASE) balancing LLM call efficiency and accuracy. The \plangen{} (Mixture of Algo.) method achieves the highest performance, suggesting that combining diverse planning strategies enhances efficiency.