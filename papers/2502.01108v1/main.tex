%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
% \documentclass[acmsmall]{acmart}

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
% \documentclass[acmlarge,screen]{acmart}

%%%% Large double column format, used for TOG
% \documentclass[acmtog, authorversion]{acmart}

%%%% Generic manuscript mode, required for submission
%%%% and peer review
\DocumentMetadata{}
% \documentclass[acmlarge,review,anonymous]{acmart}
\documentclass[acmlarge]{acmart}

% \documentclass[acmlarge,authorversion]{acmart}

% \documentclass[acmlarge, review, anonymous]{acmart}
% \documentclass[manuscript,review,anonymous]{acmart}

%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%

% \usepackage[demo]{graphicx}
% \usepackage{subcaption}
% \usepackage{tabularx}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{caption}
%\usepackage[table]{xcolor}
% \usepackage{colortbl}
\usepackage{booktabs}
%\usepackage[dvipsnames]{xcolor}
\usepackage{subcaption}
\usepackage{array}
\usepackage{float}
\usepackage{makecell}
\usepackage{mathtools, amsmath}
% \usepackage{algpseudocode}
\usepackage{relsize}
\usepackage{lscape}
\usepackage{multicol, multirow}
\usepackage{enumitem}
\usepackage{tablefootnote}
\usepackage{xcolor}
% \usepackage{authblk}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
%\usepackage{ulem}
% \usepackage{adjustbox}
\usepackage{url}
% \urlstyle{urlstyle}
\urlstyle{tt}
\usepackage{array}
\usepackage{wrapfig}
% \usepackage[symbol]{footmisc}


\definecolor{maroon}{cmyk}{0,0.87,0.68,0.32}

\newcommand{\anonymize}[1]{Study}

\newcommand*{\SuperScriptSameStyle}[1]{%
  \ensuremath{%
    \mathchoice
      {{}^{\displaystyle #1}}%
      {{}^{\textstyle #1}}%
      {{}^{\scriptstyle #1}}%
      {{}^{\scriptscriptstyle #1}}%
  }%
}

\newtheorem{implication}{Implication}
\newcommand*{\oneS}{\SuperScriptSameStyle{*}}
\newcommand*{\twoS}{\SuperScriptSameStyle{**}}
\newcommand*{\threeS}{\SuperScriptSameStyle{*{*}*}}
\newlist{questions}{enumerate}{2}

\setlist[questions,1]{label=\bf{RQ\arabic*}.,ref=RQ\arabic*}



\newcommand{\edit}[1]{{\color{blue} #1}}

\newcommand{\mxnote}[1]{{\color{teal}{\bf MX: }#1} }
\newcommand{\msnote}[1]{{\color{red}{\bf MS: }#1} }
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2025}
% \acmYear{2025}
% \acmDOI{}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX']{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}

% \acmConference[Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.]{Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.}{}{}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
% \acmBooktitle{Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.} 
% \acmPrice{15.00}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.

\setcopyright{none}
\begin{document}

%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.



% \title[Foundation Model]{A PPG Foundation Model Suitable for a Diverse Array of Mobile Health Applications in both Lab and Field Environments}
\title[PPG Foundation Model]{Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for Wearable Applications Across Lab and Field Settings}


%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

% \thanks{\textbf{$^{*}$Both authors contributed equally to this research.}}

% \thanks{\textbf{------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------}}


\author{Mithun Saha$^{*}$}
\email{msaha1@memphis.edu}
\affiliation{%
  \institution{University of Memphis}
  % \city{Memphis}
  % \state{Tennessee}
  % \postcode{38152}
  \country{USA}
}

\author{Maxwell A. Xu$^{*}$}
\email{maxu@illinois.edu}
\affiliation{%
  \institution{University of Illinois Urbana-Champaign}
  % \city{Urbana}
  % \state{Illinois}
  % \postcode{61801}
  \country{USA}
}

\author{Wanting Mao}
\email{wmao8@illinois.edu}
\affiliation{%
  \institution{University of Illinois Urbana-Champaign}
  % \city{Urbana}
  % \state{Illinois}
  % \postcode{61801}
  \country{USA}
}

\author{Sameer Neupane}
\email{sameer.neupane@memphis.edu}
\affiliation{%
  \institution{University of Memphis}
  % \city{Memphis}
  % \state{Tennessee}
  % \postcode{38152}
  \country{USA}
}

\author{James M. Rehg}
\email{jrehg@illinois.edu}
\affiliation{%
  \institution{University of Illinois Urbana-Champaign}
  % \city{Urbana}
  % \state{Illinois}
  % \postcode{61801}
  \country{USA}
}  

\author{Santosh Kumar}
\email{skumar4@memphis.edu}
\affiliation{%
  \institution{University of Memphis}
  % \city{Memphis}
  % \state{Tennessee}
  % \postcode{38152}
  \country{USA}
}
\thanks{\textbf{$^{*}$Both authors contributed equally to this research.}}
\thanks{\noindent\rule{\linewidth}{0.4pt}}
% \thanks{\noindent\rule{8cm}{0.4pt}}
% \thanks{\textbf{------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------}}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Mithun et al.}
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

Photoplethysmography (PPG)-based foundation models are gaining traction due to the widespread use of PPG in biosignal monitoring and their potential to generalize across diverse health applications. In this paper, we introduce Pulse-PPG, the first open-source\footref{open} PPG foundation model trained exclusively on raw PPG data collected over a 100-day field study with 120 participants. Existing PPG foundation models are either open-source but trained on clinical data or closed-source, limiting their applicability in real-world settings. We evaluate Pulse-PPG across multiple datasets and downstream tasks, comparing its performance against a state-of-the-art foundation model trained on clinical data. Our results demonstrate that Pulse-PPG, trained on uncurated field data, exhibits superior generalization across clinical and mobile health applications in both lab and field settings. This suggests that exposure to real-world variability enables the model to learn fine-grained representations, making it more adaptable across tasks. Furthermore, pre-training on field data surprisingly outperforms its pre-training on clinical data in many tasks, reinforcing the importance of training on real-world, diverse datasets. To encourage further advancements in robust foundation models leveraging field data, we plan to release Pulse-PPG, providing researchers with a powerful resource for developing more generalizable PPG-based models.     
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%


\keywords{Foundation models, Wearables, Health-wellbeing, Contrastive Learning  }

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003120.10003138</concept_id>
       <concept_desc>Human-centered computing~Ubiquitous and mobile computing</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257</concept_id>
       <concept_desc>Computing methodologies~Machine learning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

% \ccsdesc[500]{Human-centered computing~Ubiquitous and mobile computing}
% \ccsdesc[500]{Computing methodologies~Machine learning}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Unobtrusive, ubiquitous, and cost-effective wearable sensors have demonstrated the potential to revolutionize real-time monitoring of health and wellness by enabling the detection of various physical and mental health states. Photoplethysmography (PPG) in smartwatches has emerged as a widely used modality due to its non-invasive assessment of physiology without the need for firm attachment. It is used for estimating physiological metrics such as heart rate, heart rate variability~\cite{sarhaddi2022comprehensive}, blood glucose~\cite{ali2024comparison}, oxygen saturation~\cite{rajakariar2024accuracy}, and blood pressure~\cite{he2022new,fortino2010ppg}. For diagnosis, it can detect cardiovascular conditions~\cite{ouyang2017self}, including atrial fibrillation~\cite{poh2018diagnostic} and detect hypoxia~\cite{lazazzera2020detection}. For mental health and wellness, it can track stress~\cite{zhu2023stress}, emotion~\cite{kontaxis2020photoplethysmographic}, focus~\cite{wang2024classifying}, and depression~\cite{kontaxis2020photoplethysmographic}. 

However, PPG-based inference in real-world settings remains challenging due to its high susceptibility to noise from motion artifacts~\cite{pollreisz2022detection}, ambient light \citep{cubas2023design}, and skin conditions~\cite{ajmal2021monte}. This has slowed the progress in realizing the full potential of PPG in the natural environment. For example, high accuracies for stress classification has been reported on lab data~\cite{choi2022attention,hasanpoor2022stress,motaman2022stress,alshareef2022transformer,mitro2023ai}, but they do not generalize to the field settings~\cite{mitro2023ai}. Training models on lab data from the same participants and then applying it to their field data can lead to a better performance~\cite{toshnazarov2024sosw}, but it doesn't scale to unseen participants. Models trained using larger field datasets report low performance~\cite{zhang2024reproducible,aqajari2024enhancing}.


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/overview.png}
    \caption{\textbf{Overview of our Pulse-PPG Foundation Model.} Our model is trained on wearable field PPG with relative contrastive learning, based on the relative distances captured in our learned motif-based distance function. This model demonstrates strong performance on a wide variety of tasks across wearable field, wearable lab, and clinical settings. 
    % (Figure Not finalized yet, still in progress)
    }
    \label{fig:overview}
\end{figure}

The emergence of Foundation Models (FM) offers a new opportunity to address these challenges and accelerate our progress. In other domains such as natural language processing and computer vision, the foundation model paradigm has transformed the development of machine learning solutions to real-world problems. The key property of an FM is that it is pre-trained on a large-scale dataset to ensure that the resulting feature representation encompasses all of the complexity of the data domain, which is then validated by demonstrating that the FM can solve multiple downstream tasks without additional representation learning. In computer vision, the field has moved away from collecting individual special-purpose datasets and training task-specific models to leveraging existing foundation model representations, such as DINOv2~\cite{oquab2023dinov2} in solving a variety of perceptual tasks. A key enabling property is that, while the training datasets are private and not publicly available, the model weights are released to the research community, enabling everyone to benefit from its powerful representation. This transition in utilizing publicly available FMs has not yet occurred for mHealth, and it is a crucial next step.

While there have been some exciting recent efforts to develop FMs for PPG~\cite{abbaspourazad2023large,pillai2024papagei}, current approaches suffer from two important limitations: 1) Private models which define the SOTA but whose weights are not available to the research commmunity~\cite{abbaspourazad2023large}, and 2) Models which demonstrate impressive performance on lab-collected data but have not been developed or evaluated for use in the field environment~\cite{pillai2024papagei}. The growing availability of field datasets has created tremendous opportunities for advancing PPG-based research. However, many of these datasets~\cite{truslow2024understanding,van2019protocol,schmidt2019multi} remain inaccessible or the computational resources required to fully exploit their potential are prohibitively expensive for many researchers and practitioners. 


In this paper, we present the first open-source\footnote{\label{open}Model weights and code will be released under the MIT license \citep{saltzer2020mit} upon acceptance of our work.} foundation model for wearable PPG signals, \emph{Pulse-PPG}, which can be seen in Figure~\ref{fig:overview}. We pre-train our model on the raw PPG signals from a large-scale wearable field dataset, composed of 21 billion data points collected from 120 participants, organized in 822,247 unique 4-minute PPG segments. We avoid filtering our signals because such real-world noise patterns offer meaningful contextual cues on a given individual's motion or environment, which we show, enables better performance on wearable field, lab and clinical downstream tasks, compared to training on cleaner clinical data. In order to use our foundation model for the field setting, we utilize a learnable motif-based distance function \citep{xurebar}, as well a relative contrastive learning loss \citep{xu2024relcon} in order to extract subtle but semantically meaningful patterns from the data. To this end, our model achieves state-of-the-art performance, substantially outperforming another state-of-the-art open sourced PPG foundation model on 10/11 of our downstream evaluation tasks that span 5 different datasets, across the wearable field, wearable lab, and clinical PPG domains. 

With the weights publicly available, researchers will be able to  build upon our work for their own specialized tasks and can bypass the costly and time-consuming process of training from scratch, significantly lowering the barrier to entry for those without access to extensive computational resources. Moreover, our model can serve as an embedding function, enabling researchers to extract meaningful physiological representations from raw PPG data. Beyond these, the availability of our model weights will encourage reproducibility and benchmarking across datasets and settings. Researchers can compare their methods against a common baseline, accelerating research progress.
\\ 
[.15cm]
The key contributions of this work are as follows:
\begin{enumerate}[leftmargin=*,noitemsep]
    \item Pulse-PPG is the first open-source\footref{open} PPG foundation model trained entirely on raw, uncurated PPG data collected from a longitudinal field study involving 120 subjects from diverse backgrounds over 100 days.

    \item We achieve state-of-the-art performance across a wide set of downstream datasets and tasks for wearable applications within field and lab settings, as well as clinical applications. 
    \item We show that for two different foundation model approaches, training on field data achieves better performance than a lab-trained model on wearable lab downstream tasks, highlighting the field-to-lab generalizability of the models, with additional potential implications for clinical settings.   
   
\end{enumerate} 

\section{Related Works}
\subsection{Foundation Models for PPG signals}

There have been some recent work on PPG-specific foundation models, but none of them are open and designed for wearable PPG signals (i.e., collected from a smartwatch). The closest work with open model availability is PaPaGei, an open-source PPG foundation model that improves classification and regression performance across 14 tasks~\cite{pillai2024papagei}, including some  wearable lab tasks. However, it is exclusively trained on clean clinical PPG signals, and was not evaluated on field data in~\cite{pillai2024papagei}. We show (Section \ref{sec:exppapagei}) that training on this clinical data negatively impacted its generalization performance on wearable tasks involving field-collected data. The most relevant work for wearable PPGs is a foundation model pre-trained on a large-scale wearable field PPG dataset and presented in~\cite{abbaspourazad2023large}. However, the model is closed-source and its training datasets are private, restricting accessibility for the research community. Similalry, SiamQuality is a foundation model pre-trained on clinical data that demonstrates  generalizability on wearable lab and clinical datasets, but the model remains private \cite{ding2024siamquality}. 

Additional related works addressed tasks and goals which are adjacent to this paper. 
REGLE employs autoencoders to extract disentangled embeddings from clinical PPGs, focusing on their applicability to genetic and disease prediction tasks \cite{yun2024unsupervised}. TS2TC introduces a generative self-supervised learning framework for clinical PPG data but does not provide pre-trained weights and has been evaluated exclusively on clinical PPG regression tasks \cite{zhang2024general}. PPG-PT demonstrates the utility of pre-training, but does not meet all of the requirements of a foundation model, %%is not a foundation model that demonstrates generalizability, 
as the model was only evaluated on one clinical PPG-specific downstream task \cite{davies2024interpretable}.


While these prior works have made significant strides in PPG representation learning, they either lack open-source accessibility, generalizability across tasks, or applicability to field-collected wearable data, highlighting a critical gap that our work aims to address. With the release of our foundation model's weights, ours will be the first open-source\footref{open} PPG model trained exclusively on field-collected data.

\subsection{Foundation Models in General}
Our work is inspired by prior successes in developing foundation models for other data modalities. These works have shown that models  pre-trained on large-scale datasets can capture underlying data representations and are highly adaptable for various downstream applications \cite{bommasani2021oppoandriskforfm}. These models have had substantial impact in fields like Natural Language Processing and Computer Vision. In the language domain, models such as BERT \citep{devlin2018bert} and the Generative Pretrained Transformer (GPT) \citep{radford2018improving} have demonstrated remarkable generalizability across a diverse range of natural language processing tasks, catalyzing the development of large language models (i.e. ChatGPT \cite{zhou2024comprehensive}). 
The vision domain has seen similar breakthroughs with models like CLIP \citep{radford2021learning} and the Segment Anything Model~\cite{kirillov2023segment} that are trained on large-scale data and exhibit strong performance on a broad set of downstream tasks. These works have demonstrated that pre-training on large-scale data can yield effective feature representations and serve as the motivation for this work.

\section{Designing Pulse-PPG Methodology}
Our goal is to develop a foundation model capable of learning a generalizable embedding space that can be effectively applied to a diverse set of datasets (ranging from clinical PPG to mHealth field PPG) and downstream tasks (ranging from blood pressure prediction to stress detection). In Section \ref{sec:whyfield}, we detail our rationale for pre-training the foundation model on noisy field PPG data, as opposed to the traditional approach of pre-training on clean, lab-study-based PPG data. Then, in Section \ref{sec:designpretrain}, we outline the key challenges involved in learning a robust foundation model in this setting and describe how our methodology addresses each of these challenges.

\subsection{Motivation for Pre-training on Field PPG data} \label{sec:whyfield}

Traditionally, prior work has focused on training models using clean, lab-collected PPG data and evaluating them on mHealth datasets~\cite{pillai2024papagei}. This approach is largely due to the significant challenges associated with utilizing real-world field data, which is inherently noisy and variable. However, we hypothesize that by designing a methodology capable of effectively learning patterns from field data, we can develop representations that are more robust, generalize better, and achieve superior performance on downstream mHealth tasks, such as instantaneous heart rate prediction or activity recognition. In this way, the noise present from the wearable field sensor becomes a benefit, as it could introduce additional contextual information of a person's current state and environment. Importantly, pre-training on mHealth data and evaluating on mHealth data reduces domain shift, as the model is exposed to the same types of noise, variability, and environmental factors during both training and evaluation. 

Clinical datasets, while less noisy and more consistent due to their controlled collection processes administered by professionals, lack the dynamic variability and environmental factors present in real-world settings. For example, clinical datasets often involve participants with specific health conditions and are collected under static, controlled conditions, which fail to capture the wide range of physiological and environmental variability encountered in the wild. In contrast, field data includes participants with and without specific conditions, as well as a diverse array of physiological patterns influenced by real-world factors, such as poor sensor contact from loose bands and daily motion. This richness and diversity make field data a more comprehensive source for pre-training a foundation model. 

By pre-training on field data, we aim to create a foundation model that better reflects the complexity of real-world scenarios and generalizes more effectively across diverse applications.


\subsection{Designing the Pre-training Task for the Pulse-PPG Foundation Model} \label{sec:designpretrain}


To build a robust foundation model, we must design a pre-training task that not only captures high-level, obvious semantic features—such as heart rate changes—but also uncovers subtle, latent patterns in the data, such as variations in pulse waveform morphology or transient physiological responses. PPG signals, which measure blood volume changes, encode a wealth of physiological information. However, this information is often obscured by noise, particularly in mHealth settings. Traditional approaches address noise by normalizing or aggregating data into statistical features and training models on these features. While this reduces the impact of noise, it often discards fine-grained details that are critical for accurate inference or discovering hidden clusters and trends in the data.

To overcome these challenges, we introduce Pulse-PPG, a method specifically designed to address three key issues: 1) the development of a domain-specific pre-training task that leverages PPG-specific domain knowledge; 2) exploiting the inherent noise of real-world, mHealth PPG data; and 3) the ability to capture subtle, yet meaningful, patterns in the data that are essential for ensuring robustness and generalizability across diverse and dynamic real-world settings.


\subsubsection{PPG-Specific Pre-training Task:} \hfill\\

A classic self-supervised objective for time-series foundation models is contrastive learning, where augmentations (e.g., cropping, scaling, jittering, shuffling) create distinct but semantically similar positive pairs \citep{meng2023unsupervised, yue2022ts2vec, zhang2022tfc, eldele2023tstcc, woo2022cost, yang2022btsf, yang2022timeclr, ozyurt2022cluda, lee2022SSLAPP}. However, PPG signals face unique challenges: (1) time-series lack image-like invariances (e.g., rotation), and (2) augmentations can distort PPG semantics. For example, scaling alters amplitude (critical for blood volume inference) and shuffling disrupts temporal dependencies (key for heart rate variability). Alternatively, subject-aware contrastive learning avoids augmentations by leveraging long-term recordings from individual subjects \citep{abbaspourazad2024applefoundation}. While especially effective for biosignals \citep{jeong2024finding}, existing methods rely on naive temporal sampling \citep{tonekaboni2021tnc} or supplemental context (e.g. event labels) \citep{jeong2023event}. To address this, we propose directly exploiting raw PPG semantics in our large-scale unlabeled dataset, eliminating reliance on augmentations or auxiliary data. To unlock the potential of our large-scale unlabeled wearable field PPG dataset, we need to design a methodology that utilizes semantic information from only the raw PPG signals. 

A critical characteristic of raw PPG signals lies in their origins in the cardiopulmonary system. PPG signals are pulsative: quasiperiodic and composed of repeating motifs that correspond to individual cardiac cycles. We define motifs abstractly as the short temporal shapes within the overall PPG morphology that reflect specific semantic information, such as the first upward slope in a specific PPG beat that represents systolic rise time. These motifs encode critical physiological information, such as heart rate variability, blood pressure changes, and vascular dynamics. 

Therefore, we design our pre-training task to learn differences between PPG instances, based upon the differences in their motifs. In this way, we utilize a distance function, $d(\textbf{X}_{\text{anchor}}, \textbf{X}_{\text{cand}})$ that compares two PPG sequences, $\textbf{X}_{\text{anchor}} \in \mathbb{R}^{T}$ and $\textbf{X}_{\text{cand}} \in \mathbb{R}^{T}$ by finding the motif in $\textbf{X}_{\text{cand}}$ that best matches a given motif $\textbf{X}_{\text{anchor}}$. This idea can be seen below in Equation \ref{eq:distance}, with the "matching" function given by a distance function $d_m$.
\begin{align}
    d(\textbf{X}_{\text{anchor}}, \textbf{X}_{\text{cand}}) \approx \sum_{i=0}^T \underset{j \in [0,\cdots, T]} {\text{argmin }} \Big\{ d_m\Big(\textrm{Motif}\big(\textbf{X}_{\text{anchor}}[i]\big), \textrm{Motif}\big(\textbf{X}_{\text{cand}}[j]\big)\Big)\Big\} \label{eq:distance}
\end{align}

This distance function would enable semantically aware comparisons between PPG sequences, learning an embedding space that captures physiologically meaningful relationships.

\subsubsection{Exploiting the Noise of Real-World, mHealth PPG data:} \label{sec:robustness} \hfill\\

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/ppg_snippet.png}
    \vspace{-.3cm}
    \caption{\textbf{Random 5s Snippets of PPG signals from Clinical, Wearable Lab, and Wearable Field Settings}. Signal quality declines gradually but manageably when moving from clinical-grade PPG in hospital settings to wearable PPG in controlled lab conditions. However, a marked deterioration occurs in the uncontrolled real-world environment of the wearable field setting, driven by daily wear factors, such as motion artifacts~\cite{pollreisz2022detection}, ambient light \citep{cubas2023design}, and poor sensor contact \citep{lo2024advanced}.
    }    
    \label{fig:fieldhard}
\end{figure}


Many prior works that utilize morphology information from raw PPG signals for their task-specific application, such as for blood pressure prediction \citep{salah2022beat, shin2017feasibility} or stress detection \citep{ahmed2022ppg, rinkevivcius2019photoplethysmogram}, will first utilize a peak detection algorithm to segment out individual PPG beats that correspond to as single cardiac cycle  \citep{haddad2020beat, kavsaouglu2016innovative, fischer2016algorithm}. After segmentation, specific temporal features within the beat can be investigated, such as the dicrotic notch or the systolic rise time. However, these methods are evaluated exclusively on clinical or lab-collected PPG data, where signal quality is controlled and high. 

Exploiting PPG morphological information is significantly more challenging in real-world wearable field PPG data, where persistent noise and motion artifacts, make beat-to-beat segmentation and morphological feature extraction infeasible. As shown in Figure \ref{fig:fieldhard}, the sharp decline in PPG signal quality—ubiquitous in mobile health settings—severely undermines the reliability of traditional segmentation and feature isolation methods.

% Beat-by-beat segmentation on real-world, wearable field PPG data is much harder, where motion artifacts, ambient noise, and skin condition variability render beat-to-beat segmentation and morphological feature extraction infeasible. As shown in Figure \ref{fig:fieldhard}, the sharp decline in PPG signal quality—common in mobile health settings—severely degrades the reliability of traditional segmentation and feature isolation methods.

% We can clearly see this in Table~\ref{tab:performance_comparison}, in which PaPaGei, a method that identifies PPG morphology via segmentation, performs much more poorly than our models, especially for the mHealth field tasks. 


% To address this challenge, we utilize a learnable motif-based distance function that is specifically trained on mHealth field PPG data, shown in Equation \ref{eq:rebardistance} below. Instead of relying on predefined morphological features or clean beat segmentations, our approach learns to identify motifs directly from noisy, real-world signals. By training the distance function under the same noisy conditions as the target application, we can exploit the diversity of motifs present in field data. Importantly, noise in the data is not treated as a detriment but rather as a potential source of useful information. For instance, certain noise patterns may not occur randomly but could instead correlate with specific activities, environmental conditions, or physiological states. These patterns can help make certain PPG motifs more unique, enabling the model to identify semantic clusters and learn robust representations that generalize across diverse real-world scenarios. 

To address this challenge, we utilize a learnable motif-based distance function (Equation \ref{eq:rebardistance}) trained directly on noisy wearable field PPG data. Unlike traditional approaches that depend on predefined morphological features or clean beat segmentation, our method learns to identify motifs from raw, uncurated signals. By training under real-world noise conditions, we leverage the inherent diversity of field data motifs. \textbf{Crucially, noise patterns are not discarded but instead leveraged as meaningful contextual cues: non-random artifacts (e.g., from motion or environment) correlate with specific activities, states, or conditions, enhancing motif uniqueness.} This enables the model to identify semantic clusters and learn robust representations generalizable to diverse real-world scenarios.
\begin{align}
    d(\textbf{X}_{\textrm{anc}},\textbf{X}_{\textrm{cand}})&\coloneqq \sum_{\textbf{x}_\textrm{anc} \in {\textbf{X}_\textrm{anc}}} \Bigg( \bigg(
    \sum_{\textbf{x}_\textrm{cand}  \in {\textbf{X}_\textrm{cand}}}  \underset{{\textbf{x}_\textrm{cand} \in {\textbf{X}_\textrm{cand}}}}{\textrm{softmax}} \Big(  \textrm{sim}\big( f_q( {\textbf{x}}_{\textrm{anc}} ), f_k( \textbf{x}_{\textrm{cand}}) \Big) f_v( \textbf{x}_{\textrm{cand}}) \bigg) -\textbf{x}_{\textrm{anc}} \Bigg)^2 \label{eq:rebardistance} \\
    % \hat{\textbf{X}}_{\textrm{anc} \vert \textrm{cand}}  = & = \sum_{\textbf{x}_\textrm{cand} \in {\textbf{X}_\textrm{cand}}} \underset{{\textbf{x}_\textrm{cand} \in {\textbf{X}_\textrm{cand}}}}{\textrm{softmax}} \Big( \textrm{sim}\big( f_q( {\textbf{x}}_{\textrm{anc}} ), f_k( \textbf{x}_{\textrm{cand}}) \Big) f_v( \textbf{x}_{\textrm{cand}}) \label{eq:rebarsoftmax} \\
    f_{\{q/k/v\}}( {\textbf{X}}) &= \textrm{DilatedConvNet}_{\{q/k/v\}} \big( {{\textbf{X}} } \big) \label{eq:rebardilconv} 
    % \textbf{x}_{k} \textbf{W}_v
    % &= \sum_{\textbf{x}_k \in S_{\textbf{X}_k}} \frac{k (\textbf{x}_q, \textbf{x}_k)}{\sum_{\textbf{X}_k' \in S_{\textbf{X}_k'}} k (\textbf{x}_q, \textbf{x}_k')} v(\textbf{x}_k) \\
    % &= \mathbb{E}_{p(\textbf{x}_k|\bar{\textbf{x}}_q)}[f_v( \textbf{x}_{k})]
\end{align} % \textrm{Recon}({\textbf{X}}_\textrm{anc} \vert {\textbf{X}_\textrm{cand}}) 
where $\smash{\textbf{X} \in \mathbb{R}^{T \times D}}$ and $\smash{\textbf{x}\in \mathbb{R}^{D}}$ with $T$ as the time length, and $sim()$ is a dot product. This function is implemented with a standard cross-attention block \citep{vaswani2017attnisallyouneedtransformer} with projection  and normalizations layers been omitted for brevity. 
\\ 

This trainable function closely matches our idea presented in Equation \ref{eq:distance}. $d_m$ is modeled as a reconstruction error, where the motifs of $\textbf{X}_{\text{cand}}$ that best matches a given motif $\textbf{x}_{\text{anchor}}$ are used to reconstruct $\textbf{x}_{\text{anchor}}$, in a kernel regression. Motifs are captured with the dilated convolutions of $f_{\{q/k/v\}}$, as seen in Equation \ref{eq:rebardilconv}. The weights in the kernel regression found in $\smash{(\sum_{{\textbf{X}_\textrm{cand}}}  {\textrm{softmax}})}$ serve the purpose of the original argmin, where the function is learned to find the weights that minimize the reconstruction error. 

% This methodology builds upon prior work by \citep{xurebar}, which demonstrated a proof of concept using toy scenarios across various physiological signals. Their experiments were limited to pre-training and evaluating on the same small, labeled dataset, which does not reflect the scalability or generalizability required for a foundation model. In contrast, our foundation model approach involves pre-training on a large, unlabeled dataset and evaluating across a diverse range of downstream tasks and datasets. Furthermore, their work did not address the challenges of noisy field conditions, which are critical for practical deployment in mobile health settings. We extend this idea by adapting and refining the methodology to handle the complexities of real-world, noisy PPG data, demonstrating its effectiveness in field conditions where noise and variability are inherent.


% Our pre-training task focuses on learning a distance function based on these motifs. Specifically, we aim to measure the similarity between motifs in a way that captures both their shape and temporal dynamics. This involves designing a contrastive learning framework where positive pairs are motifs from the same individual under similar physiological conditions, while negative pairs are motifs from different individuals or under differing conditions. By learning to distinguish between similar and dissimilar motifs, the model can capture the intrinsic structure of PPG signals, enabling it to generalize across diverse datasets and tasks. This approach not only addresses the noise and variability in mHealth PPG data but also ensures that the learned representations are robust and physiologically meaningful.

This work builds on \citep{xurebar}, who proposed a proof-of-concept motif-based distance for physiological signals in controlled, small-scale settings. However, their approach was limited to pre-training and evaluation on the same small labeled dataset, lacking the scalability and generalizability required for real-world foundation models. In contrast, our foundation model is pre-trained on a large, unlabeled field dataset and evaluated across diverse downstream tasks and datasets. Critically, we address noisy field conditions—a gap in prior work—by refining the methodology to handle real-world PPG variability, demonstrating robustness in dynamic, noisy environments.

% Our pre-training task learns a motif-based distance function that captures both shape (e.g., pulse waveform morphology) and temporal dynamics (e.g., heart rate trends). We design a contrastive framework where positive pairs are motifs from the same individual under similar physiological states, while negative pairs are motifs from different individuals or states. By distinguishing these pairs, the model learns intrinsic PPG structure, enabling generalization across tasks while preserving physiological meaning and robustness to noise.

\subsubsection{Capturing Subtle Yet Meaningful Patterns:}  \hfill\\

Given our motif-based distance function, we can identify positive and negative pairs to draw together and push apart in the embedding space, respectively. Traditionally, contrastive approaches will utilize the Normalized-Temperature Cross-Entropy Loss (NT-Xent) function (Equation \ref{eq:ntxent}), in which the positive pair is pulled towards the anchor, and all negatives are pushed away, equally. 
\begin{align}
     \ell(\textbf{X}_{\textrm{anc}}, \textbf{X}_{\textrm{pos}}, \mathcal{S}_{\textrm{neg}})  = - \log \frac{\exp(\textrm{sim}(\textbf{X}_{\textrm{anc}},  \textbf{X}_{\textrm{pos}}) / \tau)}{\sum_{\textbf{X}_{\textrm{neg}} \in \mathcal{S}_{\textrm{neg}}} \exp(\textrm{sim}(\textbf{X}_{\textrm{anc}},  \textbf{X}_{\textrm{neg}}) / \tau) + \exp(\textrm{sim}(\textbf{X}_{\textrm{anc}},  \textbf{X}_{\textrm{pos}}) / \tau)} \label{eq:ntxent}
\end{align}
The standard NT-Xent approach’s reliance on rigid positive/negative assignments makes it prone to false positives (misclassifying dissimilar instances as positives) and false negatives (overlooking valid positives in the candidate pool). For example, in stress assessment, being stressed due to bad traffic differs from the experience of being tailgated, but the two are more similar compared to being engaged in a verbal conflict. Treating all negatives equally ignores such nuances, resulting in overly coarse clusters that fail to capture subtle semantic relationships. 

An alternative idea to address this is utilizing a metric learning loss function \citep{kim2019metricbeyondlogratio, hoffmann2022metricrince, kan2021metricroc}, in which our self-supervised learning encoder learns to embed the PPG instances exactly as described by our motif-based distance function.  However, although our learned motif-based distance function will be able to capture semantic information, it was trained without labels and is not expected to always, perfectly identify the correct semantic relationships.

A "goldilocks" approach between these two ideas would be using a relative contrastive learning framework that models the relative positions of the distances. As such, we utilize the concept of relative dissimilarity via multiple negative sets \citep{xu2024relcon}. For each positive pairing, a negative set is created by selecting samples whose distance from the anchor exceeds the distance of the positive pair. Each candidate in the dataset is iteratively used to form a positive pair, while the remaining candidates contribute to the negative set:
\[
f_{\textrm{neg}}({X}_{\textrm{anc}}, {X}_{\textrm{pos}}, \mathcal{S}) = \{{X} \in \mathcal{S} \mid d({X}_{\textrm{anc}}, {X}) > d({X}_{\textrm{anc}}, {X}_{\textrm{pos}}) \}
\]
This concept is incorporated into the Relative Contrastive Loss function, which builds representation spaces that are aware of relative distances and semantic nuances:
\begin{align}
    \mathcal{L}_{\textrm{RelCon}} = \sum_{{X}_{\textrm{i}} \in \mathcal{S}_{\textrm{cand}}} \ell({X}_{\textrm{anc}},\ {X}_{\textrm{pos}} \coloneqq {X}_{\textrm{i}},\ \mathcal{S}_{\textrm{neg}} \coloneqq f_{\textrm{neg}}({X}_{\textrm{anc}}, {X}_{\textrm{i}}, \mathcal{S}_{\textrm{cand}})) \label{eq:relconloss}
\end{align}
By integrating relative dissimilarity into contrastive learning, the model forms tight clusters in a representation space reflecting fine-grained relationships. This improves generalization to unseen users by capturing within- and between-class nuances, ensuring robustness to intra-user variability and adaptability to new users’ physiological patterns. The resulting representations reliably handle field data variability for users unseen during training.

\section{Pre-training Procedure and Implementation for Pulse-PPG}

In this section, we provide the technical details of our foundation model's implementation and pre-training. Section \ref{sec:modelpretraining} outlines how we employ our learnable motif-based distance function and relative contrastive loss to train the Pulse-PPG foundation model. Section \ref{sec:pretraining_data} describes the composition and characteristics of our pre-training dataset, detailing how we process the wearable field PPG data to align with our pre-training task, as well as highlighting the dataset's inherent properties that make it well-suited for learning robust representations in noisy, real-world conditions. Finally, Section \ref{sec:implementation} describes specific implementation details, such as specific architecture and total GPU compute used. Please refer to Appendix \label{sec:hyperparams} for specific hyperparameter details. 
% or our code repository at \url{https://github.com/maxxu05/Pulse-PPG}.

\subsection{Model Pre-training Details} \label{sec:modelpretraining}

\subsubsection{Training the Learnable Motif-based Distance Function} \label{sec:trainrebar} As mentioned previously in Section \ref{sec:robustness}, making our distance function learnable is a key strength of our approach, as it enables motif-comparisons between PPG sequences, even when such PPGs are afflicted with noise. Therefore, before utilizing our relative contrastive loss, we learn our distance function in an unsupervised fashion. 


In our initial label-free pre-training setting, we do not know if a random pair of PPG sequences share class labels, and thus we do not know if reconstruction error should be minimized to learn a motif-matching similarity function between them. Therefore, during training, we set ${\textbf{X}}_q$ and $ {\textbf{X}}_k$ to be the same value and apply a missingness mask on ${\textbf{X}}_q$. This mask has a contiguous segment of missingness that stretches 2 seconds. In this way, our distance function will learn to retrieve the regions from the key $\textbf{X}_k$ that match the missing 2 second motif from the query, $\bar{\textbf{X}}_q$, for reconstruction. Then, after training, we can utilize this distance as a static function for the RelCon loss to identify relative distances between ${\textbf{X}}_q $ and $ {\textbf{X}}_k$, where ${\textbf{X}}_q $ and $ {\textbf{X}}_k$ are different instances with different values.


\subsubsection{Training the Pulse-PPG Foundation Model} Now, once we have our trained motif-based distance function, we can use it to identify relative distances of a candidate set of PPG sequences from an anchor PPG sequence.  This candidate set of PPG sequences is sampled from two sources: within-subject and between-subject. 
\begin{itemize}[leftmargin=.7cm,  noitemsep]
    \item Sampling within-subject, across-time, draws candidate PPG sequences from the same subject, but at a different time-window from the anchor. This allows us to model how an individual's behavioral patterns throughout their day affects their PPG signal, for example via exercise. 
    \item Sampling between-subject draws candidate PPG sequences from other persons within a batch. This allows us to model how the similarities and differences in an individual's specific physiology and routines affects their PPG signal.  
\end{itemize}

Given this candidate set, we can sort the candidates with our motif-based distance function found in Equation \ref{eq:rebardistance} and then apply our RelCon loss function in Equation \ref{eq:relconloss} based on their relative distances from the anchor. As such, our Pulse-PPG foundation model will be able to capture both within-subject and between-subject semantic information within the embedding space.



\subsection{Pre-training on a Field Wearable PPG Dataset} \label{sec:pretraining_data}
\subsubsection{Dataset Description:}  Our pre-training dataset is composed of 822,247 unique 4-minute 50 Hz PPG segments, with 120 participants who wore a smartwatch a day for up to 100 days. This is from the Mobile Open Observation of Daily Stressors (MOODS) study dataset. The study was conducted using a WearOS app for smartwatches, a cross-platform smartphone app, and cloud services. Detailed descriptions of the System and Study Design of the MOODS Study are available in~\cite{neupane2024momentarymoods}.


\subsubsection{Minimal Preprocessing:} To pre-train the physiological model, we utilize raw photoplethysmography (PPG) data collected from the participants. Out of 122 participants, the dataset recorded PPG data from 120 participants, which we use for pre-training. PPG data collected in the field is noisy, however, avoiding signal specific filtering can provide certain advantages for generalized, robust representations e.g., adapting to dataset diversity, preserving subtle meaningful variations, and transferability. Hence, we refrain from applying any particular filtering technique, but focus on addressing between person variability through a global person-specific z-normalization. 

\subsubsection{Subject-wise Train/Val/Test Splits:} To pre-train a model capable of handling the substantial within- and between-subject variability, we randomly shuffled participants and created a train/val/test split of 84/18/18. This split was consistently applied across all downstream tasks. We deliberately adopted a subject-based splitting strategy to pose a more challenging task for the model: making inferences on data from entirely unseen individuals, a common scenario in real-world applications. 

\subsubsection{PPG Sequence Inputs for Pre-Training Model}
In our modeling approach, we use 4-minute-long windows as sequence inputs for the Pulse-PPG Foundation Model. This choice is grounded in research highlighting the cyclical nature of physiological responses to stress. Studies have shown that stress-induced patterns in heart rate and heart rate variability (HRV) follow cyclical behaviors, with durations typically ranging from 3.5 to 4.2 minutes \citep{lamichhane2017towards,wilson2018couples,bari2020automated}. For example, heart rate increases during stress and remains elevated until the stressor ends, after which it returns to baseline, forming a cyclical pattern. Field studies, such as those involving stressful conversations or work-related stress, have further validated these patterns, with median cycle durations around 4 minutes \citep{bari2020automated}.

By segmenting PPG signals into non-overlapping 4-minute windows, we aim to capture both micro-level perturbations (e.g., transient changes in pulse waveform morphology) and macro-level cyclical patterns (e.g., sustained heart rate elevation during stress). This approach allows our model to learn representations that are sensitive to the temporal dynamics of stress, which are critical for accurate stress detection. The 4-minute window size strikes a balance between capturing meaningful cyclical patterns and retaining fine-grained temporal details.

It is important to note that, although our initial pre-training inputs are 4 minutes long, our model incorporates a temporal pooling mechanism that enables it to generalize to inputs of varying lengths. This flexibility ensures that the model can be applied to diverse downstream tasks, where input durations may differ. 



\subsection{Pulse-PPG Model Implementation Details}  \label{sec:implementation} 

\subsubsection{Encoder Details:}
We utilize a 1D ResNet-26 network from the \url{https://github.com/hsd1503/resnet1d} code repository as our encoder backbone, with a total of 28.5m trainable parameters. There is instance normalization placed at the start of the ResNet, in order to account for PPG distribution shifts across users and across datasets. Our ResNet implementation utilizes an initial filter size of 128, a kernel size of 11, and a stride of 2, with 12 residual blocks that linearly increase the filter size every 4 blocks. There is a global average temporal pooling at the end of the network in order for our model to generalize across different time scales. As a result of this, our encoder embeds variable-length PPG signals into a single 1D 512-dimensional embedding vector. 


\subsubsection{Distance Function Details:}
We utilize the code from the \url{https://github.com/maxxu05/rebar} code repository in order to implement the distance function. This is a lightweight model, used as a static function to identify relative distances for the RelCon loss function and only has 127k trainable parameters. The dilated convolution network is a series of a dilated convolution blocks, made up of a dilated convolution layer with residual connection skip, followed by a ReLU and instance norm. There are 5 blocks, with an initial dilation of 1 that doubles after each layer. The filter size is set to 64, the kernel size is set to 15, and stride is set to 10. The input convolution layer to the DilatedConvNet is set to be a partial convolution \citep{liu2018partialconv} in order to handle missingness in our pre-training and application procedures. The reconstruction error used as our distance function is a standard MSE loss.


\subsubsection{Compute Details:} 
Our Pulse-PPG foundation model was trained for 5 days for 6 epochs, where each epoch is composed of 606,833 unique 4-minute PPG segments or about 5,424 unique participant days. The distance function was trained over 10 hours for 20 epochs. We utilized NVIDIA L40S GPUs for our experiments, and models were constructed with the Pytorch python package.



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/visual_comp.png}
%     \caption{Visual comparison of Pulse-PPG and PaPaGei performance across different pre-training settings (clinical clean vs. wearable field) on various downstream tasks. Tick marks indicate the best-performing model in each task.}
%     \label{fig:enter-label}
% \end{figure}


\section{Results}
The power of a foundation model lies in its ability to generalize across a broad range of tasks and datasets. This makes them especially valuable to the research community, providing pre-trained, general-purpose representations that can be easily adapted to new tasks, streamlining research efforts without the need for extensive task-specific data collection or manual feature engineering. Therefore, we design our experiments to validate this by evaluating our Pulse-PPG Foundation Model on 11 different downstream tasks, across 5 datasets, spanning typical wearable PPG tasks (i.e., stress classification) and typical hospital PPG tasks (i.e., systolic blood pressure regression). We evaluate this through a series of 3 experiments, 1) Pulse-PPG vs.\ A Prior PPG Foundation Model, 2) Assessing Field-to-Lab Generalizability, 3) Evaluation of Pulse-PPG Use-cases. 


In the following sections we will describe our experimental design and results. In Section \ref{sec:downstream}, we describe our downstream tasks and datasets. Then in Section  \ref{sec:exppapagei}, \ref{sec:expfieldtolab}, and \ref{sec:finetuneeval}, we describe the design for each of our experiments and their results.

\subsection{Downstream Datasets and Tasks} \label{sec:downstream}
% To evaluate the field-to-lab generalizability of our PPG foundation model, we benchmark it across multiple datasets spanning clinical and mHealth applications for various downstream tasks. These datasets, selected by PaPaGei, cover a wide range of health-related indicators and outcomes, ensuring a thorough evaluation of the pre-trained model's generalizability.
In order to evaluate the generalizability of our Pulse-PPG foundation model, we evaluate our 5 downstream datasets and 11 tasks, which can be seen in Table \ref{tab:ppg_comparison}. Our datasets have a range of different PPG settings. 
\begin{itemize}[leftmargin=.7cm,  noitemsep]
    \item Wearable Field PPGs reflect the real-world setting for wearable applications with low signal quality.
    \item Wearable Lab PPGs originate from controlled lab studies, in which subjects are given a wearable PPG sensor and asked to do a set of scripted activities under supervision. Generally, signal quality is higher.
    \item Clinical PPGs originate from a hospital or another heavily monitored environment, in which PPG signals are collected from a finger sensor on stationary patients. Generally, signal quality is high.
\end{itemize}  

\begin{table} % [!htbp]
\centering
\renewcommand{\arraystretch}{1.2} % Adjust row height for better spacing
\resizebox{\textwidth}{!}{%
\begin{tabular}{l ccccc}
\toprule
\textbf{}                        & \textbf{MOODS} \citep{neupane2024momentarymoods} & \textbf{PPG-DaLiA} \citep{reiss2019ppgdalia} & \textbf{WESAD} \citep{schmidt2018wesad} & \textbf{SDB} \citep{garde2014development} & \textbf{PPG-BP} \citep{elgendi2022datasetppgbp}\\ 
\midrule
\textbf{PPG Type}                & Wearable        & Wearable            & Wearable        & Clinical     & Clinical         \\ 
\textbf{Quality Type}         & Field          & Clean              & Clean          & Clean        & Clean            \\ 
\textbf{Num of Subjects} & 120            & 15                 & 15             & 146          & 219              \\ 
\textbf{Time per Subject}  & 100 Days       & 103 Minutes        & 60 Minutes    & 360 Minutes           & 10 Seconds               \\ 
\midrule
\textbf{Tasks Available}         & \parbox{2.5cm}{\centering Stress (2) \\ Activity (2)}  
                                 & \parbox{2.5cm}{\centering Instant HR (R) \\ Activity (9)}  
                                 & \parbox{2.5cm}{\centering Stress (4) \\ Stress (2)}  
                                 & \parbox{2.5cm}{\centering Sleep \\ Disturbance (2)}  
                                 & \parbox{2.5cm}{\centering Systolic BP (R) \\ Diastolic BP (R) \\ Avg. HR (R) \\ Hypertension (2)} \\ 
\bottomrule
\end{tabular}%
}
\caption{\textbf{Downstream Datasets and Tasks.} $(\boldsymbol{\cdot})$ designates either a regression task or \# of classes for a classification task.}
\label{tab:ppg_comparison}
\end{table}

Additionally, with each of PPG type, there are a set of canonical downstream tasks associated with them. For wearables, that includes stress classification~\cite{schmidt2018wesad,neupane2024momentarymoods}, activity classification~\cite{neupane2024momentarymoods,reiss2019ppgdalia}, and instantaneous HR regression~\cite{reiss2019ppgdalia}. 
% \mxnote{MITHUN OR SAMEER ADD CITATIONS}. 
For clinical settings, this includes sleep disturbance classification~\cite{garde2014development}, blood pressure regression~\cite{elgendi2022datasetppgbp}, and hypertension ~\cite{elgendi2022datasetppgbp}. 
% \mxnote{MITHUN OR SAMEER ADD CITATIONS}.
While activity classification is traditionally performed using IMU sensors or motion signals, we include it as a downstream task to showcase the versatility of our model in learning meaningful representations solely from PPG data.
Each of these canonical tasks are captured in our downstream datasets. Please see Appendix \ref{sec:evaldata} for further dataset details and \ref{sec:evalmethods} for evaluation implementation details.

% \mxnote{someone write something about the metrics we use here}
\subsubsection{Metrics}
For evaluating our models across diverse downstream tasks, we utilize task-specific metrics for both regression and classification problems. For regression tasks, we report standard Mean Absolute Error (MAE) and Mean Squared Error (MSE). Additionally, we use Mean Absolute Percentage Error (MAPE) to assess relative errors across different scales.

For classification tasks, we report macro F1 score, Accuracy, Precision, and Recall to evaluate overall model performance and the trade-offs between false positives and false negatives. To provide a more comprehensive assessment, particularly for imbalanced datasets, we compute Area Under the Precision-Recall Curve (AUPRC) and Area Under the Receiver Operating Characteristic Curve (AUROC), which measure the model’s ability to distinguish between classes.
% Blood Pressure, Heart Rate Estimation and Hypertension Classification: We used a dataset to evaluate the pre-trained model's performance on clinical tasks like Blood pressure, HR estimation and identifying individuals who suffer from hypertension. The PaPaGei model was pre-trained on 10 seconds of PPG sequence and tested with the same sequence length for all classification and regression tasks. Hence, we also created 10 seconds of PPG windows for downstream evaluation with splitting participants into train/val/test groups consistent with the splits created by PaPaGei.
% The training split was used to train linear regression probes on embeddings from the self-supervised models.


\subsection{Experiment 1: Pulse-PPG vs. A Prior PPG foundation model} \label{sec:exppapagei}
\subsubsection{Background}
In order to evaluate the effectiveness of our Pulse-PPG foundation model, we benchmark against the current state-of-the-art open PPG foundation model, PaPaGei \cite{pillai2024papagei}. Their PaPaGei-S model uses a 1D ResNet backbone and leverages a novel self-supervised objective with PPG-morphology-specific metrics, like the Inflection Point Area ratio, achieving top performance on primarily, clinically focused downstream tasks. Crucially, they release their model weights and code to the public (\url{https://zenodo.org/records/13983110}), which we utilize for our experiments.

% We use the publicly available weights of PaPaGei-S available here: \url{https://zenodo.org/records/13983110}.


However, their pre-training dataset was composed solely of pre-processed, clean clinical PPG signals, and they do not demonstrate generalizability to wearable field PPG settings. As previously discussed in Section \ref{sec:robustness}, we hypothesize that their PPG-morphology-specific metrics will be unable to generalize well in the much noisier, field setting. This is unlike our learned distance metric, which can potentially learn to do motif-similarity comparisons, even in noisy settings.

We also note that while PaPaGei demonstrated strong performance across various datasets in their work, their evaluation methodology involved using a different version of the foundation model for \emph{each task}. Thus, while PaPaGei sets a critical stepping stone for PPG foundation modeling, our goal is to build a single open-source\footref{open} PPG foundation model that can generalize across many tasks, without re-training the network from scratch.
% i could have talked about this in the related work, but i decided to talk about it here so that it is less aggressive OKAY THANKS SURE FINE

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figures/comparison_1.png}
    \caption{\textbf{Comparison of Pulse-PPG vs. PaPaGei, another PPG foundation model \citep{pillai2024papagei}}. The two plots shows relative performance of a linear probe evaluation for each model, with the left for classification via F1 score and the right for regression via Mean Average Percentage Error. 10/11 of the task data points reside above the slope, demonstrating how Pulse-PPG consistently outperforms PaPaGei, with particularly substantial improvements for the Wearable Field and Lab PPG tasks. See further discussion in Section \ref{sec:badonsdb}.    
    }
    \label{fig:relconvspapagei}
\end{figure}


\subsubsection{Experimental Design}
To ensure a fair comparison, we utilize a linear probing evaluation for each downstream task for both the Pulse-PPG and PaPaGei-S model. Linear probing is a widely accepted evaluation method for foundation models because it isolates the quality of each foundation model's learned representations by freezing the model's embeddings and training only a simple logistic classifier or linear regression. Both PaPaGei and Pulse-PPG embed PPG sequences into a single 1D 512-dimensional vector, making their linear probe results directly comparable. By using linear probing—training a simple classifier on frozen embeddings—we isolate the quality of the learned representations, avoiding biases from architectural differences. 

To re-emphasize, this linear probing evaluation is \emph{not intended to achieve maximum task-specific performance}, instead serving as a controlled experiment to enable a fair comparison between foundation models. 

% In linear probing, the pre-trained weights of a model are frozen, and used only to compute feature representations from the input data. A lightweight linear classifier or regressor is then trained on these features, without modifying the pre-trained model itself. This permits evaluation of its learned representations against Pulse-PPG, which is also tested using linear probing. 

\subsubsection{Results} \label{sec:badonsdb} Figure \ref{fig:relconvspapagei} visualizes the difference in performance between our Pulse-PPG foundation model versus the publicly released pre-trained PaPaGei. Full results, can be found in the \textit{1st and 4th data columns} (i.e. Wearable Field Pulse-PPG and Clinical PaPaGei) in Table \ref{tab:exp23} in the Appendix. Our Pulse-PPG achieves much stronger performance, outperforming PaPaGei on 10/11 of our downstream tasks, consistently across the many reported metrics.

On the Wearable PPG tasks, our Pulse-PPG achieves a consistently large >.10 increase to F1 scores in the classification tasks (Stress - MOODS: 0.10 / Activity - MOODS: 0.15 / Activity - PPG-DaLiA: 0.12 / Stress (2) - WESAD: 0.15 / Stress (4) - WESAD: 0.14). Then on the instantaneous HR regression task, our Pulse-PPG model has large 10.24 improvement in MAE. These heart rate estimates are conducted on unfiltered wearable PPG data, which is significant due to the widely recognized challenge of noise-induced degradation in heart rate estimation. On the Clinical PPG tasks, our Pulse-PPG model continues to outperform PaPaGei, with $\sim0.5$ improvement to MAE for Systolic BP, Diastolic BP, and average HR regression, as well as a .05 improvement to F1 score on hypertension classification.  While clinical evaluation tasks show smaller improvements compared to wearable ones, the consistent gains highlight the model’s cross-domain generalizability.


The only task on which Pulse-PPG underperforms is the Sleep Disturbance task. This can be attributed to the nature of Pulse-PPG's pre-training dataset, MOODS. During the MOODS study, participants were instructed to wear the smartwatch during waking hours \citep{neupane2024momentarymoods}. Consequently, Pulse-PPG was not exposed to PPG signals during sleep in its pre-training phase, limiting its ability to learn sleep-related patterns, since human physiology during sleep is significantly different than that of waking hours due to natural biological processes~\cite{smets2018large}. In contrast, PaPaGei was pre-trained on the MESA dataset \citep{chen2015mesa}, which is a sleep study dataset.



\subsection{Experiment 2: Assessing Field-to-Lab Generalizability} \label{sec:expfieldtolab}


\subsubsection{Background:} Traditionally, mHealth models were built and tested in controlled lab settings due to a lack of diverse field datasets. As more real-world data became available, researchers began applying these lab-trained models to field data. However, we hypothesize that pre-training foundation models directly on diverse field data will yield better generalizability and performance.

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figures/comparison_2.png}
    \caption{\textbf{Comparison of pre-training Pulse-PPG on Wearable Field data vs. Clinical Data}. The two plots shows relative performance of a linear probe evaluation for each model, with the left for classification via F1 score and the right for regression via Mean Average Percentage Error. 10/11 of the task data points reside above the slope, demonstrating how pre-training on wearable field data is highly generalizable, demonstrating strong field-to-lab generalizability and even field-to-clinical generalizability. This is contrary to the current standard line of research, which tends to focus on lab-to-field methods.  See further discussion in Section \ref{sec:pretrainresults}.}  
    \label{fig:pretrainwearablevsclinical}
\end{figure}




\subsubsection{Experimental Design} 
In these experiments, we assess how pre-training on wearable field PPG compares to pre-training on clean PPG. Similar to the prior experiments in Section \ref{sec:exppapagei}, we utilize a linear probe for evaluation, to ensure fair comparisons. From the prior experiments, we already have our Pulse-PPG pre-trained on field PPGs and PaPaGei pre-trained on the clinical PPGs.


In order to construct a large clean clinical PPG pre-training dataset for re-training Pulse-PPG, we utilize the curated MIMIC-III PPG waveform dataset from \citep{xu2022pulseimpute}. This dataset is composed of 151,738 100 Hz 5-minute-long clean clinical PPGs from 18,210 patients.\footnote{Unfortunately, PaPaGei did not release their full curation code for constructing their clinical pre-training dataset, making it impossible for us to train on the same clinical data. However, MIMIC-III waveform did form a portion of their dataset.} We then use this clean clinical PPG dataset to pre-train our Pulse-PPG model, following the same training procedures and hyperparameters that we used when we were pre-training on the MOODS field PPG dataset.

Next, we pre-train PaPaGei from scratch with our MOODS field dataset, using its original training procedure from their codebase located here: \url{https://github.com/Nokia-Bell-Labs/PaPaGei-foundation-model}. 



\subsubsection{Results} \label{sec:pretrainresults} Figure \ref{fig:pretrainwearablevsclinical} visualizes the difference in performance between Pulse-PPG pre-trained on the wearable field PPGs vs. pre-trained on clinical PPGs. Full results, including  comparisons of PaPaGei pre-training with wearable field vs. clinical data can be found in Table \ref{tab:exp23} in the Appendix, while referencing the second row named "Pre-Train Data". 

Pulse-PPG consistently exhibits stronger performance on 10 out of 11 tasks when pre-trained on the Field Wearable PPG data versus when pre-trained on the Clean Clinical PPG.  For all other tasks except for sleep disturbance, the field wearable pre-training yielded consistently stronger results, sometimes substantially so. For example, for Field Wearable datasets, F1 score for stress improved by .0848 and F1 score for activity improved by .1218. Even for the clinical tasks, we see that pre-training on the wearable PPGs achieves much stronger performance (i.e., .10 F1 improvement in hypertension prediction and $\sim1$ MAE improvement in systolic and diastolic BP regression). In some cases, such as in the WESAD evaluation datasets, one metric was worse, but all of the remaining metrics were stronger. 

Pre-training on field wearable data appears to yield a more robust and generalizable representation, outperforming clinical pre-training—even on clinical tasks. Although performance gains in clinical tasks are modest, the consistency demonstrates that pre-training on field wearable data performs as well as, and sometimes better than, pre-training on clinical data. As discussed earlier in Section \ref{sec:badonsdb}, relatively poor results on sleep can be explained by lack of sleep data present in the MOODS dataset. This trend is also observed in PaPaGei, though to a lesser extent. When pre-trained on wearable data, PaPaGei outperforms clinical pre-training in 6 out of 11 tasks, including five wearable PPG evaluation datasets (Activity - MOODS, Instant HR - PPG-DaLiA, Activity - PPG-DaLiA, Stress (2) - WESAD, and Stress (4) - WESAD). This aligns with the idea that pre-training and evaluating on wearable data introduces less domain shift. The difference in performances across pre-training settings between Pulse-PPG and PaPaGei can be attributed to their pre-training strategies. PaPaGei relies on PPG-specific morphologies, which may not generalize well across domains with varying PPG patterns (e.g. wearable vs. clinical). In contrast, Pulse-PPG leverages a learnable motif-similarity distance, enabling it to capture more general PPG patterns, which are applicable across different settings.



% \subsection{Downstream Experiment Set-up} \label{sec:downexpswhat}
\subsection{Experiment 3: Evaluation of Pulse-PPG Use-cases}\label{sec:finetuneeval}

\subsubsection{Background} In this section we quantitatively measure the effectiveness of Pulse-PPG for 11 downstream tasks, to provide further insight into its utility to the research community.

%%The value of foundation models lies in their accessibility and utility for the broader research community. By providing a general-purpose representation, they enable researchers to easily apply the model to their specific tasks, uncover hidden trends, or build upon the model for further innovation.

\subsubsection{Experimental Design} 
% Foundation models provide accessible, general-purpose representations that replace traditional handcrafted features with learned features, enabling researchers to leverage state-of-the-art AI tools without extensive expertise. This democratizes access, accelerates progress in fields like mobile health, and allows researchers to focus on higher-level tasks like hypothesis testing and application development.
%% To assess the versatility of our Pulse-PPG foundation model, 
We evaluate downstream performance using two standard evaluation methodologies:
\begin{itemize}[leftmargin=.7cm,  noitemsep]
    \item Linear Probe: The model’s embeddings are frozen, and a logistic classifier or linear regression is trained on the embedding vectors. This is a simple evaluation designed to evaluate the quality of the learned representations to generalize to new tasks.  By requiring minimal computational resources, it also enables a broader range of researchers to utilize the model’s learned representations. 
    \item Fine Tune: This evaluation is designed to boost task-specific performance by training the entire network end-to-end to adapt to a specific task. This highlights the adaptability of the foundation model, showing its potential to serve as a strong starting point for specialized tasks. % While computationally intensive, it is essential for applications requiring peak performance. 
\end{itemize}

In addition to this, we include a naive baseline for clear comparison. The naive classifier always predicts the majority class, and the naive regressor predicts the training mean. Though simple, this benchmark contextualizes our model’s performance, highlighting the value of learned representations and ensuring transparent evaluation.

% \begin{wraptable}{r}{0.6\textwidth}
\begin{table}[!htbp]
    \tiny 
    \caption{\textbf{Performance metrics for the Pulse-PPG foundation model across diverse wearable and clinical datasets.} There are three evaluation approaches: (1) Naive methods, which serve as a baseline; (2) Linear Probing, where the model’s embeddings are frozen and a simple classifier or regressor is trained, enabling a fair assessment of the general-purpose representations; and (3) Fine-Tuning, which adapts the entire network end-to-end for improved task-specific performance.} \vspace{-.2cm} \label{tab:exp1}
    \resizebox{.6\textwidth}{!}{%
    \begin{tabular}{>{\raggedright}p{.05cm}>{\centering}p{2.2cm}>{\raggedright}p{.8cm}>{\raggedleft}p{.1cm}ccc} 
    \toprule
     &  &  & & & \multicolumn{2}{c}{\textbf{PPG RelCon}}\\
    \midrule
    & \multicolumn{3}{r}{\textbf{Pre-Train Data:}} & - & \multicolumn{2}{c}{\textbf{Wearable Field} } \\
    \midrule
    & \multicolumn{3}{r}{\textbf{Eval Method:} } & Naive & Linear Probe & Fine-Tune \\
    \midrule
    \multirow{21}{*}{\textbf{\rotatebox{90}{\centering Wearable Lab}}}
    & \multirow{3}{*}{\centering\parbox{2.2cm}{\centering \textbf{Instant HR (R)} \\ PPG-DaLiA}}
    & MAE  & \multirow{3}{*}{\rotatebox[origin=c]{-90}{\parbox{.3cm}{\rightarrowfill}}} & 19.34 & 8.936 & \textbf{3.705} \\
    & & MSE  &  & 522.7 & 149.3 & \textbf{59.81} \\
    & & MAPE  & & 0.2742 & 0.1166 & \textbf{0.0484} \\
    \addlinespace[-2pt]
    \cmidrule(lr){2-7}
    \addlinespace[-2pt]
    & \multirow{6}{*}{\centering \parbox{2cm}{\centering \textbf{Activity (9)} \\ PPG-DaLiA}}
    & F1 Score  & \multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox{.5cm}{\rightarrowfill}}} & 0.0455 & 0.3039 & \textbf{0.3648} \\
    & & Accuracy  &  & 0.2572 & 0.4104 & \textbf{0.4234} \\
    & & Precision & & 0.0286 & 0.3882 & \textbf{0.4398} \\
    & & Recall  & & 0.1111 & 0.3107 & \textbf{0.3663} \\
    & & AUPRC  & & 0.0728 & 0.3504 & \textbf{0.4056} \\
    & & AUROC  & & 0.5000 & 0.8051 & \textbf{0.8246} \\
    \addlinespace[-2pt]
    \cmidrule(lr){2-7}
    \addlinespace[-2pt]
    & \multirow{6}{*}{\centering \parbox{2cm}{\centering \textbf{Stress (2)} \\ WESAD}}
    & F1 Score  & \multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox{.5cm}{\rightarrowfill}}} & 0.4065 & 0.8759 & \textbf{0.9392} \\
    & & Accuracy  & & 0.6849 & 0.8904 & \textbf{0.9452} \\
    & & Precision & & 0.3425 & 0.8688 & \textbf{0.9259} \\
    & & Recall  & & 0.5000 & 0.8848 & \textbf{0.9600} \\
    & & AUPRC  & & 0.3151 & 0.9410 & \textbf{0.9790} \\
    & & AUROC  & & 0.5000 & 0.9687 & \textbf{0.9852} \\
    \addlinespace[-2pt]
    \cmidrule(lr){2-7}
    \addlinespace[-2pt]
    & \multirow{6}{*}{\centering \parbox{2cm}{\centering \textbf{Stress (4)} \\ WESAD}}
    & F1 Score  & \multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox{.5cm}{\rightarrowfill}}} & 0.1496 & 0.5431 & \textbf{0.5582} \\
    & & Accuracy  & & 0.4270 & 0.6517 & \textbf{0.7079} \\
    & & Precision & & 0.1067 & 0.6003 & \textbf{0.5515} \\
    & & Recall  & & 0.2500 & 0.5716 & \textbf{0.5812} \\
    & & AUPRC  & & 0.2584 & 0.5671 & \textbf{0.5898} \\
    & & AUROC  & & 0.5000 & \textbf{0.7807} & 0.7630 \\
    \midrule
    \multirow{12}{*}{\textbf{\rotatebox{90}{\centering Wearable Field}}}
    & \multirow{6}{*}{\centering \parbox{2cm}{\centering \textbf{Stress (2)} \\ MOODS}}
    & F1 Score  & \multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox{.5cm}{\rightarrowfill}}} & 0.3859 & 0.5398 & \textbf{0.5872} \\
    & & Accuracy  & & 0.6285 & 0.6156 & \textbf{0.6247} \\
    & & Precision &  & 0.3143 & 0.5606 & \textbf{0.5911} \\
    & & Recall  & & 0.5000 & 0.5460 & \textbf{0.5859} \\
    & & AUPRC  & & 0.3715 & 0.4275 & \textbf{0.5729} \\
    & & AUROC  & & 0.5000 & 0.5691 & \textbf{0.5909} \\    
    \addlinespace[-2pt]
    \cmidrule(lr){2-7}
    \addlinespace[-2pt]
    & \multirow{6}{*}{\centering\parbox{2cm}{\centering \textbf{Activity (2)} \\ MOODS}}
    & F1 Score  & \multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox{.5cm}{\rightarrowfill}}} & 0.4574 & 0.6859 & \textbf{0.7992} \\
    & & Accuracy  & & 0.8430 & 0.8705 & \textbf{0.8930} \\
    & & Precision &  & 0.4215 & 0.7835 & \textbf{0.7974} \\
    & & Recall  & & 0.5000 & 0.6509 & \textbf{0.801} \\
    & & AUPRC  & & 0.1570 & 0.5835 & \textbf{0.8596} \\
    & & AUROC  & & 0.5000 & 0.8722 & \textbf{0.9196} \\
    \midrule
    \multirow{21}{*}{\textbf{\rotatebox{90}{\centering Clinical}}} 
    & \multirow{6}{*}{\centering\parbox{2cm}{\centering \textbf{Sleep Disturbance (2)} \\ SDB}} 
    & F1 Score  & \multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox{.5cm}{\rightarrowfill}}} & 0.3945 & 0.4630 & \textbf{0.5782} \\
    & & Accuracy  & & 0.6515 & 0.5364 & \textbf{0.6428} \\
    & & Precision & & 0.3258 & 0.4633 & \textbf{0.5911} \\
    & & Recall  & & 0.5000 & 0.4673 & \textbf{0.5772} \\
    & & AUPRC  & & 0.3485 & 0.3085 & \textbf{0.5904} \\
    & & AUROC  & & 0.5000 & 0.4437 & \textbf{0.6243} \\
    \addlinespace[-2pt]
    \cmidrule(lr){2-7}
    \addlinespace[-2pt]
    & \multirow{3}{*}{\centering\parbox{2cm}{\centering \textbf{Systolic BP (R)} \\ PPG-BP}} 
    & MAE  & \multirow{3}{*}{\rotatebox[origin=c]{-90}{\parbox{.3cm}{\rightarrowfill}}} &17.21 &13.62 &\textbf{12.33} \\
    % & & SDAE & &12.63 &10.05 &14.62 \\
    & & MSE  & &455.9 &286.5 &\textbf{280.7} \\
    % & & SDMSE  & &609.5 &388.7 &836.5 \\
    & & MAPE  & &0.1345 &0.1065 &\textbf{0.0939} \\
    \addlinespace[-2pt]
    \cmidrule(lr){2-7}
    \addlinespace[-2pt]
    & \multirow{3}{*}{\centering\parbox{2cm}{\centering \textbf{Diastolic BP (R)} \\ PPG-BP}} 
    & MAE  & \multirow{3}{*}{\rotatebox[origin=c]{-90}{\parbox{.3cm}{\rightarrowfill}}} &10.12 &8.878 &\textbf{8.695} \\
    % & & SDAE &  &6.748 &6.275 &7.831 \\
    & & MSE  & &147.9 &118.2 & \textbf{119.8} \\
    % & & SDMSE  & &206.8 &169.3 &252.4 \\
    & & MAPE  & &0.1407 &0.1232 & \textbf{0.1209} \\
    \addlinespace[-2pt]
    \cmidrule(lr){2-7}
    \addlinespace[-2pt]
    & \multirow{3}{*}{\centering\parbox{2cm}{\centering \textbf{Average HR (R)} \\ PPG-BP}} 
    & MAE  & \multirow{3}{*}{\rotatebox[origin=c]{-90}{\parbox{.3cm}{\rightarrowfill}}} &7.965 &4.003 &\textbf{3.697} \\
    % & & SDAE &  &5.885 &3.317 &7.931 \\
    & & MSE  & &98.08 &27.02 &\textbf{23.26} \\
    % & & SDMSE  & &122.9 &51.7 &237.5 \\
    & & MAPE  & &0.1176 &0.0573 &\textbf{0.0531} \\
    \addlinespace[-2pt]
    \cmidrule(lr){2-7}
    \addlinespace[-2pt]
    & \multirow{6}{*}{\centering\parbox{2cm}{\centering \textbf{Hypertension (2)} \\ PPG-BP}} 
    & F1 Score  & \multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox{.5cm}{\rightarrowfill}}} &0.4459 &0.6975 &\textbf{ 0.7213} \\
    & & Accuracy  & &0.8049 &\textbf{0.8130} & 0.7967 \\
    & & Precision & &0.4024 &0.7009 & \textbf{0.7031} \\
    & & Recall  & &0.5000 &0.6944 & \textbf{0.7633} \\
    & & AUPRC  & &0.1951 &0.4266 & \textbf{0.7408}\\
    & & AUROC  & &0.5000 &0.7971 & \textbf{0.8258} \\
    \bottomrule
    \end{tabular}%
    }
\end{table}
% \end{wraptable}


\subsubsection{Results} Table \ref{tab:exp1} shows the full set of results.

Overall, the results demonstrate that Pulse-PPG consistently outperforms the naive baseline across all tasks. When evaluated with a linear probe, the model’s frozen embeddings already yield competitive performance, highlighting the strength of the learned representations. For example, in the wearable field stress detection task, a traditionally difficult task, especially in the field setting, where noise and variability are inherent, a simple linear probe is able to achieve an F1 score of 0.5398, compared to the naive method achieving an F1 of 0.3859. Additionally, in the binary stress classification for the WESAD dataset, the simple logistic regression on our learned feature can achieve an F1 of 0.8759, more than doubling the naive model's F1 score of 0.4065. 

Fine-tuning further enhances performance by adapting the entire network to the specific task. This is evident across all tasks, where fine-tuning yields the best results. Although occasionally, linear probing may outperform fine-tuning for one metric, for the remaining metrics, the fine-tuned results are stronger. This underscores the model’s adaptability and its potential to serve as a robust starting point for researchers to adapt for their specialized applications.

The linear probe evaluation offers a computationally efficient means to leverage powerful representations, while fine-tuning maximizes task-specific performance. Together, they highlight the versatility and practical utility of foundation models in both exploratory and application-focused research settings.



% \begin{wraptable}{r}{0.6\textwidth}
%     \tiny 
%     \caption{Pulse-PPG Performance} \label{tab:exp1}
%     \resizebox{0.6\textwidth}{!}{%
%     \begin{tabular}{>{\raggedright}p{.05cm}>{\centering}p{2.2cm}>{\raggedright}p{.8cm}>{\raggedleft}p{.1cm}ccc} 
%     \toprule
%      &  &  & & & \multicolumn{2}{c}{\textbf{PPG RelCon}}\\
%     \midrule
%     & \multicolumn{3}{r}{\textbf{Pre-Train Data:}} & - & \multicolumn{2}{c}{\textbf{Wearable + Field} } \\
%     \midrule
%     & \multicolumn{3}{r}{\textbf{Eval Method:} } & Naive & Linear Probe & Fine-Tune \\
%     \midrule
%     \multirow{21}{*}{\textbf{\rotatebox{90}{\centering Wearable + Clean}}}
%     & \multirow{3}{*}{\centering\parbox{2.2cm}{\centering \textbf{Instant HR (R)} \\ PPG-DaLiA}}
%     & MAE  & \multirow{3}{*}{\rotatebox[origin=c]{-90}{\parbox{.3cm}{\rightarrowfill}}} & 19.34 & 8.936 & \textbf{3.705} \\
%     & & MSE  &  & 522.7 & 149.3 & \textbf{59.81} \\
%     & & MAPE  & & 0.2742 & 0.1166 & \textbf{0.0484} \\
%     % ... (rest of the table)
%     \bottomrule
%     \end{tabular}%
%     }
% \end{wraptable}







% \subsection{Clinical Tasks in Controlled Settings} Pulse-PPG achieves a mean absolute error (MAE) of 13.62 for systolic blood pressure (BP) estimation, outperforming PaPaGei, which reports an MAE of 14.39—representing a 5.6\% higher error. For diastolic BP estimation, Pulse-PPG achieves an MAE of 8.88, slightly better than PaPaGei’s 8.71. In heart rate (HR) estimation, our model demonstrates a significant advantage, achieving an MAE of 4 bpm, representing an improvement of x\% over PaPaGei. For hypertension detection, Pulse-PPG records an F1 score of 0.71, marginally lower than PaPaGei’s 0.77. Despite being pre-trained on noisy, unfiltered PPG data, Pulse-PPG learns a representation space that outperforms its counterpart from clinical, filtered ppg for two critical physiological markers---blood pressure monitoring and heart rate estimation. It should be further noted Pulse-PPG’s ability to recover reliable HR estimates from noisy ppg despite the well known deterioration in HR estimation due to noise.

% Moving on to the next two clinical tasks, we observe that Pulse-PPG is outperformed by PaPaGei in classifying individuals with sleep disorders, with F1 scores of x versus 0.55. A similar drop in performance is seen for mood disturbance classification, with results of y versus z. At first glance, these numbers might suggest that Pulse-PPG struggles to generalize for these tasks. However, a closer examination reveals a critical factor in the data collection process that likely explains these results. Pulse-PPG was trained on data collected during a field study where participants were instructed to wear the smartwatch during waking hours. In contrast, PaPaGei’s training involved three public datasets, one of which—MESA—was exclusively focused on sleep, capturing PPG data under physiological conditions specific to sleep. Human physiology during sleep is fundamentally different from that of waking hours due to natural biological processes. Hence, Pulse-PPG’s performance is inherently limited on tasks requiring knowledge of these distinguished patterns. But, this also underscores the need for wide coverage of diversity, and fine grained extraction of underlying patterns for foundation models to be generalizable across diverse tasks.       


% \subsection{mHealth Tasks in Controlled Settings}\label{sec:mHealth_Clc}
% Stress detection and human activity recognition are two mHealth tasks in lab settings that are still of primary interest to the research community. These two tasks provide a benchmark for assessing the effectiveness of learned representations across mental and physical health. To assess stress detection performance, we evaluated both binary (stress vs. non-stress) and multi-class classifications on the WESAD dataset. Pulse-PPG, utilizing a simple linear probe, achieved superior results with an F1-score of 0.88 and an accuracy of 0.89. In contrast,  employing the same probing approach for PaPaGei yielded an F1-score of 0.8 and an accuracy of 0.8. This demonstrates that leveraging natural repetitions within extended time-series to identify both close and  distant sequences with relativel rankings significantly improved performance, resulting in an x\% gain in F1-score and a y\% gain in accuracy. Moreover, the model's ability grows due to its exposure to a wide range of diversity with clean to noisy data, enabling a smoother transition from field to controlled settings compared to transitions between solely controlled settings. Finally, we evaluate the models on a more challenging task: classifying different session types (e.g., `'baseline', `'meditation', `'amusement', `'stress') where physiological arousal levels for exhibit subtle distinctions. Pulse-PPG outperformed PaPaGei on this task too demonstrating its ability to distinguish subtle/minimum variation with reasonable accuracy for a foundation model.

% Next, we evaluate human activity recognition performance by benchmarking Pulse-PPG against PaPaGei using the PPG data from a dataset collected during various physical activities. While accelerometer can capture distinctive movement patterns across different activities with similar intensity, physiological data, such as PPG, is inherently limited in its ability to differentiate similar arousal patterns across activities. This means PPG can only learn activity-related patterns to a certain extent. Nevertheless, the reasonable performance achieved in multi-class activity recognition suggests that the underlying representation space learned from raw PPG data in the field is of high quality. When comparing our results to those of PaPaGei, we observe an improvement of X\% in F1 score and Y\% in accuracy, demonstrating the effectiveness of our representation space for a task typically performed with data from a different modality. And for the remaining task of heart rate estimation from, we have comparable performance with PaPaGei with an increased error rate of only 0.22\%.


% \subsection{mHealth Tasks in Field Settings}
% The field dataset collected ppg and accelerometry information with self-reported stress labels from participants for prompted events. Accordingly, for the field settings too, we are comparing Pulse-PPG and PaPaGei for the two key mHelath tasks involving stress and activity. However, as we have mentioned in~\ref{sec:mHealth_Clc}, the primary utility of an outside-domain task like activity detection from ppg was to assess the strength of representation. Hence, instead of activity detection from field, we set check on an easier task of stationary vs. motion classification to find out if PaPaGei can minimize the gap in performance. For stationary vs. motion labels in the field data, we applied a pre-trained activity model on 20-sec segments of raw ppg data from the first 10 days of the participants which is representative enough for detection of stationary vs. motion. Next, to ensure a fair comparison, we pre-trained PaPaGei on the field dataset and then liner-probed it for motion classification. We also liner-probe Pulse-PPG for the same purpose. The classification results show that despite a relatively easier task, Pulse-PPG continues to have a healthy lead of x\% gain in F1 over PaPaGei, indicating the challenge of learning from clinical vs. raw ppg in the field.

% Stress detection in the field is hard due to wide between-subject variability and subjective self-reports. Unlike traditional evaluation techniques e.g. K-Fold CV or LOSOCV, train/val/test split introduces more uncertainty. We next linear probe Pulse-PPG and PaPaGei to evaluate their ability to detect stress vs. non-stress from self-reported stress using the initial train/val/test set of participants. We observe that Pulse-PPG with its no filtering, retrieval-reconstruction induced encoding outnumbers PaPaGei's physiological index retrieval from filtered ppg, marking a 26\% improvement in F1-score with numbers 0.57 vs. 0.45. Moreover, it also gains by 46.15\% over baseline majority class prediction. Notably, despite being a general-purpose foundation model, Pulse-PPG's performance on this challenging task is comparable to even specialized stress detection models in the field. This supports the model's employed approach to learn efficient and effective representations from raw unfiltered ppg which can potentially guide future research in stress detection in real-world settings. 

\section{Discussions}
% This work demonstrates the feasibility of a field-based PPG foundation model. Consistent performance of a model pre-trained on noisy real-world data suggests promising implications for future clinical research, development of foundation models, and mental health assessments.

% \subsection{Shifting the Paradigm: From Lab-Based to Field-Driven Foundation Models}
\subsection{Shifting the Paradigm: From Task Specific Models to Foundation Models}
Early mHealth research was limited by the lack of large-scale field datasets, prompting researchers to collect data in controlled lab environments. A common approach involved recruiting a group of participants, collecting physiological data in a lab setting, training a model on this clean data with clean labels, and then deploying the model on field data from the same participants~\cite{ertin2011autosense, plarre2011continuous}. This systematic approach minimizes variability between training and deployment and generally results in better performance as shown in recent works~\cite{toshnazarov2024sosw}. For greater between-person generalizability, researchers trained the model on one group of lab participants and tested on a separate group of lap participants~\cite{Hovsepian-2015-cStress}. For lab-to-field generalizability, the model was tested on a third group of participants who collected data in field settings~\cite{Hovsepian-2015-cStress}. A further step toward real-world applicability involved training AI models directly on growing volumes of field data and testing them on the same participants using 10-fold cross-validation or cross-subject validation~\cite{mishra2018investigating,zhang2024reproducible}. This approach posed significant challenges due to the inherent noise and variability in real-world data, making model development more complex. The majority of these works~\cite{mishra2018investigating,zhang2024reproducible} report F1 scores of around 0.5 for stress classification when using PPG only data. 

Further, a key limitation of task-specific models is their inflexibility—adapting to even a slightly different task (e.g., transitioning from binary to multi-class classification) often requires substantial retraining and modifications to the modeling pipeline. To overcome these challenges, a more recent paradigm has emerged: foundation models that are task-agnostic and general-purpose. These models leverage large-scale datasets to learn generalizable representations, enabling adaptation to diverse downstream tasks without requiring task-specific redesigns. Our results show that this approach has a high potential to improve performance on a wide variety of mHealth tasks in the field environment.

\subsection{Field-to-Lab Generalizability}
Cleaner data has been a major advantage of lab data. For detecting psychological states such as stress, another major advantage of lab data is clean labels. In the field conditions, the labels are usually self-reported. They do not temporally align with the physiological data due to a large time lag between how quickly physiology recovers versus the perception of stress. Further, unlike lab data where an entire block of data corresponds to a stress or non-stress task, blocks of data receiving the same label can consist of a mixture of stress and non-stress segments, due to lack of control over the experiences of a participant. These label impurities make it difficult to train task-specific models that use the labels together with the data.

Unlike traditional task-specific models~\cite{choi2022attention,hasanpoor2022stress,motaman2022stress,alshareef2022transformer,mitro2023ai}, self-supervised learning-based models are designed to reduce or eliminate the dependency on such explicit labels~\cite{spathis2022breaking,haresamudram2022assessing,rabbani2022contrastive}. This is achieved through their inherent structure, where the pre-training process derives labels directly from the data itself, without needing human-provided annotations. This self-supervised approach represents a significant departure from conventional methods, as it taps into the underlying patterns of the data rather than relying on predefined labels. This shift in model design has paved the way for an exploration of field-to-lab generalizability. Results from our pre-trained foundation model on field data suggest that this approach not only enhances performance on field-based mobile health tasks but also demonstrates unexpected success in clinical and laboratory settings, which are typically more controlled and structured.

To our knowledge, this is the first work that demonstrates the benefits of pre-training a foundation model on field data for improving performance not just in real-world, field-based mHealth tasks but also in more traditional clinical and lab environments. The success of the model across such diverse task domains—ranging from clinical and mHealth lab settings to field tasks involving PPG data—challenges the conventional belief that models must be trained on clean, highly curated datasets to perform well in clinical and lab settings. This finding opens up new avenues for leveraging field data to develop more robust, generalized models capable of performing across a wide range of environments.

\subsection{Optimizing Task Specific Performance}
Previous research primarily focused on task-specific performances. The foundation models, on the other hand, are general-purpose. But, an added benefit of foundation models is that they can also be optimized for enhancing the performance of specific tasks. When the focus is more on enhancing task-specific performance with pre-trained models, instead of using a linear probe approach used to test generalizability, which keeps the pre-trained model weights frozen and simply adds a linear layer on top for training, fine-tuning is the more appropriate approach that involves training the model end-to-end, using the pre-trained model weights as a starting point. This allows the model to adapt to the task-specific characteristics of the data more closely, which enables it to perform better for that task. 
% Another important aspect that can affect a model's performance specially when trained on field data is the inherent noise which can limit the performance of single-modality models. 
Additionally, future work can explore integrating additional data modalities, such as accelerometer data, to more effectively identify when the PPG is affected explicitly by motion artifacts. This could enable our model to learn a representation that can better isolate physical activities and mental stressors. Beyond motion data, contextual features—such as activity level, time of day, or environmental conditions—can enable more comprehensive and informative feature extractions~\cite{mishra2018investigating,toshnazarov2024sosw}. Finally, selectively incorporating demographic factors, while ensuring fairness and minimizing bias, may provide valuable insights into individual differences in physiological responses. Our work provides a new starting point, enabling the community to build more comprehensive multimodal representations for more accurate task-specific results.


% This shift toward foundation models represents a significant advancement in mHealth AI, offering greater scalability and flexibility for real-world health applications.  

% Early mHealth research was constrained by the lack of diverse field datasets, leading to a focus on task-specific models developed and tested in controlled lab environments. As real-world datasets became more available, researchers began applying these lab-trained models to field data. However, this transition often resulted in a significant drop in performance compared to lab-to-lab evaluations. This performance gap stems from fundamental differences in data collection: lab datasets are carefully controlled, minimizing noise and providing gold-standard labels, making them easier to model. In contrast, field data is subject to environmental noise, sensor inconsistencies, and high within- and between-person variability, making generalization far more challenging. While this transition is promising, more efforts are needed, as many physiological models still rely primarily on lab or clinical data. Another limitation of task-specific models is their lack of flexibility—modifications are often required when adapting them to new tasks, such as switching from binary to multi-class classification. To address these challenges, the research community has started building foundation models trained on more diverse and challenging datasets. Our work contributes to this shift, demonstrating that foundation models trained on field data can outperform those trained on controlled datasets for health applications across both controlled and real-world settings. Given our findings, researchers in the health domain must carefully weigh the trade-offs between investing resources in clinical versus field data collection to ensure robust and generalizable models.

% \subsection{Stress Detection in the Field}

% Unlike task specific models specially those that use clean labels collected in controlled lab settings for training, self-supervised learning based foundation models removed the reliance of clean labels due to the way they are designed to work. Since, pre-training stage tries to extract labels from within the data itself, and independent of any explicit labels, this allowed us to explore this new paradigm of field to lab. To the best of our knowledge, we are the first to show that for different kinds of clinical, mHealth lab and field tasks with PPG data, we see evidence that pre-training a foundation model on the field data not only does better on the field mHealth tasks, but also surprisingly does well on the clinical and lab tasks.

% Our foundation model, built exclusively from field PPG data, shows potential for task generalizability in controlled and field settings. Among the various tasks, stress detection, predominantly lab based, shows comparable performance. Models trained on ideal lab data with ground truth labels, were tested within the same controlled settings and participant pool. Such studies reported high accuracies~\cite{choi2022attention,hasanpoor2022stress,motaman2022stress,alshareef2022transformer,mitro2023ai}. However, external validation on self-reported stress ratings from a separate lab study of 15 participants marked a significant drop in accuracy, from 91\% to 76\%~\cite{mitro2023ai}, revealing the challenge of generalizability even across controlled conditions. Recognizing the challenge, researchers have increasingly explored stress detection in natural settings. One such recent work~\cite{toshnazarov2024sosw} initially trained a stress model using lab data from 26 participants. This pre-trained model was then leveraged to build and test a physiological model in the field, getting a macro F1 of 0.658. Despite additional challenges in the field, the study's relatively high numbers can be attributed to: (1) use of the same small pool of participants across lab and field, mitigating the impact of unseen participant variability, and (2) excluding nearly one-third participants due to no variation in self reports or significantly low compliance rates. Unlike studies focusing on small-scale, same-sample stress validation, other research efforts have prioritized building stress detection models using independent, large-scale field data. For example, Zhang et al.\cite{zhang2024reproducible} trained a generalized stress model using data collected from 77 participants over seven days, achieving a macro F1 score of 0.502. Similarly, Aqajari et al.\cite{aqajari2024enhancing} developed a model based on 420 days of field data from 34 participants, reporting an F1 score of 0.5. These results reveal the challenges of stress detection in natural environments, where scaling the study to include diverse participant pools or extended durations often leads to a noticeable decline in performance. This contrast emphasizes the difficulty of creating models that generalize well in real-world settings compared to those tested within smaller, consistent participant groups across controlled and field scenarios.  


% \subsection{Multimodal and Contextual Integration for Improved Robustness}
% \subsection{Multimodal Data for Improved Robustness}
% This work demonstrates the potential of a foundation model pre-trained solely on raw PPG data collected in real world field settings, achieving effectiveness across diverse downstream tasks. However, field data is inherently noisy and highly variable, which can limit the performance of single-modality models, especially for complex tasks such as mental state assessment. To mitigate these challenges, future work can explore integrating additional data modalities, such as accelerometer data, to better account for motion artifacts and improve the modeling of motion-affected PPG signals. By incorporating motion signals, models can more effectively disentangle physiological patterns from noise, enhancing their ability to generalize across varying conditions. Beyond motion data, contextual features—such as activity level, time of day, or environmental conditions—can further refine model predictions. Additionally, selectively incorporating demographic factors, while ensuring fairness and minimizing bias, may provide valuable insights into individual differences in physiological responses. A multimodal approach has the potential to create richer, more comprehensive representations, ultimately improving the robustness, accuracy, and adaptability of foundation models for real-world applications and research initiatives.

      

% \subsection{Expanding the Utility of Foundational Models for Broader Mental Health Applications}
% In this work, we demonstrated the utility of a foundational model for stress detection in real-world settings. However, beyond stress detection, such models could be extended to address other critical mental health issues, including depression monitoring and anxiety management. Unlike stress, which is often transient with short-term effects, conditions like depression develop gradually and persist over extended periods. These fundamental differences across mental states suggest that foundation models will require further refinement and task-specific adaptations to effectively capture the nuances of each condition. Given the growing need for just-in-time adaptive interventions, future research could focus on developing foundation models tailored to a broader spectrum of mental health conditions. One promising approach for improving generalization across tasks involves incorporating diverse, condition-specific data from individuals in natural settings into the pre-training phase. By enriching the model with real-world physiological data from individuals experiencing various mental health conditions, we can enhance its ability to detect, interpret, and respond to these challenges more effectively. This may both improve the accuracy of mental health monitoring and facilitate personalized interventions aligned for each condition, extending mobile health technologies to a wider range of users.


% \subsection{Detection of Harmful Stressors}
% Stress research, rooted in Hans Selye’s definition of stress as the non-specific response of the body to any demand~\cite{selye1976forty}, has traditionally centered on understanding stress as an agnostic response to its cause. Biomarkers like cortisol, a product of the hypothalamic-pituitary-adrenal (HPA) axis, have been extensively studied for their sensitivity to psychosocial stress~\cite{stawski2013associations}. Cortisol exhibits distinct patterns, including the cortisol awakening response (CAR) and the diurnal cortisol slope (DCS), both of which are critical indicators of HPA axis functioning~\cite{stone2001individual,adam2009assessing}. Dysregulation in these patterns, such as hypo- or hyperactivity, is linked to negative physiological outcomes~\cite{wirtz2008higher,matthews2006diurnal}, and stressors like low socio-economic status or burnout have been shown to alter CAR and DCS~\cite{steptoe2005changes,pruessner1999burnout}.

% While stress is unavoidable and can be adaptive, research increasingly highlights that health outcomes depend more on the type of stress experienced than its frequency or severity~\cite{stawski2013associations}. Stress from both positive and negative experiences collectively shapes overall health and well-being~\cite{stress2018daily}. However, certain stressors, referred to as \emph{harmful stressors}, such as interpersonal conflicts, home overloads, and network stressors, have stronger associations with negative health outcomes~\cite{almeida2002daily,stawski2013associations}. These findings underscore the importance of identifying and managing specific stressors that contribute to adverse effects on mental and physical health.

% Foundation models, with their ability to analyze complex and context-rich data, offer significant potential in detecting harmful stressors. By leveraging patterns in physiological signals, contextual information, and user input, these models can identify stressors that are more likely to impact well-being negatively. This capability could enable targeted interventions, reducing the burden of harmful stressors and fostering better health outcomes.


% Most of the current foundational models using physiological data are pre-trained on lab or clinical datasets, which are highly controlled and curated to ensure consistency. These datasets are typically cleaner, with minimal noise, making them more predictable and easier to work with. In contrast, field data, which is collected from real-world environments, is subject to a wide range of uncontrolled factors. 
% The existing models, trained primarily on clinical or lab data, might  struggle to generalize effectively when applied to these less controlled, more unpredictable field environments. This limits their real-world applicability, especially in areas like personalized health monitoring, stress management, or wearable technology, where the variability in user behavior and environmental conditions is significant.

% To improve the robustness and reliability of these models, future research should focus on extending and adapting foundational models to field data. This would involve enhancing the models’ ability to handle the noise and variability inherent in real-world conditions. Approaches could include developing techniques to account for environmental factors, sensor calibration issues, and the broader range of behaviors and activities typical in the field. Additionally, methods like domain adaptation or transfer learning could be employed to fine-tune models based on real-time, individual data rather than relying solely on pre-trained datasets. By expanding foundational models to better incorporate field data, we can improve their accuracy and effectiveness, ultimately leading to more personalized and reliable solutions in real-world applications.





\section{Limitations}

\subsection{Stress Downstream Task in Field Conditions}
One limitation of this study is that the stress episodes used for the field downstream task of stress vs.\ non-stress classification were obtained from a dataset~\cite{neupane2024momentarymoods} that used a commercial pre-trained model to obtain self-reported stress ratings. Other datasets have employed different approaches to obtain stress ratings~\cite{toshnazarov2024sosw,zhang2024reproducible}. Although the competing foundation model was also tested on the same dataset, testing the model performance on multiple large field datasets can demonstrate greater generalizability for stress detection in the field.

\subsection{Expanding Pre-training Data for Broader Generalization}
The wearable field dataset used to pre-train the foundation models in this study originated from a single field study involving a diverse and general population from throughout the U.S. \citep{neupane2024momentarymoods}. While this approach demonstrated the model’s utility, relying on data from one study inherently limits the diversity of field conditions, stress episodes, and physiological responses. To be more inclusive and more robust to field conditions, future work should incorporate data from multiple studies across varied populations, geographic regions, and contexts, to enhance generalizability. Expanding the diversity of training data can improve the model’s adaptability to new environments, mitigate biases from a single study, and ensure more robust and equitable performance across broader applications.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/ppg_snippet.png}
%     \caption{Caption}
%     \label{fig:enter-label}
% \end{figure}

\section{Conclusion}

Our work presents Pulse-PPG, the first open-source\footref{open} PPG foundation model pre-trained on field-collected data, underscoring the crucial role of real-world variability in enhancing model robustness and applicability. By outperforming state-of-the-art foundation models trained on wearable lab or clinical data across multiple downstream tasks, Pulse-PPG establishes the necessity of incorporating real-world variability into model training. Additionally, our findings indicate that field-trained models can perform competitively even in controlled lab datasets, challenging conventional assumptions about the superiority of lab-trained models.
The open-source\footref{open} release of Pulse-PPG will allow for further advancements in PPG-based health monitoring, enabling the research community to build upon a model that is inherently designed for real-world deployment. This work highlights the importance of shifting towards field-data-driven AI models to bridge the gap between controlled research settings and practical healthcare applications. Future research should explore ways to refine field-trained models, investigate hybrid training approaches, and expand the applicability of foundation models across diverse physiological signals.








%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{biblio}




%%
%% If your work has an appendix, this is the place to put it.
% \newpage
\appendix
\section{Appendix}
\subsection{Evaluation Dataset Details} \label{sec:evaldata}

\textbf{Cardiovascular health} is a key area of focus, with biomarkers such as systolic blood pressure, diastolic blood pressure, and heart rate serving as vital indicators. The PPG-BP dataset includes short PPG recordings from 219 participants to acquire information on basic physiology of individuals.
As part of the data collection process participants were asked to relax for about 10 minutes. After that, both blood pressure and finger PPG was collected within the next 3 minutes with 3 short PPG recording of 2.1 seconds each per participant. 
, most of whom have hypertension, along with corresponding blood pressure and heart rate measurements~\cite{liang2018new}. Using this dataset, we estimate these biomarkers and predict hypertension.

\textbf{Stress and affective states}, which significantly impact physical and mental health, are evaluated using the WESAD dataset~\cite{schmidt2018introducing}. The WESAD dataset collected physiological signals from 15 participants (12 males, 3 females) using RespiBAN Professional \citep{biosignalsplux} on the chest and an Empatica E4 \citep{mccarthy2016validation} on the wrist. The dataset includes ECG, Blood Volume Pulse (BVP), Electrodermal Activity (EDA), and Skin Temperature (TEMP) collected at 700 Hz, 64 Hz, 4 Hz, and 4 Hz, respectively. The study protocol had five sessions: Baseline, Stress, Amusement, Meditation and Rest. In the baseline session, participants engaging in neutral activities for 20 minutes. Stress was induced using the Trier Social Stress for about 10 minutes. During the session, participants had to give a public speech and complete a mental arithmetic task for 5 minutes each. For the amusement session, participants watched funny video clips for 6.5 minutes. To facilitate recovery, a meditation session followed both the stress and amusement sessions, along with an additional 10-minute rest period after the stress session. As part of the downstream evaluation tasks, we perform both binary and multi-class classification using only PPG data collected during the sessions. For binary classification, we combined baseline and amusement sessions to form non-stress class while stress class was the original stress session. For multi-class classification, we use Stress, Baseline, Amusement and Mediation sessions. For both types of classifications, each session was divided into non-overlapping 1-minute windows. Hence, stress vs. non-stress class had approximately 10 vs. 26 windows for the binary classification, while for the multi-class classification, window distribution was 20, 10, 6, and 7 for Baseline, Stress, Amusement and Mediation respectively.


\textbf{Physical activity}, which is a key indicator of one's healthy lifestyle, is assessed using the PPG-DaLiA dataset~\cite{reiss2019deep}. 
This publicly available multimodal dataset comprises physiological and motion data collected from wrist- and chest-worn wearable devices. It includes recordings from 15 participants (seven males and eight females) aged 21 to 55 years, performing eight distinct activities (sitting still, ascending/descending stairs, playing table soccer, cycling, driving, taking lunch breaks, walking, and working). Additionally, transition periods between activities  were labeled as a separate ‘zero’ activity. All activities were conducted in conditions closely resembling real-life scenarios, with each participant’s recorded session lasting approximately 2.5 hours.
Data were collected using the RespiBAN Professional and Empatica E4 wearable devices. The dataset includes PPG recordings and heart rate measurements during various activities. We utilize this dataset for two key tasks: activity classification (9 classes) and instantaneous heart rate estimation.

\textbf{Sleep-disordered breathing (SDB)} can lead to significant health complications in both adults and children. In children, SDB is associated with issues ranging from daytime sleepiness to severe conditions like developmental delays and growth failure. To examine its impact, the SDB dataset~\cite{garde2014development} collected physiological data from 146 children using a Phone Oximeter~\cite{karlen2011human}, which recorded PPG at 62.5 Hz and SpO$_2$ at 1 Hz during six hours of overnight sleep. Participants were categorized into SDB-positive and SDB-negative groups based on their Apnea-Hypopnea Index (AHI) scores, which quantify the severity of obstructive sleep apnea. Following the approach in~\cite{pillai2024papagei}, we utilize these AHI ratings to frame SDB prediction as a binary classification task.

% \textbf{Emotional states and mood disturbances} are analyzed using the ECSMP dataset~\cite{gao2021ecsmp}, which includes physiological and behavioral data from 89 participants. Emotion is closely linked to cognition (e.g., decision-making and memory) and sleep quality (e.g., poor sleep often leads to heightened negative emotions). Following the methodology in~\cite{pillai2024papagei}, we categorize participants into low and high mood disturbance groups before training a classification model on our pre-trained foundation model for mood prediction.

\textbf{Field Dataset: Stress Detection and Motion Detection}
In the field study, participants received 5.2 prompts on average per day for annotation from which they responded to 3.9, yielding a response ratio of 75\% using a Likert scale with response items (\emph{`Not stressed,' `Probably not stressed,' `Unsure,' `Probably Stressed,' `Stressed'}). If the annotation was either \emph{`Stressed,' `Probably Stressed,' or `Unsure'}, the app prompted them to provide the semantic location and the likely stressor. From a total of 128,006 physiological events detected by an AI model running on the smartwatch, 35,785 events were presented to participants for annotation, of which participants annotated 26,682 events. 
Following prior research~\cite{toshnazarov2024sosw, mishra2018investigating}, we use 1-minute windows for the downstream stress vs. non-stress detection task in the field. Additionally, to address label noise, we remove participants who exhibit no variation in their stress ratings or those with a low compliance rate in submitting stress reports. Specifically, we exclude three participants due to no variation in their ratings and 16 participants with low compliance compared to the rest. This filtering results in a revised train/validation/test split of 74/14/13 participants. For classification, we group \emph{`Stressed,' `Probably Stressed,'`Unsure'} into the Stress class, as participants provided stressor information for these ratings. Meanwhile, \emph{`Not stressed,' `Probably not stressed'} are categorized under Non-Stress. The final distribution of Stress vs. Non-Stress classes is approximately 58\% and 42\%, respectively, across a total of 206,088 one-minute labeled stress and non-stress windows.

For the other field task we chose stationary vs. motion as the downstream task. To do so, we use the first 10 days of each participant's data, assuming this duration provides enough representation of an individual's stationary and motion behavior. Since the field dataset lacks explicit activity labels, we employ a pre-trained activity recognition model~\cite{saleheen2021wristprint} to classify 20-second segments of accelerometry data into five activity categories: \emph{`Stationary,' `Walking', `Stairs', `Sports', and `Exercise'}. We then group \emph{`Walking,' `Stairs,' `Sports,' and `Exercise'} into the Non-stationary class. From these 10 days of data, we obtain a total of 3,559,429 stationary and non-stationary windows, with a class distribution of 15\% stationary and 85\% non-stationary. We extract raw PPG data corresponding to these labeled 20-second intervals for each participant. To maintain consistency with prior experiments, we use the same original split of 84/18/18 to partition the total 3,559,429 stationary and non-stationary windows for the downstream task.  

This benchmarking approach will demonstrate the versatility of our PPG foundation model, showcasing its ability to adapt to the domain shift of various datasets, and generalize well across diverse health-related tasks, notably after learning from noisy PPG data from a field dataset.

% \textbf{Maternal health} is examined by the NuMoM2B dataset~\cite{facco2015numom2b} for hypertensive disorder and small-for-gestational-age delivery, which gathered overnight PPG data of $3,163$ pregnant women during early and late stages of pregnancy. Our task is to predict early vs.late stage of pregnancy, and estimate gestational age. 


\subsection{Evaluation Methodology Details} \label{sec:evalmethods}
All PPG signals were re-sampled to match the 50 Hz sampling frequency present in our pre-training MOODS dataset \citep{neupane2024momentarymoods}. For our evaluations using PaPaGei, we made sure to resample the signals to 125 Hz in order to match the sampling frequency that they trained on \citep{pillai2024papagei}. 

In order to train and evaluate our linear probes, we combined the train and validation splits and used a cross validation hyperparameter grid search with the Scikit Learn package. For the regression tasks, we utilized a linear regression with an L2 regularization and conducted a grid search over the following parameters: \url{ 'alpha': [0.1, 1.0, 10.0, 100.0], 'solver': ['auto', 'cholesky', 'sparse_cg']} with the scoring function as macro F1. For the classification tasks, we utilized a logistic regression with an L2 regularization and lbfgs solver and conducted a grid search over the following parameters \url{'C': [0.01, 0.1, 1, 10, 100], 'solver': ['lbfgs'], 'max_iter': [1000, 10_000]} with the scoring function as negative MSE. Inputs into the probes were normalized with the Scikit Learn StandardScaler module. 

In order to train and evaluate our fine-tuning results, we fit a classification or regression head ontop of the existing network, then train the entire network end-to-end. For classification, the head was made up of a layer normalization layer, followed by a linear layer, with the loss function being a linear combination of Cross Entropy loss, a soft F1 loss \citep{benedict2021sigmoidf1}, and Dice loss \citep{li2019dice}. For regression, the head was an MLP layer, with an initial layer with an output dimensionality of 128, followed by the GeLU activation, followed by a final linear layer. The loss function used was the MSE loss function. For both evaluations, we utilize an Adam optimizer. The checkpoint with the highest macro F1 score and the lowest MAE on the validation split were selected for inference on the test set. 

For more specific implementation details, please check our public\footref{open} codebase.

\subsection{Additional Results}

The full results for comparing Pulse-PPG vs. PaPaGei and comparing Pulse-PPG pre-trained on Wearable Field vs. Clinical and comparing PaPaGei pre-trained on Wearable Field vs. Clinical can be found in Table \ref{tab:exp23} below.

\begin{table}[!htbp]
    \tiny 
    \caption{Comparing \textbf{Pulse-PPG vs. PaPaGei} and \textbf{Wearable Field PPG vs. Clinical Clean PPG} pre-training data.} \label{tab:exp23}
    \resizebox{.75\textwidth}{!}{%
    \begin{tabular}{>{\raggedright}p{.05cm}>{\centering}p{2cm}>{\raggedright}p{.8cm}>{\raggedleft}p{.1cm}cccc} 
    \toprule
     &  &  & & \multicolumn{2}{c}{\textbf{PPG RelCon}} & \multicolumn{2}{c}{\textbf{PaPaGei}} \\
    \midrule
    & \multicolumn{3}{r}{\textbf{Pre-Train Data:}} & Wearable Field & Clinical & Wearable Field & Clinical \\
    \midrule
    & \multicolumn{3}{r}{\textbf{Eval Method:} } & \multicolumn{4}{c}{Linear Probe} \\
    % \addlinespace[-1.5pt]
    \midrule
    % \addlinespace[-1pt]
    \multirow{21}{*}{\textbf{\rotatebox{90}{\centering Wearable Lab}}}
    & \multirow{3}{*}{\centering\parbox{2.2cm}{\centering \textbf{Instant HR (R)} \\ PPG-DaLiA}}
    & MAE  & \multirow{3}{*}{\rotatebox[origin=c]{-90}{\parbox{.3cm}{\rightarrowfill}}} & \textbf{8.936} & 18.60 & 11.37 & 19.18 \\
    % & & SDAE  & & \textbf{8.333} & 12.34 & 9.143 & 12.56 \\
    & & MSE  &  & \textbf{149.3} & 498.2 & 212.9 & 525.7 \\
    % & & SDSE  & & \textbf{282.4} & 624.7 & 331.1 & 624.2 \\
    & & MAPE  & & \textbf{0.1166} & 0.2581 & 0.1522 & 0.2708 \\
    \addlinespace[-2pt]
    \cmidrule(lr){2-8}
    \addlinespace[-2pt]
    & \multirow{6}{*}{\centering \parbox{2cm}{\centering \textbf{Activity (9)} \\ PPG-DaLiA}}
    & F1 Score  & \multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox{.5cm}{\rightarrowfill}}} & \textbf{0.3039} & 0.2250 & 0.2884 & 0.1861 \\
    & & Accuracy  &  & \textbf{0.4104} & 0.3276 & 0.3862 & 0.3020 \\
    & & Precision & & \textbf{0.3882} & 0.2631 & 0.3519 & 0.2142 \\
    & & Recall  & & \textbf{0.3107} & 0.2457 & 0.3016 & 0.2041 \\
    & & AUPRC  & & \textbf{0.3504} & 0.2527 & 0.3313 & 0.2149 \\
    & & AUROC  & & \textbf{0.8051} & 0.7741 & 0.7964 & 0.7438 \\
    \addlinespace[-2pt]
    \cmidrule(lr){2-8}
    \addlinespace[-2pt]
    & \multirow{6}{*}{\centering \parbox{2cm}{\centering \textbf{Stress (2)} \\ WESAD}}
    & F1 Score  & \multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox{.5cm}{\rightarrowfill}}} & \textbf{0.8759} & 0.8009 & 0.8237 & 0.6896 \\
    & & Accuracy  & & \textbf{0.8904} & 0.8493 & 0.8356 & 0.7260 \\
    & & Precision & & 0.8688 & \textbf{0.8795} & 0.8149 & 0.6863 \\
    & & Recall  & & \textbf{0.8848} & 0.7726 & 0.8565 & 0.6943 \\
    & & AUPRC  & & \textbf{0.9410} & 0.8724 & 0.7518 & 0.6465 \\
    & & AUROC  & & \textbf{0.9687} & 0.9374 & 0.9130 & 0.8043 \\
    \addlinespace[-2pt]
    \cmidrule(lr){2-8}
    \addlinespace[-2pt]
    & \multirow{6}{*}{\centering \parbox{2cm}{\centering \textbf{Stress (4)} \\ WESAD}}
    & F1 Score  & \multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox{.5cm}{\rightarrowfill}}} & \textbf{0.5431} & 0.5126 & 0.4976 & 0.4010 \\
    & & Accuracy  & & 0.6517 & \textbf{0.6629} & 0.5393 & 0.5169 \\
    & & Precision & & \textbf{0.6003} & 0.5124 & 0.4976 & 0.3947 \\
    & & Recall  & & \textbf{0.5716} & 0.5387 & 0.5258 & 0.4079 \\
    & & AUPRC  & & \textbf{0.5671} & 0.6931 & 0.4813 & 0.5202 \\
    & & AUROC  & & \textbf{0.7807} & 0.8456 & 0.7257 & 0.7684 \\
    % \addlinespace[-1pt] % Reduce space before midrule
    \midrule
    \multirow{12}{*}{\textbf{\rotatebox{90}{\centering Wearable Field}}}
    & \multirow{6}{*}{\centering \parbox{2cm}{\centering \textbf{Stress (2)} \\ MOODS}}
    & F1 Score  & \multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox{.5cm}{\rightarrowfill}}} & \textbf{0.5398} & 0.4550 & 0.4141 & 0.4398  \\
    & & Accuracy  & & 0.6156 & 0.6135 & 0.6251 & \textbf{0.6257} \\
    & & Precision &  & \textbf{0.5606} & 0.5244 & 0.5355 & 0.5538 \\
    & & Recall  & & \textbf{0.5460} & 0.5085 & 0.5042 & 0.5114 \\
    & & AUPRC  & & \textbf{0.4275} & 0.3960 & 0.3923 & 0.4030 \\
    & & AUROC  & & \textbf{0.5691} & 0.5382 & 0.5275 & 0.5246 \\    
    \addlinespace[-2pt]
    \cmidrule(lr){2-8}
    \addlinespace[-2pt]
    & \multirow{6}{*}{\centering\parbox{2cm}{\centering \textbf{Activity (2)} \\ MOODS}}
    & F1 Score  & \multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox{.5cm}{\rightarrowfill}}} & \textbf{0.6859} & 0.5641 & 0.5375 & 0.5318 \\
    & & Accuracy  & & \textbf{0.8705} & 0.8508 & 0.8483 & 0.8458 \\
    & & Precision &  & \textbf{0.7835} & 0.7401 & 0.7349 & 0.7058 \\
    & & Recall  & & \textbf{0.6509} & 0.5562 & 0.5401 & 0.5365 \\
    & & AUPRC  & & \textbf{0.5835} & 0.3916 & 0.3673 & 0.3410 \\
    & & AUROC  & & \textbf{0.8722} & 0.7205 & 0.7302 & 0.6941 \\
    \midrule
    % \addlinespace[-1pt] % Reduce space after midrule
    \multirow{21}{*}{\textbf{\rotatebox{90}{\centering Clinical}}} 
    & \multirow{6}{*}{\centering\parbox{2cm}{\centering \textbf{Sleep Disturbance (2)} \\ SDB}} 
    & F1 Score  & \multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox{.5cm}{\rightarrowfill}}} & 0.4630 & 0.5005 & 0.5012 & \textbf{0.5588} \\
    & & Accuracy  & & 0.5364 & 0.5898 & 0.5808 & \textbf{0.6609} \\
    & & Precision & & 0.4633 & 0.5108 & 0.5075 & \textbf{0.6079} \\
    & & Recall  & & 0.4673 & 0.5084 & 0.5062 & \textbf{0.5672} \\
    & & AUPRC  & & 0.3085 & 0.3636 & 0.3577 & \textbf{0.4236} \\
    & & AUROC  & & 0.4437 & 0.5122 & 0.5088 & \textbf{0.5528} \\
    \addlinespace[-2pt]
    \cmidrule(lr){2-8}
    \addlinespace[-2pt]
    & \multirow{3}{*}{\centering\parbox{2cm}{\centering \textbf{Systolic BP (R)} \\ PPG-BP}} 
    & MAE  & \multirow{3}{*}{\rotatebox[origin=c]{-90}{\parbox{.3cm}{\rightarrowfill}}} &\textbf{13.62}& 14.60 & 14.49 & 13.99 \\
    % & & SDAE & &\textbf{10.05} & 11.22 &  11.33&11.19 \\
    & & MSE  & & \textbf{286.5} & 339.0 &  338.2& 321.1\\
    % & & SDMSE  & &\textbf{388.7} & 474.0 &485.7  & 478.4 \\
    & & MAPE  & & \textbf{0.1065} & 0.1138 & 0.1131 & 0.1096 \\
    \addlinespace[-2pt]
    \cmidrule(lr){2-8}
    \addlinespace[-2pt]
    & \multirow{3}{*}{\centering\parbox{2cm}{\centering \textbf{Diastolic BP (R)} \\ PPG-BP}} 
    & MAE  & \multirow{3}{*}{\rotatebox[origin=c]{-90}{\parbox{.3cm}{\rightarrowfill}}} &\textbf{8.878}& 9.415 & 9.848 & 9.246 \\
    % & & SDAE &  & 6.275 & 6.934 & \textbf{6.193} & 6.442 \\
    & & MSE  & & \textbf{118.2} & 136.7 & 135.3 & 127.0 \\
    % & & SDMSE  & & \textbf{169.3} & 212.7 & 180.8 & 189.5 \\
    & & MAPE  & & \textbf{0.1232} & 0.1299 & 0.1374 & 0.1290 \\
    \addlinespace[-2pt]
    \cmidrule(lr){2-8}
    \addlinespace[-2pt]
    & \multirow{3}{*}{\centering\parbox{2cm}{\centering \textbf{Average HR (R)} \\ PPG-BP}} 
    & MAE  & \multirow{3}{*}{\rotatebox[origin=c]{-90}{\parbox{.3cm}{\rightarrowfill}}} &4.003& 4.258 & \textbf{3.971} & 4.549 \\
    % & & SDAE &  & 3.317 & 3.078 & \textbf{3.157} & 3.449 \\
    & & MSE  & & 27.02 & 27.60 & \textbf{25.74} & 32.59 \\
    % & & SDMSE  & & 51.68 & 38.77 & \textbf{40.37} & 45.46 \\
    & & MAPE  & & \textbf{0.0573} & 0.0607 & 0.0575 & 0.0662 \\
    \addlinespace[-2pt]
    \cmidrule(lr){2-8}
    \addlinespace[-2pt]
    & \multirow{6}{*}{\centering\parbox{2cm}{\centering \textbf{Hypertension (2)} \\ PPG-BP}} 
    & F1 Score  & \multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox{.5cm}{\rightarrowfill}}} & \textbf{0.6975} & 0.5939 & 0.5839 & 0.6403 \\
    & & Accuracy  & & \textbf{0.8130} & 0.7073 & 0.6829 & 0.7317 \\
    & & Precision & & \textbf{0.7009} & 0.5887 & 0.5824 & 0.6311 \\
    & & Recall  & & \textbf{0.6944} & 0.6130 & 0.6136 & 0.6755 \\
    & & AUPRC  & & \textbf{0.4266} & 0.3051 & 0.3413 & 0.3214 \\
    & & AUROC  & & \textbf{0.7971} & 0.6936 & 0.7100 & 0.7201 \\
    \bottomrule
    \end{tabular}%
    }
\end{table}


\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
