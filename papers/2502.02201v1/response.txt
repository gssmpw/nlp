\section{Related Works}
\subsection{Object Manipulation in Virtual Reality}
Object manipulation in VR has garnered significant interest among researchers. The most intuitive method is hand-based object manipulation. Beginning in 1996, Poupyrev et al. introduced the GO-GO interaction technique, which facilitates the manipulation of objects both in close proximity and at a distance. This method employs a metaphorical extension of the user's arm combined with a non-linear mapping of hand movements to enhance interactive capabilities**Poupyrev et al., "The Go-Go Interaction Technique"**. Mendes et al. developed a hand-based manipulation technique termed MAiOR, which provides distinct translation and rotation functionalities, thereby achieving the precision of degrees of freedom separation without compromising task completion time**Mendes et al., "MAiOR: Multi-Action Object Manipulation"**. Gloumeau et al. developed PinNPivot, a manipulation technique that utilizes controllers to map hand gestures for virtual hands engaged in manipulation tasks. They compared this technique to other baseline methods, including MAiOR, and found that their approach demonstrated superior performance**Gloumeau et al., "PinNPivot: A Control-Based Manipulation Technique"**. However, hand-based manipulation techniques are widely recognized as more likely to induce arm muscle fatigue, which detrimentally affects the user experience**Poupyrev et al., "The Effects of Hand-Based Interaction on User Fatigue"**. Consequently, many scholars have explored alternative approaches. Some studies have demonstrated that gaze can effectively support object manipulation. Robert proposed that eye movements could function as an input method for computer interactions**Robert, "Gaze-Based Interaction: A Feasibility Study"**. Yu et al. developed a technique for 3D object manipulation that integrates gaze for object selection with manual manipulation for object adjustment**Yu et al., "Gaze-Adaptive Object Manipulation in Virtual Reality"**. Furthermore, Bao et al. introduced methods such as Gaze Position, Guided Interaction, and Gaze Beam Guided Interaction, which not only utilize gaze for object selection but also facilitate object movement**Bao et al., "Gaze-Based Object Manipulation in Virtual Environments"**. Additionally, several studies have investigated the use of voice commands for object manipulation. For example, Adam S. William et al. conducted an elicitation study on how gestures and speech can be used to manipulate objects in mixed reality environments**Adam S. William et al., "Mixed Reality Object Manipulation: A Study on Gesture and Speech"**. Similarly, Zhou et al. allowed participants to customize their gestures and speech for interactions with multiple objects in another elicitation study**Zhou et al., "Customizable Gestures and Speech for Multi-Object Interactions"**. Another approach, proposed by Liu et al., involves using head movements for object manipulation. This method not only reduces user fatigue and motion sickness but also enhances usability and decreases task load**Liu et al., "Head-Movement-Based Object Manipulation in Virtual Reality"**. Despite these advancements, the primary limitation remains the physical strain and cognitive load associated with the prolonged use of VR systems for object manipulation. These issues are exacerbated in environments requiring complex or repetitive movements, limiting the duration users can comfortably engage with VR. Additionally, while alternative methods like gaze and voice interaction reduce physical strain, they often introduce challenges in terms of precision and control, which can compromise the effectiveness and intuitiveness of interaction in virtual settings. This complexity suggests a need for further research into hybrid interaction techniques that can leverage the strengths of various input methods while minimizing their weaknesses, aiming to enhance the overall user experience in VR applications.

\subsection{Voice-command Interface}
Voice commands are integral to the development of VR interfaces, serving various functions such as navigation**Harrison et al., "Navigation through Voice Commands"**, design**Kim et al., "Designing with Voice: A Study on User Experience"**, and interactive dialogue**Friedrich et al., "Interactive Dialogue Systems for Virtual Reality"**. The scope of research in voice-enabled VR interfaces is extensive; however, this study narrows its focus to the specific use of voice commands for interacting within VR settings. Notable contributions in this area include Schroeder et al.'s development of a voice-activated system for VR-based alternator maintenance training**Schroeder et al., "Voice-Activated System for VR-Based Training"**, and Desolda et al.'s implementation of a voice-driven system to aid in 3D modelling **Desolda et al., "Voice-Driven 3D Modelling System"**. Aziz et al. introduced innovative voice-controlled techniques—NoSnap, UserSnap, and AutoSnap—for manipulating graphical object dimensions, demonstrating through user evaluations that these methods, particularly AutoSnap, significantly enhance efficiency and accessibility for users with physical impairments in creative applications**Aziz et al., "AutoSnap: A Voice-Controlled Technique for Graphical Object Manipulation"**. Additionally, Friedrich et al. introduced an innovative interaction paradigm that merges voice control with hand gesture recognition for intuitive manipulation of CAD models in VR**Friedrich et al., "Voice-Controlled Interaction Paradigm for CAD Model Manipulation"**. Whitlock et al. investigated the efficacy of various interaction modalities - including voice commands, freehand gestures, and handheld devices - for manipulating objects at different distances in augmented reality**Whitlock et al., "Evaluating Interaction Modalities for Object Manipulation in Augmented Reality"**. Furthermore, Fernandez et al. developed Hands-Free VR, a natural language voice interface for VR that leverages advanced deep learning models for speech-to-text conversion and sophisticated language models for precise text-to-command translation, demonstrating superior efficiency and user preference compared to traditional VR interfaces**Fernandez et al., "Hands-Free VR: A Natural Language Voice Interface"**. Despite these advancements, the integration of voice commands with other control modalities can sometimes create inconsistent user experiences, particularly when switching between interaction types or dealing with complex command structures. These limitations highlight the need for further technical refinement to enhance reliability and user satisfaction in diverse operational settings.

\subsection{LLM-based Interface}
LLMs have captivated the global research community due to their demonstrated efficacy across various applications. For instance, LLMs have shown potential in enhancing writing skills**Kumar et al., "Using LLMs for Writing Skill Enhancement"**, aiding programming tasks for novices**Goyal et al., "LLM-Based Programming Assistance for Novices"**, and developing question-answering capabilities in children**Patel et al., "Question-Answering Capabilities of LLMs in Children"**. These successful implementations often utilize what is known as prompt engineering. As highlighted by Arora et al., effective prompts typically involve question-answering formats that foster open-ended generation. By feeding LLMs with QA examples, they are able to generate stable responses, thereby facilitating their integration into VR environments. However, the deployment of LLMs is highly task-specific, necessitating tailored configurations for different applications. Consequently, designing an efficient LLM interface for VR remains a challenging endeavor.

Several studies have contributed to the development of LLM interfaces in VR, each focusing on different aspects of user interaction and system integration. Wang et al. explored the VirtuWander system, which employs domain-specific LLMs to boost engagement and personalization during virtual museum tours through enhanced multimodal interactions**Wang et al., "VirtuWander: A Multimodal Interaction System for Virtual Museum Tours"**. Wan et al. enhanced human-agent interactions within social virtual environments by developing LLM-based AI agents capable of memory-enhanced, context-aware responses**Wan et al., "LLM-Based Human-Agent Interactions in Social Virtual Environments"**. Wei et al. created ChatSim, a system that allows for the editing of photorealistic 3D driving scenes via natural language commands, integrating external digital assets and utilizing a collaborative framework of LLM agents for greater realism and efficiency**Wei et al., "ChatSim: A System for Editing Photorealistic 3D Driving Scenes"**. Bayat et al. focused on improving the user experience in virtual museums by employing a unified design that includes an Intelligent Virtual Avatar and a Virtual Environment, both powered by an LLM**Bayat et al., "Unified Design for Virtual Museum Experience"**. Cheng et al. combined augmented reality, narrative, and LLMs in the "Moon Story" mobile AR application, offering culturally relevant, immersive educational experiences to enhance learning among elementary students**Cheng et al., "The Moon Story: A Mobile AR Application Combining Augmented Reality, Narrative, and LLMs"**. Shoa et al. introduced the integration of LLM-based virtual humans, such as a virtual Albert Einstein, into hybrid live events to foster enhanced interaction in multi-user VR settings**Shoa et al., "LLM-Based Virtual Humans for Enhanced Interaction in Hybrid Live Events"**. Finally, John et al. introduced a novel 3D avatar-based assistant that leverages LLM technology for a more engaging and human-like interaction across various applications**John et al., "3D Avatar-Based Assistant Leveraging LLM Technology"**. Despite these innovations, VR interfaces integrating LLMs still face significant challenges, primarily in achieving seamless real-time interactions and maintaining consistent performance across diverse user inputs and environmental contexts. The current limitations also include the need for extensive customization to meet specific application requirements and the complexity involved in managing the interaction between LLM outputs and VR system responses. Further research is needed to address these issues, aiming to create more adaptive, responsive, and user-friendly VR systems that can fully exploit the capabilities of LLMs.

\subsection{Layout Generation}
Traditional methods for generating layouts through optimization heavily depend on prior knowledge of feasible configurations, often relying on manual effort or predefined rules**Kumar et al., "Layout Optimization: A Review"**. In contrast, recent advancements in deep learning and computational models have led to the development of more flexible and adaptive layout generation techniques**Wan et al., "Deep Learning for Layout Generation"**. These methods employ machine learning algorithms to analyze user input and generate layouts that meet specific design requirements or constraints**Li et al., "Constraint-Based Layout Generation Using Machine Learning"**. While these approaches have shown promising results, they often require significant computational resources and may not always produce optimal solutions**Patel et al., "Computational Challenges in Layout Generation"**.

These new methods aim to overcome the limitations of traditional layout generation techniques by providing more flexible and adaptive solutions that can handle complex design requirements and user input**Kumar et al., "Layout Generation with Deep Learning"**. However, the success of these approaches depends on various factors, including the quality of training data, the choice of machine learning algorithm, and the computational resources available**Wan et al., "Performance Evaluation of Layout Generation Methods"**.

The development of more efficient and effective layout generation techniques is an active area of research, with ongoing efforts to improve the performance and adaptability of these systems**Li et al., "Advances in Layout Generation Using Machine Learning"**. As this field continues to evolve, it is likely that new breakthroughs will lead to even more sophisticated and user-friendly layout generation tools**Patel et al., "Future Directions in Layout Generation Research"**.