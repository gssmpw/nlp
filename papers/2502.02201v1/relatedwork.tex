\section{Related Works}
\subsection{Object Manipulation in Virtual Reality}
Object manipulation in VR has garnered significant interest among researchers. The most intuitive method is hand-based object manipulation. Beginning in 1996, Poupyrev et al. introduced the GO-GO interaction technique, which facilitates the manipulation of objects both in close proximity and at a distance. This method employs a metaphorical extension of the user's arm combined with a non-linear mapping of hand movements to enhance interactive capabilities~\cite{poupyrev1996go}. Mendes et al. developed a hand-based manipulation technique termed MAiOR, which provides distinct translation and rotation functionalities, thereby achieving the precision of degrees of freedom separation without compromising task completion time~\cite{mendes2017using}. Gloumeau et al. developed PinNPivot, a manipulation technique that utilizes controllers to map hand gestures for virtual hands engaged in manipulation tasks. They compared this technique to other baseline methods, including MAiOR, and found that their approach demonstrated superior performance~\cite{gloumeau2020pinnpivot}. However, hand-based manipulation techniques are widely recognized as more likely to induce arm muscle fatigue, which detrimentally affects the user experience~\cite{jang2017modeling,yu2021gaze}. Consequently, many scholars have explored alternative approaches. Some studies have demonstrated that gaze can effectively support object manipulation. Robert proposed that eye movements could function as an input method for computer interactions~\cite{jacob1990you}. Yu et al. developed a technique for 3D object manipulation that integrates gaze for object selection with manual manipulation for object adjustment~\cite{yu2021gaze}. Furthermore, Bao et al. introduced methods such as Gaze Position, Guided Interaction, and Gaze Beam Guided Interaction, which not only utilize gaze for object selection but also facilitate object movement~\cite{bao2023exploring}. Additionally, several studies have investigated the use of voice commands for object manipulation. For example, Adam S. William et al. conducted an elicitation study on how gestures and speech can be used to manipulate objects in mixed reality environments~\cite{williams2020understanding}. Similarly, Zhou et al. allowed participants to customize their gestures and speech for interactions with multiple objects in another elicitation study~\cite{zhou2022eliciting}. Another approach, proposed by Liu et al., involves using head movements for object manipulation. This method not only reduces user fatigue and motion sickness but also enhances usability and decreases task load~\cite{liu2024object}. Despite these advancements, the primary limitation remains the physical strain and cognitive load associated with the prolonged use of VR systems for object manipulation. These issues are exacerbated in environments requiring complex or repetitive movements, limiting the duration users can comfortably engage with VR. Additionally, while alternative methods like gaze and voice interaction reduce physical strain, they often introduce challenges in terms of precision and control, which can compromise the effectiveness and intuitiveness of interaction in virtual settings. This complexity suggests a need for further research into hybrid interaction techniques that can leverage the strengths of various input methods while minimizing their weaknesses, aiming to enhance the overall user experience in VR applications.


\subsection{Voice-command Interface}
Voice commands are integral to the development of VR interfaces, serving various functions such as navigation~\cite{hombeck2023tell}, design~\cite{morotti2020fostering}, and interactive dialogue~\cite{gobl2021conversational}. The scope of research in voice-enabled VR interfaces is extensive; however, this study narrows its focus to the specific use of voice commands for interacting within VR settings. Notable contributions in this area include Schroeder et al.'s development of a voice-activated system for VR-based alternator maintenance training~\cite{schroeder2017presence}, and Desolda et al.'s implementation of a voice-driven system to aid in 3D modelling ~\cite{desolda2023digital}. Aziz et al. introduced innovative voice-controlled techniques—NoSnap, UserSnap, and AutoSnap—for manipulating graphical object dimensions, demonstrating through user evaluations that these methods, particularly AutoSnap, significantly enhance efficiency and accessibility for users with physical impairments in creative applications~\cite{aziz2022voice}. Additionally, Friedrich et al. introduced an innovative interaction paradigm that merges voice control with hand gesture recognition for intuitive manipulation of CAD models in VR~\cite{friedrich2021combining}. Whitlock et al. investigated the efficacy of various interaction modalities - including voice commands, freehand gestures, and handheld devices - for manipulating objects at different distances in augmented reality~\cite{whitlock2018interacting}. Furthermore, Fernandez et al. developed Hands-Free VR, a natural language voice interface for VR that leverages advanced deep learning models for speech-to-text conversion and sophisticated language models for precise text-to-command translation, demonstrating superior efficiency and user preference compared to traditional VR interfaces~\cite{fernandez2024hands}. Despite these advancements, the integration of voice commands with other control modalities can sometimes create inconsistent user experiences, particularly when switching between interaction types or dealing with complex command structures. These limitations highlight the need for further technical refinement to enhance reliability and user satisfaction in diverse operational settings. 

\subsection{LLM-based Interface}
LLMs have captivated the global research community due to their demonstrated efficacy across various applications. For instance, LLMs have shown potential in enhancing writing skills~\cite{jakesch2023co}, aiding programming tasks for novices~\cite{kazemitabaar2023novices}, and developing question-answering capabilities in children~\cite{abdelghani2024}. These successful implementations often utilize what is known as prompt engineering. As highlighted by Arora et al., effective prompts typically involve question-answering formats that foster open-ended generation. By feeding LLMs with QA examples, they are able to generate stable responses, thereby facilitating their integration into VR environments. However, the deployment of LLMs is highly task-specific, necessitating tailored configurations for different applications. Consequently, designing an efficient LLM interface for VR remains a challenging endeavor.

Several studies have contributed to the development of LLM interfaces in VR, each focusing on different aspects of user interaction and system integration. Wang et al. explored the VirtuWander system, which employs domain-specific LLMs to boost engagement and personalization during virtual museum tours through enhanced multimodal interactions~\cite{wang2024virtuwander}. Wan et al. enhanced human-agent interactions within social virtual environments by developing LLM-based AI agents capable of memory-enhanced, context-aware responses~\cite{wan2024building}. Wei et al. created ChatSim, a system that allows for the editing of photorealistic 3D driving scenes via natural language commands, integrating external digital assets and utilizing a collaborative framework of LLM agents for greater realism and efficiency~\cite{wei2024editable}. Bayat et al. focused on improving the user experience in virtual museums by employing a unified design that includes an Intelligent Virtual Avatar and a Virtual Environment, both powered by an LLM~\cite{bayat2024exploring}. Cheng et al. combined augmented reality, narrative, and LLMs in the "Moon Story" mobile AR application, offering culturally relevant, immersive educational experiences to enhance learning among elementary students~\cite{cheng2024scientific}. Shoa et al. introduced the integration of LLM-based virtual humans, such as a virtual Albert Einstein, into hybrid live events to foster enhanced interaction in multi-user VR settings~\cite{shoa2023sushi}. Finally, John et al. introduced a novel 3D avatar-based assistant that leverages LLM technology for a more engaging and human-like interaction across various applications~\cite{john2024llm}. Despite these innovations, VR interfaces integrating LLMs still face significant challenges, primarily in achieving seamless real-time interactions and maintaining consistent performance across diverse user inputs and environmental contexts. The current limitations also include the need for extensive customization to meet specific application requirements and the complexity involved in managing the interaction between LLM outputs and VR system responses. Further research is needed to address these issues, aiming to create more adaptive, responsive, and user-friendly VR systems that can fully exploit the capabilities of LLMs. 


\subsection{Layout Generation}
Traditional methods for generating layouts through optimization heavily depend on prior knowledge of feasible configurations, often derived from procedural modelling or predefined scene graphs~\cite{ma2018language,qi2018human}. These approaches necessitate specialized expertise and exhibit limited adaptability in dynamic settings. Consequently, researchers have explored generative models as a potential solution to these limitations. For instance, Miguel et al. introduced GAUDI, a generative model that facilitates both unconditional and conditional generation of 3D scenes~\cite{bautista2022gaudi}. Handa et al. developed SceneNet, a framework designed to generate annotated 3D scenes, thereby enhancing indoor scene understanding~\cite{handa2016scenenet}. Additionally, Chen et al. proposed SceneDreamer, a novel generative neural hash grid that parameterizes the latent space based on 3D positions and scene semantics~\cite{chen2023scenedreamer}. In the realm of LLMs, new perspectives on text-based layout reasoning have emerged, circumventing the limitations associated with traditional data sets. Feng et al. introduced LayoutGPT, a method that composes in-context visual demonstrations in style sheet language to enhance the visual planning capabilities of LLMs, enabling them to consider layout plans with detailed specifications such as bounding boxes and the orientation of furniture items. Furthermore, Fu et al. developed AnyHome, which utilizes text-based inputs to generate realistic spatial layouts by directing the synthesis of geometry meshes within defined constraints~\cite{wen2023anyhome}. SceneTeller, another innovative approach, employs an LLM-based pipeline to generate high-quality scenes~\cite{ocal2024sceneteller}. These advancements highlight a shift towards more flexible, adaptive layout generation technologies that leverage the power of generative models and language processing.
Our work differs from these efforts in that our goal does not rely on a smart agent to generate a scene. Instead, we aim to develop and design an interface that assists users in VR object manipulation.