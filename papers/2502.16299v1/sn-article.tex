%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
% \documentclass[pdflatex, sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference 
\documentclass{article}
\usepackage{arxiv}
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage[numbers]{natbib}
%\usepackage{multirow}   % For multi-row cells
%\usepackage{siunitx} 
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

% %% as per the requirement new theorem styles can be included as shown below
% \theoremstyle{thmstyleone}%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
%\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

% \theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

% \theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\title{A calibration test for evaluating set-based epistemic uncertainty representations}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%
\author{Mira Jürgens \\
	Department of Data Analysis \\ and Mathematical Modeling\\
	Ghent University\\
	\texttt{mira.juergens@ugent.be} \\
	%% examples of more authors
	\And
	%\href{https://orcid.org/0000-0000-0000-0000}
    Thomas Mortier \\ 
	Department of Environment\\
	Ghent University\\
	\texttt{thomasf.mortier@ugent.be} \\
    \And
    Eyke Hüllermeier \\
    Department of Informatics\\
    Munich Center for Machine Learning \\
    LMU Munich \\
    \texttt{eyke@ifi.lmu.de} \\
	\And
    Viktor Bengs \\
    Department of Informatics\\
    Munich Center for Machine Learning \\
    LMU Munich \\
    \texttt{viktor.bengs@ifi.lmu.de} \\
    \And
    Willem Waegeman\\ 
	Department of Data Analysis \\ and Mathematical Modeling\\
	Ghent University\\
	\texttt{willem.waegeman@ugent.be} \\
}

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\given}{\, | \,}
\begin{document}
\maketitle
%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
% }
% \author*[1]{\fnm{Mira} \sur{Jürgens}}\email{mira.juergens@ugent.be}

% \author[1,2]{\fnm{Thomas} \sur{Mortier}} \email{thomas.mortier@ugent.be}
% %\equalcont{These authors contributed equally to this work.}
% \author[3]{\fnm{Eyke} \sur{Hüllermeier}}\email{eyke@ifi.lmu.de}
% \author[3]{\fnm{Viktor} \sur{Bengs}}\email{viktor.bengs@ifi.lmu.de}
% \author[1]{\fnm{Willem} \sur{Waegeman}} \email{willem.waegeman@ugent.be}
% %\equalcont{These authors contributed equally to this work.}

% \affil*[1]{\orgdiv{Department of Data Analysis and Mathematical Modeling}, \orgname{Ghent University}, \orgaddress{\street{Coupure Links 653}, \city{Ghent}, \postcode{9000}, \country{Belgium}}}

% \affil[2]{\orgdiv{Department of Environment}, \orgname{Ghent University}, \orgaddress{\street{Coupure Links 653}, \city{Ghent}, \postcode{9000}, \country{Belgium}}}

% \affil[3]{\orgdiv{Institute of Informatics}, \orgname{LMU Munich}, \orgaddress{\street{Akademiestr. 7}, \city{Munich}, \postcode{80799}, \country{Germany}}}
% % \affil[4]{\orgdiv{MCML}, \city{Munich}, \postcode{80799}, \country{Germany}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{Mira Juergens\inst{1}%\orcidID{0000-1111-2222-3333} 
% \and
% Thomas Mortier \inst{1}%\orcidID{1111-2222-3333-4444} 
% \and
% Eyke Hüllermeier
% \inst{2}%\orcidID{2222--3333-4444-5555}
% \and
% Viktor Bengs\inst{2}
% \and
% Willem Waegeman\inst{1}
% }
          % typeset the header of the contribution
%
\begin{abstract}
The accurate representation of epistemic uncertainty is a challenging yet essential task in machine learning. A widely used representation corresponds to convex sets of probabilistic predictors, also known as credal sets. One popular way of constructing these credal sets is via ensembling or specialized supervised learning methods, where the epistemic uncertainty can be quantified through measures such as the set size or the disagreement among  members. In principle, these sets should contain the true data-generating distribution. As a necessary condition for this validity, we adopt the strongest notion of calibration as a proxy. Concretely, we propose a novel statistical test to determine whether there is a convex combination of the set’s predictions that is calibrated in distribution. In contrast to previous methods, our framework allows the convex combination to be instance dependent, recognizing that different ensemble members may be better calibrated in different regions of the input space. Moreover, we learn this combination via proper scoring rules, which inherently optimize for calibration. Building on differentiable, kernel-based estimators of calibration errors, we introduce a nonparametric testing procedure and demonstrate the benefits of capturing instance-level variability on of synthetic and real-world experiments.
\end{abstract}
% 
% thereby enhancing flexibility and accommodating variations in proximity to the true conditional distribution across different regions of it. 
% We introduce a novel, non-parametric calibration test tailored to this framework, which employs a bootstrapping procedure to estimate the distribution of a calibration metric.
% Accurately capturing epistemic uncertainty is a crucial yet challenging task in machine learning. A popular approach is to form credal sets—the sets of probabilistic predictions from, for instance, an ensemble—and estimate the uncertainty via measures such as set size or predictive disagreement. In principle, these sets should contain the true data-generating distribution. As a necessary condition for this validity, we adopt the strongest notion of calibration as a proxy. Concretely, we propose a statistical test to determine whether there is a convex combination of the ensemble’s predictions that is calibrated in distribution. In contrast to previous methods, our framework allows the convex combination to be instance dependent, recognizing that different ensemble members may be better calibrated in different regions of the input space. Moreover, we learn this combination via proper scoring rules, which inherently optimize for calibration. Building on differentiable, kernel-based estimators of calibration errors, we introduce a nonparametric testing procedure and demonstrate the benefits of capturing instance-level variability through a series of synthetic and real-world experiments.
\keywords{uncertainty estimation, calibration, ensembles, credal sets, epistemic uncertainty}
%
%
%
%
\section{Introduction}\label{sec: introduction}
% TODO: notation for true coditiotnal distribution
In supervised machine learning, it has become more and more important to not only have accurate predictors,
but also a reliable quantification of predictive uncertainty, i.e., the learner's uncertainty in the outcome $y \in \mathcal{Y}$ given a query instance $\vec{x} \in \mathcal{X}$ for which a prediction is sought. 
%To this end, it is crucial to have an understanding and correct representation of the sources of uncertainty for a model's predictions. 
Predictive uncertainty is often 
divided into \textit{aleatoric} and \textit{epistemic} uncertainty \cite{SENGE201416,hullermeierAleatoricEpistemicUncertainty2019,kendallWhatUncertaintiesWe2017,gruber2023sources}, where the former corresponds to uncertainty that cannot be reduced by further information (e.g.\, more training data), as it originates from inherent randomness in the relationship between features $X$ and labels $Y$. 
Therefore, we assume that the \textit{ground truth} is a conditional probability distribution $\prob_{Y \given X}$ on $\mathcal{Y}$, i.e.\, given an input sample $x \in \mathcal{X}$, each outcome $y$ has a certain probability $\prob_{X|Y}(y|x) $ to occur. Even with perfect knowledge about the underlying data-generating process, the outcome cannot be predicted with certainty. However, in a typical machine learning scenario, the learner does not know $\prob_{Y|X}$. Having a space of possible hypotheses, an estimator of the underlying probability distribution $f$ within this space typically consists of a mapping $f: \mathcal{X} \rightarrow \mathbb{P}(\mathcal{Y})$, where $\mathcal{X}$ denotes the feature space and $\mathbb{P}(\mathcal{Y})$ the space of all probability distributions over the target space $\mathcal{Y}$. In essence, epistemic uncertainty refers to the uncertainty about the true $\prob_{Y \given X}$, or the ``gap''  between $\prob_{Y \given X}$ and $f$.
%While aleatoric uncertainty
%coincides with the inherent randomness within the data, which in principle cannot be reduced, epistemic uncertainty is related to the model's lack of knowledge, due to a lack of data.
% more on representations of uncertainty: 
% while aleatoric uncertainty is usally 
%More formally, having a space of possible hypotheses $\mathcal{H}$, a predictor $f \in \mathcal{H}$ typically consists of a mapping $f: \mathcal{X} \rightarrow \mathbb{P}(\mathcal{Y})$, where $\mathcal{X}$ denotes the feature space and $\mathbb{P}(\mathcal{Y})$ the space of all probability distributions over the target space $\mathcal{Y}$. Due to its probabilistic nature, this setup already captures the aleatoric uncertainty over the outcome of the target value $y \in \mathcal{Y}$, but representing epistemic uncertainty is less straight-forward. 

A popular approach to represent this gap is 
to estimate \textit{second-order probability distributions}, assigning a probability for each of the first order predicted probability distributions, i.e.\ the candidates for $f$. This is commonly done in Bayesian methods, such as Gaussian processes and Bayesian neural networks \cite{gelmanbda04}, but also in evidential deep learning methods \cite{ulmer2024priorposteriornetworks}, which have recently gained popularity. 
However, the first usually involves computationally costly methods to approximate the intractable posterior distribution, while the latter has been criticised for producing unfaithful or unreliable representations of epistemic uncertainty \cite{bengs2022neurips,bengs2023icml,meinert2023unreasonable,juergens2024is}.
Another way of representing epistemic uncertainty -- which will be the topic of this paper -- is through (convex) sets of probability distributions. Such sets are often referred to as \textit{credal sets} \cite{walley1991} in the imprecise probability literature, and they can be obtained in a direct manner, e.g.\ via credal classifiers \cite{hullermeier2022credalunc,javanmardi2024conformalized,wang2025creinns,caprio2024credal}, or in an indirect manner, via various types of ensemble methods, such as bootstrapped ensembles, deep ensembles \cite{NIPS2017_lakshminarayan} and randomization techniques based on Monte Carlo dropout \cite{gal2016dropout}. In credal sets, epistemic uncertainty is quantified via the size of the credal set  \cite{sale2023volume} or the diversity of the corresponding ensemble \cite{NIPS2017_lakshminarayan}. 


Due to a lack of an objective ground-truth, epistemic uncertainty representations are often evaluated in an indirect manner, using downstream tasks such as out-of-distribution detection \cite{ovadia2019nips}, robustness to adversarial attacks \cite{kopetzki2021evaluating}, and active learning \cite{nguyen2022measure}. However, recent studies have raised concerns about the usefulness of such tasks w.r.t.\ epistemic uncertainty evaluation \cite{abe2022deep,meinert2023unreasonable,bengs2022neurips}. A crucial open question is whether existing representations of epistemic uncertainty can be interpreted in a statistically profound way. For the specific case of credal sets, one might wonder whether such representations are statistically \textit{valid}, i.e., whether a credal set contains with high probability the true underlying conditional target distribution. Chao et al.\ \cite{chau2024credal} address this issue by proposing a two-sample test framework for credal sets. However, they assume having a sample of sufficient size from the same underlying conditional distribution. In the "classical" machine learning scenario that we look at, we do not have access to the ground truth conditional distribution nor to more than one realization $(x_i,y_i)$ from it, making this direct evaluation infeasible.


One necessary condition for validity is \textit{calibration} \cite{niculescu2005predicting}, which measures the consistency between predicted probabilities and actual frequencies.
Focusing on multi-class classification settings, we will utilize in this paper the notion of distribution calibration \cite{vaicenavicius19a} as a surrogate method for  validity. Failing calibration necessarily means failing validity, hence in this case one can be confident that the true data generating distribution is not contained inside the credal set. 
%produced e.g.\ by having an ensemble of predictors . The  disagreement of the latter, i.e.\ the variability of their predictions, can be used as an estimate of the underlying epistemic uncertainty. On an instance based level, each predictor yields a probability distribution over the target values, resulting in a set of probability distributions, also e. For a correct quantification of epistemic uncertainty, not only the \textit{size} of the set plays an important role, but also its \textit{validity}, i.e.\ whether it contains the true underlying conditional target distribution. 
Hence, by introducing a calibration test for set-based epistemic uncertainty representations, we aim for a more direct evaluation than measuring the performance on downstream tasks. 
%A classifier is thereby called \textit{calibrated} if its outputs coincide with probability distributions that match the empirical frequencies observed from realized outcomes. In multi-class classification one distinguishes between different types of calibration; while \textit{confidence calibration} \cite{niculescu2005predicting,guo2017,pmlr-v80-kumar18a} refers to the confidence score, i.e.\ the probability of the predicted class being calibrated, we will focus of the notion of \textit{distribution calibration} \cite{vaicenavicius19a}, which analyses whether all class probabilities are calibrated.



% TODO: section about calibration tests here (Hosmer-Lemeshov, 
  Our work builds further upon the work of Mortier et al.\ \cite{mortier2023calibration}, who recently analysed the concept of calibration as a proxy for epistemic uncertainty evaluation of credal sets. The test introduced in that study analysed whether, for a given convex set of probabilistic models, there exists a calibrated convex combination in the set. However, this existing test has important limitations, which motivate our current work. Specifically, the original test focuses on determining whether a \textit{single} convex combination of ensemble predictions can achieve calibration, but it does so in an instance-agnostic manner. This limitation means that the approach does not account for the variability of model calibration in dependence on the instance space. Moreover,
  the previous test tries to simulate the distribution of the calibration error estimators under the null hypothesis via sampling predictions in the credal set at random, which possibly leads to an overestimation of how calibrated the found convex combination is.
   In this paper, we address these shortcomings by introducing an instance-dependent approach to calibration testing, with two important modifications: First, due to the instance-dependency of the underlying convex combination of probabilistic predictions, we consider situations where predictions of the set are "differently well" calibrated for different regions in the instance space. Second, by using an optimization-based instead of a sampling-based algorithm, we achieve a more reliable estimate of the calibration statistic under the null hypothesis. Together the two modifications result in better control of the statistical Type I error, while simultaneously increasing the power of the test.
% TODO: mention these things in the main body
%Furthermore, we argue that only the strongest notion of calibration, namely distribution calibration, is able to truly capture whether the true underlying data distribution is contained in the set, and use common and recently introduced estimators \cite{widmann2019calibration,popordanoska2022consistent,marx2024calibration} to test for it. 

%By furthermore adapting the test to optimize over all possible convex combinations in the beginning without having to sample predictions within the polytope of predictors, we increase flexibility and allow for a more fine-grained representation of the epistemic uncertainty. We evaluate our test experimentally both on synthetic as well as real-world datasets.

The paper is organized as follows. In Section~2 we review the literature on calibration, and we formally discuss the calibration measures that are used further on. In Section~3 we introduce both the concept 
of instance-dependent validity as well as the (weaker) notion of calibration for credal sets.  We then propose
our novel, non-parametric calibration test that is able to assess whether an epistemic uncertainty representation via credal sets is calibrated. Furthermore, we introduce an optimization algorithm for finding the most calibrated convex combination. It includes training a neural network as a meta-learner, using proper scoring rules as loss functions. In Section 4 we empirically evaluate the statistical Type I and Type II error of our test in different scenarios, thereby showing its improvement over the test proposed by Mortier et al.\ \cite{mortier2023calibration} We further demonstrate its usefulness on validating common ensemble-based epistemic uncertainty representations of models trained on real-world datasets.

% TODO: write more here
% TODO: section about related work?
\section{Calibration of first-order predictors}
\label{sec: calibration setting }
This section reviews common calibration metrics that have been introduced for evaluating distribution calibration in multi-class classification. We also discuss differentiable estimators of these metrics, and statistical tests that assess calibration for a \textit{single} probabilistic model. The extension to \textit{sets} of probabilistic models will be discussed in Section~3. 

\subsection{General setting}
We assume a multi-class classification setting with feature space $\mathcal{X} \subseteq \mathbb{R}^d$ and label space $\mathcal{Y}=\{1, \dots, K\}$, consisting of $K$ classes. Let $X$ and $Y$ be random variables that are distributed according to a joint distribution $\mathbb{P}_{X,Y}$ on $\mathcal{X} \times \mathcal{Y}$. A probabilistic model can be represented as $f: \mathcal{X} \rightarrow \mathbb{P}(\mathcal{Y})$, which is a map from the feature space to the space of probability distributions over the output space $\mathcal{Y}$. For multi-class classification problems with $K$ classes, $\mathbb{P}(\mathcal{Y}) = \Delta_K$ (the $(K-1)$-dimensional probability simplex.
Roughly speaking, a classifier is \textit{calibrated} if its outputs coincide with probability distributions that match the empirical frequencies observed from realized outcomes. In multi-class classification, one distinguishes between different types of calibration. \textit{Confidence calibration} \cite{niculescu2005predicting,guo2017,pmlr-v80-kumar18a} only analyses the confidence score, i.e., the probability of the predicted class being calibrated. It is therefore the weakest notion of calibration. \textit{Distribution calibration} \cite{brockerReliabilitySufficiencyDecomposition2009,vaicenavicius19a}, which will be the focus in this paper, analyses whether all class probabilities are calibrated. Instead of only requiring calibrated marginal probabilities as in the definition of \textit{classwise calibration} \cite{zadrozny2002transforming}, it is defined via conditioning on the full probability vector; hence, it is the strongest notion of calibration.
%The following definition describes a general notion of calibration , which states that a model $g$ is calibrated if its predicted distribution expresses the true aleatoric uncertainty for a given input. It can be evaluated by conditioning the distribution of the target variable on any given prediction of the model, which should ideally be equal exactly that prediction. 

\begin{definition}
\label{def: strong calibration}
    %A predictor $f$ is \textit{calibrated in distribution}, or \textit{reliable}, if \begin{equation}
     %   \mathbb{P}(Y \in \cdot|f(X))= f(X) \quad a.s.
    %\end{equation}
A probabilistic multi-class classifier with output vector $f(X) \in \Delta_K$ is \textit{calibrated in distribution} if for all $k \in \mathcal{Y}=\{1, \dots, K\}$ it holds that
    $$\mathbb{P}(Y = k | f(X) = \vec{s}) = s_k,$$
with $s_k$ being the $k$-th component of the probability vector $\vec{s} \in \Delta_K$.
\end{definition}


To evaluate calibration, one typically makes use of calibration \textit{errors}. 
% and their respective data-dependent estimators.
In general, these errors measure the (average) discrepancy between the conditional probability distribution $\mathbb{P}_{Y|f(X)}$ and the predictions $f(X)$. Different calibration errors have been introduced to measure distribution calibration. One can in general differentiate between calibration errors defined as an \textit{expectation} over a divergence $d: \Delta_K \times \Delta_K \rightarrow [0, \infty)$ between $f(X)$ and $\mathbb{P}_{Y|f(X)}$ \cite{popordanoska2022consistent,pmlr-v238-popordanoska24a}: \begin{equation}
\label{eq: calibration as expectation}
         CE_d(f) = \mathbb{E}\Big[d\big(f(X), \mathbb{P}_{Y|f(X)}\big)\Big],
    \end{equation}
and calibration errors defined via \textit{integral probability metrics} \cite{muller1997integral}, also called \textit{kernel calibration errors} \cite{pmlr-v80-kumar18a,widmann2019calibration,marx2024calibration}:
    \begin{equation}
    \label{eq: calibration as divergence}
        CE_{\mathcal{H}}(f) = \sup_{\phi \in \mathcal{H}}\Big|\mathbb{E} \Big[\phi(f(X), Y)\Big] - \mathbb{E}\Big[\phi(f(X), Z)\Big]\Big|,
    \end{equation}
    where $\mathcal{H}$ is typically chosen to be the unit ball of a reproducing kernel Hilbert space (RKHS) $\mathcal{H}$, and $Z$ is a conditioning variable that is assumed to be calibrated.
    %^Z|f(X) \sim f(X)$. \\

% TODO: leave definition out or put in?
% \begin{definition}
%     Let  $d: \Delta_K \times \Delta_K \rightarrow [0, \infty)$ be a distance measure and $f: \mathcal{X} \rightarrow \Delta_K$ a probabilistic classifier. Then the calibration error with respect to $d$ is defined as 
%     \begin{equation}
%          CE_d(f) = \mathbb{E}\Big[d\big(f(X), \mathbb{P}_{Y|f(X)}\big)\Big].
%     \end{equation}
%     Given a dataset $\mathcal{D}= \{(x_i, y_i)\}_{i=1}^N$, we define the estimator of $CE_d$ as \newline $\widehat{CE}_d(f, \mathcal{D})=: \widehat{CE}_d(f)$.
% \end{definition}
% using some distance measure $d: \Delta_K \times \Delta_K \rightarrow [0, \infty)$:
%  \begin{equation}
%      \text{CE}_d(f) = \mathbb{E}\Big[d\big(f(X), \mathbb{P}_{Y|f(X)}\big)\Big].
%  \end{equation}
%  In the following, we will use the notation $\widehat{CE}_d(f):= \widehat{CE}_d(f, \mathcal{D})$ to denote an estimator of $CE_d$, dependent on a dataset $\mathcal{D}= \{(x_i, y_i)\}_{i=1}^N$.

 % TODO: cut back on explanations of estimators?

 \subsection{Overview of calibration errors and estimators} 
 \label{sec: calibration errors}
  We now introduce the calibration estimators that we later use for our test. For a given dataset $\mathcal{D}=\{(x_i, y_i)\}_{i=1}^N$, we denote $\widehat{CE}(f):= \widehat{CE}(f, \mathcal{D})$ as an estimator of $CE$ based on the dataset $\mathcal{D}$. 
% \begin{itemize}
 %    \item consistent, i.e.\ $\lim_{N \rightarrow \infty} \widehat{CE}(f)=CE(f)$.
  %   \item unbiased, i.e.\ $\mathbb{E}\Big[\widehat{CE}(f) \Big]= CE(f)$, and
   %  \item differentiable.
 %\end{itemize}
A very common way to estimate the calibration error is to use binning \cite{zadrozny2001,naeini2015,guo2017}, i.e., partitioning the domain of the predictor variable $f(X)$ into a finite number of discrete intervals or \textit{bins}, and estimating the calibration error by aggregating data within each bin.
% In notation, this would be equivalent to con TODO: binning as conditioning on interval, formulate it as expectation too
However, the piecewise nature of binning functions makes binned estimators non-differentiable and unsuitable for optimization tasks requiring gradient information, a property that we will need in Section~3. Binning also introduces a bias in estimating the conditional expectation  $\mathbb{E}[Y | f(X)]$,  because it replaces continuous variations with average values within bins. Furthermore, binned estimators usually measure a weaker form of calibration than distribution calibration as defined in Definition \ref{def: strong calibration}, namely classwise calibration, which only demands calibrated marginal probabilities. For these reasons, binning is not discussed in the following overview of existing estimators. Due to the nature of our problem, we make use of calibration estimators that are consistent, (asymptotically) unbiased and differentiable.  
 One of the first and still widely used metrics \cite{guptacalibration,murphy1973brierdecomposition,rahimi2020intra} that intrinsically also assesses calibration is the (expected) \textit{Brier score} \cite{brier1950}. It is defined as \begin{equation}
    \text{BS}(f) = \mathbb{E}\Big[\Big\|f(X)-\vec{e}_Y \Big\|_2^2 \Big],
\end{equation}
with $\vec{e}_Y$ being the one-hot-encoded vector of $Y$. Its estimator is given by the mean squared error between the predicted probability and actual outcome, averaged over all classes: \begin{equation}
    \widehat{BS}(f) = \sum_{i=1}^N \sum_{j=1}^K (f_{ij} - \mathbb{I}_{(y_i =j)})^2,
\end{equation}
where $f_{ij}$ denotes the $j$-th entry of the vector $f(\boldsymbol{x}_i)$. Being a proper scoring rule \cite{gneiting2007strictly}, it does not only measure calibration, but can be decomposed into a calibration and a refinement (loss) term \cite{murphy1973brierdecomposition,kull2015noveldecompositions}, where the calibration loss consists of the expected $L_2$ error between the conditional distribution $P_{X|f(X)}$ and $f(X)$, hence Equation (\ref{eq: calibration as expectation}) with $d$ being the Euclidean distance. In their framework of \textit{proper calibration errors}, Gruber et al.\ \cite{gruber2022betteruncertaintycalibration} show that the Brier score also serves as an upper bound for other common calibration errors.\\ 

\textbf{Popordanoska et al.} \cite{popordanoska2022consistent} proposed an estimator for the \textit{$L^p$ calibration error}, for which the case $p=2$ results directly from the Brier score decomposition: 
\begin{equation}
    CE_p(f) = \Big(\mathbb{E}\Big[\Big\| f(X) - \mathbb{P}_{Y|f(X)}\Big\|_p^p\Big]\Big)^{\frac{1}{p}}.
\end{equation}
They formulate an estimator of $\mathrm{C}\mathrm{E}_{p}(f)$ using Beta and Dirichlet kernel density 
estimates for the binary and multi-class classification case, respectively. Precisely, they prove that 
\begin{equation}
    \widehat{\mathrm{CE}}_p(f)
    =\Big(\frac{1}{n}\sum_{j=1}^{n}\left[\Big\| \widehat{{\mathbb{E}}[y|f(x_j)}] -f(x_{j})\Big\|_{p}^{p}\right]\Big)^{\frac{1}{p}}
\end{equation}
is a point-wise consistent and asymptotically unbiased estimator, where the estimator of the conditional expectation is defined via kernel density estimation.\\ 
%$k$ of a kernel density estimate:  \begin{eqnarray*}
 %   \mathbb{E}[\widehat{y|f(x)}] = \frac{\sum_{i=1}^n k(f(x); f(x_i)) y_i}{\sum_{i=1}^n k(f(x); f(x_i))}.
%\end{eqnarray*}

\textbf{Widmann et al.} \cite{widmann2019calibration} introduced the \textit{kernel calibration error} and multiple differentiable estimators for it. In its kernel-based formulation, it is defined using a matrix-valued kernel $k: \Delta_K \times \Delta_K \rightarrow \mathbb{R}^{K \times K}$ as follows:
 % \begin{equation}
 % \label{eq: kernel claibration error}
 %        \text{CE}_k(f) = \sup_{\phi \in \mathcal{H}} \mathbb{E} \big[(\mathbb{P}(Y \in \cdot \,| f(X)) - f(X))^T \phi(f(X)) \big],
 %    \end{equation}
 %    with $\mathcal{H}$ being the unit ball in the reproducing kernel Hilbert space corresponding to a kernel $k$.
 %   Equivalently, one can use a kernel-based formulation,
    \begin{eqnarray}
    \label{eq: kernel calibration error}
        \mathrm{CE}_k(f)=\left(\mathbb{E}\left[(\vec{e}_{Y}-f(X))^{\mathsf{T}}k(f(X),f(X^{\prime}))(\vec{e}_{Y^{\prime}}
        -f(X^{\prime}))\right]\right)^{1/2},
    \end{eqnarray}
where $(X', Y')$ is an independent copy of $(X,Y)$ and $\vec{e}_Y$, $\vec{e}_{Y'}$ are the one-hot-encoded vectors of the random variables $Y$ and $Y'$. We will make use of the computationally more feasible unbiased estimator they propose, defined as \begin{equation*}
\widehat{CE}_k(f) = \frac{1}{\lfloor n/2\rfloor} \sum_{i=1}^{\lfloor n/2 \rfloor} h_{2i-1,2i},
\end{equation*} with $k$ being a universal matrix-valued kernel
and $h_{i,j}$ corresponds to the term in the expectation (\ref{eq: kernel calibration error}) evaluated on two instance-label pairs.\\

% = (\vec{e}_{Y_i} - f(X_i))^Tk (f(X_i), f(X_j))(\vec{e}_{Y_j}-f(X_j)).$


\textbf{Marx et al.} \cite{marx2024calibration} introduced a framework of calibration estimators for different types of calibration, which considers it as a distribution matching problem between (true) conditional distribution $\mathbb{P}_{Y|X}$ and the predicted distribution $\hat{\mathbb{P}}_{Y|X}$ induced by $f(X)$. Similar to Widmann et al.\ \cite{widmann2019calibration}, they proposed  integral probability metrics to measure the distance between real conditional and predicted distribution. For $(X, Y) \sim \mathbb{P}$ and $(X, \hat{Y}) \sim \hat{\mathbb{P}}$ they define the calibration error as the Maximum-Mean-Discrepancy (MMD) between $\mathbb{P}$ and $\hat{\mathbb{P}}$:
\begin{equation}
    \text{CE}_{MMD}(f) = \sup_{\phi \in \mathcal{H}} \Big|\mathbb{E}\big[\phi(Y, f(X))\big]- \mathbb{E}\big[\phi(\hat{Y}, f(X))\big] \Big|,
\end{equation}
where $\mathcal{H}$ is again the unit ball of an RKHS.
 For the classification case, they introduce a trainable calibration estimate for distribution calibration which measures the squared error as
\begin{equation}
    \widehat{\text{CE}}_{MMD}^2 = \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1, j \neq i}^n h_{ij}, 
\end{equation} where the terms $h_{ij}$ are defined
 dependent on the type of calibration. They define $z_i$ being a conditional random variable distributed according to $f(x_i)$, for $i \in \{1, \dots, N \}$ and
 \begin{eqnarray*}
    h_{ij} = k((y_i, z_i), (y_j, z_j)) + \sum_{y \in \mathcal{Y}}\sum_{y' \in \mathcal{Y}} q_i(y)q_j(y')k((y, z_i), (y', z_j))
    - 2 \sum_{y\in \mathcal{Y}}q_i(y)k((y, z_i), (y_j, z_j)),
 \end{eqnarray*}
 with $k:  \Delta_K \times \Delta_K \rightarrow \mathbb{R}$ being a universal kernel function, and $q_i(y)$ is the predicted probability for $y$ given $f(x_i)$.\\

\textbf{Gruber et al.} \cite{gruber2022betteruncertaintycalibration} proposed a framework of \textit{proper calibration errors} that relates various calibration errors to proper scoring rules. They derive a taxonomy for common calibration errors, showing that even if there exists a calibration error that is zero for a model, it does not automatically mean that the model is calibrated. However, they show that with proper calibration errors, this, and in particular also the reverse holds. Precisely, a proper calibration error can be defined as the expectation as in (\ref{eq: calibration as expectation}) with $d$ being a Bregman divergence based on a strictly convex function $F: \mathbb{P}(\mathcal{Y}) \rightarrow \mathbb{R}$.
% \begin{equation*}
%     \text{CE}_{D_F}(f):= \mathbb{E}\Big[D_F(\mathbb{E}[Y|f(X)], f(X))\Big],
% % \end{equation*}
% where $D_F: \mathbb{P}(\mathcal{Y}) \times \mathbb{P}(\mathcal{Y}) \rightarrow \mathbb{R}_{\geq 0} $ is a Bregman divergence based on a strictly convex function $F: \mathbb{P}(\mathcal{Y}) \rightarrow \mathbb{R}$. 
In particular, this yields the squared $L_2$ calibration error $\text{CE}_2^2$ as a proper calibration error, and, extending this notion to \textit{proper U-scores} \cite{gruber2022betteruncertaintycalibration}, one can also show that the kernel calibration error $\text{CE}_k$ is proper (but not strictly proper). Similarly, using the Kullback-Leibler divergence $D_{KL}$ as the divergence measure, 
one can define  
%\begin{equation}
 %   \text{CE}_{KL}(f) = \mathbb{E}\Big[D_{KL}(\mathbb{E}[Y|f(X], f(X))\Big]
%\end{equation}
 another proper calibration error \cite{pmlr-v238-popordanoska24a}: \begin{equation}
     \text{CE}_{KL}(f):= \mathbb{E}\Big[D_{KL}(\mathbb{E}[Y|f(X)], f(X))\Big],
 \end{equation}
which exactly forms the calibration error term of the popular log loss. An estimator is given by 
\begin{equation}
    \widehat{\mathrm{CE}}_{KL}(f)
    =\frac{1}{n}\sum_{j=1}^{n}\left\langle \widehat{{\mathbb{E}}[y|f(x_j)}], \log \frac{\mathbb{E}[y|f(x_j)]}{f(x_j)}\right\rangle,
\end{equation}
where the estimator $\widehat{{\mathbb{E}}[y|f(x_j)}]$ is again defined via kernel density estimation.
% \begin{equation*}
%     \text{LL}(f) = \mathbb{E}\Big[\langle \log f(X), \mathbf{e}_Y \rangle \Big].
% \end{equation*}
In Appendix \ref{sec: estimator analysis}, we analyse the empirical distribution of the estimators, and empirically see that there is an approximately monotonic relationship between the value of the calibration estimator for a given prediction within the simplex and the distance to the data generating distribution. The squared kernel calibration estimator $\widehat{CE}_{k}$ forms an exception here, as it seems to be generally more noisy.

% SECTION ABOUT PROPER CALIBRATION ERRORS HERE (definition with expectation)

\subsection{Statistical tests for calibration}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/estimator_analysis/histogram_cal_estimates.pdf}
%     \caption{\textit{Distributions of calibration error estimators under the null hypothesis}. Histograms (with kernel density estimates) of the estimated calibration error $ g \in \{\widehat{CE}{L_2},\widehat{CE}_{\mathrm{MMD}^2},\widehat{CE}_{k}^2,\widehat{\mathrm{BS}} \}$ are shown. Predicted probabilities were sampled from a uniform Dirichlet distribution over three classes ($K = 3$) and repeated $N = 4000$ times to simulate a perfectly calibrated model. Labels were sampled from the corresponding categorical distribution in $D = 500$ resampling steps. Vertical dashed black lines indicate the mean values of each statistic. Vertical dashed red lines indicate the $95\%$ quantile of the empirical distribution. For $\widehat{\text{CE}}_{MMD}$ and $\widehat{\text{CE}}_{k}$, the squared statistic is visualised, as this will be used later on.}
%     \label{fig: histogram estimators h0}
% \end{figure}
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/estimator_analysis/histogram_cal_estimates_new.pdf}
%     \caption{\textit{Distributions of calibration error estimators under the null hypothesis}. Histograms (with kernel density estimates) of the estimated calibration error $ g \in \{\widehat{CE}{L_2},\widehat{CE}_{KL}, \widehat{CE}_{\mathrm{MMD}},\widehat{CE}_{k} \}$ are shown. Predicted probabilities were sampled from a uniform Dirichlet distribution over three classes ($K = 3$) and repeated $N = 1000$ times to simulate a perfectly calibrated model. Labels were sampled from the corresponding categorical distribution in $D = 500$ resampling steps. Vertical dashed black lines indicate the mean values of each statistic. Vertical dashed red lines indicate the $95\%$ quantile of the empirical distribution. }
%     \label{fig: histogram estimators h0}
% \end{figure}

As the proposed estimators $\widehat{CE}(g, \mathcal{D})$ are based on finite samples, comparing them in terms of the realized values does not take their randomness into account. In Appendix \ref{sec: estimator analysis} we explore this further and show their distribution under the null hypothesis. In dependence of the sample size $N$, the realized values lie closer or further away of the true calibration error, which is zero. 
In practice, one needs to perform a statistical test to evaluate the calibration of $f$ for its predictions on a finite dataset $\mathcal{D}= \{(f(x_i), y_i)\}_{i=1}^N$.
Different tests have been introduced in the past to evaluate model calibration. For a classifier $f: \mathcal{X} \rightarrow \Delta_K$, they commonly define the null and alternative hypothesis as \begin{eqnarray*}
    H_0: \, f \,\, \text{is calibrated} \quad\quad H_1: \, \neg H_0.
\end{eqnarray*} The Hosmer-Lemeshow test \cite{hosmer1997comparison}, initially developed as a goodness-of-fit test for the logistic regression model, can be used to test for confidence calibration and considers a chi-squared test statistic based on observed and expected frequencies of events. Vaicenavicius et al. \cite{vaicenavicius19a} developed a test based on \textit{consistency sampling} \cite{broeckersmith2007}, a method that uses bootstrapping the data to estimate the distribution of the calibration estimator's values under the null hypothesis that $f$ is calibrated. This is done by sampling new labels in each bootstrap iteration, based on the distribution that $f$ predicts for the respective bootstrap sample. Given the (empirical) distribution function of the calibration estimator under the null hypothesis, one can then check how likely the given calibration estimate is under the assumption that $f$ is calibrated.
Being a non-parametric test, the latter is more flexible and can therefore be used to test for distribution calibration in combination with any of the calibration estimators mentioned above. Our proposed test will build upon this work. Widmann et al.\ \cite{widmann2019calibration} also introduced a more specific calibration test for the kernel calibration estimator.
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/estimator_analysis/histogram_cal_estimates.png}
%     \caption{\textit{Distributions of calibration error statistics under the null hypothesis}. Histograms (with kernel density estimates) of the estimated calibration error $ g \in \{\widehat{CE}{L_2},\widehat{CE}_{\mathrm{MMD}},\widehat{CE}_{k},\widehat{\mathrm{BS}} \}$ are shown. Predicted probabilities were sampled from a uniform Dirichlet distribution over three classes ($K = 3$) and repeated $N = 4000$ times to simulate a perfectly calibrated model. Labels were sampled from the corresponding categorical distribution in $D = 500$ resampling steps. Vertical dashed lines indicate the mean values of each statistic. .}
%     \label{fig: histogram estimators h0}
% \end{figure}

\section{Calibration of sets of probabilistic predictors}
\label{sec: calibration for ensemble models}
We will now come to the main part of this paper, namely the evaluation of
calibration of not only one, but a set of classifier models. To this end, after introducing the necessary methodology, we introduce our proposed test in a step-wise manner.

\subsection{Credal sets} 
We use the same setting as described in the previous section,
 where we have given a dataset of $N$ realizations, $\mathcal{D}_N = \{(x_i, y_i)\}_{i=1}^N$, generated according to the joint distribution
 $\mathbb{P}$. 
 % Assuming a hypothesis space that contains all possible predictors, we assume that the optimal model lies in the hypothesis space. 
 % We again define a probabilistic predictor
 % as a mapping $f: \mathcal{X} \mapsto \Delta_K$ where denotes the $K-1$ dimensional probability simplex.
Credal sets of probability distributions can be created in various ways \cite{wang2025creinns,caprio2024credal}. One direct way of obtaining a credal set is via sets of probabilistic classifiers. In the following, let $\mathcal{F} := \{f^{(1)}, \dots, f^{(M)}\}$, 
with $f^{(i)} : \mathcal{X} \rightarrow \Delta_K$ the $i$-th probabilistic model in a set that contains $M$ models in total.
For each feature vector $x \in \mathcal{X}$, this yields a (credal) set of probability distributions
$\mathcal{F} |_x = \{f^{(1)}(x), \dots, f^{(M)}(x)\} \subseteq \Delta_K$.
A natural way to validate a representation of epistemic uncertainty through sets of predictors is to look at their possible \textit{convex combinations}: If there is at least one prediction in the convex hull of $\mathcal{F}|_x$ that is calibrated, then one can argue that the set of predictors contains the ground truth aleatoric part of the uncertainty, hence fulfils a necessary requirement for representing epistemic uncertainty. Calibration here serves as a relevant necessary condition for validity; a calibrated combination indicates that the predictors are not systematically biased and that they can approximate the true data-generating process in a consistent manner. For each instance $x\in \mathcal{X}$, we now define the (credal) set $\mathcal{S}(\mathcal{F},x)$ as the set of all possible convex combinations of predictors in $\mathcal{F}|_x$.
\begin{definition}
\label{def:credalset}
For a feature vector $x \in \mathcal{X}$, the credal set $\mathcal{S}(\mathcal{F}, x)$ is the set of all convex combinations of $\mathcal{F} |_x$:
\begin{equation*}
\label{eq: credal set}
\mathcal{S}(\mathcal{F}, x) = \Big \{f_{\boldsymbol{\lambda}}(x) \in \mathcal{H} \Big| \, f_{\boldsymbol{\lambda}}(x) = 
    \sum_{i=1}^M \lambda_i(x)f^{(i)}(x) \, \Big| \,(\lambda_1, \dots, \lambda_M) \in \Delta_{M, \mathcal{X}} \Big\}, 
\end{equation*}
where 
%\begin{equation}
 %   \label{eq: lambda space}
    $\Delta_{M, \mathcal{X}} = \Big\{\boldsymbol{\lambda} = (\lambda_1, \dots, \lambda_M) \Big|\, \lambda_i: \mathcal{X} \mapsto [0, 1] \, \text{and}\, \sum_{i=1}^M \lambda_i(x) = 1 \, \forall x \in \mathcal{X} \Big\}$
%\end{equation}
denotes the set of all functions with co-domain being the $(M-1)$-simplex.
\end{definition}
Here $\Delta_{M,\mathcal X}$ is a function space over $\mathcal X$, while in previous work \cite{mortier2023calibration}, $\Delta_{M}$ was the $(M-1)$-simplex of all (constant) convex combinations, that is, the same convex combination for all $x \in \mathcal{X}$ was analysed in order to test for calibration.
% In this definition we see that, unlike previous work \cite{mortier2023calibration}, the weights of the convex combinations in the credal set are defined as functions of the instance space $\mathcal{X}$, whereas 
Instance-dependent convex combinations offer the flexibility needed to capture varying degrees of aleatoric and epistemic uncertainty across the input domain. Different regions of the instance space may require different degrees of model blending to appropriately represent the level of uncertainty, particularly where some models are better calibrated than others. Def.~\ref{def:credalset} constructs credal sets via realizations of sets of predictors, which yield sets of probability distributions. From the perspective of imprecise probabilities, one might also define a \textit{credal} predictor, which yields as output a set of probability distributions. The proposed framework also holds for this scenario, given the predictor outputs a \textit{convex set} of extreme points.

% However, the interpretation and the handling of uncertainty for the two cases can differ significantly: While credal predictors usually provide a more explicit representation of uncertainty by outputting a range of possible values \cite{wang2025creinns,caprio2024credal}, sets of classifiers are often used to implicitly \textit{reduce} uncertainty by aggregating multiple predictions.

\subsection{Validity and calibration for sets of predictions}
\label{sec: validity for credal sets}
In the following, we formally define our notion of validity for set representations of epistemic uncertainty. It implies that for each $x \in \mathcal{X}$, the true underlying probability distribution $\prob_{Y|X=x}$ is contained in the (credal) set of all possible convex combinations $\mathcal{S}(\mathcal{F}, x)$. 

\begin{definition}[Validity of credal sets]
    Let $\mathcal{F}= \{f^{(1)}, \dots, f^{(M)}\}$ be a set of predictors,  with $f^{(i)}: \mathcal{X} \rightarrow \mathbb{P}(\mathcal{Y})$, and for each $x\in \mathcal{X}$, define the induced credal set $\mathcal{F}|_x$ as in Definition \ref{def:credalset}. Then $\mathcal{F}$ is \textit{valid} if
        $\prob_{Y|X=x} \in \mathcal{S}(\mathcal{F}, x)$    for all $x \in \mathcal{X}$.
\end{definition}

In practice, testing validity requires having either access to the ground truth conditional distribution $\mathbb{P}_{Y|X}$ or a sufficient number of samples thereof, as assumed in \cite{chau2024credal}.
Hence, in our setting, we make use of the \textit{weaker} notion of calibration which results from conditioning on the predictions $f(X)$ instead of $X$. This way, we analyse whether there is a $f_{\lambda}(x) \in \mathcal{S}(\mathcal{F}, x)$ such that $\mathbb{P}_{Y|f_{\lambda}(x)} \in \mathcal{S}(\mathcal{F}, x)$. Similar as in \cite{mortier2023calibration}, we now define a set of classifiers, or equivalently a credal classifier, as \textit{calibrated} if there \textit{exists} (at least) one calibrated convex combination of predictors. Definition \ref{def: calibration of credal sets} forms a generalization of Definition 4 of \cite{mortier2023calibration}, where the coefficients of the convex combination do not form functions on the instance space, but are \textit{constants}, i.e., $\boldsymbol{\lambda} \in \Delta_M$.

\begin{definition}
\label{def: calibration of credal sets}
    Let $\mathcal{F}=\{f^{(1)}, \dots, f^{(M)}\}$ be a set of predictors, $f^{(i)}: \mathcal{X} \rightarrow \Delta_K$. We say that $\mathcal{F}$ is \textit{calibrated in distribution} if there exists $\boldsymbol{\lambda} \in \Delta_{M, \mathcal{X}}$ such that the \textit{combined predictor} $f_{\boldsymbol{\lambda}} : \mathcal{X} \rightarrow \mathcal{S}(\mathcal{F}, \mathcal{X})$ defined as
    $$f_{\boldsymbol{\lambda}}(x) = \sum_{i=1}^M \lambda_i (x) f^{(i)}(x)$$
is calibrated in distribution (Definition \ref{def: strong calibration}).
\end{definition}

Note that calibration provides a necessary, but not \textit{sufficient} condition for validity: A valid predictor is always calibrated, yet the reverse does not have to hold. In fact, there are (possibly) many calibrated predictors, as shown in \cite{vaicenavicius19a}. In Appendix \ref{sec: many calibrated convex combinations}, we show that this is also the case for convex combinations of probabilistic models. However, if a predictor is \textit{not} calibrated, one knows that it is also not valid.
\subsection{A novel calibration test for sets of predictors}
Our null and alternative hypothesis for testing the calibration of a set of predictors $\mathcal{F}$ can now be formulated as 
\begin{eqnarray}
\label{eq: H0 and H1 credal sets}
    H_0: \quad \exists \boldsymbol{\lambda} \in \Delta_{M, \mathcal{X}} \, \text{s.t.} f_{\boldsymbol{\lambda}} \, \text{is calibrated}, \quad H_1: \neg H_0.
\end{eqnarray}
In order to analyse whether there is a calibrated convex combination, a natural approach is to analyse the one with the lowest calibration error. Hence, 
we define the \textit{minimum calibration error} as follows:
\begin{equation}
\label{eq: minimization lambda}
    \min_{\boldsymbol{\lambda} \in \Delta_{M, \mathcal{X}}}  g(f_{\boldsymbol{\lambda}}),
\end{equation}
where $g$ is a calibration error.\\

For the experiments, we choose $g$ to be equal to the errors proposed in Section~2, i.e., $g \in \{\text{CE}_p, \text{CE}_{KL}, \text{CE}_{MMD},\text{CE}_k\}$. These have under suitable conditions the desirable property that $f$ is calibrated if and only if $\text{CE} = 0$, making them suitable for optimization. Furthermore, the respective estimators described in Section \ref{sec: calibration errors} are both consistent and at least asymptotically unbiased. Having an optimisation dataset $\mathcal{D}_{opt} = \{(x_i, y_i)\}_{i=1}^{\tilde{N}}$, 
% one needs to minimize over all possible evaluations $\boldsymbol{\lambda}(x_i) \in \Delta_M$ for $i \in {1, \dots, N}$. To do this, 
one can represent the evaluations of the weight function $\boldsymbol{\lambda}: \mathcal{X}\rightarrow \Delta_{M, \mathcal{X}}$ by an $\tilde{N} \times M$ matrix:
\begin{equation}
   \Lambda =  \begin{pmatrix}
    \lambda_1(x_1) & \dots & \lambda_M(x_1) \\
    \vdots & \ddots & \vdots \\
    \lambda_1(x_{\tilde{N}}) & \dots & \lambda_M(x_{\tilde{N}}) \\

    \end{pmatrix}.
\end{equation}
Hence, for $\hat{g}$ being the (data-dependent) estimator of $g$, finding the \textit{minimum empirical calibration error} can be formulated as follows: 
\begin{equation}
\label{eq: minimum calibration error}
    \min_{\Lambda_{i, j}, \, i \in {1, \dots, M}, j \in {1, \dots, \tilde{N}}}\hat{g}(f_{\Lambda}, \mathcal{D}_{val})
\end{equation}
with $f_{\Lambda}(x_i)=\sum_{m=1}^M \lambda_m(x_i) f^{(m)}(x_i) \in [0,1]^K$, having the constraint $\sum_{j=1}^M \Lambda_{ij} =1$ for all  $j \in \{1, \dots, \tilde{N}\}$.\\


% \ref{fig: heatmaps calibration estimates} shows the values for estimators of $g$ for a simple example with $K=3$ classes, here for the case of having a ground truth distribution that is independent on the instance. While the true data generating distribution lies 
% in one of the corners of the probability simplex, one can see that there is an approximately monotonic relationship between the value of the calibration estimator for a given prediction within the simplex and the distance to the data generating distribution (represented by the red dot). The squared kernel calibration estimator $\widehat{CE}_{k}$ forms an exception here, as it seems to be generally more noisy.\\
% % in Figure~\ref{fig: heatmaps calibration estimates} mainly illustrates that it is natural to search for a minimum in Eq.~(\ref{eq: minimization lambda}). In the limit, our estimators consistently approximate functions based on proper scoring rules (proper \textit{$U$-scoring rules} for $\text{CE}_k$ \cite{gruber2022betteruncertaintycalibration}). Since we are focusing on differentiable estimators, the loss landscape is sufficiently "smooth".

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.85\linewidth]{figures/estimator_analysis/heatmap_cal_estimates_inferno.png}
%     \caption{Simple experiment that illustrates the behavior of the calibration estimators for the case of a \textit{non-instance-dependent} underlying ground truth distribution. Log-transformed values of the empirical calibration error $\hat{g}(\hat{f}, \mathcal{D})$ where $\hat{g} \in \{\widehat{\text{CE}}_{KL}, \widehat{\text{CE}_2}, \widehat{\text{CE}}_{k}^2, \widehat{\text{CE}}_{MMD}\},$ in dependence of the (constant) predictor $\hat{f}$ within the probability simplex, for the case of $K=3$ classes. $N=2000$ labels $y_i$ in $\mathcal{D}$ were generated from the ground truth categorical distribution, $y_i \sim \text{Cat}(f^*), \, i=1, \dots, 2000$. Here, we set $f^*(x) \equiv (0.1, 0.1, 0.8)^T$ constant for all $x \in \mathcal{X}$. The red point corresponds to $f^*$ and represents the theoretical minimum of the calibration error.}
%     \label{fig: heatmaps calibration estimates}
% \end{figure}

% It is well known that for a given data generating distribution, there exist various functions $f$ that are calibrated \cite{vaicenavicius19a}. In Appendix \ref{sec: many calibrated convex combinations} we show that this also holds for credal sets; multiple convex combinations that are calibrated may exist, implying the absence of a unique global minimum for the optimization problem (\ref{eq: combined calibration loss}). 
As already described in Section \ref{sec: validity for credal sets}, there are in general many calibrated functions $f$, implying the absence of a unique global minimum for the optimization problem (\ref{eq: combined calibration loss}). Therefore, in the optimisation, we make use of a combined loss function consisting of a proper scoring rule and the respective calibration estimator, weighted by a constant. Proper scoring rules intrinsically optimise for both a calibration error and an accuracy term and provide a stable optimisation. The specific optimisation method is described in Section \ref{sec: finding the most calibrated convex combination}.
% Nonetheless, since the aim is to find out whether there exists \textit{at least} one calibrated convex combination, not necessarily \textit{how many}, we make use of calibration errors that, under suitable conditions, fulfill the desirable property that \begin{equation*}
%     f \, \text{is calibrated} \quad \iff \quad \text{CE} = 0, 
% \end{equation*}
% the minimization approach is still justified.
% Other issues may arise when looking at non-trivial convex combinations: As shown in Appendix \ref{sec: non-trivial convex combinations}, for the case where all predictors are calibrated, this necessarily leads to them being uncalibrated. This special case, however, is rather unlikely in practice \cite{guo2017}.\\

\subsection{Algorithmic procedure}
\begin{algorithm}[t]
\caption{Algorithm to test whether there exists a calibrated, instance-dependent convex combination of an ensemble of $M$ classifier models.}
\label{alg: Alg v2.0}
\begin{algorithmic}[1]
    \State \textbf{Input:} validation set including features and labels $\mathcal{D}_{val}= \{(x_i, y_i)\}_{i=1}^N$, instance-wise evaluated point predictors  $\mathcal{F} |_{(x_i)_{i=1}^N} = \big(f^{(1)}(x_i), \dots, f^{(M)}(x_i)\big)_{i=1}^N$, estimator of calibration error $\hat{g}$, confidence level $\alpha$, number of bootstrapping iterations $D \in \mathbb{N}$
    \State \textbf{Output:} boolean stating whether to reject $H_0$, with $H_0$ as in (\ref{eq: H0 and H1 credal sets})
    \State $\Lambda^* \gets \displaystyle \argmin_{\Lambda_{i, j}, \, i \in \{1, \dots, M\}, j \in \{1, \dots, N\}}\hat{g}(\hat{p}_{\Lambda}, \mathcal{D}_{val})$
    \State $f_{\Lambda^*} \gets \Lambda^* \cdot \mathcal{F}$
    
    \For{$d = 1, \dots, D$}
        \State $\mathcal{D}_d \gets \{\tilde{x}_1, \dots, \tilde{x}_N\}$ bootstrap sample of instances
        \For{$n = 1, \dots, N$}
            \State sample $\tilde{y}_n \in \{1, \dots, K\} \sim \text{Cat}(f_{\Lambda^*}(x_n))$
        \EndFor
        \State $\tilde{\mathcal{D}}_{val} \gets \{(x_i, \tilde{y}_i)\}_{i=1}^N$ \Comment{replace labels of validation set}
        \State $t_{0, n} \gets \hat{g}(f_{\Lambda^*}, \tilde{\mathcal{D}}_{val})$
    \EndFor
    
    \State $q_{1-\alpha} \gets (1-\alpha)$-quantile of empirical CDF $\hat{F}$ of $(t_{0,1}, \dots, t_{0, N})$
    \State $t \gets \hat{g}(f_{\Lambda^*}, \mathcal{D}_{val})$
    \If{$t > q_{1-\alpha}$}
        \State reject $H_0$
    \Else
        \State do not reject $H_0$
    \EndIf
\end{algorithmic}
\end{algorithm}
We now explain in detail our new, adapted version of the statistical test proposed in Mortier at al.\ \cite{mortier2023calibration}, which in turn is based on the bootstrap test introduced by Vaicenavicius et al. \cite{vaicenavicius19a}. 
Algorithm \ref{alg: Alg v2.0} shows the pseudo code of the test, which consists of the following steps:
\begin{itemize}
    \item First, using an appropriate optimization algorithm, the per-instance convex combination $\Lambda^*$ with minimal calibration error is found (line 3).
    \item Using $\Lambda^*$, the predictions of the combined classifier $f_{\Lambda^*}$ are computed (line 4).
    \item Third, the calibration test of Vaicenavicius et al.\ \cite{vaicenavicius19a} is employed to assess whether the predictions of $f_{\Lambda^*}$ are calibrated (line 5-19). This statistical test is based on bootstrapping the data and \textit{consistency resampling}, that is, resampling the labels from the distribution induced by $f_{\Lambda^*}$. This way, one is able to estimate the distribution of the calibration error estimator $\hat{g}$ under the null hypothesis that $f_{\Lambda^*}$ is calibrated (see also Figure \ref{fig: histogram estimators h0} for an example).
\end{itemize}
 For a given significance level $\alpha$, the test rejects the null hypothesis if the value of the calibration error estimator is higher than the $(1-\alpha)$-quantile of the bootstrapped empirical distribution. In the other case, it cannot reject it.
Algorithm \ref{alg: Alg v2.0} differs from the proposed algorithm in Mortier at al.\ \cite{mortier2023calibration} in two important aspects:
\begin{enumerate}
    \item It allows for the case where the weights for the most calibrated convex combination \textit{depend on the instance space}, i.e., for different regions in it, it accounts for the fact that some predictors might be better calibrated than others and vice versa.
    \item It uses an \textit{optimization} instead of a sampling-based approach: For constructing the distribution under the null hypothesis, the previous test used uniform sampling of the weights. Directly optimizing over the calibration error and performing the test on the found convex combination has two main
    % disadvantages: First, it does not lead to uniform sampling in the credal set spanned by the predictors, as we show in Appendix \ref{sec: uniform sampling}. Advanced sampling techniques that could result in a more uniform sampling in the credal set, like rejection-sampling or triangulizations, are computationally costly. Secondly, for finding the most calibrated convex combination, which has two important 
    advantages: 
    (a), the uniform sampling of weights used in \cite{mortier2023calibration} does not lead to uniform sampling in the credal set, which we further elaborate on in Appendix \ref{sec: uniform sampling}. By putting emphasis on combinations in the inner of the polytope, it could lead to a biased estimate of the distribution under the null hypothesis. Advanced sampling techniques that could result in a more uniform sampling in the credal sets, like rejection-sampling or triangulizations, are computationally costly. (b), the minimization step automatically "guides" the algorithm towards the region with low calibration errors, avoiding the need to explore the whole credal set. 
\end{enumerate}
% In addition to the instance-dependency of the weight coefficients $\Lambda$, Algorithm~\ref{alg: Alg v2.0} also differs from Algorithm $1$ in \cite{mortier2023calibration} in the order in which certain building blocks are executed. Instead of sampling convex combinations in the credal sets spanned by the predictor models, we do the minimization step in line $1$ for finding the most calibrated convex combination, which has two important advantages. First, uniform sampling of weights does not lead to uniform sampling in the credal sets. In Appendix \ref{sec: uniform sampling} we show that by sampling the weights according to $\boldsymbol{\lambda} \sim Dir(1, \dots, 1)$, the resulting predictions tend to concentrate in the center of the probability simplex, due to the fact that these can be expressed with more weight combinations than the ones at the boundaries (because the transformation from $\Delta_{M, \mathcal{X}}$ to $\Delta_K$ is not injective). Advanced sampling techniques that could result in a more uniform sampling in the credal sets, like rejection-sampling or triangulizations, are computationally costly. As a second advantage of optimizing for the most calibrated convex combination in the first step of the algorithm, the minimization step automatically "guides" the algorithm towards the region with low calibration errors, avoiding the need to explore the whole credal set. 
 
%The resulting predictions $f_{\Lambda^*}$ are then used to perform the calibration test based on consistency resampling as suggested in the literature \cite{broeckersmith2007}, where we use bootstrapping and resampling of the targets to estimate the empirical distribution function of the calibration error estimator under the null hypothesis.

\subsection{A robust way of finding the most calibrated convex combination}
\label{sec: finding the most calibrated convex combination}
\begin{figure}[t]
    \centering
    \includegraphics[width=.9\textwidth]{figures/mlp_meta_mode_combinedl_2.pdf}
    \caption{General concept of learning the optimal function $\boldsymbol{\lambda}^*: \mathcal{X} \rightarrow \Delta_{M, \mathcal{X}}$, here for the example of $M=5$ ensemble models. The neural network is trained using the combined calibration loss function as in (\ref{eq: combined calibration loss}), using the calibration estimators introduced in Section \ref{sec: calibration errors} for the calibration term. It predicts for a given instance value $\boldsymbol{x}\in \mathbb{R}^D$ the optimal $\boldsymbol{\lambda}^*(x)$ such that the empirical calibration error for the combined predictor $f_{\boldsymbol{\lambda}^*}$ is minimized.}
    \label{fig: mlp optimization}
\end{figure}
We will now analyse further how optimization problem (\ref{eq: minimum calibration error}) can be solved in an efficient and robust manner. Specifically, we try to avoid the problem of \textit{overfitting} on the empirical calibration error on the respective dataset: Since the latter is finite, there might be a mismatch between the estimated and the population-level calibration error. Classical gradient-based solvers, which purely optimise the calibration error on the same dataset that is used for the test, might therefore run into the problem of underestimating the true calibration error, thereby making the test more conservative. This can also be seen in the empirical results of Mortier et al.\ \cite{mortier2023calibration}. 
% TODO: We need a valid justification for the data-dependent minimization here! 
% Possibilities: Inequalities, e.g. P(\hat{g} > eps | H_0) < alpha
% Arguing with proper scoring rules for Brier Score Lp calibration error, MMD (?)
 % Furthermore, (\ref{eq: minimum calibration error}) has a lot of variables, which could result in overfitting to the data-dependent estimators, because the learned functions $\boldsymbol{\lambda} = (\lambda_1, \dots, \lambda_M): \mathcal{X} \rightarrow \Delta_{M, \mathcal{X}}$ do not have to be smooth.
 Therefore, we suggest an alternative approach to optimize (\ref{eq: minimum calibration error}), which incorporates two important aspects:
 \begin{itemize}
     \item We use a neural network for learning the weight function $\boldsymbol{\lambda}: \mathcal{X} \rightarrow \Delta_{M, \mathcal{X}}$, exemplary visualized in Figure \ref{fig: mlp optimization}. This approach is similar to stacking \cite{wolpert1992stacked}, where a second meta-learner is trained to find the optimal combination of predictions. It is trained on a separate optimization dataset, and then used to predict the optimal convex combination for our test.
     \item As a loss function, we use a proper scoring rule $\mathcal{L}\in \{\widehat{\text{BS}}, \widehat{\text{LL}}\}$, combined with a term controlling the calibration error: \begin{equation}
  \label{eq: combined calibration loss}
    \boldsymbol{\lambda}^* = \argmin_{\boldsymbol{\lambda}\in \Delta_{M, \mathcal{X}}} \mathcal{L}(f_{\lambda}, \mathcal{D}_{val}) + \gamma \cdot \hat{g}(f_{\lambda}, \mathcal{D}_{val}), 
\end{equation}
where $\hat{g}(f_{\lambda}, y) \in \{\widehat{CE}_{KL}, \widehat{\text{CE}}_2, \widehat{\text{CE}}_k, \widehat{\text{CE}}_{MMD}\}$ is the respective calibration error estimator of interest, and $\gamma \geq 0$ denotes a weight factor. This approach takes recent insights \cite{popordanoska2022consistent,marx2024calibration} that adding a calibrating penalty to a proper scoring rule can help ensuring calibration, into account, while avoiding trivial or degenerate solutions. In particular, many convex combinations can be calibrated (Appendix \ref{sec: many calibrated convex combinations}), hence, the proper scoring rule serves as a \textit{regularizer}, guiding the optimizer toward solutions that not only reduce calibration error but also maintain high predictive accuracy. Furthermore, we learn $\boldsymbol{\lambda}^*$ on a validation set but evaluate calibration on a separate test set. This split reduces the risk of overfitting to our chosen calibration metric, ensuring the resulting test does not become overly conservative. 
 \end{itemize}
 
% Due to the immense popularity of deep learning models, it also allows that state-of-the-art gradient-based optimization algorithms for non-convex optimization can be utilized.    

\section{Experimental results}
\label{sec: experimental results}
%that consists of $3$ hidden layers, LeakyReLU
We evaluate our test both on synthetic and real-world data. A more detailed description of the experimental setup can also be found in Appendix \ref{sec: experimental setup} and the code for reproducing the experiments is available on GitHub.\footnote{\url{https://github.com/mkjuergens/EnsembleCalibration}}
% We thereby investigate different minimization strategies in order to do the minimization step in line 1 of Algorithm \ref{alg: Alg v2.0}. This includes using standard numerical solvers like the gradient-free COBYLA \cite{powell1994direct} and the gradient-based SLSQP \cite{kraft1988software} method, as well as the proposed neural network approach. 
For the experiments on synthetic data, a simple MLP architecture with  $3$ hidden layers is used. As a loss function, we use the combined loss as in (\ref{eq: combined calibration loss}), with Brier score or log loss as a proper scoring rule.

\subsection{Binary classification}
\label{sec: binary classification}
\begin{figure}[t]
    \centering
  

    \includegraphics[width=.8\textwidth]{figures/gp_experiment/gp_setting.png}
    % \caption{}
    % \end{subfigure}
    % \begin{subfigure}{.49\textwidth}
    %     \centering
    %     \includegraphics[width=.9\textwidth]{figures/gp_experiment/gp_weights.png}
    %     \caption{}
    % \end{subfigure}
\caption{$1$-simplex spanned by the ensemble members $\{f^{(1)}, f^{(2)}\}$ (grey) that are generated according to a GP, and the calibrated predictions for test cases $H_{0,1}$ - $H_{1,3}$ described in Section \ref{sec: binary classification} (left). In the right graphic, the respective weights for $f^{(1)}$ and the cases $H_{0,1}, H_{0,2}$ are visualised as a function on the instance space.
    % for the binary classification case, with
    %  $M=2$ predictors $\{f^{(1)}, f^{(2)}\}$. The calibrated classifier's predictions form $a)$
    %    a non-instance convex combination of the two ensemble classifiers ($f_{\lambda,const}$), $b)$ an
    %  instance-dependent convex combination ($f_{\lambda, x}$), $c)$ no convex combination by lying outside of the convex hull, but very close to the boundaries of the convex hull ($f_{h_1, s_1}$), $d)$ no convex combination 
    %  with more distance to boundaries($f_{h_1, s_2}$), $e)$ no convex combination with the most distance 
    %  to the boundaries ($f_{h_1, s_3}$).
     }
    \label{fig: gp calibration cases}
\end{figure}
For illustration purposes, we first examine the case of binary classification by simulating $M=2$ probabilistic predictors, each outputting a probability for the positive class. For each input $x$, the predictor's probabilities $f^{(1)}(x)$ and $f^{(2)}(x)$ are sampled from a Gaussian process (GP) constrained to $[0,1]$. We then define a \textit{true} calibrated predictor $f^*$ that lies inside (under $H_0$) or outside (under $H_1$) the convex hull of the two predictors.
Figure \ref{fig: gp calibration cases} shows the exemplary setting for this experiment. Under $H_0$, we consider two cases:
\begin{enumerate}
    \item \textbf{Non-instance dependency} ($\boldsymbol{H_{0,1}}$): The calibrated predictor $f^*$ is a constant convex combination of $f^{(1)}$ and $f^{(2)}$.
    % simulated by $\lambda_1^*(x) =c \in [0,1]$ where $c \sim Unif([0,1])$, $\lambda_2^*(x) = 1 - \lambda_1^*(x)$.
    \item \textbf{Instance dependency} ($\boldsymbol{H_{0,2}}$): $\lambda^*(x)$ is a randomly generated polynomial function and $f^*(x)= \lambda^*(x) f^{(1)}(x) +(1-\lambda^*(x))f^{(2)}(x)$.
\end{enumerate}
Under $H_1$, $f^*(x)$ lies strictly outside or near the boundary of the credal set. We generate $3$ scenarios of increasing distance from it: 
\begin{enumerate}
    \item $\boldsymbol{H_{1,1}}$: $f^*(x)$ is set at a small $\epsilon$-distance to one boundary.
    
    % We randomly select one of the boundaries of the polytope spanned by the two predictors, and set the calibrated predictor $f^*$ at an $\epsilon$-distance to the boundary, with $\epsilon \sim Unif([0, 0.05])$. 
    \item $\boldsymbol{H_{1,2}}$: is sampled from a GP that remains close but outside the credal set.
    \item $\boldsymbol{H_{1,3}}$: similar to $\boldsymbol{H_{1,2}}$, but allowing a larger maximum distance.
    % TODO: this has to be improved! (also distance based approaches for setting 2 and 3?)
\end{enumerate}
\begin{figure}[t]
    \centering
    \begin{subfigure}{.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gp_experiment/t1_t2_errors/fig_t1_gp.pdf}
     \caption{average Type 1 error}
    \end{subfigure}
    \begin{subfigure}{.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/gp_experiment/t1_t2_errors/fig_t2_gp.pdf}
        \caption{average Type 2 error}
    \end{subfigure}
    \caption{Average Type $1$ and Type $2$ error given a significance level ($x$-axis) for Algorithm \ref{alg: Alg v2.0} with $D=100$ bootstrap iterations run on the binary classification experiment and the cases $\boldsymbol{H}_{0,1} - \boldsymbol{H}_{1,3}$ where the null hypothesis is true (a), and false (b), respectively. The average is taken over $100$ resampling iterations. In each iteration we newly generate a dataset of size $N_{test}=400$ on which the tests are performed. For the proposed test, we do the optimisation on a separate dataset $N_{opt}$ of the same size.}
    \label{fig: type12 error gp experiment}
\end{figure}
Figure \ref{fig: type12 error gp experiment} shows the average Type $1$ and Type $2$ error for $100$ different runs of the experiment, in comparison with the results  given by the previously proposed algorithm \cite{mortier2023calibration}. The average Type $1$ error of our newly proposed test lies much closer to the chosen significance level, while the baseline test is in general too conservative. In some cases, the average Type $1$ error of our test lies above the significance level. This is due to the generalization error on the unseen data: Since we learn the optimally calibrated convex combination on a separate dataset, the learned convex combination on the test set might be not the one with lowest calibration error. This depends on the test set sample size, but underestimating the calibration error can lead to more false rejections.
Both the previous test as well as our new test show a high power for this setting, with zero Type $2$ error also in the case where the true distribution lies in very close distance to the polytope, except for the kernel calibration error, which shows a slightly higher Type $2$ error.

% The Type $2$ error shown in Table \ref{tab:type2 error gp} is zero for almost all cases for both tests, implying that it is very easy to reject the null hypothesis for both tests in this setup.
% \begin{table}[htbp]
% \centering
% \begin{tabular}{@{\hspace{0.5cm}}l@{\hspace{0.5cm}}@{\vline}@{\hspace{0.5cm}}
%                 c@{\hspace{0.5cm}}c@{\hspace{0.5cm}}c@{\hspace{0.5cm}}@{\vline}@{\hspace{0.5cm}}
%                 c@{\hspace{0.5cm}}c@{\hspace{0.5cm}}c@{\hspace{0.5cm}}}
% \toprule
%  & \multicolumn{3}{c}{previous test} & \multicolumn{3}{c}{proposed test} \\
% % \cmidrule(lr){2-4} \cmidrule(lr){5-7}
% cal. estimator & \textbf{$H_{1,1}$} & \textbf{$H_{1,2}$} & \textbf{$H_{1,3}$} &  \textbf{$H_{1,1}$} & \textbf{$H_{1,2}$} & \textbf{$H_{1,3}$} \\
% \midrule
% $\widehat{CE}_{MMD}$  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
% $\widehat{BS}$  & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
% $\widehat{CE}_{L_p}$   & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
% $\widehat{CE}_{k}$   & 0.01 & 0.005 & 0.005 & 0.005 & 0.0 & 0.0 \\
% \bottomrule
% \end{tabular}
% \caption{Average Type $2$ error for a significance level of $\alpha=0.05$ for the binary classification experiment with the $3$ different settings described in Section \ref{sec: binary classification}. The error is averaged over a number of $200$ iterations of the tests, for both the test proposed by \cite{mortier2023calibration}, and extension of it, using MLP optimization.}
% \label{tab:type2 error gp}
% \end{table}
\subsection{Multi-class classification}
We consider $K > 2$ classes, and follow a Dirichlet-based scheme to generate an ensemble of $M$ probabilistic predictors. Specifically, we generate a prior $\boldsymbol{p} \sim Dir(\boldsymbol{1})$ and predictions $f^{(1)}(x), \dots, f^{(M)}(x) \sim Dir(\frac{\boldsymbol{p}|x \cdot K}{u(x)})$, where $u: \mathcal{X} \rightarrow \mathbb{R}_{>0}$ is a function defining the epistemic uncertainty (i.e.\ the \textit{spread} between the predictions within the set) over the instance space. For the experiments, we use a constant $u(x) \equiv 0.5$. Under $H_0$, we again distinguish between
\begin{enumerate}
    \item  \textbf{Non-instance dependency} ($\boldsymbol{H_{0,1}}$):  $\boldsymbol{\lambda^*}(x) \equiv \boldsymbol{c} \in [0,1]$ where $\boldsymbol{c} \sim Dir(1, \dots,1) \in \Delta_M$ ($\boldsymbol{\lambda^*}$ is constant across the instance space).
    \item \textbf{Instance dependency} ($\boldsymbol{H_{0,2}}$): $\boldsymbol{\lambda^*}(x) = (\lambda_1^*(x), \dots, \lambda_M^*(x)) \in \Delta_M$ with $\lambda^*_m(x) = \sum_{i=0}^D \beta_i x^i$ for $m =1, \dots,M$ and $\sum_{m=1}^M \lambda_m^*(x) =1$ (the components of $\boldsymbol{\lambda^*}$ form scaled polynomials of a certain degree). 
    % TODO: this has to be improved! (also distance based approaches for setting 2 and 3?)
\end{enumerate}
For the alternative hypothesis, we select a random corner $f_c \notin \mathcal{F}$ of the simplex and then set the true underlying conditional distribution $f^*$ as a point on the  connecting line between the corner and the boundary point $f_b \in \mathcal{F}$ of the credal set: $f^*(x) = \nu \cdot f_c(x) + (1- \nu) \cdot f_b(x)$. We define three cases with increasing distance to the credal set by varying the mixing coefficient $\nu$: $\boldsymbol{H_{1,1}}$: $\nu = 0.01$,  $\boldsymbol{H_{1,2}}$: $\nu = 0.1$ and  $\boldsymbol{H_{1,3}}$: $\nu = 0.2$.\\

The resulting Type $1$ and Type $2$ errors of this experiment are shown in Figure \ref{fig: type 1 error dirichlet experiment}. We see that the proposed test yields more reliable decisions, not exceeding the given significance level while following it closely. The test of Mortier et al.\ seems to be too conservative for all estimators, except for the kernel calibration error estimator $\widehat{\text{CE}}_k$, which in general leads to unreliable results and low power. Note here that we use the same datasets for both tests, but we do the optimisation for our proposed test on a separate dataset of the same size.
The Type $2$ error analysis shows that, except for the kernel calibration error, our proposed test has a lower average Type $2$ error. Furthermore, aligns better with the chosen significance level.
% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/dirichlet_experiment/dirichlet_setting_new.png}
%     \caption{Caption}
%     \label{fig: setting multiclass}
% \end{figure}
\begin{figure}[t]
    \centering
    \begin{subfigure}{.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dirichlet_experiment/t1_t2_errors/fig_t1_dir.pdf}
     \caption{average Type $1$ error}
    \end{subfigure}
    \begin{subfigure}{.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dirichlet_experiment/t1_t2_errors/fig_t2_dir.pdf}
        \caption{average Type $2$ error}
    \end{subfigure}
    \caption{Average Type $1$ and Type $2$ error given a significance level ($x$-axis) for Algorithm \ref{alg: Alg v2.0} with $D=100$ bootstrap iterations run on the multi-class classification experiment, and the settings $\boldsymbol{H}_{0,1} - \boldsymbol{H}_{1,3}$ where the null hypothesis is true (a), and false (b), resp. with $N_{test}=400$ instances, $M=10$ ensemble members and $K=10$ classes. The average is taken over $100$ resampling iterations.}
    \label{fig: type 1 error dirichlet experiment}
\end{figure}

% To analyse the two cases of instance- and non-instance dependency for the case where the null hypothesis is true, we use a similar setting as in the case of binary classification: \begin{itemize}
%     \item for non-instance dependency, we generate $\boldsymbol{\lambda} \sim Dir(1, \dots, 1)$
%     \item for instance-dependency, we generate generate $M$ polynomial functions $p_m$ with values in an interval $[a_m, b_m]$ for $m=1, \dots, M$, where $a_m, b_m \sim Unif(0, 5)$ and define \begin{eqnarray*}
%         \boldsymbol{\lambda}(x) = \text{Softmax}(\boldsymbol{p}(x)), \quad \boldsymbol{p} = (p_1(x), \dots, p_m(x))
%     \end{eqnarray*}
% \end{itemize}
% For the case where the alternative hypothesis is true, we again look at $3$ cases of sampling the underlying calibrated prediction with different distances to the polytope. This is done by selecting a random corner of the simplex and then setting the true underlying prediction as a point on the underlying connecting line between the corner and the border of the polytope, where we define $d_{h_1} \in [0,1]$ as the coefficient of the convex combination between the point $f_b \in \mathcal{F}$ on the border of $\mathcal{F}$ and the point $f_c \notin \mathcal{F}$: \begin{eqnarray*}
%     f^*(x) = d_{h_1} \cdot f_c(x) + (1- d_{h_1}) \cdot f_b(x),
% \end{eqnarray*}
% and
% \begin{itemize}
%     \item $S_1$: $d_{h_1} = 0.05$,
%     \item $S_2$: $d_{h_1} = 0.2$
%     \item $S_3$: $d_{h_1} = 0.8$.
% \end{itemize}

\subsection{Experiments on real-world data}
Since the true data-generating distribution is unknown in real-world datasets, we cannot directly quantify Type $1$ or Type $2$ errors. Instead, we demonstrate the practical usefulness of our test by applying it to well-known image classification tasks (CIFAR-$10$, CIFAR-$100$). More specifically, we apply our test on the predictions of three ensemble methods, combined with two different architectures that are trained on the two datasets, respectively. We train a deep ensemble (DE), a dropout network with dropout rate $0.5$ (DN($0.5$) and a dropout network with dropout rate $0.2$ (DN($0.2$)). For the deep ensemble, we train $10$ different models using different weight initializations, while in the dropout networks, dropout is applied after specific layers. After training, we obtain a set of $10$ different predictions from these models.
As model architectures, the ResNet-$18$ ($11.6 \times 10^6$ parameters) and VGG-$19$ ($138 \times 10^6$ parameters) are employed. We leverage the pre-trained ResNet-18 or VGG-19 to embed images and attach a fully-connected layer to predict the weight function $\boldsymbol{\lambda}$. Section \ref{sec: experimental setup} describes the full experimental setup. 
The results are shown in Table \ref{tab: results cifar10 and cifar100}. For each model and calibration estimator, we run the respective bootstrapping test (line 5-19 of Algorithm \ref{alg: Alg v2.0}), with the optimal weight function $\boldsymbol{\lambda}^*$ learned by the neural network. For comparison, we also perform the bootstrapping part of the test with the mean predictor (Algorithm \ref{alg: Alg v2.0} from line $5$ with $f_{\boldsymbol{\lambda}_{AVG.}}=\frac{1}{M}\sum_{j=1}^M f^{(j)}$). The results differ significantly, depending on the used calibration error. In general, using the calibration error $\text{CE}_{KL}$ led to the fewest rejections (i.e.\, the highest p-values), potentially because the models were trained with a log-loss objective, which aligns more closely with KL-based calibration measures. In general, for the CIFAR-$100$ dataset with more classes, the test rejects more often, and the dropout networks are less calibrated. In most cases, the learned convex combination $\boldsymbol{\lambda}^*$ leads to a slight increase in accuracy for the combined prediction $f_{\boldsymbol{\lambda}^*}$.
% Interestingly, the test always rejects the null hypothesis when using $\widehat{\text{CE}}_2$ as calibration estimator, while it always accepts when using $\widehat{CE}_k$. 
% Seeing the results confirms the general consensus that deep learning models are generally not calibrated, even for this relatively "easy" classification task with only $10$ different classes.
% \begin{table}[htbp]
% \centering
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{c|c|c|*{6}{>{\centering\arraybackslash}p{1.5cm}}}
% \toprule
% ARCH. & TYPE & CAL. ERR. & \multicolumn{3}{c|}{$\boldsymbol{\lambda}_{AVG.}$} & \multicolumn{3}{c}{$\boldsymbol{\lambda}^*$} \\
% \cmidrule(lr){4-6} \cmidrule(lr){7-9}
%  & & & Acc. & $H_0$ & $p$-value & Acc. & $H_0$ & $p$-value \\
% \midrule
% \multirow{12}{*}{VGG} & \multirow{4}{*}{DN($0.5$)} & $\widehat{CE}_2$ & $0.892$ & REJ. & $0.0$ & $0.897$ & REJ. & $0.0$ \\
%  &  & $\widehat{CE}_k$ & $0.892$ & $\neg$REJ. & $0.18$ & $0.891$ & $\neg$REJ. & $0.15$ \\
%  &  & $\widehat{CE}_{\text{MMD}}$ & $0.892$ & REJ. & $0.0$ & $0.894$ & REJ. & $0.03$ \\
%  &  & $\widehat{BS}$ & $0.892$ & REJ. & $0.0$ & $0.896$ & REJ. & $0.0$ \\
% \cmidrule(lr){2-9}
%  & \multirow{4}{*}{DE(5)} & $\widehat{CE}_2$ & $0.901$ & REJ. & $0.0$ & $0.89$ & REJ. & $0.0$ \\
%  &  & $\widehat{CE}_k$ & $0.901$ & $\neg$ REJ. & $0.95$ & $0.903$ & $\neg$REJ. & $0.94$ \\
%  &  & $\widehat{CE}_{\text{MMD}}$ & $0.901$ & $\neg$REJ. & $0.27$ & $0.902$ & $\neg$REJ. & $0.87$ \\
%  &  & $\widehat{BS}$ & $0.901$ & $\neg$REJ. & $0.89$ & $0.911$ & $\neg$REJ. & $0.91$ \\
% \cmidrule(lr){2-9}
%  & \multirow{4}{*}{DE(10)} & $\widehat{CE}_2$ & $0.905$ & REJ. & $0.0$ & $0.89$ & REJ. & $0.0$ \\
%  &  & $\widehat{CE}_k$ & $0.905$ & $\neg$REJ. & $0.35$ & $0.904$ & $\neg$REJ. & $0.33$ \\
%  &  & $\widehat{CE}_{\text{MMD}}$ & $0.905$ & REJ. & $0.03$ & $0.915$ & REJ. & $0.04$ \\
%  &  & $\widehat{BS}$ & $0.905$ & $\neg$REJ. & $0.05$ & $0.909$ & REJ. & $0.02$ \\
% \midrule
% \multirow{12}{*}{ResNet} & \multirow{4}{*}{DN($0.5$)} & $\widehat{CE}_2$ & $0.831$ & REJ. & $0.0$ & $0.826$ & REJ. & $0.0$ \\
%  &  & $\widehat{CE}_k$ & $0.831$ & $\neg$REJ. & $0.19$ & $0.838$ & $\neg$REJ. & $0.20$ \\
%  &  & $\widehat{CE}_{\text{MMD}}$ & $0.831$ & $\neg$REJ. & $0.07$ & $0.838$ & REJ. & $0.05$ \\
%  &  & $\widehat{BS}$ & $0.831$ & REJ. & $0.0$ & $0.841$ & REJ. & $0.0$ \\
% \cmidrule(lr){2-9}
%  & \multirow{4}{*}{DE(5)} & $\widehat{CE}_2$ & $0.835$ & REJ. & $0.0$ & $0.821$ & REJ. & $0.0$ \\
%  &  & $\widehat{CE}_k$ & $0.835$ & $\neg$REJ. & $0.06$ & $0.852$ & $\neg$REJ. & $0.07$ \\
%  &  & $\widehat{CE}_{\text{MMD}}$ & $0.835$ & REJ. & $0.03$ & $0.829$ & $\neg$REJ. & $0.09$ \\
%  &  & $\widehat{BS}$ & $0.835$ & $\neg$REJ. & $0.99$ & $0.88$ & $\neg$REJ. & $1.0$ \\
% \cmidrule(lr){2-9}
%  & \multirow{4}{*}{DE(10)} & $\widehat{CE}_2$ & $0.857$ & REJ. & $0.0$ & $0.825$ & REJ. & $0.0$ \\
%  &  & $\widehat{CE}_k$ & $0.857$ & $\neg$REJ. & $0.75$ & $0.854$ & $\neg$REJ. & $0.79$ \\
%  &  & $\widehat{CE}_{\text{MMD}}$ & $0.857$ & $\neg$REJ. & $0.70$ & $0.852$ & $\neg$REJ. & $0.99$ \\
%  &  & $\widehat{BS}$ & $0.857$ & $\neg$REJ. & $1.0$ & $0.891$ & $\neg$REJ. & $1.0$ \\
% \bottomrule
% \end{tabular}
% }
% \caption{Results fur running the calibration test on different classifier models: dropout network with rate $0.5$ (DN($0.5$)), Deep Ensemble with ensemble size of $5$ (DN($5$)) and $10$ (DN($10$)). The CIFAR-$10$ dataset was used to train the models and learn the instance dependent optimal convex combination $\boldsymbol{\lambda}^*$ by minimizing the respective estimator $\hat{g} \in \{\widehat{CE}_2, \widehat{CE}_k, \widehat{CE}_{\text{MMD}}, \widehat{BS} \}$. For comparison, the same bootstrap test was performed using the mean predictions ($\boldsymbol{\lambda}_{AVG.}$) of the ensemble members. We report the resulting accuracy (Acc.), the outcome of the test (reject/not reject $H_0$) and its $p$-value. As model architectures, we use VGG$16$ (VGG) and ResNet-$18$ (ResNet).}
% \label{tab: results real data}
% \end{table}

\begin{table}[t]
\centering
\resizebox{.9\linewidth}{!}{%
\begin{tabular}{c|c|c|c|cccc|cccc}
\toprule
DATA & ARCH. & TYPE & ERR.
      & \multicolumn{4}{c|}{$\boldsymbol{\lambda}_{\mathrm{AVG}}$} 
      & \multicolumn{4}{c}{$\boldsymbol{\lambda}^*$} \\
\cmidrule(lr){5-8} \cmidrule(lr){9-12}
& & & 
& Acc. & $H_0$ & $p$--val. & $\hat{g}$
& Acc. & $H_0$ & $p$--val. & $\hat{g}$ \\
\midrule
%-------------------------------------------------------
% 1) ResNet + DE
\multirow{24}{*}[7ex]{\rotatebox[origin=c]{90}{%
    \parbox{2cm}{\centering CIFAR-$10$}}} & \multirow{12}{*}[4ex]{\rotatebox[origin=c]{90}{\parbox{2cm}{\centering ResNet-$18$}}}
 & \multirow{2}{*}{DE} 
   & $\text{CE}_{KL}$   
     & 0.869 & \(\neg\)REJ. & 1.000 & 0.259  
     & 0.873 & \(\neg\)REJ. & 0.990 & 0.262 \\[0.3em]
& &  & $\text{CE}_2$   
     & 0.869 & \(\neg\)REJ. & 0.970 & 0.125  
     & 0.872 & \(\neg\)REJ. & 0.950 & 0.121 \\[0.3em]
     
\cmidrule(lr){3-12}
% 2) ResNet + DN(0.2)
& & \multirow{2}{*}{DN($0.2$)} 
   & $\text{CE}_{KL}$
     & 0.847 & \(\neg\)REJ. & 0.810 & 0.293
     & 0.848 & \(\neg\)REJ. & 0.780 & 0.297 \\[0.3em]
& &  & $\text{CE}_2$    
     & 0.847 & \(\neg\)REJ. & 0.800 & 0.129
     & 0.849 & \(\neg\)REJ. & 0.760 & 0.295 \\[0.3em]

\cmidrule(lr){3-12}
% 3) ResNet + DN(0.5)
& & \multirow{2}{*}{DN($0.5$)} 
   & $\text{CE}_{KL}$   
     & 0.507 & \(\neg\)REJ. & 0.800 & 0.330
     & 0.507 & \(\neg\)REJ. & 0.780 & 0.329 \\[0.3em]
& &  & $\text{CE}_2$  
     & 0.507 & \(\neg\)REJ. & 0.220 & 0.104
     & 0.507 & \(\neg\)REJ. & 0.350 & 0.106 \\[0.3em]
     
\cmidrule(lr){2-12}
% 4) VGG + DE
& \multirow{12}{*}[4ex]{\rotatebox[origin=c]{90}{\parbox{2cm}{\centering VGG-$19$}}}
 & \multirow{2}{*}{DE} 
   & $\text{CE}_{KL}$  
     & 0.900 & \(\neg\)REJ. & 0.530 & 0.176
     & 0.902 & \(\neg\)REJ. & 0.420 & 0.166 \\[0.3em]
& &  & $\text{CE}_2$     
     & 0.900 & \(\neg\)REJ. & 0.150 & 0.079
     & 0.901 & \(\neg\)REJ. & 0.180 & 0.084 \\[0.3em]
     
 % &  & \textbf{MMD}   
 %     & 0.900 & \(\neg\)REJ. & 0.990 & \(1.00\times10^{-4}\)
 %     & 0.901 & \(\neg\)REJ. & 0.910 & \(1.00\times10^{-4}\) \\[0.3em]
 % &  & \textbf{SKCE}  
 %     & 0.900 & \(\neg\)REJ. & 0.310 & \(1.72\times10^{-6}\)
 %     & 0.901 & \(\neg\)REJ. & 0.160 & \(1.13\times10^{-5}\) \\
\cmidrule(lr){3-12}
% 5) VGG + DN(0.2)
& & \multirow{2}{*}{DN($0.2$)} 
   & $\text{CE}_{KL}$ 
     & 0.903 & REJ.         & 0.000 & 0.134
     & 0.903 & REJ.         & 0.000 & 0.133 \\[0.3em]
& &  & $\text{CE}_2$  
     & 0.903 & REJ.         & 0.000 & 0.042
     & 0.903 & REJ.         & 0.000 & 0.042 \\[0.3em]
     
\cmidrule(lr){3-12}
% 6) VGG + DN(0.5)
& & \multirow{2}{*}{DN($0.5$)} 
   & $\text{CE}_{KL}$   
     & 0.899 & REJ.         & 0.000 & 0.131
     & 0.896 & REJ.         & 0.000 & 0.129 \\[0.3em]
& &  & $\text{CE}_2$     
     & 0.899 & REJ.         & 0.010 & 0.047
     & 0.896 & REJ.         & 0.000 & 0.056 \\[0.3em]
     

\midrule
\midrule
% results for CIFAR100
\multirow{24}{*}[7ex]{\rotatebox[origin=c]{90}{%
    \parbox{2cm}{\centering CIFAR-$100$}}} & \multirow{12}{*}[4ex]{\rotatebox[origin=c]{90}{\parbox{2cm}{\centering ResNet-$18$}}}
 & \multirow{2}{*}{DE} 
   & $\text{CE}_{KL}$   
   & 0.586 & \(\neg\)REJ. & 1.000 & 1.200
     & 0.589 & \(\neg\)REJ. & 1.000 & 1.203 \\[0.3em]
& &  & $\text{CE}_2$   
     & 0.586 & REJ.         & 0.000 & 0.476
     & 0.589 & REJ.         & 0.000 & 0.482 \\[0.3em]
     
\cmidrule(lr){3-12}
& & \multirow{2}{*}{DN($0.2$)} 
   & $\text{CE}_{KL}$
     & 0.5162 & REJ. & 0.000 & 1.4419
     & 0.5154 & REJ. & 0.000 & 1.4411 \\[0.3em]
& &  & $\text{CE}_2$    
     & 0.5162 & REJ.         & 0.000 & 0.4426
     & 0.5162 & REJ.         & 0.000 & 0.4423 \\[0.3em]
\cmidrule(lr){3-12}
% 3) ResNet + DN(0.5)
& & \multirow{2}{*}{DN($0.5$)} 
   & $\text{CE}_{KL}$   
        & 0.352 & \(\neg\)REJ. & 1.000 & 2.123
     & 0.353 & \(\neg\)REJ. & 1.000 & 2.136 \\[0.3em]
& &  & $\text{CE}_2$  
      & 0.352 & REJ.         & 0.000 & 0.620
     & 0.351 & REJ.         & 0.030 & 0.624 \\[0.3em]
     
\cmidrule(lr){2-12}
% 4) VGG + DE
& \multirow{12}{*}[4ex]{\rotatebox[origin=c]{90}{\parbox{2cm}{\centering VGG-$19$}}}
 & \multirow{2}{*}{DE} 
   & $\text{CE}_{KL}$  
     & 0.647 & \(\neg\)REJ. & 0.510 & 0.925
     & 0.648 & \(\neg\)REJ. & 0.390 & 0.927 \\[0.3em]
& &  & $\text{CE}_2$     
     & 0.647 & REJ.         & 0.000 & 0.347
     & 0.647 & REJ.         & 0.000 & 0.346 \\[0.3em]
     

\cmidrule(lr){3-12}
% 5) VGG + DN(0.2)
& & \multirow{2}{*}{DN($0.2$)} 
   & $\text{CE}_{KL}$ 
      & 0.621 & REJ.         & 0.030 & 0.999
     & 0.620 & REJ.         & 0.000 & 1.006 \\[0.3em]
& &  & $\text{CE}_2$  
     & 0.621 & REJ.         & 0.020 & 0.343
     & 0.620 & REJ.         & 0.020 & 0.343 \\[0.3em]

\cmidrule(lr){3-12}
% 6) VGG + DN(0.5)
& & \multirow{2}{*}{DN($0.5$)} 
   & $\text{CE}_{KL}$   
    & 0.588 & \(\neg\)REJ. & 0.210 & 1.282
     & 0.587 & \(\neg\)REJ. & 0.410 & 1.274 \\[0.3em]
& &  & $\text{CE}_2$     
      & 0.588 & REJ.         & 0.010 & 0.417
     & 0.587 & REJ.         & 0.010 & 0.416 \\[0.3em]
\bottomrule
\end{tabular}}
\caption{Calibration test results on CIFAR-$10$ and CIFAR-$100$ for the significance level $\alpha=0.05$ and $D=100$ bootstrap iterations. 
For each architecture (ResNet-$18$ or VGG-$19$) and model type 
(Deep Ensemble (DE) or MC Dropout (DN($p$)) with rate $p \in \{0.2, 0.5\}$, we show results of running the proposed test using two different calibration errors, $g\in \{\text{CE}_{KL}, \text{CE}_2\}$. For $g=\text{CE}_{KL}$, log loss, for $g=\text{CE}_2$ the Brier score is used as the proper scoring rule for optimization.
Columns 4--7 show accuracy, test decision, 
\(p\)--value, and value of the test statistic for the average ensemble combination 
(\(\boldsymbol{\lambda}_{\mathrm{AVG}}\)), and columns 8--11 for the learned convex combination 
(\(\boldsymbol{\lambda}^*\)).}
\label{tab: results cifar10 and cifar100}
\end{table}



\section{Discussion}
%TODO
We developed a novel statistical test for the calibration of epistemic uncertainty representations based on sets of probability distributions. It is defined on an instance-based level, thereby making the test more flexible. As calibration forms a necessary condition for \textit{validity}, we claim that by testing for it, one can safely detect scenarios where the credal set is \textit{not} valid. For this case, the next step to include would be \textit{actionability}, i.e., ways to increase the size of the credal set to include at least one calibrated convex combination.
Here, we model calibration as a property that applies across the entire instance space. Alternatively, calibration could be viewed as an instance-specific concept, allowing analysis in different regions of the instance space. However, there has been limited research on this form of calibration, often referred to as conditional or \textit{local} calibration \cite{luo2022local}, and the challenge of partitioning the instance space in a way that enables the computation of expectations remains unresolved. Future work includes also the analysis of the regression case.
% \paragraph{Sample Heading (Fourth Level)}
% The contribution should contain no more than four levels of
% headings. Table~\ref{tab1} gives a summary of all heading levels.

% \begin{table}
% \caption{Table captions should be placed above the
% tables.}\label{tab1}
% \begin{tabular}{|l|l|l|}
% \hline
% Heading level &  Example & Font size and style\\
% \hline
% Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
% 1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
% 2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
% 3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
% 4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
% \hline
% \end{tabular}
% \end{table}


% \noindent Displayed equations are centered and set on a separate
% line.
% \begin{equation}
% x + y = z
% \end{equation}
% Please try to avoid rasterized images for line-art diagrams and
% schemas. Whenever possible, use vector graphics instead (see
% Fig.~\ref{fig1}).


% \begin{theorem}
% This is a sample theorem. The run-in heading is set in bold, while
% the following text appears in italics. Definitions, lemmas,
% propositions, and corollaries are styled the same way.
% \end{theorem}
% %
% % the environments 'definition', 'lemma', 'proposition', 'corollary',
% % 'remark', and 'example' are defined in the LLNCS documentclass as well.
% %
% \begin{proof}
% Proofs, examples, and remarks have the initial word in italics,
% while the following text appears in normal font.
% \end{proof}
% For citations of references, we prefer the use of square brackets
% and consecutive numbers. Citations using labels or the author/year
% convention are also acceptable. The following bibliography provides
% a sample reference list with entries for journal
% articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
% book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
% and a homepage~\cite{ref_url1}. Multiple citations are grouped
% \cite{ref_article1,ref_lncs1,ref_book1},
% \cite{ref_article1,ref_book1,ref_proc1,ref_url1}.
\subsection*{Acknowledgements}
 M.J. and W.W. received funding from the Flemish Government under the “Onderzoeksprogramma Artifici\"ele Intelligentie (AI) Vlaanderen” programme.


% \section*{Declarations}

% Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

% \begin{itemize}
% \item Funding
% \item Conflict of interest/Competing interests (check journal-specific guidelines for which heading to use)
% \item Ethics approval and consent to participate
% \item Consent for publication
% \item Data availability 
% \item Materials availability
% \item Code availability 
% \item Author contribution
% \end{itemize}

% \noindent
% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. 

% %%===================================================%%
% %% For presentation purpose, we have included        %%
% %% \bigskip command. Please ignore this.             %%
% %%===================================================%%
% \bigskip
% \begin{flushleft}%
% Editorial Policies for:

% \bigskip\noindent
% Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

% \bigskip\noindent
% Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

% \bigskip\noindent
% \textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

% \bigskip\noindent
% BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
% \end{flushleft}
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{plainnat} 
\bibliography{sn-bibliography}
\begin{appendices}
\section{Estimator analysis}\label{sec: estimator analysis}
In this section we empirically analyse the distribution of the chosen calibration estimators and their behaviour with respect to the distance from the underlying ground truth probability distribution.
Figure \ref{fig: histogram estimators h0} shows the distribution of the estimators under the null hypothesis.
In the figure, we see that the mean of the estimators $\widehat{\text{CE}}_{2}$ and $\widehat{\text{CE}}_{KL}$ is slightly above zero - since they are only asymptotically unbiased - while 
$\widehat{\text{CE}}_{MMD}$ and $\widehat{\text{CE}}_{k}$ also empirically show their unbiasedness. The kernel calibration estimator $\widehat{CE}_k$ is symmetrically distributed around zero -- its asymptotic normality was also shown by Widmann et al.\ \cite{widmann2019calibration}. As the estimators $\widehat{CE}_k$ and $\widehat{CE}_{MMD}$ can obtain negative values, we use their squared versions for the optimization in Eqn.\ (\ref{eq: combined calibration loss}).
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/estimator_analysis/histogram_cal_estimates_new.pdf}
    \caption{\textit{Distributions of calibration error estimators under the null hypothesis}. Histograms (with kernel density estimates) of the estimated calibration error $ g \in \{\widehat{CE}_{2},\widehat{CE}_{KL}, \widehat{CE}_{\mathrm{MMD}},\widehat{CE}_{k} \}$ are shown. Predicted probabilities were sampled from a uniform Dirichlet distribution over three classes ($K = 3$) and repeated $N = 1000$ times to simulate a perfectly calibrated model. Labels were sampled from the corresponding categorical distribution in $D = 500$ resampling steps. Vertical dashed black lines indicate the mean values of each statistic. Vertical dashed red lines indicate the $95\%$ quantile of the empirical distribution. }
    \label{fig: histogram estimators h0}
\end{figure}

Figure \ref{fig: heatmaps calibration estimates} shows the values of different calibration estimators in dependence of the position in the simplex, where the true underlying calibrated convex combination $f_{\boldsymbol{\lambda}^*}$ is set with $\boldsymbol{\lambda}^*=(0.1, 0.1, 0.8)$, and the predictions are sampled from a Dirichlet distribution. We see that by optimizing over $\boldsymbol{\lambda}$, the true calibrated convex combination is learned differently "well enough". However, $\boldsymbol{\lambda}^*$ still lies within a certain region of low estimator values. For $\widehat{CE}_k$, we see in general more noisy behavior in the region around the ground truth distribution (red dot).
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/estimator_analysis/heatmap_cal_estimates_inferno.png}
    \caption{Simple experiment that illustrates the behavior of the calibration estimators for the case of a \textit{non-instance-dependent} underlying ground truth distribution. Log-transformed values of the empirical calibration error $\hat{g}(\hat{f}, \mathcal{D})$ where $\hat{g} \in \{\widehat{\text{CE}}_{KL}, \widehat{\text{CE}_2}, \widehat{\text{CE}}_{k}^2, \widehat{\text{CE}}_{MMD}\},$ in dependence of the (constant) predictor $\hat{f}$ within the probability simplex, for the case of $K=3$ classes. $N=2000$ labels $y_i$ in $\mathcal{D}$ were generated from the ground truth categorical distribution, $y_i \sim \text{Cat}(f^*), \, i=1, \dots, 2000$. Here, we set $f^*(x) \equiv (0.1, 0.1, 0.8)^T$ constant for all $x \in \mathcal{X}$. The red point corresponds to $f^*$ and represents the theoretical minimum of the calibration error.}
    \label{fig: heatmaps calibration estimates}
\end{figure}

We also do a similar analysis in $\lambda$-space: In Figure \ref{fig: heatmaps lambda space}, we show the value of the calibration estimators not in distance to the ground truth probability distribution, but the ground truth weight vector $\boldsymbol{\lambda}\in \Delta_3$, given the predictions of two predictors $\{f^{(1)}(x), f^{(2)}(x), f^{(3)}(x)\}$ and a ground truth probability distribution $f_{\boldsymbol{\lambda}^*}(x) = 0.1 \cdot f^{(1)}(x) + 0.1 \cdot f^{(2)}(x) + 0.8 \cdot f^{(3)}(x)$, that is $\boldsymbol{\lambda}*=(0.1,0,1,0.8)^T$. We see that here the heatmaps vary more significantly for the different calibration estimators. 
\begin{figure}[h]
    \centering
    \includegraphics[width=.8\linewidth]{figures/heatmap_cal_estimates_lambda_viridis.png}
    \caption{Heatmaps of the values of the chosen calibration estimators for a given location within the space of weights $\boldsymbol{\lambda} \in \Delta_3$. The predictions $f^{(i)} \in \{f^{(1)}, f^{(2)}, f^{(3)}\}$ are generated according to a Dirichlet distribution with parameters $ \alpha_j = 1 + \mathbb{I}_{i=j} \cdot 10$ for $j \in \{1,2,3\}$.}
    \label{fig: heatmaps lambda space}
\end{figure}
\section{Non-uniqueness of calibrated models}
\label{sec: many calibrated convex combinations}
Using simple examples, we will show the non-uniqueness of calibrated convex combinations, that is, that there are often many different (possibly instance-dependent) convex combinations leading to a calibrated predictor. Using a proposition proven by Vaicenavicius et al.\ \cite{vaicenavicius19a}, which states that conditioning $Y$ on any measurable function $h$ yields a calibrated predictor, we show this with a concrete example.
\begin{example}[Many calibrated classifiers]
\label{example: many calibrated classifiers} For the following, assume the case of having $3$ classes, i.e.\ $\mathcal{Y}= \{1,2,3\}$.
    We have (\cite{vaicenavicius19a}, Proposition 1) for any measurable function $h: \mathcal{X} \mapsto \mathcal{Z}$ with $\mathcal{Z}$ being
    some measurable space, that the function \begin{equation*}
        f(X):= \mathbb{P}[Y \in \cdot | h(X)]
    \end{equation*}
    is a calibrated classifier. 
        As an example, taking a constant $h(x) = c \in \mathbb{R} \, \forall x \in \mathcal{X}$,
    the classifier that simply predicts the marginal class probabilities \begin{equation*}
        f \equiv \begin{pmatrix}
            \mathbb{P}(Y=1) \\
            \mathbb{P}(Y=2) \\
            \mathbb{P}(Y=3) \\
        \end{pmatrix}
    \end{equation*}
    is calibrated for $\mathcal{Y}= \{1, 2, 3 \}$. Further, if we take $h: \mathcal{X}\mapsto \mathcal{X}$ with $h(x)=x$, then \begin{equation*}
        f' \equiv \begin{pmatrix}
            \mathbb{P}(Y=1|X) \\
            \mathbb{P}(Y=2|X) \\
            \mathbb{P}(Y=3|X) \\
        \end{pmatrix}
    \end{equation*}
    is also a calibrated classifier. To make this example more concrete, let $\mathcal{X}=\{1,2,3\}$ with
    $\mathbb{P}(X=x)=\frac{1}{3} \forall \, x \in \mathcal{X}$. Further let again $\mathcal{Y}=\{1,2,3\}$ and \begin{equation*}
        \mathbb{P}(Y=i|X=j) = \begin{cases}
            1, i=j \\
            0, \text{else}
        \end{cases}
    \end{equation*}
    for $i, j \in \{1,2,3\}$.
Then  $\mathbb{P}(Y=i)= \sum_{j=1}^3 \mathbb{P}(Y=i|X=j)\mathbb{P}(X=j)= \frac{1}{3}$
for $i \in \{1,2,3\}$, and \begin{equation*}
    f(x) \equiv \begin{pmatrix}
        1/3 \\
        1/3 \\
        1/3
    \end{pmatrix},
\quad f'(x) = \mathbf{e}_{x}
\end{equation*}
where $\mathbf{e}_{x}$ denotes the $x$-th unit vector, are both calibrated predictors.
\end{example}
Going from the case of one single classifier, similarly, one can also show that there usually exists \textit{various calibrated convex combinations} for a set of classifiers. The following example illustrates this for the simple case of binary classification.
\begin{example}
    Let us look at a very simple problem of binary classification with $\mathcal{Y} = \{1, 2 \}$ and $\mathcal{X}= \{-1,1\}$. Assume \begin{equation*}
        \mathbb{P}(Y=i|X=x_j)= \begin{cases}

            1, \quad (i, x_j) \in \{(1, -1), (2,1)\} \\
            0, \quad \text{else},
        \end{cases}
    \end{equation*}
 and $\mathbb{P}(X=x) = \frac{1}{2}, \quad x \in \mathcal{X}$. Then the two classifiers \begin{align*}
        g_1: \, & \mathcal{X} \rightarrow \Delta^2 \\
        & x \mapsto \begin{pmatrix}
            \frac{1}{2} \\
            \frac{1}{2}
        \end{pmatrix}, \quad x \in \{-1,1\}
    \end{align*}
    and \begin{align*}
        g_2: \, & \mathcal{X} \rightarrow \Delta^2 \\
        & x \mapsto \begin{cases}
            \begin{pmatrix}
                1 \\
                0 
            \end{pmatrix}, \quad x = -1 \\
        \begin{pmatrix}
            0 \\
            1
        \end{pmatrix}, \quad x = 1
        \end{cases}
    \end{align*}
    are both calibrated (which can easily be seen by Proposition 1 of \cite{vaicenavicius19a} and 
    conditioning on $h_1(x)=c$ and $h_2(x)=x$). Further, they can be written as convex combinations of the two classifiers $m_1(x) = \begin{pmatrix}
        0 \\
        1
    \end{pmatrix}, \forall x$ and $m_2(x) = \begin{pmatrix}
        1 \\
        0
    \end{pmatrix} \forall x$ using the two different convex combinations \begin{align*}
        \lambda_1:\, & \mathcal{X} \rightarrow \Delta^2 \\
        & x \mapsto \begin{cases}
            \begin{pmatrix}
                0 \\
                1
            \end{pmatrix}, x = -1 \\
            \begin{pmatrix}
                1 \\
                0
            \end{pmatrix}, x = 1
        \end{cases}
    \end{align*}
    and \begin{align*}
        \lambda_2: \, & \mathcal{X} \rightarrow \Delta^2 \\
        & x \mapsto \begin{pmatrix}
            \frac{1}{2} \\
            \frac{1}{2}
        \end{pmatrix}, x \in \{-1,1\}.
    \end{align*}
    \end{example}

\section{Uniform sampling in the credal set}
\label{sec: uniform sampling}
 \begin{figure}[t]
        \centering
        \subfloat[]{\includegraphics[width=0.29\textwidth]{figures/sampling/simplex_triangle_sampling.png}
        }
        \subfloat[]{\includegraphics[width=0.29\textwidth]{figures/sampling/simplex_hexagon_sampling.png}}
         \subfloat[]{\includegraphics[width=0.29\textwidth]{figures/sampling/simplex_hexagon+3.png}}
         \hfill
        \subfloat[]{\includegraphics[width=0.29\textwidth]
        {figures/sampling/hexagon_sampling_matrix_hit_and_run.png}} 
        \subfloat[]{\includegraphics[width=0.29\textwidth]{figures/sampling/hexagon+3_mcmc_sampling.png}}
        \captionsetup{width=\linewidth}
        \caption{Sampling example for $K=3$: The point predictions $f^{(i)}(x)$ are visualized as red dots on the corners and/or 
        the inside of the $2$-simplex. In (a) and (b), and (c) samples are generated by uniformly sampling the weights of the convex combination,
        in (d) and (e), an MCMC approach is used.}
        \label{fig: simplices}
    \end{figure}
In the work of Mortier et al.\ \cite{mortier2023calibration}, a sampling within the polytope of convex combinations of predictions is done by sampling weights $\boldsymbol{\lambda} \in \Delta_M\sim Dir(1, \dots, 1)$. This however does in general not lead to a uniform sampling within the polytope, as is exemplary shown in Figure \ref{fig: simplices}. As soon as we have more than $M=3$ predictors, the sampled predictions start "accumulating" in the centre of the simplex, due to the fact that there are more possible combinations in the centre than for the regions in the boundary. Possible ways to achieve uniform sampling is by using triangulization methods, rejection sampling or Markov Chain Monte Carlo sampling methods, however, these methods come with increased computational effort.

\section{Experimental setup}
\label{sec: experimental setup}
 In this section we explain the experimental setup for the experiments conducted in Section \ref{sec: experimental results}.
\subsection{Synthetic data}
For optimising the weights $\boldsymbol{\lambda}$ in the synthetic experiments, we use an MLP with $3$ hidden layers each consisting of $16$ neurons that is trained on an optimization dataset of size $N_{opt}=400$. Both tests (the previous one proposed by Mortier et al.\ and ours) are performed on a test set of size $N_{test}=400$.\\

For the case of \textbf{binary classification}, we draw $\{x_i\}_{i=1}^N \sim U([0,5])$, and generate predictions from two probabilistic predictors $\{f^{(1)}(x), f^{(2)}(x)\}$ by sampling from a Gaussian process constrained to output probabilities $[0,1]$. Under $H_0$, we generate the probability for class $1$, $f^*(x) = \lambda^*\,f^{(1)}(x) + (1-\lambda^*)\,f^{(2)}(x)$ with $\lambda^*$ generated as \begin{itemize}
    \item $\boldsymbol{H_{0,1}}$: $\lambda^*$ is a constant and sampled as $\lambda^* \sim U(0,1)$,
    \item $\boldsymbol{H_{0,2}}$: $\lambda^*: \mathcal{X}\rightarrow [0,1]$ is a polynomial of degree $D$. In the experiments, we set $D=2$.
\end{itemize}
Under $H_1$, $f^*(x)$ lies outside the credal set with \begin{itemize}
    \item $\boldsymbol{H_{1,1}}$: $f^*(x)$ lies at an $\epsilon$-distance to one of the boundaries of the credal set. We sample $\epsilon \sim U([0,0.05])$.
    \item $\boldsymbol{H_{1,2}}-\boldsymbol{H_{1,3}}$ reflect increasing distance to the boundary by sampling a new GP that intentionally remains outside the credal set.
\end{itemize} 

For the \textbf{multi-class classification} case, we again draw $\{x_i\}_{i=1}^N \sim U([0,5])$ and generate the set of probabilistic predictors $f^{(1)}, \dots, f^{(M)}\}$ as follows: For each $x_i$, draw a prior $\boldsymbol{p}\in \mathbb{R}^K \sim Dir(1, \dots, 1)$, then sample $f^{(m)}(x_i) \sim Dir(\frac{\boldsymbol{p} \cdot K}{0.5})$. Under $H_0$, the weight function $\boldsymbol{\lambda}^*$ is generated as \begin{itemize}
    \item $\boldsymbol{H_{0,1}}$: $\boldsymbol{\lambda}^*\sim Dir(1, \dots, 1)$
    \item $\boldsymbol{H_{0,2}}$: $\boldsymbol{\lambda}^*=(\lambda_1^*, \dots, \lambda_M^*)$ with $\lambda_m^* : \mathcal{X} \rightarrow [0,1]$; the components of $\boldsymbol{\lambda}^*$ are randomly generated and scaled polynomials of degree $D=2$.
\end{itemize}
Under $H_1$, $f^c(x)$ is randomly chosen as one of the corners of the simplex, outside of the convex set. $f^*(x)$ is then given by the point prediction on the line segment between $f^c(x)$ and the boundary point $f^b(x)$ in the credal set, $f^*(x)(x) = \nu f^c(x) + (1-\nu)\,f^b(x)$, with \begin{itemize}
    \item $\boldsymbol{H_{0,1}}$: $\nu=0.01$,
    \item $\boldsymbol{H_{0,2}}$: $\nu=0.1$,
    \item $\boldsymbol{H_{0,3}}$: $\nu=0.2$.
\end{itemize}
In both settings, the labels $\{(y_i)\}_{i=1}^N$ are then sampled from the resulting categorical distribution with parameter $f^*(x_i)$.
\subsection{Real Data}
Here we provide details on the data preprocessing, model architectures, hyper-parameters, and calibration-test implementation used in the real-world experiments.\\

\textbf{Data splitting}:
We split the data into training data, used to train the ensembles/dropout models, validation and test. The validation data together with the predictions are then used to optimise for $\boldsymbol{\lambda}^*$, and the test is performed on the test data. 
\begin{itemize}
    \item CIFAR-$10$: 50000 training, 5000 validation and 5000 test samples,
    \item CIFAR-$100$: 60000 training, 5000 validation and 5000 test samples.
\end{itemize}
\textbf{Models}: We used the following training parameters to train to deep ensemble and dropout models: \begin{itemize}
    \item Adam Optimizer with $\text{lr}=0.001$,
    \item loss function: cross-entropy (log loss)
    \item batch size: $128$
    \item number of epochs: $50$
    \item early stopping: monitor validation loss, stop if does not decrease for $\text{patience}=10$ epochs.
\end{itemize}
\textbf{Optimization of weights}: We concatenate a small MLP consisting of $3$ hidden layers and $32$ neurons to the same VGG-19/ResNet-18 architecture that has been used for learning the prediction sets. For optimization, we use the combined loss as in (\ref{eq: combined calibration loss}) with \begin{itemize}
    \item log loss and $\widehat{\text{CE}}_{KL}$ with $\gamma=0.1$ for testing $\text{CE}_{KL}$
    \item Brier score and $\widehat{\text{CE}}_2$ with $\gamma=0.1$ for testing $\text{CE}_2$.
\end{itemize}
As optimisation parameters, we use $\text{lr}=0.0001$, $\text{batch size} = 256$, Adam optimizer and $200$ epochs.

\end{appendices}
\end{document}