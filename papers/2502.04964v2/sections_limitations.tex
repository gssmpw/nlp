% !TEX root = ../main.tex

\section{Limitations}

While our proposed \texttt{CoCoA} approach demonstrates robust empirical performance, several important considerations remain.

\paragraph{Task and Domain Dependency.}
  Our method relies on both an information-based confidence score and a semantic similarity function. The effectiveness of each can vary across models, tasks, and domains. For open-ended tasks with multiple equally valid outputs (e.g., creative generation), consistent rephrasing may inflate the perceived certainty. Conversely, in domains that demand highly precise factual or logical correctness (e.g., math problem solving), small deviations in reasoning can lead to large outcome differences that are not fully captured by a generic similarity measure. Adapting both the confidence measure and the similarity function to specific domains or prompt types is an important direction for future work.

\paragraph{Limited Sample Size.}
  \texttt{CoCoA} estimates the model's consistency by sampling multiple outputs and comparing them. In practice, generating a large number of samples can be computationally expensive and may increase the inference latency. Consequently, our experiments (like many sampling-based approaches) rely on relatively small sample sets. Although even a handful of samples can provide a meaningful estimate of consistency, it may not fully capture the diversity of the underlying distribution for certain tasks or for more complex prompts.

\paragraph{Quality Metric.}
  Finally, the \texttt{CoCoA}'s performance assessment depends on quality metrics (e.g., COMET for machine translation, and Accuracy for QA) that may not capture every nuance of textual outputs. Automatic metrics can have blind spots, particularly in evaluating coherence, factual correctness, or subtle aspects of style. Further refining or extending quality metrics to account for deeper reasoning, factual faithfulness, and stylistic appropriateness would better align uncertainty scores with real-world perceptions of model correctness.
