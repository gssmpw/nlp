% !TEX root = ../main.tex

\section{Experiments}
\subsection{Experimental Setup}
  To evaluate the effectiveness of the proposed method, we extended the \texttt{LM-Polygraph} library~\citep{vashurin2024benchmarkinguncertaintyquantificationmethods,fadeeva-etal-2023-lm} by implementing our approach within its framework. Since the library already includes tools for calculating other uncertainty scores, it provided a convenient and efficient environment for setting up and running experiments. The primary objective of our experiments is to evaluate whether our method offers improved performance in key tasks such as question answering (QA), summarization (SUM), and machine translation (MT), compared to existing baselines.

  \begin{figure}[t!]
    \centering
    \includegraphics[trim={0.cm 0.cm 0.cm 0.cm},clip,width=0.99\linewidth]{img_prr.jpg}
    \caption{Prediction-Rejection Ratio (PRR) Curve. The curve illustrates the quality of the non-rejected predictions as a function of the rejection rate. \textit{Oracle} represents the optimal rejection strategy, \textit{Random} is a random rejection, and \textit{UE} is rejection based on the evaluated uncertainty estimation method.}
  \label{fig:prr}
  \end{figure}
  
\paragraph{Datasets.}
  For QA, we selected diverse datasets to capture a variety of challenges: TriviaQA~\citep{joshi-etal-2017-triviaqa}, an open-domain factual QA dataset; CoQA~\citep{coqa}, a conversational QA benchmark requiring multi-turn contextual understanding; MMLU~\citep{mmlu}, a multi-task dataset spanning 57 topics to test broad knowledge; and GSM8k~\citep{gsm8k}, which focuses on grade-school math problems requiring logical reasoning. For translation, we evaluated our method on WMT14 French-English~\citep{wmt14} and WMT19 German-English~\citep{wmt19translate}. Finally, for summarization, we used XSum~\citep{xsum}, a dataset of complex documents paired with concise abstractive summaries.

  For all datasets, we follow~\citep{vashurin2024benchmarkinguncertaintyquantificationmethods} for selecting the subsets, for prompt formatting, and for number and sourcing of few-shot examples.

\paragraph{Models.}
  We evaluated our method using the base versions of three open-weights language models: LLaMA 3.1 8B~\cite{touvron2023llama}, Mistral 7B~\cite{mistral}, and Falcon~3~7B~\cite{Falcon3}. The open-source nature of these models provides full access to their token probabilities, which are essential for implementing our UQ method. For all models, we consider base versions, without instruction-tuning.

  \input{tables_final_table_best_sample} 

\paragraph{Similarity Function.}
  To measure the similarity, we use the RoBERTa-large cross-encoder model, fine-tuned on the Semantic Textual Similarity benchmark dataset~\citep{liu2019robertarobustlyoptimizedbert,DBLP:journals/corr/abs-1908-10084,huggingface:dataset:stsb_multi_mt}. This model is widely regarded as one of the most reliable and commonly used approaches for evaluating sentence similarity. The cross-encoder processes two sequences jointly and directly outputs a similarity score ranging from 0 to 1, providing a nuanced measure.

  Appendix~\ref{sec:ablation_sim_mat} contains comparative experiments with cross-encoder and other choices of the similarity function, substantiating this choice.


\paragraph{Baselines.} 
  We compare the performance of the proposed method against a diverse set of baseline and state-of-the-art UQ scores, including confidence-based, consistency-based, and hybrid approaches. Information-based methods include Maximum Sequence Probability (MSP), Perplexity (PPL), Mean Token Entropy (MTE), Monte Carlo Sequence Entropy (MCSE), and Monte Carlo Normalized Sequence Entropy (MCNSE). Consistency-based methods include the Degree Matrix (DegMat) and the Sum of Eigenvalues of the Graph Laplacian (EigValLaplacian). Finally, Hybrid methods include Semantic Entropy and SAR. All formulations for these baselines can be found in Appendix~\ref{sec:appendix_methods}.

\paragraph{Evaluation measure.}
  As our evaluation measure, we choose the Prediction Rejection Ratio (PRR), which measures the effectiveness of the uncertainty scores for identifying high-quality predictions~\citep{malinin2020uncertainty}. PRR operates by progressively rejecting predictions with uncertainty scores above a threshold $a$ and observing how the average quality $Q(f(\xv_i), \yv_i)$ of the remaining predictions changes. The metric is calculated as the ratio of two areas: the area between the Prediction Rejection (PR) curves for the evaluated uncertainty score and a random baseline, and the area between the oracle (the ideal uncertainty score that perfectly ranks instances by quality) and the random baseline. Formally, PRR is defined as follows:
    \begin{equation}
    PRR = \frac{\text{AUC}_{\text{unc}}-\text{AUC}_{\text{rnd}}}{\text{AUC}_{\text{oracle}}-\text{AUC}_{\text{rnd}}}.
  \label{eq:prr}
  \end{equation}
  %
  Higher PRR values indicate better alignment of uncertainty scores with prediction quality, approaching the performance of an oracle. To ensure practical applicability, we compute PRR only up to a rejection threshold of 50\%, preventing cases where excessive rejection artificially inflates quality measures. Figure~\ref{fig:prr} gives a visual representation of the PRR calculation, highlighting the relationship between the uncertainty threshold and the quality measures.

  \input{tables_final_table_greedy}

\paragraph{Quality Measures.}
  The Predictive Rejection Ratio (PRR) requires an appropriate quality measure for each specific task to effectively evaluate the model output. For question-answering tasks, we use \textit{Accuracy} to directly evaluate whether the generated answers match the ground truth in short-form QA tasks (e.g., MMLU), and we use the \textit{AlignScore} between correct answer and generated sequence for assessing the performance for long-form QA tasks~\citep{zha2023alignscore}. For summarization tasks, we use  \textit{AlignScore} to measure the alignment between the output summary and the input document. It serves as a quality indicator by evaluating the relevance and the overlap between the generated content with the source text. For translation tasks, we use \textit{COMET}, as it captures both semantic adequacy and fluency, ensuring that translations are accurate and linguistically appropriate~\citep{rei-etal-2020-comet}. 

\paragraph{Generation Setup.}
  We discuss the generation parameters, the decoding strategy and sample selection procedure in depth in Appendix~\ref{sec:experimental_results}. In short, we report evaluation results in two distinct setups: greedy decoding and stochastic sampling with focus on the most probable sequence among the generated outputs (\textit{best-sample}). These two setups offer the highest-quality outputs and are the most reasonable generation approaches in practice. 


\subsection{Results}
\paragraph{Main results.}
  Tables~\ref{tab:best_sample_results} and \ref{tab:greedy_results} show the PRR scores under the \textit{best‐sample} and  \textit{greedy} generation setup. We calculate a single representative PRR for each task -- question answering, neural machine translation (NMT), and summarization (SUM) -- by averaging the results across all relevant datasets (e.g., TriviaQA, MMLU, CoQA, GSM8k for QA). This aggregated score provides a concise measure of the performance for each model for each task. Detailed results for each dataset separately can be found in Appendix~\ref{sec:experimental_results}. 

  We can see that our \texttt{CoCoA} methods are \textit{the best} across all tasks and models in our experiments. They outperform existing consistency‐based and hybrid state‐of‐the‐art approaches, like Semantic Entropy and SAR. In addition, our proposed \texttt{CoCoA} approach consistently surpasses the baseline UE metrics: for example, $\texttt{CoCoA}_{PPL}$ outperforms standard $\text{Perplexity}$, illustrating the advantage of combining token‐level confidence with semantic consistency. This pattern holds for other information‐based metrics as well, demonstrating that using the consistency between multiple sampled outputs reliably enhances uncertainty quantification.

\paragraph{Ablation study.}
  As a part of our ablation study (see Appendix~\ref{suppl:ave_dissim_ue}), we evaluate the performance of the average dissimilarity component (\( u_*^{\text{cons}} \)) independently to assess its effectiveness as a standalone uncertainty measure and to investigate whether it could potentially outweigh the contribution of the information-based component (\( u_*^{\text{info}} \)) in the enriched uncertainty measure. This evaluation enables us to isolate and better understand the complementary roles and the relative importance of each component. Our experiments demonstrate that the combination of consistency- and confidence-based metrics outperforms the pure consistency-based measure on a vast majority of tasks. Notably, in the few cases where the pure consistency messure outperforms the combined approach, the performance difference is minimal. It is possible that a more suitable choice of a similarity measure or confidence-based metric for the task could further improve the performance.
 
  This leads us to the next part of our ablation study, where we investigate the impact of different similarity measures (see Appendix~\ref{sec:ablation_sim_mat}). We find that for some tasks, the similarity score computed by the Cross-encoder does not yield optimal performance. For example, for question-answering tasks on CoQA and Trivia, NLI-derived similarity performs better than the Cross-encoder similarity and outperforms the pure consistency-based uncertainty discussed above.
  
  The next section of our ablation study focuses on alternative forms of combining model confidence $u_*^{\text{info}}$ and consistency $u_*^{\text{cons}}$ (see Apendix~\ref{sec:sum_cocoa}). First, we consider an additive form of combining them: $\mathrm{U}_{\text{AdditiveCoCoA}} = u_*^{\text{info}} + u_*^{\text{cons}}$. The results show that this additive formulation does not perform as well compared to the multiplicative one. The additive form tends to underemphasize the interaction between the two components, which is critical for capturing the nuanced relationships between confidence and consistency. 

  We also consider an alternative formulation of the consistency term $u_*^{\text{cons}} $, as the average of the full pairwise dissimilarity. In this formulation,  $u_*^{\text{cons}}$ represents the average inconsistency across all samples rather than focusing solely on the dissimilarity of the evaluated sequence with the other samples. Our experiments demonstrate that this formulation is not very strong. By distributing the consistency computation across all samples, it loses focus on the specific sequence being evaluated.

 Lastly, in Appendix~\ref{sec:sum_cocoa}, we also consider alternative formulations of the information-based metric that do not rely on logarithmic transformations. While we primarily use logarithms due to their numerical stability, we explore an alternative approach by converting these values back to probabilities and analyzing their impact on uncertainty estimation. Our findings indicate that both formulations exhibit consistent performance and yield similar results. This suggests that while logarithmic transformations enhance numerical stability, the choice between log-based and probability-based formulations does not affect much the overall performance.

