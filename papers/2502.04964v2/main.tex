\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage[table]{xcolor}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage{placeins}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{easyeqn}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{ulem}
\usepackage{bm}
\usepackage{multirow}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}

\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}

% L

% Commands %%%%%%%%%%%%%%%%%%%%%%%
% Notations
\newcommand{\uv}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\thetav}{\bm{\theta}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\BC}{\mathcal{B}}
\newcommand{\DC}{\mathcal{D}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\HC}{\mathcal{H}}
\newcommand{\IC}{\mathcal{I}}
\newcommand{\KC}{\mathcal{K}}
\newcommand{\MC}{\mathcal{M}}
\newcommand{\YC}{\mathcal{Y}}
\newcommand{\PT}{P(True)\xspace}
\DeclareMathOperator*{\argmax}{argmax}


\icmltitlerunning{CoCoA: Uncertainty via Confidence and Consistency of LLM Outputs}

\begin{document}

\twocolumn[
\icmltitle{CoCoA: A Generalized Approach to Uncertainty Quantification\\ by Integrating Confidence and Consistency of LLM Outputs}


\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
  \icmlauthor{Roman Vashurin}{equal,mbzuai}
  \icmlauthor{Maiya Goloburda}{equal,mbzuai}
  \icmlauthor{Preslav Nakov}{mbzuai}
  \icmlauthor{Artem Shelmanov}{mbzuai}
  \icmlauthor{Maxim Panov}{mbzuai}
\end{icmlauthorlist}

\icmlaffiliation{mbzuai}{Mohamed bin Zayed University of Artificial Intelligence}
\icmlcorrespondingauthor{Roman Vashurin}{Roman.Vashurin@mbzuai.ac.ae}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution} 
\begin{abstract}
  Uncertainty quantification (UQ) methods for Large Language Models (LLMs) encompasses a variety of approaches, with two major types being particularly prominent: information-based, which focus on model confidence expressed as token probabilities, and consistency-based, which assess the semantic relationship between multiple outputs generated using repeated sampling. Several recent methods have combined these two approaches and shown impressive performance in various applications. However, they sometimes fail to outperform much simpler baseline methods. Our investigation reveals distinctive characteristics of LLMs as probabilistic models, which help to explain why these UQ methods underperform in certain tasks. Based on these findings, we propose a new way of synthesizing model confidence and output consistency that leads to a family of efficient and robust UQ methods. We evaluate our approach across a variety of tasks such as question answering, abstractive summarization, and machine translation, demonstrating sizable improvements over state-of-the-art UQ approaches.
\end{abstract}



\input{sections_introduction}

\input{sections_method}

% \input{sections_related_works}

\input{sections_experiments}

\input{sections_limitations}

\input{sections_conclusions}

\input{sections_impact_statement}

% Fix from misc to article / inproceedings 
\bibliography{custom}
\bibliographystyle{acl_natbib}


\newpage
\clearpage

\appendix
\onecolumn

\section{Decoding Strategy and Sample Selection}
\label{sec:generation_params}
  Modern LLMs are capable of producing output using a wide range of decoding strategies, and it is not readily apparent which one to use as a foundation for UQ experiments. On top of that, when sampling multiple outputs stochastically, one has to decide which sample to select for comparison with the target sequence and UQ purposes. 

  To facilitate the choice of decoding and sample selection strategies for our experiments, we conducted an evaluation of model performance with different approaches to both. Table~\ref{tab:base_quality} shows average values of corresponding quality metrics for all combinations of models and datasets. We considered 4 approaches for the selection of output that subsequently is used to calculate the quality of generation.

  \begin{itemize}
    \item \textbf{Greedy decoding} produces single output by selecting top-1 candidate token at each generation step, thus not further selection of sample is needed.

    \item \textbf{Random sample} corresponds to the case where random output is selected among the number of samples produced by repeatedly prompting the model with the same question. In practice we use first generated sample, highlighting model performance when stochastic decoding is done only once.

    \item \textbf{Best (normalized) sample} selects the output with highest model-assigned (length-normalized) probability among several sampled outputs.
  \end{itemize}

  We note that selecting a random sample from the model outputs incurs a significant drop in the quality of results on several datasets, most prominently on GSM8k. Based on these observations, we evaluate the efficacy of UE on two setups: greedy decoding and stochastic sampling with a focus on the highest-probability sample.

  In all experiments, we performed stochastic sampling with temperature $t=1.0$, top-k equal to 50, and top-p equal to $1.0$.

  \input{tables_base_quality_base_quality}

\newpage

\section{Ablation}
\label{sec:appendix_ablation}

\subsection{Average Dissimilarity as Uncertainty Measure}
\label{suppl:ave_dissim_ue}
  Tables~\ref{tab:ablation_dissim_greedy} and~\ref{tab:ablation_dissim_best} report PRRs of \texttt{CoCoA}-family methods with uncertainty estimates based solely on average dissimilarity of samples, as proposed in equation~\eqref{eq:ave_dissim} We observe that it is still widely beneficial to synthesize consistency of outputs with model confidence, even when limiting consistency evaluation to the particular sample to be scored.

  \input{tables_ablation_dissim_greedy_ablation}
  \input{tables_ablation_dissim_best_sample_ablation}


\newpage
\subsection{Choice of Similarity Function}
  \label{sec:ablation_sim_mat}
  For sample consistency estimation, one could come up with a variety of similarity functions $g(\yv, \yv')$. We perform a comparison of the effectiveness of \texttt{CoCoA}-family methods using several such functions. We consider the following functions:
  \begin{itemize}
    \item AlignScore~\cite{zha2023alignscore} with \texttt{AlignScore-large} model
    \item RougeL~\cite{lin-2004-rouge}
    \item NLI~\cite{he2021deberta} based on \texttt{microsoft/deberta-large-mnli} model
    \item CrossEncoder~\cite{liu2019robertarobustlyoptimizedbert} based on \texttt{cross-encoder/stsb-roberta-large} model.
  \end{itemize}

  Tables~\ref{suppl:ablation_sim_mat_greedy} and~\ref{suppl:ablation_sim_mat_best} report these results. There exists a considerable variation of relative effectiveness of proposed methods with various similarity function choices, depending on a task at hand. We opt to report all results in other sections with CrossEncoder-based similarity as it by itself provides a significant improvement over baselines, and for consistency and ease of comparison reasons. However, when applying these methods to a particular task, we encourage users to select appropriate underlying similarity function for best results.

\newpage

\FloatBarrier
  \input{tables_ablation_sim_mat_greedy_ablation}
\FloatBarrier

\FloatBarrier
  \newpage
  \input{tables_ablation_sim_mat_best_sample_ablation}
\FloatBarrier



\newpage

\subsection{Different Ways of Combining Confidence and Consistency}
\label{sec:sum_cocoa}

  We justify the particular form of equation~\eqref{eq:cocoa} by considering alternative ways to combine sample-focused confidence with consistency estimation. Results are presented in Tables~\ref{tab:ablation_cocoa_greedy} and~\ref{tab:ablation_cocoa_best}. In particular, we investigate the performance of the additive approach (AdditiveCoCoA):
  \begin{equation}
    \mathrm{U}_{AdditiveCoCoA} = u_*^{\text{info}} + u_*^{\text{cons}},
  \end{equation}
  and the same multiplicative combination, replacing sample-focused dissimilarity from~\eqref{eq:ave_dissim} with the average of the full pairwise dissimilarity matrix \eqref{eq:degmat}:
  \begin{equation}
    \mathrm{U}_{FullSampleCoCoA} = u_*^{\text{info}} \cdot U_{Deg}.
  \end{equation}
  %
  It is evident that on average the multiplicative form proposed in equation~\eqref{eq:cocoa} with both confidence and consistency terms focused on a single sample is the better performing variant.

  \input{tables_ablation_sum_unsup_greedy_ablation}
  \newpage
  \input{tables_ablation_sum_unsup_best_sample_ablation}


\newpage
\section{Detailed Description of Uncertainty Quantification Methods}
\label{sec:appendix_methods}
  In this section, we provide a detailed description of the uncertainty quantification methods used in this study. 

\subsection{Information-Based Methods}
\label{suppl:confidence}
  Information-based methods are commonly used to estimate uncertainty by analyzing the probability distributions of tokens within a given output. These methods examine different levels of model generation, such as the model's confidence in producing a specific sequence, its ability to predict individual tokens at each generation step, and the variability in the token-level predictions across the sequence.

  \textit{Maximum Sequence Probability (MSP)} is one of the simplest and most direct methods for estimating uncertainty. It measures the probability of the most likely output sequence given a specific input. Thus, uncertainty is quantified by calculating the probability of the sequence with the highest likelihood, under the assumption that the model is most confident in this output. It is defined as:
  \begin{equation}
    U_{MSP}(\yv \mid \xv, \thetav) = - \log P(\yv \mid \xv).
  \label{eq:msp}
  \end{equation}

  \textit{Perplexity (PPL)} is another widely used metric for estimating uncertainty in language models~\citep{fomicheva-etal-2020-unsupervised}. It measures the model's confidence by evaluating the average likelihood of generating the sequence tokens: 
  \begin{equation}
    U_\mathrm{PPL}(\yv, \xv) = -\frac{1}{L} \log P(\yv \mid \xv).
  \label{eq:ppl}
  \end{equation}
  
 \textit{Mean Token Entropy} takes a broader view of uncertainty by considering the token-level predictions across the entire sequence~\citep{fomicheva-etal-2020-unsupervised}. Instead of evaluating the model's confidence in a single output or individual token predictions, Mean Token Entropy calculates the average entropy of the token probability distributions for each token in the sequence: 
  \begin{equation}
    U_{\HC_T}(\yv, \xv) = \frac{1}{L} \sum_{l = 1}^L \HC(y_l \mid \yv_{<l}, \xv),
  \label{eq:entropy}
  \end{equation}
  where $\HC(y_l \mid \yv_{<l}, \xv)$ is an entropy of the token distribution $P(y_l \mid \yv_{<l}, \xv)$.

  The \textit{TokenSAR} method, introduced in~\cite{duan-etal-2024-shifting}, generalizes length-normalized log probability by computing a weighted average of the negative log probabilities of generated tokens, where weights are based on token relevance to the overall text. Using a similarity function $g(\cdot, \cdot)$ and token relevance function $R_T(y_k, \yv, \xv) = 1 - g(\xv \cup \yv, \xv \cup \yv \setminus y_k)$, the uncertainty estimate is calculated as:
  \begin{equation}
    U_\mathrm{TokenSAR}(\yv, \xv) = -\sum_{l = 1}^L \tilde{\mathrm{R}}_T(y_l, \yv, \xv) \log P(y_l \mid \yv_{<l}, \xv),
  \end{equation}
  where 
  \begin{equation}
  \tilde{\mathrm{R}}_T(y_k, \yv, \xv) = \frac{\mathrm{R}_T(y_k, \yv, \xv)}{\sum\nolimits_{l = 1}^L \mathrm{R}_T(y_l, \yv, \xv)}.
  \end{equation}
  %
  This measure is central for computing \textit{SAR} uncertainty measure.  


\subsection{Consistency-Based Methods}
\label{suppl:consistency}
  Consistency-based methods assess the uncertainty of a language model by evaluating the semantic consistency of its predictions across multiple outputs for the same prompt. The core idea is that semantically similar outputs indicate higher confidence, while diverse or conflicting outputs suggest greater uncertainty. Since language models can express the same meaning in different surface forms, these methods construct a semantic similarity matrix  $G = (g_{ij})$, where each entry represents the degree of similarity between pairs of responses. By clustering responses into groups with equivalent meanings, these methods provide a semantic measure of the model's consistency.

  \citet{lin2023generating} offers two similarity measures to evaluate the similarity of sequences. The first is the Jaccard similarity, which treats sequences as sets of words and calculates the proportion of shared words to the total number of unique words in both sequences: $ g(\yv, \yv') = |\yv \cap \yv'| / |\yv \cup \yv'|$.
  
  Natural Language Inference (NLI) provides another method for computing similarity between sequences. We use the DeBERTa-large NLI model~\cite{he2021deberta}, following~\citet{kuhn2023semantic}. For each pair of sequences, an NLI model predicts two probabilities: \( {p}_{\mathrm{entail}}(\yv, \yv') \), indicating entailment, and \( {p}_{\mathrm{contra}}(\yv, \yv') \), indicating contradiction. Similarity is then defined as either $g_{\mathrm{entail}}(\yv, \yv') = {p}_{\mathrm{entail}}(\yv, \yv')$ or $ g_{\mathrm{contra}}(\yv, \yv') = 1 - {p}_{\mathrm{contra}}(\yv, \yv')$.  


  Among the simplest consistency-based approaches are the \textit{Number of Semantic Sets} and the \textit{Sum of Eigenvalues of the Graph Laplacian}~\citep{lin2023generating}. \textit{Number of Semantic Sets} estimates how many distinct ``meanings'' the model produces by clustering its outputs with an NLI model. The number of semantic sets is initially equal to the total number of generated answers, \( M \). Two sentences are grouped into the same cluster if the following conditions are satisfied: $ {p}_{\text{entail}}(\yv^{i}, \yv^{j}) > {p}_{\text{contra}}(\yv^{i}, \yv^{j}) \quad \text{and} \quad {p}_{\text{entail}}(\yv^{j}, \yv^{i}) > {p}_{\text{contra}}(\yv^{j}, \yv^{i})$. This computation is performed for all pairs of answers, and the final number of distinct clusters is denoted by $ U_{\text{NumSemSets}} $.

  \textit{Sum of Eigenvalues of the Graph Laplacian} examines global diversity: it constructs a similarity matrix among the sampled outputs and computes a continuous uncertainty score from the eigenvalues of the Laplacian of that similarity graph. \citet{lin2023generating} proposes computing an averaged similarity matrix as $g_{ij} = \bigl(g\bigl(\yv^{(i)}, \yv^{(j)}\bigr) + g\bigl(\yv^{(j)}, \yv^{(i)}\bigr)\bigr) / 2$. The Laplacian for the matrix $G$ is defined as  $L = I - D^{-\frac{1}{2}} G D^{-\frac{1}{2}}$, where $D$ is a diagonal matrix with elements $D_{ii} = \sum_{j = 1}^M g_{ij}$. Consequently, the following formula is derived: 
  \begin{equation}
    U_{\text{EigV}} = \sum_{i = 1}^M \max(0, 1 - \lambda_i).
  \end{equation}

  Both \textit{Number of Semantic Sets} and \textit{Sum of Eigenvalues of the Graph Laplacian} effectively capture overall variation in generated text but cannot produce an individual uncertainty score for each output. To address this, \citet{lin2023generating} proposes to use the diagonal \textit{Degree Matrix} $D$ which represents the total similarity of each answer with all others. The corrected trace of $D$ provides an average pairwise distance between answers, and uncertainty is computed as:
  \begin{equation}
    U_{\mathrm{DegMat}} = 1 - \mathrm{trace}(D) / M^2. 
    \label{eq:degmat}
  \end{equation}
  
\subsection{Information-Based Methods with Repeated Sampling}
  In this section we detail methods that integrate model confidence with consistency.

  We can compute the entropy on the sequence level $\EE \bigl[-\log P(\yv \mid \xv)\bigr]$, where the expectation is taken over the sequences $\yv$ randomly generated from the distribution $P(\yv \mid \xv)$. Unfortunately, while for token level, we have an exact way of computing the entropy, for the sequence level, we need to adhere to some approximations. In practice, we can use Monte-Carlo integration, i.e. generate several sequences $\yv^{(i)}, \, i = 1, \dots, M$ via random sampling and compute \textit{Monte Carlo Sequence Entropy}:
  \begin{equation}
    U_{\HC_S}(\xv) = -\frac{1}{M} \sum_{i = 1}^M \log P(\yv^{(i)} \mid \xv).
  \label{eq:seq_entropy}
  \end{equation}
  %
  We can replace $P(\yv^{(i)} \mid \xv)$ with its length-normalized version $\bar{P}(\yv^{(i)} \mid \xv)$ leading to a more reliable uncertainty measure in some cases.
  
  \textit{Semantic Entropy}~\cite{kuhn2023semantic} addresses the issue of generated sequences with similar meanings but differing probabilities according to the model, which can heavily influence the resulting entropy value~\eqref{eq:seq_entropy}. The method clusters generated sequences $\yv^{(i)}, \, i = 1, \dots, M$ into semantically homogeneous groups $\CC_k, ~ k = 1, \dots, K$ (where $K \le M$) using a bi-directional entailment algorithm. Probabilities of sequences are averaged within each cluster. The entropy estimate is then defined as:
  \begin{equation}
    U_\mathrm{SE}(\xv) = -\sum_{k = 1}^K \frac{|\CC_k|}{M} \log \hat{P}_k(\xv),
  \end{equation}
  where $\hat{P}_k(\xv) = \sum_{\yv \in \CC_k} P(\yv \mid \xv)$ represents the aggregated probability for cluster $\CC_k$.

  \textit{SentenceSAR}~\cite{duan-etal-2024-shifting} enhances the probability of sentences that are more relevant.  It uses a sentence relevance measure $g\bigl(\yv^{(j)}, \yv^{(k)}\bigr)$ to evaluate the relevance of $\yv^{(j)}$ with respect to $\yv^{(k)}$. SentenceSAR is calculated as:
  
  \begin{equation}
    U_\mathrm{SentSAR}(\xv) = -\frac{1}{M} \sum_{i = 1}^M \log \Bigl(P(\yv^{(i)} \mid \xv) + \frac{1}{t} \mathrm{R}_S (\yv^{(i)}, \xv)\Bigr),
  \end{equation}
  where $t$ is a temperature parameter used to control the scale of shifting to relevance, and
  \begin{equation}
    \mathrm{R}_S (\yv^{(j)}, \xv) \! = \sum_{k \neq j} g\bigl(\yv^{(j)}, \yv^{(k)}\bigr) P\bigl(\yv^{(k)} \mid \xv \bigr).
  \end{equation}

  The combination of SentenceSAR and TokenSAR results in a unified method called \textit{SAR}~\cite{duan-etal-2024-shifting}. In this approach, the generative probability $P(\yv \mid \xv)$ in the SentenceSAR formula is replaced with the token-shifted probability $P'(\yv \mid \xv) = \exp\bigl\{-\mathrm{TokenSAR}(\yv, \xv)\bigr\}$, creating a comprehensive measure that integrates both sentence- and token-level adjustments.

\newpage

\section{Detailed Experimental Results}
\label{sec:experimental_results}
  In this section, we present detailed experimental results, which were used for computing values in Tables~\ref{tab:best_sample_results} and~\ref{tab:greedy_results}.

  \input{tables_experiments_greedy_detailed}
  \input{tables_experiments_best_sample_detailed}
  % \input{tables_experiments_random_sample}

\end{document}

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}
