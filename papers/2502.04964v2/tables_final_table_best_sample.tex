
% \begin{table*}[th!]
%     \centering
%     \renewcommand{\arraystretch}{1.2} % Adjust row height
%     \scalebox{0.85}{
%     \begin{tabular}{lccccccccc}
%     \bottomrule
%     \multirow{2}{*}{\textbf{Metric}}  & \multicolumn{3}{c}{\textbf{Llama}} & \multicolumn{3}{c}{\textbf{Mistral}} & \multicolumn{3}{c}{\textbf{Falcon}} \\  
%     \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
%     & \textbf{QA} & \textbf{NNMT} & \textbf{SUM} 
%     & \textbf{QA} & \textbf{NNMT} & \textbf{SUM}  
%     & \textbf{QA} & \textbf{NNMT} & \textbf{SUM}  \\
%     \midrule
%     $\text{MCSE}$ & 0.357 & 0.380 & 0.192 & 0.453 & 0.406 & 0.162 & 0.460 & 0.409 & 0.128 \\
% $\text{MCNSE}$ & 0.380 & 0.429 & 0.186 & 0.466 & 0.489 & 0.196 & 0.530 & 0.424 & 0.153 \\
% $\text{Semantic Entropy}$ & 0.397 & 0.411 & 0.194 & 0.482 & 0.438 & 0.164 & 0.479 & 0.440 & 0.134 \\
% $\text{SAR}$ & 0.479 & 0.506 & 0.159 & 0.542 & 0.576 & 0.175 & 0.590 & 0.488 & 0.193 \\
%     \midrule
% $\text{MSP}$ & 0.395 & 0.376 & \underline{0.464} & 0.444 & 0.252 & 0.330 & 0.343 & 0.381 & 0.099 \\
% $\text{CoCoA}_{MSP}$ & 0.484  \(\uparrow\)   & \underline{0.607}  \(\uparrow\)   & \textbf{0.484}  \(\uparrow\)   & 0.526  \(\uparrow\)   & \underline{0.721}  \(\uparrow\)   & 0.366  \(\uparrow\)   & 0.529  \(\uparrow\)   & \underline{0.631}  \(\uparrow\)   & 0.210  \(\uparrow\)   \\
%     \midrule
% $\text{Perplexity}$ & 0.532 & 0.563 & 0.458 & 0.587 & 0.686 & 0.365 & 0.627 & 0.589 & 0.275 \\
% $\text{CoCoA}_{Perplexity}$ & \textbf{0.571}  \(\uparrow\)   & \textbf{0.617}  \(\uparrow\)   & 0.450    & \textbf{0.613}  \(\uparrow\)   & \textbf{0.745}  \(\uparrow\)   & \underline{0.372}  \(\uparrow\)   & \textbf{0.647}  \(\uparrow\)   & \textbf{0.648}  \(\uparrow\)   & \textbf{0.310}  \(\uparrow\)   \\
%     \midrule
% $\text{MeanTokenEntropy}$ & 0.477 & 0.469 & 0.449 & 0.559 & 0.637 & 0.350 & 0.602 & 0.492 & 0.186 \\
% $\text{CoCoA}_{MeanTokenEntropy}$ & \underline{0.548}  \(\uparrow\)   & 0.579  \(\uparrow\)   & 0.451  \(\uparrow\)   & \underline{0.600}  \(\uparrow\)   & 0.720  \(\uparrow\)   & \textbf{0.373}  \(\uparrow\)   & \underline{0.641}  \(\uparrow\)   & 0.614  \(\uparrow\)   & \underline{0.289}  \(\uparrow\)   \\
%     \bottomrule
%     \end{tabular}}
%     \caption{Results for Evaluated Sequence - Best Sample: Mean PRR across datasets for each task. The best performing method is in bold, and the second-best is underscored. Arrows indicate improvement in CoCoA over the base version.}
%     \label{tab:best_sample_results}
%     \end{table*}


 \begin{table*}[th!]
    \centering
    \renewcommand{\arraystretch}{1.2} % Adjust row height
    %\scalebox{0.85}{
    \begin{tabular}{lccccccccc}
    \bottomrule
    \multirow{2}{*}{\textbf{Metric}} & \multicolumn{3}{c}{\textbf{Llama}} & \multicolumn{3}{c}{\textbf{Mistral}} & \multicolumn{3}{c}{\textbf{Falcon}} \\  
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
    & \textbf{QA} & \textbf{NMT} & \textbf{SUM} 
    & \textbf{QA} & \textbf{NMT} & \textbf{SUM}  
    & \textbf{QA} & \textbf{NMT} & \textbf{SUM}  \\
    \midrule
    $\text{MCSE}$ & 0.357 & 0.380 & 0.192 & 0.453 & 0.406 & 0.162 & 0.460 & 0.409 & 0.128 \\
$\text{MCNSE}$ & 0.380 & 0.429 & 0.186 & 0.466 & 0.489 & 0.196 & 0.530 & 0.424 & 0.153 \\
$\text{Semantic Entropy}$ & 0.397 & 0.411 & 0.194 & 0.482 & 0.438 & 0.164 & 0.479 & 0.440 & 0.134 \\
$\text{SAR}$ & 0.479 & 0.506 & 0.159 & 0.542 & 0.576 & 0.175 & 0.590 & 0.488 & 0.193 \\
$\text{DegMat}$ & 0.422 & 0.342 & 0.191 & 0.465 & 0.425 & 0.205 & 0.543 & 0.386 & 0.177 \\
$\text{EigValLaplacian}$ & 0.388 & 0.274 & 0.190 & 0.426 & 0.366 & 0.197 & 0.498 & 0.336 & 0.174 \\
    \midrule
$\text{MSP}$ & 0.395 & 0.376 & \underline{0.464} & 0.444 & 0.252 & 0.330 & 0.343 & 0.381 & 0.099 \\
$\text{CoCoA}_{MSP} $  & 0.484  \(\uparrow\)   & \underline{0.607}  \(\uparrow\)   & \textbf{0.484}  \(\uparrow\)   & 0.526  \(\uparrow\)   & \underline{0.721}  \(\uparrow\)   & 0.366  \(\uparrow\)   & 0.529  \(\uparrow\)   & \underline{0.631}  \(\uparrow\)   & 0.210  \(\uparrow\)   \\
    \midrule
$\text{PPL}$ & 0.532 & 0.563 & 0.458 & 0.587 & 0.686 & 0.365 & 0.627 & 0.589 & 0.275 \\
$\text{CoCoA}_{PPL} $ & \textbf{0.571}  \(\uparrow\)   & \textbf{0.617}  \(\uparrow\)   & 0.450   & \textbf{0.613}  \(\uparrow\)   & \textbf{0.745}  \(\uparrow\)   & \underline{0.372}  \(\uparrow\)   & \textbf{0.647}  \(\uparrow\)   & \textbf{0.648}  \(\uparrow\)   & \textbf{0.310}  \(\uparrow\)   \\
    \midrule
$\text{NMTE}$ & 0.477 & 0.469 & 0.449 & 0.559 & 0.637 & 0.350 & 0.602 & 0.492 & 0.186 \\
$\text{CoCoA}_{NMTE}$ & \underline{0.548}  \(\uparrow\)   & 0.579  \(\uparrow\)   & 0.451  \(\uparrow\)   & \underline{0.600}  \(\uparrow\)   & 0.720  \(\uparrow\)   & \textbf{0.373}  \(\uparrow\)   & \underline{0.641}  \(\uparrow\)   & 0.614  \(\uparrow\)   & \underline{0.289}  \(\uparrow\)   \\
    \bottomrule
    \end{tabular}%}
    \caption{Results for Evaluated Sequence -- Best Sample: Mean PRR across datasets for each task. The best performing method is in bold, and the second-best is underscored. Arrows indicate improvement in \texttt{CoCoA} over the base version.}
    \label{tab:best_sample_results}
    \end{table*}    

    