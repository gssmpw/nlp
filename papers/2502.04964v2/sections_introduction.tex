% !TEX root = ../main.tex

\section{Introduction}
\label{sec:intro}
  Large Language Models (LLMs) have revolutionized natural language processing (NLP), enabling advancements in information retrieval, question answering, machine translation, and other language-driven applications. As these models become an integral part of everyday life, ensuring the reliability of their outputs is crucial, especially in high-stakes scenarios where errors or uncertainty can have serious consequences. One way to address this challenge is through uncertainty quantification (UQ), which measures how confident a model is in its outputs, and makes possible the rejection of generations with a high risk of being incorrect.

  UQ for LLMs is a rapidly advancing research area, with new methods for estimating uncertainty emerging each year. The large portion of novel techniques is based on two fundamental approaches: information-theoretic analysis and the assessment of output consistency.

  \begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{img_inconsistent_probabilities_final.pdf}
    \caption{Example of inconsistent probabilities assigned to semantically identical answers by an LLM, demonstrating the limitation of relying solely on sequence-level information.}
  \label{fig:inconsistent_probability}
  \end{figure}
  
  Information-theoretic methods quantify the confidence of a model by analyzing the probability distributions it induces for predictions~\citep{malinin2020uncertainty, fomicheva-etal-2020-unsupervised}. A key limitation of these methods is that they cannot account for the semantic variability across multiple possible outputs for the same input. Specifically, the model may generate answers with the same meaning but with very different assigned probabilities; see Figure~\ref{fig:inconsistent_probability}. LLMs are trained to predict the next token in a sequence based on patterns observed in vast amounts of data, resulting in varying probabilities for semantically equivalent output sequences.
 
  In contrast, consistency-based methods directly analyze the semantic relationships between the sampled outputs~\citep{lin2023generating, fomicheva-etal-2020-unsupervised}. Information-theoretic and consistency-based methods have complementary strengths: the former provides insights into the model's internal confidence, while the latter captures the uncertainty as objective variability of meaning among sampled outputs. For this reason, recent state-of-the-art methods aimed to unify these approaches~\citep{kuhn2023semantic, duan-etal-2024-shifting}. Although such methods show good performance in various applications, they sometimes fail to outperform their simpler counterparts in certain scenarios~\citep{vashurin2024benchmarkinguncertaintyquantificationmethods}.

  Our investigation revealed distinctive characteristics of LLMs as probabilistic models, shedding light on why current state-of-the-art UQ methods that attempt to integrate both approaches often underperform in certain tasks. Specifically, we highlight the complexity of the token prediction process and the absence of a unified framework that simultaneously addresses model confidence and output variability, both of which can limit the effectiveness of existing UQ techniques. This insight drives our proposal for a novel family of methods that integrate model confidence with output consistency, resulting in more efficient and robust UQ techniques. Our approach combines the strengths of both information-based and consistency-based methods, providing a more comprehensive and accurate assessment of uncertainty.

  Our main contributions can be summarized as follows.
  \begin{itemize}
    \item We identify key limitations in current UQ methods for LLMs, particularly in addressing both token- and sequence-level confidence and output consistency.

    \item We present a family of \textit{\underline{Co}nfidence and \underline{Co}nsistency-based \underline{A}pproaches} (\texttt{CoCoA}) to UQ, offering a new way to merge information- and consistency-based measures for uncertainty quantification in LLMs.
        
    \item We evaluate our approaches across a variety of NLP tasks, including question answering, summarization, and translation. Our experiments demonstrate sizable improvements in the reliability and robustness of UQ compared to state-of-the-art methods.
  \end{itemize}
