@article{jaccard1912distribution,
  title={The distribution of the flora in the alpine zone. 1},
  author={Jaccard, Paul},
  journal={New phytologist},
  volume={11},
  number={2},
  pages={37--50},
  year={1912},
  publisher={Wiley Online Library}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}
@inproceedings{chen2024transformers,
  title={How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression},
  author={Chen, Xingwu and Zhao, Lei and Zou, Difan},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024}
}
@inproceedings{chen2024can,
  title={What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks},
  author={Chen, Xingwu and Zou, Difan},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}
@misc{rai2024practicalreviewmechanisticinterpretability,
      title={A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models}, 
      author={Daking Rai and Yilun Zhou and Shi Feng and Abulhair Saparov and Ziyu Yao},
      year={2024},
      eprint={2407.02646},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.02646}, 
}

@misc{ferrando2024primerinnerworkingstransformerbased,
      title={A Primer on the Inner Workings of Transformer-based Language Models}, 
      author={Javier Ferrando and Gabriele Sarti and Arianna Bisazza and Marta R. Costa-jussà},
      year={2024},
      eprint={2405.00208},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.00208}, 
}

@misc{zhao2024surveylargelanguagemodels,
      title={A Survey of Large Language Models}, 
      author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
      year={2024},
      eprint={2303.18223},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.18223}, 
}

@inproceedings{NEURIPS2021_4f5c422f,
 author = {Geiger, Atticus and Lu, Hanson and Icard, Thomas and Potts, Christopher},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {9574--9586},
 publisher = {Curran Associates, Inc.},
 title = {Causal Abstractions of Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/4f5c422f4d49a5a807eda27434231040-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{wang2024knowledgemechanismslargelanguage,
  title={Knowledge Mechanisms in Large Language Models: A Survey and Perspective},
  author={Mengru Wang and Yunzhi Yao and Ziwen Xu and Shuofei Qiao and Shumin Deng and Peng Wang and Xiang Chen and Jia-Chen Gu and Yong Jiang and Pengjun Xie and Fei Huang and Huajun Chen and Ningyu Zhang},
  booktitle={Proceedings of EMNLP 2024 Findings},
  year={2024},
  pages={1--39},  
  url={https://arxiv.org/abs/2407.15017},
  note={EMNLP 2024 Findings; 39 pages (v4)}
}

@INPROCEEDINGS{10136140,
  author={Räuker, Tilman and Ho, Anson and Casper, Stephen and Hadfield-Menell, Dylan},
  booktitle={2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)}, 
  title={Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks}, 
  year={2023},
  volume={},
  number={},
  pages={464-483},
  keywords={Surveys;Deep learning;Training;Neural networks;Taxonomy;Reverse engineering;Neurons;interpretability;explainability;transparency},
  doi={10.1109/SaTML54575.2023.00039}}

@article{10.1145/3639372,
author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
title = {Explainability for Large Language Models: A Survey},
year = {2024},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {20},
numpages = {38},
keywords = {Explainability, interpretability, large language models}
}

@misc{wu2024usablexai10strategies,
      title={Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era}, 
      author={Xuansheng Wu and Haiyan Zhao and Yaochen Zhu and Yucheng Shi and Fan Yang and Tianming Liu and Xiaoming Zhai and Wenlin Yao and Jundong Li and Mengnan Du and Ninghao Liu},
      year={2024},
      eprint={2403.08946},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.08946}, 
}

@article{bereska2024mechanisticinterpretabilityaisafety,
  title={Mechanistic Interpretability for AI Safety -- A Review},
  author={Leonard Bereska and Efstratios Gavves},
  journal={Transactions on Machine Learning Research (TMLR)},
  year={2024},
  url={https://arxiv.org/abs/2404.14082},
  note={Accepted to TMLR}
}

@inproceedings{friedman2024interpretabilityillusionsgeneralizationsimplified,
  title={Interpretability Illusions in the Generalization of Simplified Models},
  author={Dan Friedman and Andrew Lampinen and Lucas Dixon and Danqi Chen and Asma Ghandeharioun},
  booktitle={Proceedings of the 41st International Conference on Machine Learning (ICML)},
  year={2024},
  url={https://arxiv.org/abs/2312.03656},
  note={ICML 2024}
}


@inproceedings{sun2024learningunlearningfabricatedknowledge,
  title={Learning and Unlearning of Fabricated Knowledge in Language Models},
  author={Chen Sun and Nolan Andrew Miller and Andrey Zhmoginov and Max Vladymyrov and Mark Sandler},
  booktitle={Proceedings of the ICML 2024 Workshop on Mechanistic Interpretability},
  year={2024},
  url={https://arxiv.org/abs/2410.21750},
  note={ICML 2024 Workshop on Mechanistic Interpretability}
}

@misc{lee2024mechanisticunderstandingalignmentalgorithms,
      title={A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity}, 
      author={Andrew Lee and Xiaoyan Bai and Itamar Pres and Martin Wattenberg and Jonathan K. Kummerfeld and Rada Mihalcea},
      year={2024},
      eprint={2401.01967},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.01967}, 
}

@inproceedings{zhang2024truthxalleviatinghallucinationsediting,
  title={TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space},
  author={Shaolei Zhang and Tian Yu and Yang Feng},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2024},
  url={https://arxiv.org/abs/2402.17811},
  note={ACL 2024 Main Conference}
}

@inproceedings{NEURIPS2023_81b83900,
 author = {Li, Kenneth and Patel, Oam and Vi\'{e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {41451--41530},
 publisher = {Curran Associates, Inc.},
 title = {Inference-Time Intervention: Eliciting Truthful Answers from a Language Model},
 volume = {36},
 year = {2023}
}

@article{cohen2024evaluating,
  title={Evaluating the ripple effects of knowledge editing in language models},
  author={Cohen, Roi and Biran, Eden and Yoran, Ori and Globerson, Amir and Geva, Mor},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={283--298},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@misc{meng2023masseditingmemorytransformer,
      title={Mass-Editing Memory in a Transformer}, 
      author={Kevin Meng and Arnab Sen Sharma and Alex Andonian and Yonatan Belinkov and David Bau},
      year={2023},
      eprint={2210.07229},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.07229}, 
}

@inproceedings{koh2024faithful,
  title={Faithful and Fast Influence Function via Advanced Sampling},
  author={Koh, Jungyeon and Lyu, Hyeonsu and Jang, Jonggyu and Yang, Hyun Jong},
  booktitle={ICML 2024 Workshop on Mechanistic Interpretability}
}

-------------------------------------------------------------
@inproceedings{NEURIPS2022_6f1d43d5,
 author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {17359--17372},
 publisher = {Curran Associates, Inc.},
 title = {Locating and Editing Factual Associations in GPT},
 volume = {35},
 year = {2022}
}

@misc{vig2020causalmediationanalysisinterpreting,
      title={Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias}, 
      author={Jesse Vig and Sebastian Gehrmann and Yonatan Belinkov and Sharon Qian and Daniel Nevo and Simas Sakenis and Jason Huang and Yaron Singer and Stuart Shieber},
      year={2020},
      eprint={2004.12265},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.12265}, 
}

@inproceedings{NEURIPS2023_3927bbdc,
 author = {Hase, Peter and Bansal, Mohit and Kim, Been and Ghandeharioun, Asma},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {17643--17668},
 publisher = {Curran Associates, Inc.},
 title = {Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models},
 volume = {36},
 year = {2023}
}

@inproceedings{NEURIPS2023_34e1dbe9,
 author = {Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri\`{a}},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {16318--16352},
 publisher = {Curran Associates, Inc.},
 title = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
 volume = {36},
 year = {2023}
}

@inproceedings{syed2023attributionpatchingoutperformsautomated,
  title={Attribution Patching Outperforms Automated Circuit Discovery},
  author={Aaquib Syed and Can Rager and Arthur Conmy},
  booktitle={Proceedings of the NeurIPS 2023 ATTRIB Workshop},
  year={2023},
  url={https://arxiv.org/abs/2310.10348},
  note={NeurIPS 2023 ATTRIB Workshop}
}


@inproceedings{bhaskar2024findingtransformercircuitsedge,
  title={Finding Transformer Circuits with Edge Pruning},
  author={Adithya Bhaskar and Alexander Wettig and Dan Friedman and Danqi Chen},
  booktitle={Proceedings of the 38th Conference on Neural Information Processing Systems (NeurIPS), Spotlight},
  year={2024},
  url={https://arxiv.org/abs/2406.16778},
  note={NeurIPS 2024 Spotlight}
}

@inproceedings{hanna2024faithfaithfulnessgoingcircuit,
  title={Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms},
  author={Michael Hanna and Sandro Pezzelle and Yonatan Belinkov},
  booktitle={Proceedings of the Conference on Learning Mechanisms (COLM)},
  year={2024},
  url={https://arxiv.org/abs/2403.17806},
  note={COLM 2024}
}
-------------------------------------------------------------
@article{elhage2021mathematical,
  title={A mathematical framework for transformer circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and others},
  journal={Transformer Circuits Thread},
  volume={1},
  number={1},
  pages={12},
  year={2021}
}

@inproceedings{tigges2024llmcircuitanalysesconsistent,
  title={LLM Circuit Analyses Are Consistent Across Training and Scale},
  author={Curt Tigges and Michael Hanna and Qinan Yu and Stella Biderman},
  booktitle={Proceedings of the 38th Conference on Neural Information Processing Systems (NeurIPS)},
  year={2024},
  url={https://arxiv.org/abs/2407.10827},
  note={NeurIPS 2024}
}

@article{heimersheim2023circuit,
  title={A circuit for Python docstrings in a 4-layer attention-only transformer},
  author={Heimersheim, Stefan and Janiak, Jett},
  journal={URL: https://www. alignmentforum. org/posts/u6KXXmKFbXfWzoAXn/acircuit-for-python-docstrings-in-a-4-layer-attention-only},
  year={2023}
}

@inproceedings{merullo2024circuitcomponentreusetasks,
  title={Circuit Component Reuse Across Tasks in Transformer Language Models},
  author={Jack Merullo and Carsten Eickhoff and Ellie Pavlick},
  booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://arxiv.org/abs/2310.08744},
  note={ICLR 2024}
}


@inproceedings{burns2024discoveringlatentknowledgelanguage,
  title={Discovering Latent Knowledge in Language Models Without Supervision},
  author={Collin Burns and Haotian Ye and Dan Klein and Jacob Steinhardt},
  booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2023},
  url={https://arxiv.org/abs/2212.03827},
  note={ICLR 2023}
}

@inproceedings{geva2021transformerfeedforwardlayerskeyvalue,
  title={Transformer Feed-Forward Layers Are Key-Value Memories},
  author={Mor Geva and Roei Schuster and Jonathan Berant and Omer Levy},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2021},
  url={https://arxiv.org/abs/2012.14913},
  note={EMNLP 2021}
}


@inproceedings{geva2022transformerfeedforwardlayersbuild,
  title={Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space},
  author={Mor Geva and Avi Caciularu and Kevin Ro Wang and Yoav Goldberg},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2022},
  url={https://arxiv.org/abs/2203.14680},
  note={EMNLP 2022}
}

@misc{bhattacharya2024understandingroleffnsdriving,
      title={Understanding the role of FFNs in driving multilingual behaviour in LLMs}, 
      author={Sunit Bhattacharya and Ondřej Bojar},
      year={2024},
      eprint={2404.13855},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.13855}, 
}

@misc{wu2024retrievalheadmechanisticallyexplains,
      title={Retrieval Head Mechanistically Explains Long-Context Factuality}, 
      author={Wenhao Wu and Yizhong Wang and Guangxuan Xiao and Hao Peng and Yao Fu},
      year={2024},
      eprint={2404.15574},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.15574}, 
}

@misc{mcdougall2023copysuppressioncomprehensivelyunderstanding,
      title={Copy Suppression: Comprehensively Understanding an Attention Head}, 
      author={Callum McDougall and Arthur Conmy and Cody Rushing and Thomas McGrath and Neel Nanda},
      year={2023},
      eprint={2310.04625},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.04625}, 
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@misc{gould2023successorheadsrecurringinterpretable,
      title={Successor Heads: Recurring, Interpretable Attention Heads In The Wild}, 
      author={Rhys Gould and Euan Ong and George Ogden and Arthur Conmy},
      year={2023},
      eprint={2312.09230},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.09230}, 
}

@misc{cabannes2024iterationheadmechanisticstudy,
      title={Iteration Head: A Mechanistic Study of Chain-of-Thought}, 
      author={Vivien Cabannes and Charles Arnal and Wassim Bouaziz and Alice Yang and Francois Charton and Julia Kempe},
      year={2024},
      eprint={2406.02128},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.02128}, 
}

@inproceedings{geva2023dissectingrecallfactualassociations,
  title={Dissecting Recall of Factual Associations in Auto-Regressive Language Models},
  author={Mor Geva and Jasmijn Bastings and Katja Filippova and Amir Globerson},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023},
  url={https://arxiv.org/abs/2304.14767},
  note={EMNLP 2023}
}

@inproceedings{stolfo2023mechanisticinterpretationarithmeticreasoning,
  title={A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis},
  author={Alessandro Stolfo and Yonatan Belinkov and Mrinmaya Sachan},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023},
  url={https://arxiv.org/abs/2305.15054},
  note={EMNLP 2023}
}

@inproceedings{NEURIPS2023_efbba771,
 author = {Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {76033--76060},
 publisher = {Curran Associates, Inc.},
 title = {How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model},
 volume = {36},
 year = {2023}
}


@misc{wang2022interpretabilitywildcircuitindirect,
      title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small}, 
      author={Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
      year={2022},
      eprint={2211.00593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.00593}, 
}

-------------------------------------------------------------
@misc{nikankin2024arithmeticalgorithmslanguagemodels,
      title={Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics}, 
      author={Yaniv Nikankin and Anja Reusch and Aaron Mueller and Yonatan Belinkov},
      year={2024},
      eprint={2410.21272},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21272}, 
}

@inproceedings{prakash2024finetuningenhancesexistingmechanisms,
  title={Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking},
  author={Nikhil Prakash and Tamar Rott Shaham and Tal Haklay and Yonatan Belinkov and David Bau},
  booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://arxiv.org/abs/2402.14811},
  note={ICLR 2024}
}

@inproceedings{chhabra2024neuroplasticity,
  title={Neuroplasticity and Corruption in Model Mechanisms: A Case Study of Indirect Object Identification},
  author={Chhabra, Vishnu Kabir and Zhu, Ding and Khalili, Mohammad Mahdi},
  booktitle={Proceedings of the ICML 2024 Workshop on Mechanistic Interpretability},
  year={2024},
  note={ICML 2024 Workshop on Mechanistic Interpretability}
}

@inproceedings{yu2024interpretingarithmeticmechanismlarge,
  title={Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis},
  author={Zeping Yu and Sophia Ananiadou},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2024},
  url={https://arxiv.org/abs/2409.14144},
  note={EMNLP 2024 Main Conference}
}

@inproceedings{ghosal2024understandingfinetuningfactualknowledge,
  title={Understanding Finetuning for Factual Knowledge Extraction},
  author={Gaurav Ghosal and Tatsunori Hashimoto and Aditi Raghunathan},
  booktitle={Proceedings of the 41st International Conference on Machine Learning (ICML)},
  year={2024},
  url={https://arxiv.org/abs/2406.14785},
  note={ICML 2024}
}


@misc{jain2024mechanisticallyanalyzingeffectsfinetuning,
      title={Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks}, 
      author={Samyak Jain and Robert Kirk and Ekdeep Singh Lubana and Robert P. Dick and Hidenori Tanaka and Edward Grefenstette and Tim Rocktäschel and David Scott Krueger},
      year={2024},
      eprint={2311.12786},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.12786}, 
}

@misc{jain2024makesbreakssafetyfinetuning,
      title={What Makes and Breaks Safety Fine-tuning? A Mechanistic Study}, 
      author={Samyak Jain and Ekdeep Singh Lubana and Kemal Oksuz and Tom Joy and Philip H. S. Torr and Amartya Sanyal and Puneet K. Dokania},
      year={2024},
      eprint={2407.10264},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.10264}, 
}

@misc{grosse2023studyinglargelanguagemodel,
      title={Studying Large Language Model Generalization with Influence Functions}, 
      author={Roger Grosse and Juhan Bae and Cem Anil and Nelson Elhage and Alex Tamkin and Amirhossein Tajdini and Benoit Steiner and Dustin Li and Esin Durmus and Ethan Perez and Evan Hubinger and Kamilė Lukošiūtė and Karina Nguyen and Nicholas Joseph and Sam McCandlish and Jared Kaplan and Samuel R. Bowman},
      year={2023},
      eprint={2308.03296},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2308.03296}, 
}

@inproceedings{alabi2024hiddenspacetransformerlanguage,
  title={The Hidden Space of Transformer Language Adapters},
  author={Jesujoba O. Alabi and Marius Mosbach and Matan Eyal and Dietrich Klakow and Mor Geva},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2024},
  url={https://arxiv.org/abs/2402.13137},
  note={ACL 2024 Main Conference}
}

@inproceedings{hong2024dissectingfinetuningunlearninglarge,
  title={Dissecting Fine-Tuning Unlearning in Large Language Models},
  author={Yihuai Hong and Yuelin Zou and Lijie Hu and Ziqian Zeng and Di Wang and Haiqin Yang},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2024},
  url={https://arxiv.org/abs/2410.06606},
  note={EMNLP 2024 Main Conference}
}

-------------------------------------------------------------

@article{ding2023parameter,
  title={Parameter-efficient fine-tuning of large-scale pre-trained language models},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={Nature Machine Intelligence},
  volume={5},
  number={3},
  pages={220--235},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}

@inproceedings{kumar2022finetuningdistortpretrainedfeatures,
  title={Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},
  author={Ananya Kumar and Aditi Raghunathan and Robbie Jones and Tengyu Ma and Percy Liang},
  booktitle={Proceedings of the 10th International Conference on Learning Representations (ICLR)},
  year={2022},
  url={https://arxiv.org/abs/2202.10054},
  note={ICLR 2022 Oral}
}

@inproceedings{NEURIPS2023_ac662d74,
 author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and YU, LILI and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {55006--55021},
 publisher = {Curran Associates, Inc.},
 title = {LIMA: Less Is More for Alignment},
 volume = {36},
 year = {2023}
}

@inproceedings{panigrahi2023task,
  title={Task-specific skill localization in fine-tuned language models},
  author={Panigrahi, Abhishek and Saunshi, Nikunj and Zhao, Haoyu and Arora, Sanjeev},
  booktitle={International Conference on Machine Learning},
  pages={27011--27033},
  year={2023},
  organization={PMLR}
}


@misc{lialin2024scalingscaleupguide,
      title={Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning}, 
      author={Vladislav Lialin and Vijeta Deshpande and Xiaowei Yao and Anna Rumshisky},
      year={2024},
      eprint={2303.15647},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.15647}, 
}

@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}


@misc{zhou2024loradropefficientloraparameter,
      title={LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation}, 
      author={Hongyun Zhou and Xiangyu Lu and Wang Xu and Conghui Zhu and Tiejun Zhao and Muyun Yang},
      year={2024},
      eprint={2402.07721},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.07721}, 
}

@inproceedings{zhang2023adaloraadaptivebudgetallocation,
  title={AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning},
  author={Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
  booktitle={Proceedings of the 11th International Conference on Learning Representations (ICLR)},
  year={2023},
  url={https://arxiv.org/abs/2303.10512},
  note={ICLR 2023}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@article{black2021gpt,
  title={Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow},
  author={Black, Sid and Gao, Leo and Wang, Phil and Leahy, Connor and Biderman, Stella},
  journal={If you use this software, please cite it using these metadata},
  volume={58},
  number={2},
  year={2021}
}

@misc{zhang2022optopenpretrainedtransformer,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.01068}, 
}

