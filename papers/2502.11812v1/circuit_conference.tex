\documentclass{article}
\usepackage{circuit_conference,times}

\input{math_commands.tex}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{multirow}
% \usepackage[dvipsnames]{xcolor}
\usepackage{colortbl}
\definecolor{CustomSkyBlue}{RGB}{109, 163, 209} 
\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm}    
\usepackage{algorithmic}  
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage[textsize=tiny]{todonotes}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\sectsymbol}{\S} % Section symbol

\title{Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis}
\author{%
Xu Wang$^{1 }$, Yan Hu$^2$, Wenyu Du$^1$, Reynold Cheng$^1$, Benyou Wang$^2$, Difan Zou$^1$ \\
$^1$The University of Hong Kong \\
$^2$The Chinese University of Hong Kong, Shenzhen \\
}

\begin{document}
\maketitle

\begin{abstract}
Fine-tuning significantly improves the performance of Large Language Models (LLMs), yet its underlying mechanisms remain poorly understood. This paper aims to provide an in-depth interpretation of the fine-tuning process through circuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike previous studies \citep{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} that focus on tasks where pre-trained models already perform well, we develop a set of mathematical tasks where fine-tuning yields substantial performance gains, which are closer to the practical setting. In our experiments, we identify circuits at various checkpoints during fine-tuning and examine the interplay between circuit analysis, fine-tuning methods, and task complexities. First, we find that while circuits maintain high node similarity before and after fine-tuning, their edges undergo significant changes, which is in contrast to the previous work \citep{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity} that show circuits only add some additional components after fine-tuning. Based on these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method, which assigns ranks to layers based on edge changes in the circuits. Experimental results demonstrate that our circuit-based LoRA algorithm achieves an average performance improvement of 2.46\% over standard LoRA with similar parameter sizes. Furthermore, we explore how combining circuits from subtasks can enhance fine-tuning in compositional tasks, providing new insights into the design of such tasks and deepening the understanding of circuit dynamics and fine-tuning mechanisms. 
\end{abstract}

\section{Introduction}
Mechanistic Interpretability (MI) has become a powerful approach for exploring the inner workings of machine learning models, particularly Large Language Models (LLMs) \cite{rai2024practicalreviewmechanisticinterpretability}. It provides valuable insights into how information flows and transforms across different layers \cite{ferrando2024primerinnerworkingstransformerbased}. One of the most critical aspects of deploying LLMs in real-world scenarios is fine-tuning \cite{chung2024scaling}. However, the interpretability of how pre-trained models improve during fine-tuning remains limited, and the underlying mechanisms enabling their success across tasks require further investigation.

Many studies in MI regard models as computational graphs \cite{NEURIPS2021_4f5c422f}, where circuits are specific subgraphs that perform identifiable functions \cite{wang2022interpretabilitywildcircuitindirect}. Notably, this framework has been successfully applied to various LLMs, revealing emergent behaviors within attention heads and Multi-Layer Perceptrons (MLPs) \cite{heimersheim2023circuit,burns2024discoveringlatentknowledgelanguage, NEURIPS2023_efbba771, gould2023successorheadsrecurringinterpretable}. Moreover, circuits have recently been leveraged to investigate the fine-tuning process of language models, seeking to understand the mechanisms behind fine-tuning \cite{prakash2024finetuningenhancesexistingmechanisms, chhabra2024neuroplasticity,jain2024mechanisticallyanalyzingeffectsfinetuning}. However, these studies often focus on tasks where pre-trained models already perform well (e.g., GPT-2~\cite{radford2019language} achieves around 98\% accuracy on the IOI task), or they use general data for fine-tuning rather than domain-specific datasets \cite{prakash2024finetuningenhancesexistingmechanisms}. Under such conditions, fine-tuning mainly enhances existing mechanisms (e.g., by adding some attention heads). Consequently, their arguments may not be applicable in more practical fine-tuning scenarios where models initially perform poorly and require fine-tuning on domain data.

To better understand fine-tuning mechanisms in practical settings, it is crucial to focus on tasks where fine-tuning leads to performance improvements. In this work, we design a class of mathematical tasks on which pre-trained large language models initially perform poorly with low accuracy, yet demonstrates a performance boost after fine-tuned. We employ the \emph{Edge Attribution Patching with Integrated Gradients (EAP-IG)}~\cite{hanna2024faithfaithfulnessgoingcircuit} method to identify circuits within both pre-trained and fine-tuned models. Surprisingly, we observe that this approach consistently finds circuits with high faithfulness, even though the two models differ markedly in performance (see \S\ref{sec:circuittask}). To further validate the stability of the discovered circuits, we introduce another circuit metric, \textit{robustness}, which measures the stability of identified circuits by assessing their edge similarity under different perturbation ratios of the dataset. We show that when compared with a randomly initialized transformer model, the pre-trained model, despite exhibiting very low prediction accuracy, can still achieve substantially higher robustness.  This finding further supports the validity of the circuits discovered during the fine-tuning process, irrespective of their prediction performance.

\textbf{Our Main Findings.}
Based on the circuits analysis techniques and tasks introduced in \S\ref{sec:circuittask}, we provide a comprehensive interpretation of the key factors in the fine-tuning process. Specifically, we focus on three central research questions and summarize our main observations as follows:

\begin{enumerate}[nosep,leftmargin=*]
    \item \textbf{(\S\ref{evolve_part}) How do circuits evolve during the fine-tuning process?} We use pythia-1.4B-deduped~\cite{biderman2023pythia}, gpt-neo-2.7B~\cite{black2021gpt}, opt-6.7B~\cite{zhang2022optopenpretrainedtransformer} to fine-tune on five math tasks. By extracting the circuits at each stage of the model during fine-tuning and analyzing these circuits, the circuits identified by EAP-IG demonstrate high fidelity in both pre-trained and fine-tuned models, despite significant performance differences. \textit{We observe that during fine-tuning, circuits gradually converge as modifications to nodes and edges decrease. Meanwhile, new circuits emerge after fine-tuning, with edge changes playing a more significant role in this process.}
    \vspace{2.5mm}
    \item \textbf{(\S\ref{Circuitlora}) Can circuit insights enhance the fine-tuning process?} We develop a circuit-aware Low-Rank Adaptation (LoRA) method, which assigns higher ranks to layers that have more edge changes in the circuits. We demonstrate across five different mathematical tasks that using circuit insights to optimize the fine-tuning algorithm is effective, significantly improving LoRA's accuracy and parameter efficiency. \textit{Our experiments highlight how Mechanistic Interpretability enhances fine-tuning efficiency, improving performance with fewer parameters using circuit change insights.}
    \vspace{2.5mm}
    \item \textbf{(\S\ref{compositional_part}) How capable is the Union Circuit in performing compositional tasks?} To validate our hypothesis, we design a two-step compositional task, such as "(61 - 45) * 45 =". This compositional task was decomposed into an addition/subtraction task and a multiplication/division task and we use the union of the circuits from these subtasks to approximate the circuit for the compositional task. \textit{Our results indicate that the circuit for the combination task can be approximated by the union of subtask circuits, enhancing the model's performance on the combination task during fine-tuning.}
\end{enumerate}

\section{Related work}
\subsection{Mechanistic Interpretability}
Mechanistic Interpretability investigates how components in large language models process and represent information~\cite{wang2024knowledgemechanismslargelanguage}.
At present, many MI studies have been applied in various fields of AI Safety. For instance, oversimplified probes risk~\cite{friedman2024interpretabilityillusionsgeneralizationsimplified}, unlearning fabricated knowledge~\cite{sun2024learningunlearningfabricatedknowledge}, reducing toxicity via alignment~\cite{lee2024mechanisticunderstandingalignmentalgorithms}, mitigating hallucinations by editing representations~\cite{zhang2024truthxalleviatinghallucinationsediting}, and generating truthful outputs through inference-time interventions~\cite{NEURIPS2023_81b83900}. Other studies explore how local model edits propagate across tasks~\cite{cohen2024evaluating,meng2023masseditingmemorytransformer}, Multi-Head Attention in-context learning~\cite{chen2024transformers,chen2024can} and enhance influence-function sampling~\cite{koh2024faithful}. Specifically, our study examines how circuits evolve during fine-tuning for mathematical tasks, focusing on node and edge changes to reveal mechanisms behind performance improvements.

\subsection{Circuit Analysis and Fine-Tuning}
One direction of Circuit Analysis focuses on building complete circuits. Early work localizes factual associations in mid-layer modules~\cite{NEURIPS2022_6f1d43d5} and uses causal mediation to uncover biases~\cite{vig2020causalmediationanalysisinterpreting,NEURIPS2023_3927bbdc}. Automated methods like Automated Circuit Discovery identify significant units~\cite{NEURIPS2023_34e1dbe9}, while techniques like attribution patching, and refine circuit extraction by handling near-zero gradients~\cite{syed2023attributionpatchingoutperformsautomated,hanna2024faithfaithfulnessgoingcircuit}. Edge pruning~\cite{bhaskar2024findingtransformercircuitsedge} provide insights into building the edge of the circuit. Another line of research investigates the functional roles of circuit components, such as Attention heads~\cite{wu2024retrievalheadmechanisticallyexplains, mcdougall2023copysuppressioncomprehensivelyunderstanding, olsson2022context, gould2023successorheadsrecurringinterpretable, cabannes2024iterationheadmechanisticstudy} and Feed Forward Networks (FFNs) / MLPs~\cite{geva2021transformerfeedforwardlayerskeyvalue, geva2022transformerfeedforwardlayersbuild, bhattacharya2024understandingroleffnsdriving}. 
Additionally, circuits have been used to analyze specific tasks, such as factual knowledge retrieval~\cite{geva2023dissectingrecallfactualassociations}, arithmetic computation~\cite{stolfo2023mechanisticinterpretationarithmeticreasoning}, Greater Than task~\cite{NEURIPS2023_efbba771}, and circuit recognition in Indirect Object Identification~\cite{wang2022interpretabilitywildcircuitindirect}. Unlike these analyses, which focus on smaller-scale tasks and models, our work offers a new lens on how circuits evolve specifically during fine-tuning on mathematical tasks, revealing crucial roles of edge changes.

As pre-trained language models scale, fine-tuning methods have emerged, optimizing only a small subset of parameters~\cite{ding2023parameter}. Parameter-efficient fine-tuning (PEFT) methods, such as LoRA~\cite{hu2021lora}, reduce computational costs while preserving functionality~\cite{ding2023parameter}. Advances in LoRA, including pruning~\cite{zhou2024loradropefficientloraparameter} and adaptive budget allocation~\cite{zhang2023adaloraadaptivebudgetallocation, liu2022few, lialin2024scalingscaleupguide}, further improve efficiency. In our study, we introduce a circuit-aware LoRA approach that adaptively assigns higher ranks to layers with more edge changes, boosting efficiency and accuracy in mathematical tasks, and further illustrates how combining circuits from subtasks can enhance performance in compositional tasks during fine-tuning.

\section{Circuit Discovery and Task Design}
\label{sec:circuittask}

%%%%%%%%%%%%% SECTION 3 %%%%%%%%%%%%%

\subsection{Circuit Discovery: EAP-IG}
\label{eap}
Attribution patching is a technique for identifying circuits using two forward passes and one backward pass~\cite{syed2023attributionpatchingoutperformsautomated}. In our experiments, we use Edge Attribution Patching with Integrated Gradients (EAP-IG)~\cite{hanna2024faithfaithfulnessgoingcircuit}, which addresses computational inefficiency in large models and resolves zero-gradient issues with KL divergence. EAP-IG computes importance scores by integrating gradients along the path between clean and corrupted activations, making it our method of choice. The formula for scoring each edge is:
\[
\Delta L(E) \approx 
\bigg| 
(\mathbf{e}_{\text{corr}} - \mathbf{e}_{\text{clean}})^\top 
\frac{1}{m} \sum_{k=1}^m 
\nabla_{\mathbf{e}_{k}} L(x) 
\bigg|,
\]
where $\mathbf{e}_{\text{clean}}$ and $\mathbf{e}_{\text{corr}}$ denote the activations in the circuit under the clean and corrupted inputs, respectively. $m$ is the total number of interpolation steps, and $k$ represents the index of a specific step.  $\nabla_{\mathbf{e}_{k}} L(x)$ denotes the gradient of the loss function $L(x)$ with respect to the interpolated activations $\mathbf{e}_{k}$.  

In this study, we choose $m = 5$ based on Hanna et al.’s (2024) recommendations~\cite{hanna2024faithfaithfulnessgoingcircuit}. 

\subsection{Circuit Evalutaion: Faithfulness and Robustness}
\label{sec:Robustness Calculation}
\textbf{Faithfulness.} Faithfulness serves as a key metric to evaluate the reliability of circuits discovered in MI and it quantifies how closely a circuit replicates the behavior of the original model~\cite{wang2022interpretabilitywildcircuitindirect,chhabra2024neuroplasticity,prakash2024finetuningenhancesexistingmechanisms}. We adopt Kullback-Leibler divergence (KL-divergence) as the metric, following Conmy et al.~\cite{NEURIPS2023_34e1dbe9}. Let \(M\) denote the model and \(C\) the discovered circuit. Faithfulness is defined as the percentage of the model’s performance captured by the circuit. The formula for faithfulness is:
\[
\text{Faithfulness} = \left(1 - \frac{|F(M) - F(C)|}{F(M)}\right) \times 100\%,
\]
where \( F(M) \) represents the performance of the full model \( M \) and  \( F(C) \) represents the performance of the circuit \( C\).

\textbf{Robustness.} To evaluate the stability of the identified circuit, we propose a robustness score based on its robustness under dataset perturbations. Taking addition and subtraction tasks as an example, perturbations include numeric noise (e.g., changing \(7 + 12\) to \(7 + 15\)), and operator noise (e.g., replacing \(12 + 7\) with \(12 - 7\)). And we conduct robustness calculations on these perturbed datasets, applying noise at varying levels to create noisy datasets.

The robustness score is computed using the Jaccard Similarity \cite{jaccard1912distribution} between the initial circuit \( G_0 \) and perturbed circuits \( G_p \). The formula is:
\[
\text{Robustness(\(p\))} = J_E(G_0, G_p),
\]
where \( J_E(G_0, G_p) \) represents the Jaccard Similarity for \textbf{edges} between the initial circuit \( G_0 \) and the perturbed circuit \( G_p \), and \( p \) denotes the perturbation level.

This modification focuses on edge similarity, as it better reflects structural integrity. A high robustness score indicates that the perturbed circuits maintain a similar edge structure to the original, with a score closer to 1 reflecting a robust circuit structure.

\subsection{Tasks Design}

To examine the effect of fine-tuning on circuit dynamics, we construct a suite of challenging mathematical tasks in which pre-trained models initially perform poorly. As shown in Figure~\ref{fig:tasks}, these tasks help reveal the underlying fine-tuning mechanisms that drive significant performance gains during the process.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=.95\linewidth]{Figure/tasks.pdf}
\caption{\textbf{The workflow of understanding fine-tuning mechanisms using circuits.} The initial pre-trained model shows low accuracy on tasks. The corresponding circuits were found in both the pre-trained model and the fine-tuned model, and the fine-tuning mechanism was understood by comparing the changes in the circuits before and after.}
\label{fig:tasks}
\end{center}
\vskip -.1in
\end{figure}

\textbf{Addition and Subtraction (Add/Sub).} This task evaluates the model's ability to perform basic addition and subtraction operations. Corrupted data involves altering the arithmetic operation. The task includes five subtasks categorized by the range of numbers involved within 100, 200, 300, 400, and 500. Each subtask contains 5,000 instances.

\textbf{Multiplication and Division (Mul/Div).} This task assesses the model's capability to handle multiplication and division accurately. Corrupted data involves changing the operation between multiplication and division. A total of 2,000 instances are included in this task.

\textbf{Arithmetic and Geometric Sequence (Sequence).} This task measures the model's ability to recognize and extend arithmetic or geometric sequences. Corrupted data involves altering one term in the sequence. The dataset for this task contains 5,000 instances.

\textbf{Least Common Multiple (LCM).} This task tests the model's ability to calculate the Least Common Multiple (LCM) of two integers. Corrupted data involves changing the input numbers or the conditions of the LCM calculation. The task includes 2,500 instances.

\textbf{Function Evaluation (Function).} This task focuses on the model's ability to compute values for linear functions, typically of the form \( y = mx + b \). Corrupted data involves altering the constant term in the function. The dataset contains 5,000 instances.
For each task, we ensure a strict separation between the dataset used for fine-tuning and the dataset used for circuit analysis. Specifically, 80\% of the dataset is allocated for fine-tuning, and the remaining 20\% is reserved for identifying circuits and evaluating the model's and circuit's accuracies. This separation guarantees that performance evaluation is conducted on data unseen during fine-tuning.

\section{How Do Circuits Evolve During the Fine-Tuning Process?}
\label{evolve_part}

\subsection{Model Accuracy, Circuit Faithfulness, and Robustness Analysis}
\begin{figure}[t!]
\begin{center}
\includegraphics[width=\linewidth]{Figure/4_1.pdf}
\vskip -.1in
\caption{\textbf{Circuit Accuracy,  Faithfulness, and Robustness during
Fine-tuning.}
\textbf{Left:} Training progress of the model accuracy across different mathematical tasks, showing continuous improvement over checkpoints. 
\textbf{Middle:} Evolution of faithfulness metrics during training, demonstrating consistently high faithfulness across five mathematical tasks. 
\textbf{Right:} Robustness analysis for the Add/Sub task. Robustness evaluation under different perturbation levels, comparing Fine-tuned, Pre-trained, and Random models.}
\label{fig:4_1}
\end{center}
\vskip -.1in
\end{figure}

\textit{To analyze circuit evolution, we first evaluate model accuracy across fine-tuning checkpoints.}
We use LoRA~\cite{hu2021lora} to fine-tune the Pythia-1.4B model ~\cite{biderman2023pythia} on five different mathematical tasks. The experimental settings for fine-tuning are shown in Appendix~\ref{appendixA}. The left panel of Figure~\ref{fig:4_1} depicts the accuracy dynamics of the model on five mathematical tasks during fine-tuning. We track the model's accuracy at various training stages across different tasks, revealing consistent improvements in performance throughout the fine-tuning process.

\textit{Next, we explore the faithfulness of the circuits found at each stage of fine-tuning.} Prior work~\cite{hanna2024faithfaithfulnessgoingcircuit} achieved over 85\% faithfulness by selecting 1–2\% of edges. Given our more complex tasks and larger model, we select 5\% of edges to ensure reliable circuits (faithfulness \textgreater70\%). As shown in the middle panel of Figure~\ref{fig:4_1}, circuit faithfulness consistently exceeds 80\% across most tasks, both before fine-tuning (Checkpoint 0) and throughout fine-tuning (Checkpoints 1–10). The only exception is Add/Sub task, where faithfulness is 77.52\% before fine-tuning. These results confirm high circuit faithfulness in both pre-trained and fine-tuned models across all tasks.

\textit{Finally, we conduct robustness analysis on the circuits identified by EAP-IG.} We evaluate the robustness of circuits in the pre-trained model, the fine-tuned model, and a randomly initialized model. In this section, we present the robustness analysis for the Add/Sub (100), with analysis for other tasks provided in Appendix~\ref{appendixC}. As discussed in Section~\ref{sec:Robustness Calculation}, we perturb the original dataset by 10\% to 90\% and identify the circuit of three models in perturbed datasets with varying noise levels. Then, we compute the robustness score of Fine-tuned, Pre-trained, and Random models under different perturbation levels. Results in the right part of Figure~\ref{fig:4_1} reveal that circuits identified by EAP-IG demonstrate high fidelity in both pre-trained and fine-tuned models, despite significant performance differences.

\tcbset{
    myblueboxstyle/.style={
        colback=CustomSkyBlue!10!white, % 天蓝色
        colframe=CustomSkyBlue!10!white, % 天蓝色
        coltitle=black, % Title text color
        fonttitle=\bfseries, % Bold title font
        boxrule=1mm, % Border thickness
        width=\linewidth, % Full width
        left=3mm, % Left padding
        right=3mm, % Right padding
        top=2mm, % Top padding
        bottom=2mm, % Bottom padding
        sharp corners, % Sharp corners
        before=\vspace{2mm}, % Space before the box
        after=\vspace{2mm} % Space after the box
    }
}
\vspace{-.1in}
\begin{tcolorbox}[myblueboxstyle]
\textbf{Key Observation 1:} Circuits can be identified in both pre-trained and fine-tuned models with high faithfulness and robustness, regardless of their significant performance differences.
\end{tcolorbox}
\vspace{-.3in}

\subsection{Circuit is Converging During Fine-Tuning}
\label{edge_change}
We conjecture that as the model's accuracy on the task continues to improve, the model's internal circuits should continue to stabilize. To verify our hypothesis, we analyze the change of nodes and edges across consecutive checkpoints.

\textit{First, we analyze node and edge changes across checkpoints.} The top right of Figure~\ref{fig:4_2} illustrates three mathematical tasks, corresponding to the model's increasing accuracy during fine-tuning. By tracking the number of node and edge modifications between different checkpoints, we assess whether circuit changes diminish over time and tend toward convergence as the accuracy of the model improves. Details for the remaining tasks are provided in Appendix~\ref{appendixD}. As shown in Figure~\ref{fig:4_2}, the number of node/edge state changes decreases consistently over time, indicating stabilization and convergence of the circuit. 

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\linewidth]{Figure/4_2.pdf}
\vskip -.1in
\caption{
\textbf{The structural differences of Add/Sub(100) circuit (24 layers) and evolution of circuits across checkpoints in terms of node and edge changes.} 
\textbf{Left: Differences of Add/Sub(100) circuit before and after fine-tuning.} The layout organizes nodes hierarchically from input to output (logits). Nodes and edges are color-coded based on their status: added (blue), removed (red), or unchanged (grey/white). Darker grey nodes indicate higher degrees. 
\textbf{Top Right: Node and edge changes between checkpoints during fine-tuning in three tasks.} The left chart depicts the number of node changes per transition, while the right chart focuses on edge changes. 
\textbf{Bottom Right: Change rate of Add/Sub, Sequence, LCM three tasks.} Bars with diagonal lines represent the change rate of edges.}
\label{fig:4_2}
\end{center}
\end{figure}

\textit{Subsequently, we propose a new metric to measure the degree of change of nodes and edges during fine-tuning.} To quantify the changes in edges and nodes during fine-tuning across $n$ checkpoints, we define a unified change rate:
\[
\Delta_S = \frac{1}{n} \sum_{t=0}^{n-1} \frac{\Delta s_{t \to t+1}}{S_0} \times 100\%,
\]
where $\Delta s_{t \to t+1}$ denotes the number of nodes or edges that change from checkpoint $t$ to checkpoint $t+1$, and $S_0$ denotes the total number of nodes or edges in the initial circuit.

As shown in Figure~\ref{fig:4_2}, fine-tuning induces structural changes, with \(\Delta_S\) (Edge) consistently exceeding \(\Delta_S\) (Node) by a factor of 2–3 across three tasks. This underscores the pivotal role of edges as the primary drivers of structural adaptation during fine-tuning. For the other tasks, the change rates of nodes and edges in the circuit are also shown in Appendix~\ref{appendixD}.

\vspace{-.1in}
\begin{tcolorbox}[myblueboxstyle]
\textbf{Key Observation 2:}  Fine-tuning performs more significant edge modifications than node modifications. 
\end{tcolorbox}
\vspace{-.3in}

\subsection{Reorganizing Circuit Edges to Form a New Circuit}
\label{change_result}
As discussed in Section~\ref{eap}, each edge's score is computed as the dot product of the averaged loss gradients and activation difference, quantifying its influence on model predictions. To examine structural changes in circuits during fine-tuning, we use the 95th percentile of edge scores as a dynamic threshold. Edges in the initial and final circuits exceeding this threshold are retained, yielding sparser circuits that capture the model’s core information flow. Experimental results for all other tasks are provided in Appendix~\ref{appendixE}.
\textit{The distribution of added and deleted nodes and edges follows a distinct pattern.} As illustrated in the left part of Figure~\ref{fig:4_2}, added nodes are predominantly located in the middle and later layers of the circuit, whereas added and deleted edges are concentrated in the middle layers. The shallow layers exhibit minimal changes, providing a stable foundation for task-specific adaptations.       

In order to prove our conclusions, we conduct investigations into how the circuit evolves under different fine-tuning regimes. Specifically, Appendix~\ref{appendixF} examines the circuit modifications resulting from various PEFT strategies, while Appendix~\ref{appendixG} focuses on the changes induced by full-parameter fine-tuning and LoRA. Finally, Appendix~\ref{appendixH} provides a comparison of circuit changes observed under different LLMs.

\section{Can Circuit Insights Enhance the Fine-tuning Process?}
\label{Circuitlora}

In the previous section, we observe that while the nodes in the model's circuit exhibit minimal changes during fine-tuning, the edges undergo significant modifications. This observation raises an intriguing question: \textit{Can LoRA be improved by fine-tuning the edges that change the most?} We would like to improve the fine-tuning algorithm from the perspective of \textbf{Mechanistic Interpretability}.

\subsection{Applying Circuit Edge Changes into LoRA Fine-Tuning}

Based on the score of edges and the result of section~\ref{eap}, we assume that the most “active” edges play a key role in the fine-tuning process. Also, considering that LoRA is fine-tuned in layers of the model, we want to focus on the layers where the most “active” edges are located.

We propose \textbf{\texttt{CircuitLoRA}}, a circuit-aware Low-Rank Adaptation (LoRA) method that incorporates circuit-level analysis to enhance fine-tuning efficiency and performance. \textbf{\texttt{CircuitLoRA}} operates in two phases: first, the edges with the largest score changes are analyzed to identify \textit{Critical Layers}; second, higher-rank LoRA modules are assigned to layers with more edge changes, while standard-rank modules are applied to other layers. The complete procedure is detailed in Algorithm~\ref{alg:circuitlora}.

Our hypothesis is that this improved fine-tuning algorithm, which leverages circuit-based analysis, can make better use of the fine-tuning mechanism. In the subsequent section, we investigate this hypothesis, designing experiments across different mathmatical tasks to compare our strategy against full parameter fine-tuning and LoRA baseline. 

\begin{algorithm}[!t]
   \caption{\texttt{CircuitLoRA}: Improve LoRA Using Circuit-Based Critical Layers Identification}
   \label{alg:circuitlora}
\begin{algorithmic}
   \STATE {\bfseries Input:} Pre-trained model $M$, Pre-finetuning circuit $C_{before}$, Post-finetuning circuit $C_{after}$, LoRA ranks $r_{o}$, $r_{c}$, Scaling factors $\alpha$, $\alpha_{critical}$
   \STATE {\bfseries Phase 1: Critical Layers Identification}
   \STATE Compute edge differences $\Delta_e$ between $C_{before}$ and $C_{after}$
   \STATE Aggregate $\Delta_e$ to layer scores $\Delta_l$ and select critical layers $\mathcal{L}_{critical}$
   \STATE {\bfseries Phase 2: Module Replacement}
   \FOR{each layer $l \in M$}
      \IF{$l \in \mathcal{L}_{critical}$}
         \STATE Replace $l$ with \texttt{EnhancedLoRALinear} using $r_{c}$ and $\alpha_{critical}$
      \ELSE
         \STATE Replace $l$ with \texttt{LoRALinear} using $r_{o}$ and $\alpha$
      \ENDIF
   \ENDFOR
   \STATE {\bfseries Return:} Updated model $M^{*}$
\end{algorithmic}
\end{algorithm}

\subsection{Improving Fine-Tuning Efficiency and Accuracy by Circuit Insights}

To verify our hypothesis, we perform experiments on a range of arithmetic and mathematical reasoning tasks. The experimental results of \textbf{\texttt{CircuitLoRA}} are summarized in two tables. In our experiments, 5 \textit{Critical Layers} are selected. We compare \textbf{\texttt{CircuitLoRA}} against control groups including LoRA and RandomLoRA (5 \textit{Critical Layers} are randomly selected). For each method in the experiment, we report the final accuracy as the mean of five runs with different random seeds.  

\begin{table}[t!]
\centering
\caption{\textbf{Performance metrics for Add/Sub (within 300) and four other math tasks: Mul/Div, Sequence, LCM, and Function across different configurations.} The control groups of \texttt{CircuitLoRA} (\(r_o=8\), \(r_c=32\)) are LoRA (\(r_o=16\)) and RandomLoRA (\(r_o=8\), \(r_c=32\)), and the control groups of \texttt{CircuitLoRA} (\(r_o=32\), \(r_c=64\)) are LoRA (\(r_o=32\)) and RandomLoRA (\(r_o=32\), \(r_c=64\)). Here, \(r_o\) and \(r_c\) represent the ranks used in \texttt{CircuitLoRA}, where \(r_c\) is the rank for \textit{critical layer} modules, and \(r_o\) is the rank for \textit{non-critical layer} modules. Model Accuracy is expressed as percentages.}
\label{circuitlora_result}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\textbf{Method} & \textbf{Parameter Ratio} & \textbf{Add/Sub(300)} & \textbf{Mul/Div} & \textbf{Sequence }& \textbf{LCM}& \textbf{Function} \\
\midrule
Pre-trained & 0\% & 18.30 & 39.75 & 15.70 & 18.80 & 32.00 \\
Full FT & 100\% & 79.20 & 95.75 & 91.50 & 91.40 & 100.00 \\
LoRA (\(r_o=2\)) & 0.1111\% & 72.60 & 90.00 & 67.10 & 86.40 & 84.10 \\
LoRA (\(r_o=8\)) & 0.4428\% & 78.30 & 94.25 & 79.60 & 91.20 & 96.80 \\
LoRA (\(r_o=16\)) & 0.8816\% & 78.40 & 95.50 & 83.40 & 91.20 & 97.30 \\
LoRA (\(r_o=32\)) & 1.7479\% & 80.50 & 96.25 & 92.70 & 92.80 & 98.60 \\
\midrule
\rowcolor[gray]{0.9}
\textbf{\texttt{CircuitLoRA} (\(r_o=8\), \(r_c=32\))} & 0.7175\% & \textbf{82.70} & \textbf{96.00} & \textbf{92.20} & \textbf{92.60} & \textbf{99.40} \\
RandomLoRA (\(r_o=8\), \(r_c=32\)) & 0.7175\% & 77.50 & 95.50 & 81.70 & 90.40 & 97.70 \\
\rowcolor[gray]{0.9}
\textbf{\texttt{CircuitLoRA} (\(r_o=32\), \(r_c=64\))} & 1.4248\% & \textbf{83.10} & \textbf{97.00} & \textbf{94.60} & \textbf{93.00} & \textbf{99.50} \\
RandomLoRA (\(r_o=32\), \(r_c=64\)) & 1.4248\% & 79.10 & 95.75 & 92.10 & 92.00 & 98.50 \\
\bottomrule
\end{tabular}%
}

\end{table}

As shown in Table~\ref{circuitlora_result}, \textbf{\texttt{CircuitLoRA}} consistently outperforms baseline methods, including LoRA and RandomLoRA, across all five tasks. For instance, in the "within 300" task, \textbf{\texttt{CircuitLoRA}} (\(r_o=8, r_c=32\)) achieves an accuracy of \textbf{82.70\%}, with fewer training parameters, surpassing RadomLoRA and LoRA. When configured as \textbf{\texttt{CircuitLoRA}} (\(r_o=32, r_c=64\)) reaches \textbf{83.10\%}, outperforming RandomLoRA and LoRA. \textbf{\texttt{CircuitLoRA}} experiments on other tasks refer to the Appendix~\ref{appendixI}.


By focusing on the edges with the highest score during fine-tuning, \textbf{\texttt{CircuitLoRA}} demonstrates significant improvements in both accuracy and parameter efficiency across various mathematical tasks. The experimental results presented in this study provide a compelling answer to the question posed in Section~\ref{Circuitlora}. This approach leverages insights from \textbf{Mechanistic Interpretability}, identifying and prioritizing \textit{Critical Layers} where critical changes occur. 

\begin{tcolorbox}[myblueboxstyle]
\textbf{Key Observation 3:} Circuits can in turn improve fine-tuning with higher accuracy and parameter efficiency across various mathematical tasks.
\end{tcolorbox}

\section{How Capable is the Union Circuit in Performing Compositional Tasks?}
\label{compositional_part}
In this section, we further explore the behavior of circuits in compositional tasks, aiming to investigate whether these tasks can be interpreted through the combination of circuits.

\subsection{Compositional Tasks, Compositional Circuits and Union Circuits}

In the beginning, we first introduce a series of definitions regarding the composition of tasks and circuits.

\textbf{Compositional Tasks.} A compositional task is composed of multiple interrelated subtasks, each contributing to the overall objective. 
\textbf{Compositional Circuits.} A Compositional Circuit captures the structural and functional dependencies within a compositional task. 
\textbf{Union Circuits.} A Union Circuit is constructed by merging the circuits of all subtasks. 

To design the compositional tasks, we consider the two-step operation, which involves the calculation of two different types of mathematical problem, such as addition/subtraction and multiplication/division.
For instance, the compositional task `\((61 - 45) \times 45 =\)'' involves two mathematical operations: (1) (Addition/Subtraction): \(61 - 45 =\)''; and (2) (Multiplication/Division): \(16 \times 45 =\)''. More examples of compositional tasks can be found in Appendix~\ref{appendixJ}.

Our intuition is that if the circuits can represent the minimum calculation block for one tasks, then it is conjectured that the Union Circuits of the two subtasks 
can exhibit the power to represent the circuits for the compositional task. In the following, we will investigate the conjecture through two approaches: (1) we compare the similarities between the Union Circuits and the Compositional Circuits; (2) we use the Union Circuits to develop the \textbf{\texttt{CircuitLoRA}} algorithm and evaluate whether the performance of the compositional task can also be improved.

\subsection{Efficient Single-Phase Fine-Tuning on Compositional Task with Union Circuit}

We conduct overlap analysis and fine-tuning experiments on the two-step operation combination task. For a circuit \( \mathcal{C} \), we define a \(\mathrm{Top}_{k}(\mathcal{C})\) metric to quantify how many of the top-\(k\) edges, ranked by their scores, are shared between two circuits.
Then we define the Overlap metric as follows:
\[
  \mathrm{Overlap}_{k}\bigl(\mathcal{C}_1, \mathcal{C}_2\bigr)
  \;=\;
  \bigl\lvert
    \mathrm{Top}_{k}(\mathcal{C}_1)
    \;\cap\;
    \mathrm{Top}_{k}(\mathcal{C}_2)
  \bigr\rvert.
\]
First, we calculate the Union Circuit and Combination Circuit under the two-step operation combination task.

\begin{table}[t!]
\centering
\caption{\textbf{Overlap for Different Values of \(k\) in Circuit Comparisons.} The table presents  \(\mathrm{Overlap}_{k}\) between the Union Circuit and the Combination Circuit. Additionally, circuits from the Add/Sub task are compared with those from the Mul/Div and Sequence tasks as control groups.}
\label{table:overlap}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{\textbf{Circuit Comparison}} & \multicolumn{3}{c}{\(\mathrm{Overlap}_{k}\bigl(\mathcal{C}_1, \mathcal{C}_2\bigr)\)} \\
\cmidrule(lr){2-4}
& \(k=100\) & \(k=500\) & \(k=1000\) \\
\midrule
\rowcolor[gray]{0.9}
Union vs Compositional &69 &259 & 470 \\
Add/Sub vs Mul\_Div &51 &187 & 357\\
Add/Sub vs Sequence &42 &156 & 286\\
\bottomrule
\end{tabular}%
}
\end{table}

\textit{Through overlap analysis, we prove the efficiency of Union Circuit to a certain extent.} Table~\ref{table:overlap} analyzes the overlap for different values of $k$ to evaluate the efficiency of the Union Circuit. The results show that, regardless of the value of $k$, the overlap between the Union Circuit and the Compositional Circuit is consistently the highest. Comparisons are made between the addition/subtraction circuit and circuits from control tasks, such as multiplication/division and arithmetic/geometric sequences. The overlaps in these cases are notably lower. These findings demonstrate that the Union Circuit provides an approximate representation of the Compositional Circuit.

Then, we use Union Circuit and Compositional Circuit to identify the \textit{Critical Layers} to further explore the `approximation ability" of Union Circuit. Table~\ref{tab:two_step_operations} summarizes the performance of \textbf{\texttt{CircuitLoRA}} and LoRA on the two-step operations task. Specifically, \textbf{\texttt{CircuitLoRA}} with Compositional Circuit achieves the highest accuracy of \textbf{67.20\%}. Surprisingly, when using the Union Circuit for \textit{Critical Layer} identification, \textbf{\texttt{CircuitLoRA}} achieves \textbf{65.50\%}, still exceeding the performance of LoRA except the Compositional Circuit configuration.

\begin{table}[t!]
\centering
\caption{\textbf{Performance metrics for Two-Step Operations Task.} \textbf{\texttt{CircuitLoRA\textsubscript{C}}} represents using Compositional Circuit for \textit{Critical Layer} Identification and \textbf{\texttt{CircuitLoRA\textsubscript{U}}} represents using Union Circuit. Model Accuracy all expressed as percentages.}
\label{tab:two_step_operations}
\resizebox{0.64\linewidth}{!}{%
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Parameter Ratio} & \textbf{M.Acc.} \\
\midrule
Pre-trained & / & 0.90 \\
LoRA (\(r_o=2\)) & 0.1111\% & 59.60 \\
LoRA (\(r_o=8\)) & 0.4428\% & 60.50 \\
LoRA (\(r_o=16\)) & 0.8816\% & 61.10 \\
LoRA (\(r_o=32\)) & 1.7479\% & 64.70 \\
\midrule
\rowcolor[gray]{0.9} 
\textbf{\texttt{CircuitLoRA\textsubscript{C}} (\(r_o=8, r_c=32\))} & 0.7175\% & \textbf{67.20} \\
\rowcolor[gray]{0.9}
\textbf{\texttt{CircuitLoRA\textsubscript{U}} (\(r_o=8, r_c=32\))} & 0.7175\% & \textbf{65.50} \\ 
RandomLoRA (\(r_o=8, r_c=32\)) & 0.7175\% & 62.30 \\
\bottomrule
\end{tabular}%
}
\end{table}

\textit{This demonstrates we can use Union Circuit for single-phase fine-tune.} This means that for fine-tuning of the combination task, if we want to use \textbf{\texttt{CircuitLoRA}}, we do not need to find its combinational circuit first, but can replace it with the union of the circuits of the subtasks that have been discovered to some extent.

\begin{tcolorbox}[ myblueboxstyle]
\textbf{Key Observation 4:} The composition of the circuits can effectively represent the circuits of the compositional task.
\end{tcolorbox}

\section{Conclusion and Future Work}
\label{sec:conclusion}
In this paper, we build on circuit analysis to deepen our understanding of fine-tuning and better leverage learned mechanisms. Our findings show that fine-tuning primarily modifying edges rather than merely introducing new components to form new circuits. Building on this insight, we develop a circuit-aware LoRA method. Across multiple tasks, our results demonstrate that incorporating this MI perspective enhances fine-tuning efficiency. Additionally, we show that the composition of subtask circuits effectively represents the circuit of compositional task.

Moving forward, we will explore the following directions. Although our work focused on math tasks, applying circuit-based methods to more tasks would further validate the generality of our insights. Additionally, while our compositional experiments only explore two-step arithmetic, extending this analysis to multi-step or more compositional tasks could provide deeper insights into circuit interactions, enhancing interpretability and fine-tuning efficiency.

\section*{Impact Statement}
Our work provides concrete insights for advancing Mechanistic Interpretability. This deeper understanding of the internal processes guiding model updates paves the way for more efficient, accurate, and trustworthy AI systems. We hope these findings inspire new methods and applications that take advantage of circuit-based analysis to unlock greater transparency, reliability, and performance in LLMs development, and to make better use of the learned mechanisms in these models.

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bibliography{circuit_conference}
\bibliographystyle{circuit_conference}

\appendix
\onecolumn

\section{Experimental Setup of Fine-Tuning}
\label{appendixA}
Fine-tuning experiments were conducted across various arithmetic tasks, with configurations tailored to each. All tasks were trained with a batch size of 8, gradient accumulation steps of 4, and a warmup of 50 steps, using a weight decay of 0.01.

\textbf{Addition and Subtraction (Add/Sub)} task, which includes subtasks with ranges of 100, 200, 300, 400, and 500, each subtask consists of 5,000 samples. The 100-range subtask was trained for 2 epochs, while others were trained for 4 epochs. LoRA experiments were performed with ranks \( r = 2, 8, 16, 32 \), using a learning rate of 3e-4, except for the 400-range (\( r = 32 \), lr=2e-4). Full Parameter Fine-Tuning (FPFT) used learning rates of 8e-6 (100-range), 6e-6 (200-range), 5e-6 (400-range), and 4e-6 (500-range). \texttt{CircuitLoRA} applied higher learning rates (4e-4 or 5e-4) for \textit{Critical Layers} and 3e-4 for \textit{non-Critical Layers}.

\textbf{Multiplication and Division (Mul/Div)} task contains 2,000 samples and was trained for 2 epochs. LoRA used a learning rate of 3e-4, FPFT used 4e-6, and \texttt{CircuitLoRA} used 2e-4 for \textit{Critical Layers} and 3e-4 for \textit{non-Critical Layers}.

\textbf{Arithmetic and Geometric Sequence (Sequence)} task includes 5,000 samples, trained for 4 epochs. LoRA experiments used a learning rate of 3e-4, FPFT used 8e-6, and \texttt{CircuitLoRA} applied 6e-4 (\( r = 32 \)) and 5e-4 (\( r = 64 \)) for \textit{Critical Layers}, with 3e-4 for \textit{non-Critical Layers}.

\textbf{Least Common Multiple (LCM)} task, which contains 2,500 samples and was trained for 2 epochs, LoRA used learning rates of 3e-4 (\( r = 2, 8 \)), 4e-4 (\( r = 16 \)), and 2e-4 (\( r = 32 \)). FPFT used 4e-6, and \texttt{CircuitLoRA} used 4e-4 (\( r = 32 \)) and 6e-5 (\( r = 64 \)) for \textit{Critical Layers}, with 3e-4 for \textit{non-Critical Layers}.

\textbf{Function Evaluation (Function)} task, with 5,000 samples trained for 2 epochs, used consistent LoRA learning rates of 3e-4 (\( r = 2, 8, 16, 32 \)), FPFT with 8e-6, and \texttt{CircuitLoRA} with 4e-4 for \textit{Critical Layers} and 3e-4 for \textit{non-Critical Layers}.

\section{Model Accuracy and Circuit Faithfulness on Other Tasks}
\label{appendixB}
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.95\linewidth]{Figure/Appendix/Appendix_B.pdf}
\caption{
    \textbf{Performance analysis of Add/Sub tasks across different checkpoints.} 
    \textbf{Left:} Model Accuracy (M.Acc.) shows the performance trends of four tasks—\textit{Add/Sub (200)}, \textit{Add/Sub (300)}, \textit{Add/Sub (400)}, and \textit{Add/Sub (500)} across checkpoints. 
    \textbf{Right:} Faithfulness scores measure the reliability of predictions for the same tasks across the same checkpoints. 
    }
\label{fig:add_sub_analysis}
\end{center}
\end{figure}

The left part of Figure~\ref{fig:add_sub_analysis} presents the model’s accuracy for four task and the results indicate a consistent improvement in accuracy across all tasks.

The right part of Figure~\ref{fig:add_sub_analysis} illustrates the faithfulness of circuits for the same tasks. Faithfulness scores remain above 70\% for all tasks. These results highlight both the accuracy improvements and the reliability of circuits throughout the fine-tuning process.

\section{Robustness Analysis Experiments on Other Tasks}
\label{appendixC}
Building on the results reported in the main text, this appendix details our additional robustness experiments conducted across multiple arithmetic tasks. Following the methodology presented in Section~\ref{sec:Robustness Calculation}, we systematically apply input perturbations to Multiplication/Division, Arithmetic/Geometric Sequence, Least Common Multiple, and Function Evaluation tasks. Our findings further corroborate the consistency and fidelity of circuits identified by EAP-IG, demonstrating their ability to adapt under varying perturbation conditions while preserving core computational relationships.

\paragraph{Multiplication and Division Tasks}
Data perturbation in multiplication and division tasks involves altering one of the operands within a specified range while maintaining the validity of the operation. This introduces variability without disrupting the fundamental arithmetic relationship.

\textbf{Example:}
\begin{itemize}
    \item \texttt{Original: Calculate the result of the following arithmetic expression and provide only the final answer: 26 * 15 =}
    \item \texttt{Perturbed: Calculate the result of the following arithmetic expression and provide only the final answer: 26 * 20 =}
\end{itemize}

\paragraph{Arithmetic and Geometric Sequence Tasks}
For arithmetic sequences, perturbation is achieved by uniformly shifting each term by a fixed integer. In geometric sequences, the first term is adjusted, and subsequent terms are recalculated using the original common ratio to preserve the sequence's structure.

\textbf{Example:}
\begin{itemize}
    \item \texttt{Original: Derive the following sequence: 26, 66, 106, 146,}
    \item \texttt{Perturbed: Derive the following sequence: 21, 61, 101, 141,}
\end{itemize}

\paragraph{Least Common Multiple (LCM) Tasks}
Data perturbation for LCM tasks involves regenerating the last LCM expression using one of three strategies: generating multiples, coprimes, or pairs with common factors that are not multiples. This ensures diversity and prevents redundancy in the dataset.

\textbf{Example:}
\begin{itemize}
    \item \texttt{Original: Calculate the least common multiple (LCM) of two numbers. LCM(189, 84) = 756, LCM(200, 400) =}
    \item \texttt{Perturbed: Calculate the least common multiple (LCM) of two numbers. LCM(189, 84) = 756, LCM(75, 120) =}
\end{itemize}

\paragraph{Function Evaluation Tasks}
In function evaluation tasks, perturbation involves modifying the constant term \( b \) in a linear function \( y = ax + b \) by a value within a specified range. The corresponding \( y \)-values are recalculated to reflect the change, ensuring the functional relationship remains intact.

\textbf{Example:}
\begin{itemize}
    \item \texttt{Original: There is a function y=5x+201. Given x=1,2,3,4, y=206,211,216,}
    \item \texttt{Perturbed: There is a function y=5x+151. Given x=1,2,3,4, y=156,161,166,}
\end{itemize}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.95\linewidth]{Figure/Appendix/C_robustness.pdf}
\caption{\textbf{Robustness analysis across four additional tasks: LCM, Sequence, Mul/Div, and Function. }Despite the varying perturbation levels, both the pre-trained and fine-tuned models exhibit consistently high confidence scores compared to the randomly initialized model.}
\label{fig:robustness_analysis}
\end{center}
\end{figure}

In line with the observations for addition and subtraction, our experiments on LCM, Sequence, Multiplication/Division, and Function Evaluation tasks demonstrate that circuits can be identified in both pre-trained and fine-tuned models with high faithfulness and robustness. This finding holds true despite the significant performance gap between the two model states, underscoring the reliability and stability of the discovered circuits across diverse arithmetic tasks.

\section{Node, Edge, and Change Rate Analysis on Other Tasks}
\label{appendixD}
This section combines the analysis of node and edge changes with the change rates for various tasks. By evaluating these metrics together, we provide a comprehensive view of the structural and dynamic adjustments observed across different tasks. The node and edge changes reflect the structural variations in the underlying data or models, while the change rate quantifies the intensity of these changes, offering deeper insights into task-specific behaviors.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.95\linewidth]{Figure/Appendix/Appendix_D.pdf}
\caption{
    \textbf{Analysis of Node and Edge Changes Across Add/Sub Tasks, including Within 200, Within 300, Within 400, Within 500, Multiplication/Division, and Function Evaluation.}
    \textbf{Left:} Node changes during fine-tuning. 
    \textbf{Middle:} Edge changes during fine-tuning.
    \textbf{Right:} Change rates for nodes and edges across tasks.
}
\label{fig:node_edge_analysis}
\end{center}
\end{figure}

The Figure~\ref{fig:node_edge_analysis} presents an analysis of node and edge dynamics during fine-tuning across six tasks: Within 200, Within 300, Within 400, Within 500, Multiplication/Division, and Function Evaluation. It highlights how the interplay between node, edge, and change rate metrics contributes to the overall task dynamics, ensuring a holistic understanding of the transformations involved in each scenario.

\section{Changes in Circuits Before and After Fine-Tuning in Other Tasks}
\label{appendixE}
In this section, we compare the circuits before and after fine-tuning for tasks involving addition and subtraction within the ranges of 200, 300, 400, and 500, as well as tasks on Multiplication and Division, Arithmetic and Geometric Sequence, Least Common Multiple, and Function Evaluation. Please refer to Figures~\ref{fig:diff_before_after_200_300}, \ref{fig:diff_before_after_400_500}, \ref{fig:diff_before_after_mul_seq}, and \ref{fig:diff_before_after_lcm_func} for the comparison results of all tasks before and after fine-tuning.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.85\linewidth]{Figure/Appendix/Appendix_E_200_300.pdf}
\caption{
    \textbf{Differences of Add/Sub(200) and Add/Sub(300) circuit (24 layers).} Darker gray indicates a higher node degree.
}
\label{fig:diff_before_after_200_300}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.85\linewidth]{Figure/Appendix/Appendix_E_400_500.pdf}
\caption{
    \textbf{Differences of Add/Sub(400) and Add/Sub(500) circuit (24 layers).} Darker gray indicates a higher node degree.
}
\label{fig:diff_before_after_400_500}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.85\linewidth]{Figure/Appendix/Appendix_E_mul_seq.pdf}
\caption{
    \textbf{Differences of Mul/Div and Sequence circuit (24 layers).} Darker gray indicates a higher node degree.}
\label{fig:diff_before_after_mul_seq}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.85\linewidth]{Figure/Appendix/Appendix_E_lcm_func.pdf}
\caption{
    \textbf{Differences of LCM and Function circuit (24 layers).} Darker gray indicates a higher node degree.
}
\label{fig:diff_before_after_lcm_func}
\end{center}
\end{figure}

\textbf{Common Observations: The distribution of added and deleted nodes and edges follows a distinct pattern.} Our analysis reveals similarities with the findings from addition and subtraction tasks within the range of 100. Same as Figure~\ref{fig:4_2} in Section~\ref{change_result}, added nodes predominantly appear in the middle and later layers of the circuit. Similarly, added and deleted edges are concentrated in the middle layers. In contrast, the shallow layers exhibit minimal changes, serving as a stable foundation for task-specific adaptations.

\textbf{Trends with Increasing Number Ranges in Addition and Subtraction:} We observe a distinct trend in the circuits fine-tuned for addition and subtraction tasks as the number range increases. The edges in the fine-tuned circuits become more concentrated, reflecting a refined structure to handle the broader numeric range efficiently.

\textbf{Comparative Observations Across Tasks:} Tasks such as Multiplication and Division, Arithmetic and Geometric Sequence, Least Common Multiple, and Function Evaluation exhibit different circuit adaptations compared to addition and subtraction tasks. As illustrated in Figures~\ref{fig:diff_before_after_mul_seq} and \ref{fig:diff_before_after_lcm_func}, these tasks utilize circuits that focus more on the later layers. This difference indicates a shift in computational emphasis, highlighting the tailored adaptations of the circuit for distinct task requirements.

\section{Circuit Changes During Fine-Tuning: A Comparison Across PEFT Methods}
\label{appendixF}
In this appendix, we compare three different PEFT methods—\textit{LoRA}, \textit{AdaLoRA}, and \textit{IA3} across various mathematical tasks (e.g., addition/subtraction with 200, multiplication/division, Sequence, LCM, and Function). Figures~\ref{fig:f_200} through~\ref{fig:f_function} illustrate the model accuracy, faithfulness, and the evolution of nodes and edges at each checkpoint, as well as the corresponding change rates. By examining these metrics, we can observe how the internal circuits of each model evolve during fine-tuning until they converge.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.85\linewidth]{Figure/Appendix/F_200.pdf}
\caption{
\textbf{Comparison of Add/Sub (200) circuits during fine-tuning with LoRA, AdaLoRA, and IA3.} 
\textbf{Top:}  Model accuracy and faithfulness across checkpoints.
\textbf{Bottom Left:} Node and edge changes across checkpoint transitions. 
\textbf{Bottom Right:} Change rate of nodes and edges during fine-tuning.
}
\label{fig:f_200}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.85\linewidth]{Figure/Appendix/F_mul_div.pdf}
\caption{
\textbf{Comparison of Mul/Div circuits during fine-tuning with LoRA, AdaLoRA, and IA3.} 
\textbf{Top:}  Model accuracy and faithfulness across checkpoints.
\textbf{Bottom Left:} Node and edge changes across checkpoint transitions. 
\textbf{Bottom Right:} Change rate of nodes and edges during fine-tuning.
}
\label{fig:f_mul_div}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.85\linewidth]{Figure/Appendix/F_sequence.pdf}
\caption{
\textbf{Comparison of Sequence circuits during fine-tuning with LoRA, AdaLoRA, and IA3.} 
\textbf{Top:}  Model accuracy and faithfulness across checkpoints.
\textbf{Bottom Left:} Node and edge changes across checkpoint transitions. 
\textbf{Bottom Right:} Change rate of nodes and edges during fine-tuning.
}
\label{fig:f_sequence}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.85\linewidth]{Figure/Appendix/F_lcm.pdf}
\caption{
\textbf{Comparison of LCM circuits during fine-tuning with LoRA, AdaLoRA, and IA3.} 
\textbf{Top:}  Model accuracy and faithfulness across checkpoints.
\textbf{Bottom Left:} Node and edge changes across checkpoint transitions. 
\textbf{Bottom Right:} Change rate of nodes and edges during fine-tuning.
}
\label{fig:f_lcm}
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.85\linewidth]{Figure/Appendix/F_function.pdf}
\caption{
\textbf{Comparison of Function circuits during fine-tuning with LoRA, AdaLoRA, and IA3.} 
\textbf{Top:}  Model accuracy and faithfulness across checkpoints.
\textbf{Bottom Left:} Node and edge changes across checkpoint transitions. 
\textbf{Bottom Right:} Change rate of nodes and edges during fine-tuning.
}
\label{fig:f_function}
\end{center}
\end{figure}

The three PEFT methods each create new circuits after fine-tuning on different mathematical tasks. Additionally, we draw the following conclusions:
\begin{enumerate}
    \item \textbf{Across different PEFT methods (LoRA, AdaLoRA, IA3) and diverse math tasks, as model accuracy improves, the circuits converge while edges undergo more significant modifications than nodes, consistent with previous observations.}
    \item \textbf{Moreover, IA3 has lower node and edge change rates compared to the other two PEFT methods, which can be attributed to its smaller number of trainable parameters.}
\end{enumerate}
Hence, the choice among these three PEFT methods does not alter our primary conclusions that edges exhibit greater changes than nodes, and that the circuits ultimately converge as accuracy increases. 

\section{Circuit Changes During Fine-Tuning: Full Parameter Fine-Tuning vs. LoRA}
\label{appendixG}
This appendix compares the circuit changes during fine-tuning between Full Parameter Fine-Tuning (Full-FT) and LoRA. Figure~\ref{fig:g_ft_lora} presents the model accuracy and faithfulness at different checkpoints, along with the node and edge modifications and their respective change rates. By examining these metrics, we can observe the similarities and differences in circuit evolution under these two fine-tuning approaches.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.85\linewidth]{Figure/Appendix/G_ft_lora.pdf}
\caption{
\textbf{Comparison of circuit changes during fine-tuning between Full Parameter Fine-Tuning (Full-FT) and LoRA.} 
\textbf{Top:}  Model accuracy and faithfulness across checkpoints.
\textbf{Bottom Left:} Node and edge changes across checkpoint transitions. 
\textbf{Bottom Right:} Change rate of nodes and edges during fine-tuning.
}
\label{fig:g_ft_lora}
\end{center}
\end{figure}

\textbf{Full parameter fine-tuning and LoRA exhibit highly similar convergence trends in circuit evolution.} Therefore, both full parameter fine-tuning and parameter-efficient fine-tuning can create new circuit structures. In full parameter fine-tuning, the changes in edges are significantly greater than those in nodes, which is consistent with the previous findings using LoRA. 

\section{Circuit Changes During Fine-Tuning: A Comparison Across Different Large Language Models}
\label{appendixH}
In this appendix, we compare circuit changes across different large language models (LLMs), including \textit{pythia-1.4B-deduped}, \textit{gpt-neo-2.7B}, and \textit{opt-6.7B}. Figure~\ref{fig:H} illustrates their accuracy and faithfulness during fine-tuning, as well as the modifications of nodes and edges through the checkpoints and the corresponding change rates. By examining these metrics, we can gain insights into how the internal circuits evolve under different model sizes and architectures.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.85\linewidth]{Figure/Appendix/H_llms.pdf}
\caption{
\textbf{Comparison of circuit fine-tuning changes between different LLMs (pythia-1.4B-deduped, gpt-neo-2.7B, opt-6.7B).} 
\textbf{Top:}  Model accuracy and faithfulness across checkpoints.
\textbf{Bottom Left:} Node and edge changes across checkpoint transitions. 
\textbf{Bottom Right:} Change rate of nodes and edges during fine-tuning.
}
\label{fig:H}
\end{center}
\end{figure}

\textbf{Circuits in the addition/subtraction task converge across all models, with edge change rates consistently exceeding node change rates.} Fine-tuning in each model creates new circuits through significant reconfiguration of connections among components.

\textbf{Larger models exhibit higher node and edge change rates.} As shown in Figure~\ref{fig:H}, the opt-6.7B model demonstrates the largest change rates, while the faithfulness of its discovered circuit remains stable throughout fine-tuning.

Therefore, using different model architectures and sizes, we consistently observe the conclusions presented in Section~\ref{edge_change} and Section~\ref{change_result}: \textit{Circuit Convergence During Fine-Tuning} and \textit{Reorganization of Circuit Edges to Form a New Circuit}.

\section{CircuitLoRA Performance on Other Tasks}
\label{appendixI}
In this appendix, we extend our investigation of \texttt{CircuitLoRA} to additional tasks beyond those discussed in the main text. These tasks include a variety of numerical operations, such as addition and subtraction with varying ranges, to further examine the performance and robustness of our circuit-aware fine-tuning approach. By testing \texttt{CircuitLoRA} on these additional benchmarks, we aim to provide a more comprehensive evaluation, highlighting how incorporating circuit-based insights can yield consistent gains across a broader set of mathematical tasks.

\begin{table}[h!]
\centering
\caption{\textbf{Performance metrics for addition and subtraction across various configurations in different numerical ranges.} The control groups of \texttt{CircuitLoRA} (\(r_o=8\), \(r_c=32\)) are LoRA (\(r_o=16\)) and RandomLoRA (\(r_o=8\), \(r_c=32\)), and the control groups of \texttt{CircuitLoRA} (\(r_o=32\), \(r_c=64\)) are LoRA (\(r_o=32\)) and RandomLoRA (\(r_o=32\), \(r_c=64\)). Here, \(r_o\) and \(r_c\) represent the ranks used in \texttt{CircuitLoRA}, where \(r_c\) is the rank for \textit{critical layer} modules, and \(r_o\) is the rank for \textit{non-critical layer} modules. Model Accuracy is expressed as percentages.}
\label{H_result}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Parameter Ratio} & \textbf{Add/Sub(100)} & \textbf{Add/Sub(200)} & \textbf{Add/Sub(400)} & \textbf{Add/Sub(500)} \\
\midrule
Pre-trained & 0\% & 46.90 & 24.90 & 12.20 & 9.90 \\
Full FT & 100\% & 96.80 & 90.50 & 75.30 & 63.60 \\
LoRA (\(r_o=2\)) & 0.1111\% & 94.40 & 82.90 & 68.90 & 55.60 \\
LoRA (\(r_o=8\)) & 0.4428\% & 95.40 & 86.40 & 73.10 & 64.30 \\
LoRA (\(r_o=16\)) & 0.8816\% & 96.70 & 87.80 & 77.90 & 68.30 \\
LoRA (\(r_o=32\)) & 1.7479\% & 97.40 & 90.30 & 78.20 & 69.70 \\
\midrule
\rowcolor[gray]{0.9}
\textbf{\texttt{CircuitLoRA} (\(r_o=8\), \(r_c=32\))} & 0.7175\% & \textbf{96.90} & \textbf{90.40} & \textbf{77.90} & \textbf{70.60} \\
RandomLoRA (\(r_o=8\), \(r_c=32\)) & 0.7175\% & 95.70 & 87.30 & 73.30 & 63.70 \\
\rowcolor[gray]{0.9}
\textbf{\texttt{CircuitLoRA} (\(r_o=32\), \(r_c=64\))} & 1.4248\% & \textbf{97.90} & \textbf{91.00} & \textbf{78.20} & \textbf{73.00} \\
RandomLoRA (\(r_o=32\), \(r_c=64\)) & 1.4248\% & 97.00 & 89.60 & 77.70 & 64.20 \\
\bottomrule
\end{tabular}%
}
\end{table}

In summary, the results presented in Table~\ref{H_result} demonstrate that \texttt{CircuitLoRA} maintains its advantage over both LoRA and RandomLoRA baselines across multiple configurations and numerical ranges. Even when the parameter ratio is constrained, \texttt{CircuitLoRA} effectively identifies and prioritizes \textit{Critical Layers}, ensuring superior accuracy compared to methods that allocate ranks uniformly or randomly. These findings further validate the effectiveness of circuit-based analysis in enhancing fine-tuning efficiency and performance, reinforcing \textbf{Key Observation 3}: \textit{Circuits can improve fine-tuning by achieving higher accuracy and parameter efficiency across various mathematical tasks}. In the addition and subtraction task, we can see that after using \texttt{CircuitLoRA} (\(r_o=8\), \(r_c=32\)), we can achieve almost the same accuracy or even higher with half the training parameters of LoRA (\(r_o=32\)).

\section{Examples of Compositional Task}
\label{appendixJ}
Our compositional task involve two-step arithmetic operations, requiring reasoning across different mathematical operations. This task requires the model to perform addition and subtraction operations first, and then multiplication and division operations. The following examples demonstrate a diverse set of arithmetic problems designed for this purpose.

\textbf{Example:}
\begin{itemize}
    \item \texttt{Clean: Calculate the result of the following arithmetic expression and provide only the final answer: (43 - 7) * 21 =}
    \item \texttt{Corrupted: Calculate the result of the following arithmetic expression and provide only the final answer: (43 - 7) * 88 =}
\end{itemize}

\textbf{Example:}
\begin{itemize}
    \item \texttt{Clean: Calculate the result of the following arithmetic expression and provide only the final answer: (82 - 43) / 13 =}
    \item \texttt{Corrupted: Calculate the result of the following arithmetic expression and provide only the final answer: (82 - 43) / 3 =}
\end{itemize}

These tasks demonstrate each example in this task can be divided into two subtasks: addition and subtraction tasks and multiplication and division tasks. The purpose of this task is to see whether the circuit in the combination task can be approximately replaced by the union of the circuits of the two subtasks. This provides ideas and experimental basis for exploring more complex combination tasks.

\end{document}
