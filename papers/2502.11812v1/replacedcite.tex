\section{Related work}
\subsection{Mechanistic Interpretability}
Mechanistic Interpretability investigates how components in large language models process and represent information____.
At present, many MI studies have been applied in various fields of AI Safety. For instance, oversimplified probes risk____, unlearning fabricated knowledge____, reducing toxicity via alignment____, mitigating hallucinations by editing representations____, and generating truthful outputs through inference-time interventions____. Other studies explore how local model edits propagate across tasks____, Multi-Head Attention in-context learning____ and enhance influence-function sampling____. Specifically, our study examines how circuits evolve during fine-tuning for mathematical tasks, focusing on node and edge changes to reveal mechanisms behind performance improvements.

\subsection{Circuit Analysis and Fine-Tuning}
One direction of Circuit Analysis focuses on building complete circuits. Early work localizes factual associations in mid-layer modules____ and uses causal mediation to uncover biases____. Automated methods like Automated Circuit Discovery identify significant units____, while techniques like attribution patching, and refine circuit extraction by handling near-zero gradients____. Edge pruning____ provide insights into building the edge of the circuit. Another line of research investigates the functional roles of circuit components, such as Attention heads____ and Feed Forward Networks (FFNs) / MLPs____. 
Additionally, circuits have been used to analyze specific tasks, such as factual knowledge retrieval____, arithmetic computation____, Greater Than task____, and circuit recognition in Indirect Object Identification____. Unlike these analyses, which focus on smaller-scale tasks and models, our work offers a new lens on how circuits evolve specifically during fine-tuning on mathematical tasks, revealing crucial roles of edge changes.

As pre-trained language models scale, fine-tuning methods have emerged, optimizing only a small subset of parameters____. Parameter-efficient fine-tuning (PEFT) methods, such as LoRA____, reduce computational costs while preserving functionality____. Advances in LoRA, including pruning____ and adaptive budget allocation____, further improve efficiency. In our study, we introduce a circuit-aware LoRA approach that adaptively assigns higher ranks to layers with more edge changes, boosting efficiency and accuracy in mathematical tasks, and further illustrates how combining circuits from subtasks can enhance performance in compositional tasks during fine-tuning.