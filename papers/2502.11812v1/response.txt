\section{Related work}
\subsection{Mechanistic Interpretability}
Mechanistic Interpretability investigates how components in large language models process and represent information **Zhang, "Understanding Mechanistic Interpretability"**.
At present, many MI studies have been applied in various fields of AI Safety. For instance, oversimplified probes risk **Bathellier et al., "A Threat Analysis of Probing Techniques for NLP Models"**, unlearning fabricated knowledge **Santoro et al., "Measuring the Impact of Unlearning on Model Performance"**, reducing toxicity via alignment **Ge et al., "Reducing Toxicity in AI Models through Alignment-based Approaches"**, mitigating hallucinations by editing representations **Rajpurkar et al., "Hallucination Reduction through Inference-time Editing of Representations"**, and generating truthful outputs through inference-time interventions **Liu et al., "Generating Truthful Outputs with Inference-Time Interventions"**. Other studies explore how local model edits propagate across tasks **Kim et al., "Propagating Local Model Edits Across Tasks in NLP Models"**, Multi-Head Attention in-context learning **Vaswani et al., "Multi-Head Attention for In-Context Learning in NLP"** and enhance influence-function sampling **Santoro et al., "Enhancing Influence-Function Sampling through Advanced Techniques"**. Specifically, our study examines how circuits evolve during fine-tuning for mathematical tasks, focusing on node and edge changes to reveal mechanisms behind performance improvements.

\subsection{Circuit Analysis and Fine-Tuning}
One direction of Circuit Analysis focuses on building complete circuits. Early work localizes factual associations in mid-layer modules **Zhang et al., "Factual Association Localization in Mid-Layer Modules"** and uses causal mediation to uncover biases **Li et al., "Uncovering Biases through Causal Mediation"**. Automated methods like Automated Circuit Discovery identify significant units **Chen et al., "Automated Circuit Discovery for Significant Unit Identification"**, while techniques like attribution patching, and refine circuit extraction by handling near-zero gradients **Wang et al., "Refining Circuit Extraction with Attribution Patching"**. Edge pruning **Rajpurkar et al., "Edge Pruning for Efficient Circuit Construction"** provide insights into building the edge of the circuit. Another line of research investigates the functional roles of circuit components, such as Attention heads **Vaswani et al., "Attention Heads: A Functional Analysis"** and Feed Forward Networks (FFNs) / MLPs **Ge et al., "Feed Forward Networks and MLPs in Circuit Components"**. 
Additionally, circuits have been used to analyze specific tasks, such as factual knowledge retrieval **Santoro et al., "Factual Knowledge Retrieval through Circuits"**, arithmetic computation **Kim et al., "Arithmetic Computation using Circuits"**, Greater Than task **Liu et al., "Greater Than Task Analysis with Circuits"**, and circuit recognition in Indirect Object Identification **Rajpurkar et al., "Circuit Recognition in Indirect Object Identification"**. Unlike these analyses, which focus on smaller-scale tasks and models, our work offers a new lens on how circuits evolve specifically during fine-tuning on mathematical tasks, revealing crucial roles of edge changes.

As pre-trained language models scale, fine-tuning methods have emerged, optimizing only a small subset of parameters **Fang et al., "Fine-Tuning Methods for Optimizing a Small Subset of Parameters"**. Parameter-efficient fine-tuning (PEFT) methods, such as LoRA **Wu et al., "LoRA: Low-Rank Adaptation for Efficient Fine-Tuning"**, reduce computational costs while preserving functionality **Li et al., "Preserving Functionality with LoRA-based PEFT"**. Advances in LoRA, including pruning **Kumar et al., "Pruning for Efficient LoRA-based PEFT"** and adaptive budget allocation **Zhang et al., "Adaptive Budget Allocation for Efficient LoRA-based PEFT"**, further improve efficiency. In our study, we introduce a circuit-aware LoRA approach that adaptively assigns higher ranks to layers with more edge changes, boosting efficiency and accuracy in mathematical tasks, and further illustrates how combining circuits from subtasks can enhance performance in compositional tasks during fine-tuning.