@inproceedings{NEURIPS2022_6f1d43d5,
 author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {17359--17372},
 publisher = {Curran Associates, Inc.},
 title = {Locating and Editing Factual Associations in GPT},
 volume = {35},
 year = {2022}
}

@inproceedings{NEURIPS2023_34e1dbe9,
 author = {Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri\`{a}},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {16318--16352},
 publisher = {Curran Associates, Inc.},
 title = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
 volume = {36},
 year = {2023}
}

@inproceedings{NEURIPS2023_3927bbdc,
 author = {Hase, Peter and Bansal, Mohit and Kim, Been and Ghandeharioun, Asma},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {17643--17668},
 publisher = {Curran Associates, Inc.},
 title = {Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models},
 volume = {36},
 year = {2023}
}

@inproceedings{NEURIPS2023_81b83900,
 author = {Li, Kenneth and Patel, Oam and Vi\'{e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {41451--41530},
 publisher = {Curran Associates, Inc.},
 title = {Inference-Time Intervention: Eliciting Truthful Answers from a Language Model},
 volume = {36},
 year = {2023}
}

@inproceedings{NEURIPS2023_efbba771,
 author = {Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {76033--76060},
 publisher = {Curran Associates, Inc.},
 title = {How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model},
 volume = {36},
 year = {2023}
}

@inproceedings{bhaskar2024findingtransformercircuitsedge,
  title={Finding Transformer Circuits with Edge Pruning},
  author={Adithya Bhaskar and Alexander Wettig and Dan Friedman and Danqi Chen},
  booktitle={Proceedings of the 38th Conference on Neural Information Processing Systems (NeurIPS), Spotlight},
  year={2024},
  url={https://arxiv.org/abs/2406.16778},
  note={NeurIPS 2024 Spotlight}
}

@misc{bhattacharya2024understandingroleffnsdriving,
      title={Understanding the role of FFNs in driving multilingual behaviour in LLMs}, 
      author={Sunit Bhattacharya and Ondřej Bojar},
      year={2024},
      eprint={2404.13855},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.13855}, 
}

@misc{cabannes2024iterationheadmechanisticstudy,
      title={Iteration Head: A Mechanistic Study of Chain-of-Thought}, 
      author={Vivien Cabannes and Charles Arnal and Wassim Bouaziz and Alice Yang and Francois Charton and Julia Kempe},
      year={2024},
      eprint={2406.02128},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.02128}, 
}

@inproceedings{chen2024can,
  title={What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks},
  author={Chen, Xingwu and Zou, Difan},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{chen2024transformers,
  title={How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression},
  author={Chen, Xingwu and Zhao, Lei and Zou, Difan},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024}
}

@article{cohen2024evaluating,
  title={Evaluating the ripple effects of knowledge editing in language models},
  author={Cohen, Roi and Biran, Eden and Yoran, Ori and Globerson, Amir and Geva, Mor},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={283--298},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{ding2023parameter,
  title={Parameter-efficient fine-tuning of large-scale pre-trained language models},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={Nature Machine Intelligence},
  volume={5},
  number={3},
  pages={220--235},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{friedman2024interpretabilityillusionsgeneralizationsimplified,
  title={Interpretability Illusions in the Generalization of Simplified Models},
  author={Dan Friedman and Andrew Lampinen and Lucas Dixon and Danqi Chen and Asma Ghandeharioun},
  booktitle={Proceedings of the 41st International Conference on Machine Learning (ICML)},
  year={2024},
  url={https://arxiv.org/abs/2312.03656},
  note={ICML 2024}
}

@inproceedings{geva2021transformerfeedforwardlayerskeyvalue,
  title={Transformer Feed-Forward Layers Are Key-Value Memories},
  author={Mor Geva and Roei Schuster and Jonathan Berant and Omer Levy},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2021},
  url={https://arxiv.org/abs/2012.14913},
  note={EMNLP 2021}
}

@inproceedings{geva2022transformerfeedforwardlayersbuild,
  title={Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space},
  author={Mor Geva and Avi Caciularu and Kevin Ro Wang and Yoav Goldberg},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2022},
  url={https://arxiv.org/abs/2203.14680},
  note={EMNLP 2022}
}

@inproceedings{geva2023dissectingrecallfactualassociations,
  title={Dissecting Recall of Factual Associations in Auto-Regressive Language Models},
  author={Mor Geva and Jasmijn Bastings and Katja Filippova and Amir Globerson},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023},
  url={https://arxiv.org/abs/2304.14767},
  note={EMNLP 2023}
}

@misc{gould2023successorheadsrecurringinterpretable,
      title={Successor Heads: Recurring, Interpretable Attention Heads In The Wild}, 
      author={Rhys Gould and Euan Ong and George Ogden and Arthur Conmy},
      year={2023},
      eprint={2312.09230},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.09230}, 
}

@inproceedings{hanna2024faithfaithfulnessgoingcircuit,
  title={Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms},
  author={Michael Hanna and Sandro Pezzelle and Yonatan Belinkov},
  booktitle={Proceedings of the Conference on Learning Mechanisms (COLM)},
  year={2024},
  url={https://arxiv.org/abs/2403.17806},
  note={COLM 2024}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{koh2024faithful,
  title={Faithful and Fast Influence Function via Advanced Sampling},
  author={Koh, Jungyeon and Lyu, Hyeonsu and Jang, Jonggyu and Yang, Hyun Jong},
  booktitle={ICML 2024 Workshop on Mechanistic Interpretability}
}

@misc{lee2024mechanisticunderstandingalignmentalgorithms,
      title={A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity}, 
      author={Andrew Lee and Xiaoyan Bai and Itamar Pres and Martin Wattenberg and Jonathan K. Kummerfeld and Rada Mihalcea},
      year={2024},
      eprint={2401.01967},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.01967}, 
}

@misc{lialin2024scalingscaleupguide,
      title={Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning}, 
      author={Vladislav Lialin and Vijeta Deshpande and Xiaowei Yao and Anna Rumshisky},
      year={2024},
      eprint={2303.15647},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.15647}, 
}

@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}

@misc{mcdougall2023copysuppressioncomprehensivelyunderstanding,
      title={Copy Suppression: Comprehensively Understanding an Attention Head}, 
      author={Callum McDougall and Arthur Conmy and Cody Rushing and Thomas McGrath and Neel Nanda},
      year={2023},
      eprint={2310.04625},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.04625}, 
}

@misc{meng2023masseditingmemorytransformer,
      title={Mass-Editing Memory in a Transformer}, 
      author={Kevin Meng and Arnab Sen Sharma and Alex Andonian and Yonatan Belinkov and David Bau},
      year={2023},
      eprint={2210.07229},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.07229}, 
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@inproceedings{stolfo2023mechanisticinterpretationarithmeticreasoning,
  title={A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis},
  author={Alessandro Stolfo and Yonatan Belinkov and Mrinmaya Sachan},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023},
  url={https://arxiv.org/abs/2305.15054},
  note={EMNLP 2023}
}

@inproceedings{sun2024learningunlearningfabricatedknowledge,
  title={Learning and Unlearning of Fabricated Knowledge in Language Models},
  author={Chen Sun and Nolan Andrew Miller and Andrey Zhmoginov and Max Vladymyrov and Mark Sandler},
  booktitle={Proceedings of the ICML 2024 Workshop on Mechanistic Interpretability},
  year={2024},
  url={https://arxiv.org/abs/2410.21750},
  note={ICML 2024 Workshop on Mechanistic Interpretability}
}

@inproceedings{syed2023attributionpatchingoutperformsautomated,
  title={Attribution Patching Outperforms Automated Circuit Discovery},
  author={Aaquib Syed and Can Rager and Arthur Conmy},
  booktitle={Proceedings of the NeurIPS 2023 ATTRIB Workshop},
  year={2023},
  url={https://arxiv.org/abs/2310.10348},
  note={NeurIPS 2023 ATTRIB Workshop}
}

@misc{vig2020causalmediationanalysisinterpreting,
      title={Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias}, 
      author={Jesse Vig and Sebastian Gehrmann and Yonatan Belinkov and Sharon Qian and Daniel Nevo and Simas Sakenis and Jason Huang and Yaron Singer and Stuart Shieber},
      year={2020},
      eprint={2004.12265},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.12265}, 
}

@misc{wang2022interpretabilitywildcircuitindirect,
      title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small}, 
      author={Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
      year={2022},
      eprint={2211.00593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.00593}, 
}

@inproceedings{wang2024knowledgemechanismslargelanguage,
  title={Knowledge Mechanisms in Large Language Models: A Survey and Perspective},
  author={Mengru Wang and Yunzhi Yao and Ziwen Xu and Shuofei Qiao and Shumin Deng and Peng Wang and Xiang Chen and Jia-Chen Gu and Yong Jiang and Pengjun Xie and Fei Huang and Huajun Chen and Ningyu Zhang},
  booktitle={Proceedings of EMNLP 2024 Findings},
  year={2024},
  pages={1--39},  
  url={https://arxiv.org/abs/2407.15017},
  note={EMNLP 2024 Findings; 39 pages (v4)}
}

@misc{wu2024retrievalheadmechanisticallyexplains,
      title={Retrieval Head Mechanistically Explains Long-Context Factuality}, 
      author={Wenhao Wu and Yizhong Wang and Guangxuan Xiao and Hao Peng and Yao Fu},
      year={2024},
      eprint={2404.15574},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.15574}, 
}

@inproceedings{zhang2023adaloraadaptivebudgetallocation,
  title={AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning},
  author={Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
  booktitle={Proceedings of the 11th International Conference on Learning Representations (ICLR)},
  year={2023},
  url={https://arxiv.org/abs/2303.10512},
  note={ICLR 2023}
}

@inproceedings{zhang2024truthxalleviatinghallucinationsediting,
  title={TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space},
  author={Shaolei Zhang and Tian Yu and Yang Feng},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2024},
  url={https://arxiv.org/abs/2402.17811},
  note={ACL 2024 Main Conference}
}

@misc{zhou2024loradropefficientloraparameter,
      title={LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation}, 
      author={Hongyun Zhou and Xiangyu Lu and Wang Xu and Conghui Zhu and Tiejun Zhao and Muyun Yang},
      year={2024},
      eprint={2402.07721},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.07721}, 
}

