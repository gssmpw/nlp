\section{Related Work}
\subsection{Hallucination in lower-resource languages}
In the context of generative LLMs, hallucination describes a variegated array of phenomena involving the output either not being factual or otherwise not adhering to the user's instructions \citep{huang2023survey}. Hallucinated output has serious safety and ethical implications, as it could for instance contribute directly to misinformation \citep{huang2023survey}. For tractability, this study focusses only on faithfulness-related hallucination, in particular scenarios where LLMs produce inaccurate summaries of longer texts \citep[cf.][]{qiu2023detecting}.

Much work has indicated that insufficient good-quality data is one of the main factors behind unfaithfulness and other forms of hallucination \citep{huang2023survey}. Unfortunately, the low-data regimes that many lower-resource languages instantiate effectively rule out data-hungry methods such as retrieval-augmented generation (RAG) that have been used to mitigate hallucination with some success \citep[cf.][]{kirchenbauer2024hallucination, tonmoy2024comprehensive}. In this study, we hence propose rescoring as a computationally light alternative to deal with unfaithful generation.

\subsection{Rescoring}
\label{sec:rescoring}
Rescoring, also known as reranking, is a set of methods especially common in machine translation and speech recognition, where an auxiliary model, a BERT variant for example, is used to refine the outputs of a larger, mostly generative, foundation model \citep{khedar2024automatic}. Because such auxiliary models typically do not require vast amounts of data to train, they have been used for lower-resource languages for many years now, especially in situations implicating information retrieval \citep[e.g.][]{lee2014graph, litschko2022parameter}. 

Discriminative reranking in particular has been productively applied to various use cases \citep{shen2004discriminative, mizumoto2016discriminative}. It involves the larger generative model producing \textit{N} best candidate outputs via beam search for instance, whereupon the smaller auxiliary model rescores these to optimise a ranking objective and lead to more robust decisions. The auxiliary model could either be coupled to the foundation model for inference or used to fine-tune it. However, to the best of our knowledge, the potential of rescoring has thus far not been exploited to improve LLM faithfulness. It is moreover unclear to what extent the hyperparameters of auxiliary models need to be adapted to the typological features of each low-resource language for optimal performance.

\subsection{Morphology and hyperparameter choice}
\label{sec:morphology_hyperparameters}
The last point above is especially crucial as lower-resource languages have hugely diverse typological traits, forming a class only by virtue of not being English \citep{jin2024better}. Despite the widespread application of auxiliary models to these languages, not much research exists to our best knowledge on whether or how to optimise the models' hyperparameters in a manner sensitive to their morphological characteristics. \citet{otmakhova2022cross} is a partial exception. For a more nuanced perspective, we approach the issue from the following three angles:

\textbf{Regularisation}. It is widely reported that morphologically simple languages tend to have a lower type-to-token ratio, which correlates very strongly with lower perplexity \citep[e.g.][]{mielke2019kind, levshina2022frequency}. We could therefore theorise that in low-data regimes, models trained on morphologically simple languages would be more exposed to overfitting than those trained on morphologically complex ones and would benefit from stronger regularisation. This assumption finds support in \citet{otmakhova2022cross}'s observations on model depth, which the next paragraph turns to.

\textbf{Model depth}. Work on this front is more limited, but \citet{otmakhova2022cross}'s study shows that complex morphology is learnt in the upper layers of monolingual BERT-Base models. This leads them to conjecture that morphologically complex languages require deeper models than morphologically simpler ones to learn properly. Conversely, they also report that overly deep models trained on morphologically simpler languages cause a drop in performance, potentially indicating that shallow architectures would be best for these languages. 

\textbf{Training objectives}. To train the original BERT, masked language modelling (MLM) and next-sentence prediction (NSP) were used in conjunction \citep{devlin2019bert}. These are, however, not the only options, with one notable alternative to the latter objective being sentence-order prediction (SOP) \citep{lan2019albert}. While its designers claim SOP to potentially be more helpful for tasks relating to discourse coherence, we are not aware of work that has rigorously tested if the purported advantages of alternative objective functions hold across languages. Given that morphologically complex languages are associated with freer word order \citep[e.g.][]{nijs2025word}, we would expect NSP to be less effective for these languages especially in low-resource settings, as their flexible word order would lead to more possible variations of any given sentence and correspondingly higher perplexity. SOP, on the other hand, deals with higher-level discourse features less directly connected to word order, and could thus be justifiably hypothesised to better facilitate the modelling of morphologically complex languages.