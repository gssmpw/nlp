@article{devlin2019bert,
  title={{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={arXiv preprint arXiv:1810.04805},
  year={2019},
  url={https://aclanthology.org/N19-1423/}
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={arXiv preprint arXiv:2311.05232},
  year={2023},
  url={https://ar5iv.labs.arxiv.org/html/2311.05232}
}

@inproceedings{jin2024better,
  title={Better to ask in English: Cross-lingual evaluation of large language models for healthcare queries},
  author={Jin, Yiqiao and Chandra, Mohit and Verma, Gaurav and Hu, Yibo and De Choudhury, Munmun and Kumar, Srijan},
  booktitle={Proceedings of the ACM on Web Conference 2024},
  pages={2627--2638},
  year={2024},
  url={https://dl.acm.org/doi/pdf/10.1145/3589334.3645643}
}

@article{khedar2024automatic,
title = {Automatic speech recognition using advanced deep learning approaches: A survey},
journal = {Information Fusion},
volume = {109},
pages = {102422},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102422},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524002008},
author = {Hamza Kheddar and Mustapha Hemis and Yassine Himeur},
keywords = {Automatic speech recognition, Deep transfer learning, Transformers, Federated learning, Reinforcement learning},
abstract = {Recent advancements in deep learning (DL) have posed a significant challenge for automatic speech recognition (ASR). ASR relies on extensive training datasets, including confidential ones, and demands substantial computational and storage resources. Enabling adaptive systems improves ASR performance in dynamic environments. DL techniques assume training and testing data originate from the same domain, which is not always true. Advanced DL techniques like deep transfer learning (DTL), federated learning (FL), and deep reinforcement learning (DRL) address these issues. DTL allows high-performance models using small yet related datasets, FL enables training on confidential data without dataset possession, and DRL optimizes decision-making in dynamic environments, reducing computation costs. This survey offers a comprehensive review of DTL, FL, and DRL-based ASR frameworks, aiming to provide insights into the latest developments and aid researchers and professionals in understanding the current challenges. Additionally, Transformers, which are advanced DL techniques heavily used in proposed ASR frameworks, are considered in this survey for their ability to capture extensive dependencies in the input ASR sequence. The paper starts by presenting the background of DTL, FL, DRL, and Transformers and then adopts a well-designed taxonomy to outline the state-of-the-art (SOTA) approaches. Subsequently, a critical analysis is conducted to identify the strengths and weaknesses of each framework. Additionally, a comparative study is presented to highlight the existing challenges, paving the way for future research opportunities.}
}

@article{kirchenbauer2024hallucination,
  title={Hallucination reduction in large language models with retrieval-augmented generation using {W}ikipedia knowledge},
  author={Kirchenbauer, Jason and Barns, Caleb},
  year={2024},
  publisher={OSF},
  url={https://files.osf.io/v1/resources/pv7r5/providers/osfstorage/6657166cd835c421594ce333?format=pdf&action=download&direct&version=1}
}

@article{lan2019albert,
  title={Albert: A lite {BERT} for self-supervised learning of language representations},
  author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019},
  url={https://arxiv.org/pdf/1909.11942}
}

@inproceedings{lee2014graph,
  title={Graph-based re-ranking using acoustic feature similarity between search results for spoken term detection on low-resource languages},
  author={Lee, Hung-yi and Zhang, Yu and Chuangsuwanich, Ekapol and Glass, James R},
  booktitle={Fifteenth Annual Conference of the International Speech Communication Association},
  year={2014},
  url={https://www.isca-archive.org/interspeech_2014/lee14c_interspeech.pdf}
}

@Article{levshina2022frequency,
AUTHOR = {Levshina, Natalia},
TITLE = {Frequency, Informativity and Word Length: Insights from Typologically Diverse Corpora},
JOURNAL = {Entropy},
VOLUME = {24},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {280},
URL = {https://www.mdpi.com/1099-4300/24/2/280},
PubMedID = {35205578},
ISSN = {1099-4300},
ABSTRACT = {Zipfâ€™s law of abbreviation, which posits a negative correlation between word frequency and length, is one of the most famous and robust cross-linguistic generalizations. At the same time, it has been shown that contextual informativity (average surprisal given previous context) is more strongly correlated with word length, although this tendency is not observed consistently, depending on several methodological choices. The present study examines a more diverse sample of languages than the previous studies (Arabic, Finnish, Hungarian, Indonesian, Russian, Spanish and Turkish). I use large web-based corpora from the Leipzig Corpora Collection to estimate word lengths in UTF-8 characters and in phonemes (for some of the languages), as well as word frequency, informativity given previous word and informativity given next word, applying different methods of bigrams processing. The results show different correlations between word length and the corpus-based measure for different languages. I argue that these differences can be explained by the properties of noun phrases in a language, most importantly, by the order of heads and modifiers and their relative morphological complexity, as well as by orthographic conventions.},
DOI = {10.3390/e24020280}
}

@article{litschko2022parameter,
  title={Parameter-efficient neural reranking for cross-lingual and multilingual retrieval},
  author={Litschko, Robert and Vuli{\'c}, Ivan and Glava{\v{s}}, Goran},
  journal={arXiv preprint arXiv:2204.02292},
  year={2022},
  url={https://arxiv.org/pdf/2204.02292}
}

@inproceedings{mielke2019kind,
    title = "What Kind of Language Is Hard to Language-Model?",
    author = "Mielke, Sabrina J.  and
      Cotterell, Ryan  and
      Gorman, Kyle  and
      Roark, Brian  and
      Eisner, Jason",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url={https://aclanthology.org/P19-1491/},
    doi = "10.18653/v1/P19-1491",
    pages = "4975--4989",
    abstract = "How language-agnostic are current state-of-the-art NLP tools? Are there some types of language that are easier to model with current methods? In prior work (Cotterell et al., 2018) we attempted to address this question for language modeling, and observed that recurrent neural network language models do not perform equally well over all the high-resource European languages found in the Europarl corpus. We speculated that inflectional morphology may be the primary culprit for the discrepancy. In this paper, we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual Bible corpus. Methodologically, we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora. In other words, the model is aware of inter-sentence variation and can handle missing data. Exploiting this model, we show that {\textquotedblleft}translationese{\textquotedblright} is not any easier to model than natively written language in a fair comparison. Trying to answer the question of what features difficult languages have in common, we try and fail to reproduce our earlier (Cotterell et al., 2018) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample."
}

@inproceedings{mizumoto2016discriminative,
  title={Discriminative reranking for grammatical error correction with statistical machine translation},
  author={Mizumoto, Tomoya and Matsumoto, Yuji},
  booktitle={Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1133--1138},
  year={2016},
  url={https://aclanthology.org/N16-1133.pdf}
}

@article{nijs2025word,
  title={Is word order responsive to morphology? {Disentangling} cause and effect in morphosyntactic change in five {Western European} languages},
  author={Nijs, Julie and Van de Velde, Freek and Cuyckens, Hubert},
  journal={Entropy},
  volume={27},
  number={1},
  pages={53},
  year={2025},
  url={https://pmc.ncbi.nlm.nih.gov/articles/PMC11765092/}
}

@inproceedings{otmakhova2022cross,
    title = "Cross-linguistic Comparison of Linguistic Feature Encoding in {BERT} Models for Typologically Different Languages",
    author = "Otmakhova, Yulia  and
      Verspoor, Karin  and
      Lau, Jey Han",
    editor = "Vylomova, Ekaterina  and
      Ponti, Edoardo  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the 4th Workshop on Research in Computational Linguistic Typology and Multilingual NLP",
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.sigtyp-1.4/",
    doi = "10.18653/v1/2022.sigtyp-1.4",
    pages = "27--35",
    abstract = "Though recently there have been an increased interest in how pre-trained language models encode different linguistic features, there is still a lack of systematic comparison between languages with different morphology and syntax. In this paper, using BERT as an example of a pre-trained model, we compare how three typologically different languages (English, Korean, and Russian) encode morphology and syntax features across different layers. In particular, we contrast languages which differ in a particular aspect, such as flexibility of word order, head directionality, morphological type, presence of grammatical gender, and morphological richness, across four different tasks."
}

@article{qiu2023detecting,
  title={Detecting and mitigating hallucinations in multilingual summarisation},
  author={Qiu, Yifu and Ziser, Yftah and Korhonen, Anna and Ponti, Edoardo M and Cohen, Shay B},
  journal={arXiv preprint arXiv:2305.13632},
  year={2023},
  url={https://arxiv.org/pdf/2305.13632}
}

@inproceedings{shen2004discriminative,
  title={Discriminative reranking for machine translation},
  author={Shen, Libin and Sarkar, Anoop and Och, Franz Josef},
  booktitle={Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004},
  pages={177--184},
  year={2004},
  url={https://aclanthology.org/N04-1023.pdf}
}

@misc{tonmoy2024comprehensive,
      title={A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models}, 
      author={S. M Towhidul Islam Tonmoy and S M Mehedi Zaman and Vinija Jain and Anku Rani and Vipula Rawte and Aman Chadha and Amitava Das},
      year={2024},
      eprint={2401.01313},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.01313}, 
}

