\section{Related Work}
In this section, we first provide an overview of existing methods for uncertainty estimation, especially for monocular depth estimation. Then, we discuss several methods that use gradients to analyse the robustness of neural networks. Finally, we present recent work on monocular depth estimation.
\subsection{Uncertainty Estimation}
Neural network uncertainties are mainly categorised into epistemic and aleatoric uncertainties____. While epistemic uncertainty refers to model uncertainty resulting from lack of knowledge, aleatoric uncertainty refers to data uncertainty caused by noise such as reflections or occlusions____. The existing approaches to uncertainty estimation can be mainly divided into empirical, predictive, and post hoc methods, depending on how the uncertainty is determined. They deal with either epistemic or aleatoric uncertainty or both, the so-called predictive uncertainty. In the following, we will discuss different methods and the addressed uncertainty type in more detail. 
\subsubsection{Empirical Methods}
Empirical uncertainty estimation methods place a distribution over the model weights and therefore address the epistemic uncertainty. To this end, bootstrapped ensembles____ train multiple models with different initialisation to compute mean and variance over their outputs as prediction and uncertainty measures, respectively. Snapshot ensembles____ remove the training overhead by leveraging cyclic learning rate scheduling. Monte Carlo (MC) Dropout____, on the other hand, applies dropout____ during training and takes multiple samples with dropout enabled during inference which avoids storing multiple models. 
\subsubsection{Predictive Methods}
In contrast, predictive approaches maximise the log-likelihood by placing a distribution over the model output, which in turn accounts for aleatoric uncertainty. In regression tasks, the Laplacian____ or Gaussian____ distribution can be used to predict mean and variance as depth estimate and uncertainty measure, respectively. Klodt \textit{et al.}____, transfer this to the photometric loss used in self-supervised monocular depth estimation. Poggi \textit{et al.}____ apply a self-learning paradigm in which a second model is trained in a supervised manner with depth predictions from a self-supervised model to learn the depth and corresponding uncertainty prediction by maximising the log-likelihood. 
Instead of targeting only one type of uncertainty, Amini \textit{et al.}____ start from the evidential distribution to distinguish between aleatoric and epistemic uncertainty.
\subsubsection{Post Hoc Methods}
All of these approaches require a training procedure tailored to uncertainties and thus may affect the depth estimation performance. Therefore, so-called \textit{post hoc} methods estimate the uncertainty of already trained models. Since model re-training is not always desirable, a second model is optimised in____ assuming the generalised Gaussian distribution to estimate the uncertainty of already trained image-to-image translation models. Similarly, we also estimate the uncertainty of fixed models but without the need to train a second model. Another approach to \textit{post hoc} uncertainty estimation is the approximation of the model output distribution by sampling with input augmentations or dropout applied only during inference____. In this work, we also estimate the uncertainty of already trained depth estimation models training-free, but without exhaustive sampling strategies, by extracting gradients with an auxiliary loss function. 

\subsection{Model Robustness by Gradient Analysis}
In neural network optimisation, gradients are used as an indication of how to adjust the model weights to learn a mapping function that best represents the given inputs. For this reason, gradients are used in recent works____ to determine whether an input is well represented by the model, and thus to detect inputs that are not in the training distribution. For gradient extraction, a loss function must be defined that can be back-propagated through the neural network. While Oberdiek \textit{et al.}____ use the negative log-likelihood of the predicted class, Lee \textit{et al.}____ utilise the binary cross entropy between the logits and a confounding label defined as a vector containing only ones to identify whether the input can be associated with one of the learned classes. Huang \textit{et al.}____, in contrast, rely on the KL divergence between the Uniform distribution and the softmax output. Recently, in____, gradients are used for uncertainty estimation in object detection, whereas Maag and Riedlinger____ leverage gradients to segment unknown objects in semantic segmentation. In our work, we use the informativeness of gradients for uncertainty estimation in the computationally expensive dense regression task of monocular depth estimation. In contrast to image classification and object detection, we require a pixel-wise uncertainty score. To accomplish this, we define the auxiliary loss function as the distance between the predicted depth and a reference depth, which makes it independent of ground truth.  

\subsection{Monocular Depth Estimation}
Early works in monocular depth estimation rely on supervised training____. More recently, self-supervision with stereo pairs____ or monocular video sequences____ is used instead to reduce the need for costly ground truth collection of dense depth maps. 
While most methods rely on a static scene assumption, Bian \textit{et al.}____ and Xu \textit{et al.}____ introduce self-discovered masks and deformation-based motion representation to handle dynamic objects, respectively. With the emerging success of vision transforms____, recent works take advantage of them for monocular depth estimation____. These works integrate the attention module as backbone____, in skip connections____, in the decoder____ or combined with Conditional Random Fields____. While most works target depth estimation as a regression task, Bhat \textit{et al.}____ phrase it as a classification-regression task. 
In contrast to those supervised trained models, MonoViT____ is the first transformer-based model trained in a self-supervised manner. 
Different methods aim at zero-shot generalization to unseen inputs by training on a large amount of diverse datasets____ or using the representation capabilities of generative models____.
Therefore, Ranftl~\textit{et al.}____ propose a robust training objective to handle different depth scales and annotations.
Yang~\textit{et al.}____ combine training on both labelled and unlabelled data using a teacher-student approach to obtain pseudo-labels for the great amount of unlabelled data. Thereafter, enhancement in finer details is accomplished by replacing the labelled real data with synthetic data____.
In contrast, Ke~\textit{et al.}____ leverage the representation capabilities of pre-trained diffusion models to obtain improved generalizability in monocular depth estimation only finetuning on sythetic data.
In this work, we propose a method for uncertainty estimation for depth predictions of already trained models. Importantly, our auxiliary loss function makes our approach independent of whether the model is supervised or self-supervised trained. Furthermore, we demonstrate in our experiments that our gradient-based uncertainty is applicable to both convolutional and transformer-based models.