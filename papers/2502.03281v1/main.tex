% https://link.springer.com/journal/44007/updates/26871398
%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys-num]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{rotating}

%\input{abbrev}

%\usepackage{authblk}
\usepackage{setspace}
%\usepackage{bbold}
\usepackage{makecell}
\usepackage{cleveref}
\usepackage{subcaption}

%\RequirePackage{amsmath, amsthm}
% \RequirePackage{alltt, moreverb}           % improve verbatim
% \RequirePackage{showkeys, showidx}         % print labels, etc. in draft-mode (fold)
%\RequirePackage{graphicx}
% \RequirePackage[utf8]{inputenc}            % write with Umlaute (fold)

% \RequirePackage{algorithm}
%\RequirePackage{algpseudocode}
% \RequirePackage{colortbl}
\RequirePackage{multirow}
\RequirePackage{adjustbox}
%\RequirePackage{algorithmicx}
\RequirePackage{colortbl}
\RequirePackage{import}
\RequirePackage{hyperref}
\RequirePackage{stackrel}
\RequirePackage{wrapfig}
\RequirePackage{xspace} % create space or not after defined word
% \RequirePackage{shellesc}
\RequirePackage{bm}

\newcommand{\normtwo}[1]{\|#1\|_2}
\newcommand{\krylov}[1]{\mathcal{K}_{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\B}[1]{\boldsymbol{#1}}
\newcommand{\post}{\boldsymbol\Gamma_\text{\rm post}}
\newcommand{\postmu}{\boldsymbol\mu_\text{\rm post}}
\newcommand{\Span}[1]{\text{Span}\left\{#1\right\}}
\newcommand{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\posth}{\widehat{\boldsymbol{\Gamma}}_\text{\rm post}}
\newcommand{\xpost}{{\bfx}_\text{post}}
\newcommand{\xsamp}{\bfx^\star}
\newcommand{\ysamp}{\bfy^\star}
\newcommand{\zsamp}{\bfz^\star}
\newcommand{\lamsamp}{\lambda^\star}
\newcommand{\delsamp}{\delta^\star}
\newcommand{\xmap}{\bfx_{\rm MAP}}
\newcommand{\zmap}{\bfz_{\rm MAP}}
\newcommand{\ymap}{\bfy_{\rm MAP}}
\newcommand{\zmean}{\hat{\bfz}}
\newcommand{\xmean}{\hat{\bfx}}
\newcommand{\ymean}{\hat{\bfy}}
\newcommand{\sposth}{\widehat{\bfs}_\text{post}}
\newcommand{\gammah}{\widehat{\boldsymbol{\Gamma}}}
\newcommand{\fhat}{\hat{f}(\bfz)}
\newcommand{\fhaty}{\hat{f}_y(\bfy)}

\newcommand{\eps} {\varepsilon}                            % redefine epsilon
\newcommand{\mdot} {\,\cdot\,}
\newcommand{\D}   {{\rm D}}                                % differential D
\newcommand{\F}   {{\rm F}}                  
\newcommand{\fro}   {{\rm F}} % differential D
\newcommand{\tol} {{\rm tol}}                              % tolerance
\newcommand{\e}   {\textnormal{e}}                         % Eulerian number
\newcommand{\clos}{\textnormal{Closure}}                   % closure
\newcommand{\im}  {\textnormal{Im }}                       % imaginary part
\newcommand{\re}  {\textnormal{Re }}                       % real part
\newcommand{\hf}  {\frac{1}{2}}                            % 1/2
\newcommand{\thf}  {\tfrac{1}{2}}                            % 1/2
\DeclareMathOperator*{\argmin}{arg\,min}                   % arg min
\DeclareMathOperator*{\argmax}{arg\,max}                   % arg max
\renewcommand{\i} {\textnormal{i}}                         % imaginary unity
\renewcommand{\t} {^{\top}}                                % transpose, e.g. $A\t$
\renewcommand{\d} {{\rm d}}                                % differential d
\renewcommand{\H} {^{{\rm H}}}                             % Hermitian
\renewcommand{\vec}[1] {{\rm vec}(#1)}                     % vectorize matrix
\newcommand{\inner} [2]{\langle #1,#2\rangle}           % f.e. \inner product[\infty]{A-B}
\newcommand{\norm} [2][]{\left\|#2\right\|_{#1}}           % f.e. \norm[\infty]{A-B}
\newcommand{\abs}  [2][]{\left|#2\right|_{#1}}             % f.e. \abs[\infty]{A-B}
\newcommand{\cond} [2][]{{\rm cond}_{#1}\left( #2 \right)} % f.e. \cond[\infty]{A}
\newcommand{\pow}  [1]  {^{\left(#1\right)}}               % brackets exponent
\newcommand{\sign} [1]  {{\rm sign\!}\left( #1 \right)}    % signum
\newcommand{\rank} [1]  {{\rm rank\!}\left( #1 \right)}    % rank of a matrix
\newcommand{\diag} [1]  {{\rm diag\!}\left( #1 \right)}    % diagonal of a matrix
\newcommand{\spann}[1]  {{\rm span\!}\left( #1 \right)}    % span of a function % Careful \span conflicts with amsmath!!!!!!!!!!
\newcommand{\range}[1]  {{\rm range\!}\left( #1 \right)}    % span of a function
\renewcommand{\ker}[1]  {{\rm ker\!}\left( #1 \right)}     % kernel of a function
\newcommand{\ul}   [1]  {\underline{#1}}                   % shorted underline
% \newcommand{\email}[1]  {\texttt{#1}}                      % email
\newcommand{\kron}{\otimes}                                % Kronecker product
% \newcommand{\trace} [1]  {{\rm trace\!}\left( #1 \right)}  % trace of a matrix
\newcommand{\trace} [1]  {{\rm tr\!}\left( #1 \right)}  % trace of a matrix
% \newcommand{\acknowledgement}[1]{\noindent\textbf{Acknowledgement: }#1}   % acknowledgement
% \newcommand\figurescale{1}
\newcommand{\bfxh}{\widehat\bfx}
\newcommand{\exact} {{\rm exact}}                              % exact

\newcommand{\Var}{\mathbb{V}\mathrm{ar}}
\newcommand{\Cov}{\mathbb{C}\mathrm{ov}}
\newcommand{\iid}{iid}
\newcommand{\bigO}  [1]  {\calO\!\left(#1\right)}
% \newcommand{\tia}[1] {{\color{orange} #1}}
% \newcommand{\eric}[1]{{\color{maroon} #1}}
% small o-notation
\newcommand\calo{
  \mathchoice
    {{\scriptstyle\mathcal{O}}}% \displaystyle
    {{\scriptstyle\mathcal{O}}}% \textstyle
    {{\scriptscriptstyle\mathcal{O}}}% \scriptstyle
    {\scalebox{.7}{$\scriptscriptstyle\mathcal{O}$}}%\scriptscriptstyle
  }


  % <fold> bold letters and numbers with \bf
\newcommand{\p}{{\rm p}}                     % probability density

\newcommand{\bfGamma}{{\boldsymbol{\Gamma}}}
\newcommand{\bfDelta}{{\boldsymbol{\Delta}}}
\newcommand{\bfTheta}{{\boldsymbol{\Theta}}}
\newcommand{\bfLambda}{{\boldsymbol{\Lambda}}}
\newcommand{\bfXi}{{\boldsymbol{\Xi}}}
\newcommand{\bfPi}{{\boldsymbol{\Pi}}}
\newcommand{\bfSigma}{{\boldsymbol{\Sigma}}}
\newcommand{\bfUpsilon}{{\boldsymbol{\Upsilon}}}
\newcommand{\bfPhi}{{\boldsymbol{\Phi}}}
\newcommand{\bfPsi}{{\boldsymbol{\Psi}}}
\newcommand{\bfOmega}{{\boldsymbol{\Omega}}}
\newcommand{\bfalpha}{{\boldsymbol{\alpha}}}
\newcommand{\bfbeta}{{\boldsymbol{\beta}}}
\newcommand{\bfgamma}{{\boldsymbol{\gamma}}}
\newcommand{\bfdelta}{{\boldsymbol{\delta}}}
\newcommand{\bfepsilon}{{\boldsymbol{\epsilon}}}
\newcommand{\bfvarepsilon}{{\boldsymbol{\varepsilon}}}
\newcommand{\bfeps}{{\boldsymbol{\varepsilon}}}
\newcommand{\bfzeta}{{\boldsymbol{\zeta}}}
\newcommand{\bfeta}{{\boldsymbol{\eta}}}
\newcommand{\bftheta}{{\boldsymbol{\theta}}}
\newcommand{\bfvartheta}{{\boldsymbol{\theta}}}
\newcommand{\bfiota}{{\boldsymbol{\iota}}}
\newcommand{\bfkappa}{{\boldsymbol{\kappa}}}
\newcommand{\bflambda}{{\boldsymbol{\lambda}}}
\newcommand{\bfmu}{{\boldsymbol{\mu}}}
\newcommand{\bfnu}{{\boldsymbol{\nu}}}
\newcommand{\bfxi}{{\boldsymbol{\xi}}}
\newcommand{\bfo}{{\boldsymbol{o}}}
\newcommand{\bfpi}{{\boldsymbol{\pi}}}
\newcommand{\bfvarpi}{{\boldsymbol{\varpi}}}
\newcommand{\bfrho}{{\boldsymbol{\rho}}}
\newcommand{\bfvarrho}{{\boldsymbol{\varrho}}}
\newcommand{\bfsigma}{{\boldsymbol{\sigma}}}
\newcommand{\bfvarsigma}{{\boldsymbol{\varsigma}}}
\newcommand{\bftau}{{\boldsymbol{\tau}}}
\newcommand{\bfupsilon}{{\boldsymbol{\upsilon}}}
\newcommand{\bfphi}{{\boldsymbol{\phi}}}
\newcommand{\bfvarphi}{{\boldsymbol{\varphi}}}
\newcommand{\bfchi}{{\boldsymbol{\chi}}}
\newcommand{\bfpsi}{{\boldsymbol{\psi}}}
\newcommand{\bfomega}{{\boldsymbol{\omega}}}
\newcommand{\bfA}{{\bf A}}
\newcommand{\bfB}{{\bf B}}
\newcommand{\bfC}{{\bf C}}
\newcommand{\bfD}{{\bf D}}
\newcommand{\bfE}{{\bf E}}
\newcommand{\bfF}{{\bf F}}
\newcommand{\bfG}{{\bf G}}
\newcommand{\bfH}{{\bf H}}
\newcommand{\bfI}{{\bf I}}
\newcommand{\bfJ}{{\bf J}}
\newcommand{\bfK}{{\bf K}}
\newcommand{\bfL}{{\bf L}}
\newcommand{\bfM}{{\bf M}}
\newcommand{\bfN}{{\bf N}}
\newcommand{\bfO}{{\bf O}}
\newcommand{\bfP}{{\bf P}}
\newcommand{\bfQ}{{\bf Q}}
\newcommand{\bfR}{{\bf R}}
\newcommand{\bfS}{{\bf S}}
\newcommand{\bfT}{{\bf T}}
\newcommand{\bfU}{{\bf U}}
\newcommand{\bfV}{{\bf V}}
\newcommand{\bfW}{{\bf W}}
\newcommand{\bfX}{{\bf X}}
\newcommand{\bfY}{{\bf Y}}
\newcommand{\bfZ}{{\bf Z}}
\newcommand{\bfa}{{\bf a}}
\newcommand{\bfb}{{\bf b}}
\newcommand{\bfc}{{\bf c}}
\newcommand{\bfd}{{\bf d}}
\newcommand{\bfe}{{\bf e}}
\newcommand{\bff}{{\bf f}}
\newcommand{\bfg}{{\bf g}}
\newcommand{\bfh}{{\bf h}}
\newcommand{\bfi}{{\bf i}}
\newcommand{\bfj}{{\bf j}}
\newcommand{\bfk}{{\bf k}}
\newcommand{\bfl}{{\bf l}}
\newcommand{\bfm}{{\bf m}}
\newcommand{\bfn}{{\bf n}}
\newcommand{\bfp}{{\bf p}}
\newcommand{\bfq}{{\bf q}}
\newcommand{\bfr}{{\bf r}}
\newcommand{\bfs}{{\bf s}}
\newcommand{\bft}{{\bf t}}
\newcommand{\bfu}{{\bf u}}
\newcommand{\bfv}{{\bf v}}
\newcommand{\bfw}{{\bf w}}
\newcommand{\bfx}{{\bf x}}
\newcommand{\bfy}{{\bf y}}
\newcommand{\bfz}{{\bf z}}
\newcommand{\bfzero}{{\bf0}}
\newcommand{\bfone}{{\bf1}}
\newcommand{\bftwo}{{\bf2}}
\newcommand{\bfthree}{{\bf3}}
\newcommand{\bffour}{{\bf4}}
\newcommand{\bffive}{{\bf5}}
\newcommand{\bfsix}{{\bf6}}
\newcommand{\bfseven}{{\bf7}}
\newcommand{\bfeight}{{\bf8}}
\newcommand{\bfnine}{{\bf9}}
% </fold>

% <fold> bold letters with \mx
\newcommand{\mxA}{{\bf A}}
\newcommand{\mxB}{{\bf B}}
\newcommand{\mxC}{{\bf C}}
\newcommand{\mxD}{{\bf D}}
\newcommand{\mxE}{{\bf E}}
\newcommand{\mxF}{{\bf F}}
\newcommand{\mxG}{{\bf G}}
\newcommand{\mxH}{{\bf H}}
\newcommand{\mxI}{{\bf I}}
\newcommand{\mxJ}{{\bf J}}
\newcommand{\mxK}{{\bf K}}
\newcommand{\mxL}{{\bf L}}
\newcommand{\mxM}{{\bf M}}
\newcommand{\mxN}{{\bf N}}
\newcommand{\mxO}{{\bf O}}
\newcommand{\mxP}{{\bf P}}
\newcommand{\mxQ}{{\bf Q}}
\newcommand{\mxR}{{\bf R}}
\newcommand{\mxS}{{\bf S}}
\newcommand{\mxT}{{\bf T}}
\newcommand{\mxU}{{\bf U}}
\newcommand{\mxV}{{\bf V}}
\newcommand{\mxW}{{\bf W}}
\newcommand{\mxX}{{\bf X}}
\newcommand{\mxY}{{\bf Y}}
\newcommand{\mxZ}{{\bf Z}}
\newcommand{\mxSigma}{{\bf \Sigma}}
% </fold>

% <fold> calligraphic characters
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calG}{\mathcal{G}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calI}{\mathcal{I}}
\newcommand{\calJ}{\mathcal{J}}
\newcommand{\calK}{\mathcal{K}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calM}{\mathcal{M}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calO}{\mathcal{O}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\calQ}{\mathcal{Q}}
\newcommand{\calR}{\mathcal{R}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calW}{\mathcal{W}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calZ}{\mathcal{Z}}
% </fold>

% <fold> blackboard characters
\newcommand{\bbA}{\mathbb{A}}
\newcommand{\bbB}{\mathbb{B}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbD}{\mathbb{D}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbG}{\mathbb{G}}
\newcommand{\bbH}{\mathbb{H}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbJ}{\mathbb{J}}
\newcommand{\bbK}{\mathbb{K}}
\newcommand{\bbL}{\mathbb{L}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbO}{\mathbb{O}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbS}{\mathbb{S}}
\newcommand{\bbT}{\mathbb{T}}
\newcommand{\bbU}{\mathbb{U}}
\newcommand{\bbV}{\mathbb{V}}
\newcommand{\bbW}{\mathbb{W}}
\newcommand{\bbX}{\mathbb{X}}
\newcommand{\bbY}{\mathbb{Y}}
\newcommand{\bbZ}{\mathbb{Z}}
% </fold>

% <fold> mathematical fracture characters (requires amsmath)
\newcommand{\mfA}{\mathfrak{A}}
\newcommand{\mfB}{\mathfrak{B}}
\newcommand{\mfC}{\mathfrak{C}}
\newcommand{\mfD}{\mathfrak{D}}
\newcommand{\mfE}{\mathfrak{E}}
\newcommand{\mfF}{\mathfrak{F}}
\newcommand{\mfG}{\mathfrak{G}}
\newcommand{\mfH}{\mathfrak{H}}
\newcommand{\mfI}{\mathfrak{I}}
\newcommand{\mfJ}{\mathfrak{J}}
\newcommand{\mfK}{\mathfrak{K}}
\newcommand{\mfL}{\mathfrak{L}}
\newcommand{\mfM}{\mathfrak{M}}
\newcommand{\mfN}{\mathfrak{N}}
\newcommand{\mfO}{\mathfrak{O}}
\newcommand{\mfP}{\mathfrak{P}}
\newcommand{\mfQ}{\mathfrak{Q}}
\newcommand{\mfR}{\mathfrak{R}}
\newcommand{\mfS}{\mathfrak{S}}
\newcommand{\mfT}{\mathfrak{T}}
\newcommand{\mfU}{\mathfrak{U}}
\newcommand{\mfV}{\mathfrak{V}}
\newcommand{\mfW}{\mathfrak{W}}
\newcommand{\mfX}{\mathfrak{X}}
\newcommand{\mfY}{\mathfrak{Y}}
\newcommand{\mfZ}{\mathfrak{Z}}
\newcommand{\mfa}{\mathfrak{a}}
\newcommand{\mfb}{\mathfrak{b}}
\newcommand{\mfc}{\mathfrak{c}}
\newcommand{\mfd}{\mathfrak{d}}
\newcommand{\mfe}{\mathfrak{e}}
\newcommand{\mff}{\mathfrak{f}}
\newcommand{\mfg}{\mathfrak{g}}
\newcommand{\mfh}{\mathfrak{h}}
\newcommand{\mfi}{\mathfrak{i}}
\newcommand{\mfj}{\mathfrak{j}}
\newcommand{\mfk}{\mathfrak{k}}
\newcommand{\mfl}{\mathfrak{l}}
\newcommand{\mfm}{\mathfrak{m}}
\newcommand{\mfn}{\mathfrak{n}}
\newcommand{\mfo}{\mathfrak{o}}
\newcommand{\mfp}{\mathfrak{p}}
\newcommand{\mfq}{\mathfrak{q}}
\newcommand{\mfr}{\mathfrak{r}}
\newcommand{\mfs}{\mathfrak{s}}
\newcommand{\mft}{\mathfrak{t}}
\newcommand{\mfu}{\mathfrak{u}}
\newcommand{\mfv}{\mathfrak{v}}
\newcommand{\mfw}{\mathfrak{w}}
\newcommand{\mfx}{\mathfrak{x}}
\newcommand{\mfy}{\mathfrak{y}}
\newcommand{\mfz}{\mathfrak{z}}

\newcommand{\jmc}[1]{\textcolor{blue}{#1}}

%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Efficient sampling approaches based on generalized Golub-Kahan methods for large-scale hierarchical Bayesian inverse problems]{Efficient sampling approaches based on generalized Golub-Kahan methods for large-scale hierarchical Bayesian inverse problems}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1]{\fnm{Elle} \sur{Buser}}\email{ebuser@emory.edu}
\author*[1]{\fnm{Julianne} \sur{Chung}}\email{jmchung@emory.edu}


\affil[1]{\orgdiv{Department of Mathematics}, \orgname{Emory University}, \orgaddress{\street{400 Dowman Drive}, \city{Atlanta}, \postcode{30322}, \state{Georgia}, \country{USA}}}


%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{Uncertainty quantification for large-scale inverse problems remains a challenging task. For linear inverse problems with additive Gaussian noise and Gaussian priors, the posterior is Gaussian but sampling can be challenging, especially for problems with a very large number of unknown parameters (e.g., dynamic inverse problems) and for problems where computation of the square root and inverse of the prior covariance matrix are not feasible.  Moreover, for hierarchical problems where several hyperparameters that define the prior and the noise model must be estimated from the data, the posterior distribution may no longer be Gaussian, even if the forward operator is linear.  Performing large-scale uncertainty quantification for these hierarchical settings requires new computational techniques. In this work, we consider a hierarchical Bayesian framework where both the noise and prior variance are modeled as hyperparameters.  Our approach uses Metropolis-Hastings independence sampling within Gibbs where the proposal distribution is based on generalized Golub-Kahan based methods. We consider two proposal samplers, one that uses a low rank approximation to the conditional covariance matrix and another that uses a preconditioned Lanczos method. Numerical examples from seismic imaging, dynamic photoacoustic tomography, and atmospheric inverse modeling demonstrate the effectiveness of the described approaches.}


\keywords{hierarchical Bayes, Gibbs sampler, inverse problems, uncertainty quantification, Krylov methods}

%%\pacs[JEL Classification]{D8, H51}

\pacs[MSC Classification]{65F22, 65M32, 62F10}

\maketitle
\section{Introduction}
\label{sec:intro}
Inverse problems arise in many scientific applications, where the main goal is to use collected measurements or observations to estimate some underlying unknown parameters of physical models. We focus on inverse problems in imaging, where the unknown parameters represent detailed spatial or spatiotemporal reconstructions of physical properties such as images of attenuation coefficients in X-ray tomography or spatiotemporal maps of greenhouse gas emission fluxes in atmospheric inverse modeling \cite{chung2024computational}. For example, in dynamic inverse problems, the unknown parameters can change over time, easily resulting in millions of unknown parameters.  For example, in dynamic atmospheric inverse modeling, the goal is to estimate thousands of emission fluxes at 3-hourly time intervals over multiple months or years.  Obtaining such estimates is computationally challenging, and recent works in the field of computational inverse problems have addressed various theoretical and computational advancements (e.g., developing improved reconstruction algorithms that enable faster reconstructions at higher resolutions with higher accuracy), e.g.,~\cite{cho2022computationally,pasha2023computational,chung2018efficient}.  
Many of these approaches rely on sophisticated tools from optimization and numerical linear algebra for obtaining reconstructions.  However, to provide quantification of uncertainty about the solutions of inverse problems, we follow a Bayesian interpretation of inverse problems.

In a Bayesian formulation, the parameters of interest and the observed data are modeled as random variables, and any prior knowledge or lack thereof (e.g., uncertainty in the parameters) is encoded in the prior distribution and any noise or measurement error is encoded in the likelihood function (along with the forward process).  Contrary to deterministic approaches where a single solution is provided, the Bayesian approach provides a distribution of plausible solutions in the form of samples from a posterior probability distribution.  Given the observation data, Bayes' law allows full uncertainty quantification (UQ) via incorporation of prior knowledge about the unknown parameters (in the form of the prior) and the likelihood. Good references on Bayesian or statistical approaches to inverse problems and computational UQ include \cite{calvetti2007introduction,bardsley2018computational}.  

However, there are various challenges that have hindered the extension of many of these approaches to the large-scale problems of interest. For hierarchical Bayesian approaches where the prior and/or likelihood distributions depend on additional (hyper-)parameters, hyperpriors must be incorporated.  This usually results in complicated posterior distributions that do not have a closed form, thereby requiring expensive approximation techniques \cite{ghattasinfinitebayes,BuiThanh2014}.  Moreover, even when it is possible to derive a closed-form for the posterior distribution, drawing samples from the posterior distribution can become a computational nightmare.  For hierarchical Bayesian inverse problems, there exists a wide range of methods and approaches, each of which has advantages and disadvantages.  A full comparison is beyond the scope of this paper.  

For sampling the posterior distribution in hierarchical Bayesian problems, we focus on Markov chain Monte Carlo (MCMC) methods, particularly Gibbs sampling and its variants.  For large-scale inverse problems, the main computational bottleneck of these MCMC routines is the repeated sampling from high dimensional Gaussian random variables, which requires a symmetric factorization of a large, and often dense, covariance matrix that is changing at each iteration.  We seek to reduce the computational burden of repeated sampling by using Metropolis-Hastings within Gibbs, with proposal samplers based on generalized Golub-Kahan methods. Similar to the approach described in \cite{brown2018low}, one approach we consider is to use a proposal distribution based on a low-rank approximation of the prior-preconditioned Hessian.  Previous approaches have considered low-rank approximations that are obtained via randomization (e.g., the randomized SVD), but such methods can only provide good approximations for severely low-rank matrices.  Instead, we exploit generalized Golub-Kahan approximations for independence sampling, where the added benefits are that more general prior covariance matrices can be included (since we only require matrix-vector multiplications with the prior covariance matrix) and more general inverse problems can be considered.  A second approach that we consider for independence sampling incorporates a preconditioned Lanczos method. Similar approximation samplers were considered in \cite{saibaba2020efficient}, but not in the context of hierarchical problems.  For these independence samplers, we derive explicit formulas for the acceptance rates and demonstrate the computational benefits of their use for hierarchical Bayesian inverse problems in a wide range of applications.

\paragraph{Overview of contributions}
The paper is organized as follows. In \Cref{sec:BayesianIP}, we describe a hierarchical Bayesian framework for a general linear inverse problem.  We formulate the posterior density function and review various MCMC samplers for sampling the posterior.  Then in \Cref{sec:methods} we provide an overview of generalized Golub-Kahan methods and describe two approaches for its use for independence sampling in Metropolis-Hastings within Gibbs. Numerical results for various large-scale image processing applications are provided in \Cref{sec:numerics}. Conclusions and future work are described in \Cref{sec:conclusions}.

\section{Hierarchical Bayesian Inverse Problem}
\label{sec:BayesianIP}
Consider a linear inverse problem of the form 
\begin{equation}
     \bfb =  \bfA\bfx + \bfe
\end{equation}
where observation data $\bfb\in\bbR^{m}$ are corrupted by measurement error $\bfe\in\bbR^{m}$, $\bfA\in\bbR^{m\times n}$ represents the forward parameter-to-observable map, and $\bfx\in\bbR^{n}$ contains the unknown solution.  In a deterministic inverse problem setting, the goal of the inverse problem is to compute a reconstruction of $\bfx$ (e.g., a point estimate),  given $\bfA$ and $\bfb$.  In a Bayesian setting, the goal is to fully characterize the posterior probability distribution (thereby quantifying uncertainty about the solutions), given assumptions or prior knowledge about the unknowns.  

Given $\lambda, \delta >0$ and $\bfmu \in \bbR^n$, we assume that $\bfe$ and $\bfx$ are independent Gaussian random variables such that 
\begin{equation}
    \label{eq:assumptions_e_x}
\bfe \mid \lambda \sim\calN\left( \bfzero,\lambda^{-1}\bfR \right)\quad  \mbox{and} \quad \bfx \mid \delta \sim\calN\left( \bfmu, \delta^{-1}\bfQ \right)
\end{equation}
where $\bfQ$ and $\bfR$ are symmetric positive definite (SPD) covariance matrices. With this model, $\bfb \mid \bfx, \lambda \sim \calN(\bfA \bfx,\lambda^{-1}\bfR)$ so that the likelihood is given by
\begin{equation}
\label{eq:likelihood}
\pi(\bfb \mid \bfx, \lambda) \propto \lambda^{m/2} \exp\left( -\frac{\lambda}{2} (\bfb - \bfA \bfx)\t \bfR^{-1} (\bfb - \bfA \bfx) \right)
\end{equation}
and the prior is given by
\begin{equation}
\label{eq:prior}
\pi(\bfx \mid \delta) \propto \delta^{n/2} \exp\left( - \frac{\delta}{2} (\bfx- \bfmu)\t \bfQ^{-1} (\bfx - \bfmu) \right).
\end{equation}
For fixed $\lambda$ and $\delta$, we get a conditional that is also Gaussian.  That is, $\bfx \mid \bfb, \lambda, \delta \sim \calN(\bfx_{\rm cond}, \bfGamma_{\rm cond})$, where 
\begin{equation}
    \label{eq:cond_x}
\bfGamma_{\rm cond} = (\lambda \bfA\t \bfR^{-1} \bfA + \delta \bfQ^{-1})^{-1} \mbox{ and }\bfx_{\rm cond} =  \bfGamma_{\rm cond} (\lambda \bfA\t \bfR^{-1}\bfb + \delta \bfQ^{-1}\bfmu).
\end{equation}
More specifically,
\begin{equation}
\label{eq:conditional}
\pi(\bfx \mid \bfb, \lambda, \delta) \propto  \exp\left(-\frac{\lambda}{2} (\bfb - \bfA \bfx)\t \bfR^{-1} (\bfb - \bfA \bfx) - \frac{\delta}{2} (\bfx - \bfmu)\t \bfQ^{-1} (\bfx-\bfmu) \right).
\end{equation}
The conditional posterior mode is the minimizer of the negative log-likelihood and corresponds to a general-form Tikhonov problem,
\begin{equation}
\label{eq:Tik}
    \widehat \bfx = \argmax_\bfx \pi(\bfx \mid \bfb, \lambda, \delta) = \argmin_\bfx \frac{\lambda}{2} \|\bfb - \bfA \bfx \|_{\bfR^{-1}}^2 + \frac{\delta}{2} \| \bfx - \bfmu \|_{\bfQ^{-1}}^2,
\end{equation}
where $\norm[\bfM]{\bfx} = \sqrt{\bfx\t\bfM\bfx}$ for SPD $\bfM$.  For our problems, we assume that the inverse and square root of $\bfR$ are feasible, e.g, an identity or diagonal matrix.  However, we do not make such assumptions for the prior covariance matrix $\bfQ$.  We focus on problems where explicit computation of the square root and inverse of the covariance matrix $\bfQ$ are not feasible.  This scenario often arises for large-scale problems where $\bfQ$ is highly structured or is constructed from covariance kernels. We focus on the Mat\'ern class of covariance kernels and assume that matrix-vector multiplication with $\bfQ$ can be performed easily, e.g., in $\calO(n \log n)$ time by exploiting fast Fourier transform if the solution is represented on a uniform equispaced grid. For such covariance matrices, a symmetric factorization of $\bfQ^{-1} = \bfL\t \bfL$ is not available, and thus it is not possible to reformulate the regularization term as $\|\bfL (\bfx - \bfmu)\|_2^2$, which is common practice.  Instead, we develop methods that work directly with $\bfQ$.

We assume that hyperparameters $\lambda$ and $\delta$ are unknown, and we assume that they are random variables distributed according to some hyperprior. For example, a common assumption is to use gamma hyper-priors defined by
\begin{equation}
\label{eq:hyperpriors}
    \pi(\lambda)\propto \lambda^{\alpha_{\lambda}-1}\exp(-\beta_{\lambda}\lambda) \quad \mbox{and} \quad
    \pi(\delta)\propto \delta^{\alpha_{\delta}-1}\exp(-\beta_{\delta}\delta),
\end{equation}
where $\alpha_\lambda, \beta_\lambda, \alpha_\delta, $ and $\beta_\delta$ are given parameters defining the hyperpriors. Using Bayes' theorem with assumptions \eqref{eq:likelihood}, \eqref{eq:prior}, and \eqref{eq:hyperpriors}, the (non-Gaussian) joint posterior probability density is given by,

\begin{align}
    & \pi(\bfx, \lambda,\delta\mid\bfb) \\
    &\propto \lambda^{m/2} \delta^{n/2} \pi(\bfb\mid\bfx,\lambda)\pi(\bfx\mid\delta)\pi(\delta)\pi(\lambda) \\
    &\propto \frac{\lambda^{m/2+\alpha_{\lambda}-1}\delta^{n/2+\alpha_{\delta}-1}}{\left( (2\pi)^{n+m} |\bfQ| |\bfR| \right)^{1/2}}\exp\left( -\frac{\lambda}{2}\norm[\bfR^{-1}]{\bfA\bfx-\bfb}^{2} - \frac{\delta}{2}\norm[\bfQ^{-1}]{\bfx-\bfmu}^{2} -\beta_{\lambda}\lambda -\beta_{\delta}\delta\right) 
    \label{eq:post_nonGaus}
\end{align}
where $\pi(\bfb|\bfx,\lambda)$ and $\pi(\bfx\mid\delta)$ are the liklihood and prior density functions respectively and $| \cdot |$ denotes the determinant.

In a Bayesian framework, $\pi(\bfx, \lambda,\delta\mid\bfb)$ is the solution to the inverse problem.  However, since the full joint posterior \eqref{eq:post_nonGaus} is non-Gaussian, exploring the posterior is more challenging especially for large-scale problems. There are various approaches.  One idea is to compute point estimates such as the maximum a posteriori (MAP) estimate, corresponding to the maximum of the posterior density function.  Obtaining this point estimate requires sophisticated nonlinear optimization techniques, e.g., computing the MAP requires solving
\begin{equation}
\label{eq:MAP}
    \min_{\bfx,\lambda,\delta} \frac{\lambda}{2}\norm[\bfR^{-1}]{\bfA\bfx-\bfb}^{2} + \frac{\delta}{2}\norm[\bfQ^{-1}]{\bfx-\bfmu}^{2} + \beta_{\lambda}\lambda + \beta_{\delta}\delta - (m/2+\alpha_\lambda -1) \log \lambda - (n/2+\alpha_\delta -1) \log \delta.
\end{equation}
Moreover, these point estimates give no information regarding the uncertainties of our solution. Another common approach is to approximate \eqref{eq:post_nonGaus} by a Gaussian distribution (e.g., by linearizing around the MAP estimate), but such approximations can be poor and yield unsatisfactory uncertainty estimates \cite{gelman2013bayesian}.
We consider an alternative approach, which is to use Monte Carlo methods for sampling from the posterior \eqref{eq:post_nonGaus} and obtaining summary statistics (e.g., estimation of the posterior mean and variances).

A common type of MCMC algorithm is the standard block Gibbs approach.  The main idea is to alternate sampling from the conditional distributions,
\begin{align}
    \lambda\mid\bfb,\bfx,\delta &\sim \Gamma\big(m/2 + \alpha_{\lambda},\frac{1}{2}\norm[\bfR^{-1}]{\bfA\bfx-\bfb}^{2} + \beta_{\lambda}\big), \\
    \delta\mid\bfb,\bfx,\lambda &\sim \Gamma\big(n/2 + \alpha_{\delta},\frac{1}{2}\norm[\bfQ^{-1}]{\bfx-\bfmu}^{2} + \beta_{\delta}\big), \\
    \bfx\mid\bfb,\lambda,\delta &\sim \calN\left( \bfx_{\rm cond}, \bfGamma_{\rm cond} \right), \label{eq:condGaus}
\end{align}
where $\bfGamma_{\rm cond}$ and $\bfx_{\rm cond}$

are the conditional covariance matrix and conditional mean defined in \eqref{eq:cond_x}. The corresponding conditional densities are given by

\begin{align}
    \pi(\lambda\mid\bfb,\bfx,\delta) &\propto \exp\left(\left[-\frac{1}{2}\norm[\bfR^{-1}]{\bfA\bfx-\bfb}^{2} -\beta_{\lambda}\right]\lambda\right), \\
    \pi(\delta\mid\bfb,\bfx,\lambda) &\propto \exp\left(\left[- \frac{1}{2}\norm[\bfQ^{-1}]{\bfx-\bfmu}^{2} - \beta_{\delta}\right]\delta\right), \\
    \pi(\bfx\mid\bfb,\lambda,\delta) &\propto \exp\left( -\frac{1}{2}\norm[\bfR^{-1}]{\bfA\bfx-\bfb}^{2} - \frac{\delta}{2}\norm[\bfQ^{-1}]{\bfx-\bfmu}^{2} \right) \label{eq:xcond}
\end{align}
respectively. Here, $\bfx$ is drawn separately from $\lambda$ and $\delta$ in order to exploit the conditionally conjugate Gaussian distribution \eqref{eq:condGaus}.  Oftentimes $\lambda$ and $\delta$ are drawn separately, but this is not necessary. A block Gibbs algorithm for sampling from \eqref{eq:post_nonGaus} is outlined in \Cref{alg:gibbs}.  
\begin{algorithm}[H]
        \caption{Block Gibbs algorithm for sampling the posterior density \eqref{eq:post_nonGaus}}\label{alg:gibbs}
        \begin{algorithmic}[1]
        \Require{Number of samples $T$ and burn-in period $T_b$}
        \Ensure{Approximate samples from \eqref{eq:post_nonGaus}: $\left\{\bfx^t, \lambda^t, \delta^t \right\}_{t=T_b + 1}^T$}
        \State Initialize $\bfx^0, \ \lambda^0, \ \delta^0$
        \For{t = 1 to T}
        \State {Compute $\lambda^t \sim \Gamma\big(m/2 + \alpha_{\lambda},\frac{1}{2}\norm[\bfR^{-1}]{\bfA\bfx^{t-1}-\bfb}^{2} + \beta_{\lambda}\big)$}
        \State {Compute $\delta^t \sim \Gamma\big(n/2 + \alpha_{\delta},\frac{1}{2}\norm[\bfQ^{-1}]{\bfx^{t-1}-\bfmu}^{2} + \beta_{\delta}\big)$}
        \State {Compute $\bfx^t \sim \calN(\bfGamma_{\rm cond}^t (\lambda^t \bfA\t \bfR^{-1} \bfb + \delta^t \bfQ^{-1} \bfmu), \bfGamma_{\rm cond}^t )$, \\ \qquad  \qquad where $\bfGamma_{\rm cond}^t = (\lambda^{t}\bfA\t \bfR^{-1} \bfA + \delta^{t} \bfQ^{-1})^{-1}$}
        \EndFor
        \end{algorithmic}
\end{algorithm}

The Gibbs sampler generates a Markov chain $\{\bfx^t,\lambda^t,\delta^t\}_{t=1}^T$ that converges in distribution to the posterior density $\pi(\bfx,\lambda,\delta\mid\bfb)$ \cite{bardsley2018computational}.

However, the computational cost of this Gibbs sampler is prohibitive for problems with large $n$.  This is due to the fact that, even though the conditional distribution $\pi(\bfx\mid \bfb, \lambda,\delta)$ is Gaussian, drawing a sample requires the solution of an $n \times n$ linear system.  Moreover, the number of Gibbs samples increases with $n$ since the integrated autocorrelation time of the MCMC chain tends to $\infty$ \cite{saibaba2019efficient}.

For large-scale problems, computing a sample from \eqref{eq:condGaus} is infeasible, so to address this, previous approaches substitute direct sampling with a Metropolis-Hastings independence sampler.  That is, we replace the computation of $\bfx^t$ in line 5 of \Cref{alg:gibbs} with an accept-reject step where a sample is drawn from a proposal distribution, $\hat{\pi}_x(\bfx\mid\lambda,\delta,\bfb)$, which is an approximation of the conditional distribution \eqref{eq:xcond}, and the sample is accepted with some probability. In \cite{brown2018low}, a proposal distribution based on a low-rank approximation of the prior-preconditioned Hessian was used, where randomized SVD techniques were used to compute a low-rank approximation.  Such low-rank approximations were considered in \cite{ghattasinfinitebayes} and were combined with marginalization based MCMC methods in \cite{saibaba2019efficient}. In this paper, we are interested in low-rank independence samplers that are based on the generalized Golub-Kahan bidiagonalization.  These will be discussed in \cref{sec:methods}.

First, we provide an overview of independence sampling.  Let $\bfx \in \bbR^n$ and denote the target density by $h(\bfx)$. The Metropolis-Hastings algorithm generates at iteration $t$ a sample $\bfx^\star$ from a proposal distribution, possibly conditioned on the current state $\bfx^{t-1}$, and sets $\bfx^t = \bfx^\star$ with probability $\min(1,\alpha)$ where, for fixed $\lambda$ and $\delta$,
$$\alpha (\bfx^{t-1}, \bfx^\star) = \frac{h(\bfx^\star) q(\bfx^{t-1} \mid \bfx^\star)}{h(\bfx^{t-1}) q(\bfx^\star \mid \bfx^{t-1})}$$
where $q(\cdot \mid \bfx^{t-1})$ is the density of the proposal distribution. The algorithm generates a Markov chain $\{ \bfx^t \}$ that converges to the target distribution \cite{MCstatmethods}.

An independence Metropolis-Hastings sampler generates proposal states from a density that is independent of the current state of the chain, i.e., the proposal density has the form $q(\bfx^\star \mid \bfx^{t-1}) = g(\bfx^\star)$, and the ratio can now be written as 
\begin{equation}
\label{eq:ratio}
\frac{h(\bfx^\star) g(\bfx^{t-1})}{h(\bfx^{t-1}) g(\bfx^\star)} = \frac{w(\bfx^\star)}{w(\bfx^{t-1})}
\end{equation}
where $w(\bfx;\lambda,\delta) \propto \frac{h(\bfx)}{g(\bfx)}$.  Let $h(\bfx) = \pi (\bfx \mid \bfb, \lambda, \delta)$ from \eqref{eq:xcond} where the conditional variables have been dropped, and let $g(\bfx)$ be a proposal density function (to be defined later). An independence Metropolis-Hastings within Gibbs algorithm for sampling the posterior density \eqref{eq:post_nonGaus} is described in Algorithm \Cref{alg:MHwithinGibbs}.


\begin{algorithm}[H]
        \caption{Independence Metropolis-Hastings within Gibbs algorithm for sampling the posterior density \eqref{eq:post_nonGaus}}\label{alg:MHwithinGibbs}
        \begin{algorithmic}[1]
        \Require{Number of samples $T$ and burn-in period $T_b$}
        \Ensure{Approximate samples from \eqref{eq:post_nonGaus}: $\left\{\bfx^t, \lambda^t, \delta^t \right\}_{t=T_b + 1}^T$}
        \State Initialize $\bfx^0, \ \lambda^0, \ \delta^0$
        \For{t = 1 to T}
        \State {Compute $\lambda^t \sim \pi(\lambda \mid \bfb, \bfx^{t-1})$}
        \State {Compute $\delta^t \sim \pi(\delta \mid \bfb, \bfx^{t-1})$}
        \State {Compute proposal sample $\bfx^\star \sim g(\bfx)$}
        \State {Accept $\bfx^t = \bfx^\star$ with probability $\min(1,\alpha)$ where
        $\alpha = \frac{w(\bfx^\star)}{w(\bfx^{t-1})}$}
        \State {Otherwise set $\bfx^t = \bfx^{t-1}$}
        \EndFor
        \end{algorithmic}
\end{algorithm}

\section{Generalized Golub-Kahan Based Proposals for Independence Sampling}
\label{sec:methods}

The target conditional Gaussian density function is given by
\begin{equation}
\label{eq:target}
h(\bfx) := \frac{1}{\sqrt{(2 \pi)^n |\bfGamma_{\rm cond}|)}} \exp \left(-\frac{1}{2} (\bfx - \bfx_{\rm cond})\t \bfGamma_{\rm cond}^{-1} (\bfx - \bfx_{\rm cond}) \right),
\end{equation}
and samples from the distribution $\calN(\bfx_{\rm cond}, \bfGamma_{\rm cond})$ can be generated as $\bfx = \bfx_{\rm cond} + \bfG \bfepsilon$ where $\bfGamma_{\rm cond} = \bfG \bfG\t$ and $\bfepsilon \sim \calN(\bfzero, \bfI)$.  However, it is computationally expensive to compute $\bfx_{\rm cond}$ and the product $\bfG \bfepsilon$.  Works such as \cite{brown2018low,ghattasinfinitebayes} exploit low-rank structure of the forward operator $\bfA$ and rely on factorizations of the prior covariance $\bfQ^{-1} = \bfL \bfL\t$ to get efficient representations for $\bfGamma_{\rm cond}$ that can exploit the fast decay of singular values.  These approaches are not always feasible.  For example, in atmospheric emissions tomography, the forward model matrices are nowhere near low rank and, moreover, covariance kernels are defined using complicated prior covariance kernels. The approach we describe herein can handle these scenarios.

Specifically, we use a Krylov subspace method based on the generalized Golub-Kahan (genGK) bidiagonalization process to approximate $\bfx_{\rm cond}$.  Then we consider two proposal distributions, one which uses the resulting genGK matrices to form an approximation of $\bfGamma_{\rm cond}$ and another which uses preconditioned Krylov sampling.  Similar approximations were considered in \cite{UQlargebayesian,dynamicinverseChung_2018}, but the main difference in this work is that we will use the genGK approximations to define a proposal distribution $g(\bfx)$ that approximates the target distribution  \eqref{eq:target}.  To the best of our knowledge, the use of genGK low-rank approximations within MCMC sampling approaches has not been explored. We begin in Section \Cref{sub:genGK} with a brief overview of the genGK approach, followed by details of the genGK approximation to the target distribution in \Cref{sub:sampling_prop} and details regarding the Metropolis-Hastings within Gibbs algorithm with the genGK approximation used for the proposal distribution.  For many problems, the genGK approximate distribution provides a computationally efficient approach for generating proposal samples. However, for problems where the genGK proposal distribution may require very large ranks to obtain a sufficient approximation, we propose in \Cref{sub:MH_genGK} an alternative proposal sampler that is based on efficient preconditioned Krylov methods and consider its use within MCMC sampling.

\subsection{Generalized Golub-Kahan methods}
\label{sub:genGK}
The genGK bidiagonalization process is an iterative Krylov subspace projection method that was developed in \cite{arioli2013generalized} and can be used to efficiently compute general Tikhonov solutions \eqref{eq:Tik} \cite{ChungSaibabaHybrid2017}.  The genGK approach is well suited for problems where matvecs with $\bfA,$ $\bfA\t$, and $\bfQ$ can be done efficiently, but $\bfQ^{-1}$ or any factorization of $\bfQ$ is expensive.

The first step is to introduce a change of variables $\bfy = \bfQ^{-1}(\bfx - \bfmu)$ to avoid $\bfQ^{-1}$. The equivalent transformed problem is
    \begin{equation}
        \min_{\bfy} \frac{\lambda}{2}\norm[\bfR^{-1}]{\bfA\bfQ\bfy - (\bfb - \bfA \bfmu)}^{2} + \frac{\delta}{2}\norm[\bfQ]{\bfy}^{2}. \label{eq:trans}
    \end{equation}
Then, the transformed problem \eqref{eq:trans} is projected onto subspaces of increasing dimension in an iterative generalized Krylov projection process.  That is, given matrices $\bfA, \bfR,$ and $\bfQ$ and vectors $\bfb$ and $\bfmu$, we initialize the genGK bidiagonalization process with
$$\gamma_1 \bfu_1 = \bfb - \bfA \bfmu \quad \mbox{and} \quad \alpha_1 \bfv_1 = \bfA\t \bfR^{-1} \bfu_1.$$ 
At the $k$th iteration of the genGK process, we generate vectors $\bfu_{k+1}$ and $\bfv_{k+1}$ such that
\begin{equation*}
\gamma_{k+1} \bfu_{k+1} = \bfA \bfQ \bfv_{k} - \alpha_k \bfu_k \qquad \alpha_{k+1} \bfv_{k+1} = \bfA\t \bfR^{-1}\bfu_{k+1} - \gamma_{k+1} \bfv_k,
\end{equation*}
where after $k$ iterations, we have matrices $\bfU_{k+1}=[\bfu_1,\ldots,\bfu_{k+1}]$, $\bfV_{k+1}=[\bfv_1,\ldots,\bfv_{k+1}]$, and $(k+1) \times k$ bidiagonal matrix
\begin{equation*}
    \bfB_{k} = 
    \begin{bmatrix}
        \alpha_1 & & \\
        \gamma_2 & \ddots &  \\
        & \ddots & \alpha_k\\
        & & \gamma_{k+1}
    \end{bmatrix}
\end{equation*}
that satisfy the following relationships, in exact arithmetic,
\begin{equation}
    \bfA\bfQ\bfV_{k} = \bfU_{k+1}\bfB_{k}\quad \mbox{and} \quad \bfA\t\bfR^{-1}\bfU_{k+1} = \bfV_{k}\bfB_{k}\t + \alpha_{k+1}\bfv_{k+1}\bfe_{k+1}\t, \label{eq:gk1}
\end{equation}
        with 
        \begin{equation}
            \bfU_{k+1}\t\bfR^{-1}\bfU_{k+1}=\bfI_{k+1} \quad \text{and} \quad \bfV_{k}\t\bfQ\bfV_{k}=\bfI_{k}. \label{eq:gk2}
        \end{equation}
The vector $\bfe_i$ is the $i$th column of the identity matrix of the appropriate size.

The genGK process constructs a basis for the Krylov subspaces,
\begin{equation*}
    \calR(\bfV_k) = \calK_k(\bfA\t\bfR^{-1}\bfA\bfQ, \bfA\t\bfR^{-1}\bfb)
\end{equation*}
and \begin{equation*}
    \calR(\bfU_k) = \calK_k(\bfA \bfQ \bfA\t \bfR^{-1}, \bfb)
\end{equation*}
where $\calK_{k}(\bfM,\bfg) = \Span{\bfg,\bfM\bfg,\ldots, \bfM^{k-1}\bfg}$ and $\calR$ denotes the column space. 

At the $k$th iteration, we have an approximate solution for \eqref{eq:Tik}, given by
 $\bfx_{k} = \bfmu + \bfQ \bfV_{k}\bfz_{k}$, where $\bfz_k$ solves the projected problem.  That is,
\begin{equation}
    \min_{\bfy_{k}\in\calR(\bfV_{k})}\frac{\lambda}{2}\norm[\bfR^{-1}]{\bfA\bfQ\bfy_{k}-(\bfb-\bfA \bfmu)}^{2}+\frac{\delta}{2}\norm[\bfQ]{\bfy_{k}}^{2} \quad \Longleftrightarrow \quad \min_{\bfz_{k}\in\bbR^{k}}\frac{\lambda}{2}\norm[2]{\bfB_{k}\bfz_{k}-\gamma_{1}\bfe_{1}}^{2} + \frac{\delta}{2}\norm[2]{\bfz_{k}}^{2} \label{eq:proj}
\end{equation}
or equivalently,
\begin{equation}
    \min_{\bfz_k \in\bbR^{k}}\norm[2]{\begin{bmatrix} \sqrt{\lambda}\bfB_{k} \\ \sqrt{\delta}\bfI \end{bmatrix}\bfz_{k} - \begin{bmatrix} \sqrt{\lambda}\gamma_{1}\bfe_{1} \\ \bf0 \end{bmatrix}}^{2}.
\end{equation}

Note that by using the genGK approach, we avoid the need for $\bfQ^{-1}$ and rely on projections of the original problem to obtain a reconstruction in $k$ iterations ($k \ll n$).    
Moreover, by using the genGK relations we can define oblique projectors 
$$\bfP_{\bfV_k} = \bfV_k \bfV_k\t \bfQ \qquad \mbox{and} \qquad \bfP_{\bfU_{k+1}} = \bfU_{k+1} \bfU_{k+1}\t \bfR^{-1},$$
and we can build a low-rank approximation for $\bfA$ as $$\bfA \approx \bfA \bfP_{\bfV_k}\t = \bfU_{k+1} \bfB_k \bfV_k\t \equiv \widehat \bfA .$$
Such approximations were used in the context of hyperparameter estimation in \cite{hall2024efficient}. Next we will use the genGK approximation to build a proposal distribution that can be used within a Metropolis-Hastings within Gibbs approach for sampling from \eqref{eq:post_nonGaus}.  A key feature to note is that the genGK bidiagonalization process does not depend on $\lambda$ and $\delta$.  That is, all of the resulting matrices $\bfV_k, \bfB_k, \bfU_{k+1}$ are independent of $\lambda$ and $\delta$ and can be reused for changing parameters.  We will exploit this property in \Cref{sub:sampling_prop} for efficient sampling from the genGK proposal distribution and in \Cref{sub:MH_genGK} for efficient conditional mean estimation.

\subsection{genGK approximation to the target distribution}
\label{sub:sampling_prop}
Now consider the target conditional distribution \eqref{eq:target} with conditional mean $\bfx_{\rm cond}$ and covariance matrix $\bfGamma_{\rm cond}$.  
Recall that after $k$ iterations of the genGK process, we have $\bfx_k \approx \bfx_{\rm cond}$ and matrices $\bfU_{k+1}, \ \bfV_{k},$ and $\bfB_{k}$ that satisfy the relations in \eqref{eq:gk1} and \eqref{eq:gk2}. Following a similar approach as in \cite{UQlargebayesian}, we aim to use these matrices to approximate $\bfGamma^{1/2}$. Notice that one can factorize the covariance matrix as $\bfGamma_{\rm cond} = \bfGamma_{\rm cond}^{1/2}\bfGamma_{\rm cond}^{1/2}$, where
\begin{equation}
    \bfGamma_{\rm cond}^{1/2} = \delta^{-1/2}\bfQ^{1/2}\left( \bfI + \frac{\lambda}{\delta}\bfQ^{1/2}\bfA\t\bfR^{-1}\bfA\bfQ^{1/2} \right)^{-1/2}
\end{equation}
Notice that although we write out such a factorization for exposition purposes, we will not compute $\bfQ^{1/2}$ explicitly.  We will use a Lanczos algorithm to access it in a matrix-free fashion, following the procedure outlined in appendix \Cref{sec:lowrank}, and it will be part of a preprocessing step (that is independent of MCMC sampling).

Prior to sampling, we compute the low-rank representation $\bfQ^{1/2}\bfV_{k}\bfB_{k}\t\bfB_{k}\bfV_{k}\t\bfQ^{1/2} = \bfZ_k\bfTheta_k\bfZ_k\t$.
Then using the genGK approximation $\bfA\t \bfR^{-1}\bfA \approx \widehat \bfA\t \bfR^{-1}\widehat \bfA = \bfV_k \bfB_k \t \bfB_k \bfV_k\t$, we can define the matrix approximation
\begin{align*}
    \widehat \bfGamma_{\rm cond} & = (\lambda \bfV_k \bfB_k\t \bfB_k \bfV_k\t + \delta \bfQ^{-1})^{-1}\\
    & = \bfQ^{1/2}(\lambda \underbrace{\bfQ^{1/2} \bfV_k \bfB_k\t \bfB_k \bfV_k\t \bfQ^{1/2}}_{\bfZ_k \bfTheta_k \bfZ_k\t} + \delta \bfI)^{-1} \bfQ^{1/2}
\end{align*}
Thus, the square root of the conditional covariance matrix can be approximated as,
\begin{align*}
        \bfGamma_{\rm cond}^{1/2} 
        &= \delta^{-1/2}\bfQ^{1/2}\left( \frac{\lambda}{\delta}\bfQ^{1/2}\underbrace{\bfA\t\bfR^{-1}\bfA}_{\approx \bfV_{k}\bfB_{k}\t\bfB_{k}\bfV_{k}\t}\bfQ^{1/2} +\bfI \right)^{-1/2} \\
        &\approx \delta^{-1/2}\bfQ^{1/2}\left(  \frac{\lambda}{\delta} \underbrace{\bfQ^{1/2}\bfV_{k}\bfB_{k}\t\bfB_{k}\bfV_{k}\t\bfQ^{1/2}}_{\approx \bfZ_k\bfTheta_k\bfZ_k\t}+ \bfI  \right)^{-1/2}\\
        & = \delta^{-1/2}\bfQ^{1/2}(\bfI - \bfZ_{k}\bfD_{k}\bfZ_{k}\t) \equiv \widehat{\bfGamma}_{\rm cond}^{1/2}
    \end{align*}
where  $\bfD_{k} \equiv \bfI_{k} - (\bfI_{k} + \frac{\lambda}{\delta}\mathbf{\Theta}_{k} )^{-1/2}$. Finally, we can define the proposal distribution $\calN(\bfx_k, \widehat \bfGamma_{\rm cond})$, i.e.,
\begin{equation}
\label{eq:proposal_genGK}
g_1(\bfx) := \frac{1}{\sqrt{(2 \pi)^n |\widehat \bfGamma_{\rm cond}|)}} \exp \left(-\frac{1}{2} (\bfx - \bfx_{k})\t \widehat \bfGamma_{\rm cond}^{-1} (\bfx - \bfx_{k}) \right),
\end{equation}
and draw a sample,
\begin{align}
    \bfx^{\star} &= \bfx_k + \widehat{\bfGamma}_{\rm cond}^{1/2}\bfxi \\
    &= \bfmu+ \bfQ\bfV_{k}\bfz_{k} + \delta^{-1/2}\bfQ^{1/2}\left(\bfI - \bfZ_{k}
\bfD_{k}\bfZ_{k}\t\right)\bfxi \label{eq:propsample_genGK}
\end{align}
for $\bfxi\sim\calN(\bf0,\bfI)$. 
Theoretical results regarding the accuracy of the approximate distribution to the true conditional distribution can be found in \cite{UQlargebayesian}.  

Next we incorporate the genGK approximation above as a proposal distribution within a Metropolis-Hastings within Gibbs approach to sample from \eqref{eq:post_nonGaus}. We need to investigate the acceptance ratio.  Let $\bfx$ denote the current state of the chain and let $\bfx^\star$ denote the proposed state.  Then from Proposition 1 of \cite{brown2018low}, we have that the acceptance ratio can be computed as $\alpha_1 = \frac{w(\bfx^\star)}{w(\bfx)}$ where $$w(\bfx) = \exp \left(- \frac{1}{2}\bfx\t \left(\bfGamma_{\rm cond}^{-1} - \widehat \bfGamma_{\rm cond}^{-1} \right) \bfx\right).$$
Note that at each iteration, we have the weight from the previous iteration $w(\bfx^{t-1})$ but we must compute the weight $w(\bfx^\star)$.  An efficient implementation of this can be obtained by observing that
\begin{align}
\log w(\bfz) & = - \frac{1}{2}\bfz\t \left(\bfGamma_{\rm cond}^{-1} - \widehat \bfGamma_{\rm cond}^{-1} \right) \bfz \\
& = - \frac{1}{2}\bfz\t \left(\lambda\bfA\t \bfR^{-1} \bfA + \delta \bfQ^{-1}  - \bfQ^{-1/2} (\lambda \bfZ_k \bfTheta_k \bfZ_k\t + \delta \bfI) \bfQ^{-1/2} \right) \bfz \\
& = - \frac{\lambda}{2}\bfz\t \left(\bfA\t \bfR^{-1} \bfA - \bfQ^{-1/2} \bfZ_k \bfTheta_k \bfZ_k\t\bfQ^{-1/2} \right) \bfz
\end{align}
Similarly, the quality of the low-rank approximation to the target distribution will be evident from the acceptance ratio. The MH within Gibbs algorithm with the genGK approximation used for proposal sampling is summarized in Algorithm \Cref{alg:gibbs_genGK}.  Notice that mat-vecs with $\bfA$ and $\bfA\t$ are required in the genGK process (i.e., $k$ multiplications with $\bfA$ and $\bfA\t$) and again in the computation of the acceptance ratio.  Also, for each new sample of $\lambda^t$ and $\delta^t$, $\bfB_k$ and $\bfV_k$ are reused for efficient computation of $\bfx_k$, and $\bfZ_k$ and $\bfTheta_k$ are reused for the efficient computation of the proposal sample.

\begin{algorithm}[H]
        \caption{Metropolis-Hastings within Gibbs with genGK approximation }\label{alg:gibbs_genGK}
           \begin{algorithmic}[1]
        \Require{$\bfA, \bfb, \bfQ, \bfR$, number of genGK iterations $k$}
        \State Run $k$ iterations of genGK to get bidiagonal matrix $\bfB_k$ and basis vectors $\bfV_k$
        \State Initialize $\lambda^{0}, \delta^0,$ and $\bfx^0$
        \State Precompute $\bfZ_k$, $\bfTheta_k$
        \For{t = 1 to T}
        \State {Compute $\lambda^t \sim \pi(\lambda \mid \bfb, \bfx^{t-1})$}
        \State {Compute $\delta^t \sim \pi(\delta \mid \bfb, \bfx^{t-1})$}
        \State {Compute $\bfx_k$ with fixed $\lambda^t,\ \delta^t$}
        \State {Compute proposal sample $\bfx^\star \sim g_1(\bfx)$ as in \eqref{eq:propsample_genGK}}
        \State {Accept $\bfx^t = \bfx^\star$ with probability $\min(1,\alpha_1)$ where
        $\alpha_1 = \frac{w(\bfx^\star)}{w(\bfx^{t-1})}$}
        \State {Otherwise set $\bfx^t = \bfx^{t-1}$}
        \EndFor
        \end{algorithmic}
\end{algorithm}

\subsection{Proposal sampling using a preconditioned Lanczos method}
\label{sub:MH_genGK}
 Next we consider an alternative proposal distribution, where we generate an approximate sample from $\calN(\bfx_k, \bfGamma_{\rm cond})$ by using a preconditioned Lanczos method. This approach follows that of Method 2 in \cite{UQlargebayesian}. First rewrite the covariance matrix as 
$$
\bfGamma_{\rm cond} = \left(\delta\bfQ^{-1} + \lambda\bfA\t\bfR^{-1}\bfA\right)^{-1} = \lambda\bfQ\bfF^{-1}\bfQ
$$
where $$\bfF = \frac{\delta}{\lambda}\bfQ + \bfQ\bfA\t\bfR^{-1}\bfA\bfQ. $$
Next define $$\bfS_F = \lambda^{-1/2}\bfQ\bfF^{-1/2}$$ which is a square root matrix of $\bfGamma_{\rm cond}$, that is $\bfGamma_{\rm cond}=\bfS_F\bfS_F\t$. Let $\bfG$ be a preconditioner satisfying $\bfG\bfG\t \approx \bfF^{-1}$ which allows us to compute a square root of $\bfF$. Now we have the exact factorization
$$
\bfGamma_{\rm cond} = \bfS_F\bfS_F\t \qquad \bfS_F = \lambda^{-1/2}\bfQ\bfG\t\left(\bfG\bfF\bfG\t\right)^{-1/2}.
$$

Then we draw the sample as
\begin{equation}
\label{eq:method2prop}
\bfx^{\star} = \bfx_k + \bfS_F\bfxi 
\end{equation}
for $\bfxi\sim\calN(\bf0,\bfI)$. It should be noted that while the factorization $\bfGamma_{\rm cond}=\bfS_F\bfS_F\t$ is exact, the mean $\bfx_k$ is the genGK solution computed after $k$ iterations.  Thus, the sample drawn is not from the exact target conditional density function $h(\bfx)$ but a proposal distribution $\calN(\bfx_k,\bfGamma_{\rm cond})$ with a probability density given by
$$
g_2(\bfx) := \frac{1}{\sqrt{(2 \pi)^n | \bfGamma_{\rm cond}|)}} \exp \left(-\frac{1}{2} (\bfx - \bfx_{k})\t \bfGamma_{\rm cond}^{-1} (\bfx - \bfx_{k}) \right). 
$$
As shown in appendix \Cref{sec:accept_norm_precond}, the acceptance ratio for $g_2(\bfx)$ can be evaluated efficiently as
\begin{align*}
\alpha_2 (\bfx^{t-1}, \bfx^\star) &= \frac{h(\bfx^\star) g_2(\bfx^{t-1})}{h(\bfx^{t-1}) g_2(\bfx^\star)} \\
&= \exp\left(\left( \bfx^{\star} - \bfx^{t-1} \right)\t \left( \lambda\left(\gamma_1\alpha_1\bfv_1 - \bfV_{k+1}\begin{bmatrix}
    \bfB_k\t \\ \alpha_{k+1}\bfe_{k+1}\t
\end{bmatrix} \bfB_k\bfz_k\right)- \delta\bfV_k\bfz_k\right)\right).
\end{align*}

Lastly, to draw the proposal sample \eqref{eq:method2prop}, the matrix $\bfS_F$ is never actually formed.  Instead applications of the matrix $\bfG\t(\bfG\bfF\bfG\t)^{-1/2}$ on a vector $\bfxi$ are performed using a preconditioned Lanczos approach.  Note that a mat-vec with $\bfF$ requires one mat-vec with $\bfA$ and $\bfA\t$ and two mat-vecs with $\bfQ$.  For computing the acceptance ratio, we can avoid further mat-vecs with $\bfA$ and $\bfA\t$ by exploiting the gen-GK relationship.
Although this approach can be more computationally expensive compared to using the approximation $\widehat \bfGamma_{\rm cond}$ as in \Cref{sub:sampling_prop}, especially if the forward operator is very expensive, it serves as a good alternative in cases where very few good samples from the proposal density are needed.

\begin{algorithm}[H]
        \caption{Metropolis-Hastings within Gibbs with preconditioned Lanczos}\label{alg:gibbs_precond}
           \begin{algorithmic}[1]
        \Require{$\bfA, \bfb, \bfQ, \bfR, \bfG$, number of genGK iterations $k$}
        \State Run $k$ iterations of genGK to get bidiagonal matrix $\bfB_k$ and basis vectors $\bfV_k$
        \State Initialize $\lambda^{0}, \delta^0,$ and $\bfx^0$
        \For{t = 1 to T}
        \State {Compute $\lambda^t \sim \pi(\lambda \mid \bfb, \bfx^{t-1})$}
        \State {Compute $\delta^t \sim \pi(\delta \mid \bfb, \bfx^{t-1})$}
        \State {Compute $\bfx_k$ with fixed $\lambda^t,\ \delta^t$}
        \State {Compute proposal sample $\bfx^\star \sim g_2(\bfx)$ as in \eqref{eq:method2prop}}
        \State {Accept $\bfx^t = \bfx^\star$ with probability $\min(1,\alpha_2)$}
        \State {Otherwise set $\bfx^t = \bfx^{t-1}$}
        \EndFor
        \end{algorithmic}
\end{algorithm}

\section{Numerical results}
\label{sec:numerics}
In this section, we provide multiple examples to demonstrate the effectiveness and applicability of the proposed sampling approaches. We begin in \Cref{sub:seismic} with a seismic tomography example, where the problem is small enough so that comparisons to existing methods is feasible.  Then in \Cref{sub:atmospheric} we consider an atmospheric tomography example where comparisons to existing methods is not feasible, but the genGK approximation provides good proposal samples.  Finally in \Cref{sub:PAT} we consider a dynamic photoacoustic tomography problem, where the number of unknowns is very large and represents a spatialtemporal image.  This is a highly underdetermined problem, so the prior is critical.

In all of the examples, we initialize Gibbs sampling using a wavelet based estimation of the noise covariance parameter $\lambda^0$ \cite{donoho1995noising} and an initial regularization parameter $\delta^0$ computed using the genHyBR method \cite{ChungSaibabaHybrid2017} with the weighted GCV method on the projected problem.

%\subsection{Convergence of chains}
To ensure there is no bias in the results, we remove samples from the initial stage of the MCMC chain. During this stage, called burn-in, the samples are moving from the starting position to a region that has a higher probability of being the target distribution \cite{bardsley2018computational}. For all examples, the first $10\%$ of samples from the initial burn-in stage are removed before any analysis is done on the mean and variance of $\{\bfx^t\}$ or the hyperparameter chains. 

Next, to examine whether the MCMC chain is in equilibrium, we perform the Geweke test on the individual $\lambda$ and $\delta$ chains. Here we compare the mean of the first $10\%$ of samples, denoted $\mu_{10}$ to the mean of the last $50\%$, denoted $\mu_{50}$, and either accept or reject the null hypothesis, $H_0: \mu_{10}=\mu_{50}$. For $p$ values close to 1, there is strong evidence that the chain is in equilibrium.

Finally, we test the independence of the individual $\lambda$ and $\delta$ chains through autocorrelation functions (ACF), as described in \cite{bardsley2018computational}. If the ACF decays to 0 fast enough, this means that there is no correlation between the samples in the individual chains. Additionally, we compute the essential (or effective) sample size (ESS) of each chain to show the number of independent samples. 

\subsection{Seismic Tomography}
\label{sub:seismic}
In this example, we use the \texttt{PRspherical} test problem  from IRTools \cite{gazzola2019ir} with default settings. The goal is to approximate $\bfx\in\bbR^{1,296}$, the vectorized $36\times 36$ true image shown in \Cref{fig:trueTomo}, given measurements $\bfb\in\bbR^{1,800}$ and the forward model matrix $\bfA\in\bbR^{1,800\times 1,296}$ representing spherical means tomography.  The observations contain $2\%$ Gaussian white additive noise, i.e., $\frac{\sigma\norm[2]{\bfxi}}{\norm[2]{\bfA\bfx_{\rm true}}}=0.02$ where $\sigma$ is the standard deviation and $\bfxi\sim\calN\left(\bf0,\bfI\right)$. Using this setup, the hyperparameter associated with the noise, $\lambda$, should be $\frac{1}{\sigma^2}\approx 7.11$. Additionally, the prior covariance matrix $\bfQ$ represents a Mat\'ern kernel with $\nu=1/2$ and $\ell = 1/4$. 

We compute $T=500$ samples using the Metropolis-Hastings within Gibbs method with proposal sampling using the genGK approximation (\Cref{alg:gibbs_genGK}) with $k=$ 500, 750, and 1,000.  
In \Cref{tab:genGK_seismic} we provide the number of accepted samples, the mean and variance images for reconstructions, and the distribution of computed hyperparameters $\lambda$ and $\delta$.  We observe that as the rank of the approxiomation $\widehat\bfGamma_{\rm cond}$ increases, $g_1(\bfx)$ becomes closer to the target distribution $h(\bfx)$ and the acceptance rate approaches $100\%$. However, for this problem, the mean and variance are comparable regardless of the rank. In terms of the $\lambda$ chain, the distributions converge roughly to the true value with $95\%$ confidence intervals of $[7.02,\ 8.14], \ [7.03,\ 8.14]$, and $[7.03,\ 8.17]$ for $k=500,\ 750$ and $1,000$ respectively.

An analysis of the $\lambda$ and $\delta$ chains are provided in \Cref{tab:lamchain} and \Cref{tab:delchain}
respectively. For the $\lambda$ chain, each of the ACF quickly decays to 0 meaning there is little correlation between samples. Given the ESS for $\lambda$ with a rank $k=500$ approximation exceeds the number of samples accepted, this indicates not enough samples were drawn. However, for $k=750$ and $k=1,000$, the ESS shows that a majority of $\lambda$ samples are independent. Compared to $\lambda$, the $\delta$ chain has a significantly smaller ESS for each rank and requires a higher rank to exhibit a fast ACF decay. For both $\lambda, \ \delta$ chains, the $p$-values for the Geweke null hypothesis are greater than $.88$ for all ranks giving strong evidence the chains are in equilibrium.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{Fig1.png}
    \caption{True reconstruction for $36\times36$ \texttt{PRspherical} seismic tomography test problem}
    \label{fig:trueTomo}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tomo: genGK with MH

\begin{table}[bthp]
\begin{tabular}{|cccccc|}
\hline
\multicolumn{6}{|c|}{\textbf{500 Samples using Metropolis-Hastings within Gibbs: genGK}} \\ \hline
\textbf{$k$} & \textbf{acc.} & \hspace{1.1cm}\textbf{Mean} & \hspace{1cm}\textbf{Variance} & \hspace{1.4cm}\textbf{$\lambda$} & \hspace{.7cm}\textbf{$\delta$} \\  
\raisebox{1.7cm}{500} & \raisebox{1.7cm}{83} 
& \multicolumn{4}{c|}{\includegraphics[width=.8\textwidth]{Fig2.png}}
\\
\raisebox{1.7cm}{750} & \raisebox{1.7cm}{334} &  \multicolumn{4}{c|}{\includegraphics[width=.8\textwidth]{Fig3.png}}
\\
\raisebox{1.7cm}{1,000} & \raisebox{1.7cm}{493} & \multicolumn{4}{c|}{\includegraphics[width=.8\textwidth]{Fig4.png}}
\\ \hline
\end{tabular}
\caption{Results for $T=500$ samples using \Cref{alg:gibbs_genGK} on a \texttt{PRspherical} test problem.The proposed distribution $g_1(\bfx)$ uses genGK for the approximate covariance matrix $\bfGamma$. In column 1, $k$ denotes the rank of $\widehat\bfGamma$ and the second column provides the number of accepted samples. The mean and variance represent the mean and variance of all accepted samples after burn-in. The $\lambda$ and $\delta$ distributions contain all draws from $\pi_\lambda$ and $\pi_\delta$ after burn-in.}
\label{tab:genGK_seismic}
\end{table}

\begin{table}[bthp]
\begin{tabular}{|ccccc|}
\hline
\multicolumn{5}{|c|}{\textbf{ $\lambda$ Chain Analysis for 500 Samples using Metropolis-Hastings within Gibbs: genGK}} \\ \hline
\textbf{$k$} & \textbf{p-value} & \textbf{ESS} & \textbf{$\lambda$ Samples} & \textbf{Autocorrelation} \\  
\raisebox{1.5cm}{500} & \raisebox{1.5cm}{.98} & \raisebox{1.5cm}{193.23} & {\includegraphics[width=.23\linewidth]{Fig5.png}} & {\includegraphics[width=.23\linewidth]{Fig6.png}} 
\\
\raisebox{1.5cm}{750} & \raisebox{1.5cm}{.99} & \raisebox{1.5cm}{331.2} & {\includegraphics[width=.23\linewidth]{Fig7.png}} & {\includegraphics[width=.23\linewidth]{Fig8.png}} 
\\
\raisebox{1.5cm}{1,000} & \raisebox{1.5cm}{.99} & \raisebox{1.5cm}{338.48} & {\includegraphics[width=.23\linewidth]{Fig9.png}} & {\includegraphics[width=.23\linewidth]{Fig10.png}} 
\\
\hline
\end{tabular}
\caption{Analysis of $\lambda$ chain for $T=500$ samples using Algorithm \ref{alg:gibbs_genGK} on a \texttt{PRspherical} test problem. Here, $k$ denotes the rank of $\widehat\bfGamma$, p-value gives the probablity of accepting or rejecting the Geweke null hypothesis and ESS provides essential sample size. Then, $\lambda$ Samples gives a plot of the sample value of $\lambda^j$ and the last column plots the autocorrelation function.}
\label{tab:lamchain}
\end{table}

\begin{table}[bthp]
\begin{tabular}{|ccccc|}
\hline
\multicolumn{5}{|c|}{\textbf{ $\delta$ Chain Analysis for 500 Samples using Metropolis-Hastings within Gibbs: genGK}} \\ \hline
\textbf{$k$} & \textbf{p-value} & \textbf{ESS} & \textbf{$\delta$ Samples} & \textbf{Autocorrelation} \\  
\raisebox{1.5cm}{500} & \raisebox{1.5cm}{.91} & \raisebox{1.5cm}{11.63} & {\includegraphics[width=.23\linewidth]{Fig11.png}} & {\includegraphics[width=.23\linewidth]{Fig12.png}} 
\\
\raisebox{1.5cm}{750} & \raisebox{1.5cm}{.88} & \raisebox{1.5cm}{83.12} & {\includegraphics[width=.23\linewidth]{Fig13.png}} & {\includegraphics[width=.23\linewidth]{Fig14.png}} 
\\
\raisebox{1.5cm}{1,000} & \raisebox{1.5cm}{.96} & \raisebox{1.5cm}{94.21} & {\includegraphics[width=.23\linewidth]{Fig15.png}} & {\includegraphics[width=.23\linewidth]{Fig16.png}} 
\\
\hline
\end{tabular}
\caption{Analysis of $\delta$ chain for $T=500$ samples using Algorithm \ref{alg:gibbs_genGK} on a \texttt{PRspherical} test problem. Here, $k$ denotes the rank of $\widehat\bfGamma$, p-value gives the probablity of accepting or rejecting the Geweke null hypothesis and ESS provides essential sample size. Then, $\delta$ Samples gives a plot of the sample value of $\delta^j$ and the last column plots the autocorrelation function.}
\label{tab:delchain}
\end{table}

Since this problem is small enough such that we can compute the singular value decomposition (SVD), we also compare to the Metropolis-Hastings within Gibbs method with low-rank proposal sampling given by truncated SVD (TSVD) and randomized SVD (rSVD) approximations with ranks of 500, 750, and 1,000. Details for using these alternative methods can be found in \Cref{sec:svdapprox}.
In \Cref{tab:svd} and \Cref{tab:rsvd} we provide the number of accepted samples, as well as the mean and variance image for $\bfx$ and the $\lambda$ and $\delta$ distributions using TSVD and rSVD approximations respectively.
Compared with the results in \Cref{tab:genGK_seismic}, we observe that the number of accepted samples for genGK is higher than that for rSVD and slightly lower than that for SVD.  However, the mean and variance image for $\bfx$ and the $\lambda$ and $\delta$ distributions are all very similar.
In each experiment, 450 samples after burn-in are used to compute the means and variances.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tomo: svd with MH
\begin{table}[bthp]
\begin{tabular}{|cccccc|}
\hline
\multicolumn{6}{|c|}{\textbf{500 Samples using Metropolis-Hastings within Gibbs: TSVD}} \\ \hline
\textbf{$k$} & \textbf{acc.} & \hspace{1.1cm}\textbf{Mean} & \hspace{1cm}\textbf{Variance} & \hspace{1.4cm}\textbf{$\lambda$} & \hspace{.7cm}\textbf{$\delta$} \\ 
\raisebox{1.7cm}{500} & \raisebox{1.7cm}{179} &  \multicolumn{4}{c|}{\includegraphics[width=.8\textwidth]{Fig17.png}}
\\
\raisebox{1.7cm}{750} & \raisebox{1.7cm}{458} &  \multicolumn{4}{c|}{\includegraphics[width=.8\textwidth]{Fig18.png}}
\\
\raisebox{1.7cm}{1,000} & \raisebox{1.7cm}{499} &  \multicolumn{4}{c|}{\includegraphics[width=.8\textwidth]{Fig19.png}}
\\ \hline
\end{tabular}
\caption{Results for $T=500$ samples using Algorithm \ref{alg:MHwithinGibbs} on a \texttt{PRspherical} test problem. The proposed distribution $g_1(\bfx)$ uses the TSVD for the approximate covariance matrix $\bfGamma$. In column 1, $k$ denotes the rank of $\widehat\bfGamma$ and the second column provides the number of accepted samples. The mean and variance represent the mean and variance of all accepted samples after burn-in. The $\lambda$ and $\delta$ distributions contain all draws from $\pi_\lambda$ and $\pi_\delta$ after burn-in.}
\label{tab:svd}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Tomo: svd with MH
\begin{table}[bthp]
\begin{tabular}{|cccccc|}
\hline
\multicolumn{6}{|c|}{\textbf{500 Samples using Metropolis-Hastings within Gibbs: rSVD}} \\ \hline
\textbf{$k$} & \textbf{acc.} & \hspace{1.1cm}\textbf{Mean} & \hspace{1cm}\textbf{Variance} & \hspace{1.4cm}\textbf{$\lambda$} & \hspace{.7cm}\textbf{$\delta$} \\  

\raisebox{1.7cm}{500} & \raisebox{1.7cm}{15} &
\multicolumn{4}{c|}{\includegraphics[width=.8\textwidth]{Fig20.png}}
\\
\raisebox{1.7cm}{750} & \raisebox{1.7cm}{217} &  \multicolumn{4}{c|}{\includegraphics[width=.8\textwidth]{Fig21.png}}
\\
\raisebox{1.7cm}{1,000} & \raisebox{1.7cm}{462} &  \multicolumn{4}{c|}{\includegraphics[width=.8\textwidth]{Fig22.png}}
\\ \hline
\end{tabular}
\caption{Results for $T=500$ samples using Algorithm \ref{alg:MHwithinGibbs} on a \texttt{PRspherical} test problem. The proposed distribution $g_1(\bfx)$ uses the rSVD for the approximate covariance matrix $\bfGamma$. In column 1, $k$ denotes the rank of $\widehat\bfGamma$ and the second column provides the number of accepted samples. The mean and variance represent the mean and variance of all accepted samples after burn-in. The $\lambda$ and $\delta$ distributions contain all draws from $\pi_\lambda$ and $\pi_\delta$ after burn-in.}
\label{tab:rsvd}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Atmospheric Inverse Modeling}
\label{sub:atmospheric}

Now we consider an atmospheric transport problem where $\bfA\in\bbR^{98,880\times 3,222}$ represents a forward atmospheric transport model from NOAA's CarbonTracker-Lagrange project \cite{GeostatInvModel,DataReductInvModel} modeling simulations from the weather research and forecasting stochastic time-inverted Lagrangian transport model \cite{LagrangTransport,LagrangTransportModel} sampled at 3,222 grid locations covering North America. The goal is to approximate $\bfx\in\bbR^{3,222}$, the vectorized true average fluxes at the grid locations. The observations $\bfb\in\bbR^{98,880}$ are sampled from OCO-2 during July through mid-August 2015. For this problem, we do not use realistic CO$_2$ emissions. Instead, $\bfx_{\rm true}$ is a randomly generated vectorized emission map that is used to produce the observations $\bfb$. We add Gaussian white noise corresponding to a $50\%$ noise level to the observations, i.e., $\frac{\sigma\norm[2]{\bfxi}}{\norm[2]{\bfA\bfx_{\rm true}}}=0.50$ where $\sigma$ is the standard deviation and $\bfxi\in\bbR^{98,880}\sim\calN\left(\bf0,\bfI\right)$. Using this setup, the hyperparameter associated with the noise, $\lambda$, should converge to $\frac{1}{\sigma^2}\approx 64.35$ for our generated $\bfxi$. 

Let $\bfS\in\{0,1\}^{3,222\times 11,900}$ be the sampling matrix that extracts the 3,222 grid locations over North America from the entire $11,900 \times 11,900$ grid. Then, the prior covariance matrix $\bfQ\in\bbR^{3,222\times 3,222}$ can be created by sampling a Mat\'ern kernel, $\bfQ_M^{11,900\times 11,900}$, with $\nu=2.5$ and $\ell = 0.05$,
\begin{equation}
    \bfQ = \bfS\bfQ_M\bfS\t.
\end{equation}

We compute $T=500$ samples using the Metropolis-Hastings within Gibbs method with proposal sampling using the genGK approximation (\Cref{alg:gibbs_genGK}) with rank $k=750$. In \Cref{fig:atmos} we provide the true image along with the mean and variance images of the 450 samples after burn-in and the distributions of computed hyperparameters $\lambda$ and $\delta$. It should be noted that when plotting the variance of the solution, the colorbar is set to have a maximum value of $0.07$ to give a better representation of the variance throughout the grid by excluding the high variances found only at the boundaries. 
This setup produced an acceptance rate of $63\%$ and it was found that the distribution of the $\lambda$ chain converges to the true value with a $95\%$ confidence interval of $[64.06,\ 65.22]$.
In \Cref{tab:atmochain}, an analysis of the $\lambda$ samples shows that the chain is in equilibrium given a $p$-value of $0.99$. The ACF for $\lambda$ gives evidence that there is little to no correlation in the chain and an ESS of 308 shows most all of the accepted samples are independent. However, the $\delta$ chain produced a smaller $p$-value and ESS than $\lambda$. Combined with a slower ACF decay, the $\delta$ samples from \Cref{alg:gibbs_genGK} are more dependent and potentially out of equilibrium. This can be seen in the plot of $\delta$ samples which follows an upward trend. 

%%%%% Atmo Results Not in table:
\begin{figure}[bthp]
    \centering
    \begin{tabular}{ccc}
         \textbf{True} & 
         \textbf{Mean} & \textbf{Variance}  \\
         \includegraphics[width=.3\textwidth]{Fig23.png}& 
         \includegraphics[width=.3\textwidth]{Fig24.png} & \includegraphics[width=.3\textwidth]{Fig25.png} \\ &
         $\lambda$ Distribution & $\delta$ Distribution \\ &
         \includegraphics[width=.3\textwidth]{Fig26.png} & \includegraphics[width=.3\textwidth]{Fig27.png}
    \end{tabular}
    \caption{True atmospheric image and results for $T=500$ samples from Algorithm \ref{alg:gibbs_genGK} for the atmospheric transport problem with an approximate covariance matrix of rank $k=750$. The mean and variance are taken over the 315/500 accepted samples after burn-in. The $\lambda$ and $\delta$ distributions are over all samples after burn-in.}
    \label{fig:atmos}
\end{figure}
%\begin{table}[bthp]
\begin{table}[bthp]
\begin{tabular}{|c|cccc|}
\hline
\multicolumn{5}{|c|}{\textbf{ Chain Analysis for 500 Samples using Metropolis-Hastings within Gibbs: genGK}} \\ \hline \textbf{Param.} & \textbf{p-value} & \textbf{ESS} & \textbf{Samples} & \hspace{-.1cm}\textbf{Autocorrelation} \\  

\raisebox{1.7cm}{$\lambda$} & \hspace{-.2cm}\raisebox{1.7cm}{.99} & \raisebox{1.7cm}{308.76} & \raisebox{.15cm}{\includegraphics[width=.28\linewidth]{Fig28.png}} & \hspace{-.28cm}\raisebox{.15cm}{\includegraphics[width=.3\linewidth]{Fig29.png}} 
\\
\raisebox{1.7cm}{$\delta$} & \hspace{-.2cm}\raisebox{1.7cm}{.64} & \raisebox{1.7cm}{13.23} & \raisebox{.15cm}{\includegraphics[width=.28\linewidth]{Fig30.png}} & \hspace{-.28cm}\raisebox{.15cm}{\includegraphics[width=.3\linewidth]{Fig31.png}} 
\\
\hline
\end{tabular}
\caption{Analysis of $\lambda$ and $\delta$ chain for $T=500$ samples using Algorithm \ref{alg:gibbs_genGK} on the Atmospheric transport problem with a rank of $k=750$. Here, p-value gives the probablity of accepting or rejecting the Geweke null hypothesis and ESS provides essential sample size. Samples gives a plot of the hyperparameter values at the $j^{th}$ sample and the last column plots the autocorrelation function.}
\label{tab:atmochain}
\end{table}


\subsection{Dynamic Photoacoustic Tomography}
\label{sub:PAT}
Here we consider a dynamic photoacoustic tomography test problem, based on the \texttt{PRspherical} example in IRTools\cite{gazzola2019ir}. For this example, 20 $64\times 64$ true images were generated using two Gaussians moving in different directions. Such problems require a spatiotemporal prior, and we use a Kronecker product $\bfQ = \bfQ_t \kron \bfQ_s \in\bbR^{81,920 \times 81,920}$ where $\bfQ_t\in\bbR^{20\times 20}$ and $\bfQ_s\in\bbR^{4,096\times 4,096}$ are temporal and spatial priors corresponding to Mat\'ern kernels with $\nu=2.5, \ \ell=0.1$ and $\nu=0.5, \ \ell=0.25$ respectively. The linear problem can be formed by combining the subproblem at each time point $i$ as follows,
$$
\bfx = \begin{bmatrix} \bfx_1 \\ \vdots \\ \bfx_{20} \end{bmatrix}, \quad \bfA = \begin{bmatrix}
    \bfA_1 & & \\ & \ddots & \\ & & \bfA_{20}
\end{bmatrix}, \quad \text{and} \quad \bfb = \begin{bmatrix}
    \bfb_1 \\ \vdots \\ \bfb_{20}
\end{bmatrix},
$$
where $\bfA_i \in \bbR^{1,638 \times 4,096}$ is a spherical projection matrix corresponding to 18 equally spaced angles between $i$ and $340+i$ for $i=1,\ldots,20$, $\bfx_i\in\bbR^{4,096}$ is the vectorized image at $i$, and $\bfb_i \in\bbR^{1,638}$ is the simulated projection data. We add Gaussian white noise corresponding to a $2\%$ noise level to the observations, i.e., $\frac{\sigma\norm[2]{\bfxi}}{\norm[2]{\bfA\bfx_{\rm true}}}=0.02$ where $\sigma$ is the standard deviation and $\bfxi\sim\calN\left(\bf0,\bfI\right)$. Using this setup, the hyperparameter associated with the noise, $\lambda$, should be $\frac{1}{\sigma^2}\approx 2.02 \times 10^4$.
Given the small size of $\bfQ_t$, we can obtain the Cholesky factorization $\bfQ_t^{-1} = \bfG_t\t\bfG_t$. Now we can define a preconditioner of the form $\bfG=\bfG_t \kron \bfG_s$ with $\bfG_s$ being the Cholesky factorization of $(-\Delta)^{\gamma}$ for $\gamma\geq 1$ where $\Delta$ the Laplacian operator discretized using finite difference. 

The goal is this problem is to obtain a sequence of images from a sequence of projection datasets. 
\Cref{fig:PATtrue} contains the true image for 5 time points.  We used a genGK approximation to the target distribution, but due to the large rank of this problem, the proposal failed to accept a sufficient number of samples without taking $k$ to be very large. Thus, we used the preconditioned Lanczos method from \Cref{alg:gibbs_precond}. In \Cref{fig:PAT500} we provide the results for $T=500$ samples. The approximate mean, $\bfx_k$, is obtained using $k=$1,000 genGK iterations. Additionally, the corresponding $\lambda$ and $\delta$ distributions are given in \Cref{fig:PAT500_dist}.  We also provide a sample from the prior.
We observe that the method produced a $100\%$ acceptance rate. While this is higher than that of the previous examples, it is expected given an exact factorization of $\bfGamma_{\rm cond}$ is used.
While the $\lambda$ distribution approaches the true value, it is outside the $95\%$ confidence interval of $[19,341.09, \ 20,057.78]$. This might be improved by increasing the rank of the approximate mean $\bfx_k$, but the magnitude of the hyperparameter could potentially make converging to the true value more difficult. 

In \Cref{tab:genGK_dynamic} we provide an analysis of the $\lambda$ and $\delta$ chains and observe that \Cref{alg:gibbs_precond} produces samples of $\lambda$ and $\delta$ that have $p$-values of .99 providing strong evidence the chains are in equilibrium. The ACFs corresponding to both $\lambda$ and $\delta$ quickly decay to 0 so the chains are highly uncorrelated. Finally, from the ESS we see that a majority of the $\lambda$ and $\delta$ samples are independent. In comparison with the previous two examples, \Cref{alg:gibbs_precond} produced a better $\delta$ chain.


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Fig32.png}
    \caption{True $64\times 64$ images for the dynamic photoacoustic tomography problem at time points $i=1,5,10,15, \rm and \ 20$.}
    \label{fig:PATtrue}
\end{figure}


\begin{figure}[h]
    \begin{subfigure}[]{1\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{Fig33.png}    
    \end{subfigure} \hfill 
    \begin{subfigure}[]{1\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{Fig34.png}    
    \end{subfigure} 
    \caption{Results for 500/500 accepted samples from Algorithm \ref{alg:gibbs_precond} for the dynamic photoacoustic tomography test problem with an approximate mean with $k=1,000$ genGK iterations for time points $i=1,5,10,15,$ and $20$. Top: Random sample from the prior. Middle: Means of accepted samples after burn-in.
    Bottom: Variances of accepted samples after burn-in.}
    \label{fig:PAT500}
\end{figure}

\begin{figure}[bthp]
    \centering
    \begin{tabular}{cc}
     $\lambda$ Distribution & $\delta$ Distribution  \\
     \includegraphics[width=.35\textwidth]{Fig35.png}    & 
     {\includegraphics[width=.35\textwidth]{Fig36.png}}
    \end{tabular}
    
    \caption{$\lambda$ and $\delta$ distributions of $T=500$ samples from Algorithm \ref{alg:gibbs_precond} for the dynamic photoacoustic tomography test problem with an approximate mean with $k=1,000$ genGK iterations }
    \label{fig:PAT500_dist}
\end{figure}

\begin{table}[bthp]
\begin{tabular}{|c|cccc|}
\hline
\multicolumn{5}{|c|}{\textbf{ Chain Analysis for $500$ Samples using Metropolis-Hastings within Gibbs: genGK}} \\ \hline \textbf{Param.} & \textbf{p-value} & \textbf{ESS} & \textbf{Samples} & \hspace{-.1cm}\textbf{Autocorrelation} \\  

\raisebox{1.7cm}{$\lambda$} & \raisebox{1.7cm}{.99} & \raisebox{1.7cm}{292.77} & \raisebox{.15cm}{\includegraphics[width=.28\linewidth]{Fig37.png}} & \hspace{-.28cm}\raisebox{.15cm}{\includegraphics[width=.27\linewidth]{Fig38.png}} 
\\
\raisebox{1.7cm}{$\delta$} & \raisebox{1.7cm}{.99} & \raisebox{1.7cm}{365.99} & \raisebox{.15cm}{\includegraphics[width=.28\linewidth]{Fig39.png}} & \hspace{-.28cm}\raisebox{.15cm}{\includegraphics[width=.27\linewidth]{Fig40.png}}
\\
\hline
\end{tabular}
\caption{Analysis of $\lambda$ and $\delta$ chain for $T=500$ samples using Algorithm \ref{alg:gibbs_precond} photoacoustic tomography test problem with an approximate mean using $k=1,000$ genGK iterations. Here, p-value gives the probablity of accepting or rejecting the Geweke null hypothesis and ESS provides essential sample size. Samples gives a plot of the hyperparameter values at the $j^{th}$ sample and the last column plots the autocorrelation function.}
\label{tab:genGK_dynamic}
\end{table}


\section{Conclusions}
\label{sec:conclusions}
This paper provides an approach to perform uncertainty quantification for large-scale hierarchical Bayesian inverse problems, where Metropolis-Hastings independence sampling within Gibbs is used to overcome the limitations of Markov chain Monte Carlo methods.  We consider two proposal distributions, both of which are based on generalized Golub-Kahan methods. First, using a genGK based approach to form an approximation of the distribution, we are able to reuse the genGK matrices to efficiently draw samples by forming a low-rank approximation to the conditional covariance matrix. Second, for matrices where a low-rank approximation is not sufficient, we describe a preconditioned Lanczos based method to efficiently perform computations with the square-root of the conditional covariance to draw proposal samples. In addition to comparisons with existing approaches for small problems, we demonstrate the performance of these methods on a variety of large-scale inverse problems, including atmospheric models and dynamic tomography problems.


\backmatter


\section*{Statements and Declarations}

% Some journals require declarations to be submitted in a standardized format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item Funding:
This work was partially supported by the National Science Foundation program under grants DMS-2411197 and DMS-2038118. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
\item Conflict of interest/Competing interests: 
On behalf of all authors, the corresponding author states that there is no conflict of interest.
\item Ethics approval and consent to participate: Not applicable
\item Consent for publication: Not applicable
\item Data availability: Not applicable
\item Materials availability: Not applicable
\item Code availability: Not applicable
\item Author contribution: Both authors contributed equally to this work.
\end{itemize}

\begin{appendices}
\section{Low-Rank Representation of Square-Root Covariance} \label{sec:lowrank}

First note that operations with the square-root matrix $\bfQ^{1/2}$ can be performed efficiently, using a Lanczos algorithm that only requires mat-vecs with $\bfQ$, see \cite{UQlargebayesian} and references therein. Next recall after $k$ iterations of the genGK bidiagonalization process, we have  
\begin{equation*}
    \bfQ^{1/2}\bfA\t\bfR^{-1}\bfA\bfQ^{1/2} \approx \bfQ^{1/2}\bfV_{k}\bfB_{k}\t\bfB_{k}\bfV_{k}\t\bfQ^{1/2}.
\end{equation*}
Let $\bfQ^{(1)}\bfR^{(1)} = \bfB_{k}$ and $\bfQ^{(2)}\bfR^{(2)} = \bfQ^{1/2}\bfV_{k}(\bfR^{(1)})\t$ be QR decompositions, then
\begin{align*}
    \bfQ^{1/2}\bfV_{k}\bfB_{k}\t\bfB_{k}\bfV_{k}\t\bfQ^{1/2} %&= \bfQ^{1/2}\bfV_{k}(\bfR^{(1)})\t\bfR^{(1)}\bfV_{k}\t\bfQ^{1/2} \\
    &= \bfQ^{(2)}\bfR^{(2)}(\bfR^{(2)})\t(\bfQ^{(2)})\t.
\end{align*}
Taking the eigenvalue decomposition $\bfR^{(2)}(\bfR^{(2)})\t = \bfW_k \mathbf{\Theta}_{k}\bfW_{k}\t$ and assigning $\bfZ_k = \bfQ^{(2)}\bfW_k$, we get the low-rank representation,
\begin{equation}
    \bfQ^{1/2}\bfA\t\bfR^{-1}\bfA\bfQ^{1/2} \approx \bfZ_k\mathbf{\Theta}_{k}\bfZ_k \quad \text{where} \quad \mathbf{\Theta}_{k} = \diag{[\theta_{1},\ldots,\theta_{k}]}.
\end{equation}

To compute the inverse square-root $\left( \delta\bfI + \lambda\bfQ^{1/2}\bfA\t\bfR^{-1}\bfA\bfQ^{1/2} \right)^{-1/2}$, following \cite{ghattasinfinitebayes}, we use the Woodbury identity to get
\begin{align*}
    \left( \delta\bfI + \lambda\bfQ^{1/2}\bfA\t\bfR^{-1}\bfA\bfQ^{1/2} \right)^{-1} %&= \delta^{-1}\left( \bfI + (\lambda/\delta)\bfQ^{1/2}\bfA\t\bfR^{-1}\bfA\bfQ^{1/2}\right)^{-1} \\
    &= \delta^{-1}\left(\bfI + \bfZ_{k}(\lambda/\delta)\mathbf{\Theta}_k \bfZ_k \t \right)^{-1} \\
    &= \delta^{-1}\left(\bfI - \bfZ_{k}\left(\bfI_k + (\lambda/\delta)^{-1}\mathbf{\Theta}_k^{-1} \right)^{-1}\bfZ_{k}\t\right).
\end{align*}

 Then we can compute a square-root matrix as
 \begin{align}
    \delta^{-1/2} \left(\bfI + \bfZ_{k}(\lambda/\delta)\mathbf{\Theta}_k \bfZ_k \t \right)^{-1/2}
     &= \delta^{-1/2}\left(\bfI_{n} - \bfZ_{k}\bfD_{k}\bfZ_{k}\t\right)
 \end{align}
 where $\bfD_k \equiv \bfI_k - \left((\lambda/\delta)\mathbf{\Theta}_k + \bfI_k\right)^{-1/2}$.

\section{Prior norm for genGK proposal}
To evaluate $\norm[\bfQ^{-1}]{\bfx^{t-1}}^2$, we use an equivalent expression that avoids computations with $\bfQ^{-1}$. Let $\bfx_k=\bfQ\bfV_k\bfz_k$ be the genGK approximation used as the mean of the distribution from which $\bfx^{t-1}$ was drawn and $\bfD_k$ be the matrix used to form the corresponding $\widehat\bfGamma_{\rm cond}^{1/2}$.
Then, the norm can be written as
\begin{align*}
    & \norm[\bfQ^{-1}]{\bfx^{t-1}}^2 \\
    &= \left(\bfx^{t-1}\right)\t \bfQ^{-1}\left(\bfx^{t-1}\right) \\
    & = \left(\bfQ\bfV_{k}\bfz_k + \frac{\bfQ^{1/2}}{\sqrt{\delta}}\left(\bfI - \bfZ_{k}
\bfD_{k}\bfZ_{k}\t\right)\bfxi\right)\t \bfQ^{-1}\left(\bfQ\bfV_{k}\bfz_k + \frac{\bfQ^{1/2}}{\sqrt{\delta}}\left(\bfI - \bfZ_{k}
\bfD_{k}\bfZ_{k}\t\right)\bfxi\right) \\
&= \left(\bfQ^{1/2}\bfV_{k}\bfz_k + \delta^{-1/2}\left(\bfI - \bfZ_{k}
\bfD_{k}\bfZ_{k}\t\right)\bfxi\right)\t \left(\bfQ^{1/2}\bfV_{k}\bfz_k + \delta^{-1/2}\left(\bfI - \bfZ_{k}
\bfD_{k}\bfZ_{k}\t\right)\bfxi\right) \\
&= \left((\bfV_{k}\bfz_k)\t\bfQ^{1/2} + \delta^{-1/2}\bfxi\t\left(\bfI - \bfZ_{k}
\bfD_{k}\bfZ_{k}\t\right)\right) \left(\bfQ^{1/2}\bfV_{k}\bfz_k + \delta^{-1/2}\left(\bfI - \bfZ_{k}
\bfD_{k}\bfZ_{k}\t\right)\bfxi\right)
\end{align*}
so finally we have
\begin{equation}
    \norm[\bfQ^{-1}]{\bfx^{t-1}}^2 = \left(\bfV_k\bfz_k\right)\t(\bfx_k +2\widehat\bfGamma_{\rm cond}^{1/2}\bfxi) + \bfxi\t\delta^{-1}\left(\bfI- \bfZ_k\left((\bfD_k)^2-2\bfD_k\right)\bfZ_k\t\right)\bfxi.
\end{equation} 

\section{Derivation of acceptance ratio and prior norm for \Cref{sub:MH_genGK}}\label{sec:accept_norm_precond}

Let $\bfx_k$ be the genGK approximation used as the mean of the distribution from which $\bfx^{t-1}$ was drawn.
Consider the log of the full acceptance ratio,
\begin{align*}
&\log\alpha_2(\bfx^{t-1},\bfx^{\star}) \\ &\quad = -\frac{1}{2}\Big( (\bfx^{\star} - \bfx_{\rm cond})\t \bfGamma_{\rm cond}^{-1}(\bfx^{\star} - \bfx_{\rm cond}) + (\bfx^{t-1} - \bfx_{k})\t \bfGamma_{\rm cond}^{-1}(\bfx^{t-1} - \bfx_{k})  \\
& \qquad - (\bfx^{t-1} - \bfx_{\rm cond})\t \bfGamma_{\rm cond}^{-1}(\bfx^{t-1} - \bfx_{\rm cond}) + (\bfx^{\star} - \bfx_{k})\t \bfGamma_{\rm cond}^{-1}(\bfx^{\star} - \bfx_{k}) \Big) \\
& \quad = \bfx_{\rm cond}\t\bfGamma_{\rm cond}^{-1}\bfx^{\star} + \bfx_{k}\t\bfGamma_{\rm cond}^{-1}\bfx^{t-1} - \bfx_{\rm cond}\t\bfGamma_{\rm cond}^{-1}\bfx^{t-1} - \bfx_{k}\t\bfGamma_{\rm cond}^{-1}\bfx^{\star},
\end{align*}
after expanding and canceling like terms. Using $\bfx_{\rm cond} = \lambda\bfGamma_{\rm cond}\bfA\t\bfR^{-1}\bfb$, $\bfGamma_{\rm cond}^{-1} = \lambda\bfA\t\bfR^{-1}\bfA + \delta\bfQ^{-1}$, and $\bfx_k = \bfQ\bfV_k\bfz_k$, the equation reduces to
\begin{align*}
\log\alpha_2(\bfx^{t-1},\bfx^{\star}) &= \lambda(\bfA\t\bfR^{-1}\bfb)\t\bfx^\star + (\lambda\bfx_k\t\bfA\t\bfR^{-1}\bfA + \delta(\bfV_k\bfz_k)\t)\bfx^{t-1} \\
& \quad - \lambda(\bfA\t\bfR^{-1}\bfb)\t\bfx^{t-1} - (\lambda\bfx_k\t\bfA\t\bfR^{-1}\bfA + \delta(\bfV_k\bfz_k)\t)\bfx^{\star} \\
&=\left( \bfx^{\star} - \bfx^{t-1} \right)\t \left( \lambda\bfA\t\bfR^{-1}\left(\bfb - \bfA\bfx_k\right) - \delta\bfV_k\bfz_k\right)\\
&= \left( \bfx^{\star} - \bfx^{t-1} \right)\t \left( \lambda\left(\gamma_1\alpha_1\bfv_1 - \bfV_{k+1}\begin{bmatrix}
    \bfB_k\t \\ \alpha_{k+1}\bfe_{k+1}\t
\end{bmatrix} \bfB_k\bfz_k\right)- \delta\bfV_k\bfz_k\right).
\label{eq:accept_precond}
\end{align*}
Note that 
\begin{equation}
    \bfV_{k+1}\begin{bmatrix}
    \bfB_k\t \\ \alpha_{k+1}\bfe_{k+1}\t
\end{bmatrix} \bfB_k\bfz_k = \bfV_k\bfb_k\t\bfB_k\bfz_k + \alpha_{k+1}\gamma_{k+1}\left( \bfe_{k}\t\bfz_k \right)\bfv_{k+1}.
\end{equation}
Next, to avoid $\bfQ^{-1}$ when computing $\norm[\bfQ^{-1}]{\bfx^{t-1}}^2$, notice that the norm can be equivalently written as
\begin{align*}
\norm[\bfQ^{-1}]{\bfx^{t-1}}^2 &= \left(\bfx^{t-1}\right)\t\bfQ^{-1}\left(\bfx^{t-1}\right) \\ 
&= \left(\bfx^{t-1}\right)\t\bfQ^{-1}\left(\bfx_k + \bfS_F\bfxi\right) \\ 
& = \left(\bfx^{t-1}\right)\t\bfQ^{-1}\left(\bfQ\bfV_k\bfz_k + \lambda^{-1/2}\bfQ\bfG\t\left(\bfG\bfF\bfG\t\right)^{-1/2}\bfxi\right) 
\end{align*}
where $\bfF = \frac{\delta}{\lambda}\bfQ+\bfQ\bfA\t\bfR^{-1}\bfA\bfQ$. Distributing $\bfQ^{-1}$ results in
\begin{equation}
    \norm[\bfQ^{-1}]{\bfx^{t-1}}^2 = \left(\bfx^{t-1}\right)\t\left( \bfV_k\bfz_k + \lambda^{-1/2}\bfG\t\left(\bfG\bfF\bfG\t\right)^{-1/2}\bfxi\right)
\end{equation}
which reuses the previously computed $\bfV_k\bfz_k$ and $\bfG\t\left(\bfG\bfF\bfG\t\right)^{-1/2}\bfxi$.

\section{SVD and rSVD approximations of the conditional covariance matrix} \label{sec:svdapprox}
Assume the factorization $\bfQ=\bfL\t\bfL$ is accessible. Then, for fixed $\lambda$ and $\delta$,
\begin{align*}
\bfGamma_{\rm cond} &= \left(\delta\bfL^{-1}\bfL^{-\top} + \lambda\bfA\t\bfR^{-1}\bfA\right)^{-1} \\
&= \bfL\t\left(\delta\bfI + \lambda\bfL\bfA\t\bfR^{-1}\bfA\bfL\t \right)^{-1}\bfL.
\end{align*}
To form a low-rank approximation of $\bfH = \bfL\bfA\t\bfR^{-1}\bfA\bfL\t$, we consider two approaches. 

One approach is to use the truncated SVD (TSVD) where a rank $k$ approximation is formed by computing $\bfR^{-1/2}\bfA\bfL\t\approx\widehat\bfU_k\bfSigma_k\widehat\bfV_k$ where $\widehat\bfU_k\in\bbR^{m\times k}$ and $\widehat\bfV_k\in\bbR^{n \times k}$ are orthonormal matrices containing the first $k$ left and right singular vectors of $\bfR^{-1/2}\bfA\bfL\t$ respectively and $\bfSigma_k\in\bbR^{k\times k}$ is a diagonal matrix with the first $k$ largest singular values. Then $\bfH\approx \widehat\bfV_k\bfSigma^2_k\widehat\bfV_k\t$. 

Another approach is to use the randomized SVD (rSVD) algorithm which begins by using a random matrix $\bfOmega\in\bbR^{n \times (k+p)}$ whose entries are realizations of i.i.d. standard Gaussian random variables to approximate the column space of $\bfH$, i.e., $\bfY=\bfH\bfOmega$. Here $k$ is the target rank and $p$ is an oversampling parameter usually taken to be a small integer ($p=5$ or $p=10$). Compute a thin-QR factorization, $\bfY=\widehat\bfQ\widehat\bfR$. Now an approximation of $\bfH$ is given by
$$
  \bfH \approx \widehat\bfQ\widehat\bfQ\t\bfH\widehat\bfQ\widehat\bfQ\t 
  = \widehat\bfQ \Tilde{\bfU}\Tilde{\bfSigma} \Tilde{\bfU}\t\widehat\bfQ\t 
  = \Tilde{\bfV}\Tilde{\bfSigma}\Tilde{\bfV}\t
$$
where $\widehat\bfQ\t\bfH\widehat\bfQ = \Tilde{\bfU}\Tilde{\bfSigma} \Tilde{\bfU}\t$ is an eigendecomposition and $\Tilde{\bfV} = \widehat\bfQ\Tilde{\bfU}$.

For both methods, the approximation of $\bfH$ can be used to form a low-rank representation of $\bfGamma_{\rm cond}$ following a similar approach to the one in \Cref{sub:sampling_prop}. Using the rSVD approximation as a proposal sampler was considered in \cite{Saibaba2018LowRankIS}.

\end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{references}


\end{document}
