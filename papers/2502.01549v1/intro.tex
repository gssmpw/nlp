\section{Introduction}\label{sec:intro}

Recent advances in Large Language Models (LLMs) have revolutionized NLP, yet their performance is inherently limited by the knowledge captured during pre-training. To address this limitation, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm that enhances LLMs by dynamically retrieving and incorporating external knowledge during inference~\cite{ChunkRAG,gutierrez2024hipporag}. While RAG has demonstrated success across various text-based applications, such as question answering, and factual reasoning, its potential remains largely untapped in the rich domain of multi-modal content, particularly video understanding. The extension of RAG to video content presents unique challenges and opportunities, as videos contain complex multi-modal features, temporal dynamics, and intricate semantic relationships that go beyond traditional text-based knowledge integration approaches.

Although large vision models have achieved impressive progress in video understanding tasks, they face limitations when processing long-context videos. These models (\eg, VideoLLaMA3~\cite{VideoLLaMA3} and LLaVA-Video~\cite{LLaVA-Video}), primarily designed for short video clips, struggle to effectively capture and reason about temporal dependencies spanning multiple hours. The challenge becomes particularly acute in scenarios requiring cross-video understanding and knowledge integration, such as lecture series comprehension, documentary analysis, or sequential entertainment content interpretation. Current approaches often fragment long videos into isolated clips, leading to loss of contextual information and inability to establish meaningful connections across different videos. This limitation severely hampers applications in educational content analysis, media archiving, and video-based knowledge extraction, where understanding the broader context across multiple videos is essential.

The key challenges in realizing Retrieval-Augmented Generation for extreme long-context videos are multifaceted. (i) \textbf{Capturing Heterogeneous Video Knowledge}. Videos contain rich information across multiple modalities, including visual frames, audio streams, and textual descriptions. Effectively capturing and organizing this diverse knowledge presents a unique challenge that cannot be addressed by existing text-based RAG approaches. Existing methods are ill-equipped to handle the complexity of cross-modal information and their relationships. (ii) \textbf{Preserving Semantic Coherence for Cross-Video Understanding}. Maintaining the semantic connections across numerous videos, which may span hours or days, is more complex than a single video. Preserving these intricate relationships and comprehensive knowledge interdependencies is crucial for holistic video understanding. (iii) \textbf{Efficient Video Knowledge Retrieval}. When the video knowledge base consists of an unrestricted number of lengthy videos, quickly identifying the most pertinent clips in response to user queries becomes significantly more challenging. The retrieval system must provide users with the most relevant information to answer queries accurately.

By addressing these key challenges, the \model\ aims to unlock the full potential of RAG in the domain of extreme long-context videos, enabling powerful and comprehensive video understanding capabilities. At the heart of VideoRAG are two interlocking components - the \textbf{Multi-Modal Video Knowledge Indexing} framework and the \textbf{Knowledge-Grounded Multi-Modal Retrieval} paradigm. The indexing framework transforms video content into structured textual and visual representations, with graph-based textual knowledge grounding to preserve semantic relationships across videos, complemented by multi-modal context encoding to capture fine-grained cross-modal interactions. This dual-channel architecture enables VideoRAG to effectively organize and index long-context videos, preserving the rich semantics of the multimedia content. The knowledge-grounded retrieval paradigm then integrates textual semantic and visual content matching, leveraging the indexed knowledge graph and embeddings to identify the most relevant video content. Finally, VideoRAG employs a two-stage content extraction process that combines LLM-powered keyword extraction and vision-language model-based text grounding to enrich the visual analysis with text-based retrieval, generating comprehensive outputs for the final response.

The comprehensive evaluation on the benchmark datasets demonstrates the advantages and effectiveness of the \model\ framework in understanding extremely long-context videos, going beyond the limitations of existing RAG alternatives and large vision models. The results showcase \model's superior performance in effectively organizing and indexing long-form video content, allowing for precise retrieval of relevant segments across different video sources in response to user queries. Our ablation studies provide deeper insights into the individual contributions of \model's key components, highlighting the importance of the graph-based knowledge grounding and multi-modal retrieval mechanisms in elevating its performance. Furthermore, case studies demonstrate \model's practical applications in real-world scenarios, such as video-based knowledge extraction and educational content analysis, unlocking new possibilities for cross-video comprehension.

Moreover, the proposed LongerVideos curates a diverse collection of over 160 long-form videos spanning 134+ hours across lecture, documentary, and entertainment categories - a substantial advancement over existing datasets that are limited to inference on single~\cite{MLVU, LVBench} or relatively short-video content~\cite{Video-MME, EgoSchema}. LongerVideos enables the assessment of models' capabilities in reasoning across multiple long-context videos, a crucial requirement for real-world cross-video understanding scenarios, like video-based knowledge extraction and educational content analysis. By providing a robust testbed for evaluating long video understanding methods, this benchmark will advance the development of systems that can comprehend and reason about long-form video content at scale.