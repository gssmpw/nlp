\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}

\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[preprint]{neurips_2024}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks,
            linkcolor=red,
            anchorcolor=blue,
            citecolor=green
            ]{hyperref}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{caption}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bbm}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{tcolorbox}
\usepackage{tikz}
\usepackage{footnote}
\usepackage{wrapfig}

% Tableau colors
\definecolor{tblue}{RGB}{31,119,180}
\definecolor{torange}{RGB}{255,127,14}
\definecolor{tgreen}{RGB}{44,160,44}
\definecolor{tred}{RGB}{214,39,40}
\definecolor{tpurple}{RGB}{148,103,189}
\definecolor{lightblue}{RGB}{173, 216, 230}
\definecolor{lightpink}{RGB}{255, 182, 193}
\definecolor{lightgreen}{RGB}{144, 238, 144}

\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{array}

\newcommand{\hide}[1]{} %hide
\newcommand{\vpara}[1]{\vspace{0.05in}\noindent\textbf{#1 }}
\newcommand{\noind}[1]{\hspace{-0.05in}\noindent{#1}}
\newcommand{\etal}{\textit{et al}.}
\newcommand{\beq}[1]{\vspace{-0.03in}\begin{equation}#1\end{equation}\vspace{-0.02in}}
\newcommand{\beqn}[1]{\vspace{-0.03in}\begin{eqnarray}#1\end{eqnarray}\vspace{-0.03in}}
\newcommand{\besp}[1]{\begin{split}#1\end{split}}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.} 
\newcommand{\wrt}{\textit{w}.\textit{r}.\textit{t}} 

\def\model{VideoRAG}

\title{VideoRAG: Retrieval-Augmented Generation with Extreme Long-Context Videos}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Xubin Ren\textsuperscript{1}\thanks{Equal contribution.}\footnotemark[1] ~~~
  Lingrui Xu\textsuperscript{1}\footnotemark[1] ~~~
  Long Xia\textsuperscript{2} ~~~
  Shuaiqiang Wang\textsuperscript{2} ~~~
  Dawei Yin\textsuperscript{2} ~~~
  Chao Huang\textsuperscript{1}\thanks{Chao Huang is the Corresponding Author.} \\
  \textsuperscript{1}The University of Hong Kong ~~~
  \textsuperscript{2}Baidu Inc. \\
  \texttt{\{xubinrencs, lingruixu.db, long.phil.xia, chaohuang75\}@gmail.com} \\
  \texttt{wangshuaiqiang@baidu.com yindawei@acm.org}
}

\begin{document}

\maketitle

\begin{abstract}
Retrieval-Augmented Generation (RAG) has demonstrated remarkable success in enhancing Large Language Models (LLMs) through external knowledge integration, yet its application has primarily focused on textual content, leaving the rich domain of multi-modal video knowledge predominantly unexplored. This paper introduces \model, the first retrieval-augmented generation framework specifically designed for processing and understanding extremely long-context videos. Our core innovation lies in its dual-channel architecture that seamlessly integrates (i) graph-based textual knowledge grounding for capturing cross-video semantic relationships, and (ii) multi-modal context encoding for efficiently preserving visual features. This novel design empowers \model\ to process unlimited-length videos by constructing precise knowledge graphs that span multiple videos while maintaining semantic dependencies through specialized multi-modal retrieval paradigms. Through comprehensive empirical evaluation on our proposed LongerVideos benchmark-comprising over 160 videos totaling 134+ hours across lecture, documentary, and entertainment categories-\model\ demonstrates substantial performance compared to existing RAG alternatives and long video understanding methods. The source code of \model\ implementation and the benchmark dataset are openly available at: \textcolor{blue}{\url{https://github.com/HKUDS/VideoRAG}}.
\end{abstract}

% \clearpage

\input{intro}
\input{preliminary}
\input{solution}
\input{evaluation}
\input{relate} 
\input{conclusion}

\clearpage

\bibliographystyle{unsrtnat}
\bibliography{neurips_2024}

\input{appendix}

\end{document}