\appendix
\clearpage
\section{Details of LongerVideos}\label{apd:data}
LongerVideos is a comprehensive benchmark dataset designed to evaluate a model's ability to comprehend and extract knowledge from long-form videos. By leveraging semantic connections across multiple sources, the dataset facilitates the development of efficient, knowledge-based question-answering. The core methodology presents the model with diverse video lists of varying lengths, then assesses the model's output in terms of completeness, accuracy, and diversity. This holistic approach ensures the evaluated models demonstrate a robust understanding of the content, the ability to synthesize information, and the aptitude to generate well-rounded responses.
\vspace{-0.05in}
\begin{itemize}[leftmargin=*]
    \item \textbf{Input Data}: A diverse collection of long videos, with durations ranging from minutes to hours.
    \item \textbf{Question}: A series of open-ended questions carefully tailored to the provided video list.
    \item \textbf{Expected Output}: Individual responses generated based on the information extracted from videos.
\end{itemize}
\vspace{-0.05in}
The LongerVideos dataset was constructed by systematically curating diverse video lists from YouTube, leveraging the platform's structure where creators often compile thematic content. A major data source comprised online course videos, typically segmented into multiple recordings corresponding to distinct course chapters. For each video, the team employed the yt-dlp tool to download the content in 720P resolution, after which they prepared open-ended questions for each list with the assistance of NotebookLM, a robust multi-video understanding model from Google that can process various videos as input to generate relevant answers. The final LongerVideos dataset consists of 22 carefully curated video lists, with detailed statistics provided in Table~\ref{tab:detail stats}.
\begin{table}[h]
\vspace{-0.05in}
\centering
\caption{Detailed statistics of the \textit{LongerVideos} dataset.}
\label{tab:detail stats}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{@{}l|lcccc@{}}
\toprule
Video Type & video list name & \#video & \#query & \#overall duration \\
\midrule
\multirow{12}{*}{\textbf{Lecture}}
& \texttt{climate-week-at-columbia-engineering} & 4 & 26 & 5.91 hours \\
& \texttt{rag-lecture} & 19 & 38 & 5.34 hours \\
& \texttt{ai-agent-lecture} & 39 & 45 & 9.35 hours \\
& \texttt{daubechies-wavelet-lecture} & 4 & 25 & 8.97 hours \\
& \texttt{daubechies-art-and-mathematics-lecture} & 4 & 21 & 4.87 hours \\
& \texttt{tech-ceo-lecture} & 4 & 31 & 4.83 hours \\
& \texttt{dspy-lecture} & 9 & 38 & 4.22 hours \\
& \texttt{trading-for-beginners} & 2 & 23 & 4.11 hours \\
& \texttt{ahp-superdecision} & 11 & 24 & 2.40 hours \\
& \texttt{decision-making-science} & 4 & 26 & 2.20 hours \\
& \texttt{12-days-of-openai} & 12 & 35 & 3.43 hours \\
& \texttt{autogen} & 23 & 44 & 8.70 hours \\
\midrule
\multirow{5}{*}{\textbf{Documentary}}
& \texttt{fights-in-animal-kingdom} & 1 & 11 & 3.00 hours \\
& \texttt{nature-scenes} & 1 & 17 & 3.98 hours \\
& \texttt{education-united-nations} & 6 & 39 & 8.41 hours \\
& \texttt{elon-musk} & 1 & 13 & 8.63 hours \\
& \texttt{jeff-bezos} & 3 & 34 & 4.47 hours \\
\midrule
\multirow{5}{*}{\textbf{Entertainment}}
& \texttt{black-myth-wukong} & 10 & 23 & 21.36 hours \\
& \texttt{primetime-emmy-awards} & 3 & 17 & 7.31 hours \\
& \texttt{journey-through-china} & 1 & 27 & 3.37 hours \\
& \texttt{fia-awards} & 1 & 27 & 3.02 hours \\
& \texttt{game-awards} & 2 & 18 & 6.73 hours \\
\bottomrule
\end{tabular}
\vspace{-0.05in}
}
\end{table}

\section{Details of Case Study}\label{apd:case study}
This section provides further details on the case study presented in Section 2, which investigates the purpose and functionality of 'graders' in the context of reinforcement fine-tuning. The investigation utilizes input from the "12 Days of OpenAI" video series, comprising 12 videos that showcase OpenAI's activities in late 2024. To effectively answer the question, the model retrieves relevant content that specifically discusses the role of graders within the reinforcement fine-tuning context. To further illustrate our model's capabilities in retrieving detailed information from videos for generating nuanced answers, we also present a response from another retrieval-augmented generation model, LightRAG, for comprehensive analysis. A comparison of the generated answer by our model, as shown in Table 1, reveals that it provides greater detail, including the scoring criteria used by graders and specific examples illustrating the partial scores assigned. Moreover, it delineates the processes through which graders operate in reinforcement learning, thereby enhancing the overall quality of the generated response. Consequently, the win-rate comparison with GPT-4o-mini emphasizes the empowerment and trustworthiness of the response generated by our \model.
\vspace{-0.05in}
\begin{itemize}[leftmargin=*]

    \item \textbf{Empowerment}: Our \model\ empowers the reader by explaining how graders work in a practical sense, including details on scoring, input comparison, and their role in the feedback loop. This allows readers to grasp how to utilize graders effectively in machine learning contexts.
    
    \item \textbf{Trustworthiness}: Our \model\ aligns closely with established concepts in machine learning and provides more operational details, enhancing credibility. It clearly explains the scoring method and implications for model performance, reinforcing its trustworthy nature.
\end{itemize}
\vspace{-0.1in}

\input{figs/detailed_case}

\section{Instructions for Win-Rate \& Quantitative Performance Comparison}\label{apd:prompt4evaluation}

\input{figs/prompt}

We present the instructions employed for LLM-based evaluation in Figure~\ref{fig:prompt}, which includes both win-rate comparison and quantitative comparison. For the win-rate comparison, we input the query alongside two competing answers, designated as \texttt{answer1} and \texttt{answer2}, while alternating their positions across multiple iterations to mitigate any positional bias affecting LLM inference.

In the quantitative comparison, we leverage a standard answer from NaiveRAG~\cite{NaiveRAG} labeled \texttt{baseline\_answer}, against which the evaluated answer, referred to as \texttt{evaluation\_answer}, is assessed. The LLM assigns a score from 1 to 5, indicating whether the evaluated answer is inferior or superior to the baseline. This instruction allows us to compare the outputs of multiple models against the same standard answer, thus eliminating the need to adjust their positions. Since all methods are evaluated consistently against the same baseline, positional bias is inherently mitigated, enabling a direct comparison of scores across different methods.

