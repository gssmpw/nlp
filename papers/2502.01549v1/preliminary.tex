\section{Preliminary}\label{sec:preliminary}

Retrieval-Augmented Generation (RAG) represents a significant advancement in addressing the inherent limitations of LLMs. By intelligently incorporating external knowledge bases, RAG effectively reduces model hallucinations and enables access to domain-specific information without requiring costly model retraining. At its core, the RAG architecture consists of two fundamental components:

\begin{itemize}[leftmargin=*]

    \item \textbf{Indexing Module $\varphi(\cdot)$}: This component processes a knowledge database $\mathcal{D}$ (such as document collections in text-based RAG) to create an optimized index structure $\hat{\mathcal{D}} = \varphi(\mathcal{D})$. The data structure enables rapid and efficient knowledge retrieval during query processing. The indexing process transforms raw information into an organized, searchable format facilitating retrieval operations.

    \item \textbf{Retrieval Module $\psi(\cdot)$}: When presented with a user query $q$, this module performs query-specific knowledge retrieval from the indexed data structure, denoted as $\psi(q, \hat{\mathcal{D}})$. The process involves identifying and extracting pertinent information from the indexed knowledge database, yielding informative sources that directly support answering the user's query.
\end{itemize}

In essence, the RAG framework operates in two phases. In the preprocessing phase, the indexing module organizes all data into searchable structures. In the query phase, the retrieval module finds relevant knowledge for each input query $q$. The large language model (LLM) then processes both the query and retrieved knowledge to generate responses, expressed as $\text{LLM}(q, \psi(q, \hat{\mathcal{D}}))$.


\textbf{Retrieval-Augmented Generation with Videos.} While text-based RAG techniques are well-established, their extension to video knowledge remains largely unexplored. Our work advances the capability of Large Language Models (LLMs) to comprehend extremely long videos as a rich knowledge source. We achieve this by: i) Effectively capturing multi-modal characteristics (visual, audio, textual) and their temporal dynamics; ii) Modeling complex cross-modal alignment and inter-dependencies between different information streams.

We formulate this real-world challenge as a novel retrieval scenario with an unconstrained video knowledge base $\mathcal{D} = {\mathcal{V}_1, \ldots, \mathcal{V}_n}$, where each video $\mathcal{V}_i$ can be of arbitrary duration and the total number of videos $n$ is unrestricted. To address this challenge, our \model\ framework enables effective video knowledge discovery and semantic understanding while ensuring comprehensive responses through effectively multi-modal context modeling.