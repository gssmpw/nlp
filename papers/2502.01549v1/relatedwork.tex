\section{Related Work}
\textbf{Retrieval-Augmented Generation}. 
RAG has emerged as a pivotal paradigm in elevating performance of LLMs. By seamlessly integrating relevant information retrieved from external databases, these systems are able to ground their responses in rich, factual, and domain-specific knowledge~\cite{LightRAG, MemoRAG, RAGSurvey}. At the core of the RAG process lie three essential components: indexing, retrieval, and generation. First, raw data is meticulously processed and structured into a comprehensive database. Next, this database is intelligently queried to retrieve most pertinent information based on user inputs. Finally, this retrieved knowledge is leveraged to generate informed and insightful responses.

Recent advancements in RAG have followed two distinct methodological trajectories. Chunk-based approaches~\cite{NaiveRAG, ChunkRAG, RQ-RAG} have focused on optimizing text segmentation and retrieval through advanced vector space embeddings. In parallel, graph-based methods~\cite{GraphRAG, LightRAG, SubgraphRAG} have explored the use of structured knowledge representations to enhance the efficiency and precision of the retrieval process. Concurrent to these text-centric innovations, the research community has also made significant strides in developing multi-modal RAG systems~\cite{VisRAG, ColPali, MM-VID}, leveraging databases as rich, multi-faceted documents to enrich the knowledge retrieval and generation capabilities.

However, one crucial medium of knowledge remains relatively underexplored in the context of RAG â€“ videos. Preliminary efforts, such as MM-VID~\cite{MM-VID} and iRAG~\cite{iRAG}, have taken initial steps to bridge this gap, but substantial challenges remain in effectively organizing and extracting video-based knowledge. This is where the VideoRAG framework stands as a groundbreaking innovation. By synthesizing state-of-the-art techniques from both text-based and multi-modal RAG approaches, \model\ constructs a comprehensive knowledge graph that seamlessly integrates knowledge from multiple long-form video sources. Coupled with its multi-modal retrieval matching capabilities, \model\ empowers LLMs to tap into the wealth of information inherent in video content, elevating their ability to provide accurate, informed, and contextually relevant responses to user queries.

\textbf{Long Video Understanding}
Extracting meaningful knowledge from long-context videos poses a challenge in the domain of video understanding. Traditional approaches, such as large video language models (LVLMs), have made significant strides by converting video frames into vision tokens for comprehension by large language models~\cite{LongVideoBench, LVBench, HourVideo, Llama-VID, Video-XL, LongVA, INTP}. However, as video length and quantity increase, the computational demands also escalate, necessitating a balance between video length and available resources. This has motivated the exploration of more efficient and scalable solutions to address the growing need for effective long-video understanding.

To this end, our innovative approach presents a novel framework that leverages an efficient video language model for video segment-based general knowledge extraction and query-specific information retrieval. By constructing a graph that integrates information from multiple videos with visual features, we enhance the precision and comprehensiveness of query responses, while accommodating input videos of arbitrary lengths and quantities. This contrasts with existing agent- or retrieval-augmented generation-based methods~\cite{VideoAgent, Video-Agent, Video-RAG, DrVideo}, which rely heavily on external tools for frame-level information extraction, limiting their capacity to respond to diverse queries due to the inherent constraints of these tools. Our holistic approach, which seamlessly combines efficient video understanding with advanced knowledge organization and retrieval, represents a significant advancement in the field, poised to unlock new possibilities in long-video comprehension and query-answering.