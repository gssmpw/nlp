\section{Evaluation}
We conduct comprehensive empirical evaluations of our \model\ framework on established benchmark datasets to address the following key research questions (RQs): \textbf{RQ1}: How effectively does \model\ perform in handling long-form video content compared to existing RAG alternative approaches? \textbf{RQ2}: What advantages does \model\ demonstrate over large vision models (LVMs) in understanding extremely long-context videos? \textbf{RQ3}: How do ablation studies reveal the effectiveness of individual components (textual and visual retrieval) in \model? \textbf{RQ4}: What insights can be derived from qualitative case studies of \model\ across diverse application scenarios?

\subsection{Experimental Settings}

\begin{table}[t]
\centering
\caption{Statistics of the experimental dataset \textit{LongerVideos}.}
\label{tab:stats}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{@{}l|cccccc@{}}
\toprule
Video Type & \#video list & \#video & \#query & \#avg. queries per list & \#overall duration \\
\midrule
\textbf{Lecture}        & 12 & 135 & 376 & 31.3 & $\sim$~64.3 hours\\ 
\textbf{Documentary}    &  5 &  12 & 114 & 22.8 & $\sim$~28.5 hours\\
\textbf{Entertainment}  &  5 &  17 & 112 & 22.4 & $\sim$~41.9 hours\\
\midrule
\textbf{All}            & 22 & 164 & 602 & 27.4 & $\sim$~134.6 hours\\
\bottomrule
\end{tabular}
}
\end{table}

\textbf{Evaluation Datasets.} Current benchmarks for video-based question answering are limited by relatively short durations (average <1 hour per video)~\cite{Video-MME} or single-video understanding scenarios (e.g., MLVU~\cite{MLVU} and LVBench\cite{LVBench}). These constraints make it challenging to evaluate models' capabilities in processing and reasoning across multiple extremely long-context videos for question-answering. To address this limitation in existing evaluation frameworks, we introduce \textit{\textbf{LongerVideos}}, a comprehensive benchmark comprising over twenty video collections across three distinct categories:
\begin{itemize}[leftmargin=*]
    \item \textbf{Lecture Video}: Open-access educational content featuring contemporary technical topics, including AI Agents and RAG Techniques, delivered through comprehensive tutorials.
    \item \textbf{Documentary Video}: High-quality documentaries spanning wildlife exploration, natural landscapes, and expert interviews, each produced with professional cinematography.
    \item \textbf{Entertainment Video}: Diverse content including award ceremonies, gaming commentary with strategic analysis, and travel experiences documenting global cultural explorations.
\end{itemize}
All content is sourced from open-access YouTube videos, ensuring broad accessibility and reproducibility. Using NotebookLM\footnote{\url{https://notebooklm.google/}}, we systematically generate an average of 25+ high-quality queries per collection by processing video transcripts. Each collection averages over 4 hours in total duration, containing between 1 to 20+ individual videos, ultimately yielding a robust evaluation set of 600+ diverse queries. Detailed statistical analysis of the benchmark is presented in Table~\ref{tab:stats}.


\textbf{Evaluation Protocols and Metrics.} We implement two distinct protocols to evaluate model performance across different scenarios. The first protocol, \textbf{Win-Rate Comparison}, follows established Retrieval-Augmented Generation (RAG) evaluation methodologies~\cite{GraphRAG, LightRAG} using LLM-based judgment. This approach employs \texttt{GPT-4o-mini} to comparatively rank responses generated by two models, providing explanatory justification for each ranking decision. The second protocol, \textbf{Quantitative Comparison}, extends the LLM-based judgment by incorporating score assignment. It establishes a standard baseline answer for each query, against which other responses are evaluated on a 5-point scale, ranging from 1 (strongly worse) to 5 (strongly better).

We strategically apply these protocols for different evaluation purposes. The Win-Rate Comparison protocol is utilized to assess our methods against various RAG techniques and their ablation variants, enabling competitive analysis of our \model. Conversely, the Quantitative Comparison protocol facilitates fine-grained analysis when comparing \model\ with long video understanding methods. Following the framework established in~\cite{GraphRAG}, our evaluation encompasses multiple dimensions for comprehensive analysis, focusing on five distinct aspects detailed as follows:

(i) \textit{\textbf{Comprehensiveness}} evaluates answer coverage of question aspects. (ii) \textit{\textbf{Empowerment}} measures how effectively the answer enables reader understanding and informed judgment. (iii) \textit{\textbf{Trustworthiness}} assesses the answer's credibility through detail sufficiency and alignment with common knowledge. (iv) \textit{\textbf{Depth}} examines the presence of thorough analysis versus superficial information. (v) \textit{\textbf{Density}} evaluates the concentration of relevant information while minimizing redundancy.

We implement two key strategies to ensure reliable results. First, to mitigate position-related bias in LLM inference, we alternate the answer positions within each prompt and collect two judgments per query during win-rate comparisons. Second, to minimize statistical variance, we perform five evaluation repetitions for both win-rate and quantitative assessments, then aggregate wins or calculate mean scores to determine final results. Complete evaluation prompts are provided in Appendix~\ref{apd:prompt4evaluation}.

\textbf{Implementation Details of \model}. For vision-text grounding (Section~\ref{sec:textual indexing}), we segment videos into 30-second clips and use $k=5$ frames for initial visual captioning. We employ the quantized MiniCPM-V~\cite{MiniCPMV} as the VLM model and Distil-Whisper~\cite{Whisper, DistilWhisper} as the VSR model. For multi-modal encoding (Section~\ref{sec:visual indexing}), we utilize ImageBind~\cite{Imgebind} as $\text{MEnc}(\cdot)$ for both visual and textual encoding. Entity and textual chunk retrieval leverage OpenAI's \texttt{text-embedding-3-small} model, while section-run visual captioning uses an increased frame count of $\hat{k}=15$. Throughout the implementation, \texttt{GPT-4o-mini} serves as our core LLM for indexing, retrieval, and answer generation. Complete implementation details are available in our open-source codebase.


\subsection{Overall Comparison (RQ1 \& RQ2)}

\begin{table}[t]
\centering
\caption{We analyze the performance of \model\ against RAG baselines on the LongerVideos dataset, presenting results both by individual video categories and across the complete dataset.}
\label{tab:rag performance}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{}    & \multicolumn{2}{c}{\textbf{Lecture}} & \multicolumn{2}{c}{\textbf{Documentary}} & \multicolumn{2}{c}{\textbf{Entertainment}} & \multicolumn{2}{c}{\textbf{All}} \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
                      & NaiveRAG & \textbf{\model} & NaiveRAG & \textbf{\model} & NaiveRAG & \textbf{\model} & NaiveRAG & \textbf{\model} \\
\midrule
Comprehensiveness      & 47.63\%      & \underline{52.37}\%     & 44.04\%      & \underline{55.96}\%     & 46.43\%      & \underline{53.57}\%     & 46.73\%      & \underline{53.27}\%     \\
Empowerment            & 45.85\%      & \underline{54.15}\%     & 40.00\%      & \underline{60.00}\%     & 45.36\%      & \underline{54.64}\%     & 44.65\%      & \underline{55.35}\%     \\
Trustworthiness        & 46.73\%      & \underline{53.27}\%     & 42.54\%      & \underline{57.46}\%     & 44.46\%      & \underline{55.54}\%     & 45.51\%      & \underline{54.49}\%     \\
Depth                  & 46.70\%      & \underline{53.30}\%     & 43.25\%      & \underline{56.75}\%     & 46.07\%      & \underline{53.93}\%     & 45.93\%      & \underline{54.07}\%     \\
Density                & 46.73\%      & \underline{53.27}\%     & 44.21\%      & \underline{55.79}\%     & 44.29\%      & \underline{55.71}\%     & 45.80\%      & \underline{54.20}\%     \\
Overall Winner         & 47.66\%      & \underline{52.34}\%     & 44.04\%      & \underline{55.96}\%     & 46.43\%      & \underline{53.57}\%     & 46.74\%      & \underline{53.26}\%     \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
                      & GraphRAG-$l$ & \textbf{\model} & GraphRAG-$l$ & \textbf{\model} & GraphRAG-$l$ & \textbf{\model} & GraphRAG-$l$ & \textbf{\model} \\
\midrule
Comprehensiveness      & 44.60\%      & \underline{55.40}\%     & 48.68\%      & \underline{51.32}\%     & 49.29\%      & \underline{50.71}\%     & 46.25\%      & \underline{53.75}\%     \\
Empowerment            & 42.34\%      & \underline{57.66}\%     & 47.54\%      & \underline{52.46}\%     & 49.02\%      & \underline{50.98}\%     & 44.57\%      & \underline{55.43}\%     \\
Trustworthiness        & 42.79\%      & \underline{57.21}\%     & 47.11\%      & \underline{52.89}\%     & 46.07\%      & \underline{53.93}\%     & 44.22\%      & \underline{55.78}\%     \\
Depth                  & 42.34\%      & \underline{57.66}\%     & 48.33\%      & \underline{51.67}\%     & 49.55\%      & \underline{50.45}\%     & 44.82\%      & \underline{55.18}\%     \\
Density                & 39.26\%      & \underline{60.74}\%     & 45.26\%      & \underline{54.74}\%     & 46.52\%      & \underline{53.48}\%     & 41.74\%      & \underline{58.26}\%     \\
Overall Winner         & 44.44\%      & \underline{55.56}\%     & 48.68\%      & \underline{51.32}\%     & 49.20\%      & \underline{50.80}\%     & 46.13\%      & \underline{53.87}\%     \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
                      & GraphRAG-$g$ & \textbf{\model} & GraphRAG-$g$ & \textbf{\model} & GraphRAG-$g$ & \textbf{\model} & GraphRAG-$g$ & \textbf{\model} \\
\midrule
Comprehensiveness      & 42.66\%      & \underline{57.34}\%     & 46.23\%      & \underline{53.77}\%     & 48.48\%      & \underline{51.52}\%     & 44.42\%      & \underline{55.58}\%     \\
Empowerment            & 39.55\%      & \underline{60.45}\%     & 44.04\%      & \underline{55.96}\%     & 48.30\%      & \underline{51.70}\%     & 42.03\%      & \underline{57.97}\%     \\
Trustworthiness        & 38.54\%      & \underline{61.46}\%     & 41.49\%      & \underline{58.51}\%     & 43.48\%      & \underline{56.52}\%     & 40.02\%      & \underline{59.98}\%     \\
Depth                  & 40.61\%      & \underline{59.39}\%     & 45.26\%      & \underline{54.74}\%     & 47.23\%      & \underline{52.77}\%     & 42.72\%      & \underline{57.28}\%     \\
Density                & 37.55\%      & \underline{62.45}\%     & 46.93\%      & \underline{53.07}\%     & 48.04\%      & \underline{51.96}\%     & 41.28\%      & \underline{58.72}\%     \\
Overall Winner         & 42.23\%      & \underline{57.77}\%     & 46.32\%      & \underline{53.68}\%     & 48.75\%      & \underline{51.25}\%     & 44.22\%      & \underline{55.78}\%     \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
                      & LightRAG & \textbf{\model} & LightRAG & \textbf{\model} & LightRAG & \textbf{\model} & LightRAG & \textbf{\model} \\
\midrule
Comprehensiveness      & 42.42\%      & \underline{57.58}\%     & 45.09\%      & \underline{54.91}\%     & 43.84\%      & \underline{56.16}\%     & 43.19\%      & \underline{56.81}\%     \\
Empowerment            & 39.55\%      & \underline{60.45}\%     & 38.95\%      & \underline{61.05}\%     & 42.05\%      & \underline{57.95}\%     & 39.90\%      & \underline{60.10}\%     \\
Trustworthiness        & 39.52\%      & \underline{60.48}\%     & 42.11\%      & \underline{57.89}\%     & 40.00\%      & \underline{60.00}\%     & 40.10\%      & \underline{59.90}\%     \\
Depth                  & 40.13\%      & \underline{59.87}\%     & 41.93\%      & \underline{58.07}\%     & 41.96\%      & \underline{58.04}\%     & 40.81\%      & \underline{59.19}\%     \\
Density                & 39.57\%      & \underline{60.43}\%     & 42.37\%      & \underline{57.63}\%     & 41.61\%      & \underline{58.39}\%     & 40.48\%      & \underline{59.52}\%     \\
Overall Winner         & 42.15\%      & \underline{57.85}\%     & 44.30\%      & \underline{55.70}\%     & 43.75\%      & \underline{56.25}\%     & 42.86\%      & \underline{57.14}\%     \\
\bottomrule
\end{tabular}
}
\end{table}

We assess \model's capabilities in comprehending long-form, multi-video content by comparing its retrieval-augmented generation performance against state-of-the-art RAG baselines.
\begin{itemize}[leftmargin=*]
    \item \textbf{NaiveRAG~\cite{NaiveRAG}}: A standard RAG implementation that segments documents into uniform-sized chunks and retrieves contextually relevant content through text embedding similarity matching, serving as a widely-adopted baseline for retrieval-augmented generation systems.
    \item \textbf{GraphRAG~\cite{GraphRAG}}: An enhanced RAG system that that leverages LLMs to construct entity knowledge graphs from input documents. It improves answer generation by performing community-based graph summarization to capture global context and relationships between entities.
    \item \textbf{LightRAG~\cite{LightRAG}}: A lightweight graph-based RAG framework that implements dual-level retrieval architecture, integrating both low-level and high-level semantic knowledge discovery. The system enables efficient and contextually-aware document retrieval to process complex queries.
\end{itemize}


\textbf{Details of Baseline Implementation.} To ensure fair comparison, we implement all baseline methods with the following specifications: $\bullet$ \textbf{Input Data}: We utilize grounded textual knowledge (\eg, visual captions and transcripts) from all videos, employing identical chunk-splitting protocols as our method. $\bullet$ \textbf{Visual Processing}: For frame-level analysis, we maintain 15 frames per video clip for visual caption generation, matching the fine-grained section-run captions produced by \model's retrieval process. $\bullet$ \textbf{Baseline Variants}: GraphRAG: Implemented with both local (GraphRAG-$l$) and global (GraphRAG-$g$) search capabilities; LightRAG: Deployed with full hybrid search functionality.

\textbf{Comparison Results and Analysis (RQ1)}.
Table~\ref{tab:rag performance} presents the win rate evaluation results comparing \model\ with baseline methods. Our analysis reveals several significant findings:

\begin{itemize}[leftmargin=*]

    \item \textbf{Superior Video-based RAG Performance}. Our evaluation demonstrates that \model\ consistently outperforms all baseline methods across performance metrics. The superior performance stems from our innovative multi-modal video knowledge indexing framework, which combines graph-based knowledge grounding with multi-modal context encoding, enabling effective capture and organization of visual dynamics and semantic information across videos.\\\vspace{-0.12in}

    Furthermore, \model's multi-modal retrieval paradigm significantly enhances performance through its hybrid approach to knowledge discovery. By integrating textual semantic matching with visual content embedding-based retrieval, the system achieves precise and contextually relevant video clip retrieval. This comprehensive retrieval strategy enables more accurate and nuanced responses compared to traditional single-modality approaches, while ensuring the extracted information remains semantically coherent and contextually appropriate. \\\vspace{-0.12in}

    \item \textbf{Performance Analysis Across Baseline Methods}. In comparison with NaiveRAG, \model\ demonstrates exceptional performance across evaluation dimensions, with particular strengths in Comprehensiveness and Empowerment. This superiority stems from our effective knowledge indexing framework, which interlinks information across multiple videos, enabling sophisticated synthesis of diverse information during retrieval and yielding more comprehensive responses. \\\vspace{-0.12in}

    When benchmarked against GraphRAG and LightRAG, \model\ achieves superior performance through multi-modal context integration capabilities. Our approach excels in two aspects: (1) sophisticated knowledge indexing that effectively fuses visual-textual information, and (2) query-aware retrieval that leverages unified multi-modal representations for precise content selection. This architecture enables more nuanced understanding and contextually coherent response generation, significantly outperforming existing methods in knowledge-grounded video question answering.
    
\end{itemize}

\begin{table}[t]
\centering
\caption{We conduct quantitative comparisons between \model\ and existing long-context video understanding models on the benchmark dataset. Each model's performance is rated against NaiveRAG (our baseline) on a 5-point scale, where 1 indicates 'strongly worse than baseline' and 5 represents 'strongly better than baseline'. We evaluate across three video categories: lectures ('lec'), documentaries ('doc'), and entertainment content ('ent'), with 'all' representing the aggregate performance.}
\label{tab:vision performance}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lcccccccccccccccc@{}}
\toprule
\textbf{}    & \multicolumn{4}{c}{LLaMA-VID} & \multicolumn{4}{c}{VideoAgent} & \multicolumn{4}{c}{NotebookLM} & \multicolumn{4}{c}{\textbf{\model}} \\ 
\cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13} \cmidrule(lr){14-17}
& lec & doc & ent & \textbf{all} & lec & doc & ent & \textbf{all} & lec & doc & ent & \textbf{all} & lec & doc & ent & \textbf{all} \\
\midrule
Comprehensiveness      & 2.36 & 2.62 & 2.54 & 2.44 & 2.02 & 1.99 & 1.80 & 1.98 & 3.53 & 3.20 & 2.96 & 3.36 & 4.48 & 4.51 & 4.44 & \underline{4.48} \\
Empowerment            & 2.79 & 3.03 & 2.86 & 2.85 & 2.42 & 2.37 & 2.10 & 2.35 & 3.88 & 3.62 & 3.29 & 3.72 & 4.51 & 4.55 & 4.45 & \underline{4.51} \\
Trustworthiness        & 3.15 & 3.30 & 3.35 & 3.22 & 2.83 & 2.73 & 2.65 & 2.78 & 3.95 & 3.80 & 3.71 & 3.88 & 4.50 & 4.54 & 4.48 & \underline{4.50} \\
Depth                  & 2.01 & 2.06 & 2.00 & 2.02 & 1.79 & 1.75 & 1.62 & 1.75 & 3.14 & 2.89 & 2.55 & 2.98 & 4.34 & 4.42 & 4.31 & \underline{4.35} \\
Density                & 3.15 & 3.28 & 3.21 & 3.18 & 2.82 & 2.73 & 2.52 & 2.75 & 4.07 & 3.82 & 3.61 & 3.94 & 4.59 & 4.63 & 4.56 & \underline{4.59} \\
Overall Score          & 2.36 & 2.61 & 2.54 & 2.44 & 2.03 & 2.01 & 1.80 & 1.98 & 3.54 & 3.21 & 2.97 & 3.37 & 4.45 & 4.49 & 4.41 & \underline{4.45} \\

\bottomrule
\end{tabular}
}
\end{table}

To establish comprehensive performance benchmarks, we evaluate \model\ against state-of-the-art large vision models specifically designed for long-context video understanding, encompassing both advanced vision-language models and intelligent agent systems.
\begin{itemize}[leftmargin=*]
    
    \item \textbf{LLaMA-VID~\cite{Llama-VID}}: A vision-language framework that leverages context and content tokens for efficient long video processing, addressing token complexity in video understanding tasks.
    
    \item \textbf{VideoAgent~\cite{VideoAgent}}: A multi-modal agent that integrates diverse foundation models through a unified memory architecture. It enables powerful video understanding through fine-grained object detection, tracking, and modeling of temporal dependencies within short video clips.
    
    \item \textbf{NotebookLM}: An assistant system by Google designed for video content analysis. It enables efficient multi-video comprehension and information retrieval through advanced transcript analysis, allowing users to extract contextually coherent insights across multiple video sources.
    
\end{itemize}

\textbf{Details of Baseline Implementation}. We implement all baselines using their official codebases or available platforms for fair evaluation. For vision-language model baselines like LLaMA-VID, we standardize the implementation through three crucial modifications: (i) Replacing their original ASR model with the one used in \model\ for consistent transcript extraction; (ii) Uniformly sampling 3,600 frames per video due to GPU memory constraints (48GB per GPU); and (iii) Employing prompting instructions consistent with the compared RAG methods.

\textbf{Comparison Results and Analysis (RQ2)}. We present comprehensive performance comparisons between \model\ and existing video understanding methods in Table~\ref{tab:vision performance}. The results demonstrate that our model consistently outperforms all compared long-context video understanding methods across various metrics. We attribute these improvements to the following aspects:

\begin{itemize}[leftmargin=*]
    \item \textbf{Enhanced Long-Context Modeling}. \model\ proposes a graph-enhanced multi-modal indexing and retrieval mechanism that significantly extends video processing capabilities beyond existing vision models. Unlike conventional approaches that face length constraints, our model can effectively handle unlimited long-context videos by establishing and leveraging cross-video knowledge connections and inter-dependent relations. This architectural advantage enables comprehensive knowledge extraction and integration across extended video sequences, surpassing models like LLaMA-VID that are limited by computational constraints when processing video frames directly.
    
    \item \textbf{Superior Multi-Modal Fusion}. \model\ excels in capturing, reasoning, and aligning diverse multi-modal contexts through advanced cross-modal knowledge integration. Our approach effectively fuses information across different modalities (visual, audio, and textual), enabling superior cross-modal alignment and comprehensive understanding. This multi-modal synthesis surpasses single-modality focused approaches like VideoAgent (visual-only) and NotebookLM (transcript-only), leading to more nuanced, coherent, and expressive video understanding.
\end{itemize}

\subsection{Ablation Study (RQ3)} 
To evaluate the effectiveness of our multi-modal indexing and retrieval design, we conduct comprehensive ablation studies using two model variants: $\bullet$ \textbf{Variant 1 (-\textit{Graph}):} Removes the graph-based index-retrieval pipeline, limiting the model's ability to establish multi-video relationships. $\bullet$ \textbf{Variant 2 (-\textit{Vision}):} Eliminates the visual indexing and retrieval component from the multi-modal encoder.

The ablation study results in Figure~\ref{fig:ablation study} reveal the crucial contribution of each component to the model performance of \model, as evidenced by the following analyses:

\begin{itemize}[leftmargin=*]
\item The -\textit{Graph} variant exhibits significant performance degradation across all evaluation metrics, demonstrating that our graph-based index-retrieval mechanism is essential for two key capabilities: (1) capturing complex inter-video relationships and (2) establishing cross-video knowledge dependencies. This validates the effectiveness of our graph-enhanced architecture in connecting and synthesizing information across multiple videos.

\item The -\textit{Vision} variant shows substantially decreased win rates, underscoring the critical role of visual information processing in our framework. This performance drop validates our model's effective multi-modal context fusion mechanism, which successfully integrates and aligns visual features with other modalities for comprehensive video understanding.

\end{itemize}

These comprehensive findings underscore that the synergistic integration of graph-based architecture and visual modality processing serves as a cornerstone for achieving superior performance in multi-modal indexing and retrieval tasks, validating our architectural design choices.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/ablation_study.pdf}
    \caption{Ablation on graph-based knowledge grounding and cross-modal retrieval components.}
    \vspace{-0.2in}
    \label{fig:ablation study}
\end{figure*}

\subsection{Case Study Analysis (RQ4)}
\label{sec:case study}
\input{figs/case}

To comprehensively evaluate \model's capabilities, we conduct a detailed case study examining its response to a specific query: ``\emph{the role of graders in reinforcement fine-tuning}'', drawn from OpenAI's landmark 12-day video series released in late 2024 [as educational materials]. The query's target information is primarily located in Day 2's content, which details OpenAI's systematic and innovative approach to model enhancement through reinforcement fine-tuning techniques.

\noindent \textbf{Retrieval Accuracy and Response Quality}.
Table~\ref{tab:case study} presents \model's response alongside its retrieved video clips. Our analysis reveals that \model successfully identified and extracted relevant content from Day 2, specifically focusing on reinforcement fine-tuning discussions within the broader context of the 12-video series. The retrieved two-minute clips comprehensively cover: (1) Fundamental concepts of graders; (2) Operational mechanisms of the grading system; (3) Practical examples of partial credit assignment. Within the table, we highlight portions of \model's response that directly correspond to the retrieved video clips. This visualization demonstrates how \model leverages retrieved information to construct detailed and well-supported answers.

\noindent \textbf{Comparative Performance}. A comparative analysis with LightRAG (detailed in Appendix~\ref{apd:case study}) reveals performance distinctions in handling technical content. While both models successfully convey the core concepts of ``\emph{the grading system}'' in reinforcement learning, LightRAG's response demonstrates notable limitations in granularity and scope. Specifically, LightRAG's output lacks crucial technical elements in explaining ``\emph{grader scoring mechanisms}''. Although LightRAG's response maintains fundamental accuracy, it falls short of the comprehensive depth and technical precision exhibited by \model, which provides a nuanced explanation of the grading system's intricacies.


\noindent \textbf{Key Findings}. This case study provides compelling evidence of \model's effectiveness in three critical areas: its ability to construct precise knowledge graph structures that capture complex relationships, its successful leverage of multi-modal information for highly accurate content retrieval, and its enhanced capability to process and synthesize information from multiple long-context videos. These capabilities collectively demonstrate \model's advanced proficiency in handling sophisticated multi-modal tasks while maintaining high standards of accuracy and relevance.

