\section{The \model\ Framework}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/framework.pdf}
    \vspace{-0.2in}
    \caption{The overall framework of our proposed RAG framework \model\ for videos.}
    \vspace{-0.1in}
    \label{fig:framework}
\end{figure*}

We present our retrieval-augmented generation framework designed for understanding unlimited-length video content. Our approach addresses two fundamental challenges: (1) multi-modal knowledge indexing that effectively captures and organizes visual, audio, and semantic information from videos, and (2) knowledge-grounded information retrieval that enables precise retrieval of relevant video clips for generating accurate responses through large language models. In the following sections, we detail these components and their integration into a unified video understanding system.

\subsection{Multi-Modal Video Knowledge Indexing}



Unlike traditional text documents, videos encapsulate information through multiple modalities - primarily visual frames - creating unique challenges for knowledge extraction and organization. Standard text-based RAG methods prove insufficient for video content due to several fundamental limitations: (1) text-based systems cannot directly capture visual dynamics; (2) temporal dependencies that traditional RAG approaches fail to preserve across video frames; (3) cross-modal interactions that simple text encoding cannot capture between visual and textual information. 

To address these challenges, we introduce a comprehensive indexing framework with two components: \textbf{Graph-based Textual Knowledge Grounding} that transforms multi-modal signals into structured textual representations while preserving semantic relationships and temporal dependencies, and \textbf{Multi-Modal Context Encoding} that captures fine-grained cross-modal interactions through unified embeddings. This dual-channel architecture enables \model\ to effectively organize and index long-context videos while preserving the semantic richness of multi-modal content.

\subsubsection{\textbf{Graph-based Textual Knowledge Grounding}}
\label{sec:textual indexing}

Our framework transforms multi-modal video content into structured textual knowledge through graph-based techniques for enhanced indexing and retrieval. The conversion process operates across two key modalities: for visual content, we employ state-of-the-art Vision Language Models (VLMs) to generate comprehensive textual descriptions capturing scene dynamics and contextual information; for auditory streams, we leverage high-fidelity Automatic Speech Recognition (ASR) to extract spoken content with temporal alignment. This dual-stream processing ensures both visual semantics and audio information are preserved in our textual knowledge representation.

\begin{itemize}[leftmargin=*]
    \item \textbf{Vision-Text Grounding}: To analyze visual content effectively, we segment each video $\mathcal{V}$ into short clips ${\mathcal{S}_1, \ldots, \mathcal{S}_m}$, enabling processing of unlimited-length videos. For each clip $\mathcal{S}_j$, we transform visual information into text through a two-step process: first, we uniformly sample $k$ frames ($k \leq 10$) chronologically to capture key visual elements; then, we employ Vision-Language Models (VLMs) to generate comprehensive natural language descriptions capturing objects, actions, and scene dynamics. The caption generation process follows:
    \begin{align}
        \mathcal{C}_j = \text{VLM}(\mathcal{T}_j, \{\textbf{F}_1, \ldots, \textbf{F}_k\} \mid \textbf{F} \in \mathcal{S}_j),
    \end{align}
    where ${\textbf{F}}$ denotes the chronologically ordered set of $k$ sampled frames from the clip $\mathcal{S}_j$. We maintain $k \leq 10$ to optimize efficiency while preserving temporal coherence. The model integrates both visual frames and clip transcript $\mathcal{T}_j$ as input prompts, enabling the VLM to generate contextually rich captions $\mathcal{C}_j$ that capture both visual dynamics and associated speech content.
    
    \item \textbf{Audio-Text Grounding}: To capture crucial elements like dialogue and narration that enrich video understanding, we employ Automatic Speech Recognition (ASR) technology to transcribe each video clip, where $\mathcal{T}_j = \text{ASR}(\mathcal{S}_j)$ represents the extracted transcript from the clip $\mathcal{S}_j$.
\end{itemize}


For each video clip $\mathcal{S}$, we then create a unified and semantically rich textual representation by systematically merging the generated visual captions and ASR transcriptions $(\mathcal{C}, \mathcal{T})$. For a video $\mathcal{V}$ containing $m$ sequential clips, we formalize the complete knowledge extraction process as:
\begin{align}
    \mathcal{V}^{t} = \{(\mathcal{C}_l, \mathcal{T}_l) \mid l \in [1, m]\}.
\end{align}
At the core of our \model\ lies the challenge of organizing and retrieving multi-video knowledge efficiently. To address this, we propose a graph-based indexing framework that systematically links textual knowledge across different videos. This architecture enables incremental construction of a comprehensive knowledge graph from the extracted textual information, while maintaining semantic relationships and contextual dependencies. The entire process is executed through these essential steps, each designed to optimize multi-modal knowledge representation and retrieval:
\begin{itemize}[leftmargin=*]
    \item \textbf{Semantic Entity Recognition and Relationship Mapping}:
    Our framework leverages Large Language Models (LLMs) to construct a high-quality knowledge graph $\mathcal{G} = (\mathcal{N}, \mathcal{E})$ that comprehensively captures and connects video knowledge. To optimize LLM performance and manage their context window limitations effectively, we implement a strategic processing pipeline: \\\vspace{-0.12in}
    
    $\bullet$ (i) \textbf{Text Segmentation}. The first stage focuses on text segmentation, where we methodically divide video textual descriptions $\mathcal{V}^{t}$ into manageable chunks $\mathcal{H}_i \in \mathcal{V}^{t}$. Each chunk is carefully constructed to contain multiple video clip descriptions while adhering to a predefined length threshold, ensuring optimal processing while maintaining semantic coherence. \\\vspace{-0.12in}

    $\bullet$ (ii) \textbf{Entity-Relation Extraction}. In the entity-relation extraction phase, we process each chunk's caption-transcript pairs through LLMs to identify key entities (represented as nodes $\mathcal{N}$) and extract meaningful relationships (represented as edges $\mathcal{E}$). For instance, given the text ``\textit{GPT-4 utilizes transformer architecture for advanced natural language understanding, while incorporating visual features through ViT's patch-based image encoding}'', the system extracts entities ``\textit{GPT-4}'', ``\textit{transformer architecture}'', and ``\textit{Vision Transformer (ViT)}'', along with relationships ``\textit{GPT-4 utilizes transformer architecture}'' and ``\textit{GPT-4 incorporates ViT's encoding}'' in the domain of LLMs.

    \item \textbf{Incremental Graph Construction and Cross-Video Knowledge Integration}:
    The construction of our comprehensive knowledge graph follows an iterative and systematic approach across multiple video sources. Our framework implements a sophisticated incremental construction process that ensures coherent knowledge integration through several key mechanisms:

    $\bullet$ (i) \textbf{Entity Unification and Merging}. Our cross-video entity unification process systematically identifies and merges semantically equivalent entities across various videos into unified nodes within the knowledge graph $\mathcal{G}$. This unification approach not only maintains a consistent representation of entities but also preserves the rich contextual information derived from diverse video sources. As a result, it effectively creates a cohesive and interconnected knowledge network that enhances the overall understanding and usability of the information contained within the graph. \\\vspace{-0.12in}

    $\bullet$ (ii) \textbf{Dynamic Knowledge Graph Evolution}. As new video content is processed, our knowledge graph undergoes systematic expansion through dual-track evolution: the integration of newly discovered entities and the establishment of previously unobserved relationships. When processing textual chunks from incoming videos, the system not only identifies and incorporates novel entities (e.g., emerging AI architectures or methodologies) but also discovers new semantic connections between existing nodes. This bidirectional growth process simultaneously reinforces established knowledge patterns while accommodating emerging concepts, ensuring the graph maintains both comprehensiveness and adaptability as it scales. \\\vspace{-0.12in}

    $\bullet$ (iii) \textbf{LLM-Powered Semantic Synthesis}. To maintain semantic coherence, we strategically leverage Large Language Models (LLMs) to generate unified entity descriptions by synthesizing information from multiple video clips. This synthesis process ensures each entity maintains a comprehensive yet consistent representation, effectively consolidating knowledge across different video contexts while preserving semantic accuracy throughout the knowledge structure. \\\vspace{-0.12in}

    Formally, we define the construction of our complete knowledge graph as follows:
    \begin{align}
        \mathcal{G} = (\mathcal{N}, \mathcal{E}) = \bigcup_{\mathcal{H} \in \{ \mathcal{V}_{1}^{t}, \ldots, \mathcal{V}_{n}^{t} \}} (\mathcal{N}_{\mathcal{H}}, \mathcal{E}_{\mathcal{H}}),
    \end{align}
    Let $(\mathcal{N}_{\mathcal{H}}, \mathcal{E}_{\mathcal{H}})$ denote the extracted entities and their relations from each text chunk $\mathcal{H}$, split from the video description $\mathcal{V}^{t}$. Through processing of all videos, we construct the complete graph $\mathcal{G}$.

    \item \textbf{Text Chunk Embedding}. For each text chunk $\mathcal{H}$, we encode a text embedding $\mathbf{e}_{\mathcal{H}}^{t} = \text{TEnc}(\mathcal{H})$ using a text encoder $\text{TEnc}(\cdot)$, enabling efficient retrieval of raw chunks. We denote the complete set of chunks as $H$ and represent their collective text embeddings as $\mathbf{E}_{H}^{t} \in \mathbb{R}^{|H| \times d_{t}}$, where $|H|$ represents the total chunk count and $d_{t}$ denotes the embedding dimension. The knowledge graph $\mathcal{G}$ and chunk embeddings $\mathbf{E}_{H}^{t}$ together form the core components of our graph indexing module.
\end{itemize}

\subsubsection{Multi-Modal Context Encoding}
\label{sec:visual indexing}

In vision-to-text grounding, certain visual nuances are inherently lost, such as lighting dynamics and intricate object details that resist accurate textual representation. To preserve these visual elements, we employ a multi-modal encoder $\text{MEnc}(\cdot)$ that transforms video content into retrieval-optimized embeddings. This encoder is capable of mapping both visual content and textual queries into a shared feature space, enabling efficient semantic retrieval. Building upon powerful multi-modal encoding frameworks like CLIP~\citep{CLIP} and ImageBind~\citep{Imgebind}, we formalize our video encoding as:
\begin{align}
    \mathbf{E}_{S}^{v} \in \mathbb{R}^{|S| \times d_{v}} \quad \textit{w.r.t.} \quad \textbf{e}_{\mathcal{S}}^{v} = \text{MEnc}(\mathcal{S}).
\end{align}
In this formulation, each video clip $\mathcal{S}$ is encoded into an embedding, collectively forming $\mathbf{E}_{S}^{v}$. Here, we utilize the capital $S$ to represent the complete clip set, with $|S|$ denoting the total clip count and $d_{v}$ representing the visual embedding dimensionality. Our \model\ framework's indexing module $\varphi(\cdot)$ processes the video knowledge base $\mathcal{D} = {\mathcal{V}_1, \ldots, \mathcal{V}_n}$ to create a hybrid index combining both knowledge graph and multi-modal context embeddings:
\begin{align}
    \hat{\mathcal{D}} = \varphi(\mathcal{D}) = (\mathcal{G}, \mathbf{E}_{H}^{t}, \mathbf{E}_{S}^{v}).
\end{align}

\subsection{Multi-Modal Retrieval Paradigm}

The Multi-Modal Retrieval Paradigm aims to efficiently retrieve relevant knowledge from videos in response to queries by integrating both textual semantic and visual content matching. Leveraging a hybrid indexing framework $\hat{\mathcal{D}}$, this approach identifies informative video clips and generates query-specific descriptions using VLMs, ensuring a comprehensive retrieval process that captures both semantic understanding and visual context for more accurate responses.
\begin{itemize}[leftmargin=*]
    \item \textbf{Textual Semantic Matching}. The textual retrieval process leverages our constructed knowledge graph $\mathcal{G}$, where each entity contains a text description derived from relevant text chunks. The retrieval process consists of four sequential steps: (i) \textbf{Query Reformulation}: In the initial step, we employ LLMs to reformulate the input query into a declarative sentence, optimizing it for subsequent entity matching operations. (ii) \textbf{Entity Matching}: The system then calculates similarity scores between this reformulated query and entity descriptions within the knowledge graph, identifying the most relevant entities along with their associated text chunks. (iii) \textbf{Chunk Selection}: Following entity matching, we apply a GraphRAG~\cite{GraphRAG}-based methodology to sort and identify the most pertinent chunks ${\mathcal{H}}_{q}$ from the retrieved collection. (iv) \textbf{Video Clip Retrieval}: Finally, we extract video clips from the selected chunks, as each chunk contains descriptions of multiple video clips, resulting in our final textual retrieval set ${\mathcal{S}}_{q}^{t}$.    
    
    \item \textbf{Visual Retrieval via Content Embeddings}. Our framework complements textual matching with direct visual retrieval, enabling semantic alignment between queries and video clips. Building upon our established visual indexing framework (Section~\ref{sec:visual indexing}), each video clip is encoded through a multi-modal encoder $\text{MEnc}(\cdot)$ to generate content-based embeddings. The visual retrieval process operates in two stages: (i) \textbf{Scene Information Extraction from Query:} We leverage LLMs to distill the query $q$ into its core visual scene components, creating a focused scene description. For instance: Original question: ``\emph{In the movie, what color is the car that chases the main character through the city?}''; Scene-focused reformulation: ``\emph{An intense urban chase sequence featuring a car pursuing someone through city streets, with buildings and traffic in the background}'' (ii) \textbf{Cross-Modal Feature Alignment:} This scene-centric query reformulation is projected into the same feature space as our visual embeddings using the multi-modal encoder, leveraging its cross-modal capabilities~\cite{CLIP,Imgebind} to align the context from different modalities. We compute similarity scores between the query embedding and video clip embeddings $\mathbf{E}_{S}^{v}$ through cosine similarity, denoted as $\text{Sim}(\text{MEnc}(q), \mathbf{E}_{S}^{v})$. The top-K matching clips form the visual retrieval result ${\mathcal{S}}_{q}^{v}$.
    
    \item \textbf{LLMs-based Video Clip Filtering}: To filter out noisy clips from the retrieved results, we employ LLMs to evaluate each clip $\mathcal{S} \in {\mathcal{S}}_{q}^{t} \cap {\mathcal{S}}_{q}^{v}$ for its relevance to query $q$ using textual and visual information $\mathcal{V}^{t}_{\mathcal{S}}$ (Section~\ref{sec:textual indexing}). The filtered clips are formally defined as:
    \begin{align}
        \{\hat{\mathcal{S}} \mid (\hat{\mathcal{S}} \in \{\mathcal{S}\}_{q}^{t} \cap \{\mathcal{S}\}_{q}^{v}) \land  \text{LLMs-Judge}(\mathcal{V}^{t}_{\hat{\mathcal{S}}}) = 1\},
    \end{align}
    where the function $\text{LLMs-Judge}(\cdot)$ serves as a binary decision maker that evaluates clip relevance via carefully-designed prompting instructions, returning 1 if the clip contains information vital to answering query $q$. This approach leverages LLMs' advanced semantic reasoning capabilities to effectively filter out irrelevant clips while preserving key information.
    
\end{itemize}

\subsection{Query-Aware Content Integration and Response Generation}

With the retrieved video clips, we implement a two-stage content extraction process. First, we utilize LLMs to extract keywords $\mathcal{K}_{q}$ from query $q$, which are then integrated into VLM prompts alongside sampled frames to generate detailed visual captions $\hat{\mathcal{C}}$:
\begin{align}
    \hat{\mathcal{C}} = \text{VLM}(\mathcal{K}_q, \hat{\mathcal{T}}, \{\textbf{F}_1, \ldots, \textbf{F}_{\hat{k}}\} \mid \textbf{F} \in \hat{\mathcal{S}}),
\end{align}
where $\hat{\mathcal{T}}$ represents the audio transcription for clip $\hat{\mathcal{S}}$, with a larger $\hat{k} > k$ sampled frames. For each clip $\hat{\mathcal{S}}_j$, we create a comprehensive description $\hat{\mathcal{V}}_j^{t} = (\hat{\mathcal{C}}_j, \hat{\mathcal{T}}_j)$ by combining its visual caption and transcription. These descriptions are collected into set ${ \hat{\mathcal{V}}^{t} }$ for enhanced generation. We then enrich this visual analysis with traditional text-based retrieval, employing semantic similarity matching between query $q$ and text chunks ${ \mathcal{H} }$ to obtain relevant textual information ${ \hat{\mathcal{H}} }$. The complete output of our retrieval module $\psi(\cdot)$ thus comprises both query-specific video descriptions and relevant text chunks: $\psi(q, \hat{\mathcal{D}}) = ({ \hat{\mathcal{V}}^{t} }, { \hat{\mathcal{H}} })$. Finally, \model\ leverages a general-purpose LLM (\eg, GPT4 or DeepSeek) to generate responses based on the query $q$ and retrieved content, as detailed in Section~\ref{sec:preliminary}.