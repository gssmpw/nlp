@article{ChunkRAG,
  title={ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems},
  author={Allahverdiyev, Ritvik Aggarwal Ishneet Sukhvinder Singh Ibrahim and Taha, Muhammad and Akalin, Aslihan and Zhu, Kevin},
  journal={arXiv preprint arXiv:2410.19572},
  year={2024}
}

@article{ColPali,
  title={Colpali: Efficient document retrieval with vision language models},
  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C{\'e}line and Colombo, Pierre},
  journal={arXiv preprint arXiv:2407.01449},
  year={2024}
}

@article{DrVideo,
  title={DrVideo: Document Retrieval Based Long Video Understanding},
  author={Ma, Ziyu and Gou, Chenhui and Shi, Hengcan and Sun, Bin and Li, Shutao and Rezatofighi, Hamid and Cai, Jianfei},
  journal={arXiv preprint arXiv:2406.12846},
  year={2024}
}

@article{GraphRAG,
  title={From local to global: A graph rag approach to query-focused summarization},
  author={Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Larson, Jonathan},
  journal={arXiv preprint arXiv:2404.16130},
  year={2024}
}

@article{HourVideo,
  title={Hourvideo: 1-hour video-language understanding},
  author={Chandrasegaran, Keshigeyan and Gupta, Agrim and Hadzic, Lea M and Kota, Taran and He, Jimming and Eyzaguirre, Crist{\'o}bal and Durante, Zane and Li, Manling and Wu, Jiajun and Fei-Fei, Li},
  journal={arXiv preprint arXiv:2411.04998},
  year={2024}
}

@article{INTP,
  title={Interpolating Video-LLMs: Toward Longer-sequence LMMs in a Training-free Manner},
  author={Shang, Yuzhang and Xu, Bingxin and Kang, Weitai and Cai, Mu and Li, Yuheng and Wen, Zehao and Dong, Zhen and Keutzer, Kurt and Lee, Yong Jae and Yan, Yan},
  journal={arXiv preprint arXiv:2409.12963},
  year={2024}
}

@article{LVBench,
  title={Lvbench: An extreme long video understanding benchmark},
  author={Wang, Weihan and He, Zehai and Hong, Wenyi and Cheng, Yean and Zhang, Xiaohan and Qi, Ji and Gu, Xiaotao and Huang, Shiyu and Xu, Bin and Dong, Yuxiao and others},
  journal={arXiv preprint arXiv:2406.08035},
  year={2024}
}

@article{LightRAG,
  title={LightRAG: Simple and Fast Retrieval-Augmented Generation},
  author={Guo, Zirui and Xia, Lianghao and Yu, Yanhua and Ao, Tu and Huang, Chao},
  journal={arXiv preprint arXiv:2410.05779},
  year={2024}
}

@inproceedings{Llama-VID,
  title={Llama-vid: An image is worth 2 tokens in large language models},
  author={Li, Yanwei and Wang, Chengyao and Jia, Jiaya},
  booktitle={ECCV},
  pages={323--340},
  year={2025},
  organization={Springer}
}

@article{LongVA,
  title={Long context transfer from language to vision},
  author={Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2406.16852},
  year={2024}
}

@article{LongVideoBench,
  title={Longvideobench: A benchmark for long-context interleaved video-language understanding},
  author={Wu, Haoning and Li, Dongxu and Chen, Bei and Li, Junnan},
  journal={arXiv preprint arXiv:2407.15754},
  year={2024}
}

@article{MM-VID,
  title={Mm-vid: Advancing video understanding with gpt-4v (ision)},
  author={Lin, Kevin and Ahmed, Faisal and Li, Linjie and Lin, Chung-Ching and Azarnasab, Ehsan and Yang, Zhengyuan and Wang, Jianfeng and Liang, Lin and Liu, Zicheng and Lu, Yumao and others},
  journal={arXiv preprint arXiv:2310.19773},
  year={2023}
}

@article{MemoRAG,
  title={Memorag: Moving towards next-gen rag via memory-inspired knowledge discovery},
  author={Qian, Hongjin and Zhang, Peitian and Liu, Zheng and Mao, Kelong and Dou, Zhicheng},
  journal={arXiv preprint arXiv:2409.05591},
  year={2024}
}

@article{NaiveRAG,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@article{RAGSurvey,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@article{RQ-RAG,
  title={Rq-rag: Learning to refine queries for retrieval augmented generation},
  author={Chan, Chi-Min and Xu, Chunpu and Yuan, Ruibin and Luo, Hongyin and Xue, Wei and Guo, Yike and Fu, Jie},
  journal={arXiv preprint arXiv:2404.00610},
  year={2024}
}

@article{SubgraphRAG,
  title={Simple is effective: The roles of graphs and large language models in knowledge-graph-based retrieval-augmented generation},
  author={Li, Mufei and Miao, Siqi and Li, Pan},
  journal={arXiv preprint arXiv:2410.20724},
  year={2024}
}

@inproceedings{Video-Agent,
  title={Videoagent: Long-form video understanding with large language model as agent},
  author={Wang, Xiaohan and Zhang, Yuhui and Zohar, Orr and Yeung-Levy, Serena},
  booktitle={ECCV},
  pages={58--76},
  year={2025},
  organization={Springer}
}

@article{Video-RAG,
  title={Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension},
  author={Luo, Yongdong and Zheng, Xiawu and Yang, Xiao and Li, Guilin and Lin, Haojia and Huang, Jinfa and Ji, Jiayi and Chao, Fei and Luo, Jiebo and Ji, Rongrong},
  journal={arXiv preprint arXiv:2411.13093},
  year={2024}
}

@article{Video-XL,
  title={Video-xl: Extra-long vision language model for hour-scale video understanding},
  author={Shu, Yan and Zhang, Peitian and Liu, Zheng and Qin, Minghao and Zhou, Junjie and Huang, Tiejun and Zhao, Bo},
  journal={arXiv preprint arXiv:2409.14485},
  year={2024}
}

@inproceedings{VideoAgent,
  title={Videoagent: A memory-augmented multimodal agent for video understanding},
  author={Fan, Yue and Ma, Xiaojian and Wu, Rujie and Du, Yuntao and Li, Jiaqi and Gao, Zhi and Li, Qing},
  booktitle={ECCV},
  pages={75--92},
  year={2025},
  organization={Springer}
}

@article{VisRAG,
  title={Visrag: Vision-based retrieval-augmented generation on multi-modality documents},
  author={Yu, Shi and Tang, Chaoyue and Xu, Bokai and Cui, Junbo and Ran, Junhao and Yan, Yukun and Liu, Zhenghao and Wang, Shuo and Han, Xu and Liu, Zhiyuan and others},
  journal={arXiv preprint arXiv:2410.10594},
  year={2024}
}

@inproceedings{iRAG,
  title={iRAG: Advancing RAG for Videos with an Incremental Approach},
  author={Arefeen, Md Adnan and Debnath, Biplob and Uddin, Md Yusuf Sarwar and Chakradhar, Srimat},
  booktitle={CIKM},
  pages={4341--4348},
  year={2024}
}

