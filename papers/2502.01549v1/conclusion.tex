\section{Conclusion}

This paper introduces \model, a novel retrieval-augmented generation framework designed for understanding extremely long-context videos. Through a dual-channel architecture that seamlessly integrates graph-based textual knowledge grounding with multi-modal context encoding, \model effectively processes, indexes, and retrieves information from unlimited-length videos for large language model enhancement. Our comprehensive empirical evaluation on the established LongerVideos benchmark demonstrates \model's superior performance compared to existing RAG alternatives and long video understanding methods across multiple dimensions. The framework's demonstrated capabilities—constructing precise video knowledge structures, leveraging multi-modal information for accurate content retrieval, and processing information from multiple long-context videos—showcase its significant potential for advancing video-based knowledge retrieval and generation tasks.