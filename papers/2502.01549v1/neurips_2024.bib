@inproceedings{CLIP,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{Imgebind,
  title={Imagebind: One embedding space to bind them all},
  author={Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  booktitle={CVPR},
  pages={15180--15190},
  year={2023}
}

@article{gutierrez2024hipporag,
  title={HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models},
  author={Guti{\'e}rrez, Bernal Jim{\'e}nez and Shu, Yiheng and Gu, Yu and Yasunaga, Michihiro and Su, Yu},
  journal={NeurIPS},
  year={2024}
}

@article{GraphRAG,
  title={From local to global: A graph rag approach to query-focused summarization},
  author={Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Larson, Jonathan},
  journal={arXiv preprint arXiv:2404.16130},
  year={2024}
}

@article{LVBench,
  title={Lvbench: An extreme long video understanding benchmark},
  author={Wang, Weihan and He, Zehai and Hong, Wenyi and Cheng, Yean and Zhang, Xiaohan and Qi, Ji and Gu, Xiaotao and Huang, Shiyu and Xu, Bin and Dong, Yuxiao and others},
  journal={arXiv preprint arXiv:2406.08035},
  year={2024}
}

@article{LightRAG,
  title={LightRAG: Simple and Fast Retrieval-Augmented Generation},
  author={Guo, Zirui and Xia, Lianghao and Yu, Yanhua and Ao, Tu and Huang, Chao},
  journal={arXiv preprint arXiv:2410.05779},
  year={2024}
}

@article{MiniCPMV,
  title={Minicpm-v: A gpt-4v level mllm on your phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}

@inproceedings{Whisper,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={ICML},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}

@article{DistilWhisper,
  title={Distil-whisper: Robust knowledge distillation via large-scale pseudo labelling},
  author={Gandhi, Sanchit and von Platen, Patrick and Rush, Alexander M},
  journal={arXiv preprint arXiv:2311.00430},
  year={2023}
}

@article{NaiveRAG,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@article{Video-XL,
  title={Video-xl: Extra-long vision language model for hour-scale video understanding},
  author={Shu, Yan and Zhang, Peitian and Liu, Zheng and Qin, Minghao and Zhou, Junjie and Huang, Tiejun and Zhao, Bo},
  journal={arXiv preprint arXiv:2409.14485},
  year={2024}
}

@inproceedings{Llama-VID,
  title={Llama-vid: An image is worth 2 tokens in large language models},
  author={Li, Yanwei and Wang, Chengyao and Jia, Jiaya},
  booktitle={ECCV},
  pages={323--340},
  year={2025},
  organization={Springer}
}

@inproceedings{VideoAgent,
  title={Videoagent: A memory-augmented multimodal agent for video understanding},
  author={Fan, Yue and Ma, Xiaojian and Wu, Rujie and Du, Yuntao and Li, Jiaqi and Gao, Zhi and Li, Qing},
  booktitle={ECCV},
  pages={75--92},
  year={2025},
  organization={Springer}
}

@inproceedings{Video-Agent,
  title={Videoagent: Long-form video understanding with large language model as agent},
  author={Wang, Xiaohan and Zhang, Yuhui and Zohar, Orr and Yeung-Levy, Serena},
  booktitle={ECCV},
  pages={58--76},
  year={2025},
  organization={Springer}
}

@article{MemoRAG,
  title={Memorag: Moving towards next-gen rag via memory-inspired knowledge discovery},
  author={Qian, Hongjin and Zhang, Peitian and Liu, Zheng and Mao, Kelong and Dou, Zhicheng},
  journal={arXiv preprint arXiv:2409.05591},
  year={2024}
}

@article{RAGSurvey,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@article{ChunkRAG,
  title={ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems},
  author={Allahverdiyev, Ritvik Aggarwal Ishneet Sukhvinder Singh Ibrahim and Taha, Muhammad and Akalin, Aslihan and Zhu, Kevin},
  journal={arXiv preprint arXiv:2410.19572},
  year={2024}
}

@article{RQ-RAG,
  title={Rq-rag: Learning to refine queries for retrieval augmented generation},
  author={Chan, Chi-Min and Xu, Chunpu and Yuan, Ruibin and Luo, Hongyin and Xue, Wei and Guo, Yike and Fu, Jie},
  journal={arXiv preprint arXiv:2404.00610},
  year={2024}
}

@article{SubgraphRAG,
  title={Simple is effective: The roles of graphs and large language models in knowledge-graph-based retrieval-augmented generation},
  author={Li, Mufei and Miao, Siqi and Li, Pan},
  journal={arXiv preprint arXiv:2410.20724},
  year={2024}
}

@article{RA-CM3,
  title={Retrieval-augmented multimodal language modeling},
  author={Yasunaga, Michihiro and Aghajanyan, Armen and Shi, Weijia and James, Rich and Leskovec, Jure and Liang, Percy and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2211.12561},
  year={2022}
}

@article{MMRAGSurvey,
  title={Retrieving multimodal information for augmented generation: A survey},
  author={Zhao, Ruochen and Chen, Hailin and Wang, Weishi and Jiao, Fangkai and Do, Xuan Long and Qin, Chengwei and Ding, Bosheng and Guo, Xiaobao and Li, Minzhi and Li, Xingxuan and others},
  journal={arXiv preprint arXiv:2303.10868},
  year={2023}
}

@article{VisRAG,
  title={Visrag: Vision-based retrieval-augmented generation on multi-modality documents},
  author={Yu, Shi and Tang, Chaoyue and Xu, Bokai and Cui, Junbo and Ran, Junhao and Yan, Yukun and Liu, Zhenghao and Wang, Shuo and Han, Xu and Liu, Zhiyuan and others},
  journal={arXiv preprint arXiv:2410.10594},
  year={2024}
}

@article{ColPali,
  title={Colpali: Efficient document retrieval with vision language models},
  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C{\'e}line and Colombo, Pierre},
  journal={arXiv preprint arXiv:2407.01449},
  year={2024}
}

@inproceedings{iRAG,
  title={iRAG: Advancing RAG for Videos with an Incremental Approach},
  author={Arefeen, Md Adnan and Debnath, Biplob and Uddin, Md Yusuf Sarwar and Chakradhar, Srimat},
  booktitle={CIKM},
  pages={4341--4348},
  year={2024}
}

@article{MM-VID,
  title={Mm-vid: Advancing video understanding with gpt-4v (ision)},
  author={Lin, Kevin and Ahmed, Faisal and Li, Linjie and Lin, Chung-Ching and Azarnasab, Ehsan and Yang, Zhengyuan and Wang, Jianfeng and Liang, Lin and Liu, Zicheng and Lu, Yumao and others},
  journal={arXiv preprint arXiv:2310.19773},
  year={2023}
}

@article{LongVideoBench,
  title={Longvideobench: A benchmark for long-context interleaved video-language understanding},
  author={Wu, Haoning and Li, Dongxu and Chen, Bei and Li, Junnan},
  journal={arXiv preprint arXiv:2407.15754},
  year={2024}
}

@article{Video-RAG,
  title={Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension},
  author={Luo, Yongdong and Zheng, Xiawu and Yang, Xiao and Li, Guilin and Lin, Haojia and Huang, Jinfa and Ji, Jiayi and Chao, Fei and Luo, Jiebo and Ji, Rongrong},
  journal={arXiv preprint arXiv:2411.13093},
  year={2024}
}

@article{DrVideo,
  title={DrVideo: Document Retrieval Based Long Video Understanding},
  author={Ma, Ziyu and Gou, Chenhui and Shi, Hengcan and Sun, Bin and Li, Shutao and Rezatofighi, Hamid and Cai, Jianfei},
  journal={arXiv preprint arXiv:2406.12846},
  year={2024}
}

@article{LongVA,
  title={Long context transfer from language to vision},
  author={Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2406.16852},
  year={2024}
}

@article{INTP,
  title={Interpolating Video-LLMs: Toward Longer-sequence LMMs in a Training-free Manner},
  author={Shang, Yuzhang and Xu, Bingxin and Kang, Weitai and Cai, Mu and Li, Yuheng and Wen, Zehao and Dong, Zhen and Keutzer, Kurt and Lee, Yong Jae and Yan, Yan},
  journal={arXiv preprint arXiv:2409.12963},
  year={2024}
}

@article{HourVideo,
  title={Hourvideo: 1-hour video-language understanding},
  author={Chandrasegaran, Keshigeyan and Gupta, Agrim and Hadzic, Lea M and Kota, Taran and He, Jimming and Eyzaguirre, Crist{\'o}bal and Durante, Zane and Li, Manling and Wu, Jiajun and Fei-Fei, Li},
  journal={arXiv preprint arXiv:2411.04998},
  year={2024}
}

@article{VideoLLaMA3,
  title={VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding},
  author={Boqiang Zhang and Kehan Li and Zesen Cheng and Zhiqiang Hu and Yuqian Yuan and Guanzheng Chen and Sicong Leng and Yuming Jiang and Hang Zhang and Xin Li and Peng Jin and Wenqi Zhang and Fan Wang and Lidong Bing and Deli Zhao},
  journal={arXiv preprint arXiv:2501.13106},
  year={2025}
}

@article{LLaVA-Video,
  title={Video Instruction Tuning With Synthetic Data}, 
  author={Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li},
  journal={arXiv preprint arXiv:2410.02713}, 
  year={2024}
}

@article{Video-MME,
  title={Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yongdong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}

@article{MLVU,
  title={MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding},
  author={Zhou, Junjie and Shu, Yan and Zhao, Bo and Wu, Boya and Xiao, Shitao and Yang, Xi and Xiong, Yongping and Zhang, Bo and Huang, Tiejun and Liu, Zheng},
  journal={arXiv preprint arXiv:2406.04264},
  year={2024}
}

@article{EgoSchema,
  title={Egoschema: A diagnostic benchmark for very long-form video language understanding},
  author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
  journal={NeurIPS},
  volume={36},
  pages={46212--46244},
  year={2023}
}