% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{times}
\usepackage{latexsym}
\usepackage{inconsolata}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{inconsolata}
\usepackage{microtype}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{hhline}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{array}
\usepackage{booktabs}
\usepackage{verbdef}
\usepackage{colortbl}
\usepackage{color}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{float}
\usepackage{caption}
\usepackage{arydshln}
\usepackage{inconsolata}
\usepackage{rotating}
\usepackage{xcolor}
\usepackage{placeins}
\usepackage{makecell}
\usepackage{afterpage}
\usepackage{blindtext}
\usepackage{float}
\usepackage{threeparttablex}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage{hhline}
\usepackage{hyperref}
\geometry{a4paper, margin=1in}
\definecolor{mygray}{gray}{0.9}
\definecolor{mypink}{rgb}{0.99,0.91,0.95}
\definecolor{Coral}{RGB}{245, 218, 210}
\definecolor{mycyan}{cmyk}{0.3,0,0,0}
\definecolor{Celadon}{RGB}{172, 225, 175}
\definecolor{Peach}{RGB}{255, 229, 180}
\definecolor{orange}{HTML}{FFF8E3}
\definecolor{green}{HTML}{DAE3D1}
\definecolor{blue}{HTML}{EAF8FF}
\newcommand{\rscomment}[1]{{\color{darkblue}[#1 -- By Rohit (to be reviewed)]}}
\newcommand{\PlanSum}{\texttt{\mbox{Plan-mPlug-Owl3}}\xspace}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations}

% Datset: VISTA
% Method: PlanSum

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Dongqi Liu\textsuperscript{$\Omega$}},
 \textbf{Chenxi Whitehouse\textsuperscript{$\Delta$}},
 \textbf{Xi Yu\textsuperscript{$\Omega$}},
 \textbf{Louis Mahon\textsuperscript{$\Theta$}},
 \textbf{Rohit Saxena\textsuperscript{$\Theta$}},
 \\
 \textbf{Zheng Zhao\textsuperscript{$\Theta$}},
 \textbf{Yifu Qiu\textsuperscript{$\Theta$}},
 \textbf{Mirella Lapata\textsuperscript{$\Theta$}},
 \textbf{Vera Demberg\textsuperscript{$\Omega$}\textsuperscript{$\Psi$}}
 \\
 \textsuperscript{$\Omega$}Saarland University,
 \textsuperscript{$\Psi$}MPI for Informatics,
 \textsuperscript{$\Delta$}University of Cambridge, 
 \textsuperscript{$\Theta$}University of Edinburgh \\
 \small{
   \shortstack[c]{
     \textsuperscript{$\Omega$}\texttt{\{dongqi,xiyu,vera\}@lst.uni-saarland.de} \\
     \textsuperscript{$\Delta$}\texttt{chenxi.whitehouse@cl.cam.ac.uk} \\
     \textsuperscript{$\Theta$}\texttt{\{lmahon,rohit.saxena,zheng.zhao,yifu.qiu\}@ed.ac.uk}, \texttt{mlap@inf.ed.ac.uk}
   }
 }
}


\begin{document}
\maketitle
\begin{abstract}
Transforming recorded videos into concise and accurate textual summaries is a growing challenge in multimodal learning. This paper introduces VISTA, a dataset specifically designed for video-to-text summarization in scientific domains. VISTA contains 18,599 recorded AI conference presentations paired with their corresponding paper abstracts. We benchmark the performance of state-of-the-art large models and apply a plan-based framework to better capture the structured nature of abstracts. Both human and automated evaluations confirm that explicit planning enhances summary quality and factual consistency. However, a considerable gap remains between models and human performance, highlighting the challenges of scientific video summarization.\footnote{Code and dataset are available \href{https://github.com/dongqi-me/VISTA}{here}.}
\end{abstract}

\section{Introduction}

Large multimodal models (LMMs), which integrate components from different modalities through cross-modal alignment training \cite{koh2023grounding, cheng-etal-2023-edit, li2024seed, ahn-etal-2024-tuning, fu2025blink, wu2025comprehensive}, have achieved considerable progress in video-to-text summarization tasks for general-purpose content such as YouTube, movies, and news videos \cite{li-etal-2020-vmsmo, lin2023videoxum, krubinski-pecina-2023-mlask, hua2024v2xum, chen2024personalized, zhang2024multi, qiu2024mmsum, patil-etal-2024-refinesumm, mahon-lapata-2024-modular, mahon2024screenwriter}. However, many recent studies have highlighted that these LMMs exhibit reduced performance in scientific contexts, particularly when processing technical terminology and scientific visual elements like figures and tables \cite{li-etal-2024-multimodal-arxiv, lu2024mathvista, yue2024mmmu, hu2024mplug, bai2024hallucination, liang2024foundations, patil-etal-2024-refinesumm, huang2024pixels}. This performance gap might be largely attributed to the absence of specialized training datasets for multimodal scientific content \cite{chen-etal-2024-m3av, hu2024mplugpaperowl, pramanick2024spiqa, zhang-etal-2024-comprehensive-survey}.

\begin{figure}[t]
  \centering \includegraphics[width=0.47\textwidth]{figs/dataset_illustration.pdf}
  \caption{An example from VISTA: a video paired with its abstract. The paper \cite{mallen-etal-2023-trust} was presented at ACL 2023 and received the Best Video Recordings award.}
  \label{fig:dataset_illustration}
\end{figure}

Thus, we introduce \textbf{VISTA} (\textbf{\underline{Vi}}deo to \textbf{\underline{S}}cien\textbf{\underline{t}}ific \textbf{\underline{A}}bstract), an English dataset for video-to-text summarization in scientific domains. VISTA consists of 18,599 aligned pairs of conference presentation recordings and their corresponding paper abstracts, collected from leading conferences in computational linguistics (\href{https://aclanthology.org/}{ACL Anthology} including ACL, EMNLP, NAACL, EACL, Findings of *ACL) and machine learning (\href{https://icml.cc/}{ICML} and \href{https://neurips.cc/}{NeurIPS}). \autoref{fig:dataset_illustration} illustrates an example selected from VISTA: a conference presentation video (top) paired with the abstract of the corresponding paper (bottom).

We benchmark VISTA using several state-of-the-art (SOTA) large models, including closed-source LMMs (\texttt{Claude 3.5 Sonnet}, \texttt{Gemini 2.0}, \texttt{GPT-o1}), as well as video-specific open-source LMMs (\texttt{Video-LLaMA}, \texttt{Video-ChatGPT}, \texttt{mPLUG-Owl3}, etc.; \citealp[]{zhang-etal-2023-video, maaz-etal-2024-video, lin-etal-2024-video, ye2024mplug, li2024llava, li2025llama}). For comparison, we also include strong baselines: the text-to-text \texttt{LLaMA-3.1} \cite{touvron2023llama} and the audio-to-text \texttt{Qwen2-Audio} \cite{chu2024qwen2}. Experiments across zero-shot, QLoRA, and full fine-tuning settings reveal that in-domain fine-tuning improves summarization performance across different large models, and video-based models generally outperform text- and audio-based models on our dataset. However, simpler end-to-end approaches may often struggle to capture the underlying structure of scientific abstracts. 

To address this, we explore a plan-based approach, which has been shown to improve coherence and factual grounding through a predefined planning component \cite{liu-chen-2021-controllable, narayan-etal-2021-planning, narayan-etal-2023-conditional}. Unlike direct end-to-end generation, plan-based method could leverage the fact that scientific abstracts often follow a well-defined format \cite{takeshita-etal-2024-aclsum}. By explicitly modeling the latent structure of the abstract through a sequence of intermediate plan questions, the summary generation process is better guided. Empirical results confirm that the plan-based method outperforms existing SOTA models in terms of summary quality and factual accuracy. Nevertheless, despite these improvements, all candidate models still struggle with hallucinations and factual errors.

\textbf{In summary, our contributions are as follows:}
\begin{itemize}[leftmargin=8pt,itemsep=1pt,topsep=1pt,parsep=1pt]
\item We present VISTA, a novel large-scale multimodal dataset with 18,599 video-abstract pairs, tailored for summarizing scientific presentations from video recordings.
\item We establish benchmark performance on VISTA through a comprehensive evaluation of leading large (language/audio/multimodal) models.
\item We apply a plan-based framework that improves upon SOTA video LMMs on summary quality and factual accuracy. 
\item We conduct error analysis, case studies, and human evaluations to identify the pivotal issues in the model-generated summaries.
\end{itemize}

\section{Related Work}
\paragraph{Video-to-Text Summarization} generates coherent summaries by integrating multimodal information \cite{hua2024v2xum}, supported by datasets like MSS \cite{li-etal-2017-multi}, VideoXum \cite{10334011}, MMSum \cite{qiu2024mmsum}, Hierarchical3D \cite{papalampidi-lapata-2023-hierarchical3d}, and \mbox{LfVS-T} \cite{argaw2024scaling}, spanning tasks from instructional videos to general web content \cite{li-etal-2017-multi, zhou2018towards, li2019video, li-etal-2020-vmsmo, liu-wan-2021-video, fu-etal-2021-mm, krubinski-pecina-2023-mlask, han2023shot2story20k, he2023align, hua2024v2xum, islam2024video, qiu2024mmsum}. Technical advancements include hierarchical attention models \cite{sanabria2018how2}, extractive methods using multimodal features \cite{cho-etal-2021-streamhover, krubinski-pecina-2023-mlask}, and hybrid extractive-abstractive frameworks \cite{9939279, papalampidi-lapata-2023-hierarchical3d}. Transformer-based systems have further improved performance \cite{krubinski-pecina-2023-mlask,li-etal-2020-vmsmo,10.1145/3474085.3475321,mahon-lapata-2024-modular}. However, challenges in summarizing academic videos remain under-explored.

\paragraph{Scientific Text Summarization} condenses complex scholarly content into concise formats \cite{cachola-etal-2020-tldr, ju-etal-2021-leveraging-information, sotudeh-goharian-2022-tstr, pu-demberg-2023-chatgpt}, supported by datasets like TalkSumm \cite{lev-etal-2019-talksumm} for academic video transcripts, SumSurvey \cite{liu-etal-2024-sumsurvey} for survey papers, ACLSum \cite{takeshita-etal-2024-aclsum} for ACL discourse, and SciNews \cite{pu-etal-2024-scinews} for simplifying research for broader audiences. M$^3$AV \cite{chen-etal-2024-m3av} supports tasks like ASR, TTS, and slide-script generation. Methods like HAESum \cite{zhao-etal-2024-hierarchical} and SAPGraph \cite{qi-etal-2022-sapgraph} improve discourse and structural summarization, while CiteSum \cite{mao-etal-2022-citesum} and SSR \cite{fatima-strube-2023-cross} focus on scalability and audience-specific customization. Despite these efforts, scientific summarization remains a challenging domain due to the inherent complexity and diversity of scholarly texts. 

\paragraph{Plan-based Summarization} employs structured representations to improve summary quality and reduce hallucinations \cite{narayan-etal-2021-planning, amplayo2021unsupervised, wang-etal-2022-guiding, narayan-etal-2023-conditional}. Research focuses on text-only planning with elements like entities \cite{narayan-etal-2021-planning, liu-chen-2021-controllable, huot-etal-2024-mplan}, keyword prompts \cite{creo2023prompting}, and question-answer pairs \cite{narayan-etal-2023-conditional}. Examples include PlanVerb \cite{canal2022planverb}, which converts task plans into natural language via semantic tagging, and domain-specific approaches in dialogue summarization that align with knowledge structures for improved quality \cite{srivastava-etal-2024-knowledge}. Blueprint-based frameworks utilize intermediate plans such as question-answer pairs to create coherent narratives for visual storytelling \cite{liu-etal-2023-visual-storytelling}. However, plan-based strategies for multimodal tasks, particularly video-to-text summarization, have received limited attention. 

\section{VISTA Dataset}
\paragraph{Data Acquisition and Cleaning}
VISTA is derived from computational linguistics and machine learning conferences, including \href{https://aclanthology.org/}{ACL Anthology} (ACL, EMNLP, NAACL, EACL, Findings of *ACL), \href{https://icml.cc/}{ICML}, and \href{https://neurips.cc/}{NeurIPS}, covering content from 2020 to 2024. All materials (paper abstracts and video recordings) are contributed by the respective paper authors, ensuring narrative consistency. Since these metadata are stored in XML/JSON files on their respective websites, no further preprocessing (e.g. extracting abstracts from PDFs) is required. We collect paper titles, author lists, paper abstracts, links to papers, and presentation videos, in accordance with platform terms for academic research purposes (or obtain written confirmation).\footnote{We discuss copyright in \autoref{copyrights}.} To maintain one-to-one video-to-text alignments, we exclude samples that may cover multiple papers (e.g., tutorials, invited talks) and videos shorter than one minute or longer than 30 minutes.

\paragraph{Quality Control}
The data are sourced directly from official proceedings websites, including textual summaries and presentation videos authored/recorded by corresponding researchers, eliminating the need for additional annotations. We verify the data quality through both human and automated checks. We discuss quality control guidelines and results in Appendix \autoref{fig:quality_control} and \autoref{quality_control}, respectively.

\begin{figure}[t]
  \centering \includegraphics[width=0.45\textwidth]{figs/venue_distribution.pdf}
  \caption{Venue distribution of the VISTA dataset.}
  \label{fig:venue_distribution}
\end{figure}

\paragraph{Data Splits}
After quality control, our dataset comprises 18,599 samples, with venue distributions shown in \autoref{fig:venue_distribution}. To ensure balanced domain coverage in each subset, we proportionally sample to split the dataset into training (80\%), validation (10\%), and test (10\%) sets. All subsequent experiments are conducted using these splits.

\input{tables/datasets_comparison}

\begin{figure*}[htb!]
  \centering \includegraphics[width=1\textwidth]{figs/dataset_attribute_distributions.pdf}
  \caption{Distribution of summary sentences, summary tokens, video durations, and video shots in VISTA.}
  \label{fig:dataset_attribute_distributions}
\end{figure*}

\input{tables/vista_stats}

\paragraph{Dataset Comparison and Statistics}

\autoref{tab:datasets_comparison} compares VISTA with several existing video-to-text summarization datasets. While many focus on open-domain (e.g., MMSum, Instruct-V2Xum) or focus on specific areas like news (MLASK, MM-AVS) and activities (VideoXum), VISTA is tailored for summarizing scientific presentations, addressing a distinct niche in video-to-text summarization. On average, it features longer inputs (6.8 minutes) than VideoXum (2.1 minutes) and MSS (3.4 minutes), as well as longer summaries (192.6 tokens), compared to YouCook2 (67.8 tokens) and VideoXum (49.9 tokens).

\autoref{tab:statistics} summarizes the dataset statistics: videos average 6.76~minutes and 16.36~shots (we use \href{https://www.scenedetect.com/}{PySceneDetect} with \texttt{ContentDetector} to calculate video shots), while summaries contain 192.62~tokens on average across 7.19~sentences. The average dependency tree depth (Avg. Depth of Dep Tree) is 6.02, indicating the syntactic complexity of the summaries. Meanwhile, the Type-Token Ratio (TTR) is 0.62, reflecting lexical diversity. Both metrics are calculated using \href{https://spacy.io/}{spaCy}. Diversity metrics \cite{li-etal-2016-diversity}, which measure the variety of unique n-grams, yield Distinct-1, Distinct-2, and Distinct-3 scores of 0.62, 0.93, and 0.97, respectively. \autoref{fig:dataset_attribute_distributions} visualizes key attributes: most summaries remain under 250 tokens and 10 sentences, and most videos last fewer than 10 minutes with under 30 shots. In \autoref{data_sample}, we present a random sample from the VISTA dataset.

\begin{figure*}[t]
  \centering \includegraphics[width=1\textwidth]{figs/plan_extraction.pdf}
  \caption{\texttt{GPT-o1} generates plans based on reference summaries. Each question \( q_i \) corresponds to summary sentence  \( t_i \) which we assume constitutes its answer. Index \( i \) ranges from \( 1 \) to the number of summary sentences.}
  \label{fig: plan_extraction}
\vspace{-15pt}
\end{figure*}

\section{Benchmarking VISTA}

\paragraph{Task Overview}

We formalize the task of summarizing recorded scientific videos as follows: Let~$v$ and~$s$ denote a video (or its transcript/audio) and its paired summary from dataset \mbox{\(D = \{(v_1, s_1), (v_2, s_2), \ldots, (v_n, s_n)\}\)}, where \( n \) signifies the number of video-abstract pairs. The objective is to train a (multimodal) model \(\mathcal{M}\) to learn the conditional probability distribution \(P(s~|~v)\). Given a new video, the trained model \(\mathcal{M}\) is expected to generate an appropriate summary.

A challenge in video-to-text summarization is structuring the generated summaries in a coherent and faithful manner. Directly learning the mapping from \( v \) to \( s \) could lead to inadequate outputs, as the model lacks explicit guidance on how to organize and present the extracted information. Scientific abstracts often follow a relatively well-defined structure, making them suitable for a more structured generation approach \cite{takeshita-etal-2024-aclsum}. We follow previous work \cite{narayan-etal-2021-planning, liu-etal-2023-visual-storytelling, narayan-etal-2023-conditional} in adopting a plan-based framework that introduces an intermediate representation to capture latent structure more effectively than simpler end-to-end approaches. Specifically, given input video~$v$, we first generate plan \( p \), which consists of a sequence of automatically generated questions \( \{q_1, q_2, \ldots, q_m\} \), each corresponding to a sentence to be verbalized in the summary. The plan explicitly controls the structure of the summary as a whole and the content of each of its sentences (which are meant to answer the questions in the plan). The model is then trained to learn the extended conditional probability distribution \( P(s~|~v,p) \), ensuring that the generated summaries follow the structure and flow of plan~$p$.

\paragraph{Plan Generation}
\label{sec: plan_generation}
We hypothesize that summary sentences can be viewed as responses to plan questions directly associated with them. This idea is inspired by the theory of Questions Under Discussion (QUD) \cite{roberts2012information, wu-etal-2023-elaborative, suvarna-etal-2024-qudselect}, which posits that discourse often revolves around a set of questions that guide the structure and interpretation of the conversation.

We leverage \texttt{GPT-o1} \cite{achiam2023gpt} to generate silver-standard plans based on reference summary sentences and their preceding context. As shown in \autoref{fig: plan_extraction}, for example, question \(q_3\) is generated based on target sentence~\(t_3\) and the summary sentences preceding it (i.e.,~\(t_1\) and \(t_2\)), and so on. As a result, the question sequence preserves the order of sentences in the reference summaries, ensuring that the plan maintains a natural and coherent flow consistent with the structure of reference summaries. The prompt used to generate plan questions is provided in Appendix \autoref{plan_question_generation}.

\paragraph{Summarization Model}
We train two independent models corresponding to Plan Generation (\texttt{PG}) and  Summary Generation (\texttt{SG}). The \texttt{PG} module is trained on pairs of  \( (v, p) \)~samples, where \( v \)~represents the input and \( p \)~is the silver-standard plan. The \texttt{SG} module is trained on tuples \( ([v; p], s) \), where \( [v; p] \) is the concatenation of the input~\( v \) and its plan~\( p \). During inference, the trained \texttt{PG} module predicts plan \( \hat{p} \) for input~\( v \), and the tuple \( [v; \hat{p}] \) is fed into the \texttt{SG} module to generate the final summary. Both modules have the same backbone but are trained independently.

\input{tables/main_results}

\section{Experiments}

\paragraph{Baseline Models} We benchmark our dataset using three settings: zero-shot learning, QLoRA fine-tuning \cite{dettmers2024qlora}, and full-parameter fine-tuning. For zero-shot, we test closed-source multimodal models, including \texttt{GPT-o1} \cite{achiam2023gpt}, \texttt{Gemini 2.0} \cite{team2023gemini}, \texttt{ \texttt{Claude 3.5 Sonnet} } \cite{anthropic_claude3_5_sonnet}, as well as open-source video LMMs such as \texttt{Video-LLaMA} \cite{zhang-etal-2023-video}, \texttt{Video-ChatGPT} \cite{maaz-etal-2024-video}, \texttt{Video-LLaVA} \cite{lin-etal-2024-video}, \texttt{LLaMA-VID} \cite{li2025llama}, \texttt{LLaVA-NeXT-Interleave} \cite{li2024llava}, and \texttt{mPLUG-Owl3} \cite{ye2024mplug}. These open-source video LMMs process videos by extracting multimodal features, such as visual and/or audio components, using cross-modal attention mechanisms to align and integrate information across modalities. 

We also assess \texttt{LLaMA-3.1} and \texttt{Qwen2-Audio} to examine if text- or audio-based models can accomplish the summarization task without taking video information into account. For \texttt{LLaMA-3.1}, we explore two variants: in \texttt{LLaMA-3.1$_{transcript}$}, we extract audio from video files using \href{https://zulko.github.io/moviepy/}{\texttt{moviepy}} and transcribe it with OpenAI's \texttt{Whisper-1} to generate text input for the model. In \texttt{LLaMA-3.1$_{OCR}$}, we apply \href{https://github.com/JaidedAI/EasyOCR}{\texttt{EasyOCR}} to extract on-screen text from video frames and use the OCR-generated text as input for summarization. Similarly, for \texttt{Qwen2-Audio}, we use \href{https://zulko.github.io/moviepy/}{moviepy} to convert video files into audio and treat the audio as input. Exact model versions are provided in \autoref{model_versions}. Based on our benchmarking results, we select the best-performing model as the backbone for the plan-based strategy and evaluate its performance. Prompts for the above models are provided in \autoref{sec:prompts-study} (Figures~\ref{non_plan_based_summary_generation}--\ref{SG_model}).

\paragraph{Experimental Setup}
To ensure a fair comparison, all models, including baselines, plan-based models, and ablation models, are evaluated under identical hyperparameter settings unless explicitly stated otherwise. All models are tested using identical prompt instructions. Detailed hyper-parameter configurations are provided in \autoref{hyper-parameters_settings}.

\paragraph{Evaluation Metrics}
We report a set of evaluation metrics to measure informativeness, alignment, and factual consistency in summaries. For informativeness, we use ROUGE \cite{lin-2004-rouge}, SacreBLEU \cite{post-2018-call}, METEOR \cite{banerjee-lavie-2005-meteor}, BERTScore \cite{zhang2019bertscore}, and CIDEr-D \cite{vedantam2015cider}. Specifically, we provide the F1 scores for Rouge-1 (R1), Rouge-2 (R2), and Rouge-LSum (RLSUM). Alignment to the input video is evaluated with VideoScore \cite{he-etal-2024-videoscore}, and factual consistency with FactVC \cite{liu-wan-2023-models}. Detailed descriptions of these metrics are given in \autoref{automatic_evaluation_metrics}.

\section{Results and Analysis}
\label{results_analysis}

\paragraph{General Results}

\autoref{tab:model_performance} illustrates the performance differences between closed-source and open-source models. In the zero-shot setting, closed-source models generally outperform their open-source counterparts. Among open-source models, \texttt{mPLUG-Owl3} stands out, particularly in semantic alignment (BERTScore) and video-text consistency (VideoScore). Fine-tuning on in-domain data yields noticeable improvements for open-source models with both QLoRA and full-parameter fine-tuning. QLoRA shows overall lower performance than full parameter fine-tuning. 

\texttt{LLaMA-3.1$_{transcript}$}, \texttt{LLaMA-3.1$_{OCR}$}, and \texttt{Qwen2-Audio} perform similarly on our dataset. While both text- and audio-based models achieve competitive results, video-based LMMs demonstrate overall superior performance, with \texttt{mPLUG-Owl3} achieving SOTA results across most metrics. This result further underlines the importance of video for our summarization task.

\PlanSum is the plan-based approach built on \texttt{mPLUG-Owl3}, outperforming all open-source baselines in both zero-shot and fine-tuned settings. For zero-shot inference, the \PlanSum$^\clubsuit$ variant, which fine-tunes only the Plan Generation (PG) module, surpasses other models in summary quality, factual consistency, and semantic alignment. With full-parameter fine-tuning, \PlanSum achieves the highest overall scores across models, showing improvements in factual accuracy (+3.47 in FactVC) and quality (+0.34 in RLsum) compared to \texttt{mPLUG-Owl3}. However, all models (including the plan-based method) exhibit hallucinations (FactVC) and alignment (VideoScore) issues, and there are still significant differences (p-value of the paired t-test is less than 0.05) between the human performance in this task, with reference abstracts scoring 88.54 on FactVC and 4.62 on VideoScore.

\paragraph{Impact of Plan Generation Strategy}

We analyze the plan generation ablation strategy by comparing it with simpler baselines: Lead-3$_Q$, Tail-3$_Q$, and Random-3$_Q$. In these ablation baselines, plans are generated by selecting the first three, last three, or three randomly chosen summary sentences, respectively. Each selected sentence serves as a target for generating a question, with its preceding sentences providing the context. For instance, in the Lead-3$_Q$ setting, the first sentence is used as the target (without any preceding context), prompting the first question in the plan, while subsequent sentences incorporate earlier ones as context. Additionally, we compare the case where QUD is not considered. That is, we directly let \texttt{GPT-o1} generate all planning questions at once only based on the reference summary (\texttt{NoQUD}).

\begin{table}[t]
\centering
\scalebox{0.8}{
\tabcolsep=4pt
\begin{threeparttable}
\begin{tabular}{l c c c c}
\toprule
Model & R2 & RLsum & VideoScore & FactVC\\
\hline
\PlanSum & 13.74 & 33.25 & 3.33 & 75.41 \\
\hdashline
\texttt{NoQUD} & 13.66 & 33.02 & 3.28 & 73.32 \\
\hdashline
Lead-3$_Q$ & 12.87 & 30.64 & 2.95 & 71.26\\
Tail-3$_Q$ & 11.62 & 30.51 & 2.88 & 63.82\\
Random-3$_Q$ & 11.57 & 30.48 & 2.87 & 64.28\\
\bottomrule
\end{tabular}
\end{threeparttable}
}
\caption{Performance comparison of different plan generation strategies under full fine-tuning settings. Textual content at the start of the summary is more helpful for generating plans.}
\label{tab:ablation_study}
\end{table}

\autoref{tab:ablation_study} underlines the performance differences across different plan generation ablation strategies. \texttt{NoQUD} is also a plan-based approach. It has lower data processing overhead than our original method and performs better than the end-to-end method. However, it still falls short to some extent compared to our approach. The Lead-3$_Q$ strategy performs better overall compared to Tail-3$_Q$ and Random-3$_Q$, indicating that initial sentences offer stronger contextual continuity for generating plan questions. Nonetheless, these heuristic strategies fail to match the performance of the original planning method.

\paragraph{Impact of Plan Quality}

We assess how the quality of the plan questions affects model performance. We applied \texttt{GPT-o1} as a question generator in a zero-shot setting in our previous experiments. For comparative analysis, we additionally incorporate \texttt{Llama-3.1} and a state-of-the-art question generation algorithm (RAST) from \citet{gou-etal-2023-diversify} to generate the plan questions. In addition, we apply a Random Replacement (RR) method, where questions generated by \texttt{GPT-o1} are randomly replaced with irrelevant ones. The number of replaced questions per summary ranges from one to the entire set. We also introduce full random replacement (FRR), where questions generated by \texttt{GPT-o1} are all replaced with randomly irrelevant questions.\footnote{The prompt for generating irrelevant questions is given in Appendix~\autoref{Irrelevant_Question_Generation}.}

\begin{figure}[t]
  \centering 
  \includegraphics[width=0.48\textwidth]{figs/plan_quality_impact.pdf}
  \caption{Noise in plan generation impacts summarization performance. FRR is a shorthand for Full Random Replacement and RR for Random Replacement. RAST is a SOTA question generation method.}
  \label{fig:plan_quality_impact}
\vspace{-10pt}
\end{figure}

\autoref{fig:plan_quality_impact} reveals that the quality of plan questions does influence the summarization performance: using \texttt{GPT-o1} to generate questions outperforms the rest. The FRR method performs worst, as irrelevant questions disrupt the alignment between the plan and summary content. We also find that the plan-based method exhibits a certain degree of robustness, as it performs reasonably well even when the plans contain some degree of noise (RR vs. FRR). These findings emphasize the importance of question relevance and quality in structuring the output summaries. In \autoref{additional_analysis}, we further explore the effect of video content on our summarization task,  varying the length of the video given as input to the model. We also perform experiments with different textual contexts for generating plan questions, and with controlled generation. Additionally, we present an error analysis of model output in \autoref{case_study}, which highlights the gap between model-generated summaries and human-written references.

\section{Human Evaluation}
\label{human_evaluation}
We conduct a human evaluation on 50 randomly selected instances from the VISTA test set. Annotators include master’s and doctoral students in computer science or computational linguistics with advanced English proficiency. They receive compensation per our university’s standard rate and are blind to the source of each summary to ensure impartial assessment. We compare \PlanSum, \texttt{mPLUG-Ow13}, \texttt{LLAVA-NeXT-Interleave}, and \texttt{GPT-o1} against human reference summaries/abstracts. Three independent annotators are asked to review the source video and evaluate corresponding model summaries (and the human upper bound) on a 1--5 Likert scale for Faithfulness, Relevance, Informativeness, Conciseness, and Coherence (higher scores indicate better quality). They are also asked to provide an overall ranking. In total, participants rated~750 samples ($50 \times 5 \times 3$). \autoref{human_evaluation_guideline}  contains the full annotation instructions.

\begin{figure}[t]
  \centering \includegraphics[width=0.48\textwidth]{figs/human_evaluation.pdf}
  \caption{Human evaluation results. Human-written summaries consistently outperform all neural models.}
  \label{fig:human_evaluation}
\vspace{-10pt}
\end{figure}

\autoref{fig:human_evaluation} presents the performance of each model, along with the proportion of instances where models are rated best or worst. Fleiss’ Kappa scores for Faithfulness (\(\kappa = 0.767\)), Relevance (\(\kappa = 0.842\)), Informativeness (\(\kappa = 0.721\)), Conciseness, and Coherence (\(\kappa = 0.813\)) indicate a substantial level of agreement, with an average agreement score of \(\kappa = 0.787\). Overall, human-written summaries outperform all neural summarization models in quality, as they are perceived as substantially more faithful, coherent, concise, and informative. Human-written summaries are 81.7\% more likely to be rated as best compared to model-generated summaries. 

Among the four neural models, \texttt{GPT-o1} performs worst, being rated as worst~63.2\% of the time. \texttt{LLAVA-NeXT-Interleave} follows suit, with a 17.8\%~chance of receiving the worst ranking. The plan-based model, \texttt{Plan-mPLUG-Owl3}, outperforms \texttt{mPLUG-Owl3} and demonstrates superior performance across all metrics. Additionally, it stands out among neural summarization systems for its higher likelihood of generating high-quality summaries. Paired t-tests show that human answers are considered significantly better than all neural models in all metrics (\(p < 0.05\)), revealing a clear gap between automatic systems and human performance on the VISTA dataset. The plan-based method is significantly better (\(p < 0.05\)) than other neural models in faithfulness, coherence, and informativeness, although it falls short of human performance. We also evaluate all samples of the test set with an LMM-as-Judge and obtain results that are broadly consistent with human evaluation. We describe the details of this study in \autoref{lmm-as-judge}.

\section{Conclusion}
This paper introduces VISTA, a dataset for summarizing scientific video presentations into concise textual summaries. Comprehensive evaluations across multiple large models demonstrate that the summarization task is challenging, relying on the interplay of multiple modalities (video, text, and audio). We further introduce a plan-based approach, which yields improvements in summary quality and factual accuracy. Beyond dataset creation, our work also confirms that current leading large models still exhibit a noticeable gap compared to human performance.

\section*{Ethical Considerations}
All data in our dataset are sourced from publicly accessible resources, strictly adhering to relevant copyright regulations. Each data sample explicitly includes the corresponding source URL and author attribution. Throughout the processes of data processing, experimental analysis, model training, and evaluation, no instances of privacy infringement were identified. In human evaluations, all participants volunteered willingly and were fairly compensated. We provided a safe and comfortable environment for our participants and complied with \href{https://www.aclweb.org/adminwiki/index.php/ACL_Policy_on_Publication_Ethics}{ACL's Policy on Publication Ethics} throughout our studies.

\section*{Limitations}

\paragraph{Data} All the summary and video data used in this study are open source. While our sources are generally of high quality and exhibit a broad range of diversity, we have not investigated inherent biases in the data. Moreover, as these data represent only a small fraction of real-world data, our findings may not extend to all video-to-text summarization scenarios. In addition, our dataset is restricted to English, which limits its generalizability to other languages.

\paragraph{Task} In our task, we consider the paper abstract as the summary of the corresponding video. This hypothesis has been supported by our two-stage quality control process, which ensures a strong alignment. However, we acknowledge that there may be nuanced differences between the abstract and a textual summary derived solely from the video. That said, authors often present the abstract as a summary of the video, as it conveys the key contributions, objectives, and findings of the research, which are typically central to the content discussed.

\paragraph{Model} We use several state-of-the-art large models in our experiments and select the best-performing model, \texttt{mPLUG-Owl3}, to demonstrate the effectiveness of the planning strategy. These large models may carry biases introduced during pretraining. We have not assessed the extent of these biases, as they lie beyond the scope of this study. Furthermore, we have not tested the plan-based approach on all model variants presented in our experiments (e.g.,~text-based large models and audio-based large models). Our work does not aim to prove that the plan-based method is effective in \emph{all} models of different modalities, but rather to demonstrate that the plan-based method can improve the performance of video-based models on our dataset. Moreover, plan-based methods can take many different forms, and our work does not aim to identify the optimal planning approach for our dataset. Future work could examine how the plan-based method performs across a wider range of models and modalities.

\paragraph{Modality} In our experiments, we explore the performance of individual modalities on the downstream summarization task (e.g., text-to-text model and audio-to-text model). However, we do not conduct an in-depth analysis of how different modality combinations impact the final summarization results. For instance, combining video transcripts with their visual content (transcript $+$ video-to-text) or with their audio (transcript $+$ audio-to-text) could yield different outcomes. Moreover, most video LMMs do not incorporate audio components; we also do not investigate how the integration of different modality components within video LMMs affects summarization results. The exploration of such modality combinations and their influence on summarization is left for future work.

\paragraph{Data Contamination and Prompt Selection} It is worth noting that we have not found evidence in the original papers describing the open-source models we use to suggest that the contents of the VISTA dataset are included in their pretraining stage. However, for closed-source models, such verification is not possible due to the lack of transparency in their pretraining datasets. Additionally, for the sake of consistency and fairness, we utilize the same prompts throughout our experiments, chosen primarily based on human judgment. However, since the number of possible prompts is limitless, other prompts could yield different outcomes. These factors represent potential directions for future research.

\paragraph{Scope} Our study focuses on video-to-text summarization within scientific domains. We have not investigated applying the plan-based method to other natural language processing (NLP) tasks, such as multimodal machine translation, multimodal question answering, or multimodal reasoning. Although the plan-based approach could likely be adapted to these tasks with minimal effort, such possibilities remain unexplored and warrant future investigation.

\paragraph{Automated Evaluation} While we employ a suite of automated metrics and hallucination detection methods to assess model performance on the test set, these metrics have inherent limitations and may fail to capture all aspects of model quality.

\paragraph{Human Evaluation} Similar to many earlier studies \cite{papalampidi-lapata-2023-hierarchical3d, krubinski-pecina-2023-mlask, krubinski-pecina-2024-towards, patil-etal-2024-refinesumm}, we only evaluate 50 video-summary pairs, a subset that may not represent the entire dataset. Additionally, while all evaluators are graduate students, they are not necessarily experts in video-to-text summarization and possess varying levels of reading and assessment skills. Consequently, although their evaluations are valuable, they should not be treated as the only indicator of performance.

\paragraph{LMM-as-Judge Evaluation} Although the LMM-based judge paradigm (\texttt{GPT-o1}) enables large-scale and relatively consistent evaluations, it may inherit biases from its pretraining data, and its black-box nature makes the rating process difficult to interpret. Data contamination also remains a concern if \texttt{GPT-o1} is trained on overlapping data. We validate \texttt{GPT-o1}’s ratings with human evaluations on a small subset of samples, but this may not fully capture the model’s reliability across diverse topics, domains, or summary styles. Therefore, results should be interpreted with caution and supplemented by human judgment where possible.

\section*{Acknowledgements}
This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 Research and Innovation Programme (Grant Agreement No. 948878).
\begin{figure}[H] 
\centering
\includegraphics[width=0.8\columnwidth]{figs/ERC.pdf}
\end{figure}


\bibliography{custom}

\appendix

\section{Copyright}
\label{copyrights}
According to the statement displayed on the \href{https://aclanthology.org/}{ACL Anthology} website, \textit{``Permission is granted to make copies for the purposes of teaching and research''}, allowing us to use the corresponding data. For \href{https://icml.cc/}{ICML} and \href{https://neurips.cc/}{NeurIPS}, we (the authors) have obtained written confirmation granting permission to use the paper titles, author lists, paper abstracts, full papers, and presentation videos available on their websites for research purposes.

\section{Quality Control}
\label{quality_control}
\paragraph{Manual Control:}
We randomly select 500 video-summary pairs to assess whether the summaries provide accurate descriptions of the videos. Two Ph.D. candidates in Computer Science or Computational Linguistics perform binary judgments on these pairs. Across all 500 samples, neither evaluator rejected any sample.

\paragraph{Automated Control:}
To go beyond the limited scope of manual checks, we employ \texttt{GPT-o1} for automated assessment using the same binary criteria across all data samples. The model initially flagged 39 pairs as potentially invalid. These flags were likely caused by difficulties in interpreting domain-specific terms or rare expressions and sensitivity to variations in summary length. After further manual review, all 39 samples were confirmed as valid and retained in the dataset.

\section{Data Sample}
\label{data_sample}

The VISTA dataset contains carefully curated video-text pairs, predominantly sourced from published papers, aiming to ensure a high standard of quality and relevance. The accompanying texts are designed to function as summaries of their respective videos, offering a concise representation of their content (see \autoref{data_sample_1}). Additionally, our dataset focuses on topics within the field of artificial intelligence, making it a good resource for research in AI-related video-to-text summarization and comprehension.

\begin{figure*}[t]
\centering
\begin{tcolorbox}[colback=gray!10!white, colframe=black!50!black, width=\textwidth, arc=0mm, boxrule=0.5mm, sharp corners=south]
   \begin{minipage}[t]{\textwidth}
       % \centering
     \includegraphics[width=\textwidth]{figs/data_sample_1.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{\textwidth}
        \raggedright
        \begin{footnotesize}
        Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations that require context. This methodology confirms the difficulty of previously studied phenomena while uncovering others which were not previously addressed. We find that commonly studied context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage the MT community to focus on accurately capturing discourse phenomena.
        \end{footnotesize}
    \end{minipage}
\end{tcolorbox}
\caption{A random sample from the VISTA dataset, originating from \citet{fernandes-etal-2023-translation}.}
\label{data_sample_1}
\end{figure*}

\section{Model Version Details}
\label{model_versions}
 
\autoref{tab:model_version} provides the detailed version identifiers for the models evaluated in our study, showing both model names as referenced in the main text and the specific versions used in our experiments.

\begin{table*}[t]
\centering
\scalebox{0.95}{
\tabcolsep=8pt
\begin{tabular}{l l c}
\toprule
Model & Version & Model Size\\
\midrule
\texttt{GPT-o1} \cite{achiam2023gpt} & \texttt{o1-2024-12-17} & Unknown\\
\texttt{Gemini 2.0} \cite{team2023gemini} & \texttt{Gemini 2.0 Flash} & Unknown\\
\texttt{Claude 3.5 Sonnet} \cite{anthropic_claude3_5_sonnet} & \texttt{claude-3-5-sonnet-20241022} & Unknown\\
\hdashline
\texttt{LLaMA-3.1} \cite{touvron2023llama} & \texttt{LLaMA-3.1-8B-Instruct} & 8B \\
\texttt{Qwen2-Audio} \cite{chu2024qwen2} & \texttt{Qwen2-Audio-7B-Instruct} & 7B \\
\hdashline
\texttt{Video-LLaMA} \cite{zhang-etal-2023-video}& \texttt{VideoLLaMA2-7B-16F} & 7B\\
\texttt{Video-ChatGPT}  \cite{maaz-etal-2024-video}& \texttt{Video-ChatGPT-7B} & 7B\\
\texttt{Video-LLaVA}  \cite{lin-etal-2024-video}& \texttt{Video-LLaVA-7B-hf} & 7B\\
\texttt{LLaMA-VID}  \cite{li2025llama}& \texttt{LLaMA-VID-7B-Full-224-Long-Video} & 7B\\
\texttt{LLaVA-NeXT-Interleave}  \cite{li2024llava}& \texttt{LLaVA-NeXT-Interleave-Qwen-7B} & 7B\\
\texttt{mPLUG-Owl3} \cite{ye2024mplug}& \texttt{mPLUG-Owl3-7B-241101}& 7B\\
\bottomrule
\end{tabular}
}
\caption{Model version details.}
\label{tab:model_version}
% \vspace{-15pt}
\end{table*}
\begin{table}[t]
\centering
\scalebox{0.65}{
\tabcolsep=4.5pt
\begin{threeparttable}
\begin{tabular}{c l c c c c}
\toprule
Context & Model & R2 & RLsum & VideoScore & FactVC\\
\hline
\multirow{2}*{\rotatebox[origin=c]{360}{\mbox{All}}} 
& \texttt{mPLUG-Owl3} & 13.62 & 32.91 & 3.28 & 71.94 \\
~ & \PlanSum & 13.74 & 33.25 & 3.33 & 75.41 \\
\midrule
\multirow{2}*{\rotatebox[origin=c]{360}{\mbox{First 10\%}}} 
& \texttt{mPLUG-Owl3} & 6.31 & 25.44 & 2.37 & 51.02 \\
~ & \PlanSum & 7.37 & 27.38 & 2.52 & 52.39 \\
\midrule
\multirow{2}*{\rotatebox[origin=c]{360}{\mbox{First 30\%}}} 
& \texttt{mPLUG-Owl3} & 9.42 & 28.88 & 2.78 & 54.10 \\
~ & \PlanSum & 10.59 & 30.13 & 2.78 & 55.37 \\
\midrule
\multirow{2}*{\rotatebox[origin=c]{360}{\mbox{Last 10\%}}}
& \texttt{mPLUG-Owl3} & 6.53 & 27.34 & 2.51 & 53.64  \\
~ & \PlanSum & 7.62 & 29.73 & 2.77 & 55.93  \\
\midrule
\multirow{2}*{\rotatebox[origin=c]{360}{\mbox{Last 30\%}}} 
& \texttt{mPLUG-Owl3} & 7.32 & 29.17 & 2.82 & 57.36 \\
~ & \PlanSum & 10.72 & 31.29 & 2.98 & 62.05 \\
\bottomrule
\end{tabular}
\end{threeparttable}
}
\caption{Model performance under different video context configurations (full fine-tuning). The video content at the end is more helpful for summary generation.}
\label{tab:context_configurations}
\end{table}

\begin{figure}[hbt!]
  \centering 
  \includegraphics[width=0.48\textwidth]{figs/text_contextual_variations_impact.pdf}
  \caption{Impact of context for plan generation.}
  \label{fig:text_contextual_variations_impact}
\end{figure}

\section{Hyper-parameters Settings}
\label{hyper-parameters_settings}
For all fine-tuning experiments, we utilize the AdamW optimizer \cite{loshchilovdecoupled} with $\beta_1$ = 0.9, $\beta_2$ = 0.999, $\epsilon$ = $10^{-9}$, and a weight decay of~0.1, combined with a warm-up ratio of~0.15. The initial learning rate is set to 5e-5, with cosine learning rate scheduling. DeepSpeed is configured with ZeRO-3 Offload. We set the random seed to 2025 and apply a dropout rate of 0.1. In the QLoRA setting, the rank~$r$ is set to~32, the scaling factor~$\alpha$ is set to 64, and the dropout rate for the low-rank matrices is~0.1. All other parameters follow the default settings of the Transformers library. 

During training, we save the checkpoint with the highest Rouge-2 F1 score on the validation set as the final model. All experiments are conducted over 16 epochs with a batch size of 16 and early stopping  (all models converged before 16 epochs). For model inference (including zero-shot learning), we employ a beam search with a beam of size~4, a length penalty of~3.0, a no-repeat n-gram size of 3, and the maximum number of new tokens generated is limited to 256. For video-based LMMs, the sampling rate is set to 0.1 fps, and the number of extracted frames is set to 32.

For closed-source models, results are obtained via API requests during the experimental period from 01/09/2024 to 10/02/2025. The hyper-parameter settings for these API requests include a temperature of 1, top\_p of 1, a frequency penalty of 0.2, and a presence penalty of 0.2. All other parameters adhere to the default settings specified by their respective platforms.

\section{Automatic Evaluation Metrics}
\label{automatic_evaluation_metrics}

In line with common practice in video-to-text summarization research, we evaluate the model-generated summaries using the following metrics:
\begin{itemize}[leftmargin=8pt,itemsep=1pt,topsep=1pt,parsep=1pt]
\item ROUGE \cite{lin-2004-rouge}: measures n-gram overlap between machine-generated and human reference texts. We report F1 scores for Rouge-1 (R1), Rouge-2 (R2), and Rouge-Lsum (RLSUM).
\item SacreBLEU \cite{post-2018-call}: assesses linguistic consistency and fluency between generated and reference texts.
\item METEOR \cite{banerjee-lavie-2005-meteor}: calculates the harmonic mean of unigram precision and recall, placing greater emphasis on recall for a balanced evaluation.
\item BERTScore \cite{zhang2019bertscore}: uses contextual embeddings from BERT to evaluate semantic similarity and word overlap between texts.
\item CIDEr-D \cite{vedantam2015cider}: evaluates the consensus between generated summaries and references by using TF-IDF weighting combined with a decay factor to reduce the impact of repeated terms.
\item VideoScore \cite{he-etal-2024-videoscore}: focuses on text-to-video alignment, evaluating how accurately video content matches the given text prompts using fine-grained multi-aspect scoring.
\item FactVC \cite{liu-wan-2023-models}: calculates the factual consistency of text with video content by aligning coarse-grained video-text similarity and precision-based fine-grained matching. The values of FactVC range from 0 to 1, and in our experiments, we scale them by 100 to convert them into percentages.
\end{itemize}

\section{Additional Analyses}
\label{additional_analysis}
\paragraph{Impact of Video Context on Summary Generation}
We examine the impact of different video context configurations on summary generation, comparing   \texttt{mPLUG-Owl3} with \PlanSum. Unlike earlier experiments that use the full video as input, here only the first or last 10\% or 30\% of the video is provided as input.  We report results with  R2, BERTScore, VideoScore, and FactVC in the full fine-tuning setting. 

The results in \autoref{tab:context_configurations} indicate that partial video context consistently underperforms compared to using the full video. Using the last part of the video generally produces better results than using the first part, as concluding sections often summarize key findings while opening sections primarily introduce background information. Additionally, utilizing 30\% of the video outperforms using only 10\%, highlighting that more content generally yields better outputs. Across all configurations, the \PlanSum model consistently outperforms \texttt{mPLUG-Owl3}.

\paragraph{Impact of Text Context on Plan Generation}

The generation of plan questions in our experiments is influenced by the target sentence and its context.  In our main experiments, plan questions are generated based on the target sentence and its preceding summary text (Previous-Context), in line with the original Questions Under Discussion (QUD) requirements \cite{wu-etal-2023-qudeval, wu-etal-2023-elaborative}. We now assess configurations that generate questions only based on the target sentence (No-Context) or the entire summary (All-Context).

As shown in \autoref{fig:text_contextual_variations_impact}, performance differences between different context configurations are relatively small (yet superior to models without planning components shown as red and blue dashed lines). No-Context shows the lowest performance but is the most cost-effective, as it requires the shortest input length for \texttt{GPT-o1} during question generation. All-Context achieves slightly better results but at the highest computational cost due to the long input length. Previous-Context is aligned with QUD and strikes a good balance, achieving the best performance for a moderate cost.

\paragraph{Controllable Generation}  
An advantage of plan-based models is their ability to control the output summaries by modifying the plans used for generation. We investigate how modifying the structure and composition of these plans impacts the generated summaries, specifically comparing their performance against direct summary generation control through instructions. To this end, we design two controlled experiments:

\begin{itemize}[leftmargin=8pt,itemsep=1pt,topsep=1pt,parsep=1pt]
\item \textit{Summary Readability}: How question complexity affects readability, tailored for lay readers or expert readers.
\item \textit{Summary Length}: How the number of questions influences summary length, by removing 10\%, 30\%, and 60\% of questions.
\end{itemize}

\begin{table}[t]
\centering
\scalebox{0.9}{
\tabcolsep=5pt
\begin{tabular}{l c c c c}
\toprule
\multirow{2}{*}{Condition} & \multicolumn{2}{c}{\PlanSum} & \multicolumn{2}{c}{\texttt{GPT-o1}} \\
\cmidrule(lr){2-3} \cmidrule(l){4-5}
 & R2 & FRE & R2 & FRE \\
\midrule
No change        & 13.74 & 30.62 & 5.69 & 26.37 \\
Lay questions    & 13.38 & 35.17 & 4.26 & 28.94 \\
Expert questions & 13.24 & 23.54 & 4.13 & 24.33 \\
\bottomrule
\end{tabular}
}
\caption{Control experiment for summary readability. FRE $=$ Flesch Reading Ease.}
\label{tab:readability_control}
\end{table}

\begin{table}[t]
\centering
\scalebox{0.85}{
\tabcolsep=3pt
\begin{tabular}{l c c c c}
\toprule
\multirow{2}{*}{Condition} & \multicolumn{2}{c}{\PlanSum} & \multicolumn{2}{c}{\texttt{GPT-o1}} \\
\cmidrule(lr){2-3} \cmidrule(l){4-5}
 & R2 & Avg. \#Tokens & R2 & Avg. \#Tokens \\
\midrule
No deletion  & 13.74 & 202.39 & 5.69 & 267.32 \\
Delete 10\%  & 11.05 & 178.47 & 4.32 & 220.49 \\
Delete 30\%  & 10.41 & 137.72 & 3.17 & 192.42 \\
Delete 60\%  & 8.01 & 100.32 & 2.98 & 185.28 \\
\bottomrule
\end{tabular}
}
\caption{Control experiment for summary length.}
\label{tab:length_control}
\end{table}

We note that the plan-based method employs an explicit planning component where each sentence is guided by a corresponding question that facilitates fine-grained control over the summary’s style or content. Specifically, after \texttt{PG} produces the plan, we use \texttt{GPT-o1} to edit it and then feed the edited questions back to \texttt{SG} for the final output. For \texttt{GPT-o1}, which operates in a zero-shot manner, we prepend constraints directly in the prompt. Specifically, \texttt{GPT-o1} generates an initial summary in one pass and then applies additional prompt-based instructions during a secondary rewriting step to control the output. Both control experiments (\autoref{tab:readability_control}) (\autoref{tab:length_control}) reveal similar trends: while performance declines for both models, the plan-based method is more robust and controllable.

In the readability control experiment (\autoref{tab:readability_control}), both models show reductions in R2, but \PlanSum declines less, averaging an R2 loss of~0.43 compared to~1.50 for \texttt{GPT-o1}. Furthermore, \PlanSum controls readability more effectively, achieving a higher Flesch Reading Ease (FRE) score\footnote{The FRE score, which ranges from 0 to 100, measures text readability, with higher scores indicating easier-to-read content, and lower scores reflecting greater complexity.} of~35.17 for lay questions, compared to 28.94~for \texttt{GPT-o1}, and a lower FRE score of 23.54 for expert questions.

In the length control experiment (\autoref{tab:length_control}), R2 scores decline as content is removed, but plan-based model aligns more closely with target compression ratios, producing summaries averaging 100.32 tokens under 60\% deletion, while \texttt{GPT-o1} generates longer summaries (185.28 tokens).

\section{Case Study and Error Analysis}
\label{case_study}
For our case study, we randomly select a sample \cite{kubler2020learning} from the test split. The analysis in \autoref{tab:case_study}  reveals differences in summary quality across models,  and against the human-written text. Specifically, \texttt{GPT-o1} often produces concise summaries but at the cost of precision. For example, it incorrectly claims that ``data splitting helps control test thresholds,'' which is a hallucination --- while data splitting ensures a tractable null distribution, it does not explicitly control test thresholds. Furthermore, its summaries frequently oversimplify complex concepts, reducing the depth of explanations and omitting crucial distinctions, such as the role of dependency calibration in the proposed method. Similarly, \texttt{mPLUG-Owl3} introduces factual inaccuracies, such as stating that data splitting ``ensures a reliable null distribution.'' This phrasing misleadingly implies that reliability is an inherent property of data splitting, whereas the correct point is that it makes the null distribution tractable rather than necessarily more reliable.

\PlanSum is more factually accurate than the other models. It correctly captures the main idea of full-sample hyperparameter learning and testing without data splitting. However, it still introduces subtle distortions, such as falsely suggesting a ``trade-off'' between test power and tractability, which misrepresents the actual relationship. These inaccuracies, while less severe than those in \texttt{GPT-o1} and \texttt{mPLUG-Owl3}, highlight the model’s tendency to infer unstated causal links, leading to potential misinterpretations. Despite the relative strengths of \PlanSum, all generated summaries fall short of human-written text. The model-generated outputs consistently struggle with informativeness, coherence, and factual accuracy. These shortcomings underscore the ongoing challenge of improving automated summarization systems to better align with human standards in both accuracy and clarity.

Controlled generation experiments also reveal that hallucination issues are further amplified when imposing constraints on readability and length. Under readability control (\autoref{tab:readability_control_1}), \texttt{GPT-o1} is more likely to introduce fabricated or misleading content when forced to generate more complex outputs. This occurs because it lacks an explicit mechanism to ensure factual consistency while adapting to varying readability demands. Rather than relying on implicit internal heuristics,  \PlanSum has an explicit planning mechanism which makes it less likely to introduce unsupported claims. Planning provides an additional layer of control, helping the model maintain factual alignment even as readability demands change.  

A similar trend is observed in length control experiments (\autoref{tab:readability_control_2}). As the compression ratio increases, \texttt{GPT-o1} struggles to balance conciseness and informativeness, sometimes hallucinating missing details to compensate for omitted content. This suggests that purely instruction-based control (i.e.,~prompting the model to shorten outputs) does not effectively enforce content retention, leading to greater inconsistencies. In contrast, the plan allows \PlanSum to selectively retain essential elements, reducing the risk of generating misleading content; it can also avoid answering deleted questions, to a certain extent.

These findings reinforce the advantages of plan-based control over instruction-based prompting. While neither approach fully eliminates hallucinations, planning provides a structured mechanism to manage content selection, ensuring greater alignment with the input source compared to freeform generative adjustments.

\begin{table*}[t]
\centering
\scalebox{1}{\tabcolsep=3pt
\begin{threeparttable}
\begin{tabular}{|p{1.9cm}|p{13cm}|}
\hline
\scriptsize Model & \scriptsize Output\\
\hline
\scriptsize Reference & \scriptsize Modern large-scale kernel-based tests such as maximum mean discrepancy (MMD) and kernelized Stein discrepancy (KSD) optimize kernel hyperparameters on a held-out sample via data splitting to obtain the most powerful test statistics. While data splitting results in a tractable null distribution, it suffers from a reduction in test power due to a smaller test sample size. Inspired by the selective inference framework, we propose an approach that enables learning the hyperparameters and testing on the full sample without data splitting. Our approach can correctly calibrate the test in the presence of such dependency, and yield a test threshold in closed form. At the same significance level, our approach's test power is empirically larger than that of the data-splitting approach, regardless of its split proportion. \\
\hline
\scriptsize \texttt{GPT-o1} & \scriptsize Modern kernel-based tests like MMD and KSD use data splitting to tune hyperparameters. This splitting \textcolor{red}{helps control test thresholds} but reduces power because it splits the data into smaller parts. We propose a new method to tune hyperparameters without splitting the data. Our method provides better test power than data splitting. \\
\hline
\scriptsize \texttt{mPLUG-Owl3} & \scriptsize  Large-scale kernel-based tests like maximum mean discrepancy (MMD) and kernelized Stein discrepancy (KSD) usually split the data to tune hyperparameters. While this approach ensures \textcolor{red}{a reliable null distribution}, it reduces test power due to smaller sample sizes. We propose a method that avoids data splitting, enabling full-sample hyperparameter learning and testing. This method improves test power at the same significance level compared to data splitting, with \textcolor{red}{a closed-form threshold for the test.} \\
\hline
\scriptsize Plan Questions & \scriptsize q1: How do kernel-based tests like MMD and KSD typically optimize hyperparameters? \newline
q2: What is the key idea behind the proposed approach? \newline  
q3: What are the advantages of the proposed method compared to data-splitting methods? \\
\hdashline
\scriptsize \PlanSum  & \scriptsize Modern kernel-based tests like maximum mean discrepancy (MMD) and kernelized Stein discrepancy (KSD) typically split the data to optimize hyperparameters, \textcolor{red}{trading off} test power for a tractable null distribution. Inspired by the selective inference framework, we propose a novel approach that eliminates the need for data splitting, allowing full-sample hyperparameter learning and testing. Our method addresses dependencies introduced by learning and provides a closed-form test threshold while achieving higher test power than data-splitting methods, regardless of split proportions. \\
\hline
\end{tabular}
\end{threeparttable}
}
\vspace*{-.2cm}
\caption{Comparative case study of model outputs. Hallucinations are highlighted in \textcolor{red}{red} color.}
\label{tab:case_study}
\end{table*}

\begin{table*}[h]
\centering
\scalebox{1}{\tabcolsep=3pt
\begin{threeparttable}
\begin{tabular}{|p{1.9cm}|p{13cm}|}
\hline
\scriptsize Model & \scriptsize Output \\
\hline
\scriptsize \texttt{GPT-o1} & \scriptsize Modern kernel-based tests, such as MMD and KSD, use data splitting to \textcolor{red}{adjust hyperparameters}. This data splitting ensures \textcolor{red}{proper test thresholds} but weakens test power because the data is divided into smaller portions. We propose a new method that avoids splitting the data and provides better test power. \\
\hline
\scriptsize Plan  Questions & \scriptsize q1: What do MMD and KSD do with data splitting? \newline  
q2: What is the main purpose of the proposed method? \newline  
q3: Why is the proposed method better than data splitting?\\
\hdashline
\scriptsize \PlanSum  & \scriptsize Kernel-based tests like maximum mean discrepancy (MMD) and kernelized Stein discrepancy (KSD) typically divide the data into subsets to \textcolor{red}{adjust hyperparameters}. This approach ensures a manageable null distribution but reduces test power. Inspired by selective inference, we propose a new method that avoids splitting the data. This method uses the entire dataset for hyperparameter tuning and testing, textcolor{red}{eliminating dependencies introduced by learning}. It also \textcolor{red}{provides a clear, closed-form test threshold} and achieves higher test power compared to data-splitting methods, regardless of the split proportions. \\
\hline
\end{tabular}
\end{threeparttable}
}
\vspace*{-.2cm}
\caption{Decreasing reading difficulty: plan modifications vs instruction adjustments. Hallucinations are in \textcolor{red}{red}.}
\label{tab:readability_control_1}
\end{table*}
\begin{table*}[h!]
\centering
\scalebox{1}{\tabcolsep=3pt
\begin{threeparttable}
\begin{tabular}{|p{2cm}|p{13cm}|}
\hline
\scriptsize Model & \scriptsize Output \\
\hline
\scriptsize \texttt{GPT-o1} & \scriptsize Kernel-based statistical methodologies, exemplified by maximum mean discrepancy (MMD) and kernelized Stein discrepancy (KSD), conventionally employ data partitioning mechanisms to facilitate hyperparameter optimization. While this stratagem \textcolor{red}{ensures a well-calibrated null distribution}, it inherently compromises the statistical power of the test due to data fragmentation. We advance a paradigm wherein \textcolor{red}{hyperparameter selection is conducted holistically}, circumventing data splitting and consequently preserving statistical power. \\
\hline
\scriptsize Plan Questions & \scriptsize q1: Through what operational principles do MMD and KSD navigate the interplay between hyperparameter tuning and null distribution calibration? \newline
q2: How does the selective inference framework inform the methodological innovations of the proposed approach? \newline
q3: What are the quantifiable and theoretical advantages of the proposed strategy in circumventing the limitations imposed by data-splitting conventions? \\
\hdashline
\scriptsize \PlanSum  & \scriptsize Modern kernel-based tests like maximum mean discrepancy (MMD) and kernelized Stein discrepancy (KSD) typically split the data to optimize hyperparameters, \textcolor{red}{trading off} test power for a tractable null distribution. Inspired by the selective inference framework, we propose a novel approach that eliminates the need for data splitting, allowing full-sample hyperparameter learning and testing. Our method addresses dependencies introduced by learning and provides a closed-form test threshold while achieving higher test power than data-splitting methods, regardless of split proportions. \\
\hline
\end{tabular}
\end{threeparttable}
}
\vspace*{-.2cm}
\caption{Increasing reading difficulty: plan modifications vs. instruction adjustments. Hallucinations are in \textcolor{red}{red}.}
\label{tab:readability_control_2}

\end{table*}
\begin{table*}[htb!]
\centering
\scalebox{1}{\tabcolsep=3pt
\begin{threeparttable}
\begin{tabular}{|p{3cm}|p{12cm}|}
\hline
\scriptsize Model & \scriptsize Output \\
\hline
\scriptsize \texttt{GPT-o1} & \scriptsize Kernel-based tests like MMD and KSD use data splitting for hyperparameter tuning, \textcolor{red}{which controls thresholds} but reduces power. We propose a method that avoids splitting, \textcolor{red}{improving test power}. \\
\hline
\scriptsize Plan Questions & \scriptsize q1: How do kernel-based tests like MMD and KSD typically optimize hyperparameters? \newline
q2: What are the advantages of the proposed method compared to data-splitting methods? \\
\hdashline
\scriptsize \PlanSum & \scriptsize Kernel-based tests, such as maximum mean discrepancy (MMD) and kernelized Stein discrepancy (KSD), rely on data splitting for hyperparameter tuning. Our method \textcolor{red}{delivers analytically derived test thresholds} and consistently outperforms data-splitting techniques in test power, regardless of how the data is divided. \\
\hline
\end{tabular}
\end{threeparttable}
}
\vspace*{-.2cm}
\caption{Deleting 30\% of plan questions vs. instruction adjustments. Hallucinations are highlighted in \textcolor{red}{red}.}
\label{tab:length_control_case}
\end{table*}

\section{LMM-as-Judge Evaluation}
\label{lmm-as-judge}

To facilitate large-scale comparisons of model outputs, we adopt a method inspired by LLM-as-Judge \cite{liusie-etal-2024-llm, liu2024aligning, zheng:ea:2024}, extending it to use a large multimodal model \cite{chen2024mllm}. The proposed LMM-based evaluator incorporates both textual and video modalities and assesses the same summary quality dimensions used in our human evaluation. Specifically, we use \texttt{GPT-o1} as the evaluator, following the hyperparameter settings in \autoref{hyper-parameters_settings}. To minimize potential bias from prior queries, the conversation history is reset before each evaluation.

We validate the agreement between \texttt{GPT-o1} and human ratings by comparing its ratings with human evaluations on the same 50 samples from the VISTA test set. We calculate Fleiss' Kappa between \texttt{GPT-o1} and mean human ratings across the dimensions of Faithfulness (\(\kappa\)=0.732), Relevance (\(\kappa\)=0.803), Informativeness (\(\kappa\)=0.730), Conciseness (\(\kappa\)=0.792) and Coherence (\(\kappa\)=0.721) at instance level. These results indicate that human evaluators and \texttt{GPT-o1} achieve substantial levels of agreement across these dimensions. Following this, we expand the evaluation to include all samples in our test set.

\begin{figure}[t]
  \centering \includegraphics[width=0.48\textwidth]{figs/gpto1_evaluation.pdf}
  \caption{LMM-as-Judge evaluation results showing that human-written summaries consistently outperform neural models.}
  \label{fig:gpto1_evaluation}
\vspace{-10pt}
\end{figure}

Compared to fine-tuned models, \texttt{GPT-o1} assigns the lowest scores to its own responses (see \autoref{fig:gpto1_evaluation}). Human-written summaries consistently receive the highest scores and are generally regarded as the best. Aligning with our human evaluations, \texttt{GPT-o1} also recognizes that the plan-based model outperformed other models. We further conduct paired t-tests to find that human summaries outperform all neural models across all metrics with statistical significance (\(p < 0.05\)). Moreover, the plan-based model demonstrates significantly better performance  (\(p < 0.05\)) than other neural models across all metrics except for conciseness. Our results also indicate that although the plan-based method can improve the performance of end-to-end models to some extent, there is a considerable gap between machine-generated and human summaries, which also reflects the challenging nature of our dataset.

\clearpage
%\onecolumn
\section{Prompts Used in Our Study}
\label{sec:prompts-study}

\begin{figure}[h]
\centering
\begin{tcolorbox}[
  colback=gray!10!white,
  colframe=black!50!black,
  title=Quality Control Guidelines,
  fonttitle=\bfseries,
  halign title=flush center,
]
\vspace*{-.2cm}
\textbf{Guidelines:}\\
Evaluate each video-text pair to determine whether the text provides a concise and accurate summary of the corresponding video.
\begin{itemize}[leftmargin=8pt,itemsep=1pt,topsep=1pt,parsep=1pt]
    \item \textbf{Concise:} Ensure the text is brief, focused, and free of unnecessary details.
    \item \textbf{Accurate:} Verify that the text faithfully represents the video's content.
\end{itemize}
Make binary judgments (\texttt{Valid} or \texttt{Invalid}) for each pair. If flagged as \texttt{Invalid}, provide a brief justification.

\textbf{Answer:}\\
Judgment: (\texttt{Valid} or \texttt{Invalid})\\
Justification: (\texttt{Justification if flagged as invalid})
\vspace*{-.2cm}
\end{tcolorbox}
\vspace{-8pt}
\caption{Quality control guidelines.}
\label{fig:quality_control}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tcolorbox}[colback=gray!10!white,colframe=black!50!black,title=Summary Generation (without plan), fonttitle=\bfseries, halign title=flush center]%, width=1\textwidth]
\vspace*{-.2cm}
Generate a summary for the provided content. \\
Content: \{\texttt{Video/Audio/Transcript/OCR}\} \\
Summary:
\vspace*{-.2cm}
\end{tcolorbox}
\vspace{-10pt}
\caption{Prompt to generate summaries without plans.}
\label{non_plan_based_summary_generation}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tcolorbox}[
  colback=gray!10!white,
  colframe=black!50!black,
  title=Question  Generation,
  fonttitle=\bfseries,
  halign title=flush center,
 % width=1\textwidth
]
\vspace*{-.2cm}
Generate a coherent and contextually relevant question based on the provided context and target sentence, ensuring that the target sentence can be treated as an answer to the generated question. \\
Context: \{\texttt{Context Text}\} \\
Target: \{\texttt{Target Sentence}\} \\
Question Sentence:
\vspace*{-.2cm}
\end{tcolorbox}
\vspace{-10pt}
\caption{Prompt for question generation.}
\label{plan_question_generation}
\end{figure}

\begin{figure}[h!]
\centering
\begin{tcolorbox}[colback=gray!10!white,colframe=black!50!black,title=Prompt for PG model, fonttitle=\bfseries, halign title=flush center]%, width=1\textwidth]
\vspace*{-.2cm}
Generate a list of questions for the provided video. \\
Video: \{\texttt{Video}\} \\
Questions:
\vspace*{-.2cm}
\end{tcolorbox}
\vspace{-10pt}
\caption{Prompt for PG model.}
\label{PG_model}
\end{figure}

\begin{figure}[h]
\centering
\begin{tcolorbox}[colback=gray!10!white,colframe=black!50!black,title=Prompt for SG model,fonttitle=\bfseries, halign title=flush center]%, width=1\textwidth]
\vspace*{-.2cm}
Generate a summary for the following video based on the plan questions.\\
Video: \{\texttt{Video}\} \\
Plan Questions: \{\texttt{Questions}\} \\
Ensure that the generated summary sequentially answers the plan questions.\\
Summary:
\vspace*{-.2cm}
\end{tcolorbox}
\vspace{-10pt}
\caption{Prompt for SG model.}
\label{SG_model}
\end{figure}

\begin{figure}[h]
\centering
\begin{tcolorbox}[colback=gray!10!white,colframe=black!50!black,title=Irrelevant Question Generation,fonttitle=\bfseries, halign title=flush center]%, width=1\textwidth]
\vspace*{-.2cm}
Randomly generate a question with a question mark. \\ 
Question Sentence: 
\vspace*{-.2cm}
\end{tcolorbox}
\vspace{-8pt}
\caption{Prompt used by GPT-o1 to generate irrelevant questions.}
\label{Irrelevant_Question_Generation}
\end{figure}

\begin{figure}[h]
\centering
\begin{tcolorbox}[colback=gray!10!white,colframe=black!50!black,title=Summary Readability Modification,fonttitle=\bfseries, halign title=flush center]%, width=1\textwidth]
\vspace*{-.2cm}
Rewrite the following text to further adjust style or detail.

Here is the text to be rewritten: \{Text\}

Refine the above text to be more \{lay/expert\} style.\\
Modified Text:
\vspace*{-.2cm}
\end{tcolorbox}
\vspace{-8pt}
\caption{Summary readability modification.}
\label{fig:Summary_Readability}
\end{figure}

\begin{figure}[h]
\centering
\begin{tcolorbox}[
  colback=gray!10!white,
  colframe=black!50!black,
  title=Summary Length Modification,
  fonttitle=\bfseries,
  halign title=flush center,
 % width=1\textwidth
]
\vspace*{-.2cm}
Rewrite the following text to further adjust style or detail.

Here is the text to be rewritten: \{Text\}

Shorten the above text by about \{10\% / 30\% / 60\%\}. Focus on the key points and remove less critical details.\\
Modified Text:
\vspace*{-.2cm}
\end{tcolorbox}
\vspace{-8pt}
\caption{Summary length modification.}
\label{fig:Summary_Length}
\end{figure}

\begin{figure}[h]
\centering
\begin{tcolorbox}[colback=gray!10!white,colframe=black!50!black,title=Plan Readability Modification,fonttitle=\bfseries, halign title=flush center]%, width=1\textwidth]
\vspace*{-.2cm}
Rewrite the following questions to further adjust style or detail.

Here are the questions to be rewritten:\\ 
1. \{Q1\}\\
2. \{Q2\}\\
...

Refine the above questions to be more \{lay/expert\} style.\\
Modified Questions:
\vspace*{-.2cm}
\end{tcolorbox}
\vspace{-8pt}
\caption{Plan readability modification.}
\label{fig:Plan_Readability}
\end{figure}



\onecolumn
\section{Human Evaluation Guidelines}
\label{human_evaluation_guideline}
\begin{figure*}[h]
\footnotesize
\centering
\begin{tcolorbox}[]
\vspace*{-.2cm}
\paragraph{Prerequisites} To participate in this evaluation, you must meet the following two criteria: (1) be a Master's or Ph.D. student in Computer Science or Computational Linguistics, and (2) demonstrate English proficiency at C2 level or higher.\footnote{\url{https://en.wikipedia.org/wiki/C2_Proficiency}} If you do not meet both criteria, we kindly ask you to refrain from participating in this task. Eligible participants are encouraged to follow the instructions below carefully.\\

\paragraph{Instructions} The following section provides detailed descriptions of the evaluation metrics and criteria used in this study. Please review the accompanying source video and the candidate summaries thoroughly. After evaluating each summary, assign scores based on the five criteria below, using a 1-to-5 Likert scale where higher scores indicate better quality:

\begin{itemize}[leftmargin=8pt,itemsep=1pt,topsep=1pt,parsep=1pt]
    \item \textbf{Faithfulness:} Assess the accuracy of the summary in representing the content of the source video. A faithful summary should adhere closely to the source material, avoiding contradictions, misinterpretations, or unverified information.
    \item \textbf{Relevance:} Measure how well the summary includes the topics and themes central to the source video. A relevant summary should focus on the content that is most pertinent to the original video.
    \item \textbf{Informativeness:} Evaluate the extent to which the summary captures the main points and essential details of the source video. An informative summary should provide a clear and comprehensive understanding of the video’s core ideas and findings.
    \item \textbf{Conciseness:} Determine the efficiency of the summary in conveying information. A concise summary should avoid redundancy and extraneous details while retaining all critical information from the source video.
    \item \textbf{Coherence:} Examine the logical flow and overall structure of the summary. A coherent summary should present information in an organized and easy-to-follow manner, ensuring that ideas connect naturally and transitions between points are smooth.
\end{itemize} 

\paragraph{Rating System}
For each metric, use the following Likert scale:
\begin{itemize}[leftmargin=8pt,itemsep=1pt,topsep=1pt,parsep=1pt]
    \item 1 (Worst):  Does not meet the criteria at all.
    \item 2 (Poor): Meets the criteria minimally.
    \item 3 (Fair): Meets the criteria adequately.
    \item 4 (Good): Meets the criteria well.
    \item 5 (Best): Fully meets the criteria.
\end{itemize}

\paragraph{Overall Ranking}
After assigning scores to each summary for the individual criteria, rank all candidates from best to worst based on their overall quality. Consider the summaries' performance across all criteria when determining the final rankings.

\end{tcolorbox}
\caption{A snapshot of the experimental instructions provided to human evaluators.}
\end{figure*}

%\onecolumn
\section{Prompt for \texttt{GPT-o1} to Evaluate Summary Quality}
\label{gpto1_summary_evaluation}
\begin{figure*}[ht!]
\footnotesize
\centering
\begin{tcolorbox}
\vspace*{-.2cm}
\textbf{Source Video:} \{\texttt{Source Video}\} \\
\textbf{Candidate Summary:} \{\texttt{Candidate Summary}\} \\ 
You are tasked with evaluating the quality of the candidate summary based on the provided source video. Please adhere strictly to the following evaluation guidelines and scoring criteria to ensure a consistent and objective evaluation. 

\textbf{Evaluation Guidelines:} \{\texttt{Guidelines}\} 

\textbf{Instructions for Output:}
\begin{itemize}[leftmargin=8pt,itemsep=1pt,topsep=1pt,parsep=1pt]
    \item Provide your evaluation using the following format, outputting scores only.
    \item Assign a score from 1 to 5 for each dimension, with 1 being the lowest and 5 being the highest.
\end{itemize}

\textbf{Output Format:}
\begin{itemize}[leftmargin=8pt,itemsep=1pt,topsep=1pt,parsep=1pt]
    \item Faithfulness: [Score]
    \item Relevance: [Score]
    \item Informativeness: [Score]
    \item Conciseness: [Score]
    \item Coherence: [Score]
\end{itemize}

If you encounter ambiguity in evaluating any dimension, prioritize adherence to the evaluation guidelines and provide the most accurate score possible based on the provided information. Do not include any additional comments or justifications in your response.
\vspace*{-.2cm}
\end{tcolorbox}
\vspace*{-.2cm}
\caption{Prompt for \texttt{GPT-o1} to evaluate summary quality.}
\end{figure*}


\end{document}
