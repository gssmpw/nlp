\begin{table*}[t]
\centering
\scalebox{0.7}{
\tabcolsep=5pt
\begin{threeparttable}
\begin{tabular}{c l c c c c c c c c c}
\toprule
Method & Model & R1 & R2 & RLsum & SacreBLEU & Meteor & BERTscore & CIDEr-D & VideoScore & FactVC\\
\hline
\multirow{12}*{\rotatebox[origin=c]{90}{Zero-shot Learning}}
& \texttt{Claude 3.5 Sonnet} & 27.71 & 5.59 & 24.14 & 3.14 & 17.53 & 82.57 & 1.32 & 1.91 & 50.11 \\
~ & Gemini 2.0 & 27.82 & 5.66 & 24.29 & 4.22 & 17.83 & 82.64 & 1.47 & 2.02 & 52.02 \\
~ & \texttt{GPT-o1} & 27.90 & 5.69 & 24.37 & 4.38 & 17.90 & 82.63 & 1.61 & 2.17 & 51.36 \\
\cdashline{2-11}
& \texttt{LLaMA-3.1$_{transcript}$} & 23.68 & 4.22 & 21.39 & 2.70 & 14.62 & 80.93 & 1.17 & 1.53 & 34.32 \\
~ & \texttt{LLaMA-3.1$_{OCR}$} & 24.02 & 4.37 & 21.42 & 2.63 & 14.59 & 80.33 & 1.19 & 1.50 & 34.06 \\
~ & \texttt{Qwen2-Audio} & 23.52 & 4.29 & 21.53 & 2.49 & 14.77 & 80.62 & 1.15 & 1.59 & 34.31 \\
\cdashline{2-11}
~ & \texttt{Video-LLaMA} & 20.18 & 3.19 & 21.24 & 1.76 & 13.73 & 81.31 & 1.08 & 1.63 & 32.25 \\
~ & \texttt{Video-ChatGPT}  & 20.36 & 3.52 & 21.43 & 1.79 & 14.01 & 81.35 & 1.11 & 1.63 & 33.21 \\
~ & \texttt{Video-LLaVA}  & 25.29 & 4.50 & 22.52 & 2.82 & 15.13 & 81.39 & 1.17 & 1.65 & 36.45 \\
~ & \texttt{LLaMA-VID}  & 25.31 & 4.77 & 22.53 & 2.88 & 15.27 & 81.32 & 1.14 & 1.64 & 36.39 \\
~ & \texttt{LLaVA-NeXT-Interleave}  & 25.41 & 4.82 & 22.68 & 2.92 & 15.25 & 81.40 & 1.18 & 1.73 & 40.12 \\
~ & \texttt{mPLUG-Owl3} & 25.57 & 4.82 & 22.84 & 2.99 & 15.33 & 81.39 & 1.21 & 1.77 & 42.07 \\
~ & \PlanSum$^\clubsuit$ & \cellcolor{green} \textbf{25.62}$^\dag$ & \cellcolor{green} \textbf{4.95}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{22.97}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{3.14}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{15.39}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{81.45}$^\ddag$ & \cellcolor{green} \textbf{1.27}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{1.86}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{47.37}$^\dag$$^\ddag$\\
\midrule
% \midrule
\multirow{10}*{\rotatebox[origin=c]{90}{QLoRA Fine-tuning}}
& \texttt{LLaMA-3.1$_{transcript}$} & 32.24 & 11.38 & 30.39 & 8.03 & 21.57 & 82.39 & 3.86 & 2.81 & 53.22 \\
~ & \texttt{LLaMA-3.1$_{OCR}$} & 33.01 & 12.11 & 30.52 & 8.04 & 21.55 & 82.41 & 3.92 & 2.77 & 53.19 \\
~ & \texttt{Qwen2-Audio} & 32.17 & 12.05 & 30.77 & 7.87 & 21.86 & 82.36 & 4.11 & 2.80 & 54.27 \\
% ~ & \texttt{PlanSum$_{TextOnly}$} & * & * & * & * & * & * & * & * & * \\
\cdashline{2-11}
~ & \texttt{Video-LLaMA} & 30.74 & 9.44 & 28.33 & 6.45 & 22.49 & 82.61 & 3.99 & 2.77 & 52.05 \\
~ & \texttt{Video-ChatGPT}  & 31.68 & 10.50 & 30.40 & 7.63 & 23.67 & 82.62 & 4.02 & 2.78 & 55.02 \\
~ & \texttt{Video-LLaVA}  & 33.16 & 12.64 & 30.37 & 8.17 & 23.92 & 82.81 & 4.26 & 2.83 & 59.13 \\
~ & \texttt{LLaMA-VID}  & 33.31 & 12.73 & 30.49 & 8.22 & 23.90 & 83.01 & 4.31 & 2.88 & 62.20 \\
~ & \texttt{LLaVA-NeXT-Interleave}  & 33.37 & 12.77 & 30.56 & 8.30 & 23.95 & 83.47 & 4.47 & 2.93 & 66.14 \\
~ & \texttt{mPLUG-Owl3} & 33.40 & 12.82 & 30.66 & 8.29 & 23.97 & 83.49 & 4.47 & 2.92 & 70.08 \\
~ & \PlanSum & \cellcolor{green} \textbf{33.52}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{13.01}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{31.10}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{8.33} & \cellcolor{green} \textbf{24.11}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{83.53}$^\dag$ & \cellcolor{green} \textbf{4.52} & \cellcolor{green} \textbf{3.11}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{73.11}$^\dag$$^\ddag$ \\
\midrule
% \midrule
\multirow{10}*{\rotatebox[origin=c]{90}{Full Fine-tuning}}
& \texttt{LLaMA-3.1$_{transcript}$} & 33.37 & 11.93 & 30.86 & 8.27 & 25.12 & 83.71 & 4.87 & 3.21 & 63.38 \\
~ & \texttt{LLaMA-3.1$_{OCR}$} & 34.02 & 12.42 & 31.72 & 8.51 & 15.11 & 84.09 & 4.89 & 3.32 & 65.84 \\
~ & \texttt{Qwen2-Audio} & 33.82 & 12.37 & 31.63 & 8.33 & 25.09 & 83.62 & 4.83 & 3.22 & 66.62 \\
% ~ & \texttt{PlanSum$_{TextOnly}$} & * & * & * & * & * & * & * & * & * \\
\cdashline{2-11}
~ & \texttt{Video-LLaMA} & 32.19 & 11.86 & 31.68 & 8.41 & 24.99 & 83.83 & 4.77 & 3.04 & 64.21 \\
~ & \texttt{Video-ChatGPT}  & 32.47 & 12.11 & 32.21 & 8.72 & 25.09 & 83.91 & 4.82 & 3.11 & 66.09 \\
~ & \texttt{Video-LLaVA}  & 33.28 & 13.39 & 32.78 & 9.10 & 25.42 & 83.97 & 4.87 & 3.13 & 66.12 \\
~ & \texttt{LLaMA-VID}  & 33.47 & 13.53 & 32.80 & 9.21 & 25.41 & 84.03 & 4.91 & 3.17 & 68.30 \\
~ & \texttt{LLaVA-NeXT-Interleave}  & 33.75 & 13.61 & 32.88 & 9.26 & 25.63 & 84.11 & 5.01 & 3.23 & 73.42 \\
~ & \texttt{mPLUG-Owl3} & 34.22 & 13.62 & 32.91 & 9.32 & 25.72 & 84.22 & 5.03 & 3.28 & 71.94 \\
~ & \PlanSum & \cellcolor{green} \textbf{34.53}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{13.74}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{33.25}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{9.56}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{25.88}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{84.37}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{5.15}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{3.33}$^\dag$$^\ddag$ & \cellcolor{green} \textbf{75.41}$^\dag$$^\ddag$ \\
\bottomrule
\end{tabular}
\end{threeparttable}
}
\caption{Model performance on VISTA dataset. In \PlanSum$^\clubsuit$, only the Plan Generation (\textsc{PG}) module is trained. Plans generated by the \textsc{PG} module on the test set serve as input to the Summary Generation (\textsc{SG}) module for zero-shot inference (no training is applied to the \textsc{SG} module).  Symbols~$^\dag$ and $^\ddag$ indicate that the performance of \PlanSum is significantly ($p<0.05$) different from  \texttt{LLaVA-NeXT-Interleave} (third best) and \texttt{mPLUG-Owl3} (second best), when using a paired t-test.}
\label{tab:model_performance}
\vspace{-15pt}
\end{table*}