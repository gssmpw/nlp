\section{Related Work}
\label{Related Work}


Because domain shift frequently occurs in real-life applications,  \textbf{DA} has attracted a lot of attention from the research communities and recorded a number of original and  effective methods \cite{DBLP:journals/csur/LuLHWC20, 7078994, pan2010survey, luo2022attention}. In this paper, we are mainly focused  on strengthening  \textbf{MMD}-based \textbf{DA} methods. We therefore overview the state-of-the-art \textbf{MMD-DA} techniques according to  the following two main research streams: 1) Shallow \textbf{MMD}-\textbf{DA}; 2) Deep \textbf{MMD}-\textbf{DA}, to better clarify the rationale of our proposed method. For comprehensiveness, we also discuss recent decision boundary optimization enforced \textbf{DA} to highlight the differences and the innovations of our proposed method.
% Despite the fact that the appealing performance of enormous \textbf{DA} techniques has been witnessed in lifting the traditional machine learning tasks, in this research, we focused mainly on strengthening the \textbf{MMD} measurement based \textbf{DA}. We therefore generalize the state-of-the-arts \textbf{MMD-DA} techniques into the following two main research streams: 1) Shallow \textbf{MMD}-\textbf{DA}; 2) Deep \textbf{MMD}-\textbf{DA}, to better clarify the rationale of our proposed method. For comprehensiveness, we also discuss recent decision boundary optimization enforced \textbf{DA} to clarify the differences and the innovations of our proposed method.







\subsection{Shallow \textbf{MMD}-\textbf{DA}}
\label{subsect: Shallow DA}


Shallow \textbf{MMD}-\textbf{DA} aims to decrease the  \textit{domain shift} by minimizing a statistic measurement in a shared feature space across domains, \textit{i.e.}, \textbf{MMD}, within the traditional optimization-based shallow models. Typical approaches \cite{luo2020discriminative, long2013transfer, wang2020transfer} hybridize the merits of convex optimization, compressing sensing and manifold learning, \textit{etc}, to reduce the cross-domain divergence of feature representations while explicitly minimizing the \textbf{MMD} enforced distribution measurements, \textit{i.e.}, marginal distribution \cite{4967588} \cite{pan2011domain}, conditional distribution \cite{long2013transfer} or hybridized distrubution \cite{wang2020transfer}. In the search of such a domain shift reduced functional learning, these approaches can be further distinguished based on whether they incorporate some form of data discriminativeness or not.




\subsubsection{Nondiscriminative distribution alignment (\textbf{NDA})} \textbf{NDA} strategies propose to align the marginal and conditional distributions across the source and target domains in reducing the \textbf{MMD} induced distance measurement to explicitly shrink the cross-domain divergence of marginal data distributions  \cite{pan2011domain}, as well as the  marginal and conditional data distributions \cite{long2013transfer}. Lately, \textbf{ARTL} \cite{DBLP:journals/tkde/LongWDPY14} further hybridizes the distribution alignment and label propagation within a single unified optimization framework,  enabling the \textbf{NDA} techniques to also leverage data geometric knowledge. To avoid geometric structure distortion which could be present in the original feature space, \textbf{MEDA} \cite{wang2018visual} specifically learns the Grassmann manifold. The main drawback of \textbf{NDA} is that it does not explore data discriminativeness as induced by labeled data in the source domain, making it more difficult to search for a proper cross-domain classifier. This observation inspires the research communities for further exploration of the discriminative functional learning.



\subsubsection{Discriminative distribution alignment (\textbf{DDA})} \textbf{DDA} approaches improve \textbf{NDA} methods by incorporating data discriminativeness for the task-oriented model design.  \textbf{ILS} \cite{herath2017learning} learns a discriminative latent space using Mahalanobis metric and makes use of Riemannian optimization strategy to match statistical properties across different domains. \textbf{OBTL} \cite{karbalayghareh2018optimal} proposes bayesian transfer learning-based domain adaptation, which explicitly discusses the relatedness across different sub-domains.  \textbf{SCA} \cite{DBLP:journals/pami/GhifaryBKZ17} achieves discriminativeness in optimizing the interplay of both between and within-class scatters. \textbf{DGA-DA} \cite{luo2020discriminative} introduces a novel \textit{repulsive force} term to describe the data discriminativeness, which also optimizes the underlying data manifold structure when performing label inference. 


As visualized in Fig.\ref{fig:2} (c),  \textbf{DDA} methods potentially reduce \textbf{Term.1} of Eq.(\ref{eq:bound}) in exploring the \textbf{MMD} induced data discriminativeness. However, such data discriminativeness is mainly focused on increasing the distances between the center points of different sub-domains and thus falls short in proactively handling the samples lying around the decision boundary. It therefore, is unable to generate the boundary aware domain adaptation as depicted in Fig.\ref{fig:2} (e) where data distributions are aligned across the two domains, inter-class distances are enlarged while intra-class samples are compacted. 





\subsection{Deep learning-based \textbf{DA}}
\label{subsect: Deep DA}


Boosted by the success of the paradigm of deep learning (\textbf{DL}),  shallow \textbf{MMD-DA} approaches have been extended to DL-based ones. They can be distinguished based on whether they incorporate adversarial learning.



% Benefiting from the popular deep learning (\textbf{DL}) techniques, the shallow \textbf{MMD-DA} approaches are improved as the deep learning-based version and can be distinguished based on their ability to incorporate adversarial learning strategies.


\subsubsection{MMD alignment based deep DA}

The principle of narrowing data distribution shift in shallow \textbf{MMD}-\textbf{DA} can be seamlessly embedded into deep models to formalize the \textit{\textbf{MMD} alignment-based deep \textbf{DA}}, thereby leveraging  highly discriminative deep features for further improved DA performance. Specifically, \textbf{DAN}\cite{long2015learning} reduces the marginal distribution divergence in incorporating the multi-kernel \textbf{MMD} loss on the fully connected layers of AlexNet. \textbf{JAN}\cite{DBLP:conf/icml/LongZ0J17} improves \textbf{DAN} by jointly decreasing the divergence of both the marginal and conditional distributions. \textbf{D-CORAL}\cite{sun2016deep} further introduces the second-order statistics into the AlexNet\cite{krizhevsky2012imagenet} framework for more effective \textbf{DA} strategy. 
 



\subsubsection{Adversarial learning-based MMD-DA}
\label{Adversarial loss-based DA}
These methods make use of \textbf{GAN} \cite{goodfellow2014generative} and propose to align data distributions across domains in making sample features indistinguishable \textit{w.r.t} the domain labels through an adversarial loss on a domain classifier \cite{ganin2016domain,tzeng2017adversarial,pei2018multi}. \textbf{DANN} \cite{ganin2016domain} and \textbf{ADDA}\cite{tzeng2017adversarial} learn a  domain-invariant feature subspace in reducing the marginal distribution divergence. \textbf{MADA} \cite{pei2018multi} also makes use of multiple domain discriminators, thereby aligning conditional data distributions. Lately, \textbf{DSN}\cite{bousmalis2016domain} achieves domain-invariant representations in explicitly separating the similarities and dissimilarities in the source and target domains. Using multi-source domains, \textbf{MADAN} \cite{zhao2019multi} explores the multi-domain knowledge to fulfill \textbf{DA} tasks. \textbf{CyCADA} \cite{pmlr-v80-hoffman18a} addresses the distribution divergence using a bi-directional \textbf{GAN} based training framework.  \textbf{ATM} \cite{li2020maximum} specifically introduces maximum density divergence  (\textbf{MDD}), an original distance loss, into the adversarial learning framework to quantify distribution divergence for effective \textbf{DA}.


The main advantage of these \textbf{DL}-based \textbf{DA} methods is that they  shrink the across domain divergence of data distributions in deep features  and search at the same time an optimal classifier through a single unified end-to-end learning framework. Therefore, the optimized model naturally enjoys the merits similar to those of \textbf{META} learning \cite{wei2021toalign}. This suggests that the model can be further reinforced by harnessing different tasks so as to receive the best candidate hyper-parameters. However, when compared with traditional shallow \textbf{MMD}-\textbf{DA} approaches, they generally suffer from the noisy '\textit{batch learning}' \cite{goodfellow2016deep} strategy in contrast to '\textit{global modeling}' \cite{belkin2003laplacian} based optimization. Moreover, these approaches do not explicitly take into account decision boundaries  and ignore in particular to properly handle samples lying around decision boundaries for the decision boundary aware domain adaptation.







\subsection{Decision boundary optimization-based \textbf{DA}}
\label{subsect: Decision boundary optimization-based DA}

In line with max-margin confident prediction principle of classical semi-supervised learning \cite{roller2004max}, \textbf{D}ecision \textbf{B}oundary (\textbf{DB}) optimization aims to place class boundaries in low-density regions,  and shows its effectiveness in a number of machine learning tasks, \textit{i.e.}, active learning \cite{cho2022mcdal}, knowledge distillation \cite{heo2019knowledge}, domain adaptation, \textit{etc}. 

In domain adaptation, Lu \textit{et al.} \cite{lu2018embarrassingly} adopts the principle of linear discriminant analysis \cite{goodfellow2016deep} to optimize the \textbf{DB}, and demonstrate its ability to solve the cross-domain tasks even without explicit divergence reduction. \textbf{Asm-DA}\cite{DBLP:conf/icml/SaitoUH17}  propose asymmetric tri-training based \textbf{DA}, whereof  two auxiliary classifiers  trained on the source domain are encouraged to be highly different on the target domain, and show that their strategy optimizes the samples lying around the decision boundary for the \textbf{DB} optimized model training. Utilizing adversarial learning strategies, \textbf{Asm-DA} is further  improved by \textbf{MCD} \cite{DBLP:conf/cvpr/SaitoWUH18}, which dynamically optimizes the maximum classifier discrepancy regularized \textbf{DA}. Subsequently, \textbf{GPDA} \cite{kim2019unsupervised} hybridizes \textbf{MCD}’s principle and the Bayesian optimization framework to further simplify the learning paradigm. \textbf{BCMD}\cite{li2021bi} argues that the \textbf{DB} optimization without conditional distribution alignment may result to a deterioration of the representation discriminability, making it category agnostic. As a result, \textbf{BCMD} designs \textit{'classifier determinacy disparity metric'} to generate the conditional distribution alignment enforced \textbf{DB-DA}.




Inspired by the aforementioned research, we propose a novel decision boundary optimization enforced mechanism, namely, \textit{decision boundary aware graph}, which is composed of the '\textit{Compacting graph}' and 'Separation graph'. Specifically, the compacting graph aims to shrink each subdomain's divergence by properly regularizing the samples lying around decision boundaries, while the separation graph propagates the discriminativeness to further optimize  the cross-domain samples lying around decision boundaries for comprehensive decision boundary optimization. Therefore, the strengthened \textbf{DB-MMD} measurement discriminatively approaches the cross-domain adaptation and can seamlessly hybridize with the next round of classification for the decision boundary-aware \textbf{DA}.