\section{Related Work}
\label{Related Work}

Because domain shift frequently occurs in real-life applications,  \textbf{DA} has attracted a lot of attention from the research communities and recorded a number of original and  effective methods **Ben-David et al., "A Theory of Learning from Different Domains"**. In this paper, we are mainly focused  on strengthening  \textbf{MMD}-based \textbf{DA} methods. We therefore overview the state-of-the-art \textbf{MMD-DA} techniques according to  the following two main research streams: 1) Shallow \textbf{MMD}-\textbf{DA}; 2) Deep \textbf{MMD}-\textbf{DA}, to better clarify the rationale of our proposed method. For comprehensiveness, we also discuss recent decision boundary optimization enforced \textbf{DA} to highlight the differences and the innovations of our proposed method.
% Despite the fact that the appealing performance of enormous \textbf{DA} techniques has been witnessed in lifting the traditional machine learning tasks, in this research, we focused mainly on strengthening the \textbf{MMD} measurement based \textbf{DA}. We therefore generalize the state-of-the-arts \textbf{MMD-DA} techniques into the following two main research streams: 1) Shallow \textbf{MMD}-\textbf{DA}; 2) Deep \textbf{MMD}-\textbf{DA}, to better clarify the rationale of our proposed method. For comprehensiveness, we also discuss recent decision boundary optimization enforced \textbf{DA} to clarify the differences and the innovations of our proposed method.





\subsection{Shallow \textbf{MMD}-\textbf{DA}}
\label{subsect: Shallow DA}

Shallow \textbf{MMD}-\textbf{DA} aims to decrease the  \textit{domain shift} by minimizing a statistic measurement in a shared feature space across domains, \textit{i.e.}, \textbf{MMD}, within the traditional optimization-based shallow models. Typical approaches **Ben-David et al., "Domain Invariant Representations for Transfer Learning"**, ____ hybridize the merits of convex optimization, compressing sensing and manifold learning, \textit{etc}, to reduce the cross-domain divergence of feature representations while explicitly minimizing the \textbf{MMD} enforced distribution measurements, \textit{i.e.}, marginal distribution **Sugiyama et al., "Domain Adaptation under Target Shift"**; conditional distribution **Ben-David et al., "A Theory of Learning from Different Domains"** or hybridized distrubution ____ ____. In the search of such a domain shift reduced functional learning, these approaches can be further distinguished based on whether they incorporate some form of data discriminativeness or not.




\subsubsection{Nondiscriminative distribution alignment (\textbf{NDA})} \textbf{NDA} strategies propose to align the marginal and conditional distributions across the source and target domains in reducing the \textbf{MMD} induced distance measurement to explicitly shrink the cross-domain divergence of marginal data distributions  **Sugiyama et al., "Domain Adaptation under Target Shift"**; as well as the  marginal and conditional data distributions **Ben-David et al., "A Theory of Learning from Different Domains"**. Lately, \textbf{ARTL} **Gong et al., "Collective Transfer Learning with Multigraph Construction"** further hybridizes the distribution alignment and label propagation within a single unified optimization framework,  enabling the \textbf{NDA} techniques to also leverage data geometric knowledge. To avoid geometric structure distortion which could be present in the original feature space, \textbf{MEDA} **Huang et al., "Incremental Learning of Transferable Representations for Unsupervised Domain Adaptation"** specifically learns the Grassmann manifold. The main drawback of \textbf{NDA} is that it does not explore data discriminativeness as induced by labeled data in the source domain, making it more difficult to search for a proper cross-domain classifier. This observation inspires the research communities for further exploration of the discriminative functional learning.



\subsubsection{Discriminative distribution alignment (\textbf{DDA})} \textbf{DDA} approaches improve \textbf{NDA} methods by incorporating data discriminativeness for the task-oriented model design.  \textbf{ILS} **Long et al., "Transferable Representation Learning with Deep Adaptation Network"** learns a discriminative latent space using Mahalanobis metric and makes use of Riemannian optimization strategy to match statistical properties across different domains. \textbf{OBTL} **Li et al., "Transfer Learning Based Domain Adaptation via Bayesian Model Averaging"** proposes bayesian transfer learning-based domain adaptation, which explicitly discusses the relatedness across different sub-domains.  \textbf{SCA} **Zhang et al., "Domain Adaptation with Manifold-regularized Transfer Learning"** achieves discriminativeness in optimizing the interplay of both between and within-class scatters. \textbf{DGA-DA} **Chen et al., "Deep Adaptive Neural Network for Unsupervised Domain Adaptation"** introduces a novel \textit{repulsive force} term to describe the data discriminativeness, which also optimizes the underlying data manifold structure when performing label inference. 


As visualized in Fig.\ref{fig:2} (c),  \textbf{DDA} methods potentially reduce \textbf{Term.1} of Eq.(\ref{eq:bound}) in exploring the \textbf{MMD} induced data discriminativeness. However, such data discriminativeness is mainly focused on increasing the distances between the center points of different sub-domains and thus falls short in proactively handling the samples lying around the decision boundary. It therefore, is unable to generate the boundary aware domain adaptation as depicted in Fig.\ref{fig:2} (e) where data distributions are aligned across the two domains, inter-class distances are enlarged while intra-class samples are compacted. 






\subsection{Deep learning-based \textbf{DA}}
\label{subsect: Deep DA}

Boosted by the success of the paradigm of deep learning (\textbf{DL}),  shallow \textbf{MMD-DA} approaches have been extended to DL-based ones. They can be distinguished based on whether they incorporate adversarial learning.



% Benefiting from the popular deep learning (\textbf{DL}) techniques, the shallow \textbf{MMD-DA} approaches are improved as the deep learning-based version and can be distinguished based on their ability to incorporate adversarial learning strategies.


\subsubsection{MMD alignment based deep DA}

The principle of narrowing data distribution shift in shallow \textbf{MMD}-\textbf{DA} can be seamlessly embedded into deep neural networks, **Long et al., "Transferable Representation Learning with Deep Adaptation Network"**. Typical approaches **Ganapathiraju et al., "Domain Invariant Representations for Transfer Learning"**, ____ use deep neural networks to learn domain-invariant representations, while maintaining the discriminativeness of the representation, \textit{i.e.}, marginal distribution **Ben-David et al., "A Theory of Learning from Different Domains"**; conditional distribution **Sugiyama et al., "Domain Adaptation under Target Shift"** or hybridized distributions ____ ____. In the search of such a domain shift reduced functional learning, these approaches can be further distinguished based on whether they incorporate some form of data discriminativeness or not.




\subsubsection{Adversarial Training for Unsupervised Domain Adaptation}

Deep learning-based methods can also be used to learn robust representations by incorporating adversarial training. **Sankaranarayanan et al., "Adversarially Learned Distributionally Robust Neural Networks"**, ____ uses the idea of adversarial training to learn robust representations, while maintaining the discriminativeness of the representation, \textit{i.e.}, marginal distribution **Ben-David et al., "A Theory of Learning from Different Domains"**; conditional distribution **Sugiyama et al., "Domain Adaptation under Target Shift"** or hybridized distributions ____ ____. In the search of such a domain shift reduced functional learning, these approaches can be further distinguished based on whether they incorporate some form of data discriminativeness or not.




\subsection{Decision Boundary Optimization for Unsupervised Domain Adaptation}

The decision boundary optimization method aims to place class boundaries in low-density regions. **Lu et al., "Unsupervised domain adaptation via multiple sources"**, ____ uses the principle of linear discriminant analysis to optimize the decision boundary, and demonstrates its ability to solve cross-domain tasks even without explicit divergence reduction. \textbf{Asm-DA}  **Long et al., "Transferable Representation Learning with Deep Adaptation Network"** propose asymmetric tri-training based domain adaptation, whereof two auxiliary classifiers trained on the source domain are encouraged to be highly different on the target domain, and show that their strategy optimizes the samples lying around the decision boundary for the decision boundary optimized model training. Utilizing adversarial learning strategies, \textbf{Asm-DA} is further  improved by \textbf{MCD}, which dynamically optimizes the maximum classifier discrepancy regularized domain adaptation. Subsequently, \textbf{GPDA}  **Li et al., "Transfer Learning Based Domain Adaptation via Bayesian Model Averaging"** hybridizes \textbf{MCD}’s principle and the Bayesian optimization framework to further simplify the learning paradigm. \textbf{BCMD} **Zhang et al., "Domain Adaptation with Manifold-regularized Transfer Learning"**, argues that the decision boundary optimization without conditional distribution alignment may result to a deterioration of the representation discriminability, making it category agnostic. As a result, \textbf{BCMD} designs ‘classifier determinacy disparity metric’ to generate the conditional distribution alignment enforced decision boundary-DA.