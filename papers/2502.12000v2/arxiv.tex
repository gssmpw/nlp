\documentclass[a4paper,USenglish,cleveref, autoref, thm-restate]{lipics-v2021}
\usepackage{tcolorbox}
\usepackage{tikz}
\usepackage{mathtools}

% Math macros
\newcommand{\LPF}{\mathsf{LPF}}
\newcommand{\chain}[3]{\mathsf{chain}_{#1}(#2,#3)}
\newcommand{\Occurrences}[2]{\mathsf{Occ}_{#1}(#2)}
\newcommand{\LCP}{\mathsf{LCP}}
\newcommand{\IPM}{\mathsf{IPM}}
\newcommand{\ilast}{b_C}
\newcommand{\Alast}{B}
\newcommand{\LCS}{\mathsf{LCS}}
\newcommand{\LCA}{\mathsf{LCA}}
\newcommand{\LPFpos}{\mathsf{LPFpos}}
\newcommand{\Otild}{\tilde{O}}
\newcommand{\per}{\mathsf{per}}
\newcommand{\Insert}{\mathsf{Insert}}
\newcommand{\Link}{\mathsf{Link}}
\newcommand{\Delete}{\mathsf{Delete}}
\newcommand{\MoveInterval}{\mathsf{MoveInterval}}
\newcommand{\findIAncestor}{\mathsf{LevelAncestor}}
\newcommand{\SeqGenQuery}{\mathsf{SeqGenQuery}}
\newcommand{\SubPM}[4]{\mathsf{SubPM}(#1,#2,#3,#4)}
\newcommand{\UpdateTreesByRange}{\mathsf{UpdateTreeByRange}}
\newcommand{\GetHeight}{\mathsf{GetDepth}}
\newcommand{\SelectPhrase}{\mathsf{SelectPhrase}}
\newcommand{\LA}{\mathsf{LevelAncestor}}
\newcommand{\ChargeTwo}{\mathsf{Charge}_2}
\newcommand{\ContainingPhrase}{\mathsf{ContainingPhrase}}
\newcommand{\LZLength}{\mathsf{LZLength}}
\newcommand{\LZss}{\text{LZ77}}
\newcommand{\depth}{\mathsf{depth}}

\newcommand{\runstart}{\mathsf{runStart}}
\newcommand{\runend}{\mathsf{runEnd}}
\newcommand{\firstocc}{\mathsf{firstOcc}}
\newcommand{\clustoccurence}{\mathsf{clustOccurence}}
\newcommand{\lastocc}{\mathsf{lastOcc}}



\newcommand{\OV}{\mathsf{OV}}
\newcommand{\eps}{\varepsilon}


\DeclareMathOperator*{\argmax}{arg\,max}

\newtheorem{fact}[theorem]{Fact}
\newcommand{\T}{\mathcal{T}}


\newcommand{\lpos}{\mathsf{L_z}}
\newcommand{\rpos}{\mathsf{R_z}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% Organization macros
\newcommand{\para}[1]{\subparagraph*{#1}}


\newenvironment{problem}[1]{%
  \refstepcounter{problemctr}% increment and make it ref-steppable
  \begin{tcolorbox}[%
    colback=blue!5,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=Problem~\theproblemctr: #1,
    boxrule=0.8mm,
    width=\textwidth
  ]%
}{%
  \end{tcolorbox}%
}

\newcounter{problemctr}
\renewcommand{\theproblemctr}{\arabic{problemctr}}

\crefname{problemctr}{Problem}{Problems}

\crefname{claim}{Claim}{Claims}

\bibliographystyle{plainurl}


\title{\~{O}ptimal Algorithm for Fully Dynamic LZ77  } 

\author{Itai Boneh}{Reichman University and University of Haifa, Israel}{itai.bone@biu.ac.il}{https://orcid.org/0009-0007-8895-4069}{supported by Israel Science Foundation grant 810/21.}

\author{Shay Golan}{Reichman University and University of Haifa, Israel \and \url{https://sites.google.com/view/shaygolan}}{golansh1@biu.ac.il}{https://orcid.org/0000-0001-8357-2802}{supported by Israel Science Foundation grant 810/21.}

\author{Matan Kraus}{Bar Ilan Univesity, Israel}{matan3@gmail.com}{https://orcid.org/0000-0002-2989-1113}{supported by the ISF grant no. 1926/19, by the BSF grant 2018364, and by the ERC grant MPM under the EU's Horizon 2020 Research and Innovation Programme (grant no. 683064).}
 
\authorrunning{Boneh, Golan and Kraus}

\ccsdesc[500]{Theory of computation~Design and analysis of algorithms}


\Copyright{Jane Open Access and Joan R. Public}

\keywords{Text Algorithms, Lempel-Ziv 77, Dynamic Algorithms}

\category{} %optional, e.g. invited paper

\relatedversion{}
\nolinenumbers

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hideLIPIcs

\begin{document}

\maketitle
\begin{abstract}
The Lempel-Ziv 77 (LZ77) factorization is a fundamental compression scheme widely used in text processing and data compression.
In this work, we study the time complexity of maintaining the LZ77 factorization of a dynamic string.
We present an algorithm that dynamically maintains the $\LZss$ factorization of a string $S$ that undergoes edit operations (character substitutions, insertions, and deletions).
The data structure can be built in $\Otild(n)$ time on an initial string $S$ of length $n$, and updates are supported in $\Otild(n^{2/3})$ time, where $n$ is the current length of $S$.
We also show that there is no algorithm with polynomially faster update time unless the Strong Exponential Time Hypothesis fails.
Our lower bound holds even for the restricted settings in which only substitution operations are allowed, and only the length of the $\LZss$ factorization is maintained.
\end{abstract}



\section{Introduction}

Lempel-Ziv 77 ($\LZss$)~\cite{LZ77} is an algorithm that partitions a string $S[1..n]$ into disjoint substrings, called \textit{phrases}, as follows: 
The first phrase consists of the first character, $S[1]$. 
The starting index of the $i$th phrase is immediately after the ending index of the $(i-1)$th phrase. 
The phrase itself is the longest substring starting in this index that is not a leftmost-occurrence in $S$ (or, if no such substring exists, simply a substring of length $1$).

More formally, if the $(i-1)$th phrase is $S[a..b-1]$, then the $ i $th phrase is $S[b..b+r]$, where $ r $ is defined as
$r = \max \{ x \mid S[b..b+x] \text{ occurs in } S[1..b+x-1] \} \cup \{0\}$.

The string $S$ can be recovered from the $\LZss$ factorization of $S$ (provided each phrase is stored with some constant-size additional information) and the $\LZss$ factorization is typically compact, meaning it consists of relatively few phrases when $ S $ contains many repetitions. 
This property makes $\LZss$ an effective compression scheme. 

In practice, $\LZss$ is one of the most widely used compression algorithms, forming the foundation of 57 out of 210 compressors listed in the Large Text Compression Benchmark \cite{ltcb}. 
Its influence spans a diverse range of file formats, such as PNG, PDF, and ZIP, highlighting its versatility and efficiency. 
Moreover, LZ77 plays a crucial role in modern web infrastructure, being embedded in virtually all contemporary web browsers and servers \cite{AFFKOS19}.


The $\LZss$ factorization has recently garnered significant attention from the theory community, leading to the development of numerous algorithms aimed at optimizing various aspects of its computation. 
For instance, in recent years, breakthroughs have been made in computing the $\LZss$ factorization of a text in sub-linear time~\cite{kempa2024lempel,ellert2023sublinear}, as well as in developing quantum algorithms for its computation~\cite{gibney2024near}.

In this work, we investigate the maintenance of the LZ77 factorization in dynamic settings. 
Specifically, we consider scenarios in which the input string $S$ undergoes insertions, deletions, and symbol substitutions. 
Our objective is to maintain a data structure alongside $S$ that allows access to its current LZ77 factorization at any point during the sequence of updates.
The interface we provide for accessing the LZ77 factorization is exceptionally versatile and well-suited for dynamic applications: 
given an index $i$, the data structure can either report the LZ77 phrase that contains $i$ in $S$ or return the 
$i$th phrase in the factorization of $S$.
All access queries are supported in poly-logarithmic time.
For some previously studied applications (see~\cite{BCR24}, discussed below), a more limited notion of dynamic LZ77 maintenance suffices: after each update to $S$, only the current size of its LZ77 factorization is reported. 
Our data structure supports this functionality as well.

The problem of dynamic LZ77 maintenance has recently been considered by Bannai, Charalampopoulos, and Radoszewski~\cite{BCR24}. In their work, they focus on a limited semi-dynamic setting where only symbol insertions at the end of the string and deletions from the beginning are permitted. 
In these settings, \cite{BCR24} provide an algorithm with an amortized running time of $\Otild(\sqrt{n})$\footnote{Throughout the paper, we use the $\Otild$ notation to ignore multiplicative polylogarithmic factors of $n$, i.e., $\Otild(f(*)) = O(f(*) \cdot \log^c n)$ for some constant $c$, where $n$ is the current length of the string.}, which reports the size of $LZ77(S)$ after every update.  

Bannai, Charalampopoulos, and Radoszewski~\cite{BCR24} motivate their study with the following problem: given a string $S$ that is to be compressed, find the rotation $S'$ of $S$ (i.e., a string $S' = B \cdot A$ such that $ S = A \cdot B$) that minimizes the size of the LZ77 factorization, $\LZss(S')$. A naive solution would involve computing the LZ77 factorization of every rotation of $S$, resulting in an $O(n^2)$ running time where $n = |S|$. 
However, if one has access to a semi-dynamic data structure with an update time of $O(u)$ , the problem can be solved in $O(n \cdot u)$ time. 
This is achieved by initializing an empty data structure, inserting the symbols of $S$ one by one, and then performing $|S|$ rotations, each rotation consisting of a deletion of the first symbol and an insertion of that symbol at the end. 
Since all rotations of $S$ are generated in this manner, the LZ77 sizes of all rotations are reported in $O(n \cdot u)$ time.

Although the semi-dynamic setting studied in \cite{BCR24} is well-motivated, a more natural and general approach to developing dynamic string algorithms is to consider the fully dynamic model, where the string undergoes insertions, deletions, and substitutions. Recent work on dynamic strings in the fully dynamic model (and in some cases, even stronger models) includes Substring equality queries \cite{gawrychowski2018optimal}, Longest Common Factor Maintenance \cite{amir2020dynamic,charalampopoulos2020dynamic}, Approximate Pattern Matching \cite{charalampopoulos2020faster}, maintenance of all periodic substrings \cite{amir2019repetition}, maintenance of Suffix Arrays and Inverted Suffix Arrays \cite{kempa2022dynamic}, Edit Distance computation \cite{charalampopoulos2020alignment}, and Dynamic Time Warping Computation \cite{bringmann2024dynamic}.
Building on the ongoing progress in developing fully dynamic algorithms for classical string problems and drawing inspiration from \cite{BCR24}’s exploration of LZ77 in dynamic settings, we investigate the complexity of maintaining the LZ77 factorization of a dynamic string.

\subsection{Our Contribution}
We present a fully dynamic data structure that supports access to the LZ77 factorization of a dynamic string $S$.
We present the following interface for querying the LZ77 factorization of a string $S$.
\begin{enumerate}
    \item $\SelectPhrase_S(i)$: given an index $i$, report the $i$'th LZ77 phrase $S[a..b]$ in the factorization of $S$ represented by $a$ and $b$.
    \item $\ContainingPhrase_S(i)$: given an index $i$, report the LZ77 phrase $S[a..b]$ in the factorization of $S$ such that $i\in [a..b]$.
    \item $\LZLength_S(i)$ get the length of the LZ77 factorization of $S[1..i]$.
\end{enumerate}
We use the name '\textit{LZ77 queries}' to collectively refer to $\SelectPhrase$, $\ContainingPhrase$, and $\LZLength$.
The paper is dedicated to providing the following data structure for dynamic LZ77, which we formalize as the follows.

\begin{problem}{Fully Dynamic \LZss}\label{prb:ub}
\textbf{Preprocess$(S)$:} preprocess a string $S$.
\medskip
\\
\textbf{Query:} an $\LZss$ query on $S$.
\medskip
\\
\textbf{Updates:} Apply a given edit operation on $S$.
\end{problem}



\begin{theorem}\label{thm:lz77}
    There is a data structure solving \cref{prb:ub} with $\Otild(n)$ preprocessing time, $\Otild(1)$ time per query and $\Otild(n^{2/3})$ time per update, where $n$ is the current length of $S$.
\end{theorem}

In \cref{sec:lb}, we prove that our data structure is optimal up to subpolynomial factors.  
In particular, we prove that even if only substitutions are allowed and the data structure only needs to report $|\LZss(S)|$ after each update, achieving this with polynomial preprocessing and an update time of $O(n^{2/3-\eps})$ is impossible, assuming the Strong Exponential Time Hypothesis (SETH)~\cite{IP01,IPZ01}.

\begin{theorem}\label{thm:intro-lb}
For every $c,\eps>0$, there is no data structure that solves \cref{prb:ub} with $O(n^c)$ preprocessing time, $\Otild(n^{2/3-\eps})$ query time and $O(n^{2/3-\eps})$ update time, where $n$ is the current length of $S$, unless SETH is false.
\end{theorem}



\section{Preliminaries}
\para{Integers Notations.}
We use range notation to denote consecutive ranges of integers.
For integers $i,j$ denote $[i..j]=\{i,i+1,\ldots j\}$ and $[i..j)=[i..j-1]$. 
Denote $[i]=[1..i]$ (if $j<i$, $[i..j]=\emptyset$).
We sometimes deal with arithmetic progressions of integers.
We represent a set $A= \{a,a+p,a+2p,\ldots ,b=a+p\cdot (|A|-1) \}$ by the triplet $(a,b,d)$.
We call $d$ the difference of the arithmetic progression $(a,b,d)$.

\para{Strings.}
For a string $S=S[1]S[2]S[3]\ldots S[n]$, we denote a substring $S[i]S[i+1]\ldots S[j]$ as $S[i..j]$.
We say that $S[i..j]$ is a substring that starts at index $i$ and ends at index $j$.
We call a substring that starts at index $1$ a prefix of $S$, and a substring that ends at index $n$ a suffix of $S$.
We denote the reversed string of $S$ as $S^R=s[n],s[n-1],\ldots s[1]$.
Let $\odot$ denote
concatenation (in increasing order of the running index).

For a pattern $P$, we denote $\Occurrences{S}{P}=\{i\mid S[i..i+|P|-1]=P\}$.
We call an index $i\in \Occurrences{S}{P}$ an occurrence of $P$ in $i$, and say that $P$ occurs in $S$ if $\Occurrences{S}{P} \neq \emptyset$.

For two strings $S$ and $T$, the length of longest common prefix of $S$ and $T$ is denoted by $\LCP(S,T)=\max \{\ell \mid S[1..\ell] = T[1..\ell]\}$.
The length of the longest common suffix of $S$ and $T$ is denoted by $\LCS(S,T)=\LCP(S^R,T^R)$.

For a string $S$ and two indices $i,j \in [|S|]$, we denote $\LCP_S(i,j) = \LCP(S[i..n],S[j..n])$ and $\LCS_S(i,j) = \LCS(S[1..i],S[1..j])$.

We say that an integer $p$ is a period of $S$ if $S[i]=S[i+p]$ for every $i\in [1..n-p]$.
We say that $p$ is the period of $S$, denoted as $\per(S)$ if it is the minimal period of $S$.
We say that $S$ is periodic if $\per(S) \le \frac{n}{2}$.
Otherwise, $S$ is aperiodic.

The following is a well-known fact regarding strings and periodicity.
It follows directly from the periodicity lemma (\cite{FiWi65}).
\begin{fact}\label{fact:aperfarapart}
    Let $T[1..n]$ be a string and $P[1..m]$ be a string with period $\per(P)=p$.
    Let $i<j$ be two consecutive occurrences of $P$ in $T$.
    It must holds that either $j-i=p$, or $j-i \ge \frac{m}{2}$.
    
    In particular, if $P$ is aperiodic then $j-i \ge \frac{m}{2}$
\end{fact}

\para{trees.}
For a node $v$ in a tree $T$, we denote as $\depth(v,T)$ the number of edges in the unique path from $v$ to the root of $T$.

Given a node $v$ and an integer $k$, the $k$-th ancestor of $v$, denoted $\LA_T(v, k)$, is the ancestor of $v$ with depth $\depth(v) - k$.

\para{Edge-Labled Trees in Tries.}
An edge-labeled tree (over alphabet $\Sigma$) is a tree with root $r$ such that every edge is assigned a label in $\sigma$.
Each node $v$ in an edge-labeled tree is associated with a string $L(v)$, also called the label of $v$.
The root $r$ is associated with the empty string $L(r) = \varepsilon$, and every non-root node $v$ has $L(v) = L(u) \cdot \sigma$ such that $u$ is the parent of $v$ and $\sigma$ is the label of the edge $(u,v)$.
Notice that for every two nodes $u,v$ in an edged-labeled tree with lowest common ancestor $z$, it holds that $L(z) = L(u)[1..\LCP(L(u),L(v)]]$.

A Trie is an edge-labeled tree that is derived from a set of strings.
We define the Trie of a set $\mathcal{S}$ of strings revursievely.
The Trie of an empty set $\mathcal{S}= \emptyset$ is a single root vertex.
Let $T'$ be the Trie of a set $\mathcal{S}$ of $t\ge 0$ strings.
For a string $S$, let $A= \argmax_{S \in \mathcal S }\LCP((S,A))$.
Let $v$ be the node in $T'$ with $L(v) = S[1,\LCP(S,A)]$.
The Trie of $\mathcal{S} \cup \{ S\}$ is obtained by adding a path from $v$ of length $|S| - \LCP(S,A)$ with the labels $S[\LCP(S,A)+1..|S|]$ on the edges.
Notice that the label of the node at the end of this path is $S$.
Also notice that if $\LCP(S,A) = |S|$, nothing is added to $T'$.
In words, the Trie $T$ of $\mathcal S$ is the minimal edge labeled tree such that for every string $S$ in $\mathcal S$ there is a vertex $v\in T$ with $L(v) = S$.

For two nodes $u,v$, denote $\LCA_T(u,v)$ as the longest common ancestor of $u$ and $v$ in $T$.
It holds that for $S,S'\in\mathcal{S}$, $\LCP(S,S')=\depth(\LCA_T(S,S'),T)$.

\para{LZ77.} The Lempel-Ziv \cite{LZ77} compression is an algorithm that given a string $S$, outputs a sequence of  \emph{phrases}, denoted $LZ77(S)$.
The algorithm starts by initializing a left to right scan with $i=1$ and an empty sequence $P$ of phrases.

If $i = n+1$, the algorithm  halts and return the sequence $P$.
Otherwise, the algorithm adds a new phrase as follows. 
If $S[i]$ is a new character, i.e. $\Occurrences{S[1..i-1]}{S[i]}= \emptyset$, add $S[i]$ as a phrase to LZ77 and continue with $i \leftarrow i+1$.
Otherwise, let $j$ be an index in $[i]$ that maximizes $\LCP(i,j)$. The algorithm adds $(j,\LCP(i,j))$ as a phrase to the end of $P$ and continues with $i \leftarrow i+\LCP(j,i)$.

We denote the final sequence of phrases obtained from applying the above procedure on $S$ as $\LZss(S)$.

\subsection{Dynamic strings}
\para{Edit Operations.}
In this paper, we develop and analyze algorithm over a dynamic string $S$ over alphabet $\Sigma$ that undergoes \textit{edit operations}.
An edit operation on $S[1..n]$ can be one of three options listed below, each resulting in a new string $S'$.
\begin{enumerate}
    \item \textit{Substitution:} Represented as a tuple $(i,\sigma)\in [n]\times \Sigma$. 
    Results in $S'=S[1,i-1] \cdot \sigma \cdot S[i+1,n]$.
    \item \textit{Deletion:} Represented as an index $i\in [n]$. 
    Results in $S'=S[1,i-1]  \cdot S[i+1,n]$.
    \item \textit{Insertion:} Represented as a tuple $(i,\sigma)\in [n+1]\times \Sigma$. 
    Results in $S'=S[1,i-1] \cdot \sigma \cdot S[i,n]$.
\end{enumerate}
A dynamic algorithm receives as inputs a sequence of operations, each represented by the type of the edit operation (substitution, deletion, or insertion) and a tuple representing the operation.

When describing our algorithms, we abstain from providing a direct implementation for the substitution update, as it can be simulated using a deletion update and an insertion update.

\para{Dynamic Indices.} 
Even though edit operations are very local, a single operation may 'shift' a large number of indices.
For instance, when applying a deletion at index $i$, every index $j\ge i$ in $S$ becomes the index $j-1$ in $S$.
This phenomena is problematic form any data structure that stores indices from the text, as every index $j>i$ stored implicitly in the data structure should be modified.
These 'shifts' also introduce clutter when making statement about strings in dynamic indices.
For instance, One would like to sa

Therefore, we represent the indices of $S$ not as explicit number but as nodes in a dynamic tree.
Specifically, we store a balanced search tree $T_I$ over $n$ nodes, such that every node $v\in T_I$ stores the size of the sub-tree rooted at $v$ as auxiliary information.
The $i$'th node in the in-order traversal of $T_I$ \textit{is} to the $i$'th index of $S$.
Note that due to the auxiliary information, one can find the $i$'th node in $O(\log n)$ time given $i$, and also, given a node - one can find the index $i$ corresponding to it in $O(\log n)$ time.
When an index is deleted (resp. inserted) from $S$, we delete (resp. insert) the corresponding index from $T_I$, which automatically shift all other indices.

With this framework, when discussing an index $i$ of a dynamic string, we actually refer to the node in $T_I$ representing $i$ rather than the actual numeric value $i$.
This introduces an additional multiplicative factor of $O(\log n)$ whenever our algorithm accesses an index of the dynamic text.

To avoid clutter, we omit this dynamic index implementation when describing algorithms and instead assume that indices are stored explicitly.


\subsection{Stringology Tools}

First, we make use of a data structure with the following functionality.

\begin{lemma}[\cite{kempa2022dynamic},Section 8]\label{lem:dympillar}
There is a data structure for maintaining a dynamic string $S$ that undergoes edit operations and supports the following queries in $\Otild(1)$ time:
\begin{enumerate}
    \item $\LCP$ query: Given two indices $i,j$, report $\LCP_S(i,j)$.
    \item $\LCS$ query: Given two indices $i,j$, report $\LCS_S(i,j)$.
    \item $\IPM_S(P,T)$ query: Given a pattern $P=S[i_P..j_P]$ and a text $T=S[i_T..j_T]$, both substrings of $S$ represented using their endpoints such that $|T| \le 2|P|$, returns $\Occurrences{T}{P}$ represented as at most 2 arithmetic progressions with difference $ \per(P)$.
\end{enumerate}
The update time of the data structure is $\Otild(1)$
\end{lemma}

It is well known that the following can be obtained from \cref{lem:dympillar} by applying a 'standard trick'.
\begin{lemma}\label{lem:ipmalllengths}
There is a data structure for maintaining a dynamic string $S$ that undergoes edit operations and supports the following query:

Given a pattern $P=S[i_P..j_P]$ and a text $T=S[i_T..j_T]$, both substrings of $S$ represented using their endpoints, returns $\Occurrences{T}{P}$ represented as $O(\frac{|T|}{|P|})$ arithmetic progressions with difference $ \per(P)$.
The query time is $\Otild(\frac{|T|}{|P|})$ and the update time of the data structure is $\Otild(1)$.
\end{lemma}
\begin{proof}
    We simply maintain the data structure of \cref{lem:dympillar}.
    Upon a query for $T$ and $P$, for every integer $i$ we define $T_i = T[i_T + i\cdot |P|.. \min(i_T +2|P|+i|P| ,j_T)]$.
    It can be easily verified that every occurrence of $P$ in $T$ is contained in $T_i$ for some $i\in [0..\frac{|T|}{|P|}]$.
    We query for $\IPM_S(P,T_i)$ for every $i\in [\frac{2|T|}{|P|}]$ and return all arithmetic progression returned from all queries.
    The number of reported arithmetic progression and the running time are direct implications of \cref{lem:dympillar}
\end{proof}


We also use a data structure for the Dynamic Substring Pattern Matching queries.

\begin{tcolorbox}[title={$\SubPM{i_T}{j_T}{i_P}{j_P}$}]
\textbf{Input:} Two substrings $T=S[i_T..j_T]$ and $P=S[i_P..j_P]$ of a string $S$ given as their starting and ending indices. \\
\textbf{Output:} TRUE if there is an occurrence of $P$ in $T$, and FALSE otherwise.
\end{tcolorbox}

In \cref{sec:DynSPM}, we prove the following lemma.

\begin{restatable}{lemma}{subPMDS}
\label{lem:subPMDS}
    There is a data structure that maintains a dynamic string that undergoes edit operation and can answer a query $\SubPM{i_T}{j_T}{i_P}{j_P}$ in $\Otild(1)$ time.
    The update time of the data structure is $\Otild(1)$.
\end{restatable}

We further exploit \cref{lem:subPMDS} to obtain the following data structure.
\begin{lemma}\label{lem:first_occ}
    There is a data structure that maintains a dynamic string that undergoes edit operation and given four indices $i_T,j_T,i_P,j_P$, can answer the following  queries:
    \begin{enumerate}
        \item Find $\min(\Occurrences{S[i_T..j_T]}{S[i_P..j_P]})$.
        \item Find $\max(\Occurrences{S[i_T..j_T]}{S[i_P..j_P]})$.
    \end{enumerate}
    The queries cost $\Otild(1)$ time per query.
    The update time of the data structure is $\Otild(1)$.
\end{lemma}
\begin{proof}
    We describe an algorithm for finding the first occurrence of $P=[i_P..j_P]$ in $T=[i_T..j_T]$, an algorithm for finding the last occurrence can be constructed in a similar way.
    We use the data structure of \cref{lem:subPMDS}.
    First we check whether $P$ occurs in $T$.
    If not, return null.
    Otherwise, binary search for the minimal index $k\le j_T$ such that $T[i_T..k]$ contains an occurrence of $P$ and return $k-|P|+1$.
\end{proof}



\section{LPF and LPF-Trees}

In this section, we repeat the notations and describe the data structure introduced in ~\cite{BCR24} that are useful in our fully dynamic algorithm. 

Let $S[1..n]$ be a string and let $i\in [n]$.
The \emph{Longest Previous Factor} of $i$ in $S$ (denoted as $\LPF_S(i)$) is the length of the maximal longest prefix of $S[i..n]$ that occurs strictly before $i$.
Formally,
\[
\LPF_S(i) = \max_{j<i}(\LCP_S(i,j)).
\]


Similarly, let $\LPF'_S(i)=\max(\LPF_S(i),1)$.
The following observation connects the concepts of $\LPF$ and $\LZss$.

Additionally, let $\LPFpos_S(i)$ be the rightmost position that is a witness for $\LPF_S(i)$.

Formally, 
\[
\LPFpos_S(i) = \max \{ j < i \mid \LCP_S(i,j)=\LPF(i)\}. 
\]
We omit the subscript $S$ when it is clear from context.

In \cite{BCR24}, the authors prove the following.

\begin{lemma}[Proof of Lemma 19, \cite{BCR24}]\label{lem:19}
    For every text $S$ and $i\in [|S|-1]$, it holds that $i+\LPF'_S(i)\le i+1+\LPF'_S(i+1)$
\end{lemma}

We prove the next useful lemma regarding $\LPF$ and $\LPFpos$.
\begin{lemma}\label{lem:dynamicLPF}
    There is a data structure that maintains a dynamic string that undergoes edit operations and that given an index $i$, returns $\LPF'(i)$ in $\Otild (1)$ time.
    The update time of the data structure is $\Otild(1)$.
\end{lemma}


\begin{proof}
    We use the data structure of \cref{lem:first_occ}.
    We demonstrate the method for calculating $\LPF_S(i)$, noting that the query $\LPF'_S(i)$ simply provides $\max(\LPF_S(i),1)$. 
    Observe that $\LPF_S(i) = \max\{\ell \mid \Occurrences{S[1..i+\ell-1)}{S[i..i+\ell)}\neq \emptyset \}$.
    In particular, for every $x \in [1..n]$ we have that $\Occurrences{S[1..i+x-1)}{S[i..i+x)}\neq \emptyset$ if and only if $x \le \LPF_S(i)$.
    the algorithm determines the correct value of $\LPF_S(i)$ by performing a binary search on the range $[0..n-1]$ (this search is valid due to \cref{lem:19}), checking if the current value $x$ is larger or smaller than $\LPF_S(i)$ by querying the data structure of \cref{lem:subPMDS}.
    Each step in the binary search is executed in $\Otild(1)$ time, so the overall time complexity of computing $\LPF_S(i)$ is $\Otild(1)$.
\end{proof}

\para{The $\LPF$-tree data structure.}
We adapt the framework of ~\cite{BCR24} to the fully dynamic settings.
Namely, they define $\LPF$-tree and show how to maintain it in the semi-dynamic settings.
We will show how to maintain the same structure in the fully dynamic settings.
The $\LPF$-tree of a string $S$ with length $n$, denoted as $\T_s$ is a tree $\T_s=(V,E)$ with nodes $V=[n+1]$ representing the indices of $S$ and an additional node $n+1$, and $E=\{ (i,j)\mid i\in [n], j= i+\LPF'_S(i) \}.$\footnote{In ~\cite{BCR24}, a label is also defined for every edge in the tree. 
Our algorithm does not use these labels, hence we omit them.}
This defines a tree with the node $n+1$ as the root. 
For a node $i\in \T_S$, we denote $\depth_S(i)= \depth(i,\T_S)$.

~\cite{BCR24} observed the following connection between $\T_S$ and $\LZss(S)$.
\begin{observation}\label{obs:lpflzconnection}[\cite{BCR24}, Observation 18, rephrased]
Let $\pi$ be the $1$-to-$|S|$ path in $\T_S$. Then, $\depth_S(1)=|\LZss(S)|$. Additionally, for $k \le |\pi|$, if the $k$th edge on $\pi$ is $(i,i+\LPF'_S(i))$, then the $k$th phrase of $\LZss(S)$ is equal to $S[i..i+\LPF'_S(i)]$.
\end{observation}



\subsection{Updating an LPF-tree}
In this section, we introduce the data structure representing the $\LPF$-tree.
The data structure maintains a rooted tree $U=(V,E)$ that represents $\mathcal{T}_S$.
Specifically, when an update modifies $S$ to be $S'$ the algorithm modifies $U=\T_S$ to obtain $U=\T_{S'}$.

\para{Supported Updates}
Our algorithm manipulates and queries $U$ via the following interface.
\begin{enumerate}
    \item $\Insert(v)$ - add an isolated vertex $v$ (may temporarily make $U$ a forest).
    \item $\Delete(v)$ - Removes a leaf $v$ and edge adjacent to $v$ from $U$.
    \item $\Link(v,u)$ adds root $v$ as a child of node $u$.
    \item $\MoveInterval(v,[i..j])$ - Gets an interval of vertices $[i..j]$ such that there is a vertex $u\in V$ with all the vertices in $[i..j]$ being children of $u$.
    Moves all the vertices in $[i..j]$ to be children of $v$ instead.
    We define $\MoveInterval(v,i)=\MoveInterval(v,[i..i])$.
    \item $\GetHeight(v)$ - returns $\depth(v,U)$.
    \item $\findIAncestor(v,i)$ - returns the $i$'th ancestor of $v$.
\end{enumerate}
Each of these operations can be executed in $O(\log n)$ time. 
\cite{BCR24} show how to support this functionality using link-cut trees, achieving amortized $\Otild(1)$ time per operation. 
By employing top trees \cite{AHLT05} instead of link-cut trees, this functionality can be obtained with worst-case $\Otild(1)$ update time.
We further explain how this is done in \cref{sec:top_trees}.

\section{Chains}
In this section, we present a problem that is closely related to the runtime analysis of out algorithm.
Let $S$ be a string and let $I$ be a set of indices in $S$.
For $i\in I$ define the sequence $\chain{I}{i}{S}$ as follows.
Let $I_i=I\cap[1..i-1]$ be all the indices in $I$ that are smaller then $I$.
Sort $I_i$ in a non-increasing order of $\LCP(i,\cdot)$.
Break ties in a non-decreasing order of $\LCS(i,\cdot)$.
If there are still ties, break ties arbitrarily.
The first element in $\chain{I}{i}{S}$ is $\chain{I}{i}{S}[1]= I_i[1]$.
The $k$th element of $\chain{I}{i}{S}$ is the first element $i'\in I_i$ such that $\LCS(i,i')>\LCS(i,\chain{I}{i}{S}[k-1]))$.
If there is no such $i'$, $\chain{I}{i}{S}[k-1]$ is the last element of $\chain{I}{i}{S}$.
Notice that the $k$th element in $\chain{I}{i}{S}$ \textbf{must} have a better $\LCS$ than all the previous $k-1$ elements in $\chain{I}{i}{S}$.


Obviously, the sum $\Sigma_I = \Sigma_{i\in I} |\chain{I}{i}{S}|$ is bounded by $O(|I|^2)$.
In the next lemma we prove a tighter upper bound on $\Sigma_{I}$.


\begin{lemma}\label{lem:chain}$\Sigma_I=O(|I|\log^2n)$
\end{lemma}

\begin{proof}
Let $T_R$ be a Trie over the strings $S_R= \{S[i,n] \mid i \in I \}$.
Let $T_L$ be a Trie over the strings $S_L = \{ S[1,i]^R \mid i \in I \}$.
In $T_R$ (resp. in $T_L$) the leaf corresponding to $S[i,n]$ (resp, to $S[1,i]^R$) is labeled as '$i_R$' (resp $i_L$).
The size of each string in $T_R$ or in $T_L$ is at most $n$, so the total size of $T_L$ and $T_R$ is $O(n^2)$ and in particular $O(\log |T_L|) = O(\log(|T_R|)=O(\log n)$.
For $d\in \{ L,R\}$, we denote the lowest common ancestor of nodes $i_d,j_d$ in a tree $T_d$ as $u_{i_d,j_d}$.

For $d\in \{L,R\}$,let  $\LCA^d_{i}= \{u_{i_d,j_d} \mid j\in \chain{I}{i}{S}\}$.
Recall that  $\depth(u_{i_R,j_R},T_R)=\LCP(i,j)$ and $\depth(u_{i_L,j_L},T_L)=\LCS(i,j)$.
Also recall that $\{\LCP(i,\chain{I}{i}{S}[k])\mid {k\in [|\chain{I}{i}{S}|]}\}$ is decreasing, and therefore there are no two indices $j,j' \in \chain{I}{i}{S}$ such that $u_{i_R,j_R} = u_{i_R,j'_R}$.

We make use of the following well-known fact.
\begin{fact}[Heavy Path Decomposition]\label{fact:heavypathdecompos}
    A rooted tree with $n$ nodes can be decomposed into simple paths (called \textit{heavy paths}) such that every root-to leaf path intersects $O(\log n)$ heavy paths.
\end{fact}
We assume that $T_L$ and $T_R$ are partitioned into Heavy Paths.
For every $i\in I$ we denote as $p^i_L$ and $p^i_R$ the leaf to node path from $i_L$ to the root of $T_L$ and from $i_R$ to the root of $T_R$, respectively.
Denote as $H^i_L$ and $H^i_R$ the heavy paths that intersect with $p^i_L$ and with $p^i_R$ respectively.
Then, $|H^i_L|+|H^i_R|\in O(\log n)$.

We make the following charging argument.
Let $j\in \chain{I}{i}{S}$.
Let $h^R_{i,j}$ and $h^L_{i,j}$ be the heavy paths containing $u_{i_R,j_R}$ and $u_{i_L,j_L}$, respectively.
We say that $j$ is a type 1 element of $\chain{I}{i}{S}$ if $u_{i_R,j_R}$ has the maximal depth among the vertices of $\LCA^R_i \cap h^R_{i,j}$ or if $u_{i_L,j_L}$ has the maximal depth in $\LCA^L_{i} \cap h^L_{i,j}$.
Otherwise, $j$ is type 2 element of $\chain{I}{i}{S}$.
We charge every type 1 element of $\chain{I}{i}{S}$ on $i$, and every type 2 element $j$ of $\chain{I}{i}{S}$ on $j$.

Since both $p^i_L$ and $p^i_R$ visit $O(\log n)$ heavy paths, and since the depths of elements in $\LCA^L_i$ and in $\LCA^R_i$ are distinct, every $i\in I$ is charged on $O(\log n)$ elements of type 1.

We next prove that every $i\in I$ is charged on $O(\log^2 n)$ elements of type 2.

Let $\ChargeTwo(i)$ be the set of indices $j$ such that $i$ is a type 2 element in $\chain{I}{j}{S}$.
We claim that there are no two elements $j,k\in \ChargeTwo(i)$ such that $(h^L_{i,j},h^R_{i,j})= (h^L_{i,k},h^R_{i,k})$.
Notice that since for every $j\in I$ it holds that $(h^L_{i,j},h^R_{i,j}) \in H^i_{L} \times H^i_R$, the above claim directly implies $|\ChargeTwo(k)|=O(\log^2 n)$.
This would conclude the proof, as it shows that every index in $|I|$ is charged at most $O(\log^2 n + \log n)$ times, leading to $\Sigma_I \in O(|I| \log^2 n)$.

Assume by contradiction that there are two elements $j<k\in \ChargeTwo(i)$ such that $(h^L_{i,j},h^R_{i,j})=(h^L_{i,k},h^R_{i,k})$.
For $d\in \{L,R\}$, let $v_d$ be the lowest node in $h^d_{i,k}$ and let $z_d$ be the successor of $u_{v_d,i_d}$ in $h^d_{i,k}$.
Notice that $z_d$ exists because $i$ is charged as type 2 for $i\in \chain{I}{k}{S}$.
Moreover, $z_d\in p^k_d\cap p^j_d$ which implies $\depth(u_{j_d,k_d},T_d)\ge \depth(z_d,T_d)$.
In addition, $u_{v_d,i_d}=u_{k_d,i_d}=u_{j_d,i_d}$. See \cref{fig:chain}.



\begin{figure}[h]  
    \centering
    \includegraphics[width=0.6\textwidth]{chain.pdf} 
    \caption{A schematic figure of $\T_d$}
    \label{fig:chain}
\end{figure}


Therefore,
\[
\LCP(j,k)= \depth(u_{j_R,k_R},T_R)\ge \depth(z_R,T_R)>\depth(u_{v_R,i_R},T_R)=
\LCP(i,k)
\]
and
\[
\LCS(j,k)= \depth(u_{j_L,k_L},T_L)\ge \depth(z_L,T_L)>\depth(u_{v_L,i_L},T_L)=
\LCS(i,k)
\]


We have shown that $\LCP(j,k)>\LCP(i,k)$ and  $\LCS(j,k)>\LCS(i,k)$.
Therefore, $i$ would never be picked as the first/next element of $\chain{I}{k}{S}$ in the presence of $j$.
Hence, $i\not\in \chain{I}{k}{S}$, a contradiction.

To summarize, each node $i\in I$ is charged at most $O(\log n)$ on elements of type 1 and at most $O(\log^2 n)$ on elements of type 2.
We clearly execute exactly $\Sigma_I$ charges, so $\sum_{i\in[x]}|\chain{I}{i}{S}|=O(|I| \log^2 n)$.
\end{proof}

\section{Algorithm}\label{sec:alg}

In this section we introduce the data strucute of \cref{thm:lz77}.
We describe the update algorithm in the following way.
We assume that we currently have $U= \T_S$ for the dynamic string $S$, and that an edit operation is applied to $S$ at index $z$, resulting in $S'$.
This section is dedicated to showing how to modify $U$ to be $\T_{S'}$.
We fix the notation of $S$,$S'$, and $z$.
Also, we refer to the update as $(S,S')$.

Let $M$ be an integer to be fixed later. 
Let $M_L$ be the substring of length $M$ to the left of $z$, i.e. $M_L = S[z-M..z-1]$, and let $M_R$ be the substring of length $M$ to the right of $z$, i.e. $M_R = S[z+1..z+M]$.\footnote{If $z \le M$ or $z \ge n-M+1$, $M_L$ and $M_R$ are truncated at the edges of $S$, we ignore that in future discussion to avoid clutter.}
\begin{center}
\begin{tikzpicture}
    % Draw the full box outline
    \draw (1,0) rectangle (10,0.5);

    % Draw S label on the left
%    \node[left] at (1,0.3) {$S$};

    % Draw M_L (blue), touching z
    \draw[fill=blue!20] (3,0) rectangle (5,0.5) node[pos=.5] {\Large $M_L$};

    % Draw z (red)
    \draw[fill=red!30] (5,0) rectangle (5.5,0.5) node[pos=.5] {};

    % Draw M_R (green), touching z
    \draw[fill=green!20] (5.5,0) rectangle (7.5,0.5) node[pos=.5] {\Large $M_R$};

    % Position labels below the diagram
    \node[below] at (5.25,0) {$z$};

\end{tikzpicture}
\end{center}
\begin{definition}[Heavy Index]
    Let $P$ be a string.
    We say that $i$ is a $P$-heavy index for the update $(S,S')$ if $\LPF'_{S}(i) \neq \LPF_{S'}(i)$ and there is an occurrence of $P$ in index $k$ such that
    $k\in [i.. i+ \min(\LPF'_{S'}(i),\LPF'_{S}(i)) - |P|]$  \end{definition}

In the following lemma, we classify the indices of $S'$ into 4 types.
\begin{lemma}[Indices types]\label{lem:indextypes} 
    Every index $i\in [|S'|]$ satisfies at least one of the following.
    \begin{enumerate}
        \item \textbf{Inactive index:} $\LPF'_{S}(i)=\LPF'_{S'}(i)$.
        \item \textbf{Super Light index:}  $i\in [z-M-1..z]$
        \item \textbf{Light index:} For some $T\in \{S,S'\}$, there are integers $a,b\in[0..M]$ such that $i=\min(\Occurrences{{T}[z+1..n]}{T[z-a..z+b]})$.
        \item \textbf{Heavy index:} $i$ is $M_L$-heavy or $M_R$-heavy for the update.
    \end{enumerate}
\end{lemma}

\begin{proof}
Let $i$ be an index that is not an Inactive index.

We consider the following cases regarding where the edit happens (see \cref{fig:cases}):





\begin{figure}[h]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \begin{minipage}{0.65\textwidth}          \includegraphics[width=\linewidth]{case1.pdf}
        \end{minipage}
        \hspace{15pt} 
        \begin{minipage}{0.25\textwidth}
            \caption{example of Case 1}
            \label{fig:case1}
        \end{minipage}
    \end{subfigure}
    
    \vspace{10pt} 
    
    \begin{subfigure}{\textwidth}
        \centering
        \begin{minipage}{0.65\textwidth}
            \includegraphics[width=\linewidth]{case2.pdf}
        \end{minipage}
        \hspace{15pt}
        \begin{minipage}{0.25\textwidth}
            \caption{example of Case 2}
            \label{fig:case2}
        \end{minipage}
    \end{subfigure}
    
    \vspace{10pt}
    
    \begin{subfigure}{\textwidth}
        \centering
        \begin{minipage}{0.65\textwidth}
            \includegraphics[width=\linewidth]{case3.pdf}
        \end{minipage}
        \hspace{15pt}
        \begin{minipage}{0.25\textwidth}
            \caption{example of Case 3}
            \label{fig:case3}
        \end{minipage}
    \end{subfigure}

    \caption{Examples of the three cases in the proof of \cref{lem:indextypes}. 
    In \cref{fig:case2}, there are three subcases.
    The index $z_1$ corresonds to case 2.1,
    The index $z_2$ corresonds to case 2.2,
    and the index $z_3$ corresonds to case 2.3.}
    \label{fig:cases}
\end{figure}






\underline{Case 1}: $z\in[i..i+\LPF'_{\hat S}(i)]$ for some $\hat S \in \{ S,S'\}$.
    If $z-i \le M$, we have that $i$ is a Super Light index.
    Otherwise, let $d=z-i-1 < \LPF'_{\hat S}(i)$.
    Since the update does not affect indices smaller than $z$, we have $S[i..i+d] = S'[i..i+d]$.
    From the definition of $\LPFpos_{\hat S}(i)$ and from $d < \LPF'_{\hat S}(i)$ we have $\hat S[i..i+d] = \hat S[\LPFpos_{\hat s}(i).. \LPFpos_{\hat s}(i) + d]$.
    Since $\LPFpos_S(i) < i$, we have that $\LPFpos_{\hat S}(i) + d < z-1$.
    Again, since the update does not affect indices smaller than $z$, we have
    $S[\LPFpos_S(i)..\LPFpos_S(i) + d]=S'[\LPFpos_{S}(i)..\LPFpos_{S}(i) + d]$.
    This implies that $\min(\LPF'_S(i),\LPF'_{S'}(i)) \ge d+1$.
    It follows that $i$ is an $M_L$ heavy index since $[i..z-M]= [i..i+d+1-M]\subseteq [i..i+\min(\LPF_S(i),\LPF_{S'}(i)) - M]$ and $M_L$ occurs at $z-M$.
    
    \underline{Case 2}: $z\in [\LPFpos_{\hat{S}}(i)..\LPFpos_{\hat{S}}(i)+\LPF'_{\hat{S}}(i)]$ - for some $\hat{S}\in \{S,S'\}$. 
    First notice that $\hat{S}=S$ iff $\LPF'_{S}(i)>\LPF'_{S'}(i)$.
    
    Let $\lpos=\hat{S}[\LPFpos_{\hat{S}}(i)..z]$ and $\rpos=\hat{S}[z..\LPFpos_{\hat{S}}(i)+\LPF'_{\hat{S}}(i)]]$
    there are three sub-cases:
\begin{enumerate}
    \item  $z-M \le \LPFpos_{\hat S}(i)$.
    Since $\lpos$ occurs before $i$ both in $S$ and in $S'$, we have that $\min(\LPF'_S(i),\LPF'_{S'}(i))\ge|\lpos|$.
    It follows that $\LPFpos_{S'}(i) + \min(\LPF'_{S}(i), \LPF'_{S'}(i)) \ge z$.
    From $S[i..i+\LPF_S'(S_i)]=S[\LPFpos_S(i)+\LPF'_S(i)]$ we have, in particular, $S[i+|\lpos|-M+1.. i+|\lpos|] = S[\LPFpos_{S}(i) + |\lpos|-M+1 ...\LPFpos_{S}(i) + |\lpos|] = S[z-M..z]=M_L$.
    We have shown that there is an occurrence of $M_L$ in $i+|\lpos|-M$ and that $\min(\LPF'_S(i), \LPF'_{S'}(i)) \ge |\lpos|$, which together indicates that $i$ is an $M_L$ heavy index.
    \item  $z-M > \LPFpos_{ \hat S}(i)$ and also $z+M\ge \LPFpos_{\hat{S}}(i)+\LPF_{\hat{S}}(i)$.
    In this case, we have $i > \LPFpos_{\hat S}(i) \ge z-M$.
    If $i\in [z-M,z]$ then $i$ is super light.
    Otherwise, $i\ge z+1$.
    Notice that in this case, $\hat S[\LPFpos_{\hat S}(i) .. \LPFpos_{\hat S}(i) + \LPF'_{\hat S}(i)-1]=\hat S[z-a..z+b]$ for some $a,b\in [M]$.
    Recall that among all occurrences of $\hat S[i..i+\LPF'_{\hat S}(i)-1]$ to the left of $i$, the occurrence at $\LPFpos_{\hat S}(i)$ is the rightmost one. 
    Therefore, $i$ is a Light index as the first occurrence of $\hat{S}[z-a..z+b]$ after $z$.
    \item $z-M > \LPFpos_{\hat S}(i)$ and $z+M < \LPFpos_{\hat S}(i) + \LPF'_{\hat S}(i)$.
    Note that in this case $\LPFpos_{\hat S}(i) = z-a$ for some $a\in [M]$.
    As in the previous case, $i > z-a \ge z-M$ and therefore if $i\le z$ then $i$ is a Super Light index.
    We assume $i \ge z+1$.
    Let $j=\min(\Occurrences{\hat{S}[z+1..n]}{\hat{S}[z-a..z+M]})$.
    Notice that $j\le i$.
    
    If $j=i$, then $i$ is a Light index. 
    Otherwise, we claim that $i$ is $M_R$-heavy.
    Notice that there is an occurrence of $M_R=\hat S[z..z+M]$ in index $i+a$.
    Additionally, due to the indices after $z+1$ not being affected by the update we have $\min(\LCP_S(i,j),\LCP_{S'}(i,j)) \ge a+M$.
    It follows from the maximality of $\LPF'$ that $\min(\LPF'_S(i),\LPF'_{S'}(i)) \ge a+M$ and therefore the occurrence of $M_R$ at $i+a$ satisfies $i+a \in [i.. i+\min(\LPF'_S(i),\LPF'_{S'}(i)) - |M_R|]$, as required.
\end{enumerate}

    \underline{Case 3}: Otherwise - we claim that $i$ is an Inactive index.
    Assume by contradiction that $i$ is not an Inactive index.
    Let $j=\LPFpos_S(i)$ and let $j'=\LPFpos_{S'}(i)$, and assume w.l.o.g. that $\LPF_S(i) < \LPF_{S'}(i)$.
    Since $z\not\in  [\LPFpos_{{S'}}(i)..\LPFpos_{{S'}}(i)+\LPF_{{S'}}(i)-1]$, and $z\not\in[i..i+\LPF'_{S'}(i)]$, we have that $S[i..i+\LPF'_{S'}(i)-1]= S'[i..i+\LPF'_{S'}(i)-1]= S'[j'..j'+\LPF'_{S'}(i)-1]= S[j'..j'+\LPF'_{S'}(i)-1]$.
    In particular, $\LCP_S(j',i) \ge \LCP_{S'}(j',i) >\LCP_S(j,i)$, contradicting the maximality of $\LPF_S(i)$.
    \end{proof}

In the rest of the section we describe how to modify $U$, which is initially $\T_S$, to obtain $\T_{S'}$.
We present three separate algorithms, one for updating each type of indices.
By 'updating' a type of indices, we mean that after the algorithm for this type is applied, every index $i$ of the this type has its parent in $U$ properly set to $i+\LPF'_{S'}(i)$. 
Notice that inactive indices do not need to be treated.
The algorithms for the Light and for the Super Light indices are straightforwardly implemented following the definition of the types.
The $M_L$-Heavy and $M_R$-Heavy indices require a more intricate care.

\para{Super Light Indices.}
To update all Super Light Indices, the algorithm applies the following procedure.
For every $i\in [z-M-1..z]$, calculate $\LPF'_{S'}(i)$ on $S'$ (this is the only place where the algorithm handles only $S'$).
and applies the update to $\MoveInterval(i+\LPF'_{S'}(i),i)$ to $U$.

It is easy to see that all Super Light indices have their parents properly set after these procedure is applied.
The following directly follows.
\begin{observation}\label{obs:superlighttime}
    There is an algorithm that sets the parent of every Super Light index $i$ of the update to $i+\LPF'_{S'}(i)$ in $\Otild(M)$ time.
\end{observation}

\para{Light Indices.}
The algorithm updates Light indices as follows.
For every $a,b\in [0..M]$, the algorithm finds $k = \min (\Occurrences{S[z+1..n]}{S[z-a..z+b]})$.
The algorithm calls $\MoveInterval(k+\LPF'_{S'}(k),k)$.
For  $k' = \min (\Occurrences{S'[z+1..n]}{S'[z-a..z+b]})$, the algorithm calls $\MoveInterval(k+\LPF'_{S'}(k),k)$.

Clearly, every Light index has its parent in $U$ properly set after running the above procedure.

The following directly follows.
\begin{observation}\label{obs:lighttime}
    There is an algorithm that sets the parent of every Light index $i$ of the update $i+\LPF'_{S'}(i)$ in $\Otild(M^2)$ time.
\end{observation}
\subsection{The Heavy Algorithm}\label{sec:heavy}

The section is devoted to proving the following.
\begin{lemma}\label{lem:heavytime}
    There is an algorithm that sets the parent of every $M_L$-Heavy and $M_R$-Heavy index $i$ of the update to $i+\LPF'_{S'}(i)$ in $\Otild(M)$ time.
\end{lemma}

For clarity, we present a proof of \cref{lem:heavytime} under the simplifying assumption that both $M_L$ and $M_R$ are aperiodic. 
The general proof is deferred to \cref{sec:periodic}.

We start by presenting some tools.

The algorithm computes ranges in $S'$ and $S$ that are needed to be moved based on the occurrences of $M_L$ and $M_R$.
We represent sequence of ranges by a sequence of starting indices followed by the last index of the last range i.e. $[s_1,s_2,\dots,s_k]$.

\para{$\SeqGenQuery$.} Given a string $S$ and two indices $i,j$ we define the set $L_S(i,j)=\{l_{i,0},l_{i,1},..\}$ as follows.
Let $l_{\min}$ be the minimal index in $[1,i]$ such that $l_{\min}+\LPF'(l_{\min})\ge j$.
We define an element $l_{i,k}\in L_S(i,j)$ recursively.
\[
l_{i,k} =
\begin{cases}
i & \text{if }k=0,\\
\min(i'\in[l_{\min}..i]\mid i'+\LPF'_S(i')=i+\LPF'_{S}(i)) 
& \text{if } k = 1, \\ 
\min(i'\in[l_{\min}..i]\mid i'+\LPF'_S(i')=l_{i,k-1}-1+\LPF'_{S}(l_{i,k-1}-1)) & \text{if } k > 1.
\end{cases}
\]
$|L_S(i,j)|$ is the first $k$ such that $l_{i,k}$ is undefined.
We sometimes omit the subscript $i$ when it is clear from the context.
\begin{tcolorbox}[title={$\SeqGenQuery(S,i,j)$}]
\textbf{Input:} a string $S$, and two indices $i \le j \in [|S|]$.\. \\
\textbf{Output:} The set $L_S(i,j)$
\end{tcolorbox}

\begin{lemma}\label{lem:fastintervalscomputation}
    There is an algorithm that given $i$, $j$, and access to the data structure of \cref{lem:dynamicLPF}, computes $L_S(i,j)$ in $\Otild(|L_S(i,j)|+1)$.
\end{lemma}
\begin{proof}
Recall that due to \cref{lem:19}, the values $j+\LPF_S(j)$ are monotonically increasing.
This property allows us to find, given a value $x$ and a range $[a,b]$, the minimal index $z\in [a,b]$ with $z+\LPF_S(z)=x$ (or with $z + \LPF_S(z) \ge x$).
Since $\LPF_S(z)$ can be found in $\Otild(1)$ time for an input vertex $z$, this binary search is executed in $\Otild(1)$ time.

The lemma follows by applying this procedure $O(1+|L_S(i,j)|)$ times as follows.
First, find $\LPF_S(i)$.
Then, find $l_{\min}$ which is the minimal index in $[1,i]$ with $l_{\min} + \LPF_S(l_{\min})\ge j$.
Then, find $l_{i,1}$ as the minimal element in $[l_{\min}..i]$ with $l_{i,1} + \LPF'_S(l_{i,1}) = i+\LPF'_S(i)$.
If no such element exists - report that $L_{S}(i,j)$ is empty.
As long as $l_{k}$ exists, $l_{k+1}$ can be found straightforwardly according to the definition of $L_S(i,j)$ by applying the binary search procedure.
\end{proof}
Notice that the last element of $L_S(i,j)$ is $l_{\min}$, and that $L_S(i,j)$ has a decreasing order.

One can think of $L_S(i,j)$ as a sequence of ranges.
However, we denote $|L_S(i,j)|$ as the number of indices in $L_S(i,j)$, not the number of ranges.
By \cref{lem:19}, there is an index $u$ such that for all $i\in[l_k..l_{k-1}-1]$,
$u=i+\LPF'(i)$.
We sometimes say that $[l_k..l_{k-1}-1]\in L_S(i,j)$.

We say that a range $[s..t]$ is \emph{$S$-clean} if there is an index $u$ such that for all $i,j\in [s..t]$, it holds that $i+\LPF'_S(i) = j+\LPF'_S(j)$.
A sequence of ranges $R=[s_1,s_2,\dots,s_k]$ is $S$-clean if every $r\in R$ is $S$-clean.
It holds that $L_S(i,j)$ is $S$-clean. 

We observe the following connection between $\chain{I}{i}{S}$ and $L_S(i,j)$.


\begin{lemma}\label{lem:bound_Li}
    Let $S$ be a string and let $i,j$ be two indices. Then,  \[|L_S(i,j)|\le 
    |\chain{\Occurrences{S}{S[i..j]}}{i}{S}|+1.\]
\end{lemma}

\begin{proof}
    We denote $A=\Occurrences{S}{S[i..j]}$.
    Denote $\chain{A}{i}{S}=i_1,i_2,i_3,..i_{|\chain{I}{i}{S}|}$ and $L_{S}(i,j)=i,l_1,l_2,\dots,l_{k}$.
    For every $x\in [k]$, we have $l_x + \LPF'_S(l_x) \ge j$ since $l_x \in [1,l_{\min}]$.
    We denote $j_x=\LPFpos_S(l_x)$.
    Due to the definition of $\LPF$, we have $S[l_x,l_x+\LPF_S(x)]= S[j_x,j_x +\LPF_S(l_x)]$ which in particular gives raise to the equality $S[i,j]= S[j_x + (i-l_x)..j_x+(j-l_x)]$.
    We have shown that $o_x = j_x + (i-l_x) \in A$ ($o_x$ is an occurrence of $S[i,j]$).

    We will show that for every $x\in [k]$ it holds that $\LCP_S(i,o_x) =\LCP_S(i,i_x)$.
    Since the sequence $\{\LCP(i,i_x) \}^{|\chain{I}{i}{S}|}_{x=1}$ is strictly decreasing, this introduces an Injection from $[k]$ to $|\chain{I}{i}{S}|$, hence proving the statement of the lemma.
   \begin{claim}\label{clm:asmuchtotheleft}
        For every $x\in [k]$, it holds that $\LCS_s(i,o_x) = i-l_x+1$
    \end{claim}
    \begin{claimproof}  
        Recall that $S[j_x,o_x] = S[l_x,i]$.
        This already implies that $\LCS_S(i,o_x)\ge i-l_x+1$.
        Assume to the contrary that $\LCS_S(i,o_x)> i-l_x+1$, we therefore have that $S[l_x-1]=S[j_x-1]$ which implies $\LCP(l_x-1,j_x-1) = \LCP(l_x,j_x) + 1$.
        This in turn implies $\LPF(l_x-1) \ge \LPF(l_{x})+1$ which due to \cref{lem:19} yields $\LPF_S(l_x-1) = \LPF_S(l_x)+1$.
        Due to the definition of $L_S(i,j)$, we have $l_{x}-1 + \LPF'_S(l_{x}-1) = l_{x-1} - 1 + \LPF'_S(l_{x-1} -1 )$.
        This is a contradiction to the minimality of $l_x$.
    \end{claimproof}
    We prove the claim by induction.
    
    \para{Induction Base.}
    Notice that if $\LPF_S(i) < j-i+1$, $l_1$ is undefined and the lemma holds trivially.
    We can therefore assume $\LPF_S(i) \ge j-i+1$.
    We claim $\LCP_S(i,i_1) = \LPF_S(i)$.
    Clearly, for every $i' \in A\cap [1..i-1]$ it hold that $\LCP_S(i,i_1) \ge \LCP_S(i,i')$.
    For every $i'\notin A$, we have $\LCP(i,i') < |S[i..j]| = j-i+1\le \LPF_S(i)$.

    We need to prove that $\LCP_S(i,o_1) = \LPF_S(i)$.
    Recall that $l_1$ is the leftmost index in $[1,l_{\min}]$ with $l_1+\LPF_S(l_1) = i +\LPF_S(i)$.
    Assume to the contrary that there is an index $o'<i$ with $\LCP_S(i,o') > \LCP_S(i,o_1)$.
    Recall that $S[l_1..i]=S[j_1..o_1]$.
    This leads to 
\begin{align*}
    \LCP_S(i,o') > \LCP_S(i,o_1) = \LCP_S(l_1,j_1)-(i-l_1)&=\LPF_S(l_1)-i+l_1\\
&=\LPF_S(i) + i - l_1 -i + l_1 =\LPF_S(i).
\end{align*}
    A contradiction to the maximality of $\LPF_S(i)$.

    \para{Induction Step.}
    Assume that for $x\in[k-1]$ we have $\LCP(i,i_x) = \LCP(i,o_x)$.
    Let $A_x = \{o\in A\cap [1..i-1] \mid \LCS(i,o) > \LCS(i,i_x) \}$.
    By definition of $\chain{A}{i}{S}$, we have that $\LCP(i,i_{x+1}) = \max_{o\in A_x}(\LCP(i,o))$.
    We therefore need to prove that $\LCP(i,o_{x+1}) = \max_{o\in A_x}(\LCP(i,o))$
    We start by proving that $o_{x+1} \in A_x$.
    Assume to the contrary that $\LCS_S(i,o_{x+1}) \le \LCS_S
    (i,i_x)$.
    It follows from \cref{clm:asmuchtotheleft} that $\LCS(i,i_x) \ge i- l_{x+1} + 1$.
    It follows from the induction hypothesis that $\LCP_S(i,i_x) = \LCP_S(i,o_x)$.
    Recall that $\LCP(i,o_x) = i-\ell_x + \LPF_S(l_x)$
    putting the two equalities implied by $\LCP(i,i_x)$ and by $\LCS(i,i_x)$ together, we obtain $S[l_{x+1}..i+\LPF_S(l_x)] = S[i_{x} -i+ l_{x+1} + l_x..i_x + \LPF_S(l_x)]$
    It follows that $l_{x+1} + \LPF_S(l_{x+1}) \ge l_x + \LPF(l_x)$, which according to \cref{lem:19} implies $l_{x+1} + \LPF_S(l_{x+1}) = l_x + \LPF(l_x)$.
    From the definition of $L_S(i,j)$ we have that $l_x$is the minimal index with $l_x + \LPF_S(l_x) = l_{x-1} -1 + \LPF_S(l_{x-1}-1)$.
    This contradicts $l_{x+1} + \LPF_S(l_{x+1}) = l_x + \LPF_S(l_x)$, as $l_{x+1} < l_x$.
    
    We have shown that $o_{x+1} \in A_x$.
    It remains to show that $\LCP(i,o_{x+1}) = \max_{o\in A_x}(\LCP(i,o))$.
    Recall that among indices $\hat o\in A\cap [1..i-1]$ with $\LCP(i,\hat o) = \LCP(i,i_x)$, the index $i_x$ maximizes $\LCS(i,i_x)$ (i.e. $\LCS$ is a tiebreaker in the generation of $\chain{A}{i}{S}$).
    Therefore the induction hypothesis implies $\LCS(i,i_x) \ge \LCS(i,o_x)$.
    Assume to the contrary that there is $o' \in A_x$ with $\LCP_S(i,o') > \LCP_S(i,o_{x+1})$.
    It holds that $\LCS(i,o') > \LCS(i,i_{x})\ge \LCS(i,o_x)=i-l_x+1$.
    Recall that $\LCP(i,i_x) = \LCP(i,o_x)=i-l_x + \LPF_S(l_x)$.
    Therefore, $S[l_x-1..i+\LPF(l_x)] = S[i_x + l_x -i- 1..i_x + \LPF(l_x)]$.
    This indicates that $(l_x - 1) + \LPF_S(l_x-1)  \ge l_x + \LPF_S(l_x)$ which due to \cref{lem:19} yields $(l_x - 1) + \LPF_S(l_x-1)  = l_x + \LPF_S(l_x)$
    Again, we have reached a contradiction to the minimality of $l_x$ as an index with $l_x+\LPF(l_x)= \mathcal{X}_x$ for some number $\mathcal{X}_x$.
\end{proof}


\para{Update Trees.} 
We present a procedure for updating $U$,
Given two sequences of ranges (represented by indices), $(L_S,L_{S'})$ such that $L_S$ is $S$-clean and $L_{S'}$ is $S'$-clean,  $\UpdateTreesByRange(L_S,L_{S'})$ updates the $U$ as follows.

First, merge $(L_S,L_{S'})$ into one non-increasing ordered sequence $\hat L$.


Second, remove from $\hat L$ ranges that appear only in one of $L_S$ and $L_{S'}$ to obtain $L$.
Namely, let $s=\max(\min(L_S),\min(L_{S'}))$ and $t=\min(\max(L_S),\max(L_{S'}))$.
Then, $L=\hat L\cap [s..t]$.

Finally, for every range $[a..b]$ represented by $L$, update the $\LPF$-tree of $S'$ by calling $\MoveInterval(a+\LPF'_{S'}(a),[a,b])$.

\begin{lemma}\label{lem:range}
    $L$ is both $S$-clean and $S'$-clean.
\end{lemma}
\begin{proof}
    Given a range $R=[a..b]$ in $L$, we prove that $R$ is $S$-clean (having that $R$ is $S'$-clean is symmetric).
    Since $a\ge \min {(L_S)}$ and $b\le \max {(L_S)}$, it holds that $[a..b]\subseteq[\min (L_S)..\max (L_S)]$.
    Moreover, by the merge operation, there is an $S$-clean range  $R'\in L_s$ such that $R\subseteq R'$.
    Therefore, $R$ is $S$-clean.
\end{proof}

Notice that after applying $\UpdateTreesByRange(L_S,L_{S'})$, every index in $[s..t]$ has its parent in $U$ properly set to $i + \LPF'(i)$.
Specifically, the index $i$ is on some range $[a,b] \in L$, and the parent of $i$ was set to be $a + \LPF'_{S'}(a)$ which is equal to $\LPF'_{S'}(i)$ due to $[a,b]$ being $S'$-clean.

Given the data structure of \cref{lem:dynamicLPF}, the procedure $\UpdateTreesByRange$ costs $\Otild(|L_S|+|L_{S'}|)=\Otild(|L|)$ time.

\subsubsection{Non-Periodic Case}
In this section, we provide an algorithm that has a sufficient running time only under the assumption that $M_L$ and $M_R$ are aperiodic.
Let $A_1 = \Occurrences{S}{M_L}$, $A_2 = \Occurrences{S'}{M_L}$.
and $A=A_1\cap A_2$ 
(notice that $A_1$ is similar to $A_2$ up to indices that are in $[z-M..z]$).
Due to our aperiodic settings, \cref{fact:aperfarapart} suggests that $|A| \in O(\frac{n}{M})$.

For every $i\in A$, the algorithm computes $L_{S,i}=L_{S}(i,i+M-1)=\SeqGenQuery(S,i,i+M-1)$ and $L_{S',i}(i,i+M-1)=\SeqGenQuery(S',i,i+M-1)$.
Then the algorithm calls $\UpdateTreesByRange(L_{S,i},L_{S',i})$ to update $U$.

\para{Correctness.}
Since $L_{S,i}$ is $S$-clean and $L_{S',i}$ is $S'$-clean,  \cref{lem:range} holds and therefore calling to the procedure $\UpdateTreesByRange(L_{S,i},L_{S',i})$ is valid and every index set by a call to $\UpdateTreesByRange(L_{S,i}, L_{S',i})$ has its parent in $U$ set to be its correct parent in $\T_s$.
We claim that the algorithm achieves the following.

\begin{lemma}\label{lem:update_i}
    Let $i$ be an $M_L$-Heavy index or an $M_R$-Heavy index.
    For some $k\in A$, the call to $\UpdateTreesByRange(L_{S,k},L_{S',k})$ updates $i$.
\end{lemma}
\begin{proof}
    Assume that $i$ is $M_L$-heavy.
    The proof for the case where $i$ is $M_R$-heavy is symmetric.

    Since $i$ is $M_L$-heavy, there is an occurrence of $M_L$ in an index $k$ such that
    $k\in [i.. i+ \min(\LPF'_{S'}(i),\LPF'_{S}(i)) - |M_L|]$.
    Let $s=\max(\min(L_{S,i}),\min(L_{S',i}))$.
    We prove that $i\in [s..k]$.
    Let $q=\min(L_{S}(k,k+M-1))$.
    Recall that $q$ is the leftmost index with $q+\LPF_S(q) \ge k+M-1$.
    By the definition of $M_L$-heavy $i+\LPF'_S(i)\ge k+M-1$ it follows that $i\ge q=\min(L_{S,k})$.
    Symmetrically, we have that $i\ge \min(L_{S',i})$.
    By the definition of $M_L$-Heavy, $i\le k$.

    $\UpdateTreesByRange(L_{S,i},L_{S',i})$ updates all the indices in $[q..k]$ and we have shown that $i\in [q..k]$, which completes the proof.
\end{proof}

\para{Time Complexity.}
The algorithm uses \cref{lem:ipmalllengths} to obtain all occurrences of $M_L$ and all occurrences of $M_R$ in $\Otild(\frac{n}{M})$ time.
For each $i\in A$, the algorithm computes $L_{S,i}$ and $L_{S',i}$ using \cref{lem:fastintervalscomputation} in $\Otild(|L_{S,i}| + |L_{S',i}|)$.
Then, the algorithm applies $\UpdateTreesByRange(L_{S,i},L_{S',i})$ in
$\Otild(|L_{S,i}|+|L_{S',i}|)$ time.
By \cref{lem:bound_Li,lem:chain}, $\sum_{i\in A}|L_{S,i}|\le\sum_{i\in A}\chain{A}{i}{S}+|A|\in\Otild(|A|)$.
Due to the same reasoning, $\sum_{i\in A}|L_{S',i}|\in\Otild(|A|)$ as well.
Therefore, the algorithm has a running time of $\Otild(\frac{n}{M})$.

\subsection{Proof of \cref{thm:lz77}.}
We are now ready to prove \cref{thm:lz77}.


\begin{proof}[proof of \cref{thm:lz77}]
    Given a string $S$, the algorithm builds the dynamic data structures described in \cref{lem:dympillar,lem:ipmalllengths,lem:first_occ,lem:dynamicLPF} that supports pattern matching, $\LCP$, $\LCS$, and $\LPF'$ queries.
    Since the insertion time for each of these data structures is$\Otild(1)$, they can be constructed in $\Otild(n)$ time by initializing them for the empty string and inserting $S$ symbol by symbol. 

    In addition, the algorithm builds $U=\T_S$ in $\Otild(n)$ time using $O(n)$ $\LPF'$ queries.
    
When an update is applied to the string $S$ in index $z$, resulting in a modified string $S'$, the algorithm applies \cref{obs:superlighttime} ,\cref{obs:lighttime}, and \cref{lem:heavytime} to properly assign the parents of all Super Light, Light, and Heavy indices in $S$ with respect to the update.
The remaining indices are Inactive due to \cref{lem:indextypes}, and therefore require no update and the application of \cref{obs:superlighttime} ,\cref{obs:lighttime}, and \cref{lem:heavytime} on $U = \T_S$ results in $U=\T_{S'}$.
We refer to this part of the algorithm as 'the transition'.

If the updates inserts an index $z$, the algorithm adds $z$ to $U$ with $\Insert(z)$ prior to the transition.
If the update deletes the index $z$, the algorithm deletes the index $z$ with $\Delete(z)$ from $U$ after the transition.

By \cref{obs:superlighttime} ,\cref{obs:lighttime}, and \cref{lem:heavytime}, the transition is applies in $\Otild(M^2 + \frac{n}{M})$ time.
By setting $M= n^{2/3}$ we obtain $\Otild(n^{2/3})$ update time.


To query $\SelectPhrase(i)$, the algorithm calls $a= \findIAncestor(1,i-1)$ and $b= \findIAncestor(1,i)-1$ in $\Otild(1)$ time.
It then reports that the $i$'th phrase of $\LZss(S)$ is $S[a,b]$.

To query $\ContainingPhrase(i)$, the algorithm binary search LZ77 using $\SelectPhrase$ to find $k$ such that th $k$'th phrase is a range $[a..b]$ satisfies $i\in [a..b]$.
Each step in the binary search takes $\Otild(1)$ time, and hence $\ContainingPhrase(i)$ is implemented in $\Otild(1)$ time.

To query $\LZLength(i)$, the algorithm applies a binary search for $z$ such that  $S[a,b] = \SelectPhrase(z)$ satisfies $i\in [a,b]$.
This is implemented using $O(\log n)$ queries to $\SelectPhrase$ for a total of $\Otild(1)$ time.
\end{proof}





\section{Periodic Case}\label{sec:periodic}
We handle the case where $M_L$ is periodic with period $p$.
The case where $M_R$ is periodic is symmetric.
The algorithm uses \cref{lem:ipmalllengths} to obtain a set $\mathcal C$ of $O(\frac{n}{m})$ arithmetic progressions representing all occurrences of $M_L$ in $S$ and in $S'$.

In particular, let $\mathcal{C}_1$ and $\mathcal{C}_2$ be the sets of clusters representing occurrences of $M_L$ in $S$ and in $S'$, respectively.
Let $\mathcal{C}=\mathcal{C}_1\cup \mathcal{C}_2$.
We can assume without loss of generality that all clusters in $\mathcal{C}$ are maximal,
meaning that if for a cluster $C=(a,b,p)$ of $S$ (resp. of $S'$), we have $a-p,b+p\notin \Occurrences{S}{M_L}$ (resp. $\notin \Occurrences{S'}{M_L}$ ).
This can be easily achieved via $\Otild(|\mathcal C|)$ processing on $\mathcal C$.

For some cluster $C=(a_C,b_C,p)$ of occurrences in $S$ (resp. in $S'$), we denote the unique run in $S$ (resp. in $S'$) containing all occurrences of $C$ as $R_C=S[s_C..e_C]$ (resp. $=S'[s_c..e_c]$).
For every  cluster of occurrences of $M_L$ $C=(a_C,b_C,p)$ and for every $\hat{S}\in \{S,S'\}$, the algorithm computes the following.

\begin{enumerate}
    \item $L_{\hat S,a_C}=\SeqGenQuery(\hat S,a_C,a_C+M-1)$.
    \item $L_{\hat S,a_C+p}=\SeqGenQuery(\hat S,a_C+p,a_C+p+M-1)$.
    \item $L_{\hat S,b_C}=\SeqGenQuery(\hat S,b_C,e_C)$.
\end{enumerate}

Then, for every $x\in \{a_C,a_C+p,b_C\}$ the algorithm calls $\UpdateTreesByRange(L_{S,x},L_{S',x})$.


\para{Correctness.} Since for all $x\in \{a_C,a_C+1,b_C\}$ it holds that $L_{S,x}$ is $S$-clean and $L_{S',x}$ is $S'$-clean,  \cref{lem:range} holds and therefore calling to the procedure $\UpdateTreesByRange(L_{S,x},L_{S',x})$ is valid.
It remains to prove that every heavy index $j$ that needs to be updated, is indeed updated.

We first prove that if $j$ is $M_L$-heavy, then there is an occurrence $k$ of $M_L$ that is a witness for $j$ being $M_L$-Heavy such that $k$ is either the first, the second or the last occurrence in a cluster.

\begin{lemma}\label{lem:heavy_i'}
    Let $j$ be an  $M_L$-heavy index.
    Then, there is a cluster $C$ and a corresponding index $x\in \{a_C,a_C+p,b_C\}$ such that $x\in [j..j+ \min(\LPF'_{S'}(j),\LPF'_{S}(j)) - |M_L|]$.
\end{lemma}

\begin{proof}
    Since $j$ is $M_L$-heavy, there is an occurrence of $M_L$ in an index $k$ such that
    $k\in [j..j+ \min(\LPF'_{S'}(j),\LPF'_{S}(j)) - |M_L|]$.
    $k$ must belong to some cluster $C=(a_C,b_C,p)$ in $S$ and a cluster $C'=(a'_C,b'_C,p)$ in $S'$.
    If $\hat k\in \{a_C,a_{C'},a_C+p,a_{C'}+p\}$, we are done.
    Otherwise, both $C$ and $C'$ are of size at least 3.
    Let $\hat a = \max(a_C,a_{C'})$.
    Since $k$ is at least the third occurrence in both clusters, we have $\hat{a}+p\le k \le  j+ \min(\LPF'_{S'}(j),\LPF'_{S}(j))$.
    It follows that if $\hat a+p \ge j$, we are done.
    Assume $j\ge \hat a+p$.
    Recall that $R_{C} = S[s_C..e_C]$ (resp. $R_{C'}=S[s_{C'}..e_{C'}]$) is the run with period $p$ containing all occurrences of $M_L$ in $S$ (resp. in $S'$) represented by $C$ (resp. $C'$).
    Let $\hat s = \max(s_C,s_{C'})$ and $\hat e=\min (e_C,e_{C'})$.
    Notice that for all $\hat S\in \{ S,S'\}$, the string $\hat S[\hat s..\hat e]$ is periodic with period $p$.
    Due to $j-p \ge a_{\hat C}+p \ge \hat s+p$, it holds that $\hat S[j..\hat e]=\hat S[j-p..\hat e-p]$.
    It follows that $j + \LCP_{\hat S}(j,j-p)\ge \hat e$ and in particular $\hat e \le  j+ \min(\LPF'_{S'}(j),\LPF'_{S}(j))-1$.
    If $\hat e = e_C$, the run $R_C$ contains the occurrence $b_C$ so $\hat e \ge b_C-|M_L|+1$ and therefore $b_C \le  j+ \min(\LPF'_{S'}(j),\LPF'_{S}(j))-|M_L|$ and $b_C\ge k\ge i$ satisfies the claim.
    If $\hat e = b_{C'}$, the same arguments can be made to show that $b_{C'}$ satisfies the claim.
\end{proof}


\begin{lemma}\label{lem:update_i_periodic}
    Let $j$ be an $M_L$-heavy index.
    There is a cluster $C$ and a corresponding index $x\in \{a_C,a_C+p,b_C\}$ such that the procedure $\UpdateTreesByRange(L_{S,x},L_{S',x})$ updates $j$.
\end{lemma}
\begin{proof}
    Since $j$ is $M_L$-heavy, by \cref{lem:heavy_i'} there is an occurrence of $M_L$ in an index $k'$ such that
    $k'\in [j..j+ \min(\LPF'_{S'}(j),\LPF'_{S}(j)) - |M_L|]$.
    Let $C$ be the cluster representing the occurrence $k'$ in $S$ and let $C'$ be the cluster representing the occurrence $k'$ in $S'$.
    Note that we may have $C\neq C'$ only if $z\in [s_C..e_{C}]$ or $z\in [s_{C'}..e_{C'}]$).
    If we indeed have $C\neq C'$, we claim that either $[s_C,e_C] \subset [s_{C'},e_{C'}]$ or $[s_{C'}..e_{C'}] \subset [s_C..e_C]$.
    In particular, we claim that $s_C = s_{C'}$ or $e_C = e_{C'}$.
    Clearly, if $C\neq C'$ then $[s_c..e_c]\neq [s_{C'}..e_{C'}]$.
    If $z \ge k'+ p \in [k'..k'+M-1]$, then $S[s_C-1..k'+p]=S'[s_C-1..k'+p]$.
    It follows that in this case, the extension to the left of the run to the with period $p$ containing $[k'..k'..k'+M-1]$ in $S$ and in $S'$ i.e. $s_{C}=s_{C'}$.
    In the complementary case where $z < k'+p \le k'+M-p$ we have that the $e_C=e_{C'}$.
    
    We proceed with the assumption that $[s_C..e_C] \subset [s_{C'}..e_{C'}]$.
    
    If $k'\in \{a_C,a_C+p,a_{C'},a_{C'}+p\}$, then the claim follows from \cref{lem:update_i}.
    Otherwise, both clusters are of size at least $3$, and $k'\in \{b_C,b_{C'}\}$.
    Observe that if $j \le a_C+p$,
    we have that $a_C +p \in [j..k']\subseteq[j..j+\min(\LPF'_{S'}(j),\LPF'_{S}(j)-|M_L|]$.
    It remains to treat the case where $j > a_C+p$.
    We have that both $R_C= S[s_C..e_C]$ and $R'=S'[s_C..e_C]$ are periodic with period $p$.
    From $j > a_C+p \ge s_C + p$, $S[j..e_C] = S[j-p..e_C-p]$.
    Therefore, $b_C \in [j..e_C-M+1]\subseteq[j..j+\min(\LPF'_{S'}(j),\LPF'_{S}(j))-|M_L|]$.
\end{proof}

\para{Time.}
The time complexity of the algorithm is $\Otild(n/M)$.
We first bound $\sum_{C\in \mathcal C}|L_{S,b_C}|$.
Note that for every cluster $C$ such that $z\notin [s_C-1..e_C+1]$, both $S[s_C..e_C]$ and $S'[s_C..e_C]$ are runs.
At most one run with period $p$ can intersect an index, so for all but 3 clusters in each string (one touching each of $z-1$,$z$, and $z+1$), the endpoints of $R_C$ enclose a run both in $S$ and in $S'$. 
Let $\mathcal{C}'$ be the set $O(1)$ clusters such that $z\in [s_C-1..e_C+1]$ for $C\in \mathcal
C '$.

Let $B=\cup_{C\in \mathcal{C}}\{b_C\}$.

\begin{lemma}\label{lem:lasts}
    $\sum_{C\in \mathcal C\setminus\mathcal{C}'}|L_{S,b_C}||\in \Otild(|B| + n/M+M)$ 
\end{lemma}
\begin{proof}

    Let $\mathcal{P}=\cup_{C\in {\mathcal{C}\setminus}\mathcal{C}'}S[b_C..e_C+1]$ be a set of non-periodic strings beginning in the last occurrence of $M_L$ in a cluster $C$ and ending in the first character after $R_C$.
    We partition the set of indices $B$ into smaller sets, such that for each $P\in \mathcal{P}$, $B_{P}= \Occurrences{S}{P}$.
    Notice that $B=\cup_{P\in\mathcal{P}}B_P$.
    That follows from the fact that for each $P \in \mathcal{P}$, $P[1..|P|-1]$ is periodic with period $p$, and therefore every occurrence of $P$ is attached to some run.

    Given a cluster $C$, let $q=|L_{S,b_C}|$, let $P_{C}=S[b_C..e_C+1]$ an element in $\mathcal{P}$.
    Given a string $P$ let $\mathcal{C}_P=\{C\in\mathcal{C}\setminus\mathcal{C}'\mid S[b_C..e_C+1]=P\}$.
    
    By the construction of $L_{S,b_C}$, the element $l_{b_C,q-1}$ is the leftmost index in $L_{S,b_C}$, and $l_{b_C,q-2}$ is the leftmost index in $L_{S,b_C}\setminus \{l_{b_C,q-1}\}$.
    By the construction of $L_{S,b_C}$, it holds that  $l_{b_C,q-2}+\LPF'_S(l_{\ilast,q-2})>e_C$.
    Denote $L_3=\SeqGenQuery(S,b_C,e_C+1)$.
    By the construction of $L_{S,b_C}$, 
    \[
    |L_{S,b_C}\setminus \{l_{b_C,q-1}\}|
    =|\SeqGenQuery(S,b_C,e_C)|-1
    \le
    |\SeqGenQuery(S,b_C,e_C-1)|
    =|L_3|
    \]
    By \cref{lem:bound_Li}, $|L_3|\le |\chain{B_{P_{C}}}{b_C}{S}|+1$.
    Therefore, for a given cluster $C$, it holds that $|L_{S,b_C}|\le |\chain{B_{P_{C}}}{b_C}{S}|+2$.
    To conclude, 

\begin{align*}
\sum_{C\in \mathcal{C}\setminus\mathcal{C}'}|L_{S,\ilast}|&\le 
    \sum_{C\in \mathcal{C}\setminus\mathcal{C}'}(|\chain{A_{P_{C}}}{b_C}{S}|+2)
=|\mathcal{C}\setminus \mathcal{C}'| + \sum_{C\in \mathcal{C}\setminus\mathcal{C}'}|\chain{A_{P_{C}}}{b_C}{S}|\\   
    &= |\mathcal{C}\setminus \mathcal{C}'| + 
    \sum_{P\in\mathcal{P}} \sum_{C\in \mathcal{C}_P}|\chain{B_{P}}{b_C}{S}|
    \\
    &= |\mathcal{C}\setminus \mathcal{C}'| + \sum_{P\in\mathcal{P}}\Otild(|B_P|)
    \in \Otild (|B|+n/M).
\end{align*}

\end{proof}

Next we bound $L_{S,a_C}$ and $L_{S,a_C+p}$.
We define the sets $A_1=\cup_{C\in \mathcal{C}}\{a_C\}$ and $A_2=\cup_{C\in \mathcal{C}}\{a_C+p\}$.

\begin{lemma}
    $\sum_{C\in \mathcal{C}\setminus\mathcal{C}'}|L_{S,a_C}|+|L_{S,a_C+p}|\in \Otild(|B|+|A_{1}|+|A_{2}|+n/M)$
\end{lemma}

\begin{proof}
    We prove the lemma for $\sum_{C\in \mathcal{C}\setminus\mathcal{C}'}|L_{S,a_C+p}|$.
    The proof for $\sum_{C\in \mathcal{C}\setminus\mathcal{C}'}|L_{S,a_C}|$ is symmetric.
    
    Fix some $M_L$-cluster $C$ in $S$.
    For every $l_k\in L_{S,a_C+p}$, denote $r_k=l_k+\LPF'(l_k)$.

    Let  $L_{i,1}$ be the indices $\{l_k\in L_{S,a_C+p}\mid r_k>b_C\}$.
    We prove that $|L_{i,1}|\le|L_{S,\ilast}|$.
    Let $l_k\in L_{i,1}$.
    It holds that $r_k>e_C$.
    By the construction of $L_{S,\ilast}$, $l_k\in L_{S,\ilast}$.
    Therefore, by \cref{lem:lasts}, $\sum_{C\in \mathcal{C}\setminus\mathcal{C}'}|L_{i,1}|\le\sum_{C\in \mathcal{C}\setminus\mathcal{C}'}|L_{S,b_C}|\in \Otild(|\Alast|+n/M)$.

    Let $L_{i,2}$ be the indices $\{l_k\in L_{S,a_C+p}\mid l_k<s_C\}$ and let $\hat L_{i,2}=\SeqGenQuery(S,s_C-1,a_C+M-1)$.
    Let $l_k\in L_{i,2}$.
    It holds that $r_k\ge a_C+M-1$.
    The construction of $\hat L_{i,2}$ ensures that $l_k\in \hat L_{i,2}$.
    Hence, $L_{i,2}\subseteq\hat L_{i,2}$.
    Since $S[s_C-1..a_C+M-1]$ is non-periodic and $a_C+M-1-s_C\ge M$, $\Occurrences{S}{S[s_C-1..a_C+M-1]}\in O(n/M)$.
    Using \cref{lem:bound_Li}, we have that 
    \[
        \sum_{C\in \mathcal{C}\setminus\mathcal{C}'}|L_{i,2}|\le\sum_{C\in \mathcal{C}\setminus\mathcal{C}'}|\hat L_{i,2}|\le \sum_{C\in \mathcal{C}\setminus\mathcal{C}'}|\chain{\Occurrences{S}{S[s_C-1..a_C+M-1]}}{s_C-1}{S}| \in \Otild(n/M).
    \]

    Finally, let $L_{i,3}=L_{S,a_C+p}\setminus (L_{i,1}\cup L_{i,2}\cup \{s_C\})$. 
    We prove that $\sum_{C\in \mathcal{C}}|L_{i,3}|\in \Otild(n/M+|A_{1}|+|A_{2}|)$.
    We prove the following claim.
    \begin{claim}

    Let $l_k\in L_{i,3}$. 
    Then there is an index $q_k$ such that $l_k=a_C+p-\LCS_S(q_k,a_C+p)$ and $q_k\in\chain{A_1\cup A_{2}}{a_C+p}{S}$
    \end{claim}
    \begin{claimproof}
    $l_k\in L_{i,3}$ is the leftmost index such that 
    $l_k+\LPF'(l_k)=l_{k-1}-1+\LPF'(l_{k-1}-1)=r_k\ge a_C+p+M_L$
    (where $l_{k-1}-1=a_C+p$ if $k=1$).

    Therefore, there is an index $q_k\in \Occurrences{S}{M_L}$ such that $\LCP_S(q_k,a_C+p)+a_C+p=r_k$ and $\LCS_S(q_k,a_C+p)=a_C+p-l_k$.
    We prove that $q_k\in\chain{A_1\cup A_{2}}{a_C+p}{S}$.

    There is an integer $x=r_k-a_C-p+1$ such that For all 
    $\{q'<a_C+p\mid\LCP(q',a_C+p)<x\}$, it holds that $\LCP(q_k,a_C+p)\ge\LCP(q',a_C+p)$.
    Moreover, by the maximality of $l_k$, for all $q'<a_C+p$ such that $\LCP(q_k,a_C+p)= \LCP(q',a_C+p)$, it holds that $\LCS(q_k,a_C+p)\ge \LCS(q',a_C+p)$.
    Therefore, $q_k\in\chain{A_1\cup A_{2}\cup \{q_k\}}{a_C+p}{S}$.

We are left to prove that $q_k\in A_1\cup A_{2}$.
Let $C'$ be the cluster where $q_k\in [s_{C'}..e_{C'}]$.
Let $p=\per(C')=\per(C)$.
Assume by contradiction that $q_k\ge s_C+2p$ (recall that $q_k\in \Occurrences{S}{M_L})$.
Let $q'_k=q_k-\LCS_S(q_k,a_C+p)$, so $S[q'_k..q_k]=S[l_k..a_C+p]$, and in particular, there is a character $c=S[q'_k+p-1]=S[l_k+p-1]$ (notice that $\LCP_S(q'_k,l_k)\ge M_L\ge p$).
Since $l_k>i_s$, then $S[l_k-1]=S[l_k+p-1]=c$.
It holds that $\LCS_S(q_k,a_C+p)=a_C+p-l_k\le 2p-1$ and that $|S[s_{C'}..q_k]|\ge2p$.
Therefore, $|S[i'_s..q_k]|>\LCS_S(q_k,a_C+p)$ and in particular, $S[q'_k-1]=S[q'_k+p-1]=c$.
Therefore, $\LCS_S(q_k,a_C+p)\ge a_C+p-l_k+1$, in contradiction.
\end{claimproof}
    
    
    It holds that 
\[
\sum_{C_i\in \mathcal{C}}|L_{i,3}|\le 
\sum_{a_C\in A_1}|\chain{A_1\cup A_{2}}{a_C}{S}|
\in \Otild(n/M+|A_{1}|+|A_{2}|)
\]

and therefore, 
\[
\sum_{C\in \mathcal{C}\setminus\mathcal{C}'}|L_{S,a_C+p}|
\le
\sum_{C\in \mathcal{C}\setminus\mathcal{C}'}|L_{i,1}\cup L_{i,2}\cup L_{i,3}\cup\{s_C\}|\in \Otild(|\Alast|+|A_{1}|+|A_{2}|+n/M)
\]
\end{proof}


There are $O(n/M)$ clusters.
Since $|A_2|\le|A_1|=|B|\in O(n/M)$, it holds that it only remains to bound $\sum_{C\in \mathcal{C}'}\sum_{x\in \{ a_C,a_C+p,b_C\} }(|L_{S,x}|)$ $\mathcal{C}'$.
Notice that there are $O(1)$ elements in the sum, and each is bounded by $\Otild(M+ \frac{n}{M})$ due to the following lemma.

\begin{lemma}\label{lem:annoyinguys}
    For every cluster $C=(a_C,b_C,p)\in \mathcal{C}$, for every $x\in \{a_C,a_C+p,b_C\}$, and for every $\hat S \in \{ S,S'\}$ it holds that $|L_{\hat S,x}| \in O(M+\frac{n}{M})$
\end{lemma}
\begin{proof}
    Let $R= S[s..e]$ be the run with period $p$ containing $x$ in $\hat S$.
    Let $L_{\hat S,x}= l_1,l_2,\ldots, l_q$ and recall that $l_i + \LPF_{\hat S}(l_i)$ is a decreasing sequence as well as $l_i$.
    Let $z_1$ be the minimal index in $[q]$ such that $l_{z+1} + \LPF'_{\hat S}(l_{z_1}) \le e$ ($z_1 = q+1$ if no such index exist).
    Let $z_2$ be the first index in $[z_1..q]$ which $l_i < s$ (we defined $z_2 = q+1$ if no such index exists.
    We claim that $z_1 \in O( \frac{n}{M})$.
    Due to the definition of $z_1$, $l_1,l_2,\ldots l_{z_1}$ is a subsequence of $L_{\hat S}(x,e+1)$.
    Since $S[x..e+1]$ is aperiodic with length at least $M$, it follows from \cref{fact:aperfarapart} that $|\Occurrences{\hat S}{S[x..e+1]}| \in O(\frac{n}{M})$.
    It follows from \cref{lem:bound_Li} that $z_1 \le |\chain{\Occurrences{\hat S}{S[x..e+1]}}{x}{\hat S}| \in O(\frac{n}{M})$.

    We claim that $z_2-z_1 \le M$.
    For every $l\in [s+p..e]$, it holds that $\hat S[l..e] = S[l-p..e-p]$ and therefore $l + \LPF_{\hat S}(l) \ge e$.
    
    Therefore, all $\ell_z$ with $z\in [z_1,z_2-2]$ are in $[s..s+p]$.
    Since $l_z$ is strictly decreasing, we have that $z_2 - z_1-1 \le p+1 \le M$.

    Finally, we claim that $q-z_2 \in O(\frac{n}{M})$.
    Since all $l_z$ with $z\in [z_2+1..q]$ have $l_z \le s-1$ and $l_z + \LPF_{\hat S}(z) \ge x+M-1$, it holds that  $l_{z_2+1},l_{z_2+2},\ldots l_{q}$ is a subsequence of $L_{\hat S}(s-1,x+M-1)$.
    This is an aperiodic string with length more than $M$, therefore, due to similar arguments as for $z_1$, we have $q-z_2 \in O(\frac{n}{M})$.
    In conclusion, we have shown that $q = z_1 + (z_2-z_1) + (q-z_2) \in O(M+\frac{n}{M})$ as required.
    \end{proof}






\section{Lower Bound}\label{sec:lb}
\newcommand{\ourvec}{\mathbf}
\newcommand{\ACoord}{\mathsf{ACoordinate}}
\newcommand{\BCoord}{\mathsf{BCoordinate}}
\newcommand{\AVect}{\mathsf{AVector}}
\newcommand{\BVect}{\mathsf{BVector}}
\newcommand{\Phrases}{\mathsf{Phrases}}

\newcommand{\Mat}{\mathsf{Matrix}}
\newcommand{\skipo}{\mathsf{Skip\text{-}}0}
\newcommand{\skiphalves}{\mathsf{Skip\text{-}}2\mathsf{\text{-}halves}}
\newcommand{\Dict}{\mathsf{Dictionary}}


In this section, we show that there is no data structure solving \cref{prb:ub} with polynomial preprocessing time and $O(n^{2/3-\eps})$ update time for any $\eps>0$, unless SETH~\cite{IP01,IPZ01} is false, thus proving \cref{thm:intro-lb}.
We first introduce an easier version of \cref{prb:ub}.


\begin{problem}{Dynamic $|\LZss(S)|$ under substitutions}\label{prb:lb}
\textbf{Preprocess$(S)$:} preprocess a string $S$ of length $N$ and return $|\LZss(S)|$.
\medskip
\\
\textbf{Update$(i,\sigma)$:} apply $S[i]\gets \sigma$ and return $|\LZss(S)|$.
\end{problem}
Clearly, the following lower bound implies \cref{thm:intro-lb}.
\begin{lemma}\label{thm:lb}
For every $c,\eps>0$, there is no data structure that solves \cref{prb:lb} with $O(N^c)$ preprocessing time and $O(N^{2/3-\eps})$ update time, unless SETH is false.
\end{lemma}
The rest of this section is dedicated for the proof of \cref{thm:lb}.
We build upon the lower bound of Abboud and Vassilevska Williams~\cite{AVW21} for the Orthogonal Vectors ($\OV$) problem in the following setting.
A set $A$ of vectors is given for preprocessing, and then a set $B$ is queried, the goal is to decide if there is are vectors $\ourvec{v}\in A$ and $\ourvec{u}\in B$ such that $\ourvec{v}\cdot \ourvec{u} = 0$, where $\ourvec{v} \cdot \ourvec{u}= \bigvee_{j=1}^d(\ourvec{v}[j] \wedge \ourvec{u}[j]) $ denotes the boolean inner product of $\ourvec{u}$ and $\ourvec{v}$.
The lower bound of \cite{AVW21} is stated as follows.
\begin{lemma}[{\cite[{cf. Theorem 13}]{AVW21}}]\label{lem:OVA}
There is no algorithm that given two sets $A$ and $B$ of $n$ boolean vectors in
$d(n) = \log^{\omega(1)} n$ dimensions, preprocesses the set $A$ in $O(n^c)$ time, and subsequently solves $\OV$ on $A$ and $B$ in $O(n^{2-\eps})$ time, for some $c,\eps > 0$, unless SETH is false.     
\end{lemma}
We will show how to solve the problem of \cref{lem:OVA} using a data structure of \cref{prb:lb} with $N=O(d\cdot n^{1.5})$ and $O(d\cdot n)$ updates.

\para{Overview.}
Let $A$ be a set of $n$ Boolean $d = d(n)$-dimensional vectors.  
The string $S$ consists of three components, which we refer to as the dictionary, the vector, and the matrix.  
The dictionary is defined as a function of $n$, independent of the vectors in $A$ or $B$.  
The matrix represents all vectors in $A$, while the vector component at any given time represents a single $\ourvec{u} \in B$.  
Our goal is to encode $A$ and some $\ourvec{u} \in B$ such that if all vectors in $A$ are non-orthogonal to $\ourvec{u}$, then $|\LZss(S)|$ attains a predetermined value.  
Conversely, if there exists at least one $\ourvec{v} \in A$ such that $\ourvec{u} \cdot \ourvec{v} = 0$, then $|\LZss(S)|$ is strictly smaller.  
We introduce a representation that encodes each $\ourvec{v} \in A$ separately.  
The number of phrases in $\LZss(S)$ contributed by the encoding of $\ourvec{v}$ is $d+1$ if $\ourvec{u} \cdot \ourvec{v} = 0$ and $d+2$ otherwise.  
(In particular, this allows the algorithm not only to detect the existence of an orthogonal pair but also to count the number of such pairs.)
Finally, transitioning the representation from one vector in $B$ to another requires at most $O(d)$ updates.  
Thus, iterating over all vectors in $B$ involves at most $O(d \cdot n)$ updates.  



\para{The reduction.}
Without loss of generality, we assume that  $\sqrt n$ is an integer number.
We first explain how to represent the set $A$.
For any $i\in[1,n]$ let $(i_1,i_2)\in[0..\sqrt n]\times [0..\sqrt n-1]$ be the two unique integers such that $i=i_1\cdot \sqrt n+i_2$.
We start by encoding of a single entry in a vector of $A$.
Let $\ourvec{v}_1,\ourvec{v}_2,\dots. \ourvec{v}_n$ be the vectors of $A$ in some arbitrary order and let $\ourvec{v}_i[j]$ denote the $j$th entry of $\ourvec{v}_i$. 
$\ourvec{v}_i[j]$ is represented by \[\ACoord(i,j)= \begin{cases}
        0_j{}^{3\sqrt n+i_1}2_j{}^{i_2} & \ourvec{v}_i[j]=0\\ 
        1_j{}^{3\sqrt n+i_1}2_j{}^{i_2}  & \ourvec{v}_i[j]=1
\end{cases}.\]

The representation of the $i$th vector of $A$ is obtained by concatenating the representation of all its coordinates by \[    \AVect(i)=\bigodot_{j=1}^d \ACoord(i,j).\]

Finally, to represent the set $A$ we concatenate the representations of all the vectors. 
We use $\#$ to denote a unique character that appears only once in $S$ (every time we use $\#$ it means a completely new symbol).
Thus, \[\Mat(A)=\bigodot_{i=1}^n \left(\AVect(i)\#\right).\]

The dictionary is made up of two types of gadgets.
The types, called \textit{skip-$0$} and \textit{skip-two-halves} gadgets, defined for every $j\in[d], i_2\in[0..\sqrt n],(x,y)\in\{0,1\}^2$ as 
\[\skipo(j)=0_j{}^{4\sqrt n}2_j{}^{\sqrt n}\]
\[\skiphalves(j,i_2,x,y)=x_j{}^{2\sqrt n}2_j{}^{i_2}y_{j+1}{}^{2\sqrt n}.\]
We define the dictionary as 
\[ \Dict=\left(\bigodot_{j=1}^d\skipo(j)\#\right)\cdot \left(
\bigodot_{j=1}^d\bigodot_{i_2=0}^{\sqrt n}\bigodot_{(x,y)\in\{0,1\}^2}\skiphalves(j,i_2,x,y)\# 
\right).\]
A coordinate of a vector $\ourvec{u}\in B$ is encoded by \[\BCoord(\ourvec{u},j)= \begin{cases}
        1_j{}^{4\sqrt n+1}2_j{}^{\sqrt n} & \ourvec{u}[j]=0\\ 
        1_j{}^{2\sqrt n}{\#}1_j{}^{2\sqrt n}2_j{}^{\sqrt n}  & \ourvec{u}[j]=1
\end{cases}.\]
Thus, a vector $\ourvec{u}\in B$ is encoded by \[\BVect(\ourvec{u})=\bigodot_{j=1}^d \BCoord(\ourvec{u},j)\#.\]

As described above, the dynamic string $S(\ourvec{u})$ when testing a vector $\ourvec{u}\in B$ is a concatenation of the three main gadgets: \[S(\ourvec{u})=\Dict\cdot \BVect(\ourvec{u})\cdot \Mat(A).\]



\subsection{Analysis}
In our analysis, we make use of the concept of $\LZss$-like factorization.
\begin{definition}[cf. \cite{LZ76}]
    A factorization $F$ of a string $S$ into disjoint substrings is $\LZss$-like if every substring in $F$ is either a single character, or a substring that is not a left-most occurrence of itself in $S$.
\end{definition}

We extensively use the following lemma by Lempel and Ziv~\cite{LZ76}, that says that the \LZss{} factorization is optimal.
\begin{fact}[{\cite[{Theorem 1}]{LZ76}}]\label{thm:LZ_optimal}
For any strings $S$, the \LZss{} factorization of $S$ has the smallest number of phrases among all \LZss-like factorizations of $S$.
\end{fact}

We apply \cref{thm:LZ_optimal} in the following manner.
Let $S = X \# G \# Y$, where $\#$ are unique symbols.  
Due to the uniqueness of $\#$ in $S$, an \LZss{} phrase starts both before, and after $G$ in $\LZss(S)$.
Consider the phrases in $\LZss(S)$ covering $G$, assume there are $z'$ such phrases.
By \cref{thm:LZ_optimal}, any partition of $G$ into $\LZss$-like phrases uses at least $z$ phrases (as otherwise, one can 'replace' the phrases of $\LZss(S)$ covering $G$ with  $z'<z$ phrases, obtaining a strictly smaller $\LZss$-like factorization of $S$).
We conclude this discussion with the following.

\begin{corollary}\label{lem:LZ_optimal}
    Let $G$ be a substring surrounded by unique $\#$ symbols in some string $S$, i.e. $S= X \# G \# Y$.
    Let $z$ be the number of $\LZss$-phrases in $\LZss(S)$ that cover $G\#$, and let $z'$ be the number of factors covering $G\#$ in an $\LZss$-like factorization of $S$.
    Then it must hold that $z \le z'$.
\end{corollary}


Let $\ourvec{u}\in B$.
For every $i \in [0..n]$ we define $\Mat_i(A)=\bigodot_{k=1}^i\left(\AVect(k)\#\right)$ is the prefix of $\Mat(A)$ until the end of the representation of the $i$th vector (including the $\#$).
Moreover, let $S_i(\ourvec{u})=\Dict\cdot\BVect(\ourvec{u})\cdot\Mat_i(A)$ be the prefix of $S(\ourvec{u})$ until the end of the representation of the $i$th vector (including the $\#$).

We define $\Phrases(S(\ourvec{u}),i)$ to be the number of phrases covering $\AVect(i)\#$  in $\LZss(S(\ourvec{u}))$.
Formally, $\Phrases(S(\ourvec{u}),i)=|\LZss(S_i(\ourvec{u}))|-|\LZss(S_{i-1}(\ourvec{u}))|$.
Notice that due to the usage of $\#$ separators, the phrases covering $\AVect(i)\#$ are disjoint from the phrases covering $\AVect(k)\#$  for every $k\ne i$.


\begin{lemma}\label{lem:phrases}
    For every $i\in [n]$ it holds that $\Phrases(S(\ourvec{u}),i)= d+1 + \ourvec{u}\cdot \ourvec{v}_i$.
\end{lemma}
\begin{proof}
    We start by showing that $\Phrases(S(\ourvec{u}),i)\le d+2$.
    \begin{claim}\label{clm:atmostd2}
        $\Phrases(S(\ourvec{u}),i)\le d+2$.
    \end{claim}
    \begin{claimproof}
        We introduce an $\LZss$-like factorization of $\AVect(i)\#$ into $d+2$ phrases, thus by \cref{lem:LZ_optimal} the claim follows.
        Let $P_1=(\ourvec{v}_i[1])_1{}^{2\sqrt n}$ (that is, $P_1= \begin{cases}
        0_1{}^{2\sqrt n} & \ourvec{v}_i[j]=0\\ 
        1_1{}^{2\sqrt n} & \ourvec{v}_i[j]=1
\end{cases}$).
        For every $j\in [2..d]$ let $P_j=(\ourvec{v}_i[j-1])_{j-1}{}^{\sqrt n+i_1}2_{j-1}{}^{i_2}(\ourvec{v}_i[j])_{j}{}^{2\sqrt n}$, and let $P_{d+1}=(\ourvec{v}_i[d])_{d}{}^{\sqrt n+i_1}2_d{}^{i_2}$ and $P_{d+2}=\#$.
        Notice that for $P_1$ is a substring of $\skiphalves(1,i,\ourvec{v}_i[1],0)$ and for every $j\in [2..d+1]$ we have $P_j$ is a substring of $\skiphalves(j-1,i,\ourvec{v}_i[j-1],\ourvec{v}_i[j])$ and $\#$ is a single (fresh) character.
        Moreover, $\bigodot_{j=1}^{d+2}P_j=\AVect(i)\#$.
        Since all gadgets $\skiphalves$ appear in $S(\ourvec{u})$ before $\AVect(i)$, these are valid $\LZss$-like phrases.
        Therefore, $P_1,P_2,\dots,P_{d+2}$ is indeed a valid $\LZss$-like factorization of $\AVect(i)\#$ of length $d+2$.
    \end{claimproof}

    The following shows that every $\ACoord(i,j)$ gadget induces at least one unique phrase.
    \begin{claim}\label{clm:no-superstring}
        For any $j\in [1..d]$ there is no $\LZss$-like phrase of $S(\ourvec{u})$ covering part of $\AVect(i)\#$ that is a proper superstring of $\ACoord(i,j)$.
    \end{claim}
    \begin{claimproof}
        Notice that any proper superstring of $\ACoord(i,j)$ is a superstring of either $x_{j-1}\ACoord(i,j)$ or $\ACoord(i,j)x_{j+1}$ for $x\in\{0,1,2\}$ (the case where the following or preceding character is $\#$ is clearly impossible).
        We will prove that the claim for superstrings of $\ACoord(i,j)x_{j+1}$, the proof for $x_{j-1}\ACoord(i,j)$ is symmetric.
        First, note that for every $x,y\in\{0,1,2\}$, $\ACoord(i,j)x_{j+1}$ is not a substring of $\BVect(\ourvec{u})$ or $\Dict$.
        This is because the only gadgets (in $\BVect(\ourvec{u})$ or $\Dict$) that contain both $y_j$ and $x_{j+1}$ characters are $\skiphalves$ gadgets and none of them contain more than $2\sqrt n$ occurrences of $(\ourvec{v}_i[j])_j$.

        Finally, for every $k<i$ the gadget $\ACoord(i,j)$ is not a substring of $\AVect(k)$.
        This is because the only substring of $\AVect(k)$ that contains characters $y_j$ for $y\in\{0,1,2\}$ is $\ACoord(k,j)$.
        Since $k<i$ it must be that either $k_1<i_1$ or $k_2<i_2$.
        Therefore, $\ACoord(k,j)$ either does not contain enough $(\ourvec{v}_i[j])_j$ characters, or it does not contain enough $2_j$ characters to be a superstring of $\ACoord(i,j)$. 
    \end{claimproof}

    The following claim immediately follows from \cref{clm:no-superstring} since at least one phrase starts in every $\ACoord$ gadget and an additional phrase for $\#$.

    \begin{claim}\label{clm:atleastd1}
        $\Phrases(S(\ourvec{u}),i)\ge d+1$ 
    \end{claim}

    
The following two claims complete the proof of the lemma.
\begin{claim}
        If $\ourvec{u}\cdot \ourvec{v}_i=0$ then $\Phrases(S(\ourvec{u}),i)=d+1$.
    \end{claim}
    \begin{claimproof}
        Let $P_j=\ACoord(i,j)$ and $P_{d+1}=\#$.
        Notice that $\AVect(i,j)\#=\bigodot_{j=1}^{d+1}P_j$.
        We will show that for every $j\in[1..d]$ the string $P_j$ occurs in $\Dict\cdot\BVect(\ourvec{u})$ (which is a prefix of $S(\ourvec{u})$).
        Let $j\in [1..d]$. 
        If $\ourvec{v}_i[j]=0$, then $P_j=\ACoord(i,j)=0_j{}^{3\sqrt n+i_1}2_j{}^{i_2}$ is a substring of $\skipo(j)=0_j{}^{4\sqrt n}2_j{}^{\sqrt n}$.
        Otherwise, if $\ourvec{v}_i[j]=1$ then it must be that $\ourvec{u}[j]=0$ (since $\ourvec{u}\cdot \ourvec{v}_i=0$) and therefore $P_j=\ACoord(i,j)=1_j{}^{3\sqrt n+i_1}2_j{}^{i_2}$ is a substring of $\BCoord(\ourvec{u},j)=1_j{}^{4\sqrt n+1}2_j{}^{\sqrt n}$.
        We have shown that $P_1,P_2,\dots,P_{d+1}$ is an $\LZss$-like factorization of $\AVect(i)$, thus by \cref{lem:LZ_optimal} we have $\Phrases(S(\ourvec{u}),i)\le d+1$.
        Combining with \cref{clm:atleastd1} we have $\Phrases(S(\ourvec{u}),i)= d+1$, as required.
    \end{claimproof}
    \begin{claim}
        If $\ourvec{u}\cdot \ourvec{v}_i=1$ then $\Phrases(S(\ourvec{u}),i)=d+2$.
    \end{claim}
    \begin{claimproof}
        Assume to the contrary that $\Phrases(S(\ourvec{u}),i)\ne d+2$.
        By \cref{clm:atmostd2,clm:atleastd1} it must be that $\Phrases(S(\ourvec{u}),i)=d+1$.
        Clearly, the last phrase is exactly $\#$.
        By \cref{clm:no-superstring} it must be that the remaining $d$ phrases are exactly $\ACoord(i,j)$ for $j\in[1..d]$.
        Let $j\in [1..d]$ be a coordinate such that $\ourvec{u}[j]=\ourvec{v}_i[j]=1$ (such a $j$ must exist due to $\ourvec{u}\cdot \ourvec{v}_i=1$).
        We will show that $\ACoord(i,j)=1_j{}^{3\sqrt n+i_1}2_j{}^{i_2}$ does not occur in $S(\ourvec{u})$ before $\AVect(i)\#$.
        Consider all occurrences of $X=1_j{}^{3\sqrt n}$ is $S(\ourvec{u})$ before $\AVect(i)$.
        Clearly, $X$ does not occur in $\Dict$.
        The only occurrence of $1_j$ in $\BVect(\ourvec{u})$ is in $\BCoord(\ourvec{u},j)$, which in this case is $\BCoord(\ourvec{u},j)=1_j{}^{2\sqrt n}{\#}1_j{}^{2\sqrt n}2_j{}^{\sqrt n}$ which does not contain an occurrence of $X$.
        
        Finally, for every $k<i$ the gadget $\ACoord(i,j)$ is not a substring of $\AVect(k)$.
        This is because the only substring of $\AVect(k)$ that may contain  $1_j$ is $\ACoord(k,j)$.
        Since $k<i$ it must be that either $k_1<i_1$ or $k_2<i_2$.
        Therefore, $\ACoord(k,j)$ either does not contain enough $1_j$ characters, or it does not contain enough $2_j$ characters to be a superstring of $\ACoord(i,j)$. 

        To conclude $\ACoord(i,j)$ cannot be an $\LZss$ phrase, a contradiction.
    \end{claimproof}
    Thus, the lemma is proven.
\end{proof}




For a vector $\ourvec{u}$, we denote $S'(\ourvec{u})=\Dict\cdot\BVect(\ourvec{u})$.
That is, $S'(\ourvec{u})$ is the prefix of $S(\ourvec{u})$ obtained by removing the suffix $\Mat(A)$.
\begin{lemma}\label{lem:allPhrases}
    $|\LZss(S(\ourvec{u}))|-|\LZss(S'(\ourvec{u}))|=(d+2)n$ if and only if $u$ is not orthogonal to any $v\in A$.
\end{lemma}
\begin{proof}
    By definition of $\Phrases$ and by the $\#$ separators we have \[|\LZss(S(\ourvec{u}))|-|\LZss(\Dict\cdot\BVect(\ourvec{u}))|=\sum_{i=1}^n\Phrases(S(\ourvec{u}),i).\]
    The lemma follows immediately from \cref{lem:phrases}.
\end{proof}

We are ready to prove \cref{thm:lb}.

\begin{proof}[Proof of \cref{thm:lb}]
Assume by contradiction the there is an algorithm $D$ that solves \cref{prb:lb} with $O(N^c)$ preprocessing time and $O(N^{2/3-\eps})$ update time.
We will show how to exploit $D$ to preprocess a set $A$ of $n$ binary $d$ dimensional vectors in $O(n^{1.5c})$ such that given a set $B$ of $n$ vectors, we can solve $\OV$ on $A,B$ in $O(\mathsf{poly}(d)\cdot n^{2-1.5\eps})$.
Thus, by \cref{lem:OVA} the theorem follows.

Given the set $A$ we construct two strings: $S=S(\ourvec{1})=\Dict\cdot \BVect(\ourvec{1})\cdot \Mat(A)$ and $S'=\Dict\cdot \BVect(\ourvec{1})$, and initialize two instance of $D$, denoted by $D_S$ and $D_{S'}$.
When receiving a set $B$ of vectors we iterate over all $\ourvec{u}\in B$.
For every $\ourvec{u}\in B$ we apply at most $d$ updates to transform $\BVect(\ourvec{1})$ to $\BVect(\ourvec{u})$ both in $D_S$ and $D_{S'}$ (notice that to transform $\BCoord(\ourvec 1,j)$ to $\BCoord(\ourvec{u},j)$ a single substitution is sufficient). 
After all updates are applied we have $S=S(\ourvec{u})$ and $S'=S'(\ourvec{u})$.
At this point $D_S$ and $D_{S'}$ report $|\LZss(S(\ourvec{u}))|$ and $|\LZss(S'(\ourvec{u}))|$, respectively.
Thus, the algorithm checks if $|\LZss(S(\ourvec{u}))|-|\LZss(S'(\ourvec{u}))|<(d+2)\cdot n$, and if so reports that $A$ and $B$ contain a pair of orthogonal vectors.
Otherwise, the algorithm updates $S$ and $S'$ to be $S(\ourvec{1})$ and $S'(\ourvec{1})$, undoing all the substitutions.
If when finishing the scan of $B$ it was not reported that $A$ and $B$ contain a pair of orthogonal vectors, the algorithm reports that no such pair exists.
The correctness of the algorithm follows immediately from \cref{lem:allPhrases}.

\para{Complexity.}
Notice that the lengths of the strings are 
$|\Dict|=O(\sqrt n\cdot d\cdot \sqrt n)=O(dn)$, $|\BVect(\ourvec{1})|=O(d\sqrt n)$ and
$|\Mat(A)|=O(n\cdot d \cdot \sqrt n)=O(d\cdot n^{1.5})$.
Therefore $|S|,|S'|\in O(d\cdot n^{1.5})$.
It immediately follows that the preprocess of $A$ takes $O(|S|^c+|S'|^c)=O(d^cn^{1.5c})$.
Moreover, the algorithm processes every vector $\ourvec{u}\in B$ with $O(d)$ updates, summing up to $O(d\cdot n)$ updates which take in total $O(d\cdot n\cdot (d\cdot n^{1.5})^{2/3-\eps})=O(d^{5/3-\eps}n^{2-1.5\eps})$.
By taking $d\in \Theta(2^{\sqrt{\log n}})$ and assuming SETH, this contradicts \cref{lem:OVA}.   
\end{proof}




\bibliography{ref}
\appendix



\section{Implementation of a Dynamic LPF-tree using Top Trees}\label{sec:top_trees}

In this section, we reproduce the proof from \cite{BCR24} for completeness, showing that the $\LPF$-tree operations can be performed in $\Otild(1)$ time.
We replace link-cut trees with top trees\cite{AHLT05}, as they support the same operations as link-cut trees, but top trees guarantee $\Otild(1)$ worst-case time complexity.
The operations supported by top trees are as follows.
\begin{enumerate}
    \item insert a single node to the forest.
    \item remove a leaf from a tree.
    \item attach a root to another node as a child.
    \item detach a node from its parent.
    \item add a weight $x$ to a node and all of its descendants.
    \item get the weight of a given node.
    \item get the $i$'th ancestor of a node.
\end{enumerate}

As in \cite{BCR24}, we use Red-black (RB) trees, that are balanced search trees supporting the following operations.
\begin{enumerate}
    \item Insert an element to an RB tree
    \item Delete an element from an RB tree
    \item Join two RB trees $T_1,T_2$ into a single RB containing the values of both, under the promise that the maximal value in $T_1$ is smaller than the minimal value in $T_2$.
    \item Given a value $c$ and a RB tree $T$, split $T$ into two RB trees $T_1$ and $T_2$ such that $T_1$ contains all values smaller than $c$ and the rest of the values are in $T_2$. 
\end{enumerate}
All these operations cost $\Otild(1)$ time.

The following observation is useful.

\begin{observation}[Observation 23 in \cite{BCR24}]\label{obs:23}
    A collection of red-black trees (RB trees) on $n$ nodes can be simulated using top trees.
The cost of every operation on an RB tree is then $O(\log^2 n)$.
\end{observation}

The indices of the string $S$ are stored as nodes of one top tree.
For every node $i$, the children of $i$ are stored in an RB tree, where only the root of the RB tree is linked to node $i$.
Internal RB tree edges have weight 0, and the rest have weight 1.
Hence, for example, the weight of a path from node 1 to the root is $|LZ77(S)|$.
Let the weight of a node $i$ be the weight of the path from $i$ to root.

$\Insert, \Link,\Delete$ are simple top tree operations, and cost $\Otild(1)$ by \cref{obs:23}.
For the $\MoveInterval$ operation, we need to move consecutive nodes from an RB tree to another RB tree.
This is done in $\Otild(1)$ by \cref{obs:23}.
In addition, the weight of these nodes should be updated.
This is done in $O(1)$ calls to get-weight and add-weight.
Therefore, $\MoveInterval$ costs $\Otild(1)$ time.


$\Insert, \Link,\Delete$ require the move of a node from one RB-tree to another RB-tree in our top tree.
This costs $\Otild(1)$ time by \cref{obs:23}.
$\GetHeight, \findIAncestor$ are simple top tree queries.

\section{Implementation of Dynamic Substring Pattern Matching}\label{sec:DynSPM}

In this section, we provide the following data structure for the Dynamic Substring Pattern Matching problem.
\subPMDS*


For a string $S[1..n]$, the suffix array of $S$ is an array of length $n$ that contains all the suffixes of $S$ in their lexicographic order.
The suffix $S[i..n]$ is represented in $S$ by its starting index $i$.

We make use of the following data structure.
\begin{lemma}[ \cite{kempa2022dynamic}, cf. Theorem 10.16]\label{lem:DymSA}
There is a data structure for maintaining a dynamic string $S$ that undergoes edit operations and supports the following query in $\Otild(1)$ time:
Given an index $i$, return $SA_S[i]$.

The update time of the data structure is $\Otild(1)$ and the construction time for an initial string of length $n$ is $\Otild(n)$.
\end{lemma}

It is well-known that the suffix array can be used to search a pattern in a text.
We bring the following observation to describe the exact framework in which we use suffix arrays for text searching.
\begin{observation}\label{obs:SAandLCPisPM}
There is an algorithm that decides if a pattern $P$ occurs in a text $T$ of length $n$ using $O(\log n)$ queries of the form $\LCP(P, T[i..n])$ for some $i\in [|T|]$ and $O(\log n)$ queries to $SA_T$, and $O(\log n)$ additional time.
\end{observation}
\begin{proof}
    One can binary search for an occurrence of $P$ in $T$.
    At every point, the algorithm checks if there is a starting index of an occurrence of $P$ in $SA_T[i..j]$ for some $i$ and $j$ initially set as $i=1$ and $j=|T|$.
    If $i = j$, the algorithm simply checks if $x=SA_T[i]$ is a starting index of an occurrence of $P$ by querying $\LCP(P,T[x..n])$.
    Otherwise, the algorithm sets $c = \left \lceil{\frac{j-i}{2}}\right \rceil $, obtains $x =SA_T[c]$ and queries for $\ell = \LCP(P,T[x..n])$.
    If $\ell = |P|$, we have found an occurrence of $P$ in $T$.
    Otherwise, the lexicographical order between $P$ and $T[x..n]$ is decided by the order between $P[\ell]$ and $T[x+\ell]$.
    We proceed our search in the half of $SA_T$ that may contain an occurrence of $P$.
    Clearly, the binary search terminates in $O(\log(|T|)$ steps.
    In each step, the algorithm executes a single $\LCP$ query and a single query to $SA_T$.
\end{proof}

We are ready to describe our data structure for \cref{lem:subPMDS}.
We describe a data structure with amortized update time.
The running time can be de-amortized using standard techniques.
We maintain the dynamic data structure of \cref{lem:dympillar} on $S$.

Additionally, for every $i\in [0..\left \lceil{\log |S|}\right \rceil]$, we are interested in maintaining a partition $\mathcal{P}^i$ of the indices of $[|S|]$ into intervals $I^i_1,I^i_2, \ldots I^i_{|\mathcal{P}^i|}$ with the following properties.
\begin{enumerate}
    \item For every $j\in [|\mathcal{P}^i|]$, $|I^i_j| < 2^{i+1}$
    \item For every $j\in [|\mathcal{P}^i|-1]$, $|I^i_{j}| + |I^i_{j+1}| \ge 2^i$
\end{enumerate}
Every interval $I^i_j = [a^i_j..b^i_j]$ corresponds to a substring $S^i_j= S[a^i_j..b^i_j]$.
For every $S^i_j$ the algorithm stores the structure of \cref{lem:DymSA} allowing queries for $SA_{S^i_j}$.

We maintain the intervals as follows.
Initially, $\mathcal{P}^i$ is simply a partition of $[1..|S|]$ into disjoint intervals of length exactly $2^i$ (excluding a last interval that may be shorter).
When an insertion (resp. deletion) is applied at index $x$ in $S$, the algorithm increases (resp. reduces) the ending index of $I^i_j$ that contains $x$ by $1$ (and implicitly shift all intervals following $I^i_j$).
The algorithm also applies the appropriate insertion (resp. deletion) to the dynamic data structure of $S^i_j$.

If the update results in the length of $I^i_j$ reaching $2^{i+1}$, the algorithm splits $I^i_j$ into two intervals $I^i_j$ and $I^i_{j+1}$ with length $2^i$ each (thus shifting all intervals following $I^i_j$ prior to the update).
Then, the algorithm constructs the data structure of \cref{lem:DymSA} for $S^i_{j}$ and for $S^{i}_{j+1}$ from scratch.
Similarly, if the update results in two adjacent intervals $I^i_j$ and $I^i_{j+1}$ with combined length that exactly $2^{i}-1$, the algorithm merges $I^i_j$ and $I^i_{j+1}$ into a single interval $I^i_j$ with length $2^i-1$, and computes $S^i_j$ for the new $I^i_j$ from scratch.

It should be clear that all invariants are maintained and that for every interval $I^i_j$ the data structure of \cref{lem:DymSA} is maintained for the corresponding $S^i_j$.

Finding the interval $I^i_j$ can be implemented in $O(\log n)$ time by storing the dynamic endpoints of the intervals in a balanced search tree.
Then, applying a constant number of updates for the data structure of \cref{lem:DymSA} takes $\Otild(1)$ time.
The only case when we apply more then a single operation to the data structure of \cref{lem:DymSA} is when we merge or split intervals.

The newly created intervals, either merged or splitted, are always of size $O(2^i)$, so initializing the data structure of \cref{lem:DymSA} can be implemented by initializing an empty data structure and applying $O(2^i)$ insertions, which requires $\Otild(2^i)$ time.
We make a standard charging argument to account for this running time.

When an interval $I^i_j$ is split, its length is $2^{i+1}$, meaning that at least $2^i-1$ insertion operations were applied to the interval since its creation.
We can charge the $\Otild(2^i)$ cost of computing $S^i_j$ and $S^i_{j+1}$ from scratch on these $2^i$ edit operations for an amortized cost of $\Otild(1)$.
Similarly, when two intervals $I^i_j$,$I^i_{j+1}$ are merged, one of $I^i_j$,$I^i_{j+1}$ must be of length at most $2^{i-1} - 1$.
It follows that at least $2^{i-1} + 1$ deletion operations were applied to the shorter of the two merged intervals since its creation.
Again, we can charge the $\Otild(2^i)$ cost of constructing the data structure of \cref{lem:DymSA} for the merged inteval on these deletions, resulting in $\Otild(1)$ amortized running time.

The only part of the algorithm that is not worst-case but amortized is the construction of the suffix array data structure for new intervals.

we can modify the algorithm to de-amortize part by employing the standard technique of
gradually constructing the data structure for future intervals in advance.

We show how to implement this approach to de-amortize the running time for splitting an interval.
A similar de-amortization can be applies to de-amortize the running time for merging intervals.

Every Interval $I^i_j$ stores, in addition to the data structure of $S^i_j=S[a^i_j..b^i_j]$, additional two suffix array data structures $SA^i_j(1)$ and $SA^i_j(2)$, for the strings $S^i_j(1) = S[a^i_j..c]$ and $S^i_j(2) = S[c+1..b^i_j]$ for $c = \floor*{\frac{a+b}{2}}$, respectively.
To be more precise, $SA^i_j(1)$ and $SA^i_j(2)$ are \textit{under construction}, and we wish to have the property that if $I^i_j$ is spitted, $SA^i_j(1)$ and $SA^i_j(2)$ are the proper suffix array data structures of the newly created intervals.
We also want $I^i_j$ to store a copy of the string $S^i_j$ in its state when the interval $I^i_j$ was created.

When $I^i_j$ is created, both $SA^i_j(1)$ and $SA^i_j(2)$ data structures are initialized as suffix array data structures for the empty string in $\Otild(1)$ time.
The interval $I^i_j$ also initializes an empty list $U^i_j$ meant for storing all future insertions, substitutions, and deletions applied to $I^i_j$.

Denote $S^i_j(1,init)$ (resp. $S^i_j(2,init)$) as the content of $S^i_j(1)$ (resp $S^i_j(2)$) at the time of the creation of the interval $I^i_j$.
Let us consider the following implementation of splitting an interval in the amortized data structure:
When the interval $I^i_j$ is split, the algorithm initializes the suffix array data structure for $SA^i_j(1)$ as a data structure for the empty string, inserts all symbols of $S^i_j(1,init)$ one by one an then applies all updates that $S^i_j(1)$ received during the lifetime of $I^i_j$ (a similar procedure is applied to obtain $SA^i_j(2)$).
Clearly, this correctly constructs $SA^i_j(1)$ and $SA^i_j(2)$.
This implementation requires $1+ 2^{i+1}+|U^i_j|$ operations of the dynamic suffix array data structure of \cref{lem:DymSA}.
We call this sequence of operations the \textit{construction sequence} of $I^i_j$.

Now, every time that an operation is applied to $I^i_j$, if $SA^i_j(1)$ and $SA^i_j(2)$ are already fully constructed for the current left and right halves of $S^i_j$, the algorithm applies the appropriate update to one of them.

Otherwise, $SA^i_j(1)$ and $SA^i_j(2)$ are still under construction.
The algorithm applies the next $8$ operations from the construction sequence of $I^i_j$ to $SA^i_j(1)$ and to $SA^i_j(2)$.
Note that these operations are known, as we have access to $S^i_j(1,init)$ and to $S^i_j(2,init)$ (which is sufficient for applying the first $2^{i+1}$ operations) and to $U^i_j$.
Recall that at least $2^{i-1}$ updates are applied to $I^i_j$ before it is split.
It is therefore guaranteed that by the time $I^i_j$ splits, the algorithm has already applied $8 |U^i_j| = 4|U^i_j| + 4|U^i_j| \ge 2^{i+2} + |U^i_j| + 1$ operations from the construction sequence.
Therefore, the construction sequence has already been executed, and $SA^i_j(1)$ and $SA^i_j(2)$ are already constructed as required.
We also need to have an already prepared, copy of the current $S^i_j(1)$ and $S^i_j(2)$, which could take $O(2^{i+1})$ time to write and store if done at the time of the split.
This can be de-amortized in the same manner.

Recall that the data structure maintains dynamic partition of intervals for every $i\in [\ceil*{\log n}+1]$.
Each partition is stored as a balanced search tree with delta representation, storing the starting indices of the intervals.
We make the following observation.
\begin{claim}\label{clm:intervalpartition}
    For every $a\le b \in [|S|]$, there is a set $\mathcal{I}_{a,b}$ of $O(\log n)$ intervals in $\cup_{i=1}^{\ceil*{\log n} + 1}\mathcal{P}^i$ such that $\cup _{I \in \mathcal{I}_{a,b}} I = [a..b]$.
    The set $\mathcal{I}_{a,b}$ can be found in $\Otild(1)$ time given $a$ and $b$.
\end{claim}
\begin{proof}
    We describe a greedy algorithm for finding $\mathcal{I}_{a,b}$.
    The algorithm initializes an empty list $L$ and an index $k=a$ and runs the following procedure until a termination condition is met.
    \begin{enumerate}
        \item \label{intpartition:1} If $k>b$, return $L$ as $\mathcal{I}_{a,b}$.
        \item \label{intpartition:2} Find the maximal $i$ such that the interval $I^i_j$ of $\mathcal{P}^i$ containing $k$ has $I^i_j \subseteq [a..b]$.
        \item \label{intpartition:3} Append $I^i_j$ to $L$ and set $k \leftarrow b^i_j+1$.
    \end{enumerate}

    First, we claim that $I^i_{j}$ is well defined for every $k\in [a..b]$.
    This follows from the fact that all intervals in $\mathcal{P}^0$ are of length exactly 1, therefore the interval $I^0_k=[k..k]$ is a feasible candidate for $I^i_j$.
    It follows that the algorithm terminates.
    It should be clear that at the beginning of every iteration of the algorithm, the list $L$ already contains a set of interval covering $[a..k-1]$, so when the algorithm terminates the list $L$ indeed satisfies $\cup_{I\in L}I=[a..b]$.

    Every iteration of the algorithm is implemented in $\Otild(1)$ time by querying each of the search trees of $\mathcal{P}^i$ for the interval containing $k$.
    It is left to prove that $|L|\in O(\log n)$ - notice that this also leads to the running time of the algorithm being $\Otild(1)$.
    
    Let $k_t$ be the value of $k$ at the beginning of the $t$'th iteration.
    Similarly, let $i_t$ be the maximal $i$ value found in Step \ref{intpartition:2} of the algorithm in the $t$'th iteration, and $I_t$ be the interval added to $L$ in the $t$'th iteration.
    
    We claim that for every $i\in [\ceil*{\log n}+1]$, $i$ can appear in the sequence $i_1,i_2 ,\ldots i_{|L|}$ at most 8 times - the bound on $|L|$ directly follows from this claim.

    Let us fix some $i\in [\ceil*{\log n}+1]$.
    Assume to the contrary that there exist $t_1<t_2<\ldots < t_9$ iterations of the algorithm in which $i_t = i$.
    Notice that for every $x< y\in [9]$ we have $I_{t_x}\neq I_{t_y}$.
    It follows from $k_{t_y} \ge k_{t_{x+1}}> b^i_{t_x}$ i.e. $I_{t_x}$ does not contain $k_y$.
    It immediately follows that the claim is correct for $i=\ceil*{\log n} + 1$ as $|\mathcal{P}^{\ceil*{\log n} + 1}|\le 2$.
    
    Let us assume that $i\in [\ceil*{\log n}]$.
    Recall that the length of two consecutive intervals in $\mathcal{P}^i$ is least $2^i$.
    Therefore, $k_{t_5} > k_{t_1} + 2^{i+1}$ and $k_{t_9} > k_{t_5} + 2^{i+1}$.
    Since the length of all intervals in $\mathcal{P}^{i+1}$ is less than $2^{i+1}$, there must be an interval  $I^{i+1}_j \in \mathcal{P}^{i+1}$ starting in $a^{i+1}_j \in [k_{t_1}..k_{t_5}]\subseteq [a..k_{t_5}]$.
    It follows from the fact that $i_{t_5} = i$ that $I^{i+1}_j$ is not contained in $[a..b]$, which means that $b^{i+1}_j >b$.
    From $|I^{i+1}_j| < 2^{i+1}$ we have $a^{i+1}_j + 2^{i+1} -1 > b^{i+1}_{j}>b$.
    It follows from $a^{i+1}_j \le k_{t_5}$ that $k_{t_5} >b - 2^{i+1} +1$.
    Finally, recall that $k_{t_{9}} > k_{t_5} + 2^{i+1}$, which in turn implies $k_{t_{9}}>b$, a contradiction ($i_{t_9}$ was selected as $i$, so the iteration $t_9$ is not be the halting iteration).
    \end{proof}

We now show how to answer the query $\SubPM{i_T}{j_T}{i_P}{j_P}$ given the partitions, and the data structure \cref{lem:dympillar}.
First, we apply \cref{clm:intervalpartition} to obtain a set $\mathcal{I}_{i_T,j_T}$ of intervals from the partitions such that the union of the intervals is exactly $S[i_T..j_T]$.
For every $I^i_j \in \mathcal{I}_{i_T,j_T}$, we can access entries of the suffix array of $S^i_j = S[a^i_j..b^i_j]$ in $\Otild(1)$ time, and we can query the $\LCP$ between indices in $S^i_j$ and $i_P$ via the data structure of \cref{lem:dympillar} in $\Otild(1)$ time.
It follows from \cref{obs:SAandLCPisPM} that we can report if there is an occurrence of $P= S[i_P..j_P]$ in $S^i_j$ in $\Otild(1)$ time.
We do this for each of the $\Otild(1)$ intervals of $\mathcal{I}_{i_T,j_T}$ for a total running time of $\Otild(1)$ and report that there is an occurrence of $P$ in $T = S[i_T..j_T]$ if one of the applications of \cref{obs:SAandLCPisPM} finds an occurrence.

Notice that this is insufficient: there may be an occurrence of $P$ in $T$ that is not contained in any of the intervals of $\mathcal{I}_{i_T,j_T}$.
We observe that every such 'missed' occurrence must contain an endpoint of an interval in $\mathcal{I}_{i_T,j_T}$.

We complement our algorithm with the following procedure.
For every $x\in \cup_{[a..b]\in \mathcal{I}_{i_T,j_T}}\{a,b\}$ (i.e. each endpoint of an interval of $\mathcal{I}_{i_T,j_T}$) we find a representation of all occurrences of $P$ in $S[\max(x-|P|..i_T),\min(x+|P|,j_T)]$ using an internal pattern matching query to the data structure of \cref{lem:dympillar}.
This adds another factor of $\Otild(1)$ to the time complexity.
It is easy to see that each occurrence of $P$ that contains an endpoint of an interval is found in this way, thus concluding the algorithm and proving \cref{lem:subPMDS}.

\end{document}

