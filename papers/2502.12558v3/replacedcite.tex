\section{Related Works}
\subsection{Video Understanding Benchmarks}
As shown in Table~\ref{tab:dataset_comparison}, current video retrieval benchmarks include MSR-VTT____, MSVD____, VATEX____, and ActivityNet____. These benchmarks evaluate cross-modal retrieval by using video captions as queries to rank videos within a candidate pool. Temporal grounding benchmarks like QVHighlights____, Charades-STA____, THUMOS14____, and DiDemo____ attempt to address moment retrieval but are limited to short videos (under three minutes). This leaves a gap in handling long-form content, where events may  require multi-step reasoning. 

There are also long-video understanding benchmarks, such as Video-MME____ and MLVU____. However, these benchmarks focus on video question answering and do not directly evaluate the representational capability of video encoders. In contrast, our proposed MomentSeeker benchmark is designed for long-video moment retrieval, assessing a model's retrieval ability within a single long video and addressing the limitations of existing benchmarks.

\subsection{Multi-modal Embedding Models}

Most video retrievers____ use a dual-tower architecture like CLIP____ and train with contrastive learning on large datasets. InternVideo____ integrates generative and discriminative self-supervised learning via masked video modeling and contrastive learning. InternVideo2____ expands upon this by incorporating larger models, featuring an encoder with up to 6 billion parameters. LanguageBind____ aligns multiple modalities in a shared language space, while VAST____ integrates vision, audio, and text encoders to have a thorough understanding. These models capture spatiotemporal information, forming the basis of video understanding.

In image retrieval field, several studies____ have explored multi-modal input for composed image retrieval. For example,  MagicLens____ uses open-ended instructions to capture complex semantic relationships and trains a composed image retriever, while MM-Ret____ explores further with more effective training data. Inspired by this, CoVR____ applies the same pipeline in video retrieval, generating video-text-video triplets from paired videos with similar captions, and provides a BLIP-style____ composed video retriever. These advancements expand the capabilities of foundation models in multi-modal understanding, broadening task adaptability.

Furthermore, with the rise of large language models, some works attempt to transform multi-modal large models (MLLMs) into universal embedders to construct more powerful representations, such as E5-V____ and VLM2VEC____. However, in the video domain, a robust universal embedder has yet to emerge.  For this reason, we propose V-Embedder, which outperforms existing video embedders across multiple tasks through synthetic-data-driven training.