\section{Related Works}
\subsection{Video Understanding Benchmarks}
As shown in Table~\ref{tab:dataset_comparison}, current video retrieval benchmarks include MSR-VTT~\cite{msrvtt}, MSVD~\cite{msvd}, VATEX~\cite{vatex}, and ActivityNet~\cite{2015ActivityNet}. These benchmarks evaluate cross-modal retrieval by using video captions as queries to rank videos within a candidate pool. Temporal grounding benchmarks like QVHighlights~\cite{qvhighlights}, Charades-STA~\cite{gao2017charadeSTA}, THUMOS14~\cite{THUMOS14}, and DiDemo~\cite{hendricks2017didemo} attempt to address moment retrieval but are limited to short videos (under three minutes). This leaves a gap in handling long-form content, where events may  require multi-step reasoning. 

There are also long-video understanding benchmarks, such as Video-MME~\cite{fu2024videomme} and MLVU~\cite{zhou2025mlvu}. However, these benchmarks focus on video question answering and do not directly evaluate the representational capability of video encoders. In contrast, our proposed MomentSeeker benchmark is designed for long-video moment retrieval, assessing a model's retrieval ability within a single long video and addressing the limitations of existing benchmarks.

\subsection{Multi-modal Embedding Models}

Most video retrievers~\cite{li2022uniformer,wang2022internvideo,zhu2023languagebind} use a dual-tower architecture like CLIP~\cite{radford2021clip} and train with contrastive learning on large datasets. InternVideo~\cite{wang2022InterVideo} integrates generative and discriminative self-supervised learning via masked video modeling and contrastive learning. InternVideo2~\cite{wang2025internvideo2} expands upon this by incorporating larger models, featuring an encoder with up to 6 billion parameters. LanguageBind~\cite{zhu2023languagebind} aligns multiple modalities in a shared language space, while VAST~\cite{chen2023vast} integrates vision, audio, and text encoders to have a thorough understanding. These models capture spatiotemporal information, forming the basis of video understanding.

In image retrieval field, several studies~\cite{zhang2024magiclens,zhou2024megapairs,wei2025uniir} have explored multi-modal input for composed image retrieval. For example,  MagicLens~\cite{zhang2024magiclens} uses open-ended instructions to capture complex semantic relationships and trains a composed image retriever, while MM-Ret~\cite{zhou2024megapairs} explores further with more effective training data. Inspired by this, CoVR~\cite{ventura2024covr} applies the same pipeline in video retrieval, generating video-text-video triplets from paired videos with similar captions, and provides a BLIP-style~\cite{li2022blip} composed video retriever. These advancements expand the capabilities of foundation models in multi-modal understanding, broadening task adaptability.

Furthermore, with the rise of large language models, some works attempt to transform multi-modal large models (MLLMs) into universal embedders to construct more powerful representations, such as E5-V~\cite{jiang2024e5} and VLM2VEC~\cite{jiang2025vlm2vec}. However, in the video domain, a robust universal embedder has yet to emerge.  For this reason, we propose V-Embedder, which outperforms existing video embedders across multiple tasks through synthetic-data-driven training.