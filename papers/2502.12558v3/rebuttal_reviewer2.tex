\documentclass[11pt]{article}
\begin{document}
\section{Responses of Weaknesses}
\subsection{R1}
We sincerely appreciate the feedback and apologize for the lack of clarity. Below, we provide a detailed explanation of V-Embedder:  

Our model builds on InternVideo2-Chat, a video-centric multimodal LLM with a video encoder + projector + LLM architecture.  
- Input Processing:  
  - Video: Encoded by the video encoder (spatiotemporal features) and projected to the LLM's latent space via the projector.  
  - Text: Directly embedded into the LLM's space using its tokenizer.  
  - Image: Treated as a single-frame video, inheriting the video processing pipeline.  
- Retrieval Mechanism:Following [1,2], we prepend task-specific prompts (e.g., "Represent the given question in one word.") to the LLM input and extract the last token's hidden state as the embedding. Cosine similarity between query (T / I+T / V+T) and video candidate embeddings is used for ranking.  
- Key Innovation:V-Embedder is the first work to leverage a video-centric VLM as a universal multimodal embedder. Unlike prior methods that rely on fixed encoders, our model inherits the VLM's instruction-following capability through synthetic training data (detailed in Section 4), enabling strong generalization across tasks.  

We will add a framework diagram in the appendix to visually clarify the architecture and training pipeline.  

\subsection{R2}
We categorized baselines into three groups based on their relevance:  
1. CLIP-Style Video Retrievers:  
  1.  General video Retrievers (LanguageBind, InternVideo2)
    1. Input Handling: Uniformly sample 8 frames for video encoding; text is encoded separately. For multimodal queries (e.g., V+T), we average video and text embeddings as in [3,4].  
    2. Rationale: These dominate general video retrieval due to their strong multimodal alignment from pretraining, explaining their high CA performance. However, experiments show that their lack of instruction tuning limits performance on compositional tasks (MS/IMS/VMS).  

  2. Image-Based Retrievers (MM-RET, MagicLens, UniIR):  
    1. Input Handling: Use the video's middle frame as image input. Following their original setting, text is fused with image features via average pooling.
    2. Rationale: These models excel at compositional tasks (IMS/MS) due to instruction tuning on (image-text)<->text pairs, demonstrating the value of composed retrieval synthetic data. However, experiments show that their inability to process temporal video features limits VMS performance.  
    
2. Composed Video Retrievers (COVR):  
  - Input Handling: Encode 15 uniformly sampled frames; fuse text and video via a dedicated multimodal encoder.  
  - Rationale: COVR is state-of-the-art for text+video→video tasks (IMS/VMS), which serves as a strong baseline in IMS and VMS. However, COVR overfits to its narrow training objective, underperforming on CA.  
3. MLLM-Based Retrievers (VLM2VEC, E5V):  
  - Input Handling: Extract the last token's hidden state from frozen MLLMs for embeddings.  
  - Rationale: These provide a direct comparison to our MLLM-based design, highlighting the benefits of end-to-end training with video-centric data.  

This structured comparison underscores how V-Embedder unifies strengths across paradigms via VLM flexibility and powerful synthetic training data. We will expand this analysis in the experiment settings.


\subsection{R3}
We sincerely appreciate this suggestion and apologize for the oversight. We will include detailed framework diagrams in our next version. These figures will:
  1. Illustrate the V-Embedder architecture, explicitly visualizing how multi-modal inputs (text, image, video) are processed through the video encoder, multimodal projector, and LLM modules.
  2. Clarify the retrieval pipeline, including the role of system prompts (e.g., "Represent the given question in one word") in extracting embeddings from the VLM’s last token hidden states.
  3. Detail the training process, highlighting the contrastive learning framework and synthetic data integration.
We agree that such diagrams are critical for interpreting complex multi-modal systems  and thank the reviewer for this constructive feedback.


\subsection{R4}
We include examples and a data pipeline diagram in the appendix. We clarify the data synthesis process as follows:
- Transition-Specific Annotations: For a retrieved video pair (e.g., "a boy running on grassland" and "a girl running in a gym"), a powerful MLLM (Qwen2VL-72B) generates transformation instructions such as "Replace the boy with a girl and change the scene to a gym." These instructions guide the embedder in learning compositional reasoning.
- Two Retrieval Methods:
 Different retrieval methods yield video pairs from different perspectives, as verified in previous work [3].
  1. Text-Based Retrieval: Finds videos with similar captions (e.g., "Michael Jackson dancing" → "Michael Jackson in an interview").
  2. Content-Based Retrieval: Retrieves videos with visual similarity (e.g., "Michael Jackson dancing" → "a boy dancing").
This dual strategy ensures diversity in attribute transitions and mitigates dataset bias.

\section{Responses of Comments Suggestions And Typos}
\subsection{R1-4} Thank you for your kind and constructive advice. We will add visualizations and detailed explanation of the problem definitions in our revision.
\subsection{R5} Since our goal is to enable the retriever to perform retrieval within long videos, our problem formulation may involve some ambiguous references, such as "Where did I put my phone?" When addressing such questions, we do not ensure that no other videos contain relevant answers; instead, we focus solely on retrieval within the current video. Therefore, we do not consider other videos as negative examples.
\subsection{R6} Following previous works that utilize MLLMs as retrievers [1][2], we provide the model with a system prompt: "Represent the given question in one word." / "Represent the video in one word." This ensures that the model understands the embedding task. The hidden states of the output tokens from the model are then used as representations of the current query or candidate video, which are subsequently used for similarity computation.
\subsection{R7,9} Thank you for your careful reading. We will correct this in our revision.
\subsection{R8} Table 4 and Table 5 present our ablation studies, where we investigate the effectiveness of training the embedder using generated data versus using widely adopted caption-video pair training data. Additionally, we analyze the difference between employing a hybrid retrieval approach and using a single retriever. The experiments demonstrate that our proposed data generation method is highly effective. 



\end{document}
