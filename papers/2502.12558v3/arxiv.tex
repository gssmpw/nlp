
\documentclass[11pt]{article}

\usepackage[preprint]{acl}
\usepackage{multirow}
\usepackage{makecell}


\usepackage{array}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{wasysym}
\usepackage{utfsym}
\usepackage{fontawesome}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}  %
\usepackage{microtype}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}

\usepackage{xcolor}
\usepackage{colortbl}



\usepackage[capitalize]{cleveref}
\usepackage{inconsolata}


\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}    %
\usepackage{makecell}    %
\usepackage{siunitx}     %
\usepackage{amssymb}     %

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}
\definecolor{customcolor_blue}{RGB}{227, 248, 248}



\title{MomentSeeker: A Comprehensive Benchmark and A Strong Baseline For Moment Retrieval Within Long Videos} 


\author{\textbf{Huaying Yuan\textsuperscript{1}}, \textbf{Jian Ni\textsuperscript{3}}, \textbf{Yueze Wang\textsuperscript{1}}, \textbf{Junjie Zhou\textsuperscript{4}}, \textbf{Zhengyang Liang\textsuperscript{2}}, 
\\
\textbf{Zheng Liu\textsuperscript{2*}}, \textbf{Zhao Cao\textsuperscript{3}}, 
 \textbf{Zhicheng Dou\textsuperscript{1*}} \and \textbf{Ji-Rong Wen\textsuperscript{1*}} \\
 \textsuperscript{1}Gaoling School of Artificial Intelligence, Renmin University of China \\
 \textsuperscript{2}Beijing Academy of Artificial Intelligence \\
  \textsuperscript{3}School of Smart Governance, Renmin University of China \\
  \textsuperscript{4}Beijing University of Posts and Telecommunications \\
 \texttt{
   {\{hyyuan, dou\}@ruc.edu.cn}
 } 
}


\begin{document}
\maketitle



\begin{abstract}

Retrieval augmented generation (RAG) holds great promise in addressing challenges associated with long video understanding. These methods retrieve useful moments from long videos for their presented tasks, thereby enabling multi-modal large language models (MLLMs) to generate high-quality answers in a cost-effective way. In this work, we present \textbf{MomentSeeker}, a comprehensive benchmark to evaluate retrieval models' performance in handling general long-video moment retrieval (\textbf{LVMR}) tasks. MomentSeeker offers three key advantages. First, it incorporates \textbf{long videos} of over 500 seconds on average, making it the first benchmark specialized for long-video moment retrieval. Second, it covers a wide range of task categories (including Moment Search, Caption Alignment, Image-conditioned Moment Search, and Video-conditioned Moment Search) and diverse application scenarios (e.g., sports, movies, cartoons, and ego), making it a \textbf{comprehensive tool} for assessing retrieval models' general LVMR performance. Additionally, the evaluation tasks are carefully curated through \textbf{human annotation}, ensuring the reliability of assessment. We further fine-tune an MLLM-based LVMR retriever on synthetic data, which demonstrates strong performance on our benchmark. We perform extensive experiments with various popular multi-modal retrievers based on our benchmark, whose results highlight the challenges of LVMR and limitations for existing methods. Our created resources is available at \href{https://github.com/yhy-2000/MomentSeeker}{https://github.com/yhy-2000/MomentSeeker}.



\end{abstract}


\section{Introduction}



\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{images/benchmark_demo.pdf}

\caption{Demonstration of our benchmark. Given a natural language query $\textbf{q}_\textbf{T}$ (optionally accompanied by an image $\textbf{q}_\textbf{I}$ or video $\textbf{q}_\textbf{V}$), models should search the provided video to precisely identify the most relevant clips. The ground truth clip is pointed by arrows, and we marked the parts that directly answer the query with red circles.}
  \label{fig:intra_demo}
\end{figure*}



Long video understanding remains a significant challenge~\cite{fu2024videomme,zhou2025mlvu,wang2024lvbench}. Retrieval augmented generation (RAG)~\cite{luo2024videorag,ataallah2024goldfish} holds great promise in addressing challenges associated with long video understanding. These methods retrieve useful moments from long videos for their presented tasks, thereby enabling multi-modal large language models (MLLMs) to generate high-quality answers in a cost-effective way. A key research problem is how to accurately retrieve essential information to answer a given query based on user instructions. In real word search scenarios such as video agent~\cite{fan2024videoagent,wang2024videoagent}, users may ask ``Where did I put my phone?'', which requires the model to analyze the complete historical context to pinpoint relevant moments. These emerging demands underscore the growing importance of video retrieval in practical applications. In this paper, we formally define the task of precisely locating key information segments within a long video based on a given query as long-video moment retrieval (\textbf{LVMR}).



Existing video retrieval benchmarks~\cite{msrvtt, msvd, qvhighlights, hendricks2017didemo} evaluate cross-modal retrieval by using video captions as text queries to retrieve relevant videos from a candidate pool. However, these benchmarks are primarily designed for short videos, resulting in a significant disparity from real-world long-video search scenarios. Practical applications often require capabilities such as retrieving discontinuous events within lengthy context, which remains unaddressed in current frameworks. Furthermore, these benchmarks rely on sentence-like captions as queries, which presents a marked contrast with real-world retrieval scenarios where queries are typically user-generated (e.g., natural language questions).



To address these gaps, we present \textbf{MomentSeeker}, a comprehensive benchmark specifically designed for evaluating long-video moment retrieval~(LVMR) capabilities. It introduces three fundamental innovations:

\textbf{(1) First LVMR-Specialized Benchmark.}
Existing multi-modal retrieval benchmarks predominantly focus on short video clips, failing to address the unique challenges of LVMR where systems must localize precise moments within hour-long videos. MomentSeeker pioneers long-video understanding with an average video duration of over 500 seconds and a maximum of 7100 seconds, requiring the ability to distinguish subtle temporal relationships within continuous footage. This contrasts with conventional cross-video retrieval paradigms, simulating practical applications like retrieval-augmented video understanding.

\textbf{(2) Comprehensive Task Coverage.}
As shown in Figure~\ref{fig:intra_demo}, our benchmark spans 4 meta-tasks and 18 sub-tasks across three dimensions: 1) \textit{Question types} covering both descriptive captions and naturalized instructional queries (e.g. ``Where was the weighing scale?''), which is more practical in real-world search scenarios; 2) \textit{Multi-modal Query Fusion}, supporting text, image, and video conditionals.Text alone has inherent limitations in expression, and humans naturally tend to convey information through a combination of multiple modalities. As illustrated in Figure~\ref{fig:intra_demo}, a user might ask, ``Did this athlete achieve the highest score?'' while providing an accompanying image. Our benchmark systematically incorporates diverse multi-modal integration patterns, designed to encompass a broader spectrum of multi-modal scenarios and better reflect real-world application complexity.
3) \textit{Diverse domains} (sports, movies, cartoon, egocentric videos and anomaly detection) ensuring ecological validity. This structure captures the spectrum of real-world retrieval intents.

\textbf{(3) High Reliability via Human-Centric Design.}
Unlike synthetic benchmarks, our tasks are meticulously curated through human annotation of 1800 query samples. This ensures query-moment relevance and temporal boundary accuracy, addressing the reliability crisis in automated benchmark generation.



To assess the performance of the current video retrievers, we evaluate three lines of different baselines: clip-style encoders, MLLM-based encoders, and composed video encoders. Our experiments reveal that while these models perform well in specific domains, they lack robustness when tackling more complex, real-world tasks. To address this limitation, we propose a robust embedding framework, \textbf{V-Embedder}, which leverages MLLM~\cite{wang2025internvideo2} as its backbone and introduces a synthetic data generation framework to train a more generalizable multi-modal video retriever. Our results show that the proposed model delivers robust performance with minimal training data across a wide range of retrieval tasks, highlighting its capability to effectively handle complex multi-modal queries.


In summary, our contributions are threefold:

1. We introduce \textbf{MomentSeeker}, a comprehensive benchmark to evaluate retrieval models' performance in handling general LVMR tasks.

2. We conduct extensive experiments with various popular multi-modal retrievers based on our benchmark, whose results highlight the challenges of LVMR and limitations for existing methods.

3. We further provide an MLLM-based LVMR retriever, \textbf{V-Embedder}, fine-tuned on synthetic data, which demonstrates strong performance on our benchmark.


\begin{figure*}[]
    \centering
    \includegraphics[width=\linewidth]{images/benchmark_overview.pdf}
    \caption{Overview of the MomentSeeker benchmark. MomentSeeker comprises four meta-tasks, leading to 18 distinct retrieval tasks and 1.8K queries. Each meta-task is illustrated with one or two examples.}
    \label{fig:bench_overview}
    \vspace{-0.5cm}
\end{figure*}







\section{Related Works}
\subsection{Video Understanding Benchmarks}
As shown in Table~\ref{tab:dataset_comparison}, current video retrieval benchmarks include MSR-VTT~\cite{msrvtt}, MSVD~\cite{msvd}, VATEX~\cite{vatex}, and ActivityNet~\cite{2015ActivityNet}. These benchmarks evaluate cross-modal retrieval by using video captions as queries to rank videos within a candidate pool. Temporal grounding benchmarks like QVHighlights~\cite{qvhighlights}, Charades-STA~\cite{gao2017charadeSTA}, THUMOS14~\cite{THUMOS14}, and DiDemo~\cite{hendricks2017didemo} attempt to address moment retrieval but are limited to short videos (under three minutes). This leaves a gap in handling long-form content, where events may  require multi-step reasoning. 

There are also long-video understanding benchmarks, such as Video-MME~\cite{fu2024videomme} and MLVU~\cite{zhou2025mlvu}. However, these benchmarks focus on video question answering and do not directly evaluate the representational capability of video encoders. In contrast, our proposed MomentSeeker benchmark is designed for long-video moment retrieval, assessing a model's retrieval ability within a single long video and addressing the limitations of existing benchmarks.

\subsection{Multi-modal Embedding Models}

Most video retrievers~\cite{li2022uniformer,wang2022internvideo,zhu2023languagebind} use a dual-tower architecture like CLIP~\cite{radford2021clip} and train with contrastive learning on large datasets. InternVideo~\cite{wang2022InterVideo} integrates generative and discriminative self-supervised learning via masked video modeling and contrastive learning. InternVideo2~\cite{wang2025internvideo2} expands upon this by incorporating larger models, featuring an encoder with up to 6 billion parameters. LanguageBind~\cite{zhu2023languagebind} aligns multiple modalities in a shared language space, while VAST~\cite{chen2023vast} integrates vision, audio, and text encoders to have a thorough understanding. These models capture spatiotemporal information, forming the basis of video understanding.

In image retrieval field, several studies~\cite{zhang2024magiclens,zhou2024megapairs,wei2025uniir} have explored multi-modal input for composed image retrieval. For example,  MagicLens~\cite{zhang2024magiclens} uses open-ended instructions to capture complex semantic relationships and trains a composed image retriever, while MM-Ret~\cite{zhou2024megapairs} explores further with more effective training data. Inspired by this, CoVR~\cite{ventura2024covr} applies the same pipeline in video retrieval, generating video-text-video triplets from paired videos with similar captions, and provides a BLIP-style~\cite{li2022blip} composed video retriever. These advancements expand the capabilities of foundation models in multi-modal understanding, broadening task adaptability.

Furthermore, with the rise of large language models, some works attempt to transform multi-modal large models (MLLMs) into universal embedders to construct more powerful representations, such as E5-V~\cite{jiang2024e5} and VLM2VEC~\cite{jiang2025vlm2vec}. However, in the video domain, a robust universal embedder has yet to emerge.  For this reason, we propose V-Embedder, which outperforms existing video embedders across multiple tasks through synthetic-data-driven training.

\section{Benchmark: MomentSeeker}



\subsection{Overview}


We present \textbf{MomentSeeker}, a comprehensive benchmark to evaluate retrieval models' performance in handling general long-video moment retrieval (\textbf{LVMR}) tasks.  Our framework systematically evaluates model capabilities through three dimensions: 1). \textit{Question Types:} Descriptive captions vs. natural language queries.
2). \textit{Query Modalities:} Text-only, text+image, or text+video inputs to support real-world multi-modal search.
3). \textit{Data Domains:} Five diverse domains (sports, egocentric, movies, cartoons, anomalies) to test generalization. These axes define 18 retrieval scenarios, consolidated into four meta-tasks that probe distinct capabilities:

\begin{itemize}
    \item \textbf{Caption Alignment:} Cross-modal matching between caption and video moments.
    \item \textbf{Moment Search:} Text-only localization according to instructional queries.
    \item \textbf{Image-Conditioned Moment Search:} Retrieval guided by text and image exemplars.
    \item \textbf{Video-Conditioned Moment Search:} Retrieval guided by text and video content.
\end{itemize}

We formulate the moment retrieval task in a long-video-friendly manner. For each video, we extract the ground truth clips for all questions, while the remaining video segments are randomly cropped (2 to 30s) as candidate clips. The embedding models compress the query into a vector and the candidate clips into a set of vectors. The candidate with the highest dot product is selected as the prediction for evaluation. We measure Recall@1 and mAP@5 to reflect the percentage of top candidates matching the ground truth. Figure~\ref{fig:bench_overview} provides an overview and examples of MomentSeeker.


The benchmark statics is shown in Table~\ref{tab:dataset_comparison}. Compared with existing benchmarks, MomentSeeker supports videos up to 7,108 seconds (around 2 hours), 5$–$300 times longer than existing benchmarks, while maintaining comparable annotation density and supporting more diverse tasks. This forces models to handle long-form search scenario rather than relying on clip-level patterns. 



\begin{table*}[t!]
\centering
\caption{Comparison with existing video retrieval benchmarks~(top) and moment retrieval benchmarks~(bottom). \textit{INS=Support Instructional Queries, MR=Moment Retrieval, RET=Retrieval, Q=Query, MOD=Modality, Len=Length, VID=Video, Dur=Duration, GT=Ground Truth.}}
\label{tab:dataset_comparison}


\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccccc}
\toprule
\textbf{Benchmark} & \textbf{Domain} & \textbf{INS} & \textbf{MR} & \textbf{RET} & \makecell[c]{\textbf{Q}\\ \textbf{MOD}} & \makecell[c]{\textbf{Avg.}\\ \textbf{Q Len.}} & \makecell[c]{\textbf{\#VID/}\\ \textbf{\#Clip}} & \makecell[c]{\textbf{Min/Max}\\ \textbf{VID Dur. (s)}} & \makecell[c]{\textbf{Sum}\\ \textbf{VID Dur. (h)}} & \makecell[c]{\textbf{Avg.}\\ \textbf{GT Dur. (s)}} \\

\midrule
MSR-VTT     & Open    & --         & --         & $\checkmark$ & T   & 9.42  & 1000/1000 & \num{9.97}/\num{30.0} & \num{4.2}  & \num{15.1} \\
MSVD        & Open    & --         & --         & $\checkmark$ & T   & 7.27  & 670/670  & \num{1.9}/\num{60.0} & \num{1.9}  & \num{10.0} \\
VATEX       & Open    & --         & --         & $\checkmark$ & T   & 14.52 & 6000/6000 & \num{1.3}/\num{10.1}  & \num{2.3}  & \num{9.7}  \\
DiDeMo      & Flickr  & --         & --         & $\checkmark$ & T   & 29.42  & 1036/1036 & \num{21.96}/\num{180.0} & \num{13.8}  & \num{49.8} \\
\midrule
CharadesSTA & Activity& --         & $\checkmark$& $\checkmark$ & T   & 6.32  & 1334/3720 & \num{7.6}/\num{72.1} & \num{8.2}  & \num{6.9}  \\
QVHighlights& Vlog/News & --      & $\checkmark$& $\checkmark$ & T   & 10.45 & 476/1529 & \num{124.0}/\num{150.0} & \num{41.6} & \num{7.9}  \\
THUMOS14    & Action  & --         & $\checkmark$& $\checkmark$ & T   & 1.00  & 216/3460& \num{15.0}/\num{1673.4}& \num{12.8} & \num{5.9}  \\
\midrule
\textbf{MomentSeeker} & Open   & $\checkmark$& $\checkmark$& $\checkmark$& \makecell[c]{T/T+V/T+I} & 13.24 & 268/9976  & \num{5.0}/\num{7108.9} & \num{39.9} & \num{14.9} \\
\bottomrule
\end{tabular}%
}
\vspace{-0.5cm}
\end{table*}

\subsection{Task Definition}

To achieve video content retrieval, we first have human experts annotate precise answer intervals within each video, which serve as ground truth. The remaining segments are randomly split into short candidate clips, forming a retrieval pool. The goal is to retrieve the most relevant clip from a pool of candidates based on a given query. Formally, given a query \( q \) and a set of N candidate clips \( C = \{c_\text{1}, c_\text{2}, \dots, c_\text{N}\} \), the retrieval process is defined as:

\[
c^* = \arg\max_{c_\text{i} \in C} S(q, c_\text{i}),
\]

where \( S(q, c_\text{i}) \) denotes the similarity score between the query \( q \) and candidate clip \( c_\text{i} \).


In the domain of video retrieval, tasks are categorized into four meta-tasks: Moment Search (MS), Caption Alignment (CA), Image-conditioned Moment Search (IMS), and Video-conditioned Moment Search (VMS). Each of these tasks presents unique challenges and serves distinct purposes in the broader context of multimedia understanding and retrieval. Below, we elaborate on each task.



\subsubsection{Caption Alignment}
Caption Alignment is defined as retrieving the most relevant segment from a set of candidate videos based on a textual video description \( q_\text{caption} \) . Caption alignment is a fundamental task in video retrieval, aiming to match a textual description with the corresponding video segment. This task requires the system to bridge the gap between explicit textual and visual modalities, serving as the foundation of video retrieval.


\subsubsection{Moment Search}
Moment Search is defined as retrieving the most relevant segment from a set of candidate videos based on a instructional natural language query \( q_\text{inst} \) . Unlike the straightforward matching in caption alignment, Moment search requires the system to comprehend the natural language instructions, which can be semantically ambiguous or context-dependent. The system must map these instructions to specific visual content within a video, recognizing objects and actions while also comprehending the deeper semantics of the temporal sequence of events.


\subsubsection{Image-conditioned Moment Search}
Image-conditioned Moment Search is defined as retrieving the most relevant segment from a set of candidate videos based on a natural language query \( q_\text{inst} \) combined with an image \( I \), denoted as \( q = \{q_\text{inst},  I \} \). Image-conditioned moment search introduces an additional layer of complexity by incorporating both textual and visual information in the query. The system must not only understand the natural language query but also interpret the visual context provided by the image. This task requires the integration of multi-modal information, where the image may provide contextual clues that are not explicitly mentioned in the text. 


\subsubsection{Video-conditioned Moment Search}
Video-conditioned Moment Search is defined as retrieving the most relevant segment from a set of candidate videos based on a natural language query \( q_\text{inst}  \) combined with a reference video \( \text{V} \), denoted as \( q = \{q_\text{inst},  V\} \). Video-conditioned moment search is perhaps the most complex of the four tasks, as it requires the system to process and understand both a textual query and a reference video. The reference video provides a rich source of contextual information, but it also introduces challenges related to temporal alignment and the extraction of relevant features from the video. The challenge lies in effectively leveraging the temporal and visual information from the reference video to enhance the retrieval process.


Each of these meta-tasks presents unique challenges that push the boundaries of current video retrieval systems.  By addressing these challenges, retrievers will benefit more to downstream tasks.









\section{V-Embedder: A Flexible and Powerful Video Embedder}


Given the diverse range of tasks outlined in the previous section, existing state-of-the-art embedding models~\cite{wang2025internvideo2,zhu2023languagebind,zhou2024megapairs} struggle to offer comprehensive support. To address this, we introduce \textbf{V-Embedder}, a MLLM-based video embedding framework that leverages synthetic data for task-flexible, multi-modal video retrieval. Unlike traditional clip-style retrievers~\cite{wang2025internvideo2, zhu2023languagebind}, which process textual and visual inputs separately, V-Embedder utilizes a MLLM as its embedding backbone and dynamically integrates textual descriptions with conditioned image frames based on specific task instructions. This unique approach enables fixed-dimensional embeddings optimized for diverse applications, from caption alignment to visual-conditioned moment search. The overall training pipeline consists of two key components: synthetic data generation and contrastive learning.



\paragraph{Synthetic Data Generation.}  
Due to the scarcity of long-video datasets with transition-specific textual annotations, we use a retrieval-based approach to mine similar video pairs from a large candidate pool. A VLM~\cite{qwen2vl} generates transformation instructions describing transitions between video pairs.

To ensure diverse training data, we integrate text-text and video-video retrieval methods, following prior works~\cite{zhou2024megapairs,wang2024omnibind}. For each video \( v_\text{i} \) and caption \( t_\text{i} \), we compute video and text embeddings \( \mathbf{e}_v^{(i)} \) and \( \mathbf{e}_t^{(i)} \), selecting the top-\( k \) ranked pairs from both methods to create a diverse dataset:  $\mathcal{D} = \{ (V_1, V_2), (V_1, V_3), \dots \}$. A powerful pre-trained VLM~\cite{qwen2vl}, helps distinguish globally similar yet locally varying videos. A LLM~\cite{grattafiori2024llama3herdmodels} generates relational descriptions, creating flexible, multi-modal training triplets:  $\mathcal{D} = \{ (V_1, T_1, V_2), (V_1, T_2, V_3), \dots \}$.

This approach generates multi-modal data, enhancing the model’s flexibility and generalization for downstream tasks.






\paragraph{Contrastive Training Framework.}
We leverage a contrastive training framework to training a MLLM-based embedding model. The input query follows the format:  $q_\text{inst} =\{ \text{Instruction} \} \text{[VISUAL TOKEN]} \{ \text{Query} \}$, where [VISUAL TOKEN] is optional.

Within this framework, each multi-modal query embedding \( \mathbf{e}_\text{i} \) is paired with a positive candidate embedding \( \mathbf{e}_\text{i}^+ \). The contrastive loss is:  

\[
   \mathcal{L} = -\frac{1}{N} \sum_{\text{i}=1}^{N} \log \frac{\exp\left(\mathrm{sim}(\mathbf{e}_\text{i}, \mathbf{e}_\text{i}^+)/\tau\right)}{\sum_{\text{j}=1}^{N} \exp\left(\mathrm{sim}(\mathbf{e}_\text{i}, \mathbf{e}_\text{j})/\tau\right)}
\]  


To increase the number of in-batch random negatives and enhance the performance of the embedding model, we integrate GradCache~\cite{jiang2025vlm2vec,gao2021gradcache}, which splits inputs into small chunks and optimizes backpropagation.

\section{Experiments}


\subsection{Annotation Overflow}




\begin{table*}[t!]
\centering
\small
\caption{Results on the MomentSeeker benchmark. We report R@1 scores averaged across meta-tasks, with all baselines evaluated in a zero-shot setting. For image-based models (MM-Ret, MagicLens, UniIR, VLM2VEC, E5V), we use the middle frame of videos as input; other models follow their original optimal settings. Additionally, we provide mAP@5 scores and detailed performance on individual sub-tasks in Appendix~\ref{sec:full_result}.}
\label{tab:results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cc|ccccc|c}
\toprule
 \multirow{2}{*}{\textbf{Method}} & 
  \multirow{2}{*}{\textbf{Backbone}} & 
  \multirow{2}{*}{\textbf{\# Params}} & 
  \multirow{2}{*}{\textbf{Frames}} &
  \multicolumn{4}{c|}{\textbf{Meta-Task Average Score}} & 
  \multirow{2}{*}{\textbf{Overall}} \\
  \cmidrule(lr){5-8}
  & & & & \textbf{CA} & \textbf{MS} & \textbf{IMS} & \textbf{VMS} & \\


\midrule
\rowcolor{gray!10} \multicolumn{1}{l|}{\texttt{number of tasks}} & -&-&-&\multicolumn{1}{c|}{\texttt{5}} & \multicolumn{1}{c|}{\texttt{5}} & \multicolumn{1}{c|}{\texttt{4}} & \multicolumn{1}{c|}{\texttt{4}} & \multicolumn{1}{c}{18} \\

\hline
\rowcolor{customcolor_blue!60}
\multicolumn{9}{c}{\textit{CLIP-style Visual Encoder}} \\
\hline
MM-Ret~\cite{zhou2024megapairs} & CLIP-Base & 149M & 1 & 23.2 & 15.4 & 10.5 & 10.5 & 14.9 \\
MagicLens~\cite{zhang2024magiclens} & CLIP-Large & 428M & 1 & 9.0 & 2.4 & 3.2 & 2.8 & 4.4 \\
UniIR~\cite{wei2025uniir} & CLIP-Large & 428M & 1 & 25.0 & 15.2 & 6.4 & 0.0 & 10.9 \\
LanguageBind~\cite{zhu2023languagebind} & CLIP-Large & 428M & 8 & 39.6 & 16.4 & 3.2 & 0.0 & 14.8 \\
InternVideo2~\cite{wang2025internvideo2} & ViT & 1B & 8 & \textbf{44.6} & \underline{18.2} & 4.8 & 0.0 & 16.9 \\
\hline
\rowcolor{customcolor_blue!60} \multicolumn{9}{c}{\textit{Composed Video Encoder}} \\
\hline
CoVR~\cite{ventura2024covr} & BLIP-Large & 588M & 15 & 25.8 & 17.4 & \underline{12.3} & \underline{12.3} & \underline{17.1} \\
\hline
\rowcolor{customcolor_blue!60} \multicolumn{9}{c}{\textit{MLLM-based Embedding Model}} \\
\hline
VLM2VEC~\cite{jiang2025vlm2vec} & LLaVA-1.6 & 8.4B & 8 & 6.4 & 6.2 & 3.0 & 3.0 & 4.7 \\
E5V~\cite{jiang2024e5} & LLaVA-1.6 & 8.4B & 8 & 25.8 & 16.8 & 6.2 & 5.2 & 13.5 \\
\hline
\textbf{V-Embedder} & InternVideo2-Chat & 8B & 8 & \underline{42.2} & \textbf{20.4} & \textbf{15.0} & \textbf{15.8} & \textbf{23.3} \\
\bottomrule
\end{tabular}
}
\vspace{-0.5cm}
\end{table*}










We recruit graduate students in computer science to annotate the dataset. Annotators watch videos and perform: (1) Moment Search (MS): posing a question and marking relevant segments, (2) Image-Conditioned Moment Search (IMS): selecting a frame, posing a question, and marking relevant segments. To balance quality and efficiency, MS and IMS are manually labeled, while Video-Conditioned Moment Search (VMS) is auto-generated by extending IMS frames with a random temporal window. Annotation guidelines are in Appendix~\ref{sec:annotation}. Qwen2VL-72B~\cite{qwen2vl} generates CA captions for ground truth clips. Annotators cross-examined the dataset to correct errors.





\subsection{Experiment Settings}
\subsubsection{Baselines}


Our baselines are grouped into three categories:

\noindent\textbf{CLIP-style Video Encoders}. This category includes methods such as MagicLens~\cite{zhang2024magiclens}, MegaPairs~\cite{zhou2024megapairs}, UniIR~\cite{wei2025uniir}, InternVideo2~\cite{wang2025internvideo2}, and LanguageBind~\cite{zhu2023languagebind}. These approaches leverage joint training of video and text to efficiently extract and align cross-modal features.

\noindent\textbf{Composed Video Encoder Approaches}. Represented by COVR~\cite{ventura2024covr}, these methods simultaneously consider both text and video queries to search for relevant videos in a database, enabling a more integrated query handling.

\noindent\textbf{MLLM-based Multi-Modality Encoders}. Methods like E5V~\cite{jiang2024e5} and VLM2VEC~\cite{jiang2025vlm2vec} fall into this category. They utilize large-scale pre-trained multi-modal models to handle flexible modality compositions (e.g., video + text or text alone) and generating multi-modal embeddings.



\subsubsection{Implementation Details}  

\paragraph{Evaluation.} For each video, we extract the ground truth video clips of all questions, fully retaining them. The left parts of the video is randomly cropped (2 to 30s) as the candidate video clips. Each query's candidate clips are sourced exclusively from its corresponding video.  

Following previous works~\cite{jiang2025vlm2vec, jiang2024e5}, we incorporated instructions (e.g., ``Represent the given question in one word.'' and ``Represent the video in one word.'') to guide the model in generating informative embeddings for queries and candidates.  All baselines are reproduced using their official implementations with default settings. We evaluate the video models (LanguageBind and InternVideo2) using an input of uniformly sampled 8 frames, while COVR follows its default setting of 15 frames. For image models (E5V, VLM2Vec, UniIR and MagicLens), we use the temporally middle frame as the video input. Instruction usage remains consistent with each baseline’s original configuration. For image-based baselines, we use the middle frame of the video as input. CoVR~\cite{ventura2024covr} follows its original configuration with 15 frames at a resolution of 384, while LanguageBind, InternVideo2~\cite{wang2025internvideo2}, and V-Embedder each use 8 frames at a resolution of 224.  

\paragraph{V-Embedder Training.}  
For synthetic data, we use LanguageBind~\cite{zhu2023languagebind} encoders to mine training samples from the WebVid~\cite{webvid} subset of LVD~\cite{lvd}. We employ both text-text and video-video retrievers to retrieve the top-5 results each, which are then merged. Transition text labeling is performed by Qwen2VL-72B~\cite{qwen2vl} as the MLLM annotator and LLaMA3-8B~\cite{grattafiori2024llama3herdmodels} as the LLM query generator.

For training, we initialize our model from the InternVideo2-Chat(8B)~\cite{wang2025internvideo2} checkpoint and fine-tune it using LoRA with a rank of 32. After applying GradCache~\cite{gao2021gradcache}, we train with a batch size of 480 and a learning rate of 5e-4 for 80 steps. The video input consists of 8 frames at a resolution of 224, while images are treated as static videos with the same dimensions.  



\subsubsection{Evaluation Metrics}
We evaluated performance using Recall@1 and MAP@5, with each query ranking video clips from the same long video. Recall is based on exact matches, while MAP considers Intersection over Union (IoU) with ground truth to account for partial correctness, ensuring a balanced evaluation.



\subsection{Main Results}

The overall evaluation results for all baselines in the MomentSeeker benchmark are shown in Table~\ref{tab:results}. Individual performances are reported for each task, while average score of Recall@1 are provided. From the results, we derive five primary conclusions:



\textbf{1. All baseline models exhibit suboptimal performance on our benchmark, underscoring the challenge of the tasks.} Even state-of-the-art video retrievers like InternVideo2, which achieves 57.9 Recall@1 on Didemo, show drastically reduced effectiveness for long-video temporal grounding (16.9 mean score). This stems from two critical challenges: (1) Extended videos contain numerous hard negative segments requiring fine-grained visual-semantic discrimination, and (2) Instructional queries demand deeper temporal reasoning compared to descriptive queries in standard benchmarks. Current video-text alignment methods prove inadequate for these human-centric instructional scenarios.


\textbf{2. SOTA Models Exhibit Divergent Strengths and Weaknesses Across Tasks.}


\begin{itemize}
\item  \textit{CLIP-Style Encoders excel in text-based retrieval but falter in multi-modal reasoning.} For instance, InternVideo2 achieves strong cross-modal alignment (44.6 on CA, 18.2 on MS), yet performs poorly on tasks requiring explicit multi-modal interaction, scoring only 4.8 on IMS and 0.0 on VMS. This suggests its temporal modeling capabilities are constrained by isolated modality processing.

\item \textit{Composed Video Retrievers (e.g., CoVR) invert this trend.} Explicit multi-modal fusion enables CoVR to achieve 12.3 VMS (vs. 0.0 for CLIP-style baselines), but sacrifices text retrieval robustness, scoring 25.8 on CA versus InternVideo2’s 44.6. This highlights a trade-off between multi-modal interaction and cross-modal alignment.

\item \textit{Composed Image Retrievers (e.g., MM-Ret) demonstrate unexpected generalization to video tasks.} Despite relying on single-frame inputs, MM-Ret scores 10.5 on both IMS and VMS, outperforming specialized video encoders like LanguageBind (3.2 IMS). However, its lack of temporal understanding limits overall performance.

\item \textit{MLLM-Based Embedding Models (e.g., E5V) dominate instruction-based tasks.} E5V surpasses LanguageBind on MS (16.8 vs. 16.4) and VMS (5.2 vs. 0.0), leveraging instruction-tuning for complex queries. Yet, it underperforms in fine-grained temporal alignment (22.3 CA vs. LanguageBind’s 39.6) due to the lack of temporal awareness.
\end{itemize}


\textbf{3. Our proposed V-Embedder achieves top performance across all tasks and serves as a strong baseline for future work.} V-Embedder unifies temporal perception, multimodal reasoning, and instruction understanding into a unified multimodal video retriever, outperforming the previous SOTA baseline CoVR by 6.2 points. This improvement leverages our backbone model's robust video understanding capability, inherited from the pretrained video LLM, combined with fine-tuning on our curated multimodal retrieval dataset—together enabling a breakthrough in multimodal comprehension.




\begin{table}[!t]
  \centering
  \small
  \caption{Results of zero-shot video retrieval in MSR-VTT and VATEX. $\dag$ indicates that the results are reproduced using their official weights. }
  \label{tab:other_dataset}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Model} & \textbf{MSR-VTT}   & \textbf{VATEX}\\
    \midrule
    CLIP~\cite{radford2021clip} & 30.4  & - \\
    ViCLIP~\cite{wang2024internvid} & 42.4    & -\\    
    InternVideo-L~\cite{wang2022internvideo} & 40.7  & 49.5 \\
    UMT-L~\cite{li2024umt} & 40.7  & - \\
    VideoCoCa-g~\cite{yan2023videococa} &34.4  &53.2  \\
    VideoPrism-g~\cite{zhao2024videoprism} &39.7 & 62.5\\
    LanguageBind$\dag$~\cite{zhu2023languagebind} & \textbf{43.8}   & \underline{61.5}\\
    \midrule
    \textbf{V-Embedder} & \underline{43.4} &\textbf{67.7} \\
    \bottomrule
  \end{tabular}  
  \vspace{-0.2cm}
\end{table}


\subsection{Dataset Cross-validation}  
To assess the generalization capability of V-Embedder, we evaluate its zero-shot performance on traditional video retrieval benchmarks: MSRVTT~\cite{msrvtt} and VATEX~\cite{vatex}. As shown in Table~\ref{tab:other_dataset}, V-Embedder achieves state-of-the-art performance on VATEX with a score of 67.7, significantly outperforming VideoPrism-g by around 5 points. While slightly trailing LanguageBind on MSR-VTT, it still surpasses all other baselines. These results highlights V-Embedder's robust performance across conventional datasets.


\subsection{Impact of Training Data}  
To assess training data impact, we compare our generated data with InternVid~\cite{wang2024internvid} under equal training steps (Table~\ref{tab:data_ablation}). Experiments demonstrate that the generated data is more efficient than the text-video dataset. This improvement is primarily attributed to its instruction-friendly design and multi-modal support, making it well-suited for handling complex tasks.

\subsection{Impact of Retrievers}

\begin{table}[!t]
\centering
\small
\caption{Performance comparison of different retrieval strategies. ``Hybrid'' refers to the combination of Text-Text and Video-Video retrievers.}
\label{tab:retriever_ablation}
\begin{tabular}{lcccccc}
\toprule
\textbf{Retriever} & \textbf{CA} & \textbf{MS} & \textbf{IMS} & \textbf{VMS} & \multicolumn{2}{c}{\textbf{Overall}} \\
\midrule
Hybrid  & \textbf{42.2} & \textbf{33.5} & \textbf{15.0} & \textbf{15.8} & \textbf{26.6} & -\\
\midrule
Text-Text  & 41.9 & 33.2 & 14.2 & 15.1 & 26.1 & -1.8\%\\
Video-Video  & 41.3 & 32.9 & 14.0 & 14.8 &25.8 & -3.0\%\\
\bottomrule
\end{tabular}
\end{table}



\begin{table}[!t]
\centering
\small
\caption{Performance comparison of InternVid~\cite{wang2024internvid} and our synthetic data.}
\label{tab:data_ablation}
\begin{tabular}{lccccc}
\toprule
\textbf{Data}  & \textbf{CA} & \textbf{MS} & \textbf{IMS} & \textbf{VMS} & \textbf{Overall} \\
\midrule
InternVid  & 28.8 & 18.8 & 9.5 & 6.5 & 15.9 \\
Synthetic  & \textbf{42.2} & \textbf{20.4} & \textbf{15.0} & \textbf{15.8} & \textbf{23.3}\\
\bottomrule
\end{tabular}
\end{table}

To evaluates the impact of different retrieval strategies on model performance, we compare text-text retriever, video-video retriever and our hybrid retriever. As shown in Table \ref{tab:retriever_ablation}, using single-modality retrievers yields suboptimal results: the text-text retriever achieves 26.1 overall score while the video-to-video variant performs 25.8 overall score. The hybrid approach combining both retrievers demonstrates superior performance (26.6 AVG), suggesting that cross-modal retrieval enriches training data diversity, whcih leads to more robust video representations.





\section{Conclusion}


In this paper, we introduce MomentSeeker, a benchmark for evaluating retrieval models in LVMR. MomentSeeker offers three main advantages: it focuses on long videos, covers a wide range of task categories and application scenarios, and ensures reliability through human annotation. We also demonstrates strong performance using a fine-tuned MLLM-based LVMR retriever. Extensive experiments highlight the challenges of LVMR and the limitations of existing methods, with resources shared to advance future research.


\section{Limitations}
This paper focuses on moment retrieval within long videos and presents a comprehensive benchmark evaluation. For the model, we employ synthetic data to train a MLLM capable of understanding complex instructions and integrating multiple modalities. However, compared to a standalone encoder~\cite{wang2025internvideo2,zhu2023languagebind}, the MLLM has more parameters, resulting in longer inference time. In the future, we plan to explore techniques such as model distillation to reduce model size and inference cost.



\section*{Ethics Statement}
Our benchmark video data is sourced from publicly available datasets ~\cite{mangalam2023egoschema,zhou2025mlvu,wang2024lvbench,sultani2019anomaly}, which have undergone rigorous screening by the original teams to remove harmful content. Despite our best efforts, we acknowledge that these videos may not be entirely comprehensive or free from omissions. Furthermore, we strongly discourage the use of V-Embedder models for encoding or retrieving sensitive content.


\bibliography{arxiv}

\newpage
\newpage
\section{Appendix}
\label{sec:appendix}


\subsection*{Annotation Guideline}\label{sec:annotation}



\subsubsection{Task 1: Video Search (MS)}

\textbf{Objective}: Generate a question answerable by a specific video segment and annotate the corresponding one or more answering segments.

\textbf{Scope}: Both the question and answer must originate from the same video.

\textbf{Steps:}

\textbf{Watch the entire video} to understand its content (e.g., actions, events, objects).
\begin{enumerate}
    
    \item \textbf{Formulate a question}:
    \begin{itemize}
        \item Ensure the question is specific and requires one or several continuous segments as the answer.
        \item Example questions:
        \begin{itemize}
            \item \textit{``Where was the weighing scale?''}
            \item \textit{``What did I put in the trashcan?''}
        \end{itemize}
    \end{itemize}
    \item \textbf{Annotate the answer segment}:
    \begin{itemize}
        \item Mark the \textbf{start and end timestamps} (format: \texttt{[[MM:SS--MM:SS], [MM:SS--MM:SS]...]})
        \item Ensure the segments fully answer the question without truncation.
        \item Cover all segments that answer the question—no omissions allowed.
    \end{itemize}
    \item \textbf{Validation}: Replay the annotated segment to confirm alignment with the question.


\end{enumerate}

\subsubsection{Task 2: Image-Conditioned Video Search (IMS)}

\textbf{Objective}: Generate a question based on a static image (from the same or a different video) and annotate the answer segments in the target video.

\textbf{Steps:}
\begin{enumerate}
    \item \textbf{Select an image}:
    \begin{itemize}
        \item \textbf{Same video}: Choose a key frame (e.g., action initiation, critical object appearance).
        \item \textbf{Different video}: Use a frame from another video with relevant content (e.g., similar objects, scenes).
    \end{itemize}
    \item \textbf{Formulate a question}:
    \begin{itemize}
        \item The question must directly relate to the selected image, and the answer must exist in the \textbf{target video}.
        \item Example questions:
        \begin{itemize}
            \item \textbf{Same video} (image shows a road with street scenery): \textit{``What’s the color of the dog that once appeared on this road?''}
            \item \textbf{Different video} (image shows the same character in different clothing): \textit{``What did the man in this picture give to the person next to him?''}
        \end{itemize}
    \end{itemize}
    \item \textbf{Annotate the answer segment}:
    \begin{itemize}
        \item Mark the timestamps in the \textbf{target video} that answer the question.
        \item Ensure logical consistency between the image, question, and segment.
    \end{itemize}
    \item \textbf{Validation}: Verify that the image, question, and annotated segment are coherent.
\end{enumerate}



\subsubsection{Quality Requirements}

\begin{enumerate}
    \item \textbf{Consistency}: Ensure IVS questions are tightly linked to the image, and answers are precise.
    \item \textbf{Cross-video logic}: If using an image from another video, the question must relate to the \textbf{target video’s content} (avoid mixing contexts).
    \item \textbf{Timestamp accuracy}: Annotated segments must have \(\leq\)1-second error tolerance.
\end{enumerate}

\textbf{Report ambiguities or edge cases to the project lead for resolution.}








\subsection{Full results on MomentSeeker}
\label{sec:full_result}


\begin{table*}[t!]
\centering
\small

\caption{R@1 on each task.}
\label{tab:full_results_r1}

\begin{tabular}{lccccccccc}

\toprule
Task & V-Embedder & CoVR & E5V & InternVideo2 & LanguageBind & MagicLens & MM-Ret & UniIR & VLM2VEC \\
\midrule

\rowcolor{customcolor_blue!60}
\multicolumn{10}{l}{\textit{CA}} \\

\rowcolor{customcolor_blue!60} Average & 42.2 & 26.6 & 25.8 & 44.6 & 39.6 & 9.0 & 23.2 & 25.0 & 6.4 \\
\hline

cartoon & \underline{43.0} & 23.0 & 16.0 & 42.0 & \textbf{45.0} & 8.0 & 19.0 & 19.0 & 4.0 \\
ego & 46.0 & 23.0 & 29.0 & \textbf{54.0} & \underline{51.0} & 8.0 & 14.0 & 20.0 & 2.0 \\
movie & \textbf{64.0} & 45.0 & 46.0 & \underline{63.0} & 62.0 & 19.0 & 40.0 & 44.0 & 6.0 \\
anomaly & \textbf{31.0} & 23.0 & \underline{25.0} & \textbf{31.0} & 19.0 & 1.0 & \textbf{31.0} & \textbf{31.0} & 20.0 \\
sports & \underline{27.0} & 19.0 & 13.0 & \textbf{33.0} & 21.0 & 9.0 & 12.0 & 11.0 & 0.0 \\


\rowcolor{customcolor_blue!60}
\hline
\multicolumn{10}{l}{\textit{MS}} \\

\rowcolor{customcolor_blue!60} Average & 20.4 & 17.4 & 16.8 & 18.2 & 16.4 & 2.4 & 15.4 & 15.2 & 6.2 \\


\hline
cartoon & 12.0 & 8.0 & \textbf{14.0} & \underline{13.0} & 12.0 & 2.0 & 9.0 & 9.0 & 4.0 \\
ego & \textbf{25.0} & 18.0 & 13.0 & 21.0 & \underline{23.0} & 3.0 & 17.0 & 16.0 & 1.0 \\
movie & \textbf{35.0} & 31.0 & 27.0 & \textbf{35.0} & \underline{33.0} & 6.0 & 19.0 & 22.0 & 6.0 \\
anomaly & 12.0 & \textbf{12.0} & 18.0 & 7.0 & 5.0 & 0.0 & \underline{23.0} & 18.0 & 20.0 \\
sports & \textbf{18.0} & 18.0 & 12.0 & \underline{15.0} & 9.0 & 1.0  & 9.0 & 11.0 & 0.0 \\


\hline
\rowcolor{customcolor_blue!60} \multicolumn{10}{l}{\textit{IMS}} \\

\rowcolor{customcolor_blue!60}
Average & 15.0 & 12.3 & 6.2 & 4.8 & 3.2 & 3.2 & 10.5 & 3.2 & 3.0 \\
\hline

cartoon & \textbf{20.0} & 14.0 & 14.0 & 6.0 & 4.0 & 5.0 & \underline{12.0} & 3.0 & 4.0 \\
ego & \textbf{15.0} & 11.0 & 9.0 & 4.0 & 4.0 & 0.0 & \underline{11.0} & 5.0 & 3.0 \\
movie & \textbf{16.0} & 9.0 & 5.0 & 9.0 & 4.0 & 6.0 & \underline{10.0} & 3.0 & 4.0 \\
sports & \textbf{9.0} & 15.0 & 1.0 & 0.0 & 1.0 & \underline{2.0} & \textbf{9.0} & \underline{2.0} & 1.0 \\

\hline

\rowcolor{customcolor_blue!60}\multicolumn{10}{l}{\textit{VMS}} \\


\rowcolor{customcolor_blue!60}Average & 15.8 & 12.3 & 5.2 & 0.0 & 0.0 & 2.8 & 10.5 & 0.0 & 3.0 \\
\hline

cartoon & \textbf{14.0} & 14.0 & \underline{8.0} & 0.0 & 0.0 & 4.0 & \textbf{14.0} & 0.0 & 4.0 \\
ego & \textbf{21.0} & 11.0 & 7.0 & 0.0 & 0.0 & 0.0 & \underline{9.0} & 0.0 & 3.0 \\
movie & \textbf{20.0} & 9.0 & 4.0 & 0.0 & 0.0 & 5.0 & \underline{10.0} & 0.0 & 4.0 \\
sports & \underline{8.0} & 15.0 & 2.0 & 0.0 & 0.0 & 2.0 & \textbf{9.0} & 0.0 & 1.0 \\
\hline

\rowcolor{customcolor_blue!60} Overall Average & \textbf{23.3} & \underline{17.1} & 13.5 & 16.9 & 14.8 & 4.4 & 14.9 & 10.9 & 4.7 \\
\bottomrule
\end{tabular}
\end{table*}



\begin{table*}[t!]
\centering
\small

\caption{mAP@5 on each task.}
\label{tab:full_results_map}

\begin{tabular}{lccccccccc}

\toprule
Task & V-Embedder & CoVR & E5V & InternVideo2 & LanguageBind & MagicLens & MM-Ret & UniIR & VLM2VEC \\
\midrule



\rowcolor{customcolor_blue!60}
\multicolumn{10}{l}{\textit{CA}} \\

\rowcolor{customcolor_blue!60}
Average & 63.1 & 46.4 & 37.5 & 54.0 & 62.0 & 17.8 & 41.3 & 42.4 & 11.6 \\

cartoon & \underline{84.6} & 59.9 & 46.6 & 82.2 & \textbf{89.8} & 25.4 & 47.0 & 51.6 & 11.4 \\
ego & 60.6 & 36.6 & 41.0 & \textbf{68.5} & \underline{63.1} & 10.1 & 31.2 & 33.8 & 6.5 \\
movie & \underline{85.7} & 66.1 & 64.6 & 84.0 & \textbf{88.4} & 32.6 & 57.2 & 61.8 & 11.0 \\
anomaly & \underline{50.4} & 44.9 & 45.5 & 46.9 & 39.9 & 8.7 & \textbf{52.2} & 47.4 & 38.2 \\
sports & \underline{34.4} & 24.3 & 22.0 & \textbf{40.4} & 28.7 & 12.0 & 18.7 & 17.2 & 0.5 \\



\hline
\rowcolor{customcolor_blue!60}
\multicolumn{10}{l}{\textit{MS}} \\

\rowcolor{customcolor_blue!60}
Average & 37.7 & 32.1 & 30.9 & 33.7 & 32.1 & 7.5 & 30.1 & 28.6 & 13.4 \\

cartoon & \underline{35.3} & 28.1 & 30.3 & 33.6 & \textbf{37.5} & 8.0 & 28.4 & 26.7 & 11.4 \\
ego & \textbf{40.5} & 25.8 & 25.8 & \underline{33.0} & 32.1 & 5.4 & 25.6 & 25.5 & 6.1 \\
movie & \textbf{53.5} & 51.6 & 40.8 & 52.9 & \underline{53.3} & 14.6 & 38.7 & 36.0 & 11.0 \\
anomaly & 36.1 & \textbf{33.4} & 40.7 & 25.7 & 23.8 & 6.5 & \underline{44.6} & 39.3 & 38.2 \\
sports & \underline{23.1} & 21.4 & 16.9 & \textbf{23.1} & 13.8 &3.0 & 13.4 & 15.7 & 0.3 \\


\hline
\rowcolor{customcolor_blue!60}
\multicolumn{10}{l}{\textit{IMS}} \\

\rowcolor{customcolor_blue!60}
Average & 30.1 & 24.4 & 19.1 & 21.4 & 18.2 & 7.9 & 20.3 & 16.1 & 5.2 \\
\hline

cartoon & \textbf{41.6} & 37.2 & \underline{30.9} & 30.6 & 27.5 & 11.1 & 29.7 & 24.2 & 7.6 \\
ego & \textbf{32.3} & 22.6 & 19.9 & \underline{20.0} & 18.9 & 3.9 & 16.7 & 16.4 & 7.7 \\
movie & \textbf{32.5} & 22.4 & 20.3 & \underline{32.0} & 22.7 & 12.8 & 23.0 & 18.7 & 5.1 \\
sports & \textbf{14.1} & 15.4 & 5.3 & 3.2 & 3.7 & 3.9 & \underline{11.7} & 5.0 & 0.4 \\



\hline
\rowcolor{customcolor_blue!60} \multicolumn{10}{l}{\textit{VMS}} \\
\rowcolor{customcolor_blue!60} Average & 27.4 & 24.4 & 18.0 & 18.8 & 17.8 & 7.4 & 19.7 & 13.9 & 5.2 \\
\hline

\hline
cartoon & \textbf{31.6} & 37.2 & 28.5 & 26.8 & 24.2 & 10.2 & \underline{29.9} & 22.9 & 7.6 \\
ego & \textbf{31.9} & 22.6 & 19.0 & \underline{21.6} & 18.8 & 3.5 & 14.9 & 13.1 & 7.7 \\
movie & \textbf{34.2} & 22.4 & 19.1 & \underline{24.6} & 24.2 & 12.7 & 22.3 & 15.5 & 5.1 \\
sports & \textbf{12.1} & 15.4 & 5.4 & 2.3 & 3.8 & 3.2 & \underline{11.7} & 4.1 & 0.4 \\
\hline


\rowcolor{customcolor_blue!60} Overall Average & \textbf{39.6} & 31.8 & 26.4 & 32.0 & \underline{32.5} & 10.2 & 27.9 & 25.2 & 8.8 \\
\bottomrule
\end{tabular}

\end{table*}


























\end{document}
