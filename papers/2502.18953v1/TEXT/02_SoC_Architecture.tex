\section{SoC Architecture}
% archi overview
Fig.~\ref{fig:soc-archi} shows the SoC architecture, operating across three clock domains, each driven by a dedicated PLL. A \gls{secd} acts as the SoCâ€™s \gls{hwrot}, handling the secure boot and crypto services. 
% hard real-time
For hard real-time and safety-critical tasks, the \gls{safed} features a lockstepped triple-RV32-core for reliability, \gls{ecc}-protected private instruction and data \gls{spm} for deterministic memory access, and an enhanced \gls{rv} \gls{clic} with 6-cycle interrupt latency.

%enhanced with custom ISA extensions, namely \textit{Xfastirq}, providing as low as 6-cycle interrupt latency. 

% soft real-time
Soft real-time tasks with less stringent safety requirements run in the \gls{hostd}, based on the Cheshire platform~\cite{ottavianoCheshireLightweightLinuxCapable2023}, extended with a dual-core RV64GCH processor with hardware-assisted virtualization capabilities: it supports concurrent execution of RTOS and GPOS \glspl{vg} through \gls{rv}-compliant H-extension, and virtual interrupts are managed by per-core \gls{vclic} to reduce interrupt handling and context-switching latency among \glspl{vg}. 
%
The \gls{hostd} cores have private 32KiB L1 \glspl{dc} and share a 128KiB \gls{dpllc}, which interfaces with two external HyperRAM chips via a 400Mb/s deterministic access time HyperBUS memory controller. 
%
The system interconnect is based on a 64b AXI4 bus. A 1MiB on-chip shared L2 \gls{dcspm} is accessible by all domains with 128b/cyc bandwidth. The SoC also includes conventional peripherals, as shown in Fig.~\ref{fig:soc-archi}. Compute-intensive \gls{ai}/\gls{dsp} workloads are offloaded to two domain-specific accelerators: the vector and \gls{amr} clusters. % detailed later in this section. 

\subsubsection{Hardware IPs For Time-Predictability}

\glspl{mct} on the SoC can interfere through the interconnect and memory endpoints - L2 \gls{dcspm} and HyperRAM accessed via the \gls{dpllc}, resulting in non-deterministic behavior and significantly increasing execution time of \glsplf{tct}, as detailed in \looseness=-1 Sec.~\ref{sec:interference-free}.

To ensure predictable communication over AXI4, each initiator is equipped with a software programmable \glsf{tsu}, shown in Fig.~\ref{fig:hw-ips-for-predictability}.a), aimed at reducing execution latency of \glspl{tct} in interference scenarios by controlling each initiator's bandwidth, thereby enforcing a configurable latency upper bound. The \gls{tsu} comprises three components: 
\textbf{1)} The \textit{\gls{gbs}} fragments long AXI4 bursts to a configurable size to ensure fair arbitration between asynchronous initiators with burst capabilities running \glspl{nct} (e.g., \gls{dma} engines paired with \glspl{dsa}) and initiators running higher-priority \glspl{tct}; 
\textbf{2)} The \textit{\gls{wb}} buffers \emph{AW} and \emph{W} channels, forwarding \emph{AW} requests and \emph{W} bursts only when write data is fully within the buffer. This prevents an initiator from holding the \emph{W} channel, avoiding interconnect stalls; 
\textbf{3)} The \textit{\gls{tru}} assigns each initiator a fixed transfer budget within a configurable communication period. 

Favoring \glspl{tct} in the interconnect unavoidably affects the performance of \glspl{nct}. However, \glspl{mct} conflicting on L2 or HyperRAM endpoints can be further isolated by creating interference-free memory paths. 
%
\gls{dcspm} can be accessed via two AXI4 ports and addressed in contiguous mode to isolate its physical banks. This is configurable at runtime with zero additional latency through aliased memory map addresses, as shown in Fig.~\ref{fig:hw-ips-for-predictability}.b). For \glspl{nct} sharing L2 data, the \gls{dcspm} operates in interleaved mode to statistically minimize conflicts. 
%
For \glspl{mct} accessing HyperRAM, the \gls{dpllc} reduces non-deterministic cache misses by creating set-based spatial partitions of configurable sizes, isolated in hardware and assigned to \glspl{vg}' tasks via \textit{part\_id} identifiers linked to AXI4 user signals, as shown in Fig.~\ref{fig:hw-ips-for-predictability}.c). Predictable cache states associated with tasks sharing a partition are maintained by selective partition flushing, preserving the isolation of other \looseness=-1 partitions. 

\subsubsection{Compact, Efficient, \gls{rv} Vector Cluster}
\label{sec:vector}
%\begin{figure}[t]
%    \centering
%    \includegraphics[width=0.95\linewidth]{FIGURES/Vector_cluster_archi.pdf}
%    \caption{\gls{rv} Vector Engine diagram and characterization.}
%    \label{fig:vector-cluster-archi}
%\end{figure}

%To minimize area and power in large \gls{vrf} vector processors designed for \gls{hpc}, which are unsuitable for embedded \glspl{mcs}, 
The proposed SoC integrates a cluster (Fig.~\ref{fig:soc-archi}, top-right) of two compact, energy-efficient \glspl{rvu} controlled by two 32b \gls{rv} scalar cores, forming a clock-gatable \gls{cc}. 
%
To minimize access energy on the \gls{vrf}, each \gls{rvu} instantiate 2KiB latch-based private \gls{vrf}, connected to the 16-banks, 1024b/cyc-bandwidth L1 \gls{spm}, via four independent 64b \gls{vlsu} ports and a low-latency interconnect. This allows vector engines to quickly perform unit-strided, non-unit-strided, and indexed memory accesses, improving compute efficiency for both dense and sparse workloads. A third \gls{rv} core in the cluster manages a 512b/cyc read/write DMA for double-buffered L2-L1 transfers. 

%
Each \gls{rvu} is a VLEN=512b RVZve64d processor, supporting formats from FP8 to FP64, bfloat16, integer, and mixed-precision, including \gls{sdotp} operations. Instructions are fetched by the scalar core, decoded by the vector controller to determine the vector length and element width, and executed by the \gls{vau}, achieving a maximum throughput of 256b/cyc for \gls{fp} operations. The \gls{vrf} is organized in four banks, each with 3 read and 1 write 256b ports to meet 3x256b/cyc input, 256b/cyc output bandwidth needs of the \emph{vfmacc} instruction. 
%
The vector cluster achieves 97.9\% \gls{fpu} utilization at 15.67 DP-FLOP/cyc on edge-sized \glspl{matmul}, up to 121.8 FLOP/cyc on FP8xFP8 \glspl{matmul}, improving performance by 23.8$\times$ to 190.3$\times$ over the \gls{hostd}.

\subsubsection{\gls{amr} Cluster for Mission-Critical \gls{ai}}
\label{sec:int_cluster}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\columnwidth]{FIGURES/amr_cluster_features.png}
    \caption{a) Hardware fast recovery (HFR) mechanisms for dual-lockstep mode (DLM) case. b) HFR Finite State Machine. c) \gls{amr} performance.}
    \label{fig:fast-recovery-cluster}
\end{figure}

The \glsf{amr} cluster, shown in bottom right of Fig.~\ref{fig:soc-archi}, includes 12 RV32IMFC cores sharing a 32-banked 256KiB \gls{ecc}-protected L1 \gls{spm}, accessible through a one-cycle latency interconnect with 921Gb/s (@ 900MHz) bandwidth. A 64b/cyc read, 64b/cyc write DMA enables double-buffered L2-L1 data transfers. Mixed-precision integer \gls{dsp}/\gls{ai} tasks are accelerated by the cores through custom \gls{rv} extensions supporting SIMD \gls{sdotp} on data formats ranging from 16b to 2b (all possible mixed permutations). A custom \emph{mac-load} instruction increases MAC unit utilization to 94\% on \glspl{matmul}, overlapping \gls{sdotp} operations with load instructions. 

%
The RV32 cores can be reconfigured through the \gls{amr} hardware to prioritize reliable vs. more performant execution. In the \gls{indip}, all 12 cores operate in MIMD for maximum performance. In \glsf{dlm} /\gls{tlm} mode, six/four main cores have one/two shadow cores, and instructions are committed after a checker/voting mechanism if no error occurs. The \gls{amr} is runtime-programmable; reconfiguration among modes takes 82-183 clock cycles when switching from safety-critical to high-performance sections within application codes (Fig.~\ref{fig:fast-recovery-cluster}.c)). 

%
In case of errors, faulty cores are restored to the nearest reliable state in as few as 24 clock cycles thanks to the \glsf{hfr}, shown in Fig.~\ref{fig:fast-recovery-cluster}.a). 
The \gls{hfr} includes \gls{ecc}-protected recovery registers to back up the internal state of non-faulty cores cycle-by-cycle without extra latency. 
As shown in Fig.~\ref{fig:fast-recovery-cluster}.b), \gls{dlm} with \gls{hfr} prevents rebooting the cluster upon fault detection, while \gls{tlm} with \gls{hfr} is 15$\times$ faster than \gls{tlm} software recovery. 
%
When executing in \gls{dlm} (\gls{tlm}), the performance penalty is limited to 1.89$\times$ (2.85$\times$) compared to \gls{indip}, still achieving 23.1 MAC/cyc (15.3 MAC/cyc) \looseness=-1 on 8b \glspl{matmul}.

% In \gls{dlm}, with half the cores executing compared to \gls{indip}, the performance penalty is limited to 1.89$\times$ (2.85$\times$ in \gls{tlm} mode), still achieving 23.1 MAC/cyc on 8b \glspl{matmul} while ensuring reliability.
