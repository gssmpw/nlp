\section{Evaluation and Measurements}
\label{sec:evaluation-measurements}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\columnwidth]{FIGURES/chip.pdf}
    \caption{a) SoC micro-graph; b) testing setup. }
    \label{fig:chip-die}
\end{figure}
%\begin{figure}[t]
%    \centering
%    \includegraphics[width=\linewidth]{FIGURES/host_safed_sweeps_v2.pdf}
%    \caption{Voltage, Frequency, Power sweeps for HOST domain and SAFED.}
%    \label{fig:host-safed}
%\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\columnwidth]{FIGURES/sweep_clusters.pdf}
    \caption{Voltage/Frequency/Power and Performance/Energy Efficiency sweeps of \gls{amr}  (\textbf{a}, \textbf{b}) and vector (\textbf{c}, \textbf{d}) clusters.}
    \label{fig:clus-sweeps2}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=.98\columnwidth]{FIGURES/interference_aware_plots.pdf}
    \caption{Inteference-aware execution of \glspl{mct} on the SoC: a) \gls{hostd} runs a \gls{tct} accessing HyperRAM, while vector cluster interferes; b) \glspl{mct} running on \gls{amr} and vector clusters in double-buffering, sharing AXI and \gls{dcspm} resources.}
    \label{fig:interference-free}
\end{figure}

Fig.~\ref{fig:chip-die} shows the chip micrograph and the standalone \gls{pcb} designed for testing it. The SoC, fabricated with Intel 16nm FinFet technology, operates from 0.6V to 1.1V, with an overall 1.2W power envelope at a nominal 0.8V. %Both \gls{hostd} and \gls{safed} achieve a maximum operating frequency of 1GHz.

%IT is implemented with the Intel 16nm FinFet technology, and it operates from 0.6V to 1.1V. %To test the prototype, we design a custom-made bring-up PCB shown in Fig.~\ref{fig:chip-die}. This board is connected to external voltage for power delivery, while it integrates a 100MHz oscillator that provides the input to the on-chip PLLs. The board also features a set of peripherals, including HyperRAM chips, and connectors to boot and run applications on the chip.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% PERF vs E.E. sweeps
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Performance vs Energy Efficiency}
\label{sec:perf-vs-ee}
% : the first runs FP64 \glspl{matmul}, the second the CoreMark benchmark. Both achieve a maximum operating frequency of 1GHz. 
%Fig.~\ref{fig:host-safed} shows the frequency-power sweeps of the \gls{hostd} and \gls{safed}, while 
Fig.~\ref{fig:clus-sweeps2} evaluates the two clusters, in terms of performance and energy efficiency. Measurements are taken at the maximum operating frequency, sweeping the supply voltage from 0.6V to 1.1V. 

The \gls{amr} cluster is benchmarked on integer \glspl{matmul}, the core kernel of \gls{dsp} and \gls{ai} tasks, spanning all supported operands' precisions, from 32b down to 2b, including mixed-precision formats. The cluster's cores are configured to run either in \gls{indip} mode for the best performance or in \gls{dlm} mode for a good trade-off between performance and reliability. The \gls{amr} cluster achieves up to 304.9 GOPS on 2b$\times$2b \glspl{matmul} (161.4 GOPS in \gls{dlm}) at 1.1V, 900 MHz, with a peak energy efficiency of 1.6 TOPS/W at 0.6V, 300 MHz (1.1 TOPS/W in \gls{dlm}).
%
Similarly, we benchmark the vector cluster on FP \glspl{matmul} and \gls{ffts}, spanning all supported precisions, reaching a peak performance of 122 GFLOPS on FP8 \glspl{matmul} at 1.1V and 1GHz, and a peak energy efficiency of 1.1 TOPS/W at 0.6V and 250 MHz.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% INTERFERENCE-FREE EXECUTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Interference-aware \glspl{mct} execution}
\label{sec:interference-free}
Fig.~\ref{fig:interference-free} shows how the hardware IPs introduced in Sec.~\ref{fig:hw-ips-for-predictability} enable interference-aware execution of \glsplf{mct}. 
%
In Fig.~\ref{fig:interference-free}.a) the \gls{hostd} runs a \glsf{tct} accessing HyperRAM via the \gls{dpllc} with contiguous stride, while the system DMA interferes by asynchronously transferring data from HyperRAM to the \gls{dcspm} with linear bursts. The measurements show the task latency and jitter, as well as the \gls{dpllc} misses generated by the eviction of cache lines among interfering tasks. In unregulated interference, the \gls{tct} latency degrades by 225$\times$ compared to the isolated (no interference) case (i.e. no interfering DMA). Tuning the \textit{granular burst splitter} and the \textit{traffic regulation unit} of the \glsf{tsu} in software, we regulate the traffic on the interconnect, reducing the latency by 44.4$\times$ compared to the unregulated case. The \gls{tsu} incurs an additional latency of at most 1 clock cycle due to its write buffer. Moreover, by assigning $> 50\%$ spatial partition of the \gls{dpllc} to the \gls{tct}, we reduce cache misses, achieving 75\% of the isolated (no interference) performance.

In a second scenario, the \gls{amr} cluster executes a compute-intensive \gls{tct} in reliable mode, and the vector cluster interferes by executing a \gls{fp} \gls{matmul}. Both accelerators move data in double-buffering from L2 to private L1, overlapping data transfer and computation phases. 
In Fig.~\ref{fig:interference-free}.b), R-E2, the performance of \gls{amr} cluster drops by 12.2$\times$ due to conflicts generated by the vector cluster on the interconnect and the \gls{dcspm}. Programming the \gls{tsu} to regulate the traffic in favor of the \gls{amr} cluster, we reach 95\% of its isolated (no interference) performance (R-E3), degrading the performance of \glsplf{nct}. However, using aliased addresses to access the \gls{dcspm}, we create private memory paths (at zero extra performance overhead) and achieve interference-free execution, matching the isolated (no interference) performance for both tasks \looseness=-1 (R-E4).

We show that these hardware IPs ensure interference-aware concurrent execution of \gls{mct} on shared SoC resources, prioritizing \glspl{tct} over \glspl{nct}  \looseness=-1 with negligible performance overhead.
%Our assessment shows that these hardware IPs ensure interference-aware, concurrent execution of \glspl{mct} on shared resources, prioritizing \glspl{tct} with minimal overhead on \glspl{nct}.%, unlike costly software-based approaches in commodity \gls{mcs} processors. %We discuss these comparisons in \looseness=-1 Sec.~\ref{sec:mcs-soa}.







