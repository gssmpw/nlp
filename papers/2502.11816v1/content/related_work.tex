\section{Related Work}
\subsection{RMTS Forecasting}
The Regular Multivariate Time Series (RMTS) forecasting literature recently focused on transformer-based~\cite{Vaswani2017.Attention} architectures~\cite{Zhou2021.Informer, Wu2021.Autoformer,Zhou2022.FEDformer,Nie2022.Time,Liu2023.ITransformer}. 
However, \citeauthor{Zeng2023.Are} showed that a simple model based on linear layers (DLinear~\cite{Zeng2023.Are}) can be competitive and partially outperform the more complex transformer architectures. 
Additionally, mixer architectures have been adapted from the computer vision domain~\cite{Tolstikhin2021.MLPMixer} to time series~\cite{Ekambaram2023.TSMixer,Chen2023.TSMixer}.
They exclusively rely on fully-connected networks, that are alternately applied along the channel and time dimension and achieve state-of-the-art results. 

\subsection{IMTS Forecasting}
\subsubsection{Ordinary Differential Equations}
Initially, researchers relied on Neural ODE~\cite{Chen2018.Neural} related approaches to model IMTS~\cite{Rubanova2019.Latent, DeBrouwer2019.GRUODEBayes,Bilos2021.Neural,Schirmer2022.Modeling}. 
ODEs are the prevalent framework in science and engineering to model how systems evolve over time in a continuous manner. 
However, using neural networks to represent ODE systems has proven to be less effective than other IMTS modeling approaches and also incurs relatively long inference times~\cite{Yalavarthi2023.Forecasting,Zhang.Irregular}. 

\subsubsection{Graph Neural Networks for IMTS forecasting}
Apart from the ODE-based approaches the Transformable Patch Graph Neural Network (tPatchGNN)~\cite{Zhang.Irregular} applies a patching mechanism to each channel. 
The patches are than processed with a Transformer and serve as input for a Graph Neural Network (GNN) that models inter-channel dependencies. 

Additionally, we want to highlight Graphs for Forecasting Irregular Multivariate Time Series (GraFITi)~\cite{Yalavarthi2023.Forecasting}. 
Here, the IMTS is represented with a graph where channels and time points are nodes and observed values are stored in the edges between the corresponding time and channel nodes. 
Forecasts are made by predicting the edge values of query-time nodes with graph attention layers. 
While this approach provides state-of-the-art performance it also comes with major drawbacks, that relate to the graph structure itself. 
GraFITi relies on time nodes that are connected to multiple channel nodes, corresponding to observation steps that observe more than one channel. 
If only one channel is observed at a time, the resulting graph is disconnected, rendering GraFITi incapable of modeling inter-channel interactions. 

Another problem of GraFITi lies in the fact that the query nodes are indirectly connected and hence influence each other. 
That entails that predictions will change depending on what \emph{other} queries we give to the model. 

\subsubsection{Multi Time Attention}\label{sec:mTAN}
The Multi Time Attention (mTAN)~\cite{Shukla2020.MultiTime} mechanism imputes an IMTS at predefined reference points, by aggregating the observations within a channel with multi-head attention (MHA)~\cite{Vaswani2017.Attention} where attention weights are computed only dependent on the observation time.
The estimates of these reference points are then further processed by a linear layer and consequently fed into a Recurrent Neural Network (RNN). 

The output of this RNN can then either be used to solve IMTS classification or interpolation. IMTS interpolation and forecasting are closely related. 
The problems only differ in the position of the queries.
While interpolation queries lay in between observations, forecasting queries are positioned after the latest observation time. 
mTAN interpolates by applying the same attention mechanism replacing reference points with query points. 
The same approach can be followed for IMTS forecasting. 
Imputing reference points can be interpreted as encoding IMTS channels into vectors where each vector-element corresponds to a reference point.
This is closely related to the approach of this work. 
Furthermore, we also use vector encoding to leverage a module that was originally designed for RMTS, but instead of RNN we rely on TS-mixer.


