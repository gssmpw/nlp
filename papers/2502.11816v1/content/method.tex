\section{Method}
\Cref{fig:model} depicts an overview over \model{}.
In the subsections below we explain each module, starting with
the channel aggregation method, the key innovation of this work that is depicted in \Cref{fig:obs_enc}. 
Subsequently, we explain how we apply the mixer blocks to the aggregated channels and how we predict for the query time points.
\subsection{Observation-Encoder}\label{sec:obsenc}
To aggregate the observations of a channel in a meaningful way, we first transform observation tuples $(v_i, t_i)$ into higher dimensional vector-embeddings $h_i \in \R^D$.
From here on, we will write $v_i, t_i$ for $v_{c,i}, t_{c,i}$, if the channel $c$ is not relevant. 
First we, encode the scalar value $v$ into the hidden dimension $D$ with a linear layer:
\begin{align}
    v_i^{\text{enc}} = W^{v_\text{enc}} v_i +  b^{v_\text{enc}}, 
\end{align}
where $W^{\text{v}} \in \R^{D}$ and $b^{\text{v}} \in \R^{D}$ are trainable weights and biases.
To include information about the observation time, we multiply the encoded observation value
element-wise with a vector of the same length that is inferred by feeding the scalar observation time through a 2-layer fully connected network with relu activation function:
\begin{align}\label{eq:obs_encoding}
    t^\text{enc}_i &= W_2^{t_\text{enc}}(\relu(W_1^{t_\text{enc}}t_i + b^{t_\text{enc}}_1)) + b^{t_\text{enc}}_2 \\
    h_i &=  t^\text{enc}_i \odot v^\text{enc}_i 
\end{align}

\input{figures/obs_enc.tex}

\subsection{Channel Aggregation}
After embedding each observation into a vector we aggregate the embeddings within a weighted convex sum.
The respective weights $\alpha_i$ are based on the observation time and the observed value. 
Here, we reuse the value-encoding $v^{enc}_i$ that was used to compute the observation embedding and add it to the output of another
neural network that inputs time as a scalar.
\begin{align}\label{eq:weight}
    \alpha_i = W_2^{t_\text{weight}}(\relu(W_1^{t_\text{weight}}t_i + b^{t_\text{weight}}_1)) + b^{t_\text{weight}}_2
\end{align}
We aggregate the observations embeddings of each channel to obtain the channel encoding $Z_c \in \R^D$. Specifically $Z_c$ is the encoding
of the IMTS channel $X_c$. While $X_c$ is a set of arbitrary many observation tuples, $Z_c$ is a vector of size $D$ for every channel $c$.
The computation of the weighted convex sum can be expressed with:   
\begin{align}\label{eq:agg}
    Z_c = \sum^{N_c}_{i=1} \softmax{(A_c)}_i \odot h_i
\end{align}
Here, $A_c \in \R^{N_c \times D}$ is the matrix created by concatenating the $N_c$ many $\alpha_i$-vectors as defined by \Cref{eq:obs_encoding},
with $N_c$ referring to the number of observations in channel $X_c$. The softmax function is applied 
over the rows of $A_c$ to ensure that all weights contained in the vectors $\alpha_i$ are positive and add up to one in each dimension.    

\subsection{Channel-Bias}
The proposed channel encoder shares the weights for all channels and thus only learns to embed them based on the patterns in the observation horizon. 
To learn channel specific information and distinguish the channel embeddings, we add channel biases $b_c \in R^D$. 
The final channel encoding is then computed as
\begin{align}\label{eq:bias}
    Z_c^+ = Z_c + b_c
\end{align}
This helps also for the case where a channel is unobserved in the observation horizon.
In that case, we define the channel encoding simply to be the bias term of that channel. 
In \Cref{sec:bias} we show an ablation study showing that this is sufficient for encoding the channels and no other channel specific weights are necessary.
Note also that the subsequent mixer blocks can learn channel specific patterns as well as mix information across channels.

\subsection{Mixer Blocks}
\input{figures/mixer.tex}
\input{figures/model.tex}

Time series mixer networks~\cite{Ekambaram2023.TSMixer,Chen2023.TSMixer} apply fully connected layers along the time and channel dimension alternately.  
They are well-established in regular time series forecasting and yield state-of-the-art forecasting despite exclusively relying on vanilla fully connected layers~\cite{Ekambaram2023.TSMixer,Chen2023.TSMixer}. 

A mixer network can only be applied to 2-D inputs where both dimensions are fixed across instances. 
Additionally, it cannot directly process \nan-values. 
While these conditions are met in regular time series, they can never be satisfied in any IMTS dataset.

However, when we aggregate each channel of an IMTS into fixed-sized vectors as we propose in the previous subsections, we encode an IMTS in matrix form, in which the rows 
directly represent the channels. As a result, we are able to harness the power of mixer networks to learn channel interactions and extract the information relevant to forecasting.   

Instead of using mixer blocks, we could also flatten the hidden state and apply fully connected layers  $\R^{CD} \to \R^{CD}$. 
However, the number of parameters (${(CD)}^2$) of such a layer would explode.
Hence, the resulting model would demand an unnecessary amount of memory while being prone to overfitting.

We define a mixer block as two fully connected layers that are applied subsequently along the hidden dimension $D$ and the input channels $C$.
To learn non-linear dependencies we add relu as an activation function to each layer. 
Aligning with TS-mixer architectures established in RMTS forecasting~\cite{Chen2023.TSMixer,Ekambaram2023.TSMixer}, we add residual connections as they are known to simplify the optimization while enhancing expressiveness~\cite{He2016.Deep}.
Furthermore, we also apply normalization before each fully connected layer as it is known to improve learning deep architectures and was also added to RMTS TS-mixer architectures~\cite{Chen2023.TSMixer,Ekambaram2023.TSMixer}.   
Specifically, we use Rooted-Mean-Square Layer Normalization (RMS-Norm)~\cite{Zhang2019.Root}. 

Theoretically, we could vary the hidden dimension $D$ after each layer as well as the number of (latent) channels $C$. The next channel-to-channel layer would simply have to adjust its input dimension to the output 
dimension of the previous channel-to-channel layer. However, the final channel-to-channel layer is restricted to have an output dimension of $C$, due to our method of utilizing the outcome of the mixer blocks for the final prediction.
We describe the respective method in the next subsection. We decide to keep the output dimension of channel-to-channel layers as $C$. 

Furthermore, we also keep the hidden dimension at $D$ except for the last layer where we allow us to choose a different output dimension $D_\text{out}$.
This decision is supposed to account for the fact that the range in which queries are given can significantly vary from the observation time. 
For example, a model might be tasked to predict a month based on two years.
As the mixer blocks are supposed to map a sequence of observations into a latent encoding of an IMTS's state during the forecasting span~\cite{Chen2023.TSMixer,Ekambaram2023.TSMixer}
the output dimension $D_\text{out}$ should match the necessary complexity. 
Staying in the given example, the aggregated channels of dimension $D$ need to contain all the relevant information 
of two years, but the channel representations emitted by the final mixer block only have to encode information that correspond to one month. 
Therefore, it would be adequate if $D_\text{out}$ is smaller than $D$.   

\model{} contains $L$ mixer blocks, and we define the output of the $l^{th}$ mixer block as:
\begin{align}
    Z^{\prime{(l)}} & = Z^{(l-1)} + \relu(W_C^{(l)} \RMS(Z^{(l-1)}) + b_C^{(l)}) \\
    Z^{(l)} & = Z^{(l-1)} +  Z^{\prime{(l)}} + \relu(W_D^{(l)} \RMS(Z^{\prime{(l)}}) + b_D^{(l)}),
\end{align}
with $W_D^{(l)} \in \R^{D\times D}$, $b_D^{(l)} \in \R^D$, $W_C^{(l)} \in \R^{C \times C}$ and $b_C^{(l)} \in \R^C$.
We use $Z^+$ from \Cref{eq:bias} as the input for the first mixer block ($Z^0 = Z^+$). \Cref{fig:mixer} summarizes the architecture of the mixer blocks as introduced in this subsection.
\subsection{Decoder}
To obtain the forecasts we need to map the hidden state of each channel to the queries and decode it into a scalar for each pair of channel and query. 
Predicting continuous queries from fixed-sized vectors that represent a complete channel is not a novel problem and has been previously solved by concatenating the time stamp to the hidden state.
The final prediction can then be inferred by a neural network~\cite{Zhang.Irregular}. 

However, we propose an alternative method, that closely relates to how we computed the embeddings for observation-tuples, but is reversely applied. 
We input the query time as a scalar into a 2-layer neural network with $D_{out}$ output neurons. 
The vector encoding of the query time is then multiplied element-wisely with the channel encoding and fed into linear layer that aggregates the vector product into a scalar.
Formally, we obtain the prediction $\hat{y}$ of channel $c$ at query time $q_{i,c}$ with:
\begin{align}
    q^{enc}_{i,c} &=  W^{q}_2 \relu(W^{q}_1  q_{i,c}  + b^{q}_{c,1}) + b^{q}_{c,2} \\
    \hat{y}_{i,c} &=  W^{\text{out}} (q^{enc}_{i,c} \odot Z^L_c)  + b^\text{out} 
\end{align}
\subsection{Sharing weights}
The modules for channel aggregation can either be shared across channels or kept separate for each channel. Sharing these modules reduces the number of parameters by a factor of $C$, though at the cost of some modeling flexibility.

Theoretically, a model must learn that a given set of observation tuples can have different implications depending on the channel. When channel aggregation and decoder weights are shared across channels, the model's ability to differentiate between channels is limited to the mixer blocks. However, this weight-sharing approach not only enhances parameter efficiency but also increases robustness against overfitting.

Based on our ablation studies, we found that the trade-off between expressiveness and robustness favors weight-sharing, at least within the evaluated setting.
Since, the channels within one dataset can already be very different, we could theoretically train these modules over different datasets enabling a form of transfer learning. 
We do not share the weights of \model{}'s decoder.