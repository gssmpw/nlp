\section{Problem Formulation}
% TODO: \rho is really ugly for a distribution. better p_{\text{gt}} ?
%
% imts
\begin{definition}[IMTS]
An \emph{Irregularly sampled Multivariate Time Series (IMTS)} can be represented as a sequence of $C$ channels $X \coloneqq (X_1, \ldots, X_C) \in \mathcal{X}$. 
A channel $X_c \in {(\R \times \R)}^*$ is a sequence of observation-tuples:  $X_c \coloneqq {((t_{c,i},v_{c,i}))}_{i=1:N_c}$ of pairs where: 
\begin{itemize}
    \item $t_{c,i} \in \R$ is the time of observation 
    \item $v_{c,i} \in \R $ is the observed value of channel $c$ at $t_{c,i}$
\end{itemize}

We use $^*$ to indicate the space of sequences.

  
\end{definition}

\begin{definition}[Query]
Analogous to the definition of IMTS, we define a forecasting query as the sequence of $C$ 
channel specific queries: $Q \coloneqq (Q_1, \ldots ,Q_C) \in \mathcal{Q}$, with $Q_c \coloneqq {(q_{c,i})}_{i=1:K_c}$.
A query includes the future time points that need to be forecasted.
The answer to the queries can be then represented by $Y \coloneqq (Y_1,\ldots,Y_C) \in \mathcal{Y}$, with $Y_c \coloneqq {(y_{c,i})}_{i=1:K_c}$, where $y_{c,i}$ corresponds to $q_{i,c}$.
Hence, $Q_c$ and $Y_c$ are sequences of equal lengths ($K_c \in \N$).  
\end{definition}
% query

\begin{problem}
We define the \emph{IMTS forecasting problem} as follows:
given $M$ triplets ${\Bigl( (X^{(m)}, Q^{(m)}, Y^{(m)})\Bigr)}_{m=1:M}$ drawn from a distribution $\rho$
and a loss function $\ell: \mathcal{Y} \times \mathcal{Y} \to \R$, find a model 
$\hat{Y}:~\mathcal{X}~\times~\mathcal{Q}~\to~\mathcal{Y}$,
that minimizes the expected loss of predictions and ground truth:
\begin{align}
    \argmin_{\hat{Y}} \quad \E_{\{X,Q, Y\} \sim \rho } [\ell(\hat{Y}(X,Q), Y)].
\end{align} 
\end{problem}

%\begin{problem}
%    The IMTS forecasting problem is defined as follows: given a sequence of triplets $\left(\left(X^{(m)}, Q^{(m)}, Y^{(m)}\right)\right)_{m=1:M} \in (\mathcal{X}\times \mathcal{Q}\times \mathcal{Y})^M$ called training data drawn from a random distribution $\rho$;
%    a loss function $\ell: (\R)^*\times (\R)^*\to \R$ such as Mean Squared Error or Mean Absolute Error that computes the difference between two equilength sequences;
%    find a model $\hat{Y}: \mathcal{X}\times \mathcal{Q} \to \mathcal{Y}$ such that the expected loss is minimized:
%    
%    \begin{align*} 
%    \mathcal{L}(\hat{Y};\rho) := \mathop \mathbb{E}_{(X,Q,Y)\sim \rho} \ell(\hat{Y}(X,Q), Y)
%    \end{align*} 
%    
%\end{problem}
    

In this work, we focus on two loss functions for training and evaluation: Mean Squared Error (MSE) and Mean Absolute Error (MAE). 
In the experiments we refer to implementations defined as follows:
\begin{align}
    \text{MSE}(Y,\hat{Y}) &\coloneqq \frac{1}{\sum^M_{m=1} \sum^C_{c=1} K_c^{(m)}} \sum^M_{m=1} \sum^C_{c=1} \sum^{K_c^{(m)}}_{i=1} {(y^{(m)}_{c,i} - \hat{y}^{(m)}_{c,i})}^2  \\
    \text{MAE}(Y,\hat{Y}) &\coloneqq \frac{1}{\sum^M_{m=1} \sum^C_{c=1} K_c^{(m)}} \sum^M_{m=1} \sum^C_{c=1} \sum^{K_c^{(m)}}_{i=1} |y^{(m)}_{c,i} - \hat{y}^{(m)}_{c,i}| 
\end{align}
Here, $K_c^{(m)}$ refers to the number of queries in an instance's ($m$) channel ($c$) and $\hat{y}^{(m)}_{c,i} = \hat{Y}(X^{(m)},q^{(m)}_{c,i})$. 
We explicitly define these well-established loss functions to avoid ambiguity, as other variants are theoretically possible.
For example one could compute the mean over all queries from an instance or channel and then compute the average of these means. 
This would prevent instances and channels with an extraordinary number of queries to have a higher weight on the training and evaluation. 
However, in this work we rely on the loss functions established in the literature~\cite{DeBrouwer2019.GRUODEBayes, Bilos2021.Neural,Schirmer2022.Modeling, Yalavarthi2023.Forecasting, Zhang.Irregular} and follow the definitions above. 