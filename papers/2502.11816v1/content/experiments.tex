\section{Experiments}
\input{content/table.tex}
We conduct our experiments on an NVIDIA 3080TI with 12 GB of GPU Memory.
We used the benchmark introduced by \citeauthor{Zhang.Irregular}~\cite{Zhang.Irregular},
because it includes a wide range of baselines that are evaluated on various datasets and tasks. 
\subsection{Datasets}
\begin{table}
    \small
    \caption{Summary of dataset characteristics. 
            \emph{Obs. / Forc. Range} refers to the observation range and forecasting horizon.
            \emph{Instances} are the IMTS from training- validation- and test set combined.
            \emph{Channels} are the number of variables observed.
            \emph{min/max avg. Obs.} refers to the minimum/maximum number of observation per channel averaged over instances.}
    \centering
    \begin{tabular}{lcccc}
    \toprule
    & {PhysioNet} & {MIMIC} & {H. Activity} & {USHCN} \\ 
    \midrule
    Obs. / Forc.~range & 24h/24h & 24h/24h & 3s/1s & 2y/1m\\
    Instances & 12,000 & 23,457 & 5,400 & 26,736 \\ 
    Channels                 & 41                 & 96             & 12                     & 5              \\ 
    min avg. Obs. & 0.07               &  0.0007            & 19.7                   & 32.7           \\ 
    max avg. Obs. & 31.1               & 3.3            & 23.9                   & 35.6           \\ 

    \bottomrule
    \end{tabular}\label{tab:dataset_summary}
\end{table} 

\textbf{PhysioNet}~\cite{Silva2012.Predicting} and \textbf{MIMIC}~\cite{Johnson2016.MIMICIII} both contain vital signs of intensive care unit patients. 
To evaluate the models we query them to forecast all the measured variables for 24 hours based on all observations from the initial 24 hours after a patient's admission.

\textbf{Human Activity}~\cite{VedranaVidulin2010.Localization} contains 3-dimensional positional records of four sensors attached resulting in 12 variables. 
The sensors are attached to individuals, which perform various activities (walking, sitting among others). 
Models are tasked to predict 1 second of human motion based on 3 seconds of observation.

Finally, \textbf{USHCN}~\cite{Menne2006.US} is created by combining daily meteorological measurements from over one thousand weather stations that are distributed over the US.\@
While the 3 datasets mentioned above contain sparsely sampled IMTS intrinsically, USHCN originally is observed regularly and transformed into IMTS by randomly sampling observations. 
In each IMTS we use the 2 years of observations to predict the climate conditions of a single month.
For each dataset we split the samples into 60\% train 20\% validation and 20\% test data.
Additional information about the datasets are given in \Cref{tab:dataset_summary}.

Previous works~\cite{DeBrouwer2019.GRUODEBayes, Bilos2021.Neural, Schirmer2022.Modeling,  Yalavarthi2023.Forecasting} used the parts of these datasets, but with different preprocessing, chunking and validation protocols.
We want to emphasize that therefore, the results reported in these works are incomparable. 


\subsection{Hyperparameters}
Following \citeauthor{Yalavarthi2023.Forecasting}~\cite{Yalavarthi2023.Forecasting}, we sample 10 hyperparameter configurations and select the one with the lowest MSE on the validation split.
We tune the number of mixer-blocks, the dimension of the observation tuple embeddings and channel aggregation ($D$), as well as the output dimension of the final mixer block ($D^{out}$). 
The ranges are given in \Cref{app:model}.
Furthermore, we set the hidden dimension of the non-linear networks that encode observation and query time stamps to 32.
To train \model{}, we implement the schedule-free AdamW\cite{Defazio2024.Road,Kingma2017.Adam} optimizer with an initial learning rate of 0.01 and a weight decay of 1e-4 while
using early stopping with a patience of 10 epochs.
Following \citeauthor{Zhang.Irregular} we use a batch-size of 32 for PhysioNet, MIMIC and Human Activity and a larger batch size of 128 for USHCN.\@

\subsection{Baselines} 

The baseline models from our main performance comparison are models not only from the IMTS forecasting literature, but also from  
RMTS forecasting and IMTS classification. 

DLinear~\cite{Zeng2023.Are}, TimesNet~\cite{Wu2022.TimesNet}, PatchTST~\cite{Nie2022.Time}, Crossformer~\cite{Zhang2022.Crossformer}
Graph Wavenet~\cite{Wu2019.Graph}, MTGNN~\cite{Wu2020.Connecting}, CrossGNN~\cite{Huang2023.CrossGNN} and FourierGNN~\cite{Yi2023.FourierGNN} are state-of-the-art models for RMTS Forecasting.
GRU-D~\cite{Che2018.Recurrent}, SeFT~\cite{Horn2020.Set}, Raindrop~\cite{Zhang2021.GraphGuided},
Warpformer~\cite{Zhang2023.Warpformer} and mTAND~\cite{Shukla2020.MultiTime} are 
all models primarily introduced for IMTS classification, that are equipped with a predictor network which inputs 
hidden  IMTS representations and query times.  
 
Finally, Latent-ODE~\cite{Rubanova2019.Latent}, Continuous Recurrent Units (CRU)~\cite{Schirmer2022.Modeling}, Neural Flow~\cite{Bilos2021.Neural}, tPatchGNN~\cite{Zhang.Irregular} and GraFITi~\cite{Yalavarthi2023.Forecasting} 
are models designed for IMTS Forecasting.

GraFITi has not been evaluated on \citeauthor{Zhang.Irregular}'s benchmark, and we contribute to the field of IMTS forecasting by running these experiments.
In \Cref{app:graf}, we list GraFITi's hyperparameter space as mentioned in the corresponding paper~\cite{Yalavarthi2023.Forecasting}.
We apply the random search protocol with 10 tries as explained above.  

Following the same protocol and for the sake of a fair comparison, we also tune tPatchGNN, because it is the strongest method that was already contained in the benchmark. 
The hyperparameter ranges are build closely around the parameters that were reported by \citeauthor{Zhang.Irregular} and are listed in \Cref{app:tpatch}. 

For the remaining baselines, we report results directly from \citeauthor{Zhang.Irregular}.
We refer to them for details including hyperparameters and the implemented strategies to evaluate RMTS Forecasting and IMTS classification models for IMTS forecasting.

\subsection{Main Results}

\Cref{tab:main} reports the forecasting accuracy of \model{} and GraFITi from our experiments as well as the baselines as given by \citeauthor{Zhang.Irregular}~\cite{Zhang.Irregular}.
While all models were trained on Mean Squared Error (MSE) we also report the test Mean Absolute Error (MAE).
The triplet of tPatchGNN, GraFITi, and \model{} collectively represents the state-of-the-art as these models consistently achieve the top three performances across all datasets and metrics, while their relative rankings vary.

The MSE is the more relevant evaluation metric as models were optimized for it. 
Here, \model{} performs best on PhysioNet, Activity and USHCN.\@ However on PhysioNet, GraFITi is within the standard deviation and tPatchGNN is within the standard deviation on USHCN.\@ 
On MIMIC, \model{} shows to have the second-strongest performance being just slightly worse than GraFITi.

Comparing the MAE of competing models, we observe that \model{} is even stronger than on MSE.\@ 
On MAE, \model{} is significantly better on PhysioNet, Human Activity and USHCN than the second-best model on each dataset. 
While GraFITi is still slightly better on MIMIC, the difference is insignificant.

The average ranks with respect to MSE and MAE are reported as \textbf{1.25} for \model{}, 2 for GraFITi and 2.75 for tPatchGNN.\@
We compute the average ranks exclusively based on the mean and ignore standard deviations. 

Notably, the results of tPatchGNN vary significantly from the results that were reported by \citeauthor{Zhang.Irregular}~\cite{Zhang.Irregular}.
While we expected them to be slightly better, because of tuning, the test errors are actually higher for three out of four tasks. 
However, we verified that the hyperparameter configurations that we chose performed best on the respective validation data. 
In their paper, \citeauthor{Zhang.Irregular}, show experiments with different values for the hyperparameter \emph{Patch-Size} and select the configuration that performs best on the test set, which is a form of tuning on test data. 

While the comparison to the non-tuned baselines is difficult, the evaluation between \model{}, GraFITi and tPatchGNN is definitely fair. 

\subsection{Efficiency Analysis}
In \Cref{fig:runtime}, we show the inference and training time per epoch of each model that we trained.
We refer to the hyperparameter configurations that were tuned based on validation MSE.\@ 
The training time per epoch represents the time a model takes on our setup to predict the queries, compute the loss-gradient and update its weights for all mini-batches contained in the training data.   
The inference time refers to the total time it takes to answer all queries from the test set.
We observe that \model{}'s inference time is consistently shorter than the inference time of GraFITi and tPatchGNN.\@
Furthermore, it has the fastest training time per epoch on all datasets but MIMIC, where it is slightly slower than GraFITi.
On MIMIC, the tuning results in the largest and slowest hyperparameter configuration that we made available for \model{}. 
Hence, we could obtain a significantly faster model with slightly worse forecasting accuracy. 

\input{figures/efficiency.tex}
