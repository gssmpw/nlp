\section{Ablations}

\begin{table*}
    \centering
    \caption{Test MSE and MAE of Ablations}\label{tab:abl}
    \begin{tabular}{l cc cc cc cc}
        \toprule
        Algorithm & \multicolumn{2}{c}{PhysioNet} & \multicolumn{2}{c}{MIMIC} & \multicolumn{2}{c}{Human Activity} & \multicolumn{2}{c}{USHCN} \\ 
        \midrule
        & MSE$\times 10^{-3}$ & MAE$\times 10^{-2}$ & MSE$\times 10^{-2}$ & MAE$\times 10^{-2}$ & MSE$\times 10^{-3}$ & MAE$\times 10^{-2}$ & MSE$\times 10^{-1}$ & MAE$\times 10^{-1}$ \\ 
        \midrule
        \model{} & {4.88 ± 0.03} & {3.47 ± 0.01} &  {1.25 ± 0.02} & {6.20 ± 0.05} & {2.49 ± 0.01} & {3.06  ± 0.01} & {5.01  ± 0.08} & {3.05 ± 0.03} \\ 
        \midrule
        w/o Mixer Blocks & 5.17 ± 0.03 & 3.55 ± 0.01 & 1.36 ± 0.02 & 6.57 ± 0.06 & 2.74 ± 0.01 & 3.16 ± 0.01 & 5.14 ± 0.04 &  3.07 ± 0.03 \\ 
        FC-Mixer & 4.94 ± 0.04 & 3.52 ± 0.03 & 1.31 ± 0.01 & 6.43 ± 0.04 & 2.64 ± 0.02 & 3.21 ± 0.01 & 5.15 ± 0.07 & 3.15 ± 0.05 \\
        w/o Value Emb. & 5.25 ± 0.07 & 3.52 ± 0.01 & 1.36 ± 0.03 & 6.67 ± 0.13 & 2.48 ± 0.01 & 3.04 ± 0.01 & 5.07 ± 0.07 & 3.10 ± 0.03 \\ 
        w/o Weight-Sharing & 5.06 ± 0.10 & 3.47 ± 0.01 & 1.25 ± 0.01  & 6.20 ± 0.03 & 2.51 ± 0.01 & 3.09 ± 0.01 & 5.09 ± 0.09 & 3.11 ± 0.05 \\ 
        w/o Channel Bias & 4.95 ± 0.13 & 3.50 ±  0.03 & 1.28 ± 0.02 &  6.30 ± 0.02 & 2.54 ± 0.02 & 3.10 ± 0.01 & 5.19 ±  0.01 & 3.10 ± 0.01 \\
 %       w/o \\
        \midrule
        mTAN-Mixer & 6.65 ± 0.06 & 5.07 ± 0.01   & 1.94 ± 0.03 & 9.82 ± 0.01 &   2.61 ± 0.07 & 3.16 ± 0.05 & 5.39 ± 0.10 & 3.32 ± 0.05 \\
        \bottomrule
    \end{tabular}
\end{table*}
We conduct a series of ablation experiments to evaluate the impact of each model component included in \model{}.
The results are summarized \Cref{tab:abl}.
\subsection{Mixer-Blocks}
In a first experiment we leave out the mixer blocks. Hence, our decoder is directly applied to the result of the channel aggregation. 
As expected, \model{} performs significantly better with mixer blocks than without them,
because the mixer blocks are the only modules in our architecture capable of modeling channel interactions. 
However, our mixer-less ablation (\emph{w/o Mixer Blocks}) still outperforms several competing models presented in \Cref{tab:main}, including CRU, Latent-ODE, and Neural Flow, and even surpasses tPatchGNN on some datasets, calling the usefulness of these baselines into question. 

We also want to investigate, if applying a mixer network is actually necessary. 
Alternatively, we could replace the mixer blocks with wide fully connected layers $CD \to CD$ combined with relu activation. 
The respective model is named \emph{FC-Mixer}. 
We observe that its performance is significantly worse justifying the mixer blocks as implemented for \model{}. 

\subsection{Channel aggregation with mTAN}
As we mentioned in \Cref{sec:mTAN}, the mTAN~\cite{Shukla2020.MultiTime} module imputes a time series at predefined equally spaced reference points. 
Concatenated, the scalar imputations of these reference points can be interpreted as vectors, where one vector represents one channel.
In the full mTAND architecture, the mTAN module is used not only for IMTS encoding, but additionally it is used to interpolate IMTS observations based on a fixed-sized hidden state. 
Hence, we can construct an ablation that works without the channel aggregation and decoder that we introduced replacing these modules with mTAN.\@
That model imputes reference points with mTAN, applies mixer-blocks and infers the final prediction using the mTAN module in the same way it is used for interpolation. 
We name the resulting model \emph{mTAN-Mixer}. 
As shown in \Cref{tab:abl}, it has a significantly worse performance than \model{}, while providing a lower test MSE and MAE than numerous baseline models. 
These results show that the channel aggregation introduced by us is better than mTAN.\@

\subsection{Initial Value Encoding}
As described in \Cref{sec:obsenc}, we first transform the scalar value into a vector ($v^{\text{enc}}$), that is then used to compute the observation tuple embedding and the respective weight necessary for the channel aggregation. 
To evaluate, if this initial linear layer is actually useful, we conduct an ablation study that replaces this initial linear layer with the identity function.
Hence, $v^{\text{enc}}_i$ is replaced by a vector of the same size with the scalar $v_i$ at every index. 
We list the resulting model under the name \emph{w/o Value Emb.} in \Cref{tab:abl}.
We observe that the linear value embedding has a significant positive impact on all datasets but Human Activity, where it has a minimal negative impact. 

\subsection{Sharing Weights}
We perform experiments to evaluate the impact of channel-wise weight-sharing for the channel aggregation. 
The results, presented under \emph{w/o weight-sharing}, demonstrate that the increased robustness provided by weight-sharing outweighs its reduced expressiveness.
Sharing the weights, yields in a slightly lower Test-MSE.\@

\subsection{Channel Bias}\label{sec:bias}
As a final ablation study, we investigate the usefulness of the learnable channel bias added to the channel encoding of \model{}. 
The results, shown in the line \emph{w/o Channel Bias}, indicate that the channel bias has a positive impact on both MSE and MAE for all four datasets.
