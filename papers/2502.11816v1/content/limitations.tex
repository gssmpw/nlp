\section{Limitations}
Theoretically, \model{} is limited by its fixed-size channel aggregation, which aggregates the sequence of observations from a channel into a fixed-sized embedding.
For really long sequences or if there exist significant differences in typical sequence lengths across channels, the fixed representation may become a bottleneck or struggle to represent the significantly different channels in a unified fashion.

However, in practice, we do not find this to be an issue.
Our model is able to fit all the common benchmark datasets well with varying number of channels and distribution of sequence lengths.
%We present some statistics for the datasets and our model configuration for the respective datasets in \Cref{labellist}.
We also saw in an ablation study in \Cref{tab:abl} that not sharing the weights between channels in the initial encoding is worse for all datasets. 
So even with shared weights, the model is able to learn meaningful representations for channels of different types or lengths.

Another potential limitation is that the mixer blocks currently scale quadratically with the number of channels. 
In our experiments this is not a practical issue, however, and for modelling datasets with orders of magnitudes more channels than we currently consider, it would be possible to extend the model to reduce the number of channels before the mixer blocks.