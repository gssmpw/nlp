\section{Related Work}
A growing line of research work focuses on \emph{multi-LLM collaboration}, where multiple LLMs collaboate and complement each other. These approaches focus on either roles or weights for multi-LLM collaboration.

Role-based approaches typically rely on assigning roles to LLMs through prompt engineering ____. Multiple LLMs, or even just a single LLM seeded with different prompts, then collaborate with their prompt-induced roles through exchanging generated texts. For example, specialized LLMs could augment a general-purpose model ____; LLMs could generate feedback for each other's responses to collectively self-refine ____; multi-agent systems could divide and conquer complex problems ____; multiple LLMs could debate and compete with each other to find better answers  ____. These approaches are often hindered by the need for prompt engineering and the effectiveness of prompts for model steerability ____, thus \ourmethod{} uniquely interprets model roles as input-output relationships, optimizing directed acyclic graphs of LLMs to learn contextual roles and specialization.

Weight-based approaches typically focus on adapting the logits/weights of multiple LLMs, notably through mixture-of-experts or model merging ____. The hidden states or logit distributions of multiple models could be selected, routed, and aggregated based on various MoE mechanisms ____. In addition, static ____ and dynamic ____ model merging approaches incorporate the diverse expertise of heterogeneous LLMs into a single model in zero-shot and adaptation settings. We continue to believe that weight adaptation is crucial for specializing individual LLMs in multi-LLM systems: guided by the first successes of evolutionary algorithms in weight-based collaboration ____ and LLMs in general ____, \ourmethod{} employs swarm intelligence to optimize model weights guided by each LLM's individual contribution to the multi-LLM system.

\ourmethod{} uniquely offers a flexible methodology to jointly optimize the roles and weights of diverse LLMs, discovering and adapting multi-LLM systems for wide-ranging tasks and applications.

\vspace*{-5pt}