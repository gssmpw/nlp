@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@inproceedings{feng-etal-2024-dont,
    title = "Don{'}t Hallucinate, Abstain: Identifying {LLM} Knowledge Gaps via Multi-{LLM} Collaboration",
    author = "Feng, Shangbin  and
      Shi, Weijia  and
      Wang, Yike  and
      Ding, Wenxuan  and
      Balachandran, Vidhisha  and
      Tsvetkov, Yulia",
    year = "2024",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
}

@inproceedings{duimproving,
  title={Improving Factuality and Reasoning in Language Models through Multiagent Debate},
  author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{shen-etal-2024-learning,
    title = "Learning to Decode Collaboratively with Multiple Language Models",
    author = "Shen, Zejiang  and
      Lang, Hunter  and
      Wang, Bailin  and
      Kim, Yoon  and
      Sontag, David",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
}

@inproceedings{liu2024tuning,
  title={Tuning language models by proxy},
  author={Liu, Alisa and Han, Xiaochuang and Wang, Yizhong and Tsvetkov, Yulia and Choi, Yejin and Smith, Noah A},
  booktitle={First Conference on Language Modeling},
  year={2024}
}


@InProceedings{pmlr-v235-zhuge24a,
  title = 	 {{GPTS}warm: Language Agents as Optimizable Graphs},
  author =       {Zhuge, Mingchen and Wang, Wenyi and Kirsch, Louis and Faccio, Francesco and Khizbullin, Dmitrii and Schmidhuber, J\"{u}rgen},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  year = 	 {2024},
}

@inproceedings{si2023getting,
  title={Getting MoRE out of Mixture of Language Model Reasoning Experts},
  author={Si, Chenglei and Shi, Weijia and Zhao, Chen and Zettlemoyer, Luke and Boyd-Graber, Jordan},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={8234--8249},
  year={2023}
}

@inproceedings{kennedy1995particle,
  title={Particle swarm optimization},
  author={Kennedy, James and Eberhart, Russell},
  booktitle={Proceedings of ICNN'95-international conference on neural networks},
  volume={4},
  pages={1942--1948},
  year={1995},
  organization={ieee}
}

@article{feng2024model,
  title={Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence},
  author={Feng, Shangbin and Wang, Zifeng and Wang, Yike and Ebrahimi, Sayna and Palangi, Hamid and Miculicich, Lesly and Kulshrestha, Achin and Rauschmayr, Nathalie and Choi, Yejin and Tsvetkov, Yulia and others},
  journal={arXiv preprint arXiv:2410.11163},
  year={2024}
}

@inproceedings{zhaocompeteai,
  title={CompeteAI: Understanding the Competition Dynamics of Large Language Model-based Agents},
  author={Zhao, Qinlin and Wang, Jindong and Zhang, Yixuan and Jin, Yiqiao and Zhu, Kaijie and Chen, Hao and Xie, Xing},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Gemma Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{ivison2023camels,
  title={Camels in a changing climate: Enhancing lm adaptation with tulu 2},
  author={Ivison, Hamish and Wang, Yizhong and Pyatkin, Valentina and Lambert, Nathan and Peters, Matthew and Dasigi, Pradeep and Jang, Joel and Wadden, David and Smith, Noah A and Beltagy, Iz and others},
  journal={arXiv preprint arXiv:2311.10702},
  year={2023}
}

@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International conference on machine learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR}
}

@inproceedings{yu2024languagedare,
  title={Language models are super mario: Absorbing abilities from homologous models as a free lunch},
  author={Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{yadav2024ties,
  title={Ties-merging: Resolving interference when merging models},
  author={Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{mavromatis2024pack,
  title={Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization},
  author={Mavromatis, Costas and Karypis, Petros and Karypis, George},
  journal={arXiv preprint arXiv:2404.11531},
  year={2024}
}

@article{huang2023lorahub,
  title={Lorahub: Efficient cross-task generalization via dynamic lora composition},
  author={Huang, Chengsong and Liu, Qian and Lin, Bill Yuchen and Pang, Tianyu and Du, Chao and Lin, Min},
  journal={arXiv preprint arXiv:2307.13269},
  year={2023}
}

@inproceedings{hu2024automated,
  title={Automated Design of Agentic Systems},
  author={Hu, Shengran and Lu, Cong and Clune, Jeff},
  booktitle={NeurIPS 2024 Workshop on Open-World Agents},
  year={2024}
}

@article{zhang2024g,
  title={G-designer: Architecting multi-agent communication topologies via graph neural networks},
  author={Zhang, Guibin and Yue, Yanwei and Sun, Xiangguo and Wan, Guancheng and Yu, Miao and Fang, Junfeng and Wang, Kun and Cheng, Dawei},
  journal={arXiv preprint arXiv:2410.11782},
  year={2024}
}

@article{zhang2024cut,
  title={Cut the crap: An economical communication pipeline for llm-based multi-agent systems},
  author={Zhang, Guibin and Yue, Yanwei and Li, Zhixun and Yun, Sukwon and Wan, Guancheng and Wang, Kun and Cheng, Dawei and Yu, Jeffrey Xu and Chen, Tianlong},
  journal={arXiv preprint arXiv:2410.02506},
  year={2024}
}

@misc{langgraph,
    title={LangGraph},
    author = {LangGraph},
    url={https://langchain-ai.github.io/langgraph/}, 
    year={2024}
}

@article{wang2024mmlu,
  title={Mmlu-pro: A more robust and challenging multi-task language understanding benchmark},
  author={Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and others},
  journal={arXiv preprint arXiv:2406.01574},
  year={2024}
}
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{ding-etal-2024-knowledge,
    title = "Knowledge Crosswords: Geometric Knowledge Reasoning with Large Language Models",
    author = "Ding, Wenxuan  and
      Feng, Shangbin  and
      Liu, Yuhan  and
      Tan, Zhaoxuan  and
      Balachandran, Vidhisha  and
      He, Tianxing  and
      Tsvetkov, Yulia",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    year = "2024",
}

@article{wang2024can,
  title={Can language models solve graph problems in natural language?},
  author={Wang, Heng and Feng, Shangbin and He, Tianxing and Tan, Zhaoxuan and Han, Xiaochuang and Tsvetkov, Yulia},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{fang-etal-2024-complex,
    title = "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs",
    author = "Fang, Tianqing  and
      Chen, Zeming  and
      Song, Yangqiu  and
      Bosselut, Antoine",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
}

@article{rao2024normad,
  title={Normad: A benchmark for measuring the cultural adaptability of large language models},
  author={Rao, Abhinav and Yerukola, Akhila and Shah, Vishwa and Reinecke, Katharina and Sap, Maarten},
  journal={arXiv preprint arXiv:2404.12464},
  year={2024}
}

@inproceedings{mialongaia,
  title={GAIA: a benchmark for General AI Assistants},
  author={Mialon, Gr{\'e}goire and Fourrier, Cl{\'e}mentine and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{liuagentbench,
  title={AgentBench: Evaluating LLMs as Agents},
  author={Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{dasigi2021dataset,
  title={A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers},
  author={Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A and Gardner, Matt},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4599--4610},
  year={2021}
}

@article{yao2024varying,
  title={Varying Shades of Wrong: Aligning LLMs with Wrong Answers Only},
  author={Yao, Jihan and Ding, Wenxuan and Feng, Shangbin and Wang, Lucy Lu and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2410.11055},
  year={2024}
}

@inproceedings{fengknowledge,
  title={Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models},
  author={Feng, Shangbin and Shi, Weijia and Bai, Yuyang and Balachandran, Vidhisha and He, Tianxing and Tsvetkov, Yulia},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{burnsweak,
  title={Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{liang2023encouraging,
  title={Encouraging divergent thinking in large language models through multi-agent debate},
  author={Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Shi, Shuming and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2305.19118},
  year={2023}
}

@inproceedings{wu2024autogen,
  title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation},
  author={Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and others},
  booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents},
  year={2024}
}

@article{guo2024large,
  title={Large language model based multi-agents: A survey of progress and challenges},
  author={Guo, Taicheng and Chen, Xiuying and Wang, Yaqi and Chang, Ruidi and Pei, Shichao and Chawla, Nitesh V and Wiest, Olaf and Zhang, Xiangliang},
  journal={arXiv preprint arXiv:2402.01680},
  year={2024}
}

@article{sprague2024cot,
  title={To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning},
  author={Sprague, Zayne and Yin, Fangcong and Rodriguez, Juan Diego and Jiang, Dongwei and Wadhwa, Manya and Singhal, Prasann and Zhao, Xinyu and Ye, Xi and Mahowald, Kyle and Durrett, Greg},
  journal={arXiv preprint arXiv:2409.12183},
  year={2024}
}

@inproceedings{sclarquantifying,
  title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting},
  author={Sclar, Melanie and Choi, Yejin and Tsvetkov, Yulia and Suhr, Alane},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{akiba2024evolutionary,
  title={Evolutionary optimization of model merging recipes},
  author={Akiba, Takuya and Shing, Makoto and Tang, Yujin and Sun, Qi and Ha, David},
  journal={arXiv preprint arXiv:2403.13187},
  year={2024}
}

@inproceedings{fernandopromptbreeder,
  title={Promptbreeder: Self-Referential Self-Improvement via Prompt Evolution},
  author={Fernando, Chrisantha and Banarse, Dylan Sunil and Michalewski, Henryk and Osindero, Simon and Rockt{\"a}schel, Tim},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{guoconnecting,
  title={Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers},
  author={Guo, Qingyan and Wang, Rui and Guo, Junliang and Li, Bei and Song, Kaitao and Tan, Xu and Liu, Guoqing and Bian, Jiang and Yang, Yujiu},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{yadav2024survey,
  title={A survey on model moerging: Recycling and routing among specialized experts for collaborative learning},
  author={Yadav, Prateek and Raffel, Colin and Muqeeth, Mohammed and Caccia, Lucas and Liu, Haokun and Chen, Tianlong and Bansal, Mohit and Choshen, Leshem and Sordoni, Alessandro},
  journal={arXiv preprint arXiv:2408.07057},
  year={2024}
}

@article{li2022branch,
  title={Branch-train-merge: Embarrassingly parallel training of expert language models},
  author={Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.03306},
  year={2022}
}

@article{gritsch2024nexus,
  title={Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts},
  author={Gritsch, Nikolas and Zhang, Qizhen and Locatelli, Acyr and Hooker, Sara and {\"U}st{\"u}n, Ahmet},
  journal={arXiv preprint arXiv:2408.15901},
  year={2024}
}

@article{jang2024model,
  title={Model Stock: All we need is just a few fine-tuned models},
  author={Jang, Dong-Hwan and Yun, Sangdoo and Han, Dongyoon},
  journal={arXiv preprint arXiv:2403.19522},
  year={2024}
}

@inproceedings{holtzmancurious,
  title={The Curious Case of Neural Text Degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{zhao2023malicious,
  title={Malicious agent detection for robust multi-agent collaborative perception},
  author={Zhao, Yangheng and Xiang, Zhen and Yin, Sheng and Pang, Xianghe and Chen, Siheng and Wang, Yanfeng},
  journal={arXiv preprint arXiv:2310.11901},
  year={2023}
}

@inproceedings{kumar2023language,
  title={Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey},
  author={Kumar, Sachin and Balachandran, Vidhisha and Njoo, Lucille and Anastasopoulos, Antonios and Tsvetkov, Yulia},
  booktitle={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={3299--3321},
  year={2023}
}

@article{hu2024routerbench,
  title={ROUTERBENCH: A Benchmark for Multi-LLM Routing System},
  author={Hu, Qitian Jason and Bieker, Jacob and Li, Xiuyu and Jiang, Nan and Keigwin, Benjamin and Ranganath, Gaurav and Keutzer, Kurt and Upadhyay, Shriyash Kaustubh},
  journal={arXiv preprint arXiv:2403.12031},
  year={2024}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@inproceedings{wanteach,
  title={Teach Better or Show Smarter? On Instructions and Exemplars in Automatic Prompt Optimization},
  author={Wan, Xingchen and Sun, Ruoxi and Nakhost, Hootan and Arik, Sercan O},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@inproceedings{khattab2024dspy,
  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  journal={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@article{khattab2022demonstrate,
  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}

@article{brown2024large,
  title={Large language monkeys: Scaling inference compute with repeated sampling},
  author={Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\'e}, Christopher and Mirhoseini, Azalia},
  journal={arXiv preprint arXiv:2407.21787},
  year={2024}
}

@article{wu2024inference,
  title={Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models},
  author={Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
  journal={arXiv preprint arXiv:2408.00724},
  year={2024}
}