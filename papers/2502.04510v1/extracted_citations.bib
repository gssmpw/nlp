@article{akiba2024evolutionary,
  title={Evolutionary optimization of model merging recipes},
  author={Akiba, Takuya and Shing, Makoto and Tang, Yujin and Sun, Qi and Ha, David},
  journal={arXiv preprint arXiv:2403.13187},
  year={2024}
}

@inproceedings{burnsweak,
  title={Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{duimproving,
  title={Improving Factuality and Reasoning in Language Models through Multiagent Debate},
  author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{feng-etal-2024-dont,
    title = "Don{'}t Hallucinate, Abstain: Identifying {LLM} Knowledge Gaps via Multi-{LLM} Collaboration",
    author = "Feng, Shangbin  and
      Shi, Weijia  and
      Wang, Yike  and
      Ding, Wenxuan  and
      Balachandran, Vidhisha  and
      Tsvetkov, Yulia",
    year = "2024",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
}

@article{feng2024model,
  title={Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence},
  author={Feng, Shangbin and Wang, Zifeng and Wang, Yike and Ebrahimi, Sayna and Palangi, Hamid and Miculicich, Lesly and Kulshrestha, Achin and Rauschmayr, Nathalie and Choi, Yejin and Tsvetkov, Yulia and others},
  journal={arXiv preprint arXiv:2410.11163},
  year={2024}
}

@inproceedings{fengknowledge,
  title={Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models},
  author={Feng, Shangbin and Shi, Weijia and Bai, Yuyang and Balachandran, Vidhisha and He, Tianxing and Tsvetkov, Yulia},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{fernandopromptbreeder,
  title={Promptbreeder: Self-Referential Self-Improvement via Prompt Evolution},
  author={Fernando, Chrisantha and Banarse, Dylan Sunil and Michalewski, Henryk and Osindero, Simon and Rockt{\"a}schel, Tim},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{gritsch2024nexus,
  title={Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts},
  author={Gritsch, Nikolas and Zhang, Qizhen and Locatelli, Acyr and Hooker, Sara and {\"U}st{\"u}n, Ahmet},
  journal={arXiv preprint arXiv:2408.15901},
  year={2024}
}

@article{guo2024large,
  title={Large language model based multi-agents: A survey of progress and challenges},
  author={Guo, Taicheng and Chen, Xiuying and Wang, Yaqi and Chang, Ruidi and Pei, Shichao and Chawla, Nitesh V and Wiest, Olaf and Zhang, Xiangliang},
  journal={arXiv preprint arXiv:2402.01680},
  year={2024}
}

@inproceedings{guoconnecting,
  title={Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers},
  author={Guo, Qingyan and Wang, Rui and Guo, Junliang and Li, Bei and Song, Kaitao and Tan, Xu and Liu, Guoqing and Bian, Jiang and Yang, Yujiu},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{huang2023lorahub,
  title={Lorahub: Efficient cross-task generalization via dynamic lora composition},
  author={Huang, Chengsong and Liu, Qian and Lin, Bill Yuchen and Pang, Tianyu and Du, Chao and Lin, Min},
  journal={arXiv preprint arXiv:2307.13269},
  year={2023}
}

@article{jang2024model,
  title={Model Stock: All we need is just a few fine-tuned models},
  author={Jang, Dong-Hwan and Yun, Sangdoo and Han, Dongyoon},
  journal={arXiv preprint arXiv:2403.19522},
  year={2024}
}

@article{li2022branch,
  title={Branch-train-merge: Embarrassingly parallel training of expert language models},
  author={Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.03306},
  year={2022}
}

@article{liang2023encouraging,
  title={Encouraging divergent thinking in large language models through multi-agent debate},
  author={Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Shi, Shuming and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2305.19118},
  year={2023}
}

@article{mavromatis2024pack,
  title={Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization},
  author={Mavromatis, Costas and Karypis, Petros and Karypis, George},
  journal={arXiv preprint arXiv:2404.11531},
  year={2024}
}

@inproceedings{sclarquantifying,
  title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting},
  author={Sclar, Melanie and Choi, Yejin and Tsvetkov, Yulia and Suhr, Alane},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{shen-etal-2024-learning,
    title = "Learning to Decode Collaboratively with Multiple Language Models",
    author = "Shen, Zejiang  and
      Lang, Hunter  and
      Wang, Bailin  and
      Kim, Yoon  and
      Sontag, David",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2024",
}

@article{sprague2024cot,
  title={To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning},
  author={Sprague, Zayne and Yin, Fangcong and Rodriguez, Juan Diego and Jiang, Dongwei and Wadhwa, Manya and Singhal, Prasann and Zhao, Xinyu and Ye, Xi and Mahowald, Kyle and Durrett, Greg},
  journal={arXiv preprint arXiv:2409.12183},
  year={2024}
}

@inproceedings{wu2024autogen,
  title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation},
  author={Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and others},
  booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents},
  year={2024}
}

@article{yadav2024survey,
  title={A survey on model moerging: Recycling and routing among specialized experts for collaborative learning},
  author={Yadav, Prateek and Raffel, Colin and Muqeeth, Mohammed and Caccia, Lucas and Liu, Haokun and Chen, Tianlong and Bansal, Mohit and Choshen, Leshem and Sordoni, Alessandro},
  journal={arXiv preprint arXiv:2408.07057},
  year={2024}
}

@article{yadav2024ties,
  title={Ties-merging: Resolving interference when merging models},
  author={Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{yu2024languagedare,
  title={Language models are super mario: Absorbing abilities from homologous models as a free lunch},
  author={Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

