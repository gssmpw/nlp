%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{tabularx}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\newcommand{\swj}[1]{\textcolor{blue}{\textbf{[Weijia: #1]}}}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
\setlist{leftmargin=5.5mm}
\usepackage{algorithm2e}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{adjustbox}
\usepackage{setspace}
\usepackage{bbm}
\usepackage{hyperref}
\usepackage{colortbl}
\usepackage[dvipsnames]{xcolor}
\usepackage{soul}% for underlines
\usepackage{bbm}
\usepackage{multicol}
\usepackage{multirow}
\definecolor{papercolor}{HTML}{0668E1}
\hypersetup{
    colorlinks,
    linkcolor={papercolor!75!black},
    citecolor={papercolor!75!black},
    urlcolor={papercolor!75!black}
}

\def\HiLi{\leavevmode\rlap{\hbox to \hsize{\color{yellow!50}\leaders\hrule height .8\baselineskip depth .5ex\hfill}}}
\frenchspacing
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\mathchardef\mhyphen="2D

\RestyleAlgo{ruled}

%% This is needed if you want to add comments in
%% your algorithm with \Comment
\SetKwComment{Comment}{// }{}
\DontPrintSemicolon

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\ourmethod}[1]{\textsc{Heterogeneous Swarms}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\ourmethod{}}

\begin{document}

\twocolumn[
\icmltitle{\ourmethod{}: Jointly Optimizing Model \\ Roles and Weights for Multi-LLM Systems}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Shangbin Feng}{a1}
\icmlauthor{Zifeng Wang}{a2}
\icmlauthor{Palash Goyal}{a2}
\icmlauthor{Yike Wang}{a1}
\icmlauthor{Weijia Shi}{a1}
\icmlauthor{Huang Xia}{a3}
\icmlauthor{Hamid Palangi}{a2}
\icmlauthor{Luke Zettlemoyer}{a1}
\icmlauthor{Yulia Tsvetkov}{a1}
\icmlauthor{Chen-Yu Lee}{a2}
\icmlauthor{Tomas Pfister}{a2}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% %\icmlauthor{}{sch}
% %\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{a1}{University of Washington}
\icmlaffiliation{a2}{Google Cloud AI Research \\}
\icmlaffiliation{a3}{Google}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Shangbin Feng}{shangbin@cs.washington.edu}
\icmlcorrespondingauthor{Zifeng Wang}{zifengw@google.com}
\icmlcorrespondingauthor{Chen-Yu Lee}{chenyulee@google.com}

% % You may provide any keywords that you
% % find helpful for describing your paper; these are used to populate
% % the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\vspace*{-20pt}
\begin{abstract}
%\vspace*{-10pt}
We propose \ourmethod{}, an algorithm to design multi-LLM systems by jointly optimizing model roles and weights. We represent multi-LLM systems as directed acyclic graphs (DAGs) of LLMs with topological message passing for collaborative generation.
%\swj{are people familiar with multi-LLM systems? maybe we can briefly introduce what multi-llm system is?}
Given a pool of LLM experts and a utility function, \ourmethod{} employs two iterative steps: \emph{role-step} and \emph{weight-step}. For \emph{role-step}, we interpret model roles as learning a DAG that specifies the flow of inputs and outputs between LLMs. Starting from a swarm of random continuous adjacency matrices, we decode them into discrete DAGs, call the LLMs in topological order, evaluate on the utility function (e.g. accuracy on a task), and optimize the adjacency matrices with particle swarm optimization based on the utility score. For \emph{weight-step}, we assess the contribution of individual LLMs in the multi-LLM systems and optimize model weights with swarm intelligence. We propose \emph{JFK-score} to quantify the individual contribution of each LLM in the best-found DAG of the role-step, then optimize model weights with particle swarm optimization based on the JFK-score. Experiments demonstrate that \ourmethod{} outperforms 15 role- and/or weight-based baselines by 18.5\% on average across 12 tasks. Further analysis reveals that \ourmethod{} discovers multi-LLM systems with heterogeneous model roles and substantial collaborative gains, and benefits from the diversity of language models.
\end{abstract}
\vspace*{-20pt}

%\vspace*{-20pt}
\section{Introduction}
\label{sec:introduction}

\begin{figure}
    %\vspace*{-10pt}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/teaser.pdf}
    %\vspace*{-20pt}
    \caption{Our objective: given a pool of LLMs and a task utility function $f$, discover a multi-LLM system with graph-based model roles and adapted model weights tailored to $f$.
    %\swj{1. label LM in the figure? 2. mention what is the input and output in the caption as well?}
    }
    \label{fig:teaser}
    %vspace*{-20pt}
\end{figure}

Advancing beyond training a single general-purpose large language model (LLM) \citep{brown2020language, team2023gemini}, recent research recognizes the importance of multi-LLM collaboration and advances \emph{multi-LLM systems}, where diverse models serve in a collaborative system to complement each other and expand model capabilities \citep{liu2024tuning, shen-etal-2024-learning}. Models often have different \emph{roles} in multi-LLM collaboration, governing the subtask and functionality of individual LLMs; adapting the model \emph{weights} of these LLMs are also identified as important for models to complement each other. Existing methods to develop multi-LLM systems are often \emph{fixed-weight} and/or \emph{fixed-role} and could not flexibly adapt to diverse tasks and contexts.
% \swj{are people familiar with role and weight in multi-llm system? maybe introduce it briefly in the first paragraph}

\emph{\underline{Fixed-weight}} systems employ static and often black-box LLMs and contextualize their roles through textual interaction \citep{duimproving, pmlr-v235-zhuge24a}. Despite the diversity of tasks and inputs, these static models are repeated across model roles and contexts, becoming the bottleneck of flexible adaptation. \emph{\underline{Fixed-role}} systems usually orchestrate LLMs in a fixed workflow and seed LLMs with different hand-crafted prompts and enable their interaction of message passing based on these prompt-induced roles \citep{si2023getting, feng-etal-2024-dont}. However, new tasks and domains would require substantial prompt engineering, which heavily depends on prior knowledge of the given task. Hence the static roles become the bottleneck in automating and scaling multi-LLM systems to unseen tasks and contexts \citep{khattab2022demonstrate, wanteach, khattab2024dspy}. As such, a flexible approach that jointly optimizes the weights and roles of multi-LLM systems is crucial for adapting diverse LLM experts for wide-ranging purposes.

To this end, we propose \ourmethod{} to search for adapted roles and weights guided by a utility function $f$ (\emph{e.g.} performance on a task) via swarm intelligence. Inspired by the success of LLMs and particle swarm optimization (PSO) \citep{kennedy1995particle, feng2024model}, an algorithm to optimize continuous tensors via collective search, we formulate the optimization of roles and weights in two \emph{interleaved} steps: \emph{role-step} and \emph{weight-step}.

For role-step, we use the heterogeneous expertise of models for varying roles: we interpret roles as input-output relations in a multi-LLM system. Role optimization is a graph learning problem, where natural language messages are passed between LLMs organized in a directed acyclic graph (DAG). While previous methods considered heuristics-based and hand-crafted structures such as a star or chain \citep{langgraph}, finding the optimal structure of multi-LLM systems requires task-specific adaptation \citep{hu2024automated}. We use particle swarm optimization to learn LLM DAGs: Given a set of $n$ LLM experts, we randomly initialize a swarm of continuous adjacency matrices of size $n \times n$, indicating the likelihood of having directed edge $(i,j)$. We propose \textsc{G-Decode} to \emph{decode} these continuous adjacency matrices into discrete DAGs of models. We call LLMs in the topological order of the DAG and evaluate their performance: the swarm of adjacency matrices are then optimized based on the performance scores via PSO, where matrices collectively ``move'' in the matrix search space to adapt to $f$.

For weight-step, we adapt models to the task and roles represented by the multi-LLM network. We propose \emph{JFK-score} to quantify \emph{what individual models can do for the multi-LLM system}: randomly assigning the pool of LLMs to positions in the DAG multiple times, run inference, and evaluate different assignments. The \emph{JFK-score} of one model is the aggregated performance across assignments weighted by their frequency (i.e. number of times it appears in a multi-LLM system). The swarm of LLMs is then optimized based on the JFK-scores via PSO, where models collectively update their weights to adapt to $f$.

\begin{figure*}[ht]
    \centering
    \vspace*{-5pt}
    \includegraphics[width=0.9\linewidth]{figures/overview.pdf}
    \vspace*{-10pt}
    \caption{Overview of \ourmethod{}: starting with a swarm of graphs represented by continuous adjacency matrices and a swarm of large language models, \ourmethod{} rotates between role-step and weight-step. In the role-step, we decode continuous adjacency adjacencies into discrete graphs, call the LLMs in topological order to fulfill a task, evaluate on the utility function, and optimize the adjacency matrices with particle swarm optimization. In the weight-step, models are randomly assigned to positions in the best-found network, evaluated by their individual contribution through the JFK-Score, and then optimized with particle swarm optimization. PSO denotes particle swarm optimization (Sec \ref{sec:preliminary}), G-Decode denotes Algorithm 2, and $f$ denotes the utility function.}
    \label{fig:overview}
    \vspace*{-10pt}
\end{figure*}

Role-step and weight-step are iteratively employed to jointly optimize roles and weights, until the utility $f$ does not improve or a maximum iteration limit is met. In the end, we obtain a multi-LLM network where the graph structure and model weights are both optimized for utility $f$. (Figure \ref{fig:teaser})%\swj{1. do we have more concrete results to show or how it enables inference scaling? 2. more related works to cite for inference scaling: https://arxiv.org/abs/2407.21787, https://arxiv.org/abs/2408.00724}

\ourmethod{} outperforms 15 role/weight-based approaches by 18.5\% on average across 12 tasks spanning knowledge, reasoning, and agent contexts. \ourmethod{} also enables  inference-time scaling \citep{snell2024scaling, brown2024large, wu2024inference} of smaller language models through topological collaborative generation, with a 27.1\% improvement on average when scaling the collaboration from two to ten LLMs. (Figure \ref{fig:scaling}) Further analysis reveals that roles and weights could have varying levels of importance for different tasks, \ourmethod{} benefits from the diversity of LLM experts, the optimization could speed up with increased sparsity and ``dropout'', and we discover multi-LLM systems with heterogeneous roles and collaborative gains. 
%\swj{nitpicking: show some qualitative examples?}

\vspace*{-10pt}
\section{Preliminary}
\label{sec:preliminary}

\vspace{-5pt}
\subsection{Multi-LLM Systems}

Multi-LLM collaboration could take many forms, featuring information exchange at the API \citep{hu2024routerbench}, text \citep{duimproving}, logit \citep{liu2024tuning}, and weight levels \citep{yadav2024survey}. In this work, we define multi-LLM systems as directed acyclic graphs (DAG) of LLMs and a multi-LLM system is represented by two variables: \emph{a set of LLMs} $\mathcal{X} = [\mathbf{x}_1, \cdots, \mathbf{x}_n]$ and \emph{adjacency matrix} $\mathcal{A} = \{a_{ij}\}_{n \times n}$, where $a_{ij} \in \{0,1\}$ and $a_{ij}=1$ indicates that the output of model $\mathbf{x}_i$ becomes part of the input of model $\mathbf{x}_j$ with $n$ models in total. $\mathcal{A} \mid \mathcal{X}$ then denotes a multi-LLM system, where $\mathcal{A}$ defines the structure of the DAG and each position in the graph is instantiated by an LLM in $\mathcal{X}$. Given an input, models in $\mathcal{X}$ are called in the topological order of $\mathcal{A}$ to interact and collaborate with each other. As such, optimizing \emph{roles} and input-output relations becomes optimizing the graph structure $\mathcal{A}$, and optimizing \emph{weights} becomes optimizing the weights of models in $\mathcal{X}$.

\vspace{-10pt}
\subsection{Swarm Intelligence}

Particle swarm optimization (PSO) is an optimization algorithm based on the collective behavior of individual systems for non-differentiable contexts \citep{kennedy1995particle}. Guided by its initial success in LLMs \citep{feng2024model}, we adapt PSO to optimize swarms of $\mathcal{A}$ and $\mathbf{x}$.

The input of PSO is a set of continuous vectors $\{\mathbf{x}_i\}_{i=1}^n$ and a utility function $f$ giving vectors a scalar score $f(\mathbf{x})$. Each vector $\mathbf{x}_i$ has two attributes: \underline{velocity} $\mathbf{v}_i$ of the same dimension, initialized as $\mathbf{0}$; \underline{personal best} $\mathbf{p}_i$, the best-found location of $\mathbf{x}_i$ based on utility function $f$ in its search history. The collective swarm has two other attributes: \underline{global best/worst} $\mathbf{g}$ and $\mathbf{g}_w$, indicating the best/worst-found checkpoint in all of the previous $\{\mathbf{x}_i\}_{i=1}^n$. These attributes would provide signals for vector $\mathbf{x}_i$ to move in the search space guided by personal/global information.

\begin{algorithm}[t]
\label{alg:pso}
\caption{Particle Swarm Optimization step ($\mathrm{PSO}$)}
\KwIn{ vectors $\{\mathbf{x}_i\}_{i=1}^n$ and the utility values $\{f(\mathbf{x}_i)\}_{i=1}^n$ by utility function $f$}
\textbf{Hyperparameters}: step length $\lambda$, inertia $\phi_v$, cognitive coeff. $\phi_p$, social coeff. $\phi_g$, repel coeff. $\phi_w$ \;
\textbf{State variables}: each vector $\mathbf{x}_i$ has velocity $\mathbf{v}_i$ and personal best $\mathbf{p}_i$, global best and worst $\mathbf{g}$ and $\mathbf{g}_w$ \;
%\KwOut{improved pool of vectors $\{\mathbf{x}_i\}_{i=1}^n$ and best vector $\mathbf{x}_{\textit{best}}$}

%each vector $\mathbf{x}_i$ has a state variable velocity $\mathbf{v}_i$ of the same dimension \;
\For{$i = 1$ \KwTo $n$ $\mathrm{parallel}$\footnotemark[1]}{
%\Comment*[l]{velocity update}
walk randomness $r_v, r_p, r_g, r_w \sim \mathrm{Uniform}(0,1)$ \;
$\mathbf{v}_i \leftarrow \frac{1}{\mathcal{C}} \big[ r_v \phi_v \mathbf{v}_i + r_p \phi_p (\mathbf{p}_i-\mathbf{x}_i) + r_g \phi_g (\mathbf{g}-\mathbf{x}_i) - r_w \phi_w (\mathbf{g}_w - \mathbf{x}_i) \big]$, where normalization term $\mathcal{C} = r_v\phi_v + r_p\phi_p + r_g\phi_g + r_w\phi_w$ \;
%\Comment*[l]{value update}
$\mathbf{x}_i \leftarrow \mathbf{x}_i + \lambda \mathbf{v}_i$ \;
}
Update personl/global information $\mathbf{p}_i$, $\mathbf{g}$, and $\mathbf{g}_w$ \;
\Return $\{\mathbf{x}_i\}_{i=1}^n$, $\mathbf{x}_{\textit{best}} = \argmax_{\mathbf{x}} f(\mathbf{x})$

\end{algorithm}\footnotetext[1]{All vectors perform velocity and location update in parallel.}

One PSO step changes model velocity $\mathbf{v}_i$ and takes a step towards the adjusted velocity direction. Concretely:
\begin{align*}
\mathbf{v}_i \gets \frac{1}{\mathcal{C}}\big[r_v\phi_v\mathbf{v}_i + r_p\phi_p(\mathbf{p}_i - \mathbf{x}_i) + \\ r_g\phi_g(\mathbf{g} - \mathbf{x}_i) - r_w\phi_w(\mathbf{g}_w - \mathbf{x}_i)\big]
\end{align*}
where $\mathcal{C} = r_v\phi_v + r_p\phi_p + r_g\phi_g + r_w\phi_w$ is a normalization term. The adjusted velocity is the weighted average of four terms: $\mathbf{v}_i$ makes the model keep part of its current velocity (i.e. inertia); $(\mathbf{p}_i - \mathbf{x}_i)$ draws the model towards its personal best; $(\mathbf{g} - \mathbf{x}_i)$ draws towards the global best; $-(\mathbf{g}_w - \mathbf{x}_i)$, repel from the global worst. $\phi_v$, $\phi_p$, $\phi_g$, $\phi_w$ are hyperparameters and $r_v, r_p, r_g, r_w \sim \mathcal{U}(0,1)$ are randomness factors, governing how much $\mathbf{x}_i$ is impacted by personal/global information. Vectors then take a step towards the new velocity: $\mathbf{x}_i \gets \mathbf{x}_i + \lambda \mathbf{v}_i$, where $\lambda$ is the step length hyperparameter. New vectors are then evaluated on $f$ to update personal/global best and worst. We formulate a PSO step in algorithm \ref{alg:pso} and will use $\{\mathbf{x}_i\}_{i=1}^n, \mathbf{x}_{\textit{best}} = \mathrm{PSO}(\{\mathbf{x}_i\}_{i=1}^n, \{f(\mathbf{x}_i)\}_{i=1}^n)$ to denote a PSO step to optimize $\{\mathbf{x}_i\}_{i=1}^n$ based on their scores $\{f(\mathbf{x}_i)\}_{i=1}^n$.

\section{Methodology}
\label{sec:methodology}

We propose \ourmethod{}, jointly optimizing model roles and weights for multi-LLM systems. Starting from a pool of LLM experts as input, \ourmethod{} discovers a directed acyclic graph of models representing heterogeneous roles with adapted model weights, iteratively employing \emph{role-step} and \emph{weight-step} (Figure \ref{fig:overview}).

\subsection{Role-step}

Unlike existing work that often manually specifies model roles through prompt engineering \citep{feng-etal-2024-dont, zhaocompeteai}, we propose to learn a directed acyclic graph of \emph{input-output relationships}. To this end, \ourmethod{} operationalizes the optimization of model roles as a graph optimization problem: discovering and optimizing directed acyclic graphs (DAGs) of LLMs to call them in the topological order for collaborative generation. We envision swarm intelligence as a viable and flexible tool by letting multiple potential DAGs explore the search space of graphs to adapt to a given task and utility function.

To make \emph{discrete} graphs compatible with particle swarm optimization (Sec \ref{sec:preliminary}) that optimizes \emph{continuous} tensors, we first represent graphs by continuous adjacency matrices $\mathbf{A} \in \mathbb{R}^{n \times n}$, where $a_{ij}$ denotes the likelihood of a directed edge from model $\mathbf{x}_i$ to model $\mathbf{x}_j$. We propose $\mathrm{G \mhyphen decode}$, an algorithm to decode discrete DAGs out of continuous $\mathbf{A}$s. We first select an end node $k$ based on inverse out degrees $k = \mathrm{top \mhyphen p}(\{1 / \sum_{j=1}^n a_{ij}\}_{i=1}^n)$, where $\mathrm{top \mhyphen p}$ denotes top-p sampling \citep{holtzmancurious}. We then iteratively select a remaining node based on out degrees $u = \mathrm{top \mhyphen p}(\{\sum_{j=1}^n a_{ij}\})$ and add an edge between $u$ and any existing node $v$ with probability $\frac{\exp(a_{uv})}{\sum_{i \in \mathcal{E}} \exp(a_{ui})}$, until all nodes are considered. Graphs decoded with $\mathrm{G \mhyphen decode}$ are directed since we only add directed edges and they are acyclic since new nodes only connect to existing nodes in the graph. The resulted DAGs define an input-output mapping: given an input, we call LLMs in the topological order of $\mathrm{G \mhyphen decode}(\mathbf{A})$, and the output of the end node becomes the output of the multi-LLM system (Algorithm \ref{alg:gdecode}).

\begin{algorithm}[t]
\label{alg:gdecode}
\caption{DAG Decoder ($\mathrm{G \mhyphen decode}$)}
\KwIn{continuous adjacency matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ where $a_{ij}$ denotes the likelihood of a directed edge from model $\mathbf{x}_i$ to model $\mathbf{x}_j$}
%\KwOut{directed acyclic graph $\mathbf{G}$, defines an input-output mapping based on the collaboration of $\{\mathbf{x}\}_{i=1}^n$}

edges $\mathcal{G} = \varnothing$, remaining node set $\mathcal{R} = \{1, \cdots, n\}$, existing node set $\mathcal{E} = \varnothing$ \;
select end node based on inverse out degrees $k = \mathrm{top \mhyphen p \mhyphen sampling}(\{1 / \sum_{j=1}^n a_{ij}\}_{i=1}^n)$ \;
$\mathcal{R}.$remove$($k$)$, $\mathcal{E}.$add$($k$)$ \;
\While{$\mathcal{R} \neq \varnothing$}{
select remaining node based on out degrees $u = \mathrm{top \mhyphen p \mhyphen sampling}(\{\sum_{j=1}^n a_{ij}\}_{i \in \mathcal{R}})$ \;
%select existing node to connect $v = \mathrm{top \mhyphen p}(\{a_{ui}\}_{i \in \mathcal{E}})$ \;
$\mathcal{G}$.add($\{u \rightarrow v\}$) with prob. $\frac{\exp(a_{uv})}{\sum_{i \in \mathcal{E}} \exp(a_{ui})}$, $\forall v \in \mathcal{E}$\;
$\mathcal{R}.$remove$($u$)$, $\mathcal{E}.$add$($u$)$ \;
}
\Return directed acyclic graph $\mathcal{G}$, defines an input-output mapping based on $\{\mathbf{x}_i\}_{i=1}^n$ \;

\end{algorithm}

We evaluate the utility of the continuous adjacency matrix by decoding it into a DAG, call the models in topological order, and evaluate its performance on the utility function $f$. Concretely, we initialize a swarm of $N$ continuous adjacency matrices $\{\mathbf{A}^i\}_{i=1}^N \sim \mathcal{U}_{n \times n}(0,1)$. For one role-step, we decode and evaluate their utility $f(\mathrm{G \mhyphen decode}(\mathbf{A}^ i))$ and optimize the continuous $\mathbf{A}$s through swarm intelligence:
\vspace{-10pt}

\begin{align*}
    \{\mathbf{A}^i\}_{i=1}^N, \mathbf{A}_{\textit{best}} \leftarrow \mathrm{PSO}(\{\mathbf{A}^i\}_{i=1}^N, \{f(\mathrm{G \mhyphen decode}(\mathbf{A}^ i))\}_{i=1}^N)
\end{align*}

\vspace{-10pt}
The resulting $\{\mathbf{A}^i\}_{i=1}^N$ after a single PSO step will represent graphs that slightly better adapt to the task $f$.

\subsection{Weight-step}

\begin{table*}[t]\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{lcccccccccccc}\toprule[1.5pt]
&\multicolumn{3}{c}{Knowledge} &\multicolumn{3}{c}{Reasoning} &\multicolumn{3}{c}{Agent} &\multicolumn{3}{c}{Miscellaneous} \\\cmidrule[0.75pt]{2-13}
&MMLU-pro &K-Cross &COM2 &GSM8k &NLGraph &Normad &GAIA-text &AB-kg & AB-ltp &Qasper &AbstainQA &WoW \\ \midrule[0.75pt]
\textcolor{NavyBlue}{\textsc{Best Single}} &0.231 &0.346 &0.488 &0.237 &0.535 &0.545 &0.107 &0.383 &0.120 &0.174 &0.065 &0.415 \\
\textcolor{NavyBlue}{\textsc{Pred. Merge}} &0.173 &0.309 &0.391 &0.074 &0.502 &0.481 &0.036 &0.225 &/ &/ &/ &0.471 \\ \midrule[0.75pt]
\textcolor{Lavender}{\textsc{Data Merge}} &0.176 &0.370 &0.377 &0.143 &0.423 &0.415 &0.071 &0.242 &0.112 &0.147 &-0.025 &0.461 \\
\textcolor{Lavender}{\textsc{Uniform Soup}} &0.206 &0.295 &0.519 &0.352 &0.500 &0.430 &0.036 &\ul{0.392} &0.105 &0.166 &0.003 &0.455 \\
\textcolor{Lavender}{\textsc{Dare-Ties}} &0.230 &0.372 &0.476 &0.307 &0.544 &0.427 &0.071 &0.300 &0.108 &0.137 &0.140 &0.515 \\
\textcolor{Dandelion}{\textsc{Greedy Soup}} &0.219 &0.355 &\ul{0.539} &0.330 &0.530 &0.543 &0.071 &0.333 &0.114 &0.184 &0.014 &\ul{0.565} \\
\textcolor{Dandelion}{\textsc{Pack of LLMs}} &0.235 &0.352 &0.512 &0.327 &0.532 &0.543 &\ul{0.143} &\ul{0.392} &0.106 &0.157 &0.095 &0.545 \\
\textcolor{Dandelion}{\textsc{LoraHub}} &0.231 &0.291 &0.502 &0.354 &0.568 &0.548 &0.071 &0.375 &0.106 &0.169 &0.064 &0.530 \\
\textcolor{Dandelion}{\textsc{Model Swarms}} &\ul{0.254} &\ul{0.428} &0.505 &\ul{0.459} &\textbf{0.672} &\ul{0.554} &0.107 &0.358 &0.135 &\ul{0.225} &\ul{0.175} &0.540 \\ \midrule[0.75pt]
\textcolor{Maroon}{\textsc{Chain}} &0.216 &0.310 &0.495 &0.295 &0.462 &0.489 &\ul{0.143} &0.325 &\ul{0.148} &0.218 &0.014 &0.493 \\
\textcolor{Maroon}{\textsc{Star}} &0.250 &0.342 &0.508 &0.333 &0.545 &0.518 &0.036 &0.283 &0.130 &0.216 &0.125 &0.499 \\
\textcolor{Orange}{\textsc{GPT-Swarm}} &0.216 &0.320 &0.460 &0.334 &0.611 &0.510 &\ul{0.143} &0.333 &0.134 &0.216 &0.023 &0.492 \\
\textcolor{Orange}{\textsc{Meta-Agent}} &0.212 &0.276 &0.477 &0.433 &0.515 &0.369 &0.071 &0.325 &0.112 &0.167 &0.016 &0.472 \\
\textcolor{Orange}{\textsc{Agent-Prune}} &0.214 &0.321 &0.497 &0.180 &0.460 &0.467 &0.107 &0.333 &0.122 &0.211 &-0.005 &0.470 \\
\textcolor{Orange}{\textsc{GNNs}} &0.201 &0.339 &0.479 &0.364 &0.593 &0.530 &0.071 &0.308 &\ul{0.148} &0.203 &0.076 &0.503 \\ \midrule[0.75pt]
\textcolor{OliveGreen}{\textsc{H-Swarms}} &\textbf{0.312} &\textbf{0.450} &\textbf{0.579} &\textbf{0.481} &\ul{0.660} &\textbf{0.588} &\textbf{0.250} &\textbf{0.425} &\textbf{0.215} &\textbf{0.266} &\textbf{0.220} &\textbf{0.590} \\
\bottomrule[1.5pt]
\end{tabular}
}
\caption{Performance on the 12 datasets, best in \textbf{bold} and second-best in \ul{underline}. \textcolor{OliveGreen}{\ourmethod{}} outperforms \textcolor{NavyBlue}{\textsc{Trivial}}, \textcolor{Lavender}{\textsc{Static Weight}}, \textcolor{Dandelion}{\textsc{Dynamic Weight}}, \textcolor{Maroon}{\textsc{Static Role}}, and \textcolor{Orange}{\textsc{Dynamic Role}} approaches by 18.5\% on average across tasks.}
\vspace*{-10pt}
\label{tab:big}
\end{table*}

While existing work often uses static models across tasks and contexts \citep{pmlr-v235-zhuge24a, duimproving}, we posit that model weights could be optimized to adapt to the task as well as model roles in the DAG. However, it is challenging to quantify the utility of a single LLM in a multi-LLM system, so that model weight optimization is compatible with swarm intelligence. We propose \emph{JFK-score}\footnotemark[2]\footnotetext[2]{\ \emph{``Ask not what the multi-LLM system can do for you, ask what you can do for the multi-LLM system.''} ----- Authors, 2025}, an algorithm to evaluate individual contribution in multi-LLM systems.

Concretely, given the best-found DAG in the role-step $\mathbf{A}_{\textit{best}}$ and the pool of LLM experts $\{\mathbf{x}_i\}_{i=1}^n$, we randomly select an LLM for each position in $\mathbf{A}_{\textit{best}}$ to obtain an assignment $\mathcal{X}$, where $\mathbf{A}_{\textit{best}} \mid \mathcal{X}$ represents an instantiated multi-LLM system. We repeat assignment $M$ times to obtain $\{\mathcal{X}^i\}_{i=1}^M$ and evaluate their utility $f(\mathcal{X}^i) = f(\mathrm{G \mhyphen decode}(\mathbf{A}^i \mid \mathcal{X}^i))$. The individual contribution of model $\mathbf{x}_i$ should then be the aggregate of utility weighted by model $i$'s frequency: we denote the frequency of $\mathbf{x}_i$ in $\mathcal{X}^j$ as $\mathrm{cnt}_{i,j} = \sum_{k=1}^n \mathbbm{1}(\mathcal{X}^j_k = \mathbf{x}_i)$ and the individual contribution score should be:

\begin{algorithm}[t]
\label{alg:jfkscore}
\caption{$\mathrm{JFK \mhyphen score}$}
\KwIn{adjacency matrix $\mathbf{A}_\textit{best}$ and models $\{\mathbf{x}_i\}_{i=1}^n$}

\For{$i = 1$ \KwTo $M$ }{
sample assignment $\mathcal{X}^i$ by randomly select models in $\{\mathbf{x}_i\}_{i=1}^n$ to fill each position in $\mathbf{A}_\textit{best}$ \;
utility $f(\mathcal{X}^i) = f(\mathrm{G \mhyphen decode}(\mathbf{A}_\textit{best} \mid \mathcal{X}^i))$ \;
}
\For{$i = 1$ \KwTo $n$ }{
$\mathrm{JFK \mhyphen score}(\mathbf{x}_i) = \frac{\sum_{j=1}^M \mathrm{cnt}_{i,j} \times f(\mathcal{X}^i)}{\sum_{j=1}^M \mathrm{cnt}_{i,j}}$, where $\mathrm{cnt}_{i,j}$ denotes the frequency of $\mathbf{x}_i$ in assignment $\mathcal{X}^j$
}

\Return $\{\mathrm{JFK \mhyphen score}(\mathbf{x}_i)\}_{i=1}^n$

\end{algorithm}

\begin{algorithm}[t]
\label{alg:overall}
\caption{Heterogeneous Swarms}
\KwIn{language models $\{\mathbf{x}_i\}_{i=1}^n$, utility function $f$}
%\KwOut{a directed acyclic graph $\mathcal{G}$ of adapted models $\{\mathbf{x}_i\}_{i=1}^n$}

%\Comment*[l]{initialize a bunch of DAGs}
sample $\{\mathbf{A}^i\}_{i=1}^N \sim \mathcal{U}_{n \times n}(0,1)$ \;
\While{$f$ is improving}{
    \Comment*[l]{role-step}
    $\{\mathbf{A}^i\}_{i=1}^N, \mathbf{A}_{\textit{best}} \leftarrow \mathrm{PSO}(\{\mathbf{A}^i\}_{i=1}^N, \{f(\mathrm{G \mhyphen decode}(\mathbf{A}^ i))\}_{i=1}^N)$ \;
    \Comment*[l]{weight-step}
    
    $\{\mathbf{x}_i\}_{i=1}^n, \mathbf{x}_{\textit{best}} \leftarrow \mathrm{PSO}(\{\mathbf{x}_i\}_{i=1}^n, \{\mathrm{JFK \mhyphen score}(\mathbf{x}_i)\}_{i=1}^n)$
}
\Return $\mathrm{G \mhyphen decode}(\mathbf{A}_{\textit{best}} \mid \{\mathbf{x}_i\}_{i=1}^n)$

\end{algorithm}

\vspace{-10pt}

\begin{align*}
    \mathrm{JFK \mhyphen score}(\mathbf{x}_i) = \frac{\sum_{j=1}^n \mathrm{cnt}_{i,j} \times f(\mathcal{X}^i)}{\sum_{j=1}^n \mathrm{cnt}_{i,j}}
\end{align*}

\vspace{-10pt}
In this way, $\mathrm{JFK \mhyphen score}(\mathbf{x}_i)$ quantifies the individual contribution of model $\mathbf{x}_i$ across multiple roles and collaboration partners. (Algorithm \ref{alg:jfkscore}) We optimize model weights guided by their individual contribution with swarm intelligence:
\vspace{-10pt}

\begin{align*}
    \{\mathbf{x}_i\}_{i=1}^n, \mathbf{x}_{\textit{best}} \leftarrow \mathrm{PSO}(\{\mathbf{x}_i\}_{i=1}^n, \{\mathrm{JFK \mhyphen score}(\mathbf{x}_i)\}_{i=1}^n)
\end{align*}

\ourmethod{} alternates between role-step and weight-step, iteratively optimizing model roles and weights in the multi-LLM system to adapt to the task $f$. We present the overall procedure in algorithm \ref{alg:overall}.

\section{Experiment Settings}
\label{sec:experiment_settings}

\paragraph{Models and Implementation} We implement a prototype of \ourmethod{} with \textsc{Gemma-7B} (\emph{google/gemma-7b-it}) \citep{team2024gemma} in the main paper and also employ other LLMs such as \textsc{Mistral-7B} in Table \ref{tab:mistral}. We employ the pool of 10 LLM experts in \citet{feng2024model} for fair comparison, fine-tuned from \textsc{Gemma-7B} using 10 domains in Tulu-v2 \citep{ivison2023camels} spanning reasoning, code, general instruction following, and more: We employ $p=0.8$ for top-p sampling, $N=10$, $M=10$, search patience 6, max iteration 20, while running grid search over other hyperparameters and report performance of the best-found multi-LLM systems.

\paragraph{Baselines} We compare with 15 baselines across 5 categories that focus on optimizing model roles and/or weights.

\begin{itemize}
    \item \emph{trivial baselines}: 1) \emph{Best Single} expert, essentially $\argmax_{\mathbf{x}} f(\mathbf{x})$ for $\mathbf{x} \in \{\mathbf{x}_i\}_{i=1}^n$; 2) \emph{Prediction Merge}, where the predictions of $\{\mathbf{x}_i\}_{i=1}^n$ are ensembled via plurality vote (if applicable).
    \item \emph{static weight}: these approaches conduct model merging independent of the task and utility function $f$: \emph{Data Merge}, \emph{Uniform Soup} \citep{wortsman2022model}, and \emph{Dare-Ties} \citep{yu2024languagedare, yadav2024ties}.
    \item \emph{dynamic weight}: these approaches optimize model weights based on the utility function $f$: \emph{Greedy Soup} \citep{wortsman2022model}, \emph{Pack of LLMs} \citep{mavromatis2024pack}, \emph{LoraHub} \citep{huang2023lorahub}, and \emph{Model Swarms} \citep{feng2024model}.
    \item \emph{static role}: we employ two structures in hand-crafted agent systems \citep{langgraph}: \emph{chain} and \emph{star}.
    \item \emph{dynamic role}: these approaches optimize model roles and connections based on $f$: \emph{GPT-Swarm} \citep{pmlr-v235-zhuge24a}, \emph{Meta-Agent} \citep{hu2024automated}, \emph{Agent-Prune} \citep{zhang2024cut}, and \emph{GNNs} \citep{zhang2024g}.
\end{itemize}

\paragraph{Data and Evaluation} We compare \ourmethod{} against baselines on 12 datasets spanning 4 categories: 1) knowledge: MMLU-pro \citep{wang2024mmlu}, Knowledge Crosswords \citep{ding-etal-2024-knowledge}, and COM2 \citep{fang-etal-2024-complex}; 2) reasoning: GSM8k \citep{cobbe2021training}, NLGraph \citep{wang2024can}, and Normad \citep{rao2024normad}; 3) agent: GAIA-text \citep{mialongaia}, the knowledge graph and lateral thinking puzzle subtasks of AgentBench \citep{liuagentbench}; 4) miscellaneous: long-context with Qasper \citep{dasigi2021dataset}, reliability with AbstainQA \citep{feng-etal-2024-dont}, and LLM-as-a-judge with WoW \citep{yao2024varying}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/collaborative_gains.pdf}
    \vspace*{-20pt}
    \caption{Evaluating collaborative gains: we create problem buckets by how many out of the 10 individual LLMs could solve it correctly. Top row: the problem count as well as whether the multi-LLM correctly solves the problems in each bucket. Bottom row: accuracy of the multi-LLM system in each bucket and expected accuracy denoted by the dotted line. \ourmethod{} achieves collaborative gains ($\mathrm{C \mhyphen Gain}$) of 0.143, 0.184, 0.101, and 0.426 on the four datasets, all $>0$, and demonstrate consistent collaborative gains.
    %\swj{nitpicking: the text on the images is bit too small}
    }
    \label{fig:collaborative_gains}
    \vspace*{-10pt}
\end{figure*}

\begin{table*}[!htp]\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{lcccccccccccc}\toprule[1.5pt]
&\multicolumn{3}{c}{Knowledge} &\multicolumn{3}{c}{Reasoning} &\multicolumn{3}{c}{Agent} &\multicolumn{3}{c}{Miscellaneous} \\\cmidrule[0.75pt]{2-13}
&MMLU-pro &K-Cross &COM2 &GSM8k &NLGraph &Normad &GAIA-text &AB-kg & AB-ltp &Qasper &AbstainQA &WoW \\ \midrule[0.75pt]
Role Baselines &0.218 &0.318 &0.486 &0.323 &0.531 &0.480 &0.095 &0.318 &0.132 &0.205 &0.042 &0.488 \\
Weight Baselines &0.222 &0.352 &0.490 &0.325 &0.538 &0.494 &0.082 &0.342 &0.112 &0.169 &0.067 &0.516 \\
Ours w/o Role &0.242 &0.352 &0.515 &0.392 &0.530 &0.564 &0.107 &0.317 &0.140 &0.222 &0.133 &0.539 \\
Ours w/o Weight &0.237 &0.342 &0.492 &0.363 &0.588 &0.557 &0.143 &0.325 &0.164 &0.241 &0.119 &0.510 \\
Ours Full &0.312 &0.450 &0.579 &0.481 &0.660 &0.588 &0.250 &0.425 &0.215 &0.266 &0.220 &0.590 \\ \midrule[0.75pt]
Consistent? &\textcolor{OliveGreen}{\textsc{True}} &\textcolor{OliveGreen}{\textsc{True}} &\textcolor{OliveGreen}{\textsc{True}} &\textcolor{OliveGreen}{\textsc{True}} &\textcolor{Maroon}{\textsc{False}} &\textcolor{OliveGreen}{\textsc{True}} &\textcolor{OliveGreen}{\textsc{True}} &\textcolor{Maroon}{\textsc{False}} &\textcolor{OliveGreen}{\textsc{True}} &\textcolor{OliveGreen}{\textsc{True}} &\textcolor{OliveGreen}{\textsc{True}} &\textcolor{OliveGreen}{\textsc{True}} \\
\bottomrule[1.5pt]
\end{tabular}
}
\caption{Ablation study of removing the role-step or weight-step in \ourmethod{}, comparing whether the importance of role/weight is consistent between baselines and our approach. The pattern is consistent in 10 out of 12 datasets, confirming that model roles and weights could have different levels of importance for varying tasks.}
\vspace*{-10pt}
\label{tab:ablation_study}
\end{table*}

\section{Results}
\label{sec:results}

We present the performance of \ourmethod{} and baselines on the 12 tasks in Table \ref{tab:big}.

\paragraph{\ourmethod{} consistently discovers state-of-the-art multi-LLM systems.} \ourmethod{} achieves the best performance on 11 of the 12 datasets, outperforming the second-best approach by 18.5\% on average. This indicates that starting from the same pool of initial LLMs, \ourmethod{} could flexibly discover multi-LLM systems that adapt to diverse tasks and contexts spanning knowledge, reasoning, and more.

\paragraph{Role and weight are disproportionately important for different tasks.} For knowledge tasks, weight baselines (\textcolor{Lavender}{\textsc{Static Weight}} and \textcolor{Dandelion}{\textsc{Dynamic Weight}}) outperform role baselines (\textcolor{Maroon}{\textsc{Static Role}} and \textcolor{Orange}{\textsc{Dynamic Role}}) by 4.3\% on average. However, for agent tasks, role baselines are 9.2\% better. This indicates that role and weight have varying importance in different tasks: by jointly optimizing roles and weights in multi-LLM systems, \ourmethod{} flexibly adapts to both scenarios. We further investigate their importance in Section \ref{sec:analysis}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/role_statistics.pdf}
    \vspace*{-10pt}
    \caption{Analyzing the roles in Multi-LLM systems. Top left: the percentage of LLM roles aggregated per dataset. Bottom left: the change of LLM roles in the optimization process for NLGraph. Right: Per-LLM role distribution in the best-found multi-LLM system for NLGraph. Together these figures demonstrate the heterogeneous roles in the multi-LLM systems by \ourmethod{}.}
    \label{fig:role_statistics}
    \vspace*{-10pt}
\end{figure*}

\paragraph{Dynamic adaptation works better than static engineering.} We find that \textcolor{Dandelion}{\textsc{Dynamic Weight}} approaches outperform \textcolor{Lavender}{\textsc{Static Weight}} by 30.1\% across tasks, while \textcolor{Orange}{\textsc{Dynamic Role}} outperforms \textcolor{Maroon}{\textsc{Static Role}} by up to 8.2\%. This indicates that instead of hand-crafted ways to design roles or merge weights, dynamic adaptation approaches guided by utility function $f$ offer better adaptation. \ourmethod{} employs role-step and weight-step to dynamically adapt both for $f$, resulting in flexible multi-LLM systems adapted to diverse tasks and applications.

\section{Analysis}
\label{sec:analysis}

\paragraph{Collaborative Gains} To justify the cost of calling multiple LLMs in topological order, a multi-LLM system should unlock $1+1>2$ effects: the multi-LLM collaboration should produce \emph{collaborative gains} compared to employing a single LLM. Concretely:

\resizebox{1\linewidth}{!}{
\fbox{%
    \parbox{\linewidth}{%
        \ \ \ \ \ \ \ For problem $q$, if $p\%$ of the component LLMs could solve it individually, then the multi-LLM system should have a larger than $p\%$ likelihood of solving it.
    }%
}
}

To quantify this, we group problems in dataset $\mathcal{D}$ by how many component LLMs could solve it individually: problem $q \in B_n$ if $n$ of the $N$ component LLMs could solve it. We then calculate the metric \emph{Collaborative Gain} as:
\vspace*{-5pt}
\begin{align*}
    \mathrm{C \mhyphen Gain} = \sum_{n=1}^N \frac{\mid B_n \mid}{\mid \mathcal{D} \mid} \big(\mathrm{Acc}(B_n) - \mathrm{EA}(B_n)\big)
\end{align*}
where $\mathrm{Acc}(B_n)$ denotes the accuracy of the multi-LLM systems for problems in $B_n$, $\mathrm{EA}(B_n)$ denotes the Expected Accuracy $n/N$, $|B_n|$ and $|\mathcal{D}|$ denote the number of problems in the bucket and the dataset. If a multi-LLM system satisfies the principle, then $\mathrm{Acc}(B_n) > \mathrm{EA}(B_n)$ and $\mathrm{C \mhyphen Gain} > 0$.

We present the collaborative gains of \ourmethod{} in Figure \ref{fig:collaborative_gains}.  \ourmethod{} achieves consistent positive collaborative gains with an average of 0.213. For problems in bucket $B_0$, i.e. none of the initial LLMs could solve individually, \ourmethod{} discovers multi-LLM systems that solve 18.1\% of them on average. This indicates that \ourmethod{} could find adapted multi-LLM systems with new compositional skills and substantial collaborative gains.

\paragraph{Importance of Role and Weight through Ablation Study} In section \ref{sec:results}, we discover that model roles and weights could be disproportionately important for different tasks based on baseline performance. We compare the trend with our approach, specifically through disabling either the role step or the weight step in \ourmethod{} (\textit{w/o Role} and \textit{w/o Weight}). Let $B_r$ and $B_w$ denote the average performance of role/weight-based baselines, then the importance of role/weight is consistent when the following logic expression is True:
\vspace*{-5pt}
\begin{align*}
    & \big((\textit{w/o Role} < \textit{w/o Weight}) \ \mathrm{AND} \ (B_r > B_w)\big) \\
    & \mathrm{OR} \ \big((\textit{w/o Role} > \textit{w/o Weight}) \ \mathrm{AND} \ (B_r < B_w)\big)
\end{align*}
\vspace*{-15pt}

\begin{table}[t]\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{lcccccc}\toprule[1.5pt]
&\multicolumn{2}{c}{K-Cross} &\multicolumn{2}{c}{NLGraph} &\multicolumn{2}{c}{AB-kg} \\\cmidrule[0.75pt]{2-7}
&Acc &Speedup &Acc &Speedup &Acc &Speedup \\\midrule
original &0.450 &0.0\% &0.660 &0.0\% &0.425 &0.0\% \\ \midrule[0.75pt]
$\tau = 0.05$ &0.438 &3.8\% &0.628 &4.9\% &0.392 &9.2\% \\
$\tau = 0.1$ &0.392 &12.4\% &0.609 &9.3\% &0.375 &19.8\% \\
$\tau = 0.2$ &0.352 &36.1\% &0.581 &13.6\% &0.383 &21.4\% \\ \midrule[0.75pt]
$\lambda = 0.01$ &0.436 &1.2\% &0.642 &0.9\% &0.400 &3.4\% \\
$\lambda = 0.05$ &0.425 &3.5\% &0.614 &2.3\% &0.383 &9.1\% \\
$\lambda = 0.1$ &0.426 &7.1\% &0.598 &2.5\% &0.392 &12.8\% \\
\bottomrule[1.5pt]
\end{tabular}
}
\caption{Encouraging sparsity in multi-LLM systems with thresholed pruning ($\tau$) or normalization ($\lambda$). These strategies bring various tradeoffs bewteen performance and inference speedup.}
\label{tab:sparsity}
\end{table}

We present performance of the ablated settings and the value of the logic expression across 12 datasets in Table \ref{tab:ablation_study}. Results demonstrate that roles and weights could have varying importance (e.g. weight is more important for knowledge tasks while role is more important for agent) and such importance is consistent in 10 of the 12 tasks. In addition to jointly adapting model roles and weights, \ourmethod{} offers insights into their importance for the task at hand.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/diversity.pdf}
    \vspace*{-10pt}
    \caption{\ourmethod{} with increasing levels of diversity in initial LLM experts. Results show a general upward trend and an 89\% increase on average from the least to most diverse initial experts.}
    \label{fig:diversity}
    \vspace*{-10pt}
\end{figure*}

\paragraph{Role Statistics} We first manually examine the input/output of multi-LLM systems, identifying four potential roles of individual models in the multi-LLM system: 1) \emph{divide}, where the multi-LLM identifies and solves part of the problem; 2) \emph{refine}, where the LLM proposes a new (sub)answer based on previous steps; 3) \emph{feedback}, where the LLM provides feedback on previous steps; 4) \emph{irrelevant}, where the LLM fails to generate relevant text. We employ Gemini-as-a-judge \citep{team2023gemini} (\textsc{gemini-1.5-flash}) to automatically identify the role of individual LLMs and conduct three analysis in Figure \ref{fig:role_statistics}.

We analyze the variation of model roles across tasks in Fig.\ref{fig:role_statistics}, top left. For the graph reasoning task NLGraph, there is greater \emph{divide} and conquer to solve part of the problem; for the knowledge tasks such as K-Crosswords, there is greater \emph{feedback} to identify knowledge gaps in existing answers. Guided by different $f$s, \ourmethod{} discover multi-LLM systems with different role distributions.

We investigate the roles of LLMs in different positions of the DAG on NLGraph in Fig.\ref{fig:role_statistics}, right. We find that individual LLMs do have heterogeneous role distributions given their topological position: for the branching nodes there is often higher \emph{divide}, while for the converging nodes there is often higher \emph{refine} and \emph{feedback}. This indicates that \ourmethod{} successfully discovers multi-LLM systems where individual LLMs play heterogeneous roles.

We finally plot the change of model roles in the optimization process in Figure \ref{fig:role_statistics}, bottom left, when adapting to NLGraph. We observe that \emph{irrelevant} roles gradually decrease while the three other functional roles consistently increase in the course of optimization. This indicates that by integrating weight optimization, \ourmethod{} improves the quality of LLMs to adapt to the task at hand.

\paragraph{Encouraging Sparsity} We observe that if the network is dense, i.e. when one LLM takes the output of too many LLMs as input, it might lead to irrelevant outputs. In addition, encouraging sparsity in the network would also reduce context lengths and hence inference costs. To this end, we encourage sparsity in the multi-LLM network by two means: 1) by setting a threshold $\tau \in [0,1]$ and pruning the continuous adjacency $\mathcal{A}$ with $\mathcal{A}' = \{a_{ij} \cdot \mathbbm{1}[\max(a_{ij}-\tau, 0)] \}$; 2) by adding a $\mathcal{L}_1$ normalization term to the utility function: $f' = f + \lambda \|\mathcal{A}\|_1$. We employ various values of $\tau$ and $\lambda$: Table \ref{tab:sparsity} presents the performance and speedup calculated by the percentage of reduced connections in the network. Results demonstrate that $\mathcal{L}_1$ better retains performance while thresholding presents a larger reduction in inference cost.

\paragraph{Diversity Matters} \ourmethod{} operates on the assumption that the multiple large language models would complement each other based on their diverse expertise. To investigate whether this model diversity is crucial, we conduct an controlled experiment where we fix the LLM pool size as 10, but with $a$ distinct models each repeated $b$ times. We denote this as $a \times b$ and employ $1 \times 10$, $2 \times 5$, $5 \times 2$, and $10 \times 1$ (default setting) with increasing diversity. Figure \ref{fig:diversity} illustrates that the diversity of initial LLMs does help greatly, with an average relative improvement of 89\% from the least to most diverse.

%\vspace*{-10pt}
\section{Related Work}

A growing line of research work focuses on \emph{multi-LLM collaboration}, where multiple LLMs collaboate and complement each other. These approaches focus on either roles or weights for multi-LLM collaboration.

Role-based approaches typically rely on assigning roles to LLMs through prompt engineering \citep{duimproving, feng-etal-2024-dont}. Multiple LLMs, or even just a single LLM seeded with different prompts, then collaborate with their prompt-induced roles through exchanging generated texts. For example, specialized LLMs could augment a general-purpose model \citep{fengknowledge, shen-etal-2024-learning}; LLMs could generate feedback for each other's responses to collectively self-refine \citep{feng-etal-2024-dont, burnsweak}; multi-agent systems could divide and conquer complex problems \citep{wu2024autogen, guo2024large}; multiple LLMs could debate and compete with each other to find better answers  \citep{liang2023encouraging, duimproving}. These approaches are often hindered by the need for prompt engineering and the effectiveness of prompts for model steerability \citep{sprague2024cot, sclarquantifying}, thus \ourmethod{} uniquely interprets model roles as input-output relationships, optimizing directed acyclic graphs of LLMs to learn contextual roles and specialization.

Weight-based approaches typically focus on adapting the logits/weights of multiple LLMs, notably through mixture-of-experts or model merging \citep{yadav2024survey}. The hidden states or logit distributions of multiple models could be selected, routed, and aggregated based on various MoE mechanisms \citep{li2022branch, gritsch2024nexus}. In addition, static \citep{yu2024languagedare, yadav2024ties, jang2024model} and dynamic \citep{mavromatis2024pack, akiba2024evolutionary, huang2023lorahub} model merging approaches incorporate the diverse expertise of heterogeneous LLMs into a single model in zero-shot and adaptation settings. We continue to believe that weight adaptation is crucial for specializing individual LLMs in multi-LLM systems: guided by the first successes of evolutionary algorithms in weight-based collaboration \citep{feng2024model} and LLMs in general \citep{akiba2024evolutionary, fernandopromptbreeder, guoconnecting}, \ourmethod{} employs swarm intelligence to optimize model weights guided by each LLM's individual contribution to the multi-LLM system.

\ourmethod{} uniquely offers a flexible methodology to jointly optimize the roles and weights of diverse LLMs, discovering and adapting multi-LLM systems for wide-ranging tasks and applications.

\vspace*{-5pt}
\section{Conclusion}
We propose \ourmethod{}, an algorithm to jointly optimize model roles and weights for multi-LLM systems and collaborative generation. By rotating between role-step and weight-step to optimize the network of LLMs as well as model weights, \ourmethod{} discovers directed acyclic graphs of LLMs that could be called in topological order to adapt to a given task. \ourmethod{} outperforms 15 role and weight-based baselines on 12 tasks, demonstrating collaborative gains and heterogeneous roles through multi-LLM collaboration.

\section*{Impact Statement}
\ourmethod{} by default operates with a pool of LLMs sharing the same model architecture: this is to ensure that models share the same parameter space for weight-step optimization. If the pool of LLMs contain different architectures, then \ourmethod{} could either operate with only the role-step to optimize the multi-LLM network, or employ token-level collaboration for the weight step similar to \citet{liu2024tuning} and \citet{feng2024model}.

Both the optimization and inference process of \ourmethod{} features calling multiple LLMs, increasing its potential cost. We propose various strategies to alleviate this: by encouraging sparsity in the multi-LLM network in Table \ref{tab:sparsity}, by analyzing the time complexity in Section \ref{sec:analysis_cont}, and by employing Dropout-W/R strategies in Figure \ref{fig:drop}, we show that \ourmethod{} could speed up with one or multiple strategies used in conjunction.

While we conduct experiments where all the nodes in the network are neural language models, we highlight several other possibilities we weren't able to evaluate: employing black-box models in the LLM pool, which is compatible if we skip these models in the weight-step; employing tools and APIs as nodes, as long as they define an input-output relationship.

We envision two extra steps to take to further expand the expressive power of \ourmethod{}: allowing for cycles in the multi-LLM network in case one model needs to be called multiple times, together with an exit function to decide when to pull out from the cycle; allowing for instance-level changes to the multi-LLM network with a learnable edit function.

Since multiple LLMs are called in topological order of the multi-LLM system, we envision potential risks when one or several of the component LLMs are unintentionally or intentionally compromised. For example, if one of the LLMs were to exhibit substantial social biases \citep{kumar2023language} or malicious intent \citep{zhao2023malicious}, it might pass on to future models in the network and result in cascading effects. We thus argue that \ourmethod{} should be initialized with carefully selected pool of specialized LLMs and envision future work on identifying harmful/malicious components in multi-LLM collaboration.


\bibliography{example_paper}
\bibliographystyle{icml2025}

%\newpage

\appendix

\section{Analysis (cont.)}
\label{sec:analysis_cont}

\paragraph{Scaling Multi-LLM Collaboration} Recent research explored the inference-time scaling of a single, often gargantuan, LLM \citep{snell2024scaling, brown2024large, wu2024inference}. However, it is unclear whether inference-time scaling is possible for small language models, while we believe \ourmethod{} offers a way of scaling through topological message passing and collaborative generation. We conduct a scaling study by employing 2, 4, 6, 8, and 10 (default) of initial LLM checkpoints as the starting model swarn and run \ourmethod{}: results in Figure \ref{fig:scaling} demonstrates that \ourmethod{} indeed offers an inference-time scaling paradigm for smaller language models through multi-LLM collaboration.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/scaling.pdf}
    \caption{Scaling the number of LLM experts from 2 to 10 results in consistent improvements across 4 datasets.}
    \label{fig:scaling}
    \vspace*{-10pt}
\end{figure}

\paragraph{Time Complexity} Given a pool of $n$ LLMs, we employ a swarm of $N$ graphs and $M$ model assignment instances. We measure the complexity by the number of model inferences as they are the main computational cost. At optimization time, the role step calls model inference $O(nN)$ times and the weight step $O(nM)$ times, so the overall optimization cost per iteration is $O(n(N+M)|f|)$ where $|f|$ denotes the adaptation data size, linear to $n$, $N$, and $M$. At inference time, the cost is $O(n)$ where the adapted LLMs are called in topological order of the network. We further illustrate the empirical complexity by plotting the time per iteration given different amount of A100 40GB GPUs in Figure \ref{fig:time}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/time.pdf}
    \caption{Optimization time changes with the number of A100 40GB GPUs employed. We employ 1, 2, 5, and 10 GPUs since they could be divided by 10, the amount of LLMs.}
    \label{fig:time}
    \vspace*{-10pt}
\end{figure}

\paragraph{HS(weak) $>$ strong} A successful collaboration of multiple weak models can beat a stronger LLM: we withhold the top-1 LLM in the given task and discover a multi-LLM system out of the remaining 9 models with \ourmethod{} (HS(2-10)). Additionally, we also withhold the top half and evaluate the collaboration of the bottom half (HS(6-10)). Figure \ref{fig:weak} demonstrates that \ourmethod{} enables the collaboration of weaker models to outperform the top-1 individual LLM, enabling the weak-to-strong transition through collaborative generation.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/weak.pdf}
    \caption{The collaboration of the all-but-top-1 and bottom-half LLMs through \ourmethod{} could beat the top-1 individual LLM.}
    \label{fig:weak}
    \vspace*{-10pt}
\end{figure}

\paragraph{Accelerating with Dropout-R/W} To further speedup the optimization process, we process to randomly skip the role-step or weight-step with $d_r\%$ or $d_w\%$ likelihood. We illustrate the performance with $d_r, d_w \in \{0.2, 0.5, 0.8\}$ in Figure \ref{fig:drop}. It is illustrated that the acceleration only comes with a minor performance drop, showing the flexibility of \ourmethod{} to adapt to different computational budget.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/drop.pdf}
    \caption{By randomly skipping the role-step and weight-step through Drop-R and Drop-W, we could speed up the optimization process.}
    \label{fig:drop}
    \vspace*{-10pt}
\end{figure}

\paragraph{Another LLM} We by default employ \textsc{Gemma-7B} as the component LLMs. We additionally employ \textsc{Mistral-7B} ((\emph{mistralai/Mistral-7B-Instruct-v0.3})) \citep{jiang2023mistral} for \ourmethod{} and present performance in Table \ref{tab:mistral}. Results show that \ourmethod{} is compatible with \textsc{Mistral-7B} too, outperforming the best single Mistral-based LLM.

\begin{table}[t]\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1}
\resizebox{1\linewidth}{!}{
\begin{tabular}{lccccc}\toprule[1.5pt]
Setting &MMLU-pro &K-Cross &GSM8k &NLGraph &AbstainQA \\ \midrule[0.75pt]
Best Single &0.146 &0.364 &0.303 &0.325 &0.081 \\
Ours &0.212 &0.417 &0.313 &0.505 &0.123 \\
\bottomrule[1.5pt]
\end{tabular}
}
\caption{Performance of \ourmethod{} with 10 Mistral-based LLMs.}
\label{tab:mistral}
\end{table}

\section{Experiment Details}

\paragraph{Dataset Details} We employ 12 datasets to evaluate \ourmethod{} and baselines spanning knowledge, reasoning, agent, and miscellaneous capabilities. We filter examples in GAIA to retain examples where the human-provided tool use contexts could support the final answer for GAIA-text. We incorporate each tool use context for the input of one LLM in the multi-LLM network respectively. We truncate the context in Qasper in ten-fold and incorporate each chunk as context for an individual LLM. We by default sample 200 examples for optimization and 1,000 for evaluation, while downsampling if there's not enough data. We present data statistics in Table \ref{tab:dataset_statistics}. MMLU-pro, Knowledge Crosswords, COM2, Normad, AgentBench-KG, AbstainQA, and WoW are evaluated in multiple-choice settings. GSM8k, NLGraph, and GAIA-text are evaluated via exact match. AgentBench-LTP and Qasper are evaluated by Gemini for answer similarity on a scale of 1 to 10, normalized to 0 to 1. We also employ the z-test with the one-tailed hypothesis and present statistical significance test results on the datasets.

\begin{table}
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1}
%\vspace*{-10pt}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{lccc}
\toprule[1.5pt]
\multirow{2}{*}{Dataset} &\multirow{2}{*}{Source} &\multicolumn{2}{c}{Size} \\\cmidrule{3-4}
& &dev &test \\\midrule
MMLU-pro*** & \citet{wang2024mmlu} & 70 & 1000 \\
K-Crosswords*** & \citet{ding-etal-2024-knowledge} & 200 & 1000 \\
COM$^2$** & \citet{fang-etal-2024-complex} & 317 & 1000 \\
GSM8k & \citet{cobbe2021training} & 200 & 1000 \\
NLGraph & \citet{wang2024can} & 200 & 1000 \\
Normad** & \citet{rao2024normad} & 500 & 2000 \\
GAIA-text & \citet{mialongaia} & 13 & 28 \\
AgentBench-KG & \citet{liuagentbench} & 50 & 120 \\
AgentBench-LTP & \citet{liuagentbench} & 20 & 50 \\
Qasper & \citet{dasigi2021dataset} & 100 & 100 \\
AbstainQA*** & \citet{feng-etal-2024-dont} & 200 & 1000 \\
WoW & \citet{yao2024varying} & 200 & 1000 \\
\bottomrule[1.5pt]
\end{tabular}
}
\caption{Statistics of employed datasets. *, **, and *** indicates the improvement on this dataset is statistically significant with $p<0.1$, $p<0.05$, and $p<0.01$ with one-tailed z-test.}
\vspace*{10pt}
\label{tab:dataset_statistics}
\end{table}

\paragraph{Implementation Details} We employ the 10 LLM experts in \citet{feng2024model} as the initial swarm of models. We employ the Gemma-based \citep{team2024gemma} versions for experiments in the main paper for a fair comparison and employ the Mistral-based \citep{jiang2023mistral} versions for experiments in Table \ref{tab:mistral}. Aside from the hyperparmaters specified in Section \ref{sec:experiment_settings}, we run grid search over other hyperparameters and report the best-found expert based on utility function $f$. Specifically, $\phi_v \in \{0.1, 0.2, 0.3\}$, $\phi_p \in \{0.1, 0.2, 0.3, 0.4, 0.5\}$, $\phi_g \in \{0.2, 0.3, 0.4, 0.5, 0.6\}$, $\phi_w \in \{0.01, 0.05, 0.1\}$, $\lambda \in \{0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}$. We run up to 50 to 200 runs by randomly choosing over these hyperparameter search settings and report the best-found expert on utility function $f$.

\paragraph{Baseline Details} For \emph{best single}, \emph{prediction merge}, \emph{data merge}, \emph{uniform soup}, \emph{dare-ties}, \emph{greedy soup}, \emph{pack of LLMs}, \emph{lorahub}, and \emph{model swarms}, we follow the settings in \citet{feng2024model}. For \emph{chain}, we organize the 10 initial LLMs into a chain, randomly permute their assignments for 20 times, and report the best-found chain on $f$. For \emph{star}, we organize them into a 1-8-1 structure, randomly permute their assignments for 20 times, and report the best-found star on $f$. For \emph{gpt-swarm}, we employ its method to optimize the network structure and do not consider prompt optimization for a fair comparison. To make the optimization compatible to our non-differentiable setting, we modify it into interpolation of adjacency matrices based on $f$ utility values. For \emph{meta-agent}, we employ Gemini-pro (\emph{gemini-pro-1.5-001}) with the original prompt to iteratively suggest multi-LLM structures, while representing each individual LLM by a natural language description of their training data and expertise. For \emph{agent-prune}, we randomly sample multi-LLM structures, conduct pruning and re-evaluate, repeat this process for 20 times and report the best-found structure on $f$ across runs and pruning degrees. For \emph{GNNs}, we employ graph attention networks to encode the multi-LLM network with randomly initialized node features, and learn a link prediction task based on whether adding this edge between two LLMs would lead to a performance increase on $f$ or not. At inference-time, the GNN conducts link prediction for $n \times n$ edges to obtain a multi-LLM network.

\paragraph{Prompts} We present the prompts employed in various contexts in Tables \ref{tab:llm_instruction}, \ref{tab:gemini_as_a_judge}, and \ref{tab:gemini_role}. We make dataset-specific changes to the general prompt format if necessary.

\paragraph{Qualitative Examples} We present two working examples of the multi-LLM systems in \ourmethod{}, on Knowledge Crosswords \citep{ding-etal-2024-knowledge} and NLGraph \citep{wang2024can} datasets in Figures \ref{fig:example1} and \ref{fig:example2}. These examples demonstrate that \ourmethod{} discovers multi-LLM systems to adapt to the task and play heterogeneous roles through collaborative generation.

\paragraph{Illustrating the Optimization} We illustrate the changes in the best $f$ utility values for the swarm of graphs and swarm of LLMs in Figure \ref{fig:lines}: it is illustrated that there is co-improvement of roles and weights thanks to the alternating optimization algorithm.

\begin{table*}[h]
\small
\resizebox{1\textwidth}{!}{
\begin{tabularx}{\textwidth}{m{\linewidth}}
% {\textwidth}{l}
\toprule[1pt]
\emph{$>$ prompt for the first/entry-point LLM} \\
``Please answer the following question.'' \\ \\
\emph{$>$ prompt for the middle/intermediate LLMs} \\
``Please answer the following question with the help of previous responses, feel free to ignore wrong or unhelpful responses.'' \\ \\
\emph{$>$ prompt for the final/end-point LLM} \\
``Please answer the following question with the help of previous responses, feel free to ignore wrong or unhelpful responses. Make sure to provide a final and definitive answer.'' \\
\bottomrule[1pt]
\end{tabularx}
}
\vspace{3pt}
\caption{Default LLM instructions in the multi-LLM system.}
\label{tab:llm_instruction}
\end{table*}

\begin{table*}[h]
\small
\resizebox{1\textwidth}{!}{
\begin{tabularx}{\textwidth}{m{\linewidth}}
% {\textwidth}{l}
\toprule[1pt]
``Please evaluate how similar is the following response to the ground truth answer. Please rate the response on a scale of 1 to 10, where 1 is the worst and 10 is the best. Please respond with Rating: ?/10 first and then provide your reason.\\ \\
Ground truth: [ground truth] \\
Generated response: [generated response]'' \\
\bottomrule[1pt]
\end{tabularx}
}
\vspace{3pt}
\caption{Default Gemini-as-a-judge evaluation prompt.}
\label{tab:gemini_as_a_judge}
\end{table*}

\begin{table*}[h]
\small
\resizebox{1\textwidth}{!}{
\begin{tabularx}{\textwidth}{m{\linewidth}}
% {\textwidth}{l}
\toprule[1pt]
``Below is the output of a model solving part of a problem. Please judge whether the output is one of the four scenarios: 1. solving part of the problem, 2. refining the previous answer, 3. providing feedback, 4. irrelevant. \\ \\
The problem is: [problem text] \\
The output is: [intermediate output]'' \\
\bottomrule[1pt]
\end{tabularx}
}
\vspace{3pt}
\caption{Prompt for Gemini to evaluate the roles of intermediate model outputs in multi-LLM systems.}
\label{tab:gemini_role}
\end{table*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/example1.pdf}
    \caption{Working example one of \ourmethod{}.}
    \label{fig:example1}
    \vspace*{-10pt}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/example2.pdf}
    \caption{Working example two of \ourmethod{}.}
    \label{fig:example2}
    \vspace*{-10pt}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/lines.pdf}
    \caption{Illustrating how the utility of the role-step (left) and weight-step (right) changes through time on three datasets, Knowledge Crosswords, NLGraph, and AB-kg. We observe co-improvement in the utility of graphs and model weights.}
    \label{fig:lines}
    \vspace*{-10pt}
\end{figure*}

% prompts, examples, and best role/weight change through time

\end{document}