[
  {
    "index": 0,
    "papers": [
      {
        "key": "duimproving",
        "author": "Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor",
        "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate"
      },
      {
        "key": "feng-etal-2024-dont",
        "author": "Feng, Shangbin  and\nShi, Weijia  and\nWang, Yike  and\nDing, Wenxuan  and\nBalachandran, Vidhisha  and\nTsvetkov, Yulia",
        "title": "Don{'}t Hallucinate, Abstain: Identifying {LLM} Knowledge Gaps via Multi-{LLM} Collaboration"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "fengknowledge",
        "author": "Feng, Shangbin and Shi, Weijia and Bai, Yuyang and Balachandran, Vidhisha and He, Tianxing and Tsvetkov, Yulia",
        "title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models"
      },
      {
        "key": "shen-etal-2024-learning",
        "author": "Shen, Zejiang  and\nLang, Hunter  and\nWang, Bailin  and\nKim, Yoon  and\nSontag, David",
        "title": "Learning to Decode Collaboratively with Multiple Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "feng-etal-2024-dont",
        "author": "Feng, Shangbin  and\nShi, Weijia  and\nWang, Yike  and\nDing, Wenxuan  and\nBalachandran, Vidhisha  and\nTsvetkov, Yulia",
        "title": "Don{'}t Hallucinate, Abstain: Identifying {LLM} Knowledge Gaps via Multi-{LLM} Collaboration"
      },
      {
        "key": "burnsweak",
        "author": "Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others",
        "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wu2024autogen",
        "author": "Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and others",
        "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation"
      },
      {
        "key": "guo2024large",
        "author": "Guo, Taicheng and Chen, Xiuying and Wang, Yaqi and Chang, Ruidi and Pei, Shichao and Chawla, Nitesh V and Wiest, Olaf and Zhang, Xiangliang",
        "title": "Large language model based multi-agents: A survey of progress and challenges"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "liang2023encouraging",
        "author": "Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Shi, Shuming and Tu, Zhaopeng",
        "title": "Encouraging divergent thinking in large language models through multi-agent debate"
      },
      {
        "key": "duimproving",
        "author": "Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor",
        "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "sprague2024cot",
        "author": "Sprague, Zayne and Yin, Fangcong and Rodriguez, Juan Diego and Jiang, Dongwei and Wadhwa, Manya and Singhal, Prasann and Zhao, Xinyu and Ye, Xi and Mahowald, Kyle and Durrett, Greg",
        "title": "To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning"
      },
      {
        "key": "sclarquantifying",
        "author": "Sclar, Melanie and Choi, Yejin and Tsvetkov, Yulia and Suhr, Alane",
        "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yadav2024survey",
        "author": "Yadav, Prateek and Raffel, Colin and Muqeeth, Mohammed and Caccia, Lucas and Liu, Haokun and Chen, Tianlong and Bansal, Mohit and Choshen, Leshem and Sordoni, Alessandro",
        "title": "A survey on model moerging: Recycling and routing among specialized experts for collaborative learning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "li2022branch",
        "author": "Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke",
        "title": "Branch-train-merge: Embarrassingly parallel training of expert language models"
      },
      {
        "key": "gritsch2024nexus",
        "author": "Gritsch, Nikolas and Zhang, Qizhen and Locatelli, Acyr and Hooker, Sara and {\\\"U}st{\\\"u}n, Ahmet",
        "title": "Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "yu2024languagedare",
        "author": "Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin",
        "title": "Language models are super mario: Absorbing abilities from homologous models as a free lunch"
      },
      {
        "key": "yadav2024ties",
        "author": "Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit",
        "title": "Ties-merging: Resolving interference when merging models"
      },
      {
        "key": "jang2024model",
        "author": "Jang, Dong-Hwan and Yun, Sangdoo and Han, Dongyoon",
        "title": "Model Stock: All we need is just a few fine-tuned models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "mavromatis2024pack",
        "author": "Mavromatis, Costas and Karypis, Petros and Karypis, George",
        "title": "Pack of LLMs: Model Fusion at Test-Time via Perplexity Optimization"
      },
      {
        "key": "akiba2024evolutionary",
        "author": "Akiba, Takuya and Shing, Makoto and Tang, Yujin and Sun, Qi and Ha, David",
        "title": "Evolutionary optimization of model merging recipes"
      },
      {
        "key": "huang2023lorahub",
        "author": "Huang, Chengsong and Liu, Qian and Lin, Bill Yuchen and Pang, Tianyu and Du, Chao and Lin, Min",
        "title": "Lorahub: Efficient cross-task generalization via dynamic lora composition"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "feng2024model",
        "author": "Feng, Shangbin and Wang, Zifeng and Wang, Yike and Ebrahimi, Sayna and Palangi, Hamid and Miculicich, Lesly and Kulshrestha, Achin and Rauschmayr, Nathalie and Choi, Yejin and Tsvetkov, Yulia and others",
        "title": "Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "akiba2024evolutionary",
        "author": "Akiba, Takuya and Shing, Makoto and Tang, Yujin and Sun, Qi and Ha, David",
        "title": "Evolutionary optimization of model merging recipes"
      },
      {
        "key": "fernandopromptbreeder",
        "author": "Fernando, Chrisantha and Banarse, Dylan Sunil and Michalewski, Henryk and Osindero, Simon and Rockt{\\\"a}schel, Tim",
        "title": "Promptbreeder: Self-Referential Self-Improvement via Prompt Evolution"
      },
      {
        "key": "guoconnecting",
        "author": "Guo, Qingyan and Wang, Rui and Guo, Junliang and Li, Bei and Song, Kaitao and Tan, Xu and Liu, Guoqing and Bian, Jiang and Yang, Yujiu",
        "title": "Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers"
      }
    ]
  }
]