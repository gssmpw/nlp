\begin{table*}[h]
\begin{center}

  \begin{tabular}{c|cccc|cccc}
    \hline
      & \multicolumn{4}{c|}{\bf Evaluation by GPT} & \multicolumn{4}{c}{\bf Evaluation by Llama} \\
    \bf Generation Model & \bf Comp & \bf Conc & \bf Read & \bf Faith & \bf Comp & \bf Conc & \bf Read & \bf Faith\\
    % \bf Generation Model & \bf Completeness & \bf Conciseness & \bf Readability & \bf Faithfulness & \bf Completeness & \bf Conciseness & \bf Readability & \bf Faithfulness\\
    \hline
    Original & 4.07 & 6.83 & 6.04 & 0.34 & 5.02 & 7.35 & 7.14 & 0.27\\
    \hline
    Header+Sample & 3.50 & 5.39 & 4.36 & 0.80 & 3.56 & 4.0 & 4.15 & 0.65\\
    MVP & 1.52 & 3.24 & 2.41 & 0.71 & 1.57 & 3.25 & 2.84 & 0.48\\
    ReasTAP & 1.96 & 4.38 & 3.33 & 0.60 & 1.91 & 5.81 & 4.85 & 0.50\\
    \hline
    AutoDDG-UFD-GPT & 8.19 & \bf 8.97 & \bf 8.99 & \underline{0.99} & 8.57 & \bf 8.00 & \bf 8.99 & 0.90\\
    AutoDDG-UFD-Llama & 7.72& \underline{8.51} & \underline{8.625} & 0.96 & 8.43 & \underline{7.86} & \underline{8.50} & 0.92\\
    AutoDDG-SFD-GPT & \bf 8.96 & 7.35 & 7.90  & \bf 0.99 & \bf 8.98 & 5.97 & 7.00 & \underline{0.93}\\
    AutoDDG-SFD-Llama & \underline{8.95} & 6.24 & 7.19 & 0.97 & \underline{8.89} & 5.56 & 6.79 & \bf 0.95\\
  \hline
\end{tabular}
\end{center}
\caption{Evaluation of dataset description quality on reference-free metrics (Comp: Completeness, Conc: Conciseness, Read: Readability, Faith: Faithfulness) for all models, including the original description. Completeness, Conciseness, and Readability are scaled from 0 to 10, while Faithfulness is scaled from 0 to 1. Higher values indicate better performance. Bold values represent the highest scores for each metric, while underlined values represent the second highest.}
  \label{tab:eval_reference_free}
\vspace{-.3cm}
\end{table*}

% \begin{table}[h]
% \begin{center}
%   \begin{tabular}{c|cccc}
%     \hline
%     \bf Model & \bf Comp & \bf Conc & \bf Read & \bf Faith \\
%     % \bf Model & \bf Completeness & \bf Conciseness & \bf Readability & \bf Faithfulness \\
%     \hline
%     % Original & 4.07/5.02 & 6.83/7.35 & 6.04/7.14 & 0.34/0.27 \\
%     Original & 4.07 & 6.83 & 6.04 & 0.34 \\
%     \hline
%     Header+Sample & 3.50 & 5.39 & 4.36 & 0.80 \\
%     MVP & 1.52 & 3.24 & 2.41 & 0.71 \\
%     ReasTAP & 1.96 & 4.38 & 3.33 & 0.60 \\
%     \hline
%     AutoDDG-UFD-GPT & 8.19 & \bf 8.97 & \bf 8.99 & \underline{0.99} \\
%     AutoDDG-UFD-Llama & 7.72& \underline{8.51} & \underline{8.63} & 0.96 \\
%     AutoDDG-SFD-GPT & \bf 8.96 & 7.35 & 7.90  & \bf 0.99 \\
%     AutoDDG-SFD-Llama & \underline{8.95} & 6.24 & 7.19 & 0.97 \\
%   \hline
% \end{tabular}
% \end{center}
% \caption{Evaluation of dataset description quality on reference-free metrics (Comp: Completeness, Conc: Conciseness, Read: Readability, Faith: Faithfulness). Completeness, Conciseness, and Readability are scaled from 0 to 10, while Faithfulness is scaled from 0 to 1. Higher values indicate better performance. Bold values represent the highest scores for each metric, while underlined values represent the second highest.}
%   \label{tab:eval_reference_free_gpt}
% \vspace{-.77cm}
% \end{table}