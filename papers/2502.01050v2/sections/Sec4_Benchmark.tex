\section{Evaluation Strategy and New Benchmarks}

% 
Our framework evaluates both the \textit{intrinsic} quality of descriptions and their \textit{extrinsic} impact on keyword-based retrieval. We describe the evaluation strategies as well as the metrics used. 
% 
We also describe new benchmarks which we have tailored from existing resources to support the evaluation of dataset descriptions.


\subsection{Evaluating Description Quality}
\label{sec:solution_evaluation}


We propose three complementary strategies to evaluate description quality: (1) \textit{dataset retrieval evaluation}, which assesses the impact of descriptions on search effectiveness, (2) \textit{reference-based evaluation}, which compares generated descriptions to existing ones using natural language generation metrics, and (3) \textit{reference-free evaluation}, which leverages LLMs to assess descriptions without reference texts. The following subsections detail each approach.

% The final stage of the system focuses on evaluating the quality and effectiveness of the generated descriptions. This evaluation is conducted across three complementary dimensions: 
% % 
% (1) an extrinsic evaluation through dataset retrieval performance, which assesses the impact of generated descriptions on the effectiveness of keyword based dataset search engines. By measuring how well the descriptions improve dataset findability in search results, we evaluate their practical utility in real-world search scenarios; 
% % 
% (2) intrinsic quality evaluation for datasets that already have existing descriptions, where the generated descriptions are compared against the originals. This allows us to assess how closely our automated descriptions match or potentially improve upon the existing ones. Such evaluation is important when considering the enhancement or updating of dataset metadata in existing data portals.
% % 
% and (3) intrinsic quality evaluation for datasets without existing descriptions. This helps us ensure that our method produces high-quality descriptions from scratch, which is essential for datasets being published for the first time.
% % 
% The details of each evaluation method, including the specific metrics used, are introduced in the following.

\myparagraph{Dataset Retrieval Evaluation}
%
Evaluating the impact of generated descriptions on dataset retrieval performance is critical, as keyword-based search remains the primary application for dataset search engines. This evaluation specifically assesses the ability of description generation models to enhance the findability of datasets by improving search relevance and ranking—arguably the most important metric for practical use cases.
% 
To measure retrieval performance, we utilize the Normalized Discounted Cumulative Gain (NDCG@k) metric \cite{jarvelin2002ndcg}, a widely-adopted standard for ranking evaluation. NDCG@k measures the effectiveness of a ranking by comparing the ideal ranking of relevant items to the actual ranking produced by the system, up to position k. It accounts for both the relevance of the datasets and their positions in the result list, assigning higher scores when relevant datasets appear earlier. 
%This makes it an essential metric for evaluating the effectiveness of descriptions in keyword-based search scenarios.

In our evaluation, we integrate the generated descriptions into a keyword-based search engine and perform search queries representative of typical user behavior. By calculating NDCG@k scores for search results with and without the generated descriptions, we can quantify the improvement in dataset findability attributable to our method. This retrieval-based evaluation serves as the cornerstone of our assessment framework, providing a direct measure of the effectiveness of the descriptions in supporting dataset search engines. Unlike intrinsic evaluations, which focus on the quality of the descriptions themselves, this extrinsic evaluation measures their practical impact on search outcomes, highlighting their role in enabling better dataset findability.

\myparagraph{Reference-Based Evaluation}
%
For datasets that already have existing descriptions, a  reference-based evaluation is applied by comparing the generated descriptions with the original ones. This comparison leverages traditional natural language generation (NLG) metrics such as METEOR~\cite{banerjee2005meteor} and ROUGE~\cite{lin2004rouge}. METEOR incorporates synonymy and stemming for a more flexible comparison, and ROUGE focuses on recall by evaluating overlapping units like n-grams and sequences. In addition, we use semantic similarity metrics like BERTScore \cite{zhang2019bertscore}, which employs contextual embeddings from BERT to capture the meaning and context of the text beyond surface-level word matches. These metrics provide a structured, quantitative evaluation of how well the generated description matches the reference in terms of wording, content, and meaning.

By comparing our generated descriptions to existing ones, we assess whether our automated approach produces descriptions of comparable quality. This is particularly important for data portals aiming to enhance or update their metadata, ensuring that automated descriptions meet the standards expected by users.

\myparagraph{Reference-Free Evaluation}
For datasets that do not have reference descriptions, reference-free evaluation becomes essential. In this scenario, we leverage large language models (LLMs) to evaluate the quality of the generated descriptions. Instead of relying on direct comparison with reference texts, LLM-based methods assess attributes such as coherence, relevance, clarity, and coverage of key dataset features~\cite{liu2023geval, gao2024llmEval}. We prompt the LLM to rate the descriptions based on specific criteria. For example: "On a scale from 1 to 10, how well does the description capture the main characteristics of the dataset?" This provides a quantitative assessment of the descriptions' quality in the absence of reference texts.

However, it is important to acknowledge that LLM-based evaluations may introduce biases inherent in the models' training data. The models might favor certain styles or content, especially when evaluating outputs generated by the same model \cite{panickssery2024llm_favor}. To mitigate this issue, we utilize cross-evaluation -- for example, we use Llama to evaluate descriptions generated by GPT (and vice-versa). By employing different models for generation and evaluation, we reduce the likelihood of shared biases influencing the assessment.
% 
This reference-free evaluation approach complements traditional metrics by introducing semantic and contextual assessments, making it valuable for datasets without existing descriptions. It ensures that newly-published datasets have high-quality descriptions that effectively convey their content to potential users, thereby enhancing their findability.

\subsection{New Dataset Retrieval Benchmarks}
\label{sec:dataset_benchmark}

%\subsubsection{\textbf{Challenges in Selecting Suitable Dataset Search Benchmarks}}
\myparagraph{Challenges in Selecting Suitable Dataset Search Benchmarks}

\noindent Identifying suitable evaluation benchmarks for keyword-based dataset search poses significant challenges due to the diverse nature of datasets and application scenarios. Since our goal is to generate descriptions that improve keyword-based dataset search, selecting appropriate benchmarks is critical for evaluating the performance of \SystemName.
% 
We focus on tabular datasets in CSV format because they are widely used for representing structured data. Moreover, to simulate real-world scenarios, we aim to include CSV datasets that feature a large number of rows and columns, presenting a  realistic and challenging evaluation environment.
% 
When selecting table search benchmarks, we consider the following criteria: (1) support for keyword-based queries; (2) inclusion of CSV datasets collected from Open Data Portals; and (3) availability of query relevance data, typically represented as triples of (CSV dataset, keyword query, relevance score).
% 
While there are several table search benchmarks\cite{lin2022acordar1, chen2024acordar2, hulsebos2023gittables, chen2021wtr, zhang2018semsearch, zhang2021semantic, leventidis2024STSD, ji2024target}, only two meet these criteria:
%There are two benchmarks that meet these criteria which we describe below.
%
the ECIR~\cite{chen2020ecir} and the NTCIR~\cite{kato2021ntcir} datasets.
%—referred to as ECIR 
%since it does not have an official name and was introduced at the ECIR conference—and the NTCIR dataset \cite{kato2021ntcir}.

However, both benchmarks require modifications before they can be used for our application scenario. In the ECIR benchmark, some datasets are labeled as relevant to a query, but closer (manual) examination reveals that they are not truly relevant. In the NTCIR benchmark, only 4.27\% of the datasets are in CSV format.
% 
Building upon these two benchmarks, as we discuss below, we construct ECIR-DDG and NTCIR-DDG, tailored specifically for evaluating the effectiveness of description generation in keyword-based dataset search and retrieval.

\begin{table*}
    \centering
    \begin{tabular}{c|c|ccc|ccc|c|ccc|ccc}
        \hline
        \bf Benchmark & \bf Query & \multicolumn{3}{c|}{\bf Rel Tabs/Query} & \multicolumn{3}{c|}{\bf Tabs/Query} & \bf Tabs/Bench & \multicolumn{3}{c|}{\bf Rows/Tab} & \multicolumn{3}{c}{\bf Cols/Tab} \\
        \hline
        & & Avg & Min & Max & Avg & Min & Max & & Avg & Min & Max & Avg & Min & Max \\
        \hline
        \bf ECIR-DDG & 120 & 12 & 8 & 16 & 169 & 76 & 549 & 1015 & 17977 & 4 & 183539 & 22 & 4 & 337 \\
        \bf NTCIR-DDG & 32 & 10 & 5 & 22 & 19 & 10 & 42 & 615 & 78630 & 3 & 2717008 & 14 & 2 & 74\\
        \hline
    \end{tabular}
    \caption{Statistics of the ECIR-DDG and NTCIR-DDG benchmarks. The table provides an overview of the number of queries, the average, minimum, and maximum number of relevant tables per query (Rel Tabs/Query), total tables per query (Tabs/Query), total tables per benchmark (Tabs/Bench), rows per table (Rows/Tab) and columns per table (Cols/Tab). These statistics highlight the diversity and scale of the datasets in the benchmarks.
    }
    \label{tab:benchmark_stats}
    \vspace{-0.77cm}
\end{table*}


\myparagraph{ECIR-DDG Benchmark}
The original ECIR benchmark focuses on tabular datasets from U.S. federal data sources. The queries are designed to reflect real-world information needs, with six distinct tasks covering various domains such as public health, economic data, and environmental statistics. Each query was manually created, and relevance judgments were obtained through crowdsourced annotations.
% 
We examined the original ECIR benchmark before constructing the new benchmark. For each query $q$, we first randomly sampled datasets $D_{rel}$ that are labeled as relevant and datasets $D_{irrel}$ that are labeled as irrelevant to $q$. 
% 
We then manually reviewed dataset snippets—including the title, description, column names, and a sample of rows—to determine how many datasets in $D_{rel}$ were truly relevant to the query $q$ (true positives) and how many in $D_{irrel}$ were truly irrelevant to $q$ (true negatives). 
% 
Our manual review revealed a true positive rate of 9.87\%, with 23 out of 233 dataset-query pairs ($D_{rel}, q$) identified as valid. The true negative rate was 98.5\%, with 65 out of 66 dataset-query pairs ($D_{irrel}, q$) confirmed as irrelevant.


Based on these results, we construct the new ECIR-DDG benchmark using the following steps. For datasets labeled as irrelevant to each query, we retain them as irrelevant candidates without modification. For relevant candidates, we issue the query through search engines such as Google Dataset Search and collect the top-ranked datasets. The collected datasets, along with their original titles and descriptions, are treated as relevant for the benchmark.
% 
The constructed ECIR-DDG benchmark includes 120 queries\footnote{The original ECIR benchmark consists of 6 tasks, each with 20 queries.} with an average of 12 relevant datasets per query and 169 total datasets per query (Table~\ref{tab:benchmark_stats}). The datasets vary in size, with average 17,977 rows and 22 columns. These statistics demonstrate the diversity and scale of the datasets in the benchmark.


\myparagraph{NTCIR-DDG Benchmark}
The original NTCIR benchmark is derived from Data.gov (US) and e-Stat (Japan), consisting of datasets across various domains such as climate, population, economy, and transportation. Queries in this benchmark were constructed from real user questions found in community Q\&A forums and refined through crowdsourcing. The queries often include geographical, temporal, and numerical terms, reflecting common information needs in dataset search.
% 
% The queries often include geographical, temporal, and numerical terms, reflecting common information needs in dataset search. Additionally, the dataset retrieval task proved challenging due to high topic variability and the presence of location, time, and numerical expressions, which require specialized retrieval techniques.
%
% The NTCIR-DDG benchmark is constructed based on the NTCIR dataset, which originally consists of diverse datasets in various formats. 
% 
Focusing on English topics, we filtered the dataset to include only CSV files, which account for 4.27\% of the original NTCIR collection. 
% 
To ensure sufficient data for meaningful evaluation, we selected queries with at least 5 relevant datasets and at least 10 total datasets (relevant and irrelevant combined). This filtering process resulted in a benchmark with 32 queries (Table~\ref{tab:benchmark_stats}) that meet these criteria, providing a targeted evaluation framework for keyword-based retrieval of CSV datasets.



