\vspace{-0.15cm}
\section{Introduction}
\label{sec:intro}

We have witnessed a proliferation of data portals and data lakes~\cite{gregory2022human, figshare, zenodo, dataverse, hendler2012datagov, kassen2013opendata_chicago, NYC_opendata,nargesian2019data} as more data is generated and made available. It is estimated that there are tens of millions of datasets on the web~\cite{google-dataset-analysis}.
While these datasets present significant opportunities for data-driven insights, they must be easily findable and interpretable to maximize their utility~\cite{wilkinson2016fair}.

Many dataset search systems and infrastructure that powers data portals~\cite{brickley2019google, CKAN, socrata} follow the paradigm of web search engines: they rely on metadata—such as dataset names and descriptions--to build an inverted index for keyword-based queries. As a result, findability is directly influenced by the quality of dataset descriptions, particularly how well they convey dataset contents and align with users’ information needs.
%
For this reason, Google Dataset Search excludes from their index datasets discovered by the crawler which lack a description~\cite{benjelloun2020google}. 
%
Open platforms like CKAN do not mandate descriptions but encourage them to improve search precision and recall, facilitate dataset understanding, and help users determine relevance~\cite{CKAN_description}.
% 
However, many datasets are published with no descriptions or minimal ones. 
As a data point, consider the index of the Auctus dataset search engine \cite{castelo2021auctus}: 3,121 out of 23,520 datasets (13.2\%) have no descriptions, while 2,346 datasets (approximately 10\%) have descriptions with 10 words or fewer. 
% 
\autoref{fig:example_original_generate_description} illustrates issues with dataset descriptions in NYC Open Data~\cite{NYC_opendata}. Descriptions (a) and (b) fail to convey key characteristics of the dataset, such as their attributes, spatial coverage, or temporal extent.
% 
% for example, their attributes or the their spatial and temporal extent. 
% 
In some cases, descriptions are inconsistent with the actual contents; for example,  description (c) implies the dataset includes taxi trips from 2022, whereas the actual data contains records that span multiple years.

Datasets published without detailed and accurate descriptions may be technically available but are effectively invisible. \citet{koesten2017trials} compare the problem of finding data today to the early days of the web, when people needed to know the URL of web pages or use manually created directories such as DMOZ to access content.
%
Beyond findability, descriptions are also crucial for assessing dataset relevance. Just as web search users rely on snippets to decide which results to explore, dataset search users depend on descriptions to filter relevant datasets. This issue is even more pronounced for datasets, where downloading and inspecting large files is far costlier than simply opening a webpage.


\begin{figure*}[ht]
  \includegraphics[width=0.99\linewidth]{figures/description_example.pdf}
  \vspace{-0.3cm}
  \caption[]{Examples of datasets from NYC Open Data with inadequate ((a) \textit{Health Insurance}\footnotemark and (b) \textit{Citi Bike}\footnotemark) and inconsistent descriptions  ((c) \textit{Yellow Taxi}\footnotemark) information. The \textit{2022 Yellow Taxi Trip Data} dataset includes taxi trips that spans multiple years, not just 2022. Descriptions automatically generated by \SystemName for dataset (a) are shown on the right. The complete SFD is given in\subm{ the extended version of this paper \cite{AutoDDG_arxiv}}\arxiv{the Appendix \Cref{tab:example_sfd}}.
  }
  \label{fig:example_original_generate_description}
  \vspace{-0.3cm}
\end{figure*}
\footnotetext[1]{The \textit{Health Insurance} dataset:\url{https://data.ny.gov/Economic-Development/Health-Insurance-Premiums-on-Policies-Written-in-N/xek8-zfrt/about_data}}
\footnotetext[2]{The \textit{Citi Bike} dataset: \url{https://data.cityofnewyork.us/NYC-BigApps/Citi-Bike-System-Data/vsnr-94wk/about_data}}
\footnotetext[3]{The \textit{Yellow Taxi} dataset: \url{https://data.cityofnewyork.us/Transportation/2022-Yellow-Taxi-Trip-Data/qp3b-zxtp/about_data}}


\vspace{.5em}
\myparagraph{Desiderata for Dataset Descriptions}
%
Studies on information-seeking behavior and data discovery have revealed a gap between datasets that are available and those that users can effectively find~\cite{chapman2020dataset}. These studies have also highlighted shortcomings in existing dataset metadata that hinder findability and relevance assessment~\cite{papenmeier2021genuine, koesten2017trials, Sostek2024Discovering}. Building on these findings, which we summarize in \Cref{sec:related_work}, we propose a desiderata for dataset descriptions:

\noindent \emph{Comprehensiveness and Alignment with Users' Information Needs:}
  Widely-used search systems~\cite{CKAN,socrata,brickley2019google} often do not account for dataset contents, limiting users' ability to locate relevant data. Therefore, dataset descriptions should provide as much detail as possible about the contents, ensuring that they address users' information needs.

\noindent \emph{Faithfulness to Data:} Descriptions must be accurate representations of the dataset contents.

\noindent \emph{Conciseness and Readability:}
 Dataset descriptions serve a critical role in helping users assess relevance. They should be concise enough to facilitate quick evaluation yet detailed enough to convey meaningful insights. 

\noindent \emph{Uniformity:} While  variability is expected in heterogeneous dataset collections, it is important to maintain a certain uniformity in the descriptions to facilitate comparison and relevance assessment.

There is an inherent tradeoff between conciseness and comprehensiveness: overly brief descriptions may fail to capture key aspects of the dataset, while excessively detailed ones can be difficult and time-consuming for users to process. Furthermore, while readability enhances users’ ability to assess relevance, it is less critical for indexing and automated findability. Striking the right balance among these factors is essential for designing effective dataset descriptions.
% 
However, crafting high-quality descriptions manually is both difficult and time-consuming. 
% 
This raises a critical question: \textit{can dataset descriptions be generated automatically?} The ability to do so holds significant potential for improving dataset findability and enabling users to better navigate the vast amounts of data currently available.

\myparagraph{LLMs for Automatic Description Generation}
%
Large language models (LLMs) offer a promising avenue for generating dataset descriptions due to their ability to produce fluent, readable text and leverage broad world knowledge~\cite{brown2020gpt,touvron2023llama}. These capabilities enable LLMs not only to generate coherent descriptions but also to enrich them with inferred semantic and  contextual information. Prior work has demonstrated their effectiveness in semantic inference tasks for tabular data, including the ability to determine the semantic types of attributes and the table class~\cite{archetype@vldb2024,chorus@vldb2024}. By connecting external knowledge that is not explicitly encoded in the data, LLMs can enhance dataset descriptions in ways that manual approaches might overlook.

However, using LLMs for this task presents several challenges. LLMs are designed for processing and generating textual data, whereas datasets are structured in tabular form. This mismatch raises questions about how best to represent tables in prompts—a challenge that remains an active area of research in table representation learning~\cite{sui2024table}. Moreover, LLMs operate within fixed context windows, limiting their ability to ingest and process large datasets in their entirety. As a result, we must rely on data samples, which introduces the risk of deriving incomplete or misleading descriptions, as a sample may fail to capture crucial global properties such as the dataset’s spatial and temporal coverage.

Another key challenge is ensuring that generated descriptions align with the desiderata outlined earlier. Careful prompt design is essential to guide LLMs toward producing descriptions that are comprehensive, faithful to the data, and aligned with user information needs. Furthermore, LLMs are known to generate hallucinated or inaccurate content, making it critical to implement safeguards that ensure the descriptions remain grounded in the actual dataset. 
%Strategies such as retrieval-augmented generation (RAG), structured prompt engineering, and post-generation validation can help mitigate these risks.


\begin{figure*}
  \centering
  \includegraphics[width=0.95\linewidth]{figures/solution_overview.png}
  \vspace{-0.3cm}
  \caption{
  \textit{\SystemName: a multi-stage framework for tabular dataset description generation.}
    %
    The workflow begins with the \textit{Context Preparation} stage, where the system processes the dataset to obtain dataset profile and semantic profile.
    %
    In the \textit{Generation} stage, the profiles are sent the \textit{Description Generation Engine} powered by an LLM, which produces descriptions based on the processed input.
    %
    The \textit{Quality Evaluation} stage involves both \textit{Reference-based} (if reference descriptions are available) and \textit{Reference-free Evaluation}, enabling comprehensive quality assessment.
    %
    Finally, the descriptions are utilized in a \textit{Dataset Search Engine}, with \textit{User-Focused Descriptions (UFD)} designed for front-end readability and \textit{Search-Focused Descriptions (SFD)} optimized for back-end findability.  
  }
  \label{fig:solution_overview}
  \vspace{-0.3cm}
\end{figure*}

\myparagraph{Our Approach: \SystemName}
%
We introduce \SystemName, an end-to-end system for automated tabular dataset description generation.
\Cref{fig:solution_overview} outlines its key components.  Given a tabular dataset, \SystemName constructs a structured summary by combining two complementary profiles: a data-driven profile and a semantic profile (\Cref{sec:solution_STA}). The data-driven profile is generated using traditional data profiling techniques, capturing essential dataset characteristics such as attribute types, statistical summaries (e.g., value ranges), and distributions. The semantic profile, on the other hand, is derived with the assistance of an LLM and enriches the summary with contextual information, such as the dataset's topic.
%
These profiles serve as input to the Description Generation Engine, which employs an LLM to generate textual descriptions (\Cref{sec:solution_UFD_SFD}). The generated descriptions are then evaluated (\Cref{sec:solution_evaluation}) before being indexed by the dataset search engine.

By leveraging the data-driven profile, \SystemName addresses two important challenges: 1) it ensures that the prompts reflect a global view of the data without relying on sampling, which may miss critical aspects; 2) it enhances the faithfulness of descriptions by grounding them in concrete dataset characteristics, reducing the risk of incomplete or misleading information.

The  LLM-derived semantic profile further augments the descriptions with contextual information that may not be explicitly present in the data. As demonstrated in our experiments, this additional information improves dataset findability by enriching metadata with meaningful keywords and topical cues.

The profiling step plays a crucial role in extracting relevant metadata from both the dataset and the LLM. Therefore, the profilers should be designed to ensure alignment with the users' information needs.  
%Additionally, automated description generation enables greater uniformity across datasets, 
By using profilers, we also address a common challenge in dataset search engines: the inconsistency in the representation of metadata and descriptions for datasets from different providers and domains. Users "find it frustrating when metadata varies unpredictably across search results"~\cite{Sostek2024Discovering}. By enforcing a consistent format, \SystemName enhances the usability of dataset search tools.

As discussed above, balancing conciseness and comprehensiveness is a fundamental challenge in dataset description generation. To address this, we propose two types of description: User-Focused Descriptions (UFD),
optimized for readability and clarity, provide users with a succinct yet informative overview of the dataset’s content; and Search-Focused Descriptions (SFD), which incorporate dataset overviews, themes, and keyword-rich snippets to enhance search engine indexing (Section~\ref{sec:solution_UFD_SFD}). As \autoref{fig:example_original_generate_description} shows,
UFDs offer concise, user-friendly summaries, while SFDs include additional metadata to improve discoverability.
These two description types can be used in tandem and we evaluate to improve dataset search.
%their effectiveness in Section~\ref{sec:experiment}.

While it is possible to generate descriptions automatically, a crucial challenge is determining their quality.
%
We propose a multi-pronged evaluation strategy that measures: 1)  the improvement in dataset
retrieval within a dataset search engine, 2) the similarity between the generated
descriptions and existing ones (when available) to evaluate completeness and consistency, and (3) intrinsic quality metrics, using LLMs as a judge~\cite{liu2023geval, gao2024llmEval, seo2024unveiling, zhao2023investigating} to evaluate readability, faithfulness to the data,
and conciseness (Section~\ref{sec:solution_evaluation}). 
% 
To conduct an evaluation, we need gold data that includes the relevance of a dataset for a given query. 
% 
%While existing benchmarks provide partial coverage, they are not fully suitable for our evaluation needs. Therefore, 
% 
% A robust evaluation requires gold-standard data that includes dataset relevance judgments for specific queries. 
% 
Since existing benchmarks only partially meet our needs, we introduce two new benchmarks specifically designed to support this evaluation (Section~\ref{sec:dataset_benchmark}).
%
In Section~\ref{sec:experiment}, we present a detailed experimental evaluation which shows that our approach is generates high-quality descriptions that lead to significant improvement in dataset retrieval. The results reinforce our design decisions and demonstrate the effectiveness of the different components of \SystemName.   

\myparagraph{Summary of Contributions}
%
To the best of our knowledge, this paper introduces the first comprehensive framework for systematically and automatically generating descriptions for tabular datasets, aiming to improve keyword-based dataset search. Our approach has the potential to significantly improve dataset findability by assisting dataset authors (creators) as well as managers of repositories and portals, in the creation of comprehensive and accurate descriptions.
%
Our main contributions are as follows:

% \noindent\textit{\textbf{Identify the Problem:}} We provide a definition of the Dataset Description Generation (DDG) problem for tabular datasets.
\noindent\textit{\textbf{Problem Formulation}} We identify the Dataset Description Generation (DDG) problem for tabular datasets, outline the challenges involved, and the desiderata for effective descriptions.

\noindent\textit{\textbf{Methods and Framework for Description Generation:}} We introduce \SystemName, a framework that combines a data-driven approach and LLMs to generate informative descriptions for tabular data.

% generates both \textit{User-Focused Descriptions (UFD)} for readability and \textit{Search-Focused Descriptions (SFD)} for enhancing discoverability.

\noindent\textit{\textbf{Evaluation Strategies:}} We propose a set of strategies to evaluate different quality aspects of descriptions, including  \textit{dataset retrieval evaluation}, \textit{reference-based evaluation} and \textit{reference-free evaluation}, to assess the quality of descriptions in the absence of reference descriptions.

\noindent\textit{\textbf{Dataset Search Benchmarks:}} We construct two new benchmarks, ECIR-DDG and NTCIR-DDG, tailored to evaluate the quality of dataset descriptions for keyword-based search and retrieval tasks.

\noindent\textit{\textbf{Experimental Evaluation:}} We report the results of an extensive experimental evaluation that validates our design decisions and demonstrates the effectiveness
of \SystemName at generating descriptions that improve dataset retrieval and that can assist users understand and assess dataset relevance.   





