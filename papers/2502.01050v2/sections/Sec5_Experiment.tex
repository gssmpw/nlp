\vspace{-.3cm}
\section{Experiment Evaluation}
\label{sec:experiment}

\subsection{Experiment Overview}
To evaluate our dataset description generation framework, we conduct experiments following the evaluation strategy introduced in Section~\ref{sec:solution_evaluation} using multiple baselines and description models.

%\subsubsection{\textbf{Models for Dataset Descriptions}}
\myparagraph{Models for Dataset Descriptions}
% 
We consider the following models for constructing descriptions:\\
% 
(1) \textbf{Original}: the dataset descriptions provided with the datasets. %\\ %, used as a baseline for comparison.
% 
(2) \textbf{Header+Sample}: a simple method that concatenates column headers with sample values from each column.

\noindent Additionally, we evaluate two pre-trained language models by providing them the Header and Sample: \\
% 
(3) \textbf{MVP}: the Multi-task Supervised Pre-training for Natural Language Generation (MVP) model~\cite{tang2023mvp} leverages multi-task training for improved text generation, including data-to-text generation.\\
% 
(4) \textbf{ReasTAP}: a table pre-training model that incorporates reasoning skills to enhance table comprehension, including table-to-text generation \cite{zhao2022reastap}.

\noindent Finally, we evaluate our proposed AutoDDG framework, which applies large language models (LLMs) to generate dataset descriptions automatically. We experiment with two variations of AutoDDG, and ablate them using different LLMs (GPT and Llama):\\
% 
(5) \textbf{\SystemName-UFD-GPT/Llama}: generates user-focused descriptions (UFDs) optimized for readability and clarity while maintaining relevance for search.\\
% 
(6) \textbf{\SystemName-SFD-GPT/Llama}: generates search-focused descriptions (SFDs) designed to improve dataset retrieval in keyword-based search engines.

%\subsubsection{\textbf{Evaluation Metrics}}
\myparagraph{Evaluation Metrics}
% 
We assess the generated descriptions using following metrics:
% 
(1) \textbf{Retrieval Metrics}: We measure retrieval effectiveness using NDCG scores from BM25 \cite{robertson2009bm25} and SPLADE \cite{formal2021splade}. BM25 evaluates keyword-based lexical matching, while SPLADE captures semantic relevance by expanding terms based on contextual meaning.
% 
(2) \textbf{Reference-Based Metrics}: We assess alignment with ground-truth descriptions using METEOR, ROUGE, and BERTScore, which capture phrase similarity, lexical overlap, and semantic consistency.
% 
(3) \textbf{Reference-Free Metrics}: We evaluate the descriptions' intrinsic quality using LLM-based scoring for Completeness, Conciseness, Readability, and Faithfulness. These metrics assess how well the descriptions convey relevant information while maintaining clarity and accuracy.
% 
Further details on these evaluation are provided in the corresponding sections.

\begin{figure}[b]
  \centering
  \includegraphics[width=0.90\linewidth]{figures/radar_chart.png}
  \vspace{-0.3cm}
  \caption{Radar chart comparing the performance of various dataset description methods across evaluation metrics. AutoDDG-UFD excels in Conciseness and Readability, while AutoDDG-SFD leads in Lexical Matching and Completeness. AutoDDG-UFD-SFD combines these strengths, outperforming baselines (Original, Header+Sample, MVP, ReasTAP). Metrics are scaled from 0 to 1.}
  \label{fig:summary_radar}
  \vspace{-0.3cm}
\end{figure}

%\subsubsection{\textbf{Overview of Experimental Results}}
\myparagraph{Overview of Experimental Results}
%We take an overview of our dataset description generation methods through experiments that highlight their strengths across multiple metrics, including Lexical Matching (BM25), Semantic Matching (SPLADE), Completeness, Faithfulness, Readability, and Conciseness.
% 
The radar chart in Figure~\ref{fig:summary_radar} provides a comparative overview, illustrating how different methods perform across these dimensions.
% 
The results show that UFD excels in readability and conciseness, making it more suitable for user-facing applications, while SFD improves completeness and retrieval relevance, increasing dataset findability. By combining these strengths, AutoDDG achieves a balanced and superior performance compared to baseline methods (Original, Header+Sample, MVP, and ReasTAP), demonstrating its effectiveness in generating high-quality descriptions for both human interpretation and machine-based search.
%


\vspace{-.2cm}
\subsection{Retrieval Performance}
%of Dataset Descriptions}

To assess the effectiveness of dataset descriptions in search tasks, we evaluate retrieval performance using \textbf{BM25}~\cite{robertson2009bm25} and \textbf{SPLADE}~\cite{formal2021splade}. BM25 focuses on lexical matching, ranking documents based on term frequency and inverse document frequency, while SPLADE learns sparse text representations by expanding query terms with semantically related words. For example, it may expand `lunar' to include `moon,' capturing broader context.


% This section delves into the evaluation of our dataset description generation methods by examining their performance in retrieval tasks.
% 
% To evaluate retrieval performance, we use BM25 \cite{robertson2009bm25} and SPLADE \cite{formal2021splade} methods.
% 
% BM25 is a well-established probabilistic ranking method that measures the relevance of documents based on term frequency and inverse document frequency while incorporating document length normalization. BM25 is particularly suited for keyword-based search tasks, making it an ideal choice for ranking datasets based on their generated descriptions.
% 
% SPLADE is a novel approach for learning sparse text representations. A key feature of SPLADE is term expansion, which allows the model to generate weights for tokens absent in the document but semantically related to its content. For example, the model might expand "lunar" to include "moon", thereby capturing the broader context of the document. This capability, combined with its strong performance and ease of integration with systems like Elasticsearch, makes SPLADE an excellent choice for handling large-scale text data.

\begin{table*}[h!]
    \centering
    \begin{tabular}{l|cccc|cccc}
        \hline
         & \multicolumn{4}{c|}{\bf ECIR-DDG} & \multicolumn{4}{c}{\bf NTCIR-DDG} \\
         \bf Model & \bf NDCG@5 & \bf NDCG@10 & \bf NDCG@15 & \bf NDCG@20 & \bf NDCG@5 & \bf NDCG@10 & \bf NDCG@15 & \bf NDCG@20 \\
        \hline
         Original & 0.666 & 0.658 & 0.668 & 0.694 & 0.439 & 0.575 & 0.662 & 0.688\\
         \hline
         Header+Sample & 0.589 & 0.573 & 0.575 & 0.593 & 0.323 & 0.451 & 0.553 & 0.590\\
         MVP & 0.557 & 0.538 & 0.533 & 0.543 & 0.356 & 0.472 & 0.575 & 0.617\\
         ReasTAP & 0.638 & 0.598 & 0.599 & 0.609 & 0.406 & 0.485 & 0.583 & 0.627\\
         \hline
         AutoDDG-UFD-GPT & \underline{0.803} & \underline{0.807} & \underline{0.816} & \underline{0.836} & 0.415 & 0.529 & 0.624 & 0.670\\
         AutoDDG-UFD-Llama & 0.739 & 0.741 & 0.766 & 0.788 & 0.425 & 0.533 & 0.631 & 0.669 \\
         AutoDDG-SFD-GPT & \bf 0.846 & \bf 0.849 & \bf 0.857 & \bf 0.877 & \bf 0.512 & \bf 0.613 & \bf 0.688 & \bf 0.725 \\
         AutoDDG-SFD-Llama & 0.763 & 0.764 & 0.784 & 0.809 & \underline{0.437} & \underline{0.567} & \underline{0.651} & \underline{0.689}\\
      \hline
    \end{tabular}
    \caption{BM25. Comparison of NDCG scores across different description generation models on the ECIR-DDG and NTCIR-DDG benchmarks. Higher NDCG scores indicate better alignment between the generated descriptions and the relevance of retrieved results for keyword-based search. Bold values represent the highest scores for each metric, while underlined values indicate the second highest.
    }
    \label{tab:ndcg_bm25}
    \vspace{-0.77cm}
\end{table*}

% We conduct experiments on \textbf{ECIR-DDG} and \textbf{NTCIR-DDG} benchmarks, comparing retrieval effectiveness across baselines and AutoDDG-generated descriptions. The dataset descriptions are indexed using BM25 and SPLADE, and retrieval effectiveness is measured using \textbf{NDCG scores}.


We experimented with both ECIR-DDG and NTCIR-DDG benchmarks. For the LLM models, we select cost-effective versions, GPT-4o-mini\footnote{\url{https://platform.openai.com/docs/models}} and LLaMA-3.1-8B-Instruct\footnote{\url{https://deepinfra.com/meta-llama/Meta-Llama-3.1-8B-Instruct}}, for our experiments. 
% 
To evaluate retrieval performance, we created indices for both BM25 and SPLADE using the dataset descriptions generated by the models.
% 
For BM25, we created an inverted index, where terms from each dataset description were tokenized and stored along with their term frequencies and document frequencies.\footnote{We used Okapi BM25 from this library: \url{https://github.com/dorianbrown/rank_bm25}} During Evaluation, keyword-based queries were issued against this index, and BM25 ranked the dataset descriptions by calculating their relevance scores.
% 
For SPLADE, we created a sparse vector representation of each dataset description.\footnote{We used SPLADE with FastEmbed: \url{https://qdrant.github.io/fastembed/examples/SPLADE_with_FastEmbed/}} Each term in the description was assigned a weight based on its semantic importance, including terms absent from the text but inferred from the context (e.g., expanding "lunar" to "moon"). During evaluation, keyword-based queries were processed, and we ranked the dataset descriptions by calculating cosine similarity between the query's sparse representation and each document's sparse embedding.
% 

\myparagraph{BM25 Results}
% 
%In the BM25 evaluation 
The results in Table~\ref{tab:ndcg_bm25} show that \SystemName-SFD methods consistently outperforms UFD across both ECIR-DDG and NTCIR-DDG benchmarks, confirming that the effectiveness of SFDs.  By tailoring descriptions to maximize keyword matching with query topics, SFDs improve search relevance.
% 
GPT leads to higher NDCG values than Llama on the ECIR-DDG benchmark, and Llama shows better performance on the NTCIR-DDG benchmark. This suggests that different LLMs exhibit varying strengths depending on the characteristics of the dataset and query topics.
% 
%This result aligns with the design goal of SFD, which optimizes for search relevance by tailoring descriptions to maximize keyword matching with query topics. 

The performance of UFD is competitive with SFD 
%especially with Llama,  %JF: UFD seems to be better with GPT...
but, as expected, the focus on user readability limits its effectiveness in purely search-oriented metrics. 
% 
Traditional baselines such as MVP and ReasTAP perform significantly worse than both UFD and SFD, highlighting their limitations in generating effective descriptions for retrieval tasks. Interestingly, original descriptions sometimes outperform these baselines, underscoring the importance of domain-specific context and the limitations of generic data-to-text approaches in keyword-based retrieval.



\begin{figure}
	\centering
	\begin{subfigure}{0.49\columnwidth} 
		\centering
		\includegraphics[width=1.0\linewidth]{figures/retrieval_summary_ecir.png}
		\caption{ECIR-DDG Benchmark}
        \label{fig:ablation_noSTA}
	\end{subfigure}
        \begin{subfigure}{0.49\columnwidth} 
		\centering
		\includegraphics[width=1.0\linewidth]{figures/retrieval_summary_ntcir.png}
		\caption{NTCIR-DDG Benchmark}
        \label{fig:ablation_ufd}
	\end{subfigure}
    \vspace{-1.0em}
	\caption{Summary plot of NDCG@20 scores for Original, Original+SPLADE, AutoDDG, and AutoDDG+SPLADE on ECIR-DDG and NTCIR-DDG benchmarks. This visualization highlights key improvements from AutoDDG and its combination with SPLADE.}
	\label{fig:summary_retrieval}
    \vspace{-.4cm}
\end{figure}


\myparagraph{SPLADE Results}
Our evaluation using SPLADE shows that semantic term expansion significantly enhances retrieval quality across all models. However, the adaptability of SFD to diverse query topics continues to set it apart as the best-performing method for retrieval tasks, particularly in benchmarks with more complex query characteristics.
% 
\autoref{fig:summary_retrieval} 
%summarizes the results and provides 
shows a comparison of the retrieval performance using the Original description and \SystemName, with BM25 and SPLADE. %further illustrates the impact of retrieval enhancements by summarizing key results from BM25 and SPLADE evaluations, focusing on the most critical methods: Original, Original+SPLADE, AutoDDG, and AutoDDG+SPLADE. 
% 
%The plot provides a concise visualization of how AutoDDG improves retrieval performance. 
The results consistently show that AutoDDG outperforms Original descriptions, with additional gains achieved when combined with SPLADE. This trend underscores the robustness of AutoDDG, even when combined with SPLADE, in enhancing retrieval effectiveness.
% 
\subm{A complete table of NDCG scores with SPADE is available in our extended version of this paper \cite{AutoDDG_arxiv}.}
% 
\arxiv{A complete table of NDCG scores with SPADE is available in the Appendix \Cref{tab:experiment_splade}.}
% 
% The SPLADE evaluation results, presented in Table~\ref{tab:ndcg_splade}, show a different trend compared to BM25 due to SPLADE's ability to perform semantic term expansion. This feature allows SPLADE to assign weights to tokens that are not explicitly present in the original text but are semantically related, thereby narrowing the performance gap between the original descriptions and the generated descriptions.
% 
% For the SPLADE evaluation, SPLADE's term expansion capability boosts the retrieval performance for all models.
% % 
% For the ECIR-DDG benchmark, SPLADE's term expansion capability also reduces the performance difference between UFD and SFD. By capturing additional context, SPLADE makes UFD descriptions more competitive with SFD, resulting in closely matched NDCG scores. However, for the NTCIR-DDG benchmark, SFD maintains a clear advantage over UFD. This may be attributed to the higher diversity and broader topic range of queries in NTCIR-DDG, where the search-focused optimization of SFD remains critical for achieving better performance.
% % 
% Overall, the SPLADE results demonstrate that semantic term expansion significantly enhances retrieval quality across all models. However, the adaptability of SFD to diverse query topics continues to set it apart as the best-performing method for retrieval tasks, particularly in benchmarks with more complex query characteristics.


\vspace{-.2cm}
\subsection{Evaluation of Dataset Description Quality}


In this section, we evaluate the quality of the generated dataset descriptions using both reference-based and reference-free methods (Section~\ref{sec:solution_evaluation}). These evaluation strategies offer insights into how well the descriptions meet human readability standards and perform in automated search and retrieval tasks. This is particularly useful for users looking to publish new datasets, as these evaluations can predict the performance of the generated descriptions both for front-end user interaction and back-end dataset retrieval. We apply following evaluation metrics:

\noindent \textbf{METEOR}: The METEOR score~\cite{banerjee2005meteor} combines precision, recall, synonymy, and stemming to measure the similarity between the generated and reference descriptions. 

\noindent \textbf{ROUGE}: The ROUGE score~\cite{lin2004rouge} focuses on recall by measuring the overlap of n-grams, longest common subsequences, and skip-bigrams between the generated and reference texts.


\noindent \textbf{BERTScore}: BERTScore \cite{zhang2019bertscore} utilizes pre-trained language models, such as BERT, to measure the similarity of embeddings between the generated text and the reference text. Unlike exact match metrics, BERTScore evaluates semantic similarity by comparing the contextualized representations of the words in both texts, providing a more meaningful assessment of whether the generated description conveys the same information as the reference.

\noindent \textbf{LLM-Based Evaluations}: LLM-based evaluations leverage large language models (LLMs) to assess the quality of generated descriptions, especially when reference descriptions are unavailable. Instead of direct comparison with reference texts, LLMs evaluate key attributes such as coherence, relevance, clarity, and coverage of dataset features \cite{liu2023geval, gao2024llmEval}. 

We follow the prompt design from G-Eval \cite{liu2023geval}, incorporating task introduction, evaluation criteria, and evaluation steps. Additionally, we include example evaluations to guide the LLM in rating dataset descriptions. 
% 
We focus on evaluation criteria \textit{\textbf{completeness}}, \textit{\textbf{conciseness}}, and \textit{\textbf{readability}}. They are chosen because they effectively assess the quality of dataset descriptions and predict their performance in different contexts. For instance, compared to search-focused descriptions (SFD), user-focused descriptions (UFD) are expected to score higher in conciseness and readability, but lower in completeness.
% 
In addition, we consider \textit{\textbf{faithfulness}} evaluation to measure the extent to which the generated descriptions accurately reflect the underlying dataset content including dataset sample, dataset profile and semantic profile. We follow the prompt design in previous works \cite{seo2024unveiling, zhao2023investigating} to assess the truthfulness of dataset descriptions.
% 
\subm{The complete prompts for LLM-based evaluation can be found in our extended version of this paper \cite{AutoDDG_arxiv}.}
% 
\arxiv{The complete prompts for LLM-based evaluation can be found in Appendix \Cref{tab:llm_eval_comp_conc_read} and \Cref{tab:faithfulness_evaluation}.}
% 
% \subm{our extended version of this paper \cite{AutoDDG_arxiv}).}
% %
% \arxiv{the \Cref{app:example_sfd}) and \Cref{app:template_sfd}.}
% 
This approach provides a quantitative measure of the description’s quality in the absence of references. However, it is important to account for potential biases inherent in LLMs, as they may favor certain styles or content \cite{panickssery2024llm_favor}. To mitigate these biases, we employ cross-evaluation techniques using models with higher intelligence, GPT-4o and LLaMA-3.1-70B-Instruct, to evaluate the generated descriptions and report the average score from these two models.


\begin{table}
\begin{center}
  \begin{tabular}{c|ccc}
    \hline
     % & \multicolumn{3}{c|}{\bf Reference-based} \\
    \bf Model & \bf METEOR & \bf ROUGE & \bf BERT Score \\
    \hline
    Original & - & - & - \\
    \hline
    Header+Sample & 2.28 & 3.61 & 75.17\\
    MVP & 1.55 & 1.83 & 78.85 \\
    ReasTAP & 6.48 & 8.82 & 79.99 \\
    \hline
    AutoDDG-UFD-GPT & \underline{15.48} & 29.78 & \bf 82.24 \\
    AutoDDG-UFD-Llama & \bf 16.46 & 30.08 & \underline{82.26} \\
    AutoDDG-SFD-GPT & 13.00 & \bf 34.50 & 80.07 \\
    AutoDDG-SFD-Llama & 12.27 & \underline{33.85} & 79.17 \\
  \hline
\end{tabular}
\end{center}
\caption{Evaluation of dataset description quality on reference-based metrics. Higher values indicate better performance. Bold represents the highest scores for each metric, while underlined values represent the second highest.}
  \label{tab:eval_reference_based}
\vspace{-.77cm}
\end{table}



\begin{table}
\begin{center}
  \begin{tabular}{c|cccc}
    \hline
    \bf & \bf Comp & \bf Conc & \bf Read& \bf Faith \\
    \bf Model & \bf (G/L) & \bf (G/L) & \bf (G/L) & \bf (G/L) \\
    \hline
    Original & 4.07/5.02 & 6.83/7.35 & 6.04/7.14 & 0.34/0.27 \\
    \hline
    H+S & 3.50/3.56 & 5.39/4.00 & 4.36/4.15 & 0.80/0.65 \\
    MVP & 1.52/1.57 & 3.24/3.25 & 2.41/2.84 & 0.71/0.48 \\
    ReasTAP & 1.96/1.91 & 4.38/5.81 & 3.33/4.85 & 0.60/0.50 \\
    \hline
    UFD-GPT & 8.19/8.57 & \bf 8.97/8.00 & \bf 8.99/8.99 & \underline{0.99}/0.90 \\
    UFD-Llama & 7.72/8.43 & \underline{8.51}/\underline{7.86} & \underline{8.63}/\underline{8.50} & 0.96/0.92 \\
    SFD-GPT & \bf 8.96/8.98 & 7.35/5.97 & 7.90/7.00 & \textbf{0.99}/\underline{0.93} \\
    SFD-Llama & \underline{8.95}/\underline{8.89} & 6.24/5.56 & 7.19/6.79 & 0.97/\bf 0.95 \\
    \hline
  \end{tabular}
\end{center}
\caption{Evaluation of dataset description quality on reference-free metrics (Comp: Completeness, Conc: Conciseness, Read: Readability, Faith: Faithfulness) for all models. Scores are reported as GPT/Llama pairs. Completeness, Conciseness, and Readability are scaled from 0 to 10, while Faithfulness is scaled from 0 to 1. Higher values indicate better performance. Bold represents the highest scores for each metric, while underlined values indicate the second highest.}
  \label{tab:eval_reference_all}
\vspace{-.3cm}
\end{table}


%The results presented in Table~\ref{tab:eval_reference_based} highlight the performance differences for reference-based . The results presented in Table~\ref{tab:eval_reference_all} highlight the reference-free evaluation metrics for various models.

\myparagraph{Reference-Free Evaluation}
% 
In the reference-free evaluation (Table~\ref{tab:eval_reference_all}), AutoDDG models consistently outperform other methods. The AutoDDG-UFD-GPT model excels in \textit{Conciseness} and \textit{Readability}, confirming the expectation that user-focused descriptions (UFDs) prioritize clarity and readability. This reinforces our design decision of creating user-centered descriptions. 
%tend to present information in a more coherent and complete manner, which improves their semantic alignment. 
Conversely, AutoDDG-SFD-GPT ranks highest in \textit{Completeness}, as expected from search-focused descriptions (SFDs) that prioritize comprehensive data coverage, even at the expense of readability or conciseness.
% 
For the evaluation of \textit{Faithfulness}, AutoDDG also outperforms other methods. 
% 
Despite numerical differences, GPT and Llama evaluations show consistent trends across all reference-free metrics, further validating the reliability of these assessments.
% 
One interesting observation is that the \textit{Original} descriptions provided with the datasets exhibit low faithfulness.
%
This is understandable, as domain experts often include extra knowledge beyond what is explicitly contained in the dataset when creating descriptions, whereas generated descriptions rely solely on the dataset content.
%
For example, a dataset containing statistics on wind speed, wind time, and wind direction may not include any information about the location of these measurements. Domain experts might add sentences about the location in the original description, but since this information is not present in dataset itself, the evaluation would consider these sentences unfaithful.
%
This underscores the importance of having the dataset creators enrich automatically-generated descriptions to include metadata that cannot be automatically inferred, in particular, the provenance of the data.
%We will leave the exploration of incorporating external domain knowledge into the evaluation process while maintaining faithfulness to the dataset for future work.

\myparagraph{Reference-Based Evaluation}
% 
The trends observed in the reference-based evaluation (Table~\ref{tab:eval_reference_based}) are consistent with the other results.  The AutoDDG-UFD models perform exceptionally well in \textit{BERT Score} and \textit{METEOR}, reflecting better semantic alignment with reference descriptions. This suggests that UFDs, designed with human-centered presentation in mind, better capture the intent and meaning of the original data. On the other hand, AutoDDG-SFD models achieve superior scores in \textit{ROUGE}, signaling a higher overlap with the reference text's exact terms and phrases. This aligns with the typical goal of search-focused descriptions to ensure high precision in matching key data points.

\myparagraph{Summary}
% 
The findings from our experimental evaluation confirm our hypothesis: UFDs score higher in conciseness, readability, and semantic coherence (as seen in \textit{BERT Score}), while SFDs perform better in completeness and exact overlap with reference descriptions. These insights suggest that while UFDs are more suitable for user engagement and semantic clarity, SFDs may be better for datasets where comprehensive and precise detail is paramount for search-related tasks. Therefore, using both description in a dataset search engine enables both a better user experience and search result quality.


\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{figures/ndcg_300words_run.png}
  \vspace{-0.3cm}
  \caption{Comparison of NDCG scores across different settings of \SystemName (noSP-noSFD, noSFD, and full configuration) on BM25. The results demonstrate the impact of enabling semantic profile (SP) and search-focused description (SFD) on retrieval performance. 
  }
  \label{fig:ablation_different_settings}
  \vspace{-0.3cm}
\end{figure}


\vspace{-.3cm}
\subsection{Ablation Study}

To assess the contributions of each module in \SystemName, we conduct an ablation study by selectively disabling key components and analyzing their impact on retrieval performance. This allows us to isolate the effects of the Semantic Profile (SP) module and the Search-Focused Description (SFD) module.  
% 
We compare the following variations:  
% 
(1) \textbf{\SystemName-noSP-noSFD}: Generates descriptions without SP and SFD, relying only on basic dataset samples and statistics.  
% 
(2) \textbf{\SystemName-noSFD}: Includes SP but omits SFD, enhancing descriptions with semantic insights while not optimizing for search retrieval.  
% 
(3) \textbf{\SystemName}: Includes both SP and SFD, leveraging semantic insights while optimizing descriptions for search retrieval.




\myparagraph{Impact of Different Modules in \SystemName}  
% 
Figure~\ref{fig:ablation_different_settings} summarizes the results, 
%shows the impact of different \SystemName modules on retrieval performance, 
measured by NDCG scores.  
%  
The inclusion of the semantic profile (SP) module (\SystemName-noSFD) improves retrieval compared to the baseline (\SystemName-noSP-noSFD).  
%  
Adding the search-focused description (SFD) module (\SystemName) provides further gains, particularly when initial descriptions are concise, reinforcing its role in enhancing search relevance.  

