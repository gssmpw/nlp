\section{Concept Development}
We introduce the concept of \modeourstxt. At a high level, we take inspiration from mobile microtasking UIs and integrate a local response interface directly into the incoming email. Overall, our final design is situated in the  unexplored space in between sentence-level and message-level approaches for AI support.


\begin{figure*}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/formative_prototype}
    \caption{Replying to an email with our first prototype: \textit{(1)} In the first screen, users insert responses directly while reading the email. Tapping on a sentence opens a response widget, with a text box where users enter a response or a prompt that affects the sentence suggestions below. \textit{(2)} After adding local responses, users can edit their reply on a second screen, by reordering paragraphs via drag-and-drop, by deleting paragraphs via swiping left-to-right, and by manual editing via the integrated keyboard.  \textit{(3)} On the third screen, users can finalise the reply before sending it.}
    \Description{This Figure shows how to reply to an email with our first prototype: On the left it shows the first screen, where users insert responses directly while reading the email. Tapping on a sentence opens a response widget, with a text box where users enter a response or a prompt that affects the sentence suggestions below. After adding local responses, users can edit their reply on a second screen, which is shown in the centre of the figure. By reordering paragraphs via drag-and-drop, swiping left-to-right to delete paragraphs, and via manually editing using the integrated keyboard user can edit their reply. On the third screen, which is shown on the right, users to finalise the reply, before sending it.
    }
    \label{fig:BA_prototype}
\end{figure*}

\subsection{Design Goals and Rationale} 
With insights from the literature (\cref{sec:related_work}), we designed our system with several goals in mind. For each goal, we briefly mention our approach here, with more details on its final realisation in \cref{sec:implementation}.
\begin{enumerate}[leftmargin=*]
    \item \textbf{Human decides, AI supports:}
    \label{dg:humandecides}
    The user should be able to make all important decisions, while AI supports these. 
    In our design, the user selects the sentences they want to reply to. 
    The system then suggests response sentences, designed to offer a mix of positive, neutral, and negative responses. 
    \item \textbf{The user stays in control:}
    \label{dg:control}
    The user should stay in control of their reply. 
    The AI should not make unnoticed or unwanted adjustments. 
    Our system does not change text unless requested and the user can always edit the reply before sending it.
    \item \textbf{Support mobile microtasking:}
    \label{dg:microtasking}
    The user should be able to leverage microtasking principles for mobile email replies. %
    Our design provides the surrounding email as context while entering reply text and thus shifts from recall to recognition by eliminating the need to remember the email or scroll back to it.%
    \item \textbf{Support diverse workflows -- with and without AI:}
    \label{dg:workflows}
    In each situation, users should be able to answer in their preferred way.
    Our skippable components offer flexibility.
    Even without AI, users get supportive microtasking structure.
    Conversely, users can choose to rely on AI text to respond fast, with little typing. %
\end{enumerate}





\subsection{First Prototype}
We implemented the concept as an Android app with React Native.\footnote{\url{https://reactnative.dev/}}
At this point, the prototype had our sentence-based mode as shown in \cref{fig:BA_prototype}, with three screens.
The first screen (\cref{fig:BA_prototype} left) showed the email for users to select sentences via touch. Each selection triggered a card view that displayed AI suggestions and a text box for entering text (as a manual response or as a prompt to refine the suggestions).
The LLM always generated two positive, two negative, and two neutral answering options.
One positive and one negative suggestion were shown on the first page, if possible. Users could click on the arrows on the sides to access the others.
A second screen (\cref{fig:BA_prototype} centre) supported manual editing of paragraphs, inserting new ones and/or reordering them via drag and drop.
Finally, a third screen showed the result in a standard text editor view for a last check and final edits, if necessary (\cref{fig:BA_prototype} right). %


\subsection{Formative Study}\label{sec:formative_study}
We conducted a first study to understand how users perceive and interact with our concept, and to inform a design iteration. We recruited 17 participants (1 female, 15 male, 1 preferred not to disclose) from our university network. The study followed our institute's regulations, including information on goals, process, data recording, opt-out and consent.

Participants used our prototype on their own phones. They received a tutorial beforehand. The app did not integrate with actual email accounts to preserve privacy. Instead, it simulated to receive two emails per day, for five days. \revision{These emails were quite long, ranging from 140 to 491 words per email (median: 227), to allow participants to test the prototype extensively.} People were asked to respond to the emails in a reasonable time frame. 

After each reply, the app displayed three 5-point Likert items (``The AI tool was helpful'', ``The AI tool helped me reply to the email faster'', ``The AI tool helped me write a better reply'') and space for open feedback. Following the final email, participants completed a questionnaire about their overall experience and demographics.


\subsection{Results}\label{sec:formative_study_results}
The median time of engaging with each email task in our study was 6.9 minutes, including the time taken to enter feedback. 
People accepted 9.17 suggestions per email.
Nearly \pct{80} of accepted suggestions were accepted without making use of the text input for refinement. %
In \pct{30} of emails, participants composed the email entirely with suggestions without edits afterwards.
When they indeed made edits, the most common ones we identified through manual coding were the following: On the first screen, they removed text (25 instances), added information (11), changed details (8), shortened text (6), and added salutations (5) or closing statements (5). 
On the second screen, they reordered or merged sentences and paragraphs (35), shortened text (10), and changed minor details (8). 
They made similar edits on the third screen. %





The Likert results (\cref{fig:likert_items_formative_study} \revision{in \cref{sec:appendix_extra_figures}}) indicated that participants found the AI to be helpful and that it supported them in writing the replies. They felt in control of the email content and found the suggestions to make sense and not be distracting. They generally agreed that the approach helped them remember to address all parts of an email. However, they were more divided on whether they overall preferred the step-by-step process or the traditional one-step approach for replying to emails.

Open in-app feedback was provided by 14 out of 17 participants. Positive aspects mentioned there and in the final questionnaire included ease of use, faster replies, the quality and inspirational potential of AI suggestions, and an overall improved workflow. 

Negative aspects included slow AI response times, quality of suggestions (e.g. too short or not aligning with their input), minor bugs (e.g. failure to load suggestions), and the number of steps (e.g. some suggested to merge the last two screens into one).  

The final questionnaire asked people to reflect on their workflow with our app. They reported different strategies, such as generating custom replies with keywords, reading the entire email before replying, or reviewing generated suggestions first. Some manually merged or adjusted AI-generated text, while others used it as is. 
The final questionnaire also included the System Usability Scale (SUS)~\cite{brooke1996sus}. The mean score was 78.67 (``very good'', \revision{details in \cref{fig:sus_items_formative_study} in \cref{sec:appendix_extra_figures}}).












\subsection{Prototype Iteration}
\label{ssec:protiter}
In summary, the findings from this first study indicated that participants appreciated our concept as it helped them to write fast and high-quality replies with AI, while still feeling in control. It also revealed individual approaches when answering emails and interacting with the suggestions. Based on the study insights, we made the following concrete changes to our prototype:

\begin{itemize}[leftmargin=*]
    \itemsep.2em
    \item \textit{Reduced number of steps:} We removed the second screen (\cref{fig:BA_prototype} centre) %
    and direly offered the third one for free text editing and finalising. Some participants suggested this and they overall made very similar edits across these two screens.
    \item \textit{Added optional improvement pass:} %
    \revision{We added an ``improve email'' button to the final screen to better support users' varying strategies and preferences for answering emails with AI. We observed that some participants manually edited their emails to create transitions between individual paragraphs generated on the first screen. The \imppass{} feature automates this process, adding missing greetings, sign-offs, and correcting grammar and spelling (see \cref{subsec:appendix_improve_email_prompt} for the used prompt)}. %
    \item \textit{Faster suggestion generation:} We switched from GPT-3.5 Turbo\footnote{\url{https://platform.openai.com/docs/models}} to Llama 3 8B Instruct\footnote{\url{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}}\cite{llama3modelcard}, which we hosted locally to reduce latency and avoid request failures. \revision{While we did not conduct an in-depth evaluation of the models, we compared a set of generations qualitatively and found that the output quality was similar for our use case. Related, we envision that real-world applications could rely on smaller models that can be executed on devices locally to avoid the need to send private email content to a model provider.}%
    \item \textit{Refined suggestions:} We refined our prompting templates to improve suggestions, even with the smaller model. Our new prompts included more context, i.e. all sentence-level replies that were already given up to this point.  
    \item \textit{\revision{Port to a React web app}:} \revision{We ported the app from React Native to a React web app and optimised it. This eased access for participants, as running a React Native prototype required several steps for setup. Instead, a React web app can be accessed via a web browser on any smartphone.}%
\end{itemize}

In the rest of the paper, we always refer to the improved prototype. We next describe it in detail.
