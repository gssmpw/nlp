\section{Method}
\revision{For the main study, we switched from the field study design of the formative study to a more controlled web-based experiment. Our motivation was to scale the study, to compare interaction designs quantitatively, and to introduce task briefings that would allow us to evaluate if people accept even unfitting suggestions for convenience in the study. Thus, we} conducted a within-subject user study with our \revision{iterated} prototype.
The independent variable \ivmode{} had three levels: \modeoursTxt{} (\modeours) -- our proposed design (\cref{sec:implementation}); \modemailtxt{} (\modemail) -- a one-prompt reply generation design close to currently available UIs (\cref{sec:related_work_current_products}); and writing without any AI features (\modemanual). As dependent variables, we logged interaction metrics and collected subjective feedback via questionnaires.

\subsection{Apparatus}

\subsubsection{Web App}
For \modeours, we hosted our prototype (\cref{sec:implementation}) as a web app, with added study information, study logic, and logging.
We integrated a screen for briefings (\cref{sec:method_emails_briefings}) before each email task and one with four Likert items (\cref{sec:procedure_email_tasks}) after each email task (\cref{fig:briefing_and_feedback} \revision{in \cref{sec:appendix_extra_figures}}).
We used a custom study framework to manage counterbalancing and the flow from consent information to prototypes to surveys.

\subsubsection{Comparative Designs}
We implemented two alternatives for the study: For \modemanual, the app only showed a typical drafting view (\cref{fig:baseline_uis_manual} \revision{in \cref{sec:appendix_extra_figures}}). For \modemail, it was designed similar to the typical UI pattern shown in \cref{fig:current_products} -- it offered a text field for (optional) prompting and displayed the generated text (\cref{fig:baseline_uis_msg} \revision{in \cref{sec:appendix_extra_figures}}). Accepting the text with a button inserted it into a draft view for further editing or sending. Rejecting it allowed users to refine their prompt and generate a new draft.

\subsubsection{Incoming Emails and Reply Briefings}\label{sec:method_emails_briefings}
We prepared nine emails, covering an idea pitch contest, a high school reunion, a sales offer, a lunch meeting, a marketing slogan, proofreading for a friend, a sales report deadline, server access, and a gift idea for a retiring coworker. This set was motivated to cover various plausible email topics, with and without (multiple) questions. It also covered various emails lengths, \revision{ranging from 24 to 155 words (median 57).}%

We also prepared a reply briefing for each email. It provided information relevant for \textit{what} to answer, without specifying \textit{how} to write (e.g. tone, structure). For example, for the high school reunion email, the briefing specified that the user was unavailable on a certain date.
These briefings were \textit{not} given to the LLM, which would have simulated unrealistic ``mindreading''. In contrast, our motivation for the briefings was to assess to what extent participants might accept unfitting suggestions for convenience. %
Moreover, the briefings mimic an email workflow where some information is readily available while details may need to be retrieved. For instance, in the high school example, reading the briefing could be seen as similar to checking a calendar app.


\subsection{Participants}
\label{sec:participants}
We recruited 162 participants through the online platform Prolific.\footnote{\url{https://www.prolific.com/}} We excluded 36 participants from our analysis because they either did not complete all tasks (18 participants) or the logs indicated that a technical issue had occurred (18 participants). Our analyses are based on the remaining \studyTwoN{} participants (83 male, 40 female, 1 non-binary, 2 preferred not to disclose). 
Their age ranged from 18 to 72 years (median 32). %
All were proficient in English (91.3\% native speakers).
Their occupations included both professions where frequent email usage is expected (e.g. IT consultant, project manager) and others (e.g. gardener, waiter).
Participants were compensated with about \pounds 10 per hour.

Most participants reported to answer emails at least once a day (\pct{48.41}) or even more than 10 times a day (\pct{21.43}).
Another \pct{16.67} answer emails at least once a week, \pct{7.94} less than once a week, and \pct{5.56} less than once a month.

Most participants (\pct{85.71}) use their smartphone for answering emails. Many also use a laptop (\pct{82.54}) or desktop computer (\pct{53.97}). Some also use a tablet (\pct{19.05}).
They answer emails at home (\pct{84.92}), at the office (\pct{69.84}), and on the go (\pct{53.17}).
Many answer emails for business (\pct{84.92}) and in a private context (\pct{58.73}).

Only \pct{14.29} reported no previous experience with AI.
Many have used ChatGPT (\pct{72.22}). Many have experience with auto-correction (\pct{51.59}), some also with auto-completion (\pct{26.98}), with word or sentence suggestions (\pct{21.42}), and with Smart Reply (\pct{15.87}).
\revision{An overview of all questions and answer options can be found in \cref{sec:appendix_questionnaires}}.


\subsection{Procedure}\label{sec:procedure}
The study was conducted remotely on participants' own smartphones. Access via Prolific was restricted to one person at a time to balance the load for our server.
The sessions were scheduled for 45 minutes and structured as follows:

\subsubsection{Study Intro}
An introduction page explained the study, including information about GDPR compliance, privacy, data collection, and informed consent, in accordance with our institutional regulations. 
In addition, whenever encountering a UI for the first time, our study framework showed an explanation of its features.

\subsubsection{Email Answering Task}\label{sec:procedure_email_tasks}
Participants were asked to reply to nine given emails, three per \ivmode{} (\modeours, \modemail, \modemanual).
We counterbalanced the email topics and also the order of the UIs with a Latin square design to address potential learning or fatigue effects.

For each email task, the briefing (\cref{sec:method_emails_briefings}) was shown at the start and could be accessed again anytime via the information button in the top right corner of the app (\cref{fig:teaser}).
Participants were instructed to ``consider the information in the briefing for answering the email[s]''.


After submitting each email, participants rated four Likert items: ``The app interface was helpful'', ``The app interface helped me reply to the email quickly'', ``The app interface helped me write a good reply'', and ``I was in control of the content of my reply''. 
They could share comments in a text field.


\subsubsection{Final Questionnaire}
This questionnaire was displayed after the final email task.
Participants provided demographics, selected their favourite UI mode, and explained their choice.
They could also leave comments, questions, and feedback. %


\subsection{Qualitative Analysis}
Here we describe our approach to coding open feedback and analysing email texts.

\subsubsection{Coding of Open Feedback}
We followed Grounded Theory~\cite{corbin1990basics} to analyse participants' open feedback. 
In the open coding round, two researchers independently reviewed the data, identifying and labelling sentences that represent specific ideas and principles. 
We then refined these initial codes by merging and clustering related ones, forming (sub-)categories during an axial coding round. 
Our research team discussed emerging themes, leading to synthesised, overarching labels for the clusters and, in some cases, further split categories to capture more nuanced insights from the feedback.
We repeated this process until reaching consensus.

\subsubsection{Analysis of the Email Replies}
\label{sec:quality_m}
Assessing email quality is complex and subjective, as known from studies on people's preferences (cf. \cite{Liu2022aimailperception, Robertson2021cantreply}).
Therefore, we use multiple  quality indicators: 
Formal indicators~\cite{reeves2008emailover50, lewi_jones2014email} include the presence of a \textit{salutation} and a \textit{closing statement} (\cref{sec:results_structure}), and proper \textit{spelling and grammar} (\cref{sec:results_errors}).
We also analysed \textit{briefing conformity}, that is, we checked whether replies covered the key information provided in the briefings (\cref{sec:results_briefing}).
Finally, we share our subjective impressions (\cref{sec:result_quality}).

We employed Binary Coding~\cite{miles2013qualitative} to assess the briefing conformity as well as the formal indicators, except spelling and grammar, which we checked using the language-tool-python\footnote{\url{https://pypi.org/project/language-tool-python/}} library. 
Two researchers coded all email replies independently, assigning a ``1'' if the email met the criteria and a ``0'' if it did not. 
Ambiguous emails were flagged for further review. 
In a second round, these were re-evaluated by the research team until reaching consensus.


\subsection{\revision{Statistical Analysis}}\label{sec:appendix_sigtest}


\revision{To declutter our following report,} \cref{tab:lmm_overview} and \cref{tab:lmm_overview2} summarise the statistical analyses \revision{and results} referred to throughout \cref{sec:results}.
We computed (generalised) linear mixed-effects models (LMMs) in R~\cite{R2020}, using the packages \textit{lme4}~\cite{Bates2015} and \textit{lmerTest}~\cite{Kuznetsova2017}. These models accounted for the individual differences between participants and for differences between the incoming emails via random intercepts. 

The models' fixed effects were \ivmode{} and whether the \imppass{} feature was used in \modeours. 
For the model for briefing conformity, we additionally included a predictor for whether the reply was generated without user input, such that the result was generated fully by the LLM based on the incoming email only. In \modemail, this is done by not entering a prompt for the reply generation. In \modeours, this is done by not providing any local response (manual or suggestion) on screen 1, before using the \imppass{} feature on screen 2. 
Pairwise comparisons were computed with the \textit{emmeans} package with Bonferroni-Holm correction.


For the Likert data, we used rank-aligned repeated measures ANOVA  (ART)~\cite{wobbrock2011art} and ART-C contrasts with Bonferroni-Holm correction for the follow-up analysis~\cite{elkin2021artc}.

We report significance at p~<~0.05. 


\begin{table*}[t!]
\centering
\footnotesize
\newcolumntype{L}{>{\raggedright\arraybackslash}X}
\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}
\renewcommand{\arraystretch}{1.4}
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\linewidth}{lP{2.75em}P{5em}P{22em}P{7em}L}
\toprule
    &
    \textbf{Section} &
    \textbf{Aspect}\newline and model &
    \textbf{Predictors} (baseline: \modemanual) &
    \textbf{Pairwise comparisons} &
    \textbf{Takeaways in words}\newline(only considering sig. results) \\ \midrule
1 &
    \ref{sec:results_time} 
    &
    Completion time\medskip\newline
    \textit{LMM on seconds}
    &
    \modeours{}	$\downarrow$ \newline 
    \deemph{(\lmmci{-4.34}{12.00}{-27.89}{19.21}{=.718})}\medskip\newline 
    \modemail{} $\downarrow^*$ \newline 
    \deemph{(\lmmci{-70.05}{7.72}{-85.20}{-54.90}{<.0001})}\medskip\newline 
    \Imppass{} feature used $\downarrow$ \newline 
    \deemph{(\lmmci{-15.18}{13.23}{-41.14}{10.79}{=.252})}
    &
    \modeours{} vs \modemanual{} \deemph{(\posthoc{-4.34}{=.718})} \medskip\newline 
    \modemail{} vs \modemanual{} \deemph{(\posthoc{-70.05}{<.0001})} \medskip\newline 
    \modeours{} vs \modemail{} \deemph{(\posthoc{65.71}{<.0001})}
    &
    People finished replying faster with \modemailtxt{} than without AI (by 70 seconds on average). \modemailTxt{} was also faster than \modeourstxt{} (by 66 seconds on average).
    \\
    \midrule 
2 &
    \ref{sec:results_speed}
    &
    Writing speed\medskip\newline
    \textit{LMM on characters per second}
    &
    \modeours{} $\uparrow$ \newline 
    \deemph{(\lmmci{.61}{.56}{-.49}{1.71}{=.278})}\medskip\newline 
    \modemail{} $\uparrow^*$ \newline 
    \deemph{(\lmmci{5.16}{.37}{4.43}{5.88}{<.0001})}\medskip\newline 
    \Imppass{} feature used $\uparrow^*$ \newline 
    \deemph{(\lmmci{2.48}{.61}{1.29}{3.68}{<.0001})}
    &
    \modeours{} vs \modemanual{} \deemph{(\posthoc{.61}{=.278})} \medskip\newline 
    \modemail{} vs \modemanual{} \deemph{(\posthoc{5.16}{<.0001})} \medskip\newline 
    \modeours{} vs \modemail{} \deemph{(\posthoc{-4.55}{<.0001})}
    &
    People produced more characters per second with \modemailtxt{} (5.2 chars more per s) and if they used the \imppass{} feature in \modeourstxt{} (2.5 chars more per s).
    \\
    \midrule
3 &
    \ref{sec:results_keystrokes}
    &
    Manual typing\medskip\newline
    \textit{GLMM (Poisson) on keystroke counts}
    &
    \modeours{}	$\downarrow^*$ \newline 
    \deemph{(\lmmci{-.65}{.009}{-.66}{-.63}{<.0001})}\medskip\newline
    \modemail{}	$\downarrow^*$ \newline 
    \deemph{(\lmmci{-.86}{.005}{-.87}{-.85}{<.0001})}\medskip\newline 
    \Imppass{} feature used $\downarrow^*$ \newline 
    \deemph{(\lmmci{-.06}{.011}{-.08}{-.04}{<.0001})}
    &
    \modeours{} vs \modemanual{} \deemph{(\posthoc{-.65}{<.0001})} \medskip\newline 
    \modemail{} vs \modemanual{} \deemph{(\posthoc{-.86}{<.0001})} \medskip\newline 
    \modeours{} vs \modemail{} \deemph{(\posthoc{.21}{<.0001})}
    &
    People needed fewer keystrokes with AI than without it; concretely, even fewer with \modemailtxt{} (\pct{58} decrease) than with \modeourstxt{} (\pct{48} decrease). Using the \imppass{} feature in \modeourstxt{} reduced them further for that UI (\pct{5.9} decrease).
    \\
    \midrule 
4 &
    \ref{sec:results_lengths}
    &
    Reply lengths\medskip\newline
    \textit{GLMM (Poisson) on character counts}
    &
    \modeours{} $\uparrow^*$ \newline 
    \deemph{(\lmmci{.24}{.0060}{.22}{.25}{<.0001})}\medskip\newline
    \modemail{} $\uparrow^*$ \newline 
    \deemph{(\lmmci{.57}{.0037}{.56}{.58}{<.001})}\medskip\newline 
    \Imppass{} feature used $\uparrow^*$ \newline 
    \deemph{(\lmmci{.32}{.0063}{.31}{.33}{<.0001})}
    &
    \modeours{} vs \modemanual{} \deemph{(\posthoc{.24}{<.0001})} \medskip\newline 
    \modemail{} vs \modemanual{} \deemph{(\posthoc{.57}{<.0001})} \medskip\newline 
    \modeours{} vs \modemail{} \deemph{(\posthoc{-.33}{<.0001})}
    &
    People wrote longer replies with AI, even more so with \modemailtxt{} (exp($\beta$)=exp(.57)=1.77 i.e. \pct{77} increase) than with \modeourstxt{} (\pct{27} increase). Using the \imppass{} feature in \modeourstxt{} increased it further for that UI (\pct{38} increase).
    \\
    \midrule 
5 &
    \ref{sec:results_errors}
    &
    Error rates\medskip\newline
    \textit{LMM on errors per character}
    &
    \modeours{}	$\downarrow^*$ \newline 
    \deemph{(\lmmci{-.0011}{.0004}{-.0018}{-.0004}{=.0024})}\medskip\newline 
    \modemail{}	$\downarrow^*$ \newline 
    \deemph{(\lmmci{-.0020}{.0002}{-.0025}{.0015}{<.0001})}\medskip\newline 
    \Imppass{} feature used $\downarrow^*$ \newline 
    \deemph{(\lmmci{-.0012}{.0004}{-.0020}{-.0005}{=.0012})}
    &
    \modeours{} vs \modemanual{} \deemph{(\posthoc{-.0011}{=.0048})} \medskip\newline 
    \modemail{} vs \modemanual{} \deemph{(\posthoc{.-0020}{<.0001})} \medskip\newline 
    \modeours{} vs \modemail{} \deemph{(\posthoc{.0009}{=.0096})}
    &
    People wrote emails with lower error rates with AI than without, even lower with \modemailtxt{} than with \modeourstxt. Using the \imppass{} feature in \modeourstxt{} reduced the error rates further for that UI.
    \\
    \midrule
6 &
    \ref{sec:results_email_similarity}
    &
    Email similarity\medskip\newline
    \textit{LMM on cosine similarity of SBERT embeddings}
    &
    \modeours{} $\uparrow^*$ \newline 
    \deemph{(\lmmci{.09}{.0026}{.09}{.10}{<.0001})}\medskip\newline 
    \modemail{} $\uparrow^*$ \newline 
    \deemph{(\lmmci{.17}{.0023}{.17}{.18}{<.0001})}\medskip\newline 
    \Imppass{} feature used $\uparrow^*$ \newline
    \deemph{(\lmmci{.07}{.0029}{.07}{.08}{<.0001})}
    &
    \modeours{} vs \modemanual{} \deemph{(\posthoc{.09}{<.0001})} \medskip\newline 
    \modemail{} vs \modemanual{} \deemph{(\posthoc{.17}{<.0001})} \medskip\newline 
    \modeours{} vs \modemail{} \deemph{(\posthoc{-.08}{<.0001})}
    &
    People wrote semantically more similar (i.e. less diverse) emails with AI than without, more so with \modemailtxt{} than with \modeourstxt{}. For the latter, using the \imppass{} feature contributed to increasing the similarity of emails.
    \\
    \midrule
7 &
    \ref{sec:results_lexical_diversity}
    &
    Lexical diversity\medskip\newline
    \textit{LMM on the distinct2 metric}
    &
    \modeours{} $\downarrow^*$ \newline 
    \deemph{(\lmmci{-.03}{.0043}{-.04}{-.03}{<.0001})}\medskip\newline 
    \modemail{} $\downarrow^*$ \newline 
    \deemph{(\lmmci{-.02}{.0029}{-.03}{-.01}{<.0001})}\medskip\newline 
    \Imppass{} feature used $\uparrow^*$ \newline
    \deemph{(\lmmci{.01}{.0045}{.003}{.02}{=.0087})}
    &
    \modeours{} vs \modemanual{} \deemph{(\posthoc{-.03}{<.0001})} \medskip\newline 
    \modemail{} vs \modemanual{} \deemph{(\posthoc{-.02}{<.0001})} \medskip\newline 
    \modeours{} vs \modemail{} \deemph{(\posthoc{-.015}{=.0011})}
    &
    People wrote emails with lower lexical diversity (measured as: unique bigrams / number of words) with AI than without it, even more so with \modeourstxt{} than with \modemailtxt{}. Using the \imppass{} feature in \modeourstxt{} closed this gap.
    \\
  \bottomrule
\end{tabularx}
\caption{Overview of significance tests with links to the section, tested measure, predictors, pairwise comparisons, and written interpretation. The arrows indicate if predictors increase ($\uparrow$) or decrease ($\downarrow$) the outcome aspect, with an asterix if these impacts are significant (*).}
\Description{Overview of significance tests with links to the section, tested measure, predictors, pairwise comparisons, and written interpretation. For each statistical test it describes the Section, Aspect and model, Predictors (baseline: NoAI), Pairwise comparisons, and Takeaways in words (only considering sig. results).}
\label{tab:lmm_overview}
\end{table*}




\begin{table*}[t!]
\centering
\footnotesize
\newcolumntype{L}{>{\raggedright\arraybackslash}X}
\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}
\renewcommand{\arraystretch}{1.4}
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\linewidth}{lP{2.75em}P{5em}P{22em}P{7em}L}
\toprule
    &
    \textbf{Section} &
    \textbf{Aspect}\newline and model &
    \textbf{Predictors} (baseline: \modemanual) &
    \textbf{Pairwise comparisons} &
    \textbf{Takeaways in words}\newline(only considering sig. results) \\ \midrule
1 &
    \ref{sec:results_briefing}
    &
    Briefing conformity\medskip\newline
    \textit{GLMM (Binomial) on binary conformity coding}
    &
    \modeours{} $\downarrow^*$ \newline 
    \deemph{(\lmmci{-.77}{.3107}{-1.38}{-.16}{=.013})}\medskip\newline 
    \modemail{} $\downarrow$ \newline 
    \deemph{(\lmmci{-.04}{.2417}{-.52}{.43}{=.857})}\medskip\newline
    \Imppass{} feature used $\uparrow$ \newline
    \deemph{(\lmmci{.25}{.3310}{-.40}{.90}{=.444})}\medskip\newline
    Full reply generated without input $\downarrow^*$ \newline
    \deemph{(\lmmci{-1.70}{.3467}{-2.38}{-1.02}{<.0001})}
    &
    \modeours{} vs \modemanual{} \deemph{(\posthoc{-.77}{=.040})} \medskip\newline 
    \modemail{} vs \modemanual{} \deemph{(\posthoc{-.04}{=.857})} \medskip\newline 
    \modeours{} vs \modemail{} \deemph{(\posthoc{-.73}{=.040})}
    &
    With \modeours, people wrote emails that had a higher chance to miss a key aspect of the study briefing than those written with \modemail{} or manually (\pct{23} of emails missed it for \modeours{} vs \pct{18} for \modemail{} vs \pct{13} for \modemanual). People's prompting behaviour had a larger impact here: Across \modeours{} and \modemail{}, generating a full reply without any own input (83 emails in the data) missed a key aspect of the briefing in half of the cases (\pct{49}).
    \\
    \midrule
2 &
    \ref{sec:results_workflows}
    &
    Skipping local response \medskip\newline
    \textit{GLMM (Binomial) on skipped yes/no}
    &
    Length of incoming email\newline
    (num. standardised words, i.e. characters/5) $\downarrow^*$ \newline
    \deemph{(\lmmci{-.025}{.010}{-.050}{-.004}{=.0171})}
    &
    -
    &
    Each additional word (defined as 5 additional characters) in the incoming email is associated with a \pct{2.46} decreased chance of skipping the local response step in \modeours{}. Skipping is defined as not entering any text on the local response screen of that UI.
    \\
  \bottomrule
\end{tabularx}
\caption{Further significance tests with links to the section, tested measure, predictors, pairwise comparisons, and written interpretation. The arrows indicate if predictors increase ($\uparrow$) or decrease ($\downarrow$) the outcome aspect, with an asterix if these impacts are significant (*).}
\Description{Further significance tests with links to the section, tested measure, predictors, pairwise comparisons, and written interpretation. For each statistical test it describes the Section, Aspect and model, Predictors (baseline: NoAI), Pairwise comparisons, and Takeaways in words (only considering sig. results).}
\label{tab:lmm_overview2}
\end{table*}
