
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/local_response_prompting}
    \caption{The text suggestions in the local response widget are flexible:  \textit{(A)} Users get suggestions without any input.  \textit{(B)} \revision{Suggestions} can be adapted and refined by entering text, for example keywords or a draft snippet. In all cases, suggestions are generated with an LLM based on the text of the incoming email and all local responses that the user has entered so far, \revision{even if responses have been} added to later parts of the email \revision{first}. \revision{In \textit{(C)}, for example, the suggested title of the idea pitch is generated based on the information about the project that the user has already entered in local responses below.} Note that suggestions are paginated, with three pages of two suggestions each.}
    \Description{
    This figure shows screenshots of our user interface displaying an incoming email. The figure is divided into three sections:
    A) Left Section: Email-driven Suggestions without User Input
    The selected sentence in the incoming email reads: "Please feel free to tell me any ideas what we could get her!"
    Below the email, there is an empty text field for optional user input.
    Two AI-generated responses are shown beneath the text field: one suggests a piece of jewellery as a gift, while the other does not offer any ideas.
    B) Centre Section: Prompt-driven Suggestions with User Input
    The same sentence from the email is selected: "Please feel free to tell me any ideas what we could get her!"
    This time, the keywords "balloon ride" are entered into the text field below.
    As a result, the AI-generated suggestions include the idea of a balloon ride in both proposed texts.
    C) Right Section: AI Suggestions Respecting Existing Responses
    One sentence from the incoming email is selected, and the text field below remains empty.
    The AI-generated responses incorporate information from existing responses elsewhere in the email.
    Additionally, all AI suggestions are paginated, with three pages of two suggestions each, as indicated by arrows next to the suggestions.}
    \label{fig:local_response_prompting}
\end{figure*}



\section{Implementation}
\label{sec:implementation}
We implemented a frontend and backend, which preprocessed emails, logged user data, and generated responses. 

\subsection{Frontend}
We implemented our web app with the React\footnote{\url{https://react.dev/}} framework.

\subsubsection{Display of the Incoming Email}
This view matches standard mobile email UIs: It includes the sender's name and picture, the email subject, and the main text body (\cref{fig:teaser} left). 
The user can select sentences in the incoming email by tapping on them (cf. design goal \ref{dg:humandecides}: \revision{Human decides, AI supports}; and goal \ref{dg:microtasking}: \revision{Support mobile microtasking}). This opens the local response widget (\cref{sec:impl_local_response_widget}).
The ``Finalize Reply'' button at the bottom of the UI switches to the next screen (\cref{fig:teaser} centre), which we describe in \cref{sec:impl_finalize}.
In accordance with design goal \ref{dg:workflows} \revision{(Support diverse workflows -- with and without AI)}, no interaction with any sentence or AI feature is required before proceeding to this next screen.



\subsubsection{Local Response Widget}\label{sec:impl_local_response_widget}

This UI widget is inserted into the email text below the user's selected sentence. It comprises of a text field (\cref{fig:teaser} C) and a paginated card view that shows text suggestions (\cref{fig:teaser} D). In the text field, users can enter both manual responses or prompts to refine these suggestions (\cref{fig:local_response_prompting}). 

Concretely, the widget offers six suggestions (2 positive, 2 negative, 2 neutral), showing two at once. The system aims to show one positive and one negative response on the first page, if possible. \revision{This was motivated by findings on positivity bias in AI-generated communication text~\cite{Mieczkowski2021} and to increase the chance of offering a response option fitting to the user's intent (cf.~\cite{Kannan2016smartreply}).} \revision{We realised this by prompting the LLM to do so (see \cref{sec:appendix_sentence_without_input_prompt}). Concretely, the variable ``attribute'' in the prompt template was replaced with \textit{accepting}, \textit{declining}, and \textit{neutral} to generate varying suggestions. In our tests, we observed that this simple prompting approach worked well and that it did not negatively impact generated suggestions in cases where these attributes do not apply (e.g. our ``cat'' example in \cref{fig:teaser}D).} %
Users can navigate through suggestions using the adjacent arrow buttons. They can accept a suggestion by tapping on it.

The widget has two states -- open and collapsed (\cref{fig:teaser} A, B): 
It is collapsed by tapping the currently selected sentence again, by selecting a different sentence, by accepting a suggestion, or by clicking on the check mark in the top right corner. When the text field is empty, the check mark transforms into a trash icon to delete the local reply. Multiple widgets can be in the collapsed state throughout the email but only one widget at a time can be open and in focus. %
A widget's text is shown in the collapsed state. This allows users to keep track of all their local replies so far. Tapping on a collapsed widget opens it again for further editing. 






\subsubsection{Finalising the Reply}\label{sec:impl_finalize}
This view  (\cref{fig:teaser} centre) shows the current state of the reply after the local response step. That is, it displays any text entered in response to individually selected sentences together in a single text field.

Users can manually adjust this text and/or tap the ``Improve'' button to request the AI to enhance the email. 
This \imppass{} feature is realised with a prompt \revision{(see \cref{subsec:appendix_improve_email_prompt})} to the underlying LLM to correct spelling and grammar, refine wording, and add missing salutations or regards while adhering to both the incoming email's content and the existing reply text. 

If no text is entered first, the ``Improve'' button acts as message-level support, generating a reply based on the incoming email's text and the current input on this screen. For example, a user could skip the local response and enter a prompt here, effectively realising a message generation workflow similar to the industry default (\cref{sec:related_work_current_products}). This flexibility contributes to our design goal \ref{dg:workflows} \revision{(Support diverse workflows â€“ with and without AI).}

When the user is satisfied with their reply, the email can be sent by tapping the ``Send Email'' button at the bottom of this screen.


\subsubsection{Improved Email Pop-up}\label{sec:impl_imppass}
The \imppass{} feature does not change the user's text directly, in line with our design goal \ref{dg:control} \revision{(The user stays in control).}
Instead, the new text is shown in a pop-up view with formatting familiar from ``track changes'' in text editors (\cref{fig:teaser} right). 
Users can approve these changes, which updates the text, or discard them (cf. design goal \ref{dg:humandecides}: \revision{Human decides, AI supports}.) Further editing after acceptance and/or requesting improvements repeatedly is possible. 


\subsection{Backend}
Our prototype's backend has three purposes: 
(1) It \textit{hosts the web app} on a Next.js server. 
(2) It provides \textit{email preprocessing}, which handles tasks such as sentence-splitting and making API calls to the LLM. 
(3) It \textit{hosts the LLM}. 

We experimented with several models and APIs and discussed factors such as latency, stability of service, and subjective response quality in meetings with all authors. Based on this, we used the Llama 3 8B Instruct \cite{llama3modelcard} model for the main study. 

Similarly, we iterated over several prompting approaches for the text generation features. Overall, this resulted in a few-shot approach, providing the model with several input-output examples to generate fitting responses. 
As an overview, we use the following prompt templates (details in \cref{sec:appendix_prompts}):

\paragraph{Sentence-level support, without user input:}
We prompted six suggestions for the sentence selected in the email (2 positive, 2 neutral, 2 negative). This balanced the options, following related work~\cite{Kannan2016smartreply}, as the LLM favoured positive responses in our tests.

\paragraph{Sentence-level support, with user input:} 
This was identical to the above case but now it included the user's input in addition to the selected sentence. %
We emphasised alignment with the user's sentiment (e.g. no negative suggestions if the user had entered ``yes'').


\paragraph{Message-level support:}
We prompted the LLM to answer to the whole email, also by taking into account any current user input, if available. A variation of this was also used for the \imppass{} feature (\cref{sec:impl_imppass}). That prompt emphasised improving the current state of the reply while closely adhering to the information provided by the user. %
