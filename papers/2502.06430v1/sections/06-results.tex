\section{Results}\label{sec:results}


\subsection{Analysis of Interaction Logs}\label{sec:results_interaction_logs}
We report our analyses of the interaction data. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/interaction_logs_boxplots.pdf}
    \caption{Three measures of interaction behaviour: Task completion time \textit{(left)}, manual typing \textit{(centre)}, and writing speed \textit{(right)}. All AI features increased typing speed and reduced the time taken (both sig. for \modemail). They also reduced the number of keystrokes (sig. for \modeours{} and \modemail). If people made use of the optional \imppass{} feature \revision{(impr.)} in \modeours, this contributed to narrowing the gap between the otherwise sentence-level design of \modeours{} and the message-level design of \modemail{} (sig. for manual typing and writing speed). See \cref{sec:results_interaction_logs} for details.}
    \Description{This figure presents three box plots that measure interaction behaviour from the study across three different user interfaces: NoAI (manual mode), CDLR (AI-supported with and without the optional improvement pass feature), and MSG (AI-supported). The three metrics being compared are:
    Task Completion Time (Left):
    This plot shows how long it took participants to complete the task in minutes.
    Both CDLR (with and without improvement) and MSG show reduced task completion times compared to NoAI, with MSG showing the most significant reduction.
    Manual Typing (Center):
    This plot shows the number of keystrokes made by participants during the task.
    Both CDLR and MSG significantly reduced the number of keystrokes compared to NoAI, particularly when participants used the optional improvement pass feature in CDLR.
    Writing Speed (Right):
    This plot shows the writing speed of participants measured in characters per second.
    Both CDLR and MSG increased writing speed compared to NoAI, with MSG again showing the most notable improvement.
    Overall, the figure demonstrates that AI-supported interfaces (CDLR and MSG) led to faster task completion, fewer keystrokes, and increased writing speed. In particular, the optional improvement pass feature in CDLR helped narrow the performance gap between the sentence-level design of CDLR and the message-level design of MSG. For detailed statistical analysis, see the corresponding section of the paper (Section 6.1).}
    \label{fig:interaction_logs_boxplots}
\end{figure*}


\subsubsection{Task Completion Time}\label{sec:results_time}

On average, participants took \mins{3.20} (SD 2.51, median 2.46) to write an email manually.
With \modemail, this decreased to \mins{2.03} (SD 1.90, median 1.44), while \modeours{} reduced it to \mins{2.95} (SD 2.49, median 2.32). For \modeours, using the \imppass{} feature resulted in \mins{2.90} (SD 2.58, median 2.26), while not using it had \mins{3.06} (SD 2.29, median 2.57).
\cref{fig:interaction_logs_boxplots} (left) shows this as box plots.
These differences were significant as follows (\cref{tab:lmm_overview}, row 1): 
Participants finished replying significantly faster with \modemail{} than without AI (-\secs{70}). \modemail{} was also significantly faster than \modeours{} (-\secs{66}).



\subsubsection{Writing Speed}\label{sec:results_speed}

On average, participants wrote 2.05 characters per second without AI (SD 1.57, median 1.78).
Replying with \modemail{} had a mean of 7.21 (SD 7.90, median 4.89), while the mean speed with \modeours{} was 4.38 (SD 5.59, median 2.91). For \modeours, using the \imppass{} feature resulted in 5.03 (SD 6.47, median 3.23), while not using it had 2.91 (SD 2.00, median 2.31).
\cref{fig:interaction_logs_boxplots} (right) shows this as box plots.
These differences were significant as follows (\cref{tab:lmm_overview}, row 2): 
Compared to writing without AI, participants produced significantly more characters per second with \modemail{} (5.2 chars more per s) and if they used the \imppass{} feature in \modeours{} (2.5 chars more per s). The difference between \modemail{} and \modeours{} was also significant.




\subsubsection{Manual Typing}\label{sec:results_keystrokes}
Without AI, participants on average needed 321.69 keystrokes (SD 217.76, median 284), compared to 146.67 (SD 144.97, median 108) with \modemail, and 174.62 (SD 166.57, median 128) with \modeours. For \modeours, using the \imppass{} feature had a mean of 176.0 (SD 173.7, median 126), while not using it had 171.4 (SD 149.9, median 133.5).
\cref{fig:interaction_logs_boxplots} (centre) shows this as box plots.
These differences were significant (\cref{tab:lmm_overview}, row 3): People needed significantly fewer keystrokes with AI features than without them, and even significantly fewer with \modemail{} (\pct{58} decrease) than with \modeours{} (\pct{48} decrease). Using the \imppass{} feature in \modeours{} significantly reduced this further for that UI (\pct{5.9} decrease). In summary, all AI features significantly reduced manual typing.




\subsubsection{Interaction with \modeours}
We logged interactions specific to \modeours.
On average participants tapped on 2.64  (SD 2.89, median 2) sentences per email, that is, on \pct{30.36} (SD \pct{29.59}, median \pct{23.08}) of sentences in each email.
They replied to \pct{87.37} of tapped sentences. In \pct{83.27} of the cases, they did so by accepting a suggestion. %

Suggestions were paginated; most suggestions (\pct{67.15}) were accepted on the first page. Another \pct{19.50} and \pct{13.36} were accepted on the second and third page, respectively.
The majority of accepted suggestions (\pct{80.14}) were generated without an explicit prompt, and most (\pct{92.30}) were not edited afterwards.


\revision{On the first screen, \pct{69.05} of participants accepted a sentence suggestion at least once, and \pct{55.56} manually entered text for at least one local response. %
On the second screen, \pct{83.33} of participants accepted at least one email-level suggestion. Only \pct{1.59} (two participants) did not use any \modeours-specific features.}




On average, \mins{1.67} (\pct{57.23}) were spent on the first screen and \mins{1.25} (\pct{42.77}) on the second (\cref{fig:time_spent_on_screens_barplot}).
Participants used the \imppass{} feature for 287 emails (\pct{75.93}) and accepted an improved email for 274 emails (\pct{72.49}).
When the \imppass{} feature was used at least once, an improved email was requested on average 1.35 times (SD 0.95, median 1.00) and accepted 1.13 times (SD 0.47, median 1.00) per email.
The last accepted improved email was identical to the sent email in \pct{83.94} of all cases.
When participants made changes these had a mean edit distance of 72.73 (SD 97.10, median 37).


\subsubsection{Interaction with Full Email Generation (\modemail)}
With this UI, \pct{71.98} of the first generated replies were accepted; otherwise, a new generation was requested.
Users spent \mins{0.43} (\pct{21.20}) on the incoming email screen, \mins{1.31} (\pct{65.09}) on the generation view and \mins{0.28} (\pct{13.71}) on the editing screen (\cref{fig:time_spent_on_screens_barplot}).

\subsubsection{Workflow Analysis}\label{sec:results_workflows}


\begin{figure}
    \centering
    \includegraphics[width=\minof{\columnwidth}{0.66\textwidth}]{figures/sentence_based_workflow_scatterplot}
    \caption{Analysis of workflows with \modeourstxt: Each point is one email and its position is the state of the drafting process at the moment when the user switched from the first screen (\cref{fig:teaser}.1) to the second (\cref{fig:teaser}.2). Concretely, the x-axis shows normalised time (0-\pct{100}), i.e. temporal progression. The y-axis shows normalised length, i.e. draft progression. Note that y-values >\pct{100} are possible if an intermediate draft is longer than the final version. Colour and marker shape indicate if the \imppass{} feature was used or not. The figure reveals three clusters: \textit{(1) Bottom left} -- here, people skipped to the second screen and used the \imppass{} feature to generate a draft. \textit{(2) Top right} -- mostly drafting on the first screen, with light manual editing on the second. \textit{(3) In between} -- partly drafting on the first screen and finalising it with AI on the second one.}
    \Description{This figure displays a scatter plot analysing workflows with content-driven local responses, focusing on when participants switched from the first screen to the second screen during the email drafting process.
    X-axis (Progress in Time [\% total time]): This axis represents the normalised time (ranging from 0 to 100) indicating the temporal progression of the drafting process.
    Y-axis (Draft Progress [\% final length]): This axis represents the normalised length of the draft at the moment of switching screens. Values greater than 1 are possible if an intermediate draft was longer than the final version.
    Colour Coding: The colour of each point indicates whether the "improvement pass" feature was used:
    Blue points represent emails where the improvement pass feature was used.
    Orange points indicate emails where it was not used.
    The scatter plot reveals three distinct clusters of participant behaviour:
    Bottom Left Cluster:
    Participants in this group quickly moved to the second screen and utilised the improvement pass feature to generate a draft with minimal work on the first screen.
    Top Right Cluster:
    Participants in this group spent most of their time drafting on the first screen, with only light manual editing on the second screen.
    In Between Cluster:
    This group represents participants who partly drafted on the first screen and then finalised the draft with AI assistance on the second screen.
    This analysis highlights the different strategies participants used during the email drafting process, depending on their interaction with the interface and the improvement pass feature.}
    \label{fig:workflow_scatterplot}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\minof{\columnwidth}{0.75\textwidth}]{figures/time_spent_on_screens.pdf}
    \caption{Participants spent their time on different screens and thus different aspects. The figure shows the means for the time spent on screens that focus on reading the incoming email vs on screens that focus on responding (colour). Borders indicate which steps required AI (solid) or offered it optionally (dashed). For \textit{\modemanual}, users read the email, then spend most of the time writing the reply. For \textit{\modeours}, the local response screen (\cref{fig:teaser}.1) enables reading and responding in parallel (striped), followed by responding on the second screen (\cref{fig:teaser}.2), both with optional AI (sentence suggestions, \imppass{}). In contrast, \textit{\modemail} requires AI after the initial reading phase to generate the response, which can then be manually edited.}
    \Description{This figure displays a bar plot analysing times spent on each step in the answering process. The figure shows the means for the time spent on screens that focus on reading the incoming email vs on the screens that focus on responding. For NoAI, users read the email, then spend most of the time writing the reply. For CDLR, the local response screen enables reading and responding in parallel, followed by responding on the second screen, both with optional AI (sentence suggestions, improvement pass). In contrast, MSG requires AI after the initial reading phase to generate the response, which can then be manually edited.}
    \label{fig:time_spent_on_screens_barplot}
\end{figure}

For \modeours, we discovered three main workflows by plotting when people switched from the first screen (\cref{fig:teaser}A) to the second (\cref{fig:teaser}B). \cref{fig:workflow_scatterplot} reveals three clusters: (1) Sometimes people went straight to the second screen and used the \imppass{} feature to create a draft. (2) Alternatively, they spent most of their time drafting on the first screen, with light manual editing on the second. (3) Finally, people partially drafted on the first screen and finished it on the second screen, using AI.
We fitted a GMM\footnote{Gaussian Mixture Model with 3 components using \url{https://scikit-learn.org/}} to estimate the number of emails: Cluster 1 had 136, cluster 2 had 54, and cluster 3 had 188 emails.

We also examined the relationship of the incoming email's length and whether people skipped the local response screen without entering any text. This was significant (\cref{tab:lmm_overview2}, row 2): Each additional word (i.e. 5 additional characters) in the incoming email is associated with a \pct{2.46} decreased chance of skipping the local response step in \modeours{}. 

For \modemail, we found that in most cases (\pct{77.8} of emails) people sent the generated drafts without manually editing them further. When they indeed edited them (\pct{22.2} of emails), the mean edit distance between generated and edited version was 64.87 (SD 67.48, median 44.50). This corresponds to typing about twelve words~\cite{kristensson2014inviscid}.

We also analysed how people prompted with \modemail: In a majority of cases (\pct{82.01} of emails), participants entered a prompt right away. Otherwise, they generated text solely based on the information in the incoming email. In half of those cases (\pct{51.47}), participants did not accept the result and generated another. In comparison, such a regeneration was only needed in \pct{22.9} of the cases where participants entered a prompt. We observed a learning effect for some: \pct{40} of people started entering a prompt if they were not happy with the initial result generated without a prompt. %



\subsection{Perception of Interaction}\label{sec:results_perception}
We analysed participants' perception of the three UIs.


\subsubsection{In-app Questionnaire (Likert Data)}\label{sec:results_in_app}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/inapp_likert_items.pdf}
    \caption{Likert results on perception of the UIs and interaction, rated after each email task. Overall, participants rated the AI-supported UIs higher on speed, quality, and helpfulness compared to the manual mode. However, the latter was rated higher on control.}
    \Description{This figure presents bar charts displaying Likert scale results from participants' perceptions of different UIs and their interactions, rated after completing email tasks. 
    The figure is divided into four sections, each comparing three UIs: NoAI (manual mode), CDLR (AI-supported), and MSG (AI-supported).
    Top-Left Chart: The app interface was helpful
    The NoAI interface received mixed responses, with a significant portion of participants disagreeing or remaining neutral, and fewer strongly agreeing.
    Both CDLR and MSG interfaces were rated more positively, with a larger number of participants agreeing or strongly agreeing that the interfaces were helpful.
    Top-Right Chart: The app interface helped me reply to the email quickly
    The NoAI interface again had a more varied response, with some participants disagreeing or remaining neutral, while others agreed.
    CDLR and MSG interfaces were rated highly for helping users reply quickly, with the majority of participants agreeing or strongly agreeing.
    Bottom-Left Chart: The app interface helped me write a good reply
    Similar to the other charts, the NoAI interface had a mix of responses, with fewer participants strongly agreeing.
    CDLR and MSG interfaces were again rated highly, with most participants agreeing or strongly agreeing that these interfaces helped them write good replies.
    Bottom-Right Chart: I was in control of the content of my reply
    For this aspect, the NoAI interface was rated slightly higher, with more participants strongly agreeing that they felt in control of their reply content.
    Although CDLR and MSG interfaces were also rated positively, there was a slight decrease in the number of participants who strongly agreed compared to the NoAI interface.
    In summary, the figure shows that participants generally rated the AI-supported UIs (CDLR and MSG) higher in terms of speed, quality, and helpfulness. 
    However, the manual mode (NoAI) was rated slightly higher in terms of giving users a sense of control over their replies.}
    \label{fig:inapp_likert_items}
\end{figure*}

Participants rated four Likert items in the app after each email (\cref{fig:inapp_likert_items}). 
We found statistically significant \revision{effects of \ivmode{} on all four items -- speed (\artf{2}{996.11}{433.71}{<.0001}, \petasq{.46}), control (\artf{2}{996.11}{20.60}{<.0001}, \petasq{.04}), quality (\artf{2}{996.13}{466.99}{<.0001}, \petasq{.48}), and helpfulness (\artf{2}{996.1}{430.61}{<.0001}, \petasq{.46}).}
\revision{Concretely,} \modeours{} and \modemail{} were both rated significantly higher than \modemanual{} on speed \revision{(\modeours{}: \artc{996}{23.34}{<.0001}; \modemail{}: \artc{996}{27.25}{<.0001})}, quality \revision{(\modeours{}: \artc{996}{25.72}{<.0001}; \modemail{}: \artc{996}{27.18}{<.0001})}, and helpfulness \revision{(\modeours{}: \artc{996}{24.65}{<.0001}; \modemail{}: \artc{996}{26.14}{<.0001})}. They were both rated significantly lower than \modemanual{} on control \revision{(\modeours{}: \artc{996}{-5.88}{<.0001}; \modemail{}: \artc{996}{-5.17}{<.0001})}. 
The only significant difference between the UIs with AI was that \modemail{} was rated higher on speed than \modeours{} \revision{(\artc{996}{3.91}{=.0001})}. 


\subsubsection{In-App Feedback}
We reviewed the in-app feedback optionally provided after each reply.
For both AI modes it was overwhelmingly positive, such as: ``Im really enjoying this kind of AI help mode.'' (P60\oldId{P1351}, \modemail), ``It made work easy'' (P76\oldId{P1370}, \modemail), ``Very smooth process, good suggestions for each part.'' (P47\oldId{P1329}, \modeours), and ``This made my reply look way better.'' (P81\oldId{P1375}, \modeours).



The negative feedback was less homogeneous.
For \modemail, around half of these critiques highlighted difficulties in getting the AI to incorporate specific information, such as: ``the AI who seemed to resist wanting to offer access to my colleague'' (P25\oldId{P1298}); or ``had a bit of trouble trying to get the AI to properly acknowledge that \$200 was okay [...]''  (P37\oldId{P1312}). 
Related, P53\oldId{P1338} noted that ``Control in replying was lacking, It didn't give me many options to 'add' ideas of my own.''
People found ways to steer the system; P47\oldId{P1329} said that ``I had to adjust the prompt a few times to get the sort of reply that I was looking for, but it did generate a good reply overall and I was satisfied with the end result.''

Notably, issues with including specific information were rarely mentioned for \modeours.
Most of the negative feedback instead concerned the tone: ``This was too wordy for an informal email.'' (P56\oldId{P1341}), %
and ``The ai was helpful but it made the response feel slightly too formal and professional.'' (P49\oldId{P1333}) %

For the manual mode, people ``had no issues, [and] felt able to use the platform freely and there was no technical faults'' (P7\oldId{P1276}), and that ``It was just like normal email.'' (P60\oldId{P1351}). %


\subsubsection{Favourite Reply Support}\label{sec:results_fav_mode}
Only \pct{4} (5 people) preferred \modemanual, %
\pct{49.2} (62 people) favoured \modemail, and \pct{43.7} (55 people) preferred \modeours.
The remaining \pct{3.2} (4 people) did not pick a favourite.

Notably, the high-level code ``Efficiency'' occurred in \pct{56.45} of comments for \modemail{} and \pct{32.73} for \modeours{}.
``Quality'' in \pct{25.82} for \modemail{} and \pct{30.91} for \modeours{}.
``Control'' in \pct{8.07} for \modemail{} and \pct{29.09} for \modeours{}.
``Tailoring'' zero times for \modemail{} and \pct{5.46} for \modeours{}.
The remaining comments were assigned the code ``Others'' (e.g. P105\oldId{P1405} ``just liked the interface'' of \modemail{} and P76\oldId{P1370} favoured \modeours{} because it supported them in being creative).

Two out of the four people who did not select a favourite stated that they liked both depending on the ``context'' (P77\oldId{P1371}).
For instance, P19\oldId{P1290} explained that ``both have different advantages in different situations. Single prompt allows to produce a full email much faster so is handy when you are short of time but still want to respond. Sentence based provides the user the ability to create a much more tailored email which can cover all bases.''

The \pct{4} (5 people) who preferred \modemanual{} said they were ``used to it'' (P4\oldId{P1273}) or ``confident in [their] writing ability'' (P78\oldId{P1372}).
















\subsubsection{Summary}
People perceived AI features as helpful and preferred having them. They were divided about their favourite and perceived meaningful tradeoffs between the two designs with AI on control vs efficiency: While people felt in control with all UIs (\cref{sec:results_in_app}), when reflecting on their favourite, they mentioned control aspects relatively more frequently for \modeours{} than \modemail{} -- and vice versa for efficiency.




\subsection{Analysis of Emails}\label{sec:results_emails}
We analysed the content of the emails. \cref{fig:quality_boxplots} shows four box plots as an overview.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/quality_boxplots.pdf}
    \caption{Four measures of email characteristics from our study. The plots show email length \textit{(left)} and rate of spelling/grammar/punctuation errors \textit{(centre left)}. Moreover, we measured lexical diversity with the distinct-2 score \textit{(centre right)}, which is defined as an email's number of distinct bigrams divided by its number of words (higher = more diverse). Finally, we measure diversity between emails \textit{(right)}, based on the cosine similarity of vector embeddings (higher = less diverse). Overall, all AI features increased reply lengths, decreased error rates, and lowered diversity (all sig.).  \revision{For \modeours{} we further distinguish between emails where the improvement pass feature was used (impr.) and those where it was not (no impr.).}
    See \cref{sec:results_emails} for details.}
    \Description{This figure presents four box plots comparing different characteristics of emails across three user interfaces: NoAI (manual mode), CDLR (AI-supported with and without the optional improvement pass feature), and MSG (AI-supported). The metrics being compared are:
    Email Length (Left):
    This plot shows the length of the emails in characters.
    AI-supported interfaces (CDLR and MSG) led to longer emails compared to the NoAI interface, with MSG producing the longest emails.
    Error Rate (Center Left):
    This plot displays the rate of spelling, grammar, and punctuation errors relative to the number of characters in the email.
    Both CDLR and MSG reduced the error rate compared to NoAI, with slightly better performance for the improvement pass-enabled CDLR and MSG interfaces.
    Diversity within Emails (Center Right):
    The distinct-2 score measures the lexical diversity within individual emails by calculating the number of unique bigrams (word pairs) relative to the total word count. A higher score indicates greater diversity.
    NoAI emails had higher within-email diversity compared to CDLR and MSG, which showed similar scores indicating reduced lexical diversity.
    Diversity between Emails (Right):
    This plot measures the cosine similarity between vector embeddings of emails, with higher scores indicating less diversity between emails.
    Emails generated using CDLR and MSG were more similar to one another, as indicated by higher cosine similarity scores, compared to those written using the NoAI interface.
    Overall, the figure demonstrates that AI-supported interfaces (CDLR and MSG) increased email length, reduced error rates, and resulted in slightly less diversity both within and between emails. For detailed analysis, refer to the corresponding section in the paper (Section 6.2).
    }
    \label{fig:quality_boxplots}
\end{figure*}


\subsubsection{Email Lengths}\label{sec:results_lengths}

On average, emails written without AI were 302.5 characters long (SD 169.7, median 267).
\modemail{} resulted in 536.1 (SD 319.0, median 447.5), while \modeours{} had 483.0 (SD 285.4, median 382). For \modeours, using the \imppass{} feature had a mean length of 523.9 (SD 294.1, median 412.0), while not using it had 390.6 (SD 241.5, median 324.5).
These differences were significant (\cref{tab:lmm_overview}, row 4): People wrote significantly longer replies with the AI features than without them, and significantly more so with \modemail{} (\pct{77} increase) than with \modeours{} (\pct{27} increase). Using the \imppass{} feature in \modeours{} significantly increased this further for that UI (\pct{38} increase). In summary, all AI features significantly increased text lengths.


\subsubsection{Error Rates}\label{sec:results_errors}
We checked grammar and spelling with the language-tool-python\footnote{\url{https://pypi.org/project/language-tool-python/}} library.
Per email, we recorded the minimum of British English and American English spell checking to avoid penalising spelling differences. %
Manual writing had the highest mean error rate of .00375 errors per character.  Both AI versions were about half of that: \modemail{} had .00176 and \modeours{} had .00182. Using the \imppass{} feature in \modeours{} contributed to reducing errors (mean .00147 when using it vs .00260 when not). All these differences were significant (\cref{tab:lmm_overview}, row 5). In summary, all AI features significantly reduced error rates.

\subsubsection{Diversity Across Emails}\label{sec:results_email_similarity}
We analysed the semantic similarity between emails, following related work~\cite{padmakumar2024diversity}. We computed the cosine similarity of the vector embeddings of all pairs of emails written with the same mode and for the same briefing, using the Sentence Transformers library (SBERT\footnote{\url{https://sbert.net}, specifically \url{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}})~\cite{reimers2019sbert}.
As expected from the literature, manual emails had the lowest mean pairwise similarity (.582), that is, they were the most diverse. \modemail{} had the highest similarity (.756), followed by \modeours{} (.726). 
Using the \imppass{} feature in \modeours{} contributed to the increase (.676 without using it vs .749 with it). 
All these differences were significant (\cref{tab:lmm_overview}, row 6). In summary, all AI features significantly reduced semantic diversity.


\subsubsection{Diversity Within Emails}\label{sec:results_lexical_diversity}
We analysed lexical diversity, as in related work~\cite{Fu2023sentencevsmessage}, with the distinct-2 metric, defined as the number of distinct bigrams divided by the total number of words.
Our results match the related work: Writing without AI had the highest mean lexical diversity (.950), \modemail{} lowered it to .930, and \modeours{} had .924.
Using the \imppass{} feature in \modeours{} (almost) closes the gap between \modeours{} and \modemail{} (.915 without it vs .927 with it). 
All these differences were significant (\cref{tab:lmm_overview}, row 7). In summary, lexical diversity was significantly affected by all AI features, in the way we would expect from related work~\cite{Fu2023sentencevsmessage}: Sentence-level generations decreased it more than message-level generations.







\subsubsection{Email Structure}\label{sec:results_structure}

Salutations were missing in \pct{8.5} of manually written replies and in \pct{10.6} with \modeours. All replies with \modemail{} had salutations. All replies with \modeours{} and the \imppass{} feature had a salutation.
Similarly, only one email with \modemail{} lacked a closing statement, compared to \pct{14.3} for \modemanual{} and \pct{9.0} for \modeours. Again, when the \imppass{} was used in \modeours, all emails ended with a closing signature.


\subsubsection{Briefing Conformity}\label{sec:results_briefing}
Each email reply task showed a briefing that asked participants to respond with certain information (see \cref{sec:procedure_email_tasks}). This allowed us to analyse if their emails conformed to this or not (see \cref{sec:quality_m}).
This varied across the UIs: With \modeours, \pct{23} of emails missed a key aspect of the study briefing, compared to \pct{18} for \modemail{} and \pct{13} for \modemanual.

The differences between \modeours{} and the other two UIs were significant (\cref{tab:lmm_overview2}, row 1). %
However, people's prompting behaviour had a larger impact here: Across \modeours{} and \modemail{}, generating a full reply without any own input (83 emails in the data) missed a key aspect of the briefing in half of the cases (\pct{49}). We return to this in the discussion (\cref{sec:discussion_methods})



\subsubsection{Subjective Assessment of Quality}\label{sec:result_quality}
All emails were read by (at least) two researchers. %
While length and structure varied (\cref{sec:results_lengths}, \cref{sec:results_structure}), we did not notice ``nonsense'' responses. Even those emails which did not conform to the briefings (\cref{sec:results_briefing}) reflected the general topic and most would have been believable replies. We noticed that sensible replies can vary drastically -- from short responses to elaborate, formal emails. The latter mostly coincided with message-level AI generation. We do not consider this an issue of quality since we did not specify a level of formality to follow. Overall, based on our subjective assessment, we thus concluded that reply quality was suitable across all UIs. 
