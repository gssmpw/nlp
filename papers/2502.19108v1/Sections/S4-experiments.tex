% \section{Downstream Application}
\section{Application Case 2: Emoticon Recommendation}
\label{sec:experiments}
In this section, we evaluate the feasibility of our dataset through quantitative experiments. Specifically, we evaluate our dataset on current emoticon recommendation.

% We \todo{highlight} the following research questions (RQ):
% Interested in user behavior analysis and emoticon retrieval task.
% \begin{itemize}
%   \item \textbf{RQ1:} Are user's preferences distinctive?
%   \item \textbf{RQ2:} Do users behave differently in different groups or with different people?
%   \item \textbf{RQ3:} Do user's preference styles change over time?
%   \item \textbf{RQ4:} How does emoticonU behave with current emoticon retrieval methods?
% \end{itemize}

% \subsection{emoticon Retrieval}
\subsection{Experiment Settings} 
We evaluate two baseline methods for emoticon recommendation, described as follows:

\begin{enumerate} 
    \item \textbf{MOD} \cite{MOD}: Takes a list of negative samples along with one true positive sample and calculates the probability of each sample being the correct match. The results are then used to rank the samples.
    \item \textbf{SRS} \cite{learning-to-respond-2021}: This approach ranks a list containing negative and one positive sample, returning the ranked results.
\end{enumerate}

% 1. language restriction
% 2. domain restriction
% 3. 2 emoticon retrieval models, 1.2.
We conduct experiments on three distinct perspectives of the dataset: (1) the English and Chinese subsets, (2) the domain-specific subset, and (3) the complete dataset.

\subsubsection{English and Chinese Subset}
Since MOD can only handle English and Chinese, we select groups with more than 80\% of text being either English or Chinese and train the model on it. We do the same for SRS.

\subsubsection{Domain}
To interpretate the performance between different domains, we separate the dataset by domains as shown in Table \ref{tab:category-language-sparsity}. Since SRS is not limited by multilingualism, we evaluate using SRS.

\subsubsection{Complete}
Finally, the entire dataset is utilized for evaluation on SRS.

\subsection{Results}
\subsubsection{English and Chinese Subset}
Based on the experimental results presented in Table \ref{tab:experiments-language}, we observe that the English language performs better than the Chinese language across both the MOD and SRS methods. Specifically, the English subset consistently shows higher scores in all evaluation metrics (MRR, R@1, R@3, R@5, and R@10). For instance, in the 1:9 ratio, the MOD method for English achieves an MRR of 0.289, while the Chinese MOD method only reaches 0.317, this could be due to the smaller data size of Chinese in comparison to English.

% The general behavior of the emoticon recommendation models is consistent with typical recommendation system patterns. Both MOD and SRS methods exhibit higher performance when the negative-to-positive sample ratio is 1:9, as seen in the higher metrics across both languages. This suggests that the models perform better when the positive sample is more prevalent. Performance drops when the negative-to-positive ratio increases to 1:19, which is a common trend in recommendation tasks where increasing the number of negative samples makes the task more difficult.

% The results also provide valuable insights into the emoticonU dataset. The variation in performance across different sample ratios indicates that the dataset contains a diverse set of emoticons and user interactions. The generally lower performance in the Chinese subset could suggest potential language-specific challenges, such as linguistic or cultural nuances that affect the model’s ability to recommend emoticons effectively. In contrast, the higher performance on the English dataset might reflect better training data quality or more focused user interactions. These findings highlight the importance of considering language-specific factors when designing recommendation models, and the difference in performance between MOD and SRS emphasizes how the composition of the sample affects recommendation outcomes.



% only full day, epoch 50, 9+1, 19+1
% \begin{table*}[ht]
% \centering
% \begin{tabular}{lcccccccc}
% \toprule
% Baseline & Epochs & \# of Negative Samples & MRR & R@1 & R@3 & R@5 & R@10 & R@20 \\ \midrule
% \textbf{MOD} &&&&&&&& \\ \hline
% \textit{Chinese} & 20 & 4+1 & \textbf{0.435} & 0.156 & \textbf{0.625} & \textbf{1.000} & - & - \\
%  & & 9+1 & 0.255 & 0.063 & 0.313 & 0.500 & 1.000 & - \\
%  & & 19+1 & 0.224 & 0.063 & 0.281 & 0.344 & 0.563 & 1.000 \\
%  & 50 & 4+1 & 0.362 & 0.063 & 0.593 & \textbf{1.000} & - & - \\
%  & & 9+1 & 0.317 & 0.125 & 0.344 & 0.531 & 1.000 & \\
%  & & 19+1 & 0.156 & 0.063 & 0.094 & 0.156 & 0.406 & 1.000 \\
% \hline
% \textit{English} & 20 & 4+1 & \textbf{0.450} & 0.194 & \textbf{0.585} & \textbf{1.000} & & \\
%  & & 9+1 & 0.293 & 0.095 & 0.314 & 0.510 & \textbf{1.000} & \\
%  & & 19+1 & 0.178 & 0.051 & 0.144 & 0.238 & 0.494 & \textbf{1.000} \\
%  & 50 & 4+1 & \textbf{0.459} & 0.199 & \textbf{0.607} & \textbf{1.000} & - & - \\
%  & & 9+1 & 0.289 & \textbf{0.307} & 0.495 & 0.224 & \textbf{1.000} & - \\
%  & & 19+1 & 0.180 & 0.051 & 0.145 & 0.250 & 0.511 & 1.000 \\
% \midrule
% \textbf{SRS} &&&&&&&& \\ \hline
% \textit{Chinese} & half day & 4+1 & 0.447 & 0.214 & 0.582 & \textbf{1.000} & & \\
%  & & 9+1 & 0.264 & 0.056 & 0.281 & 0.500 & \textbf{0.891} & \\
%  & & 19+1 & 0.270 & 0.061 & 0.138 & 0.219 & 0.377 & \textbf{0.891} \\
%  & full day & 4+1 & 0.447 & 0.214 & 0.582 & \textbf{1.000} & - & - \\
%  & & 9+1 & 0.264 & 0.056 & 0.281 & 0.500 & \textbf{0.891} & \\
%  & & 19+1 & 0.270 & 0.061 & 0.138 & 0.219 & 0.377 & \textbf{0.891} \\
% \hline
% \textit{English} & half day & 4+1 & 0.413 & 0.187 & 0.597 & \textbf{1.000} & - & - \\
%  & & 9+1 & 0.291 & 0.098 & 0.298 & 0.495 & 1.0 & - \\
%  & & 19+1 & 0.299 & 0.050 & 0.155 & 0.256 & 0.429 & 1.0 \\
%  & full day & 4+1 & 0.413 & 0.187 & 0.597 & \textbf{1.000} & - & - \\
%  & & 9+1 & 0.291 & 0.098 & 0.298 & 0.495 & 1.0 & - \\
%  & & 19+1 & 0.299 & 0.050 & 0.155 & 0.256 & 0.429 & 1.0 \\
% \bottomrule
% \end{tabular}
% \caption{emoticon Retrieval Results for MOD and SRS (English and Chinese)}
% \label{tab:experiments-language}
% \end{table*}

\begin{table}[htbp]
\centering
\caption{emoticon Retrieval Results for MOD and SRS (English and Chinese). We shortform Negative Samples for NS.}
\begin{tabular}{lclccccc}
\toprule
Lang & NS & Method & MRR & R@1 & R@3 & R@5 & R@10 \\ \midrule
% \textbf{MOD} &&&&&&&& \\ \hline
% \textit{English}  & 4+1 & \textbf{0.450} & 0.194 & \textbf{0.585} & \textbf{1.000} & & \\
 % & 9+1 & 0.293 & 0.095 & 0.314 & 0.510 & \textbf{1.000} & \\
 % & 19+1 & 0.178 & 0.051 & 0.144 & 0.238 & 0.494 & \textbf{1.000} \\
 % & 4+1 & \textbf{0.459} & 0.199 & \textbf{0.607} & \textbf{1.000} & - & - \\
 \textit{En} & 1:9 & MOD  & 0.289 & 0.307 & 0.495 & 0.224 & 1.000  \\
  & & SRS  & 0.291 & 0.098 & 0.298 & 0.495 & 1.000 \\  \cmidrule(lr){2-8} 
 &  1:19 & MOD & 0.180 & 0.051 & 0.145 & 0.250 & 0.511 \\ 
 &  & SRS & 0.299  & 0.050 & 0.155 & 0.256 & 0.429 \\ \hline
 
 % \textbf{MOD}& \textit{English} & 9+1 & 0.289 & \textbf{0.307} & 0.495 & 0.224 & \textbf{1.000}  \\
 % & & 19+1 & 0.180 & 0.051 & 0.145 & 0.250 & 0.511 \\ \hline
 
% \textit{Chinese} & 4+1 & \textbf{0.435} & 0.156 & \textbf{0.625} & \textbf{1.000} & - & - \\
 % & 9+1 & 0.255 & 0.063 & 0.313 & 0.500 & 1.000 & - \\
 % & 19+1 & 0.224 & 0.063 & 0.281 & 0.344 & 0.563 & 1.000 \\
 % &  4+1 & 0.362 & 0.063 & 0.593 & \textbf{1.000} & - & - \\
\textit{Zh}  &  1:9 & MOD  & 0.317 & 0.125 & 0.344 & 0.531 & 1.000  \\
& & SRS   & 0.264 & 0.056 & 0.281 & 0.500 & 1.000 \\ \cmidrule(lr){2-8} 
 &  1:19 & MOD & 0.156 & 0.063 & 0.094 & 0.156 & 0.406  \\ 
 &  & SRS & 0.270 & 0.061 & 0.138 & 0.219 & 0.377  \\

% \textbf{SRS} &&&&&&& \\ \hline
% \textit{English}  & 4+1 & 0.413 & 0.187 & 0.597 & \textbf{1.000} & - & - \\
% \textit{English}  & 9+1 & 0.291 & 0.098 & 0.298 & 0.495 & 1.0 & - \\
%  & 19+1 & 0.299 & 0.050 & 0.155 & 0.256 & 0.429 & 1.0 \\
%  &  4+1 & 0.413 & 0.187 & 0.597 & \textbf{1.000} & - & - \\

 % \textbf{SRS} & \textit{English} & 9+1 & 0.291 & 0.098 & 0.298 & 0.495 & 1.0 \\
 % & & 19+1 & 0.299 & 0.050 & 0.155 & 0.256 & 0.429 \\ \hline
 
% \textit{Chinese} &  4+1 & 0.447 & 0.214 & 0.582 & \textbf{1.000} & & \\
 % \textit{Chinese}& 9+1 & 0.264 & 0.056 & 0.281 & 0.500 & \textbf{0.891} & \\
 % & 19+1 & 0.270 & 0.061 & 0.138 & 0.219 & 0.377 & \textbf{0.891} \\
 % & 4+1 & 0.447 & 0.214 & 0.582 & \textbf{1.000} & - & - \\
 % & \textbf{SRS}  & 9+1 & 0.264 & 0.056 & 0.281 & 0.500 & \textbf{1.000} \\
 % & & 19+1 & 0.270 & 0.061 & 0.138 & 0.219 & 0.377  \\
\bottomrule
\end{tabular}
\label{tab:experiments-language}
\end{table}


% - English and Chinese only (MOD, SRS)
% - Full (SRS only)
% - domain (MOD, SRS)
% \subsection{User Behavior Modeling} % put to 4.3 user analysis

% \subsubsection{User Across the Same Categories}
% We showcase the styles of a user across different categories.

\subsubsection{Domain}
Table \ref{tab:domain-negative-samples} reveal that the \textbf{emoticons} domain performs the best overall, with an MRR of 0.335, R@1 of 0.143, R@3 of 0.365, and R@5 of 0.552. On the other hand Arts domain has a relatively lower performance, possibly due to the higher sparsity (0.019143).
\begin{table}[h]
    \centering
    \caption{Results of SRS on individual domains.}
    \begin{tabular}{lllll}
    \toprule
    \textbf{Domain} & \textbf{MRR} & \textbf{R@1} & \textbf{R@3} & \textbf{R@5} \\
    \hline
    
     \textbf{Anime} 
    % & 4+1 & 0.424 & 0.2 & 0.6 & 1 & - & - \\
     & 0.294 & 0.101 & 0.298 & 0.500 \\
    % & 19+1 & - & - & - & - & - & - \\
    
    \textbf{Arts} 
    % & 4+1 & 0.468 & 0.281 & 0.656 & 1 & 0 & 0 \\
   & 0.288 & 0.094 & 0.281 & 0.484 \\
    % & 19+1 & 0.336 & 0.031 & 0.188 & 0.266 & 0.507 & 1 \\

    \textbf{Fan Club}
    % & 4+1 & 0.417 & 0.18 & 0.603 & 1 & 0 & 0 \\
     & 0.280 & 0.080 & 0.280 & 0.510 \\
    % & 19+1 & 0.318 & 0.128 & 0.163 & 0.268 & 0.326 & 1 \\

    \textbf{Finance} 
    % & 4+1 & 0.426 & 0.2 & 0.605 & 1 & 0 & 0 \\
    & 0.296 & 0.080 & 0.280 & 0.510 \\
    % & 19+1 & - & - & - & - & - & - \\
    
    \textbf{Game} 
    % & 4+1 & 0.428 & 0.203 & 0.606 & 1 & 0 & 0 \\
     & 0.293 & 0.100 & 0.301 & 0.498  \\
    % & 19+1 & - & - & - & - & - & - \\

     \textbf{Language} 
    % & 4+1 & 0.425 & 0.201 & 0.598 & 1 & 0 & 0 \\
     & 0.296 & 0.103 & 0.307 & 0.501 \\
     
    \textbf{Outdoor} 
    % & 4+1 & 0.434 & 0.195 & 0.589 & 1 & 0 & 0 \\
    & 0.298 & 0.103 & 0.324 & 0.500 \\
    % & 19+1 & - & - & - & - & - & - \\
     
     \textbf{Social}
    % & 4+1 & 0.42 & 0.2 & 0.587 & 1 & 0 & 0 \\
    & 0.300 & 0.105 & 0.305 & 0.513 \\
    % & 19+1 & - & - & - & - & - & - \\
    
    \textbf{Media Sharing} 
    % & 4+1 & 0.462 & 0.242 & 0.636 & 1 & 0 & 0 \\
    &  0.335 & 0.143 & 0.365 & 0.552 \\
    % & 19+1 & - & - & - & - & - & - \\
    \textbf{Tech} 
    % & 4+1 & 0.409 & 0.153 & 0.569 & 1 & 0 & 0 \\
    & 0.321 & 0.125 & 0.361 & 0.611  \\
    % & 19+1 & 0.287 & 0.083 & 0.125 & 0.278 & 0.384 & 1 \\
   
    \bottomrule
    \end{tabular}
    \label{tab:domain-negative-samples}
\end{table}


\subsubsection{Complete Dataset}
The results in Table \ref{tab:srs-full} show that performance decreases as the number of negative samples increases. With 4 negative samples, the model achieves a \textbf{MRR} of 0.425 and \textbf{R@1} of 0.202, indicating good performance. As the number of negative sample increase, a decline in accuracy is observed, which is consistent with general recommendation models. 

\begin{table}[h!]
\centering
\caption{Performance metrics for varying negative samples across multiple evaluation criteria on the entire dataset using SRS method.}
\begin{tabular}{ccccccc}
\hline
\multirow{2}{*}{{NS}} & \multicolumn{6}{c}{Evaluation Metrics} \\
\cline{2-7}
& MRR  & R@1  & R@3  & R@5  & R@10 & R@20 \\
\hline
1:4  & 0.425 & 0.202 & 0.603 & 1.0  & -    & - \\
1:9  & 0.294 & 0.102 & 0.303 & 0.499 & 1.0  & - \\
1:19 & 0.296 & 0.054 & 0.153 & 0.251 & 0.332 & 1.0 \\
\hline
\end{tabular}
\label{tab:srs-full}
\end{table}

\section{Discussions}

\subsection{Other Potential Applications}
Beyond user behavior analysis and emoticon recommendation, the dataset presents several opportunities for further research. One potential avenue is a more extensive \textbf{User Behavior Modeling}, which could involve analyzing temporal changes in user behavior and investigating whether emoticon usage varies based on the responder. Additional potential applications include, but are not limited to, \textbf{Personalized Emoticon Retrieval} and \textbf{Generative AI}, both of which have previously faced dataset limitations \cite{pmg, pigeon}.

We acknowledge that there are several limitations in our work. Despite extensive automatic and manual data verification, privacy and safety concerns may still arise, presenting an opportunity for advancements in addressing such issues.

\subsection{Ethical Considerations}
We affirm that we have used the data in accordance with Telegram's Terms and Conditions \cite{telegram_tos}. Given a user's expression of opposition, we refrain from crawling to respect their privacy. Additionally, our dataset aligns with the FAIR principles, which emphasize making data \textit{Findable}, \textit{Accessible}, \textit{Interoperable}, and \textit{Reusable} \cite{fair_principles}.


\section{Conclusion}

% In this paper, we present emoticonU dataset, which is the largest emoticon dataset to date, containing 22.6k users 105.9k emoticons and 8.8M conversation messages of texts and emoticons.
% It is  a multi-domain dataset that includes rich and diverse information. Covering 10 domains, it captures temporal, multilingual, and cross-domain behaviors that are not present in previous datasets.
% Extensive quantitative and qualitative experiments demonstrate emoticonU’s practical applications on user behavior analysis and modeling, personalized emoticon recommendation, which potentially can be used in more downstream scenarios.

In this paper, we introduce our emoticon dataset, the largest emoticon dataset to date, comprising data from 22K users, 370.2K emoticons, and 8.3M conversation messages. This multi-domain dataset encompasses a wealth of diverse information, capturing temporal, multilingual, and cross-domain behaviors not found in previous datasets. 
Extensive quantitative and qualitative experiments demonstrate the practical applications of our dataset in user behavior modeling and personalized emoticon recommendation. It also holds potential for further research in areas such as personalized retrieval and conversational studies.