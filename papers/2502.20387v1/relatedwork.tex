\section{Related Work}
\vspace{-1mm}

\noindent\textbf{3D Talking Head Synthesis.}
Talking head synthesis aims to use arbitrary audio to reenact a talking person to generate audio-visual synchronized videos. Early works \cite{prajwal2020wav2lip, ezzat2002trainable, jamaludin2019you, chen2019hierarchical, wiles2018x2face, zhou2021pcavs, wang2020mead} are mainly built on generative models, creating talking heads by manipulating the given 2D images. Later, to solve the temporal inconsistency when the head moves, 3D-based methods \cite{thies2020nvp, yi2020audio, zhang2021facial, lu2021lsp} utilize explicit 3D face structure and successfully improve the naturalness. 

Recently, radiance fields like NeRF~\cite{mildenhall2021nerf} and 3D Gaussian Splatting (3DGS) \cite{kerbl2023gaussian} have been introduced as the representation to allow 3D talking head synthesize. Inheriting the radiance field optimization, most methods \cite{guo2021ad, liu2022semantic, tang2022rad, li2023efficient, peng2023synctalk, li2024talkinggaussian, cho2024gaussiantalker} train a person-specific model on minutes of high-quality video, achieving success in reconstructing photorealistic rendering and personalized talking style. However, the strict quality requirements of the training video data and the long training time for every adaptation to a new identity have hugely limited their application.
Although some works \cite{shen2022dfrf, ye2023geneface, li2023hide, yu2024gaussiantalker, ye2024real3d, ye2024mimictalk} attempt to solve the problem by a one-shot generator \cite{li2023hide, ye2024real3d, ye2024mimictalk} or take pre-trained motions from external modules to loosely join the adaptation \cite{shen2022dfrf, ye2023geneface} to achieve one- or few-shot, they often compromise image quality and loses the personalization as the trade-off. A concurrent work MimicTalk \cite{ye2024mimictalk} injects LoRA \cite{hu2021lora} to improve fine-tuning efficiency, but its inference is slow due to the large backbone.
Instead, by enabling the pretraining of person-specific models and using pre-trained motion field to directly drive the talking head, our method obtains a more compact and consistent architecture to fully exploit the motion priors in an end-to-end way, facilitating modeling precise lip-synchronization with high efficiency. 

\noindent\textbf{Few-shot 3D Head Reconstruction.}
Fitting high-fidelity 3D human heads from RGB images has been a hot topic. Early model-based methods \cite{yu2018headfusion, thies2016face2face, yang2020facescape, gecer2019ganfit} focus on fitting a 3D head with 3DMMs \cite{paysan2009bfm, FLAME:SiggraphAsia2017, booth20173d} but are short in detail. Combined with neural fields, some model-free methods \cite{buhler2023preface, ramon2021h3d, mihajlovic2022keypointnerf} learn static heads from a few images with structure priors. Although many works  \cite{giebenhain2024mononphm, gafni2021dynamic, song2024tri, xu2023avatarmav, yang2024have, hong2022headnerf, cao2022authentic} attempt to reconstruct dynamic heads from few videos, their parametric controls are difficult to predict from audio and limit the mouth motion granularity, making it hard for them to express the highly personalized 3D talking head with delicate audio-driven mouth motions achieved by InsTaG. 

Some generative methods  \cite{zakharov2019few, gan2023eat, xu2024vasa, gururani2023space, li2023hide, ye2024real3d} focus on reconstructing talking heads with 3D structure, however, they can hardly infer personalized talking style from only one image, leading to weak personalization and fidelity. Learning 3D talking heads with few-shot NeRF, recent works \cite{shen2022dfrf, ye2024mimictalk, li2024ae} introduce 2D-to-3D modules with pre-training but are often slow due to their overhead. 
Instead, with an identity-free motion prior and person-specific adaptation instead of expensive 2D-to-3D priors, our InsTaG can rapidly learn a personalized audio-driven 3D talking head from few data while attaining fast inference.

\vspace{-1mm}