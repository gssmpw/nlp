\begin{figure*}[t]
    \small\centering
    % \vspace{-5pt}
    \includegraphics[width=1.0\textwidth]{figure_files/method_v5.pdf}
    % \vspace{-5pt}
    \caption{
    \textbf{Overview}. 
    (a) We generate $N$ images using pretrained T2I diffusion model using the prompt dataset, and infer the scores from reward models. 
    (b) Then, we calibrate the rewards by making pairwise comparison between images. For each image, we compute the win-rates between other $N-1$ images using Eq.~\eqref{eq:bt}, and average them to obtain calibrated reward $R_{\textrm{ca}}$ (see Sec.~\ref{sec:capo}).
    (c) We select pair by choosing the best-of-$N$ and worst-of-$N$ when using single reward. For multi-reward, we use non-dominated sorting algorithm to select upper Pareto set as positives, and lower Pareto set as negatives. The accepted and rejected pairs are also listed using proposed rejection sampling method.
    (d) Lastly, during training, we select a pair from (c), and compute CaPO loss (\emph{i.e.}, Eq.~\eqref{eq:capoloss}), which perform regression task to match the difference in calibrated rewards (\emph{i.e.}, $\Delta R_{\textrm{ca}}$ by the difference of implicit reward model (\emph{i.e.}, $\Delta R_\theta$).
    % perform regression on the difference between implicit reward of a model for  the margin between calibrated rewards by the margin between model's (\emph{i.e.}, Eq.~\eqref{eq:dffpo}) by regression loss. \yl{this sentence seems not right. Between what and what?} (see Sec.~\ref{sec:capo}).
    }
    \label{fig:overview}
    \vspace{-10pt}
\end{figure*}