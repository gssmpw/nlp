\begin{figure}[t]
\begin{algorithm}[H]
\caption{Calibrated reward optimization}
\label{alg:dm}
\begin{algorithmic}[1]
\REQUIRE Prompt dataset $\mathcal{D}$, pretrained model $\eps_{\textrm{ref}}$, fine-tuning model $\eps_\theta$, learning rate $\eta>0$, reward models $\{r^{(j)}\}_{j=1}^L$
\STATE Sample $\{\mathbf{x}_i\}_{i=1}^N$ from $p_{\textrm{ref}}(\mathbf{x}|\mathbf{c})$ for $\mathbf{c}\sim\mathcal{D}$
\STATE Compute $L$ reward scores $r(\mathbf{x}_i, \mathbf{c})$ for each $\mathbf{x}_i, \mathbf{c}$
\STATE Compute $r_{\textrm{c}}(\mathbf{x}_i, \mathbf{c})$ from Eq.~\eqref{eq:calre}
\STATE Choose positive set: $X^+(\mathbf{c}) = \{\mathbf{x}^+\}$ by 
\STATE Choose negative set: $X^-(\mathbf{c}) = \{\mathbf{x}^-\}$
\WHILE{not converged}  
\STATE Sample $\mathbf{x}^+\sim X^+(\mathbf{c})$, $\mathbf{x}^-\sim X^-(\mathbf{c})$
\STATE Sample $\eps^+\sim\noise, \eps^-\sim\noise, t\sim\mathcal{U}(0,1)$, compute $w_t = \sigma(-\lambda_t + b)$
\STATE $\mathbf{z}_t^+ \leftarrow \alpha_t\mathbf{x}^+ + \sigma_t\eps^+$, $\mathbf{z}_t^- \leftarrow\alpha_t\mathbf{x}^- + \sigma_t\eps^-$
\STATE Compute $\ell_{\textrm{CaPO}}(\theta)$ using Eq.~\eqref{eq:cro}
\STATE Update $\theta \leftarrow \theta - \eta \nabla_\theta \ell_{\textrm{CaPO}}(\theta)$
\ENDWHILE
\end{algorithmic}
\end{algorithm}
\vspace{-15pt}
\end{figure}