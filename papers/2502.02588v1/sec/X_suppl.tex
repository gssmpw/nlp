\clearpage
% \setcounter{page}{1}
% \maketitlesupplementary

\onecolumn
\appendix
\begin{center}
    {\bf {\Large Calibrated Multi-Preference Optimization for Aligning Diffusion Models}} \\ 
    \vspace{3mm}
    { \Large Supplementary Materials}
    
\end{center}



\section{Additional description}
In this section, we provide additional details to Sec. 3 and Sec. 4 of the main manuscript. Specifically, we review the preliminaries on diffusion models and flow-based models (Sec.~\ref{sec:bg}), preference optimization for diffusion models (Sec.~\ref{sec:diffusionpo}), and provide details on loss weighting scheme (Sec.~\ref{sec:lossweighting}).

\subsection{Background on diffusion and flow-based models}\label{sec:bg}

\vspace{0.05in}
\noindent
{\bf Diffusion models.}
Let $q(\mathbf{x})$ be the density of data distribution of a sample $\mathbf{x}$ and $p_\theta(\mathbf{x})$ be a generative model parameterized by $\theta$ that approximates $q$. 
Given $\mathbf{x}\sim q(\mathbf{x})$, the diffusion model considers a series of latent variables $\mathbf{x}_t$ at time $t\in[0,1]$. 
Specifically, the forward process forms a conditional distribution $q(\mathbf{x}_t|\mathbf{x})$, where the marginal distribution is given by 
\begin{equation}
\mathbf{x}_t = \alpha_t \mathbf{x} + \sigma_t \boldsymbol{\epsilon}\text{,}
\end{equation}
where $\boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{0}, \mathbf{I})$, and $\alpha_t, \sigma_t$ are noise scheduling functions such that satisfies $\alpha_0 \approx 1$, $\alpha_1\approx 0$, and $\sigma_0 \approx 0$, $\sigma_1 \approx 1$.
Let us denote $\lambda_t = \log (\alpha_t^2 / \sigma_t^2)$ log signal-to-noise ratio (log-SNR), then $\lambda_t$ is a decreasing function of $t$.
Here, $\alpha_t$ and $\sigma_t$ (or equivalently $\lambda_t$) is chosen to satisfy that $\mathbf{x}_1$ is indiscernible from Gaussian noise (\emph{i.e.}, $p(\mathbf{x}_1)\approx \mathcal{N}(\boldsymbol{0}, \mathbf{I}))$, and conversely, $\mathbf{x}_0$ matches the data density $q(\mathbf{x})$. 
Then the reverse generative process gradually denoises the random Gaussian noise $\mathbf{x}_1\sim\mathcal{N}(\boldsymbol{0},\mathbf{I})$ to recover $\mathbf{x}_0$.
Specifically, the sampling process is governed by solving time-discretized SDE~\citep{song2020score, ho2020denoising} or probability flow ODE~\citep{song2020denoising, karras2022elucidating}, by using the score function $\nabla \log q(\mathbf{x}_t)$.
Training diffusion model then optimizes the neural network to approximate the score function by $\mathbf{s}_\theta(\mathbf{x}_t;t)$.
Especially, using the noise-prediction model~\citep{ho2020denoising} is a common practice, where the training objective can be written as following weighted loss objective~\citep{kingma2023understanding}:
\begin{equation}\label{eq:epsloss}
    \mathcal{L}_{\textrm{DM}}(\theta;\mathbf{x}) =\mathbb{E}_{t\sim \mathcal{U}(0,1), \boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{0},\mathbf{I})}\big[-\tfrac{1}{2}w_t\lambda_t'\|\boldsymbol{\epsilon}_\theta(\mathbf{x}_t;t) - \boldsymbol{\epsilon}\|_2^2\big]\text{,}
\end{equation}
where $w_t$ is a weighting function and $\lambda_t'$ is a time-derivative of $\lambda_t$.
Note that when $w_t=1$ for all $t\in(0,1)$, it becomes the variational lower bound (vlb) of KL divergence~\citep{kingma2021variational}, and the original DDPM uses $w_t\lambda_t'=-1$. 

\vspace{0.05in}
\noindent
{\bf Flow models.}
Alternatively, flow-based models or stochastic interpolants~\citep{lipman2022flow, albergo2023stochastic, ma2024sit} consider approximating the velocity field $\boldsymbol{v}(\mathbf{x}_t,t)$ on $\mathbf{x}$ at time $t\in(0,1)$, and solve following probability flow ODE to transport noise to data distribution:
\begin{equation}
    \mathbf{x}_t' = \boldsymbol{v}(\mathbf{x}_t, t)\text{,}
\end{equation}
where the marginal distribution of the solution of ODE matches the distribution $q_t(\mathbf{x}_t)$.
Given $\mathbf{x}_t=\alpha_t\mathbf{x}+\sigma_t\boldsymbol{\epsilon}$ for some $t\in(0,1)$ and $\boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{0}, \mathbf{I})$, the velocity field satisfies following:
\begin{equation}
    \boldsymbol{v}(\mathbf{x}_t, t) = \mathbb{E}[{\mathbf{x}}_t' \,|\, \mathbf{X}_t = \mathbf{x}_t] = \alpha_t'\, \mathbb{E}[\mathbf{x} \,|\, \mathbf{X}_t = \mathbf{x}_t] + \sigma_t'\,\mathbb{E}[\boldsymbol{\epsilon}\,|\, \mathbf{X}_t= \mathbf{x}_t]\text{,}
\end{equation}
and training objective for flow matching model is given as follows:
\begin{equation}\label{eq:vloss}
    \mathcal{L}_{\textrm{FM}}(\theta) = \mathbb{E}_{t\sim\mathcal{U}(0,1), \boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{0}, \mathbf{I})}\big[ \| \boldsymbol{v}_\theta(\mathbf{x}_t,t) - (\alpha_t' \mathbf{x} + \sigma_t'\boldsymbol{\epsilon})\|_2^2\big]\text{.}
\end{equation}
Note that Eq.~\eqref{eq:vloss} is a special case of Eq.~\eqref{eq:epsloss}, when $w_t=-\tfrac{1}{2}\lambda_t'\sigma_t^2$~\citep{kingma2023understanding, esser2024scaling}. 
In case of Rectified Flow~\citep{lipman2022flow}, we set $\alpha_t= 1-t$, $\sigma_t = t$, and $\lambda_t = 2 \log (\tfrac{1-t}{t})$, and the training objective of rectified flow model is given as follows:
\begin{equation}\label{eq:rfloss}
    \mathcal{L}_{\textrm{RF}}(\theta) = \mathbb{E}_{t, \eps}[\|\boldsymbol{v}_\theta(\mathbf{x}_t,t) - (\eps - \mathbf{x})\|_2^2]\text{.}
\end{equation}
For SD3-M~\citep{esser2024scaling}, we use Eq.~\eqref{eq:rfloss} to compute the loss.

%\newpage
\subsection{Diffusion preference optimization}\label{sec:diffusionpo}
For preference optimization with diffusion models, we consider following relaxation of original RLHF objective:
\begin{equation}\label{eq:diffobj}
    \max_\theta \bar{R}(\mathbf{x}_{0:1}, \mathbf{c}) - \beta D_{\textrm{KL}}\big(p_\theta(\mathbf{x}_{0:1}|\mathbf{c}) \,\|\, p_{\textrm{ref}}(\mathbf{x}_{0:1} | \mathbf{c})\big)\text{,}
\end{equation}
where $\bar{R}(\mathbf{x}_{0:1}, \mathbf{c})$ satisfies following:
\begin{equation}
    R(\mathbf{x}, \mathbf{c}) = \mathbb{E}_{q(\mathbf{x}_{0:1}|\mathbf{x})}
    \big[ \bar{R}(\mathbf{x}_{0:1}, \mathbf{c})\big] \text{.}
\end{equation}
Then by rearranging the equation derived from the closed solution of Eq.~\eqref{eq:diffobj}, we have following:
\begin{equation}\label{eq:diffreward}
    \bar{R}(\mathbf{x}_{0:1}, \mathbf{c}) = \beta \log\frac{p_\theta(\mathbf{x}_{0:1}|\mathbf{c})}{p_{\textrm{ref}}(\mathbf{x}_{0:1}|\mathbf{c})} - \beta \log Z(\mathbf{c})\text{,}
\end{equation}
where $Z(\mathbf{c})$ is a partition function. From Eq.~\eqref{eq:diffreward} and by rearranging $q(\mathbf{x}_{0:1} | \mathbf{x})$ in the inside term, we have
\begin{align}\label{eq:derivation1}
\begin{split}
    \mathbb{E}_{q(\mathbf{x}_{0:1}|\mathbf{x})}[\bar{R}(\mathbf{x}_{0:1},\mathbf{c})-\beta\log Z(\mathbf{c})] 
    &= \mathbb{E}_{q(\mathbf{x}_{0:1}|\mathbf{x})}\bigg[\beta\log\frac{p_\theta(\mathbf{x}_{0:1}|\mathbf{c}) }{q(\mathbf{x}_{0:1}|\mathbf{x})} - \beta\log \frac{p_{\textrm{ref}}(\mathbf{x}_{0:1}|\mathbf{c})}{q(\mathbf{x}_{0:1}|\mathbf{x})}\bigg] \\
    &= \beta \big(D_{\textrm{KL}}(q(\mathbf{x}_{0:1}|\mathbf{x}) \,\|\, p_{\textrm{ref}}(\mathbf{x}_{0:1}|\mathbf{c})) - D_{\textrm{KL}}(q(\mathbf{x}_{0:1}|\mathbf{x}) \,\|\, p_\theta(\mathbf{x}_{0:1}|\mathbf{c}))\big)\text{.}
\end{split}
\end{align}
Note that the KL divergence satisfies following (see \citep{kingma2023understanding} for details):
\begin{equation}\label{eq:diffeps}
    \frac{\mathrm{d}}{\mathrm{d}t}D_{\textrm{KL}}\big(q(\mathbf{x}_{t:1}|\mathbf{x}\,\|\, p_\theta(\mathbf{x}_{t:1}|\mathbf{c})\big) =  \frac{1}{2}\lambda_t'\mathbb{E}_{\boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{0}, \mathbf{I})}\big[ \|\boldsymbol{\epsilon}_\theta(\mathbf{x}_t;\mathbf{c},t) - \boldsymbol{\epsilon}\|_2^2 \big]\text{.} 
\end{equation}
By taking integration of Eq.~\eqref{eq:diffeps} over $t\in (1,0)$, one can rewrite $R(\mathbf{x}, \mathbf{c})$ as follows:
\begin{equation}
    R(\mathbf{x}, \mathbf{c}) = \frac{\beta}{2}\mathbb{E}_{t\sim\mathcal{U}(0,1), \boldsymbol{\epsilon}\sim\mathcal{N}(0,1)}\big[ \lambda_t'\big(\|\boldsymbol{\epsilon}_\theta(\mathbf{x}_t;\mathbf{c},t) - \boldsymbol{\epsilon}\|_2^2 - \|\boldsymbol{\epsilon}_\phi(\mathbf{x}_t;\mathbf{c},t) - \boldsymbol{\epsilon}\|_2^2 \big)\big]\text{,}    
\end{equation}
For a triplet $(\mathbf{c}, \mathbf{x}^+, \mathbf{x}^-)$, we consider following upper bound of a training objective for any convex function $g:\mathbb{R}\rightarrow\mathbb{R}$:
\begin{align*}
\begin{split}
    \bar{\ell}(\theta) &= g\big(R(\mathbf{x}^+, \mathbf{c}) - R(\mathbf{x}^-, \mathbf{c})\big) \\
    &= g\bigg( \frac{\beta}{2}\mathbb{E}_{t, \boldsymbol{\epsilon}^+, \boldsymbol{\epsilon}^-}\big[ \lambda_t'\big(\|\boldsymbol{\epsilon}_\theta(\mathbf{x}_t^+;\mathbf{c},t) - \boldsymbol{\epsilon}^+\|_2^2 - \|\boldsymbol{\epsilon}_{\textrm{ref}}(\mathbf{x}_t^+;\mathbf{c},t) - \boldsymbol{\epsilon}^+\|_2^2 - \|\boldsymbol{\epsilon}_\theta(\mathbf{x}_t^-;\mathbf{c},t) - \boldsymbol{\epsilon}^-\|_2^2 + \|\boldsymbol{\epsilon}_{\textrm{ref}}(\mathbf{x}_t^-;\mathbf{c},t) - \boldsymbol{\epsilon}^-\|_2^2 \big)\big] \bigg)\\
    &\leq \mathbb{E}_{t,\boldsymbol{\epsilon}^+, \boldsymbol{\epsilon}^-}\bigg[g\bigg(
    \tfrac{1}{2}\beta\lambda_t'\big(\|\boldsymbol{\epsilon}_\theta(\mathbf{x}_t^+;\mathbf{c},t) - \boldsymbol{\epsilon}^+\|_2^2 - \|\boldsymbol{\epsilon}_{\textrm{ref}}(\mathbf{x}_t^+;\mathbf{c},t) - \boldsymbol{\epsilon}^+\|_2^2 - \|\boldsymbol{\epsilon}_\theta(\mathbf{x}_t^-;\mathbf{c},t) - \boldsymbol{\epsilon}^-\|_2^2 + \|\boldsymbol{\epsilon}_{\textrm{ref}}(\mathbf{x}_t^-;\mathbf{c},t) - \boldsymbol{\epsilon}^-\|_2^2\big)\bigg)
    \bigg]\text{,}
\end{split}
\end{align*}
where $t\sim\mathcal{U}(0,1)$, $\boldsymbol{\epsilon}^+\sim\mathcal{N}(\boldsymbol{0}, \mathbf{I})$, $\boldsymbol{\epsilon}^- \sim \mathcal{N}(\boldsymbol{0}, \mathbf{I})$, and the last inequality comes from the Jensen's inequality.
% from the convexity of $g$.
Using the equation we defined in our main paper, \emph{i.e.},
\begin{equation}\label{eq:diffratio2}
    R_\theta(\mathbf{x}_t, \mathbf{c}, t)  \! = \!  \lambda_t'\big(\|\boldsymbol{\epsilon}_\theta(\mathbf{x}_t;\mathbf{c},t) - \boldsymbol{\epsilon}\|_2^2 - \|\boldsymbol{\epsilon}_{\textrm{ref}}(\mathbf{x}_t;\mathbf{c},t) -\boldsymbol{\epsilon}\|_2^2\big)\text{,}
\end{equation}
we derive following training objectives for DPO, IPO, and CaPO:
\begin{align}
\begin{split}
    \ell_{\textrm{DPO}}(\theta) &= \mathbb{E}_{t,\boldsymbol{\epsilon}^+, \boldsymbol{\epsilon}^-}\bigg[ -\log \sigma \big(\beta \big(R_\theta(\mathbf{x}^+, \mathbf{c}, t) - R_\theta(\mathbf{x}^-, \mathbf{c}, t )\big)\big)\bigg]\\
    \ell_{\textrm{IPO}}(\theta) &= \mathbb{E}_{t,\boldsymbol{\epsilon}^+, \boldsymbol{\epsilon}^-}\bigg[ \bigg(1 -  \beta\big(R_\theta(\mathbf{x}^+, \mathbf{c}, t) - R_\theta(\mathbf{x}^-, \mathbf{c}, t )\big)\bigg)^2\bigg]\\
    \ell_{\textrm{CaPO}}(\theta) &= \mathbb{E}_{t,\boldsymbol{\epsilon}^+, \boldsymbol{\epsilon}^-}\bigg[ \bigg(R(\mathbf{x}^+, \mathbf{c}) - R(\mathbf{x}^-, \mathbf{c}) -  \beta\big(R_\theta(\mathbf{x}^+, \mathbf{c}, t) - R_\theta(\mathbf{x}^-, \mathbf{c}, t )\big)\bigg)^2\bigg]\text{,}
\end{split}
\end{align}
where $R(\mathbf{x}, \mathbf{c})$ is a reward from the external reward model.


\vspace{0.05in}
\noindent
{\bf Independent noise sampling.}
Note that in original Diffusion-DPO paper~\citep{wallace2023diffusion}, the author proposed to use same noise for $\mathbf{x}^+$ and $\mathbf{x}^-$, \emph{i.e.},  $\boldsymbol{\epsilon}^+ = \boldsymbol{\epsilon}^-$, while we sample independent noise for $\eps^+$ and $\eps^-$. We believe this is more theoretically grounded, and empirically found that it has slightly better performance than using the same noise (even for DPO and IPO).
% We empirically observe that sampling independent noise gives us more theoretically grounded derivation, and gives us slightly better performance for DPO, IPO, and CaPO (see Appendix~ for details).

%\newpage
\input{tables/fig_weight}
\subsection{Loss weighting}\label{sec:lossweighting}
In practice, we multiply $w_t$ to the noise-prediction loss for diffusion preference optimization. One can consider this as setting timestep-wise different $\beta_t = \beta w_t$, \emph{i.e.}, giving different regularization hyperparameters at each time $t\in(0,1)$. Thus, we have 
\begin{equation}\label{eq:diffratioweight}
    R_\theta(\mathbf{x}_t, \mathbf{c}, t)  \! = \!  w_t\lambda_t'\big(\|\boldsymbol{\epsilon}_\theta(\mathbf{x}_t;\mathbf{c},t) - \boldsymbol{\epsilon}\|_2^2 - \|\boldsymbol{\epsilon}_{\textrm{ref}}(\mathbf{x}_t;\mathbf{c},t) -\boldsymbol{\epsilon}\|_2^2\big)\text{,}
\end{equation}
and applies to each DPO, IPO, and CaPO loss. 
As we mentioned in Sec.~4.4 in our main draft, we use sigmoid loss weighting~\citep{kingma2023understanding}, where the loss weights are sigmoid function of log-SNR $\lambda_t$ with bias $b$:
\begin{equation}
    w_t = w(\lambda_t) = \frac{1}{1+\exp(b - \lambda_t)}\text{.}
\end{equation}
Note that SDXL uses a modified DDPM schedule~\citep{ho2020denoising}, where $\beta = \big(\sqrt{\beta_0} + \tfrac{t}{T-1}(\sqrt{\beta_{T-1}} - \sqrt{\beta_0})\big)^2$, and $\alpha_t = (\prod_{s=0}^t (1-\beta_s))^{1/2}$.
Since it is impractical to compute $\lambda_t'$, we simply set it as constant (\emph{i.e.}, linear $\lambda_t$, which empirically holds when $\lambda_t \in [-15, 5]$, and for $\lambda_t > 0.5$ the weight $w_t$ is close to $0$, so one can ignore it).

SD3-M uses a rectified flow scheduler~\citep{lipman2022flow}, where $\lambda_t = 2 \log (\tfrac{1-t}{t})$. Note that we have 
\begin{equation}
\mathbb{E}_{t\sim\mathcal{U}(0,1),\eps}\big[\|\boldsymbol{v}_\theta(\mathbf{x}_t,t) - (\eps - \mathbf{x})\|_2^2\big] = \mathbb{E}_{\lambda\sim \mathcal{U}(\lambda_{\textrm{min}}, \lambda_{\textrm{max}}), \eps}\big[e^{-\lambda/2}\|\eps_\theta(\mathbf{x}_\lambda,\lambda) - \eps\|_2^2\big]\text{,}
\end{equation}
where $\mathbf{x}_\lambda$ denotes forward process of $\mathbf{x}$ with log-SNR value $\lambda$, and $\lambda_{\textrm{min}}$ and $\lambda_{\textrm{max}}$ denotes the minimal and maximal value for log-SNR (see \citep{kingma2023understanding} Appendix D.3 for details).
As such, multiplying $w = \sigma(-\lambda)$ to noise-prediction loss is equivalent to multiplying $(e^{\lambda/2} + e^{-\lambda / 2})^{-1}$ to flow matching objective:
\begin{equation}
    \sigma(-\lambda)\|\eps_\theta(\mathbf{x}_\lambda,\lambda) - \eps\|_2^2 = \sigma(-\lambda)\cdot \frac{e^{-\lambda / 2}}{e^{-\lambda / 2}}\|\eps_\theta(\mathbf{x}_\lambda,\lambda) - \eps\|_2^2 = \frac{1}{e^{\lambda / 2} + e^{-\lambda / 2}}\|\boldsymbol{v}_\theta(\mathbf{x}_\lambda, \lambda) - (\eps - \mathbf{x})\|_2^2\text{.} 
\end{equation}
If we shift with bias $b$, it becomes $w_\lambda = (e^{-(\lambda -b)/2} + e^{(\lambda -b)/2})^{-1}$.
In Fig.~\ref{fig:loss_weight}, we plot loss weighting functions for noise-prediction loss and flow matching loss with different bias $b\in\{-2, -1, 0, 1, 2\}$.
In practice, we select $\lambda \sim \mathcal{U}[-10, 10]$ and multiply $w_\lambda$ to flow matching loss.
Note that multiplying $w_\lambda$ has a similar effect in log-normal sampling proposed in \citep{karras2022elucidating, esser2024scaling}, where we empirically find similar performance. To ensure consistency with SDXL experiments, we use loss weighting instead of logit-normal sampling for SD3-M experiments.

%\newpage
\input{tables/fig_data_stats}
\section{Implementation Details}
\subsection{Dataset}\label{appendix:dataset}
\vspace{0.05in}
\noindent
{\bf Reward models.}
For reward models learned by fine-tuning CLIP models (\emph{e.g.}, Pickscore~\citep{kirstain2023pick}, MPS~\citep{zhang2024learning}), we compute the reward by the dot product between the image embedding and the text embedding. To compute MPS score, we additionally multiply the text embedding from condition textual description (\emph{e.g.}, textual description for aesthetic quality).
For VQAscore, we use CLIP-FlanT5-XXL~\citep{lin2025evaluating}, and compute the score by probability of {\tt{"Yes"}} token given the image and question provided to the model: 
\begin{equation}
    P\big({\texttt{"Yes"}} |\mathbf{x}, {\texttt{"Does this figure shows \{prompt\}? Please answer yes or no."}}\big)\text{,}
\end{equation}
where $\mathbf{x}$, {\tt{prompt}} are image and text input.
While VQAscore is not a Bradley-Terry model, we simply approximate the win-rate by following:
\begin{equation}
    \mathbb{P}(\mathbf{x} \succ \mathbf{x}' | \mathbf{c}) =\frac{ s(\mathbf{x}, \mathbf{c})^\alpha}{ s(\mathbf{x}, \mathbf{c})^\alpha + s(\mathbf{x}', \mathbf{c})^\alpha}\text{,}
\end{equation}
where $s(\mathbf{x}, \mathbf{c})$ is a VQAscore and $\alpha > 0$ is a hyperparameter to control the temperature. We find $\alpha=1$ works well in our experiments.
Lastly, VILA-R score~\citep{ke2023vila} outputs the aesthetic score between 1-10, and we apply the Bradley-Terry model to compute the win-rate.
In Fig.~\ref{fig:dataset_statistic}, we plot the histogram of reward scores and calibrated rewards of our training dataset.

\vspace{0.05in}
\noindent
{\bf Training dataset.}
We use 100K prompts from DiffusionDB~\citep{wang2022diffusiondb} and generate $N=16$ images per prompt.
For SDXL, we use DDIM~\citep{song2020denoising} scheduler, guidance scale of 7.5 and sampling steps of 50. 
For SD3-M, we use the DPM solver~\citep{lu2022dpm} for flow-based models, guidance scale of 5.0 and sampling steps of 50. 
Furthermore, as described in \citep{esser2024scaling}, we shift the timestep schedules to reside more on higher timesteps, \emph{i.e.}, we set $t \leftarrow \frac{ts}{1+ t(s-1)}$ with shift scale $s=3.0$.
 
% In Fig.xx, we demonstrate the examples of generated images and their scores. 
% Also, we plot the histograms of the reward scores and calibrated rewarIn Tab. xx and Fig.xx, we plot the statistics of reward scores and calibrated rewards of training dataset.

% \begin{itemize}
%     \item how to compute calibrated rewards for VILA, and VQAscore.
%     \item Show examples of a dataset, with scores
%     \item plot the histogram with score, and after win-rate
%     \item table the statistics / mean \& variance
% \end{itemize}

\subsection{Training and evaluation}
\vspace{0.05in}
\noindent
{\bf Training configuration.} Throughout experiments, we use Jax~\citep{frostig2018compiling} and train models using the Optax library on TPU chips.
For both SDXL and SD3-M experiments, we use Adam~\citep{kingma2014adam} optimizer.
Regarding training configuration for SDXL experiments, we use batch size of 256, learning rate of 1e-5 with linear warmup for first 1000 steps, and train for maximum 10000 steps. 
For SD3-M, we use batch size of 256, learning rate of 1.5e-5 with linear warmup for first 1000steps, and train for maximum 5000 steps.
We choose hyperparameter $\beta$ by sweeping over $\{300, 500, 1000\}$ for CaPO, $\{500, 1000, 2000\}$ for IPO, and $\{2000, 3000, 4000\}$ for DPO when training SDXL model. 
For SD3-M, we sweep over $\{30, 50, 100\}$ for CaPO, $\{50, 100, 200\}$ for IPO, and $\{100, 200, 300\}$ for DPO.
For all training, we use sigmoid loss weighting with $b=1.5$ for SDXL and $b=-1.0$ for SD3-M (including all DPO, IPO, and CaPO). 
During training, we generate images using subset of Parti prompts at each 1000-th iteration, and choose the final model with maximum validation win-rate (average of win-rates for multi-reward signals).


\input{tables/fig_single}

\vspace{0.05in}
\noindent
{\bf Model soup.}
For model merging experiments, we follow \citep{rame2024warp}. Specifically, suppose $\theta_0$ be weights of a pretrained model and $\theta_1$, $\theta_2$ be weights of fine-tuned models. Then the spherical linear interpolation (SLERP) between $\theta_1$ and $\theta_2$ is given by 
\begin{equation}
    \textrm{SLERP}(\theta_0, \theta_1, \theta_2, \lambda) = \theta_0 + \frac{\sin ((1-\lambda)\Omega)}{\sin (\Omega)} (\theta_1 - \theta_0) + \frac{\sin (\lambda \Omega)}{\sin (\Omega)}(\theta_2 - \theta_0)\text{,}
\end{equation}
where $\Omega$ is the angle between two task vectors $\theta_1 - \theta_0$ and $\theta_2 - \theta_0$, and $\lambda \in (0,1)$ is a coefficient. 
To merge three fine-tuned models, we first merge two models with $\lambda=0.5$ to obtain $\theta_{12} = \textrm{SLERP}(\theta_0, \theta_1, \theta_2, 0.5)$, then merge $\theta_{12}$ and $\theta_3$ with $\lambda = 1/3$ to obtain final model.

\vspace{0.05in}
\noindent
{\bf Evaluation.}
For evaluation, we generate images with the same configuration as in Sec.~\ref{appendix:dataset} for different benchmark prompt dataset.
For SDXL, we use Parti~\citep{yu2022scaling} prompts, and for SD3-M we use DPG-bench~\citep{hu2024ella}. 
Then we compute the win-rate against the base model by comparing one-by-one comparison for each image, \emph{e.g.}, if we have $K$ images from base model and $K$ images from fine-tuned model, we make $K^2$ comparison and count the number of win and divide by $K^2$.
We also report the average reward scores.
Remark that the average reward scores and win-rate could show different trends, as the model achieves a higher score gain for some prompts, but it fails to improve on others. Thus, we found win-rate is a more general metric to see the generalization over different prompts.

\vspace{0.05in}
\noindent
{\bf Benchmark evaluation.}
We use GenEval~\citep{ghosh2024geneval} and T2I-Compbench~\citep{huang2023t2i} to evaluate our models. 
For T2I-Compbench, we use BLIP-VQA model~\citep{li2022blip} to evaluate Color, Shape, Texture, Complex, and UniDet~\citep{zhou2022simple} for Spatial, and CLIP~\citep{radford2021learning} for Non Spatial. 
For baselines, we compare with the state-of-the-art open-source text-to-image diffusion models Flux-dev (12B)~\citep{flux2024},  Flux-schnell (12B)~\citep{flux2024}, and Stable Diffusion 3.5-Large (8B)~\citep{esser2024scaling}. 
Since those models are much larger than SDXL (2.6B) and SD3 (2B), we remark that this is not a fair comparison, yet we show the comparable performance of our method.
%\newpage


\section{Additional ablation study}\label{appendix:impl}

\vspace{0.05in}
\noindent
{\bf Effect of multi-reward.}
We demonstrate the effect of multi-reward CaPO compared to single-reward CaPO. As we demonstrated in Tab.~1 and Tab.~2 in our main draft, the single-reward model achieves the best score in which they have trained with, but the other metrics score below the multi-reward cases. We showcase the qualitative examples on the effect of multi-reward preference optimization compared to single-reward cases in Fig.~\ref{fig:multi_effect}. We notice that single-reward fine-tuning is often imperfect, \emph{e.g.}, fine-tuning with only VILA score loses image-text alignment, and fine-tuning with only VQAscore lacks image aesthetics. On the other hand, multi-reward fine-tuning complements those issues and improves the overall image quality.



\vspace{0.05in}
\noindent
{\bf Loss weighting for SD3-M.}
We show the effect of loss weighting when training SD3-M models. Similar to SDXL, we compare the results of CaPO multi-reward fine-tuning with different bias parameters. In Tab.~\ref{tab:abl_appendix}, we show that sigmoid weighting with bias $b=-1.0$ shows the best result, outperforming the constant weighting counterpart. Note that for SDXL, $b=1.5$ performs the best, while for SD3-M, negative bias $b=-1.0$ performs the best. Remark that as SD3-M performs diffusion modeling on $16\times 128 \times 128$, and SDXL performs on $4\times 128\times 128$, the bias shifts toward negative as the total variance becomes higher, and the log-SNR should be increased~\citep{hoogeboom2023simple, hoogeboom2024simpler}.
% , as the dimension of the input grows, the bias should


\vspace{0.2in}
\noindent
{\bf User study evaluation.}
We conduct additional user evaluations to compare our method with base models. For SDXL vs CaPO+SDXL, we randomly select 200 prompts from Parti prompt dataset~\citep{yu2022scaling}, and for SD3-M vs CaPO+SD3-M, we randomly select 200 prompts from GenAI bench prompt dataset~\citep{li2024genai}. Additionally, we compare CaPO+SDXL with Diffusion-DPO~\citep{wallace2023diffusion} again with 200 randomly selected prompts. We give following instructions to the raters:
\begin{itemize}[leftmargin=1cm]
    \item Instruction: Given the text below, pick the left or the right image with better looking.
    \item Good example: Images are beautiful and following text description.
    \item Bad example: Images are not looking good or not following text description.
\end{itemize}
We use Amazon mechanical Turk~\citep{amt} and 5 raters answered to each pair. 
In Tab.~\ref{tab:user}, we show the results of user study. 
We observe that CaPO+SD3-M and CaPO+SDXL consistently outperform SD3-M and SDXL, respectively. Also, CaPO+SDXL outperforms Diffusion-DPO, yet the margin is smaller than CaPO+SDXL vs SDXL.



\input{tables/tab_abl_sd3}
\input{tables/user}


\newpage
\begin{figure}[!htb]
    \small\centering
    \includegraphics[width=0.98\columnwidth]{figure_files/qual_appendix_sdxl_v2.pdf}
    \vspace{-10pt}
    \caption{
    \textbf{Additional qualitative comparison between CaPO SDXL and SDXL.}
    We provide additional qualitative comparison between CaPO SDXL and SDXL. The CaPO SDXL model demonstrates better image-text alignment (\emph{e.g.}, counting, attribute binding, etc), as well as image aesthetics (\emph{e.g.}, artistic style, detail, etc). We bold the text to highlight the prompts that demonstrate improvement in image-text alignment, and ({\bf Aesthetic $\uparrow$}) to demonstrate the improvement in image aesthetic quality.
    }
    \label{fig:qual_sdxl}
    \vspace{-20pt}
\end{figure}
\newpage
\begin{figure}[!htb]
    \small\centering
    \includegraphics[width=0.98\columnwidth]{figure_files/qual_appendix_sdxl_dpo_v3.pdf}
    \vspace{-10pt}
    \caption{
    \textbf{Additional qualitative comparison between CaPO SDXL and Diffusion-DPO~\citep{wallace2023diffusion}.}
    We provide additional qualitative comparison between CaPO SDXL and Diffusion-DPO~\citep{wallace2023diffusion}. 
    CaPO SDXL shows better image-text alignment and image aesthetics compared to Diffusion-DPO without using any human annotated data.
    We bold the text to highlight the prompts that demonstrate improvement in image-text alignment, and ({\bf Aesthetic $\uparrow$}) to demonstrate the improvement in image aesthetic quality.
    }
    \label{fig:qual_sdxl_dpo}
    \vspace{-20pt}
\end{figure}
\newpage
\begin{figure}[!htb]
    \small\centering
    \includegraphics[width=0.98\columnwidth]{figure_files/qual_appendix_v2.pdf}
    \vspace{-10pt}
    \caption{
    \textbf{Additional qualitative comparison between CaPO SD3-M and SD3-M.}
    We provide additional qualitative comparison between CaPO SD3-M and SD3-M. CaPO SD3-M shows better image-text alignment, \emph{e.g.}, negation (first row), counting (second row), visual text rendering (third row). Also it demonstrates better image aesthetics (fourth row right).
    We bold the text to highlight the prompts that demonstrate improvement in image-text alignment, and ({\bf Aesthetic $\uparrow$}) to demonstrate the improvement in image aesthetic quality.
    }
    \label{fig:qual_sd3m}
    \vspace{-20pt}
\end{figure}



\vspace{10pt}
% \subsection{Additional qualitative results for SDXL}

% SD3 results
% SDXL results
% Effect of multi-reward (i.e., single vs multi)
