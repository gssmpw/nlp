% \input{tables/table1}

\input{tables/tab_single}
\input{tables/tab_multi}
% \input{tables/t2icompbench}
\section{Experiments}
\label{sec:experiment}
\input{tables/fig_comp}


\input{tables/geneval}

\vspace{0.05in}
\noindent
{\bf Models and datasets.}
We use Stable Diffusion XL (SDXL) \citep{podell2023sdxl} and Stable Diffusion 3 medium (SD3-M) \citep{esser2024scaling} as our base text-to-image diffusion models in all experiments. 
To collect the training dataset, we use 100K prompts from DiffusionDB~\citep{wang2022diffusiondb}, and generate $N=16$ images per prompt.
We also experiment with using 8, or 32 images per prompt, and select 16 images per prompt, which provides a good trade-off between computational cost and performance improvement.
To generate images with SDXL, we use DDIM \citep{song2020denoising} sampler with guidance scale 7.5 for 50 steps, and for SD3-M we use flow DPM-solver~\citep{lu2022dpm} with guidance scale 5.0 for 50 steps.
%MH: check the following sentence... still not clear... model... model...
%Remark that we only use the images generated by the model itself to train the model.
Note that we only use the images generated by the same diffusion model for experiments. We refer to supplementary for detailed experimental setup.

\vspace{1mm}
\noindent
{\bf Reward models.}
We consider three reward models that cover diverse aspects of the T2I generation. 
For general human preference (\emph{i.e.}, overall quality), we use MPS score~\citep{zhang2024learning}, which is a state-of-the-art reward model for human preference.
For image-text alignment, we use VQAscore~\citep{lin2025evaluating}, which uses a vision-language model (CLIP-FlanT5-XXL) to compute scores by performing visual question answering tasks. 
Specifically, VQAscore measures the probability $P(\texttt{Yes}|\mathbf{x}, \texttt{Q}(\mathbf{c}))$ by using the output logits of the model, where $\texttt{Q}$ is a template for the question. 
We also use VILA score~\citep{ke2023vila} pretrained on AVA~\cite{murray2012ava} dataset to evaluate image aesthetics.
While VQAscore and VILA score are not trained with BT model, we adjust to approximate with BT model (see supplementary for details).

\subsection{Single reward experiments}
\label{sec:single_exp}
\vspace{1mm}
\noindent
{\bf Experimental setups.} We evaluate CaPO against state-of-the-art preference learning objectives such as DPO~\citep{wallace2023diffusion} and IPO~\citep{azar2024general} for diffusion models.
For each method, we train with three reward models (MPS, VQAscore, and VILA) by selecting the top--1 and worst--1 pair.
For evaluation, we use Parti prompts~\citep{yu2022scaling} to generate images for SDXL fine-tuned model and DPG-Bench~\citep{hu2024ella} prompts to generate images for SD3-M fine-tuned model.
We report the win-rate against the base model using each reward model.
% compute the win-rate over the base model with reward models.

\vspace{0.05in}
\noindent
{\bf Results.} Tab.~\ref{tab:single} shows that CaPO achieves the highest win-rate for each reward model used for fine-tuning, as well as other reward models. 
Especially, when using the VILA model for training, DPO shows significant drop in VQAscore, while showing comparable performance with CaPO in VILA score.
On the other hand, IPO shows better robustness than DPO in reward hacking, but the gain of the performance is lower than DPO and CaPO in general. 
We notice that even though we optimized for a reward, other rewards also increase at some cases. 
This is partially due to the inherent correlation residing in reward models, \emph{e.g.}, increasing MPS score results in increase in VILA score, as image aesthetics is an important factor in overall quality. 
% as they both consider image aesthetics.
% is trained with overall quality, it correlates with  , due to inherent correlation resides in the black-box reward models.  
% In Fig.~\ref{fig:single}, we provide some qualitative examples on fine-tuning with each objective. We observe similar trends as we see in Tab.~\ref{tab:single}.


% \paragraph{Setup.} We demonstrate the effectiveness of CaPO compared to various preference learning objectives, \emph{e.g.}, DPO~\citep{wallace2023diffusion}, conservative DPO (cDPO), and Diffusion-IPO~\citep{azar2024general}.
% We use same experimental setup in Sec.~\ref{sec:exp_main}, but use only single reward of each MPS, VQAscore, and VILA. Note that for DPO and IPO, we only use reward signals to select the pair, and for cDPO, the model considers only pairwise soft preference label as a target for training. For evaluation, we use Parti prompts~\citep{yu2022scaling} to generate images and measure the scores for each MPS, VQAscore, and VILA, then we compute the win-rate over base model with each reward signal.



\subsection{Multi-reward experiments}
\label{sec:exp_main}
\vspace{0.05in}
\noindent
{\bf Experimental setups.}
We consider MPS, VQAscore, and VILA scores for multi-reward experiments. 
For baselines, we evaluate CaPO against DPO and IPO as in Sec.~\ref{sec:single_exp}. 
Furthermore, we conduct experiments on different methods in adapting for multiple rewards. 
Specifically, we compare frontier-based rejection sampling (FRS) (\emph{i.e.}, Sec.~\ref{sec:frs}) with sum-of-rewards (Sum), and merging the models fine-tuned with single reward (\emph{i.e.}, model soup~\citep{wortsman2022model}). 
For sum-of-rewards, we directly add the calibrated rewards, and perform top-1 and worst-1 pair selection for training data. 
For model soup, we re-use fine-tuned models from Sec.~\ref{sec:single_exp}, and use spherical linear interpolation~\citep{shoemake1985animating, rame2024warp} to merge models, which performs slightly better than linear interpolation in our experiments. 
We use uniform weights (\emph{i.e.}, 1/3 each) for both sum-of-rewards and model soup.
For evaluation, we generate images using Parti prompt dataset~\citep{yu2022scaling} and DPG-bench prompt dataset for SDXL and SD3-M, respectively. 
For evaluation, we report the average reward scores, and the win-rate against the base model by using each reward model.
% then measure the win-rate over images generated from the base model by using each reward model and average scores.

\vspace{1mm}
\noindent
{\bf Quantitative results.}
% \paragraph{Effect of frontier-based rejection sampling.}
% For multi-reward optimization, we compare our frontier-based rejection sampling (FRS) with rejection sampling with sum of rewards, as well as rewarded soup~\citep{rame2024rewarded}. 
% For sum of rewards, we compute the calibrated win-rate and average three values to select the top and worst pair. 
% For rewarded soup, we re-use the model fine-tuned with single reward from Sec.~\ref{sec:single_exp} and use spherical linear interpolation (SLERP)~\citep{shoemake1985animating, rame2024warp}, which showed better performance than linear interpolation in our experiments.
% Following previous section, we compare CaPO with DPO and IPO for each experiments.
Tab.~\ref{tab:multi} shows the results. 
First, joint training of multiple rewards by using frontier-based rejection sampling consistently outperforms pair selection with sum of rewards on all preference optimization objectives.
While model merging (Soup) shows comparable performance to FRS when using DPO and IPO objectives for training, using CaPO objective with FRS outperforms model soup of CaPO fine-tuned models with single reward.
When comparing CaPO, DPO, and IPO, CaPO with FRS shows higher win-rates and average rewards compared to DPO and IPO with FRS, which is consistent with the results of Tab.~\ref{tab:single}.


\vspace{1mm}
\noindent
{\bf Qualitative results.}
%MH: fix these
In Fig.~\ref{fig:qual_comparison}, we provide qualitative comparison of our method on SDXL and SD3-M, compared to DPO trained with multi-reward frontier-based rejection sampling. 
While both DPO and CaPO shows improved image aesthetics such as contrast or color compared to base SDXL, we see that CaPO demonstrates better image-text alignment and aesthetic quality compared to DPO, following the quantitative results in Tab.~\ref{tab:multi}. We refer to supplementary for additional examples.
% We see that CaPO shows better image aesthetics and image-text alignment compared to DPO.
% While DPO also shows improvement in some image quality, we observe the lack of improvement in image-text alignment compared to 

% results of our method on SD3-M and SDXL. 
% First, we observe the improvement in intricate prompt alignment such as numerical counting, and text rendering. Furthermore, the generated images show improved visual aesthetics, such as higher contrast and better color attribution.




\vspace{1mm} 
\noindent
{\bf Benchmark results.}
For quantitative analysis, we evaluate our models on various T2I benchmarks; GenEval~\citep{ghosh2024geneval} which evaluates object-focused generation, and T2I-Compbench~\citep{huang2023t2i} for compositional generation.
We compare our method with the base models SDXL and SD3-M, as well as open-source state-of-the-art T2I diffusion models such as FLUX-dev~\citep{flux2024}, FLUX-schnell~\citep{flux2024}, and Stable Diffusion 3.5-large (SD3.5-L)~\citep{esser2024scaling}.
Tab.~\ref{tab:geneval} shows that CaPO improves the performance of the base model, \emph{e.g.}, 0.55$\rightarrow$0.59 for SDXL, 0.68$\rightarrow$0.71 on GenEval overall score, and on almost every metrics in T2I-Compbench. 
% Furthermore, we note that CaPO-SD3-M achieves comparable performance to SD3.5-L. 
%More details are in the supplementary.
% especially on colors


% For GenEval benchmark, we see that CaPO consistently improve over base models, especially show a large improvement in counting and color attribution for both SDXL and SD3. 
% Regarding T2I-Compbench, we see that CaPO also demonstrates improvement over base model, showing off better compositional generation ability.

% \input{tables/tab_single}


% \subsection{Single reward experiment}\label{sec:single_exp}
% \paragraph{Setup.} We demonstrate the effectiveness of CaPO compared to various preference learning objectives, \emph{e.g.}, DPO~\citep{wallace2023diffusion}, conservative DPO (cDPO), and Diffusion-IPO~\citep{azar2024general}.
% We use same experimental setup in Sec.~\ref{sec:exp_main}, but use only single reward of each MPS, VQAscore, and VILA. Note that for DPO and IPO, we only use reward signals to select the pair, and for cDPO, the model considers only pairwise soft preference label as a target for training. For evaluation, we use Parti prompts~\citep{yu2022scaling} to generate images and measure the scores for each MPS, VQAscore, and VILA, then we compute the win-rate over base model with each reward signal.

% \paragraph{Results.} Tab.~\ref{tab:single} shows the results. We observe that CaPO achieves the best gain for each reward signal, and also the robustness towards reward hacking problem. DPO shows the worst performance in preventing reward over-optimization, \emph{e.g.}, the win-rate of VQAscore drops to 46.4\%, which shows over-optimization towards VILA score. For IPO, since the method itself provides conservative update, it shows better performance in preventing reward-hacking than DPO, but IPO tends to underfit towards the maximal gain.
% In Fig.~\ref{fig:single}, we provide some qualitative examples on fine-tuning with each objective. We observe similar trends as we see in Tab.~\ref{tab:single}.



% \input{tables/fig_single}






\input{tables/dpo_comp}
\input{tables/fig_diffdpo_comp}
% \input{tables/fig_weightingeffect}



\subsection{Ablation Studies}
%\vspace{1mm}
\noindent
{\bf Comparison with Diffusion-DPO~\citep{wallace2023diffusion}.} 
We compare our method with Diffusion-DPO~\citep{wallace2023diffusion}, which fine-tunes SDXL on the human preference dataset Pick-a-pic~\citep{kirstain2023pick}.
For fair comparisons, we use the same 58K prompts in Pick-a-pic v2 dataset, and use Pickscore~\citep{kirstain2023pick}, which is a reward model trained on Pick-a-pic dataset, as our reward signal.
Here, we generate $N=16$ images for each prompt, and select a pair by choosing highest and lowest reward, following Sec.~\ref{sec:single_exp}.
% To demonstrate the effectiveness of our approach, we compare our method with Diffusion-DPO~\citep{wallace2023diffusion}, which fine-tuned SDXL with human preference dataset Pick-a-pic~\citep{kirstain2023pick}. 
% For fair comparison, we use same 58K prompts used in Pick-a-pic-v2 dataset, and use Pickscore~\citep{kirstain2023pick} as our reward model. 
% As in Sec.~\ref{sec:exp_main}, we generate $N=16$ images per prompt, and select pairs by choosing highest and lowest reward.
We also train DPO on our synthetic data (DPO-Syn), to show the effect of synthetic data for fine-tuning. 
For evaluation, we generate images using Parti prompts~\citep{yu2022scaling}, and compare Pickscore, MPS, VQAscore, and VILA scores. 
Tab.~\ref{tab:dpo_comp} shows the results. 
Note that while DPO-Syn scores higher than Diffusion-DPO on Pickscore and VILA score, Diffusion-DPO outperforms on VQAscore. 
On the other hand, CaPO strictly shows better performance than Diffusion-DPO. In Fig~\ref{fig:diffdpocomp}, we show visual comparison between SDXL, Diffusion-DPO, and CaPO, which shows consistent trends as in Tab.~\ref{tab:dpo_comp}.




\vspace{1mm}
\noindent
{\bf Effect of loss weighting.} 
We demonstrate the effect of loss weighting that we proposed in Sec.~\ref{sec:adv}. Specifically, we compare the performance of CaPO when using sigmoid loss weighting, and without loss weights (\emph{i.e.}, $w_t\lambda_t' = -1$). We vary the bias of loss weight by $b=1.0, 1.5, 2.0$. 
Tab.~\ref{tab:abl} shows the results of SDXL CaPO models trained with multi-reward experimental setup. We notice using loss weighting significantly improves the performance, while the best $b$ achieves at $b=1.5$. 
Note that the trend of bias differs for SD3-M, which we refer to supplementary for details.

% \vspace{1mm}
% \noindent
% {\bf Effect of }
\input{tables/table_ablation}
% \lipsum[1]




% \paragraph{Experimental setup.} We consider Stable Diffusion XL (SDXL)~\citep{podell2023sdxl}, and Stable Diffusion 3.5-medium (SD3.5-M)~\citep{esser2024scaling} as our base model. To generate images, we use 100K prompts from DiffusionDB~\citep{wang2022diffusiondb}, which encompasses diverse prompts given by real users. Then for each prompt and model, we generate $N=16$ images per prompt. For all experiments, we use same dataset for each model, but use different reward models for each experiment.

% \paragraph{Reward signals.} We consider various reward signals to incorporate multi-dimensional aspect of human preference for text-to-image generation. We use MPS~\citep{zhang2024learning} overall human preference score which is the state-of-the-art model for predicting human preference. While there are also other human preference predictors such as Pickscore~\citep{kirstain2023pick} or HPS~\citep{wu2023human}, we found MPS, Pickscore, and HPS have high correlation, thus improving MPS results in higher Pickscore and HPS score. Nonetheless, we provide empirical results on Pickscore and HPS in Appendix~xx. 
% For reward signals to assess image aesthetics, we use VILA~\citep{ke2023vila}, which is a contrastive image-text encoder fine-tuned with users' comment on image aesthetics. 
% Finally, we consider VQAscore~\citep{lin2025evaluating} that measure image-text alignment by asking question to vision-language model and take the probability of token output as a measure. While VQAscore is not trained with BT model, but we approximate the win-rate by comparing the score of a two data. We provide further analysis in Appendix~xx.

% % \paragraph{Baselines.} 
% % For baselines, we consider DPO and IPO for diffusion models. We choose different hyperparameter $\beta$, by using the 

% \subsection{CaPO with single reward}

% \paragraph{Experimental setup.}
% \begin{itemize}
%     \item Model, prompts, reward models
%     \item elaborate on why we use those reward models (maybe explain with the correlation, e.g., it is the most independent metrics, while the correlation remains.)
%     \item hyperparameter explain
%     \item baselines: DPO, IPO, and calibrated DPO.
%     \item Evaluation: trained rewards on various prompt dataset (GenAI, Parti, Gecko)
%     \item Qualitative comparison
% \end{itemize}
% \paragraph{Quantitative results.}
% \textcolor{red}{\lipsum[1]}

% \paragraph{Qualitative results.}
% \textcolor{red}{\lipsum[1]}
% \subsection{Multi-reward experiment}
% \paragraph{Experimental setup.}
% \begin{itemize}
%     \item multiple rewards: MPS, VQAscore, VILA
%     \item Baselines: usm of rewards, model soup with each fine-tuned model
%     \item Evaluation on Parti prompts
%     \item General T2I benchmark: GenEval, T2I-Compbench, DPG-bench
% \end{itemize}

% \paragraph{Results.}
% \textcolor{red}{\lipsum[2]}

% \paragraph{Benchmark results.}

% \begin{itemize}
%     \item explain the results of table
%     \item Sum of reward performs the worst, then soup, then our pareto algorithm.
%     \item among DPO, IPO, CAPO, CAPO performs the best
% \end{itemize}


% \input{tables/table2}

% \input{tables/fig1}

% \input{tables/fig2}

% \input{tables/robust}
% \subsection{Robustness analysis}
% \textcolor{red}{\lipsum[1]}


% \input{tables/ablation}
% \subsection{Ablation study}
% \textcolor{red}{\lipsum[1]}
\subsection{Discussions}
While our method can improve the quality of T2I generation, some of the improvements (\emph{e.g.}, improving the text rendering for SDXL) is difficult, which is bounded by the performance of original model.
% As we generate samples from a pretrained model to self-improve, it is not feasible to generate high-quality texts when the baseline model is not equipped with such capabilities or trained on such data. 
However, for more powerful diffusion models (\emph{e.g.}, SD3), we show that our method can improve the text rendering as well. 
Furthermore, our approach is built upon offline data generation, which often suffers from slow convergence. Extending CaPO to online learning problems is a promising direction and we leave it for future work.