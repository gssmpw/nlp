% \input{tables/algorithm}

%HERE
\section{Proposed Method}
In this section, we introduce our method for calibrated preference optimization. We refer to Fig.~\ref{fig:overview} for overview.

\subsection{Motivation}\label{sec:problem}
The challenges in multi-reward optimization is in achieving the Pareto optimality among reward signals, especially even when they conflict. 
For example, when optimizing models for image aesthetics, it often results in reduced image-text alignment as aesthetic reward models do not consider textual information (\emph{e.g.}, see Tab.~\ref{tab:single}). 
One common practice is to use weighted sum of rewards as a proxy for the total reward function~\citep{clark2023directly}.
However, those rigid formulations cannot effectively consider all aspects of utilities, which might lead to suboptimal performance, \emph{e.g.}, biased towards certain reward signals.
Another approach is using the rewarded soups~\citep{rame2024rewarded}, which merges the independently reward fine-tuned model with model soup~\citep{wortsman2022model}.
Nevertheless, optimizing for a single reward is prone to reward over-optimization~\citep{gao2023scaling, rafailov2024scaling} and result in significant performance loss.

Our core assumption is that the difficulties in multi-reward optimization lie in the inconsistency between the black-box distribution of rewards. 
To address this challenge, we propose calibrated preference optimization to minimize inconsistencies by fine-tuning with general and unified metrics. 
In the following, we provide details of our method.

% \vspace{1mm}
% \noindent
% {\bf Overview.}
% We provide our general training pipeline in Fig.~\ref{fig:overview}.
% We first generate multiple samples from pretrained model and use multiple reward models to construct preference dataset for fine-tuning (\emph{e.g.}, Fig.~\ref{fig:overview}(a)). 
% Then to effectively leverage the information from reward model, we use reward calibration to approximate the general preference distribution induced by reward models (Fig.~\ref{fig:overview}(b)).
% Furthermore, we introduce a pair selection method, especially, frontier-based rejection sampling for multi-reward scenario to achieve Pareto optimality (Fig.~\ref{fig:overview}(c)).

%; in Sec.~\ref{sec:capo} we introduce calibrated rewards that accommodate various reward signals into an unified measure of goodness, and fine-tuning objective that distills this into diffusion models.
%Then, in Sec.~\ref{sec:frs}, we provide data selection method that equips joint optimization of multiple reward signals with efficiency.
%Lastly, we provide a loss weighting scheme to improve diffusion preference optimization (Sec.~\ref{sec:adv}).

% the inconsistency of reward signals by reward fine-tuning of T2I diffusion models introduces an unified and general metrics to handle the inconsistency between reward signals. effectively handles th
% One of the common practice in multi-reward optimization is to use the weighted sum of rewards as a proxy for the total reward function~\citep{clark2023directly} and control the trade-off by choosing coefficients.
% However, this resorts to heuristics that often results in suboptimal performance
% Alternative approach is using rewarded soups~\citep{rame2024rewarded}, which performs reward optimization for each model, and use model soup~\citep{wortsman2022model} (\emph{i.e.}, model weight interpolation) to merge fine-tuned models.

% While model soups have shown  
% multi-reward optimization necessity
% calibration necessity
% better options for multi-task learning
% \vspace{0.05in}
% \noindent
% {\bf Problem setup.}

% \vspace{0.05in}
% \noindent
% {\bf Method overview.}
% Given a prompt dataset $\mathcal{D}$, we generate $N$ images per prompt
% We generate $N$ images per prompt

% 1. general explanation
%  - our goal is to improve the performance of T2I diffusion model (image-text alignment, aesthetics)
%  - we assume no high-quality data / paired human preference data, but only reward signals as a proxy objective to align.
%  - rewards are from oracle, not differentiable
%  - we consider multi-reward
% 1. why do we need multi-reward?
%  - Using a single reward cannot fully achieve improvement, 
%  - single reward is more prone to reward over-optimization
% 2. Main challenges
% - reward signals are often not calibrated, they are good at predicting the better ones, but the values might not represent the general goodness of the data.
% - rewards form incoherent distributions, which disperses the effect at worst case, the model is biased towards certain reward signals. 
% Directly using Rewards are not uniformly scaled -> directly using the rewards can be misleading
% - Hard to obtain balanced optimization, is worse when objectives can conflict.
% - 
% usign the reward values from reward model can be misleading. 
% we use multi-reward to prevent over-optimization

% calibrated reward optimization
% - common objective e.g., RLHF trian reward model with bradley-terry model, and maximize the reward. however, using reward can be misleading b/c
% 1) reward does not represent the absolute goodness
% 2) different scales of reward prevents multi-reward optimization challenging

% \subsection{Problem setup}

% Our goal is to fine-tune a pretrained T2I diffusion models to improve its utility (\emph{e.g.}, image-text alignment, aesthetics) by using diverse reward signals that covers a wide-ranging aspect of text-to-image synthesis. 
% The reward optimization for T2I diffusion models has following challenges.
% First, 
% We remark some differences of our problem with previous works; in contrast to other works that utilizes high-quality data~\citep{dai2023emu} or human preference data~\citep{wallace2023diffusion} for fine-tuning, we narrow down the scope of data usage to be that generated from pretrained model. 
% Furthermore, we consider an oracle reward model, which supports a wider range of reward signals including large multimodal language models, than others that assumes differentiable rewards~\citep{clark2023directly}.

% Those problem setup gives us following challenges.
% First, it is unclear how to incorporate the reward information during fine-tuning. 
% Second, 

% Furthermore, 
% fine-tune a pre-trained T2I diffusion models to improve
% We consider fine-tuning T2I diffusion models to maximize the expected rewards of the output images from various prompts. 
% To this end, we collect dataset by sampling images from the base model, and obtain oracle rewards from reward model(s).
% $\max_\theta J(\theta)$
% collecting dataset by sampling images with various prompts, and updating the diffusion model with reward functions. 

% \paragraph{Comparison to previous works.}
% Previous works have also considered different setups in fine-tuning T2I diffusion models with reward models. Here, we elaborate the difference in the problem setup.
% While we only consider oracle feedback from the reward models, some works consider differentiable reward models and perform fine-tuning by using the reward gradients~\citep{clark2023directly, prabhudesai2023aligning}. 
% We remark that our method does not require differentiability of rewards, thus support more diverse form of rewards, and especially more beneficial when scaling to larger diffusion and reward models (\emph{e.g.}, using multimodal LLMs).
% Another line of works consider using pre-collected human-rated dataset such as Pick-a-pic dataset~\citep{kirstain2023pick} to perform DPO algorithm, but we assume there is no access to offline human preference dataset.

% and perform DPO While using DPO on pre-collected dataset showed promising results, collecting
% \citet{dong2023raft, prabhudesai2023aligning} considered using differentiable reward models to directly fine-tune T2I diffusion models with gradient signal from rewards, 


% \begin{itemize}
%     \item Difference between previous works
%     \item core challenge: how to mitigate reward-overoptimization while maximizing the gain?
%     \item how to achieve balanced gain over all axis
%     \item how to 
% \end{itemize}
% We assume the simplest setup

% \begin{itemize}
%     \item Our problem: how to fine-tune diffusion models when multiple rewards are given
%     \item We consider more general problem than previous methods, e.g., which requires gradient of reward or require the paired dataset collected.
%     \item Some approaches consider diffusion sampling as RL problem, yet those approach requires memory expensive sampling chain, which makes difficult to scale up to large diffusion models.
%     \item We consider generating images from current model, then score the images, and update the model. When doing this iteratively, this can simulate online learning. (see Fig. xx)
%     \item Two major questions: how to fine-tune (optimization method) and how to select training data (data)
%     \item Next section, we propose a novel approach in fine-tuning, which is not only effective for multi-reward problem, but also for a single reward optimization problem.
%     \item Then for multi-objective optimization, we propose an effective data sampling method for multi-objective problem.
% \end{itemize}


%HERE
%\subsection{Calibrated Preference Optimization}\label{sec:capo}
\subsection{CaPO}\label{sec:capo}
Although we consider reward models as a proxy to represent the utility of a sample, directly using the reward values can lead to unsatisfactory results if they are not properly calibrated. 
Specifically, when using Bradley-Terry model~\citep{bradley1952rank}, the reward value often does not measure the goodness of a sample, even though the model exhibits high prediction accuracy in classifying the human preference. 
% cannot most reward models are trained by performing ranking tasks (\emph{e.g.}, Bradley-Terry model~\citep{bradley1952rank}), where the value itself cannot meaningfully measure goodness of a sample. 
Furthermore, the varying range of reward becomes problematic when using multiple reward signals, making it difficult to obtain balanced updates. 
% the various range of different reward signals makes hard to obtain balanced optimization. distribution of reward values varies among different reward models, thus it requires heuristics to balance the trade-offs.
% and it requires heuristics to merge them.

\vspace{1mm}
\noindent
{\bf Calibrated rewards.}
To address these issues, we propose to use expected win-rate as a unified measure for maximization target. 
Formally, let $\mathbb{P}(\mathbf{x} \succ \mathbf{x}' | \mathbf{c})$ be a win-rate of data $\mathbf{x}$ over $\mathbf{x}'$ with prompt $\mathbf{c}$. 
We define the win-rate of data $\mathbf{x}$ over a distribution $p(\cdot | \mathbf{c})$:
\begin{equation}\label{eq:winrate}
    \mathbb{P}(\mathbf{x} \succ p | \mathbf{c}) \coloneqq \mathbb{E}_{\mathbf{x}' \sim p(\cdot | \mathbf{c})}\big[\mathbb{P}(\mathbf{x} \succ \mathbf{x}'|\mathbf{c})\big]\text{.}
\end{equation}
As our goal is to improve over reference model $p_{\textrm{ref}}$, we consider $\mathbb{P}(\mathbf{x} \succ p_{\textrm{ref}}|\mathbf{c})$ as our target of interest. 
By using expected win-rate over reference model, we directly seek for improvement over a pretrained model, which quantifies the general goodness of a data. Furthermore, the bounded range makes it more favorable for multi-reward optimization.
% favors to the application for multi-reward optimization.
Since the expected win-rate is not available in general,
we approximate it through averaging the pairwise win-rate computed by a reward model.
Suppose we generate $N$ batch of samples $\{\mathbf{x}_i\}_{i=1}^N$ from $p_{\textrm{ref}}(\cdot|\mathbf{c})$, then we define \emph{calibrated reward} $R_{\textrm{ca}}(\mathbf{x}_i, \mathbf{c})$ for each sample $i$:
\begin{equation}\label{eq:calre}
\vspace{-3mm}
    R_{\textrm{ca}}(\mathbf{x}_i, \mathbf{c}) = \frac{1}{N-1} \sum_{j\neq i} \sigma \big( r(\mathbf{x}_i, \mathbf{c}) - r(\mathbf{x}_j, \mathbf{c}) \big)\text{,}
\end{equation}
where we have $R_{\textrm{ca}}(\mathbf{x}, \mathbf{c}) \approx \mathbb{P}(\mathbf{x}\succ p_{\textrm{ref}}|\mathbf{c})$ for large $N$.

\vspace{1mm}
\noindent
{\bf CaPO loss.}
We replace $R(\mathbf{x}, \mathbf{c})$ in Eq.~\eqref{eq:obj} with $R_{\textrm{ca}}(\mathbf{x}, \mathbf{c})$, and introduce calibrated preference optimization objective that fine-tunes the model to maximize the calibrated reward. 
Similar to Eq.~\eqref{eq:dffpo}, we define CaPO loss by matching the difference of the calibrated rewards with regression loss~\citep{deng2024prdp, fisch2024robust}, which also guarantees the optimality condition. 
Specifically, given data pair $(\mathbf{x}^+, \mathbf{x}^-)$, we define CaPO objective:
% we define CRO loss by the regression objective between the difference between calibrated rewards and difference between implicit rewards:
% \begin{equation}
%     \bigg(R_{\textrm{ca}}(\mathbf{x}, \mathbf{c}) - R_{\textrm{ca}}(\mathbf{x}', \mathbf{c}) - \beta\log\frac{p_\theta(\mathbf{x}|\mathbf{c})p_{\textrm{ref}}(\mathbf{x}'|\mathbf{c}) }{p_{\textrm{ref}}(\mathbf{x}|\mathbf{c}) p_\theta(\mathbf{x}|\mathbf{c})} \bigg)^2\text{,}
% \end{equation}
% and corresponding CRO loss for diffusion model is given by
\vspace{-1mm}
\begin{align}\label{eq:capoloss}
\begin{split}
    \mathcal{L}_{\textrm{CaPO}}(\theta) &= \underset{t,\eps, \eps'}{\mathbb{E}}\bigg[
    \big( R_{\textrm{ca}}(\mathbf{x}^+, \mathbf{c}) - R_{\textrm{ca}}(\mathbf{x}^-, \mathbf{c}) \\
    &- \beta \big(R_\theta(\mathbf{x}_t^+,\mathbf{c},t) - R_\theta(\mathbf{x}_t^-, \mathbf{c}, t)\big)\big)_2^2\bigg]\text{,}
\end{split}
\end{align}
where $\mathbf{x}_t^+=\alpha_t\mathbf{x}^+ + \sigma_t\eps^+$, $\mathbf{x}_t^-=\alpha_t\mathbf{x}^- + \sigma_t\eps^-$, for $t\sim\mathcal{U}(0,1)$ and $(\eps^+, \eps^-)\sim \noise \times \noise$.
% \vspace{0.05in}
% \noindent
% {\bf Connection to IPO~\citep{azar2024general}.}
Note that CaPO is a special case of Eq.~\eqref{eq:dffpo} with $g(u)=(\Delta R - u)^2$, where $\Delta R = R^+ - R^- $ is a difference between calibrated rewards.
Thus, CaPO is a generalization of IPO~\citep{azar2024general}, which strictly assign $\Delta R = 1$ for all pairs.
Compared to IPO, CaPO assigns a dynamic target for the preference learning, which helps maximizing the gain without reward over-optimization. 
% Practically, this allows to explore smaller KL regularization without reward over-optimization (\emph{e.g.}, see xx).
% By using calibrated rewards, CaPO is beneficial at mitigating reward over-optimization compared to IPO. 
% In practice, this allows to use smaller KL regularization compared to IPO, which and helps 

% By using calibrated rewards, there are two advantages:
% First, CaPO helps mitigating reward over-optimization by incorporating the calibrated rewards.

% \lipsum[1]
% \noindent
% {\bf Comparison with IPO}
% By taking gradient of Eq.~\eqref{eq:cro}, we derive following equivalent form (see Appendix xx for derivation):
% \begin{equation}
%     \mathcal{L}_{\textrm{CRO}}(\theta) = \mathbb{E}_{t,\eps,\eps'}[(R_{\textrm{ca}}(\mathbf{x}, \mathbf{c}) - \beta Q_\theta(\mathbf{z}_t;\mathbf{c},t)) \|\eps_\theta(\mathbf{z}_t;\mathbf{c},t) - \eps\|_2^2 ]    
% \end{equation}

% and CRO loss for diffusion model is given as follows
% where we show that minimizer of CRO loss satisfies $p_\theta(\mathbf{x}|\mathbf{c}) = p_{\textrm{ref}}(\mathbf{x}|\mathbf{c})\exp (R_{\textrm{ca}}(\mathbf{x}, \mathbf{c}))$

% \noindent
% {\bf Comparison to DPO }


% We remark that calibrated reward is a sufficiently good estimator of $\mathbb{P}(\mathbf{x} \succ p_{\textrm{ref}}|\mathbf{c})$ compared to $r(\mathbf{x}, \mathbf{c})$. 
% Suppose $r^*(\mathbf{x},\mathbf{c})$ is a ground-truth reward model that follows Bradley-Terry model, then we have 
% \begin{equation}
%     \mathbb{P}(\mathbf{x} \succ p_{\textrm{ref}}|\mathbf{c}) = \mathbb{E}_{\mathbf{x}'\sim p_{\textrm{ref}}(\mathbf{x}|\mathbf{c})}[\sigma(r^*(\mathbf{x},\mathbf{c}) - r^*(\mathbf{x}', \mathbf{c}))]\text{.}
% \end{equation}
% Suppose the error of proxy reward $r(\mathbf{x}, \mathbf{c})$ in estimating $r^*(\mathbf{x}, \mathbf{c})$ can 

% Now our objective 
% \noindent
% {\bf Calibrated Reward Optimization}


% In practice, we approximate the win-rate by averaging the pairwise win-rate using reward models, \emph{i.e.}, given $N$ batch of samples $\mathbf{x}_1,\ldots, \mathbf{x}_N$, the calibrated reward $r_c(\mathbf{x},\mathbf{c})$  for  $\mathbf{x}$ we generate multiple samples $\{\mathbf{x}_i\}_{i=1}^N$

% Using Eq.~\eqref{eq:winrate} as reward function provides provides several advantages.
% Subsequently, by replacing $r(\mathbf{x}, \mathbf{c})$ in Eq.~\eqref{eq:obj} with $\mathbb{P}(\mathbf{x} \succ p_{\textrm{ref}} |\mathbf{c})$, our objective is given as follows:
% \begin{equation}\label{eq:cobj}
%     \max_\theta \mathbb{E}_{\mathbf{c},\mathbf{x}\sim p_\theta(\cdot|\mathbf{c})} \big[\mathbb{P}(\mathbf{x} \succ p_{\textrm{ref}}|\mathbf{c})\big] - \beta D_{\textrm{KL}}(p_\theta \| p_{\textrm{ref}})\text{.}
% \end{equation}
% In practice, we estimate the expected win-rate by averaging the pairwise win-rate 

% generate multiple samples from pretrained 
% to solve  
% $\mathbb{P}(\mathbf{x} \succ p | \mathbf{c})$
% Most of the reward models  
% The underlying misuse of reward models is due to the
% However, this often results in suboptimal performance when reward values are not properly calibrated,
% However, this can results in suboptimal performance when reward values are not properly calibrated, which requires heuristics such as reward standardization. 

% post-processing such as reward standardization. 
% However, most of the reward models are not properly calibrated, often results in misleading performance. 
% For example, most of the reward models such as human preference reward models are trained with ranking losses, which performs pairwise comparison to learning. Thus, 
% The common practice in reward optimization is directly use the reward valu use the reward as 


% Suppose we have access to the ground-truth reward model $r^*(\mathbf{x}, \mathbf{c})$, then it is reasonable choice to directly incorporate the reward values to optimize the model.
% To this end, we consider following reward-matching loss function for efficient offline optimization:
% \begin{equation}\label{eq:rewardmatching}
%     \mathbb{E}_{(\mathbf{x}, \mathbf{x}',\mathbf{c})}\left[ (r^*(\mathbf{x},\mathbf{c}) - r^*(\mathbf{x}', \mathbf{c}) - (r_\theta(\mathbf{x}, \mathbf{c}) - r_\theta(\mathbf{x}', \mathbf{c})))^2 \right]\text{,}
% \end{equation}
% where $\mathbf{x}, \mathbf{x}'\sim p_{\textrm{ref}}(\cdot | \mathbf{c})$ for $\mathbf{c}\sim\mathcal{D}$. 
% Then it is straightforward to check that the optimal $p_{\theta^*}$ that minimizes Eq.~\eqref{eq:rewardmatching} is equivalent to the optimal distribution of Eq.~\eqref{eq:rlhf}. 

% However, the ground-truth reward model $r^*$ is not known in practice, and we consider the proxy reward model to use for reward matching loss. From BT model, what we can assume is that $\|r^*(\mathbf{x}, \mathbf{c}) - r(\mathbf{x}, \mathbf{c}) - C(\mathbf{c}) \|_2^2 < \delta$ for some $\delta > 0$ and partition function $C(\mathbf{c})$. Then the difference between reward model $r(\mathbf{x}, \mathbf{c}) - r(\mathbf{x}', \mathbf{c})$ has at most error of $2\delta$. This makes problematic when $\delta$ is large. 

% \paragraph{Calibrated reward matching loss.}
% Instead of using $r(\mathbf{x}, \mathbf{c})$ as a target for our reward-matching loss, we consider win-rate $\mathbb{P}(\mathbf{x} \succ \mu | \mathbf{c})$ of $\mathbf{x}$ over a reference distribution $\mu$ as our target reward.
% While $\mathbb{P}(\mathbf{x} \succ \mu | \mathbf{c})$ is not achievable in general, we sample multiple images from $\mu$ to approximate $\mathbb{P}(\mathbf{x} \succ \mu | \mathbf{c})$. Suppose $N$ images $\mathbf{x}_1,\ldots, \mathbf{x}_N$ from $\mu(\cdot | \mathbf{c})$, then we compute the approximate win-rate $\hat{\mathbb{P}}(\mathbf{x} \succ \mu|\mathbf{c})$ as follows:
% \begin{equation}
%     \hat{\mathbb{P}}(\mathbf{x}_i \succ \mu | \mathbf{c}) = \frac{1}{N}\sum_{i=1}^N \sigma \big(r(\mathbf{x}, \mathbf{c}) - r(\mathbf{x}_i, \mathbf{c})\big)\text{.}
% \end{equation}
% Remark that this gives us multiple advantages over using $r(\mathbf{x}, \mathbf{c})$ as target. 
% First, for sufficiently large $N$, the error can be reduced with rate $O(\tfrac{1}{N})$. 
% Specifically, let us assume $|r^*(\mathbf{x}, \mathbf{c}) - r(\mathbf{x}, \mathbf{c}) -C(\mathbf{c}) | < \delta$. Then we have 
% \begin{align*}
% \begin{split}
%     &|\sigma (r(\mathbf{x}, \mathbf{c}) - r(\mathbf{x}', \mathbf{c})) - \sigma( r^*(\mathbf{x}, \mathbf{c}) - r^*(\mathbf{x}', \mathbf{c})| \\
%     <&\tfrac{1}{4} | r(\mathbf{x}, \mathbf{c}) - r(\mathbf{x}', \mathbf{c}) - (r^*(\mathbf{x}, \mathbf{c}) - r^*(\mathbf{x}', \mathbf{c})| < \frac{\delta}{2}\text{.}
% \end{split}
% \end{align*}
% Then for $N$ samples, we have 
% \begin{equation}
%     |\mathbb{P}(\mathbf{x} \succ \mu |\mathbf{c}) - \hat{\mathbb{P}}_N(\mathbf{x} \succ \mu | \mathbf{c}) | <\frac{\delta}{2N}\text{.}
% \end{equation}
% Furthermore, since the win-rate is in range $[0,1]$, this makes more amenable for multi-reward optimization. Otherwise, the vast difference in scale of $r(\mathbf{x}, \mathbf{c})$ makes multi-reward optimization non-harmonic, where large scaled $r$ is being more optimized.
% % Then, our final Win-rate matching Preference Optimization loss is given as
% % \begin{equation}
% %     \mathcal{L}_{WPO}(\theta) = \mathbb{E}
% % \end{equation}
% As a result, one can show the following:
% \begin{equation}
%     J(p^*) - J(p_{\theta}) < C_{\textrm{cov}}\frac{\delta}{N}\text{,}
% \end{equation}
% where $C_{\textrm{cov}} \coloneqq \max_{\mathbf{x},\mathbf{c}} \frac{p_\theta(\mathbf{x}|\mathbf{c})}{p_\textrm{ref}(\mathbf{x}|\mathbf{c})}$ is a global coverage.

% \paragraph{Relationship with IPO~\citep{azar2024general}.}

% we show that the error rate can go down to $O(\tfrac{\delta}{N})$

% our proxy reward model can only have a best estimate where we can assume $\|r^*(\mathbf{x}, \mathbf{c}) - r(\mathbf{x}, \mathbf{c}) - \C(\mathbf{c})\|_2\leq \delta$ for some normalization constant $C(\mathbf{c})$. That said, the scale of the reward $r$ can be largely differ for each prompt $\mathbf{c}$, and the scale of reward can be largely differ when considering multiple reward models. 
% This 

% reward model $r(\mathbf{x}, \mathbf{c})$ can be 

% Suppose we have access to the ground-truth reward function $r^*(\mathbf{x},\mathbf{c})$, its

% Suppose we have $N$ images $\mathbf{x}_1, \ldots, \mathbf{x}_N$ per prompt
% \begin{itemize}
%     \item Given $N$ generated images from a prompt, it is straightforward to consider winning image and losing image by comparing the rewards and apply DPO or IPO. 
%     \item While it's simple. It may prone to over-optimization, where all images have similar rewards. 
%     \item This occurs us to consider how to leverage the information of reward into direct alignment algorithms. 
%     \item Propose reward distillation loss
%     \item It is a common practice to choose R from the logits of BT model. However, this might be problematic, as the it is not calibrated.
%     \item That being said, high value of $r(x, c)$ does not guarantee the absolute goodness of a pair $(x,c)$. For example, $(x,c)$ can be a good pair, but the logit value can be in a lower value than a bad pair $(x',c')$.
%     \item Instead, we aim to compute the calibrated reward by computing the pairwise win-rate among $N$ samples. 
%     \item This approximates the goodness $P(x\succ \mu)$ where $\mu$ is a pretrained model. 
%     \item Good thing 1) better for multi-reward problem since it is scaled from [0,1], while the scale differs for each reward model
%     \item Comparison with pair-wise soft version of DPO / IPO. Those only consider the win-rate between pair. This is not idealistic as the both images can be bad or good, but the gap might be large.
%     \item (Theory) Theoretically, this aligns with the theory of IPO.
%     \item (Analysis) If we generalize to $k$ pairs, i.e, kC2 pairs, then one can show that it is equivalent to following, which can be considered as a reward weighted regression, while subtracting baseline by leave-one-out. Difference is that we subtract KL regularization term. 
% \end{itemize}

%HERE
\subsection{Preference Pair Selection}
\label{sec:frs}
The best-of-$N$ sampling~\citep{nakano2021webgpt, cobbe2021training} or rejection sampling~\citep{touvron2023llama} methods that 
select samples with highest reward from $N$ generation
are commonly used in RLHF.
% reward optimization. 
For a single reward, it is straightforward to choose the sample $\mathbf{x}^+$ with highest reward, and $\mathbf{x}^-$ that has lowest reward to maximize the margin between the pair.
For multi-reward optimization, the na\"ive approach is to use weighted sum as the total proxy reward model, and perform rejection sampling with it. 
However, choosing the weights often relies on heuristics, and the optimal weights might be dynamic depending on the input, which can lead to suboptimal performance.


% However, those strategy is not starightforward to apply when multi-reward optimization
% that has highest reward
% One of the common practice in reward fine-tuning is rejection sampling~\citep{touvron2023llama}, or as known as \emph{best-of-N} sampling, which takes a sample with highest reward from $N$ generation. For contrastive algorithm such as CaPO or DPO, it can be straightforward to consider the pair with highest reward and lowest reward, so that the model learns from the pair with maximal margin. When dealing with multiple rewards, formally let $r_i$, $i=1,\ldots, L$ be reward models, the common approach is to consider the weighted sum as the best proxy reward model, \emph{i.e.}, $r_{\textrm{sum}}(\mathbf{x}, \mathbf{c}) = \sum_{i=1}^L \lambda_i r_i(\mathbf{x}, \mathbf{c})$ for $\lambda_i \geq 0$ with $\sum_{i=1}^L \lambda_i = 1$. However, choosing $\lambda_i$ often relies on heuristics, and the optimal $\lambda_i$ might be dynamic depending on the input $(\mathbf{x}, \mathbf{c})$. Therefore, conducting rejection sampling using the sum of rewards might give suboptimal gain. 

In order to achieve the Pareto optimal solution, we propose frontier-based rejection sampling (FRS), which selects the set of positive samples $X^+(\mathbf{c})$ and negative samples $X^-(\mathbf{c})$ for each prompt $\mathbf{c}$ by finding Pareto optimal set. 
Specifically, we use a non-dominated sorting algorithm~\citep{deb2002fast} to find the upper and lower Pareto frontier. 
The goal of FRS is to push apart from the lower Pareto frontier and pull towards the upper Pareto frontier, which helps to achieve Pareto optimality.
Given $L$ reward models, let $R_{\textrm{ca}}^{(j)}$ be $j$-th calibrated rewards for $j=1,\ldots, L$, then we define $\mathbf{x}$ dominates $\mathbf{x}'$ if and only if $R_{\textrm{ca}}^{(j)}(\mathbf{x}, \mathbf{c}) \geq R_{\textrm{ca}}^{(j)}(\mathbf{x}', \mathbf{c})$ for all $j=1,\ldots, L$. 
Then finding a set of non-dominated data points is referred as finding Pareto set, which forms an upper frontier. Conversely, one can define a set of dominated data that forms a lower frontier. 
After removing the potential duplicates of non-dominated and dominated sets, we take $X^+(\mathbf{c})$ by filtered non-dominated sets and $X^-(\mathbf{c})$ by set of dominated set. 
Given positive set ${X}^+(\mathbf{c})$ and $X^-(\mathbf{c})$, we sample a positive sample $\mathbf{x}^+\sim X^+(\mathbf{c})$ and $\mathbf{x}^-\sim X^-(\mathbf{c})$ to construct a pair.
We use CaPO loss to update the model with ensemble of calibrated rewards for optimization target:
\vspace{-2mm}
\begin{equation*}
\vspace{-2mm}
    R_{\textrm{ca}}(\mathbf{x}, \mathbf{c}) = \frac{1}{L} \sum_{j=1}^L R_{\textrm{ca}}^{(j)}(\mathbf{x}, \mathbf{c})\text{,}
    % \Delta(\mathbf{x}^+, \mathbf{x}^-, \mathbf{c}) = \frac{1}{L}\sum_{j=1}^L \big[R_{\textrm{ca}}^{(j)}(\mathbf{x}^+, \mathbf{c}) - R_{\textrm{ca}}^{(j)}(\mathbf{x}^-, \mathbf{c})\big]  
\end{equation*}
and use CaPO loss in Eq.~\eqref{eq:capoloss} for the update.
% In Algorithm~\ref{alg:dm}, we provide a pseudocode for calibrated reward optimization with multiple rewards.
% multi-reward CRO in Algorithm~\ref{alg:dm}.

% and at each iteration we select positive sample $x^+$ from $X^+$ and negative sample $x^-$ from $X^-$ to construct a pair.

% joint optimization of multiple reward signals that learns pair with diverse directions, which helps achieving balanced optimization
% with  learning of multiple direction to ensure balanced learning of each reward and prevent the bias towards certain reward signal. 
% In Fig.~xx, we provide an illustration to our frontier-based rejection sampling method.

% $k$

% dominance of $\mathbf{x}$ over $\mathbf{x}'$ is defined

% we define $\mathbf{x}$ dominates $\mathbf{x}'$ 

% selects a pair $(\mathbf{x}^+, \mathbf{x}^-)$ of positive sample $\mathbf{x}^+$ and negative sample $\mathbf{x}^-$ by 

% selecting positive sample from upper Pareto frontier, and a negative sample from lower Pareto frontier by using non-dominated sorting algorithm~\cite{deb2002fast}. 
% Formally, we define $\mathbf{x}$ dominates $\mathbf{x}'$ if $\mathbf{x}$ is better or equal in each reward values than that of $\mathbf{x}'$, \emph{i.e.}, $r_i(\mathbf{x}, \mathbf{c}) \geq r_i(\mathbf{x}', \mathbf{c})$ for all $i=1,\ldots, L$. Then finding a set of non-dominated data is referred as finding Pareto set, which forms an upper frontier. Conversely, one can consider a set of dominated data that forms an lower frontier. Then, after removing the potential duplicates of non-dominated and dominated set, we choose a positive sample from non-dominated set and a negative sample from dominated set to feed into our preference learning objective. 

% \paragraph{Reward ensemble.}
% Given a pair of samples $(\mathbf{c}, \mathbf{x}^+, \mathbf{x}^-)$ and multiple reward models $\{r_i\}_{i=1}^L$, again we compute the approximate win-rate using batch of samples, \emph{i.e.}, $\mathbb{P}_i(\mathbf{x}\succ \mu)$. Then the simplest approach is to average the win-rate, \emph{i.e.}, $\mathbb{P}_{\textrm{avg}}(\mathbf{x}\succ \mu| \mathbf{c}) = \frac{1}{L}\sum_{j=1}^L \mathbb{P}_i(\mathbf{x} \succ \mu|\mathbf{c})$. While it is the most simplest, averaging the win-rate might be misleading if the overwhelming one has large error. 
% To circumvent this issue, we also consider uncertainty-weighted optimization (UWO)~\citep{coste2023reward, yu2020mopo, brantley2019disagreement, wu2021uncertainty}, where we penalize the estimate by variance of all estimates, \emph{i.e.}, 
% \begin{align}
% \begin{split}
%     &\mathbb{P}_{\textrm{UWO}}(\mathbf{x}\succ\mu |\mathbf{c}) \coloneqq \frac{1}{L}\sum_{j=1}^L \mathbb{P}_i(\mathbf{x}\succ \mu |\mathbf{c}) -\\
%     &\frac{\lambda}{L}\sum_{j=1}^L ( \mathbb{P}_i(\mathbf{x}\succ\mu|\mathbf{c}) - \mathbb{P}_{\textrm{avg}}(\mathbf{x}\succ\mu|\mathbf{c}))^2\text{,}
% \end{split}
% \end{align}
% where $\lambda > 0$ is a regularization hyperparameter.
%MH: add more sentences to the caption  to make the figure self-explanatory (without reading the text) 
% \input{tables/fig_weight}

\subsection{Loss weighting}\label{sec:adv}
% \vspace{0.05in}
% \noindent
% {\bf Loss weighting.}
The choice of log-SNR $\lambda_t$ and weighting function $w_t$ has large impact on the generation quality and convergence of diffusion model pretraining.
Intuitively, when $\lambda_t$ is large, \emph{i.e.}, small amount of noise is added, the denoising task becomes easier, and conversely the task becomes harder as $\lambda_t$ becomes smaller, thus weighting function as a monotonically decreasing weighting function of $\lambda_t$ seems a reasonable choice. 
In \citep{kingma2023understanding}, those monotonic weighting are theoretically shown to be the weighted evidence lower bound (ELBO), and demonstrated better quality than the non-monotonic counterpart.
In this work, we also propose to use monotonic loss weighting to our CaPO loss, which is equivalent to regularizing with weighted ELBO instead of KL divergence in Eq.~\eqref{eq:obj}. 
Specifically, we apply sigmoid weighting with bias, \emph{i.e.}, $w_t = w(\lambda_t) = \sigma(-\lambda_t + b)$, where $b$ is a bias hyperparameter~\citep{kingma2023understanding, hoogeboom2024simpler}. See supplementary for details.
% Note that for flow matching objective~\citep{lipman2022flow} which predicts the velocity instead of noise, this weighting is equivalent to multiplying $(\exp(\lambda_t/2) + \exp(-\lambda_t/2))^{-1}$ to the flow matching objective (see supplementary for detail).
%MH: fill in xx


% Remark that original Diffusion-DPO~\citep{wallace2023diffusion} considers non-monotonic weighting (\emph{i.e.}, $-w_t \lambda_t' = 1$) for SDXL.
% We remark that this provides better performance compared to non-monotonic weighting approaches
% weighted flow matching objective becomes $(\exp(\lambda_t/2) +\exp(-\lambda_t/2))^{-1}\|v_\theta(\mathbf{z}_t;t) - (\eps-\mathbf{x})\|_2^2$

% using logit-normal sampling~\citep{esser2024scaling} (\emph{i.e.}, sampling $\lambda_t$ from normal distribution) is a popular choice. 



% When training diffusion models, it has been well-studied that the lower noise level, \emph{i.e.}, for small $t$, the model tends to learn the high-frequency detail of an image, while the model at higher $t$ entails the low-frequency detail of an image. Intuitively, it could be harmful when we change the model's capability in generating high-frequency detail, and modifying the low-frequency detail suffices. To this end, we provide dynamic $\beta_t$ for each timestep instead of constant $\beta$ by providing more weights on higher timesteps. For discrete-time diffusion model, \emph{e.g.}, DDPM~\citep{ho2020denoising, rombach2022high}, we use continuous adaptation of Min-SNR-$\gamma$ weighting, where we use $\beta_t = \frac{\beta\gamma}{\gamma + e^{\lambda_t}}$ and $\gamma>0$ is a hyperparameter. For continuous-time diffusion model (or flow model), we sample $\lambda_t \sim \mathcal{N}(m, \sigma^2)$, where $m, \sigma$ is a mean and standard deviation, following the practice in \citep{karras2022elucidating, esser2024scaling}. Intuitively, choosing higher $m$ leads to sampling from higher timestep region. We find that this techniques provide faster convergence, and better image quality for CaPO, yet the effect is marginal for Diffusion-DPO. We provide detailed analysis in Appendix~xx.
% \vspace{0.05in}
% \noindent
% {\bf Independent noise sampling.} 
% Note that the implicit reward $R_t(\mathbf{z}_t,\mathbf{c}, \theta)$ requires expectation over all $t\sim\mathcal{U}(0,1)$ and $\boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{0}, \mathbf{I})$. From Diffusion-DPO paper~\citep{wallace2023diffusion}, the author claimed that using sample noise for computing the implicit reward is beneficial as it reduces the variance of optimization. However, we empirically find that the effect of variance is mere, and providing independent sampled noise for each $\mathbf{x}^+$ and $\mathbf{x}^-$ shows improved performance for CaPO and even for DPO. Intuitively, using different noise enforces more stronger regularization that any trajectory that ends with $\mathbf{x}^+$ at $t=0$ should have higher likelihood than that ends with $\mathbf{x}^-$ at $t=0$. We provide detailed analysis in Appendix~xx.



% we find that since we fine-tune the model to keep the original capability while modify the 

% We find that providing more weights to the higher timestep region 

% only cleans up the fine-detail without changing the entire layout
% % can be regarded as improving the likelihood of $\mathbf{x}^+$ than $\mathbf{x}^-$ given same initial noise distribution, while 

% This can be thought as 
% it has been common practice that  original Diffusion-DPO~\citep{wallace2023diffusion}
% We remark some improved techniques to fine-tune diffusion models with CaPO. 
% First, in contrast to Diffusion-DPO~\citep{wallace2023diffusion}, we found that sampling 
% Given triplet $(\mathbf{c}, \mathbf{x}^+, \mathbf{x}^-)$ and the approximate win-rate $\mathbb{P}(\mathbf{x}\succ \mu |\mathbf{c})$ a

% % one overwhelms others. 

% % multiple reward si
% % We illustrate the 

% % , thus prevent the biased preference optimization

% % The illustration of  

% % Given a multiple reward signals $r_i$, $i=1,\ldots, N$, 

% % While this method is straightforward for a single reward, it is non-trivial to extend to 
% % The common practice in RLHF is using rejection sampling~\cite{touvron2023llama}
% % When considering a single reward signal for training, it is 
% % \begin{itemize}
% %     \item Given $N$ images per prompt, it is questionable if we have to use all samples for fine-tuning.
% %     \item Instead, we only consider the effective samples for efficiency. 
% %     \item For single reward, it is straightforward and empirically validated that choosing the maximum and minimum score pairs. 
% %     \item For multiple rewards, it is a common approach to set a sum (or weighted-sum) of reward as a proxy for ground-truth reward and solve as a single reward problem.
% %     \item However, this might give us spurious pair which wins big in axis1 but lose small at axis2.
% %     \item To this end, we propose a multi-objective rejection sampling method. 
% %     \item Define non-dominated sorting, dominated sorting. Explain sorting algorithm. Filter duplicates. explain with figure
% %     \item Confirm the necessity of sample complexity of $N$.
% %     \item Is it really efficient? show in ablation study.
% %     \item Then propose DPO-FRS, IPO-FRS, CAPO-FRS. explain with pseudocode.
% % \end{itemize}
