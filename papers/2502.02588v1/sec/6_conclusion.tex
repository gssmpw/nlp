\section{Conclusion}
\label{sec:conclusion}
%MH: no need to separate discussion and conclusion. In addition, I usually move discussion to the end of experiments as a subsection. I will move this there
%\paragraph{Discussion.}
\if 0
While our method can improve the quality of text-to-image generation, some of the improvements (\emph{e.g.}, improving text rendering in images for SDXL) are hard to achieved. This is mainly due to that we consider generating samples from pretrained model to self-improve, thus the lack of such capabilities in base model makes harder to improve certain capability. But for high-quality diffusion models (\emph{e.g.}, SD3), we show that our method can improve the text rendering as well. Furthermore, our approach is built upon offline data generation, which often suffers from slow convergence. Extending CaPO to online learning problem is a promising direction and we leave it for future work.
\fi


%\paragraph{Conclusion.}
In this paper, we present calibrated preference optimization, a robust preference learning objective that fine-tunes the diffusion models to align with human preference by using multiple reward models. 
Specifically, we propose a simple, yet effective method to calibrate the rewards to approximate the win-rate against the base model.
% which theoretically can have sufficiently small error compared to point-wise reward estimation by increasing the number of generated samples. 
We then propose a diffusion preference optimization objective that regresses the difference between the calibrated rewards, which effectively learns from the reward without over-optimization. Furthermore, we extend our approach to a multi-reward problem by providing a frontier-based rejection sampling method that enables joint optimization of various reward signals. 
%MH: not sure what you mean by the models are noised.
Extensive experimental results demonstrate that our approach is efficient and can boost the model performance without using any human-collected preference dataset.