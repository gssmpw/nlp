% Improving Text-to-Image Diffusion Models by Maximizing Improvement 

\begin{abstract}
% Aligning text-to-image (T2I) diffusion models with preference optimization has shown its promise on large-scale, human-annotated datasets, but the heavy cost of manual data collection significantly restricts scalability and accommodation of different preference domains. 
% Proxy reward models provide a sustainable alternative, 
% but optimization with these signals remains challenging due to inconsistencies in their distributions across different preferences, especially when deriving multiple rewards from these preferences.
% To address these challenges, we introduce Calibrated Preference Optimization (CaPO), which improves preference optimization by applying post-hoc reward calibration using pairwise comparisons, and introducing a new objective to align calibrated reward distributions. 
% We further extend CaPO to multi-reward scenarios by introducing a pair selection strategy for Pareto optimality. 
% Lastly, we equip our approach with timestep-aware loss weighting mechanism that enhances performance.
% Experimental results show that CaPO consistently outperforms prior methods, such as Direct Preference Optimization (DPO), in both single and multi-reward settings. 
% Especially, fine-tuning Stable Diffusion 3 medium with CaPO achieves state-of-the-art results on T2I benchmarks, including GenEval and T2I-Compbench.


Aligning text-to-image (T2I) diffusion models with preference optimization is valuable for human-annotated datasets, but the heavy cost of manual data collection limits scalability. Using reward models offers an alternative, however, current preference optimization methods fall short in exploiting the rich information, as they only consider pairwise preference distribution. Furthermore, they lack generalization to multi-preference scenarios and struggle to handle inconsistencies between rewards. To address this, we present Calibrated Preference Optimization (CaPO), a novel method to align T2I diffusion models by incorporating the general preference from multiple reward models without human annotated data. The core of our approach involves a reward calibration method to approximate the general preference by computing the expected win-rate against the samples generated by the pretrained models. Additionally, we propose a frontier-based pair selection method that effectively manages the multi-preference distribution by selecting pairs from Pareto frontiers. Finally, we use regression loss to fine-tune diffusion models to match the difference between calibrated rewards of a selected pair. Experimental results show that CaPO consistently outperforms prior methods, such as Direct Preference Optimization (DPO), in both single and multi-reward settings validated by evaluation on T2I benchmarks, including GenEval and T2I-Compbench. 
% Especially, fine-tuning Stable Diffusion 3 medium with CaPO achieves state-of-the-art results on T2I benchmarks, including GenEval and T2I-Compbench.


% Preference optimization for text-to-image (T2I) diffusion models has achieved notable success with large-scale human-annotated datasets, but the cost of data collection limits scalability. 
% Proxy reward models offer a sustainable alternative, yet optimizing with these signals is challenging due to their inconsistent distributions, particularly with multiple reward functions. 
% To address this, we introduce Calibrated Preference Optimization (CaPO), which enhances preference optimization through post-hoc reward calibration using pairwise comparisons and a new objective that aligns calibrated reward distributions. 
% CaPO also extends to multi-reward scenarios with a pair selection strategy for Pareto optimality and incorporates a timestep-aware loss weighting mechanism to boost performance. Experiments show that CaPO consistently outperforms prior methods, such as Direct Preference Optimization (DPO), in both single and multi-reward settings. Fine-tuning Stable Diffusion 3-medium with CaPO achieves state-of-the-art results on T2I benchmarks, including GenEval and T2I-Compbench.



% Aligning text-to-image (T2I) diffusion models with human preference requires massive human annotated data.

% reward signals remains challenging due to various practical constraints in existing approaches. 
% This paper presents Calibrated Preference Optimization (CaPO), a novel framework that addresses this challenge through effective multi-reward optimization.
% At its core, CaPO leverages expected win-rate over base model as an unified measure to combine diverse reward signals, where it is computed by empirical mean of pairwise comparison among generated images.
% Then, we introduce calibrated reward matching objective that fine-tunes T2I diffusion models through regressing the marginal difference between expected win-rates between samples, which shows better performance and robustness than that directly uses rewards.
% Furthermore, we employ a strategic Pareto frontier sampling method that selects training batch from upper and lower frontier to effectively capture the trade-offs between different reward signals.
% We also equip our approach with advanced diffusion loss-weighting scheme, which helps improving the performance and training stability.
% Through experiments on various T2I diffusion models, 
% we show that CaPO outperforms previous preference optimization methods (\emph{e.g.}, DPO) for both using single or multiple reward signals.
% Furthermore, our best model by fine-tuning Stable Diffusion 3-medium, achieves state-of-the-art performance evaluated by various T2I benchmarks such as GenEval and T2I-Compbench.
% table Diffusion 3
% Extensive experiments on SD3 and SDXL demonstrate consistent improvements in image-text alignment validated through various T2I benchmarks (\emph{e.g.}, GenEval and T2I-CompBench).
% Our comprehensive analyses validate the effectiveness of each components through detailed ablation studies.
% Comprehensive analyses show that CROWN not only outperforms Diffusion-DPO with human-curated datasets but also validates the effectiveness of each proposed component through detailed ablation studies.
% By applying CROWN on pretrained T2I diffusion models (\emph{e.g.}, SD3-medium and SDXL), the model demonstrates improved image-text alignment validated through various T2I benchmarks, and enhanced visual generation quality affirmed by human evaluation. 
% Comprehensive analyses validates the effectiveness of each proposed component, 

% Applying CROWN to pretrained T2I diffusion models such as SDXL and SD3-medium results in improved image-text alignment validated on various benchmarks including GenEval and T2I-Compbench, and enhanced visual quality validated by user studies.
% We fur
% Our experiments validate that CROWN improves the generation quality of T2I pretrained diffusion models including SDXL and SD3, on GenEval and T2I-Compbench benchmarks, especially in image-text alignment base models, \emph{e.g.}, SDXL and SD3-medium

% Our analysis shows that this enables better robustness than more robust 

% and  which is more robust than using rewards directly.  is more robust than using rewards directly.
% Then, we propose a calibrated reward matching objective that maximizes the expected win-rate, which is compu
% directly using reward which enables robust optimization than 
% Our key innovation lies in the introduction of expected win-rate as a unified measure in combining diverse reward signals, which is computed through pairwise comparisons across generated images, enabling more robust optimization than direct reward approaches. 
% The framework consists of three key components: a calibrated reward matching objective that leverages empirical win-rate statistics, a strategic Pareto frontier sampling strategy, and practical fine-tuning techniques including adaptive loss weighting. Extensive experiments on SDXL and SD3-medium demonstrate consistent improvements in image-text alignment on GenEval and T2I-CompBench benchmarks, with results further validated through human evaluation. Comprehensive analyses show that CROWN not only outperforms Offline Diffusion-DPO and human-curated datasets but also validates the effectiveness of each proposed component through detailed ablation studies.


% Post-training text-to-image (T2I) diffusion models 
% is non-scalable when using reward models,
% or requires expensive human collected dataset
% or requires differentiable rewards.
% In this paper, we propose a method to improve T2I diffusion models that seamlessly incorporates diverse reward signals.
% First, we propose to use calibrated win-rate instead of directly using the reward signals,

% with diverse reward signals by 


% Post-training of text-to-image models using reward signals can be challenging due to the noise inherent in learned reward models, which often results in sub-optimal performance. In this paper, we introduce Calibrated Preference Optimization, a robust framework for fine-tuning text-to-image diffusion models by maximizing the effective gain from reward signals. Our approach focuses on maximizing the win-rate over a pretrained model by approximating it through the average of pairwise win-rates across multiple samples from the pretrained model. This calibration technique reduces noise in win-rate approximation by leveraging sufficient sampling, leading to more reliable and robust outcomes. Additionally, we introduce Frontier-Based Rejection Sampling, which facilitates joint optimization across multiple reward signals by selecting pairs from the upper and lower frontiers through a non-dominated sorting algorithm. Extensive evaluations demonstrate that our method achieves superior performance, surpassing prior baselines across various benchmarks.
\end{abstract}