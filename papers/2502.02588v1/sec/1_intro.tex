\vspace{-0.2in}
\section{Introduction}
\label{sec:intro}

Recent text-to-image (T2I) diffusion models~\citep{saharia2022photorealistic, nichol2021glide, rombach2022high, esser2024scaling, betker2023improving} generate high-quality images from text prompts.
While these models perform well, synthesizing images that closely match subtle human preferences is a challenging task. 
Following the success of reinforcement learning from human feedback (RLHF) in language models~\citep{ouyang2022training}, training a reward model to mimic human preference~\citep{lee2023aligning, kirstain2023pick, xu2023imagereward, wu2023human, wu2023human2, zhang2024learning}, and fine-tuning diffusion models with RL algorithms shows promise~\citep{black2023training, fan2023dpok, deng2024prdp, lee2025parrot}. 
However, the computational expense of backpropagation through the diffusion trajectories limits the scalability to large-scale diffusion models. 
To address this problem, Diffusion-DPO~\citep{wallace2023diffusion} applies direct preference optimization (DPO)~\citep{rafailov2024direct} to diffusion models, with good results for large-scale diffusion models~\citep{esser2024scaling}. Nonetheless,
since Diffusion-DPO entails an labor-expensive paired human preference dataset,
it remains unclear how to leverage reward models to fine-tune large-scale T2I diffusion models without using human preference dataset.

Building on this line of research, we explore an alternative approach to fine-tune large-scale T2I diffusion models without relying on human preference datasets. Instead, we generate training data using pretrained T2I diffusion models and simulate human preferences through multiple reward models. Unlike Diffusion-DPO~\citep{wallace2023diffusion}, which relies on explicit pairwise preference data, our method fully leverages the rich knowledge embedded in reward signals. However, directly optimizing rewards risks overfitting and reward hacking if the reward values are not properly calibrated~\citep{gao2023scaling}.


\input{tables/fig_main}
% Building on this line of research, we explore an alternative approach to fine-tune large-scale T2I diffusion models without training on human preference dataset. 
% To accommodate the absence of human dataset, we collect training data by generating images from pretrained T2I diffusion models, and use reward models to simulate the human preference.
% Unlike Diffusion-DPO~\citep{wallace2023diffusion}, we seek to utilize the reward signals in fine-tuning to fully leverage the rich knowledge embedded in the rewards.
% However, directly using the rewards might result in misleading performance, if the reward values are not properly calibrated, which leads to reward over-optimization~\citep{gao2023scaling}.
To address this issue, we propose Calibrated Preference Optimization (CaPO) to enhance preference optimization of T2I diffusion models by improving how reward signals are used. 
Instead of directly optimizing reward values, we introduce the concept of general preference~\citep{azar2024general}, defined as the expected win-rate against a pretrained model. 
We approximate this by averaging pairwise win-rates among multiple samples, providing a robust and calibrated signal.
Our fine-tuning objective uses the regression loss to match the difference of calibrated rewards with the difference of implicit reward from diffusion models, which is simple and effective that enhances the performance.
We extend CaPO to multi-reward optimization problems by introducing a novel Frontier-based rejection sampling method.
This approach avoids the limitations of combining rewards with linear weights~\citep{clark2023directly, deng2024prdp} by selecting training pairs from the upper and lower Pareto frontiers using a non-dominated sorting algorithm. Jointly optimizing diverse reward signals enables the model to achieve balanced response to multiple rewards and mitigate the over-optimization problem when using a single reward.
Lastly, we propose an effective technique to improve the preference optimization of diffusion models by incorporating loss weighting scheme. 


Through extensive experiments, we show that CaPO consistently outperforms other fine-tuning methods including DPO~\citep{wallace2023diffusion}, achieving better alignment with human preferences across different benchmarks. 
Our contributions are:
\begin{itemize}
    \item We propose CaPO, which leverages a novel reward calibration method by incorporating approximated win-rates to fine-tune diffusion models and mitigate reward hacking;
    \item We expand the applicability of CaPO to multi-reward fine-tuning problems by introducing frontier-based rejection sampling to jointly optimize with diverse reward signals; 
    \item We demonstrate the effectiveness of CaPO with favorable visual generation quality against state-of-the-art models on benchmark datasets.
\end{itemize}
