\section{Preliminaries}
\label{sec:prelim}
We first describe the preliminaries on preference optimization for diffusion models before presenting our method. 
More details can be found in supplementary.
% detailed explanation in Appendix~\ref{appendix:prelim}.

%HERE
\vspace{1mm}
\noindent
{\bf Diffusion models.}
The denoising diffusion model~\citep{sohl2015deep, song2019generative, ho2020denoising, song2020score}  consists of forward processes, which gradually add noise to the data, and reverse processes, which generate data from noise. 
The forward process of a data $\mathbf{x}$ at time $t\in[0,1]$ forms a distribution $q(\mathbf{x}_t|\mathbf{x})$, given by $\mathbf{x}_t=\alpha_t\mathbf{x}+\sigma_t\eps$, where $\eps\sim\noise$, and $\alpha_t, \sigma_t$ are noise schedules. 
Let $\lambda_t = \log (\alpha_t^2 / \sigma_t^2)$ be log signal-to-noise ratio (SNR), then 
% noise schedules are designed to ensure small enough $\lambda_0$ so that $\mathbf{z}_0$ is close to data distribution, and $\mathbf{z}_1$ is close to Gaussian noise.
we express the diffusion training objective as a weighted $\eps$-prediction loss as in~\citep{kingma2023understanding}:
\begin{equation}
\label{eq:diffloss}
    \mathcal{L}_{\textrm{DM}}(\mathbf{x}) = \mathbb{E}_{t\sim\mathcal{U}(0,1),\eps}\big[-w_t \lambda_t' \|\eps_\theta(\mathbf{x}_t;t)-\eps\|_2^2\big]\text{,} 
\end{equation}
where $w_t$ is a weighting function, and $\lambda_t' = \mathrm{d}\lambda / \mathrm{d}t$.
Note that most of diffusion~\citep{karras2022elucidating} and flow matching~\citep{lipman2022flow} training objectives can be expressed in Eq.~\eqref{eq:diffloss} by choosing $w_t$ and $\lambda_t$.
The reverse process generates data by solving time-discretized SDE~\citep{song2020score} or ODE~\citep{song2020denoising, karras2022elucidating}, which gradually denoises Gaussian noise into data by using trained diffusion model.
Text-to-image diffusion models~\citep{nichol2021glide, ramesh2021zero, ramesh2022hierarchical, saharia2022photorealistic, rombach2022high} are conditional diffusion models that use text embeddings $\mathbf{c}$ from text encoders~\citep{radford2021learning, raffel2020exploring} as condition to generate image from text input. 
In this work, we denote $\eps_\theta(\mathbf{x}_t;\mathbf{c},t)$ as T2I diffusion model, and $p_\theta(\cdot|\mathbf{c})$ as distribution of the generated data given prompt $\mathbf{c}$. 

\vspace{1mm}
\noindent
{\bf Reward models.}
Given an image $\mathbf{x}$ and a condition $\mathbf{c}$, a reward model $R(\mathbf{x}, \mathbf{c})$ is a function that measures an utility of the input. 
The common approach is Bradley-Terry (BT) model~\citep{bradley1952rank, ouyang2022training}, which defines the preference distribution for a triplet $(\mathbf{c}, \mathbf{x}, \mathbf{x}')$:
\begin{equation}\label{eq:bt}
    \mathbb{P}(\mathbf{x} \succ \mathbf{x}' | \mathbf{c}) \coloneqq \sigma\big( R(\mathbf{x}, \mathbf{c}) - r(\mathbf{x}', \mathbf{c})\big )\text{,}
    % \frac{\exp (R(\mathbf{x}, \mathbf{c}))}{\exp (R(\mathbf{x}, \mathbf{c})) + \exp (r(\mathbf{x}', \mathbf{c}))}\text{.}
\end{equation}
where $\sigma(u) = (1+\exp(-u))^{-1}$ is a sigmoid function. 
% Suppose $(\mathbf{x}^+, \mathbf{x}^-)$ is a ranked dataset with $\mathbf{x}^+\succ \mathbf{x}^-$, then BT model is trained by using logistic loss on Eq.~\eqref{eq:bt}.


\vspace{1mm}
\noindent
{\bf Diffusion preference optimization.}
The goal of reward fine-tuning is to optimize the model $p_\theta$ that maximizes the expected reward of generated output, which comes with KL regularization to prevent reward over-optimization~\citep{gao2023scaling}:
\begin{equation}\label{eq:obj}
    \max_\theta~\mathbb{E}_{\mathbf{c}, \mathbf{x}\sim p_\theta(\mathbf{x}|\mathbf{c})}[R(\mathbf{x}, \mathbf{c})] - \beta D_{\textrm{KL}}\big(p_\theta(\cdot | \mathbf{c}) \| p_{\textrm{ref}}(\cdot|\mathbf{c})\big)\text{,}
\end{equation}
where $\beta$ is a hyperparameter that controls the divergence.
To solve Eq.~\eqref{eq:obj}, direct alignment methods, \emph{e.g.}, DPO~\citep{rafailov2024direct}, have been applied to diffusion models~\citep{wallace2023diffusion}.
At its core, it uses the closed-form solution of Eq.~\eqref{eq:obj}, which is given by $p^*(\mathbf{x}|\mathbf{c}) \propto p_{\textrm{ref}}(\mathbf{x}|\mathbf{c})\exp\big(\tfrac{1}{\beta}R(\mathbf{x}, \mathbf{c})\big)$.
By replacing $p^*$ with $p_\theta$ and rearranging for $r$, applying Eq.~\eqref{eq:bt} for a ranked data pair $(\mathbf{x}^+, \mathbf{x}^-)$ gives us following general preference optimization objective~\citep{tang2024generalized}:
\begin{equation}\label{eq:dpoloss}
    \ell(\theta) = g \bigg(\beta\log\frac{p_\theta(\mathbf{x}^+|\mathbf{c})}{p_{\textrm{ref}}(\mathbf{x}^+|\mathbf{c})} - \beta\log\frac{p_\theta(\mathbf{x}^-|\mathbf{c})}{p_{\textrm{ref}}(\mathbf{x}^-|\mathbf{c})}\bigg)\text{,}
\end{equation}
where $g$ is any convex loss function, \emph{e.g.}, $g(u) =-\log u$ gives us DPO~\citep{rafailov2024direct} objective, and $g(u) = (1-u)^2$ gives us identity preference optimization (IPO)~\citep{azar2024general} objective.
% where the data pair $(\mathbf{x}^+, \mathbf{x}^-)$ satisfies $\mathbf{x}^+ \succ \mathbf{x}^- | \mathbf{c}$ and $g$ is any convex loss function. 
% Similarly
% Note that other preference objectives can
% where $\sigma(u)=(1+\exp(-u))^{-1}$ is a sigmoid function and partition constant $Z(\mathbf{c})$ is canceled out.

However, directly applying Eq.~\eqref{eq:dpoloss} to diffusion models is not straightforward as the log-likelihoods of diffusion models are intractable.
\citet{wallace2023diffusion} propose a method to compute log-ratio and derive DPO loss for diffusion models by marginalizing the log-ratio through forward process $q(\mathbf{x}_{0:1}|\mathbf{x})$ to compute the log-ratio with $\eps$-prediction losses: 
\begin{equation*}\label{eq:diffratio}
    \mathbb{E}_{q(\mathbf{x}_{0:1}|\mathbf{x})}\bigg[\log \frac{p_\theta(\mathbf{x}_{0:1}|\mathbf{c})}{p_{\textrm{ref}}(\mathbf{x}_{0:1}|\mathbf{c})}\bigg] = \mathbb{E}_{t,\eps}\big[ R_\theta(\mathbf{x}_t, \mathbf{c}, t)\big]\text{,}%\quad\text{where}
\end{equation*}
where  $R_\theta(\mathbf{x}_t, \mathbf{c}, t)  \! = \!  \lambda_t'\big(\|\eps_\theta(\mathbf{x}_t;\mathbf{c},t) -$$ \eps\|_2^2 - \|\eps_{\textrm{ref}}(\mathbf{x}_t;\mathbf{c},t) -\eps\|_2^2\big)$ 
%R_\theta(\mathbf{x}_t, \mathbf{c}) = \lambda_t'\big(\|\eps_\theta(\mathbf{x}_t;\mathbf{c},t) - \eps\|_2^2 - \|\eps_{\textrm{ref}}(\mathbf{x}_t;\mathbf{c},t) -\eps\|_2^2\big)$ 
for $\mathbf{x}_t=\alpha_t\mathbf{x}+\sigma_t\eps$ with $t\sim\mathcal{U}(0,1)$ and $\eps\sim\noise$. 
By applying this to Eq.~\eqref{eq:dpoloss} and taking the expectation out of $g$ yields diffusion preference optimization objective:
\begin{equation}\label{eq:dffpo}
    \mathcal{L}(\theta) = \mathbb{E}_{t,\eps^+, \eps^-}\big[g\big(\beta R_\theta(\mathbf{x}_t^+, \mathbf{c}, t) - \beta R_\theta(\mathbf{x}_t^-,\mathbf{c}, t) \big)\big]\text{,}
\end{equation}
where $\mathbf{x}_t^+ \! =  \alpha_t\mathbf{x}^+ + \sigma_t\eps$, $\mathbf{x}_t^-=\alpha_t\mathbf{x}^- + \sigma_t\eps^-$ for $t\sim\mathcal{U}(0,1)$ and $(\eps^+, \eps^-)\sim \noise \times \noise $.



% where $\eps_\theta \coloneqq \eps_\theta(\mathbf{x}_t;\mathbf{c},t)$, $\eps_{\textrm{ref}} \coloneqq \eps_{\textrm{ref}}(\mathbf{x}_t;\mathbf{c},t)$ for $\mathbf{x}_t=\alpha_t\mathbf{x}+\sigma_t\eps$ and $\eps\sim\noise$.
% Then substituting Eq.~\eqref{eq:diffratio} to DPO loss, and taking out the expectation out of sigmoid loss yields Diffusion-DPO~\citep{wallace2023diffusion} loss given as follows:
% \begin{equation}
%     \ell_{\textrm{DPO}}(\theta) = \mathbb{E}_{t, \eps, \eps'}[-\log \sigma(\beta\lambda_t'(\|\eps_\theta-\eps\|^2 - \|\eps_{\textrm{ref}} - \eps\|_2^2))]\text{,}
% \end{equation}
% Since computing the log-likelihoods of diffusion models are intractable, \citet{wallace2023diffusion} proposed an method to approximate the log-ratio between fine-tuning and reference model by marginalizing through forward processes. Specifically, the log-ratio (\emph{i.e.}, the implicit reward) is computed by the difference between diffusion training losses, which we define as follows:
% % To apply DPO loss for diffusion models, \citet{wallace2023diffusion} proposed an approximation method to estimate the log-ratio, 
% % Meanwhile, since computing the likelihoods of diffusion models is infeasible, \citet{wallace2023diffusion} proposed a workaround to estimate the implicit reward using diffusion models. Specifically, this approximation leverage the marginalization through all forward processes, which results in difference between diffusion training losses. Formally, let us define the implicit reward by
% % results to compute the log-ratio in Eq.~\eqref{eq:reward} by using the diffusion training loss, which is given as follows:
% \begin{equation}\label{eq:diffusionreward}
%     Q_\theta(\mathbf{x}_t, \mathbf{c}, t) = w_t\lambda_t' (\|\eps_\theta(\mathbf{x}_t;\mathbf{c},t) - \eps\|_2^2 - \|\eps_{\textrm{ref}}(\mathbf{x}_t;\mathbf{c}, t) - \eps\|_2^2)\text{,} 
% \end{equation}
% where $\mathbf{x}_t = \alpha_t \mathbf{x} + \sigma_t \eps$ for $\eps\sim\noise$ and $t\in (0,1)$.
% Then, diffusion-DPO~\citep{wallace2023diffusion} loss for pair $(\mathbf{x}, \mathbf{x}')$ is given by
% \begin{equation}\label{eq:diffdpo}
%     \ell_{\textrm{DPO}}(\theta) = \underset{t, \eps, \eps'}{\mathbb{E}}\big[-\log \sigma\big( \beta\big(Q_\theta(\mathbf{x}_t;\mathbf{c}, t) - Q_\theta(\mathbf{x}_t', \mathbf{c}, t)\big)\big)\big]\text{,}
% \end{equation}
% where $t\sim\mathcal{U}(0,1)$ and $\eps, \eps'\sim\noise$.

% implicit reward function is computed by the marginalization over all forward processes, which allows computation 

% directly applying Eq.~\eqref{eq:dpoloss} is infeasible as computing the likelihood of diffusion model is 



% which shows the implicit relationship between the optimal distribution and reward function. 

% Then by substituting $p^*$ with $p_\theta$ in Eq.~\eqref{eq:reward}, we define an implicit reward function by the log-ratio between the fine-tuning and reference distribution, and optimize the implicit reward function to match the reward $r$.
% Note that original DPO does not use reward models, instead use labeled pair dataset and perform binary classification task. 
% Formally, given a pair $(\mathbf{x}_1, \mathbf{x}_2)$ from prompt $\mathbf{c}$ such that $\mathbf{x}_1$ is of preferred than $\mathbf{x}_2$, then DPO loss is given as follows:

% where $\sigma(u)=(1+\exp(-u))^{-1}$ is a sigmoid function and the partition function $Z(\mathbf{c})$ conveniently canceled out.
% If the reward model is given instead of labeled dataset, one can consider either approaches; 1) use reward model to generate hard label and perform DPO loss, or 2) compute soft label $\hat{p} = \sigma(r(\mathbf{x}_1,\mathbf{c}) - r(\mathbf{x}_2, \mathbf{c}))$ and perform conservative DPO (CDPO)~\citep{mitchell2024cdpo} loss given as follows:
% \begin{align}
% \begin{split}
%     \ell_{\textrm{CDPO}}(\theta) = &-\hat{p}\log \sigma\big(\Delta_\theta(\mathbf{x}_1, \mathbf{x}_2, \mathbf{c})\big) \\
%     &- (1-\hat{p})\log \big(1-\sigma\big(\Delta_\theta(\mathbf{x}_1, \mathbf{x}_2,\mathbf{c})\big)\big)\text{,}
% \end{split}
% \end{align}
% where $\Delta_\theta(\mathbf{x}_1, \mathbf{x}_2, \mathbf{c}) = \beta\log\frac{p_\theta(\mathbf{x}_1|\mathbf{c})}{p_{\textrm{ref}}(\mathbf{x}_1|\mathbf{c})} - \beta\log\frac{p_\theta(\mathbf{x}_2|\mathbf{c})}{p_{\textrm{ref}}(\mathbf{x}_2|\mathbf{c})}$. 


% Applying Eq.~\eqref{eq:reward} to diffusion models is not straightforward as we cannot compute the log-likelihood of $p_{\theta}(\mathbf{x}|\mathbf{c})$.
% Instead, following \cite{wallace2023diffusion}, we consider the implicit reward function $r(\mathbf{x}_{0:1}, \mathbf{c})$ on a sequence $z_{0:1}$ and marginalize through all forward processes $q(\mathbf{x}_{0:1}|\mathbf{x})$. 
% Specifically, we consider $r_\theta(\mathbf{x}, \mathbf{c}) = \mathbb{E}_{q(\mathbf{x}_{0:1}|\mathbf{x})}[r_\theta(\mathbf{x}_{0:1}, \mathbf{c})]$, where
% \begin{align}\label{eq:reward_diff}
%     r_\theta(\mathbf{x}_{0:1}, \mathbf{c}) = \beta\log \frac{p_\theta(\mathbf{x}_{0:1}|\mathbf{c})}{p_{\textrm{ref}}(\mathbf{x}_{0:1}|\mathbf{c})} - \beta\log Z(\mathbf{c})\text{.}
% \end{align}
% Let $\boldsymbol{\epsilon}_\theta$ and $\boldsymbol{\epsilon}_{\textrm{ref}}$ be diffusion model for each $p_\theta$ and $p_{\textrm{ref}}$, respectively. Then one can express the implicit reward function $r_\theta(\mathbf{x}, \mathbf{c})$ as follows:
% \begin{align}
% \begin{split}
%     &r_\theta(\mathbf{x}, \mathbf{c}) = -\tfrac{1}{2}\beta\lambda_t'\mathbb{E}_{\boldsymbol{\epsilon}, t} [\Delta(\mathbf{x}_t, \mathbf{c}, t)], \quad \text{where}\\
%     % -\tfrac{1}{2}\beta\lambda_t'\mathbb{E}_{\boldsymbol{\epsilon}, t} [
%     &\Delta(\mathbf{x}_t, \mathbf{c}, t) = \|\boldsymbol{\epsilon}_\theta(\mathbf{x}_t; \mathbf{c},t) - \boldsymbol{\epsilon}\|_2^2 - \|\boldsymbol{\epsilon}_\textrm{ref}(\mathbf{x}_t; \mathbf{c},t) - \boldsymbol{\epsilon}\|_2^2\text{,}
% \end{split}
% \end{align}
% for $t\sim\mathcal{U}(0,1)$ and $\boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{0}, \mathbf{I})$.
% Then by taking out the expectation, Diffusion-DPO~\cite{wallace2023diffusion} objective is given as follows:
% \begin{align}
%     \mathcal{L}_{\textrm{DPO}}(\theta) = \mathbb{E}_{t, \boldsymbol{\epsilon}}[-\log \sigma(\beta_t (\Delta(\mathbf{x}_t, \mathbf{c}, t) - \Delta(\mathbf{x}_t', \mathbf{c}, t)))]\text{,}
% \end{align}
% where $\beta_t = -\tfrac{1}{2}\beta \lambda_t'$


% Remark that original DPO considers labeled pair dataset, \emph{i.e.}, $(\mathbf{x}_1, \mathbf{x}_2)$ such that $\mathbf{x}_1$ 


% Then the reward optimization can be done by optimizing $p_\theta$ to match $p^*$ using Eq.~\eqref{eq:reward}.
% For instance, direct preference optimization (DPO)~\citep{rafailov2024direct} performs binary classification over a pair $(\mathbf{x}_1, \mathbf{x}_2)$ 
% For instance, direct preference optimizaiton (DPO)~\citep{rafailov2024direct} performs binary classification from a data pair to distill the reward. Formally, given a pair $(\mathbf{x}_1, \mathbf{x}_2)$ with $\mathbf{x}_1 \succ \mathbf{x}_2$

% match the  
% given a data pair $(\mathbf{x}_1, \mathbf{x}_2)$ from a prompt $\mathbf{c}$,
% For instance, direct preference optimization (DPO)~\citep{rafailov2023direct} conducts binary classification task by using sigmoid loss
% Since computing the log-ratio in Eq.~\eqref{eq:reward} is not accessible for diffusion models, \citet{wallace2023diffusion} proposed to approximate 
% When reward model is 






% % Given a data distribution $q(\mathbf{x})$ for an image $\mathbf{x}$, diffusion models~\citep{sohl2015deep, ho2020denoising, song2019generative} consider a series of latent variables $\mathbf{x}_t$ for timesteps $t\in[0,1]$. The forward process of a diffusion model defines the marginal density by $\mathbf{x}_t = \alpha_t\mathbf{x} + \sigma_t\boldsymbol{\epsilon}$ at each $t\in[0,1]$, where $\boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{0}, \mathbf{I})$ is a random noise, and $\alpha_t, \sigma_t$ are noise scheduling function of $t$. 
% % Here, let us denote $\lambda_t = \log (\alpha_t^2 / \sigma_t^2)$ be log-signal-to-noise ratio (log-SNR), then the noise schedule is chosen to satisfy such that $\lambda_1$ is large enough so that $\mathbf{z}_1$ is identical to pure Gaussian noise (\emph{i.e.}, $p(\mathbf{z}_1) \approx \mathcal{N}(\boldsymbol{0},\mathbf{I})$) and $\lambda_0$ is small enough that $\mathbf{z}_0$ is identical to data distribution. Then the generative reverse process starts from random noise $\mathbf{z}_1\sim\mathcal{N}(\boldsymbol{0}, \mathbf{I})$ and iteratively denoise through $\mathbf{z}_0$ to achieve sample.
% % We refer $p_\theta(\mathbf{x}|\mathbf{c})$ be a distribution of sample $\mathbf{x}$ drawn from diffusion model $p_\theta$ with context prompt $\mathbf{c}$.
% % For brevity, we denote $\mathbf{x}_{0:1} \coloneqq \mathbf{z}_0,\ldots, \mathbf{z}_1$, and $q(\mathbf{x}_{0:1}|\mathbf{x})$ be conditional distribution of a forward process $\mathbf{x}_{0:1}$ given data $\mathbf{x}$, and $p_\theta(\mathbf{x}_{0:1}|\mathbf{c})$ be distribution of sampled trajectories of generative process.

% % Text-to-image (T2I) diffusion models are class of diffusion models conditioned on text prompts, usually by extracting text embeddings from pre-trained text encoders~\cite{}. In general, T2I diffusion models are trained by minimizing following weighted loss:
% % % Then, the generative diffusion model $p_\theta$ is trained by minimizing following weighted loss
% % \begin{align}\label{eq:diffloss}
% %     \mathcal{L}_{\textrm{DM}}(\mathbf{x}) = -\tfrac{1}{2}\mathbb{E}_{t, \boldsymbol{\epsilon}} \big[w_t \lambda_t' \|\boldsymbol{\epsilon}_\theta(\mathbf{x}_t; \mathbf{c}, t) - \boldsymbol{\epsilon}\|_2^2\big]\text{,}
% % \end{align}
% % where $\mathbf{c}$ is a text condition, $t\sim\mathcal{U}(0,1)$, $\boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{0}, \mathbf{I})$, $w_t$ is a weighting function and $\lambda_t'$ is derivative of $\lambda_t$ over $t$. It is known that Eq.~\eqref{eq:diffloss} is equivalent to maximizing evidence lower bound (ELBO) if $w_t$ is a monotonic function of $t$~\cite{kingma2023understanding}.

% \subsection{Learning from Human Feedback}
% \paragraph{Reward models.} 
% One of the popular choice in learning a reward model is training a binary classifier to discriminate between preferred and non-preferred one.
% Given prompt $\mathbf{c}$ and samples $\mathbf{x}, \mathbf{x}'$, let us denote the ground-truth human preference probability by $\mathbb{P}^*(\mathbf{x}\succ \mathbf{x}'|\mathbf{c})$. Then one can define the preference of $\mathbf{x}$ over a distribution $p\coloneqq p(\cdot | \mathbf{c})$ by 
% \begin{align*}
%     \mathbb{P}^*(\mathbf{x} \succ p) = \mathbb{E}_{\mathbf{x}'\sim p(\mathbf{x}'|\mathbf{c})}[\mathbb{P}^*(\mathbf{x}\succ \mathbf{x}' | \mathbf{c})]\text{.}
% \end{align*}
% In practice, we assume Bradley-Terry (BT) model, which approximates the preference probability by $\mathbb{P}(\mathbf{x}\succ\mathbf{x}'|\mathbf{c})=\sigma(r(\mathbf{x},\mathbf{c}) - r(\mathbf{x}',\mathbf{c}))$, where $\sigma(u)=(1+\exp(-u))^{-1}$ is a sigmoid function, and $R(\mathbf{x}, \mathbf{c})$ is reward function from a logit of binary classifier.

% Given the learned reward function, our goal is to fine-tune the model $p_\theta$ to maximize the reward while minimizing the deviation from the reference model $p_{\textrm{ref}}$ by following KL-divergence regularized objective:
% \begin{align}\label{eq:rlhf}
%     \max_\theta J(\theta) = \mathbb{E}_{\mathbf{c}, \mathbf{x}\sim p_\theta}[r(\mathbf{x},\mathbf{c})] - \beta D_{\textrm{KL}}(p_\theta(\cdot|\mathbf{c}) \| p_{\textrm{ref}}(\cdot | \mathbf{c}))\text{,}
% \end{align}
% where $\beta > 0$ is a regularization hyperparameter. The objective can be optimized with RL (\emph{e.g.}, PPO~\cite{schulman2017proximal})), where the reward is given as $\hat{r}(\mathbf{x}, \mathbf{c}) = R(\mathbf{x}, \mathbf{c}) - \beta \log \frac{p_\theta(\mathbf{x}|\mathbf{c})}{p_{\textrm{ref}}(\mathbf{x}|\mathbf{c})}$.


% \paragraph{Preference Optimization.}
% An alternative approach is using direct preference optimization (DPO)~\cite{rafailov2023direct}, which leverages a closed-from optimal solution to Eq.~\eqref{eq:rlhf} without resorting to RL algorithms. 
% Given the optimal solution $p_{\theta^*}(\mathbf{x}|\mathbf{c}) = p_{\textrm{ref}}(\mathbf{x}|\mathbf{c}) \exp \big(\tfrac{1}{\beta} R(\mathbf{x}, \mathbf{c})\big) / Z(\mathbf{c})$, rearranging this equation to solve for reward lets us define the implicit reward function given as follows:
% \begin{align}\label{eq:reward}
%     r_\theta(\mathbf{x}, \mathbf{c}) = \beta \log \frac{p_{\theta}(\mathbf{x}|\mathbf{c})}{p_{\textrm{ref}}(\mathbf{x}|\mathbf{c})} - \beta \log Z(\mathbf{c})\text{,}
% \end{align}
% where $Z(\mathbf{c})$ is a partition function.
% Then given a dataset $(\mathbf{c}, \mathbf{x}, \mathbf{x}')\sim \mathcal{D}$ such that $\mathbf{x}\succ \mathbf{x}' | \mathbf{c}$, DPO minimizes following loss function: 
% \begin{align}
%     \mathcal{L}_{\textrm{DPO}}(\theta) = \mathbb{E}_{\mathcal{D}}\left[-\log \sigma(r_\theta(\mathbf{x}, \mathbf{c}) - r_\theta(\mathbf{x}', \mathbf{c}) )\right]\text{.}
% \end{align}


% \paragraph{Preference Optimization for diffusion models.}


% % Eq.~\eqref{eq:reward_diff}
% % and approximate the implicit reward function by
% % \begin{align}
% %     R(\mathbf{x}, \mathbf{c}) = \mathbb{E}_{q(\mathbf{x}_{0:1} | \mathbf{x})}[r_\theta(\mathbf{x}_{0:1}, \mathbf{c})]
% % \end{align}

% % , and marginalize through  

% % resort to the marginalization over all conditional distribution $q(\mathbf{x}_{0:1}| \mathbf{x})$ which allows approximation of the log-ratio by using diffusion model $\boldsymbol{\epsilon}_\theta$. 
% % Formally, let us consider the implicit reward function over $\mathbf{x}_{0:1}$, where it is given as
% % \begin{align}
% %     % r_\theta(\mathbf{x}_{0:1}, \mathbf{c}) = 
% %     \log \frac{p_\theta(\mathbf{x}_{0:1}|\mathbf{c})}{p_{\textrm{ref}}(\mathbf{x}_{0:1}|\mathbf{c})} = \log \frac{p_\theta(\mathbf{x}_{0:1}|\mathbf{c})}{q(\mathbf{x}_{0:1}|\mathbf{x})} -  \log\frac{p_\textrm{ref}(\mathbf{x}_{0:1}|\mathbf{c})}{q(\mathbf{x}_{0:1}|\mathbf{c})}
% % \end{align}

% % Formally, the implicit reward function for diffusion model $\boldsymbol{\epsilon}_\theta$ is given as
% % \begin{align}
% %     r_\theta = \mathbb{E}_{q(\mathbf{x}_{0:1}|\mathbf{x})} [ r_\theta(\mathbf{x}_{0:1}, \mathbf{c})]
% % \end{align}

% % However, i

% % optimizing BT model  $\mathcal{D}$
% % Then given a paired dataset $\mathcal{D}$, DPO use the binary cross-entropy loss function given as follows:
% % \begin{align}
% %     \mathcal{L}_{\textrm{DPO}}(\theta) = \mathbb{E}_{\mathcal{D}}\left[-\log \sigma\left(\beta\log\frac{p_\theta(\mathbf{x}|\mathbf{c})}{p_{\textrm{ref}}(\mathbf{x}|\mathbf{c})} - \beta\log\frac{p_\theta(\mathbf{x}'|\mathbf{c})}{p_{\textrm{ref}}(\mathbf{x}'|\mathbf{c})}\right)\right]    
% % \end{align}

% % optimize the 
% % for a paired dataset $\mathcal{D} = \{ (\mathbf{x}, \mathbf{x}', \mathbf{c}\}$
% % Alternatively, direct preference optimization (DPO)~\cite{rafailov2023direct} considers using the closed-form solution to Eq.~\eqref{eq:rlhf} without using RL algorithms. there is a line of research in direct alignment method which directly model the reward function by using the closed-form solution to the Eq.~\eqref{eq:rlhf} as follows: 
% % \begin{align}
% %     p_{\theta^*}(\mathbf{x}|\mathbf{c}) = p_{\textrm{ref}}(\mathbf{x}|\mathbf{c}) \exp (R(\mathbf{x}, \mathbf{c}) / \beta) / Z(\mathbf{c})\text{,}
% % \end{align}
% % where $\theta^*$ is a optimal solution to Eq.~\eqref{eq:rlhf} and $Z(\mathbf{c})$ is a partition function. 

% % \paragraph{Preference Optimization for diffusion models.}
% % For diffusion models, we aim to solve following:
% % \begin{align}
% %     \max_\theta \mathbb{E}_{\mathbf{c}, \mathbf{x}_{0:1}\sim p_\theta(\mathbf{x}_{0:1}|\mathbf{c})}[r(\mathbf{x}_{0:1}, \mathbf{c})] - \beta D_{\textrm{KL}}( p_\theta(\mathbf{x}_{0:1}|\mathbf{c}) \| p_{\textrm{ref}}(\mathbf{x}_{0:1}|\mathbf{c}))\text{,}
% % \end{align}
% % where the optimal solution of $r(\mathbf{x}_{0:1},\mathbf{c})$ is given as
% % \begin{align}
% %     r(\mathbf{x}_{0:1}, \mathbf{c}) = \beta \log \frac{p_\theta(\mathbf{x}_{0:1}|\mathbf{c})}{p_{\textrm{ref}}(\mathbf{x}_{0:1}|\mathbf{c})}\text{,}
% % \end{align}
% % and the total reward is marginalized through all $\mathbf{x}_{0:1} \sim q(\mathbf{x}_{0:1} | \mathbf{x})$ such that 
% % \begin{align}
% %     R(\mathbf{x}, \mathbf{c}) = \mathbb{E}_{q(\mathbf{x}_{0:1}|\mathbf{x})} [r(\mathbf{x}_{0:1}, \mathbf{c})]\text{.}
% % \end{align}
% % Since it is unavailable to directly evaluate $\log p_\theta$ and $\log p_\textrm{ref}$, the following  
