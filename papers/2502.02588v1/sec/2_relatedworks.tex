
\section{Related Work}
\label{sec:relatedworks}
\noindent
{\bf Modeling human preference for visual generation.}
Motivated by the recent success in incorporating human preference modeling to convert language models into advanced chatbot models, numerous methods have been developed to transfer the success into visual generation.
\citet{lee2023aligning} demonstrates the capability of training reward models according to human preference using a small dataset. 
Subsequently, numerous reward models~\citep{kirstain2023pick, xu2023imagereward, wu2023human, wu2023human2, zhang2024learning} for text-to-image generation have been proposed by fine-tuning a vision-language model (\emph{e.g.}, CLIP~\citep{radford2021learning} or BLIP~\citep{li2022blip}) with Bradley-Terry model
based on a paired human preference dataset.
Alternatively, \citet{lin2025evaluating} uses a multi-modal large language models to exploit the knowledge of LLMs by performing visual question answering task to measure the alignment between texts and images.
% bi-directional vision-language model with large instruction-tuned language models~\citep{chung2024scaling}, which achieves better alignment between texts and images for text-to-image generation. 
While existing reward models can operate as a proxy for ground-truth reward models, the inherent noise within the data due to finite capacity and coverage, 
inevitably affect the performance negatively when used for fine-tuning. 
%
Our work addresses the above-mentioned issues by introducing a calibration method that approximates the win-rate, rather than using the rewards. 

\noindent
{\bf Fine-tuning diffusion models with rewards.}
Numerous methods have been developed for fine-tuning T2I diffusion models with reward models~\citep{lee2023aligning, black2023training, fan2023dpok, deng2024prdp, clark2023directly, lee2025parrot, wallace2023diffusion}.
By formulating discrete diffusion sampling process as a reinforcement learning problem, \citet{black2023training} and \citet{fan2023dpok} develop fine-tuning diffusion models with policy gradient algorithms. 
However, those methods are computationally expensive and the training processes are usually unstable.  
While \citet{deng2024prdp} presents a scheme to scale RL fine-tuning for large-scale prompt dataset, it is not clear whether this approach can be applied to large-scale diffusion models. 
Instead of using RL, \citep{clark2023directly} proposes to directly fine-tune diffusion models by using the gradients from reward models. Yet, those approaches can only be applied to differentiable reward models, and extending to large-scale reward models (\emph{e.g.}, LLMs) is computationally prohibitive in practice.
Inspired by the success of DPO~\citep{rafailov2024direct}, \citet{wallace2023diffusion} introduce Diffusion-DPO, which can effectively alleviate the computational loads and can be applied to large-scale diffusion models~\citep{esser2024scaling}. 