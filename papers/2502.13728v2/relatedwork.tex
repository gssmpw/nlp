\section{Related Work}
\label{sec:related}

Dataset Distillation (DD, hereafter) \cite{wang2018dataset} has recently emerged as a novel paradigm to synthesize a significantly
smaller dataset from a large dataset, aiming to maintain the same training accuracy performance as if it was trained on the original large dataset. 
In this section, we will describe the proposals that combine FL with Distillation.
The work presented in \cite{li2019fedmd,zhu2021data,jeong2018communication,lin2020ensemble,afonin2021towards} leverage Knowledge Distillation (KD, for short) to transfer knowledge from local client models to a centralized FL server model to improve FL performance.
In particular, \cite{li2019fedmd} describes FedMD, a new FL framework that allows participants to independently design their models and maintain them private due to privacy and intellectual property concerns. To collaborate with each client, owning a private dataset and a uniquely designed model, FedMD relies on KD to translate its learned knowledge into a standard format.
The authors of \cite{zhu2021data} try to solve the classical FL issue of data heterogeneity (i.e., data from the real world are usually not independent and identically distributed) by proposing a data-free Knowledge Distillation approach for FL. This framework extracts the knowledge from users without depending on any external data through KD and then it regulates local model updating using this extracted knowledge.
Instead, the proposed framework called FD \cite{jeong2018communication} utilizes distillation to reduce FL communication costs. It synchronizes logits per label which are accumulated during the local training. The averaged
logits per label (over local steps and clients) are exploited as a distillation regularizer for the next round's local training. Moreover, Lin et al. \cite{lin2020ensemble} propose ensemble distillation for model fusion with the aim of training
the central classifier through unlabeled data on the outputs of the models from the clients. 
Finally, Afonin and Karimireddy \cite{afonin2021towards}
describe a theoretical framework, called Federated Kernel
ridge regression, which is a model-agnostic FL scheme allowing each agent to train
their model of choice on the combined dataset.
All the above works propose KD-based Federated Learning protocols with a purpose that is quite different from ours. Indeed, firstly, they deal with Knowledge Distillation instead of Data Distillation, secondly, we aim to design a new scheme to distribute DD on FL, but we do not aim to improve the FL protocol through Distillation.

Only a little research has explored Dataset Distillation approaches in FL. For instance, the strategies proposed in \cite{zhou2020distilled,song2023federated,cazenavette2022dataset} try to reduce the communication cost of FL while achieving comparable performance by exploiting DD. In these protocols, instead of sharing model updates, the clients distill
the local datasets independently in just one round, and then they send the synthetic data (e.g. images or sentences) to the central server that aggregates those decentralized distilled datasets. In these works, similarly to ours, DD is used instead of KD. In particular, Zhou et al. \cite{zhou2020distilled} propose a method combining DD and distributed one-shot learning. For every local update step, each synthetic data successively updates the network for one gradient descent step. Thus, synthetic data is closely related to one specific network weight, and the eavesdropper cannot reproduce the result with only leaked synthetic data.

As already said we employ a different perspective. Indeed in the above-cited approaches, DD has been applied to FL to develop a
privacy-preserving distributed model training scheme such that
multiple clients collaboratively learn a model without sharing
their private data. Instead of transmitting model updates as the standard way of FL, which may cause demanding communication costs they transmit the locally generated synthetic datasets proved for
privacy protection and essence information preservation. Instead, we aim to improve the Dataset Distillation framework by federating it.
To the best of our knowledge, our proposed scheme is the first of its kind.