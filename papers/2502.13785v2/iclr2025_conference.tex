
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
% \usepackage{times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{graphicx} 
\usepackage{url}


\title{Helix-mRNA: A Hybrid Foundation Model For Full Sequence mRNA Therapeutics}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
% \thanks{ Equal contribution to the work.} \thanks{Model implementation and weights can be found at \\ \url{https://github.com/helicalAI/helical} \\ \url{https://huggingface.co/helical-ai/helix-mRNA}} \\
\author{Matthew Wood \\
Helical\\
Luxembourg \\
\texttt{\small matthew@helical-ai.com} \\
\And
Mathieu Klop \\
Helical \\
Luxembourg \\
\texttt{\small mathieu@helical-ai.com}
\And
Maxime Allard
\thanks{Corresponding Author} \\
Helical \\
Luxembourg \\
\texttt{\small maxime@helical-ai.com}
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
mRNA-based vaccines have become a major focus in the pharmaceutical industry. The coding sequence as well as the Untranslated Regions (UTRs) of an mRNA can strongly influence translation efficiency, stability, degradation, and other factors that collectively determine a vaccineâ€™s effectiveness. However, optimizing mRNA sequences for those properties remains a complex challenge. Existing deep learning models often focus solely on coding region optimization, overlooking the UTRs. We present Helix-mRNA, a structured state-space-based and attention hybrid model to address these challenges. In addition to a first pre-training, a second pre-training stage allows us to specialise the model with high-quality data. We employ single nucleotide tokenization of mRNA sequences with codon separation, ensuring prior biological and structural information from the original mRNA sequence is not lost. Our model, Helix-mRNA, outperforms existing methods in analysing both UTRs and coding region properties. It can process sequences 6x longer than current approaches while using only 10\% of the parameters of existing foundation models. Its predictive capabilities extend to all mRNA regions.
We open-source the model (\url{https://github.com/helicalAI/helical}) and model weights (\url{https://huggingface.co/helical-ai/helix-mRNA}).
% \url{https://github.com/helicalAI/helical} \\ \url{https://huggingface.co/helical-ai/helix-mRNA}} 
\end{abstract}
% \section{Submission of conference papers to ICLR 2025}

% ICLR requires electronic submissions, processed by
% \url{https://openreview.net/}. See ICLR's website for more instructions.

% If your paper is ultimately accepted, the statement {\tt
%   {\textbackslash}iclrfinalcopy} should be inserted to adjust the
% format to the camera ready requirements.

% The format for the submissions is a variant of the NeurIPS format.
% Please read carefully the instructions below, and follow them
% faithfully.

% \subsection{Style}

% Papers to be submitted to ICLR 2025 must be prepared according to the
% instructions presented here.

% %% Please note that we have introduced automatic line number generation
% %% into the style file for \LaTeXe. This is to help reviewers
% %% refer to specific lines of the paper when they make their comments. Please do
% %% NOT refer to these line numbers in your paper as they will be removed from the
% %% style file for the final version of accepted papers.

% Authors are required to use the ICLR \LaTeX{} style files obtainable at the
% ICLR website. Please make sure you use the current files and
% not previous versions. Tweaking the style files may be grounds for rejection.

% \subsection{Retrieval of style files}

% The style files for ICLR and other conference information are available online at:
% \begin{center}
%    \url{http://www.iclr.cc/}
% \end{center}
% The file \verb+iclr2025_conference.pdf+ contains these
% instructions and illustrates the
% various formatting requirements your ICLR paper must satisfy.
% Submissions must be made using \LaTeX{} and the style files
% \verb+iclr2025_conference.sty+ and \verb+iclr2025_conference.bst+ (to be used with \LaTeX{}2e). The file
% \verb+iclr2025_conference.tex+ may be used as a ``shell'' for writing your paper. All you
% have to do is replace the author, title, abstract, and text of the paper with
% your own.

% The formatting instructions contained in these style files are summarized in
% sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

% \section{General formatting instructions}
% \label{gen_inst}

% The text must be confined within a rectangle 5.5~inches (33~picas) wide and
% 9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
% Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
% preferred typeface throughout. Paragraphs are separated by 1/2~line space,
% with no indentation.

% Paper title is 17~point, in small caps and left-aligned.
% All pages should start at 1~inch (6~picas) from the top of the page.

% Authors' names are
% set in boldface, and each name is placed above its corresponding
% address. The lead author's name is to be listed first, and
% the co-authors' names are set to follow. Authors sharing the
% same address can be on the same line.

% Please pay special attention to the instructions in section \ref{others}
% regarding figures, tables, acknowledgments, and references.


% There will be a strict upper limit of 10 pages for the main text of the initial submission, with unlimited additional pages for citations. 

% \section{Headings: first level}
% \label{headings}

% First level headings are in small caps,
% flush left and in point size 12. One line space before the first level
% heading and 1/2~line space after the first level heading.
\section {Introduction}
Engineered messenger Ribonucleic Acids (mRNAs) have emerged as a versatile platform for advanced therapeutics and vaccines, offering rapid design, scalable production, and flexible administration  \cite{pardi2018mrna}. The swift deployment of COVID-19 mRNA vaccines exemplified these advantages, reducing conventional vaccine development timelines from years to months \cite{park2021covid}. Beyond infectious disease, mRNA-based immunotherapies have shown durable anti-tumour efficacy in cancer \cite{sahin2017personalized}. Further, optimizing mRNA for stability and translation efficiency enables high-yield protein expression in both microbial and mammalian hosts \cite{eisenhut2020systematic}. These advances underscore the broad applicability of mRNA engineering for biomedical and industrial needs, ranging from gene therapies to large-scale protein manufacturing. Deep learning methods have shown promise in further accelerating mRNA therapeutic and vaccine design \cite{castillo2024optimus}, spanning traditional architectures to foundation models, which are models trained on large datasets that serve as building blocks for different downstream tasks\cite{yazdanijahromi2024helm}. 

Current approaches face three key limitations that particularly impact mRNA sequence analysis. First, the short context length used by current approaches is problematic because mRNA sequences vary dramatically in length, from only a few nucleotides to several thousand, making it difficult to capture long-range dependencies effectively. Second, simplified tokenization schemes risk losing important biological structures that exist in the original sequences \cite{yazdanijahromi2024helm}. Third, existing models like HELM \cite{yazdanijahromi2024helm}, CodonBERT \cite{codonbert2024}, and Optimus 5-Prime \cite{castillo2024optimus} are specialised for specific mRNA regions, making them inflexible and inefficient to adapt to other mRNA regions without complete retraining. 
% This underscores the importance of developing a general, robust base model that can serve as a foundation for diverse applications. Attention only approaches make it difficult to capture long sequences due to their quadratic scaling nature \cite{vaswani2023attentionneed}. Selective State Models (SSMs) overcome attention's fixed-length constraint by sequentially processing inputs while maintaining a hidden state that preserves contextual information from previous inputs \cite{gu2022s4}, \cite{gu2024mambalineartimesequencemodeling}, \cite{nguyen2023hyenadna}. Although new SSMs are competitive with attention-based models, they do not outperform attention-based models on all tasks \cite{dao2024mamba2}. Hybrid foundation models emerged recently combining the benefits of both attention-based models and SSMs \cite{nvidia2024hybrid}. 

To address these limitations, we introduce Helix-mRNA, a hybrid (attention-based~\cite{vaswani2023attentionneed} and state-space-based~\cite{gu2022s4,gu2024mambalineartimesequencemodeling}) foundation model capable of capturing full-length mRNA sequences. We employ a two-stage pre-training method that addresses task-specific specialisation challenges. Our approach features single nucleotide tokenization with a structural codon representation. Helix-mRNA outperforms current models across various downstream benchmarks. These tasks include prediction of translation efficiency, stability and degradation from UTR and codon information. 

\section{Methods}
\label{method}
We break down the different components of Helix-mRNA and how each of them overcome current challenges in the mRNA modality. Helix-mRNA enables long sequence processing through a hybrid state-based and attention architecture, achieving single nucleotide resolution and still maintaining precise coding region representation. Further, we use a two stage pre-training approach through Warmup-Stable-Decay (WSD) scheduling \cite{hu2024wsd_scheduler}, allowing for a balance between generalisation and specialisation. 

\subsection{Hybrid Architecture}
\label{architecture}

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.5]{images/architecture_embedding_combined.pdf}
\end{center}
\caption{\textbf{A)} Helix-mRNA hybrid architecture incorporating state-based and attention-based approaches. We show how we retain coding region structure with an additional token highlighted in the figure. \textbf{B)} Embeddings from initial pre-training, generated using only coding regions from mRNA sequences not seen during training.}
\label{architecture_emb_multi_figure}
\end{figure}

We train a 5.19 million parameter model, 10\% of the parameters of Transformer HELM \cite{yazdanijahromi2024helm}, with a hybrid SSM and attention-based architecture shown in Figure \ref{architecture_emb_multi_figure} taking inspiration from Nemotron \cite{parmar2024nemotron415btechnicalreport}, Jamba \cite{lieber2024jambahybridtransformermambalanguage}, Zamba \cite{glorioso2024zambacompact7bssm} and Samba \cite{ren2024samba}. The ratio of Mamba-2 layers, MLP layers, and attention layers was selected from the different ablation studies performed by \cite{nvidia2024hybrid} with 44.4\% Mamba-2 layers, 44.4\% MLP layers and 11.1\% attention layers. By preceding attention layers with Mamba-2 layers~, the recurrent nature of the Mamba-2 layer encodes positional information needed by the attention layer \cite{vaswani2023attentionneed,dao2024mamba2}. This architectural approach enables Helix-mRNA to read long mRNA sequences with high granularity, combining state-based long sequence handling with the in-context retention capabilities of attention-based methods.



\subsection{Single Nucleotide and Codon Encoding }
\label{tokenization}
Long mRNA sequences are traditionally computationally challenging, addressed by analysing subregions of the sequence \cite{yazdanijahromi2024helm} and coarser tokenization methods that group nucleotides together \cite{codonbert2024}. With our hybrid architecture enabling long sequence lengths, we tokenize full-length sequences at single nucleotide resolution. We map each base (\textit{A, C, U,} and \textit{G}) to a unique integer, ensuring the preservation of complete biological information without any data loss. A key focus of our approach is the mRNA coding region, which plays a critical role in translation efficiency, protein expression, and mRNA stability \cite{boel2016codon}, \cite{hanson2018codon}. To emphasize the importance of codon structure and enhance pattern extraction, we introduce a special character \textit{E} to clearly denote codon separation. We pre-train on a maximum input length of 12288 tokens, 6x the context length of Transformer HELM \cite{yazdanijahromi2024helm}. 
% Although this approach increases the context length needed, it both maintains single nucleotide resolution and preserves the full sequential and structural information of the mRNA molecule, enabling a more accurate analysis of its biological properties.

\subsection{Diverse mRNA Sequences Across Multiple Phyla}
\label{data_selection}
We use a taxonomically diverse pre-training dataset which was specifically curated to capture both deep evolutionary conservation patterns in eukaryotic sequences and the distinct nucleotide compositions and structural characteristics of viral genomes. The broad phylogenetic coverage enables the model to learn robust representations of sequence features across different evolutionary distances and genomic architectures. We gathered all RefSeq mRNA sequences\footnote{\url{{https://ftp.ncbi.nih.gov/refseq/release/}}} from a diverse set of eukaryotes, including vertebrates (mammals and non-mammals), plants, invertebrates, fungi, and 238 clinically relevant viruses \cite{o2016reference} to pre-train our Helix-mRNA model (See Appendix~\ref{supp:data}).

\subsection{Two-Stage Specialised Pre-Training}
\label{pre-training}
We utilise a two stage autoregressive unsupervised pre-training strategy, guided by the WSD learning rate scheduler \cite{hu2024wsd_scheduler}. In the first stage, the WSD scheduler manages a warm-up phase followed by a stable learning rate, allowing the model to process data of varying quality, creating a general and robust base model. In the second stage, the WSD scheduler transitions into its decay phase, where training focuses exclusively on high-quality data. During the second phase, human only mRNA sequences are used to refine the model for human specific tasks. This two stage process enables efficient learning from mixed-quality data in the initial stage and achieving specialised distillation through the targeted second stage.

\section{Results}\label{results}
\begin{table}[!h]
\label{benchmark_results}
\caption{Benchmark comparisons showing Helix-mRNA, CodonBERT, Transformer HELM and Transformer XE.
% Average accuracy across the test set is shown for the E. coli Proteins classification task.
Spearman rank correlations are shown for all tasks.}
\begin{center}
\begin{tabular}{lcccc}
\label{benchmark_results}
\textbf{Task} & \textbf{CodonBERT} & \textbf{HELM} & \textbf{XE} & \textbf{Helix-mRNA} 
\\ \hline \\
\text{MLOS Flu Vaccines} & 0.54 & 0.70 & 0.65 & \textbf{0.79$\pm$}0.121 \\
\text{mRFP Expression} & 0.77 & 0.85 & 0.82 & \textbf{0.86$\pm$}0.008 \\
% \text{E.\ coli Proteins} & \textbf{0.57} & - & - & 0.56 \\
\text{mRNA Stability/iCodon} & 0.35 & \textbf{0.53} & 0.50 & 0.52$\pm$0.004 \\
\text{Tc-Riboswitch} & 0.50 & 0.63 & 0.57 & \textbf{0.64}$\pm$0.033 \\
\text{Vaccine Degradation} & 0.78 & 0.83 & 0.80 & \textbf{0.84$\pm$}0.032 \\
% \text{Fungal Expression (S)} & \textbf{0.89} & - & - & - \\
\hline
\end{tabular}
\end{center}
\end{table}

After the first stage of pre-training, we use Uniform Manifold Approximation (UMAP) to visualise the embeddings of unseen coding sequences from each of the different phyla used during pre-training. 
% The embeddings are derived from the hidden state of the final non-special token after processing through the final layer. 
Figure \ref{architecture_emb_multi_figure} shows that Helix-mRNA clearly clusters coding sequences from different phyla through unsupervised pre-training alone. We demonstrate that despite pre-training on full sequences, Helix-mRNA can extract relevant information from different subregions without the need for further training. Viruses cluster very clearly and separately from all other eukaryotic phyla in the embedding space. This distinct separation implies that the model has learned biologically meaningful features, such as differences in codon usage and sequence structure, enabling it to discriminate viral from non-viral sequences. 


% \begin{figure}[h]
% \begin{center}
% \label{embedding_figures_cds}
% \includegraphics[scale=0.15]{images/codon_only_embs.pdf}
% \end{center}
% \caption{Helix-mRNA embeddings from initial pre-training, generated using coding sequence only on mRNA sequences not seen during training.}
% \end{figure}
% We validate Helix-mRNA through downstream benchmarks that demonstrate the modelâ€™s broad applicability and performance.

\subsection{Biological Downstream Tasks}
We first evaluate Helix-mRNA on coding region related property prediction tasks including stability, degradation and translation efficiency. Our results demonstrate that Helix-mRNA outperforms existing approaches like CodonBert, Transformer HELM, and Transformer XE across multiple benchmarks, including an E. coli related tasks even though Helix-mRNA was not pre-trained on any prokaryotic data, further highlighting generalisability to various tasks (See Appendix~\ref{ref:supp_eval}. We assessed performance using the following benchmarks where we use a 70\% training split, a 15\% validation split and a 15\% test split:
\begin{itemize}
    \item mRNA Stability/iCodon : Stability profiles from vertebrates \cite{diez2022icodon}.
    \item mRFP Expression : Protein production levels in E. coli variants \cite{nieuwkoop2023revealing}.
    \item MLOS dataset: Haemagglutinin antigen encodings for flu vaccines \cite{codonbert2024}.
    \item Tc-Riboswitches : Tetracycline riboswitch dimers with switching factor measurements \cite{groher2018tuning}.
    \item SARS-CoV-2 Vaccine Degradation : mRNA sequences optimized for structural features \cite{leppek2022combinatorial}.
\end{itemize}


% During benchmarking a large percentage of the model layers are frozen, with only a few layers and the task-specific fine-tuning head being updated during training. This further emphasizes the importance of pre-training, as the majority of the model weights remain unchanged. These benchmarks vary in both sequence length and samples available, showing Helix-mRNA's ability to perform in the low data fine-tuning domain.

\subsection{UTR5 Mean Ribosome Load Prediction}
\label{utr5}

\begin{figure}[h]
\begin{center}
\label{optimus_helix}
\includegraphics[scale=0.4]{images/optimus_helix.pdf}
\end{center}
\caption{Benchmark comparison between Helix-mRNA and Optimus 5-Prime on task specific fine-tuning to predict Mean Ribosome Load (MRL) across 3 cell lines (HEK293T, T cells, and HepG2) using two replicates, reproduced from the Optimus 5-Prime codebase released with the paper \cite{castillo2024optimus}. Results show the $r^2$ correlation values between the predicted MRL and the true MRL.}
\end{figure}
The benchmarks presented in Table \ref{benchmark_results} focus solely on the coding region. However, since Helix-mRNA is capable of utilising both UTR regions and coding regions, we evaluate the model on tasks specific to the 5â€™ UTR. We evaluated our modelâ€™s predictive capabilities by replicating the tasks from \cite{castillo2024optimus}, predicting Mean Ribosome Load (MRL) across three cell lines (HEK293T, T cells, and HepG2) using two replicates, shown in Figure \ref{optimus_helix}. Fine-tuning for 5 epochs enabled us to outperform Optimus 5-Prime on MRL prediction tasks, with the greatest improvements seen in T cells and HepG2 cell lines, where Optimus 5-Prime struggles the most.  By applying our model to their established tasks for cell line specific MRL prediction, we demonstrate our approachâ€™s performance and generalisability to different downstream tasks, further highlighting our modelâ€™s versatility.

\section{Conclusion}
In this work we presented Helix-mRNA, a hybrid architecture combining attention mechanisms and SSMs, taking inspiration from recent advances in long-context natural language models \cite{nvidia2024hybrid}, allowing for longer context lengths and better in-context retention capabilities. The extra context length combined with single-nucleotide and codon structure tokenization allows the model to capture additional features useful in mRNA sequence design, namely codons, UTR regions, and secondary structures. We show that our tokenization methods doesn't impact performance and allows us to use the model in coding region only tasks, outperforming Transformer HELM \cite{yazdanijahromi2024helm}, Transformer XE and CodonBERT \cite{codonbert2024} or in UTR 5' region specific tasks by outperforming Optimus 5-Prime \cite{castillo2024optimus}. 
% Furthermore, our 2-stage pre-training still preserves differences seen between samples from different phyla in the embedding space, showing that keeping a large diverse initial dataset is useful even with different donwstream strategies. Such versatility addresses the multifaceted requirements of mRNA design, from vaccine development and immunotherapy to gene therapy. 
By effectively merging state-of-the-art sequence modelling techniques with two-stage pre-training, Helix-mRNA paves the way for more robust, efficient, and broadly applicable mRNA therapeutics in both clinical and industrial domains. 

% \subsection{Headings: second level}

% Second level headings are in small caps,
% flush left and in point size 10. One line space before the second level
% heading and 1/2~line space after the second level heading.

% \subsubsection{Headings: third level}

% Third level headings are in small caps,
% flush left and in point size 10. One line space before the third level
% heading and 1/2~line space after the third level heading.

% \section{Citations, figures, tables, references}
% \label{others}

% These instructions apply to everyone, regardless of the formatter being used.

% \subsection{Citations within the text}

% Citations within the text should be based on the \texttt{natbib} package
% and include the authors' last names and year (with the ``et~al.'' construct
% for more than two authors). When the authors or the publication are
% included in the sentence, the citation should not be in parenthesis using \verb|\citet{}| (as
% in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
% should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
% towards AI~\citep{Bengio+chapter2007}.'').

% The corresponding references are to be listed in alphabetical order of
% authors, in the \textsc{References} section. As to the format of the
% references themselves, any style is acceptable as long as it is used
% consistently.

% \subsection{Footnotes}

% Indicate footnotes with a number\footnote{Sample of the first footnote} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches
% (12~picas).\footnote{Sample of the second footnote}

% \subsection{Figures}

% All artwork must be neat, clean, and legible. Lines should be dark
% enough for purposes of reproduction; art work should not be
% hand-drawn. The figure number and caption always appear after the
% figure. Place one line space before the figure caption, and one line
% space after the figure. The figure caption is lower case (except for
% first word and proper nouns); figures are numbered consecutively.

% Make sure the figure caption does not get separated from the figure.
% Leave sufficient space to avoid splitting the figure and figure caption.

% You may use color figures.
% However, it is best for the
% figure captions and the paper body to make sense if the paper is printed
% either in black/white or in color.
% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \end{center}
% \caption{Sample figure caption.}
% \end{figure}

% \subsection{Tables}

% All tables must be centered, neat, clean and legible. Do not use hand-drawn
% tables. The table number and title always appear before the table. See
% Table~\ref{sample-table}.

% Place one line space before the table title, one line space after the table
% title, and one line space after the table. The table title must be lower case
% (except for first word and proper nouns); tables are numbered consecutively.

% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

% \section{Default Notation}

% In an attempt to encourage standardized notation, we have included the
% notation file from the textbook, \textit{Deep Learning}
% \cite{goodfellow2016deep} available at
% \url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
% is not required and can be disabled by commenting out
% \texttt{math\_commands.tex}.


% \centerline{\bf Numbers and Arrays}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1in}p{3.25in}}
% $\displaystyle a$ & A scalar (integer or real)\\
% $\displaystyle \va$ & A vector\\
% $\displaystyle \mA$ & A matrix\\
% $\displaystyle \tA$ & A tensor\\
% $\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
% $\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
% $\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
% $\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
% $\displaystyle \ra$ & A scalar random variable\\
% $\displaystyle \rva$ & A vector-valued random variable\\
% $\displaystyle \rmA$ & A matrix-valued random variable\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Sets and Graphs}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \sA$ & A set\\
% $\displaystyle \R$ & The set of real numbers \\
% $\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
% $\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
% $\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
% $\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
% $\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
% $\displaystyle \gG$ & A graph\\
% $\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
% \end{tabular}
% \vspace{0.25cm}


% \centerline{\bf Indexing}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
% $\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
% $\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
% $\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
% $\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
% $\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
% $\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
% $\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}


% \centerline{\bf Calculus}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% % NOTE: the [2ex] on the next line adds extra height to that row of the table.
% % Without that command, the fraction on the first line is too tall and collides
% % with the fraction on the second line.
% $\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
% $\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
% $\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
% $\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
% $\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
% $\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
% $\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
% $\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
% $\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Probability and Information Theory}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
% $\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
% a variable whose type has not been specified\\
% $\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
% $\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
% $\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
% $\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
% $\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
% $\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
% $\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
% over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Functions}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
% $\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
%   $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
%   (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
% $\displaystyle \log x$ & Natural logarithm of $x$ \\
% $\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
% $\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
% $\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
% $\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
% $\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
% $\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}



% \section{Final instructions}
% Do not change any aspects of the formatting parameters in the style files.
% In particular, do not modify the width or length of the rectangle the text
% should fit into, and do not change font sizes (except perhaps in the
% \textsc{References} section; see below). Please note that pages should be
% numbered.

% \section{Preparing PostScript or PDF files}

% Please prepare PostScript or PDF files with paper size ``US Letter'', and
% not, for example, ``A4''. The -t
% letter option on dvips will produce US Letter files.

% Consider directly generating PDF files using \verb+pdflatex+
% (especially if you are a MiKTeX user).
% PDF figures must be substituted for EPS figures, however.

% Otherwise, please generate your PostScript and PDF files with the following commands:
% \begin{verbatim}
% dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
% ps2pdf mypaper.ps mypaper.pdf
% \end{verbatim}

% \subsection{Margins in LaTeX}

% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+
% from the graphicx package. Always specify the figure width as a multiple of
% the line width as in the example below using .eps graphics
% \begin{verbatim}
%    \usepackage[dvips]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.eps}
% \end{verbatim}
% or % Apr 2009 addition
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% for .pdf graphics.
% See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

% A number of width problems arise when LaTeX cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command.

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
Thanks to the Helical team, especially Issac Goh and Martina Oliver Huidobro, for the discussions around the paper. Thanks also to Prof. Antoine Cully for his support and to LuxProvide for providing computational resources for model pre-training.


% \bibliography{iclr2025_conference}

\bibliographystyle{iclr2025_conference}
\begin{thebibliography}{24}

\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bo{\"e}l et~al.(2016)Bo{\"e}l, Letso, Neely, Price, Wong, Su, Luff, Valecha, Everett, Acton, et~al.]{boel2016codon}
Gr{\'e}gory Bo{\"e}l, Reka Letso, Helen Neely, W~Nicholson Price, Kam-Ho Wong, Min Su, Jon~D Luff, Mayank Valecha, John~K Everett, Thomas~B Acton, et~al.
\newblock {Codon influence on protein expression in E. coli correlates with mRNA levels}.
\newblock \emph{Nature}, 529\penalty0 (7586):\penalty0 358--363, 2016.

\bibitem[Castillo-Hair et~al.(2024)Castillo-Hair, Fedak, Wang, Linder, Havens, Certo, and Seelig]{castillo2024optimus}
Sebastian Castillo-Hair, Stephen Fedak, Ban Wang, Johannes Linder, Kyle Havens, Michael Certo, and Georg Seelig.
\newblock {Optimizing 5â€™UTRs for mRNA-delivered gene editing using deep learning}.
\newblock \emph{Nature Communications}, 15\penalty0 (1):\penalty0 5284, 2024.

\bibitem[Dao \& Gu(2024)Dao and Gu]{dao2024mamba2}
Tri Dao and Albert Gu.
\newblock {Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality}, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.21060}.

\bibitem[Diez et~al.(2022)Diez, Medina-Mu{\~n}oz, Castellano, da~Silva~Pescador, Wu, and Bazzini]{diez2022icodon}
Michay Diez, Santiago~Gerardo Medina-Mu{\~n}oz, Luciana~Andrea Castellano, Gabriel da~Silva~Pescador, Qiushuang Wu, and Ariel~Alejandro Bazzini.
\newblock {iCodon customizes gene expression based on the codon composition}.
\newblock \emph{Scientific Reports}, 12\penalty0 (1):\penalty0 12126, 2022.

\bibitem[Eisenhut et~al.(2020)Eisenhut, Mebrahtu, Moradi~Barzadd, Thal{\'e}n, Klanert, Weinguny, Sandegren, Su, Hatton, Borth, et~al.]{eisenhut2020systematic}
Peter Eisenhut, Aman Mebrahtu, Mona Moradi~Barzadd, Niklas Thal{\'e}n, Gerald Klanert, Marcus Weinguny, Anna Sandegren, Chao Su, Diane Hatton, Nicole Borth, et~al.
\newblock Systematic use of synthetic 5'-utr rna structures to tune protein translation improves yield and quality of complex proteins in mammalian cell factories.
\newblock \emph{Nucleic Acids Research}, 48\penalty0 (20):\penalty0 e119--e119, 2020.

\bibitem[Glorioso et~al.(2024)Glorioso, Anthony, Tokpanov, Whittington, Pilault, Ibrahim, and Millidge]{glorioso2024zambacompact7bssm}
Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge.
\newblock {Zamba: A Compact 7B SSM Hybrid Model}, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.16712}.

\bibitem[Groher et~al.(2018)Groher, Jager, Schneider, Groher, Hamacher, and Suess]{groher2018tuning}
Ann-Christin Groher, Sven Jager, Christopher Schneider, Florian Groher, Kay Hamacher, and Beatrix Suess.
\newblock {Tuning the performance of synthetic riboswitches using machine learning}.
\newblock \emph{ACS synthetic biology}, 8\penalty0 (1):\penalty0 34--44, 2018.

\bibitem[Gu \& Dao(2024)Gu and Dao]{gu2024mambalineartimesequencemodeling}
Albert Gu and Tri Dao.
\newblock {Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 2024.
\newblock URL \url{https://arxiv.org/abs/2312.00752}.

\bibitem[Gu et~al.(2022)Gu, Goel, and RÃ©]{gu2022s4}
Albert Gu, Karan Goel, and Christopher RÃ©.
\newblock Efficiently modeling long sequences with structured state spaces, 2022.
\newblock URL \url{https://arxiv.org/abs/2111.00396}.

\bibitem[Hanson \& Coller(2018)Hanson and Coller]{hanson2018codon}
Gavin Hanson and Jeff Coller.
\newblock {Codon optimality, bias and usage in translation and mRNA decay}.
\newblock \emph{Nature reviews Molecular cell biology}, 19\penalty0 (1):\penalty0 20--30, 2018.

\bibitem[Hu et~al.(2024)Hu, Tu, Han, He, Cui, Long, Zheng, Fang, Huang, Zhao, Zhang, Thai, Zhang, Wang, Yao, Zhao, Zhou, Cai, Zhai, Ding, Jia, Zeng, Li, Liu, and Sun]{hu2024wsd_scheduler}
Shengding Hu, Yuge Tu, Xu~Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng~Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun.
\newblock {MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies}, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.06395}.

\bibitem[Leppek et~al.(2022)Leppek, Byeon, Kladwang, Wayment-Steele, Kerr, Xu, Kim, Topkar, Choe, Rothschild, et~al.]{leppek2022combinatorial}
Kathrin Leppek, Gun~Woo Byeon, Wipapat Kladwang, Hannah~K Wayment-Steele, Craig~H Kerr, Adele~F Xu, Do~Soon Kim, Ved~V Topkar, Christian Choe, Daphna Rothschild, et~al.
\newblock {Combinatorial optimization of mRNA structure, stability, and translation for RNA-based therapeutics}.
\newblock \emph{Nature communications}, 13\penalty0 (1):\penalty0 1536, 2022.

\bibitem[Lieber et~al.(2024)Lieber, Lenz, Bata, Cohen, Osin, Dalmedigos, Safahi, Meirom, Belinkov, Shalev-Shwartz, Abend, Alon, Asida, Bergman, Glozman, Gokhman, Manevich, Ratner, Rozen, Shwartz, Zusman, and Shoham]{lieber2024jambahybridtransformermambalanguage}
Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, and Yoav Shoham.
\newblock {Jamba: A Hybrid Transformer-Mamba Language Model}, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.19887}.

\bibitem[Nieuwkoop et~al.(2023)Nieuwkoop, Terlouw, Stevens, Scheltema, De~Ridder, Van~der Oost, and Claassens]{nieuwkoop2023revealing}
Thijs Nieuwkoop, Barbara~R Terlouw, Katherine~G Stevens, Richard~A Scheltema, Dick De~Ridder, John Van~der Oost, and Nico~J Claassens.
\newblock {Revealing determinants of translation efficiency via whole-gene codon randomization and machine learning}.
\newblock \emph{Nucleic acids research}, 51\penalty0 (5):\penalty0 2363--2376, 2023.

\bibitem[O'Leary et~al.(2016)O'Leary, Wright, Brister, Ciufo, Haddad, McVeigh, Rajput, Robbertse, Smith-White, Ako-Adjei, et~al.]{o2016reference}
Nuala~A O'Leary, Mathew~W Wright, J~Rodney Brister, Stacy Ciufo, Diana Haddad, Rich McVeigh, Bhanu Rajput, Barbara Robbertse, Brian Smith-White, Danso Ako-Adjei, et~al.
\newblock {Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation}.
\newblock \emph{Nucleic acids research}, 44\penalty0 (D1):\penalty0 D733--D745, 2016.

\bibitem[Pardi et~al.(2018)Pardi, Hogan, Porter, and Weissman]{pardi2018mrna}
Norbert Pardi, Michael~J Hogan, Frederick~W Porter, and Drew Weissman.
\newblock {mRNA vaccinesâ€”a new era in vaccinology}.
\newblock \emph{Nature reviews Drug discovery}, 17\penalty0 (4):\penalty0 261--279, 2018.

\bibitem[Park et~al.(2021)Park, Lagniton, Liu, and Xu]{park2021covid}
Jung~Woo Park, Philip~NP Lagniton, Yu~Liu, and Ren-He Xu.
\newblock {mRNA vaccines for COVID-19: what, why and how}.
\newblock \emph{International journal of biological sciences}, 17\penalty0 (6):\penalty0 1446, 2021.

\bibitem[Parmar et~al.(2024)Parmar, Prabhumoye, Jennings, Patwary, Subramanian, Su, Zhu, Narayanan, Jhunjhunwala, Dattagupta, Jawa, Liu, Mahabaleshwarkar, Nitski, Brundyn, Maki, Martinez, You, Kamalu, LeGresley, Fridman, Casper, Aithal, Kuchaiev, Shoeybi, Cohen, and Catanzaro]{parmar2024nemotron415btechnicalreport}
Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa, Jiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel Martinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath Aithal, Oleksii Kuchaiev, Mohammad Shoeybi, Jonathan Cohen, and Bryan Catanzaro.
\newblock Nemotron-4 15b technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.16819}.

\bibitem[Ren et~al.(2024{\natexlab{a}})Ren, Liu, Lu, Shen, Liang, and Chen]{ren2024samba}
Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen.
\newblock {Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling}.
\newblock \emph{arXiv preprint arXiv:2406.07522}, 2024{\natexlab{a}}.

\bibitem[Ren et~al.(2024{\natexlab{b}})Ren, Jiang, Di, Zhang, Gong, Gong, Jiang, Fu, Sun, Zhou, and Ni]{codonbert2024}
Zilin Ren, Lili Jiang, Yaxin Di, Dufei Zhang, Jianli Gong, Jianting Gong, Qiwei Jiang, Zhiguo Fu, Pingping Sun, Bo~Zhou, and Ming Ni.
\newblock {CodonBERT: a BERT-based architecture tailored for codon optimization using the cross-attention mechanism}.
\newblock \emph{Bioinformatics}, 40\penalty0 (7):\penalty0 btae330, 05 2024{\natexlab{b}}.
\newblock ISSN 1367-4811.
\newblock \doi{10.1093/bioinformatics/btae330}.
\newblock URL \url{https://doi.org/10.1093/bioinformatics/btae330}.

\bibitem[Sahin et~al.(2017)Sahin, Derhovanessian, Miller, Kloke, Simon, L{\"o}wer, Bukur, Tadmor, Luxemburger, Schr{\"o}rs, et~al.]{sahin2017personalized}
Ugur Sahin, Evelyna Derhovanessian, Matthias Miller, Bj{\"o}rn-Philipp Kloke, Petra Simon, Martin L{\"o}wer, Valesca Bukur, Arbel~D Tadmor, Ulrich Luxemburger, Barbara Schr{\"o}rs, et~al.
\newblock Personalized rna mutanome vaccines mobilize poly-specific therapeutic immunity against cancer.
\newblock \emph{Nature}, 547\penalty0 (7662):\penalty0 222--226, 2017.

\bibitem[Vaswani et~al.(2023)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2023attentionneed}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock {Attention Is All You Need}, 2023.
\newblock URL \url{https://arxiv.org/abs/1706.03762}.

\bibitem[Waleffe et~al.(2024)Waleffe, Byeon, Riach, Norick, Korthikanti, Dao, Gu, Hatamizadeh, Singh, Narayanan, Kulshreshtha, Singh, Casper, Kautz, Shoeybi, and Catanzaro]{nvidia2024hybrid}
Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, and Bryan Catanzaro.
\newblock {An Empirical Study of Mamba-based Language Models}, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.07887}.

\bibitem[Yazdani-Jahromi et~al.(2024)Yazdani-Jahromi, Prakash, Mansi, Moskalev, and Liao]{yazdanijahromi2024helm}
Mehdi Yazdani-Jahromi, Mangal Prakash, Tommaso Mansi, Artem Moskalev, and Rui Liao.
\newblock {HELM: Hierarchical Encoding for mRNA Language Modeling}, 2024.
\newblock URL \url{https://arxiv.org/abs/2410.12459}.

\end{thebibliography}


\appendix
\section{Appendix}

\subsection{Specific Architecture Implementation Details}

We use a hidden dimension of 256 throughout the model with SiLU activation functions. For the Mamba-2 layers, we employ a head dimension of 32, a single group, and a kernel size of 4 for convolution, with an expansion factor of 2. The attention implementation uses Flash Attention 2 with 32 attention heads and 8 key-value heads. For MLP layers, we maintain an intermediate size of 512, representing an expansion ratio of 2. No biases are used in the attention and MLP layers. The model consists of 9 total layers, with a layer configuration of 4 Mamba-2 layers, 4 MLP layers and a single attention layer in the configuration \textit{M+M*+M+M+}, where M represents a Mamba-2 layer, + represents an MLP layer and * represents attention layers. 

\begin{table}[h] 
    \centering
    \caption{Model parameters used for Helix-mRNA.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccccc}
         Params (M) & No. Layers & Model Dim & Attn. Heads & State Dim. & No. Groups & Pos. Emb. & Seq. Len \\
        \hline \\
        5.19 & 8 & 256 & 16 & 128 & 1 & None & 12288 \\
        \\ \hline \\
    \end{tabular}
    }
\end{table}

\begin{table}[h] 
    \centering
    \caption{Pre-training Details.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccc}
         No. Tokens (B) & Pre-Training Time (Days) & No. Devices & Device Type  \\
        \hline \\
        332 & 2.3 & 32 & NVIDIA A100-SXM4-40GB   \\
        \\ \hline \\
    \end{tabular}
    }
\end{table}

\subsection{Pre-training Datasets}\label{supp:data}

All of the datasets can be found under the NCBI ftp server: \url{https://ftp.ncbi.nih.gov/refseq/release/}. We downloaded all the files ending by `.rna.gbff.gz` and extracted the utr and coding sequences via our pre-processing pipeline. Out of the 56 Million sequences we subsampled 27 million sequences due to budgetary constrains to train our model. 

The viral component encompassed the main human pathogens, including respiratory viruses (SARS-CoV-2, influenza A/B/C, RSV), retroviruses (HIV-1/2, HTLV-1/2), hepatotropic viruses (HBV, HCV), herpesviruses (HSV-1/2, EBV, VZV), and emerging arboviruses (Zika, Dengue 1-4). The dataset consists of 27 million sequences, with class contributions being, 37.6\% other vertebrates (\url{https://ftp.ncbi.nih.gov/refseq/release/vertebrate_other/}), 24.4\% mammals (\url{https://ftp.ncbi.nih.gov/refseq/release/vertebrate_mammalian/}), 22.8\% invertebrates (\url{https://ftp.ncbi.nih.gov/refseq/release/invertebrate/}), 13.7\% fungi (\url{https://ftp.ncbi.nih.gov/refseq/release/fungi/}) and 1.4\% viruses (\url{https://ftp.ncbi.nih.gov/refseq/release/viral/}). 



\subsection{Fine-Tuning}
We unfreeze the 2 last layers when fine-tuning on downstream tasks, while freezing the rest of the parameters.

\subsection{Evaluation Details}\label{ref:supp_eval}
Due to the HELM code not yet being publicly available, we referenced their published research findings directly instead of independently reproducing their experimental methodology in Table \ref{benchmark_results} \cite{yazdanijahromi2024helm}. For our analysis, we selected the highest performance values from either the Masked Language Modelling or Causal Language Modelling variants of the Transformer XE and Transformer HELM models. The authors of HELM report the best results achieved for the tasks, with the exception of the MLOS Flu Vaccines task, where the average performance across three random splits is reported due to the absence of predefined splits. To align with their methodology, we adopt a similar approach but also include an error margin to highlight the variability of results across five runs.


\subsection{Full Sequence Embeddings After Pre-Training}

\begin{figure}[h]
\begin{center}
\label{embedding_figures_full_seq}
\includegraphics[scale=0.3]{images/full_seq_embs.pdf}
\end{center}
\caption{Helix-mRNA embeddings from initial pre-training, generated using full mRNA sequences including both the coding and untranslated regions.}
\end{figure}

Embeddings presented in Figure \ref{embedding_figures_full_seq} show Helix-mRNA's ability to process full length sequences not limited to just the coding region.
% \subsection{MRL UTR 5' Predictions}
% \begin{figure}[h]
% \label{mrl_full_fig}
% \begin{center}
% \includegraphics[scale=0.35]{images/output.png}
% \end{center}
% \caption{Full results from task specific MRL fine-tuning predictions for HEK293T, T and HepG2 cells \cite{castillo2024optimus}.}
% \end{figure}
% You may include other additional sections here.


\end{document}
