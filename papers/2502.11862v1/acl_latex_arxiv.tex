% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{listings}
\usepackage{url}
\usepackage{CJKutf8}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{supertabular}
\usepackage{subfigure}
\usepackage{dcolumn,booktabs}
\usepackage{tikz}
\usepackage{xspace}
\usepackage{authblk}

\def\secref#1{\S\ref{sec:#1}}
\def\seclabel#1{\label{sec:#1}}

\newcounter{notecounter}
\newcommand{\enotesoff}{\long\gdef\enote##1##2{}}
\newcommand{\enoteson}{\long\gdef\enote##1##2{{
\stepcounter{notecounter}
{\large\bf
\hspace{1cm}\arabic{notecounter} $<<<$ ##1: ##2
$>>>$\hspace{1cm}}}}}
\enoteson
\enotesoff

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Towards Better Understanding of LLM-based Low-Resource \\ Machine Translation: A Case Study in Manchu}

\title{Understanding In-Context Machine Translation for Low-Resource Languages: A Case Study on Manchu}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author[1,*]{\bf Renhao Pei}
\author[1,2,*]{\bf Yihong Liu}
\author[1,2]{\bf Peiqin Lin}
\author[3,$\dag$]{\bf François Yvon}
\author[1,2,$\dag$]{\bf Hinrich Sch\"utze}

\affil[]{Center for Information and Language Processing, LMU Munich \protect\\ $^{2}$Munich Center for Machine Learning (MCML) \protect\\ $^{3}$Sorbonne Université, CNRS, ISIR, France
 \protect\\ \texttt{renhaopei@gmail.com} \ \ \ \ \ \ \ \texttt{yihong@cis.lmu.de}} 


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\newcommand{\tempcitation}{[\textbf{citation}]\xspace}

\begin{document}
\maketitle

\def\thefootnote{*}\footnotetext{Equal contribution.}\def\thefootnote{\arabic{footnote}}

\def\thefootnote{$\dag$}\footnotetext{Equal advising.}\def\thefootnote{\arabic{footnote}}

\begin{abstract}
% Parallel data are scarce for most low-resource or endangered languages, making it almost unlikely to train effective neural machine translation (NMT) systems.
In-context machine translation (MT) with large language models (LLMs) is a promising approach for low-resource MT, as it can readily take advantage of linguistic resources such as grammar books and dictionaries.
Such resources are usually selectively integrated into the prompt so that LLMs can directly perform translation without any specific training, via their in-context learning capability (ICL).
However, the relative importance of each type of resource e.g., dictionary, grammar book, and retrieved parallel examples, is not entirely clear.
To address this gap, this study systematically investigates how each resource and its quality affects the translation performance, with the \textbf{Manchu} language as our case study. To remove any prior knowledge of Manchu encoded in the LLM parameters and single out the effect of ICL, we also experiment with an encrypted version of Manchu texts.
Our results indicate that high-quality dictionaries and good parallel examples are very helpful, while grammars hardly help.
% Additionally, larger LLMs can better leverage information provided in the context, even when their prior knowledge about Manchu is blocked by character-level encryption.
In a follow-up study, we showcase a promising application of in-context MT: parallel data augmentation as a way to bootstrap the conventional MT model. When monolingual data abound, generating synthetic parallel data through in-context MT offers a pathway to mitigate data scarcity and build effective and efficient low-resource neural MT systems.\footnote{We make our code and data publically available at: \url{https://github.com/cisnlp/manchu-in-context-mt}.}
% Our code is publicly available.
% [Publish our codes at..\footnote{{URL hidden.}} ]
\end{abstract}
%\textbf{[need to choose one term and be consistent with it: In-context MT or LLM-based MT?]}

\section{Introduction}

% \paragraph{Machine Translation for Low-Resource languages}
% Machine translation systems have seen remarkable advancements, which are largely driven by neural machine translation (NMT) models that rely on substantial amounts of parallel data. 


Neural machine translation (NMT) systems have achieved remarkable performance in high-resource languages for which parallel sentence-level or document-level data are abundant \citep{Bahdanau2015attention,Vaswani2017transformer,tiedemann-scherrer-2017-neural,laubli-etal-2018-machine}. 
However, parallel data is scarce or even unavailable for many low-resource or endangered languages \citep{haddow-etal-2022-survey}, which prevents the training of dedicated MT systems for these languages. While multilingual models partly mitigate this issue \cite{nllbteam2022languageleftbehindscaling}, they only cover a small fraction of the world's languages, and their performance remains unsatisfactory for many language pairs.
% available for many languages, and the disparity across languages in terms of MT quality is enormous \citep{blasi-etal-2022-systematic}. While some current state-of-the-art in multilingual NMT \citep{nllbteam2022languageleftbehindscaling} can cover 200 languages, it is only a small fraction of the world’s 7000+ languages. [should we also mention \cite{bapna2022buildingmachinetranslationsystems} which claims to cover 1500+ languages?] 
% Consequently, the scarcity of parallel data does not allow for training well-performed NMT models, making them less likely to be the backbone for effective translation systems featuring low-resource languages.

% Recently, LLMs have emerged as promising tools for machine translation. Although not explicitly trained for translation task, LLMs can achieve very competitive performance for translating between several high-resource languages \citep{brown2020language,workshop2023bloom176bparameteropenaccessmultilingual}. Most notably, LLMs perform much better at translating from non-English language into English, than translating into non-English \citep{zhu2024multilingualmachinetranslationlarge}.
% On the other hand, the LLMs' capabilities are also severely limited for low-resource languages \citep{hendy2023goodgptmodelsmachine,robinson-etal-2023-chatgpt}, as even the huge web-scraped training corpora do not have much data for low-resource languages. 


% \paragraph{Linguistic Resources}: 
On the other hand, owing to the work of field linguists,
% , although large parallel corpora are unrealistic for most of the world’s 7000+ languages,
grammar books or dictionaries are available for at least 60\% languages \citep{nordhoff2011glottolog,zhang-etal-2024-hire}. 
Some low-resource languages are well-documented, with rich linguistic resources gathered over decades of meticulous fieldwork and analysis by linguists: this is for instance the case of Japhug, a minority Sino-Tibetan language, for which a comprehensive grammar, including plentiful glossed and translated examples, has been released by \citet{Jacques2021}.
% For instance, \citet{Jacques2021} produced a comprehensive 1,500-page study of a low-resource language Japhug, after 20 years of linguistic fieldwork: detailed grammar explanations, wordlists, and many glossed and translated sentences are included. 
The situation of Manchu is even more favorable, as multiple grammar books, dictionaries, and textbooks are readily available. 
Yet, all these languages are still considered low-resource in the context of data-driven MT, simply due to the scarcity of parallel data. A natural question is then to explore whether such linguistic knowledge can make up for the lack of parallel data, and help develop MT systems.
% Thus, a significant gap exists, as the rich information encapsulated in these linguistic resources has largely remained underutilized in the NLP field, mainly due to their unstructured format.

% \paragraph{In-context Learning}: 
The recent emergence of LLMs seems to offer new promising ways to address this question, based on the 
in-context learning ability of LLMs \citep{tanzer2024a,zhang-etal-2024-hire,hus-anastasopoulos-2024-back,merx-etal-2024-low}.
% With the recent emergence of LLMs, now there is hope to finally utilize the goldmine of linguistic resources.
% There have been a few studies showing that LLMs are promising in MT for low-resource languages without any training. 
% can leverage carefully designed prompts where linguistic resources such as dictionaries and grammar books
In these studies, linguistic resources such as dictionaries, parallel examples, and grammar books are integrated into the prompt and encoded together with the sentence to be translated.
% to handle unstructured textual instructions and adapt to new tasks through in-context learning, there has been increasing amount of interest on combining linguistic resources such as dictionaries and grammar books, with LLMs' in-context learning ability for Machine Translation task \cite{tanzer2024a,zhang-etal-2024-hire,hus-anastasopoulos-2024-back,merx-etal-2024-low}. 
% However, several notable limitations lie in this line of research, such as lack of proper morphological analysis, inadequate dictionaries, inefficient utilization of grammar, and suboptimal retrieval of parallel examples.
% Additionally, there is no clear ablation between the LLMs' prior knowledge of the explored low-resource languages and the linguistic information provided in context.
% Most importantly, the importance of each type of linguistic resource  is not fully clear. there is no clear and rigorous analysis 
% These limitations have rendered their analyses less reliable.
We continue this line of work, trying to better analyze the role and impact of each type of linguistic knowledge that can be put to use in LLM-based machine translation systems.

For this, we perform a systematic investigation of how each component affects the in-context MT performance, with \textbf{Manchu} as a case study.
Specifically, we leverage a wide range of state-of-the-art open-source and close-source LLMs and consider the following linguistic resources (components): dictionaries, parallel examples, grammar books, and Chain-of-Thought (CoT) prompting.
For each component, we consider several variants that vary in the amount of information or the degree of relevance to the sentence to be translated.
To quantify the influence of prior knowledge of Manchu in LLMs, we perform a character-level encryption to disentangle the effect of LLMs’ prior knowledge of Manchu from their in-context learning ability.
In addition, we demonstrate a use case of our in-context MT system, using it as a data-augmentation tool to translate a monolingual Manchu corpus into a parallel corpus. With these synthetic parallel data incorporated into the training set, we fine-tune the mT5 model \citep{xue-etal-2021-mt5}, achieving a substantial performance gain compared to the baseline that only uses real parallel data.

The main contributions of this work are as follows: 
\textbf{(i)} We conduct a comprehensive investigation of in-context MT for Manchu, exploring the most important knowledge sources provided in the context, highlighting the positive role of high-quality dictionaries and closely related parallel examples. 
\textbf{(ii)} Using an encrypted version of Manchu, we isolate the limited prior knowledge of Manchu encoded in the LLMs considered in our work and show that most of their translation performance depends on their in-context learning abilities.
\textbf{(iii)} We use in-context MT to generate synthetical parallel data from monolingual data of Manchu and measure how much this form of data augmentation actually benefits low-resource NMT.

\section{Related Work}

%\paragraph{In-Context Machine Translation}: talk about some related papers that use the in-context learning ability: selecting enough examples as context, zero-shot, one-shot, few-shot MT; requiring some knowledge in the low-resource language \citep{hendy2023goodgptmodelsmachine} : GPT3.5,both high and low-resource, low-resource language example: Icelandic, Hausa. Mentioned Document-level MT, but only EN-DE \citep{robinson-etal-2023-chatgpt} uses FLORES-200 dataset, compare 204 high and low-resource languages, GPT3.4 and GPT4, GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages \citep{cahyawijaya-etal-2024-llms} LLMs Are Few-Shot In-Context Low-Resource Language Learners \citep{ramos2024grammamtimprovingmachinetranslation}: GrammaMT, interlinear glossing

\paragraph{Low-resource NMT}
The challenges posed by limited parallel data motivate extensive research into innovative strategies for low-resource NMT
\citep{haddow-etal-2022-survey,yazar2023lowresource}. 
Various approaches have been proposed to improve translation quality in such settings. Data augmentation techniques, such as back-translation \citep{sennrich-etal-2016-improving,edunov-etal-2018-understanding} and forward-translation \citep{bogoychev2020domaintranslationesenoisesynthetic}, have been widely used to generate synthetic parallel data and enhance model performance. 
Data augmentation, coupled with unsupervised and semi-supervised methods for bilingual dictionary induction have enabled translation with minimal parallel data, relying instead on monolingual resources \citep{Lample2018unsupervised,Artetxe2018Unsupervised}.
Transfer learning has also proven effective, where models pretrained on high-resource language pairs can be adapted to low-resource languages \citep{zoph-etal-2016-transfer}. 
Recent advancements in multilingual NMT also show that models trained on multiple language pairs can better deal with low-resource languages \citep{ko-etal-2021-adapting,mohammadshahi-etal-2022-small,nllbteam2022languageleftbehindscaling}.
Despite these advancements, achieving high-quality translation in low-resource scenarios remains a significant challenge.

\paragraph{LLM-based In-context MT for Low-Resource Languages}
% LLMs can be directly used to perform translation without explicitly being trained. This is achieved by providing instructions and demonstrations in the prompt \citep{NEURIPS2020_1457c0d6,lin-etal-2022-shot,vilar-etal-2023-prompting}. However, this LLM-based MT has been shown to struggle with rare words -- words that appear less often in the training set \citep{ghazvininejad2023dictionarybasedphraselevelpromptinglarge}. This is particularly the case for low-resource languages that are underrepresented in the training data of LLM \citep{touvron2023llama2openfoundation,workshop2023bloom176bparameteropenaccessmultilingual}. To address this issue, some studies integrate relevant \textit{dictionary entries} and similar \textit{parallel sentence examples} into the prompt \citep{ghazvininejad2023dictionarybasedphraselevelpromptinglarge,zhang-etal-2024-teaching}. 
% \textit{Grammar excerpts} retrieved from Grammar books has also been shown to benefit the low-resource languages \citep{tanzer2024a,hus-anastasopoulos-2024-back}.
% Another line of work additionally leverages \textit{morphological analyzers} to transform the input sentence into morphemes before retrieving relevant information \citep{zhang-etal-2024-hire}.
% In addition to components of linguistic resources, \citet{elsner-needle-2023-translating} show CoT prompt can help improve translation quality. However, although there have been a few investigations into the effectiveness of each component for high-resource languages \citep{raunak-etal-2023-dissecting,chitale-etal-2024-empirical}, little attention is given to low-resource languages. This work presents the first of its kind to investigate the components in LLM-Based MT for low-resource languages, using Manchu as a case study.
Although not explicitly trained for machine translation, LLMs can perform translation by following instructions and demonstrations in the prompt \citep{NEURIPS2020_1457c0d6,lin-etal-2022-shot,vilar-etal-2023-prompting}. LLM-based MT however struggles with rare words that appear infrequently in the training data \citep{ghazvininejad2023dictionarybasedphraselevelpromptinglarge}. This issue is particularly pronounced for low-resource languages that are underrepresented in the LLM's training corpora \citep{workshop2023bloom176bparameteropenaccessmultilingual,touvron2023llama2openfoundation}.
To mitigate this, some studies incorporate linguistic resources into prompts, such as \textbf{dictionary entries} and \textbf{parallel sentence examples} \citep{ghazvininejad2023dictionarybasedphraselevelpromptinglarge,zhang-etal-2024-teaching}, as well as \textbf{grammars} \citep{tanzer2024a,hus-anastasopoulos-2024-back}. Some works also include \textbf{morphological analyzers} to decompose input sentences into morphemes \citep{zhang-etal-2024-hire}.
In addition to linguistic resources, \textbf{CoT}  has also been experimented in a machine translation context \citep{elsner-needle-2023-translating}. 
%While some studies have explored the effectiveness of these components for high-resource languages \citep{raunak-etal-2023-dissecting,chitale-etal-2024-empirical}, little attention has been given to low-resource languages. This work is the first to systematically investigate the role of these components in LLM-based in-context MT for low-resource languages, using Manchu as a case study.
However, little attention has been given to how the quality of each component affects the LLM-based in-context MT. Moreover, there has been no clear ablation between the LLMs' prior knowledge of the language and the linguistic information provided in context.
Addressing these limitations of previous studies, our work systematically investigates the role of each of these components in LLM-based MT for low-resource languages, using Manchu as a case study.

% As one of the earliest pioneer works, 
% \citet{ghazvininejad2023dictionarybasedphraselevelpromptinglarge} proposes an approach called DIPMT (Dictionary-based
% Prompting for Machine Translation). Namely, for a given input sentence in the source language, bilingual \textbf{dictionary entries} corresponding to the words in the sentence are retrieved and incorporated into a prompt, which is then provided to LLMs to generate the translation in the target language. Built upon DIPMT, \citet{zhang-etal-2024-teaching} introduces DIPMT++, which makes two improvements: 1. Improving lexical coverage through methods such as fuzzy-matching, and 2. further incorporating \textbf{parallel examples} into the prompt, which are retrieved through algorithms like BM25.
% \citet{tanzer2024a} further enhances this line of works by their MTOB (Machine Translation from One Book) approach, which includes \textbf{grammar books} as another source of information. They extracted a bilingual wordlist, a small parallel corpus and grammar book excerpts from a single grammar book of Kalamang, and experimented incorporating these 3 kinds of resources into the prompt to see how they affect the in-context machine translation performance. They conclude that retrieved parallel examples are the most beneficial, followed by the wordlist, followed by excerpts from the grammar book, and combining these 3 kinds of information tends to further improve performance.

% While \citet{tanzer2024a}  represents a crucial contribution in this stream of research, 
% %it also exhibits several notable limitations: Firstly, its test set for evaluating machine translation performance has only 100 parallel sentences, which renders their results less reliable. Secondly, 
% their approach lacks any morphological analysis, relying instead on fuzzy matching techniques like the longest-common substring distance to handle inflected and out-of-vocabulary words, which results in not only less reliable retrieval of dictionary entries, but also suboptimal retrieval of parallel examples, as these examples are retrieved based on word matching between the input sentence and sentences within the parallel English-Kalamang corpus.
% Moreover, they tried to retrieve excerpts from the grammar book that are most similar to the input sentence using cosine similarity in an embedding space and longest common substring distance, which is inaccurate due to inherent mismatch between the nature of an input source language sentence and that of a grammar book passage.
% Lastly, as \citet{aycock2024llmsreallylearntranslate} pointed out, the grammar book excerpts used in \citet{tanzer2024a} are actually a mixture of parallel examples and grammatical explanations, and the improvement actually stems from the parallel examples. With the parallel examples removed, there is no evidence that LLMs can effectively exploit grammatical explanations for machine translation task.

% Addressing MTOB's limitation of lacks of morphological analysis, \citet{zhang-etal-2024-hire} makes use of \textbf{morphological analyzers} to firstly transform the input sentence into morphemes, before retrieving dictionary entries and grammatical information. However, the full potential of the morphological analysis has yet been overlooked: while the grammatically analyzed morphemes could have been greatly helpful for retrieving grammar passages that are relevant to the specific input sentence, they unconditionally made use of the same grammar book without any retrieval mechanism, resulting in an excessively long prompt with a lot of information irrelevant to the input sentence. 
% %They claimed that including a grammar is beneficial, but only provided a qualitative comparison between with-grammar and without-grammar. Moreover, 
% Their approach also lacks a proper retrieval of parallel examples, as parallel examples were simply randomly sampled as few-shot prompting. 
% %The test set size was also very small (e.g. 70 sentences for Manchu).

% \citet{hus-anastasopoulos-2024-back} scale up MTOB's approach to 16 low-resource languages using PanLex
% \citep{kamholz-etal-2014-panlex} for dictionaries, 
% FLORES-200 dataset
% \footnote{\url{https://github.com/openlanguagedata/flores}} for parallel examples and the DReaM corpus
% \citep{virk-etal-2020-dream} for grammars. 
% %For more reliable evaluation of the translation performance, they increased the test set size using FLORES-200's devtest split, which has 1012 sentences for each language. 
% However, their approach also inherits the limitations of the original MTOB approach such as the lack of separation of grammatical explanations from parallel examples and the absence of morphological analysis. On the other hand, it shares with \citet{zhang-etal-2024-hire} the limitation of including an entire grammar book in the prompt, without the retrieval of specific grammar book excerpts.

% Some previous works \citep{elsner-needle-2023-translating,zhang-etal-2024-hire} have additionally experimented with the \textbf{Chain-of-Thought} (CoT) prompting, in which the LLM is instructed to explicitly perform some steps such as identifying the part-of-speech of each word, identifying the subject and object of the verb, and annotating grammatical features (e.g. case, number, tense, aspect) of the words, before generating the actual translation. However, results of these experiments have shown that chain-of-thought is unhelpful for the in-context machine translation task.

% Recognizing the limitations of previous works, our approach aims to address all these shortcomings and offer a more comprehensive and refined methodology.

% [Mention \citet{chitale-etal-2024-empirical}?]
%\begin{itemize} \item Small Test Set Size: Earlier studies typically used small test sets comprising 70, 100, or 200 sentences, \citet{hus-anastasopoulos-2024-back} has 997, whereas our dataset includes approximately 2,000 sentences, providing a more robust basis for evaluation. \item Suboptimal Parallel Examples: Many previous methods randomly sampled parallel examples as few-shot examples, leading to suboptimal results. In contrast, our approach retrieves parallel examples from dictionary example sentences for each word in the input sentence, ensuring the parallel examples are highly relevant for explaining the word's meaning. \item Lack of Proper Morphological Analysis: Previous studies either did not incorporate any morphological analysis or relied on fuzzy matching to handle inflected words. Our method employs a rule-based morphological analyzer to accurately split words into stems and affixes, improving precision. \item Inadequate Dictionary: Earlier approaches often used dictionaries without explanations for affixes or information about word phrases and collocations. (\cite{tanzer2024a} includes affixes, while \cite{zhang-etal-2024-hire} does not.) Our approach overcomes this by providing detailed explanations for individual words, affixes, and collocations, offering a more comprehensive linguistic resource. \item Inefficient Grammar Utilization: Traditional methods used the same grammar book unconditionally, resulting in irrelevant information being included for many input sentences. While Tanzer et al. (2024) attempted to retrieve relevant grammar book sections based on longest common substring distance, their method was highly inaccurate. Benefiting from our morphological analyzer, we can identify suffixes in any given Manchu sentence and use a manually compiled dictionary linking suffixes to relevant grammar book sections. This enables us to generate a tailored grammar book for each sentence, enhancing both accuracy and efficiency. \end{itemize} By addressing these limitations, our approach provides a more effective and nuanced methodology for processing low-resource languages like Manchu.

% \section{Data and General Setup}\seclabel{setup}
\section{Language, Data and General Setup}\seclabel{setup}
%[Put this entire section into appendix, then links to them by ref in the methodology part?]

\paragraph{Manchu Language} Manchu (ISO 639-3: \texttt{mnc}) is a critically endangered Tungusic language native to Northeast China. It is the traditional language of the Manchu people and was one of the official languages of Qing dynasty (1644-1911) of China. Because of its significant historical importance, Manchu has been extensively studied and there exist abundant linguistic resources, including dictionaries, grammar books, and some bilingual parallel sentences, which makes Manchu well-suited for our case study. A more detailed description of the Manchu language is given in Appendix~\ref{sec:appendix A}.

\paragraph{Dictionary}
We use the data from \citet{norman2020comprehensive},\footnote{ \url{https://buleku.org/home}. 
% We have received explicit consent from the website owner to use the data for research purposes.
}
which is a comprehensive dictionary that contains rich information such as the multiple senses for polysemous words and frequent collocations. This serves as our main Manchu-English lexicon.
Additionally, we compile a dictionary for Manchu suffixes based on \citep{clark1980manchu},
which contains brief explanations for each suffix.\footnote{Manchu exclusively uses suffixation, therefore neither prefix nor circumfix is involved.}
%\footnote{Data is available at \url{https://manc.hu/en/grammar/suffix-list}}
% \footnote{We have received explicit consent from the website owner to use the data for research purposes.}.

\paragraph{Parallel Corpus}\label{par:Parallel Data}
The source of the parallel data is a Manchu-Chinese dictionary \citep{1994}, which contains parallel example sentences for many dictionary entries.\footnote{Data is available from \url{https://gerel.net/}.}
We extract parallel sentences from the dictionary, followed by data-cleaning and filtering steps, to ensure that the Chinese sentences are in modern Standard Chinese.
% so that they do not pose excessive challenges when translating into English.
The result is a Manchu-Chinese parallel corpus consisting of 3,520 sentence pairs, encompassing diverse genres, including everyday conversations, historical records, and literary works.
We then use the Google Cloud Translation API
%and DeepL API\footnote{\url{https://www.deepl.com/en/pro-api}} 
to translate the Chinese sentences into English, thereby creating a Manchu-English parallel corpus.\footnote{\url{https://cloud.google.com/translate?hl=en}}
%, in which each Manchu sentence is paired with two English translations.
\paragraph{Monolingual Corpus}
We also compile a monolingual Manchu corpus consisting of 
% \textbf{number would change: 35,200}
42,240
sentences collected from websites, encompassing a diverse range of genres.\footnote{\url{https://manc.hu/} and \url{https://gerel.net/}} During our data augmentation experiment presented in \secref{augmentation}, this monolingual Manchu corpus serves to build a synthetic Manchu-English parallel corpus.
\enote{Why manchu english? R:for better comparison with previous work}{a}

\paragraph{Grammar}
\label{para:grammar}
We use two grammar books: a concise grammar book \citep{norman1965grammatical}
%(Jerry Norman's \textit{Manchu Grammatical Sketch}
%\footnote{The Manchu Grammatical Sketch is obtained from \url{https://manc.hu/en/grammar/sketch}.}) 
and a more detailed grammar from \citep{gorelova2002manchu}.


% [Although this volume of parallel data is insufficient for training a high-quality NMT model, it serves as a valuable resource for parallel examples in in-context machine translation.]

% [- To prevent future data contamination of web-scraping tools, we should encrypted the source files of our test set. (a simple .zip with password will do?) (c.f. the end of appendix of \cite{hus-anastasopoulos-2024-back})]

\paragraph{Evaluation Set}
We compile a test set of 337 Manchu-English parallel sentences for evaluation. 
This test set consists of 70 sentences from \textit{Nogeoldae}, a book containing dialogues in Manchu, paired with English translations \citep{zhang-etal-2024-hire}, and 267~sentence pairs extracted from \citep{di2007diary}.\footnote{\url{https://github.com/ulingga/Manchu-English_babyMT}.}
We have ensured no overlap between the parallel corpus and the evaluation set.
%There are some overlaps between the parallel example corpus and the test set. But because each call to the LLM operates independently, it is unnecessary to completely exclude the test set from the the parallel corpus. Instead, we ensure that for each independent LLM call, the answer to the input source sentence is excluded from the retrieved parallel sentences.

\paragraph{Models}
We conduct our experiments with multiple LLMs: GPT-4o \citep{openai2024gpt4technicalreport}, DeepSeek-V3 \citep{deepseekai2024deepseekv3technicalreport}, and %the open-source 
Llama3 models \citep{grattafiori2024llama3herdmodels}. For the Llama3 family, we test models of varying sizes -- 1B, 3B, 8B, and 70B -- to evaluate how model size impacts performance.

\paragraph{Evaluation Metrics}
We use BLEU \citep{papineni-etal-2002-bleu} and chrF \citep{popovic-2015-chrf} to measure the translation quality, as implemented by SacreBLEU \citep{post-2018-call}.\footnote{Signature:
\texttt{nrefs:1|case:lc|eff:no|tok:13a|smooth:\newline exp|version:2.4.3.}} Additionally, we use SBERT \citep{reimers-2019-sentence-bert}, an encoding based-metric, which assess the semantic relatedness between a hypothesis and a reference using the cosine similarity of their embeddings (scores are multiplied by 100 to ensure a uniform magnitude).

\enote{Check signature R:updated}{a}

\section{Assessing Each Component}\seclabel{component}
  
Following the standard pipeline for in-context MT \citep{tanzer2024a,hus-anastasopoulos-2024-back,zhang-etal-2024-hire,zhang-etal-2024-teaching}, our goal is to conduct a rigorous investigation of the importance of each type of linguistic resource (component) and its quality to the translation performance. For each input Manchu sentence, a structured prompt is constructed by integrating various components. This prompt is then fed to the LLM to generate a response, from which the translation is extracted. The translation is finally evaluated against the ground truth reference using various metrics.

% Following the standard pipeline of using instruction-tuned LLMs for machine translation, our approach follows these steps: Firstly, a structured translation prompt is constructed for each input sentence in the source language. This prompt is then fed to the LLM to generate a response, from which the translation in the target language is extracted as the final output. Finally, the final translation outputs are evaluated against the gold-standard using various metrics.

%cf \cite{hus-anastasopoulos-2024-back}'s Preliminaries and Problem Definition

\paragraph{Formulation of Prompts} We represent the prompt formulation as $\pi(\cdot)$ which takes several arguments as input. Let $\mathbf{x}$ be the Manchu sentence to be translated. The simplest prompt is $\pi(\mathbf{x})$, which asks the LLM to directly translate $\mathbf{x}$ into the target language, without providing any additional information. The prompt template can be augmented by adding more optional arguments as follows -- each representing one component.\footnote{Prompt templates are illustrated in Appendix~\ref{sec:appendix prompts}.}

\begin{itemize}
    \item A morphological analyzer $\mu(\cdot)$, which transforms $\mathbf{x}$ into segmented and analyzed morphemes. The result is represented as $\mu(\mathbf{x})$.
    \item Dictionary entries $\mathrm{D}$ retrieved from a bilingual dictionary $\mathcal{D}$.
    \item Parallel examples $\mathrm{P}$ retrieved from a parallel corpus $\mathcal{P}$.
    \item Grammar excerpts $\mathrm{G}$ retrieved from a grammar book $\mathcal{G}$.
    \item CoT prompting instructions $\mathrm{C}$ selected from a set of prompting varieties $\mathcal{C}$.
\end{itemize}

\paragraph{Sequential Integration of Components} 
As we also consider several implementations of various quality of most components, assessing all possible combinations
would result in an untractable number of combinations.
Therefore, we add components to $\pi(\cdot)$ sequentially and compare performance between implementations for that component. 
The best-performing one is used as a new baseline when we evaluate the next component.
Specifically, starting with the simple baseline $\pi(\mathbf{x})$, we first add the morphological analyzer, a fundamental element for subsequent retrieval components, resulting in $\pi(\mu(\mathbf{x}))$.\footnote{We only consider one version of $\mu(\mathbf{x})$, which constitutes an essential component for all other linguistic resources.}
We then consider components that have multiple variants.
To begin with, we consider various ways to specify $\mathrm{D}$ and select the best one $\pi(\mu(\mathbf{x}), \mathrm{D}^*)$ which is the new baseline for subsequent add-ons. 
We then follow the order $\mathrm{P}$, $\mathrm{G}$, $\mathrm{C}$, resulting in
$\pi(\mu(\mathbf{x})^*, \mathrm{D}^*, \mathrm{P})$ (assessing multiple ways to select parallel examples), 
$\pi(\mu(\mathbf{x})^*, \mathrm{D}^*, \mathrm{P}^*, \mathrm{G})$ (assessing varieties of grammar excerpts), and 
$\pi(\mu(\mathbf{x})^*, \mathrm{D}^*, \mathrm{P}^*, \mathrm{G}^*, \mathrm{C})$ (assessing variants of CoT instruction).
This order prioritizes components that are expected to be most beneficial, with less helpful components introduced at later stages, as suggested by previous works \citep{zhang-etal-2024-hire,hus-anastasopoulos-2024-back}.
The pipeline is demonstrated in Figure~\ref{fig:pipeline}.
In the following sections, we study each component in detail and report experimental results obtained using the \textbf{GPT-4o model} on the evaluation set of 337~Manchu-English parallel sentences.

\begin{figure}
    \centering
    \setlength{\abovecaptionskip}{-0.2cm}
    \setlength{\belowcaptionskip}{-0.4cm}
    \includegraphics[width=0.48\textwidth]{pipeline_illustration.pdf}
    \caption{Illustration of the in-context MT pipeline with the components of $\pi(\mu(\mathbf{x}), \mathrm{D}, \mathrm{P}, \mathrm{G})$}
    \label{fig:pipeline}
\end{figure}

% This approach ensures the baseline evaluation score is initially high, enhancing the reliability of evaluation, as differences between low scores are more likely attributable to noise [as noted by Prof. Yvon; may cite some work] rather than genuine impact from the addition of a component.

% To evaluate how the quality of each component affects translation performance, we test multiple variants for each component. To avoid excessive complexity in the experiment due to the multiplicative combinatorial possibilities, we analyze one component at a time, identify the best-performing variant, and retain only this optimal variant for subsequent steps.

% [Skip these for better focus of paper?: Sentence-level: mnc-eng, mnc-zho,
% Document-level: mnc-eng, mnc-jpn
% - we add this after the optimal pipeline is completed, the only run once for languages other than English, to compare the difference acorss languages (testing goal: would zhn be higher? would japanese be higher because of similarity to manchu?)]
% [Do we add directions of eng/zho/jpn-to-mnc as well?]

\subsection{Morphological Analysis}\seclabel{morphological_analyzer}

% \citet{zhang-etal-2024-hire} used a Manchu morphological analyzer\footnote{
% According to released codes, \citet{zhang-etal-2024-hire}'s Manchu morphological analyzer essentially relies on the morphologically analyzed words from the dictionary of \citet{norman2020comprehensive}, accessed from \url{https://buleku.org/home}. The dictionary contains numerous entries that provide explanations for the derivations or inflections of particular words, but the coverage is limited and it is not a true morphological analyzer in itself.
% }, which in practice is a dictionary look-up with very limited coverage.

Morphological analysis is usually performed in a naive way in previous studies. For instance, \citet{zhang-etal-2024-hire} simply perform a dictionary look-up -- searching for \textbf{inflected word forms} in a dictionary, of which the coverage is limited.\footnote{ The dictionary is from \citet{norman2020comprehensive}. Access can be made on \url{https://buleku.org/home}.} As Manchu is an agglutinative language and exclusively uses suffixation, identifying word stems and suffixes is straightforward. Therefore, we implement a rule-based morphological analyzer that splits an input sentence into a stem and a sequence of suffixes.  Both the \textbf{list of word stems} and the \textbf{set of allowed suffixes} are obtained from the dictionary. Our morphological analyzer then attempts to recursively detach a suffix from the end of a string until the remaining segment matches a known word stem. After the morphological analysis, a Manchu sentence is transformed into a list of morphemes (containing word stems and suffixes), which serves as the basis for retrieving dictionary entries, parallel examples, and grammar excerpts.

It is possible for a Manchu word to have multiple analyses. For example, \textit{tere} could be a demonstrative pronoun meaning ``that'', or it could be analyzed as \textit{te-re}, meaning ``sitting'' as the present participle of the verb \textit{te} ``to sit''. 
In such cases, we include all possible analyses in the prompt and let the LLM resolve the ambiguity by selecting the most contextually appropriate interpretation, as shown in Table~\ref{example:disambiguation} of Appendix~\ref{sec:output_examples}.

%[- we might need an experiment(?) or some example with/without morphological analysis to more strongly validate our point of morphological analyzer being very helpful. Without morphological analyzer, our retrieval of dictionary entries and grammar section will be bad. - we can compared the results of the following: fuzzy matching methods, LingoLLM's Manchu methods, our own morphological analyzer. To convincely show that our morphological analyzer is the best.]

\subsection{Dictionary}
The dictionary $\mathcal{D}$ comprises lexical entries, suffixes, and their corresponding collocations. These three elements -- lexical entries, suffixes, and collocations -- form the foundation of our three variants:
\enote{Could we use 'glosses' instead of 'explanations'? R: I think 'explanations' is better here, as our entry includes more detailed multi-word explanations than simple gloss}{a}

\begin{itemize} 
    \item $\mathrm{D^l}$ includes only the \textbf{lexical} entries retrieved for each word in the input sentence, without explanation for the suffixes.\footnote{For polysemous words, all senses are included in the prompt, so that the LLM can choose the most contextually appropriate sense. See example in Table~\ref{example:disambiguation} of Appendix~\ref{sec:output_examples}.}
    \item $\mathrm{D^{l+s}}$ includes both the \textbf{lexical} entries and explanations for \textbf{suffixes} for all morphemes appearing in the input sentence.
    \item $\mathrm{D^{l+s+c}}$ includes the \textbf{lexical} entries, explanations of \textbf{suffixes}, and the \textbf{collocations} for all morphemes in the input sentence.
    %\item $\mathrm{D^{m(l+s+c)}}$, which follows $\mathrm{D^{l+s+c}}$ while the entries for infrequent words or suffixes are \textbf{masked} out. Any words or suffixes outside the top 500 most frequent morphemes within the corpus of 1795 sentences, are considered as infrequent. This setting mimics a dictionary with limited coverage to assess the impact of coverage on performance.
\end{itemize}

\begin{table}[!ht]
\setlength{\abovecaptionskip}{0cm}
\setlength{\belowcaptionskip}{-0.4cm}
\setlength{\tabcolsep}{3mm}{}
\centering
\begin{tabular}{lrrr}
\hline
\textbf{Variant} & \textbf{BLEU} & \textbf{chrF} & \textbf{SBERT} \\
\hline
$\pi(\mathbf{x})$  & 3.44 & 21.86 & 34.21 \\
\hline
$\pi(\mu(\mathbf{x}))$  & 3.10 & 21.68 & 33.49 \\
w/ $\mathrm{D^{l}}$  & 7.40 & 31.84 & 58.91 \\
w/ $\mathrm{D^{l+s}}$  & 7.47 & \textbf{32.93} & 59.78 \\
w/ $\mathrm{D^{l+s+c}}$  & \textbf{7.55} & 32.71 & \textbf{61.07} \\
%$+\mathrm{D^{m(l+s+c)}}$  & 7.88 & 26.21 & 0.4348 \\
\hline
\end{tabular}
\caption{MT scores for direct prompting $\pi(\mathbf{x})$ and prompting with morphologically analyzed sentences $\pi(\mu(\mathbf{x}))$, and with \textbf{dictionary} entries of increasing complexities. \textbf{Bold}: best result for each column.}
\label{tab:dict}
\end{table}

\paragraph{Comprehensive dictionary entries are important.}
As shown in Table~\ref{tab:dict}, using a morphological analyzer alone is not helpful -- $\pi(\mu(\mathbf{x}))$ performs worse than $\pi(\mathbf{x})$. This is expected as simply transforming the input sentence to a segmented list of morphemes does not provide the model much knowledge about Manchu.
Once the explanations for the lexical entries are included, performance improves significantly. 
The translation quality can be further improved with the inclusion of suffixes ($\mathrm{D^{l+s}}$) and then collocations ($\mathrm{D^{l+s+c}}$).
Although the chrF score of $\mathrm{D^{l+s+c}}$ is slightly lower than $\mathrm{D^{l+s}}$, both the BLEU and SBERT scores suggest that the $\mathrm{D^{l+s+c}}$ delivers the best overall translations.
Therefore, $\pi(\mu(\mathbf{x}), \mathrm{D}^{l+s+c})$ will be used as a new baseline when assessing the following component.
We illustrate the benefits of dictionary information (lexical entries, suffixes, and collocations) for translation in Tables~\ref{example:disambiguation} and~\ref{tab:output_dict} in Appendix~\ref{sec:output_examples}.

\subsection{Parallel Examples}
Parallel examples are drawn from the corpus introduced in~\secref{setup}.
Ideally, these parallel examples $\mathcal{P}$ should closely resemble the input sentence, as higher similarity with the source text is known to improve translation quality \citep{zhang-etal-2024-teaching}.
To explore this, we construct three variants for $\mathcal{P}$, each exhibiting a different degree of similarity:

\begin{itemize} 
\item $\mathrm{P^r}$ includes 10 parallel sentences \textbf{randomly} selected as few-shot examples.
\item $\mathrm{P^{d}}$ includes up to 10 parallel sentences retrieved based on shared terms. As the parallel sentences are extracted from the \textbf{dictionary}, they are originally meant to illustrate the meaning of a specific dictionary entry. Therefore, for a lexeme in the input sentence, the parallel examples for its dictionary entry are retrieved. % (cf. \secref{setup}).
\item $\mathrm{P^{bm}}$ includes 10 parallel sentences retrieved using the \textbf{BM25} algorithm \citep{robertson1995okapi} implemented by Rank-BM25.\footnote{\url{https://github.com/dorianbrown/rank_bm25}}
  % BM25 scores a document's relevance to a query by term frequency-inverse document frequency (TF-IDF). -- FY: not true
  The terms used by the retriever are morphemes segmented by the analyzer of \secref{morphological_analyzer}.
\end{itemize}

\begin{table}[!ht]
\setlength{\abovecaptionskip}{0cm}
\setlength{\belowcaptionskip}{-0.4cm}
\setlength{\tabcolsep}{2mm}{}
\centering
\begin{tabular}{lrrr}
\hline
\textbf{Variant} & \textbf{BLEU} & \textbf{chrF} & \textbf{SBERT} \\
\hline
$\pi(\mu(\mathbf{x}), \mathrm{D^*})$   & 7.55 & 32.71 & 61.07 \\
w/ $\mathrm{P^{r}}$  & 7.66 & 32.94 & 60.85 \\
w/ $\mathrm{P^{d}}$  & 8.10 & 32.95 & 61.04 \\
w/ $\mathrm{P^{bm}}$  & \textbf{8.84} & \textbf{33.72} & \textbf{61.35} \\
\hline
\end{tabular}
\caption{Performance comparison between the baseline (no parallel examples) and 3 ways to select \textbf{parallel examples}. \textbf{Bold}: best result for each column.}
\label{tab:para}
\end{table}

\paragraph{More similar parallel examples improve the translation.}
As shown in Table~\ref{tab:para},  randomly retrieved parallel examples provide only a slight improvement over the baseline, as they do not seem to introduce much useful information into the context. On the other hand, selecting parallel examples that are similar to the input sentence yields more noticeable improvements (see lines $\mathrm{P^{d}}$ and $\mathrm{P^{bm}}$ in Table~\ref{tab:para}).
$\mathrm{P^{bm}}$ achieves the best performance across all 3 evaluation metrics, as BM25 aims to retrieve parallel examples that are globally similar to the input sentence; $\pi(\mu(\mathbf{x}), \mathrm{D}^{l+s+c}, \mathrm{P^{bm}})$ will be used as a new baseline when assessing the following component.
An example of how parallel examples help translation is in Table~\ref{example:output_para} in Appendix~\ref{sec:output_examples}.

\subsection{Grammar}
As mentioned in % Paragraph \ref{para:grammar} of
\secref{setup}, two grammar books -- a short and a more detailed one -- serve as source materials. 
%We manually compiled 26 pairs, each indexed by a specific Manchu grammatical feature and accompanied by relevant excerpts from both grammar books.
For each book, we manually compile 26 tuples consisting of (\textit{feature, excerpt}), in which each Manchu grammatical feature is paired with the corresponding excerpt from the short or the long grammar book.
% We manually compiled 26 triples consisting of \textit{(feature, short excerpt, long excerpt)} in which each Manchu grammatical feature is paired with the relevant excerpts from both grammar books.
\enote{fy}{Not clear at all - we need an example in the annex R: rewrote the paragraph, is it clearer now?}
With our morphological analyzer, we extract a set of grammatical features from the source Manchu sentence and generate a tailored grammar combination accordingly, consisting of only excerpts that are relevant to that sentence. 
This approach is much more efficient than dumping the entire grammar book into the context.
%which might exceed the context window limit of some LLMs.
We consider 3 ways to retrieve excerpts $\mathrm{G}$ from grammar books:
\begin{itemize} 
\item $\mathrm{G^{s}}$ is a combination of grammar excerpts, retrieved from the \textbf{short} book.
\item $\mathrm{G^l}$ is a combination of grammar excerpts, retrieved from the \textbf{long} grammar book with more detailed explanations. 
% where we still exclude any parallel example.
    % Importantly, these grammatical explanations exclude any parallel examples, ensuring a clear ablation of the impact of grammatical explanations alone.
    \item $\mathrm{G^{l+p}}$ additionally adds \textbf{parallel} examples that illustrate the grammar excerpts, which are originally included in the \textbf{long} grammar book.
    % is also a subset of excerpts from the \textbf{long} grammar book, including \textbf{parallel} examples that illustrate the grammar excerpts.
\end{itemize}
In addition to the excerpts, we include a fixed paragraph shared by all variants, which contains basic information about the word order and typological features of Manchu
% that is relevant for most sentences 
(see Appendix~\ref{sec:appendix prompts}).

\begin{table}[ht!]
\setlength{\abovecaptionskip}{0cm}
\setlength{\belowcaptionskip}{-0.5cm}
\setlength{\tabcolsep}{2mm}{}
\centering
\begin{tabular}{lrrr}
\hline
\textbf{Variant} & \textbf{BLEU} & \textbf{chrF} & \textbf{SBERT} \\
\hline
$\pi(\mu(\mathbf{x}),\mathrm{D^{*}},\mathrm{P^{*}})$ & 8.84 & 33.72 & \textbf{61.35} \\
w/ $\mathrm{G^{s}}$  & 8.26 & 33.12 & 60.70 \\
w/ $\mathrm{G^{l}}$  & 8.46 & \textbf{33.79} & 61.17 \\
w/ $\mathrm{G^{l+p}}$  & \textbf{8.90} & 33.77 & 60.40 \\
\hline
\end{tabular}
\caption{Performance comparison between the baseline without \textbf{grammar} and 3 different variants of retrieving grammar excerpts. \textbf{Bold}: best result for each column.}
\label{tab:grammar}
\end{table}

\paragraph{Grammars hardly help.}
As shown in Table \ref{tab:grammar}, $\mathrm{G^{s}}$ yields scores worse than the baseline. With more detailed explanations, $\mathrm{G^{l}}$ leads to a slight improvement in chrF score, and when further accompanied by parallel examples, $\mathrm{G^{l+p}}$ leads to a small improvement in BLEU score. 
Nevertheless, compared to the performance reported for the other components, i.e., dictionary and parallel examples (cf. Tables~\ref{tab:dict} and \ref{tab:para}), the improvement seems marginal and is not reflected in SBERT scores.
%This suggests that comprehensiveness affects whether the model can leverage the grammar excerpts.
This suggests that grammars do not help much in in-context MT, %except when they also include parallel examples, 
which is aligned with the previous findings of \citet{aycock2024llmsreallylearntranslate}.
Nevertheless, we have found instances where grammar explanations could aid translation, such as the example of Table \ref{example:grammar} in Appendix~\ref{sec:output_examples}.
Moreover, since the next component -- CoT -- involves grammatical annotation and syntactic analysis, which are closely tied to the information provided in the grammar excerpts, we will still include the grammar component into the new baseline for assessing the CoT component. The variant $\mathrm{G^{l+p}}$ is selected based on the BLEU score.

\subsection{Chain-of-Thought}

CoT prompting instructs LLMs to generate a series of intermediate results
before solving the final task \citep{cot2022wei}. We draw CoT prompt templates from \texttt{LingoLLM} \citep{zhang-etal-2024-hire}, which are explicit instructions provided in the context. 
We consider 2 variants for CoT prompting $\mathrm{C}$:
\begin{itemize} 
\item $\mathrm{C^a}$ asks the LLM to \textbf{annotate} the grammatical and semantic features of each word in the sentence before computing the translation. 
\item $\mathrm{C^{a+s}}$ asks the LLM to proceed step by step, first to annotate the grammatical and semantic features of each word, then analyze the sentence's \textbf{syntactic structure}, finally produce the translation. 
\end{itemize}

\begin{table}[ht!]
\setlength{\abovecaptionskip}{0cm}
\setlength{\belowcaptionskip}{-0.5cm}
\setlength{\tabcolsep}{1.2mm}{}
\centering
\begin{tabular}{lccc}
\hline
\textbf{Variant} & \textbf{BLEU} & \textbf{chrF} & \textbf{SBERT} \\
\hline
$\pi(\mu(\mathbf{x}),\mathrm{D^{*}},\mathrm{P^{*}},\mathrm{G^{*}})$ & \textbf{8.90} & \textbf{33.77} & \textbf{60.40} \\
w/ $\mathrm{C^{a}}$  & 8.01 & 33.13 & 59.81 \\
w/ $\mathrm{C^{a+s}}$  & 8.49 & 33.43 & 59.01 \\
\hline
\end{tabular}
\caption{Performance comparison between the baseline without CoT prompting and 2 different variants of CoT. \textbf{Bold}: best result for each column.}
\label{tab:cot}
\end{table}

\paragraph{CoT does not help the model generate better translations.}
Explicitly prompting the model to perform intermediate generation steps results in a noticeable decline in both $\mathrm{C^{a}}$ and $\mathrm{C^{a+s}}$. This aligns with the findings of \citet{elsner-needle-2023-translating},
%\citep{huang-etal-2023-languages}
where CoT does not improve performance.
This discrepancy seems to arise from erroneous or incomplete deductions within the intermediate steps (cf. Table \ref{example:cot_unhelpful} in Appendix~\ref{sec:output_examples}). This further indicates that, even with the CoT prompting, the model is still unable to effectively utilize the grammar. Consequently, we exclude the CoT component and the Grammar component from our final pipeline.

\section{Analysis} \seclabel{analysis}
\enote{Improve title}{a}

\subsection{Performance Across Models}

We have so far used the GPT-4o model to assess the importance of each component and its quality, finding that the best setting is  $\pi(\mu(\mathbf{x}),\mathrm{D^{l+s+c}},\mathrm{P^{bm}})$.
%$\pi(\mu(\mathbf{x}),\mathrm{D^{l+s+c}},\mathrm{P^{bm}},\mathrm{G^{lp}})$.
We now study performance variation across models for this setting.
Table \ref{tab:model_size} presents the results.
% We are also interested in how different LLMs, especially the open-source ones with different sizes, perform on Manchu-to-English translation. 

\paragraph{Model size matters.}
The smallest model, i.e., Llama3-1B, yields an extremely low BLEU score of 0.27. When manually checking the translation, we found that the Llama3-1B model often does not follow the instructions, generating outputs where the translation is difficult to extract or missing entirely. With the size increase in the Llama3 family, we see a consistent improvement in translation scores. Through manual inspection of the translations from varying model sizes (see Table~\ref{example:size} in~\ref{sec:output_examples}), we observe that larger models not only exhibit better instruction-following abilities but also are also better at leveraging the information included in the context. Therefore, we hypothesize that LLM-based MT relies on both good instruction-following and in-context learning abilities, which are closely related to the model size. 
% Among the LLama3 fami, Llama3-70 achieves even comparable results to the close-source model GPT-4o.

\begin{table}
% \setlength{\abovecaptionskip}{0cm}
\setlength{\belowcaptionskip}{-0.5cm}
%\setlength{\tabcolsep}{2mm}{}
\centering
\begin{tabular}{lrrr}
\hline
\textbf{Model} & \textbf{BLEU} & \textbf{chrF} & \textbf{SBERT} \\
\hline
Llama3-1B & 0.27 & 9.95 & 16.37 \\
Llama3-3B  & 1.81 & 21.95 & 38.46 \\
Llama3-8B  & 3.05 & 26.59 & 49.10 \\
Llama3-70B  & 6.31 & 31.01 & 56.82 \\
\hline
GPT-4o  & 8.84 & 33.72 & 61.35 \\
\hline
DeepSeek-V3 & \textbf{12.35} & \textbf{37.93} & \textbf{65.64} \\
\hline
\end{tabular}
\caption{Performance of different LLMs using the best setting, i.e., $\pi(\mu(\mathbf{x}),\mathrm{D^{l+s+c}},\mathrm{P^{bm}})$. \textbf{Bold}: best result for each column.}%
\label{tab:model_size}
\end{table}

\paragraph{The performance could be underestimated}
The best performance is obtained with 
%GPT-4o (close-source) and 
DeepSeek-V3, achieving BLEU scores of 12.35. The score is still low, especially when compared with LLM-based translation for high-resource languages \citep{alves-etal-2023-steering,sia-etal-2024-anti}.
However, we often observe that the in-context MT translations are semantically close to the reference, yet exhibit significant differences in wordings, suggesting that BLEU and chrF scores actually underestimate the MT quality, as illustrated by the example in Table~\ref{tab:sbert_scores}.
When assessed with SBERT, the best-performing model (DeepSeek-V3) achieves a score of 65.64, indicating a strong semantic similarity between the translation and the reference.

\begin{table}[h!]
\centering
   % \setlength{\abovecaptionskip}{0cm}
    \setlength{\belowcaptionskip}{-0.5cm}
\begin{tabular}{p{1.9cm}p{5cm}}
\hline
Input: & \textit{ereci julesi gurgu elgiyen} \\
\hline
Translation: & From this point forward, wild animals are abundant. \\
Reference: & From there onwards beasts were plentiful. \\
\hline
\textbf{BLEU:} 4.99 & \textbf{chrF:} 24.02 \,\,\,\, \textbf{SBERT:} 62.42\\
\end{tabular}
\caption{An example where BLEU and chrF scores (\textbf{sentence-level}) underestimate the translation quality, while SBERT better reflects the translation quality.}
\label{tab:sbert_scores}
\end{table}

\enote{Is this sentence level BLEU ? A metric that is not too recommanded 
R:Yes, sentence-level BLEU}{a}


% \begin{figure}
%     \centering
%     \setlength{\abovecaptionskip}{-0.2cm}
%     \setlength{\belowcaptionskip}{-0.3cm}
%     \includegraphics[width=0.48\textwidth]{plot1.pdf}
%     \caption{Performance on $\pi(\mu(\mathbf{x}),\mathrm{D^{l+s+c}},\mathrm{P^{bm}})$ across different LLMs.}
%     \label{fig:plot1}
% \end{figure}

\subsection{Exposing Prior Knowledge of Manchu with Character-Level Encryption} \seclabel{encryption}

\begin{figure}
    \centering
    \setlength{\abovecaptionskip}{-0.2cm}
    \setlength{\belowcaptionskip}{-0.5cm}
    \includegraphics[width=0.48\textwidth]{plot2.pdf}
    \caption{Performance comparison between encrypted $\pi(\mu(\mathbf{x})_{e}, \mathrm{D^{l+s+c}_{e}}, \mathrm{P^{bm}_{e}})$ and original $\pi(\mu(\mathbf{x})), \mathrm{D^{l+s+c}}, \mathrm{P^{bm}})$ across different LLMs}
    \label{fig:plot2}
\end{figure}

We have so far assumed that the MT performance of LLMs is mostly attributed to their \textbf{in-context abilities}, rather than to some \textbf{prior knowledge of Manchu} that can possibly be gained in its training stage.
% An underlying issue when most previous studies assess the in-context translation -- whether the performance should be attributed to the in-context learning capability or the LLM's prior knowledge of the low-resource language. 
To explore this question, we create a ``fake Manchu'' aimed to eliminate this possible confounding factor.
Specifically, we ``encrypt'' all Manchu tokens by a simple character-level mapping as follows:
% \footnote{This encrytion method is inspired by the Chain Shift in historical linguitics, which describes a phenomenon where the sounds involved in a historical sound change can be ordered into a ``chain'', such as the Great Vowel Shift of the English language.}
each vocalic character in Manchu (\textit{a, e, i, o, u}), it is mapped to the \textbf{next} character in this list, e.g., $\textit{a} \rightarrow \textit{e}$, $\textit{e} \rightarrow \textit{i}$, and $\textit{u} \rightarrow \textit{a}$. The same mapping applies to consonantal characters in (\textit{b, c, d, f, g, h, j, k, l, m, n, p, q, r, s, t, v, w, x, y, z}). Through this mapping, a Manchu token \textit{amban} is encrypted as \textit{encep}.

Encryption applies to all tokens in the input Manchu sentence as well as the linguistic resources involving Manchu such as dictionary entries and parallel examples,
%and grammatical explanations, 
while the English parts remain unchanged.
This approach ensures that the LLM can only rely on the information provided in the prompt and its in-context learning ability.
Encrypted prompts are denoted with the subscript $_{e}$ as in: $\pi(\mu(\mathbf{x})_{e}, \mathrm{D_{e}}, \mathrm{P_{e}})$. We experiment with the original template $\pi(\mu(\mathbf{x}), \mathrm{D^{l+s+c}}, \mathrm{P^{bm}})$ and its encrypted version $\pi(\mu(\mathbf{x})_{e}, \mathrm{D^{l+s+c}_{e}}, \mathrm{P^{bm}_{e}})$.\footnote{We use $\pi(\mu(\mathbf{x})), \mathrm{D^{l+s+c}}, \mathrm{P^{bm}})$ because the grammar excerpts contain a mixture of Manchu and English tokens, which makes it difficult to encrypt only the Manchu tokens.} The results achieved for multiple LLMs are in Figure~\ref{fig:plot2}.

\paragraph{LLMs already know some Manchu.}
The performance of the encrypted version tends to be slightly lower than the original version for all LLMs. This suggests that all models have seen some Manchu in their pretraining stage, possibly due to contamination -- pretraining corpora often contain significant amounts of non-English texts, including many low-resourced ones \citep{blevins-zettlemoyer-2022-language}. The performance drop is particularly noticeable for DeepSeek-V3. We hypothesize that DeepSeek-V3 has seen more Manchu data during its pretraining stage because it was trained on large Chinese corpora, which may contain more Manchu texts.

\paragraph{LLMs rely more on their in-context learning ability.} Even though all LLMs have some prior knowledge of Manchu, 
%its influence is almost marginal, as indicated by only small performance drops from the original version to the encrypted version (except for DeepSeek-V3). 
as indicated by the drop in performance from the original version to the encrypted version, the encrypted versions can still achieve comparable results, and the performance gap remains relatively small (except for DeepSeek-V3).
This confirms that LLMs are not fully relying on their prior knowledge, but are rather mainly depending on their in-context learning ability. This argument can be further supported by the consistent performance improvement for both the original and the encrypted Manchu texts when increasing the model size. Since the Llama3 family models are trained on the same data, the observed performance gain, such as from Llama3-1B to Llama3-70B, should be largely attributable to the enhanced in-context learning capabilities of the larger model.

\section{NMT Data Augmentation}\seclabel{augmentation}

We present a follow-up study where we use our in-context MT system to generate more parallel data for training an NMT model. This data augmentation approach follows the \textit{forward-translation} method \citep{bogoychev2020domaintranslationesenoisesynthetic}.
% parallel data augmentation. 

\paragraph{Translating Monolingual Corpus.} Specifically, we use our in-context MT system to translate 42,240 sentences 
% \textbf{[number might change]}
from the monolingual Manchu corpus (cf.~\secref{setup}) into English, using our best-performing method $\pi(\mu(\mathbf{x})), \mathrm{D^{l+s+c}}, \mathrm{P^{bm}})$ with DeepSeek-V3. The resulting synthetic parallel corpus is combined with the real parallel corpus to train an NMT model of Manchu-to-English.

\paragraph{Fine-Tuning mT5.}
We fine-tune mT5-small \citep{xue-etal-2021-mt5}, an encoder-decoder multilingual pre-trained model on the Manchu-to-English translation task. To systematically assess the impact of synthetic data, we use different data-mixing strategies, e.g., only real parallel data, or additionally with synthetic data that is several times the real data. The performance is evaluated on the same evaluation set of 337 parallel sentences (cf. \secref{setup}).

Figure \ref{fig:plot_finetune} presents the results. The model trained exclusively on real data performs extremely badly across all metrics, suggesting that 3,520 parallel sentences are insufficient for training an effective NMT model. However, as more synthetic parallel data is introduced, performance improves consistently. The best-performing model -- trained with real data and synthetic parallel data that are 12 times larger than the real data -- achieves results comparable to or even surpassing Llama3-70B. The resulting fine-tuned mT5-small model only contains around 300M parameters and is significantly more efficient than a 70B-parameter in-context MT system. 
This study underscores the potential of leveraging in-context MT for data augmentation, enabling the development of more effective and efficient NMT models for low-resource languages.
\enote{Cite forward translation paper by Sennrich ? R: \citet{bogoychev2020domaintranslationesenoisesynthetic} cited}{a}

\begin{figure}
    \centering
    \setlength{\abovecaptionskip}{-0.2cm}
    \setlength{\belowcaptionskip}{-0.5cm}
    \includegraphics[width=0.48\textwidth]{plot_finetune.pdf}
    \caption{Performance comparison of the fine-tuned mT5 model using only real parallel data versus incorporating varying proportions of synthetic parallel data generated by our in-context MT system. We observe a steady improvement in performance as more synthetic parallel data is added, ultimately achieving scores that match the in-context MT results of Llama3-70B.}
    \label{fig:plot_finetune}
\end{figure}

\section{Conclusion}

In this paper, we conduct a comprehensive investigation of in-context MT for low-resource languages, using Manchu as a case study. We examine the impact of different linguistic resources and the quality of each component on translation performance. Our findings highlight that high-quality dictionaries and properly retrieved parallel examples are the most influential factors, while grammar
and CoT prompting appears to have no noticeable benefit.
Furthermore, through encrypted evaluation, we disentangle the effects of LLMs' prior knowledge of Manchu from their in-context learning ability. Our results show that while LLMs possess some prior knowledge of the language, 
%its impact is limited.
they primarily rely on in-context learning for translation.
Finally, our follow-up study demonstrates a practical application of in-context MT: generating synthetic parallel data. This approach has the potential to enhance NMT systems, offering a viable strategy for improving translation in low-resource languages.

\section*{Limitations}
Our current work only includes a single language, Manchu, as a case study. Although our encryption method can be considered a generalization effort applicable to any language unfamiliar to the LLMs, the encryption did not alter the fundamental properties of Manchu, which is an agglutinative language characterized by a relatively clear separation between morphemes. It remains unclear whether our findings extend to typologically distinct languages.

We have only focused on the translation direction from Manchu to English and have not explored the reverse direction. However, if the goal is to produce synthetic parallel data with good quality, we believe it is advantageous to translate authentic low-resource language into a high-resource language that the LLM is proficient in. This ensures fluency and authenticity of the texts in both the source and target languages.

Lastly, we have only explored a limited range of CoT strategies. Our current results indicate that the extra CoT steps often introduce new errors, resulting in a poorer final translation. Future work could investigate ways to mitigate these undesired effects, such as through better prompt engineering or by providing guiding examples for the CoT process.

\section*{Acknowledgments}
This research was supported by DFG (grant SCHU 2246/14-1). We are deeply thankful to Fresco Sam-Sin of the Manchu Foundation and Professor Hitoshi Kuribayashi from the Tohoku University, for generously granting us the permission to use the digitized Manchu materials available on their websites.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\section{Manchu Language}
\label{sec:appendix A}
Manchu (ISO 639-3: \texttt{mnc}) is a critically endangered Tungusic language native to Northeast China (historically also known as Manchuria). 
Typologically, Manchu is a head-final agglutinative language that makes exclusive use of suffixation for denoting grammatical features, with each suffix having one single function. In this regard, Manchu is often grouped together with other typologically similar languages across Eurasia, e.g., Japanese, Korean, Mongolian, Turkish and Hungarian.

Manchu is the traditional language of the Manchu people, who founded the Qing dynasty (1644-1911) of China. During this period, Manchu was one of the official government language, leaving behind a rich collection of historical texts. However, from the 18th century onward, the Manchu language experienced a gradual decline, which accelerated significantly after the fall of the Qing dynasty up until 1980s, at which point the number of native speakers was down to only a few hundreds,\footnote{This number exludes the speakers of the Xibe language (ISO 639-3: \texttt{sjo}), which is sometimes considered as a dialect of Manchu. Xibe is in more vigorous usage and has around 30,000 native speakers. The written form of Xibe is mostly identical to Manchu aside from several orthographical conventions.}
%(Currently it is estimated that there are only around 10 elderly individuals alive who can speak Manchu natively,) 
out of more than 10 million ethnic Manchu population.

Starting from 1980s, there have been increased efforts to revitalize the Manchu language, as the Manchu communities strive to restore their lost heritage. Primary schools in several Manchu Autonomous Counties, as well as some universities, began offering Manchu courses at various levels. Through the revitalization movements, despite the ultra-small number of genuine native speakers, the number of people who can now speak Manchu as a second language has been steadily growing.

Traditionally, Manchu is written in the Manchu script, an alphabetic writing system that can be easily transliterated into Latin script. All the Manchu data used within our research were already in the form of Latin transliteration.

Manchu is a low-resource language in terms of the available text data on the internet. On the other hand, because of its significant historical importance, Manchu has been extensively studied by generations of linguists and philologists. There exist abundant linguistic resources, including dictionaries, grammar books, some bilingual parallel sentences, and a decent amount of monolingual text, which makes Manchu well-suited for our case study.

%[the previous works not only did a LowResourceLang-to-English but also English-to-LowResourceLang, we may need to motivate why we did not do English-to-Manchu?: native speaker of Manchu is not so many, and there is abundance of Manchu monolingual data, importance of Manchu historical documents so the best practise could be to generate English from Manchu monolingual data then train a neural machine translation model?]

%\section{Data?}\label{sec:appendix B}

\section{Prompts}
\label{sec:appendix prompts}
\begin{itemize}
\item \textbf{Direct Translation:}\\
Please help me translate the following sentence from \textcolor{teal}{\{source language\}} to \textcolor{red}{\{target language\}}:\\
\textcolor{green}{\{sentence\}}\\
Please try your best to translate, it's okay if your translation is bad. Do not refuse to try it. I won't blame you.\\
Please enclose your translation in \#\#\#.
For example, if your translation is ``Hello world'', the last part of your output should be \#\#\# Hello world \#\#\#\\

\item \textbf{Direct Translation with Morphologically Analyzed Sentence:}\\
Please help me translate the following sentence from \textcolor{teal}{\{source language\}} to \textcolor{red}{\{target language\}}:\\
\textcolor{blue}{\{morphologically analyzed sentence\}}\\
The morphemes in this sentence have been segmented: the verb stem and verbal suffixes are separated by '=', the noun stem and nominal suffixes are separated by '$\sim$'. 
Note that some words can be either analyzed as a whole, or as a word stem plus a suffix, the different analyses are separated by '/'. In such case, explanations for both analyses are given below, and you need to choose which one is the most appropriate in the given context.\\
Please try your best to translate, it's okay if your translation is bad. Do not refuse to try it. I won't blame you.\\
Please enclose your translation in \#\#\#.
For example, if your translation is "Hello world", the last part of your output should be \#\#\# Hello world \#\#\#\\

\item \textbf{General Template for Prompts with Components:}\\
Please help me translate the following sentence from \textcolor{teal}{\{source language\}} to \textcolor{red}{\{target language\}}:\\
\textcolor{blue}{\{morphologically analyzed sentence\}}\\
The morphemes in this sentence have been segmented: the verb stem and verbal suffixes are separated by '=', the noun stem and nominal suffixes are separated by '$\sim$'. 
Note that some words can be either analyzed as a whole, or as a word stem plus a suffix, the different analyses are separated by '/'. In such case, explanations for both analyses are given below, and you need to choose which one is the most appropriate in the given context.\\
\textcolor{purple}{\{components\}}

Using all the information provided above, now please translate the sentence into \textcolor{red}{\{target language\}}.
Remember your source sentence is: \textcolor{blue}{\{morphologically analyzed sentence\}}\\
Please enclose your translation in \#\#\#.
For example, if your translation is "Hello world", the last part of your output should be \#\#\# Hello world \#\#\#\\

\item \textbf{Component Dictionary:}\\
For the translation task, you are given the word by word mapping from the \textcolor{teal}{\{source language\}} words to the \textcolor{red}{\{target language\}} words.
Some words can be polysemous and there might be multiple possible English translations. In such a case, please choose the most appropriate one.
Note that for some words, they might be derived from a more basic form, we call this the parent word. The parents are also given in the word-by-word translation.
Here are the dictionary entries for each individual word in the source sentence:\\
\textcolor{orange}{\{dictionary entries\}}

Note that sometimes two or more words can form a collocation and express a specific meaning. You should refer to the collocations listed under the dictionary entries. 
For example, 'mama' means 'grandmother', 'erxe=' means 'to attend', but 'mama erxe=' as a collocation means 'to get smallpox'.
In such case, explain which collocation meaning you think is most appropriate in the context.

\item \textbf{Component Parallel Examples:}\\
To help with the translation, here are some \textcolor{teal}{\{source language\}}-\textcolor{red}{\{target language\}} parallel sentences that may be helpful for your translation:\\
\textcolor{pink}{\{parallel examples\}}

\item \textbf{Component Grammar:}\\
You are also given this grammar book below. Feel free to rely on this grammar book in your translation task:\\
- Manchu Grammar Book\\
The Manchu language is typologically similar to the Mongolic and Turkic languages. 
All Manchu phrases are head-final; the head-word of a phrase (e.g. the noun of a noun phrase, or the verb of a verb phrase) always falls at the end of the phrase. 
Thus, adjectives and adjectival phrases always precede the noun they modify, and the arguments to the verb always precede the verb. 
As a result, Manchu sentence structure is subject–object–verb (SOV).\\
Manchu also makes extensive use of converb structures and has an inventory of converbial suffixes to indicate the relationship between the subordinate verb and the finite verb that follows it.\\
Unlike English, which uses prepositions, Manchu exclusively uses postpositions.\\
The Manchu language is agglutinative in word structure, meaning that words are formed by adding suffixes to the root, and each morpheme in a word has one distinct meaning or grammatical function.\\
\textcolor{magenta}{\{grammar excerpts\}}"""

\item \textbf{Component Chain-of-Thought Prompting (Annotation):}\\
Given the previous information, please first annotate the meaning and grammatical features of each word in the sentence.\\
For each word, based on their English translation and whether it ends with '='(marker of verb stems), first decide whether the word is nominal (noun/adjective), or a verbal(verb, converb) or else (other part of speech such as adverb, postposition ect.).\\
Then for each noun, please annotate its number (singular/plural) and case (Nominative/Genitive
/Dative-Locative/Accusative/Ablative), based on the particles/suffixes that follow the noun.\\
And for each verb, please annotate its tense (perfect/imperfect) and form (Affirmative/Negative/Interrogative/Imperative/Optative/Desiderative), based on the suffixes attached to the verb.

Then based on the annotations, translate the sentence from \textcolor{teal}{\{source language\}} into \textcolor{red}{\{target language\}} based on the annotations and the analyzed sentence structure. 

\item \textbf{Component Chain-of-Thought Prompting (Annotation + Syntactic Analysis):}\\
Given the previous information, please proceed with the following steps:\\
Step 1:\\
Please first annotate the meaning and grammatical features of each word in the sentence.\\
For each word, based on their English translation and whether it ends with '='(marker of verb stems), first decide whether the word is nominal (noun/adjective), or a verbal (verb, converb) or else (other part of speech such as adverb, postposition etc.).\\
Then for each noun, please annotate its number (singular/plural) and case (Nominative/Genitive
/Dative-Locative/Accusative/Ablative), 
based on the particles/suffixes that follow the noun.\\
And for each verb, please annotate its tense (perfect/imperfect) and form (Affirmative/Negative/Interrogative/Imperative/Optative/Desiderative), based on the suffixes attached to the verb.\\
Step 2:\\
Then based on the annotations, please analyze the sentence structure by figuring out what the subject and object of each verb is. Keep in mind that \textcolor{teal}{\{source language\}}'s basic word order is subject–object–verb (SOV) and it is a head-final language, so that the adjectives and participles always precede the noun they modifies, and the arguments to the verb always precede the verb.\\
Note that clauses can be combined into a single sentence by using converbs, which relate the first action to the second.\\
The final step:\\
Translate the sentence into \textcolor{red}{\{target language\}} based on the annotations and the analyzed sentence structure.

\end{itemize} 

\section{Implementation Details}\label{sec:implementation_details}

\paragraph{Hyperparameters of LLM Generations}
For the Llama3 models, we performed our translation experiments using vLLM.\footnote{\url{https://github.com/vllm-project/vllm}} The model was configured to use half-precision (\texttt{dtype=`float16'}) with a maximum context length of 20,000 tokens. For generation, we used a temperature of 0.9, top‑$p$ sampling with $p = 0.9$, and a maximum output length of 5,000 tokens.

For translation with GPT-4o and DeepSeek-v3, we used OpenAI and DeepSeek's APIs respectively, with their default settings (i.e., a temperature of 1.0 and top‑$p$ sampling with $p = 1.0$).

\paragraph{Hyperparameters of Fine-Tuning}
For fine-tuning mT5-small, we set the learning rate to 5e-4 and batch size to 16. We evaluated the model per epoch and employed the early stopping: the training is terminated if no improvement (drop in loss on validation set) is observed over 2 consecutive evaluation steps.
The best-performing checkpoint (with the minimum loss on the validation set) is selected as the final model.

\section{Output Examples}\label{sec:output_examples}

\begin{table*}[h!]
%\small
\centering
\begin{tabular}{p{2.1cm}p{7.5cm}p{4.4cm}}
\toprule
    & Retrieved Dictionary Entries\newline
    (Input Sentence: \textit{se udu oho})  & Translation \\ 
\midrule
$\pi(\mu(\mathbf{x}))$ & \textbf{morphological segmentation:}\newline
 se udu oho/o-ho.\newline
 \textsl{(alternative analyses are separated by `/' )} & It happened to be the time. \\
$+\mathrm{D^{l}}$ & \textbf{+lexical entries:}\newline
 se: {\color{blue}1. year (said of age), age} 2. raw silk, unprocessed silk 3. the juncture of the stem and root on the ginseng plant 4. (plural suffix)\newline
 udu: {\color{orange}1. ``How many?", ``How much?"} 2. several 3. although \newline
oho: armpit \newline
o-: 1. to become, to change into 2. {\color{red}to be, to exist} 3. to be proper, to be permissible
& {\color{orange}\underline{How many}} {\color{red}\underline{are}} the {\color{blue}\underline{years}}? \\
 \midrule
Ground Truth & & {\color{orange}\underline{How many}} {\color{blue}\underline{years}} old? \\
\bottomrule
\end{tabular}
\caption{Comparison between the outputs of $\pi(\mu(\mathbf{x}))$ and $\mathrm{D^{l}}$. The LLM selects the most appropriate sense, interpreting \textit{se} as ``year'' and \textit{udu} as ``How many''. Additionally, the LLM correctly identifies that \textit{oho} is better analyzed as \textit{o-ho}, where \textit{o-} means ``to be'', rather than as \textit{oho} (``armpit'') which does not fit in this context.}
\label{example:disambiguation}
\end{table*}


\begin{table*}[h!]
%\small
\centering
\begin{tabular}{p{2.1cm}p{7.5cm}p{5cm}}
\toprule
    & Retrieved Dictionary Entries\newline
    (Input Sentence: \textit{gvsai ejen jiha...})  & Translation \\ 
\midrule
$\mathrm{D^{l}}$ & \textbf{lexical entries:}\newline
 gvsa: banner \hspace{1cm} ejen: master, ruler\newline
 ji-: to come & the ruler of the banner comes...\\
$\mathrm{D^{l+s}}$ & \textbf{+suffixes:}\newline
 -i: it is used to indicate possession (Genitive) or the means by which something is accomplished (Instrumental)\newline
 {\color{orange}-ha: Perfect participle: ara- 'to do', ara-ha 'he.did, who has done, what was done'} & the ruler of the banner {\color{orange}\underline{came}}... \\
$\mathrm{D^{l+s+c}}$ & \textbf{+collocations:}\newline
 {\color{blue}gvsa-i ejen: Lieutenant-General (of a banner)} & the {\color{blue}\underline{Lieutenant-General}} {\color{orange}\underline{came}}... \\ \midrule
Ground Truth     &      & Lieutenant-general came...  \\ 
\bottomrule         
\end{tabular}
\caption{Comparison of the outputs for $\mathrm{D^{l}}$, $\mathrm{D^{l+s}}$ and $\mathrm{D^{l+s+c}}$. As the example shows, the additional information about suffixes and collocations helps the model to correctly translate the tense of the verb and the phrase \textit{gvsai ejen}.}
\label{tab:output_dict}
\end{table*}

\begin{table*}[h!]
%\small
\centering
\begin{tabular}{p{3.2cm}p{6cm}p{5.5cm}}
\toprule
    & Retrieved Parallel Examples\newline
    (Input Sentence: \textit{geli emu hvlha be we waha be sarkv.})  & Translation \\ 
\midrule
$\pi(\mu(\mathbf{x}), \mathrm{D^{l+s+c}})$ & - & Still, \underline{it is unknown} who has killed one bandit.\\
$+\mathrm{P^{r}}$ & ... & Also, one bandit, who has killed, \underline{is not known}.\\
$+\mathrm{P^{d}}$ & ... & Also, who has killed one bandit \underline{without knowing it}?\\
$+\mathrm{P^{bm}}$ & Manchu: wede hvlhame gamabuha be inu sarkv.\newline
English: {\color{orange}I} don't know who stole it.\newline... & Still, \underline{{\color{orange}I} do not know} who killed one bandit. \\ \midrule
Ground Truth     &      & Who killed the other rebel, \underline{{\color{orange}I} do not know}.  \\ \bottomrule
\end{tabular}
\caption{Comparison between the outputs of the baseline $\pi(\mu(\mathbf{x}), \mathrm{D^{l+s+c}})$ and the variants of adding parallel examples retrieved by different ways ( $+\mathrm{P^{r}}$, $+\mathrm{P^{d}}$ and $+\mathrm{P^{bm}}$). As the example demonstrates, when using the BM25 algorithm, the retrieved parallel example helps the model to recover the subject `I', which is often omitted in Manchu but can be inferred from the context.}
\label{example:output_para}
\end{table*}

\begin{table*}[h!]
%\small
\centering
\begin{tabular}{p{3.2cm}p{8cm}p{2.5cm}}
\toprule
    & Retrieved Grammar Excerpts\newline
    (Input Sentence: \textit{muke \underline{be} genekini})  & Translation \\ 
\midrule
$\pi(\mu(\mathbf{x}),\mathrm{D^{l+s+c}},\mathrm{P^{bm}})$ & \textbf{lexical entries:}\newline
be: accusative particle & go {\color{blue}\underline{by}} water\\
$+\mathrm{G^{s}}$ & \textbf{+grammar excerpts:}\newline
  An object of a verb having definite or specific reference is shown with the particle be.
& go \underline{to} the water \\
$+\mathrm{G^{l}}$ & \textbf{+grammar excerpts:}\newline
...{\color{blue}The accusative may express space within and means of conveyance by which the motion is going on.} & go {\color{blue}\underline{by}} water\\
$+\mathrm{G^{l+p}}$ & \textbf{+grammar excerpts +example:}\newline 
...{\color{blue}The accusative may express space within and means of conveyance by which the motion is going on.}\newline
  jugvn {\color{blue}be} yabu-me\newline
  road {\color{blue}ACC} go-CONV “to go {\color{blue}along} the road;”
& go {\color{blue}\underline{by}} water\\
\midrule
Ground Truth     &      & travel {\color{blue}\underline{by}} water  \\ \bottomrule
\end{tabular}
\caption{Comparison between the outputs of the baseline $\pi(\mu(\mathbf{x}),\mathrm{D^{l+s+c}},\mathrm{P^{bm}})$ and the variants of adding grammar excerpts( $+\mathrm{G^{l}}$, $+\mathrm{G^{l+p}}$ and $+\mathrm{G^{s+p}}$). With the more detailed long grammar excerpt explaining a specific function of Manchu accusative as \textit{expressing space within and means of conveyance by which the motion is going on}, the model correctly selects the English preposition `by'. In contrast, when provided with the short grammar excerpt of $\mathrm{G^{s+p}}$, the model choose the preposition `to' as the translation for the accusative particle. This likely stems from the model's broader knowledge of the accusative case, including its usage in other languages such as German, where it can correspond to the English preposition `to'. However, in the case of Manchu, such a translation is incorrect. Notably, in the baseline $\pi(\mu(\mathbf{x}),\mathrm{D^{l+s+c}},\mathrm{P^{bm}})$ without any grammar excerpts, the model also translates the preposition correctly, either by chance or through contextual inference.}
\label{example:grammar}
\end{table*}

\begin{table*}[h!]
%\small
\centering
\begin{tabular}{p{4cm}p{4cm}p{6cm}}
\toprule
    & Translation\newline
    (Input Sentence: \textit{nikan coohiyan tob seme emu dulin})  & Model Response \\ 
\midrule
$\pi(\mu(\mathbf{x}),\mathrm{D^{l+s+c}},\mathrm{P^{bm}},\mathrm{G^{l+p}})$ & Chinese and Korea are exactly in one middle & 
... \\
$+\mathrm{C^{a}}$ 
 & {\color{red}Halfway} between China and Korea & Now, let's analyze ... "emu dulin" modifies the action, indicating the manner or status as "one middle" or "a middle stage" contextually suggesting {\color{red}halfway} or middle grounds ...\\
$+\mathrm{C^{a+s}}$ 
& {\color{red}The statement or identity concerning} the Chinese and Korea is exactly {\color{red}as described} in the middle. & Step 2: Analyze the sentence structure ... 
The sentence implies a direct {\color{red}identification or explanation regarding} the entities "Chinese" (nikan) and "Korea" (coohiyan) with respect to something being exactly in the middle or balanced ("emu dulin") {\color{red}described} as "just" or "right" ...\\ \midrule
Ground Truth    & Chinese and Korean each just one half.  & \\ \bottomrule
\end{tabular}
\caption{The translations and some of the model responses involving CoT steps, comparing the baseline with the variants of adding CoT prompting instruction($+\mathrm{C^{a}}$ and $+\mathrm{C^{a+s}}$). As shown in the model responses, the CoT prompting makes the model engage in multiple iterative reasoning steps. While these steps are intended to help the translation, it may also introduce errors at each turn, leading to a worse final translation.}
\label{example:cot_unhelpful}
\end{table*}


\begin{table*}[h!]
%\small
\centering
\begin{tabular}{p{1.1cm}p{8.5cm}p{5cm}}
\toprule
    & Retrieved Dictionary Entries\newline
    (Input Sentence: \textit{mini morisa be ganabufi...})  & Translation \\ 
\midrule
$\mathrm{D^{l}}$ & \textbf{lexical entries:}\newline
 mini: my, of me \hspace{1cm} morin: horse\newline
 be: 1. we (exclusive) 2. (accusative particle)
ganabu-: (causative of gana-); gana-: to fetch
 & After my \underline{horse{\color{orange}s}} were fetched...\\
 $\mathrm{D^{l}_{e}}$ & \textbf{lexical entries:}\newline (\textbf{encrypted sentence:} nopo nusote ci hepecago)\newline
 nopo: my, of me \hspace{1cm} nusop: horse\newline
 ci: 1. we (exclusive) 2. (accusative particle)
hepeca-: (causative of hepe-); hepe-: to fetch
 & My \underline{horse} was caused to fetch...\\
$\mathrm{D^{l+s}}$ & \textbf{+suffixes:}\newline
 -fi: Perfect converb: ara- 'to do', ara-fi 'having done, he did and (then did something else)'.\newline
 {\color{orange}-sa: (-sa/-se/-so) Plural: sakda 'old man', sakda+sa 'old men'.} & My \underline{horse{\color{orange}s}} having been fetched...\\ \midrule
Ground Truth &      & I had my \underline{horse{\color{orange}s}} fetched and... \\ \bottomrule         
\end{tabular}
\caption{Comparison of the outputs betweem $\mathrm{D^{l}}$, $\mathrm{D^{l}_{e}}$ and $\mathrm{D^{l+s}}$. This example suggests that the LLM already has some prior knowledge about the Manchu plural suffix so that even when the information about plural suffix \textit{-sa} is not included in the prompt of $\mathrm{D^{l}}$, the model is still able to identify the plurality. In contrast, when the Manchu tokens are encrypted, there is no clue for plurality in the context, leading the model to incorrectly identify the noun as singular.}
\label{tab:encrypt}
\end{table*}

\begin{table*}[h!]
%\small
\centering
\begin{tabular}{p{2.4cm}p{7cm}p{1cm}p{1cm}p{1cm}}
\toprule
    Model & Translation \newline(Input Sentence: \textit{ere uthai tere gucu inu})  & BLEU & chrF& SBERT\\ 
\midrule
Llama3-1B & te=re/tere inu uju be tongki & 0.0 & 7.75 & 1.67\\
Llama3-3B & This friend and that one are the same, at the same time, or both are friends. & 2.86 & 27.46 & 42.74 \\
Llama3-8B & Even though he/she/it sits, a friend. & 6.77 & 31.36 & 42.81
\\
Llama3-70B & This one, then, is also living with a friend. & 8.05 & 35.86 & 46.97 \\ 
GPT-4o & This immediately is that friend too & 19.3 & \textbf{45.72} & 70.88 \\
DeepSeek-V3 & This is also that friend & \textbf{20.8} & 45.26 & \textbf{81.96}\\
\midrule
Ground Truth     &   This is that very friend.  &\\ \bottomrule
\end{tabular}
\caption{Comparison of outputs across different LLMs, along with their \textbf{sentence-level} BLEU and chrF scores, as well as SBERT scores. We \textbf{bold} the best score for each metric. In this example, Llama3-1B fails to follow the instructions, generating a sentence in Manchu. This illustrates a clear trend that as models become larger and/or more advanced, the translation quality consistently improves.}
\label{example:size}
\end{table*}


\end{document}
