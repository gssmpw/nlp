\section{Related Work}
%\paragraph{In-Context Machine Translation}: talk about some related papers that use the in-context learning ability: selecting enough examples as context, zero-shot, one-shot, few-shot MT; requiring some knowledge in the low-resource language \citep{hendy2023goodgptmodelsmachine} : GPT3.5,both high and low-resource, low-resource language example: Icelandic, Hausa. Mentioned Document-level MT, but only EN-DE \citep{robinson-etal-2023-chatgpt} uses FLORES-200 dataset, compare 204 high and low-resource languages, GPT3.4 and GPT4, GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages \citep{cahyawijaya-etal-2024-llms} LLMs Are Few-Shot In-Context Low-Resource Language Learners \citep{ramos2024grammamtimprovingmachinetranslation}: GrammaMT, interlinear glossing

\paragraph{Low-resource NMT}
The challenges posed by limited parallel data motivate extensive research into innovative strategies for low-resource NMT
\citep{haddow-etal-2022-survey,yazar2023lowresource}. 
Various approaches have been proposed to improve translation quality in such settings. Data augmentation techniques, such as back-translation \citep{sennrich-etal-2016-improving,edunov-etal-2018-understanding} and forward-translation \citep{bogoychev2020domaintranslationesenoisesynthetic}, have been widely used to generate synthetic parallel data and enhance model performance. 
Data augmentation, coupled with unsupervised and semi-supervised methods for bilingual dictionary induction have enabled translation with minimal parallel data, relying instead on monolingual resources \citep{Lample2018unsupervised,Artetxe2018Unsupervised}.
Transfer learning has also proven effective, where models pretrained on high-resource language pairs can be adapted to low-resource languages \citep{zoph-etal-2016-transfer}. 
Recent advancements in multilingual NMT also show that models trained on multiple language pairs can better deal with low-resource languages \citep{ko-etal-2021-adapting,mohammadshahi-etal-2022-small,nllbteam2022languageleftbehindscaling}.
Despite these advancements, achieving high-quality translation in low-resource scenarios remains a significant challenge.

\paragraph{LLM-based In-context MT for Low-Resource Languages}
% LLMs can be directly used to perform translation without explicitly being trained. This is achieved by providing instructions and demonstrations in the prompt \citep{NEURIPS2020_1457c0d6,lin-etal-2022-shot,vilar-etal-2023-prompting}. However, this LLM-based MT has been shown to struggle with rare words -- words that appear less often in the training set \citep{ghazvininejad2023dictionarybasedphraselevelpromptinglarge}. This is particularly the case for low-resource languages that are underrepresented in the training data of LLM \citep{touvron2023llama2openfoundation,workshop2023bloom176bparameteropenaccessmultilingual}. To address this issue, some studies integrate relevant \textit{dictionary entries} and similar \textit{parallel sentence examples} into the prompt \citep{ghazvininejad2023dictionarybasedphraselevelpromptinglarge,zhang-etal-2024-teaching}. 
% \textit{Grammar excerpts} retrieved from Grammar books has also been shown to benefit the low-resource languages \citep{tanzer2024a,hus-anastasopoulos-2024-back}.
% Another line of work additionally leverages \textit{morphological analyzers} to transform the input sentence into morphemes before retrieving relevant information \citep{zhang-etal-2024-hire}.
% In addition to components of linguistic resources, \citet{elsner-needle-2023-translating} show CoT prompt can help improve translation quality. However, although there have been a few investigations into the effectiveness of each component for high-resource languages \citep{raunak-etal-2023-dissecting,chitale-etal-2024-empirical}, little attention is given to low-resource languages. This work presents the first of its kind to investigate the components in LLM-Based MT for low-resource languages, using Manchu as a case study.
Although not explicitly trained for machine translation, LLMs can perform translation by following instructions and demonstrations in the prompt \citep{NEURIPS2020_1457c0d6,lin-etal-2022-shot,vilar-etal-2023-prompting}. LLM-based MT however struggles with rare words that appear infrequently in the training data \citep{ghazvininejad2023dictionarybasedphraselevelpromptinglarge}. This issue is particularly pronounced for low-resource languages that are underrepresented in the LLM's training corpora \citep{workshop2023bloom176bparameteropenaccessmultilingual,touvron2023llama2openfoundation}.
To mitigate this, some studies incorporate linguistic resources into prompts, such as \textbf{dictionary entries} and \textbf{parallel sentence examples} \citep{ghazvininejad2023dictionarybasedphraselevelpromptinglarge,zhang-etal-2024-teaching}, as well as \textbf{grammars} \citep{tanzer2024a,hus-anastasopoulos-2024-back}. Some works also include \textbf{morphological analyzers} to decompose input sentences into morphemes \citep{zhang-etal-2024-hire}.
In addition to linguistic resources, \textbf{CoT}  has also been experimented in a machine translation context \citep{elsner-needle-2023-translating}. 
%While some studies have explored the effectiveness of these components for high-resource languages \citep{raunak-etal-2023-dissecting,chitale-etal-2024-empirical}, little attention has been given to low-resource languages. This work is the first to systematically investigate the role of these components in LLM-based in-context MT for low-resource languages, using Manchu as a case study.
However, little attention has been given to how the quality of each component affects the LLM-based in-context MT. Moreover, there has been no clear ablation between the LLMs' prior knowledge of the language and the linguistic information provided in context.
Addressing these limitations of previous studies, our work systematically investigates the role of each of these components in LLM-based MT for low-resource languages, using Manchu as a case study.

% As one of the earliest pioneer works, 
% \citet{ghazvininejad2023dictionarybasedphraselevelpromptinglarge} proposes an approach called DIPMT (Dictionary-based
% Prompting for Machine Translation). Namely, for a given input sentence in the source language, bilingual \textbf{dictionary entries} corresponding to the words in the sentence are retrieved and incorporated into a prompt, which is then provided to LLMs to generate the translation in the target language. Built upon DIPMT, \citet{zhang-etal-2024-teaching} introduces DIPMT++, which makes two improvements: 1. Improving lexical coverage through methods such as fuzzy-matching, and 2. further incorporating \textbf{parallel examples} into the prompt, which are retrieved through algorithms like BM25.
% \citet{tanzer2024a} further enhances this line of works by their MTOB (Machine Translation from One Book) approach, which includes \textbf{grammar books} as another source of information. They extracted a bilingual wordlist, a small parallel corpus and grammar book excerpts from a single grammar book of Kalamang, and experimented incorporating these 3 kinds of resources into the prompt to see how they affect the in-context machine translation performance. They conclude that retrieved parallel examples are the most beneficial, followed by the wordlist, followed by excerpts from the grammar book, and combining these 3 kinds of information tends to further improve performance.

% While \citet{tanzer2024a}  represents a crucial contribution in this stream of research, 
% %it also exhibits several notable limitations: Firstly, its test set for evaluating machine translation performance has only 100 parallel sentences, which renders their results less reliable. Secondly, 
% their approach lacks any morphological analysis, relying instead on fuzzy matching techniques like the longest-common substring distance to handle inflected and out-of-vocabulary words, which results in not only less reliable retrieval of dictionary entries, but also suboptimal retrieval of parallel examples, as these examples are retrieved based on word matching between the input sentence and sentences within the parallel English-Kalamang corpus.
% Moreover, they tried to retrieve excerpts from the grammar book that are most similar to the input sentence using cosine similarity in an embedding space and longest common substring distance, which is inaccurate due to inherent mismatch between the nature of an input source language sentence and that of a grammar book passage.
% Lastly, as \citet{aycock2024llmsreallylearntranslate} pointed out, the grammar book excerpts used in \citet{tanzer2024a} are actually a mixture of parallel examples and grammatical explanations, and the improvement actually stems from the parallel examples. With the parallel examples removed, there is no evidence that LLMs can effectively exploit grammatical explanations for machine translation task.

% Addressing MTOB's limitation of lacks of morphological analysis, \citet{zhang-etal-2024-hire} makes use of \textbf{morphological analyzers} to firstly transform the input sentence into morphemes, before retrieving dictionary entries and grammatical information. However, the full potential of the morphological analysis has yet been overlooked: while the grammatically analyzed morphemes could have been greatly helpful for retrieving grammar passages that are relevant to the specific input sentence, they unconditionally made use of the same grammar book without any retrieval mechanism, resulting in an excessively long prompt with a lot of information irrelevant to the input sentence. 
% %They claimed that including a grammar is beneficial, but only provided a qualitative comparison between with-grammar and without-grammar. Moreover, 
% Their approach also lacks a proper retrieval of parallel examples, as parallel examples were simply randomly sampled as few-shot prompting. 
% %The test set size was also very small (e.g. 70 sentences for Manchu).

% \citet{hus-anastasopoulos-2024-back} scale up MTOB's approach to 16 low-resource languages using PanLex
% \citep{kamholz-etal-2014-panlex} for dictionaries, 
% FLORES-200 dataset
% \footnote{\url{https://github.com/openlanguagedata/flores}} for parallel examples and the DReaM corpus
% \citep{virk-etal-2020-dream} for grammars. 
% %For more reliable evaluation of the translation performance, they increased the test set size using FLORES-200's devtest split, which has 1012 sentences for each language. 
% However, their approach also inherits the limitations of the original MTOB approach such as the lack of separation of grammatical explanations from parallel examples and the absence of morphological analysis. On the other hand, it shares with \citet{zhang-etal-2024-hire} the limitation of including an entire grammar book in the prompt, without the retrieval of specific grammar book excerpts.

% Some previous works \citep{elsner-needle-2023-translating,zhang-etal-2024-hire} have additionally experimented with the \textbf{Chain-of-Thought} (CoT) prompting, in which the LLM is instructed to explicitly perform some steps such as identifying the part-of-speech of each word, identifying the subject and object of the verb, and annotating grammatical features (e.g. case, number, tense, aspect) of the words, before generating the actual translation. However, results of these experiments have shown that chain-of-thought is unhelpful for the in-context machine translation task.

% Recognizing the limitations of previous works, our approach aims to address all these shortcomings and offer a more comprehensive and refined methodology.

% [Mention \citet{chitale-etal-2024-empirical}?]
%\begin{itemize} \item Small Test Set Size: Earlier studies typically used small test sets comprising 70, 100, or 200 sentences, \citet{hus-anastasopoulos-2024-back} has 997, whereas our dataset includes approximately 2,000 sentences, providing a more robust basis for evaluation. \item Suboptimal Parallel Examples: Many previous methods randomly sampled parallel examples as few-shot examples, leading to suboptimal results. In contrast, our approach retrieves parallel examples from dictionary example sentences for each word in the input sentence, ensuring the parallel examples are highly relevant for explaining the word's meaning. \item Lack of Proper Morphological Analysis: Previous studies either did not incorporate any morphological analysis or relied on fuzzy matching to handle inflected words. Our method employs a rule-based morphological analyzer to accurately split words into stems and affixes, improving precision. \item Inadequate Dictionary: Earlier approaches often used dictionaries without explanations for affixes or information about word phrases and collocations. (\cite{tanzer2024a} includes affixes, while \cite{zhang-etal-2024-hire} does not.) Our approach overcomes this by providing detailed explanations for individual words, affixes, and collocations, offering a more comprehensive linguistic resource. \item Inefficient Grammar Utilization: Traditional methods used the same grammar book unconditionally, resulting in irrelevant information being included for many input sentences. While Tanzer et al. (2024) attempted to retrieve relevant grammar book sections based on longest common substring distance, their method was highly inaccurate. Benefiting from our morphological analyzer, we can identify suffixes in any given Manchu sentence and use a manually compiled dictionary linking suffixes to relevant grammar book sections. This enables us to generate a tailored grammar book for each sentence, enhancing both accuracy and efficiency. \end{itemize} By addressing these limitations, our approach provides a more effective and nuanced methodology for processing low-resource languages like Manchu.

%