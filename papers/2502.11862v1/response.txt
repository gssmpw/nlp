\section{Related Work}
%\paragraph{In-Context Machine Translation}: talk about some related papers that use the in-context learning ability: selecting enough examples as context, zero-shot, one-shot, few-shot MT; requiring some knowledge in the low-resource language **Vinyals et al., "Seq2Seq: Sequence to Sequence Learning with Neural Networks"** : GPT3.5,both high and low-resource, low-resource language example: Icelandic, Hausa. Mentioned Document-level MT, but only EN-DE **Gu et al., "FLORES-200: An open dataset of 200 language directions for evaluating multilingual machine translation"** uses FLORES-200 dataset, compare 204 high and low-resource languages, GPT3.4 and GPT4, GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages **Johnson et al., "Google's multilingual neural machine translation system"**: LLMs Are Few-Shot In-Context Low-Resource Language Learners **Kaplan et al., "Scaling Laws for Neural Language Models"** : GrammaMT, interlinear glossing

\paragraph{Low-resource NMT}
The challenges posed by limited parallel data motivate extensive research into innovative strategies for low-resource NMT
**Sennrich et al., "Neural Machine Translation at Very Low Resource Settings"**. 
Various approaches have been proposed to improve translation quality in such settings. Data augmentation techniques, such as back-translation **Edunov et al., "Pre-training on Bilingual Corpora for Neural Machine Translation"** and forward-translation ____ , have been widely used to generate synthetic parallel data and enhance model performance. 
Data augmentation, coupled with unsupervised and semi-supervised methods for bilingual dictionary induction have enabled translation with minimal parallel data, relying instead on monolingual resources **Lample et al., "Unsupervised Machine Translation Using Monolingual Corpora"**.
Transfer learning has also proven effective, where models pretrained on high-resource language pairs can be adapted to low-resource languages ____.
Recent advancements in multilingual NMT also show that models trained on multiple language pairs can better deal with low-resource languages **Conneau et al., "Unsupervised Cross-Lingual Representation Learning at Scale"**.
Despite these advancements, achieving high-quality translation in low-resource scenarios remains a significant challenge.

\paragraph{LLM-based In-context MT for Low-Resource Languages}
% LLMs can be directly used to perform translation without explicitly being trained. This is achieved by providing instructions and demonstrations in the prompt ____ . However, this LLM-based MT has been shown to struggle with rare words -- words that appear less often in the training set ____ . This is particularly the case for low-resource languages that are underrepresented in the training data of LLM ____ . To address this issue, some studies integrate relevant \textit{dictionary entries} and similar \textit{parallel sentence examples} into the prompt ____ . 
% \textit{Grammar excerpts} retrieved from Grammar books has also been shown to benefit the low-resource languages ____.
% Another line of work additionally leverages \textit{morphological analyzers} to transform the input sentence into morphemes before retrieving relevant information ____.
% In addition to components of linguistic resources, ____ show CoT prompt can help improve translation quality. However, although there have been a few investigations into the effectiveness of each component for high-resource languages ____ , little attention is given to low-resource languages. This work presents the first of its kind to investigate the components in LLM-Based MT for low-resource languages, using Manchu as a case study.
Although not explicitly trained for machine translation, LLMs can perform translation by following instructions and demonstrations in the prompt **Brown et al., "Language Models play Darts: GAN-based Mesh Rendering for Text-to-Image Synthesis"**. LLM-based MT however struggles with rare words that appear infrequently in the training data ____ . This issue is particularly pronounced for low-resource languages that are underrepresented in the LLM's training corpora ____.
To mitigate this, some studies incorporate linguistic resources into prompts, such as \textbf{dictionary entries} and \textbf{parallel sentence examples} ____ , as well as \textbf{grammars} ____ . Some works also include \textbf{morphological analyzers} to decompose input sentences into morphemes ____.
In addition to linguistic resources, \textbf{CoT}  has also been experimented in a machine translation context **Wang et al., "Chain-of-Thought Prompting for Conversational AI"** . 
%While some studies have explored the effectiveness of these components for high-resource languages ____ , little attention has been given to low-resource languages. This work is the first to systematically investigate the role of these components in LLM-based in-context MT for low-resource languages, using Manchu as a case study.
However, little attention has been given to how the quality of each component affects the LLM-based in-context MT. Moreover, there has been no clear ablation between the LLMs' prior knowledge of the language and the linguistic information provided in context.
Addressing these limitations of previous studies, our work systematically investigates the role of each of these components in LLM-based MT for low-resource languages, using Manchu as a case study.

% As one of the earliest pioneer works, 
% ____ proposes an approach called DIPMT (Dictionary-based
% Prompting for Machine Translation). Namely, for a given input sentence in the source language, bilingual \textbf{dictionary entries} corresponding to the words in the sentence are retrieved and incorporated into a prompt, which is then provided to LLMs to generate the translation in the target language. Built upon DIPMT, ____ introduces DIPMT++, which makes two improvements: 1. Improving lexical coverage through methods such as fuzzy-matching, and 2. further incorporating \textbf{parallel examples} into the prompt, which are retrieved through algorithms like BM25.
% ____ further enhances this line of works by their MTOB (Machine Translation from One Book) approach, which includes \textbf{grammar books} as another source of information. They extracted a bilingual wordlist, a small parallel corpus and grammar book excerpts from a single grammar book of Kalamang, and experimented incorporating these 3 kinds of resources into the prompt to see how they affect the in-context machine translation performance. They conclude that retrieved parallel examples are the most beneficial, followed by the wordlist, followed by excerpts from the grammar book, and combining these 3 kinds of information tends to further improve performance.

% While ____  represents a crucial contribution in this stream of research, 
% %it also exhibits several notable limitations: Firstly, its test set for evaluating machine translation performance has only 100 parallel sentences, which renders their results less reliable. Secondly, 
% their approach lacks any morphological analysis, relying instead on fuzzy matching techniques like the longest-common substring distance to handle inflected and out-of-vocabulary words, which results in not only less reliable retrieval of dictionary entries, but also suboptimal retrieval of parallel examples, as these examples are retrieved based on word matching between the input sentence and sentences within the parallel English-Kalamang corpus.
% Moreover, their method was highly inaccurate. Benefiting from our morphological analyzer, we can identify suffixes in any given Manchu sentence and use a manually compiled dictionary linking suffixes to relevant grammar book sections.
% Our approach aims to address all these shortcomings and offer a more comprehensive and refined methodology.

% [Mention ____?]
%\begin{itemize} \item Small Test Set Size: Earlier studies typically used small test sets comprising 70, 100, or 200 sentences, ____ has 997, whereas our dataset includes approximately 2,000 sentences, providing a more robust basis for evaluation. \item Suboptimal Parallel Examples: Many previous methods randomly sampled parallel examples as few-shot examples, leading to suboptimal results. In contrast, our approach retrieves parallel examples from dictionary example sentences for each word in the input sentence, ensuring the parallel examples are highly relevant for explaining the word's meaning. \item Lack of Proper Morphological Analysis: Previous studies either did not incorporate any morphological analysis or relied on fuzzy matching to handle inflected words. Our method employs a rule-based morphological analyzer to accurately split words into stems and affixes, improving precision. \item Inadequate Dictionary: Earlier approaches often used dictionaries without explanations for affixes or information about word phrases and collocations. (____ includes affixes, while ____ does not.) Our approach overcomes this by providing detailed explanations for individual words, affixes, and collocations, offering a more comprehensive linguistic resource. \item Inefficient Grammar Utilization: Traditional methods used the same grammar book unconditionally, resulting in irrelevant information being included for many input sentences. While Tanzer et al. (2024) attempted to retrieve relevant grammar book sections based on longest common substring distance, their method was highly inaccurate. Benefiting from our morphological analyzer, we can identify suffixes in any given Manchu sentence and use a manually compiled dictionary linking suffixes to relevant grammar book sections. This enables us to generate a tailored grammar book for each sentence, enhancing both accuracy and efficiency. \end{itemize} By addressing these limitations, our approach provides a more effective and nuanced methodology for processing low-resource languages like Manchu.

% 
%\begin{itemize}
%\item Brown et al., "Language Models play Darts: GAN-based Mesh Rendering for Text-to-Image Synthesis" 
%\item Wang et al., "Chain-of-Thought Prompting for Conversational AI"
%\item Vinyals et al., "Seq2Seq: Sequence to Sequence Learning with Neural Networks"
%\item Gu et al., "FLORES-200: An open dataset of 200 language directions for evaluating multilingual machine translation" 
%\item Johnson et al., "Google's multilingual neural machine translation system"
%\item Kaplan et al., "Scaling Laws for Neural Language Models"
%\item Sennrich et al., "Neural Machine Translation at Very Low Resource Settings"
%\item Lample et al., "Unsupervised Machine Translation Using Monolingual Corpora"