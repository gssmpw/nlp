\section{Related Work}
\paragraph{LLM Training for Issue Resolving.} To enhance the issue resolving capabilities of open-source LLMs, several research works____ have attempted to use software development resources from the open-source community to construct training data and fine-tune open-source LLMs.
____ crawled open-source repositories and utilized closed-source models (e.g., GPT-4o____ and Claude-3.5-Sonnet____) to generate Openhands____ Agent trajectories, and filtered them through unit tests. Then they used the trajectories to fine-tune the Qwen____ model, enabling it to serve as the base model for Openhands.
____ used GPT-4o to generate Agent trajectories on open-source repository issues, and fine-tuned an open-source model with the filtered trajectories.
____ generated CoT data for and edit generation tasks using GPT-4o, and fine-tuned an open-source model to apply it to SWE-Fixer RAG pipeline.
All the above work used SFT to fine-tune models. To the best of our knowledge, we are the first work to leverage reinforced fine-tuning____ to enhance the issue-resolving capabilities of LLMs.

\paragraph{Reinforcement Learning with Rule-based Reward.}
Since OpenAI released o1____ model, many efforts have attempted to enhance LLMs' long-form reasoning capabilities through rule-based reinforcement learning. DeepSeek's R1____ model with rule-based GRPO____ further demonstrates the potential of rule-based rewards. ____ released Kimi-k1.5, also trained with rule-based reinforcement learning. The research community____ has also been working on replicating rule-based reinforcement learning process. ____ trained a 3B model with PPO____ on the Countdown task and observed "\textit{Aha moment}"____ phenomenon, aligns closely with the behavior of R1. ____ trained a 7B model using PPO on Math task and observed that response length initially decreased and then increased (similar as the tendency in Figure~\ref{fig:f3_score}). Previous work mainly focused on mathematical tasks, where rewards can be straightforwardly computed based on ground truth. In this paper, we improve the performance of open-source models in issue-resolving framework through subtask-oriented rule-based reinforcement learning.