\section{Related Work}
\paragraph{LLM Training for Issue Resolving.} To enhance the issue resolving capabilities of open-source LLMs, several research works~\citep{ma2024lingma, xie2025swe, ma2024repository, pan2024training} have attempted to use software development resources from the open-source community to construct training data and fine-tune open-source LLMs.
~\citet{pan2024training} crawled open-source repositories and utilized closed-source models (e.g., GPT-4o~\cite{gpt4o} and Claude-3.5-Sonnet~\cite{Claude}) to generate Openhands~\cite{wang2024executable, wang2024openhands} Agent trajectories, and filtered them through unit tests. Then they used the trajectories to fine-tune the Qwen~\cite{hui2024qwen2} model, enabling it to serve as the base model for Openhands.
~\citet{ma2024lingma} used GPT-4o to generate Agent trajectories on open-source repository issues, and fine-tuned an open-source model with the filtered trajectories.
~\citet{pan2024training} generated CoT data for and edit generation tasks using GPT-4o, and fine-tuned an open-source model to apply it to SWE-Fixer RAG pipeline.
All the above work used SFT to fine-tune models. To the best of our knowledge, we are the first work to leverage reinforced fine-tuning~\cite{luong2024reft} to enhance the issue-resolving capabilities of LLMs.

\paragraph{Reinforcement Learning with Rule-based Reward.}
Since OpenAI released o1~\cite{o1-preview} model, many efforts have attempted to enhance LLMs' long-form reasoning capabilities through rule-based reinforcement learning. DeepSeek's R1~\cite{guo2025deepseek} model with rule-based GRPO~\cite{shao2024deepseekmath} further demonstrates the potential of rule-based rewards. ~\citet{team2025kimi} released Kimi-k1.5, also trained with rule-based reinforcement learning. The research community~\cite{zeng2025simplerl,logic-rl} has also been working on replicating rule-based reinforcement learning process. ~\citet{tinyzero} trained a 3B model with PPO~\cite{schulman2017proximal} on the Countdown task and observed "\textit{Aha moment}"~\cite{guo2025deepseek} phenomenon, aligns closely with the behavior of R1. ~\citet{zeng2025simplerl} trained a 7B model using PPO on Math task and observed that response length initially decreased and then increased (similar as the tendency in Figure~\ref{fig:f3_score}). Previous work mainly focused on mathematical tasks, where rewards can be straightforwardly computed based on ground truth. In this paper, we improve the performance of open-source models in issue-resolving framework through subtask-oriented rule-based reinforcement learning.