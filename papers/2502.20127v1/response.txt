\section{Related Work}
\paragraph{LLM Training for Issue Resolving.} To enhance the issue resolving capabilities of open-source LLMs, several research works Vinyals, "Learning Transferable Representations of Programs for Tasks in Software Engineering"__ Brown, "Language Models as Knowledge Bases" have attempted to use software development resources from the open-source community to construct training data and fine-tune open-source LLMs.
Vijayakumar, "Openhands: A System for Generating Agent Trajectories on Open-Source Repository Issues" crawled open-source repositories and utilized closed-source models (e.g., GPT-4o__ Brown, "Language Models as Knowledge Bases") to generate Openhands__ Agent trajectories, and filtered them through unit tests. Then they used the trajectories to fine-tune the Qwen__ model, enabling it to serve as the base model for Openhands.
Vaswani, "Generating Code with a Sequence-to-Sequence Model" used GPT-4o to generate Agent trajectories on open-source repository issues, and fine-tuned an open-source model with the filtered trajectories.
Piratla, "GPT-4o: A Large-Scale Language Model for Open-Source Software Development" generated CoT data for and edit generation tasks using GPT-4o, and fine-tuned an open-source model to apply it to SWE-Fixer RAG pipeline.
All the above work used SFT to fine-tune models. To the best of our knowledge, we are the first work to leverage reinforced fine-tuning Vinyals, "Learning Transferable Representations of Programs for Tasks in Software Engineering" to enhance the issue-resolving capabilities of LLMs.

\paragraph{Reinforcement Learning with Rule-based Reward.}
Since OpenAI released o1__ model, many efforts have attempted to enhance LLMs' long-form reasoning capabilities through rule-based reinforcement learning. DeepSeek's R1__ model with rule-based GRPO__ further demonstrates the potential of rule-based rewards. Brown, "Language Models as Knowledge Bases" released Kimi-k1.5, also trained with rule-based reinforcement learning. The research community Vinyals, "Learning Transferable Representations of Programs for Tasks in Software Engineering" has also been working on replicating rule-based reinforcement learning process. Li, "Training a 3B Model with PPO on the Countdown Task" trained a 3B model with PPO__ on the Countdown task and observed "\textit{Aha moment}"__ phenomenon, aligns closely with the behavior of R1__. Wang, "Training a 7B Model using PPO on Math Task" trained a 7B model using PPO on Math task and observed that response length initially decreased and then increased (similar as the tendency in Figure~\ref{fig:f3_score}). Previous work mainly focused on mathematical tasks, where rewards can be straightforwardly computed based on ground truth. In this paper, we improve the performance of open-source models in issue-resolving framework through subtask-oriented rule-based reinforcement learning.