\section{Introduction}
\label{sec:intro}
3D point cloud segmentation (\eg, semantic segmentation, instance segmentation) is a fundamental research task in computer vision, particularly in the field of autonomous driving. In recent years, several studies~\cite{zhu2021cylindrical, hou2022point, xu2021rpvnet, kong2023rethinking, lai2023spherical, wu2024ptv3, wang2024sfpnet} have achieved promising results, 
% thanks in part to advancements in neural network backbones~\cite{yan2018second, zhu2021cylindrical, zhao2021ptv1, wu2022ptv2, wu2024ptv3} and the development of autonomous driving datasets~\cite{caesar2020nuscenes, behley2019semantickitti, geiger2013kitti, sun2020waymo}. 
largely attributable to advancements in neural network architectures~\cite{yan2018second, zhu2021cylindrical, zhao2021ptv1, wu2022ptv2, wu2024ptv3} and the emergence of high-quality autonomous driving datasets~\cite{caesar2020nuscenes, behley2019semantickitti, geiger2013kitti, sun2020waymo}. However, point cloud segmentation tasks typically rely on dense point-wise annotations, which are labor-intensive and costly. For example, in the ScanNet dataset~\cite{dai2017scannet}, it takes an average of 22.3 minutes to annotate a single scene. Consequently, reducing the reliance on dense point annotations is an economically beneficial yet challenging problem.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.90\linewidth]{img/finetune.pdf}
    \vspace{-2mm}
    \caption{\textbf{3D Instance Segmentation Performance Comparison.} The weakly supervised YoCo for fine-tuning compared to fully supervised methods. The results show that our YoCo outperforms fully supervised Cylinder3D without fine-tuning (0\%). Fine-tuning YoCo with 0.8\% and 5\% labeled data exceeds the fully supervised SparseUnet and state-of-the-art (SOTA) PTv3, respectively.}
    \vspace{-4mm}
    \label{fig:finetune}
\end{figure}

Recent studies have attempted to address the problem of weakly supervised segmentation on 3D point clouds. Existing methods utilize various types of weak labels, such as sparse point-level labels~\cite{hu2022sqn, zhang2022not, chen2024foundation}, scribble-level labels~\cite{unal2022scribble, wang2024label}, and box-level labels~\cite{li2024box2mask, ngo2023gapro, yu20243when3dbox, jiang2024mwsis}. However, most of these approaches focus on semantic segmentation, while instance segmentation is more complex, as it requires distinguishing different instances within the same semantic category. For 3D instance segmentation tasks, the annotation of 3D bounding boxes is still expensive, although methods~\cite{li2024box2mask, ngo2023gapro, yu20243when3dbox} using 3D bounding boxes as weak supervision have achieved promising results. A recent work, MWSIS~\cite{jiang2024mwsis}, explores weakly supervised instance segmentation for outdoor LiDAR point clouds using low-cost 2D bounding boxes as weak supervision, but with a large performance gap with the fully supervised approach.
% and compares the performance of using pseudo labels generated by SAM~\cite{kirillov2023segment}.
% \textit{we ask whether it is possible to leverage the zero-shot capabilities of vision models like SAM, where a single click on an object could generate higher-quality pseudo labels}.
% \textit{Inspired by the above work, we asked whether it is possible to use a lower cost click-level annotation for outdoor Lidar point cloud instance segmentation, narrowing the gap with the full supervision method and even achieving similar performance.}
% 2024/10/24
Inspired by the work mentioned above, we rethink whether there is a method with lower labeling cost to get better instance segmentation performance, and even narrow the gap between weakly supervised and fully supervised methods.
% 2024/10/23
% Inspired by the work mentioned above, we aim to explore using lower-cost click-level annotations as weak supervision for outdoor LiDAR point cloud instance segmentation, aiming to achieve performance comparable to fully supervised methods.

With this motivation, we propose a single-point supervised instance segmentation framework called \textbf{YoCo}. In this framework, a single click annotation per object in the bird's eye view (BEV) plane is sufficient to generate the corresponding 3D pseudo label for that object. As known, it is a non-trivial task to generate dense 3D pseudo labels from sparse click annotations. Following ~\cite{jiang2024mwsis}, click annotations can be used as prompts for SAM~\cite{kirillov2023segment} to generate corresponding 2D masks, which are then projected to obtain dense 3D pseudo labels. However, a major challenge lies in the limited zero-shot capability of SAM, resulting in noisy or inaccurate 2D masks. Therefore, a critical challenge lies in filtering high-quality 3D pseudo labels from these noisy outputs.
To address this challenge, we introduce a 3D pseudo label generation module based on the vision foundation models (VFMs), named VFM-PLG. Specifically, we use click annotations to obtain the corresponding 2D masks through VFMs, then leverage the geometric constraints (\eg, size, volume, depth, \etc) of the corresponding 3D masks to filter out high-quality 3D pseudo labels. 
In addition, to further improve the quality of pseudo labels, we take advantage of the generalization and robustness of neural networks by introducing two key modules: temporal and spatial-based label updating (TSU) module, and intersection-over-union (IoU)-guided label enhancement (ILE) module. The TSU module refines and updates the pseudo labels by incorporating predictions from adjacent frames, while the ILE module further enhances label quality by replacing lower-quality labels offline with more accurate predictions.

Experimental results show that our YoCo significantly outperforms previous state-of-the-art weakly supervised 3D instance segmentation methods and even surpasses fully supervised Cylinder3D~\cite{zhu2021cylindrical}, as shown in Figure \ref{fig:finetune}.
% In addition, our approach can exceed the fully supervised baseline by fine-tuning the model using only 0.8\% of fully supervised labels.
Additionally, YoCo demonstrates strong generality, making it suitable for various networks. Moreover, by fine-tuning the model with only 0.8\% of fully supervised data, it can surpass the performance of the fully supervised baseline.
In summary, our contributions are summarized as follows:
\begin{itemize}
    \item To the best of our knowledge, we are the first to propose using click annotations for instance segmentation of outdoor LiDAR point clouds. This method significantly reduces the burden of instance segmentation annotations.
    \item We propose the VFM-PLG, which combines VFMs with geometric constraints information of the object to generate high-quality pseudo labels. In addition, the TSU and ILE modules further improve pseudo label quality by leveraging the generalization and robustness of neural networks.
    \item Extensive experiments on the Waymo dataset demonstrate that our YoCo achieves state-of-the-art performance in weakly supervised instance segmentation, surpassing fully supervised methods such as Cylinder3D. Furthermore, YoCo exhibits strong generality, making it applicable across various networks.
\end{itemize}