\section{Method}
\label{sec:method}
% 我们的目标是缩小click-level弱监督和全监督算法间的性能差距，如何生成高质量的伪标签是一个关键问题。
% Our goal is to narrow the performance gap between click-level weakly supervised and fully supervised algorithms. Generating high-quality pseudo-labels is key to achieving this.
Our goal is to generate high-quality 3D instance pseudo labels using sparse click-level annotations, and to narrow the performance gap between weakly supervised and fully supervised methods. To achieve this, we propose a simple yet effective framework, \textbf{YoCo}, which integrates pseudo label generation with network training, as illustrated in Figure \ref{fig:framework}. By leveraging the minimal input of click annotations, YoCo efficiently creates reliable pseudo labels that maintain strong performance, even with limited supervision. The detailed process is outlined as follows:

For pseudo label generation in Figure \ref{fig:framework}(a), given a set of calibrated images and point cloud data, we first annotate the point cloud with click-level labels in the BEV plane. These labels are then projected onto the corresponding images, and processed by our proposed VFM-PLG module. The VFM-PLG leverages the VFMs and geometric constraints from the point cloud to generate high-quality 3D pseudo labels, as described in Section \ref{sec:module_1}.

For network training in Figure \ref{fig:framework}(b), we adopt the MeanTeacher~\cite{tarvainen2017meanteacher} method, which involves a student network and a teacher network. The teacher network is updated using an exponential moving average (EMA) of the student’s weights. It predicts labels from adjacent frames, and the TSU module uses these predictions to refine the 3D pseudo labels generated by the VFM-PLG. This refinement incorporates temporal and spatial information from adjacent frames, as detailed in Section \ref{sec:module_2}. 

Additionally, to further boost the reliability of the pseudo labels, we introduce the ILE module. This module enhances the labels offline by using high-confidence and high-IoU results to update the 3D pseudo labels, further improving the performance of our method, as outlined in Section \ref{sec:module_3}.
% To leverage the robustness capability of neural networks, we employ the Mean Teacher framework during training. This setup consists of a student network and a teacher network, where the teacher is updated using an exponential moving average (EMA) of the student’s weights. The teacher network predicts labels from adjacent frames, and our TSU module refines the pseudo 3D labels generated by the VFM-PLG. This refinement incorporates temporal and spatial information from nearby frames, enhancing label quality.
% Additionally, to further boost the reliability of the pseudo labels, we introduce the ILE module. This module enhances the labels offline by using high-confidence scores and Intersection-over-Union (IoU) values to update the pseudo 3D labels, further improving the accuracy of our method.

\subsection{Preliminary}
\label{sec:prelimiinary}
Given a set of calibrated images and point cloud data, we utilize sensor calibration to project the point cloud onto the images, establishing a mapping relationship between the 3D points and image pixels. Specifically, consider a set of points $\mathbf{P}^{3\mathbf{d}}=\left\{ p_{i}^{3d} \right\} _{i=1}^{N}=\left\{ \left( x_i,y_i,z_i \right) \right\} _{i=1}^{N} \in \mathbb{R}^{N\times 3}$ we can obtain the corresponding pixel coordinates $\mathbf{P}^{2\mathbf{d}}=\left\{ p_{i}^{2d} \right\} _{i=1}^{N}=\left\{ \left( u_i,v_i \right) \right\} _{i=1}^{N} \in \mathbb{R}^{N\times 2}$ by applying the projection transformation formula:
\begin{equation}
\label{eq:project}
    z_c\left( u_i,v_i,1 \right) ^T=\mathbf{M}_{int}\times \mathbf{M}_{ext}\times \left( x_i,y_i,z_i,1 \right) ^T
\end{equation}
where $N$ is the number of the points, $z_c$ represents the depth of the point in the camera coordinate system, $\mathbf{M}_{int}$ and $\mathbf{M}_{ext}$ denote the intrinsic and extrinsic parameters of the camera, respectively.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{img/sam4cyc.pdf}
    \vspace{-2mm}
    % 同一个实例使用不同的Prompt（图中不同颜色的五角星）生成的2DMask的结果对比。w/o表示不结合DAM，w 表示结合DAM，使用DAM模型在不同Prompt下获得了更一致且更准确的结果。
    % Our VFM-PLG for the composite categories.
    \caption{\textbf{Our VFM-PLG for the Composite Categories.} Comparison of 2D mask results for the same instance with different prompts (colored stars). \textit{w/o} shows results without DAM, while \textit{w} shows results with DAM. Using the DAM model yields more consistent and accurate results across different prompts.}
    \vspace{-1mm}
    \label{fig:sam4cyc}
\end{figure}

\subsection{VFM-Based Pseudo Label Generation}
\label{sec:module_1}
SAM~\cite{kirillov2023segment} is a vision foundation model that inputs images and prompts to generate corresponding 2D masks. The prompts include points, bounding boxes, masks, and texts. By leveraging the projection relationship described in Equation \ref{eq:project}, we project the click-level annotations from point clouds onto the image as prompts to obtain the object's 2D masks. Points that fall within these 2D mask regions can be considered as 3D pseudo labels. This process can be formally expressed by the following equation:
\begin{equation}
\label{eq:color}
    m_i = Color(SAM(I_i,p_i,c_i))
\end{equation}
where $m_i$ denotes the 3D pseudo label to the $i$-th click annotation, \textit{Color} represents the operation of mapping a 2D mask to a 3D pseudo label, $I_i$, $p_i$, and $c_i$ represent the image, 2D coordinate, and the class label corresponding to the $i$-th click annotation, respectively.
% 最后一个是，在BEV平面下，由于缺少高度信息，当前click可能存在多个对应的3D点，错误的prompt将会导致错误的分割结果

However, this approach faces three challenges. First, SAM struggles with segmenting composite categories like cyclists. The second one is inaccuracies in the SAM segmentation mask and the projection relationship. Finally, due to the lack of height information in the BEV plane, there may be multiple corresponding 3D points in the current click, and an incorrect prompt will result in an inaccurate segmentation result.

To address the first issue, we utilize the Depth Anything Model (DAM)~\cite{yang2024depthanythin} as an auxiliary tool to perform depth-based smoothing, particularly for composite categories like cyclists (as shown in Figure \ref{fig:sam4cyc}).
% 具体来说，图片被送入DAM获得深度图，然后使用深度图的特征与prompt进行交互，得到对应的2D掩码
Specifically, the image is processed through the DAM to generate a depth map, and the depth map's features are then used to interact with the prompt's features, resulting in the corresponding 2D mask. (as shown in Figure \ref{fig:vfm-plg}). Therefore, Equation \ref{eq:color} is updated as:
\begin{equation}
\label{eq:color2}
    % m_i=\left\{ \begin{array}{l}
    % 	Color\left( SAM\left( I_i,p_i,c_i \right) \right)\\
    % 	Color\left( SAM\left( DAM\left( I_i \right) ,p_i,c_i \right) \right)\\
    % \end{array} \right.
    m_i= Color\left( SAM\left( DAM\left( I_i \right) ,p_i,c_i \right) \right)
\end{equation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{img/VFM-PLG-V2.pdf}
    \vspace{-2mm}
    \caption{\textbf{Overview of VFM-PLG Module.} The blue dashed line indicates that if the generated 3D mask does not satisfy geometric constraints, another point is selected as the prompt. \textit{GC} denotes that the point cloud is processed using geometric constraints.}
    \label{fig:vfm-plg}
    \vspace{-3mm}
\end{figure}

As for the last two issues, point cloud geometric constraints are proposed for filtering the labels. Specifically, we first obtain a 3D pseudo label $m_i$ for the $i$-th object through the projection relationship. Then a clustering algorithm is applied to $m_i$, resulting in a set of clusters. The cluster containing the click annotation $p_i$ is identified as the 3D pseudo label for that object ($Find$ operation in Equation \ref{eq:filter}).
Next, a geometric consistency check is performed on the identified cluster ($Filter$ operation in Equation \ref{eq:filter}). The label is retained if the cluster satisfies certain geometric conditions; otherwise, the pseudo label is discarded, \ie, $m_i=\left\{0\right\}^N_{i=1}$. This process can be represented as follows:

\begin{equation}
\label{eq:filter}
    \tilde{m}_i=Filter\left( Find\left( Cluster\left( m_i \right) \right) \right)
\end{equation}

If the current click corresponds to multiple points, we iterate to select one as the prompt. If the current result meets the geometric constraints, it is retained as the 3D pseudo label; otherwise, a new point is re-selected as the prompt to generate the pseudo label, as indicated by the blue dashed line in Figure \ref{fig:vfm-plg}.

% 若当前click对应多个点，我们遍历选择一个作为prompt，若当前的结果满足几何约束则保留作为伪标签，否则我们重新选择一个点作为prompt来生成伪标签，如图3蓝色虚线所示。

\subsection{Temporal and Spatial-Based Label Updating}
\label{sec:module_2}
To improve the quality of pseudo labels generated by the VFM-PLG module, we propose the temporal and spatial-based label updating module. This module leverages the generalization of neural networks by using high-reliability predictions from adjacent frames to update the pseudo labels of the current frame online.

To transform the point clouds from adjacent frames to the current frame, a coordinate system transformation is required, which can be expressed by the following equation:
\begin{equation}
\label{eq:transform}
    \mathbf{P}_t=\mathbf{T}_{t}^{-1}\times \mathbf{T}_{adj}\times \mathbf{P}_{adj}
\end{equation}
where $\mathbf{T}_{t}$ is the ego-car pose in the current frame, $\mathbf{T}_{adj}$ is the ego-car pose in the adjacent frame, $\mathbf{P}_t$ and $\mathbf{P}_{adj}$ correspond to the coordinates of the point cloud in the current frame and the adjacent frame, respectively.

Unlike MWSIS's~\cite{jiang2024mwsis} PVC module, our method does not require establishing a voting space from the previous training epochs, which reduces memory requirements during training. Moreover, since it is difficult to establish a one-to-one correspondence between point clouds of adjacent frames, we employ a voxel voting mechanism. An online voxel voting space $S$ is set up, where each voxel updates its corresponding label according to a predefined update strategy. The current frame requires voxelization of the point cloud, and the updated labels are obtained from the corresponding voxel space. The specific update strategy is as follows:
\begin{itemize}
    \item \textbf{Soft voting strategy.} Consider a voxel that contains $n$ points, where each point $p_i^v$ has an associated classification confidence scores $s^v_i \in \mathbb{R}^{num}$, where the $num$ denotes the class number. We average the classification confidence scores for all points within the voxel, and then identify the class $c^v_i$ with the highest score. If this score exceeds a set threshold $T_{s}$, the class is assigned as the voxel's label. The above process can be represented by Equation \ref{eq:softvoting}. Unlike the method of directly selecting the class with the most points, this approach enhances robustness against prediction noise.
    \begin{equation}
    \label{eq:softvoting}
        c_{i}^{v}=\begin{cases}
        	\text{arg}\max \bar{s}&		\text{if\,\,}\max \bar{s} >T_s\\
        	-1&		\text{otherwise}\\
        \end{cases}
    \end{equation}
    where $\bar{s}= \frac{1}{n}\sum_{i=1}^{n}{s_{i}^{v}}$, $-1$ denotes the ignored label.
    \item \textbf{Distance-based reliability update strategy.} To improve the reliability of voting labels, we consider that a greater number of voxel points and higher point confidence result in more reliable voting. Given that LiDAR point clouds are dense near the sensor and sparse farther away, we dynamically adjust the voting threshold: voxels closer to the sensor require more votes and higher confidence for label assignment.
\end{itemize}
By applying the aforementioned update strategy, a reliable voxel voting space $S$ is constructed, which is then used to update the labels of the current frame. For further details, refer to Algorithm \ref{alg:tsu} in the supplementary material.

\begin{table*}[ht]
  \centering
  % add the adjustbox
  \adjustbox{max width=1.0\textwidth}{
    \begin{tabular}{ccccccccccc} 
    \toprule
    \multirow{2}{*}{Supervision} & \multirow{2}{*}{Annotation} & \multirow{2}{*}{Model} & \multicolumn{4}{c}{3D Instance Segmentation (AP)}        & \multicolumn{4}{c}{3D Semantic Segmentation (IoU)}            \\ 
    \cline{4-11}
                                 &                             &                        & mAP   & Veh.  & Ped.  & Cyc.  & mIoU   & Veh.   & Ped.   & Cyc.    \\ 
    \hline
    \multirow{3}{*}{Full}        & 3D Mask                     & Cylinder3D~\cite{zhu2021cylindrical}             & 51.40 & 75.31 & 38.12 & 40.76 & 78.903 & 96.476 & 83.666 & 56.567  \\
                                 & 3D Mask                     & PTv3~\cite{wu2024ptv3}                   & 60.08 & 75.73 & 53.63 & 51.32 & 83.679 & 96.686 & 85.500 & 68.852  \\
                                 & 3D Mask                     & SparseUnet~\cite{shi2020PartA2}             & 59.26 & 80.25 & 56.95 & 40.59 & 79.505 & 96.675 & 81.906 & 59.933  \\ 
    \arrayrulecolor{black}\cline{1-1}\arrayrulecolor{black}\cline{2-11}
    \multirow{9}{*}{Weak}        & 3D Box                      & SparseUnet~\cite{shi2020PartA2}             & 49.32 & 69.00 & 45.96 & 33.01 & 72.545 & 89.471 & 73.581 & 54.582  \\
                                 & 2D Box                      & SparseUnet~\cite{shi2020PartA2}             & 35.48 & 44.54 & 36.84 & 25.08 & 63.831 & 74.102 & 72.113 & 45.278  \\
                                 & 2D Box$^*$                      & SparseUnet~\cite{shi2020PartA2}             & 45.12 & 64.06 & 40.06 & 31.23 & 75.571 & 93.418 & 77.982 & 55.312  \\
                                 & 2D Box                      & MWSIS~\cite{jiang2024mwsis}                  & 48.42 & 61.45 & 45.23 & 38.59 & 75.898 & 90.369 & 78.996 & 58.329  \\
                                 & Click$^*$                       & SparseUnet~\cite{shi2020PartA2}             & 40.19 & 59.15 & 36.98 & 24.45 & 67.510 & 76.225 & 76.118 & 50.187  \\ 
    \cline{2-11}
                                 & Click$^\dag$                       & Cylinder3D~\cite{zhu2021cylindrical}             & 45.12 & 60.72 & 36.52 & 38.11 & 70.068 & 76.653 & \underline{79.123} & 54.428  \\
                                 & Click$^\dag$                       & PTv3~\cite{wu2024ptv3}                   & 46.59 & 62.59 & 38.12 & \underline{39.06} & \underline{72.776} & \textbf{82.109} & 77.056 & \underline{59.163}  \\
                                 & Click$^\dag$                      & SparseUnet~\cite{shi2020PartA2}             & \underline{47.37} & \underline{64.10} & \underline{41.50} & 36.51 & 72.189 & 79.850 & 78.619 & 58.097  \\ 
    \cline{2-11}
                                 & Click$^\dag$\cellcolor{lightgray}  & \textbf{YoCo (Ours)}\cellcolor{lightgray} & \textbf{55.35}\cellcolor{lightgray} & \textbf{67.69}\cellcolor{lightgray} & \textbf{55.25}\cellcolor{lightgray} & \textbf{43.12}\cellcolor{lightgray} & \textbf{74.770}\cellcolor{lightgray} & \underline{81.136}\cellcolor{lightgray} & \textbf{81.716}\cellcolor{lightgray} & \textbf{64.459}\cellcolor{lightgray}  \\
    \bottomrule
    \end{tabular}
  }
  \caption{\textbf{Performance Comparisons of 3D Instance and Semantic Segmentation on Waymo Validation Dataset.} \textbf{Bold} indicates optimal performance, and \underline{underline} indicates sub-optimal performance. $^*$ represents the pseudo label generated by SAM using the corresponding annotation as the prompt. $^\dag$ denotes the pseudo label generated by our VFM-PLG module. Abbreviations: vehicle (Veh.), pedestrian (Ped.), cyclist (Cyc.). }
  \vspace{-1mm}
  \label{tab:main results}
\end{table*}

\subsection{IoU-Guided Label Enhancement}
\label{sec:module_3}
To further leverage the robustness of neural networks and correct the erroneous pseudo labels generated by the VFM-PLG, we introduce an IoU-guided label enhancement module. This module performs an offline update of pseudo labels using high-confidence scores and high-IoU value predictions. Additionally, we adjust the confidence score threshold accordingly to accommodate the characteristic density variation in LiDAR point clouds (dense near, sparse far). The process can be represented as follows:
\begin{equation}
    m'_i=\begin{cases}
        \text{arg}\max \left( IoU_i \right)&		\,\,\text{if\,\,}s_i\ge T_{s_2}\,\,\text{and\,\,}IoU_i\ge T_{IoU}\\
        \tilde{m}_i&		\,\,\text{otherwise\,\,}\\
    \end{cases}
\end{equation}
where $IoU_i=\frac{\tilde{m}_i\cap \hat{m}_i}{\tilde{m}_i\cup \hat{m}_i}$, $m'_i$ and $\hat{m}_i$ represent the updated labels and the predicted labels, respectively, while $T_{s_2}$ and $T_{IoU}$ correspond to the predefined confidence scores threshold and IoU threshold.

\subsection{Loss}
\label{sec:loss}
The overall loss function of the YoCo is defined as:
\begin{equation}
    L = \alpha_1 L_{cls} + \alpha_2 L_{vote}
\end{equation}
where $L_{cls}$ denotes the cross entropy loss or focal loss, and $L_{vote}$ represents the L1 loss, $\alpha_1$ and $\alpha_2$ are hyperparameters to balance loss terms.


