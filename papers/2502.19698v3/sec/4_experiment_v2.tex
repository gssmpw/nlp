

\section{Experiments}
\label{sec:experiment}
\subsection{Waymo Open Dataset}
% 我们遵循MWSIS论文的设置，在Waymo1.4.0数据集上进行实验
Following the state-of-the-art MWSIS~\cite{jiang2024mwsis}, we conduct our experiments on version 1.4.0 of the Waymo Open Dataset (WOD)~\cite{sun2020waymo}, which includes both well-synchronized and aligned LiDAR points and images. The WOD consists of 1,150 sequences (over 200K frames), with 798 sequences for training, 202 sequences for validation, and 150 sequences for testing. For the 3D segmentation task, the dataset contains 23,691 and 5,976 frames for training and validation, respectively. We specifically focus on the vehicle, pedestrian, and cyclist categories for evaluation.

\subsection{Implementation Details}
\noindent\textbf{Click Setting.} For the click annotation, we use the average coordinates of each instance from the BEV plane as a reference and then select the nearest point to simulate the manual click. Meanwhile, in Table \ref{tab:click}, we also simulate the results with the manual annotation error.

\noindent\textbf{Evaluation Metric.} We adopt the same evaluation metrics as~\cite{jiang2024mwsis}. For 3D instance segmentation, we use average precision (AP) across different IoU thresholds to assess performance, while for 3D semantic segmentation, we use mean intersection-over-union (mIoU) as the evaluation metric.

\noindent\textbf{Training Setting.} 
% 我们选择了几种经典的backbone，如 Cylinder3D，SparseUnet, Point Transformer V3 (PTV3), and then employ two separate heads to predict semantic masks and group pixels to instances. We train Cylinder3D for 40 epochs, SparseUnet for 24 epochs, and PTV3 for 40 epochs. 所有模型都在4张3090上训练，with a batch size of 8，且采用AdamW优化器。
We choose several classic backbones, including Cylinder3D~\cite{zhu2021cylindrical}, SparseUnet~\cite{shi2020PartA2}, and Point Transformer V3 (PTv3)~\cite{wu2024ptv3}, and use two separate heads: one for predicting semantic masks and another for grouping pixels into instances. Cylinder3D, PTv3, and SparseUnet are trained for 40, 50, and 24 epochs, respectively. All models are trained on 4 NVIDIA 3090 GPUs with a batch size of 8, using the AdamW~\cite{loshchilov2017adamw} optimizer.

\subsection{Results on WOD}
% 在表格1中我们使用不同的模型进行实验，同时考虑时间和内存效率，我们选择SparseUnet作为baseline开展实验，SparseUnet对比PTv3方法拥有更快的训练速度和更低的显存需求，且有相似的性能。我们将YoCo与其他全监督和弱监督方法进行对比，在3D实例分割任务上，我们的方法对比基于其他弱监督的方法取得最佳性能，对比基于Click+SAM的方法，我们的方法提高了15.16% mAP，同时，对比标注成本更大的3D框，我们的方法提升了6.03% mAP。除此之外，我们的方法对比基于Cylinder3D的全监督的方法获得了3.95% mAP的提升。除此之外，我们还提供了3D语义分割的指标。对比基于Click+SAM的方法，我们的方法获得7.260% mIoU提升，与基于3D框的方法对比，我们实现了2.225% mIoU提升，并达到全监督性能的94.76%。
% \noindent\textbf{Baseline.} In Table \ref{tab:main results}, we present experiments conducted with various models. Considering both computational time and memory efficiency, we select SparseUnet~\cite{shi2020PartA2} as the baseline for our experiments. SparseUnet, in comparison to the state-of-the-art PTv3~\cite{wu2024ptv3} method, offers several advantages: it delivers faster training times and requires significantly less memory while achieving a similar level of performance. This makes SparseUnet a more practical choice for the network without compromising on performance. 

We compare the YoCo with other weakly supervised and fully supervised methods for 3D instance segmentation. Considering both computational time and memory efficiency, we select SparseUnet~\cite{shi2020PartA2} as the baseline for our experiments. Additionally, Table \ref{tab:yoco4other} presents the results of YoCo with different networks, further demonstrating the generality of our method. In Table \ref{tab:main results}, our YoCo achieves the best performance among weakly supervised methods. It outperforms the Click$^*$-based approach with a 15.16\% improvement in mAP and surpasses the state-of-the-art method MWSIS by 6.93\% mAP, while utilizing more cost-effective sparse click annotations. Additionally, compared with methods using 3D bounding boxes, which have higher annotation costs, our approach achieves a 6.03\% mAP improvement. Moreover, our method outperforms fully supervised Cylinder3D by 3.95\% mAP.

We also provide metrics for 3D semantic segmentation. Compared to the Click$^*$-based method, our approach achieves a 7.260\% improvement in mIoU. When compared to methods based on 3D bounding boxes, it achieves a 2.225\% mIoU increase. Additionally, our method reaches 94.76\% of the fully supervised performance.

% In Figure 5, we provide a visual comparison, and our YoCo achieves better results in some cases.

\begin{table}[ht]
    \centering
    \adjustbox{max width=0.75\linewidth}{
    \begin{tabular}{ccccc} 
    \toprule
    \multicolumn{3}{c}{Method}                 & \multirow{2}{*}{mAP}      & \multirow{2}{*}{mIoU}       \\ 
    \cline{1-3}
    VFM-PLG          & TSU          & ILE          &                           &                             \\ 
    \hline
    $\mbox{-}$   & $\mbox{-}$   & $\mbox{-}$   & \multicolumn{1}{l}{25.36} & \multicolumn{1}{l}{51.919}  \\
    \hline
    $\checkmark$ & $\mbox{-}$   & $\mbox{-}$   & 47.37                     & 72.189                      \\
    $\checkmark$ & $\checkmark$ & $\mbox{-}$   & 52.18                     & 74.449                      \\
    $\checkmark$\cellcolor{lightgray} & $\checkmark$\cellcolor{lightgray} & $\checkmark$\cellcolor{lightgray} & \textbf{55.35}\cellcolor{lightgray}            & \textbf{74.770}\cellcolor{lightgray}             \\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Ablation Study of All Modules.}}
    \label{tab:ile}
    \vspace{-4mm}
\end{table}

\subsection{Ablation Study and Analysis}
\label{sec:ablation}

\noindent\textbf{Effect of all modules.}
% 表格5提供我们提出的所有模块的消融实验，当仅使用VFM-PLG模块时，性能提升明显，实例分割和语义分割性能分别提升22.01%mAP和20.27%mIoU，这证明结合VFM和点云的几何约束进行伪标签生成是一种有效的方法。当引入基于时间和空间的标签更新策略后，性能进一步提升4.81%mAP和2.26%mIoU，这表明利用神经网络的泛化能力将相邻帧的预测信息作为更新标签可以进一步的提高标签质量。除此之外，我们提出的ILE策略进一步的利用该泛化能力，对VFM-PLG生成的伪标签进行离线增强，提升标签质量，实例分割和语义分割性能分别提高了3.17%mAP，0.321%mIoU.
Table \ref{tab:ile} presents the ablation study of all proposed modules. When solely the VFM-PLG module is utilized, we observe a substantial performance improvement, with instance segmentation and semantic segmentation improving by 22.01\% mAP and 20.270\% mIoU, respectively. This demonstrates the effectiveness of generating pseudo labels by combining the VFM module with geometric constraints from point clouds. When the TSU module is introduced, the performance further improves by 4.81\% mAP and 2.26\% mIoU. This highlights the value of leveraging the neural network’s generalization by using high-confidence predictions from adjacent frames to update the current frame’s labels,  improving the quality of the pseudo labels. Moreover, our proposed ILE module leverages the robustness of the network to perform offline refinement of the pseudo labels generated by the VFM-PLG. This approach leads to additional gains in label quality, with instance segmentation improving by 3.17\% mAP and semantic segmentation by 0.321\% mIoU. These results demonstrate the effectiveness of our framework in progressively refining pseudo labels and narrowing the performance gap between weakly supervised and fully supervised methods.



\noindent\textbf{Effect of the VFM-PLG.}
% 图2展示了我们方法可以生成高质量的伪标签，与与仅使用点击提示的方法相比，我们的VFM-PLG模块实现了近10％的性能提升。
% 图片2展示了我们方法在生成伪标签上的优越性能，在表2中，我们提供了VFM-PLG模块的消融实验。表格2的第二行表示baseline性能，仅仅使用Click和SAM得到的伪标签进行训练。仅仅使用聚类算法处理，mAP提升了14.83%。如果进一步的使用DAM对组合类别，如Cylist进行处理，使其性能提升5.82%AP。当结合几何约束(GC)时，我们将性能mAP提高到47.37%，超过了使用2D Box作为prompt的方法。通过结合上述的方法，我们大大降低了标注成本。
% Figure \ref{fig:miou} demonstrates the effectiveness of our method in generating high-quality pseudo labels, and our VFM-PLG module achieves nearly a 10\% performance improvement compared to the method that uses only clicks as prompts. 
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=1.0\linewidth]{img/miou.pdf}
%     \vspace{-6mm}
%     \caption{\textbf{Comparisons of IoU on WOD Validation Dataset.} We combine click-level and 2D bounding box-level annotations with various methods to generate pseudo labels for the validation dataset and evaluate them with ground truth.}
%     \label{fig:miou}
%     \vspace{-1mm}
% \end{figure}
In Table \ref{tab:plg}, we conduct an ablation study to evaluate the impact of each component in the VFM-PLG module. The second row is the baseline performance, where the model is trained using labels generated from click annotations and the SAM. When a clustering algorithm is adopted to refine pseudo labels, the mAP improves substantially by 14.83\%. Additionally, applying the DAM to handle composite categories, such as cyclists, can further improve performance by 5.82\% AP. When geometric constraints are incorporated, the mAP reaches 47.37\%, surpassing methods that utilize 2D boxes as prompts. Combining these methods not only improves segmentation performance but also significantly reduces annotation costs, demonstrating the efficiency and practicality of our approach in weakly supervised 3D instance segmentation.
\begin{table}[ht]
    \centering
    \adjustbox{max width=0.7\linewidth}{
    \begin{tabular}{cccc}
    \toprule
    \multirow{2}{*}{Annotation~} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{AP}           \\ 
    \cline{3-4}
                                 &                         & mAP            & Cyc.            \\ 
    \hline
    2D Box                       & SAM                     & 45.12          & 31.23           \\ 
    \hline
    \multirow{4}{*}{Click}       & SAM                     & 25.36          & 16.42           \\
                                 & + Cluster               & 40.19          & 24.45           \\
                                 & + DAM                   & 42.13          & 30.27           \\
                                 & + GC\cellcolor{lightgray}                    & \textbf{47.37}\cellcolor{lightgray} & \textbf{36.51}\cellcolor{lightgray}  \\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Ablation Study of VFM-PLG Module.}}
    \label{tab:plg}
    \vspace{-2mm}
\end{table}

\begin{table}[ht]
    \centering
    \adjustbox{max width=0.45\linewidth}{
    \begin{tabular}{ccc} 
    \toprule
    Frames & mAP            & mIoU             \\ 
    \hline
    0      & 47.37          & 72.189           \\ 
    \hline
    1      & 47.52          & 68.346           \\
    2\cellcolor{lightgray}      & \textbf{52.18}\cellcolor{lightgray} & \textbf{74.449}\cellcolor{lightgray}  \\
    3      & 51.07          & 74.107           \\
    5      & 48.17          & 72.345           \\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Ablation Study of the Number of Adjacent Frames.}}
    \label{tab:tsu1}
    \vspace{-1mm}
\end{table}

\begin{table}[ht]
    \centering
    \adjustbox{max width=0.9\linewidth}{
    \begin{tabular}{cccccc} 
    \toprule
    \multirow{2}{*}{Frames} & \multirow{2}{*}{Vote Mode} & \multicolumn{2}{c}{Dynamic} & \multirow{2}{*}{mAP} & \multirow{2}{*}{mIoU}  \\ 
    \cline{3-4}
                            &                            & Count        & Score        &                      &                        \\ 
    \hline
    0                       & $\mbox{-}$                 & $\mbox{-}$   & $\mbox{-}$   & 47.37                & 72.189                 \\ 
    \hline
    \multirow{5}{*}{2}      & \multirow{2}{*}{Hard}      & $\mbox{-}$   & $\mbox{-}$   & 48.54                & 73.138                 \\
                            &                            & $\checkmark$ & $\mbox{-}$   & 48.68                & 73.215                 \\ 
    \cline{2-6}
                            & \multirow{3}{*}{Soft}      & $\mbox{-}$   & $\mbox{-}$   & 49.97                & 73.152                 \\
                            &                            & $\checkmark$ & $\mbox{-}$   & 50.83                & 73.564                 \\
                            &                            & $\checkmark$\cellcolor{lightgray} & $\checkmark$\cellcolor{lightgray} & \textbf{52.18}\cellcolor{lightgray}       & \textbf{74.449}\cellcolor{lightgray}        \\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Ablation Study of Voting Strategy in the TSU Module.} \textit{Hard} voting means that the predicted label of the majority class among the points within a voxel is selected as the updated label. In contrast, \textit{Soft} voting assigns the updated label based on the class with the highest average confidence score across all points within the voxel. \textit{Count} and \textit{Score} represent the use of dynamic thresholds, \textit{Count} dynamically adjusts the required number of votes, while \textit{Score} dynamically adjusts the confidence score threshold for label updates.}
    % These strategies aim to optimize label reliability based on the varying density of LiDAR point clouds.
    \label{tab:tsu2}
    \vspace{-2mm}
\end{table}

\noindent\textbf{Effect of the TSU.}
% 表格3和表格4提供了我们TSU模块的消融实验。在表格3中，我们可以看到在一定帧数范围内，随着帧数的增多，性能有提升，当使用2帧生成的结果进行更新时，达到最大性能52.18%mAP和74.449%mIoU。当帧数超过3帧时，动态目标移动过大，进步使得投票出现错误，导致性能下降。在表格4中，我们提供了多种不同投票策略的性能对比。当使用Hard投票方式时，第三行对比baseline（第一行），实例分割和语义分割性能分别提升1.17%mAP和0.949%mIoU，性能提升有限。当仅仅使用Soft投票方式时，性能提升2.6%mAP和0.363%mIoU。当配合我们提出的Distance-based reliability update strategy，性能提升明显，实例分割和语义分割性能分别提升4.81%mAP和2.26%mIoU。证明我们基于LiDAR点云近密远疏特性提出的更新策略的有效性。
Tables \ref{tab:tsu1} and \ref{tab:tsu2} provide the ablation studies for the TSU module. In Table \ref{tab:tsu1}, we analyze the impact of using different numbers of frames for label updates. We observe a steady improvement in performance as the number of frames increases up to a certain point. Specifically, when updating labels using predictions from two adjacent frames, the method achieves its best performance, with 52.18\% mAP and 74.449\% mIoU. However, as the number of frames exceeds three, the increased motion of dynamic objects causes greater discrepancies between frames, leading to erroneous votes and a consequent drop in overall performance.

Table \ref{tab:tsu2} compares several voting strategies to assess their impact on segmentation results. When employing the hard voting method (row 3), compared to the baseline method (row 1), we observe only modest improvements, with gains of 1.17\% mAP and 0.949\% mIoU. This limited improvement suggests that the hard voting is not sufficiently robust to handle noise in the predictions. In contrast, when applying the soft voting method (row 4), the performance improvement is more pronounced, yielding 2.6\% mAP and 0.963\% mIoU gains.

Furthermore, when we integrate our distance-based reliability update strategy (row 6), which adapts to the varying density of LiDAR point clouds based on their proximity to the sensor, we observe a significant increase in performance. Specifically, instance segmentation improves by 4.81\% mAP, while semantic segmentation sees a gain of 2.260\% mIoU. These results highlight the robustness of our proposed strategy in leveraging the inherent properties of LiDAR point clouds, particularly the denser distribution of points near the sensor and the sparser distribution at greater distances, to enhance the reliability of pseudo labels during training. This approach not only improves segmentation accuracy but also addresses challenges posed by noisy or uncertain predictions.

% Hard表示在投票时，将落在该体素中类别数量最多的点的预测标签作为更新标签，Soft表示将落在该体素中置信度平均得分最大类别的预测标签作为更新标签。Count和Score分别表示使用动态的投票数阈值和动态的置信度阈值。

\noindent\textbf{Effect of the manual annotation error.}
% Manual annotation 
% 为了模拟在手工标注过程中的误差，我们设置了不同click标注范围的扰动实验，如表格6所示。
% 表格第一列的值表示在bev视角下以实例中心为圆心，click range为半径的园内随机选择点作为click标注，其中0.0表示距离实例中心最近的点。
To simulate potential errors that may arise during manual annotation, we conduct experiments with varying click annotation ranges, as detailed in Table \ref{tab:click}. These results demonstrate that our method exhibits strong stability across different annotation ranges. Notably, even as the click range expands to 0.5 meters, there is no significant drop in performance, with both mAP and mIoU remaining well within acceptable limits. This robustness indicates that our approach is resilient to variations in the click annotation radius, consistently maintaining high performance regardless of the distance from the instance center. Such stability is crucial in real-world applications, where manual annotations can introduce variability, yet reliable segmentation results are still required.
\begin{table}[t]
    \centering
    \adjustbox{max width=0.6\linewidth}{
    \begin{tabular}{ccc}
    \toprule
    Error Range (m) & mAP            & mIoU             \\ 
    \hline
    0.0         & \textbf{55.35} & 74.770           \\
    \hline
    0.1         & 54.86          & 75.688           \\
    0.2         & 54.98          & 75.591           \\
    0.3         & 54.39          & \textbf{75.936}  \\
    0.5         & 54.76          & 74.444           \\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Ablation Study of the Manual Annotation Error.} The first column of the table presents the click error range during manual annotation. Single click annotations are randomly selected from a circular region centered at the instance's center in the BEV plane, with the \textit{Error Range} defining the radius. A value of 0.0 indicates that the point closest to the instance center is selected.}
    \label{tab:click}
    \vspace{-3mm}
\end{table}

\noindent\textbf{Fine-tuning with the YoCo.}
% Following the MWSIS, 我们将在我们YoCo框架下训练好的网络在不同比例的全监督数据上进行微调。当仅使用0.8%的数据进行微调，已经取得与全监督相同的性能（59.27% vs 59.26%）。当使用5%的数据进行微调时，已经超过了基于PTv3网络的全监督性能。这表明，在我们的框架下仅使用极少的数据即可达到与全监督相当的性能，大大降低了标注的成本，对比更加复杂的point-based网络，我们的方法还更具效率。
Following ~\cite{jiang2024mwsis}, we fine-tune the trained network within our YoCo framework across different proportions of fully supervised data. As shown in Figure \ref{fig:finetune}, fine-tuning with a mere 0.8\% of the fully supervised data achieves performance comparable to that obtained with full supervision (59.27\% vs. 59.26\%). Moreover, when increasing the fully supervised data utilization to 5\%, our approach surpasses the fully supervised performance of the state-of-the-art PTv3.

% The above experiments show that under our YoCo framework, this greatly reduces annotation costs and provides higher training efficiency compared to more complex point-based networks.

The above experiments demonstrate that our YoCo framework achieves performance comparable to fully supervised methods using only a minimal amount of annotations, significantly reducing annotation costs.
% In comparison to more complex point-based networks, our method is more efficient.
% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{img/finetune.pdf}
%     \caption{\textbf{3D Instance Segmentation Performance Comparison.} The weakly supervised training backbone for fine-tuning compared to the fully supervised performance.}
%     \label{fig:finetune}
% \end{figure}

\begin{table}[ht]
    \centering
    \adjustbox{max width=1.0\linewidth}{
    \begin{tabular}{ccccc} 
    \toprule
    Supervision                & Annotation    & Model         & YoCo         & mAP    \\ 
    \hline
    \multirow{3}{*}{Full}      & 3D Mask       & Cylinder3D~\cite{zhu2021cylindrical}   & $\mbox{-}$   & 51.40  \\
                               & 3D Mask       & SparseUnet~\cite{shi2020PartA2}   & $\mbox{-}$   & 59.26  \\
                               & 3D Mask       & PTv3~\cite{wu2024ptv3}         & $\mbox{-}$   & 60.08  \\ 
    \hline
    \multirow{6}{*}{Weak}      & Click$^\dag$  & Cylinder3D~\cite{zhu2021cylindrical}   & $\mbox{-}$   & 45.12  \\
                               & Click$^\dag$  & SparseUnet~\cite{shi2020PartA2}   & $\mbox{-}$   & 47.37  \\
                               & Click$^\dag$  & PTv3~\cite{wu2024ptv3}          & $\mbox{-}$   & 46.59  \\ 
    \cline{2-5}
                               & Click$^\dag$  & Cylinder3D~\cite{zhu2021cylindrical}   & $\checkmark$ & 49.71  \\
                               & Click$^\dag$  & SparseUnet~\cite{shi2020PartA2}   & $\checkmark$ & 55.35  \\
                               & Click$^\dag$  & PTv3~\cite{wu2024ptv3}          & $\checkmark$ & 53.83  \\ 
    \hline
    \multirow{3}{*}{Fine-Tune} & 0.8\% 3D Mask & Cylinder3D~\cite{zhu2021cylindrical}& $\checkmark$ & 53.13  \\
                               & 0.8\% 3D Mask & SparseUnet~\cite{shi2020PartA2}& $\checkmark$ & 59.27  \\
                               & 5.0\% 3D Mask   & PTv3~\cite{wu2024ptv3}        & $\checkmark$ & 60.44  \\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Generality Experiment of the YoCo.}}
    \label{tab:yoco4other}
    \vspace{-3mm}
\end{table}

\noindent\textbf{The generality of the YoCo.}
% 在表格7中，我们展示了在YoCo框架下使用不同网络（包括Cylinder3D、SparseUnet和PTv3）进行训练的实验结果，以进一步验证我们方法的通用性（generality）。从表格可以看出，在弱监督情况下，YoCo生成的伪标签在所有三种网络中均表现出优异的性能。对于3D实例分割任务，我们的方法使弱监督的Cylinder3D SparseUnet和PTv3性能分别提升4.59% mAP，7.24%mAP和7.98%mAP。3D语义分割任务，mIoU性能分别提升2.163%，2.581%和2.146%。
% 除此之外，我们还将在YoCo框架下训练的不同网络使用部分标注数据进行微调，当使用0.8%的数据进行微调，Cylinder3D和SparseUnet均超过了其全监督的性能，当使用5%的标注数据时，PTv3网络超过其全监督的性能。
In Table \ref{tab:yoco4other}, we present experimental results on training various networks, including Cylinder3D, SparseUnet, and PTv3, within the YoCo framework to validate the generality of our approach. As shown, the YoCo exhibits strong performance across all three networks under weak supervision. Specifically, for the 3D instance segmentation task, our method improves mAP by 4.59\% for Cylinder3D, 7.24\% for SparseUnet, and 7.98\% for PTv3. In addition, we fine-tune different networks trained under the YoCo framework using a small portion of fully supervised data. The results indicate that with only 0.8\% of fully supervised data for fine-tuning, both Cylinder3D and SparseUnet surpass their fully supervised performance. When 5\% of labeled data is used, the PTv3 network also exceeds its fully supervised performance.

These results reveal that YoCo is not only effective for a single network architecture but also adaptable to multiple architectures, demonstrating its generality.