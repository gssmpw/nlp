\begin{figure*}[ht]
\centering
   \includegraphics[width=0.95\linewidth]{img/framework-3.pdf}
   \vspace{-2mm}
   \caption{\textbf{Overview of YoCo Framework.} The YoCo consists of two main components: (a) pseudo label generation and (b) network training. For pseudo label generation, the VFM-PLG module produces high-quality pseudo labels using both VFMs and geometric constraints. For network training, our YoCo adopts the classic Mean Teacher~\cite{tarvainen2017meanteacher} structure. The TSU module performs online updates to the pseudo labels by using predictions from adjacent frames in a voxelized manner. Additionally, the ILE module enhances offline pseudo labels by leveraging high-confidence and high-IoU predictions to improve their quality.}
   \vspace{-2mm}
   % YoCo框架主要包含两部分，第一部分是伪标签生成，使用VFM-PLG模块生成高质量的伪标签，第二部分是网络训练，使用经典的MeanTeacher结构。其中TSU模块在线的利用相邻帧的预测信息通过体素化的方式对伪标签进行更新；ILE模块使用高置信度和高IoU的预测结果对伪标签进行离线更新。
\label{fig:framework}
\end{figure*}
\section{Related Work}
\label{sec:related_work}
\noindent\textbf{LiDAR-Based Fully Supervised 3D Segmentation.}
% 参考Center Focusing Network for Real-Time LiDAR Panoptic Segmentation
Existing 3D LiDAR point cloud segmentation methods can be divided into three types according to data representation: point-based, projection-based, and voxel-based.
% Based on these three data representations, different backbone has emerged. 

Point-based methods~\cite{qi2017pointnet, qi2017pointnet++, wu2019pointconv, thomas2019kpconv, zhao2021ptv1, wu2022ptv2, wu2024ptv3} directly use raw point clouds as the input. The classic PointNet~\cite{qi2017pointnet} utilizes the permutation invariance of both pointwise MLPs and pooling layers to aggregate features across a set. KPConv~\cite{thomas2019kpconv} and PointConv~\cite{wu2019pointconv} construct continuous convolution to directly process 3D points. The Point Transformer series~\cite{zhao2021ptv1, wu2022ptv2, wu2024ptv3} adopt the transformer architecture to extract features from 3D points.
Projection-based methods~\cite{ando2023rangevit, kong2023rangeformer, milioto2019rangenet++} project 3D points onto 2D images to form regular representations, allowing the use of well-established neural networks from 2D image processing. RangeViT~\cite{ando2023rangevit} directly applies a pre-trained ViT model as the encoder and fine-tunes it, demonstrating the feasibility of transferring 2D knowledge to 3D tasks. Rangeformer~\cite{kong2023rangeformer} and RangeNet++~\cite{milioto2019rangenet++} use an encoder-decoder hourglass-shaped architecture as the backbone for feature extraction.
Other methods~\cite {cheng20212-s3net, choy2019Minkowski, zhu2021cylindrical, graham20183d} convert point clouds into regular 3D voxelization. SSCN~\cite{graham20183d} introduces sparse convolutional networks to handle voxelized sparse point clouds. Cylinder3D~\cite{zhu2021cylindrical} introduces 3D cylindrical partitioning and asymmetrical 3D convolutions to handle the sparsity and varying density of outdoor point clouds.

While point-based methods deliver high performance, they come with significant computational costs due to the large-scale raw LiDAR data. On the other hand, projection-based methods are more efficient but lose valuable internal geometric information, resulting in suboptimal performance. Taking both time and memory efficiency into account, we adopt voxelized representations and select a sparse convolutional U-Net~\cite{shi2020PartA2} as our backbone.

\noindent\textbf{Weakly Supervised 3D Instance Segmentation.}
Point cloud segmentation has made significant progress in fully supervised settings. However, dense point-wise annotation is costly. To reduce the annotation burden, some work~\cite{wei2020mprm, hu2022sqn, zhang2022not, chen2024foundation, unal2022scribble, wang2024label,li2024box2mask, ngo2023gapro, yu20243when3dbox, jiang2024mwsis, guo2024sam} has explored using various weak supervision signals.

% MPRM~\cite{wei2020mprm} uses sub-cloud labels as pseudo labels and employs class activation mapping to generate these pseudo labels. SQN~\cite{hu2022sqn} leverages geometric prior information and achieves results comparable to fully supervised methods using only 0.1\% sparse annotations. Chen \etal~\cite{chen2024foundation} utilize scatter image annotation, combined with a pre-trained optical flow estimation network and a basic image segmentation model, to propagate manual annotations into dense labels for both images and point clouds, generating segmentation labels. ScribbleKITTI~\cite{unal2022scribblekitti} re-annotates the KITTI dataset~\cite{geiger2013kitti} and provides a scribble-supervised benchmark for LiDAR segmentation on SemanticKITTI~\cite{behley2019semantickitti}.

For 3D instance segmentation tasks, 3D bounding boxes provide coarse information about instance objects, making instance segmentation feasible. Box2Mask~\cite{li2024box2mask} is the first work to use 3D bounding boxes as weak supervision labels. GaPro~\cite{ngo2023gapro} proposes a gaussian process method to address pseudo label ambiguity in overlapping regions of multiple 3D bounding boxes. CIP-WPIS~\cite{yu20243when3dbox} leverages 2D instance knowledge and 3D geometric constraints to handle the 3D bounding box perturbation issues. Additionally, MWSIS~\cite{jiang2024mwsis} is the first work to use 2D bounding boxes as weak supervision signals for outdoor point cloud segmentation. It introduces various fine-grained pseudo label generation and refinement methods, and explores the possibility of integration with SAM~\cite{kirillov2023segment}. However, both 2D and 3D bounding boxes still involve considerable annotation costs. Our method only requires a click on the object in the BEV plane to generate high-quality pseudo labels.

\noindent\textbf{Click-Level Annotation for 3D Perception Tasks.}
% 参考OC3D论文
Click-level annotation is a highly efficient and labor-saving labeling method. The recent work ~\cite{liu2021onethingoneclick, tao2022seggroup, liu2023clickseg, meng202ws3d, meng2021ws3d, zhang2023vitwss3d, xia2024oc3d} has begun to incorporate it into various 3D perception tasks.

One Thing One Click~\cite{liu2021onethingoneclick} employs click-level labels and introduces a graph propagation module to iteratively generate semantic pseudo labels. SegGroup~\cite{tao2022seggroup} propagates click-level labels to unlabeled segments through iterative grouping, generating instance pseudo labels. Meanwhile, ClickSeg~\cite{liu2023clickseg} presents a method that uses k-means clustering with fixed initial seeds to generate instance pseudo labels online. In the field of 3D object detection, WS3Ds~\cite{meng202ws3d, meng2021ws3d} annotate object centers in the BEV plane. It utilizes these center clicks as supervision signals to generate cylindrical proposals, and then employs a small amount of ground truth to train the network to produce 3D bounding boxes. ViT-WSS3D~\cite{zhang2023vitwss3d} proposes using a vision transformer to construct a point-to-box converter. 
% OC3D~\cite{xia2024oc3d} introduces a two-stage training strategy, where the first stage generates box-level and mask-level pseudo labels for static and dynamic objects, and the second stage uses the network trained in the first stage to produce high-quality box-level pseudo labels for dynamic objects.

The aforementioned studies, especially those focusing on weakly supervised 3D instance segmentation, primarily address indoor point clouds. In contrast, 3D instance segmentation of outdoor LiDAR point clouds remains largely unexplored. Although MWSIS~\cite{jiang2024mwsis} utilizes 2D bounding boxes, which incur lower annotation costs, it still exhibits a significant performance gap compared to fully supervised methods. To further minimize annotation costs, we propose a weakly supervised instance segmentation framework that relies solely on click-level annotations, effectively narrowing the gap with fully supervised approaches.
