\begin{abstract}
Outdoor LiDAR point cloud 3D instance segmentation is a crucial task in autonomous driving. However, it requires laborious human efforts to annotate the point cloud for training a segmentation model. To address this challenge, we propose a \textbf{YoCo} framework, which generates 3D pseudo labels using minimal coarse click annotations in the bird's eye view plane. It is a significant challenge to produce high-quality pseudo labels from sparse annotations. Our YoCo framework first leverages vision foundation models combined with geometric constraints from point clouds to enhance pseudo label generation. Second, a temporal and spatial-based label updating module is designed to generate reliable updated labels. It leverages predictions from adjacent frames and utilizes the inherent density variation of point clouds (dense near, sparse far). Finally, to further improve label quality, an IoU-guided enhancement module is proposed, replacing pseudo labels with high-confidence and high-IoU predictions. Experiments on the Waymo dataset demonstrate YoCo’s effectiveness and generality, achieving state-of-the-art performance among weakly supervised methods and surpassing fully supervised Cylinder3D. Additionally, the YoCo is suitable for various networks, achieving performance comparable to fully supervised methods with minimal fine-tuning using only 0.8\% of the fully labeled data, significantly reducing annotation costs.
\end{abstract}
% 在waymo数据集上大量的实验证明了我们方法的有效性和通用性。仅使用click-level标注，对比其他弱监督方法，我们获得了最优的性能，且超过了基于Cylinder3D的全监督的方法。除此之外，我们的方法可以适用多种网络结构，且在少量标注数据微调下（0.8%）可以获得与全监督相当的性能，降低了标注的成本。
% 

% 第三版
% 11/2
% Outdoor LiDAR point cloud 3D instance segmentation is a crucial task in autonomous driving and robotics. However, it requires laborious human efforts to annotate the point cloud for training a segmentation model. To address this challenge, we propose a \textbf{YoCo} framework, which generates 3D pseudo labels using minimal coarse click annotations in the bird's eye view plane. It is a significant challenge to produce high-quality pseudo labels from sparse annotations. Our YoCo framework first leverages vision foundation models (VFMs) in combination with geometric constraints from point clouds to enhance pseudo label generation. Second, a temporal and spatial-based label updating (TSU) module is designed to generate reliable updated labels. It leverages predictions from adjacent frames and utilizes the inherent density variation of point clouds, where points are denser nearby and sparser with distance. Finally, to further improve the pseudo label quality, we propose an IoU-guided label enhancement (ILE) module, which uses the robustness of neural networks to replace pseudo labels with high-confidence and high-IoU predictions. Extensive experiments on the Waymo dataset demonstrate the effectiveness and generality of our method. By utilizing only click-level annotations, we achieve optimal performance compared to other weakly supervised methods and surpass fully supervised methods based on Cylinder3D. Additionally, our method is suitable for various network architectures, achieving performance comparable to fully supervised methods with minimal fine-tuning using only 0.8\% of the fully labeled data, significantly reducing annotation costs.

% 第一版
% 10/23
% Outdoor LiDAR point cloud 3D instance segmentation is a crucial task in autonomous driving and robotics. However, annotating the point cloud for training a segmentation model requires laborious human effort. To address this challenge, we propose the \textbf{YoCo} framework, which generates 3D pseudo labels using minimal coarse click annotations in the bird's eye view plane. Producing high-quality pseudo labels from sparse annotations is a significant challenge, which our YoCo framework addresses with two key innovations. First, we leverage the powerful capabilities of vision foundation models (VFMs) combined with the geometric constraints of point clouds to enhance the pseudo label generation process. Second, we leverage the generalization capability of neural networks by implementing both online and offline updates, continuously improving the quality of pseudo labels as the network's performance progressively enhances. Extensive experiments on the widely-used Waymo dataset demonstrate that our YoCo framework, with only click-level annotations, achieves state-of-the-art performance compared to other weakly supervised methods. Remarkably, our method surpasses fully supervised approaches based on Cylinder3D, achieving competitive results using only 0.8\% of the fully supervised labels, significantly reducing annotation costs.

% 第二版
% Outdoor LiDAR point cloud 3D instance segmentation is a crucial task in autonomous driving and robotics. However, annotating the point cloud for training a segmentation model requires laborious human effort. To address this challenge, we propose the \textbf{YoCo} framework, which generates 3D pseudo labels using minimal coarse click annotations in the bird's eye view plane. Producing high-quality pseudo labels from sparse annotations is a significant challenge. Our YoCo framework first leverages the powerful capabilities of vision foundation models, combined with the geometric constraints of point clouds, to enhance the pseudo label generation process. Second, the Temporal-Spatial Update (TSU) module is introduced, exploiting the generalization ability of neural networks along with temporal and spatial information from the point clouds to obtain reliable updated labels. Finally, to further improve pseudo label quality, we propose the IoU-guided Label Enhancement (ILE) module, which uses the robustness of neural networks to replace labels with high-confidence and high-IoU threshold predictions. Extensive experiments on the widely-used Waymo dataset demonstrate that our YoCo framework, with only click-level annotations, achieves state-of-the-art performance compared to other weakly supervised methods. Remarkably, our method surpasses fully supervised approaches based on Cylinder3D, achieving competitive results using only 0.8\% of the fully supervised labels, significantly reducing annotation costs.