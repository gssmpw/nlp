\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\section{Appendix}
\label{sec:appendix}
Our supplementary material contains the following details:
    \begin{itemize}
        \item \textbf{Algorithm Implementation.} In Section \ref{sec:tsu_alg}, we provide algorithmic implementations of the TSU.
        \item \textbf{More Experiments.} In Section \ref{sec:more experiments}, we discuss the impact of YoCo on the field of autonomous driving.
        \item \textbf{Visualization.} In Section \ref{sec:visualization}, we provide some visualization results.
    \end{itemize}

\subsection{Algorithm Implementation}
\label{sec:tsu_alg}

In Algorithm \ref{alg:tsu}, we provide the detailed implementation of the TSU module, omitting the voxelization process of the point cloud.

\begin{algorithm}[ht]
    % 显示end
    \SetAlgoLined 
    % 算法名字
	\caption{TSU}
    \label{alg:tsu}
    % 输入
    % \SetKwInOut{Input}{Input}
    % \SetKwInOut{Output}{Output}
	\KwIn{
 
    $s_v$ is voxel size. 

    $D$ is the depth threshold.

    $S$ is the voxel voting space.

    $Y$ is the current frame pseudo label. 
    
    $T_{v}$ is the threshold for the number of votes. 
    
    $T_{s}$ is the threshold for the confidence scores. 

    $(v_{x_e}, v_{y_e}, v_{z_e})$ is the ego voxelization coordinates.
 
    $V_a=\{v_a:\{(x_a, y_a, z_a, s_a)\}_i^{n}\}$ is adjacent frame point voxelization dictionary, where $v_a$ is the set 
    
    of $n$ points whose current voxelization coordinates are $(v_{x_a},$ $v_{y_a}, v_{z_a})$, and each set contains the coord- inates $(x_a, y_a, z_a)$ and confidence scores $s_a$ for the current voxel $v_a$. 
 
    $V_t=\{v_t:\{$ $(x_t, y_t, z_t, s_t)\}_i^{n'}\}$ is current frame po- int voxelization dictionary. 
    }
    % 输出
	\KwOut{
    
    Updated pseudo label $\hat{Y}$.}
        
	\For{$v_a$ \text{in} $V_a$}{
        \tcp{1.Update Threshold.}
		$dist = s_v\sqrt{(v_{x_a}-v_{x_e})^2+(v_{y_a}-v_{y_e})^2} $\;
        $T'_{vote} = {dist}/{D}\times T_{vote}$\;
        $T'_{score} = {dist}/{D}\times T_{score}$\;
        \tcp{2.Build Voxel Voting Space.}
        $\bar{s}=\frac{1}{n}\left(\sum{s_a}\right)$\;
		\If{$\max\left(\bar{s}\right)\ge T'_{score}$ \text{and} $n\ge T'_{vote}$,}{
			$S\left[v_{x_a}, v_{y_a}, v_{z_a}\right]=\arg\max_{cls}\left(\bar{s}\right)$;
		}
        \Else{$S\left[v_{x_a}, v_{y_a}, v_{z_a}\right]=-1$;}
	}

        \tcp{3.Update Current Pseudo Label.}
        $\hat{Y} = S\left[v_{x_t}, v_{y_t}, v_{z_t}\right]$\;
        $Mask = \hat{Y}==-1$\;
        $\hat{Y}\left[Mask\right]=Y\left[Mask\right]$.
\end{algorithm}


\subsection{More Experiments}
\label{sec:more experiments}
\noindent\textbf{Pseudo label quality comparison.}
Figure \ref{fig:miou} demonstrates the effectiveness of our method in generating high-quality pseudo labels, and our VFM-PLG module achieves nearly a 10\% performance improvement compared to the method that uses only clicks as prompts. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{img/miou.pdf}
    % \vspace{-6mm}
    \caption{\textbf{Comparisons of IoU on WOD Validation Dataset.} We combine click-level and 2D bounding box-level annotations with various methods to generate pseudo labels for the validation dataset and evaluate them with ground truth.}
    \label{fig:miou}
    % \vspace{-1mm}
\end{figure}

\noindent\textbf{Results on nuScenes.} We conduct experiments on the nuScenes dataset, using only the point clouds projected onto the image. Data augmentation for single-frame data included global rotation, translation, scaling, random flipping, and shuffling. We specifically focus on the barrier, bicycle, bus, car, construction vehicle, motorcycle, pedestrian, traffic cone, trailer, and truck categories for experiments. As shown in Table \ref{tab:nuscenes}, our method YoCo achieves a 14\% improvement in mAP and a 17.868\% improvement in mIoU compared to the Click-based method, demonstrating its effectiveness.
\begin{table}[ht]
    \centering
    \adjustbox{max width=0.8\linewidth}{
    % \begin{tabular}{cccc} 
    % \hline
    % Sup.           & Method     & mAP   & mIoU    \\ 
    % \hline
    % \multirow{3}{*}{Full} & SparseUnet & \textbf{49.56} & 68.056  \\
    %                       & Cylinder3D & 45.74 & 66.198  \\
    %                       & PTv3       & 46.1  & \textbf{71.972}  \\ 
    % \hline
    % \multirow{2}{*}{Weak} & Click      & 19.35 & 37.686  \\
    %                       & \textbf{YoCo (Ours)}       & \textbf{33.35} & \textbf{55.554}  \\
    % \hline
    % \end{tabular}
    \begin{tabular}{ccccc} 
    \hline
    Sup.                  & Method               & mAP            & mIoU            & mIoU (\textit{fg})         \\ 
    \hline
    \multirow{3}{*}{Full} & SparseUnet           & \textbf{49.56} & 68.056          & 78.548           \\
                          & Cylinder3D           & 45.74          & 66.198          & 76.335           \\
                          & PTv3                 & 46.1           & \textbf{71.972} & \textbf{81.264}  \\ 
    \hline
    \multirow{2}{*}{Weak} & Click                & 19.35          & 37.686          & 58.067           \\
                          & \textbf{YoCo (Ours)} & \textbf{33.35} & \textbf{55.554} & \textbf{68.267}  \\
    \hline
    \end{tabular}
    }
    % \vspace{-2mm}
    \caption{\textbf{Performance Comparisons on NuScenes Validation Dataset.} \textit{fg} denotes that only foreground points participate in the evaluation, ignoring background points.}
    \label{tab:nuscenes}
    \vspace{-2mm}
\end{table}

\noindent\textbf{Greater click range error.}
To evaluate the robustness of YoCo for larger target sizes in autonomous driving, we conduct additional experiments with error ranges of 1.0m, 1.5m, and 2.0m for vehicle classes, as shown in Table \ref{tab:veh_range}. Expanding the click range to 2.0m results in stable performance and even improvement, demonstrating the robustness of our method. These experiments suggest that the BEV center is not the optimal click, and identifying the optimal click will be explored in future work.
\begin{table}[ht]
% \vspace{-2mm}
    \centering
    \adjustbox{max width=1.0\linewidth}{
    \begin{tabular}{ccccccc} 
    \hline
    Range & 0.0    & 0.1    & 0.5    & 1.0    & 1.5    & 2.0     \\ 
    \hline
    AP   & 67.69  & 67.28  & 67.70  & 67.22  & 67.60  & \textbf{68.06}   \\
    IoU  & 81.136 & 82.288 & 82.229 & 84.350 & \textbf{85.400} & 83.885  \\
    \hline
    \end{tabular}
    }
    % \vspace{-2mm}
    \caption{\textbf{Greater Click Range Error for Vehicle.}}
    \label{tab:veh_range}
    \vspace{-4mm}
\end{table}

\noindent\textbf{False positives (FP) click.}
We consider that some small objects (such as \textit{Construction Cone/Pole}) may be mislabeled as people in the annotation. To address this issue, we propose incorporating 2D semantic information to improve segmentation accuracy. Specifically, we intentionally introduce \textit{Construction Cone/Pole} objects as \textit{Pedestrian} noise in each scene to simulate real-world mislabeling scenarios. As demonstrated in Tab. \ref{tab:box}, our method achieves 53.48\% mAP and 74.936\% mIoU when using ground truth (GT) boxes, which is comparable to the performance of YoCo. Furthermore, even with imprecise predictions from YOLO, our approach maintains robust performance, validating the efficacy of leveraging 2D semantic information for verification.
\begin{table}[ht]
    \vspace{-4mm}
    \centering
    \adjustbox{max width=0.4\linewidth}{
    \begin{tabular}{cccc} 
    \hline
    Box   & GT    & YOLO  \\ 
    \hline
    mAP   & \textbf{55.12} & 53.48      \\ 
    \hline
    mIoU  & \textbf{75.120} & 74.936      \\
    \hline
    \end{tabular}
    }
    % \vspace{-2mm}
    \caption{\textbf{YoCo with FP Problem.}}
    \label{tab:box}
    \vspace{-4mm}
\end{table}


\noindent\textbf{Discard click analysis.}
To verify whether all foreground objects need to be clicked, we conduct a random discard experiment. It is noteworthy that our default method inherently discards objects with insufficient point cloud representations in the Bird's Eye View (BEV) perspective, resulting in a 16.31\% object discard rate. We further evaluate the impact of additional discarding by randomly removing instances with probabilities of 10\%, 25\%, and 30\%, as detailed in Table \ref{tab:drop1} (b), (c), and (d). The experimental results indicate that as the discard rate increases, the network performance gradually declines.  However, even with a discard proportion of 37.19\%, our method still achieves 53.96\% mAP and 73.248\% mIoU, demonstrating the robustness of YoCo.

\begin{table}[ht]
    \centering
    \adjustbox{max width=0.8\linewidth}{
    \begin{tabular}{ccccc} 
    \hline
        & Method                                                                   & Proportion     & mAP            & mIoU             \\ 
    \hline
    (a) & YoCo                                                                     & 16.31\%        & \textbf{55.35} & \textbf{74.770}  \\ 
    \hline
    (b) & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Discard\\Object\end{tabular}} & 24.65\% (10\%) & 54.29          & 73.742           \\
    (c) &                                                                          & 37.19\% (25\%) & 53.96          & 73.248           \\
    (d) &                                                                          & 58.10\% (50\%) & 46.68          & 70.008           \\
    \hline
    \end{tabular}
    }
    % \vspace{-2mm}
    \caption{\textbf{Ablation Study of Drop Click.}}
    \label{tab:drop1}
    \vspace{-2mm}
\end{table}

\noindent\textbf{Scalability across sequences.}
% In Table \ref{tab:drop2}, we evaluate the effectiveness of YoCo using finite annotated sequences by annotating each point cloud sequence at intervals. In Table \ref{tab:drop2} (b) and (c), we present results using 1/3 and 1/2 of the click annotation sequence. Even with only 1/3 of the data, mAP reaches 53.13\%, demonstrating the scalability of our method.
In Table \ref{tab:drop2}, we assess the efficacy of YoCo under limited annotation scenes by subsampling point cloud sequences at intervals. Specifically, in Table \ref{tab:drop2} (b) and (c), we report performance metrics using 1/3 and 1/2 of the annotated sequences, respectively. Notably, even with only 1/3 of the annotation data, our method achieves 53.13\% mAP and 72.304\% mIoU, highlighting its scalability and robustness in resource-constrained settings.
\begin{table}[ht]
    \centering
    \adjustbox{max width=0.55\linewidth}{
    \begin{tabular}{cccc} 
    \hline
        & Interval & mAP            & mIoU             \\ 
    \hline
    (a) & 1        & \textbf{55.35} & \textbf{74.770}  \\ 
    \hline
    (b) & 2        & 53.90          & 72.999           \\
    (c) & 3        & 53.13          & 72.304           \\
    \hline
    \end{tabular}
    }
    % \vspace{-2mm}
    \caption{\textbf{Ablation Study of Across Sequences.}}
    \label{tab:drop2}
    \vspace{-2mm}
\end{table}

\noindent\textbf{Unsupervised application of YoCo.}
To extend the applicability of our approach, we investigate the use of the 2D detection model YOLO for unsupervised point cloud segmentation tasks. Specifically, we align YOLO's detection categories with the annotation categories of the Waymo dataset. Since YOLO does not directly support the \textit{Cyclist} category, we propose a heuristic strategy: the \textit{Cyclist} category is inferred by combining detections from the intersection of the \textit{Person} and \textit{Bicycle} categories. As shown in Table \ref{tab:uns}, our unsupervised approach achieves significant performance improvements, with a 5.59\% increase in mAP and a 4.672\% increase in mIoU compared to the click-based method. These results validate the effectiveness of leveraging 2D detection models for unsupervised point cloud segmentation tasks.
\begin{table}[ht]
    \centering
    \adjustbox{max width=0.95\linewidth}{
    \begin{tabular}{ccccc} 
    \hline
    Supervision           & Annotation & Model      & mAP   & mIoU    \\ 
    \hline
    Full                  & 3D Mask    & SparseUnet & 59.26 & 79.505  \\ 
    \hline
    \multirow{2}{*}{Weak} & Click$^*$  & SparseUnet & 40.19 & 67.510  \\
                          & Click$^\dag$ & YoCo       & 55.35 & 74.770  \\ 
    \hline
    Unsupervised          & YOLO       & YoCo       & 45.78 & 72.182  \\
    \hline
    \end{tabular}
        }
        % \vspace{-2mm}
     \caption{\textbf{Performance Comparison of Supervision Strategies on the Waymo Validation Dataset.} YOLO refers to pseudo labels derived from YOLO prediction results.}
    \label{tab:uns}
\end{table}


% \begin{table*}[t]
%   \centering
%   % add the adjustbox
%   \adjustbox{max width=1.0\textwidth}{
%     \begin{tabular}{ccccccccccc} 
%     \hline
%     \multirow{2}{*}{Supervision} & \multirow{2}{*}{Annotation}      & \multirow{2}{*}{Model} & \multicolumn{4}{c}{3D Instance Segmentation (AP)}        & \multicolumn{4}{c}{3D Semantic Segmentation (IoU)}            \\ 
%     \cline{4-11}
%                                  &                                  &                        & mAP   & Veh.  & Ped.  & Cyc.  & mIoU   & Veh.   & Ped.   & Cyc.    \\ 
%     \hline
%     Full                         & 3D Mask                          & SparseUnet             & 59.26 & 80.25 & 56.95 & 40.59 & 79.505 & 96.675 & 81.906 & 59.933  \\ 
%     \hline
%     \multirow{2}{*}{Weak}        & Click$^*$                   & SparseUnet             & 40.19 & 59.15 & 36.98 & 24.45 & 67.510 & 76.225 & 76.118 & 50.187  \\
%                                  & Click$^\dag$ & YoCo                   & 55.35 & 67.69 & 55.25 & 43.12 & 74.770 & 81.136 & 81.716 & 64.459  \\ 
%     \hline
%     Unsupervised                           & YOLO                             & YoCo                   & 45.78 & 62.05 & 53.78 & 21.53 & 72.182 & 87.954 & 79.020 & 49.571  \\
%     \hline
%     \end{tabular}
%   }
%   \caption{\textbf{Performance Comparisons of Different Supervision Method on Waymo Validation Dataset.} $^*$ represents the pseudo label generated by SAM using the corresponding annotation as the prompt. $^\dag$ denotes the pseudo label generated by our VFM-PLG module. YOLO denotes the pseudo label generated by the YOLO prediction results. Abbreviations: vehicle (Veh.), pedestrian (Ped.), cyclist (Cyc.). }
%   \vspace{-1mm}
%   \label{tab:main results}
% \end{table*}

\subsection{Visualization}
\label{sec:visualization}
In Figure \ref{fig:vis_cyc}, we evaluate the performance of SparseUnet trained with various annotation strategies. Our proposed method exhibits superior visual quality, significantly outperforming click-based approaches by generating more accurate predictions. This improvement is largely driven by the integration of deep prior information into the pseudo label generation process, which enhances the quality of the generated labels.

In Figure \ref{fig:vis_scene}, we provide visualization segmentation results for some scenes.

\begin{figure*}[t]
\centering
   \includegraphics[width=0.95\linewidth]{img/vis_cyc.pdf}
   % \vspace{-2mm}
   \caption{\textbf{Visiualization for Cyclist.}}
   % \vspace{-2mm}

\label{fig:vis_cyc}
\end{figure*}

\begin{figure*}[t]
\centering
   \includegraphics[width=1.0\linewidth]{img/more_vis.pdf}
   % \vspace{-2mm}
   \caption{\textbf{Visiualization for Scenes.}}
   % \vspace{-2mm}

\label{fig:vis_scene}
\end{figure*}
% \subsection{Social Impact of the YoCo}
% \label{sec:social impact}
% YoCo’s approach to point cloud instance segmentation offers significant practical value for the autonomous driving industry, where precise labeling is costly and time-consuming. For instance, annotating a single scene in the ScanNet dataset takes an average of 22.3 minutes. By using only click-level annotations to generate high-quality pseudo labels, YoCo greatly reduces reliance on fully labeled data, saving both time and costs associated with data labeling—a critical bottleneck in autonomous driving system development.

% Furthermore, YoCo’s ability to achieve comparable performance with minimal fine-tuning (using only 0.8\% of fully supervised labels) means that autonomous vehicle companies can maintain high segmentation accuracy with less annotated data. This is particularly valuable for scaling up the collection and processing of diverse driving scenarios, ultimately accelerating the training and deployment of robust models in various environments.

% \subsection{Experiments}
% \label{sec:extra_exp}
