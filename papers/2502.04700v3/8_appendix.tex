\section{Appendix}\label{sec:appendix}

\subsection{Experiments}\label{appendix:hyperparameters}
For VeRA, LoRA and PiSSA, we experimented with a range of learning rates, from higher to lower, along with three different scheduling approaches: ReduceLRonPlateau, Linear, and Cosine. The hyperparameters that yielded the best average performance were selected for further experimentation. The observed discrepancies with EigenLoRAx hyperparameters are attributable to these methodological choices. Comprehensive hyperparameter tuning for EigenLoRAx was not pursued extensively, as the initially selected hyperparameters, notably a high learning rate paired with ReduceLRonPlateau or Linear, demonstrated satisfactory performance, thereby conserving computational resources.

\subsubsection{Image Classification}
\paragraph{Trainable parameters for EigenLoRAx}
The base model is vit-base-patch16-224. The following are the trainable parameters in ViT~\citep{vision_transformer} that are trained for EigenLoRAx. We ignore the last linear layer for simplicity since it is trained for all models and baselines and is constant. The loading parameter has the shape of $[\text{number of EigenLoRAx PC} , 1]$ (we only have $2$ in each EigenLoRAx PC for this experiment). Therefore, the total number of trainable parameters (for the number of components$=2$) is $12\text{ (layers) }\times4\text{ (set of parameters per layers) }\times2\text{ (number of trainable parameter per coefficient) } = 96$ trainable parameters.

\paragraph{Hyperparameters} LoRA~\citep{hu2021lora} and VeRA~\citep{kopiczko_vera_2023} implementations are taken from the HuggingFace PEFT~\citep{peft} library with hyperparameters of the default method. For Food101~\citep{food101} experiment, we randomly remove 1 class for ease of compute. Experimental hyperparameters are reported in \autoref{tab:appendix_classn_lora_hyp} and \autoref{tab:appendix_classn_leora_hyp}.
\begin{table}[h]
  \centering
   \caption{Hyperparameters for LoRA~\citep{hu2021lora} and VeRA~\citep{kopiczko_vera_2023} for the Image Classification Experiment}
  \begin{tabular}{cccc}
    \toprule
    & \textbf{CIFAR100} & \textbf{Flowers102} & \textbf{Food101} \\
    \midrule
    Learning Rate            & $1\mathrm{e}{-4}$
 & $1\mathrm{e}{-4}$ & $1\mathrm{e}{-4}$  \\ 
    Weight Decay            & 0.1      & 0.1      & 0.1          \\ 
    Warmup ratio  & 0.06     & 0.06     & 0.06      \\ 
    Epochs        & 10       & 10       & 10             \\ 
    Number of Subsets          & 5       & 6        & 5        \\ 
    Categories/Subset          & 20      & 17        & 20     \\
    Seed          & 42        & 42        & 42            \\ 
    Batch Size          & 128       & 64        & 128      \\ 
    \bottomrule
  \end{tabular}
  \label{tab:appendix_classn_lora_hyp}
\end{table}

\begin{table}[h]
  \centering
    \caption{Hyperparameters for EigenLoRAx for the Image Classification Experiment}
  \begin{tabular}{ccccc}
    \toprule
    & \textbf{CIFAR100} & \textbf{Flowers102} & \textbf{Food101}  \\
    \midrule
    Learning Rate            & $1\mathrm{e}{-2}$
 & $1\mathrm{e}{-2}$ & $1\mathrm{e}{-2}$ \\ 
    Weight Decay            & 0.1      & 0.1      & 0.1        \\ 
    Warmup ratio  & 0.06     & 0.06     & 0.06      \\ 
    Epochs        & 10       & 10       & 10            \\ 
    Number of Subsets          & 5       & 6        & 5   \\ 
    Categories/Subset          & 20      & 17        & 20  \\
    Seed          & 42        & 42        & 42        \\ 
    Batch Size          & 128       & 64        & 128     \\ 
    \bottomrule
  \end{tabular}
  % \caption{Hyperparameters for EigenLoRAx for the Image Classification Experiment}
  \label{tab:appendix_classn_leora_hyp}
\end{table}

\paragraph{Experimental Results} The experiments were conducted $5$ times utilizing randomly generated dataset splits. The mean accuracy values are reported in \autoref{tab:vision_models}. Empirical analysis indicates that without control and annealing of learning rates, the loss for both LoRA and VeRA may diverge or plateau, particularly with high learning rates. Even with the lower learning rate, Full training or LoRA can overfit to the training data without proper regularization. In contrast, no such instability was observed during EigenLoRAx training, where a relatively higher learning rate proved advantageous for rapid convergence. 

\begin{table}[h]
    \centering
    \caption{Image Classification Accuracy results on CIFAR100~\citep{cifar100}}
    \begin{tabular}{llcccccc}
        \toprule
         & \textbf{Trainable} & &  &  &  &  & \\
        \textbf{Model} & \textbf{Params} & \textbf{subset1} & \textbf{subset2} & \textbf{subset3} & \textbf{subset4} & \textbf{subset5} & \textbf{Avg.} \\ 
        \midrule
        \textbf{FT}             & 86389248             & 98.8                        & 97.95                       & 95.55                       & 96.05                       & 96.3                        & 96.93                       \\
        \textbf{LoRA ($r=1$)}   & 36864                & 97.6                        & 93.95                       & 93.75                       & 91.75                       & 85.2                        & 92.45                       \\
        \textbf{LoRA ($r=4$)}   & 147456               & 98.15                       & 95.2                        & 93.5                        & 92.85                       & 89.25                       & 93.79                       \\
        \textbf{VeRA ($r=2$)}   & 18480                & 93.65                       & 89.7                        & 89.5                        & 89.95                       & 91.55                       & 90.87                       \\
        \textbf{EigenLoRAx ($K=2$)} & 96                   & 97.25                       & 95.05                       & 94.55                       & 93                          & 94.15                       & 94.8                        \\
        % \textbf{EigenLoRA ($K=1$)} & 48                   & 96.35                       & 94.6                        & 94.01                       & 92                          & 92.9                        & 93.97\\
            \bottomrule
    \end{tabular}
    \label{tab:appendix_cifar100}
\end{table}


\begin{table}[h]
    \centering
    \caption{Image Classification Accuracy results on Food101~\citep{food101}}
    \begin{tabular}{llcccccc}
        \toprule
         & \textbf{Trainable} & &  &  &  &  & \\
        \textbf{Model} & \textbf{Params} & \textbf{subset1} & \textbf{subset2} & \textbf{subset3} & \textbf{subset4} & \textbf{subset5} & \textbf{Avg.} \\ 
        \midrule
        \textbf{FT}               & 86389248             & 98.64                       & 97                          & 97.36                       & 94.28                       & 95.92                       & 96.64                       \\
\textbf{LoRA ($r=1$)}   & 36864                & 93.36                      & 88.44                       & 94.28                       & 89.4                        & 89.9                        & 91.076                      \\
\textbf{LoRA ($r=4$)}   & 147456               & 98.2                        & 96.96                       & 96.08                       & 92.88                       & 94.52                       & 95.728                      \\
\textbf{VeRA ($r=2$)}   & 18480                & 91.22                       & 88.42                       & 94.42                       & 91.88                       & 92.82                       & 91.752                      \\
\textbf{EigenLoRAx ($K=2$)} & 96                   & 97.24                       & 95.96                       & 96                          & 91.88                       & 94.6                        & 95.136                      \\
% \textbf{EigenLoRA ($K=1$)} & 48                   & 96.88                       & 96.04                       & 96                          & 90.8                        & 94.5                        & 94.844  \\
            \bottomrule
    \end{tabular}
    \label{tab:appendix_food101}
\end{table}

\begin{table}[h]
    \centering
    \caption{Image Classification Accuracy results on Flowers102~\citep{flowers102}}
    \begin{tabular}{lccccccc}
        \toprule
        \textbf{Model}  & \textbf{subset1} & \textbf{subset2} & \textbf{subset3} & \textbf{subset4} & \textbf{subset5} & \textbf{subset6} & \textbf{Avg.} \\ 
        \midrule
        \textbf{FT}                          & 99.7                        & 99.3                        & 98.01                       & 98.22                       & 99.7                        & 98.01                       & 98.82          \\
\textbf{LoRA ($r=1$)}                   & 85.9                        & 88.47                       & 92.69                       & 91.02                       & 91.7                        & 91.01                       & 90.13          \\
\textbf{LoRA ($r=4$)}                  & 96.23                       & 92.76                       & 97.22                       & 95.01                       & 98.24                       & 90.73                       & 95.03         \\
\textbf{VeRA ($r=2$)}                & 99.2                        & 95.4                        & 97.7                        & 94.7                        & 90.9                        & 95                          & 95.48        \\
\textbf{EigenLoRAx ($K=2$)}                    & 99.686                      & 97.905                      & 97.689                      & 98.291                      & 99.344                      & 97.718                      & 98.43         \\
% \textbf{EigenLoRA ($K=1$)}                    & 99.581                      & 98.286                      & 96.673                      & 97.863                      & 99.125                      & 96.9                        & 98.07\\
            \bottomrule
    \end{tabular}
    \label{tab:appendix_flowers102}
\end{table}
\FloatBarrier


\subsection{Natural Language Processing - GLUE benchmark}~\label{appendix:glue}

\paragraph{Hyperparameters} LoRA~\citep{hu2021lora}, VeRA~\citep{kopiczko_vera_2023} and PISSA~\citep{meng_pissa_2024} implementations are taken from the HuggingFace PEFT~\citep{peft} library. Refer to \autoref{tab:appendix_glue_lora_hyp} and \autoref{tab:appendix_glue_elora_hyp} for hyperparameter details. For LoRA~\citep{hu2021lora}, we use the ranks $\in \{8,16\}$. For VeRA~\citep{kopiczko_vera_2023}, we use rank$=256$, and for EigenLoRAx, we use $K\in\{16, 32\}$ and $r=8$. Here, $r$ refers to the dimensionality of the trainable coefficients and not the rank. For both PISSA~\citep{meng_pissa_2024} and LoRA, all the parameters of the low rank matrix are trainable. For the EigenLoRAx initialization experiment, we train both the components and coefficients for a fair comparison with PISSA. In practice, however, we do not need to do so - we can tune only the sparse coefficients and after the loss converges, finetune the components for a few training steps.



\begin{table}[h]
  \centering
   \caption{Hyperparameters for LoRA~\citep{hu2021lora}, VeRA~\citep{kopiczko_vera_2023} and PiSSA~\citep{meng_pissa_2024} for the GLUE benchmark.~\citep{glue}}
  \begin{tabular}{ccccccc}
    \toprule
    & CoLA & MRPC & QNLI & RTE & SST-2 & STSB \\
    \midrule
    Learning Rate            & $4\mathrm{e}{-4}$
 & $4\mathrm{e}{-4}$ & $4\mathrm{e}{-4}$ & $5\mathrm{e}{-4}$ & $5\mathrm{e}{-4}$ & $4\mathrm{e}{-4}$ \\ 
    Weight Decay            & 0.1      & 0.1      & 0.1      & 0.1      & 0.1      & 0.1      \\ 
    Warmup ratio  & 0.06     & 0.06     & 0.06     & 0.06     & 0.06     & 0.06     \\ 
    Epochs        & 80       & 30       & 25       & 80       & 60       & 40       \\ 
    Scheduler     & Linear   & Linear   & Linear   & Linear   & Linear   & Linear   \\
    Seed          & 0        & 0        & 0        & 0        & 0        & 0        \\ 
    Batch Size          & 64       & 64        & 64        & 64        & 64        & 64\\ 
    \bottomrule
  \end{tabular}
  \label{tab:appendix_glue_lora_hyp}
\end{table}

\begin{table}[h]
  \centering
    \caption{Hyperparameters for EigenLoRAx for the GLUE benchmark.~\citep{glue}.\\ (RLrP - ReduceLRonPlateau)} 
  \begin{tabular}{ccccccc}
    \toprule
    & CoLA & MRPC & QNLI & RTE & SST-2 & STSB \\
    \midrule
    Learning Rate            & $4\mathrm{e}{-3}$
 & $4\mathrm{e}{-3}$ & $4\mathrm{e}{-3}$ & $5\mathrm{e}{-3}$ & $5\mathrm{e}{-3}$ & $4\mathrm{e}{-3}$ \\ 
    Weight Decay            & 0.1      & 0.1      & 0.1      & 0.1      & 0.1      & 0.1      \\ 
    Warmup ratio  & 0.06     & 0.06     & 0.06     & 0.06     & 0.06     & 0.06     \\ 
    Epochs        & 80       & 30       & 25       & 80       & 60       & 40       \\ 
    Scheduler     & RLrP & RLrP   & RLrP   & RLrP & RLrP   & RLrP   \\
    Seed          & 0        & 0        & 0        & 0        & 0        & 0        \\
    Batch Size          & 64       & 64        & 64        & 64        & 64        & 64\\ 
    \bottomrule
  \end{tabular}
  \label{tab:appendix_glue_elora_hyp}
\end{table}
\FloatBarrier
%%%%%%%%%Lots of LoRAs here

\FloatBarrier
\subsection{Text-to-Image Generation (Stable Diffusion Models)}\label{appendix:diffusion}
\autoref{fig:diffusion3} and \autoref{fig:diffusion2} show more examples of a text-to-image stable diffusion model finetuned using EigenLoRAx. Note that not only there is no publicly available code for VeRA that allows its usage in complex text-to-image generation tasks, but our VeRA implementation also did not work well in this task.
\begin{figure}[htb]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.8\textwidth]{images/diff_app3.jpg}
\end{center}
\caption{(Part 1) A single EigenLoRAx (identical components, varying loadings) was employed to produce these images utilizing the Stable Diffusion-XL~\cite{sdxl} model. A comparison between our results and those obtained from multiple LoRAs does not show a noticeable degradation in visual quality.}
\label{fig:diffusion3}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=0.8\textwidth]{images/diffuse2.pdf}
\end{center}
\caption{(Part 2) A single EigenLoRAx (identical components, varying loadings) was employed to produce these images utilizing the Stable Diffusion-XL~\cite{sdxl} model. A comparison between our results and those obtained from multiple LoRAs demonstrates no noticeable degradation in visual quality. }
\label{fig:diffusion2}
\end{figure}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=0.8\textwidth]{images/diff_an1.jpg}
\end{center}
\caption{
\small{Analytical reconstruction of LoRAs using EigenLoRAx which shows no degradation in relative visual quality. 
See Appendix~\ref{appendix:diffusion} for more examples.}}
\label{fig:diffusion}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=0.9\textwidth]{images/learndiffuse1.pdf}
\end{center}
\caption{\small{
Comparison of generated images by LoRA and EigenLoRAx trained on Torino Aqua anime style images. 
For EigenLoRAx, we utilized 12 components with only trainable coefficients to finetune the base model.
}}
\label{fig:traindiffusion}
\end{figure}
\FloatBarrier

\subsection{Additional Experiments}
Furthermore, we also performed a 3D object pose estimation~\citep{wang2020NeMo, Kaushik2024SourceFreeAI} finetuning experiment using a modified Resnet-101. The task of 3D object pose estimation involves the prediction of three rotation parameters (azimuth, elevation, in-plane rotation) of an object relative to the camera. The pose estimation error between the predicted rotation matrix and the ground truth rotation matrix is given as
$\Delta (R_{pred}, R_{gt}) = \frac{\vert\vert \log_b (R_{pred}^{\intercal} R_{gt}) \vert\vert_F}{\sqrt{2}} $
We show the results for the $\frac{\pi}{6}$ accuracy threshold for this experiment.

\begin{table}[!ht]
    \centering
        \caption{3D object pose estimation accuracy ($\frac{\pi}{6}$ threshold)}
    \begin{tabular}{lcccccccc}
    \toprule
        Method & Param & Airplane & Motorbike & Boat & Bottle & Bus & Car & Average \\ 
        \midrule
        LoRA ($r=16$) & 215K & 79.9 & 80.1 & 71.5 & 89.8 & 90.1 & 96.6 & 84.67 \\ 
        VeRA ($r=256$) & 40K & 68.4 & 72.4 & 64.3 & 88.4 & 87.2 & 94.4 & 79.18 \\ 
        EigenLoRAx ($K=2$) & 16K & 81.4 & 80.0 & 71.4 & 90 & 92.3 & 97.5 & 85.43 \\ 
        \bottomrule
    \end{tabular}
\end{table}
\FloatBarrier
\section{Method Analysis and Ablation}
\label{sec:ablation}
Through a rigorous comparative analysis of EigenLoRAxs and their target LoRAs, we identified that the most pronounced reconstruction discrepancies manifest in the initial and terminal layers of the neural network, as depicted in~\autoref{fig:reconserror}. Allowing the EigenLoRAx PCs in these layers to undergo fine-tuning alongwith the coefficients can alleviate failure scenarios, thereby alleviating the need for comprehensive model fine-tuning.
\begin{figure}[h]
  \begin{center}
    \includegraphics[width=.8\textwidth]{images/to_k_lora_down_graph.png}
  \end{center}
  \caption{\small{Average reconstruction error between EigenLoRAx and a set of LoRAs for all UNet layers in a stable diffusion model.}}
  \label{fig:reconserror}
\end{figure}

\subsection{How to Choose $K$ Principal Components and $r$ for EigenLoRAx}
We perform an ablation study on the selection of EigenLoRAx principal components ($K$). Our analysis concentrates on one experiment as shown in ~\autoref{fig:mrpc_K}, specifically pertaining to the MRPC task within the GLUE~\citep{glue} benchmark. The analysis in \autoref{fig:mrpc_loss} shows the training loss in relation to increasing number of EigenLoRAx principal components $K$, as well as the explained variance of the LoRAs used to initialize the EigenLoRAx in \autoref{fig:mrpc_var}. We find, empirically, that choosing EigenLoRAx PCs for the explained variance of $50-80\%$ of the LoRAs used to initialize EigenLoRAx is sufficient for a robust initialization. This is shown in \cref{fig:mrpc_var} where we choose $K=8$ which roughly corresponds to the explained variance of $55-60\%$. We further ablate this choice in \cref{fig:mrpc_loss}, where although substantial improvements are evident up to $K = 8$, an increase in the number of $K$ thereafter yields only marginal gains, demonstrating diminishing returns as the number of components increases. The parameter $r$ in EigenLoRAx does not equate the \textit{rank} parameter in LoRA and its variants. It reflects the dimensionality of the EigenLoRAx coefficients. Although $r=1$ works well, we observe slight performance improvements as we increase this value as shown in \cref{fig:r_ablate}. Increasing this value corresponds to a small amount of parameter increase. We observe no finetuning instability by changing this value and recommend that it can be set to anywhere between 1 and the rank of the LoRAs used to initialize EigenLoRAx.
\begin{figure}[!htb]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/mrpc_K_ablate.png} % first figure itself
        \caption{Training Loss Convergence for different numbers of EigenLoRAx PCs}
        \label{fig:mrpc_loss}
    \end{minipage}\hfill
    % Remove or comment out this line
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/mrpc_var.png} % second figure itself
        \caption{Explained Variance for increasing number of PCs}
    \label{fig:mrpc_var}
    \end{minipage}
    \caption{Ablation of Number of EigenLoRAx Principal Components}
    \label{fig:mrpc_K}
\end{figure}
\begin{figure}[h]
  \begin{center}
    \includegraphics[width=\textwidth]{images/r_ablation_updated.png}
  \end{center}
  \caption{Ablation for the EigenLoRAx's $r$ hyperparameter. This experiment was done for the MRPC task in the GLUE benchmark.}
  \label{fig:r_ablate}
\end{figure}
\subsection{Failure Cases}
\label{ssec:failure_append}
\autoref{fig:failurediffusion} illustrates a potential failure case of EigenLoRAx, where the incorrect number of principal components (PCs) was selected. In this instance, the "mosaic style" information was excluded from the principal subspace identified by EigenLoRAx due to an insufficient number of PCs. However, this issue can be resolved by selecting a larger number of PCs, as the extended principal subspace contains the necessary information for the task.

Another hypothetical failure scenario arises if the domain gap between the low-rank adapters used to initialize EigenLoRAx and the downstream task is significantly large. Although we do not observe such a case in our experiments, it is plausible that under such conditions, EigenLoRAx might underperform. This issue could potentially be mitigated by allowing only a subset of PCs to remain trainable, enabling the model to adapt more effectively to the target domain.

A further observed limitation of EigenLoRAx occurs in complex tasks like Text-to-Image generation, which may extend to other tasks as well. If the majority of LoRAs used to initialize EigenLoRAx encode biases (e.g., related to gender, race, or context), these biases tend to propagate into EigenLoRAx outputs. While such biases are a common issue in deep learning models trained using stochastic gradient descent or similar methods, addressing them remains a critical area of future work. We consider this an important avenue for improvement and discuss the broader implications in \cref{sec:impact}.

\subsection{Impact of LoRA adapter quality on EigenLoRAx PC initialization}

To evaluate EigenLoRAx’s robustness to adapter quality and its resistance to noise, we conducted an ablation study on a subset of tasks of the NLU experiment specified in Section~\ref{sec:nlp}. Specifically, we generated EigenLoRAx adapters using LoRA matrices with varying levels of random noise added. The results are shown in \autoref{tab:noise}

\begin{table}[!h]
    \centering
        \caption{EigenLoRAx performance on subset of GLUE task using noisy LoRA adapters for initialization}
    \begin{tabular}{cccccc}
    \toprule
        Noise Level & CoLA & MRPC & RTE & STS-B & Avg \\ 
        \midrule
        5\% & 60.51 & 85.45 & 74.73 & 89.9 & 77.65 \\ 
        15\% & 57.53 & 83.09 & 72.92 & 89.9 & 75.86 \\ 
        30\% & 55.23 & 76.47 & 71.84 & 89.8 & 73.34 \\ 
        \bottomrule
    \end{tabular}
    \label{tab:noise}
\end{table}
The results show that EigenLoRAx exhibits only minor performance changes even as noise levels increase significantly, indicating some robustness to adapter quality. This suggests that EigenLoRAx can still perform effectively without high quality adapters. However, there is a limit to this robustness. If the signal-to-noise ratio (SNR) in the initial LoRA matrices becomes extremely low—where the LoRAs primarily encode noise rather than meaningful information—the effectiveness of EigenLoRAx diminishes.
In such cases, the principal components (PCs) extracted by EigenLoRAx would correspond to random directions in the parameter space. Consequently, EigenLoRAx’s performance would resemble that of random matrix methods, such as VeRA and NoLA. These methods rely on a large number of random components or bases to approximate meaningful results. While they can achieve reasonable performance, they require fine-tuning a substantially larger number of weights associated with these large number of random components, leading to less efficient learning compared to EigenLoRAx.
This highlights an important consideration: for EigenLoRAx to maintain its efficiency and effectiveness, the initial LoRA matrices must contain at least a minimal level of meaningful signal. This requirement ensures that EigenLoRAx can leverage the structured information encoded in the LoRAs while avoiding the inefficiencies of purely random approaches.

\subsection{Forward pass and backward pass FLOPs}

While it is obvious that EigenLoRAx utilized significantly less number of model parameters as the number of tasks in a domain increase, we show that even in terms of floating point operations on a single task, EigenLoRAx is more efficient than LoRA for our experiments. Even for a single task, the number of floating point operations or multiply-accumulate operations in a forward pass for EigenLoRAx is lower than LoRA for all our experiments. Here are the comparisons of the floating point operations (FLOPs) for the forward (fwd FLOPs) and including backward pass (fwd+bwd FLOPs) for each of the Image Classification and GLUE benchmark (batch size = 1) (MFLOPs - MegaFlops):

\begin{table}[!htb]
    \centering
        \caption{Floating Point Operation calculations for GLUE Benchmark experiment}
    \begin{tabular}{lccc}
    \toprule
        Method & Training Parameters & fwd FLOPs & fwd+bwd FLOPs \\ 
        \midrule
        LoRA & 1.2M & 97,930 MFLOPS & 293,800 MFLOPS \\ 
        VeRA & 25K & 106,390 MFLOPS & 319,170 MFLOPS \\ 
        EigenLoRAx & 12K & 97,030 MFLOPS & 291,080 MFLOPS \\ 
        \bottomrule
    \end{tabular}
    \label{tab:flop_glue}
\end{table}

\begin{table}[!htb]
    \centering
        \caption{Floating Point Operation calculations for Image Classification experiment}
    \begin{tabular}{lccc}
    \toprule
        Method & Training Parameters & fwd FLOPs & fwd+bwd FLOPs \\ 
        \midrule
        LoRA & 36K & 33,773.8 MFLOPS & 101,322 MFLOPS \\ 
        VeRA & 18K & 33,744.8 MFLOPS & 101.234 MFLOPS \\ 
        EigenLoRAx & 96 & 33,730.2 MFLOPS & 101,191 MFLOPS \\ 
        \bottomrule
    \end{tabular}
    \label{tab:flop_class}
\end{table}

\FloatBarrier
\section{Broader Impact and Implications}\label{sec:impact}
This work presents a novel parameter-efficient method for deep learning methods utilizing open source, pretrained Low-Rank Adaptation (LoRA) models. By substantially reducing the computational and memory demands of training and inference, our approach creates a more sustainable and environmentally friendly deep learning paradigm. Our method democratizes accessibility to larger models, making them accessible to researchers and practitioners with limited resources. Furthermore, by harnessing pretrained models, our method can accelerate development and diminish the need for extensive data collection. However, we recognize the inherent risks associated with the use of pretrained models. These include potential biases (racial, gender, etc.), explicit content, since there is no guarantee of the data or method used in training the model, and the potential presence of malicious code. Appropriate caution is advised when using unverified, open-source models.