\begin{table}[!h]
    \caption{Image classification with Vision Transformer. ZS refers to zero-shot. AUG refers to Augmented for Low-Resource. EigenLoRAx matches or increases performance with drastically fewer number of parameters.}
    \begin{center}
    \begin{sc}
    \begin{adjustbox}{width=\columnwidth}
    \begin{tabular}{lcccc}
        \toprule
         & \# Train & CIFAR & Food & Flowers \\
         & Params  & 100 & 101 & 102 \\
        \midrule
        Full Training & 86M  & 97.0 & 96.64 & 98.82 \\
        Base Model &  15K & 90.07 & 90.8 & 80.71 \\
        LoRA ($r=4$)  & +147K & 93.79 & \textbf{95.73} & 95.03  \\
        LoRA ($r=1$)  & +36K  & 92.45 & 91.07 & 90.14  \\
        VeRA & +18K  & 90.87 & 91.75 & 91.25 \\
        \rowcolor{gray!20} ELoRAx$^{\text{AUG}}$ & +1K & 94.4  & 95.01 & 97.5 \\
        \rowcolor{gray!20} ELoRAx & +\textbf{96}  & \textbf{94.8} & 95.14 & \textbf{98.44} \\
        \rowcolor{gray!20} ELoRAx$^{\text{ZS}}$ & +0& 91.4  & 92.48 & 95.7 \\
        \bottomrule
    \end{tabular}
    \label{tab:vision_models}
    \end{adjustbox}
    \end{sc}
    \end{center}
\end{table}
\begin{table*}[!hbt]
    \caption{\small{GLUE benchmark results. We report Matthew's correlation for CoLA, Pearson correlation for STS-B, and accuracy for the remaining tasks. In all cases, higher values indicate better performance.
}}
    \begin{center}
    \begin{sc}
    \begin{tabular}{lcccccccccc}
        \hline
        & \# Trainable & \multirow{2}{*}{MRPC} & \multirow{2}{*}{SST-2} & \multirow{2}{*}{CoLA} & \multirow{2}{*}{QNLI} & \multirow{2}{*}{RTE} & \multirow{2}{*}{STS-B} & \multirow{2}{*}{Avg.} \\
        Method & Parameters & & & & & & & \\
        \hline
        Full Training  & 125M & 88.97 & 91.28 & 59.81 & 92.29 & 79.78 & 90.89 & 83.84 \\
        PISSA [\citenum{meng_pissa_2024}] & 1.2M  & 86.52 & 94.15 & 61.32 & 92.15 & 71.84 & 90.25 & 82.70\\
        \rowcolor{gray!20} EigenLoRAx$^\text{init}$ & 1.2M & 89.71 & 93.35 & 61.58 & 92.2 & 74.73 & 89.56 & 83.52 \\
        \hline
        LoRA ($r=32$) & 1.2M & 86.76 & \textbf{94.72}& 59.56 & 92.53 & 77.61 & \textbf{90.81 }& \textbf{83.67} \\
        VeRA ($r=256$) & 25K  & 75.98 & 93.23 & 54.14 & 89.21 & 66.78 & 87.03 & 77.72 \\
        \rowcolor{gray!20} EigenLoRAx & \textbf{12K} & \textbf{87} & 94.15& \textbf{59.81}& \textbf{92.73}& \textbf{77.62}& 90.58 & 83.65 \\
        \hline
    \end{tabular}
    \label{tab:glue_benchmark_results}
    \end{sc}
    \end{center}
\end{table*}
\subsection{Image Classification}~\label{sec:img_class}
This simpler task involves related datasets where the LoRAs used to construct EigenLoRAx are well-aligned with the downstream tasks, highlighting its finetuning efficiency.
\paragraph{Setup} We evaluate EigenLoRAx using a pretrained Vision Transformer (ViT)~\cite{vision_transformer} across 3 datasets. Each dataset is partitioned into 5–6 non-overlapping sub-datasets, mimicking continual learning~\cite{kaushik2021understandingcatastrophicforgettingremembering} and federated learning~\cite{federated} setups. As the sub-datasets are derived from the same source, their tasks are more domain-aligned.
For EigenLoRAx, we compute principal components (PCs) using all but one LoRA trained on individual sub-datasets (leave-one-out approach, Algorithm~\ref{algo:eigenlora}). The coefficient matrix $\alpha$ for the excluded task is then learned as described in Section~\ref{sec:algo}. All methods are finetuned for 10 epochs, with additional details in Appendix~\ref{appendix:hyperparameters}.
\paragraph{Parameter Efficiency} \autoref{tab:vision_models} summarizes our experimental results. All models require training the last linear layer (approx. 15K parameters) due to the pre-trained ViT having a different number of categories. For the Base Model, no additional parameters are trained. EigenLoRAx adapts to new sub-datasets using only two principal components (96 additional parameters), enabling it to match or outperform LoRA and VeRA, which use significantly more parameters. We also tested a zero-shot EigenLoRAx (weight initialized randomly within the principal subspace), training only the last layer. This model outperforms the base model with no additional parameters, demonstrating the effectiveness of principal subspace extraction. We also test a low resource scenario (ELoRAx$^{\text{AUG}}$), where only 2 LoRAs are available for extracting the PCs, which are then augmented using random, orthogonal PCs as described in \cref{algo:eigenlora}.
\begin{figure*}[!h]
\label{fig:convergence}
  \centering
  \includegraphics[width=\textwidth]{images/glu_loss_aug.jpg}
\caption{
\small{\textbf{Fast Convergence and Better Initialization} (left) EigenLoRAx demonstrates faster convergence compared to LoRA and VeRA. EigenLoRAx achieves a speedup of up to $1.5\times$ against LoRA and up to $2\times$ compared to PISSA.
This experiment was carried out on the CoLA task of the GLUE benchmark.}}
\label{fig:traininglosscola}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{GLUE Benchmark}~\label{sec:nlp}
Next, we evaluate EigenLoRAx on the General Language Understanding Evaluation (GLUE) benchmark~\citep{glue} datasets using the RoBERTa$_{base}$ model~\citep{roberta}. We use 6 different tasks: MRPC, SST-2, CoLA, QNLI, RTE and STS-B. Following the setup of VeRA, 
% due to time and budget limitations,
we omit time-intensive MNLI and QQP tasks, thus avoiding the use of MNLI initialization for MRPC, RTE, and STS-B tasks. In this setting, LoRAs are trained not on sub-datasets but on these different datasets representing a \textit{heterogeneous} domain setting, where the domain difference may be larger relative to the more domain-aligned setting in \cref{sec:img_class}. We follow the previous leave-one-out evaluation setup, where EigenLoRAx PCs are calculated using LoRAs of all but one task, and $\alpha$ is learnt for the left-out task. %We also tested the 
Refer to Appendix~\ref{appendix:glue} for all hyperparameters and implementation details.
\paragraph{Faster Convergence} Our results in \autoref{tab:glue_benchmark_results} show that EigenLoRAx ($K=32$) matches LoRA's performance with \textbf{100$\times$ fewer trainable parameters} and outperforms VeRA. EigenLoRAx extracts a useful principal subspace across diverse domains, enabling robust adaptation to new tasks. We also evaluate EigenLoRAx($^\text{init}$) weight initialization speed-up. Unlike PiSSA~\citep{meng_pissa_2024}, which initializes LoRA matrices with principal directions of pretrained weights, we randomly initialize weights within our extracted subspace. As shown in Figure~\ref{fig:traininglosscola}, EigenLoRAx converges faster than PiSSA and VeRA, and slightly faster than LoRA, highlighting the effectiveness of the principal subspace. VeRA's poorer performance may stem from suboptimal random initialization that fails to align with task-critical components. ELoRAx is also more efficient in terms of floating point operations for both forward and backward pass, as shown in~\cref{tab:flop_glue}. 
\paragraph{Low-Resource Scenario} 
To demonstrate the effectiveness of our subspace augmentation strategy~\cref{algo:eigenlora}, we conduct an experiment where EigenLoRAx is initialized with only 1–2 LoRAs. The results are presented in \cref{tab:glue_low}. We compare our method against augmenting EigenLoRAx with random components (EigenLoRAx+random) and using entirely random components (ELoRAx$^{\text{random}}$). As shown, our augmentation approach significantly outperforms random principal component selection. Interestingly, for MRPC, the base model's performance is retained. This suggests that the learned LoRA weights may not have influenced the base model, likely because they did not capture relevant information. While we do not provide theoretical guarantees for our principal component augmentation strategy—where randomly sampled vectors are iteratively orthogonalized to the existing EigenLoRAx principal vectors—we hypothesize that this targeted guidance helps prevent redundancy within the subspace. Consequently, it increases the likelihood of capturing the necessary task-relevant components.
\begin{table}[!h]
    \caption{Low-Resource GLUE Subset Results}
    \begin{center}
    \begin{sc}
    \begin{tabular}{lccc}
        \toprule
         & \# Param & MRPC & STS-B  \\
        \midrule
        ELoRAx$^{\text{RANDOM}}$ & 24K & 68.38 & -0.73 \\
        ELoRAx+rand & 24K  & 68.38 & 0.11  \\
        \rowcolor{gray!20} ELoRAx$^{\text{AUG}}$ & 24K & 83.09 & 85.28 \\
        \bottomrule
    \end{tabular}
    \label{tab:glue_low}
    \end{sc}
    \end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Lotsof LoRA
\begin{table*}[!htb]
    \caption{\small{Results for Lots of LoRAs. We report the Rouge-L scores for each of the 5 tasks from the training set and 5 from the testing set. EigenLoRAx achieves on average \textbf{88\%} of LoRA's performance while requiring anywhere from \textbf{$12\times$} to \textbf{$95\times$} less parameters in a zero-shot setting.
}}  
    \begin{center}
    \begin{sc}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lccccccccccccc}
        \hline
        & \# Trainable & \multirow{2}{*}{076} & \multirow{2}{*}{627} & \multirow{2}{*}{664} & \multirow{2}{*}{819} & \multirow{2}{*}{1631} & \multirow{2}{*}{039} & \multirow{2}{*}{290} & \multirow{2}{*}{391} & \multirow{2}{*}{442} & \multirow{2}{*}{1598}  & \multirow{2}{*}{Avg.} \\
        Method & Parameters & & & & & & & & & & &  \\
        \hline
        LoRA ($r=16$) & 9.4M  & 69.05 & 23.96 & 25 & 75 & 99.04 & 58.77 & 93.79 & 93.45 & 67.84 & 51.58 & 65.75  \\
        EigenLoRAx$^{\text{ZS}}$  & \textbf{98-786K} & 60.78 & 18.91 & 33.33 & 65.07 & 94.74 & 49.96 & 84.54 & 88.56 & 49.78 & 39.81 & 58.25  \\
        \rowcolor{gray!20} Performance Ratio & \textbf{ } & 0.88 & 0.79 & 1.33 & 0.87 & 0.96 & 0.79 & 0.90 & 0.95 & 0.73 & 0.77 & 0.88  \\
        \hline
    \end{tabular}%
    }
    \label{tab:lola}
    \end{sc}
    \end{center}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\subsection{Lots of LoRAs}
\label{sec:lotsofloras}
Finally, we also tested our method in settings where a large number of adapters may be trained on significantly diverse domains. Lots of LoRAs \citep{brüelgabrielsson2024compressserveservingthousands} is a collection of over 500 adapters of the Mistral-7B-Instruct-v0.2 model \citep{jiang2023mistral7b}, trained on a variety of natural instruction tasks \citep{wang-etal-2022-super}. It represents the realistic setting where we directly use publicly available trained adapters, which may present significant diversity in terms of quality and task domain. As all adapters are accompanied with their respective training datasets, Lots of LoRAs is particularly useful in evaluating EigenLoRAx. The task presents significant diversity and a higher $K$ is necessary to represent this open domain. 

\paragraph{Setup} 
% As running leave-one-out experiments are expensive, 
We split adapters randomly into two sets ($490, $5). EigenLoRAx PCs were calculated using the larger ``training" set and evaluations were done on the smaller ``test" set. We evaluated EigenLoRAx in a zero-shot setting (calculated using the already available adapter weights, no finetuning) . The results are shown in Table \ref{tab:lola} where we evaluate EigenLoRAx on the 5 tasks from the test set and also on 5 tasks from the training set to check for catastrophic forgetting or concept drift from scaling. The first 5 tasks are randomly sampled from the training set.
%%%%%%%%%%%%
\begin{figure*}[!htb]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/a1.jpg} % first figure itself
    \end{minipage}\hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/a2.jpg} % second figure itself
    \end{minipage}
    \caption{\small{LoRAs (top) vs. EigenLoRAx (bottom) in Text-to-Image generation. (Left) A single EigenLoRAx analytically reconstructs multiple LoRAs, significantly reducing memory (18$\times$ reduction) and compute costs. (Right) It efficiently learns new tasks with up to 100$\times$ fewer parameters than LoRA, maintaining similar visual quality. See \cref{appendix:diffusion} for more examples.}}
    \label{fig:diffusion_joint}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%
\textbf{EigenLoRAx nearly matches LoRA with $12 - 95\times$ fewer parameters}. EigenLoRAx recovers upto 88\% of LoRA's performance even in a zero-shot setting at such a large scale. The performance of EigenLoRAx can be improved by fine-tuning the EigenLoRAx adapters. In this setting we use randomized SVD in order to speed up the calculation of the PCs. We believe this leads to some degradation in performance as there randomized methods are approximations of the actual calculations. Performance can be further improved if better implementations of SVD which do not sacrifice accuracy for speed are used in calculating the Principal Components.
%%%%%%%%%%%%%%%%%
\subsection{Text-to-Image Image Generative Models}
\label{sec:diffusion}
We showcase EigenLoRAx's versatility on complex multimodal tasks like text-to-image generation, where LoRAs are extensively used to adapt models like Stable Diffusion to various styles and datasets. Despite thousands of LoRA adapters being available, most remain underutilized, occupying significant memory alongside their data. As adapter usage grows, a critical challenge is efficiently hosting multiple adapters for diverse tasks, especially on edge devices. Switching adapters during inference, often from CPU memory or disk, introduces latency that hinders real-time applications. EigenLoRAx tackles this by extracting a shared task-invariant subspace, significantly reducing in-memory parameters and enabling memory-efficient inference without compromising flexibility or performance.
EigenLoRAx can effectively replace pretrained adapters, drastically reducing storage requirements. To demonstrate this, we extracted $K=14$ principal components from $N=20$ Stable Diffusion-XL~\citep{sdxl} LoRA adapters (rank $r=32$) from the HuggingFace diffusers library~\citep{von-platen-etal-2022-diffusers}. Using $\alpha \in \mathbb{R}^{r \times K}$, we analytically reconstructed the original LoRA weights within the extracted principal subspace. For image generation, we used 30 denoising steps with a fixed seed of 0. Results and comparisons are shown in \cref{fig:diffusion_joint}. This approach reduces storage requirements for all adapters from 4.6GB to just 261MB, achieving an \textbf{18$\times$ reduction in low-rank parameters stored in memory}. By enabling a large number of adapters to reside in VRAM simultaneously, EigenLoRAx eliminates I/O bottlenecks, significantly improving memory efficiency for real-time applications.
\begin{figure*}[!hbt]
\begin{center}
\includegraphics[width=0.9\textwidth]{images/fail_diffuse.jpg}
\end{center}
\caption{Failure Case: EigenLoRAx may fail if an important component is missing from the initialized subspace i.e. the shared subspace is incomplete, which may happen due to inadequacy in the number of initial adapters or due to the majority of the adapters being of bad quality. E.g., the model may have lost the essential "mosaic" property when generating an image for the prompt: "mosaic picture of a dog."}
\label{fig:failurediffusion}
\end{figure*}
\textbf{Failure Cases and Limitations} Despite its advantages, EigenLoRAx has limitations. Figure~\ref{fig:failurediffusion} shows a failure case where the method fails to capture a key property of the desired image. While tasks may share a principal subspace, missing critical orthogonal components can degrade performance, especially if they were absent in the pretrained LoRAs used for extraction or if the chosen top $K$ components were suboptimal. In the latter case, empirical analysis of hyperparameters (\cref{sec:ablation}) can guide optimal $K$ selection. Additionally, our subspace augmentation method (\cref{tab:glue_low}) helps by iteratively sampling and orthogonalizing more components to recover missing subspace elements. A simple extension can further mitigate this issue by allowing a small number of rank-1 weights to be trainable outside the subspace. Another key limitation (\cref{sec:lotsofloras}) is the computational cost and instability of processing a large number of initial LoRAs. A continual learning approach building on our method could address this. Finally, our experiments did not explore layer-wise or weight matrix-level optimizations; we tested different $K$ values but kept them fixed across layers and for both A and B matrices. Additional failure cases are discussed in \cref{ssec:failure_append}.
