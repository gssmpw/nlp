Low-Rank Adaptation refers to modeling neural network weight updates as a function of low-rank matrices instead of training the entire weight matrix.
This is a well-established line of research starting from Burer-Monteiro factorization~\citep{Burer2003ANP}, with a recent resurgence by~\citet{hu2021lora} (LoRA), who used it as a technique to finetune LLMs; and other related variants ~\citep{Ma2024, chi19_low, kwon2024efficientcompressionoverparameterizeddeep}. 
However, with rapid growth in the scale of models, Low-Rank Adaptation has also become relatively expensive; for example, LoRA with a rank of 16 on GPT-3~\cite{browngpt3} requires 75.5 million parameters. Consequently, more efficient low-rank fine-tuning methods are being developed. Mixture of experts models~\citep{huang2023lorahub, wu2024mixture, diao_mixture--domain-adapters_2023, zhong2024multi, zhou2018x} have been proposed as a method to adapt to new domains using a mixture of low-rank modules. But these approaches typically require a substantial number of high-quality adapters to work efficiently~\citep{acl-2024-long}, which can significantly increase the model memory requirements~\citep{zhou2022mixtureofexperts}. Furthermore, complex gating or weighting mechanisms utilized with these models can exhibit training instability~\citep{zoph2022stmoedesigningstabletransferable}. 

Recent methods have aimed to learn better subspaces for low-rank optimization, primarily by decomposing model weights into singular vectors for improved training. \citet{meng_pissa_2024} demonstrate that initializing LoRA with singular vectors is superior to random initialization, while ~\citet{sharma_laser_2023} find that removing minor singular components enhances robustness. Using randomly initialized principal components~\citep{kopiczko_vera_2023} or weight matrices~\citep{nola} has also been explored to reduce the number of trainable parameters. However, as shown in Section~\ref{sec:experiments}, random initialized subspaces may not be very useful. This is intuitive as the random subspace may not have an overlap with domain-specific principal subspaces. On the other hand, EigenLoRAx uses trained adapters to extract a \textit{principal subspace} suitable for a given domain of tasks resulting in a better subspace initialization than and parameter efficiency. 
Given our focus on resource and computation efficiency in this work, we focus primarily on LoRA~\citep{hu2021lora} as our main baseline, but EigenLoRAx can be used with any PEFT method like~\cite{dora, zhang2023adaloraadaptivebudgetallocation} where task-specific weights can be analyzed together. 