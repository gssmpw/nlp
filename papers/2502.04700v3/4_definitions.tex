In this section, we present the theoretical foundation~\cref{sec:theory} and algorithmic details~\cref{sec:algo} of our method, followed by a discussion on hyperparameter selection and an assessment of its practical advantages. Note that we use the terms EigenLoRA, EigenLoRAx, ELoRA and ELoRAx interchangeably.

\subsection{Theoretical Motivation}
\label{sec:theory}

For a full rank weight matrix $W \in \mathbb{R}^{m \times n} $ that learns to map input space $X\in\mathbb{R}^{m}$ to output space $\mathbb{R}^{n}$, the rank is expressed as $\min(m,n)$. As the rank of $ W $ increases, modifying it to accommodate new tasks becomes computationally expensive and increasingly complex. This is a common challenge faced when finetuning pretrained large foundation models. LoRA is a parameter efficient finetuning approach used for large pretrained models with weights $W_0$ that mitigates this challenge by merely learning low-rank weight updates $W$ such that the risk between $Y$ and $W_0X + WX + b$ is minimized. Instead of directly learning $W$, LoRA proposes to learn a lower ranked decomposition of $W$ by learning two low-rank matrices, $ B \in \mathbb{R}^{m \times r} $ and $ A \in \mathbb{R}^{r \times n} $, both having ranks $r$.  This factorization ensures that the product $ BA $ retains the original dimensions of $ W_0 $ while having a significantly reduced rank.
As a result, although the transformation defined by $ BA $ maps from $ \mathbb{R}^m $ to $ \mathbb{R}^n $, it does not span the full space of such mappings due to its constrained rank. The low-rank weight matrices result in substantially smaller number of trainable parameters than the full rank parameter count of $m\cdot n$. Such parameter efficient finetuning makes LoRA a computationally viable alternative for fine-tuning large-scale models.    

Previous works such as \cite{meng_pissa_2024, dora} have proposed the existence of a common parameter subspace implying the idea of shared principal subspace. We highlight that LoRA adapters share such a lower dimensional shared principal subspace when finetuned for diverse tasks. Along with reduction in computational overhead, it reinforces the idea that task-relevant transformations reside within a compact, reusable subspace. To formalize this, we first define a space of tasks representable by linear transformation matrices, providing a foundation for analyzing the role of shared principal subspaces in model adaptation.

\begin{definition}[Task definition for LoRAs] 
\label{def:lineartasks}
We first define a LoRA task $t_i (X_i,Y_i): \mathbb{R}^m \rightarrow \mathbb{R}^n$ such that $Y_i = W^*_iX_i + b$ where $b$ is some constant. Then the LoRA task domain $\mathcal{T}_d$ is a set of $d$ such tasks, $\mathcal{T}_d = \{t_i\}^{d}_{i=1}$.
\end{definition}


For a given set of pretrained weights (such as those from a foundation model) $W_0\in\mathbb{R}^{m\times n}$, LoRA weights $BA$ at any layer modify the output as $W_0X + BAX + \epsilon_t$, allowing the model to adapt to the new task and converge toward the optimal solution $W^*_t$. The key here is that only $B$ and $A$ weights are updated during finetuning. 
Without loss of generality, 
% \prakhar{can we write the full form here}\aayush{its a common term in math}
assume $r\ll n$ and let the true transformation matrix $W^*_t \in \mathbb{R}^{r \times n}$ be interpreted as $r$ $n$-dimensional vectors: $\mathbf{w}^{*1}_t, ..., \mathbf{w}^{*r}_t \in \mathbb{R}^{n}$. Finding LoRA weights is equivalent to finding sets of these $r$ vectors in $\mathbb{R}^{n}$. 

\begin{definition}[Set of LoRA weights]
\label{def:loraset}
We define the weights of a LoRA adapted for task $t_i$ as $B_iA_i$. Both $B_i$ and $A_i$ will have their own individual subspaces. For the purpose of the analysis we will consider a generic task specific weight matrix $W_i \in \mathbb{R}^{m\times n}$ adapted to task $t_i$ such that $n<m$ and its rank $r<n$. The analysis, however, is valid for both $B_i$ and $A_i$. 
Now can define a set of LoRAs as stacked (along columns) weight matrices $\hat{W} = \{W_i \}^{d}_{i=1}$ where each $W_i$ is adapted for a task $t_i\in\mathcal{T}_d$ and a training set $\mathcal{S}_i=\{\{x,y\} \mid x\in X_t, y\in Y_t\}$ where the size of the training is $s_i = |\mathcal{S}_i|$. For theoretical analysis we assume that each training set $X_i \times Y_i$ is distributed according to some unknown Gaussian distribution with mean $\Bar{X_i}$ and $\lVert X_i\lVert_F\leq M$ for some constant $M>0$. Each weight matrix can have different ranks and the following method and analysis will still hold, however, for brevity we assume all weight matrices stacked in $\hat{W}$ to have the same rank $r$. 
% Next, we define a subspace in $\mathbb{R}^{n}$ spanned by $d$ LoRAs learnt for $d$ tasks from $\mathcal{T}_d$.
\end{definition}
\begin{definition}[Subspace spanned by LoRAs from a task domain $\mathcal{T}_{d}$]
\label{def:subspace}
We define the subspace of weights $\mathcal{Z}_{d} = \{C\hat{W} \mid C\in\mathbb{R}^{m\times m}\}$ spanned within $\mathbb{R}^{m\times n}$. 
\end{definition}
Using Singular Value Decomposition (SVD) or Principal Component Analysis (PCA for a zero-centered $\hat{W}$) , we can obtain $\hat{W}=\mathcal{U}\Sigma \mathcal{V}^T$. We then represent top $K$ right singular vectors of $\hat{W}$ (or top $K$ principal components if $\hat{W}$ is zero-centered) as $\Vk \in\mathbb{R}^{K\times n}=\{\Vk\in\mathbb{R}^{1\times n}\}^K_{k=1}$.
\begin{definition}[Shared principal subspace of LoRAs finetuned in domain $\mathcal{T}_d$]
\label{def:principalsubspace}
We define the shared principal subspace of weights for a task domain $\mathcal{T}_d$ as $\mathcal{Z}^{K}_d = \{\alpha \Vk \mid \alpha\in\mathbb{R}^{m\times K}\}$ spanned by top K principal components of the LoRAs within $\mathbb{R}^{m\times n}$. 
\end{definition}

Next, we introduce the idea of defining a new related task $\tnew$
\begin{definition}[New related task $\tnew$].
\label{def:newtask}
    A new linear task $\tnew$ with true solution $\Wtrue$ is said to be related if it is spanned by the basis of $\hat{W}$ i.e. $\Wtrue = C\hat{W}$ and it holds that $\lVert \Wtrue - \alphatrue\Vk\lVert^2_F \leq \Vert\Wtrue - \alphalearnt\Vk\lVert^2_F$ for all rank $K$ linear transformation matrices $\alphalearnt$ and $\lVert \Wtrue - \alphatrue\Vk\lVert^2_F \leq \singularsum$ where $\sigma_i$'s are singular values of $\hat{W}$. For such a task, we learn coefficients of $K$ principal components $\alphalearnt \in \mathbb{R}^{m\times K}$ resulting in EigenLoRAx weights $\Wegn = \alphalearnt\Vk$. \\
\end{definition}

Definition \ref{def:newtask} establishes a bound over the related-ness of a new task with those in the known task domain $\mathcal{T}_d$. If the true solution of the new task lies majorly in the principal subspace of $\mathcal{T}_d$ i.e. has major principal components (PCs) within the top $K$ principal components of $\hat{W}$ with some finite bound on the misalignment along the PCs orthogonal to the top $K$ PCs of $\hat{W}$, then we can ideally quantify the relation between a new task and a task domain. Any task that has its true solution within a subspace defined by the PCs orthogonal to the top $K$ PCs of $\hat{W}$ is not as closely related as a task with its solution completely or majorly within the principal subspace. A task that has its solution completely orthogonal to all the PCs of $\hat{W}$ is completely unrelated and is not the main focus of this study. 

Next, we present an algorithm to find the principal subspace of the trained adapters and our experiments in Section~\ref{sec:experiments}.


\begin{algorithm}[!htb]
\caption{\textbf{EigenLoRAx PCs} Calculation}
\label{algo:eigenlora}
\begin{algorithmic}
    \STATE {\bfseries Input:} 
    LoRA matrices 
    $\{W_t \in \mathbb{R}^{m \times n}\}_{t=1}^d$ 
    
     , number of PC ($K$), number of pseudo-PC ($P$)\\
    
    \STATE {\bfseries Output:} EigenLoRAx PCs $\Vk$

    \STATE $\hat{W} = \begin{bmatrix*} W_{1}\in \mathbb{R}^{m\times n} & \text{...} & W_{d}\in \mathbb{R}^{m\times n}\end{bmatrix*}$, \COMMENT{Stack LoRA matrices}\\
    
    \STATE Compute the mean of each feature: $ \Bar{W} = \frac{1}{n} \sum_{i=1}^{n} W_i $
    \STATE Subtract the mean: $ \hat{W}_c = \hat{W} - \Bar{W} $
    
   
    \STATE Perform SVD: $ \hat{W}_c = U \Sigma V^T $
    
    \STATE Extract the top $ K $ principal components
    \STATE Select the first $ K $ columns of $ \mathcal{V} $: $ \V = V[:, 1:K] $
    
    \STATE Optionally, augment the subspace with $P$ pseudo-PCs
    \FOR{$ p = 1 $ to $ P $}
        \STATE Sample a random vector $ v_p \sim \mathcal{N}(0, I_n) $  \COMMENT{Sample from a normal distribution}
        \STATE Orthogonalize $ v_p $ against all PCs in $ \V $ using Gram-Schmidt:
        \FOR{$ i = 1 $ to $ K+p-1 $}
            \STATE $ v_p = v_p - \frac{v_p^T V_K[:, i]}{\|\V[:, i]\|^2} \V[:, i] $
        \ENDFOR
        \STATE Normalize $ v_p $: $ v_p = \frac{v_p}{\|v_p\|} $
        \STATE Append $ v_p $ to $ \V $ if $ v_p $ is not a null vector
        \STATE $K = K + 1$
    \ENDFOR

    \STATE return $\V, \mu$
    
\end{algorithmic}
\end{algorithm}

\subsection{Algorithm}
\label{sec:algo}

Assume we have $N$ LoRA adapters, each consisting of a set of $A, B$ matrix pairs for every layer, trained on various tasks within a domain $\mathcal{T}_d$ for a given base pretrained model. Algorithm~\ref{algo:eigenlora} computes a list of top $K$ principal components—referred to as EigenLoRAx PCs—that define an initial principal subspace for this domain. 

To construct this subspace, the algorithm aggregates LoRA matrices across tasks for each layer, separately for $A$ and $B$ matrices (though it can also be applied to the product $BA$). Each LoRA matrix, having rank r, is treated as a list of vectors, and a decomposition is performed on this stacked set of vectors. The most significant components extracted from this process serve as a basis for the principal subspace, providing an efficient representation that can be linearly combined to approximate the original LoRA weight matrices. We showcase our algorithm using representative weight matrices $W_t$, where each $W_t$ represents a single $A$ or $B$ matrix from a single LoRA layer of the neural network. In practice, this procedure is applied to all relevant layers.

Since real-world scenarios often involve low-resource domains with limited availability of LoRA adapters, we extend our subspace by introducing additional pseudo-PCs. Specifically, we sample random vectors of the same dimension as each PC and orthogonalize them with respect to all existing PCs. This process can be iterated to generate more pseudo-PCs, thereby augmenting the principal subspace. As empirically shown in Table~\ref{tab:glue_low}, this augmentation strategy significantly outperforms naive random selection of PCs for subspace expansion.

\paragraph{Learning new tasks}\label{method:learningnewelora} Having extracted a set of PCs (including pseudo-PCs, if needed), $\mathcal{V}_K \in\mathbb{R}^{K\times n}=\{\V\in\mathbb{R}^{1\times n}\}^K_{k=1}$, we can approximate a given (LoRA) weight matrix by minimizing $\lVert W - \alpha\Vk\lVert_F$ where $\alpha$ are linear coefficients~\cref{sec:theory}. In fact, we can analytically compute of the given LoRA matrices by calculating the linear coefficients which minimizes the above objective. For new tasks however, for which we do not have a LoRA matrix, we freeze the EigenLoRAx PCs and randomly initialize the $\alpha$s. The forward pass in layer is calculated as 
\begin{align}
    h = W_0x + \dboxed{\alpha_B^T \mathcal{V}_B \alpha_A^T \mathcal{V}_A(x)}.
\end{align}
Here, $W_0$ are the pretrained weights of the base model and $\mathcal{V}_B, \mathcal{V}_A$ are EigenLoRAx components (which represent the shared subspace) that are frozen during training. The corresponding lightweight coefficients $\alpha_B$ and $\alpha_A$ are learned. This reduces the number of learnable parameters from $O(2rn)$ to $O(2K)$, by a factor of $\frac{rn}{K}$ (assuming $\alpha$ to be scalar). 

\phantomsection

