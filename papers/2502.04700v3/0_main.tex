\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{dashbox}
\newcommand\dboxed[1]{\dbox{\ensuremath{#1}}}
%%%%%%%%%%%%
% \usepackage{hyperref}
\usepackage{url}
\usepackage[english]{babel}
\usepackage{ulem}
\usepackage{wrapfig}
\usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{xcolor}         % colors
% \definecolor{lightblue}{rgb}{0.68, 0.85, 0.9}
% \usepackage{amsmath}
% \usepackage[pdftex]{graphicx}
\usepackage{soul}
\usepackage{multirow}
\usepackage{colortbl}
\sethlcolor{LightBlue2}
\usepackage{mathrsfs}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{resizegather}
\newcommand{\new}{\marginpar{NEW}}
\usepackage{placeins}
\usepackage{subcaption}
\usepackage{adjustbox}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
% \pdfobjcompresslevel=2

\newcommand{\del}[1]{{\color{red}[#1]}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\input{paper_macros}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{EigenLoRAx}

\begin{document}

\twocolumn[
% \icmltitle{EigenAug: Enhancing Resource-Efficient Adaptation with Augmented Principal Subspaces}
\icmltitle{EigenLoRAx: Recycling Adapters to Find Principal Subspaces for Resource-Efficient Adaptation and Inference}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Prakhar Kaushik*}{yyy}
\icmlauthor{Ankit Vaidya*}{yyy}
\icmlauthor{Shravan Chaudhari}{yyy}
\icmlauthor{Alan Yuille}{yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Computer Science, Johns Hopkins University, Baltimore, USA}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Prakhar Kaushik}{pkaushi1@jh.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\input{0_abstract}
\end{abstract}

\section{Introduction}
Recent advancements in machine learning have driven the rise of large-scale models with billions of parameters. However, the size and complexity of these models not only make it impractical for most researchers to train or fine-tune them on downstream tasks but also contribute significantly to their carbon footprint, raising concerns about environmental sustainability.
To address these challenges, there has been growing interest in parameter-efficient finetuning (PEFT) methods, such as adapters~\citep{pmlr-v97-houlsby19a, Chen2022AdaptFormerAV, luo2023towards}, low rank adaptation (LoRA) methods~\citep{hu2021lora,kopiczko_vera_2023,dora}, prompt-based methods~\citep{tune1, tune2, tune3}.
LoRA and its follow-up works~\citep{meng_pissa_2024,dora} have gained significant attention for their simplicity. This has fueled the proliferation of thousands of low-rank adapters within the growing open-source community.
Given that these adapters are underutilized, an important question arises: Can we recycle the information contained in them to improve the efficiency of subsequent tasks?
Recent work has shown that weight updates in deep neural networks occur within low-dimensional invariant subspaces~\citep{kwon2024efficientcompressionoverparameterizeddeep}, aligning with the universality hypothesis that neural network behavior and learned representations often reside in shared, structured subspaces~\cite{chughtai2023toymodeluniversalityreverse, guth2024on}. This suggests that LoRA adapters may similarly share a \textit{principal subspace} that can be reused, eliminating the need to rediscover it during the training of new adapters.

We introduce \textbf{EigenLoRAx}, a parameter-efficient fine-tuning (PEFT) method that leverages this insight by decomposing the weights of a set of trained adapters into principal components, identifying a compact, information-dense subspace. EigenLoRAx reduces the number of learnable parameters by up to $\mathbf{100\times}$ compared to LoRA, accelerates optimization by up to $\mathbf{2\times}$ for new adapters, and enables more memory-efficient inference with multiple task adapters, particularly benefiting edge devices~\citep{edge}. Additionally, in low-resource domains, we demonstrate that EigenLoRAx can be further enhanced by augmenting the principal subspace with random components, orthogonalized with respect to the existing subspace, preserving its efficiency while retaining performance.

Furthermore, we provide an initial theoretical analysis of EigenLoRAx.
Our experiments across a wide range of vision and language tasks demonstrate its versatility and effectiveness, reinforcing the potential of shared subspaces in neural network adaptation.

\autoref{fig:fig1} provides an overview of our method. We introduce \textbf{EigenLoRAx} (ELoRAx), which recycles pretrained adapters by identifying a shared \textit{task-invariant} weight subspace. We hypothesize (and validate experimentally) that task-specific weights lie within this subspace, allowing for more efficient training with fewer parameters. This reduces memory footprint and enhances inference efficiency by enabling simultaneous serving of multiple adapters. EigenLoRAx is among the first to recycle pretrained adapters, replacing many while improving further training efficiency. Our key contributions are as follows:

\begin{itemize}
    \item \textbf{(Training)}: EigenLoRAx uses up to $\mathbf{100\times}$ \textbf{fewer parameters than LoRA} and converges up to $\mathbf{2\times}$ \textbf{faster} than comparable methods, achieving similar or better performance.
    \item \textbf{(Inference)}: EigenLoRAx enhances \textbf{memory efficiency during inference} by approximately $\mathbf{18\times}$ on multiple tasks, reducing the number of switchable parameters between tasks.
    \item \textbf{(Applicability)}: We empirically demonstrate the effectiveness of EigenLoRAx across a wide range 
    , including text and image data, validating the existence of shared principal subspaces across modalities. It also retains performance in \textbf{zero-shot} and \textbf{low resource} scenarios.
    \item \textbf{(Scaling)}: EigenLoRAx can be scaled up to recycle hundreds of underutilized pretrained adapters.
\end{itemize}
\begin{figure*}[!hbt]
\begin{center}
\includegraphics[width=\textwidth]{images/elorax.pdf}
\caption{\small{LoRA uses low-rank matrices for task-specific finetuning. We observe that LoRA adapters share a principal subspace across task domains. By recycling pretrained adapters, we extract \textit{task-invariant} principal components, enabling efficient representation of both existing and future LoRAs using compact \textit{task-specific} coefficients. This improves training speed, parameter efficiency, and memory usage. In low-resource settings, where pretrained adapters are scarce, we augment the subspace with randomly initialized components, ensuring orthogonality via the Gram-Schmidt process, ensuring they complement the extracted subspace without redundancy.}}
\label{fig:fig1}
\end{center}
\end{figure*}
\section{Related Works}
\input{2_relatedwork}

\section{Method}
\input{4_definitions}
\input{4_method}

\section{Experiments and Results}\label{sec:experiments}
In this section, we demonstrate the efficacy and versatility of EigenLoRAx across diverse tasks, modalities, and model architectures, highlighting its individual advantages. EigenLoRAx requires significantly fewer parameters to match or surpass LoRA’s performance (Tables~\ref{tab:vision_models}, \ref{tab:glue_benchmark_results}) and achieves similar or faster loss convergence (Figure~\ref{fig:traininglosscola}), making it a cost-effective alternative to random initialization and other methods~\citep{meng_pissa_2024}. Additionally, we showcase its memory-efficient inference capabilities with a Stable Diffusion text-to-image generation model~\citep{diffusion} (Section~\ref{sec:diffusion}). Notably, EigenLoRAx retains its efficiency even in low-resource scenarios where a large number of LoRAs are unavailable.

\paragraph{Note on Baselines} Our focus is on recycling adapter knowledge and improving training and memory efficiency while maintaining performance, not solely on maximizing performance. We compare primarily with LoRA, as EigenLoRAx builds its principal subspace using pretrained LoRA adapters. Using better adapters and optimization could further enhance the subspace and performance. 

Due to lack of space, more experiments (3D Object pose estimation) and detailed ablation experiments are presented in \cref{sec:appendix}. 

\input{3_experiments}

\section{Conclusion}

We introduce EigenLoRAx, a significantly efficient model finetuning and inference method that recycles publicly available pretrained adapters by finding a shared principal subspace. This allows finetuning on new data by simply learning the lightweight coefficients of the shared subspace, and also requires less number of parameters to be saved for new tasks. Our comprehensive and diverse experiments show that EigenLoRAx is applicable to a large range of problems and model architectures. We believe that EigenLoRAx has the potential to mitigate the perpetually widening compute resource gap~\citep{Ahmed2020TheDO, Besiroglu2024TheCD} and reduce the environmental cost of training and using machine learning models~\citep{Wu2021SustainableAE, Ligozat2021UnravelingTH}. It also holds promise for training personalized models~\citep{tan2024democratizing} on low-resource devices, in privacy-critical use-cases. We have a large number of experiments ($6+$) on a diverse set of complex models, tasks and modalities. We have shown that EigenLoRAx excels in faster and efficient learning, memory savings and zero shot performance which differentiates it from conventional PeFT models. While our work is application-focused, we believe it is the first work to hypothesize and empirically prove the existence of shared weight subspaces of neural networks. This important insight has significant implications of model merging, efficiency, mechanistic interpretability, and neural learning theory.

{\small\paragraph{A Note on Prior Work} This paper works on some ideas initially introduced in the EigenLoRA work~\cite{kaushik2025eigenlora}, which was rejected from ICLR'25 and remains available on OpenReview. Due to irreconcilable differences regarding research ethics and authorship integrity, the primary authors of this work chose to independently develop and extend their contributions. These contributions in the previous draft included the initial algorithm, all experimental results (excluding Section 4.2.2), as well as all ablation studies and additional experiments. The code used for these experiments was also developed solely by the first authors of this work and is publicly available. Since the prior work was neither accepted nor published, there are no concerns regarding self-plagiarism. We also acknowledge discussions and exchanges with some of the earlier authors, which, while not directly contributing to the technical developments of this work, provided context and background that shaped our understanding of the broader problem. We have taken extensive measures to ensure that this paper exclusively reflects the contributions of the authors listed, and any similarities beyond our own contributions are purely coincidental.}

\nocite{sun2025transformersquaredselfadaptivellms, Gain_2020_WACV, Kaushik_2024_CVPR}
\bibliography{iclr2025_conference}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{8_appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
