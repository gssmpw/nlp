\section{Related works}
\subsection{Human Multimodal Representations}

Multimodal representation learning is pivotal for human-centric analyses, especially in tasks requiring spatial-temporal reasoning to interpret complex behaviors**Parikh, "Video-LLaVA"**. Recent advancements, such as Video-LLaVA, integrate visual information from images and videos into a unified linguistic feature space, enabling improved visual reasoning for behavioral analysis**Sener, "A Temporally Implemenation of Human Action Recognition using LSTM"**. However, many models remain limited to isolated video frames and privacy concerns, constraining their effectiveness in the dynamic real world**Hou, "A Temporal Attention-based Lattice Framework for Human-Action Recognition"**. To address these limitations, motion data has emerged as a privacy-preserving alternative, allowing action analysis without revealing identifiable visual details**Xu, "Motion-Segmentation: A Unified Method for Action Detection and Tracking by Motion Segmentation"**. By combining visual and motion data, emerging multimodal frameworks offer comprehensive, privacy-aware solutions, leveraging the complementary strengths of both modalities for enhanced adaptability across diverse applications.

\subsection{Human Motion Understanding}

Human motion analysis traditionally relies on skeletal data, represented as joint keypoint sequences, to capture movement dynamics while preserving user privacy**Liu, "A Skeleton-based Approach to Human Action Recognition"**. Early methods, such as 2s-AGCN, and recent transformer-based models like MotionCLIP have demonstrated success in tasks such as activity recognition, caption generation, and behavior analysis by translating motion data into language tokens**Hou, "A Temporal Attention-based Lattice Framework for Human-Action Recognition"**. While effective in modeling structural movement patterns, these approaches often neglect environmental context, which is crucial for interpreting motions that may convey different meanings based on situational factors**Xu, "Motion-Segmentation: A Unified Method for Action Detection and Tracking by Motion Segmentation"**. To address this, recent models integrate motion and visual data, enabling improved generalization in dynamic and diverse environments**Sener, "A Temporally Implemenation of Human Action Recognition using LSTM"**. Frameworks like LLaMo have further advanced the field by incorporating motion encoders, estimators, and efficient fusion mechanisms, achieving state-of-the-art results in both general and specialized motion analysis.

\vspace{-0.10in}
\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{latex/figures/MotionCore8.png} %
\vspace{-0.20in}
\caption{Components of MotionCore: the MotionCore integrates the MotionAnalyzer and Selection modules to concurrently process and aggregate multiple human motion analyses in two specific ways. The Generation Module synthesizes and contextualizes the results to align with user queries. Additionally, an auxiliary toolbox enables dynamic expansion with supplementary tools to address evolving user requirements.}
\label{MotionCore}
\vspace{-0.15in}
\end{figure}