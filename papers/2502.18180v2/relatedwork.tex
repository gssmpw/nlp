\section{Related works}
\subsection{Human Multimodal Representations}

Multimodal representation learning is pivotal for human-centric analyses, especially in tasks requiring spatial-temporal reasoning to interpret complex behaviors~\cite{lin2023videollm, ning2023videobench, li2023videochat}. Recent advancements, such as Video-LLaVA, integrate visual information from images and videos into a unified linguistic feature space, enabling improved visual reasoning for behavioral analysis~\cite{lin2023videollm}. However, many models remain limited to isolated video frames and privacy concerns, constraining their effectiveness in the dynamic real world.~\cite{ning2023videobench, heilbron2015activitynet, maaz2023video}. To address these limitations, motion data has emerged as a privacy-preserving alternative, allowing action analysis without revealing identifiable visual details~\cite{song2023adaptive, yang2023recognizing}. By combining visual and motion data, emerging multimodal frameworks offer comprehensive, privacy-aware solutions, leveraging the complementary strengths of both modalities for enhanced adaptability across diverse applications.

\subsection{Human Motion Understanding}

Human motion analysis traditionally relies on skeletal data, represented as joint keypoint sequences, to capture movement dynamics while preserving user privacy~\cite{shi2023learning, plappert2018bidirectional, yang2023understanding}. Early methods, such as 2s-AGCN~\cite{shi2019two}, and recent transformer-based models like MotionCLIP~\cite{chen2023motiongpt}, have demonstrated success in tasks such as activity recognition, caption generation, and behavior analysis by translating motion data into language tokens. While effective in modeling structural movement patterns, these approaches often neglect environmental context, which is crucial for interpreting motions that may convey different meanings based on situational factors~\cite{song2023finegrained, maaz2023video}. To address this, recent models integrate motion and visual data, enabling improved generalization in dynamic and diverse environments~\cite{liu2024pose, he2023activitynet}. Frameworks like LLaMo\cite{li2024human} have further advanced the field by incorporating motion encoders, estimators, and efficient fusion mechanisms, achieving state-of-the-art results in both general and specialized motion analysis.
% However, these systems focus exclusively on motion evaluation, thereby limiting their applicability in scenarios requiring flexible and comprehensive analyses driven by user intent, and consequently constraining their utility in complex, user-driven contexts.

\vspace{-0.10in}
\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{latex/figures/MotionCore8.png} %
\vspace{-0.20in}
\caption{Components of MotionCore: the MotionCore integrates the MotionAnalyzer and Selection modules to concurrently process and aggregate multiple human motion analyses in two specific ways. The Generation Module synthesizes and contextualizes the results to align with user queries. Additionally, an auxiliary toolbox enables dynamic expansion with supplementary tools to address evolving user requirements.}
\label{MotionCore}
\vspace{-0.15in}
\end{figure}