% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{color}
\newcommand{\lei}[1]{\textcolor{red}{{\bf L:} #1}}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{amsmath}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{ChatMotion: A Multimodal Multi-Agent for Human Motion Analysis}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\makeatletter
\def\thanks#1{\protected@xdef\@thanks{\@thanks
        \protect\footnotetext{#1}}}
\makeatother

\author{\textnormal{Lei Li\textsuperscript{\rm 1, 2, *, \dag}, 
Sen Jia\textsuperscript{\rm 3, *}, 
Jianhao Wang\textsuperscript{\rm 4}, 
Zhaochong An\textsuperscript{\rm 1}, }\\
Jiaang Li\textsuperscript{\rm 1}, 
Jenq-Neng Hwang\textsuperscript{\rm 2}, 
Serge Belongie\textsuperscript{\rm 1}
\thanks{\(^{*}\) These authors contributed equally to this work.} \thanks{\(^{\dag}\) Corresponding Author. (\href{mailto:lilei@di.ku.dk}{\color{black}{\texttt{lilei@di.ku.dk}}}) \textsuperscript{\rm 1} University of Copenhagen \textsuperscript{\rm 2} University of Washington \textsuperscript{\rm 3} Shandong University \textsuperscript{\rm 4} Xi'an Jiaotong University
}}

\maketitle
\begin{abstract}
% (Large Language and Human Motion Omni) for next work

Advancements in Multimodal Large Language Models (MLLMs) have improved human motion understanding. However, these models remain constrained by their "instruct-only" nature, lacking interactivity and adaptability for diverse analytical perspectives. To address these challenges, we introduce ChatMotion, a multimodal multi-agent framework for human motion analysis. ChatMotion dynamically interprets user intent, decomposes complex tasks into meta-tasks, and activates specialized function modules for motion comprehension. It integrates multiple specialized modules, such as the MotionCore, to analyze human motion from various perspectives. Extensive experiments demonstrate ChatMotion's precision, adaptability, and user engagement for human motion understanding.


% By utilizing multiple models, it offers a comprehensive analysis from different perspectives, with intermediate results refined through a scoring mechanism. This approach ensures high-quality outputs, mitigates hallucinations, and improves accuracy. Empirical evaluations show that ChatMotion outperforms current MLLMs in precision, adaptability, and user engagement, highlighting the value of multi-agent systems in human motion understanding field.


% Advancements in Multimodal Large Language Models (MLLMs) have driven significant progress in the field of human motion understanding. However, traditional MLLMs are often constrained as “instruct-only” systems, lacking robust user interactivity and the adaptability necessary to integrate diverse analytical perspectives. To address these limitations, we introduce \textbf{ChatMotion}, a general multi-agent framework designed for motion analysis. This framework is capable of interpreting user intent, decomposing comprehensive tasks into meta-tasks, and dynamically invoking function modules to mitigate biases in motion understanding. By leveraging multiple motion understanding models, ChatMotion enables a comprehensive analysis from various perspectives. Intermediate results are aggregated and refined through a scoring mechanism, ensuring high-quality outputs. Furthermore, the framework supervises its analytical workflow, continuously aligning with user requirements while facilitating iterative dialogues to adjust outputs according to evolving goals. This design effectively mitigates hallucinations and enhances inference accuracy. Empirical evaluations across a range of motion-to-text tasks demonstrate that ChatMotion significantly improves precision, adaptability, and user engagement compared to state-of-the-art motion understanding MLLMs. These findings underscore the potential of multi-agent architectures in providing richer, more interactive, and context-sensitive solutions for human motion understanding.


\end{abstract}

\section{Introduction}

% Human-centric motion understanding has attracted increasing attention due to its extensive applications in robotics, human--machine interaction and professional sports. In particular, the comprehensive analysis for complex sports motions, i.e., multiple golf swings comparison, visualization of the elbow angle during swimming, has become crucial for athletes seeking performance improvements. Such analyses demand systems capable of interpreting user intentions and embedded function-calling modules. Consequently, there is a growing need for AI agents designed to perform comprehensive analyses in human-centric motion sports.


% Human-centric motion understanding has attracted increasing attention due to its extensive applications in digital human, human-computer interaction, sports analytics, healthcare, and virtual human modeling~\cite{panagov2023human,plappert2016kit, zhang2021we, hong2022versatile,qu2024llms}. In particular, the comprehensive analysis for complex sports motions, i.e., multiple golf swings comparison, visualization of the elbow angle during swimming, has become crucial for athletes seeking performance improvements~\cite{raza2023eatsense}. Such analyses demand systems capable of interpreting user intentions and embedded function-calling modules~\cite{wang2023hulk, xu2023human}. Consequently, there is a growing need for AI agents designed to perform comprehensive analyses in human-centric motion sports~\cite{fowler2018human, aloimonos2010language}.

Human motion understanding~\cite{li2024human,zhou2024efficient,loper2023smpl,jiang2024back,jiang2023unihpe} has gained attention due to its wide-ranging applications in fields such as healthcare, human-computer interaction, rehabilitation, sports science, and virtual human modeling~\cite{plappert2016kit, zhang2021we, hong2022versatile, qu2024llms}. A deep understanding of human motion can drive advancements in areas like physical therapy~\cite{smeddinck2020human}, immersive virtual experiences~\cite{xiao2024study}, and assistive technology interfaces~\cite{khiabani2021semg}. As human motion data becomes more accessible, the demand for systems capable of effectively processing and analyzing this data has increased~\cite{zhang2024research}. However, existing motion understanding models often struggle to handle the accurate analysis of human motions and the dynamic nature of user requirements~\cite{meng2020recent, smeddinck2020human}. These MLLMs tend to exhibit limited adaptability to complex, multi-faceted user queries and are often constrained by biases inherent in single-model analyses~\cite{frangoudes2022assessing}, failing to integrate diverse insights into a comprehensive, generalizable, and accurate analysis~\cite{xu2021human}.

% \vspace{-0.10in}
\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{latex/figures/ChatMotion_instruction.png} %
\vspace{-0.20in}
\caption{ChatMotion compares with LLaMo~\cite{li2024human}, a state-of-the-art MLLM for motion understanding. By integrating insights from multiple MLLM results, ChatMotion delivers more accurate analysis.}
\label{MotionCore}
\vspace{-0.22in}
\end{figure}


With the LLM-driven application development\cite{shi2025explaining,shi2024chops,cai2024t,liu2024graph,zheng2025reassessing,yang2024chain,cai2024role}, recent advancements in human motion understanding have progressed, particularly with LLM-based methods targeting specialized tasks and domain-specific applications. Models such as MotionGPT~\cite{jiang2023motiongpt} and MotionLLM~\cite{chen2024motionllm} propose methods to encode motion into structured formats, translating motion data (e.g., videos) into textual descriptions for general motion understanding tasks. Building on this foundation, LLaMo~\cite{li2024human} integrates a motion encoder and cross-talker without relying on motion quantification, demonstrating capabilities in general motion comprehension and specialized analysis across professional domains. These LLM-based motion models aim to bridge raw motion data and interpretable insights, enabling applications in diverse fields.


Despite these advancements, existing approaches still face limitations when applied to broader motion analysis tasks. A key challenge is their reliance on single-model architectures, which often struggle to address complex user requirements~\cite{wei2024motion}. These models show limited adaptability to dynamic user goals and lack mechanisms to integrate insights from multiple MLLMs, constraining their ability to provide comprehensive results. Additionally, they lack effective frameworks for verifying outcomes or refining analyses based on user feedback, which may affect reliability~\cite{lan2022analyzing}. As a result, current Motion LLMs encounter challenges in delivering accurate and complete human motion analyses.


% \paragraph{Challenge 1: Adaptability to User Tasks} Current systems often struggle with understanding and adapting to complex user tasks. This includes the ability to break down multifaceted requests into smaller sub-tasks, invoke the right functions, and handle dynamic changes in user requirements as they evolve. As a result, these models fail to effectively address complex, multi-step motion analysis queries.
    
% \paragraph{Challenge 2: Biases of Analysis} 
% Existing LLMs often exhibit biases in their motion understanding due to their limited scope and reliance on training data. As a result, they struggle to provide accurate and comprehensive analysis of complex motion data, with outputs that may be partial or skewed, limiting their ability to capture subtle nuances.

% \paragraph{Challenge 3: Robustness of Results}
% Motion LLMs often lack inherent self-correction mechanisms, rendering them susceptible to errors such as hallucinations or system failures when confronted with complex or ambiguous user inputs. This limitation in robustness can lead to the propagation of inaccuracies, particularly in multi-step analyses, compromising the reliability and robustness of the results.


% To address these challenges and based on LLaMo \cite{li2024human}, we introduce \textbf{ChatMotion}, the first agent-based framework for motion understanding that combines multi-agent systems with a powerful MotionCore toolbox. This approach offers a flexible, robust, and reliable method for general human motion analysis. Given motion or video data along with a user prompt, ChatMotion first uses a planner to analyze and break down the main task into smaller, manageable sub-tasks. These sub-tasks are then passed to the Executor, which utilizes various tools within MotionCore to handle each task. The MotionCore comprises four functional modules: the MotionAnalyzer, Aggregator, Generator, and Auxiliary Module. For motion understanding, the Executor calls upon the MotionAnalyzer in MotionCore and uses multiple motion LLMs to analyze the data from different perspectives. The aggregator integrated with two distinct mechanisms, is then employed to synthesize the most probable true result from the outputs of the MotionAnalyzer. Finally, the generator will review the user's request and synthesize the answer, leveraging context from the other modules. Throughout the process, a verifier ensures the consistency and relevance of intermediate results, strengthening the reliability of the final output. Through the coordinated efforts of these agents, ChatMotion provides a flexible, precise, and reliable approach to motion analysis, overcoming the limitations of traditional motion LLMs.

To address these challenges and based on LLaMo \cite{li2024human}, we introduce \textbf{ChatMotion}, the first agent-based framework for motion understanding, combining multi-agent systems with the MotionCore toolbox. Given motion or video data with a user prompt, ChatMotion uses a planner to decompose the task into sub-tasks, which are then handled by the Executor using tools within MotionCore. The MotionCore consists of four modules: MotionAnalyzer, Aggregator, Generator, and Auxiliary Module. The Executor calls upon the MotionAnalyzer, utilizing multiple motion LLMs to analyze data from various perspectives. The Aggregator, with two mechanisms, synthesizes the most probable result from the MotionAnalyzer outputs. The Generator reviews the user's request and synthesizes the answer, leveraging contextual information from other modules. A verifier ensures consistency and relevance of intermediate results, enhancing the reliability of the final output. Through coordinated agent efforts, ChatMotion provides a flexible, precise, and reliable approach to motion analysis, overcoming the limitations of traditional motion LLMs.



% First, to tackle task adaptability, ChatMotion utilizes a planner-Executor-verifier architecture. The planner decomposes complex queries into sub-tasks, aligning them with evolving goals and adjusting dynamically. The Executor selects the most suitable models or tools based on task needs. This design ensures flexibility and responsiveness, efficiently handling a broad spectrum of user requests, from simple to multi-step tasks. 

% Second, to solve the biases of analysis thereby improving motion understanding, ChatMotion employs a dynamic, multi-model approach. It integrates Motion LLMs(e.g., Motionllm), each with a confidence score to assess and compare results. Applying the "majority wins" principle, the system selects the most accurate outcome based on confidence levels. In addition, LLaMo's multimodal capabilities enable us to achieve an initial evaluation, refining the final result. This blend of model diversity and confidence-based evaluation ensures robust, precise, and comprehensive analysis.

% Finally, to enhance result robustness, the verifier module supervises task execution, ensuring intermediate outputs are consistent and relevant. This prevents error propagation, validating each step to ensure the final output meets user expectations, thereby improving reliability, accuracy, and overall system robustness. Through these innovations, ChatMotion overcomes the limitations of traditional single-model approaches, offering a more comprehensive, accurate, and user-adaptive solution for general human motion analysis.

% We validate ChatMotion on a wide range of general human motion datasets, including Movid~\cite{chen2024motionllm}, BABEL-QA~\cite{endo2023motion}, MVBench\cite{li2024mvbench} and RepCount~\cite{hu2022transrac}, demonstrating its effectiveness across both standard and complex tasks. Experimental results highlight significant improvements in accuracy, adaptability, and user engagement, establishing new benchmarks in the field of human motion analysis. In summary, the contributions of this work are as follows:

We validate ChatMotion across a wide range of general human motion understanding datasets (e.g., Movid~\cite{chen2024motionllm}, BABEL-QA~\cite{endo2023motion}, MVbench~\cite{li2024mvbench}, and Mo-Repcount~\cite{li2024human} ), demonstrating its effectiveness across both standard and complex tasks. Experimental results highlight the improvements in accuracy, adaptability, and user engagement, establishing new benchmarks in the field of human motion analysis. In summary, the contributions of this work are as follows:
\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{ChatMotion}, a multi-agent system with a planner-Executor-verifier architecture for comprehensive human motion analysis.
    \item A robust \textbf{MotionCore} for invoking functional tools to achieve advanced comprehension by synthesizing multiple perspectives from various MLLMs and can be readily extended, ensuring adaptability and scalability.
    \item Empirical validation across multiple datasets demonstrates that ChatMotion achieves improved performance in human motion analysis compared to existing MLLMs.
\end{itemize}





% These contributions make ChatMotion a promising framework for advancing human motion understanding, opening the door to more interactive, accurate, and adaptable motion analysis systems across various applications.






% Recent studies have advanced human-centric motion understanding in various ways. MotionGPT employed vector quantized variational autoencoders (VQ-VAE) to encode motion data into a discrete codebook~\cite{chen2023motiongpt}. MotionLLM used a motion translator for motions or videos to generate textual representations for general human-centric motion understanding tasks~\cite{chen2024motionllm}. Taking a significant step forward, LLaMo incorporates a motion encoder and estimator tailored to professional sports analysis~\cite{li2024human}. Although LLaMo excels in single-motion analyses, it remains constrained by its inability to accommodate multi-step user intentions and more comprehensive tasks, thus limiting its broader applicability.


% These limitations necessitate further innovation. One potential avenue involves deploying LLMs to interpret user intent in conjunction with Retrieval-Augmented Generation (RAG)~\cite{lewis2020retrieval}, an approach that combines retrieval mechanisms with generative models, to synthesize analyses derived from LLaMo. However, our preliminary results reveal that this rudimentary agent exhibits suboptimal performance, largely due to three principal challenges:

% \paragraph{Challenge 1: Inadequate Mechanisms for Adaptive Function Invocation.} 

% Preliminary methods lack the capacity to adaptively invoke external functions, thereby constraining their ability to address complex tasks, such as comparing a user's elbow motion to standardized postures. Advancing agent design to enable the interpretation of user intent and dynamic function invocation is crucial for achieving personalized and effective answers.


% % Existing simple agent lack the capability to adaptively call external functions, thereby failing to accommodate complex user demands. For instance, in scenarios where a user seeks a visual comparison of their elbow position during motion against a standardized posture, such agent cannot fulfill the request because the necessary function calls are missing. Consequently, devising an agent that can interpret user intent and dynamically deploy function calls is imperative for delivering robust and personalized solutions.

% \paragraph{Challenge 2: Efficient memorization and retrieval of multimodal data.} 
% Straightforward RAG-based retrieval suffers from misalignment between motion data and text~\cite{sharmachart}, hampering accurate and efficient motion retrieval, leading to suboptimal performance on comprehensive tasks.

% .

% \paragraph{Challenge 3: Single-LLM limitations for complex tasks.} 
% Single LLMs handling both interpretation and execution are ill-suited for multi-step, complex tasks, as their limited planning capabilities allow stepwise errors to propagate, thereby compromising reliability and robustness~\cite{wu2025talk}.


% \medskip
% To address these issues, we introduce \textbf{ChatMotion}, a multi-agent system built upon LLaMo, featuring multimodal memory retrieval and function tools. This design effectively addresses both the comprehension and the execution of complex tasks. First, to improve function invocation, we introduce a specialized helper module comprising explicitly annotated functions. A multi-agent architecture then interprets user requests, devises solution strategies and coordinating function calls. By dynamically selecting the most relevant functions based on user objectives, ChatMotion achieves greater adaptability and effectiveness.

% Second, to enhance memory and retrieval, we incorporate a multimodal paging memory that stores text-based motion identifiers rather than raw motion data, thereby bridging semantic gaps. A multi-stage retrieval process, supported by caching, ensures higher precision and efficiency for analyses.

% Finally, ChatMotion employs a planner–Executor–verifier architecture to handle complex task decomposition. The planner segments the user’s query, the Executor invokes specific function modules, and the verifier supervises each step to mitigate errors. This structured design markedly improves system robustness and its capacity for multifaceted tasks.





% % First, to address limitations arising from inadequate adaptive function invocation, we introduce a specialized helper module for function invocation, comprising a suite of functions with explicitly annotated capabilities. We then employ a multi-agent architecture to interpret user requests, devise appropriate solution pathways, and coordinate function calls. By dynamically selecting and invoking the most relevant functions based on the user’s objectives, this approach ensures a more adaptable and effective system for handling complex user demands.

% % Second, we address memory and retrieval challenges by integrating a multimodal paging memory with multi-stage retrieval strategies. To mitigate semantic gaps, ChatMotion’s paging memory stores text-based motion identifiers that uniquely correspond to motion embeddings and analyses derived from user interactions, rather than raw motion data. This system organizes content into structured blocks, enhancing retrieval precision and efficiency. Furthermore, a multi-stage retrieval process, augmented by caching mechanisms, optimizes performance for large-scale analyses. Collectively, these approaches ensure the high accuracy and efficient retrieval essential for multi-agent interpretation and comprehensive tasks.

% % Finally, to accommodate complex task decomposition, ChatMotion employs a multi-agent architecture consisting of a planner, an Executor, and a verifier. The planner interprets and segments the user’s query, the Executor calls specific function modules to fulfill each sub-task, and the verifier supervises every step, mitigating errors as they arise. This structured division of responsibilities substantially boosts system robustness and capacity to handle multifaceted tasks.

% We validate ChatMotion on general motion datasets such as Movid and HA500~\cite{chen2024motionllm} as well as professional sports datasets like Golf-Swing~\cite{li2024human}. Experimental results confirm its substantial gains in complex motion analysis, especially in professional sports, thereby establishing new performance benchmarks in advanced, complex human-centric motion understanding. Our contributions can be summarized as follows:
% \begin{enumerate}
%     \item We introduce \textbf{ChatMotion}, a multi-agent system with integrated multimodal memory retrieval and a series of function tools to enable comprehensive motion analysis and address complex tasks posed by users, thus supporting in-depth human-centric analysis of all professional sports.
%     \item We propose an efficient memory and retrieval mechanism tailored for human-centric professional sports motion analysis, laying the foundation for large-scale, real-time motion assessment.
%     \item We introduce a new paradigm for solving complex user-defined tasks. Our planner--Executor--verifier paradigm clearly separates task interpretation, execution, and error checking, thereby enhancing reliability and robustness in multi-step motion analysis.
%     \item We provide empirical evidence on standard benchmarks showing that ChatMotion outperforms existing models, demonstrating notable improvements in complex human-centric motion analysis and establishing new milestones in professional sports applications.
% \end{enumerate}



\begin{figure*}[ht]
\centering
\includegraphics[width=1\linewidth]{latex/figures/ChatLLaMo_pipeline11.png} %
\vspace{-0.2in}
\caption{The ChatMotion pipeline operates through a three-stage framework designed to optimize task resolution. The Planner interprets the user’s query and breaks it into meta-tasks. Then, the Executor selects and applies appropriate MotionCore tools to execute these tasks. Finally, the Verifier ensures overall correctness, coherence, and completeness.}
\label{ChatMotion_pipeline}
\vspace{-0.15in}
\end{figure*}



\section{Related works}

\subsection{Human Multimodal Representations}

Multimodal representation learning is pivotal for human-centric analyses, especially in tasks requiring spatial-temporal reasoning to interpret complex behaviors~\cite{lin2023videollm, ning2023videobench, li2023videochat}. Recent advancements, such as Video-LLaVA, integrate visual information from images and videos into a unified linguistic feature space, enabling improved visual reasoning for behavioral analysis~\cite{lin2023videollm}. However, many models remain limited to isolated video frames and privacy concerns, constraining their effectiveness in the dynamic real world.~\cite{ning2023videobench, heilbron2015activitynet, maaz2023video}. To address these limitations, motion data has emerged as a privacy-preserving alternative, allowing action analysis without revealing identifiable visual details~\cite{song2023adaptive, yang2023recognizing}. By combining visual and motion data, emerging multimodal frameworks offer comprehensive, privacy-aware solutions, leveraging the complementary strengths of both modalities for enhanced adaptability across diverse applications.

\subsection{Human Motion Understanding}

Human motion analysis traditionally relies on skeletal data, represented as joint keypoint sequences, to capture movement dynamics while preserving user privacy~\cite{shi2023learning, plappert2018bidirectional, yang2023understanding}. Early methods, such as 2s-AGCN~\cite{shi2019two}, and recent transformer-based models like MotionCLIP~\cite{chen2023motiongpt}, have demonstrated success in tasks such as activity recognition, caption generation, and behavior analysis by translating motion data into language tokens. While effective in modeling structural movement patterns, these approaches often neglect environmental context, which is crucial for interpreting motions that may convey different meanings based on situational factors~\cite{song2023finegrained, maaz2023video}. To address this, recent models integrate motion and visual data, enabling improved generalization in dynamic and diverse environments~\cite{liu2024pose, he2023activitynet}. Frameworks like LLaMo\cite{li2024human} have further advanced the field by incorporating motion encoders, estimators, and efficient fusion mechanisms, achieving state-of-the-art results in both general and specialized motion analysis.
% However, these systems focus exclusively on motion evaluation, thereby limiting their applicability in scenarios requiring flexible and comprehensive analyses driven by user intent, and consequently constraining their utility in complex, user-driven contexts.

\vspace{-0.10in}
\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{latex/figures/MotionCore8.png} %
\vspace{-0.20in}
\caption{Components of MotionCore: the MotionCore integrates the MotionAnalyzer and Selection modules to concurrently process and aggregate multiple human motion analyses in two specific ways. The Generation Module synthesizes and contextualizes the results to align with user queries. Additionally, an auxiliary toolbox enables dynamic expansion with supplementary tools to address evolving user requirements.}
\label{MotionCore}
\vspace{-0.15in}
\end{figure}

\section{ChatMotion}

% \lei{Overview}

As shown in Fig.~\ref{ChatMotion_pipeline}, ChatMotion is a multi-agent system that processes user queries involving motion and video data through the Planner, Executor, and Verifier, with LLaMA-70B~\cite{touvron2023llama} employed for all agents. The Planner decomposes the task into meta-tasks, the Executor executes them via MotionCore function calls, and the Verifier ensures accuracy, delivering context-aware, precise results for complex motion analysis.

% (The prompts and examples can be found in~\ref{prompt}).

\subsection{Planner}
The planner serves as the decision-maker, interpreting user intent and subdividing complex tasks into structured meta-tasks. It first analyzes the input query to identify the core objectives and dependencies within the task, and then breaks the task down into smaller, manageable meta-tasks. It operates as the initial step in the multi-agent framework, ensuring that user requirements are translated into a structured workflow that aligns with evolving goals. 

Specifically, let us denote a user query by \(R\). As the simplified version is illustrated in Fig.~\ref{ChatMotion_pipeline}, the Planner will receive an instruction containing user query and available tools functionality in MotionCore which is a function toolbox tailored for human motion analysis (see Sec.~\ref{MotionCore}). Then, the Planner will follow the instructions and identify a set of core objectives \(\mathcal{O} = \{O_1, O_2, \ldots, O_m\}\) simply based on \(R\). 
These objectives are then decomposed into finer-grained meta-tasks guided by the specific functionalities available in the MotionCore tools. 
\[
\mathcal{M} = \{M_1, M_2, \ldots, M_k\},
\]
where each \(M_i\) represents a meta-task in the overall workflow. This decomposition allows the system to handle a wide range of user inputs, from simple queries to multi-step, dynamic tasks.

\subsection{Executor}
Executor serves as the core execution component, responsible for translating the Planner’s meta-tasks into actionable operations using a suite of function tools. After provided the meta-tasks \(\mathcal{M}\), the Executor will process each task in turn guided by the instruction as illustrated in Fig.~\ref{ChatMotion_pipeline}, determining and using the most appropriate function tools in MotionCore (see Sec.~\ref{MotionCore}) based on the alignment between their functional description and the objectives of the meta-task. 

Formally, for a given meta-task \(M_i \in \mathcal{M}\), The Executor will traverse functions capabilities within MotionCore and choose an appropriate tool \(\phi_i\) from a function tool set \(\Phi = \{\phi_1, \phi_2, \ldots, \phi_s\}\) in MotionCore, according to a mapping
\[
\Phi(M_i) \; \rightarrow \; \phi_i,
\]
where \(\phi_i\) is the specific function tool that best addresses the requirements of meta-task \(M_i\). 

If any meta-task proves infeasible, e.g., due to missing functionality, the Executor returns complete error information to the Planner, which will then update its tasks accordingly. The Executor reattempts these updated tasks, iterating through multiple rounds until the overall complex objective is met.

% Within this multi-agent collaboration, the Executor plays a pivotal role, both mapping the Planner’s abstract task requirements onto concrete tools and providing the Verifier (see Sec.~\ref{verifier}) and Planner with all intermediate outputs and scheduling details. Through these cooperative, iterative engagements, ChatMotion can systematically address intricate, multimodal requirements, delivering final results in a reliable manner.

\subsection{Verifier}
\label{verifier}

The Verifier acts as a supervisory agent, ensuring the accuracy and reliability of the multi-agent workflow. It has two main roles: first, it checks that the Planner’s meta-tasks are logically structured and aligned with the user’s prompt; second, it verifies that the meta-tasks can be executed using available tools and that the results meet expectations. If any meta-task cannot be executed or produces incorrect results, or if the Executor calls an inappropriate function, the Verifier prompts the Planner to revise the task list or the Executor to select a different tool. This feedback loop ensures that tasks are executed correctly using the right tools.
% The verifier functions as a supervisory agent, ensuring the fidelity and reliability of the multi-agent workflow. It serves two primary functions. First, it supervises the planner to ensure that the meta-tasks generated effectively address the user’s prompt. This involves verifying that the task breakdown is logical and that each meta-task contributes towards fulfilling the overall goal. Additionally, the verifier supervises the Executor by confirming that the meta-tasks can be executed using the available tools and that the results align with the expected outcomes.

% To formalize the correctness check, we define a binary feasibility function:

% \[
% S(M_i) = 
% \begin{cases}
% 1, & \text{if sub-task } M_i \text{ is executed}, \\
% 0, & \text{otherwise}.
% \end{cases}
% \]

% If there exists a meta-task \( M_i \) such that \( S(M_i) = 0 \), the verifier triggers an error report to the planner, prompting a revision of the meta-task list. As such, the verifier ensures that the Executor’s tools are suitable for each task and that they are executed correctly. In case of discrepancies, the verifier provides real-time feedback to help the planner and Executor adjust their workflows to achieve the desired results.

% The verifier functions as a supervisory agent, ensuring the fidelity and reliability of the multi-agent workflow. It continuously monitors whether the planner accurately interprets the user’s requirements and effectively addresses them in the meta-task list. Specifically, the verifier assesses the reasonableness of the planner-generated meta-list and its ability to fulfill user demands.  

% Additionally it also supervises the meta-tasks whether can be fulfilled by Executor.

% To formalize the correctness check, define a binary feasibility function:
% \[
% S(M_i) = 
% \begin{cases}
% 1, & \text{if sub-task } M_i \text{ is executed}, \\
% 0, & \text{otherwise}.
% \end{cases}
% \]
% If \(\exists\, M_i \in \mathcal{M} \text{ such that } S(M_i) = 0,\) the verifier triggers an error report to the Planner, prompting the reorganization or refinement of the meta-task list.

% Additionally, the verifier scrutinizes the Executor’s mapping and execution of sub-tasks using designated function tools, verifying that the meta-tasks are achievable with the deployed tools and that the results are correct. In cases of unexpected issues within the trajectory, the verifier identifies errors and integrates error messages, providing real-time, context-rich feedback to both the planner and Executor. This enables the planner and Executor to promptly rectify mistakes by reorganizing the meta-tasks or adopting alternative execution strategies.

% Ultimately, the verifier’s supervisory role seamlessly integrates with the planner and Executor, creating an iterative feedback loop that ensures error-free, user-aligned analyses across diverse motion tasks. This methodology significantly enhances system reliability and robustness, allowing ChatMotion to deliver precise analyses in comprehensive human-centric motion understanding tasks.


\subsection{MotionCore}
\label{MotionCore}

MotionCore is a comprehensive toolkit that enables efficient human motion understanding by integrating various modules and auxiliary functions. It also includes auxiliary tools for tasks like motion visualization and video retrieval, meeting users' diverse requirements. MotionCore is orchestrated by the Executor Agent, which autonomously selects the appropriate tools from the toolkit to complete tasks based on a given meta-task list.


\subsubsection{MotionAnalyzer}

The  MotionAnalyzer in MotionCore enhances motion understanding and mitigates biases through a dynamic, multi-model approach. It integrates human motion models, such as MotionLLM~\cite{chen2024motionllm}, MotionGPT~\cite{jiang2023motiongpt}, and LLaMo\cite{li2024human}, alongside video captioning models such as VideoChat2~\cite{li2024mvbench}, GPT-4v~\cite{openai20234v}, and video-LLaVA~\cite{lin2023video} to handle human motion input.

Let the set of motion understanding models be denoted as $\{F_1, F_2, \ldots, F_N\}$, where each model $F_i$ processes the multimodal input data $D$ (e.g., video frames, motion capture data) to produce text analysis $r_i$, i.e., $(r_i) = F_i(D), \quad i = 1, 2, \ldots, N$. Each model is assigned a predefined confidence score $c_i$, based on the previous evaluation performance, independent of the model’s predictions. These confidence scores are allocated based on the input modalities, which can be motion capture, video, or motion-video. The outputs and their corresponding confidence scores are represented as $\{(r_1, c_1), (r_2, c_2), \ldots, (r_N, c_N)\}$, where $c_i$ denotes the predefined confidence score for the output $r_i$ of model $F_i$ in its respective task. This integration of predefined confidence scores ensures a robust and flexible understanding of motion, leveraging the strengths of each model across diverse modalities and tasks.



% \subsubsection{Multi-LLM Motion Analysis Module}
% The Multi-LLM Motion Analysis Module in ChatMotion addresses biases and enhances motion understanding by employing a dynamic, multi-model approach. This module integrates various human motion understanding models, such as MotionLLM, MotionGPT, LLaMo, and video understanding models like VideoChat2, GPT-4v and video-LLaVA to process both video and motion data inputs. 

% Denote the set of models by \(\{F_1, F_2, \ldots, F_N\}\). Each model \(F_i\) produces an output \(r_i\) together with a confidence score \(c_i\), i.e.,
% \[
% (r_i, c_i) = F_i(D), \quad i = 1,2,\ldots,N,
% \]
% where \(D\) represents the multimodal input data (e.g., video frames, motion capture data, etc.). These models are selected for their complementary strengths in understanding human motion from different perspectives, ensuring that diverse motion characteristics are effectively captured. By leveraging the diverse capabilities of these models, the system can produce a more comprehensive understanding of human motion, offering enhanced flexibility for various tasks and applications.

% \subsubsection{Selection Module}
% The Selection Module in ChatMotion is responsible for aggregating the results from the different models and selecting the most accurate output. This module employs two primary strategies for result selection, designed to optimize the accuracy and reliability of the motion analysis:

% \paragraph{Confidence-based Mechanism}
% Given the set of $\{(r_1, c_1), (r_2, c_2), \dots, (r_N, c_N)\}$, where each pair consists of an output \( r_i \) and its corresponding confidence score \( c_i \). The confidence-based mechanism prioritizes both overlap and confidence in its decision-making process. First, analysis with higher confidence scores are given more weight, but the process is not solely driven by the highest score. If multiple models produce the same or similar results (i.e., overlap), this collective agreement increases the likelihood of the output being correct, as per a "majority wins" principle. 

% Specially, these analysis-confidence pairs are passed to LLaMA-70B for synthesis rather than being combined via a fixed, hand-crafted function. The model adaptively weighs overlapping results to reinforce consensus while also recognizing uniquely accurate analyses with high confidence. By leveraging LLaMA-70B’s advanced semantic understanding and context-sensitive reasoning, the method balances majority agreement against individual expertise, ensuring a robust and flexible decision-making process that can accommodate both prevailing consensus and exceptional high-confidence insights. 

% However, in cases where a less confident but distinct output from a more advanced model differs from the majority, the LLaMA-70B model takes the confidence scores into account to assess the plausibility of this outlier. The overlap of results among models is crucial; LLaMA-70B evaluates how consistent or inconsistent these outputs are, using the confidence scores as a guide to give more weight to overlapping results that suggest consensus. Ultimately, LLaMA-70B synthesizes these factors to select the most reliable output, considering both the majority agreement and any advanced model's potential to correctly identify a unique but accurate result. This method effectively balances consensus across models with individual expertise, ensuring robust and contextually informed motion analysis.


% \paragraph{Confidence-based Mechanism}
% In this approach, each model generates an output \(r_i\) along with a confidence score \(c_i\). These results, along with their respective confidence levels, are aggregated, and a “majority wins” rule is applied in a simplified form as
% \[
% r^{\ast} = \arg \max_{r_i} \bigl\{\,c_i : i = 1,2,\ldots,N\bigr\}.
% \]
% This means that the result chosen will be the one supported by the highest confidence score across the models. The final selection is made by the LLaMo-70B model, which synthesizes the aggregated data and ensures that the chosen result represents the most reliable analysis. This method helps mitigate the impact of less reliable models, increasing the overall robustness of the motion understanding process.

% \paragraph{Motion-aware Mechanism}
% The Motion-aware Mechanism involves an initial selection step performed by LLaMo, which is a advanced model for motion understanding. Based on its superior motion understanding and multimodal processing capabilities, LLaMo reviews all the $\{(r_i, c_i)\}$ pairs and original motion sequence $\mathcal{M}$, making an initial selection choice: 
% \[
% r' = \mathrm{LLaMo}(r_1, \ldots, r_N;\; c_1, \ldots, c_N;\; \mathcal{M}),
% \]
% considering the possible bias existing in LLaMo and more powerful context reasoning ability in LLaMA-70B, the initial choice $r'$ from LLaMo along with the $\{(r_i, c_i)\}$ pairs, is then reasoning by LLaMo-70B, which refines the decision-making process and selects the most possible result. This dual-layered evaluation ensures that both the contextual understanding and confidence of the models are accounted for, improving the overall accuracy and precision of the motion analysis.

% \subsubsection{Aggregator}
% The Aggregator in MotionCore is designed to identify the most reliable candidate result from a set of \(\{(r_i, c_i)\}\) pairs. By employing two strategies, the Confidence Mechanism and the Motion-aware Mechanism, it selects the most accurate outcome from a range of diverse perspectives, thereby enhancing the robustness of motion understanding.



% \paragraph{Confidence Mechanism} 
% The first method, rooted in game theory principles, 
% Considers the set 
% \[
% \{ (r_i, c_i) \mid i = 1, 2, \dots, N \},
% \]
% where \(r_i\) represents a model’s output and \(c_i\) denotes its associated confidence score. This mechanism assigns greater weight to outputs with higher confidence, while considering the degree of overlap among proposed solutions. When multiple models produce similar outcomes, their collective agreement reinforces the likelihood of correctness, mirroring a "majority wins" principle.

% Instead of relying on a fixed function for combining these outputs, the analysis-confidence pairs \(\{(r_i, c_i)\}\) are passed to LLaMA~\cite{touvron2023llama} for final synthesis. LLaMA then adaptively integrates the outputs, balancing consensus and individual model expertise. This allows the mechanism to emphasize shared conclusions while remaining receptive to outlier predictions with high confidence. By leveraging LLaMA's advanced semantic understanding and context-sensitive reasoning, the aggregation process is both flexible and robust, accommodating prevalent agreements and exceptional insights.

% While foundational, this approach is basic, relying on confidence scores and model consensus. The next step introduces a motion-aware mechanism, which, in addition to aggregating results, incorporates motion priors to refine the process.


% \paragraph{Motion-aware Mechanism}
% The Motion-aware Mechanism leverages the specialized motion-understanding capabilities of LLaMo~\cite{li2024human}. During an initial screening, LLaMo~\cite{li2024human} evaluates all \(\{(r_i, c_i)\}\) pairs in conjunction with the original motion or video data \(\mathcal{M}\), providing a preliminary best estimate:
% \[
% r' = \mathrm{LLaMo}(r_1, \ldots, r_N;\; c_1, \ldots, c_N;\; \mathcal{M}).
% \]
% Subsequently, to mitigate any potential model-specific bias and further refine the decision, LLaMA~\cite{touvron2023llama} re-examines both the preliminary result \(r'\) and the original sets \(\{(r_i, c_i)\}\). Through this dual-layered evaluation, the combined strengths of context-aware reasoning in LLaMA~\cite{touvron2023llama} and domain-focused motion expertise in LLaMo~\cite{li2024human} jointly enhance the reliability and precision of the final outcome.

% The Aggregator serves as a powerful tool within MotionCore for the Executor's deployment. It also enables ChatMotion to identify the most reliable and accurate analyses from a diverse set of model outputs, thereby promoting a more robust and comprehensive understanding of human motion.

\subsubsection{Aggregator}  
The Aggregator in MotionCore identifies the most reliable result from a set of \(\{(r_i, c_i)\}\) pairs, employing two strategies: the Confidence Mechanism and the Motion-aware Mechanism, which enhance the robustness of motion understanding by selecting the most accurate outcome from diverse perspectives.

\paragraph{Confidence Mechanism}  
Rooted in game theory, this method considers the set  
\[
\{ (r_i, c_i) \mid i = 1, 2, \dots, N \},
\]
where \(r_i\) is a model’s output and \(c_i\) is its associated confidence score. The mechanism assigns higher weight to more confident outputs, with a "majority wins" principle when models converge on similar results. Rather than using a fixed function, the analysis-confidence pairs \(\{(r_i, c_i)\}\) are passed to LLaMA~\cite{touvron2023llama}, which adaptively integrates the outputs by balancing consensus with individual model expertise. This ensures a flexible and robust aggregation process, emphasizing shared conclusions while considering outlier predictions.

Though foundational, this approach is basic, relying primarily on confidence scores and model consensus. The next step incorporates a motion-aware mechanism to refine the process.

\paragraph{Motion-aware Mechanism}  
With LLaMo's~\cite{li2024human} specialized motion-understanding capabilities, this mechanism evaluates \(\{(r_i, c_i)\}\) pairs alongside the original motion or video data \(\mathcal{M}\), generating an initial estimate:
\[
r' = \mathrm{LLaMo}(r_1, \ldots, r_N;\; c_1, \ldots, c_N;\; \mathcal{M}).
\]
LLaMA~\cite{touvron2023llama} then re-examines the preliminary result \(r'\) and the original pairs \(\{(r_i, c_i)\}\) to mitigate model bias and refine the outcome. This dual-layer evaluation leverages LLaMo's domain-specific motion expertise and LLaMA's context-aware reasoning, improving both reliability and precision.

The Aggregator is a powerful tool within MotionCore, enabling ChatMotion to identify the most accurate analyses from diverse model outputs, fostering a more comprehensive understanding of human motion.



% \subsubsection{Analysis Generation Module}
% In the MotionCore, the Analysis Generation Module is response for integrating context from other function calls and reviews the user's prompt to generate the final output. Concretely, we denote the context from other function calls by \(c^*\). The Analysis Generation Module then combines \(c^*\) with the user’s original requirements \(R\) to produce a final answer:
% \[
% \mathrm{Answer} = \Gamma(c^*, R),
% \]
% where \(\Gamma(\cdot)\) is LLaMA that organizes the final answer (textual analysis, visual feedback, etc.) into a coherent, user-facing format. This module consolidates the context into a format that directly addresses the user's needs, providing the requested results in a concise and coherent manner.

\subsubsection{Generator}
In MotionCore, the Generator is responsible for synthesizing contextual information from previous function calls and the user's original request to produce a final answer. As illustrated in Fig.~\ref{MotionCore}, the Generator reviews the user query and organizes the context into a coherent and accurate answer. The answer could be in the form of textual analysis, motion feedback, or other formats, depending on the user's request. Contextual information from earlier interactions is denoted as \(t^*\). The module then integrates this context with the user’s specific requirements, represented as \(R\), to generate a comprehensive response:
\[
\mathrm{Answer} = \Gamma(t^*, R),
\]
where \(\Gamma(\cdot)\) denotes LLaMA~\cite{touvron2023llama} by default. The purpose of the Generator is to transform the context into an answer that directly addresses the user’s needs, ensuring the answer is concise and contextually accurate.




\subsubsection{Auxiliary Tools} 
The Auxiliary Tools in MotionCore, which can be accessed by the Executor, extend ChatMotion's capabilities by orchestrating external, domain-specific functionalities that go beyond the scope of the multimodal model alone. For instance, the system can retrieve professional analysis by querying specialized knowledge bases, which provide context-specific insights based on user inputs. Additionally, it enables motion retrieval by identifying relevant motion data based on the user’s request, leveraging a stored database of labeled motion data and utilizing vector-based search to match the query to the most relevant motion. As a result, it equips ChatMotion with diverse motion analysis capabilities that simple MLLMs do not possess. By offering a unified, modular interface for diverse auxiliary function calls, ChatMotion readily integrates and extends new capabilities without overburdening the core model.


% For example, it enables specialized tasks such as professional analysis in motion or retrieving reference motions based on user requirements. As a result, it equips ChatMotion with diverse motion analysis capabilities that simple MLLMs do not possess. Formally, let 
% \[
% \mathcal{F} = \{\varphi_1, \varphi_2, \ldots, \varphi_p\}
% \]
% denote a set of domain-specific function calls. By offering a unified, modular interface for diverse auxiliary function calls, ChatMotion readily integrates and extends new capabilities without overburdening the core model.


% \section{ChatMotion}

% \subsection{Planner}
% The planner serves as the decision-maker, interpreting user intent and subdividing complex tasks into structured subtasks. 
% The Planner module in ChatMotion is responsible for decomposing complex user tasks into manageable sub-tasks that can be effectively executed by the system. It operates as the initial step in the multi-agent framework, ensuring that user requirements are translated into a structured workflow that aligns with evolving goals. The Planner first analyzes the input query to identify the core objectives and dependencies within the task, and then breaks the task down into smaller, manageable components. This decomposition can be mathematically represented as:

% \[
% \text{Task decomposition:} \quad \mathcal{T} \rightarrow \{ \mathcal{T}_1, \mathcal{T}_2, \dots, \mathcal{T}_n \}
% \]

% where \( \mathcal{T} \) is the original task, and \( \mathcal{T}_1, \mathcal{T}_2, \dots, \mathcal{T}_n \) are the decomposed subtasks. This decomposition allows the system to handle a wide range of user inputs, from simple queries to multi-step, dynamic tasks.

% \subsection{Executor}
% Executor serves as the core execution component, responsible for translating the Planner’s meta-tasks into actionable operations using a suite of function tools. After the Planner generates a list of meta-tasks, the Executor processes each task in turn, determines the most appropriate function tools, and orchestrates their execution to accomplish detailed analyses and operations on multimodal data. The execution of each meta-task can be described as:

% \[
% \mathcal{E}(\mathcal{T}_i) = \text{Execute}(\mathcal{T}_i, \mathcal{F}_i)
% \]

% where \( \mathcal{T}_i \) represents the i-th meta-task, and \( \mathcal{F}_i \) is the corresponding function tool used for execution. If any meta-task proves infeasible, e.g., due to missing functionality, the Executor returns complete error information to the Planner, which updates its tasks accordingly. The Executor reattempts these updated tasks, iterating through multiple rounds until the overall complex objective is met.

% \subsection{Verifier}

% The verifier functions as a supervisory agent, ensuring the fidelity and reliability of the multi-agent workflow. It continuously monitors whether the planner accurately interprets the user’s requirements and effectively addresses them in the meta-task list. Specifically, the verifier assesses the reasonableness of the planner-generated meta-list and its ability to fulfill user demands. Mathematically, this can be expressed as:

% \[
% \mathcal{V}(\mathcal{T}_i, \mathcal{F}_i) = \text{Verify}(\mathcal{T}_i, \mathcal{F}_i)
% \]

% where \( \mathcal{V} \) denotes the verification process, ensuring that \( \mathcal{T}_i \) and \( \mathcal{F}_i \) are valid for execution. Additionally, the verifier scrutinizes the Executor’s mapping and execution of sub-tasks using designated function tools, verifying that the meta-tasks are achievable with the deployed tools and that the results are correct. In cases of unexpected issues within the trajectory, the verifier identifies errors and integrates error messages, providing real-time, context-rich feedback to both the planner and Executor. This enables the planner and Executor to promptly rectify mistakes by reorganizing the meta-tasks or adopting alternative execution strategies.

% \subsection{Multi-LLM Motion Analysis Module}

% The Multi-LLM Motion Analysis Module in ChatMotion addresses biases and enhances motion understanding by employing a dynamic, multi-model approach. This module integrates various human motion understanding models, such as MotionLLM, MotionGPT, LLaMo, and video understanding models like VideoChat2, to process both video and motion data inputs. The models are selected based on a selection criterion, and the aggregation of multiple model outputs can be expressed as:

% \[
% \mathcal{M} = \{ \mathcal{M}_1, \mathcal{M}_2, \dots, \mathcal{M}_k \}
% \]

% where \( \mathcal{M}_1, \mathcal{M}_2, \dots, \mathcal{M}_k \) are the individual models processing the data. These models are selected for their complementary strengths in understanding human motion from different perspectives, ensuring that diverse motion characteristics are effectively captured. By leveraging the diverse capabilities of these models, the system can produce a more comprehensive understanding of human motion, offering enhanced flexibility for various tasks and applications.

% \subsection{Selection Module}

% The Selection Module in ChatMotion is responsible for aggregating the results from the different models and selecting the most accurate output. This module employs two primary strategies for result selection, designed to optimize the accuracy and reliability of the motion analysis:

% \paragraph{Confidence-based Mechanism} In this approach, each model generates an output along with a confidence score \( \mathcal{C}_i \) that reflects the model's certainty in its prediction. The selection of the final result is based on the highest confidence score:

% \[
% \mathcal{R} = \arg\max_{i} (\mathcal{C}_i)
% \]

% where \( \mathcal{R} \) represents the selected result and \( \mathcal{C}_i \) is the confidence score for model \( i \).

% \paragraph{Motion-aware Mechanism} The Motion-aware Mechanism involves an initial selection step performed by LLaMo, which is a specialized model for motion understanding. The selection process can be expressed as:

% \[
% \mathcal{R} = \mathcal{L}(\mathcal{M}_1, \mathcal{M}_2, \dots, \mathcal{M}_k, \mathcal{C}_1, \mathcal{C}_2, \dots, \mathcal{C}_k)
% \]

% where \( \mathcal{L} \) denotes the selection function that incorporates both the model outputs and their respective confidence scores to make an initial selection. This output, along with the individual model results and their confidence scores, is then processed by the final layer \( \mathcal{L}_{70B} \), which refines the decision-making process.

% \subsection{Function Calls}

% The Function Caller extends ChatMotion’s capabilities by orchestrating external, domain-specific functionalities beyond the scope of the multimodal model alone. For instance, it enables specialized tasks such as visualizing elbow angles changes in a Tai Chi motion or retrieving reference videos for different Tai Chi postures, thereby bridging ChatMotion’s comprehensive motion understanding with auxiliary computational tools. This can be mathematically expressed as:

% \[
% \mathcal{F}_\text{aux} = \text{Call}(\mathcal{T}_\text{special})
% \]

% where \( \mathcal{F}_\text{aux} \) denotes the auxiliary function call, and \( \mathcal{T}_\text{special} \) is the specific task that requires external computation.

% \subsection{Analysis Generation Module}

% The Analysis Generation Module integrates results from other modules and reviews the user's prompt to generate the final output. It synthesizes the processed motion data into a cohesive, user-friendly response, ensuring that all relevant insights are clearly presented. This synthesis can be formalized as:

% \[
% \mathcal{A} = \text{Generate}(\mathcal{M}, \mathcal{R}, \mathcal{T}_\text{user})
% \]

% where \( \mathcal{A} \) is the final analysis output, \( \mathcal{M} \) are the model results, \( \mathcal{R} \) is the selected result, and \( \mathcal{T}_\text{user} \) is the user query or task.


% \section{ChatMotion}

% \subsection{Planner}
% The planner serves as the decision-maker, interpreting user intent and subdividing complex tasks into structured subtasks. 
% The Planner module in ChatMotion is responsible for decomposing complex user tasks into manageable sub-tasks that can be effectively executed by the system. 
% It operates as the initial step in the multi-agent framework, ensuring that user requirements are translated into a structured workflow that aligns with evolving goals. 
% Formally, let us denote the user query as 
% \[
%    Q \in \mathcal{U},
% \]
% where \(\mathcal{U}\) is the space of possible user inputs. 
% The Planner first analyzes the input query to identify the core objectives and dependencies within the task, and then breaks the task down into smaller, manageable components. 
% We can define a set of meta-tasks as 
% \[
%    \mathcal{M} = \{m_1, m_2, \ldots, m_k\},
% \]
% where each \(m_i\) represents a distinct subtask derived from the overall query \(Q\). 
% This decomposition allows the system to handle a wide range of user inputs, from simple queries to multi-step, dynamic tasks.

% \subsection{Executor}
% Executor serves as the core execution component, responsible for translating the Planner’s meta-tasks into actionable operations using a suite of function tools. 
% After the Planner generates a list of meta-tasks 
% \(\mathcal{M} = \{m_1, m_2, \ldots, m_k\}\),
% the Executor processes each task in turn, determines the most appropriate function tools, and orchestrates their execution to accomplish detailed analyses and operations on multimodal data. 
% If any meta-task proves infeasible, e.g., due to missing functionality, the Executor returns complete error information to the Planner, which updates its tasks accordingly. 
% The Executor reattempts these updated tasks, iterating through multiple rounds until the overall complex objective is met. 
% We can express the Executor's operation as a mapping 
% \[
%    E : \mathcal{M} \rightarrow \mathcal{O},
% \]
% where \(\mathcal{O}\) denotes the set of all possible operation outputs. 

% Within this multi-agent collaboration, the Executor plays a pivotal role, both mapping the Planner’s abstract task requirements onto concrete tools and furnishing the Verifier and Planner with all intermediate outputs and scheduling details. 
% Through these cooperative, iterative engagements, ChatMotion can systematically address intricate, multimodal requirements, delivering final results in a reliable manner.

% \subsection{Verifier}
% The verifier functions as a supervisory agent, ensuring the fidelity and reliability of the multi-agent workflow. 
% It continuously monitors whether the planner accurately interprets the user’s requirements and effectively addresses them in the meta-task list. 
% Specifically, the verifier assesses the reasonableness of the planner-generated meta-list and its ability to fulfill user demands. 
% Additionally, the verifier scrutinizes the Executor’s mapping and execution of sub-tasks using designated function tools, verifying that the meta-tasks are achievable with the deployed tools and that the results are correct. 
% In cases of unexpected issues within the trajectory, the verifier identifies errors and integrates error messages, providing real-time, context-rich feedback to both the planner and Executor. 
% This enables the planner and Executor to promptly rectify mistakes by reorganizing the meta-tasks or adopting alternative execution strategies.

% We may denote the Verifier's checking function as
% \[
%    V : (\mathcal{M}, \mathcal{O}) \rightarrow \{\text{valid}, \text{error}\},
% \]
% where \(\text{valid}\) means the sub-task results align with expectations, and \(\text{error}\) indicates the presence of issues to be corrected.

% Ultimately, the verifier’s supervisory role seamlessly integrates with the planner and Executor, creating an iterative feedback loop that ensures error-free, user-aligned analyses across diverse motion tasks. 
% This methodology significantly enhances system reliability and robustness, allowing ChatMotion to deliver precise analyses in comprehensive human-centric motion understanding tasks.

% \subsection{Multi-LLM Motion Analysis Module}
% The Multi-LLM Motion Analysis Module in ChatMotion addresses biases and enhances motion understanding by employing a dynamic, multi-model approach. 
% This module integrates various human motion understanding models, such as MotionLLM, MotionGPT, LLaMo, and video understanding models like VideoChat2, to process both video and motion data inputs. 
% Let us represent these models as 
% \[
%    \{\mathcal{H}_1, \mathcal{H}_2, \ldots, \mathcal{H}_r\},
% \]
% each of which outputs a motion representation 
% \(\mathbf{z}_i\) for the \(i\)-th model. 
% These models are selected for their complementary strengths in understanding human motion from different perspectives, ensuring that diverse motion characteristics are effectively captured. 
% By leveraging the diverse capabilities of these models, the system can produce a more comprehensive understanding of human motion, offering enhanced flexibility for various tasks and applications.

% \subsection{Selection Module}
% The Selection Module in ChatMotion is responsible for aggregating the results from the different models and selecting the most accurate output. 
% This module employs two primary strategies for result selection, designed to optimize the accuracy and reliability of the motion analysis:

% \paragraph{Confidence-based Mechanism} 
% In this approach, each model \(\mathcal{H}_i\) generates an output \(\mathbf{z}_i\) along with a confidence score \(c_i \in [0, 1]\) that reflects the model's certainty in its prediction. 
% These results, along with their respective confidence levels, are aggregated, and a "majority wins" rule is applied. 
% Mathematically, let 
% \[
%    \hat{i} = \arg\max_{i} \{c_i\},
% \]
% then the final selection is taken as 
% \(\mathbf{z}_{\hat{i}}\). 
% This means that the result chosen will be the one supported by the highest confidence score across the models. 
% The final selection is made by the LLaMo-70B model, which synthesizes the aggregated data and ensures that the chosen result represents the most reliable analysis. 
% This method helps mitigate the impact of less reliable models, increasing the overall robustness of the motion understanding process.

% \paragraph{Motion-aware Mechanism} 
% The Motion-aware Mechanism involves an initial selection step performed by LLaMo, which is a specialized model for motion understanding. 
% In this method, the models' outputs \(\mathbf{z}_i\) and their corresponding confidence scores \(c_i\) are provided to LLaMo. 
% Based on its in-depth motion understanding, LLaMo makes an initial selection, considering the context and dynamics of the motion data. 
% Let this initial choice be \(\mathbf{z}_{\hat{j}}\). 
% This output, along with the individual model results and their confidence scores, is then processed by LLaMo-70B, which refines the decision-making process and selects the most accurate result. 
% Symbolically, 
% \[
%    \mathbf{z}^* = \text{Refine}(\mathbf{z}_{\hat{j}}, \{\mathbf{z}_i\}_{i=1}^r, \{c_i\}_{i=1}^r).
% \]
% This dual-layered evaluation ensures that both the contextual understanding and confidence of the models are accounted for, improving the overall accuracy and precision of the motion analysis.

% By implementing these two evaluation methods, the Selection Module enables ChatMotion to choose the most reliable and accurate analysis from a range of diverse model outputs. 
% This multi-step approach ensures that the system benefits from the strengths of each individual model while minimizing the impact of potential biases or inaccuracies, ultimately leading to more robust and comprehensive human motion understanding.

% \subsection{Function Calls} 
% The Function Caller extends ChatMotion’s capabilities by orchestrating external, domain-specific functionalities beyond the scope of the multimodal model alone. 
% For instance, it enables specialized tasks such as visualizing elbow angles changes in a Tai Chi motion or retrieving reference videos for different Tai Chi postures, thereby bridging ChatMotion’s comprehensive motion understanding with auxiliary computational tools. 
% By offering a unified, modular interface for diverse function calls, the system readily integrates new capabilities without overburdening the core model. 
% We can formalize a function call as 
% \[
%    \mathrm{Call}(\mathrm{func}, \mathrm{args}) \rightarrow \mathrm{ret},
% \]
% where \(\mathrm{func}\) is the domain-specific function name, \(\mathrm{args}\) the arguments, and \(\mathrm{ret}\) the returned data.

% \subsection{Analysis Generation Module}
% The Analysis Generation Module integrates results from other modules and reviews the user's prompt to generate the final output. 
% It synthesizes the processed motion data into a cohesive, user-friendly response, ensuring that all relevant insights are clearly presented. 
% Formally, we denote this synthesis step as a function 
% \[
%    \Gamma : \bigl(\{\mathbf{z}_i\}, \{c_i\}, Q \bigr) \rightarrow \mathcal{R},
% \]
% where \(\{\mathbf{z}_i\}\) and \(\{c_i\}\) represent the final selected motion analysis outputs and their confidences, \(Q\) is the user query, and \(\mathcal{R}\) is the space of all possible response forms. 
% This module serves as the final step, consolidating the analysis into a format that directly addresses the user's needs, providing the requested results in a concise and coherent manner.




% \section{ChatMotion}


% \subsection{Planner}
% The planner serves as the decision-maker, interpreting user intent and subdividing complex tasks into structured subtasks. 

% The Planner module in ChatMotion is responsible for decomposing complex user tasks into manageable sub-tasks that can be effectively executed by the system. It operates as the initial step in the multi-agent framework, ensuring that user requirements are translated into a structured workflow that aligns with evolving goals. The Planner first analyzes the input query to identify the core objectives and dependencies within the task, and then breaks the task down into smaller, manageable components. This decomposition allows the system to handle a wide range of user inputs, from simple queries to multi-step, dynamic tasks.


% \subsection{Executor}
% Executor serves as the core execution component, responsible for translating the Planner’s meta-tasks into actionable operations using a suite of function tools. After the Planner generates a list of meta-tasks, the Executor processes each task in turn, determines the most appropriate function tools, and orchestrates their execution to accomplish detailed analyses and operations on multimodal data. If any meta-task proves infeasible, e.g., due to missing functionality, the Executor returns complete error information to the Planner, which updates its tasks accordingly. The Executor reattempts these updated tasks, iterating through multiple rounds until the overall complex objective is met.

% Within this multi-agent collaboration, the Executor plays a pivotal role, both mapping the Planner’s abstract task requirements onto concrete tools and furnishing the Verifier and Planner with all intermediate outputs and scheduling details. Through these cooperative, iterative engagements, ChatMotion can systematically address intricate, multimodal requirements, delivering final results in a reliable manner.



% \subsection{Verifier}

% The verifier functions as a supervisory agent, ensuring the fidelity and reliability of the multi-agent workflow. It continuously monitors whether the planner accurately interprets the user’s requirements and effectively addresses them in the meta-task list. Specifically, the verifier assesses the reasonableness of the planner-generated meta-list and its ability to fulfill user demands. Additionally, the verifier scrutinizes the Executor’s mapping and execution of sub-tasks using designated function tools, verifying that the meta-tasks are achievable with the deployed tools and that the results are correct. In cases of unexpected issues within the trajectory, the verifier identifies errors and integrates error messages, providing real-time, context-rich feedback to both the planner and Executor. This enables the planner and Executor to promptly rectify mistakes by reorganizing the meta-tasks or adopting alternative execution strategies.

% Ultimately, the verifier’s supervisory role seamlessly integrates with the planner and Executor, creating an iterative feedback loop that ensures error-free, user-aligned analyses across diverse motion tasks. This methodology significantly enhances system reliability and robustness, allowing ChatMotion to deliver precise analyses in comprehensive human-centric motion understanding tasks.



% \subsection{Multi-LLM Motion Analysis Module}

% The Multi-LLM Motion Analysis Module in ChatMotion addresses biases and enhances motion understanding by employing a dynamic, multi-model approach. This module integrates various human motion understanding models, such as MotionLLM, MotionGPT, LLaMo, and video understanding models like VideoChat2, to process both video and motion data inputs. These models are selected for their complementary strengths in understanding human motion from different perspectives, ensuring that diverse motion characteristics are effectively captured. By leveraging the diverse capabilities of these models, the system can produce a more comprehensive understanding of human motion, offering enhanced flexibility for various tasks and applications.


% \subsection{Selection Module}

% The Selection Module in ChatMotion is responsible for aggregating the results from the different models and selecting the most accurate output. This module employs two primary strategies for result selection, designed to optimize the accuracy and reliability of the motion analysis:

% \paragraph{Confidence-based Mechanism} In this approach, each model generates an output along with a confidence score that reflects the model's certainty in its prediction. These results, along with their respective confidence levels, are aggregated, and a "majority wins" rule is applied. This means that the result chosen will be the one supported by the highest confidence score across the models. The final selection is made by the LLaMo-70B model, which synthesizes the aggregated data and ensures that the chosen result represents the most reliable analysis. This method helps mitigate the impact of less reliable models, increasing the overall robustness of the motion understanding process.

% \paragraph{Motion-aware Mechanism} The Motion-aware Mechanism involves an initial selection step performed by LLaMo, which is a specialized model for motion understanding. In this method, the models' outputs and their corresponding confidence scores are provided to LLaMo. Based on its in-depth motion understanding, LLaMo makes an initial selection, considering the context and dynamics of the motion data. This output, along with the individual model results and their confidence scores, is then processed by LLaMo-70B, which refines the decision-making process and selects the most accurate result. This dual-layered evaluation ensures that both the contextual understanding and confidence of the models are accounted for, improving the overall accuracy and precision of the motion analysis.

% By implementing these two evaluation methods, the Selection Module enables ChatMotion to choose the most reliable and accurate analysis from a range of diverse model outputs. This multi-step approach ensures that the system benefits from the strengths of each individual model while minimizing the impact of potential biases or inaccuracies, ultimately leading to more robust and comprehensive human motion understanding.


% \subsection{Function Calls} The Function Caller extends ChatMotion’s capabilities by orchestrating external, domain-specific functionalities beyond the scope of the multimodal model alone. For instance, it enables specialized tasks such as visualizing elbow angles changes in a Tai Chi motion or retrieving reference videos for different Tai Chi postures, thereby bridging ChatMotion’s comprehensive motion understanding with auxiliary computational tools. By offering a unified, modular interface for diverse function calls, the system readily integrates new capabilities without overburdening the core model.

% \subsection{Analysis Generation Module}


% The Analysis Generation Module integrates results from other modules and reviews the user's prompt to generate the final output. It synthesizes the processed motion data into a cohesive, user-friendly response, ensuring that all relevant insights are clearly presented. This module serves as the final step, consolidating the analysis into a format that directly addresses the user's needs, providing the requested results in a concise and coherent manner.


\begin{table*}[ht]
\centering
\renewcommand{\arraystretch}{0.8} % Compress row height

\small
\resizebox{\textwidth}{!}{
\begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc}
\toprule
\textbf{MoVid-Bench-Motion} & \multicolumn{2}{c|}{\textbf{Body.}} & \multicolumn{2}{c|}{\textbf{Seq.}} & \multicolumn{2}{c|}{\textbf{Dir.}} & \multicolumn{2}{c|}{\textbf{Rea.}} & \multicolumn{2}{c|}{\textbf{Hall.}} & \multicolumn{2}{c}{\textbf{All}} \\
& \textbf{Acc.} & \textbf{Score} & \textbf{Acc.} & \textbf{Score} & \textbf{Acc.} & \textbf{Score} & \textbf{Acc.} & \textbf{Score} & \textbf{Acc.} & \textbf{Score} & \textbf{Acc.} & \textbf{Score} \\ 
\midrule
GT & \textbf{100.00} & \textbf{5.00} & \textbf{100.00} & \textbf{5.00} & \textbf{100.00} & \textbf{5.00} & \textbf{100.00} & \textbf{5.00} & \textbf{100.00} & \textbf{5.00} & \textbf{100.00} & \textbf{5.00} \\
GPT-3.5~\cite{openai2023gpt35} & 24.51 & 2.04 & 30.41 & 2.25 & 27.14 & 2.19 & 39.19 & 2.64 & 58.33 & 3.22 & 31.33 & 2.31 \\
MotionGPT~\cite{jiang2023motiongpt} & 31.22 & 3.98 & 42.69 & \textbf{3.16} & 44.29 & 3.50 & 35.81 & 3.06 & 16.66 & 2.25 & 36.86 & 3.11 \\
MotionLLM~\cite{chen2024motionllm} & 50.49 & 3.55 & 36.84 & 3.14 & 58.57 & 3.76 & 52.70 & 3.58 & 55.56 & 3.39 & 49.50 & 3.49 \\
LLaMo~\cite{li2024human} & 59.30 & 4.01 & 44.01 & 3.12 & 60.91 & 3.99 & 58.21 & 3.64 & 61.17 & 3.53 & 55.32 & 3.67 \\
\midrule
\textbf{ChatMotion(CB)} & \textbf{60.89} & 4.03 & 46.21 & \textbf{3.30} & 62.11 & 4.03 & 59.53 & 3.77 & 68.95 & 3.78 & 56.90 & 3.72 \\
\textbf{ChatMotion} & 60.43 & \textbf{4.08} & \textbf{46.56} & 3.28 & \textbf{64.21} & \textbf{4.11} & \textbf{60.58} & \textbf{3.87} & \textbf{70.39} & \textbf{3.82} & \textbf{58.79} & \textbf{3.80} \\
\midrule
\textbf{MoVid-Bench-Video} & \multicolumn{2}{c|}{\textbf{Body.}} & \multicolumn{2}{c|}{\textbf{Seq.}} & \multicolumn{2}{c|}{\textbf{Dir.}} & \multicolumn{2}{c|}{\textbf{Rea.}} & \multicolumn{2}{c|}{\textbf{Hull.}} & \multicolumn{2}{c}{\textbf{All}} \\
& \textbf{Acc.} & \textbf{Score} & \textbf{Acc.} & \textbf{Score} & \textbf{Acc.} & \textbf{Score} & \textbf{Acc.} & \textbf{Score} & \textbf{Acc.} & \textbf{Score} & \textbf{Acc.} & \textbf{Score} \\ 
\midrule
GT & \textbf{100.00} & \textbf{5.00} & \textbf{100.00} & \textbf{5.00} & \textbf{100.00} & \textbf{5.00} & \textbf{100.00} & \textbf{5.00} & \textbf{100.00} & \textbf{5.00} & \textbf{100.00} & \textbf{5.00} \\
GPT-3.5~\cite{openai2023gpt35} & 2.40 & 1.23 & 1.39 & 1.00 & 4.65 & 1.09 & 5.41 & 1.65 & 0.00 & 0.94 & 3.03 & 1.26 \\
Video-LLAVA~\cite{lin2023video} & 33.53 & 2.76 & 25.46 & 2.72 & 41.86 & 2.84 & 52.97 & 3.28 & 58.83 & 1.89 & 42.53 & 2.70 \\
MotionLLM~\cite{chen2024motionllm} & 34.13 & 2.93 & 32.87 & 2.92 & 44.18 & 3.14 & 63.20 & 3.55 & 70.59 & 2.30 & 49.00 & 2.97 \\
LLaMo~\cite{li2024human} & 33.83 & 2.85 & 36.01 & 3.11 & 45.50 & 3.32 & 67.59 & 3.73 & 72.81 & 2.25 & 52.33 & 3.10 \\
\midrule
\textbf{ChatMotion(CB)} & \textbf{38.31} & \textbf{3.40} & 36.80 & 3.17 & 47.22 & 3.59 & 70.89 & 3.85 & 73.22 & \textbf{2.35} & 53.51 & 3.19 \\
\textbf{ChatMotion} & 38.06 & 3.34 & \textbf{37.39} & \textbf{3.18} & \textbf{47.92} & \textbf{3.65} & \textbf{72.16} & \textbf{3.99} & \textbf{74.01} & 2.30 & \textbf{54.96} & \textbf{3.25} \\
\bottomrule
\end{tabular}
}
\caption{Comparison between ChatMotion and existing Motion LLMs on the MoVid-Bench. The top part of the table presents motion-related results, and the bottom part presents video-related results. Higher accuracy and score values indicate better performance.}
\label{table:evaluation_on_movid}
\end{table*}

\begin{table*}[t]
\centering
\renewcommand{\arraystretch}{0.8} % 减小表格行高
\setlength{\tabcolsep}{6pt}      % 调整列间距
\scriptsize                      % 使用较小的字体, 也可以试 \footnotesize
\resizebox{\linewidth}{!}{      % 将表格宽度强制缩放到当前双栏的 linewidth
\begin{tabular}{l|c|cccc|ccc}
\toprule
\textbf{Model} & \textbf{Pred. type} & \textbf{Overall} $\uparrow$ & \textbf{Action} $\uparrow$ & \textbf{Direction} $\uparrow$ & \textbf{Body Part} $\uparrow$ & \textbf{Before} $\uparrow$ & \textbf{After} $\uparrow$ & \textbf{Other} $\uparrow$ \\ 
\midrule
MotionCLIP-M~\cite{tevet2022motionclip} & cls. & 0.430 & 0.485 & 0.361 & 0.272 & 0.372 & 0.321 & 0.404\\ 
MotionCLIP-R~\cite{tevet2022motionclip} & cls. & 0.420 & 0.489 & 0.310 & 0.250 & 0.398 & 0.314 & 0.387\\ 
MotionLLM~\cite{chen2024motionllm} & gen. & 0.436 & 0.517 & 0.354 & 0.154 & 0.427 & 0.368 & 0.529 \\ 
LLaMo~\cite{li2024human} & gen. & 0.458 & 0.525 & 0.398 & 0.224 & 0.443 & 0.392 & 0.518 \\ 
\midrule
\textbf{ChatMotion(CB)} & gen. & 0.467 & 0.534 & 0.410 & \textbf{0.272} & 0.445 & 0.396 & 0.536 \\
\textbf{ChatMotion} & gen. & \textbf{0.473} & \textbf{0.537} & \textbf{0.412} & 0.265 & \textbf{0.451} & \textbf{0.406} & \textbf{0.537} \\
\bottomrule
\end{tabular}
} % 结束 \resizebox
\caption{Comparison on BABEL-QA dataset. Higher scores indicate better performance. The results for ChatMotion's two methods are also included.}
\label{table:babel_qa_comparison}
\end{table*}

\section{Experimental Setup}


\paragraph{Datasets}
We evaluate ChatMotion on general human motion understanding benchmarks including Movid-bench~\cite{chen2024motionllm}, BABEL-QA~\cite{endo2023motion} and MVbench~\cite{li2024mvbench}, as well as Mo-Repcount~\cite{li2024human} for fine-grained motion capture capabilities. MoVid-Bench specifically assesses the model's ability to understand human behavior in both motion and video contexts. It consists of 1,350 data pairs, with 700 motion and 650 video samples, covering diverse daily scenarios in real-world. In addition, ChatMotion is tested on BABEL-QA and MVbench to evaluate motion-based and video-based question answering respectively. 

\paragraph{Tasks and Metrics}
ChatMotion is evaluated on tasks including action recognition, motion reasoning, and question answering. For MoVid-Bench, we follow established LLM evaluation metrics, assessing body-part recognition, sequential analysis, directionality, reasoning, and hallucination control in both motion and video contexts. BABEL-QA uses similar metrics with a focus on motion-related question answering, while Mo-Repcount employs specialized metrics like OBO, MAE, OBZ, and RMSE for fine-grained motion tracking accuracy. In the MVBench video understanding evaluation, we respond to multiple-choice questions by selecting the most suitable option as outlined in.

\paragraph{Baselines}
For our baselines, we select SoTA Motion LLMs for human-centric motion understanding, e.g., LLaMo~\cite{li2024human}, MotionLLM~\cite{chen2024motionllm} and MotionGPT~\cite{jiang2023motiongpt}. These models are widely recognized for their ability to process and understand human motion in both video and action contexts. For ChatMotion, \textbf{ChatMotion(CB)} and \textbf{ChatMotion} denote the versions using confidence-based and motion-aware aggregation, respectively. Through extensive comparison, our results highlight ChatMotion's exceptional ability to handle complex human motion understanding tasks, outperforming the selected baselines across a range of evaluation metrics.


\begin{figure*}[h]
    \centering
    \includegraphics[width=1\textwidth]{latex/figures/Explanation.png}
    \caption{Examples of ChatMotion's responses in various human activities and sports, demonstrating its reasoning skills and specialized knowledge in active, movement-heavy scenarios.}
    \label{fig:qualitative}
\end{figure*}

\begin{table*}[ht]
\centering
\renewcommand{\arraystretch}{0.8} % 减小表格行高
\setlength{\tabcolsep}{6pt}      % 调整列间距
\scriptsize 
\resizebox{\linewidth}{!}{
\begin{tabular}{l|l|c|c|c|c|c|c|c|c|c}
\hline
\textbf{Model} & \textbf{LLM} & \textbf{Frames} & \textbf{AL} & \textbf{AP} & \textbf{AS} & \textbf{EN} & \textbf{FA} & \textbf{FP} & \textbf{UA} & \textbf{Avg.} \\
\hline
\text{Otter-V} & \text{Llama-7B} & 16 & 23.5 & 23.0 & 23.0 & 23.5 & 27.0 & 22.0 & 29.5 & 24.5 \\
\text{mPLUG-Owl-V } & \text{Llama-7B} & 16 & 23.0 & 28.0 & 22.0 & 26.0 & 29.0 & 24.0 & 29.0 & 25.8 \\
\text{VideoChatGPT } & \text{Vicuna-7B} & 100 & 20.0 & 26.0 & 23.5 & 29.5 & 22.5 & 22.5 & 29.0 & 25.2 \\
\text{VideoLLaMA } & \text{Vicuna-7B} & 16 & 22.5 & 25.5 & 27.5 & 30.0 & 29.0 & 32.5 & 39.0 & 29.4 \\
\text{VideoChat} & \text{Vicuna-7B} & 16 & 27.0 & 26.5 & 33.5 & 23.5 & 33.5 & 26.5 & 40.5 & 30.1 \\
\text{Video-LLAVA} & \text{Vicuna-7B} & 8 & 22.5 & 25.5 & 29.5 & 29.0 & 24.5 & 28.5 & 24.5 & 26.3 \\
\text{GPT-4v} & \text{GPT-4} & 16 & 40.5 & 63.5 & 55.5 & 31.0 & 46.5 & 47.5 & 73.5 & 51.1 \\
\text{VideoChat2} & \text{Vicuna-7B} & 16 & 23.0 & \textbf{66.0} & 47.5 & \textbf{35.0} & \textbf{49.5} & 49.0 & 60.0 & 47.1 \\
\text{MotionLLM} & \text{Vicuna-7B} & 8 & 33.0 & 29.5 & 32.5 & 29.0 & 31.5 & 28.5 & 37.5 & 31.6 \\
\hline
\textbf{ChatMotion(CB)} & \textbf{Agent} & \textbackslash & 42.0 & 65.5 & 56.0 & 33.0 & 48.0 & 50.5 & 72.0 & 52.4 \\
\textbf{ChatMotion} & \textbf{Agent} & \textbackslash & \textbf{43.0} & 65.5 & \textbf{58.0} & 34.0 & 49.0 & \textbf{51.0} & \textbf{74.0} & \textbf{53.2} \\
\hline
\end{tabular}
}
\caption{Performance of various models across different metrics, including GPT-4v, VideoChat2, MotionLLM and ChatMotion.}
\label{MVBench}
\end{table*}

\begin{table}[h!]
\small % Reduce font size for compact layout
\centering
\begin{tabular}{p{2.2cm}|p{0.8cm} p{0.8cm} p{0.8cm} p{0.8cm}}
\toprule
\textbf{Model}   & \textbf{OBO} & \textbf{MAE} & \textbf{OBZ} & \textbf{RMSE} \\ \midrule
EScounts         & 0.397        & 0.291        & 0.198        & 5.58          \\
PoseRAC         & 0.382        & 0.312        & 0.204        & 5.95          \\
TransRAC         & 0.276        & 0.444        & 0.105        & 8.56          \\
RepNet          & 0.009        & \textbackslash          & \textbackslash            & \textbackslash             \\ 
MotionLLM      & 0.011   & \textbackslash   & \textbackslash   & \textbackslash  \\
LLaMo   & 0.389 & 0.324 & 0.222 & 6.15 \\
\midrule
\textbf{ChatMotion(CB)} & \textbf{0.412} & 0.279 & 0.229 & 5.33 \\
\textbf{ChatMotion} & 0.410 & \textbf{0.271} & \textbf{0.240} & \textbf{5.21} \\
\bottomrule
\end{tabular}
\caption{Motion and video details capture evaluation on Mo-RepCount.}

\label{table:mo-repcount-evaluation}
\vspace{-0.22in}
\end{table}


\section{Results}




\subsection{Quantitative Analysis}




\paragraph{Evaluation on Motion Understanding in MoVid-Bench.}
% Table~\ref{table:expected_results} 
% presents the quantitative results of various motion-based LLMs on MoVid-Bench-Motion. Both ChatMotion and its motion-aware variant, ChatMotion(MA), outperform existing baselines across all metrics. Specifically, ChatMotion(MA) achieves an accuracy of 58.79\% and a score of 3.80, surpassing the LLaMo baseline by 3.47\% in accuracy and 0.13 in score. Notable improvements are observed in hallucination control, with an accuracy of 70.39\%, compared to LLaMo’s 61.17\%, attributed to the model’s superior synergy of different advanced motion LLMs with a reasonable selection strategy. These results highlight the effectiveness of motion-aware agent aggregation in improving motion recognition accuracy and reducing spurious generations in real-world sequences.

Table~\ref{table:evaluation_on_movid} compares the performance of motion-based LLMs on MoVid-Bench-Motion. Both ChatMotion(CB) and ChatMotion outperform existing baselines across all metrics. ChatMotion achieves an accuracy of 58.79\% and a score of 3.80, surpassing LLaMo by 3.47\% in accuracy and 0.13 in score. It also demonstrates strong hallucination control, achieving 70.39\% accuracy compared to LLaMo’s 61.17\%, underscoring the effectiveness of ChatMotion’s multi-model integration via its robust selection strategy. 

Previous models, such as MotionLLM and MotionGPT, lose fine-grained motion details due to motion discretization, leading to lower performance. Although LLaMo improves motion encoding, its single LLM-based structure introduces biases that limit its motion understanding capabilities. In contrast, ChatMotion leverages multi-agent collaboration and multi-model aggregation to enhance motion understanding. This approach reduces biases inherent in single LLM-based motion models and improves performance in motion sequence analysis. By integrating multiple agents, ChatMotion achieves greater robustness, demonstrating its superior capabilities to capture diverse motion dynamics and delivers more accurate, reliable results in complex motion understanding tasks.

\paragraph{Evaluation on Video Understanding in MoVid-Bench.}
ChatMotion(CB) demonstrates improvements across multiple metrics on MoVid-Bench-Video as shown in Table~\ref{table:evaluation_on_movid}, achieving an overall accuracy of 53.51\% and a score of 3.19, surpassing baseline models in all evaluated tasks. This performance gain is due to its effective aggregation of diverse video analysis perspectives, combined with confidence scores to ensure more reliable and stable reasoning. Furthermore, ChatMotion, with its motion-aware mechanism, further refines the analysis by better handling motion-related tasks, surpassing ChatMotion(CB) with an accuracy improvement of 1.45\% and a score increase of 0.06. This enhancement allows it to more effectively aggregate and analyze motion data, pushing performance beyond that of standard models. These innovations in model design, coupled with the synergistic effects of specialized modules, allow ChatMotion(CB) and ChatMotion to set new benchmarks in multimodal human motion analysis, outperforming existing LLM-based motion models across multiple tasks and metrics.


\paragraph{Evaluation on BABEL-QA.}
We evaluated ChatMotion on the BABEL-QA dataset to assess its performance in responding to complex motion-based queries. As shown in Table \ref{table:babel_qa_comparison}, both ChatMotion(CB) and ChatMotion outperform other LLM-based motion models across several metrics. ChatMotion(CB) achieves an overall score of 0.467, while ChatMotion further improves this to 0.473, demonstrating its enhanced capability. This improvement is due to ChatMotion's motion-aware mechanism, which takes both motion inputs and candidate results into account. By leveraging LLaMo's advanced multimodal capabilities, ChatMotion esures more robust and stable results. Despite some limitations on specific metrics, ChatMotion compensates for these and delivers superior overall results. These advancements position ChatMotion as a new benchmark in motion-based question answering, highlighting the effectiveness of multimodal aggregation and motion-aware mechanisms in achieving more accurate and reliable results.


\paragraph{Evaluation on MVBench.}
We evaluated ChatMotion on the MVBench dataset to assess its performance in video question answering across seven motion understanding sub-tasks. As shown in Table~\ref{MVBench}, ChatMotion(CB) outperforms MotionLLM~\cite{chen2024motionllm}, the LLM-based motion understanding model, achieves an average score of 52.4, while ChatMotion increases this to 53.2. These results highlight the efficacy of ChatMotion’s multi-agent framework, which reduces biases inherent to LLM-based motion models by incorporating dynamic function calls. Performance gains are particularly evident in most metrics, demonstrating the advantages of multi-agent integration for robust motion understanding. While slight performance gaps persist in specific tasks compared with expert models (e.g., EN of VideChat2), the overall improvement over the LLM-based motion model, MotionLLM, remains statistically better.



\paragraph{Evaluation on Mo-Repcount}
To evaluate ChatMotion's performance on fine-grained motion tasks, we benchmarked it on Mo-Repcount against SoTA Motion LLMs. The results in Table~\ref{table:mo-repcount-evaluation} show that ChatMotion outperforms LLaMo by 4\%-8\% across all metrics, demonstrating ChatMotion's advanced capability to aggregate the strengths of specialized models and achieve superior performance in fine-grained motion tasks.

% In conclusion, these results position ChatMotion as a new benchmark for human motion video question answering, demonstrating the efficacy of multi-agent systems and motion-aware mechanisms in enhancing performance on complex, human-centric understanding tasks.


% \paragraph{Evaluation on Repcount}




\subsection{Qualitative Analysis}

Qualitative results, as shown in Fig.~\ref{fig:qualitative}, demonstrate ChatMotion's superior capabilities in understanding human motion across diverse scenarios. In a task where a human expresses sadness, using both video and motion inputs, MotionLLM fails to provide a correct interpretation, while LLaMo identifies the emotion, though with some ambiguity. Notably, ChatMotion excels in tasks that current LLM-based motion models struggle with, including fine-grained counting and comprehensive analyses utilizing RAG, alongside detailed comparisons of motion-capture and video data. These results showcase the model's ability to handle complex, multimodal motion tasks that require context-sensitive reasoning beyond the capabilities of existing models.


\section{Conclusion}

% In this paper, we introduced ChatMotion, a multi-agent framework that unifies large language models with specialized motion-analysis modules to overcome the limitations of single-model systems. By dynamically decomposing tasks, aggregating diverse outputs, and rigorously selecting the most reliable results, ChatMotion addresses biases in motion understanding and provides robust, contextually informed analyses. Experiments on human motion benchmarks, including MoVid-Bench and BABEL-QA, demonstrated improved gains in both accuracy and adaptability. Future directions include refining the Aggregator for even richer human-centric applications and scaling ChatMotion to more interactive, real-time motion analysis tasks. We hope this work inspires ongoing research in the field of multimodal agent architectures for human motion understanding.
In this paper, we introduced ChatMotion, a sophisticated multi-agent framework that integrates large language models with specialized motion-analysis modules to address the limitations inherent in single-model systems. By dynamically breaking down complex tasks, aggregating diverse model outputs, and carefully selecting the most reliable results, ChatMotion effectively mitigates biases in motion understanding and delivers robust, context-aware analyses. Through experiments conducted on human motion benchmarks such as MoVid-Bench and BABEL-QA, we demonstrated significant improvements in both accuracy and adaptability across various motion tasks.
% Looking ahead, future work will focus on enhancing the Aggregator to enable even richer applications centered around human motion and expanding ChatMotion's capabilities to handle more interactive, real-time motion analysis scenarios. We believe this research lays a foundation for continued advancements in the development of multimodal agent architectures, particularly in the area of human motion understanding, and we hope it will inspire further exploration in this emerging field.

% \section{Limitations}

% While ChatMotion demonstrates improved performance in human motion analysis, several limitations remain:
% \begin{itemize}
%     \item \textbf{Computational Overhead}: The multi-agent architecture and multi-model aggregation increase computational costs compared to single-model systems, potentially limiting real-time applications.
    
%     \item \textbf{Dependency on Base Models}: Performance relies on the quality and biases of the underlying MLLMs (e.g., LLaMo, MotionLLM), which may inherit limitations from their training data.
    
%     \item \textbf{Generalization to Unseen Tasks}: While effective on benchmark datasets, the framework's adaptability to entirely novel motion analysis scenarios requires further validation.
    
%     \item \textbf{Annotation Bottlenecks}: The expert-validated dataset, while high-quality, is limited in scale due to the manual annotation process, potentially restricting model generalization.
% \end{itemize}

% \section{Ethic Statement}
% This work raises several ethical considerations:
% \begin{itemize}
%     \item \textbf{Privacy}: While using anonymized social media data mitigates direct privacy risks, public posts may still contain sensitive personal information. We advocate strict adherence to data protection regulations.
    
%     \item \textbf{Dual Use}: The motion analysis framework could potentially be repurposed for surveillance. We strongly discourage non-consensual applications and support ethical AI governance policies.
% \end{itemize}

% \noindent We commit to open collaboration with domain experts to address these challenges responsibly.

\clearpage
\bibliography{acl_latex}



\end{document}
