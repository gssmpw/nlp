\section{Related Work}
We propose that data can be decomposed into multiple simple structures where complex feature interactions may cancel out, enabling models trained on these structures to make optimal decisions for instances within them. This perspective aligns with the broader concept of training data optimization or instance selection, which seeks to select or organize data to enhance model performance. 

Several instance-based learning methods, such as nearest neighbor rules \citep{hart_condensed_1968, gates_reduced_1972}, data density-based selection \citep{carbonera_density-based_2015}, and ranking-based approaches \citep{cavalcanti_ranking-based_2020, ghadikolaei_learning_2019}, aim to optimize training data by removing noise and redundancy \citep{olvera-lopez_review_2010}. Advanced techniques, like mixed-integer programming \citep{ghadikolaei_learning_2019} and local search algorithms \citep{lin_simultaneous_2021, neri_local_2020}, further compress datasets without sacrificing performance. As discussed, none of these methods explicitly try to model the underlying local simple structures but focus on improving accuracy and training time.

In machine learning, several methodologies incorporate the idea of approximating data using simple models, either directly or indirectly. Boosting, autoencoders, and collaborative filtering all use the idea of approximating the data using simple models or similar instances directly or indirectly.

Boosting attempts to exploit different structures in the data by learning a series of weak learners. 
The underlying idea is to combine weak learners to make a decision with outstanding performance~\citep{meir_introduction_2003},~\citep{freund_decision-theoretic_1997},~\citep{friedman_greedy_2001},~\citep{chen_xgboost_2016}.
Boosting attempts to fit close to the data, so the learned weak structures are dependent on the residuals from previous weak learners; thus, patterns may not be intuitive and lack model explainability~\citep{caruana_intelligible_2015}.  

The autoencoder compresses input into a meaningful latent representation and disentangles variations that are features~\citep{bank_autoencoders_2020},~\citep{bengio_representation_2013} akin to identifying simple structures in feature space as latent representations combine similar instances~\citep{chung_learning_2017},~\citep{tschannen_recent_2018}. However, nonlinearities can alter original data structures, forming new, complex ones even for simpler linear relations.

In recommendation systems and fraud detection, data comprises distinct cohorts as people have different characteristics, but individuals with similar traits form a simple structure. 
Collaborative filtering uses a user-item matrix and matches users with similar interests and preferences by calculating similarity~\citep{su_survey_2009}, deep learning-based recommendation systems learn complex non-linear relations between users and items and make recommendations~\citep{he_neural_2017},~\citep{elkahky_multi-view_2015},~\citep{zhang_deep_2019}.
However, existing methods focus on finding the most similar instances to a query versus which instances form simple structures. 

Our approach of approximating the underlying data distribution through an ensemble of simple structures shares conceptual parallels with Gaussian Mixture Models (GMMs) and clustering. GMMs model data as a combination of Gaussian distributions, probabilistically assigning data points to clusters using the Expectation-Maximization (EM) algorithm \citep{bishop_pattern_2006}. However, GMMs aim to fit the entire dataset to a predefined number of distributions, often missing meaningful local structures and performing poorly in real-world applications compared to advanced models like XGBoost and Neural Networks \citep{wan_novel_nodate}.

In contrast, our method assumes datasets comprise multiple, potentially overlapping distributions and focuses on identifying simple structuresâ€”subsets with locally significant, linearly separable classes. Unlike GMMs, which provide a global clustering solution, our approach uncovers interpretable local patterns that highlight distinct subgroups, offering deeper insights into data heterogeneity.


Unlike traditional top-down methods that focus on minimizing global loss or boosting approaches that iteratively fit closer to the data without identifying inherent structures, our bottom-up method explicitly searches for simple structures. Instance-based learning optimizes data to reduce global loss, recommendation systems identify similar instances, and GMMs cluster data in feature space; however, none of these approaches explicitly address the separability of classes in local regions, which is central to the concept of simple structures. By focusing on class separability within localized regions, our method offers a distinct and interpretable approach to modeling data. This is particularly relevant in real-world scenarios, such as medical data, where identifying distinct distributions can support precision healthcare by tailoring decisions to specific populations \citep{colijn_toward_2017}.