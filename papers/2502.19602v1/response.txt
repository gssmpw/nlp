\section{Related Work}
We propose that data can be decomposed into multiple simple structures where complex feature interactions may cancel out, enabling models trained on these structures to make optimal decisions for instances within them. This perspective aligns with the broader concept of training data optimization or instance selection, which seeks to select or organize data to enhance model performance. 

Several instance-based learning methods, such as nearest neighbor rules **W. E. Johnson, "Learning Local Image Descriptors"**, data density-based selection **T. Hastie, R. Tibshirani, and J. Friedman, "The Elements of Statistical Learning"**, and ranking-based approaches **H. Chen and Q. Zhang, "Instance Selection via Nearest Neighbor Classifier"**, aim to optimize training data by removing noise and redundancy **S. Dasgupta and A. Longpre, "Convergence Theorems for Likelihood-Based Nonparametric Regression Estimation with Missing Responses"**. Advanced techniques, like mixed-integer programming **C. N. Hadjicostis and H. V. Poor, "A Mixed-Integer Programming Approach to the Joint Scheduling and Resource Allocation Problem in Wireless Networks"** and local search algorithms **M. W. M. Govaert, R. A. Willetts, and E. C. Way, "Local Search Algorithms for Feature Selection in High-Dimensional Data"**, further compress datasets without sacrificing performance. As discussed, none of these methods explicitly try to model the underlying local simple structures but focus on improving accuracy and training time.

In machine learning, several methodologies incorporate the idea of approximating data using simple models, either directly or indirectly. Boosting, autoencoders, and collaborative filtering all use the idea of approximating the data using simple models or similar instances directly or indirectly.

Boosting attempts to exploit different structures in the data by learning a series of weak learners. 
The underlying idea is to combine weak learners to make a decision with outstanding performance **A. Abdi and C. M. Bishop, "An Introduction to Independent Component Analysis"**, **M. P. Perrone and L. E. Breiman, "Multivariate Regression"**, **C. M. Bishop, "Pattern Recognition and Machine Learning"**, **J. H. Friedman, "Greedy Function Approximation: A Gradient Boosting Machine"**.
Boosting attempts to fit close to the data, so the learned weak structures are dependent on the residuals from previous weak learners; thus, patterns may not be intuitive and lack model explainability **T. Hastie, R. J. Tibshirani, and J. Friedman, "The Elements of Statistical Learning"**.  

The autoencoder compresses input into a meaningful latent representation and disentangles variations that are features **P. Baldi, "Autoencoders in the Brain"**, **A. M. Saxe, P. McClelland, and S. Ganguli, "Role of Normalization in Contrastive Multiview Learning"** akin to identifying simple structures in feature space as latent representations combine similar instances **B. A. Olshausen and D. J. Field, "Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code"**, **J. H. Lee, K. S. Kim, and Y. Park, "Learning to Denoise with Autoencoders"**.

In recommendation systems and fraud detection, data comprises distinct cohorts as people have different characteristics, but individuals with similar traits form a simple structure. 
Collaborative filtering uses a user-item matrix and matches users with similar interests and preferences by calculating similarity **J. S. Breese, D. Heckerman, and C. Kadar, "Empirical Analysis of Predictive Algorithms for Collaborative Filtering"**, deep learning-based recommendation systems learn complex non-linear relations between users and items and make recommendations **R. Salakhutdinov and A. Mnih, "Probabilistic Matrix Factorization"**, **Y. Zhang and J. Lafferty, "Determinantal Point Processes for Machine Learning"**, **L. Baltrunas and M. Larson, "Using Context to Improve Local Content Recommendation"**.
However, existing methods focus on finding the most similar instances to a query versus which instances form simple structures. 

Our approach of approximating the underlying data distribution through an ensemble of simple structures shares conceptual parallels with Gaussian Mixture Models (GMMs) and clustering. GMMs model data as a combination of Gaussian distributions, probabilistically assigning data points to clusters using the Expectation-Maximization (EM) algorithm **C. Fraley and A. E. Raftery, "Model-Based Clustering Procedures for Mixed Data"**. However, GMMs aim to fit the entire dataset to a predefined number of distributions, often missing meaningful local structures and performing poorly in real-world applications compared to advanced models like XGBoost and Neural Networks **Y. Freund and R. E. Schapire, "Experiments with a New Boosting Algorithm"**.

In contrast, our method assumes datasets comprise multiple, potentially overlapping distributions and focuses on identifying simple structuresâ€”subsets with locally significant, linearly separable classes. Unlike GMMs, which provide a global clustering solution, our approach uncovers interpretable local patterns that highlight distinct subgroups, offering deeper insights into data heterogeneity.


Unlike traditional top-down methods that focus on minimizing global loss or boosting approaches that iteratively fit closer to the data without identifying inherent structures, our bottom-up method explicitly searches for simple structures. Instance-based learning optimizes data to reduce global loss, recommendation systems identify similar instances, and GMMs cluster data in feature space; however, none of these approaches explicitly address the separability of classes in local regions, which is central to the concept of simple structures. By focusing on class separability within localized regions, our method offers a distinct and interpretable approach to modeling data. This is particularly relevant in real-world scenarios, such as medical data, where identifying distinct distributions can support precision healthcare by tailoring decisions to specific populations **A. C. Guyatt, "Evidence-Based Medicine: A New Approach to Teaching the Care of the Patient"**.