% !TEX program = pdflatex

\documentclass{article}
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{stmaryrd}
\usepackage{array}
\usepackage{float}
\usepackage[table]{xcolor}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{fancyhdr}
\usepackage{booktabs}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{endnotes}
\usepackage{url}
\usepackage{authblk}
\usepackage[T2A]{fontenc}
\usepackage[russian,english]{babel}

\definecolor{darkgreen}{rgb}{0,0.5,0}
\definecolor{darkred}{rgb}{0.6,0.0,0}
\definecolor{teal}{rgb}{0.00, 0.62, 0.45}
\definecolor{coral}{rgb}{0.83, 0.37, 0.00}
\newcommand{\usc}{\underline{\hspace{0.4cm}}\ }
\newcommand{\angles}[1]{\langle {#1} \rangle\ }
\newcolumntype{L}{>{\raggedright\arraybackslash}X}
\newcommand{\colorA}      [1]{\textcolor{Maroon}      {#1}}
\newcommand{\colorB}      [1]{\textcolor{NavyBlue}      {#1}}
\newcommand{\heatmap}[1]{%
    \ifdim#1pt<50pt\cellcolor{gray!20}#1\%\else%
    \ifdim#1pt>72pt\cellcolor{blue!50}#1\%\else%
    \ifdim#1pt>60pt\cellcolor{blue!20}#1\%\else%
    \ifdim#1pt>50pt\cellcolor{blue!10}#1\%\fi\fi\fi\fi}
\newcommand{\emptycell}{\cellcolor{white} }


\usepackage{expex}

\begin{document}


\author[1]{Imry Ziv}
\author[2]{Nur Lan}
\author[2]{Emmanuel Chemla}
\author[1]{Roni Katzir}


\affil[1]{Tel Aviv University
}
\affil[2]{École Normale Supérieure}


\title{Large Language Models as Proxies for Theories of Human Linguistic Cognition}


\maketitle              %


\begin{abstract}
We consider the possible role of current large language models (LLMs) in the study of human linguistic cognition. We focus on the use of such models as proxies for theories 
of cognition that are relatively linguistically-neutral in their representations and learning but differ from current LLMs in key ways. 
We illustrate this potential use of LLMs as proxies for theories of cognition in the context of two kinds of questions:  (a) whether the target theory accounts for the acquisition of a given pattern from a given corpus; and (b) whether the target theory makes a given typologically-attested pattern easier to acquire than another, typologically-unattested pattern. For each of the two questions we show, building on recent literature, how current LLMs can potentially be of help, but we note that at present this help is quite limited.
\end{abstract}

\noindent \textbf{Keywords:} Learning, Poverty of the stimulus, Linguistic typology, Large language models

\section{Introduction}
\label{sec:intro}

The question of whether current large language models (LLMs) might be relevant scientifically for the study of human linguistic cognition (HLC) is often framed in terms of a perceived threat that LLMs pose to generative linguistics. On the one hand, some researchers have suggested that these models discredit prevailing views within  linguistics \citep{Piantadosi:2023}, while others have suggested that LLMs can teach us nothing about HLC (\citealp{chomsky2023falsepromise}, \citealp{Moro:2023}).

Here we chart a more collaborative path forward for the integration of LLMs into the study of HLC. While the idea that current LLMs are themselves good theories of HLC is untenable (as reviewed below), we note that LLMs are a potentially useful tool that can inform the evaluation of theories of HLC. In this capacity, LLMs can serve as proxies for explicit theories of HLC that are different from LLMs in important ways but are still relatively unbiased with respect to linguistic representations and learning. We take it that much of the recent literature on the potential cognitive relevance of LLMs takes current models to support relatively linguistically neutral theories of this kind. To the extent that this use of LLMs as proxies is justified, they can assist in comparing such relatively unbiased theories of HLC with more traditional theories of HLC coming out of theoretical linguistics, where the combination of representations and learning is often highly biased in favor of linguistic patterns.



We start in section \ref{sec:bg} by briefly reviewing some considerations pertaining to the role of LLMs in studying HLC. We start with a reminder that the study of HLC, like the rest of empirical science, proceeds through inference to the best explanation, with explicit theories competing in light of known observations. We review the recent idea that LLMs themselves can serve as better explanations than generative theories and then summarize some of the reasons noted in the literature to reject this idea. We then consider the idea mentioned above that the best explanation is that HLC is quite different from current LLMs but still involves representations and learning that are relatively linguistically neutral. It is the potential of LLMs to serve as proxies for such a relatively linguistically-neutral theory that will preoccupy us in the remainder of the paper, and in particular the question of whether evidence from LLMs can bear on the question of whether the linguistically-neutral theory is a better explanation than generative approaches. In section \ref{sec:aps} we ask – building on \citealt{WilcoxFutrellLevy:2023} and \citealt{LanChemlaKatzir:2024a}, among others –  whether evidence from LLMs suggests that a relatively linguistically-neutral theory of HLC might succeed in acquiring various linguistic patterns from a developmentally-realistic corpus. In section \ref{sec:typ} we ask – building on \citealt{KalliniPapadimitriouFutrellMahowaldPotts:2024} and others – whether evidence from LLMs suggests that a relatively linguistically-neutral theory of HLC makes attested languages easier to acquire than various unattested variants of those languages. In both cases, we will see that the evidence from LLMs gives no reason to think that a linguistically-neutral theory of HLC could account for the data, which in turn means that current LLMs contribute little to the ability of such linguistically-neutral theories to challenge generative theories. 


\section{Background: LLMs, HLC, and the inference to the best explanation}
\label{sec:bg}

Being part of empirical science, the study of HLC proceeds through inference to the best explanation: explicit theories are evaluated in light of the observations, and those theories that explain the data best are preferred to the other contenders. The theories coming out of generative linguistics over the past 70 years or so are routinely evaluated in this way. These theories are often strongly linguistically biased, in the sense that they make it relatively easy to represent and learn knowledge of the kind found across languages and hard or impossible to represent or learn many other patterns.
Let us use $H_1$ to refer to some concrete instantiation of the generative approach. We will return to the biases embodied by the architecture and representations of $H_1$ in the following sections.






More recently, \citet{Piantadosi:2023} has suggested that current LLMs %
should be treated as explicit theories of HLC that should be taken seriously as part of this evaluation. Following \citet{FoxKatzir:2024} we refer to this new approach to HLC as the \textit{LLM Theory}, and we discuss it in Section \ref{sec:llm:th}, through a concrete instantiation we will call $H_2$. Perhaps in light of the glaring inadequacies of the LLM Theory, some recent literature, including \citealt{WilcoxFutrellLevy:2023} and \citealt{MahowaldIvanovaBlankKanwisherTenenbaumFedorenko:2024}, has advocated for a revision of generative linguistics on the basis of LLMs but without committing to the LLM Theory. We refer to this idea as the \textit{Proxy View}: 

\ex \label{} The Proxy View of LLMs in HLC: current LLMs are not themselves good theories of HLC, but their performance is indicative of the success of some other theory of HLC, presumably more linguistically-neutral than the generative approach, and can inform the inference to the best explanation.
\xe


Since the literature that promotes the Proxy View generally does not make its supported theory explicit, our ability to evaluate it at present is limited. For concreteness, we will outline what such a target theory might look like, a theory that we will refer to as $H_3$ and that addresses at least some of the most immediate inadequacies of the LLM Theory but is still more linguistically neutral than generative proposals. But we can of course have no certainty that this is what \citet{WilcoxFutrellLevy:2023}, \citet{MahowaldIvanovaBlankKanwisherTenenbaumFedorenko:2024}, and others have in mind. We will therefore focus on methodological considerations that pertain to the evaluation of the target theory once it is provided and show how LLMs might bear on this evaluation. We show that, at least at present, LLMs lend no support to the Proxy View.


\subsection{The LLM Theory and its inadequacy}
\label{sec:llm:th}


As mentioned, \citet{Piantadosi:2023} argues for the LLM Theory of HLC. That is, he suggests that current LLMs, taken as real theories of HLC, are a better explanation of the known observations than anything coming out of the generative tradition. This claim is incorrect, for reasons discussed in detail in the literature and rehearsed briefly here. Following \citealt{Katzir:2023} and \citealt{FoxKatzir:2024} we structure our brief discussion of the failure of the LLM Theory around three key issues: (a)~competence vs.\ performance, (b)~correctness vs.\ probability, and (c)~learning and representations.%
\footnote{See  \citealt{KodnerPayneHeinz:2023}, \citealt{MoroGrecoCappa:2023}, \citealt{RawskiBaumont:2023}, and others for further critical discussion of the LLM Theory.}

\textbf{Competence vs.\ performance.}  Humans sometimes struggle with sentences that, given more time and focus, they ultimately accept. Familiar examples include center embedding, as in "The mouse that the cat that the dog chased bit died". Humans also sometimes initially accept sentences that, upon further thought, they reject. Familiar examples include variants of sentences such as the above but in which the verbs and the nouns are not balanced, as in "The mouse that the cat that the dog chased died". Other examples include so-called agreement attraction, as in "The keys to the cabinet is on the table". 

The best explanation to these and other examples, going back to \citealt{Yngve:1960} and \citealt{MillerChomsky:1963}, distinguishes linguistic competence from performance. The competence of English speakers, for example, licenses center-embedded structures regardless of depth, and it requires agreement between verbs and subjects regardless of intervening nouns. Performance factors get in the way and make center embedding and agreement across intervening nouns difficult and error prone. Among other things, the explanation in terms of a distinction between competence and performance accounts for the way center embedding can become easier or harder given various manipulations (tiredness, noise, parallel tasks, etc.).

Differently from common generative proposals, the design and behavior of current LLMs does not suggest a meaningful distinction between competence and performance of the kind discovered in humans: there is no reason to think that more (or less) time and working memory would make them change their behavior on center embedding or agreement. This leaves current LLMs with no good explanation for the increasing difficulty of sentences with increasing levels of center embedding or for the other observations that have been taken to support the distinction between competence and performance. 




\textbf{Correctness vs.\ likelihood.}  Humans recognize some sentences as correct but unlikely (one example again is center embedding as in the example above), and they recognize some other sentences as incorrect but likely (again, unbalanced variants of center embedding and agreement attraction, among other examples).
LLMs provide straightforward information regarding their representation of likelihood. But it remains unclear whether they even have a distinct representation of correctness.\footnote{A potentially important area of research concerns understanding the inner workings of LLMs, sometimes relating the discovered dynamics to linguistic patterns. See \citealt{Lakretz:2019} and \citealt{Ravfogel:2021}, among others. It is conceivable that such investigations will eventually uncover a notion of correctness within LLMs.} %

\textbf{Learning and representations.} The inductive leaps that humans make, sometimes based on very little data, suggest specific representations and biases. This is the essence of so-called \textit{arguments from the poverty of the stimulus} (APS). A particularly well-known APS, originally outlined by \citet{Chomsky:1971} and discussed in detail in much later work, concerns constituency. This is often illustrated with the formation of yes-no questions, as in (\ref{sai}). The generalization that English-learning children reach is that such questions are formed by fronting the auxiliary that is structurally highest, as in (\ref{sai:good}). They do not seem to seriously consider an alternative generalization that says that the fronted auxiliary is the one that is first linearly (which would yield the incorrect (\ref{sai:bad})), even though the data that children are typically exposed to has been argued to be unhelpful in making the choice.\footnote{But see \citealt{{Reali:Christiansen:2005}} for further discussion of the accessibility of subject-aux inversion to acquisition from data alone.} 
\pex \label{sai}
    \a[] \label{sai:main} [The boy who is singing] can dance.
    \a[] \label{sai:good} Can [the boy who is singing] \usc\ dance?
    \a[] \label{sai:bad} *Is [the boy who \usc\ singing] can dance?
\xe

Generative theories of HLC typically derive this inductive leap by assuming that children are born with a combination of representations and learning biases that favor the representation of constituency-based dependencies over dependencies that rely on linear order (in some approaches by preventing the latter from being represented in the first place). Current LLMs, on the other hand, seem to provide no basis for this inductive leap. In fact, recent work by \citet{LanGeyerChemlaKatzir:2022,LanChemlaKatzir:2024b} casts doubt on the ability of networks trained by current methods to even acquire the basic notion of constituency, let alone to explain the preference for constituency-based generalizations over linear-based ones. Not surprisingly, then, \citet{yedetore:2023} find evidence suggesting that at least some neural networks fail to acquire the correct constituency-based generalization from their training data. We will return to APS in section \ref{sec:aps}, where we will consider the possible role of LLMs in reasoning about the inductive leaps predicted by non-LLM theories of HLC.

Evidence about the representations and learning biases of humans also comes from the highly skewed distribution of linguistic patterns cross-linguistically. We can again consider constituency (following \citealt{FoxKatzir:2024}), though we emphasize that a very wide range of cross-linguistic patterns have been discussed in the linguistic literature. We just mentioned that constituency is part of the best explanation of the competence of English speakers, noting its role in the formation of yes-no questions. Constituency is central to the explanation of many other aspects of English, including center-embedding and agreement mentioned earlier, wh-movement discussed in section \ref{sec:aps} below, the computation of meaning, and more. But it seems clear that this is not just an accident of actual English: constituency is part of the best explanation of almost all languages that have been studied closely, which suggests that humans are born with representations and learning biases that favor constituency. Again, the representations and learning biases of current LLMs might not even be able to support the acquisition of constituency, let alone explain its cross-linguistic prevalence. We will return to cross-linguistic evidence in section \ref{sec:typ}, where we will consider the possible role of LLMs in reasoning about the typological patterns predicted by non-LLMs theories of HLC. 


\subsection{LLMs as proxies for good learners}
But if the LLM Theory is discredicted, does this mean that LLMs are irrelevant to the scientific study of HLC? As mentioned above, the Proxy View explains why the answer is no: in principle LLMs can serve as proxies for theories of a third kind, theories that involve relatively linguistically-neutral representations and learning (and are different in this regard from generative theories) but that are not vulnerable to the most immediate arguments against the LLM Theory.

Here we consider two such uses of LLMs as proxies. Following \citealt{LanChemlaKatzir:2024a} we use LLMs as tools for reasoning about whether a relatively linguistically-neutral theory of HLC predicts a given inductive leap based on a given amount of linguistic exposure. And following \citealt{KalliniPapadimitriouFutrellMahowaldPotts:2024} we use LLMs as tools for reasoning about whether a relatively linguistically-neutral theory of HLC derives a learning asymmetry that might be part of an explanation of a typological asymmetry. 

At this point we should mention again an immediate obstacle to the proxy use of LLMs: while recent literature, including  \citet{WilcoxFutrellLevy:2023} and \citet{MahowaldIvanovaBlankKanwisherTenenbaumFedorenko:2024}, suggests that the performance of LLMs supports linguistically-neutral theories of HLC, this literature does not provide much by way of specifics. Until such specifics are provided, expressions of hope about the undisclosed theories remain just that. 

Still, we can outline some methodological considerations regarding the possible role of current LLMs in evaluating a linguistically-neutral theory of HLC once such a theory is made sufficiently explicit. 

Let us illustrate with a concrete target theory, though we emphasize that this is for presentational purposes only and that we obviously cannot tell whether this target theory is at all close to what the literature supporting the Proxy View envisions. Consider then the following theory of HLC, which we will call $H_3$. 
According to $H_3$, the syntax is written and understood as Multiple Context-Free Grammar: any MCFG can be written, using the formalism of \citet{SekiMatsumuraFujiiKasami:1991}, and no other grammar can be written.  The semantics in $H_3$ is model-theoretic and is computed compositionally based on the syntax, while the syntax is blind to most of the semantics. Learning according to this theory follows the principle of Minimum Description Length (MDL; \citealp{Rissanen:1978}). And processing relies on a parser with access to very little working memory and that can output not just parses (for grammatical inputs) but also probabilities.

$H_3$ is quite different from the LLM Theory and does not share the inadequacies of that theory reviewed above. For one thing, $H_3$ has a meaningful distinction between competence and performance: by relying on a limited amount of working memory, the parser will run into trouble with some of the same grammatically correct sentences that humans struggle with such as those with center embedding. $H_3$ also has a meaningful distinction between correct and probable. As to representations and learning, the combination of (a) MCFG as the representational framework for syntax and (b) MDL as the learning criterion will allow the learner to represent and learn constituency and perhaps also constituency-based dependencies. And the combination of those representations with compositional semantics can go a long way toward explaining the centrality of constituency across languages. As to modularity, this is baked into the assumptions mentioned earlier about the information available to the syntax. But while $H_3$ is very different from the LLM Theory, it is also more linguistically-neutral than common generative theories of HLC such as $H_1$. The representational system of $H_1$ would mostly likely include a specific preparation for grammars that involve linguistic patterns attested across langauges, such as constraints on movement commonly known as syntactic islands (\citealt{Ross:1967}), and including the phenomena we discuss in the following section. While $H_3$ makes it possible to state such grammars, it does not postulate anything specific to these patterns inside the theory, and does not provide further explicit biases beyond those present in a constituency-based syntax and model-theoretic semantics. It is therefore conceivable that the performance of LLMs might be informative to some extent about what can be learned and under what conditions under $H_3$, though adequately justifying the Proxy View for $H_3$ would require careful argumentation that goes beyond the present outline. 

\begin{table}
\centering
\small
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|p{5.4cm}|c|c|c|}
\hline
\textbf{Criterion} & \textbf{Generative~Linguistics} & \textbf{LLM Theory} & \textbf{Proxy View} \\
\hline
\textbf{Linguistic Bias} & \cellcolor{green!25} Strong & \cellcolor{red!25} Weak & \cellcolor{green!25} Intermediate \\
\hline
\textbf{Competence vs. Performance} & \cellcolor{green!25} $\checkmark$ & \cellcolor{red!25} $\times$ & \cellcolor{green!25} $\checkmark$ \\
\hline
\textbf{Grammaticality $\neq$ Likelihood} & \cellcolor{green!25} $\checkmark$ & \cellcolor{red!25} $\times$ & \cellcolor{green!25} $\checkmark$ \\
\hline
\textbf{Explanation for learning assymetries} &
\cellcolor{green!25} $\checkmark$ &
\cellcolor{red!25} $\times$ &
\cellcolor{green!25} $\checkmark$ \\
\hline
\textbf{Concrete Instantiation} &
\cellcolor{green!25} $H_1 $&
\cellcolor{red!25} 
$H_2$ &
\cellcolor{green!25} $H_3$ \\
\hline
\end{tabular}

\caption{Comparison of the three theories outlined above on key issues, following~\citealt{FoxKatzir:2024}.} 
\label{tab:theories}
\end{table}






As mentioned, we have no way of telling whether $H_3$ is the theory of HLC that works such as \citealt{WilcoxFutrellLevy:2023} and \citealt{MahowaldIvanovaBlankKanwisherTenenbaumFedorenko:2024} seem to have tried to promote on the basis of LLM performance. None of our methodological discussion below depends in any way on the specific properties of $H_3$: this theory can serve as a concrete example of a relatively linguistically-neutral theory of HLC if having such an example is helpful, but the reader should feel perfectly free to ignore it. It should also be clear that any serious attempt to promote $H_3$ - or any other theory of HLC - as the best explanation would need to examine it in light of the whole body of work within theoretical linguists since the 1950s and that linguists have relied on to argue for theories of HLC that incorporate strong linguistic biases. Since our goal here is merely to illustrate the potential use of LLMs as proxies for linguistically-neutral theories, we will proceed as if this task has already been carried out.


\section{Large Language Models and alignment with the stimulus}
\label{sec:aps}

Consider then a theory of HLC - $H_3$ above if one so wishes, or something else -that is neutral with respect to some linguistic property $PROP$ for which generative theories include a strong linguistic bias. A potential argument against this relatively linguistically-neutral theory and in favor of a generative theory might take the form of an APS: if humans show knowledge of $PROP$ after exposure to a corpus that seems too impoverished to support the learning of $PROP$ by a linguistically-neutral learner, this will support the strongly biased generative theory. The problem is that it is generally very hard to determine what a given learner might acquire from a given corpus, even for a theory such as $H_{3}$ for which both the representations and the learning mechanism are explicitly provided. 

LLMs might be able to help. We can train them on a suitable corpus and then probe their knowledge. Note that this is not entirely straightforward, for various reasons discussed in \citealt{LanChemlaKatzir:2024a}. Most immediately, while we might be able to directly inspect a grammar within a theory of HLC such as $H_3$ and ask whether it has knowledge of $PROP$, we cannot do the same with current LLMs, whose inner workings are extremely complex and remain mostly opaque, as discussed in Section~\ref{sec:llm:th}. Instead of inspection, we might turn to behavioral measures such as acceptability judgments in minimal pairs where one member satisfies $PROP$ and is acceptable and a minimal variant violates it and is unacceptable, a methodology that has been used in \citealt{MarvinLinzen:2018}, \citealt{Hu:2020} and \citealt{WilcoxLevyFutrell:2019}, among others. But while the theory of HLC under consideration might offer such judgments, current LLMs provide likelihood assessments, which are an entirely different thing, as mentioned above. However, if we focus on areas where acceptability and likelihood are reasonably well aligned, we might be justified in looking at whether the LLM assigns a higher probability to the $PROP$-satisfying member of the pair than to the $PROP$-violating one. If this probabilistic preference is sufficiently strong, we might consider the fact that the LLM has acquired this preference as reason to think that the learner under our target theory of HLC would acquire the actual $PROP$. This might be so even if the probabilistic preference of the LLM reflects a flawed approximation of $PROP$ rather than knowledge of $PROP$ in any meaningful sense.

Following \citeauthor{WilcoxFutrellLevy:2023} and \citeauthor{LanChemlaKatzir:2024a}, our success criterion considers the model successful if and only if it assigns a higher probability to the grammatical member of the minimal pair. Specifically, we evaluate probabilities on a critical region token whose preceding context is the same in both the grammatical and the ungrammatical sentence (see Table~\ref{tab:phenomena}, where critical regions are indicated with underscores). Note that this is an extremely lenient notion of success: the model is considered successful even if the preference for the grammatical member of the minimal pair is extremely small, and even if ungrammatical sentences are considered more likely than some (less related) grammatical sentences.

\subsection{Methods}

\begin{table}[htbp]
\begin{tabularx}{\textwidth}{@{}X r@{ }l r@{ }l@{}}
    \toprule
    \textbf{Model} & \multicolumn{2}{l}{\textbf{Train Dataset Size}} & \multicolumn{2}{l}{\textbf{Human Equivalent}} \\
    \midrule
    \textbf{CHILDES LSTM,\ Transformer} & 8.6  & million tokens    & 10      & months \\
    \textbf{BabyLM 10M}                & 10   & million tokens    & 1       & year \\
    \textbf{Wikipedia Transformer}     & 90   & million tokens    & 8       & years \\
    \textbf{BabyLM 100M}               & 100  & million tokens    & 9       & years \\
    \textbf{bert-based-uncased}        & $\approx$ 3.5 & billion tokens    & 320     & years \\
    \textbf{GPT2}                      & $\approx$ 8   & billion tokens    & 730     & years \\ 
    \textbf{llama3.2-3b}               & $\approx$ 9   & trillion tokens   & 821,250 & years \\
    \bottomrule
    \end{tabularx}
    \caption{Training data size of the eight language models considered here, and
    the human linguistic experience equivalent to these data sizes, following \citeauthor{HartRisley:1995}.}
    \label{tab:model-training-sizes}
\end{table}


Let us illustrate   this kind of reasoning with concrete examples. To do so, we use an empirical setup that consists of eight models of different architectures and training schemes (see Section~\ref{sec:appendix-aps}).\footnote{All experimental material can be found in https://osf.io/5zh6q/.} The models were trained on datasets that roughly correspond to different timeframes of linguistic experience, ranging from 10 months of human experience (CHILDES) through eight years of linguistic experience (English Wikipedia subset) to 320,730 or even 821,250 years of linguistic experience (BERT base uncased, GPT-2 and Llama 3.2-3b respectively).\footnote{More concretely, we use two models from \citeauthor{yedetore:2023}, an LSTM and a Transformer, that were trained on the CHILDES corpus of child-directed speech; a Transformer we trained on a subset of English Wikipedia, for which we used one of the large Transformer architectures used in \citeauthor{yedetore:2023}; the \textit{base-strict} and \textit{base-strict-small} versions of the BabyLM LLMs  from \citet{conll-2023-babylm}, trained on the 100M and 10M BabyLM datasets correspondingly; the \textit{BERT base uncased} version of BERT (\citealt{DBLP:journals/corr/abs-1810-04805}), trained on the BooksCorpus and English Wikipedia datasets; OpenAI’s GPT-2 (\citealt{radford2019language}); Meta's \textit{Llama 3.2-3b} LLAMA model (released September 2024). See Section~\ref{sec:appendix-aps} for further technical detail.} The models were probed on their knowledge of three linguistic phenomena using the success criterion introduced above (see Table~\ref{tab:phenomena}). The linguistic test cases presented in the sections below show that all models, despite being trained on multitudes of data, perform around chance level and worse on the lenient success criterion. This result is not optimistic for LLMs as a proxy for an adequate theory of HLC.


\begin{table}[h]
\centering
\small
\renewcommand{\arraystretch}{1.3} %
\setlength{\tabcolsep}{8pt} %
\begin{tabular}{p{2.5cm} p{7.5cm}}  
\toprule
\textbf{Phenomenon} & \textbf{Example (\textcolor{teal}{Grammatical}/\textcolor{coral}{*Ungrammatical})} \\ 
\midrule

\multirow{2}{=}{Across-the-board movement (ATB)}  
& \textcolor{teal}{Which boy did you say that Kim hated and Mary loved \underline{yesterday}?} \\  
& \textcolor{coral}{* Which boy did you say that Kim hated and Mary loved \underline{Ann} yesterday?} \\  
\midrule

\multirow{2}{=}{Parasitic gaps (PG)}  
& \textcolor{teal}{I know who John's talking to is going to annoy \underline{soon}.} \\  
& \textcolor{coral}{* I know who John's talking to is going to annoy \underline{you} soon.} \\  
\midrule

\multirow{4}{=}{That-trace effects (TTE)}  
& \textcolor{teal}{Who did you say that \underline{Sue} loves?} \\  
& \textcolor{teal}{Who did you say \underline{loves} Sue?} \\  
& \textcolor{teal}{Who did you say \underline{Sue} loves?} \\  
& \textcolor{coral}{* Who did you say that \underline{loves} Sue?} \\  
\bottomrule
\end{tabular}
\caption{Examples of linguistic phenomena with grammatical and ungrammatical sentences. Model probabilities are compared on the underscored critical regions.}
\label{tab:phenomena}
\end{table}

\subsection{Test Case: Across-the-board movement and parasitic gaps}
\label{sec:atb}

Above we mentioned displacement phenomena, illustrating with the position of the auxiliary in yes-no questions in English. Displacement phenomena have featured in recent discussions of the role of LLMs in evaluating APSs, both for the placement of the auxiliary (see, e.g., \citealt{yedetore:2023}) and for wh-movement (see \citealt{ChowdhuryZamparelli:2018}, \citealt{WilcoxFutrellLevy:2023}, and \citealt{LanChemlaKatzir:2024a}). The latter will concern us in the current two subsections. Here is a simple illustration:
\ex \label{ex:wh:book} [Which book] did you say that Mary read \usc\ last week?
\xe

The common analysis of wh-movement in generative theories involves a constituent that is attached in one position and is then re-attached higher up in the structure. In (\ref{ex:wh:book}), the constituent \textit{[which book]} is attached within the embedded clause as a sister to \textit{Mary} and is then re-attached at or near the root of the matrix clause, which among other things leads to it being pronounced in the beginning of the sentence. 

Some work within generative linguistics (\citealt{PearlSprouse:2013}, \citealt{Philips:2011}) suggests that a relatively linguistically-neutral theory cannot account for the full knowledge of wh-movement given the kind of data that children are exposed to, an instance of the APS. This is not a consensus view, however, and some recent work such as \citeauthor{WilcoxFutrellLevy:2023} suggests that LLMs undermine this APS and that these models show that a linguistically neutral theory would, in fact, acquire a full knowledge of wh-movement from a developmentally-realistic corpus. Following \citeauthor{LanChemlaKatzir:2024a} let us show how LLMs (used as proxies for an undisclosed linguistically-neutral theory of HLC) might bear on the APS under consideration and why they currently do nothing to undermine it.

Before proceeding, let us reiterate that it is hard to tell whether LLMs are good proxies for the relatively linguistically-neutral theory of HLC that is envisioned in works such as \citet{WilcoxFutrellLevy:2023} in the absence of at least some specifics about that theory. But if they are, and if we focus on cases where correctness and likelihood are reasonably well-aligned, we could reason that if the LLM overwhelmingly assigns much higher probability to the correct member of each minimal pair, this indicates that there was sufficient information in the training data for the target linguistically-neutral theory to have acquired the relevant knowledge. And if the LLM does not perform as well, this suggests that the target theory would also struggle. The former possibility weakens the APS, and the latter strengthens it.

Following \citet{LanChemlaKatzir:2024a} we focus on a family of wh-movement configurations where there is complex interaction between two gaps, in a way that seemingly violates island constraints on syntactic movement (\citealt{Ross:1967}). The first such phenomenon is \textit{across-the-board (ATB) movement} in coordinate structures. While extraction from a single coordinate is ungrammatical, as in \ref{atb:badleftextraction} and \ref{atb:badrightextraction}, it becomes grammatical when extracting from both coordinates, as in \ref{atb:good}:
\pex \label{atb}
    \a[] \label{atb:badleftextraction} *[Which book] did you say that [Kim wrote \usc\ last year] and [Mary read a new novel yesterday]?
    \a[] \label{atb:badrightextraction} *[Which book] did you say that [Kim wrote a new novel last year] and [Mary read \usc\ yesterday]?
    \a[] \label{atb:good} [Which book] did you say that [Kim wrote \usc\ last year] and [Mary read \usc\ yesterday]?
\xe


The second nuance of wh-movement we test is when extraction from an island is made possible by the presence of another gap downstream, a phenomenon known as a \textit{parasitic gap (PG)}: 
\pex \label{pg}
    \a[] \label{pg:bad} *I know who [John's talking to \usc] is going to annoy you soon.
    \a[] \label{pg:good} I know who [John's talking to \usc] is going to annoy \usc\ soon.
\xe


We test four models from Table~\ref{tab:model-training-sizes} on their knowledge of across-the-board movement and parasitic gaps using the minimal pair method. We compare sentences that both have an upstream filler - like \textit{which book} in Example~\ref{atb}, or \textit{who} in Example~\ref{pg} - and differ in whether they are gapped accordingly.\footnote{The paradigm sentences in \cite{LanChemlaKatzir:2024a} were generated by template for ATB and PG. Here we unify the templates for both phenomena so that they use the same lexical choices where relevant (e.g., proper names), and by adding more lexical choices.
From each template, represented by a context-free grammar, we sampled 10,000 sentence pairs and ran them through each model to get the relevant probability values (see Appendix~\ref{sec:appendix-typ}).} As is shown in Figure~\ref{fig:atb-pg-raw}, all prefer the ungrammatical over the grammatical members of the minimal pairs in the vast majority of cases. This failure suggests that the linguistically-neutral theory for which the LLMs are taken to be useful proxies would not acquire wh-movement from a developmentally-realistic corpus. 

\begin{figure}[t]
    \includegraphics[width=0.9\textwidth]{aps-atb-raw.png} \\
    \includegraphics[width=0.9\textwidth]{aps-pg-raw.png}
    \caption{Model accuracy on ATB and PG datasets averaged over five experiment seeds. Accuracy is measured as the ratio of cases where the model assigns a higher probability to the grammatical sentence continuation.}
    \label{fig:atb-pg-raw}
\end{figure}


\subsection{Test Case: that-trace effects}
\label{sec:tte}

Another property of wh-movement in English is that when the moved wh-phrase is a subject, the clause from which it moves cannot have an overt complementizer such as `that':

\pex \label{ex:tte}
    \a[] \label{tte:pthatptrace} *Who did you say that \usc\ \underline{loves} Sue?\textsubscript{(+that, +trace)}
    \a[] \label{tte:pthatmtrace} Who did you say that \underline{Sue} loves \usc?\textsubscript{(+that, -trace)}
    \a[] \label{tte:mthatptrace} Who did you say \usc\ \underline{loves} Sue?\textsubscript{(-that, +trace)}
    \a[] \label{tte:mthatmtrace} Who did you say that \underline{Sue} loves \usc?\textsubscript{(-that, -trace)}
\xe


This asymmetry, known as that-trace effects (TTE), has been thoroughly discussed in theoretical linguistics (see \citealt{Perlmutter:1968}, \citealt{ChomskyLasnik:1977}, and \citealt{Phillips:2013a}, among others). It has been shown that extraction of any kind from clauses with \textit{that} is vanishingly rare in adult-directed corpora, and even rarer in child-directed speech \citet{Phillips:2013a}\footnote{Out of 11,308 analyzed utterances from child-directed speech, there were only two instances of non-subject extraction with \textit{that} and zero instances of subject extraction with \textit{that}, contrasting with 159 instances of object extraction without \textit{that}, and 13 instances of subject extraction without \textit{that}.}. This distribution is claimed (\citealt{ChomskyLasnik:1977}, \citealt{Phillips:2013b}) to be insufficient to warrant acquisition of the paradigm above. We set aside the important theoretical discussion of what bias present in the architecture or representations of $H_1$ underlies the TTE according to generative linguistics.\footnote{See \citet{Sobin:1987}, \citet{Lohndal:2009}, \citet{Boskovic:2016} and \citet{Pesetksy:2017}, among others, for discussion.} Instead, we focus on its status as an APS, which warrants evaluating the performance of the linguistically-neutral LLM contender on it. 

Similarly to the previous case, we test eight LLM models (see Table~\ref{tab:model-training-sizes}). The four-way asymmetry warrants two criteria that constitute knowledge of the TTE: first, given an overt complementizer upstream, the model should prefer an overt subject to a gap in the embedded clause. This preference should translate into probabilities so that $P(loves)$ in \ref{tte:pthatptrace} should be smaller than $P(Sue)$ in \ref{tte:pthatmtrace}. Second, given a gapped sentence downstream ("... \usc\ loves Sue"), the model should prefer a prefix without an overt complementizer. That is, $P(loves)$ in \ref{tte:pthatptrace} is expected to be smaller than $P(loves)$ in \ref{tte:mthatptrace}. This way, we measure the dispreference for an overt complementizer in the presence of a moved subject along both axes - that of the complementizer and that of the gap.

Results for both success criteria are presented in Figure~\ref{fig:tte-raw}. Under both criteria, the performance of the models hovers around chance. While it is unclear how to interpret these results, we see nothing in the performance of the models that suggests that the target hypothesis will account for the learning of that-trace effects from similar training corpora.

\begin{figure}[h]
    \includegraphics[width=0.8\linewidth]{aps-tte-raw-old-criterion.png} \\[1ex] %
    \includegraphics[width=0.8\linewidth]{aps-tte-raw-new-criterion.png}
    \caption{Model accuracy values for the $P(+that, +trace) < P(+that, -trace)$ criterion (top) and $P(+that, +trace) < P(-that, +trace)$ criterion (bottom), over samples of 10,000 sentence pairs from the TTE test set. Results are averaged over five seeds. The dark plot represents model training sizes.}
    \label{fig:tte-raw}
\end{figure}

\section{Large Language Models and cross-linguistic evidence}
\label{sec:typ}

Consider again the comparison of a strongly linguistically-biased generative theory and a more linguistically-neutral alternative. Suppose that the strong linguistic biases of the generative theory can be shown to help derive a cross-linguistic pattern, perhaps by making it easier to represent and learn the languages that adhere to the pattern than those that do not. In the linguistically-neutral theory it can be much harder to see directly whether the attested patterns are predicted to be easier to learn than the unattested ones. One might try to evaluate this matter empirically, by simulating learning of languages of the attested and the unattested kind given the linguistically-neutral theory, but this can be hard to do. Consequently, it can be hard to see whether the theory can explain the cross-linguistic pattern under consideration and whether it can compete with the generative theory. Again, however, if an LLM is a good proxy for the theory, the problem is avoided to some extent. One can examine the ease with which the LLM learns to approximate both languages that follow the pattern and languages that violate it. If the LLM approximates the former more easily than the latter, this can suggest that the target theory might be able to account for the cross-linguistic pattern (subject to various further assumptions about how relative ease of learning derives the typology, and to an adequate definition of how ease-of-learning via LLM is to be measured). If, on the other hand, some of the pattern-violating languages are easier to learn than some of the pattern-satisfying ones, the linguistically-neutral theory would need further mechanisms to derive the typology. 

We should emphasize that the inferences from this argument are particularly weak. On the one hand, even if the linguistically-neutral theory derives a learning asymmetry between pattern-satisfying and pattern-defying languages but the difference is very small, this might not suffice to account for a robust typological pattern. On the other hand, even if the linguistically-neutral theory does not derive a learning asymmetry, it might be able to account for it in terms of other factors, such as communicative pressure. In order to overcome these limitations one could try to situate the learning component within a model of cultural evolution, which allows for the amplification of small asymmetries over generations and also factors in communicative pressures and other considerations. We believe that such a model is the proper context for the use ease of learning by LLMs to evaluate typological asymmetries, but following \citet{KalliniPapadimitriouFutrellMahowaldPotts:2024}, whose setup we use here, we leave such a model for future work and only assess currently available models.\footnote{\citet{KalliniPapadimitriouFutrellMahowaldPotts:2024} contextualize their work differently. We will not attempt to determine whether the present work is in line with their goals.} 


\subsection{Ease of learning by LLM as a theory of HLC?}

\citet{KalliniPapadimitriouFutrellMahowaldPotts:2024}\ compare GPT-2 when trained on an English corpus and when trained on corpora that are derived from the original corpus through various perturbations, each corresponding to an unattested language. They find that the original English corpus was easier for GPT-2 than the derived corpora of unattested languages when ease-of-learning is measured through test set perplexities through time (see Figure~\ref{fig:il-partial-reverse}). In doing so, \citet{KalliniPapadimitriouFutrellMahowaldPotts:2024} make use of LLMs as proxies in the context of explaining the cross-linguistic skew that rules out the unattested perturbations of English, even though this theory is never made explicit in the paper.
If a model like GPT-2 is indeed a good proxy for this contender, then we may say that what  \citet{KalliniPapadimitriouFutrellMahowaldPotts:2024} have shown to hold for English should also hold cross-linguistically, as presented in the introduction: all unattested languages should be harder to learn than attested ones. 

In the following subsections we consider a few languages for which the evaluation is simple given current resources and test them with the perturbations used by \citet{KalliniPapadimitriouFutrellMahowaldPotts:2024} and one additional perturbation, listed in Table~\ref{tab:perturbations}. For each perturbation, we create perturbed versions and baseline versions for each of the four languages - English, Italian, Russian and Hebrew, based on datasets from \citet{GulordavaBojanowskiGraveLinzenBaroni:2018}. We then train a transformer LLM based on the architecture and training scheme from \citet{yedetore:2023}, and consider the validation set perplexities as training progresses.\footnote{The model was trained for 48 hours and validation perplexity was computed every
200 training batches. The 48 hours of training amounted to roughly 1.5 epochs per dataset, with batch size 10, such that each epoch amounted to an average of 181,552 batches, see Appendix~\ref{sec:appendix-typ} for further details.} We find we find several different languages of an unattested kind that are easier for the LLM to approximate than a corresponding attested language, as measured by the perplexity metric used by \citet{KalliniPapadimitriouFutrellMahowaldPotts:2024}. From the perspective of the Proxy View, this means that the target, relatively linguistically-neutral theory fails to receive support from the LLM: if the target hypothesis can explain the typological pattern, this is not detected by the LLM experiment.

\begin{table}[h!] 
    \renewcommand{\arraystretch}{1.5}  %
    \begin{tabular}{p{2cm}p{7cm}}  %
        \toprule
        Perturbation & \textcolor{teal}{Attested} / \textcolor{coral}{Perturbed} \\ 
        \midrule
        \textit{partial-reverse} & 
        \textcolor{teal}{Colorless\textsubscript{0} green\textsubscript{1} \texttt{<rev>}\textsubscript{2} ideas\textsubscript{3} sleep\textsubscript{4} furiously\textsubscript{5}.} 
        \newline
        \textcolor{coral}{Colorless\textsubscript{0} green\textsubscript{1} \texttt{<rev>}\textsubscript{2} furiously\textsubscript{5} sleep\textsubscript{4} ideas\textsubscript{3}.} \\

        \midrule
        \textit{full-reverse} & 
        \textcolor{teal}{Colorless\textsubscript{0} green\textsubscript{1} \texttt{<rev>}\textsubscript{2} ideas\textsubscript{3} sleep\textsubscript{4} furiously\textsubscript{5}.} 
        \newline
        \textcolor{coral}{Furiously\textsubscript{5} sleep\textsubscript{4} ideas\textsubscript{3} \texttt{<rev>}\textsubscript{2} green\textsubscript{1} colorless\textsubscript{0}.} \\
        \midrule
        \textit{switch-indices} & 
        \textcolor{teal}{Colorless\textsubscript{0} green\textsubscript{1} ideas\textsubscript{2} sleep\textsubscript{3} furiously\textsubscript{4}.} 
        \newline 
        \textcolor{coral}{Ideas\textsubscript{2} green\textsubscript{1} colorless\textsubscript{0} sleep\textsubscript{3} furiously\textsubscript{4}.} \\
        \midrule
        \textit{token-hop} & 
        \textcolor{teal}{They\textsubscript{0} were\textsubscript{1} sleeping\textsubscript{2} \textbf{v\textsubscript{3}} next\textsubscript{4} to\textsubscript{5} the\textsubscript{6} colorless\textsubscript{7} green\textsubscript{8} ideas\textsubscript{9}.} 
        \newline
        \textcolor{coral}{They\textsubscript{0} were\textsubscript{1} sleeping\textsubscript{2} next\textsubscript{3} to\textsubscript{4} the\textsubscript{5} \textbf{v\textsubscript{6}} colorless\textsubscript{7} green\textsubscript{8} ideas\textsubscript{9}.} \\
        \bottomrule
    \end{tabular}
\label{tab:perturbations}
\caption{Perturbation test cases. Ease-of-learning is evaluated for the attested and perturbed versions of English, Italian, Hebrew and Russian.}
\end{table}


\subsection{Test case: \textit{partial-reverse}}


We demonstrate the ease-of-learning method in our first test case - a cross-linguistic reproduction of the \textit{partial-reverse} perturbation from \citealt{MitchellBowers:2020} and \citealt{KalliniPapadimitriouFutrellMahowaldPotts:2024}.  Each sentence is reversed starting from a randomly chosen index in the input sentence, in which a special marker token \textit{\textless rev\textgreater} is inserted:

\pex \label{ex:partial-reverse}
    \a \textbf{Baseline:} \label{partial-reverse-baseline} Colorless$_{0}$ green$_{1}$ \texttt{<rev>}$_{2}$ ideas$_{3}$ sleep$_{4}$ furiously$_{5}$.
    \a \textbf{Perturbed:} \label{partial-reverse-perturbed} Colorless$_{0}$ green$_{1}$ \texttt{<rev>}$_{2}$ furiously$_{5}$ sleep$_{4}$ ideas$_{3}$.
\xe



The target, relatively linguistically-neutral theory in this case might be a version of a generative theory in which there is a parameter for reverse. Any grammar that is statable within the theory now comes in two variants. If the parameter is set to 0, the grammar works as in the original generative theory. But if the parameter is set to 1, every derivation ends with reversing the output starting from a random point. This may or may not be the target theory that \citeauthor{KalliniPapadimitriouFutrellMahowaldPotts:2024} have in mind; other target theories are of course imaginable, and \citeauthor{KalliniPapadimitriouFutrellMahowaldPotts:2024} do not comment explicitly about this. We note that, like \citeauthor{KalliniPapadimitriouFutrellMahowaldPotts:2024}'s other perturbations, reversal seems quite far away from the typological questions that generative linguistics have usually concerned themselves with. Here we stay with \citeauthor{KalliniPapadimitriouFutrellMahowaldPotts:2024}'s perturbations and setup simply in order to illustrate the relevant methodological considerations, but a proper exploration of the Proxy View would need to look at those cross-linguistic patterns that have informed linguistic research.  


\begin{figure}[h]
\includegraphics[width=\linewidth]{il-partial-reverse-en-it-ru-cl-view.png}
\caption{Validation perplexity during training for English, Italian, and Russian and their \textit{partial-reverse} perturbations. The results indicate that $\Pi(\textit{attested}) < \Pi(\textit{partial-reverse})$.}

\label{fig:il-partial-reverse}
\end{figure}

We find that validation perplexities follow an attested-unattested divide: the partial-reverse perturbed versions of English, Italian and Russian are harder to learn than their attested counterparts (see Fig.~\ref{fig:il-partial-reverse}).\footnote{The \textit{partial-reverse} languages are compared to a modified baseline of English in which the marker \textit{\textless rev\textgreater} is inserted randomly without reversing, to control for the effect for additional textual material in the sentence on perplexities. Since we use the same seed for both perturbations, the markers are inserted in the same indices (see example \ref{ex:partial-reverse}).}



This, as discussed above, could be part of an argument that linguistic biases are not needed to rule out partial-reverse languages, but crucially only if there were a serious contender for the best explanation that allows for such languages in the first place, and could be compared theoretically with the restrictive generative account. However, other perturbations paint a different picture under the cross-lingustic view.


\subsection{Test case: \textit{full-reverse}}

 We now consider the perturbation $\Pi=$ \textit{full-reverse} from \citealt{KalliniPapadimitriouFutrellMahowaldPotts:2024}, where each sentence is reversed in its entirety and a special marker token is randomly inserted: 

 \pex \label{ex:full-reverse}
 \a \textbf{Baseline:} \label{full-reverse-baseline} Colorless$_{0}$ green$_{1}$ \texttt{<rev>}$_{2}$ ideas$_{3}$ sleep$_{4}$ furiously$_{5}$.
 \a \textbf{Perturbed:} \label{full-reverse-perturbed} Furiously$_{5}$ sleep$_{4}$ ideas$_{3}$ \texttt{<rev>}$_{2}$ green$_{1}$ colorless$_{0}$.
\xe


 We find that the perturbed versions of Italian, Russian and Hebrew are projected as easier than attested English (see Fig.~\ref{fig:il-full-reverse}), even though the perturbation is considered humanly impossible. In this case, ease of learning according to \citet{KalliniPapadimitriouFutrellMahowaldPotts:2024}’s perplexity metric does not provide support for the idea that the target theory can explain away the typological asymmetry through asymmetries of ease of learning. As mentioned above, this conclusion is weak. It is possible that the target theory is the best explanation and that the typology is explained with the help of factors other than ease of learning. It is also very possible that \citeauthor{KalliniPapadimitriouFutrellMahowaldPotts:2024}'s perplexity metric is a poor way to evaluate ease of approximation by the LLM. All we can conclude at present is that proponents of the Proxy View have work to do in light of the LLM's performance in this case, and that previous results only provide a limited picture.


\begin{figure}

\includegraphics[width=\linewidth]{il-full-reverse-en-he-it-ru.png}
\caption{Validation perplexity during training for attested (baseline) and \textit{full-reverse} versions of English, Russian, Italian, and Hebrew.}
\label{fig:il-full-reverse}
\end{figure}


\subsection{Test case: \textit{switch-indices}}

We turn to another asymmetry that the Proxy View does not at present explain. Consider $\Pi=$ \textit{switch-indices}, where the tokens at index 0 and index 2 in every sentence are switched:


\pex \label{ex:switch-indices}
    \a \textbf{Baseline:} \label{switch-indices-baseline} Colorless$_{0}$ green$_{1}$ ideas$_{2}$ sleep$_{3}$ furiously$_{4}$.
    \a \textbf{Perturbed:} \label{switch-indices-perturbed} Ideas$_{2}$ green$_{1}$ colorless$_{0}$ sleep$_{3}$ furiously$_{4}$.
\xe

One can again imagine various target theories that allow for languages like English as well as their switch-index variants. And again the question would be whether such a target theory can account for the typological observation that such switch-index variants are unattested.

If the target theory can account for the typological asymmetry, this is again not detected through the perplexity curve of the LLM proxy: each language is wrongly projected as harder than its perturbed version, and the perplexity delta between each two attested languages is significantly larger (see Fig. \ref{fig:il-switch-indices}). In all cases, there is an unattested language that is projected as easier than an attested language.


\begin{figure}[t]
\includegraphics[width=1\linewidth]{il-switch-indices-en-he-it-ru.png}

\caption{Validation perplexity during training for the attested (baseline) and \textit{switch-indices} versions of English, Italian, Hebrew and Russian.}
\label{fig:il-switch-indices}

\end{figure}


\subsection{Test case: \textit{token-hop}, \textit{no-hop}}

We look at one final typological asymmetry, based on the $\Pi=\textit{token-hop}$ perturbation employed by  \citet{KalliniPapadimitriouFutrellMahowaldPotts:2024}.  For each of English, Italian, and Russian, we create a perturbed variant by inserting a new marker token three tokens after each verb:\footnote{New marker tokens were randomly generated as single-character tokens that do not already exist in the model's vocabulary (English: "v", Italian: "v", Russian: "\textit{\textcyrillic{Ч}}").}
\pex \label{ex:token-hop-no-hop}
    \a \textbf{no-hop (baseline):} \label{no-hop-baseline} They$_0$ were$_1$ sleeping$_2$ \textbf{v$_3$} next$_4$ to$_5$ the$_6$ colorless$_7$ green$_8$ ideas$_9$.
    \a \textbf{token-hop (perturbed):} \label{token-hop-perturbed} They$_0$ were$_1$ sleeping$_2$ next$_3$ to$_4$ the$_5$ \textbf{v$_6$} colorless$_7$ green$_8$ ideas$_9$.
\xe

Since additional textual material is inserted, in a way that might confound perplexities over time, we compare the perplexities achieved by our model on \textit{token-hop} datasets to \textit{no-hop} datasets, in which the marker token is inserted exactly after each verb. This achieves control for the effect of the marker token itself, such that difference in perplexities can be attributed solely to the effect of the token-counting generalization. The \textit{no-hop} version adheres to attested generalizations, as it can be thought of as post-verbal clitic that marks for parts of speech. Learning \textit{token-hop} on the other hand crucially requires counting tokens, an ability that is particularly non-humanlike, both because human languages have no atomic concept that corresponds to tokens, and because counting rules violate the strictly hierarchal nature of linguistic generalizations across languages. Although the distance of the token from its associated verb has an effect on perplexity, we find that the \textit{no-hop} version of English is still harder to learn than \textit{token-hop} Italian and Russian (see Fig.~\ref{fig:il-token-hop}).  Unperturbed English datasets are consistently projected as harder than \textit{token-hop} perturbed versions of Italian and Russian, despite a clear typological skew against counting rules. The same point is made with the validation perplexities of \textit{token-hop} Italian and \textit{no-hop} Russian patterning very similarly. The best explanation of linguistic typology should predict a clear divide between unattested languages that crucially require counting linguistic elements and attested languages. Ease-of-learning via LLM does not explain this clear typological skew.

\begin{figure}
\includegraphics[width=\linewidth]{il-token-hop-no-hop-en-it-ru.png}
\caption{Validation perplexity during training for the \textit{no-hop} (baseline) and \textit{token-hop} versions of English, Italian, and Russian.}
\label{fig:il-token-hop}

\end{figure}

\section{Conclusion}
\label{sec:conc}

LLMs have been presented as a challenge to more traditional approaches to the study of HLC, and in particular to generative linguistics. A blunt version of this idea is the LLM Theory, promoted by \citet{Piantadosi:2023}, which maintains that current LLMs are themselves good theories of HLC. Current LLMs can, of course, be viewed as theories of HLC, but this treatment does these models no favors, as the literature was quick to note. 

A more cautious approach is the Proxy View, promoted by \citet{WilcoxFutrellLevy:2023} and  \citet{MahowaldIvanovaBlankKanwisherTenenbaumFedorenko:2024}, which maintains that current LLMs are good proxies - as far as various behavioral measures are concerned - for good theories of HLC that are more linguistically-neutral than generative theories. No commitment to shared essentials between LLMs and the relevant good theories is implied. The Proxy View is of course perfectly coherent, but differently from the LLM Theory it is too vague about the theories of HLC that it champions to be of much direct use within empirical science. Empirical science is based on inference to the best explanation and proceeds through competition between reasonably well-understood theories, and saying just that one's favorite theory learns roughly as well as an LLM is insufficient for this purpose. One would like to see enough detail at least for a proper comparison of the relevant theory to generative linguistics in light of the discoveries and considerations in the literature of the past 70 years or so. More than anything else, the present paper is a plea for supporters of the Proxy View to provide this kind of detail and evaluation for the theory they have in mind.

Still, already at this stage we can outline an empirical evaluation of the Proxy View on various kinds of test cases, and here we focused on two: alignment with the stimulus and cross-linguistic variation. The results were not encouraging for the Proxy View. The LLMs failed to even approximate key pieces of knowledge that humans have, even when the models were trained on much larger corpora than children are exposed to. The LLMs also had an easier time approximating various typologically-unattested languages than actual human languages, at least with respect to a success criterion used in \citet{KalliniPapadimitriouFutrellMahowaldPotts:2024}.
\pagebreak

\appendix 
\section{Section~\ref{sec:aps} - Appendix}
\label{sec:appendix-aps}

\subsection{General Notes}
In all experiments, probabilities are evaluated on the critical region in boldface (as in the example sentences in Tables~\ref{tab:cfg-atb}, \ref{tab:cfg-pg}, \ref{tab:cfg-tte}). For models listed as unidirectional, the probability value takes into consideration the left context of the evaluated token, whereas for bidirectional models, the probability value factors in both the left and the right context. The bidirectional models we consider were pretrained with masked language modeling (\textit{fill-mask}). 

In all generated sentences, the critical region unambiguously disambiguates the sentence as an instance of the phenomenon under consideration (there is no other grammatical reading). To promote lexical variation in target regions, possible target tokens in the CFGs were chosen at random from a POS-tagged English Wikipedia dump. We filtered out potential target tokens that were not part of the vocabulary of one the models considered in our setup, which are detailed in Table~\ref{tab:models-verbose}.
\begin{table}[h!]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Model Name} & \textbf{Training Size} & \textbf{Directionality} & \textbf{Source} \\
    \hline
    CHILDES LSTM & 8.6M & Uni & \citealt{yedetore:2023} \\
    CHILDES Transformer & 20M & Uni & \citealt{yedetore:2023} \\
    BabyLM 10M (\textit{strict-small}) & 10M & Bi & \citealt{Babylm:proceedings}  \\
    Wikipedia Transformer & 15M & Uni & Trained by authors \\
    BabyLM 100M & 100M & Bi & \citealt{Babylm:proceedings} \\
    BERT Base Uncased & $\approx$3.5B & Bi & \citealt{Devlin:2018} \\
    GPT2 & $\approx$8B & Uni & \citealt{Radford:2019} \\
    Llama 3.2-3b & $\approx$9T & Uni & \citealt{Touvron:2023} \\
    \hline
    \end{tabular}
    \caption{Models used in the experiments in Section~\ref{sec:atb}.}
    \label{tab:models-verbose}
    \end{table}
    


\subsection{ATB and PG}
For the ATB and PG experiments, we replicate the experimental setup from \citealt{LanChemlaKatzir:2024a} with a wider variety of test sentences to increase robustness. We also unify the templates for both phenomena
so that they use the same lexical choices where relevant (e.g., proper names). From each template, represented by a context free grammar, we generated tuples of sentences that correspond to the \textit{(+filler,+gap), (+filler,-gap)} conditions (see CFGs and example sentences in Tables~\ref{tab:cfg-atb} and~\ref{tab:cfg-pg}). We then evaluated the success criterion $P(+filler,-gap) < P(+filler,+gap)$ on a sample of 10,000 tuples for each phenomenon. For ATB, sentence structure was crucially changed such that the two conjuncts do not agree in number, to prevent a problematic non-ATB reading present in some of the \citeauthor{LanChemlaKatzir:2024b} test sentences. Underlined words alternate according to the $\pm filler$ condition; words in bold mark the position where the $\pm gap$ condition becomes evident and probability is measured. We evaluate the performance of four models from Table~\ref{tab:models-verbose}. We do not consider bidirectional models for the PG and ATB experiments because they are highly sensitive to whether the token is the last token in the sentence, which in the ATB and PG case is one of the minimal differences between the conditions we consider.


\begin{table}[h!]
    \begin{tabular}{p{9cm}}
        \hline
        ATB Grammar \\
        \hline
        \input{cfg-example-atb} \\
    \end{tabular}
    \caption{
    Excerpt from the context-free grammar used to generate Across-the-Board sentences for the experiments in Section~\ref{sec:aps}.}
    \label{tab:cfg-atb}
\end{table}



\begin{table}[h!]
\begin{tabular}{p{9cm}}
\hline
PG Grammar \\
\hline
\input{cfg-example-pg} \\
\end{tabular}
\caption{Excerpt from the context-free grammar used to generate Parasitic Gap sentences for the experiments in Section~\ref{sec:aps}.}
\label{tab:cfg-pg}
\end{table}


\subsection{TTE}

To create test sets, we generate 1,178,496 quadruplets, out of which
we randomly sample 10,000 quadruplets per experiment. Paradigms are varied lexically in several ways: we first create paradigms that differ
structurally from each other, to control for the possibility that our results are an
artifact of the specific sentence structure introduced in the example sentences in Table~\ref{tab:cfg-tte}. We
also introduce lexical variation to all non-target elements, such as the main subject,
the embedding verb, the wh-element and the verb inside the wh-clause. Most
importantly, lexical choices for target verbs and nouns are varied using 30 target nouns and 30 target verbs. We perform five inference experiments on five such samples and consider average accuracy over all experiments. Quadruplets are evaluated with the two success criteria introduced in Section~\ref{sec:tte}.

\begin{table}[h!]
\begin{tabular}{p{12.5cm}}\hline
TTE Grammar \\
\hline
\input{cfg-example-tte}
\end{tabular}
\caption{Excerpt from the context-free grammar used to generate That-Trace-Effect sentences for the experiments in Section~\ref{sec:aps}.}
\label{tab:cfg-tte}
\end{table}


\section{Section~\ref{sec:typ} - Appendix}
\label{sec:appendix-typ}

\subsection{Dataset Creation}

Our baseline datasets are based on Wikipedia dumps in English, Italian, Hebrew and Russian from \citet{GulordavaBojanowskiGraveLinzenBaroni:2018}, which extracted 90M token subsets for each language, and switched all tokens that do not belong to the top 50K most frequent tokens into $\angles{unk}$ tokens (see \citealt{GulordavaBojanowskiGraveLinzenBaroni:2018}). For each baseline dataset we create perturbed \textit{train}, \textit{test} and \textit{validation} datasets (see Table~\ref{tab:perturbations}), similarly to \citealt{KalliniPapadimitriouFutrellMahowaldPotts:2024}, such that each perturbed dataset has the same number of tokens as its corresponding baseline dataset. 

The \textit{no-hop} and \textit{token-hop} perturbations require POS tagging, which we performed using the \href{https://spacy.io/models}{Spacy} Python library. Since the POS tagging accuracy is low for Hebrew, we did not perform \textit{no-hop} and \textit{token-hop} experiments on it. New marker tokens were randomly generated as single-character tokens that do not already exist in the model's vocabulary (English: "v", Italian: "v", Russian: "\textit{\textcyrillic{Ч}}"). We used the following Spacy NLP pipelines to perform POS tagging and identify verbs: 
\begin{enumerate}
    \item "en\_core\_web\_sm", accuracy of POS tagger: 0.97
    \item "it\_core\_news\_sm", accuracy of POS tagger: 0.97
    \item "ru\_core\_news\_sm", accuracy of POS tagger: 0.99
\end{enumerate}
We chose to replace \citet{KalliniPapadimitriouFutrellMahowaldPotts:2024}'s 3rd person agreement \textit{token-hop} with a POS marker because of the lower accuracy of morphological taggers on languages that are not English and cross-linguistic differences in verb morphology.



\subsection{Model Training and Evaluation}

We use the optimal Transformer architecture chosen in a hyperparameter search conducted by \citealt{yedetore:2023}, which has four layers, a hidden and embedding size of 800, a batch size of 10, a dropout rate of 0.2 and a learning rate of 5. Every 200 batches, we evaluate the average perplexity per token of the current trained model on a held out validation set. We set a 48-hour training threshold, during which each model processed a different number of batches. For consistency in comparison graphs, we use the smallest batch count achieved and compare all models up to that point. Table~\ref{tab:typ-batches} details the number of training batches achieved by the Transformer model on each perturbed dataset.

\begin{table}[h!]
\
\renewcommand{\arraystretch}{1.3}  %

\begin{tabular}{|p{1.7cm}|p{2cm}|p{3cm}|p{1.5cm}|}
\hline

\textbf{Language} & \textbf{Perturbation} & \textbf{Size of Validation Set (batches)} & \textbf{Training batches} \\
\hline
\multirow{2}{*}{English} & \textit{no-perturb} & 138430 & 137600 \\
                         & \textit{full-reverse}   & 143518 & 117400 \\
                         & \textit{partial-reverse}   &  143517 & 131800 \\
                         & \textit{switch-indices}   & 166116 & 205516 \\
                        & \textit{no-hop}   & 181502 &  189102 \\
                         & \textit{token-hop} &  181502   & 142200 \\
\hline
\multirow{2}{*}{Italian} & \textit{no-perturb} & 138589 & 137200 \\
                         & \textit{full-reverse}   & 143840 & 154200 \\
                         & \textit{partial-reverse}   & 143506 & 152000 \\
                         & \textit{switch-indices}   & 166307 & 213507 \\
                         & \textit{no-hop}   & 181552 & 146400 \\
                         & \textit{token-hop}  & 181552 & 191752 \\
\hline
\multirow{2}{*}{Russian} & \textit{no-perturb} & 138602 & 136200 \\
                         & \textit{full-reverse}   & 145993 & 153600 \\
                         & \textit{partial-reverse}   & 145984 & 125400 \\
                         & \textit{switch-indices}   & 166323 & 213323 \\
                         & \textit{no-hop}   & 178771 & 148800 \\
                         & \textit{token-hop}   & 178771 & 192571 \\

\hline
\multirow{2}{*}{Hebrew} & \textit{no-perturb}& 137773 & 138000 \\
                         & \textit{full-reverse}   & 142884 & 154800 \\
                         & \textit{partial-reverse}   & 171460 & 203860 \\
                         & \textit{switch-indices}   & 165328 & 210128 \\
                         & \textit{no-hop}   & - & - \\
                         & \textit{token-hop}   & - & - \\
\hline
\end{tabular}
\caption{Number of training batches achieved in 48 hours of training on our perturbed datasets from Section~\ref{sec:typ}.}
\label{tab:typ-batches}
\end{table}



\clearpage

\bibliographystyle{custom} %
\input{main.bbl}

\end{document}
