\section{Experimental Study}

We propose two experimental studies. First, we demonstrate the capability of our proposed \name{} method to generate collections of diverse solutions in uncertain domains. Second, we illustrate the benefits of our proposed \framework{} by showing how it improves the performance of the widely used PGA-ME algorithm~\cite{nilsson2021policy} in uncertain domains at no additional evaluation cost.

\subsection{Experimental study of \Longname{}} \label{sec:results_algo}

% This section compares \name{} to existing UQD approaches.



\subsubsection{Tasks}
We consider the following tasks (Table~\ref{tab:tasks}), used in previous UQD work~\cite{flageat2023uncertain, flageat2024exploring, grillotti2023don} (more details in Appendix~\ref{app:setup}):
% We select those tasks to cover a range of possible genotype-size, archive-size and uncertainty-type to highlight the limitations and constraints of existing approaches. 
% We give the sampling-size (see Section~\ref{sec:sampling_size}) as part of the task definition ("Size" in Table~\ref{tab:tasks}). We chose these values to best illustrate the limitations of existing approaches. However, we highlight that this choice was not made to benefit \name{} and support this claim by providing results for other sampling-size in Appendix~\ref{app:results}.

\input{Tables/tasks_table}



\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{Figures/main_results.pdf}
  \caption{
    Comparison of our \name{} approach with similar existing UQD approaches. The Corrected QD-Score reflects performance on the task, while the Average Samples offer insights into how the methods manage the available sampling budget. The vertical line represents the median across $10$ replications; the box shows the quartiles, the whiskers indicate $1.5$ times the interquartile range, and the dots represent outliers. Blank lines indicate undefined approach-task pairs.
  }
  \label{fig:main_results}
  \Description{Main comparison results}
\end{figure*}


\subsubsection{Baselines} We use the following baselines from Section~\ref{sec:uqd}: (1) Vanilla-ME, (2) ME-Sampling, (3) Adapt-ME, (4) AS, and (5) Deep-Grid. We use the parameters from previous works~\cite{flageat2023uncertain, flageat2024exploring}. We replicate each algorithm-task pair across $10$ seeds. 
All our implementations use the QDax library~\cite{chalumeau2024qdax, lim2022accelerated}.
To facilitate later works, we open-source our code at \textit{URL-to-be-released-upon-acceptance}.


\subsubsection{Metrics}
A key challenge in UQD is that the archive returned by an algorithm is not trustworthy. For instance, when ME uses only a single evaluation per solution, the resulting archive is likely to be filled with misplaced "lucky" solutions. Thus, the archive returned by an algorithm is referred to as an \textbf{illusory archive}, and it is usually adjusted into a \textbf{corrected archive}~\cite{adaptive}, used to compute metrics. To construct the corrected archive, we add all the solutions from the illusory archive in an empty archive (with no depth) using their "ground-truth" fitness and descriptor feature. 
When the ground-truth is not directly accessible, we approximate it as the median of $512$ re-evaluations (shown to be a reasonable number in previous work~\cite{flageat2023uncertain}). 
Similar to previous work, we then report the \textbf{Corrected QD-Score}: the sum of all fitness in the corrected archive. It quantifies the overall performance and diversity of the final collection. 
Additionally, to better understand the dynamic of \name{}, we report the \textbf{Average Samples} spent on solutions in the illusory archive (only in the top layer if there is a depth). 
We report $p$-values using the Wilcoxon test with Holm-Bonferroni correction. 
% We then report the \textbf{Corrected QD-Score}: sum of all fitness in the Corrected archive, and the \textbf{Loss-QD-Score}: normalised QD-Score difference between the Illusory and Corrected Archive. 
% The Corrected QD-Score quantifies the overall performance and diversity of the final collection, while the Loss-QD-Score quantifies the ability of algorithms to accurately approximate solutions' performance. The former aims to be maximised and the latter minimised. 



\begin{figure}[t!]
  \centering
  \includegraphics[width=0.95\linewidth]{Figures/archives.png}
  \caption{
    Final Corrected Archive of a random replication of each approach across all tasks. Blank panels indicate undefined approach-task pairs. 
    Each cell corresponds to a solution, with brighter colours indicating higher fitness. The axes represent the descriptor dimensions detailed in Table~\ref{tab:tasks}.
  }
  \Description{Final archive comparison plots}
  \label{fig:archives}
  \vspace{-1mm}
\end{figure}

\subsubsection{Sampling-Size Comparison} \label{sec:sampling_size}
UQD algorithms are typically compared using a fixed maximum number of evaluations per generation, referred to as sampling-size~\cite{flageat2023uncertain}.
For instance, with a sampling-size of $1024$, ME produces $1024$ offspring per generation, evaluated $1$ time each, while ME-Sampling generates only $32$ offspring per generation, each evaluated $32$ times, giving also a total of $1024$ evaluations. All our comparisons use a sampling size of $1024$.
% QD algorithms are typically compared based on a fixed number of evaluations per generation, which in standard QD corresponds directly to the number of offspring generated per generation. However, in the UQD setting, where most algorithms employ more complex sampling strategies, these two quantities are no longer equivalent. To ensure consistency, we compare algorithms using a fixed maximum number of evaluations per generation, referred to as the sampling size~\cite{flageat2023uncertain}.
% AS uses $2048$ samples to reevaluate the content of the archive (i.e. the number of cells), thus it generates $4096 - 2048 = 2048$ offspring per generation. 
% We use sampling-size comparison in this work, meaning we compare all baselines for the same sampling-size, and we consider it as part of the task definition.



\subsubsection{Results}

We report the results of our experiments in Figure~\ref{fig:main_results} and the corresponding final corrected archives in Figure~\ref{fig:archives}. 
\textbf{Vanilla-ME} performs among the worst across the board which is consistent with the fact that it does not account for uncertainty. 
\textbf{ME-Sampling} performs well on tasks with descriptor noise but its tendency to hinder exploration is visible on more complex control tasks such as Ant.  
\textbf{Deep-Grid} is a strong competitor in tasks with limited descriptor noise but, by construction, it struggles when the noise on the descriptor becomes too large compared to the size of the cells (Arm Desc Noise and Ant tasks). 
\textbf{AS} is not defined in two of the tasks (Hexapod and Ant) because re-evaluating everything in the archive requires too many evaluations for the available budget, showcasing the limitations of this approach. However, when defined, it is one of the best-performing approaches. 
\textbf{Adapt-ME} performs well on the low-dimensional Arm task but struggles with higher-dimensional tasks. The Average Samples indicate that a significant portion of the sampling budget is spent on re-evaluating solutions, which severely hampers its exploration capability. 
Finally, our proposed \textbf{\name{}} performs well across the board, matching the performance of the best approach for each task and even outperforming all other approaches on Ant ($p<10^{-3}$). 
Average Samples indicate that \name{} uses significantly more samples than AS on the elites in the final archive for Arm Desc Noise and Walker ($p<10^{-3}$). This highlights that the extraction mechanism achieves a better allocation of samples compared to re-evaluating the entire archive.
More broadly, our results show the wide applicability of \name{} and its strength as a "first guess" method for new UQD tasks. 


\subsection{Benefits of the \Longframework{}: Example of PGA-ME} \label{sec:results_qdrl}

Quality-Diversity Reinforcement Learning (QD-RL) algorithms~\cite{tjanaka2022approximating, pierrot2022diversity, nilsson2021policy, faldor2023synergizing, tjanaka2023training} are QD algorithms specifically designed for Reinforcement Learning (RL) tasks. The properties available in these RL tasks enable the development of efficient targeted QD algorithms. 
Surprisingly, while most RL environments are uncertain (e.g. random initialisation or stochastic transitions), the aspects of UQD are rarely considered in the QD-RL literature.
Here, we propose to combine a common QD-RL approach: Policy-Gradient-Assisted ME (PGA)~\cite{nilsson2021policy} within our \framework{}. 
By doing so, we aim to illustrate how accounting for uncertainty and integrating UQD insights can improve the performance of common QD algorithms.

\subsubsection{Algorithm} 
We propose \Longnamepga{} (\namepga{}), a variant of PGA using our proposed \framework{}. 
The original PGA algorithm builds on ME, replacing its variation operator with RL variations based on the Twin Delayed Deep Deterministic (TD3)~\cite{fujimoto2018addressing} algorithm. Similarly, \namepga{} corresponds to our \name{} algorithm swapping the variation operator to use the one from PGA.


\subsubsection{Tasks}
We consider three tasks of the QD-Gym suite proposed in the original PGA paper~\cite{nilsson2021policy}, detailed in Table~\ref{tab:tasks_qdrl}.

\input{Tables/tasks_qdrl_table}

\subsubsection{Baselines and Metrics}
We compare PGA, ME and \namepga{} for $128$ evaluations per generation. While ME and PGA generate $128$ new solutions, \namepga{} only generates $96$ new solutions and uses the remaining $32$ evaluations to re-evaluate elites from the archive. 
We report the final Corrected QD-Score for each algorithm, as in the previous experimental section in Section~\ref{sec:results_algo}. 

\subsubsection{Results}

We report the results of our experiments in Figure~\ref{fig:qdrl_results}. 
Our proposed \namepga{} approach, built with our \framework{} outperforms the PGA algorithm across all tasks ($p<5.10^{-3}$). 
Crucially, this improvement is done at no additional evaluation cost: all approaches are evaluated for the same total number of evaluations and with the same number of evaluations per generation (i.e. sampling size). 
In \namepga{}, re-evaluating elites enables accounting for uncertainty and building better estimation of the fitness and feature descriptors of solutions, allowing the method to generate higher-quality archives. These results highlight the importance of accounting for uncertainty in QD tasks and demonstrate the value of our \framework{} in developing better QD algorithms. 



\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{Figures/qdrl_results.pdf}
  \caption{
    Comparison of our proposed \namepga{}{} approach with Vanilla-ME and PGA in terms of final Corrected QD-Score. The vertical line represents the median across $10$ replications; the box shows the quartiles, the whiskers indicate $1.5$ times the interquartile range, and the dots represent outliers. 
  }
  \label{fig:qdrl_results}
  \Description{QDRL comparison results}
  \vspace{-4mm}
\end{figure}
