%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Framework details

\section{Additional Details on UQD Approaches in the \framework{}} \label{app:framework}

This Section provides additional details on how each UQD approach fits within our proposed \framework{}. 

\paragraph{ME-Sampling}
As detailed in Section~\ref{sec:framework}, ME-Sampling (see Section~\ref{sec:fixed_sampling}) is exactly similar to ME except that it uses $32$ samples per solution during their first evaluation. Thus, ME-Sampling does not use any extraction or mechanisms that can be assimilated. In other words, in ME-Sampling, once elites have been evaluated and added to the archive, they cannot be re-evaluated or re-questioned.


\paragraph{Deep-Grid}
As mentioned in Section~\ref{sec:framework}, Deep-Grid~\cite{flageat2020fast} (see Section~\ref{sec:deep_grid}) differs from ME by using a depth of $32$ solutions in each cell and fitness-proportional selection within this depth.
Additionally, in Deep-Grid, new solutions can replace any solutions already in the cell, no matter the results of their evaluations, and only the highest-fitness ones are returned as the final collection.
This corresponds to using Depth-Ordering based on seniority and fitness, in which only the latest solutions are kept within the depth, and these solutions are ordered based on fitness.
Deep-Grid does not use any extraction operator, meaning that elites never get explicitly re-evaluated. Instead, Deep-Grid relies on the assumption that, as elites are selected as parents, they generate offspring close to them, leading to indirectly re-evaluating their area of the search space. 


\paragraph{Archive-Sampling (AS)}
As detailed in Section~\ref{sec:framework}, AS~\cite{flageat2023uncertain} (see Section~\ref{sec:adaptive_sampling}) uses the same container and selection operator as ME. Its container is a grid with depth $d$, ordered based on fitness, i.e. only the $d$ best-performing solutions are kept within each cell. AS also uses an extraction operator that extracts the full content of the archive at each generation, depth included, thus emptying it fully. All solutions in the archive are re-evaluated with the new offspring before being added back into the archive. Thus, the number of extracts in AS is equal to the total size of the container: $Cd$ where $C$ is the number of cells in the grid and $d$ is the depth.


\paragraph{Adapt-ME}
Adapt-ME~\cite{adaptive} (see Section~\ref{sec:adaptive_sampling}) uses the same container and selection operator as ME. However, it re-evaluates part of the elites, who are chosen as the ones a new offspring has challenged. This behaviour can easily be modelled as an extraction. Additionally, Adapt-ME requires offspring to be evaluated the same number of times as the elites in their respective cells. 
We propose to express this using a container composed of a grid and a buffer of the offspring that still needs re-evaluation. At each generation, solutions are extracted from both the grid and the buffer using the Extraction Operator, unifying these processes into a single extraction mechanism.
This new perspective on Adapt-ME enables better parallelisation of evaluations compared to the original algorithm, where evaluations were performed sequentially. However, it does not address the issue that the number of extracted solutions varies between generations. Thus, we can only provide an upper bound for this quantity: $b + bd$, where $b$ represents the number of offspring per generation and $d$ is the depth. If all offspring require an additional evaluation before addition, $b$ additional offspring from the buffer would need to be re-evaluated, and if all offspring trigger elite re-evaluation, $bd$ elites would be extracted from the grid.


\paragraph{ME-Reprod}
ME-Reprod~\cite{grillotti2023don} is a variant of ME-Sampling that uses the $32$ samples to compute reproducibility and only optimises for this. Thus, all its components are the same as ME-Sampling except that the Depth-ordering operator is based on reproducibility only instead of fitness. ME-Reprod is commonly used as a baseline to estimate an upper bound on reproducibility~\cite{grillotti2023don, flageat2024exploring}. However, due to the Performance-Reproducibility Trade-off (the third problem in UQD, see Section~\ref{sec:problems}), the reproducibility values achieved by ME-Reprod are often unattainable for high-performing solutions. For instance, in robotics, controllers that remain stationary can reach reproducibility values that are out of reach for moving controllers.

\paragraph{ME-Weighted}
ME-Weighted~\cite{flageat2024exploring} is another fixed-sampling approach, a variant of ME-Sampling, that modifies the addition of solutions to the archive to account for both their fitness and reproducibility. 
Similarly to ME-Reprod, in our \framework{}, this corresponds to an alternative depth-ordering operator. In the case of ME-Weighted, solutions are compared based on a weighted sum of fitness and reproducibility. 
ME-Weighted was originally introduced with a parametrisation to choose these weights based on user preference on the Performance-Reproducibility Trade-off~\cite{flageat2024exploring}.


\paragraph{AS-Weighted}
AS-Weighted~\cite{flageat2024exploring} was introduced jointly with ME-Weighted and uses the same depth-ordering mechanism, also based on a weighted sum of fitness and reproducibility. The parametrisation to choose the weights from user preferences also applies. 
However, unlike ME-Weighted, AS-Weighted uses all other components from AS. Thus, it uses a depth and an extraction operator that extracts the full content of the archive at every generation. 


\paragraph{ME-Low-Spread}
ME-Low-Spread~\cite{mace2023quality} is another fixed-sampling approach, a variant of ME-Sampling, that modifies the addition of solutions to the archive to account for both their fitness and reproducibility. 
Similarly to ME-Reprod, in our \framework{}, this corresponds to an alternative depth-ordering operator. 
In the case of ME-Low-Spread, a new solution can replace an elite in its cell only if it improves both its fitness and reproducibility. 
The other components stay the same as for ME-Sampling and ME. 


\paragraph{ME-Delta}
ME-Delta~\cite{flageat2024exploring} can be seen as a generalisation of ME-Low-Spread~\cite{mace2023quality} that enables some more flexible comparisons. ME-Delta relies on the definition by the final user of two thresholds $\delta_f$ and $\delta_r$ respectively for fitness and reproducibility. A solution can replace an existing elite if it either improves the performance of the elite by at least $\delta_f$ or if it improves the performance of the elite without losing more than $\delta_r$ reproducibility or if it improves the reproducibility of the elite by at least $\delta_r$ without loosing more than $\delta_f$ performance. 
This delta comparison allows implementing user preference over the Performance-Reproducibility Trade-off (see Section~\ref{sec:problems}), given in the form of $\delta_f$ and $\delta_r$. It also enables overcoming deceptive reproducibility or fitness traps, in which ME-Low-Spread would get stuck due to its strict criteria. 

\paragraph{AS-Delta}
AS-Delta~\cite{flageat2024exploring} was introduced jointly with ME-Delta and relies on the same delta comparison. However, AS-Delta introduces this comparison with all components from AS. Thus, it uses a depth and an extraction operator that extracts the full content of the archive at every generation. 


\paragraph{MOME-X}
MOME-X~\cite{flageat2024exploring} goes one step further than ME-Delta and AS-Delta in accounting for the Performance-Reproducibility Trade-off. MOME-X build on top of existing multi-objective QD works~\cite{pierrot2022multi, janmohamed2023improving} and keeps a Pareto front of fitness and reproducibility trade-offs in each cell of the archive. This allows the user to choose its favourite trade-off a-posteriori, after optimisation. For better estimation, MOME-X relies on the same mechanism as ME-Sampling (i.e. fixed sampling) and samples every solution $32$ times during their first evaluation. 
Thus, MOME-X uses a Multi-Objective ME (MOME)~\cite{pierrot2022multi} container that consists of a grid of $C$ Pareto front. MOME-X also uses selection based on crowding distances within cells from follow-up work~\cite{janmohamed2023improving}. However, MOME-X does not use any extraction or mechanisms that can be assimilated. Thus, in MOME-X, once elites have been evaluated and added to the archive, they cannot be re-evaluated or re-questioned.

\paragraph{ARIA}
The Archive Reproducibility Improvement Algorithm (ARIA)~\cite{grillotti2023don} differs slightly from the approaches described in this section. ARIA was originally designed as an optimization module to improve the reproducibility of an existing archive. However, in the original work, ARIA was also applied from scratch to certain tasks. In this study, we consider this setup to ensure comparability with existing approaches.
Despite this, ARIA’s different motivations mean it was not developed under the same constraints as other UQD approaches. The most significant distinction is its requirement for a significantly higher number of evaluations. In our \framework{}, ARIA employs the same grid container as ME, without depth or extraction mechanisms. However, ARIA utilizes an exceptionally high number of samples per solution ($1024$ to $2048$, depending on the task) and selects only solutions that are at the edge of filled cells in the archive, i.e. neighbouring an empty cell. Finally, ARIA employs a variation operator based on Natural Evolution Strategy~\cite{wierstra2014natural} that leverages a constrained optimization formulation to optimize both fitness and reproducibility simultaneously.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Experiments details

\section{Additional Details on Experimental Setup} \label{app:setup}


We give in Table~\ref{tab:tasks_complete} and Table~\ref{tab:tasks_qdrl_complete} a more detailed experimental setup for our two experimental studies. These tables include the details already presented in the main paper but complement them for a better understanding of the experimental setup. 

\paragraph{Main experimental study}
We give all parameters in Table~\ref{tab:tasks_complete}. 
The details for the Hexapod, Walker, and Ant tasks are adopted from prior UQD research~\cite{flageat2020fast, flageat2023uncertain, grillotti2023don, flageat2024exploring, mace2023quality}. Similarly, for the two Arm tasks, the main parameters align with those used in earlier UQD works~\cite{adaptive, flageat2020fast, flageat2023uncertain}. However, unlike most previous studies, which applied both types of noise simultaneously, we opted to separate them to decorrelate their effects.
We believe this separation offers a clearer understanding of the limitations of existing algorithms. For instance, our main results in Section~\ref{sec:results_algo} demonstrate that Deep-Grid struggles with one type of noise but not the other.

\paragraph{QDRL experimental study}
We give all parameters in Table~\ref{tab:tasks_qdrl_complete}. 
All details are based on previous QDRL works~\cite{nilsson2021policy, tjanaka2022approximating, faldor2023synergizing, mace2023quality} and the original task specifications from OpenAI Gym~\cite{towers2024gymnasium}. Consequently, the uncertainty in these tasks is derived directly from the original OpenAI Gym implementations~\cite{towers2024gymnasium}, without introducing additional elements to increase uncertainty.
For this study, we use neural network controllers with hidden layers of size $(64, 64)$. While this is smaller than the configuration in the original PGA paper~\cite{nilsson2021policy}, it matches the settings used in subsequent works, where it has been shown to maintain PGA’s performance~\cite{tjanaka2022approximating, faldor2023synergizing}.

\input{Tables/tasks_table_complete}

\input{Tables/tasks_qdrl_table_complete}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.9\linewidth]{Figures/convergence.pdf}
  \caption{
    Results of our \name{} approach compared to existing UQD approaches tackling the Performance Estimation problem. The plain line indicates the median over $10$ replications, the shaded area corresponds to the quartiles. 
  }
  \label{fig:convergence}
  \Description{Convergence results}
\end{figure*}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Additional Results

\section{Additional Results} \label{app:results}

\subsection{Convergence Plots and Run Time for \name{}}

We provide the convergence plots corresponding to the experimental analysis of \name{} in Figure~\ref{fig:convergence}. As detailed in Section~\ref{sec:results_algo}, all algorithms are run with the same evaluation budget per generation (i.e. sampling-size). Therefore, while the x-axis shows the number of generations, it directly translates into the number of evaluations.

We also provide the total run time in hours of our implementations in Figure~\ref{fig:time}. These measures exclude any metric computation time. 
The results show that \name{} does not induce additional time overhead compared to other UQD approaches. 
These results also highlight that Adapt-ME is multiple orders of magnitude slower than other approaches due to its sequential approach. 


\subsection{Archives and Convergence Plots for \namepga{}}

For completeness, we provide the archives and convergence plots of the experimental analysis of \namepga{} in Figure~\ref{fig:archives_qdrl} and~\ref{fig:qdrl_convergence} respectively. 



\begin{figure}[t!]
  \centering
  \includegraphics[width=0.89\linewidth]{Figures/qdrl_archives.png}
  \caption{
    Final Corrected Archive of a random replication of each QDRL approach across all tasks. Blank panels indicate undefined approach-task pairs. 
    Each cell corresponds to a solution, with brighter colours indicating higher fitness. The axes represent the descriptor dimensions detailed in Table~\ref{fig:archives_qdrl}.
  }
  \Description{Final archive comparison plots}
  \label{fig:archives_qdrl}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{Figures/time.pdf}
  \caption{
    Run time in hours taken by all approaches on all tasks, excluding metric computation time. We provide a different axis for Adapt-ME as it is multiple of magnitude slower than other approaches. 
  }
  \vspace{-2mm}
  \label{fig:time}
  \Description{Time Comparison Results}
\end{figure}


\begin{figure}[t!]
  \centering
  \includegraphics[width=0.95\linewidth]{Figures/qdrl_convergence.pdf}
  \caption{
    Results of our \namepga{}{} approach compared to Vanilla-ME and PGA. The plain line indicates the median over $10$ replications, the shaded area corresponds to the quartiles. 
  }
  \vspace{-2mm}
  \label{fig:qdrl_convergence}
  \Description{Convergence results}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Implementation details

% \section{Implementation details and Algorithms Hyperparameters} \label{app:params}

% We also provide in Table a complete table of hyperparameters for all approaches considered in this paper. 
