% \vspace{3mm} 
\section{Introduction}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{Figures/uqd.pdf}
  \caption{
    Comparison of the (A) standard QD setup, where each solution gets a single deterministic fitness and descriptor feature value, and the (B) UQD setup, where each solution gets a distribution of these values. 
    We propose the \Longframework{} and \Longlongname{} for UQD, equivalent respectively to the QD-Framework and MAP-Elites for QD. 
  }
  \Description{Motivation for our contributions}
  \label{fig:uqd}
\end{figure}

% 1. QD is great and has many applications.

Problems in decision-making and content generation benefit from finding a diverse set of high-performing solutions. Such diversity enables users to choose among a variety of effective options~\cite{map_elites}, serves as a repository of potential solutions for unforeseen situations~\cite{nature}, or enables to identify stepping stones that might otherwise be overlooked~\cite{novelty_search}. 
Quality-Diversity (QD) algorithms~\cite{pugh, book_chapter, framework} have emerged as a powerful approach to uncovering such sets of diverse and high-performing solutions. 
To do so, they not only evaluate the solution's performance, known as its fitness but also characterise its way of solving the task (i.e. its behaviour), known as its feature descriptor, which allows them to quantify its novelty with respect to other solutions.
Thus, QD algorithms have found applications across various domains, including discovering diverse robotic controllers~\cite{nature, huber2023quality}, generating synthetic datasets for model training~\cite{huber2024qdgset, gaier2024generative} or adversarial examples for model testing~\cite{samvelyan2024rainbow}, and producing innovative designs~\cite{gaier2017generative} or video game content~\cite{gravina, earle2022illuminating}. 

% 2. It struggles in uncertain domains. 

Despite their potential, a key limitation of QD algorithms is their reliance on deterministic evaluations: they traditionally assume that a single evaluation of a solution reliably estimates its fitness and feature descriptor. 
However, this assumption does not hold true for many real-world tasks involving noise or stochasticity. 
For a motivating example, let's consider a QD method used to find a collection of controllers for a legged robot~\cite{nature}. If the sensor used to estimate the fitness (for example the walking speed) is noisy, the same controller may get different fitness from one evaluation to another. Thus, a slow controller could get "lucky" and appear the fastest thanks to a sensor glitch. QD elitism would lead this "lucky" to replace a truly good-performing one, resulting in a final population that contains underperforming controllers.
Similarly, if the sensor that estimates the feature descriptor incorrectly identifies a controller as walking efficiently on three legs when it actually uses all four, this "lucky" controller would replace genuine three-legged controllers, as none would match its speed. Consequently, the final population would fail to include true three-legged controllers, resulting in a drastic loss of diversity and potentially failing to meet user's requirements. 
Additionally, uncertainty in QD tasks extends beyond simple sensor noise to include stochastic environment dynamics~\cite{flageat2023uncertain, tjanaka2023training, mace2023quality, flageat2024exploring} and noise inherent to the task definition~\cite{huber2024domain, huber2024speeding}. This uncertainty also extends beyond robotics and has been observed in various QD tasks, such as content generation~\cite{earle2022illuminating} or design applications~\cite{gaier2024generative}.
This setup where the fitnesses and descriptor features of solutions are distributions instead of single deterministic values is known as Uncertain QD (UQD)~\cite{flageat2023uncertain} and illustrated in Figure~\ref{fig:uqd}.


% 3. Many papers have tried to address UQD but hard to decipher which one to try when you get a new task.

Prior work has proposed multiple algorithms to tackle the challenges of UQD~\cite{mace2023quality, flageat2020fast, flageat2023uncertain, flageat2024exploring, grillotti2023don, adaptive}. 
While these algorithms are promising, they all have different constraints and limitations. 
For example, Adaptive-Sampling~\cite{adaptive} rely on sequential evaluations, making it slow in parallelisable contexts, while Deep-Grid~\cite{flageat2020fast} assumes specific noise structure, and Archive-Sampling~\cite{flageat2023uncertain} requires sufficiently small populations to allow for periodic re-evaluation of their entire content.
This complex landscape of UQD approaches makes it difficult to know which one to try when facing a new UQD task. 
In this work, we aim to provide users with a set of accessible tools to bring UQD insights into their own QD applications.


% 4. To do this, two main contributions: (1) first, a framework \Longframework{} (\framework{}) and (2) second a simple instantiation of it: \Longname{} (\name{}). 

To this end, we present two main contributions: first, the \Longframework{} (\framework{}), and second, an instantiation of it, \Longname{} (\name{}).
The \framework{} proposes a new view on existing UQD approaches, allowing to encompass them all under the same common modular view. 
Thus, it also facilitates defining new UQD approaches by swapping modules.
\name{} is a new instantiation of the \framework{} that demonstrates strong performances across commonly studied tasks. 
Together, these two contributions aim to provide accessible tools to tackle any new uncertain QD application: \name{} acts as a solid "first guess" method, likely to perform well, while the \framework{} serves as a comprehensive toolbox for developing new tailored UQD algorithms.


% 5. Experimental study and implementation. 

To demonstrate the benefits of our contributions, we present two experimental studies.
First, we evaluate \name{} on a set of common UQD tasks from prior work. Our results show that \name{} consistently performs at least as well as the best-performing method for each task, unlike previous methods, which exhibit varying performance across tasks. Second, we explore how integrating our \framework{} with widely-used QD approaches leads to significant performance improvements. Here we consider the well-established PGA-MAP-Elites~\cite{nilsson2021policy} algorithm and show how accounting for uncertainty using our framework drastically improves its performance on uncertain tasks at no additional evaluation cost. 
% Additionally, we release an open-source Jax implementation~\cite{jax2018github} of our contributions, compatible with the QDax library~\cite{chalumeau2024qdax, lim2022accelerated}, facilitating reuse in future work.
We hope these contributions help lower the cost of integrating UQD insights into QD algorithms, thereby expanding their applicability.

