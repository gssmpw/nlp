\section{Method}

This section introduces our two contributions: the \Longframework{} (\framework{}) and one of its instantiations \Longname{} (\name{}). 
When faced with a new UQD task, \name{} constitutes a solid "first guess" method, while our \framework{} is meant as a toolbox to develop tailored UQD algorithms. 
For clarity, we introduce \name{} first, before generalising it into the \framework{}. 


\subsection{\Longname{}}

\name{} is an adaptive-sampling UQD approach (see Section~\ref{sec:adaptive_sampling}) that tackles uncertainty by re-evaluating part of the elites in the archive $\mathcal{A}$ during the optimisation process. 
As shown in Figure~\ref{fig:algo}, \name{} follows the same loop as ME but adds three components: an extraction mechanism, an estimation mechanism and an archive depth. 
In the following, we introduce each of these three components and detail the relation between \name{} and existing similar approaches. 

\subsubsection{Extraction} \label{sec:extraction}
\name{} proposes to dedicate a fixed proportion (here $25\%$) of the evaluations per generation to re-evaluating elites and leave the rest to new offspring. 
For example, if ME evaluates $128$ offspring per generation, \name{} would only generate and evaluate $96$ new offspring, and use the remaining $32$ evaluations to re-evaluate elites already in the archive. 
When selected to be re-evaluated, elites are "extracted" from the archive, meaning that they are removed from the archive. This is unlike parents chosen to create new offspring that are typically "copied" from the archive. 
As detailed in Section~\ref{sec:adaptive_sampling}, re-evaluated elites tend to "drift". i.e. prove to belong to another cell than the one they were originally in. The extraction in \name{} accounts for this by concatenating extracted elites and new offspring into an array of solutions that can be evaluated and added to the archive together following the same rules, as illustrated in Figure~\ref{fig:algo}. 
% Extracted elites can easily drift from one cell to another if their re-evaluation proves that they belong to another cell than the one they were originally in. 
In \name{}, elites are extracted randomly from the archive to be re-evaluated, following a probability law detailed in Section~\ref{sec:depth}. 


\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{Figures/algo.pdf}
  \caption{
    Comparison of (A) the existing ME algorithm with (B) our proposed \name{} algorithm, which is designed to address the uncertainty present in UQD tasks. 
    \name{} introduces an extraction mechanism that re-evaluates solutions within the archive, an estimation mechanism and a depth to the archive. 
    % The extraction mechanism removes solutions from the archive and only re-adds them after the re-evaluation.
  }
  \Description{Illustration of the difference between our proposed \name{} and MAP-Elites}
  % \vspace{-4mm}
  \label{fig:algo}
\end{figure}


\subsubsection{Performance Estimation} 
In \name{}, each solution has a buffer of fitness and descriptor feature evaluations, used to store all its evaluations, whether they come from its first evaluation or from re-evaluations. 
The evaluation process concatenates new evaluations to this buffer as shown in Figure~\ref{fig:algo}, accumulating information that is used to infer better and better estimates of the performance of elites as they get extracted and re-evaluated. 
% We note that in our implementation of \name{}, each solution is sampled twice during its first evaluation to get better initial estimates, as done in AS~\cite{flageat2024exploring}. 

\subsubsection{Archive Depth} \label{sec:depth}
\name{} maintains a depth in each cell that contains the $d$ more promising elites as illustrated in Figure~\ref{fig:algo} (we use $d=8$). Only the top layer of this depth is returned as the final archive, the rest of the depth is only used during optimisation, as a memory buffer of promising solutions. Elites in the depth might move to the top when they get re-evaluated or replace an extracted elite.
In \name{}, elites are extracted from the depth with a probability exponentially-proportional to their rank in their cell: the higher in the depth of the cell a solution is, the more likely it would get re-evaluated. Using exponential (instead of linear, for example) enables using a significantly larger depth $d$ while maintaining a higher probability to extract the best-performing solutions (i.e. maintains extraction pressure). 
At each generation of \name{}, the extraction phase leaves empty slots in each cell, which are filled by shifting upward the solutions stored lower in the depth.

\subsubsection{Relation to AS and Adapt-ME} \name{} is an adaptive-sampling approach, like AS and Adapt-ME (see Section~\ref{sec:adaptive_sampling}), this section details how they compare.
Unlike AS, \name{} adapts to the number of evaluations available per generation instead of re-evaluating the whole archive content. Lifting this constraint is crucial as it makes \name{} independent of the total archive size and allows it to generalise to a wide range of domains with large archives or limited evaluations per generation. 
Additionally, \name{} distributes its evaluation more wisely than AS, as it has a higher chance to re-evaluate solutions higher in the depth, while AS re-evaluates all solutions in the depth equally. 
Unlike Adapt-ME, \name{} is parallelisable and has a fixed per-generation evaluation budget, drastically reducing its run time. 
Additionally, \name{} re-evaluates any elites in the archive and not only the ones that get challenged by offspring, thus improving performance estimation across the descriptor space. 
%This might allow us to spot elites that artificially filled cells that they do not belong to thanks to luck and to avoid returning them in the final collection. 


\subsection{\Longframework{}} \label{sec:framework}


The \framework{} proposes a general modular view of all existing UQD approaches introduced in Section~\ref{sec:uqd}, including our proposed \name{}. 
It builds on top of the QD-Framework from Section~\ref{sec:qd_framework}, as illustrated in Figure~\ref{fig:framework}. 
In this section, we first introduced its modules, before detailing how it encompasses previous approaches and how it enables building new UQD approaches. 

\subsubsection{Modules} The \framework{} comprises the following modules, also illustrated in Figure~\ref{fig:framework}:

\begin{itemize}
    \item \textbf{Selection Operator:} as in the original QD Framework, this operator selects parents to mutate new offspring solutions. Possible Selection Operators include uniform selection, fitness-proportional selection, biased wheel, or random tournament~\cite{framework}. 

    \item \textbf{Variation Operator:} this operator was present in the original QD Framework, though not explicitly identified as a module. Recent developments in QD algorithms have focused on modifying this operator~\cite{tjanaka2022approximating, pierrot2022diversity, nilsson2021policy, faldor2023synergizing, flageat2024enhancing}, making it crucial in algorithm design. Thus, we include it as a distinct module in both the QD Framework and the \framework{} in Figure~\ref{fig:framework}. It applies mutation to evolve parent solutions into offspring. It includes polynomial or Gaussian random mutations as well as more complex mechanisms such as gradient-based mutations. 

    \item \textbf{Extraction Operator:} at each generation, part of the evaluations are spent re-evaluating elites that are extracted from the container (as in \name{}, see Section~\ref{sec:extraction}). The Extraction Operator chooses which and how many elites are re-evaluated. 
    It includes cases where elites are extracted randomly, as in \name{}, as well as cases where a fixed deterministic subset of elites is extracted, such as in AS where the entire population is extracted (Section~\ref{sec:adaptive_sampling}). 
    The concatenation of the solutions output by the Variation and Extraction Operators forms the overall population to evaluate, as shown in Figure~\ref{fig:framework}. 

    \item \textbf{Container with Depth:} the \framework{} keeps the concept of container from the original QD Framework but proposes to augment it with a depth $d$. The container organizes solutions into an ordered collection that covers the feature descriptor space and constructs the final collection. Adding depth $d$ involves maintaining a memory buffer of $d$ promising elites for each neighbourhood of the feature descriptor space. Thus, a depth of $d=1$ corresponds to the container of the standard QD Framework. 
    % As in \name{}, the depth is only used during optimisation and not returned as part of the final collection. 
    This paper focuses on ME-based algorithms, where the feature descriptor neighbourhoods are the cells of the grid, so the depth is directly added to the cells as in \name{}. 
    However, this can be extended to other types of containers. 
    In NSLC-based approaches, the container structure autonomously emerges from the encountered solutions, defining neighbourhoods~\cite{lehman2011evolving}. These neighbourhoods are updated when encountering better solutions, so they can maintain $d$ solutions instead of a single one. 
    For multi-objective QD that maintain pareto-fronts~\cite{pierrot2022multi, janmohamed2023improving}, the depth can maintain multiple Pareto fronts via Pareto ranking, as done in similar works in EC~\cite{siegmund2013comparative, siegmund2015hybrid}. 
    
    \item \textbf{Depth-Ordering Operator:} this operator ranks the elites within the depth of the container. Thus, it defines which elites belong to the "top" layer and are returned as part of the final collection. 
    This operator enables the incorporation of more complex mechanisms than simply fitness-based ranking. For example, previous UQD works~\cite{flageat2024exploring} have used a combination of fitness and reproducibility to order solutions within the depth, not necessarily returning the highest-fitness one.

    \item \textbf{Samples:} the number of samples spent on the first evaluation of solutions is an important parameter in uncertain tasks, so we include it as a framework's parameter. 
    % We refer to it as "initial samples" in opposition to the re-sampling after extraction. 

\end{itemize}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{Figures/framework.pdf}
  \caption{
    Illustration of (A) the existing QD Framework for the QD setup, and (B) our proposed \framework{} that extends it for uncertain tasks. The \framework{} encompasses existing UQD approaches and allows practitioners to easily build new tailored approaches for UQD tasks.
  }
  \Description{Illustration of the difference between our proposed \framework{} and the QD-Framework}
  % \vspace{-4mm}
  \label{fig:framework}
\end{figure}


\input{Tables/appraoches_table}


\subsubsection{UQD Approaches in the \framework{}}
A key strength of our \framework{} is its ability to encompass existing UQD approaches, as shown in Table~\ref{tab:approaches}. This section details the most common approaches, we provide the others in Appendix~\ref{app:framework}.

Although \textbf{ME}~\cite{map_elites} is not a UQD method, we begin by showing its compatibility with our framework, as it forms the foundation of all other approaches.
ME utilizes a grid-based container and applies uniform selection over the grid as its selection operator, consistent with the original QD Framework~\cite{framework}. ME also has a depth of $1$, does not use extraction (i.e., number of extracts of $0$ with any extraction operator), and retains only the highest-fitness solution in each cell (i.e., depth-ordering operator based solely on fitness). 
\textbf{ME-Sampling}~\cite{adaptive}, the most widely spread UQD approach, is identical to ME but uses $32$ samples for each solution instead of $1$. Other fixed-sampling approaches such as \textbf{ME-Low-Spread}~\cite{mace2023quality} or \textbf{ME-Weighted}~\cite{flageat2024exploring} simply swap the depth-ordering operator to use a mixture of fitness and reproducibility to choose the best solutions. 
\textbf{MOME-X}~\cite{flageat2024exploring} swaps the grid container for a MOME container~\cite{pierrot2022multi} that maintains a Pareto front of solutions within each cell. 
\textbf{Deep-Grid}~\cite{flageat2020fast} uses a depth of $32$ solutions in each cell and fitness-proportional selection operator within this depth.
% Additionally, in Deep-Grid, new solutions can replace any solutions already in the cell, no matter the results of their evaluations, but only the highest-fitness ones are returned as the final collection.
It also uses a depth-ordering operator based on seniority and fitness, in which only the latest solutions are kept within the depth, and ordered based on fitness.
\textbf{AS}~\cite{flageat2023uncertain} also uses depth and is characterised by a periodic re-evaluation of all elites in the archive that corresponds to an extraction operator that extracts the full content of the archive. 


\subsubsection{Building New UQD Approaches using the \framework{}}
The \framework{} also aims to identify the modular components that can be swapped to design new UQD approaches. These modules can be customised using various types of knowledge, such as task-specific insights.
% For a simple example, consider a task where all fitness values are known to be identical. In this case, the uncertainty in the descriptor becomes the primary focus, and the depth-ordering operator can be redefined to reflect this.
Consider a task where the distributions of fitness and descriptors are known, this knowledge can be utilised to design improved depth-ordering or extraction operators, for example, based on confidence as done in EC~\cite{lecarpentier2022lucie}.
% Additionally, operators can incorporate other domain-specific information. 
Alternatively, if the mapping from genotype to descriptor and fitness is known to be smooth, neighbouring solutions in the genotype space can be used to approximate descriptors and fitness values, reducing the need for evaluations, as done in similar work in EC~\cite{ea_archive}.
Similarly, our \framework{} enables incorporating uncertainty-handling mechanisms with existing QD approaches. 
In our experiments, we propose Extract-PGA (EPGA), a variant of \name{} that swaps the Variation Operator module to use the variation from the widely-spread PGA-ME algorithm~\cite{nilsson2021policy}, adapting it for uncertain tasks.
