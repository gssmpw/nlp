% \vspace{1mm} 
\section{Background and Related Work}


\subsection{Quality-Diversity} \label{sec:qd}

Quality-Diversity (QD) optimisation~\cite{book_chapter, pugh, framework} aims to find collections of solutions to an optimisation task that are both diverse and high-performing. While traditional optimisation algorithms focus on producing a single high-fitness solution, QD algorithms instead generate an archive $\mathcal{A}$ of solutions: a population of solutions that maximise fitness while maintaining diversity across predefined dimensions. These diversity dimensions are typically defined as part of the optimisation task, we refer to them as feature descriptors.

\subsubsection{MAP-Elites} The most widely used QD method is \textbf{MAP-Elites (ME)}~\cite{map_elites}, illustrated in Figure~\ref{fig:algo}.A. ME partitions the feature descriptor space into a grid of cells and retains only the highest-performing solution in each cell, known as the elite of the cell. The collection of elites constitutes the archive $\mathcal{A}$. To populate and refine $\mathcal{A}$, ME operates iteratively over a fixed number of generations. In each generation, a batch of solutions is copied uniformly from $\mathcal{A}$ and mutated to produce offspring, which are then evaluated and added back into $\mathcal{A}$. 
In ME, new offspring are added to $\mathcal{A}$ if they either occupy an empty cell or outperform the current elite in their cell. 
The main alternative to ME is Novelty Search with Local Competition (NSLC)~\cite{lehman2011evolving}. 
All methods in this work build upon ME, following previous work in QD applied to uncertain domains, but our proposed \framework{} includes NSLC-based approaches. 
% , as ME appeared to outperform NSLC on a wide range of tasks~\cite{huber2023quality}. 

\subsubsection{QD Framework} \label{sec:qd_framework}
The QD Framework~\cite{framework} aims to unify QD algorithms, be they ME-based or NSLC-based. It proposes to decompose QD algorithms into two main building blocks: (1) the container, which gathers all the solutions found so far into an ordered collection (the grid in ME), and (2) the selection operator, which chooses the solutions used as parents to generate the next generation of offspring (uniform sampling from the grid in ME).
In this work, we propose an extension of this framework for uncertain tasks in QD. 

% \vspace{1mm} 

\subsection{Uncertain Quality-Diversity} \label{sec:uqd}

Standard QD algorithms assume that the fitness $f_i$ and feature descriptor $d_i$ of a solution $i$ can be reliably estimated from a single evaluation. 
On the contrary, Uncertain QD (UQD)~\cite{flageat2023uncertain} assumes that each solution can get a distribution of possible fitness and feature descriptor values: $f_i \sim \mathcal{D}_{f_i}$ and $d_i \sim \mathcal{D}_{d_i}$. This distinction is illustrated in Figure~\ref{fig:uqd}.
This Section covers existing UQD works, defining the sub-problems that arise in UQD setups and introducing existing approaches. 
% Standard QD algorithms applied to UQD setups tend to retain “lucky” solutions: those with overestimated $f$ or $d$ due to favourable but unrepresentative evaluations. Standard QD algorithms also have no incentive to favour solutions that are less impacted by the uncertainty at hand. 
% This Section first presents the problems that arise in UQD tasks, followed by an overview of existing UQD approaches. 
% This reliability is crucial for comparing solutions when updating the archive $\mathcal{A}$. Due to this assumption, standard QD algorithms lack mechanisms to handle scenarios where evaluations yield uncertain or variable estimates.
% In complex tasks, such as robotics, for example, stochastic dynamics or inherent noise can cause a solution’s performance to vary across evaluations. As a result, QD algorithms may retain “lucky” solutions: those with overestimated $f$ or $d$ due to favourable but unrepresentative evaluations. For instance, in a standard QD algorithm, a robotic controller that successfully walks only $1\%$ of the time might still be added to $\mathcal{A}$ if it happens to walk during its single evaluation. These outliers can undermine the archive’s quality, returning solutions that are neither reliably high-performing nor diverse.
% This is known as the Uncertain QD (UQD) setup~\cite{flageat2023uncertain}, where fitness and feature values are represented as distributions: $f \sim \mathcal{D}_f$ and $d \sim \mathcal{D}_d$. Several variants of QD algorithms have been proposed for this UQD setup.
We note that optimization under uncertainty has been independently extensively studied within the Evolutionary Computation (EC) community~\cite{ea_uncertain, ea_uncertain_2}.



\subsubsection{Uncertain Quality-Diversity Sub-Problems} \label{sec:problems}
Three main sub-problems emerge from the UQD literature: 
\begin{enumerate}
    \item \textbf{Performance Estimation:} the first problem in UQD is to accurately estimate the expected fitness and descriptor features of solutions within a trackable number of samples~\cite{adaptive, flageat2020fast, flageat2023uncertain}. 
    QD algorithms usually rely on a single evaluation of each solution. Due to their elitism, they tend to retain “lucky” solutions: solutions with overestimated $f$ or misplaced $d$, caused by favourable but unrepresentative (i.e. outlier) evaluations. 
    Thus, single-sample approaches do not translate well to UQD tasks. 
    On the other end of the spectrum, approaches that evaluate every solution enough times to obtain highly accurate estimates of $f$ and $d$ are likely to be computationally intractable.
    Thus, effectively distributing samples across solutions to accurately estimate solutions' performance and keep truly high-performing and diverse solutions in the archive $\mathcal{A}$ is a crucial issue in UQD. 
    
    \item \textbf{Reproducibility Maximisation:} the second problem in UQD is to prioritise reproducible solutions over non-reproducible ones with similar performance.
    Reproducibility~\cite{flageat2023uncertain, grillotti2023don, mace2023quality} refers to the ability of a solution to consistently reproduce its feature descriptor across multiple evaluations (i.e. tight spread of feature descriptor distribution). Reproducibility is a desirable property in QD as it provides guarantees to the final users that solutions will behave similarly when used in a downstream task~\cite{huber2024domain, mace2023quality}. 
    For example, when finding a set of solutions that can move to different target positions, it is desirable to get solutions that consistently reach their target position all or most of the time. 
    Thus, when given the choice between two solutions that perform equivalently but have different reproducibility, UQD algorithms should favour the most reproducible one. 
    It is important to note that this problem does not always arise as in certain tasks, all solutions have the same reproducibility. 
    % only arises in tasks where solutions can get different reproducibilities. This is not the case for example of a task with a zero-mean Gaussian fitness noise.
    
    \item \textbf{Performance-Reproducibility Trade-off:} the third UQD problem is to implement preferences among solutions that present different trade-offs between fitness and reproducibility~\cite{flageat2024exploring}.
    In many UQD tasks, fitness and reproducibility are not aligned: for instance, a controller that enables a robot to walk very quickly (high fitness) may also be more prone to lead the robot to fall (low reproducibility) compared to a slower controller (lower fitness but higher reproducibility). As a result, there may not be a clearly dominating solution for the task. 
    Thus, UQD algorithms should be able to enforce preferences specified by a user over this trade-off. 
    % Given some preferences over this trade-off, defined a-priori by a user, UQD algorithms should be able to enforce them. Alternatively, they should be able to return a choice of different trade-off values for the final user to be able to choose which one to deploy a-posteriori. 
    Again, this third UQD problem only arises in tasks where solutions get different reproducibility. 
\end{enumerate}
% Specific metrics~\cite{adaptive, flageat2023uncertain, flageat2024exploring} and benchmark tasks~\cite{flageat2023benchmark, flageat2024exploring} have been introduced for each of these three problems, along with a variety of algorithms and approaches, detailed in the following sections. 
For simplicity and clarity, this paper primarily focuses on the Performance Estimation problem. However, we detail in the next sections all existing methods tackling all three problems, and we show that our \framework{} generalizes to all of them. 

\subsubsection{Fixed-Sampling UQD Approaches} \label{sec:fixed_sampling}
This is the most common type of UQD approach. They re-evaluate each solution $N$ times, where $N$ is a fixed number, to estimate its expected fitness and feature descriptor before adding it to the archive $\mathcal{A}$.
\textbf{ME-Sampling}~\cite{hbr}, the most common fixed-sampling approach, simply adds solutions to $\mathcal{A}$ based on the average of the $N$ reevaluations, thus only focusing on the Performance Estimation problem (see Section~\ref{sec:problems}).
\textbf{ME-Reprod}~\cite{grillotti2023don}, \textbf{ME-Low-Spread}~\cite{mace2023quality}, \textbf{ME-Weighted}~\cite{flageat2024exploring}, \textbf{ME-Delta}~\cite{flageat2024exploring} and \textbf{MOME-Reprod}~\cite{flageat2024exploring} are all variants of ME-Sampling that also tackle the other two sub-problems of UQD. To do so, they use the $N$ samples to also estimate solution reproducibility and optimise different combinations of the fitness and reproducibility values. 
% As mentioned, this work primarily focuses on performance estimation and thus only uses ME-Sampling as a baseline.
Despite their ease of implementation, fixed-sampling approaches induce high sampling costs, limiting their applications. Additionally, they have been shown to hinder exploration by overlooking fragile solutions that might be key intermediate step toward promising solutions~\cite{flageat2023uncertain, flageat2024exploring, ea_uncertain_2}.


\subsubsection{Adaptive-Sampling UQD Approaches} \label{sec:adaptive_sampling}
This type of UQD approach, inspired by similar works in EC~\cite{ea_adaptive, ea_adaptive_2}, aims to lower the sampling cost of fixed-sampling approaches by distributing samples more wisely. To do so, instead of evaluating every solution a fixed number of times, they only reevaluate promising solutions~\cite{adaptive, flageat2023uncertain}. 
In \textbf{Adaptive-Sampling-ME (Adapt-ME)}~\cite{adaptive}, an offspring solution can replace an elite only if it is still better after having been sampled the same number of times as this elite. 
Additionally, each time a new solution proves less performing than an elite, this elite is re-sampled, leading powerful elites to be more and more evaluated, increasing the certainty about their values. As re-evaluated elites may prove to belong to another cell than the one they were initially evaluated into, elites might need to "drift" to other cells during the optimisation. To prevent a cell from suddenly becoming empty, Adapt-ME keeps multiple candidate elites per cell. 
This is known as a "depth", for example, an archive of depth $d=8$ would keep $8$ elites in each cell. Among these $8$ elites, only $1$, the best performing one, or "top" elite is returned in the final archive $\mathcal{A}$, but the others are used during the optimisation to replace drifting elites. 
While Adapt-ME has proven promising, its major drawback is that its evaluations can only be partially parallelisable, as the number of evaluations for one solution is directly dependent on the evaluations of other solutions.
This reason led to propose \textbf{Archive-Sampling (AS)}~\cite{flageat2023uncertain}, a simple variant of Adapt-ME that simply re-evaluates all the solutions in the archive $\mathcal{A}$ at each generation. AS also keeps a depth $d$ of elites, and re-evaluates the elites in the depth with the one in the top-layer at each generation. 
While simpler than Adapt-ME, AS has been shown to significantly outperform fixed-sampling approaches~\cite{flageat2023uncertain,flageat2024exploring}, while Adapt-ME has shown more contrasted results~\cite{adaptive}. To the best of our knowledge, these two approaches have not been directly compared before. 
While Adapt-ME and AS only explicitly tackle the Performance Estimation problem (see Section~\ref{sec:problems}), \textbf{AS-Weighted}~\cite{flageat2024exploring} and \textbf{AS-Delta}~\cite{flageat2024exploring} are both variants of AS that also address the other two sub-problems of UQD by approximating the reproducibility of solutions from re-evaluations. 


\subsubsection{Other UQD Approaches}  \label{sec:deep_grid}
Two other approaches have been proposed for UQD that do not belong to the two previous categories. 
\textbf{Deep-Grid}~\cite{flageat2020fast} is based on the observation that "lucky" solutions (those with overestimated fitness due to stochastic evaluations) tend to dominate archive cells, preventing more representative solutions from replacing them. This can escalate over time, as only increasingly luckier solutions can displace these outliers, leading to unrealistic fitness values within the archive.
To counteract this effect, Deep-Grid constantly "questions" elites: it allows them to be replaced by any solution, even those that appear lower performing. To still optimise performance, Deep-Grid maintains a depth $d$ of elites (see adaptive-sampling approaches) and selects parents fitness-proportionally within cells.
Alternatively, \textbf{ARIA}~\cite{grillotti2023don} is an optimisation module that aims to improve the reproducibility of the solutions contained in the final population returned by any QD algorithm. While ARIA tackles the UQD setup, it has been designed to be run on top of another QD algorithm, and requires an order of magnitude more evaluations than any other UQD algorithm, making it hard to compare. 
For these reasons, we do not include it in our comparison. 
Finally, we note that several works have highlighted the benefits of gradient-based and evolution-strategy-based mutations in promoting reproducible solutions~\cite{colas2020scaling, flageat2023empirical, flageat2024enhancing, mace2023quality, faldor2023synergizing, tjanaka2023training}. 
% We demonstrate in our results how these emitters can be integrated into our \framework{} for targeted tasks.
