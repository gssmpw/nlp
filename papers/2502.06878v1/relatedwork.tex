\section{Related Work}
\label{sec:related}

Machine learning techniques for alleviating class imbalance issue can be categorized under three primary directions, namely, Data level, Algorithmic level, and Hybrid level approaches \cite{sharma2022review,sowah2021hcbst}. Data-level approaches mainly focused on modifying training set data using resampling techniques such as minority class oversampling \cite{gosain2017handling} and majority class undersampling \cite{devi2020review}, with the goal of balancing the class distribution. On the other hand, algorithmic-level approaches modify the learning algorithm using techniques such as cost-sensitive learning \cite{ling2008cost} to give more importance to minority classes in the learning process. Hybrid-level approaches combine both data-level and algorithmic-level methods in provide solutions that could improve model performance on imbalanced datasets. We refer the reader to the survey articles by Tyagi et al.\cite{tyagi2020sampling}, Fernandez et al.\cite{fernandez2018algorithm}, and Ahmed et al.\cite{ahmed2023comparative} for detailed discussions on these approaches.

Our work falls into the category of Data-level approaches where we present an oversampling approach to alleviate class imbalance. Existing oversampling techniques can be divided into two categories, namely, traditional methods, and deep learning-based methods.

\paragraph{\textbf{Traditional Oversampling Methods: }} Traditional oversampling algorithms have been widely investigated and prominently applied for imbalanced classification. The most well-known oversampling technique, Synthetic Minority Over-sampling Technique (SMOTE), generates synthetic minority samples by combining existing minority data instances through linear interpolation \cite{chawla2002smote}. Building on the increased popularity of this approach, many subsequent works have been proposed with further refinements. SVMSMOTE is one such extension that combines SMOTE with principles of Support Vector Machines in order to generate minority instances that reside near the class-separating decision boundary, making the oversampling process more informative \cite{tang2008svms}. K-means SMOTE utilizes a grouping approach where they first cluster the minority class instances and then generate synthetic samples within each cluster using SMOTE, aiming to enhance the diversity in the data generation process \cite{last2017kmeanssmote}. Borderline-SMOTE \cite{han2005borderline} and ADASYN \cite{he2008adasyn} are another two techniques that aim to enhance the significance of the data generation of SMOTE by focusing on minority data instances near the decision boundary or difficult to classify, respectively. SMOTE-N and SMOTE-NC are another two extensions of SMOTE, proposed to enhance the oversampling process in categorical and continuous features effectively \cite{chawla2002smote}.

\paragraph{\textbf{Deep Learning-based Oversampling Methods: }} Deep oversampling approaches leverage deep learning models to handle synthetic data generation. DeepSMOTE \cite{dablain2022deepsmote} and GAMO \cite{mullick2019generative} have utilized Generative Adversarial Networks to generate synthetic samples for image datasets, whereas cWGAN \cite{engelmann2021conditional} employed Conditional GANs for generating synthetic data in tabular formats. Moreover, GENDA \cite{troullinou2023generative} utilized AutoEncoders (AE) for the oversampling process enabling their approach to handle imaging and time series data. These works are based on the hypothesis that the GANs and AEs excel at generating intricate, high-dimensional data and can potentially be utilized to generate minority data instances with enhanced semantic alignment. Additionally,  Ando et al. \cite{ando2017deep} proposed a Convolution Neural Network(CNN) based oversampling approach for image classification. Karunasingha et al. \cite{karunasingha2023oc} introduced OC-SMOTE-NN, an adaptive oversampling algorithm that models the SMOTE  algorithm using learnable parameters.

Our work is significantly different from these works. Firstly, GAN and AE are known to have higher training complexity and challenging training requirements such as a higher number of training epochs.  Consequently, applications utilizing those methods could suffer from model overfitting and limited generalization capabilities. Further, GAN and AE related works have limited interpretability as these models are designed to learn compact representations of data in their latent space, which may not directly correspond to interpretable features.  We use multi-layer perceptrons (MLPs) \cite{murtagh1991multilayer}, which are scalable and easy to train. Also, MPLs have higher interpretability due to their direct mapping between input and output features and are known to have the capability to approximate complex functions \cite{kruse2022multi}. Secondly, we propose utilizing a set of learnable discrete decision criteria to govern the oversampling process, which we believe is a novel perspective from all these existing works.