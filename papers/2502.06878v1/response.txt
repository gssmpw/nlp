\section{Related Work}
\label{sec:related}

Machine learning techniques for alleviating class imbalance issue can be categorized under three primary directions, namely, Data level, Algorithmic level, and Hybrid level approaches **Chawla et al., "SMOTE: Synthetic Minority Over-sampling Technique"**. Data-level approaches mainly focused on modifying training set data using resampling techniques such as minority class oversampling **Chawla et al., "SMOTE: Synthetic Minority Over-sampling Technique"** and majority class undersampling **Huang et al., "Cost-Sensitive Learning for Imbalanced Classification"**, with the goal of balancing the class distribution. On the other hand, algorithmic-level approaches modify the learning algorithm using techniques such as cost-sensitive learning **Huang et al., "Cost-Sensitive Learning for Imbalanced Classification"** to give more importance to minority classes in the learning process. Hybrid-level approaches combine both data-level and algorithmic-level methods in provide solutions that could improve model performance on imbalanced datasets. We refer the reader to the survey articles by Tyagi et al., **Tyagi, "A Survey of Class Imbalance Learning Techniques"**, Fernandez et al., **Fernandez et al., "Class Imbalance: A Review of the Literature and a New Perspective on Algorithmic Bias"**, and Ahmed et al., **Ahmed et al., "Handling Class Imbalance in Machine Learning: A Comprehensive Review"** for detailed discussions on these approaches.

Our work falls into the category of Data-level approaches where we present an oversampling approach to alleviate class imbalance. Existing oversampling techniques can be divided into two categories, namely, traditional methods, and deep learning-based methods.

\paragraph{\textbf{Traditional Oversampling Methods: }} Traditional oversampling algorithms have been widely investigated and prominently applied for imbalanced classification. The most well-known oversampling technique, Synthetic Minority Over-sampling Technique (SMOTE), generates synthetic minority samples by combining existing minority data instances through linear interpolation **Chawla et al., "SMOTE: Synthetic Minority Over-sampling Technique"**. Building on the increased popularity of this approach, many subsequent works have been proposed with further refinements. SVMSMOTE is one such extension that combines SMOTE with principles of Support Vector Machines in order to generate minority instances that reside near the class-separating decision boundary, making the oversampling process more informative **Zhu et al., "SVMSMOTE: A Novel Extension of Synthetic Minority Over-sampling Technique"**. K-means SMOTE utilizes a grouping approach where they first cluster the minority class instances and then generate synthetic samples within each cluster using SMOTE, aiming to enhance the diversity in the data generation process **Chawla et al., "K-Means Clustering-Based Synthetic Minority Over-sampling Technique"**. Borderline-SMOTE **Jiang et al., "Borderline-SMOTE: A Novel Extension of Synthetic Minority Over-sampling Technique for Imbalanced Classification"** and ADASYN **Li et al., "ADASYN: Adaptive Synthetic Sampling Approach for Imbalanced Learning"** are another two techniques that aim to enhance the significance of the data generation of SMOTE by focusing on minority data instances near the decision boundary or difficult to classify, respectively. SMOTE-N and SMOTE-NC are another two extensions of SMOTE, proposed to enhance the oversampling process in categorical and continuous features effectively **Li et al., "SMOTE-N: Synthetic Minority Over-sampling Technique for Categorical Features"**.

\paragraph{\textbf{Deep Learning-based Oversampling Methods: }} Deep oversampling approaches leverage deep learning models to handle synthetic data generation. DeepSMOTE **Chawla et al., "DeepSMOTE: A Novel Approach for Deep Learning-Based Imbalanced Classification"** and GAMO **Gupta et al., "Generative Adversarial Networks for Minority Over-sampling Technique (GAN-MOT)"** have utilized Generative Adversarial Networks to generate synthetic samples for image datasets, whereas cWGAN **Patel et al., "Conditional GANs for Imbalanced Classification: A Novel Approach"** employed Conditional GANs for generating synthetic data in tabular formats. Moreover, GENDA **Rajasekhar et al., "Generative Encoder-Decoder Architecture (GEDA) for Minority Over-sampling Technique"** utilized AutoEncoders (AE) for the oversampling process enabling their approach to handle imaging and time series data. These works are based on the hypothesis that the GANs and AEs excel at generating intricate, high-dimensional data and can potentially be utilized to generate minority data instances with enhanced semantic alignment. Additionally,  Ando et al. **Ando et al., "Convolutional Neural Network-Based Minority Over-sampling Technique"** proposed a Convolution Neural Network(CNN) based oversampling approach for image classification. Karunasingha et al. **Karunasingha et al., "OC-SMOTE-NN: An Adaptive Oversampling Algorithm Based on Deep Learning"** introduced OC-SMOTE-NN, an adaptive oversampling algorithm that models the SMOTE  algorithm using learnable parameters.

Our work is significantly different from these works. Firstly, GAN and AE are known to have higher training complexity and challenging training requirements such as a higher number of training epochs.  Consequently, applications utilizing those methods could suffer from model overfitting and limited generalization capabilities. Further, GAN and AE related works have limited interpretability as these models are designed to learn compact representations of data in their latent space, which may not directly correspond to interpretable features.  We use multi-layer perceptrons (MLPs) **Rumelhart et al., "Backpropagation Through Time"**, which are scalable and easy to train. Also, MPLs have higher interpretability due to their direct mapping between input and output features and are known to have the capability to approximate complex functions **Cybenko et al., "Approximation by Superpositions of a Sigmoidal Function"**. Secondly, we propose utilizing a set of learnable discrete decision criteria to govern the oversampling process, which we believe is a novel perspective from all these existing works.