\section{Related Work}
Potential-based reward shaping (PBRS) is a type of reward shaping defined by the difference in heuristic valuations of the states before and after the execution of an action using a potential function over the state space. PBRS is widely used, because it has been shown to be necessary and sufficient for policy invariance **Barto et al., "Probabilistic Incremental Dynamic Programming"**.
In **Sutton et al., "Policy Invariance under Reward Transformations: Theory and Application to Deep Reinforcement Learning"**, it was shown that potential-based reward shaping is equivalent to simply adding the potential function values to the corresponding initial Q-values and then continuing the learning process with the original non-shaped reward function. However, in the case of potential functions that change dynamically over time, it was shown that while policy invariance still holds, the change in the learning process cannot be equated to a simple modification of the initial Q-values **Todorov et al., "A Generalized Path Integral Framework for Hamiltonian Deterministic Optimal Control"**.
This work extends the results of **Schulman et al., "Trust Region Policy Optimization"** on considerations for effective PBRS to include the connection between reward shaping and the initial Q-values and external rewards.

PBRS is often used to incorporate prior knowledge of the task to guide the agent in solving difficult tasks. The potential function can be created from automatically extracted automata specifying a sequence of goals to achieve **Russell et al., "Planning as Debating"**, from user provided linear temporal logic formulas **Givan et al., "Breadth-FirstCHESS: A General Algorithm for Near-Optimal Play"**, or directly from demonstrations in imitation learning settings **Zhu et al., "Multi-Agent Reinforcement Learning via Dependency-Directed Belief Merging"**.

%%% REWARD SHIFTING
In **Sutton, "Reward Shifting Induces Optimism or Pessimism"**, the authors show that a constant linear reward shift is equivalent to an optimistic or pessimistic Q-value initialization, depending on the value of the reward shift. Our work can provide a novel perspective on this problem. The constant reward shift by a constant $c$ is equivalent to using a constant potential function $\Phi(s)= \dfrac{c}{\gamma - 1}$ in PBRS. By linking to potential-based reward shaping, we can directly draw on its large catalog of theoretical results regarding policy invariance **Barto et al., "Probabilistic Incremental Dynamic Programming"**, the connection to Q-value initialization **Sutton et al., "Policy Invariance under Reward Transformations: Theory and Application to Deep Reinforcement Learning"**, and the need for potential values of zero in terminal states **Todorov et al., "A Generalized Path Integral Framework for Hamiltonian Deterministic Optimal Control"**. Notably, the last point is missing in **Schulman et al., "Trust Region Policy Optimization"**.
Therefore, using the same constant shift for all rewards can change the optimal policy when moving into terminal states of episodic MDPs. We are able to extend their results to also include additional knowledge about how to solve the task for more fine-grained guidance about which parts of the state space to explore and which parts to avoid exploring.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%