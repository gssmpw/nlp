\section{Related Work}
Potential-based reward shaping (PBRS) is a type of reward shaping defined by the difference in heuristic valuations of the states before and after the execution of an action using a potential function over the state space. PBRS is widely used, because it has been shown to be necessary and sufficient for policy invariance ____. 
In ____, it was shown that potential-based reward shaping is equivalent to simply adding the potential function values to the corresponding initial Q-values and then continuing the learning process with the original non-shaped reward function. However, in the case of potential functions that change dynamically over time, it was shown that while policy invariance still holds, the change in the learning process cannot be equated to a simple modification of the initial Q-values ____.
This work extends the results of ____ on considerations for effective PBRS to include the connection between reward shaping and the initial Q-values and external rewards.

PBRS is often used to incorporate prior knowledge of the task to guide the agent in solving difficult tasks. The potential function can be created from automatically extracted automata specifying a sequence of goals to achieve ____, from user provided linear temporal logic formulas ____, or directly from demonstrations in imitation learning settings ____.

%%% REWARD SHIFTING
In ____, the authors show that a constant linear reward shift is equivalent to an optimistic or pessimistic Q-value initialization, depending on the value of the reward shift. Our work can provide a novel perspective on this problem. The constant reward shift by a constant $c$ is equivalent to using a constant potential function $\Phi(s)= \dfrac{c}{\gamma - 1}$ in PBRS. By linking to potential-based reward shaping, we can directly draw on its large catalog of theoretical results regarding policy invariance ____, the connection to Q-value initialization ____, and the need for potential values of zero in terminal states ____. Notably, the last point is missing in ____. 
Therefore, using the same constant shift for all rewards can change the optimal policy when moving into terminal states of episodic MDPs. We are able to extend their results to also include additional knowledge about how to solve the task for more fine-grained guidance about which parts of the state space to explore and which parts to avoid exploring.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%