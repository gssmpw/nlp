\section{Related Work}
Potential-based reward shaping (PBRS) is a type of reward shaping defined by the difference in heuristic valuations of the states before and after the execution of an action using a potential function over the state space. PBRS is widely used, because it has been shown to be necessary and sufficient for policy invariance \citep{ng1999invariance}. 
In \citet{wiewiora2011Qinitialization}, it was shown that potential-based reward shaping is equivalent to simply adding the potential function values to the corresponding initial Q-values and then continuing the learning process with the original non-shaped reward function. However, in the case of potential functions that change dynamically over time, it was shown that while policy invariance still holds, the change in the learning process cannot be equated to a simple modification of the initial Q-values \citep{devlin2012dynamicPBRS}.
This work extends the results of \citet{grzes2009potential-function-analysis} on considerations for effective PBRS to include the connection between reward shaping and the initial Q-values and external rewards.

PBRS is often used to incorporate prior knowledge of the task to guide the agent in solving difficult tasks. The potential function can be created from automatically extracted automata specifying a sequence of goals to achieve \citep{Hasanbeig2021DeepSynthAS}, from user provided linear temporal logic formulas \citep{elbarbari2022tlrl}, or directly from demonstrations in imitation learning settings \citep{wang2023dshape, brys2015demonstration, suay2016demonstration, wu2021demonstrations}.

%%% REWARD SHIFTING
In \citet{sun2022optimistic}, the authors show that a constant linear reward shift is equivalent to an optimistic or pessimistic Q-value initialization, depending on the value of the reward shift. Our work can provide a novel perspective on this problem. The constant reward shift by a constant $c$ is equivalent to using a constant potential function $\Phi(s)= \dfrac{c}{\gamma - 1}$ in PBRS. By linking to potential-based reward shaping, we can directly draw on its large catalog of theoretical results regarding policy invariance \citep{ng1999invariance}, the connection to Q-value initialization \citep{wiewiora2011Qinitialization}, and the need for potential values of zero in terminal states \citep{grzes2017episodicPBRS}. Notably, the last point is missing in \citet{sun2022optimistic}. 
Therefore, using the same constant shift for all rewards can change the optimal policy when moving into terminal states of episodic MDPs. We are able to extend their results to also include additional knowledge about how to solve the task for more fine-grained guidance about which parts of the state space to explore and which parts to avoid exploring.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%