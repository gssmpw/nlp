\section{Related work}
\subsection{Sequential Recommendation}
The use of deep sequential models for understanding user-item interactions in recommender systems has significantly evolved, with various approaches making notable contributions. GRU4REC \cite{hidasi2015gru4rec} introduced the use of GRU-based RNNs to model sequential user behaviors effectively. SASRec \cite{kang2018sasrec} implemented self-attention mechanisms akin to those found in decoder-only transformer models to enhance recommendation accuracy. Drawing inspiration from the success of masked language modeling in NLP, BERT4Rec \cite{sun2019bert4rec} applied transformers with masking techniques specifically tailored for sequential recommendation tasks. Additionally, TIGER \cite{rajput2024tiger} has started emphasizing the use of semantic IDs. In this approach, each item is represented by a series of tokens that reflect its related details, and the system predicts the sequence of upcoming item tokens using a seq2seq method. Additionally EAGER \cite{wang2024eager} advances the investigation by implementing a dual-stream generation architecture that incorporates both semantic and behavioral information. In this work, we extend EAGER \cite{wang2024eager} to EAGER-LLM by bridging LLMs and recommenders with dual-source knowledge-rich item indices and non-invasive multiscale alignment reconstruction, which not only enhances recommendation accuracy but also retains conversation and explanation generation abilities of LLMs. Recently, P5 \cite{geng2022p5, hua2023p5_cid} fine-tunes a pre-trained LLMs for multi-task recommender systems. In this study, we endeavor to further investigate a paradigm designed to mitigate the substantial discrepancies between LLMs in recommendation tasks and their original training tasks by integrating exogenous semantic and behavioral information.

\subsection{LLMs as Recommenders}
% Large Language Models (LLMs) have become pivotal in Natural Language Processing (NLP). 
Recently, LLMs have been utilized in recommendation tasks due to their ability to understand, generate, and infer natural language properties. LLM-based RSs \cite{liu2024store} constructs user/item correlations through its powerful high-quality textual representations and extensive external knowledge, and is expected to solve the problems of poor generalization \cite{lin2023can} and poor performance of traditional RSs on sparse historical interaction data, etc.
Chat-Rec \cite{gao2023chat} aims to enhance conversational recommendation systems by integrating ChatGPT’s interactive capabilities with established recommendation models, such as MF \cite{koren2009matrix} and LightGCN \cite{he2020lightgcn}. P5 \cite{geng2022p5} fine-tunes a pre-trained large language model for multi-task recommender systems, utilizing the LLM tokenizer (SentencePiece tokenizer) to generate tokens from randomly assigned item pseudo-IDs. M6 \cite{cui2022m6}explores the use of item text information (such as names) as identifiers for items. LC-Rec \cite{zheng2024lcrec} designs a learning-based vector quantization method to generate ID from Item’s semantic representation and proposes alignment tuning tasks to enhance the intergration of collaborative semantics in LLMs.
%However, merely using directive-based fine-tuning falls short in effectively leveraging the inherent capabilities of LLMs to understand collaborative information and inadequately learn from the implicit interaction data crucial for recommendations.
Recently, new research has emerged to bridge the significant gap between pre-trained language models and recommendation tasks. CoLLM \cite{zhang2023collm} infuses behavior information into LLMs by incorporating representations from an external collaborative model into the input.
%CLLM4Rec\cite{zhu2024cllm4rec} develops dual embeddings for users and items tailored to recommendation tasks and content generation tasks, predict items based on soft+hard prompts.
In this work, we aim to further explore recommender frameworks that can integrate endogenous and exogenous behavioral and semantic signals based on LLM.