%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\copyrightyear{2025}
\acmYear{2025}
\setcopyright{acmlicensed}\acmConference[WWW '25]{Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia} \acmBooktitle{Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia} \acmDOI{10.1145/3696410.3714933} \acmISBN{979-8-4007-1274-6/25/04}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}
\usepackage{multirow}
\usepackage{colortbl}  %彩色表格需要加载的宏包
\usepackage{xcolor}
\usepackage{array}   %对表列和表格线的设置需要用到array宏包
\usepackage{marvosym} %Letter
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{enumitem}
\definecolor{linecolor}{gray}{.89} % soft gray
\definecolor{linecolor1}{gray}{.93} % soft gray
\definecolor{linecolor2}{gray}{.97} % soft gray

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[EAGER-LLM: Enhancing LLMs as Recommenders through Exogenous Behavior-Semantic Integration]{EAGER-LLM: Enhancing Large Language Models as Recommenders through Exogenous Behavior-Semantic Integration}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Minjie Hong}
\email{hongminjie@zju.edu.cn}
\authornote{Both authors contributed equally to this research.}
\affiliation{%
  \institution{Zhejiang University}
  \city{Hangzhou}
  \state{Zhejiang}
  \country{China}
}
\orcid{0009-0000-0368-2527}

\author{Yan Xia}
\email{xiayan.zju@gmail.com}
\authornotemark[1]
\affiliation{%
  \institution{Zhejiang University}
  \city{Hangzhou}
  \state{Zhejiang}
  \country{China}
}
\orcid{0000-0003-4631-741X}

\author{Zehan Wang}
\email{wangzehan01@zju.edu.cn}
\affiliation{%
  \institution{Zhejiang University}
  \city{Hangzhou}
  \state{Zhejiang}
  \country{China}
}
\orcid{0009-0007-7509-7563}

\author{Jieming Zhu}
\authornote{Project Lead.}
\email{jiemingzhu@ieee.org}
\affiliation{%
  \institution{Huawei Noah's Ark Lab}
  \city{Shenzhen}
  \state{Guangdong}
  \country{China}
}
\orcid{0000-0002-5666-8320}

\author{Ye Wang}
\email{22151150@zju.edu.cn}
\affiliation{%
  \institution{Zhejiang University}
  \city{Hangzhou}
  \state{Zhejiang}
  \country{China}
}
\orcid{0009-0007-0974-9834}

\author{Sihang Cai}
\email{caisihang@zju.edu.cn}
\affiliation{%
  \institution{Zhejiang University}
  \city{Hangzhou}
  \state{Zhejiang}
  \country{China}
}
\orcid{0009-0007-8693-7142}

\author{Xiaoda Yang}
\email{xiaodayang@zju.edu.cn}
\affiliation{%
  \institution{Zhejiang University}
  \city{Hangzhou}
  \state{Zhejiang}
  \country{China}
}
\orcid{0009-0002-7297-4536}

\author{Quanyu Dai}
\email{daiquanyu@huawei.com}
\affiliation{%
  \institution{Huawei Noah's Ark Lab}
  \city{Shenzhen}
  \state{Guangdong}
  \country{China}
}
\orcid{0000-0001-7578-2738}

\author{Zhenhua Dong}
\email{dongzhenhua@huawei.com}
\affiliation{%
  \institution{Huawei Noah's Ark Lab}
  \city{Shenzhen}
  \state{Guangdong}
  \country{China}
}
\orcid{0000-0002-2231-4663}

\author{Zhimeng Zhang}
\email{zhimeng@zju.edu.cn}
\affiliation{%
  \institution{Zhejiang University}
  \city{Hangzhou}
  \state{Zhejiang}
  \country{China}
}
\orcid{0000-0001-9492-4252}

\author{Zhou Zhao}
\email{zhaozhou@zju.edu.cn}
\authornote{Corresponding Author.}
\affiliation{%
  \institution{Zhejiang University}
  \city{Hangzhou}
  \state{Zhejiang}
  \country{China}
}
\orcid{0000-0001-6121-0384}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Minjie Hong et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Large language models (LLMs) are increasingly leveraged as foundational backbones in the development of advanced recommender systems, offering enhanced capabilities through their extensive knowledge and reasoning.
  Existing llm-based recommender systems (RSs) often face challenges due to the significant differences between the linguistic semantics of pre-trained LLMs and the collaborative semantics essential for RSs. 
  %Typically, these systems apply pre-trained linguistic semantics while learning collaborative semantics from scratch using the LLM-Backbone. However, as LLM architectures are not inherently tailored for recommendation tasks, this approach results in inefficient learning of collaborative information, poor understanding of result correlations, and a failure to leverage traditional RSs features effectively.
  These systems use pre-trained linguistic semantics but learn collaborative semantics from scratch via the llm-Backbone. However, LLMs are not designed for recommendations, leading to inefficient collaborative learning, weak result correlations, and poor integration of traditional RS features.
  To address these challenges, we propose \textbf{EAGER-LLM}, a decoder-only llm-based generative recommendation framework that integrates endogenous and exogenous behavioral and semantic information in a non-intrusive manner.
  Specifically, we propose 1) dual-source knowledge-rich item indices that integrates indexing sequences for exogenous signals, enabling efficient link-wide processing; 2) non-invasive multiscale alignment reconstruction tasks guide the model toward a deeper understanding of both collaborative and semantic signals; 3) an annealing adapter designed to finely balance the model’s recommendation performance with its comprehension capabilities.
  We demonstrate EAGER-LLM’s effectiveness through rigorous testing on three public benchmarks.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003317.10003347.10003350</concept_id>
       <concept_desc>Information systems~Recommender systems</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Recommender systems}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Generative Recommendation; Large Language Models; LLM-as-Recommender; Behavior-Semantic Collaboration}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Recommender systems are tools designed to alleviate the phenomenon of information overload in Web environments by algorithmically analyzing user behavior to predict and push content that may be of interest to users.
Typical recommender systems \cite{wang2021two_tower, hidasi2015gru4rec, li2017neural,kang2018sasrec} encode users and items as latent representations within a shared space to capture semantic similarities, followed by efficient retrieval using Approximate Nearest Neighbors (ANNs) algorithms \cite{johnson2019faiss, guo2020scann}. The distinct separation of these two phases often introduces performance limitations due to the absence of end-to-end optimization.

%The emerging paradigm of recommender systems (RS) leveraging pre-trained large language models (LLMs) is showing great promise. Research across domains like vision\cite{liu2024llava, alayrac2022flamingo}, speech\cite{ji2024wavtokenizer, chu2023qwen}, and multimodality\cite{zhu2023minigpt} demonstrates the broader applicability of LLMs, where various task instructions are encoded in language and fused with other forms, optimized via end-to-end training to adapt effectively to target domains. This paradigm harnesses the deeply embedded knowledge and logical reasoning capabilities of LLMs to discern intricate associations between user behavior and item semantics, leading to more accurate and nuanced recommendations.

Research across domains like vision \cite{liu2024llava, alayrac2022flamingo}, speech \cite{ji2024wavtokenizer, chu2023qwen}, and multimodality \cite{zhu2023minigpt} demonstrates the broader applicability of LLMs. Several recent studies investigate the potential roles of LLMs in recommender systems (RSs). Unlike traditional models that encode users and items as embedding vectors, some LLM-based RSs \cite{hou2024large, cui2022m6, li2023personalized, bao2023tallrec} converts user behaviors and preferences, alongside the candidate item set, into discrete natural language text sequences or prompts. These prompts are then used to extract item-related information from the LLM’s textual outputs. \cite{geng2022p5, hua2023p5_cid, liu2024store, zhu2024cllm4rec} enhance collaboration by incorporating additional or existing tokens into the LLM to represent user and item IDs, which are then fine-tuned during specialized training to fit interaction data. \cite{wang2024letter, zhang2023collm} employs exogenous collaboration models to obtain collaboration embeddings, which are integrated into the inputs of the LLM, thereby enriching the recommendation process. 

But these paradigms suffer from several flaws: \textbf{(1)} While the plain text approach can yield favorable outcomes in zero-shot recommendation \cite{cui2022m6, hou2024large} , it primarily analyzes only the surface-level textual semantics of behavioral sequences. This method heavily relies on candidate sets and incurs significant computational overhead when modeling extensive historical sequences. \textbf{(2)} In real-world applications, where the number of candidate recommendation items vastly exceeds the vocabulary size of LLMs, the tokenization redundancy introduced by Vanilla IDs complicates LLMs’ ability to accurately interpret commands. This redundancy results in low learning efficiency and a failure to effectively leverage semantic features. \textbf{(3)} The substantial disparity between the domains of external collaborative signals and the semantic signals of pre-trained LLMs means that directly integrating these signals can significantly disrupt the original functionalities of the LLMs. Consequently, the model struggles to effectively process and interpret the information contained in these external signals \cite{wang2024letter, zhang2023collm}.

\begin{figure}[t]
  \centering 
  \includegraphics[width=.98\linewidth]{figures/figure1.pdf}
  \caption{{\bf A framework of LLM-as-Recommender. It frames recommendation as next token generation tasks, which directly generate target items.} }
  \label{fig:intro}
\end{figure}

To overcome the challenges outlined, we introduce \textbf{EAGER-LLM}, a novel decoder-only LLM-based generative recommendation framework. In this framework, we compress massive exogenous signals into a few newly added tokens with extremely high compression ratios. Additionally, we incorporate a non-invasive multi-scale alignment reconstruction tasks and Multi-Stage Training that facilitates an efficient understanding and integration of exogenous behaviors, semantic signals, and recommendation data knowledge with the LLM’s original parameters. Our approach is detailed through three aspects:

Primarily, we introduce \textbf{Dual-source Knowledge-rich Item Indices} to address the inefficiencies of previous approaches that used atomic tokens to represent item IDs, which resulted in tokenization redundancy and overly discrete and independent semantics that did not effectively support the recommendation task. Our method efficiently characterizes large candidate sets with a small number of identifiers, incorporating a useful priori knowledge with a high compression ratio to integrate exogenous semantic and behavioral information into the decoding inference process. We implement an indexing structure where semantically similar items share identifier prefixes. Given the distinct domain differences between behavioral and semantic feature spaces, prior research in multimodal and bimodal models \cite{chen2024internvl} has shown that even advanced encoder-side feature fusion approaches like Q-former \cite{li2023blip} are insufficient for effective integration of dual-source features. Consequently, we discretize and separately splice the exogenous behavioral and semantic signals. This decoupled indexing scheme minimizes information loss from encoder-side feature fusion and enables the model to more effectively represent the complex interplay between behavior and semantics during subsequent training.

Furthermore, we have introduced \textbf{Non-Invasive Multiscale Alignment Reconstruction Tasks}. %This approach helps the model process complex collaborative and semantic signals from tokens rich with exogenous information, while integrating the LLM’s own parameters and reasoning capabilities to deepen its understanding of recommendations and associated tasks. 
Given the vast amount of exogenous semantic and behavioral signals compressed into a small number of tokens at a very high compression ratio, it is challenging for the model to directly assimilate adequate exogenous knowledge. To address this, we have devised the Global Contrast Decompression Task and Comprehensive Interaction Modeling Tasks. These initiatives aid the model in decompressing extensive exogenous knowledge from a limited number of highly compressed tokens. By incorporating additional summarization tokens and leveraging the restricted context of recommendation data, these tasks effectively minimize the domain gap between natural language and collaborative semantics, enhancing the efficiency of the recommendation process. In addition, we introduced a multi-stage training scheme centered on the \textbf{Annealing Adapter}, which flexibly balances recommendation accuracy and model text inference capability.
The contributions of this paper can be summarized as follows:
\begin{itemize}[leftmargin=*]
    \item We present EAGER-LLM, an innovative decoder-only LLM-based generative recommendation framework that synergistically integrates endogenous and exogenous behavioral and semantic information
    \item We propose Dual-source Knowledge-rich Item Indices, Multiscale Alignment Reconstruction Tasks and the Annealing Adapter that non-intrusively guide the model towards a deep understanding of collaborative and semantic signals. 
    \item Extensive experiments across three public recommendation benchmarks demonstrate the superiority of EAGER-LLM over existing methods, emphasizing its effectiveness and robustness.
\end{itemize}
\begin{figure*}[t]
  \centering 
  \includegraphics[width=0.9\linewidth]{figures/figure2.pdf}
  \caption{{\bf \bf The illustration of Dual-source Knowledge-rich Item Indice. Incorporate massive exogenous behavioral semantic signals into project indices with extremely high compression ratios.} }
  \label{fig:code}
\end{figure*}
\section{Related work}
\subsection{Sequential Recommendation}
The use of deep sequential models for understanding user-item interactions in recommender systems has significantly evolved, with various approaches making notable contributions. GRU4REC \cite{hidasi2015gru4rec} introduced the use of GRU-based RNNs to model sequential user behaviors effectively. SASRec \cite{kang2018sasrec} implemented self-attention mechanisms akin to those found in decoder-only transformer models to enhance recommendation accuracy. Drawing inspiration from the success of masked language modeling in NLP, BERT4Rec \cite{sun2019bert4rec} applied transformers with masking techniques specifically tailored for sequential recommendation tasks. Additionally, TIGER \cite{rajput2024tiger} has started emphasizing the use of semantic IDs. In this approach, each item is represented by a series of tokens that reflect its related details, and the system predicts the sequence of upcoming item tokens using a seq2seq method. Additionally EAGER \cite{wang2024eager} advances the investigation by implementing a dual-stream generation architecture that incorporates both semantic and behavioral information. In this work, we extend EAGER \cite{wang2024eager} to EAGER-LLM by bridging LLMs and recommenders with dual-source knowledge-rich item indices and non-invasive multiscale alignment reconstruction, which not only enhances recommendation accuracy but also retains conversation and explanation generation abilities of LLMs. Recently, P5 \cite{geng2022p5, hua2023p5_cid} fine-tunes a pre-trained LLMs for multi-task recommender systems. In this study, we endeavor to further investigate a paradigm designed to mitigate the substantial discrepancies between LLMs in recommendation tasks and their original training tasks by integrating exogenous semantic and behavioral information.

\subsection{LLMs as Recommenders}
% Large Language Models (LLMs) have become pivotal in Natural Language Processing (NLP). 
Recently, LLMs have been utilized in recommendation tasks due to their ability to understand, generate, and infer natural language properties. LLM-based RSs \cite{liu2024store} constructs user/item correlations through its powerful high-quality textual representations and extensive external knowledge, and is expected to solve the problems of poor generalization \cite{lin2023can} and poor performance of traditional RSs on sparse historical interaction data, etc.
Chat-Rec \cite{gao2023chat} aims to enhance conversational recommendation systems by integrating ChatGPT’s interactive capabilities with established recommendation models, such as MF \cite{koren2009matrix} and LightGCN \cite{he2020lightgcn}. P5 \cite{geng2022p5} fine-tunes a pre-trained large language model for multi-task recommender systems, utilizing the LLM tokenizer (SentencePiece tokenizer) to generate tokens from randomly assigned item pseudo-IDs. M6 \cite{cui2022m6}explores the use of item text information (such as names) as identifiers for items. LC-Rec \cite{zheng2024lcrec} designs a learning-based vector quantization method to generate ID from Item’s semantic representation and proposes alignment tuning tasks to enhance the intergration of collaborative semantics in LLMs.
%However, merely using directive-based fine-tuning falls short in effectively leveraging the inherent capabilities of LLMs to understand collaborative information and inadequately learn from the implicit interaction data crucial for recommendations.
Recently, new research has emerged to bridge the significant gap between pre-trained language models and recommendation tasks. CoLLM \cite{zhang2023collm} infuses behavior information into LLMs by incorporating representations from an external collaborative model into the input.
%CLLM4Rec\cite{zhu2024cllm4rec} develops dual embeddings for users and items tailored to recommendation tasks and content generation tasks, predict items based on soft+hard prompts.
In this work, we aim to further explore recommender frameworks that can integrate endogenous and exogenous behavioral and semantic signals based on LLM.
\section{METHODOLOGY}
\begin{figure*}[t]
  \centering 
  \includegraphics[width=0.95\linewidth]{figures/figure3.pdf}
  \caption{{\bf Overview of Non-Invasive Multiscale Alignment Reconstruction Tasks. Includes Global Contrast Decompression Task (GCT), Comprehensive Interaction Modeling Task (CIT). These tasks facilitate the LLM-Backbone’s comprehension of complex exogenous signals from these densely packed, knowledge-rich tokens in a non-invasive manner, and to integrate the llm's  reasoning capabilities for deep understanding of recommendations.} }
  \label{fig:framework}
\end{figure*}
\subsection{Problem Formulation}
Sequential recommendation is a crucial metric in LLM-based recommender systems (RSs). We transform the traditional two-tower model, which computes similarity followed by reordering, into a generative recommendation paradigm. In this framework, each item  $\mathbf{x}$  is represented by a set of tokens ${\bf Y}=[{\bf y}_1, {\bf y}_2, \cdots, {\bf y}_k]\in \mathcal{Y}$.
As illustrated in \ref{fig:framework}, given an input sequence  $\mathbf X$, which includes instructions and the interaction history, the sequence of the target item  $\mathbf{Y}$  is generated directly in an autoregressive manner. The probability can be calculated by:
\begin{equation}
    p({\bf Y}|{\bf X}) = {\prod}_{i=1}^{k} p({\bf y}_i|\bf{X},{\bf y}_1,{\bf y}_2,\dots,{\bf y}_{i-1})
\end{equation}

\subsection{Dual-source Knowledge-rich Item Indices}\label{section:indices}
Some existing LLM-based methods utilize bracket notations as newly-introduced atomic tokens to represent items. However, this method can be problematic in data-rich real-world scenarios, where the number of potential recommended items greatly exceeds the vocabulary of LLMs. This leads to tokenization redundancy, making it challenging for LLMs to process commands accurately. Moreover, the description-based approach \cite{bao2023tallrec}, which assigns tokens to index items based on the semantics of item titles or descriptions, introduces a strong inductive bias. This can obscure the true intent of user behaviors, as it does not model behavioral sequences clearly and unbiasedly, compromising the model’s ability to understand and predict user preferences effectively.

Additionally, existing methods often overlook the value of exogenous prior knowledge. Furthermore, our experiments show that significant results are achieved when effectively integrating exogenous behavioral and semantic signals, showing subtle interaction, understanding, and cooperation.

To address our objectives, we aim to: 1) introduce a small number of tokens to efficiently represent a vast set of candidates; 2) infuse useful a priori knowledge into identifiers to incorporate exogenous semantic and behavioral information about items into the reasoning process; and 3) design an indexing structure where semantically similar items share identifier prefixes. To achieve these goals, we utilize a discretized indexing algorithm that encodes dual-source information for item representation. As illustrated in \ref{fig:code}, for any given item along with its descriptive text (title, synopsis, etc.), semantic embeddings are derived using pre-trained language models (e.g., T5 \cite{raffel2020t5}, Llama \cite{touvron2023llama}). In this study, we specifically employ the LLM-Backbone itself for semantic extraction:
\begin{equation}
\begin{split}
\text{hiddenstate}_t = llm(x_{t},&\text{hiddenstate}_{t-1}), \quad t=1,2,3, ... ,n \\
z^s_i = \frac{1}{n}\sum_{i=1}^{n}&\text{hiddenstate}_i 
\end{split}
\end{equation}
The descriptive text $x_{1:n}$ of item $i$ is entered sequentially into the last hidden state transformed by the LLM and averaged as the semantic representation  $z^s$ of the item. Behavioral features $z^b$ are extracted by the encoder of a two-tower model (e.g., DIN \cite{zhou2018din}) that uses only ID sequences as recommendations:
\begin{equation}
z^b_i = BehaviorEncoder(i)
\end{equation}

Given the domain disparities between behavioral and semantic feature spaces, prior research has shown that even advanced encoder-side feature fusion techniques (e.g., Q-former \cite{li2023blip}) often result in significant compression losses and fail to effectively integrate dual-source features. This places supernumerary strain on the decoding process. Consequently, we opt to separately and discretely process the exogenous semantic and behavioral signals. While vector quantization is commonly used for discretization, it proves unstable for training, leading to issues like item ID conflicts.

To facilitate reproducibility in our study, we employ hierarchical K-Means to discretize the semantic embedding  $Z^s$  and behavioral embedding $Z^b $ , where each cluster is progressively subdivided into  $k$  child clusters until each cluster contains only a single item. The embeddings for each item are discretized into $C^s$ and  $C^b$. Specifically, the  j-th ID of the i-th item is denoted as  $c^t_{ij}$ , where  t  includes  s  for semantic,  b  for behavioral, and  u  for the unitive. Theoretically, with each item represented by four tokens and each token capable of 256 distinct values, this method can uniquely characterize up to  $256^4=4294967296$ items. This capacity is more than adequate for real-world applications, ensuring efficiency in vocabulary expansion and the subsequent encoding and decoding processes.

\subsection{Non-Invasive Multiscale Alignment Reconstruction Tasks}
After incorporating additional tokens to represent items, we achieve a high compression ratio ($\approx$ 2,000,000:1), which significantly condenses massive exogenous signals into a very small number of tokens. This extreme compression ratio presents a challenge for the model to independently learn substantial, useful knowledge. Drawing inspiration from \cite{zheng2024lcrec, wang2024eager}, we have devised a series of multi-scale alignment reconstruction tasks. 
%These tasks facilitate the LLM’s comprehension of complex collaborative and semantic signals from these densely packed, knowledge-rich tokens in a non-invasive manner, and to integrate the llm's own parameters and reasoning capabilities for deep understanding of recommendations and their related tasks.
\subsubsection{\textbf{\textit{Global Contrast Decompression Task}}}
A method that non-intrusively enhances the model’s ability to quickly and easily interpret knowledge-rich indices at extreme compression rates. This is achieved by incorporating additional summarization token and trainable Decompression Guidance Projectors.
%This approach facilitates effective interactions between multiple exogenous signals, significantly improving the model’s efficiency and comprehension.

\begin{equation}
    \text{seq} = \{ X^{\text{Prompts}} , X_1^u, ......, X_n^u, y_1^s,......,y_{k}^b,y_{[CON]}\}
\end{equation}

Where $X^{\text{Prompts}}$ denotes the sequence of text prompts $\{x^{\text{P}}_1, ... , x^{\text{P}}_m\}$ and $X_i^u$ represents the indexed tokens for the i-th item, reflecting the user’s chronological behavior sequence. As outlined in \ref{section:indices},  $y_j^t$ denotes the j-th level of the predicted item tokens, where $t = b$ corresponds to the item’s behavioral token, and $t = s$ to the semantic token. The summary token $y_{[CON]}$ is strategically placed at the end to encapsulate the global knowledge of the preceding sequence.

To efficiently transmit exogenous dual-source signals into the preordered tokens through gradient updating, we introduce non-intrusive Decompression Guidance Projectors $f^t$  . This projector transforms the global hidden state distilled by $y_{[CON]}$ into semantically and behaviorally-guided latent states. Additionally, we employ a contrastive learning paradigm that utilizes original exogenous semantic embbeddings $Z^s$ and behavioral embbeddings $Z^b$ to accelerate and assist the decompression process of hyper-compressed Tokens.

\begin{equation}
\begin{split}
\text{hiddenstate}_t = \hat{llm}(x_{t},\text{hiddenstate}_{t-1}), \quad t=1,2,3, ... ,n \\
\mathcal{L}^{t}_{\mathrm{con}} = \mathcal{F}(f^t({\text{hiddenstate}_{\text{[CON]}})}, \mathbf{Z}^{t}), \quad t\in \{b, s\}
\end{split}
\end{equation}

The total contrastive loss $\mathcal{L}_{\mathrm{con}}$ , is calculated by proportionally summing  $\mathcal{L}^{t}_{\mathrm{con}}$ and $\mathcal{L}^{b}_{\mathrm{con}}$ . The function $\mathcal{F}(\cdot,\cdot)$ serves as the metric for contrastive learning. Importantly, the Decompression Guidance Projectors $f^t$ are utilized only during training, not in inference.

\subsubsection{\textbf{\textit{Comprehensive Interaction Modeling Task}}} To effectively harness the inference capabilities, pre-training knowledge, and trainable parameters of the LLM-Backbone for fitting recommendation data, we have restructured the traditional sequence recommendation task and its auxiliary tasks into a Next Token Prediction task, which LLMs are good at. Unlike using additional selectors as suggested by \cite{zhu2024cllm4rec}, we contend that this could alter the model’s output form and output domain distribution, potentially compromising the original capabilities of the LLM-Backbone and diminishing the framework’s generalizability across different backbones.

As illustrated in \ref{fig:framework}, Comprehensive Interaction Modeling is segmented into three subtasks: Sequence Recommendation Task, Semantic Reconstruction Task and Preference Understanding Task. These tasks effectively leverage the model’s own parameters to integrate exogenous signals, recommendation data knowledge, and the model’s intrinsic reasoning capabilities organically.

\subsection{Initial training, Annealing Adapter Tuning and Inference}

\subsubsection{\textbf{\textit{Initial training}}}
In the initialization phase of enhancing the model's recommendation capabilities, we devised various conditional language modeling objectives. This strategy encourages highly divergent models, compared to pre-recommendation pretrain tasks, to cultivate in-depth generalization, understanding, and reasoning abilities pertinent for recommendation tasks.
%These pertain to understanding user preferences, item semantics, and sequential behaviours etc. %

The initial training can be formulated as follows:

\begin{equation}
  \max _{\Phi} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left(P_{\Phi+\varphi_{r}  }\left(y_{t} \mid x, y_{<t}\right)\right)
\end{equation}

$x$ represents the "Instruction Input". $y$ denotes the "Instruction Output" within the initial training data. $y_t$ stands for the $t$-th token of $y$. $\Phi$ corresponds to the original parameters of the LLM-Backbone. $\varphi_{r}$ represents the additional parameters in Sequence Recommendation Task (SRT), and $\mathcal{Z}$ refers to the training set. We combine the generation and exogenous Semantic, Behavioral Reconstruction Loss to train our model, given by:

\begin{equation}
    \mathcal{L}=\mathcal{L}_{gen} +I_{\text{SRT}} (\lambda _{1}\mathcal{L}_{con}^{s}+\lambda _{2}\mathcal{L}_{con}^{b})
\end{equation}

Where $I_{\text{SRT}}$  is an indicator function that is 1 if the task is SRT and 0 otherwise.  $\lambda_{1}$ and $\lambda_{2}$ are loss coefficients.

\begin{figure}[t]
  \centering 
  \includegraphics[width=1\linewidth]{figures/figure4.pdf}
  \caption{{\bf The illustration of multi-stage training \& inference process.} }
  \label{fig:train}
\end{figure}

\subsubsection{\textbf{\textit{Annealing Adapter Tuning}}}
%Llama Team [llama3] revealed that the performance of pre-trained models on critical benchmarks is significantly augmented when annealed with minuscule amounts of high-quality code and mathematical data. In a similar context, 
we observed that annealing with restricted quantities of high-grade sequence recommendation data considerable improves the performance of the LLM-Backbone on pivotal benchmarks subsequent to the initial training of recommendation capabilities.

Achieving the optimal solution for enhancing sequence recommendation performance remains a formidable challenge without adjusting the data volume ratio across various tasks. Integrating tasks such as sequence recommendation, preference understanding, and semantic reconstruction, while neglecting to bridge the gap between natural language processing and sequential behavior, complicates the optimization of sequence recommendation performance without modifying the proportion of data volume allocated to different tasks.

Conversely, training on a limited set of high-grade sequence recommendation tasks during the Annealing Training phase can also impair the model’s original capabilities due to the significant disparity between the language semantics modeled by LLMs and the collaborative semantics implicit in recommender systems. Therefore, the use of an Adapter to introduce additional parameters in this phase, as shown in \ref{fig:train}, constitutes an efficient and pragmatic approach to mitigate the adverse effects associated with Annealing Training.

Formally,

\begin{equation}
\max _{\varphi_{a}} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left(P_{\Phi+\varphi_{r}+\varphi_{a}}\left(y_{t} \mid x, y_{<t}\right)\right)
\end{equation}

where $\varphi_{a}$ denotes the parameters of Annealing Adapter.

\subsubsection{\textbf{\textit{Inference}}}
It is noteworthy that the additional parameter $\phi$ , brought forth by the SRT, is disregarded during the inference stage. Moreover, employing the Annealing Adapter dynamically to meet varying task demands acts as a potent strategy for achieving a flexible balance between the model's textual reasoning abilities and recommendation accuracy.

\section{EMPIRICAL STUDY}
We analyze the proposed EAGER-LLM method on three datasets and demonstrate its effectiveness by answering the following research questions: 

\begin{itemize}[leftmargin=*]
    \item {RQ1}: How does EAGER-LLM compare to state-of-the-art sequential recommendation methods in different datasets?
    \item {RQ2}: How do the components of EAGER-LLM (e.g., DKI, GCT, AAT) and hyper-parameter adjustments affect its performance?
    % \item {RQ3}: How do various ablation variants and hyper-parameter adjustments impact the performance of EAGER-LLM?
\end{itemize}

\begin{table}[tp]
\centering
\caption{Statistics of the Datasets.}
\label{tab:dataset}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
\toprule
Dataset & \#Users & \#Items & \#Interactions & \#Sparsity \\ \midrule
Beauty &22,363  & 12,101  & 198,360 & 0.00073\\
Sports and Outdoors &35,598  & 18,357  &296,175 & 0.00045\\
Instruments &24,733  & 9,923  & 206,153 & 0.00083\\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table*}[htp]
\centering
\caption{Performance comparison of different methods. The best performance is highlighted in bold while the second best performance is underlined. The last column indicates the improvements over the best baseline models and all the results of EAGER-LLM are statistically significant with p < 0.05 compared to the best baseline models. We used the official LC-Rec checkpoints to rerun the inference with the conflicts removed in Instruments. }
\begin{tabular}{clccccccccccc}
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Metric} & \multicolumn{3}{c}{\emph{Traditional}} & \multicolumn{3}{c}{\emph{Transformer-based}} & \multicolumn{2}{c}{\emph{Generative}} & \multicolumn{2}{c}{\emph{LLM-based}} & \multirow{2}{*}{Improv.} \\ \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
 &  & GRU4REC & Caser & HGN & Bert4Rec & S\textasciicircum{}3-Rec & FDSA & P5-CID & TIGER & LC-Rec & Ours &  \\ \midrule
\multirow{4}{*}{Beauty} & Recall@5 & 0.0164 & 0.0205 & 0.0325 & 0.0203 & 0.0387 & 0.0267 & 0.0400 & 0.0454 & \underline{0.0482} & \textbf{0.0548} & \cellcolor{linecolor1}{13.69\%} \\
 & Recall@10 & 0.0283 & 0.0347 & 0.0512 & 0.0347 & 0.0647 & 0.0407 & 0.0590 & 0.0648 & \underline{0.0681} & \textbf{0.0830} & \cellcolor{linecolor}{21.88\%} \\
 & NDCG@5 & 0.0099 & 0.0131 & 0.0206 & 0.0124 & 0.0244 & 0.0163 & 0.0274 & 0.0321 & \underline{0.0327} & \textbf{0.0369} & \cellcolor{linecolor1}{12.84\%} \\
 & NDCG@10 & 0.0137 & 0.0176 & 0.0266 & 0.0170 & 0.0327 & 0.0208 & 0.0335 & 0.0384 & \underline{0.0409} & \textbf{0.0459} & \cellcolor{linecolor1}{12.22\%} \\ \midrule
\multirow{4}{*}{Sports} & Recall@5 & 0.0129 & 0.0116 & 0.0189 &  0.0115 & 0.0251 & 0.0182 & \underline{0.0313} & 0.0264 & 0.0304 & \textbf{0.0373} & \cellcolor{linecolor}{19.17\%} \\
 & Recall@10 & 0.0204 & 0.0194 & 0.0313 & 0.0191 & 0.0385 & 0.0288 & 0.0431 & 0.0400 & \underline{0.0451} & \textbf{0.0569} & \cellcolor{linecolor}{26.16\%} \\
 & NDCG@5 & 0.0086 & 0.0072 & 0.0120 & 0.0075 & 0.0161 & 0.0122 & \underline{0.0224} & 0.0181 & 0.0196 & \textbf{0.0251} & \cellcolor{linecolor1}{12.05\%} \\
 & NDCG@10 & 0.0110 & 0.0097 & 0.0159 & 0.0099 & 0.0204 & 0.0156 & \underline{0.0262} & 0.0225 & 0.0246 & \textbf{0.0315} & \cellcolor{linecolor}{20.23\%} \\ \midrule
 \multirow{4}{*}{Instruments} & Recall@5 & 0.0821 & 0.0543 & 0.0813 & 0.0671 & 0.0863 & 0.0834 & 0.0827 & 0.0863 & \underline{0.0964} & \textbf{0.0991} & \cellcolor{linecolor2}{2.80\%} \\
 & Recall@10 & 0.1031 & 0.0710 & 0.1048 & 0.0822 & 0.1136 & 0.1046 & 0.1016 & 0.1064 & \underline{0.1177} & \textbf{0.1224} & \cellcolor{linecolor2}{3.99\%} \\
 & NDCG@5 & 0.0698 & 0.0355 & 0.0668 & 0.0560 & 0.0626 & 0.0681 & 0.0708 & 0.0738 & \underline{0.0819} & \textbf{0.0851} & \cellcolor{linecolor2}{3.91\%} \\
 & NDCG@10 & 0.0765 & 0.0409 & 0.0744 & 0.0608 & 0.0714 & 0.0750 & 0.0768 & 0.0803 & \underline{0.0890} & \textbf{0.0926} & \cellcolor{linecolor2}{4.04\%} \\ \midrule
\end{tabular}\label{tab:results}
\end{table*}

\begin{table}[htp]
\centering
\caption{Performance comparison of LETTER and our method. For fair comparison, llm-bacbone is uniformly Llama2-7b. All the results of EAGER-LLM are statistically significant with p < 0.05.}
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{4}{c}{\emph{Instruments}}\\
\cmidrule(lr){2-5}
& R@5 & R@10 & N@5 & N@10\\ \midrule
 TIGER & 0.0870 & 0.1058 & 0.0737 & 0.0797 \\
 LETTER-TIGER & 0.0909 & \underline{0.1122} & 0.0763 & 0.0831 \\
 LC-Rec & 0.0824 & 0.1006 & 0.0712 & 0.0712 \\
 LETTER-LC-Rec & \underline{0.0913} & 0.1115 & \underline{0.0789} & \underline{0.0854} \\ \midrule
 EAGER-LLM (Llama2-7B) & \textbf{0.0994} & \textbf{0.1206} & \textbf{0.0854} & \textbf{0.0922} \\ \midrule
 Improv. & \cellcolor{linecolor}{8.95\%} & \cellcolor{linecolor2}{7.48\%} & \cellcolor{linecolor1}{8.26\%} & \cellcolor{linecolor1}{8.02\%} \\\midrule
\end{tabular}\label{tab:llama2_results}
\end{table}

% \begin{table*}[htp]
% \centering
% \caption{Performance comparison of different methods. The best performance is highlighted in bold while the second best performance is underlined. The last column indicates the improvements over the best baseline models and all the results of EAGER-LLM are statistically significant with p < 0.05 compared to the best baseline models.}
% \begin{tabular}{c|cccc|cccc|cccc}
% \toprule
% \multirow{2}{*}{Model} & \multicolumn{4}{c}{\emph{Beauty}} & \multicolumn{4}{c}{\emph{Sports}} &\multicolumn{4}{c}{\emph{Instruments}}\\
% \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13}
% & R@5 & R@10 & N@5 & N@10 & R@5 & R@10 & N@5 & N@10 & R@5 & R@10 & N@5 & N@10 \\ \midrule
%  GRU4REC & 0.0164 & 0.0205 & 0.0325 & 0.0387 & 0.0203 & 0.0387 & 0.0267 & 0.0400 & 0.0454 & \underline{0.0482} & \textbf{0.0548} & 0.0164\\ 
%  Caser & 0.0164 & 0.0205 & 0.0325 & 0.0387 & 0.0203 & 0.0387 & 0.0267 & 0.0400 & 0.0454 & \underline{0.0482} & \textbf{0.0548} & 0.0164\\  
%  HGN & 0.0164 & 0.0205 & 0.0325 & 0.0387 & 0.0203 & 0.0387 & 0.0267 & 0.0400 & 0.0454 & \underline{0.0482} & \textbf{0.0548} & 0.0164\\  
%  SASRec & 0.0164 & 0.0205 & 0.0325 & 0.0387 & 0.0203 & 0.0387 & 0.0267 & 0.0400 & 0.0454 & \underline{0.0482} & \textbf{0.0548} & 0.0164\\  
%  Bert4Rec & 0.0164 & 0.0205 & 0.0325 & 0.0387 & 0.0203 & 0.0387 & 0.0267 & 0.0400 & 0.0454 & \underline{0.0482} & \textbf{0.0548} & 0.0164\\  
%  S\textasciicircum{}3-Rec & 0.0164 & 0.0205 & 0.0325 & 0.0387 & 0.0203 & 0.0387 & 0.0267 & 0.0400 & 0.0454 & \underline{0.0482} & \textbf{0.0548} & 0.0164\\  
%  FDSA & 0.0164 & 0.0205 & 0.0325 & 0.0387 & 0.0203 & 0.0387 & 0.0267 & 0.0400 & 0.0454 & \underline{0.0482} & \textbf{0.0548} & 0.0164\\  
%  P5-CID & 0.0164 & 0.0205 & 0.0325 & 0.0387 & 0.0203 & 0.0387 & 0.0267 & 0.0400 & 0.0454 & \underline{0.0482} & \textbf{0.0548} & 0.0164\\  
%  TIGER & 0.0164 & 0.0205 & 0.0325 & 0.0387 & 0.0203 & 0.0387 & 0.0267 & 0.0400 & 0.0454 & \underline{0.0482} & \textbf{0.0548} & 0.0164\\  
%  LC-Rec & 0.0164 & 0.0205 & 0.0325 & 0.0387 & 0.0203 & 0.0387 & 0.0267 & 0.0400 & 0.0454 & \underline{0.0482} & \textbf{0.0548} & 0.0164\\  \midrule
%  Ours   & 0.0164 & 0.0205 & 0.0325 & 0.0387 & 0.0203 & 0.0387 & 0.0267 & 0.0400 & 0.0454 & \underline{0.0482} & \textbf{0.0548} & 0.0164\\  
%  Improv.   \\
% \end{tabular}\label{tab:results_2}
% \end{table*}

\subsection{Experimental Setting}
\subsubsection{\textbf{\textit{Dataset}}}
We conducted experiments using three real-world public datasets of Amazon product reviews \cite{mcauley2015amazon, he2016amazon_2}, which are among the most widely utilized benchmarks for sequence recommendation. Specifically, the experiments focused on three subcategories: “Beauty”, “Sports and Outdoors” and “Musical Instruments”. In line with previous studies \cite{hou2022towards, rendle2010factorizing, zhang2019feature}, we utilized the 5-core dataset approach, which excludes unpopular items and inactive users with fewer than five interaction records. The statistics for these datasets are presented in \ref{tab:dataset}.

\subsubsection{\textbf{\textit{Evaluation Metrics}}}
We utilize two widely recognized criteria for the matching phase: Recall and Normalized Discounted Cumulative Gain (NDCG). We present metrics calculated for the top 5/10 recommended candidates. In line with the standard evaluation protocol \cite{kang2018sasrec}, we adopt the leave-one-out method for assessments. 
%Specifically, for each sequence of user behavior, the most recent item is designated as the test data, the next most recent as the validation data, and all previous interactions are used for training.
During training phases, we restrict the user’s historical item count to 20. Additionally, for generative methods employing beam search, we consistently set the beam size to 20.

\subsubsection{\textbf{\textit{Implementation Details}}}
We utilize Llama-7b \cite{touvron2023llama} as LLM-Backbone. In constructing the item indexes, LLM-Backbone itself and DIN \cite{zhou2018din} as encoders. For training, our approach mirrors that of LC-Rec for ease of comparison, employing the AdamW optimizer with a learning rate set to 5e-5 and weight decay at 0.01.
%We use a cosine scheduler with warmup to adjust the learning rate effectively.
We implement data parallelism and gradient accumulation to achieve an overall batch size of 128. For GCT, we adopt InfoNCE to serve as the loss metric.
% Unlike LC-Rec, which conducts training for three epochs across each dataset, our method involves training for less epoch. This suggests that our approach makes it easier for the model to understand the semantics embedded in the added Token.

\begin{table*}[t]
\centering
\caption{Ablation studies by selectively discarding the Dual-source Knowledge-rich Item Indices (DKI), Global Contrastive Task (GCT), and Annealing Adapter Tuning (AAT). }
% \setlength{\tabcolsep}{0.7em}
\begin{tabular}{ccccccccccccc}
% \begin{tabular}{ccc*{10}{p{1.1cm}}}
\toprule
\multicolumn{3}{c}{Variants} & \multicolumn{5}{c}{Beauty} & \multicolumn{5}{c}{Musical Instruments} \\ \cmidrule(lr){1-3} \cmidrule(lr){4-8} \cmidrule(lr){9-13}
DKI & GCT & AAT & R@1 & R@5 & R@10 & NDCG@5 & NDCG@10 & R@1 & R@5 & R@10 & NDCG@5 & NDCG@10 \\ \midrule
 &  &  & 0.0135  & 0.0453  & 0.0650  & 0.0295  & 0.0358  & 0.0631  & 0.0883  & 0.1071  & 0.0757  & 0.0817 \\
\rowcolor{linecolor2} \checkmark &  &  & 0.0152  & 0.0499  & 0.0760  & 0.0329  & 0.0413  & 0.0696  & 0.0978  & 0.1199  & 0.0802  & 0.0886 \\
\rowcolor{linecolor1} \checkmark & \checkmark &  & 0.0175  & 0.0513  & 0.0781  & 0.0346  & 0.0432  & 0.0694  & 0.0981  & 0.1215  & 0.0839  & 0.0914 \\
\rowcolor{linecolor} \checkmark & \checkmark & \checkmark & \textbf{0.0176}  & \textbf{0.0544}  & \textbf{0.0817}  & \textbf{0.0363}  & \textbf{0.0451}  & \textbf{0.0707}  & \textbf{0.0991}  & \textbf{0.1225}  & \textbf{0.0852}  & \textbf{0.0927}  \\ \bottomrule
\end{tabular}  \label{tab:ablation}
\end{table*}

\subsection{Performance Comparison (RQ1)}
\subsubsection{\textbf{\textit{Baselines}}}
We compare the following four categories of methods:

\noindent (1) \emph{Traditional seqiential methods}
\begin{itemize}[leftmargin=*]
    \item \textbf{GRU4REC} \cite{hidasi2015gru4rec}: An RNN-based sequential recommendation model that utilizes GRU model to encode the item sequence.
    \item \textbf{Caser} \cite{tang2018caser}: a CNN-based approach that utilizes horizontal and vertical CNN layers to model the patterns in user behavior.
    \item \textbf{HGN} \cite{ma2019hgn}:  employs hierarchical gating networks to effectively discern long-term and short-term user preferences.
\end{itemize}


\noindent (2) For \emph{transformer-based methods}, we have:
\begin{itemize}[leftmargin=*]
    % \item \textbf{SASRec} \cite{kang2018sasrec}: Utilizes a Transformer encoder with a multi-head attention mechanism to model user behaviors and predict subsequent item sequences.
    \item \textbf{S\textasciicircum{}3-Rec} \cite{zhou2020s3}: enhances recommendation by pre-training bidirectional Transformer to maximize mutual information.
    \item \textbf{BERT4Rec} \cite{sun2019bert4rec}:  Utilizes a bidirectional Transformer to overcome the constraints of unidirectional models.
    \item \textbf{FDSA} \cite{zhang2019feature}: models feature sequence transition patterns using a self-attention module.
\end{itemize}


% \noindent (3) For \emph{tree-based methods}, we have:
% \begin{itemize}
%     \item \textbf{TDM} \cite{zhu2018tdm}: TDM organizes items using a tree index where each leaf node represents an item and employs a maximum heap-based tree model for efficient retrieval.
%     \item \textbf{RecForest} \cite{feng2022recforest}: RecForest builds a forest consisting of multiple k-branch trees and incorporates a transformer-based architecture to manage routing operations.
% \end{itemize}


\noindent (3) For \emph{generative methods}, we have:
\begin{itemize}[leftmargin=*]
    \item \textbf{TIGER} \cite{rajput2024tiger}: employs T5 to generate IDs for items and uses an autoregressive decoding process to identify target candidates.
    \item \textbf{P5-CID} \cite{hua2023p5_cid}: leverages collaborative signals to construct ID identifiers for T5-based generative recommender model.
\end{itemize}

\noindent (4) For \emph{LLM-Based methods}, we have:
\begin{itemize}[leftmargin=*]
    \item \textbf{LC-Rec} \cite{zheng2024lcrec}: LC-Rec designs a vector quantization method to generate semantic IDs and use Llama as backbone to autoregressively decodes the identifiers of the target candidates items.
    \item \textbf{LETTER} \cite{wang2024letter}: Integrates collaborative signals into LLM-Backbone through a series of regularizations.
\end{itemize}

\subsubsection{\textbf{Overall Performance}} We provide a detailed report in \ref{tab:results} on the sequence recommendation performance of our method across three datasets, comparing it against various baseline models. The results lead to several key observations :

%EAGER-LLM obtains better results than base modes on all three datasets. We believe that this is mainly attributed to the fact that EAGER-LLM effectively introduces exogenous semantic and behavioral signals and makes it possible for the LLM-Backbone to understand this information in depth through a series of non-invasive tasks.
%In particular, compared with the second strongest baseline, EAGER-LLM achieves 26.16\% and 21.88\% improvement over LC-Rec for Recall@10 in the Sports dataset and Recall@10 in the Beauty dataset. 

\textbf{Traditional baselines} employ a simple inner-product matching approach, which segments the process and limits its ability to effectively model complex user interaction histories and intentions. Moreover, this approach’s computational complexity grows exponentially with the candidate set, also restricting the representational space size. In contrast, EAGER-LLM aligns with the generative recommendation paradigm. It not only leverages pre-training knowledge to enhance recommendation-relevant capabilities, but it also reduces computational costs by directly generating the target item ID through beam search. This approach expands the limitations of latent space size in item representation, allowing it to incorporate significantly more exogenous information. 
%This additional data supports the recommendation task, substantially improving both the accuracy and robustness of the recommendations.

\textbf{Generative recommendation, llm-based approaches} (TIGER, LC-REC etc.) neglected the importance of exogenous behavioral signals for sequence recommendation. While the transformer architecture with generation loss works well in various domains, it is not designed for the task of sequence recommendation. These non-native approaches ignore the rank-order relationship of the candidates in the recommendation task, which leads to poor model performance on ranking-related metrics such as ndcg. Therefore, we believe that introducing additional behavioral signals is the key to improving the overall performance of model recommendation without changing the model architecture and training process.
%Our experiments strongly prove this point, and we are surprised to find that introducing exogenous semantic and behavioral signals at the same time can make the separated domain knowledge wonderfully intertwined and realize the magic phenomenon of 1+1>2.

There are also some approaches that attempt to \textbf{incorporate exogenous behavioral signals into the recommendations} (P5-CID, LETTER). LETTER, for instance, integrates collaborative signals into discrete coding through a series of regularizations.  However LETTER does not have open source code and is only implemented as Llama2-7b \cite{touvron2023llama2},  we evaluated our EAGER-LLM using the Llama2-7b on the Instruments dataset, as detailed in \ref{tab:llama2_results}. Our method outperforms LETTER by over 8\% across all metrics. We contend that even sophisticated encoder-side feature fusion methods can introduce additional compression loss, hindering the efficient integration of multi-source features. Therefore, allowing the LLM-Backbone itself to handle the fusion of information without introducing extra generalization bias at the input emerges as a simpler and more effective strategy.
\begin{figure}[t]
  \centering 
  \includegraphics[width=0.9\linewidth]{figures/figure5.pdf}
  \caption{{\bf The performance of our architecture (w/o GCT, AAT), under indexing with different exogenous signal compositions.} }
  \label{fig:ablation_2}
\end{figure}
\subsection{Ablation Study (RQ2)}
In the ablation experiments, the Sequence Recommendation Prediction Task was used as the core metric to evaluate the performance impact of each component. The main components of EAGER-LLM include Dual-source Knowledge-rich Item Indices (DKI), Global Contrast Decompression Task (GCT), and Annealing Adapter Tuning (AAT). The results are reported in \ref{tab:ablation}, we can observe that:
\begin{itemize}[leftmargin=*]
    \item Removing EAGER-LLM of the DKI, GCT, ATT (random index) achieves the worst results in different datasets, but still outperforms the vast majority of traditional baselines. This underscores the inherent superiority and robustness of our foundational framework in addressing the sequence recommendation task, and highlights significant potential for further development and enhancement in future work.
    \item Removing DKI significantly impacts sequence recommendation performance, illustrating the base model’s effectiveness in enhancing recommendations by integrating exogenous behavioral and semantic information. This also showcases DKI’s capability to encapsulate vast information within a few tokens at a high compression ratio.
    \item GCT significantly enhances the NDCG metrics compared to Recall, indicating its efficacy in decoding exogenous behavioral signals compressed by DKI. GCT effectively incorporates external knowledge that optimizes the model’s ability to sequence recommendations. This underscores our architecture’s adaptability in harnessing distinct properties from various exogenous information sources.
\end{itemize}
\begin{figure}[t]
  \centering 
  \includegraphics[width=1\linewidth]{figures/figure6.pdf}
  \caption{{\bf Analysis of the performance impact of Items Indices schemes of different lengths.} }
  \label{fig:ablation_3}
\end{figure}
\subsection{Further Analysis (RQ2)}
As depicted in \ref{fig:ablation_2}, we conducted performance experiments on the Beauty and Instruments datasets using various exogenous signal indexing methods: \textbf{(1)} Random, where each level of indices is randomly selected from candidates; \textbf{(2)} Semantic, utilizing indices derived solely from textual semantic signals; \textbf{(3)} Behavior, using indices generated solely from behavioral signals; and \textbf{(4)} Unit, combining indices from both Semantic and Behavior. The Unit index significantly exceeds the sum of the individual contributions from the two sources, yielding much higher results.

\textbf{To our shock}, in the Beauty dataset, the Semantic index performs worse than the Random index, likely due to a high compression ratio that complicates the model’s ability to decode separate exogenous signals, especially after removing GCT and AAT. This often results in a diminished or even negative impact on recommendation performance. More intriguingly, the integration of exogenous Behavioral signals enables effective interaction between the dual information streams, enhancing their mutual comprehension and decoding. This synergy not only mitigates the negative impacts but also transforms them into substantial positive outcomes.

In \ref{fig:ablation_3}, we demonstrate the impact of varying lengths of the Item Indices scheme on performance within the Beauty dataset. Four layers provide sufficient information for learning, while additional layers do not hinder ID generation but increase inference time.

\section{CONCLUSION}
In this paper, we introduce EAGER-LLM, a novel decoder-only, LLM-based generative recommendation framework that seamlessly integrates both endogenous and exogenous behavioral and semantic information non-intrusively. Extensive experiments validate the effectiveness and robustness of EAGER-LLM, showcasing superior performance compared to existing state-of-the-art methods.
%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
% This work was supported in part by the National Natural Science Foundation of China under Grant No.U24A20326 and MindSpore\footnote{https://www.mindspore.cn}.
This work was partially supported by the National Natural Science Foundation of China under Grant No. U24A20326. We also thank MindSpore\footnote{https://www.mindspore.cn} for the partial support of this work, which is a new deep learning computing framework.
\end{acks}
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\balance
\bibliography{eagerllm}


%%
%% If your work has an appendix, this is the place to put it.
% \appendix
\end{document}

\endinput
%%
%% End of file `sample-sigconf.tex'.
