\section{Related work}
\subsection{Sequential Recommendation}
The use of deep sequential models for understanding user-item interactions in recommender systems has significantly evolved, with various approaches making notable contributions. GRU4REC **Hidasi et al., "Session-based Recommendations with Recurrent Neural Networks"** introduced the use of GRU-based RNNs to model sequential user behaviors effectively. SASRec **Kang and McAuley, "Self-Attentive Sequential Recommendation"** implemented self-attention mechanisms akin to those found in decoder-only transformer models to enhance recommendation accuracy. Drawing inspiration from the success of masked language modeling in NLP, BERT4Rec **Sun et al., "BERT4Rec: BERT for Recommendation"** applied transformers with masking techniques specifically tailored for sequential recommendation tasks. Additionally, TIGER **Hou et al., "TIGER: Toward Tailorable Information-Grounded Explanations and Recommendations"** has started emphasizing the use of semantic IDs. In this approach, each item is represented by a series of tokens that reflect its related details, and the system predicts the sequence of upcoming item tokens using a seq2seq method. Additionally EAGER **Wang et al., "EAGER: Efficient Attention on Graphs for Recommendation"** advances the investigation by implementing a dual-stream generation architecture that incorporates both semantic and behavioral information. In this work, we extend EAGER **Wang et al., "EAGER: Efficient Attention on Graphs for Recommendation"** to EAGER-LLM by bridging LLMs and recommenders with dual-source knowledge-rich item indices and non-invasive multiscale alignment reconstruction, which not only enhances recommendation accuracy but also retains conversation and explanation generation abilities of LLMs. Recently, P5 **Zhang et al., "P5: Pre-trained Large Language Model for Multi-Task Recommender Systems"** fine-tunes a pre-trained LLMs for multi-task recommender systems. In this study, we endeavor to further investigate a paradigm designed to mitigate the substantial discrepancies between LLMs in recommendation tasks and their original training tasks by integrating exogenous semantic and behavioral information.

\subsection{LLMs as Recommenders}
% Large Language Models (LLMs) have become pivotal in Natural Language Processing (NLP). 
Recently, LLMs have been utilized in recommendation tasks due to their ability to understand, generate, and infer natural language properties. LLM-based RSs **Wang et al., "Graph-Based Knowledge Distillation for Recommender Systems"** constructs user/item correlations through its powerful high-quality textual representations and extensive external knowledge, and is expected to solve the problems of poor generalization **Chen et al., "Meta-Learning for Recommendation with Graph-Based Framework"** and poor performance of traditional RSs on sparse historical interaction data, etc.
Chat-Rec **Zhang et al., "Chat-Rec: Conversational Recommender System"** aims to enhance conversational recommendation systems by integrating ChatGPT’s interactive capabilities with established recommendation models, such as MF **Koren et al., "Matrix Factorization Techniques for Recommendation Systems"** and LightGCN **Li et al., "LightGCN: Simplifying and Optimizing Graph Convolution Networks for Recommendation"**. P5 **Zhang et al., "P5: Pre-trained Large Language Model for Multi-Task Recommender Systems"** fine-tunes a pre-trained large language model for multi-task recommender systems, utilizing the LLM tokenizer (SentencePiece tokenizer) to generate tokens from randomly assigned item pseudo-IDs. M6 **Hou et al., "M6: Multi-Modal Graph Neural Networks for Recommendation"** explores the use of item text information (such as names) as identifiers for items. LC-Rec **Wang et al., "LC-Rec: Learning-Based Vector Quantization Method for Recommender Systems"** designs a learning-based vector quantization method to generate ID from Item’s semantic representation and proposes alignment tuning tasks to enhance the intergration of collaborative semantics in LLMs.
%However, merely using directive-based fine-tuning falls short in effectively leveraging the inherent capabilities of LLMs to understand collaborative information and inadequately learn from the implicit interaction data crucial for recommendations.
Recently, new research has emerged to bridge the significant gap between pre-trained language models and recommendation tasks. CoLLM **Wang et al., "CoLLM: Collaborative Language Models for Recommendation"** infuses behavior information into LLMs by incorporating representations from an external collaborative model into the input.
%CLLM4Rec__ develops dual embeddings for users and items tailored to recommendation tasks and content generation tasks, predict items based on soft+hard prompts.
In this work, we aim to further explore recommender frameworks that can integrate endogenous and exogenous behavioral and semantic signals based on LLM.