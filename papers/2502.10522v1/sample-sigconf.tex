%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
\newtheorem{definition}{Definition}


\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    %basicstyle=\ttfamily\footnotesize,
    %basicstyle=\fontsize{8}{9}\selectfont\ttfamily,
}
\lstset{style=mystyle}

\usepackage{booktabs}						% professional-quality tables
\usepackage{multirow}						% tabular cells spanning
\usepackage{amsfonts}						% blackboard math symbols
\usepackage{graphicx}						% figures
\usepackage{duckuments}	



%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{none}
%\copyrightyear{2025}
%\acmYear{2025}
%\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Conference acronym 'XX]{Make sure to enter the correct
%  conference title from your rights confirmation email}{June 03--05,
%  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
%\acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{GraphiT: Efficient Node Classification on Text-Attributed Graphs with Prompt Optimized LLMs}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.




\author{Shima Khoshraftar, Niaz Abedini and Amir Hajian}
\affiliation{%
  \institution{Arteria AI}
  \city{Toronto}
  \country{Canada}}
%\email{shima.khoshraftar@arteria.ai}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Khoshraftar et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  The application of large language models (LLMs) to graph data has attracted a lot of attention recently.
LLMs allow us to use deep contextual embeddings from pretrained models in text-attributed graphs, where shallow embeddings are often used for the text attributes of nodes. However, it is still challenging to efficiently encode the graph structure and features into a sequential form for use by LLMs. In addition, the performance of an LLM alone, is highly dependent on the structure of the input prompt, which limits their effectiveness as a reliable approach and often requires iterative manual adjustments that could be slow, tedious and difficult to replicate programmatically.
In this paper, we propose GraphiT (\textbf{Graph}s \textbf{i}n \textbf{T}ext), a framework for encoding graphs into a textual format and optimizing LLM prompts for graph prediction tasks. Here we focus on node classification for text-attributed graphs. We encode the graph data for every node and its neighborhood into a concise text to enable LLMs to better utilize the information in the graph. We then further programmatically optimize the LLM prompts using the DSPy framework to automate this step and make it more efficient and reproducible. GraphiT outperforms our LLM-based baselines on three datasets and we show how the optimization step in GraphiT leads to measurably better results without manual prompt tweaking. We also demonstrated that our graph encoding approach is competitive to other graph encoding methods while being less expensive because it uses significantly less tokens for the same task.


\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%




%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Graphs, Large language models, Node classification, DSPy}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\footnotetext{Corresponding author: shima.khoshraftar@arteria.ai}

\section{Introduction}
Graphs are powerful tools for representing entities and the relationships between them in different applications such as social networks and citation networks. For instance, in a citation network, nodes are the articles and there is an edge between two articles if one article cites another one. In text-attributed graphs, nodes have text attributes which provide further information about the nodes. In the citation network described above, the text attributes of a node could be the content of the associated article. One of the main applications of graphs is the node classification task in which a model predicts a label for the nodes in the test set.%Graphs have been used in different machine learning applications including node classification and link prediction tasks. In the node classification task, the goal is to predict a label for each node in the graph. The link prediction task refers to predicting the existence of a link between two given nodes. 

%Given the limited number of labels for the training data in the real world, utilizing few-shot learning for these tasks has become very important. For example, in a few-shot node classification, a prediction model predicts a node label, given a few labeled node samples.    
Graph Neural Nets (GNNs) \cite{zhang2020deep, khoshraftar2024survey} are the state of the art in graph representation learning. They typically generate a node embedding by aggregating the embeddings of neighbors of the node in a message passing mechanism \cite{kipf2016semi, velivckovic2018graph}. GNNs consider the structure and the attributes of graphs in generating embeddings. The text attributes of nodes are often represented by shallow embeddings such as bag-of-words \cite{katz1985philosophy} and word2vec \cite{mikolov2013efficient} which can not capture the contextual relationships between words in text attributes. However, large language models have demonstrated great success in generating contextual text embeddings with superior performance than shallow embeddings in natural language processing (NLP) tasks. The success of the LLM models is mainly due to their pre-training on a vast amount of text corpora which gives them massive knowledge and semantic comprehension capabilities. Hence, many recent efforts have explored combining LLMs and GNNs \cite{he2023harnessing, duan2023frustratingly, yu2023empower, zhu2024efficient,xue2023efficient}.
While effective, this combination results in a complex system involving two large models which increase the computational demands and require labeled training data. 

Consequently, other studies focused on evaluating the potential of LLMs to act as standalone models for both embedding generation and prediction \cite{fatemitalk, ye2024language, perozzi2024let,chen2024exploring}.
%However, this combination while successful creates a complex system consisting of two large models which needs computational resources. Therefore, other works focus on evaluating the performance of LLMs as the only model to handle embedding generation and prediction \cite{}.  
%Considering the strong performance of Large language models in NLP tasks \cite{zhao2023survey, d2024context}, there have been many efforts to apply LLMs to graph applications recently \cite{ye2024language,chen2024exploring,fatemitalk,perozzi2024let}. LLMs typically take text as an input prompt, generate text embeddings and perform tasks such as text summarizaion and translation. These models have been trained on a massive text corpora and depict great success in the text prediction tasks. 
 These methods employ various techniques for optimizing LLMs, which can be broadly categorized into prompt engineering \cite{fatemitalk} which relies heavily on manual adjustments or fine-tuning which require labeled training data \cite{ye2024language}. 
Additionally, different approaches are explored for converting graph structures into sequential formats suitable for LLMs, including using text attributes, lists of a node's neighbors \cite{ye2024language}, and neighbor summaries \cite{chen2024exploring} which can lead to increasing the context length of prompts making the LLM calls more expensive.

%As the input data to LLMs are in a sequential format, one important question in the integration of LLMs and graphs is converting the graph structure and features to a suitable form for consumption by LLMs \cite{fatemitalk}. In \cite{ye2024language}, each node is represented using the text attributes and a list of node's neighbors. In \cite{chen2024exploring} nodes are encoded by incorporating text attributes and 2-hop neighbors summaries.

In this paper, we investigate the promise and limitations of using LLMs for the node classification tasks by proposing new approaches for graph encoding and prompt optimization in terms of instruction and examples using DSPy framework \cite{khattab2023dspy}. Specifically, we use a prompt programming approach which automates the optimization of LLMs for node classification without extra training, manual tweaks and with a small set of labeled data. 
%In addition, as nodes are most similar to their immediate neighbors in real world datasets \cite{},
%In addition, as nodes most likely share similar characteristic to their immediate neighbors \cite{ciotti2016homophily, khoshraftar2022temporal}, 
%we focus on including 1-hop neighbors in our optimization approach. 
%We also propose using keyphrases from text attributes as an effective method for encoding nodes, capturing the essential content of text attributes with minimal data.
Furthermore, %previous studies have utilized text attributes and neighbor information, such as summary of the text attributes of a node's neighbors, to represent a node \cite{ye2024language, fatemitalk, chen2024exploring}. However,
we propose using keyphrases of neighbor nodes to represent a node, which offer several advantages. First, keyphrases require significantly less of the LLMs' context window while effectively conveying the key points. Second, when neighbor summaries are lengthy, LLMs may experience the "lost-in-the-middle" effect \cite{liu2024lost}, where critical information representing a node's neighbors is overlooked. Lastly, in certain graph applications, including multi-hop neighbors is essential. However, summarizing such extended neighborhood information becomes challenging and less interpretable. By using keyphrases, we can generate concise yet informative summaries that capture a broader span of information within the graph.
Our main contributions in this paper are as follows:
\begin{itemize}
    \item We present GraphiT, a novel technique for graph encoding and LLM prompt optimization in node classification task.
    \item GraphiT provides an efficient solution for minimizing the use of LLM context window and automating LLM prompt optimization.
    \item We evaluate the performance of our approach with three baselines on three datasets. In addition, we perform ablation studies to show the effectiveness of GraphiT components. GraphiT can be easily adapted to new tasks and datasets with minimal effort. 

\end{itemize}


%In \cite{}, the nodes are represented using text attributes and 2-hop neighbor summaries. In this study we focus on 1-hop neighbors as nodes most likely share similar characteristic to their
%immediate neighbors \cite{} and represent nodes using neighbors summaries, keywords and labels.  
%In addition, different approaches tried to optimize the performance of LLMs through prompt optimization and training \cite{}. These approaches use prompt engineering which relies heavily on manual adjustments or fine-tuning which require training labeled data.  We instead use a prompt programming approach without extra training and manual tweaks. 
%}
%which is the process of finding the most effective way to formulate a question so that an LLM returns the desired answer is crucial for achieving the optimal performance in LLMs. 
%As the input data to LLMs are in a sequential format, one important question in the integration of LLMs and graphs is converting the graph structure and features to a suitable form for consumption by LLMs \cite{fatemitalk}. In addition, the prompt optimization which is the process of finding the most effective way to formulate a question so that an LLM returns the desired answer is crucial for achieving the optimal performance in LLMs.
%There are different ways to represent graphs in downstream applications. GNNs are the state of the art in the graph representation learning. GNNs consider the structure and the attributes of graphs in generating embeddings. The text attributes of nodes are often converted into shallow embeddings such as bag-of-words and word2vec and then are input to the GNNs to be combined with structural information in a message passing mechanism and generate the final graph embeddings. However, in Natural language processing (NLP), large language models have demonstrated great success in generating contextual text embeddings with superior performance than shallow embeddings in NLP tasks. The success of the LLM models is mainly due to their pre-training on a vast amount of text corpora which gives them massive knowledge and semantic comprehension capabilities. Therefore, there have been several recent works on utilizing the strength of llms in graph representation learning. 
%An optimal prompt can be found by manually checking the performance of different prompts which is time consuming. To deal with this issue, DSPy framework \cite{khattab2023dspy} has been proposed recently which algorithmically finds the most optimal prompt for llms. The power of DSPy lies in introducing optimizers which can tune the prompts based on a given metric. Some of these optimizers optimize a few shot examples for the task and add them to the prompts. Therefore, they can facilitate the few-shot node classification. 
%In this paper, we propose the utilization of DSPy framework in optimizing the LLM prompt for the few-shot node classification task in text attributed graphs. 
%In this paper, we focus on the node classification task by LLMs. We consider different combinations of information in the graph to use as input to LLMs. In addition, we optimize the LLM prompt in terms of instruction and examples using DSPy framework \cite{khattab2023dspy}. 
% using a small number of training and validation examples. 
%we show that with a simple program written in DSPy framework, we can optimiz increase the performance of LLMs. %In our experiments, a node in a graph is represented using different node features including the text attributes, neighboring node labels and structural information. Then, given a set of node features and node labels, the llm predicts the most likely label for the node. 
%{\color{red} Our experiments show that an prompt optimized LLM can outperform the vanilla LLM and other unoptimized few-shot method in the node classification task. }
%Our experiments demonstrate that our approach outperforms other LLM-based baselines we evaluated. %While promising for optimizing LLMs in graph prediction tasks, our performance falls short of state-of-the-art GNN results. This gap highlights a key area for future research to bridge the performance disparity between LLMs and GNNs.

\section{Related works}
%{\color{red}Write about Large language models in graphs: Large Language models have been used in graphs mainly as a predictor or enhancer.}
%LLMs have been mainly used as enhancers and predictors on graph data. As an enhancer, LLMs enhance the features or structure of the graph and then their outputs are used in downstream applications \cite{he2023harnessing}. As a predictor, LLMs are directly applied to the graph data to perform the machine learning task \cite{ye2024language, zhao2024pre}. 
GNNs are the frontier techniques in the field of graph representation learning \cite{zhang2020deep, khoshraftar2024survey}. However, they use shallow embeddings to represent text attributes of nodes. Given the capability of LLMs to generate rich contextual embeddings, several works have combined LLMs with GNNs to enhance GNN performance \cite{he2023harnessing, duan2023frustratingly, yu2023empower, zhu2024efficient,xue2023efficient}. Leading \cite{xue2023efficient} employs an end-to-end training of LMs and GNNs for graph prediction tasks. Engine \cite{zhu2024efficient} combines LLMs and GNNs using a tunable side structure. Despite their effectiveness, these integrations create complex systems that are often computationally intensive and require labeled data for training.
%However, this adds to the computation resources needed for performing the prediction specifically if it involves training of the models. 
Therefore, other studies investigate the possibility of using LLMs alone for graph prediction tasks. 
%The combination of graphs and LLMs has been an active research area recently. 
%Specifically, encoding graphs into text for use in LLMs and optimizing the performance using LLM's fine-tuning and prompt engineering are two key issues in applying LLMs on graph applications. 
In \cite{wang2024can}, LLMs are utilized for several graph reasoning tasks such as connectivity, shortest path and topological sort using two instruction-based prompt engineering techniques. InstructGLM \cite{ye2024language} proposes a instruction fine-tuning method for node classification by LLMs. In \cite{chen2024exploring}, LLMs have been used both as enhancer and predictor for node classification task. It encodes the nodes into text by incorporating text attributes and 2-hop neighbors summaries. In \cite{fatemitalk}, different methods for graph encoding and prompt engineering were investigated. In \cite{perozzi2024let}, graphs are input to LLMs using a graph encoder which was trained similar to soft prompting methods \cite{lester2021power}. 
%P2TAG \cite{zhao2024pre} jointly pre-trains the LLMs and graph neural net on the node classification task. 
%Different from fine-tuning, soft prompting and prompt engineering, we use a prompt programming technique to prompt optimization.
Fine-tuning and soft promoting techniques require training with labeled data. Traditional prompt engineering relies heavily on human expertise and manual adjustments. In contrast, we programmatically optimize LLM usage with only a small set of labeled data. In addition, we efficiently capture the information in a node's neighborhood by extracting keyphrases from text attributes of neighboring nodes. 
%In addition, as nodes are most similar to their immediate neighbors in real world datasets \cite{},
%In addition, as nodes most likely share similar characteristic to their immediate neighbors \cite{ciotti2016homophily, khoshraftar2022temporal}, 
%we focus on including 1-hop neighbors in our optimization approach.

%investigate their effects in node classification task using their labels, summaries and keywords.



%There are different graph encoding methods including text features of the graph \cite{fatemitalk, ye2024language} and graph embeddings obtained from a graph encoder \cite{perozzi2024let}.



%\section{Node classification with LLMs}
\section{Method}
\subsection{Problem definition}
Let $G=(V, E, S)$ be a text-attributed graph $G$ where $V$, $E$ and $S$ represent nodes, edges and text attributes of nodes in the graph, respectively. For each node $v_i \in V$, $s_i \in S$ represents the text attributes of $v_i$. $Y$ is the set of labels associated with nodes. Our goal is to perform node classification on the graph using a large language model. In the node classification, a label is predicted for each node in the graph. Formally, a classifier $f$ maps the set of nodes $V$ to the set of labels $Y$ represented as:  $f: V \rightarrow Y$. 
%In this study, we use a large language model as a node classifier and improve the model performance on this task by optimizing the structure and format of the input prompt through instruction optimization and adding demonstrative examples.
%node classfication formulation
%In this study, we use a large language model as a node classifier. 
%Our approach, consists of two main steps: 1) each node $v_i$ in the graph is encoded into a sequential form for use by LLM, 2) the label for the node is predicted using an LLM with an optimized prompt in terms of instruction and demonstrative examples. Figure \ref{framework} demonstrate the overview of our framework.
The core of our approach consists of three main steps: 1) each node $v_i$ in the graph is encoded into a sequential form for use by LLM, 2) an LLM prompt is optimized in terms of instruction and demonstrative examples. 3) the LLM with the optimized prompt is utilized to assign a label to each node. 


\begin{figure}
    \centering
    \includegraphics[scale=0.6]{Picture1.png}
    \caption{The general framework of GraphiT. First, node features, including neighbors keyphrases, are extracted for each node in the graph. Next, a small subset of nodes, along with an initial prompt, are fed into DSPy to produce an optimized prompt. Finally, node classification is performed using the optimized prompt.}
    \label{framework}
\end{figure}

%\subsection{Method overview}
%We perform node classification on a text-attributed graph using an prompt optimized LLM in two main steps: 1) each node $v_i$ in the graph is encoded into a sequential form for use by LLM, 2) the label for the node is predicted using an LLM with an optimized prompt.% in terms of instruction and examples.

\subsection{Node feature preparation}
\label{sec:Node feature preparation}
%Despite the success of LLMs on textual data, it is still an important question to know how LLMs can best realize the graph data 
While LLMs have shown remarkable success with textual data, a crucial question remains: how LLMs can best utilize the information in structured graph data?
\cite{perozzi2024let}. In this study, we use the homophily assumption in graphs which says connected nodes are similar \cite{ciotti2016homophily} and for each node consider the features of the 1-hop neighbors of a node to help the LLM in predicting the node labels. For a node $v_i$ with $N_i = \{v_0,...,v_k\}$ representing the set of its 1-hop neighbors, $v_0$ to $v_k$, we consider the following features beside node text attributes.

%\begin{itemize}
%    \item 
%\begin{definition}
%(Text attributes). This is a sequence of text attributes of the node represented as $s_i$.
%\item 
%\end{definition}

\begin{definition}
(Neighbors labels). This set consists of labels of 1-hop neighbors of node $v_i$ denoted as $l_{N_i} = \{l_0,...,l_k\}$. 
%\item 
\end{definition}

%\begin{definition}
%(Neighbors summary). Let $s_{N_i} = \{s_0,...,s_k\}$ be a set consisting of text attributes of 1-hop neighbors of the node. The neighbors summary, $\tilde{s}_i$, is the textual summary of this set.  %\footnote{We use the llama.cpp \cite{llama.cpp} quantized version of Phi3.5 for all of our summarization experiments: \url{https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF}}. 
%\item 
%\end{definition}

\begin{definition}
(Neighbors keyphrases). Let $s_{N_i} = \{s_0,...,s_k\}$ be a set consisting of text attributes of 1-hop neighbors of the node. %for each 1-hop neighbor of the node, neighbors keyphrases is a set containing the main phrases of the node's neighbors' text attributes.
Neighbors keyphrases denoted by $p_{N_i}= \{p_0,...,p_\zeta\}$ is a set containing the $\zeta$  keyphrases that are shared among the node's neighbors text attributes.
\end{definition}



The process for extracting neighbors keyphrases for each node is detailed in the next section. We apply the keyphrase extraction algorithm to the concatenation of elements in $s_{N_i}$.



\subsubsection{Keyphrase extraction}
\label{sec:keyphrase} 
Keyphrase extraction (KPE) is an automated process that identifies the most important words or phrases from a given text. These keyphrases %provide a concise summary of the text and 
are useful for various downstream applications, such as document classification, clustering, summarization, indexing documents, query expansion, and interactive document retrieval.
Various approaches have been developed for efficient extraction of keyphrases over the past three decades. See \cite{Papagiannopoulou2019ARO} and the references therein for a comprehensive review of well-established keyphrase extraction methods and \cite{Schopf2022PatternRankLP} for a more recent approach using semantic extractions. In our current work, we use a semantic KPE approach that works as follows: first $n$-grams are generated using a count vectorizer using the optimal choice of $n$ identified experimentally. The candidate keyphrases are then mapped into dense representations using Transformer-based contextual embeddings. Similar keyphrases are identified through an embedding-based semantic similarity measure and ranked based on their similarity scores to the input text {\cite{DBLP:journals/corr/abs-1801-04470} using KeyBERT implementation by \cite{grootendorst2020keybert}. The top-ranked candidates are considered the most relevant keywords/keyphrases for the text. Then a diversity module is applied to the selected keyphrases to ensure redundancy reduction using techniques like maximum marginal relevance and max sum similarity \cite{DBLP:journals/corr/abs-1801-04470}. The final result is a set of most important keyphrases that encompass the main content of the input text. 


%We generated the keyphrases for each node using a reranked contextual keyphrase extraction of \cite{DBLP:journals/corr/abs-1801-04470}. We use KeyBERT implementation by \cite{grootendorst2020keybert} for this purpose. 


 %{\color{red}An example of extracted keyphrases from a document is shown in Table \ref{example-table}. In this example, we see that the generated keyphrases are highly relevant to the main context of the document and capture the main concepts of the document in the most concise manner. }
An example of node neighbors summary and node neighbors keyphrases is shown in Table \ref{example-table}. In this example, we see that the keyphrases capture the main concepts of the neighbors summary in a concise manner.


\begin{table}[]
\caption{Example of extracted node neighbors keyphrases compared to the node neighbors summary for a random node with one neighbor in the Cora dataset. }
\label{example-table}
\begin{tabular}{lp{4.5cm}}
\hline
\textbf{Neighbors summary}    & This paper presents an algorithm using reinforcement learning at each node for packet routing in networks; it utilizes local information and outperforms traditional methods with minimal routing times through experiments, even in irregularly-connected network structures. \\ \hline
\textbf{Neighbors keyphrases} & \begin{tabular}[c]{@{}l@{}}distributed reinforcement learning, \\
network routing,\\
routing policies,\\
packet routing \end{tabular} \\ \hline
\end{tabular}
\end{table}




%\end{itemize}
%We use different combination of these features as input to an LLM and evaluate their effectiveness.

\subsection{Prompt optimization}
Considering that the quality of the input prompt to an LLM has a huge effect on the output of the model, we optimize the LLM prompt for the node classification both in terms of instruction and examples. In order to do that, we use the optimization framework of DSPy programming model. DSPy provides a framework in which we can define our task as a program and automatically optimize the prompt for the best performance. %{\color{red}Compared to previous prompt engineering methods, this approach is fast and requires  minimal labeled data. }
%Using a small number of training and validation samples, we bootstrap a few-shot sample set and improve the instructions for the LLM iteratively. 
We will explain each step in the following sections.


\subsubsection{Node classification program} 
The program for node classification is illustrated in Code Snippet \ref{program}. Given the node features and a set of node labels as options, an LLM predicts a label for each node. We use the chain of thought technique \cite{wei2022chain} to let the LLM solve the problem step by step by breaking down the question into simpler tasks. 

\renewcommand{\lstlistingname}{Code Snippet} %
\begin{lstlisting}[caption={\texttt{DSPy} code for NodeClassification program with minor alterations for brevity.}, label={program}, language=Python,breaklines=true,showstringspaces=false,literate={í}{{\'i}}1]
class NodeClassification(dspy.Module):
  def __init__(self):
    self.cot = dspy.ChainOfThought(NodeClassficationSignature)
  
  def forward(self, NodeFeatures: str, options: list[str]) -> Prediction:
    # Predict with LM
    output = self.cot(NodeFeatures=NodeFeatures, options=options).completions.output
    return dspy.Prediction(predictions=output)
\end{lstlisting}

\subsubsection{Signature} 
The prompt for the LLM in the node classification program is defined using a signature abstraction. The signature for node classification on Cora and PubMed datasets are defined in Code Snippet \ref{signature}. This signature contains a task description in a docstring, the node features and a set of labels as inputs and the predicted output along with description and formatting information. As we encode the node information into a text format for use by LLM, we also formulate the node classification task into a text classification task in the task description and ask the LLM to classify a given text into the most applicable category. 
%In the task description, we formulate the node classification task as a text classification task in which we ask the LLM to classify a given text into the most applicable category. The text contains the information about the node.
As Cora and PubMed are citation datasets, the task description specifies that the text is a scientific paper but this can be adjusted for any new dataset depending on the dataset graph content.


\renewcommand{\lstlistingname}{Code Snippet} %
\begin{lstlisting}[caption={\texttt{DSPy} signature of NodeClassification program for Cora and PubMed.}, label={signature}, language=Python,breaklines=true,showstringspaces=false,literate={í}{{\'i}}1]
class NodeClassficationSignature(dspy.Signature):
    __doc__ = f"""Given a snippet from a scientific paper, pick the most applicable category from the options."""
 
    NodeFeatures = dspy.InputField(prefix="Paper:")
    options = dspy.InputField(
        prefix="Options:",
        desc="List of comma-separated options to choose from",
        format=lambda x: ", ".join(x) if isinstance(x, list) else x,
    )
    output = dspy.OutputField(
        prefix="Category:"
    )

\end{lstlisting}

\subsubsection{Compilation}
%To optimize the LLM prompt, we compile the node classification program using different compilers in DSPy. 
We optimize the node classification program in terms of instruction and prediction examples. DSPy compilers handle this optimization programmatically. For instruction optimization, we use COPRO (Coordinate-ascent Optimization by Prompting) \cite{khattab2023dspy, opsahl2024optimizing}, an extension of OPRO approach \cite{opro}. %starting from an initial set of instructions, 
The OPRO method relies on LLMs to iteratively optimize their own prompt based on a given problem description. COPRO generalizes OPRO \cite{opro} by incorporating a coordinate ascent strategy, allowing it to be applied to programs with multiple prompts. In this approach, each prompt is optimized individually while the other parameters remain fixed. In DSPy, the compiler continuously refines the program's instructions based on the LLM’s performance on the validation set, ultimately converging to a set of optimized instructions tailored to the task. 

Similarly, a set of optimized demonstrative examples are added to the prompt by an iterative process using bootstrap few-shot random search approach \cite{khattab2023dspy, opsahl2024optimizing}. In this process, 
a prediction is generated for each example within the training set. Let $\phi(x)$ represent the prediction for an example $x$, $x'$ denote the ground truth, and $\mu(\phi(x), x')$ be the score of the prediction compared to the ground truth based on a metric  $\mu$. If $\mu(\phi(x), x') \ge \lambda$ where $\lambda$ is a predefined threshold, the prediction is considered successful. Upon successful prediction, a demonstration comprising the input to the LLM and the corresponding output is recorded. A predetermined maximum number of these demonstrations are then incorporated into the prompt.
This process is repeated multiple times and the most performant demonstrations on the validation set are selected through random search.
%by initiating different programs and selecting the the most optimized program based on the performance on the validation set. 
We measure the performance of each program using the rank-precision at top K results (RP@K) and the metric defined as \cite{d2024context}:
\begin{equation}
%\fontsize{8}{5}
    RP@K = \frac{1}{N} \sum_{n=1}^{N}\frac{1}{min(K,R_n)}\sum_{k=1}^{K}Rel(n,k)
\end{equation}
where $R_n$ is the set of labels for a node $n$, $Rel(n,k)$ is $1$ if the $k$-th predicted label for node $n$ is relevant and otherwise is $0$. $N$ is the total number of nodes in the set. %We present the results of our measurement in Table \ref{results}.
%\vspace{-0.37cm}
\begin{table}[]
\centering
\caption{The node classification results in terms of accuracy}
\label{results2}
\begin{tabular}{l|llll}
\cline{1-4}
Method   & Cora  & PubMed & Ogbn-arxiv &  \\ \cline{1-4}
Vanilla    & 74.49 & 87.56  & 49.5 &  \\
Chen et al \cite{chen2024exploring} & 74    & 90.75  & 55    &  \\

GraphiT  & 79.84  & 93.28  & 57.25  &  \\ \cline{1-4}
GCN \cite{kipf2016semi}  & 82.20 & 81.01  & 73.10 &  \\ \cline{1-4}
\end{tabular}
\end{table}

\begin{table*}[]
\centering
%\fontsize{9}{10}\selectfont
\caption{Node classification results from our experiments with different node encoding techniques for GraphiT and a Vanilla LLM.} %Using optimization consistently improves the results.}%the vanilla LLM, prompt optimized LLM and the method by Chen et al \cite{chen2024exploring} on Cora and PubMed datasets. We considered four types of graph encodings in vanilla and optimized methods and reported the best results from Chen et al paper.} 
\label{results}
\begin{tabular}{l|l|ccc}
\hline
Dataset                 & Graph info                                  & Vanilla & GraphiT  \\ \hline
\multirow{4}{*}{Cora}   & Text attributes                             & 57.65  & 59.18 \\
                        & Text attributes + Neighbors labels               & 71.68  & 78.31 \\
                        & Text attributes + Neighbors labels + Neighbors summary & 72.19  & 80.1\\
                        & Text attributes + Neighbors labels + Neighbors keyphrases & 74.49  & 79.84  \\ 
                        
                        %& text attributes & -&-&-&82.20\\
                        \hline
\multirow{4}{*}{PubMed} & Text attributes                             & 89.55  & 93.03\\
                        & Text attributes + Neighbors labels               & 87.06  & 90.54  \\
                        & Text attributes + Neighbors labels + Neighbors summary & 87.31  & 92.78 \\
                        & Text attributes + Neighbors labels + Neighbors keyphrases & 87.56  & 93.28 \\
                         
                         %& text attributes & -&-&-&81.01\\
                         \hline
\multirow{4}{*}{Ogbn-arxiv} & Text attributes                             & 40  &  45.25\\
                        & Text attributes + Neighbors labels               &  49.75 &  55\\
                        & Text attributes + Neighbors labels + Neighbors summary & 49 & 58.5 \\
                        & Text attributes + Neighbors labels + Neighbors keyphrases &49.5&  57.25\\
                         
                         %& text attributes & -&-&-&81.01\\
                         \hline

\end{tabular}
\end{table*}
\section{Experiments}
 %In this section, we explain the datasets, settings and the results of the experiments.
\subsection{Datasets}
%The node classification task was performed on the following two public datasets.
%\begin{itemize}
    %\item 
%The node classification task was performed on the followi two public datasets.    
    %\textbf{Cora} ~\cite{he2023harnessing}: a citation network in which each node is an article and there is an edge between two nodes, if one node cites another node. Number of nodes and edges are 2708 and 5429. Each node belongs to one of the 7 classes: case based, genetic algorithms, neural networks, probabilistic methods, reinforcement learning, rule learning, and theory. Each node is associated with a text attribute containing the title and the abstract of the article.
    
    %\item 
    %\textbf{PubMed} ~\cite{he2023harnessing}: a citation network similar to Cora dataset. Number of nodes are 19k and number of edges are 44k. Each node in the dataset has one of the three labels: Experimental induced diabetes, Type 1 diabetes, and Type 2 diabetes. The text attributes of nodes in PubMed are similar to Cora.
%\end{itemize}
%The node classification
We evaluated GraphiT was on three public datasets: Cora, PubMed and Ogbn-arxiv. Cora ~\cite{he2023harnessing} is a citation network where each node is an article and each edge indicates a citation relationship between two articles. Number of nodes and edges are 2708 and 5429. Each node belongs to one of the 7 classes: case based, genetic algorithms, neural networks, probabilistic methods, reinforcement learning, rule learning, and theory. Each node is associated with a text attribute containing the title and the abstract of the article. Similarly, PubMed ~\cite{he2023harnessing} is a citation network with 19k nodes and 44k edges. Each node in the dataset has one of the three labels: experimental induced diabetes, type 1 diabetes, and type 2 diabetes. The text attributes of nodes in PubMed are similar to Cora. Ogbn-arxiv \cite{hu2020open} is also a citation networks between all Computer Science arxiv papers containing 169k nodes, 1M edges and 40 subject areas.

%\vspace{-0.3cm}
\subsection{Settings}
Similar to ~\cite{chen2024exploring}, we randomly selected 200 nodes from the test set of each dataset as our test data. %, where each dataset was split 60:20:20 for training, validation, and test sets.
The reported scores are averaged over two sampled test sets. Our evaluation metric is $RP@1$ which is equivalent to accuracy in our experiments. The LLM that we used was \lstinline|gpt-3.5-turbo-1106|. We used \lstinline|BootstrapFewShotWithRandomSearch| and  \lstinline|COPRO| compilers from DSPy. The length of ngrams in the keyphrase extraction step is set to $ngram \in \{1,2,3\}$ and we set $\zeta=5$. The nodes neighbors summaries are generated using the quantized version of the Phi 3.5 model \cite{abdin2024phi3technicalreporthighly, phi3-quantized} by llama.cpp \cite{llama.cpp}.

\subsection{Node classification}
We evaluate the performance of GraphiT compared to three baselines on three datasets. Each node in the graph is encoded by integrating the node's neighbors' keyphrases with its text attributes and the labels of its neighbors. Without the loss of generality, for prompt optimization, we generate small training and validation sets by randomly sampling 3 and 2 nodes per class from training and validation sets of Cora dataset. Then, we use the optimized programs for inference on arbitrarily large test sets. 

Table \ref{results2} presents the node classification results from GraphiT, the result from the vanilla LLM using the same graph encoding, the best results from an unoptimized few-shot learning approach using LLMs by Chen et al \cite{chen2024exploring} and a graph convolutional network (GCN) result \cite{kipf2016semi} obtained from \cite{chen2024exploring}. We were able to compare with the methods reported in \cite{chen2024exploring} as they used the same number of nodes in the test sets for each dataset as us and were designed for the node classification task. GraphiT outperforms the results by the LLM-based models on all three datasets. It also achieves superior performance on PubMed compared to GCN. However, GraphiT falls short of the performance by GCN on Cora and Ogbn-arxiv datasets. This could be because GCN captures information from 2-hop neighbors for each node, which is useful for node classification on those datasets. Exploring the incorporation of neighbors beyond 1-hop in GraphiT will be one of our future research directions.
%It could be an interesting direction for our future research to examine the effect of including neighbors beyond 1-hop in GraphiT. }


%{\color{red}However, while promising for optimizing LLMs in the node classification task, GraphiT falls short of the performance achieved by GNNs, indicating an important direction for future research.}
%However, while promising for optimizing LLMs in the node classification task, GraphiT lags behind the state-of-the-art performance achieved by GNNs in Cora and ogbn-arxiv datasets. This gap highlights an important area for future research: exploring ways to bridge the performance disparity between LLMs and GNNs. We plan to investigate strategies for improving the effectiveness of LLMs in this context, such as integrating them with GNNs or developing more sophisticated node feature encoding techniques.



\subsection{Ablation study}
We investigate the effects of different components of GraphiT across three dataset. One major component of our model is the node neighbors keyphrases. We consider four settings to encode nodes into a sequence format, beginning with only the text attributes of the nodes and progressively incorporating additional features through concatenation. Table \ref{results} shows the results of the node classification for GraphiT in four settings. For all the datasets, incorporating neighbors keyphrases alongside the text attributes and neighbors labels enhances performance. Moreover, this approach has a comparable or better results compared to using neighbor summaries while significantly reducing the context length in the LLM prompt.  In Table \ref{results}, we also have the node classification results from Vanilla LLM across the four settings. Using neighbors keyphrases has a similar effects on Vanilla LLM. Additionally, we can see that GraphiT consistently outperforms the Vanilla method across all node encoding techniques.




%We consider four settings to encode nodes into a sequence format, beginning with only the text attributes of the nodes and progressively incorporating additional features through concatenation. Without the loss of generality, we generate small training and validation sets for prompt optimization by randomly sampling 3 and 2 nodes per class from training and validation sets of Cora dataset. Then, we use the optimized programs for inference on arbitrarily large test sets. 

%Table \ref{results} shows the results of the node classification with the vanilla LLM and GraphiT. For Cora, the results show that using the combination of text attributes, neighbor labels and summaries in GraphiT achieves the highest score with a significant improvement over using only text attributes with vanilla LLM. The best performing combination for the PubMed is text attributes, neighbor labels and keyphrases with GraphiT. For ogbn-arxiv, the best performing node encoding is the combination of text attributes, neighbor labels and summaries.

%Table \ref{results2} presents the node classification results of our best result compared to the best result from the vanilla LLM, the best results from an unoptimized few-shot approach method by Chen et al \cite{chen2024exploring} and a graph convolutional network (GCN) result \cite{kipf2016semi} obtained from \cite{chen2024exploring}. We were able to compare with these methods reported in \cite{chen2024exploring} as they used the same number of nodes in the test sets for each dataset as us. GraphiT outperforms the results by the LLM-based models. While promising for optimizing LLMs in graph prediction tasks, the results show that GraphiT lags behind the state-of-the-art performance achieved by GNNs. This gap highlights an important area for future research: exploring ways to bridge the performance disparity between LLMs and GNNs. We plan to investigate strategies for improving the effectiveness of LLMs in this context, such as integrating them with GNNs or developing more sophisticated node feature encoding techniques. An important insight from the results is that incorporating neighbors’ keyphrases can have a similar or even better effect than using neighbors’ summaries, while significantly reducing the context length in the LLM prompt. 



%\begin{table}[]
%\centering
%\footnotesize
%\caption{The node classification results with and without optimization.} 
%\label{results}
%\begin{tabular}{l|l|ll}
%\hline
%Dataset                 & Graph info                                  & Vanilla & Ours   \\ \hline
%\multirow{4}{*}{Cora}   & text attributes                             & 57.65  & 58.92 \\
%                        & text attributes + neighbor labels               & 71.68  & 78.82  \\
%                        & text attributes + neighbor labels + neighbor summary & 72.19  & \textbf{80.0}  \\
%                        & text attributes + neighbor labels + neighbor keyphrases & 74.49  & 79.84  \\ 
%                        \hline
%\multirow{4}{*}{Pubmed} & text attributes                             & 89.55  & 91.54 \\
%                        & text attributes + neighbor labels               & 87.06  & 92.29 \\
%                        & text attributes + neighbor labels + neighbor summary & 87.31  & 91.79 \\
%                        & text attributes + neighbor labels + neighbor keyphrases & 87.56  & \textbf{93.28} \\\hline
%\end{tabular}
%\end{table}





%\begin{table}[]
%\centering
%\caption{{\color{red}Comparing our final results with other methods for the node classification task}}
%\label{results2}
%\begin{tabular}{l|lllll}
%\cline{1-5}
%dataset & Chen et al \cite{chen2024exploring} & GNN/Sage \cite{chen2024exploring} & Vanilla & Optimized  \\ \cline{1-5}
%Cora    & 74         & 82.20   & 74.49 & 80.1  &  \\ 
%PubMed  & 90.75      & 81.01   & 89.55 & 93.28 &  \\ 
%Arxiv   & 55         & 73.10    & 49.75       & 58.5 \\ \cline{1-5}
%\end{tabular}
%\end{table}






\subsection{Cost comparison of using neighbors keyphrases versus summary}
  In Figure \ref{hist}, we present a histogram depicting the ratio of the number of tokens in neighbors summary to those in neighbors keyphrases for all datasets combined. The figure indicates that the average number of tokens resulting from the KPE approach on the node neighbors text is a few times smaller that the ones from the summarization method. %as can be seen in Fig. \ref{hist}. 
 As a result, leveraging keyphrases leads to lower LLM API costs while still delivering competitive results compared to the summarization approach, as shown in Table \ref{results}.
 % Using these keyphrases as the input to the LLM for the node classification task, yields competitive results to using the summaries as it is shown in Table \ref{results} at a lower LLM API costs due to the shorter input context length of the keyphrases.  
In addition, for the KPE method, we use small encoder models for the generation of embeddings which is fast and lightweight, easily suitable for running on ordinary CPUs of today's laptops \cite{reimers-2019-sentence-bert}.

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{Picture2.png}
    \caption{Histogram of the ratio of the number of tokens for summaries to those obtained from the KPE approach. The KPE method applied to the node neighbors results in significantly less tokens compared to the summarization method with minimal impact on the quality of the classification results. This reduction translates to lower LLM API costs by making the input context length considerably shorter.  }
    \label{hist}
\end{figure}



\section{Conclusions}
Our paper focuses on graph encoding and LLM optimization for the node classification task on text-attributed graphs. We demonstrate that the information in the nodes neighborhood is efficiently represented by the right choice of keyphrases. In addition, we optimize the LLM prompt automatically by refining instructions and adding demonstrative examples to the prompt leveraging the DSPy optimization framework. %simple program. 
We compare the performance of our approach, GraphiT, with three baselines across three public datasets. The results demonstrate that our approach has a better performance compared to other models that are based on LLM models in all experiments. While promising for optimizing LLMs in the node classification task, GraphiT falls short of the GNNs performance on two datasets, highlighting a key area for our future research. Strategies like incorporating more neighborhood information for a node and integrating LLMs with GNNs could help bridge this performance gap. Furthermore, we will extend our approach to other graph prediction tasks, including link prediction.



%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
