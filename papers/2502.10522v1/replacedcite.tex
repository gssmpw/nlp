\section{Related works}
%{\color{red}Write about Large language models in graphs: Large Language models have been used in graphs mainly as a predictor or enhancer.}
%LLMs have been mainly used as enhancers and predictors on graph data. As an enhancer, LLMs enhance the features or structure of the graph and then their outputs are used in downstream applications ____. As a predictor, LLMs are directly applied to the graph data to perform the machine learning task ____. 
GNNs are the frontier techniques in the field of graph representation learning ____. However, they use shallow embeddings to represent text attributes of nodes. Given the capability of LLMs to generate rich contextual embeddings, several works have combined LLMs with GNNs to enhance GNN performance ____. Leading ____ employs an end-to-end training of LMs and GNNs for graph prediction tasks. Engine ____ combines LLMs and GNNs using a tunable side structure. Despite their effectiveness, these integrations create complex systems that are often computationally intensive and require labeled data for training.
%However, this adds to the computation resources needed for performing the prediction specifically if it involves training of the models. 
Therefore, other studies investigate the possibility of using LLMs alone for graph prediction tasks. 
%The combination of graphs and LLMs has been an active research area recently. 
%Specifically, encoding graphs into text for use in LLMs and optimizing the performance using LLM's fine-tuning and prompt engineering are two key issues in applying LLMs on graph applications. 
In ____, LLMs are utilized for several graph reasoning tasks such as connectivity, shortest path and topological sort using two instruction-based prompt engineering techniques. InstructGLM ____ proposes a instruction fine-tuning method for node classification by LLMs. In ____, LLMs have been used both as enhancer and predictor for node classification task. It encodes the nodes into text by incorporating text attributes and 2-hop neighbors summaries. In ____, different methods for graph encoding and prompt engineering were investigated. In ____, graphs are input to LLMs using a graph encoder which was trained similar to soft prompting methods ____. 
%P2TAG ____ jointly pre-trains the LLMs and graph neural net on the node classification task. 
%Different from fine-tuning, soft prompting and prompt engineering, we use a prompt programming technique to prompt optimization.
Fine-tuning and soft promoting techniques require training with labeled data. Traditional prompt engineering relies heavily on human expertise and manual adjustments. In contrast, we programmatically optimize LLM usage with only a small set of labeled data. In addition, we efficiently capture the information in a node's neighborhood by extracting keyphrases from text attributes of neighboring nodes. 
%In addition, as nodes are most similar to their immediate neighbors in real world datasets ____,
%In addition, as nodes most likely share similar characteristic to their immediate neighbors ____, 
%we focus on including 1-hop neighbors in our optimization approach.

%investigate their effects in node classification task using their labels, summaries and keywords.



%There are different graph encoding methods including text features of the graph ____ and graph embeddings obtained from a graph encoder ____.



%