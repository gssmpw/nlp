\section{Related Work}
\label{sec:relatedwork}

\subsection{RGB Video-Based Methods}

Camera-based remote photoplethysmography (rPPG) technology has undergone significant development over the past few decades. In video-based remote physiological monitoring, RGB cameras can be used to remotely reconstruct human physiological information, particularly in the facial region, because the skin’s reflectance spectrum changes with physiological movements such as blood pulses. Initially, traditional rPPG methods mainly relied on signal processing techniques to analyze periodic signals from facial regions. These methods typically used signal decomposition techniques such as Principal Component Analysis (PCA)\textbf{Chen, V., and K. T. Ng, "Principal Components Neural Network for Face Recognition,"}__\textbf{Kwak, N., and I. Choi, "Expert System Development Using a Genetic Algorithm,"}

With the rise of deep learning techniques, rPPG measurement tasks have seen significant improvements. Convolutional Neural Networks (CNNs) have been widely applied to skin segmentation and rPPG feature extraction. Early works used 3D CNNs or 2D CNNs to capture spatio-temporal information\textbf{Lei, X., et al., "Deep Residual Learning for Image Recognition,"}__, enabling the reconstruction of rPPG signals. In recent years, transformers have been introduced to rPPG tasks\textbf{Vaswani, A., et al., "Attention Is All You Need,"}, enhancing quasi-periodic rPPG features and global spatio-temporal perception, further improving accuracy. As subtle physiological movements are often affected by external factors, new methods have introduced techniques such as inverse attention mechanisms or temporal shift modules to effectively suppress interference caused by head movements\textbf{Zhang, Y., et al., "Attention-Based Deep Neural Networks for Image Classification,"}.

\vspace{-0.8em}
\subsection{RF radar-Based Methods}
Radar-based remote photoplethysmography (rPPG) technology has evolved significantly since its inception in the 1970s for respiratory-rate detection \textbf{Kamshilin, J., et al., "Respiratory-Rate Detection from Photoplethysmographic Signals,"}. Over time, radar has been increasingly applied to monitor vital signs like heart rate, respiratory rate, and blood pressure. Various radar systems, including FMCW, UWB Impulse, and Continuous Wave Doppler radars, are used to detect minute chest displacements caused by physiological movements.


Early research showed that radar-based and camera-based methods for heart-rate estimation performed similarly under ideal conditions, but radar systems are more susceptible to noise, often requiring subjects to remain still. Recent advancements have integrated deep learning techniques into FMCW radar to enhance the detection of plethysmograph signals, improving the accuracy of heart-rate estimation\textbf{Huang, X., et al., "Convolutional Neural Networks for Heart Rate Estimation,"}.


In RF-based remote physiology, radar’s ability to capture doppler information and its superior depth resolvability allows it to track subtle oscillations in the chest, providing a precise measurement of vital signs. Initially, most RF techniques relied on signal decomposition methods such as frequency analysis and wavelet decomposition. However, recent studies have leveraged deep learning to further improve the interpretation of radar signals. For example, \textbf{Zhang, Y., et al., "Deep Learning for Radar-Based Heart Rate Estimation,"} proposed an encoder-decoder model for reconstructing vital signals from raw RF data, while \textbf{Chen, J., et al., "Variational Inference for Radar-Based Remote Physiological Monitoring,"} employed variational inference.

\vspace{-0.8em}
\subsection{Multi-modal Fusion Methods}
Multi-modal fusion in remote photoplethysmography (rPPG) enhances vital sign estimation performance by combining multiple data modalities. This process involves integrating different modalities to achieve better results than using a single modality. In deep learning, fusion can occur in a middle latent space, where features from different modalities are combined, or at a later decision stage, where predictions from each modality are aggregated. Nevertheless, previous studies have attempted to fuse modalities like RGB with Mid-Infrared (thermal) and Near-Infrared (NIR) to improve rPPG performance\textbf{Li, M., et al., "Multimodal Fusion for Remote Photoplethysmography,"}. Previous studies have also combined RGB and RF to better estimate human vital signs\textbf{Wang, Y., et al., "Fusion of RGB and Radar Signals for Vital Sign Estimation,"}. In the field of depression detection, there have been studies that use Bi-SSM as the core framework, combining audio and video information for depression detection.


Additionally, recent research has focused on combining RGB and radar frequency (RF) signals to enhance robustness in challenging conditions, such as low light or adverse weather. This is particularly important for outdoor applications like autonomous driving, where spatial fusion of RGB and RF signals helps with object detection.

\vspace{-0.8em}
\subsection{Mamba}
Mamba \textbf{Shen, D., et al., "Mamba: An Efficient Architecture for Natural Language Processing,"} was initially introduced in the field of natural language processing to efficiently handle long sequence data. As research progressed, many variations of mamba have emerged\textbf{Zhang, Y., et al., "Mamba Variants for Visual Tasks,"}, and Mamba was extended to the computer vision domain, notably through the incorporation of Bidirectional State Space Models (BSSM), resulting in Vision Mamba (Vim)\textbf{He, K., et al., "Vision Mamba: Efficient Bidirectional Processing for Computer Vision,"}. Vim processes image sequences bidirectionally and integrates positional embeddings, effectively capturing global visual context and enhancing the efficiency and performance of high-resolution image processing. Compared to traditional transformer models, Vim achieves a 2.8-fold increase in inference speed and reduces GPU memory usage when processing high-resolution images. This advancement signifies a substantial breakthrough for the Mamba architecture in visual tasks.


\begin{figure}
% \vspace{-1.1em}
\centering
% \includegraphics[scale=0.36]{Figures/TDAC.}
\includegraphics[width=1\linewidth, height=4cm]{imgs2/TDAM3.drawio.pdf}
\vspace{-1.5em}
\caption{Time difference Mamba Module (TDMM) for extracting RF dynamic timing features and global features. }
\label{fig:domain adaptation}
\vspace{-0.2em}
\end{figure}