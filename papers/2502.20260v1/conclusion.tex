\section{Conclusion}
\label{sec:conclusion}

In this paper, we first investigate the challenges posed by temporal distribution shifts in tabular data, with a focus on effective strategies for addressing them. Starting with a \textit{training protocol} that fully leverages temporal data, we analyze the impact of training lag, validation bias, and the equivalence of validation. Building on these insights, we propose a novel splitting strategy that significantly improves model performance. We further demonstrate that capturing temporal information during training is crucial, and observe that periodic and trend information is often \textit{lost in the learned model representations}. To compensate for this loss, we introduce a \textit{temporal embedding} method that incorporates temporal information from timestamps, improving the modelâ€™s adaptability to temporal shifts. By combining the new temporal split with the proposed embedding, we observe marked improvements in model performance, particularly for retrieval-based models that previously struggled under temporal shifts. These findings provide valuable insights for advancing deep learning approaches for temporal tabular data, highlighting the importance of both temporal data training protocol and temporal feature integration.