\section{Dataset Explanation}
\label{sec:appendix_dataset}

The detailed information about the dataset is provided in \cref{tab:dataset}. We used the dataset from TabReD \cite{rubachev2024tabred} in its entirety and applied the same preprocessing method.

\begin{table}[ht]
\vspace{-5pt}
\centering
\setlength{\tabcolsep}{5pt}
\begin{tabular}{clrrcl}
\toprule
Abbr. & Dataset        &  Samples &  Features & Task Type & Task Description \\ \midrule
HI & Homesite Insurance       & 260753      & 296         & Classification          & Insurance plan acceptance prediction \\ 
EO & Ecom Offers              & 160057       & 119         & Classification          & Predict whether a user will redeem an offer \\ 
HD & HomeCredit Default       & 381664 & 696        & Classification          & Loan default prediction \\ 
SH & Sberbank Housing         & 28321      & 387         & Regression          & Real estate price prediction \\ 
CT & Cooking Time             & 319986 & 195       & Regression             & Restaurant order cooking time estimation \\
DE & Delivery ETA             & 350516  & 225       & Regression             & Grocery delivery courier ETA prediction \\ 
MR & Maps Routing             & 279945 & 1026      & Regression             & Navigation app ETA from live road-graph features \\
WE & Weather                  & 423795  & 98        & Regression             & Weather prediction (temperature) \\ \bottomrule
\end{tabular}
\vspace{-10pt}
\caption{Overview of Datasets. Task descriptions from \citet{rubachev2024tabred}.}
\label{tab:dataset}
\vspace{-5pt}
\end{table}

In \cref{fig:split}, \cref{tab:stability}, and \cref{tab:ablation}, we compare the average performance improvement for each method under different strategies ({\it e.g.} splitting or temporal embedding), specifically the percentage increase in AUC on classification datasets and the percentage decrease in RMSE on regression datasets. In \cref{fig:random} and \cref{fig:temporal}, we present a comparison of performance across different training protocols and methods, where all results are reported as performance improvements relative to the MLP performance under the original split in \cite{rubachev2024tabred}. Since performance improvements between methods can often be influenced by outliers, we apply a robust average, excluding the maximum and minimum performance improvements across the eight datasets before calculating the mean.

The only distinction in the implementation is that TabReD employs different embedding methods for numerical and categorical features for each method-dataset pair. Specifically, it uses identity or noisy-quantile encoding for numerical features and one-hot or ordinal encoding for categorical features.
To ensure a fair comparison, we reproduced the experiment from TabReD by removing numerical embeddings and fixing categorical embeddings to one-hot encoding where necessary. This adjustment is essential for accurately evaluating the performance of the methods and for advancing future research on temporal embeddings.

It is important to note that the HomeCredit Default dataset suffers from severe class imbalance, which makes it challenging for methods with limited feature extraction capabilities, such as MLP, SNN \cite{klambauer2017self}, and DCNv2 \cite{wang2021dcn}, to perform well without additional numerical feature embeddings.
While the AUC of the naive MLP (as well as SNN and DCNv2 methods) on this dataset drops to around 0.55, rendering the comparison with other methods (which typically achieve an AUC greater than 0.80) meaningless, it is noteworthy that the insights on training protocols and temporal embeddings explored in this work still lead to significant performance improvements on this dataset, as shown in \cref{sec:appendix_result}.

\begin{figure*}[b]
  \vspace{-10pt}
  \centering
  \includegraphics[width=0.23\linewidth]{image/sample/1.png}
  \includegraphics[width=0.23\linewidth]{image/sample/2.png}
  \includegraphics[width=0.23\linewidth]{image/sample/3.png}
  \includegraphics[width=0.23\linewidth]{image/sample/4.png} \\
  \includegraphics[width=0.23\linewidth]{image/sample/5.png}
  \includegraphics[width=0.23\linewidth]{image/sample/6.png}
  \includegraphics[width=0.23\linewidth]{image/sample/7.png}
  \includegraphics[width=0.23\linewidth]{image/sample/8.png}
  \vspace{-10pt}
  \caption{The temporal sampling distribution for TabReD datasets. The time slices are divided into half days ({\it i.e.}, 12 hours).}
  \label{fig:sample}
\end{figure*}

At the same time, our experiment on the equivalence (\cref{fig:split}) of the validation set shows that the datasets whose experimental results seriously fail to meet the equivalence assumption, such as Sberbank Housing and Ecom Offers, are confirmed to have the most serious nonuniformity in sampling. The temporal sampling distribution for each dataset is shown in \cref{fig:sample}. 
Instead of focusing on the time span between the training and validation sets, we concentrate on ensuring that the number of instances in both sets is equal across different splits, thereby enabling a fair comparison of the splitting strategies. When the temporal sampling distribution becomes highly non-uniform, the observation in \cref{fig:mmd} no longer holds, as it relies on a constant time span for the validation set. Consequently, our insights regarding the equivalence of the validation sets are significantly affected in such situations.

This indicates that the conclusion regarding the equivalence of the validation set on different temporal direction is actually stronger, especially for datasets with uniform sampling over time.