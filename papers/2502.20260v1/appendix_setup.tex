\section{Experimental Setup}
\label{sec:appendix_setup}

We adopt training, evaluation and tuning setup from \citet{ye2024closer} and \citet{liu2024talent}. We tune hyper-parameters using Optuna \cite{akiba2019optuna}, performing 100 trials for most methods to identify the best configuration. The hyper-parameter search space follows exactly the settings in \citet{rubachev2024tabred}. Using these optimal hyper-parameters, each method is trained with 15 random seeds, and the average performance across seeds is reported. We label the attribute type (numerical or categorical) for gradient boosting methods such as CatBoost. For all deep learning methods, we use a batch size of 1024 and
AdamW \cite{loshchilov2019decoupled} as the optimizer.

We followed \citet{rubachev2024tabred} and performed only 25 hyper-parameter tuning runs for FT-T and TabR, as these methods exhibit lower efficiency on datasets with large feature dimensions and sample sizes. For our temporal embedding, we conducted separate hyper-parameter searches for the periodic order and linear trend. However, since 25 tuning trials

For classification tasks, we evaluate models using AUC (higher is better) as the primary metric and use RMSE (lower is better) for regression tasks to select the best-performing model during training on the validation set.

To ensure the validity of random splitting, each group of random split experiments was tested on three distinct random splits, with 15 random seeds run on each split. The mean performance across these runs is reported as the final result. The variance of the random split is calculated based on all 45 results (3 splits Ã— 15 seeds), as the random split is subject to variance from both the split selection and the running seeds during the training phase. This approach better reflects the overall stability of the standard procedure.