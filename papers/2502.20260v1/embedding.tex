\section{Temporal Embeddings}
\label{sec:embedding}

\begin{figure}[t]
  \centering 
  \includegraphics[width=\linewidth]{image/adaptive.pdf}
  \vspace{-22pt}
  \caption{Illustration of adaptive approaches for temporal shift mitigation. Our proposed temporal embedding method offers a lightweight alternative that implicitly enables adaptation by embedding temporal information. This allows the model to learn phase-specific knowledge and adjust the mapping \( f_t \) accordingly, thereby achieving temporal generalization. }
  \label{fig:adaptive}
  \vspace{-3pt}
\end{figure}
\begin{table}[t]
  \centering
  \setlength{\tabcolsep}{4pt}
\begin{tabular}{lcccc}
    \toprule
    \textbf{Emb.} & MLP & MLP-PLR & ModernNCA & \textbf{Avg. Imp.} \\
    \midrule
    Num & $-$0.04\% & $-$0.06\% & $-$0.04\% & $-$0.05\% \\
    Time & $-$0.70\% & $-$0.15\% & $-$0.32\% & $-$0.39\% \\
    PLR & $+$0.70\% & \textbf{$+$0.01\%} & $+$0.02\% & $+$0.25\% \\
    Ours & \textbf{$+$1.31\%} & \textbf{$+$0.01\%} & \textbf{$+$0.30\%} & \textbf{$+$0.54\%} \\
    \bottomrule
    \end{tabular}
  \vspace{-10pt}
  \caption{Performance improvement after adopting different temporal embeddings on TabReD benchmark. The embedding is applied only to timestamps, which are then treated as numerical features for the model. Non-learnable embeddings are generally ineffective, while the learnable PLR embedding provides a slight improvement only when applied to MLP. Our temporal embedding outperforms all other embeddings. Detailed results are provided in \cref{sec:appendix_result}.}
  \label{tab:ablation}
  \vspace{-15pt}
\end{table}

Building on the above analysis, it is essential to incorporate temporal information into the model in a manner that effectively captures the underlying temporal dependencies. To address this issue, we propose a \textit{lightweight, plug-and-play temporal embedding method} specifically designed for timestamps, aiming to investigate whether providing explicit temporal information through embedding can lead to performance improvements. Empirically, timestamps are often treated as noise and discarded. However, in the context of temporal shifts and temporal splitting, timestamps likely contain crucial temporal information that enables the model to align with the periodicity and trends inherent in the temporal sequence. Existing works have also emphasized that, in certain contexts, timestamps serve as a valuable feature \cite{wang2024rethinking, zeng2024howmuch}.

Our temporal embedding also serves as an \textit{adaptive model-based approach}, as the model can learn temporal stage-specific knowledge by leveraging timestamp information, thereby adaptively adjusting its mapping at different temporal stages after deployment. 
Formally, the model’s objective function can be written as \( f = g \circ h\), where \( g \) maps the input features to the same representation space. 
When the data exhibits temporal shifts, this implies that there exists a specific mapping \( f_t = g_t \circ h_t\) at time step \( t \). The model can now learn phase-specific knowledge and adjust the mapping \( f_t \) accordingly, as illustrated in \cref{fig:adaptive}.

In our embedding method, We fit multi-scale periodicity using Fourier series expansion \cite{tancik2020fourier, li2021learnable}. The Fourier series can be expressed as:
\begin{align*}
    f(t) = \sum_{k=1}^{\infty} a_k \cos\left( \frac{2\pi k t}{T} \right) + b_k \sin\left( \frac{2\pi k t}{T} \right).\notag
\end{align*}
By increasing the order of the expansion, we can approximate any continuous periodic function $f$ with period $T$ according to the approximation properties of Fourier series.
Our temporal embedding can be described as follows:
\begin{align*}
    \text{Temporal}(t) = \left[ \text{ReLU}\left( \text{Linear}\left( \text{Periodic}(t) \right) \right), \text{Trend}(t) \right]\notag
\end{align*}
where $t$ is the timestamp, and the two components of the embedding each capture the periodicity and trend of the timestamp, respectively. Those are further defined as
\begin{align*}
    \text{Periodic}(t) = \left[ \text{Fourier}(t, T_1), \dots, \text{Fourier}(t, T_m) \right],\notag
\end{align*}
where $T_i$, $i \in [m]$ are \( m \) given periodicity priors for Fourier-based embedding. In our experiments, we set $T_i$ for yearly, monthly, weekly, and daily periodicity. These periodicity are chosen based on the common temporal patterns observed in the datasets, which effectively capture natural patterns in datasets with inherent temporal correlations, such as Weather, by modeling yearly and daily periodicity, and societal-related temporal patterns, such as monthly and weekly periodicity, in datasets like Homesite Insurance and Cooking Time.
Each Fourier embedding is defined as
\begin{align*}
    \text{Fourier}(t, T) = \left[ \sin\left( \frac{2\pi t k}{T} \right), \cos\left( \frac{2\pi t k}{T} \right) \right],\notag
\end{align*}
for $k \in \{1, 2, \dots, \text{order}\}$, which represents the process of Fourier series expansion. By decomposing multiple periods into the components of a Fourier series and passing them through a learnable linear layer, the process of solving and aggregating parameters $a$ and $b$ in the Fourier series can be simulated, resulting in a set of periodic functions that incorporate multiple cycles. The ReLU activation further facilitates the selection process, ensuring the quality of the learned periodic information.

\begin{figure*}[t]
  \centering 
  \vspace{-5pt}
  \includegraphics[width=\linewidth]{image/performance_comparison_temporal_embedding.pdf}
  \vspace{-28pt}
  \caption{Performance comparison before and after adopting our proposed temporal embedding into our training protocol on the TabReD benchmark. This figure follows the same setup as \cref{fig:random}, allowing for direct comparison. Detailed results are provided in \cref{sec:appendix_result}.}
  \label{fig:temporal}
  \vspace{-10pt}
\end{figure*}

In addition to the periodic component, we also provide an optional trend term for the temporal embedding. When the trend is enabled, the final embedding is augmented with a standardized timestamp, which captures the linear temporal shift beyond the periodic components, represented by
\begin{align*}
    \text{Trend}(t) = \text{z-score}(t).\notag
\end{align*}
To thoroughly evaluate the effectiveness of our embedding method, we designed a series of comparative experiments, including the following configurations:
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=*]
    \item None: Timestamps are not utilized as the baseline.
    \item Num: Treating the timestamp as a single numerical feature and directly inputting it into the model.
    \item Time: Decomposing the timestamp into six numerical features: year, month, day, hour, minute, and second, which introduces partial periodicity information while maintaining a human-readable representation of the timestamp.
    \item PLR: Applying a learnable one-dimension PLR  embedding \cite{gorishniy2022embeddings} to the timestamp, enabling the model to capture temporal patterns adaptively.
\end{itemize}

We choose MLP, MLP-PLR and ModernNCA for comparison, the results are presented in \cref{tab:ablation}. The results indicate that while directly using non-learnable embedding methods can be effective in certain cases, it generally leads to a performance degradation, which is consistent with the common practice of treating timestamps as noise. 
The performance of PLR embedding is not stable, likely because it discards linear trends and struggles to accurately capture periodic patterns. Results from the MLP-PLR method show no significant improvement, which may be attributed to its incompatibility with the existing numerical embedding. 

The models’ performance improvement after adopting our temporal embedding are presented in \cref{fig:temporal}. Most methods demonstrate improvements, highlighting the importance of leveraging temporal information in addressing temporal shift tasks. Notably, MLP, DCNv2, and FT-T show significant improvements, while as previously mentioned, MLP-PLR, TabR, TabM, and ModernNCA exhibit limited gains, likely due to their incompatibility with numerical embeddings. This suggests that temporal features may require dedicated embedding strategies rather than relying on existing numerical embedding approaches. After applying temporal embedding, both ModernNCA and TabR demonstrate strong performance, indicating that with an appropriate training protocol and temporal embedding, even retrieval-based methods, which are typically most affected by distributional shift, can regain their practical utility.

The MMD heatmap of the model representation after adopting temporal embedding is shown in \cref{fig:periodic} right. The patterns are closer to the original data, reflecting that it captures rich and correct temporal information, thus effectively alleviating the loss of temporal information during training. The reappearance of diagonal stripes indicates that the model has learned independent representations for each temporal phase within the period, thereby confirming the adaptive role of temporal embedding.