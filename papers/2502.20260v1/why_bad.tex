\section{Why Temporal Split is Ineffective for Tabular Data?}
\label{sec:why_bad}

\begin{figure*}[t]
  \centering
  % \vspace{-15pt}
  \begin{minipage}{0.39\linewidth}
      \centering
      \includegraphics[width=\linewidth]{image/cooking-time-mlp_plr-performance.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.59\linewidth}
      \centering
      \includegraphics[width=0.24\linewidth]{image/mmd/homesite-insurance_time_linear.png}
      \includegraphics[width=0.24\linewidth]{image/mmd/ecom-offers_time_linear.png}
      \includegraphics[width=0.24\linewidth]{image/mmd/homecredit-default_time_linear.png}
      \includegraphics[width=0.24\linewidth]{image/mmd/sberbank-housing_time_linear.png} \\
      \includegraphics[width=0.24\linewidth]{image/mmd/cooking-time_time_linear.png}
      \includegraphics[width=0.24\linewidth]{image/mmd/delivery-eta_time_linear.png}
      \includegraphics[width=0.24\linewidth]{image/mmd/maps-routing_time_linear.png}
      \includegraphics[width=0.24\linewidth]{image/mmd/weather_time_linear.png}
  \end{minipage}
  \vspace{-10pt}
  \caption{Left: The loss distribution shows how the model's performance distributed across time slices under different validation splitting strategies. The vertical axis represents the loss, where lower is better. Non-lagged splits (b) and (d) achieve better performance around \(T_\text{train}\) compared to lagged splits (a) and (c), while the higher-biased split (c) performs better on training-available data but fails to generalize compared to the lower-biased split (a). The loss distribution is smoothed by a Gaussian filter for better visualization. Right: The MMD heatmap visualizing the distribution distance between different time slices using linear kernel. The time slices are divided by date.}
  \label{fig:mmd}
  \vspace{-10pt}
\end{figure*}

The fundamental challenge in addressing temporal shifts stems from the inherent difficulty in characterizing evolving data distributions, which requires models to maintain robustness against unknown future variations. 
Temporal splits are frequently used in forecasting-related tasks to uphold the sequential dependencies inherent in the data \cite{bergmeir2012use, zeng2023transformers, han2024revisiting}, yet they generally show inferior performance compared to random splits in tabular data contexts, as evidenced in \cref{sec:introduction} and \cref{fig:random}.
This counterintuitive phenomenon warrants systematic analysis through multiple perspectives:
We first start with the most intuitive distinction between random split and temporal split, and verify the hypothesis of how training lag (\cref{subsec:lag}) and validation bias (\cref{subsec:bias}) impact the model performance.
Then we discover the equivalence of the validation set in different temporal directions (\cref{subsec:equivalence}). Building on this, we propose an enhanced splitting strategy (\cref{subsec:citerion}) for temporally shifted tabular data, which achieves performance comparable to random splitting while offering improved stability.

\subsection{Training Lag}
\label{subsec:lag}

From the perspective of the training set, temporal split adopted in \citet{rubachev2024tabred} introduces a \textit{temporal lag} between training and test set, while random split provides more instances closer to the test time for training. An intuitive hypothesis is that instances closer to the test time are more reliable, as the distribution near the test time tends to be more similar to the actual test-time distribution, and using them for training could have a greater effect than for validation. 
Hence we adapt a pair of splitting strategies, shown in \cref{fig:split} left (a) and (b), where identical validation and test sets are maintained while varying the lag between training and test set, thereby isolating and quantifying the impact of training lag on model performance.

We selected MLP-PLR, ModernNCA, and TabM as representative methods for MLP architecture, retrieval-based, and ensemble-based methods, respectively, and conducted experiments of splitting strategy on these three method.
Experimental results shown in \cref{fig:split} right infers that this hypothesis holds for all three methods, with a total average improvement of 1.62\%. Among them, the retrieval-based methods ModernNCA shows the highest improvement of 2.19\%, indicating the importance of no-lag candidates for retrieval-based methods in the presence of temporal shifts.

\subsection{Validation Bias}
\label{subsec:bias}

Current deep tabular methods rely on information from the validation set for model selection, as deep learning models are optimized epoch-wise during training. In the context of temporal shifts, this reliance becomes particularly problematic, since there is a
lack of accurate validation of the test data in the training
stage \cite{ganin2015unsupervised, blanchard2021domain}. The time gap between the training and test sets is larger than the gap between the training and validation sets in the previous temporal split, leading to a more significant distribution shift at test time. This makes it more challenging to accurately predict test-time instances compared to validation, thereby introducing \textit{bias into the validation process}.
Therefore we further design the splitting strategy shown in \cref{fig:split} left (c), which shares the training and test sets with (a), but the latter has a more considerable validation bias since the time interval difference between train-val and train-test is much larger.

The performance comparison of split (a) and (c), as shown in \cref{fig:split} right, confirms that validation bias also has a notable impact on performance, especially for ensemble-based methods. The total average improvement is 0.59\%, while the ensemble-based TabM show a more significant improvement of 0.83\%. This is explainable since the ensemble-based methods are robust to the training data quality by reducing the variance, but sensitive to the bias of validation.

\begin{table*}[t]
    \centering
    {\footnotesize{
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{lcccccccccccc}
    \toprule
    \textbf{Splits} & {MLP} & {PLR} & {FT-T} & {SNN} & {DCNv2} & {TabR} & {MNCA} & {TabM} & {XGBoost} & {CatBoost} & {LGBM} & \textbf{Avg. Imp.} \\
    \midrule
    \multicolumn{13}{l}{\textbf{Mean Performance $\uparrow$}} \\
    Random & $+$4.30\% & $+$0.73\% & $+$3.76\% & $+$1.38\% & $+$2.11\% & $+$2.00\% & $+$2.53\% & $+$1.51\% & $+$1.79\% & $+$2.09\% & $+$1.73\% & $+$2.17\% \\
    Ours & $+$3.50\% & $+$0.75\% & $+$2.78\% & $+$1.38\% & $+$3.01\% & $+$2.20\% & $+$2.49\% & $+$1.25\% & $+$2.06\% & $+$2.37\% & $+$2.14\% & \textbf{$+$2.18\%} \\
    \midrule
    \multicolumn{13}{l}{\textbf{Standard Deviation $\downarrow$}} \\
    Random & $+$1.81\% & $+$126\% & $+$0.15\% & $+$44.0\% & $+$0.06\% & $+$224\% & $+$74.9\% & $+$44.3\% & $+$456\% & $+$105\% & $+$616\% & $+$154\% \\
    Ours & $+$29.7\% & $+$50.9\% & $+$5.59\% & $+$16.8\% & $+$8.86\% & $+$82.2\% & $-$10.8\% & $+$37.7\% & $-$15.2\% & $-$30.6\% & $+$8.59\% & \textbf{$+$16.7\%} \\
    \midrule
    \multicolumn{13}{l}{\textbf{Performance Rankings $\downarrow$}} \\
    Random & 8.250 & 5.625 & 5.625 & 10.250 & 9.625 & 8.000 & \underline{4.750} & \textbf{2.750} & \underline{3.125} & \underline{3.125} & 4.875 & -- \\
    Ours & 8.000 & 5.750 & 7.500 & 9.500 & 8.375 & 8.125 & 4.875 & \underline{4.000} & \underline{3.375} & \textbf{2.125} & \underline{4.375} & -- \\
    \bottomrule
    \end{tabular}}}
    \vspace{-10pt}
    \caption{Comparison of performance and stability between the random split in \cref{fig:random} and our proposed temporal split in \cref{subsec:citerion}, measured by the \textit{average percentage change} on the TabReD benchmark, along with the performance ranking of each method. 
    ``PLR,'' ``MNCA,'' and ``LGBM'' denote ``MLP-PLR,'' ``ModernNCA,'' and ``LightGBM,'' respectively. 
    The percentage change represents the difference in the mean (higher is better) or the standard deviation (lower is better, indicating stability) of performance, relative to the baseline temporal split in \citet{rubachev2024tabred}, for each method.
    The results show that our temporal splitting strategy achieves performance comparable to the random split, while offering significantly better stability. The ranking change is minimal, with token-based methods favoring the random split and tree-based methods performing better with our temporal split. The comparison of methods under random and temporal splits is also plotted in \cref{fig:random} and \cref{fig:temporal}. Detailed results are provided in \cref{sec:appendix_result}, with extended rankings.}
    \label{tab:stability}
    \vspace{-12pt}
\end{table*}

\subsection{Bridging the Past and Future in Temporal Split}
\label{subsec:equivalence}

Through the above exploration, we have confirmed the improvements in model performance achieved by reducing the training lag and validation bias. The main question now becomes how to utilize the above insights.

To illustrate how reducing the training lag and validation bias improves model performance, we visualize the loss distribution of the model across different time slices under different validation splitting strategies. \cref{fig:mmd} left shows the loss distribution for the Cooking Time dataset using MLP-PLR model, it clearly shows how reducing training lag and validation bias improves the model performance. Each splits achieve the best performance among the training-available time slices, which allows the model to perform better on instances closer to the test time in the non-lagged split (b) compared to the lagged splits (a) and (c). Furthermore, the higher-biased split (c) works better on training splits but struggles on the test splits, while the lower-biased split (a) achieves a more balanced performance across training and test splits. 
This indicates that a lower-biased validation set brings a more precise direction of the test-time distribution, enabling the model to generalize better.
The above observation again enhances the insights of training lag and validation bias. On a deeper level, reducing the training lag concentrates the model's performance around a time point closer to the test time, while precise validation ensures that the model's performance can effectively generalize from this concentrated time point to the test period.

In addition, another interesting observation in \cref{fig:mmd} left is that the model in split (b) never meet the most former data splits during training and validation, but achieves a relatively well performance on it compared to the test splits. Building on the insight that the validation set is mainly for maintaining generalization performance, this finding inspired us to explore whether data from the opposite temporal direction could be an effective validation set, which is rely on the assumption that the temporal shifts distributed uniformly across each time slices. 
We further use a heatmap to visualize the distribution distance between different time slices using Maximum Mean Discrepancy (MMD) \cite{gretton2006kernel}, as shown in \cref{fig:mmd} right. The MMD heatmap reveals that all datasets exhibit temporal shifts characterized by trends and multiple periodic components. The regular diagonal stripes both indicate the presence of periodicity, and suggest that the sample distributions at identical time intervals are similar, which confirms the empirical uniformity of temporal shifts across time slices.

Finally we design the splitting strategy shown in \cref{fig:split} left (d),  where the validation set is in the opposite temporal direction compared to split (b), meanwhile maintaining the training lag and validation bias. The results in \cref{fig:split} right indicate a performance drop of 0.91\% compared to split (b). However, it is important to highlight that, this performance degradation is noticeably smaller than the improvement observed in (b) relative to (a). This indicates that adopting this alternative splitting strategy to minimize the training gap is always a desirable approach.

In detail, the model performance on this splitting strategy is highly dependent on the dataset, which is primarily due to the nonuniform temporal sampling in some datasets, where maintaining the same amount of validation data compromises the consistency of the time intervals. Further explanation can be found in \cref{sec:appendix_dataset}.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{image/MMD_Explaination_new_new.pdf}
  \vspace{-20pt}
  \caption{Left: Detailed MMD heatmap visualization of the Homesite Insurance dataset, showing the \textit{trend} (lighter colors farther from the diagonal) and yearly/weekly \textit{periodicity} (stripes at different scales) in the data. Mid and Right: The MMD heatmap of the representation learned by MLP \textit{before and after adopting our temporal embedding} on this dataset. Without temporal embedding, the model only learns two patterns (weekdays and weekends) with weak discrimination. After applying temporal embedding, the model's representation aligns with the original data, capturing phase-specific knowledge for each temporal stage (day of the week) and achieving clear distinction.}
  \label{fig:periodic}
  \vspace{-10pt}
\end{figure*}

\subsection{Our Temporal Split}
\label{subsec:citerion}

Based on the above findings, we introduce the following training protocol for temporal tabular data:
\begin{enumerate}[noitemsep,topsep=0pt,leftmargin=*]
  \item The lag between training and test set should be minimized since instances near the test time are more valuable for training, rather than for model selection.
  \item The validation bias should be minimized, which can be achieved by reducing the time interval difference between train-val and train-test.
  \item The equivalence property of the validation set in different temporal direction is maintained for most tasks. 
  An effective validation is available in the opposite temporal direction by aligning the degree of shift in the validation set with the actual shift between training and testing data.
\end{enumerate}
We further propose a more effective temporal splitting strategy that fully leverages this protocol, shown in the bottom of \cref{fig:split} left. 
In this strategy, the training lag is minimized to zero, and the validation set is also aligned with the test set, as it has a similar time interval relative to the training set thus exhibits a similar degree of distribution shift.

The performance and stability comparison between the random split and our newly proposed temporal split is presented in \cref{tab:stability}. The results demonstrate that our splitting strategy achieves a performance improvement comparable to the random split (2.18\% vs. 2.17\% on average) relative to the baseline temporal split in \citet{rubachev2024tabred}. 
Additionally, while our temporal split shows a modest increase in the standard deviation of performance scores (AUC for classification and RMSE for regression) by 16.69\%, the random split results in a much larger increase of 153.81\%. This indicates that, while both methods achieve similar performance gains, our temporal split significantly outperforms the random split in terms of stability. A comparison of stability using the robustness score is provided in \cref{sec:appendix_result}.