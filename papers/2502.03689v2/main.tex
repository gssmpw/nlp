\documentclass{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{comment}
\usepackage{longtable}
\usepackage{booktabs} 
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[accepted]{icml2025}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% \usepackage[space]{xurl}
\usepackage[hidelinks,
            breaklinks=true,
            colorlinks=true,
            linkcolor=blue,
            urlcolor=blue,
            citecolor=blue]{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage[hyphens]{url}
% \usepackage{hyperref}
\usepackage[hyphenbreaks]{breakurl}
% \usepackage{breakurl}
% \PassOptionsToPackage{hyphens}{url}
% \RequirePackage{xurl}  % Replace \RequirePackage{url}
% \urlstyle{same}



\icmlsetsymbol{equal}{*}
\begin{document}

\twocolumn[
\icmltitle{Stop treating `AGI' as the north-star goal of AI research}

\begin{icmlauthorlist}
\icmlauthor{Borhane Blili-Hamelin}{equal,ARVA}
\icmlauthor{Christopher Graziul}{equal,uc}
\icmlauthor{Leif Hancox-Li}{vijil}
\icmlauthor{Hananel Hazan}{tufts}
\icmlauthor{El-Mahdi El-Mhamdi}{ep} 
\icmlauthor{Avijit Ghosh}{hf,uconn}
\icmlauthor{Katherine Heller}{goog}
\icmlauthor{Jacob Metcalf}{ds}
\icmlauthor{Fabricio Murai}{wpi}
\icmlauthor{Eryk Salvaggio}{rit}
\icmlauthor{Andrew Smart}{goog}
\icmlauthor{Todd Snider}{tu}
\icmlauthor{Mariame Tighanimine}{cn}
\icmlauthor{Talia Ringer}{uiuc}
\icmlauthor{Margaret Mitchell}{hf}
\icmlauthor{Shiri Dori-Hacohen}{uconn}
\end{icmlauthorlist}

\icmlaffiliation{ARVA}{AI Risk and Vulnerability Alliance, NY, USA}
\icmlaffiliation{uc}{University of Chicago, IL, USA}
\icmlaffiliation{tufts}{Tufts University, MA, USA}
\icmlaffiliation{vijil}{Vijil, CA, USA}
\icmlaffiliation{goog}{Google, CA, USA}
\icmlaffiliation{wpi}{Worcester Polytechnic Institute, MA, USA}
\icmlaffiliation{ep}{Ecole Polytechnique, France}
\icmlaffiliation{cn}{Conservatoire national des arts et métiers, Lise‐CNRS, France}
\icmlaffiliation{tu}{Eberhard Karls Universit\"at T\"ubingen, T\"ubingen, Germany}
\icmlaffiliation{uiuc}{University of Illinois Urbana-Champaign, IL, USA}
\icmlaffiliation{hf}{Hugging Face, NY, USA}
\icmlaffiliation{uconn}{University of Connecticut, CT, USA}
\icmlaffiliation{ds}{Data \& Society Research Institute, NY, USA}
\icmlaffiliation{rit}{Rochester Institute of Technology, NY, USA}
\vskip 0.2in
\icmlcorrespondingauthor{Borhane Blili-Hamelin}{borhane@avidml.org}
\icmlcorrespondingauthor{Chris Graziul}{graziul@uchicago.edu}
\icmlcorrespondingauthor{Talia Ringer}{tringer@illinois.edu}
\icmlcorrespondingauthor{Shiri Dori-Hacohen}{shiridh@uconn.edu}
\icmlcorrespondingauthor{Margaret Mitchell}{meg@huggingface.co}

\icmlkeywords{Scientific Methodology, Artificial General Intelligence}
]
\vskip 0.2in 

\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{abstract}

The AI research community plays a vital role in shaping the scientific, engineering, and societal goals of AI research. In this position paper, we argue that focusing on the highly contested topic of `artificial general intelligence' (`AGI') undermines our ability to choose effective goals. We identify six key traps---obstacles to productive goal setting---that are aggravated by AGI discourse: Illusion of Consensus, Supercharging Bad Science, Presuming Value-Neutrality, Goal Lottery, Generality Debt, and Normalized Exclusion. To avoid these traps, we argue that the AI research community needs to (1) prioritize \textbf{specificity} in engineering and societal goals, (2) center \textbf{pluralism} about multiple worthwhile approaches to multiple valuable goals, and (3) foster innovation through greater \textbf{inclusion} of disciplines and communities. Therefore, the AI research community needs to \textbf{stop treating ``AGI'' as the north-star goal of AI research}.

\end{abstract}

\section{Introduction}

How can we ensure that AI research goals serve scientific, engineering, and societal needs? What constitutes good science in AI research? Who gets to shape AI research goals? What makes a research goal legitimate or worthwhile? In this position paper, we argue that a widespread emphasis on AGI threatens to undermine the ability of researchers to provide well-motivated answers to these questions. 

Recent advances in large language models (LLMs) have sparked interest in ``achieving human-level `intelligence''' as a ``north-star goal'' of the AI field \citep{morris_levels_2023,mccarthy_proposal_1955}. This goal is often referred to as ``artificial general intelligence'' (``AGI'') \cite{chollet_openai_2024, tibebu_deepseek_2025}. Yet rather than helping the field converge around shared goals, AGI discourse has mired it in controversies. Researchers diverge on what AGI is and assumptions about goals and risks \cite{summerfield_natural_2023, morris_levels_2023, blili-hamelin_unsocial_2024}. Researchers further contest the motivations, incentives, values, and scientific standing of claims about AGI \cite{gebru_tescreal_2024,mitchell_debates_2024,ahmed_field-building_2024,altmeyer_position_2024}. Finally, the building blocks of AGI as a concept---intelligence and generality---are contested in their own right \cite{gould_mismeasure_1981, anderson_situated_2002, hernandez2018paradigms, cave_problem_2020, raji_ai_2021, alexandrova_democratising_2022, blili-hamelin_making_2023, KarenHaoPanel_2023, guest_metatheory_2024, pmlr-v235-paolo24a, mueller2024myth}. 


Building on prior work on the ambiguity between exploratory and confirmatory research in ML \cite{herrmann_position_2024}, unscientific performance claims \cite{altmeyer_position_2024}, SOTA-chasing \cite{raji_ai_2021}, homogenization of research approaches \cite{kleinberg_algorithmic2021, fishman_should_2022, bommasani_picking_2022}, the values embedded in ML research \cite{birhane_values_2022}, and more, our account identifies key obstacles to productive goal setting in AI research---\textbf{traps}\footnote{Our terminology parallels \citet{selbst_fairness_2019} on fairness.}. We argue that the AGI narrative makes it more difficult to overcome each obstacle. To avoid these traps, \textbf{we posit that communities should stop treating AGI as a north-star goal of AI research}. 

An overarching theme in our discussion is the research community's \emph{unique responsibility to help distinguish hype from reality}. The outputs of AI research are deployed as real-world products at a staggering pace, in proliferating contexts, affecting billions of people. This warrants urgent work on trusted, evidence-based answers to questions about the scientific, engineering, and societal merits of AI tools. As argued by the U.N.'s AI Advisory Body, there is ``an overwhelming amount of information... making it difficult to decipher hype from reality. This can fuel confusion, forestall common understanding and advantage major AI companies at the expense of policymakers, civil society and the public,'' \cite{united_nations_governing_2024}. Our position paper addresses this theme: Each of the 6 traps in our account is an obstacle to distinguishing hype from reality. 

A secondary theme in our discussion is the relationship between people and technology. Ultimately, we argue that instead of a single north-star goal, the AI community needs to pursue \emph{multiple specific} scientific, engineering, and societal goals. Narratives about the future of complex technologies are, we argue, prone to blurring hype and reality. If building consensus around an alternative unifying goal proves useful, we propose the goal of \emph{supporting and benefiting human beings}. %This proposal is not intended as a premise or presupposition of our critique of AGI. We instead offer it as a potential path forward. 

In the next section, we examine six `traps' in AI research---obstacles to productive goal setting (\S\ref{sec:traps}). We argue that AGI discourse reinforces and amplifies each problem. Subsequently, we defend three recommendations to avoid the traps (\S\ref{sec:recommendations}): specificity of goals; pluralism of goals and approaches; and more inclusive goal setting. Our concluding rebuttal champions our position against an \emph{alternative view} (\S\ref{sec:alternative}): that AGI should remain the north-star goal of the field. 

Because the contested nature of AGI is a central theme in the present paper, we avoid providing our own definition for the term. Instead, we provide example definitions throughout the present discussion, as well as a table of illustrative definitions (\cref{sec:definition}). 

\section{Traps}
\label{sec:traps}
We examine six key \emph{traps} that hinder the research community's ability to set worthwhile goals. We argue that each is aggravated by AGI narratives. 

Although our discussion strongly focuses on AGI, we believe that our points apply to the broader project of developing trusted, evidence-based approaches to deliberating over the scientific, engineering, and societal goals of AI research. 

The problems we discuss are highly interrelated. For instance, SOTA-chasing, discussed in relationship to the role of misaligned incentives in shaping goal setting (\S\ref{sec:lottery}), also has implications for bad science (\S\ref{sec:pseudoscience}). Similarly, the problem of the lack of consensus on AGI (\S\ref{sec:illusion}), is a theme that recurs throughout. We do not intend the traps to be mutually exclusive. Rather, our goal for each trap is to provide distinct and useful resources for mitigating failure modes in productive goal setting.


\subsection{Illusion of Consensus}
\label{sec:illusion}

%\textbf{Illusion of Consensus Trap} 
\emph{Using shared term(s) in a way that gives a false impression of consensus about goals, despite goals being contested.}


The increasing popular use of the term ``AGI'' \cite{reuters2025trumpai, venturebeat2023agi, ibm2023agi} creates a sense of familiarity, giving the illusion that there is a shared understanding on what AGI is, and broad agreement on research goals in AGI development. However, there are vastly different opinions on what the term AGI refers to, what an AGI research agenda looks like, and what the goals in AGI development are.  Left unchecked, this illusion obstructs explicit engagement on what the goals of AI research are and should be.

From popular discourse to research papers to corporate marketing materials, the vast majority of references to AGI fall into this trap when they uncritically cite claims about so-called AGI. For examples of uncritical media claims, see \citet{venturebeat2023agi, ibm2023agi}; see \citet{altmeyer_position_2024} for examples of overhyped research.\footnote{Some researchers who advocate for AGI as a goal have avoided the Illusion of Consensus trap. \citet{morris_levels_2023} explicitly call for investigating disagreements about goals, predictions, and risks that underpin prominent accounts of AGI.} \citet{summerfield_natural_2023} summarizes the issue: ``AI researchers hope to discover how to build AGI. The problem is that nobody really knows exactly what an AGI would look like.'' \citet{mueller2024myth}, in turn, calls AGI ``a meaningless concept, an emperor with no clothes.'' \citet{blili-hamelin_unsocial_2024} identify multiple types of disagreements among attempts to define AGI or human-level AI. The contested nature of AGI as a goal becomes even more acute in critiques of AGI concepts \citep[see, e.g.,][]{altmeyer_position_2024,mueller2024myth,van2024reclaiming}.

Beyond AGI, AI research is rife with topics that involve disagreement about goals, values, and concepts. For example, \citet{mulligan_privacy_2016} argue that \emph{privacy} should be understood as an ``essentially contested concept.'' They argue that lack of agreement about the meaning and significance of privacy is not merely a matter of confusion---rather, disagreement and contestation are desirable features that enable privacy to adapt to changing technical and social contexts. Similarly, there is now widespread understanding that \emph{fairness} should be understood as a contested topic, not only admitting incompatible mathematical formalizations but also incompatible values, worldviews, and theoretical assumptions \cite{friedler_impossibility_2021, jacobs_measurement_2021}. 

In suggesting this trap, we do not presume that contested topics are inherently problematic. Rather, we argue that when dealing with the important question of the goals of AI research, the significant disagreements that surround AGI should be embraced as signals of conflicting values. 


\subsection{Supercharging Bad Science}
\label{sec:pseudoscience}

%\todo{Bring back a bit of anthropomorphism in the section, drawing on appendix material.}

%\textbf{Bad Science Trap} 

\emph{Poorly defined concepts and experimental procedures deployed in pursuit of AGI worsen current problems with bad science in AI.}

Research that produces reliable empirical knowledge about AI is vital to public interest decisions about AI's potential for societal and environmental benefit and harm. Yet, many experts have noted a pervasive lack of scientific grounding in AI research \cite{united_nations_governing_2024, sloane_silicon_2022, narayanan_ai_2024, raji_fallacy_2022, widder_watching_2024, suchman_uncontroversial_2023, hullman_worst, van2024reclaiming, guest_metatheory_2024}. We argue that vagueness in AGI discourse exacerbates existing problems with the scientific validity of AI research.

\textbf{Problem 1: Underspecification and external validity.} One problem with the pursuit of AGI as a concrete goal is \textbf{underspecification}, where \emph{lack of specificity in goals or concepts leads to cascading epistemic problems}, including irrefutability, lack of external validity, flawed experimental design, and flawed evaluation. These common problems in AI research are worsened in the AGI context by the lack of scientifically grounded definitions of AGI (\S\ref{sec:illusion}). 

Underspecification of learning goals also undermines \emph{external validity}---the question of whether a measurement corresponds to the real-world phenomenon it's supposed to capture. A good example is the debate about whether ``language understanding'' benchmarks actually measure language understanding\cite{jacobs_measurement_2021, liao_are_2021}.


External validity is also relevant when researchers equate human faculties with model proxies~\cite{hullman_worst}, such as claiming that a model ``capable of linking specific objects with more general visual context'' is evidence of ``imagination''~\cite{fei_towards_2022}. This rhetorical move is enabled by using colloquial terms like ``imagination'' without considering whether it corresponds to the human faculty. 
\citet{altmeyer_position_2024} likewise critiques \citet{gurnee2024languagemodelsrepresentspace}
for inflated claims enabled by the vagueness of the term ``world model.'' Underspecified goals trickle down into many areas of experimental design, such as learning pipelines, evaluation metrics, tasks, representations, and methods.% (see \cref{underspecification}).

External validity is also undermined when researchers claim to measure concepts from other fields, like intelligence.
The fields of psychology, neuroscience, and cognitive science have studied human intelligence for generations, yet even they lack consensus on what ``intelligence'' is \cite{KarenHaoPanel_2023,gopnik_ais_2019}. Conversely, AI research is no longer concerned with modeling human cognition \cite{van2024reclaiming, guest_metatheory_2024}. Instead, AI developers define ``intelligence'' on their own terms, privileging definitions convenient for benchmarking or selling products (\S\ref{sec:lottery}), while benefiting from historically positive connotations of the term ``intelligence''. 
 
\textbf{Problem 2: Ambiguity between science and engineering.} Another problem with the pursuit of AGI is \emph{confusion between science and engineering} \cite{agre2014toward, hutchinson_evalgaps, altmeyer_position_2024}. As \citet{hullman_worst} point out, a ``typical supervised ML paper'' (one that shows some accuracy metrics on a benchmark) is often just an ``engineering artifact'', a tool attached to performance claims that cannot be refuted because of replication challenges. \citet{altmeyer_position_2024} argue that this ambiguity between science and engineering means rigorous hypothesis testing with ``specific conditions and considering effect sizes'' is often omitted, with results often presented as ``engineering achievements'' without specifying \emph{precisely} what is being tested, relevant hypotheses, and what effect sizes would constitute a null hypothesis. 

This ambiguity invites experimenter and confirmation biases, since researchers are incentivized to ``pay little or no attention to competing hypotheses or explanations'' or ``[fail] to articulate a sufficiently strong null hypothesis,''~\cite{altmeyer_position_2024}. Confusion between science and engineering also manifests when it is unclear if a study is pursuing scientific goals (of explanation, hypothesis confirmation, etc.) or goals of specific engineering applications (e.g., a proof-of-concept) \cite{hutchinson_evalgaps}. This exacerbates questions about external validity: without clear and specific experimental goals, it is easier to provide post-hoc interpretations of experiments that ``support'' a wide variety of goals.

\textbf{Problem 3: Ambiguity between confirmatory and exploratory research.} The ambiguity between engineering and scientific methodology is related to another problem: \emph{confusion between confirmatory and exploratory research} \cite{pmlr-v97-bouthillier19a, herrmann_position_2024}. \citet{herrmann_position_2024} state that confirmatory research ``aims to test preexisting hypotheses to confirm or refute existing theories [while] exploratory research is an open-ended approach that aims to gain insight and understanding in a new or unexplored area.'' They go on to argue that ``most current empirical machine learning research is fashioned as confirmatory research while it should rather be considered exploratory'' and that experiments are ``set up to \emph{confirm} the (implicit) hypothesis that the proposed method constitutes an improvement'' (emphasis theirs). By implicitly conflating exploratory analysis with confirmatory research, ``exploratory findings have a slippery way of `transforming' into planned findings as the research process progresses'' \cite{calin2019new}. Using the vague and contested concept of AGI to frame confirmatory claims worsens this problem, as it makes it harder to figure out \emph{what} is being claimed.% and the extent to which results are affected by experimenter and confirmation biases.

\subsection{Presuming Value-Neutrality}
\label{sec:valueladen}

\emph{Framing goals as purely technical or scientific in nature when political, social, or ethical considerations are implied and/or essential.}

Presuming Value-Neutrality occurs when technical or scientific goals become disconnected from their \textbf{value-laden} assumptions: aspects of AI research that are---and should be---informed by political, social, and ethical considerations. The AI research community has recently begun examining these value-laden assumptions \cite{pmlr-v235-zhao24a, fishman_should_2022, dotan_value-laden_2020, birhane_values_2022, scheuerman2021datasets, hutchinson_evalgaps, bommasani_evaluation_2022, denton_bringing_2020, denton2021genealogy, mathur_disordering_2022, shilton_values_2018, broussard_artificial_2019, green_data_2021, blodgett_language_2020, viljoen_relational_2021, abebe_roles_2020, birhane_towards_2021, blili-hamelin_making_2023, blili-hamelin_unsocial_2024, costanza-chock_design_2020}. 

When efforts to define AGI and related concepts do not explicitly examine the societal goals and values embedded in their definitions, they fall into the Presuming Value-Neutrality trap. Examples include proposals for ``universal intelligence'' \cite{legg_universal_2007, HERNANDEZORALLO201450}.  

The pursuit of value-neutral approaches echoes debates about psychometric views of human intelligence. Intelligence, like ``health,'' and ``well-being,'' inherently carries normative assumptions about which behaviors or abilities are desirable \citep{anderson_situated_2002,alexandrova_democratising_2022}. Researchers fall into the Presuming Value-Neutrality trap by sidestepping these value-laden dimensions \cite{anderson_situated_2002, cave_problem_2020, blili-hamelin_making_2023}. %For instance, 
\citet{warne_spearmans_2019} exemplify this by advocating for purely statistical definitions of intelligence, precisely because cultural definitions vary. 

Value-laden assumptions within concepts like AGI drive legitimate disagreement about their meaning, reflecting divergent societal goals \cite{blili-hamelin_unsocial_2024}. This makes consensus on AGI challenging, as it requires alignment on political, social, and ethical priorities. Similar disagreements affect related concepts like AI\cite{cave_problem_2020, blili-hamelin_making_2023}, ``human-level AI'', ``superintelligence'', and ``strong AI'', reinforcing the Illusion of Consensus trap (\S\ref{sec:illusion}). 


\subsection{Goal Lottery}
\label{sec:lottery}

\emph{This trap occurs when incentives, circumstances, or luck drive the adoption of goals inadequately supported by scientific, engineering, or societal merit.}

%

Researchers have studied the role of socioeconomic factors, trends, and circumstantial factors in shaping AI research. For instance, \citet{hooker_hardware_2021} has argued that a form of hardware lottery---the greater availability of hardware with strengths in parallel processing---was key to the resurgence of deep learning in the 2010s.\footnote{On similar lottery or path dependence effects, see \cite{dehghani_benchmark_2021, fishman_should_2022, hooker_diminishing_2024, liebowitz_path_1995, peacock_path_2009, bauer_mirror_2024, rossbach_innocent_2023}.} Similarly, researchers have examined the role of incentives, socioeconomic factors, and hype cycles in AI research~\cite{narayanan_ai_2024,wang_against_2024,hicks_chatgpt_2024,gebru_tescreal_2024,sartori_minding_2023,delgado_participatory_2023,raji_fallacy_2022,widder_dislocated_2023}. With this trap, we focus on cases where lotteries (luck) or incentives drive the adoption of unjustified goals---goals that are inadequately supported by scientific, engineering, or societal merit. 

Consider AGI definitions centered on economic value, like OpenAI's emphasis on ``outperform[ing] humans at most economically valuable work'' \cite{openai_2018}. The primacy of economic value for setting AI research goals is contentious from both engineering and societal perspectives. Such definitions create misalignment between incentives and justifications by reducing complex societal, engineering, and scientific considerations to purely economic metrics.

Another example is benchmark SOTA-chasing---pursuing top scores on popular benchmarks  \cite{bender_dangers_2021, church_emerging_2022, hullman_worst, raji_ai_2021}. Despite strong professional incentives encouraging this practice, it lacks engineering, scientific, and societal justification. Benchmarks poorly reflect model performance in real application contexts because of problems like data leakage, overfitting to benchmarks, and data heterogeneity \cite{balloccu-etal-2024-leak, xu2024benchmarkingbenchmarkleakagelarge, zhang2024careful, el2022impossible, hanneke2022no, el2021collaborative}. In short, the measurement method lacks \emph{external validity}. Yet the practice persists due to reputational and financial rewards, demonstrating misalignment between incentivized goals and their actual merits. 

The dynamics of goal lotteries are also visible in the story of the multi-decade neglect of deep learning architectures. In this case, a research agenda was sidelined for reasons that eventually proved to be misguided from an engineering, scientific, or societal perspective (e.g., due to ``gatekeeping'' effects against less popular research agendas; see \citealt{SilerEtAl2014}). Meanwhile, the AI industry went all in on the expert systems ``bubble'' \cite{haigh-bust}. \emph{Reductions in diversity within} contemporary AI research can be a sign that similar mistakes are at play (\S\ref{sec:exclusion}). 
%Following \citet{klinger2022narrowingairesearch}, we call the kind of pattern illustrated by the multi-decade neglect of deep-learning, and the current neglect of non-deep-learning approaches \emph{technological homogenization}.
Some recent initiatives to counter homogenization \cite{chollet_arc_technical_report_2024} rely on operationalizing AGI through benchmarks.\footnote{Chollet proposes that ``We will have AGI when creating [benchmarks `that are easy for humans, yet impossible for AI'] becomes outright impossible'' \cite{chollet_so_2024}.} In practice, they end up as yet another benchmark: incentivizing SOTA-chasing, supercharged by intense media and marketing attention \cite{jones_how_2025}. For this reason, we remain somewhat skeptical of whether approaches like ARC \cite{chollet_measure_2019} outweigh the negative consequences of news-cycle-accelerated SOTA-chasing.

\subsection{Generality Debt} % 
\label{sec:generality}

%\textbf{Generality Debt Trap} 
\emph{Relying on the generality or flexibility of tools to postpone crucial engineering, scientific, or societal decisions.}

AGI definitions differ on how much ``generality'' is desirable \cite{blili-hamelin_unsocial_2024}. This indicates a lack of clarity and consensus about the goals of AI research, forming a trap that (a) encourages suboptimal science/engineering practices (related to points made in \ref{sec:pseudoscience}); (b) suppresses important social/ethical questions about which research directions are worth pursuing. We term this trap ``Generality Debt'' to parallel technical debt \cite{sculley2014machine}: it delays the work that needs to be done as part of AI research which, if left undone, takes more work to address in the future.

This trap includes the appeal to many different notions of generality at play in machine learning: (1) variety of tasks \cite{hernandez2018paradigms}; (2) capability to be trained for ``any task'' vs.\ ability to perform many predefined tasks \cite{hernandez2018paradigms}; (3) whether the task or data distribution the model is being evaluated on is ``seen'' or ``unseen'' (i.e., available, or not, to the model during its training phase) \cite{altmeyer_position_2024}; (4) variety of data in model input/output, such as structured vs unstructured, modality, etc.; (5) whether the performance of the model reflects ``performance considered `surprising' to humans'' \cite{altmeyer_position_2024}; (6) variety of goals; (7) ability to ``accept a general language for the problem statement'' \cite{newell1965search}; and (8) having a ``general'' internal representation \cite{newell1965search, mccarthy1981some}.

As \citet{pmlr-v235-paolo24a} note, despite the multiple possible meanings of ``generality'', most papers do not define generality even if it's central to their argument. Without formal definition, assessing or improving generalization becomes challenging. Assuming that ``generalization'' is desirable while acknowledging its poor definition is misguided. We should first define specific, measurable properties before arguing that they are desirable.

\textbf{Without proper definition, the value of generality remains unclear.} Different types of generality support different future visions, raising unexplored questions about their relative importance. Vague definitions of generality lead to bad science and engineering (\S\ref{sec:pseudoscience}). For example, \citet{altmeyer_position_2024} note how the pursuit of generality has led to vague task specifications. In parallel, \citet{gebru_tescreal_2024} argue that some conceptions of AGI contravene good engineering practices: it is hard to test the functionality of systems under ``standard operating conditions'' if the system is advertised as a ``universal algorithm for learning and acting in any environment''.

Aiming to achieve certain forms of generality could also mean making a tradeoff with ecological validity, as argued by \citet{saxon2024benchmarksmicroscopesmodelmetrology}. They argue that ``holistic'' benchmarks tend to be in practice a collection of disparate specific benchmark tasks, meaning that they have task-level construct validity. However, these tasks do not always match with \emph{user-relevant capabilities}. 

Finally, the vagueness around ``generality'' is also an ethical concern, as it sidesteps normative questions about \emph{which types of generality merit pursuit} and obscures implicit decisions about how to prioritize different research directions. 

\subsection{Normalized Exclusion}
\label{sec:exclusion}

\emph{Excluding communities and experts from shaping the goals of AI research limits development of innovative  and impactful ideas.}

The negative consequences of exclusion in AI have been extensively discussed, both in terms of how it affects product quality and model performance~\citep[e.g.,][]{obermeyer_dissecting_2019}, and also how it harms people~\citep[e.g.,][]{buolamwini_gender_2018,shelby_sociotechnical_2023,whitney_real_2024}. We argue that AGI discourse aggravates problems of exclusion. 

\textbf{Problem 1: Excluding communities.} Many communities are left out of meaningful participation in shaping the goals of AI research \cite{delgado_participatory_2023}. Excluding communities causes serious harm, especially to minoritized communities~\cite{pierregetting_2021}. It also undermines the utility of end products, reduces model performance, and impedes innovation~\citep[e.g.,][]{burt_structural_2004}. For instance, the infamous case of facial recognition engines---such as those used by Google, Apple, and Meta---mistaking Black people for gorillas \cite{bbcGoogleApologises} is still occurring more than 8 years after the problem was first identified \cite{nytimesGooglesPhoto,racismandtechnologyRacistTechnology}, with downstream impacts on surveillance and law enforcement \cite{jones2020law,pour2023police}. Similarly, selective forms of inclusion in data annotation raise ethical and practical concerns about downstream effects~\cite{wang_whose_2022,bertelsen_data_2024}. For other examples of exclusion or inclusion of communities impacting performance, see \cite{buolamwini_gender_2018, young_beyond_2019, raji_saving_2020, andrews_reanimation_2024, salavati_reducing_2024, bergman_stela_2024, weidinger_star_2024}). Excluding communities from meaningful feedback can also undermine societal goals, such as fostering collective legitimacy through accountability to impacted communities~\cite{birhane_power_2022,young_participation_2024, costanza-chock_design_2020,mikesell_ethical_2013,schulz_addressing_2002}. 

The recent prominence of AGI discourse intensifies the existing problem of community exclusion in AI research~\cite{cnn2024muskai, frank2017machines}. Many proponents of AGI envision a future where AI systems perform an extraordinary range of tasks for countless communities. However, research and design processes fall short of the inclusiveness demanded by this ambitious vision. For example, December 2024 reporting suggests that OpenAI and Microsoft ``signed an agreement last year stating OpenAI has only achieved AGI when it develops AI systems that can generate at least \$100 billion in profits'' \cite{zeff2024microsoft,OpenAIStructure}, a stark departure from OpenAI's public definition of AGI~\cite{openai_2018}. As several authors argue, economic value is not the only type of desirable value~\cite{morris_levels_2023, pierson_using_2025, dulka_use_2022, harrigian_characterization_2023, agrawal_large_2022}. The economic definition helps guide the engineering decisions of OpenAI. But it is questionable whether an emphasis on profits will lead to the most beneficial or useful end products or to meaningful consensus about goals, especially for minoritized groups. 

\textbf{Problem 2: Excluding disciplines.} From application domains (e.g. medicine~\cite{obermeyer_dissecting_2019}, finance~\cite{cao_finance2022}, cybersecurity~\cite{salem_advancing_2024}, learning \cite{leong_testing_2024})to the practices involved in building AI---data annotation, qualitative and quantitative methods, domain expertise, computer science, and many more \cite{wang_whose_2022,bertelsen_data_2024,widder_epistemic_2024}---AI research crosses disciplinary boundaries. The cross-disciplinary challenges of AI research mirror those of other disciplines \cite{stirling2014disciplinary,amoo2020breaking,stokols2003evaluating,vestal2020interdisciplinarity,royal2024science}. 

One such major challenge is disciplinary silos, where knowledge is inadequately shared across disciplines~\cite{ballantyne_epistemic_2019,dipaolo_whats_2022,royal2024science, stirling2014disciplinary, amoo2020breaking, stokols2003evaluating}. For instance, lack of knowledge sharing could be partly responsible for low attention to the distinction between explanatory and exploratory research in ML, discussed in \S\ref{sec:pseudoscience} \cite{herrmann_position_2024}. 

Another challenge is epistemic hierarchies---where the expertise of some disciplines is explicitly or implicitly devalued \cite{knorr_cetina_epistemic_1999,knorr_cetina_culture_2007, fourcade_superiority_2015, simonton_psychologys_2004}. This can manifest as expert groups being limited to narrow input rather than contributing to broader research design decisions \cite{bertelsen_data_2024}. 

AI researchers' focus on applying their work to other domains creates another major challenge. Insufficient domain knowledge might affect the functionality of AI tools---whether they operate as advertised \cite{raji_fallacy_2022}. For instance, AI tools are deployed to make predictions about future outcomes that concern human individuals, from pre-trial risk prediction and predictive policing to automated employment decisions~\cite{wang_against_2024}. Yet inadequate evidence of effectiveness often fails to prevent predictive tools from being built, marketed, and deployed~\cite{connealy_staggered_2024,doucette_impact_2021,cameron_us_2023}.

The problem of disciplinary silos becomes particularly acute in AGI-oriented research due to two factors: its claims to be creating cognates or replacements of human intelligence, and its claims to expertise in many disciplines. In the former case, claims are often made while ignoring debates in cognitive science and psychology about the nature of intelligence~\cite{mitchell_debates_2024,guest_metatheory_2024,van2024reclaiming,summerfield_natural_2023}. In the latter case, achieving AGI is often framed in terms of being able to ``replace'' domain experts in various domains---which are then often taken up uncritically by the media without input from the domain experts themselves~\citep[see, e.g.,][]{henshall_when_2024}.

\textbf{Problem 3: Resource disparities.} In recent years, we have witnessed an unprecedented growth in computational resources required for training machine learning models, with requirements doubling approximately every few months~\cite{Sevilla_2022}. This trend compounds existing resource disparities, as state-of-the-art AI research often relies on access to computational resources accessible to very few researchers~\cite{yu_ai_2023,laforge_dangers_2024}. The financial cost of these resources excludes a wide range of actors from contributing to AI research. Even top research universities have a fraction of the computational resources that many corporations use to advance AI research. Efforts are underway to address this resource disparity by supporting access to large-scale computational resources maintained by government entities (e.g., NAIRR in the United States). Yet, resource parity is an aspirational goal in response to widespread recognition that AI researchers in industry enjoy a \textit{de facto} advantage in setting the goals of AI research due to their access to industrial scale computational resources. This structural advantage is reinforced by the use of pre-print archives to publicize AI research without peer review \cite{devlin2019bertpretrainingdeepbidirectional, vaswani2023attentionneed, rombach2022highresolutionimagesynthesislatent, openai2024gpt4technicalreport, bubeck2023sparksartificialgeneralintelligence}, a strategy which legitimizes this work as scientific in nature without applying traditional standards for scientific integrity \cite{tenopir_trustworthiness_2016,lin_how_2020, soderberg_credibility_2020,kwon_use_2025, rastogi_arxiv_2022}.

While resource disparities exist for all forms of AI research, they are particularly stark when AGI is taken as a north-star goal for the discipline, due to the orientation of current AGI efforts towards sheer computational scale and the concentration of such efforts in large tech companies.\footnote{Large-scale efforts also have detrimental impacts on climate, reinforcing resource disparities further \cite{kaack2022aligning,bucknall_current_2022,luccioni2024light}.} Concentration of power makes it even more important that those efforts include, rather than exclude, relevant communities and experts. AGI discourse accelerates the existing trend in AI of discounting domain expertise and lived experiences in favor of models that are allegedly experts in everything. 

\section{Recommendations}\label{sec:recommendations}
We have argued that AGI discourse hinders setting well-motivated science and engineering goals in AI development, while being destructive to the development of AI that has social merit. We now provide three recommendations for avoiding these traps. 


\textbf{Recommendation 1: Goal Specificity.}
\emph{The AI community must prioritize \textbf{highly specific} language when discussing the scientific, engineering, and societal goals of AI.}

More specific definitions of tangible scientific, engineering, and societal goals promote a shared understanding of these goals, and thus, the capacity to evaluate whether these goals are well-motivated. Without such specificity, researchers, practitioners, and others can develop divergent understandings of a goal and how it should be achieved. This divergence enables conceptual arbitrage on the part of AI researchers and practitioners who seek to advance their own goals, since these actors ultimately determine the details of model development and implementation. People external to tech development may then be left assuming that a system achieves one goal when, in fact, it does not.

This recommendation addresses the Illusion of Consensus trap (\S\ref{sec:illusion}), promoting conceptual clarity as an essential part of goal-setting. Clarity also helps to avoid the Goal Lottery trap (\S\ref{sec:lottery}) by making goal selection explicit. It similarly addresses the Presuming Value-Neutrality Trap (\S\ref{sec:valueladen}) by explicitly surfacing values tied to specific goals, and directly reduces Generality Debt (\S\ref{sec:generality}) . Finally, goal specificity addresses the underspecification issues highlighted in the Supercharging Bad Science Trap (\S\ref{sec:pseudoscience}).


\textbf{Recommendation 2: Pluralism of goals and approaches.}
\emph{Rather than a single general north-star goal (or small set of goals), the AI community should articulate \textbf{many} worthwhile scientific, engineering, and societal goals---and many possible paths to fulfilling them.}

Reaching meaningful scientific and societal consensus on the goals of a field as broad-ranging as AI is challenging. When consensus may not be viable or desirable, we recommend pluralism: allowing multiple viable conceptions of the goals of AI research. Pluralism is healthy in a society composed of individuals and institutions with divergent values. By default, the research community should be pluralistic about goals and paths to achieving them, aiming for heterogeneity instead of homogeneity~\cite{sorensen_value_2024,sorensen_roadmap_2024}.\footnote{We're not rejecting consensus on unifying, general north-star goals as a matter of principle. In some circumstances, like coordinating collective action in response to the climate crisis, strong consensus may become crucial. But the research community should not begin from the assumption that such consensus is necessary, or that consensus is optimal from a scientific or societal perspective.}

In practice, pluralism can manifest in how resources are distributed among different research approaches. For example, rather than investing most computational resources in pursuit of AGI, they could be more evenly distributed among diverse goals and approaches within AI. Researchers who study the dynamics of knowledge production and problem-solving in groups have found pluralism to be beneficial \cite{hong_groups_2004,muldoon_diversity_2013}, including unique benefits ascribable to egalitarian group dynamics~\cite{xu_flat_2022}. Similarly, researchers have found that methodological diversity is beneficial to advancing knowledge \cite{midgley_methodological_2000,veit_model_2020, zhu_paradigm_2022}.

Pluralism addresses the Illusion of Consensus trap (\S\ref{sec:illusion}) by acknowledging the lack of consensus, and using diversity of perspectives as a tool for scientific and social progress; the Goal Lottery trap (\S\ref{sec:lottery}) by reducing the chances of arbitrarily or prematurely excluding some goals from consideration; and the Exclusion trap (\S\ref{sec:exclusion}) by encouraging a plurality of goals and approaches.

\textbf{Recommendation 3: Greater Inclusion in Goal Setting.}
\emph{Greater inclusion of communities and disciplines in shaping the goals of AI research is beneficial to innovation.}

Inclusion supports innovation~\cite{zhang_relationship_2021,xu_flat_2022,burt_structural_2004,hewlett_how_2013}. Identifying worthwhile goals, related use cases, and potential unintended consequences depends on engaging diverse viewpoints. Within AI research, these viewpoints must include those of end users, experts from other fields, those affected by research outcomes, and data annotators. Excluding any of these groups impoverishes the potential of AI to achieve worthwhile goals since it would discount the perspectives that define these goals as worthwhile. Including them enriches AI research. 

Cross-pollination of ideas between disciplines leads to more impactful research \citep{shi_surprising_2023,dorihacohen2021fairness}. Such impact requires that we abandon silos of (tacit) knowledge (e.g., epistemic cultures:  \citealp{knorr_cetina_epistemic_1999,knorr_cetina_culture_2007}) and prioritize epistemic hierarchies that value non-computational research \citep{fourcade_superiority_2015,simonton_psychologys_2004}. While technical complexity can make participation by non-experts challenging~\cite{pierregetting_2021}, working through these challenges can surface issues experts have not anticipated \citep{cooper_systematic_2022} and enable practical scientific contributions by integrating the insights and experiences of non-experts, or experts in other fields, into system design decisions \citep{delgado_participatory_2023,salavati_reducing_2024}.

As a topic, AGI often involves imagining AI technologies that impact the lives of everyone. Exclusion thus causes socially significant disagreements regarding the goals, processes, and actors who shape AI research and deployment. These disagreements are often overlooked or ignored by those with the power to shape the field. Inclusion is necessary to ensure that decisions about technology are sufficiently justified to institutions, communities, and individuals \cite{anderson_epistemology_2006, binns_algorithmic_2018, alexandrova_democratising_2022, birhane_power_2022, lazar_power_2022, ovadya_reimagining_2023}.

This recommendation addresses the Normalized Exclusion trap (\S\ref{sec:exclusion}) by treating inclusion as essential to innovation. Moreover, it addresses the Illusion of Consensus and the Presuming Value-Neutrality traps (\S\ref{sec:illusion},\S\ref{sec:valueladen}) by acknowledging socially significant disagreements about the value-laden goals of AI research and development and using those disagreements productively.


\section{Conclusion}
\label{sec:alternative}
We have argued that AGI is a poor choice as a north-star to guide AI research. We conclude by championing our position against a strong alternative view: that the traps we have identified can be addressed through a modified pursuit of AGI. We argue that improved approaches to AGI would not go far enough. 

\subsection{Alternative View}
``\emph{AGI is a good north-star goal; to avoid the above traps, improved definitions and accounts of AGI are needed}.''

Thoughtful attempts to address shortcomings in accounts of AGI indeed exist \cite{adams2012mapping, chollet_measure_2019, morris_levels_2023, summerfield_natural_2023}. If prior accounts of AGI are counterproductive or flawed, why not pursue new accounts that address those flaws? 

As an example of the alternative view, \citet{morris_levels_2023} arguably mitigate the Illusion of Consensus trap by disentangling the disagreements about goals, predictions, and risks that plague prior accounts of AGI. Moreover, their proposal for a practical strategy analogous to Levels of Driving Automation standards \cite{sae_international_j3016_202104_2021} could be viewed as mitigating the Presuming Value-Neutrality trap. Setting society-wide standards could be done in a way that explicitly centers \emph{specific} risks that the standards address \cite{morris_levels_2023}. In this way, the values being favored manifest in the risks that are centered by the standards.

Works like \citet{morris_levels_2023} showcase a potential response to traps. In arguing that AGI discourse aggravates multiple standing problems, we cannot rule out the possibility of efforts that mitigate these same problems while retaining AGI as a goal. Moreover, given that lack of agreement about how to define AGI is likely to persist, as \citet{morris_levels_2023, summerfield_natural_2023} and many others do, it would be especially implausible for us to presume to have a complete enough view of the landscape of possible conceptions of AGI to draw definitive conclusions. 

\subsection{Rebuttal}
%\textbf{Rebuttal:} Why favor our position against this alternative? 
Why favor our position against this alternative? 

\textbf{Reason 1: Conflict with our recommendations.} First, we believe that specificity of goals and pluralism of goals and approaches are at odds with this alternative view. Although the definition of AGI is highly contested, a frequent motivation for embracing AGI as a north-star goal is the desire for a single, large-scale, unifying vision for the field \cite{summerfield_natural_2023}. 

\textbf{Reason 2: Distinguishing hype from reality.} Another reason to favor our position is the research community's responsibility to help distinguish hype from reality. We believe that the AI research community must  find a path to providing trusted, evidence-based answers to increasingly complex questions about AI technologies, their goals, and their impacts. In the current moment, the hyped terminology of AGI tends to undermine this goal. 

No matter how well or poorly defined, AGI has acquired a cultural significance that exacerbates the challenge of distinguishing hype from reality. ``Intelligence'' and ``generality'' hold the promise of being beneficial for countless needs and contexts (\S\ref{sec:valueladen},\S\ref{sec:generality}). No matter how cautious the research community attempts to be, the cultural associations of these terms risk stoking the flames of unscientific thinking about AI. This enables various interested parties to loosely project utopian or dystopian characteristics onto AGI in ways that support their calls for more power and resources. 

\textbf{Reason 3: Benefiting humans as the goal of technology.} We worry that large-scale stories about unifying goals for AI are prone to blurring the line between hype and reality. If the AI community nevertheless wants an overarching goal to strive towards, the goal should be the support and benefit of human beings. The goals of technology are shaped by \emph{people}. Evidence-based approaches to examining whether technology effectively meets the needs of people---be they ``users'', ``consumers'', ``patients'', ``scholars'', or a myriad of business and social monikers---are well-established. In a quest to achieve AGI, communities often lose sight of the needs of people as a goal, in favor of focusing on just the technology.

There is another, more ambitious reason to work towards consensus on supporting and benefiting human beings as a goal. We have noted the role of socially significant disagreements about the goals of technology in our third recommendation of inclusion. Processes ensuring that technology benefits humans have the potential to provide \emph{collectively legitimate} responses to such disagreements. This could occur through processes that embrace democratic ideals: such as universal inclusion in interrogation, deliberation, and dissent about the ``common good'' and ``public interest'', while enacting strong accountability to participants as ``rights-holders'' \cite{anderson_epistemology_2006, putnam_reconsideration_2011, binns_algorithmic_2018, gabriel_artificial_2020, birhane_power_2022, lazar_ai_2023, ovadya_reimagining_2023, blili-hamelin_unsocial_2024}. Aiming for collective legitimacy amounts to requiring politically and socially effective forms of consensus. 

In sum, we urge communities to \textbf{stop treating ``AGI'' as the north-star goal of AI research}.

\section{Acknowledgments}
Borhane Blili-Hamelin was funded in part through a Magic Grant from the Brown Institute for Media Innovation. Chris Graziul was supported by the National Institute of Minority Health and Health Disparities of the National Institutes of Health under award number R01MD015064. This material is based upon work supported in part by the NSF Program on Fairness in AI in Collaboration with Amazon under Award IIS-2147305. Any opinion, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, National Institutes of Health, the Brown Institute for Media Innovation, or Amazon.

\nocite{zhi-xuan_beyond_2024, jain_algorithmic_2024, lazar_agi_2024, goertzel_artificial_2014, lazar_ai_2023, birhane_towards_2021, legg_collection_2007, legg_machine_2008, smart_beyond_2015}

\sloppy
\bibliographystyle{icml2025}
\bibliography{references}


\newpage
\appendix


\section{Definitions of AGI and related concepts}
\label{sec:definition}

\textbf{Table 1} below presents illustrative definitions of AGI and usefully related concepts. We agree with \citet{morris_levels_2023}'s proposal to broaden discussions of AGI definitions to include accounts that avoid the term ``AGI'' yet address similar goals of achieving human-level intelligence. For example, while OpenAI's influential definition \cite{openai_2018} focuses on outperforming humans at economically valuable work, sharing key parallels with \citet{nilsson_human-level_2005}. Yet it notably differs from \citet{chollet_arc_technical_report_2024, summerfield_natural_2023, morris_levels_2023, chollet_measure_2019, goertzel_artificial_2014} and others by not explicitly emphasizing generality. 

Following \citet{blili-hamelin_unsocial_2024}, we believe discussions of AGI definition should include approaches that challenge AGI's central premises. Below, we include \citet{weizenbaum_computer_1976} and \citet{attard-frost_queering_2023}. Including these critical accounts enables noticing a surprising similarity with \citet{summerfield_natural_2023}'s reconceptualization of AGI through the lens of natural intelligence: all three accounts favor a strong form of contextualism and pluralism about what intelligence means.


\onecolumn
\begin{center}
% \begin{table*}[ht]
%     \centering
    \small
    \begin{longtable}{p{0.99\linewidth}}
    \toprule
    \multicolumn{1}{c}{\textbf{Sample of proposed definitions of AGI and related concepts---human-level AI, strong AI, artificial superintelligence (ASI), etc.}} \\ \midrule
    
\citet{weizenbaum_computer_1976}. ``Intelligence is a meaningless concept in and of itself. It requires a frame of reference, a specification of a domain of thought and action, in order to make it meaningful. [...] [T]hese domains are themselves not measurable. [...].'' Argues that any argument that calls for the conclusion or denial that ``machines may surpass us in general intelligence'' is ``ill-framed and therefore sterile'' due to ``our inability to compute an upper bound on machine intelligence''. We follow \citet{blili-hamelin_unsocial_2024} in considering this critical account relevant to debates about how to conceive AGI. 

\\ \midrule

\citet{searle_minds_1980}. ``{\it according to strong AI, the computer is not merely a tool in the study of the mind; rather, the appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states}.'' \\ \midrule

\citet{gubrud_nanotechnology_1997} ``By advanced artificial general intelligence, I mean AI systems that rival or surpass the human brain in complexity and speed, that can acquire, manipulate and reason with general knowledge, and that are usable in essentially any phase of industrial or military operations where a human intelligence would otherwise be needed. Such systems may be modeled on the human brain, but they do not necessarily have to be, and they do not have to be "conscious" or possess any other competence that is not strictly relevant to their application. What matters is that such systems can be used to replace human brains in tasks ranging from organizing and running a mine or a factory to piloting an airplane, analyzing intelligence data or planning a battle.'' \\ \midrule

\citet{nilsson_human-level_2005}. ``\textit{achieving real human-level artificial intelligence would necessarily imply that most of the tasks that humans perform for pay could be automated. Rather than work toward this goal of automation by building special-purpose systems, I argue for the development of general-purpose, educable systems that can learn and be taught to perform any of the thousands of jobs that humans can perform.}'' \\ \midrule

\citet{legg_universal_2007} ``\textit{Intelligence measures an agent’s ability to achieve goals in a wide range of environments.}'' \\ \midrule

\cite{Wozniak_coffee_2010} ``\textit{Could a computer make a cup of coffee?}''  Tasks the machine to go into an “average” American home, find ingredients, and make a cup of coffee. This requires embodied AI systems. Wozniak's test has since been included in discussions of AGI. \cite{goertzel2012architecture} \\ \midrule

\citet{chalmers_singularity_2010}. ``AI is artificial intelligence of human level or greater (that is, at least as intelligent as an average human). Let us say that AI+ is artificial intelligence of greater than human level (that is, more intelligent than the most intelligent human). Let us say that AI++ (or superintelligence) is AI of far greater than human level (say, at least as far beyond the most intelligent human as the most intelligent human is beyond a mouse).''%Explores the concept of the technological singularity---the point at which AI surpasses human intelligence, potentially leading to rapid and profound societal changes. A key mechanism that could lead to singularity is the development of a machine more intelligent than humans, which will hence be capable of enhancing its own capabilities, consequently initiating an ``intelligence explosion''. %The singularity could either solve major human problems and create unprecedent prosperity, or lead to existential risks, including human extinction or severe societal upheaval. %
\\ \midrule

\citet{goertzel2012architecture}. Propose an architecture for human-like general intelligence that integrates slightly modified versions of previously existing architectures, emphasizing the commonalities across different approaches.
\\ \midrule

\citet{bostrom_superintelligence_2014}. ``We can tentatively define a superintelligence as \textit{any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest}.'' %Defines three forms of superintelligence: ``Speed superintelligence: A system that can do all that a human intellect can do, but much faster.'', ``Collective superintelligence: A system composed of a large number of smaller intellects such that the system’s overall performance across many very general domains vastly outstrips that of any current cognitive system.'', and ``Quality superintelligence: A system that is at least as fast as a human mind and vastly qualitatively smarter.''. %Denies that achieving human-level intelligence or superintelligence requires ``general'' intelligence at all. Bostrom sees this as part of his strong rejection of the requirement for human-like cognitive processes. Bostrom’s line of argument presumes that replicating the instrumental performance of human in- telligence is the goal—but there are other possible goals for AI. Bostrom ar- gues that when thinking about what it means for machines to match or outclass human intelligence, the relevant unit of comparison includes not just the intelligence of an indi- vidual human but also “the combined intellectual capability of all of humanity” at present (Bostrom 2014). Specifically, he argues that collectives—such as “firms, work teas, gossip networks, advocacy groups, academic communities, coun- tries, even humankind as a whole” (Bostrom 2014)—can be understood as a mechanism for increasing intelligence. Bostrom also refers to collective intelligence as “collective intellectual problem-solving capacity”.
\\ \midrule

\citet{goertzel_artificial_2014}. ``roughly speaking, an AGI system is a synthetic intelligence that has a general scope and is good at generalization across various goals and contexts.''. %Cites key features of General Intelligence for which there is ``broad agreement in the AGI community'': ``ability to achieve a variety of goals, and carry out a variety of tasks, in a variety of different contexts and environments.'', ``Arbitrarily general intelligence is not possible given realistic resource constraints.''.
\\ \midrule

\citet{smart_beyond_2015}. ``a strong AI system would be an entirely autonomous computer system in no way controlled or influenced by human operators. It could successfully adapt to its environment or even be part of its environment, making intelligent decisions, and for all intents and purposes interacting with humans naturally. It would have vastly superior memory and computational abilities but would also be able to reason and act accordingly. What all of this boils down to is that a strong AI would have to be conscious.'' \\ \midrule

\citet{openai_2018}. ``\textit{OpenAI’s mission is to ensure that artificial general intelligence (AGI)—by which we mean highly autonomous systems that outperform humans at most economically valuable work—benefits all of humanity.}'' December 2024 reporting suggests that OpenAI and Microsoft ``signed an agreement last year stating OpenAI has only achieved AGI when it develops AI systems that can generate at least \$100 billion in profits'' \cite{zeff2024microsoft}. If true, this is a significant departure from their former definition. \\ \midrule

\citet{chollet_arc_technical_report_2024,chollet_measure_2019}. Defines AGI as``\textit{a system capable of efficiently acquiring new skills and solving novel problems for which it was neither explicitly designed nor trained}''. In 2019, introduced an as yet (January 2025) unsolved benchmark for incentivizing progress towards AGI thus defined. Proposes that ``it's still feasible to create unsaturated, interesting benchmarks that are easy for humans, yet impossible for AI -- without involving specialist knowledge. We will have AGI when creating such evals becomes outright impossible'' \cite{chollet_so_2024}. \\ \midrule

\citet{hernandez-orallo_general_2021}. ``independently of its overall capability, \textit{an agent can only be called fully general if it covers all tasks up to an equivalent level of difficulty, determined by the resources that are needed for them}''. Introduces two individual-specific (be them humans, other animals or AI systems) measures that, together, decouple the concept of general intelligence: (i) `generality', which refers to ``[...] the distribution of the tasks the agent can solve,'' and (ii) `capability', which indicates ``[...] how far, on average, an agent can reach in terms of task difficulty''. \\ \midrule

\citet{Marcus_2022}. Defines AGI as \textit{``a shorthand for any intelligence ... that is flexible and general, with resourcefulness and reliability comparable to (or beyond) human intelligence''}. Calls for the need to operationalize the definition as a single system that can succeed in at least 3 of 5 proposed tasks, including Wozniak's coffee cup benchmark~\cite{Wozniak_coffee_2010}. \\ \midrule

\citet{morris_levels_2023}. Proposes a practical strategy analogous to Levels of Driving Automation standards \cite{sae_international_j3016_202104_2021}. Describes a graded set of levels of achievement of target characteristics that can each be associated with tangible ``metrics'', the introduction of ``risks'', and changes in ``Human-AI Interaction paradigm'' \cite{morris_levels_2023}. The framework targets 2 characteristics: levels of \emph{performance} (which they define as ``the depth of an AI system’s capabilities, i.e., how it compares to human-level performance for a given task''), and levels of \emph{generality} (defined as ``breadth of an AI system’s capabilities, i.e., the range of tasks for which an AI system reaches a target performance threshold.'').  \\ \midrule

\citet{summerfield_natural_2023} We consider this account to endorse a strong form of \emph{pluralism} about intelligence: natural intelligence takes \emph{many} different shapes across cultures and species, serving \emph{many} goals and functions that are irreducibly shaped by ``the internal model by which an animal understands the world'', which itself ``depends on its local environment, its embodied form, its desires and goals, and its interactions with conspecifics.'' The same should be expected for ``strong AI'' or ``AGI''. However, building on \citet{dreyfus_mind_1986}, Summerfield argues we have practical reasons to constrain the forms AI takes. The goal of AI is ``to help humans in their endeavours.'' To that end,``if we want to build AI systems that exhibit human-like intelligence, with whom we can interact in pursuit of human-centred goals, these agents will need to think in ways that make sense to us.'' \footnote{``[W]e are building AI to make the world a better place. But if we want AI to be useful to people, it will need to share our umwelt. If we build an AI that sees the world in a radically different way to us, its behaviour and mental states will be unintelligible. Such an agent will be at best unreliable and at worst unsafe.'' \cite{summerfield_natural_2023}}  \\ \midrule

\citet{attard-frost_queering_2023} Defines human and artificial intelligence as ``value-dependent cognitive performance'', and ``centres interdependencies between agents, their environments, and their measurers in collectively constructing and measuring context-specific performances of intelligent action''. Although this account is not presented as a conception of AGI, \citet{blili-hamelin_unsocial_2024} argue that the account is relevant to the topic.  \\ \midrule

\citet{suleyman_coming_2023} ``\textit{Artificial intelligence (AI) is the science of teaching machines to learn humanlike capabilities. Artificial general intelligence (AGI) is the point at which an AI can perform all human cognitive skills better than the smartest humans.}'' \\ \midrule

\citet{aguera_y_arcas_artificial_2023}. ``\textit{`General intelligence' must be thought of in terms of a multidimensional scorecard, not a single yes/no proposition.}'' Dimensions discussed include topics, tasks, modalities, languages, and instructability.  \\
    \bottomrule
    \caption{Sample definitions of AGI and related concepts.}
    \label{tab:agi_definitions}
    \end{longtable}
%     \caption{Sample definitions of AGI.}
%     \label{tab:agi_definitions}
% \end{table*}
\end{center}


\twocolumn


\end{document}