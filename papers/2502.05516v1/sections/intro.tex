In today's data-driven landscape, private and public organizations increasingly rely on data collected from individuals for decision-making and to enhance service provision. While insights obtained from data undoubtedly offer value to societies, it is essential not to overlook the risks to \emph{privacy} that come along with it. As a result, extensive research over the past decades has led to the emergence of various privacy definitions and frameworks.

Among the various definitions proposed, \emph{differential privacy} (DP)\cite{dworkDifferentialPrivacy,dworkCalibratingNoiseSensitivity} stands out as the most widely accepted framework for understanding and enforcing privacy. DP has been adopted by both public agencies, such as the U.S. Census Bureau~\cite{abowd2018us}, and major industry players like Apple~\cite{thakurta2017learning}, Google~\cite{erlingsson2014rappor}, and Microsoft~\cite{ding2017collecting}. DP assumes that data collected from individuals is stored in a database that returns answers to queries in a privacy-preserving manner. Its objective is to reveal population-level insights about the data while preserving the privacy of each individual. Specifically, DP ensures that two databases differing in a single entry, presumably information pertaining to a single individual, cannot be distinguished based on their corresponding query responses. This approach aligns with the fundamental idea that \say{nothing should be learnable about an individual participating in a database that could not be learned without participation}~\cite{dworkDifficultiesDisclosurePrevention2010a}. 

Despite its widespread success, several studies have raised concerns that DP may not provide sufficient protection for databases containing \emph{correlated} data~\cite{kiferNoFreeLunch2011,cormode2011personal,he2014blowfish,liu2016dependence,yang2015bayesian,liMembershipPrivacyUnifying2013,zhu2014correlated}. Informally, this is because there may be no one-to-one mapping between individuals and entries in the database, and each person's information may contribute to multiple entries. To illustrate this issue, consider the following example from~\cite{kiferNoFreeLunch2011}. 

\begin{example}
\label{ex:bob}
Suppose Bob is part of a medical database where his sensitive attribute can take one of the values $1, \ldots, k$. Assume the database is sampled from a distribution such that when Bob's sensitive attribute is $j$, there are $j \times 10,000$ cancer patients in the data. Suppose an adversary queries the database about the number of cancer patients. Let $\mathrm{Lap}(b)$ denote the zero mean Laplace distribution with scale parameter $b>0$ (i.e., variance $2b^2$). To answer this query while satisfying $0.1$-DP, the mechanism returning the response adds $\mathrm{Lap}(10)$ noise to the true count and releases the result (see Definitions~\ref{def:dp} and~\ref{def:laplace_mech} in Section~\ref{sec:background}). However, in this case, the attacker can infer Bob's sensitive attribute with high probability by dividing the noisy answer by 10,000 and rounding to the nearest integer $j$.
\end{example}

Example~\ref{ex:bob} and similar ones underscore the necessity for privacy definitions that take into account the data-generating distribution. Consequently, several privacy frameworks have emerged to address this concern, including Pufferfish privacy~\cite{kiferPufferfishFrameworkMathematical2014}, membership privacy~\cite{liMembershipPrivacyUnifying2013}, Bayesian differential privacy~\cite{yang2015bayesian}, and coupled-worlds privacy~\cite{bassily2013coupled}, to give a few examples. Among these distribution-dependent frameworks, one that is particularly promising is based on a recent notion of information leakage called \emph{pointwise maximal leakage} (PML)~\cite{saeidian2023pointwise_it,saeidian2023pointwise_isit}. 

At its core, PML is an information measure that quantifies the amount of information leaking about a secret to a publicly available and correlated quantity. What sets PML apart is its strong operational meaning in the context of privacy. Specifically, PML is derived by assessing risks posed by adversaries in highly general threat models (see Section~\ref{sec:background} for details). Moreover, PML exhibits remarkable robustness by considering a wide range of adversaries, and flexibility in its application to various data types. Another noteworthy aspect of PML is that its guarantees and privacy parameters are easily interpretable. Informally speaking, it was shown in \cite{inferential_privacy} that enforcing privacy according to PML aligns with the fundamental principle that \say{nothing should be learnable about the secret that could not be learned from its distribution alone.} Therefore, on an abstract level, the objectives of PML parallel those of differential privacy since PML aims to reveal population-level insights about the data while concealing its intricate details. Furthermore, PML is suitable for guaranteeing privacy in complex data-processing systems through various inequalities that it satisfies, including pre-processing, post-processing, and composition inequalities, among others~\cite[Lemma 1]{saeidian2023pointwise_it}.


Interestingly, while PML is not a generalization or relaxation of differential privacy, connections have been established between the two frameworks.  More precisely, it was shown in~\cite[Thm. 4.2]{inferential_privacy} that when our goal is to protect a database containing independent entries, then differential privacy is equivalent to restricting the PML of each entry in the database across all possible outcomes of a mechanism. This result provides deep insights for protecting databases containing independent entries. However, it also prompts the question: \emph{What is the relationship between PML and differential privacy in scenarios where the entries in a database are correlated?}

\subsection{Contributions and Outline}
In this work, we establish that in scenarios where the entries in a database are correlated, the PML guarantees of mechanisms satisfying pure DP can be arbitrarily weak. Specifically, we prove that there exists a pure DP mechanism with PML levels arbitrarily close to that of a mechanism which releases individual entries from a database without any perturbation. The significance of this result lies in its quantitative nature. In particular, we rely on analytical arguments to demonstrate that DP may not be a suitable framework for protecting correlated databases, in contrast to its performance in scenarios involving independent entries. Our analysis based on PML also distinguishes our work from previous studies, which often rely on intuition, qualitative examples~\cite{kifer2012rigorous,yang2015bayesian,zhu2014correlated}, or experimental evidence~\cite{cormode2011personal,liu2016dependence} to illustrate the weakness of DP in the presence of correlations. Overall, our discussions aim to raise awareness in order to prevent creating a false sense of security that results from applying privacy mechanisms indiscriminately. 

The remainder of this paper is organized as follows: Section~\ref{sec:background} presents preliminaries, including definitions of pure differential privacy and pointwise maximal leakage, highlighting their connections and distinctions. In Section~\ref{sec:main}, we demonstrate that in scenarios involving correlated databases, mechanisms satisfying pure differential privacy can be weak when evaluated through the lens of PML. Section~\ref{sec:discussion} contains a brief discussion about our result and some concluding remarks.  

