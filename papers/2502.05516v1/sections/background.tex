\subsection{Notation}
We use uppercase letters to denote random variables, lower case letters to denote realizations, and calligraphic letters to denote sets. In particular, we reserve $X$ for representing some data containing sensitive information, e.g., a dataset containing information about individuals. For simplicity, we assume that the alphabet of $X$, denoted by $\cX$, is a finite set. With a slight abuse of notation, we use $P_X$ to describe both a distribution for $X$ and its corresponding probability mass function. Moreover, $\cP_\cX$ denotes the set of all distributions with full support on $\cX$. 

Let $Y$ be a random variable representing some information released about $X$, taking values on a set $\cY$. The random variable $Y$ is induced by a \emph{mechanism}, i.e., a conditional probability distribution $P_{Y \mid X}$. Essentially, $Y$ represents the answer to a query posed about $X$. The set $\cY$ can be finite (e.g., when $P_{Y \mid X}$ is the randomized response mechanism~\cite{warner1965randomized}) or infinite (e.g., when $P_{Y \mid X}$ is the Laplace mechanism~\cite{dworkCalibratingNoiseSensitivity}). With a slight abuse of notation, we use $P_{Y \mid X}$ to denote both the conditional distribution of $Y$ given $X$ as well as its density with respect to a suitable dominating measure, e.g., the counting measure or the Lebesgue measure.  

Let $P_{XY}$ denote the joint distribution of $X$ and $Y$. We write $P_{XY} = P_{Y \mid X} \times P_X$ to imply that $P_{XY}(x,y) = P_{Y \mid X =x}(y) P_X(x)$ for all $(x,y) \in \mathcal X \times \mathcal Y$. Furthermore, we write $P_{Y} = P_{Y \mid X} \circ P_X$ to represent marginalization over $X$, i.e., to imply that ${P}_{Y}(y) = \sum_{x \in \mathcal X}  P_{Y \mid X=x}(y)  P_X(x)$ for all $y \in \mathcal Y$. 

We say that the Markov chain $U - X - Y$ holds if random variables $U$ and $Y$ are conditionally independent given $X$, that is, if $P_{UY \mid X} = P_{U \mid X} \times P_{Y \mid X}$. The Markov chain $U - X - Y$ implies that  $Y$ depends on $U$ only through $X$ and vice versa. We may think of a $U$ satisfying the Markov chain $U-X-Y$ as either a feature of $X$ or a (randomized) function of $X$. 

Finally, $[n] \coloneqq \{1, \ldots, n\}$ describes the set of all positive integers smaller than or equal to $n$, and $\log(\cdot)$ denotes the natural logarithm.

%============================================%
\subsection{Differential Privacy}
Often called the gold standard of privacy, differential privacy (DP)~\cite{dworkCalibratingNoiseSensitivity,dwork2014algorithmic} stands as the most widely adopted privacy framework, both in theoretical developments as well as real-world deployments. Conceptually, DP considers scenarios where individuals' data is aggregated into a database, with the aim of responding to queries posed to the database without compromising the privacy of any individual contributor. To achieve this goal, DP guarantees that two databases that differ in only one entry (referred to as \say{neighboring} databases) produce query responses that are hard to distinguish. 

Over nearly two decades of extensive research has led to the development of various DP variants and adaptations, such as approximate DP~\cite{dworkOurDataOurselves2006a}, concentrated DP~\cite{bun2016concentrated,dwork2016concentrated}, Rényi DP~\cite{mironov2017renyi}, and Gaussian DP~\cite{dong2022gaussian}, among others. However, in this paper, our focus is on the original, and arguably the strongest, form of DP, known as \say{pure} DP~\cite{dworkCalibratingNoiseSensitivity}.

Let $X = (D_1, \ldots, D_n)$ be a database containing $n$ entries. Given $i \in [n]$, $D_i$ represents the $i$-th entry, which takes values on a finite set $\mathcal D$ and $D_{-i} = (D_1, \ldots, D_{i-1}, D_{i+1}, \ldots, D_n)$ represents the database with its $i$-th entry removed. Let $P_X = P_{D_1, \ldots, D_n}$ be the distribution according to which databases are drawn from $\cX = \mathcal D^n$. To obtain the distribution of the $i$-th entry, we marginalize over the remaining $n-1$ entries, that is, for each $d_i \in \mathcal D$ and $i \in [n]$ we have
\begin{align}
\begin{split}
\label{eq:marginal}
    P_{D_i}(d_i) &= \sum_{d_{-i} \in \mathcal D^{n-1}} P_X(d_i, d_{-i})\\
    &= \sum_{d_{-i} \in \mathcal D^{n-1}} P_{D_i \mid D_{-i} = d_{-i}}(d_i) \; P_{D_{-i}} (d_{-i}),
\end{split}
\end{align}
where $d_{-i} \coloneqq (d_1, \ldots, d_{i-1}, d_{i+1}, \ldots, d_n) \in \mathcal D^{n-1}$ is a tuple describing the database with its $i$-th entry removed. Note that this setup is very general since we make no independence assumptions and the entries can be arbitrarily correlated. 

Suppose an analyst poses a query to the database, with the answer returned by the mechanism $P_{Y \mid X}$.

\begin{definition}[Differential privacy]
\label{def:dp}
Given $\varepsilon \geq 0$, the mechanism $P_{Y \mid X}$ satisfies $\varepsilon$-differential privacy if 
\begin{equation*}
    \max_{\substack{d_i, d_i' \in \mathcal D: \\ i \in [n]}} \; \max_{d_{-i} \in \mathcal D^{n-1}} \log \frac{P_{Y \mid D_i=d_i, D_{-i} = d_{-i}}(\cE)}{P_{Y \mid D_i=d_i', D_{-i} = d_{-i}}(\cE)} \leq \varepsilon,
\end{equation*}
for all measurable events $\cE \subseteq \cY$. 
\end{definition}

Importantly, Definition~\ref{def:dp} is agnostic to the distribution of the database, $P_X$. Therefore, differential privacy is a property of the mechanism $P_{Y \mid X}$ alone. This observation has led to the widespread belief that DP offers strong privacy guarantees for individuals in a database irrespective of the distribution $P_X$. However, in Section 3 we will present a quantitative example to demonstrate that a differentially private mechanism can in fact leak a large amount of information about individual entries in scenarios where the entries are correlated. 

Next, let us recall one of the most commonly employed differentially private mechanisms, namely the \emph{Laplace mechanism}~\cite{dworkCalibratingNoiseSensitivity}. The Laplace mechanism is often used to answer numerical queries with bounded $\ell_1$-\emph{sensitivity}. Let $\mathrm{Lap}(\mu, b)$ denote the Laplace distribution with mean $\mu \in \bR$ and scale parameter $b>0$ (i.e., variance $2b^2$). 

\begin{definition}[Laplace mechanism]
\label{def:laplace_mech}
Let $f : \cX \to \bR$ be a query with $\ell_1$-sensitivity 
\begin{equation*}
    \Delta_1(f) \coloneqq \sup_{x_1, x_2 \in \cX : x_1 \sim x_2} \abs{f(x_1) - f(x_2)}.
\end{equation*}
Given $\varepsilon > 0$ the Laplace mechanism returns a query response according to the distribution $Y \mid X=x \sim \mathrm{Lap}\left(f(x), \frac{\Delta_1(f)}{\varepsilon} \right)$, where $x \in \cX$. 
\end{definition}

It has been shown in~\cite{dworkCalibratingNoiseSensitivity} that the Laplace mechanism satisfies $\varepsilon$-DP.

The $\ell_1$-sensitivity of a query $f$ describes the largest change in $f(x)$ upon altering the value of one entry in database $x$. The Laplace mechanism then computes $f$ and perturbs it with Laplace noise scaled according to $\Delta_1(f)$ and $\varepsilon$. Examples of queries that can be answered via the Laplace mechanism include \emph{counting queries}, that is, queries of the form \say{How many entries in the database satisfy property $A$?,} and \emph{histogram queries}~\cite{dwork2014algorithmic}.

%============================================%
\subsection{Pointwise Maximal Leakage}
% First, we recall the definition of Rényi divergence of order infinity~\cite{renyi1961measures,van2014renyi} which will be useful for expressing PML. For simplicity, we only define the divergence on finite sets. 

% \begin{definition}[{Rényi divergence of order $\infty$~\cite[Thm. 6]{van2014renyi}}] 
% Let $P$ and $Q$ be probability distributions on a finite set $\cX$ and suppose $P$ is absolutely continuous with respect to $Q$. The Rényi divergence of order $\infty$ of $P$ from $Q$ is 
% \begin{equation*}
%     D_\infty(P \Vert Q) = \log \, \max_{x \in \cX} \, \frac{P(x)}{Q(x)}. 
% \end{equation*}
% \end{definition}

Pointwise maximal leakage (PML)~\cite{saeidian2023pointwise_it,saeidian2023pointwise_isit} is a recently introduced privacy measure that enjoys a strong operational meaning and robustness. It measures the amount of information leaking about a secret $X$ to the outcomes of a mechanism $P_{Y \mid X}$. PML is obtained by evaluating the risk posed by adversaries in two highly versatile threat models: the \emph{randomized functions} model~\cite{issaOperationalApproachInformation2020} and the \emph{gain function} model of leakage~\cite{alvim2012measuring}.

Below, we define PML using both models.
\begin{definition}[Randomized function view of PML~{\cite[Def. 1]{saeidian2023pointwise_it}}]
\label{def:randomized_function_view}
Suppose $X$ is a random variable on the finite set $\cX$, and $Y$ is a random variable on a set $\cY$ induced by a mechanism $P_{Y \mid X}$. 
% Let $P_{XY} = P_{Y \mid X} \times P_X$ denote the joint distribution if $X$ and $Y$ with $P_X \in \cP_\cX$.
According to the randomized function view, the pointwise maximal leakage from $X$ to $y \in \cY$ is defined as
\begin{equation}
\label{eq:pml_u_sup}
    \ell(X\to y) \coloneqq \log \sup_{U:U-X-Y} \frac{\sup_{P_{\hat U \mid Y}} \mathbb P \left(U=\hat U \mid Y=y \right)}{\max_{u \in \cU} P_U(u)},
\end{equation}
where $U$ and $\hat U$ are random variables on a finite set $\cU$, $P_{U} = P_{U \mid X} \circ P_X$, and the Markov chain $U-X-Y-\hat U$ holds.
\end{definition}

Definition~\ref{def:randomized_function_view} can be understood as follows. Let $U$ be a randomized function of $X$. For instance, when $X$ is a database, $U$ can represent a single entry or a subset of the entries in $X$. To quantify the amount of information leakage associated with a single released outcome of the mechanism, denoted by $y$, in \eqref{eq:pml_u_sup} we compare the probability of correctly guessing the value of $U$ after observing $y$ in the numerator of~\eqref{eq:pml_u_sup} with the \emph{a priori} probability of correctly guessing the value of $U$ in the denominator. The probability of correctly guessing $U$ after observing $y$ is assessed by assuming that the adversary uses the best guessing kernel $P_{\hat U \mid Y}$, represented by the supremum over $P_{\hat U \mid Y}$ in the numerator of~\eqref{eq:pml_u_sup}. Similarly, the prior probability of correctly guessing $U$ is $\max_{u \in \mathcal{U}} P_U(u)$. Crucially, Definition~\ref{def:randomized_function_view} results in a highly robust measure of privacy since the posterior-to-prior ratio is maximized over all possible randomized functions of $X$, represented by the supremum over all $U$'s satisfying the Markov chain $U-X-Y$. This makes PML particularly useful when we do not know what feature of $X$ an adversary is interested to guess, or different adversaries may be interested in different features of $X$. 

Next, we define PML using the gain function model. 
\begin{definition}[Gain function view of PML~{\cite[Cor. 1]{saeidian2023pointwise_it}}]
\label{def:gain_function_view}
Suppose $X$ is a random variable on the finite set $\cX$, and $Y$ is a random variable on a set $\cY$ induced by a mechanism $P_{Y \mid X}$. 
% Let $P_{XY} = P_{Y \mid X} \times P_X$ denote the joint distribution if $X$ and $Y$ with $P_X \in \cP_\cX$. 
According to the gain function view, the pointwise maximal leakage from $X$ to $y \in \cY$ is defined as
\begin{equation}
\label{eq:g-leakage}
    \ell(X\to y) \coloneqq \log \; \sup_g \; \frac{\sup_{P_{W \mid Y}} \mathbb{E} \left[g(X,W) \mid Y=y \right]}{\max_{w \in \cW} \bE\left[g(X, w)\right]},
\end{equation}
where the supremum is over all non-negative gain functions $g: \cX \times \cW \to \bR_+$ with a finite range, and $\cW$ is a finite set.  
\end{definition}

Definition~\ref{def:gain_function_view} can be understood as follows. Consider an adversary whose objective is to construct a guess of $X$, denoted by $W$, in order to maximize the expected value of a non-negative gain function $g$. The gain function $g$ captures the adversary's objective and can be tailored to model a wide array of privacy attacks. For example, when $X$ is a database, $g$ can model \emph{membership inference} attacks or \emph{reconstruction} attacks~\cite{dwork2017exposed} (see \cite{saeidian2023pointwise_it} for concrete examples of gain functions). To quantify the amount of information leakage associated with a single released outcome $y$, we compare the expected value of $g$ after observing $y$ in the numerator of~\eqref{eq:g-leakage} with the prior expected value of $g$ in the denominator. The posterior expected gain is assessed using the best kernel $P_{W \mid Y}$, represented by the supremum over $P_{W \mid Y}$ in the numerator of~\eqref{eq:g-leakage}. Similarly, the prior expected gain is $\max_{w \in \cW} \bE\left[g(X, w)\right]$. Then, to obtain a privacy measure robust to different types of attacks, the posterior-to-prior ratio of the expected gain is maximized over all possible non-negative $g$'s with a finite range. 

While Definitions \ref{def:randomized_function_view} and \ref{def:gain_function_view} offer different approaches to defining PML, we showed in \cite[Thm. 2]{saeidian2023pointwise_it} that, in fact, they are mathematically equivalent. Moreover, both definitions can be simplified to the following concise expression.
\begin{theorem}[{\cite[Thm. 1]{saeidian2023pointwise_it}}]
\label{thm:pml}
Let $P_{XY}$ be a distribution on the set $\mathcal X \times \mathcal Y$ with the marginal distribution $P_X \in \cP_\cX$ for $X$. The pointwise maximal leakage from $X$ to $y \in \mathcal Y$ is\footnote{We use the convention that $P_{X \mid Y=y} = P_X$ if $P_Y(y) =0$. That is, conditioning on outcomes with density zero equals no conditioning.}
\begin{equation}
\label{eq:pml_simple}
    \ell(X \to y) = D_\infty(P_{X \mid Y=y} \Vert P_X),
\end{equation}
where $P_{X \mid Y=y}$ denotes the posterior distribution of $X$ given $y \in \mathcal Y$, and
\begin{align*}
    D_\infty(P_{X \mid Y=y} \Vert P_X) &= \log \; \max_{x \in \cX} \; \frac{P_{X \mid Y=y}(x)}{P_X(x)}\\
    &= \log \; \max_{x \in \cX} \; \frac{P_{Y \mid X=x}(y)}{P_Y(y)},
\end{align*}
denotes the \Ren divergence of order infinity~\cite{renyi1961measures} of $P_{X \mid Y=y}$ from $P_X$.  
\end{theorem}

In addition to its strong operational meaning and robustness, PML satisfies several useful properties that render it suitable for deployment in complex data-processing systems. Notably, PML satisfies a pre-processing inequality, a post-processing inequality, and increases (at most) linearly under composition~\cite[Lemma 1]{saeidian2023pointwise_it}. Furthermore, as evident from~\eqref{eq:pml_simple}, PML is non-negative and satisfies the bound
\begin{equation}
\label{eq:pml_bounds}
    \ell(X \to y) \leq \log \; \frac{1}{\min\limits_{x \in \cX} P_X(x)},
\end{equation}
for all mechanisms and all $y \in \cY$. The right hand side of the above inequality essentially describes the maximum amount of information that can leak about $X$ through any mechanism. In other words, it represents the largest PML across all outcomes of a mechanism that releases $X$ without perturbing it.

%============================================%
\subsection{Differential Privacy as a PML Constraint}
In general, PML and DP offer fundamentally distinct approaches to privacy and differ in several key aspects. PML quantifies the amount of information leaked to an outcome of a privacy mechanism and the secret $X$ may encompass various types of sensitive data. For example, $X$ can be a password, an individual's medical records, or an entire database. In contrast, DP was specifically formulated to protect private databases. More importantly, PML depends on both the mechanism $P_{Y \mid X}$ and the data-generating distribution $P_X$. Consequently, a mechanism that leaks little information leakage under one distribution may leak a lot of information under another distribution. Conversely, DP depends only on the mechanism. 

Despite their differences, in~\cite[Thm. 4.2]{inferential_privacy}, we established that when $X$ is a database containing independent entries, DP is equivalent to restricting the amount of information leaked about each entry across all outcomes of a mechanism. Let $\cX = \cD^n$ denote the set of all possible databases, and $\mathcal Q_\mathcal X$ denote the set of product distributions in $\mathcal P_{\mathcal X}$ defined as $\mathcal Q_\mathcal X \coloneqq \{P_{X} \in \mathcal P_{\mathcal X} \colon P_{X} = \prod_{i=1}^n P_{D_i} \}$.

\begin{theorem}[{\cite[Thm. 4.2]{inferential_privacy}}]
\label{thm:dp_pml}
Given $\varepsilon \geq 0$, the mechanism $P_{Y \mid X}$ satisfies $\varepsilon$-differential privacy if and only if 
\begin{equation*}
    \sup_{P_X \in \mathcal Q_\mathcal X} \; \sup_{y \in \mathcal Y} \; \max_{i \in [n]} \; \ell(D_i \to y) \leq \varepsilon. 
\end{equation*}
\end{theorem}

Theorem~\ref{thm:dp_pml} demonstrates that when the entries in a database are independent, DP is adequate for ensuring privacy in the sense of PML. However, it also raises the possibility that when the entries are correlated, DP might fall short in terms of PML. We delve into this topic in the next section. 
