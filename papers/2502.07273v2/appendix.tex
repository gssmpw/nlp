\section{Derivations}

\subsection{Derivation of GLM with Newton's method}
\label{app:glm_newton}
Newton's update shown in \cref{eq:newton} is equivalent to the following surrogate minimization,
 \begin{equation}
       \vparam_{t+1} = \,\, \arg\min_{\vparam} \,\,  \vparam^\top \grad \barloss(\vparam_t) + \frac{1}{2} (\vparam-\vparam_t)^\top \nabla^2 \barloss(\vparam_t) (\vparam-\vparam_t) .
\label{eq:newton_surrogate}
\end{equation}
This can be verified by simply taking the derivative of the above objective and setting it to zero. We will now show that the VON update give rise to a similar surrogate but where the gradient and Hessian are replaced by their expected values. 

To do so, we use the result of \citet{khan2019approximate} who show that each step of VON algorithm can be seen as inference on a linear model. Essentially, the VON update can be expressed as follows (see \citet[App. C.3]{nickl2024memory} for a derivation):
 \begin{equation}
    \begin{split}
       q_{t+1}(\vparam) &\propto q_t(\vparam)^{1-\rho_t} \prod_{i=1}^N e^{ \rho_t \rnd{ \vparam^\top \myexpect_{q_t}[-\nabla \loss_i(\vparam) + \nabla^2 \loss_i(\vparam) \vparam_t] -\half \vparam^\top \myexpect_{q_t}[\nabla^2 \loss_i(\vparam)] \vparam} } \\
       &\propto q_t(\vparam)^{1-\rho_t} \prod_{i=1}^N e^{ -\rho_t \rnd{ \vparam^\top \myexpect_{q_t}[\nabla \loss_i(\vparam)] + \half (\vparam - \vparam_t)^\top \myexpect_{q_t}[\nabla^2 \loss_i(\vparam)] (\vparam-\vparam_t)} }, 
    \end{split}
    \label{eq:blr_sgd}
 \end{equation}
 where we subtracted $\vparam_t^\top \myexpect_{q_t}[\nabla^2 \loss_i(\vparam)] \vparam_t$ and completed the square. This is a constant which is absorbed in the normalizing constant of $q_{t+1}$. From here, we can simply match the mode $\vparam_{t+1}$ of $q_{t+1}$ to the mode of the right hand side. For $\rho_t = 1$, this gives us the following minimization problem to recover $\vparam_{t+1}$:
\begin{equation}
   \vparam_{t+1} = \,\, \arg\min_{\vparam} \,\,  \vparam^\top \myexpect_{q_t}[\grad \barloss(\vparam)] + \frac{1}{2} (\vparam-\vparam_t)^\top \myexpect_{q_t}[\nabla^2 \barloss(\vparam)] (\vparam-\vparam_t).
\end{equation}
This shows that, for $\rho_t =1$, VON updates can be seen as Newton step where gradient and Hessian are replaced by their expected values. The proof is identical to the one shown in the main tex, therefore we omit it.

 \begin{thm}
    For the loss function of \cref{eq:bce}, the VON update in \cref{eq:von} with $\rho_t = 1$ is equivalent to Newton's update in \cref{eq:newton} but where the label $y_i$ are replaced by $y_i + \epsilon_{i|t}$ with noise defined as
    \begin{equation}
       \epsilon_{i|t} =  \sigmoid\rnd{f_{i|t}} - \myexpect_{\text{\gauss}(e|0,1)} \sqr{\sigmoid\rnd{f_{i|t} + e \sqrt{\vphi_i^\top\vSigma_t \vphi_i}  }},
       \label{eq:sgd_noise}
    \end{equation}
    and the Hessian $\nabla^2 \barloss(\vparam_t)$ is replaced by its noisy version $\myexpect_{q_t}[\nabla^2 \barloss(\vparam)]$.
 \end{thm}
% \begin{proof}
   % \[
   %    \vparam^\top \tvlambda_{i|t} + \vparam^\top \tvLambda_{i|t} \vparam
   %    = -\vparam^\top \myexpect_{q_t}[\nabla \loss_i(\vparam)] - \half (\vparam - \vparam_t)^\top \myexpect_{q_t} [\nabla^2 \loss_i(\vparam)] (\vparam - \vparam_t)
   % \]
    
% \end{proof}

\clearpage

\subsection{IVON pseudo code}

The pseudo-code is given in \cref{alg:ivon}.

\definecolor{commentcolor}{RGB}{128, 179, 89}
\renewcommand\algorithmiccomment[1]{\hfill{\textcolor{commentcolor}{\eqparbox{COMMENT}{#1}}}}
\begin{algorithm}[!h]
   \caption{Improved Variational Online Newton (IVON) \citep{IVON}.}
	\label{alg:ivon}
   \begin{algorithmic}[1]
		\setstretch{1.15}
      \REQUIRE Learning rates $\{ \alpha_t \}$, weight-decay $\delta >
      0$.
      \REQUIRE Momentum parameters $\beta_1, \beta_2 \in [0, 1)$.
      \REQUIRE Hessian init $h_0 > 0$.
      \renewcommand{\algorithmicrequire}{\textbf{Init:}}
		\REQUIRE $\vm \leftarrow \text{(NN-weights)}$,\,\, $\vh \leftarrow h_0 $,
                      \,\, $\vg \leftarrow 0$, \,\, $\lambda \leftarrow N$.
      \REQUIRE $\vsigma
                      \leftarrow 1 / \sqrt{\lambda (\vh +
                        \delta)}$.
      \renewcommand{\algorithmicrequire}{\textbf{Optional:}}
      \REQUIRE $\alpha_t \leftarrow (h_0 +
                      \delta) \alpha_t$\,  for all $t$.
                      \FOR{$t=1,2,\hdots$}
                      \STATE \hspace{-0.15cm}$\widehat \vg \leftarrow
                      {\widehat \nabla} \barloss(\vparam)$,
      \text{\textcolor{black}{ where} } 
      $\vparam \sim q$
      \STATE \hspace{-0.15cm}$\widehat \vh \leftarrow {\widehat \vg\cdot (\vparam-\vm) / \vsigma^2}$
		\STATE \hspace{-0.15cm}$\vg \leftarrow \beta_1 \vg\hspace{-0.03cm}+\hspace{-0.05cm}(1\hspace{-0.05cm}-\hspace{-0.05cm}\beta_1) \widehat \vg$ 
                \STATE \hspace{-0.15cm}$\vh \leftarrow \beta_2 \vh+(1
                - \beta_2)\widehat \vh$${+\half (1 - \beta_2)^2
                  (\vh - \widehat \vh)^2 / (\vh + \delta)}$
      \STATE \hspace{-0.15cm}$\bar \vg \leftarrow \vg / (1 - \beta_1^{t})$ 
      \STATE \hspace{-0.15cm}$\vm \leftarrow \vm - \alpha_t(\bar \vg + \delta \vm) / ({\vh} + \delta)$
		\STATE \hspace{-0.15cm}$\vsigma \leftarrow 1 / \sqrt{\lambda (\vh + \delta)}$
      \ENDFOR
		\STATE \textbf{return} $\vm, \vsigma$ 
	\end{algorithmic}
	\setstretch{1}
\end{algorithm}
 
 
 
\section{Additional Experiments} \label{sec: additional exp}
\subsection{Hessian Initialization}

We analyze how Hessian initialization $h_0$ of IVON affects the accuracy. The results are in \cref{fig:cifar100_ivon_hess}. IVON's accuracy can only vary by up to 10\% when the Hessian is bigger than $0.05$, and this variation is less sensitive compared to SAM's sensitivity to $\rho$, as shown in \cref{fig:cifar10_all}, \cref{fig:cifar100_all} and \cref{fig: clothing1m_all}. 

\begin{figure}[h]
    \centering 
    \includegraphics[width=0.61\textwidth]{figures/cifar100_results/cifar100_ivon_hess.pdf}
    \caption{Results for IVON on CIFAR-100 with multiple Hessian initializations. IVON's accuracy is consistent when having different Hessian initializations.}
        \label{fig:cifar100_ivon_hess}
\end{figure}

% \subsection{Fix Covariance}
% We fix the Hessian to fix the covariance $\vSigma$ of weight posterior $q(\vparam)$. The experiment results of fixed covariance for IVON are in Figure \ref{fig:cifar100_fix_cov}. Compared with IVON with learnable covariance in Figure \ref{fig:cifar100_ivon_hess}, the accuracy is always worse. We think this is because learned covariance can customize for each parameter, whereas fixed Hessian apply same convariance to all parameters. As shown in the \cref{equ: VL newton noise}, the induced label-smoothing is depended on the $\vSigma$, so learned covariance can be more adaptive to fixed covariance.

\section{Experiment details} \label{sec: exp details}
\subsection{Experiment details of \cref{sec: noise analyses} and \cref{sec: als comparison}}

In \cref{fig: mnist_noise_distribution}, we test IVON on a 3-layers convolutional neural networks. In \cref{fig: ols_comparison}, we do experiments on ResNet-34 model. We uses the PyTorch implementation verison\footnote{\href{https://github.com/ankandrew/online-label-smoothing-pt}{https://github.com/ankandrew/online-label-smoothing-pt}} of Online Label Smoothing \citep{zhang2021delving}. 

\subsection{Experiments on Synthetic Noisy Datasets} \label{sec: cifar_exp_details}
For pairflip setting in CIFAR-10, the classes flipping order is: AIRPLANE $\rightarrow$ AUTOMOBILE $\rightarrow$ BIRD $\rightarrow$ CAT $\rightarrow$ DEER $\rightarrow$ DOG $\rightarrow$ FROG $\rightarrow$ HORSE $\rightarrow$ SHIP $\rightarrow$ TRUCK $\rightarrow$ AIRPLANE.
In CIFAR-10 experiments, we train a ResNet 34 for 200 epochs with batch size set to 50 and weight decay set to 0.001. For SAM and LS, we set initial learning rate as $0.05$ and reduce it by 0.1 at 100 epoch and 150 epoch, following hyper-parameters from previous papers. For IVON \citep{IVON}, we follow the original paper to set initial learning rate as $0.2$ and anneal the learning rate to zero with a cosine schedule after a linear warmup phase over 5 epochs. We set momentum to $0.9$ for all methods, and hessian momentum $\beta_2$ to $1 - e^{-5}$, hessian initial $h_0$ to $0.9$, scaling parameter $\lambda$ to the number of training data for IVON. For SAM, we follow the original paper \citep{SAM_org} and choose best neighborhood size $\rho$ from $[0.01, 0.05, 0.1, 0.2, 0.5]$. In CIFAR-100 experiments, we tune the hyperparamters to the best for each method. The hyperparameters are specified in \cref{tab: cifar100 hyperparameter tuning results}. 

In \cref{fig:cifar100_fix_cov}, we fix the Hessian of IVON by setting $\beta_2=1$ in Line $5$ of \cref{alg:ivon}. Therefore, standard deviation $\vsigma$ defined in Line $8$ is fixed since Hessian $\vh$ is fixed.

\input{table/cifar100_hyper}
\subsection{Clothing 1M Details}
The noisy labels in Clothing1M \citep{xiao2015learning} are derived from the text surrounding the images on the web. In constructing the dataset, noisy labels are assigned to images based on this contextual text. 
