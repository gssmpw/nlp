\section{Experiments}

\begin{figure*}[t!]
	\centering
	\begin{subfigure}{0.33\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/ols_compare/cifar10_ivon_ols_05_clean_epoch199_target0.pdf}
	\end{subfigure}
	\begin{subfigure}{0.33\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/ols_compare/cifar10_ivon_ols_05_clean_epoch199_target3.pdf}
	\end{subfigure}
	\begin{subfigure}{0.33\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/ols_compare/cifar10_ivon_ols_05_clean_epoch199_target6.pdf}
	\end{subfigure} \\
	\vspace{0.3cm}
	\begin{subfigure}{0.33\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/ols_compare/cifar100_ivon_ols_05_clean_epoch199_target3.pdf}
	\end{subfigure}
	\begin{subfigure}{0.33\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/ols_compare/cifar100_ivon_ols_05_clean_epoch199_target21.pdf}
	\end{subfigure}
	\begin{subfigure}{0.33\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figures/ols_compare/cifar100_ivon_ols_05_clean_epoch199_target66.pdf}
	\end{subfigure}
	\caption{Smoothed label comparison among IVON \citep{IVON}, LS \citep{Szegedy_rethinking} and Online Label Smoothing (OLS) \citep{zhang2021delving}. IVON has a similar adaptive label smoothing effect as OLS. $\alpha$ is the smoothing rate defined in \cref{equ: label smoothing def}. Y-axis is in the log scale. We randomly pick 10 classes for CIFAR-100 due to image size limit.}
	\label{fig: ols_comparison}
\end{figure*}

We do extensive experiments to show adaptive label noise via variational learning and its benefits. In \cref{sec: noise analyses}, we show that IVON adapts the label noise for each example, and generally assigns higher noise magnitude to ambiguous ones. In \cref{sec: als comparison}, we show that IVON's smoothed labels are similar to an existing adaptive smoothing method \citep{zhang2021delving}. In \cref{sec: mislabelled data}, we show that IVON consistently outperforms LS when datasets have labeling errors in various settings. Additional experiments are reported in \cref{sec: additional exp}, and experiment details are reported in \cref{sec: exp details}.

\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.83\textwidth]{figures/cifar_results/cifar10_all.pdf}
	\caption{Results on CIFAR-10 with symmetric noisy labels. Top: IVON outperforms Label Smoothing (LS) with different smoothing rates $\alpha$. Down: IVON has comparable results with SAM peak performances, while SAM is sensitive to the choice of perturbation $\rho$. Accuracy improvements are shown in blue. Results are reported over 5 random seeds.}
	\label{fig:cifar10_all}
\end{figure*}

\subsection{IVON's Adaptive Label Noise}\label{sec: noise analyses}

We demonstrate IVON label noise's adaptivity on MNIST dataset \citep{lecun-mnisthandwrittendigit-2010}. We plot IVON's label noise distribution in \cref{fig: mnist_noise_distribution}, which shows that IVON adds different label noise on each example whereas traditional Label Smoothing defines a uniformly distributed noise for all. By further visualizing the data, we see that IVON induces stronger noise to unclear examples, which prevent models from being overconfident in these datapoints. 

\subsection{Comparisons to Existing Adaptive LS Strategies}\label{sec: als comparison}

In this section, we show that IVON's label smoothing is similar to an adaptive method called Online Label Smoothing (OLS) \citep{zhang2021delving}. In the CIFAR-10 and CIFAR-100 dataset \citep{krizhevsky2009cifar}, we compare the smoothed labels of IVON with traditional LS \citep{Szegedy_rethinking} and Online Label Smoothing (OLS) \citep{zhang2021delving}. OLS adjusts the label noise according to the model's predictions, as described in \cref{sec: label smoothing}. As \cref{fig: ols_comparison} shows, IVON has surprisingly similar smoothed label distributions as the OLS in both datasets, while IVON tends to induce stronger label noises. Variational learning's adaptive label smoothing is similar to existing work's, without needing any additional effort to design or estimate the adaptive label noise.

\subsection{Comparisons on Datasets with Labeling Errors} \label{sec: mislabelled data}

 We compare IVON to Label Smoothing (LS) \citep{Szegedy_rethinking} and SAM \citep{SAM_org} in presence of labeling errors, and the results show that IVON consistently outperforms LS in various settings. To find the best performance of the baselines, we tune several LS's smoothing rates $\alpha$ (defined in \cref{equ: label smoothing def}), and various SAM's adversarial perturbation size $\rho$ (discussed in \cref{sec: IVON on NN}). We conduct studies on benchmark datasets with synthetic noise, where the noise level can be adjusted, followed by evaluations on datasets with natural noise, where the noise level is fixed and unknown. For synthetic noise experiments, we use the CIFAR-10 and CIFAR-100 datasets \citep{yu2019does}. For natural noise experiments, we use the benchmark Clothing1M \citep{xiao2015learning}. All datasets include a clean test set. 

\subsubsection{Synthetic Noisy Datasets} \label{sec: synthetic cifar}
We consider two commonly used corruptions \citep{patrini2017making, li2019learning, yu2019does}: Symmetric flipping and Pair flipping. In symmetric flipping, a true label is replaced by a randomly generated class with a probability. In pair flipping, it tries to mimic real world mistakes for similar classes, where a true label is replaced by the next class with a probability. For training dataset, we use previous work's  \citep{yu2019does} code to generate noisy labels. More experiment details are in \cref{sec: cifar_exp_details}. 


In CIFAR-10, \cref{fig:cifar10_all} shows that IVON outperforms Label Smoothing and SAM in different scenarios. We also observe that SAM is sensitive to the choice of $\rho$, while IVON does not need to tune any hyperparameters to perform well. In CIFAR-100, \cref{fig:cifar100_all} shows similar trends. For instance, in pairflip 20\% noise setting, IVON outperforms best LS performance by 9.1\% and best SAM performance by 13.3\%. 

Meanwhile, we test the effectiveness of flexible $\vSigma_t$ by comparing it with the fixed diagonal variance. \cref{fig:cifar100_fix_cov} shows that learned $\vSigma_t$ consistently outperforms fixed $\vSigma_t$ in three noisy datasets. The experiment results demonstrate the importance of flexible $\vSigma_t$ as stated in \cref{sec: newton on GLM}.

\begin{figure}[t]
    \centering 
    \includegraphics[width=\linewidth]{figures/cifar100_results/cifar100_ivon_fix_hess.pdf}
    \caption{In synthetic noisy datasets of CIFAR-100, we test IVON with multiple fix diagonal variance $\vSigma_t$. The fixed diagonal $\vSigma_t$ is worse than learned diagonal variance in all datasets.}
        \label{fig:cifar100_fix_cov}
\end{figure}

\subsubsection{Data Dependent Labeling Errors} \label{sec: data dependent noise}
In this experiment, we try to understand the adaptivity of these methods in the data-dependent noisy dataset. When each class has different noise levels, we expect LS will fail since it adds uniform noises to all classes, while IVON's adaptivity makes it stand.


\begin{figure}[t]
    \centering
    \includegraphics[height=5.6cm]{figures/cifar_results/cifar10_increase.pdf}
    \caption{Results for CIFAR-10 with data dependent noise. IVON outperforms LS and SAM in all noise levels. Furthermore, IVON can learn extremely noisy scenario, while LS and SAM cannot.}
    \label{fig:cifar10_inc_acc}
\end{figure}

\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.83\textwidth]{figures/cifar100_results/cifar100_all.pdf}
	\caption{Results on CIFAR-100 with symmetric noisy labels over 5 random seeds, which is similar to CIFAR-10 results in \cref{fig:cifar10_all}.}
	\label{fig:cifar100_all}
\end{figure*}

First, we create a new transition matrix $P$ of noisy label $\mathbf{y}'=P\mathbf{y}$, where $\mathbf{y, y'} \in \mathbb{R}^K, P \in \mathbb{R}^{K\times K}$. We inject difference noise level to each class, so the noise level of each class is different:
\begin{equation} \label{equ: diagonal noise}
    P_{i,i}=1-(\kappa+\beta i), i \in [1,K].
\end{equation}
where $\kappa$ is the starting noise level and $\beta$ is the increase factor. Afterwards, we give the same transition probability to the rest of the wrong classes:
\begin{equation}
    P_{i,j}=\frac{\kappa+\beta i}{K-1}, i,j \in [1,K], i\neq j.
\end{equation}

In experiments, we follow the hyperparameters in CIFAR-10 synthetic noise experiment from \cref{sec: synthetic cifar}. For LS, we run smoothing rate $\{0,0.1,0.3,0.5,0.7,0.9\}$ and report the best accuracy. For SAM, we run $\rho$ for $\{0,0.05,0.1,0.15,0.2,0.5\}$ and report the best accuracy.


The experiment results for $\kappa= \{0.1 \sim 0.5\}$ and $\beta=0.05$ are in \cref{fig:cifar10_inc_acc}. Overall, IVON outperforms LS and SAM in all noise levels. Meanwhile, IVON can learn in very noisy scenarios $\kappa=\{0.4, 0.5\}$ while baselines can only reach around $10\%$ accuracy. The experiment results support our claim that adaptive label noise induced by variational learning is more effective than traditional label smoothing.


\subsubsection{Uncontrolled Noisy Datasets}
We now report results on Clothing1M \citep{xiao2015learning}, a large-scale dataset that features natural label noise from the web and consists of 1 million images across 14 categories. We conduct experiments by using ResNet-50 as the model. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.92\linewidth]{figures/clothing1M_results/clothing1m_all.pdf}
	\caption{ Clothing 1M experiment result. The result is similar to synthetic noisy datasets reported in \cref{fig:cifar10_all} and \cref{fig:cifar100_all}. Results are reported over 5 seeds.}
	\label{fig: clothing1m_all}
\end{figure}

The results on Clothing1M, illustrated in \cref{fig: clothing1m_all}, demonstrate that IVON outperforms Label Smoothing and is comparable to SAM. This experiment shows that IVON's performance is consistent in the large scale dataset. 

