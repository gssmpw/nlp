
\ificml 
\vspace{-0.2em}
\fi
\section{Experiments}
\label{sec:experiments}

\ificml 
\vspace{-0.1em}
\fi
In this section, we evaluate \OURS on two natural language processing (NLP) benchmarks and report end-to-end LLM performance metrics. We provide a comparison with the more important baselines. Note, Appendix \ref{app:lshvariants} provides an additional comparison with LSH partitioning. 

\begin{figure*}
%
%
    \includegraphics[width=0.45\linewidth]{figs/e2e_NIH.pdf}
    \hfill
    \includegraphics[width=0.45\linewidth]{figs/e2e_infinity.pdf}
    \vspace{-0.5em}
    \caption{
    Scores of Needle-in-haystack (left) and the En.QA task from InfiniteBench (right) as a function of the budget (fraction of KV-cache visited), on a Llama 3 8B with scaled rope to 500k. 
    We compare several ways of assigning keys and queries to buckets.
    We also show the score with no sparse attention (StreamingLLM) and the full attention score. 
    }
    \label{fig:accuracyatbudget}
\end{figure*}



\subsection{Models, measures, benchmarks and baselines}

We perform experiments using a Llama 3.1 8B, a LLM with 32 layers, 32 query heads and 8 key heads.
This model has been trained for context sizes up to 128k tokens. 
We extend the context size to 500k tokens using  rope scaling~\citep{peng2023yarnefficientcontextwindow}. 
We do not fine-tune the model weights. %

\OURS first partitions the keys with a pre-trained k-means and trains a Multilayer Perceptron (MLP) in order to assign queries to buckets (offline), denoted by \textit{K-means+Q-model}. The MLP has two layers with an intermediate dimension of 1024, batch normalization and a ReLU nonlinearity.
We use the \OURS assignment models on PG19 books~\citep{raecompressive2019} for the MLP training.
These contain natural-language text, but may be out-of-distribution for math or coding tasks.
We sample key and query vectors across the prompt for training, and keep only long-range queries: we discard all queries whose distance to the top-1 matching key is less than 1024 tokens.
We use frozen key and query embeddings for training. The training takes around 25~minutes per head on one GPU. 
This is not a limitation since the training is meant to be re-used across contexts and prompts.

%
%
%
%


%

%

%
\paragraph{Performance metrics: selectivity and generation timings. }
The main goal of \OURS is to speed up the attention computation and, consequently, decoding.
Therefore, we measure and report resource consumption as a trade-off between accuracy and computing speed. 
We measure the computing speed in two ways: 
the first one is the \emph{selectivity}~\citep{zhuoming2024magicpig,pauleve2010locality}, or the percentage of keys that are involved in the computation, averaged over all heads; 
the second one is a direct \emph{timing} of the end-to-end generation.
We used H100 GPUs to measure the timings.

%
%
%

\paragraph{Benchmarks.}

We evaluate the end-to-end accuracy on long-context tasks that require different types of lookups~\citep{li2024evaluating}, more precisely:
 \begin{itemize}
\item 
\textit{Needle-in-a-Haystack}~\citep{NIHGithubURL} is a benchmark that consists on a ``hard'' text matching task where a secret, short text (the needle) is inserted at an arbitrary position in an arbitrarily long context made of book texts (which are in-domain for the training data). 
The prompt then attempts to retrieve the text.
We average the Needle-in-a-Haystack scores over prompt lengths between 10k and 128k,  reporting separately for lengths between 128k and 500k, and for various insertion points for the needle. 

%
%
\item
\textit{InfiniteBench benchmark}~\citep{zhang2024infinitybench} is a series of tasks intended to probe LLM reasoning capabilities beyond a 100k context size. 
The benchmarks relate to question answering, coding and math questions as well as retrieval tasks for numbers, passkeys and KV.
%
%
%
%
\end{itemize}


\paragraph{Baselines.} 

We compare \OURS to the following methods:  
\ificml \\[-2.3em] \fi

\begin{itemize}
\item 
\textit{Full attention} is the default brute-force attention computation. 
We use FlashAttention-v2~\citep{daoflashattention}, which is a strong open-source reference implementation for long-context attention.  \\[-1.8em]
%
\item 
\textit{K-means} refers to a partition-based approximate attention using k-means partitioning.
We report results with k-means directly applied to the full keys and queries, as well as applied to keys and queries without the rope transformation.
%
%
This is similar to Faiss' IVFFlat index \citep{douze2024faiss}, that has been used as a baseline by \cite{liu2024retrievalattention}, but not restricted to the top-k nearest keys (see Section~\ref{sec:implem}). \\[-1.8em]
\item 
\textit{StreamingLLM}~\cite{xiao2024efficientstreaminglanguagemodels} this corresponds to setting $\nprobe=0$ in IVFFlat, using only the dense part of the attention computation. 
\ificml \\[-1.8em] \fi
\end{itemize}


%


\subsection{Comparison with baselines}
\label{sec:baselines}

\begin{table*}
%
    \centering
    \caption{
    Scores with a Llama 3.1-8B model using rope extrapolation to increase the context length of 500k and different partitioning schemes. We report in \textbf{bold} the best results among approximate attention schemes. 
    \label{tab:scores}
    }
    \scalebox{0.87}{
\begin{tabular}{l|rr|rrrrrrr}
    \toprule
    Methods & \multicolumn{2}{c|}{NIH} & \multicolumn{7}{c}{InfiniteBench~\citep{zhang2024infinitybench}} \\
& 128k & 500k & Retr.N& Retr.P&Retr.KV& Math.F & Code.D & En.QA & En.MC\\
    \midrule
\textcolor{gray}{Full attention}  & \textcolor{gray}{100.00} & 	\textcolor{gray}{100.00} &\textcolor{gray}{100.00}  & \textcolor{gray}{100.00} & \textcolor{gray}{21.40}	& 	\textcolor{gray}{30.29} &\textcolor{gray}{31.98}&\textcolor{gray}{29.08}&\textcolor{gray}{48.03}\\
K-means (roped inputs, 
%
$\nprobe=32$) & 95.48 & 69.33	 &98.98& 100.00 &4.40& 26.00&31.47&25.42& 48.03 \\
K-means (no rope,
%
$\nprobe=32$) & 100.00 & 98.67	 & 99.66 & 100.00 & 4.40&\bf{34.29}&31.47&29.58&50.66  \\
StreamingLLM (1+2047) & 9.05 & 6.67 &1.69  & 1.69	 & 0.8	 &20.29 &\bf{31.97}&11.60&46.72 \\
\OURS, K-means+Q ($\nprobe=32$)
%
& \bf{100.00} & 	\bf{100.00} & \bf{100.00} &\bf{100.00}& \bf{7.40}& 32.57&30.20&\bf{29.68}& \bf{50.21}\\
\hline
\OURS, K-means+Q ($\nprobe=32$, selectivity)
%
& 4.0 & 5.3	 &15.0 &13.8&21.9 & 47.8& 4.8&3.7&3.7 \\
    \hline
  \end{tabular}}
\end{table*}
\begin{table*}[ht]
    \centering
    \caption{
    Comparison of \OURS with a few other methods from the state of the art. 
    The base model is a Llama 3 8B finetuned by Gradient AI for context length of 262k.
    Rows marked with * are reported by~\citet{liu2024retrievalattention}.
    }
    \smallskip
%
%
\scalebox{0.95}{
  \begin{tabular}{l|rrrrrrr}
    \toprule
    &  \multicolumn{7}{c}{InfiniteBench~\citep{zhang2024infinitybench}} \\
    Methods &  Retr.N& Retr.P&Retr.KV& Math.F & Code.D & En.QA & En.MC\\
    \hline
    \textcolor{gray}{Full attention*} & \textcolor{gray}{100.0} &\textcolor{gray}{100.0}&\textcolor{gray}{17.5}& \textcolor{gray}{19.0} &\textcolor{gray}{39.5} & \textcolor{gray}{9.1} & \textcolor{gray}{68.0}\\
    StreamingLLM~\citep{xiao2024efficient} * &5.0 &5.0 &1.0&18.5 &39.5 &5.9 &66.5\\
    SnapKV~\citep{li2024snapkv} * &100.0 &100.0&0.5&18.0 &40.0 &11.8 &67.0\\
    InfLLM~\citep{xiao2024infllm} * &100.0 & 100.0&0.5&20.5 &48.0 &7.0 &37.0\\
    RetrievalAttention~\citep{liu2024retrievalattention} * & 100.0 &100.0&9.0/14.0& 19.0 & 40.0 & 7.5 &67.0\\
    \midrule
    \textcolor{gray}{Full attention (reproduced)}   &\textcolor{gray}{100.0}&\textcolor{gray}{100.0}&\textcolor{gray}{16.0}& \textcolor{gray}{41.5} & \textcolor{gray}{24.5}&\textcolor{gray}{10.3}  & \textcolor{gray}{54.5}\\
    \OURS, K-means+Q ($\nprobe=$ 32) 
    %
    &100.0& 100.0 & 5.5 & 38.5 &25.5&11.6&56.0 \\
    \hline
    \OURS, K-means+Q ($\nprobe=$ 32,  selectivity)  
    & 25.8 & 25.9  & 19.1 & 51.0 & 4.8 & 4.3 & 4.3\\
    \bottomrule
  \end{tabular}
    \label{tab:SOTA}}
   \bigskip  ~
\end{table*}


\autoref{fig:accuracyatbudget} shows the trade-off between budget and score (we report detailed numbers in~\autoref{tab:scores}). 
%
%
The operating point is controlled by setting the query-time $\nprobe\in \{0, 4, 8, 16, 32, 64, 128\}$. We also show 1) the $\nprobe=0$ setting (\textit{StreamingLLM} ~\cite{xiao2024efficientstreaminglanguagemodels}), that corresponds to the bottom line; and 2)
the full attention, where the keys are compared exhaustively, the topline.

Needle-in-a-Haystack scores reach 100\% accuracy from $\nprobe$\,$=$\,$8$ for context sizes up to 128k, but requires $\nprobe$\,$=$\,$32$ beyond that. 
%
On InfiniteBench, the behavior depends on the tasks. 
 In~\autoref{fig:accuracyatbudget}, En.QA exhibits is a clear positive trend, the topline accuracy is reached with around 5\% selectivity. 
 
For Code.D and En.MC the results for $\nprobe = 0$ and full attention are close, so the scores are noisy. 
For Math.F, the selectivity increases quickly with $\nprobe$, which is indicative of an OOD behavior between training and testing prompts. See also ~\autoref{fig:ibres} in ~\autoref{app:infinitebenchres}.   
This is because for this task, the context consists of a long lists of numbers, which is very different from the books on which the assignment models are trained. 
This causes a large imbalance factor.

Interestingly, the \OURS{} score is higher than the full attention score. 
This may be because the sparse attention focuses only on a relevant subset of the prompt instead of averaging information from the entire sequence.



\subsection{Timings}


End-to-end timings do not only depend on the budget, but also on how balanced the clusters are and whether the heads all perform the same number of FLOPs. 
This is due to the parallelization over GPUs and over computation cores that we use (Section~\ref{sec:implem}).

We measure the attention computation speed for one head on a 171k context (with $\nprobe=32$, yielding 100\% needle-in-a-haystack accuracy). 
We compare against a strong attention implementation, FlashAttention-v2~\cite{dao2023flash}. 

%
%
%
\begin{center}
\scalebox{0.95}{
\begin{tabular}{l|rr}
\toprule
& FlashAttention-v2 & \OURS $\nprobe$=32 \\
\midrule
selectivity  & 100\% & 4.4\% \\
runtime & 50$\mu$s & 18$\mu$s \\
\bottomrule
\end{tabular}
}
\end{center}


The FLOPs necessary to compute the sparse attention are about 20$\times$ lower than for the full attention. 
The reduction of runtime is 65\%, because the computation is not as well optimized as FlashAttention and includes overheads. 






\subsection{Comparison with the state of the art}

For comparison with the SOTA method RetrievalAttention~\cite{liu2024retrievalattention}, we run \OURS on a Llama 3 model fine-tuned by Gradient AI to support 262k contexts. 
We set $\nprobe=32$, yielding a selectivity around 5\% for textual content. 

\autoref{tab:SOTA} shows that the baseline numbers are often different, probably because of slightly different experimental settings (prompts). 
However, the gap between \OURS and the full attention baseline is very similar to the RetrievalAttention results. 
%
Note that in our experimental setting, the dense part of the attention is 1+2047 tokens, while it is 128+512 for RetrievalAttention. 
However, for long-range tasks, this does not matter much as is still a tiny fraction of the attention size (see the ablations in Section~\ref{sec:ablations}). 

%

The critical advantage of \OURS is that the slowest stage (training) is performed offline, while RetrievalAttention's RoarGraph index~\cite{chen2024roargraph} has to be built once the context is available, which takes several minutes for a 100k context. 
As mentioned in~\autoref{sec:related}, this makes graph-based indexing of a KV cache attractive only in settings where the same KV cache is reused multiple times.
For \OURS, the pre-fill of our index takes a few seconds on modern GPUs. 

%
%
%
%
%
%




\subsection{Ablation experiments}
\label{sec:ablations}

In this section, we run \OURS with some variations on the default design, and measure their impact. 

\paragraph{Q-model architecture and training.}

In Appendix~\ref{app:archablation}, we analyze some variants of the Q-model training. 
This analysis shows that using long-range queries-key pairs for training is useful. 
Using a residual architecture rather than a simple MLP does not help. 

\paragraph{Dense context size.}

\OURS's default dense context is 2048 tokens (the attention sink of size 1 plus the 2047 most recent keys), see Section~\ref{sec:densepart}. 
We experiment with some other settings for attention sink+dense context: 1+511, 1+63 and 128+512. 
The results in Appendix~\ref{app:densecontext} show that the performance remains similar. 

\paragraph{Batched queries.}

\OURS combines all four queries belonging to the same group into a single search, see Section~\ref{sec:implem}. 
These queries could also be performed independently. In Appendix~\ref{app:archablation}, we show that batching the queries corresponding to the same KV head outperforms ranking the buckets independently for each query.


\balance



%

%

%

%

%

