
\section{Introduction}
\label{sec:intro}



Current transformer-based large language models (LLMs) ~\citep{devlin2019bertpretrainingdeepbidirectional,chowdhery2022palmscalinglanguagemodeling,brown2020language, Vaswani2017AttentionIA} implement contextual memory under the form of self-attention: The model attends to previously processed tokens in order to predict the next one.
To avoid recomputing the previous keys and values, a key-value (KV) cache is stored and reused, allowing for faster next-token prediction. 

%
%
%

The KV cache is memory intensive: on a Llama 3-8B model~\cite{dubey2024llama3herdmodels}, a single token involves a cache of size 128~kiB -- about 10000 times larger than storing the token itself.  
%
%
%
%
%
%
%
Therefore storing the KV cache becomes increasingly memory-demanding as the context length increases.
This is partially addressed by quantization to compress key and value vectors \citep{shi2024keep}. However compression involves a trade-off, as the generation quality can suffer from information loss. 
%
%
%
%
%
%
%
%
%
%
Long-context self-attention is also computationally expensive, as it requires matching queries one by one to the entire KV cache, which becomes increasingly problematic as the context grows.

In this work, we address this challenge by adopting a vector search perspective: we regard self-attention as a setting where a query vector is matched with a database of vectors (the key vectors in our case). The goal is to retrieve a subset of relevant vectors. 
The self-attention softmax can then be computed on this restricted set of $k$ vectors, yielding an approximation that improves in accuracy as $k$ increases.
%
An efficient implementation of this retrieval with exhaustive search %
%
%
%
can improve the speed only to some extent, 
since all keys still need to be accessed even if only $k$ values are used. 
%
%

Instead, we advocate approximating nearest-neighbor search by partitioning the keys into buckets. 
Practically, this enables a sparse, data-dependent, self-attention: 
At search time, we leverage only a small subset of buckets for each query, reducing memory access, and therefore time complexity.
Multiple choices exist for the partition: random projections, as in original Locality Sensitive Hashing methods~\cite{datar2004locality}, or data-dependent partitions, like k-means. The latter offer better trade-offs in practice on typical approximate neighbor search tasks ~\cite{pauleve2010locality}. 
%
%
When using this approach for retrieving keys, the slow k-means training is performed offline on a held-out training set of keys.  
After this offline phase, and during the pre-fill phase with a new context and therefore new keys, performing the assignment to a given bucket of the partition is fast, as it amounts to finding the nearest centroid for each new key. %

However, leveraging approximate k-nearest neighbor (ANN) methods to improve the KV cache associative faces several limitations and challenges:
\ificml \\[-2.3em]\fi

\begin{enumerate}
\item 
keys and queries have very different distributions, hence vector search is out-of-distribution in this setting. This hinders the effectiveness of indexing algorithms.  \\[-1.8em]
\item 
top-$k$ search is harmfully restrictive for queries for which there is significant useful information in the keys that are not retrieved.  \\[-1.8em]
\item 
current brute-force attention implementations, such as FlashAttention-v2~\citep{daoflashattention} are highly tuned for the training and inference hardware. In contrast, many ANN algorithms are not GPU-compliant. 
\ificml  \\[-2em] \fi
\end{enumerate}

%

To address these limitations, we propose a novel approach called {\bf S}parse {\bf A}ttention with {\bf A}symmetric {\bf P}artitions (\OURS), which casts the partitioning of key and query vectors as a classification task: the bucket membership is predicted separately for keys and query vectors. %
%
%
In addition to this approach, we make the following contributions: 
\ificml \\[-2.3em] \fi

\begin{enumerate}
\item 
     We quantitatively evaluate and analyze the out-of-distribution nature of query-key matching in Section~\ref{sec:motivation};  \\[-1.6em]
\item 
     We mitigate the loss of information that occurs when one limits the attention to top-$k$ keys.
     Instead we leverage the partition structure (Section~\ref{sec:beyondtopk}), which leads to a variable size sparsity pattern, and show that this choice improves the self-attention approximation ; \\[-1.6em]
     %
\item 
     We propose a way to efficiently run the partition-based search on GPUs, based on an optimized batched implementation (Section~\ref{sec:implem}).  
\ificml \\[-1.8em] \fi
\end{enumerate}

Our \OURS approach reduces the resources needed to compute the self-attention of a model without finetuning it, in a hardware-compatible manner.  \OURS decreases the time taken by self-attention kernels by more than 60\% compared to FlashAttention-v2 without compromising the generation quality, as shown by experiments in Section~\ref{sec:experiments}. 


%
%
%
%
%
%
%


%
%


%
%
%
%
%
%
%
%
%
%
%



%

%

%

%

%

 %

%


%

%





