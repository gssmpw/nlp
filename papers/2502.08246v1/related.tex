
\section{Related work}
\label{sec:related}


\begin{table}
    \centering
    \caption{\OURS \emph{v.s.}  comparable KV-indexing methods: fast implementations like flashattention-v2 corresponds to the baseline. The others are indexing methods that induce sparse-attention. }
    \begin{tabular}{@{\ }l@{\ \ }|@{\ \ }cccc@{\ }}
    \toprule
           & \begin{minipage}{1.2cm}\ \ \ Full  \\
           attention\end{minipage} %
           & LSH %
           & \begin{minipage}{1.2cm}Retrieval \\
           Attention\end{minipage} %
           & \OURS \\
           \midrule
    Frozen LLM         & \checkmark & \checkmark & \checkmark  & \checkmark \\  
    Sparse attention   &            & \checkmark & \checkmark  & \checkmark \\
    Fast index build   & \checkmark & \checkmark &             & \checkmark \\
    GPU compliant      & \checkmark & \checkmark &             & \checkmark \\
    Data adaptive      & \checkmark &            &  \checkmark & \checkmark \\
    Approximation      & exact      & medium     & good        & good  \\ 
    \bottomrule
    \end{tabular}
    \label{tab:sparsemethods}
\end{table}

In this section, we review related work on accelerating attention for autoregressive transformers. In Table~\ref{tab:sparsemethods}, we compare our method with a selection of the most relevant approaches along with their characteristics. 


\paragraph{KV cache compression.}

Multiple techniques address the large size of the KV cache for long sequences. 
The approaches for this are often based on vector compression and pruning, similar to neural network pruning \citep{lecun1989optimal}  and compression~\citep{han2015deep}.
%
The most straightforward way is to adopt low-precision computation, or more generally, variants of scalar quantization, to the KV cache.
%
This is effective until the number of bits per weight reaches a minimum (around 4 bits), below which the compression severely degrades the accuracy~\citep{li2024evaluating}, even when the compression is accounted for during training~\citep{adepu2024framequant} using quantization-aware training~\citep{hubara2018quantized}. 
The keys and values can also be compressed by reducing the dimensionality of the vectors, \eg with Principal Component Analysis~\citep{kang2024gear}.

Another approach to compress the KV cache is to prune the least-used keys and vectors by measuring their utilization on-the-fly~\citep{ge2023model}, maintaining a cache of the most used vectors~\citep{liu2024scissorhands}, ignoring the oldest keys~\citep{xiao2024efficientstreaminglanguagemodels}, or using a heuristic such as H2O~\citep{zhang2023h2o}.
The GEAR method~\citep{kang2024gear} combines pruning, dimensionality reduction and vector compression.
For more context on KV cache compression and compute optimization, we refer the reader to the recent overview by \citet{shi2024keep}.

Our \OURS approach is complementary to compression and static pruning. 
Indeed, partition-based vector search techniques are often combined with compression, see for instance the canonical IVFADC method~\citep{jegou2010product}. 

%

%

%
%

%

%
%

\paragraph{Vector search data structures.}

Several families of vector indexing structures have been applied to KV cache indexing.  
In the vector search literature, the ``keys'' are called ``database'' or ``reference'' vectors. 
%
Tree-based indexes are inspired by one- and low-dimensional search such as the seminal KD-tree~\cite{bentley1975multidimensional}, 
%
but are not effective in high dimensions. 
%
%
 Locality Sensitive Hashing (LSH) is based on hash tables where buckets store the database vectors~\citep{datar2004locality}. 
The advantage of LSH is its fast indexing time, as it amounts to computing a limited number of dot products for each key. 
This advantage has led to its use in several recent works on KV cache indexing~\citep{zandieh2023kdeformer,zhuoming2024magicpig}. 

\textit{Remark:} Appendix~\ref{app:lshvariants} discusses LSH-based methods and shows that it performs poorly in our context: vanilla LSH is not data-adaptive, so the number of independent hash tables has to be increased significantly to achieve a decent retrieval accuracy. This consumes a lot of memory and overall does not provide a significant gain with pretrained models.  
%

Graph-based indices store database vectors as nodes and searching hops from node to node~\citep{dong2011efficient,malkov2018efficient,fu2017fast,chen2024roargraph,jayaram2019diskann,ootomo2024cagra}. 
Graph-based search is efficient because many routing decisions are built into the graph during its construction. 
Therefore, RetrievalAttention~\citep{liu2024retrievalattention} applies graph-based index RoarGraph~\cite{chen2024roargraph} to the KV cache. 
The downsides are that the index building is slow and the graph is bulky to store, as each node is linked to dozens of neighbors. 
The consequence is that graph indexing of a KV cache is attractive only if the same KV cache is reused, \eg because multiple prompts use the same context. 

\paragraph{Vector search based on partitions.}

\newcommand{\nlist}{C}
\newcommand{\nprobe}{\ell}

The reference dataset is divided into $\nlist$ buckets.
Partitions are defined by a function that assigns each vector to one bucket. 
The classical variant employs an inverted file (IVF) \cite{witten1999managing}. 
When the partitioning is based on k-means, as advocated by \citet{pauleve2010locality}, the usage is as follows: 
%
\ificml
\\[-2.1em]
\fi

\begin{itemize}
\item At training time, k-means clustering to the $N$ database vectors provides a set of $\nlist$ centroids.  \\[-1.7em]
\item At indexing time, the assignment of a new vector amounts to finding its nearest centroid.  
Ideally, the IVF structure stores the vectors associated to one bucket in contiguous memory.  \\[-1.7em]
%
\item At search time, the query vector is likewise assigned to the nearest centroid and all the database vectors found in the bucket are compared with the query vector to find the top-$k$ nearest neighbors. 
A straightforward extension is to visit \emph{several} 
%
nearest buckets instead of the single nearest one \citep{lv2007multi}. 
\ificml
 \\[-1.9em]
 \fi
\end{itemize}

The NeuralLSH method~\cite{dong2019learning} predicts the bucket corresponding to the queries and database vectors alike using a neural network. 
%
In contrast with our approach it does not attempt to handle out-of-distribution data. 

Vector search tooling such as Faiss has been used for KV cache lookups in the Unlimiformer method~\citep{bertsch2024unlimiformer}, in the setting of an encoder-decoder LLM where the entire decoder shares a single cache, independently of the layer. 
This factorization is not possible in decoder-only LLM architectures, where each layer and each head needs to store the (key, value) pairs independently.
%



%

%


%


\paragraph{Out-of-distribution (OOD) search.}

By default, vector search assumes query and database vectors follow the same distribution. 
OOD search is needed when it is not the case, for example if the query and database vectors come from different modalities as with text-to-image search~\citep{simhadri2024results} or user-to-item search in recommendation systems~\citep{paterek2007improving}.
%
The effects of OOD search on partition-based indexes, as identified by \citet{jaiswal2022ood} and \citet{chen2024roargraph}, are that 
(1) the nearest database vectors to the query are spread over many buckets, and 
(2) the query assignment does not visit the correct buckets.
The authors adapt graph-based search to OOD search by taking into account how the graph navigation must be re-routed, based on a training set of query vectors. 

%

%
%
%

KV cache indexing has challenging OOD characteristics that we discuss in Section~\ref{sec:motivation}, and that our method \OURS addresses in a principled way in  Section~\ref{sec:method}. 
%
Note, Dedrift~\citep{baranchuk2023dedrift} addresses temporal drift in vector databases for partition-based indexes by re-training mid-way during key ingestion. 
OUr \OURS approach mitigates the temporal biases in a simpler way, see  Section~\ref{sec:derope}.
%

%
%

%
%



%
%



