\section{Conclusion}

We introduced \OURS, a new method for non-exhaustive attention computations in LLMs. 
\OURS is data adaptive, while incurring negligible run-time or memory overhead compared to exhaustive attention. 
We show the efficiency of \OURS for typical long-context tasks, with significant computation speedups and a small impact on accuracy.


There are numerous more complex variants we tried which did not yield consistent improvements: 
increasing the model capacity, 
training a key classification model, 
fine-tuning the query classification model to the current prompt, 
using causal masking in the training batches, 
training on book summarization data rather than books to force long-range queries. 
In general, we observed that good results from experiments at the level of one head do not guarantee an improved e2e performance. 
This is problematic, because the training itself is local to one head in the current setting. 

%
There are natural extensions to \OURS that we leave as future work. 
\OURS can be combined with vector compression to reduce the amount of GPU memory used. 
Currently, we apply \OURS at the generation stage because this is when memory I/O has the strongest impact on performance, but it should be possible to improve pre-fill time as well. 
%




%

%

