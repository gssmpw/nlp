
%
\newcommand{\igN}[1]{\includegraphics[width=0.25\linewidth]{figs/LSH_comparison/LSH_variants_#1.pdf}}
\begin{figure*}
    \centering
    \igN{layer11_head0}%
    \igN{layer13_head2}%
    \igN{layer14_head6}%
    \igN{layer15_head5}   
    \caption{
MSE vs. selectivity for three LSH variants, compared to a random selection and a partition based on k-means (the baseline for \OURS{}), on 4 representative heads. 
    }
    \label{fig:LSHvariants}
\end{figure*}



\section{Partitioning the KV-cache with LSH}
\label{app:lshvariants}

Locality Sensitive Hashing is a generic term that covers several algorithms and variants.
The common component is that the vector to hash $x\in\mathbb{R}^d$ is projected onto $r$ directions drawn randomly, which yields the $r$-bit bucket number:
\begin{equation}
    B(x) = \sum_{i=0}^{r}
    2^i \mathbf{1}[x^\top\pi_i > 0] \in \{0,\dots,2^r - 1\}
    \label{eq:lshone}
\end{equation}

\paragraph{Multiple hash tables}

MagicPig~\cite{zhuoming2024magicpig} uses \emph{several} (up to $L=200$) hash tables and the vector search relies on collisions between the buckets: a key is considered only if it is hashed together with the query in at least two hash tables. 
Using such a large number of hash tables is classical for LSH, since a single hash table is very unbalanced. 
The theoretical guarantees of LSH apply only when the number of hash tables is large. 

Therefore, it is not strictly speaking a partitioning method of the keys. 
Besides, the large number of hash tables means that the memory used for that indexing structure is larger than the KV-cache itself ($4L = 800$ bytes per vector to store, compared with the $2\times2\times 128=512$ bytes to store the key-value pairs). 
This is why MagicPig stores the hash tables in CPU RAM. 

\paragraph{A single hash table?}

In the KDEformer method of~\citet{zandieh2023kdeformer},  a single hash table is considered, with a large $r$, so that the buckets are sparse. 
The bucket numbers corresponding to they keys $B(k)$ are ordered by their Gray codes\footnote{\url{https://en.wikipedia.org/wiki/Gray_code}} (which the authors re-invent).
Then the sequence of keys ordered in this way is partitioned in equal sized bins.  
At query time, only the bin the query vector ``falls'' in is visited. 

Since successive binary strings ordered by Gray codes differ by only 1 bit, the hope is that nearby buckets end up being nearby in this ordering.  
However, it is not guaranteed that if $B(k)$ and $B(k')$ differ by only 1 bit, then they will be nearby in this enumeration. 
Indeed, the bit that they differ by is not necessarily the one that changes in successive Gray codes. 
Besides, it is \emph{also} unlikely that many keys differ by only one bit because if $r$ is large then most vectors will differ by more than one. 


\paragraph{Alternative hash functions}

In the Reformer of~\citet{kitaev2020reformer}, the hashing function is replaced with the maximum dot product of random directions:
\begin{equation}
    B(x) = \underset{i=1..2^r}{\mathrm{argmax}}~~ x^\top\pi_i
\end{equation}
Compared to Equation~(\ref{eq:lshone}), each random projection only generates a single bucket instead of a bit for the bucket number.  
This is close to our k-means approach, if the centroids were drawn randomly. 

Similar to KDEFormer, the sequence of keys is sorted and split arbitrarily, but the bucket numbering does not have a significance here. 
The authors observe that it is beneficial to use multiple hash tables. 

\paragraph{Discussion}

The theoretical grounding of these LSH based methods is fragile. 
They rely on a supposed property explicited in the HyperAttention paper~\cite{han2023hyperattention}:
\begin{quote}
 A very
useful property of this LSH variant is that its buckets are ordered in such a way that geometrically
adjacent buckets have consecutive buckets.    
\end{quote}
The fundamental error in this reasoning is that it is not possible to map a high-dimensional space to a sequence while maintaining neighborhood relations, otherwise nearest neighbor search in high dimensions would be as easy as in 1D. 

Besides, the subsequent arbitrary splitting of linear sequences into regular buckets defined to minimize computations defeats the purpose of the original data buckets. 

\paragraph{Experiment}

We compare different LSH approaches in the tradeoff between selectivity and mean squared error (MSE) for a few attention heads. 
The MSE is computed between the approximate attention output and the exact one. 
We use random sampling as the baseline. 
Since the attention mechanism can be seen as Gaussian kernel smoothing~\cite{zhang2024memory}, random sampling is actually a reasonable estimator, that reaches MSE=0 when all vectors are sampled (we use sampling without replacement). 

All the variants have a parameter that adjusts the tradeoff:  
for KDEFormer and Reformer, fewer larger buckets improve the accuracy and increase the computational cost (1 bucket covering all keys and queries is equivalent to full attention); 
for MagicPig's LSH with multiple hash tables, increasing the number $L$ of hash tables improves the coverage of the key vectors. 

Figure~\ref{fig:LSHvariants}  shows that for this metric, the KDEFormer and Reformer are on-par or slightly worse than random sampling. 
Note, however, that these two paritioning methods were developed to be used at pre-fill time, so their performance could be better in a self-attention setting where the contexts embeddings are matched among themselves. 
In fact, Reformer uses the query corresponding to a key to assign that key to a bucket instead of attempting to assign the key itself. 
MagicPig implements the classical multi-hashtable LSH, so it performs better, but note that since MagicPig uses multiple hash tables, it does not yield a (single) partition of the dataset. 
The k-means approach obtains much better results, mainly because it is data-adaptive. 

These LSH based methods are confronted with the same bucketing balancing issues that we encounter in \OURS.
MagicPig resolves this balancing issue by running the search on CPU, that is more tolerant to irregular computation patterns than GPUs. 
The two other variants arbitrarily split the keys and queries into fixed-size buckets that can be handled easily by block computations.  
\OURS attempts to balance the buckets at training time with a specific loss term. 

%

%

%
%
%
%
%
%
%
%
%
%


\section{Ablation experiments}

\paragraph{Architecture and training setting}
\label{app:archablation}

Figure~\ref{fig:nih-arch} shows the performance on the NIH and En.QA tasks using different architectures.
It is comparable to Figure~\ref{fig:accuracyatbudget} with a few more variants for the Q model: 
\begin{itemize}
    \item 
    ``Q-model short-range'' is the standard 2-layer MLP, but trained without filtering out short-range queries ;
    \item 
    ``Q-model residual'' relies on a residual architecture with the same capacity as the MLP, also trained without filtering out short-range queries.
\end{itemize}

The NIH plot shows that training on long-range queries only improves the accuracy significantly. 
Besides, the residual architecture is not an improvement over the MLP. 
The En.QA results are more contrasted but the task is more noisy, so we chose to standardize on the MLP architecture with short-range filtering. 

\begin{figure*}
    \centering
    %
    \includegraphics[width=0.5\linewidth]{figs/e2e_NIH_abl_1.pdf}%
    \includegraphics[width=0.5\linewidth]{figs/e2e_infinity_abl_1.pdf}  
    \caption{
        Comparison of architectural variants on the needle-in-haystack (left) and InfiniteBench En.QA (right) tasks. 
    }
    \label{fig:nih-arch}
\end{figure*}

\paragraph{Dense context size}
\label{app:densecontext}

Figure~\ref{fig:nih-dense} shows the performance of various dense context sizes, which remain similar.

\begin{figure*}
    \centering
    \includegraphics[width=0.5\linewidth]{figs/e2e_NIH_abl_2.pdf}%
    \includegraphics[width=0.5\linewidth]{figs/e2e_infinity_abl_2.pdf}  
    \caption{
        Comparison of dense context sizes on the needle-in-haystack (left) and InfiniteBench En.QA (right) tasks. 
    }
    \label{fig:nih-dense}
\end{figure*}

\paragraph{Batched queries}
\label{app:batchedqueries}

Figure~\ref{fig:nih-batched} shows the effect of batching the 4 queries belonging to the same KV head.

\begin{itemize}
    \item 
    "Batched": Jointly rank partitions across multiple queries to identify the top-k partitions to access.
    \item 
    "Independent": Access the top-k ranked partitions for each query separately.
\end{itemize}

\begin{figure*}
    \centering
    \includegraphics[width=0.5\linewidth]{figs/e2e_NIH_abl_3.pdf}%
    \includegraphics[width=0.5\linewidth]{figs/e2e_infinity_abl_3.pdf}  
    \caption{
        Comparison of batched and independent queries on the needle-in-haystack (left) and InfiniteBench En.QA (right) tasks. 
    }
    \label{fig:nih-batched}
\end{figure*}

\section{InfiniteBench results}

\paragraph{Additional results} Figure~\ref{fig:ibres} complements ~\autoref{fig:accuracyatbudget} and illustrates the task-dependent behaviors described in Section~\ref{sec:baselines}.
\label{app:infinitebenchres} 

\begin{figure*}
    \centering
    \includegraphics[width=0.5\linewidth]{figs/e2e_infinity_det_Retr.N.pdf}%
    \includegraphics[width=0.5\linewidth]{figs/e2e_infinity_det_Retr.P.pdf}
    \vspace{10pt}
    \includegraphics[width=0.5\linewidth]{figs/e2e_infinity_det_Retr.KV.pdf}%
    \includegraphics[width=0.5\linewidth]{figs/e2e_infinity_det_Math.F.pdf}
    \vspace{10pt}
    \includegraphics[width=0.5\linewidth]{figs/e2e_infinity_det_Code.D.pdf}%
    \includegraphics[width=0.5\linewidth]{figs/e2e_infinity_det_En.MC.pdf}    
    \caption{
        Performance on selected InfiniteBench tasks 
    }
    \label{fig:ibres}
\end{figure*}

