\section{Related Work}
\subsection{Transformer-based MIL}
TransMIL____ solves MIL problem using two transformer layers with Nyström-Attention____ and a Pyramid Position Encoding Generator (PPEG) to encode positional information. The idea of positional encoding for images originated in the Vision Transformer (ViT), which splits images into square patches, preserving all background patches as valid____. This leads to only minor discontinuity between patches, except at row boundaries. In contrast, when processing Whole Slide Image (WSI), background patches without tissue are typically removed, creating significant discontinuities between the remaining patches (see Figure~\ref{fig:discontinuity}). These discontinuities introduce noise into positional encoding. Although PPEG resizes bag-level features into two dimensions and processes them with convolutions, the one-dimensional positional encoding treats the embeddings as a continuous sequence. This disregards the spatial discontinuities, preventing PPEG from effectively representing the two-dimensional feature map and leading to inaccuracies in convolution operations.

\subsection{Instance-level Learnable MIL}
Recent studies have shown that the instance-level learnability of attention-based and transformer-based MIL models is not guaranteed, both theoretically and empirically____. This limitation arises from the attention pooling operation which multiplies attention weights over instance features, incorporating the hypothesis space for bag-level features into the instance predictions. While much effort has focused on improving MIL from the instance-level perspective, most work aims to fine-tune feature extractors to obtain better representations____. However, the MIL framework itself often remains based on conventional designs. These approaches are computationally expensive and time-consuming, which contradicts the goal of using MIL to reduce computational costs, especially with the availability of foundation models pre-trained on histopathological data____. Therefore, an efficient approach to enable instance-level learning is needed to enhance MIL’s overall capability, with self-distillation being a possible option.

% \paragraph{Knowledge Distillation in MIL.}
DTFD-MIL____ employs feature distillation for two-tier bag-level training, creating smaller pseudo-bags to alleviate the effects of limited cohort sizes. However, DTFD-MIL still focuses on bag-level only training. In contrast, WENO____ incorporates knowledge distillation at both the bag and instance levels. Despite, the alternating training of teacher and student branches, along with distillation on the image encoder, results in high computational complexity.

Unlike the methods discussed above, our PathMIL offers several advantages. In terms of positional encoding, we utilise two-dimensional coordinates to better capture the inter-instance relationships at the bag level, compared to the one-dimensional alternatives. From the perspective of knowledge distillation, our CFSD trains only one attention network and applies self-distillation, eliminating the need for alternating training between teacher and student networks. Leveraging the attention network, CFSD is designed to perform two tasks simultaneously: generating the attention map for bag-level classification and serving as the classifier for instance-level classification, which ensures that knowledge from the bag-level branch is utilised in the instance-level branch, and vice versa. In contrast to WENO, where the image encoder is kept fine-tuning from the bag-level and instance-level training, our CFSD operates at the frozen embeddings, which significantly reduces both computational and time costs. Overall, the design of CFSD facilitates cross-reference between the two branches, and we demonstrate that higher performance improvements can be achieved in this way, even without the need to fine-tune the image encoder.

%Furthermore, the nature of CFSD, operating at the embedding level, significantly reduces both computational and time costs.

% we demonstrate that PathMIL is capable to (1) learn inter-instance contextual information with 2DPE and learn instance-level information with CFSD. Meanwhile, the design of CFSD make the attention net to simultaneously serves for the conventional attention map generation and the instance-level classification, which is highly integrated and efficient. The adaptive threshold scheduling further strengthens the flexibility of CFSD by automatically searching for optimal instance selection threshold during training.