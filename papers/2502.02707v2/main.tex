%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}
\usepackage[preprint]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{condition}{Condition}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Multiple Instance Learning with Coarse-to-Fine Self-Distillation}
\usepackage{multirow}

\begin{document}

\twocolumn[
% \icmltitle{PathMIL: Incorporating Instance-level Assisted Learning and 2D Positional Encoding for MIL in Computational Pathology}
\icmltitle{Multiple Instance Learning with Coarse-to-Fine Self-Distillation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
% \icmlauthor{Shuyang Wu}{med}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Shuyang Wu}{med}
\icmlauthor{Yifu Qiu}{inf}
\icmlauthor{Ines P. Nearchou}{indica}
\icmlauthor{Sandrine Prost}{med}
\icmlauthor{Jonathan A. Fallowfield}{med}
\icmlauthor{Hakan Bilen}{inf}
\icmlauthor{Timothy J. Kendall}{med}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% % \icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% \icmlauthor{}{sch}
% \icmlauthor{}{sch}
\end{icmlauthorlist}
\icmlaffiliation{med}{Centre for Inflammation Research, Institute of Regeneration and Repair, University of Edinburgh, Edinburgh, UK}
\icmlaffiliation{inf}{School of Informatics, University of Edinburgh, Edinburgh, UK}
% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{indica}{Indica Labs, 8700 Education Pl NW, Bldg. B Albuquerque, US}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Hakan Bilen}{H.Bilen@ed.ac.uk}
% \icmlcorrespondingauthor{Timothy J. Kendall}{tim.kendall@ed.ac.uk}
\icmlcorrespondingauthor{Shuyang Wu}{frank.wu@ed.ac.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}
\icmlkeywords{Multiple-instance Learning, Attention, Self-Distillation, Transformer, Computational Pathology}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\ie}{\textit{i.e.}}

\begin{abstract}
Multiple Instance Learning (MIL) for whole slide image (WSI) analysis in computational pathology often neglects instance-level learning as supervision is typically provided only at the bag level. In this work, we present \textbf{PathMIL}, a framework designed to improve MIL through two perspectives: (1) \textit{employing instance-level supervision} and (2) \textit{learning inter-instance contextual information on bag level}. Firstly, we propose a novel \textbf{C}oarse-to-\textbf{F}ine \textbf{S}elf-\textbf{D}istillation (\textbf{CFSD}) paradigm, to probe and distil a classifier trained with bag-level information to obtain instance-level labels which could effectively provide the supervision for the same classifier in a finer way. Secondly, to capture inter-instance contextual information in WSI, we propose \textbf{Two}-\textbf{D}imensional \textbf{P}ositional \textbf{E}ncoding (\textbf{2DPE}), which encodes the spatial appearance of instances within a bag. We also theoretically and empirically prove the instance-level learnability of CFSD.  PathMIL is evaluated on multiple benchmarking tasks, including subtype classification (TCGA-NSCLC), tumour classification (CAMELYON16), and an internal benchmark for breast cancer receptor status classification. Our method achieves state-of-the-art performance, with AUC scores of 0.9152 and 0.8524 for estrogen and progesterone receptor status classification, respectively, an AUC of 0.9618 for subtype classification, and 0.8634 for tumour classification, surpassing existing methods.


% In addition, while MIL has been shown to be capable of bag-level learning, instance-level learnability is not guaranteed. We therefore proposed \textbf{PathMIL} that considers both positional information and the learnability of instances, by incorporating two-dimensional positional encoding (2DPE) and  (CFSD). The 2DPE module mitigates the limitation of instance discontinuity in a background removed image by encoding position with normalised $x$ and $y$ coordinates, while the CFSD employed an instance-level classifier to ensure the instance-level learnability, which proved both theoretically and empirically.  

% Our method is compared to state-of-the-art methods in three types of computational pathology task, namely the breast cancer receptor status classification on our internal cohort of breast cancer biopsy dataset\footnote{Temporarily removed dataset name for blind review.}, subtype classification on TCGA-NSCLC dataset and the tumour classification on the CAMELYON16 dataset. The test results demonstrated the superior performance of \textbf{PathMIL} and proved the efficacy of 2DPE and CFSD.



% obtaining the best AUC/f1-score of $0.8652/0.7118$ and $0.8214/0.7146$ in the classification of estrogen receptor and progesterone receptor status, respectively, the AUC/f1-score of $xxxx$ in subtype classification, and the AUC/f1-score of $0.8786/0.7994$ in tumour classification. At least $13\%$ and $17\%$ of the improvements are witnessed from the perspective of AUC and f1-score compared across the former pioneer frameworks.
\end{abstract}

\section{Introduction}
\label{submission}
Computational pathology (CPATH) has been widely studied in the automatic analysis of digital gigapixel whole slide images (WSIs) and has demonstrated immense potential for oncology and precision medicine~\cite{niazi2019e253, zhang2022tehm, khosravi2022pcr_bc, liang2023macronet, gao2024her2}.
% WSIs are commonly huge in image resolution, causing it being computationally intensive for slide-level training and labour intensive for patch-wise annotation~\cite{srinidhi2021cpath_survey}.
In contrast to regular daily photos, WSIs pose two challenges~\cite{srinidhi2021cpath_survey}. 
Firstly, due to the costly expert annotation, WSIs are typically annotated with slide-level labels.
Secondly, due to their extremely high resolution, it is common to divide WSIs into multiple patches and compute an embedding for each patch independently through a feature encoder, and then concatenate them to form frozen bag-level features (Figure~\ref{fig:frameworks}a).
Hence, the analysis of WSI usually omits the online feature encoder, and patches in negative labelled bags are assumed to be all negative while at least one of them are assumed to be positive in positive bags.
% their very high resolution, each WSI are commonly  huge in image resolution, causing it unavailable for slide-level training and patch-wise annotation~\cite{srinidhi2021cpath_survey}. Therefore, in practice, WSIs are typically cropped into square patches, while a pre-trained feature extractor is used to create patch-level embeddings which are then concatenated to form slide-level embeddings. 
Multiple instance learning (MIL; ~\citealt{dietterich1997mil}) has been the standard machinery to model WSI as bag of patches (or instances) and to learn classifying them from only bag-level supervision. 

% is a weakly supervised learning method that is commonly used to model WSIs as bags, and patches as instances, with supervision usually provided at the bag level.

\begin{figure}[h]
\vskip 0in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/arch_compare.pdf}}
\caption{\textbf{Comparison of popular frameworks with our novel CFSD.} The CFSD can efficiently introduce instance-level learnability by using self-distillation that take one attention network to simultaneously learn knowledge from bag-level and instance-level.}
\label{fig:frameworks}
\end{center}
\vskip -0.2in
\end{figure}

Most conventional MIL frameworks in CPATH built on the success of deep networks. As shown in Figure~\ref{fig:frameworks}, we compared three popular frameworks with our method. The attention-based framework uses gated attention mechanisms~\cite{ilse2018amil, lu_2021_data-efficient, li2021dsmil, chen2022pathomic, chen2022pancancer}, where the latent feature for classification is computed through matrix multiplying a bag-level attention map on the bag-level feature.
Unlike these prior works that independently processes patches, vision transformers~\cite{dosovitskiy2021vit} have recently been applied to the CPATH problems to capture correlations across instances through multi-head attention~\cite{shao2021transmil, zhang2023attentionchallenging}.
% based, leveraging multi-head attention and positional encoding to learn instance correlations~\cite{shao2021transmil, zhang2023attentionchallenging}. 
However, both prior approaches suffer from poor instance classification, as learning to classify at bag evel does not guarantee accurate learning at instance level~\cite{jang2024learnability}.
It is also reported that bag-level classifiers typically tend to focus on few easier positive instances and fail to classify bags correctly when only harder positive are present~\cite{qu2022bidirectional}.

% However, neither of these frameworks is instance-level learnable as the multiplication of attention weights over bag-level features incorporates the hypothesis space for bag-level features into the instance predictions~\cite{jang2024learnability}.
%Self-distillation~\cite{zhang2022self_distillation}, which is a concept inhereted from 
A promising strategy to provide instance-level supervision is knowledge distillation~\cite{hinton2015distillingknowledgeneuralnetwork} by computing the instance-level predictions from the bag-level classifier and using them as pseudo-labels for training a instance classifier. WENO~\cite{qu2022bidirectional} follows this strategy to learn bag and instance classifiers but it has three main limitations. 
Firstly, the distillation in WENO is applied at the image level rather than the embedding level, which requires training the image encoder, resulting in 
% resulting in high computational complexity. 
low training efficiency.
Secondly, the teacher and student branches are optimised alternately, whereas higher efficiency and better performance are able to be achieved with self-distillation~\cite{zhang2022self_distillation} that distilling knowledge with only one network and allows simultaneous knowledge share between bag-level branch and instance-level branch.
Thirdly, the thresholding parameter for selecting important instances is inflexible that being manually determined using grid search.

In this paper, to efficiently enable instance-level supervision for MIL, we introduce the novel Coarse-to-Fine Self-Distillation (CFSD) framework, which facilitates learning from coarser (bag-level) knowledge to finer (instance-level) knowledge. In the bag-level branch, CFSD actively probes and distills the attention network within the attention-based framework trained with bag-level information to obtain instance-level labels for high-confidence instances. In the instance-level branch, the same attention network serves as an instance-level classifier, and the selected high-confidence instances are used for further instance-level training. Unlike WENO, we show that powerful performance can be achieved with self-distillation on frozen bag-level features, while we do not need to alternate between training the two branches, which ensures both capability and efficiency. Additionally, an adaptive threshold scheduling mechanism is designed to automatically update the threshold for selecting instances during training, offering more flexibility than the grid search approach.

Furthermore, we integrate the advantages of transformer-based frameworks that use self-attention~\cite{vaswani2017transformer} and positional encoding to capture inter-instance contextual information at the bag level. It is worth noting that the positional information in previous works is encoded as a one-dimensional sequence, which is problematic because background (non-tissue) instances in WSI are typically removed during preprocessing, meaning many instances are not adjacent in their original two-dimensional position in the WSI (Figure~\ref{fig:discontinuity}). To address this, we apply two-dimensional positional encoding (2DPE), using the intra-bag normalised $x$ and $y$ coordinates to more accurately represent the spatial arrangement of instances within a bag.

With the integration of these modules, we propose PathMIL, a hybrid framework capable of both bag-level and instance-level learning, with CFSD and 2DPE plugged in. We demonstrate the efficacy of PathMIL using several benchmarking tasks, including an internal benchmark for breast cancer estrogen and progesterone receptor status classification, subtype classification (TCGA-NSCLC) and tumour classification (CAMELYON16), achieving state-of-the-art performance with AUC scores of 0.9152, 0.8524, 0.9618, and 0.8634, respectively. Moreover, the instance-level learnability of PathMIL is theoretically proven following~\citeauthor{jang2024learnability} and empirically validated using the synthetic MNIST dataset.

% The attention-based framework is popular because of its attention mechanism and good interpretability, while in recent years, a variety of efforts have been made to improve it. For example, some works aim at fine-tuning the feature extractor to obtain better feature representations~\cite{li2021dsmil, liu2023multiple, lin2023interventional, huang2024hnm}, whereas the MIL part generally shares the same design as the conventional one. It is not recommended to this way, since part of the intuition of MIL is to reduce computational complexity, instead of re-training the feature extractor. On the other hand, transformer-based framework like TransMIL~\cite{shao2021transmil} applied Nyström-Attention~\cite{xiong2021nystrom} with positional encoding to capture correlation across instances. However, the positional information in TransMIL are originally encoded as a sequence. It triggers problem because background (non-tissue instances) removal is always been done in preprocessing that many instances are not next to each other position-wise, thereby making no sense to encode them in one sequence. To mitigate this, we applied the two-dimensional positional encoding (2DPE) on the transformer-based framework, which uses normalised $x$ and $y$ coordinates for positional encoding, being more precise. 

% However, for the methods mentioned above, supervision is only on bag-level. It has recently been shown that neither attention-based framework nor transformer-based framework guarantees the instance-level learnability, which explains the significant performance gaps between bag-level and instance-level for both ABMIL and TransMIL~\cite{jang2024learnability}. Hence, we further introduce a novel Coarse-to-Fine Self-Distillation (CFSD) module that assist the learning from coarser (bag-level) knowledge to finer (instance-level) knowledge. An adaptive threshold scheduling is designed automatically select appropriate instances for self-distillation.

Our main contributions are summarised as followed: 
\begin{itemize}
    \item We integrate attention-based and transformer-based frameworks, enabling inter-instance contextual information to be learnable with 2DPE.
    \item We propose CFSD, proving instance-level learnability both theoretically and empirically. And CFSD is a universal module that fits across various MIL frameworks.
    \item We show that PathMIL, which combining both 2DPE and CFSD, achieves state-of-the-art performance on multiple benchmarking tasks.
\end{itemize}

% % do so because of space limit
% (1)We integrate attention-based and transformer-based frameworks, enabling inter-instance contextual information to be learnable with 2DPE. (2) We propose CFSD, proving instance-level learnability both theoretically and empirically. (3) We show that PathMIL, which combining both 2DPE and CFSD, achieves state-of-the-art performance on multiple benchmarking tasks.

\section{Related Work}
\subsection{Transformer-based MIL}
TransMIL~\cite{shao2021transmil} solves MIL problem using two transformer layers with Nyström-Attention~\cite{xiong2021nystrom} and a Pyramid Position Encoding Generator (PPEG) to encode positional information. The idea of positional encoding for images originated in the Vision Transformer (ViT), which splits images into square patches, preserving all background patches as valid~\cite{dosovitskiy2021vit}. This leads to only minor discontinuity between patches, except at row boundaries. In contrast, when processing Whole Slide Image (WSI), background patches without tissue are typically removed, creating significant discontinuities between the remaining patches (see Figure~\ref{fig:discontinuity}). These discontinuities introduce noise into positional encoding. Although PPEG resizes bag-level features into two dimensions and processes them with convolutions, the one-dimensional positional encoding treats the embeddings as a continuous sequence. This disregards the spatial discontinuities, preventing PPEG from effectively representing the two-dimensional feature map and leading to inaccuracies in convolution operations.

\subsection{Instance-level Learnable MIL}
Recent studies have shown that the instance-level learnability of attention-based and transformer-based MIL models is not guaranteed, both theoretically and empirically~\cite{jang2024learnability}. This limitation arises from the attention pooling operation which multiplies attention weights over instance features, incorporating the hypothesis space for bag-level features into the instance predictions. While much effort has focused on improving MIL from the instance-level perspective, most work aims to fine-tune feature extractors to obtain better representations~\cite{liu2023multiple, lin2023interventional, huang2024hnm}. However, the MIL framework itself often remains based on conventional designs. These approaches are computationally expensive and time-consuming, which contradicts the goal of using MIL to reduce computational costs, especially with the availability of foundation models pre-trained on histopathological data~\cite{wang2021transpath, wang2022ctrans, xu2024gigapath, chen2024uni, vorontsov2024virchow}. Therefore, an efficient approach to enable instance-level learning is needed to enhance MIL’s overall capability, with self-distillation being a possible option.

% \paragraph{Knowledge Distillation in MIL.}
DTFD-MIL~\cite{zhang2022dtfd} employs feature distillation for two-tier bag-level training, creating smaller pseudo-bags to alleviate the effects of limited cohort sizes. However, DTFD-MIL still focuses on bag-level only training. In contrast, WENO~\cite{qu2022bidirectional} incorporates knowledge distillation at both the bag and instance levels. Despite, the alternating training of teacher and student branches, along with distillation on the image encoder, results in high computational complexity.

Unlike the methods discussed above, our PathMIL offers several advantages. In terms of positional encoding, we utilise two-dimensional coordinates to better capture the inter-instance relationships at the bag level, compared to the one-dimensional alternatives. From the perspective of knowledge distillation, our CFSD trains only one attention network and applies self-distillation, eliminating the need for alternating training between teacher and student networks. Leveraging the attention network, CFSD is designed to perform two tasks simultaneously: generating the attention map for bag-level classification and serving as the classifier for instance-level classification, which ensures that knowledge from the bag-level branch is utilised in the instance-level branch, and vice versa. In contrast to WENO, where the image encoder is kept fine-tuning from the bag-level and instance-level training, our CFSD operates at the frozen embeddings, which significantly reduces both computational and time costs. Overall, the design of CFSD facilitates cross-reference between the two branches, and we demonstrate that higher performance improvements can be achieved in this way, even without the need to fine-tune the image encoder.

%Furthermore, the nature of CFSD, operating at the embedding level, significantly reduces both computational and time costs.

% we demonstrate that PathMIL is capable to (1) learn inter-instance contextual information with 2DPE and learn instance-level information with CFSD. Meanwhile, the design of CFSD make the attention net to simultaneously serves for the conventional attention map generation and the instance-level classification, which is highly integrated and efficient. The adaptive threshold scheduling further strengthens the flexibility of CFSD by automatically searching for optimal instance selection threshold during training.


\section{Methodology}

% ==== Following part is the overview figure ==== 
\begin{figure*}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/overview_new.pdf}}
\caption{\textbf{Overview of the PathMIL.} Before patching, CLAM~\cite{lu_2021_data-efficient} is used to remove the background of WSIs. An ImageNet pre-trained ResNet-50 is used to extract embedding from each patch. (1) The embedded bag-level feature is then concatenated with a CLS token and processed by the bag-level branch of PathMIL, while obtaining the bag-level prediction $\hat{Y}$ and attention map $A$. (2) Subsequently, the top-$p$ instances in each bag are selected and assigned a label according to the bag-level label, to form an instance-level dataset $H'$ with the corresponding labels $Y'$ across all bags. (3) Next, these data are processed by the attention network, which serves as a instance-level classifier, in the self-distillation learning.}
\label{fig:overview}
\end{center}
\vskip -0.2in
\end{figure*}
% ==== Above is the overview figure ==== 

\subsection{Multiple-instance Learning (MIL)}
\paragraph{Problem formulation.}
Given a bag of $K$ instances $X=\{x_{1}, x_{2},...,x_{K}\}$, we would like to learn a binary classifier that accurately predicts a bag-level target value $Y\in\{0,1\}$ without access to instance-level labels $\{y_{1}, y_{2},...,y_{K}\}$, where $y_{k}\in{\{0,1\}},k=1,2,...K$. The MIL problem is defined as follows:
\begin{equation}
Y =  
    \begin{cases}
    0, & \text{iff} \sum_{k} y_{k}=0, \\
    1, & \text{otherwise.}
    \end{cases}
\end{equation}

\paragraph{Attention-based MIL (AMIL).} In computational pathology, a feature extractor $G$ with output dimension $1 \times D$ is used to create bag-level feature $H=\{h_{1}, h_{2},...,h_{K}\}\in\mathbb{R}^{K\times D}$, where $h_{k}$ are instance-level embeddings. A fully connected layer is used as the first layer, reducing the embedding dimension to $512$, such that $\mathbf{h}\in\mathbb{R}^{K\times 512}$. To implement MIL with attention, the attention network comprises two linear layers $U\in\mathbb{R}^{256\times 512}$ and $V\in\mathbb{R}^{256\times 512}$, splitting into N parallel attention branches, $W_1,W_2,...,W_N\in\mathbb{R}^{1\times 256}$. The gated attention $\alpha$ and attention-applied bag-level feature $\mathbf{M}$ are expressed as:
% \vspace{-3mm}
\begin{equation}
\label{eqn:gated_attn}
    \alpha_{k} = \frac{\text{exp}\{W(\text{tanh}(V\mathbf{h}_k^\top)\odot\text{sigm}(U\mathbf{h}_k^\top))\}}{\sum_{j=1}^K\text{exp}\{W(\text{tanh}(V\mathbf{h}_j^\top)\odot\text{sigm}(U\mathbf{h}_j^\top))\}}
\end{equation}
\begin{equation}
\label{eqn:amil}
    \mathbf{M}=\sum_{k=1}^{K}\alpha_{k}\mathbf{h}_k
\end{equation}
% \vspace{-3mm}

\paragraph{Transformer-based MIL (TransMIL).} The transformer-based MIL framework differs from the attention-based designs. Following the TransMIL architecture, we construct our model using two transformer layers $f'_{trans}$ with self-attention, two-dimensional positional encoding module (2DPE) in between, and a bag-level classifier $f_{cls}$ after the second transformer layer. The architecture of TransMIL $F_{trans}$ and our modified version $F'_{trans}$ are as follows:
\begin{equation}
    \begin{aligned}
        f_{trans} & = \text{Nyström-Attention}({\text{LayerNorm}}(\cdot)) \\
        F_{trans} & =f_{cls}(f_{trans,2}(\text{PPEG}(f_{trans,1}(\cdot))))
    \end{aligned}
\end{equation}
\begin{equation}
\label{eqn:our_trans}
\begin{aligned}
    f'_{trans} & = \text{Self-Attention}({\text{LayerNorm}}(\cdot)) \\
    F'_{trans} &= f_{cls}(f'_{trans,2}(\text{2DPE}(f'_{trans,1}(\cdot))))
\end{aligned}
\end{equation}

\subsection{Instance-level Learnable MIL}
% Before applying self-distillation on important instances derived from bag-level training, it is essential to ensure that the bag-level classifier focuses on the desired instances. Many
The attention-based framework has been widely used in previous work, and it has been demonstrated that the attention network can highlight the important instances related to the bag-level label~\cite{lu_2021_data-efficient, chen2022pancancer, liang2023macronet, huang2024pcr_rcc}. This suggests that annotating high-attention instances with bag-level labels is reasonable and reveals the potential for using self-distillation learning with bag-level knowledge, laying the groundwork for instance-level supervision in MIL. We also carried out a preliminary experiment to verify the aforementioned idea in Appendix~\ref{appendix:preliminary}.

% \paragraph{Preliminary experiment.}
% In this experiment, we applied CLAM-SB~\cite{lu_2021_data-efficient}, a model based on the framework of AMIL, to the CAMEYLON16 benchmarking task. By comparing the instances with top-$k$ importance and reverse top-$k$ importance in the attention map $A$, we observed that CLAM-SB effectively focused on tumour instances (Figure~\ref{fig:preliminary}). This result suggests that annotating high-attention instances with bag-level labels is reasonable and highlights the potential for using self-distillation learning with bag-level knowledge, laying the groundwork for instance-level supervision in MIL.


\paragraph{Coarse-to-Fine Self-Distillation (CFSD).}
Building on the previous work, we introduce a novel approach to improve instance-level learnability in MIL: Coarse-to-Fine Self-Distillation (CFSD). Based on the finding that the top-$p$ instances are highly relevant to the prediction label $Y$, we apply an adaptive threshold scheduling method. This technique updates $p$ dynamically during training, selecting the top-$p$ instances $H'_{p}\in\mathbb{R}^{P\times D}$ and their corresponding instance-level label $Y'_{p}\in\{0,1\}$ from the bag $H$ using the trained attention map $A$ and bag-level label $Y$, where $p\in[0.05k, 0.2k]$. Initially, we set $p=0.05k$ (top-$5\%$) to prioritise the high-confidence instances, and the threshold $p$ is incremented by $0.01k$ if the bag-level metrics no longer increase for three consecutive epochs, eventually reaching $p=0.2k$ (top-$20\%$). This approach ensures that instance-level supervision is gradually introduced, providing flexibility and adaptability throughout the training process. The pseudocode for instance-level label annotation is provided in Algorithm \ref{alg:1}.

Once we have acquired the selected high attention instance-level embeddings across all bags, we concatenate them together to form all selected instances $H'_{all}$ and their corresponding labels $Y'_{all}$. Then, $H_{all}'$ and $Y'_{all}$ are used to regularly train the instance-level classifier. In an attention-based framework like CLAM-SB, the attention network $F_{attn}$, which has attention gates $U$, $V$ and the attention branch $W_n$, can simultaneously act as the instance-level classifier since it outputs the attention map $A\in\mathbb{R}^{K\times 1}$ that can be interpreted as binary classification of instances. Hence, during CFSD, we optimise $F_{attn}$ instead of the bag-level classifier. Meanwhile, the implementation of CFSD in PathMIL follows the same design such that $F_{attn}$ is applied before the transformer layers and positional encoding. However, PathMIL is attention-based and transformer-based hybrid, thereby $F_{attn}$ only works for the CFSD and attention-based output, while the final prediction of PathMIL considers both attention-based and transformer-based outputs. Details will be introduced in section~\ref{sec:pathmil}.

\begin{algorithm}[htb]
   \caption{Self-annotating instance-level label}
   \label{alg:1}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $H$, target $Y$, attn\_map $A$
   \STATE {\bfseries Output:} inst\_data $H'$, inst\_target $Y'$
   \FOR{each bag}
   \STATE 1) argsort and rank attention score;\\
   $A_{\text{ranked}}\leftarrow \text{argsort}(A)/K$
   \STATE 2) get top-$p$ instances based on threshold $th$;\\
   $\text{selected\_idx}\leftarrow A_{\text{ranked}}>th\in [0.8,0.95]$
   \STATE 3) Get $H'$ and $Y'$;\\
   $H'\leftarrow H[\text{selected\_idx}]$, \\
   $Y'\leftarrow Y$ repeat for len(selected\_idx)
   \ENDFOR
   \STATE Given the bag number $m$:
   \STATE $H'_{all}\leftarrow\text{concat}(H'_1, H'_2,...,H'_m)$
   \STATE $Y'_{all}\leftarrow\text{concat}(Y'_1, Y'_2,...,Y'_m)$
\end{algorithmic}
\end{algorithm}

\paragraph{Proving the instance-level learnability.} To prove the instance-level learnability of CFSD, we follow the lemma \ref{lemma:inst_learnability} and condition \ref{con:inst_learnability} proved by ~\citeauthor{jang2024learnability} listed in Appendix~\ref{appendix:prove_inst_learnability}. The proof is as follows:
\begin{proof}
Given $\mathcal{H}_{inst_k}$ and $\mathcal{H}_{add_k}$ as the hypothesis space for the $k^{th}$ instance and the hypothesis space for the $k^{th}$ instance generated through elements outside of the $k^{th}$ instance, respectively. The instance-level classifier in CFSD is denoted as $g(\cdot)$ and $f_\mathcal{H}$ denotes the individual hypothesis in corresponding hypothesis space $\mathcal{H}$. Hence we have:
\begin{equation*}
F(h) = g_k(\mathbf{h}_k)
\end{equation*}
\begin{equation*}
\mathcal{H}_{add_k}=\{f_{\mathcal{H}}:F(h)\rightarrow y_k\}\
\end{equation*}
\begin{equation*}
\mathcal{H}_{add_k}=\{f_{\mathcal{H},k}:g_k(\mathbf{h}_k)\rightarrow y_k\}
\end{equation*}
which obeys the pattern of $\mathcal{H}_{inst_k}$ that produces results dependent solely on the $k^{th}$ instance feature:
\begin{equation*}
\mathcal{H}_{inst_k}=\{f_{\mathcal{H},k}:f_{\mathcal{H},k}(\mathbf{h}_k)\rightarrow y_k\}
\end{equation*}
The condition \ref{con:inst_learnability} is satisfied that:
\begin{equation*}
\mathcal{H}_{add_k}\subset\mathcal{H}_{inst_k}
\end{equation*}
and according to lemma \ref{lemma:inst_learnability}, CFSD is instance-level learnable.
\end{proof}

% Based on the proof above, we demonstrate CFSD is instance-level learnable and therefore theoretically reasonable for implementing CFSD.

\subsection{Two-dimensional positional encoding (2DPE)}
To address the limitations caused by the discontinuity of background-removed WSIs and to capture inter-instance contextual information, we record the coordinates $(cx_k,cy_k)\in\mathbb{R}^{1\times 2}$ for each valid instance, and concatenate them to be coordinates in a bag, denoted as $(\mathbf{cx},\mathbf{cy})\in\mathbb{R}^{K\times 2}$. Positional encodings are first performed separately for $\mathbf{cx}$ and $\mathbf{cy}$, and then added together. Given the variation in the width and height of WSIs, the coordinates are normalised within each bag, such that $(\mathbf{cx'},\mathbf{cy'})=\{(cx'_1,cy'_1),...(cx'_k,cy'_k)\}$ with $cx_k',cy_k'\in [0,1]$. The normalised coordinates can reveal the inter-instance contextual relationship within each bag and positional encoding for each axis is implemented according to the approach outlined in the Transformer model~\cite{vaswani2017transformer}. The process of 2DPE is summarised in Algorithm \ref{alg:2}.

\begin{algorithm}[htb]
   \caption{Two-dimensional positional encoding}
   \label{alg:2}
\begin{algorithmic}
   \STATE {\bfseries Input:} data with CLS token $\mathbf{h}^\ell \in \mathbb{R}^{(k+1)\times 512}$, coordinates $(\mathbf{cx},\mathbf{cy})$
   \STATE {\bfseries Output:} positional encoded embeddings $\mathbf{h}_{pe}^\ell$
   \STATE 1) Normalise coordinates;\\
   \FOR{each $(\mathbf{cx},\mathbf{cy})$}
   \STATE $\text{max\_scale}\leftarrow max(max(\mathbf{cx}), max(\mathbf{cy}))$
   \STATE $(\mathbf{cx}',\mathbf{cy}')\leftarrow
   \text{min\_max\_scaler} (\mathbf{cx},\mathbf{cy}, \text{max\_scale})$\\
   \ENDFOR
   \STATE 2) Positional encoding;\\
   $\mathbf{h}_{pe}^\ell\leftarrow \mathbf{h}^\ell+\text{sincos\_posenc($\mathbf{cx}$)}+\text{sincos\_posenc($\mathbf{cy}$)}$
\end{algorithmic}
\end{algorithm}

% As outlined in the CFSD implementation, although self-attention is present in the transformer-based framework, we still utilise $f_{attn}$ to be the instance-level classi fier, which serves solely for CFSD and subsequent attention heatmap visualisation. 
\subsection{PathMIL} 
\label{sec:pathmil}
PathMIL is a hybrid framework incorporating the attention-based and transformer-based frameworks with CFSD and 2DPE applied. In the bag-level branch, we use two self-attention-based transformer layers, with a 2DPE module located in between. Meanwhile, the attention-based bag level feature is obtained with the attention network $F_{attn}$. Building on Eqn(\ref{eqn:gated_attn})(\ref{eqn:amil})(\ref{eqn:our_trans}) with the bag-level prediction head denoted as $f_{cls}$, the bag-level branch $F_{bag}$ is composed as:
\begin{equation}
    \mathbf{h}=\text{FC}(H)\\
\end{equation}
\begin{equation}
    F_{trans} = f'_{trans,2}(\text{2DPE}(f'_{trans,1}(\mathbf{h}, \mathbf{x}, \mathbf{y}))) \\
\end{equation}
\begin{equation}
    F_{attn} =\alpha(\mathbf{h})\otimes \mathbf{h}\\
\end{equation}
\begin{equation}
    F_{bag} =f_{cls}(\text{concat}(F_{trans}(\mathbf{h,\mathbf{x},\mathbf{y}}),F_{attn}(\mathbf{h})))\\
\end{equation}

while the training is implemented following:
\begin{equation}
\hat{Y}=F_{bag}(\mathbf{h}, \mathbf{x}, \mathbf{y})
\end{equation}
\begin{equation}
\mathcal{L}_{bag} = CELoss(\hat{Y}, Y)
\end{equation}

For the instance-level branch, CFSD is applied to facilitate instance-level learning. Let $F_{inst}$ denotes the instance-level branch network, the instance-level branch is written as follows and also trained with cross entropy loss:
\begin{equation}
        F_{inst} =Sigmoid(\alpha(\mathbf{h}))\\
\end{equation}
\begin{equation}
\mathcal{L}_{inst} = BCELoss(\hat{Y}', Y')
\end{equation}

By combining both branches, PathMIL is trained in an end-to-end manner, optimising the following objective:
\begin{equation}
\mathcal{L} = \mathcal{L}_{bag}+\mathcal{L}_{inst}
\end{equation}

The implementation of PathMIL in pseudo-code is described in Algorithm \ref{alg:3}.

\section{Experiments and Results}
To evaluate the performance of PathMIL, we conducted experiments across three different datasets: (1) our internal breast receptor status classification dataset, (2) the TCGA-NSCLC public dataset, and (3) the CAMELYON16 public dataset.

\subsection{Datasets and Preprocessing}
\paragraph{Breast Cancer Receptor Status Classification.} 
Our internal breast cancer receptor status classification dataset consists of 491 clinical cases, each reported by expert consultant breast pathologists, with available hormone receptor status. We use this dataset to perform the classification for two receptors that are highly related to breast cancer treatment decision making: the estrogen receptor (ER) and the progesterone receptor (PR). In clinical practice, both ER and PR are scored using a proportion score ($PS$) and an intensity score ($IS$), where $PS\in \mathbb{Z}\cap[0,5]$ and $IS\in \mathbb{Z}\cap[0,3]$. These scores are then combined to form a total score ($TS$), where $TS\in \mathbb{Z}\cap [0,8]$, with $TS\ne1$, and a higher score indicates greater receptor positivity. When converting into binary positive or negative status for classification, we take $TS$ of 0 and 2 as negative, and $TS$ from 3 to 8 as positive, in line with the clinical guideline~\cite{iccr_allred}. Note that a $TS=1$ does not exist, as either $PS = 0$ or $IS = 0$ would imply the absence of receptor expression. It is worth noting that receptor status classification is more challenging than tumour classification. For example, in the case of ER status classification, not all tumour cells in a sample are guaranteed to be ER+ due to tumour cell hormone receptor heterogenity, making it a more complex task than simply distinguishing tumour from normal tissue.

\paragraph{TCGA-NSCLC.} The TCGA-NSCLC dataset is a subtype classification dataset that includes two lung cancer subtypes: Lung Squamous Cell Carcinoma (TCGA-LUSC) and Lung Adenocarcinoma (TCGA-LUAD). After removing corrupted slides, the dataset consists of 938 diagnostic slides, including 398 LUSC cases and 540 LUAD cases. The annotations are given in binary as LUSC or LUAD.

\paragraph{CAMELYON16.} CAMELYON16 is a public dataset focused on lymph node metastasis tumour/normal classification in breast cancer. It consists of 270 training cases, including 160 normal cases and 110 cases containing tumour metastases, as well as 130 test cases. The dataset comes with annotations that label each slide as either containing tumour metastasis or normal tissue only.

\paragraph{Preprocessing.}
We followed a consistent preprocessing protocol across all three datasets, refraining from any data curation such as normalisation to better demonstrate the robustness of our method against inherent staining and scanning variations. The resolution of the whole slide images (WSI) was standardised to 0.2631 microns per pixel (MPP), and a $20\times$ magnification was used for patching. Background removal and patching were carried out using the CLAM~\cite{lu_2021_data-efficient} and OpenSlide~\cite{goode2013openslide}, with patches extracted in a non-overlapping manner at a size of $256\times 256$. For feature extraction, we employed a ResNet-50 model pre-trained on ImageNet~\cite{he2016resnet}. We extracted embeddings from the third layer, applying mean pooling to obtain instance-level embeddings of size $1\times 1024$. These instance embeddings were then concatenated to form the bag-level feature $H\in\mathbb{R}^{K\times 1024}$. It is worth noting that, while there are more advanced foundation models pre-trained on histopathology images, such as CTransPath~\cite{wang2022ctrans} and GigaPath~\cite{xu2024gigapath}, both TCGA-NSCLC and CAMELYON16 are included as the pre-train dataset. Therefore, we opted for the ImageNet pre-trained ResNet-50 to ensure a fair comparison.

\subsection{Baseline Models}
To demonstrate the superior performance of our framework, we compared PathMIL with several baseline models, including the basic max-pooling and mean-pooling, ABMIL that utilises attention-based pooling module~\cite{ilse2018amil}, the popular CLAM-SB (single-attention-branch) and CLAM-MB (multi-attention-branch) that used gate attention~\cite{lu_2021_data-efficient}, DSMIL that applied dual-stream MIL with instance and bag classifiers~\cite{li2021dsmil}, and TransMIL that applied PPEG and Nyström-Attention~\cite{shao2021transmil}. It is important to note that SimCLR~\cite{chen2020simclr}, a self-supervised contrastive learning method, was originally used to pre-train a ResNet-18 as the feature extractor for DSMIL, However, we omitted this step in our benchmarking as we aimed to compare the performance of the MIL frameworks rather than different feature extractors. 

Additionally, we specifically compared our CFSD with WENO~\cite{qu2022bidirectional} by evaluating the combination of ABMIL+CFSD and DSMIL+CFSD, then comparing the performance gaps with those of vanilla ABMIL and DSMIL. Finally, we used these performance gaps to compare with the results of ABMIL+WENO and DSMIL+WENO, as reported in the original paper.

\subsection{Implementations}
\paragraph{Experiment settings.}
For evaluation, we used the area under the curve (AUC) and F1-score as performance metrics. To ensure the robustness of our results, we rigorously employed five-fold cross-validation for the training of all datasets. For both our internal dataset and the TCGA-NSCLC dataset, we split the data into a train:val:test ratio of 3:1:1, reporting the average metrics on the test set. For the CAMELYON16 dataset, we divided the training data into a train:val ratio of 4:1 for five-fold cross-validation and evaluated the model on the official test set, with the average metrics from the test set reported.

\paragraph{Training details.}
We performed all experiments on a workstation with an RTX 3060 graphics card. We used cross-entropy loss for both bag-level and instance-level training, with the AdamW optimiser~\cite{loshchilov2018adamw} and CosineAnnealing~\cite{loshchilov2017sgdr} scheduler for optimisation. The learning rate was set to be $2\times 10^{-4}$ with batch size of $1$, while gradient accumulation was set to 32. The maximum epoch number was 150, with early stopping applied if the metrics did not improve over 15 consecutive epochs. For fair comparison, we used the Lookahead optimiser~\cite{zhang2019lookahead} for TransMIL, adhering to their original design.

For the CFSD, two alternative approaches were compared in the experiment: (1) parallel training of instance-level CFSD alongside bag-level classification, and (2) first training the bag-level classifier, followed by fine-tuning using CFSD.

\subsection{Results}
The results of binary classification for ER+/ER-, PR+/PR-, LUAD/LUSC and tumour/normal classification are presented in Table \ref{table:model_compare}. The findings demonstrated that PathMIL achieved significant performance improvements over the baseline models on all benchmarks, with AUC scores of 0.9152 and 0.8524 for estrogen and progesterone receptor status classification, respectively, an AUC of 0.9618 for subtype classification, and 0.8634 for tumour classification. 

On average, PathMIL showed improvements of 9\% and 11\% in AUC and F1-score, respectively, across the four benchmarks, comparing to the best baseline. When evaluated by both AUC and F1-score, though it witnessed fluctuation between the performance of PathMIL with CFSD for parallel and fine-tuning, PathMIL still significantly surpassed other methods. Among the baseline models, the attention-based framework CLAM-SB and CLAM-MB generally outperformed the other frameworks. On the other hand, the performance of DSMIL was limited due to the absence of SimCLR pre-trained features, revealing a lack of robustness. Similarly, TransMIL did not perform as expected under the training scheme with the Lookahead optimiser, even when compared to models that did not use positional encoding. We attribute this underperformance to the fact that PPEG is also influenced by the discontinuity of instances, thus failing to capture the true inter-instance relationships. 

Meanwhile, we also benchmarked our CFSD with other knowledge distillation method \ie, WENO. In the comparison between WENO and CFSD, we focus on the performance gap rather than the absolute performance, in order to mitigate the influence of differences in data splitting and hyperparameter settings. As shown in Table~\ref{table:cfsd_vs_weno}, CFSD significantly outperformed WENO, achieving the highest performance improvements of 0.1986 and 0.1998 for ABMIL and DSMIL, respectively.


\begin{table*}[t]
\caption{\textbf{Results on ER+/ER- classification, PR+/PR- classification on the internal dataset, subtype classification on the TCGA-NSCLC dataset and the tumour/normal classification the CAMELYON16 dataset.}\textbf{Bold} indicates overall the best while \underline{underline} indicates the best in subgroup.}
\label{table:model_compare}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{l|c|cccccccr}
\toprule
\multirow{2}{*}{Dataset \& Metrics} & \multirow{2}{*}{Params.}
&  \multicolumn{2}{c}{Internal (ER)} &  \multicolumn{2}{c}{Internal (PR)} & \multicolumn{2}{c}{TCGA-NSCLC} & \multicolumn{2}{c}{CAMELYON16}\\
\cmidrule{3-10}
& & AUC & F1-score & AUC & F1-score & AUC & F1-score & AUC & F1-score \\
\midrule
\midrule
Max-Pooling  & 2.05K  & 0.6934    & 0.5858    & 0.6408    & 0.5992    & 0.8610   & 0.7764    & 0.6528    & 0.6032\\
Mean-Pooling & 2.05K  & 0.6664    & 0.5106    & 0.6602    & 0.5828    & 0.8180   & 0.7486    & 0.5832    & 0.5430\\
ABMIL        & 0.26M  & 0.6344    & 0.5274    & 0.6702    & 0.6022    & 0.9030   & 0.8320    & 0.6264    & 0.6020\\
CLAM-SB      & 0.79M  & \underline{0.8504}    & \underline{0.7138}    & 0.7542    & \underline{0.6764}    & 0.9302   & 0.8612    & 0.7444    & 0.6554\\
CLAM-MB      & 0.79M  & 0.8304    & 0.7026    & \underline{0.7560}    & 0.6596    & \underline{0.9328}   & \underline{0.8630}    & \underline{0.7764}    & \underline{0.6758}\\
DSMIL        & 0.15M  & 0.6460    & 0.5416    & 0.6062    & 0.5570    & 0.7910   & 0.7160    & 0.5718    & 0.5346\\
TransMIL     & 2.67M  & 0.6622    & 0.5344    & 0.6422    & 0.5648    & 0.9258   & 0.8472    & 0.6400    & 0.6000\\
\midrule
\midrule
PathMIL (Parallel)  & 2.89M   & \textbf{0.9152}            & \textbf{0.7788}       & 0.8442     & 0.7454    & 0.9528   & 0.8782    & \textbf{0.8634}     & 0.7672\\
PathMIL (Fine-tuning) & 2.89M  & 0.9146   & 0.7738   & \textbf{0.8524}   & \textbf{0.7642}    & \textbf{0.9618}   & \textbf{0.8956}   & 0.8578   & \textbf{0.7908}\\
\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table}[h]
\caption{\textbf{Benchmarking the effectiveness of CFSD and WENO on CAMELYON16.} The performance gap $\Delta$ compared with vanilla model in AUC score is reported. Note that the $\Delta$WENO is directly referenced from the original paper~\cite{qu2022bidirectional}.}
\label{table:cfsd_vs_weno}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{l|cc}
\toprule
\multirow{2}{*}{Modules} & \multicolumn{2}{|c}{Models}\\
\cmidrule{2-3}
& ABMIL & DSMIL\\
% & $\times$ & CFSD-para. & CFSD-ft & $\Delta$CFSD & $\Delta$ WENO \\
\midrule
\midrule
$\times$            & 0.6264                        & 0.5718\\
CFSD (Parallel)          & 0.8136                        & 0.7702\\
CFSD (Fine-tuning)            & \textbf{0.8250}                        & \textbf{0.7716}\\
\midrule
$\Delta$CFSD (Parallel)  & \color{red}+0.1872            & \color{red}+0.1984\\
$\Delta$CFSD (Fine-tuning)     & \textbf{\color{red}+0.1986}   & \textbf{\color{red}+0.1998}\\
$\Delta$WENO        & \color{red}+0.0284       & \color{red}+0.0094\\
% ABMIL   & 0.6264    & 0.8136    & 0.8250    & \textbf{\color{red}+0.1986} & \colors{red}+0.0284 \\
% DSMIL   & 0.9240    & 0.7329    & 0.4750    & 0.0646 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Ablation Study}
We next assessed the effectiveness of CFSD and 2DPE through an ablation study (Table~\ref{table:ablation}). The results demonstrate that CFSD leads to obvious improvements in the attention-based framework of CLAM-SB, although the AUC and F1-score metrics vary depending on whether parallel training or fine-tuning is used. This highlights that enhancing learnability at the instance level is empirically beneficial to bag-level learning. Additionally, we show that the efficacy of two-dimensional positional encoding is substantial. TransMIL with 2DPE outperforms both traditional TransMIL and TransMIL with basic 1DPE measured by both AUC and F1-score. This improvement is attributed to the more accurate $(cx, cy)$ coordinates, which better capture inter-instance contextual information in background-removed WSI.
% While some performance improvements may also stem from self-attention, as evidenced by TransMIL$^\dagger$+1DPE with self-attention generally outperforming TransMIL$^*$+PPEG with Nyström attention, 
Overall, the combination of CFSD and 2DPE in PathMIL yields the best performance across most configurations in the ablation study, except for CAMELYON16 that CLAM-SB with parallel CFSD performed the best on AUC score. We suspect this because only a small proportion of tumour is contained in positive WSI and there always be multiple tissues in one WSI, making it more challenging for PathMIL to integrate information learned from the two branches.

Furthermore, we demonstrate instance-level learnability using the synthetic MNIST dataset~\cite{deng2012mnist}, following the approach outlined by~\citeauthor{jang2024learnability}. As shown in Table \ref{table:inst_learnability}, the CLAM-SB baseline shows limited performance in instance-level classification. In contrast, CFSD is empirically proven to enable instance-level learning. The detailed implementation is described in Appendix~\ref{appendix:empirical_inst_learnability_implementation}.

\begin{table}[htb]
\caption{\textbf{Comparison of bag-level performance $P_{bag}$ and instance-level $P_{inst}$ performance for CFSD on synthetic MNIST dataset.} The framework of CLAM-SB (baseline), CLAM-SB (CFSD-parallel) and CLAM-SB (CFSD-fine-tuning) are tested in the experiment. Full table with performance gap is reported in Appendix~\ref{appendix:empirical_inst_learnability_full_table}.}
\label{table:inst_learnability}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{l|cccr}
\toprule
Modules & \multicolumn{2}{c}{$P_{bag}$}& \multicolumn{2}{c}{$P_{inst}$}\\
\midrule
CFSD & AUC & F1-score & AUC & F1-score \\
\midrule
\midrule
$\times$    & 0.9240            & 0.7329            & 0.4750 & 0.0646 \\
Parallel    & \textbf{0.9233}   & 0.7282            & 0.7160 & \textbf{0.4030} \\
Fine-tuning  & 0.9213            & \textbf{0.7363}   & \textbf{0.7858} & 0.3994 \\
% \midrule
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Interpretability}
To demonstrate the interpretability of PathMIL, we visualised the attention heatmaps from two internal cohort ER+ cases, as presented in Appendix~\ref{appendix:interpretability}. Immunohistochemistry (IHC) is a laboratory technique that uses antibodies to detect antigens in tissue samples. This technique is widely used in clinical settings to determine receptor status and is regarded as the gold standard for guiding receptor-related treatment decisions in breast cancer~\cite{walker2008ihc}. The intensity of brown staining in IHC corresponds to the amount of target antigen present in the tissue, with the brown regions indicating ER+ cells. By comparing the visualised heatmaps with their IHC reference images, we observe that the areas of high attention identified by PathMIL align closely with the brown-stained regions that reflect the ER protein detection seen in IHC. This similarity indicates that the classification decisions made by PathMIL are interpretable from a clinical perspective, providing valuable insights into the underlying biological features of the tissue.

\section{Conclusion}
In this paper, we propose a novel framework, PathMIL, which integrates a coarse-to-fine self-distillation (CFSD) paradigm and two-dimensional positional encoding (2DPE) for MIL. The CFSD design enables efficient instance-level supervision during training by probing and distilling a classifier trained with bag-level information. This approach effectively addresses the challenge of limited instance-level learnability in MIL, without the need for manually introducing additional prior knowledge. Moreover, 2DPE mitigates issues arising from the discontinuity of instances in background-removed WSI, and reduces the positional inaccuracies associated with traditional one-dimensional positional encoding (1DPE) by encoding inter-instance contextual information using $(cx,cy)$ coordinates. Furthermore, the framework's design aligns with the decision-making and reasoning processes of human pathologists, who concurrently evaluate both bag-level features and instance-level features while considering inter-instance contextual information. By leveraging CFSD and 2DPE, PathMIL outperforms state-of-the-art frameworks in terms of both AUC and F1-score, while also demonstrating instance-level learnability and model interpretability.

\paragraph{Limitations.} While the efficacy of CFSD and 2DPE has been demonstrated in our experimental setting, it will be crucial to explore how this method performs under different magnifications of WSI. Moreover, it would be valuable to investigate whether more advanced techniques, such as cross-attention~\cite{chen2021crossvit}, can be leveraged for multi-scale learning with instances across various magnifications. Taking the overlays shown in our framework, we hope this work will inspire future research aimed at developing a more universally applicable MIL architecture.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work. It is worth mentioning that MIL is a fundamental technique in computational pathology research, where researchers not only are looking for solutions in morphological tasks such as tumour detection but also hope to use deep learning to discover new knowledge in treatment response prediction~\cite{wang2021pcr_gastric,khosravi2022pcr_bc, huang2024pcr_rcc} and prognosis prediction~\cite{chen2022pathomic, chen2022pancancer, liang2023macronet, qian2024prognosis, volinsky-fremond2024prognosis} by analysing whole-slide images. Our design of coarse-to-fine self-distillation (CFSD) reveals an applicable method for reasonable knowledge discovery, since no prior knowledge has been introduced and instead the model optimises itself with the already learned information. 
% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.

\pagebreak
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{reference}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Discontinuity between Patches}
\label{appendix:discontinuity}
\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.5\columnwidth]{figures/discontinuity.pdf}}
\caption{\textbf{A comparison shows the differences between the patching of ViT with ordinary square images and the patching of WSI.} (a) shows a square aircraft image, which typically processed by ViT that with minor discontinuity. The implementation of ViT splits it in to fixed-size patches as the dash lines indicate. (b) shows a background removed WSI. (c) the a corresponding zoom-in view for better visualisation. The red arrows points out examples of discontinuous patches.}
\label{fig:discontinuity}
\end{center}
\vskip -0.2in
\end{figure}

% \pagebreak
\section{Examples for Visulising Top-$k$ Patches in Preliminary Experiment}
\label{appendix:preliminary}
In this experiment, we applied CLAM-SB~\cite{lu_2021_data-efficient}, a model based on the framework of AMIL, to the CAMEYLON16 benchmarking task. By comparing the instances with top-$k$ importance and reverse top-$k$ importance in the attention map $A$, we observed that CLAM-SB effectively focused on tumour instances (Figure~\ref{fig:preliminary}). This result suggests that annotating high-attention instances with bag-level labels is reasonable and highlights the potential for using self-distillation learning with bag-level knowledge, laying the groundwork for instance-level supervision in MIL.
\begin{figure}[htb]
\vskip 0.2in
\begin{center}
\subfigure[Examples of top-$k$ importance instances.]{\centerline{\includegraphics[width=0.5\columnwidth]{figures/preliminary_topk.pdf}}}
\subfigure[Examples of reverse top-$k$ importance instances.]{\centerline{\includegraphics[width=0.5\columnwidth]{figures/preliminary_rtopk.pdf}}}
\caption{\textbf{Instances visualisation of preliminary experiments.} In (a), we can see top-$k$ instances contain tumour areas, while in (b) the reverse top-$k$ instances contain mainly stroma, inflammatory cells and red blood cells. The bag-level model can provide correct classification for top-$k$ instances.}
\label{fig:preliminary}
\end{center}
\vskip -0.2in
\end{figure}

\section{Algorithm for implementing PathMIL}
\begin{algorithm}[htb]
   \caption{PathMIL}
   \label{alg:3}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $H$, coordinates $(\mathbf{x},\mathbf{y})$
   \FOR{each iteration}
   \STATE $\mathbf{h}\leftarrow \text{FC}(H)$, where $\mathbf{h}\in \mathbb{R}^{K\times 512}$
   \IF{bag-level}
   \STATE $\mathbf{M}\leftarrow \alpha(\mathbf{h})\otimes\mathbf{h}$
   \STATE $\mathbf{h}^\ell\leftarrow\text{Concat}(\text{CLS}, \mathbf{h})$
   \STATE $\mathbf{h}'\leftarrow F_{trans} (\mathbf{h^\ell},\mathbf{x}, \mathbf{y})$
   \STATE $\mathbf{h}''\leftarrow \text{layer\_norm}(\mathbf{h}'')[:,0]$
   \STATE $\hat{Y}\leftarrow f_{cls}(\text{concat}(\mathbf{h'',M}))$
   \STATE {\bfseries Output:} $\hat{Y}$, $A$
   \ELSIF{instance-level}
   \STATE $\hat{Y}'\leftarrow Sigmoid(\alpha(\mathbf{h}))$\\
   \STATE {\bfseries Output:} $\hat{Y}'$
   \ENDIF
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Lemma and Condition for Proving Instance-level Learnability~\cite{jang2024learnability}}
\label{appendix:prove_inst_learnability}
Given $\mathcal{H}$ denotes the hypothesis space, $\mathcal{H}_{inst_i}$ is the $i^{th}$ instance hypothesis space, where $\mathcal{H}_{inst_i}=\{h_i:h_i(X_i)\rightarrow Y_i\}$. And $\mathcal{H}_{add_i}$ is the extra hypothesis space from external values for the $i^{th}$ instance. With $\mathcal{X}:=\{\mathcal{X}_{inst_1},\mathcal{X}_{inst_2},...,\mathcal{X}_{inst_N}\}$ to be the bag-level feature space and $\mathcal{Y}:=\{1,...k\}$ to be the bag label space, we have:
\begin{condition}$\mathcal{H}_{add_i}$ must be a subset of $\mathcal{H}_{inst_i}$ that:\\
\begin{equation}
    \mathcal{H}_{add_i}\subset\mathcal{H}_{inst_i}:=\{h_{add_i}: \mathcal{X}_{add_i}\rightarrow\mathcal{Y}\}
\end{equation}
\label{con:inst_learnability}
\end{condition}

\begin{lemma}Condition \ref{con:inst_learnability} is a necessary condition for the learnability of instances, when the hypothesis space for the $i^{th}$ instance of a MIL algorithm is $\mathcal{H}_{inst_i}\cup \mathcal{H}_{add_i}$, where $\mathcal{H}_{inst_i}$ denotes the hypothesis space for the $i^{th}$ instance and $\mathcal{H}_{add_i}$ denotes the hypothesis space for the $i^{th}$ instance generated through elements outside the $i^{th}$ instance.
\label{lemma:inst_learnability}
\end{lemma}

\section{Supplementary Results}
\subsection{Ablation Study}
\label{appendix:sup_ablation_full}
\begin{table*}[htb]
\caption{\textbf{Ablation study performed on ER+/ER- classification, PR+/PR- classification on the internal dataset, subtype classification on the TCGA-NSCLC dataset and the tumour/normal classification the CAMELYON16 dataset.} $^*$ indicates the original design of corresponding model. $^\dagger$ Although we follow the TransMIL architecture, the Lookahead optimiser is omitted during the training of TransMIL with 1DPE and 2DPE. \textbf{Bold} indicates overall the best while \underline{underline} indicates the best in subgroup.}
\label{table:ablation}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{ccc|cccccccr}
\toprule
\multicolumn{3}{c|}{Modules} &  \multicolumn{2}{c}{Internal (ER)} &  \multicolumn{2}{c}{Internal (PR)} & \multicolumn{2}{c}{TCGA-NSCLC} & \multicolumn{2}{c}{CAMELYON16}\\
\cmidrule{1-11}
Framework & CFSD & PE & AUC & F1-score & AUC & F1-score & AUC & F1-score & AUC & F1-score \\
\midrule
\midrule
CLAM-SB$^*$     & $\times$      & $\times$  & 0.8504            & 0.7138            & 0.7542            & 0.6764            & 0.9302   & 0.8612   & 0.7444            & 0.6554\\
CLAM-SB         & Parallel      & $\times$  & 0.8360            & 0.6908            & 0.8020            &\underline{0.7142}    & 0.9110   & 0.8374    &\textbf{0.8728}    &\underline{0.7796}\\
CLAM-SB         & Fine-tuning    & $\times$  &\underline{0.8548}    &\underline{0.7142}    &\underline{0.8194}    & 0.7178            & \underline{0.9350}  & \underline{0.8652}    & 0.8414            & 0.7466\\
\cmidrule{1-11}
TransMIL$^*$    & $\times$      & PPEG        & 0.6622            & 0.5344            & 0.6422            & 0.5648        & 0.9258   & 0.8472    & 0.6400            & 0.6000\\
TransMIL$^\dagger$ & $\times$     & 1D        & 0.8204            & 0.6996            & 0.5842            & 0.5398          & 0.9462   & 0.8706    & 0.7670            & 0.7022\\
TransMIL$^\dagger$ & $\times$     & 2D        & \underline{0.8766}   & \underline{0.7368}   & \underline{0.8200}   & \underline{0.7120} & \underline{0.9516}   & \underline{0.8796}    & \underline{0.8570}   & \underline{0.7386}\\
\midrule
\midrule
PathMIL & $\times$     & 2D  & 0.8842    & 0.7294    & 0.8036     & 0.7082    & 0.9462   & 0.8704   & 0.7878     & 0.7056\\
PathMIL & Parallel     & 2D  & \textbf{0.9152}            & \textbf{0.7788}       & 0.8442     & 0.7454    & 0.9528   & 0.8782    & \underline{0.8634}     & 0.7672\\
PathMIL & Fine-tuning   & 2D  & 0.9146   & 0.7738   & \textbf{0.8524}   & \textbf{0.7642}    & \textbf{0.9618}   & \textbf{0.8956}   & 0.8578   & \textbf{0.7908}\\
\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[htb]
\caption{\textbf{Number of parameters for models in ablation study.} $^*$ indicates the original design of corresponding model. $^\dagger$ Although we follow the TransMIL architecture, the Lookahead optimiser is omitted during the training of TransMIL with 1DPE and 2DPE.}
\label{table:ablation_mdl_compare}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{ccc|c}
\toprule
\multicolumn{3}{c|}{Modules} &  \multirow{2}{*}{Params}\\
\cmidrule{1-3}
Framework & CFSD & PE & \\
\midrule
\midrule
CLAM-SB$^*$     & $\times$      & $\times$  & 0.79M \\
CLAM-SB         & Parallel      & $\times$  & 0.79M \\
CLAM-SB         & Fine-tuning   & $\times$  & 0.79M \\
% \cmidrule{1-4}
\midrule
TransMIL$^*$        & $\times$     & PPEG      & 2.67M  \\
TransMIL$^\dagger$  & $\times$     & 1D        & 2.63M  \\
TransMIL$^\dagger$  & $\times$     & 2D        & 2.63M  \\
\midrule
\midrule
PathMIL & $\times$     & 2D  & 2.89M  \\
PathMIL & Parallel     & 2D  & 2.89M  \\
PathMIL & Fine-tuning  & 2D  & 2.89M  \\
\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

% \pagebreak
\subsection{Standard Deviation}
\label{appendix:sup_std}
\begin{table*}[htb]
\caption{\textbf{Standard deviation on ER+/ER- classification, PR+/PR- classification on the internal dataset, subtype classification on the TCGA-NSCLC dataset and the tumour/normal classification the CAMELYON16 dataset.}}
\label{table:model_compare_std}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{l|cccccccr}
\toprule
\multirow{2}{*}{Dataset \& Metrics}
&  \multicolumn{2}{c}{Internal (ER)} &  \multicolumn{2}{c}{Internal (PR)} & \multicolumn{2}{c}{TCGA-NSCLC} & \multicolumn{2}{c}{CAMELYON16}\\
\cmidrule{2-9}
 & AUC & F1-score & AUC & F1-score & AUC & F1-score & AUC & F1-score \\
\midrule
\midrule
Max-Pooling     & $\pm$0.0973    & $\pm$0.0564    & $\pm$0.0583    & $\pm$0.0467    & $\pm$0.0274    & $\pm$0.0259 & $\pm$0.0442   & $\pm$0.0631   \\
Mean-Pooling    & $\pm$0.0780    & $\pm$0.0123    & $\pm$0.0830    & $\pm$0.0622   & $\pm$0.0284    & $\pm$0.0369 & $\pm$0.0765   & $\pm$0.0454    \\
ABMIL           & $\pm$0.0464    & $\pm$0.0569    & $\pm$0.0689    & $\pm$0.0610  & $\pm$0.0239    & $\pm$0.0342  & $\pm$0.0955   & $\pm$0.0906    \\
CLAM-SB         & $\pm$0.0545    & $\pm$0.0409    & $\pm$0.0644    & $\pm$0.0769    & $\pm$0.0188    & $\pm$0.0254 & $\pm$0.0522   & $\pm$0.0454   \\
CLAM-MB         & $\pm$0.0511    & $\pm$0.0289    & $\pm$0.0682    & $\pm$0.0637   & $\pm$0.0178    & $\pm$0.0282  & $\pm$0.0417   & $\pm$0.0350   \\
DSMIL           & $\pm$0.0556    & $\pm$0.0496    & $\pm$0.1061    & $\pm$0.0859    & $\pm$0.0298  & $\pm$0.0340   & $\pm$0.0788    & $\pm$0.0725\\
TransMIL        & $\pm$0.0982    & $\pm$0.0639    & $\pm$0.0537    & $\pm$0.0238    & $\pm$0.0147   & $\pm$0.0249  & $\pm$0.0379    & $\pm$0.0287\\
\midrule
\midrule
PathMIL (Parallel)      & $\pm$0.0311            & $\pm$0.0649            & $\pm$0.0586            & $\pm$0.0523    & $\pm$0.0241   & $\pm$0.0349    & $\pm$0.0189        & $\pm$0.0291\\
PathMIL (Fine-tuning)   & $\pm$0.0315  & $\pm$0.0585   & $\pm$0.0415   & $\pm$0.0448    & $\pm$0.0160   & $\pm$0.0304   & $\pm$0.0353   & $\pm$0.0428\\
\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\begin{table*}[htb]
\caption{\textbf{Standard deviation for ablation study performed on ER+/ER- classification, PR+/PR- classification on the internal dataset, subtype classification on the TCGA-NSCLC dataset and the tumour/normal classification the CAMELYON16 dataset.} $^*$ indicates the original design of corresponding model. $^\dagger$ Although we follow the TransMIL architecture, the Lookahead optimiser is omitted during the training of TransMIL with 1DPE and 2DPE.}
\label{table:ablation_std}
\vskip 0.15in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{ccc|cccccccr}
\toprule
\multicolumn{3}{c|}{Modules} &  \multicolumn{2}{c}{Internal (ER)} &  \multicolumn{2}{c}{Internal (PR)} & \multicolumn{2}{c}{TCGA-NSCLC} & \multicolumn{2}{c}{CAMELYON16}\\
\cmidrule{1-11}
Framework & CFSD & PE & AUC & F1-score & AUC & F1-score & AUC & F1-score & AUC & F1-score \\
\midrule
\midrule
CLAM-SB$^*$     & $\times$      & $\times$  & $\pm$0.0545    & $\pm$0.0409    & $\pm$0.0644    & $\pm$0.0769    & $\pm$0.0188    & $\pm$0.0254 & $\pm$0.0522   & $\pm$0.0454   \\
CLAM-SB         & Parallel      & $\times$  & $\pm$0.0229            & $\pm$0.0452       & $\pm$0.0483   & $\pm$0.0392    & $\pm$0.0337   & $\pm$0.0477    & $\pm$0.0439    & $\pm$0.0583\\
CLAM-SB         & Fine-tuning    & $\times$  & $\pm$0.0498    & $\pm$0.0365    & $\pm$0.0724    & $\pm$0.0517    & $\pm$0.0174  & $\pm$0.0313    & $\pm$0.0310  & $\pm$0.0388\\
\cmidrule{1-11}
TransMIL$^*$    & $\times$      & PPEG        & $\pm$0.0982    & $\pm$0.0639    & $\pm$0.0537    & $\pm$0.0238    & $\pm$0.0147   & $\pm$0.0249   & $\pm$0.0379    & $\pm$0.0287\\
TransMIL$^\dagger$ & $\times$     & 1D        & $\pm$0.1158            & $\pm$0.1069            & $\pm$0.1642            & $\pm$0.1188          & $\pm$0.0213   & $\pm$0.0310    & $\pm$0.1156            & $\pm$0.0894\\
TransMIL$^\dagger$ & $\times$     & 2D        & $\pm$0.0538   & $\pm$0.0780   & $\pm$0.0534   & $\pm$0.0392 & $\pm$0.0161   & $\pm$0.0193    & $\pm$0.0430   & $\pm$0.0574\\
\midrule
\midrule
PathMIL & $\times$     & 2D  & $\pm$0.0361            & $\pm$0.068            & $\pm$0.0497            & $\pm$0.0535    & $\pm$0.0230   & $\pm$0.0428    & $\pm$0.0679        & $\pm$0.0450\\
PathMIL & Parallel     & 2D  & $\pm$0.0311            & $\pm$0.0649            & $\pm$0.0586            & $\pm$0.0523    & $\pm$0.0241   & $\pm$0.0349    & $\pm$0.0189        & $\pm$0.0291\\
PathMIL & Fine-tuning   & 2D  & $\pm$0.0315  & $\pm$0.0585   & $\pm$0.0415   & $\pm$0.0448    & $\pm$0.0160   & $\pm$0.0304   & $\pm$0.0353   & $\pm$0.0428\\
\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\pagebreak
\section{Instance-level Learnability}
\subsection{Implementation detail of instance-level experiments}
\label{appendix:empirical_inst_learnability_implementation}
To demonstrate instance-level learnability, we followed the setup in \citeauthor{jang2024learnability} using a synthetic MNIST dataset. The task is framed as a multi-class classification MIL problem, with bag-level labels assigned as shown in Table~\ref{table:synthetic_mnist}. To isolate the impact of 2DPE and given that the MNIST dataset lacks inherent positional information, we employed CLAM-SB (baseline) with CFSD to assess instance-level learnability, rather than using PathMIL with positional encoding. Hyperparameters were set in accordance with our main experiments, except for the learning rate, which was adjusted to 0.001. The MNIST dataset was split into 80\% training and 20\% evaluation data. Performance was evaluated using one-vs-rest AUC and F1-score. In this experiment, we not only empirically demonstrated instance-level learnability but also validated the multi-class classification capability of our framework.

\begin{table}[htb]
\caption{\textbf{Annotation of the synthetic MNIST dataset.}}
\label{table:synthetic_mnist}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{c|l}
\toprule
Bag-label & Description \\
\midrule
\midrule
3   & the bag contains both 3 and 5\\
2   & the bag contains 1 but not 7\\
1   & the bag contains both 1 and 7\\
0   & other combinations\\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Full table comparing the performance of CFSD on bag level and instance level}
\label{appendix:empirical_inst_learnability_full_table}
\begin{table}[htb]
\caption{\textbf{Full table: the comparison of bag-level performance $P_{bag}$ and instance-level $P_{inst}$ performance for CFSD on synthetic MNIST dataset.} The framework of CLAM-SB (baseline), CLAM-SB (CFSD-parallel) and CLAM-SB (CFSD-fine-tuning) are tested in the experiment.}
\label{table:inst_learnability_full}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{c|cccc|cc}
\toprule
Modules & \multicolumn{2}{c}{$P_{bag}$}& \multicolumn{2}{c}{$P_{inst}$}& \multicolumn{2}{c}{$P_{inst}-P_{bag}$}\\
\midrule
CFSD & AUC & F1-score & AUC & F1-score & AUC & F1-score \\
\midrule
\midrule
$\times$    & 0.9240            & 0.7329            & 0.4750 & 0.0646 & -0.4490 & -0.6683\\
Parallel    & \textbf{0.9233}   & 0.7282            & 0.7160 & \textbf{0.4030} & -0.2073 & \textbf{-0.3252} \\
Fine-tuning  & 0.9213            & \textbf{0.7363}   & \textbf{0.7858} & 0.3994 & \textbf{-0.1355} & -0.3369\\
% \midrule
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

% \pagebreak
\section{Interpretability}
\label{appendix:interpretability}
\begin{figure*}[htb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/vis.pdf}}
\caption{\textbf{Heatmap visualisation of PathMIL.} The heatmaps from two ER+ cases are shown as examples, and compare with their pathological gold standard - immunohistochemistry (IHC) stainings. In heatmap, red denotes high attention while blue denotes low attention. In IHC staining image, brown colour reveals the cells that are considered as ER+.}
\label{fig:vis}
\end{center}
\vskip -0.2in
\end{figure*}
% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
