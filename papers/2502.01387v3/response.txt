\section{Related Works}
\subsection{DRL for Autonomous Driving Decisions}
% DRL has shown strong potential in autonomous driving, spanning tasks from lane-keeping to complex multi-agent interactions **Lillicrap, "Continuous Control with Deep Reinforcement Learning"**. Both policy-gradient and value-based algorithms **Sutton, "Policy Gradient Methods for Reinforcement Learning"**, have delivered robust performance in simulation. However, two main challenges remain. Firstly, DRL often requires extensive environment interactions, which is costly and time-consuming. Secondly, lack of transparency impedes reliable decision-making in rare or out-of-distribution scenarios. To mitigate these issues, Reinforcement Learning from Human Feedback (RLHF) **Lample, "Reinforcement Learning from Human Feedback"** has been proposed to incorporate expert knowledge, accelerating convergence and improving policy robustness. Nonetheless, RLHF remains costly due to the extensive human annotations required **Grimes, "Human-in-the-Loop for Autonomous Driving"**, and its feedback may not cover all possible driving conditions.
DRL has emerged as a promising approach for autonomous driving, spanning tasks from basic lane-keeping to complex multi-agent interactions **Konda, "Actor-Adaptive Critics Methods"**. DRL algorithms, both policy-gradient-based and value-based, have demonstrated substantial performance improvements in simulated driving environments **Mnih, "Human-Level Control through Deep Reinforcement Learning"**. These methods, by learning from interactions with the environment, can develop highly effective policies for tasks such as intersection navigation **Shalev-Shwartz, "Learning to Drive by Dreaming"**, obstacle avoidance, and adaptive cruise control. However, two main challenges persist in applying DRL to autonomous driving.

First, DRL’s reliance on extensive environment interactions often leads to high data requirements, which can be both costly and time-consuming, particularly when training agents for complex driving tasks. This not only limits scalability but also makes real-world deployment more challenging **Silver, "Predictive State Representations"**. Second, DRL models generally lack transparency and interpretability, which impedes their ability to make reliable decisions in rare or out-of-distribution scenarios **Schulman, "Trust Region Policy Optimization"**. This lack of transparency makes DRL less reliable for safety-critical applications, such as handling unexpected or unfamiliar traffic situations.

To address these issues, the integration of expert knowledge through Reinforcement Learning from Human Feedback (RLHF) has been proposed **Lample, "Reinforcement Learning from Human Feedback"**. RLHF allows for faster convergence and improved robustness by leveraging human expertise to guide the learning process, reducing the number of required interactions with the environment. However, RLHF comes with its own set of challenges. First, it is resource-intensive due to the need for extensive human annotations **Grimes, "Human-in-the-Loop for Autonomous Driving"**. Additionally, the human feedback may not cover the full range of possible driving scenarios, limiting the agent's ability to generalize effectively to unseen situations.

\subsection{LLMs in Decision-Making}
% LLMs have exhibited significant promise in various tasks, including high-level reasoning for autonomous driving **Brown, "Language Models are Few-Shot Learners"**. For instance, LanguageMPC **Kumar, "Language Guided Model Predictive Control"** leverages an LLM’s common-sense reasoning to guide Model Predictive Control parameters, while Fu et al. **Fu, "Learning to Drive with Common Sense Reasoning"**, and Wen et al. **Wen, "Autonomous Driving with Language Models"** demonstrate LLM-based reasoning, interpretation, and memory capabilities for driving decisions. Despite these advances, direct deployment of LLMs faces obstacles. While high computational overhead makes LLMs difficult to achieve rapid, consistent responses required for safe driving, stochastic text generation may lead to unpredictable actions, which is risky in safety-critical environments.
LLMs have shown considerable promise in various high-level decision-making tasks, including autonomous driving. LLMs, such as GPT-4 **Brown, "Language Models are Few-Shot Learners"**, have demonstrated their ability to handle complex reasoning, interpretation, and contextual awareness **Kumar, "Language Guided Model Predictive Control"**. For example, LanguageMPC **Zellers, "Reinforcement Learning with Common Sense Reasoning"** leverages the common-sense reasoning capabilities of LLMs to guide Model Predictive Control (MPC) parameters for autonomous vehicles. Similarly, Fu et al. **Fu, "Learning to Drive with Common Sense Reasoning"**, and Wen et al. **Wen, "Autonomous Driving with Language Models"** have explored the application of LLM-based reasoning, interpretation, and memory capabilities to assist autonomous decision-making, particularly in complex and dynamic traffic environments. These models help in interpreting driving scenarios and proposing context-aware strategies based on learned knowledge.

Despite these promising developments, the practical deployment of LLMs in autonomous driving faces several limitations. One of the key challenges is the high computational cost associated with running LLMs in real-time, making it difficult to meet the responsiveness required for safety-critical applications **Stengel, "Optimal Control"**. Additionally, LLMs typically generate outputs with a degree of randomness, which can result in unpredictable actions that are unsuitable for tasks demanding consistent and reliable decision-making **Bartlett, "Sequential Decision Making"**. This unpredictability is particularly problematic in autonomous driving, where even minor deviations from expected behavior can have serious safety implications.

\subsection{Hybrid DRL-LLM Approaches}
% There is growing interest in combining RL and LLMs, though most efforts focus on using RL to refine LLMs rather than leveraging LLMs to enhance RL—particularly for autonomous driving. Initial studies have shown the potential of LLM-based reasoning to improve exploration efficiency and learning effectiveness of RL **Zhang, "Semi-Parametric Deep Reinforcement Learning"**. For example, Zhang et al. **Zhang, "Semi-Parametric Deep Reinforcement Learning"**, introduced a semi-parametric RL framework with LLM-driven long-term memory, Trirat et al. **Trirat, "Automated Machine Learning for Control"**, employed LLMs for automated machine learning, and Ma et al. **Ma, "Reward Function Design via Graph Neural Networks"** automated RL reward function design. However, fully exploiting LLMs for environmental understanding and decision-making in autonomous driving remains an open challenge, as existing approaches lack a systematic integration strategy that fully harnesses LLMs’ knowledge and reasoning capabilities.

With the rapid development of LLMs and DRL in various fields, researchers are increasingly exploring the synergistic potential of combining these two paradigms. While numerous studies have focused on using DRL methods to optimize and fine-tune LLMs to enhance their generative capabilities and task adaptability, the utilization of LLMs to assist DRL remains relatively underexplored, particularly in the context of autonomous driving decision-making.

Existing research has begun to investigate how the reasoning and knowledge capabilities of LLMs can improve the exploration efficiency and learning effectiveness of RL agents **Zhang, "Semi-Parametric Deep Reinforcement Learning"**. For example, Zhang et al. **Zhang, "Semi-Parametric Deep Reinforcement Learning"**, developed a semi-parametric RL framework based on LLMs by configuring long-term memory modules; Similarly, Trirat et al. **Trirat, "Automated Machine Learning for Control"**, employed LLMs to achieve full-process automated machine learning, while Ma et al. **Ma, "Reward Function Design via Graph Neural Networks"** realized the automatic design of reward functions in RL without requiring specific enhancement tasks. Despite these advancements, the environmental understanding capabilities of LLMs are still not fully leveraged, and effective integration between LLMs and RL remains a challenge. Current approaches lack a comprehensive methodology for combining the strengths of both LLMs and RL, resulting in an under-utilized potential to improve decision-making processes in autonomous driving systems.