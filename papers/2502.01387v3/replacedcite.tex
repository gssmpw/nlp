\section{Related Works}
\subsection{DRL for Autonomous Driving Decisions}
% DRL has shown strong potential in autonomous driving, spanning tasks from lane-keeping to complex multi-agent interactions ____. Both policy-gradient and value-based algorithms____ have delivered robust performance in simulation. However, two main challenges remain. Firstly, DRL often requires extensive environment interactions, which is costly and time-consuming. Secondly, lack of transparency impedes reliable decision-making in rare or out-of-distribution scenarios. To mitigate these issues, Reinforcement Learning from Human Feedback (RLHF) ____ has been proposed to incorporate expert knowledge, accelerating convergence and improving policy robustness. Nonetheless, RLHF remains costly due to the extensive human annotations required ____, and its feedback may not cover all possible driving conditions.
DRL has emerged as a promising approach for autonomous driving, spanning tasks from basic lane-keeping to complex multi-agent interactions ____. DRL algorithms, both policy-gradient-based and value-based, have demonstrated substantial performance improvements in simulated driving environments ____. These methods, by learning from interactions with the environment, can develop highly effective policies for tasks such as intersection navigation ____, obstacle avoidance, and adaptive cruise control. However, two main challenges persist in applying DRL to autonomous driving.

First, DRL’s reliance on extensive environment interactions often leads to high data requirements, which can be both costly and time-consuming, particularly when training agents for complex driving tasks. This not only limits scalability but also makes real-world deployment more challenging ____. Second, DRL models generally lack transparency and interpretability, which impedes their ability to make reliable decisions in rare or out-of-distribution scenarios ____. This lack of transparency makes DRL less reliable for safety-critical applications, such as handling unexpected or unfamiliar traffic situations.

To address these issues, the integration of expert knowledge through Reinforcement Learning from Human Feedback (RLHF) has been proposed ____. RLHF allows for faster convergence and improved robustness by leveraging human expertise to guide the learning process, reducing the number of required interactions with the environment. However, RLHF comes with its own set of challenges. First, it is resource-intensive due to the need for extensive human annotations ____. Additionally, the human feedback may not cover the full range of possible driving scenarios, limiting the agent's ability to generalize effectively to unseen situations. These limitations point to the need for a more efficient and scalable method that integrates expert guidance while addressing DRL's inherent drawbacks.

\subsection{LLMs in Decision-Making}
% LLMs have exhibited significant promise in various tasks, including high-level reasoning for autonomous driving ____. For instance, LanguageMPC ____ leverages an LLM’s common-sense reasoning to guide Model Predictive Control parameters, while Fu et al. ____ and Wen et al. ____ demonstrate LLM-based reasoning, interpretation, and memory capabilities for driving decisions. Despite these advances, direct deployment of LLMs faces obstacles. While high computational overhead makes LLMs difficult to achieve rapid, consistent responses required for safe driving, stochastic text generation may lead to unpredictable actions, which is risky in safety-critical environments.
LLMs have shown considerable promise in various high-level decision-making tasks, including autonomous driving. LLMs, such as GPT-4, have demonstrated their ability to handle complex reasoning, interpretation, and contextual awareness ____. For example, LanguageMPC ____ leverages the common-sense reasoning capabilities of LLMs to guide Model Predictive Control (MPC) parameters for autonomous vehicles. Similarly, Fu et al. ____ and Wen et al. ____ have explored the application of LLM-based reasoning, interpretation, and memory capabilities to assist autonomous decision-making, particularly in complex and dynamic traffic environments. These models help in interpreting driving scenarios and proposing context-aware strategies based on learned knowledge.

Despite these promising developments, the practical deployment of LLMs in autonomous driving faces several limitations. One of the key challenges is the high computational cost associated with running LLMs in real-time, making it difficult to meet the responsiveness required for safety-critical applications ____. Additionally, LLMs typically generate outputs with a degree of randomness, which can result in unpredictable actions that are unsuitable for tasks demanding consistent and reliable decision-making. This unpredictability is particularly problematic in autonomous driving, where even minor deviations from expected behavior can have serious safety implications. Thus, while LLMs offer significant potential for decision-making in autonomous driving, their practical use as standalone decision-making agents is limited by their real-time performance and output consistency.


\subsection{Hybrid DRL-LLM Approaches}
% There is growing interest in combining RL and LLMs, though most efforts focus on using RL to refine LLMs rather than leveraging LLMs to enhance RL—particularly for autonomous driving. Initial studies have shown the potential of LLM-based reasoning to improve exploration efficiency and learning effectiveness of RL____. For example, Zhang et al. ____ introduced a semi-parametric RL framework with LLM-driven long-term memory, Trirat et al. ____ employed LLMs for automated machine learning, and Ma et al. ____ automated RL reward function design. However, fully exploiting LLMs for environmental understanding and decision-making in autonomous driving remains an open challenge, as existing approaches lack a systematic integration strategy that fully harnesses LLMs’ knowledge and reasoning capabilities.

With the rapid development of LLMs and DRL in various fields, researchers are increasingly exploring the synergistic potential of combining these two paradigms. While numerous studies have focused on using DRL methods to optimize and fine-tune LLMs to enhance their generative capabilities and task adaptability, the utilization of LLMs to assist DRL remains relatively underexplored, particularly in the context of autonomous driving decision-making.

Existing research has begun to investigate how the reasoning and knowledge capabilities of LLMs can improve the exploration efficiency and learning effectiveness of RL agents____. For example, Zhang et al.____ developed a semi-parametric RL framework based on LLMs by configuring long-term memory modules; Similarly, Trirat et al.____ employed LLMs to achieve full-process automated machine learning, while Ma et al.____ realized the automatic design of reward functions in RL without requiring specific enhancement tasks. Despite these advancements, the environmental understanding capabilities of LLMs are still not fully leveraged, and effective integration between LLMs and RL remains a challenge. Current approaches lack a comprehensive methodology for combining the strengths of both LLMs and RL, resulting in an under-utilized potential to improve decision-making processes in autonomous driving systems.