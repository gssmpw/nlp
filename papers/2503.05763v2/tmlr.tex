
\documentclass[10pt]{article} % For LaTeX2e
%\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}
\usepackage{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[capitalise]{cleveref}
\usepackage{multirow}
\usepackage{ulem}
\usepackage{amsmath, amsfonts}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{Graph Masked Language Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Aarush Sinha \email aarush.sinha2021@vistudent.ac.in \\
      \addr School of Computer Science and Engineering (SCOPE)\\
      Vellore Institute of Technology-Chennai, India
      \AND
      \name Om Kumar CU \email omkumar.cu@vit.ac.in \\
      \addr School of Computer Science and Engineering (SCOPE)\\
      Vellore Institute of Technology-Chennai, India
}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
    Language Models (LMs) and Graph Neural Networks (GNNs) have shown great promise in their respective areas, yet integrating structured graph data with rich textual information remains challenging. In this work, we propose \emph{Graph Masked Language Models} (GMLM), a novel dual-branch architecture that combines the structural learning of GNNs with the contextual power of pretrained language models. Our approach introduces two key innovations: (i) a \emph{semantic masking strategy} that leverages graph topology to selectively mask nodes based on their structural importance, and (ii) a \emph{soft masking mechanism} that interpolates between original node features and a learnable mask token, ensuring smoother information flow during training. Extensive experiments on multiple node classification and language understanding benchmarks demonstrate that GMLM not only achieves state-of-the-art performance but also exhibits enhanced robustness and stability. This work underscores the benefits of integrating structured and unstructured data representations for improved graph learning.
\end{abstract}

\section{Introduction}

Graph Neural Networks (GNNs) \cite{4700287}, \cite{zhou2021graphneuralnetworksreview} have demonstrated remarkable success in tasks such as node classification, link prediction, and graph classification. However, most existing GNN architectures primarily focus on structural information and local neighborhood features, potentially overlooking rich semantic relationships between nodes. While recent advances in natural language processing, particularly pre-trained language models like BERT \cite{devlin-etal-2019-bert}, have revolutionized our ability to capture contextual information, their integration with graph learning remains largely unexplored.

Graph Attention Networks (GATs) \cite{veličković2018graphattentionnetworks} have shown promise in selectively aggregating neighborhood information through learnable attention mechanisms. However, their effectiveness can be limited by the challenge of capturing long-range dependencies and semantic relationships in graphs. Meanwhile, masked language modeling, a key innovation in BERT, has proven highly effective for learning contextual representations in text data. This raises an intriguing question: Can we adapt masked modeling techniques to enhance graph representation learning?

Traditional node masking approaches in GNNs \cite{mishra2021nodemaskingmakinggraph} often employ random masking strategies or complete feature removal, which can lead to information loss and suboptimal learning. Furthermore, these approaches typically ignore the structural importance of nodes in the graph topology, potentially masking critical nodes that are essential for understanding the global graph structure.

In this paper, we present a novel framework that bridges these gaps by combining the structural learning capabilities of GATs with the contextual understanding of BERT-style masked modeling. Our approach introduces two key innovations: (1) a semantic masking strategy that considers node importance in the graph topology, and (2) a soft masking mechanism that allows for partial information flow during training. This combination enables our model to learn both structural and semantic patterns effectively, leading to more robust node representations.
% ~\footnote{\href{https://github.com/chungimungi/GMLM}{GitHub Repository}}.

Our main contributions of this work can be summarized as follows:

\begin{enumerate}
    \item We propose a novel dual-branch architecture that effectively combines Graph Attention Networks with BERT-style masked modeling for enhanced node representation learning.
    
    \item We introduce a semantic masking strategy that selectively masks nodes based on their structural importance in the graph, ensuring that the model learns from topologically significant nodes. We also develop a soft masking mechanism that creates interpolated node representations, allowing for gradual information flow during training and better preservation of feature relationships \cref{sem}.
    
    \item We design a multi-layer fusion network that effectively combines structural and semantic information from the graph and language model branches.
    
\end{enumerate}

\section{Related Works}

Graph Neural Networks (GNNs) \cite{zhou2021graphneuralnetworksreview} have emerged as a powerful paradigm for learning representations from graph-structured data. Early seminal works (e.g., Kipf and Welling, 2017) formalized spectral and spatial convolutional approaches that have since been extended to diverse applications ranging from molecular property prediction to social network analysis. In parallel, the development of Graph Attention Networks (GATs) \cite{veličković2018graphattentionnetworks} introduced self-attention \cite{10.5555/3295222.3295349} into the GNN framework, thereby allowing each node to weigh its neighbors differentially during message passing. This adaptive weighting not only enhances the model’s expressivity but also addresses the challenges of learning on irregular graph structures. These advances are surveyed comprehensively in recent literature that categorizes and contrasts various GNN and graph attention models.
A notable line of work focuses on adapting the Transformer architecture—which was originally developed for sequence modeling—to graphs. Graph Transformers aim to overcome limitations of traditional message passing by leveraging global self-attention mechanisms that can capture long-range dependencies across nodes. Several recent surveys have systematically reviewed these models, highlighting their ability to integrate positional information and to operate on both small‐scale and large-scale graph data \cite{shehzad2024graphtransformerssurvey} .In this vein, transformer‐based architectures have been shown to achieve competitive performance on graph tasks while also inheriting benefits from pretrained language models when applied to text‐rich graphs.

\textbf{Masked Language Models} such as BERT \cite{devlin-etal-2019-bert} has redefined the state-of-the-art in natural language understanding by pretraining on large text corpora with a masked token prediction objective. MLMs are inherently designed to capture deep contextual representations, and recent investigations have explored their capacity to encode rich semantic features when applied in isolation or as a backbone for downstream tasks. The Masked Graph Autoencoder (MaskGAE) \cite{mgae} framework employs this strategy by randomly masking a significant portion of edges and training the model to reconstruct them, thereby enhancing its ability to learn meaningful representations for downstream tasks like link prediction and node classification

\textbf{Integration of Language Models and Graph Structures} have increasingly focused on bridging the gap between textual and structural modalities. One direction leverages the strengths of pretrained language models (LMs) and integrates them with graph neural architectures to jointly reason over both text and graph structure. For instance, Graph Language Models (GLMs) have been introduced to fuse structural biases from graphs with the linguistic capabilities of LMs, enabling joint processing of interleaved textual and graph inputs \cite{plenz-frank-2024-graph}. Complementary approaches, such as GraSAME, inject token-level structural information directly into pretrained LMs via a graph-guided self-attention mechanism, thereby eliminating the need for separate GNN modules while preserving semantic richness \cite{yuan-farber-2024-grasame}. Moreover, recent proposals argue that the Transformer itself can be interpreted as a special case of a graph neural network, wherein fully connected attention corresponds to complete graph structures over tokens. Extending this perspective, architectures such as “Transformers as Graph-to-Graph Models” make the graph nature of the attention mechanism explicit, allowing for iterative graph refinement and explicit modeling of edge information in latent representations \cite{henderson-etal-2023-transformers}. Language Model GNN (LM-GNN) framework jointly trains large-scale language models and GNNs, aiming to combine the contextual understanding of language models with the structural learning capabilities of GNNs \cite{ioannidis2022efficienteffectivetraininglanguage}. 

\section{Methodology}

Given an attributed graph $G = (V, E)$ with node feature matrix $X \in \mathbb{R}^{|V| \times d}$ and binary adjacency matrix $A \in \{0,1\}^{|V| \times |V|}$, our goal is to learn robust node representations that capture both the graph structure and semantic context. Each node $v_i \in V$ is associated with a feature vector $x_i \in \mathbb{R}^d$ and belongs to one of $C$ classes. To achieve this, we introduce (i) a \emph{Semantic Node Masking} strategy that selectively perturbs node features based on structural importance, and (ii) a \emph{Dual-Branch Architecture} that integrates graph-based and language-based embeddings.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{images/semantic.pdf}
    \caption{Semantic and Soft Masking of Nodes allows partial information to flow instead of binary on/off masking, the model can learn from both masked and original information simultaneously, creating a smoother, more continuous learning space with improved gradient flow.}
    \label{sem}
\end{figure*}

\subsection{Semantic Node Masking}
Conventional random masking treats all nodes uniformly, often masking nodes that are crucial for maintaining graph connectivity. In contrast, our semantic masking strategy uses degree centrality to determine the importance of each node. By doing so, we ensure that nodes with higher connectivity, which are more influential in maintaining the overall structure are masked in a controlled manner. This design is inspired by graph theory concepts related to network resilience and influence maximization, where highly connected nodes play a pivotal role.\cref{alg:semantic_mask} provides an overview of the first step as seen in \cref{sem}.

\begin{algorithm}[!ht]
\caption{Semantic Node Mask Generation}
\label{alg:semantic_mask}
\begin{algorithmic}[1]
\Require Graph data with:
    \begin{itemize}
        \item Node features $X \in \mathbb{R}^{N \times d}$
        \item Edge index $E$
        \item Boolean training mask \texttt{train\_mask} (size $N$)
        \item Mask ratio $r \in (0,1)$
    \end{itemize}
\Ensure Boolean mask vector \texttt{mask} of length $N$

\State $train\_idx \gets \{ i \mid \texttt{train\_mask}[i] = \text{True} \}$
\State $num\_train \gets |train\_idx|$
\State $num\_mask \gets \max(1, \lfloor r \cdot num\_train \rfloor)$

\For {each node $i \in train\_idx$}
    \State Compute degree: $d_i \gets \sum_{j \in \mathcal{N}(i)} 1$
\EndFor

\State Compute total degree: $D \gets \sum_{i \in train\_idx} d_i$
\For {each node $i \in train\_idx$}
    \State Set sampling probability: $p_i \gets \frac{d_i}{D}$
\EndFor

\State Sample \textbf{without replacement} a set $S \subseteq train\_idx$ of $num\_mask$ nodes according to probabilities $\{p_i\}$
\State Initialize \texttt{mask} as a Boolean vector of length $N$ with all entries \texttt{False}
\For {each node $i \in S$}
    \State Set \texttt{mask}[i] $\gets$ \texttt{True}
\EndFor
\State \Return \texttt{mask}
\end{algorithmic}
\end{algorithm}

\subsubsection{Degree-based Node Selection}

We compute the degree $d_i$ of a node $v_i$ as:
\[
    d_i = \sum_{j \in V} A_{ij}.
\]
The probability of selecting a node for masking is proportional to its degree, computed over the training set $V_{\text{train}}$:
\[
    P(v_i) = \frac{d_i}{\sum_{j \in V_{\text{train}}} d_j}.
\]
This selection strategy ensures that:
\begin{itemize}
    \item Highly connected nodes, which are crucial for graph connectivity, are more likely to be masked.
    \item The masking probability distribution is adaptive to the topology of the graph.
    \item Only training nodes are considered for masking, thereby preserving the integrity of validation and test sets.
\end{itemize}


\subsection{Soft Masking Mechanism}

\Cref{alg:soft_masking} provides an overview of the second step, as illustrated in \Cref{sem}. Unlike binary masking approaches that completely remove a node’s feature information, we propose a \textit{soft masking} strategy that interpolates between the original feature vector and a learnable mask token \(m \in \mathbb{R}^d\). This design ensures that critical information is not abruptly removed, improving gradient flow and optimizing information retention.

Specifically, for a node \(v_i\), the masked feature \(\tilde{x}_i\) is computed as follows:

\begin{equation}
    \tilde{x}_i =
    \begin{cases}
        (1-\beta)x_i + \beta\, m, & \text{if } v_i \text{ is selected for masking}, \\
        x_i, & \text{otherwise}.
    \end{cases}
\end{equation}

The interpolation coefficient \(\beta \in [0,1]\) controls the degree of masking. In our experiments, we set \(\beta = 0.7\), striking a balance between preserving original node features and introducing masked noise. This gradual transformation allows partial information to persist, leading to:

\begin{itemize}
    \item \textbf{Improved Gradient Flow:} Unlike hard masking, which creates sharp discontinuities in the loss landscape, soft masking ensures a smoother optimization process. This continuous transition reduces gradient variance and mitigates training instability.
    \item \textbf{Enhanced Information Retention:} Soft masking maintains a partial connection between masked nodes and their original features, preserving important semantic and structural information.
    \item \textbf{Alignment with Information Theory Concepts:} Inspired by the information bottleneck principle \cite{tishby2000informationbottleneckmethod}, soft masking ensures that only a controlled amount of information is removed, reducing redundancy while preserving essential task-relevant features.
\end{itemize}

\begin{algorithm}[!ht]
\caption{Soft Masking of Node Features}
\label{alg:soft_masking}
\begin{algorithmic}[1]
\Require 
    \begin{itemize}
        \item Node feature matrix \(X \in \mathbb{R}^{N \times d}\)
        \item Boolean mask vector \texttt{mask} (length \(N\))
        \item Learnable mask token \(m \in \mathbb{R}^d\)
        \item Interpolation coefficient \(\beta \in [0,1]\)
    \end{itemize}
\Ensure Soft-masked feature matrix \(\tilde{X} \in \mathbb{R}^{N \times d}\)

\State \(\tilde{X} \gets X\) \quad \Comment{Copy original features}
\For {each node \(i = 1\) to \(N\)}
    \If {\texttt{mask}[i] is \textbf{True}}
        \State Update feature: \(\tilde{X}[i] \gets (1-\beta) \cdot X[i] + \beta \cdot m\)
    \EndIf
\EndFor
\State \Return \(\tilde{X}\)
\end{algorithmic}
\end{algorithm}

\subsection{Node Importance and Semantic Masking Rationale}

In our framework, semantic masking is guided by the notion of node importance, with degree centrality serving as a straightforward and effective measure. Degree centrality, defined as the number of edges incident on a node, is a strong proxy for node influence because highly connected nodes are generally more influential in propagating information and maintaining the overall connectivity of the graph. In many real-world datasets, these nodes are critical for ensuring that the graph remains cohesive, as their removal or perturbation can significantly disrupt network structure.

Masking nodes with high degree centrality compels the model to learn more robust representations. By targeting these \textit{critical nodes}, we force the network to rely less on a few dominant features and instead capture diverse pathways and latent structural patterns. This idea is conceptually aligned with notions of \textit{network robustness} in graph theory, where the resilience of a network is evaluated by its ability to withstand the removal of key nodes.

While other centrality measures such as betweenness, closeness, and eigenvector centrality offer valuable insights into different aspects of node importance, degree centrality was chosen for its simplicity and computational efficiency. Betweenness centrality, which quantifies the frequency of a node appearing on shortest paths between other nodes, can be computationally prohibitive for large-scale graphs. Closeness centrality measures the average distance of a node from all others, and eigenvector centrality reflects the influence of a node’s neighbors. Despite their theoretical advantages, these alternative measures often incur higher computational costs without a proportional gain in effectiveness for our masking strategy. Consequently, degree centrality strikes an optimal balance, making it a practical and powerful choice for our semantic masking approach.

\subsection{Dual-Branch Architecture}

% \begin{figure*}
%     \centering
%     \includegraphics[width=1\linewidth]{archi.pdf}
%     \caption{Overall architecture of the dual branched architecture of proposed Graph Masked Language Model}
%     \label{method}
% \end{figure*}

The novelty of GMLM also lies in its dual-branch design that integrates a GAT branch with a BERT branch. The GAT branch captures the intrinsic graph structure using attention mechanisms, while the BERT branch extracts rich semantic representations from node-associated text. A multi-layer fusion network then combines these embeddings to produce a robust node representation that benefits from both structural and semantic perspectives. This synergistic combination overcomes the limitations seen in methods that rely exclusively on one modality.

\subsubsection{Graph Attention Branch}
Our model consists of two branches that extract complementary features from the graph. The first GAT layer computes hidden representations as:
{\small
\[
    h^{(1)}_i = \text{ELU}\Bigl( \text{BN} \Bigl( \big\Vert_{k=1}^{K_1} \sum_{j \in \mathcal{N}_i} \alpha_{ij,k}^{(1)} W_k^{(1)} \tilde{x}_j \Bigr) \Bigr),
\]}
where:
\begin{itemize}
    \item $K_1 = 4$ is the number of attention heads.
    \item $\mathcal{N}_i$ denotes the set of neighbors of node $v_i$.
    \item $W_k^{(1)} \in \mathbb{R}^{d' \times d}$ is the learnable weight matrix for head $k$.
    \item $\alpha_{ij,k}^{(1)}$ are attention coefficients computed by:
    {\small
    \[
        \alpha_{ij,k}^{(1)} = \frac{\exp\bigl(\text{LeakyReLU}(a_k^T [W_k^{(1)} \tilde{x}_i \, \Vert \, W_k^{(1)} \tilde{x}_j])\bigr)}{\sum_{l \in \mathcal{N}_i} \exp\bigl(\text{LeakyReLU}(a_k^T [W_k^{(1)} \tilde{x}_i \, \Vert \, W_k^{(1)} \tilde{x}_l])\bigr)},
    \]}
    with $a_k \in \mathbb{R}^{2d'}$ being the attention vector for head $k$, and $\Vert$ denoting concatenation.
    \item BN and ELU denote batch normalization and the Exponential Linear Unit activation function, respectively.
\end{itemize}

The second GAT layer refines the representations as:
{\small
\[
    h^{(2)}_i = \text{ELU}\Bigl( \text{BN} \Bigl( \big\Vert_{k=1}^{K_2} \sum_{j \in \mathcal{N}_i} \alpha_{ij,k}^{(2)} W_k^{(2)} h^{(1)}_j \Bigr) \Bigr),
\]}
where $K_2 = 2$ denotes the number of attention heads in the second layer.

\subsubsection{BERT Branch}
We use DistilBERT \cite{sanh2020distilbertdistilledversionbert} as our masking model. The BERT branch processes node states as tokens:

\[
    t_i =
    \begin{cases}
        \text{``[MASK]''}, & \text{if } v_i \text{ is masked}, \\
        \text{``node''}, & \text{otherwise}.
    \end{cases}
\]
The token is then processed by BERT:
\[
    b_i = \text{BERT}(\text{tokenize}(t_i)),
\]
yielding the BERT embedding $b_i \in \mathbb{R}^{d_b}$ for node $v_i$. These embeddings capture contextual information that complements the structural cues obtained from the GAT branch.

\subsubsection{Multi-layer Fusion Network}
The fusion network combines the graph structural features and the BERT contextual embeddings. First, the two embeddings are concatenated:
\[
    z_i = \bigl[ h^{(2)}_i \, \Vert \, b_i \bigr].
\]
Next, the concatenated vector is passed through a two-layer fully-connected network with residual connections and layer normalization:
\[
    f_i = \text{LayerNorm}\Bigl( W_2 \, \text{ReLU}\bigl( \text{LayerNorm}(W_1 z_i) \bigr) \Bigr),
\]
where $W_1$ and $W_2$ are learnable weight matrices. This fusion step ensures that both structural and semantic features are effectively integrated while maintaining stable training dynamics.

\subsection{Node Classification}
The final node representations $f_i$ are used to perform node classification. A classification head maps the fused representations to class probabilities:
\[
    \hat{y}_i = \text{softmax}\Bigl( W_c \, \text{ReLU}(W_h f_i) \Bigr),
\]
where $W_h$ and $W_c$ are learnable projection matrices. The softmax function ensures that the output $\hat{y}_i \in \mathbb{R}^C$ represents a valid probability distribution over the $C$ classes.

\section{Training}

\subsection{Node Classification}

To train GMLM for node classification tasks we follow the below described procedure.

Our training framework consists of two phases:
\begin{enumerate}
    \item \textbf{Contrastive Pretraining:} Learn robust node representations by aligning embeddings from different views of the graph.
    \item \textbf{Supervised Fine-Tuning:} Adapt the pretrained representations for node classification using a label-smoothed cross-entropy loss.
\end{enumerate}

\subsubsection{Contrastive Pretraining}
The goal of the contrastive pretraining phase is to maximize the agreement between different views of the same node while ensuring separation from other nodes.

\subsubsection{View Generation}
For each training iteration, two views of the graph are generated by applying distinct random masks to the node features:
\[
    \tilde{X}_1 = \text{Mask}(X, r_1), \quad \tilde{X}_2 = \text{Mask}(X, r_2),
\]
where the mask ratios $r_1$ and $r_2$ are independently sampled from:
\[
    r_1, r_2 \sim \text{Uniform}(0.2, 0.4).
\]
This procedure provides complementary views that challenge the model to learn invariant representations.

\subsubsection{Contrastive Loss}
We adopt the NT-Xent (Normalized Temperature-scaled Cross Entropy) loss~\cite{10.5555/3157096.3157304} to align the embeddings from the two views. Let $z_i$ and $z'_i$ denote the $\ell_2$-normalized embeddings for node $v_i$ from the two views. The loss is defined as:
\[
    \mathcal{L}_{\text{cont}} = \frac{1}{2N}\sum_{i=1}^N \left[-\log \frac{\exp\left(z_i^T z'_i/\tau\right)}{\sum_{k=1}^{2N} \mathbbm{1}_{[k \neq i]}\exp\left(z_i^T z_k/\tau\right)}\right],
\]
where:
\begin{itemize}
    \item $\tau = 0.5$ is the temperature parameter.
    \item $N$ is the number of nodes.
    \item $\mathbbm{1}_{[k \neq i]}$ is an indicator function that excludes the positive pair from the denominator.
\end{itemize}

\subsubsection{Supervised Fine-Tuning}
In the fine-tuning phase, the pretrained model is optimized for the node classification task.

\subsubsection{Loss Function with Label Smoothing}
We utilize a cross-entropy loss with label smoothing to prevent overconfidence. The loss function is given by:
{\small
\[
    \mathcal{L}_{\text{cls}} = -\sum_{i \in V_{\text{train}}} \left[(1-\epsilon) \, y_i \log(\hat{y}_i) + \epsilon \sum_{j \neq y_i} \frac{1}{C-1}\log(\hat{y}_j)\right],
\]}
where:
\begin{itemize}
    \item $\epsilon = 0.1$ is the smoothing parameter.
    \item $y_i$ denotes the ground-truth label (expressed in one-hot encoding).
    \item $\hat{y}_i$ is the predicted probability for the correct class.
    \item $C$ is the number of classes.
\end{itemize}

\subsubsection{Learning Rate Strategy}
Different components of the model are optimized with different learning rates:
\[
    \eta = \begin{cases}
        10^{-3} & \text{for graph parameters},\\[1ex]
        10^{-5} & \text{for BERT parameters},\\[1ex]
        10^{-4} & \text{for other parameters}.
    \end{cases}
\]

We employ cosine annealing with warm restarts to update the learning rate during training:
\[
    \eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{T_{\text{cur}}}{T_{i}}\pi\right)\right),
\]
where:
\begin{itemize}
    \item $T_{\text{cur}}$ is the number of epochs since the last restart.
    \item $T_{i}$ is the total number of epochs in the current cycle.
    \item The initial cycle is set to $T_0 = 20$ epochs.
    \item A multiplication factor $T_{\text{mult}} = 2$ is used to extend each subsequent cycle.
\end{itemize}

\subsection{Language understanding}

To train \textbf{GMLM} for language understanding tasks, we follow a two-phase setup similar to node classification. However, instead of using fixed hyperparameters, we employ \textit{grid search} for hyperparameter tuning.

We perform grid search over three key variables: \texttt{pre\_train\_epochs}, \texttt{temperature}, and \texttt{finetune\_epochs}. The specific values considered for each parameter are as follows:

\begin{itemize}
    \item \texttt{temperature\_values} = \{0.3, 0.5, 0.7\}
    \item \texttt{pretrain\_epoch\_values} = \{5, 10\}
    \item \texttt{finetune\_epoch\_values} = \{5, 10\}
\end{itemize}

\section{Results}

\subsection{Node Classification}

\begin{table*}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c}
\toprule
Method & \textbf{Cora} & \textbf{Citeseer} & \textbf{Pubmed} & \textbf{Amazon P} & \textbf{Amazon Comp.} & \textbf{Coauthor CS} \\
\midrule
DGI & 81.7 $\pm$ 0.6 & 71.5 $\pm$ 0.7 & 77.3 $\pm$ 0.6 & 83.1 $\pm$ 0.3 & 83.6 $\pm$ 0.2 & 90.0 $\pm$ 0.3 \\
MVGRL & 82.9 $\pm$ 0.7 & 72.6 $\pm$ 0.7 & 79.4 $\pm$ 0.3 & 87.3 $\pm$ 0.1 & 82.8 $\pm$ 0.1 & 91.3 $\pm$ 0.1 \\
GRACE & 80.0 $\pm$ 0.4 & 71.7 $\pm$ 0.6 & 79.5 $\pm$ 1.1 & 81.8 $\pm$ 0.8 & \underline{89.5 $\pm$ 0.3} & 71.1 $\pm$ 0.2 \\
CCA-SSG & 84.2 $\pm$ 0.4 & 73.1 $\pm$ 0.3 & 81.6 $\pm$ 0.4 & 93.1 $\pm$ 0.1 & 88.7 $\pm$ 0.3 & \underline{93.3 $\pm$ 0.2} \\
SUGRL & 83.4 $\pm$ 0.5 & 73.0 $\pm$ 0.4 & 81.9 $\pm$ 0.3 & 93.2 $\pm$ 0.4 & 88.9 $\pm$ 0.2 & 92.2 $\pm$ 0.5 \\
S3-CL & \underline{84.5 $\pm$ 0.4} & \underline{74.6 $\pm$ 0.4} & 80.8 $\pm$ 0.3 & 89.0 $\pm$ 0.5 & N/A & 93.1 $\pm$ 0.4 \\
GraphMAE & 84.2 $\pm$ 0.4 & 73.1 $\pm$ 0.4 & 83.9 $\pm$ 0.3 & 90.7 $\pm$ 0.4 & 79.4 $\pm$ 0.5 & 93.1 $\pm$ 0.1 \\
GMI & 82.7 $\pm$ 0.2 & 73.3 $\pm$ 0.3 & 77.3 $\pm$ 0.6 & 83.1 $\pm$ 0.3 & N/A & 91.0 $\pm$ 0.0 \\
BGRL & 83.8 $\pm$ 1.6 & 72.3 $\pm$ 0.9 & \underline{86.0 $\pm$ 0.3} & \underline{93.2 $\pm$ 0.3} & \textbf{90.3 $\pm$ 0.2} & 93.3 $\pm$ 0.1 \\
GCN & 81.5 $\pm$ 1.3 & 71.9 $\pm$ 1.9 & 77.8 $\pm$ 2.9 & 91.2 $\pm$ 1.2 & 82.6 $\pm$ 2.4 & 91.1 $\pm$ 0.5 \\
GAT & 81.8 $\pm$ 1.3 & 71.4 $\pm$ 1.9 & 78.7 $\pm$ 2.3 & 85.7 $\pm$ 20.3 & 78.0 $\pm$ 19.0 & 90.5 $\pm$ 0.6 \\
MoNet & 81.3 $\pm$ 1.3 & 71.2 $\pm$ 2.0 & 78.6 $\pm$ 2.3 & 91.2 $\pm$ 1.3 & 83.5 $\pm$ 2.2 & 90.8 $\pm$ 0.6 \\
GS-mean & 79.2 $\pm$ 7.7 & 71.6 $\pm$ 1.9 & 77.4 $\pm$ 2.2 & 91.4 $\pm$ 1.3 & 82.4 $\pm$ 1.8 & 91.3 $\pm$ 2.8 \\
GS-maxpool & 76.6 $\pm$ 1.9 & 67.5 $\pm$ 2.3 & 76.1 $\pm$ 2.3 & 90.4 $\pm$ 1.3 & N/A & 85.0 $\pm$ 1.1 \\
GS-meanpool & 77.9 $\pm$ 2.4 & 68.6 $\pm$ 2.4 & 76.5 $\pm$ 2.4 & 90.7 $\pm$ 1.6 & 79.9 $\pm$ 2.3 & 89.6 $\pm$ 0.9 \\
MLP & 58.2 $\pm$ 2.1 & 59.1 $\pm$ 2.3 & 70.0 $\pm$ 2.1 & 69.6 $\pm$ 3.8 & 44.9 $\pm$ 5.8 & 88.3 $\pm$ 0.7 \\
LogReg & 57.1 $\pm$ 2.3 & 61.0 $\pm$ 2.2 & 64.1 $\pm$ 3.1 & 73.0 $\pm$ 6.5 & 64.1 $\pm$ 5.7 & 86.4 $\pm$ 0.9 \\
LabelProp & 74.4 $\pm$ 2.6 & 67.8 $\pm$ 2.1 & 70.5 $\pm$ 5.3 & 72.6 $\pm$ 11.1 & 70.8 $\pm$ 8.1 & 73.6 $\pm$ 3.9 \\
LabelProp NL & 73.9 $\pm$ 1.6 & 66.7 $\pm$ 2.2 & 72.3 $\pm$ 2.9 & 83.9 $\pm$ 2.7 & 75.0 $\pm$ 2.9 & 76.7 $\pm$ 1.4 \\
\midrule
GMLM (w/o Semantic Masking) & 86.8 $\pm$ 0.6 & 75.6 $\pm$ 1.1 & 85.3 $\pm$ 0.3 & 92.9 $\pm$ 0.4 & 78.3 $\pm$ 6.8 & 93.0 $\pm$ 0.3 \\
GMLM (w/o Soft Masking) & 85.2 $\pm$ 1.8 & 74.8 $\pm$ 1.1 & 85.4 $\pm$ 0.1 & 93.5 $\pm$ 0.8 & 71.8 $\pm$ 11.6 & 92.4 $\pm$ 0.6 \\
GMLM  & \textbf{87.5 $\pm$ 0.9} & \textbf{75.7$\pm$0.6} & \textbf{86.2$\pm$0.2} & \textbf{93.6$\pm$0.5} & 85.1$\pm$1.7 & \textbf{93.7$\pm$0.3} \\
\bottomrule
\end{tabular}%
}
\caption{Mean test set accuracy and standard deviation across 7 runs in percent. Highest scores are in \textbf{bold}, second highest are \underline{underlined}.}
\label{main}
\end{table*}

\Cref{main} presents the test set accuracy and standard deviation for all models across six benchmark datasets. Our proposed method, \textit{Graph Masked Language Models} (GMLM), is compared with self-supervised learning approaches, including \textit{BGRL} \cite{10.5555/3157096.3157304}, \textit{S3-CL} \cite{ding2022elicitingstructuralsemanticglobal}, \textit{GraphMAE} \cite{hou2022graphmaeselfsupervisedmaskedgraph}, as well as semi-supervised and unsupervised techniques such as \textit{GCN} \cite{kipf2017semisupervisedclassificationgraphconvolutional} and \textit{SUGRL} \cite{Mo_AAAI_2022}. Our results are reported across 7 runs on all the datasets and follow the same evaluation protocol as \cite{veličković2018graphattentionnetworks}, \cite{ju2023multitaskselfsupervisedgraphneural}, \cite{ding2022elicitingstructuralsemanticglobal}, etc.

Our method achieves SOTA performance on multiple datasets. Specifically, on the \textbf{Cora} dataset \cite{planet}, we attain an accuracy of \textbf{87.5\%}, surpassing existing methods. Similarly, on \textbf{Citeseer} \cite{planet}, our model achieves \textbf{75.7\%}, and on \textbf{Pubmed} \cite{planet}, we reach \textbf{86.2\%}. 

Furthermore, GMLM also achieves SOTA performance, with an accuracy of \textbf{93.7\%} on \textbf{Coauthor CS} \cite{shchur2019pitfallsgraphneuralnetwork}, and \textbf{93.6\%} on \textbf{Amazon Photo} \cite{shchur2019pitfallsgraphneuralnetwork}. 

We also evaluate GMLM without Semantic and Soft Masking. \textit{GMLM w/o Semantic Masking}, which retains only Soft Masking, achieved performance relatively close to GMLM but exhibited lower stability, as indicated by high standard deviations, particularly in Citeseer (\textbf{1.1}) and Amazon Computers (\textbf{6.8}). In contrast, \textit{GMLM w/o Soft Masking} experienced significant performance drops in both mean test accuracy and standard deviation, most notably in Amazon Computers, where it recorded \textbf{71.8 $\pm$ 11.6}.

Hence, incorporating both Semantic Masking and Soft Masking ensures the most stable performance deviations while simultaneously achieving SOTA results.

\subsection{Language Understanding}

Through extensive grid search, we identified the best-performing hyperparameters for each dataset in the GLUE benchmark. We optimized three key variables: \texttt{temperature}, \texttt{pre\_train\_epochs}, and \texttt{finetune\_epochs}. The results indicate that different datasets benefit from distinct hyperparameter configurations, reinforcing the importance of task-specific tuning. The best-performing settings for each task are summarized in Table~\ref{tab:best_hyperparams}.

\begin{table}[!ht]
    \centering
    \begin{tabular}{l c c c}
        \toprule
        Task  & Temperature & Pretrain Epochs & Finetune Epochs \\
        \midrule
        WNLI  & 0.3 & 10  & 5  \\
        CoLA  & 0.3 & 5   & 10 \\
        STS-B & 0.7 & 5   & 10 \\
        MRPC  & 0.5 & 5   & 10 \\
        SST-2 & 0.7 & 10  & 10 \\
        RTE   & 0.5 & 10  & 5  \\
        \bottomrule
    \end{tabular}
    \caption{Best hyperparameter settings for each GLUE task, determined via grid search.}
    \label{tab:best_hyperparams}
\end{table}


\begin{table*}[!ht]
    \centering
    \begin{tabular}{l c c c c c c c}
        \toprule
        Model & \textbf{Score} & CoLA & MRPC & RTE & SST-2 & STS-B & WNLI \\
        \midrule
        ELMo & 66.6 & 44.1 & 76.6 & 53.4 & 91.5 & 70.4 & 56.3 \\
        BERT-base & \textbf{74.3} & \textbf{56.3} & \underline{}{88.6} & \textbf{69.3} & \textbf{92.7} & \textbf{89.0} & 53.5 \\
        DistilBERT & 72.2 & 51.3 & 87.5 & 59.9 & \underline{91.3} & 86.9 & 56.3 \\
        \midrule
        GMLM (w/o Semantic Masking) & 71.6 & 51.5 & 86.8 & 59.8 & 88.4 & 85.9 & 57.7 \\
        GMLM (w/o Soft Masking) & 70.6 & 49.7 & 86.4 & 57.9 & 87.6 & 84.8 & 57.2 \\
        GMLM & \underline{73.3} & \underline{52.7} & \textbf{88.8} & \underline{61.3} & 89.6 & \underline{87.0} & \textbf{60.6} \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation on GLUE. \textbf{Bold} denotes the best score, and \underline{underline} denotes the second best. ELMo, BERT, and DistilBERT results are as reported by the authors.}
    \label{tab:glue}
\end{table*}

To further assess the effectiveness of our proposed model, we evaluate its performance on the General Language Understanding Evaluation (GLUE) benchmark \cite{wang2018glue} in Table \ref{tab:glue}. GLUE is a widely used benchmark that consists of multiple natural language understanding (NLU) tasks, including CoLA, MRPC, RTE, SST-2, STS-B, and WNLI. We compare GMLM against well-established models such as \textbf{ELMo} \cite{peters2018deepcontextualizedwordrepresentations}, \textbf{BERT-base} \cite{devlin-etal-2019-bert}, and \textbf{DistilBERT} \cite{sanh2020distilbertdistilledversionbert}. 

GMLM, using DistilBERT as its masking model, demonstrates a performance improvement of \textbf{1.1} points on average compared to the baseline DistilBERT. Notably, GMLM achieves the best results on MRPC and WNLI, surpassing even the performance of BERT-base.

\section{Ablation Studies}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{images/citeseer_tsne_comparison_perp30.pdf}
    \caption{t-SNE embeddings (perplexity = 30) for the untrained (left) and fine-tuned (right) Graph Masked Language Model. Points are colored by research area: \texttt{Agents}, \texttt{AI}, \texttt{DB}, \texttt{IR}, \texttt{ML}, and \texttt{HCI}.}
    \label{fig:tsne_comparison}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{images/citeseer_umap_comparison_n15_d0.1.pdf}
    \caption{UMAP embeddings (\texttt{n\_neighbors = 15}, \texttt{min\_dist = 0.1}) for the untrained (left) and fine-tuned (right) Graph Masked Language Model. Each color corresponds to a different research area.}
    \label{fig:umap_comparison}
\end{figure*}

We present two-dimensional embeddings of the node representations learned by our Graph Masked Language Model (GMLM). We use both t-SNE and UMAP to project the high-dimensional embeddings onto two dimensions. CITESEER contain nodes (papers) from different research areas, labeled as \texttt{Agents}, \texttt{AI}, \texttt{DB}, \texttt{IR}, \texttt{ML}, and \texttt{HCI}..

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{images/3comptsne.pdf}
    \caption{t-SNE visualization of embeddings for different ablated versions. 
    (\textbf{Left}) w/o Soft Masking, 
    (\textbf{Center}) w/o Semantic Masking, 
    (\textbf{Right}) GMLM(Full).}
    \label{fig:tsne_ablation}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{images/3compumap.pdf}
    \caption{UMAP visualization of embeddings for different ablated versions. 
    (\textbf{Left}) w/o Soft Masking, 
    (\textbf{Center}) w/o Semantic Masking, 
    (\textbf{Right}) GMLM(Full).}
    \label{fig:umap_ablation}
\end{figure*}

\subsubsection{t-SNE Embeddings (Perplexity = 30)}

Figure~\ref{fig:tsne_comparison} shows t-SNE embeddings for both the untrained (left) and fine-tuned (right) GMLM. We set the perplexity parameter to 30. While the untrained model does not display clear, well-separated clusters, we can still observe some degree of grouping among nodes of similar research areas. This suggests that even before fine-tuning, the initial random weights or other initialization factors lead to some weak organization of the data in the embedding space.

After fine-tuning, however, we observe substantially clearer clustering behavior: nodes of the same research area form tight, well-separated clusters. This indicates that the fine-tuned GMLM effectively captures relationships among nodes, aligning them in a lower-dimensional space according to their semantic or structural similarity.

Figure~\ref{fig:tsne_ablation} presents the t-SNE embeddings for the three ablated configurations on the \textit{Citeseer} dataset. Each point is colored according to its class label (e.g., \texttt{Agents}, \texttt{DB}, \texttt{IR}, \texttt{ML}, \texttt{HCI}), providing a visual assessment of how well the learned embeddings separate different categories. Notably, \textbf{GMLM(Full)} (rightmost plot) produces the most compact and well-separated clusters, indicating superior disentanglement of underlying classes.

\subsubsection{UMAP Embeddings (n\_neighbors = 15, min\_dist = 0.1)}

Figure~\ref{fig:umap_comparison} presents UMAP embeddings for the same set of node representations. We use \texttt{n\_neighbors = 15} and \texttt{min\_dist = 0.1}. Similar to the t-SNE results, the untrained model (left) displays some partial grouping of nodes, indicating that there is still a small degree of structural or semantic signal in the initialization. However, the clusters are not as distinct or well-defined compared to the fine-tuned model (right).

In the fine-tuned model, we observe distinct clusters for each research area, suggesting that fine-tuning helps to significantly refine the node representations. The improved separation and tighter grouping underscore the effectiveness of the fine-tuning procedure in capturing and representing meaningful relationships among nodes.

Similarly, Figure~\ref{fig:umap_ablation} shows UMAP embeddings with $n\_neighbors=15$ and $\min\_dist=0.1$. Again, \textbf{GMLM(Full)} outperforms the ablated versions by producing visually distinct clusters with minimal overlap. Both the t-SNE and UMAP results consistently highlight the importance of including both soft and semantic masking  mechanisms. We also provide a parameter sweep study done on CITESEER for the masking strategies in Appendix \ref{param}.

\section{Conclusion}

In this paper, we introduce Graph Masked Language Models (GMLM), a novel approach to enhancing node classification in graph neural networks. Our method introduces a two-step masking process designed to improve the representation learning of graph attention networks (GATs) by incorporating the masking capabilities of pre-trained language models such as BERT. This strategy enables more effective feature learning by selectively masking and reconstructing node attributes, leading to improved generalization and robustness.  

Through extensive experiments on six benchmark datasets like CORA, Citeseer, Pubmed, Amazon Photo, Amazon Computers, and Coauthor CS, we demonstrate that GMLM achieves state-of-the-art (SOTA) performance in node classification. We further evaluate on language understanding benchmarks such as GLUE and find that GMLM outperform the models it is compared against on WNLI and MRPC and achieves the second highest on average.Additionally, our results highlight the importance of both Semantic Masking and Soft Masking, which contribute to more stable and reliable performance across different datasets.

\section{Limitations}
While we achieve state-of-the-art performance using DistilBERT, we have not yet explored the use of larger masked language models. Our decision to use DistilBERT was driven by the need for efficiency, as larger models come with increased computational costs and longer training times. For this study, we prioritized a more lightweight yet effective approach to demonstrate that our framework can achieve competitive performance compared to other self-supervised, semi-supervised, and unsupervised training methods for node classification tasks. Future work could explore the impact of larger models on performance and assess the trade-off between efficiency and accuracy.

\bibliography{tmlr}
\bibliographystyle{tmlr}

\appendix

\section{Parameter Sweep Results for Citeseer} \label{param}

In this section, we present the parameter sweep results for the GMLM (Graph Masked Language Model) on the Citeseer dataset. We visualize these results using parallel coordinate plots, exploring different masking strategies.  Our analysis compares three distinct configurations to understand the impact of various masking approaches on model performance.

We examine:
\begin{enumerate}
    \item \textbf{Full GMLM with semantic and soft masking} (Figure~\ref{fig:full_gmlm}).
    \item \textbf{GMLM with only semantic masking} (Figure~\ref{fig:semantic_only}).
    \item \textbf{GMLM with only soft masking (no semantic masking)} (Figure~\ref{fig:soft_only}).
\end{enumerate}

Each figure leverages a parallel coordinates plot to illustrate the relationship between hyperparameter variations and model accuracy.  The color bar on the right of each plot visually encodes accuracy, ranging from 71\% to 76\%. Within the plots, lines represent individual parameter combinations. For visualization purposes, each parameter value is normalized along its respective vertical axis.

\subsection{Full GMLM with Semantic and Soft Masking}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.75\textwidth]{images/Citeseer_parameter_sweep.png}
    \caption{Parameter sweep results for the full GMLM with both semantic and soft masking.
    The parallel coordinates plot displays four axes: \textit{beta}, \textit{mask\_min},
    \textit{mask\_max}, and \textit{accuracy}. Lines are colored according to model accuracy.}
    \label{fig:full_gmlm}
\end{figure}

Figure~\ref{fig:full_gmlm} illustrates the parameter sweep results when both semantic and soft masking are active in the GMLM framework.  The parameters explored in this configuration are \textit{beta}, \textit{mask\_min}, and \textit{mask\_max}, alongside the resulting model accuracy.  Specifically, \textit{beta} (often denoted as $\beta$) is the weighting factor that governs the contribution of semantic masking to the overall masking strategy. The parameters \textit{mask\_min} and \textit{mask\_max} define the minimum and maximum thresholds for applying masking, respectively.  Finally, \textit{accuracy} represents the model's performance on the Citeseer dataset.

In the parallel coordinates plot, each line corresponds to a unique combination of the hyperparameters $(\beta, \text{mask\_min}, \text{mask\_max})$.  The color of each line is determined by the final model accuracy achieved with that parameter combination, allowing for a visual assessment of performance across the parameter space.

Observing Figure~\ref{fig:full_gmlm}, we can discern several trends.  When the value of $\beta$ is excessively low, the influence of semantic masking may be insufficient.  This can hinder the model's ability to focus on semantically relevant context, potentially leading to reduced accuracy. Conversely, an excessively high $\beta$ might cause the model to overemphasize semantic relationships.  This over-reliance could lead to the model neglecting other informative patterns that soft masking is designed to capture, also resulting in suboptimal performance.

Furthermore, \textit{mask\_min} and \textit{mask\_max} play a crucial role in determining the aggressiveness of the masking process.  These parameters directly influence how many tokens (or nodes/edges in a graph context) are masked during training. The level of masking difficulty, therefore, is modulated by these thresholds.  The results suggest that a moderate range for both \textit{mask\_min} and \textit{mask\_max}, when combined with a balanced $\beta$ value, tends to yield the most favorable outcomes.  This is evidenced by the higher accuracy values, often in the 75--76\% range, observed in these regions of the parameter space.

\textbf{Importance of Combined Semantic and Soft Masking.}
The effectiveness of the full GMLM configuration stems from the synergistic interaction between semantic and soft masking.  Semantic masking is designed to prioritize the masking of tokens that are deemed more "meaningful" based on semantic relationships within the graph.  Soft masking, in contrast, provides a complementary approach by capturing more generalizable and structural patterns present in the data.

By integrating both strategies, we achieve a more comprehensive masking approach.  This ensures that the model is exposed to a diverse set of masked positions during training.  These positions include both semantically salient elements and those capturing broader structural variations.  A well-tuned $\beta$ parameter is essential to balance these two masking aspects, preventing the model from becoming overly biased towards a single masking strategy.  In conclusion, achieving an optimal balance between $\beta$ and the masking ratio (defined by \textit{mask\_min} and \textit{mask\_max}) is key. This balance facilitates the learning of rich and robust representations, ultimately manifesting in improved model accuracy.

\subsection{GMLM with Only Semantic Masking}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.75\textwidth]{images/Citeseer_parameter_sweep_sem.png}
    \caption{Parameter sweep results for GMLM with \textbf{only} semantic masking. The axes
    are \textit{mask\_min}, \textit{mask\_max}, and \textit{accuracy}.}
    \label{fig:semantic_only}
\end{figure}

Figure~\ref{fig:semantic_only} presents the parameter sweep results for a GMLM configuration employing only semantic masking.  In this scenario, soft masking is disabled, and consequently, the \textit{beta} parameter is not relevant and therefore absent from the figure.  The parameters under investigation are solely \textit{mask\_min} and \textit{mask\_max}, along with the resultant \textit{accuracy}.

The plot reveals how variations in \textit{mask\_min} and \textit{mask\_max} influence the intensity of semantic masking.  This, in turn, affects the difficulty of the masked language model prediction task.  Notably, the highest accuracy values are still concentrated within a specific region of the \textit{mask\_min}--\textit{mask\_max} space.  This observation suggests that both excessively aggressive and overly lenient masking ratios can negatively impact model performance, even when relying solely on semantic masking.

\textbf{Effect of Semantic-Only Masking.}
Utilizing semantic information in isolation can be advantageous when the underlying dataset structure strongly aligns with semantic relationships.  In such cases, focusing on semantic masking may effectively guide the model.  However, this approach also carries the risk of overfitting to specific semantic patterns present in the training data.  Without the complementary influence of soft masking, the model's ability to generalize to broader structural variations within the graph may be compromised.

\subsection{GMLM with Only Soft Masking}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.75\textwidth]{images/Citeseer_parameter_sweep_soft.png}
    \caption{Parameter sweep results for GMLM with \textbf{only} soft masking
    (no semantic masking). The axes are \textit{mask\_min}, \textit{mask\_max},
    and \textit{accuracy}.}
    \label{fig:soft_only}
\end{figure}

Figure~\ref{fig:soft_only} displays the parameter sweep outcomes when semantic masking is deactivated, and only soft masking is employed.  Similar to the semantic-only configuration, the parameters under consideration are \textit{mask\_min}, \textit{mask\_max}, and the resulting \textit{accuracy}.

Analyzing this plot, we observe a similar trend to the semantic-only setup.  The careful tuning of \textit{mask\_min} and \textit{mask\_max} remains crucial for achieving competitive accuracy levels.  The model's performance is directly linked to the extent of soft masking applied.  While soft masking can effectively highlight more generalizable features within the data, it lacks the direct semantic guidance provided by semantic masking.

\textbf{Effect of Soft-Only Masking.}
Soft masking, when used in isolation, encourages the model to learn from a wider range of masked positions.  This approach does not specifically prioritize semantically significant nodes or tokens.  While this broader exposure can contribute to robust generalization capabilities, the absence of semantic information may limit the model's capacity to concentrate on key relationships. These key relationships are often critical for effectively tackling many graph-based tasks.

\subsection{Discussion and Conclusions}

The figures presented offer insights into how the interplay between \textit{beta} (semantic weight), \textit{mask\_min}, and \textit{mask\_max} impacts the final accuracy on the Citeseer dataset.  Notably, when both semantic and soft masking are combined, achieving an appropriate balance between semantic weighting and masking thresholds is crucial.  This balanced approach can lead to accuracy in the 75--76\% range, outperforming configurations that rely on only a single masking strategy.

The primary conclusion drawn from these parameter sweeps is the importance of combining semantic and soft masking.  Furthermore, the careful tuning of the \textbf{beta} parameter and the masking ratio (\textit{mask\_min}, \textit{mask\_max}) are equally vital.  This combined approach effectively leverages the strengths of both masking strategies.  \emph{Semantic masking} effectively highlights nodes, edges, or tokens that carry important contextual information, guiding the model towards semantically relevant features.  Conversely, \emph{soft masking} ensures that the model is exposed to sufficient data variation to learn generalizable patterns inherent in the data.  This balanced approach, utilizing the synergy of semantic and soft masking, ultimately results in improved downstream task performance.

\end{document}
