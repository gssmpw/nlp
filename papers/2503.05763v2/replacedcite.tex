\section{Related Works}
Graph Neural Networks (GNNs) ____ have emerged as a powerful paradigm for learning representations from graph-structured data. Early seminal works (e.g., Kipf and Welling, 2017) formalized spectral and spatial convolutional approaches that have since been extended to diverse applications ranging from molecular property prediction to social network analysis. In parallel, the development of Graph Attention Networks (GATs) ____ introduced self-attention ____ into the GNN framework, thereby allowing each node to weigh its neighbors differentially during message passing. This adaptive weighting not only enhances the model’s expressivity but also addresses the challenges of learning on irregular graph structures. These advances are surveyed comprehensively in recent literature that categorizes and contrasts various GNN and graph attention models.
A notable line of work focuses on adapting the Transformer architecture—which was originally developed for sequence modeling—to graphs. Graph Transformers aim to overcome limitations of traditional message passing by leveraging global self-attention mechanisms that can capture long-range dependencies across nodes. Several recent surveys have systematically reviewed these models, highlighting their ability to integrate positional information and to operate on both small‐scale and large-scale graph data ____ .In this vein, transformer‐based architectures have been shown to achieve competitive performance on graph tasks while also inheriting benefits from pretrained language models when applied to text‐rich graphs.

\textbf{Masked Language Models} such as BERT ____ has redefined the state-of-the-art in natural language understanding by pretraining on large text corpora with a masked token prediction objective. MLMs are inherently designed to capture deep contextual representations, and recent investigations have explored their capacity to encode rich semantic features when applied in isolation or as a backbone for downstream tasks. The Masked Graph Autoencoder (MaskGAE) ____ framework employs this strategy by randomly masking a significant portion of edges and training the model to reconstruct them, thereby enhancing its ability to learn meaningful representations for downstream tasks like link prediction and node classification

\textbf{Integration of Language Models and Graph Structures} have increasingly focused on bridging the gap between textual and structural modalities. One direction leverages the strengths of pretrained language models (LMs) and integrates them with graph neural architectures to jointly reason over both text and graph structure. For instance, Graph Language Models (GLMs) have been introduced to fuse structural biases from graphs with the linguistic capabilities of LMs, enabling joint processing of interleaved textual and graph inputs ____. Complementary approaches, such as GraSAME, inject token-level structural information directly into pretrained LMs via a graph-guided self-attention mechanism, thereby eliminating the need for separate GNN modules while preserving semantic richness ____. Moreover, recent proposals argue that the Transformer itself can be interpreted as a special case of a graph neural network, wherein fully connected attention corresponds to complete graph structures over tokens. Extending this perspective, architectures such as “Transformers as Graph-to-Graph Models” make the graph nature of the attention mechanism explicit, allowing for iterative graph refinement and explicit modeling of edge information in latent representations ____. Language Model GNN (LM-GNN) framework jointly trains large-scale language models and GNNs, aiming to combine the contextual understanding of language models with the structural learning capabilities of GNNs ____.