[
  {
    "index": 0,
    "papers": [
      {
        "key": "zhou2021graphneuralnetworksreview",
        "author": "Jie Zhou and Ganqu Cui and Shengding Hu and Zhengyan Zhang and Cheng Yang and Zhiyuan Liu and Lifeng Wang and Changcheng Li and Maosong Sun",
        "title": "Graph Neural Networks: A Review of Methods and Applications"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "veli\u010dkovi\u01072018graphattentionnetworks",
        "author": "Petar Veli\u010dkovi\u0107 and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Li\u00f2 and Yoshua Bengio",
        "title": "Graph Attention Networks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "10.5555/3295222.3295349",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \\L{}ukasz and Polosukhin, Illia",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "shehzad2024graphtransformerssurvey",
        "author": "Ahsan Shehzad and Feng Xia and Shagufta Abid and Ciyuan Peng and Shuo Yu and Dongyu Zhang and Karin Verspoor",
        "title": "Graph Transformers: A Survey"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "devlin-etal-2019-bert",
        "author": "Devlin, Jacob  and\nChang, Ming-Wei  and\nLee, Kenton  and\nToutanova, Kristina",
        "title": "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "mgae",
        "author": "Li, Jintang and Wu, Ruofan and Sun, Wangbin and Chen, Liang and Tian, Sheng and Zhu, Liang and Meng, Changhua and Zheng, Zibin and Wang, Weiqiang",
        "title": "What's Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "plenz-frank-2024-graph",
        "author": "Plenz, Moritz  and\nFrank, Anette",
        "title": "Graph Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "yuan-farber-2024-grasame",
        "author": "Yuan, Shuzhou  and\nF{\\\"a}rber, Michael",
        "title": "{G}ra{SAME}: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "henderson-etal-2023-transformers",
        "author": "Henderson, James  and\nMohammadshahi, Alireza  and\nComan, Andrei  and\nMiculicich, Lesly",
        "title": "Transformers as Graph-to-Graph Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ioannidis2022efficienteffectivetraininglanguage",
        "author": "Vassilis N. Ioannidis and Xiang Song and Da Zheng and Houyu Zhang and Jun Ma and Yi Xu and Belinda Zeng and Trishul Chilimbi and George Karypis",
        "title": "Efficient and effective training of language and graph neural network models"
      }
    ]
  }
]