



\begin{table*}[htbp]
\small
\renewcommand\arraystretch{1.2}
  \centering
  \caption{Innovations in Multimodal Large Model Continual Learning Frameworks.}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm}|p{0.4\textwidth}|p{0.4\textwidth}}
    \hline
   \multicolumn{1}{c|}{Framework} & \multicolumn{1}{c|}{Starting point of the problem} & \multicolumn{1}{c}{How to solve} \\
         
   \hline
   \multirow{2}{*}{\makecell{\textbf{PathWeave}\\~\cite{yu2024llms}}} & 
To reduce the dependency on large-scale joint pre-training.& PathWeave enhances modality alignment and collaboration.\\
   
   \hline
   \multirow{2}{*}{\textbf{CLAP}~\cite{jha2024clap4clip}} & 
To enhance the model's uncertainty estimation capabilities.& CLAP is compatible with various prompt methods.\\
   
   \hline
   \multirow{2}{*}{\textbf{DIKI}~\cite{tang2025mind}} & 
To reduce catastrophic forgetting.& DIKI proposes a residual mechanism and distribution-aware calibration.\\
         
   \hline
   \multirow{2}{*}{\textbf{GMM}~\cite{cao2024generative}} & 
To reduce catastrophic forgetting.& GMM implements incremental learning through generated label text and feature matching.\\
         
   \hline
   \multirow{2}{*}{\makecell{\textbf{PriViLege}\\~\cite{park2024pre}}} & 
To address catastrophic forgetting and overfitting in MLLMs.& PriViLege proposes prompt functionality and knowledge distillation.\\
         
   \hline
   \multirow{2}{*}{\makecell{\textbf{ModalPrompt}\\~\cite{zeng2024modalprompt}}} & 
To address catastrophic forgetting and overfitting in MLLMs.& ModalPrompt proposes bi-modal guided prototype prompts and knowledge transfer.\\
         
   \hline
   \multirow{2}{*}{\textbf{CGIL}~\cite{frascaroli2024clip}} & 
To reduce catastrophic forgetting.& CGIL uses VAEs to learn class-conditioned distributions and generate synthetic samples.\\
         
   \hline
   \multirow{2}{*}{\makecell{\textbf{CoLeCLIP}\\~\cite{li2024coleclip}}} & 
To  reduce interference between tasks.& CoLeCLIP proposes joint learning of task prompts and cross-domain vocabularies.\\
         
   \hline
   \multirow{2}{*}{\textbf{ICL}~\cite{qi2024interactive}} & 
To enhance the efficiency of continual learning in MLLMs.& ICL enables interaction between a fast intuition model and a slow deep thinking model.\\
         
   \hline
   \multirow{2}{*}{\textbf{EMT}~\cite{zhai2023investigating}} & 
To evaluate catastrophic forgetting in MLLMs.& EMT offers a new perspective for improving fine-tuning strategies in MLLMs.\\
         
   \hline
   \multirow{2}{*}{\makecell{\textbf{Freeze-Omni}\\~\cite{wang2024freeze}}} & 
To reduce catastrophic forgetting.& Freeze-Omni implements a three-stage training strategy.\\
         
   \hline
   \multirow{2}{*}{\textbf{Adapt-$\infty$}~\cite{maharana2024adapt}} & 
To reduce catastrophic forgetting.& Adapt-$\infty$ proposes dynamic data selection and a clustering-based permanent pruning strategy.\\
         
   \hline
   \multirow{3}{*}{\makecell{\textbf{Mono-}\\ \textbf{InternVL}~\cite{luo2024mono}}} & 
To address the performance degradation and catastrophic forgetting issues that arise when expanding the visual and language capabilities of MLLMs.& Mono-InternVL integrates visual experts using a MOE structure and introduces endogenous visual pretraining.\\
         
   \hline
   \multirow{2}{*}{\makecell{\textbf{MoExtend}\\~\cite{zhong2024moextend}}} & 
To  address the issues of catastrophic forgetting and high training costs.& MoExtend designes a three-stage training process, including alignment, extension, and fine-tuning.\\


    \hline
    \end{tabularx}
  \label{CL_MLLM_Framework}%
\end{table*}%



\section{Continual Learning in Multimodal Large Language Model}

\subsection{Preliminary}


In recent years, multimodal large models (MLLMs) have made significant progress and demonstrated powerful capabilities across various domains. However, as the scale of these models continues to expand, maintaining the long-term effectiveness of the model in ever-changing environments has become a critical challenge~\cite{young2014image,achiam2023gpt,anil2023palm,bai2023qwen,chen2015microsoft,chen2024internvl,panagopoulou2023x,dong2024internlm,fu2024video,goyal2017making,gurari2018vizwiz,liu2024llava}. Continual Learning (CL), as a key technology to address this challenge, aims to enable models to gradually learn new tasks without forgetting previously acquired knowledge in dynamic and evolving task and data environments. For multimodal large models, the task of continual learning is even more complex, as these models often need to handle vast amounts of data and perform complex computations. Continual learning requires models to continually update and expand their knowledge, which often demands substantial computational resources and storage space. Although existing research on multimodal large models and continual learning provides important theoretical foundations and experimental support~\cite{liu2025mmbench,luo2024cheap,yang2023dawn,team2023gemini,team2023internlm,touvron2023llama,wang2023cogvlm,wu2024parameter,yue2024mmmu}, the application of multimodal large models in continual learning still faces many challenges. In this section, we will delve into the existing innovations in multimodal large model continual learning and the evaluation benchmarks.


\subsection{Model Innovation}

As show in table~\ref{CL_MLLM_Framework} and table~\ref{CL_MLLM_Method}, to achieve multi-task continual learning in multimodal large models and avoid catastrophic forgetting, researchers have proposed numerous innovative frameworks and methods~\cite{li2024coleclip,qi2024interactive,maharana2024adapt,luo2024mono,lester2021power,yan2022generative,villa2023pivot,he2024towards}. These innovations not only facilitate knowledge sharing and transfer between multiple tasks but also effectively address challenges such as catastrophic forgetting, modality conflicts, and computational resource constraints. With the continuous innovation of frameworks and methods for multimodal large model continual learning, a solid foundation is being laid for the multitask continual learning of multimodal large models. These efforts collectively advance the continual learning capabilities of multimodal large models in dynamic environments.

More details of the model innovation in the continual learning of MLLMs are provided in Section \ref{Appendix_MLLMCL} of the Appendix.





\begin{table*}[htbp]
\small
\renewcommand\arraystretch{1.2}
  \centering
  \caption{Innovations in Multimodal Large Model Continual Learning Methods.}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm}|p{0.4\textwidth}|p{0.4\textwidth}}
    \hline
   \multicolumn{1}{c|}{Method} & \multicolumn{1}{c|}{Starting point of the problem} & \multicolumn{1}{c}{How to solve} \\
         
   \hline
   \multirow{2}{*}{\textbf{NoRGa}~\cite{yu2024llms}} & 
To enhance the continual learning performance of multimodal large language models. & NoRGa proposes the non-linear residual gate.\\
         
   \hline
   \multirow{2}{*}{\textbf{ZAF}~\cite{gaostabilizing}} & 
To reduce catastrophic forgetting.& ZAF preserves knowledge through zero-shot stability regularization. \\
         
   \hline
   \multirow{3}{*}{\makecell{\textbf{DualLoRA}\\~\cite{chen2024dual}}} & 
Improving the efficiency and effectiveness of continual learning in multimodal large language models.& DualLoRA utilizes orthogonal and residual low-rank adapters along with a dynamic memory mechanism to balance model stability and plasticity. \\
         
   \hline
   \multirow{3}{*}{\textbf{LPI}~\cite{yan2024low}} & 
To address the insufficient interaction between modalities and tasks.& LPI enhances inter-modal and inter-task interactions through low-rank decomposition and contrastive learning. \\
         
   \hline
   \multirow{2}{*}{\makecell{\textbf{Model Tailor}\\~\cite{zhu2024model}}} & 
To reduce catastrophic forgetting.& Retaining most of the pre-trained parameters and  replacing a small number of fine-tuned parameters. \\
         
   \hline
   \multirow{2}{*}{\textbf{HVCLIP}~\cite{vesdapunt2025hvclip}} & 
Enhancing the model's ability to retain critical information while adapting to new tasks or domains.& HVCLIP uses strategies such as forgetting reduction, discrepancy reduction, and feature enhancement. \\
         
   \hline
   \multirow{2}{*}{\makecell{\textbf{Continual}\\ \textbf{LLaVA}~\cite{cao2024continual}}} & 
Enhancing the ability to preserve knowledge from previous tasks while accommodating new ones..& Continual LLaVA proposes a parameter-efficient tuning method that does not require rehearsal. \\
         
   \hline
   \multirow{2}{*}{\textbf{LLaCA}~\cite{qiao2024llaca}} & 
To reduce forgetting and lower computational costs.& LLaCA dynamically adjusts the EMA weights and introduces an approximation mechanism. \\
         
   \hline
   \multirow{2}{*}{\textbf{CVM}~\cite{rebillard2024continually}} & 
To reduce forgetting and improve generalization.&CVM maps the representations of small visual models to the knowledge space of a fixed LLM. \\
         
   \hline
   \multirow{2}{*}{\textbf{RE-tune}~\cite{mistretta2024re}} & 
Addressing challenges related to computational resources, data privacy, and catastrophic forgetting.& RE-tune freezes the backbone of the model and trains adapters, using text prompts to guide training. \\
         
   \hline
   \multirow{2}{*}{\textbf{CluMo}~\cite{cai2024clumo}} & 
Enhancing the performance of MLLMs in CL and improving their ability to retain old knowledge.& CluMo employs a two-stage training and modality fusion prompt strategy. \\
         
   \hline
   \multirow{2}{*}{\makecell{\textbf{Fwd-Prompt}\\~\cite{zheng2024beyond}}} & 
To achieve anti-forgetting and positive transfer.& Fwd-Prompt utilizes gradient projection techniques and proposes a multimodal prompt pool. \\
         
   \hline
   \multirow{2}{*}{\makecell{\textbf{CPE-CLIP}\\~\cite{d2023multimodal}}} & 
Enhancing the performance of few-shot class incremental learning in MLLMs.& CPE-CLIP using learnable prompts and regularization strategies. \\
         
   \hline
   \multirow{2}{*}{\textbf{TG}~\cite{zhang2024preserving}} & 
To reduce catastrophic forgetting.& TG proposes the model-agnostic self-uncompression method. \\
         
   \hline
   \multirow{2}{*}{\textbf{LiNeS}~\cite{wang2024lines}} & 
Preserving the generalization ability of pretraining while improving fine-tuning task performance. & LiNeS proposes parameter updates with differentiated layer depth. \\
         
   \hline
   \multirow{2}{*}{\textbf{AttriCLIP}~\cite{wang2023attriclip}} & 
Enhancing the generalization and continual learning capabilities of MLLMs in multimodal tasks. & AttriCLIP adapts to new tasks using an attribute lexicon and textual prompts. \\
         
   \hline
   \multirow{2}{*}{\textbf{AttriCLIP}~\cite{wang2023attriclip}} & 
Enhancing the generalization and continual learning capabilities of MLLMs in multimodal tasks. & AttriCLIP adapts to new tasks using an attribute lexicon and textual prompts. \\
         
   \hline
   \multirow{2}{*}{\textbf{C-LoRA}~\cite{smith2023continual}} & 
To reduce catastrophic forgetting.& C-LoRA performs continual adaptive low-rank adjustments in the cross-attention layers of MLLMs. \\


    \hline
    \end{tabularx}
  \label{CL_MLLM_Method}%
\end{table*}%






\subsection{Benchmarks}

As the application of multimodal large models in continual learning increases, evaluating their continual learning capability has become a key issue. To comprehensively assess the continual learning performance of multimodal large models, benchmarks and evaluation frameworks have emerged. However, benchmarks specifically designed for continual learning in multimodal large models are still relatively scarce, and the relevant evaluation standards are still in the process of development. This section will analyze and list the few existing benchmarks for evaluating the continual learning capability of multimodal large models, exploring their design concepts, evaluation metrics, and applicability in different application scenarios.

\subsubsection{CoIN: Continual Instruction Tuning Benchmark}

MLLMs adapt to new tasks and users' evolving needs through instruction tuning. However, these models face challenges in adapting to the constantly changing knowledge requirements of users. To address this, Cheng Chen et al.~\cite{chen2024coin} proposed the CoIN benchmark to evaluate MLLMs' performance under the sequential instruction tuning paradigm. They also introduced the MoELoRA method to help MLLMs retain previous instruction alignment, reducing catastrophic forgetting. 

More details of the scale and diversity of the CoIN benchmark and an overview of MLLM performance on the CoIN are provided in Section \ref{appendix_CoIN} of the Appendix.




\subsubsection{CliMB: The Continual Learning in Multimodality Benchmark}


Existing multimodal large language models are typically fine-tuned separately for each downstream task, requiring a new model to be fine-tuned and stored for each task. In contrast, multitask learning involves training on a fixed set of tasks, but it cannot dynamically learn new tasks. To address this, Tejas Srinivasan et al.~\cite{srinivasan2022climb} proposed the CLiMB benchmark, designed to study the continual learning challenges faced by multimodal large models in multimodal tasks and to systematically evaluate how upstream continual learning can quickly generalize to new multimodal and unimodal tasks. The CLiMB benchmark includes vision-and-language input tasks, such as VQAv2, NLVR2, SNLI-VE, and VCR. Additionally, the evaluation phase of the CLiMB benchmark includes: 1) Upstream Continual Learning: The model is trained on a series of vision-language tasks, and its ability to forget old tasks and transfer knowledge to new tasks is evaluated after each task. 2) Downstream Low-Shot Transfer: After training on upstream tasks, the model's adaptability to new multimodal and unimodal tasks with limited samples is assessed.

More details of the scale and diversity of the CliMB benchmark and an overview of MLLM performance on the CliMB are provided in Section \ref{appendix_CliMB} of the Appendix.





\subsubsection{COAST: Continual Instruction Tuning Benchmark}
An ideal MLLM should be able to continuously adjust to new tasks in the face of task flow distributions across different domains, new capabilities, and new datasets, while minimizing forgetting of prior knowledge. However, most existing MLLMs are limited to single-task adaptation and lack performance evaluation standards for continual learning of new tasks. To comprehensively assess MLLMs' continual learning performance across different domains, capabilities, and datasets, Meng Cao et al.~\cite{cao2024continual} proposed the COAST benchmark. COAST includes three incremental learning settings: 1) Domain-incremental: Simulates scenarios where MLLMs continuously adapt to different domains.
Capability-incremental: Evaluates the ability of MLLMs to progressively acquire and integrate new capabilities. 2) Dataset-incremental: Assesses the ability of MLLMs to adapt to and generalize across varying dataset distributions. 3) By chaining and reusing existing benchmark tests, the COAST benchmark creates a streaming task distribution to evaluate the performance of MLLMs when continually learning new tasks.

More details of the overview of MLLM performance on the COAST are provided in Section \ref{appendix_COAST} of the Appendix.



\subsubsection{ViLCo-Bench: Video Language Continual learning Benchmark}

Multimodal large models in the domain of video-language continual learning involve the continuous adaptation to information from both video and text inputs, enhancing the model's ability to handle new tasks while retaining previous knowledge. This is a relatively under-explored field, and establishing appropriate benchmarks is crucial to promoting communication and research in this area. To address this, Tianqi Tang et al.~\cite{tang2024vilco} proposed the first benchmark specifically designed for video-language continual learning in multimodal large models, called ViLCo-Bench. This benchmark aims to evaluate continual learning models across a range of video-text tasks.

ViLCo-Bench includes three unique video-language tasks: 1) Moment Queries (MQ). 2) Natural Language Queries (NLQ). 3) Visual Queries (VQ). These tasks require the model to understand video content and retrieve relevant segments of the video based on language queries.


More details of the overview of MLLM performance on the ViLCo-Bench are provided in Section \ref{appendix_ViLCo-Bench} of the Appendix.





Existing benchmarks for multimodal large model continual learning provide some reference value for assessing a model's learning ability. However, due to the scarcity of such benchmarks, with only a few available for use, many issues and limitations remain to be addressed.In the future, there is a need to design more comprehensive, flexible, and scalable evaluation benchmarks to meet the evolving demands of multimodal large model continual learning technologies. 

In summary, significant progress has been made in the field of continual learning for multimodal large models. Although many methods have been proposed to address the issue of how models can learn new tasks without forgetting old ones in dynamically changing environments, several challenges remain unsolved. Current research primarily focuses on effectively preventing catastrophic forgetting in multimodal contexts, enhancing the persistence of knowledge through innovative architectural designs, and maintaining cross-modal knowledge consistency while learning new tasks. However, the effectiveness of these approaches is still constrained by the complexity of multimodal data and the dynamic nature of tasks, especially when dealing with large-scale and diverse modality data. Furthermore, to effectively assess the continual learning capabilities of multimodal large models, researchers have proposed specialized benchmarks for this field. These benchmarks simulate sequential learning of multiple tasks to evaluate models' transfer abilities between tasks, catastrophic forgetting phenomena, and the retention of cross-modal knowledge. These evaluation standards not only help measure the effectiveness of existing methods but also provide clear guidance for the design and optimization of future models. Overall, despite the challenges that continue to persist in the continual learning of multimodal large models, research in this area will play a critical role in advancing the long-term learning and generalization capabilities of artificial intelligence systems.



% todo
% 1. 图标风格统一（格式统一）
% 2. 表格重新绘画一下
% 3. pipline