
\section{Continue Learning}

\subsection{Preliminary}

Continual Learning (CL) has become an important research direction in the field of artificial intelligence and has gained widespread attention in recent years~\cite{li2017learning,loo2020generalized,pellegrini2021continual,sarfraz2023sparse,abbasi2022sparsity,aljundi2019gradient,ke2020continual,schwarz2018progress,chaudhry2018riemannian,huang2024etag,huang2024kfc,ke2020continual,vijayan2023trire,wang2022sparcl,yu2020semantic}. With the rapid development of deep learning and large language models, how to enable models to maintain and enhance their learning capabilities when faced with continuously changing data and tasks has become an urgent problem to solve. Traditional training methods often assume that models can learn all tasks at once and maintain a fixed knowledge representation after training. However, in reality, data and tasks are often dynamically changing, and this static learning approach typically leads to "Catastrophic Forgetting"~\cite{chaudhry2018efficient,chen2020mitigating,de2021continual,miao2021continual,pham2021dualnet,dhar2019learning,kao2021natural,boschini2022class,mai2022online,konishi2023parameter,li2023memory,li2023variational}. Therefore, continual learning, as a learning paradigm that better aligns with real-world application needs, aims to enable models to effectively accumulate and update knowledge across multiple stages, thereby adapting better to dynamic and evolving environments.


This section will provide a detailed classification and overview of the latest innovative research in continual learning. The specific content is divided into three parts: 1) First, we will explore unimodal  continual learning for non-large language models, focusing on traditional models' continual learning research in Unimodal data; 2) We will analyze multi-modal continual learning for non-large models, discussing the challenges and research in continual learning across multi-modal data; 3) Finally, we will analyze and summarize the latest advancements in continual learning for large language models (LLMs), examining the unique challenges and solutions they face when handling large-scale textual data.

\subsection{Non-Large Language Model Unimodal Continual Learning}
In traditional unimodal  learning, research on continual learning primarily focuses on how to prevent models from forgetting previously learned knowledge when learning new tasks. Many researchers have proposed solutions to this problem, including strategies based on knowledge retention, incremental learning methods, and improvements to neural network architectures~\cite{shin2017continual,tao2020topology,verma2021efficient,wang2021training,zhu2021class,zhu2021prototype,smith2021always,sun2023exemplar,sun2023decoupling,sun2021ilcoc,rypesc2024divide,sarfraz2023sparse,shi2023prototype}. For non-large models, the challenges of continual learning are particularly pronounced due to limitations in computational resources. Furthermore, the unimodal  continual learning for non-large models primarily focuses on individual modalities such as vision, speech, and text. To address the specific characteristics of these tasks, researchers have proposed a variety of innovative frameworks and methods.

\subsubsection{Framework Innovation}

Xiaoxue Han et al.~\cite{han2024topology} proposed the TACO framework, which combines graph coarsening and continual learning to dynamically store information from previous tasks. They designed an efficient graph coarsening algorithm, RePro, based on node similarity, and introduced a node fidelity preservation strategy. The effectiveness of this approach in preventing the disappearance of minority classes was theoretically validated.

Ari S. Benjamin et al.~\cite{benjamin2024continual} proposed the Neural Tangent Ensemble (NTE) framework, which views a neural network as an ensemble of fixed experts. They derived its posterior update rule, which is equivalent to a specific form of stochastic gradient descent (SGD), offering a novel perspective for understanding and mitigating catastrophic forgetting.

Daehee Lee et al.~\cite{lee2024incremental} proposed the IsCiL framework, which improves sample efficiency and task adaptability by incrementally learning shared skills. They introduced prototype-based skill retrieval and adapter learning to enable effective knowledge sharing across different tasks.

Kunlun Xu et al.~\cite{xu2024mitigate} proposed the CKP framework, which purifies data through the CDP and ILR modules, and filters out erroneous knowledge using the EKF algorithm. This approach addresses the performance degradation issue caused by incorrect labels in the Lifelong Person Re-Identification task.

Lei Liu et al.~\cite{liu2024prior} proposed the PBR framework, which operates without prior knowledge. It reduces forgetting and enhances long-tail continual learning performance through an uncertainty-guided sampling strategy and two prior-free constraints.

Yusong Hu et al.~\cite{hutask} proposed the Task-Aware Orthogonal Sparse Network (OSN), which explores shared knowledge between old and new tasks through parameter sharing. They introduced sharpness-aware orthogonal projections to optimize the update of shared parameters and reduce interference with old tasks.

Daeun Lee et al.~\cite{lee2024becotta} proposed the Mixture-of-Domain Low-rank Experts (MoDE) framework, which includes domain-adaptive routing and domain-expert collaborative loss. This framework enables input-dependent online expert fusion, improving adaptation to new domains while preserving old knowledge.

Meng Ding et al.~\cite{ding2024understanding} proposed a theoretical analysis framework for linear regression applicable to different parameterization scenarios. They revealed the impact of task sequences and algorithm parameters on forgetting and experimentally validated the theoretical findings.

Soochan Lee et al.~\cite{lee2024learning} proposed the SB-MCL framework, which achieves continual learning through sequential Bayesian updates. The neural network is fixed to prevent forgetting, and the framework is domain- and model-agnostic.

Mikel et al.~\cite{malagonself} proposed CompoNet, a modular neural network with linearly growing parameters. By combining strategies, it prevents forgetting while achieving efficient knowledge transfer and scalability.

Raymond L. Wang et al.~\cite{wangrapid} proposed a Vector-HaSH-based neural model that combines hetero-associative memory and spatially invariant CNNs to enable fast learning and continual memory. They introduced the vHSN method, which utilizes attention mechanisms and grid encoding to prevent catastrophic forgetting and enhance generalization across different environments.

Jinglin Liang et al.~\cite{liang2025diffusion} proposed the DDDR framework, which utilizes diffusion models to generate historical data. By employing contrastive learning, the framework enhances the model's generalization ability on both generated and real data, addressing the issue of catastrophic forgetting in federated continual learning.

Fernando Julio Cendra et al.~\cite{cendra2025promptccd} proposed the PromptCCD framework, which uses GMM as a prompting method to address the CCD problem. They introduced the GMP module, which dynamically generates prompts to adapt to new classes, thus solving the problem of automatically discovering new classes in continuous data streams while mitigating catastrophic forgetting.

Dong Li et al.~\cite{li2024efficient} proposed the Mecoin framework, which employs Structured Memory Units (SMU) and a Memory Construction Module (MeCo) for efficient storage and updating of class prototypes. They introduced the Memory Representation Adaptation Module (MRaM) and the Graph Knowledge Interchange Module (GKIM) to reduce parameter fine-tuning, lower the forgetting rate, and enhance the model's generalization ability.

Linglan Zhao et al.~\cite{zhao2024safe} proposed the SAFE framework, which, in the first session, inherits the knowledge of the pre-trained model through knowledge transfer loss. In subsequent sessions, the framework balances model stability and adaptability by fixing slow parameters and updating fast parameters. It introduces an entropy-based aggregation strategy to dynamically fuse the advantages of two types of learners. This approach enables the efficient use of the rich knowledge from pre-trained models in continual learning while maintaining the model's adaptability and stability when facing new data.

Wenju Sun et al.~\cite{sun2024incremental} proposed the RP2F framework, which directly combines the posterior parameters of new and old tasks. They introduced a parameter robustness prior and used perturbation methods to approximate the Hessian matrix, enabling effective knowledge sharing and backward knowledge transfer.

Xiaoqian Liu et al.~\cite{liu2024hierarchical} proposed the HAMMER framework, which identifies shared knowledge and guides multilingual learning through online knowledge analysis and a hierarchical language evaluation mechanism, effectively alleviating the forgetting problem.

Hao Yu et al.~\cite{yu2024overcoming} proposed the FedCBC framework, which overcomes forgetting through category-specific binary classifiers and selective knowledge fusion.

Xiaochen Li et al.~\cite{xiaochen2024ts} proposed the TS-ILM framework, which includes a task-level temporal pattern extractor and a time-sensitive example selector. This framework effectively captures cross-task temporal patterns, selects representative frames for replay, reduces information redundancy, and enhances memory retention.

Depeng Li et al.~\cite{li2024harnessing} proposed the AutoActivator model, which dynamically adapts neural units to new tasks, enabling on-demand network expansion. This approach addresses the issue of forgetting old classes when learning new classes incrementally in class-incremental learning.

Tom Fischer et al.~\cite{fischer2024inemo} proposed iNeMo, an incremental neural grid model, which achieves efficient class-incremental learning through latent space initialization and position regularization.



\subsubsection{Method Innovation}

Huiping Zhuang et al.~\cite{zhuang2024gacl} proposed the sample-free Generalized Analytical Continual Learning (GACL) technique, which avoids catastrophic forgetting through analytical learning. It establishes the equivalence between incremental learning and joint training, effectively addressing the challenges of handling mixed data categories.

Ang Bian et al.~\cite{bian2024make} proposed the C-Flat method, which enhances continual learning (CL) performance by optimizing the flatness of the loss landscape. The method is easy to integrate and outperforms traditional approaches comprehensively.

Yan Fan et al.~\cite{fan2024dynamic} proposed the Dynamic Subgraph Distillation (DSGD) method, which uses structural and semantic information for stable knowledge distillation. This approach enhances the model's robustness to distribution shifts and adapts to different supervision settings, addressing the practical deployment challenges in continual learning that arise from relying on a large number of labeled samples.

Li Jiao et al.~\cite{jiao2024vector} proposed the VQ-Prompt method, which utilizes vector quantization to achieve end-to-end optimization of discrete prompt selection. They introduced gradient estimation, regularization terms, and representation statistics to stabilize task knowledge learning and improve continual learning performance.

Ameya Prabhu et al.~\cite{prabhurandom} proposed the RanDumb method, which uses random transformations and linear classifiers to investigate whether the representations produced by continual learning algorithms are truly effective in online continual learning.

Yue Lu et al.~\cite{lu2024visual} proposed two consistency conditions and an invariant prompt distribution constraint to reduce interference from new tasks on old tasks, overcoming catastrophic forgetting.

Botos Csaba et al.~\cite{csabalabel} proposed the IWMS method, which addresses label delay by prioritizing the memory of samples similar to new data. This approach helps mitigate the label delay issue in online continual learning.

Qiwei Li et al.~\cite{li2024progressive} proposed the Progressive Prototype Evolution (PPE) method, which learns class prototypes during the online learning phase to alleviate forgetting. They introduced prototype similarity preservation and prototype-guided gradient constraint modules, effectively combating dual forgetting.

Chengyi Yang et al.~\cite{yang2024introducing} proposed the Gradient Projection Common Null Space (GPCNS), which enhances plasticity by utilizing gradient information from old tasks. They integrated feature and gradient information through a collaborative framework, improving the performance of continual learning.

Zeyang Zhang et al.~\cite{zhangdisentangled} introduced a factor-based task-module router to optimize task routing and reduce forgetting. They designed an invariance-based architecture search mechanism to capture shared knowledge between tasks, enhancing knowledge sharing. This approach addresses the static assumptions and catastrophic forgetting issues in Graph Neural Architecture Search (GNAS) when handling continuous graph tasks.

Jeevan Thapa et al.~\cite{thapabayesian} proposed a non-parametric Bayesian method that infers network depth using a Beta process and adapts the width through a conjugate Bernoulli process. This approach enables joint inference of both network structure and weights, enhancing continual learning performance.

Nicolas Michel et al.~\cite{michel2023rethinking} proposed a new method based on momentum knowledge distillation, which dynamically updates the teacher model using exponential moving averages. This approach effectively overcomes the challenges of data stream processing and catastrophic forgetting in online continual learning.

Yichen Wen et al.~\cite{wen2024provable} proposed the CILA algorithm, which improves model performance in continual tasks through an adaptive distillation coefficient and theoretical performance guarantees.

Yichen Wu et al.~\cite{wumitigating} proposed the POCL algorithm, which models task relationships through Pareto optimization and dynamically adjusts weights to reduce forgetting.

Hongming Piao et al.~\cite{piaofederated} proposed the Powder algorithm, which enables prompt-based dual knowledge transfer. By selectively transferring knowledge based on task relevance, it reduces communication costs, addressing the challenge of cross-task and cross-client knowledge transfer in federated continual learning.

Weichen Lin et al.~\cite{lin2024effective} proposed the Dynamic Gradient Calibration (DGC) method, which effectively utilizes historical data to calibrate gradients. By combining it with existing continual learning methods, DGC helps alleviate the issue of catastrophic forgetting caused by data stream updates in continual learning.

Doyoung Kim et al.~\cite{kim2023one} proposed an adaptive prompting method, AdaPromptCL, which effectively adapts to varying degrees of semantic change through dynamic semantic grouping and prompt adjustment. This approach addresses the challenge of task-specific semantic variations in continual learning that fixed prompting strategies face.

Jason Yoo et al.~\cite{kim2023one} proposed the Layerwise Proximal Replay (LPR) method, which adjusts the optimization geometry to balance the learning of new and old data, enabling progressive changes. This approach reduces catastrophic forgetting and underfitting, improving the model's adaptability to both new and old data.

Zhen Zhu et al.~\cite{zhu2025anytime} proposed a dynamic weight prediction method and attention-weighted PCA feature compression, enabling efficient updates and storage compression in continual learning. This approach enhances model accuracy and flexibility.

Yanshuo Liang et al.~\cite{liang2024inflora} proposed the InfLoRA method, which injects parameter reparameterization into pre-trained weights, effectively fine-tuning within a subspace. The method designs subspace elimination to prevent new tasks from interfering with old tasks, addressing the issue of forgetting old tasks when adapting to new tasks in continual learning.

Chaoxi Niu et al.~\cite{niu2024replay} proposed a Laplace smoothing-based graph task analysis and prompting method, which enables accurate prediction of task IDs and learning of task-specific knowledge without the need for data replay. This approach effectively prevents forgetting and improves classification accuracy.

Huiping Zhuang et al.~\cite{zhuangf} proposed a forward online analytical learning method, F-OAL, which does not rely on backpropagation. It updates the linear classifier using recursive least squares, helping to alleviate the issue of catastrophic forgetting in online class-incremental learning.

Wuxuan Shi et al.~\cite{shiprospective} proposed Prospective Representation Learning (PRL), which aligns reserved space and latent space to adapt new class features to the reserved space. This method balances new and old classes, improving performance in non-sample class-incremental learning.

Zitong Huang et al.~\cite{huang2024class} proposed the ACIL task and CBS strategy, which implement class balancing through clustering and greedy selection, enhancing performance in incremental learning.

Xuze Hao et al.~\cite{hao2024addressing} proposed the CIL-balanced classification loss and distribution margin loss to reduce classifier bias and enhance class separability. This approach addresses the issue of catastrophic forgetting in class-incremental learning for medical image classification.

Zhiwen Yang et al.~\cite{yang2024domain} proposed the DSSP method, which leverages domain sharing and task-specific prompt learning, along with the S²-Adapter to adapt to deep space variations. This approach eliminates the need for sample replay and effectively mitigates catastrophic forgetting.

Shiye Wang et al.~\cite{wang2024importance} proposed Shared Parameter Subspace Learning, which combines momentum updates and an importance-aware mechanism, along with cross-domain contrast and orthogonality constraints, to capture cross-domain shared information and reduce forgetting.

Bowen Zheng et al.~\cite{zhengmulti} proposed the MRFA method, which optimizes the entire layer margin by enhancing the features of review samples. By increasing the margin, this approach helps reduce catastrophic forgetting.

Kishaan Jeeveswaran et al.~\cite{jeeveswaran2024gradual} proposed the DARE method, which reduces representation drift through a three-stage training process. They introduced the IRS strategy to optimize buffer sampling, thereby improving the model's performance on old tasks.

Dawei Zhou et al.~\cite{zhou2024expandable} proposed the EASE method, which constructs task-specific subspaces using lightweight adapters and synthesizes new features for old classes by leveraging semantic information. This approach effectively alleviates catastrophic forgetting.


Overall, unimodal  continual learning with non-large models has made significant progress in scenarios with limited computational resources. Many innovative frameworks and methods have been developed to effectively mitigate catastrophic forgetting. However, how to scale these approaches to multimodal and large-scale data remains an important direction for future research.

\subsection{Non-large Language Model Multimodal Continual Learning}

Compared to unimodal continual learning, multimodal continual learning presents more complex challenges. Data from different modalities often exhibit heterogeneity, and the key difficulty in multimodal continual learning for non-large models lies in how to effectively fuse information across modalities while retaining previously acquired knowledge during the process of learning new modalities. In recent years, researchers have proposed various methods to address these challenges, including inter-modal collaborative learning, shared and independent representations for each modality, and others~\cite{kirkpatrick2017overcoming,rebuffi2017icarl,ahn2019uncertainty,zenke2017continual,yoon2017lifelong,lee2020neural,madaan2021representational,cossu2024continual,fini2022self,yoon2023continual,yan2022generative,pian2023audio,mo2023class}. These innovative frameworks and methods enable non-large models to perform continual learning in multimodal environments, while minimizing knowledge conflicts between different modalities.

\subsubsection{Framework Innovation}

Bo Yuan et al.~\cite{yuan2024continual} proposed the CPP model for multi-task joint learning, which incorporates the CCE, TKD, and TPL mechanisms to achieve end-to-end multimodal general vision perception, significantly enhancing the efficiency of continual learning.

Yu Feng et al.~\cite{feng2024cp} proposed the CP-Prompt framework, which utilizes a dual-prompt strategy and parameter-efficient adjustments to achieve domain-specific knowledge extraction and inter-domain knowledge sharing, significantly reducing the forgetting rate.

Xianghu Yue et al.~\cite{yue2024mmal} proposed the MMAL framework, which includes the modality fusion module and MSKC module. It effectively integrates audio-visual information without requiring samples, reducing forgetting and enhancing incremental learning performance.

Yuchu Yu et al.~\cite{yu2025select} proposed a selective dual-teacher knowledge transfer framework, which utilizes unlabeled data to identify teacher networks, thereby ensuring knowledge retention and maintaining zero-shot capability.

Xiang Chen et al.~\cite{chen2023continual} proposed the MSPT framework, which optimizes multimodal learning through gradient modulation and attention distillation. It balances knowledge retention and new data integration, effectively mitigating catastrophic forgetting.

Jiazuo Yu et al.~\cite{yu2024boosting} proposed a dynamic expansion framework based on MoE adapters and DDAS, enabling parameter-efficient and zero-shot continual learning.

Yiwen Ye et al.~\cite{ye2024continual} proposed MedCoSS, a staged multimodal self-supervised learning framework that avoids modality conflicts. It introduces rehearsal strategies and feature distillation, effectively preventing catastrophic forgetting and enhancing knowledge retention.



\subsubsection{Method Innovation}

Jieren Deng et al.~\cite{deng2024zero} proposed the ZiRa method, which effectively alleviates the challenge of adapting visual-language object detection models to new domains while retaining zero-shot generalization capabilities in incremental learning. This is achieved through zero-interference loss and a reparameterized dual-branch structure, without increasing memory burden.

Tao Jin et al.~\cite{jin2024calibrating} proposed a historical prompt calibration strategy, which includes intra-modal correlation estimation and inter-modal consistency alignment to calibrate prompts in pre-trained models. This enhances the task and modality relationships, addressing the issues of task unfamiliarity and modality heterogeneity in multimodal continual learning.

Jaewoo Lee et al.~\cite{leestella} proposed a localized patch importance scoring method, emphasizing the semantic interweaving of audio-visual patches. The replay-guided relevance assessment reduces forgetting of previously learned knowledge.

Longrong Yang et al.~\cite{yang2024rcs} proposed the RCS-Prompt method, which reduces category space overlap and establishes clear boundaries between sessions through bidirectional prompt optimization and prompt magnitude normalization. This addresses the issue of overlap between old and new category spaces in continual learning.

Zangwei Zheng et al.~\cite{zheng2023preventing} proposed the ZSCL method, which mitigates forgetting through feature space distillation and parameter space weight integration.

Kaiyang Zhou et al.~\cite{zhou2022conditional} proposed the CoCoOp method, which generates dynamic prompts using a lightweight neural network to enhance model generalization. This addresses the issue of insufficient zero-shot generalization to unseen categories when pre-trained vision-language models adapt to new tasks.

Martin Menabue et al.~\cite{menabue2025semantic} proposed a dual-level prompt mechanism and semantic residual prompts, combined with multimodal generative replay, to enhance the stability and adaptability of models in continual learning.

Yicheng Xu et al.~\cite{xu2024advancing} proposed the RAIL method, which uses recursive ridge regression and a no-training fusion module, along with the introduction of the X-TAIL setup, aiming to address the challenge of improving cross-domain classification capabilities in vision-language models during continual learning.

Linlan Huang et al.~\cite{huang2025class} proposed an adaptive representation adjustment and parameter fusion method, which adjusts the representations of old categories affected by new categories using text features. Additionally, they employ a decomposition-based parameter fusion strategy to reduce forgetting.

Through continuously innovative frameworks and methods, multimodal continual learning in non-large models has achieved a certain level of effective integration and learning across different modalities. However, with the diversification of data types and application scenarios, non-large model multimodal continual learning will face more complex tasks and dynamic environments, necessitating more flexible and efficient solutions.


\subsection{Continual Learning in Large Language Model}

Large language models (LLMs) such as GPT and BERT, with their powerful language understanding and generation capabilities, have achieved remarkable results across various natural language processing tasks~\cite{devlin2018bert,du2024chinese,eloundou2023gpts,gupta2023continual,li2022large,cao2023instruction,kukreja2024literature,kasneci2023chatgpt,zhao2023survey,naveed2023comprehensive,chang2024survey,chen2021evaluating,unlu2023interpretutor,wu2024survey,zhang2023instruction}. However, LLMs still face unique challenges in continual learning. Particularly in the context of increasing data volume and task diversity, how to effectively update models, avoid catastrophic forgetting, and maintain efficient computational capabilities are key focuses in the research of LLMs for continual learning. To address these issues, researchers have proposed a variety of innovative frameworks and methods. In this section, we will provide a detailed explanation from two aspects: model improvements and instruction fine-tuning.

\subsubsection{Model Innovation}

Yeongbin Seo et al.~\cite{seo2024train} proposed the TAALM method, which uses meta-learning to dynamically predict token importance, enabling targeted knowledge updates and reducing forgetting.

Haoran Que et al.~\cite{que2024d} proposed the D-CPT Law and Cross-Domain D-CPT Law, which predict the optimal training ratio to address the issue of selecting the mixed corpus ratio during continual pre-training of large language models. These methods reduce GPU resource consumption and improve domain adaptability.

Srikanth Malla et al.~\cite{malla2024copal} proposed the COPAL algorithm, which enables continual pruning without the need for retraining, thereby avoiding model retraining. This solution addresses the high computational demands and model adaptability limitations faced by large language models when adapting to new domains.

Daniel Marczak et al.~\cite{marczak2025magmax} proposed the MagMax method, which achieves effective cross-task knowledge integration through sequential fine-tuning and maximum magnitude weight selection. This approach mitigates the problem of catastrophic forgetting of old knowledge in large pre-trained models during continual learning, enabling adaptation to the continuously evolving data stream.

Weixiang Zhao et al.~\cite{zhao2024sapt} proposed the SAPT framework, which aligns the learning and selection of PET blocks through a shared attention mechanism. They introduced the ARM module to recall old tasks using pseudo-samples, enabling effective knowledge retention and transfer.

Jianheng Huang et al.~\cite{huang2024mitigating} proposed the SSR framework, which utilizes LLM-generated synthetic instances for rehearsal. This approach effectively mitigates forgetting, improves data efficiency, and maintains the model's generalization ability.

Shihan Dou et al.~\cite{dou2024loramoe} proposed the LoRAMoE framework, which integrates LoRA and router networks, introducing local balance constraints to effectively mitigate the forgetting of world knowledge while enhancing multi-task handling capabilities.

Shiwen Ni et al.~\cite{ni2023forgetting} proposed the F-Learning paradigm, which first forgets old knowledge before learning new knowledge. Experiments show that it outperforms traditional fine-tuning, and the LoRA parameter reduction method achieves results comparable to full-parameter fine-tuning.

Junhao Zheng et al.~\cite{zheng2023learn} proposed the SEQ method, which enhances the performance of LLMs in incremental learning through simple strategies, reducing both parameters and training time.


\subsubsection{Instruction Fine-tuning}

To mitigate catastrophic forgetting, Continual-T0~\cite{scialom2022fine} uses a memory buffer for rehearsal~\cite{shin2017continual}, storing data from previous tasks and replaying them during training.

ConTinTin~\cite{yin2022contintin} proposed InstructionSpeak, which includes two strategies that fully leverage task instructions to improve both forward and backward transfer. The first strategy involves learning from negative outputs, while the second focuses on revisiting the instructions of previous tasks.

ELM~\cite{jang2023exploring} trains a small expert adapter for each task on top of the LLM. It then adopts a retrieval-based approach to select the most relevant expert LLM for each new task.

Based on the parameter-efficient tuning (PET) framework, OLoRA~\cite{wang2023orthogonal} introduces orthogonal low-rank adaptation for CIT. O-LoRA gradually learns new tasks in orthogonal subspaces while preserving the LoRA parameters learned from past tasks, thereby minimizing catastrophic forgetting.

DAPT~\cite{zhao2024dapt} introduces an innovative dual-attention framework, which coordinates the learning and selection of LoRA parameters through a dual-attention learning and selection module.

LLaMA PRO~\cite{wu2024llama} introduces an innovative block expansion technique that allows new knowledge to be injected into the LLM while efficiently retaining the initial functionality through post-training.

AdaptLLM~\cite{cheng2023adapting} adapts the LLM to different domains by enriching the original training corpus with a series of content-related reading comprehension tasks. These tasks are designed to help the model leverage domain-specific knowledge while enhancing prompt performance.

\cite{zhang2023reformulating} designed an adapt-retrieve-revise process to enable the LLM to adapt to new domains.

\cite{dong2023abilities} analyzed LLMs that continuously adapt to different domains and found that the order of training data has a significant impact on the performance of LLMs.

DynaInst~\cite{mok2023large} proposes a hybrid approach that combines dynamic instruction replay with a local minima-inducing regularizer. These two components enhance the generalization of the LLM while reducing memory and computational usage in the replay module.

Large language models have made significant progress in the research of continual learning. Through model improvements and methods such as instruction fine-tuning, LLMs are able to expand their knowledge while effectively addressing the issue of catastrophic forgetting. However, as model sizes continue to grow, core challenges in the field of continual learning for LLMs remain, such as how to handle updates and learning with large-scale data, and how to maintain good adaptability in multi-task and cross-modal environments. These remain critical issues that need to be addressed.

Continual learning is a multidimensional and complex research field, characterized by both challenges and opportunities. From unimodal to multimodal, and then to continual learning in large language models, each category of methods and strategies presents its own unique challenges and innovations. Future research will not only need to deepen the understanding of existing methods but also explore how to achieve more efficient and robust continual learning in environments with large-scale, multimodal data and tasks. As computational power and data scale continue to expand, research in continual learning will provide a more solid theoretical and technological foundation for the adaptability, robustness, and sustainability of intelligent systems.


