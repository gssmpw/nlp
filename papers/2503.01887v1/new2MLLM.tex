
\begin{table*}[htbp]
\small
\renewcommand\arraystretch{1.2}
  \centering
  \caption{Innovations in MLLM Frameworks.}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm}|p{0.4\textwidth}|p{0.4\textwidth}}
    \hline
   \multicolumn{1}{c|}{MLLMs} & \multicolumn{1}{c|}{Starting point of the problem} & \multicolumn{1}{c}{How to solve} \\
   
   \hline
   \multirow{2}{*}{\textbf{MaVEn}~\cite{jiang2024maven}} & 
Enhancing the image visual understanding of MLLMs.& MaVEn proposes an effective multi-granularity hybrid visual encoding framework.\\
   
   \hline
   \multirow{2}{*}{\textbf{MoVA}~\cite{zong2024mova}} & 
No single visual encoder can dominate the understanding of various image contents.& MoVA incorporates coarse-grained context-aware expert routing and fine-grained expert fusion.\\

   \hline
   \multirow{2}{*}{\textbf{MoME}~\cite{shen2024mome}} & 
The performance of general-purpose MLLMs is typically inferior to that of expert MLLMs. & MoME combines the MoVE and the MoLE to reduce task interference.\\

   \hline
   \multirow{2}{*}{\textbf{Meteor}~\cite{lee2024meteor}} & 
The performance gap of MLLMs in understanding and answering complex questions.& Meteor introduced the new concept of "traversal of rationales."\\

    \hline
   \multirow{2}{*}{\textbf{CORY}~\cite{ma2024coevolving}} & 
The stability and performance issues MLLMs encounter in RL fine-tuning.& CORY leverages the inherent cooperative evolution and emergence capabilities of multi-agent systems.\\

    \hline
   \multirow{2}{*}{\textbf{Lumen}~\cite{jiao2024lumen}} & 
MLMs overlook the intrinsic characteristics of different visual tasks.& Lumen enhances multimodal understanding by separating task-agnostic and task-specific learning.\\

    \hline
   \multirow{2}{*}{\textbf{Octopus}~\cite{zhaooctopus}} & 
MLLMs combine visual recognition and understanding sequentially at the LLM, which is suboptimal.& Octopus proposed the "Parallel Recognition â†’ Sequential Understanding" MLLM framework.\\

    \hline
   \multirow{2}{*}{\textbf{Wings}~\cite{zhang2024wings}} & 
MLLMs tend to forget knowledge acquired from text-only instructions during training.& Wings introduces additional modules and mechanisms to compensate for attention shifts.\\

    \hline
   \multirow{2}{*}{\textbf{Cantor}~\cite{gao2024cantor}} & 
The "hallucination" problem in decision-making is caused by insufficient visual information.& Cantor inspires a multimodal chain-of-thought of MLLM.\\

    \hline
   \multirow{2}{*}{\textbf{AutoM3L}~\cite{luo2024autom3l}} & 
The limitations of automation in multimodal machine learning.& AutoM3L proposes an automated multimodal machine learning framework with MLLMs.\\

    \hline
   \multirow{2}{*}{\textbf{DI-MML}~\cite{fan2024detached}} & 
The modality competition issue in multimodal learning.& DI-MML proposes detached and interactive multimodal learning.\\

    \hline
   \multirow{2}{*}{\textbf{MEM}~\cite{liu2024multimodal}} & 
Data scraped from networks may leak personal privacy.& MEM optimizes by combining image noise and text triggers to mislead the model into learning shortcuts.\\

    \hline
   \multirow{2}{*}{\textbf{CREAM}~\cite{zhang2024cream}} & 
The lack of cross-page interaction support in document visual question answering.& CREAM proposes Coarse-to-Fine retrieval and multi-modal efficient tuning for document VQA.\\

    \hline
   \multirow{3}{*}{\textbf{SLUDA}~\cite{zheng2024self}} & 
Insufficient labeled data and the underutilization of unlabeled data.& SLUDA generates fine-grained data, optimizes unlabeled data usage, and employs adaptive selection and dynamic threshold strategies. \\

    \hline
   \multirow{2}{*}{\textbf{SAM}~\cite{wu2024semantic}} & 
The semantic alignment issue in MLLMs when processing multi-image instructions.& SAM enhances image-semantic associations through a bidirectional semantic guidance mechanism. \\

    \hline
   \multirow{3}{*}{\textbf{CTVLMs}~\cite{lu2024collaborative}} & 
Improving performance and reducing computational resource demands in MLLMs for multimodal tasks.& CTVLMs use knowledge distillation and multimodal alignment to transfer knowledge from large models to smaller ones.\\

    \hline
   \multirow{2}{*}{\textbf{Bloom}~\cite{kim2024efficient}} & 
Reducing the high computational cost of large-scale multilingual visual data modeling.& Bloom proposes pre-training with discretized visual speech representation.\\

    \hline
   \multirow{4}{*}{\shortstack{\textbf{MA-AGIQA} \\ \cite{wang2024large}}}& 
The quality evaluation issue of AI-generated images (AGIs).& MA-AGIQA combines multimodal models and traditional DNNs, utilizing semantic information extraction and the mixture of experts (MoE) structure to dynamically integrate quality-aware features.\\

    \hline
   \multirow{2}{*}{\makecell{\textbf{WorldGPT}\\~\cite{ge2024worldgpt}}} & Enhancing the applicability and generalization ability of MLLMs.& WorldGPT includes memory offloading, knowledge retrieval, and a Context Reflector.\\

    \hline
   \multirow{2}{*}{\makecell{\textbf{Q-ALIGN}\\~\cite{wu2023q}}} & 
Enhancing the applicability and generalization ability of MLLMs.& Q-ALIGN unifies IQA, IAA, and VQA tasks to enhance the model's cross-task generalization ability.\\

    \hline
   \multirow{2}{*}{\textbf{Flextron}~\cite{cai2024flextron}} & 
The deployment challenges of MLLMs in resource-constrained environments.& Flextron selects different sub-models or sub-networks by using routers.\\

    \hline
   \multirow{2}{*}{\makecell{\textbf{NExT-GPT}\\~\cite{wu2023next}}} & 
Existing MLLMs can only understand the input modality.& NExT-GPT proposes lightweight alignment techniques and modality-switching instruction tuning.\\


    \hline
    \end{tabularx}
  \label{MLLM_frame}%
  \vspace{-5mm} 
\end{table*}%



\section{Multimodal Large Language Model}

\subsection{Preliminary}
In this section, we provide an overview of the latest research on MLLMs, including various model innovation strategies, a range of benchmarks, and the application of MLLMs in diverse domains.

\begin{table*}[htbp]
\small
\renewcommand\arraystretch{1.2}
  \centering
  \caption{Innovations in MLLM Methods.}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm}|p{0.4\textwidth}|p{0.4\textwidth}}
    \hline
   \multicolumn{1}{c|}{Method} & \multicolumn{1}{c|}{Starting point of the problem} & \multicolumn{1}{c}{How to solve} \\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{DenseFusion}\\~\cite{li2024densefusion}}} & 
Enhancing the visual perception ability of MLLMs.& DenseFusion proposes a multimodal perception fusion method that integrates visual experts.\\
      
   \hline
   \multirow{2}{*}{\textbf{E2E-MFD}~\cite{zhang2024e2e}} & 
The complex training process hinders the broader application of MLLMs.& E2E-MFD proposes a novel end-to-end algorithm for multimodal fusion detection.\\
      
   \hline
   \multirow{2}{*}{\textbf{NAM}~\cite{fangtowards}} & 
Neuron attribution in MLLMs has not been fully explored yet.& NAM proposes a neuron attribution method tailored for MLLMs.\\
      
   \hline
   \multirow{2}{*}{\textbf{CODE}~\cite{kim2024code}} & 
Addressing the hallucination problem in MLLMs when generating visual content.& CODE utilizes self-generated descriptions as contrastive references to adjust the information flow.\\
      
   \hline
   \multirow{2}{*}{\makecell{\textbf{MULTEDIT}\\~\cite{basu2024understanding}}} & 
To correct errors and insert new information. & MULTEDIT introduces a multimodal causal tracking method.\\
      
   \hline
   \multirow{2}{*}{\textbf{QSLAW}~\cite{xie2024advancing}} & 
Tackling the resource consumption issue faced by MLLMs in visual-language instruction tuning. & QSLAW learns group scale factors of quantized weights and adopts multimodal pretraining method.\\
      
   \hline
   \multirow{2}{*}{\textbf{LECCR}~\cite{wang2024multimodal}} & 
To improve the quality of cross-modal alignment.& LECCR proposes the MLLM-enhanced cross-lingual, cross-modal retrieval method.\\
      
   \hline
   \multirow{2}{*}{\textbf{ERL-MR}~\cite{han2024erl}} & 
To address the modality imbalance problem in MLLMs.& ERL-MR uses Euler transformations and multimodal constraint loss.\\
      
   \hline
   \multirow{2}{*}{\textbf{AMMPL}~\cite{wu2024adaptive}} & 
Enhancing the model's performance and reasoning ability. & AMMPL proposes an adaptive multimodal prompt learning method.\\
      
   \hline
   \multirow{2}{*}{\textbf{PaRe}~\cite{cai2024enhancing}} & 
Enhancing the model's performance and reasoning ability. & PaRe progressively generates intermediate modalities and replaces modality-agnostic fragments.\\
      
   \hline
   \multirow{2}{*}{\textbf{MCL}~\cite{liimproving}} & 
Addressing the insufficient interaction problem when handling complex multimodal scenarios. & MCL proposes the multimodal combination learning (MCL) method.\\
      
   \hline
   \multirow{2}{*}{\textbf{FARE}~\cite{schlarmann2024robust}} & 
MLLMs are vulnerable to adversarial attacks in the visual modality. & FARE proposes the unsupervised adversarial fine-tuning scheme.\\
      
   \hline
   \multirow{2}{*}{\textbf{DICL}~\cite{huang2023machine}} & 
Reducing the reliance on manual annotations. & DICL leverages MLLMs knowledge to enhance the robustness of visual models.\\
      
   \hline
   \multirow{2}{*}{\textbf{API}~\cite{yu2025attention}} & 
Addressing the limitations of traditional visual prompting techniques. & API enhances model perception through attention heatmaps guided by text queries.\\
      
   \hline
   \multirow{2}{*}{\textbf{IVTP}~\cite{huang2025ivtp}} & 
Addressing the high computational cost problem in MLLMs. & IVTP proposeS the instruction-guided visual token pruning method.\\


   \hline
   \multirow{2}{*}{\makecell{\textbf{ChatTracker}\\~\cite{sun2024chattracker}}} & 
Enhancing the tracking performance of MLLM trackers.& ChatTracker proposes a novel reflection-based prompt optimization module.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{Optimus-1}\\~\cite{li2024optimus}}} & 
Current general agents lack the necessary world knowledge and multimodal experience.& Optimus-1 proposes a hybrid multimodal memory module.\\
   
   \hline
   \multirow{2}{*}{\textbf{CuMo}~\cite{li2024cumo}} & 
Improving the performance of MLLMs on multimodal tasks.& CuMo integrates sparse gated Top-K MoE blocks in the visual encoder and MLP connectors.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{AcFormer}\\~\cite{liu2024visual}}} & 
The connection between visual encoders and LLMs has limitations.& AcFormer identified visual anchors and proposed a novel vision-language connector\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{Chain-of-Sight}\\~\cite{huang2024accelerating}}} & 
Accelerating the pretraining process and improving model performance.& Chain-of-Sight captures visual details at different spatial scales through a multi-scale visual resampler.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{Dense Con-}\\ \textbf{nector}~\cite{yao2024dense}}} & 
Existing MLLMs underutilise the visual encoder while overly emphasising the language modality.& Dense Connector enhances the visual perception ability by integrating multi-layer visual features.\\
   
   \hline
   \multirow{2}{*}{\textbf{GCG}~\cite{wang2024weakly}} & 
In video question answering, MLLMs overlook visually relevant cues related to the question.& GCG learns to represent the temporal structure of videos and selects key frames.\\
   
   \hline
   \multirow{2}{*}{\textbf{Q-MoE}~\cite{wang2024q}} & 
Connection structure struggles with filtering visual information according to task requirements.& Q-MoE proposes a query-based hybrid expert connector.\\



    \hline
    \end{tabularx}
  \label{MLLM_method}%
  \vspace{-5mm} 
\end{table*}%

\subsection{Model Innovation}
With the continuous development of MLLMs, researchers have made various innovations in their structure, methods, and functional modules to enhance model performance, generalization ability, and adaptability. This section reviews the main innovations, which focus on three core directions: framework design, method optimization, and functional module improvements. These innovations collectively drive the performance of MLLMs in complex multimodal tasks. This section will explore the latest research advancements in these areas.

\subsubsection{Framework Innovation}
Framework innovation is the foundation of MLLM development, aiming to achieve efficient fusion and processing of cross-modal information by improving the overall architectural design. In recent years, researchers have proposed many efficient framework designs. As shown in Table \ref{MLLM_frame}, researchers have proposed several efficient framework designs, such as MaVEn, MoVA, AutoM3L, DI-MML and et. These framework innovations provide more efficient tools and methods for MLLMs to handle multimodal tasks involving language, vision, and hearing. They enable MLLMs to achieve more precise reasoning and decision-making in the interaction of multimodal data, thereby offering strong support for solving complex problems in practical applications.
More details of the innovation of MLLMs frameworks are provided in Section \ref{appendix_MLLM_MI} of the Appendix.

\subsubsection{Method Innovation}

Method innovation is the core driving force behind the performance improvement of MLLMs. By designing more efficient training methods and optimization objectives, it helps models better adapt to dynamic task environments. As shown in Table~\ref{MLLM_method}, in recent years, researchers have proposed numerous novel and efficient methods to enhance the accuracy and robustness of MLLMs. These method research has explored cutting-edge techniques such as multimodal contrastive learning, self-supervised learning objectives, and multimodal alignment mechanisms. These methods not only enhance the model's generalization ability but also significantly improve the accuracy and robustness of cross-modal tasks.
More details of the innovation of MLLMs methods are provided in Section \ref{appendix_MLLM_MI} of the Appendix.



% \subsubsection{Module Innovation}

% In terms of module innovation, researchers have focused on refining the internal design of models by improving specific modules to enhance cross-modal interaction and representational capabilities. These modules have increased the flexibility and efficiency of the models. As shown in Table~\ref{MLLM_module}, in recent years, many efficient and streamlined modules have been proposed, further boosting the overall performance of the models.These modules, through independent optimization and joint modeling of different modalities, enable the model to perform exceptionally well in tasks such as multimodal reasoning, generation, and understanding. They allow for more accurate cross-modal knowledge fusion and semantic understanding, providing crucial support for the functional expansion and practical application of MLLMs.

% More details of the innovation of MLLMs Modules are provided in Section \ref{appendix_MLLM_MI} of the Appendix.





% \begin{table*}[htbp]
% \small
% \renewcommand\arraystretch{1.2}
%   \centering
%   \caption{Innovations in Multimodal Large Model Modules.}
%     \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm}|p{0.4\textwidth}|p{0.4\textwidth}}
%     \hline
%    \multicolumn{1}{c|}{Module} & \multicolumn{1}{c|}{Starting point of the problem} & \multicolumn{1}{c}{How to solve} \\
   




%     \hline
%     \end{tabularx}
%   \label{MLLM_module}%
%   \vspace{-5mm} 
% \end{table*}%


\begin{table*}[htbp]
\small
\renewcommand\arraystretch{1.2}
  \centering
  \caption{Innovations in Non-LLM Unimodal CL Frameworks.}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm}|p{0.4\textwidth}|p{0.4\textwidth}}
    \hline
   \multicolumn{1}{c|}{Framework} & \multicolumn{1}{c|}{Starting point of the problem} & \multicolumn{1}{c}{How to solve} \\
   
   \hline
   \multirow{2}{*}{\textbf{NTE}~\cite{benjamin2024continual}} & 
Addressing the catastrophic forgetting problem in graph neural networks.& NTE views a neural network as an ensemble of fixed experts.\\
   
   \hline
   \multirow{2}{*}{\textbf{IsCiL}~\cite{lee2024incremental}} & 
To address the issue of new data lacking labels due to annotation delays in continual learning.&IsCiL improves sample efficiency and task adaptability by incrementally learning shared skills.\\
   
   \hline
   \multirow{3}{*}{\textbf{CKP}~\cite{xu2024mitigate}} & 
To address the performance degradation caused by incorrect labels in the Lifelong Person Re-Identification task.& CKP purifies data through the CDP and ILR modules, and filters out erroneous knowledge using the EKF algorithm.\\
   
   \hline
   \multirow{2}{*}{\textbf{PBR}~\cite{liu2024prior}} & 
To reduce forgetting and enhances long-tail continual learning performance.& PBR proposes an uncertainty-guided sampling strategy and two prior-free constraints.\\
   
   \hline
   \multirow{2}{*}{\textbf{OSN}~\cite{hutask}} & 
Reducing the interference of new tasks on old tasks.& OSN explores shared knowledge between old and new tasks through parameter sharing.\\
   
   \hline
   \multirow{2}{*}{\textbf{MoDE}~\cite{lee2024becotta}} & 
Improving adaptation to new domains while preserving old knowledge.& MoDE includes domain-adaptive routing and domain-expert collaborative loss.\\
   
   \hline
   \multirow{2}{*}{\textbf{SB-MCL}~\cite{lee2024learning}} & 
To address the catastrophic forgetting problem in continual learning.& SB-MCL achieves continual learning through sequential Bayesian updates.\\
   
   \hline
   \multirow{2}{*}{\textbf{PNR}~\cite{charegularizing}} & 
Addressing the knowledge transfer and catastrophic forgetting issues.& PNR Generates pseudo-negative samples and optimizing knowledge transfer.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{CompoNet}\\~\cite{malagonself}}} & 
Addressing the issue of old task forgetting caused in continual reinforcement learning.& CompoNet proposes a modular neural network with linearly growing parameters.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{Vector-HaSH}\\~\cite{wangrapid}}} & 
To enable fast learning and continual memory.& Vector-HaSH combines hetero-associative memory and spatially invariant CNNs.\\
   
   \hline
   \multirow{2}{*}{\textbf{DDDR}~\cite{liang2025diffusion}} & 
Addressing the issue of catastrophic forgetting in federated continual learning.& DDDR uses diffusion models to generate historical data and employs contrastive learning.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{PromptCCD}\\~\cite{cendra2025promptccd}}} & 
Mitigating catastrophic forgetting.& PromptCCD introduces the GMP, which dynamically generates prompts to adapt to new classes.\\
   
   \hline
   \multirow{2}{*}{\textbf{Mecoin}~\cite{li2024efficient}} & 
To reduce parameter fine-tuning, lower the forgetting rate.& Mecoin employs SMU and a MeCo for efficient storage and updating of class prototypes.\\
   
   \hline
   \multirow{2}{*}{\textbf{RP2F}~\cite{sun2024incremental}} & 
Enabling effective knowledge sharing and backward knowledge transfer.& RP2F uses perturbation methods to approximate the Hessian matrix and introduces a prior.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{HAMMER}\\~\cite{liu2024hierarchical}}} & 
To address the catastrophic forgetting issue in multilingual text recognition.& HAMMER proposes online knowledge analysis and a hierarchical language evaluation mechanism.\\
   
   \hline
   \multirow{2}{*}{\textbf{FedCBC}~\cite{yu2024overcoming}} & 
Mitigating catastrophic forgetting.& FedCBC proposes category-specific binary classifiers and selective knowledge fusion.\\
   
   \hline
   \multirow{2}{*}{\textbf{TS-ILM}~\cite{xiaochen2024ts}} & 
Reducing information redundancy and enhancing memory retention.& TS-ILM proposes a task-level temporal pattern extractor and a time-sensitive example selector.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{AutoActivator}\\~\cite{li2024harnessing}}} & 
To address the issue of model forgetting old classes when continuously learning new classes.& AutoActivator dynamically adapts neural units to new tasks, enabling on-demand network expansion.\\
   
   \hline
   \multirow{2}{*}{\textbf{iNeMo}~\cite{fischer2024inemo}} & 
To achieve efficient class-incremental learning.& iNeMo proposes latent space initialization and position regularization.\\
   
   \hline
   \multirow{3}{*}{\textbf{TACO}~\cite{han2024topology}} & 
Offering a novel perspective for understanding and mitigating catastrophic forgetting.& TACO combines graph coarsening and continual learning to dynamically store information from previous tasks.\\



    \hline
    \end{tabularx}
  \label{CL_NonL_Framework}%
  \vspace{-5mm} 
\end{table*}%

\subsection{Benchmarks}
As MLLMs continue to achieve breakthroughs in multimodal tasks such as vision, language, and speech, comprehensive benchmarks have become crucial for systematically evaluating and comparing model performance. These benchmarks not only provide standardized datasets and tasks, but also define metrics for assessing models' abilities in cross-modal reasoning, generation, classification, and other areas. They play a key role in guiding research directions, identifying model limitations, and advancing technological progress. More details of the overview of MLLM benchmarks are provided in Section~\ref{appendix_benchmarks} of the Appendix. Section~\ref{appendix_benchmarks} in the Appendix introduces some of the recent representative benchmarks, covering a wide range of scenarios from academic research to practical applications, reflecting the diverse needs and challenges in the multimodal field.


\begin{table*}[htbp]
\small
\renewcommand\arraystretch{1.2}
  \centering
  \caption{Innovations in Non-LLM Unimodal CL Methods.}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm}|p{0.4\textwidth}|p{0.4\textwidth}}
    \hline
   \multicolumn{1}{c|}{Method} & \multicolumn{1}{c|}{Starting point of the problem} & \multicolumn{1}{c}{How to solve} \\
   
   \hline
   \multirow{2}{*}{\textbf{GACL}~\cite{zhuang2024gacl}} & 
Addressing the catastrophic forgetting problem of models in class-incremental learning.& GACL establishes the equivalence between incremental learning and joint training.\\
   
   \hline
   \multirow{2}{*}{\textbf{C-Flat}~\cite{zhuang2024gacl}} & 
Addressing the balance between new task training sensitivity and memory retention.& C-Flat optimizes the flatness of the loss landscape.\\
   
   \hline
   \multirow{2}{*}{\textbf{DSGD}~\cite{fan2024dynamic}} & 
Addressing the practical deployment challenge.& DSGD uses structural and semantic information for stable knowledge distillation.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{VQ-Prompt}\\~\cite{jiao2024vector}}} & 
To improve continual learning performance.& VQ-Prompt utilizes vector quantization to achieve end-to-end optimization of discrete prompt selection.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{RanDumb}\\~\cite{prabhurandom}}} & 
Exploring whether the representations generated by continual learning algorithms are truly effective.& RanDumb uses random transformations and linear classifiers to address.\\
   
   \hline
   \multirow{2}{*}{\textbf{IWMS}~\cite{csabalabel}} & 
The label delay issue in online continual learning.& IWMS prioritizes the memory of samples similar to new data.\\
   
   \hline
   \multirow{2}{*}{\textbf{PPE}~\cite{li2024progressive}} & 
To address the catastrophic forgetting problem in non-sample online continual learning.& PPE learns class prototypes during the online learning phase.\\
   
   \hline
   \multirow{2}{*}{\textbf{GPCNS}~\cite{yang2024introducing}} & 
Improving the performance of continual learning.& GPCNS enhances plasticity by utilizing gradient information from old tasks.\\
   
%    \hline
%    \multirow{2}{*}{\makecell{\textbf{Bayesian}\\ \textbf{Adaptation}~\cite{thapabayesian}}} & 
% Improving the performance of continual learning.& Non-parametric Bayesian adapts the width through a conjugate Bernoulli process.\\
   
   \hline
   \multirow{2}{*}{\textbf{CILA}~\cite{wen2024provable}} & 
Improving the performance of continual learning.& CILA proposes an adaptive distillation coefficient and theoretical performance guarantees.\\
   
   \hline
   \multirow{2}{*}{\textbf{POCL}~\cite{wumitigating}} & 
Existing methods fail to fully leverage the inter-task dependencies.& POCL models task relationships through Pareto optimization and dynamically adjusts weights.\\
   
   \hline
   \multirow{2}{*}{\textbf{Powder}~\cite{piaofederated}} & 
Addressing the cross-task and cross-client knowledge transfer in federated continual learning.& Powder enables prompt-based dual knowledge transfer.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{AdaPromptCL}\\~\cite{kim2023one}}} & 
Addressing the challenge of task-specific semantic variations.& AdaPromptCL proposes dynamic semantic grouping and prompt adjustment.\\
   
   \hline
   \multirow{2}{*}{\textbf{LPR}~\cite{kim2023one}} & 
To reduce catastrophic forgetting and underfitting.& LPR adjusts the optimization geometry to balance the learning of new and old data.\\
   
   \hline
   \multirow{2}{*}{\textbf{InfLoRA}~\cite{liang2024inflora}} & 
To address the issue of forgetting old tasks when adapting to new tasks.& InfLoRA injects parameter reparameterization into pre-trained weights.\\
   
   \hline
   \multirow{2}{*}{\textbf{F-OAL}~\cite{zhuangf}} & 
To alleviate the issue of catastrophic forgetting in online class-incremental learning.& F-OAL proposes a forward online analytical learning method.\\
   
   \hline
   \multirow{2}{*}{\textbf{PRL}~\cite{shiprospective}} & 
Improving performance in non-sample class-incremental learning.& PRL aligns reserved space and latent space to adapt new class features to the reserved space.\\
   
   \hline
   \multirow{2}{*}{\textbf{CIL}~\cite{hao2024addressing}} & 
To address the issue of catastrophic forgetting.& CIL proposes the CIL-balanced classification loss and distribution margin loss.\\
   
   \hline
   \multirow{2}{*}{\textbf{DSSP}~\cite{yang2024domain}} & 
To eliminate the need for sample replay.& DSSP leverages domain sharing and task-specific prompt learning.\\
   
   \hline
   \multirow{2}{*}{\textbf{MRFA}~\cite{zhengmulti}} & 
To reduce catastrophic forgetting.& MRFA optimizes the entire layer margin by enhancing the features of review samples.\\
   
   \hline
   \multirow{2}{*}{\textbf{DARE}~\cite{jeeveswaran2024gradual}} & 
Improving the model's performance on old tasks.& DARE reduces representation drift through a three-stage training process.\\
   
   \hline
   \multirow{2}{*}{\textbf{EASE}~\cite{zhou2024expandable}} & 
To reduce catastrophic forgetting.& EASE constructs task-specific subspaces using lightweight adapters.\\



    \hline
    \end{tabularx}
  \label{CL_NonL_Method}%
  \vspace{-5mm} 
\end{table*}%



\subsection{Applications of MLLMs}

Multimodal large models (MLLMs) have emerged as a significant direction in artificial intelligence research in recent years~\cite{zhan2024anygpt,chiang2023vicuna,jiang2023motiongpt,zhang2024motiongpt,huang2023visual,li2023large,liu2024improved,mu2024embodiedgpt}. With the rapid development of technologies such as natural language processing, computer vision, and speech recognition, single-modal intelligent systems can no longer meet the increasingly complex requirements of real-world applications~\cite{park2023generative,radford2021learning,rocamonde2023vision,sun2023aligning}. Multimodal learning, by integrating different types of data inputs, simulates the diversity and complexity of human information processing, offering more comprehensive and flexible intelligent services. At the same time, with the deepening of interdisciplinary research, MLLMs will not only play a role in traditional AI tasks but will also expand into more edge domains, driving artificial intelligence from closed systems to a more open and intelligent ecosystem.
More details of the applications of MLLMs are provided in Section \ref{applications_MLLM} of the Appendix.

In summary, the application prospects of multimodal large models are vast. However, to fully unleash their potential, this requires the combined advancement of technological innovation and theoretical breakthroughs. In the future, with ongoing progress in algorithms, hardware, and cross-domain collaboration, it is expected that MLLMs will achieve more efficient and intelligent performance in a wider range of practical applications, further advancing the development of artificial intelligence.