


\begin{table*}[htbp]
\small
\renewcommand\arraystretch{1.2}
  \centering
  \caption{Innovations in Non-LLM Multimodal CL Methods.}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm}|p{0.4\textwidth}|p{0.4\textwidth}}
        \hline
   \multicolumn{1}{c|}{Method} & \multicolumn{1}{c|}{Starting point of the problem} & \multicolumn{1}{c}{How to solve} \\
   
   \hline
   \multirow{2}{*}{\textbf{CPP}~\cite{yuan2024continual}} & 
Improving the performance of continual learning.& CPP incorporates the CCE, TKD, and TPL mechanisms to achieve multimodal vision perception.\\
      
   \hline
   \multirow{2}{*}{\makecell{\textbf{CP-Prompt}\\~\cite{feng2024cp}}} & 
To reduce catastrophic forgetting.& CP-Prompt utilizes a dual-prompt strategy and parameter-efficient adjustments.\\
      
   \hline
   \multirow{2}{*}{\textbf{MMAL}~\cite{yue2024mmal}} & 
Reducing forgetting and enhancing incremental learning performance.& MMAL proposes the modality fusion module and MSKC module.\\
      
   \hline
   \multirow{2}{*}{\textbf{MSPT}~\cite{chen2023continual}} & 
To reduce catastrophic forgetting.& MSPT optimizes multimodal learning through gradient modulation and attention distillation.\\
      
   \hline
   \multirow{2}{*}{\makecell{\textbf{MedCoSS}\\~\cite{ye2024continual}}} & 
To reduce catastrophic forgetting.& MSPT propose a staged multimodal self-supervised learning framework that avoids modality conflicts.\\

   
   \hline
   \multirow{2}{*}{\textbf{ZiRa}~\cite{deng2024zero}} & 
Retaining zero-shot generalization ability.& ZiRa proposes zero-interference loss and a reparameterized dual-branch structure.\\
   
   \hline
   \multirow{2}{*}{\textbf{STELLA}~\cite{leestella}} & 
To reduce forgetting of previously learned knowledge.& STELLA proposes a localized patch importance scoring method.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{RCS-Prompt}\\~\cite{yang2024rcs}}} & 
To address the issue of overlap between old and new category spaces.& RCS-Prompt proposes bidirectional prompt optimization and prompt magnitude normalization.\\
   
   \hline
   \multirow{2}{*}{\textbf{ZSCL}~\cite{zheng2023preventing}} & 
To reduce catastrophic forgetting.& ZSCL proposes feature space distillation and parameter space weight integration.\\
   
   \hline
   \multirow{3}{*}{\textbf{CoCoOp}~\cite{zhou2022conditional}} & 
To address the issue of pretrained models lacking generalization ability to unseen classes when adapting to new tasks.& CoCoOp generates dynamic prompts using a lightweight neural network.\\
   
   \hline
   \multirow{2}{*}{\textbf{RAIL}~\cite{xu2024advancing}} & 
Improving cross-domain classification capabilities during continual learning.& RAIL uses recursive ridge regression and a no-training fusion module.\\

    \hline
    \end{tabularx}
  \label{CL_NonLM_Method}%
  \vspace{-5mm} 
\end{table*}%



\section{Continue Learning}

\subsection{Preliminary}

Continual Learning (CL) has become a central focus in AI research due to the rapid growth of deep learning and LLMs~\cite{li2017learning,loo2020generalized,pellegrini2021continual,sarfraz2023sparse,abbasi2022sparsity,huang2024etag,huang2024kfc,ke2020continual,yu2020semantic}. The challenge is to enable models to retain and enhance learning capabilities when faced with continuously changing data and tasks. Traditional methods assume that models can learn all tasks at once and maintain a fixed knowledge base, but in reality, data and tasks evolve, often leading to ``Catastrophic Forgetting''~\cite{chaudhry2018efficient,chen2020mitigating,de2021continual,miao2021continual,pham2021dualnet,konishi2023parameter,li2023memory,li2023variational}. Therefore, CL, as a learning paradigm that better aligns with real-world application needs, aims to enable models to effectively accumulate and update knowledge across multiple stages, thereby better adapting to dynamic and evolving environments.


This section will provide a detailed classification and overview of the latest innovative research in continual learning. The specific content is divided into three parts: 1) Exploring non-LLMs unimodal continual learning and focusing on traditional models' continual learning research in unimodal data; 2) Analyzing non-LLMs multimodal continual learning and discussing the challenges and research in continual learning across multi-modal data; 3) Analyzing and summarizing the latest advancements in continual learning for LLMs and examining the unique challenges and solutions they face when handling large-scale textual data.

% \subsection{Non-LLM Unimodal Continual Learning}
\subsection{Non-LLM Unimodal CL}
In traditional unimodal learning, research on continual learning primarily focuses on how to prevent models from forgetting previously learned knowledge when learning new tasks. Many researchers have proposed solutions to this problem, including strategies based on knowledge retention, incremental learning methods, and improvements to neural network architectures~\cite{shin2017continual,tao2020topology,wang2021training,sun2023decoupling,sun2021ilcoc,rypesc2024divide,sarfraz2023sparse,shi2023prototype}. For non-large models, the challenges of continual learning are particularly pronounced due to limitations in computational resources. Furthermore, the unimodal continual learning for non-large models primarily focuses on individual modalities such as vision, speech, and text. As show in Tables~\ref{CL_NonL_Framework} and~\ref{CL_NonL_Method}, to address the specific characteristics of these tasks, researchers have proposed a variety of innovative frameworks and methods. Overall, unimodal continual learning with non-large models has made significant progress in scenarios with limited computational resources. Many innovative frameworks and methods have been developed to effectively mitigate catastrophic forgetting. However, how to scale these approaches to multimodal and large-scale data remains an important direction for future research.
More details of the non-LLM unimodal continual learning are provided in Section \ref{appendix_cl_nlu} of the Appendix.


% \subsection{Non-LLM Multimodal Continual Learning}
\subsection{Non-LLM Multimodal CL}
Compared to unimodal continual learning, multimodal continual learning presents more complex challenges. Data from different modalities often exhibit heterogeneity, and the key difficulty in multimodal continual learning for non-large models lies in how to effectively fuse information across modalities while retaining previously acquired knowledge during the process of learning new modalities. In recent years, researchers have proposed various methods to address these challenges, including inter-modal collaborative learning, shared and independent representations for each modality, and others~\cite{kirkpatrick2017overcoming,rebuffi2017icarl,ahn2019uncertainty,zenke2017continual,yoon2017lifelong,lee2020neural,madaan2021representational,cossu2024continual,fini2022self,yoon2023continual,yan2022generative,pian2023audio,mo2023class}. As shown in Table~\ref{CL_NonLM_Method}, these innovative methods enable non-large models to perform continual learning in multimodal environments, while minimizing knowledge conflicts between different modalities.
More details of the non-LLM multimodal continual learning are provided in Section \ref{appendix_cl_nlm} of the Appendix.

\begin{table*}[htbp]
\small
\renewcommand\arraystretch{1.2}
  \centering
  \caption{Innovations in LLM Instruction Fine-tuning Methods.}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm}|p{0.4\textwidth}|p{0.4\textwidth}}
    \hline
   \multicolumn{1}{c|}{Method} & \multicolumn{1}{c|}{Starting point of the problem} & \multicolumn{1}{c}{How to solve} \\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{ConTinTin}\\~\cite{yin2022contintin}}} & 
To reduce catastrophic forgetting.& InstructionSpeak learns from negative outputs and revisites the instructions of previous tasks.\\
      
   \hline
   \multirow{2}{*}{\textbf{OLoRA}~\cite{wang2023orthogonal}} & 
Improving the performance of continual learning.& OLoRA introduces orthogonal low-rank adaptation for CIT.\\
      
   \hline
   \multirow{2}{*}{\textbf{DAPT}~\cite{zhao2024dapt}} & 
To reduce catastrophic forgetting.&DAPT proposes a dual-attention learning and selection module.\\
      
   \hline
   \multirow{2}{*}{\textbf{ELM}~\cite{jang2023exploring}} & 
To reduce catastrophic forgetting.& ELM trains a small expert adapter for each task on top of the LLM.\\
      
   \hline
   \multirow{2}{*}{\makecell{\textbf{LLaMA PRO}\\~\cite{wu2024llama}}} & 
Retaining the initial functionality through post-training. & LLaMA PRO introduces an innovative block expansion technique.\\
      
   \hline
   \multirow{3}{*}{\makecell{\textbf{AdaptLLM}\\~\cite{cheng2023adapting}}} & 
To help the model leverage domain-specific knowledge while enhancing prompt performance. & AdaptLLM adapts the LLM to different domains by enriching the original training corpus with a series of content-related reading comprehension tasks.\\
      
   \hline
   \multirow{2}{*}{\textbf{DynaInst}~\cite{mok2023large}} & 
To enhance the generalization of the LLM.& DynaInst combines dynamic instruction replay with a local minima-inducing regularizer.\\

   \hline
   \multirow{2}{*}{\textbf{TAALM}~\cite{seo2024train}} & 
Enabling targeted knowledge updates and reducing forgetting.& TAALM uses meta-learning to dynamically predict token importance.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{D-CPT Law}\\~\cite{que2024d}}} & 
To reduce GPU resource consumption and improve domain adaptability.& D-CPT Law predicts the optimal training ratio.\\
   
   \hline
   \multirow{2}{*}{\textbf{COPAL}~\cite{malla2024copal}} & 
High computational demands and model adaptability limitations. & COPAL enables continual pruning without the need for retraining.\\
   
   \hline
   \multirow{2}{*}{\textbf{MagMax}~\cite{marczak2025magmax}} & 
To reduce catastrophic forgetting. & MagMax proposes sequential fine-tuning and maximum magnitude weight selection.\\
   
   \hline
   \multirow{2}{*}{\textbf{SAPT}~\cite{zhao2024sapt}} & 
Enabling effective knowledge retention and transfer.& SAPT aligns the learning and selection of PET blocks through a shared attention mechanism.\\
   
   \hline
   \multirow{2}{*}{\textbf{SSR}~\cite{huang2024mitigating}} & 
To reduce catastrophic forgetting.& SSR utilizes LLM-generated synthetic instances for rehearsal.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{LoRAMoE}\\~\cite{dou2024loramoe}}} & 
Enhancing multi-task handling capabilities.& LoRAMoE integrates LoRA and router networks, and introduces local balance constraints.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{F-Learning}\\ \textbf{paradigm}~\cite{dou2024loramoe}}} & 
Improving the performance of continual learning.& F-Learning paradigm first forgets old knowledge before learning new knowledge.\\




    \hline
    \end{tabularx}
  \label{CL_LLM_instr}%
  \vspace{-5mm} 
\end{table*}%




% \subsection{Continual Learning in LLM}
\subsection{CL in LLM}

LLMs such as GPT and BERT, with their powerful language understanding and generation capabilities, have achieved remarkable results on various natural language processing tasks~\cite{devlin2018bert,du2024chinese,eloundou2023gpts,kukreja2024literature,kasneci2023chatgpt,zhao2023survey,naveed2023comprehensive,chang2024survey,chen2021evaluating,unlu2023interpretutor,wu2024survey,zhang2023instruction}. However, LLMs still face unique challenges in continual learning. Particularly in the context of increasing data volume and task diversity, how to effectively update models, avoid catastrophic forgetting, and maintain efficient computational capabilities are key focuses in the research of LLMs for continual learning. As shown in Table~\ref{CL_LLM_instr}, researchers have proposed a variety of instruction fine-tuning methods. Through model improvements and methods such as instruction fine-tuning, LLMs are able to expand their knowledge while effectively addressing the issue of catastrophic forgetting. However, as model sizes continue to grow, core challenges in the field of continual learning for LLMs remain, such as how to handle updates and learning with large-scale data, and how to maintain good adaptability in multi-task and cross-modal environments. These remain critical issues that need to be addressed.
More details of the LLM continual learning are provided in Section \ref{appendix_cl_llm} of the Appendix.


Continual learning is a multidimensional and complex research field, characterized by both challenges and opportunities. From unimodal to multimodal, and then to continual learning in LLMs, each category of methods and strategies presents its own unique challenges and innovations. Future research will not only need to deepen the understanding of existing methods, but also explore how to achieve more efficient and robust continual learning in environments with large-scale, multimodal data and tasks. As computational power and data scale continue to expand, research in continual learning will provide a more solid theoretical and technological foundation for the adaptability, robustness, and sustainability of intelligent systems.








