\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth,height=0.5\linewidth]{fig/develop.png}
    \caption{Timeline of Multimodal Large Model Development.}
    \label{develop}
    \vspace{-4mm} 
\end{figure*} 

\section{Introduction}
Research on Multimodal Large Language Models (MLLMs) has rapidly advanced in recent years, becoming a significant direction in the field of artificial intelligence~\cite{zhan2024anygpt,chiang2023vicuna,jiang2023motiongpt,zhang2024motiongpt,liu2024sphinx,sarch2024ical,wu2024visionllm,li2024single,dong2024internlm,mu2024embodiedgpt}. By integrating multimodal information such as language, vision, and audio, these models demonstrate powerful cross-modal understanding and generation capabilities, providing innovative solutions to complex real-world problems~\cite{huang2024multimodal,dong2024multiood,wu2024hypergraph,liu2024multi,wang2024cloud}. To enhance the performance of MLLMs, researchers have proposed various improvement strategies. Firstly, for cross-modal information fusion, more efficient architectural designs have been introduced~\cite{shukor2024implicit,zhang2024caption,xu2024libra}, such as Transformer-based multimodal joint encoders and decoders, as well as lightweight cross-modal attention modules~\cite{zhuang2024towards,forouzandehmehr2024decoding,gao2024clova}. Secondly, pre-training techniques have been further developed, significantly improving the model's generalization ability and robustness through the introduction of multimodal contrastive learning, cross-modal consistency constraints, and self-supervised learning objectives~\cite{szot2024grounding,wu2024controlmllm,li2024membership,sagar2024failures}. In addition, fine-tuning techniques have become increasingly refined~\cite{zhai2024fine}, including efficient parameter adjustment methods (such as LoRA~\cite{hu2021lora}) and task-specific adaptation layer designs. These approaches enable MLLMs to adapt to diverse task scenarios with lower computational costs~\cite{houlsby2019parameter,liu2021p,liu2024gpt,li2021prefix}. As shown in Figure~\ref{develop}, the performance evaluation of MLLMs is based on multimodal benchmarks that cover a wide range of task categories. For example, benchmarks in the vision and language domain include Visual Question Answering (VQA)~\cite{antol2015vqa, mathew2021docvqa, masry2022chartqa,kafle2018dvqa,chen2021geoqa}, Image Captioning~\cite{lin2014microsoft,sharma2018conceptual,sidorov2020textcaps,gurari2020captioning,pont2020connecting,agrawal2019nocaps}, and Visual Grounding~\cite{cui2021s,yu2016modeling,mao2016generation,tanaka2019generating}; in the audio and language domain, benchmarks include Audio-Text Alignment and Audio Generation~\cite{gemmeke2017audio,hernandez2018ted,ardila2019common}; there are also more complex cross-modal reasoning tasks, among others~\cite{wadhawan2024contextual,wu2024see}. Moreover, MLLMs are also showing great potential in real-world applications. They are playing an increasingly important role in fields such as healthcare, education, robotics, and autonomous driving~\cite{guo2024llava,chen2024llm,huang2025making}.


Continual learning aims to address the challenge of how models can effectively learn new tasks while retaining prior knowledge when faced with dynamically changing data streams, thus mitigating the problem of catastrophic forgetting~\cite{cossu2024continual,qin2022elle,sun2020ernie}. In recent years, research in the field of continuous learning has been deepened, particularly with significant developments in its application across models of various scales and multimodal learning scenarios~\cite{wang2022learning,fu2024data,guo2025pilora,pang2024ftf,hu2023dense,abati2020conditional}. In unimodal settings, the focus has mainly been on the design of algorithms to alleviate the problem of catastrophic forgetting, enabling models to maintain performance in previous tasks while incorporating new ones~\cite{yang2024generating,guha2024diminishing,cha2024regularizingpseudonegativescontinualselfsupervised,lee2024becotta,zhao2024statistical,garg2025poet}. Research in multimodal continual learning is more challenging than in unimodal  settings, as models must simultaneously handle the characteristics of different modalities and their cross-modal interactions~\cite{kim2025one,pang2024ftf,yang2024semantic,nori2024task}. Researchers have primarily focused on techniques for cross-modal feature extraction, alignment, and processing, aiming to reduce cross-modal interference, enhance inter-modal consistency, and improve the model's generalization ability~\cite{kim2025open,marczak2025revisiting,li2024learn,chen2023promptfusion}. With the widespread application of large language models (LLMs) in natural language processing, research on their continual learning has become a new hotspot~\cite{guo2023continuous,colombo2024saullm,deng2024k2,gururangan2020don,han2020econet,ma2023ecomgpt}. Due to the massive parameter scale of LLMs and their reliance on vast amounts of pre-trained data, traditional continual learning strategies face challenges such as high computational costs and limited adaptability. To address these challenges, researchers have proposed several optimization directions: Parameter-Efficient Fine-Tuning (PEFT) methods (such as LoRA, Prefix Tuning, etc.)~\cite{hu2021lora,houlsby2019parameter,liu2021p,liu2024gpt,li2021prefix}, prompt learning methods, and so on. These approaches have shown tremendous potential in tasks such as open-domain question answering, continual dialogue systems, and cross-domain text generation~\cite{wang2017rtheta3,yang2019end,li2024reinforcement}.


The rapid development of MLLMs and the in-depth integration of CL research have provided new perspectives for the exploration of the frontier in the field of artificial intelligence~\cite{dong2024internlm,liu2024multi,zhang2024caption,li2024membership,guo2024llava,hadsell2020embracing,guha2024diminishing,garg2025poet,deng2024k2}. A key research challenge in this domain is how to efficiently retain knowledge from previous tasks while learning new ones while maintaining cross-modal collaboration capabilities~\cite{roth2024practitioner,zhang2023citb,panagopoulou2023x}. This has become a central research question in the field. Building on existing research, this paper provides a systematic review and summary of the research on continual learning in multimodal large models. It delves into the innovations in model structure and methods, including the design of various model frameworks, dynamic parameter adjustment mechanisms, and modules that support task adaptation~\cite{le2024mixture,jha2024clap4clip,chen2024dual,vesdapunt2025hvclip}. These techniques not only significantly mitigate the problem of catastrophic forgetting, but also effectively enhance the task adaptability and generalization ability of MLLMs. In addition, this paper also introduces existing benchmarks for evaluating continual learning in multimodal large models, which provide important support for assessing model performance in continual learning tasks~\cite{chen2024coin,srinivasan2022climb,cao2024continual,tang2024vilco}. Research on continual learning in multimodal large models not only provides new technological means for the dynamic adaptation of cross-modal tasks, but also offers innovative solutions for complex tasks in real-world domains such as intelligent education, healthcare, and robotic interaction~\cite{he2024towards,panagopoulou2023x,wang2024freeze,qi2024interactive}.

Finally, this paper offers a forward-looking discussion on the challenges and future development trends of continual learning in multimodal large models, covering aspects such as catastrophic forgetting, the improvement and standardization of evaluation benchmarks, and the enhancement of interpretability and transparency in multimodal large model continual learning. Through these discussions, the paper aims to provide valuable research insights for scholars in the field and promote the further development and application of continual learning technologies in multimodal large models.


