



\section{Multimodal Large Language Model}

\subsection{Preliminary}
In this section, we provide an overview of the latest research on MLLMs, including various model innovation strategies, a range of benchmarks, and the application of MLLMs in diverse domains.

\subsection{Model Innovation}
With the continuous development of MLLMs, researchers have made various innovations in their structure, methods, and functional modules to enhance model performance, generalization ability, and adaptability. This section reviews the main innovations, which focus on three core directions: framework design, method optimization, and functional module improvements. These innovations collectively drive the performance of MLLMs in complex multimodal tasks. This section will explore the latest research advancements in these areas.

\subsubsection{Framework Innovation}
Framework innovation is the foundation of MLLM development, aiming to achieve efficient fusion and processing of cross-modal information by improving the overall architectural design. In recent years, researchers have proposed many efficient framework designs. 

Chaoya Jiang et al.~\cite{jiang2024maven} introduced the multi-granularity hybrid visual encoding framework MaVEn, which combines discrete visual symbol sequences representing abstract, coarse-grained semantic concepts with traditional continuous representation sequences that simulate fine-grained features. This combination enhances the model's ability to understand visual information in images.

Zhuofan Zong et al.~\cite{zong2024mova} proposed the MoVA framework, which incorporates coarse-grained context-aware expert routing and fine-grained expert fusion. This framework adaptively routes and fuses visual experts for specific tasks through a coarse-to-fine mechanism, thereby mitigating the bias of the CLIP visual encoder and enhancing the model's ability to understand and process diverse image content.

Leyang Shen et al.~\cite{shen2024mome} proposed a multimodal expert mixing framework, MoME, which combines the visual expert mixture model (MoVE) and the language expert mixture model (MoLE) to reduce task interference.

Byung-Kwan Lee et al.~\cite{lee2024meteor} proposed the Meteor model, based on the Mamba architecture, which enhances the comprehension and response capabilities of large language and vision models through multifaceted reasoning.

Hao Ma et al.~\cite{ma2024coevolving} proposed the sequential cooperative multi-agent reinforcement learning framework, CORY, which enhances the stability and performance of multimodal large models in reinforcement learning fine-tuning by leveraging the inherent collaborative evolution and emergent capabilities of multi-agent systems.

Yang Jiao et al.~\cite{jiao2024lumen} proposed a vision-centric multimodal large model framework, Lumen, which strengthens multimodal content understanding by decoupling task-agnostic and task-specific learning. This framework enables flexible adaptation to various vision tasks, enhancing the LMM's capabilities in visual perception and instruction following.

Chuyang Zhao et al.~\cite{zhaooctopus} proposed the "Parallel Recognition → Sequential Understanding" MLLM framework, Octopus. This framework achieves parallel recognition of object queries at the lower LLM layers and passes the results to the top LLM layers for sequential understanding, thereby improving the efficiency and accuracy of MLLMs.

Yikai Zhang et al.~\cite{zhang2024wings} proposed the Wings framework, which introduces additional modules and mechanisms to compensate for attention shifts. This allows the model to effectively process visual information while maintaining focus on textual information.

Timin Gao et al.~\cite{gao2024cantor} proposed the Cantor framework, which integrates visual inputs with logical reasoning and leverages the advanced cognitive functions of MLLMs. By acting as a multifaceted expert, it directly acquires higher-level information, thereby improving decision-making quality.

Daqin Luo et al.~\cite{luo2024autom3l} proposed the AutoM3L framework, based on the AutoML architecture, which automates the construction of multimodal training pipelines, feature engineering, and model selection using LLMs, thereby reducing manual intervention.

Yunfeng Fan et al.~\cite{fan2024detached} proposed the DI-MML framework, which addresses modality competition in multimodal learning by independently training modality encoders. They introduced a shared classifier and DUC loss to facilitate cross-modal interaction and knowledge transfer, thereby mitigating the modality competition issue in multimodal learning.

Xinwei Liu et al.~\cite{liu2024multimodal} proposed the multi-step error minimization framework, MEM, which optimizes by combining image noise and text triggers. This approach misleads the model into learning shortcuts, thereby protecting data privacy.

Jinxu Zhang et al.~\cite{zhang2024cream} proposed the CREAM framework, which integrates high-performance retrieval enhancement, multi-image and multimodal processing, and efficient instruction tuning. This effectively addresses the challenges in document-based VQA tasks.

Li Zheng et al.~\cite{zheng2024self} proposed the Adaptive Multimodal Data Augmentation framework, SLUDA, which generates fine-grained data, optimizes the utilization of unlabeled data, and employs adaptive selection strategies and dynamic threshold adjustments. This approach addresses the issues of insufficient labeled data and the underutilization of unlabeled data.

Tao Wu et al.~\cite{wu2024semantic} proposed the SAM model, which enhances semantic associations between images by introducing a bidirectional semantic guidance mechanism. This improves the semantic alignment ability of multimodal instructions.

Shichen Lu et al.~\cite{lu2024collaborative} proposed the Tiny-Large collaborative training framework, CTVLMs, which leverages knowledge distillation and multimodal alignment to enable large models to transfer knowledge to smaller models. This approach achieves a dual improvement in both performance and efficiency.

Minsu Kim et al.~\cite{kim2024efficient} proposed the Bloom framework, which uses bidirectional modality transformation and adaptive cross-modal fusion. It pretrains a VSR (Visual Speech Recognition) model with visual and speech units and introduces a curriculum learning strategy to enhance training efficiency and multilingual recognition performance.

Yunshan Ma et al.~\cite{ma2024cirp} proposed the CIRP framework, which uses a multimodal encoder and cross-item contrastive loss to learn individual item semantics and relationships. By introducing a relationship pruning module, this framework enhances the ability to align cross-modal information and capture cross-item relationships in cold-start items.

Puyi Wang et al.~\cite{wang2024large} proposed the multimodal large model-assisted artificial intelligence-generated image quality assessment framework, MA-AGIQA. By combining multimodal models with traditional DNNs, and utilizing semantic information extraction and a mixture of experts (MoE) structure, the framework dynamically integrates quality perception features. This significantly improves the quality assessment performance of AGIs, particularly excelling in reducing the false-negative rate.

Zhiqi Ge et al.~\cite{ge2024worldgpt} proposed a novel cognitive framework, WorldGPT, which includes memory offloading, knowledge retrieval, and a Context Reflector to enhance the model's performance in specific scenarios and long-term tasks.

Haoning Wu et al.~\cite{wu2023q} proposed the ONEALIGN model, which unifies IQA, IAA, and VQA tasks, thereby enhancing the model's cross-task generalization ability.

Zixin Zhang et al.~\cite{zhangenhancing} proposed the M2FEDSA framework, which combines segmentation learning and multimodal federated learning. By introducing dual-adaptive fine-tuning and dual knowledge transfer strategies, the framework improves both computational and storage efficiency, as well as performance, when deploying large-scale multimodal models in federated learning settings.

Ruisi Cai et al.~\cite{cai2024flextron} proposed an elastic architecture called Flextron, which supports adaptive subnetwork selection. By using routers to choose different sub-models or subnetworks, Flextron addresses the deployment challenges of multimodal large models in resource-constrained environments.

Shengqiong Wu et al.~\cite{wu2023next} proposed an end-to-end Any-to-Any multimodal large model framework, which achieves efficient cross-modal understanding and generation through lightweight alignment techniques and modality-switching instruction tuning.

These framework innovations provide more efficient tools and methods for MLLMs to handle multimodal tasks involving language, vision, and hearing. They enable MLLMs to achieve more precise reasoning and decision-making in the interaction of multimodal data, thereby offering strong support for solving complex problems in practical applications.


\subsubsection{Method Innovation}

Method innovation is the core driving force behind the performance improvement of MLLMs. By designing more efficient training methods and optimization objectives, it helps models better adapt to dynamic task environments. In recent years, researchers have proposed numerous novel and efficient methods to enhance the accuracy and robustness of MLLMs.

Xiaotong Li et al.~\cite{li2024densefusion} proposed a comprehensive multimodal perception fusion method that integrates visual experts, thereby enhancing the visual perception capability of MLLMs.

Jiaqing Zhang et al.~\cite{zhang2024e2e} proposed a novel end-to-end algorithm for multimodal fusion detection, achieving high performance through a single training phase and simplifying the overall process.

Junfeng Fang et al.~\cite{fangtowards} proposed a neuron attribution method tailored for MLLMs, called NAM. NAM reveals the modality-specific semantic knowledge learned by neurons in MLLMs and highlights certain neuron characteristics that collectively elucidate the internal workings of MLLMs.

Jayneel Parekh et al.~\cite{parekh2024concept} proposed a concept extraction method based on dictionary learning to interpret the internal representations of large multimodal models. They innovatively defined multimodal concepts and validated their effectiveness in interpreting models and understanding test sample representations.

Junho Kim et al.~\cite{kim2024code} proposed CODE, which utilizes self-generated descriptions as contrastive references to dynamically adjust the information flow, enhancing the coherence and informativeness of responses. This approach addresses the hallucination problem in MLLMs when generating visual content.

Samyadeep Basu et al.~\cite{basu2024understanding} proposed the model editing algorithm MULTEDIT, which can correct errors and insert new information. They also introduced a multimodal causal tracking method, extending the research on information storage to other domains.

Jingjing Xie et al.~\cite{xie2024advancing} proposed the Quantized Scale Learning Method (QSLAW), which effectively reduces quantization errors, prevents overfitting, and improves model adaptability and efficiency by learning the group scale factors of quantized weights and employing a multimodal pretraining strategy.

Yabing Wang et al.~\cite{wang2024multimodal} proposed the MLLM-enhanced cross-lingual, cross-modal retrieval method LECCR. This approach leverages MLLMs to generate visual descriptions, which are then aggregated into multi-view semantic slots to enhance the semantic richness of visual features. By incorporating English feature guidance, it improves the quality of cross-modal alignment.

Zihao Liu et al.~\cite{liu2024adaptively} proposed a visual perception adapter and fine-grained tri-modal contrastive learning method. By aligning tokens across modalities, they reduce semantic gaps, thereby improving the performance of multimodal video tasks.

Weixiang Han et al.~\cite{han2024erl} proposed the ERL-MR strategy, which uses Euler transformations and multimodal constraint loss to transform inter-modal competition into cooperation, thereby achieving performance improvement.

Qiang Wang et al.~\cite{wang2024bilateral} proposed a bilateral adaptive cross-modal fusion prompt learning paradigm, Bloom, which achieves more flexible cross-modal interaction and alignment through bidirectional modal transformation and adaptive fusion functions. This significantly enhances the performance of CLIP on a variety of generalization tasks.

Zongqian Wu et al.~\cite{wu2024adaptive} proposed an adaptive multimodal prompt learning method, AMMPL, which effectively handles meaningless image patches and enhances the model's generalization ability through image prompts and cross-modal interaction learning.

Minghe Gao et al.~\cite{gao2024fact} proposed the Fact paradigm, which teaches MLLMs by generating Faithful, Concise, and Transferable multimodal rationales, enhancing the model's performance and reasoning ability across various visual tasks.

Lincan Cai et al.~\cite{cai2024enhancing} proposed the PaRe method, which enhances the stability and transferability of cross-modal fine-tuning by progressively generating intermediate modalities and replacing modality-agnostic fragments.

Wei Li et al.~\cite{liimproving} proposed the Multimodal Combination Learning (MCL) method, which strengthens the mapping between visual and language modalities. By leveraging LLMs to automatically generate multimodal learning samples, they introduced a stacked retrieval mechanism to extract diverse multimodal information.

Christian Schlarmann et al.~\cite{schlarmann2024robust} proposed the FARE unsupervised adversarial fine-tuning scheme, which enhances the robustness of the CLIP model while preserving its original performance, without the need for retraining on downstream tasks.

Zhuo Huang et al.~\cite{huang2023machine} proposed the DICL strategy, which leverages MLLM knowledge to enhance the robustness of visual models and align MLLMs with visual tasks. This approach enables unsupervised fine-tuning, improving performance in out-of-distribution (OOD) scenarios.

Runpeng Yu et al.~\cite{yu2025attention} proposed the API technique, which enhances model perception through attention heatmaps guided by text queries. This approach enables model self-reflection and integration, improving performance on visual-linguistic tasks and addressing the limitations of traditional visual prompting techniques.

Kai Huang et al.~\cite{huang2025ivtp} proposed the Instruction-guided Visual Token Pruning method (IVTP), which includes an intra-group Token Pruning (GTP) module and cross-modal instruction-guided pruning. This approach effectively reduces the number of visual tokens and lowers computational complexity, while maintaining model performance.

Recent method research has explored cutting-edge techniques such as multimodal contrastive learning, self-supervised learning objectives, and multimodal alignment mechanisms. These methods not only enhance the model's generalization ability but also significantly improve the accuracy and robustness of cross-modal tasks.

\subsubsection{Module innovation}

In terms of module innovation, researchers have focused on refining the internal design of models by improving specific modules to enhance cross-modal interaction and representational capabilities. These modules have increased the flexibility and efficiency of the models. In recent years, many efficient and streamlined modules have been proposed, further boosting the overall performance of the models.

Wenfang Yao et al.~\cite{sun2024chattracker} proposed a novel reflection-based prompt optimization module, leveraging multimodal large language models to generate high-quality language descriptions to improve tracking performance. By iteratively refining the vague and inaccurate descriptions of targets through tracking feedback, this approach addresses the issue of frequent ambiguous language descriptions in annotations.

Zaijing Li et al.~\cite{li2024optimus} proposed a hybrid multimodal memory module that transforms knowledge into a hierarchical directed knowledge graph, enabling agents to explicitly represent and learn world knowledge. Additionally, historical information is summarized into an abstract multimodal experience pool, providing agents with rich contextual learning references. This approach addresses the challenge of general agents struggling to complete long-term tasks in open-world environments.

Jiachen Li et al.~\cite{li2024cumo} enhanced model capabilities by integrating sparse gated Top-K MoE (Mixture-of-Experts) blocks in the visual encoder and MLP connectors, and by introducing MoE blocks during the visual instruction fine-tuning phase. This approach improves the performance of MLLMs on multimodal tasks.

Haogeng Liu et al.~\cite{liu2024visual} innovatively identified visual anchors and proposed a novel vision-language connector, AcFormer. By utilizing visual anchors to aggregate information, this approach significantly enhances the accuracy and computational efficiency of MLLMs.

Ziyuan Huang et al.~\cite{huang2024accelerating} proposed the Chain-of-Sight module, which captures visual details at different spatial scales through a multi-scale visual resampler. This module enables flexible expansion of the number of visual tokens after pretraining, accelerating the pretraining process while maintaining or improving model performance.

Huanjin Yao et al.~\cite{yao2024dense} proposed a new connector, the Dense Connector, which enhances the visual perception ability of MLLMs by integrating multi-layer visual features. It is characterized by high computational efficiency and ease of integration, addressing the issue of existing MLLMs underutilizing the visual encoder while overly emphasizing the language modality.

Haibo Wang et al.~\cite{wang2024weakly} designed the Gaussian Contrastive Localization (GCG) module, which learns to represent the temporal structure of videos and selects key frames relevant to the question. This approach addresses the issue in video question answering where large multimodal models neglect question-related visual cues and lack key timestamp annotations.

Hanzi Wang et al.~\cite{wang2024q} proposed a query-based hybrid expert connector, Q-MoE, which utilizes text-driven routing and an optimal expert path training strategy to achieve precise extraction and processing of task-specific visual information. This approach addresses the issue in MLLMs where the connection structure struggles to filter visual information according to task requirements in vision-language tasks.

These modules, through independent optimization and joint modeling of different modalities, enable the model to perform exceptionally well in tasks such as multimodal reasoning, generation, and understanding. They allow for more accurate cross-modal knowledge fusion and semantic understanding, providing crucial support for the functional expansion and practical application of MLLMs.

\subsection{Benchmarks}
As MLLMs continue to achieve breakthroughs in multimodal tasks such as vision, language, and speech, comprehensive benchmarks have become crucial for systematically evaluating and comparing model performance. These benchmarks not only provide standardized datasets and tasks but also define metrics for assessing models' abilities in cross-modal reasoning, generation, classification, and other areas. They play a key role in guiding research directions, identifying model limitations, and advancing technological progress. This section will introduce some of the recent representative benchmarks, covering a wide range of scenarios from academic research to practical applications, reflecting the diverse needs and challenges in the multimodal field.

\subsubsection{ROPE: Recognition-based Object Probing Evaluation Benchmark}
Despite the impressive performance of MLLMs in various downstream applications, they often encounter the issue of object hallucination~\cite{rohrbach2018object,dai2022plausible,li2023evaluating,zhang2024groundhog,zhai2023halle,liu2023mitigating,you2023ferret,zhou2023analyzing,wang2023llm}, where the model erroneously generates objects that do not exist in the image. Current benchmarks for evaluating object hallucination mainly focus on the presence of a single object category, rather than individual entities. 

Xuweiyi Chen et al.~\cite{chen2024multi} conducted a systematic study of the multi-object hallucination problem, examining how models misidentify objects when attending to multiple objects simultaneously (e.g., inventing non-existent objects or being distracted). They introduced an automated evaluation protocol called Recognition-based Object Probing Evaluation (ROPE), which considers the distribution of object categories within a single image during testing. By using visual reference to disambiguate, the protocol systematically analyzes multi-object hallucination, revealing the hallucination behaviors and influencing factors when models process multiple objects. In addition, ROPE designs multiple task prompts, including Default Multi-Object, Student-Forcing, Teacher-Forcing, and Single-Object. The dataset is divided into four subsets, each considering different object category distributions: 1) Homogeneous: All test objects belong to the same category. 2) Heterogeneous: All test objects belong to different categories. 3) In-the-Wild: A mixed object category distribution, with test objects randomly selected and ordered. 4) Adversarial: After multiple repetitions of the same category, a different category object is introduced. The dataset is further divided into Seen and Unseen based on whether the model has encountered these images during instruction tuning. Table~\ref{rope} provides an overview of MLLM performance on the  ROPE.


\begin{table*}[t]
\small
\centering
\caption{Averaged accuracy of baselines on the \textit{In-the-Wild, \textit{Homogeneous}, and \textit{Heterogeneous} splits.} \vspace{3mm}}
\begin{threeparttable}
\begin{tabular}{l|rrr|rrr|rrr|rrr}
\hline
\multicolumn{1}{c|}{\multirow{2}[4]{*}{Model}}
& \multicolumn{3}{c|}{Default Multi-Object}
& \multicolumn{3}{c|}{Student-Forcing}            & \multicolumn{3}{c|}{Teacher-Forcing}            & \multicolumn{3}{c}{Single-Object}             \\
\cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule(r){8-10}\cmidrule(r){11-13}
% \multirow{-2}{*}{Models}
& Wild
& Hom.
& Het.
& Wild
& Hom.
& Het.
& Wild
& Hom.
& Het.
& Wild 
& Hom.
& Het. \\ 
\hline
% \vspace{0.5mm}
\multicolumn{13}{c}{\textit{Seen}} \\ 
 \hline
Yi-VL-6B~\cite{young2024yi} & 2.95         & 5.65         & 1.99         & 3.44 & 6.80 & 3.78 & 5.45 & 26.25 & 4.36 & 0.19 & 0.30 & 0.13 \\
 
Yi-VL-34B~\cite{young2024yi}                  & 8.50         & 15.35        & 3.33         & 8.97   & 16.30                  & 4.23   & 10.09                  & 19.75   & 4.94   & 0.22   & 2.60   & 0.13   \\
 
LLaVA-7B~\cite{liu2024visual}   & 31.29  & 67.50 & 8.00 & 31.28  & 67.25 & 11.22  & 31.49  & 92.15   & {\color[HTML]{434343} 12.37} & 35.32  & 62.35  & 17.37  \\
 
LLaVA-13B~\cite{liu2024visual}                  & 31.54        & 67.63        & 12.64        & 31.49                  & 73.25                  & 11.54                  & 34.97                  & 94.25   & 16.03                  & 43.13                  & 80.60                  & 23.91                  \\
 
LLaVA-34B~\cite{liu2024visual}                  & 39.95        & 85.75        & 18.85        & \textbf{52.75}             & \textbf{85.20}             & \textbf{33.91}             & \textbf{56.41}             & \textbf{95.81}            & \textbf{25.31}             & 55.05                  & \textbf{86.50}             & 18.97                  \\
\hline
 
Qwen VL~\cite{bai2023qwen}    & 2.73         & 6.60         & 1.03         & 6.25   & 16.00                  & 3.65   & 18.74                  & 71.50   & 5.45   & 8.73   & 16.05                  & 5.58   \\
 
Qwen VL-C~\cite{bai2023qwen}                  & 8.72         & 16.90        & 6.67         & 5.26   & 8.60   & 4.10   & 12.11                  & 47.75   & 8.08   & 25.99                  & 43.40                  & 13.21                  \\
 
CogVLM~\cite{wang2023cogvlm}     & 0.04         & 0.00         & 0.00         & 0.00   & 0.00   & 0.00   & 0.10   & 0.95    & 0.00   & 0.00   & 0.00   & 0.00   \\
 
CogVLM-G~\cite{wang2023cogvlm}   & 0.00         & 0.00         & 0.00         & 9.86   & 13.50                  & 6.79   & 22.64                  & 75.45   & 0.45   & 11.25                  & 22.65                  & 7.12   \\
 
CogVLM-C~\cite{wang2023cogvlm}   & 12.89        & 22.75        & 7.18         & 25.37                  & 43.63                  & 12.03                  & 28.25                  & 72.80   & 17.50                  & 30.16                  & 56.00                  & 16.35                  \\
\hline

LLaVA-7B~\cite{liu2024visual}      &- &- &-   & 9.16               & 16.40              & 5.51        &- &- &-                & 11.68              & 23.55              & 9.36                  \\
GLaMM~\cite{rasheed2024glamm}      &- &- &-   & 27.11        & 53.35        & 13.01        &- &- &-                 & \textbf{63.81}                  & 81.75                  & 53.40                  \\
GroundHOG~\cite{zhang2024groundhog}     &- &- &-   & 23.57              & 30.80              & 24.23        &- &- &-               & 44.80              & 43.10              & 38.97                  \\
\hline
IDEFICS~\cite{laurenccon2024obelics}    & 0.00         & 1.45         & 0.13         & 6.25   & 18.70                  & 0.64   & 17.37                  & 76.15   & 10.06                  & 4.62   & 0.00   & 0.32   \\
IDEFICS~\cite{laurenccon2024obelics}    & 0.00         & 1.45         & 0.13         & 6.25   & 18.70                  & 0.64   & 17.37                  & 76.15   & 10.06                  & 4.62   & 0.00   & 0.32   \\
CogVLM-2~\cite{wang2023cogvlm}   & 21.51        & 37.55        & 17.31        & 37.02                  & 70.85                  & 12.69                  & 37.10                  & 73.50   & 17.44                  & 21.16                  & 38.75                  & 13.65                  \\
MiniCPM-V~\cite{hu2024minicpm}    & 34.75        & 59.91        & 17.37        & 31.62                  & 62.80                  & 13.65                  & 32.16                  & 68.05   & 16.79                  & 27.42                  & 55.35                  & 16.92                  \\
GPT-4V~\cite{achiam2023gpt}     & 53.80        & 77.55        & 40.83       &- &- &-   &- &- &-                 & 55.89                  & 78.25                  & 41.03                  \\
 
GPT-4O~\cite{hurst2024gpt}      & \textbf{71.27} & \textbf{89.25} & \textbf{66.03} &- &- &-   &- &- &-               & 60.77             & 73.92                  & \textbf{54.31}             \\
\hline
LLaVA-7B~\cite{liu2024visual}      & 21.26 & 52.40 & 7.69 &- &- &-   &- &- &-               & 30.59             & 60.85                  & 12.69            \\
+OPERA~\cite{huang2024opera}      & 24.07 & 58.65 & 7.35 &- &- &-   &- &- &-               & 30.44             & 60.85                  & 13.27            \\
\cmidrule(r){1-13}

\multicolumn{13}{c}{\textit{Unseen}} \\    \hline
Yi-VL-6B~\cite{young2024yi} & 2.74         & 3.88         & 1.14         & 3.18   & 4.24   & 5.20   & 4.04   & 10.90   & 10.57                  & 0.14   & 0.45   & 0.08   \\
 
Yi-VL-34B~\cite{young2024yi}                  & 7.77         & 15.63        & 4.23         & 10.28                  & 18.04                  & 7.97   & 11.24                  & 22.49   & 12.03                  & 0.46   & 2.37   & 0.41   \\
 
LLaVA-7B~\cite{liu2024visual}   & 30.56        & 68.12        & 10.33        & 30.55                  & 68.16                  & 10.24                  & 31.89                  & 90.33   & {\color[HTML]{434343} 13.25} & 34.88                  & 64.41                  & 16.18                  \\
 
LLaVA-13B~\cite{liu2024visual}                  & 27.56        & 63.10        & 8.37         & 27.41                  & 63.10                  & 8.37   & 35.65                  & 91.09   & 14.80                  & 42.66                  & 71.92                  & 23.41                  \\
 
LLaVA-34B~\cite{liu2024visual}                  & 29.30        &79.43        & 17.72        & 29.45                  & \textbf{91.18}             & 14.39             & \textbf{37.40}             & \textbf{95.51}            & 17.92                  & 51.71                  & 77.88                  & 30.81                  \\
\hline
 
Qwen VL~\cite{bai2023qwen}    & 2.80         & 1.95         & 7.06         & 7.17   & 16.41                  & 4.15   & 10.34                  & 58.00   & 4.07   & 17.73                  & 31.22                  & 9.51   \\
 
Qwen VL-C~\cite{bai2023qwen}                  & 18.86        & 30.73        & 8.78         & 16.16                  & 27.80                  & 7.72   & 21.81                  & 58.00   & 11.14                  & 34.20                  & 57.31                  & 15.37                  \\
 
CogVLM~\cite{wang2023cogvlm}     & 0.03         & 0.00         & 0.00         & 0.00   & 0.00   & 0.00   & 0.00   & 0.15    & 0.00   & 0.00   & 0.00   & 0.00   \\
 
CogVLM-G~\cite{wang2023cogvlm}   & 0.00         & 0.00         & 0.00         & 8.20   & 1.47   & 5.77   & 23.82                  & 81.20   & 1.81   & 10.32                  & 10.74                  & 9.11   \\
 
CogVLM-C~\cite{wang2023cogvlm}   & 15.56        & 26.57        & 5.53         & 17.18                  & 41.27                  & 6.02   & 22.81                  & 56.04   & 6.67   & 30.56                  & 52.00                  & 13.50                  \\
\hline
LLaVA-7B~\cite{liu2024visual}      &- &- &-  & 7.59               & 12.12              & 4.88       &- &- &-                 & 12.71              & 22.49              & 8.46                  \\
GLaMM~\cite{rasheed2024glamm}      &- &- &-   & 29.11        & 54.53        & 14.23        &- &- &-                & \textbf{68.65}             & 77.06                  & 52.28                  \\
GroundHOG~\cite{zhang2024groundhog}      &- &- &-   & 23.11              & 24.69              & \textbf{26.26}       &- &- &-                & 40.73              & 30.37              & 38.13                  \\
\hline
IDEFICS~\cite{laurenccon2024obelics}    & 0.39         & 0.37         & 0.33         & 9.03   & 24.45                  & 2.68   & 24.80                  & 83.02   & 7.64   & 4.62   & 3.67   & 6.50   \\
CogVLM-2~\cite{wang2023cogvlm}   & 20.99        & 35.06        & 15.93        & 24.64                  & 38.04                  & 23.17                 & 26.74                  & 46.04   & \textbf{26.59}             & 11.13                  & 30.94                  & 5.77   \\
MiniCPM-V~\cite{hu2024minicpm}    & 32.96        & 59.92        & 16.60        & \textbf{31.77}             & 58.98                  & 14.15                  & 31.87                  & 60.98   & 16.34                  & 25.56                  & 47.76                  & 14.39                  \\
GPT-4V~\cite{achiam2023gpt}      & 45.46        & 63.12        & 34.17        &- &- &-   &- &- &-                 & 47.34                  & 64.94                  & 35.45                  \\
 
GPT-4O~\cite{hurst2024gpt}      & \textbf{63.27} & \textbf{80.29} & \textbf{54.47} &- &- &-   &- &- &-                & 63.45                  & \textbf{79.84}             & \textbf{53.74}           \\  
\hline
LLaVA-7B~\cite{liu2024visual}      & 13.96 & 31.88 & 3.98 &- &- &-  &- &- &-                & 26.95                  & 54.41             & 11.06         \\  
+OPERA~\cite{huang2024opera}      & 13.20 & 37.14 & 3.82 &- &- &-   &- &- &-                 & 27.90                  & 56.69             & 11.22         \\  
\hline
\end{tabular}
\end{threeparttable}
% \vspace*{2pt}
\label{rope}
\end{table*}



\subsubsection{CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark}
Visual Question Answering (VQA) is a crucial task in MLLMs, designed to test their understanding and reasoning capabilities across visual and textual data~\cite{antol2015vqa, mathew2021docvqa,masry2022chartqa,kafle2018dvqa,chen2021geoqa}. However, most existing VQA datasets primarily focus on English and a few major world languages, with images often being Western-centric. While recent efforts have expanded the linguistic coverage of VQA datasets, they still lack diversity in low-resource languages. Moreover, these datasets typically extend their language range through translation or other methods while keeping the images unchanged, leading to limited cultural representation. To address these limitations, David Romero et al.~\cite{wang2023llm} developed a new benchmark, CVQA, which aims to encompass rich linguistic and cultural diversity. This benchmark involves native speakers and cultural experts in the data collection process to ensure authenticity and inclusivity. Figure~\ref{cvqa_fig1} illustrates the scale and diversity of the CVQA benchmark, which includes 10,374 questions and languages from 30 different countries. This demonstrates how it covers a wide range of languages and cultures.


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.96\linewidth,height=0.48\linewidth]{fig/cvqa_fig1.png}
    \caption{Statistics of the CVQA Benchmark.~\cite{mathew2021docvqa}}
    \label{cvqa_fig1}
\end{figure*} 


Figure~\ref{cvqa_fig23} shows the performance of different models across various country-language pairs, including question-option pairs in both English and local languages. The blue line in the figure represents performance separated by continents. Despite differences in scale, it highlights the similar behavior of all models in most cases. This figure reveals the challenges models face when handling questions in local languages, as well as the performance variations across different regions and languages.




% 插入两张并排的图片
\begin{figure*}[t]
\centering  %图片全局居中
\subfigure[image 1]{
\label{cvqa_fig2}
\includegraphics[width=0.4\linewidth,height = 0.4\linewidth]{fig/cvqa_fig2.png}}
\subfigure[image 2]{
\label{cvqa_fid3}
\includegraphics[width=0.4\linewidth,height = 0.4\linewidth]{fig/cvqa_fig3.png}}
\caption{Model performance per Country-Language pair. The blue lines indicate separation by continent. All models show similar behaviour in the majority of cases, despite having different sizes.~\cite{mathew2021docvqa}}
\label{cvqa_fig23}
\end{figure*}


Table~\ref{cvqa_tab1} shows the average performance of different MLLMs on the CVQA dataset using English prompts (EN) and local language prompts (LOC). These results indicate that even the best-performing open models, such as LLaVA-1.5-7B, significantly lag behind closed models on CVQA. Furthermore, their performance is poorer with local language prompts, highlighting the challenges models face when processing non-English prompts.


\begin{table*}[htbp]
 \setlength{\tabcolsep}{0pt}
 \renewcommand\arraystretch{1.4}
\centering
\belowrulesep=0pt
\aboverulesep=0pt
\caption{Average performance of MLLMs on our CVQA dataset with English prompts~(EN) and local language prompts~(LOC).~\cite{mathew2021docvqa}}
% \vspace{0.05in}
\resizebox{\textwidth}{!}{
\begin{tabular}{cc|cc|cc|cc|cc|cc|cc|ccccc}
\hline
\multicolumn{2}{c|}{\textbf{LLaVA-1.5-7B}~\cite{liu2024visual}} &\multicolumn{2}{c|}{\textbf{M-CLIP}~\cite{chen2023mclip}} & \multicolumn{2}{c|}{\textbf{CLIP}~\cite{radford2021learning}} & 
\multicolumn{2}{c|}{\textbf{mBLIP-mT0}~\cite{geigle2023mblip}} & \multicolumn{2}{c|}{\textbf{mBLIP-BLOOMZ}~\cite{geigle2023mblip}} & \multicolumn{2}{c|}{\textbf{InstructBLIP}~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels}} &  \multicolumn{2}{c|}{\textbf{Gemini-1.5-Flash}~\cite{team2024gemini}} & \multicolumn{2}{c}{\textbf{GPT-4o}~\cite{hurst2024gpt}} \\
\cmidrule(r){0-2}\cmidrule(r){2-4}\cmidrule(r){4-6}\cmidrule(r){6-8}\cmidrule(r){8-10}\cmidrule(r){10-12}\cmidrule(r){12-14}\cmidrule(r){14-16}
\textbf{EN} & \textbf{LOC} &
\textbf{EN} & \textbf{LOC} &
 \textbf{EN} & \textbf{LOC} & \textbf{EN} & \textbf{LOC} & \textbf{EN} & \textbf{LOC} & \textbf{EN} & \textbf{LOC} & \textbf{EN} & \textbf{LOC} & \textbf{EN} & \textbf{LOC} \\
\hline
49.6 & 35.5 & 38.0 & 33.7 & 42.7 & 30.6 & 31.3 & 30.9 & 39.3 & 32.7 & 49.0 & 31.9 & 66.9 & 68.5 & 75.4 & 74.3 \\
\hline
\end{tabular}
}
\label{cvqa_tab1}
\end{table*}

Table~\ref{cvqa_tab2} compares the performance of LLaVA-1.5-7B and InstructBLIP on CVQA and other established English VQA benchmarks. The results show that while LLaVA-1.5-7B performs better on other English VQA benchmarks, it still faces challenges on CVQA, highlighting the difficulty of culturally specific questions in CVQA.

\begin{table*}[htbp]
\small
\centering
\belowrulesep=0pt
\aboverulesep=0pt
\caption{LLaVA-1.5-7B and InstructBLIP results on various VQA datasets.~\cite{mathew2021docvqa}}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|c|c|c|c|c|c|cccc}
\hline
\textbf{Model} & \textbf{VQAv2} & \textbf{GQA} & \textbf{VizWiz} & \textbf{SciQA-IMG} & \textbf{TextVQA} & \textbf{CVQA~(EN)} & \textbf{CVQA~(LOC)}\\
\hline
LLaVA-1.5-7B~\cite{liu2024visual}& 78.5 & 62.0 & 50.0 & 66.8 & 58.2 & 48.9 & 36.5\\
InstructBLIP~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} & - & 49.2 & 34.5 & 60.5 & 50.1& 47.8 & 32.7\\
\hline
\end{tabular}}
\label{cvqa_tab2}
\end{table*}


Table~\ref{cvqa_tab3} presents the performance of models across 10 categories in CVQA. It shows that models achieve the highest accuracy in the "People" category, while the accuracy in the "Food" and "Pop Culture" categories is lower with local language prompts. This indicates that the diversity of food and pop culture across different cultures presents a challenge for the generalization of MLLMs.


\begin{table*}[htbp]
\centering
\caption{Accuracy of models across categories.~\cite{mathew2021docvqa}}
\resizebox{1\textwidth}{!}{
\begin{tabular}{@{}l|cc|cc|cc|cc|cc|cc@{}}
\hline
\multirow{2}{*}{\textbf{Categories}} & \multicolumn{2}{c|}{\textbf{LLaVA-1.5-7B}~\cite{liu2024visual}} & \multicolumn{2}{c|}{\textbf{M-CLIP}~\cite{chen2023mclip}} & \multicolumn{2}{c|}{\textbf{CLIP}~\cite{radford2021learning}} & \multicolumn{2}{c|}{\textbf{mBLIP-mT0}~\cite{geigle2023mblip}} & \multicolumn{2}{c|}{\textbf{mBLIP-BLOOMZ}~\cite{geigle2023mblip}} & \multicolumn{2}{c}{\textbf{InstructBLIP}~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels}} \\
\cmidrule(r){2-3}\cmidrule(r){3-5}\cmidrule(r){5-7}\cmidrule(r){7-9}\cmidrule(r){9-11}\cmidrule(r){11-13}
 & \textbf{EN} & \textbf{LOC} & \textbf{EN} & \textbf{LOC} & \textbf{EN} & \textbf{LOC} & \textbf{EN} & \textbf{LOC} & \textbf{EN} & \textbf{LOC} & \textbf{EN} & \textbf{LOC} \\
\hline
Brands & \textbf{49.9} & 36.5 & 37.2& 35.7& 36.6& 29.7& 33.7 & 30.8 & 40.5 & 35.1 & 48.4 & 32.6 \\
Food & \textbf{45.4} & 31.9 & 34.5& 29.1& 39.2& 30.4& 28.1 & 27.6 & 37.7 & 29.8 & 44.4 & 30.6 \\
Geography & \textbf{47.1} & 38.2 & 37.1& 34.2& 41.8& 31.9& 30.6 & 31.6 & 35.0 & 32.3 & 45.3 & 33.2 \\
Objects & 51.8& 33.0& 39.4& 34.5 & 39.7& 25.4& 34.3 & 33.0 & 43.1 & 34.0 & \textbf{52.3}& 29.1 \\
People & 58.9& 38.1 & 45.0& 37.8& 46.8& 30.9& 35.3 & 34.7 & 46.3 & 36.7 & \textbf{59.8}& 34.0 \\
Plants \& Animals & \textbf{55.7}& 37.5 & 43.7& 32.0& 48.0& 27.2& 35.2 & 35.5 & 46.0 & 36.0 & 55.4 & 35.1 \\
Pop Culture & 44.5& 36.3 & 33.7& 31.5& \textbf{46.1}& 36.3& 28.8 & 29.9 & 35.7 & 30.7 & 45.1 & 34.6 \\
Sports & \textbf{50.7} & 39.1& 39.3& 33.3& 43.5& 32.4& 32.6 & 31.4 & 40.1 & 34.9 & 50.5 & 34.7 \\
Tradition & \textbf{50.4} & 35.8 & 37.0& 35.2& 41.9& 32.2& 31.6 & 31.5 & 39.0 & 32.2 & 47.9 & 30.8 \\
Vehicles & 50.6& 41.4& 39.5& 41.1& 44.6& 30.5& 35.6 & 33.9 & 42.0 & 34.0 & \textbf{55.0}& 33.0 \\
\hline
\end{tabular}
}
\label{cvqa_tab3}
\end{table*}


\subsubsection{II-Bench: Image Implication Understanding Benchmark}
Images often contain rich emotional and cultural narratives, and understanding their meaning and exploring the human emotions and cultural context they reflect requires attention to detail~\cite{bubeck2023sparks,achiam2023gpt,wachowiak2023does}. While MLLMs have made significant progress in understanding and generating cross-modal content, achieving new breakthroughs in benchmarks like image captioning~\cite{lin2014microsoft,sharma2018conceptual,sidorov2020textcaps,gurari2020captioning,pont2020connecting,agrawal2019nocaps} and visual question answering~\cite{antol2015vqa, mathew2021docvqa, masry2022chartqa,kafle2018dvqa,chen2021geoqa}, there has been insufficient exploration of their higher-order perceptual abilities. Ziqiang Liu et al.~\cite{liu2024ii} introduced a new benchmark, II-Bench, designed to evaluate MLLMs' ability to understand and reason about the complex implicit meanings in images, addressing the gap in existing benchmarks for assessing higher-order perceptual abilities in MLLMs. II-Bench includes 1,222 images across six different domains: life, art, society, psychology, environment, and others. The images consist of various types, including illustrations, memes, posters, comics, logos, and paintings. Each image is accompanied by one to three multiple-choice questions, totaling 1,434 questions. Of these, 1,399 questions are used to construct the test set, and 35 questions are used for the development and validation sets.

Table~\ref{iibench_tab} presents the overall results of different MLLMs and human participants on the II-Bench benchmark. It shows model performance across various domains, such as life, art, society, psychology, and environment, as well as across different emotional categories (positive, neutral, and negative). The table lists the average and best accuracies for multiple open-source and closed-source MLLMs, alongside the performance of human participants.

\begin{table*}[!thp]
\renewcommand\arraystretch{1.2}
\centering
\caption{Overall results of different MLLMs and humans on different domains and emotions.~\cite{liu2024ii}}
\begin{tabular}{l|c|cccccc|ccc}
\hline
\textbf{Models} & \textbf{Overall} & \textbf{Life} & 
\textbf{Art} & \textbf{Society} & \textbf{Psy.} & \textbf{Env.} & \textbf{Others} & \textbf{Positive} & \textbf{Neutral} & \textbf{Negative} \\
 {} & (1,399) & (585) & (85) & (461) & (152) & (51) & (65) & (196) & (789) & (414) \\
\hline
\multicolumn{11}{c}{\textit{Open-source Models}} \\ 
\hline
InstructBLIP-T5-XL~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} & 47.3 & 45.6 & 48.2 & 48.8 & 44.7 & 52.9 & 50.8 & 46.9 & 48.3 & 45.4 \\
BLIP-2 FLAN-T5-XL~\cite{li2023blip2} & 52.8 & 53.0 & 58.8 & 52.5 & 42.8 & 64.7 & 58.5 & 56.1 & 52.9 & 51.0 \\
mPLUGw-OWL2~\cite{ye2023mplugowl2} & 53.2 & 54.0 & 56.5 & 50.5 & 52.0 & 60.8 & 56.9 & 55.6 & 52.6 & 53.1 \\
Qwen-VL-Chat~\cite{bai2023qwen}  & 53.4 & 53.2 & 49.4 & 52.1 & 50.0 & 60.8 & 72.3 & 56.1 & 52.6 & 53.6 \\
InstructBLIP-T5-XXL~\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} & 56.7 & 56.2 & 58.8 & 58.6 & 45.4 & 64.7 & 64.6 & 63.3 & 56.1 & 54.6 \\
Mantis-8B-siglip-Llama3 & 57.5 & 56.8 & 61.2 & 57.5 & 53.9 & 64.7 & 61.5 & 59.2 & 58.0 & 55.6 \\
BLIP-2 FLAN-T5-XXL~\cite{li2023blip2} & 57.8 & 57.1 & 63.5 & 57.0 & 53.3 & 66.7 & 66.2 & 67.9 & 57.2 & 54.3 \\
DeepSeek-VL-Chat-7B~\cite{lu2024deepseekvl} & 60.3 & 59.0 & 58.8 & 58.4 & 61.8 & 68.6 & 76.9 & 65.8 & 60.1 & 58.0 \\
Yi-VL-6B-Chat~\cite{young2024yi} & 61.3 & 60.9 & 63.5 & 60.7 & 56.6 & 66.7 & 72.3 & 61.7 & 61.7 & 60.1 \\
InternLM-XComposer2-VL~\cite{dong2024internlmxcomposer2} & 62.1 & 61.7 & 62.4 & 62.3 & 58.6 & 70.6 & 66.2 & 65.8 & 63.0 & 58.7 \\
InternVL-Chat-1.5~\cite{chen2024far} & 66.3 & 63.6 & 65.9 & 68.5 & 65.8 & 64.7 & 76.9 & 73.5 & 65.4 & 64.5 \\
Idefics2-8B~\cite{laurenccon2024obelics} & 67.7 & 67.2 & \textbf{74.1} & 67.7 & 62.5 & 74.5 & 70.8 & 68.9 & 67.0 & 68.4 \\
Yi-VL-34B-Chat~\cite{young2024yi} & 67.9 & 67.5 & 70.6 & 67.7 & 63.8 & 70.6 & 76.9 & 74.0 & 68.2 & 64.5 \\
MiniCPM-Llama3-2.5~\cite{hu2024minicpm} & 69.4 & 68.4 & 71.8 & 69.4 & 64.5 & \textbf{80.4} & 78.5 & 75.0 & 69.3 & 66.9\\
CogVLM2-Llama3-Chat~\cite{hong2024cogvlm2} & 70.3 & 68.9 & 68.2 & 70.9 & 67.8 & 72.5 & \textbf{86.2} & 69.9 & 71.1 & 69.1 \\
LLaVA-1.6-34B~\cite{liu2024visual} & \textbf{73.8} & \textbf{73.8} & 71.8 & \textbf{73.3} & \textbf{71.1} & 78.4 & 81.5 & \textbf{79.1} & \textbf{72.9} & \textbf{72.9} \\
\hline
\multicolumn{11}{c}{\textit{Closed-source Models}} \\ 
\hline
GPT-4V~\cite{achiam2023gpt} & 65.9 & 65.0 & 69.4 & 65.3 & 59.9 & 76.5 & 80.0 & 69.4 & 66.0 & 64.0 \\
GPT-4o~\cite{hurst2024gpt} & 72.6 & 72.5 & 72.9 & 73.3 & 68.4 & 76.5 & 75.4 & 78.6 & 71.2 & 72.5 \\
Gemini-1.5 Pro~\cite{geminiteam2024} & 73.9 & 73.7 & \textbf{74.1} & 74.4 & 63.2 & \textbf{80.4} & 83.1 & \textbf{80.1} & 70.8 & \textbf{75.4} \\
Qwen-VL-MAX~\cite{bai2023qwen} & \textbf{74.8} & \textbf{74.7} & 71.8 & \textbf{74.6} & \textbf{73.0} & 76.5 & \textbf{84.6} & \textbf{80.1} & \textbf{74.5} & 72.9 \\ 
\hline
\multicolumn{11}{c}{\textit{Humans}} \\ 
\hline
Human\_avg~\cite{liu2024ii} & 90.3 & 90.0 & 88.2 & 91.4 & 86.6 & 96.1 & 92.3 & 84.7 & 89.1 & 92.2  \\ 
Human\_best~\cite{liu2024ii} & \textbf{98.2} & \textbf{97.9} & \textbf{98.8} & \textbf{98.3} & \textbf{97.4} & \textbf{100.0} & \textbf{100.0} & \textbf{98.0} & \textbf{98.0} & \textbf{98.8} \\ 
\hline
\end{tabular}%
\label{iibench_tab}
\end{table*}


\subsubsection{ConBench: MLLMs Answer Consistency Evaluation Benchmark}
MLLMs have made rapid progress in visual information perception and reasoning. Although MLLMs are capable of generating high-quality task prompt responses, simply modifying the prompt can lead to contradictory answers, even when the correct answer is provided. Specifically, under different prompt space sizes, these models lack consistency in answers to the same knowledge point, which significantly undermines trust in these models~\cite{li2023benchmarking,lin2023generating}. To ensure that MLLMs can predict correct and consistent answers when faced with various query formats, Yuan Zhang et al.~\cite{zhang2024unveiling} proposed a multimodal benchmark tool, ConBench, designed to comprehensively assess the consistency of MLLMs—specifically, their ability to provide the same answer to the same knowledge point across different query formats.

ConBench evaluates MLLMs by offering a diverse set of question formats, including true/false questions, multiple-choice questions, and limited visual question answering (VQA) problems. It also introduces two multidimensional evaluation metrics: 1)Discriminative Domain Evaluation Metric (ConScore[D]): Assesses consistency based on the accuracy of the model's answers to discriminative questions. 2)Generative Domain Evaluation Metric (ConScore[C]): Evaluates consistency by comparing the coherence between the model-generated captions and the discriminative answers.

The specific structure of ConBench is shown in Figure ~\ref{conbench_fig}, providing an overview of the 19 evaluation categories in ConBench. These categories are distributed across three core capabilities: Sensation, Cognition, and Knowledge. The benchmark comprehensively covers tasks of varying difficulty levels, thereby assessing the performance of MLLMs across different aspects.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth,height=0.65\linewidth]{fig/ConBench_fig1.pdf}
    \caption{Overview of 19 evaluation detailed categories in ConBench.~\cite{zhang2024unveiling}}
    \label{conbench_fig}
\end{figure} 


Table~\ref{conbench_tab1} presents the performance evaluation results of different MLLMs on ConBench. These results are based on ConScore[D], which evaluates the correctness of the model's answers to discriminative questions. The table includes three types of questions: True/False (T), Multiple-Choice (C), and Limited Visual Question Answering (VQA) (V). It also shows the models' performance across the three core capabilities: Sensation, Cognition, and Knowledge.


\begin{table*}[t]
  \renewcommand\arraystretch{1.2}
    \centering
	\caption{\textbf{Evaluation[D] of mainstreams series of MLLMs on ConBench.} The detailed results of the Sensation, Cognition, and Knowledge core capabilities are listed below. T, C, and V represent true-false, multiple-choice, and limited VQA questions, respectively. The ranking can be found below the respective numbers. $\dagger$: \scriptsize{Due to safety considerations, GPT-4V declined to answer the celebrity category.}~\cite{zhang2024unveiling}}
	\label{conbench_tab1}
	\scriptsize
        \begin{center}
        \scalebox{1.05}{
	\begin{tabular}{l|c|cccc|cccc|cccc}
	    \shline
	    \multirow{2}{*}{\footnotesize{Method}} & \multirow{2}{*}{\footnotesize{ConScore[D]}} & \multicolumn{4}{c|}{\footnotesize{Sensation}} & \multicolumn{4}{c|}{\footnotesize{Cognition}} & \multicolumn{4}{c}{\footnotesize{Knowledge}}  \\
        \cmidrule(r){3-14}
          & & \multicolumn{1}{c}{\footnotesize{T}} & \multicolumn{1}{c}{\footnotesize{C}} & \multicolumn{1}{c}{\footnotesize{V}} & Con & \multicolumn{1}{c}{\footnotesize{T}} & \multicolumn{1}{c}{\footnotesize{C}} & \multicolumn{1}{c}{\footnotesize{V}} & Con & \multicolumn{1}{c}{\footnotesize{T}} & \multicolumn{1}{c}{\footnotesize{C}} & \multicolumn{1}{c}{\footnotesize{V}} & Con \\
	    \shline
	    \multicolumn{14}{c}{\textit{Closed-source Vision Language Models}}\\
        \hline
            GPT-4V$^\dagger$~\cite{achiam2023gpt}   & $29.20$ & $80.4$ & $79.0$ & $61.7$ & $48.3$ & $68.8$ & $53.2$ & $39.9$ & $20.4$ & $63.1$ & $57.2$ & $30.0$ & $14.2$ \\
            GPT-4-Omni~\cite{hurst2024gpt}  & $35.70$ & $89.2$ & $79.4$ & $64.4$ & $55.0$ & $71.8$ & $62.8$ & $44.9$ & $27.8$ & $64.7$ & $61.7$ & $39.7$ & $23.3$ \\
    	Gemini-Pro-Vision~\cite{geminiteam2024geminifamilyhighlycapable}  & $25.00$ & $85.2$ & $60.7$ & $63.4$ & $39.3$ & $71.8$ & $45.0$ & $44.2$ & $15.1$ & $65.0$ & $51.4$ & $39.7$ & $15.8$ \\
            Gemini-Ultra-Vision~\cite{geminiteam2024geminifamilyhighlycapable}  & $33.10$ & $78.9$ & $78.6$ & $66.3$ & $50.3$ & $68.1$ & $58.5$ & $47.9$ & $28.5$ & $62.9$ & $62.2$ & $44.7$ & $19.7$ \\
            Qwen-VL-Plus~\cite{bai2023qwen}  & $28.10$ & $82.7$ & $74.9$ & $60.4$ & $45.0$ & $64.2$ & $41.7$ & $30.8$ & $16.3$ & $63.6$ & $54.2$ & $33.3$ & $15.8$ \\
            Qwen-VL-Max~\cite{bai2023qwen} & $\mathbf{37.00}$ & $86.4$ & $80.7$ & $65.4$ & $\mathbf{56.3}$ & $72.9$ & $51.4$ & $51.3$ & $28.1$ & $68.3$ & $58.6$ & $38.9$ & $\mathbf{24.2}$ \\
	    \hline
	    \multicolumn{14}{c}{\textit{7B Vision Language Models}}\\
        \hline
            LLaVA-v1.5-7B~\cite{liu2024visual}  & $16.60$ & $79.3$ & $56.8$ & $44.3$ & $28.3$ & $51.4$ & $33.5$ & $15.8$ & $4.7$ & $61.7$ & $44.4$ & $16.9$ & $7.8$ \\
            Qwen-VL-Chat~\cite{bai2023qwen}  & $26.40$ & $81.0$ & $79.6$ & $54.2$ & $39.0$ & $55.0$ & $46.3$ & $33.2$ & $13.5$ & $60.3$ & $54.2$ & $28.9$ & $14.7$ \\
	    \hline
	    \multicolumn{14}{c}{$\sim$ \textit{13B Vision Language Models}}\\
        \hline
            LLaVA-v1.5-13B~\cite{liu2024visual}  & $24.00$ & $82.9$ & $77.1$ & $49.6$ & $39.5$ & $53.6$ & $37.8$ & $20.1$ & $10.4$ & $65.6$ & $50.3$ & $17.2$ & $9.7$ \\
            MiniGemini-13B~\cite{li2024minigemini}  & $21.80$ & $81.9$ & $69.7$ & $52.8$ & $39.3$ & $51.9$ & $38.2$ & $21.1$ & $6.9$ & $52.8$ & $36.7$ & $17.5$ & $9.2$ \\
            InternVL-v1.5-26B~\cite{chen2024far} & $31.40$ & $85.6$ & $84.8$ & $65.0$ & $54.3$ & $59.7$ & $58.6$ & $44.4$ & $19.4$ & $58.1$ & $55.8$ & $25.3$ & $12.2$ \\
	    \hline
	    \multicolumn{14}{c}{$\sim$ \textit{34B Vision Language Models}}\\
        \hline
            LLaVA-NeXT-34B~\cite{li2024llavanextinter} & $27.70$ & $82.4$ & $81.7$ & $55.6$ & $43.6$ & $50.7$ & $47.5$ & $25.6$ & $9.9$ & $60.4$ & $56.1$ & $27.8$ & $12.8$ \\
            MiniGemini-34B~\cite{li2024minigemini} & $23.00$ & $80.8$ & $76.8$ & $48.2$ & $39.7$ & $36.9$ & $30.7$ & $18.9$ & $6.0$ & $58.1$ & $42.3$ & $20.8$ & $8.2$ \\
            InternVL-v1.2P-40B~\cite{chen2024internvl} & $34.70$ & $83.7$ & $83.2$ & $66.6$ & $53.4$ & $74.2$ & $67.6$ & $57.1$ & $\mathbf{34.9}$ & $72.2$ & $58.3$ & $28.6$ & $13.6$ \\

	    \shline
	\end{tabular}
    }
         \end{center}
\end{table*}


Table~\ref{conbench_tab2} further evaluates the consistency between the captions generated by MLLMs and the discriminative answers (ConScore[C]). This includes the overall ConScore[C], as well as consistency scores for the three question types: True/False (T), Multiple-Choice (C), and Limited Visual Question Answering (VQA) (V).


\newcommand{\fg}[1]{\mathbf{\mathcolor{ForestGreen}{#1}}}
\newcommand{\fr}[1]{\mathbf{\mathcolor{Forestred}{#1}}}
\begin{table*}[t]
\renewcommand\arraystretch{1.2}
    \centering
	\caption{\textbf{Evaluation of Consistency between caption and three discriminative types of answer on ConBench.} The Con[$X$] is the Consistency ratio between discriminative answer type $X$ and caption. The "ordered" represents whether Con[T] $<$ Con[C] $<$ Con[V] is in its line.~\cite{zhang2024unveiling}}
	\label{conbench_tab2}
	% \footnotesize
	\scriptsize
	%\normalsize
    \begin{center}
    \scalebox{1.2}{
	\begin{tabular}{l|c|c|cccc}
	    \shline
        \footnotesize{Method} & \footnotesize{ConScore[C]} & \footnotesize{Con[T]} & \footnotesize{Con[C]} & \footnotesize{Con[V]} & \footnotesize{Ordered}\\
	    \shline
	    \multicolumn{6}{c}{\textit{Closed-source Vision Language Models}}\\ \hline
            GPT-4V~\cite{achiam2023gpt}  & 55.6 & $51.20$ & $56.50$ & $59.10$ & Y  \\
            GPT-4-Omni~\cite{hurst2024gpt}  & $\mathbf{62.2}$ & $58.00$ & $62.50$ & $66.10$ & Y \\
            Gemini-Pro-Vision~\cite{geminiteam2024geminifamilyhighlycapable}  & $48.4$ & $43.30$ & $45.20$ & $56.80$ & Y \\
            Gemini-Ultra-Vision~\cite{geminiteam2024geminifamilyhighlycapable} & $54.6$ & $47.80$ & $55.20$ & $60.70$ & Y \\
            Qwen-VL-Plus~\cite{bai2023qwen}  & $50.2$ & $47.10$ & $49.10$ & $54.30$ & Y \\
            Qwen-VL-Max~\cite{bai2023qwen}  & $ 58.4$ & $54.30$ & $58.00$ & $62.90$ & Y \\
	    \hline
    
	    \multicolumn{6}{c}{\textit{7B Vision Language Models}}\\     \hline
            LLaVA-v1.5-7B~\cite{liu2024visual}  & $38.4$ & $39.20$ & $36.60$ & $39.50$ & N\\
            Qwen-VL-Chat~\cite{bai2023qwen}    & $48.0$ & $42.00$ & $50.80$ & $51.30$ & Y\\
	    \hline
	    \multicolumn{6}{c}{$\sim$ \textit{13B Vision Language Models}}\\     \hline
            LLaVA-v1.5-13B~\cite{liu2024visual}   & $44.4$ & $41.50$ & $45.80$& $46.00$ & Y\\
            MiniGemini-13B~\cite{li2024minigemini}  & $41.7$ & $38.80$ & $42.90$ & $43.30$ & Y\\
            InternVL-v1.5-26B~\cite{chen2024far} & $50.9$ & $44.50$ & $53.90$& $54.20$ & Y\\
	    \hline
	    \multicolumn{6}{c}{$\sim$ \textit{34B Vision Language Models}}\\    \hline
            LLaVA-NeXT-34B & $48.3$ & $46.00$ & $52.20$ & $46.80$ & N\\
            MiniGemini-34B~\cite{li2024minigemini}  & $49.6$ & $56.80$ & $48.00$ & $44.10$ & N\\
            InternVL-v1.2P-40B~\cite{chen2024internvl} & $53.7$ & $49.80$ & $55.50$ & $55.80$ & Y\\

	    \shline
	\end{tabular}
    }
         \end{center}
\end{table*}


\subsubsection{COMPBENCH: Comparative Reasoning Benchmark}
The ability to compare objects, scenes, or situations is crucial for decision-making and problem-solving in everyday life~\cite{masry2022chartqa,hudson2019gqa,lu2022learn}. Although this ability is widespread in human cognition, it has not been fully explored in the field of Artificial General Intelligence (AGI). Jihyung Kil et al.~\cite{kil2024compbench} proposed a benchmark, COMPBENCH, designed to evaluate the comparative reasoning ability of MLLMs, as outlined in Table~\ref{COMPBENCH_tab1}. COMPBENCH questions are carefully crafted to distinguish relative features between two images, testing the models' performance across eight different comparative dimensions by providing image pairs and related questions. Table~\ref{COMPBENCH_tab2} presents the performance of recent MLLMs on the COMPBENCH benchmark.


\begin{table}[t]
\renewcommand\arraystretch{1.2}
\caption{Overall statistics of COMPBENCH.~\cite{kil2024compbench}}
\setlength{\tabcolsep}{3mm}{
\begin{tabular}{cccc}
\toprule
\multirow{2}{*}{Relativity} & \multirow{2}{*}{Dataset} &\multirow{2}{*}{Domain}  &\multirow{2}{*}{samples}\\ \\
\midrule
\multirow{5}{*}{Attribute} & MIT-States & Open & 0.2K \\
& Fashionpedia & Fashion & 2.4K \\
& VAW & Open & 0.9K \\
& CUB-200-2011 & Bird & 0.9K \\
& Wildfish++ & Fish & 0.9K \\
\cmidrule{1-4}
\multirow{2}{*}{Existence} & MagicBrush & Open & 0.9K \\
& Spot-the-diff & Outdoor Scene & 1.2K \\
\cmidrule{1-4}
\multirow{2}{*}{State} & MIT-States & Open & 0.6K \\
& VAW & Open & 0.5K \\
\cmidrule{1-4}
\multirow{2}{*}{Emotion} & CelebA & Face & 1.5K \\
& FER-2013 & Face & 3.8K \\
\cmidrule{1-4}
\multirow{2}{*}{Temporality} & SoccerNet & Sport & 8.3K \\
& CompCars & Car & 5K \\
\cmidrule{1-4}
Spatiality & NYU-Depth V2 & Indoor Scene & 1.9K \\
\cmidrule{1-4}
Quantity & VQAv2 & Open & 9.8K \\
\cmidrule{1-4}
Quality & Q-Bench2 & Open & 1K \\
\cmidrule{1-4}
Total & - & - & 39.8K \\
\bottomrule
\end{tabular}}
\label{COMPBENCH_tab1}
\end{table}



\begin{table*}[t]
\renewcommand\arraystretch{1.2}
\centering
\caption{Overall results on COMPBENCH test split. Evaluating four leading MLLMs across eight relative comparisons spanning sixteen tasks.~\cite{kil2024compbench}
}
\scalebox{0.9}{
\begin{tabular}{l|ccccc|cc|cc|cc|cc|c|c|c|c}
\hline
\multirow{2}{*}{Model} &
\multicolumn{5}{c|}{Attribute} &
\multicolumn{2}{c|}{Exist.} &
\multicolumn{2}{c|}{State} &
\multicolumn{2}{c|}{Emot.} &
\multicolumn{2}{c|}{Temp.} &
\multicolumn{1}{c|}{Spat.} &
Quan. &
Qual. & 
\multirow{2}{*}{Avg} \\

 \cmidrule(r){2-17}
%\cmidrule(r){2-6} \cmidrule(r){7-8} \cmidrule(r){9-10} \cmidrule(r){11-12} \cmidrule(r){13-14} \cmidrule(r){15-15} \cmidrule(r){16-16} \cmidrule(r){17-17}
& ST & FA & VA & CU & WF & MB & SD & ST & VA & CE & FE & SN & CC & ND & VQ & QB \\
%& MST & FAS & VAW & CUB & WFS & MBR & SPD & MST & VAW & CEA & FER & SCN & COC & NYD & VQA & QBE \\
\hline
GPT-4V~\cite{achiam2023gpt}                  & \textbf{91.8} & \textbf{89.0}    & 76.9 & 71.4 & \textbf{72.1}     & \textbf{58.3} & 41.9      & \textbf{92.2} & \textbf{87.8} & 91.8   & 83.4     & \textbf{71.4} & \textbf{73.7} & 56.1  & \textbf{63.8} & \textbf{73.0}     & \textbf{74.7}                 \\ 
Gemini1.0-Pro~\cite{geminiteam2024geminifamilyhighlycapable}              & 71.9 & 76.3    & 69.3 & 59.9 & 54.9     & 53.7 & \textbf{53.0}      & 81.8 & 70.7 & 60.6   & 71.2     & 55.1 & 58.2 & 56.6  & 54.6 & 59.5     & 63.0                 \\
LLaVA-1.6~\cite{liu2024visual}                & 84.9 & 72.1    & \textbf{77.7} & \textbf{72.6} & 68.7     & 26.5 & 20.7      & 89.7 & 79.3 & \textbf{96.2}   & \textbf{83.5}     & 51.0 & 50.2 & \textbf{67.2}  & 50.1 & 64.8     & 66.0                 \\
VILA-1.5~\cite{lin2024vila}                 & 69.9 & 66.2    & 70.9 & 55.9 & 52.0     & 49.5 & 36.8      & 71.9 & 74.5 & 57.1   & 55.6     & 51.1 & 52.9 & 51.8  & 47.7 & 64.8     & 58.0                 \\
Chance level~\cite{kil2024compbench} & 50.0 & 50.0    & 50.0 & 50.0 & 50.0     & 8.6 & 9.7      & 50.0 & 50.0 & 50.0   & 50.0     & 50.0 & 50.0 & 50.0  & 33.3 & 37.4     & 43.1 \\

\hline
\end{tabular}
}
%\vspace{-12pt}
\label{COMPBENCH_tab2}
\end{table*}


\subsubsection{Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs}
Similarly, in the context of the hallucination problem faced by MLLMs in visual-language understanding and generation tasks~\cite{rohrbach2018object,dai2022plausible,li2023evaluating,zhang2024groundhog,zhai2023halle,liu2023mitigating,you2023ferret,zhou2023analyzing,wang2023llm}, Peng Ding et al.~\cite{ding2024hallu} pointed out that previous studies have mainly focused on evaluating hallucinations on standard, undisturbed benchmarks, neglecting the prevalent interference inputs in the real world. This is crucial for a comprehensive evaluation of hallucinations in MLLMs. They proposed the first benchmark designed to evaluate hallucinations in MLLMs under disturbed inputs, called Hallu-PI, which includes seven types of disturbed scenarios: noise, blur, weather, digits, image stitching, image cropping, and prompt misdirection.

Table~\ref{hallupi_tab1} presents the performance of MLLMs under four basic disturbance types (noise, blur, weather, and digits). The "Before/After" columns compare the performance before and after the perturbation, using the ACC+ (Accuracy+) and CHAIR (Hallucinated Object Occurrence Rate) metrics to measure the level of hallucinations in the models.

\begin{table*}[htbp]
\renewcommand\arraystretch{1.2}
  \centering
  \caption{The results under noise, blur, weather, and digital perturbations. Before/After means before/after perturbation.~\cite{ding2024hallu}}
    \begin{tabular}{l|cc|cc|cc|cc|cc}
   \hline
    \multicolumn{1}{c|}{\multirow{3}[4]{*}{Model}} & \multicolumn{2}{c|}{\multirow{2}[1]{*}{Before}} & \multicolumn{8}{c}{After} \\

     \cmidrule(r){4-11}
          & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{Noise} & \multicolumn{2}{c|}{Blur} & \multicolumn{2}{c|}{Weather } & \multicolumn{2}{c}{Digital } \\
\cmidrule{2-11}          & \multicolumn{1}{l}{ACC+} & \multicolumn{1}{l|}{CHAIR} & \multicolumn{1}{l}{ACC+} & \multicolumn{1}{l|}{CHAIR} & \multicolumn{1}{l}{ACC+} & \multicolumn{1}{l|}{CHAIR} & \multicolumn{1}{l}{ACC+} & \multicolumn{1}{l|}{CHAIR} & \multicolumn{1}{l}{ACC+} & \multicolumn{1}{l}{CHAIR} \\
    \hline
    CogVLM~\cite{wang2023cogvlm} & \textbf{49.0}  & 62.0    & \textbf{48.5}  & 68.2 & \textbf{47.4} & 68.6 & 42.8 & 67.9 & \textbf{48.4} & 69.8 \\
    Multi-GPT~\cite{achiam2023gpt} & 13.3  & \textbf{73.5}  & 9.6  & 73.6 & 12.8 & \textbf{76.1} & 11.2 & \textbf{73.4} & 9.2  & \textbf{77.8} \\
    LLaVA~\cite{liu2024visual} & 6.3   & 68.5  & 4.33  & 67.7 & 5.0     & 70.6 & 4.17  & 69.8 & 3.6  & 74.2 \\
    LLaVA1.5~\cite{liu2024visual} & 43.0    & 68.9  & 42.6 & 70.1 & 42.4 & 68.7 & 43.3 & 68.0  & 36.8 & 74.5 \\
    MiniGPT-4~\cite{zhu2023minigpt} & 16.0    & 72.4  & 15.8 & 70.2 & 15.9 & 72.1 & 14.5  & 72.6 & 13.8 & 73.9 \\
    MiniGPT4-v2~\cite{chen2023minigpt} & 28.3  & 72.1  & 26.7 & \textbf{74.7} & 28.8 & 74.0  & 28.2 & 72.8 & 27.1 & 74.9 \\
    mPLUG2~\cite{xu2023mplug} & 38.0    & 65.0    & 33.3 & 67.6 & 33.1 & 69.1 & 35.3 & 66.9 & 32.3 & 73.6 \\
    Gemini~\cite{team2023gemini} & 46.0    & 57.3  & 44.2 & 60.0   & 45.1 & 59.7   & 44.8 & 58.5 & 37.5  & 61.3 \\
    GPT-4V~\cite{achiam2023gpt} & 47.3  & 66.1  & 42.3 & 66.9 & 41.8 & 68.4 & \textbf{47.8} & 60.9 & 34.0    & 65.4 \\
    \hline
    \end{tabular}%
  \label{hallupi_tab1}%
\end{table*}%


Table~\ref{hallupi_tab2} focuses on the performance of MLLMs under three additional disturbance types in Hallu-PI: Concat, Cropping, and Prompt Mislead. The PI-Score (a comprehensive evaluation metric) is used to assess the overall performance of the models under these specific disturbance scenarios.

\begin{table}[htbp]
\renewcommand\arraystretch{1.2}
\small
  \centering
  \caption{The results under image concatenation, image cropping, and prompt misleading perturbations.~\cite{ding2024hallu}}
  \setlength{\tabcolsep}{1mm}{
    \begin{tabular}{l|cc|cc|cc}
    \hline
    \multicolumn{7}{c}{PI-Score} \\
    \hline
    \multicolumn{1}{c|}{\multirow{2}[2]{*}{MLLMs}} & \multicolumn{2}{c|}{Concat} & \multicolumn{2}{c|}{Cropping} & \multicolumn{2}{c}{Prompt Mislead} \\
     \cmidrule(r){2-7}
          & \multicolumn{1}{l}{Before} & \multicolumn{1}{l|}{After} & \multicolumn{1}{l}{Before} & \multicolumn{1}{l|}{After} & \multicolumn{1}{l}{Before} & \multicolumn{1}{l}{After} \\
    \hline
    CogVLM~\cite{wang2023cogvlm} & \textbf{45.4}  & 22.5 & 10.0    & 5.0     & 39.6  & 11.4 \\
    Multi-GPT~\cite{achiam2023gpt} & 8.3   & 15.0    & 11.7  & 0.0     & 18.9  & 7.2 \\
    LLaVA~\cite{liu2024visual} & 6.5   & 2.2   & 3.4   & 6.7   & 14.4  & 5.2 \\
    LLaVA1.5~\cite{liu2024visual} & 32.4  & 5.9   & 10.0    & 8.4   & 26.4  & 8.1 \\
    MiniGPT-4~\cite{zhu2023minigpt} & 8.9   & 5.9   & 10.0    & 8.4   & 18.5  & 7.0 \\
    MiniGPT-v2~\cite{chen2023minigpt} & 15.8  & 12.3  & 16.7  & 15.0    & 26.4  & 11.3 \\
    mPLUG2~\cite{xu2023mplug} & 25.7  & 18.9  & 10.0    & 8.3   & 29.7  & 15.7 \\
    InternLM~\cite{cai2024internlm2} & 38.3  & \textbf{37.3}  & 8.3   & 10.0    & 34.4  & 28.0 \\
    Qwen-VL~\cite{bai2023qwen} & 46.3  & 19.6  & 20.0    & 11.7  & 53.2  & 38.2 \\
    VisualGLM~\cite{du2022glm} & 6.8   & 0.6   & 34.0    & 0.0     & 21.2  & 11.3 \\
    Gemini~\cite{team2023gemini} & 44.6  & 21.4  & \textbf{45.0}    & 26.7  & 59.2  & 39.4 \\
    GPT-4V~\cite{achiam2023gpt} & 42.0    & 18.0    & 43.4  & \textbf{30.0}    & \textbf{61.4}  & \textbf{48.2} \\
    \hline
    \end{tabular}
    }
  \label{hallupi_tab2}%
\end{table}%


Table~\ref{hallupi_tab3} provides the performance details of MLLMs in generation tasks under the Concat, Cropping, and Prompt Mislead disturbances. The metrics CHAIR, Cover, Hal, and Cog are used to evaluate the models' performance in generation tasks. These metrics help us understand the models' accuracy and hallucination tendencies when generating descriptions that are consistent with the image content.


\begin{table*}[htbp]
\renewcommand\arraystretch{1.2}
\small
  \centering
  \caption{The results of generative task on image concatenation, cropping, and prompt misleading.~\cite{ding2024hallu}
 }
 \scalebox{0.95}{
    \begin{tabular}{l|cc|cc|cc|cc|cc|cc}
    \hline
    \multicolumn{1}{c|}{\multirow{3}[6]{*}{MLLMs}} & \multicolumn{8}{c|}{Image Concatenation}                      & \multicolumn{2}{c|}{Image Cropping} & \multicolumn{2}{c}{Prompt Misleading} \\
\cmidrule{2-13}          & \multicolumn{2}{c|}{ CHAIR } & \multicolumn{2}{c|}{ Cover } & \multicolumn{2}{c|}{Hal } & \multicolumn{2}{c|}{Cog} & \multicolumn{2}{c|}{Hal} & \multicolumn{2}{c}{Hal} \\
\cmidrule{2-13}          & \multicolumn{1}{l}{Before} & \multicolumn{1}{l|}{After} & \multicolumn{1}{l}{Before} & \multicolumn{1}{l|}{After} & \multicolumn{1}{l}{Before} & \multicolumn{1}{l|}{After} & \multicolumn{1}{l}{Before} & \multicolumn{1}{l|}{After} & \multicolumn{1}{l}{Before} & \multicolumn{1}{l|}{After} & \multicolumn{1}{l}{Before} & \multicolumn{1}{l}{After} \\
    \hline
    CogVLM~\cite{wang2023cogvlm} & 62.0  & 69.0  & 55.3  & 48.3  & 58.3  & 97.1  & 4.3   & 5.9   & 80.0  & 90.0  & 36.7  & \textbf{93.3}  \\
    Multi-GPT~\cite{achiam2023gpt} & 73.5  & \textbf{97.5}  & 22.5  & 2.0   & 96.7  & 86.3  & \textbf{30.8}  & \textbf{77.1}  & 76.7  & \textbf{100.0}  & 63.3  & \textbf{93.3}  \\
    LLaVA~\cite{liu2024visual} & 68.5  & 92.3  & 38.8  & 7.4   & 93.3  & 96.7  & 4.3   & 14.9  & \textbf{93.3}  & 86.7  & \textbf{66.7}  & \textbf{93.3}  \\
    LLaVA1.5~\cite{liu2024visual} & 68.9  & 76.1  & 43.8  & 25.0  & 78.3  & 96.3  & 3.4 & 5.7   & 86.7  & 90.0  & 63.3  & 90.0  \\
    MiniGPT-4~\cite{zhu2023minigpt} & 72.4  & 89.3  & 46.5  & 24.8  & 98.3  & 95.8  & 5.1   & 8.2   & 80.0  & 83.3  & 63.3  & \textbf{93.3}  \\
    MiniGPT-v2~\cite{chen2023minigpt} & 72.1  & 88.9  & 49.6  & 32.5  & \textbf{100.0}  & 96.7  & 4.0   & 7.1   & \textbf{93.3}  & 93.3  & 53.3  & \textbf{93.3}  \\
    mPLUG2~\cite{xu2023mplug} & 65.0  & 82.3  & 44.6  & 14.3  & 86.7  & 89.6  & 6.2   & 6.4   & \textbf{93.3}  & 96.7  & 46.7  & 80.0  \\
    InternLM~\cite{cai2024internlm2} & 58.4  & 79.2  & 16.3  & 9.5   & 71.7  & 62.5  & 18.8  & 16.7  & 86.7  & 86.7  & 43.3  & 63.3  \\
    Qwen-VL~\cite{bai2023qwen}  & 58.2  & 56.3  & 35.8  & 32.3  & 46.7  & 79.2  & 9.8   & 11.1  & 83.3  & 93.3  & 6.7   & 16.7 \\
    VisualGLM~\cite{du2022glm} & \textbf{76.9}  & 89.1  & 45.0  & 29.6  & \textbf{100.0}  & \textbf{99.2}  & 4.4   & 9.2   & \textbf{93.3}  & \textbf{100.0}  & 46.7  & 66.7  \\
    Gemini~\cite{team2023gemini} & 57.3  & 63.4  & 50.2  & 43.7  & 56.7  & 90.8  & 3.6   & 4.5   & 26.7  & 56.7  & 12.1  & 30.0  \\
    GPT-4V~\cite{achiam2023gpt} & 66.1  & 63.6  & \textbf{66.6}  & \textbf{53.6}  & 63.3  & 98.3  & 1.6   & 1.9   & 33.3  & 73.3  & 1.1   & 3.3  \\
    \hline
    \end{tabular}%
    }
  \label{hallupi_tab3}%
\end{table*}%




Table~\ref{hallupi_tab4} presents the performance of MLLMs in discriminative tasks under image stitching, cropping, and prompt misdirection disturbances. The metrics ACC, ACC+, and F1 are used to measure the models' accuracy in discriminative tasks. These data provide insights into the models' ability to handle disturbed inputs in discriminative tasks.


\begin{table*}[htbp]
\renewcommand\arraystretch{1.2}
\small
  \centering
  \caption{The results of discriminative task on image concatenation, cropping, and prompt misleading.~\cite{ding2024hallu} }
  \setlength{\tabcolsep}{1mm}{
    \begin{tabular}{l|ccc|ccc|ccc|ccc|ccc}   
    \hline
    \multicolumn{1}{c|}{\multirow{3}[3]{*}{MLLMs}} & \multicolumn{5}{c}{Image Concatenation} &       & \multicolumn{5}{c}{Image Cropping}    &       & \multicolumn{3}{c}{Prompt Misleading} \\
\cmidrule{2-16}          & \multicolumn{3}{c|}{Before} & \multicolumn{3}{c|}{After} & \multicolumn{3}{c|}{Before} & \multicolumn{3}{c|}{After} & \multicolumn{3}{c}{After} \\

\cmidrule{2-16}
          & \multicolumn{1}{l}{ACC} & \multicolumn{1}{l}{ACC+} & \multicolumn{1}{l|}{F1} & \multicolumn{1}{l}{ACC} & \multicolumn{1}{l}{ACC+} & \multicolumn{1}{l|}{F1} & \multicolumn{1}{l}{ACC} & \multicolumn{1}{l}{ACC+} & \multicolumn{1}{l|}{F1} & \multicolumn{1}{l}{ACC} & \multicolumn{1}{l}{ACC+} & \multicolumn{1}{l|}{F1} & \multicolumn{1}{l}{ACC} & \multicolumn{1}{l}{ACC+} & \multicolumn{1}{l}{F1} \\
    \hline
    CogVLM~\cite{wang2023cogvlm} &  69.9  & \textbf{49.0}  &  74.4  & \textbf{67.2}  & \textbf{42.0}  & \textbf{73.1}  & 50.0  & 0.0   &  66.7  & 50.0  & 0.0   & \textbf{66.7}  & 56.7  & 33.3  & 51.9  \\
    Multi-GPT~\cite{achiam2023gpt} & 46.8  & 13.3  & 52.4  & 41.8  & 16.3  & 48.9  & 48.3  & 0.0   & 65.2  & 45.0  & 0.0   & 62.1  & 28.3  & 6.7   & 41.1  \\
    LLava~\cite{liu2024visual} & 51.5  & 6.3   & 57.2  & 50.3  & 1.0   & 54.0  & 50.0  & 0.0   &  66.7  & 50.0  & 0.0   & \textbf{66.7}  & 1.7   & 0.0   & 3.2  \\
    LLava1.5~\cite{liu2024visual} & \textbf{70.5}  & {43.0}  & \textbf{76.1}  & 51.7  & 8.0   & 61.7  & 51.7  & 6.7   & 56.7  & 48.3  & 6.7   & 45.6  & 40.0  & 3.3   & 5.2  \\
    MiniGPT-4~\cite{zhu2023minigpt} & 43.0  & 16.0  & 47.6  & 30.2  & 7.7   & 25.4  & 38.3  & 0.0   & 55.4  & 30.0  & 0.0   & 46.2  & 20.0  & 0.0   & 33.4  \\
    MiniGPT-v2~\cite{chen2023minigpt} & 55.8  & 28.3  & 56.4  & 48.2  & 21.3  & 41.3  & 55.0  &  26.7  & 62.0  & 48.3  & \textbf{23.3}  & 47.5  & 88.3  & 80.0  & 88.8  \\
    mPLUG2~\cite{xu2023mplug} & 62.3  & 38.0  & 68.3  & 51.5  & 27.3  & 54.5  & 50.0  & 13.3  & 62.5  & 48.3  & 13.3  & 59.7  & 43.3  & 13.3  & 34.6  \\
    InternLM~\cite{cai2024internlm2} & 68.2  & 48.3  & 70.8  &  61.2  & 37.0  & 55.9  & 50.0  & 3.3   & 60.5  &  51.7  & 6.7   & 61.3  & 75.0  & 50.0  & 68.1  \\
    Qwen-VL~\cite{bai2023qwen}  & 62.5  & 39.3  & 62.0  & 55.7  & 18.3  & 52.4  &  58.3  & 23.3  & 65.7  & 48.3  & 16.7  & 53.7  & 93.3 & 86.7  &  92.9  \\
    VisualGLM~\cite{du2022glm} & 46.3  & 5.3   & 50.9  & 43.3  & 0.3   & 45.0  & 50.0  & 0.0   &  66.7  & 50.0  & 0.0   & \textbf{66.7}  & 30.0  & 13.3  & 36.3  \\
    Gemini~\cite{team2023gemini} & 65.7  & 46.0  & 64.1  & 60.0  & 33.7  &  63.2 & {56.7}  & 16.7  & \textbf{67.5}  & 
\textbf{53.3}  & 10.0  & \textbf{66.7}  & 53.3  & 13.3  & 33.3  \\
    GPT-4V~\cite{achiam2023gpt} & 66.7  & 47.3  & 66.1  & 59.8  & 34.3  & 55.8  & \textbf{61.7}  & \textbf{33.3}  &  66.7  & \textbf{53.3}  &  20.0  & 62.5  & \textbf{95.0}  & \textbf{90.0}  & \textbf{94.7}  \\
    \hline
    \end{tabular}}
  \label{hallupi_tab4}%
\end{table*}%


\subsubsection{ReForm-Eval: Evaluating MLLMs via Unified Re-Formulation of Task-Oriented Benchmarks}

MLLMs have made significant progress in understanding and reasoning about visual information~\cite{achiam2023gpt,zhu2023minigpt,liu2024visual,ye2023mplug,chen2023shikra}. However, this has posed challenges for the automatic evaluation of free-form text outputs from MLLMs. To leverage annotations from existing benchmarks and reduce the manual effort required to construct new benchmarks, Zejun Li et al.~\cite{li2024reform} proposed a method for reformatting existing benchmarks into a unified format compatible with MLLMs. Through systematic data collection and reformatting, they introduced the ReForm-Eval benchmark, which is designed to comprehensively and quantitatively assess the capabilities of MLLMs. This approach overcomes the structural differences between existing task-oriented multimodal benchmarks and MLLMs.

Figure~\ref{reform_eval_fig} illustrates the capabilities and task dimensions of the ReForm-Eval benchmark. It categorizes the evaluation dimensions into two major categories with eight subcategories: 1)Visual Perception Tasks: Coarse-Grained Perception (CG), Fine-Grained Perception (FG), Scene Text Perception (STP). 2)Visual Cognition Tasks: Visually Grounded Reasoning (VGR), Spatial Understanding (Spatial), Cross-Modal Inference (CMI), Visual Description (Desc), Multi-Turn Dialogue (Dialog).

These categories and subcategories comprehensively cover different aspects of MLLMs' visual understanding and reasoning capabilities, providing a comprehensive benchmark  for evaluating model performance.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth,height=0.9\linewidth]{fig/reform_eval.pdf}
    \caption{Assessed capability dimensions and tasks in ReForm-Eval. ''Desc'' and ''Classif'' are respectively short for description and classification.~\cite{li2024reform}}
    \label{reform_eval_fig}
\end{figure} 


Table~\ref{reform_eval_tab} shows a comprehensive performance evaluation of 16 open-source MLLMs across different capability dimensions, based on the ReForm-Eval benchmark.


\begin{table*}[t]
\renewcommand\arraystretch{1.2}
    \centering
        \caption{General evaluation results of MLLMs across different capability dimensions. ``CG'', ``FG'', ``CMI'', and ``Desc'' are respectively short for coarse-grained perception, fine-grained perception, cross-modal inference, and description. ``$\bar{R}$'' represents the average rank across dimensions.~\cite{li2024reform}}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|ccc|ccccc|c|cc|cccc|c}
    \hline
    &  \multicolumn{9}{c}{\textbf{Generation Evaluation}}
    &  \multicolumn{7}{c}{\textbf{Likelihood Evaluation}} \\ \cmidrule{2-17}  
    \textbf{Model}
    & \multicolumn{3}{c|}{\textbf{Perception}}
    & \multicolumn{5}{c|}{\textbf{Cognition}} 
    & \multirow{2}{*}{$\bar{R}$}
    & \multicolumn{2}{c|}{\textbf{Perception}}
    & \multicolumn{4}{c|}{\textbf{Cognition}} 
    & \multirow{2}{*}{$\bar{R}$}  \\  \cmidrule{2-9}  \cmidrule{11-16}  
    & CG & FG & STP  & Spatial & VGR  & Dialog & CMI & Desc 
    &    & CG & FG   & Spatial & VGR  & Dialog & CMI &  \\ \midrule
    $\text{BLIP-2}_F$~\cite{li2023blip} 			& 69.4 & 76.6 & 38.1  & 43.2 & 73.3 & \textbf{61.8} & 66.9 & \textbf{74.3} & 2   & 60.7  & 74.4  & 51.1  & 69.8  & 62.6  & 58.9 & 4 \\
    $\text{InstructBLIP}_F$~\cite{dai2023instructblip}		& \textbf{71.2} & \textbf{78.1} & \textbf{41.2}  & \textbf{46.1} & \textbf{73.9} & 60.6  & \textbf{71.4} & 43.8 & \textbf{2}   & 60.4  & 75.6  & 51.2  & 71.0  & 67.2  & 55.5 & 4 \\
    $\text{InstructBLIP}_V$~\cite{dai2023instructblip} 	& 69.1 & 70.8 & 40.7  & 44.4 & 63.0 & 48.6  & 53.8 & 27.3 & 4   & 58.5  & 77.8  & 52.3  & \textbf{73.5}  & \textbf{68.7}  & 55.4 & 3 \\
    $\text{LLaVA}_V$~\cite{liu2024visual}  			& 28.7 & 34.4 & 18.4  & 28.7 & 44.0 & 35.6  & 47.3 & 36.8 & 11  & 61.0  & 70.3  & 42.4  & 58.9  & 52.3  & 48.0 & 8 \\
    $\text{LLaVA}_{L_2}$~\cite{liu2024visual}  		& 48.3 & 59.8 & 21.5  & 41.2 & 59.7 & 46.3  & 49.9 & 39.5 & 6   & 49.9  & 65.6  & 47.4  & 56.7  & 48.6  & 49.7 & 11 \\
    MiniGPT4~\cite{zhu2023minigpt}					& 46.2 & 53.2 & 33.0  & 34.6 & 45.6 & 39.5  & 45.4 & 47.5 & 7   & 54.9  & 70.6  & 49.2  & 57.3  & 54.1  & 50.9 & 8 \\
    mPLUG-Owl~\cite{ye2023mplugowl2} 					& 42.0 & 37.2 & 39.8  & 26.8 & 37.5 & 35.2  & 40.4 & 44.7 & 11  & 57.9  & 66.1  & 48.6  & 54.3  & 45.5  & 49.8 & 10 \\
    PandaGPT~\cite{su2023pandagpt}					& 28.2 & 34.6 & 4.5   & 33.3 & 41.9 & 34.1  & 36.6 & 1.6  & 14  & 42.3  & 47.4  & 39.4  & 43.3  & 41.5  & 37.0 & 16 \\
    IB-LLM~\cite{han2023imagebind}					& 29.2 & 32.7 & 8.2   & 35.6 & 36.7 & 35.3  & 36.6 & 27.6 & 13  & 49.6  & 54.4  & 46.1  & 50.3  & 39.5  & 45.6 & 15 \\
    LA-V2~\cite{gao2023llama}						& 33.2 & 30.8 & 24.2  & 23.8 & 36.3 & 35.4  & 41.1 & 36.0 & 13  & 42.7  & 61.4  & 48.6  & 54.1  & 43.4  & 49.9 & 12 \\
    mmGPT~\cite{gong2023multimodal}						& 30.4 & 30.3 & 16.7  & 26.9 & 33.0 & 31.8  & 38.2 & 27.7 & 14  & 52.6  & 62.4  & 47.2  & 56.2  & 43.1  & 44.1 & 13 \\
    Shikra~\cite{chen2023shikra}						& 47.2 & 47.5 & 8.3   & 33.3 & 41.2 & 35.2  & 44.5 & 31.8 & 11  & 60.9  & 66.8  & 45.5  & 58.5  & 59.5  & \textbf{59.3} & 7 \\
    Lynx~\cite{zeng2024matters}						& 59.5 & 62.6 & 18.6  & 40.2 & 58.4 & 47.0  & 53.0 & 60.7 & 5   & \textbf{66.1}  & 76.2  & \textbf{53.9}  & 69.9  & 60.0  & 57.4 & 3 \\
    $\text{Cheetor}_V$~\cite{li2023empowering} 			& 52.0 & 50.3 & 25.9  & 30.6 & 49.9 & 40.3  & 47.4 & 61.6 & 7   & 56.1  & 69.0  & 48.4  & 58.7  & 57.6  & 50.6 & 8 \\
    $\text{Cheetor}_{L_2}$~\cite{li2023empowering}		& 46.5 & 51.4 & 18.8  & 34.5 & 54.4 & 40.6  & 44.0 & 43.9 & 8   & 61.6  & 56.1  & 48.7  & 57.5  & 46.8  & 47.2 & 11 \\
    BLIVA~\cite{hu2024bliva}						& 41.7 & 43.4 & 40.8  & 33.3 & 42.4 & 39.8  & 45.2 & 52.5 & 8   & 64.9  & \textbf{78.2}  & 51.7  & 72.9  & 68.1  & 53.7 & \textbf{2}     
    \\ \hline
    \end{tabular}}
    \label{reform_eval_tab}
\end{table*}


\subsubsection{VisionGraph: Graph Theory Problems Benchmark in Visual Context}

MLLMs have achieved significant success in visual understanding and reasoning~\cite{luo2023wizardmath,liu2024visual,chen2023shikra,achiam2023gpt}, but multimodal graph reasoning remains a challenging task~\cite{wang2024can}. It requires MLLMs to accurately understand graph structures and perform multi-step reasoning on visual graphs. To explore the ability of advanced MLLMs to address multimodal graph reasoning tasks, Yunxin Li et al.~\cite{li2024visiongraph} designed a benchmark called VisionGraph, which includes a series of graph reasoning problems aimed at testing MLLMs' understanding of graph structures and their multi-step reasoning capabilities.

Table~\ref{VisionGraph_tab1} presents the performance of different MLLMs on the VisionGraph benchmark, including evaluation metrics such as node recognition accuracy, edge recognition accuracy, and solution accuracy for specific graph theory problems. These results provide valuable insights for researchers into the models' abilities to understand and reason about graph structures.


\begin{table*}[t]
\renewcommand\arraystretch{1.20}
\caption{MLLMs results in the VisionGraph benchmark.~\cite{li2024visiongraph}}
\label{VisionGraph_tab1}
\centering
\scriptsize
\scalebox{0.95}{
\begin{tabular}{l|ccccccccc}
\hline
Model$\downarrow$ Task Types $\rightarrow$ & Connect & Cycle & Topo. Sort & Shortest Path & Max. Flow & Bipartite Graph & Hamilton Path & GNNs \\ 
\hline
 & \multicolumn{9}{c}{\textit{Node Recognition}} \\
\hline
%MiniGPT-4 $^{\clubsuit}$(Vicuna-7b, T=1.0) & 19.41 & 16.75 & 36.30 & 37.50 & 29.31 & 9.52 & 43.10 & 41.03 & \\
MiniGPT-4 (Vicuna-7b)~\cite{zhu2023minigpt} & 19.14 & 12.04 & 42.96 & 42.19 & 32.76 & 8.33 & 60.34 & 53.85 & \\
BLIP-2 (FlanT5-xxl)~\cite{li2023blip} & 37.74 & 52.88 & 47.41 & 81.25 & 67.24 & 22.62 & 62.07 & 61.54 &  \\
%mPLUG-Owl$^{\clubsuit}$ (Vicuna-7b) \\
%mPLUG-Owl$^{\clubsuit}$ (V-4-shot) \\
InstructBLIP (FlanT5-xl)~\cite{dai2023instructblip} & 36.12 & 47.64 & 46.67 & 75.00 & 56.90 & 36.90 & 53.45 & 74.36 & \\
InstructBLIP (FlanT5-xxl)~\cite{dai2023instructblip} & 35.31 & 52.88 & 61.48 & 85.94 & 77.59 & 17.86 & 65.52 & 61.54 &  \\
Sphinx~\cite{lin2023sphinx}  &	61.99	&98.95	&94.07&	100.00	&91.38	&55.95	&\textbf{100.00}&	97.44\\
Internlm~\cite{cai2024internlm2}  &	\textbf{67.92}	& \textbf{100.00}	&\textbf{97.78}	&\textbf{100.00}	&\textbf{98.25}	&\textbf{77.38}	&\textbf{100.00}	&\textbf{100.00}\\
Llava-v1.5-7b~\cite{liu2024visual}  &   64.15 & 96.86 & 92.59 & 100.00 & 93.10 & 13.10 & \textbf{100.00} & 94.87 \\
Llava-v1.5-13b~\cite{liu2024visual}  &   62.26 & 97.91 & 91.11 & 100.00 & 96.55 & 11.9 & \textbf{100.00} & 97.44 \\
Qwen-Plus (0-shot)~\cite{bai2023qwen}	& 2.96	&0.00	&0.00	&0.00	&5.17&	0.00	&0.00	&56.41\\
Qwen-max (0-shot)~\cite{bai2023qwen}	&29.11	&31.94	&30.37&	12.50	&3.45	&14.29	&29.31	&46.15\\
Gemini (0-shot)~\cite{geminiteam2024geminifamilyhighlycapable} &  40.97 & 42.93 & 47.41 & 67.19 & 72.41 & 10.71 & 65.52 & 35.90 \\
GPT-4V (0-shot)~\cite{achiam2023gpt} &   46.49 & 81.15 & 81.48 & 89.06 & 58.62 & 20.24 & 100.00 & 97.44 \\
\hline


& \multicolumn{9}{c}{\textit{Edge Recognition (Correct / Error)}} \\
\hline
MiniGPT-4 (Vicuna-7b)~\cite{zhu2023minigpt} & 11.78/31.78 & 0.68/1.59 & 12.54/58.89 & 4.78/87.20 & 0.61/61.15 & 14.45/47.53 & 28.48/34.69 & 37.48/55.05 & \\
BLIP-2 (FlanT5-xxl)~\cite{li2023blip} & 12.49/84.03 & 15.11/84.69 & 0.08/2.14 & 1.75/96.84 & 0.00/0.00 & 9.92/75.89 & 11.73/45.55 & 17.26/\textbf{88.84} & \\
Sphinx ~\cite{lin2023sphinx}	&44.76/66.69	&22.13/79.69	&37.84/73.07	&39.88/70.62	&20.68/86.57	&\textbf{83.93}/53.51	&66.26/71.15	&60.66/61.43\\
Internlm ~\cite{cai2024internlm2} &53.08/35.01&	40.78/60.05&	\textbf{55.70}/50.85	&\textbf{57.82}/45.02	&\textbf{23.45}/80.27	&71.21/42.34	&\textbf{73.98}/36.00	&\textbf{83.00}/19.69\\
InstructBLIP (FlanT5-xl)~\cite{dai2023instructblip} & 17.24/\textbf{87.62} & 26.02/\textbf{88.06} & 0.00/0.00 & 5.70/93.93 & 0.00/0.00 & 12.72/83.13 & 37.07/\textbf{82.85} & 49.18/81.28 & \\
InstructBLIP (FlanT5-xxl)~\cite{dai2023instructblip} & 16.34/81.50 & 16.04/85.54 & 0.00/0.00 & 3.58/\textbf{98.31} & 0.00/0.00 & 13.26/\textbf{76.86} & 32.05/65.84 & 37.70/67.57 & \\
Llava-v1.5-7b~\cite{liu2024visual}    & 46.81/58.13 & 23.23/77.63 & 36.56/72.97 & 38.76/66.47 & 9.80/91.56 & 63.10/54.70 & 80.14/48.06 & 69.85/32.92 \\

Llava-v1.5-13b~\cite{liu2024visual}    & 51.18/53.41 & 22.60/76.91 & 38.80/70.26 & 41.93/63.50 & 9.89/91.72 & 67.88/54.21 & 76.26/45.21 & 67.40/33.59 \\

Qwen-Plus~\cite{bai2023qwen}	&30.46/64.78	&27.42/82.37	&10.59/68.46&	6.16/81.60	&1.32/64.62	&75.93/58.65	&48.63/50.41	&33.71/60.56 \\
Qwen-max~\cite{bai2023qwen}	& 25.71/63.21&	20.92/83.50&	16.70/\textbf{76.00}	&1.63/95.70	&1.12/\textbf{96.58}	&42.59/55.55&	40.47/51.61&	35.17/55.81\\
Gemini (0-shot)~\cite{geminiteam2024geminifamilyhighlycapable}  & 23.26/52.35 & 21.65/80.09 & 19.11/66.94 & 16.18/83.09 & 4.79/94.78 & 66.01/53.90 & 39.40/37.80 & 40.83/52.60 \\
GPT-4V (0-shot)~\cite{achiam2023gpt}   & 14.10/23.09 & 17.50/72.97 & 9.64/30.58 & 23.01/66.85 & 5.31/43.62 & 24.13/32.33 & 29.22/38.03 & 46.14/42.74 \\
GPT-4V (4-shot)~\cite{achiam2023gpt}  & 20.63/34.52 & 26.25/69.95 & 13.19/51.75 & 23.40/61.90 & 6.12/84.94 & 46.33/51.69 & 58.49/49.79 & 48.06/35.01 \\
\hline


 & \multicolumn{9}{c}{\textit{Accuracy on Specific Graph Theory Problems}} \\
\hline
%MiniGPT-4 $^{\clubsuit}$(Vicuna-7b, T=1.0) & 54.45 & 51.83 & 0.00 & 0.00 & 0.00 & 1.19 & 0.00 & 0.00 \\
MiniGPT-4 (Vicuna-7b)~\cite{zhu2023minigpt} & 50.67 & 48.69 & 0.00 & 0.00 & 0.00 & \textbf{5.95} & 0.00 & 0.00 \\
BLIP-2 (FlanT5-xxl)~\cite{li2023blip} & 46.63 & \textbf{61.26} & 0.00 & 0.00 & \textbf{13.79} & 0.00 & 0.00 & 0.00 \\
%mPLUG-Owl$^{\clubsuit}$ (Vicuna-7b) \\
%mPLUG-Owl$^{\clubsuit}$ (V-4-shot) \\
InstructBLIP (FlanT5-xl)~\cite{dai2023instructblip} & 48.79 & 47.12 & 0.00 & 0.00 & 6.90 & 0.00 & 0.00 & 0.00 \\
InstructBLIP (FlanT5-xxl)~\cite{dai2023instructblip} & 48.25 & 52.88 & 0.00 & 0.00 & 12.07 & 0.00 & 0.00 & 0.00 \\
Llava-v1.5-7b~\cite{liu2024visual}     & 53.37 & 47.12 & 0.00 & 3.12 & 1.72 & 0.00 & 0.00 & 0.00\\

Llava-v1.5-13b~\cite{liu2024visual}   & 52.83 & 47.12 & 0.00 & 4.69 & 3.45 & 0.00 & 0.00 & 0.00\\

 Gemini (0-shot)~\cite{geminiteam2024geminifamilyhighlycapable}  & 55.52 & 48.69 & 0.00 & 0.00 & 3.45 & 1.72 & 0.00 & 0.00 &\\
GPT-4V (0-shot)~\cite{achiam2023gpt}  & 38.81 & 49.21 & - & 3.12 & - & - & 0.00 & -\\
GPT-4V (2-shot)~\cite{achiam2023gpt}  & 54.98 & 52.35 & - & 6.25 & - & - & 0.00 & -\\
GPT-4V (0-COT)~\cite{achiam2023gpt}  & 30.45 & 50.26 & - & \textbf{7.69} & - & - & 0.00 & -\\
GPT-4V (2-COT)~\cite{achiam2023gpt}  & 54.71 & 52.87 &  -& 6.25 & - & - & 0.00 & -\\

%\\没有GPT-4V (4-shot)
\hline
\end{tabular}
}
\end{table*}


Table~\ref{VisionGraph_tab2} shows the performance improvements of models on three representative graph theory problems (Connectivity, Cycle, and Shortest Path) after applying the Description-Program-Reasoning (DPR) method. The DPR approach enhances MLLMs' multi-step reasoning abilities by combining natural language processing and programming logic.

\begin{table*}[t]
\renewcommand\arraystretch{1.2}
\caption{Model performance on three common graph theory problems in VisionGraph.  ~\cite{li2024visiongraph}}
\label{VisionGraph_tab2}
\centering
\scriptsize
\begin{tabular}{l|cccc|cccc|ccc}
\hline
Task Types $\rightarrow$ & \multicolumn{4}{c|}{Connectivity } & \multicolumn{4}{c|}{Cycle } & \multicolumn{3}{c}{Shortest Path } \\ 
 \cmidrule(r){2-12}
Model$\downarrow$ & Easy & Medium & Hard & Avg. & Easy & Medium & Hard  & Avg. & Easy & Hard  & Avg.\\
\hline
MiniGPT-4 (Vicuna-7b)~\cite{zhu2023minigpt} & 60.71 & 53.57 & 52.94 & 54.45 & 36.00 & 51.40 & \textbf{59.32} & 51.83 & 0.00 & 0.00 & 0.00 \\
%MiniGPT-4 $^{\clubsuit}$(Vicuna-7b, T=0.2) & 51.79 & 55.61 & 42.02 & 50.67 & 36.00 & 49.53 & 52.54 & 48.69 & 0.00 & 0.00 & 0.00 \\
BLIP-2 (FlanT5-xxl)~\cite{li2023blip} &  37.50&  43.37&  \textbf{56.30}&  46.63&  \textbf{88.00}&  \textbf{63.55}&  45.76&  \textbf{61.26}&  0.00&  0.00&  0.00\\
%mPLUG-Owl$^{\clubsuit}$ (Vicuna-7b) \\
%mPLUG-Owl$^{\clubsuit}$ (V-4-shot) \\
%InstructBLIP$^{\clubsuit}$ (Vicuna-13b) &  &  &  &  &  &  &  &  & \\
InstructBLIP (FlanT5-xl)~\cite{dai2023instructblip} &  46.43&  46.43&  53.78&  48.79&  36.00&  50.47&  45.76&  47.12&  0.00&  0.00&  0.00\\
Sphinx~\cite{lin2023sphinx}	&39.29	&45.41	&52.10&	46.63&	64.00	&49.53	&54.24	&52.88	&6.90	&0.00&	3.12\\

Internlm~\cite{cai2024internlm2}	&78.57&	\textbf{66.33}	&52.10&	52.94	& 52.00	&55.14&	\textbf{59.32}	&56.02&	0.00&	0.00&	0.00\\

Llava-v1.5-7b~\cite{liu2024visual}   & 64.29 & 50.00 & 53.78 & 53.27 & 36.00 & 50.47 & 45.76 & 47.12 & 6.90 & 0.00 & 3.12 \\



Llava-v1.5-13b~\cite{liu2024visual}   & \textbf{71.43} & 49.49 & 49.58 & 52.83 & 36.00 & 50.47 & 45.76 & 47.12 & 10.34 & 0.00 & 4.69 \\

Gemini (0-shot)~\cite{geminiteam2024geminifamilyhighlycapable}  & 69.64 & 56.63 & 47.06 & 55.52 & 60.00 & 47.66 & 45.76 & 48.69 & 0.00 & 0.00 & 0.00  \\

Gemini (DPR)~\cite{geminiteam2024geminifamilyhighlycapable}  & 66.07 & 52.04 & 36.97 & 49.32 & 76.00 & 27.10 & 22.03 & 31.93 & 0.00 & 0.00 & 0.00  \\
Qwen-plus~\cite{bai2023qwen}	&62.50 &	56.63	&47.06&	54.45	&64.00	&49.53	&54.24&	52.88&	0.00&	0.00	&0.00 \\


Qwen-max~\cite{bai2023qwen}	&62.50&	56.63&	46.22&	54.18	&64.00&	49.53	&54.24&	52.88	&0.00	&0.00	&0.00\\



GPT-4V (0-shot)~\cite{achiam2023gpt}  & 69.64 & 42.86 & 17.65 & 38.81 & 60.00 & 48.60 & 45.76 & 49.21 & 6.90 & 0.00 & 3.12 \\
GPT-4V (2-shot)~\cite{achiam2023gpt}  & 67.86 & 56.12 & 47.06 & 54.98 & 64.00 & 48.60 & 54.24 & 52.35 & 13.79 & 0.00 & 6.25 \\
GPT-4V (0-COT)~\cite{achiam2023gpt}  & 64.29 & 34.69 & 7.56 & 30.45 & 64.00 & 47.66 & 49.15 & 50.26 & 17.24 & 0.00 & 7.69 \\
GPT-4V (2-COT)~\cite{achiam2023gpt}  & 67.86 & 56.63 & 45.38 & 54.71 & 64.00 & 49.53 & 54.24 & 52.87 & 13.79 & 0.00 & 6.25  \\
GPT-4V (DPR)~\cite{achiam2023gpt} & 92.86 & 58.67 & 36.97 & \textbf{56.87} & 76.00 & 48.60 & 45.76 & 51.30 & \textbf{24.14} & \textbf{2.86} & \textbf{12.50} \\


%\\没有GPT-4V (4-shot)
\hline
\end{tabular}

\end{table*}

\subsection{Applications of MLLMs}

Multimodal large models (MLLMs) have emerged as a significant direction in artificial intelligence research in recent years~\cite{zhan2024anygpt,chiang2023vicuna,jiang2023motiongpt,zhang2024motiongpt,huang2023visual,li2023large,liu2024improved,mu2024embodiedgpt}. With the rapid development of technologies such as natural language processing, computer vision, and speech recognition, single-modal intelligent systems can no longer meet the increasingly complex requirements of real-world applications~\cite{park2023generative,radford2021learning,rocamonde2023vision,sun2023aligning}. Multimodal learning, by integrating different types of data inputs, simulates the diversity and complexity of human information processing, offering more comprehensive and flexible intelligent services. The emergence of multimodal models like CLIP~\cite{radford2021learning} and GPT-4~\cite{achiam2023gpt} marks the rapid progress in this field, showcasing exceptional performance in tasks such as image generation, text generation, and cross-modal retrieval~\cite{snell2022offline,wang2022self,wei2022chain,betker2023improving,brooks2023instructpix2pix}. The ability to integrate information from various modalities enables MLLMs to demonstrate tremendous potential across multiple domains, particularly in applications such as autonomous driving, smart healthcare, and robotics, where they can significantly enhance system robustness and intelligence levels~\cite{brohan2023rt,driess2023palm,chen2023shikra,huang2023instruct2act,huang2022language,huang2022inner,huang2022inner,liu2024visual,chen2024llm,lv2024robomp}.

This section will delve into various applications of MLLMs, including but not limited to cross-modal retrieval, visual question answering, autonomous driving, smart healthcare, and sentiment analysis. By analyzing these applications, this section aims to help readers better understand the advantages and challenges of MLLMs across different tasks, while also providing insights for future research directions.

Zebang Cheng et al.~\cite{cheng2024emotion} proposed Emotion-LLaMA, which integrates audio, visual, and text inputs through an emotion-specific encoder, and significantly improves emotion recognition and reasoning accuracy through instruction tuning. This approach enhances the model’s ability to understand and reason about emotional content across different modalities.

Xun Wu et al.~\cite{wumultimodal} created the VisionPrefer dataset, which includes fine-grained human preference annotations. They then trained the VP-Score reward model on this dataset to guide the training of image generation models, improving the alignment between images and text prompts. Finally, they fine-tuned the model using reinforcement learning to make the generated images more aligned with human aesthetics and preferences.

Zhenyu Wang et al.~\cite{wang2024genartist} proposed the GenArtist system, which enables unified image generation and editing coordinated by a multimodal large language model. The system introduces location-aware tool execution and integrates tool libraries, enhancing the model's flexibility and applicability.

Yushi Hu et al.~\cite{hu2024visual} proposed the Visual SKETCHPAD framework, enabling multimodal language models to draw sketches and perform reasoning based on visual artifacts. This significantly enhances the model's performance in mathematical and visual tasks.

Haoyu Chen et al.~\cite{chen2024restoreagent} proposed an MLLM-based intelligent image restoration system, RestoreAgent, which can automatically assess degradation, determine tasks, select models, and perform restoration.

Haodong Chen et al.~\cite{chen2024finecliper} proposed the FineCLIPER framework, which enhances facial expression recognition performance by incorporating text description augmentation, hierarchical information mining, and parameter-efficient fine-tuning to achieve multimodal feature fusion and cross-modal contrastive learning.

Shuo Ma et al.~\cite{ma2024sleepmg} proposed SleepMG, which addresses the classification and domain-discrepancy performance issues in sleep staging by quantifying modal performance differences and adaptively adjusting gradients to achieve multimodal balance. This method specifically tackles the challenges posed by the classification of multimodal physiological signals, such as EEG, EOG, EMG, and ECG.

Yifeng Xie et al.~\cite{xie2024moba} proposed the MoBA model, which employs bidirectional adapters and a mixture of experts system to achieve efficient cross-modal interaction with a low parameter count. This approach addresses the issues of large parameter sizes and low fine-tuning efficiency in multimodal sarcasm detection.

Pinxue Guo et al.~\cite{guo2024x} proposed the X-Prompt framework, which pretrains an RGB-based model and then adapts it to downstream tasks using multimodal prompts and specialized expert adapters. This approach addresses the limitations of traditional video object segmentation in complex scenarios such as extreme lighting and fast motion.

Daiqing Wu et al.~\cite{wu2024robust} proposed the DRF method, which addresses the issues of poor modality quality and missing data in sentiment analysis of image-text pairs on social media by approximating modality distributions using feature queues.

Lv Tang et al.~\cite{tang2024chain} proposed the MMCPF framework and CoVP strategy based on MLLMs, which effectively detect camouflaged objects without labeled data, addressing the issue of weak generalization in supervised learning models for zero-shot camouflaged object detection.

Deji Zhao et al.~\cite{zhao2024autograph} proposed AutoGraph, an automatic method for constructing visual context graphs. They designed a graph sampling syntax and employed a two-stage fine-tuning strategy to enhance the visual dialogue capabilities of LLMs.

Kangzheng Liu et al.~\cite{liu2024dysarl} proposed DySarl, which effectively enhances multimodal knowledge graph reasoning performance through dual-space multi-hop structural learning and interactive symmetric attention fusion.

Bowen Zhao et al.~\cite{zhao2024ct2c} proposed the CT2C-QA dataset and the AED multi-agent system. The former includes three modalities, while the latter unifies multimodal data processing through collaborative agents and introduces new evaluation metrics to enhance question-answering performance.

Linhui Xiao et al.~\cite{xiao2024hivg} proposed the HiVG framework, which includes multi-level adaptive cross-modal bridges and hierarchical low-rank adaptation. This framework enables fine-grained multimodal feature modulation, enhancing the accuracy and efficiency of visual localization.

Ruofan Wang et al.~\cite{wang2024white} proposed a multimodal attack strategy with dual optimization objectives, which jointly attacks both the text and image modalities to increase the success rate of attacking MLLMs.

Feihong Lu et al.~\cite{lu2024miko} proposed the Miko framework, which combines LLMs and MLLMs to automatically capture user intentions by analyzing text and images, and constructs an intention knowledge base to enhance intention understanding in social media.

Pinhan Fu et al.~\cite{fu2024core} proposed CoMO-NAS, which guides multi-objective search through core structure optimization to balance model complexity and performance, improving search efficiency and meeting the diverse needs of users.

Jianing Zhao et al.~\cite{zhao2024hawkeye} addressed the challenge of detecting implicit abnormal emotions in reconnaissance videos by proposing the scene-enhanced MLLM, Hawkeye, for the IasDig task. It integrates graph-structured scene modeling with a balanced heterogeneous MoE module to optimize scene information modeling and balance, effectively reducing false alarm rates and improving detection efficiency.

Xian Zhang et al.~\cite{zhang2024differential} proposed the FINER-MLLM model, which enhances image feature extraction capabilities by fine-tuning the image encoder with LoRA and applying dual feature constraints. The model also introduces a retrieval-augmented mechanism to assist in generating accurate change descriptions.

Zhanyu Wang et al.~\cite{wang2024gpt4video} proposed the GPT4Video framework, which aims to enhance the capabilities of large language models in video understanding and generation, enabling them to better handle multimodal inputs and efficiently generate video content.

Xiuliang Duan et al.~\cite{duan2024reason} proposed the Reason-and-Execute prompting method, which enhances the model's ability to solve geometric problems by combining reasoning templates and execution templates.

Xuechen Guo et al.~\cite{guo2024llava} proposed the LLaVA-Ultra model, which introduces a fine-grained visual encoder and an adaptive sampling module through architecture improvements, addressing the performance limitations of current multimodal large language models in medical visual question answering (Med-VQA).

Yi Bin et al.~\cite{bin2024gallerygpt} constructed the large-scale painting analysis dataset, PaintingForm, and proposed the GalleryGPT model. By fine-tuning for tasks focused on visual feature analysis, the model significantly improved the performance and generalization ability of art analysis.

Dan Kondratyuk et al.~\cite{kondratyuk2023videopoet} proposed VideoPoet, a zero-shot video generation model based on LLMs. It uses a decoder architecture to process multimodal inputs and enables high-quality video synthesis, demonstrating the ability to generate complex dynamic scenes.

Yongshuo Zong et al.~\cite{zong2024safety} proposed post hoc and hybrid fine-tuning strategies to effectively enhance the safety of MLLMs, addressing the issues of harmful content generation and susceptibility to attacks in MLLMs.

Yang Jin et al.~\cite{jin2024video} proposed the Video-LaVIT framework, which achieves efficient video decomposition using keyframes and motion vectors. This approach enables unified pretraining for video, image, and text, improving the safety and efficiency of MLLMs.

Long Qian et al.~\cite{qian2024momentor} proposed the Momentor model, which incorporates a time-aware module and event-based sequence modeling to achieve fine-grained temporal understanding and video segment-level reasoning.

Zhisheng Zheng et al.~\cite{zheng2024bat} designed the SPATIAL-AST encoder, which jointly performs sound event detection, spatial localization, and distance estimation. By integrating SPATIAL-AST with LLaMA-2, they constructed the BAT model, capable of answering questions about sound source relationships in 3D environments. The model utilizes a multi-stage training strategy to progressively enhance its spatial audio perception and reasoning capabilities.

Guangzhi Sun et al.~\cite{sun2024video} proposed Video-SALMONN, the first unified model to simultaneously process video, speech, and music. They designed the MRC Q-Former structure to achieve multi-resolution information extraction, enhancing the ability of AV-LLMs to integrate speech information for comprehensive video content understanding.

Ling Li et al.~\cite{ligeoreasoner} introduced the concept of "localizability" to quantify street view images and filter high-quality data. They proposed the GeoReasoner model, which combines human reasoning knowledge and employs a two-stage fine-tuning approach to achieve geographic localization and reasoning, addressing the challenges of geographic localization in street view images.

Yunheng Li et al.~\cite{li2024cascade} proposed the Cascade-CLIP framework, which aligns multi-level visual features with text embeddings in a cascading manner. By introducing independent decoders to handle features at different levels, the framework enhances the transferability to new categories. This approach addresses the issue where the pre-trained model CLIP fails to fully leverage intermediate visual feature information in zero-shot semantic segmentation tasks.

Zhijian Huang et al.~\cite{huang2025making} proposed the RDA-Driver model, which ensures the consistency between reasoning and decision-making in MLLMs through reasoning-decision alignment constraints and a redesigned Chain-of-Thought (CoT) framework. This approach enhances the interpretability and performance of autonomous driving systems.

With the increasing richness of data sources and the continuous improvement of computational power, multimodal large models (MLLMs) are expected to see widespread application in more practical scenarios. At the same time, with the deepening of interdisciplinary research, MLLMs will not only play a role in traditional AI tasks but will also expand into more edge domains, driving artificial intelligence from closed systems to a more open and intelligent ecosystem.

In summary, the application prospects of multimodal large models are vast. However, to fully unleash their potential, this requires the combined advancement of technological innovation and theoretical breakthroughs. It is anticipated that, in the future, with the ongoing progress in algorithms, hardware, and cross-domain collaboration, MLLMs will achieve more efficient and intelligent performance in a wider range of practical applications, further advancing the development of artificial intelligence.