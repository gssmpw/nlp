
\section{Conclusion}

In this review, we systematically discuss the latest advancements and challenges in the continual learning of multimodal large models (MLLM). First, we review the innovative strategies of multimodal large models and their applications across different fields, highlighting their advantages in handling diverse data sources. We also introduce the most commonly used benchmark testing methods and provide application examples in various domains such as natural language processing and computer vision. 

Next, we provide a detailed overview of the latest research in continual learning, offering a classification of unimodal and multimodal continual learning in non-large models, and delving into the current state of research on large language models (LLMs) in continual learning. By comparing research across these different areas, we further clarify their approaches and limitations in dealing with data distribution changes. 

The extensive and in-depth research in both the multimodal large model and continual learning domains has laid a solid foundation for research in multimodal large model continual learning. We conduct a thorough analysis of the current state of research in this area, discussing aspects such as benchmark evaluation, model structures, and innovations in methods, revealing both the potential and the challenges faced by MLLM in continual learning. 

Finally, we provide a forward-looking discussion on the challenges and future development trends in the continual learning of multimodal large models. Our goal is to inspire researchers in the field and provide valuable insights for future research directions, aiming to promote the advancement and innovation of technologies related to the continual learning of multimodal large models.