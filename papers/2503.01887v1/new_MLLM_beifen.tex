


\begin{table*}[htbp]
\small
\renewcommand\arraystretch{1.2}
  \centering
  \caption{Innovations in Multimodal Large Model Frameworks.}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm}|p{0.4\textwidth}|p{0.4\textwidth}}
    \hline
   \multicolumn{1}{c|}{MLLMs} & \multicolumn{1}{c|}{Starting point of the problem} & \multicolumn{1}{c}{How to solve} \\
   
   \hline
   \multirow{2}{*}{\textbf{MaVEn}~\cite{jiang2024maven}} & 
Enhancing the image visual understanding of MLLMs.& MaVEn proposes an effective multi-granularity hybrid visual encoding framework.\\
   
   \hline
   \multirow{2}{*}{\textbf{MoVA}~\cite{zong2024mova}} & 
No single visual encoder can dominate the understanding of various image contents.& MoVA incorporates coarse-grained context-aware expert routing and fine-grained expert fusion.\\

   \hline
   \multirow{2}{*}{\textbf{MoME}~\cite{shen2024mome}} & 
The performance of general-purpose MLLMs is typically inferior to that of expert MLLMs. & MoME combines the MoVE and the MoLE to reduce task interference.\\

   \hline
   \multirow{2}{*}{\textbf{Meteor}~\cite{lee2024meteor}} & 
The performance gap of MLLMs in understanding and answering complex questions.& Meteor introduced the new concept of "traversal of rationales."\\

    \hline
   \multirow{2}{*}{\textbf{CORY}~\cite{ma2024coevolving}} & 
The stability and performance issues MLLMs encounter in RL fine-tuning.& CORY leverages the inherent cooperative evolution and emergence capabilities of multi-agent systems.\\

    \hline
   \multirow{2}{*}{\textbf{Lumen}~\cite{jiao2024lumen}} & 
MLMs overlook the intrinsic characteristics of different visual tasks.& Lumen enhances multimodal understanding by separating task-agnostic and task-specific learning.\\

    \hline
   \multirow{2}{*}{\textbf{Octopus}~\cite{zhaooctopus}} & 
MLLMs combine visual recognition and understanding sequentially at the LLM, which is suboptimal.& Octopus proposed the "Parallel Recognition → Sequential Understanding" MLLM framework.\\

    \hline
   \multirow{2}{*}{\textbf{Wings}~\cite{zhang2024wings}} & 
MLLMs tend to forget knowledge acquired from text-only instructions during training.& Wings introduces additional modules and mechanisms to compensate for attention shifts.\\

    \hline
   \multirow{2}{*}{\textbf{Cantor}~\cite{gao2024cantor}} & 
The "hallucination" problem in decision-making is caused by insufficient visual information.& Cantor inspires a multimodal chain-of-thought of MLLM.\\

    \hline
   \multirow{2}{*}{\textbf{AutoM3L}~\cite{luo2024autom3l}} & 
The limitations of automation in multimodal machine learning.& AutoM3L proposes an automated multimodal machine learning framework with MLLMs.\\

    \hline
   \multirow{2}{*}{\textbf{DI-MML}~\cite{fan2024detached}} & 
The modality competition issue in multimodal learning.& DI-MML proposes detached and interactive multimodal learning.\\

    \hline
   \multirow{2}{*}{\textbf{MEM}~\cite{liu2024multimodal}} & 
Data scraped from networks may leak personal privacy.& MEM optimizes by combining image noise and text triggers to mislead the model into learning shortcuts.\\

    \hline
   \multirow{2}{*}{\textbf{CREAM}~\cite{zhang2024cream}} & 
The lack of cross-page interaction support in document visual question answering.& CREAM proposes Coarse-to-Fine retrieval and multi-modal efficient tuning for document VQA.\\

    \hline
   \multirow{3}{*}{\textbf{SLUDA}~\cite{zheng2024self}} & 
Insufficient labeled data and the underutilization of unlabeled data.& SLUDA generates fine-grained data, optimizes unlabeled data usage, and employs adaptive selection and dynamic threshold strategies. \\

    \hline
   \multirow{2}{*}{\textbf{SAM}~\cite{wu2024semantic}} & 
The semantic alignment issue in MLLMs when processing multi-image instructions.& SAM enhances image-semantic associations through a bidirectional semantic guidance mechanism. \\

    \hline
   \multirow{3}{*}{\textbf{CTVLMs}~\cite{lu2024collaborative}} & 
Improving performance and reducing computational resource demands in MLLMs for multimodal tasks.& CTVLMs use knowledge distillation and multimodal alignment to transfer knowledge from large models to smaller ones.\\

    \hline
   \multirow{2}{*}{\textbf{Bloom}~\cite{kim2024efficient}} & 
Reducing the high computational cost of large-scale multilingual visual data modeling.& Bloom proposes pre-training with discretized visual speech representation.\\

    \hline
   \multirow{4}{*}{\shortstack{\textbf{MA-AGIQA} \\ \cite{wang2024large}}}& 
The quality evaluation issue of AI-generated images (AGIs).& MA-AGIQA combines multimodal models and traditional DNNs, utilizing semantic information extraction and the mixture of experts (MoE) structure to dynamically integrate quality-aware features.\\

    \hline
   \multirow{2}{*}{\makecell{\textbf{WorldGPT}\\~\cite{ge2024worldgpt}}} & Enhancing the applicability and generalization ability of MLLMs.& WorldGPT includes memory offloading, knowledge retrieval, and a Context Reflector.\\

    \hline
   \multirow{2}{*}{\textbf{Q-ALIGN}~\cite{wu2023q}} & 
Enhancing the applicability and generalization ability of MLLMs.& Q-ALIGN unifies IQA, IAA, and VQA tasks to enhance the model's cross-task generalization ability.\\

    \hline
   \multirow{2}{*}{\textbf{Flextron}~\cite{cai2024flextron}} & 
The deployment challenges of MLLMs in resource-constrained environments.& Flextron selects different sub-models or sub-networks by using routers.\\

    \hline
   \multirow{2}{*}{\makecell{\textbf{NExT-GPT}\\~\cite{wu2023next}}} & 
Existing MLLMs can only understand the input modality.& NExT-GPT proposes lightweight alignment techniques and modality-switching instruction tuning.\\





    \hline
    \end{tabularx}
  \label{MLLM_frame}%
\end{table*}%




 
\section{Multimodal Large Language Model}

\subsection{Preliminary}
In this section, we provide an overview of the latest research on MLLMs, including various model innovation strategies, a range of benchmarks, and the application of MLLMs in diverse domains.







\begin{table*}[htbp]
\small
\renewcommand\arraystretch{1.2}
  \centering
  \caption{Innovations in Multimodal Large Model Methods.}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm}|p{0.4\textwidth}|p{0.4\textwidth}}
    \hline
   \multicolumn{1}{c|}{Method} & \multicolumn{1}{c|}{Starting point of the problem} & \multicolumn{1}{c}{How to solve} \\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{DenseFusion}\\~\cite{li2024densefusion}}} & 
Enhancing the visual perception ability of MLLMs.& DenseFusion proposes a multimodal perception fusion method that integrates visual experts.\\
      
   \hline
   \multirow{2}{*}{\textbf{E2E-MFD}~\cite{zhang2024e2e}} & 
The complex training process hinders the broader application of MLLMs.& E2E-MFD proposes a novel end-to-end algorithm for multimodal fusion detection.\\
      
   \hline
   \multirow{2}{*}{\textbf{NAM}~\cite{fangtowards}} & 
Neuron attribution in MLLMs has not been fully explored yet.& NAM proposes a neuron attribution method tailored for MLLMs.\\
      
   \hline
   \multirow{2}{*}{\textbf{CODE}~\cite{kim2024code}} & 
Addressing the hallucination problem in MLLMs when generating visual content.& CODE utilizes self-generated descriptions as contrastive references to adjust the information flow.\\
      
   \hline
   \multirow{2}{*}{\makecell{\textbf{MULTEDIT}\\~\cite{basu2024understanding}}} & 
To correct errors and insert new information. & MULTEDIT introduces a multimodal causal tracking method.\\
      
   \hline
   \multirow{2}{*}{\textbf{QSLAW}~\cite{xie2024advancing}} & 
Tackling the resource consumption issue faced by MLLMs in visual-language instruction tuning. & QSLAW learns group scale factors of quantized weights and adopts multimodal pretraining method.\\
      
   \hline
   \multirow{2}{*}{\textbf{LECCR}~\cite{wang2024multimodal}} & 
To improve the quality of cross-modal alignment.& LECCR proposes the MLLM-enhanced cross-lingual, cross-modal retrieval method.\\
      
   \hline
   \multirow{2}{*}{\textbf{ERL-MR}~\cite{han2024erl}} & 
To address the modality imbalance problem in MLLMs.& ERL-MR uses Euler transformations and multimodal constraint loss.\\
      
   \hline
   \multirow{2}{*}{\textbf{AMMPL}~\cite{wu2024adaptive}} & 
Enhancing the model's performance and reasoning ability. & AMMPL proposes an adaptive multimodal prompt learning method.\\
      
   \hline
   \multirow{2}{*}{\textbf{PaRe}~\cite{cai2024enhancing}} & 
Enhancing the model's performance and reasoning ability. & PaRe progressively generates intermediate modalities and replaces modality-agnostic fragments.\\
      
   \hline
   \multirow{2}{*}{\textbf{MCL}~\cite{liimproving}} & 
Addressing the insufficient interaction problem when handling complex multimodal scenarios. & MCL proposes the multimodal combination learning (MCL) method.\\
      
   \hline
   \multirow{2}{*}{\textbf{FARE}~\cite{schlarmann2024robust}} & 
MLLMs are vulnerable to adversarial attacks in the visual modality. & FARE proposes the unsupervised adversarial fine-tuning scheme.\\
      
   \hline
   \multirow{2}{*}{\textbf{DICL}~\cite{huang2023machine}} & 
Reducing the reliance on manual annotations. & DICL leverages MLLMs knowledge to enhance the robustness of visual models.\\
      
   \hline
   \multirow{2}{*}{\textbf{API}~\cite{yu2025attention}} & 
Addressing the limitations of traditional visual prompting techniques. & API enhances model perception through attention heatmaps guided by text queries.\\
      
   \hline
   \multirow{2}{*}{\textbf{IVTP}~\cite{huang2025ivtp}} & 
Addressing the high computational cost problem in MLLMs. & IVTP proposeS the instruction-guided visual token pruning method.\\


    \hline
    \end{tabularx}
  \label{MLLM_method}%
\end{table*}%









\subsection{Model Innovation}
With the continuous development of MLLMs, researchers have made various innovations in their structure, methods, and functional modules to enhance model performance, generalization ability, and adaptability. This section reviews the main innovations, which focus on three core directions: framework design, method optimization, and functional module improvements. These innovations collectively drive the performance of MLLMs in complex multimodal tasks. This section will explore the latest research advancements in these areas.

\subsubsection{Framework Innovation}
Framework innovation is the foundation of MLLM development, aiming to achieve efficient fusion and processing of cross-modal information by improving the overall architectural design. In recent years, researchers have proposed many efficient framework designs. As shown in Table \ref{MLLM_frame}, researchers have proposed several efficient framework designs, such as MaVEn, MoVA, AutoM3L, DI-MML and et. These framework innovations provide more efficient tools and methods for MLLMs to handle multimodal tasks involving language, vision, and hearing. They enable MLLMs to achieve more precise reasoning and decision-making in the interaction of multimodal data, thereby offering strong support for solving complex problems in practical applications.

More details of the innovation of MLLMs Frameworks are provided in Section \ref{appendix_MLLM_MI} of the Appendix.






\subsubsection{Method Innovation}

Method innovation is the core driving force behind the performance improvement of MLLMs. By designing more efficient training methods and optimization objectives, it helps models better adapt to dynamic task environments. As shown in Table~\ref{MLLM_method}, in recent years, researchers have proposed numerous novel and efficient methods to enhance the accuracy and robustness of MLLMs. These method research has explored cutting-edge techniques such as multimodal contrastive learning, self-supervised learning objectives, and multimodal alignment mechanisms. These methods not only enhance the model's generalization ability but also significantly improve the accuracy and robustness of cross-modal tasks.

More details of the innovation of MLLMs methods are provided in Section \ref{appendix_MLLM_MI} of the Appendix.




\subsubsection{Module Innovation}

In terms of module innovation, researchers have focused on refining the internal design of models by improving specific modules to enhance cross-modal interaction and representational capabilities. These modules have increased the flexibility and efficiency of the models. As shown in Table~\ref{MLLM_module}, in recent years, many efficient and streamlined modules have been proposed, further boosting the overall performance of the models.These modules, through independent optimization and joint modeling of different modalities, enable the model to perform exceptionally well in tasks such as multimodal reasoning, generation, and understanding. They allow for more accurate cross-modal knowledge fusion and semantic understanding, providing crucial support for the functional expansion and practical application of MLLMs.

More details of the innovation of MLLMs Modules are provided in Section \ref{appendix_MLLM_MI} of the Appendix.





\begin{table*}[htbp]
\small
\renewcommand\arraystretch{1.2}
  \centering
  \caption{Innovations in Multimodal Large Model Modules.}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm}|p{0.4\textwidth}|p{0.4\textwidth}}
    \hline
   \multicolumn{1}{c|}{Module} & \multicolumn{1}{c|}{Starting point of the problem} & \multicolumn{1}{c}{How to solve} \\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{ChatTracker}\\~\cite{sun2024chattracker}}} & 
Enhancing the tracking performance of MLLM trackers.& ChatTracker proposes a novel reflection-based prompt optimization module.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{Optimus-1}\\~\cite{li2024optimus}}} & 
Current general agents lack the necessary world knowledge and multimodal experience.& Optimus-1 proposes a hybrid multimodal memory module.\\
   
   \hline
   \multirow{2}{*}{\textbf{CuMo}~\cite{li2024cumo}} & 
Improving the performance of MLLMs on multimodal tasks.& CuMo integrates sparse gated Top-K MoE blocks in the visual encoder and MLP connectors.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{AcFormer}\\~\cite{liu2024visual}}} & 
The connection between visual encoders and LLMs has limitations.& AcFormer identified visual anchors and proposed a novel vision-language connector\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{Chain-of-Sight}\\~\cite{huang2024accelerating}}} & 
Accelerating the pretraining process and improving model performance.& Chain-of-Sight captures visual details at different spatial scales through a multi-scale visual resampler.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{Dense}\\ \textbf{Connector}~\cite{yao2024dense}}} & 
Existing MLLMs underutilise the visual encoder while overly emphasising the language modality.& Dense Connector enhances the visual perception ability by integrating multi-layer visual features.\\
   
   \hline
   \multirow{2}{*}{\textbf{GCG}~\cite{wang2024weakly}} & 
In video question answering, MLLMs overlook visually relevant cues related to the question.& GCG learns to represent the temporal structure of videos and selects key frames.\\
   
   \hline
   \multirow{2}{*}{\textbf{Q-MoE}~\cite{wang2024q}} & 
Connection structure struggles with filtering visual information according to task requirements.& Q-MoE proposes a query-based hybrid expert connector.\\



    \hline
    \end{tabularx}
  \label{MLLM_module}%
\end{table*}%



\subsection{Benchmarks}
As MLLMs continue to achieve breakthroughs in multimodal tasks such as vision, language, and speech, comprehensive benchmarks have become crucial for systematically evaluating and comparing model performance. These benchmarks not only provide standardized datasets and tasks but also define metrics for assessing models' abilities in cross-modal reasoning, generation, classification, and other areas. They play a key role in guiding research directions, identifying model limitations, and advancing technological progress. This section will introduce some of the recent representative benchmarks, covering a wide range of scenarios from academic research to practical applications, reflecting the diverse needs and challenges in the multimodal field.



\subsubsection{ROPE: Recognition-based Object Probing Evaluation Benchmark}
Despite the impressive performance of MLLMs in various downstream applications, they often encounter the issue of object hallucination~\cite{rohrbach2018object,dai2022plausible,li2023evaluating,zhang2024groundhog,zhai2023halle,liu2023mitigating,you2023ferret,zhou2023analyzing,wang2023llm}, where the model erroneously generates objects that do not exist in the image. Current benchmarks for evaluating object hallucination mainly focus on the presence of a single object category, rather than individual entities. 

Xuweiyi Chen et al.~\cite{chen2024multi} conducted a systematic study of the multi-object hallucination problem, examining how models misidentify objects when attending to multiple objects simultaneously (e.g., inventing non-existent objects or being distracted). They introduced an automated evaluation protocol called Recognition-based Object Probing Evaluation (ROPE), which considers the distribution of object categories within a single image during testing. By using visual reference to disambiguate, the protocol systematically analyzes multi-object hallucination, revealing the hallucination behaviors and influencing factors when models process multiple objects. In addition, ROPE designs multiple task prompts, including Default Multi-Object, Student-Forcing, Teacher-Forcing, and Single-Object. The dataset is divided into four subsets, each considering different object category distributions: 1) Homogeneous: All test objects belong to the same category. 2) Heterogeneous: All test objects belong to different categories. 3) In-the-Wild: A mixed object category distribution, with test objects randomly selected and ordered. 4) Adversarial: After multiple repetitions of the same category, a different category object is introduced. The dataset is further divided into Seen and Unseen based on whether the model has encountered these images during instruction tuning. 

More details of the overview of MLLM performance on the ROPE are provided in Section \ref{appendix_rope} of the Appendix.




\subsubsection{CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark}
Visual Question Answering (VQA) is a crucial task in MLLMs, designed to test their understanding and reasoning capabilities across visual and textual data~\cite{antol2015vqa, mathew2021docvqa,masry2022chartqa,kafle2018dvqa,chen2021geoqa}. However, most existing VQA datasets primarily focus on English and a few major world languages, with images often being Western-centric. While recent efforts have expanded the linguistic coverage of VQA datasets, they still lack diversity in low-resource languages. Moreover, these datasets typically extend their language range through translation or other methods while keeping the images unchanged, leading to limited cultural representation. To address these limitations, David Romero et al.~\cite{wang2023llm} developed a new benchmark, CVQA, which aims to encompass rich linguistic and cultural diversity. This benchmark involves native speakers and cultural experts in the data collection process to ensure authenticity and inclusivity. 

More details of the scale and diversity of the CVQA benchmark and an overview of MLLM performance on the CVQA are provided in Section \ref{appendix_cvqa} of the Appendix.


\begin{table*}[htbp]
\small
\renewcommand\arraystretch{1.2}
  \centering
  \caption{Innovations in Non-Large Language Model Unimodal Continual Learning Frameworks.}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm}|p{0.4\textwidth}|p{0.4\textwidth}}
    \hline
   \multicolumn{1}{c|}{Framework} & \multicolumn{1}{c|}{Starting point of the problem} & \multicolumn{1}{c}{How to solve} \\
   
   \hline
   \multirow{2}{*}{\textbf{NTE}~\cite{benjamin2024continual}} & 
Addressing the catastrophic forgetting problem in graph neural networks.& NTE views a neural network as an ensemble of fixed experts.\\
   
   \hline
   \multirow{2}{*}{\textbf{IsCiL}~\cite{lee2024incremental}} & 
To address the issue of new data lacking labels due to annotation delays in continual learning.&IsCiL improves sample efficiency and task adaptability by incrementally learning shared skills.\\
   
   \hline
   \multirow{3}{*}{\textbf{CKP}~\cite{xu2024mitigate}} & 
To address the performance degradation caused by incorrect labels in the Lifelong Person Re-Identification task.& CKP purifies data through the CDP and ILR modules, and filters out erroneous knowledge using the EKF algorithm.\\
   
   \hline
   \multirow{2}{*}{\textbf{PBR}~\cite{liu2024prior}} & 
To reduce forgetting and enhances long-tail continual learning performance.& PBR proposes an uncertainty-guided sampling strategy and two prior-free constraints.\\
   
   \hline
   \multirow{2}{*}{\textbf{OSN}~\cite{hutask}} & 
Reducing the interference of new tasks on old tasks.& OSN explores shared knowledge between old and new tasks through parameter sharing.\\
   
   \hline
   \multirow{2}{*}{\textbf{MoDE}~\cite{lee2024becotta}} & 
Improving adaptation to new domains while preserving old knowledge.& MoDE includes domain-adaptive routing and domain-expert collaborative loss.\\
   
   \hline
   \multirow{2}{*}{\textbf{SB-MCL}~\cite{lee2024learning}} & 
To address the catastrophic forgetting problem in continual learning.& SB-MCL achieves continual learning through sequential Bayesian updates.\\
   
   \hline
   \multirow{2}{*}{\textbf{PNR}~\cite{charegularizing}} & 
Addressing the knowledge transfer and catastrophic forgetting issues.& PNR Generates pseudo-negative samples and optimizing knowledge transfer.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{CompoNet}\\~\cite{malagonself}}} & 
Addressing the issue of old task forgetting caused in continual reinforcement learning.& CompoNet proposes a modular neural network with linearly growing parameters.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{Vector-HaSH}\\~\cite{wangrapid}}} & 
To enable fast learning and continual memory.& Vector-HaSH combines hetero-associative memory and spatially invariant CNNs.\\
   
   \hline
   \multirow{2}{*}{\textbf{DDDR}~\cite{liang2025diffusion}} & 
Addressing the issue of catastrophic forgetting in federated continual learning.& DDDR uses diffusion models to generate historical data and employs contrastive learning.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{PromptCCD}\\~\cite{cendra2025promptccd}}} & 
Mitigating catastrophic forgetting.& PromptCCD introduces the GMP, which dynamically generates prompts to adapt to new classes.\\
   
   \hline
   \multirow{2}{*}{\textbf{Mecoin}~\cite{li2024efficient}} & 
To reduce parameter fine-tuning, lower the forgetting rate.& Mecoin employs SMU and a MeCo for efficient storage and updating of class prototypes.\\
   
   \hline
   \multirow{2}{*}{\textbf{RP2F}~\cite{sun2024incremental}} & 
Enabling effective knowledge sharing and backward knowledge transfer.& RP2F uses perturbation methods to approximate the Hessian matrix and introduces a prior.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{HAMMER}\\~\cite{liu2024hierarchical}}} & 
To address the catastrophic forgetting issue in multilingual text recognition.& HAMMER proposes online knowledge analysis and a hierarchical language evaluation mechanism.\\
   
   \hline
   \multirow{2}{*}{\textbf{FedCBC}~\cite{yu2024overcoming}} & 
Mitigating catastrophic forgetting.& FedCBC proposes category-specific binary classifiers and selective knowledge fusion.\\
   
   \hline
   \multirow{2}{*}{\textbf{TS-ILM}~\cite{xiaochen2024ts}} & 
Reducing information redundancy and enhancing memory retention.& TS-ILM proposes a task-level temporal pattern extractor and a time-sensitive example selector.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{AutoActivator}\\~\cite{li2024harnessing}}} & 
To address the issue of model forgetting old classes when continuously learning new classes.& AutoActivator dynamically adapts neural units to new tasks, enabling on-demand network expansion.\\
   
   \hline
   \multirow{2}{*}{\textbf{iNeMo}~\cite{fischer2024inemo}} & 
To achieve efficient class-incremental learning.& iNeMo proposes latent space initialization and position regularization.\\
   
   \hline
   \multirow{3}{*}{\textbf{TACO}~\cite{han2024topology}} & 
Offering a novel perspective for understanding and mitigating catastrophic forgetting.& TACO combines graph coarsening and continual learning to dynamically store information from previous tasks.\\



    \hline
    \end{tabularx}
  \label{CL_NonL_Framework}%
\end{table*}%





\subsubsection{II-Bench: Image Implication Understanding Benchmark}
Images often contain rich emotional and cultural narratives, and understanding their meaning and exploring the human emotions and cultural context they reflect requires attention to detail~\cite{bubeck2023sparks,achiam2023gpt,wachowiak2023does}. While MLLMs have made significant progress in understanding and generating cross-modal content, achieving new breakthroughs in benchmarks like image captioning~\cite{lin2014microsoft,sharma2018conceptual,sidorov2020textcaps,gurari2020captioning,pont2020connecting,agrawal2019nocaps} and visual question answering~\cite{antol2015vqa, mathew2021docvqa, masry2022chartqa,kafle2018dvqa,chen2021geoqa}, there has been insufficient exploration of their higher-order perceptual abilities. Ziqiang Liu et al.~\cite{liu2024ii} introduced a new benchmark, II-Bench, designed to evaluate MLLMs' ability to understand and reason about the complex implicit meanings in images, addressing the gap in existing benchmarks for assessing higher-order perceptual abilities in MLLMs.

More details of the scale and diversity of the II-Bench benchmark and an overview of MLLM performance on the II-Bench are provided in Section \ref{appendix_II-Bench} of the Appendix.








\begin{table*}[htbp]
\small
\renewcommand\arraystretch{1.2}
  \centering
  \caption{Innovations in Non-Large Language Model Unimodal Continual Learning Methods.}
    \begin{tabularx}{\textwidth}{>{\centering\arraybackslash}m{2cm}|p{0.4\textwidth}|p{0.4\textwidth}}
    \hline
   \multicolumn{1}{c|}{Method} & \multicolumn{1}{c|}{Starting point of the problem} & \multicolumn{1}{c}{How to solve} \\
   
   \hline
   \multirow{2}{*}{\textbf{GACL}~\cite{zhuang2024gacl}} & 
Addressing the catastrophic forgetting problem of models in class-incremental learning.& GACL establishes the equivalence between incremental learning and joint training.\\
   
   \hline
   \multirow{2}{*}{\textbf{C-Flat}~\cite{zhuang2024gacl}} & 
Addressing the balance between new task training sensitivity and memory retention.& C-Flat optimizes the flatness of the loss landscape.\\
   
   \hline
   \multirow{2}{*}{\textbf{DSGD}~\cite{fan2024dynamic}} & 
Addressing the practical deployment challenge.& DSGD uses structural and semantic information for stable knowledge distillation.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{VQ-Prompt}\\~\cite{jiao2024vector}}} & 
To improve continual learning performance.& VQ-Prompt utilizes vector quantization to achieve end-to-end optimization of discrete prompt selection.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{RanDumb}\\~\cite{prabhurandom}}} & 
Exploring whether the representations generated by continual learning algorithms are truly effective.& RanDumb uses random transformations and linear classifiers to address.\\
   
   \hline
   \multirow{2}{*}{\textbf{IWMS}~\cite{csabalabel}} & 
The label delay issue in online continual learning.& IWMS prioritizes the memory of samples similar to new data.\\
   
   \hline
   \multirow{2}{*}{\textbf{PPE}~\cite{li2024progressive}} & 
To address the catastrophic forgetting problem in non-sample online continual learning.& PPE learns class prototypes during the online learning phase.\\
   
   \hline
   \multirow{2}{*}{\textbf{GPCNS}~\cite{yang2024introducing}} & 
Improving the performance of continual learning.& GPCNS enhances plasticity by utilizing gradient information from old tasks.\\
   
%    \hline
%    \multirow{2}{*}{\makecell{\textbf{Bayesian}\\ \textbf{Adaptation}~\cite{thapabayesian}}} & 
% Improving the performance of continual learning.& Non-parametric Bayesian adapts the width through a conjugate Bernoulli process.\\
   
   \hline
   \multirow{2}{*}{\textbf{CILA}~\cite{wen2024provable}} & 
Improving the performance of continual learning.& CILA proposes an adaptive distillation coefficient and theoretical performance guarantees.\\
   
   \hline
   \multirow{2}{*}{\textbf{POCL}~\cite{wumitigating}} & 
Existing methods fail to fully leverage the inter-task dependencies.& POCL models task relationships through Pareto optimization and dynamically adjusts weights.\\
   
   \hline
   \multirow{2}{*}{\textbf{Powder}~\cite{piaofederated}} & 
Addressing the cross-task and cross-client knowledge transfer in federated continual learning.& Powder enables prompt-based dual knowledge transfer.\\
   
   \hline
   \multirow{2}{*}{\makecell{\textbf{AdaPromptCL}\\~\cite{kim2023one}}} & 
Addressing the challenge of task-specific semantic variations.& AdaPromptCL proposes dynamic semantic grouping and prompt adjustment.\\
   
   \hline
   \multirow{2}{*}{\textbf{LPR}~\cite{kim2023one}} & 
To reduce catastrophic forgetting and underfitting.& LPR adjusts the optimization geometry to balance the learning of new and old data.\\
   
   \hline
   \multirow{2}{*}{\textbf{InfLoRA}~\cite{liang2024inflora}} & 
To address the issue of forgetting old tasks when adapting to new tasks.& InfLoRA injects parameter reparameterization into pre-trained weights.\\
   
   \hline
   \multirow{2}{*}{\textbf{F-OAL}~\cite{zhuangf}} & 
To alleviate the issue of catastrophic forgetting in online class-incremental learning.& F-OAL proposes a forward online analytical learning method.\\
   
   \hline
   \multirow{2}{*}{\textbf{PRL}~\cite{shiprospective}} & 
Improving performance in non-sample class-incremental learning.& PRL aligns reserved space and latent space to adapt new class features to the reserved space.\\
   
   \hline
   \multirow{2}{*}{\textbf{CIL}~\cite{hao2024addressing}} & 
To address the issue of catastrophic forgetting.& CIL proposes the CIL-balanced classification loss and distribution margin loss.\\
   
   \hline
   \multirow{2}{*}{\textbf{DSSP}~\cite{yang2024domain}} & 
To eliminate the need for sample replay.& DSSP leverages domain sharing and task-specific prompt learning.\\
   
   \hline
   \multirow{2}{*}{\textbf{MRFA}~\cite{zhengmulti}} & 
To reduce catastrophic forgetting.& MRFA optimizes the entire layer margin by enhancing the features of review samples.\\
   
   \hline
   \multirow{2}{*}{\textbf{DARE}~\cite{jeeveswaran2024gradual}} & 
Improving the model's performance on old tasks.& DARE reduces representation drift through a three-stage training process.\\
   
   \hline
   \multirow{2}{*}{\textbf{EASE}~\cite{zhou2024expandable}} & 
To reduce catastrophic forgetting.& EASE constructs task-specific subspaces using lightweight adapters.\\



    \hline
    \end{tabularx}
  \label{CL_NonL_Method}%
\end{table*}%






\subsubsection{ConBench: MLLMs Answer Consistency Evaluation Benchmark}
MLLMs have made rapid progress in visual information perception and reasoning. Although MLLMs are capable of generating high-quality task prompt responses, simply modifying the prompt can lead to contradictory answers, even when the correct answer is provided. Specifically, under different prompt space sizes, these models lack consistency in answers to the same knowledge point, which significantly undermines trust in these models~\cite{li2023benchmarking,lin2023generating}. To ensure that MLLMs can predict correct and consistent answers when faced with various query formats, Yuan Zhang et al.~\cite{zhang2024unveiling} proposed a multimodal benchmark tool, ConBench, designed to comprehensively assess the consistency of MLLMs—specifically, their ability to provide the same answer to the same knowledge point across different query formats.

ConBench evaluates MLLMs by offering a diverse set of question formats, including true/false questions, multiple-choice questions, and limited visual question answering (VQA) problems. It also introduces two multidimensional evaluation metrics: 1)Discriminative Domain Evaluation Metric (ConScore[D]): Assesses consistency based on the accuracy of the model's answers to discriminative questions. 2)Generative Domain Evaluation Metric (ConScore[C]): Evaluates consistency by comparing the coherence between the model-generated captions and the discriminative answers.

More details of the scale and diversity of the ConBench benchmark and an overview of MLLM performance on the ConBench are provided in Section \ref{appendix_ConBench} of the Appendix.



\subsubsection{COMPBENCH: Comparative Reasoning Benchmark}
The ability to compare objects, scenes, or situations is crucial for decision-making and problem-solving in everyday life~\cite{masry2022chartqa,hudson2019gqa,lu2022learn}. Although this ability is widespread in human cognition, it has not been fully explored in the field of Artificial General Intelligence (AGI). Jihyung Kil et al.~\cite{kil2024compbench} proposed a benchmark, COMPBENCH, designed to evaluate the comparative reasoning ability of MLLMs.

More details of the scale and diversity of the COMPBENCH benchmark and an overview of MLLM performance on the COMPBENCH are provided in Section \ref{appendix_COMPBENCH} of the Appendix.





\subsubsection{Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs}
Similarly, in the context of the hallucination problem faced by MLLMs in visual-language understanding and generation tasks~\cite{rohrbach2018object,dai2022plausible,li2023evaluating,zhang2024groundhog,zhai2023halle,liu2023mitigating,you2023ferret,zhou2023analyzing,wang2023llm}, Peng Ding et al.~\cite{ding2024hallu} pointed out that previous studies have mainly focused on evaluating hallucinations on standard, undisturbed benchmarks, neglecting the prevalent interference inputs in the real world. This is crucial for a comprehensive evaluation of hallucinations in MLLMs. They proposed the first benchmark designed to evaluate hallucinations in MLLMs under disturbed inputs, called Hallu-PI, which includes seven types of disturbed scenarios: noise, blur, weather, digits, image stitching, image cropping, and prompt misdirection.

More details of the overview of MLLM performance on the Hallu-PI are provided in Section \ref{appendix_Hallu-PI} of the Appendix.



\subsubsection{ReForm-Eval: Evaluating MLLMs via Unified Re-Formulation of Task-Oriented Benchmarks}

MLLMs have made significant progress in understanding and reasoning about visual information~\cite{achiam2023gpt,zhu2023minigpt,liu2024visual,ye2023mplug,chen2023shikra}. However, this has posed challenges for the automatic evaluation of free-form text outputs from MLLMs. To leverage annotations from existing benchmarks and reduce the manual effort required to construct new benchmarks, Zejun Li et al.~\cite{li2024reform} proposed a method for reformatting existing benchmarks into a unified format compatible with MLLMs. Through systematic data collection and reformatting, they introduced the ReForm-Eval benchmark, which is designed to comprehensively and quantitatively assess the capabilities of MLLMs. This approach overcomes the structural differences between existing task-oriented multimodal benchmarks and MLLMs.

More details of the scale and diversity of the ReForm-Eval benchmark and an overview of MLLM performance on the ReForm-Eval are provided in Section \ref{appendix_ReForm-Eval} of the Appendix.




\subsubsection{VisionGraph: Graph Theory Problems Benchmark in Visual Context}

MLLMs have achieved significant success in visual understanding and reasoning~\cite{luo2023wizardmath,liu2024visual,chen2023shikra,achiam2023gpt}, but multimodal graph reasoning remains a challenging task~\cite{wang2024can}. It requires MLLMs to accurately understand graph structures and perform multi-step reasoning on visual graphs. To explore the ability of advanced MLLMs to address multimodal graph reasoning tasks, Yunxin Li et al.~\cite{li2024visiongraph} designed a benchmark called VisionGraph, which includes a series of graph reasoning problems aimed at testing MLLMs' understanding of graph structures and their multi-step reasoning capabilities.

More details of the overview of MLLM performance on the VisionGraph are provided in Section \ref{appendix_VisionGraph} of the Appendix.







\subsection{Applications of MLLMs}

Multimodal large models (MLLMs) have emerged as a significant direction in artificial intelligence research in recent years~\cite{zhan2024anygpt,chiang2023vicuna,jiang2023motiongpt,zhang2024motiongpt,huang2023visual,li2023large,liu2024improved,mu2024embodiedgpt}. With the rapid development of technologies such as natural language processing, computer vision, and speech recognition, single-modal intelligent systems can no longer meet the increasingly complex requirements of real-world applications~\cite{park2023generative,radford2021learning,rocamonde2023vision,sun2023aligning}. Multimodal learning, by integrating different types of data inputs, simulates the diversity and complexity of human information processing, offering more comprehensive and flexible intelligent services. The emergence of multimodal models like CLIP~\cite{radford2021learning} and GPT-4~\cite{achiam2023gpt} marks the rapid progress in this field, showcasing exceptional performance in tasks such as image generation, text generation, and cross-modal retrieval~\cite{snell2022offline,wang2022self,wei2022chain,betker2023improving,brooks2023instructpix2pix}. The ability to integrate information from various modalities enables MLLMs to demonstrate tremendous potential across multiple domains, particularly in applications such as autonomous driving, smart healthcare, and robotics, where they can significantly enhance system robustness and intelligence levels~\cite{brohan2023rt,driess2023palm,chen2023shikra,huang2023instruct2act,huang2022language,huang2022inner,huang2022inner,liu2024visual,chen2024llm,lv2024robomp}.With the increasing richness of data sources and the continuous improvement of computational power, multimodal large models (MLLMs) are expected to see widespread application in more practical scenarios. At the same time, with the deepening of interdisciplinary research, MLLMs will not only play a role in traditional AI tasks but will also expand into more edge domains, driving artificial intelligence from closed systems to a more open and intelligent ecosystem.

More details of the applications of MLLMs are provided in Section \ref{applications_MLLM} of the Appendix.

In summary, the application prospects of multimodal large models are vast. However, to fully unleash their potential, this requires the combined advancement of technological innovation and theoretical breakthroughs. It is anticipated that, in the future, with the ongoing progress in algorithms, hardware, and cross-domain collaboration, MLLMs will achieve more efficient and intelligent performance in a wider range of practical applications, further advancing the development of artificial intelligence.