

\section{Continual Learning in Multimodal Large Language Model}

\subsection{Preliminary}


In recent years, multimodal large models (MLLMs) have made significant progress and demonstrated powerful capabilities across various domains. However, as the scale of these models continues to expand, maintaining the long-term effectiveness of the model in ever-changing environments has become a critical challenge~\cite{young2014image,achiam2023gpt,anil2023palm,bai2023qwen,chen2015microsoft,chen2024internvl,panagopoulou2023x,dong2024internlm,fu2024video,goyal2017making,gurari2018vizwiz,liu2024llava}. Continual Learning (CL), as a key technology to address this challenge, aims to enable models to gradually learn new tasks without forgetting previously acquired knowledge in dynamic and evolving task and data environments. For multimodal large models, the task of continual learning is even more complex, as these models often need to handle vast amounts of data and perform complex computations. Continual learning requires models to continually update and expand their knowledge, which often demands substantial computational resources and storage space. Although existing research on multimodal large models and continual learning provides important theoretical foundations and experimental support~\cite{liu2025mmbench,luo2024cheap,yang2023dawn,team2023gemini,team2023internlm,touvron2023llama,wang2023cogvlm,wu2024parameter,yue2024mmmu}, the application of multimodal large models in continual learning still faces many challenges. In this section, we will delve into the existing innovations in multimodal large model continual learning and the evaluation benchmarks.


\subsection{Model Innovation}

To achieve multi-task continual learning in multimodal large models and avoid catastrophic forgetting, researchers have proposed numerous innovative frameworks and strategies~\cite{li2024coleclip,qi2024interactive,maharana2024adapt,luo2024mono,lester2021power,yan2022generative,villa2023pivot,he2024towards}. These innovations not only facilitate knowledge sharing and transfer between multiple tasks but also effectively address challenges such as catastrophic forgetting, modality conflicts, and computational resource constraints. This subsection will provide a detailed analysis and list the key framework and methodological innovations in the field of multimodal large model continual learning. Through this discussion, we aim to help readers gain a deeper understanding of the technological evolution and future development trends in multimodal continual learning.

\subsubsection{Framework Innovation}

Jiazuo Yu et al.~\cite{yu2024llms} introduced the Adapter-in-Adapter framework to enhance modality alignment and collaboration. They also proposed a flexible and scalable framework, PathWeave, which incorporates modality path switching and expansion capabilities. This allows MLLMs to continuously evolve on the modality used for X-modality reasoning, addressing the high computational burden when expanding to new modalities and reducing the dependency on large-scale joint pre-training.

Saurav Jha et al.~\cite{jha2024clap4clip} proposed the CLAP framework, which enhances the model's generalization ability and reduces forgetting through probabilistic fine-tuning. It is compatible with various prompt methods and strengthens the model's uncertainty estimation capabilities.

Longxiang Tang et al.~\cite{tang2025mind} proposed the DIKI framework, which efficiently preserves pre-trained knowledge through a residual mechanism and distribution-aware calibration. This approach addresses the problem of forgetting pre-trained knowledge in MLLMs during domain-category incremental learning, maintaining a balance between the model's adaptability to new tasks and the retention of old knowledge.

Xusheng Cao et al.~\cite{cao2024generative} proposed the GMM framework based on multimodal large models, which implements incremental learning through generated label text and feature matching. This approach reduces bias toward the current task and effectively minimizes forgetting.

Keon-Hee Park et al.~\cite{park2024pre} proposed the PriViLege framework, which effectively addresses catastrophic forgetting and overfitting in MLLMs through prompt functionality and knowledge distillation.

Fanhu Zeng et al.~\cite{zeng2024modalprompt} proposed the ModalPrompt framework, which implements continuous learning without data replay through bi-modal guided prototype prompts and knowledge transfer. This approach addresses the issue of forgetting old tasks when large multimodal models sequentially learn new tasks.

Emanuele Frascaroli et al.~\cite{frascaroli2024clip} proposed the CGIL framework, which combines prompt learning and latent generative replay. It uses VAEs to learn class-conditioned distributions and generate synthetic samples, effectively addressing the issue of catastrophic forgetting in multimodal large models during continual learning.

Yukun Li et al.~\cite{li2024coleclip} proposed the CoLeCLIP framework, which enhances the performance of multimodal large models in open-domain continual learning through joint learning of task prompts and cross-domain vocabularies. It achieves cross-domain vocabulary learning, maintaining a unified semantic space for multimodal large models, and reduces interference between tasks. The framework introduces task prompt learning, addressing domain differences and category associations, thereby improving the model's adaptability and discriminative ability for new tasks.

Biqing Qi et al.~\cite{qi2024interactive} proposed the ICL framework, which combines Vision Transformers (ViT) and MLLMs. By enabling interaction between a fast intuition model and a slow deep thinking model, the framework enhances the efficiency of continual learning in multimodal large language models.

Yuexiang Zhai et al.~\cite{zhai2023investigating} proposed the EMT framework to evaluate catastrophic forgetting in MLLMs. They found that moderate fine-tuning can improve continual learning performance, but excessive fine-tuning leads to a decline in performance and the emergence of hallucinations. This offers a new perspective for improving fine-tuning strategies in MLLMs.

Xiong Wang et al.~\cite{wang2024freeze} proposed the Freeze-Omni model, which implements a three-stage training strategy to enable speech input-output capabilities without unfreezing the LLM parameters. This approach addresses the issue of catastrophic forgetting when integrating the speech modality into multimodal LLMs, preserving the LLM's intelligence level and enabling low-latency speech-to-speech conversations.

Adyasha Maharana et al.~\cite{maharana2024adapt} proposed the Adapt-$infty$ framework, which optimizes model learning efficiency and reduces computational burden through dynamic data selection and a clustering-based permanent pruning strategy. This approach effectively mitigates catastrophic forgetting in multimodal large models.

Gen Luo et al.~\cite{luo2024mono} proposed Mono-InternVL, which integrates visual experts using a mixture-of-experts structure without altering the pre-trained language model. By introducing endogenous visual pretraining, it enables progressive learning of visual knowledge from noise to high-quality data through incremental learning, effectively preventing forgetting. This approach addresses the performance degradation and catastrophic forgetting issues that arise when expanding the visual and language capabilities of multimodal large language models.

Shanshan Zhong et al.~\cite{zhong2024moextend} proposed the MoExtend framework, which expands modality capabilities without adjusting the pre-trained model by integrating new experts. They designed a three-stage training process, including alignment, extension, and fine-tuning, to enable rapid modality adaptation. Additionally, they introduced an image localization score as a new scoring function to optimize multimodal sample selection. This approach addresses the issues of catastrophic forgetting and high training costs that arise when large language models are extended to multimodal tasks, particularly in the visual-language understanding domain.

Artemis Panagopoulou et al.~\cite{panagopoulou2023x} addressed the challenges faced by multimodal large language models in continual learning, particularly in self-supervised pretraining environments. They focused on how to effectively integrate and reason across knowledge from different modalities to overcome the performance limitations of traditional methods when handling multimodal data. They proposed the HiDe-Prompt framework, which is an scalable solution designed to align multiple modalities (such as images, 3D, audio, and video) with frozen large language models and enable cross-modal reasoning without joint optimization.



\subsubsection{Method Innovation}

Minh Le et al.~\cite{le2024mixture} revealed the connection between self-attention and mixture-of-experts, proposing the Non-linear Residual Gate (NoRGa) to enhance the continual learning performance of multimodal large language models.

Zangwei Zheng et al.~\cite{gaostabilizing} proposed the ZAF method, which preserves knowledge through zero-shot stability regularization. They introduced the EMA-based parameter-efficient EMA-LoRA architecture, achieving the decoupling of learning and forgetting.

Huancheng Chen et al.~\cite{chen2024dual} proposed DualLoRA, which utilizes orthogonal and residual low-rank adapters along with a dynamic memory mechanism to balance model stability and plasticity, thereby improving the efficiency and effectiveness of continual learning in multimodal large language models.


Weicai Yan et al.~\cite{yan2024low} proposed the Low-Rank Prompt Interaction (LPI) method, which enhances inter-modal and inter-task interactions through low-rank decomposition and contrastive learning. They introduced task semantic distance to guide prompt learning, addressing the insufficient interaction between modalities and tasks in continual learning of multimodal large language models (MLLMs), thereby reducing catastrophic forgetting.

Didi Zhu et al.~\cite{zhu2024model} proposed the Model Tailor method, which alleviates catastrophic forgetting during fine-tuning by retaining most of the pre-trained parameters and only replacing a small number of fine-tuned parameters. This approach helps to mitigate the forgetting problem while improving performance on new tasks.

Tianxiang Hao et al.~\cite{chen2024quantized} proposed a quantized prompt technique, which uses quantization errors as a form of regularization. They designed an efficient quantization-aware training algorithm that enhances the model's generalization ability while reducing its size. This approach addresses the issues of overfitting and catastrophic forgetting in MLLMs during downstream tasks, as well as the high storage and inference costs associated with large models.

Noranart Vesdapunt et al.~\cite{vesdapunt2025hvclip} proposed HVCLIP, which transforms CLIP features into a high-dimensional vector space. Through strategies such as forgetting reduction, discrepancy reduction, and feature enhancement, HVCLIP addresses the catastrophic forgetting issue encountered during fine-tuning of MLLM pre-trained models like CLIP in unsupervised domain adaptation. This approach helps mitigate the loss of pre-trained knowledge, enhancing the model's ability to retain critical information while adapting to new tasks or domains.

Meng Cao et al.~\cite{cao2024continual} proposed a parameter-efficient tuning method that does not require rehearsal. This approach constructs intrinsic and contextual incremental embeddings to encode task-specific features and inter-task dependencies. By doing so, the model can continuously adapt to new tasks while retaining prior knowledge. This significantly alleviates the catastrophic forgetting problem in MLLMs, enhancing their ability to preserve knowledge from previous tasks while accommodating new ones.

Shikhar Srivastava et al.~\cite{srivastava2024improving} proposed and evaluated five MLLM continual learning methods aimed at mitigating linguistic forgetting. Their findings revealed that the best-performing method significantly enhanced both language and vision task performance while maintaining multimodal accuracy. 

Jingyang Qiao et al.~\cite{qiao2024llaca} proposed the LLaCA method, which dynamically adjusts the EMA weights to reduce forgetting and introduces an approximation mechanism to lower computational costs, thereby addressing the issue of catastrophic forgetting in MLLMs when learning new tasks.

Clea Rebillard et al.~\cite{rebillard2024continually} proposed the Continual Visual Mapping (CVM) method, which reduces forgetting and improves generalization by mapping the representations of small visual models to the knowledge space of a fixed large language model.

Marco Mistretta et al.~\cite{mistretta2024re} proposed the RE-tune method, which freezes the backbone of the model and trains adapters, using text prompts to guide training. This approach enables privacy-preserving, computationally efficient, and anti-forgetting incremental learning. It optimizes pre-trained multimodal biomedical models for incremental learning scenarios in chest X-ray multi-label classification, addressing challenges related to computational resources, data privacy, and catastrophic forgetting.

Yuliang Cai et al.~\cite{cai2024clumo} proposed the CluMo method, which employs a two-stage training and modality fusion prompt strategy to combine visual and textual modalities, thereby enhancing the performance of multimodal large models in continual learning and improving their ability to retain old knowledge.

Yiduo Guo et al.~\cite{guo2024efficient} proposed three strategies to overcome the stability gap, including multi-round pretraining on small-scale high-quality datasets, selecting high-quality sub-corpora for pretraining, and employing a data-mixing strategy using data similar to pretraining data. These strategies effectively enhanced the performance and adaptability of multimodal large language models in new domains.

Jinghan He et al.~\cite{he2023continual} proposed a task similarity-guided regularization and model expansion method, which effectively enhances the continual learning capability of multimodal large models.

Junhao Zheng et al.~\cite{zheng2024beyond} proposed the Fwd-Prompt method, which utilizes gradient projection techniques and a multimodal prompt pool to achieve anti-forgetting and positive transfer, without requiring old samples and with minimal parameter updates. This approach improves the performance of multimodal large models in multimodal continual learning tasks.

Yuliang Cai et al.~\cite{cai2024dynamic} proposed dynamic model expansion and task attention layers to adapt to different tasks, while employing knowledge distillation and experience replay to mitigate catastrophic forgetting in multimodal large models.

\cite{d2023multimodal} proposed an incremental learning strategy for multimodal large language models, the CPE-CLIP method. By using learnable prompts and regularization strategies, it achieves parameter-efficient transfer learning for multimodal large language models, reducing the parameter size and training costs, while enhancing the performance of few-shot class incremental learning in multimodal large models.

Zilun Zhang et al.~\cite{zhang2024preserving} proposed the model-agnostic self-uncompression method, TG, which decompresses knowledge into the training corpus to reduce forgetting. They also designed the TG-SFT strategy for supervised fine-tuning of MLLMs, addressing the common issue of catastrophic forgetting encountered during post-training or supervised fine-tuning (SFT) on domain-specific data for multimodal large models.

Ke Wang et al.~\cite{wang2024lines} proposed the LiNeS technique, which performs parameter updates with layer-specific depth differentiation, preserving the generalization ability of pretraining while improving fine-tuning task performance. This approach addresses the issue of forgetting prior knowledge during the fine-tuning of multimodal pre-trained models.

Brian Lester et al.~\cite{lester2021power} proposed an end-to-end learning soft prompt method, which adapts to new tasks by adjusting input prompts rather than the entire model parameters. This approach enhances the performance and domain adaptability of multimodal large language models in continual learning.

Runqi Wang et al.~\cite{wang2023attriclip} proposed an non-incremental learning method based on CLIP, called AttriCLIP. This method adapts to new tasks using an attribute lexicon and textual prompts, without the need for additional memory data, thereby enhancing the generalization and continual learning capabilities of multimodal large models in multimodal tasks.

Shipeng Yan et al.~\cite{yan2022generative} introduced pseudo-text replay and multimodal knowledge distillation to enhance negative sample diversity, align predictions between old and new models, and improve the performance of multimodal large models in multimodal continual learning tasks.

Andrea Cossu et al.~\cite{cossu2024continual} explored how multimodal large language models can reduce catastrophic forgetting in continual learning environments through continuous pretraining, while maintaining adaptability to new knowledge. They demonstrated the advantages of self-supervised pretraining in preserving old knowledge and proposed effective pretraining strategies.

James Seale Smith et al.~\cite{smith2023continual} proposed the C-LoRA method, which effectively mitigates catastrophic forgetting by performing continual adaptive low-rank adjustments in the cross-attention layers of multimodal large models. This approach adapts to new concepts through a self-regulating mechanism while preserving knowledge of old concepts.

Tao He et al.~\cite{he2024towards} introduced a lifelong scene graph generation task and a knowledge-aware contextual prompt learning strategy, enabling the model to effectively retain old knowledge in incremental learning. This approach addresses the issue of updating and forgetting old and new knowledge in multimodal large models during scene graph generation tasks.

Overall, with the continuous innovation of frameworks and methods for multimodal large model continual learning, researchers have proposed numerous effective frameworks and strategies, laying a solid foundation for the multitask continual learning of multimodal large models. These efforts collectively advance the continual learning capabilities of multimodal large models in dynamic environments.

\subsection{Benchmarks}

As the application of multimodal large models in continual learning increases, evaluating their continual learning capability has become a key issue. To comprehensively assess the continual learning performance of multimodal large models, benchmarks and evaluation frameworks have emerged. However, benchmarks specifically designed for continual learning in multimodal large models are still relatively scarce, and the relevant evaluation standards are still in the process of development. This section will analyze and list the few existing benchmarks for evaluating the continual learning capability of multimodal large models, exploring their design concepts, evaluation metrics, and applicability in different application scenarios.

\subsubsection{CoIN: Continual Instruction Tuning Benchmark}

MLLMs adapt to new tasks and users' evolving needs through instruction tuning. However, these models face challenges in adapting to the constantly changing knowledge requirements of users. To address this, Cheng Chen et al.~\cite{chen2024coin} proposed the CoIN benchmark to evaluate MLLMs' performance under the sequential instruction tuning paradigm. They also introduced the MoELoRA method to help MLLMs retain previous instruction alignment, reducing catastrophic forgetting. 

CoIN consists of 10 commonly used datasets, covering 8 different task categories, ensuring diversity in both instructions and tasks. Table~\ref{coin_tab1} provides a detailed list of the datasets included in the CoIN benchmark, along with their corresponding instruction types, training sample sizes, and test sample sizes. The datasets cover a variety of task types, including Referring Expression Comprehension (REC), Classification, Image Question Answering (IQA), and Knowledge Grounded IQA, among others. Each task has two versions of instructions, Type1 and Type2, to ensure the diversity and comprehensiveness of the evaluation. 

Furthermore, CoIN evaluates MLLMs from two perspectives: 1) Truth Alignment. The ability to generate the correct result in the desired format to follow task instruc- tion is the basic requirement for instruction tuning. 2) Reasoning Capability. The performance of MLLMs depends not only on the instruction following but also on the knowledge maintained in MLLMs.
Three metrics are used to measure the performance of MLLMs: 1) Backward Transfer (BWT): Measures the catastrophic forgetting that occurs after learning all tasks. 2) Mean Average Accuracy (MAA): Assesses the model's performance throughout the entire training process.


\begin{table*}[ht]
	\caption{The statistic of collected datasets and instructions in CoIN benchmark.~\cite{chen2024coin}}
	\label{coin_tab1}
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|c|c|cc}
			\hline
			\textbf{Task} &
			\textbf{Dataset} &
			\textbf{Instruction} &
			\makecell[c]{\textbf{Train} \\ \textbf{Number}}
			& \makecell[c]{\textbf{Test} \\ \textbf{Number}} \\
			\hline
			\multicolumn{1}{c|}{\makecell[c]{\textbf{Grounding}}} & \makecell[c]{RefCOCO\;\;\; \\ RefCOCO+ \\ RefCOCOg\;} & \makecell[c]{Please provide the bounding \\ box coordinate of the region \\ this sentence describes} &  55k & 31k \\ \hline
			
			\multicolumn{1}{c|}{\textbf{Classification}} & ImageNet & \makecell[c]{What is the object in the image? \\ Answer the question using a \\ single word or phrase} &  129k & 5k \\ \hline
			
			\textbf{Image Question Answering (IQA)} & VQAv2 & \makecell[c]{Answer the question using a \\ single word or phrase} &  82k & 107k \\ \hline
			
			\makecell[c]{\textbf{Knowledge Grounded IQA}} & ScienceQA & \makecell[c]{Answer with the option's letter \\ from the given choices directly} &  12k & 4k \\ \hline
			
			\makecell[c]{\textbf{Reading Comprehension IQA}} & \makecell[c]{TextVQA} & \makecell[c]{Answer the question using a \\ single word or phrase} &  34k & 5k \\ \hline
			
			\makecell[c]{\textbf{Visual Reasoning IQA}} & GQA & \makecell[c]{Answer the question using a \\ single word or phrase} &  72k & 1k \\ \hline
			
			\makecell[c]{\textbf{Blind People} IQA} & VizWiz & \makecell[c]{Answer the question using a \\ single word or phrase} &  20k & 8k \\ \hline
			
			\makecell[c]{\textbf{OCR IQA}} & OCR-VQA & \makecell[c]{Answer the question using a \\ single word or phrase} &  165k & 100k \\ 
			\bottomrule
		\end{tabular}
	}
\end{table*}


Table~\ref{coin_tab2} shows the results of Truth Alignment ability for different methods on the CoIN benchmark. These methods include multitask training, zero-shot learning, and fine-tuning. The table lists the performance of each method on individual tasks, as well as the average performance across all tasks, including metrics such as MAA, and BWT.

\begin{table*}
	\caption{The results evaluating the \textit{Truth Alignment} ability are presented below. 
		The first line of \textbf{Sequential Finetune} are the results for each task evaluated when just tuned on the corresponding task, and the second line displays the final results of each task after fine-tuning on the last task.~\cite{chen2024coin}}
	\label{coin_tab2}
	\renewcommand\arraystretch{1.3}
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|cccccccc|cc}
			\hline
			\multirow{2}{*}{\textbf{MLLM}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{8}{c|}{\textbf{Accuracy on Each Task}} & \multicolumn{2}{c}{\textbf{Overall Results}} \\ \cline{3-12} 
			&  & ScienceQA & TextVQA & ImageNet & GQA & VizWiz & Grounding & VQAV2 & OCR-VQA & MAA & BWT \\ \hline
			
			\multirow{4}{*}{LLaVA~\cite{liu2024visual}} & Multi-task & 56.77 & 49.35 & 95.55 & 56.65 & 53.90 & 30.09 & 59.50 & 55.65 & \textbf{57.18} & - \\  \cmidrule(r){2-12}
			& Zero-shot & 49.91 & 2.88 & 0.33 & 2.08 & 0.90 & 0.00 & 0.68 & 0.17 & 7.12 & -  \\ \cmidrule(r){2-12}
			
			& \multirow{2}{*}{\makecell[c]{Sequential \\ Finetune}} & \textbf{82.45} & 49.99 & \textbf{96.05} & 56.40 & \textbf{55.45} & 31.27 & 62.20 & 57.08 & \multirow{2}{*}{32.97} & \multirow{2}{*}{-32.62} \\ 
			&  & 21.26 & 28.74 & 10.25 & 36.78 & 32.45 & 0.83 & 42.50 & 57.08  \\ \hline
			
			\multirow{4}{*}{Qwen-VL~\cite{bai2023qwen}} & Multi-task & 25.70 & 60.88 & 17.05 & 56.77 & 35.58 & 6.78 & 68.67 & \textbf{63.50} & 41.87 & - \\ \cmidrule(r){2-12}
			& Zero-shot & 64.56 & 48.15 & 11.82 & 44.50 & 9.57 & 0.00 & 64.10 & 27.50 & 33.78 & - \\ \cmidrule(r){2-12}
			& \multirow{2}{*}{\makecell[c]{Sequential \\ Finetune}} & 67.69 & \textbf{66.36} & 53.70 & \textbf{59.30} & 36.38 & \textbf{63.10} & \textbf{71.00} & 47.80 & \multirow{2}{*}{43.35} & \multirow{2}{*}{-16.94} \\
			& & 31.05 & 42.45 & 29.57 & 55.57 & 15.30 & 40.33 & 67.75 & 47.80 &  & \\ \hline
			
			\multirow{4}{*}{MiniGPT-v2~\cite{chen2023minigpt}} & Multi-task & 43.55 & 19.24 & 10.57 & 28.43 & 41.62 & 0.00 & 27.12 & 1.45 & 21.50 & -  \\ \cmidrule(r){2-12}
			& Zero-shot & 32.16 & 6.83 & 0.07 & 11.58 & 35.20 & 0.00 & 12.20 & 0.03 & 12.26 & - \\ \cmidrule(r){2-12}
			& \multirow{2}{*}{\makecell[c]{Sequential \\ Finetune}} & 28.81 & 10.40 & 7.25 & 31.55 & 41.35 & 0.00 & 36.10 & 6.15 & \multirow{2}{*}{25.45} & \multirow{2}{*}{6.04} \\
			& & 44.35 & 29.89 & 11.90 & 36.95 & 42.58 & 0.00 & 38.10 & 6.15 &  &  \\ \hline 
		\end{tabular}
	}
\end{table*}


Table~\ref{coin_tab3} presents the results of Reasoning Capability for different methods on the CoIN benchmark. Similar to Table~\ref{coin_tab2}, these results provide a comprehensive evaluation of the model's understanding and reasoning capabilities across different tasks.

\begin{table*}
	\caption{The evaluation results of \textit{Reasoning Capability} are presented below. ~\cite{chen2024coin}}
	\label{coin_tab3}
	\renewcommand\arraystretch{1.3}
	\renewcommand\tabcolsep{4.0pt}
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|cccccccc|cc}
			\hline
			\multirow{2}{*}{\textbf{MLLM}} &
			\multirow{2}{*}{\textbf{Method}} &
			\multicolumn{8}{c|}{\textbf{Accuracy on Each Task}} &
			\multicolumn{2}{c}{\textbf{Overall Results}} \\ \cline{3-12}
			&  & ScienceQA & TextVQA & ImageNet & GQA & VizWiz & Grounding & VQAV2 & OCR-VQA & MAA & BWT \\ 
			\hline
			\multirow{4}{*}{LLaVA~\cite{liu2024visual}} & Multi-task & 80 & 75 & \textbf{97} & 72 & 42 & 86 & 73 & 79 & 75.50 & - \\ \cmidrule(r){2-12}
			& Zero-shot & 93 & \textbf{83} & 69 & 64 & 48 & 35 & 64 & 66 & 65.25& - \\ \cmidrule(r){2-12}
			& \multirow{2}{*}{\makecell[c]{Sequential \\ Finetune}} & 92 & 75 & \textbf{97} & 72 & 42 & 58 & 75 & 78 & \multirow{2}{*}{71.28}  & \multirow{2}{*}{-10.88} \\
			& & 82 & 74 & 55 & 56 & 47 & 52 & 58 & 78  \\ 
			% &\multirow{2}{*}{MoELoRA} & 86 & 76 & 98 & 72 & 59 & 59 & 70 & 74 & \multirow{2}{*}{70.11} & \multirow{2}{*}{65.38} & \multirow{2}{*}{-8.88}  \\ 
			% & & 82 & 76 & 55 & 61 & 54 & 58 & 63 & 74  \\ 
			\cline{1-12}
			
			\multirow{4}{*}{Qwen-VL~\cite{bai2023qwen}} & Multi-task & \textbf{98} & 82 & 68 & 77 & 50 & 51 & \textbf{82} & \textbf{88} & 74.50 & - \\ \cmidrule(r){2-12}
			& Zero-shot & 97 & 81 & 78 & 74 & \textbf{54} & 58 & 81 & 74 & 74.63 & - \\ \cmidrule(r){2-12}
			& \multirow{2}{*}{\makecell[c]{Sequential \\ Finetune}} & 96 & \textbf{83} & 86 & \textbf{78} & 51 & 82 & \textbf{82} & 75 & \multirow{2}{*}{\textbf{80.97}} & \multirow{2}{*}{-3.25} \\
			& &  95 & 78  & 77 & 77  & 47 & 76 & \textbf{82}  & 75  \\ 
			% &\multirow{2}{*}{MoELoRA} &   &   &   &   &   &   &  &   & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{}  \\ 
			% &&   &   &   &   &   &   &   &  \\
			\cline{1-12}
			
			\multirow{4}{*}{MiniGPT-v2~\cite{chen2023minigpt}} & Multi-task & 96 & 76 & 58 & 62 & 44 & 89 & 63 & 59 & 68.38 & - \\ \cmidrule(r){2-12}
			& Zero-shot & \textbf{98} & 72 & 48 & 63 & 48 & 80 & 64 & 61 & 66.75 & - \\ \cmidrule(r){2-12}
			& \multirow{2}{*}{\makecell[c]{Sequential \\ Finetune}} & 97 & 71 & 55 & 61 & 44 & 91 & 63 &  52 & \multirow{2}{*}{75.05}  & \multirow{2}{*}{0.00} \\
			& & 89 & 73 & 59 & 60 & 44 & \textbf{94} & 63 & 52 \\ 
			% \cdashline{2-13}
			% &\multirow{2}{*}{MoELoRA} &   &   &   &   &   &   &  &   & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{}  \\ 
			% &&   &   &   &   &   &   &   &  \\
			\hline
		\end{tabular}
	}
\end{table*}


Table~\ref{coin_tab4} explores the impact of different data volumes on MLLMs' instruction following ability on the CoIN benchmark. By randomly selecting varying proportions of samples from each dataset, Table~\ref{coin_tab4} illustrates how the volume of data affects the model's performance.

\begin{table*}[ht]
	\caption{The results of LLaVA about \textbf{different data volumes} are presented below. ~\cite{chen2024coin}}
	\label{coin_tab4}
	\renewcommand\arraystretch{1.3}
	\renewcommand\tabcolsep{4.0pt}
	\centering
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|cccccccc|cc}
			\hline
			\multirow{2}{*}{\textbf{Volume}} &
			\multicolumn{8}{c|}{\textbf{Accuracy on Each Task}} &
			\multicolumn{2}{c}{\textbf{Overall Results}} \\ \cline{2-11}
			& ScienceQA & TextVQA & ImageNet & GQA & VizWiz & Grounding & VQAV2 & OCR-VQA & MAA & BWT \\ 
			\hline
			\multirow{2}{*}{0.1} & 70.00 & 42.88 & 93.45 & 36.93 & 43.7 & 3.73 & 40.48 & 45.62 & \multirow{2}{*}{30.27} & \multirow{2}{*}{-16.17} \\
			& 53.71 & 32.62 & 5.38 & 33.50 & 36.98 & 2.85 & 36.77 & 45.62  \\  \cmidrule(r){1-11}
			
			\multirow{2}{*}{0.2} & 69.86 & 46.86 & 94.38 & 44.98 & 44.15 & 4.81 & 32.55 & 52.10 & \multirow{2}{*}{30.33} & \multirow{2}{*}{-19.89} \\
			& 41.12 & 33.25 & 5.53 & 33.80 & 25.85 & 1.77 & 37.10 & 45.62  \\ \cmidrule(r){1-11}
			
			\multirow{2}{*}{0.4} & 75.33 & 47.06 & 94.95 & 52.95 & 50.77 & 10.25 & 56.73 & 55.33 & \multirow{2}{*}{33.18} & \multirow{2}{*}{-24.85} \\
			& 49.96 & 23.60 & 7.22 & 36.12 & 33.05 & 0.09 & 39.20 & 55.33  \\ \cmidrule(r){1-11}
			
			\multirow{2}{*}{0.6} & 78.09 & 47.65 & 95.85 & 55.93 & 53.08 & 10.00 & 59.17 & 46.33 & \multirow{2}{*}{31.47} & \multirow{2}{*}{-32.57} \\
			& 27.42 & 19.54 & 7.03 & 33.52 & 13.15 & 0.05 & 38.48 & 46.33 \\ \cmidrule(r){1-11}
			
			\multirow{2}{*}{0.8} & 80.02 & 48.13 & 95.45 & 54.00 & 49.85 & 28.33 & 58.35 & 56.67 & \multirow{2}{*}{30.00} & \multirow{2}{*}{-33.60} \\
			& 11.74 & 16.94 & 8.85 & 32.62 & 35.50 & 0.00 & 39.67 & 56.67  \\ \cmidrule(r){1-11}
			
			\multirow{2}{*}{1.0} & \textbf{82.45} & \textbf{49.99} & \textbf{96.05} & \textbf{56.40} & \textbf{55.45} & \textbf{31.27} & \textbf{62.20} & \textbf{57.08} & \multirow{2}{*}{\textbf{32.97}} & \multirow{2}{*}{-32.62} \\
			& 21.26 & 28.74 & 10.25 & 36.78 & 32.45 & 0.83 & 42.50 & \textbf{57.08}
			\\ \hline
		\end{tabular}
	}
\end{table*}



\subsubsection{CliMB: The Continual Learning in Multimodality Benchmark}


Existing multimodal large language models are typically fine-tuned separately for each downstream task, requiring a new model to be fine-tuned and stored for each task. In contrast, multitask learning involves training on a fixed set of tasks, but it cannot dynamically learn new tasks. To address this, Tejas Srinivasan et al.~\cite{srinivasan2022climb} proposed the CLiMB benchmark, designed to study the continual learning challenges faced by multimodal large models in multimodal tasks and to systematically evaluate how upstream continual learning can quickly generalize to new multimodal and unimodal tasks. The CLiMB benchmark includes vision-and-language input tasks, such as VQAv2, NLVR2, SNLI-VE, and VCR. Additionally, the evaluation phase of the CLiMB benchmark includes: 1) Upstream Continual Learning: The model is trained on a series of vision-language tasks, and its ability to forget old tasks and transfer knowledge to new tasks is evaluated after each task. 2) Downstream Low-Shot Transfer: After training on upstream tasks, the model's adaptability to new multimodal and unimodal tasks with limited samples is assessed.


Table~\ref{climb_tab1} presents the results of different continual learning algorithms for multimodal large models in upstream multimodal task learning. It compares the upstream knowledge transfer ($\knowledgetransfer{i}$) relative to direct fine-tuning, along with the task scores $[S_{\algorithm}^i]$.

\begin{table*}[t]
\renewcommand\arraystretch{1.2}
    \centering
    \caption{
        Upstream Knowledge Transfer $\knowledgetransfer{i}$ relative to direct fine-tuning on each task, along with task score $[S_{\algorithm}^i]$ (\%), for different CL algorithms $\algorithm$ applied to \vilt.
        No CL algorithms achieve notable positive Knowledge Transfer, while the majority in fact \emph{hurt} learning of new tasks.~\cite{srinivasan2022climb}
    }
    \begin{small}
    \begin{tabular}{l|r|r|r|r|rr}
    \hline
    \multirow{2}{*}{Alg $\algorithm$} & Params &  \multicolumn{1}{c|}{Task 1} & \multicolumn{1}{c|}{Task 2} & \multicolumn{1}{c|}{Task 3} & \multicolumn{1}{c}{Task 4} \\
    & Trained &  \multicolumn{1}{c|}{VQAv2} & \multicolumn{1}{c|}{NLVR2} & \multicolumn{1}{c|}{SNLI-VE} & \multicolumn{1}{c}{VCR} \\
    \hline
    Direct FT & 100\% &  [67.70]	& [73.07] & [76.31] & [61.31] \\
    \hline
    SeqFT~\cite{yigit2023enhancing}  & 100\% &  \transfer{0.13}{67.79}	& \transfer{-1.80}{72.66} & \transfer{-3.33}{74.89} & \transfer{-5.09}{59.47} \\
    Frozen Enc~\cite{srinivasan2022climb} & 7.88\% &  \transfer{-14.10}{58.15} & \transfer{-40.78}{63.66} & \transfer{-15.98}{69.45} & \transfer{-53.47}{41.90} \\
    Frozen B9~\cite{srinivasan2022climb} & 25.92\% &  \transfer{-0.58}{67.30} & \transfer{-0.58}{72.94} & \transfer{-3.31}{74.90} & \transfer{-15.49}{55.69} \\
    ER~\cite{chaudhry2019tiny} & 100\% &  \transfer{0.26}{67.87} & \transfer{0.56}{73.20} & \transfer{-2.89}{75.08} & \transfer{-4.45}{59.70} \\
    EWC~\cite{kirkpatrick2017overcoming} & 100\% &  \transfer{0.20}{67.84} & \transfer{-2.79}{72.39} & \transfer{-4.52}{74.38} & \transfer{-4.86}{59.55} \\
    Adapters~\cite{houlsby2019parameter} & 13.02\% &  \textbf{\transfer{0.59}{68.10}} & \textbf{\transfer{2.55}{73.66}} & \textbf{\transfer{-0.56}{76.08}} & \textbf{\transfer{-0.36}{61.18}} \\
    \hline
    \end{tabular}
    \end{small}
\label{climb_tab1}
\end{table*}



Table~\ref{climb_tab2} presents the Forgetting Transfer results for six continual learning algorithms applied to multimodal large models. It shows the performance degradation on previous tasks after training on subsequent tasks, indicating the extent of catastrophic forgetting.


\begin{table*}[ht]
\renewcommand\arraystretch{1.2}
    \centering
    \caption{Full numbers for forgetting transfer $\forgetting{j}{i}$ of previously seen tasks for each CL algorithm. We also show the  transfer score $[S_\algorithm^{j \leftarrow i}]$ when evaluated on that task after training on future task $i$. The first row contains task score $[S_\algorithm^j]$ after originally training on $j^{th}$ task.~\cite{srinivasan2022climb}
    }
\begin{tabular}{l|r|r|r}
    \multicolumn{4}{c}{CL Algorithm: Sequential Fine-tuning} \\
    \hline
    %\multicolumn{7}{c}{Comparison of Algorithms (Fixed Task Order, Encoder=ViLT)}\\
    %\toprule
    \multirow{2}{*}{\backslashbox{Checkpoint}{Evaluated on}} & \multicolumn{1}{c|}{Task 1} & \multicolumn{1}{c|}{Task 2} & \multicolumn{1}{c}{Task 3} \\
    &  \multicolumn{1}{c|}{VQAv2} & \multicolumn{1}{c|}{NLVR2} & \multicolumn{1}{c}{SNLI-VE} \\
    \hline
    After training on that task & [67.79]	& [72.66] & [74.89] \\
    \hline
    Task 2: NLVR2  &  \transfer{40.97}{40.02}	& - & - \\
    Task 3: SNLI-VE &   \transfer{39.25}{41.18} & \transfer{43.81}{62.73} & - \\
    Task 4: VCR &  \transfer{63.90}{24.47} & \transfer{93.74}{51.24} & \transfer{89.93}{37.52} \\
    \hline
    % \\
    \multicolumn{4}{c}{CL Algorithm: Frozen Encoder} \\
    \hline
    %\multicolumn{7}{c}{Comparison of Algorithms (Fixed Task Order, Encoder=ViLT)}\\
    %\toprule
    \multirow{2}{*}{\backslashbox{Checkpoint}{Evaluated on}} & \multicolumn{1}{c|}{Task 1} & \multicolumn{1}{c|}{Task 2} & \multicolumn{1}{c}{Task 3} \\
    &  \multicolumn{1}{c|}{VQAv2} & \multicolumn{1}{c|}{NLVR2} & \multicolumn{1}{c}{SNLI-VE} \\
    \hline
    After training on that task & [58.15]	& [63.66] & [69.45] \\
    \hline
    Task 2: NLVR2  &  \transfer{-0.38}{58.37}	& - & - \\
    Task 3: SNLI-VE &   \transfer{-0.38}{58.37} & \transfer{-0.31}{63.70} & - \\
    Task 4: VCR &  \transfer{-0.38}{58.37} & \transfer{-0.42}{63.72} & \transfer{0.00}{69.45} \\
    \hline
    % \\
    \multicolumn{4}{c}{CL Algorithm: Frozen Bottom-9} \\
    \hline
    %\multicolumn{7}{c}{Comparison of Algorithms (Fixed Task Order, Encoder=ViLT)}\\
    %\toprule
    \multirow{2}{*}{\backslashbox{Checkpoint}{Evaluated on}} & \multicolumn{1}{c|}{Task 1} & \multicolumn{1}{c|}{Task 2} & \multicolumn{1}{c}{Task 3} \\
    &  \multicolumn{1}{c|}{VQAv2} & \multicolumn{1}{c|}{NLVR2} & \multicolumn{1}{c}{SNLI-VE} \\
    \hline
    After training on that task & [67.30]	& [72.94] & [74.90] \\
    \hline
    Task 2: NLVR2  &  \transfer{16.97}{55.90}	& - & - \\
    Task 3: SNLI-VE &   \transfer{21.36}{52.93} & \transfer{29.32}{66.21} & - \\
    Task 4: VCR &  \transfer{71.61}{19.11} & \transfer{78.52}{54.93} & \transfer{35.01}{60.34} \\
    \hline
    % \\
    \multicolumn{4}{c}{CL Algorithm: Experience Replay} \\
    \hline
    %\multicolumn{7}{c}{Comparison of Algorithms (Fixed Task Order, Encoder=ViLT)}\\
    %\toprule
    \multirow{2}{*}{\backslashbox{Checkpoint}{Evaluated on}} & \multicolumn{1}{c|}{Task 1} & \multicolumn{1}{c|}{Task 2} & \multicolumn{1}{c}{Task 3} \\
    &  \multicolumn{1}{c|}{VQAv2} & \multicolumn{1}{c|}{NLVR2} & \multicolumn{1}{c}{SNLI-VE} \\
    \hline
    After training on that task & [67.87]	& [73.20] & [75.08] \\
    \hline
    Task 2: NLVR2  &  \transfer{12.88}{59.13}	& - & - \\
    Task 3: SNLI-VE &   \transfer{12.96}{59.07} & \transfer{17.10}{69.23} & - \\
    Task 4: VCR &  \transfer{43.62}{38.27} & \transfer{78.27}{55.04} & \transfer{33.45}{61.11} \\
    \hline
    % \\
    \multicolumn{4}{c}{CL Algorithm: Elastic Weight Consolidation} \\
    \hline
    %\multicolumn{7}{c}{Comparison of Algorithms (Fixed Task Order, Encoder=ViLT)}\\
    %\toprule
    \multirow{2}{*}{\backslashbox{Checkpoint}{Evaluated on}} & \multicolumn{1}{c|}{Task 1} & \multicolumn{1}{c|}{Task 2} & \multicolumn{1}{c}{Task 3} \\
    &  \multicolumn{1}{c|}{VQAv2} & \multicolumn{1}{c|}{NLVR2} & \multicolumn{1}{c}{SNLI-VE} \\
    \hline
    After training on that task & [67.84] &	[72.39]	& [74.38] \\
    \hline
    Task 2: NLVR2  &  \transfer{39.81}{40.83}	& - & - \\
    Task 3: SNLI-VE &   \transfer{31.52}{46.46} & \transfer{25.73}{66.66} & - \\
    Task 4: VCR &  \transfer{65.25}{23.58} & \transfer{81.03}{54.25} & \transfer{73.61}{43.34} \\
    \hline
    % \\
    \multicolumn{4}{c}{CL Algorithm: Adapters} \\
    \hline
    %\multicolumn{7}{c}{Comparison of Algorithms (Fixed Task Order, Encoder=ViLT)}\\
    %\toprule
    \multirow{2}{*}{\backslashbox{Checkpoint}{Evaluated on}} & \multicolumn{1}{c|}{Task 1} & \multicolumn{1}{c|}{Task 2} & \multicolumn{1}{c}{Task 3} \\
    &  \multicolumn{1}{c|}{VQAv2} & \multicolumn{1}{c|}{NLVR2} & \multicolumn{1}{c}{SNLI-VE} \\
    \hline
    After training on that task & [68.10]	& [73.66] & [76.08] \\
    \hline
    Task 2: NLVR2  &  \transfer{-0.01}{68.11}	& - & - \\
    Task 3: SNLI-VE &   \transfer{0.04}{68.07} & \transfer{3.51}{72.83} & - \\
    Task 4: VCR &  \transfer{0.67}{67.64} & \transfer{6.48}{72.13} & \transfer{0.89}{75.70} \\
    \hline
    \end{tabular}
\label{climb_tab2}
\end{table*}




Table~\ref{climb_tab3} illustrates the impact of different upstream task sequences on the upstream knowledge forgetting of multimodal large models.

\begin{table*}[ht]
\renewcommand\arraystretch{1.2}
    \centering
    \caption{Full forgetting results with different task orders.~\cite{srinivasan2022climb}
    }
\begin{tabular}{l|r|r|r}
    \multicolumn{4}{c}{Task Order: VQAv2 $\rightarrow$ NLVR2 $\rightarrow$ SNLI-VE $\rightarrow$ VCR} \\
    \hline
    %\multicolumn{7}{c}{Comparison of Algorithms (Fixed Task Order, Encoder=ViLT)}\\
    %\toprule
    \multirow{2}{*}{\backslashbox{Checkpoint}{Evaluated on}} & \multicolumn{1}{c|}{Task 1} & \multicolumn{1}{c|}{Task 2} & \multicolumn{1}{c}{Task 3} \\
    &  \multicolumn{1}{c|}{VQAv2} & \multicolumn{1}{c|}{NLVR2} & \multicolumn{1}{c}{SNLI-VE} \\
    \hline
    After training on that task & [67.79]	& [72.66] & [74.89] \\
   \hline
    Task 2: NLVR2  &  \transfer{40.97}{40.02}	& - & - \\
    Task 3: SNLI-VE &   \transfer{39.25}{41.18} & \transfer{43.81}{62.73} & - \\
    Task 4: VCR &  \transfer{63.90}{24.47} & \transfer{93.74}{51.24} & \transfer{89.93}{37.52} \\
    \hline
    % \\
    \multicolumn{4}{c}{Task Order: SNLI-VE $\rightarrow$ VCR $\rightarrow$ VQAv2 $\rightarrow$ NLVR2} \\
    \hline
    %\multicolumn{7}{c}{Comparison of Algorithms (Fixed Task Order, Encoder=ViLT)}\\
    %\toprule
    \multirow{2}{*}{\backslashbox{Checkpoint}{Evaluated on}} & \multicolumn{1}{c|}{Task 1} & \multicolumn{1}{c|}{Task 2} & \multicolumn{1}{c}{Task 3} \\
    &  \multicolumn{1}{c|}{SNLI-VE} & \multicolumn{1}{c|}{VCR} & \multicolumn{1}{c}{VQAv2} \\
    \hline
    After training on that task & [76.29]	& [60.75] & [63.27] \\
    \hline
    Task 2: VCR  &  \transfer{84.50}{39.99}	& - & - \\
    Task 3: VQAv2 &   \transfer{85.86}{39.40} & \transfer{91.47}{28.05} & - \\
    Task 4: NLVR2 &  \transfer{77.56}{42.97} & \transfer{86.11}{29.97} & \transfer{41.94}{36.73} \\
    \hline
    % \\
    \multicolumn{4}{c}{Task Order: NLVR2 $\rightarrow$ VQAv2 $\rightarrow$ VCR $\rightarrow$ SNLI-VE} \\
    \hline
    %\multicolumn{7}{c}{Comparison of Algorithms (Fixed Task Order, Encoder=ViLT)}\\
    %\toprule
    \multirow{2}{*}{\backslashbox{Checkpoint}{Evaluated on}} & \multicolumn{1}{c|}{Task 1} & \multicolumn{1}{c|}{Task 2} & \multicolumn{1}{c}{Task 3} \\
    &  \multicolumn{1}{c|}{NLVR2} & \multicolumn{1}{c|}{VQAv2} & \multicolumn{1}{c}{VCR} \\
    \hline
    After training on that task & [73.25]	& [66.55] & [59.10] \\
    \midrule
    Task 2: VQAv2  &  \transfer{58.06}{59.68}	& - & - \\
    Task 3: VCR &   \transfer{90.63}{52.16} & \transfer{68.69}{20.87} & - \\
    Task 4: SNLI-VE &  \transfer{91.75}{51.90} & \transfer{62.59}{24.94} & \transfer{34.04}{47.51} \\
    \hline
    \end{tabular}
    \label{subtab:askorder-forgetting}
\label{climb_tab3}
\end{table*}


\subsubsection{COAST: Continual Instruction Tuning Benchmark}


An ideal MLLM should be able to continuously adjust to new tasks in the face of task flow distributions across different domains, new capabilities, and new datasets, while minimizing forgetting of prior knowledge. However, most existing MLLMs are limited to single-task adaptation and lack performance evaluation standards for continual learning of new tasks. To comprehensively assess MLLMs' continual learning performance across different domains, capabilities, and datasets, Meng Cao et al.~\cite{cao2024continual} proposed the COAST benchmark. COAST includes three incremental learning settings: 1) Domain-incremental: Simulates scenarios where MLLMs continuously adapt to different domains.
Capability-incremental: Evaluates the ability of MLLMs to progressively acquire and integrate new capabilities. 2) Dataset-incremental: Assesses the ability of MLLMs to adapt to and generalize across varying dataset distributions. 3) By chaining and reusing existing benchmark tests, the COAST benchmark creates a streaming task distribution to evaluate the performance of MLLMs when continually learning new tasks.


Table~\ref{coast_tab1} presents the average accuracy (Avg.↑) and average forgetting rate (Fgt.↓) of different continual learning methods under the COAST-domain setting. These results reflect the performance of multimodal large models on new tasks and their ability to retain performance on previous tasks while learning new ones.

\begin{table*}[t]
\renewcommand\arraystretch{1.2}
\centering
%\small
%\setlength{\tabcolsep}{1.3mm}
%\setlength{\arraycolsep}{2pt}
%\caption{\textbf{Continual instruction tuning results (\%) on COAST-Domain.} ``Doc.", ``Med.", ``Avg." and ``Fgt." represent document, medical, average accuracy and average forgetting, respectively. ``Reh.", ``Seq." and ``Joint" denote rehearsal, sequential and joint training.}
\caption{\textbf{Evaluation results (\%) of continual instruction tuning on COAST-domain.} ``Avg." and ``Fgt." represent average accuracy and average forgetting, respectively. ``Reh.", ``Seq." and ``Joint" denote rehearsal, sequential and joint training.~\cite{cao2024continual}}
%\resizebox{0.99\linewidth}{!}{
%\begin{tabular}{lcx{28}x{28}|x{28}x{28}x{28}x{28}}
\begin{tabular}{l|ccccccc}
%\begin{tabular}{lcx{28}x{28}|cccc}
%\begin{tabular}{lccccccc}
	\hline
        \textbf{Methods}  & \textbf{Params} & \textbf{Avg.} & \textbf{Fgt.}  & \textbf{ChartQA} & \textbf{DocVQA} & \textbf{IconQA} & \textbf{MedicalQA}    \\
        %\textbf{Methods}  & \# \textbf{Param↓} & \textbf{Avg.↑} & \textbf{Fgt.↓}  & \textbf{ChartQA} & \textbf{DocVQA} & \textbf{IconQA} & \textbf{MedicalQA}    \\
        %\multirow{2}{*}{\textbf{Method}}  & \multirow{2}{*}{\#\textbf{Param}} & \multicolumn{2}{c}{\textbf{Final Results}} & \multicolumn{4}{c}{\textbf{Results of Each Task}} \\ \cline{3-8}
        %& & \textbf{Avg.↑} & \textbf{Fgt.↓} & \textbf{Chart} & \textbf{Doc.} & \textbf{Icon} & \textbf{Med.}   \\
	\hline
        %\textcolor{gray!70}{GPT-4o} & --- &  \\
        Joint~\cite{cao2024continual} & 6.76B & \textbf{42.79} & --- & \textbf{21.99} & \textbf{20.08} & \textbf{64.37} & \textbf{64.73} \\
        CODA~\cite{smith2023coda} & 0.75M & 36.06 & \textbf{2.72}  & 15.03 &  16.93 & 58.96 & 53.33  \\
        Dual~\cite{wang2022dualprompt} & 0.75M & 35.80 & 2.79  & 14.92 &  16.77 & 58.60 & 52.92  \\
        L2P~\cite{wang2022learning}  & 0.75M & 35.06 & 2.91  & 14.77 &  16.73 & 57.55 & 51.20  \\
        LWF~\cite{li2017learning}  & 6.76B & 27.06 & 15.05 &  14.07 & 13.19 & 37.93 & 43.05  \\
        EWC~\cite{kirkpatrick2017overcoming}  & 6.76B & 25.82 & 15.23 &  13.73 & 11.89 & 35.12 & 42.53  \\
        Reh.~\cite{bonicelli2022effectiveness} & 6.76B & 24.92 & 15.61 &  13.10 & 11.20 & 34.83 & 40.53  \\
        Seq.~\cite{cao2024continual} & 6.76B & 24.02 & 15.83 &  11.77 & 11.29 & 33.73 & 39.27  \\
        \hline
\end{tabular}
\label{coast_tab1}
\end{table*}



Table~\ref{coast_tab2} presents the performance of different methods on the continual instruction tuning tasks under the COAST-capability setting, focusing on the ability of MLLMs to acquire and integrate new capabilities. The table categorizes tasks into Conv. (Conversation), Desc. (Detail Description), Reason (Complex Reasoning), and Ref. (Referring qa).

\begin{table*}[t]

\renewcommand\arraystretch{1.2}
\centering
%\small
%\setlength{\tabcolsep}{1.3mm}
%\setlength{\arraycolsep}{2pt}
% \renewcommand\arraystretch{1.1}
\caption{\textbf{Evaluation results (\%) of continual instruction tuning on COAST-capability.} ``Conv.", ``Desc.", ``Reason" and ``Ref." represent conversation, detail description, complex reasoning, and referring qa, respectively. ``Reh.", ``Seq." and ``Joint" denote rehearsal, sequential, and joint training.~\cite{cao2024continual}}
%\resizebox{0.99\linewidth}{!}{
% \begin{tabular}{lcx{30}x{30}|x{30}x{30}x{30}x{30}}
\begin{tabular}{l|ccccccc}
	\hline
        %\makecell{\multirow{2}{*}{Methods}}  & \multirow{2}{*}{\#Params} & \multicolumn{6}{c|}{Accuracy (\%)}  \\
	  %& & Chart & Doc. & Icon & Med.  & Avg. & Fgt.  \\
        \textbf{Methods}  & \textbf{Params} & \textbf{Avg.} & \textbf{Fgt.}  & \textbf{Conv.} & \textbf{Desc.} & \textbf{Reason} & \textbf{Ref.}   \\
	\hline
        %\textcolor{gray!70}{GPT-4o} & --- &  \\
        Joint~\cite{cao2024continual} & 6.76B & \textbf{57.95} & --- & \textbf{62.48} & \textbf{43.45} & \textbf{74.02} & \textbf{51.84} \\
        CODA~\cite{smith2023coda}   &  0.75M & 54.21 & \textbf{4.99}  &  58.91 & 40.12 & 70.71 & 47.08  \\
        Dual~\cite{wang2022dualprompt}   &  0.75M & 53.62 & 5.01  &  58.09 & 39.85 & 70.03 & 46.52  \\
        L2P~\cite{wang2022learning}    &  0.75M & 53.31 & 5.04  &  57.90 & 39.33 & 69.70 & 46.32  \\
        LWF~\cite{li2017learning}    &  6.76B & 44.15 & 9.77  & 46.11 & 24.16 & 61.43 & 44.90 \\
        EWC~\cite{kirkpatrick2017overcoming}    &  6.76B & 43.69 & 9.72  & 46.23 & 24.20 & 60.11 & 44.20 \\
        Reh.~\cite{bonicelli2022effectiveness}   &  6.76B & 43.34 & 9.79  & 45.11 & 23.93 & 60.54 & 43.76 \\
        Seq.~\cite{cao2024continual}   &  6.76B & 41.51 & 10.56 & 44.29 & 23.25 & 58.39 & 40.13 \\
        \hline
 \end{tabular}
\label{coast_tab2}
\end{table*}


Table~\ref{coast_tab3} presents the performance of various methods on the continual instruction tuning task under the COAST-dataset setting, evaluating the ability of MLLMs to adapt to and generalize across dataset distributions. The terms "SciQA," "Text," "ImgNet," "GQA," "Viz," "REC," "VQA," and "OCR" in the table represent different visual question answering datasets. 


\begin{table*}[t]
\centering
%\small
%\setlength{\tabcolsep}{1.3mm}
%\setlength{\arraycolsep}{2pt}
\renewcommand\arraystretch{1.1}
\caption{\textbf{Evaluation results (\%) of continual instruction tuning on COAST-dataset.} ``Reh.", ``Seq." and ``Joint" denote rehearsal, sequential, and joint training.~\cite{cao2024continual}}
%\resizebox{0.99\linewidth}{!}{
\scalebox{0.96}{
%\begin{tabular}{lcx{22}x{22}|x{22}x{22}x{22}x{22}x{22}x{22}x{22}x{22}}
\begin{tabular}{lcc|cccccccc}
	\toprule
        %\makecell{\multirow{2}{*}{Methods}}  & \multirow{2}{*}{\#Params} & \multicolumn{6}{c|}{Accuracy (\%)}  \\
        %\textbf{Methods}  & \#\textbf{Params}  & \textbf{Chart} & \textbf{Doc.} & \textbf{Icon} & \textbf{Med.}  & \textbf{Avg.↑} & \textbf{Fgt.↓}  \\
        \textbf{Methods} & \textbf{Avg.↑} & \textbf{Fgt.↓} & \textbf{SciQA} &  \textbf{Text} &  \textbf{ImgNet} &  \textbf{GQA} &  \textbf{Viz} & \textbf{REC} & \textbf{VQA} & \textbf{OCR} \\
	\midrule
        %\textcolor{gray!70}{GPT-4o} & --- &  \\
        Joint~\cite{cao2024continual} & 57.03 & --- & 61.74 & 52.14 & 60.93 & 65.56 & 47.46 & 21.86 & 67.54 & 79.04 \\
        CODA~\cite{smith2023coda}  &  50.27  &  9.70  & 54.80 & 44.55 & 53.64 & 58.43 & 39.07 & 14.97 & 62.63 &  74.08   \\
        Dual~\cite{wang2022dualprompt}  &  49.40  &  12.03 & 53.82 & 41.88 & 52.21 & 59.24 & 39.13 & 14.05 & 62.80 &  72.14  \\
        L2P~\cite{wang2022learning}   & 49.01   & 12.12  & 53.13 & 41.64 & 51.69 & 58.96 & 38.90 & 13.78 & 62.22 &  71.78  \\
        LWF~\cite{li2017learning}   &  26.41  & 36.94  & 52.40 & 30.02 & 23.99 & 27.30 & 14.65 & 3.43 & 35.13 &  24.32 \\
        EWC~\cite{kirkpatrick2017overcoming}   &  27.24  & 32.52  & 52.93 & 31.84 & 25.13 & 28.61 & 15.25 & 5.03 & 35.21 &  23.91 \\
        Reh.~\cite{bonicelli2022effectiveness}  &  26.49  & 33.17  & 52.02 & 31.29 & 24.44 & 28.03 & 14.80 & 4.14 & 34.14 &  23.03 \\
        Seq.~\cite{cao2024continual}  &  25.35  & 35.82  & 51.57 & 30.19 & 23.27 & 26.08 & 14.19 & 1.32 & 33.49 &  22.67 \\
        \bottomrule
\end{tabular}}
\label{coast_tab3}
\end{table*}






\subsubsection{ViLCo-Bench: Video Language Continual learning Benchmark}

Multimodal large models in the domain of video-language continual learning involve the continuous adaptation to information from both video and text inputs, enhancing the model's ability to handle new tasks while retaining previous knowledge. This is a relatively under-explored field, and establishing appropriate benchmarks is crucial to promoting communication and research in this area. To address this, Tianqi Tang et al.~\cite{tang2024vilco} proposed the first benchmark specifically designed for video-language continual learning in multimodal large models, called ViLCo-Bench. This benchmark aims to evaluate continual learning models across a range of video-text tasks.

ViLCo-Bench includes three unique video-language tasks: 1) Moment Queries (MQ). 2) Natural Language Queries (NLQ). 3) Visual Queries (VQ). These tasks require the model to understand video content and retrieve relevant segments of the video based on language queries.

Table~\ref{vilco_tab1} presents the results of different continual learning methods on the MQ task. The evaluation used Average Recall, including R@1 and R@5 (IoU=0.3 and IoU=0.5), to measure the model's performance at different Intersection over Union (IoU) thresholds.

\begin{table*}[!t]
\small
\centering
\caption{Results of Methods on Moment Query.~\cite{tang2024vilco}}
\scalebox{0.9}{
    \begin{tabular}{l|cc|c|ccc|ccc}
        \toprule 
        \multirow{2}{*}{\centering Method}
        & \multirow{2}{*}{\centering Num. Task}
        & \multirow{2}{*}{\centering Mem. Capacity}
        & \multirow{2}{*}{\centering BwF$\downarrow$}
        & \multicolumn{3}{c}{Avg R@1 ($\%$)$\uparrow$}
        & \multicolumn{3}{c}{Avg R@5 ($\%$)$\uparrow$} \\
        
        ~& & & & IoU=0.3   & IoU=0.5 & mean
        & IoU=0.3   & IoU=0.5 & mean  \\
        \midrule 

        Upper-Bound
        & None & None & None
        & $48.07_{\pm 0.09}$ & $38.71_{\pm 0.02}$ & 43.39
        & $67.30_{\pm 0.03}$ & $56.87_{\pm 0.005}$ & 62.09 \\
        Lower-Bound
        & None & None & None
        & $19.62_{\pm 0.25}$ & $10.87_{\pm 0.06}$ & 15.25
        & $31.61_{\pm 0.76}$ & $19.11_{\pm 0.41}$ & 25.36 \\
        \midrule
        % EWC~\cite{} 
        % & 10 & None & 20.9
        % & 11.74 & 10.15 & 10.95 
        % & 16.75 & 13.60 & 15.18 \\
        % Naive~\cite{} 
        % & 5 & None & 18.8
        % & 22.74 & 17.58 & 20.16
        % & 32.92 & 27.90 & 30.41 \\
        EWC~\cite{kirkpatrick2017overcoming}
        & 5 & None & $24.2_{\pm 0.03}$
        & $17.61_{\pm 0.57}$ & $12.51_{\pm 0.14}$ & 15.06
        & $28.13_{\pm 0.03}$ & $22.33_{\pm 0.51}$ & 25.23 \\
        MAS~\cite{aljundi2018memory}
        & 5 & None & $11.5_{\pm 0.01}$
        & $14.45_{\pm 0.01}$ & $9.88_{\pm 0.003}$ & 12.17
        & $22.50_{\pm 0.06}$ & $16.89_{\pm 0.07}$ & 19.70 \\
        iCaRL~\cite{rebuffi2017icarl}
        & 5 & 1010 & $4.6_{\pm 0.01}$
        & $32.01_{\pm 0.14}$ & $23.66_{\pm 0.30}$ & 27.84
        & $50.59_{\pm 0.12}$ & $39.68_{\pm 0.003}$ & 45.14 \\
        BiC~\cite{wu2019large}
        & 5 & 1010 & \bf $1.4_{\pm 0.001}$
        & $5.28_{\pm 0.42}$ & $3.39_{\pm 0.09}$ & 4.34
        & $6.90_{\pm 0.30}$ & $4.53_{\pm 0.003}$ & 5.72 \\
       VilCo~\cite{tang2024vilco}
        & 5 & 1010 & $2.9_{\pm 0.09}$
        & $\bf 33.58_{\pm 0.06}$ & $\bf 26.24_{\pm 0.04}$ & \bf 29.91
        & $\bf 53.75_{\pm 0.33}$ & $\bf 42.70_{\pm 0.006}$ & \bf 48.23 \\
        \bottomrule
    \end{tabular}
    }
\label{vilco_tab1}
\end{table*}



Table~\ref{vilco_tab2} presents the results of various continual learning methods on the NLQ task. The NLQ task is more complex than the MQ task, as language queries are not limited to human activities but involve open-vocabulary descriptions.


\begin{table*}[!t]
\small
\centering
\caption{Results of Methods on Natural Language query.~\cite{tang2024vilco}}
\scalebox{0.9}{
    \begin{tabular}{l|cc|c|ccc|ccc}
        \toprule 
        \multirow{2}{*}{\centering Method}
        & \multirow{2}{*}{\centering Num. Task}
        & \multirow{2}{*}{\centering Mem. Capacity}
        & \multirow{2}{*}{\centering BwF$\downarrow$}
        & \multicolumn{3}{c}{Avg R@1 ($\%$)$\uparrow$}
        & \multicolumn{3}{c}{Avg R@5 ($\%$)$\uparrow$} \\
        
        ~& & & & IoU=0.3   & IoU=0.5 & mean
        & IoU=0.3   & IoU=0.5 & mean  \\
        \midrule 

        Upper-Bound
        & None & None & None
        & 13.82 & 9.20 & 11.51
        & 33.59 & 23.18 & 28.39 \\
        \midrule
        Naive
        & 13 & None & 48.76
        & 6.05 & 3.61 & 4.83
        & 16.77 & 10.07 & 13.42 \\
        EWC~\cite{kirkpatrick2017overcoming}
        & 13 & None & 50.05
        & 6.34 & 4.05 & 5.20
        & 19.50 & 12.08 & 15.79 \\
        MAS~\cite{aljundi2018memory}
        & 13 & None & 35.92
        & 7.04 & 4.22 & 5.63
        & 21.56 & 12.63 & 17.10 \\
        % iCaRL~\cite{} 
        % & 13 & 1010 &
        % & & &
        % & & & \\
       ViLCo~\cite{tang2024vilco}\
        & 13 & 1010 & \textbf{10.60}
        & \textbf{9.49} & \textbf{6.21} & \textbf{7.85}
        & \textbf{25.52} & \textbf{16.36} & \textbf{20.94} \\
        \bottomrule
    \end{tabular}
}
\label{vilco_tab2}
\end{table*}


Table~\ref{vilco_tab3} presents the results of various continual learning methods on the VQ task. The VQ task requires the system to understand the visual content of the queried image. tAP (temporal Average Precision) is used as the performance metric, which measures the distance between predicted and true locations in continuous tasks.


\begin{table*}[!t]
\small
\centering
\caption{Results of Methods on Visual Query.~\cite{tang2024vilco}}
\scalebox{0.9}{
    \begin{tabular}{l|cc|c|cccc}
        \toprule 
        \centering Method
        & \centering Num. Task
        & \centering Mem. Capacity
        & \centering BwF$\downarrow$
        & Avg tAP$_{25}$ ($\%$)$\uparrow$
        & Avg stAP$_{25}$ ($\%$)$\uparrow$
        & Avg rec ($\%$)$\uparrow$
        & Avg Succ. ($\%$)$\uparrow$ \\
        \midrule 

        Upper-Bound
        & None & None & None
        & 31 & 22 & 47.05 & 55.89  \\
        \midrule
        % Naive
        % & 5 & None & 
        % &  &  &  & \\
        EWC~\cite{kirkpatrick2017overcoming}
        & 5 & None & 51.01
        & 11.48 & 7.81 & 16.79 & 22.05 \\
        MAS~\cite{aljundi2018memory}
        & 5 & None & 47.60
        & 12.13 & 9.16 & 17.80 & 22.51 \\
        % iCaRL~\cite{} 
        % & 13 & 1010 &
        % & & &
        % & & & \\
       ViLCo~\cite{tang2024vilco}\
        & 5 & 1010 & \textbf{23.77}
        & \textbf{17.85 }& \textbf{13.23} & \textbf{26.36} & \textbf{33.38} \\
        \bottomrule
    \end{tabular}
}
\label{vilco_tab3}
\end{table*}


Existing benchmarks for multimodal large model continual learning provide some reference value for assessing a model's learning ability. However, due to the scarcity of such benchmarks, with only a few available for use, many issues and limitations remain to be addressed.In the future, there is a need to design more comprehensive, flexible, and scalable evaluation benchmarks to meet the evolving demands of multimodal large model continual learning technologies. 

In summary, significant progress has been made in the field of continual learning for multimodal large models. Although many methods have been proposed to address the issue of how models can learn new tasks without forgetting old ones in dynamically changing environments, several challenges remain unsolved. Current research primarily focuses on effectively preventing catastrophic forgetting in multimodal contexts, enhancing the persistence of knowledge through innovative architectural designs, and maintaining cross-modal knowledge consistency while learning new tasks. However, the effectiveness of these approaches is still constrained by the complexity of multimodal data and the dynamic nature of tasks, especially when dealing with large-scale and diverse modality data. Furthermore, to effectively assess the continual learning capabilities of multimodal large models, researchers have proposed specialized benchmarks for this field. These benchmarks simulate sequential learning of multiple tasks to evaluate models' transfer abilities between tasks, catastrophic forgetting phenomena, and the retention of cross-modal knowledge. These evaluation standards not only help measure the effectiveness of existing methods but also provide clear guidance for the design and optimization of future models. Overall, despite the challenges that continue to persist in the continual learning of multimodal large models, research in this area will play a critical role in advancing the long-term learning and generalization capabilities of artificial intelligence systems.



% todo
% 1. 图标风格统一（格式统一）
% 2. 表格重新绘画一下
% 3. pipline