\section{Optimal Stochastic Trace Estimation in Generative Modeling}
Introducing the Hutchinson estimator in the dynamics solver~\citep{FFJORD} provides an unbiased log-density estimation with $\mathcal{O}(D)$ cost, allowing more flexible model architectures, as well as developing the divergence-based likelihood training paradigm in diffusion models~\cite{song2020sliced, forward_backward_SDE, VSDM, provably_schrodinger_bridge}.
However, the Hutchinson estimator leads to high variance, which gradually accumulates over the duration of each integration, posing a scalability challenge in model training. 
In this section, we will present a variance reduction technique to mitigate this issue and integrate it with modern diffusion models, enhancing its applicability for generative modeling tasks.


\subsection{Variance Reduced Divergence-Based Training} \label{sec: variance_reduction}
As discussed in~\citet{hutch_pp}, the Hutchinson estimator is less sensitive to individual large eigenvalues, which implies that matrices with a flatter spectrum can be approximated more reasonably. 
However, in real-world generative modeling scenarios, high-dimension data often exhibits underlying patterns captured by fewer dimensions~\citep{van2008tsne, ng2011sparse, VAE, carlsson2009topology}.
Motivated by the desirable properties of the low-rank approximation,
we proposed leveraging Hutch++~\citep{hutch_pp} as a natural variance reduced version of Hutchinson estimator. Hutch++ involves a low-rank approximation step and estimate $ \mathrm{Tr} \left( \frac{\partial f_t}{\partial \mathbf{z}(t)} \right)$ through two parts:
\begin{itemize}
    \item deterministic computation of large eigenvalues;
    \item stochastic estimation of small eigenvalues.
\end{itemize}
We elaborate on the Hutch++ estimator as follows.

Given $i.i.d.$ random vectors $S_i \in \R^{D}$ and $G_i \in \R^{D}$, typically from a Gaussian distribution. Hutch++ stars by computing an orthonormal span $Q$ through a single iteration of the power method with a random initial vector $S$.
Here $Q_i \in \R^D$ provides an approximate estimation of $i$-th top eigenvector of $\mathbf{A}$. 
Then we can separate $\mathbf{A}$ into two parts: the projection onto the subspace spanned by $Q$ and the projection onto the orthogonal complement of this subspace. 
Therefore, we can divide the calculation of the $\mathrm{Tr}(\mathbf{A})$ into two parts:
\setcounter{equation}{7}
\begin{equation}\label{eq: trance_separate}
    \mathrm{Tr}(\mathbf{A}) = \mathrm{Tr}(Q^\top\mathbf{A}Q)+ \mathrm{Tr}((I-QQ^\top)\mathbf{A}(I-QQ^\top))
    % \vspace{-10mm}
\end{equation}
where $Q = \mathrm{QR}(\mathbf{A}S)$ and $\mathrm{QR}(\cdot)$ performs QR decomposition and return the orthonormal basis.
The first term is derived from $\mathrm{Tr(QQ^\top\mathbf{A}QQ^\top)}$ by leveraging the cyclic property of the trace and can be computed exactly. 
Consequently, the error in estimating $\mathrm{Tr}(\mathbf{A})$ arises solely from the approximation of the second term. This term is approximated using the Hutchinson estimator with the random vector $G_i$, resulting in a significantly lower variance in the estimation of $\mathrm{Tr}((I-QQ^\top)\mathbf{A}(I-QQ^\top))$ compared to the direct estimation of $\mathrm{Tr}(\mathbf{A})$ in Equation \ref{eq: ode_likelihood}.
We present the method for trace estimation using Hutch++ \citep{hutch_pp}:
\begin{equation}
\scalebox{0.87}{$
    H_m^{++}(\mathbf{A})=\mathrm{Tr}(Q^\top\mathbf{A}Q)+H_{\frac{m}{3}}((I-QQ^\top)\mathbf{A}(I-QQ^\top)).$}\label{eqn:hutchpp} 
\end{equation}
To enhance the scalability of the diffusion model for high-dimensional data while preserving transport efficiency, we adapt the proposed method to Schr√∂dinger bridge (SB)-based diffusion models~\citep{forward_backward_SDE, mSB, VSDM}, which are alternately trained with divergence-based likelihood introduced in \Cref{sec: fb-sde}.
For the divergence-based training objectives, 
we propose leveraging the Hutch++ estimator to approximate the divergence terms in Equation \ref{eq: fb-sde-train-b} \ref{eq: fb-sde-train-f}. 

Similar to~\citep{forward_backward_SDE, VSDM, FFJORD}, we sample each initial vector $S_i$ and noise vector $G_i$ independently and fix them over the integration interval. We re-formulate Equation \ref{eq: fb-sde-train-b} as:
\begin{dmath}\label{eqn:loglike2}
    \Tilde{\mathcal{L}}_{SB}(\mathbf{x}_0) = - \int_{0}^{T}\E_{\mathbf{x}_t\sim \ref{eq: fb-sde-b}}\left[ \frac{1}{2}\|\overrightarrow{z}_t^\phi\|^2 + \overleftarrow{z}_t^\top\overrightarrow{z}_t^\phi\\
    \scalebox{0.98}{$+ \sqrt{\beta_t}\mathrm{Tr}\tiny{\left(\frac{\partial \overrightarrow{z}_t^\phi}{\partial \mathbf{x}(t)}\right)}$}\right] \mathrm{d}t \\
    = - \int_{0}^{T}\E_{\mathbf{x}_t\sim \ref{eq: fb-sde-b}}\left[ \frac{1}{2}\|\overrightarrow{z}_t^\phi\|^2 + \overleftarrow{z}_t^\top\overrightarrow{z}_t^\phi\\
    + \scalebox{1.0}{$\sqrt{\beta_t}\left(\mathrm{Tr}\left(Q^\top\frac{\partial \overrightarrow{z}_t^\phi}{\partial \mathbf{x}(t)}Q\right) 
    + \mathrm{Tr}\left(P^\perp\frac{\partial \overrightarrow{z}_t^\phi}{\partial \mathbf{x}(t)}P^\perp\right)\right)$}\right] \mathrm{d}t \\
    = - \int_{0}^{T}\E_{\mathbf{x}_t\sim \ref{eq: fb-sde-b}}\left[ \frac{1}{2}\|\overrightarrow{z}_t^\phi\|^2 + \overleftarrow{z}_t^\top\overrightarrow{z}_t^\phi \\
    + \scalebox{0.86}{$\sqrt{\beta_t}\left(\mathrm{Tr}\left(Q^\top\frac{\partial \overrightarrow{z}_t^\phi}{\partial \mathbf{x}(t)}Q\right)
    + \E_{p(G)} \left[ G^\top\left(P^\perp \frac{\partial \overrightarrow{z}_t^\phi}{\partial \mathbf{x}(t)}P^\perp \right)G\right]\right)$}\right] \mathrm{d}t
    % \tag{10}
\end{dmath}
where $Q = \mathrm{QR}(\frac{\partial f_t}{\partial \mathbf{z}(t)}S)$ and $P^\perp = I - QQ^\top$ denotes the orthogonal complement projection matrix of $Q$.
Likewise, the forward training stage can be improved with reduced variance facilitated by the Hutch++ estimator.

From the perspective of probability flow, $\Tilde{\mathcal{L}}_{SB}(\mathbf{x}_0)$ used in the backward training stage collapses to the log-likelihood in Equation \ref{eq: ode_likelihood} when the drift degenerates. 
Consequently, SB models can be articulated through the concept of flow within the flow-based model training framework~\citep{forward_backward_SDE, song2019generative, gong2021interpreting}, thereby enhancing the scalability of flow-based models while improving transport efficiency.

\vspace{-1mm}
\subsection{Acceleration Technique for Training}\label{sec: acceleration}

\vspace{-1mm}
For SB-based diffusion models, leveraging Hutch++ to estimate $\mathrm{Tr}\left(\frac{\partial z_t}{\partial \mathbf{x}(t)}\right)$\footnote{Hereafter, We use $\frac{\partial z_t}{\partial \mathbf{x}(t)}$ to represent the Jacobian matrix in log-likelihood of SB-based diffusion models} reduced the high variance introduced by the Hutchinson estimator, leading to faster convergence in training and higher quality density estimation.
However, as the data dimension and batch size increase, updating $Q$ via QR decomposition on $\frac{\partial z_t}{\partial \mathbf{x}(t)} S$ at each step over a single integral interval becomes prohibitively time-consuming.
To tackle this challenge, we propose acceleration techniques for training divergence-based likelihood under the Hutch++ estimator, where we update the orthogonal matrix $Q$ every $L_s$ steps over the duration of each integration.

Inspired by the principle of efficient communication in federated learning~\citep{mcmahan2017fedlearning1}, 
we focus on a few updates of $Q$, ensuring each update takes on more responsibility, thereby reducing the overall number of updates.
With the observation of the noising/denoising process in diffusion models,
we regard the change in $\frac{\partial f_t}{\partial \mathbf{z}(t)}$ across a continuous period of time to be minor, indicating that the top eigenvalues of $\frac{\partial f_t}{\partial \mathbf{z}(t)}S$ during this interval are approximately the same. This motivates us to share the $Q$ required for subsequent trace estimation within this continuous time interval.
Specifically, we divide the integral interval with a total of $L$ steps into multiple subintervals, each consisting of $L_s$ steps.
The orthogonal matrix $Q^t_i$ of each step can be mathematically defined as: for $t_0=0$ and $t_L=1$\footnote{For clarity, we denotes the diffusion interval as $[t_0, t_L]$.}
\begin{align}
\scalebox{0.95}{$
    Q^{t_0} =  \text{QR}\left(\frac{\partial z_t}{\partial \mathbf{x}(0)} S\right), \quad  Q^{t_L} =  \text{QR}\left(\frac{\partial z_t}{\partial \mathbf{x}(1)}S\right) $}\notag \hspace{0.95cm}\\
    % & Q^{t_i} =  \text{QR}(\frac{\partial f_{t_i}}{\partial \mathbf{z}(t_i)} S), \quad \text{where}\quad i =\lfloor\frac{L}{L_s} t\rfloor \cdot \frac{L_s}{L} \notag\\
\scalebox{0.95}{$
    Q^{t_i} = \text{QR}\left(\frac{\partial z_t}{\partial \mathbf{x}\left(\lfloor\frac{L}{L_s} t_i\rfloor \cdot \frac{L_s}{L}\right)}S\right),\quad \text{for}\quad t\in[t_i, t_{i+L_s}) $}\label{eqn:acceleration}
    % & Q^{t_i} = \text{QR}(\frac{\partial f_{t_i}}{\partial \mathbf{z}(\lfloor\frac{L}{L_s} t_i\rfloor \cdot \frac{L_s}{L})} S),\quad \text{for}\quad t\in[t_i, t_{i+L_s})
\end{align}
where $\lfloor \cdot\rfloor $ denotes the floor function.
With the help of shared orthogonal matrices $Q^t$, we only need to perform QR decomposition about $\lfloor\frac{L}{L_s}\rfloor$ times over the duration of each integration, instead of updating $Q^t$ every step.
This technique significantly accelerates the training process under divergence-based likelihood for diffusion models.
We provide a theoretical analysis in \Cref{sec: error_propagation}, along with an experimental comparison on synthetic data in~\Cref{sec: exp_simulation}.





