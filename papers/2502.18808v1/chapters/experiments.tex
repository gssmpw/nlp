\section{Experiments}
We test our method on a variety of generative modeling tasks including density estimation on simulations, time series imputation and forecasting, as well as image data modeling. 
Our baselines include various Neural ODE model variants and divergence-based Schrödinger bridge models. We describe the details of the implementation in Appendix 2.2.

\subsection{Simulations}\label{sec: exp_simulation}
We first apply the proposed Hutch++ estimator to the Neural ODE model FFJORD \citep{FFJORD}. We refer to our enhanced version as FFJORD++.
We validated FFJORD++ on several toy distributions that serve as standard benchmarks \citep{FFJORD, wehenkel2019simulations1}. 
In this section, we note that the model is evaluated using exact trace when estimating density or reporting the test loss (negative log-likelihood) of the test set during training.

\paragraph{Analysis on Training Efficiency}
\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        % \caption{2Spirals}
        \includegraphics[width=1.\textwidth]{fig/2s_convergence.pdf}
        \label{fig: 2s}
    \end{subfigure}
    \vspace{-5mm}
    \caption{\small Visualization of density estimation obtained by \textbf{(Top) FFJORD}  and \textbf{(Bottom) FJORD++} during the training phase on \texttt{2spirals}. The proposed variance-reduced model, FJORD++, improves both convergence and training stability, resulting in higher-quality estimated densities.}
    \vspace{-3mm}
    \label{fig: vis_train}
\end{figure}

\begin{figure*}[!t]
    \centering
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/2s.pdf}
        % \caption{(a)}
        \label{fig: scale_x1}
        \vspace{-4mm}
    \end{subfigure}
    \vspace{-3mm}
    \caption{\small Comparison between the FFJORD++ and FFJORD on \texttt{2spirals} with various scales. \textbf{From left to right}, each row represents the data with a default scale, as well as shapes stretched 2x and 4x along the X-axis, respectively. As the scale along the X-axis increases, FFJORD++ consistently exhibits superior convergence rates, further widening the performance gap with FFJORD.}
    \vspace{-3mm}
    \label{fig: condition_num}
\end{figure*}
Figure \ref{fig: vis_train} visualizes the comparison of the density estimated by FFJORD and FFJORD++ during the training stage on \texttt{2spirals}. 
Visually, the density estimated by FFJORD++ exhibits more distinct modes in the early stages and achieves improved stability in subsequent phases compared to the density estimated by FFJORD. 
This observation suggests that FFJORD++ has the potential to fit data distributions faster and recover data distributions more accurately, exhibiting the advantages of employing Hutch++ to reduce variance in Equation \ref{eq: ode_likelihood} in model training. We provide more examples in the Appendix 3.1.



\paragraph{Density Modeling with Different Condition Number}
To better validated the applicability of our method, we further explored its performance in fitting general shapes.
In addition to the default scale, we stretch the X-axis of the data by 2 and 4 times and make comparisons between FFJORD ++ and FFJORD on \texttt{2spiral}
with various scales in \Cref{fig: condition_num}.
We denote them by 2spiral-1X, 2spiral-2X and 2spiral-4X, respectively.
As shown in \Cref{fig: condition_num}, FFJORD++ exhibits faster convergence rates than FFJORD and even achieves lower testing loss, which is comparable to results training with exact trace. 
This observation suggests that the variance reduction technique in \Cref{sec: variance_reduction} can improve the convergence and stability of model training, resulting in higher-quality estimated densities.
Furthermore, 
as the scale along the X-axis is stretched, FFJORD++ consistently demonstrates superior convergence rates, further widening the performance gap with FFJORD.
This observation indicates that FFJORD++ is superior for modeling data distributions with higher condition numbers caused by increased stretching scales, which in turn impacts the eigenvalue distribution of the $\frac{\partial f_t}{\partial \mathbf{z}(t)}$ during training. 
We explain this to FFJORD++ can better approximate the $ \mathrm{Tr} \left( \frac{\partial f_t}{\partial \mathbf{z}(t)} \right)$ in Equation \ref{eq: ode_likelihood} by projecting the top eigenvalues when the matrix's spectrum is not flat, aligning with our estimate (Equation \ref{eqn:loglikelihoodest}). We provide more results in the Appendix 3.1.


\begin{figure}[!t]
    \centering
    % \hspace{-0.5cm}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \hspace{-0.7cm}
        \includegraphics[width=0.8\linewidth]{fig/accelerate.pdf}
        % \vspace{-1mm}
        % \caption{}
    \end{subfigure}
    % \hfill
    \begin{subfigure}{0.5\textwidth}
        \centering
        % \hspace{2cm}
        \vspace{2mm}
        \hspace{-0.5cm}
        \scalebox{0.8}{
        \begin{tabular}{l|c|cccc}
        \toprule
        Method & FFJORD &\multicolumn{4}{c}{FFJORD++} \\
         &  & S=1 & S=10 & S=25 & S=50\\
        \midrule
        Cost (second) &38 &407 &55 &50 &43\\
        \bottomrule
        \end{tabular}}
        % \caption{}
        \label{tab: cost}
    \end{subfigure}
    \label{fig: shared_step}
    \caption{\small \textbf{(Top)} Comparison of test loss during training on FFJORD++ trained with different $L_s$. \textbf{(Left)} Time cost for 100 iterations on models trained with different $L_s$.}
    \label{fig: ablation}
    \vspace{-6mm}
\end{figure}


\paragraph{Ablation on Shared Step Size $L_s$}
As described in \Cref{sec: acceleration}, there exists a trade-off between computational cost and variance when estimating the trace within integration in Equation \ref{eq: ode_likelihood}, where the lower variance can lead to faster convergence during training.
Therefore, we ablate the shared steps $L_s$ on 2spirals. Specifically, the default step size for the entire integration interval is 100 and we update the orthogonal matrix $Q$ each $L_s$ steps, where $L_s \in [1, 10, 25, 50]$. We show the comparison of test loss during training on FFJORD++ trained with different $L_s$ in \Cref{fig: ablation} \textbf{(Top)}.
As shown in~\Cref{fig: ablation} \textbf{(Bottom)}, we compare the computational cost of FFJORD and various versions of FFJORD++ with shared steps $L_s$ during training, over 100 iterations with batch size set to 512. 
Considering the results \Cref{fig: ablation}, it is evident that vanilla FFJORD++ ($L_s$=1) enhances model convergence.
However, its computational cost is roughly ten times higher than that of FFJORD, making it impractical for real-world applications.
Fortunately, increasing the length of shared subintervals, that is, reducing the count of computing $Q$ over the duration of each integration, does not significantly degrade the model's performance. 
Instead, it yields in substantial time savings, making the approach much more efficient.
Based on the above observations, we chose $L_s = 10$ as the default shared steps. 
The improvements introduced by FFJORD++ and its competitive time efficiency compared to FFJORD motivate us to further extend Hutch++ to accommodate a broader range of data types and scenarios.


\subsection{Time Series Modeling}
\begin{table*}[!t]
    \centering
    \caption{\small \textbf{(Left)} Results on Time Series Imputation. \textbf{(Right)} CRPS on Time Series Forecasting. (lower is better).}
        \label{tab: exp_time}
    \begin{minipage}{0.5\textwidth} 
        \centering
        \scalebox{0.65}{
        \begin{tabular}{lccccccccc}
        \toprule[1.5pt]
        &\multicolumn{3}{c}{PM2.5} &\multicolumn{3}{c}{PhysioNet 0.1} &\multicolumn{3}{c}{PhysioNet 0.5} \\
        \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
        Methods
        & RMSE 
        & MAE
        & CRPS
        & RMSE 
        & MAE
        & CRPS
        & RMSE 
        & MAE
        & CRPS\\
        \midrule
        V-RIN & 40.1& 25.4& 0.53&0.63 &0.27 &0.81 &0.69 &0.37 &0.83\\ 
        Multitask GP &42.9 &34.7 &0.41 &0.80 &0.46 &0.49 &0.84 &0.51 &0.56\\
        GP-VAE &43.1 &26.4 &0.41 &0.73 &0.42 &0.58 &0.76 &0.47 &0.66\\
        CSBI &20.7 &10.8 &\textbf{0.11} &0.62 &0.29 &0.35 &0.72 &0.35 &0.39\\
        \textbf{CSBI++(Ours)} &\textbf{20.2} &\textbf{10.3} &\textbf{0.11} &\textbf{0.54} &\textbf{0.24} &\textbf{0.28} &\textbf{0.66} &\textbf{0.33} &\textbf{0.34}\\
        \bottomrule
    \end{tabular}}
    \end{minipage}
    \hspace{1.5cm}
    \begin{minipage}{0.4\textwidth} 
        \centering
        \label{tab: tsi}
        \scalebox{0.78}{
        \begin{tabular}{lccc}
        \toprule[1.5pt]
        Methods & Exchange & Solar &Electricity\\
        \midrule
        GP-copula &0.008 &\textbf{0.317} &0.041 \\
        Vec &0.009 &0.384 &0.043 \\
        TransMAF &0.012 &0.368 &0.039 \\
        CSBI &0.008 &0.372 &0.038 \\
        \textbf{CSBI++ (Ours)} &\textbf{0.007} &0.363 &\textbf{0.035} \\
        \bottomrule
    \end{tabular}}
    \end{minipage}
\end{table*}

To validate the effectiveness of our method on real-world data, we applied Hutch++ to the cutting-edge model CSBI~\citep{provably_schrodinger_bridge}, 
a conditional Schrödinger bridge method for time series modeling.
Our enhanced method is referred to as CSBI++, and we conducted experimental quantitative analysis through time series imputation and forecasting.

\paragraph{Datasets and Metrics}\label{sec: exp_time_data}
For the imputation task, we run experiments on two datasets. 
The air quality dataset \texttt{PM2.5}~\citep{yi2016pm25} and the healthcare dataset \texttt{PhysioNet} in PhysioNet Challenge 2012~\citep{silva2012physio}.
For \texttt{PhysioNet}, we randomly mask a specific percentage (10\%, 50\%) of the data that comprise the test dataset, denoted as \texttt{PhysioNet 0.1} and \texttt{PhysioNet 0.5}, respectively.
For the forecast task, we validated our method on three datasets with varying feature dimensions, which are collected and preprocessed in GluonTS~\citet{alexandrov2020gluonts}. We adopt root mean square error (RMSE), mean absolute error (MAE) and continuous ranked probability score (CRPS) for evaluation.
Details can be found in Appendix 2.1.

\paragraph{Baselines}
We focused on comparing CSBI++ with its vanilla version (CSBI) on time series modeling tasks.
For the imputation task, our baselines additionally include V-RIN~\citep{mulyadi2021vrin}, multitask Gaussian process (multitask GP)~\citep{durichen2014multitaskgp} and GP-VAE~\citep{fortuin2020gpvae}.
For the forecasting task, our baselines additionally include GP-copula~\citep{salinas2019gpcopula}, Vec-LSTM-low-rank-Coupula (VEC)~\citep{salinas2019gpcopula} and TransMAF~\citep{rasul2021transmaf}.

\paragraph{Results}
For the imputation task, \Cref{tab: exp_time} \textbf{(Left)} illustrates that CSBI++ surpasses CSBI in all evaluated metrics across the three datasets, demonstrating the effectiveness of integrating Hutch++ into an SB-based model for multidimensional data modeling.
For the forecasting task, \Cref{tab: exp_time} \textbf{(Right)} shows that CSBI++ demonstrates superior performance relative to its standard version and exhibits substantial enhancements on \texttt{Solar}. It is noteworthy that the improvement on electricity with a dimension of 370 further underscores the effectiveness and applicability of our method in modeling high-dimensional data.
We provide examples in \Cref{fig: time} and more visualization in Appendix 3.2.
\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/pm25_k_12.pdf}
        \caption{}
        \label{fig: time_i}
    \end{subfigure}
    \begin{subfigure}{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/electricity_64.pdf}
        \caption{}
        \label{fig: time_f}
    \end{subfigure}
    \vspace{-1mm}
    \caption{\small \textbf{(a)Imputation} examples for \texttt{PM25} for 1 (out of 36) dimensions. \textbf{(b)Forecasting} examples for \texttt{Electricity} for 1 (out of 370) dimensions}
    \vspace{-2mm}
    \label{fig: time}
\end{figure}


\subsection{Image Data Modeling}\label{sec: image}
In light of the demonstrated effectiveness of our proposed method in simulations and its encouraging performance in multidimensional time series modeling, we further investigated its potential to handle high-dimensional data.
We apply Hutch++ to advanced SB-based diffusion models and compare their performance in image data modeling.

\paragraph{Experiment Setup}
We first proposed SB-FBSDE++ based on SB-FBSDE~\citep{forward_backward_SDE} which is a very popular SB-based generative model.
We compare their performance on \texttt{MNIST}, \texttt{Fasion-MNIST}, and \texttt{CIFAR10} in terms of negative log-likelihood. 
Moreover, to better validate our method in modeling images with complex patterns, we employed VSDM~\citep{VSDM} as the base model, a state-of-the-art SB-based diffusion model which is capable of tuning-friendly training on large-scale data.
We denote our improved model incorporating the Hutch++ estimator as VSDM++.


\paragraph{Results}
~\Cref{tab: image} \textbf{(Top)} shows the comparison of the NLL values evaluated on the test set. 
SB-FBSDE++ achieves consistent improvements across all three datasets.
Especially on the \texttt{Fashion-MNIST} dataset, SB-FBSDE++ shows promising improvements, suggesting that our proposed method can effectively adapt to SB-based diffusion models for modeling images with complex patterns.
Furthermore, we compared the FID scores recorded in ~\Cref{tab: image} \textbf{(Bottom)} throughout the training process on \texttt{CIFAR10} datasets. 
The improvements introduced by VSDM++ throughout the training process demonstrate that our approach remains effective on high-dimensional data, highlighting its potential for more complex generative modeling.
The generated images are provided in Appendix 3.3.

\begin{table}[!t]
    \centering
    \caption{\small \textbf{(Top)} Image data modeling evaluation using negative log-likelihood (NLL; bits/dim) on the test set. \textbf{(Bottom)} Comparison of convergence speed of FID scores for VSDM and VSDM++.}
        \label{tab: image}
    \begin{minipage}{0.48\textwidth} 
        \centering
        \scalebox{0.75}{
        \begin{tabular}{lccc}
        \toprule[1.5pt]
        Method &MNIST &FASION-MNIST &CIFAR10\\
        \midrule
        SB-FBSDE* &0.95 &1.63 &2.96\\ 
        \textbf{SB-FBSDE++ (Ours)} &\textbf{0.91} &\textbf{1.56} & \textbf{2.93}\\
        \bottomrule
        \end{tabular}}
    \end{minipage}
    % \vspace{2cm}
    \begin{minipage}{0.48\textwidth} 
    \vspace{3mm}
        \centering
        \scalebox{0.75}{
        \begin{tabular}{lcccccccc}
        \toprule[1.5pt]
        K Images &10K &30K &50K & 100K & 150K & 200K \\
        \midrule
        VSDM* &\textbf{13.97} &7.12 &5.56 &3.64 &3.27 &3.04 \\ 
        \textbf{VSDM++ (Ours)} &14.28 &\textbf{6.93} &\textbf{5.31} &\textbf{3.24} &\textbf{3.15} &\textbf{2.97} \\
        \bottomrule
        \end{tabular}}
    \end{minipage}
\end{table}

