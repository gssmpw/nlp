% Supplementary material: To improve readability, you must use a single-column format for the supplementary material.
\onecolumn
\aistatstitle{Supplementary Materials for ``Optimal Stochastic Trace Estimation in Generative Modeling''}
% \maketitle
% \section{FORMATTING INSTRUCTIONS}

% To prepare a supplementary pdf file, we ask the authors to use \texttt{aistats2025.sty} as a style file and to follow the same formatting instructions as in the main paper.
% The only difference is that the supplementary material must be in a \emph{single-column} format.
% You can use \texttt{supplement.tex} in our starter pack as a starting point, or append the supplementary content to the main paper and split the final PDF into two separate files.

% Note that reviewers are under no obligation to examine your supplementary material.
\vspace{-5mm}
\setcounter{section}{0}
\section{Proofs for Theoretical Analysis}\label{sec:appendix_trace_estimate}
\subsection{Proof of Lemma 5.1}
The following result gives us an estimate of the expectation and variance for the Hutchinson trace estimator:\\
To compute the expectation:
\begin{align*}
  \E[H_m(\bA)] &= \frac{1}{m}\sum_{i=1}^{m}\E[\mathbf{v}_i^\top\bA\mathbf{v}_i] \\
  &= \E[\mathbf{v}_1^\top \bA \mathbf{v}_1] \\
  &= \sum_{i,j=1}^{n}\E[\bA_{ij}\mathbf{v}_{1,i}\mathbf{v}_{1,j}] \\
  &= \sum_{i, j=1}^{n}\bA_{ij}\delta_{ij} = \sum_{i=1}^{n}\bA_{ii} = \mathrm{Tr}(\bA).
\end{align*}
For the variance, let $\bA=\bO^\top\bL\bO$ be the eigendecomposition of $\bA$:
\begin{align*}
  \Var[H_m(\bA)] &= \frac{1}{m}\Var[\mathbf{v}_1^\top \bA \mathbf{v}_1] \\
  &= \frac{1}{m}\Var[\mathbf{v}_1^\top\bO^\top \bL \bO \mathbf{v}_1] \\
  &= \frac{1}{m}\Var\left[ (\bO\mathbf{v}_1)^\top \bL (\bO\mathbf{v}_1) \right] \\
  &= \frac{1}{m}\Var\left[ \sum_{i=1}^{D}\lambda_i |\mathbf{w}_{1,i}|^2 \right] \qquad (\text{$\mathbf{w}_1:=\bO\mathbf{v}_1$})\\
  &= \frac{1}{m}\sum_{i=1}^{D}\lambda_i^2 \Var[|\mathbf{w}_{1, i}|^2] = \frac{2}{m}\|\bA\|_F^2\le \frac{m}{2}\mathrm{Tr}^2(\bA).
\end{align*}

\subsection{Analysis of Hutch++ estimator}
\label{sec:appendix_hutchpp}
The key observation here is that $\|\mathbf{A}\|_F \approx \mathrm{Tr}(\mathbf{A})$ holds when $\mathbf{A}$ has only a few large eigenvalues. This happens because in such cases, the Frobenius norm, which sums the squares of the eigenvalues, is dominated by the largest ones, making it comparable to the trace, which sums the eigenvalues. 

On the other hand, for matrices with many small eigenvalues, we observe a different behavior. In such cases, the Frobenius norm and the trace diverge. Specifically, when most eigenvalues are small, we have:
\[
\|\mathbf{A}\|_F \approx \sqrt{n} \lambda_1 = \frac{1}{\sqrt{n}} n \lambda_1 \approx \frac{1}{\sqrt{n}} \mathrm{Tr}(\mathbf{A}),
\]
where $\lambda_1$ is the largest eigenvalue and $n$ is the matrix dimension. This means the Frobenius norm grows faster than the trace when there are many small eigenvalues.
Taking advantage of this dichotomy, Meyer et al. proposed the Hutch++ algorithm, which cleverly combines a low-rank approximation for large eigenvalues with Hutchinsonâ€™s estimator for the smaller ones. The algorithm is as follows:
$$
H_m^{++}(\mathbf{A}) = \mathrm{Tr}(Q^\top\mathbf{A}Q) + H_{\frac{m}{3}}((I-QQ^\top)\mathbf{A}),
$$
\newpage
where $Q^\top\mathbf{A}Q$ is a good low-rank approximation of $\mathbf{A}$ that captures the large eigenvalues, while the second term uses the Hutchinson estimator to approximate the trace of the remaining smaller eigenvalues.

\begin{lemma}
Let $\mathbf{A}$ be an $n \times n$ PSD (positive semidefinite) matrix, and let $\mathbf{A}_k$ denote the best rank-$k$ approximation of $\mathbf{A}$. Then,
\[
\|\mathbf{A} - \mathbf{A}_k\|_F \le \frac{1}{2\sqrt{k}} \mathrm{Tr}(\mathbf{A}).
\]
\end{lemma}

We begin by expressing the Frobenius norm of the difference between $\mathbf{A}$ and its rank-$k$ approximation in terms of the eigenvalues:
\[
\frac{\|\mathbf{A} - \mathbf{A}_k\|_F}{\mathrm{Tr}(\mathbf{A})} = \frac{\sqrt{\sum_{i=k+1}^{n} \lambda_i^2}}{\sum_{i=1}^{n} \lambda_i},
\]
where $\lambda_i$ are the eigenvalues of $\mathbf{A}$.

Using an inequality that approximates this expression, we can write:
\[
\frac{\|\mathbf{A} - \mathbf{A}_k\|_F}{\mathrm{Tr}(\mathbf{A})} \le \frac{\sqrt{\lambda_{k+1} \sum_{i=k+1}^{n} \lambda_i}}{\sum_{i=1}^{k} \lambda_i + \sum_{i=k+1}^{n} \lambda_i}.
\]
Let $a = \lambda_{k+1}$, $b = \sum_{i=1}^{k} \lambda_i$, and $\lambda = \sum_{i=k+1}^{n} \lambda_i$. The expression simplifies to
\[
\frac{\|\mathbf{A} - \mathbf{A}_k\|_F}{\mathrm{Tr}(\mathbf{A})} \le \frac{\sqrt{a\lambda}}{b + \lambda}.
\]
To minimize this expression, let $f(\lambda) = \frac{\sqrt{a\lambda}}{b + \lambda}$. We take the derivative:
\[
f'(\lambda) = \frac{a(b - \lambda)}{2\sqrt{a\lambda}(b + \lambda)^2}.
\]
Setting $f'(\lambda) = 0$, we find that the critical point occurs at $\lambda = b$, where the function attains its maximum.

Substituting $\lambda = b$ into the expression for $f(\lambda)$ gives:
\[
\frac{\|\mathbf{A} - \mathbf{A}_k\|_F}{\mathrm{Tr}(\mathbf{A})} \le f(b) = \frac{1}{2} \sqrt{\frac{a}{b}} = \frac{1}{2} \sqrt{\frac{\lambda_{k+1}}{\sum_{i=1}^{k} \lambda_i}} \le \frac{1}{2} \sqrt{\frac{\lambda_{k+1}}{k \lambda_{k+1}}} = \frac{1}{2\sqrt{k}}.
\]
Thus, we have the desired result.


For low-rank approximations, one can use a randomized approach by drawing a random $D \times (k + p)$ matrix $S$.

\begin{theorem}[\cite{halko2011finding}]
Let $Q$ be any orthonormal basis for $\mathbf{A} S$. Then,
\begin{equation}
    \mathbb{E}\left[ \|(I-QQ^\top) \mathbf{A}\|_F^2 \right] \le \left( 1 + \frac{k}{p-1} \right) \|\mathbf{A} - \mathbf{A}_{k}\|_F^2.
\end{equation}
\end{theorem}


%### Proof of Theorem \ref{thm:variance_hutchpp}
\subsection{Proof of Lemma 5.2}
For the expectation of Hutch++, by law of total expectation $\E[\E[X|Y]=\E[X]$, we can show
\begin{align}
    \E\left[ H_m^{++}(\bA) \right]&=\E\left[ \mathrm{Tr}(Q^\top \bA Q)+H_{\frac{m}{3}}( (I-QQ^\top)\bA) \right]\\
    &=\E\left[ \mathrm{Tr}(Q^\top \bA Q) \right]+\E\left[ \E\left[ H_{\frac{m}{3}}( (I-QQ^\top)\bA)|Q \right] \right]\\
    &=\E\left[ \mathrm{Tr}(Q^\top \bA Q) \right]+\E\left[ \mathrm{Tr}\left( (I-QQ^\top)\bA \right) \right]\\
    &=\E[\mathrm{Tr}(QQ^\top\bA)]+\E\left[ \mathrm{Tr}(\bA-QQ^\top\bA) \right]\\
    &=\E[\mathrm{Tr}(\bA)]=\mathrm{Tr}(\bA). 
\end{align}

Using the conditional variance formula for random variables $X$ and $Y$:
\[
\Var[X] = \E[\Var[X|Y]] + \Var(\E[X|Y]),
\]
we let $X = H_m^{++}(\bA)$ and $Y = Q$ to get
\begin{align}
    &\E\left[ \Var\left[ H_m^{++}(\bA)|Q \right] \right]\\
    &=\E\left[ \Var\left[ \mathrm{Tr}(Q^\top \bA Q)+H_{\frac{m}{3}}\left( (I-QQ^\top)\bA \right)|Q \right] \right]\\
    &=\E\left[ \Var\left[ H_{\frac{m}{3}}\left( (I-QQ^\top)\bA\right)|Q \right]\right]\\
    &\le \frac{6}{m}\E\left[ \|(I-QQ^\top)\bA\|_{F}^2 \right]\\
    &\le \frac{6}{m}\left( 1+\frac{k}{p-1} \right)\|\bA-\bA_{k}\|_{F}^2,
\end{align}
while
\begin{align}
    &\Var\left[ \E\left[ H_m^{++}(\bA)|Q \right] \right]\\
    &=\Var\left[ \E\left[ \mathrm{Tr}(Q^\top \bA Q)+H_{\frac{m}{3}}( (I-QQ^T)\bA)|Q \right] \right]\\
    &=\Var\left[ \mathrm{Tr}(Q^\top \bA Q)+\mathrm{Tr}( (I-QQ^\top)\bA) \right]\\
    &=\Var[\mathrm{Tr}(\bA)]=0.
\end{align}
Now we choose $p$, $k$ such that $p=k+1$ and $k+p=\frac{m}{3}$, i.e., $p=\frac{m+3}{6}, k=\frac{m-3}{6}$ we get that 
$$\Var[H_m^{++}(\bA)]\le \frac{12}{m}\|\bA-\bA_k\|_F^2\le \frac{12}{m}\frac{1}{4k}\mathrm{Tr}^2(\bA)=\frac{3}{km}\mathrm{Tr}^2(\bA)=\frac{18}{m(m-3)}\mathrm{Tr}^2(\bA).$$
For i.i.d. sub-Gaussian vairables, we have the following Hanson-Wright inequality
\begin{lemma} Let $\mathbf{v}\in\R^n$ be a vector of a vector of i.i.d. sub-Gaussian random variables of mean zero. Then it holds
  $$\PP\left( |\mathbf{v}^\top \bar\bA\mathbf{v}-\mathbb{E}[\mathbf{v}^\top \bar\bA\mathbf{v}]|>t \right)\le 2\exp\left( -c \cdot \min \left\{ \frac{t^2}{\|\bar{\mathbf{A}}\|_F^2}, \frac{t}{\|\bar{\bA}\|_2} \right\} \right).$$
  \label{lemma:HansonWright}
  Here $\|\bar{\bA}\|_2=\max_{\mathbf{w}\in\R^n}\|\bar\bA\mathbf{w}\|_2/\|\mathbf{w}\|_2$. 
\end{lemma}
Applying Lemma \ref{lemma:HansonWright} to $\bar{\bA}=\mathrm{diag}\{\bA, \cdots, \bA\}$, we can show
Following a similar argument in \cite{hutch_pp}, it holds that 
\begin{equation}
\begin{aligned}
  \PP(\varepsilon(H_m)\ge \varepsilon)= \PP\left( \frac{|H_m(\bA)-\mathrm{Tr}(\bA)|}{\mathrm{Tr}(\bA)}\ge \varepsilon \right)\le\delta\Rightarrow C\sqrt{\frac{\log(1/\delta)}{m}}<\varepsilon\Rightarrow m=\mathcal{O}\left(\frac{\log(1/\delta)}{\varepsilon^2}  \right).
\end{aligned}
\label{}
\end{equation}
With the quadratic improvement for the variance estimate, we obtain that 
  \begin{equation}
	\begin{aligned}
		\PP(\varepsilon(H_m^{++})\ge \varepsilon)= \PP\left( \frac{|H^{++}_m(\bA)-\mathrm{Tr}(\bA)|}{\mathrm{Tr}(\bA)}\ge \varepsilon \right)\le \delta\Rightarrow m=\mathcal{O}\left(\frac{\log(1/\delta)}{\varepsilon^2}+\log(1/\delta)  \right).
	\end{aligned}
	\label{}
\end{equation}

\subsection{Proof of Proposition 5.4}
Again, we use the law of total expectation to show
\begin{align*}
    \E\left[ \tilde{H}_m^{++}(\tilde{\bA}) \right]&=\E\left[ \mathrm{Tr}(Q^\top \tilde{\bA} Q)+H_{\frac{m}{3}}( (I-QQ^\top)\tilde{\bA}) \right]\\
    &=\E\left[ \mathrm{Tr}(Q^\top \tilde{\bA} Q) \right]+\E\left[ \E\left[ H_{\frac{m}{3}}( (I-QQ^\top)\tilde{\bA})|Q \right] \right]\\
    &=\E\left[ \mathrm{Tr}(Q^\top \tilde{\bA} Q) \right]+\E\left[ \mathrm{Tr}\left( (I-QQ^\top)\tilde{\bA} \right) \right]\\
    &=\E[\mathrm{Tr}(QQ^\top\tilde{\bA})]+\E\left[ \mathrm{Tr}(\tilde{\bA}-QQ^\top\tilde{\bA}) \right]\\
    &=\E[\mathrm{Tr}(\tilde{\bA})]=\mathrm{Tr}(\tilde{\bA}). 
  \end{align*}
Now we apply the conditional variance formula for $X = \tilde{H}_m^{++}(\tilde{\bA})$ and $Y = Q$
\begin{align*}
	&\E\left[ \Var\left[ \tilde{H}_m^{++}(\tilde{\bA})|Q \right] \right]\\
	&=\E\left[ \Var\left[ \mathrm{Tr}(Q^\top \tilde{\bA} Q)+H_{\frac{m}{3}}\left( (I-QQ^\top)\tilde{\bA} \right)|Q \right] \right]\\
	&=\E\left[ \Var\left[ H_{\frac{m}{3}}\left( (I-QQ^\top)\tilde{\bA}\right)|Q \right]\right]\\
	&\le \frac{6}{m}\E\left[ \|(I-QQ^\top)\tilde{\bA}\|_{F}^2 \right]\\
    &\le \frac{12}{m}\E\left[ \|(I-QQ^\top)\bA\|_{F}^2 \right]+\frac{12}{m}  \E\left[\|(I-QQ)^\top(\tilde{\bA}-\bA) \|^2_F \right]              \\
	&\le \frac{12}{m}\left[\left( 1+\frac{k}{p-1} \right)\|\bA-\bA_{k}\|_{F}^2+\mathcal{O}((L_s-1)^2\eta^2)\right]\\
	&\le\frac{36}{m(m-3)}\mathrm{Tr}^2(\bA)+\frac{1}{m}\mathcal{O}((L_s-1)^2\eta^2),
\end{align*}
where we choose $p=\frac{m+3}{6}, k=\frac{m-3}{6}$. Meanwhile, 
\begin{align*}
	&\Var\left[ \E\left[ \tilde{H}_m^{++}(\tilde{\bA})|Q \right] \right]\\
	&=\Var\left[ \E\left[ \mathrm{Tr}(Q^\top \tilde{\bA} Q)+H_{\frac{m}{3}}( (I-QQ^T)\tilde{\bA})|Q \right] \right]\\
	&=\Var\left[ \mathrm{Tr}(Q^\top \tilde{\bA} Q)+\mathrm{Tr}( (I-QQ^\top)\tilde{\bA}) \right]\\
	&=\Var[\mathrm{Tr}(\tilde{\bA})]=0.
\end{align*}
then we could get that 
$$\mathrm{Var}[\tilde{H}^{++}(\tilde{\bA})]\le \frac{36}{m(m-3)}\mathrm{Tr}^2(\bA)+\frac{1}{m}\mathcal{O}((L_s-1)^2\eta^2).$$

\subsection{Complexity analysis}
The computational complexity of Hutch++ for $\bA\in \R^{D\times D}$ mainly comes from two parts:

Part I: multiplying $\bA$ with a thin matrix $Q$ requires $O(D^2m)$ operations;

Part II: conducting QR decomposition for the span of $\bA \bS$  to get an orthogonal matrix $Q\in \mathbb{R}^{D\times \mathcal{O}(m)}$ requires $\mathcal{O}(c D m^2)$, where $c > 1$ denotes the slowdown factor in computations of matrix decomposition compared to matrix multiplication due to the parallelism and hardware optimization in GPUs.

Specifically, our Hutch++ proposes conducting QR decomposition every $L_s$ iterations, which leads to a reduced computation $
\mathcal{O}\left(\frac{c D m^2}{L_s}\right)$ in part II on average. By Proposition 5.4, our approximate
Hutch++ needs
\begin{equation} \mathcal{O}\bigg(\frac{c D m^2}{L_s} + D^2 m\bigg) \text{ operations to achieve a variance } \mathcal{O}\bigg(\frac{1}{m^2} + \frac{\sigma^2}{m}\bigg) \end{equation}

where $\sigma^2=\mathcal{O}((L_s-1)^2 \eta^2)$.

Meanwhile,  the vanilla Hutchinson estimator requires $\mathcal{O}(m D^2)$ operations to achieve a variance of $\mathcal{O}\left(\frac{1}{m}\right)$. To achieve a similar scale of variance as our approximate Hutch++, Hutch requires a complexity $$\mathcal{O}\bigg(\frac{D^2}{\frac{1}{m^2} + \frac{\sigma^2}{m}}\bigg)=\mathcal{O}\bigg(\frac{D^2 m^2}{1 + m\sigma^2}\bigg)$$

Given smooth enough changes of the divergence $A_t$ w.r.t. $t$ such that $\sigma<1$, we can see Hutch++ is cheaper when

\begin{equation} \begin{aligned} & \underbrace{\mathcal{O}\bigg(\frac{c Dm^2}{L_s}+D^2m\bigg)}_{\text{Approximate Hutch++ cost}}<\underbrace{\mathcal{O}\bigg(\frac{D^2 m^2}{1+m\sigma^2}\bigg)}_{\text{Vinilla Hutch cost}}. \end{aligned}\label{eqn:costcomp} \end{equation}

%When $c=1$, i.e. no parallelism and no hardware optimization, we can simply set $L_s=1$ and the above inequality always holds for a high accuracy target;

For $c\gg 1$ and $\sigma<1$, we can choose $ L_s\approx \frac{cm}{D}$ such that (\ref{eqn:costcomp}) is valid, which demonstrates the necessity of reusing eigenvectors along the trajectory of the integral for Hutch++.

Similarly, instead of reusing eigenvectors only at neighboring points within the same iteration, they can also be reused across subsequent iterations, with a comparable analysis applying. Strategies that reuse previous gradients every \(K\) iterations have achieved computational success, including Stochastic Variance Reduced Gradient (SVRG) \cite{johnson2013accelerating} and Federated Averaging (FedAvg) \cite{mcmahan2017communication, li2019convergence}.

\subsection{Proof of Proposition 5.5}
In Neural ODE, the log-likelyhood could be computed by 
\begin{equation}
  \log p_\theta (\mathbf{z}(t_1))=\log q(\mathbf{z}(t_0))-\int_{t_0}^{t_1}\mathrm{Tr}\left( \frac{\pa f}{\pa \mathbf{z}} \right)dt.
  \label{}
\end{equation}
After applying the approximate Hutch++ estimator, we have 
\begin{equation}
  \log \tilde{p}_\theta(t_1)=\log q(\mathbf{z}(t_0))-\int_{t_0}^{t_1}\tilde{H}^{++}_m\left( \frac{\pa f}{\pa \mathbf{z}} \right)dt.
  \label{}
\end{equation}
Taking the expectation and use the fact that $\E[\tilde{H}^{++}_m(\bA)]=\mathrm{Tr}(\bA)$, we get that 
\begin{align*}
  \E[\log \tilde{p}_\theta(t_1)]&=\log q(\mathbf{z}(t_0))-\E\int_{t_0}^{t_1}\tilde{H}^{++}_m\left( \frac{\pa f}{\pa \mathbf{z}} \right)dt
  \\
  &=\log q(\mathbf{z}(t_0))-\int_{t_0}^{t_1}\E\left[ \tilde{H}^{++}_m\left( \frac{\pa f}{\pa \mathbf{z}} \right) \right]dt\\
  &=\log q(\mathbf{z}(t_0))-\int_{t_0}^{t_1} \mathrm{Tr}\left( \frac{\pa f}{\pa \mathbf{z}} \right)dt\\
  &=\log p_\theta(\mathbf{z}(t_1)).
\end{align*}
For the variance estimate:
\begin{align*}
  \mathrm{Var}[\log \tilde{p}_\theta(t_1)]&=\E|\log \tilde{p}_\theta(t_1)-\log p_\theta(\mathbf{z}(t_1))|^2\\
  &=\E\left|\int_{t_0}^{t_1}\left[\tilde{H}^{++}_m\left( \frac{\pa f}{\pa \mathbf{z}} \right)-\mathrm{Tr}\left( \frac{\pa f}{\pa \mathbf{z}} \right)\right]dt\right|^2\\
  &\le (t_1-t_0)\E\left[\int_{t_0}^{t_1}\left|\tilde{H}^{++}_m\left( \frac{\pa f}{\pa \mathbf{z}} \right)-\mathrm{Tr}\left( \frac{\pa f}{\pa \mathbf{z}} \right)\right|^2 dt\right]\\
  &=(t_1-t_0)\int_{t_0}^{t_1}\E\left[ \tilde{H}^{++}_m\left( \frac{\pa f}{\pa \mathbf{z}} \right)-\mathrm{Tr}\left( \frac{\pa f}{\pa \mathbf{z}} \right) \right]^2 dt\\
  &= (t_1-t_0)\int_{t_0}^{t_1}\mathrm{Var}\left[ \tilde{H}^{++}_m\left( \frac{\pa f}{\pa \mathbf{z}} \right) \right]dt\\
  &\le (t_1-t_0)\int_{t_0}^{t_1}\left[\frac{36}{m(m-3)}M^2+\frac{1}{m}\mathcal{O}((L_s-1)^2\eta^2)\right]dt\\
  &\le (t_1-t_0)^2\left[ \frac{36}{m(m-3)}M^2+\frac{1}{m}\mathcal{O}((L_s-1)^2\eta^2) \right].
\end{align*}


\section{Experimental Details}
\subsection{Datasets}

\paragraph{Simulation data}
We compare the FFJORD++ with FFJORD on several toy distributions, including \texttt{2spirals}, \texttt{checkerboard}, \texttt{rings} and \texttt{circles}.

\paragraph{Time series dataset for imputation }
We run experiments for time series imputation on two datasets. 
\texttt{PM2.5}, a air quality dataset~\citep{yi2016pm25}, consists of the hourly sampled PM2.5 air quality index from 36 monitoring stations for 12 months. Approximately 13\% of the data is missing, and the patterns of missing values are not random. The ground truth values at the missing points are known and we use them as the test data set.
\texttt{PhysioNet}, a healthcare dataset in PhysioNet Challenge 2012~\citep{silva2012physio}, consists of 4000 clinical time series with 35 variables for 48 hours from intensive care unit (ICU). Following the processing method in previous work~\citep{provably_schrodinger_bridge, CSDI, cao2018data_precoss1, che2018data_precoss2}, we aggregate data to one point per hour. The missing rate of the dataset is about 80\%. We randomly mask a specific percentage (10\%, 50\%) of the data that comprise the test dataset, denoted as \texttt{PhysioNet 0.1} and \texttt{PhysioNet 0.5}, respectively.

\paragraph{Time series dataset for forecasting}
For the forecast task, we validated our method on three datasets with varying feature dimensions, which are collected and preprocessed in GluonTS~\citet{alexandrov2020gluonts}.
\Cref{tab: time_stat} summarizes the properties of these datasets including the feature dimension, total time steps, history steps and prediction steps for each datasets.



\begin{table}[!h]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{c|c|c|c|c}
    \toprule
     \multirow{2}{*}{} & feature &total &history &prediction\\ 
     & dimension & time step & steps & steps \\
     \midrule
     Exchange & 8& 6071& 180& 30\\
     Solar & 137& 7009& 168& 24\\
     Electricity & 370 &5833& 168& 24\\
    \bottomrule
    \end{tabular}}
    \caption{\small Overview of the datasets used in time series forecasting}
    \label{tab: time_stat}
\end{table}

\subsection{Implementation Details}
We set the same hyper-parameters for each pair of both base models and our improved models which employ Hutch++ estimator.
Specifically, we set the learning rate to 5e-4 and use the second-order midpoint solver as the default ODE solver, with the same number of function evaluations of 100 in the simulation.
Additionally, we used all the parameters from their corresponding open-source code for the remaining experiments, without making any modifications.
As discussed in Section 6.1, we set the default shared step size $L_s = 10$ for all experiments. For models training with random time point in SB-based diffusion models, we re-design the training strategy based on continuous time interval and update $Q$ every $L_s$ steps.

\subsection{Computing Resources}
For all experiments, we utilized the PyTorch \citep{paszke2019pytorch} framework to implement all methods and trained models with NVIDIA GeForce RTX 4090 and RTX A6000 GPUs.

\subsection{Limitations}
To avoid computing $\frac{\partial z_t}{\partial \mathbf{x}(t)} S$ directly, 
the Jcacobian-vector products  $\frac{\partial z_t}{\partial \mathbf{x}(t)} S$ can be computed utilizing advanced deep learning frameworks through two rounds of reverse-mode automatic differentiation, whose time cost can be neglected compared to forward and backward propagation in the neural network.
However, the gradients collected in the reverse-mode automatic differentiation lead to large and non-negligible memory costs.
% This results in a small number of matrix-vector multiplication queries, limiting the greater potential of using the Hutch++ estimator in generative modeling tasks.
Therefore, how to more efficiently train divergence-based likelihood in diffusion models with optimal guarantee is still the promising direction in future work.

\section{Additional Experimental Results}
\subsection{Simulations}
\Cref{fig: ap_vis_train} visualizes the comparison of the density estimated by FFJORD and FFJORD++ during the training stage on \texttt{2spirals}, \texttt{checkerboard}, \texttt{rings} and \texttt{circles}.
We provide additional results of comparison between the FFJORD++ and FFJORD on three distinctive shapes (\texttt{2spirals}, \texttt{checkerboard} and \texttt{circles}) with various scales in \Cref{fig: ap_condition_num}.
\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        % \caption{2Spirals}
        \includegraphics[width=\textwidth]{fig/2s_convergence.pdf}
        \caption{2spirals}
        \label{fig: 2s}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        % \caption{Checkerboard}
        \includegraphics[width=\textwidth]{fig/cb_convergence.pdf}
        \caption{checkerboard}
        \label{fig: cb}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        % \caption{2Spirals}
        \includegraphics[width=\textwidth]{fig/cc_convergence.pdf}
        \caption{rings}
        \label{fig: 2s}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        % \caption{Checkerboard}
        \includegraphics[width=\textwidth]{fig/r_convergence.pdf}
        \caption{circles}
        \label{fig: cb}
    \end{subfigure}
    
    \caption{\small Visualization of density estimation obtained by \textbf{(Top)} FFJORD and \textbf{(Bottom)} FJORD++ during the training phase on four toy distributions in each sub-figure. The proposed variance reduced model, FJORD++, improves both convergence and training stability, resulting in higher-quality estimated densities.}
    \label{fig: ap_vis_train}
\end{figure}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/2s.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/cb.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/cc.pdf}
    \end{subfigure}

    \caption{\small Comparison between the FFJORD++ and FFJORD on data from three distinct shapes with various scales. \textbf{From top to bottom}, each column corresponds to \texttt{2spirals} and \texttt{checkerboard} and \texttt{circles}, respectively. \textbf{From left to right}, each row represents the data with a default scale, as well as shapes stretched 2x and 4x along the X-axis, respectively. As the scale along the X-axis increases, FFJORD++ consistently exhibits superior convergence rates, further widening the performance gap with FFJORD.}

    \label{fig: ap_condition_num}
\end{figure}

\subsection{Time Series Modeling}
\Cref{fig: ap_im_pm25} and \Cref{fig: ap_im_phy} show time series imputation examples obtained by CSBI++ for 20 dimensions on \texttt{PM25} and \texttt{Physio 0.5}, respectively.
\Cref{fig: ap_fore_elec} and \Cref{fig: ap_fore_solar} show time series forecasting examples obtained by CSBI++ for 20 dimensions on \texttt{Electricity} and \texttt{Solar}, respectively.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{fig/PM25.pdf}
    \caption{\small \textbf{Imputation} examples for \texttt{PM25} for 20 (out of 36) dimensions.}
    \label{fig: ap_im_pm25}
\end{figure}
\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{fig/physio.pdf}
    \caption{\small \textbf{Imputation} examples for \texttt{Physio 0.5} for 20 (out of 36) dimensions.}
    \label{fig: ap_im_phy}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{fig/elec.pdf}
    \caption{\small \textbf{Forecasting} examples for \texttt{Electricity} for 20 (out of 370) dimensions}
    \label{fig: ap_fore_elec}
\end{figure}
\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{fig/solar.pdf}
    \caption{\small \textbf{Forecasting} examples for \texttt{Solar} for 20 (out of 370) dimensions}
    \label{fig: ap_fore_solar}
\end{figure}

\subsection{Image Data Modeling}
We provide images generated by SB-FBSDE++ on three datasets with different patterns in \Cref{fig: ap_image}.

\begin{figure*}[!t]
    \centering
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/mnist.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/fashion-mnist.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/cifar10.png}
    \end{subfigure}
    \caption{\small Uncurated samples generated by SB-FBSDE++ on \textbf{(Top)} \texttt{MNIST}, \textbf{(Middle)} \texttt{Fashion-MNIST} and \textbf{(Bottom)} \texttt{CIFAR10}}

    \label{fig: ap_image}
\end{figure*}

