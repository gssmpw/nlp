\section{Theoretical Analysis}

\subsection{Preliminaries}

The Hutchinson trace estimator provides an efficient means to estimate the trace of a matrix, with the following properties:

\begin{lemma}[\cite{Hutchinson89}]
\begin{equation}
    \mathbb{E}[H_m(\mathbf{A})] = \mathrm{Tr}(\mathbf{A}), \; \mathrm{Var}[H_m(\mathbf{A})] \le \frac{2}{m} \mathrm{Tr}^2(\mathbf{A}), \label{eqn:VanHut}
\end{equation}
where $H_m(\mathbf{A})$ is the Hutchinson estimator with $m$ random vectors, $\mathbb{E}[\cdot]$ denotes expectation, and $\mathrm{Var}[\cdot]$ denotes variance.
\end{lemma}

To reduce the variance further, especially for matrices with large dominant eigenvalues, we can leverage a low-rank QR approximation. The Hutch++ estimator improves upon the original Hutchinson method by significantly reducing the variance:

\begin{lemma}[\cite{hutch_pp}] 
Suppose $\mathbf{A}$ is a \emph{symmetric positive semidefinite (PSD)} matrix. Then, the Hutch++ estimator satisfies:
\begin{equation}
    \mathbb{E}[H_m^{++}(\mathbf{A})] = \mathrm{Tr}(\mathbf{A}),\mathrm{Var}[H_m^{++}(\mathbf{A})] \le \frac{18}{m(m-3)} \mathrm{Tr}^2(\mathbf{A}),
\end{equation}
where $H_m^{++}(\mathbf{A})$ is the Hutch++ estimator.
\end{lemma}

It is worth noting that the PSD assumption can be generalized to estimates involving the nuclear norm of non-PSD matrices. However, in this paper, we focus on matrices where large eigenvalues are the primary concern, and thus the PSD assumption always holds.

We define the \emph{relative error} of an estimator $T(\mathbf{A})$ as:
\begin{equation*}
    \varepsilon(T) := \frac{|T(\mathbf{A}) - \mathrm{Tr}(\mathbf{A})|}{|\mathrm{Tr}(\mathbf{A})|}.
\end{equation*}

A key advantage of Hutch++ is its enhanced variance reduction, which leads to significant computational savings compared to the original Hutchinson estimator. Specifically:

\begin{proposition}[\cite{hutch_pp}]
    For a given error threshold $\varepsilon > 0$ and confidence level $\delta > 0$, the following holds with probability at least $1 - \delta$:
    \begin{itemize}
        \item The error of the Hutchinson estimator satisfies $\varepsilon(H_m) < \varepsilon$ when $m = \mathcal{O}\Big(\frac{\log(1/\delta)}{\varepsilon^2}\Big)$.
        \item The error of the Hutch++ estimator satisfies $\varepsilon(H_m^{++}) < \varepsilon$ when $m = \mathcal{O}\Big(\sqrt{\frac{\log(1/\delta)}{\varepsilon^2}} + \log(1/\delta)\Big)$.
    \end{itemize}
\end{proposition}

This result highlights that Hutch++ achieves a quadratic reduction in the required number of samples $m$ for a given accuracy, compared to the original Hutchinson estimator.

\subsection{Error Analysis for Acceleration}
Naïvely applying Hutch++ in generative modeling can incur significant computational costs due to the need for QR decompositions at every iteration. To mitigate this, we propose updating the QR decomposition every $L_s$ iterations. While this approach means the estimated eigenvalues may not always be fully up to date, it strikes a balance between computational efficiency and accuracy.
We now analyze the errors introduced by perturbations in the matrix
$\mathbf{A}$. Let $\widetilde{\mathbf{A}}$ be a perturbed version of
$\mathbf{A}$, by the smoothness of the dynamics, the difference of their traces is proportional to a small
time increment $\eta$ and the frequency of steps in QR updates $L_s$, i.e.,$\mathrm{Tr}(\widetilde{\mathbf{A}}) =\mathrm{Tr}(\mathbf{A}) +
\mathcal{O}((L_s-1)\eta)$. Consider the matrix $Q = \mathrm{QR}(\mathbf{A}S)$,
where $S$ is a random sketching matrix. The \emph{approximate Hutch++ estimator}
for this perturbed matrix is given by:
\begin{equation*}
\scalebox{0.9}{$
    \widetilde{H}_m^{++}(\widetilde{\mathbf{A}}) := \mathrm{Tr}(Q^\top \widetilde{\mathbf{A}} Q) + H_{\frac{m}{3}}\left( (I - QQ^\top) \widetilde{\mathbf{A}} (I - QQ^\top) \right).$}
\end{equation*}

This approximation results from reducing the frequency of QR updates, which is central to our acceleration method. For example, using the frozen QR decomposition (Equation \ref{eqn:acceleration}) over the time subinterval $[t_i, t_{i+L_s}]$, $\frac{\partial z_t}{\partial \mathbf{x}}$ behaves as a perturbation of $\frac{\partial z_t}{\partial \mathbf{x}\left(\lfloor\frac{L}{L_s} t_i\rfloor \cdot \frac{L_s}{L}\right)}$.

We justify the effectiveness of this estimator by proving the following expectation and variance bounds:
\begin{proposition}\label{prop:approximateHut}
\begin{align}
\scalebox{0.9}{$
    \mathbb{E}[\widetilde{H}_m^{++}(\widetilde{\mathbf{A}})] = \mathrm{Tr}(\widetilde{\mathbf{A}})$},\hspace{3.35cm}\tag{14}\label{eqn:approximateExp}\\
\scalebox{0.9}{$
    \mathrm{Var}(\widetilde{H}_m^{++}(\widetilde{\mathbf{A}})) \le \frac{36}{m(m-3)} \mathrm{Tr}^2(\mathbf{A}) +  \frac{1}{m}\mathcal{O}((L_s-1)^2\eta^2).$}\tag{15}\label{eqn:approximateVar}
\end{align}
\end{proposition}
In Equation \ref{eqn:approximateExp}, we observe that the approximate Hutch++ estimator provides the exact trace of $\tilde{\mathbf{A}}$, demonstrating its robustness to the QR decomposition used in the acceleration process. Additionally, in Equation \ref{eqn:approximateVar}, the approximate Hutch++ estimator achieves an approximately quadratic variance reduction similar as the standard Hutch++ estimator.  
Conducting QR decomposition only every \(L_s\) iterations reduces computations while maintaining a variance reduction comparable to the vanilla version. Regarding the specific guidance on the selection of \(L_s\), we refer to interested readers to the complexity analysis section 1.5 in the supplementary file.


\subsection{Error Propagation in Divergence-Based Likelihood Training}\label{sec: error_propagation}

The accumulation of error during the training of divergence-based likelihoods can be analyzed as follows. In the case of Neural ODEs \cite{neural_ode}, by applying the frozen QR decomposition (Equation \ref{eqn:acceleration}) over the time subinterval \( [t_i, t_{i+L_s}] \) for Equation \ref{eq: ode_likelihood}, we estimate the trace term, assuming that the approximate Hutch++ operator is adapted to \( \frac{\partial f}{\partial \mathbf{z}} \) within this interval. Let \( \log \tilde{p}_\theta(t) \) denote the resulting log-likelihood under this approximation. Using Proposition \ref{prop:approximateHut}, we can then establish the following result:

\setcounter{equation}{15}
\begin{proposition}
Let \( M = \max\{|\mathrm{Tr}(\frac{\partial f}{\partial \mathbf{z}})| : t \in [0, T]\} \). Then for \( t \in [0, T] \), we have:
\begin{equation}
\scalebox{0.95}{$
\begin{aligned}
  \mathbb{E}\left[ \log \tilde{p}_\theta(t) \right] &= \log p_\theta(\mathbf{z}(t)),\\
  \mathrm{Var}[\log \tilde{p}_\theta(t)] &\leq T^2 \left[\frac{36M^2}{m(m-3)} + \frac{1}{m}\mathcal{O}((L_s-1)^2\eta^2)\right].
\end{aligned}\label{eqn:loglikelihoodest}$}
\end{equation}
\end{proposition}

Since the divergence-based likelihood of the Schrödinger bridge \citep{forward_backward_SDE, mSB} differs from neural ODEs only by an additional deterministic inner product (see Equation \ref{eq: fb-sde-train-f}) vs. the ISM loss in \cite{VSDM}), the variance reduction achieved by Hutch++ estimators in Equation \ref{eqn:loglikelihoodest} naturally extends to Schrödinger bridge (SB)-based diffusion models \citep{forward_backward_SDE, mSB, VSDM} as well.
