\section{Introduction}


Diffusion models and score-based generative models (SGMs) \citep{DDPM, score_sde} have been widely used in areas such as text-to-image synthesis, video generation, and audio synthesis \citep{text_2_image, imagen_video, DiffWave}. The simulation-free diffusion process results in a simple and elegant loss function, which enhances training stability, flexibility, and interoperability, and significantly improves scalability. However, diffusion models are known to suffer from weak transport properties \citep{Lavenant_Santambrogio_22} and require many function evaluations to generate the desired data. 



SGMs draw inspiration from score matching \citep{score_matching}, which was originally developed to address energy-based models. However, the requirement to compute the trace of the model's density renders this approach unscalable in relation to data dimensions. To mitigate this challenge, \cite{Vincent_2011} transformed score matching into a denoising problem, thus circumventing the need for trace computation. Additionally, \cite{estimate_Hessian_curvature} proposed a rank-1 approximation of the Hessian by backpropagating curvature. For more efficient trace estimators, methods such as sliced score matching and FFJORD \citep{FFJORD, song2020sliced} employed the Hutchinson estimator \citep{Hutchinson89} to enhance computation by leveraging deterministic approximation of large eigenvalues and Hessian-vector products and enables more efficient transportation plans in diffusion studies.

Since the introduction of the Hutchinson estimator, significant progress has been made in diffusion models with optimal transport (OT) guarantees. For instance, the Trajectory Net (trajNet) \citep{TrajectoryNet} established a connection between continuous normalizing flows (CNF) \citep{neural_ode} and dynamic optimal transport \citep{Benamou_Brenier_2020}. Furthermore, the Schrödinger bridge \citep{forward_backward_SDE, mSB, SBP_max_llk, reflected_schrodinger_bridge, VSDM, VSMD} explored a principled framework for dynamic optimal transport \citep{DSB} by training divergence-based objectives. However, despite the advantages offered by optimal transport properties, the high variance associated with the Hutchinson estimator has posed scalability challenges, making it less favorable in real-world generative modeling tasks.

To address the large-variance issue directly while maintaining transport optimality, we study variance reduction techniques to minimize the variance of the Hutchinson estimator without resorting to simulation-free solutions \citep{flow_matching, Albergo_stochastic_interpolants}. Hutchinson estimators are known to yield reasonable approximations with a flat spectrum. However, this approach contradicts empirical evidence showing that generated high-dimensional data lies in a manifold of low intrinsic dimension \citep{fefferman2016testing, diffusion_manifold, Manifolds_Hypothesis_image}. Therefore, we propose leveraging Hutch++, the optimal stochastic trace estimator \citep{hutch_pp}, to compute large eigenvalues in a deterministic manner to reduce variance and approximate the rest using the Hutchinson estimator. We demonstrated the effectiveness of the proposed method across a wide range of generative models, including neural ODEs \citep{neural_ode, FFJORD}, sliced score matching \citep{song2020sliced}, TrajNet \citep{TrajectoryNet}, and Schrödinger-based diffusion models \citep{forward_backward_SDE, mSB, SBP_max_llk, VSDM, VSMD}. Our method also has the potential to be applied to other generative models.
We summarize our contributions in four aspects:

\begin{itemize}
    \item We propose Hutch++ estimators to optimally minimize variance, thereby improving the trace estimation of a large class of diffusion models with optimal transport guarantees.
    \item We identify that conducting matrix decompositions too frequently incurs high costs in generative modeling. We propose a practical algorithm that amortizes the decompositions to reduce costs.
    \item We provide theoretical guarantees to demonstrate the higher fidelity of the generated content using Hutch++ estimators.  
    \item The effectiveness of our proposed models is tested in simulations, time series forecasts, and image generation in both conditional and unconditional settings.  
\end{itemize}

