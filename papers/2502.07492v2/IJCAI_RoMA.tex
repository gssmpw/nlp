%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

% my add
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{setspace}
\usepackage[table]{xcolor}
% Comment out this line in the camera-ready submission


\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{RoMA: Robust Malware Attribution via Byte-level Adversarial Training with Global Perturbations and Adversarial Consistency Regularization}

\author{
Yuxia Sun$^1$
\and
Huihong Chen$^1$\and
Jingcai Guo$^{2}$\and
Aoxiang Sun$^3$\and
Zhetao Li$^1$\and
Haolin Liu$^4$\\
\affiliations
$^1$College of Information Science and Technology, Jinan University\\
$^2$Department of Computing,
The Hong Kong Polytechnic University\\
$^3$College of Information and Computational Science, Jilin University\\
$^4$School of Computer Science, Xiangtan University\\
\emails
tyxsun@email.jnu.edu.cn,
chhjv@stu.jnu.edu.cn,
jc-jingcai.guo@polyu.edu.hk,
2835604014@qq.com,
liztchina@hotmail.com (corresponding author),
liu.haolin@foxmail.com
}

\begin{document}

\maketitle

\begin{abstract}
Attributing APT (Advanced Persistent Threat) malware to their respective groups is crucial for threat intelligence and cybersecurity. However, APT adversaries often conceal their identities, rendering attribution inherently adversarial. Existing machine learning-based attribution models, while effective, remain highly vulnerable to adversarial attacks. For example, the state-of-the-art byte-level model MalConv sees its accuracy drop from over 90\% to below 2\% under PGD (Projected Gradient Descent) attacks. Existing gradient-based adversarial training techniques for malware detection or image processing were applied to malware attribution in this study, revealing that both robustness and training efficiency require significant improvement. To address this, we propose RoMA, a novel single-step adversarial training approach that integrates global perturbations to generate enhanced adversarial samples and employs adversarial consistency regularization to improve representation quality and resilience. A novel APT malware dataset named AMG18, with diverse samples and realistic class imbalances, is introduced for evaluation. Extensive experiments show that RoMA significantly outperforms seven competing methods in both adversarial robustness (e.g., achieving over 80\% Robust Accuracy—more than twice that of the next-best method under PGD attacks) and training efficiency (e.g., more than twice as fast as the second-best method in terms of accuracy), while maintaining superior Standard Accuracy in non-adversarial scenarios.
The RoMA-trained model and dataset samples are publicly available at \url{https://anonymous.4open.science/r/RoMA-D767}.
\end{abstract}

%(e.g. around 1.5 times faster than the two-step training method ranking the second in accuracy)

\section{Introduction}

\label{sec:introduction}
Advanced Persistent Threat (APT) refers to sophisticated, covert cyberattacks typically conducted by organized groups with ties to nation-states or criminal organizations \cite{ref63}. Tracing APT groups is essential for identifying threat actors and preventing future attacks. APT malware attribution, the process of linking malware to its respective APT group, plays a critical role in this defense strategy. However, APT attackers often conceal their identities, sometimes impersonating other groups to mislead investigators, rendering APT malware attribution a challenging and adversarial task \cite{ref61,ref64}. Given the significant threat posed by Windows malware to both individuals and organizations \cite{ref5,ref6}, this study focuses on Portable Executable (PE) APT malware.

%% [24threadreport]:
%CrowdStrike, “CrowdStrike 2024 Global Threat Report.” 2024.[Online]. Available: https://www.crowdstrike.com/global-threat-report/, [Accessed: 1-11-2024].
%% [ncsc]
%NCSC and NSA, “Advisory: Turla group exploits Iranian APT to expand coverage of victims.” 2019. [Online]. Available:https://www.ncsc.gov.uk/news/turla-group-exploits-iran-apt-to-expand-coverage-of-victims, [Accessed: 1-11-2024]

 
In recent years, automatic APT malware group attribution has been widely explored using dynamic and static methods. Dynamic approaches, which analyze runtime behaviors typically within virtual environments, are time-consuming and ineffective against virtual-machine escape techniques \cite{ref9,ref8}. Static methods address these issues by directly analyzing malware files, extracting features like statistical properties and program graphs from binary or disassembly code, and using classifiers like Random Forest, LSTM, and PerceiverIO \cite{ref15,ref17,ref61}. Raw-byte-based approaches further simplify the process by bypassing feature extraction and disassembly, directly processing raw byte sequences through end-to-end CNN models to capture finer details \cite{ref58}.
%\cite{ref24,ref58}.
The state-of-the-art (SOTA) raw-byte classifier, MalConv with Global Channel Gating (GCG), uses a 1D-CNN to process raw byte sequences, achieving high accuracy in malware classification \cite{ref24}. 

\vspace{0.15cm}
\noindent\textbf{Limitations. }
Current APT malware attribution models remain vulnerable to adversarial attacks \cite{ref28}, as demonstrated by MalConv’s accuracy dropping from over 90\% to below 2\% under PGD attacks (Section \ref{sec5}). 
To enhance the robustness of raw-byte malware detectors, gradient-based adversarial training approaches have been explored. FGSM (Fast Gradient Sign Method)-based methods, including Slack-FGSM and Append-FGSM, apply adversarial perturbations to regions like slack space and file ends in PE files \cite{ref31,ref29,ref27}. While fast due to single-step training, these methods are susceptible to multi-step attacks. Recently, Lucas et al. applied multi-step PGD (Projected Gradient Descent)  training method to malware detectors, using existing perturbation techniques to mitigate high-effort PGD attacks \cite{ref55,ref56}. However, this method incurs significant training time and lack novel adversarial training techniques. Our experiments adapt these adversarial training approaches, originally developed for malware detection or image processing, to the malware attribution task, showing that both robustness and training efficiency still require substantial improvement (See Section \ref{sec5}).





% In recent years, there have been notable advancements in machine-learning-based APT malware attribution approaches \cite{ref1,ref2}. These approaches have convincingly demonstrated their ability to accurately attribute APT malware to their respective APT groups with high accuracy. However, a common limitation in these approaches is the omission of a crucial consideration: the need for adversarial robustness within attribution models. Furthermore, many of these approaches heavily rely on feature engineering, which involves manual efforts. In contrast, they have not fully embraced raw-binary-based methods, which have the potential to provide greater generality and cost-effectiveness. 

% In this research, we pioneered the development of an raw-binary APT malware attribution model by building upon the malware detection architecture of MalConv with GCG (Global Channel Gating) \cite{ref3}, and further enhancing it with a multi-group classification head. However, our research has uncovered a significant challenge: while the raw-binary attribution model attains an impressive standard accuracy (SA) over 90\% on clean malware samples, it exhibits a substantially lower level of robust accuracy (RA) at 1.72\% when exposed to adversarial attacks. 

% Moreover, in the context of real-world systems, the capacity for swift training and deployment of cybersecurity models is paramount to defend against evolving threats. Thus, when dealing with APT malware, it's practical to have attribution models that can be rapidly trained without substantial delays. While adversarial training (AT) is key to developing robust attribution models, it is notably more time-consuming than standard training methods. Our study highlights this issue, demonstrating that a two-step adversarial training process can require more than 10 times the duration of non-adversarial training. This presents a significant challenge in scenarios where speed is of the essence.

\vspace{0.15cm}
\noindent\textbf{Contributions. } To address these challenges, we propose RoMA, a novel single-step adversarial training approach that efficiently trains a robust raw-byte attribution model for APT malware, offering resilience against advanced adversarial attacks, including both multi-step and optimization-based attacks. RoMA enhances FGSM-based adversarial training through two key strategies: (1) Global Perturbation (GP) strategy, which generates stronger adversarial malware by adaptively applying learned perturbation patterns (GPs) to four types of perturbation positions within malware; (2) Adversarial Consistency Regularization strategy, which introduces two loss functions, Adversarial Contrastive Loss and Adversarial Distribution Loss, to optimize the representation space by incorporating both clean and adversarial malware.

We further introduce AMG18, a novel APT malware dataset featuring varied samples and class imbalances for evaluation. Extensive experiments demonstrate that RoMA outperforms seven competitor methods, including the multi-step PGD-based training method, in both robustness and training efficiency. Under adversarial settings, such as PGD attacks, RoMA achieves over 80\% accuracy—more than twice that of the next-best method—and trains more than twice as fast. Furthermore, it achieves improved accuracy in non-adversarial scenarios.

To the best of our knowledge, this work presents the first comprehensive study on developing robust APT malware attribution models. The main contributions are as follows:
\begin{itemize}
\item Proposing RoMA, a novel single-step adversarial training approach for efficiently training a robust APT malware attribution model against advanced adversarial attacks, leveraging the Global Perturbation (GP) strategy and Adversarial Consistency Regularization.
\item Demonstrating the superiority of the RoMA-trained model in terms of adversarial robustness, clean accuracy, and training efficiency, outperforming seven competitor methods.
\item Introducing AMG18, a new APT malware dataset with diverse samples and realistic class imbalances for attribution research. The RoMA-trained model and dataset samples are publicly available \footnote{\url{https://anonymous.4open.science/r/RoMA-D767}}.
\end{itemize}



%To comprehensively tackle all the aforementioned challenges, we introduce RoMA, an adversarial training approach designed to create Robust Accurate and Rapidly-trained raw-binary Attribution models for APT malware.  Our RoMA approach involves the following crucial techniques: (1) To accelerate the adversarial training process, RoMA incorporates such strategies as single-step attacks and Adversarial Initialization with global perturbation (GPs Initialization). Specifically, RoMA generates each initial adversarial sample from a clean malware sample by obfuscating the malware file and applying GPs bytes at four specific positions within the file, and then updates the adversarial sample using the FGSM \cite{ref4} in conjunction with the adversarial CrossEntropy (CE) loss. (2)	To enhance both the robustness and accuracy of the attribution model, RoMA employs not only the standard adversarial training loss but also introduces two innovative loss functions, named ACLoss (Adversarial Contrastive Loss) and ADLoss (Adversarial Distribution Loss). ACLoss focuses on enhancing the representation quality of both clean and adversarial malware samples, while ADLoss aims to facilitate the learning of a smoother and more stable decision boundary by considering the disparities in prediction distributions between the clean and adversarial samples.
%Furthermore, we created a novel dataset, named APTMA18, specifically for APT malware attribution research. Based on APTMA18, we have conducted the following extensive experiments to affirm the performance advantages of our RoMA attribution model: (1) When compared to the baseline raw-binary model, RoMA model exhibits RoMArkable 78\% and 63\%  improvement in RA against multi-step and optimization-based attacks, respectively, while also delivering superior SA. (2) We implemented and compared five additional attribution models using advanced fast or multi-step adversarial training from computer vision. RoMA excels in SA and RA against multi-step attacks, and matches the best model in RA against optimization-based attacks. Additionally, RoMA is the second fastest in training, closely rivaling the speed of the fastest model evaluated.

% To the best of our knowledge, our study marks the first effort in APT malware attribution that places a significant emphasis on adversarial robustness. It utilizes single-step adversarial training to defend multi-step or optimization-based attacks. This research yields the following key contributions:
% \begin{itemize}
%     \item Proposing RoMA, the first rapid adversarial training approach for constructing robust APT malware attribution models. RoMA leverages obfuscation and applys global perturbations at four distinctpositions within PE files to generate initial adversarial samples, and employs the FGSM method for adversarial sample updates.
%     \item Enhancing the basic adversarial training process by incorporating novel loss functions, including ACLoss (Adversarial Contrastive Loss) to improve malware representation quality, and ADLoss (Adversarial Distribution Loss) for learning a smoother and more stable decision boundary.
%     \item Implementing the RoMA adversarial training approach, and empirically validating the superiority of our trained raw-binary attribution model in terms of robustness, accuracy, and training speed through comparative experiments.
%     \item Introducing APTMA18\footnote{https://... (to be released on acceptance of the article.)}, a novel publicly available dataset for the research realm of APT malware attribution, comprising 6,360 malware samples from 18 distinct APT groups.
% \end{itemize}

The paper is organized as follows: Section \ref{sec2} reviews related work. Section \ref{sec3} defines the problem and assumptions. Section \ref{sec4} presents the proposed RoMA approach. Section \ref{sec5} describes the experiments and results. Finally, Section \ref{sec6} concludes the paper with future research directions.
%%%%Appendix A contains a notation table for reference.

\section{Related Work}\label{sec2}

\textbf{Malware Group Attribution. } Automatic attribution of APT malware groups has been widely studied using dynamic and static approaches \cite{ref2}. Dynamic methods analyze run-time behaviors, such as function calls \cite{ref9} and network operations \cite{ref8}, but are often time-consuming and ineffective against evasion techniques like virtual machine escape. In contrast, static methods directly analyzing malware code, overcoming these limitations. Traditional static approaches rely on feature extraction, including statistical features \cite{ref15} and program graph features \cite{ref17}, and use classifiers like Random Forest \cite{ref15}, LSTM \cite{ref17} and PerceiverIO \cite{ref61}, though they require substantial manual effort for feature engineering. Other approaches automate feature extraction by processing disassembly opcode n-grams, requiring disassembly and n-gram segmentation, and use models like LSTM for classification \cite{ref18}. Raw-byte-based malware attribution simplifies the process further by bypassing feature extraction and directly capturing subtle details through end-to-end CNN models \cite{ref24,ref58}. The SOTA byte-level malware classifier, MalConv with GCG (Global Channel Gating), uses a 1D-CNN to process raw byte sequences, achieving high accuracy with innovative network designs \cite{ref24}. Despite their effectiveness, these methods remain vulnerable to adversarial attacks, which this study seeks to address.

%Deep learning-based methods avoid manual feature extraction by directly analyzing raw byte sequences or disassembly opcode n-grams \cite{ref18} in an end-to-end manner. Models such as CNN \cite{ref58} and LSTM \cite{ref18} improve classification accuracy by learning representations from raw data. 

% Identifying the groups behind Advanced Persistent Threat (APT) malware is key to comprehending the motivations and tactics of these actors \cite{ref1}. Traditional manual attribution was reliant on the in-depth expertise of cybersecurity professionals, making it a technically demanding and time-consuming endeavor \cite{ref2}. In recent years, automatic group attribution techniques that employ machine learning-based malware analysis have garnered significant research interest. These techniques are generally divided into dynamic and static analyses. Dynamic techniques focus on tracking malware runtime behaviors like process activities and network operations \cite{ref7,ref8} and function invocation sequences \cite{ref9,ref10,ref11,ref12}. Yet, they require a virtual environment and are inefficient against certain malware evasions. Static methods bypass these issues by analyzing malware code without execution. In this research, we focus on static methods. 

% Early static attribution works harness static malware file features, such as PE-headers and import tables, and leverage conventional Random Forest (RF) models for classification \cite{ref13,ref14}. Subsequent works delve into various static features and more machine learning models. Laurenza et al. extract such statistical features as section table count and resource count, and create an RF classifier \cite{ref15}. Wei et al. extract API system calls as malware features, and adopt LSTM with attention mechanisms for feature vectorization \cite{ref16}. Taking a distinct approach, Liu et al. focus on attributing particular malware functions to APT groups. They extract sequences from the disassembled code, vectorize the control flow graph, and utilize LSTM to generate function embedding \cite{ref17}. Recently, Song et al. have extracted the disassembly codes from APT malware and employed an LSTM encoder to analyze n-gram opcode sequences for group attribution \cite{ref18}.

% While the existing machine learning-driven malware attributions are promising, they come with inherent limitations. Firstly, their dependence on feature engineering necessitates constant manual adjustments, risking the omission of subtle yet vital features and introducing biases. Secondly, these techniques often overlook evasion tactics, notably the deployment of `false flags' by malicious actors, which compromises attribution robustness. Thirdly, there's a potential for enhanced accuracy in these models, especially considering the attribution challenges posed by shared tools among APT groups. In contrast, our research introduces a specialized end-to-end attribution model for raw-byte APT malware. Designed for adversarial robustness and accuracy, our approach bypasses the complexities of feature engineering.
% \subsection{Raw-binary Malware Detection}\label{sec:end2end}
% Malware detection serves as a binary classification task, distinguishing between benign and malicious software. Employing raw-binary detectors allows for direct raw data processing, bypassing traditional manual feature engineering. This approach is simpler, adaptable, and can discern intricate patterns often overlooked in traditional methods. In recent years, raw-binary malware detection methods have grown popular \cite{ref19,ref20}. It deals with the code data as it is, catching little details that other methods might miss (such as code images, or opcode sequences). 

% Early attempts at classifying malware through raw bytes focus on the Normalized Compression Distance (NCD) method \cite{ref21}, but it had its drawbacks, especially in supervised settings \cite{ref22,ref23}. A significant shift in this domain is the introduction of the MalConv model, a pioneering method that utilizes the 1D-CNN to directly process raw byte sequences, showcasing unique neural network designs for better results \cite{ref24}. Notably, studies by firms like Avast revealed that MalConv could match, and sometimes even surpass, the effectiveness of hand-crafted detection features \cite{ref25}. However, due to memory constraints, MalConv is restricted to a maximum file size of 2MB, which could compromise its performance and expose it to threats in adversarial settings. Raff et al. propose the fixed-memory convolution, which harnesses the sparsity of temporal max pooling to decouple memory usage from time series length, optimizing memory efficiency and training speed \cite{ref26}. They introduce a global channel gating (GCG) design equipped with an attention mechanism, enabling the capture of feature interactions over prolonged periods, surpassing 100 million time steps \cite{ref26}. Malconv with GCG not only addresses memory constraints faced by earlier techniques but also boosts the malware detection model's accuracy by up to 2\%. 

% In this study, we address malware attribution as a multi-classification problem. Drawing inspiration from the malware detection model, MalConv with GCG, we aim to construct an raw-binary multi-class attribution model.

\vspace{0.1cm}
\noindent\textbf{Adversarially Robust Malware Classification. } 
The adversarial nature between malware attacks and defenses makes malware classification models particularly vulnerable to adversarial attacks. For instance, byte-level classifiers like MalConv exhibit significant performance degradation under adversarial perturbations \cite{ref28}. To enhance robustness, recent studies have employed adversarial training techniques, generating adversarial examples using gradient-based methods that optimize functionality-preserving transformations, such as byte-level perturbations \cite{ref27} or assembly-level instruction modifications \cite{ref62}. FGSM (Fast Gradient Sign Method) offers a fast, single-step adversarial training approach. In the malware domain, Slack-FGSM and Append-FGSM apply adversarial bytes to slack space or the end of PE files \cite{ref31,ref29}, while extensions apply adversarial bytes to additional regions, such as the full DOS header or shifted sections \cite{ref27,ref28}. However, single-step FGSM methods are vulnerable to multi-step attacks. To mitigate this, multi-step adversarial training methods like PGD (Projected Gradient Descent) \cite{ref55} are employed in malware classification, although they incur considerable training overhead \cite{ref56}. Recently, Lucas et al. applied low-effort PGD attacks during adversarial training to improve the robustness of raw-byte malware detectors against high-effort PGD attacks, leveraging existing perturbation techniques without introducing new adversarial training approaches \cite{ref56}. Advanced FGSM methods from the image domain, such as NuAT (using a Nuclear-Norm regularizer) and FGSM-RS (with perturbation initialization), remain underexplored in malware classification. Our experiments adapting these image-based methods to malware classification reveal that both robustness and training efficiency still require substantial improvement.

%% [Demetrio et al., 2021]. 



%PGD:
% A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In Proc. ICLR, 2018.

% Recently, Keane Lucas et al. apply low-effort PGD attacks during adversarial training to improve the robustness of raw-byte malware detector against high-effort PGD attacks \cite{ref56}. They utilize existing perturbation techniques (including byte-level appending and assembly-level instruction modifications) and gradient methods without proposing new adversarial training approaches. By contrast, our work proposes a novel adversarial training approach, aiming to utilize fast adversarial training to defend advanced attacks at byte-level, including multi-step PGD attacks and optimization-based C\&W attacks. 


%generating adversarial examples based on gradient or evolutionary algorithms
%The adversarial nature of malware attacks and defenses makes malware classification models particularly vulnerable to adversarial attacks. For instance, byte-level classifiers like MalConv experience significant performance drops under adversarial perturbations [Demetrio et al., 2021]. Recent studies have introduced adversarial training techniques to enhance robustness, generating adversarial examples via methods like FGSM, which iteratively optimize functionality-preserving transformations, such as byte-level perturbations or assembly-level instruction changes \cite{ref35}. Slack-FGSM and Append-FGSM are representative FGSM-based methods that apply adversarial bytes to slack space or the end of PE malware files \cite{ref31}. Extensions include adversarial bytes applied to broader regions, such as the full DOS header or shifted sections \cite{ref27, ref29}. However, single-step FGSM-based methods are prone to multi-step attacks. Multi-step training methods like PGD-AT \cite{ref52}, developed for image domains, mitigate this issue but at a high computational cost. Advanced FGSM methods from the image domain, such as NuAT (using a Nuclear-Norm regularizer) and FGSM-RS (with perturbation initialization), remain underexplored in malware classification. Our experiments adapting these image-based methods to malware classification reveal that both robustness and training efficiency still require significant improvement.


% \subsection{Evasion Attack on Malware Detection}
% To evade Malware Detection, adversaries launch adversarial attacks, also known as evasion attacks, against machine-learning-based Malware Detectors. Depending on the adversary’s knowledge of the target malware detectors, current Evasion Attack efforts against malware detection are categorized into white-box attacks and black-box attacks \cite{ref27,ref28}. White-box attacks occur when the adversary has comprehensive knowledge of the target models, including architectures, parameters, features, and the training dataset. In contrast, black-box attacks involve scenarios where the attacker, apart from accessing the model’s outputs, lacks knowledge about other aspects of the model. The white-box attacks might target byte-based malware detectors, API-based detectors, visualization-based detectors, and so on \cite{ref27}. This study focuses on white-box attacks against byte-based PE malware detectors like MalConv.
% The majority of current white-box attacks on MalConv employ byte-level manipulations that maintain the original functionality, and iteratively update the values of bytes based on the gradient descent \cite{ref28}. For example, Kolosnjaji et al. \cite{ref29} present a gradient-driven approach to create adversarial bytes and append these bytes to the padding space at the end of a PE malware file. They use gradient descent to optimize the attack in the embedding space, and invert the embedding lookup to reconstruct the adversarial bytes. Using the same optimization and reconstruction methods as Kolosnjaji et al., Demetrio et al. \cite{ref30} propose to modify the first 58 bytes of the DOS header, named Partial DOS manipulation.  Kreuk et al. \cite{ref31} append adversarial bytes to the padding space or the slack space (i.e., inter-section gaps) in the malware file, using FGSM for crafting an embedding vector and then aligning it to the nearest input space bytes to generate adversarial malware. In a related study, Suciu et al. \cite{ref32} analyze append-FGSM and slack-FGSM, finding that slack-FGSM achieves better results with less byte alteration. Chen et al. \cite{ref33} choose highly significant data blocks from benign PE files, identified through saliency vectors created by the Grad-CAM method \cite{ref34}, and add these blocks to the end of a malware file. Demetrio et al. \cite{ref27} introduce three innovative manipulations for PE files: modifying the entire DOS header with Full DOS manipulation; generating extra space by expanding the DOS header's offset in Extend manipulation; and creating room for adversarial noise by shifting the initial section's contents in Shift manipulation. Other white-box attacks employ functionality-preserving manipulations at the assembly instruction level \cite{ref27}. For instance, Sharif et al. \cite{ref35} use the Carlini \& Wagner loss \cite{ref36} to generate adversarial samples, implementing these equivalent instruction transformations across all functions of a malware file and preserving those that yield a feature vector aligned with the gradient. 
% Drawing from earlier research on functionality-preserving byte-level manipulations over PE executables \cite{ref28}, \cite{ref29}, \cite{ref31}, our white-box attack technique involves the insertion of adversarial bytes into various regions within a malware file.


\section{Problem Definition}\label{sec3}
\quad \textbf{Byte-level APT Malware Attribution Task:} The task is to classify APT malware into their respective APT groups by analyzing raw byte sequences. Let the input space be \( X \subseteq \{ 0,1,...,255 \}^* \) with an underlying distribution \( D \), and the label space be \( Y = \{ y_1, \dots, y_G \} \) with \( G \) APT groups. For each pair \( (x, y) \in D \), \( x \) represents a variable-length byte string, and \( y \) indicates its corresponding APT group. The goal is to find the optimal parameters \( \theta \) of the attribution model \( F(\theta): X \rightarrow Y \) that minimizes the loss function \( L(\theta, x, y) \):
%, such as the cross-entropy (CE) loss:
\[
\underset{\theta}{\min} \, \mathbb{E}_{(x, y) \sim D} \left[ L(\theta, x, y) \right]
\]


\textbf{Attack Assumption:} This study aims to develop a robust malware attribution model resilient against adversarial attacks. Based on established threat modeling guidelines \cite{ref4}, we assume the following attacker characteristics: (1) The attacker seeks to misclassify APT malware into an incorrect APT group. (2) The attacker has complete knowledge of the classifier, including its architecture and parameters, constituting a white-box attack. (3) The attacker can modify raw bytes of the malware through functionality-preserving transformations but cannot alter the trained model. (4) The attacker can use a multi-step iterative approach, rather than a single-step attack, to enhance the attack’s intensity.


% 可以把这个放到附录中去
% \subsection{MalConv with GCG}
% The architecture of MalConv-GCG, as shown in Fig. \ref{fig:fig1}, processes malware's raw bytes to produce two embedding vectors, which are then channeled into two distinct sub-networks. The top sub-network is designed to learn a global context vector \(g\in \mathbb{R}^{C}\), and the bottom one is used to extract preliminary feature vector \(x_{t}\in \mathbb{R}^{C}\), where \(X=\left\{ x_{1},x_{2},...,x_{T}\right\}\) is a long sequence with \(C\) channels. The state vector \(g\) from the upper sub-network and the temporal sequence \(X\) from the lower part converge at the Global Channel Gating (GCG) module. GCG, influenced by attention principles, selectively suppresses specific time steps based on the entirety of all channel contents. Utilizing the context vector \(g\), it adjusts the components of vector \(x_{t}\) over time. As shown in Equation \ref{equ1}, the sigmoid function adjusts the values of \(x_{t}\) to fall within the \([0,1]\) range, achieving a context-sensitive reduction in its element magnitudes.
% \begin{equation}
%     GCG_{W}\left (x_{t},g\right )= x_{t}\cdot \sigma \left (x_{t}^{T}tanh\left(W^{T}g\right )\right).
%     \label{equ1}
% \end{equation}
% The result from the gating is subsequently directed into a temporal max pooling module, yielding a consistent-length feature vector. Because the max pooling over time leads to a sparse gradient concerning the time series sequence, this module, designed by Raff et al. \cite{ref26}, harnesses this sparsity. It guarantees stable memory usage irrespective of sequence length, accelerates training speeds, and removes the inherent input length restrictions of MalConv. After pooling, the refined feature vector is passed through a straightforward fully connected layer, determining the benign/malicious classification.

% {\includegraphics[width=3.2in]{pic/malconv.pdf}}
% \label{fig6_first_case}
% 已搬到附录
% \begin{figure}
%   \centering
%   \includegraphics[width=0.5\textwidth]{pic/fig1.pdf}
%   \caption{MalConv-GCG architecture}
%   \label{fig:fig1}
% \end{figure}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\textwidth]{frame.pdf}
\centering
\caption{Overview of the RoMA Training approach, with creamy-yellow components denoting the trained malware attribution model.} 
%Colored arrows indicate different data flows, with numbers denoting the flow order.}
\label{fig3}
\end{figure*}

\section{Proposed Approach}\label{sec4}
% We propose the single-step adversarial training approach, named RoMA, as shown in Fig. \ref{fig3}, to rapidly train a robust and accurate APT malware attribution model resilient to multi-step attacks. In this section, we will firstly introduce the infrastructure and training objective of RoMA, and then two versions of RoMA, based on local perturbations(RoMA-LP) and based on global perturbations(RoMA), are presented.
\subsection{RoMA Overview}
We propose RoMA, a novel single-step adversarial training approach designed to efficiently train a robust APT malware attribution model that is resilient to multi-step attacks. \textbf{\textsl{RoMA introduces GP (Global Perturbation) to FGSM for generating enhanced adversarial malware samples and leverages adversarial consistency regularization during adversarial training to boost model robustness.}} Here, GPs are learned perturbation patterns shared by adversarial malware in the embedding space, while the regularization is applied across the prediction and projection spaces. Figure \ref{fig3} provides an overview of RoMA, illustrating its key modules and the training workflow. The creamy-yellow components represent the trained attribution model, while the grayish-lavender modules are auxiliary components for training. Data flows are indicated by different colors, with numbered arrows showing the order. 

The trained-model, built on MalConv-GCG \cite{ref3}, consists of: (1) a Word Embedding Layer for converting malware raw bytes into embeddings (\(x \rightarrow e\)); (2) a Representation Layer for extracting hidden vectors (\(e \rightarrow h\)); and (3) a Classification Head for predicting probabilities (\(h \rightarrow p\)). 
The auxiliary modules include: (1) a Projection Head for generating projection vectors (\(z\)); (2) an Adversarial Initialization module that creates initial adversarial samples ($x^{adv}$ in blue flow) using random perturbations; and (3) a Global Perturbation module that generates enhanced adversarial samples ($x^{adv}$ in green flow) by applying learned global perturbations.


The adversarial training of RoMA is formulated as the following min-max optimization problem, aiming to optimize the model parameters \(\theta\) and projection module \(\theta_P\) by minimizing the adversarial loss. Specifically, the inner maximization calculates the optimal perturbation \(\delta\) (using FGSM and GP) to maximize the adversarial loss on the perturbed sample \(x + \delta\), subject to the constraint \(\|\delta\| \leq \varepsilon\), where \(\varepsilon\) is the maximum perturbation magnitude. The outer minimization updates both \(\theta\) and \(\theta_P\) to minimize the total loss (incorporating adversarial consistency regularization to enhance robustness).
\begin{equation}
    \underset{\theta, \theta_P}{\min} \mathbb{E}_{(x, y) \sim D} \left[ \underset{\|\delta\| \leq \varepsilon}{\max} \ L_{CE} (\theta, x + \delta, y) \right]
    \label{equ_min_max}
\end{equation}

% \begin{equation}
%     \underset{\theta, \theta_P}{\min} \mathbb{E}_{(x, y) \sim D} \left[ \underset{\|\delta\| \leq \varepsilon}{\max} L_{CE} (\theta, x + \delta, y) + L_{Total}(\theta, \theta_P, x, y) \right]
%     \label{equ_min_max}
% \end{equation}











\begin{algorithm} % [H]
\caption{Generating Adversarial Malware Samples: 
\(GenAdvMal\)\((x, y, \theta_{S}, W, \varepsilon, \eta, \mu)\).}\label{alg:alg1}
\textbf{Input}: clean malware sample and its group label \((x, y)\), select head’s parameter \(\theta_S\), word embedding matrix \(W\), maximum perturbation strength \(\varepsilon\), learning rate \(\eta\), momentum decay Coefficient \(\mu\).

% perturbation parameter
\textbf{Output}: perturbed malware sample \(x^{adv}\).

\textbf{Global}: \(Pos\), perturbation-positions within malware \(x^{adv}\); $GP$, a set of global perturbation vectors, initialized as random embeddings; \(m\), A set of momenta for the gradients, initialized as zeros. % during e2 updates.
\begin{algorithmic}[1]
\STATE \(x^{adv}\leftarrow Pack(x)\)
\STATE \(Pos\leftarrow\) Get all perturbation-positions in \(x^{adv}\)
% \IF {\(x.pByteList == NULL\)} 
% \STATE // the current training epoch is the first one
% \FOR {\(p\) in \(Pos\)} 
% \STATE \(x^{adv}[p]\leftarrow\) Byte of randomInt\(\sim Uniform(0, 255)\)
% \ENDFOR
% \ELSE
% \STATE//Perturbation Byte applied in the previous epoch, at \(p\) within $x^{adv}$\FOR {\(p\) in \(Pos\)}
% \STATE \(x^{adv}[p]\leftarrow x.pByteList[p]\)
% \ENDFOR
% \ENDIF

% \noindent
% \STATE
% \textbf{for} $loc \in Pos$ \textbf{do} \\
% \hspace*{2em}$x^{adv}[p] \gets RandomByte()$
\FOR {\(p\) in \(Pos\)}
\STATE \(x^{adv}[p]\leftarrow\) \textit{RandomByte()} %Byte of randomInt\(\sim Uniform(0, 255)\) 
\ENDFOR
% \STATE \(e_1 \leftarrow model.Embedding(x^{adv})\)
% \STATE \(h_1\leftarrow model.Representation(e_1\))
\STATE \((e_1, h_1) \leftarrow model.Embedding.Representation(x^{adv})\)

\STATE \(z^\prime \leftarrow Selection(h_1)\)
\\ \hfill \textbf{\textit{\{FC head outputs $K$-dimensional logits $z^\prime$}}\textbf{\textit{\}}}
\STATE \(\theta_{S}\leftarrow \theta_{S}-\eta\nabla_{\theta_{S}}CLLoss(z^\prime, y)\) 
\vspace{0.05cm} %
\\ \hfill \textbf{\textit{\{Update Selection Head with CL loss\}}}
\STATE \(i \leftarrow \arg\max_{j \in \{1, \dots, K\}} z^\prime_j\) ~~~~~~~~~~~~~~~~~~~~~~~~~\textbf{\textit{\{Select $GP_i$\}}}
\vspace{0.05cm} %
\FOR {\(p\) in \(Pos\)}
\STATE \(e_1[p] \leftarrow e_1[p] + GP[i][p]\) 
~~~~~~~~~~~~~\textbf{\textit{\{Apply $GP_i$\}}}
\STATE \(x^{adv}[p]\leftarrow \operatorname*{argmin}_{j\in 0\dots255}\left ( \left\| e_{1}\left [ p \right ]-W_{j}\right\|_{2} \right )\)
\ENDFOR
\STATE \(e_2 \leftarrow model.Embedding(x^{adv})\) 
\STATE \(p_2\leftarrow model.Representation.Classification(e_2)\)
\STATE \(gradient\leftarrow \nabla_{e_2}CELoss(p_2, y) \)
\vspace{0.05cm} %
\FOR{\(p\) in \(Pos\)}
\STATE \(e_2[p]\leftarrow e_2[p] + \varepsilon * gradient[p]\)
\STATE \(x^{adv}[p]\leftarrow \operatorname*{argmin}_{j\in 0\dots255}\left ( \left\| e_2\left [ p \right ]-W_{j}\right\|_{2} \right )\)
%\\ \hfill \textbf{\textit{\{Update injection}}\textbf{\textit{\}}}
\\ \hfill \textbf{\textit{\{Update malware content at perturbation-position}}\textbf{\textit{\}}}
\vspace{0.05cm} %
\STATE \(m[i][p] \leftarrow \mu * m[i][p] + \operatorname*{sign}(gradient[p])\)
\STATE \(GP[i][p] \leftarrow GP[i][p] + \varepsilon * \operatorname*{sign}(m[i][p])\)
\\ \hfill \textbf{\textit{\{Update $GP_i$\ in pool with momenta}}\textbf{\textit{\}}}

\ENDFOR
% \STATE \(GP[i] \leftarrow GP[i] + \varepsilon * sign(m[i])\) 
% \\ \hfill \textbf{\textit{\{Update $GP_i$\ with momenta}}\textbf{\textit{\}}}

\RETURN \(x^{adv}\)
\end{algorithmic}
\end{algorithm}


% \STATE \((z^\prime, p^\prime)\leftarrow Selection.fullConnection.softmax(h_1)\)
% \\ \hfill \textbf{\textit{\{FC haed: representation $z^\prime$, probability $p^\prime$ \}}}
% \STATE \((p_2,y^{adv})\leftarrow model.Representation.Classification(e_2)\)
% \IF{\(y^{adv}\neq y\)}
% \RETURN \(x^{adv}.\)
% \ENDIF \\
%\STATE \(k\leftarrow Selection(h_1)\)
%\* (advCEloss\leftarrow CrossEntropy(p_2, y)\)
% \STATE \(gradient\leftarrow \nabla_{e_2}advCEloss \)
\subsection{Adversarial Malware Generation with GP and FGSM}\label{subsec:adver}
The adversarial malware generation algorithm, \(GenAdvMal()\), is outlined in Algorithm \ref{alg:alg1}. RoMA first applies the UPX packing tool\footnote{https://upx.github.io/} to the clean malware \(x\) (line 1), producing a functionality-preserving compact version with an altered structure. This packed malware serves as the initial sample for adversarial perturbation. RoMA then identifies four types of \textit{Perturbation-Positions} and initializes them with random bytes (lines 2-5). \textbf{These positions} include parts of the DOS header excluding `MZ' and the PE pointer, a 1KB shifting space before the first section, unused slack space between sections, and up to 100KB of padding space at the file’s end, as \textbf{illustrated in Figure A1 (see Appendix)}. Next, RoMA refines the perturbation-position bytes using its key optimization technique (lines 6-22). 

RoMA's optimization approach is based on the assumption that \textbf{\textsl{malware adversarial samples with similar perturbations applied in the sample space may share a common perturbation pattern in the embedding space}}. Thus, as shown by the blue flow in Figure \ref{fig3}, RoMA first trains a Selection Head to adaptively choose a global perturbation (GP) for each malware in the embedding space, superimposes this GP onto the malware embedding and then adjusts the perturbation-position bytes in the sample space, generating an intermediate adversarial sample that aligns with its shared perturbation pattern (i.e., the GP). Subsequently, as indicated by the green flow in Figure \ref{fig3}, RoMA updates both the malware bytes at perturbation-positions and the GP based on FGSM, generating the final adversarial sample while refining the GP. The following subsections will detail these two steps.
% RoMA's optimization approach is based on the assumption that \textit{malware adversarial samples with similar perturbations applied in the sample space may share a common perturbation pattern in the embedding space}. As shown in Figure \ref{fig3} and Algorithm \ref{alg:alg1}, RoMA first trains a Selection Head to adaptively choose a global perturbation (GP) for each malware embedding in the embedding space, superimposing this GP onto the embedding. Subsequently, RoMA adjusts the perturbation-position bytes in the sample space accordingly, generating adversarial samples that align with the shared perturbation pattern (i.e., the corresponding GP). The following subsections will detail these two steps.

\subsubsection{Selecting GP and Applying it at Perturbation-Positions}
RoMA utilizes a global perturbation (GP) pool consisting of $K$ GP vectors, each representing a shared perturbation pattern among malware embeddings. Each GP vector is initialized as a random embedding derived from random bytes at perturbation positions via the model’s word embedding layer, and is updated based on gradients (see next subsection). For each malware sample $x^{adv}$, after obtaining its embedding vector $e_1$ and representation vector $h_1$ through the model (line 6), RoMA trains a Selection Head to choose a GP from the pool. The Selection Head employs a single-layer fully-connected network, taking $h_1$ as input and producing a $K$-dimensional logits vector $z^\prime$ (line 7). The Selection Head is updated using the following contrastive learning objective (line 8):
\begin{equation}
    L^{\prime}_{CL}=\sum_{i=1}^{N}\frac{-1}{\left | P(i) \right | }\sum_{s\in P(i)}  log\frac{exp(z^{\prime}_i\cdot z^{\prime}_s/\tau)}{ {\textstyle \sum_{j\in N(i)} exp(z^{\prime}_{i}\cdot z^{\prime}_{j}/\tau )} } 
    \label{supcon}
\end{equation}
In each batch, for each anchor logits vector \(z^{\prime}_{i}\), $P(i)$ and $N(i)$ denote its sets of positive and negative vectors, respectively. Each positive vector \(z^{\prime}_{s}\) corresponds to a malware sample that shares the same APT-group label as the anchor malware (i.e., the malware corresponding to the anchor logits), while each negative vector \(z^{\prime}_{j}\)  corresponds to a malware sample with a different label. Thus, the CL loss optimizes the logits space by bringing the logits of in-group malware closer together and pushing those of out-group malware farther apart. 

The Selection Head selects the GP with the maximum value in the \textit{K}-dimensional logits vector (line 9). Intuitively, the selected GP represents the perturbation pattern with the highest logits score, indicating the most likely perturbation for the given malware. RoMA superimposes the selected GP onto the malware embedding $e_1[p]$, then adjusts the malware bytes at the \textit{perturbation-positions} in the sample space (by utilizing the closest word embedding vector in $W$), generating an intermediate adversarial sample $x^{adv}$ (lines 10-13).



\subsubsection{Updating Perturbation Bytes and GP via FGSM}
% To maximize the likelihood of the adversarial malware being misclassified into an incorrect APT group, RoMA optimizes the perturbation bytes in the intermediate $x^{adv}$ using FGSM, a gradient-descent-based technique with a single-step attack strategy. Sepcifically, RoMA在model中前向传播$x^{adv}$以获得嵌入向量e2和交叉熵loss，然后反向传播以计算此loss对嵌入e2的梯度, 进而基于梯度和maximum perturbation strength计算嵌入扰动 $\varepsilon * gradient$ ，并将此扰动superimpose到嵌入向量e2中(lines14-16)，最后在样本空间调整扰动字节以生成最终的adversarial malware $x^{adv}$。

To maximize the likelihood of the adversarial malware being misclassified into an incorrect APT group, RoMA optimizes the perturbation bytes in the intermediate sample using FGSM, a gradient-based single-step attack strategy. Specifically, RoMA performs a forward pass of $x^{adv}$ through the model to obtain the embedding vector $e_2$ and the cross-entropy loss, then computes the gradient of this loss with respect to $e_2$ via backpropagation (lines 14-16). RoMA next calculates the embedding perturbation $\varepsilon * $\textit{gradient}$ $ based on the maximum perturbation strength and the gradient, and superimposes it onto the embedding vector $e_2$ (line 18). Finally, RoMA adjusts the perturbation bytes in the sample space to generate the final adversarial malware $x^{adv}$ (line 19).

To learn the GPs iteratively, RoMA synchronously updates the selected GP alongside applying gradient updates to each intermediate malware sample $x^{adv}$. Specifically, RoMA determines the perturbation direction for the GP using the sign of the gradient, and incorporates momentum to accumulate historical gradient information. (lines 20-21). 



% In Algorithm \ref{alg:alg1}, the input variable \(R\) refers to the upper limit of attack iterations. The RoMA approach employs Algorithm \ref{alg:alg1} for adversarial training of our attribution model, setting \(R\) to 1, which implies a single-step attack strategy, namely FGSM. Additionally, Algorithm \ref{alg:alg1} can be adapted to generate test malware for model evaluation, where \(R\) can be assigned any iteration count, such as 50, representing a multi-step attack approach like the Projected Gradient Descent (PGD)-50.

% 已搬到附录
% \begin{figure}
% \centering
% \includegraphics[width=0.5\textwidth]{pic/fig2.pdf}
% \centering
% \caption{PE file regions for perturbation injections}
% \label{fig2}
% \end{figure}
\subsection{Adversarial Training with Three Loss Functions}\label{subsec:loss}
RoMA employs fast adversarial training using single-step methods (e.g., FGSM) to defend against multi-step attacks. To enhance the effectiveness of single-step training, we propose two adversarial consistency regularization losses, $L_{AC}$ and $L_{AD}$, alongside the basic adversarial training loss, ($L_{AT}$), to optimize the representation quality and decision boundary. The Total loss is defined as below, where weight factors $\lambda _{1}$, $\lambda _{2}$ \(\in \left [0,1\right ]\):
\begin{equation}
    % \begin{split}
        L_{Total}=L_{AT}+\lambda _{1}*L_{AC}+\lambda _{2}*L_{AD}.
    % \end{split}
    \label{equ6}
\end{equation} 


As illustrated in Figure \ref{fig3}, the forward-pass data flows for each clean malware sample $x$ and its adversarial counterpart $x^{adv}$ are represented by black and red arrows, respectively. The model and the projection head produce the corresponding prediction vectors ($p$, $p^{adv}$) and projection vectors ($z$, $z^{adv}$), respectively. RoMA employs batch updates for both the model and the projection head, leveraging the cumulative effect of the three loss functions, which are detailed below:

\subsubsection{\textbf{Basic Adversarial Training loss (ATLoss)}}
\begin{equation}
    \begin{split}
        L_{AT} &= L_{AdvCE}+L_{CleanCE} \\
        &=-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{G}\left(y_{ij}log\  p_{ij}^{adv}+y_{ij}log\ p_{ij}\right ).
    \end{split}
    \label{equ3}
\end{equation}
Here, $N$ is the batch size of clean malware, and $G$ denotes the number of APT groups. This loss combines the cross-entropy (CE) loss for both adversarial and clean malware samples, balancing the model's robustness and accuracy.

\subsubsection{\textbf{Adversarial Contrastive Loss (ACLoss)}} 
We propose an Adversarial Contrastive Loss to enhance malware representation by promoting intra-group perturbation invariance and inter-group distinguishability: \\
\begin{equation}
    \begin{gathered}
       % L_{CL}&=\frac{1}{2N}\sum_{i=1}^{2N}L_{SINCERE}\left(z_{i}\right)\\
       % L_{SINCERE}(z_{i}) &=\frac{-1}{\left|P(i)\right|}\sum_{p\in P(i)}log\frac{e^{(z_{i}\cdot z_{p}/\tau)}}{e^{(z_{i}\cdot z_{p}/\tau)}+\sum_{j\in \textsl{N}_{i}}e^{(z_{i}\cdot z_{j}/\tau)}
       L_{AC} = \frac{1}{2N}\sum_{i=1}^{2N}\frac{-1}{\left|CI(i)\right|+\left|AI(i)\right|}L_{s} , \qquad where  \\
         L_{s} = \sum_{s \in CI(i)\cup AI(i)}log\frac{e^{(z_{i}\cdot z_{s}/\tau)}}{e^{(z_{i}\cdot z_{s}/\tau)}+\sum_{j\in \textsl{N}_{i}}e^{(z_{i}\cdot z_{j}/\tau)}} \\
    \end{gathered}
\label{equ4}
\end{equation}
This loss incorporates both clean malware and their adversarial counterparts in the projection vector space. Each batch contains $2N$ samples: $N$ clean malware and $N$ adversarial variants. For each sample $x_i$, $z_i$ is its projection vector. $CI(i)$ is the set of Clean In-group instances with the same APT label as $x_i$, and $N_i$ is the set of Out-group instances, comprising all samples (both clean and adversarial) in the batch with a different label.
ACLoss treats the projection vectors of in-group samples (both clean and adversarial) as positive pairs, minimizing their distance. Conversely, out-group vectors are treated as negative pairs, maximizing their distance.

\subsubsection{\textbf{Adversarial Distribution Loss (ADLoss)}}
We propose an Adversarial Distribution Loss to minimize the divergence between the probability distributions of clean malware and its adversarial counterpart:
\begin{equation}
    \begin{split}
        L_{AD}=\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{G}\left (p_{ij}log\frac{p_{ij}}{p_{ij}^{adv}}\right ).
    \end{split}
    \label{equ5}
\end{equation}
This loss helps the attribution model establish a smoother and more consistent decision boundary, therefore enhancing its generalizability to both unseen and perturbed malware. 

%%%%%%%%%%%%%%%%%%%%% 算法2 开始
% \begin{algorithm} % [H]
% \caption{Adversarial Training.}\label{alg:alg2}
% \textbf{Input}: \(D\), training set; \(Q\), maximum number of training iterations; \(N\), number of a batch of training samples; \(\eta\), learning rate; \(\mu\), the decay factor; \(\theta_{P}\), projection head module’s parameter; \(\theta_{S}\), selection head module’s parameter.
% \textbf{Output}: \(\theta\), attribution model’s parameter.
% \begin{algorithmic}[1]
% \STATE Initialize \(\theta, \theta_{P}, \theta_{S}\)
% \STATE \(epoch=0\)
% \WHILE{\(epoch < Q\)}
% \FOR {batch \(\left\{\left ( x_{i}, y_{i} \right ) \right\}_{i=1}^{N}\subset D\) } 
% \FOR {\(i=1\) to \(N\)}
% \STATE \(x_{i}^{adv}\leftarrow GenAdvMal(x_{i}, y_{i}, \theta_{S}, W, \varepsilon, \eta, \mu)\)
% \STATE \(h_{i} \leftarrow model.Embedding.Representation(x_{i})\)
% \STATE \(h_{i}^{adv}\leftarrow model.Embedding.Representation(x_{i}^{adv})\)
% \STATE \(z_{i} \leftarrow Projection(h_{i})\)
% \STATE \(z_{i}^{adv} \leftarrow Projection(h_{i}^{adv})\)
% \STATE \(p_{i}\leftarrow model.Classification(h_{i})\)
% \STATE \(p_{i}^{adv}\leftarrow model.Classification(h_{i}^{adv})\)
% \ENDFOR
% \STATE Compute \(TotalLoss\) according to Equ. (\ref{equ3})-(\ref{equ6})
% \STATE \(\theta \leftarrow \theta - \eta \nabla_{\theta}TotalLoss\)
% \STATE \(\theta_{P} \leftarrow \theta_{P} - \eta \nabla_{\theta_{P}}TotalLoss\)
% \ENDFOR
% \STATE \(epoch=epoch+1\)
% \ENDWHILE
% \RETURN \(\theta\)
% \end{algorithmic}
% \end{algorithm}
%%%%%%%%%%%%%%%%%%%%% 算法2 结束

% \subsection{RoMA-LP: Faster Training with Trade-offs}
% After determining all the above four types of perturbation positions within the PE file, RoMA-LP applies local perturbation bytes at these positions.

% At the beginning of the first training epoch, RoMA-LP initializes each perturbation byte with a random value from the Uniform distribution, while at the beginning of each remaining epoch, RARA initializes each perturbation byte with the one created in the previous epoch. To transfer the history perturbation information between epochs, RARA buffers the current perturbation bytes of each malware x at the end of the former epoch, and then utilizes the buffered bytes to initialize the malware’s perturbation bytes in the latter epoch.

% Since inserting local perturbations requires caching perturbations for each adversarial sample from the previous epoch, large memory is required. Therefore we insert global perturbation to solve this problem. We will use the Perturbation updating module to adaptively select the global perturbation and apply the global perturbation into the adversarial malware (shown by the blue arrow in Fig. \ref{fig3}).  Perturbation updating module consists of the Selection head and the global perturbation(GP) pool. The GP pool \(GP\) is a set containing \(K\) global perturbations, initialized with random values. The Selection head is a fully connected layer, that is responsible for adaptively selecting the global perturbation. 


\section{Experiments}\label{sec5}
\subsection{Dataset and Evaluation Metric}

\quad \textbf{Dataset:} We introduce AMG18, a novel APT malware dataset comprising 6360 instances across 18 APT groups. We extend the APTMalware list (3594 samples from 12 groups \cite{ref57}) by incorporating additional hash-label pairs from public threat repositories. The group labels were verified by security analysts at a leading security firm in China, and the malware samples were obtained from the VirusSign database \cite{ref37}. \textbf{Table A1 in the Appendix outlines the details of AMG18}. Compared to APTMalware, AMG18 provides greater diversity with additional groups, realistic class imbalance, and over 100 samples per group, supporting robust splits and reducing overfitting risks. The dataset was split into an 80:20 train-test ratio for experiments.

\textbf{Evaluation Metrics:} We evaluate the effectiveness and robustness of each malware attribution model under both non-adversarial and adversarial settings. To reflect performance across all APT groups, we compute weighted metrics. Specifically, we use three metrics: (1) Standard Accuracy (SA), representing the accuracy on clean samples across all groups; (2) Robust Accuracy (RA), measuring the model's ability to correctly attribute adversarial samples; and (3) Attack Success Rate (ASR), indicating the rate at which attackers succeed in misleading the model (i.e., the effect of adversarial attacks). A higher SA indicates better performance in clean settings, while higher RA and lower ASR indicate greater robustness against adversarial attacks. \textbf{For detailed metric definitions, see Subsection C in the Appendix.}
     
    % \begin{table}[!t]
    %     \centering
    %     \caption{APT Malware Dataset}
    %     \begin{tabular}{|c|c|c|c|}
    %     \hline
    %     \textbf{GroupName} & \textbf{Count} & \textbf{GroupName} & \textbf{Count} \\ \hline
    %     APT17              & 1428           & Winnti             & 245            \\ \hline
    %     Lazarus Group      & 888            & APT29              & 211            \\ \hline
    %     OceanLotus         & 512            & Blackgear          & 186            \\ \hline
    %     APT10              & 507            & Donot              & 180            \\ \hline
    %     Equation Group     & 395            & TA505              & 147            \\ \hline
    %     APT1               & 345            & BITTER             & 135            \\ \hline
    %     PatchWork          & 315            & Energetic Bear     & 124            \\ \hline
    %     BlackTech          & 280            & APT28              & 109            \\ \hline
    %     Darkhotel          & 251            & APT30              & 102            \\ \hline
    %     \end{tabular}
    %     \label{tab1}
    % \end{table}
% 已搬到附录
    % \begin{table}[h]
    % 	\centering
    % 	\caption{APT Malware Dataset}
    % 	\begin{tabular}{cccc}
    % 	\toprule
    % 	\textbf{GroupName} & \textbf{Count} & \textbf{GroupName} & \textbf{Count}\\
    % 	\midrule
    % 	APT17              & 1428           & Winnti             & 245            \\
    % 	Lazarus Group      & 888            & APT29              & 211            \\
    % 	OceanLotus         & 512            & Blackgear          & 186            \\
    % 	APT10              & 507            & Donot              & 180            \\
    % 	Equation Group     & 395            & TA505              & 147            \\
    % 	APT1               & 345            & BITTER             & 135            \\
    % 	PatchWork          & 315            & Energetic Bear     & 124            \\
    % 	BlackTech          & 280            & APT28              & 109            \\
    % 	Darkhotel          & 251            & APT30              & 102            \\
    % 	\bottomrule
    % 	\end{tabular}
    % 	\label{tab1}
    % \end{table}
\begin{figure*}
\centering
\subfloat[MalConv]{\includegraphics[width=0.24\textwidth]{fig/fig4_malconvGCT.pdf}
\label{fig4_first_case}}%
\hfill
% \subfloat[AvastNet]{\includegraphics[width=0.24\textwidth]{fig/AvastNet.pdf}
% \label{fig4_eight_case}}%
% \hfill
\subfloat[FGSM-AT]{\includegraphics[width=0.24\textwidth]{fig/fig4_FGSM_AT.pdf}
\label{fig4_second_case}}%
\hfill
% \subfloat[FGSM-RS]{\includegraphics[width=0.41\textwidth]{pic/fig4_FGSM_RS.pdf}
% \label{fig4_third_case}}% 
% \hfill
% \subfloat[NuAT]{\includegraphics[width=0.41\textwidth]{pic/fig4_NuAT.pdf}
% \label{fig4_five_case}}%
% \hfill
\subfloat[PGD-4-AT]{\includegraphics[width=0.24\textwidth]{fig/PGD_4_AT.pdf}
\label{fig4_six_case}}%
\hfill
% \subfloat[RoMA-LP]{\includegraphics[width=0.24\textwidth]{fig/RoMA-LP.pdf}
% \label{fig4_four_case}}
% \hfill
\subfloat[RoMA]{\includegraphics[width=0.24\textwidth]{fig/RoMA.pdf}
\label{fig4_seven_case}}
\caption{Visualization of malware representation distributions for attribution models trained using four representative methods.}
% (selected from a total of 8)
\label{fig4}
\end{figure*}
    

\subsection{Experimental setting}\label{subsecB}
\subsubsection{\textbf{Attack Setting}}
For adversarial robustness evaluation, we employ two widely used attack methods: (1) PGD (Projected Gradient Descent), a multi-step iterative attack method \cite{ref55}, with a maximum perturbation strength \(\varepsilon = 0.6\) and 50, 60, or 70 iterations (PGD-50, PGD-60, PGD-70); (2) C\&W (Carlini \& Wagner), an optimization-based attack method \cite{ref50}.
     % C&W攻击方法生成对抗样本：
     % 1.在PE文件四种位置插入初始扰动信息，得到一个初始对抗样本x_{init}。
     % 2.在C&W攻击方法使用L_{2}距离度量生成对抗样本x_{temp}，但是该样本不是functionality-preserving的样本。
     % 3.将x_{temp}中四种位置的扰动内容复制到样本x_{init}中，得到对抗样本x_{adv}.

\subsubsection{\textbf{Implementation Details}}
RoMA was implemented using PyTorch-Lightning 1.5.10, LIEF 0.10.0, and PyCharm as the IDE. The models were trained on a GPU server with dual RTX 4090 GPUs and Ubuntu 18.04.6 LTS. We used the Adam optimizer with a learning rate of 0.0001, and set the contrastive learning temperature \(\tau\) to 0.6. The loss weights $\lambda_{1}$ and $\lambda_{2}$ were both set to 0.3, while the number of GPs, \(K\), was set to 50, selected through extensive tuning. Training was conducted for 100 epochs with a batch size of 64.
     
\subsubsection{\textbf{Compared Methods}}
We compare the performance of the malware attribution models trained by RoMA against seven advanced competitor methods. Two non-adversarial methods: (1) MalConv \cite{ref3}: a SOTA PE malware classification method using the MalConv-GCG architecture for raw-byte analysis; (2) AvastNet \cite{ref58}: a neural network classification method for PE malware by analyzing raw bytes. One single-step adversarial training method: (3) FGSM-AT: an FGSM-based adversarial training method for PE malware classification, extending Slack-FGSM \cite{ref31} and Append-FGSM \cite{ref29} by incorporating RoMA's four perturbation positions (see Subsection \ref{subsec:adver}).
%using our method of packing and random bytes at four perturbation positions to initialize adversarial examples (see Subsection \ref{subsec:adver}).
%(3) slack-FGSM \cite{ref59}: a robust PE malware classification method based on FGSM \cite{ref35}. 
Two PGD-based multiple-step adversarial training methods: (4) PGD-2-AT and (5) PGD-4-AT \cite{ref56}: adversarial training implemented with two-step and four-step PGD attacks, respectively.
Two adversarial training methods adapted from image domains via transfer learning: (6) FGSM-RS \cite{ref49}: a single-step method enhancing FGSM with random initialization of adversarial perturbations; (7) NuAT \cite{ref42}: a single-step method incorporating a Nuclear-Norm regularizer.
 % We compare the proposed method with a series of state-of-the-art FAT methods, i.e., 服从均匀分布随机值作为初始化的方法FGSM-RS [49], 最初的单步对抗训练方法FGSM-AT[41], 以上一个epoch的扰动作为初始化方法FGSM-EP and 基于Nuclear-Norm regularizer的快速对抗训练方法NuAT [42]. We also compare with an advanced multistep AT method, i.e., One of the oldest and most effective defenses PGD-AT[37]), we use an two step PGD-based AT method in the experiments[32].






\begin{table*}[] % !t
    \centering
    \setlength{\tabcolsep}{3.8pt}
    \begin{tabular}{lcrrrcccccc}
    \toprule
    \multirow{2}{*}[-1ex]{\textbf{Method}} & \multirow{2}{*}[-1ex]
    {\textbf{SA}}     & 
    \multicolumn{3}{c}{\textbf{RA}} & \multicolumn{3}{c}{\textbf{ASR}}  & \textbf{RA} & \textbf{ASR} & \multirow{2}{*}[-1ex]{\textbf{Time} \textit{(min)}} \\
    \cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){9-9} \cmidrule(r){10-10} &  & \small\textit{\textbf{PGD-50}} & \small\textit{\textbf{PGD-60}} & \small\textit{\textbf{PGD-70}} & \small\textit{\textbf{PGD-50}} & \small\textit{\textbf{PGD-60}} & \small\textit{\textbf{PGD-70}} & \small\textit{\textbf{C\&W }}& \small\textit{\textbf{C\&W}} & \\
    \midrule
    MalConv & \underline{90.06} & 1.72 & 1.72 & 1.72 & 98.09 & 98.09 & 98.09 & 24.10 & 73.24 & %348.93 
    N/A %One batch 
    \\
    AvastNet & 89.67 & 1.41 & 1.41 & 1.41 & 98.43 & 98.43 & 98.43 & 23.32 & 74.00 & N/A \\ %wasm文献[18]
    FGSM-AT %[Wasm31] FGSM-AT
    & 87.87 & 10.33 & 9.86  & 9.62 & 88.25 & 88.78 & 89.05 & \cellcolor{gray!20}84.66 & \cellcolor{gray!20}3.65 & \underline{2706.57} \\
    
    PGD-2-AT \textit & 88.65 & 31.85 & 28.33 & 25.59 & 64.08 & 68.05 & 71.14 & \cellcolor{gray!20}86.70 & \cellcolor{gray!20}2.21
    %\textbf{2.21} 
    & 3519.12 \\
    PGD-4-AT \textit & 89.59 & \underline{36.85} & \underline{34.90} & \underline{31.61} & \underline{58.86} & \underline{61.05} & \underline{64.72} & \cellcolor{gray!20}88.65 & \cellcolor{gray!20}1.05
    %\textbf{2.21} 
    & 4965.48 \\
    FGSM-RS \textit{(TL)} & 89.44 & 11.50 & 9.15 & 6.89 & 87.14 & 89.76 & 92.30 & \cellcolor{gray!20}87.25 & \cellcolor{gray!20}2.45 
    %\underline{87.25}     & \cellcolor{gray!20}\underline{2.45} 
    & 2760.88 \\
    NuAT \textit{(TL)} & 89.51 & 1.80 & 1.80 & 1.80 & 97.99 & 97.99 & 97.99 & \cellcolor{gray!20}85.68 & \cellcolor{gray!20}4.28 & 2975.68 \\
    %FGSM-PGI & 89.67 & 76.84 & 76.53 & 74.88 & \textbf{89.20} & 14.31 & 14.66 & 16.49 & \underline{0.52} & \textbf{2032.03} & Whole dataset \\
   % \hline
   \midrule
    % RoMA-LP \textit{(Ours)} & \underline{90.85} & \underline{79.42} & \underline{79.19} & \textbf{79.03} & \underline{12.58} & \underline{12.83} & \textbf{13.01} & \textbf{2246.47} & Whole dataset \\
    \textbf{RoMA }\textit{(Ours)} & \textbf{91.00} & \textbf{80.13} & \textbf{79.97} & \textbf{78.79} & \textbf{11.95} & \textbf{12.12} & \textbf{13.41} & \cellcolor{gray!20}88.02
    %\textbf{88.02} 
    & \cellcolor{gray!20}3.27 & \textbf{2378.52} \\
	\bottomrule
    \end{tabular}
    \caption{Comparison of SA, RA, and ASR (\%) for Trained-Models across different Methods, along with adversarial training Time (minutes).}
    % \caption{Comparison of SA,RA, and ASR (\%) during testing and adversarial training time (minutes) on AMG18dataset.}
    \label{tab1}
\end{table*}


\begin{table}[]
    \small
    \setlength{\tabcolsep}{3.4pt}
    \begin{tabular}{llll}
    \toprule
    \textbf{Method} & \textbf{SA ($\Delta$)}  & \textbf{RA ($\Delta$)} & \textbf{ASR ($\Delta$)} \\
    % \cmidrule(r){3-5} \cmidrule(r){6-8}& & PGD-50 & PGD-60 & PGD-70 & PGD-50 & PGD-60 & PGD-70 \\
     \midrule
   % w/o Packing & 89.12 ($\downarrow1.88$) & 10.33 ($\downarrow69.80$) & 10.33 ($\downarrow69.64$) & 9.86 ($\downarrow68.07$) & 88.41 ($\uparrow76.46$) & 88.41 ($\uparrow76.29$) & 88.94 ($\uparrow74.58$) \\
    \textbf{w/o GP} & 90.68 ($\downarrow0.32$) & 14.16 ($\downarrow65.97$) & 84.38 ($\uparrow72.43$) \\
    \textbf{w/o ACLoss} & 84.42 ($\downarrow6.58$) & 55.71 ($\downarrow24.42$) & 33.98 ($\uparrow22.03$) \\
    \textbf{w/o ADLoss} & 89.98 ($\downarrow1.02$) & 79.03 ($\downarrow\enspace1.10$) & 12.17 ($\uparrow\enspace0.22$) \\

    \textbf{w/o AC\&AD} & 83.57 ($\downarrow7.43$) & 51.10 ($\downarrow29.03$) & 38.86 ($\uparrow26.91$) \\
    
% w/o All即FGSM-AT（零初始化，即初始化为0值）；而FGSM-RS是随机初始化。
    \textbf{w/o All} & 87.87 ($\downarrow3.13$) & 10.33 ($\downarrow69.80$) & 88.25 ($\uparrow76.30$) 
    \\
    \midrule
    RoMA \textit{(Ours)} & 91.00 ( / ) & 80.13 ( / ) & 11.95 ( / ) \\
    
%%%%%%%%%%%%%% 开始 Pack的消融   
% \rowcolor{red!30}  \textbf{w/o Pack}& 89.12 ($\downarrow1.88$) & 10.33 ($\downarrow69.80$) & 10.33 ($\downarrow69.64$) & 9.86 ($\downarrow68.07$) & 88.41 ($\uparrow76.46$) & 88.41 ($\uparrow76.29$) & 88.94 ($\uparrow74.58$) \\
% \rowcolor{red!30}  \textbf{w/o All} & 87.86 ($\downarrow3.14$) & \enspace9.00 ($\downarrow71.13$) & \enspace8.69 ($\downarrow71.28$) & \enspace8.29 ($\downarrow70.50$) & 89.76 ($\uparrow77.81$) & 90.12 ($\uparrow78.00$) & 90.56 ($\uparrow77.15$)  \\
%%%%%%%%%%%%%%%% 结束

	\bottomrule
    \end{tabular}
    \caption{Ablation study of RoMA method under PGD-50 Attacks.}
    \label{tab2}
\end{table}


% \begin{table*}[]
%     \small
%     \setlength{\tabcolsep}{4.0pt}
%     \caption{Ablation study of RoMA method under PGD-50 Attacks.}
%     \begin{tabular}{lccccccc}
%     \toprule
%     \multirow{2}{*}[-1ex]{\textbf{Method}} & \multirow{2}{*}[-1ex]{\textbf{SA ($\Delta$)}}    & \multicolumn{3}{c}{\textbf{RA ($\Delta$)}} & \multicolumn{3}{c}{\textbf{ASR ($\Delta$)}} \\
%     \cmidrule(r){3-5} \cmidrule(r){6-8}& & PGD-50 & PGD-60 & PGD-70 & PGD-50 & PGD-60 & PGD-70 \\
%         \midrule
%     RoMA \textit{(Ours)} & 91.00 & 80.13 & 79.97 & 78.79 & 11.95 & 12.12 & 13.41 \\
%    % w/o Packing & 89.12 ($\downarrow1.88$) & 10.33 ($\downarrow69.80$) & 10.33 ($\downarrow69.64$) & 9.86 ($\downarrow68.07$) & 88.41 ($\uparrow76.46$) & 88.41 ($\uparrow76.29$) & 88.94 ($\uparrow74.58$) \\
%     \textbf{w/o GP} & 90.68 ($\downarrow0.32$) & 14.16 ($\downarrow65.97$) & 14.01 ($\downarrow65.96$) & 13.54 ($\downarrow64.39$) & 84.38 ($\uparrow72.43$) & 84.56 ($\uparrow72.44$) & 85.07 ($\uparrow70.71$) \\
%     \textbf{w/o ACLoss} & 84.42 ($\downarrow6.58$) & 55.71 ($\downarrow24.42$) & 55.48 ($\downarrow24.49$) & 52.82 ($\downarrow25.11$) & 33.98 ($\uparrow22.03$) & 34.26 ($\uparrow22.14$) & 37.41 ($\uparrow23.05$) \\
%     \textbf{w/o ADLoss} & 89.98 ($\downarrow1.02$) & 79.03 ($\downarrow\enspace1.10$) & 78.48 ($\downarrow\enspace1.49$) & 76.84 ($\downarrow\enspace1.09$) & 12.17 ($\uparrow\enspace0.22$) & 12.78 ($\uparrow\enspace0.66$) & 14.61 ($\uparrow\enspace0.25$) \\
    
% % w/o All即FGSM-AT（零初始化，即初始化为0值）；而FGSM-RS是随机初始化。
%     \textbf{w/o All} & 87.87 ($\downarrow3.13$) & 10.33 ($\downarrow69.80$) & \enspace9.86 ($\downarrow70.11$) & \enspace9.62 ($\downarrow68.31$) & 88.25 ($\uparrow76.30$) & 88.78 ($\uparrow76.66$) & 89.05 ($\uparrow74.69$) \\
    
% %%%%%%%%%%%%%% 开始 Pack的消融   
% % \rowcolor{red!30}  \textbf{w/o Pack}& 89.12 ($\downarrow1.88$) & 10.33 ($\downarrow69.80$) & 10.33 ($\downarrow69.64$) & 9.86 ($\downarrow68.07$) & 88.41 ($\uparrow76.46$) & 88.41 ($\uparrow76.29$) & 88.94 ($\uparrow74.58$) \\
% % \rowcolor{red!30}  \textbf{w/o All} & 87.86 ($\downarrow3.14$) & \enspace9.00 ($\downarrow71.13$) & \enspace8.69 ($\downarrow71.28$) & \enspace8.29 ($\downarrow70.50$) & 89.76 ($\uparrow77.81$) & 90.12 ($\uparrow78.00$) & 90.56 ($\uparrow77.15$)  \\
% %%%%%%%%%%%%%%%% 结束

% 	\bottomrule
%     \end{tabular}
%     \label{tab2}
% \end{table*}

\subsection{Clean Evaluation Without Attack}
Table \ref{tab1} presents the trained-model performance of RoMA and the compared methods. The second column, SA, reports the standard accuracy on the clean test set without attacks. Among all the trained-models, RoMA achieves the highest SA, followed by the SOTA MalConv. Notably, the non-adversarially-trained models of MalConv and AvastNet, outperform the adversarially-trained models in SA. This aligns with prior studies \cite{ref25,ref36}, which indicate that adversarial training, while enhancing robustness, often compromises performance on clean samples. However, RoMA stands out by not only improving robustness but also achieving superior performance under non-adversarial conditions.

\subsection{Robustness Evaluation Under Attacks}
Table \ref{tab1} presents the trained-model robustness of RoMA and seven competing methods against two attack types. (1) PGD Attacks: As the attack intensity increases from PGD-50 to PGD-70, performance deteriorates across all trained-models. However, RoMA consistently outperforms the others, maintaining a high RA of around 80\% and a low ASR of approximately 13\%. In contrast, the second-best PGD-4-AT achieves only around 36\% RA and over 58\% ASR. Notably, the two non-adversarially-trained models of MalConv and AvastNet experience a drastic drop, with RA below 2\% and ASR exceeding 98\%, making them extremely vulnerable to PGD attacks. 
(2) C\&W Attack: All adversarial-training methods exhibit strong resilience to the C\&W attack (RA $>$ 84\%, ASR $<$ 5\%) with minimal performance variation (highlighted in gray in the table). In contrast, the two non-adversarial-training methods experience a smaller decline compared to their performance under PGD attacks. This indicates that C\&W is not effective against adversarially-trained models, which is why this study focuses primarily on evaluating robustness under PGD attacks. In summary, compared to the competitors, RoMA demonstrates remarkably superior robustness against PGD attacks and strong resilience against C\&W attacks.

Moreover, the `Time' column in Table \ref{tab1} compares the adversarial training time of all the five adversarial-training methods. RoMA requires the least training time while achieving the best performance. In contrast, PGD-4-AT, which ranks second in performance, requires more than twice the training time.
%In contrast, the two-step PGD-AT, which achieves the second-best performance, requires the most time—approximately 1.5 times that of RoMA.
%, whereas the multi-step PGD-AT requires the most time, achieving second-best performance.

%%%%%%%%%%%%%%%%%%%%% 开始 删除
% In our experiments, we evaluated our RoMA approach against seven different methods: the traditional non-adversarial training (non-AT) method known as MalConvGCT (termed 'MalConv' in this study) and AvastNet, three cutting-edge Fast Adversarial Training (FAT) techniques (specifically FGSM-AT, FGSM-RS and NuAT), and a multi-step adversarial training strategy, namely PGD-AT, depicted in SubSection \ref{subsecB}. The experimental results are shown in Table \ref{tab2}. The table is organized as follows: the initial column enumerates the seven previously mentioned methods used for attributing APT malware. The second column presents the Standard Accuracy (SA) for each method in correctly attributing clean malware samples. The next seven columns detail the Robust Accuracy (RA) and ASR for each method when attributing adversarial samples, which were created using PGD attack technique. The last two columns in the table represent the time and data memory footprint during training for each malware attribution method.
% % The final column indicates data memory footprint required during training. 
% % In the table, numbers in bold represent the best results, while those underlined indicate the second best.

% From Table \ref{tab2}, the following insights emerge: The proposed ROMA-LP and RoMA methods outperform all other methods in Standard Accuracy (SA), including non-adversarial training (non-AT) normal methods, MalConv and AvastNet. This enhanced performance is likely attributed to the incorporation of ACLoss and ADLoss in RoMA, which improves the representation quality of malware samples,  including the clean ones. 
% Our approach demonstrates enhanced robust (high Robust Accuracy (RA) while low Attack
% Success Rate(ASR)) in the face of PGD assaults, relative to other FAT methodologies such as FGSM-RS and NuAT. Even when compared to the multi-step adversarial training method PGD-AT, our approach maintains its superior performance. 
% Moreover, RoMA outperforms ROMA-LP in most of the metrics.
% % Regarding Robust Accuracy (RA), RoMA consistently outperforms all other methods in withstanding all three multi-step adversarial attacks, achieving notable superiority over the closest competitor, the FGSM-PGI method. This advantage is demonstrated by increased margins of 3.29\%, 3.44\%, and 3.05\% against PGD attacks of 50, 60, and 70 iterations, respectively. 
% % In the context of the optimization-based C\&W attack scenario, the Fast Adversarial Training (FAT) methods FGSM-PGI and FGSM-RS exhibit slightly superior Robust Accuracies (RAs) compared to our RoMA method. The performance gap is minimal, with a difference of 1.17\% when compared to the optimal FGSM-PGI method. This indicates that our RoMA method attains a comparable level of RA performance in scenarios involving optimization-based C\&W attacks. 

% In terms of training duration, among the seven AT methods, FGSM-LP emerges as the fastest in training. Our RoMA method follows closely as the second fastest, but necessitates a lesser data memory footprint in comparison to RoMA-LP. This shows that RoMA can both save memory and reduce training time during training.

% In summary, our RoMA method surpasses other malware attribution approaches in terms of accuracy, robustness, and training efficiency. It achieves the best performance in Accuracy (SA) on clean samples and demonstrates superior Robust (RA and ASR) against multi-step adversarial attack samples. Moreover, RoMA stands out as the second fastest method among the adversarial training approaches but necessitates a
% lesser data memory footprint, further underscoring its effectiveness and efficiency.
%%%%%%%%%%%%%%%%%%  结束 删除

% 已搬到附录中
% \begin{figure*}
% \centering
% \subfloat[MalConv]{\includegraphics[width=0.41\textwidth]{pic/fig4_malconvGCT.pdf}
% \label{fig4_first_case}}%
% \hfill
% \subfloat[AvastNet]{\includegraphics[width=0.41\textwidth]{pic/AvastNet.pdf}
% \label{fig4_eight_case}}%
% \hfill
% \subfloat[FGSM-AT]{\includegraphics[width=0.41\textwidth]{pic/fig4_FGSM_AT.pdf}
% \label{fig4_second_case}}%
% \hfill
% \subfloat[FGSM-RS]{\includegraphics[width=0.41\textwidth]{pic/fig4_FGSM_RS.pdf}
% \label{fig4_third_case}}% 
% \hfill
% \subfloat[NuAT]{\includegraphics[width=0.41\textwidth]{pic/fig4_NuAT.pdf}
% \label{fig4_five_case}}%
% \hfill
% \subfloat[PGD-AT]{\includegraphics[width=0.41\textwidth]{pic/fig4_PGD_AT.pdf}
% \label{fig4_six_case}}%
% \hfill
% \subfloat[RoMA-LP]{\includegraphics[width=0.41\textwidth]{pic/fig4_RARA.pdf}
% \label{fig4_four_case}}
% \hfill
% \subfloat[RoMA]{\includegraphics[width=0.41\textwidth]{pic/RARA.pdf}
% \label{fig4_seven_case}}
% \caption{Visualized distribution of feature vectors for malware samples in different methods }
% \label{fig4}
% \end{figure*}

\subsection{Ablation Study}
The proposed RoMA method incorporates the following key strategies to enhance the training of attribution models: Global Perturbation (GP) to generate stronger adversarial malware samples, and adversarial consistency regulation loss (comprising ACLoss and ADLoss) to improve malware representation. Table \ref{tab2} presents the ablation study results for removing these components, with $\Delta$ denoting the performance variation compared to models trained by the full RoMA.

Under clean conditions, as shown in the `SA' column, removing any single component causes a noticeable performance drop, with ACLoss contributing the most to improving clean accuracy. Under adversarial PGD-50 attacks, the `RA' and `ASR' columns reveal an even more pronounced decline. Specifically, removing GP dramatically reduces RA and substantially increases ASR, highlighting its critical role in training robust models; removing ACLoss significantly degrades robustness, while removing ADLoss causes a milder yet noticeable impact. The most severe degradation occurs when all three components are removed, with RA dropping by 69.80\% and ASR surging by 76.30\%. These results emphasize the importance of integrating all three components during training to achieve optimal model robustness.

% The proposed RoMA method incorporates the following key strategies to enhance the training of attribution models: Global Perturbation (GP) to generate stronger adversarial malware samples, and adversarial consistency regulation loss (comprising ACLoss and ADLoss) to improve malware representation. Specifically, ACLoss enhances intra-class compactness and inter-class separation, while ADLoss refines the decision boundary. Table \ref{tab2} presents the ablation study results for removing these components, including SA under non-adversarial conditions, and RA and ASR under adversarial PGD-50 attacks, with $\Delta$ denoting the performance variation compared to models trained by the full RoMA.

% As shown in the 'SA' column, under clean conditions, removing any single component causes a noticeable performance drop, with ACLoss contributing the most to improving clean accuracy. The 'RA' and 'ASR' columns reveal an even more pronounced decline under adversarial conditions. Specifically, removing the GP component results in a dramatic decrease in RA by 65.97\% and an increase in ASR by 72.43\%, underscoring the critical role of GP in training robust models. Removing ACLoss significantly degrades robustness, while removing ADLoss causes a milder yet noticeable impact. The most severe degradation occurs when all three components are removed, with RA dropping by 69.80\% and ASR surging by 76.30\%. These results emphasize the importance of integrating all three components during training to achieve optimal model robustness.

% The proposed RoMA method employs three distinct strategies to enhance the effectiveness of malware attribution:
% (1)The global perturbation, which applies the global perturbation into the obfuscated malware. (2) The ACLoss, aimed at improving the quality of malware representation. (3) The ADLoss, intending to learn a better decision boundary. In our ablation study, we examine the individual contributions of each strategy to the effectiveness of RoMA in malware attribution. The results are detailed in Table \ref{tab2}. This table showcases the performance of the trained-models by the full RoMA method and its variants with one of the aforementioned strategies removed, respectively, which allows us to understand the impact of each strategy on RoMA-trained model's overall performance.

% From Table\ref{tab2}, we can derive the following insights: Each of the three strategies contributes to the attribution effectiveness of RoMA, impacting both Standard Accuracy (SA) and Robust (RA and ASR). Notably, the incorporation of GP leads to considerable increases in RA, boosting performance against multi-step attacks by over 64\% while ASR is decreased by more than 70\%. The Packing strategy plays a crucial role in enhancing RA for both multi-step and optimization-based attacks, with both seeing improvements exceeding 69\%. Additionally, the ACLoss strategy demonstrates a significant impact, particularly in improving RA against multi-step attacks, with an enhancement of over 24\%. Finally, the addition of ADLoss to the RoMA method positively affects all three metrics, further elevating the overall malware attribution effectiveness of the RoMA approach.
% In the table's last column, it is shown that adding ACLoss or ADLoss to the RoMA approach slightly prolongs the duration of the adversarial training, with each loss requiring less than 100 additional minutes for processing. Conversely, incorporating the first two strategies, namely the Packing strategy and GPs initialization, into the RoMA approach reduces the adversarial training time by 184 and 812 minutes, respectively. This reduction in training time can likely be attributed to the improved efficacy of the initial adversarial examples generated by these strategies, which in turn expedites the following adversarial training phase.

% \begin{table*}[] % !t
%     \centering
%     \caption{Ablation study of our RoMA method.}
%     \begin{tabular}{cccccccccc}
%     \toprule
%     \multirow{2}{*}[-1ex]{\textbf{Strategy}} & \multirow{2}{*}[-1ex]{\textbf{SA ($\Delta$)}}    & \multicolumn{4}{c}{\textbf{RA ($\Delta$)}} & \multicolumn{4}{c}{\textbf{ASR ($\Delta$)}} \\
%     \cmidrule(r){3-6} \cmidrule(r){7-10}& & PGD-50 & PGD-60 & PGD-70 & C\&W & PGD-50 & PGD-60 & PGD-70 & C\&W\\
%         \midrule
%     w/o Packing & 89.12 ($\downarrow1.88$) & 10.33 ($\downarrow69.80$) & 10.33 ($\downarrow69.64$) & 9.86 ($\downarrow68.07$) & 18.70 ($\downarrow69.32$) & 88.41 ($\uparrow76.46$) & 88.41 ($\uparrow76.29$) & 88.94 & 79.02 \\
%     w/o GP & 90.68 ($\downarrow0.32$) & 14.16 ($\downarrow65.97$) & 14.01 ($\downarrow65.96$) & 13.54 ($\downarrow64.39$) & 88.34($\uparrow0.32$) & 84.38 ($\uparrow72.43$) & 84.56 ($\uparrow72.44$) & 85.07 & 2.59 \\
%     w/o ACLoss & 84.42 ($\downarrow6.58$) & 55.71 ($\downarrow24.42$) & 55.48 ($\downarrow24.49$) & 52.82 ($\downarrow25.11$) & 78.17 ($\downarrow9.85$) & 33.98 ($\uparrow22.03$) & 34.26 ($\uparrow22.14$) & 37.41 & 7.41 \\
%     w/o ADLoss & 89.98 ($\downarrow1.02$) & 79.81 ($\downarrow0.32$) & 79.42 ($\downarrow0.55$) & 77.39 ($\downarrow0.57$) & 87.32 ($\downarrow0.7$) & 11.30 ($\uparrow0.65$) & 11.74 ($\downarrow0.98$) & 14.00 & 2.96 \\
%     RoMA & 91.00 & 80.13 & 79.97 & 77.93 & 88.02 & 11.95 & 12.12 & 14.36 & 3.27 \\
% 	\bottomrule
%     \end{tabular}
%     \label{tab2}
% \end{table*}

\subsection{Visualization analysis}
%\subsection{Visualization of Malware Representations}
To better analyze and visualize the performance of attribution models trained using RoMA and seven competitor methods, we applied t-SNE \cite{ref60} to project malware representation distributions into two dimensions. We selected 360 natural samples (20 per APT group) from the test set and generated corresponding adversarial samples, yielding 720 representation vectors. Figure \ref{fig4} shows the representation distributions for four representative methods: MalConv (non-adversarial), FGSM-AT (single-step), PGD-4-AT (multi-step), and RoMA. \textbf{Visualizations for all eight methods are included in Figure A2 in the Appendix.} In these figures, different APT groups are distinguished by unique colors, with clean samples shown as solid-squares and adversarial samples as cross-signs.
%We selected 360 natural samples (20 per APT group) from the test set and generated corresponding adversarial samples using PGD-50 attacks, yielding 720 representation vectors through each trained model.

As the figures show, the model trained with RoMA demonstrates superior representation distribution compared to all competitors, with closer within-class clustering  (particularly between adversarial and clean samples) and improved between-class separation. The clearer decision boundary between classes further highlights its effectiveness. RoMA achieves this by generating stronger adversarial malware through GP, enhancing within-class clustering and between-class separation via ACLoss, and refining the decision boundary with ADLoss.
% intra-class compactness and inter-class separation


\section{Conclusion}\label{sec6}
This work addresses the challenge of robust APT malware group attribution through byte-level analysis. We proposed RoMA, a fast adversarial training approach that integrates global perturbations and adversarial consistency regularization, achieving significant improvements in robustness. To support evaluation, we introduced AMG18, a novel APT malware dataset with group labels. Experimental results demonstrate that RoMA consistently outperforms seven competitor methods with remarkable robustness, high accuracy, and efficient training. Future work includes applying RoMA to diverse malware types, additional classification tasks, and broader domains. The RoMA-trained model and visualized malware samples are publicly available. 
% \subsection{Conclusion}

\bibliographystyle{named}
\bibliography{ijcai25}

\newpage
\appendix
% \counterwithin{figure}{section}
% \numberwithin{figure}{section}
% \counterwithin{table}{section}
% \numberwithin{table}{section}

\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}

\section{Perturbation Positions in PE File}
Figure \ref{fig2} illustrates four types of perturbation positions (Perturb-Pos) within a PE file: (1) Full DOS Region: All positions in the DOS header, excluding the 'MZ' magic number and the 4-byte pointer to the PE header; (2) Shifting Space: A 1KB gap created by shifting the first section relative to the PE header; (3) Slack Space: Unused regions between sections; (4) Padding Space: Extra space appended at the end of the PE file, up to 100KB.

\begin{figure} [H]
\centering
\includegraphics[width=0.48\textwidth]{pic/fig2.pdf}
\centering
\caption{Perturbation positions in a PE file}
\label{fig2}
\end{figure}
% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.5\textwidth]{pic/fig1.pdf}
%   \caption{MalConv-GCG architecture}
%   \label{fig:fig1}
% \end{figure}

\section{Dataset}   
    Table \ref{tab1} details the 18 APT groups in our dataset and their respective malware sample counts. This dataset exhibits class imbalance, reflecting real-world scenarios. Every class in our dataset contains over 100 samples, ensuring adequate data for a robust train/test split and reducing the risk of overfitting due to limited training samples.

\begin{table}[h]
    	\centering
    	\caption{APT Malware Dataset}
    	\begin{tabular}{cccc}
    	\toprule
    	\textbf{GroupName} & \textbf{Count} & \textbf{GroupName} & \textbf{Count}\\
    	\midrule
    	APT17              & 1428           & Winnti             & 245            \\
    	Lazarus Group      & 888            & APT29              & 211            \\
    	OceanLotus         & 512            & Blackgear          & 186            \\
    	APT10              & 507            & Donot              & 180            \\
    	Equation Group     & 395            & TA505              & 147            \\
    	APT1               & 345            & BITTER             & 135            \\
    	PatchWork          & 315            & Energetic Bear     & 124            \\
    	BlackTech          & 280            & APT28              & 109            \\
    	Darkhotel          & 251            & APT30              & 102            \\
    	\bottomrule
    	\end{tabular}
    	\label{tab1}
\end{table}

\section{\textbf{Evaluation Metrics}}
\begin{enumerate}
        \item[a)] \textbf{Standard Accuracy (SA)}: SA measures the model's attribution accuracy on clean samples across all groups. 
        \begin{equation}
            SA=\sum_{i=1}^{n}\left ( \frac{C_{i}^{clean}}{T_{i}^{clean}} \right )\times \left ( \frac{T_{i}^{clean}}{\sum_{j=1}^{n}T_{j}^{clean}}\right ),
            \label{equ7}
        \end{equation}
where \(n\) is the number of attributed APT groups, \(C_{i}^{clean}\) is the number of correct attributions for group \(i\) on clean samples, \(T_{i}^{clean}\) represents the total number of clean samples for group \(i\), and \(\frac{T_{i}^{clean}}{\sum_{j=1}^{n}T_{j}^{clean}}\) denotes the weight for group \(i\) among the clean-sample groups.
        \item[b)] \textbf{Robust Accuracy (RA)}: RA evaluates the model's robustness in correctly attributing adversarially perturbed samples across all groups. 
        \begin{equation}
            RA=\sum_{i=1}^{n}\left ( \frac{C_{i}^{adv}}{T_{i}^{adv}} \right )\times \left ( \frac{T_{i}^{adv}}{\sum_{j=1}^{n}T_{j}^{adv}}\right ),
            \label{equ8}
        \end{equation}
where \(C_{i}^{adv}\) is the number of correct predictions for group \(i\) on adversarial samples, \(T_{i}^{adv}\) is the total number of adversarial samples for group \(i\), and \(\frac{T_{i}^{adv}}{\sum_{j=1}^{n}T_{j}^{adv}}\) represents the weight for group \(i\) in the adversarial-sample groups. 
     \item[c)] \textbf{Attack Success Rate(ASR)}: ASR is one of the most direct indicators to measure the effect of adversarial attacks. It represents the rate at which the attacker succeeds in spoofing the model and obtaining the wrong output. A higher ASR means that adversarial attacks are more effective and the model is less robust against such attacks.
     \begin{equation}
         ASR=\sum_{i=1}^{n}\left ( \frac{C_{i}^{clean}-C_{i}^{adv}}{C_{i}^{clean}} \right )\times \left ( \frac{C_{i}^{clean}}{\sum_{j=1}^{n}C_{j}^{clean}}\right ),
         \label{equ9}
     \end{equation}
     where \(\frac{C_{i}^{clean}}{\sum_{j=1}^{n}C_{j}^{clean}}\) denote the weight for correct predictions of group \(i\) in the correct predictions of clean-sample groups.
\end{enumerate}

\section{\textbf{Visualization Results}}
\renewcommand{\thefigure}{A\arabic{figure}}
The representation visualization is shown in Figure \ref{fig4}.

\begin{figure*}
\centering
\subfloat[MalConv]{\includegraphics[width=0.41\textwidth]{pic/fig4_malconvGCT.pdf}
\label{fig4_first_case}}%
\hfill
\subfloat[AvastNet]{\includegraphics[width=0.41\textwidth]{pic/AvastNet.pdf}
\label{fig4_eight_case}}%
\hfill
\subfloat[FGSM-AT]{\includegraphics[width=0.41\textwidth]{pic/fig4_FGSM_AT.pdf}
\label{fig4_second_case}}%
\hfill
\subfloat[FGSM-RS]{\includegraphics[width=0.41\textwidth]{pic/fig4_FGSM_RS.pdf}
\label{fig4_third_case}}% 
\hfill
\subfloat[NuAT]{\includegraphics[width=0.41\textwidth]{pic/fig4_NuAT.pdf}
\label{fig4_five_case}}%
\hfill
\subfloat[PGD-2-AT]{\includegraphics[width=0.41\textwidth]{pic/fig4_PGD_AT.pdf}
\label{fig4_six_case}}%
\hfill
\subfloat[PGD-4-AT]{\includegraphics[width=0.41\textwidth]{pic/fig_PGD_4_AT.pdf}
\label{fig4_six_case}}%
\hfill
% \subfloat[RoMA-LP]{\includegraphics[width=0.41\textwidth]{pic/fig4_RARA.pdf}
% \label{fig4_four_case}}
% \hfill
\subfloat[RoMA]{\includegraphics[width=0.41\textwidth]{pic/RARA.pdf}
\label{fig4_seven_case}}
\caption{Visualization of malware representation distributions for attribution models trained using all eight methods.}
\label{fig4}
\end{figure*}

\end{document}

