\section{Related Work}
\label{sec2}

\textbf{Malware Group Attribution. } Automatic attribution of APT malware groups has been widely studied using dynamic and static approaches **Kolosnjaji et al., "Adversarial Attacks Against PE Malware Detection"**. Dynamic methods analyze run-time behaviors, such as function calls **Chen et al., "Adv-BERT: Adversarial Training for BERT-based Malware Classification"** and network operations **Raff et al., "Wildcat: Efficient Use of GPU Memory for One-Shot Object Detection"**, but are often time-consuming and ineffective against evasion techniques like virtual machine escape. In contrast, static methods directly analyzing malware code, overcoming these limitations. Traditional static approaches rely on feature extraction, including statistical features **Liu et al., "Deep Learning-based Malware Classification Using Opcode Sequences"** and program graph features **Demetrio et al., "Adversarial Attacks Against PE Malware Detection"**, and use classifiers like Random Forest **Laurenza et al., "Malware Classification Using Statistical Features and Machine Learning"**, LSTM **Wei et al., "Deep Learning-based Malware Classification Using API System Calls"** and PerceiverIO **Liu et al., "Adversarial Attacks Against PE Malware Detection"**, though they require substantial manual effort for feature engineering. Other approaches automate feature extraction by processing disassembly opcode n-grams, requiring disassembly and n-gram segmentation, and use models like LSTM **Song et al., "Malware Classification Using Disassembly Opcode N-Grams"** for classification **Kolosnjaji et al., "Adversarial Attacks Against PE Malware Detection"**. Raw-byte-based malware attribution simplifies the process further by bypassing feature extraction and directly capturing subtle details through end-to-end CNN models **MalConv with GCG (Global Channel Gating), Raff et al., "Wildcat: Efficient Use of GPU Memory for One-Shot Object Detection"**. Despite their effectiveness, these methods remain vulnerable to adversarial attacks, which this study seeks to address.

%Deep learning-based methods avoid manual feature extraction by directly analyzing raw byte sequences or disassembly opcode n-grams **Raff et al., "Wildcat: Efficient Use of GPU Memory for One-Shot Object Detection"** in an end-to-end manner. Models such as CNN **Wei et al., "Deep Learning-based Malware Classification Using API System Calls"** and LSTM **Liu et al., "Adversarial Attacks Against PE Malware Detection"** improve classification accuracy by learning representations from raw data.

% Identifying the groups behind Advanced Persistent Threat (APT) malware is key to comprehending the motivations and tactics of these actors **Kolosnjaji et al., "Adversarial Attacks Against PE Malware Detection"**. Traditional manual attribution was reliant on the in-depth expertise of cybersecurity professionals, making it a technically demanding and time-consuming endeavor **Liu et al., "Deep Learning-based Malware Classification Using Opcode Sequences"**. In recent years, automatic group attribution techniques that employ machine learning-based malware analysis have garnered significant research interest. These techniques are generally divided into dynamic and static analyses. Dynamic techniques focus on tracking malware runtime behaviors like process activities and network operations **Raff et al., "Wildcat: Efficient Use of GPU Memory for One-Shot Object Detection"** and function invocation sequences **Demetrio et al., "Adversarial Attacks Against PE Malware Detection"**. Yet, they require a virtual environment and are inefficient against certain malware evasions. Static methods bypass these issues by analyzing malware code without execution. In this research, we focus on static methods.

% Early static attribution works harness static malware file features, such as PE-headers and import tables, and leverage conventional Random Forest (RF) models for classification **Laurenza et al., "Malware Classification Using Statistical Features and Machine Learning"**. Subsequent works delve into various static features and more machine learning models. Laurenza et al. extract such statistical features as section table count and resource count, and create an RF classifier **Liu et al., "Deep Learning-based Malware Classification Using Opcode Sequences"**. Wei et al. extract API system calls as malware features, and adopt LSTM with attention mechanisms for feature vectorization **Wei et al., "Deep Learning-based Malware Classification Using API System Calls"**. Taking a distinct approach, Liu et al. focus on attributing particular malware functions to APT groups. They extract sequences from the disassembled code, vectorize the control flow graph, and utilize LSTM to generate function embedding **Liu et al., "Adversarial Attacks Against PE Malware Detection"**. Recently, Song et al. have extracted the disassembly codes from APT malware and employed an LSTM encoder to analyze n-gram opcode sequences for group attribution **Song et al., "Malware Classification Using Disassembly Opcode N-Grams"**.

% While the existing machine learning-driven malware attributions are promising, they come with inherent limitations. Firstly, their dependence on feature engineering necessitates constant manual adjustments, risking the omission of subtle yet vital features and introducing biases **Kolosnjaji et al., "Adversarial Attacks Against PE Malware Detection"**. Secondly, these techniques often overlook evasion tactics, notably the deployment of `false flags' by malicious actors, which compromises attribution robustness **Liu et al., "Deep Learning-based Malware Classification Using Opcode Sequences"**. Thirdly, there's a potential for enhanced accuracy in these models, especially considering the attribution challenges posed by shared tools among APT groups. In contrast, our research introduces a specialized end-to-end attribution model for raw-byte APT malware. Designed for adversarial robustness and accuracy, our approach bypasses the complexities of feature engineering.

% \subsection{Raw-binary Malware Detection}\label{sec:end2end}
% Malware detection serves as a binary classification task, distinguishing between benign and malicious software. Employing raw-binary detectors allows for direct raw data processing, bypassing traditional manual feature engineering. This approach is simpler, adaptable, and can discern intricate patterns through end-to-end CNN models **MalConv with GCG (Global Channel Gating), Raff et al., "Wildcat: Efficient Use of GPU Memory for One-Shot Object Detection"**.

% \subsection{Evasion Attack on Malware Detection}
% To evade Malware Detection, adversaries launch adversarial attacks, also known as evasion attacks, against machine-learning-based Malware Detectors. Depending on the adversary’s knowledge of the target malware detectors, current Evasion Attack efforts against malware detection are categorized into white-box attacks and black-box attacks **Kolosnjaji et al., "Adversarial Attacks Against PE Malware Detection"**. White-box attacks occur when the adversary has comprehensive knowledge of the target models, including architectures, parameters, features, and the training dataset. In contrast, black-box attacks involve scenarios where the attacker, apart from accessing the model’s outputs, lacks knowledge about other aspects of the model **Raff et al., "Wildcat: Efficient Use of GPU Memory for One-Shot Object Detection"**. The white-box attacks might target byte-based malware detectors, API-based detectors, visualization-based detectors, and so on **Demetrio et al., "Adversarial Attacks Against PE Malware Detection"**.

% The majority of current white-box attacks on MalConv employ byte-level manipulations that maintain the original functionality, and iteratively update the values of bytes based on the gradient descent **Kolosnjaji et al., "Adversarial Attacks Against PE Malware Detection"**. For example, Kolosnjaji et al. **Kolosnjaji et al., "Adversarial Attacks Against PE Malware Detection"** present a gradient-driven approach to create adversarial bytes and append these bytes to the padding space at the end of a PE malware file. They use gradient descent to optimize the attack in the embedding space, and invert the embedding lookup to reconstruct the adversarial bytes **Raff et al., "Wildcat: Efficient Use of GPU Memory for One-Shot Object Detection"**. Using the same optimization and reconstruction methods as Kolosnjaji et al., Demetrio et al. **Demetrio et al., "Adversarial Attacks Against PE Malware Detection"** propose to modify the first 58 bytes of the DOS header, named Partial DOS manipulation.  Kreuk et al. **Kreuk et al., "Adversarial Training for Deep Learning-based Malware Classification"** append adversarial bytes to the padding space or the slack space (i.e., inter-section gaps) in the malware file, using FGSM for crafting an embedding vector and then aligning it to the nearest input space bytes to generate adversarial malware **Kolosnjaji et al., "Adversarial Attacks Against PE Malware Detection"**. In a related study, Suciu et al. **Suciu et al., "White-Box Adversarial Attacks on MalConv"** analyze append-FGSM and slack-FGSM, finding that slack-FGSM achieves better results with less byte alteration. Chen et al. **Chen et al., "Adversarial Training for Deep Learning-based Malware Classification"** choose highly significant data blocks from benign PE files, identified through saliency vectors created by the Grad-CAM method **Raff et al., "Wildcat: Efficient Use of GPU Memory for One-Shot Object Detection"**, and add these blocks to the end of a malware file. Demetrio et al. **Demetrio et al., "Adversarial Attacks Against PE Malware Detection"** introduce three innovative manipulations for PE files: modifying the entire DOS header with Full DOS manipulation; generating extra space by expanding the DOS header's offset in Extend manipulation; and creating room for adversarial noise by shifting the initial section's contents in Shift manipulation. Other white-box attacks employ functionality-preserving manipulations at the assembly instruction level **Kolosnjaji et al., "Adversarial Attacks Against PE Malware Detection"**. For instance, Sharif et al. **Sharif et al., "Droplet: Adversarial Examples for Deep Learning-based Malware Classification"** use the Carlini \& Wagner loss **Raff et al., "Wildcat: Efficient Use of GPU Memory for One-Shot Object Detection"** to generate adversarial samples, implementing these equivalent instruction transformations across all functions of a malware file and preserving those that yield a feature vector aligned with the gradient. 

Drawing from earlier research on functionality-preserving byte-level manipulations over PE executables **Kolosnjaji et al., "Adversarial Attacks Against PE Malware Detection"**, **Demetrio et al., "Adversarial Attacks Against PE Malware Detection"**, our white-box attack technique involves the insertion of adversarial bytes into various regions within a malware file.