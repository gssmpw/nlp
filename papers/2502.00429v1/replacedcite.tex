\section{Background and Related Work}
In this section, we introduce fundamental concepts important for this study, namely the ML engineering process and the chosen export formats, and discuss related work in the area.

\subsection{The Process of Engineering ML-Enabled Systems}
While existing development processes for non-ML software, such as Agile or Waterfall____, are partly applicable for the engineering of ML-enabled systems,
their peculiarities also lead to some notable differences.
While no universally accepted ML engineering process exists, most sources include steps similar to the following ones____:

\begin{itemize}
    \item \textbf{Problem Definition}: Define objectives and collect data.
    \item \textbf{Data Preprocessing}: Clean and prepare data.
    \item \textbf{Model Selection, Training, and Evaluation}: Train an appropriate ML model. Evaluate performance to ensure the model meets desired accuracy and other criteria.
    \item \textbf{Software Development}: Develop the ML and non-ML components of the ML-enabled system, e.g., user interfaces, application logic, or prediction components.
    \item \textbf{ML Integration}: As a substage of the software development stage, the integration step incorporates ML models into the ML component of an ML-enabled system. ML models are exported in a suitable format and then imported into the target ML components. Compatibility between the export format and the ML component needs to be ensured, but the format selection can make the integration easier or harder. Additionally, the variety of available formats complicates this choice.
    \item \textbf{System Testing}: Conduct system-wide testing to ensure all components function correctly and meet end-to-end performance requirements.
    \item \textbf{Deployment, Monitoring, and Maintenance}: Deploy the system, monitor its performance, and fix future issues.
\end{itemize}

\subsection{ML Model Export Formats}
Choosing a suitable model export format is vital for effectively integrating ML models into larger systems.
A poor choice is likely to introduce unnecessary dependencies, increase project complexity, and add to the overall integration effort and maintenance____.
Additionally, the export format can also impact quality attributes, e.g., in the form of performance bottlenecks, security vulnerabilities, and elevated resource demands.
For instance, improper integration of an ML model can create new attack surfaces, potentially exposing both the model and the system to malicious actors____.
In conclusion, the importance of selecting the appropriate model export format extends well beyond functionality.
A thoughtful choice in format selection simplifies integration and can directly enhance system performance and security.

In this study, we evaluated five model export formats regarding their impact on integration, for which understanding their internal mechanisms is essential.
In the following, we introduce these formats. Further justification for why these formats were chosen are provided in later sections.
Additionally, Fig.~\ref{fig:export-formats} visualizes the most important details of the formats.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/DiagramMEP.pdf}
    \caption{Serialization Process of Model Export Formats}
    \label{fig:export-formats}
\end{figure}

\subsubsection{ONNX}
The Open Neural Network Exchange (ONNX)\footnote{\url{https://onnx.ai}} format has the goal to make models portable between frameworks and languages.
The export process involves converting the model into a graph, followed by serialization into the Protobuf\footnote{\url{https://protobuf.dev}} format, known for its efficiency.
During import, these steps are reversed____.

\subsubsection{Pickle}
Pickle\footnote{\url{https://docs.python.org/3/library/pickle.html}} is widely used in Python to serialize data structures including ML models into byte streams (often referred to as \enquote{pickling}).
Deserialization (\enquote{unpickling}) converts these streams back into Python data structures.
During unpickling, referenced modules are also required to be imported____.

\subsubsection{PyTorch's TorchScript}
To export models in PyTorch, TorchScript\footnote{\url{https://pytorch.org/docs/stable/jit.html}} is commonly used, enabling models to be serialized via Pickle in an intermediate format that primarily relies on TorchScript but also involves several PyTorch libraries. TorchScript creates a structured export object that includes the model's tensors, optimizers, and hyperparameters, allowing seamless deserialization and model reconstruction during loading. As this process encompasses multiple components of PyTorch____, we will refer to the TorchScript export format simply as \enquote{PyTorch}.

\subsubsection{Tensorflow's SavedModel}
TensorFlow offers a custom export format called SavedModel\footnote{\url{https://www.tensorflow.org/guide/saved_model}}.
This format is similar to ONNX, where the model is converted to a graph and serialized into Protobuf.
While this is supposed to allow model transfer across TensorFlow implementations in different programming languages____, the difference to ONNX is that no cross-framework portability is supported.

\subsubsection{Joblib}
Joblib\footnote{\url{https://joblib.readthedocs.io/en/stable}} serializes Python objects similarly to Pickle but breaks ML models into smaller compressed chunks for parallel serialization, which is supposed to offer speed and memory benefits____.

Moreover, several model export formats come with a \textit{runtime module}.
These modules play a crucial role in integrating models into ML-enabled systems by handling tasks like model loading, dependency management, computational resource allocation, and inference execution.
They also facilitate data handling and ensure smooth interaction between the model and the system.
Official runtime modules, such as the ONNX runtime or PyTorch's JIT compiler, are developed by the creators of the model formats and offer optimized performance and reliability.
Unofficial runtime modules, developed by third-party contributors, can provide additional flexibility, but may lack the stability and support of official versions.
Generally, official runtime modules are preferred due to their seamless integration and direct support from the format developers____.

\subsection{Related Work}
Despite the undisputed importance of the integration stage of ML-enabled systems, very few studies exist on the topic of model export formats, highlighting a gap this research aims to fill.
____ analyzed ONNX's stability for integrating deep learning models with Julia-based systems, while ____ shared Google's largely positive experiences with TensorFlow, acknowledging potential bias due to their involvement in TensorFlow's development.
____ showed ONNX's performance advantages in runtime modules, while ____ emphasized the impact of integration quality on performance in autonomous driving systems, advocating for more extensive testing.
These findings collectively indicate that optimal model export format selection can significantly enhance development and system performance, whereas suboptimal choices may reduce efficiency and effectiveness.

More broadly, the topic of ML model integration in ML-enabled systems has been recently studied via mining open-source repositories.
____ found that (i) about half of the products integrate third-party, i.e., pre-trained instead of self-trained, ML models, and (ii) the majority of products integrate several mostly independent models.
____ categorized the integration of ML models in four distinct patterns: \textit{alternative} (several models for the same task), \textit{independent} (different models for different tasks), \textit{sequential} (cascading model invocations), and \textit{joining} (results of different models are combined by another model or code). 
They also noted that loading ML models can become a complex configuration and assembly problem. 
____ found that ML models are loaded from files for three reasons: for evaluation or inference (most common case), for resuming training, and for transforming them to different formats.
The latter happens, e.g., for deploying ML models in a target environment or for reducing their size.
However, none of these three studies include a discussion of the concrete export formats used in the analyzed repositories.
Our study complements this body of knowledge with best practices for model export format selection, which is part of every model integration process.

Recent works have also focused on the timely problem of reusing and re-engineering ML models, in particular for deep learning (DL).
____ identified the portability of DL operators  and the complex data pipelines as two main challenges in re-engineering DL models.
____ categorized DL model reuse into conceptual reuse, adaptation reuse, and deployment reuse.
DL interoperability is a primary challenge in the latter, and can be tackled by using standardized representations such as ONNX or model converters____.
The choice of model export format, which can be informed by our study, clearly also affects the interoperability of the resulting ML/DL model.
In summary, the concrete integration impact of different export formats has so far been neglected by existing studies.