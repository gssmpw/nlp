\section{Background}
\label{sec:background}

\noindent
In this section, we first overview the principles governing transformer architecture. Next, we present a concise overview of DP-SFGs, which we employ to map OTA circuits into transformer-friendly sequential data. Finally, we describe a precomputed LUT-based width estimator to translate DP-SFG parameters to transistor widths.
\vspace{-1mm}
\subsection{The transformer architecture}

\noindent
The transformer~\cite{vaswani_17} is viewed as one of the most promising deep learning architectures for sequential data prediction in NLP.  It relies on an attention mechanism that reveals interdependencies among sequence elements, even in long sequences. The architecture takes a series of inputs \((x_1, x_2, x_3, \ldots, x_n\)) and generates corresponding outputs \((y_1, y_2, y_3, \ldots, y_n\)).

\begin{figure}[b]
\vspace{-5mm}
\centering
\includegraphics[width=0.5\textwidth, bb=0 0 370 190]{fig/TransformermODEL.pdf}
\vspace{-5mm}
\caption{Architecture of a transformer.}
\label{fig:simpleTrans}
% \vspace{-2mm}
\end{figure}

The simplified architecture shown in Fig.~\ref{fig:simpleTrans} consists of $N$ identical stacked encoder blocks, followed by $N$ identical stacked decoder blocks. The encoder and decoder is fed by an input embedding block, which converts a discrete input sequence to a continuous representation for neural processing. Additionally, a positional encoding block encodes the relative or absolute positional details of each element in the sequence using sine-cosine encoding functions at different frequencies. This allows the model to comprehend the position of each element in the sequence, thus understanding its context. Each encoder block comprises a multi-head self-attention block and a position-wise feed-forward network (FFN); each decoder block, which has a similar structure to the encoder, consists of an additional multi-head cross-attention block, stacked between the multi-head self-attention and feed-forward blocks. The attention block tracks the correlation between elements in the sequence and builds a contextual representation of interdependencies using a scaled dot-product between the query ($Q$), key ($K$), and value ($V$) vectors:
\begin{equation}
\text{{Attention}}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V,
\end{equation}
where $d_k$ is the dimension of the query and key vectors. The FFN consists of two fully connected networks with an activation function and dropout after each network to avoid overfitting. The model features residual connections across the attention blocks and FFN to mitigate vanishing gradients and facilitate information flow.

\subsection{Driving-point signal flow graphs}

\noindent
The input data sequence to the transformer must encode information that relates the parameters of a circuit to its performance metrics.  Our method for representing circuit performance is based on the signal flow graph (SFG).  The classical SFG proposed by Mason~\cite{Mason53} provides a graph representation of linear time-invariant (LTI) systems, and maps on well to the analysis of linear analog circuits such as amplifiers. In our work, we employ the driving-point signal flow graph (DP-SFG)~\cite{ochoa_98,schmid_18}. The vertices of this graph are the set of excitations (voltage and current sources) in the circuit and internal states (e.g., voltages) in the circuit.  
% An edge is drawn between vertices that have an electrical relationship, and the weight on each edge is the gain of the edge;
An edge connects vertices with an electrical relationship, and the edge weight is the gain; 
for example, if a vertex $z$ has two incoming edges from vertices $x$ and $y$, with gains $a$ and $b$, respectively, then $z = ax + by$, using the principle of superposition in LTI systems.  To effectively use superposition to assess the impact of each node on every other node, the DP-SFG introduces auxiliary voltages at internal nodes of the circuit that are not connected to excitations. These auxiliary sources are structured to not to alter any currents or voltages in the original circuit, and simplifies the SFG formulation for circuit analysis.
% enable easy formulation of the SFG to analyze circuit behavior. 

\begin{figure}[t]
% \vspace{-6mm}
\centering
\includegraphics[width=0.9\linewidth, bb=0 0 320 140]{fig/DPSFG.pdf}
\vspace{-0.25cm}
\caption{~(a) Schematic and (b) DP-SFG for an active inductor.}
\label{fig:DP-SFG_ex}
\vspace{-5mm}
\end{figure}

Fig.~\ref{fig:DP-SFG_ex}(a) shows a circuit of an active inductor, which is an inductor-less circuit that replicates the behavior of an inductor over a certain range of frequencies. Fig.~\ref{fig:DP-SFG_ex}(b) shows the equivalent DP-SFG. In Section~\ref{sec:dp-sfg}, we provide a detailed explanation that shows how a circuit may be mapped to its equivalent DP-SFG. 


\ignore{
\subsection{Lookup table for MOSFET sizing}
\label{sec:LUT}

\noindent
As seen in Fig.~\ref{fig:DP-SFG_ex}, the edge weights in a DP-SFG include circuit parameters such as the transistor transconductance, $g_m$, and various capacitances in the circuit.  The circuit may be optimized to find values of these parameters that meet specifications, but ultimately these must be translated into physical transistor parameters such as the transistor width.   In older technologies, the square-law model for MOS transistors could be used to perform a translation between DP-SFG parameters and transistor widths, but square-law behavior is inadequate for capturing the complexities of modern MOS transistor models.
In this work, we use a precomputed lookup table (LUT) that rapidly performs the mapping to device sizes while incorporating the complexities of advanced MOS models.

\begin{figure}[htbp]
\vspace{-0.4cm}
\centering
\includegraphics[height=4cm]{fig/lut_fig_1.pdf}
\vspace{-0.55cm}
\caption{LUT generation using three DOFs, $V_{gs}$, $V_{ds}$ and $L$.}
\label{fig:lutgen}
\vspace{-0.1cm}
\end{figure}

The LUT is indexed by the $V_{gs}$, $V_{ds}$, and length $L$ of the transistor, and provides four outputs: the drain current ($I_d$), transconductance ($g_m$), source-drain conductance ($g_{ds}$), and drain-source capacitance ($C_{ds}$).
The entries of the LUT are computed by performing a nested DC sweep simulation across the three input indices for the MOSFET with a specific reference width, $W_{ref}$, as shown in Fig.~\ref{fig:lutgen}, and for each input combination, the four outputs are recorded.
\blueHL{Empirically, we see that the impact of $V_{sb}$ is small enough that it can be neglected, and therefore we set $V_{sb} = 0$ in the sweeps used to create the LUT.}

Our methodology uses this LUT, together with the $g_m/I_d$ methodology~\cite{silviera_96}, to translate circuit parameters predicted by the transformer to transistor widths. The cornerstone of this methodology relies on the inherent width independence of the ratio $g_m/I_d$ to estimate the unknown device width: this makes it feasible to use an LUT characterized for a reference width $W_{ref}$. 
We will elaborate on this procedure further in Section~\ref{sec:precomputedLUTs}, and show how the LUT, together with the $g_m/I_d$ method, can effectively estimate the device widths corresponding to the transformer outputs.
% \redHL{\sout{required to achieve equivalent DC operating characteristics within the circuit. Section III D \redHL{Do not hardcode section numbers!!} provides an in-depth explanation of the implementation details of this methodology.}}
}