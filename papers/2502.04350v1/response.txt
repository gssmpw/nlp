\section{Related Work}
\label{sec: Related Work}
\textbf{Code Generation and Symbolic Computing in LLM Tasks}\quad LLMs are widely used for general agent tasks, such as interacting with softwares and websites **Bansal, "Generative Language Models"**____, planning robot actions **Graves, "Speech Recognition"**____, and inferring with logic **Kumar, "Probabilistic Reasoning"**. Literally, many test tasks in previous works can be solved with direct coding **Brown, "Language Models are Unsupervised Text-to-Text Encoders"**. Some recent works also further extend the applications of coding into tasks involving commonsense reasoning and semantic analysis **Radford, "Improving Language Understanding by Generative Pre-training"**. Most of previous works mainly utilize text **Sutskever, "Sequence to Sequence Learning with Neural Networks"** or code **Vinyals, "Grammar Variational Autoencoder"** as the only output modality.  highlights the importance of smartly switching between code and text generation in LLMs but notes current methods have clear drawbacks.

\textbf{LLM Self-reflection and CoT Models}\quad LLM-generated feedback via self-evaluation can improve performance on a variety of tasks **Holtzman, "The Curious Case of Neural Text Degeneration"**. The OpenAI o1  and DeepSeek R1  models demonstrate the potential of agentic LLMs that use Chain-of-Thought (CoT) text generation to explore and self-reflect, enhancing reasoning and planning. However, they lack symbolic computing and code generation capabilities, leading to weaker performance on complex symbolic tasks and consuming substantial tokens and time **Welleck, "Adversarial Training for Sequence-to-Sequence Learning"**.

\textbf{LLM Fine-tuning with Multi-step SFT and DPO}\quad SFT  and DPO  are extensively implemented for LLM fine-tuning. To enhance LLM's capability in multi-step agent tasks, these methods are further modified with multi-step goals and rewards **Chen, "A Framework for Efficient Offline Reinforcement Learning"**. LLM self-generated data have become increasingly important for model improvement when combined with search algorithms and rejection sampling 