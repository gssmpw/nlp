\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{hyperref}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts, amsthm}
\usepackage{bbding}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{units}
\usepackage{booktabs}
\usepackage{array}
\usepackage{makecell}
\usepackage{xcolor}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Detecting OOD Samples via Optimal Transport Scoring Function
% \thanks{Identify applicable funding agency here. If none, delete this.}
% *\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{
% Anonymous submission
\IEEEauthorblockN{
% 1\textsuperscript{st} 
Heng Gao}
\IEEEauthorblockA{\textit{ISTBI} \\
\textit{Fudan University}\\
Shanghai, China \\
hgao22@m.fudan.edu.cn
}
\and
\IEEEauthorblockN{
% 2\textsuperscript{nd} 
Zhuolin He
}
\IEEEauthorblockA{\textit{School of Computer Science} \\
\textit{Fudan University}\\
Shanghai, China \\
zlhe22@m.fudan.edu.cn}
\and
\IEEEauthorblockN{
% 3\textsuperscript{rd} 
Jian Pu$^*$\thanks{*Corresponding author.}}
\IEEEauthorblockA{\textit{ISTBI} \\
\textit{Fudan University}\\
Shanghai, China \\
jianpu@fudan.edu.cn}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}

\maketitle

\begin{abstract}
To deploy machine learning models in the real world, researchers have proposed many OOD detection algorithms to help models identify unknown samples during the inference phase and prevent them from making untrustworthy predictions. Unlike methods that rely on extra data for outlier exposure training, post hoc methods detect Out-of-Distribution (OOD) samples by developing scoring functions, which are model agnostic and do not require additional training. However, previous post hoc methods may fail to capture the geometric cues embedded in network representations. Thus, in this study, we propose a novel score function based on the optimal transport theory, named OTOD, for OOD detection. We utilize information from features, logits, and the softmax probability space to calculate the OOD score for each test sample. Our experiments show that combining this information can boost the performance of OTOD with a certain margin. Experiments on the CIFAR-10 and CIFAR-100 benchmarks demonstrate the superior performance of our method. Notably, OTOD outperforms the state-of-the-art method GEN by $7.19\%$ in the mean FPR@95 on the CIFAR-10 benchmark using ResNet-18 as the backbone, and by $12.51\%$ in the mean FPR@95 using WideResNet-28 as the backbone. In addition, we provide theoretical guarantees for OTOD. The code is available in \url{https://github.com/HengGao12/OTOD}.
\end{abstract}

\begin{IEEEkeywords}
Out-of-distribution detection, Wasserstein distances, Deep neural networks, Machine learning safety
\end{IEEEkeywords}

\section{Introduction}
To prevent machine learning models from producing untrustworthy predictions and being overconfident on Out-of-Distribution (OOD) inputs \cite{b1}, various OOD detection algorithms have been proposed to enable models to distinguish whether a test sample comes from in-distribution (ID) data or unseen domains. 

The post hoc-based methods identify OOD samples by developing scoring functions, which are free of training and model agnostic. In MSP \cite{b2}, Hendrycks et al. first propose a strong baseline that utilizes the statistics of softmax probability outputs to identify outliers. ODIN \cite{b10} proposes detecting OOD samples by adding input perturbation and temperature scaling. In EBO \cite{b3}, the authors develop an energy-based score function that uses information from the model's logits outputs, surpassing many softmax-based scores and generative-based methods. In recent work, GEN \cite{b4} proposes using generalized entropy to calculate the OOD scores of test samples. However, limited attention is paid to study how to develop a score function based on optimal transport distances, which possess good theoretical properties \cite{b7} and are capable of capturing the spatial discrepancies between probability distributions \cite{b23}. 


\begin{figure}[t]
\centerline{\includegraphics[width=0.92\linewidth]{figures/Figure1.png}}
\caption{The AUROC (in percentage) of four OOD detection methods using ResNet-18 \cite{b5} trained on CIFAR-10 \cite{b6}. The OOD datasets are Tiny ImageNet (TIN) \cite{b11} and Texture \cite{b14}. Methods marked with blue $\lozenge$ use the softmax probability input; methods marked with orange $\circ$ use the logits inputs. Our proposed method OTOD (marked with purple $\star$) uses information from logits, softmax probability, and features.}
\label{fig-1}
\end{figure}

 Therefore, in this paper, we develop a post hoc method that leverages the optimal transport distance \cite{b7}, also known as Wasserstein distance, to calculate OOD scores without any training techniques or modifying the network architecture. Specifically, we first employ Wasserstein-1 distance to calculate OOD scores using features extracted from the penultimate layer of the pre-trained neural network. Unlike methods that utilize statistical distances, such as MDS \cite{b8} and KLM \cite{b9}, OTOD does not require estimating the empirical class mean and covariance for the ID data during the OOD testing phase. Then, we combine information from the logits and the softmax probability space to calculate OOD scores. We find that integrating this information helps to improve the OOD detection results. Moreover, we apply the temperature scaling technique \cite{b10} to further enhance the OOD detection performance. The experiments on the CIFAR-10 and CIFAR-100 \cite{b6} benchmarks demonstrate the superior performance of OTOD, which outperforms many state-of-the-art post hoc methods. Furthermore, we provide theoretical guarantees for OTOD.

The main contributions of our paper are: (1) We develop a novel post hoc method based on optimal transport theory, named OTOD, to calculate OOD scores of test samples without using any in-distribution data. (2) We find that the combination of features, logits, and softmax probability inputs allows OTOD to achieve non-trivial improvements in several commonly used metrics (the FPR@95 and AUROC) on the CIFAR-10/100 \cite{b6} benchmarks. 
(3) Both empirical and theoretical results demonstrate the superior performance of OTOD.
% This document is a model and instructions for \LaTeX.
% Please observe the conference page limits. 


\section{Background}  
% In this section, we briefly introduce the definition of OOD detection and Wasserstein distances.

% \subsection{Out-of-Distribution Detection}
% Consider a standard in-distribution dataset denoted by $\mathcal{D}=\left\{(\mathbf{x}_i, y_i)\right\}_{i=1}^N,\quad y_i\in\mathcal{Y}=\left\{1, \cdots, K\right\}$ drawn \textit{i.i.d.} from the joint data distribution $P^{in}(\mathbf{x})$, herein $N$ is the number of training samples, $K$ is the number of classes. To detect OOD samples in real-world deployment, we usually add an OOD detector on the top of a classifier which is denoted by $h\left(\cdot\right)$. During the test phase, given a test sample $\mathbf{x}$, the purpose of OOD detection is to define a decision function which can be written as 
% \begin{equation*}
%     \mathcal{G}_{\gamma}\left(\mathbf{x}\right) = \left\{\begin{array}{cc}
%       \text{ID},   & S\left(h; \mathbf{x}\right) \geq \gamma,\\
%       \text{OOD},   & S\left(h; \mathbf{x}\right) < \gamma,
%     \end{array}\right.
% \end{equation*}
% where $\gamma$ is the threshold, $S(\cdot)$ is the desired scoring function.

\subsection{Wasserstein Distances}
Now we introduce the definition of Wasserstein distances.
% Intuitively, it quantifies the difficulty of transporting goods between consumers and producers whose corresponding spatial distributions are modeled by probability measures. 

\begin{definition}[Wasserstein distances \cite{b7}]\label{w1-distance}
Denote $\left(\mathbf{X}, d\right)$ as a Polish metric space and let $p\in\left[1, \infty\right)$. For any two probability measures $\mu, \nu$ on $\mathbf{X}$, the Wasserstein distance of order $p$ between $\mu$ and $\nu$ is defined as follows:
\begin{equation*}
\mathcal{W}_{p}(\mu, \nu) =\left(\inf _{\pi \in \Pi(\mu, \nu)} \int_{\mathbf{X}} d(x, y)^{p} d \pi(x, y)\right)^{1 / p},
\end{equation*}
where $\Pi(\mu, \nu)$ is the set of all joint probability measures on $\mathbf{X}\times\mathbf{X}$ whose marginals are $\mu$ and $\nu$.
\end{definition}
 
 
Particularly, let $p=1$, we can obtain the $\mathcal{W}_1$ distance, also known as Kantorovichâ€“Rubinstein distance, which can be written in the following form, 
\begin{equation*}
    \mathcal{W}_{1}(\mu, \nu)=\sup _{\|\psi\|_{\operatorname{L}} \leq 1}\left\{\int_{\mathbf{X}} \psi d \mu-\int_{\mathbf{X}} \psi d \nu\right\},
\end{equation*}
where $\psi\in L^1(\mu)$ is c-convex and 1-Lipschitz, $\vert\vert\cdot\vert\vert_{\operatorname{L}}$ denotes the Lipschitz norm of function $\psi$.
% this equation holds on any separable metric space.
% \begin{remark}[Lipschitz norm \cite{b22}]
%   The Lipschitz norm of function $\psi$ can be written as
%   \begin{equation*}
%     \|\psi\|_{\mathrm{L}}=\sup_{\mathbf{x}\neq \mathbf{y}}\frac{|\psi(\mathbf{x})-\psi(\mathbf{y})|}{\|\mathbf{x}-\mathbf{y}\|},\quad \mathbf{x}, \mathbf{y}\in \mathbf{X}.
% \end{equation*}
% \end{remark}

% \subsection{Proper Scoring Rules}
% Denote $\mathbf{x}\in\mathbf{X}$ as the test sample given in the inference phase, $p$ the prediction of classifier $f\left(\mathbf{x}\right)=h\left(g\left(\mathbf{x}\right)\right)$, wherein $h\left(\cdot\right)$ is the classification head, and $g\left(\cdot\right)$ is the feature extractor which outputs the features in the penultimate layer of the model. The scoring rules are mostly real-valued functions. In , the authors defines the expected score under $q$ when the probabilistic prediction is $p$ as the following form
% \begin{equation*}
%     S\left(p, q\right) = \int S\left(p, w\right)dq\left(w\right).
% \end{equation*}

% For simplicity, we write the score as $S\left(\mathbf{x}\right)$. In practical use, we usually leverage logits or the penultimate layer feature to calculate the score. Thus, for a given sample $\mathbf{x}\in \mathbf{X}$, $S\left(\cdot\right)$ is a function of $h\left(g\left(\mathbf{x}\right)\right)$ and $g\left(\mathbf{x}\right)$.

\section{Optimal Transport Scoring Function}

The main idea of our method is to develop a score function based on Optimal Transport theory \cite{b7} to calculate the OOD scores for test samples. Our method consists of two parts: (\romannumeral 1) Wasserstein-1 score calculation (in Section \ref{ot-cal}); (\romannumeral 2) multi-scale information integration (in Section \ref{ms-ii}). We also provide theoretical guarantees for OTOD partially (in Section \ref{theory}).

\subsection{Optimal Transport Score Calculation}\label{ot-cal}
For each input image $\mathbf{x}\in \mathbf{X}$, $\mathbf{X}$ is the input space, the feature of the given sample is denoted by $\mathbf{f}\in\mathbb{R}^{d}$. 
Then, we normalize feature $\mathbf{f}$ by using $L^2$ normalization. Denote the normalized feature as 
$
    \hat{\mathbf{f}} = \nicefrac{\mathbf{f}}{\vert\vert \mathbf{f}\vert\vert_2}
$.
In this case, $\hat{\mathbf{f}}$ can be approximated as a d-dimensional distribution.

Afterwards, we use $\mathcal{W}_1$ distance to calculate the OOD score for each test sample. To be specific, the Wasserstein-1 OOD score for sample $\mathbf{x}$ takes the form as
\begin{equation}
    \hat{S}_{\mathcal{W}_1}(\hat{\mathbf{f}}) = -\min\left[\mathcal{W}_1 (\hat{\mathbf{f}}, \mathbf{u}),  \mathcal{W}_1 (\hat{\mathbf{f}}, \mathbf{o})\right], \label{eq-1}
    % &= -\min\left[\sup _{\|h\|_{\operatorname{L}} \leq 1}\left\{\int_{\mathbb{R}^d} h d \hat{\mathbf{f}}-\int_{\mathbb{R}^d} h d \mathbf{o}\right\}, \right. \notag\\
    % & \left.\sup _{\|h\|_{\operatorname{L}} \leq 1}\left\{\int_{\mathbb{R}^d} h d \hat{\mathbf{f}}-\int_{\mathbb{R}^d} h d \mathbf{u}\right\} \right], \label{eq-2}
\end{equation}
where 
% $h$ is a parameterized neural network satisfying the 1-Lipschitz constraint, i.e., $\vert\vert h\vert\vert_L\leq 1$. What's more, 
$\mathbf{u}$ is a mean vector, $\mathbf{o}$ is a zero vector. Note that, here we have a model ensemble effect by using both the mean vector and the zero vector to calculate the Wasserstein-1 score.

\subsection{Multi-scale Information Integration}\label{ms-ii}
To enhance the performance of OTOD, we further integrate the information contained in logits and softmax probability. Specifically, denote the logits of the given image sample as $\mathbf{l}\in\mathbb{R}^K$, $K$ is the number of classes. Denote the softmax probability as $\mathbf{p}\in[0, 1]^K$. Then, we also normalize vector $\mathbf{l}, \mathbf{p}$ and perform the same calculation as equation (\ref{eq-1}). Likewise, we denote the normalized logits and softmax probability as $\hat{\mathbf{l}}, \hat{\mathbf{p}}$. Therefore, the whole Wasserstein-1 score can be written as
\begin{equation}
    \Tilde{S}_{\mathcal{W}_1}(\hat{\mathbf{f}}, \hat{\mathbf{l}}, \hat{\mathbf{p}}) = \alpha_1\hat{S}_{\mathcal{W}_1}(\hat{\mathbf{f}}) + \alpha_2\hat{S}_{\mathcal{W}_1}(\hat{\mathbf{l}}) + \alpha_3\hat{S}_{\mathcal{W}_1}(\hat{\mathbf{p}}), \label{eq-3}
\end{equation}
where
\begin{equation}
    \sum\limits_{i=1}^3 \alpha_i = 1, \alpha_i\in [0,1]. \label{eq-4}
\end{equation}

% \subsection{Temperature Scaling}\label{ts}


Moreover, we adapt the temperature scaling technique from ODIN \cite{b10} to OTOD to further enlarge the discrepancy between ID and OOD softmax probability. Hence, the final score takes the form of the following:
\begin{equation}
    S_{\mathcal{W}_1}(\hat{\mathbf{f}}, \hat{\mathbf{l}}, \Tilde{\mathbf{p}}) = \alpha_1\hat{S}_{\mathcal{W}_1}(\hat{\mathbf{f}}) + \alpha_2\hat{S}_{\mathcal{W}_1}(\hat{\mathbf{l}}) + \alpha_3\hat{S}_{\mathcal{W}_1}\left(\Tilde{\mathbf{p}}\right),
    % &= -\alpha_1 \min\left[\mathcal{W}_1 (\mathbf{f}, \mathbf{o}),  \mathcal{W}_1 (\mathbf{f}, \mathbf{u})\right] \\
    % &- \alpha_2 \min\left[\mathcal{W}_1 (\mathbf{l}, \mathbf{o}),  \mathcal{W}_1 (\mathbf{l}, \mathbf{u})\right] \\
    % &- \alpha_3 \min\left[\mathcal{W}_1 (\mathbf{p}, \mathbf{o}),  \mathcal{W}_1 (\mathbf{p}, \mathbf{u})\right].
\end{equation}
where $\alpha_1, \alpha_2, \alpha_3$ satisfy equation (\ref{eq-4}), $\Tilde{\mathbf{p}}$ is the normalization of $\text{Softmax}(\nicefrac{\mathbf{l}}{T})$, $T$ is the temperature.

% We use this temperature scaling method as a trick to boost the performance of OTOD.

% To sum up, the entire OOD detection decision function using OTOD as scoring function can be written as 
% \begin{equation*}
%     \mathcal{G}_{\gamma}\left(\mathbf{x}\right) = \left\{\begin{array}{cc}
%       \text{ID},   & S_{\mathcal{W}_1}\left(h; \mathbf{x}\right) \geq \gamma,\\
%       \text{OOD},   & S_{\mathcal{W}_1}\left(h; \mathbf{x}\right) < \gamma,
%     \end{array}\right.
% \end{equation*}
% where $\gamma$ is the threshold.

\begin{table*}[t]

    \centering
    \caption{Evaluation on the CIFAR-10~\cite{b3} benchmark using ResNet-18 \cite{b5} and WideResNet-28 \cite{b16} as the backbone. 
    % $\uparrow$ indicates the higher the value, the better the OOD performance and vice versa. The bold numbers represent the best results.
    }
    \label{tab-1}
    \resizebox{\textwidth}{!}
{
    \begin{tabular}{ccccccccccccccccc}
    \toprule[1.5pt]
    \multirow{3}{*}{\textbf{Model}}&\multirow{3}{*}{\textbf{Methods}} & \multicolumn{14}{c}{\textbf{OOD Datasets}} & \multirow{4}{*}{\textbf{ID ACC}} \\ 
        % ~&~ & \multicolumn{4}{c}{\textbf{Near-OOD}} & \multicolumn{8}{c}{\textbf{Far-OOD}} & ~ \\ 
        ~&~& \multicolumn{2}{c}{\textbf{CIFAR-100}} & \multicolumn{2}{c}{\textbf{TIN}} & \multicolumn{2}{c}{\textbf{MNIST}} & \multicolumn{2}{c}{\textbf{SVHN}} & \multicolumn{2}{c}{\textbf{Texture}} & \multicolumn{2}{c}{\textbf{Place365}} & \multicolumn{2}{c}{\textbf{Average}} & ~ \\
         ~&~ & \textbf{FPR@95 $\downarrow$} & \textbf{AUROC $\uparrow$} & \textbf{FPR@95 $\downarrow$} & \textbf{AUROC $\uparrow$} & \textbf{FPR@95 $\downarrow$} & \textbf{AUROC $\uparrow$} & \textbf{FPR@95 $\downarrow$} & \textbf{AUROC $\uparrow$} & \textbf{FPR@95 $\downarrow$} & \textbf{AUROC $\uparrow$} & \textbf{FPR@95 $\downarrow$} & \textbf{AUROC $\uparrow$} & \textbf{FPR@95 $\downarrow$} & \textbf{AUROC $\uparrow$} & ~\\
        \midrule
        % ResNet-18& ~& ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\\
        % \midrule
        % ~ & \multicolumn{13}{c}{Without Distillation} \\ 
        % MSP & 58.66 & 78.88 & 64.27 & 78.77 & 48.37 & 80.25 & 46.91 & 84.38 & 80.94 & 68.70 & 49.98 & 83.54 & 58.19 & 79.09 & 76.22 \\ 
        % ODIN & 61.22 & 77.85 & 44.72 & 88.42 & 35.20 & 88.01 & 54.39 & 80.81 & 64.84 & 81.39 & 30.01 & 92.54 & 48.40 & 84.84 & 76.22 \\ 
        % \multirow{6}{*}{\rotatebox[origin=c]{90}{CIFAR-10 }}&
        \multirow{8}{*}{\rotatebox[origin=c]{90}{ResNet-18 \cite{b5}}}&MSP \cite{b2}& 59.82 & 86.73 & 47.33 & 88.64 & 
        19.22 & 93.95 & \textbf{23.82} & 91.68 & 40.20 & 89.13 & 41.67 & 89.35 & 38.68 & 89.91 & 95.22 \\
        ~&ODIN \cite{b10} & 84.87 & 79.55 & 84.59 & 81.18 & 
        \textbf{15.39} & 
        \textbf{96.60} & 69.04 & 84.62 & 83.19 & 
        83.86 & 76.62 & 83.87 & 68.95 & 84.95 & 95.22 \\
        ~&MDS \cite{b8} & 95.83 & 49.85 & 94.68 & 49.60 & 
        65.67 & 
        66.86 & 44.56 & 84.38 & 98.37 & 
        47.82 & 91.02 & 62.76 & 81.69 & 60.21 & 95.22 \\
        ~&EBO \cite{b3} & 72.69 & 85.55 & 63.68 & 88.35 & 15.49 & 96.32 & 29.34 & \textbf{92.60} & 60.43 & 88.63 & 56.38 & 89.63 & 49.67 & 90.18 & 95.22 \\  
        ~&Gram \cite{b19} & 95.83 & 49.85 & 94.68 & 49.60 & 65.67 & 66.86 & 44.56 & 84.38 & 98.37 & 47.82 & 91.02 & 62.76 & 81.69 & 60.21 & 95.22 \\ 
        ~&KLM \cite{b9} & 92.23 & 77.48 & 81.97 & 80.02 & 68.80 & 87.31 & 69.12 & 83.54 & 70.66 & 82.21 & 96.72 & 78.14 & 79.92 & 81.45 & 95.22 \\  
        ~&GEN \cite{b4} & 63.63 & 86.71 & 52.09 & 88.89 & 18.01 & 95.00 & 
        24.08 & 92.18 & 46.12 & 89.36 & 
        46.12 & 
        \textbf{89.74} & 41.68 & 90.31 &
        95.22 \\ 
        ~& OTOD (Ours) & \textbf{45.19} & \textbf{87.75} & \textbf{40.01} & \textbf{89.32} & 23.64 & 93.97 & 24.34 & 92.35 & \textbf{33.41} & \textbf{90.15} & \textbf{40.36} & 88.83 & \textbf{34.49} & \textbf{90.40} & \textbf{95.22} \\
        %         \multirow{6}{*}{\rotatebox[origin=c]{90}{CIFAR-10}}& MSP \cite{b1} & 59.12 & 78.54 & 49.98 & 82.22 & 
        % 63.47 & 73.54 & 55.44 & 79.37 & 61.24 & 78.07 & 55.40 & 79.61 & 57.44 & 78.56 & 77.17 \\
        % ~&ODIN \cite{b2} & 60.98 & 78.03 & 55.86 & 81.53 & \textbf{50.36} & \textbf{82.09} & 62.16 & 75.56 & 59.83 & \textbf{80.46} & 58.51 & 79.80 & 57.95 & 79.58 & 77.17 \\
        % ~&EBO & 58.88 & 79.01 & 52.53 & 82.51 & 57.57 & 77.30 & 50.84 & \textbf{82.67} & 60.21 & 79.33 & 56.47 & 79.81 & 56.08 & \textbf{80.11} & 77.17 \\  
        % ~&KLM & 88.62 & 74.18 & 70.62 & 79.57 & 63.70 & 71.94 & 58.02 & 80.07 & 75.50 & 76.32 & 83.52 & 76.04 & 73.33 & 76.35 & 77.17 \\  
        % ~&GEN & \textbf{58.56} & \textbf{79.39} & \textbf{49.44} & \textbf{83.28} & 61.42 & 75.66 & 53.68 & 81.53 & 60.78 & 79.53 & \textbf{55.03} & \textbf{80.57} & 56.49 & 79.99 & 77.17 \\ 
        % ~&OTOD (Ours) & 59.38 & 78.91 & 50.39 & 83.04 & 56.96 & 76.75 & \textbf{50.47} & 81.99 & \textbf{58.38} & 79.36 & 55.24 & 80.08 & \textbf{55.14} & 80.02 & 77.17 \\
        \midrule
        % WideResNet-28 & ~& ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\\
        % \midrule
        \multirow{8}{*}{\rotatebox[origin=c]{90}{WideResNet-28 \cite{b16}}}&MSP \cite{b2}& 51.90 & 88.56 & 41.00 & 90.34 & 
        16.07 & 95.01 & 17.59 & 93.85 & 51.87 & 89.01 & 47.56 & 89.53 & 37.67 & 91.05 & 95.81 \\
        ~&ODIN \cite{b10}& 84.12 & 80.55 & 81.61 & 82.34 & 
        17.77 & 
        95.91 & 79.61 & 80.08 & 90.88 & 
        81.39 & 82.61 & 82.59 & 72.77 & 83.81 & 95.81 \\
        ~&MDS \cite{b8}& 53.99 & 85.93 & 44.63 & 88.09 & 
        37.77 & 
        87.47 & \textbf{9.60} & \textbf{97.77} & \textbf{12.10} & 
        \textbf{97.71} & 49.28 & 86.77 & 34.56 & 90.62 & 95.76 \\
        ~&EBO \cite{b3}& 65.27 & 88.10 & 53.56 & 90.55 & \textbf{11.58} & \textbf{97.38} & 18.61 & 94.98 & 70.03 & 88.28 & 60.78 & 89.85 & 46.64 & 91.52 & 95.81 \\  
        ~&Gram \cite{b19}& 81.97 & 67.56 & 69.63 & 76.80 & 51.00 & 81.07 & 24.22 & 95.26 & 94.64 & 73.89 & 75.09 & 74.31 & 66.09 & 78.15 & 95.81 \\ 
        ~&KLM \cite{b9}& 80.71 & 79.32 & 82.76 & 81.28 & 74.03 & 87.59 & 67.28 & 86.82 & 88.42 & 82.08 & 80.38 & 79.66 & 78.93 & 82.79 & 95.81 \\  
        ~&GEN \cite{b4}& 61.88 & 88.44 & 50.94 & 90.71 & 12.73 & 97.06 & 
        17.88 & 94.95 & 65.86 & 88.76 & 
        57.34 & 
        89.98 & 44.44 & \textbf{91.65} &
        95.81 \\ 
        ~&OTOD (Ours) & \textbf{40.46} & \textbf{89.52} & \textbf{33.56} & \textbf{90.98} & 22.47 & 94.14 & 19.37 & 94.38 & 37.72 & 90.04 & \textbf{38.00} & \textbf{90.14} & \textbf{31.93} & 91.53 & \textbf{95.81} \\
        \bottomrule[1.5pt]
    \end{tabular}
}
    
\end{table*}


\begin{table*}[t]

    \centering
    \caption{Evaluation on the CIFAR-100~\cite{b3} benchmark using ResNet-18 \cite{b5} and WideResNet-28 \cite{b16} as the backbone.     
    % $\uparrow$ indicates the higher the value, the better the OOD performance and vice versa. The bold numbers represent the best results.
    }
    \label{tab-2}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccccccccccccccc}
    \toprule[1.5pt]
        \multirow{3}{*}{\textbf{Model}}&
        \multirow{3}{*}{\textbf{Methods}} & \multicolumn{14}{c}{\textbf{OOD Datasets}} & \multirow{4}{*}{\textbf{ID ACC}} \\ 
        % ~&~ & \multicolumn{4}{c}{\textbf{Near-OOD}} & \multicolumn{8}{c}{\textbf{Far-OOD}} & ~ \\ 
       ~&~ & \multicolumn{2}{c}{\textbf{CIFAR-10}} & \multicolumn{2}{c}{\textbf{TIN}} & \multicolumn{2}{c}{\textbf{MNIST}} & \multicolumn{2}{c}{\textbf{SVHN}} & \multicolumn{2}{c}{\textbf{Texture}} & \multicolumn{2}{c}{\textbf{Place365}} & \multicolumn{2}{c}{\textbf{Average}} & ~ \\
         ~&~ & \textbf{FPR@95 $\downarrow$} & \textbf{AUROC $\uparrow$} & \textbf{FPR@95 $\downarrow$} & \textbf{AUROC $\uparrow$} & \textbf{FPR@95 $\downarrow$} & \textbf{AUROC $\uparrow$} & \textbf{FPR@95 $\downarrow$} & \textbf{AUROC $\uparrow$} & \textbf{FPR@95 $\downarrow$} & \textbf{AUROC $\uparrow$} & \textbf{FPR@95 $\downarrow$} & \textbf{AUROC $\uparrow$} & \textbf{FPR@95 $\downarrow$} & \textbf{AUROC $\uparrow$} & ~\\
        \midrule
        % ResNet-18& ~& ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\\
        % \midrule
        % ~ & \multicolumn{13}{c}{Without Distillation} \\ 
        % MSP & 58.66 & 78.88 & 64.27 & 78.77 & 48.37 & 80.25 & 46.91 & 84.38 & 80.94 & 68.70 & 49.98 & 83.54 & 58.19 & 79.09 & 76.22 \\ 
        % ODIN & 61.22 & 77.85 & 44.72 & 88.42 & 35.20 & 88.01 & 54.39 & 80.81 & 64.84 & 81.39 & 30.01 & 92.54 & 48.40 & 84.84 & 76.22 \\ 
        \multirow{8}{*}{\rotatebox[origin=c]{90}{ResNet-18 \cite{b5}}}&MSP \cite{b2} & 59.12 & 78.54 & 49.98 & 82.22 & 
        63.47 & 73.54 & 55.44 & 79.37 & 61.24 & 78.07 & 55.40 & 79.61 & 57.44 & 78.56 & 77.17 \\
        ~&ODIN \cite{b10} & 60.98 & 78.03 & 55.86 & 81.53 & 50.36 & 82.09 & 62.16 & 75.56 & 59.83 & \textbf{80.46} & 58.51 & 79.80 & 57.95 & 79.58 & 77.17 \\
        ~&MDS \cite{b8} & 89.97 & 52.50 & 81.37 & 58.72 & 
        72.16 & 
        63.88 & 71.10 & 67.55 & 76.00 & 
        73.38 & 83.97 & 58.60 & 79.10 & 62.44 & \textbf{77.33} \\
        ~&EBO \cite{b3} & 58.88 & 79.01 & 52.53 & 82.51 & 57.57 & 77.30 & 50.84 & 82.67 & 60.21 & 79.33 & 56.47 & 79.81 & 56.08 & \textbf{80.11} & 77.17 \\
        ~&Gram \cite{b19} & 91.74 & 50.38 & 91.96 & 51.16 & \textbf{45.17} & \textbf{85.82} & \textbf{22.50} & \textbf{95.22} & 90.30 & 69.27 & 93.69 & 45.01 & 72.56 & 66.14 & 77.17 \\
        ~&KLM \cite{b9} & 88.62 & 74.18 & 70.62 & 79.57 & 63.70 & 71.94 & 58.02 & 80.07 & 75.50 & 76.32 & 83.52 & 76.04 & 73.33 & 76.35 & 77.17 \\  
        ~&GEN \cite{b4} & \textbf{58.56} & \textbf{79.39} & \textbf{49.44} & \textbf{83.28} & 61.42 & 75.66 & 53.68 & 81.53 & 60.78 & 79.53 & \textbf{55.03} & \textbf{80.57} & 56.49 & 79.99 & 77.17 \\ 
        ~&OTOD (Ours) & 59.38 & 78.91 & 50.39 & 83.04 & 56.96 & 76.75 & 50.47 & 81.99 & \textbf{58.38} & 79.36 & 55.24 & 80.08 & \textbf{55.14} & 80.02 & 77.17 \\
        \midrule
        % WideResNet-28 & ~& ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~\\
        % \midrule
        \multirow{8}{*}{\rotatebox[origin=c]{90}{WideResNet-28 \cite{b16}}}&MSP \cite{b2} & \textbf{55.70} & 80.45 & 49.58 & 83.01 & 
        50.50 & 79.61 & 54.58 & 81.61 & 66.18 & 75.86 & 57.49 & 79.69 & 55.67 & 80.04 & 80.07 \\  % All checked
        ~&ODIN \cite{b10} & 59.11 & 79.62 & 57.56 & 81.31 & 47.28 & \textbf{83.15} & 72.03 & 72.57 & 66.23 & 77.88 & 64.07 & 79.24 & 61.05 & 78.96 & 80.07 \\
        ~&MDS \cite{b8} & 74.32 & 75.04 & 55.47 & 81.83 & 
        71.04 & 
        70.24 & 34.13 & 90.85 & \textbf{38.97} & 
        91.07 & 65.11 & 78.46 & 56.51 & 81.25 & \textbf{80.33} \\
        ~&EBO \cite{b3} & 57.16 & 81.33 & 51.16 & 83.80 & 
        \textbf{44.73} & 84.08 & 53.44 & 83.39 & 66.97 & 76.46 & 60.92 & 79.70 & 55.73 & \textbf{81.46} & 80.07  \\
        ~&Gram \cite{b19} & 87.34 & 64.90 & 66.87 & 75.43 & 56.29 & 80.55 & \textbf{21.70} & \textbf{93.95} & 76.40 & 80.25 & 83.60 & 64.05 & 65.37 & 76.52 & 80.07 \\
        ~&KLM \cite{b9} & 74.16 & 75.76 & 73.19 & 79.61 & 66.40 & 74.41 & 59.34 & 79.82 & 83.12 & \textbf{72.78} & 81.53 & 75.51 & 72.96 & 76.32 & 80.07 \\  
        ~&GEN \cite{b4} & 55.82 & 81.33 & 48.73 & 84.29 & 48.78 & 80.45 & 53.16 & 83.40 & 65.98 & 76.97 & \textbf{57.32} & \textbf{80.78} & \textbf{54.97} & 81.20 & 80.07 \\ 
        ~&OTOD (Ours) & 56.67 & \textbf{81.52} & \textbf{48.88} & \textbf{84.49} & 47.21 & 81.43 & 52.99 & 83.06 & 69.52 & 75.68 & 57.70 & 80.47 & 55.50 & 81.11 & 80.07 \\
        %         \multirow{6}{*}{\rotatebox[origin=c]{90}{CIFAR-10}}& MSP \cite{b1} & 59.12 & 78.54 & 49.98 & 82.22 & 
        % 63.47 & 73.54 & 55.44 & 79.37 & 61.24 & 78.07 & 55.40 & 79.61 & 57.44 & 78.56 & 77.17 \\
        % ~&ODIN \cite{b2} & 60.98 & 78.03 & 55.86 & 81.53 & \textbf{50.36} & \textbf{82.09} & 62.16 & 75.56 & 59.83 & \textbf{80.46} & 58.51 & 79.80 & 57.95 & 79.58 & 77.17 \\
        % ~&EBO & 58.88 & 79.01 & 52.53 & 82.51 & 57.57 & 77.30 & 50.84 & \textbf{82.67} & 60.21 & 79.33 & 56.47 & 79.81 & 56.08 & \textbf{80.11} & 77.17 \\  
        % ~&KLM & 88.62 & 74.18 & 70.62 & 79.57 & 63.70 & 71.94 & 58.02 & 80.07 & 75.50 & 76.32 & 83.52 & 76.04 & 73.33 & 76.35 & 77.17 \\  
        % ~&GEN & \textbf{58.56} & \textbf{79.39} & \textbf{49.44} & \textbf{83.28} & 61.42 & 75.66 & 53.68 & 81.53 & 60.78 & 79.53 & \textbf{55.03} & \textbf{80.57} & 56.49 & 79.99 & 77.17 \\ 
        % ~&OTOD (Ours) & 59.38 & 78.91 & 50.39 & 83.04 & 56.96 & 76.75 & \textbf{50.47} & 81.99 & \textbf{58.38} & 79.36 & 55.24 & 80.08 & \textbf{55.14} & 80.02 & 77.17 \\
        \bottomrule[1.5pt]
    \end{tabular}
    }
\end{table*}


\subsection{Theoretical Guarantee}\label{theory}
In this section, we provide the theoretical guarantee for the feature part of OTOD (the Equation (\ref{eq-1})) approximately. 

To this end, we assume that a class conditional distribution in the model's penultimate layer's feature space follows the multivariate Gaussian distribution \cite{b20}, which is empirically demonstrated in \cite{b8}. Herein, we also verify this fact in Fig. \ref{fig-2} (a). In addition, after the normalization operation, the feature distribution does not change markedly (see Fig. \ref{fig-2}(b)). Therefore, we can still use the multivariate Gaussian assumption \cite{b20}, i.e., $\hat{\mathbf{f}}\vert y_i\sim \mathcal{N}(\boldsymbol{\mu}_i, \Sigma),$ where $\boldsymbol{\mu}_i$ is the mean of class $y_i, i=1,\cdots, K$, $\Sigma$ is the covariance matrix. 

Then, we define the Mean Discrepancy (MD) metric of a given score function, which is a concept borrowed from Maximum Mean Discrepancy (MMD) \cite{b21} that quantifies how well OTOD discerns ID samples from the OOD ones. The definition can be stated as follows:

\begin{definition}[Mean Discrepancy]
Denote $S: \Tilde{X}\rightarrow \mathbb{R}$ as a score function, where $\Tilde{X}$ is the model's penultimate layer's feature space. Then, let $P^{in}_{\Tilde{X}}$ be the in-distribution marginal probability distribution on $\Tilde{X}$, $P^{ood}_{\Tilde{X}}$ be the OOD distribution. Therefore, the Mean Discrepancy (MD) for score function $S$ can be written as 
\begin{equation*}
    \mathbb{E}_{\mathbf{x}\sim P^{in}_{\Tilde{X}}} \left[S(\mathbf{x})\right] - \mathbb{E}_{\mathbf{x}\sim P^{ood}_{\Tilde{X}}} [S(\mathbf{x})].
\end{equation*}
\end{definition}




Assume that the out-of-distribution data distribution $P_{\Tilde{X}}^{ood}=\mathcal{N}(\boldsymbol{\mu}^{ood}, \Sigma)$ \cite{b20}, herein $\boldsymbol{\mu}^{ood}$ is the mean of OOD data. Then, we can show the following Theorem, which gives the upper bound of Equation (\ref{eq-1})'s MD.

\begin{figure}[t]
\centerline{\includegraphics[width=0.87\linewidth]{figures/UMAP-Visualization-8-31-1.png}}
\caption{UMAP \cite{b22} visualization of unnormalized (a) and normalized (b) feature distribution using WideResNet-28 \cite{b16} on the CIFAR-10 \cite{b3} benchmark.}
\label{fig-2}
\end{figure}

\begin{theorem}[Mean Discrepancy Upper Bound]
    Suppose that the diameter of $\Tilde{X}$ is bounded by $D$, $V(\Tilde{X})$ is the volume of $\Tilde{X}$. Consider the case where $\Tilde{X}$ is the normalized feature space. Then, $\forall 1\leq i\leq K,$ given the setups above, we have the following inequality:
    \begin{align}
        \mathbb{E}_{\hat{\mathbf{f}}\sim P^{in}_{\Tilde{X}}} \left[\hat{S}_{\mathcal{W}_1}(\hat{\mathbf{f}})\right] &- \mathbb{E}_{\hat{\mathbf{f}}\sim P^{ood}_{\Tilde{X}}} \left[\hat{S}_{\mathcal{W}_1}(\hat{\mathbf{f}})\right] \notag\\
        &\leq  2\Tilde{C} D V(\Tilde{X})  \vert\vert \boldsymbol{\mu}_i - \boldsymbol{\mu}^{ood} \vert\vert_{TV}, \label{ineq-1}
    \end{align}
\end{theorem}
where $\vert\vert\cdot\vert\vert_{TV}$ represents the  total variation norm, $\Tilde{C}$ is a constant. 
\begin{proof}
    Denote $\text{MD} = \mathbb{E}_{\hat{\mathbf{f}}\sim P^{in}_{\Tilde{X}}}[\hat{S}_{\mathcal{W}_1}(\hat{\mathbf{f}})] - \mathbb{E}_{\hat{\mathbf{f}}\sim P^{ood}_{\Tilde{X}}}[\hat{S}_{\mathcal{W}_1}(\hat{\mathbf{f}})]$, and the probability density function of $P_{\Tilde{X}}^{in}(\hat{\mathbf{f}})$ and $P_{\Tilde{X}}^{ood}(\hat{\mathbf{f}})$ as $p_{\Tilde{X}}^{in}(\hat{\mathbf{f}}), p_{\Tilde{X}}^{ood}(\hat{\mathbf{f}})$, respectively. Then we have
\begin{equation*}
    \text{MD} \leq \vert \mathbb{E}_{\hat{\mathbf{f}}\sim P^{in}_{\Tilde{X}}}[\hat{S}_{\mathcal{W}_1}(\hat{\mathbf{f}})] - \mathbb{E}_{\hat{\mathbf{f}}\sim P^{ood}_{\Tilde{X}}}[\hat{S}_{\mathcal{W}_1}(\hat{\mathbf{f}})] \vert
\end{equation*}
\begin{align*}
    % &= \vert -\int_{\Tilde{X}} \min\left[\mathcal{W}_{1} (\hat{\mathbf{f}}, \mathbf{o}), \mathcal{W}_{1} (\hat{\mathbf{f}}, \mathbf{u})\right]p^{in}_{\Tilde{X}} (\hat{\mathbf{f}}) d\hat{\mathbf{f}}\\
    % &+ \int_{\Tilde{X}} \min\left[\mathcal{W}_{1} (\hat{\mathbf{f}}, \mathbf{o}), \mathcal{W}_{1} (\hat{\mathbf{f}}, \mathbf{u})\right]p^{ood}_{\Tilde{X}} (\hat{\mathbf{f}}) d\hat{\mathbf{f}}\vert \\ 
    ~&\leq \int_{\Tilde{X}} \mathcal{W}_{1} (\hat{\mathbf{f}}, \mathbf{u})p^{in}_{\Tilde{X}} (\hat{\mathbf{f}}) d\hat{\mathbf{f}} + \int_{\Tilde{X}} \mathcal{W}_{1} (\hat{\mathbf{f}}, \mathbf{u})p^{ood}_{\Tilde{X}} (\hat{\mathbf{f}}) d\hat{\mathbf{f}} \\
    &\leq D \left[\int_{\Tilde{X}} \vert\vert \hat{\mathbf{f}} - \mathbf{u} \vert\vert_{TV}\cdot p^{in}_{\Tilde{X}} (\hat{\mathbf{f}}) d\hat{\mathbf{f}}\right. \\
    &+ \left.\int_{\Tilde{X}} \vert\vert \hat{\mathbf{f}} - \mathbf{u} \vert\vert_{TV}\cdot p^{ood}_{\Tilde{X}} (\hat{\mathbf{f}}) d\hat{\mathbf{f}} \right] \\
    &\leq 2D \int_{\Tilde{X}} \vert\vert \hat{\mathbf{f}} - \mathbf{u} \vert\vert_{TV} d\hat{\mathbf{f}} \\
    &= 2D V(\Tilde{X}) \vert\vert  \hat{\mathbf{f}}^* - \mathbf{u}\vert\vert_{TV} \\
    &\leq 2\Tilde{C} D V(\Tilde{X})\vert\vert \boldsymbol{\mu}_i - \boldsymbol{\mu}^{ood} \vert\vert_{TV},
\end{align*}  
where $\hat{\mathbf{f}}^*\in \Tilde{X}$, $\Tilde{C}$ is a constant.
\end{proof}

Notice that, as $\vert\vert \boldsymbol{\mu}_i-\boldsymbol{\mu}^{ood}\vert\vert_{TV}\rightarrow 0$, the right-hand side of the inequality (\ref{ineq-1}) approaches to 0, which implies that the performance of the feature part of OTOD (the Equation (\ref{eq-1})) decreases as $\vert\vert \boldsymbol{\mu}_i-\boldsymbol{\mu}^{ood}\vert\vert_{TV}$ approaches to 0. In other words, Equation (\ref{eq-1}) can detect pronounced distribution shifts.

\section{Experimental Results}
\subsection{Experimental Setup}
\noindent\textbf{Datasets.}\quad In our research, we utilize two common benchmarks to evaluate our methods: the CIFAR-10 \cite{b3} and CIFAR-100 \cite{b3} benchmarks. In the CIFAR-10 benchmark, the OOD datasets used for evaluation are CIFAR-100 \cite{b3}, TIN \cite{b11}, MNIST \cite{b12}, SVHN \cite{b13}, Texture \cite{b14}, Places365 \cite{b15}. In the CIFAR-100 \cite{b3} benchmark, the OOD datasets used for evaluation are CIFAR-100 \cite{b3}, TIN \cite{b11}, MNIST \cite{b12}, SVHN \cite{b13}, Texture \cite{b14}, Places365 \cite{b15}, respectively.

\noindent\textbf{Evaluation metrics.}\quad We use the following metrics to evaluate the OOD detection performance: (\romannumeral 1) the False Positive Rate (FPR) at $95\%$ True Positive Rate; (\romannumeral 2) the Area Under the Receiver Operating Characteristic curve (AUROC); (\romannumeral 3) the in-distribution classification accuracy (ID ACC).

\noindent\textbf{Implementation details.}\quad  
In the experiments, we use ResNet-18 \cite{b5} and WideResNet-28 \cite{b16} as backbones to perform OOD detection on the aforementioned datasets. To ensure fairness, all post-hoc methods use the same pre-trained model and are evaluated on one NVIDIA V100 GPU with Python 3.8.19, CUDA 11.3 + Pytorch 1.13.1. On the CIFAR-100 benchmark \cite{b3}, the temperature of our method is set to $T=3$ for both ResNet-18 \cite{b5} and WideResNet-28 \cite{b16} architectures. On the CIFAR-10 benchmark \cite{b3}, the temperature of our method is set to $T=10$ for both ResNet-18 \cite{b5} and WideResNet-28 \cite{b16} architectures. For both ResNet-18 \cite{b5} and WideResNet-28 \cite{b16} architectures, $\alpha_1, \alpha_2, \alpha_3$ in equation (\ref{eq-3}) are all fixed to $\frac{1}{3}$ on the CIFAR-10/100 \cite{b3} benchmarks. The resolution of the pre-trained model is $32\times 32$ and the training epoch is set to 100. The learning rate of the pre-trained model is taken as $10^{-1}$ using SGD \cite{b17} as the optimizer. Our code is developed based on OpenOOD \cite{b18,b24}.

\noindent\textbf{Baselines.}\quad We compare OTOD with seven post-hoc OOD detection baselines, including MSP \cite{b2}, ODIN \cite{b10}, MDS \cite{b8}, EBO \cite{b3}, Gram \cite{b19}, KLM \cite{b9}, GEN \cite{b4}. We use OpenOOD \cite{b18,b24} to implement all the aforementioned methods.  For ODIN \cite{b10}, the temperature is set to $T=1000$ following the original work. For EBO \cite{b3}, the temperature $T$ is set to $1$.

\begin{table}[t] 
\begin{center}
\caption{Ablation of the input of OTOD on the CIFAR-100 benchmark using ResNet-18 as the backbone and the temperature $T$ is taken as 3. Here we report the mean value of FPR and AUROC over 6 OOD datasets.} 
\label{tab-ab}
    \resizebox{0.45\textwidth}{!}{
\begin{tabular}{c|ccc|c|c}
  \toprule[1.5pt]
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  % &\makecell[c]{\texttt{Logits} \\ \texttt{Distillation}}
    & \makecell[c]{Features} & \makecell[c]{Logits} & \makecell[c]{Probs} & \textbf{FPR@95}$\downarrow$ & \textbf{AUROC} $\uparrow$
  \\
  \midrule
  % (\romannumeral 1)& \XSolidBrush & \XSolidBrush & \XSolidBrush   & 48.67 & 82.42 & 76.22 \\
  (\romannumeral 1)&  \Checkmark & \XSolidBrush & \XSolidBrush & 62.98 & 74.54 \\
  (\romannumeral 2)&  \Checkmark  & \Checkmark & \XSolidBrush  &  58.68  & 77.98 \\
  % (\romannumeral 4)& \XSolidBrush & \Checkmark & \Checkmark & 51.14 & 81.66 & 75.91 \\
  (\romannumeral 3)&  \Checkmark & \Checkmark & \Checkmark & \textbf{55.14} & \textbf{80.02} \\
  % \midrule

  %  % &  &  &    & 48.67 & 82.42 & 76.22 \\
  % % &\Checkmark &   &  &  &  &  &  \\
  %  % &  \Checkmark &  &  & 50.33 & 82.41 & 76.16 \\
  %  \multirow{3}{*}{GEN \cite{liu2023gen}} &  \Checkmark  & \Checkmark &   &  50.32 & 83.10 & 76.30 \\
  %  &   & \Checkmark & \Checkmark & 51.11 & 81.66 & 75.91\\
  %  & \Checkmark & \Checkmark & \Checkmark & \textbf{46.98$\pm$1.17} & \textbf{85.28$\pm$0.51} & \textbf{77.09$\pm$0.43}\\
  \bottomrule[1.5pt]
\end{tabular}
}
\end{center}
\end{table}

\subsection{OOD Detection Performance Results}
\noindent\textbf{Evaluation on CIFAR-10.}\quad 
The comparison results on the CIFAR-10 \cite{b6} benchmark are given in Table \ref{tab-1}. As shown in the first half of Table \ref{tab-1}, OTOD outperforms all the other scoring functions on various OOD datasets by a considerable margin using ResNet-18 as the backbone. Furthermore, it is noteworthy that OTOD surpasses GEN by $12.71\%$ FPR@95 on Texture. From the second half of Table \ref{tab-1}, we can find that OTOD surpasses GEN by $12.51\%$ on FPR@95 averaged on six OOD datasets taking WideResNet-28 as the backbone. These facts indicate that OTOD works for both ResNet-18 and WideResNet-28 architectures on the CIFAR-10 benchmark.

\noindent\textbf{Evaluation on CIFAR-100.}\quad The results on CIFAR-100 benchmark is given in Table \ref{tab-2}. From the first half of Table \ref{tab-2}, we can find that OTOD achieves the best mean FPR@95 compared to other methods. Moreover, OTOD outperforms KLM by $18.19\%$ FPR@95 and $3.67\%$ AUROC on average. As shown in Table \ref{tab-2}, OTOD also achieves competitive results compared with GEN. In the second half of Table \ref{tab-2}, we can see that OTOD also works well using WideResNet-28 as the backbone. 

\subsection{Ablation Analysis of OTOD Input}
In this section, our purpose is to investigate the effects of each input in OTOD on the CIFAR-100 benchmark taking ResNet-18 as the backbone. By comparing setting (\romannumeral 1), (\romannumeral 2), and (\romannumeral 3) in Table \ref{tab-ab}, we can observe that the combination of features, logits and softmax probability inputs enhances the performance of OTOD scoring function. Note that, the AUROC of setting (\romannumeral 1) is $5.48\%$ lower than the AUROC of setting (\romannumeral 3).



\subsection{The Effect of Hyperparameters}
 In this section, we give a detailed analysis of the temperature $T$ in OTOD on the CIFAR-100 benchmark taking ResNet-18 and WideResNet-28 as backbones. The results are shown in Fig. \ref{fig-3}. From Fig. \ref{fig-3}, we observe that when temperature $T$ is small, increasing $T$ can enhance the OOD detection performance on both ResNet-18 and WideResNet-28 architectures. While $T$ is large, increasing $T$ will harm the OOD detection results on both ResNet-18 and WideResNet-28 architectures.

\begin{figure}[t]
\centerline{\includegraphics[width=0.9\linewidth]{figures/hyperparameter-analysis-final.png}}
\caption{(a)(b) Hyperparameter analysis on temperature $T$ using ResNet-18 and WideResNet-28 as backbones on the CIFAR-100 benchmark.}
\label{fig-3}
\end{figure}

% \subsection{Score Distribution Visualization}



\section{Conclusion}
In this study, we introduce a novel scoring function, named OTOD, for detecting OOD samples. We calculate OOD scores for test samples using Wasserstein-1 distances, which do not require the use of any in-distribution data during the OOD testing phase. Afterward, We leverage information from feature, logits, and softmax probability spaces to achieve better OOD detection performance using our optimal transport-based score function. Moreover, we apply the temperature scaling technique to further improve the detection result. Both theoretical and experimental results demonstrate the superior performance of our score function.


% 
% \section{Ease of Use}

% \subsection{Maintaining the Integrity of the Specifications}

% The IEEEtran class file is used to format your paper and style the text. All margins, 
% column widths, line spaces, and text fonts are prescribed; please do not 
% alter them. You may note peculiarities. For example, the head margin
% measures proportionately more than is customary. This measurement 
% and others are deliberate, using specifications that anticipate your paper 
% as one part of the entire proceedings, and not as an independent document. 
% Please do not revise any of the current designations.

% \section{Prepare Your Paper Before Styling}
% Before you begin to format your paper, first write and save the content as a 
% separate text file. Complete all content and organizational editing before 
% formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
% proofreading, spelling and grammar.

% Keep your text and graphic files separate until after the text has been 
% formatted and styled. Do not number text heads---{\LaTeX} will do that 
% for you.

% \subsection{Abbreviations and Acronyms}\label{AA}
% Define abbreviations and acronyms the first time they are used in the text, 
% even after they have been defined in the abstract. Abbreviations such as 
% IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
% abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}
% \begin{itemize}
% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
% \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
% \end{itemize}

% \subsection{Equations}
% Number equations consecutively. To make your 
% equations more compact, you may use the solidus (~/~), the exp function, or 
% appropriate exponents. Italicize Roman symbols for quantities and variables, 
% but not Greek symbols. Use a long dash rather than a hyphen for a minus 
% sign. Punctuate equations with commas or periods when they are part of a 
% sentence, as in:
% \begin{equation}
% a+b=\gamma\label{eq}
% \end{equation}

% Be sure that the 
% symbols in your equation have been defined before or immediately following 
% the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
% the beginning of a sentence: ``Equation \eqref{eq} is . . .''

% \subsection{\LaTeX-Specific Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is \cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{The class file is designed for, but not limited to, six authors.} A 
% minimum of one author is required for all conference articles. Author names 
% should be listed starting from left to right and then moving down to the 
% next line. This is the author sequence that will be used in future citations 
% and by indexing services. Names should not be listed in columns nor group by 
% affiliation. Please keep your affiliations as succinct as possible (for 
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Figure captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.
% 

\begin{thebibliography}{00}
\bibitem{b1} Nguyen, Anh, Jason Yosinski, and Jeff Clune. \textquotedblleft Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.\textquotedblright~  In \textit{2015 IEEE Conference on Computer Vision and Pattern Recognition}, pp. 427-436. IEEE Computer Society, 2015.
\bibitem{b2} Hendrycks, Dan, and Kevin Gimpel. \textquotedblleft A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks.\textquotedblright~  In \textit{International Conference on Learning Representations}. 2017.
\bibitem{b3} Liu, Weitang, Xiaoyun Wang, John D. Owens, and Yixuan Li. \textquotedblleft Energy-based out-of-distribution detection.\textquotedblright~  In \textit{Proceedings of the 34th International Conference on Neural Information Processing Systems}, pp. 21464-21475. 2020.
\bibitem{b4} Liu, Xixi, Yaroslava Lochman, and Christopher Zach. \textquotedblleft Gen: Pushing the limits of softmax-based out-of-distribution detection.\textquotedblright~  In \textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp. 23946-23955. 2023.
\bibitem{b5} He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. \textquotedblleft Deep residual learning for image recognition.\textquotedblright~  In \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp. 770-778. 2016.
\bibitem{b6} Krizhevsky, Alex. \textquotedblleft Learning Multiple Layers of Features from Tiny Images.\textquotedblright~  (2009). Citeseer.
\bibitem{b7} Villani, CÃ©dric. \textit{Optimal transport: old and new.} Vol. 338. Berlin: springer, 2009.
\bibitem{b8} Lee, Kimin, Kibok Lee, Honglak Lee, and Jinwoo Shin. \textquotedblleft A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\textquotedblright~  In \textit{Proceedings of the 32nd International Conference on Neural Information Processing Systems}, pp. 7167-7177. 2018.
\bibitem{b9} Hendrycks, Dan, Steven Basart, Mantas Mazeika, Andy Zou, Joseph Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. \textquotedblleft Scaling Out-of-Distribution Detection for Real-World Settings.\textquotedblright~  In \textit{International Conference on Machine Learning}, pp. 8759-8773. PMLR, 2022.
% \bibitem{b10} Papyan, Vardan, X. Y. Han, and David L. Donoho. \textquotedblleft Prevalence of neural collapse during the terminal phase of deep learning training.\textquotedblright~  \textit{Proceedings of the National Academy of Sciences} 117, no. 40 (2020): 24652-24663.
\bibitem{b10} Liang, Shiyu, Yixuan Li, and R. Srikant. \textquotedblleft Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks.\textquotedblright~  In \textit{International Conference on Learning Representations}. 2018.
\bibitem{b11} Le, Ya, and Xuan Yang. \textquotedblleft Tiny imagenet visual recognition challenge.\textquotedblright~  CS 231N 7, no. 7 (2015): 3.
\bibitem{b12} Deng, Li. \textquotedblleft The mnist database of handwritten digit images for machine learning research [best of the web].\textquotedblright~  \textit{IEEE signal processing magazine} 29, no. 6 (2012): 141-142.
\bibitem{b13} Netzer, Yuval, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, and Andrew Y. Ng. \textquotedblleft Reading digits in natural images with unsupervised feature learning.\textquotedblright~  In \textit{NIPS workshop on deep learning and unsupervised feature learning}, vol. 2011, no. 2, p. 4. 2011.
\bibitem{b14} Cimpoi, Mircea, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. \textquotedblleft Describing textures in the wild.\textquotedblright~  In \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp. 3606-3613. 2014.
\bibitem{b15} Zhou, Bolei, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. \textquotedblleft Places: A 10 million image database for scene recognition.\textquotedblright~  \textit{IEEE transactions on pattern analysis and machine intelligence} 40, no. 6 (2017): 1452-1464.
\bibitem{b16} Zagoruyko, Sergey, and Nikos Komodakis. \textquotedblleft Wide Residual Networks.\textquotedblright~  In \textit{British Machine Vision Conference} 2016. British Machine Vision Association, 2016.
\bibitem{b17} Robbins, Herbert, and Sutton Monro. \textquotedblleft A stochastic approximation method.\textquotedblright~  The annals of mathematical statistics (1951): 400-407.
\bibitem{b18} Zhang, Jingyang, Jingkang Yang, Pengyun Wang, Haoqi Wang, Yueqian Lin, Haoran Zhang, Yiyou Sun et al. \textquotedblleft OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection.\textquotedblright~  In \textit{NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models}.
\bibitem{b19} Sastry, Chandramouli Shama, and Sageev Oore. \textquotedblleft Detecting out-of-distribution examples with gram matrices.\textquotedblright~  In \textit{International Conference on Machine Learning}, pp. 8491-8501. PMLR, 2020.
\bibitem{b20} Morteza, Peyman, and Yixuan Li. \textquotedblleft Provable guarantees for understanding out-of-distribution detection.\textquotedblright~  In \textit{Proceedings of the AAAI Conference on Artificial Intelligence}, vol. 36, no. 7, pp. 7831-7840. 2022.
\bibitem{b21} Gretton, Arthur, Karsten M. Borgwardt, Malte J. Rasch, Bernhard SchÃ¶lkopf, and Alexander Smola. \textquotedblleft A kernel two-sample test.\textquotedblright~  \textit{The Journal of Machine Learning Research} 13, no. 1 (2012): 723-773.
% \bibitem{b23} Panaretos, Victor M., and Yoav Zemel. \textquotedblleft Statistical aspects of Wasserstein distances.\textquotedblright~  \textit{Annual review of statistics and its application} 6, no. 1 (2019): 405-431.
\bibitem{b22} McInnes, Leland, John Healy, Nathaniel Saul, and Lukas GroÃŸberger. \textquotedblleft UMAP: Uniform Manifold Approximation and Projection.\textquotedblright~  \textit{Journal of Open Source Software} 3, no. 29 (2018): 861.
\bibitem{b23} Janati, Hicham. \textquotedblleft Advances in Optimal transport and applications to neuroscience. \textquotedblright~ PhD diss., Institut Polytechnique de Paris, 2021.
\bibitem{b24} Yang, Jingkang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, WENXUAN PENG, Haoqi Wang et al. \textquotedblleft OpenOOD: Benchmarking Generalized Out-of-Distribution Detection.\textquotedblright~ In \textit{Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track}.

% \bibitem{b2} Liang, Shiyu, Yixuan Li, and R. Srikant. \textquotedblleft Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks.\textquotedblleft  In \textit{International Conference on Learning Representations. 2018.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}

\end{document}
