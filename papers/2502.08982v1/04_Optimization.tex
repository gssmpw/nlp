\vspace{-1ex}
\subsection{Discussion}
\label{sec:discussion}
\vspace{-.5ex}
\red{\textbf{General applicability on traditional data structures.}
The design principle of \sys can boost data search in traditional data structures with the capability of serving range queries. Specifically, perfect hashing can boost the search process with one-time hash computation with low memory costs that can be cached in compute nodes. For example, the binary search in B/B+ tree leaf nodes can be replaced by perfect hashing computation by searching a seed for hashing keys in leaf nodes.}
%Furthermore, the hash entry traversing in the chained-based hash table can be sped up by the perfect hashing scheme used in \sys for the first 4 elements in each bucket.}

\textbf{Ship computation to data.}
\sys decouples the process of DMPH into a memory-heavy component at memory nodes and a compute-heavy component at compute nodes and allows them to communicate via RDMA-RPC primitives. However, the memory accessing based on the given $ind\_bucket$ and $ind\_slot$ still needs a weak power computation unit close to data~\cite{ship}. We can apply \sys to another two promising approaches without using two-sided RDMA verbs. 
\begin{itemize}[left=0em]
    \vspace{-1ex}
    \item \textit{Extended RDMA READ verb.} PRISM~\cite{prism} proposes and simulates an extended one-sided RDMA indirect reading verb \texttt{RDMA\_READ} (\texttt{ptr} \textit{addr}, \texttt{size} \textit{len}, \texttt{bool} \textit{indirect}), where \textit{indirect} indicates if RNIC is supposed to read back the data pointed by the \textit{addr}. This embedded one-sided RDMA verb can free the memory node's CPU and offload the memory reading task in \sys to RNICs. The reason is that \sys can get the exact requested data address without potential data probing.
    \item \red{
    \textit{Performance capacity of Outback with hardware accelerators.} In-network computation~\cite{dinc} has gained attention for accelerating data services in distributed systems by offloading tasks to in-network computation devices~\cite{netsha,cxl-anns} such as SmartNICs/DPUs and CXL~\cite{cxl}. The idea of \sys can reduce the computation burden on SmartNICs by employing one round-trip, one-sided RDMA\_READ. For example, a SmartNIC~\cite{smartnic2,strom,bluefield,ringleader} can be placed on the memory node side, and function as an additional computation unit, and indirect data access tasks can be offloaded to it~\cite{smartnic1}. After the compute nodes in \sys issue a one-sided RDMA to read the queried key's slot and retrieve the address from the DMPH buckets, the SmartNIC can read the memory again via the PCIe switch and obtain the queried data through an additional PCIe round trip. The computation and data search tasks offloaded to the SmartNIC can be alleviated with the assistance of DMPH for the least computation burden.
    %\textit{Performance capacity of Outback with hardware accelerators.} In-network computation~\cite{dinc} using hardware accelerators such as SmartNICs/DPUs~\cite{smartnic2,smartnic1,strom} and CXL~\cite{netsha,cxl-anns,cxl}. Memory nodes can offload specific tasks from the main CPU to devices like NVIDIA Bluefield DPUs\cite{bluefield} or Broadcom Stingray SmartNICs~\cite{ringleader}, reducing extra memory copies and CPU involvement. The design of \sys could exploit this by offloading data access tasks on such devices; for example, indirect data access can be handled by SmartNICs. After the compute nodes in \sys issue a one-sided RDMA operation to read the queried keyâ€™s slot and get the address from the DMPH buckets, a SmartNIC can perform another memory read via the PCIe switch to retrieve the data using DMA without further computation burdens.
    }
    % \vspace{-1ex}
\end{itemize}

\vspace{-1.5ex}
\noindent\textbf{Shared-nothing architecture.} \sys utilizes a shared-nothing architecture~\cite{tutorial} to prevent the update of cached seeds across compute nodes in different shards. The number of KV pairs in each shard depends on the overall size of the database and the number of shards. A greater number of shards results in fewer KV pairs on each memory node. Consequently, the memory allocation for DMPH seeds and bucket locator on each compute node can be reduced, although additional memory nodes are required. Determining the granularity for sharding KV pairs has always been a tradeoff~\cite{kraska}, and it is recommended to choose the configuration based on the specific application.


%Note that the focus of this work is not to saturate RNIC of the memory node. As the CPU's power has a hard time matching the NIC's bandwidth~\cite{match}, \sys aims to reduce the computation usage on memory nodes and improve the throughput in the era of disaggregation.