\vspace{-.5ex}
\section{Introduction}
\label{sec:intro}
\vspace{-0.5ex}

Disaggregated memory systems~\cite{case,tutorial,fusee,shan2022towards,flexchain,rethinking,dex,pang} represent a transformative departure from traditional computing architectures, distributing memory storage and computational resources into distinct pools of nodes -- compute pools include nodes that carry rich CPU resources, and memory pools include nodes that carry rich DRAM and storage resources. This framework is prevalent in contemporary data centers and cloud infrastructures~\cite{redy,zhang2020understanding}, providing benefits such as enhanced resource utilization efficiency and flexibility to scale the system out by deploying more hardware. Disaggregated memory systems can harness Remote Direct Memory Access (RDMA)-capable networks~\cite{farm,clover,dinomo,fusee,redn}, featuring substantial throughput capacities (ranging from 40 to 400 Gbps) and small latency within the microsecond range. 
Memory-intensive applications, such as transaction systems~\cite{ford,drtmr,drtmh,motor} and key-value stores (KVSs)~\cite{race,rolex,herd,farm}, 
%utilize index data structures in disaggregated memory systems to achieve data accesses. These applications 
store the data and index data structures at the memory nodes and perform computation tasks at the compute nodes. %cache recently used or frequently used indices in their compute pools. 
%An application request first searches the cache in the compute pool, and then searches the remaining index data structures and obtains the data from the memory pool. 
%These systems are prevalent in contemporary data centers and cloud infrastructures~\cite{redy,zhang2020understanding}, and disaggregated systems signify a pivotal evolution in computational methodologies. 

\heiner{I am not used to these definitions. Often you have a say MySQL database and a memory-based Memcached KV-cache in front. Do you refer to such a setup or are you assuming that there is a 2nd level of caching. E.g. the compute nodes cache some data from the Memcached cache?}\liuyi{We will explain the definitions of disaggregated pools in the first sentence of introduction after bring it out} 


\begin{figure}[!t]
\centering
\subfigure[An example of one-sided RDMA.]{
    \label{fig:intro:a}
    \includegraphics[width=0.22\textwidth]{Figures/intro-new-1.pdf}}
\hspace{-1ex}
\subfigure[An example of two-sided RDMA.]{
    \label{fig:intro:b}
    \includegraphics[width=0.23\textwidth]{Figures/intro-new-2.pdf}}
\vspace{-3.5ex}
\caption{Examples of two types of RDMA systems.}
\vspace{-4.5ex}
\label{fig:intro}
\end{figure}

Existing RDMA networks for disaggregated memory can be categorized into two types. 1) One-sided RDMA~\cite{sherman,race,cell,rolex,smart} as shown in Fig.~\ref{fig:intro:a}. This type of network completely 
separates computation and memory access tasks. Each data request requires multiple round trips of communication between the compute node and the memory node. At least two round trips are necessary: one to access the index and the other to access the stored data. Note that many indices require multiple layers of accesses~\cite{cell,sherman}, hence they need much more than two round trips~\cite{pilaf,cell}. 
2) Two-sided RDMA or RDMA RPC~\cite{fasst,guidelines}, as depicted in Fig.~\ref{fig:intro:b}, involves computation tasks on both compute and memory nodes, requiring only a single round-trip communication for each request. However, two-sided RDMA cannot bypass the CPU on the memory node, necessitating the CPU on the memory node to execute the computation of the index structure, such as hash computations and key comparisons. Since the CPU resource on a memory node is very limited in disaggregated systems, this design may lead to CPU bottlenecks and potentially higher latency compared to one-sided RDMA~\cite{hstore,sherman}.
%Recent work suggests caching partial index data structures on compute nodes to reduce the number of round trips~\cite{sherman,race,cell,rolex,smart}. However, this method only prevents index computation on memory nodes for a subset of requests. In addition, replicating the index on multiple compute nodes leads to memory inefficiency and consistency issues.

\iffalse
Depending on whether the memory pools involve computation, these systems can be divided into two types: one-sided RDMA systems (not involved)~\cite{sherman,race,cell,rolex,smart}, and two-sided RDMA systems (involved)~\cite{fasst,guidelines}.

\heiner{I am wondering if the RDMA focus is warranted here. A) Is RDMA really an established sota? I would think most systems just use regular TCP to access data from a pool. B) Our approach does not use RDMA at all, correct? I think we want to mostly compare our design to a B-Tree like implementation which to me seems to most similar implemenation}\liuyi{We do want to focus on RDMA technology and build our system on top of two-sided RDMA.}

From the disaggregation perspective, we regard one-sided RDMA systems as \textbf{non-offloaded} systems, as their memory pools do not involve any computation. This design necessitates two round trips per request: \heiner{be precise: Are you retrieving an index or a whole index structure?}one for retrieving the queried index, and another for accessing data referenced by the corresponding index. On the other side, we refer to two-sided RDMA systems as \textbf{partially-offloaded} systems. This design only requires one round trip per request, but memory pools involve non-trivial computations, becoming the bottleneck for serving many compute pools~\cite{hstore,drtmh,sherman}. 
\fi

A natural question arises: "Can we design a one-round-trip RDMA-based network that does not incur computation-heavy tasks on memory nodes?" 
Achieving this goal is extremely challenging because putting the index on memory nodes leads to CPU bottlenecks while putting the index on compute nodes causes memory bottlenecks and consistency issues. 

This paper presents the first solution to this research problem.
Our key innovation is to design and implement an RDMA RPC-based system, called \sys, which decouples its index into two components. The first component is memory-efficient and includes most computation operations of the index, which is placed onto the compute nodes. The second component contributes to the most memory cost of the index, but its computation is trivial, and it is on the memory nodes. 
Such a design principle of decoupling the index is ideal for disaggregated memory systems: all computation tasks for \texttt{Get} requests and the majority of computation for data \texttt{Insert} requests are offloaded on compute nodes, while memory nodes focus on providing service for memory read and write. Hence, this approach is particularly effective for real-world workloads dominated by \texttt{Get} requests. It is also well-suited for emerging disaggregated memory systems equipped with SmartNICs with limited computation resources~\cite{bluefield, alveo, stingray}.
%, such as ARM-based SoC in NVIDIA Bluefield, AMD Alveo, and Broadcom Stingray ~\cite{bluefield, alveo, stingray}.

Similar to prior one-round-trip RDMA networks~\cite{fasst,guidelines}, \sys also relies on two-sided RDMA. %remote procedure calls (RPC). 
We implement \sys as a distributed KVS application. The index design of \sys is motivated by a recent advance of dynamic minimal perfect hashing (DMPH), called Ludo hashing~\cite{ludo}. 
The original design of Ludo hashing did not decouple the index into computation-heavy and memory-heavy components, but its perfect hashing property offers the opportunity for a novel decoupling approach that allows data \texttt{Get} requests in one round trip with trivial computation on memory nodes. 
For data \texttt{Insert} requests, we design additional operations to update the index on both the compute and memory nodes to ensure data consistency. 









\iffalse
In this paper, we introduce the \textbf{lightly-offloaded} design principle, aiming to surmount the constraints while harnessing the benefits in both one-sided and two-sided RDMA systems. This principle advocates for memory pools to execute only trivial computations, enhancing scalability compared to two-sided RDMA systems, while maintaining the efficiency of requiring just a single round trip for data access. This principle specifically aims to relocate all computational tasks for \texttt{Get} requests and the majority for write requests to compute nodes. This approach is particularly effective for real-world workloads dominated by \texttt{Get} requests. It is also well-suited for emerging disaggregated memory systems equipped with SmartNICs, which typically have limited computational capacities, such as the 8-core ARM Cortex-A72 processor in Bluefield-2~\cite{bluefield}.

Designing a \textit{lightly-offloaded} system is challenging because caching most of the indexing data structures within compute pools is impractical due to the limited memory capacity of compute nodes. In addition, replicating these structures in multiple compute nodes leads to storage inefficiency and consistency issues. On the other hand, memory nodes are inevitably involved in tasks such as node traversal and handling hash collisions, both requiring non-trivial computational resources. This dichotomy underscores the complexity involved in redistributing computation between compute and memory nodes to achieve a lightly-offloaded system.

To overcome these challenges, we propose to apply minimal perfect hashing (MPH) on disaggregated systems to redistribute computation tasks between memory nodes and compute nodes. MPH stands out among other data structures (e.g., B-Trees~\cite{cell,sherman}, hopscotch hash tables~\cite{pilaf,cuckoo}) thanks to collision-free property, ensuring that the data can be located in the exact slot with one shot without additional checks. This feature enables entire computation tasks for \texttt{Get} requests to be offloaded to compute nodes and leaves memory nodes focused on data access.

Applying Minimal Perfect Hashing (MPH) to disaggregated systems encounters several challenges. (1) The needed computation of a data \texttt{Get} request on MPH is still non-trivial, demanding well-designed partition between compute and memory nodes. (2) Handling data \texttt{Insert} requests with MPH involves complexities such as the necessity for resizing the hash table or employing a separate cache for new key-value pairs, which can result in service interruptions or reduced performance. (3) The inherent need for MPH table resizing to accommodate additional keys poses a challenge, as it leads to service pauses due to the stop-the-world reconstruction process.

We design \sys, a high-throughput RDMA RPC-based KVS adhering to the \textit{lightly-offloaded} design principle. We propose a novel \textbf{decoupled MPH} framework, which effectively separates the indexing structure, thereby transferring the majority of data operation responsibilities from memory nodes to compute nodes. For \texttt{Insert} requests, \sys employs a novel \textbf{pre-allocated and shufflable} MPH offering a two-layered strategy for key accommodation. Furthermore, \sys incorporates a \textbf{partial locking} mechanism, which allows for the continued processing of some data requests using the existing MPH table during periods of MPH resizing, thereby enhancing system efficiency and minimizing disruption.
\fi


%\todo{explain the general applicability}

Overall, this paper makes the following contributions:
\vspace{-1ex}
\begin{itemize}
    \item We present a novel solution that provides one-round-trip RDMA with RPC that incurs minimal computation tasks on memory nodes. The design principle of decoupling the index works effectively for emerging disaggregated memory systems.

    \item We design the \sys system as a distributed KVS. We design a decoupled index based on a recent data structure of DMPH. We also designed the algorithms and protocols for supporting data operations and system updates.
    
    \item We implement a prototype of \sys and evaluate the performance on YCSB workloads~\cite{ycsb} and four real-world datasets from SOSD~\cite{sosd}. The experimental results show that \sys achieves higher throughput than both the state-of-the-art one-sided RDMA and two-sided RDMA-based in-memory KVS by 1.06-5.03$\times$.
\end{itemize}

%The rest of this paper is organized as follows. We introduce the background of this work in Section~\ref{sec:background}. In section~\ref{sec:motivation} we present the observations from our measurement studies of existing RDMA-enabled disaggregated memory systems and the motivation of our work. In section~\ref{sec:design}, we present the design details of \sys. Implementation and evaluation results are shown in section~\ref{sec:eval}. We discuss the related work in section~\ref{sec:related_work} and conclude this work in ~\ref{sec:conclusion}.








%$\bullet$ \uline{\textit{Lightly-offloaded design principle.}} We revise existing 

%$\bullet$ \uline{\textit{RPC-based decoupled MPH data structure.}} 
%We introduce a novel application of applying an MPH table on a disaggregated system, establishing a KVS that leverages two-sided RDMA primitives. Our approach involves decoupling the MPH table and distributing the part of the indexing structure between compute nodes and memory nodes. This strategic division aims to achieve maximum throughput for data lookup with the constraints of limited CPU resources on remote memory nodes.

%$\bullet$ \uline{\textit{Remote data operations protocols based on MPH.}} 
%We strategically design fast and efficient protocols for data lookup/insert/update/deletion and distribute associated responsibilities and computation tasks on compute nodes and memory nodes to process data executions and indexing structure of the MPH table. 

%$\bullet$ \uline{\textit{MPH table remote resizing coordination.}} 
%We introduce a one-sided RDMA-based scheme for achieving MPH table resizing when the number of inserted KV pairs surpasses a predefined threshold. Additionally, we implement a partial bucket lock scheme to facilitate data lookup on stale MPH buckets while constructing the new MPH table.

%$\bullet$ \uline{\textit{Implementation and evaluation.}}
%We implement a prototype of \sys and evaluate the performance via YCSB workloads~\cite{ycsb} and four real-world datasets from SOSD~\cite{sosd}. Our experimental results show that \sys achieves the highest throughput with the limited CPU resources on the memory nodes and outperforms other RPC-based baselines by up to 1.63$\times$ with full read workloads. 







%The indexing data structure and the RDMA primitives have become the fundamental building blocks in the applications of disaggregated memory systems, such as transaction systems~\cite{ford,drtmr,drtmh,motor} and key-value stores (KVS)~\cite{race,rolex,herd,farm}. 

%This integrative approach signifies a strategic reconfiguration of conventional computing paradigms, endowing modern computational ecosystems with unparalleled adaptability and performance enhancements~\cite{redy,zhang2020understanding}.

%Prevalent in contemporary data centers and cloud infrastructures~\cite{redy,zhang2020understanding}, disaggregated systems signify a pivotal evolution in computational methodologies.


%\blue{In the context of a disaggregated system, with the resource separation of storage and computation emerging as a trend.~\cite{clover,dinomo,fusee,shan2022towards,case,tutorial} One-sided RDMA-based schemes~\cite{farm,rolex,race,smart,sherman} are constrained by the limited scalability of RNIC and lack lock management on data operations. RPC-based approaches~\cite{fasst,guidelines} suffer from imposing computation tasks of data retrieval on its limited CPU resources. 
%\uline{To make a further step towards the scalable disaggregated system. Not only resources (e.g., DRAM or CPU devices), but the workloads and duties are also supposed to be disaggregated on the distinct pools.}
%This work focuses on efficiently decoupling storage and computation workloads in disaggregated systems in an advanced step, especially in scenarios where limited computational resources bottleneck the memory pool's performance. The proposed approach aims to enable memory nodes to concentrate on data read and write operations, while assigning computational tasks to compute nodes, thereby enhancing overall system performance.}