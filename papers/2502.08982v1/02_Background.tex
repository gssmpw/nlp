\vspace{-2ex}
\section{Background}
\label{sec:background}
\vspace{-.5ex}

\subsection{Disaggregated Memory with RDMA}



\iffalse
\begin{figure*}[!t]
\centering
\subfigure[An exmaple of one-sided RDMA.]{
    \label{fig:intro:a}
    \includegraphics[width=0.27\textwidth]{Figures/intro-1.pdf}}
    \hspace{2ex}
\subfigure[\texttt{Get} data with RPC and B+tree.]{
    \label{fig:intro:b}
    \includegraphics[width=0.27\textwidth]{Figures/intro-2.pdf}}
    \hspace{2ex}
\subfigure[\texttt{Get} data with RPC and hash table.]{
    \label{fig:intro:c}
    \includegraphics[width=0.27\textwidth]{Figures/intro-3.pdf}}
\vspace{-2ex}
\caption{Examples of accessing different remote storage backends.}
\label{fig:intro}
\end{figure*}
\fi


Disaggregated memory systems with RDMA can be categorized into two types: one-sided RDMA systems~\cite{sherman,race,cell,rolex,smart}, and two-sided RDMA (RDMA-RPC) systems~\cite{fasst, guidelines}. An example of one-sided RDMA systems~\cite{sherman,race,cell,rolex,smart} is illustrated in Fig.~\ref{fig:intro:a}. These systems support applications such as KVS and transaction systems with various index data structures, including B/B+ trees, hash tables, radix trees, and learned indexes. However, it is widely recognized that multiple round-trip communications are needed for each \texttt{Get} request: at least one for querying the index and one for reading data. The high communication cost results in both long latency and network congestion. %The constrained RNIC resources for tracking queue pairs (QP) status incur complexity for data management~\cite{fasst,guidelines} under the reliable connection (RC) primitives. 

\heiner{if you have data to support these claims, provide a forward pointer. E.g. in Section we will show that RDMA introduces X overheads}\liuyi{I think, we can cite the data from cite{scalablerpc} to explain the bad scalability of one-sided RDMA}
\heiner{no need to rely on what other may be concerned about. just say: RPC-based systems introduce CPU overheads...}\liuyi{thanks! ``RPC-based approaches suffer from the large CPU consumption in disaggregated systems, ''}

\begin{figure}[t]
\centering
    \includegraphics[width=0.415\textwidth]{Figures/ludo.pdf}
    \vspace{-1.5ex}
    \caption{Ludo hashing.}
    \label{fig:ludo}
    \vspace{-4.5ex}
\end{figure}

\iffalse
\begin{figure*}[!h]
\centering
\renewcommand\thesubfigure{}
\begin{minipage}[t]{0.27\textwidth}
        %\vspace{1.5ex}
        \subfigure[]{
            \label{fig:mph}
            \includegraphics[width=\textwidth]{Figures/mph.pdf}}
        \vspace{-6ex}
        \caption{A case of MPH for four keys in a bucket.}
    \end{minipage}
\hspace{4ex}
\begin{minipage}[t]{0.28\textwidth}
        %\vspace{1.5ex}
        \subfigure[]{
            \label{fig:ludo}
            \includegraphics[width=\textwidth]{Figures/ludo.pdf}}
        \vspace{-4ex}
        \caption{Ludo hashing.}
    \end{minipage}
\hspace{3ex}
\begin{minipage}[t]{0.32\textwidth}
        \vspace{-1.5ex}
        \subfigure[]{
            \label{fig:othello}
            \includegraphics[width=\textwidth]{Figures/othello.pdf}}
        \vspace{-6ex}
        \caption{Othello hashing.}
    \end{minipage}
\end{figure*}
\fi


Two-sided RDMA-based systems~\cite{guidelines,fasst} have been investigated to dispatch compute nodes' requests to the memory node via RPC over the RDMA network with only one round trip. As depicted in Fig.~\ref{fig:intro:b}, a data index, such as a B-Tree or hash table, is maintained at the memory node.
%the server side\footnote{We refer to compute nodes as the client and the memory node as the server.}. 
When a data query occurs, in addition to polling the RNIC and posting messages, the CPU of the memory node is responsible for traversing the index. The memory node has to perform computational tasks, including hash computation, fingerprint checking, and key comparisons. This process introduces additional computational overhead and memory accesses.
%Alternatively, the system can use a hash table as the indexing data structure. A chained-based hash table is maintained as the data index in the memory node. The memory node needs to probe the linked node to handle the hash collision. Researchers 
Existing solutions~\cite{race,drtmr,mica} that store keys' fingerprints in their hash tables to save memory usage also introduce extra computation. 
%use a hash table to save memory usage by storing the keys' hash fingerprint and providing multiple candidate locations for inserted KV pairs.0 
For example, if the memory node employs the state-of-the-art (2,4)-Cuckoo hash table~\cite{cuckoo}, each \texttt{Get} request requires one fingerprint computation and, at most eight rounds of fingerprint checking.








\vspace{-1ex}
\subsection{Dynamic minimal perfect hashing}
\label{subsec:background:dmph}
In this subsection, we first introduce the background of DMPH and then present an existing MPH implementation, Ludo hashing~\cite{ludo}. 
%We then discuss the Othello hashing~\cite{othello}, which was used as a memory-efficient bucket locator of Ludo hashing. 

%\blux{Perfect hashing~\cite{praticalph} uses a constructed function to distribute keys to different buckets in a hash table without collisions.}
Perfect hashing~\cite{praticalph} represents a family of schemes that designs and manipulates hash algorithms to distribute keys to different buckets in a hash table without collisions.
Since it is impractical to find a single hash function that generates no collisions for a large set of keys, a common approach is to use two levels of mapping. The first level maps keys to a number of groups, each of which contains several keys. The second level addresses key collisions inside each group. Minimal perfect hashing maps $n$ keys to exactly $n$ buckets, but it is inflexible for key insertions and only applicable to a static set. To allow key dynamics, dynamic minimal perfect hashing (DMPH) may use $(1+\epsilon)n$ positions for $n$ keys \cite{scalebricks,ludo}. 
One primary advantage of perfect hashing is that it does not need to store the keys in the hash table. Since perfect hashing eliminates collisions, a key query does not need to compare keys to address collisions. Avoiding storing keys can significantly reduce memory costs, because as a secondary index, the size of keys (usually hundreds of bits) is much longer than the queried value in a hash table (usually a storage address in tens of bits). 


One of the most recent solutions of DMPH is called Ludo hashing~\cite{ludo}. 
As shown in Fig.~\ref{fig:ludo}, Ludo hashing~\cite{ludo} first uses a data structure called Othello~\cite{othello}, a dynamic implementation of Bloomier filters \cite{bloomier} with two arrays, as the \textit{bucket locator} to distribute keys into different buckets, each of which includes exactly 4 slots. Then, in each bucket $B_i$, Ludo hashing uses brute force to find a hash seed $s_i$ such that the hash function with $s_i$
can map the 4 keys in the bucket to 4 different slots without collision. Hence, there is no need to store keys in the table for collision resolution. 
The space cost of Ludo is $3.76 + 1.05l$ bits per key, where $l$ is the length of the record value, which is claimed to be the smallest memory cost in the literature~\cite{ludo}. 
%The bucket locator and the seeds together cost 3.76 bits per key. 
\red{The bucket locator leverages Othello arrays~\cite{othello}, which costs 2.33 bits per key. Each bucket contains a 5-bit long seed shared by four keys in Ludo, i.e., 1.25/0.95 bits per key when we set the load factor as 95\%.
Also, the majority of memory cost is for storing the values in the buckets, costing $1.05l$ bits per key.}
We observed that the computation for looking up the slot only needs the bucket locator and the seeds, which are memory efficient. On the other side, the hash table buckets/slots part storing all data values contributed to most memory of this index, but it requires little computation.


\iffalse
consists of two parts of hash computations. The first part is responsible for narrowing down the key space into a set of candidate indices. 
\heiner{be precise. there isn't a level of hash functions. The second level utilizes multiple hash functions to map candidate indices to unique indexes.}\liuyi{precious comments! I think I can use ``two parts/layers of hash computation''(instead of functions)?} 
The second part of the hash computation is applied to these candidate indices to obtain a unique index for each key. 
\heiner{unclear why multiple hash functions are used/needed. If you mention multiple, you have to explain}\liuyi{The first part hash to map key to bucket, the second part map key to slot. This is a common way to realize MPH, we show a specific process with example of our Ludo in the following.}
Combining these two parts ensures that each key is mapped to a distinct location within the hash table, eliminating collisions.
\heiner{your explanation does not provide a reason for why 2 levels are needed. Why not map to indexes directly in the first level? Why not use a single level and compute perfect hash for all of them?}\liuyi{Like in Ludo, the first level makes sure there wont be more than 4 keys in a bucket. In general, the first level hash will partition the number of Keys into small groups and then realize an MPH in the second level. Acutually, I just briefly explain a common way to realize MPH in this part and details with Ludo and Othello is in the following.}
\heiner{MPH eliminates collision but how does the first level do it? Explain othello/cucckoo here or provide forward reference}\liuyi{Othello uses ``two choices''(alternative buckets for each key) placement to realize it. I will cite Othello here.}
Furthermore, the distinctive feature of MPH lies in its memory usage. It maps $N$ elements into $(1+\epsilon)N$ space of the hash table and eliminates the need to store keys. 

\heiner{explictely need to define bucket and slot. say: We define bucket as.. and slot as..}\liuyi{added it!}
As shown in Fig.~\ref{fig:mph}, a bucket refers to a column of the hash table and consists of multiple slots.
Assuming there are four distinct keys in a bucket, in order to leverage MPH for quick access and minimize storage overhead, they employ a brute-force approach for seeking a seed for each bucket to locate keys. 
\heiner{need to define seed. e.g. we use seeds to realize different hash functions. Provide an example how seed+input is used to compute output}\liuyi{added content shown in blue.}
%This involves attempting various seeds starting from 0 until the hash function successfully distributes the four keys into separate slots without any collisions. 
\blue{Consider the four keys inserted to a bucket are $\{k_0,k_1,k_2,k_3\}$. Perfect hashing uses brute force to select a specific seed $s$ from 0 to 255, which can make the universal hash function $H(k_i, s)$ map the four keys into four distinct slots without collision.}  
Afterward, the chosen seed will be retained in the bucket, serving as a slot locator for key queries. They will hash the queried key using the selected seed, and the resulting value will indicate the slot number for that key.
Finding a seed for resolving keys in a small key space (e.g., a bucket of four) is one of the approaches~\cite{scalebricks,ludo} to realize MPH. 


\noindent\textbf{Ludo hashing~\cite{ludo}.}
MPH is effective only when the key space is small. For instance, they can consistently find a perfect seed within 255 (8 bits) to distinguish the four different keys without collisions in their experiments.
\heiner{what happens if we don't find one?}\liuyi{we have never seen that with the seed search space set 0-255. If }
However, when attempting to find a perfect seed for distributing over five keys, expanding the seed space (e.g., 10 bits) and dedicating more time becomes necessary.
\heiner{why would we attempt 10 bit if 8 bit is sufficient?}\liuyi{8 bits only works for 4 keys, if there are 5 keys, more bits needed.}
Ludo hashing comprises a hash table with a specified number of MPH buckets, each containing an 8-bit seed and four elements. 
\heiner{vague. In stead of "the nature of MPH" say why. e.g. because MPH eliminates collitions it is unnecessary to store keys/tags as part of an entry to determine a match.}\liuyi{thanks for your advice and fix it as shown in blue.}
\blue{Since MPH eliminates collisions, it is unnecessary to store the key field to determine a match, and only the value field is stored in each slot.}
\heiner{we still need a valid bit per entry right?}\liuyi{we do need a bit to show the new inserted keys' conflict when we apply MPH on the KVS, but that is not part of perfect hashing.}
However, due to the potential collisions of hash functions~\cite{can}, merely hashing different keys into separate buckets doesn't guarantee that the size of each bucket will be no more than four. Ludo hashing adopts "the power of two choices"~\cite{twochoices}, similar to Cuckoo hashing~\cite{cuckoo}, where each key is placed in two candidate buckets, denoted as $b_0$ and $b_1$.
A bucket locator is employed to distinguish which specific candidate bucket (e.g., $b_0$ or $b_1$) stores the queried key as shown in Fig.~\ref{fig:ludo}. Additionally, the selection of candidate buckets for all keys must ensure that no bucket is assigned more than four keys. Ludo hashing utilizes Othello hashing to construct its memory-efficient bucket locator, dividing keys into groups with a small key space.

\noindent\textbf{Othello hashing~\cite{othello}.}
\heiner{first say what problem othello addresses. the next section (separating keys into two sets does not tie. why do we need this to ensure <=4 keys per bucket?}
Provided we have $N$ distinct keys, and they are separated into two sets $S_0$ and $S_1$. To query which set they belong to, the most intuitive way is to build an index for all keys, and the corresponding values are 0 or 1, which indicates the set it is. However, storing a full table will incur large memory overheads, especially since the workload's key size is getting larger~\cite{workload} as time goes on.

\heiner{unclear. what do we need binary classification for? It is unclear how othello ties to the previous}\liuyi{added it as shown in blue.}
\blue{There are two candidate buckets $b_0$ and $b_1$ represented by two distinct hash functions $hash_1(k)$ and $hash_2(k)$ for each key to be placed. Each key can only be stored in one of the buckets, and they need to track which bucket holds the key.
One-bit Othello hashing is proposed for tracking the bucket choice of each key through binary classification}, as illustrated in Fig.~\ref{fig:othello}. There are two bit arrays $A$ and $B$, with lengths $m_a$ and $m_b$, respectively, and corresponding hash functions $hash_A(\cdot)$ and $hash_B(\cdot)$. 
Othello is constructed by finding an acyclic undirected graph $G=(V_a,V_b,E)$, where $E$ is the edge set, and $V_a$, $V_b$ are the vertex sets. Each node $v^i_a \in V_a$ ($0 \leq i < m_a$) and $v^j_b \in V_b$ ($0 \leq j < m_b$) represents the $i$-th and $j$-th bit of arrays $A$ and $B$.
They assign uniformly random values for the gray vertex (e.g., $u_2, v_1$), which will not influence the query result because they are not the hashed bit for any keys.
To query the binary result of a given key $k$, the result is given by $A[hash_A(k)] \oplus B[hash_B(k)]$, which is the exclusive OR of the hashed bits in these two arrays. 
It takes O(1) time to find a proper pair of <$hash_A, hash_B$> to successfully allocate all keys with a memory space of at least $2.33$ bits/key. Also, it takes O(1) time to query the binary result of the given key.
\fi


\vspace{-2.5ex}
\section{Measurement and Motivation}
\label{sec:motivation}
\vspace{-.5ex}
%\textbf{Disaggregated memory systems using RDMA-RPC can achieve higher throughput than one-sided RDMA by reducing index computations.} The multiple memory accesses inherent to such systems imply that adopting one-sided RDMA necessitates multiple communication round trips, completely bypassing the CPU at the memory node for serving data requests. In contrast, two-sided RDMA or RDMA-RPC uses only one round trip but requires additional computation at the memory node for querying the index.
%The state-of-the-art one-sided RDMA design provides comparable throughput to RDMA-RPC~\cite{fasst} when there are limited CPU resources for memory nodes in disaggregated systems.

We wonder if, we remove the computation cost at the memory node, will RDMA-RPC demonstrate much higher throughput than the state-of-the-art one-sided RDMA?
\textbf{If the answer is "Yes", then there is a great opportunity to design a high-throughput RDMA-based KVS by reducing the computation cost at the memory node.} 

Toward this objective, we conduct experiments to analyze the throughput performance of both one-sided RDMA and RDMA-RPC systems with 9 r320 servers in CloudLab~\cite{cloudlab}, each is configured with a Mellanox CX3 adapter (50Gbits).
We compare the performance of the following systems with \texttt{Get}-only workload. (1) RACE hashing~\cite{race}, a state-of-the-art one-sided RDMA-based scheme. Its hashing index is crafted for disaggregated memory, facilitating data retrieval within two round trips. (2) RPC-hash table, a two-sided RDMA method whose compute nodes and memory nodes communicate in RDMA unreliable datagram (UD) mode. Each memory node maintains a chained hash table in its local memory to handle remote data requests. (3) RPC-Dummy. A hypothetical RDMA-RPC method that incurs minimal computation cost at each memory node. 
RPC-Dummy only implements one memory access and then returns any data in the accessed memory at the memory node,  with no extra computation tasks.
RPC-Dummy's throughput can be considered the upper bound among all possible RDMA-RPC systems. We use this method to explore the performance potential of our design objectives.  We vary the number of memory node threads as 1, 2, and 4 in RPC-based approaches, and each memory node thread maintains one Queue Pair (QP) and runs in a distinct CPU core.
%We vary the total number of compute node threads from 8 to 64. More evaluation details can be found in Section~\ref{sec:eval}.

\begin{figure}[!t]
\centering
%\subfigure[The throughput of RACE Hashing with the varied client threads.]{
    %\label{fig:motivation:a}
    %\includegraphics[width=0.32\textwidth]{Figures/motivation-1.pdf}}
\captionsetup[subfigure]{aboveskip=-2ex}
\vspace{-2ex}
\hspace{-2.5ex}
\subfigure[Throughput of different systems with limited number of memory node threads.]{
    \label{fig:motivation:b}
    \includegraphics[width=0.465\textwidth]{Figures/motivation-2-new2.pdf}}\\
\vspace{-3ex}
\hspace{-2.2ex}
\subfigure[The CPU time breakdown on a memory node with one thread.]{
    \label{fig:motivation:c}
    \includegraphics[width=0.467\textwidth]{Figures/motivation-3-newest.pdf}}
\vspace{-3ex}
\caption{Observations from the microbenchmarks.}
\label{fig:motivation}
\vspace{-4.5ex}
\end{figure}

The results are shown in Fig.~\ref{fig:motivation:b}. For one memory node thread (one core), RPC-hash table achieves a throughput similar to that of RACE hashing.
For RACE hashing, multiple reasons limit its throughput, including the two round trips to complete one data \texttt{Get} operation and multiple RC connections of the compute node threads that incur resource contention in the RNIC cache~\cite{scalablerpc}.
%two round trips are required to complete one data read operation and multiple RC connections of the compute node threads in RACE hashing make it hard for RNIC to cache their status
RPC-hash table requires only one round trip, but the complexity of querying the index on the memory node introduces extra latency and limits its throughput.  
%The reason is that the multiple RC connections of the compute node threads in RACE hashing make it hard for RNIC to cache their status, and the two round trips are required to complete one data read operation. 
The throughput of RPC-hash table increases correspondingly when we increase the number of threads to 2 and 4. In contrast, RACE hashing maintains a static performance.
%Therefore, the RPC-based RDMA method offers better scalability compared to RACE hashing. 
%Even through systems like RACE hashing free the memory node CPU, they fail in performance scalability compared to the partially-offloaded approach represented by the RPC-hash table.
%We take a further look at the performance of RPC-Dummy. 
%Furthermore, in the comparison between RPC-Dummy and RPC-hash table, 
RPC-Dummy can outperform RPC-hash table by around 2$\times$ under the cases of both single and multiple memory node threads. 
%For example, with 64 compute node threads, RPC-hash table provides a throughput of 2.95 Mops per memory node thread. In comparison, RPC-Dummy can achieve 5.04 Mops per memory node thread under the same conditions. 
%Although RPC-Dummy needs some trivial computation tasks at the memory node, such as RNIC polling and local data lookups~\cite{fasst}, by removing the index computation and memory access cost, the throughput shows huge improvement compared to RPC-hash table that includes a hash table lookup at the memory node. 
%with the hash table requiring multiple memory reads across various layers to search for a key, as discussed in section~\ref{fig:intro}. 
Hence an RDMA-RPC network that introduces little computation overhead to the memory node can achieve higher throughput than both existing one-sided RDMA and RDMA-RPC solutions. 
The results suggest that RPC-based KVS has a potential for throughput improvement by reducing computation tasks at memory nodes, which motivates the design of this project.
%For example, a single memory node thread proves sufficient to achieve a throughput of 2.95 Mops in the RPC-hash table for . However, the RPC-Dummy can achieve 5.04 Mops with lightly-offloaded computation on the memory node.
%The performance gap between partially offloaded and light-offloaded approaches motivates us to delve into the specific computational resources on the memory node side, especially in scenarios where CPU resources are constrained.



%Concerns have been raised about the large CPU consumption in disaggregated systems when employing RPC-based approaches, particularly about the remote memory node's serving functions. Despite there being efforts~\cite{drtmh,hstore,herd} to use both RPC primitives and one-sided RDMA verbs for various data requests, performance issues persist in building efficient disaggregated systems. For instance, in the case of HStore~\cite{hstore}, which employs asynchronous data updates to a sorted index via RPC verbs and single data lookups with one-sided verbs, challenges related to RNIC resource utilization and server CPU bottlenecks for data query throughput remain. This situation prompts us to explore whether an alternative tradeoff could be achieved to enhance system performance, not just a simple combination of these approaches.








\textbf{CPU utilization breakdown for RPC-based approaches.}
%To further analyze the consumed CPU time at the memory nodes in RDMA-RPC approaches, we conduct the following microbenchmark to examine the CPU cost breakdown of different computation tasks. 
We run RDMA-RPC with different indices: hash table, Btree, and learned index, at the memory node. 
%These indices involve different query operations. RPC-Dummy is also tested for comparison purposes.
The CPU time consumed by these four RPC-based KVS systems while handling an equal number of data \texttt{Get} requests is normalized and presented in Fig.~\ref{fig:motivation:c}, with the number of compute node threads fixed at 64. RPC-Dummy takes the least time.
%to serve the requests because the memory node processes the data request by simply implementing a memory read. 
Other approaches consume more time in different amounts.
%because of the extra computation on the memory node. 
%In the case of RPC-Learned index, the learned index model~\cite{alex} consists of tree nodes in multiple layers, with each node maintaining a learned model and a gapped array for storing KV pairs. Locating a key involves the computational overhead of traversing tree nodes from the root and computing with the learned models.
%Additionally, an extra memory read from the data area is necessary. 
%Furthermore, we delve into the breakdown of the consumed CPU time of all four approaches.
For RPC-Btree, in addition to the communication overheads for polling $\mathtt{mlx4\_poll\_qp}$ (4.03\%), posting messages $\mathtt{mlx4\_post\_send}$ (7.52\%) and UD transport (6.85\%) from connection management, the most CPU-consuming event is the RPC callback function (70.59\%), which executes local index lookup and data access. 
In all four schemes, the RPC callback function consumes the most CPU time, and the variations in CPU consumption among them are mainly attributed to differences in the RPC callback function. RPC-Btree consumes the most CPU time for RPC callback, followed by RPC-hash table. 
%we can observe a similar CPU resource breakdown pattern in that most CPU time is consumed on the RPC callback function. 
%ALEX demonstrates slightly lower CPU cycles for its fast data lookup with recursive model index~\cite{learnedindex}. Local data lookup involves both computation and memory reads. For example, 
RPC-Dummy spends the least CPU time on the RPC callback function (46.11\%) and serves the most data requests because there is no computation burden for the memory node in RPC-Dummy. 
%The reason is that the computation on memory nodes of RPC-Dummy is lightly-offloaded compared with other partially-offloaded approaches.
In disaggregated systems, tasks such as computing hash functions on a hash table, traversing tree nodes in a B-Tree, and executing learned models on a learned index are not ideally suited for memory nodes.
%We should make the memory node focus on its storage responsibilities instead of computation tasks.
\textbf{The throughput of RDMA-RPC methods is mainly limited by CPU usage during the RPC callback function for index lookups and data reads. High CPU consumption from complex index computations on memory nodes reduces throughput, particularly when CPU resources are constrained, indicating that optimizing these computations can enhance performance.}

%Recently, researchers have explored the application of learned indexes in disaggregated memory with one-sided RDMA primitives~\cite{xtore,rolex}. However, despite the use of caching in the compute node, the approach only works for ordered data and it is not the focus of our work.
%still necessitates a minimum of two round trips to serve requests.



