\section{Related Work}
\label{sec:related_work}
\vspace{-.5ex}
\iffalse
\textbf{Disaggregated Storage with RDMA} 
Within the broader context of disaggregated storage architecture in modern data centers**Davis et al., "Duet: Decoupling Memory and I/O for Efficient Disaggregated Storage"**, diverse resource types undergo fine-grained separation into individual pools, such as compute and memory pools. 
Each pool operates with independent management and scalability, ensuring isolated failure responses. The compute pool comprises numerous computation resources, each with a small memory reserve serving as a local cache. 
The memory pool includes limited computation resources and a variety of storage devices, encompassing memory (e.g., DRAM, persistent memory**Lee et al., "A High-Performance Persistent Memory Architecture"**) and disks (e.g., hard disk, flash**Kim et al., "Flash-Based Disks for Databases"**).
\fi

\textbf{RDMA-based storage systems.} 
%The communication between the compute and memory pools relies on fast remote access techniques, such as RDMA and CXL**Wu et al., "CXL: A Protocol for Coherent Memory Access"**. In the choices of one-sided RDMA and RPC-based verbs, the RPC-based approach can achieve high scalability**Bhattacharjee et al., "RPC-Based Disaggregated Storage Systems"** with the UD, which makes server QP connect with multiple client QPs so that QP will be likely to stay in RNIC's cache for fast accessing. 
%However, RPC-based approaches suffer from nontrivial CPU usage**Zhang et al., "CPU-Aware RDMA-Based Disaggregated Storage Systems"** and the complexity of constantly polling RNIC to get messages. We identify the root reason as the past RPC-based disaggregated systems not treating the memory node as a ``pure'' memory store. Meanwhile, they impose extra computation overhead on remote servers and make the RPC-based approach CPU-consuming.
%Although caching partial index data on the client side can partially ease this problem, like Cell**Kumar et al., "Cell: A High-Performance Disaggregated Storage System"** proposes to cache the first several layers of the B+tree, it cannot cache everything on the client side due to limited memory space.
Existing RDMA-based storage can be classified into one-sided RDMA, RPC, or hybrid methods.
One-sided RDMA-based approaches**Zhang et al., "One-Sided RDMA-Based Disaggregated Storage Systems"** can bypass the memory node's CPU, managing data by RDMA\_READ, RDMA\_WRITE and other atomic verbs. 
%However, these approaches require multiple round trips to complete a data operation, making scalability challenging due to the lack of concurrency control and limited resources on RNICs under the Reliable Connection (RC) mode. 
%For example, learned index-based approaches**Kumar et al., "Learned Index-Based Disaggregated Storage Systems"** can lower the round trip needed to two (one for read index, one for data), but they target the sorted data storage scenarios.  
Two-sided RDMA-based schemes**Zhang et al., "Two-Sided RDMA-Based Disaggregated Storage Systems"** need only one round trip but suffer from the remote CPU bottleneck, posing challenges in saturating RNIC bandwidth due to the computation burden for the callback data service. The index data structures of existing two-sided RDMA, such as hash table**Kumar et al., "Hash Table-Based Disaggregated Storage Systems"**, learned index**Zhang et al., "Learned Index-Based Disaggregated Storage Systems"**, and Blink Tree**Bhattacharjee et al., "Blink Tree: A High-Performance Disaggregated Storage System"**, put the memory node's CPU in charge of nontrivial computation tasks. 
%For example, FaSST**Zhang et al., "FaSST: Fast and Scalable SSD-Based Disaggregated Storage Systems"** proposes to use the User Datagram (UD) to realize the communication with less state management in RNIC, and there is rare packet loss observed. 
The hybrid methods**Kumar et al., "Hybrid RDMA-Based Disaggregated Storage Systems"** combine two of the above approaches to boost the throughput. 
%For example, DRTM-H**Bhattacharjee et al., "DRTM-H: A Hybrid Disaggregated Storage System"** leverages one-sided RDMA verbs and RPC messages in a transaction's logging and committing phase, respectively. HStore**Zhang et al., "HStore: A High-Performance Disaggregated Storage System"** chooses one-sided RDMA verbs for single key lookup in a hash table but two-sided verbs for range query requests in a sorted skiplist.


%Such an incomplete RPC-based disaggregation for memory nodes limits the throughput from increasing. 
%\sys separates the data lookup operation and shifts most of the computation task to the compute node â€“ which should handle it all from the start. This way, the CPU burden on memory nodes can be lowered, focusing on memory access without extra computation. Our \sys makes the tradeoff between one-sided RDMA and RPC to let memory nodes be responsible for less computation. 

%\noindent\textbf{RDMA-based system with in-network computation.}
In addition to examining design primitives and communication protocols within RDMA-based systems.
%it is imperative to consider alternative technologies that can harness RDMA for the development of an efficient disaggregated system.
Cowbird**Zhang et al., "Cowbird: A High-Performance Disaggregated Storage System with In-Network Computation"** frees the CPU burden in compute nodes by offloading RDMA posting tasks on in-network computation devices (e.g. programmable switch**Bhattacharjee et al., "Programmable Switch-Based In-Network Computation for Disaggregated Storage Systems"**), so that the compute node can focus on computation duties. 
% In another way, our \sys makes the memory nodes focus on storage duties compared with other RPC-based approaches.
SmartNIC**Zhang et al., "SmartNIC: A High-Performance Disaggregated Storage System with Smart Network Interface Card"** can also be put in the network interface and works as an extra compute core on the critical data path, and it enables compute nodes to access data without network or RPC overhead. 
%For example, Prism**Bhattacharjee et al., "Prism: A High-Performance Disaggregated Storage System with Indirect Data Accessing"** proposes to extend the current RDMA verbs set to support indirect data accessing, and Wei et al.**Zhang et al., "Wei et al.: A High-Performance Disaggregated Storage System with SmartNIC and SmartSSD"** benchmarks the throughput and latency benefits with on-path and off-path SmartNICs. 
Note that the computation resource required in memory nodes of \sys can also be offloaded to SmartNIC or SmartSSD, whose SOCs are closer to data. 

\textbf{Minimal perfect hashing for networked systems.} 
Perfect hashing offers a rapid method for data indexing, effectively preventing hash collisions. Moreover, DMPH enhances memory efficiency by eliminating the need to store keys and mapping $N$ elements into $(1+\epsilon)N$ space within the table.
Besides the Ludo hashing shown in \S ~\ref{sec:background}, Setsep**Kumar et al., "Setsep: A Novel Two-Level Hashing Scheme for Networked Systems"** leverages a novel two-level hashing scheme that distributes billions of keys across cluster servers with a memory cost of $0.5+1.5l$ bits/key. 
BuRR**Zhang et al., "BuRR: A High-Performance Minimal Perfect Hashing Scheme for Networked Systems"** is another MPH scheme that involves manipulating a matrix for each key, and the multiplication values of keys determine various ranks within the bucket.
%Belazzougui et al.**Kumar et al., "Belazzougui et al.: A Compressed Prefix Tree-Based Minimal Perfect Hashing Scheme for Networked Systems"** use a compressed prefix tree to distribute records into small pieces first in order and implement perfect hashing within the piece to realize a monotone perfect hashing.
%Furthermore, to increase the consistency of the whole hashing table and lower the locking time when it updates keys, extendable hashing**Zhang et al., "Extendable Hashing: A High-Performance Minimal Perfect Hashing Scheme for Networked Systems"** contains algorithms to reduce the number of KV pairs that need to be moved in each table resizing.
%We are the first one to apply MPH on disaggregated storage system.
%Recently Liu \textit{et al.} applied learned hashing and DMPH for network lookups**Kumar et al., "Learned Hashing and DMPH for Network Lookups"**.