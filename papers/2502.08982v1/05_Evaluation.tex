\vspace{-2ex}
\section{Performance evaluation}
\label{sec:eval}
\vspace{-1ex}



\begin{figure*}[!t]
\centering
\renewcommand\thesubfigure{}
\subfigure[]{
    \includegraphics[width=0.58\textwidth]{Figures/legend.pdf}}\\
\vspace{-6ex}
\setcounter{subfigure}{0}
\renewcommand\thesubfigure{(\alph{subfigure})}
\subfigure[Workload A.]{
    \label{fig:eval:ycsb:a}
    \includegraphics[width=0.205\textwidth]{Figures/cx6-ycsb-1.pdf}}
\hspace{-2.ex}
\subfigure[Workload B.]{
    \label{fig:eval:ycsb:b}
    \includegraphics[width=0.20\textwidth]{Figures/cx6-ycsb-2.pdf}}
    \hspace{-2.5ex}
\subfigure[Workload C.]{
    \label{fig:eval:ycsb:c}
    \includegraphics[width=0.20\textwidth]{Figures/cx6-ycsb-3.pdf}}
    \hspace{-2.5ex}
\subfigure[Workload D.]{
    \label{fig:eval:ycsb:d}
    \includegraphics[width=0.20\textwidth]{Figures/cx6-ycsb-4.pdf}}
    \hspace{-2.5ex}
\subfigure[Workload F.]{
    \label{fig:eval:ycsb:f}
    \includegraphics[width=0.20\textwidth]{Figures/cx6-ycsb-5.pdf}}
\vspace{-3ex}
\caption{\red{Throughput under YCSB benchmark with single memory node thread with Mellanox CX-6.}}
\vspace{-2.ex}
\label{fig:eval:ycsb}
\end{figure*}

\begin{figure*}[!t]
\centering
\renewcommand\thesubfigure{}
\subfigure[]{
    \includegraphics[width=0.58\textwidth]{Figures/legend.pdf}}\\
\vspace{-6ex}
\setcounter{subfigure}{0}
\renewcommand\thesubfigure{(\alph{subfigure})}
\hspace{-1.ex}
\subfigure[Workload A.]{
    \label{fig:eval:cx3:a}
    \includegraphics[width=0.202\textwidth]{Figures/cx3_ycsb_1.pdf}}%{Figures/ycsb-new-1.pdf}}
\hspace{-1.ex}
\subfigure[Workload B.]{
    \label{fig:eval:cx3:b}
    \includegraphics[width=0.20\textwidth]{Figures/cx3_ycsb_2.pdf}}
    \hspace{-2.ex}
\subfigure[Workload C.]{
    \label{fig:eval:cx3:c}
    \includegraphics[width=0.20\textwidth]{Figures/cx3_ycsb_3.pdf}}
    \hspace{-2.ex}
\subfigure[Workload D.]{
    \label{fig:eval:cx3:d}
    \includegraphics[width=0.20\textwidth]{Figures/cx3_ycsb_4.pdf}}
    \hspace{-2.ex}
\subfigure[Workload F.]{
    \label{fig:eval:cx3:f}
    \includegraphics[width=0.20\textwidth]{Figures/cx3_ycsb_5.pdf}}
\vspace{-3ex}
\caption{Throughput under YCSB benchmark with Mellanox CX-3 RNICs.}
\vspace{-3.ex}
\label{fig:eval:cx3}
\end{figure*}


\subsection{Methodology}
\textbf{Testbed.}
\red{We run experiments in two environments. 1) 6 r650 machines from a public cluster CloudLab~\cite{cloudlab}; each of them is equipped with one Two 36-core Intel Xeon Platinum 8360Y CPU at 2.4GHz, 256 GiB DRAM and one Dual-port Mellanox ConnectX-6 (CX-6) 100 GbE NIC with Driver version as MLNX\_OFED\_LINUX-4.9-5.1.0.0. 
We conduct experiments with two shards, and each shard contains 3 machines. We use one machine as the memory node and the other two as compute nodes.
%To emulate its limited CPU, we use the $\mathtt{taskset}$ command to evenly pin the server threads on cores.
The memory node registers the memory with huge pages to reduce RNIC's page cache misses, which is beneficial for memory-intensive applications~\cite{xtore, race}. On compute nodes, we use two coroutines on each client thread to increase the query efficiency (See analysis in Section.~\ref{sec:eval:coros}).
This is the default experiment environment unless otherwise stated. 
2) 9 r320 machines in CloudLab~\cite{cloudlab}, each of them is equipped with one Xeon E5-2450 CPU (8 cores, 2.1Ghz), 16 GiB DRAM, and one Mellanox MX354A Dual port FDR CX3 adapter. We use 1 machine as the memory node and the other 8 as compute nodes.
We utilize 64-byte RDMA messages for all workloads to encapsulate various operation types (RC READ, UD SEND, and UD RECV), ensuring each request is padded to span two cache lines~\cite{guidelines}. We do not use batching at any layer to minimize the latency in all evaluations. %
%CloudLab machines do not provide a combination of weak CPUs but high-performance NICs. Hence the CPU on the memory node could be more powerful than that in an ideal disaggregated memory system.
}
% for camera-ready
%\textbf{Note that this setting gives advantages to the two-sided baselines (RPC-MICA and RPC-Cluster hashing), because they need strong CPU on memory nodes to run index computation tasks, while \sys does not.}}

\noindent\textbf{Workloads.} 
To evaluate the overall performance of \sys and other baselines, we employ YCSB~\cite{ycsb,ycsbc} workloads along with two diverse real-world datasets~\cite{sosd}. These datasets are 
(1) FB, encompassing a random assortment of Facebook user IDs to analyze patterns within social media interactions; (2) OSM, providing digitized infrastructure footprints from Open Street Map to represent geographical and spatial data usage; 
%(3) WIKI, which captures the timestamps of Wikipedia edits in the form of 64-bit unsigned integers, offering insights into high-volume edit activities; and (4) BOOK, containing data on book popularity from Amazon, indicative of consumer behavior and preferences.
To ensure the datasets reflect general, unsorted data conditions, we shuffle them if initially sorted upon loading.
%, and the default Zipfian request distribution const $\theta$ as 0.99. 
Unless specified, we use 8B keys and 8B address values to configure all workloads like existing schemes~\cite{rolex, learnedindex} for comprehensive evaluations. 
For each run, we precondition the memory node and warm up the database with 64 million KV pairs at first and then issue 10M requests to the benchmark on top of it.


\noindent\textbf{Baselines.}
We develop a prototype of \sys based on RDMA libraries rlib and r2~\cite{drtmh} with over 4000 LoC in C++. We compare \sys with the other three baselines, one is a recently proposed one-sided RDMA scheme, RACE hashing~\cite{race}, which utilizes RDMA RC READs for its operations; The other two are two-sided RDMA schemes that operate on RDMA SENDS/RECVs, differing in their underlying data structures -- MICA~\cite{herd, mica} and Cluster hashing~\cite{drtmr}.


\begin{itemize}[left=0em]
\vspace{-.3ex}
    \item \textbf{RACE hashing.}
    RACE hashing~\cite{race} is a representative one-sided RDMA scheme developed recently. It offloads all data operations to compute nodes to free the memory node CPU with one-sided RDMA primitives. RACE Hashing adopts an RDMA-friendly hash table to combine the overflow bucket for collided keys and the hashed bucket. Thus, all the candidate buckets containing the requested key can be read back together.
    We develop RACE hashing with over 1,400 lines of C++ code, excluding the benchmark part that is shared with other baselines.
    \item \textbf{RDMA RPC-MICA.}
    RPC-MICA is a two-sided RDMA-based scheme with a data structure MICA~\cite{mica,herd}, which is an efficient hopscotch hash table and it has been used in existing two-sided RDMA~\cite{herd,fasst}. The overflowed KV pairs can be stored in the bucket adjacent to its hashed bucket.
    We implement hash computation for the bucket number on the compute node and send the queried key's fingerprint and bucket number to save computation on the memory node. 
    We apply the open-source code from MICA~\cite{mica} in our benchmark, utilizing it as the underlying data structure for the RPC-based approach without batching.
    \item \textbf{RDMA RPC-Cluster hashing.}
    RPC-Cluster hashing is a two-sided RDMA baseline with Cluster hashing, a chained-based hash table with associativity, running on memory nodes~\cite{drtmh,drtmr}.
    %DRTM-R~\cite{drtmr} is one of the state-of-the-art approaches that only leverages one-sided RDMA verbs to process data requests. 
    The overflow keys that are hashed to a full bucket will be put in the linked indirect bucket. Each slot in a bucket includes 14 bits of fingerprint for key comparison. We apply the open-source code~\cite{drtm_code} of the cluster hashing as the data backend of our RPC-based scheme suit. 
    % We offload the bucket and fingerprint computation on the compute node for data lookup, and the memory node will complete the following tasks (e.g., fingerprint check and probing) in the experiments.
    \vspace{-1ex}
\end{itemize}







%%%%%%%%%%%%%%%%%%%%%%%% MAIN YCSB
\vspace{-1ex}
\subsection{Performance on YCSB}
\label{sec:eval:ycsb}

%In this section, we compare the aggregated throughput from memory nodes to compute nodes using \sys and other baseline systems with YCSB workloads. We use \texttt{taskset} to bind an equal number of cores to run the benchmark on both compute and memory nodes.

\red{\textbf{Performance with CX-6 RNICs.} We show the throughput of all evaluated methods by increasing the request load of running 8, 12, 20, 72, 108, and 144 compute node threads in a shard.
On the memory node, we consistently allocate only one thread to run on a single core. As shown in Fig~\ref{fig:eval:ycsb}, these five figures illustrate the throughput and latency results under YCSB workloads A, B, C, D, and F, respectively.}
%Note that the YCSB E workload mainly consists of range queries, and this is not the optimization focus of \sys, so we skip the YCSB E part. 



\textbf{\texttt{Get} and \texttt{Update} workloads (YCSB A and B).}
YCSB A and B workloads include 50\% and 5\% data \texttt{Update} respectively and the remaining is \texttt{Get}. 
\sys can achieve 5.50 and 5.82 Mops throughput for YCSB A and B, as shown in Fig.~\ref{fig:eval:ycsb:a} and Fig.~\ref{fig:eval:ycsb:b}. 
All other methods show lower throughput with the same number of threads. 
\sys can provide up to 1.07$\times$ and 1.06$\times$ throughput improvements on workloads A and B respectively, compared to RPC-cluster hashing. 
Compared to other RPC baselines with associative hash tables, the memory node in \sys is offloaded with less computation because it only needs to read the targeted key, and no data probing or traversing is needed to find the targeted value of the key. 
RACE hashing requires three round trips for updating data consistently, significantly increasing the latency and limiting the throughput.
By comparing the results between workloads A and B, 
when more \texttt{Update} requests are issued, \sys spends more computation resources for value rewriting and key checking by reading the underlying KV blocks indicated by the computed MPH slot. 
Hence, \sys under YCSB B provides higher throughput than \sys under YCSB A.


\textbf{\texttt{Get}-only workload (YCSB C).}
For \texttt{Get}-only workload, \sys can achieve 6.01 Mops throughput.
When the number of compute node threads reaches 72, \sys outperforms RACE hashing, MICA, and Cluster hashing by 1.31$\times$, 2.43$\times$, and 1.11$\times$ on total throughput, respectively. The performance of RACE hashing is bottle-necked by its two round trips and the limited RNIC memory to cache queue pair (QP) state of a larger number of reliable connections.
\sys reduces the average memory node's CPU time for data \texttt{Get} request with less computation overhead than the other two RPC-based baselines while looking up a key. 


\textbf{\texttt{Get} and \texttt{Insert} workloads (YCSB D and F).}
YCSB D contains  5\% \texttt{Insert} and 95\% \texttt{Get} operations. 
 YCSB F contains 25\% \texttt{Insert}, 25\% \texttt{Update}, and 50\% \texttt{Get} operations. 
Under YCSB D, \sys still shows the highest throughput among all methods. 
%, which is the most time-consuming operation in \sys. The throughput of \sys drops by 50\% when it comes to YCSB F \todo{compared to YCSB-D is 50\%?}, where 25\% is data insertion and 25\% is data update. 
For \texttt{Insert} operations, \sys will check if a slot in the target bucket is available. Key-checking is also required, and a new seed will be calculated if the target slot stores an existing value. 
%Cache insertion is needed if the hashed bucket is full. 
The high rate of \texttt{Insert} operations in YCSB F pulls the throughput down to 3.62 Mops, which is similar to RPC-Clustering hashing (3.64 Mops) when the number of client threads reaches 144.
%Cluster hashing directly links the value pointers on its hash bucket, which provides a fast way to insert data.


\red{\textbf{Performance with CX-3 RNICs.}
%\red{To evaluate \sys performance on RDMA NICs with diverse bandwidths and the varied scalability of the number of connections. We also run experiments on 9 r320 machines CloudLab~\cite{cloudlab}, each of them is equipped with one Xeon E5-2450 CPU (8 cores, 2.1Ghz), 16 GiB DRAM, and one Mellanox MX354A Dual port FDR CX3 adapter. We use 1 machine as the memory node and the other 8 as compute nodes.}
As shown in Fig.~\ref{fig:eval:cx3}, we show the throughput with the 4 memory node threads and a set of compute node threads numbers 8, 16, 24, 32, 48, and 64, respectively. 
\sys can consistently achieve the highest throughput for read-intensive workloads (A, B, C, and D). 
Significantly, \sys outperforms RACE hashing, MICA, and Cluster hashing by 5.03$\times$, 1.79$\times$, and 1.23$\times$ on total throughput for workload C, respectively. 
When we use a weaker CPU, the advantage of \sys is more significant. Unfortunately, CloudLab does not offer a weaker CPU with a high-performance network. }
%The throughput gap shown with two different network devices explains the factor that \sys can reduce the CPU burden of memory nodes.}



In summary, \sys demonstrates the highest throughput for most types of workload (YCSB A, B, C, and D). For a workload that is \texttt{Insert}-intensive such as YCSB F, \sys provides comparable throughput to other RDMA-RPC methods but still higher than that of one-sided RDMA.









%%%%%%%%%%%%%%%%%%%%%%% SOSD
\vspace{-2ex}
\subsection{\red{Evaluations on Real-World Datasets}}
\label{sec:eval:sosd}

\begin{figure}[!t]
\centering
\renewcommand\thesubfigure{}
\subfigure[]{
    \includegraphics[width=0.48\textwidth]{Figures/legend.pdf}}\\
\vspace{-5ex}
\setcounter{subfigure}{0}
\renewcommand\thesubfigure{(\alph{subfigure})}
\subfigure[Dataset FB, uniform workload.]{
    \label{fig:eval:sosd:a}
    \includegraphics[width=0.23\textwidth]{Figures/cx6-fb-unif.pdf}}
\subfigure[Dataset OSM, uniform workload.]{
    \label{fig:eval:sosd:b}
    \includegraphics[width=0.23\textwidth]{Figures/cx6-osm-unif.pdf}}\\
\vspace{-2.5ex}
\subfigure[Dataset FB, zipfian workload.]{
    \label{fig:eval:sosd:c}
    \includegraphics[width=0.23\textwidth]{Figures/cx6-fb-zipf.pdf}}
\subfigure[Dataset OSM, uniform workload.]{
    \label{fig:eval:sosd:d}
    \includegraphics[width=0.23\textwidth]{Figures/cx6-osm-zipf.pdf}}
\vspace{-2.5ex}
\caption{\red{Data \texttt{Get} throughput performance with SOSD datasets with uniform and zipfian-0.99 workloads.}}
\label{fig:eval:sosd}
\vspace{-3.5ex}
\end{figure}

\red{We leverage the SOSD datasets~\cite{sosd} for evaluations. Fig.~\ref{fig:eval:sosd} illustrates throughput results with the number of compute node threads as 8, 12, 20, 72, 108, and 144 in a shard. We set the number of memory node threads to 1. Each compute node thread issues 10 million key lookup requests selected from the datasets in a uniform or zipfian distribution.} 

\red{Compared to RACE, \sys achieves throughput of 1.38$\times$, 1.35$\times$, 1.39$\times$, and 1.38$\times$ respectively on these four different settings when the number of threads reaches 144.
RACE's performance is constrained by the multiple round trips.
%and the high number of reliable connections between numerous compute node threads and the memory node. \sys leverages the RPC-based approach and fully uses the single core on the memory node. Data accessing from local memory is faster than using one more networking round trip like RACE hashing.
Compared to RPC-MICA and Cluster hashing, \sys achieves a throughput of 2.03$\times$ and 1.1$\times$ respectively on dataset FB when the threads number reaches 144 in Fig.~\ref{fig:eval:sosd:a}.
%MICA designed a hash table with multiple candidate buckets for each key and accommodated eight keys in each bucket. Cluster hashing links the key-offset pairs in the same bucket with value pointers, but the fingerprint (lossy incarnation) and key check are required before accessing data. 
The reason that \sys can outperform them is that \sys can go directly to access data without extra check computation and indirect data accessing to probe the hash chain or buckets.
Also, \sys outperforms RACE hashing, RPC-MICA and RPC-CLuster hashing by 1.35$\times$, 2.05$\times$, and 1.13$\times$ respectively on dataset FB when the workload follows the Zipfian distribution, as shown in Fig.~\ref{fig:eval:sosd:c}. We observe the same trend in performance comparison with the dataset OSM.}









%%%%%%%%%%%%%%%%%% MEM THREADS

%\todo{add the statement of evaluating concurrency for insert/update/delete, and for resizing}

\vspace{-2ex}
\subsection{\red{Scalability with memory node threads}}
\label{sec:eval:cores}

\red{In this set of experiments, we vary the number of memory node threads from 1 to 3 and observe the throughput of different methods using real-world datasets FB and OSM. 
To exhaust the CPU resources on the memory node side, we use four r650 servers as compute nodes with 288 compute node threads.}

\red{Fig.~\ref{fig:eval:cores} shows the throughput of three RDMA-RPC schemes, by varying the memory node threads from 1 to 3. 
The throughput of \sys is around 1.10-1.21$\times$ of Cluster hashing and around 3$\times$ of MICA for dataset FB. 
The results of the two datasets exhibit the fact that as the number of compute node threads increases, the performance ratio between \sys and RPC-Cluster hashing/MICA remains similar.
The reason is that \sys can ease the CPU burden on the memory node and allow it to handle more data requests from the compute node threads by offloading the computation of indexing to compute nodes.}

\begin{figure}[!t]
\centering
\renewcommand\thesubfigure{}
\hspace{1.1ex}
\subfigure[]{
    \includegraphics[width=0.32\textwidth]{Figures/cx6_server_bar.pdf}}\\
\vspace{-5.5ex}
\setcounter{subfigure}{0}
\renewcommand\thesubfigure{(\alph{subfigure})}
\subfigure[Scalability with memory node threads on dataset FB.]{
    \label{fig:eval:cores:a}
    \includegraphics[width=0.232\textwidth]{Figures/cx6_server_threads_1.pdf}}
\hspace{-1.2ex}
\subfigure[Scalability with memory node threads on dataset OSM.]{
    \label{fig:eval:cores:b}
    \includegraphics[width=0.232\textwidth]{Figures/cx6_server_threads_2.pdf}}
\vspace{-2.5ex}
\caption{\red{Throughput vs. the number of memory node threads.}}
\vspace{-3.5ex}
\label{fig:eval:cores}
\end{figure}
\red{The fact that \sys achieves higher relative throughput to other RPC methods under a small number of memory node threads actually demonstrates the main advantage of \sys: achieving high performance when the memory node carries weak CPU power in a disaggregated memory system.}
%From the other side, MICA and Cluster hashing need more CPU to achieve the same throughput as \sys does, especially when the CPU resource is the bottleneck on the memory node in disaggregated systems.

\textbf{\red{Note that the aim of \sys is not to saturate RNIC but to increase the throughput when there are limited CPU resources in a memory node with two-sided RDMA primitives. 
The results in this section show that \sys can achieve higher CPU efficiency with the same throughput goal, and \sys can realize higher throughput with the same CPU resources. }
In disaggregated systems, this can motivate the industry to satisfy the user's throughput goal with less TCO by reducing the CPU resources equipped on memory-optimized cloud instances~\cite{ec2}.}









%%%%%%%%%%%%%%%%%% COROUTINES
\vspace{-1.5ex}
\subsection{\red{Influence of the number of coroutines}}
\label{sec:eval:coros}

\begin{figure}[!t]
\centering
\renewcommand\thesubfigure{}
\hspace{1.1ex}
\subfigure[]{
    \includegraphics[width=0.42\textwidth]{Figures/cx6_coros_bar.pdf}}\\
\vspace{-5.5ex}
\setcounter{subfigure}{0}
\renewcommand\thesubfigure{(\alph{subfigure})}
\subfigure[Latency-throughput curve on YCSB-C with 1 memory node thread.]{
    \label{fig:eval:coros:a}
    \includegraphics[width=0.235\textwidth]{Figures/cx6_coros_1.pdf}}
\hspace{-1.5ex}
\subfigure[Latency-throughput curve on YCSB-C with 2 memory node threads.]{
    \label{fig:eval:coros:b}
    \includegraphics[width=0.235\textwidth]{Figures/cx6_coros_2.pdf}}
\vspace{-3ex}
\caption{\red{Latency vs. the number of coroutines.}}
\vspace{-4ex}
\label{fig:eval:coros}
\end{figure}

\red{The coroutines within compute node threads are designed to yield upon dispatching a request and resume operation upon receiving responses from two-sided RPCs. The default setup of \sys uses two coroutines per thread, but we extend our evaluation to explore the influence of one or more per thread to ascertain the optimal configuration for maximizing server CPU utilization.
Fig.~\ref{fig:eval:coros} studies the latency-throughput performance of \sys in YCSB-C workload with different numbers of coroutines in a compute node thread. 
In Fig.~\ref{fig:eval:cores:a}, we have only one worker thread in the memory node and vary the total of compute node threads as 8,20,72,144 and 216 distributed among three compute nodes, respectively. 
We can observe that a larger number of coroutines results in higher throughput when the number of compute node threads is less than 72, and the latency doubles or triples after the throughput reaches around 6 Mops, the maximum throughput one memory thread can support. 
This phenomenon is similar when the number of memory node threads is 2, as shown in Fig.~\ref{fig:eval:cores:b}, because the CPU resource on the memory node can handle 144 compute node threads, and the total throughput of a memory node can reach to 9.89 Mops. 
However, the extra coroutines will incur high latency of the data query after the number of memory node threads becomes a bottleneck for serving 216 threads.}





%%%%%%%%%%%%%%%%%% LOAD FACTOR
\vspace{-1.5ex}
\subsection{Influence of load factor in DMPH}
\label{sec:eval:lf}

\begin{figure}[!t]
    %\centering
    \renewcommand\thesubfigure{}
    \begin{minipage}[t]{0.232\textwidth}
        \subfigure[]{
            \label{fig:eval:lf}
            \includegraphics[width=\textwidth]{Figures/cx6_load_factor.pdf}}
        \vspace{-7ex}
        \caption{Influence of different load factor set in DMPH.}
    \end{minipage}
    \hspace{-1.5ex}
    \begin{minipage}[t]{0.232\textwidth}
        \subfigure[]{
            \label{fig:eval:kvs}
            \includegraphics[width=\textwidth]{Figures/cx6_number_kvs.pdf}}
        \vspace{-7ex}
        \caption{Influence of the varied number of KV pairs.}
    \end{minipage}
    \vspace{-3ex}
\end{figure}

\red{The load factor in a hash table is the ratio of stored elements to the total number of available slots or buckets. Maintaining an optimal load factor balances memory usage and data operation throughput.
We evaluate the data \texttt{Get} throughput in \sys with varied load factors from 0.75 to 0.95.}

\red{As shown in Fig.~\ref{fig:eval:lf}, \sys can achieve around 6 Mops with 72 data query threads from compute nodes in a shard for the dataset FB.
Similarly, the influence of the varied load factors on the throughput is trivial based on the results of the dataset OSM.}



%%%%%%%%%%%%%%%%%% KVS
\vspace{-1.5ex}
\subsection{Influence of the number of KV pairs}
\label{sec:eval:kvs}
\red{Fig.~\ref{fig:eval:kvs} studies the impact of the number of KV pairs in each shard. We load 20M, 50M, and 80M KV pairs in \sys and evaluate the data \texttt{Get} throughput on two real-world datasets, respectively. 
\sys's read throughput decreases from 6.02 to 5.83 Mops as database size enlarges on the dataset FB. Similarly, we can observe the data read throughput decreases by 3.1\% on the dataset OSM.}









%%%%%%%%%%%%%%%%%% MEM
\vspace{-1.5ex}
\subsection{Memory usage in compute nodes}
\label{sec:eval:mem}

\begin{figure}[t]
\centering
\hspace{-.5ex}
    \includegraphics[width=0.45\textwidth]{Figures/cx6_memory_usage.pdf}
    \vspace{-3ex}
    \caption{Memory usage on compute node with the varied number of KV pairs.}
    \label{fig:eval:mem}
    \vspace{-4ex}
\end{figure}

In a disaggregated memory system, compute nodes are regarded as the ones with rich computing resources but limited memory space. 
To make the memory node serve data requests with the least computation based on RDMA RPC primitives, we offload as much computation to the compute side with the help of DMPH.
In this section, we evaluate the memory cost of \sys on each compute node with the varied number of KV pairs in each shard. The memory usage on a compute node consists of the bucket locator and the seeds array.

As shown in Fig.~\ref{fig:eval:mem}, we vary the load factor used in the DMPH table from 0.80 to 0.95, and we use an 8-bit seed for keys in each bucket. The memory usage at each compute node for 20 million KV pairs per shard is around 12.5MB, and the cost is below 60MB for 100M KV pairs per shard.  
%which is below 60MB for 100 million KV pairs. 
 %, thus the memory ratio of the compute node cached over the memory node index is 6.04\% when the loaf factor is 0.8.
%The actual number of KV pairs in each compute node depends on the total number of KV pairs and the shards. The more shards there are, the less memory space we need to spend on metadata maintenance. It is worthwhile to allocate the specified memory budget to achieve fast data access of memory nodes in \sys compared to learned index-based schemes. 
This is considered a small overhead because recent one-sided RDMA solutions cost hundreds of MBs or more on each compute node for index caching and other purposes~\cite{rolex,xtore}. 
For example, in XStore~\cite{xtore}, 100 million key-value pairs require over 600MB of memory at a compute node without including the cache.










%%%%%%%%%%%%%%%%%% RESIZING
\vspace{-1.5ex}
\subsection{Throughput during index resizing}
\label{sec:eval:resizing}

\begin{figure}[t]
\centering
    \includegraphics[width=0.45\textwidth]{Figures/resizing.pdf}
    \vspace{-3ex}
    \caption{\red{Influence of extendible hashing resizing.}}
    \label{fig:eval:resizing}
    \vspace{-4ex}
\end{figure}


We evaluate the throughput changes during index reconstruction and resizing. 
%To support extendible hash table resizing, we propose to leverage partial bucket locking to enable uninfluenced bucket accessing during the resizing period. 
\red{
%As the number of KV pairs increases with data insertion, hash table resizing will be triggered and implemented with extendible hashing enlarging. Half of the keys in a DMPH table and the elements in the overflowed cache will be moved to the newly allocated DMPH table. The memory node will construct the corresponding bucket locator and seeds array for the new table. 
In this set of experiments, we bulk-load 20M keys to the database with the initial DMPH table to warm up, and we set one compute node with 8, 12, and 16 threads connecting to the memory node running only one thread, respectively.} 
%The memory node needs to maintain the latest seeds and the overflow cache for new data. 
\red{This emulates a challenging scenario because the memory node has limited computing resources to handle both resizing and lookups. The workload running on compute nodes is YCSB D, which contains 5\% data insert and 95\% read.
As shown in Fig.~\ref{fig:eval:resizing}, it takes around 3 seconds to recalculate the bucket locator and the seed for each bucket. \sys still supports partial \texttt{Get} requests during resizing with a decreased throughput by approximately 52\% with only one thread in the memory node. %to support \texttt{Get} to the uninfluenced buckets. 
The CPU contention causes a performance drop, %when handling data lookup requests and reconstructing the MPH table and seeds in the same memory node thread, which is rare and not suggested in practice.
and the performance goes back to normal after resizing.} 








%%%%%%%%%%%%%%%%%% SUMMARY %%%%%%%%%%%%
% key exp findings + a set of nice parameters
\vspace{-1.5ex}
\subsection{Summary of evaluation}
\label{sec:eval:sum}
\red{
\textbf{Data lookup throughput. }
\sys achieves 1.11-2.43$\times$ and 1.23-5.03$\times$ higher throughput than baselines with Mellanox CX-6 100Gb and CX-3 50Gb RNICs in data search workload, respectively.}

\red{\noindent\textbf{Memory usage. }
The memory usage at each compute node for 20 million KV pairs per shard is around 12.5MB per shard, around 5 bits per key, with a load factor of 0.85 in DMPH.}

\red{\noindent\textbf{Scalability of memory node threads.}
When the compute nodes with enough threads exhaust the compute capability on the memory node, \sys can achieve at least 18\% performance advantage over other RPC-based baselines on read workload.}

\red{\noindent\textbf{Load factors in \sys.}
The load factor value in DMPH causes a trivial impact on data lookup throughput with the same compute complexity. We recommend 0.8-0.9 to achieve the balance between memory usage and low frequent resizing, as the low load factor supports more incremental data insertion into the hash table.}


