\section{Related Works}
\label{section: RW}

\subsection{Non-verbal Cues of Anxiety Disorder}
Over the last five decades, researchers have studied nonverbal communication in mental disorders \cite{waxer1977nonverbal, argyle1978non, perez2003nonverbal,foley2010nonverbal, schneier2011fear, gilboa2013more, weeks2019fear, asher2020out, shatz2024nonverbal}. These studies have found that nonverbal cues can play a significant role in diagnosing mental disorders and contribute to therapeutic processes. For instance, nonverbal signs such as a patient’s appearance, behavior, and eye contact are routinely assessed during psychiatric evaluations as part of the mental status examination \cite{foley2010nonverbal}.

Existing research has analyzed video recordings of individuals with anxiety disorders in various scenarios, such as therapy sessions, task performance, video watching, dyadic conversations, etc. One of the earliest studies on this topic was conducted by Waxer \cite{waxer1977nonverbal}, who analyzed the nonverbal cues of individuals with anxiety. In this study, anxious and non-anxious participants (20 participants: 5 anxious males, 5 non-anxious males, 5 anxious females, and 5 non-anxious females) were videotaped at different times during the admission period. One-minute silent session videos were then shared with 46 senior psychologists. They rated ten behavior cue areas on a 10-point scale ranging from ``not anxious at all'' to ``highly anxious'' and described how these features conveyed anxiety. The ten behavior cue areas were the forehead, eyebrows, eyelids, eyes, mouth, head angle, shoulder posture, arm position, torso position, and hands. Further, using Linear regression analysis, hands, eyes, mouth, and torso were identified as a key nonverbal indicator of anxiety.

A major focus of recent studies has been on gaze behavior and its relationship with social anxiety. Schneier et al. \cite{schneier2011fear} explored gaze avoidance in individuals with generalized social anxiety disorder, healthy controls, and undergraduate students. Their findings indicate that avoiding eye contact is associated with social anxiety. Similarly, Weeks et al. \cite{weeks2011exploring, weeks2019fear} conducted multiple studies on behavioral submissiveness in social anxiety. In one study, participants engaged in a role-play task with unfamiliar individuals (confederates), revealing that body collapse and gaze avoidance are linked to social anxiety \cite{weeks2011exploring}. In another study, Weeks et al. used eye-tracking systems to examine participants watching positive and negative video clips, further identifying gaze avoidance as a prominent marker of SAD \cite{weeks2019fear}.

Nonverbal synchrony is another area of investigation in SAD. Asher et al. \cite{asher2020out} analyzed dyadic conversations between individuals with SAD and non-anxious individuals, finding impaired nonverbal synchrony among those with SAD. Similarly, Shatz et al. \cite{shatz2024nonverbal} examined nonverbal synchrony during diagnostic interviews, showing that individuals with SAD displayed lower levels of synchrony and reduced pacing compared to non-anxious counterparts. An in-depth review by Gilboa et al. \cite{gilboa2013more} provides a comprehensive understanding of nonverbal social cues in SAD, synthesizing findings from various studies and emphasizing the role of nonverbal behaviors in the disorder. The findings from these studies underscore the role of nonverbal cues, such as gaze behavior and body posture in understanding and diagnosing SAD. These studies relied on recorded videos and human inference, thus highlighting the need for an automated tool.

%\subsection{Mental disorder detection using visual features}
\subsection{Visual Features of Mental Disorders}
To the best of our knowledge, Cohn et al. \cite{cohn2009detecting} were the first to explore the use of automated visual features from videos for research in mental health detection. They recorded interviews between clinically depressive participants and interviewers. The study used manual and automated facial analysis coding systems (FACS) as feature inputs for machine learning. They achieved accuracies of 88\% with manual features and 79\% with automated features in depression detection. Furthermore, `\textit{AVEC 2011 – The First International Audio/Visual Emotion Challenge}' introduced automated visual features, calculated using dense local appearance descriptors, for affective computing \cite{schuller2011avec}. This was done through a workshop challenge that provided an open dataset to the research community. Later, the development of OpenFace\footnote{\url{https://cmusatyalab.github.io/openface/}}, based on FaceNet \cite{schroff2015facenet}, an advanced deep learning model, offered a unified system for detecting facial features. Later, the updated version, OpenFace 2.0 \cite{baltruvsaitis2016openface}, emerged as a state-of-the-art computer vision toolkit. It enabled researchers to analyze facial behavior and study nonverbal communication without requiring comprehensive programming knowledge.

Most studies that use visual features for mental health detection have focused on depression and stress \cite{cohn2009detecting, schuller2011avec, valstar2014avec, ringeval2019avec, ringeval2017avec, valstar2016avec, valstar2013avec, wang2021multimodal, gavrilescu2019predicting, grimm2022phq, giannakakis2017stress, sun2022estimating}, with limited attention given to anxiety \cite{wang2021multimodal, gavrilescu2019predicting, grimm2022phq, mo2024multimodal, giannakakis2017stress} and even less to SAD \cite{harshit2024eyes, shafique2022towards}. Giannakakis et al. \cite{giannakakis2017stress} used an open-source model to detect the face region of interest and applied Active Appearance Models (AAM) for emotion recognition and facial expression analysis. In their study, participants were recorded with a video camera with extra lighting while undergoing three experimental phases: a social exposure phase, an emotion recall phase, and a stress/mental task phase. The computed features were then used to detect emotional states related to stress and anxiety. They achieved an average accuracy of 87.72\% in stress detection across these phases. Similarly, Sun et al. \cite{sun2022estimating} utilized visual features for remote stress detection. Participants attended an online meeting and self-reported their stress levels on a scale of 1 to 10, which served as the ground truth for a binary stress classifier. The study reported an accuracy of 70.00\% using motion features (eye and head movements) and 73.75\% using facial expressions (action units). In another study, Grimm et al. \cite{grimm2022phq} analyzed participants’ videos captured while they answered open-ended questions. A classifier was trained using GAD-7 scores as ground truth, achieving an area under the curve (AUC) score of 0.71 for the binary classification of anxiety characteristics. Similarly, Gavrilescu et al. \cite{gavrilescu2019predicting} predicted depression, anxiety, and stress using videos captured with high-end cameras while participants watched emotion-inducing clips. They achieved accuracies of 87.2\% for depression, 77.9\% for anxiety, and 90.2\% for stress.

Existing studies on SAD have predominantly focused on eye gaze. In these studies, participants typically complete a performance task involving interviews or the Trier Social Stress Test (TSST). For example, Shafique et al. \cite{shafique2022towards} used participants' eye gaze data captured during a 5-minute general conversation with an examiner, covering topics such as introductions, support, and conflict. Their method achieved an accuracy of 80\% in detecting the severity of SAD. In another study, Harshit et al. \cite{harshit2024eyes} analyzed participant's eye gaze while they performed a speech task as part of the TSST. Using an autoencoder, they extracted latent feature representations (deep features) from the eye gaze data, which was used as features for machine learning models, and achieved 100\% accuracy in detecting participants' anxiety.

In summary, most existing studies focus on interview-based videos or externally induced anxiety tasks, limiting their relevance to real-world, everyday scenarios. Additionally, non-verbal cues, such as facial expressions and head movements, remain underexplored in detecting SAD. To address these gaps, we designed a study set in a social environment where participants were surrounded by unfamiliar individuals and instructed to remain idle without engaging in any activity. Using a low-cost smartphone camera, we recorded videos of the participants' faces, extracted facial features, and analyzed them for insights.