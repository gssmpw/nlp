%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf,authordraft]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Early Anxiety Detection Through Behavioral Cues from Low-Cost Smartphone Camera Visuals}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Ben Trovato}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
% }

% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Anxiety affects a person's family, social, and work life. In 2019, anxiety was the second leading cause of disability among mental disorders in Asia. The high number of anxiety cases and the low number of mental health professionals, especially in low- and middle-income countries, make it essential to explore scalable digital methods for detecting anxiety.

People with anxiety often show non-verbal signs like reduced eye contact and hyper-awareness. In this study, we explored behavioral cues (eye gaze, head movements, facial features, and facial expressions) to predict anxiety. Participants were asked to sit in front of a low-cost smartphone camera while being recorded in the presence of three unfamiliar people. A total of ninety-one participants participated in this study. We used the OpenFace library to extract the behavioral features and then trained a machine-learning model to predict anxiety. We achieved an accuracy of 65\% and an F1-score of 0.72 using the Random Forest algorithm. Our results suggest that these behavioral cues can serve as a promising use case for early anxiety detection, thus prompting timely intervention.
\end{abstract}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Mental health, Anxiety disorder, Behavioral cues, Anxiety detection, Visual features, Human Computer Interaction, Applied Machine Learning}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction} \label{sec:intro}
One in every eight individuals experiences a mental disorder, with anxiety being the most prevalent \cite{Mentaldi93:online}. Anxiety disorders are characterized by excessive fear and worry and can manifest in various ways, including physical symptoms such as a racing heartbeat and sweating, mental symptoms like restlessness and pervasive fear, and behavioral symptoms such as avoidance of social activities \cite{szuhany2022anxiety}. In 2019, anxiety ranked 26th in disability-adjusted life years among global burden diseases in Asia \cite{chen2024burden}. Furthermore, the COVID-19 pandemic led to a significant 26\% increase in the number of individuals suffering from anxiety disorders \cite{Mentaldi93:online}.  

Early detection and intervention are crucial for mitigating the consequences of anxiety disorders, as they can significantly impair one's family, social, and work or academic life \cite{Anxietyd22:online}. While traditional methods for detecting anxiety disorders exist, they are often impractical in low-income and developing countries due to large populations and a limited number of mental health professionals. Generally, anxiety is diagnosed through clinical interviews or self-reported measures \cite{sahu2024wearable}. However, clinical interviews can be time-consuming and require frequent visits to healthcare providers \cite{sahu2024unveiling}, while self-reported measures often suffer from bias \cite{salekin2018weakly}.

The limitations of existing anxiety detection methods \cite{rashid2020predicting}, coupled with the shortage of mental health professionals in developing countries \cite{Globalhe35:online} and the high prevalence of anxiety \cite{Mentaldi93:online}, underscore the need for a scalable digital technological solution. Recent research has investigated wearable sensors to detect anxiety, but the results have not yet been conclusive.

Individuals suffering from anxiety often exhibit non-verbal behavioral cues such as low fixation, hypervigilance, and avoidance of eye contact \cite{harshit2024eyes, lima2019facial}. Therefore, exploring behavioral patterns in anxious participants appears to be a promising avenue for developing digital technology for anxiety detection. Only one study has investigated behavioral cues for anxiety detection \cite{harshit2024eyes}. Harshit et al. \cite{harshit2024eyes} specifically explored eye gaze patterns in anxious and non-anxious individuals during an anxiety-inducing speech task. However, a limitation of their study is the reliance on an external factor, i.e., the speech activity to provoke anxiety, which may not reflect natural anxiety responses in everyday situations.

In this study, we explore the use of behavioral cues recorded via a smartphone camera to develop an anxiety detection model (see Figure \ref{fig:framework}). To address this, we conducted a study where participants sat in front of the camera in the presence of three unfamiliar individuals. Behavioral features, including head position, gaze direction, facial expressions, and action units, were extracted from the recorded videos using the OpenFace library. We trained and tested machine learning models on the collected data using 5-fold cross-validation. We achieved the highest accuracy of 65\% and an F1 score of 0.72 with the Random Forest.

Our findings suggest that these behavioral cues hold the potential for early anxiety detection, enabling timely intervention. The use of a low-cost smartphone for video recording in this paper and further feature extraction indicates that this framework could be explored for anxiety detection in everyday smartphone use.

% Individuals suffering from anxiety often exhibit non-verbal behavioral cues such as low fixation, hypervigilance, and avoidance of eye contact \cite{harshit2024eyes, lima2019facial}. Therefore, exploring behavioral patterns in anxious participants appears to be a promising avenue for developing digital technology for anxiety detection. Only one study has investigated eye gaze during anxiety-provoking activities for detection purposes \cite{harshit2024eyes}. However, this approach is not practical, as individuals cannot be expected to perform anxiety-inducing tasks (such as public speaking or interviews) every time anxiety needs to be assessed, limiting its applicability in real-world settings.

% In this work, we investigate the use of behavioral cues such as head position, gaze direction, and facial expressions of participants recorded through a smartphone camera to create an AI model for anxiety detection. To address this, we conducted a study in which participants were asked to sit in front of the camera in the presence of three unfamiliar individuals.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{images/baseline_paper_framework.pdf}
    \caption{Anxiety detection framework used in this study.}
    \label{fig:framework}
\end{figure*}

\section{Method} \label{sec:method}
\subsection{Data Collection}
The study was approved by the institution's ethics board and conducted by a research assistant (RA). Three participants and the RA took part in a single study session. The RA ensured that the participants were unfamiliar with each other before sending out the study invitations. Upon arrival, the RA explained the study and obtained informed consent from each participant. After consent, camera recordings were initiated, with three individual cameras positioned on stands to record each participant respectively. The RA then instructed the participants to sit idle for two minutes. Following this, the RA asked the participants to complete a survey consisting of a single question about their anxiety state during the last two minutes. Participants rated their anxiety on a Likert scale from 1 to 5, where one indicated ``very anxious'' and five indicated ``very relaxed'' (distribution shown in Figure \ref{fig:histogram_baseline}). The participants' demographic information is presented in Table \ref{tab: participant_demographic}. A total of 111 participants participated in the study; however, 20 participants' data were excluded due to recording issues with the smartphone camera. In 91 participants, 58 were male, and 33 were female. Seventy participants reported an urban home residence, while 21 were from rural areas. The mean age of the participants was 20.71 years, with a standard deviation of 2.35 years.


\begin{table*}[h]
\centering
\caption{Participants demographic information. SR - Self-reported anxiety, Note* - SR report closure 1 represents higher anxiety, i.e., Anxious, and SR report closure to 5 represents relaxed, i.e, Non-Anxious}
\begin{tabular}{@{}cccccc@{}}
\toprule
 & \textbf{count} & \textbf{SR Score} & \textbf{Age}  & \textbf{Gender} & \textbf{Home}\\
  & \textbf{\#} & \textbf{($\mu$, $\sigma$)} & {($\mu$, $\sigma$)} & {(Male, Female)}  &{(Urban, Rural)}\\ \midrule
\textbf{All} & 91& (3.29, 1.03)& (20.71, 2.35)& (58, 33)&(70, 21)\\ \midrule
 \textbf{Anxious}
& 52& (2.52, 0.54)& (20.67, 2.39)& (32, 20)&(45, 7)\\
\textbf{Non-Anxious}& 39& (4.31, 0.47)& (20.77, 2.33)& (26, 13)&(25, 14)\\ \bottomrule
\end{tabular}
\vspace{0.1cm}
\label{tab: participant_demographic}
\end{table*}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/Histogram_Baseline.pdf}
    \caption{Distribution of participants self-reported anxiety}
    \label{fig:histogram_baseline}
\end{figure}

\subsection{Feature Extraction}
Behavioral features were extracted from the recorded two-minute videos using the OpenFace\footnote{\url{https://github.com/TadasBaltrusaitis/OpenFace}} library \cite{baltruvsaitis2016openface}. OpenFace takes a video as input and generates a time series of 709 different features, which are categorized into five main sets: Gaze-related (288 features), Pose (6 features), Facial landmark locations in 2D (136 features) and 3D (204 features), Face shape in rigid (6 features) and non-rigid (34 features) forms, and Facial Action Units intensity (17 features) and presence (18 features). Now, we will discuss each of these feature sets in detail.

% The Basic feature set provides information on the sequence of frames, the number of faces detected in each frame, the video processing timer, confidence levels, and success rates. 

The Gaze features include the gaze direction of both the left and right eyes in the x, y, and z coordinates and gaze angles in the x and y axes. Additionally, it includes 2D eye landmarks in the x and y coordinates and 3D eye landmarks in the x, y, and z dimensions, with the eye region divided into 55 landmark areas (as shown in Figure \ref{fig:eye_lm_left_eye} and \ref{fig:eye_lm_right_eye}). The pose features consist of the head's location and rotation, represented by pitch, yaw, and roll with respect to the camera. Facial landmarks indicate the positions of 67 facial landmarks in both 2D and 3D facial regions (as shown in Figure \ref{fig:facial_lm}). The Rigid and Non-Rigid Shape parameters provide information on the rigid and non-rigid face shapes, respectively. The Facial Action Units capture an individual's facial expressions, including the intensity and presence of 17 and 18 action units, respectively.


\begin{figure*}
  \centering
  
  % First figure (Facial Landmarks)
  \begin{subfigure}{0.8\textwidth}
    \centering
    \includegraphics[scale=0.1]{images/landmark_scheme_68.pdf}
    \caption{Facial Landmarks}
    \label{fig:facial_lm}
  \end{subfigure}
  
  % Second row with left and right eye
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth, height=3.25cm]{images/left_eye.pdf}
    \caption{Left Eye}
    \label{fig:eye_lm_left_eye}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth, height=3.25cm]{images/right_eye.pdf}
    \caption{Right Eye}
    \label{fig:eye_lm_right_eye}
  \end{subfigure}
  \caption [Image credit]{Landmarks in Face, left Eye, and right Eye (Image credit\protect\footnotemark).}
  \label{fig:landmark}
\end{figure*}



% \begin{figure}
%   \centering
  
%   \begin{subfigure}{0.5\textwidth}
%     \centering
%     \includegraphics[scale=0.1]{images/landmark_scheme_68.pdf}
%     \caption{Facial Landmarks}
%     \label{fig:facial_lm}
%   \end{subfigure}
  
%   \vspace{0.5cm} % Add vertical space to separate the first image
  
%   \begin{subfigure}{0.45\textwidth}
%     \centering
%     \includegraphics[width=0.9\textwidth, height=3cm]{images/left_eye.pdf}
%     \caption{Left Eye}
%     \label{fig:eye_lm_left_eye}
%   \end{subfigure}
  
%   \hfill
  
%   \begin{subfigure}{0.45\textwidth}
%     \centering
%     \includegraphics[width=0.9\textwidth, height=3cm]{images/right_eye.pdf}
%     \caption{Right Eye}
%     \label{fig:eye_lm_right_eye}
%   \end{subfigure}
%   \caption{Landmarks in Face, left Eye, and right Eye. (Image credit\tablefootnote{OpenFace - \url{https://github.com/TadasBaltrusaitis/OpenFace/wiki/Output-Format}})}
%   \label{fig:landmark}
% \end{figure}

\subsection{Data Preparation and Feature Selection}
The output of OpenFace is in time series format. For example, if a participant's video is 2 minutes long with a sampling rate of 30 frames per second, the extracted feature matrix will have dimensions of 3600 × 704, where 3600 represents the total number of frames, and 704 denotes the features extracted per frame. Now, to reduce this dimensionality, we computed statistical features, i.e., each feature's mean and standard deviation across all frames \cite{sun2022estimating}, resulting in a single feature vector of size 1 × 1418 for each participant. Further, we used correlation analysis for feature selection, dropping the highly correlated features based on a correlation threshold of 0.75, as suggested in the literature on affective computing \cite{rashid2020predicting}.

We trained and tested the classification model using different dataset configurations, as shown in Figure \ref{fig:dataset_config}. This experiment was done to determine which statistical features, i.e., mean or standard deviation, work best for anxiety prediction.

% Since the output from OpenFace is in time series format, we computed statistical features such as the mean and standard deviation for all features \cite{sun2022estimating}, excluding the Basic features, which were dropped for any analysis. We used correlation analysis for feature selection, dropping one of the highly correlated features based on a correlation threshold of 0.75, as suggested in the literature on affective computing \cite{rashid2020predicting}.
% \hl{Nilesh}
% Initially, we had 705 features for the mean and 705 features for the standard deviation, resulting in a dataset shape of 91 x 1418. We trained and tested the classification model using different dataset configurations, as shown in Table \ref{tab:evaluation_table}. This experiment aimed to determine which statistical features, i.e., mean or standard deviation, work best for anxiety prediction.

\subsection{Anxiety Classification}
We used the anxiety levels reported by participants as the ground truth. Participants who rated themselves 1, 2, or 3 on the Likert scale were labeled as the anxious group, while those who rated themselves 4 or 5 were labeled as the non-anxious group, thus creating binary labels \cite{harshit2024eyes}. We employed four machine learning classification models, including two classical Machine Learning (ML) models, i.e., Random Forest (RF), Support Vector Machine (SVM), AdaBoost, and XGBoost.

\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{images/dataset.pdf}
    \caption{Dataset Configuration used for training classification model}
    \label{fig:dataset_config}
\end{figure*}

% Add footnote text outside the figure
\footnotetext{OpenFace - \url{https://github.com/TadasBaltrusaitis/OpenFace/wiki/Output-Format}}
\section{Results}
We used 5-fold subject-independent cross-validation\footnote{Cross Validation - \url{https://scikit-learn.org/stable/modules/cross_validation.html}} using the scikit-learn library. Further, average accuracy and F1 score were used as classification evaluation metrics. The performance of the classification algorithms was assessed across various dataset configurations (see Figure \ref{fig:dataset_config}: $\mu$ of each feature (D1), Reduced $\mu$ Features (D2), $\sigma$ of each Features (D3), Reduced $\sigma$ Features (D4), $\mu$ of each feature and $\sigma$ of each feature (D5), Reduced $\mu$ + Reduced $\sigma$ Features (D6), and Reduced ($\sigma$ + $\mu$) Features (D7). 

The results of the classification models are presented in Table \ref{tab:evaluation_table}. We got the highest accuracy of 65\% and an F1 score of 0.72 using the Random Forest model with the combined mean and standard deviation features. 

% The results of the classification tasks are presented in Table \ref{tab:evaluation_table}. The highest accuracy of 65\% was achieved by the Random Forest model with the combined mean and standard deviation features, while the highest F1 score of 73\% was attained by the Support Vector Machine for all mean features, reduced mean features, combined reduced features, and reduced combined mean and standard deviation features. Notably, both Random Forest and AdaBoost exhibited a balance between accuracy and F1 score.
%--------------
\begin{table*}
\caption{Classification results with 5-fold cross-validation. Acc refers to accuracy. Both accuracy and F1 score are averaged across all folds.}
\centering
\tiny
\label{tab:evaluation_table}
{%
\begin{tabular}{lclclclc}
\toprule
\textbf{Dataset Configuration} & \textbf{\# Features} && \textbf{Classifier}  && \textbf{Acc}  && \textbf{F1} \\ \midrule
 % -& -& & Baseline& & 0.50& &0.50\\ \midrule
\multirow{4}{*}{\textbf{D1: $\mu$ of each Features}} & \multirow{4}{*}{709}  && RF  && 0.54  && 0.63 \\
 &   && SVM  && 0.57  && 0.73 \\
 &   && AdaBoost  && 0.42  && 0.51 \\
 &   && XGBoost  && 0.52  && 0.6 \\ \midrule
\multirow{4}{*}{\textbf{D2: Reduced $\mu$ Features}} & \multirow{4}{*}{75}  && RF  && 0.58  && 0.69 \\
 &   && SVM  && 0.57  && 0.73 \\
 &   && AdaBoost  && 0.48  && 0.57 \\
 &   && XGBoost  && 0.52  && 0.58 \\ \midrule
\multirow{4}{*}{\textbf{D3: Only $\sigma$ of each Features}} & \multirow{4}{*}{709}  && RF  && 0.57  && 0.66 \\
 &   && SVM  && 0.56  && 0.69 \\
 &   && AdaBoost  && 0.49  && 0.59 \\
 &   && XGBoost  && 0.57  && 0.63 \\ \midrule
\multirow{4}{*}{\textbf{D4: Reduced $\sigma$ Features}} & \multirow{4}{*}{68}  && RF  && 0.55  && 0.63 \\
 &   && SVM  && 0.56  && 0.72 \\
 &   && AdaBoost  && 0.51  && 0.57 \\
 &   && XGBoost  && 0.51  && 0.57 \\ \midrule
\multirow{4}{*} {\textbf{D5: $\mu$ of each Features and $\sigma$ of each Features}} & \multirow{4}{*}{1418}  && RF  && 0.65  && 0.72 \\
 &   && SVM  && 0.57  && 0.73 \\
 &   && AdaBoost  && 0.44  && 0.51 \\
 &   && XGBoost  && 0.53  && 0.6 \\ \midrule
\multirow{4}{*} {\textbf{D6: Reduced $\mu$ + Reduced $\sigma$ Features}} & \multirow{4}{*}{143}  && RF  && 0.54  && 0.64 \\
 &   && SVM  && 0.57  && 0.73 \\
 &   && AdaBoost  && 0.53  && 0.6 \\
 &   && XGBoost  && 0.56  && 0.62 \\ \midrule
\multirow{4}{*} {\textbf{D7: Reduced ($\mu$ + $\sigma$) Features}}& \multirow{4}{*}{114}  && RF  && 0.51  && 0.62 \\
 &   && SVM  && 0.57  && 0.73 \\
 &   && AdaBoost  && 0.63  && 0.69 \\
 &   && XGBoost  && 0.53  && 0.59 \\ \bottomrule 
\end{tabular}%
}
\end{table*}


% \begin{table}
% \caption{Classification results with 5-fold cross-validation. Acc refers to accuracy. Both accuracy and F1 score are averaged across all folds.}
% \centering
% \scriptsize
% \label{tab:evaluation_table}
% {%
% \begin{tabular}{ccccc}
% \toprule
% \textbf{Input Feature} & \textbf{\# Features}& \textbf{Classifier} & \textbf{Acc} & \textbf{F-score} \\ \midrule
% \multirow{4}{*}{\textbf{All mean}} & \multirow{4}{*}{709} & RF & 0.54 & 0.63 \\
%  &  & SVM & 0.57 & 0.73 \\
%  &  & AdaBoost & 0.42 & 0.51 \\
%  &  & XGBoost & 0.52 & 0.6 \\ \midrule
% \multirow{4}{*}{\textbf{Reduced mean}} & \multirow{4}{*}{75} & RF & 0.58 & 0.69 \\
%  &  & SVM & 0.57 & 0.73 \\
%  &  & AdaBoost & 0.48 & 0.57 \\
%  &  & XGBoost & 0.52 & 0.58 \\ \midrule
% \multirow{4}{*}{\textbf{All std}} & \multirow{4}{*}{709} & RF & 0.57 & 0.66 \\
%  &  & SVM & 0.56 & 0.69 \\
%  &  & AdaBoost & 0.49 & 0.59 \\
%  &  & XGBoost & 0.57 & 0.63 \\ \midrule
% \multirow{4}{*}{\textbf{Reduced std}} & \multirow{4}{*}{68} & RF & 0.55 & 0.63 \\
%  &  & SVM & 0.56 & 0.72 \\
%  &  & AdaBoost & 0.51 & 0.57 \\
%  &  & XGBoost & 0.51 & 0.57 \\ \midrule
% \textbf{All mean} & \multirow{4}{*}{1418} & RF & 0.65 & 0.72 \\
% \multirow{2}{*}{\textbf{+}} &  & SVM & 0.57 & 0.73 \\
%  &  & AdaBoost & 0.44 & 0.51 \\
% \textbf{All std} &  & XGBoost & 0.53 & 0.6 \\ \midrule
% \textbf{Reduced mean} & \multirow{4}{*}{143} & RF & 0.54 & 0.64 \\
% \multirow{2}{*}{\textbf{+}} &  & SVM & 0.57 & 0.73 \\
%  &  & AdaBoost & 0.53 & 0.6 \\
% \textbf{Reduced std} &  & XGBoost & 0.56 & 0.62 \\ \midrule
% \textbf{Reduced }& \multirow{4}{*}{114} & RF & 0.51 & 0.62 \\
% \textbf{(all mean} &  & SVM & 0.57 & 0.73 \\
% \textbf{+} &  & AdaBoost & 0.63 & 0.69 \\
% \textbf{all std)} &  & XGBoost & 0.53 & 0.59 \\ \bottomrule 
% \end{tabular}%
% }
% \end{table}
%------------------
\section{Discussion}
The obtained result suggests that the behavioral cues extracted from participants' videos can serve as predictive indicators for detecting anxiety. The classification evaluation metric achieved by the classical ML models suggests that the ML models can learn the distinguishing features between anxious and non-anxious groups. Furthermore, we found that the individual mean and standard deviation features performed poorly when assessed separately, but their combination yielded better results in classification.

\subsection{Implications}
Early detection is crucial for facilitating timely interventions and promoting recovery. Through our analysis, we found that behavioral cues such as head pose, facial landmarks, and eye gaze patterns extracted from videos can predict anxiety and can be explored for the early detection of anxious individuals. 

In this study, we employed a low-cost smartphone camera, demonstrating that anxiety detection through behavioral cues can be feasibly integrated into real-world settings. This technology has the potential to be incorporated into everyday smartphones, enabling early detection and monitoring of anxiety. Moreover, the extension of this work can support mental health professionals in routine assessments and interventions, thus reducing the mental healthcare gap especially in low-income and developing countries

% Additionally, our approach holds significant promise for reducing the mental healthcare gap in low-income and developing countries.

\subsection{Limitations}
% \noindent \textbf{Limitations}: 
Our study represents a pioneering step in using behavioral cues from upper body videos for detecting anxiety. However, it has certain limitations, which are outlined below:
\begin{itemize}
    \item \textit{Limited Sample Size}: We demonstrated the application of visual features for anxiety detection with only 91 participants, which may have affected the learning of the ML models. However, recruiting human participants for data collection is inherently challenging.
    \item \textit{Loss of Temporal Information}: While we computed statistical features from the time series data, this approach may have missed valuable temporal information. 
    \item \textit{Limited Statistical Features}: We only computed mean and standard deviation as the statistical features, which may have affected our results. However, calculating a larger number of statistical features with a fixed number of data points can lead to overfitting.
\end{itemize}

\section{Future Work}
Anxiety detection through behavioral cues in videos can be further enhanced by expanding data collection to real-world settings, where data can be gathered ``in the wild''. Moreover, future work could experiment with different types of cameras for data collection. Using a low-cost smartphone camera in our study may have contributed to the observed accuracy limitation of 65\%. Additionally, this study was conducted on a university student population; future research should aim to collect data from diverse age groups and demographics to increase the generalizability of the findings.

We only utilized two statistical features in our analysis, which future studies could expand upon and explore further. While we applied classical machine learning (ML) models, future work could benefit from employing deep learning (DL) models, as they can handle temporal data. In extension, we plan to use DL models, such as Convolutional Neural Networks (CNN), for feature extraction from temporal OpenFace outputs and subsequent classification in the future.


\section{Conclusion}
This study is the first to explore the use of behavioral features (eye gaze, head pose, facial landmarks, and action units) extracted from upper-body participants' videos for anxiety detection. We trained classical machine learning models using these behavioral features and self-reported data provided by participants and achieved the highest accuracy of 65\%. These findings suggest that behavioral features hold promise for further exploration in anxiety detection and could potentially be leveraged for early detection and monitoring of anxiety.

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}



\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
