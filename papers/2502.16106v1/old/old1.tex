%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
% \documentclass[authordraft]{acmart}
\documentclass[sigconf,authordraft]{acmart} % Use this later

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{adjustbox}
\usepackage[utf8]{inputenc}
\usepackage{url} % For formatting URLs
\usepackage{hyperref} % For creating clickable links
\usepackage{enumitem} % For customizing enumerate labels
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage{wrapfig}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{array}
\usepackage{tabularx}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{soul}
\usepackage{colortbl}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Early Anxiety Detection Through Behavioral Cues from Low-Cost Smartphone Camera Visuals}


\begin{abstract}
\hl{fixed}
Individuals with social anxiety often exhibit non-verbal signs such as reduced eye contact, restlessness, and gaze fixation. Our study, AnxietyFaceTrack, presents a non-intrusive method for detecting anxiety through a video captured by a low-cost smartphone camera. In our study, 91 participants were asked to sit idle in a social setting within a controlled environment. Their facial videos were captured using a low-cost smartphone camera (approx. \$81). We analyzed various facial features, such as eye movement, head position, facial landmarks, and facial action units. Self-reported survey data was used to establish ground truth for multiclass classifications (anxious, neutral, and non-anxious) and binary classification (anxious versus neutral, anxious versus non-anxious, and neutral versus non-anxious). Our results show that the Random Forest classifier achieved the highest accuracy, with 91.0\% in multiclass classification and an average of 92.33\% across binary classifications. Post-hoc analysis revealed that head rotation along the x-axis, facial edge features, and eye landmarks were among the key contributors for identifying the anxious class. Our approach provides a non-intrusive solution that can be integrated into daily-use smartphones for continuous anxiety monitoring, enabling early interventions.

\end{abstract}


\keywords{Mental health, Anxiety disorder, Behavioral cues, Anxiety detection, Visual features, Human Computer Interaction, Applied Machine Learning}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
\begin{teaserfigure}
    \centering
    \includegraphics[width=1\linewidth]{images/baseline_paper_framework.pdf}
    \caption{Anxiety detection framework used in this study.}
    \label{fig:framework}
\end{teaserfigure}
%% page.


\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\begin{figure*}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/baseline_paper_framework.pdf}
    \caption{Anxiety detection framework used in this study.}
    \label{fig:framework}
\end{figure*}

\section{Introduction} \label{sec:intro}

Social Anxiety Disorder (SAD) is characterized by excessive fear and worry and can manifest in various ways, including physical symptoms such as a racing heartbeat and sweating, mental symptoms like restlessness and pervasive fear, and behavioral symptoms such as avoidance of social activities \cite{szuhany2022anxiety}. One in every eight individuals experiences a mental disorder, with SAD being the most prevalent \cite{Mentaldi93:online}. According to the Global Burden of Disease 2019, anxiety disorder ranks second leading mental health-related contributor to disability-adjusted life-years (DALYs) and years lived with disability (YLDs) globally \cite{xiong2022trends}. Furthermore, the COVID-19 pandemic led to a significant 26\% increase in the number of individuals suffering from anxiety disorders \cite{Mentaldi93:online}.  

Currently, SAD diagnosis heavily relies upon traditional methods due to the absence of any reliable, objective markers (or measures) of anxiety disorder. The traditional method includes clinical interviews and clinically validated retrospective self-reported questionnaires. However, these traditional methods have limitations, such as clinical interviews are prone to human bias and rely on the subject self-motivation to attend the interviews that require multiple sessions, while self-reporting relies on the subject willingness to convey their behaviors and is prone to recall bias. Thus, given the high prevalence of SAD, there is a need for automated, reliable measures that are not susceptible to human bias.


Ongoing research on mental disorder detection has explored unobtrusive objective markers using methods such as wearable sensors, speech analysis, and mobile phone data \cite{rashid2020predicting, salekin2018weakly}. However, the findings remain inconclusive. While facial images and videos have shown promise in detecting depression \cite{nepal2024moodcapture, ringeval2019avec}, there is a notable lack of studies focusing on the detection of SAD using these methods. Prior research indicates that individuals with SAD often exhibit behavioral and non-verbal cues \cite{gilboa2013more}, including restlessness \cite{Chand2023}, reduced motion \cite{Chand2023}, avoidance of eye contact \cite{schneier2011fear}, gaze fixation, slumped posture, and closed body language \cite{weeks2011exploring}.


Existing research on detecting mental disorders or emotional states from video predominantly relies on participants engaging in structured, anxiety-inducing tasks, such as delivering speeches \cite{harshit2024eyes}, introducing themselves \cite{shafique2022towards}, or watching stressful videos \cite{}. These studies often depend on high-end recording equipment, such as RGB-Depth cameras, which may not be feasible for widespread use. However, there is a significant gap in exploring whether anxiety can be detected in natural, unstaged social interactions using low-cost video cameras without requiring participants to engage in specific activities. Addressing this gap is critical for making anxiety detection more accessible and applicable to real-world scenarios.


To address the gap in detecting SAD in natural social settings without requiring participants to perform specific activities, we developed \textit{AnxietyFaceTrack}, a non-intrusive recording approach. It enables the observation of natural expressions of social anxiety without the influence of artificial tasks or prompts. For the first time, we implemented this approach by inviting participants, unfamiliar with one another, to sit in trios in a simulated social scenario. Each participant was positioned to face unknown individuals seated in front, left, and right, replicating real-life situations of encountering strangers. During the 2-minute session, participants were recorded using a dedicated low-cost smartphone camera (approximately \$81), which captured their upper body, including the face, shoulders, and chest. This design ensures accessibility and ecological validity, providing valuable insights into behavioral markers of SAD in naturalistic contexts.

A total of 91 university students participated in the study. Behavioral features, such as head position, gaze direction, and facial expressions, were extracted from the recorded videos and used to train a classification model for anxiety detection (see Figure \ref{fig:framework}). We developed a multiclass classification model to categorize participants as anxious, neutral, or non-anxious, using self-reported ground truth labels provided by the participants. This multiclass approach was adopted to account for neutral behaviors, which were observed among both SAD and non-SAD participants, and to reduce potential bias in the classification process. Additionally, we evaluated binary classification performance by excluding neutral participants to focus on the distinction between anxious and non-anxious behaviors. For example, in the ``anxious versus non-anxious'' model, only participants labeled as anxious or non-anxious were included.

The key contributions of our work are as follows:
\begin{enumerate}
    \item We present a non-intrusive approach for detecting anxiety in normal settings using a low-cost smartphone camera that can integrated into daily use smartphones for continuous monitoring of anxiety, thus prompting early interventions.
    \item We evaluated several machine learning models for anxiety detection using facial features. We tested these models on 669 facial features and their subsets. Our results show that the random forest model outperformed others in nearly all classification metrics for both multiclass (Accuracy- 91\%, F1 score- 0.90, AUC- 0.98) and binary classification (Anxious vs. Neutral: Accuracy- 92\%, F1 score- 0.91, AUC- 0.98; Anxious vs. Non-Anxious: Accuracy- 92\%, F1 score- 0.89, AUC- 0.98; Neutral vs. Non-Anxious: Accuracy- 93\%, F1 score- 0.94, AUC- 0.98).
    \item We identified key features that helped the model correctly identify anxious participants and other classes. For example, larger head rotation along the X-axis, face edge features (such as jawline points: x\_16, Y\_4, x\_4, Y\_11, Y\_16), and eye landmarks (e.g., eye\_lmk\_x\_38, eye\_lmk\_x\_10) were important for anxiety detection in our AnxietyFaceTrack study.
    \item We also analyzed the model’s bias and found that it performed better for females. This may be because females generally show stronger facial expressions than males.
\end{enumerate}

\textit{AnxietyFaceTrack} contributes to affective computing, showcasing the use case of facial features captured through videos for anxiety detection. The use of low-cost smartphone cameras and machine learning models in our  study using facial cues offers a practical solution for continuously monitoring mental disorders, thus reducing the treatment gap and prompting early interventions. 

The paper is structured as follows: Section X presents related work on anxiety detection and studies that use videos for mental disorders. Section Y explains our AnxietyFaceTrack study, participant demographics, ground truth, and the analysis methods used for anxiety detection. Section X discusses our results and the ablation study conducted to draw inferences about anxiety detection. Section P presents the important features that influenced the detection of anxiety, while Section Q examines the bias in the trained models. Section Z discusses the study’s findings, implications, and limitations, while Section D provides the conclusion.


% \begin{figure*}
%     \centering
%     \includegraphics[width=1\linewidth]{images/baseline_paper_framework.pdf}
%     \caption{Anxiety detection framework of the \textit{AnxietyFaceTrack}.}
%     \label{fig:framework}
% \end{figure*}

\section{Related Works}

\subsection{Non-verbal cues of Anxiety Disorder}
Over the last five decades, researchers have studied nonverbal communication in mental disorders \cite{waxer1977nonverbal, argyle1978non, perez2003nonverbal,foley2010nonverbal, schneier2011fear, gilboa2013more, weeks2019fear, asher2020out, shatz2024nonverbal}. These studies have found that nonverbal cues can play a significant role in diagnosing mental disorders and contribute to therapeutic processes. For instance, nonverbal signs such as a patient’s appearance, behavior, and eye contact are routinely assessed during psychiatric evaluations as part of the mental status examination \cite{foley2010nonverbal}.

Existing research has analyzed video recordings of individuals with anxiety disorders in various scenarios, such as therapy sessions, task performance, video watching, dyadic conversations, etc. One of the earliest studies on this topic was conducted by Waxer \cite{waxer1977nonverbal}, who analyzed the nonverbal cues of individuals with anxiety. In this study, anxious and non-anxious participants (20 participants: 5 anxious males, 5 non-anxious males, 5 anxious females, and 5 non-anxious females) were videotaped at different times during the admission period. One-minute silent session videos were then shared with 46 senior psychologists. They rated ten behavior cue areas on a 10-point scale ranging from ``not anxious at all'' to ``highly anxious'' and described how these features conveyed anxiety. The ten behavior cue areas were the forehead, eyebrows, eyelids, eyes, mouth, head angle, shoulder posture, arm position, torso position, and hands. Further, using Linear regression analysis, hands, eyes, mouth, and torso were identified as a key nonverbal indicator of anxiety.

A major focus of recent studies has been on gaze behavior and its relationship with social anxiety. Schneier et al. \cite{schneier2011fear} explored gaze avoidance in individuals with generalized social anxiety disorder, healthy controls, and undergraduate students. Their findings indicate that avoiding eye contact is associated with social anxiety. Similarly, Weeks et al. \cite{weeks2011exploring, weeks2019fear} conducted multiple studies on behavioral submissiveness in social anxiety. In one study, participants engaged in a role-play task with unfamiliar individuals (confederates), revealing that body collapse and gaze avoidance are linked to social anxiety \cite{weeks2011exploring}. In another study, Weeks et al. used eye-tracking systems to examine participants watching positive and negative video clips, further identifying gaze avoidance as a prominent marker of SAD \cite{weeks2019fear}.

Nonverbal synchrony is another area of investigation in SAD. Asher et al. \cite{asher2020out} analyzed dyadic conversations between individuals with SAD and non-anxious individuals, finding impaired nonverbal synchrony among those with SAD. Similarly, Shatz et al. \cite{shatz2024nonverbal} examined nonverbal synchrony during diagnostic interviews, showing that individuals with SAD displayed lower levels of synchrony and reduced pacing compared to non-anxious counterparts. An in-depth review by Gilboa et al. \cite{gilboa2013more} provides a comprehensive understanding of nonverbal social cues in SAD, synthesizing findings from various studies and emphasizing the role of nonverbal behaviors in the disorder. The findings from these studies underscore the role of nonverbal cues, such as gaze behavior and body posture in understanding and diagnosing social anxiety disorder

\subsection{Mental disorder detection using visual features}
To the best of our knowledge, Cohn et al. \cite{cohn2009detecting} were the first to explore the use of automated visual features from videos for research in mental health detection. They recorded interviews between clinically depressive participants and interviewers. The study used manual and automated facial analysis coding systems (FACS) as feature inputs for machine learning. They achieved accuracies of 88\% with manual features and 79\% with automated features in depression detection. Furthermore, `\textit{AVEC 2011 – The First International Audio/Visual Emotion Challenge}' introduced automated visual features, calculated using dense local appearance descriptors, for affective computing \cite{schuller2011avec}. This was done through a workshop challenge that provided an open dataset to the research community. Later, the development of OpenFace\footnote{\url{https://cmusatyalab.github.io/openface/}}, based on FaceNet \cite{schroff2015facenet}, an advanced deep learning model, offered a unified system for detecting facial features. Later, the updated version, OpenFace 2.0 \cite{baltruvsaitis2016openface}, emerged as a state-of-the-art computer vision toolkit. It enabled researchers to analyze facial behavior and study nonverbal communication without requiring comprehensive programming knowledge.

Most studies that use visual features for mental health detection have focused on depression and stress \cite{cohn2009detecting, schuller2011avec, valstar2014avec, ringeval2019avec, ringeval2017avec, valstar2016avec, valstar2013avec, wang2021multimodal, gavrilescu2019predicting, grimm2022phq, giannakakis2017stress, sun2022estimating}, with limited attention given to anxiety \cite{wang2021multimodal, gavrilescu2019predicting, grimm2022phq, mo2024multimodal, giannakakis2017stress} and even less to SAD \cite{harshit2024eyes, shafique2022towards}. Giannakakis et al. \cite{giannakakis2017stress} used an open-source model to detect the face region of interest and applied Active Appearance Models (AAM) for emotion recognition and facial expression analysis. In their study, participants were recorded with a video camera with extra lighting while undergoing three experimental phases: a social exposure phase, an emotion recall phase, and a stress/mental task phase. The computed features were then used to detect emotional states related to stress and anxiety. They achieved an average accuracy of 87.72\% in stress detection across these phases. Similarly, Sun et al. \cite{sun2022estimating} utilized visual features for remote stress detection. Participants attended an online meeting and self-reported their stress levels on a scale of 1 to 10, which served as the ground truth for a binary stress classifier. The study reported an accuracy of 70.00\% using motion features (eye and head movements) and 73.75\% using facial expressions (action units). In another study, Grimm et al. \cite{grimm2022phq} analyzed participants’ videos captured while they answered open-ended questions. A classifier was trained using GAD-7 scores as ground truth, achieving an AUC score of 0.71 for the binary classification of anxiety characteristics. Similarly, Gavrilescu et al. \cite{gavrilescu2019predicting} predicted depression, anxiety, and stress using videos captured with high-end cameras while participants watched emotion-inducing clips. They achieved accuracies of 87.2\% for depression, 77.9\% for anxiety, and 90.2\% for stress.

Existing studies on SAD have predominantly focused on eye gaze. In these studies, participants typically complete a performance task involving interviews or the Trier Social Stress Test (TSST). For example, Shafique et al. \cite{shafique2022towards} used participants' eye gaze data captured during a 5-minute general conversation with an examiner, covering topics such as introductions, support, and conflict. Their method achieved an accuracy of 80\% in detecting the severity of SAD. In another study, Harshit et al. \cite{harshit2024eyes} analyzed participant's eye gaze while they performed a speech task as part of the TSST. Using an autoencoder, they extracted latent feature representations (deep features) from the eye gaze data, which was used as features for ML models, and achieved 100\% accuracy in detecting participants' anxiety.

In summary, there is a research gap in analyzing visual features such as facial expressions and head movements for Social Anxiety Disorder (SAD) detection. Additionally, existing studies have predominantly relied on interview-based videos, externally induced anxiety tasks, or emotion-inducing videos, which limit their applicability to real-world, day-to-day scenarios. To address this gap, we designed a study with a social scenario where participants were surrounded by unfamiliar people and were asked to sit idle without engaging in any activities. The participant's facial video was recorded using a low-cost smartphone camera, and further facial features were extracted and used for analysis.

\section{Method} \label{sec:method}
In this section, we present the study design of \textit{AnxietyFaceTrack}, participants' demographics, ground truth collection, feature extraction, and classification models.

\subsection{Study Design}
We recruited participants from across the home institute using an email advertisement. A dedicated email was sent to the student community with information related to the study and the Google form to fill out for the interested participants. The participants filled out their demographic information, such as age, gender, current educational program, location of home residence, and preferred time slot. Additionally, the participants' email and phone numbers were collected so the research assistant (RA) could contact them on the study day. 

The day before the study, the RA sent an email to the participants to confirm their availability for a specific time slot. Further, a text message was sent one hour before the study session, confirming the location and time for participation. Three participants were invited to the lab for each study session. The RA ensured that the three participants were unfamiliar and did not know each other. The purpose of inviting three unfamiliar participants was to create a socially anxious situation during the study.

Upon arrival, the participants were seated around a rectangular table with rounded edges. Each session involved three participants and a RA. The participants were labeled as P1, P2, and P3 for each study session. The seating arrangement is shown in Figure \ref{fig:sitting_position}: P1 was seated to the left of the RA, P2 was directly opposite the RA, and P3 was to the right of the RA, where P1 and P3 faced each other, while P2 faced the RA. This arrangement ensured that each participant faced an unfamiliar person, creating a socially anxious scenario. The RA explained the study to the participants and distributed the Participant Information Sheet (PIS) and the Informed Consent Form (ICF). After collecting the signed consent forms, the RA obtained permission to start camera recordings. Three individual smartphones, labeled C1, C2, and C3, with 13-megapixel back cameras were used to record P1, P2, and P3, respectively (see Figure \ref{fig:sitting_position}).  The ``Background Video Recorder'' app was selected for its ability to record video even with the screen off, which was not possible with the smartphone’s default camera app. The settings used in the video recording app are detailed in Table \ref{tab:video_recording_app_spec}.

The RA then instructed the participants to remain idle for two minutes without any further interaction with them. After two minutes, the RA announced the end of the session and provided a self-reported survey to the participants. The RA recorded the start and end times of the session, thanked the participants, and provided refreshments to compensate them for their valuable time and participation in the study.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/participant_Sitting_position.pdf}
    \caption{The study setup shows participants' sitting positions and camera positions. RA refers to Research Assistant, and labels P1, P2, and P3 refer to participants 1, 2, and 3, respectively. Labels C1, C2, and C3 refer to Cameras 1, 2, and 3, respectively.}
    \label{fig:sitting_position}
\end{figure}


\begin{table}
\centering
\caption{Video recording application (Background Video Recorder) settings \hl{Nilesh: Remove and incorporate it in text}}
\label{tab:video_recording_app_spec}
\begin{tabular}{cc}
    \toprule
     \textbf{Specification}      & \textbf{Setting} \\ \midrule
     Video Resolution            & 1920x1080 \\
     Recording Bit-Rate          & 6 Mb/s \\
     Aspect Ratio                & 16:9 \\
     frames-per-second (FPS)& Maximum (30) \\
     Zoom                        & 1.0x  \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Demographics}
The participants in our study are the student population of the author's institute. Most participants were male (\#58, 63.74\%), followed by female (\#33, 36.26\%). In terms of education status, 71.43\% (\#65) were undergraduate students, while the remaining 28.57\% (\#26) were graduate students. Regarding home location, 76.92\% (\#70) were from urban areas, and 23.08\% (\#21) were from rural areas. A detailed breakdown of the participants' demographic information is provided in Table \ref{tab: demographic}.

\begin{table}[]
\centering
\small
\caption{Participant's demographic information of our study. SR - Self Reported}
\label{tab: demographic}
\begin{tabular}{@{}lcccclcc@{}}
\toprule
\textbf{Category} & \multicolumn{1}{l}{\textbf{Count}} & \multicolumn{1}{l}{\textbf{Percentage}} & \multicolumn{2}{c}{\textbf{Age}} & & \multicolumn{2}{c}{\textbf{SR anxiety}} \\ \cmidrule{4-5} \cmidrule{7-8}
 & \multicolumn{1}{c}{(\#)} & \multicolumn{1}{c}{(\%)} & \multicolumn{1}{c}{$\mu$} & \multicolumn{1}{c}{$\sigma$} & & \multicolumn{1}{c}{$\mu$} & \multicolumn{1}{c}{$\sigma$} \\ \midrule
\multicolumn{8}{l}{\cellcolor[HTML]{EFEFEF}\textit{\textbf{Gender}}} \\
Female & 33 & 36.26 & 20.94 & 2.73  && 3.27& 1.13\\
Male & 58 & 63.74 & 20.59 & 2.13  && 3.29& 0.97 \\

\multicolumn{8}{l}{\cellcolor[HTML]{EFEFEF}\textit{\textbf{Education}}} \\
Graduate & 26 & 28.57 & 23.69 & 2.04  && 3.42& 1.06 \\
Undergraduate & 65 & 71.43 & 19.52 & 1.06  && 3.23& 1.01 \\

\multicolumn{8}{l}{\cellcolor[HTML]{EFEFEF}\textit{\textbf{Home Location}}} \\
Rural & 21 & 23.08 & 20.95 & 2.56  && 3.86& 0.96 \\
Urban & 70 & 76.92 & 20.64 & 2.3  && 3.11& 0.99 \\  \midrule
\textbf{\textit{Total}} & 91 & 100 & 20.71 & 2.35  && 3.28 & 1.02 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Ground Truth}
The self-reported survey collected during the study was used as the ground truth. The survey included a single question asking participants about their anxiety levels during the studys session (i.e., sitting idle for 2 minutes). Participants rated their anxiety on a Likert scale from 1 to 5, where: 1: Very nervous, 2: Somewhat nervous, 3: Neither relaxed nor nervous, 4: Somewhat relaxed, and 5: Very relaxed. The distribution of self-reported anxiety ratings is shown in Figure \ref{fig:histogram_baseline}.

For the ground truth, participants who rated their anxiety as 1 or 2 were labeled as \textit{anxious}, while those who rated their anxiety as 4 or 5 were labeled as \textit{non-anxious}. A considerable number of participants rated their anxiety as 3, so instead of grouping these participants into either the anxious or non-anxious categories, they were labeled as \textit{neutral}. 
%This labeling made the problem into a multiclass classification task. For the binary classification of anxious and non-anxious, the neutral group participants were dropped.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{images/Histogram_Baseline.pdf}
    \caption{Distribution of participants self-reported anxiety}
    \label{fig:histogram_baseline}
\end{figure}

\subsection{Feature Extraction}
\hl{Fixed - Narrative changed}
The video recording application was configured to use the highest possible sampling rate of 30 frames per second (FPS). During data processing, we found that the recorded videos were, on average, 124.21 seconds long, with a mean sampling rate of 20.02 FPS. After reviewing each participant's video and conducting post-study discussions with the participants, we selected the first 90 seconds of data for analysis, as participants generally became more comfortable over time. Videos shorter than 90 seconds were excluded, while those longer than 90 seconds were trimmed to the first 90 seconds. Data from six participants was lost due to issues such as delayed recording initiation by the research assistant or technical glitches in the recording application. A total of 85 participant's data were used for further analysis. This approach ensured minimal data loss while maintaining a consistent dataset.

To extract features from participant's videos, we used the open-source OpenFace\footnote{\url{https://github.com/TadasBaltrusaitis/OpenFace}} Python toolkit \cite{baltruvsaitis2016openface}. OpenFace is a well-validated Python-based tool for facial analysis tasks and has been widely used in various behavioral studies, including depression and anxiety detection. The toolkit processes video inputs and generates time-series data consisting of 714 features. In our analysis, we excluded metadata information features (\#5) and rigidity features (\#40). The features used in this paper (\#669) and their descriptions are provided in Table \ref{tab:openface_features}.

\noindent \textbf{Note:} \textit{We excluded the rigidity feature set due to limited literature on its characteristics and lack of explainability.}
\\
\\
\noindent \textbf{Data Preparation:} We adopted the methodology used by Bhatti et al. \cite{bhatti2024attx} and Schmidt et al. \cite{schmidt2018introducing} to prepare the data for the classification task. The OpenFace output was segmented using a non-overlapping window length of 10 seconds to create chunks. Then, the mean along the time dimension of all features was computed, which resulted in the data shape of each chunk as 1x669. It is to be noted that chunk size and no overlapping were selected empirically. Following the completion of segmentation, the data consisted of 1,1173 samples, distributed as follows: anxious (314 samples), neutral (384 samples), and non-anxious (475 samples). The other distribution of the samples is shown in Table \ref{tab:samples}.


\begin{table}[]
\centering
\caption{Samples count for anxious, neutral, and non-anxious class.}
\label{tab:my-table}
\begin{tabular}{@{}lccc@{}} 
\toprule
 & \textbf{Anxious} & \textbf{Neutral} & \textbf{non-Anxious} \\ \midrule
\multicolumn{4}{l}{\cellcolor[HTML]{EFEFEF}\textit{\textbf{Gender}}} \\
Female & 128 & 146 & 166 \\
Male & 186 & 238 & 309 \\

\multicolumn{4}{l}{\cellcolor[HTML]{EFEFEF}\textit{\textbf{Education}}} \\
Graduate & 69 & 114 & 139 \\
Undergraduate & 245 & 270 & 336 \\ 

\multicolumn{4}{l}{\cellcolor[HTML]{EFEFEF}\textit{\textbf{Home Location}}} \\
Rural & 29 & 65 & 163 \\
Urban & 285 & 312 & 319 \\ \midrule \midrule
Total & 314 & 384 & 475 \\ \bottomrule
\end{tabular}
\end{table}



% \hl{fix it as mentioned}
% To extract features from participants' videos, we used the open-source OpenFace\footnote{\url{https://github.com/TadasBaltrusaitis/OpenFace}} Python toolkit \cite{baltruvsaitis2016openface}. OpenFace is a well-validated Python-based tool for facial analysis tasks and has been widely used in various behavioral studies, including depression and anxiety detection. The toolkit processes a video as input and generates time series data consisting of 669 features \hl{Nilesh verify feature count}. The generated features can be grouped into six main categories. The first category includes basic features such as frame, ID, timestamp, confidence, and success, which provide metadata information. The detailed descriptions of the remaining five feature categories are given in Table \ref{tab:openface_features}.

% \noindent \textbf{Note:} \textit{We excluded the rigidity feature throughout the work set due to limited literature on its characteristics and lack of explainability.}

% \subsubsection{Data Preparation} \label{section: Data Preparation}
% The video recording application was configured to use the highest possible sampling rate in frames per second (FPS). On average, the recorded videos were 124.21 seconds long, with a mean sampling rate of 20.02 FPS. For feature extraction, we used the first 90 seconds of each video, as participants generally became comfortable with the passing time frame. Videos shorter than 90 seconds were excluded, while those longer than 90 seconds were trimmed to the first 90 seconds. Data from six participants was lost due to issues such as delayed recording initiation by the research assistant or technical glitches in the recording application. This approach minimized data loss while maintaining a consistent dataset.

% The sampling rate of the video recording application was set to the maximum possible value, which ranged from 5 to 30 frames per second (FPS). Upon reviewing the data, we found that the minimum and maximum sampling rates were 19.86 FPS and 27.16 FPS, respectively, with an average sampling rate of 20.02 FPS. Furthermore, although the activity duration was 2 minutes, the average recorded video duration was 124.21 seconds, with a minimum of 63.5 seconds and a maximum of 165.94 seconds. Due to technical issues with the recording application, 16 participants had video durations of less than 120 seconds, and only six participants had durations of less than 90 seconds. To address this, we standardized all video durations to 90 seconds. Videos shorter than 90 seconds were excluded, while videos longer than 90 seconds were cropped to the first 90 seconds. This approach ensured minimal data loss while maintaining a consistent dataset.

% We adopted the methodology used by Bhatti et al. \cite{bhatti2024attx} and Schmidt et al. \cite{schmidt2018introducing} to prepare the data for the classification task. The OpenFace output was segmented using a non-overlapping window length of 10 seconds to create chunks. The number of frames in each chunk of 10s was 226, which was calculated using the average FPS from all participants multiplied by 10 (22.6x10=226). Subsequently, a statistical mean was computed for each feature, reducing the data from 226 × 669 to 1 × 669. Figure \ref{fig:data_prep} shows an example of this process. The final dataset had a shape of 1173 × 669, where 1173 represents the total number of chunks generated from all 85 participants, and 669 corresponds to the features listed in Table \ref{tab:openface_features} excluding rigidity.

% We adopted the methodology used by Bhatti et al. \cite{bhatti2024attx} and Schmidt et al. \cite{schmidt2018introducing} to prepare the data for the classification task. The OpenFace output was segmented using a non-overlapping window length of 10 seconds to create chunks. The average number of frames per second (22.6) was calculated from all participants’ data, resulting in 226 frames for each 10-second window (22.6 × 10 = 226 frames). Subsequently, a statistical feature, the mean, was computed for each feature, reducing the data from 226 × 709 to 1 × 709. An example of this process is shown in Figure \ref{fig:data_prep}. The final dataset had a shape of 1173 × 709, where 1173 represents the total number of chunks generated from all participants, and 709 corresponds to the features listed in Table \ref{tab:openface_features}.

\begin{figure*}
  \centering
  
  % First figure (Facial Landmarks)
  \begin{subfigure}{0.8\textwidth}
    \centering
    \includegraphics[scale=0.1]{old/images/landmark_scheme_68.pdf}
    \caption{Face}
    \label{fig:facial_lm}
  \end{subfigure}
  
  % Second row with left and right eye
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth, height=3.25cm]{old/images/left_eye.pdf}
    \caption{Left Eye}
    \label{fig:eye_lm_left_eye}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=0.99\textwidth, height=3.25cm]{old/images/right_eye.pdf}
    \caption{Right Eye}
    \label{fig:eye_lm_right_eye}
  \end{subfigure}
  \caption [Image credit]{Landmarks in (a) face, (b) left eye, and  (c) right eye (Image credit\protect\footnotemark). \hl{Thinking to drop this figure}}
  \label{fig:landmark}
\end{figure*}


\begin{table*}[htbp]
\centering
\caption{OpenFace output features count and descriptions. }
\label{tab:openface_features}
\begin{tabular}{cccp{0.70\textwidth}}  % Adjust the width relative to text width 
\toprule
\textbf{Face Region} & \textbf{Feature Set} & \textbf{\#} & \textbf{Descriptions} \\ \midrule

\multirow{3}{*}{Eye} 
    & \multirow{3}{*}{\shortstack{gaze \\ 2D landmarks \\ 3D landmarks}}  & \multirow{3}{*}{\shortstack{8 \\ 112 \\ 168}} & Gaze represents the direction in which an individual is looking, using 3D vector world coordinates for both the left and right eyes. It also includes the gaze direction angle, averaged for both eyes, indicating whether a person looks left-right or up-down. Eye landmarks (pupil and eyelids) represent the positions of landmarks around the eye region in both 2D and 3D coordinates (see Figures \ref{fig:eye_lm_left_eye} and \ref{fig:eye_lm_right_eye}). \\ \midrule
    
\multirow{2}{*}{\shortstack{Head \\ Pose}} 
    & \multirow{2}{*}{\shortstack{location \\ rotation}}  & \multirow{2}{*}{\shortstack{3 \\ 3}}  & Pose location represents the position of the head relative to the camera along the X, Y, and Z axes, while the pose angle represents the rotation of the head along these axes. The rotations are referred to as pitch (X axis), yaw (Y axis), and roll (Z axis). \\ \midrule
    
\multirow{2}{*}{\shortstack{Face \\ Landmark}} 
    & \multirow{2}{*}{\shortstack{ 2D landmarks\\ 3D landmarks}} & \multirow{2}{*}{\shortstack{ 136 \\ 204}} & Face landmarks represent 68 key positions on the face. These positions include the jawline (i.e., face edge) with 17 points, eyebrows for the left and right eyes with 10 points, eyes (left and right) with 12 points, the nose bridge and tip with 9 points, and the mouth, consisting of the upper lip, lower lip, and corners, with 20 points. The landmarks are present in both 2D and 3D coordinates (see Figure \ref{fig:facial_lm}). \\ \midrule

\multirow{2}{*}{\shortstack{Facial Action \\ Units (AUs)}} 
    & \multirow{2}{*}{\shortstack{ intensity \\ presence}}  & \multirow{2}{*}{\shortstack{ 17 \\ 18}}  & AUs represent facial expressions using the Facial Action Coding System based on muscle movements. AUs are described in terms of presence and intensity. Presence indicates whether the AU is visible on the face, while intensity refers to how strong the AU is, measured on a scale of 1 to 5. \\ \bottomrule
\end{tabular}
\end{table*}


% \begin{figure*}
%     \centering
%     \includegraphics[width=1\linewidth]{images/feature_agg.pdf}
%     \caption{An example of the data preparation process for machine learning models. For example, if the number of frames for a participant's two minute video were 2034. Based on the average frame rate (computed from all participants' data) of 22.6 frames per second, the total number of chunks using a non-overlapping window size of 10 seconds will be 9 with each chunk having a shape of 226 x 669. Now, for each chunk, the statistical ``mean'' is computed for all features, reducing the chunks' shape to 1 x 669. Further, if participant was labeled as ``anxious'', then all chunks of this participant will be labeled as ``anxious''. This process is repeated for all 91 participants. Note: For any participants, if the last chunk contains fewer than the average number of frames for 10 seconds (i.e., 226), overlapping data from the previous chunk is included to ensure consistency.  \hl{Excluding this figure due to change in narrative}
%     }
%     \label{fig:data_prep}
% \end{figure*}


% \begin{table*}[htbp]
% \centering
% \caption{OpenFace output features count and descriptions. }
% \label{tab:openface_features}
% \begin{tabular}{|l|l|l|p{0.60\textwidth}|}  % Adjust the width relative to text width
% \hline
% \textbf{Face Region} & \textbf{Feature Set} & \textbf{\#} & \textbf{Descriptions} \\
% \hline
% \multirow{3}{*}{Eye} 
%     & gaze& 8   & Gaze represents the direction in which an individual is looking, using 3D vector world coordinates for both the left and right eyes. It also includes the gaze direction angle, averaged for both eyes, indicating whether a person looks left-right or up-down. Eye landmarks (pupil and eyelids) represent the positions of landmarks around the eye region in both 2D and 3D coordinates (see Figures \ref{fig:eye_lm_left_eye} and \ref{fig:eye_lm_right_eye}). \\
%     & 2D landmarks  & 112 &  \\
%     & 3D landmarks& 168 &  \\
% \hline
% \multirow{2}{*}{Head Pose} 
%     & location& 3   & Pose location represents the position of the head relative to the camera along the X, Y, and Z axes, while the pose angle represents the rotation of the head along these axes. The rotations are referred to as pitch (X axis), yaw (Y axis), and roll (Z axis). \\
%     & rotation& 3   &  \\
% \hline
% \multirow{2}{*}{Face landmark} 
%     & 2D landmarks  & 136 & Face landmarks represent 68 key positions on the face. These positions include the jawline (i.e., face edge) with 17 points, eyebrows for the left and right eyes with 10 points, eyes (left and right) with 12 points, the nose bridge and tip with 9 points, and the mouth, consisting of the upper lip, lower lip, and corners, with 20 points. The landmarks are present in both 2D and 3D coordinates (see Figure \ref{fig:facial_lm}). \\
%     & 3D landmarks& 204 &  \\
% \hline
% % \multirow{2}{*}{Rigidity} 
% %     & Rigid         & 6   & Rigidity describes facial shape using a point distribution model. Rigid shape parameters represent the face's position, while non-rigid parameters capture changes in facial appearance caused by variations in expression. \\
% %     & Non-rigid     & 34  &  \\
% % \hline
% \multirow{2}{*}{Facial Action Units (AUs)} 
%     & intensity& 17  & AUs represent facial expressions using the Facial Action Coding System based on muscle movements. AUs are described in terms of presence and intensity. Presence indicates whether the AU is visible on the face, while intensity refers to how strong the AU is, measured on a scale of 1 to 5. \\
%     & presence& 18  &  \\
% \hline
% \end{tabular}
% \end{table*}


% \begin{figure*}
%     \centering
%     \includegraphics[width=1\linewidth]{images/feature_agg.pdf}
%     \caption{An example of the data preparation process for machine learning models. For example, if the number of frames for a participant's two minute video were 2034. Based on the average frame rate (computed from all participants' data) of 22.6 frames per second, the total number of chunks using a non-overlapping window size of 10 seconds will be 9 with each chunk having a shape of 226 x 669. Now, for each chunk, the statistical ``mean'' is computed for all features, reducing the chunks' shape to 1 x 669. Further, if participant was labeled as ``anxious'', then all chunks of this participant will be labeled as ``anxious''. This process is repeated for all 91 participants. Note: For any participants, if the last chunk contains fewer than the average number of frames for 10 seconds (i.e., 226), overlapping data from the previous chunk is included to ensure consistency.  
%     }
%     \label{fig:data_prep}
% \end{figure*}

% \begin{table*}[]
% \centering
% \caption{OpenFace output features count and descriptions. }
% \label{tab:openface_features}
% \begin{tabular}{@{}llcc@{}}
% \toprule
% \multicolumn{1}{l}{\textbf{Face Region}} & \textbf{Feature Set} & \textbf{\#} & \textbf{Descriptions} \\ \midrule
% \multirow{3}{*}{Eye} & Gaze & 8 & \multirow{3}{*}{Gaze represents the direction in which an individual is looking, using 3D vector world coordinates for both the left and right eyes. It also includes the gaze direction angle, averaged for both eyes, indicating whether a person looks left-right or up-down. Eye landmarks (pupil and eyelids) represent the positions of landmarks around the eye region in both 2D and 3D coordinates (as shown in Figure X).}\\
%  & 2D landmarks & 112 &  \\
%  & 3D Landmarks & 168 &  \\ \hline
% \multirow{2}{*}{Head Pose}& location & 3 & \multirow{2}{*}{Pose location represents the position of the head relative to the camera along the X, Y, and Z axes, while the pose angle represents the rotation of the head along these axes. The rotations are referred to as pitch (X axis), yaw (Y axis), and roll (Z axis).}\\
%  & Rotation & 3 &  \\ \hline
% \multirow{2}{*}{Face landmark}& 2D landmarks & 136 & \multirow{2}{*}{Face landmarks represent 68 key positions on the face (see Figure X). These positions include the jawline (also called facial edge or facial contour) with 17 points, eyebrows for the left and right eyes with 10 points, eyes (left and right) with 12 points, the nose bridge and tip with 9 points, and the mouth, consisting of the upper lip, lower lip, and corners, with 20 points. The landmarks are present in both 2D and 3D coordinates.} \\
%  & 3D Landmarks & 204 &  \\ \hline
% \multirow{2}{*}{Rigidity}& Rigid & 6 & \multirow{2}{*}{Rigidity describes facial shape using a point distribution model. Rigid shape parameters represent the face's position, while non-rigid parameters capture changes in facial appearance caused by variations in expression.} \\
%  & non-rigid & 34 &  \\ \hline
% \multirow{2}{*}{Facial Action Units}& Intensity & 17 &  \multirow{2}{*}{Facial Action Units (AUs) represent facial expressions using the Facial Action Coding System based on muscle movements. AUs are described in terms of presence and intensity. Presence indicates whether the AU is visible on the face, while intensity refers to how strong the AU is, measured on a scale of 1 to 5.} \\
%  & Presence & 18 &  \\ \bottomrule
% \end{tabular}
% \end{table*}



\subsection{Anxiety Classification}
In this study, we aim to accurately identify anxious participants from facial videos using machine learning techniques. Specifically, we developed a multiclass classification model to classify a participant's chunks as anxious, non-anxious, or neutral.

\subsubsection{Machine Learning}
We used the data discussed in Section \ref{section: Data Preparation} as input for the machine-learning model. First, we utilized all the extracted features (i.e., 669) for the classification task. Second, we conducted Ablation Study 1 first to identify the most impactful feature using Random Forest feature importance and its effect on anxiety classification. Third, we conducted Ablation Study 2 to evaluate the effectiveness of different feature categories (see Table \ref{tab:openface_features}) in identifying the most impactful feature categories.  Finally, we performed Ablation Study 3 to examine the classification model's performance on binary classification tasks, specifically anxious vs. non-anxious, anxious vs. neutral, and neutral vs. non-anxious.

We used various classification models, each leveraging distinct strengths and methodologies, to identify the most effective model for anxiety classification using facial features. Logistic Regression (LR) was utilized for its simplicity and effectiveness in binary classification tasks. K-Nearest Neighbors (KNN) was included to assess the potential of proximity-based classification. Support Vector Machines (SVM) were selected for their capability to handle high-dimensional feature spaces effectively. Decision Trees (DT) offered interpretability, allowing for a better understanding of feature contributions, while Random Forests (RF) provided robustness against noise and the ability to capture complex feature interactions.

\hl{optional - fix me} We avoided using deep learning models because machine learning (ML) models demonstrated superior performance and offered better explainability compared to deep learning models. Additionally, deep learning models require high-end computing systems, which limits their practical application in our use case. Future studies can explore deploying these ML models on low-end smartphones to evaluate their performance—feasibility that would be challenging with deep learning models.


\subsubsection{Evaluation}
To evaluate our model effectively, we used a 5-fold cross-validation approach \cite{kohavi1995study}. This method ensures that the model is trained on different subsets of the data in each iteration and tested on unseen data, providing a more reliable and robust evaluation compared to a single train-test strategy. Further, to assess classification performance, we used evaluation metrics, including accuracy, precision, recall, F1-score, and AUC \cite{sokolova2009systematic}. These metrics were computed for each fold and then averaged across all five folds. Additionally, we present each class's average precision and recall, offering a comprehensive assessment of the model's performance.
\hl{Will just cite this paper without including any formula to save space - Include formula of multiclass - A systematic analysis of performance measures for classification tasks - https://sci-hub.se/https://doi.org/10.1016/j.ipm.2009.03.002}

\section{Results}
In this section, we present the outcomes of our analysis, including the predictive capabilities of the machine learning classification models used for the multiclass problem using facial features. Additionally, we discuss the results of the ablation studies conducted in three different scenarios: (i) classification using different feature categories, (ii) classification using handcrafted features, and (iii) classification for binary classification problems.

\subsection{Classification Performance}
In our analysis, we used classical machine learning classification models to evaluate the ability of AnxietyFaceTrack to detect anxiety in a lab setting without introducing additional anxiety-provoking situations. Table \ref{tab:Multiclass classification results} presents the performance metrics for all the classification models used. We found that KNN and RF performed well, while LR showed the poorest performance in terms of accuracy. Further inspection of the other metrics revealed that RF and KNN achieved almost identical average precision and recall. However, when focusing on the ``anxious'' label, RF outperformed KNN, achieving higher average precision for this label.

Moreover, Table \ref{tab:Multiclass classification results} provides additional insights. Firstly, RF outperformed DT across all metrics, suggesting that the single-tree structure of DT suffered from overfitting, whereas the ensemble approach of RF (using multiple trees) effectively handled high-dimensional data without overfitting. Secondly, RF outperformed LR on all metrics. This is likely because LR relies on linear decision boundaries, which failed to capture the complex and non-linear patterns in the data, while RF could handle these complexities. The overall best performance of RF in multiclass classification inspired us to conduct an ablation study to assess its effectiveness in binary classification scenarios (see Section \ref{section: AS_binary}), such as distinguishing between ``anxious'' and ``non-anxious'' participants.

% Moreover, Table \ref{tab:Multiclass classification results} provides additional insights. Firstly, RF outperformed DT across all metrics, suggesting that the single-tree structure of DT suffered from overfitting, whereas the ensemble approach of RF (using multiple trees) effectively handled high-dimensional data without overfitting. Secondly, RF outperformed LR on all metrics. This is likely because LR relies on linear decision boundaries, which failed to capture the complex and non-linear patterns in the data, while RF could handle these complexities. However, this finding highlights the importance of conducting an ablation study to evaluate LR's performance in binary classification scenarios (see Section X), such as ``anxious'' vs. ``non-anxious'' classification.

\begin{table}
    \centering
    \small
    \caption{Multiclass classification results. The table presents the evaluation metric averaged for five folds. Precision and Recall are presented as (Pr.: Precision, Re.: Recall) for each class individually. Clf: Classifier, Acc: Accuracy, non-Anx: non-Anxious}
    \label{tab:Multiclass classification results}
    \begin{tabular}{ccccccc} \toprule
         \textbf{Clf.}& \textbf{Acc.} & \textbf{Anxious}& \textbf{Neutral}& \textbf{non-Anx}& \multirow{2}{*}{\shortstack{\textbf{F1} \\ \textbf{Score}}} & \textbf{AUC} \\
 & & \textbf{\textit{(Pr., Re.)}}& \textbf{\textit{(Pr., Re.)}}& \textbf{\textit{(Pr., Re.)}}&  &\\ \midrule
         LR & 0.65 & (0.64, 0.60) & (0.68, 0.69) & (0.65, 0.67) & 0.65 & 0.83 \\
         KNN & 0.86 & (0.81, 0.91) & (0.89, 0.84) & (0.89, 0.86) & 0.86 & 0.96 \\
         SVM & 0.73 & (0.68, 0.61) & (0.76, 0.76) & (0.73, 0.79) & 0.72 & 0.90 \\
         DT & 0.75 & (0.71, 0.70) & (0.75, 0.74) & (0.77, 0.79) & 0.74 & 0.81 \\
         RF & 0.88 & (0.86, 0.86) & (0.87, 0.88) & (0.90, 0.88) & 0.88 & 0.97 \\ \bottomrule
    \end{tabular}
\end{table}

% \begin{table*}
%     \centering
% \caption{Multiclass classification results. The table presents the evaluation metric averaged for five folds. Precision and Recall is presented for each class individually}
% \label{tab:Multiclass classification results}
%     \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \hline 
%          &  Accuracy&  \multicolumn{3}{|c|}{Precision}&  \multicolumn{3}{|c|}{Recall}&  F1 Score& AUC\\ \hline 
%  & & (Anxious)& (Neutral)& (non-Anxious)& (Anxious)& (Neutral)& (non-Anxious)& &\\ \hline 
%          LR&  0.65&  0.64&  0.68&  0.65&  0.6&  0.69&  0.67&  0.65& 0.83
% \\ \hline 
%          KNN&  0.86&  0.81&  0.89&  0.89&  0.91&  0.84&  0.86&  0.86& 0.96
% \\ \hline 
%          SVM&  0.73&  0.68&  0.76&  0.73&  0.61&  0.76&  0.79&  0.72& 0.9
% \\ \hline 
%          DT&  0.75&  0.71&  0.75&  0.77&  0.7&  0.74&  0.79&  0.74& 0.81
% \\ \hline 
%          RF&  0.88&  0.86&  0.87&  0.9&  0.86&  0.88&  0.88&  0.88& 0.97\\ \hline
%     \end{tabular}
% \end{table*}


\begin{table*}[]
\centering
\caption{Multiclass classification results obtained on the training and testing on Random Forest model. The table presents the evaluation metric averaged for five folds.}
\label{tab:3_class_classification_metric_RF}
\begin{tabular}{@{}cccccccc@{}}
\toprule
\multicolumn{1}{l}{\textbf{Face Region}} & \textbf{Feature Set} & \textbf{\#} &  \textbf{Accuracy}& \textbf{Precision}& \textbf{Recall}& \textbf{F1 Score}&\textbf{AUC}\\ \midrule
\multirow{4}{*}{Eye} & gaze& 8 &   0.53& 0.52& 0.51& 0.52&0.72
\\
 & 2D landmarks & 112 &   0.70& 0.70& 0.69& 0.69&0.86
\\
 & 3D landmarks& 168 &   0.74& 0.73& 0.73& 0.73&0.9
\\ \cmidrule{2-2}
 & Combined& & 0.79& 0.79& 0.79& 0.79&0.93
\\ \hline
\multirow{3}{*}{Head Pose} & location & 3 &   0.81& 0.80& 0.80& 0.80&0.93
\\
 & rotation& 3 &   0.56& 0.56& 0.54& 0.55&0.74
\\ \cmidrule{2-2}
 & Combined& & 0.85& 0.85& 0.85& 0.85&0.96
\\ \hline
\multirow{3}{*}{Face Landmark} & 2D landmarks & 136 &   0.81& 0.81& 0.81& 0.81&0.94
\\
 & 3D landmarks& 204 &   0.88& 0.88& 0.87& 0.87&0.97
\\ \cmidrule{2-2}
 & Combined& & 0.87& 0.87& 0.87& 0.87&0.97
\\ \hline
\multirow{3}{*}{Facial Action Units} & intensity& 17 &   0.58& 0.58& 0.57& 0.57&0.78
\\
 & presence& 18 &   0.59& 0.6& 0.57& 0.58&0.76
\\ \cmidrule{2-2}
 & Combined& & 0.66& 0.67& 0.65& 0.65&0.84\\ \midrule \midrule
\multirow{2}{*}{ALL} & Top 10\% & 67& 0.90& 0.90& 0.90& 0.90&0.98 \\
 &  Top 20\%& 134& 0.91& 0.90& 0.91& 0.90&0.98\\ \bottomrule
\end{tabular}
\end{table*}

\subsection{Ablation Study}
\subsubsection{Ablation Study 1}
In this analysis, we aimed to understand the role of feature importance in anxiety classification. We identified the most important features using the feature importance module from the Scikit-learn Random Forest implementation. We then trained and tested the model using k-fold cross-validation, incrementally selecting the top 10\%, 20\%, and up to 90\% of the features ranked by importance. Our findings revealed that using only the top 10\% of important features resulted in an accuracy of 90\%, which was 2\% higher than the accuracy achieved using all features. Increasing to the top 20\% of important features further improved accuracy to 91\%. Notably, the accuracy remained constant when using 30\% and 40\% of the features but began to decline after that, with a drop of just 1–2\%. This analysis demonstrates that using only the top 67 features (10\% of the total 669) achieves the highest accuracy, significantly reducing the model's complexity while maintaining good performance (see Table \ref{tab:3_class_classification_metric_RF}). This finding underscores the effectiveness of feature selection in optimizing classification models in anxiety detection.

\subsubsection{Ablation Study 2}
In this ablation study, we evaluated each feature set listed in Table \ref{tab:openface_features} to assess their ability in anxiety classification tasks. We selected RF as the classification model for this ablation study due to its superior performance compared to other models in previous analyses. The model was first trained and tested on individual feature sets and combinations of feature sets in individual facial regions, such as the combination of location and rotation in the head pose, etc. Table \ref{tab:3_class_classification_metric_RF} summarizes the results obtained from 5-fold cross-validation, with the classification metrics averaged across all folds.

% In this ablation study, we evaluated each feature set listed in Table \ref{tab:openface_features} to assess the individual contribution of facial feature sets to classification performance. We selected RF as the classification model for this study due to its superior performance compared to other models in previous analysis. The model was first trained on individual feature sets and then on combinations based on facial regions, such as eyes, pose, etc. Table \ref{tab:3_class_classification_metric_RF} summarizes the results obtained from 5-fold cross-validation, with the classification metrics averaged across all folds.

For individual feature sets, the highest accuracy and F1 score (0.87) were achieved using 3D facial landmarks, while the lowest scores were observed for eye gaze and pose rotation features. Among the combined feature sets based on facial regions, the highest accuracy was achieved for the ``face landmark'' region, followed by ``head pose'' with just a 2\% difference in accuracy. In contrast, the lowest performance was recorded for facial action units.

Interestingly, compared to the earlier analysis where all 670 features were used, the 3D landmarks achieved similar performance with just 204 features. Notably, the pose feature set, consisting of only six features, achieved 85\% accuracy, which is just 3\% lower than the highest-performing set. This highlights the potential of reduced feature sets for maintaining strong classification performance while minimizing computational complexity.

\subsubsection{Ablation Study 3} \label{section: AS_binary}
In this analysis, we aimed to understand how the classification model performs in binary classification using OpenFace features for anxiety and non-anxiety detection. We conducted binary classifications between ``Anxious'' and ``non-anxious'' by excluding the neutral participants. Similarly, we performed classifications for ``Anxious'' versus ``Neutral'' and ``Neutral'' versus ``Non-Anxious''. Our results showed that the RF model outperformed other classification models. Table \ref{tab:binary_clf_results} presents the classification metrics obtained from binary classification across different feature sets. From Table \ref{tab:binary_clf_results}, we identified several interesting observations that offer insights into comparing multiclass and binary classification and the utility of individual feature categories. First, we found that variations in accuracy and classification metrics in binary classification were consistent with those observed in multiclass classification. For instance, pose rotation features performed poorly across all binary classification cases but showed improved performance when combined with pose location features. This pattern was also observed in multiclass classification. Second, similar to multiclass classification, 3D facial landmarks were the most effective feature set for binary classification. This suggests that facial landmarks are crucial for detecting anxiety and non-anxiety. Third, the binary classification of ``Neutral'' versus ``Non-Anxious'' achieved the best performance, with the highest accuracy of 93%.

% In this analysis, we aimed to understand how the classification model performs in binary classification using OpenFace features for anxiety and non-anxiety detection. We conducted binary classifications between ``Anxious'' and ``non-anxious'' by excluding the neutral participants. Similarly, we performed classifications for ``Anxious'' versus ``Neutral'' and ``Neutral'' versus ``Non-Anxious.'' Our results showed that the RF model outperformed other classification models, specifically LR. Initially, we assumed that LR would perform better in binary classification. However, its performance remained similar to multiclass classification, suggesting that LR struggled to capture the complex nature of the data. Table \ref{} presents the classification metrics obtained from binary classification across different feature sets. From Table X, we identified several interesting observations that offer insights into comparing multiclass and binary classification and the utility of individual feature categories. First, we found that variations in accuracy and classification metrics in binary classification were consistent with those observed in multiclass classification. For instance, pose rotation features performed poorly across all binary classification cases but showed improved performance when combined with pose location features. This pattern was also observed in multiclass classification. Second, similar to multiclass classification, 3D facial landmarks were the most effective feature set for binary classification. This suggests that facial landmarks are crucial for detecting anxiety and non-anxiety. Third, the binary classification of ``Neutral'' versus ``Non-Anxious'' achieved the best performance, with the highest accuracy of 93%.


\begin{table*}
    \centering
\caption{Binary classification results obtained on the training and testing on Random Forest model. The table presents the evaluation metric averaged for five folds.}
\label{tab:binary_clf_results}
    \begin{tabular}{lcccccclcccllllllll} \toprule
   \multirow{2}{*}{\shortstack{\textbf{Face} \\ \textbf{Region}}}& \textbf{Feature Set} & \multicolumn{5}{c}{\textbf{Anxious versus Neutral}} & & \multicolumn{5}{c}{\textbf{Anxious versus non-Anxious}} &&   \multicolumn{5}{c}{\textbf{Neutral versus non-Anxious}}\\ \cmidrule{3-7} \cmidrule{9-13} \cmidrule{15-19}
            
& &  \textbf{\textit{Acc}}&  \textbf{\textit{Pr.}}&  \textbf{\textit{Re.}}&  \textbf{\textit{F1}}&  \textbf{\textit{AUC}} & &  \textbf{\textit{Acc}}&  \textbf{\textit{Pr.}}&  \textbf{\textit{Re.}}& \textbf{\textit{F1}}& \textbf{\textit{AUC}} &&   \textbf{\textit{Acc}}&\textbf{\textit{Pr.}}&\textbf{\textit{Re.}}& \textbf{\textit{F1}}&\textbf{\textit{AUC}}\\ \bottomrule
            \multirow{4}{*}{Eye} 
&gaze
&  0.68&  0.66&  0.62&  0.64&  0.74
 & &  0.65&  0.57&  0.49& 0.52& 0.68
 &&   0.65&0.68&0.7& 0.69&0.73
\\ 
            
&2D landmarks 
&  0.81&  0.8&  0.78&  0.79&  0.91
 & &  0.78&  0.72&  0.72& 0.72& 0.87
 &&   0.76&0.78&0.79& 0.79&0.85
\\ 
            
&3D landmarks
&  0.83&  0.8&  0.81&  0.81&  0.91
 & &  0.86&  0.84&  0.79& 0.81& 0.93
 &&   0.82&0.83&0.85& 0.84&0.9
\\ \cmidrule{2-2}
            
&Combined
&  0.85&  0.85&  0.82&  0.83&  0.94
 & &  0.86&  0.84&  0.81& 0.82& 0.94
 &&   0.85&0.87&0.85& 0.86&0.93
\\ \midrule
            \multirow{3}{*}{\shortstack{Head \\ Pose}} 
&location 
&  0.88&  0.87&  0.85&  0.86&  0.95
 & &  0.87&  0.85&  0.84& 0.84& 0.94
 &&   0.86&0.87&0.88& 0.88&0.93
\\ 
            
&rotation
&  0.72&  0.69&  0.69&  0.69&  0.78
 & &  0.7&  0.66&  0.55& 0.59& 0.75
 &&   0.67&0.7&0.72& 0.71&0.73
\\ \cmidrule{2-2}
            
&Combined
&  0.91&  0.92&  0.87&  0.89&  0.98
 & &  0.88&  0.88&  0.83& 0.85& 0.96
 &&   0.9&0.9&0.91& 0.91&0.97
\\ \midrule
            \multirow{3}{*}{\shortstack{Face \\ Landmark}}
&2D landmarks 
&  0.88&  0.88&  0.84&  0.86&  0.96
 & &  0.86&  0.82&  0.82& 0.82& 0.94
 &&   0.86&0.88&0.86& 0.87&0.94
\\ 
            
&3D landmarks
&  0.91&  0.9&  0.89&  0.9&  0.97
 & &  0.9&  0.89&  0.87& 0.88& 0.97
 &&   0.92&0.91&0.94& 0.93&0.97
\\ \cmidrule{2-2}
   
&Combined
& 0.91& 0.91& 0.89& 0.9& 0.97
 & & 0.9& 0.89& 0.86& 0.87& 0.97
 &&   0.92&0.92&0.94& 0.93&0.97
\\  \midrule
   \multirow{3}{*}{\shortstack{Facial \\ Action \\ Units}}
&intensity
& 0.73& 0.71& 0.67& 0.69& 0.8
 & & 0.72& 0.71& 0.51& 0.59& 0.8
 &&   0.66&0.68&0.75& 0.71&0.74
\\ 
   
&presence
& 0.73& 0.73& 0.65& 0.69& 0.79
 & & 0.72& 0.72& 0.5& 0.59& 0.79
 &&   0.73&0.73&0.81& 0.77&0.77
\\ \cmidrule{2-2}
   &Combined
& 0.79& 0.8& 0.71& 0.75& 0.87
 & & 0.74& 0.76& 0.5& 0.6& 0.85
 &&   0.73&0.73&0.83& 0.77&0.82
\\ \midrule \midrule
 \multirow{3}{*}{ALL} & Top 10\% 
& 0.91& 0.92& 0.87& 0.9& 0.98 & & 0.9& 0.89& 0.86& 0.87& 0.97 && 0.92& 0.93& 0.93& 0.93&0.97\\
   &Top 20\%& 0.92& 0.93& 0.9& 0.91& 0.98
 & & 0.92& 0.91& 0.88& 0.89& 0.98
 && 0.93& 0.94& 0.94& 0.94&0.98
\\ 
 & Top 20\%& 0.92& 0.92& 0.9& 0.91& 0.98 & & 0.91& 0.9& 0.89& 0.89& 0.98 && 0.93& 0.94& 0.94& 0.94&0.98\\ \bottomrule
    \end{tabular}
\end{table*}

\section{Feature importance in Anxiety classification}
In this work, we use facial video features for anxiety detection, and it is important to understand which facial features are linked to anxiety. To explain the results, we conducted a post-hoc analysis using SHapley Additive exPlanations (SHAP) to examine the classification model. SHAP quantifies the contribution of each feature to the model's prediction using game-theoretic Shapley values. Additionally, the Shapley values provide insights into how each feature affects the model's decision-making. We will use a Random Forest model trained on the full feature set to identify the important features.

\subsection{Multiclass}
Figure \ref{fig:3_class_shape_summary} shows the top ten important features for multiclass classification. We observe that features like the jawline (i.e., face edge/face contour), right eye, and head position angle are significant for multiclass classification. Further, Figure \ref{fig: 3_same_features_Direction} highlights how these top ten features from Figure \ref{fig:3_class_shape_summary} influence the classification for each class (Figure \ref{fig:3_same_features_Anxious} for anxious, Figure \ref{fig:3_same_features_Neutral} for neutral, and Figure \ref{fig:3_same_features_non-Anxious} for non-anxious). From Figure \ref{fig:3_same_features_Anxious}, we notice that larger values of the face edge (Y\_11, Y\_16) and larger values of head position pitch (pose\_Rx) push the model towards predicting the anxious class.

Figure \ref{fig: 3_class_feature_Direction} presents the top ten important features of individual classes that influence the model's predictions. For instance, we observe that larger values of head position pitch (pose\_Rx), face edge (Y\_4, Y\_16), and left eye landmark (x\_3) push the model towards predicting the anxious class. On the other hand, smaller values of the remaining features (see Figure \ref{fig:3_class_Anxious}) push the model away from predicting the anxious class. Similarly, we find that larger values of right eye gaze direction (gaze\_1\_z, gaze\_1\_y) and left cheek face contour (Y\_1, Y\_4) push the model towards predicting the neutral class (see Figure \ref{fig:3_class_Neutral}). For the non-anxious class, smaller values of most features (see Figure \ref{fig:3_class_non-Anxious}) push the model away from predicting the non-anxious class.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images//SHAP/3_class_shape_summary.png}
    \caption{Top ten important features for multiclass classification}
    \label{fig:3_class_shape_summary}
\end{figure}

\begin{figure*}
  \centering
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.25]{images/SHAP/3_same_features_Anxious.png}
    \caption{Anxious}
    \label{fig:3_same_features_Anxious}
  \end{subfigure}
  % \hfill
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.25]{images/SHAP/3_same_features_Neutral.png}
    \caption{Neutral}
    \label{fig:3_same_features_Neutral}
  \end{subfigure}
  % \hspace{0.001\textwidth}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.25]{images/SHAP/3_same_features_non-Anxious.png}
    \caption{non-Anxious}
    \label{fig:3_same_features_non-Anxious}
  \end{subfigure}
  \caption{Influence of top ten features from Figure \ref{fig:3_class_shape_summary} in each class in Multiclass classification}
  \label{fig: 3_same_features_Direction}
\end{figure*}

\begin{figure*}
  \centering
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.25]{images/SHAP/3_class_Anxious.png}
    \caption{Anxious}
    \label{fig:3_class_Anxious}
  \end{subfigure}
  % \hfill
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.25]{images/SHAP/3_class_Neutral.png}
    \caption{Neutral}
    \label{fig:3_class_Neutral}
  \end{subfigure}
  % \hspace{0.001\textwidth}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.25]{images/SHAP/3_class_non-Anxious.png}
    \caption{non-Anxious}
    \label{fig:3_class_non-Anxious}
  \end{subfigure}
  \caption{Influence of top ten features in individual class in Multiclass classification}
  \label{fig: 3_class_feature_Direction}
\end{figure*}

\subsection{Binary class}
Figures \ref{fig:2_anxious_neutral_shape_summary}, \ref{fig:2_anxious_non-anxious_shape_summary}, and \ref{fig:2_neutral_non-anxious_shape_summary} show the top ten features for binary classification in three scenarios: anxious versus neutral, anxious versus non-anxious, and neutral versus non-anxious respectively. We observe that face edges contribute the most to classifications involving anxious versus non-anxious and anxious versus neutral. In contrast, eye-related features dominate in the classification of neutral versus non-anxious.
Figures \ref{fig:2_A_vs_Ne_Anxious}, \ref{fig:2_Ne_vs_NA_non-Anxious}, and \ref{fig:2_Ne_vs_NA_non-Anxious} illustrate the direction of influence for the features shown in Figures \ref{fig:2_anxious_neutral_shape_summary}, \ref{fig:2_anxious_non-anxious_shape_summary}, and \ref{fig:2_neutral_non-anxious_shape_summary} respectively. Figure \ref{fig:2_A_vs_Ne_Anxious} shows the influence of features on the anxious class in the anxious versus neutral classification. We observe that larger values of eye landmarks (i.e., eye\_lmk\_x\_38, eye\_lmk\_x\_10) and larger face edge values (i.e., x\_16, Y\_4, x\_4) push the model towards predicting the anxious class. Conversely, smaller values of other facial landmarks steer the model away from the anxious class, thereby favoring the neutral class. Figure \ref{fig:2_Ne_vs_NA_non-Anxious} shows the influence of features for the anxious class in the anxious versus non-anxious classification. Larger values of facial landmarks, such as facial edge features (i.e., Y\_0, x\_13) and the right eyebrow (i.e., Z\_25), push the model towards predicting the anxious class. In contrast, higher values of the remaining facial landmarks influence the model toward predicting the non-anxious class. Figure \ref{fig:2_Ne_vs_NA_non-Anxious} focuses on the influence of features for the non-anxious class in the neutral versus non-anxious classification. Larger values of eye landmarks (i.e., eye\_lmk\_Y\_51) and facial landmarks (i.e., Y\_15, x\_12) push the model towards predicting the non-anxious class. However, smaller values of eye gaze direction for the left and right eyes (i.e., gaze\_0\_y, gaze\_0\_z, gaze\_1\_y, gaze\_1\_z), smaller gaze angles (i.e., gaze\_angle\_y - looking up and down), and smaller left upper cheek values (i.e., Y\_1, Y\_3) steer the model away from the non-anxious class, favoring the neutral class instead.

\begin{figure*}
  \centering

  % Row 1
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.25]{images/SHAP/2_anxious_neutral_shape_summary.png}
    \caption{Anxious versus Neutral}
    \label{fig:2_anxious_neutral_shape_summary}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.25]{images/SHAP/2_anxious_non-anxious_shape_summary.png}
    \caption{Anxious versus non-Anxious}
    \label{fig:2_anxious_non-anxious_shape_summary}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.25]{images/SHAP/2_neutral_non-anxious_shape_summary.png}
    \caption{Neutral versus non-Anxious}
    \label{fig:2_neutral_non-anxious_shape_summary}
  \end{subfigure}

  \vspace{1em} % Space between rows

  % Row 2
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.25]{images/SHAP/2_A_vs_Ne_Anxious.png}
    \caption{Anxious versus Neutral}
    \label{fig:2_A_vs_Ne_Anxious}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.25]{images/SHAP/2_A_vs_NA_Anxious.png}
    \caption{Anxious versus non-Anxious}
    \label{fig:2_A_vs_NA_Anxious}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.25]{images/SHAP/2_Ne_vs_NA_non-Anxious.png}
    \caption{Neutral versus non-Anxious}
    \label{fig:2_Ne_vs_NA_non-Anxious}
  \end{subfigure}

  \caption{Binary classification Shap explanations. Figure a, b, and c shows the top ten important features. Figure d, e, and f shows the influence of features shown in Figure a, b, and c respectively. }
  \label{fig:all_images}
\end{figure*}


\section{Bias investigation}
Literature suggests that ML models can sometimes be biased due to factors such as gender, age, etc \cite{cheong2023towards, chu2023age}. Moreover, in this AnxietyFaceTrack study, we use facial features, which can vary based on gender and age \cite{bannister2022sex}. This highlights the need to assess biases in our ML models related to these factors. To investigate potential bias in our trained model, we checked if AnxietyFaceTrack suffers from any bias. We split our test data into two gender groups: Male and Female. For age, we categorized the test data into Undergrad and Graduate. Additionally, we examined bias based on participants' home locations by dividing the test data into Rural and Urban groups.

Figure \ref{fig: bias_investigation} shows the performance results of our models, revealing several key observations. First, for gender (see Figure \ref{fig:bias_gender}), the classification metrics were higher for females, with a difference of 5-7\% compared to males. This suggests that AnxietyFaceTrack works better for females, possibly because females tend to display emotions more prominently through facial expressions than males. Second, for education level (see Figure \ref{fig:bias_age}), we observed about a 10\% difference in classification metrics between graduate and undergrad participants, indicating that the model was better at learning the more subtle behaviors of graduate participants. Lastly, for location (see Figure \ref{fig:bias_location}), the model performed similarly for both rural and urban groups. However, precision was higher for the rural group. Upon closer inspection of the precision for individual labels, we found that precision for rural participants was above 95\%, while for urban participants, it was around 85\%. Our analysis of these biases aims to increase transparency in ML models for anxiety detection and provides valuable insights for future research on anxiety detection.

\begin{figure*}
  \centering
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.42]{images/bias_gender.pdf}
    \caption{Gender}
    \label{fig:bias_gender}
  \end{subfigure}
  % \hfill
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.42]{images/bias_age.pdf}
    \caption{Education Level}
    \label{fig:bias_age}
  \end{subfigure}
  % \hspace{0.001\textwidth}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[scale=0.42]{images/bias_location.pdf}
    \caption{Location}
    \label{fig:bias_location}
  \end{subfigure}
  \caption{Bias investigation}
  \label{fig: bias_investigation}
\end{figure*}

\section{Discussion}

\subsection{Summary of Results}
In this work, we designed a study, AnxietyFaceTrack, to detect anxiety in participants using facial videos recorded during a social scenario. The study aims to contribute to developing unobtrusive mental health assessment tools. The results of our AnxietyFaceTrack study provide valuable insights into anxiety detection using facial features and machine learning models. Our analysis shows that the Random Forest model, trained on either all 669 features or only the 3D landmark features (\#204), achieved the best overall classification performance, with an accuracy of 88\%. Furthermore, in the ablation study, we found that using just the top 20\% of important features determined by Random Forest feature importance yielded the highest accuracy of 91\% for multiclass classification. Interestingly, we observed that using only six head pose features (i.e., location and rotation) achieved an average accuracy of 85\%, which was just 6\% lower than the best-performing feature set in correctly identifying anxious, neutral, and non-anxious states. For binary classification, the Random Forest model also performed best, achieving average accuracies of 92\% for anxious versus neutral, 91\% for anxious versus non-anxious, and 93\% for neutral versus non-anxious. In summary, the multiclass and binary classification metrics of anxious, neutral, and non-anxious states are promising. Thus highlighting the potential of using facial features for accurate anxiety detection.

Using post hoc analysis, we obtained several key insights. First, we found that anxiety detection in both multiclass and binary classifications achieved similar performance metrics, suggesting that anxious participants can be effectively distinguished from non-anxious and neutral individuals when placed in socially anxious situations. Second, eye gaze performed the worst in both multiclass and binary classifications, even though it has been observed that socially anxious individuals tend to show avoidance gaze behavior compared to non-anxious individuals. Third, individual feature sets struggled in multiclass classification but performed better in binary classification. For example, the action unit feature achieved only 59\% accuracy in multiclass classification, but in binary classification, it achieved an average of 73\% accuracy. This suggests there may be some overlap in facial expressions or movements between anxious, non-anxious, and neutral participants. Fourth, the 3D landmarks and the combined head pose feature set performed the best in both multiclass and binary classification tasks.

Post hoc analysis using SHAP plots provided several insights. For example, facial edges (i.e., facial landmarks 0, 1, 3, 4, 11, 12, 13, 15, 16) and eye landmarks were identified as important features in anxiety detection. Larger values of these features influenced the classification model toward the anxious class. Interestingly, eye gaze features favored the neutral class in both multiclass and binary classifications. Furthermore, similar to the findings of Nepal et al., we found that 3D landmarks and head pose performed the best compared to other feature sets for anxiety detection. This alignment suggests that these feature sets could be used to develop mental disorder assessment tools.

Furthermore, our investigation into biases in the AnxietyFaceTrack model revealed several insights. Looking at the classification metrics, we found that the anxiety detection model yielded better results for female and graduate participants. It is important to note that the number of female participants was 36.26\%, and graduate students made up 28.57\%. Despite these proportions, the model was able to learn discriminating patterns more effectively for female and graduate participants. However, upon closer examination, we found that the precision of the anxious class was low for both male and female participants. The recall for the female anxious class was higher compared to males, suggesting that the model was better at capturing the anxious patterns in female participants. Another key finding was that the results were similar for rural and urban participants, even though rural participants made up only 23.08\%. This suggests that, regardless of whether participants grew up in rural or urban areas, the model does not discriminate in favor of one group over the other. These findings on biases provide crucial insights for future research, particularly involving face features and machine learning in anxiety detection.

In conclusion, our AnxietyFaceTrack study, which uses facial features extracted from low-cost smartphone camera videos and machine learning for anxiety detection, shows promise as an unobtrusive and continuous approach to mental health assessment, specifically for detecting anxiety.

\subsection{Implications}
Early detection is crucial for enabling timely interventions and promoting recovery for mental disorders. This study used facial videos captured through a low-cost smartphone camera for anxiety detection. Our promising results highlight the potential of smartphone-recorded facial videos and machine learning for the early detection of anxiety. This innovative approach can complement existing mental health assessments. Although our study was conducted in a controlled setting, the results pave the way for future research to explore our methodology in real-world settings, leading to a better understanding of anxiety and its early detection in fully naturalistic settings.

Furthermore, our use of low-cost smartphone cameras opens the possibility for anxiety detection through facial features to be feasibly integrated into everyday settings. This technology has the potential to be incorporated into smartphones, enabling early detection and monitoring of anxiety. Extending this work could also assist mental health professionals in routine assessments and interventions, helping to reduce the mental healthcare gap, especially in low-income and developing countries.

\subsection{Limitations}
\textit{AnxietyFaceTrack} study provides valuable insights into an unobtrusive mental health assessment tool for anxiety detection. However, it has certain limitations. First, the study was conducted in a controlled setting, which might limit the study findings in larger settings. However, the findings can serve as a baseline for future research conducted in either controlled or uncontrolled settings for anxiety detection. 

Second, the dataset size is limited due to the small number of participants. However, it is worth noting that machine-learning models were able to identify patterns associated with anxiety. Finally, our study used a self-reported survey questionnaire to create the ground truth, which may be subject to recall bias. Future studies could incorporate multiple questionnaires to reduce potential bias in participants' responses.
\section{Conclusion}
Through our AnxietyFaceTrack study, we demonstrated the use of facial videos recorded with low-cost smartphone cameras and machine learning to detect anxiety in social settings. Our study makes valuable contributions to the field of non intrusive mental health assessment, offering a solution that can be integrated into everyday smartphone use.

Ninety-one participants took part in our AnxietyFaceTrack study, with videos recorded while seated in a social setting created within a controlled environment. Facial features extracted from these videos were used to train multiclass and binary classification models. We achieved promising results, with an accuracy of 91\% for the multiclass classification of anxious versus neutral versus non-anxious. For binary classification, the accuracies were 92\% for anxious versus neutral, 92\% for anxious versus non-anxious, and 93\% for neutral versus non-anxious.

% \hl{OLD starts from here}
% \subsection{Data Collection}
% The study was approved by the institution's ethics board and conducted by a research assistant (RA). Three participants and the RA took part in a single study session. The RA ensured that the participants were unfamiliar with each other before sending out the study invitations. Upon arrival, the RA explained the study and obtained informed consent from each participant. After consent, camera recordings were initiated, with three individual cameras positioned on stands to record each participant respectively. The RA then instructed the participants to sit idle for two minutes. Following this, the RA asked the participants to complete a survey consisting of a single question about their anxiety state during the last two minutes. Participants rated their anxiety on a Likert scale from 1 to 5, where one indicated ``very anxious'' and five indicated ``very relaxed'' (distribution shown in Figure \ref{fig:histogram_baseline}). The participants' demographic information is presented in Table \ref{tab: participant_demographic}. A total of 111 participants participated in the study; however, 20 participants' data were excluded due to recording issues with the smartphone camera. In 91 participants, 58 were male, and 33 were female. Seventy participants reported an urban home residence, while 21 were from rural areas. The mean age of the participants was 20.71 years, with a standard deviation of 2.35 years.


% \begin{table*}[h]
% \centering
% \caption{Participants demographic information. SR - Self-reported anxiety, Note* - SR report closure 1 represents higher anxiety, i.e., Anxious, and SR report closure to 5 represents relaxed, i.e, Non-Anxious}
% \begin{tabular}{@{}cccccc@{}}
% \toprule
%  & \textbf{count} & \textbf{SR Score} & \textbf{Age}  & \textbf{Gender} & \textbf{Home}\\
%   & \textbf{\#} & \textbf{($\mu$, $\sigma$)} & {($\mu$, $\sigma$)} & {(Male, Female)}  &{(Urban, Rural)}\\ \midrule
% \textbf{All} & 91& (3.29, 1.03)& (20.71, 2.35)& (58, 33)&(70, 21)\\ \midrule
%  \textbf{Anxious}
% & 52& (2.52, 0.54)& (20.67, 2.39)& (32, 20)&(45, 7)\\
% \textbf{Non-Anxious}& 39& (4.31, 0.47)& (20.77, 2.33)& (26, 13)&(25, 14)\\ \bottomrule
% \end{tabular}
% \vspace{0.1cm}
% \label{tab: participant_demographic}
% \end{table*}

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{images/Histogram_Baseline.pdf}
%     \caption{Distribution of participants self-reported anxiety}
%     \label{fig:histogram_baseline}
% \end{figure}

% \subsection{Feature Extraction}
% Behavioral features were extracted from the recorded two-minute videos using the OpenFace\footnote{\url{https://github.com/TadasBaltrusaitis/OpenFace}} library \cite{baltruvsaitis2016openface}. OpenFace takes a video as input and generates a time series of 709 different features, which are categorized into five main sets: Gaze-related (288 features), Pose (6 features), Facial landmark locations in 2D (136 features) and 3D (204 features), Face shape in rigid (6 features) and non-rigid (34 features) forms, and Facial Action Units intensity (17 features) and presence (18 features). Now, we will discuss each of these feature sets in detail.

% The Gaze features include the gaze direction of both the left and right eyes in the x, y, and z coordinates and gaze angles in the x and y axes. Additionally, it includes 2D eye landmarks in the x and y coordinates and 3D eye landmarks in the x, y, and z dimensions, with the eye region divided into 55 landmark areas (as shown in Figure \ref{fig:eye_lm_left_eye} and \ref{fig:eye_lm_right_eye}). The pose features consist of the head's location and rotation, represented by pitch, yaw, and roll with respect to the camera. Facial landmarks indicate the positions of 67 facial landmarks in both 2D and 3D facial regions (as shown in Figure \ref{fig:facial_lm}). The Rigid and Non-Rigid Shape parameters provide information on the rigid and non-rigid face shapes, respectively. The Facial Action Units capture an individual's facial expressions, including the intensity and presence of 17 and 18 action units, respectively.


% \begin{figure*}
%   \centering
  
%   % First figure (Facial Landmarks)
%   \begin{subfigure}{0.8\textwidth}
%     \centering
%     \includegraphics[scale=0.1]{images/landmark_scheme_68.pdf}
%     \caption{Facial Landmarks}
%     \label{fig:facial_lm}
%   \end{subfigure}
  
%   % Second row with left and right eye
%   \begin{subfigure}{0.49\textwidth}
%     \centering
%     \includegraphics[width=0.99\textwidth, height=3.25cm]{images/left_eye.pdf}
%     \caption{Left Eye}
%     \label{fig:eye_lm_left_eye}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.49\textwidth}
%     \centering
%     \includegraphics[width=0.99\textwidth, height=3.25cm]{images/right_eye.pdf}
%     \caption{Right Eye}
%     \label{fig:eye_lm_right_eye}
%   \end{subfigure}
%   \caption [Image credit]{Landmarks in Face, left Eye, and right Eye (Image credit\protect\footnotemark).}
%   \label{fig:landmark}
% \end{figure*}

% \subsection{Data Preparation and Feature Selection}
% The output of OpenFace is in time series format. For example, if a participant's video is 2 minutes long with a sampling rate of 30 frames per second, the extracted feature matrix will have dimensions of 3600 × 704, where 3600 represents the total number of frames, and 704 denotes the features extracted per frame. Now, to reduce this dimensionality, we computed statistical features, i.e., each feature's mean and standard deviation across all frames \cite{sun2022estimating}, resulting in a single feature vector of size 1 × 1418 for each participant. Further, we used correlation analysis for feature selection, dropping the highly correlated features based on a correlation threshold of 0.75, as suggested in the literature on affective computing \cite{rashid2020predicting}.

% We trained and tested the classification model using different dataset configurations, as shown in Figure \ref{fig:dataset_config}. This experiment was done to determine which statistical features, i.e., mean or standard deviation, work best for anxiety prediction.

% \subsection{Anxiety Classification}
% We used the anxiety levels reported by participants as the ground truth. Participants who rated themselves 1, 2, or 3 on the Likert scale were labeled as the anxious group, while those who rated themselves 4 or 5 were labeled as the non-anxious group, thus creating binary labels \cite{harshit2024eyes}. We employed four machine learning classification models, including two classical Machine Learning (ML) models, i.e., Random Forest (RF), Support Vector Machine (SVM), AdaBoost, and XGBoost.

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.75\linewidth]{images/dataset.pdf}
%     \caption{Dataset Configuration used for training classification model}
%     \label{fig:dataset_config}
% \end{figure*}

% % Add footnote text outside the figure
% \footnotetext{OpenFace - \url{https://github.com/TadasBaltrusaitis/OpenFace/wiki/Output-Format}}
% \section{Results}
% We used 5-fold subject-independent cross-validation\footnote{Cross Validation - \url{https://scikit-learn.org/stable/modules/cross_validation.html}} using the scikit-learn library. Further, average accuracy and F1 score were used as classification evaluation metrics. The performance of the classification algorithms was assessed across various dataset configurations (see Figure \ref{fig:dataset_config}: $\mu$ of each feature (D1), Reduced $\mu$ Features (D2), $\sigma$ of each Features (D3), Reduced $\sigma$ Features (D4), $\mu$ of each feature and $\sigma$ of each feature (D5), Reduced $\mu$ + Reduced $\sigma$ Features (D6), and Reduced ($\sigma$ + $\mu$) Features (D7). 

% The results of the classification models are presented in Table \ref{tab:evaluation_table}. We got the highest accuracy of 65\% and an F1 score of 0.72 using the Random Forest model with the combined mean and standard deviation features. 

% %--------------
% \begin{table*}
% \caption{Classification results with 5-fold cross-validation. Acc refers to accuracy. Both accuracy and F1 score are averaged across all folds.}
% \centering
% \tiny
% \label{tab:evaluation_table}
% {%
% \begin{tabular}{lclclclc}
% \toprule
% \textbf{Dataset Configuration} & \textbf{\# Features} && \textbf{Classifier}  && \textbf{Acc}  && \textbf{F1} \\ \midrule
%  % -& -& & Baseline& & 0.50& &0.50\\ \midrule
% \multirow{4}{*}{\textbf{D1: $\mu$ of each Features}} & \multirow{4}{*}{709}  && RF  && 0.54  && 0.63 \\
%  &   && SVM  && 0.57  && 0.73 \\
%  &   && AdaBoost  && 0.42  && 0.51 \\
%  &   && XGBoost  && 0.52  && 0.6 \\ \midrule
% \multirow{4}{*}{\textbf{D2: Reduced $\mu$ Features}} & \multirow{4}{*}{75}  && RF  && 0.58  && 0.69 \\
%  &   && SVM  && 0.57  && 0.73 \\
%  &   && AdaBoost  && 0.48  && 0.57 \\
%  &   && XGBoost  && 0.52  && 0.58 \\ \midrule
% \multirow{4}{*}{\textbf{D3: Only $\sigma$ of each Features}} & \multirow{4}{*}{709}  && RF  && 0.57  && 0.66 \\
%  &   && SVM  && 0.56  && 0.69 \\
%  &   && AdaBoost  && 0.49  && 0.59 \\
%  &   && XGBoost  && 0.57  && 0.63 \\ \midrule
% \multirow{4}{*}{\textbf{D4: Reduced $\sigma$ Features}} & \multirow{4}{*}{68}  && RF  && 0.55  && 0.63 \\
%  &   && SVM  && 0.56  && 0.72 \\
%  &   && AdaBoost  && 0.51  && 0.57 \\
%  &   && XGBoost  && 0.51  && 0.57 \\ \midrule
% \multirow{4}{*} {\textbf{D5: $\mu$ of each Features and $\sigma$ of each Features}} & \multirow{4}{*}{1418}  && RF  && 0.65  && 0.72 \\
%  &   && SVM  && 0.57  && 0.73 \\
%  &   && AdaBoost  && 0.44  && 0.51 \\
%  &   && XGBoost  && 0.53  && 0.6 \\ \midrule
% \multirow{4}{*} {\textbf{D6: Reduced $\mu$ + Reduced $\sigma$ Features}} & \multirow{4}{*}{143}  && RF  && 0.54  && 0.64 \\
%  &   && SVM  && 0.57  && 0.73 \\
%  &   && AdaBoost  && 0.53  && 0.6 \\
%  &   && XGBoost  && 0.56  && 0.62 \\ \midrule
% \multirow{4}{*} {\textbf{D7: Reduced ($\mu$ + $\sigma$) Features}}& \multirow{4}{*}{114}  && RF  && 0.51  && 0.62 \\
%  &   && SVM  && 0.57  && 0.73 \\
%  &   && AdaBoost  && 0.63  && 0.69 \\
%  &   && XGBoost  && 0.53  && 0.59 \\ \bottomrule 
% \end{tabular}%
% }
% \end{table*}



% %------------------
% \section{Discussion}
% The obtained result suggests that the behavioral cues extracted from participants' videos can serve as predictive indicators for detecting anxiety. The classification evaluation metric achieved by the classical ML models suggests that the ML models can learn the distinguishing features between anxious and non-anxious groups. Furthermore, we found that the individual mean and standard deviation features performed poorly when assessed separately, but their combination yielded better results in classification.

% \subsection{Implications}
% Early detection is crucial for facilitating timely interventions and promoting recovery. Through our analysis, we found that behavioral cues such as head pose, facial landmarks, and eye gaze patterns extracted from videos can predict anxiety and can be explored for the early detection of anxious individuals. 

% In this study, we employed a low-cost smartphone camera, demonstrating that anxiety detection through behavioral cues can be feasibly integrated into real-world settings. This technology has the potential to be incorporated into everyday smartphones, enabling early detection and monitoring of anxiety. Moreover, the extension of this work can support mental health professionals in routine assessments and interventions, thus reducing the mental healthcare gap especially in low-income and developing countries

% \subsection{Limitations}
% % \noindent \textbf{Limitations}: 
% Our study represents a pioneering step in using behavioral cues from upper body videos for detecting anxiety. However, it has certain limitations, which are outlined below:
% \begin{itemize}
%     \item \textit{Limited Sample Size}: We demonstrated the application of visual features for anxiety detection with only 91 participants, which may have affected the learning of the ML models. However, recruiting human participants for data collection is inherently challenging.
%     \item \textit{Loss of Temporal Information}: While we computed statistical features from the time series data, this approach may have missed valuable temporal information. 
%     \item \textit{Limited Statistical Features}: We only computed mean and standard deviation as the statistical features, which may have affected our results. However, calculating a larger number of statistical features with a fixed number of data points can lead to overfitting.
% \end{itemize}

% \section{Future Work}
% Anxiety detection through behavioral cues in videos can be further enhanced by expanding data collection to real-world settings, where data can be gathered ``in the wild''. Moreover, future work could experiment with different types of cameras for data collection. Using a low-cost smartphone camera in our study may have contributed to the observed accuracy limitation of 65\%. Additionally, this study was conducted on a university student population; future research should aim to collect data from diverse age groups and demographics to increase the generalizability of the findings.

% We only utilized two statistical features in our analysis, which future studies could expand upon and explore further. While we applied classical machine learning (ML) models, future work could benefit from employing deep learning (DL) models, as they can handle temporal data. In extension, we plan to use DL models, such as Convolutional Neural Networks (CNN), for feature extraction from temporal OpenFace outputs and subsequent classification in the future.


% \section{Conclusion}
% This study is the first to explore the use of behavioral features (eye gaze, head pose, facial landmarks, and action units) extracted from upper-body participants' videos for anxiety detection. We trained classical machine learning models using these behavioral features and self-reported data provided by participants and achieved the highest accuracy of 65\%. These findings suggest that behavioral features hold promise for further exploration in anxiety detection and could potentially be leveraged for early detection and monitoring of anxiety.

\bibliographystyle{ACM-Reference-Format}
\bibliographystyle{unsrt}
\bibliography{sample-base}



\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
