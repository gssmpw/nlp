\documentclass{article}


\usepackage{arxiv}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\input{math_commands.tex}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{arydshln}
\usepackage{float}
\usepackage{hyperref}
\usepackage[OT1]{fontenc} 

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\def\H{\mathcal{H}}
\def\h{\mathbf{h}}
\def\E{\mathbb{E}}
\def\K{\mathcal{K}}
\def\P{\mathcal{P}}
\def\I{\mathcal{I}}
\newcommand{\reals}{\mathcal{R}} 
\newcommand{\W}{\mathsf{Wealth}}
\def\E{\mathbb{E}}
\newcommand{\myeqref}[1]{(\ref{#1})}



\title{Optimistic Interior Point Methods for Sequential Hypothesis Testing by Betting}

\author{ {Can Chen} \\
	University of California San Diego\\
	\texttt{cac024@ucsd.edu} \\
	%% examples of more authors
	\And
	{Jun-Kun Wang} \\
	University of California San Diego\\
	\texttt{jkw005@ucsd.edu} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}

% Uncomment to remove the date
\date{}

% Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{}
\renewcommand{\undertitle}{}
%\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle
\thispagestyle{fancy}  

\footnotetext[1]{The code and data are released at \href{https://github.com/canchen-cc/oip-betting}{https://github.com/canchen-cc/oip-betting}.}


\begin{abstract}
	The technique of ``testing by betting" frames nonparametric sequential hypothesis testing as a multiple-round game, where a player bets on future observations that arrive in a streaming fashion, accumulates wealth that quantifies evidence against the null hypothesis, and rejects the null once the wealth exceeds a specified threshold while controlling the false positive error. Designing an online learning algorithm 
    that achieves a small regret in the game can help rapidly accumulate the bettor's wealth, which in turn can shorten the time to reject the null hypothesis under the alternative $H_1$. However, many of the existing works employ the Online Newton Step (ONS) to update within a halved decision space to avoid a gradient explosion issue, which is potentially conservative for rapid wealth accumulation. In this paper, we introduce a novel strategy utilizing interior-point methods in optimization that allows updates across the entire interior of the decision space without the risk of gradient explosion. Our approach not only maintains strong statistical guarantees but also facilitates faster null hypothesis rejection in critical scenarios, overcoming the limitations of existing approaches.
\end{abstract}

\section{Introduction}

Sequential hypothesis testing examines a sequence of observations with the goal of rigorously assessing the validity of the null hypothesis $\mathcal{H}_0$ against the alternative $\mathcal{H}_1$. 
Although a classical problem in statistics, sequential hypothesis testing has gained renewed significance in contemporary contexts \cite{ramdas2023game,grunwald2024anytime}. One of the main reasons perhaps is the recent surge in algorithmic systems and machine learning applications \cite{Chugg2023,chen2024online,teneggi2024bet,bar2024protected}, which has given rise to a myriad of nuanced desiderata for the hypothesis testing methods.
% so that the underlying applications can be robust and reliable. 
These requirements include the ability to continuously monitor incoming data rather than adhere to a fixed-sample size setting; the need to conduct a nonparametric test instead of making distributional assumptions such as assuming that the data follow a certain distribution; and the call to maintain a valid test at any stopping time.
This has hence catalyzed substantial research interest in the algorithmic development of sequential hypothesis testing for simultaneously satisfying these criteria while enjoying provable statistical guarantees \cite{ramdas2024hypothesis}.

In pursuit of these desiderata, sequential hypothesis testing via betting has evolved into a cornerstone methodology and has seen substantial advancements in recent years \citep{shafer2021testing,vovk2021values,shekhar2023nonparametric,ramdas2023game,grunwald2024anytime}. 
In particular, the techniques of "testing by betting" have found use in many machine learning applications, which include: auditing the fairness of a classifier \cite{Chugg2023}, online detection of whether a text sequence source is an LLM \cite{chen2024online}, monitoring distribution shifts for test-time adaptation \cite{bar2024protected}, examining the importance of semantic concepts in a model's prediction for explainable AI \cite{teneggi2024bet}, evaluating voxel responses in neuroimaging data \cite{fischer2024multiple}, testing conditional independence of ride duration and membership for a bikeshare system \cite{grunwald2024anytime}, 
multi-arm bandit problems \cite{cho2024peeking}, adversarial attacks \cite{pandeva2024deep}, 
estimating the mean of a bounded random variable \cite{waudby2024estimating}, and more \cite{shaer2023model,podkopaev2023sequential,podkopaev2024sequential,PXL24}.

The game-theoretic approaches typically frame the sequential hypothesis testing as a repeated game between the online learner and nature \cite{shafer2019game,shafer2021testing,shekhar2023nonparametric}. Informally, the crux lies in designing a wealth process for an online learner (i.e., the bettor) in the game such that, when the learner's wealth becomes sufficiently large, it increases their confidence in rejecting the null hypothesis. 
However, to be more concrete and get the ball rolling, 
let us consider a non-parametric two-sample testing task in many prior works (e.g., \citet{shekhar2023nonparametric,Chugg2023,chen2024online,teneggi2024bet}). In this setting, one observes two sequences of samples $\{ x_t\}_{t\geq 1}$ and $\{ y_t\}_{t\geq1}$, where $x_t$ and $y_t$ are pair of bounded random variables observed at time $t$, with their population means denoted as $\mu_x:=\mathbb{E}[x_t]$ and $\mu_y:=\mathbb{E}[y_t]$ respectively. The hypothesis testing task can be expressed as: 
\begin{equation}
\H_0: \mu_x = \mu_y, \quad   \text{versus} \quad  \H_1: \mu_x \neq \mu_y.
\end{equation}
As the related works \cite{Chugg2023,shekhar2023nonparametric,teneggi2024bet}, we assume $x_t \in [0, 1]$ and $y_t \in [0, 1]$ in this paper, and we note that the modifications for extending the range is relatively straightforward (see e.g., \citet{chen2024online}). We will also denote $g_t := x_t - y_t \in [-1,1]$ in the following. 

In the game of testing by betting, the online learner usually starts with an initial wealth of $W_1 = 1$ and makes sequential decisions over time. At each round $t$, prior to observing an observation $g_t$, the learner selects a decision point $\theta_t \in \K$, where $\K$ denotes the learner's decision space.  The learner's wealth dynamic is governed by the following update rule:
%\begin{equation}
$\textstyle W_{t+1} = W_t \cdot \left(1 - g_t \theta_t \right).$
%\end{equation}
One can interpret the magnitude of $\theta_t$ as the amount of the wealth that the online learner bets on the outcome $g_t$. The high-level idea of testing by betting is that when the wealth $W_t$ surpasses a certain threshold, i.e., when $W_t \geq \frac{1}{\alpha}$, where $\alpha > 0$ is a parameter, the null hypothesis $\mathcal{H}_0$ can be rejected with high confidence. However, to ensure this strategy enjoy strong statistical guarantees, the common algorithmic design principle in the literature is to let the wealth $(W_t)_{t\geq1}$ be a nonnegative supermartingale when $\mathcal{H}_0$ is true.
Then, one can use Ville's inequality~\citep{Ville1939} to control the false positive rate.
More precisely, Ville's inequality states that if $(W_t)_{t\geq 1}$ is a nonnegative supermartingale, then $P(\exists t : W_t \geq {1}/{\alpha}) \leq \alpha \mathbb{E}[W_1]$. 
Hence, with the initial wealth $W_1=1$ and an appropriate choice of the learner's constraint set $\K$ to make sure $W_t$ is nonnegative, the Type-I error at any stopping time can be bounded by $\alpha$. Several previous works simply let the decision space $\K$ be $\K=[-1/2, 1/2]$, e.g., \citet{pandeva2024deep,shekhar2023nonparametric,podkopaev2024sequential,podkopaev2023sequential,waudby2024estimating,teneggi2024bet,Chugg2023,chen2024online}.
However, as we shall elucidate and underscore soon, the selection of the decision space in much of the related literature is potentially conservative, thereby leaving considerable room for algorithmic improvements in conjunction with novel update schemes.


When the alternative $\mathcal{H}_1$ is true, on the other hand, one would wish to quickly reject the null hypothesis $\mathcal{H}_0$. This motivates the idea of instantiating the bettor as an algorithm in online learning (a.k.a.~no-regret learning), see e.g., \citet{Hazan2016OnlineConvexOptimization,orabona2019modern}.
% which has seen significant progress over the past couple of decades. 
In online learning, the learner aims to obtain a sublinear regret (w.r.t.~the number of rounds $T$): $ 
\mathrm{Regret}_T(\theta_*) := \sum_{t=1}^T \ell_t(\theta_t) - \sum_{t=1}^T \ell_t(\theta_*)$,
where $\theta_* \in \mathcal{K}$ is a comparator in the learner's decision space $\K$. An observation is that if we define 
$\ell_t(\theta_t):= - \ln (1-g_t \theta_t)$, then there is a nice relation between \emph{regret minimization} and \emph{wealth maximization} in the betting game. In particular, we have $
\ln ( W_T) = \sum_{t=1}^T  \ln (1-g_t \theta_t) = - \sum_{t=1}^T \ell_t(\theta_t).$
Hence, a smaller regret bound translates to a faster growth of the learner's wealth $W_T$, which in turn can lead to a shorter time to reject the null when the alternative holds.
Existing works of testing by betting such as \citet{pandeva2024deep,shekhar2023nonparametric,podkopaev2024sequential,podkopaev2023sequential,waudby2024estimating,teneggi2024bet,Chugg2023,chen2024online} adopt Online Newton Steps (ONS)
\citep{hazan2007logarithmic}
to update $\theta_t$ in the decision space $\K=[-1/2, 1/2]$.
Such a \emph{heuristic} choice of the decision space $\K$ arises from the fact that if the learner's action $\theta_t$ is allowed to be $\K=[-1,1]$, there is a chance  that the learner's loss will explode, as the value of $1-g_t \theta_t$ could be (close to) $0$. Yet, the online learner has to determine its action $\theta_t$ \emph{before} observing $g_t$. 
Therefore, the conservative decision space is adopted in these works.

The limitation in prior works lies in the fact that when $g_t$ is relatively small, the learner could benefit from allocating a larger magnitude of $\theta_t$ to accelerate the growth of their wealth, which in turn helps reduce the time needed to reject the null hypothesis when the alternative $\H_1$ holds. For example, if the learner adopts a more aggressive betting strategy by selecting $\theta_t = 1$, and the outcome turns out to be $g_t = -0.1$, they would undoubtedly gain a larger fortune compared to being restricted to $\theta_t = \frac{1}{2}$. While this idea is intuitive, the challenge lies in designing an update strategy on the largest possible decision space $\K = [-1, 1]$ such that a larger bet can be made at some points while avoiding the risk of loss explosion. In other words, can we design more efficient test-by-betting algorithms that provide strong statistical guarantees? 
Our work seeks to provide solutions to this question and tackle the common issue in prior works.

We will leverage the techniques of interior-point methods in optimization literature \citep{nesterov1994interior,nemirovski2004interior,wright1997primal} to avoid the loss explosion issue and a vacuous regret, while allowing a large decision space in the betting game. 

In particular, we will use the toolkit of self-concordant barrier functions and propose two novel methods for testing by betting. We note that the toolkit of self-concordant barrier functions have found powerful for getting provable fast convergence of the Newton's method in optimization \citep{nemirovski2008interior}, designing efficient bandit online learning algorithms \citep{abernethy2012interior}, and providing non-trivial non-asymptotic analyses for logistic regression \citep{bach2010self}. However, to our knowledge, this is the first time that this technique has been incorporated in the area of sequential hypothesis testing via betting.

To give the reader a flavor of our contributions, we provide a summary of our results here.
\begin{enumerate}
\item We highlight a potential limitation that was overlooked in the literature of sequential hypothesis testing by betting. 
We propose Follow-the-Regularized-Leader (FTRL) \citep{hazan2010extracting,abernethy2012interior} with a barrier function as the regularization that tailored to the betting game. We show that our proposed method is an anytime-valid level-$\alpha$ sequential hypothesis test with asymptotic power of 1. Moreover, we identify a key condition with  concrete examples where the proposed method achieves a shorter expected time to reject the null hypothesis $\mathcal{H}_0$ under the alternative $\H_1$, compared to Online Newton Steps (ONS), which has been adopted in many prior works.
\item
Next, we incorporate the idea of optimistic online learning \citep{rakhlin2013online,syrgkanis2015fast,wang2018acceleration} to propose another strategy for testing by betting, which we denote as Optimistic-FTRL + Barrier. We show that the method is also an anytime-valid test that controls both Type-I and Type-II errors.
Furthermore, when the sequence of samples become ``predictive'', the proposed \emph{optimistic} testing-by-betting strategy could achieve a faster rejection time for $\mathcal{H}_0$ when $\mathcal{H}_1$ is true, compared to the first one without the mechanism of optimistic learning.
\item
 Finally, we evaluate our methods on three synthetic datasets, and two important machine learning applications: online detection of LLMs \cite{chen2024online}, and evaluating metrics of facial expression classifiers \cite{podkopaev2023sequential}. Our experimental results show the encouraging real-world effectiveness of the proposed methods. 
\end{enumerate}

\section{Preliminaries}

In this section, we provide additional necessary background and notations before introducing our algorithms in the following sections.

\noindent
\textbf{Level-$\alpha$ sequential hypothesis test with asymptotic power one.}
Consider a sequence of observations $\{Z_i: i \geq 1\}$ and let $\mathcal{F} = (\mathcal{F}_t)_{t \geq 0}$ be the forward filtration where each $\mathcal{F}_t = \sigma(Z_1, \ldots, Z_t)$ captures the information available up to time $t$. For any process $W := (W_t)_{t \geq 1}$ adapted to this filtration, we say $W$ is a $\P$-martingale if it satisfies
$\E_{\P}[W_{t} | \mathcal{F}_{t-1}] = W_{t-1}$ for all $t \geq 1,$
and a $\P$-supermartingale if the equality is replaced by an inequality:
$\E_{\P}[W_{t} | \mathcal{F}_{t-1}] \leq W_{t-1}, \forall t \geq 1.$
Let a binary-valued variable $\I_{t} \in \{0,1\}$ be an indicator of the rejection of the null hypothesis $H_0$ at a stopping time $t$. A sequential hypothesis test constructed from a martingale process $W$ maintains its level-$\alpha$ test if
$\sup_{\P \in H_0} P\left(\exists t \geq 1: \I_{t} = 1 \right) \leq \alpha$.
On the other hand, a sequential test achieves asymptotic power $1-\beta$ if 
$\sup_{\P \in H_1} P\left(\forall  t \geq 1: \I_{t} = 0 \right) \leq \beta$.
Of particular interest is the case where $\beta = 0$, which corresponds to asymptotic power one. Specifically, this guarantees that under $H_1$, the underlying sequential hypothesis testing method will eventually reject the null hypothesis $H_0$. As we will demonstrate, the proposed two new algorithms in this work are both level-$\alpha$ tests with asymptotic power one.


\noindent
\textbf{Self-concordant functions.}
A \textit{self-concordant function} $R(\cdot):\text{int}(\mathcal{K}) \rightarrow \mathbb{R}$ is a thrice continuously differentiable convex function such that for all $h \in \mathbb{R}^d$ and $\theta \in \text{int}(\mathcal{K})$, 
\begin{equation}
    \left| D^3 R(\theta)[h, h, h] \right| \leq 2 \left( D^2 R(\theta)[h, h] \right)^{3/2}
    \label{eq:self_concordant},
\end{equation}
where $D^3 R(\theta)[h, h, h]$ is the third-order differential, i.e.,
$ D^3 R(\theta)[q, r, s] := \frac{\partial^3}{\partial \delta_1 \partial \delta_2 \partial \delta_3} |_{\delta_1=\delta_2=\delta_3=0} R(\theta+ \delta_1 q + \delta_2 r  + \delta_3 s  )$ is the third-order differential taken at $\theta$ along the directions $q,r,s$; and similarly, $D^2 R(\theta)[h, h]$ is the second-order differential.
Given a self-concordant function $R(\cdot)$, for any point $\theta \in\text{int}(\K)$, we can define a norm $\| \cdot \|_{\theta}$ and its dual norm $\| \cdot \|_{\theta}^*$ as:
\begin{equation}
\| h \|_{\theta} := \sqrt{ h^\top \nabla^2 R(\theta) h } \, \text{ and } \,
\| h \|_{\theta}^* := \sqrt{ h^\top \nabla^{-2} R(\theta) h },
\end{equation}
where $\nabla^{-2} R(\theta) := (\nabla^2 R(\theta) )^{-1}$ is the inverse of the Hessian.
We refer the reader to \citet{nesterov1994interior,nemirovski2008interior,nemirovski2004interior} for the exposition of self-concordant functions

\noindent
\textbf{Sequential Hypothesis Testing by Betting.} %To get the ball rolling, 
Here we provide more backgrounds of testing via betting. 
The meta-algorithm that forms the basis of several prior works is detailed in Algorithm~\ref{alg:Betting} \citep{pandeva2024deep,shekhar2023nonparametric,podkopaev2024sequential,podkopaev2023sequential,waudby2024estimating,teneggi2024bet,Chugg2023,chen2024online}. 
At each round $t$, Algorithm~\ref{alg:Betting} selects a point $\theta_t$ based on an online learning algorithm $\mathrm{OAlg}$.
Subsequently, it observes the samples $x_t$ and $y_t$, and hence it sees the loss function $\ell_t(\cdot)$ as well. The bettor's wealth is then updated according to the dynamics $W_{t+1} = W_t \left(1 - g_t \theta_t \right)$. Then, $\mathrm{OAlg}$ updates the next action $\theta_{t+1}$ by using $\ell_t(\cdot)$ and potentially the history of past ones. 
If the wealth $W_t$ exceeds $1/\alpha$, it declares that the null hypothesis $\H_0$ is false (Line 8). Moreover, if there is a time budget (i.e., $T < \infty$) and the timer runs out, it may reject $H_0$ under a condition (Line 11-13). For $\mathrm{OAlg}$, the aforementioned works adopt ONS, and we replicate the update in Algorithm~\ref{alg:ONS} for the reader's convenience. For the comparison in the next section, we also provide the regret bound guarantee of ONS for the betting game in Lemma~\ref{lem:ONS} below, which is a known result in the literature (e.g., proof of Lemma~1 in \cite{chen2024online}).

\begin{algorithm}[t]
\caption{Sequential Hypothesis Testing by Betting}
\label{alg:Betting}
\begin{algorithmic}[1] 
\State \textbf{Init:} wealth $W_1 \gets 1$, significance level parameter $\alpha \in (0, 1)$, and time budget $T \in [1,\infty]$.
\State \textbf{Input:} online learning algorithm $\mathrm{OAlg}$.
%\State \small \textbf{Time budget} $T$ can be set to be $\infty$.
\State \textbf{For} $t=1,2,\dots, T$ \textbf{do}
\State \quad Play $\theta_t \in \K$ by $\mathrm{OAlg}$.
\State \quad Observe $x_t$ and $y_t$.
\State \quad Set $g_t \gets x_t - y_t$.
\State \quad Update learner's wealth $W_{t+1} = W_t \cdot \left(1 - g_t \theta_t \right)$.
\State \quad \textbf{If} $W_t \geq 1/\alpha$, \textbf{then} reject the null $\H_0$.
\State \quad Send $\ell_t(\cdot) \to \mathrm{OAlg}$, where $\ell_t(\theta_t)= - \ln (1-g_t \theta_t)$.
\State \textbf{End For}
\State \textbf{If} the null $\H_0$ has not been rejected, \textbf{then}
\State \quad Sample $\nu \sim \text{Uniform}[0, 1]$. 
\State \quad \textbf{If} $W_T \geq \nu/\alpha$, \textbf{then} reject $\H_0$.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{ONS as $\mathrm{OAlg}$ for Testing by Betting
\citep{Cutkosky2018, Chugg2023, podkopaev2023sequential, teneggi2024bet, chen2024online} 
}
\label{alg:ONS}
\begin{algorithmic}[1] 
\State Set the decision space $\K=[-1/2,1/2]$.
\State Init $a_0 \gets 1$. 
\State \textbf{For} $t=1,2,\dots, T$ \textbf{do}
\State \quad Play $\theta_t \in \K$.
\State \quad Receive loss function $\ell_t(\theta)= - \ln (1-g_t \theta)$ and incur loss $\ell_t(\theta_t)$.
\State \quad Compute $b_t = \frac{\partial \ell_t(\theta_t)}{\partial \theta}=\frac{g_t}{1-g_t\theta_t}$ and $a_t = a_{t-1} + b_t^2$.
\State \quad Update $\theta_{t+1}=\max\left(\min\left(\theta_t-\frac{2}{2-\ln 3}\frac{b_t}{a_t},\frac{1}{2}\right),-\frac{1}{2}\right)$.
\State \textbf{End For}
\end{algorithmic}
\end{algorithm}
\begin{lemma} \label{lem:ONS} Consider the scenario of bounded random variables $x_t \in [0,1]$ 
and $y_t \in [0,1]$ in the betting game. Online Newton Steps (ONS) (Algorithm~\ref{alg:ONS}) has
 $  \text{Regret}_T(\theta_*)
 \lesssim \ln\left(\sum_{t=1}^T{g_t^2}\right)$.
\end{lemma}
Lemma~\ref{lem:ONS} implies that ONS has an $O(\ln(T))$ regret. 

Now we highlight the connection of online learning and sequential hypothesis testing in the following theorem.
Recall that an algorithm is a no-regret learning algorithms if its regret is sublinear with the number of rounds $T$, i.e., $\frac{\mathrm{Regret}_T(\theta_*)}{T} \to 0 \text{ as } T \to \infty$ \citep{orabona2019modern,wang2024no}, and hence a no-regret learner has a vanishing per-round regret.
Theorem~\ref{thm:1} below shows that Algorithm~\ref{alg:Betting} with $\mathrm{OAlg}$ being any no-regret learning algorithms has strong statistical guarantees. The proof of Theorem~\ref{thm:1} is in Appendix~\ref{app:thm:1}. 
To our knowledge, Theorem~\ref{thm:1} has not been explicitly stated in prior literature. However, its result has indeed been implicitly proven through the analysis of ONS for betting in prior works (e.g., \citet{Chugg2023,shekhar2023nonparametric}). We present Theorem~\ref{thm:1} to highlight the modularity of the approach of testing by no-regret learning, and we note that Theorem~\ref{thm:1} covers the prior result when ONS is used to instantiate $\mathrm{OAlg}$.
\begin{theorem} \label{thm:1}
Algorithm~\ref{alg:Betting} with $\mathrm{OAlg}$ being a no-regret learning algorithm is a level-$\alpha$ sequential test with asymptotic power one. 
\end{theorem}

We note that the machinery can be naturally extended to the case where the distributions generating $x_t$ and $y_t$ are ever-changing without changing the algorithm --- with one modification of the testing task:
\begin{equation} \label{setting:distributionshifts}
\H_0: \mu_x(t) = \mu_y(t), \forall t \, \, \text{v.s.} \, \, \H_1: \exists t \geq 1: \mu_x(t) \neq \mu_y(t),
\end{equation}
where $\mu_x(t) = \E[x| \mathcal{F}_{t-1}]$ (and $\mu_y(t)$ similarly defined).
We refer the reader to Section 3.4 in \citet{Chugg2023} for the nice treatment of handling distribution shifts.


While Theorem~\ref{thm:1} establishes that Algorithm~\ref{alg:Betting} equipped with a no-regret learning algorithm has power $1$ asymptotically, deriving a concrete non-asymptotic bound on the rejection time when $\H_1$ holds might be more desirable from a practical standpoint.
Furthermore, one would like to quickly reject the null $\H_0$ when it is false.
These considerations motivate the development of our two algorithms that we are going to introduce next.

\section{Main results: Interior Point Methods for Testing by Betting} 


Having provided the necessary background in the preceding section, 
we now are ready to present our algorithms.
As outlined in the introduction, our algorithms employ a barrier function to constrain the learner's updates to the interior of the domain $\K=[-1,1]$. A natural barrier function for this purpose is
\begin{equation} \label{barrier}
R(\theta) = -\ln(1-\theta) - \ln (1+\theta),
\end{equation}
where we note that the domain of $R(\cdot)$ is $(-1,1)$.
However, one might wonder: how can this technique overcome the potential loss explosion issue when the learner plays $\theta_t$ such that $|\theta_t| \approx 1$ and still achieve a non-vacuous regret bound?
Recall that we have $\ell_t(\theta_t)= -\ln (1- g_t \theta_t)$, where $g_t \in [-1,1]$. The answer lies in exploiting the properties of self-concordant functions for designing no-regret learning strategies, which we detail next.

\subsection{FTRL+Barrier for Testing by Betting} \label{section:main1}

Our first sequential testing-by-betting algorithm is based on Follow-the-Regularized Leader (FTRL), a classical no-regret learning strategy in online learning \cite{shalev2006online,hazan2010extracting,abernethy2012interior,mcmahan2017survey}. 
The update is depicted in Algorithm~\ref{alg:FTRL}, where 
we use the barrier function in \myeqref{barrier} as the regularization in FTRL.
It is not hard to show that the barrier function is a self-concordant function, and we give its proof in Appendix~\ref{app:main1}.
\begin{algorithm}[h]
\caption{FTRL+Barrier as $\mathrm{OAlg}$ for Testing by Betting}
\label{alg:FTRL}
\begin{algorithmic}[1] 
\State \textbf{Require:} specifying parameter $\eta$.
\State Set the decision space $\K=[-1,1]$.
\State \textbf{For} $t=1,2,\dots, T$ \textbf{do}
\State \quad Play $\theta_t \in \K$.
\State \quad Receive $\ell_t(\theta)= - \ln (1-g_t \theta)$ and incur loss $\ell_t(\theta_t)$.
\State \quad $\theta_{t+1} \gets \underset{\theta \in \reals}{\arg\min} \, \eta \left \langle \sum_{s=1}^t \nabla \ell_t(\theta_t), \theta \right  \rangle + R(\theta)$, where $R(\theta)$ is the barrier function in \myeqref{barrier}.
\State \textbf{End For}
\end{algorithmic}
\end{algorithm}
\begin{lemma} \label{lem:barrier}
The barrier function $R(\theta) = -\ln (1-\theta) - \ln (1+\theta)$ is a self-concordant function for $\K = [-1,1]$, and hence 
$F_t(\theta):=\left \langle \sum_{s=1}^t \nabla \ell_t(\theta_t), \theta \right  \rangle + R(\theta)$ is also a self-concordant function.
\end{lemma}
\iffalse
\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/example1.png}
         \caption{Uniform distributions with disjoint supports.} 
         \label{fig:ex1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/example2.png}
         \caption{Distributions with overlapping supports. High signal-to-noise ratio leads to low overlapping of two distributions.}
         \label{fig:ex2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/example4.png}
         \caption{Distributions with small variances.}
         \label{fig:ex4}
     \end{subfigure}
        \caption{Subfigure (a) and (b) show two examples where the linear growth condition \myeqref{growth} is easily satisfied. Subfigure (c) provides an illustration of a case when the condition \myeqref{eq:op_better_eqt2} can easily hold. }
        \label{figexamples}
\end{figure*}
\fi

Our result will be built upon a prior established regret bound for FTRL with \emph{any} self-concordant barrier as the regularization.
\begin{lemma}[Theorem 4.1 in \citet{abernethy2012interior} 
\footnote{The original presentation of the theorem considers the linear loss setting and assumes $\min_{\theta \in \K} R(\theta)=0$. We modify the theorem for the application in our setting. Its proof is available in Appendix~\ref{app:main1}.}
]\label{lem:FTRL}
Suppose the learner faces a sequence of convex loss functions $\ell_t(\cdot)$ and that the regularization function of FTRL, $R(\cdot)$, is a self-concordant barrier. Set the parameter $\eta$ so that $\eta \| \nabla \ell_t(\theta_t) \|_{\theta_t}^* \leq \frac{1}{4} $. Then, FTRL has $\mathrm{Regret}_T(\theta_*)
\leq
2 \eta \sum_{t=1}^T \| \nabla \ell_t(\theta_t) \|_{\theta_t}^{*2}
+ \frac{  R(\theta_*) - \min_{\theta \in \K} R(\theta)  }{\eta}$,
where $\theta_* \in \mathcal{K}$ is any comparator.
\end{lemma}

To provide intuition on why this result can be effective in tackling the loss (and gradient) explosion issue in our betting game, let us explicitly write out the term $\| \nabla \ell_t(\theta_t) \|_{\theta_t}^{*2}$ in the regret bound of FTRL for the game. Specifically, we have:
\begin{align} \label{eqnorm}
\textstyle
\| \nabla \ell_t(\theta_t) \|_{\theta_t}^{*2}
= \underbrace{ \left(\frac{g_t}{1-g_t \theta_t}\right)^2 }_{ = (\nabla \ell_t(\theta_t))^2 }
\underbrace{ \frac{(1-\theta_t)^2(1+\theta_t)^2}{2+2\theta_t^2} }_{ =  ( \nabla^2 R(\theta_t) )^{-1} }.
\end{align}
From \myeqref{eqnorm}, one can observe a delicate balance when $\theta_t$ approaches the boundary; the gradient becomes large while the inverse of Hessian  $( \nabla^2 R(\theta_t) )^{-1}$ diminishes. This interplay makes the local gradient norm $\| \nabla \ell_t(\theta_t) \|_{\theta_t}^{*2}$ remain small.

We now identify a condition under which FTRL+Barrier achieves a smaller regret bound than ONS, with its implications for sequential hypothesis testing to be elaborated upon shortly.
\begin{lemma} \label{lem:FTRL_const}
Denote $G_t:= \sum_{s=1}^t \nabla \ell(\theta_s) $. 
Suppose that there exists a time point $t_0$ such that for all $t \geq t_0$, we have
\begin{equation} \label{growth}
\textbf{(Linear growth of cumulative gradients)}
\left | G_t   \right| \geq ct,
\end{equation}
for some $c>0$. 
Then, FTRL+Barrier (Algorithm~\ref{alg:FTRL}) satisfies  
$\mathrm{Regret}_T(\theta_*) 
\leq \frac{t_0}{8\eta} + \frac{4}{c'^2\eta} \left( \frac{1}{t_0} - \frac{1}{T-1} \right )
+ \frac{  R(\theta_*)  }{\eta}$,
where $\eta\leq \frac{1}{4}$ is the parameter, $c' > 0$ is a constant, and $\theta_* \in K$ is any comparator.
\end{lemma}

The proof of Lemma~\ref{lem:FTRL_const} is available in Appendix~\ref{app:main1}.
What Lemma~\ref{lem:FTRL_const} shows is that under the condition of 
a linear growth of cumulative gradients $G_t$, Algorithm~\ref{alg:FTRL} can actually
have a \emph{constant} regret $O(t_0)$, modulo the value of the barrier function at the benchmark $\theta_* \in \K$. In other words, once the total number of rounds $T$ is sufficiently large, i.e., $T \geq t_0$, the cumulative regret stays at $O(t_0)$, which is better than $O( \ln (T))$ of ONS (c.f. Lemma~\ref{lem:ONS}).
Hence, Algorithm~\ref{alg:Betting} with FTRL+barrier can have a shorter expected time to reject the null hypothesis $\H_0$ when $\H_1$ is true, as we will demonstrate shortly.  

In the following, we provide a couple of concrete scenarios of sequential hypothesis testing where the linear growth condition of cumulative gradients holds.

\noindent
\textbf{Example 1:}~
\textit{(Distributions with disjoint supports.) Consider $x_t \sim \rho_x$ and $y_t \sim \rho_y$, where $\rho_x$ and $\rho_y$ have disjoint but continuous supports, as illustrated in Figure~\ref{fig:ex1}. Then, the linear growth condition \myeqref{growth} is satisfied for all $t \geq 1$.}
 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/example1.png}
    \caption{Uniform distributions with disjoint supports. 
        }
    \label{fig:ex1}
\end{figure}

\noindent
\textbf{Example 2:}~
\textit{(Distributions with overlapping supports; high signal-to-noise ratio.) Denote $\sigma_x^2$ the variance of samples $\{x_t\}$ and $\sigma_y^2$ the variance of samples $\{y_t\}$. 
Then, with probability at least $1-\delta$, the linear growth condition of cumulative gradients \myeqref{growth} holds for all $t \geq t_0$, where $t_0=\frac{1}{b^2\delta(\mu_x-\mu_y)^2/(\sigma_x^2+\sigma_y^2)}$, with $b\in(0,1)$. Specifically, the constant in \myeqref{growth}, c, is proportional to $(1-b)|\mu_x-\mu_y|$. Based on the expression of $t_0$, if the signal-to-noise ratio (i.e., $\frac{(\mu_x-\mu_y)^2}{\sigma_x^2+\sigma_y^2}$) is high, then the growth condition is easily satisfied for any sufficiently large $t$.
Figure~\ref{fig:ex2} illustrates an example.}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/example2.png}
    \caption{Distributions with overlapping supports. High signal-to-noise ratio leads to less overlap of two distributions.
    }
    \label{fig:ex2}
\end{figure}


\noindent
\textbf{Example 3:}~
\textit{(Time-varying distributions with mean shift.)
Consider the modified hypothesis testing setting \myeqref{setting:distributionshifts}, where $\{x_t\}$ are from a distribution with no mean shift, while $\{y_t\}$ are from a distribution for which we would like to determine whether there exists a shift of its mean at some time point $\hat{t}$. In addition, $\{y_t\}$ has the same mean as that of $\{x_t\}$ at the beginning, i.e., $\mu_x(t)=\mu_y(t< \hat{t})$. Under $H_1$, we have $\mu_y(t\geq \hat{t})=\mu_y(t< \hat{t})+a$. Then, with probability at least $1-\delta$, the condition \myeqref{growth} holds for all $t \geq t_0$, where $t_0=\frac{2 b\hat{t} + \frac{1}{\delta^2S} + \sqrt{\frac{4b\hat{t}}{\delta^2S} + \frac{1}{\delta^2S^2}}}{2 b^2}$, with $S=\frac{a^2}{\sigma_x^2+\sigma_y^2}$, and the positive constant $b\in(0,1)$. The growth rate $c$ involved in \myeqref{growth} is proportional to $(1-b)|a|$.
}



\begin{theorem} \label{thm:main1}
Algorithm~\ref{alg:Betting} with $\mathrm{OAlg}$ being FTRL+Barrier (Algorithm~\ref{alg:FTRL}) is a level-$\alpha$ sequential test with asymptotic power one. Furthermore, denote $\Delta:= | \mu_y - \mu_x |$ and assume that the linear growth condition \myeqref{growth} holds for all $t \geq t_0$ for some $t_0$. Then,
the expected rejection time $\tau$ under $\H_1$ can be bounded as
$\E[\tau] \lesssim  \frac{1}{\Delta^2}\left(t_0 + \ln\frac{1}{\alpha}\right).$
\end{theorem}

The proof of Theorem~\ref{thm:main1} is in Appendix~\ref{app:main1}. 
The guarantee above indicates that when the alternative hypothesis is true, a larger discrepancy between two distributions leads to a faster rejection of the null hypothesis by our algorithm. If condition \myeqref{growth} consistently holds, as illustrated in Example 1, or occurs with high probability as in Example 2 and Example 3, the expected rejection time $\E[\tau]$ is  
$O \left( \frac{1}{\Delta^2}\ln\left(\frac{1}{\alpha}\right) \right)$ for a low $\alpha$ regime, and 
 $O \left( \frac{1}{\Delta^2}\right)$ for a high $\alpha$ regime. In comparison, using the ONS strategy as $\mathrm{OAlg}$ in Algorithm~\ref{alg:Betting}, the reject time bound is $O\left( \frac{1}{\Delta^2}\ln\left(\frac{1}{\Delta^2\alpha}\right) \right)$ \citep{Chugg2023}. 

\subsection{Optimistic-FTRL+Barrier for Testing by Betting} \label{section:main2}

In this subsection, we introduce another new algorithm, which we call Optimistic-FTRL+Barrier (Algorithm~\ref{alg:OptimisticFTRL}). Optimistic online learning concerns the scenario where the learner incorporates a guess of the next gradient $m_t$ to determine the action $\theta_t$ at each round before observing its loss function \citep{chiang2012online, rakhlin2013online, joulani2017modular,wang2018acceleration,chen2024optimistic}. The idea is that if the sequence of loss functions is predictive and the learner's estimation of them is accurate enough, then smaller regret can be achieved. We extend this idea to testing by betting, and the intuition is that if the player can predict their incoming sequence of samples, they should incorporate this information into their bets to enhance their performance in the testing-by-betting game.

\begin{algorithm}[h]
\caption{Optimistic-FTRL+Barrier as $\mathrm{OAlg}$ for Testing by Betting}
\label{alg:OptimisticFTRL}
\begin{algorithmic}[1] 
\State \textbf{Require:} specifying parameter $\eta$.
\State Set the decision space $\K=[-1,1]$.
\State \textbf{For} $t=1,2,\dots, T$ \textbf{do}
\State \quad Play $\theta_t \in \K$.
\State \quad Update the guess of the next gradient $m_{t+1}$.
\State \quad Receive $\ell_t(\theta)= - \ln (1-g_t \theta)$ and incur loss $\ell_t(\theta_t)$.
\State \quad { $\theta_{t+1} \gets \underset{\theta \in \reals}{\arg\min} \, \eta 
\left \langle  m_{t+1} + \sum_{s=1}^t \nabla \ell_t(\theta_t), \theta \right  \rangle + 
 R(\theta),$ }where $R(\theta)$ is the barrier function in \myeqref{barrier}.
\State \textbf{End For}
\end{algorithmic}
\end{algorithm}

We now provide the regret bound of Optimistic-FTRL+Barrier, and its proof is deferred to Appendix~\ref{app:main2}. 
\begin{lemma}\label{lem:OptimisticFTRL}
Following the same assumptions and conditions in Lemma~\ref{lem:FTRL},
Optimistic-FTRL+Barrier (Algorithm~\ref{alg:OptimisticFTRL}) has
$\mathrm{Regret}_T(\theta_*)
\leq
2\eta\sum_{t=1}^T\|\nabla \ell_t (\theta_t)-m_t(\theta_t)\|_{\theta_t}^* \|\nabla\ell_t (\theta_t)\|_{\theta_t}^*+ \frac{R( \theta_*) - \min_{\theta \in \K} R(\theta)}{\eta} ,$
where $\theta_* \in \mathcal{K}$ is any comparator.
\end{lemma}

Comparing the regret bound of Optimistic-FTRL+Barrier and that of FTRL+Barrier in Lemma~\ref{lem:FTRL}, it is evident that if the guess $m_t$ is close to the next gradient $\nabla \ell_t(\theta_t)$, then the \emph{optimistic} version has a smaller regret, thereby speeding up the process of rejecting the null when the alternative holds. 
On the other hand, even if the guess $m_{t}$ is a poor estimate of the next gradient, the regret bound remains the same order as its non-optimistic counterpart and is only worse by a constant factor, provided that $m_t$ is also bounded \cite{orabona2019modern,rakhlin2013online,chiang2012online}.

One of the possible choices of $m_{t+1}$ for $t+1$ is the latest gradient $\nabla \ell_{t}(\theta_{t})$ at $t$, i.e., set $m_{t+1} \gets \nabla \ell_{t}(\theta_{t})$. 
Then, the regret bounds above suggest that when the difference between consecutive gradients is small, i.e., 
\begin{equation}
\| \nabla \ell_{t}(\theta_{t}) - \nabla \ell_{t-1}(\theta_{t}) \|_{\theta_t}^{*} < \| \nabla \ell_{t}(\theta_{t}) \|_{\theta_t}^{*}\label{eq:op_better_vs_ft}, 
\end{equation}
being ``optimistic'' can have a real advantage. Below, we provide a concrete scenario in hypothesis testing where such a situation is likely to hold.

\noindent
\textbf{Example 4:}~
\textit{(IID Samples from distributions with small variances.) 
Consider $x_t$ and $y_t$ are respectively drawn from two uniform distributions with disjoint supports, i.e., $x_t\sim\rho_x$ and $y_t\sim\rho_y$, where $\rho_x$ and $\rho_y$ have small variances. Then, the inequality \myeqref{eq:op_better_vs_ft} becomes 
\begin{align}
  \left|\frac{g_t}{1-g_t\theta_t}-\frac{g_{t-1}}{1-g_{t-1}\theta_t} \right | &< \left |\frac{g_{t}}{1-g_{t}\theta_t} \right| \label{eq:op_better_eqt2},
\end{align}
since $\nabla \ell_t(\theta_t)=\frac{g_t}{1-g_t\theta_t}$. 
Suppose the observations at two consecutive time points are close, i.e., $g_t\approx g_{t-1}$. Then, the condition \myeqref{eq:op_better_eqt2} could be approximated as
\begin{equation*}
    |{g_t}-{g_{t-1}}| \ll |{g_{t}}|\Leftrightarrow |(x_t-y_t)-(x_{t-1}-y_{t-1})| \ll |x_t-y_t|, \label{eq:op_better_eqt3}
\end{equation*}
which is easily satisfied considering the distance between the means of two distributions is relatively larger than their variances, as illustrated in Figure~\ref{fig:ex4}.}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{figs/example4.png}
    \caption{IID Samples from distributions with small variances.}
    \label{fig:ex4}
\end{figure}


\begin{theorem} \label{thm:main2}
Algorithm~\ref{alg:Betting} with $\mathrm{OAlg}$ being Optimistic-FTRL+Barrier (Algorithm~\ref{alg:OptimisticFTRL}) is a level-$\alpha$ sequential test with asymptotic power one. Furthermore, denote $\Delta:= | \mu_y - \mu_x |$ 
and assume that the linear growth condition \myeqref{growth} holds at all $t\geq t_0$ for some $t_0$. 
The expected rejection time $\tau$ against the null $\H_0$ under $\H_1$ satisfies: $\E[\tau] \lesssim  \frac{1}{\Delta^2}\left(\zeta t_0+\ln\frac{1}{\alpha}\right)$, where $\zeta:=\max\limits_{t} \frac{\left|\nabla \ell_t(\theta_t)-\nabla\ell_{t-1}(\theta_t)\right|}{\left|\nabla \ell_t(\theta_t)\right|}$.
\end{theorem}
The proof of Theorem \ref{thm:main2} is in Appendix \ref{app:main2}. In comparison to OAlg being FTRL+Barrier, for Optimistic-FTRL+Barrier, the bound of $\mathbb{E}(\tau)$ includes a coefficient $\zeta$ before the constant term $t_0$. When the difference between $g_t$ at two consecutive time points is significantly smaller than its current value, i.e., $|g_t - g_{t-1}| \ll |g_t|$, $\zeta$ is much less than 1. 

\section{Experiments}

In the following experiments, we evaluate the performance of ``testing by betting" algorithm (Algorithm \ref{alg:Betting}) using our OAlg methods: FTRL+Barrier and Optimistic-FTRL+Barrier, compared to when using ONS. For the Optimistic-FTRL+Barrier method, we set $m_{t+1}$ in Algorithm \ref{alg:OptimisticFTRL} to the latest gradient $\nabla \ell_{t}(\theta_{t})$ at time step $t$, i.e., set $m_{t+1} \gets \nabla \ell_{t}(\theta_{t})$. Furthermore, we consider $20$ values of significance level $\alpha$, evenly spaced from $0.005$ to $0.1$. The tests under both $H_0$ and $H_1$ scenarios are repeated, with the order of samples in each sequence shuffled for every run, to compute the average results under each $\alpha$. The parameter $\eta$ in Algorithm \ref{alg:FTRL} and Algorithm \ref{alg:OptimisticFTRL} is set to $1$ in all tests.

\subsection{Synthetic Experiments}
In this part, we evaluate the performance of our online algorithms under three synthetic situations involved in the examples discussed earlier. The detailed setup are provided in Appendix \ref{app:add_exp_syn} due to the space limitation.

\textbf{Distributions with disjoint supports.} 
Consider one observes two sequences of samples drawn from two uniform distributions with small variances. Specifically, sequence $\{y_t\}$ originates from the same distribution as $\{x_t\}$ when the null hypothesis holds. Under the alternative hypothesis, $y_t$ are sampled from a distribution with different mean value than that of $x_t$.
Figure \ref{fig:syn1} demonstrates that under $H_1$, because of the satisfaction of the linear growth condition of cumulative gradients, our methods exhibits a faster rejection time under $H_1$ compared to ONS. Furthermore, Optimistic-FTRL+Barrier performs better than FTRL+Barrier because the small variances of distributions lead to a small $\zeta$ value. Hence, the shorter rejection time when using 
Optimistic-FTRL+Barrier supports the result of Theorem \ref{thm:main2}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/syn1.png}
    \caption{Comparisons of three different OAlg methods under $H_0$ and $H_1$ scenarios. The left plot shows the average time for Algorithm \ref{alg:Betting} to correctly reject $H_0$ versus the false positive rates (FPRs) under each value of the significance level ($\alpha$) over 300 runs. The plots closer to the bottom left are more desirable. The right plot shows FPRs under each $\alpha$ when the null $H_0$ holds, with the dashed line and shaded area representing the desired significance levels.}
    \label{fig:syn1}
\end{figure}


\textbf{Distributions with overlapping supports; high signal-to-noise ratio.} 
We also conduct experiments where the distributions have overlapping supports, and empirically found that Algorithm~\ref{alg:Betting} with FTRL+Barrier (Algorithm~\ref{alg:FTRL}) and Optimistic-FTRL+Barrier (Algorithm~\ref{alg:OptimisticFTRL}) can outperform ONS. Please refer to Appendix~\ref{app:add_exp_syn} for the encouraging results. 

\textbf{Time-varying distributions with mean shift.} 
To simulate the model shift in real-world scenarios, we let the distribution of $\{y_t\}$ undergoes a mean shift under $H_1$ at the time point $ \hat{t} = 300 $.
As illustrated in Figures \ref{fig:syn3_2} and \ref{fig:syn3_1}, all three betting methods can effectively adapt to the distribution shift. However, FTRL+Barrier and Optimistic-FTRL+Barrier outperform ONS by demonstrating a more rapid wealth accumulation, which leads to faster null hypothesis rejection under $H_1$. Figure \ref{fig:syn3_2} shows that before the distribution shift, there is no significant increase in accumulated wealth for all methods. After approximately 100 time steps following the shift, there is a surge in their wealth accumulation. However, the wealth $W_t$ for ONS does not reach the thresholds $1/\alpha$ for small significance levels before the time budget (500), this in turn results in a short vertical line at $t=500$ in Figure \ref{fig:syn3_1}, which means that $H_0$ is not rejected before the time budget. In contrast, both FTRL+Barrier and Optimistic-FTRL+Barrier achieve all values of specified $1/\alpha$ in fewer steps due to their more rapid wealth increase.
\begin{figure}[h]
\centering
    \hspace*{-0.3cm}
    \includegraphics[width=0.478\textwidth]{figs/syn3_2.png}
    \caption{Average wealth accumulation process over 300 runs for different OAlg methods adapting to distribution shift.  
    }
    \label{fig:syn3_2}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/syn3_1.png}
    \caption{Comparisons of three different OAlg methods under $H_0$ and $H_1$ scenarios. The left subfigure shows the average time over 300 runs for three OAlgs to reject $H_0$ after $y_t$ undergoing a distribution shift at $300$-th time step versus the false positive rates (FPRs) under each value of the significance level ($\alpha$). }
    \label{fig:syn3_1}
\end{figure}

\subsection{Real Data Experiments}
\textbf{Detecting LLM-generated texts.} 
In this task, we observe a stream of texts that comes from an unknown source and aim to determine whether these texts written by a human ($H_0$) or generated by an LLM ($H_1$). We reject $H_0$ and declare the unknown source is an LLM if the accumulated wealth exceeds the specified threshold. 
The experiment is conducted using a publicly available dataset from \citet{chen2024online}, which contains both machine-generated and human-written texts. More details about the setup can be found in Appendix~\ref{app:add_exp_detect}.


\begin{figure}[h]
\centering
    \hspace*{-0.02cm}
    \includegraphics[width=0.48\textwidth]{figs/selected_2x1_comparison.png}
    \caption{The left subfigure shows the score distributions for texts from three LLMs (Gemini-1.5-Flash, Gemini-1.5-Pro, and PaLM 2) compared to human-written texts. The right subfigure focuses on Human and Gemini-1.5-Pro, with distributions for the other LLMs provided in Appendix \ref{app:add_exp_detect}.}
    \label{fig:avg_frequency}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/avg.neo2.7.eta1_300.png}
    \caption{Average results of detecting LLM-generated and human-written texts. Here, three LLMs (Gemini-1.5-Flash, Gemini-1.5-Pro and PaLM 2) are used to generate fake news. The left subfigure shows the average time to correctly declare an LLM versus the average FPRs over $300$ runs under each $\alpha$ across three models.} 
    \label{fig:avg_detect}
\end{figure}
As shown in Figure \ref{fig:avg_detect}, Algorithm \ref{alg:Betting} equipped with FTRL+Barrier and Optimistic-FTRL+Barrier strategies can detect LLM-generated texts more quickly than ONS on average, even when the score distributions of human-written texts and machine-generated texts overlap substantially, see Figure \ref{fig:avg_frequency}.  All methods consistently has an FPR smaller than each value of the specified significance level $\alpha$. More details are available in Appendix \ref{app:add_exp_detect}.

\textbf{Evaluating facial expression classifiers.} 
For training a classifier, we can use sequential hypothesis testing to evaluate the performance of the underlying model to help training. Specifically, 
we follow the setup of \citet{podkopaev2024sequential}, where the null hypothesis is that the model's predictions are indistinguishable from random guessing (i.e., the feature distributions across different labels are equivalent from the model's perspective). 
During the iterations, samples from different categories sequentially arrives, and the learner bets on the consistency between the model’s prediction and the true label for each sample. If the accumulated wealth $W_t$ exceeds a threshold, then the null $H_0$ is rejected, and the training of the classifier is stopped, as rejecting $H_0$ means that the classifier does non-trivial classification than random guessing. On the other hand, the model continues to update when there is no evidence to reject the null. 

Following the approach in \citet{podkopaev2023sequential} of sequential classification-based two-sample test (Seq-C-2ST), we consider a small CNN trained on Karolinska Directed Emotional Faces dataset (KDEF) ~\citep{lundqvist1998karolinska}. More details are provided in Appendix~\ref{app:add_class}. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/eva_classifier.png}
    \caption{Average results of evaluating the facial expression classifier. The left plot shows the average time required for models to make non-trivial classifications beyond random guessing versus the average FPRs over 200 runs for each $\alpha$. Our methods Pareto-dominates the (rejection time, FPR) values achievable by ONS.}
    \label{fig:eva_class}
\end{figure}
Figure~\ref{fig:eva_class} shows that using the approach Seq-C-2ST \citep{podkopaev2023sequential} equipped with FTRL+Barrier and Optimistic-FTRL+Barrier strategies, the trained classifier can achieve the expected performance by using less number of training samples than that with ONS.


\section{Conclusion}
Motivated by interior-point methods, we propose two novel update strategies for sequential hypothesis testing by betting, which allow betting on a larger decision space compared to the ONS while avoiding loss explosion. Our theoretical analysis and experiments justify the value of our methods, which 1) remain an anytime-valid level-$\alpha$ sequential hypothesis test with asymptotic power of one, 2) are proven to correctly reject $H_0$ faster than ONS under a reasonable condition, and 3) demonstrate comparable or better performance in detecting LLM-generated texts and evaluating classifiers.

\bibliographystyle{unsrtnat}
\bibliography{references}

\newpage
\appendix

\section{More Related Works}
\label{app:add_works}
The notion of testing by betting can be traced back to \citet{cover1974universal} 
and \citet{robbins1974expected}, as wells as \citet{kelly1956new}, who shows that in a repeated game with a binary outcome and certain odds, a gambler can grow their fortune exponentially fast. However, many algorithmic and theoretical foundations have only developed over the recent years, and we refer the reader to a nice monograph by \citet{ramdas2024hypothesis} for the exposition. We note that the wealth in testing by betting  described above is an e-value in the modern statistics literature, see e.g., \citet{vovk2021values,ramdas2024hypothesis,grunwald2024authors,wasserman2020universal,ramdas2020admissible}. 
The idea of betting has also facilitated the design and analysis of algorithms in other domains such as portfolio selection \cite{orabona2023tight}, the construction of confidence sequences \cite{jang2023tighter}, and the design of parameter-free optimization algorithms \cite{orabona2016coin}. We refer the reader to the references therein for more details.

\section{Proof of Theorem~\ref{thm:1}} \label{app:thm:1}
\noindent
\textbf{Theorem~\ref{thm:1}}
\textit{
Algorithm~\ref{alg:Betting} with $\mathrm{OAlg}$ being a no-regret learning algorithm is a level-$\alpha$ sequential test with asymptotic power one. }

\begin{proof}
    
We recall that 
\begin{equation}
    \ln(W_T) = \ln(W_T(u)) - \mathrm{Regret}_T(u) \label{eq:second}
\end{equation}
where $\mathrm{Regret}_T(u)$ is the regret of our method (OAlg) for choosing $\theta_t$. We can use (\ref{eq:second}) to obtain the lower bound for $W_T$,
\begin{align}
    W_T \geq {W_T(u)}\cdot{\exp(-\mathrm{Regret}_T(u))} \text{ for all } |u| \leq 1 \label{eq:wt}
\end{align}

Consider
\begin{align*}
    u = C\cdot\frac{-\sum_{t=1}^T g_t}{\left(\sum_{t=1}^T g_t^2+\left\lvert\sum_{t=1}^T g_t\right\rvert\right)} \in [-1, 1]
\end{align*}
where $C \in [-1, 1]$.

Define:
\begin{equation}
    g_i := x_i^a - x_i^b, \quad S_t := \sum_{i=1}^t g_i, \quad Q_t := \sum_{i=1}^t g_i^2.
\end{equation}
\begin{align*}
    \ln(W_T(u))&= \sum_{t=1}^T \ln(1 - g_tu)\\
    &\geq -\sum_{t=1}^T g_t u - \sum_{t=1}^T (-g_t u)^2\\
    &= -\sum_{t=1}^T g_t \cdot u - \sum_{t=1}^T g_t^2 \cdot(u) ^2\\
     &= -S_T \cdot C\frac{-S_T}{Q_T+\lvert S_T\rvert} - Q_T \cdot\left(C\frac{-S_T}{Q_T+\lvert S_T\rvert} \right)^2\\
    &=C \frac{S_T^2}{Q_T+\lvert S_T\rvert} - C^2\frac{Q
    _T}{Q_T+\lvert S_T\rvert}\cdot\frac{S_T^2}{Q_T+\lvert S_T\rvert}\\
    &\geq C\cdot\frac{S_T^2}{Q_T+\lvert S_T\rvert} - C^2\cdot\frac{S_T^2}{Q_T+\lvert S_T\rvert}\quad(\text{since } \frac{Q
    _T}{Q_T+\lvert S_T\rvert}\leq 1)\\
    &=(C-C^2)\cdot\frac{S_T^2}{Q_T+\lvert S_T\rvert}.
\end{align*}
As for $C\in [-1,1]$, the maximum value of $C-C^2$ will be achieved at $C=\frac{1}{2}$. Hence, the corresponding wealth bound, with benchmark $u =\frac{-\sum_{t=1}^T g_t}{2\left(\sum_{t=1}^T g_t^2+\left\lvert\sum_{t=1}^T g_t\right\rvert\right)}$, is 
\begin{align*}
    \ln(W_T(u))\geq\frac{S_T^2}{4\left(Q_T+\lvert S_T\rvert\right)}=\frac{\left(\sum_{t=1}^T g_t\right)^2}{4\left(\sum_{t=1}^T g_t^2+\left\lvert\sum_{t=1}^T g_t\right\rvert\right)}.
\end{align*}


According to (\ref{eq:wt}), we have 
\begin{align}
    W_T \geq{\exp\left(\frac{S_T^2}{4\left(Q_T+\lvert S_T\rvert\right)}-\mathrm{Regret}_T(u)\right)} \text{ for all } |u| \leq 1.\label{eq:gen_bound_wealth}
\end{align}

We will prove this theorem by the following two parts.


\textbf{1. Level-$\alpha$ Sequential Test.}

Since $\{W_t \geq 1/\alpha \text{ or } W_T > \nu/\alpha\}$ is treated as reject "$H_0$". The level-$\alpha$ sequential test means that, when $H_0$ holds:
\begin{equation}
    \sup_{P \in H_0} P(\exists t \geq 1 : W_t \geq 1/\alpha \text{ or } W_T \geq \nu/\alpha) \leq \alpha, \quad \text{or equivalently} \quad \sup_{P \in H_0} P(\tau < \infty) \leq \alpha.
\end{equation}
Previously, we have defined the minimum rejection time as $\tau=\arg\inf_t\{ W_t \geq 1/\alpha \text{ or } W_T \geq \nu/\alpha\}$, where $\nu\sim \text{Unif}(0,1)$.
\begin{proof}
 When $P\in H_0$, i.e., $\mu_x = \mu_y$, it is true that
 \begin{align}
     \mathbb{E}_P[{x}_t - {y}_t] = \mu_x - \mu_y = 0.
     \label{eq:H0}
 \end{align}
 
Wealth process is calculated as $W_t=(1 - g_t\theta_t)\times W_{t-1}$, and the initial wealth $W_0=1$. Then,
 \begin{align*}
     W_t=(1 - g_t\theta_t)\times W_{t-1}= \prod_{s=1}^t (1 - g_i\theta_s)\times W_0= \prod_{s=1}^t (1 - g_s\theta_s),
 \end{align*}
 where $g_s={x}_s - {y}_s$. Since $\theta_t$ is $\mathcal{F}_{t-1}$-measurable and according to (\ref{eq:H0}), we have

\begin{equation}
    \mathbb{E}_P[W_t | \mathcal{F}_{t-1}] = \mathbb{E}_P \left[ (1 - g_t\theta_t)\times W_{t-1} \Bigg| \mathcal{F}_{t-1} \right] = W_{t-1} (1 - \theta_t\cdot\mathbb{E}_P[{x}_t - {y}_t]) = W_{t-1},
\end{equation}

hence, $(W_t)_{t\geq 1}$ is a $P$-martingale with $W_0=1$. Since $g_s\in[-1,1]$ and $\theta_s\in[-1,1]$, we have $g_s\theta_s\in[-1,1]$ for all $t$, then $W_t=\prod_{s=1}^t (1-g_s\theta_s)$ remains non-negative for all $t$. Thus, we can apply Ville's inequality~\citep{Ville1939} to estabilish that $P(\exists t \geq 1 : W_t \geq 1/\alpha) \leq \alpha$. This inequality shows that the sequential test: ``reject $H_0$ once the wealth $W_t$ reaches $1/\alpha$" maintains a level-$\alpha$ test. If there exists a time budget $T$, we will additionally check the wealth at the final step $T$ of the algorithm: if $W_T\geq \nu/\alpha$  holds, the null hypothesis is rejected. This is validated by the randomized Ville's inequality of \citet{RamdasManole2023}, which is
\begin{equation*}
    P(\exists t \geq 1 : W_t \geq 1/\alpha \text{ or } W_T \geq \nu/\alpha) \leq \alpha,
\end{equation*}
where $\nu\sim \text{Unif}(0,1)$.

\end{proof}

\textbf{2. Asymptotic power one.} 
Test $\phi$ with asymptotic power one, i.e., $\beta=1$, means that under $H_1$ ($\mu_x \neq \mu_y$), the algorithm ensures that wealth $W_t$ could exceed  $ 1/\alpha$ in finite time $t$ to reject $H_0$, that is:
\begin{equation}
    \sup_{P \in H_1} P(\tau = \infty) \leq 1-\beta = 0.
\end{equation}
Previously, we get the following guarantee on the wealth $W_t$, see (\ref{eq:gen_bound_wealth}):
\begin{align}
    W_T \geq{\exp\left(\frac{S_T^2}{4\left(Q_T+\lvert S_T\rvert\right)}-\mathrm{Regret}_T(u)\right)} \text{ for all } |u| \leq 1.
\end{align}

According to the definitions: $S_t= \sum_{s=1}^t g_s$, $Q_t= \sum_{s=1}^t g_s^2$. Thus, we can derive:
\begin{align}
    W_T \geq {\exp\left(\frac{S_T^2}{4\left(Q_T+\lvert S_T\rvert\right)}-\mathrm{Regret}_T(u)\right)} \geq \exp \left( \frac{S_T^2}{8T}-\mathrm{Regret}_T(u)  \right), \quad \forall T \geq 1\label{eq:general_wealth_bound}
\end{align}
We know that $ \{\tau = \infty\} \subseteq \{ \tau \geq T \}$ for all $ T \geq 1$. By definition of the rejection time, we have $\{\tau >T\} \subseteq \{W_T<\frac{1}{\alpha}\}$, which means that if the rejection of the null hypothesis occurs later than $T$, then the accumulated wealth at $T$ must be smaller than the threshold $1/\alpha$, i.e., $W_T<1/\alpha$. Thus, we get $P(\tau = \infty) \leq \liminf_{T \to \infty} P(\tau > T) \leq \liminf_{T \to \infty} P(W_T < 1/\alpha)$. By the inequality (\ref{eq:general_wealth_bound}),
\begin{align*}
    P(W_T < 1/\alpha) &\leq P \left(\exp \left( \frac{S_T^2}{8T}-\mathrm{Regret}_T(u) \right)< \frac{1}{\alpha}  , \quad \forall T \geq 1 \right)\\
    &\leq P\left( -\sqrt{\frac{\mathrm{8Regret}_T(u)}{T}+\frac{8}{T}\ln \frac{1}{\alpha} } <\frac{S_T}{T} < \sqrt{\frac{\mathrm{8Regret}_T(u)}{T}+\frac{8}{T}\ln \frac{1}{\alpha} }\right).
\end{align*}

According to the Strong Law of Large Numbers, $ S_T/T=\frac{1}{T}\sum_{t=1}^T({x}_t-{y}_t) $ converges to $\mu_x - \mu_y$ as $T\rightarrow \infty$ is almost surely. We recall that under $H_1:\mu_x \neq \mu_y$. For no-regret learning algorithm, we have $\frac{\mathrm{Regret}_T(\theta_*)}{T} \to 0 \text{ as } T \to \infty$ \citep{orabona2019modern,wang2024no}. Thus, $\frac{\mathrm{8Regret}_T(u)}{T}+\frac{8}{T}\ln \frac{1}{\alpha} \to 0 $ as $T\rightarrow \infty$. Thus, if we let $D_T $ be the event that  $\exp \left( \frac{S_T^2}{8T}-\mathrm{Regret}_T(u) \right)< \frac{1}{\alpha}$, we find that  $1_{D_T} \to 0$ almost surely. By the dominated convergence theorem,
\begin{equation}
    P(\tau = \infty)\leq\liminf_{t \to \infty} P(W_T < 1/\alpha) \leq \liminf_{t \to \infty} P(D_T) = \liminf_{T \to \infty} \int 1_{D_T} \, dP = 0.
\end{equation}

\iffalse

\textbf{3. Expected rejection time.}

Previously, we get the following guarantee on the wealth $W_t$, see (\ref{eq:gen_bound_wealth}):
\begin{align}
    W_T \geq{\exp\left(\frac{S_T^2}{4\left(Q_T+\lvert S_T\rvert\right)}-\mathrm{Regret}_T(u)\right)} \text{ for all } |u| \leq 1\notag.
\end{align}

If there is no constraint on time budget $t$, under the assumption that $H_1$ is true, we have
\begin{equation}
    \E[\tau] = \sum_{t=1}^{\infty} P(\tau > t) \leq \sum_{t=1}^{\infty} P\left(\ln(W_t) < \ln(1/\alpha)\right),
\end{equation}
where $\left\{\ln(W_t) < \ln(1/\alpha)\right\}$ is defined as $E_t$. Hence, we have
\begin{align}
    E_t &\subseteq \left\{\ln\left(\exp\left(\frac{S_t^2}{4\left(Q_t+\lvert S_t\rvert\right)}-\mathrm{Regret}_t(u)\right)\right) < \ln(1/\alpha) \right\}\notag\\
    \Rightarrow E_t &\subseteq \left\{{S_t^2} < 4\left(Q_t + |S_t|\right)\left(\ln\left(\frac{1}{\alpha}\right) + \ln\left(\exp\left(\mathrm{Regret}_t(u)\right)\right)\right)\right\} \quad\left(\text{Since }\left\lvert S_t\right\rvert=\left\lvert\sum_{s=1}^t g_s\right\rvert\leq \sum_{s=1}^t \left\lvert g_s\right\rvert\right)\notag\\
    &\subseteq \left\{{S_t^2} < 4\left(Q_t + \sum_{s=1}^t |g_s|\right)\left(\ln\left(\frac{1}{\alpha}\right) + \ln\left(\exp\left(\mathrm{Regret}_t(u)\right)\right)\right)\right\} .
    \label{eq:Et_bound}
\end{align}

We denote $V_t:=\sum_{s=1}^t \lvert g_i\rvert$ and then we can get the upper bound on $V_t$ and $Q_t$ respectively. Since $\lvert g_s \rvert$ for any $s$ are random variables in $[0,1]$, then we consider the Chernoff bound~\citep{harvey2023second}, 
\begin{equation}
    P\left(V_t > (1 + \delta)\cdot \E\left[V_t\right]\right) \leq \exp\left(-\frac{\delta^2}{3} \E\left[{V_t}\right]\right).
\end{equation}

We let the right-hand side equal to $1/t^2$ and thus $\delta$ is $\sqrt{6 \ln(t) / \E[V_t]}$. By definition, $|g_s| \leq 1\Rightarrow V_t = \sum_{s=1}^t |g_s|\leq t$. With a probability of at least $(1 - 1/t^2)$, we have
\begin{equation}
    {V_t} \leq \E\left[{V_t}\right] + \sqrt{6\E\left[{V_t}\right]\cdot\ln(t)} \leq t + \sqrt{6t\cdot\ln(t)} \leq 2t, \quad \forall t\geq 17. \label{eq:ten}
\end{equation}

Similarly, as for $Q_t=\sum_{s=1}^t g_s^2$, we know $g_s^2 \leq 1$. Then $Q_t$ is the sum of independent random variables in $[0,1]$. After applying the Chernoff bound~\citep{harvey2023second}, we have that with a probability of at least $1 - 1/t^2$,
\begin{equation}
    {Q_t} \leq \E\left[{Q_t}\right] + \sqrt{6\E\left[{Q_t}\right]\cdot\ln(t)} \leq t + \sqrt{6t\cdot \ln(t)} \leq 2t, \quad \forall t \geq 17. \label{eq:eleven}
\end{equation}
Let $H_t = \left\{{Q_t}  \leq 2t\right\} \cap \left\{{V_t}\leq 2t\right\}$. Then, by (\ref{eq:Et_bound}),
\begin{align}
    E_t \cap H_t &\subseteq \left\{{S_t^2} < 4\cdot\left(2t + 2t\right)\left(\ln\left(\frac{1}{\alpha}\right) + \ln\left(\exp\left(\mathrm{Regret}_t(u)\right)\right)\right)\right\} \notag\\
    &\subseteq \left\{\left\lvert{S_t}\right\rvert < \underbrace{ \sqrt{{16t}\cdot\left(\ln\left(\frac{1}{\alpha}\right) + \ln\left(\exp\left(\mathrm{Regret}_t(u)\right)\right)\right)}}_{:=R_t}\right\}.
    \label{eq:nine}
\end{align}

Since $S_t=\sum_{s=1}^t g_s$ is the sum of independent random variables in $[-1, 1]$, applying a Hoeffding bound~\citep{harvey2023second} gives
\begin{equation}
    P\left(\left\lvert{S_t} - \E\left[{S_t}\right]\right\rvert \geq u\right) \leq 2 \exp\left(\frac{-u^2} {2t}\right).
\end{equation}

We still let RHS be $1/t^2$ to get $u=\sqrt{2t\cdot\ln(2t^2)}$. With a probability of at least $(1 - 1/t^2)$ and according to the reverse triangle inequality, we have
\begin{equation}
    \left\lvert\left\lvert {S_t}\right\rvert - \left\lvert \E\left[{S_t}\right]\right\rvert\right\rvert \leq \left\lvert{S_t} - \E\left[{S_t}\right]\right\rvert \leq \sqrt{2t\cdot\ln(2t^2)}.
\end{equation}

This implies that,
\begin{align}
    \left\lvert{S_t}\right\rvert \geq \left\lvert \E\left[{S_t}\right]\right\rvert - \sqrt{2t\cdot\ln (2t^2)} = {t\Delta} - \sqrt{2t\cdot\ln(2t^2)}\geq {t\Delta} - \sqrt{4t\cdot\ln (2t)},
    \label{eq:inequa}
\end{align}
where $\Delta=\lvert\mu_x-\mu_y\rvert$. The above inequality (\ref{eq:inequa}) is given by the fact that 
\begin{align}
    \left\lvert \E\left[{S_t}\right]\right\rvert={\left\lvert \E\left[\sum_{s=1}^t g_s\right]\right\rvert}\notag={ \left\lvert\sum_{s=1}^t \E\left[{x}_t-{y}_t\right]\right\rvert}\notag={\left\lvert t(\mu_x-\mu_y)\right\rvert}\notag={ t\left\lvert \mu_x-\mu_y\right\rvert}\notag.
\end{align}

In the following, we show
${t\Delta} - \sqrt{4t\cdot\ln (2t)}\geq R_t$ for all $t \geq t_*$
%, where
%$$t_*:= \frac{32}{\Delta^2}\ln\left(\frac{1}{\alpha}\exp\left(\mathrm{Regret}_t(u)\right)\right),$$ 
where $R_t$ is defined in $(\ref{eq:nine})$.

We have
\begin{align}
    {t\Delta} - \sqrt{4t\cdot\ln(2t)}&\geq \sqrt{{16t}\cdot\left(\ln\left(\frac{1}{\alpha}\right) + \ln\left(\exp\left(\mathrm{Regret}_t(u)\right)\right)\right)}\notag\\
    {t\Delta}&\geq \sqrt{4t\cdot\ln(2t)} +\sqrt{{16t}\cdot\left(\ln\left(\frac{1}{\alpha}\right) + \ln\left(\exp\left(\mathrm{Regret}_t(u)\right)\right)\right)}.
    \label{eq:hard}
\end{align}
A sufficient condition of the above is
\begin{equation}
    {t\Delta} \geq 2 \sqrt{ 16 t  \max \left \{  \ln (2t), \ln \left( \frac{\exp\left(\mathrm{Regret}_t(u)\right) }{ \alpha} \right )    \right \} }.
\end{equation}

%If ${16t}\cdot\left(\ln\left(\frac{1}{\alpha}\right) + \ln\left(\exp\left(\mathrm{Regret}_T(u)\right)\right)\right)\geq 4t\ln(2t) \Rightarrow t\leq\frac{\left(\exp\left(\mathrm{Regret}_T(u)\right)\right)^4}{2\alpha^4}$, then (\ref{eq:hard}) can always hold if (\ref{eq:22}) holds:
%\begin{align}
%    {t\Delta} &\geq \sqrt{{32t}\cdot\left(\ln\left(\frac{1}{\alpha}\right) + \ln\left(\exp\left(\mathrm{Regret}_T(u)\right)\right)\right)}\notag\\
 %   t &\geq \frac{32}{\Delta^2}\ln\left(\frac{1}{\alpha}\exp\left(\mathrm{Regret}_T(u)\right)\right) .
 %   \label{eq:22}%
%end{align}

Hence, when $t\geq t_*$, we have the guarantee ${t\Delta} - \sqrt{4t\cdot\ln (2t)}\geq R_t$ with probability at least $1-1/t^2$.  Specifically, 
we can write
\begin{equation}
    \left\lvert{S_t}\right\rvert \geq R_t,\qquad \text{ if } 
    %t \geq \frac{32}{\Delta^2}\ln\left(\frac{1}{\alpha}\exp\left(\mathrm{Regret}_t(u)\right)\right).
    \label{eq:twelve}
\end{equation}

Now, by the law of total probability, for $t$ large enough such that inequalities (\ref{eq:ten}), (\ref{eq:eleven}), and (\ref{eq:twelve}) all hold:
\begin{align*}
    P(E_t) &= P(E_t \cap H_t) + P(E_t \cap H_t^c)\\
    &=P(E_t \cap H_t) + P(E_t| H_t^c)P(H_t^c)\\
    &\leq P\left(\left\lvert{S_t}\right\rvert < R\right) + P(H_t^c)  \quad (\text{by }(\ref{eq:nine}))\\
    &= \left(1-P\left(\left\lvert{S_t}\right\rvert \geq R\right)\right) + \left(1-P(H_t)\right)\\
    &= \left(1-P\left(\left\lvert{S_t}\right\rvert \geq R\right)\right) + P\left(\left\{{Q_t}>2t\right\}\cup\left\{{V_t}>2t\right\}\right) \qquad \quad  (\text{by definition of }H_t)\\
    &\leq \left(1-P\left(\left\lvert{S_t}\right\rvert \geq R\right)\right) + P\left({Q_t}>2t\right)+P\left({V_t}>2t\right)\\
    &\leq\frac{1}{t^2}+\frac{1}{t^2}+\frac{1}{t^2} \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad  \qquad \qquad \qquad (\text{by }\ref{eq:ten}, \ref{eq:eleven}, \ref{eq:twelve}) \\
    &\leq \frac{3}{t^2}.
\end{align*}

Now we can conclude that when $t$ is large enough such that $t\geq T:=\frac{32}{\Delta^2}\ln\left(\frac{1}{\alpha}\exp\left(\mathrm{Regret}_t(u)\right)\right)$,
%\end{equation}
%we have
\begin{equation}
    \E[\tau] \leq \sum_{t=1}^{\infty} P(E_t)=T+ \sum_{t\geq T}P(E_t)\leq T + \sum_{t=T}^{\infty} \frac{3}{t^2} \leq T + \frac{\pi^2}{2}.
\end{equation}

This completes the argument of this theorem.

\fi

\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proof of lemmas and theorems in Subsection~\ref{section:main1}} \label{app:main1}

\noindent
\textbf{Lemma~\ref{lem:barrier}}
\textit{
The barrier function $R(\theta) = -\ln (1-\theta) - \ln (1+\theta)$ is a self-concordant function for $\K = [-1,1]$, and hence 
$F_t(\theta):=\left \langle \sum_{s=1}^t \nabla \ell_t(\theta_t), \theta \right  \rangle + R(\theta)$ is also a self-concordant function.
}

\begin{proof}
Since $R(\theta):= \psi(\theta)$, where $\psi(\theta) = - \ln \left( 1 - \theta \right) - \ln \left( 1 + \theta \right)$. Then, we can denote $R_t(\theta)=g_t(\theta)+h_t(\theta)$, where $g_t(\theta)=-\ln(1-\theta)$, $h_t(\theta)=-\ln(1+\theta)$.  We can prove that $g_t(\theta)$ and $h_t(\theta)$ are self-concordant functions according to the above definition.  Thus, we have 
\begin{equation*}
    \nabla^2 g_t(\theta)=(1-\theta)^{-2}, \nabla^2 h_t(\theta)=(1+\theta)^{-2}.
\end{equation*}
Then, we can get their third-order derivatives as 
 \begin{equation*}
      \nabla^3 g_t(\theta)=2(1-\theta)^{-3}, \nabla^3 h_t(\theta)=-2(1+\theta)^{-3}.
 \end{equation*} 

The function $g_t(\theta)$ is self-concordant, because
\begin{align}
    \left| \nabla^3 g_t(\theta) \right|= |2(1-\theta)^{-3}|= 2 \left( \nabla^2 g_t(\theta)\right)^{3/2}=2\left((1-\theta)^{-2}\right)^{3/2}=2(1-\theta)^{-3},
    \label{eq:g_concordant}
\end{align}
where $1-\theta\geq 0$, since $\theta\in[-1,1]$.

Similarly, since $1+\theta\geq 0$, $h_t(\theta)$ is also a self-concordant function, because
\begin{align}
    \left| \nabla^3 h_t(\theta) \right|= |-2(1+\theta)^{-3}|= 2 \left( \nabla^2 h_t(\theta)\right)^{3/2}=2\left((1+\theta)^{-2}\right)^{3/2}=2(1+\theta)^{-3}.
    \label{eq:h_concordant}
\end{align}
Previously, we denote $R_t(\theta)=g_t(\theta)+h_t(\theta)$. Thus we have
\begin{align*}
    \nabla^2 R_t(\theta)&=\nabla^2 g_t(\theta)+\nabla^2 h_t(\theta)\notag\\ \nabla^3 R_t(\theta)&=\nabla^3 g_t(\theta)+\nabla^3 h_t(\theta)
\end{align*}
Based on (\ref{eq:g_concordant}) and (\ref{eq:h_concordant}), we can get that 
\begin{equation*}
    \left| \nabla^3 g_t(\theta) \right|=  2 \left( \nabla^2 g_t(\theta)\right)^{3/2}, \left| \nabla^3 h_t(\theta) \right|=  2 \left( \nabla^2 h_t(\theta)\right)^{3/2}.
\end{equation*}
Thus, the sum of self-concordant functions $R_t(x)$ is also a self-concordant function, because 
\begin{equation*}
    \left| \nabla^3 R_t(\theta) \right|=  2 \left( \nabla^2 R_t(\theta)\right)^{3/2},
\end{equation*}
which satisfies the inequality (\ref{eq:self_concordant}). This completes the proof that the barrier function $R(\theta)=-\ln(1-\theta)-\ln(1+\theta)$ is a self-concordant function.

Furthermore, if we denote $F_t(\theta)=\eta\sum_{s=1}^t \langle \theta, \nabla \ell_{s}(\theta_s)  \rangle+R(\theta)$, then $F_t$ is also a self-concordant function.
This is because the first term of $F_t$ is linear, which means its contribution to the Hessian and the third derivative of $F_t$ is zero.  Therefore, we have 
\begin{equation*}
    \left| \nabla^3 F_t(\theta) \right|=\left| \nabla^3 R_t(\theta) \right|= 2 \left( \nabla^2F_t(\theta)\right)^{3/2}= 2 \left( \nabla^2 R_t(\theta)\right)^{3/2},
\end{equation*}

which means that the self-concordance of $F_t$ directly depends on 
$R$. Since $R$ is self-concordant, $F_t$ also satisfies the self-concordance conditions because the linear term does not affect the inequality (\ref{eq:self_concordant})
required for self-concordance.
\end{proof}


\noindent
\textbf{Lemma~\ref{lem:FTRL}}\, [Theorem 4.1 in \citet{abernethy2012interior}]
\textit{
Suppose the learner faces a sequence of convex loss functions $\ell_t(\cdot)$ and that the regularization function of FTRL, $R(\cdot)$, is a self-concordant barrier. Set the parameter $\eta$ so that $\eta \| \nabla \ell_t(\theta_t) \|_{\theta_t}^* \leq \frac{1}{4} $. Then, FTRL has 
$\mathrm{Regret}_T(\theta_*)
\leq
2 \eta \sum_{t=1}^T \| \nabla \ell_t(\theta_t) \|_{\theta_t}^{*2}
+ \frac{  R(\theta_*) - \min_{\theta \in \K} R(\theta)  }{\eta}$,
where $\theta_* \in \K$ is any comparator.}


Firstly, we will show the following Lemma \ref{lem:orabona}. 
\begin{lemma}[Lemma 7.1 in \cite{orabona2019modern}] \label{lem:orabona}Follow-the-Regularized-Leader + Barrier has the following regret bound:
\begin{align}
\mathrm{Regret}_T(\mathrm{FTRL})
\leq 
\sum_{t=1}^T \langle \nabla \ell_t , \theta_t - \theta_{t+1} \rangle
+ \frac{R(\theta_*) - \min_{\theta \in \K} R(\theta) }{\eta},
\end{align}
where $\theta_* \in \K$ is any benchmark.
\end{lemma}

\begin{proof}
In our setting, we can reduce the Online Convex Optimization (OCO) to the Online Linear Optimization (OLO) problem, because the regret of OCO can always be bounded by OLO. We recall that by the definition of convexity, the following inequality holds for a convex function $\ell_t(\cdot)$,
\begin{equation}
    \ell_t(y) \geq \ell_t(x)+ \langle \nabla\ell_t(x), y - x \rangle, \quad \forall x, y \in \K. 
\end{equation}

\medskip

\noindent The per-round regret of OCO and per-round regret of OLO are given by:
\begin{align*}
    \underbrace{\ell_t(\theta_t) - \ell_t(\theta_*)}_{\text{per-round regret of OCO}} &\leq \underbrace{\langle \nabla\ell_t(\theta_t), \theta_t - \theta_* \rangle}_{\text{per-round regret of OLO}}\\
    \Rightarrow \underbrace{\sum_{t=1}^T\ell_t(\theta_t) - \sum_{t=1}^T\ell_t(\theta_*)}_{\text{$\mathrm{Regret}_T$ of OCO}} &\leq \underbrace{\sum_{t=1}^T\langle \nabla\ell_t(\theta_t), \theta_t - \theta_* \rangle}_{\text{$\mathrm{Regret}_T$ of OLO}}.
\end{align*}
This shows that the regret of OCO will be bounded by the regret upper bound of OLO. Thus, we are going to establish the regret bound for our optimization problem by deriving the bound for OLO. To prove this lemma, we will use the definition of Be the Regularized Leader (BTRL). We recall that $w_{t}\gets \arg\min_{w\in \K} \eta\sum_{i=1}^t \langle w, \nabla \ell_{s}(w_s)  \rangle+R(w)$ for BTRL method, which is the same value as $\theta_{t+1}\gets \arg\min_{\theta\in \K} \eta\sum_{i=1}^t \langle \theta, \nabla \ell_{s}(\theta_s)  \rangle+R(
\theta)$ for FTRL. Thus, the regret of solving this OLO problem via BTRL and FTRL can be defined by the following equations respectively:
\begin{equation}
        \mathrm{Regret}_T(\mathrm{BTRL})= \sum_{t=1}^T \langle \theta_{t+1}, \nabla \ell_{t}(\theta_t)  \rangle-\sum_{t=1}^T \langle \theta_*, \nabla \ell_{t}(\theta_t)  \rangle=\sum_{t=1}^T \langle \theta_{t+1}-\theta_*, \nabla \ell_{t}(\theta_t)  \rangle,
        \label{eq:btrl}
\end{equation}

\begin{equation}
        \mathrm{Regret}_T(\mathrm{FTRL})= \sum_{t=1}^T \langle \theta_{t}, \nabla \ell_{t}(\theta_t)  \rangle-\sum_{t=1}^T \langle \theta_*, \nabla \ell_{t}(\theta_t)  \rangle=\sum_{t=1}^T \langle \theta_{t}-\theta_*, \nabla \ell_{t}(\theta_t)  \rangle
        \label{eq:ftrl},
\end{equation}

where $\theta_*\in \K$ is any benchmark. Based on (\ref{eq:btrl}) and (\ref{eq:ftrl}), we can get
\begin{equation}
    \mathrm{Regret}_T(\mathrm{FTRL})= \mathrm{Regret}_T(\mathrm{BTRL})+\sum_{t=1}^T \langle \theta_{t}-\theta_{t+1}, \nabla \ell_{t}(\theta_t)  \rangle.
    \label{eq:ftrl_btrl}
\end{equation}

Thus, we can get the upper bound of $\mathrm{Regret}_T(\mathrm{FTRL})$ by establish the bound for $\mathrm{Regret}_T(\mathrm{BTRL})$ via induction first. Assume that for $t=T-1$, we have
\begin{equation}
    \sum_{t=1}^{T-1} \langle \theta_{t+1}, \nabla \ell_{t}(\theta_t)  \rangle \leq \min_{{\theta} \in \K} \left( \sum_{t=1}^{T-1} \langle \theta, \nabla \ell_{t}(\theta_t)  \rangle + \frac{R({\theta})}{\eta}\right) - \frac{R(\theta_1)}{\eta},
    \label{eq:btrl_bound_t-1}
\end{equation}
where $\theta_1=\arg\min_{\theta\in \K}R(\theta)$. We denote $v=\arg\min_{{\theta} \in \K} \sum_{t=1}^{T-1} \langle \theta, \nabla \ell_{t}(\theta_t)  \rangle+\frac{R(\theta)}{\eta}$, and add $\langle \theta_{T+1}, \nabla \ell_{T}(\theta_T)  \rangle$ on both sides of (\ref{eq:btrl_bound_t-1}) to get:
\begin{equation}
    \langle \theta_{T+1}, \nabla \ell_{T}(\theta_T)  \rangle+\sum_{t=1}^{T-1} \langle \theta_{t+1}, \nabla \ell_{t}(\theta_t)  \rangle \leq \langle \theta_{T+1}, \nabla \ell_{T}(\theta_T)  \rangle+ \sum_{t=1}^{T-1} \langle v, \nabla \ell_{t}(v)  \rangle + \frac{R({v}) - R(\theta_1)}{\eta}.
    \label{eq:btrl_bound_t}
\end{equation}

As for the LHS of (\ref{eq:btrl_bound_t}), we know that $\langle \theta_{T+1}, \nabla \ell_{T}(\theta_T)  \rangle+\sum_{t=1}^{T-1} \langle \theta_{t+1}, \nabla \ell_{t}(\theta_t)  \rangle=\sum_{t=1}^{T} \langle \theta_{t+1}, \nabla \ell_{t}(\theta_t)  \rangle$. Then, we can rewrite the above inequality as:
\begin{align}
    \sum_{t=1}^{T} \langle \theta_{t+1}, \nabla \ell_{t}(\theta_t)  \rangle &\leq \langle \theta_{T+1}, \nabla \ell_{T}(\theta_T)  \rangle+ \sum_{t=1}^{T-1} \langle v, \nabla \ell_{t}(v)  \rangle  + \frac{R({v}) - R(\theta_1)}{\eta}\notag\\
    &\leq \langle \theta_{T+1}, \nabla \ell_{T}(\theta_T)  \rangle+ \sum_{t=1}^{T-1} \langle \theta_{T+1}, \nabla \ell_{t}(\theta_t)  \rangle + \frac{R({\theta_{T+1}}) - R(\theta_1)}{\eta}\label{eq:btrl_bound_hold}\\
    &=\sum_{t=1}^{T} \langle \theta_{T+1}, \nabla \ell_{t}(\theta_t)  \rangle + \frac{R({\theta_{T+1}}) - R(\theta_1)}{\eta}\notag\\
    &=\min_{\theta \in \K} \left(\sum_{t=1}^{T} \langle \theta, \nabla \ell_{t}(\theta_t)  \rangle+ \frac{R({\theta})}{\eta}\right) - \frac{R(\theta_1)}{\eta}\label{eq:btrl_min}.
\end{align}
We recall that $\theta_{T+1}=\arg\min_{\theta\in \K}\eta\sum_{t=1}^{T} \langle \theta, \nabla \ell_{t}(\theta_t)  \rangle+R(\theta)=\arg\min_{\theta\in \K}\sum_{t=1}^{T} \langle \theta, \nabla \ell_{t}(\theta_t)  \rangle+\frac{R(\theta)}{\eta}$. Since $v=\arg\min_{\theta \in \K} \sum_{t=1}^{T-1} \langle \theta, \nabla \ell_{t}(\theta_t)  \rangle+\frac{R(\theta)}{\eta}$, we known that $\sum_{t=1}^{T-1} \langle v, \nabla \ell_{t}(\theta_t)  \rangle +\frac{R(v)}{\eta}\leq \sum_{t=1}^{T-1} \langle \theta_{T+1}, \nabla \ell_{t}(\theta_t)  \rangle+\frac{R(\theta_{T+1})}{\eta}$. Then, the inequality (\ref{eq:btrl_bound_hold}) holds. According to (\ref{eq:btrl_min}), we can get that for any benchmark $\theta_*$:

\begin{align}
    \sum_{t=1}^{T} \langle \theta_{t+1}, \nabla \ell_{t}(\theta_t)  \rangle &\leq \min_{\theta \in \K} \left(\sum_{t=1}^{T} \langle \theta, \nabla \ell_{t}(\theta_t)  \rangle+ \frac{R({\theta})}{\eta}\right) - \frac{R(\theta_1)}{\eta}\leq\sum_{t=1}^{T} \langle \theta_*, \nabla \ell_{t}(\theta_t)  \rangle+ \frac{R({\theta_*})}{\eta} - \frac{R(\theta_1)}{\eta}\notag\\
    \Rightarrow \mathrm{Regret}_T(\mathrm{BTRL})&=\sum_{t=1}^{T} \langle \theta_{t+1}, \nabla \ell_{t}(\theta_t)  \rangle - \sum_{t=1}^{T} \langle \theta_*, \nabla \ell_{t}(\theta_t)  \rangle\leq \frac{R({\theta_*}) - R(\theta_1)}{\eta}\label{eq:btrl_1}
\end{align}

Combining (\ref{eq:ftrl_btrl}) and (\ref{eq:btrl_1}), we can get
\begin{align}
    \mathrm{Regret}_T(\mathrm{FTRL})
    \leq \sum_{t=1}^T \langle \theta_{t}-\theta_{t+1}, \nabla \ell_{t}(\theta_t)  \rangle+\frac{R({\theta_*}) - R(\theta_1)}{\eta}
    \label{eq:ftrl_1},
\end{align}
where $\theta_1=\arg\min_{\theta\in \K} R(\theta)$. This completes the proof.
\end{proof}

Then, with the above Lemma \ref{lem:orabona}, we will prove Lemma \ref{lem:FTRL}. 


\begin{proof}
Let $ C(\cdot)$ in the following be a self-concordant function. For any $ \theta\in \K $, the associated norm $ \| \cdot \|_\theta $ is defined as $\|\beta\|_\theta = \sqrt{\beta^T \nabla^2 C(\theta) \beta}$, while
its dual norm $ \| \cdot \|_\theta^* $ is defined as $\|\beta\|_\theta^* = \sqrt{\beta^T (\nabla^2 C(\theta))^{-1} \beta}$. We recall the Newton decrement for $ C $ at $ \theta $:
\begin{equation}
    \|\nabla C(\theta)\|_\theta ^* = \|(\nabla^2 C(\theta))^{-1}\nabla C(\theta)\|_\theta.
    \label{eq:lambda_def}
\end{equation}

According to \cite{abernethy2012interior}, We have the following theorem, which is:
\begin{theorem}\label{them:abernethy_the2.1}
\text{(Theorem 2.1 in \cite{abernethy2012interior})} Suppose $ \|\nabla C(\theta)\|_\theta ^* \leq \frac{1}{4} $. Then, for any self-concordant function $ C $, it holds that:
\begin{equation}
    \|\theta - \arg\min_{\theta\in \K} C(\theta)\|_\theta \leq 2\|\nabla C(\theta)\|_\theta ^*.
    \label{eq:norm_newton}
\end{equation}
\end{theorem}
 We recall that $F_t(\theta)=\eta\sum_{s=1}^t \langle \theta, \nabla \ell_{s}(\theta_s) \rangle+R(\theta)$ is a self-concordant function, and $\theta_{t+1}=\arg\min_{\theta\in \K} F_t(\theta)$. According to (\ref{eq:norm_newton}), with the assumption that $\| \nabla F_t(\theta_t) \|_{\theta_t}^*\leq \frac{1}{4}$,  we have:
\begin{equation}
\|\theta_t - \theta_{t+1}\|_{\theta_t} = \|\theta_t - \arg\min_{\theta\in \K} F_t(\theta)\|_{\theta_t} \leq  2\| \nabla F_t(\theta_t) \|_{\theta_t}^*,
\label{eq:norm_bound2}
\end{equation}

Since $ \theta_t=\arg\min_{\theta\in \K} F_{t-1}(\theta) $, and we know $ F_t(\theta) = F_{t-1}(\theta) + \eta \langle \nabla\ell_t, \theta\rangle \Rightarrow \nabla F_t(\theta_t) = \eta \nabla\ell_t$. Plugging it in (\ref{eq:norm_bound2}), and recalling the definition $\|\beta\|_\theta^* = \sqrt{\beta^T (\nabla^2 C(\theta))^{-1} \beta}$ gives
\begin{equation}
    \|\theta_t - \theta_{t+1}\|_{\theta_t} \leq 2\eta\|\nabla\ell_t\|_{\theta_t}^*=2\eta\sqrt{(\nabla\ell_t)^T (\nabla^2 F_t(\theta))^{-1}(\nabla\ell_t)}=2\eta\sqrt{(\nabla\ell_t)^T (\nabla^2 R(\theta))^{-1}(\nabla\ell_t)},
    \label{eq:norm_com1}
\end{equation}
where we use the fact that the first term of $F_t=\eta\sum_{s=1}^t \langle \theta, \nabla \ell_{s}(\theta_s)  \rangle+R(\theta)$ is linear, which means its contribution to the Hessian of $F_t$ is zero.  Therefore, we have $\nabla^2 F_t(\theta)=\nabla^2 R(\theta)$.
Since $\| \nabla F_t(\theta_t) \|_{\theta_t}^*=\eta \|\nabla\ell_t\|_{\theta_t}^*$, the assumption can be written as $ \eta \|\nabla\ell_t\|_{\theta_t}^*= \eta\sqrt{(\nabla\ell_t)^T (\nabla^2 R(\theta))^{-1}(\nabla\ell_t)}\leq \frac{1}{4} $. 

Since H\"older's inequality holds for any primal-dual norm pair, we can bound $ \langle\nabla\ell_t ,\theta_t - \theta_{t+1})\rangle $ by applying the inequality :
\begin{equation}
    \langle\nabla\ell_t ,\theta_t - \theta_{t+1})\rangle \leq \|\nabla\ell_t\|_{\theta_t}^* \|\theta_t - \theta_{t+1}\|_{\theta_t}.
    \label{eq:holders}
\end{equation}
Combining (\ref{eq:norm_com1}) and (\ref{eq:holders}), we have
\begin{equation}
    \langle\nabla\ell_t ,\theta_t - \theta_{t+1})\rangle \leq \|\nabla\ell_t\|_{\theta_t}^* \|\theta_t - \theta_{t+1}\|_{\theta_t}=2\eta{(\nabla\ell_t)^T (\nabla^2 R(\theta))^{-1}(\nabla\ell_t)}.
    \label{eq:regret_ftrlb}
\end{equation}

According to (\ref{eq:ftrl_1}) and (\ref{eq:regret_ftrlb}), with the assumption $\eta\sqrt{(\nabla\ell_t)^T (\nabla^2 R(\theta))^{-1}(\nabla\ell_t)}\leq \frac{1}{4}$ for all $t$, we have
\begin{align} 
\mathrm{Regret}_T(\mathrm{FTRLBarrier})&\leq 2 \eta \sum_{t=1}^T
(\nabla \ell_t(\theta_t))^T ( \nabla^2 R(\theta_t) )^{-1}(\nabla\ell_t) + 
\frac{  R(\theta_*)-\min_{\theta\in \K} R(\theta) }{\eta}
\\
&=2 \eta \sum_{t=1}^T
\|\nabla\ell_t(\theta_t)\|_{\theta_t}^* + 
\frac{  R(\theta_*)-\min_{\theta\in \K} R(\theta) }{\eta},
\end{align}
where $\theta_* \in \K$ is any comparator. 

\begin{remark}
Here comes the intuition why we can let the decision space to be 
$\K:=[-1,1]$: 
When $\theta \to \partial \K$ approaches the boundary, $\left| \nabla \ell_t(\theta) \right| \to \infty$
but also $R^2(\theta) \to \infty$, hence it is possible that
$( \nabla \ell_t(\theta_t) )^2 ( \nabla^2 R_t(\theta_t) )^{-1}$ can remain small.
\end{remark}

Now we check if the condition 
$\eta  \sqrt{ (\nabla \ell_t(\theta_t)^2 ( \nabla^2 R(\theta_t) )^{-1} } \leq \frac{1}{4}$ holds by deriving the upper bound of 
$(\nabla \ell_t(\theta_t)^2 ( \nabla^2 R(\theta_t) )^{-1}$.
We have
\begin{align}
(\nabla \ell_t(\theta_t) )^2 ( \nabla^2 R(\theta_t) )^{-1}&=(\nabla \ell_t(\theta_t) )^2 \left( \frac{2+2\theta_t^2}{ (1-\theta_t)^2(1+\theta_t)^2} \right)^{-1}\notag\\
&=\left(\frac{g_t}{1-g_t \theta_t}\right)^2\cdot\frac{(1-\theta_t)^2(1+\theta_t)^2}{2+2\theta_t^2}\notag\\
&=\frac{g_t^2}{2+2\theta_t^2}\frac{(1-\theta_t)^2}{(1-g_t\theta_t)^2}(1+\theta_t)^2\label{eq:discuss_g}
\end{align}

We denote the second term in (\ref{eq:discuss_g}) as the function of $g_t$, i.e., $h(g_t)=\frac{(1-\theta_t)^2}{(1-g_t\theta_t)^2}$, where $g_t\in[-1,1]$. We note that when $\theta_t=0$, we have
$\frac{(1-\theta_t)^2}{(1-g_t\theta_t)^2} = 1$.
Then, we are going to show its upper bound. Because the hessian $\nabla^2 h(g_t)=\frac{6(1-\theta_t)^2\theta^2}{(1-g_t\theta_t)^4}\geq 0$, which indicates $h(g_t)$ is a convex function of $g_t$. 
% Since $\nabla h(g_t)=\frac{-2\theta_t(1-\theta_t)^2}{(1-g_t\theta_t)^3}$, the function $h(g_t)$ has no stationary point. 
Thus, the maximum value will be achieved at $-1$ or $1$, which are $h(-1)=\frac{(1-\theta_t)^2}{(1+\theta_t)^2}$ or $h(1)=1$. We know that $\theta_t\in(-1,1)$ with the barrier function. If we denote $r(\theta_t)=\frac{(1-\theta_t)^2}{(1+\theta_t)^2}$, its gradient can be computed as
\begin{align*}
    \nabla r(\theta_t)=\frac{4(\theta_t^2-1)}{(1+\theta_t)^4}.
\end{align*}

Based on it, the function $r(\theta_t)$ is monotically decreasing in $(-1,1)$. Since $r(0)=1$, we get that $\frac{(1-\theta_t)^2}{(1+\theta_t)^2}> 1$ when $\theta_t\in(-1,0)$,  and $\frac{(1-\theta_t)^2}{(1+\theta_t)^2}\leq 1$ when $\theta_t\in[0,1)$. We recall the inequality (\ref{eq:discuss_g}), when $\theta_t\in(-1,0)$:
\begin{align}
(\nabla \ell_t(\theta_t) )^2 ( \nabla^2 R(\theta_t) )^{-1}&\leq\frac{g_t^2}{2+2\theta_t^2}\frac{(1-\theta_t)^2}{(1-g_t\theta_t)^2}(1+\theta_t)^2\notag\\
&\leq \frac{g_t^2}{2+2\theta_t^2}\frac{(1-\theta_t)^2}{(1+\theta_t)^2}(1+\theta_t)^2\notag\\
&=\frac{g_t^2}{2+2\theta_t^2}(1-\theta_t)^2\notag\\
&\leq \frac{(1-\theta_t)^2}{2+2\theta_t^2} \quad(\text{Since } g_t\in[-1,1])\label{eq:denote_f1}\\
&<\frac{(1-(-1)))^2}{2+2(-1)^2}\label{eq:max_-1}\\
&=1\notag,
\end{align}

when $\theta_t\in[0,1)$:

\begin{align}
(\nabla \ell_t(\theta_t) )^2 ( \nabla^2 R(\theta_t) )^{-1}&\leq\frac{g_t^2}{2+2\theta_t^2}\frac{(1-\theta_t)^2}{(1-g_t\theta_t)^2}(1+\theta_t)^2\notag\\
&\leq \frac{g_t^2}{2+2\theta_t^2}\cdot1  \cdot(1+\theta_t)^2\notag\\
&\leq \frac{(1+\theta_t)^2}{2+2\theta_t^2} \quad(\text{Since } g_t\in[-1,1])\label{eq:denote_f2}\\
&<\frac{(1+1)^2}{2+2\cdot1^2}\label{eq:max_1}\\
&=1\notag,
\end{align}

Considering (\ref{eq:denote_f1}) and (\ref{eq:denote_f2}), we denote: $f_1(\theta_t)=(1-\theta_t)^2/{(2+2\theta_t^2)}$, and $f_2(\theta_t)=(1+\theta_t)^2/{(2+2\theta_t^2)}$. Their gradients can be computed as following: 
\begin{equation*}
    f_1'(\theta_t)=\frac{\theta^2-1}{(\theta_t^2+1)^2}, \quad \text{and} \quad f_2'(\theta_t)=\frac{\theta^2+1}{(\theta_t^2+1)^2}.
\end{equation*}
After analyzing, we know that the function $f_1(\theta_t)$ monotically decreases within $\theta_t\in(-1,0)$, while $f_2(\theta_t)$ monotically increases when $\theta_t\in[0,1)$. Thus, $f_1(\theta_t)$ and $f_2(\theta_t)$ will take the largest values at $\theta_t=-1$ and $\theta_t=1$, respectively. The largest value is $f_1(-1)=f_2(1)=1$. Thus, we get the upper bound, which is $(\nabla \ell_t(\theta_t) )^2 ( \nabla^2 R(\theta_t) )^{-1}< 1$. With this bound, we can conservatively select $\eta\leq \frac{1}{4}$ to ensure the condition for the regret bound.

\end{proof}


\noindent
\textbf{Lemma~\ref{lem:FTRL_const}}
\textit{
Denote $G_t:= \sum_{s=1}^t \nabla \ell(\theta_s) $. 
Suppose that there exists a time point $t_0$ such that for all $t \geq t_0$, it holds that
\begin{align}
\textbf{(Linear growth of cumulative gradients):} & \quad
\left | G_t   \right| \geq ct
\end{align}
for some $c>0$. 
Then, FTRL+Barrier (Algorithm~\ref{alg:FTRL}) has 
$\mathrm{Regret}_T(\theta_*) 
\leq \frac{t_0}{8\eta} + \frac{4}{c'\eta} \left(\frac{1}{t_0}-\frac{1}{T-1}\right)
+ \frac{R(\theta_*)}{\eta}$,
where $\eta\leq \frac{1}{4}$, constant $c' > 0$, and any comparator $\theta_* \in \K$.
}

\begin{proof}

We recall that $ R(\theta) = \phi(\theta) $, where $\phi(\theta)=-\ln(1 - \theta) - \ln(1 + \theta)$. We can compute the Hessian:
\begin{equation}
    \nabla^2 R(\theta_t)=\frac{1}{(1-\theta)^2}+\frac{1}{(1+\theta)^2},
    \label{eq:hessian_r}
\end{equation}
Suppose that for large enough $t > t_0$, we have
\begin{equation} \label{cond}
| \sum_{s=1}^t \nabla \ell_s(\theta_s) | \geq c t,
\end{equation}
where $c > 0$ is a constant.

We recall that $\theta_{t+1}=\arg\min_\theta F_{t}$, where $F_{t}(\theta)=\eta\sum_{s=1}^{t} \langle \theta, \nabla \ell_{s}(\theta_s) \rangle+R(\theta)$ is a convex function. According to the first-order optimality condition, we have $\nabla F_{t}(\theta_{t+1})=0$. Hence,
\begin{align*}
    \nabla F_{t}(\theta_{t+1})=\eta\sum_{s=1}^{t} \nabla \ell_{s}(\theta_s)+\nabla R(\theta_{t+1})=0\Rightarrow \eta\sum_{s=1}^{t} \nabla \ell_{s}(\theta_s)=-\nabla R(\theta_{t+1})
\end{align*}
There exists a constant $\hat{c}$ such that:
\begin{align}
    \eta\sum_{s=1}^{t} \nabla \ell_{s}(\theta_s)=\eta \hat{c}t&\Rightarrow \eta\sum_{s=1}^{t} \nabla \ell_{s}(\theta_s)=-\nabla R(\theta_{t+1})=\frac{-2\theta_{t+1}}{1-\theta_{t+1}^2}=\eta \hat{c} t\notag\\
    &\Rightarrow \theta_{t+1}=\frac{1}{\eta\hat{c}t}\pm\sqrt{\frac{1}{\eta^2\hat{c}^2t^2}+1}\Rightarrow \theta_{t+1} = 
\begin{cases} 
\frac{1}{\eta\hat{c}t}-\sqrt{\frac{1}{\eta^2\hat{c}^2t^2}+1} & \text{if } \hat{c} \geq 0, \\
\frac{1}{\eta\hat{c}t}+\sqrt{\frac{1}{\eta^2\hat{c}^2t^2}+1} & \text{if } \hat{c} < 0,
\end{cases}\label{eq:bound_eta}\\
&\Rightarrow  |\theta_{t+1}| = 
\begin{cases} 
\left|\sqrt{\frac{1}{\eta^2\hat{c}^2t^2}+1}-\frac{1}{\eta\hat{c}t}\right|\geq 1- \frac{1}{\eta\hat{c}t}& \text{if } \hat{c} \geq 0, \\
\left|\sqrt{\frac{1}{\eta^2\hat{c}^2t^2}+1}+\frac{1}{\eta\hat{c}t}\right|\geq 1- \frac{1}{\eta|\hat{c}|t} & \text{if } \hat{c} < 0, 
\label{56}
\end{cases}
\end{align}

where (\ref{eq:bound_eta}) is because $\theta_{t+1}\in[-1,1]$.
If $a>0$, $b>0$, it is obvious that $\sqrt{a+b}>\sqrt{a}$, and $\sqrt{a+b}>\sqrt{b}$. Thus, let $a=\frac{1}{\eta^2\hat{c}^2t^2}$, $b=1$, we have $\sqrt{\frac{1}{\eta^2\hat{c}^2t^2}+1}>1$. 
Hence, we get that $ |\theta_{t+1}| \geq  1- \frac{1}{\eta \bar{c}t}$, where we denote $\bar{c}:=|\hat{c}|>0$. According to (\ref{eq:hessian_r}), we can get
\begin{align*}
    \nabla^2 R(\theta_t)=\frac{2+2\theta_t^2}{(1-\theta_t^2)^2}\geq\frac{2+2\theta_t^2}{(1-(1-\frac{1}{\bar{c}\eta (t-1)})^2)^2}=\frac{2+2\theta_t^2}{(\frac{2}{\bar{c}\eta (t-1)}-\frac{1}{(\bar{c}\eta( t-1))^2})^2}\overset{(i)}{\geq} \frac{2}{(\frac{2}{\bar{c}\eta (t-1)})^2} = \frac{\bar{c}^2\eta^2 (t-1)^2}{2},
\end{align*}
where (i) we use the fact that $\frac{1}{\bar{c}\eta (t-1)}\geq\frac{1}{(\bar{c}\eta( t-1))^2}> 0$ since $\frac{1}{\bar{c}\eta (t-1)}\leq 1$ for large enough $t$, which means that $\frac{2}{\bar{c}\eta (t-1)}\geq\frac{1}{(\bar{c}\eta( t-1))^2}>0$, and $\theta_t\in[-1,1]\Rightarrow 2+2\theta_t^2\geq 2$. In this case, we can bound
\begin{align}
 \|\nabla \ell_t\|_{\theta_t}^{*2} = (\nabla \ell_t(\theta_t))^2 ( \nabla^2 R(\theta_t) )^{-1}=(\frac{g_t}{1-g_t\theta_t})^2\cdot \frac{2}{\bar{c}^2\eta^2 (t-1)^2} \leq \frac{2}{(1-\theta_t)^2\bar{c}^2\eta^2(t-1)^2}\notag\\
 \leq\frac{2}{c'^2\eta^2(t-1)^2}\label{eq:norm_bound}.
\end{align}
where $\nabla\ell_t(\cdot)$ is monotically increasing with $g_t\in[-1,1]$, and based on the expression of $\theta_t$ on 
\myeqref{eq:bound_eta}, there must exist constant $c'$ which satisfies that $((1-\theta_t)\bar{c})^2\geq c'^2$ for all $t$.

Since $ \eta \|\nabla\ell_t\|_{\theta_t}^*= \eta\sqrt{(\nabla\ell_t)^2 (\nabla^2 R(\theta))^{-1}}\leq \frac{1}{4} $, we have $ {(\nabla\ell_t)^2 (\nabla^2 R(\theta))^{-1}}\leq \frac{1}{16\eta^2} $. Then,
\begin{align} 
\mathrm{Regret}_T(\mathrm{FTRLBarrier})
&\leq 2 \eta \sum_{t=1}^{t_0}
(\nabla \ell_t(\theta_t))^2 ( \nabla^2 R(\theta_t) )^{-1} + 2\eta\sum_{t=t_0+1}^{T}
(\nabla \ell_t(\theta_t))^2 ( \nabla^2 R(\theta_t) )^{-1}+
\frac{  R(\theta_*)  }{\eta}\notag,
\\ &
\leq 2 \eta \sum_{t=1}^{t_0}
(\nabla \ell_t(\theta_t))^2 ( \nabla^2 R(\theta_t) )^{-1} + 
2\eta\sum_{t=t_0+1}^{T} \frac{2}{c'^2 \eta^2 (t-1)^2}
+
\frac{  R(\theta_*)  }{\eta}\notag,
\\ &
\leq 2 \eta \sum_{t=1}^{t_0}
(\nabla \ell_t(\theta_t))^2 ( \nabla^2 R(\theta_t) )^{-1} + 
\int_{t_0}^{T-1} \frac{4}{c'^2 \eta t^2}
+
\frac{  R(\theta_*)  }{\eta}\notag
\\ &
\leq 2 t_0 \frac{1}{16\eta} + \frac{4}{c'^2\eta} \left( \frac{1}{t_0} - \frac{1}{T-1} \right )
+ \frac{  R(\theta_*)  }{\eta}\notag\\
&=\frac{t_0}{8\eta} + \frac{4}{c'^2\eta} \left( \frac{1}{t_0} - \frac{1}{T-1} \right )
+ \frac{  R(\theta_*)  }{\eta}\notag\\
&\leq\frac{1}{\eta}\left(\frac{t_0}{8} + \frac{4}{c'^2t_0} 
+  R(\theta_*)\right)\notag.
\end{align}
We recall that the parameter $\eta\leq \frac{1}{4}$. If we choose $\eta=\frac{1}{4}$, we have a constant regret
\begin{equation}
\frac{t_0}{2} + \frac{16}{c'^2} \frac{1}{t_0}
+ 4R(\theta_*).
\end{equation}

\end{proof}

\textbf{Condition and Examples for Achieving a constant regret.} Previously, we have shown that if for large enough $t\geq t_0$, we have
\begin{equation} \label{cond}
\left| \sum_{s=1}^t \nabla \ell_t(\theta_t) \right| \geq c t,
\end{equation}
where $c > 0$ is a constant, then the algorithm has a constant regret.

This condition \myeqref{cond} is reasonable in our setting. Recall that $\nabla \ell(\theta_t) = \frac{g_t}{1-g_t \theta}$, where $g_t:= x_t- y_t\in[-1,1]$ is the difference of samples at each time, and we use a barrier function $R(\theta)=-\ln(1-\theta)-\ln(1+\theta)$ to help choose $\theta$, which indicates $1-g_t \theta \in(0,2)$ is a positive constant. Hence, when the population means are different, i.e., $\left| \sum_{s=1}^t g_s \right| \geq c' t$ for some positive constant $c'$, condition \myeqref{cond} can be easity satisfied, which aligns with the alternative hypothesis. In this scenario, the use of no-regret learning is to quickly accumulate the wealth when $H_0: \mu_x = \mu_y$ is false. Moreover, we provide a couple of lemmas highlighting a couple of examples/scenarios that the condition \myeqref{cond} is easily satisfied with high probability. 

\textbf{Example 1} (Distributions with disjoint supports.)\label{example1}
Consider $x_t \sim \rho_x$ and $y_t \sim \rho_y$, where $\rho_x$ and $\rho_y$ have disjoint but continuous supports, as illustrated in Figure~\ref{fig:ex1}. For example, $x_t$, $y_t$ are respectively drawn from the uniform distributions $\mathrm{Uniform}(a,b)$, and $\mathrm{Uniform}(m,n)$, where $a> n$. Then, we have $g_s =x_s-y_s\geq a-n$. Thus 
\begin{equation}
    \left|\sum_{s=1}^t g_s\right|=\left|\sum_{s=1}^t (x_s-y_s)\right| > (a-n)t\Rightarrow \left| \sum_{s=1}^t \nabla \ell_t(\theta_t) \right|=\left|\sum_{s=1}^t \frac{g_s}{1-g_s\theta_s}\right|>\frac{(a-n)t}{2}\geq c't,
    \label{eq:linear_cumulative}
\end{equation}

which means that we have $\left| \sum_{s=1}^t \nabla \ell_t(\theta_t) \right|\geq ct$ for any positive constant $c'\leq \frac{a-n}{2}$.



\textbf{Example 2} (Distributions with overlapping supports; high signal-to-noise ratio.) Denote $\sigma_x^2$ the variance of samples $\{x_t\}$ and $\sigma_y^2$ the variance of samples $\{y_t\}$. 
Then, with probability at least $1-\delta$, the linear growth condition of cumulative gradients  holds for all $t \geq t_0$, where $t_0=\frac{1}{b^2\delta(\mu_x-\mu_y)^2/(\sigma_x^2+\sigma_y^2)}$.
That is, if the signal-to-noise ratio (i.e., $\frac{(\mu_x-\mu_y)^2}{\sigma_x^2+\sigma_y^2}$) is high, then the growth condition is easily satisfied for any sufficiently large $t$.
Figure~\ref{fig:ex2} illustrates an example.
% of such pair of distributions. 

\begin{proof}
We denote the mean values as $\mathbb{E}(x_t)=\mu_x$ and $\mathbb{E}(y_t)=\mu_y$, their variances as $\mathrm{Var}(x_t)=\sigma_x^2$ and $\mathrm{Var}(y_t)=\sigma_y^2$.  Since $x_t$ and $y_t$ are independent random variables, then we have 
\begin{equation*}
    \mathbb{E}\left(\sum_{s=1}^t g_s\right)=\sum_{s=1}^t\mathbb{E}(x_s-y_s)=t\cdot (\mu_x-\mu_y),
\end{equation*}
\begin{equation*}
    \mathrm{Var}\left(\sum_{s=1}^t g_s\right)=\sum_{s=1}^t\mathrm{Var}(x_s-y_s)=t\cdot (\sigma_x^2+\sigma_y^2).
\end{equation*}

Let us assume that $\mu_x>\mu_y$ without the loss of generality. 
In the following, we would like to show with high probabiltiy,
the linear growth condition holds, i.e.,
\begin{equation}
\sum_{s=1}^t g_s \geq (1 - b) t ( \mu_x - \mu_y)\label{eq:che_1},
\end{equation}
where $b \in (0,1)$.

Then, we can use Chebyshev's inequality to show the probability upper bound of the complementary event of \myeqref{eq:che_1}:
\begin{align}
    \mathbb{P}\left(\mathbb{E}\left(\sum_{s=1}^t g_s\right)-\sum_{s=1}^t g_t\geq b\cdot\mathbb{E}\left(\sum_{s=1}^t g_s\right)\right)&\leq \mathbb{P}\left(\left|\mathbb{E}\left(\sum_{s=1}^t g_s\right)-\sum_{s=1}^t g_s\right|\geq b\cdot\mathbb{E}\left(\sum_{s=1}^t g_s\right)\right)\notag\\
    &\leq\frac{\mathrm{Var}(\sum_{s=1}^t g_s)}{b^2\cdot\mathbb{E}^2(\sum_{s=1}^t g_s)}\notag\\
    &= \frac{t (\sigma_x^2+\sigma_y^2)}{b^2 t^2(\mu_x-\mu_y)^2}\notag\\
    &=\frac{1}{t}\frac{1}{b^2}\frac{1}{(\mu_x-\mu_y)^2/(\sigma_x^2+\sigma_y^2)}\label{eq:t0_highpro}.
\end{align}

When $t$ is large enough, and the sum of the variance is much smaller than the difference of the mean values (i.e., high signal-to-noise ratio), the probability of the complementary event of event (\ref{eq:linear_cumulative}) is expected to be very small. It indicates that in this case, (\ref{eq:linear_cumulative}) will hold with high probability. Specifically, if we let (\ref{eq:t0_highpro}) be $\delta$, then for $t\geq \frac{1}{b^2\delta(\mu_x-\mu_y)^2/(\sigma_x^2+\sigma_y^2)}$, we have $\left| \sum_{s=1}^t \nabla \ell_t(\theta_t) \right|\geq ct$ satisfied with probability at least $1-\delta$.

\end{proof}

\textbf{Example 3} (Time-varying distribution with mean shift.)
Consider the modified hypothesis testing setting \myeqref{setting:distributionshifts}, where $\{y_t\}$ are from a distribution with no mean shift, while $\{x_t\}$ are from a distribution for which we would like to determine whether there exists a shift of its mean at some time point $\hat{t}$. In addition, $\{y_t\}$ has the same mean as that of $\{x_t\}$ at the beginning, i.e., $\mu_x(t)=\mu_y(t< \hat{t})$. Under $H_1$, we have $\mu_y(t\geq \hat{t})=\mu_y(t< \hat{t})+a$. Then, with probability at least $1-\delta$, the condition \myeqref{growth} holds for all $t \geq t_0$, where $t_0=\frac{2 b\hat{t} + \frac{1}{\delta^2S} + \sqrt{\frac{4b\hat{t}}{\delta^2S} + \frac{1}{\delta^2S^2}}}{2 b^2}$, where $S=\frac{a^2}{\sigma_x^2+\sigma_y^2}$, and $b\in(0,1)$.


\begin{proof}
We denote that the mean values of $y_t$, which is drawn from a prepared dataset with no-shift distribution, as $\mathbb{E}(y_t)=\mu_y(t)=\mu_0$, for all $t$. Under $H_0$, $\mathbb{E}(x_t)=\mathbb{E}(y_t)\Rightarrow \mu_x(t)=\mu_y(t)=\mu_0$, while we assume that under $H_1$, from time $\hat{t}$, there exists a mean shift for $y_t$, i.e., $\mu_x(t)=\mu_0+a$, where we assume $a>0$ without the loss of generality. Suppose their variances do not change with time, i.e., $\mathrm{Var}(x_t)=\sigma_x^2$, $\mathrm{Var}(y_t)=\sigma_y^2$ for all $t$.  Since $x_t$ and $y_t$ are independent random variables, then we have 
\begin{equation*}
    \mathbb{E}\left(\sum_{s=1}^t g_s\right)=\sum_{s=1}^t\mathbb{E}(x_s-y_s)=\sum_{s=1}^{\hat{t}-1}(\mu_x-\mu_y)+\sum_{s=\hat{t}}^t(\mu_x-\mu_y)=(t-\hat{t})\cdot a,
\end{equation*}
\begin{equation*}
    \mathrm{Var}\left(\sum_{s=1}^t g_s\right)=\sum_{s=1}^t\mathrm{Var}(x_s-y_s)=t\cdot (\sigma_x^2+\sigma_y^2).
\end{equation*}
 

In the following, we would like to show with high probabiltiy,
the linear growth condition holds, i.e.,
\begin{equation}
\sum_{s=1}^t g_s \geq (1 - b) a t \label{eq:che_2},
\end{equation}
where $b \in (0,1)$. Then, we can show the probability upper bound of the complementary event of \myeqref{eq:che_2}:

\begin{align}
    \mathbb{P}\left(\mathbb{E}\left(\sum_{s=1}^t g_s\right)-\sum_{s=1}^t g_t\geq b\cdot\mathbb{E}\left(\sum_{s=1}^t g_s\right)-(1-b)a\hat{t}\right)\leq \mathbb{P}\left(\left|\mathbb{E}\left(\sum_{s=1}^t g_s\right)-\sum_{s=1}^t g_s\right|\geq b\cdot\mathbb{E}\left(\sum_{s=1}^t g_s\right)-(1-b)a\hat{t}\right)\label{eq:t0_highpro_2},
\end{align}

 Based on the Chebyshev's inequality, we can further have:
\begin{align}
    \mathbb{P}\left(\left|\mathbb{E}\left(\sum_{s=1}^t g_s\right)-\sum_{s=1}^t g_s\right|\geq  b\cdot\mathbb{E}\left(\sum_{s=1}^t g_s\right)-(1-b)a\hat{t}\right)&\leq\frac{\mathrm{Var}(\sum_{s=1}^t g_s)}{\left(b\cdot\mathbb{E}(\sum_{s=1}^t g_s)-(1-b)a\hat{t}\right)^2}\notag\\
    &= \frac{t (\sigma_x^2+\sigma_y^2)}{\left(b(t-\hat{t})a-(1-b)a\hat{t}\right)^2}\notag\\
    &=\frac{1}{b^2t-2b\hat{t}+\frac{\hat{t}^2}{t}}\cdot\frac{1}{a^2/(\sigma_x^2+\sigma_y^2)}\label{eq:t0_highpro_2_3},
\end{align}


Since $a$ is the difference between $\mu_x(t)$ and $\mu_y(t)$ for $t\geq \hat{t}$. Thus, if $t$ is large enough, and the sum of the variance is much smaller than the difference of the mean values (i.e., high signal-to-noise ratio), the probability of the complementary event of event (\ref{eq:linear_cumulative}) is expected to be very small. It indicates that in this case, (\ref{eq:linear_cumulative}) will hold with high probability. Specifically, if we let (\ref{eq:t0_highpro_2_3}) be $\delta$, then for $t\geq \frac{2 b\hat{t} + \frac{1}{\delta^2S} + \sqrt{\frac{4b\hat{t}}{\delta^2S} + \frac{1}{\delta^2S^2}}}{2 b^2}
$, where $S=\frac{a^2}{\sigma_x^2+\sigma_y^2}$, we have $\left| \sum_{s=1}^t \nabla \ell_t(\theta_t) \right|\geq ct$ satisfied with probability at least $1-\delta$.

\end{proof}



\begin{theorem} %\label{thm:main1}
Algorithm~\ref{alg:Betting} with $\mathrm{OAlg}$ being FTRL+Barrier (Algorithm~\ref{alg:FTRL}) is a level-$\alpha$ sequential test with asymptotic power one. Furthermore, denote $\Delta:= | \mu_y - \mu_x |$ and assume that the linear growth condition \myeqref{growth} holds for all $t \geq t_0$ for some $t_0$. Then, the expected rejection time $\tau$ under $\H_1$ can be bounded as
\begin{equation}
\E[\tau] \lesssim  \frac{1}{\Delta^2}\left(
t_0 + \ln\frac{1}{\alpha}\right).
\end{equation}
\end{theorem}

\begin{proof}
\iffalse    
Previously, we have shown that, for any comparator $\theta_* \in \K$, we have
\begin{align} 
\mathrm{Regret}_T(\mathrm{FTRLBarrier})\leq 2 \eta \sum_{t=1}^T
\|\nabla\ell_t(\theta_t)\|_{\theta_t}^* + 
\frac{  R(\theta_*)-\min_{\theta\in \K} R(\theta) }{\eta}.
\end{align}


Assume that $\|\nabla\ell_t(\theta_t)\|_{\theta_t}^*\leq G$, with $G$ being a positive constant. Then,
\begin{align} 
\mathrm{Regret}_T(\mathrm{FTRLBarrier})\leq 2\eta T G + 
\frac{  R(\theta_*)-\min_{\theta\in \K} R(\theta) }{\eta}\\
\leq \sqrt{2TG\left(R(\theta_*)-\min_{\theta\in \K} R(\theta)\right)}
\end{align}
\fi

We recall the regret of FTRL+Barrier from Lemma~\ref{lem:FTRL_const}:
\begin{equation} 
\mathrm{Regret}_T(\mathrm{FTRLBarrier})\leq
\frac{t_0}{2} + \frac{16}{c'} \frac{1}{t_0}
+ 4R(u)\leq
\underbrace{ \frac{t_0}{2} + \frac{16}{c't_0} 
-\ln\frac{81}{256} }_{:=\Phi } , 
%\lesssim \frac{t_0}{2} + \frac{16}{c't_0} \label{eq:78},
\end{equation}
where $u:=\frac{-S_T}{2\left(Q_T+\lvert S_T\rvert\right)}$ 
and, the last one is because 
% approximation is because the term of $R(u)$ is a small constant:
%Thus, we ignore it for brevity. 
\begin{align*}
    R(u)&=-\ln\left(1+\frac{S_T}{2\left(Q_T+\lvert S_T\rvert\right)}\right)-\ln\left(1-\frac{S_T}{2\left(Q_T+\lvert S_T\rvert\right)}\right)\\
    &=-\ln\left(1-\frac{S_T^2}{4\left(Q_T+\lvert S_T\rvert\right)^2}\right)\qquad (\text{Since} \frac{S_T^2}{\left(Q_T+\lvert S_T\rvert\right)^2}\leq 1)\\
    &\leq -\ln(1-\frac{1}{4})\\
    &=-\ln\frac{3}{4}.
\end{align*}


%\textbf{3. Expected rejection time.}

Previously, we get the following guarantee on the wealth $W_t$, see (\ref{eq:gen_bound_wealth}):
\begin{align}
    W_T \geq{\exp\left(\frac{S_T^2}{4\left(Q_T+\lvert S_T\rvert\right)}-\mathrm{Regret}_T(u)\right)} \text{ for all } |u| \leq 1\notag.
\end{align}

If there is no constraint on time budget $t$, under the assumption that $H_1$ is true, we have
\begin{equation}
    \E[\tau] = \sum_{t=1}^{\infty} P(\tau > t) \leq \sum_{t=1}^{\infty} P\left(\ln(W_t) < \ln(1/\alpha)\right),
\end{equation}
where $\left\{\ln(W_t) < \ln(1/\alpha)\right\}$ is defined as $E_t$. Hence, we have
\begin{align}
    E_t &\subseteq \left\{\ln\left(\exp\left(\frac{S_t^2}{4\left(Q_t+\lvert S_t\rvert\right)}-\mathrm{Regret}_t(u)\right)\right) < \ln(1/\alpha) \right\}\notag\\
    \Rightarrow E_t &\subseteq \left\{{S_t^2} < 4\left(Q_t + |S_t|\right)\left(\ln\left(\frac{1}{\alpha}\right) + \ln\left(\exp\left(\mathrm{Regret}_t(u)\right)\right)\right)\right\} \quad\left(\text{Since }\left\lvert S_t\right\rvert=\left\lvert\sum_{s=1}^t g_s\right\rvert\leq \sum_{s=1}^t \left\lvert g_s\right\rvert\right)\notag\\
    &\subseteq \left\{{S_t^2} < 4\left(Q_t + \sum_{s=1}^t |g_s|\right)\left(\ln\left(\frac{1}{\alpha}\right) + \ln\left(\exp\left(\mathrm{Regret}_t(u)\right)\right)\right)\right\} .
    \label{eq:Et_bound}
\end{align}

We denote $V_t:=\sum_{s=1}^t \lvert g_i\rvert$ and then we can get the upper bound on $V_t$ and $Q_t$ respectively. Since $\lvert g_s \rvert$ for any $s$ are random variables in $[0,1]$, then we consider the Chernoff bound~\citep{harvey2023second}, 
\begin{equation}
    P\left(V_t > (1 + \delta)\cdot \E\left[V_t\right]\right) \leq \exp\left(-\frac{\delta^2}{3} \E\left[{V_t}\right]\right).
\end{equation}

We let the right-hand side equal to $1/t^2$ and thus $\delta$ is $\sqrt{6 \ln(t) / \E[V_t]}$. By definition, $|g_s| \leq 1\Rightarrow V_t = \sum_{s=1}^t |g_s|\leq t$. With a probability of at least $(1 - 1/t^2)$, we have
\begin{equation}
    {V_t} \leq \E\left[{V_t}\right] + \sqrt{6\E\left[{V_t}\right]\cdot\ln(t)} \leq t + \sqrt{6t\cdot\ln(t)} \leq 2t, \quad \forall t\geq 17. \label{eq:ten}
\end{equation}

Similarly, as for $Q_t=\sum_{s=1}^t g_s^2$, we know $g_s^2 \leq 1$. Then $Q_t$ is the sum of independent random variables in $[0,1]$. After applying the Chernoff bound~\citep{harvey2023second}, we have that with a probability of at least $1 - 1/t^2$,
\begin{equation}
    {Q_t} \leq \E\left[{Q_t}\right] + \sqrt{6\E\left[{Q_t}\right]\cdot\ln(t)} \leq t + \sqrt{6t\cdot \ln(t)} \leq 2t, \quad \forall t \geq 17. \label{eq:eleven}
\end{equation}
Let $H_t = \left\{{Q_t}  \leq 2t\right\} \cap \left\{{V_t}\leq 2t\right\}$. Then, by (\ref{eq:Et_bound}),
\begin{align}
    E_t \cap H_t &\subseteq \left\{{S_t^2} < 4\cdot\left(2t + 2t\right)\left(\ln\left(\frac{1}{\alpha}\right) + \ln\left(\exp\left(\mathrm{Regret}_t(u)\right)\right)\right)\right\} \notag\\
    &\subseteq \left\{\left\lvert{S_t}\right\rvert < \underbrace{ \sqrt{{16t}\cdot\left(\ln\left(\frac{1}{\alpha}\right) + \ln\left(\exp\left(\mathrm{Regret}_t(u)\right)\right)\right)}}_{:=R_t}\right\}.
    \label{eq:nine}
\end{align}

Since $S_t=\sum_{s=1}^t g_s$ is the sum of independent random variables in $[-1, 1]$, applying a Hoeffding bound~\citep{harvey2023second} gives
\begin{equation}
    P\left(\left\lvert{S_t} - \E\left[{S_t}\right]\right\rvert \geq u\right) \leq 2 \exp\left(\frac{-u^2} {2t}\right).
\end{equation}

We still let RHS be $1/t^2$ to get $u=\sqrt{2t\cdot\ln(2t^2)}$. With a probability of at least $(1 - 1/t^2)$ and according to the reverse triangle inequality, we have
\begin{equation}
    \left\lvert\left\lvert {S_t}\right\rvert - \left\lvert \E\left[{S_t}\right]\right\rvert\right\rvert \leq \left\lvert{S_t} - \E\left[{S_t}\right]\right\rvert \leq \sqrt{2t\cdot\ln(2t^2)}.
\end{equation}

This implies that,
\begin{align}
    \left\lvert{S_t}\right\rvert \geq \left\lvert \E\left[{S_t}\right]\right\rvert - \sqrt{2t\cdot\ln (2t^2)} = {t\Delta} - \sqrt{2t\cdot\ln(2t^2)}\geq {t\Delta} - \sqrt{4t\cdot\ln (2t)},
    \label{eq:inequa}
\end{align}
where $\Delta=\lvert\mu_x-\mu_y\rvert$. The above inequality (\ref{eq:inequa}) is given by the fact that 
\begin{align}
    \left\lvert \E\left[{S_t}\right]\right\rvert={\left\lvert \E\left[\sum_{s=1}^t g_s\right]\right\rvert}\notag={ \left\lvert\sum_{s=1}^t \E\left[{x}_t-{y}_t\right]\right\rvert}\notag={\left\lvert t(\mu_x-\mu_y)\right\rvert}\notag={ t\left\lvert \mu_x-\mu_y\right\rvert}\notag.
\end{align}

In the following, we show
${t\Delta} - \sqrt{4t\cdot\ln (2t)}\geq R_t$ for all $t \geq t_*$, 
%, where
%$$t_*:= \frac{32}{\Delta^2}\ln\left(\frac{1}{\alpha}\exp\left(\mathrm{Regret}_t(u)\right)\right),$$ 
where $R_t$ is defined in $(\ref{eq:nine})$
and $t_*$ will be determined soon.

We have
\begin{align}
    {t\Delta} - \sqrt{4t\cdot\ln(2t)}&\geq \sqrt{{16t}\cdot\left(\ln\left(\frac{1}{\alpha}\right) + \ln\left(\exp\left(\mathrm{Regret}_t(u)\right)\right)\right)}\notag\\
    {t\Delta}&\geq \sqrt{4t\cdot\ln(2t)} +\sqrt{{16t}\cdot\left(\ln\left(\frac{1}{\alpha}\right) + \ln\left(\exp\left(\mathrm{Regret}_t(u)\right)\right)\right)}.
    \label{eq:hard}
\end{align}
A sufficient condition of the above is
\begin{equation}
    {t\Delta} \geq 2 \sqrt{ 16 t  \max \left \{  \ln (2t), \ln \left( \frac{\exp\left(\Phi)\right) }{ \alpha} \right )    \right \} }. \label{81}
\end{equation}

Let us consider two cases.
Case (i) is when $\exp(\Phi)/\alpha$ dominates $2t$, while Case (ii) is when $2t$ dominates the $\exp(\Phi)/\alpha$.

For Case (i), a sufficient condition of \myeqref{81} is
\begin{equation}
 {t\Delta} \geq 2 \sqrt{ 16 t    \ln \left( \frac{\exp\left(\Phi \right) }{ \alpha} \right )   }
\iff
t \geq 64 \frac{\ln \left( \frac{ \exp\left(\Phi\right) }{\alpha}  \right) }{\Delta^2}. 
\end{equation}

For Case (ii), a sufficient condition of \myeqref{81} is
\begin{equation}
t \Delta \geq 2 \sqrt{ 16 t \ln (2t)},
\end{equation}
which can be guaranteed for all 
$t \geq \frac{1}{2} \exp\left(- W_{-1}(-\Delta^2/128) \right) = \tilde{\Theta} \left( \frac{1}{\Delta^2} \right)$,
where $W_{-1}$ is the lower branch of the Lambert function. 
Hence, a sufficient condition for both cases is
\begin{equation}
t \gtrsim \underbrace{ \frac{1}{\Delta^2}
\left(
 \ln\left( \frac{1}{\alpha} \right)+ \Phi
 \right) 
}_{:=t_*}.
\end{equation}


Hence, when $t\geq t_*$, we have the guarantee ${t\Delta} - \sqrt{4t\cdot\ln (2t)}\geq R_t$ with probability at least $1-1/t^2$.  Specifically, 
we can write
\begin{equation}
    \left\lvert{S_t}\right\rvert \geq R_t,\qquad \text{ if } 
    t \gtrsim t_*.
    \label{eq:twelve}
\end{equation}

Now, by the law of total probability, for $t$ large enough such that inequalities (\ref{eq:ten}), (\ref{eq:eleven}), and (\ref{eq:twelve}) all hold:
\begin{align*}
    P(E_t) &= P(E_t \cap H_t) + P(E_t \cap H_t^c)\\
    &=P(E_t \cap H_t) + P(E_t| H_t^c)P(H_t^c)\\
    &\leq P\left(\left\lvert{S_t}\right\rvert < R\right) + P(H_t^c)  \quad (\text{by }(\ref{eq:nine}))\\
    &= \left(1-P\left(\left\lvert{S_t}\right\rvert \geq R\right)\right) + \left(1-P(H_t)\right)\\
    &= \left(1-P\left(\left\lvert{S_t}\right\rvert \geq R\right)\right) + P\left(\left\{{Q_t}>2t\right\}\cup\left\{{V_t}>2t\right\}\right) \qquad \quad  (\text{by definition of }H_t)\\
    &\leq \left(1-P\left(\left\lvert{S_t}\right\rvert \geq R\right)\right) + P\left({Q_t}>2t\right)+P\left({V_t}>2t\right)\\
    &\leq\frac{1}{t^2}+\frac{1}{t^2}+\frac{1}{t^2} \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad  \qquad \qquad \qquad (\text{by }\ref{eq:ten}, \ref{eq:eleven}, \ref{eq:twelve}) \\
    &\leq \frac{3}{t^2}.
\end{align*}

Now we can conclude that when $t$ is larger enough such tht $t \gtrsim t_*$,
%Now we can conclude that when $t$ is large enough such that $t \gtrsim t_*$，
%\end{equation}
%we have
\begin{equation}
    \E[\tau] \leq \sum_{t=1}^{\infty} P(E_t)=t_*+ \sum_{t\geq t_*}P(E_t)\leq t_*+ \sum_{t=t_*}^{\infty} \frac{3}{t^2} \lesssim  
\frac{1}{\Delta^2}
\left( \ln\left( \frac{1}{\alpha} \right)+\Phi \right)
+ \frac{\pi^2}{2}
\lesssim
\frac{1}{\Delta^2}
\left( \ln\left( \frac{1}{\alpha} \right)+t_0 \right)
+ \frac{\pi^2}{2}.
\end{equation}

The proof is now completed.


\end{proof}


\section{Proof of lemmas and theorems in Subsection~\ref{section:main2}} \label{app:main2}
\begin{lemma}\label{lem:OptimisticFTRL}
Following the same assumptions and conditions in Lemma~\ref{lem:FTRL},
Optimistic-FTRL+Barrier (Algorithm~\ref{alg:OptimisticFTRL}) has
$\mathrm{Regret}_T(\theta_*)
\leq
2\eta\sum_{t=1}^T\|\nabla \ell_t (\theta_t)-m_t(\theta_t)\|_{\theta_t}^* \|\nabla\ell_t (\theta_t)\|_{\theta_t}^*+ \frac{R( \theta_*) - \min_{\theta \in \K} R(\theta)}{\eta} ,$
where $\theta_* \in \K$ is any comparator.
\end{lemma}

When Optimistic-FPRL is used as OAlg method, we will choose the fraction via $\theta_t := \arg\min_{\theta \in \K} \eta\langle m_{t}(\theta_{t-1}),\theta\rangle  + \eta\sum_{s=1}^{t-1} \langle\nabla\ell_{s}(\theta_s), \theta\rangle  +  {R(\theta)},$ for each round $t$, then we will first show the following regret bound.

\begin{lemma}{(Lemma 3 in \citet{wang2024no}.)}\label{lem:3.5_pre}
    Given $\eta > 0$ and a $\beta$-strongly convex $R: \K \rightarrow \mathbb{R}$, assume we have a sequence of loss functions $\{\ell_t(\cdot)\}_{t=1}^T$ such that each $\ell_t(\cdot)$ is $\mu_t$-strongly convex for some $\mu_t \geq 0$. Let $m_1, \dots, m_T: \K \rightarrow \mathbb{R}$ be the sequence of hint functions given to OPTIMISTICFTRL, where each $m_t(\cdot)$ is $\hat\mu_t$-strongly convex function over $\K$. Then OPTIMISTICFTRL[$R(\cdot), \eta$] satisfies

\begin{align}
  \mathrm{Regret}_T( \theta_*) &\leq \sum_{t=1}^T \langle \nabla \ell_t (\theta_t)-m_t(\theta_t), \theta_t-w_{t+1}\rangle + \frac{R( \theta_*) - \min_{\theta \in \K} R(\theta)}{\eta}\notag\\
  &- \frac{1}{2} \sum_{t=1}^T \left(\frac{\beta}{\eta} \right) \| \theta_t - w_t\|^2- \frac{1}{2} \sum_{t=1}^T \left(\frac{\beta}{\eta} \right) \| \theta_t - w_{t+1}\|^2\label{eq:3.5_pre}
\end{align}

where  $w_1, \dots, w_T$ is the alternative sequence chosen according to FTRL[$R(\cdot), \eta$], 
$w_t := \arg\min_{ \theta \in \K} \eta\sum_{s=1}^t \langle \nabla\ell_t( \theta_t),\theta\rangle + R( \theta)$, and $\theta_* \in \K$ is any benchmark.
\end{lemma} 

\iffalse
\begin{proof}[\text{Proof of Lemma~\ref{lem:3.5_pre}}] We can rewrite the regret as
\begin{align}
\mathrm{Regret}_T( \theta_*) &:= \sum_{t=1}^T \ell_t( \theta_t) - \sum_{t=1}^T \ell_t( \theta_*) \\
&\leq \sum_{t=1}^T \langle\nabla\ell_t( \theta_t),\theta_t\rangle - \sum_{t=1}^T \langle\nabla \ell_t( \theta_t), \theta_*\rangle \\
&= \sum_{t=1}^T \langle \nabla \ell_t (\theta_t)-m_t(\theta_t), \theta_t-w_{t+1}\rangle + \sum_{t=1}^T \langle m_t(\theta_t), \theta_t-w_{t+1}\rangle + \sum_{t=1}^T \langle \nabla \ell_t(\theta_t), w_{t+1}-\theta_*\rangle
\end{align}

In the following, we denote
\begin{equation}
D_T := \frac{1}{2} \left( \sum_{t=1}^T \frac{\beta}{\eta}  \| \theta_t - w_t\|^2+\sum_{t=1}^T \frac{\beta}{\eta} \| \theta_t - w_{t+1}\|^2 \right)
\end{equation}
for brevity, and define $D_0 := 0$. We will use induction to show that
\begin{equation}
\sum_{t=1}^T \langle m_t(\theta_t), \theta_t-w_{t+1}\rangle + \sum_{t=1}^T \langle \nabla \ell_t(\theta_t), w_{t+1}-\theta_*\rangle \leq \frac{R( \theta_*) - R(w_1)}{\eta} - D_T
\label{eq:26}
\end{equation}

For the base case $T = 0$, we have $\sum_{t=1}^0\langle m_t( \theta_t), \theta_t- w_{t+1}\rangle+\sum_{t=1}^0 \langle \nabla \ell_t( \theta_t), w_{t+1}-\theta_*\rangle = 0 \leq \frac{1}{\eta} (R( \theta_*) - R(w_1)) - 0$, as $w_1 := \arg\min_{ \theta \in \K} R( \theta)$. So the base case trivially holds.

Let us assume that the inequality (\ref{eq:26}) holds for $t = 0, 1, \ldots, T-1$. Now consider round $T$. We have
\begin{align}
&\sum_{t=1}^T \langle  m_t, \theta_t-w_{t+1}\rangle + \sum_{t=1}^T \langle \nabla \ell_t, w_{t+1}\rangle\notag\\
&\leq   (\langle m_T, \theta_t\rangle - \langle m_T, w_{T+1}\rangle) + \langle\nabla\ell_T, w_{T+1}\rangle) + \frac{R(w_T) - R(w_1)}{\eta}- D_{T-1}+\sum_{t=1}^{T-1} \langle \nabla \ell_t, w_T\rangle\label{a}\\
&\leq   (\langle m_T, \theta_t\rangle - \langle m_T, w_{T+1}\rangle)) + \langle\nabla\ell_T, w_{T+1}\rangle)) + \frac{R( \theta_t) - R(w_1)}{\eta} - D_{T-1} \notag\\
&\quad - \frac{1}{2} \left(\frac{\beta}{\eta} \right) \| \theta_t - w_T\|^2 + \sum_{t=1}^{T-1}  \langle \nabla \ell_t, \theta_t\rangle\label{b}\\
&=   (\langle\nabla\ell_T, w_{T+1}\rangle - \langle m_T, w_{T+1}\rangle) + \frac{R( \theta_t) - R(w_1)}{\eta} - D_{T-1} \notag\\
&\quad - \frac{1}{2} \left(\frac{\beta}{\eta} \right) \| \theta_t - w_T\|^2 + \sum_{t=1}^{T-1}  \langle \nabla \ell_t, \theta_t\rangle+\langle m_T, \theta_t\rangle \notag\\
&\leq   (\langle\nabla\ell_T, w_{T+1}\rangle - \langle m_T, w_{T+1}\rangle) + \frac{R(w_{T+1}) - R(w_1)}{\eta} - D_{T-1}\notag\\
&\quad - \frac{1}{2} \left(\frac{\beta}{\eta}\right) \| \theta_t - w_T\|^2- \frac{1}{2} \left(\frac{\beta}{\eta}\right) \| \theta_t - w_{T+1}\|^2 + \sum_{t=1}^{T-1}  \langle \nabla \ell_t, w_{T+1}\rangle+\langle m_T, w_{T+1}\rangle \label{c}\\
&= \sum_{t=1}^T \langle \nabla\ell_t, w_{T+1}\rangle + \frac{R(w_{T+1}) - \min_{\theta \in \K} R(\theta)}{\eta} - D_T\notag\\
&\leq \sum_{t=1}^T  \langle \nabla \ell_t,  \theta_*\rangle + \frac{R( \theta_*) - \min_{\theta \in \K} R(\theta)}{\eta} - D_T\label{d},
\end{align}
where (\ref{a}) we use the induction such that the inequality (\ref{eq:26}) holds for any $ \theta_* \in \K$ including $ \theta_* = w_T$, and (\ref{b}) is because
\begin{equation*}
    \sum_{t=1}^{T-1}  \langle \nabla \ell_t, w_T\rangle + \frac{R(w_T)}{\eta}  \leq \sum_{t=1}^{T-1}  \langle \nabla \ell_t, \theta_t\rangle + \frac{R( \theta_t)}{\eta}  - \frac{1}{2} \left( \frac{\beta}{\eta} \right) \| \theta_t - w_T\|^2, 
\end{equation*}
as $w_T$ is the minimizer of a strongly convex function since 
$w_T := \arg\min_{ \theta \in \K} \left(\sum_{t=1}^{T-1}\langle \nabla \ell_t, \theta\rangle + \frac{1}{\eta} R( \theta)\right)$
, and (\ref{c}) is because
\begin{align*}
\sum_{t=1}^{T-1}   (\langle\nabla\ell_t, \theta_t\rangle +  \langle m_T, \theta_t\rangle) + \frac{1}{\eta} R( \theta_t) &\leq \sum_{t=1}^T \langle \nabla\ell_t, w_{T+1}\rangle +  \langle m_T, w_{T+1}\rangle + \frac{1}{\eta} R(w_{T+1}) \\
&\quad - \frac{1}{2} \left( \frac{\beta}{\eta} + \sum_{s=1}^{T-1} \alpha_s \mu_s +   \mu_T \right) \| \theta_t - w_{T+1}\|^2,
\end{align*}
as $ \theta_t$ is the minimizer of a strongly convex function since
\begin{equation}
 \theta_t := \arg\min_{ \theta \in \K} \left(\sum_{s=1}^{T-1} \langle \nabla\ell_s, \theta\rangle +  \langle m_T, \theta\rangle + \frac{1}{\eta} R( \theta)\right),
\end{equation}
and (\ref{d}) is due to $w_{T+1} := \arg\min_{ \theta \in \K} \left(\sum_{t=1}^T   \langle \nabla\ell_t, \theta\rangle + \frac{1}{\eta} R( \theta)\right).
$
\end{proof}


With this lemma, we can prove Lemma \ref{lem:OptimisticFTRL}.

\begin{proof}
    According to the above lemma, we can get a looser upper bound by ignoring the last two positive terms of (\ref{eq:3.5_pre}):
\begin{align}
  \mathrm{Regret}_T( \theta_*) &\leq \sum_{t=1}^T \langle \nabla \ell_t (\theta_t)-m_t(\theta_t), \theta_t-w_{t+1}\rangle + \frac{R( \theta_*) - \min_{\theta \in \K} R(\theta)}{\eta}\\
  &\leq\sum_{t=1}^T\|\nabla \ell_t (\theta_t)-m_t(\theta_t)\|_{\theta_t}^* \|\theta_t - w_{t+1}\|_{\theta_t}+ \frac{R( \theta_*) - \min_{\theta \in \K} R(\theta)}{\eta}\label{holder}\\
  &\leq2\eta\sum_{t=1}^T\|\nabla \ell_t (\theta_t)-m_t(\theta_t)\|_{\theta_t}^* \|\nabla\ell_t (\theta_t)\|_{\theta_t}^*+ \frac{R( \theta_*) - \min_{\theta \in \K} R(\theta)}{\eta}\label{eq:90}
\end{align}

Then, according to H\"older's inequality, we have (\ref{holder}). We recall that $w_t := \arg\min_{ \theta \in \K} \sum_{s=1}^t \langle \nabla\ell_s, \theta\rangle + \frac{1}{\eta} R( \theta)$, and denote that $F_t(\theta):=\eta\sum_{s=1}^t \langle \nabla\ell_{s}, \theta\rangle + R(\theta)$, which is a self-concordant function, thus $w_{t+1}:=\arg\min_{\theta\in \K} F_t(\theta)$. According to (\ref{eq:norm_newton}), with the assumption that $\| \nabla F_t(\theta_t) \|_{\theta_t}^*\leq \frac{1}{4}$,  we have:
\begin{equation}
\|\theta_t - w_{t+1}\|_{\theta_t} = \|\theta_t - \arg\min_{\theta\in \K} F_t(\theta)\|_{\theta_t} \leq  2\| \nabla F_t(\theta_t) \|_{\theta_t}^*,
\label{eq:51}
\end{equation}

Since $ w_t=\arg\min_{\theta\in \K} F_{t-1}(\theta) $, and we know $ F_t(\theta) = F_{t-1}(\theta) + \eta \ell_t(\theta) \Rightarrow \nabla F_t(\theta_t) = \eta \nabla\ell_t(\theta_t)$. Plugging it in (\ref{eq:51}) gives (\ref{eq:90}).  This completes the proof.

\end{proof}
\fi

\begin{remark}\label{remark:c.3}
We recall the definition $\|f\|_\theta^* = \sqrt{f^T (\nabla^2 C(\theta))^{-1} f}$:
\begin{equation}
    \|\theta_t - w_{t+1}\|_{\theta_t} \leq 2\eta\|\nabla\ell_t(\theta_t)\|_{\theta_t}^*=2\eta\sqrt{(\nabla\ell_t(\theta_t))^2 (\nabla^2 F_t(\theta_t))^{-1}}=2\eta\sqrt{(\nabla\ell_t(\theta_t))^2 (\nabla^2 R(\theta_t))^{-1}},
    \label{eq:52}
\end{equation}
where we have $\nabla^2 F_t(\theta)=\nabla^2 R(\theta)$ as mentioned previously. Hence,

\begin{align}
  \mathrm{Regret}_T( \theta_*) &\leq2\eta\sum_{t=1}^T\|\nabla \ell_t (\theta_t)-m_t(\theta_t)\|_{\theta_t}^* \|\nabla\ell_t (\theta_t)\|_{\theta_t}^*+ \frac{R( \theta_*) -\min_{\theta \in \K} R(\theta)}{\eta}\\
  &=2\eta \sum_{t=1}^T\sqrt{\left(\nabla\ell_t(\theta_t)-m_t( \theta_t)\right)^2(\nabla^2 R(\theta_t))^{-1}}\cdot\sqrt{(\nabla\ell_t(\theta_t))^2 (\nabla^2 R(\theta_t))^{-1}}
  + \frac{R( \theta_*) - \min_{\theta \in \K} R(\theta)}{\eta}\label{def_dual}\\
  &=2\eta\sum_{t=1}^T\left|\nabla\ell_t(\theta_t)-m_t( \theta_t) \right| \cdot \left|\nabla\ell_t(\theta_t)\right|(\nabla^2 R(\theta_t))^{-1}+ \frac{R( \theta_*) -\min_{\theta \in \K} R(\theta)}{\eta}\label{eq:50}
\end{align}


By applying again the the definition of dual-norm and consider (\ref{eq:52}), we can get (\ref{def_dual}). Simplifying it gives (\ref{eq:50}), with the condition that $ \eta \|\nabla\ell_t\|_{\theta_t}^*= \eta\sqrt{(\nabla\ell_t(\theta_t))^2 (\nabla^2 R(\theta_t))^{-1}}\leq \frac{1}{4} $. We let the guess $m_t(\cdot)$ at $t$ be the previous
loss function $ m_t(\cdot) \gets \ell_{t-1}(\cdot) $. Thus, we get 
\begin{align} 
\mathrm{Regret}_T(\mathrm{FTRLOptimistic})\leq
2\eta\sum_{t=1}^T\left|\nabla\ell_t(\theta_t)-\nabla \ell_{t-1}( \theta_t) \right| \cdot \left|\nabla\ell_t(\theta_t)\right|(\nabla^2 R(\theta_t))^{-1}\notag
+ \frac{R( \theta_*) - \min_{\theta \in \K} R(\theta)}{\eta},
\end{align}

We recall the regret bound for FTRL-Barrier, which is 
\begin{align} 
\mathrm{Regret}_T(\mathrm{FTRLBarrier})&\leq
2 \eta \sum_{t=1}^T
(\nabla \ell_t(\theta_t))^2 ( \nabla^2 R(\theta_t) )^{-1} + 
\frac{  R(\theta_*)-\min_{\theta \in \K} R(\theta)}{\eta}\notag\\
&= 2 \eta \sum_{t=1}^T
|\nabla \ell_t(\theta_t)|\cdot |\nabla \ell_t(\theta_t)| ( \nabla^2 R(\theta_t) )^{-1} + 
\frac{  R(\theta_*)-\min_{\theta \in \K} R(\theta) }{\eta}.
\end{align}

Thus, if $\left|\nabla\ell_t(\theta_t)-\nabla \ell_{t-1}( \theta_t) \right|\leq \left|\nabla\ell_t(\theta_t) \right|$, Optimistic-FTRL+Barrier has a lower regret bound than that of FTRL+Barrier.


\end{remark}


\begin{theorem} %\label{thm:main2}
Algorithm~\ref{alg:Betting} with $\mathrm{OAlg}$ being Optimistic-FTRL+Barrier (Algorithm~\ref{alg:OptimisticFTRL}) is a level-$\alpha$ sequential test with asymptotic power one. Furthermore, denote $\Delta:= | \mu_y - \mu_x |$ 
and assume that the linear growth condition \myeqref{growth} holds at all $t\geq t_0$ for some $t_0$. 
The expected rejection time $\tau$ against the null $\H_0$ under $\H_1$ satisfies: 
\begin{equation}
\E[\tau] \lesssim \frac{1}{\Delta^2} \left(
\zeta t_0+\ln\frac{1}{\alpha}\right),
\end{equation}
where $\zeta:=\max\limits_{t} \frac{\left|\nabla \ell_t(\theta_t)-\nabla\ell_{t-1}(\theta_t)\right|}{\left|\nabla \ell_t(\theta_t)\right|}$.
\end{theorem}


\begin{proof}
We will first show that Optimistic-FTRL+Barrier can also achieve a constant regret under the linear growth condition.

\begin{align} 
&\mathrm{Regret}_T(\mathrm{FTRLOptimistic})\\
&{\leq}
2\eta\sum_{t=1}^T\left|\nabla\ell_t(\theta_t)-\nabla \ell_{t-1}( \theta_t) \right| \cdot \left|\nabla\ell_t(\theta_t)\right|(\nabla^2 R(\theta_t))^{-1}\notag+ \frac{R( \theta_*) - \min_{w\in K} R(w)}{\eta}\notag\\
&\overset{(i)}\leq 2 \eta \left(\sum_{t=1}^{t_0}
\left|\nabla\ell_t(\theta_t)-\nabla \ell_{t-1}( \theta_t) \right||\nabla \ell_t(\theta_t)| ( \nabla^2 R(\theta_t) )^{-1} + \sum_{t=t_0+1}^{T}
\left|\nabla\ell_t(\theta_t)-\nabla \ell_{t-1}( \theta_t) \right||\nabla \ell_t(\theta_t)| ( \nabla^2 R(\theta_t) )^{-1}\right)+
\frac{  R(u)  }{\eta}\notag,
\\ 
&\overset{(ii)}
{\leq} 2 \eta \sum_{t=1}^{t_0} \zeta \left|\nabla\ell_t(\theta_t)\right|^2(\nabla^2 R(\theta_t))^{-1}  + 
2\eta\sum_{t=t_0+1}^{T} \zeta \left|\nabla\ell_t(\theta_t)\right|^2(\nabla^2 R(\theta_t))^{-1}
+
\frac{  R(u)  }{\eta},
\\ &\overset{(iii)}
{\leq} 2 \eta \sum_{t=1}^{t_0} \frac{\zeta}{16\eta^2}  + 
2\eta\zeta\sum_{t=t_0+1}^{T}  \left(\nabla\ell_t(\theta_t)\right)^2(\nabla^2 R(\theta_t))^{-1}
+
\frac{  R(u)  }{\eta}\label{eq:3_because}
\\&\overset{(iv)}
\leq \sum_{t=1}^{t_0} \frac{\zeta}{8\eta} + 
2\eta\zeta\sum_{t=t_0+1}^{T} \frac{2}{c'^2\eta^2(t-1)^2}
+
\frac{  R(u)  }{\eta}\notag
\\ 
&\leq \frac{\zeta t_0}{8\eta} + 
\frac{4\zeta}{c'^2 \eta}\sum_{t=t_0+1}^{T}\frac{1}{(t-1)^2}+ \frac{  R(u)  }{\eta} \notag\\
&= \frac{\zeta t_0}{8\eta} + \frac{4\zeta}{c'^2\eta} \left( \frac{1}{t_0} - \frac{1}{T-1} \right )
+ \frac{  R(u)  }{\eta}\notag\\
&\leq \frac{1}{\eta}\left(\frac{\zeta t_0}{8} + \frac{4\zeta}{c'^2t_0} 
+ R(u)\right)\notag,
\end{align}

where (i) holds because for any benchmark $\theta_*$, we have $\frac{R( \theta_*) - \min_{w\in K} R(w)}{\eta}\leq\frac{R( \theta_*)}{\eta}$. For (ii), define $\zeta :=\max_{t}{\frac{\left|\nabla\ell_t(\theta_t)-\nabla\ell_{t-1}(\theta_t)\right|}{  \left|\nabla\ell_t(\theta_t)\right|}}$, which implies that for all $t$, $\left|\nabla\ell_t(\theta_t)-\nabla\ell_{t-1}(\theta_t)\right|\leq \zeta\left|\nabla\ell_t(\theta_t)\right|$. Recall the condition $\eta \|\nabla\ell_t\|_{\theta_t}^*= \eta\sqrt{(\nabla\ell_t)^2 (\nabla^2 R(\theta))^{-1}}\leq \frac{1}{4}$, hence ${(\nabla\ell_t)^2 (\nabla^2 R(\theta))^{-1}}\leq \frac{1}{16\eta^2} $, from which we derive (iii). Previously, we have shown that when $t\geq t_0$, $\left((\nabla\ell_t(\theta_t)\right)^2(\nabla^2 R(\theta_t))^{-1}$ can be bounded by $\frac{2}{c'^2\eta^2(t-1)^2}$, see \myeqref{eq:norm_bound}, thus establishing (iv). Furthermore, we recall that  $\zeta<1$ is easily satisfied for distributions with disjoint supports and small variances, as seen in Example 4.


If we let $\eta=\frac{1}{4}$, the regret for Optimistic-FTRL+Barrier is:
\begin{equation} 
\mathrm{Regret}_T(\mathrm{FTRLBarrier})\leq
\frac{\zeta t_0}{2} + \frac{16\zeta }{c'^2} \frac{1}{t_0}
+ 4R(u)\leq
\frac{\zeta t_0}{2} + \frac{16\zeta }{c'^2t_0} 
-\ln\frac{81}{256}\
%lesssim \frac{\zeta t_0}{2} + \frac{16\zeta}{c't_0}  
\label{eq:constant_reg_ftrl},
\end{equation}
where $u=\frac{-S_T}{2\left(Q_T+\lvert S_T\rvert\right)}$, and $t_0$ is the time that $\left | \sum_{s=1}^t \nabla \ell(\theta_s)   \right| \geq c t, \forall t \geq t_0$, $c>0$ is a constant.

The analysis of the expected rejection time follows the same lines as those for proving Theorem~\ref{thm:main1}, except that we let $t_* := \frac{1}{\Delta^2} \left( \ln\left( \frac{1}{\alpha} \right)+ \zeta t_0 \right)
$ here.

The proof is now completed.
\end{proof}

\section{Experiments of Synthetic Datasets}
\label{app:add_exp_syn}
\textbf{Setup.}
Each sequence of $x_t$ and $y_t$ contains $500$ samples, and each test under a specified significance level $\alpha$ will be conducted over 300 runs to get the average results under both $H_0$ and $H_1$ scenarios.

\text{(Distributions with disjoint supports.)} Consider one observes two sequences of samples that are drawn from two uniform distributions with small variances. When the null hypothesis holds, both sequences originate from identical distributions::
\begin{align}
    &\{x_t\}_{t\geq 1}\sim \text{U}(a_1, b_1)  \qquad a_1=0.799, b_1=0.801,\\
    &\{y_t\}_{t\geq 1}\sim \text{U}(a_2, b_3)  \qquad a_2=0.799, b_2=0.801.
\end{align}
When the alternative hypothesis is true, $x_t$ and $y_t$ come from two distributions with disjoint supports:
\begin{align}
    &\{x_t\}_{t\geq 1}\sim \text{U}(a_1, b_1)  \qquad a_1=0.799, b_1=0.801,\\
    &\{y_t\}_{t\geq 1}\sim \text{U}(a_2, b_3)  \qquad a_2=0.199, b_2=0.201.
\end{align}


\text{(Distributions with overlapping supports; high signal-
to-noise ratio.)} Consider $x_t$ and $y_t$ are drawn from normal distributions with high signal-to-noise ratio $\left( \text{i.e.,} \frac{(\mu_x-\mu_y)^2}{\sigma_x^2+\sigma_y^2}\right)$. Under $H_0$, sequence $\{y_t\}$ comes from the same distribution as $\{x_t\}$, 
\begin{align}
    &\{x_t\}_{t\geq 1}\sim \mathcal{N}(\mu_1, \sigma_1)  \qquad \mu_1=0.30, \sigma_1=0.01,\\
    &\{y_t\}_{t\geq 1}\sim \mathcal{N}(\mu_2, \sigma_2)  \qquad \mu_2=0.30, \sigma_2=0.01.
\end{align}
Under $H_1$, samples $y_t$ are drawn from a distribution that has slight overlap with the distribution of $x_t$:
\begin{align}
    &\{x_t\}_{t\geq 1}\sim \mathcal{N}(\mu_3, \sigma_3)  \qquad \mu_3=0.30, \sigma_3=0.01,\\
    &\{y_t\}_{t\geq 1}\sim \mathcal{N}(\mu_4, \sigma_4)  \qquad \mu_4=0.35, \sigma_4=0.01.
\end{align}


\text{(Time-varying distributions with mean shift.)} We consider a synthetic scenario where the distribution shift for the observed sequence $\{y_t\}$. The null hypothesis posits the distribution of $y_t$ remains the same over time:
\begin{align}
    &\{x_t\}_{t\geq1}\sim \mathcal{N}(\mu_1, \sigma_1)  \qquad \mu_1=0.30, \sigma_1=0.01,\\
    &\{y_t\}_{t\geq1}\sim \mathcal{N}(\mu_2, \sigma_2)  \qquad \mu_2=0.30, \sigma_2=0.01.
\end{align}
For $H_1$ scenario, we let the sequence of ${y_t}$ undergo a mean shift at the time point $\hat{t}=300$:
\begin{align}
    &\{x_t\}_{t\geq1}\sim \mathcal{N}(\mu_3, \sigma_3)  \qquad \mu_3=0.30, \sigma_3=0.01,\\
    &\{y_t\}_{t<\hat{t}}\sim \mathcal{N}(\mu_4, \sigma_4)  \qquad \mu_4=0.30, \sigma_4=0.01,\\
    &\{y_t\}_{t\geq\hat{t}}\sim \mathcal{N}(\mu_5, \sigma_5)  \qquad \mu_5=0.35, \sigma_5=0.01.
\end{align}

\textbf{Additional experiment results.} 
\text{(Distributions with overlapping supports; high signal-to-noise ratio.)} In this scenario, the condition $|\sum_{s=1}^t (x_s - y_s)| \geq ct$ can be satisfied with high probability, which facilitates faster rejection of $H_0$ when using FTRL+Barrier and Optimistic-FTRL+Barrier than ONS see Figure \ref{fig:syn2}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/syn2.png}
    \caption{Comparisons of three different OAlg methods under $H_0$ and $H_1$ scenarios when 
   $\rho_x$ and $\rho_y$ have overlapping supports. The left subfigure shows the average time over 300 runs for Algorithm \ref{alg:Betting} equipped with different OAlg methods to reject $H_0$ when $H_1$ holds versus the false positive rates (FPRs) under each value of the significance level ($\alpha$). The plots closer to the bottom left are more desirable. The dashed line and shaded area in the right plot represent the desired significance levels.}
    \label{fig:syn2}
\end{figure}

Because the difference in mean values, $\Delta=|\mu_x-\mu_y|$, is smaller than that in the first synthetic scenario, it takes a longer time under $H_1$ to reject $H_0$. 
Moreover,  FTRL+Barrier shows performance comparable to that of Optimistic-FTRL+Barrier, as $\zeta$ might not be very small in this case.


\section{Experiments of Detecting LLM-generated Texts}
\label{app:add_exp_detect}
\textbf{Setup.}
We use a publicly available dataset of \citet{chen2024online}, which consists of scores for both machine-generated and human-written texts. The scoring configuration employed is the score function of Fast-DetectGPT, and the scoring model GPT-Neo-2.7B. The score difference, denoted by $g_t=|x_t - y_t|$, falls within the interval $[-d, d]$, where $d \geq 1$. Moreover, since human-written texts in real-world scenario might be written by different individuals, there could be a small variance $\epsilon$ between $\mu_x$ and $\mu_y$ under the null hypothesis. Thus, we will consider using the composite hypotheses setting, i.e., $H_0: | \mu_x - \mu_y | \leq \epsilon $ versus~$H_1: | \mu_x - \mu_y | >\epsilon$. In this case, ONS restricts the betting fraction $\theta_t$ to $[-\frac{1}{2d}, 0]$, whereas our methods utilize the entire range $[-\frac{1}{d}, 0]$ to facilitate more aggressive betting strategies and to avoid explosion issues.

\textbf{Experiment Results.}
Figures \ref{fig:all_frequency} show the frequency distributions of scores for human-written news and fake Olympic news generated by Gemini-1.5-Flash, Gemini-1.5-Pro, and PaLM 2. The difference in means between the scores for human-written texts and machine-generated texts is largest for PaLM 2 and smallest for Gemini-1.5-Pro. Additionally, the signal-to-noise ratio for Gemini-1.5-Pro is notably low. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/all_frequency.png}
    \caption{Frequency distribution of scores for fake news generated by Gemini-1.5-Flash, Gemini-1.5-Pro, PaLM 2, and all three LLMs compared to human-written texts.}
    \label{fig:all_frequency}
\end{figure}

As illustrated by Figure \ref{fig:flash_detect}, Figure \ref{fig:pro_detect2} and Figure \ref{fig:palm2_detect}, the performance of our OAlg methods FTRL+Barrier and Optimistic-FTRL+Barrier in correctly detecting fake news generated by Gemini-1.5-Flash, Gemini-1.5-Pro, and PaLM 2 is superior to or comparable with that of ONS. All three OAlg methods generally maintain a false positive rate (FPR) below the significance level parameter $\alpha$.

 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/flash.neo2.7.eta1_300.png}
    \caption{The left subfigure illustrates the avergae time required to correctly identify texts generated by Gemini-1.5-Flash versus the average false positive rates (FPRs) over 300 runs under different significance levels ($\alpha$); a plot closer to the bottom left indicates better performance. The right subfigure evaluates the performance of each online algorithm under the null hypothesis, with a dashed line and shaded area representing the expected significance levels.}
    \label{fig:flash_detect}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/pro.neo2.7.eta1_300.png}
    \caption{The left subfigure illustrates the average time required to correctly identify texts generated by Gemini-1.5-Pro versus the average false positive rates (FPRs) over 300 runs under different significance levels ($\alpha$); a plot closer to the bottom left indicates better performance. The right subfigure evaluates the performance of each online algorithm under the null hypothesis, with a dashed line and shaded area representing the expected significance levels.}
    \label{fig:pro_detect2}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figs/palm2.neo2.7.eta1_300.png}
    \caption{The left subfigure illustrates the average time required to correctly identify texts generated by PaLM 2 versus the average false positive rates (FPRs) over 300 runs under different significance levels ($\alpha$); a plot closer to the bottom left indicates better performance. The right subfigure evaluates the performance of each online algorithm under the null hypothesis, with a dashed line and shaded area representing the expected significance levels.}
    \label{fig:palm2_detect}
\end{figure}

 Compared to Gemini-1.5-Pro, the time is shorter for Algorithm \ref{alg:Betting} to detect texts generated by Gemini-1.5-Flash or PaLM 2, which is resulted from a smaller mean difference,$\Delta=|\mu_x-\mu_y|$, between the scores of human-written texts and those generated by Gemini-1.5-Flash or PaLM 2. Despite the higher signal-to-noise ratio (i.e., $\frac{\Delta^2}{\sigma_x^2+\sigma_y^2}$) for PaLM 2 compared to Gemini-1.5-Flash—due to the larger mean difference $\Delta$—the detection time for texts generated by these models are similar when using FTRL+Barrier and Optimistic-FTRL+Barrier. In contrast, the time for ONS to detect LLM-generated texts by PaLM 2 is apparently shorter compared to those generated by Gemini-1.5-Flash. This discrepancy aligns with Theorem 3.4, which states that the expected rejection time bound $\E[\tau]$ for our methods is $O\left( \frac{1}{\Delta^2}\ln\left(\frac{1}{\alpha}\right) \right)$, whereas for ONS, it is $O\left( \frac{1}{\Delta^2}\ln\left(\frac{1}{\Delta^2\alpha}\right) \right)$, indicating greater sensitivity of ONS to the magnitude of mean difference $\Delta$. Additionally, the increase in signal-to-noise ratio might be too small to significantly enhance the performance of our OAlg methods. Moreover, we found that even when distributions of scores from human-written and LLM-generated texts by Gemini-1.5-Pro substantially overlap, our methods outperform ONS under $H_1$ due to the slow wealth accumulation of ONS in scenarios with small $\Delta$.

\section{Experiments of Evaluating Facial Expression Classifiers}\label{app:add_class}

\textbf{Dataset.}
Karolinska Directed Emotional Faces dataset (KDEF) ~\citep{lundqvist1998karolinska} dataset contains images of people expressing various emotions, including: afraid (AF), angry (AN), disgusted (DI), happy (HA), neutral (NE), sad (SA), and surprised (SU). Following \citet{lopez2016revisiting} and \citet{jitkrittum2016interpretable}, the straight profiles of facial expressions are divided into two categories: positive (HA, NE, SU) and negative (AF, AN, DI). The original images (562 × 762 pixels) are cropped to exclude the background, and then are resized to 64 × 64 pixels and converted to grayscale.


\textbf{Setup.} In our experiments, we adopt the CNN model introduced by \citet{podkopaev2023sequential} as the classifier, and implement 200 runs for each test. Specifically, the architecture of the model consists of four convolutional layers, each using a $3\times 3$ kernel, with 16, 32, 32, and 64 filters, respectively. Each convolutional layer is followed by a max-pooling layer with a $2 \times 2$ pooling window. The feature maps are flattened and passed through a fully connected layer containing 128 neurons. To avoid overfitting, dropout with a probability of 0.5 and early stopping (patience of 10 epochs, using 20 of the data for validation) are applied. ReLU activation functions are utilized across all layers. The network is trained using the Adam optimizer. Training begins after 20 observations, and the model parameters are updated after every 10 subsequent observations. Each training iteration runs for up to 25 epochs with a batch size of 32.

In $H_1$ scenario, the sequence $\{x_t\}$ consists of the corresponding real labels for 500 images, with 250 images from the negative class and 250 images from the positive class. The sequence $\{y_t\}$ includes the classification results predicted by the classifier for these images. To simulate the $H_0$ scenario, we let $\{x_t\}$ be a sequence of independent random choices between the negative and positive classes, so that each image is assigned a randomly chosen label. As a result, the shuffled labels no longer reflect the true content of the images, making the model’s predictions equivalent to random guessing.

Following \citet{podkopaev2023sequential}, we will update the betting fraction $\theta_t$ at each round $t$ within the halved decision space $[0,\frac{1}{2}]$ when using ONS. For Seq-C-2ST equipped with our two methods, the betting fraction $\theta_t$ will be updated on the whole decision space $[0,1]$.

\end{document}
