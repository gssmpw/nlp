\section{Related Work}

\subsection{Text-to-Speech}
Text-to-Speech, the transformation of text into audible signals understandable by humans, is pivotal for human-computer interaction. TTS systems can
be mainly divided into AR-based and NAR-based categories. For AR-based systems, VALL-E \cite{valle} predicts the first layer of acoustic tokens extracted by EnCodec \cite{encodec} using an AR codec language model, while a NAR model is used to predict the remaining layers. CosyVoice \cite{cosyvoice} employs an AR model to predict supervised semantic representations and combines flow matching to predict acoustic representations. AR-based TTS models, with their in-context learning capability, can generate natural, prosody-diverse speech in a streaming manner. However, AR-based TTS models exhibit shortcomings in generation efficiency. Besides the previously mentioned VALL-E 2 \cite{valle2}, MEDUSA \cite{MEDUSA} and VALL-E R \cite{Valler} introduce speculative decoding \cite{spdeco} and a codec-merging method, respectively, to accelerate autoregressive generation. Nonetheless, the efficiency gains achieved by these approaches remain limited, unable to perform synchronized decoding steps with text tokens.  


For NAR-based TTS models, most previous approaches require speech duration prediction conditioned on the input text, followed by upsampling the text representations to match the acoustic feature length before feeding them into the generation model. Following FastSpeech \cite{fastspeech2}, VoiceBox \cite{voicebox} and NaturalSpeech 2 \cite{ns2} predict phone-level durations using a regression-based approach. NaturalSpeech 3 \cite{ns3} adopts a discrete diffusion model, combining classification loss and duration prompts for duration prediction, which outperforms text-dependent regression-based duration prediction in terms of speech robustness and quality. However, NaturalSpeech 3 requires an additional duration prediction model, which complicates the pipeline, whereas SyncSpeech integrates duration and speech token predictions into a unified framework. The NAR TTS model most relevant to SyncSpeech is MaskGCT \cite{maskgct}, which predicts the total duration of the speech and then performs temporally-unordered multi-step mask prediction. Unlike MaskGCT, SyncSpeech employs temporally-ordered mask prediction and BPE token-level duration prediction to achieve speech generation in a dual-stream scenario.


\subsection{Speech Large Language Models}
Speech Large Language Models (SLLMs) empower LLMs to interact with users through speech, responding to userâ€™s instruction with  low latency \cite{wavchat}.
A basic approach \cite{audiogpt} to achieve this speech interaction involves a cascade of automatic speech recognition (ASR), LLM and TTS models, where the ASR transcribes the users' speech instruction into text, and the TTS model converts the LLM's textual response into speech. 
However, most current AR TTS models  cannot process streaming text input, resulting in significant latency in the aforementioned cascaded systems. 
In contrast, some end-to-end speech-language models have been proposed that can generate speech tokens directly,   thereby achieving extremely low response latency. LLaMA-Omni \cite{llamaomni} aligns the hidden states of LLMs with discrete HuBERT \cite{hubert} representations using CTC loss, but the generated speech exhibits less natural prosody. Mini-Omni \cite{mini1} employs a parallel decoder approach to generate text and speech tokens simultaneously. 
However, due to the significantly longer length of speech tokens compared to text tokens, its generation efficiency remains low. 
The proposed SyncSpeech can process streaming text input and generates speech in synchronization, with the potential to unite with LLMs to become end-to-end SLLMs.  