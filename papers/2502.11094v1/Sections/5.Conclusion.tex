
\section{Conclusion}
This paper presents SyncSpeech, a dual-stream speech generation model built on a temporal masked transformer. SyncSpeech can efficiently generate low-latency streaming speech from the real-time text input, maintaining the high quality and robustness of the generated speech. We conducted comprehensive performance evaluations and analysis experiments in both English and Mandarin, demonstrating its capability as a foundational model for integration with upstream LLMs. In the future, SyncSpeech will be trained on larger datasets to further improve its performance.
 

\section{Limitations}
In this section, we will analyze the limitations of
SyncSpeech and discuss potential future work. SyncSpeech requires token-level alignment information, which is challenging to achieve for sentences with mixed languages, and preprocessing becomes time-consuming on large-scale datasets. In the future, we will explore semi-supervised duration prediction, which only requires the duration of a complete sentence without strict token-level alignment information, and integrate SyncSpeech into SLLM as a speech generation module. In addition, since the off-and-shelf streaming speech decoder relies on flow matching, it limits the off-the-shelf RTF and the FPL. Moreover,` current single-codebook acoustic tokens, such as WavTokenizer \cite{wavtokenizer}, do not support streaming decoding. In the future, we will investigate efficient and low-latency streaming speech decoders.