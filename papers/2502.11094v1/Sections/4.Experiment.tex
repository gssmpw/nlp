\section{Experiments}


\subsection{Experimental Settings}
\paragraph{Datasets} We trained SyncSpeech on datasets in both English and Mandarin, including the 585-hour LibriTTS \cite{libritts} dataset and 600 hours of internal Mandarin datasets. The internal Mandarin dataset was further expanded to approximately 2000 hours, employing techniques such as speed alteration and pitch shifting. The Montreal Forced Aligner (MFA) \cite{mfa}  aligned transcripts according to its phone set, after which the alignment was transformed into text BPE-level format. We evaluated SyncSpeech using three benchmarks: (1) LibriSpeech \textit{text-clean} \cite{librispeech}, a standard English TTS evaluation set; (2) SeedTTS \textit{test-zh} \cite{seedtts}, with 2,000 samples from the out-of-domain Mandarin DiDiSpeech dataset \cite{didispeech}; and (3) SeedTTS \textit{test-hard}, containing approximately 400 difficult cases to evaluate TTS model robustness with repeated text, tongue twisters, and other complex synthesis scenarios. 

\paragraph{Settings} 
We set the number of text tokens to look ahead $q=1$. The chunk size of speech decoder is 15. 
TMT has 16 layers, 16 attention heads, 1024-dimensional
embeddings, and 2048-dimensional feed-forward layers. 
SyncSpeech was trained on 4 NVIDIA A800 80G GPUs. 
The pre-training stage lasts for 70K steps, and the second stage lasts for 20K steps. 

\paragraph{Baseline Models}
This paper focuses on low-latency and efficient TTS in dual-stream scenarios. Under the same data scale, we reproduced the following baseline models for comparison: CosyVoice \cite{cosyvoice} and recently proposed CosyVoice2 \cite{cosyvoice2.0}. CosyVoice requires complete text input before speech generation. 
CosyVoice2 uses interleaved text-speech modeling to process streaming text input and simultaneously generate streaming speech. We trained CosyVoice, CosyVoice2, and SyncSpeech using the same speech tokenizer and text tokenizer, and employed the same open-source streaming speech decoder. We utilized the official code\footnote{https://github.com/FunAudioLLM/CosyVoice} to reproduce the model and adopted a Llama-style Transformer, matching the size of SyncSpeech, as the backbone of the text-to-speech model.  Additionally, we compared the open-sourced TTS model MaskGCT \cite{maskgct}, F5-TTS \cite{F5tts}, and VALL-E \cite{valle}, which were trained on large-scale data.
 More details about baseline models can be found in the Appendix \ref{baselines}.


\paragraph{Evaluation Metrics} For the three benchmarks, we evaluated
speech quality, latency, and  efficiency. 
For speech robustness, we chose Whisper-V3 and Paraformer as the ASR models for English and Mandarin, respectively, to transcribe the generated speech. Then, we calculated the WER compared to the original transcriptions to evaluate the spech robustness. We adopted the ERes2Net-based \cite{eres2net} speaker verification model\footnote{https://github.com/modelscope/3D-Speaker} to evaluate speaker similarity (SS). We selected 100 sentences from each system and invited 10 native listeners to conduct a subjective MOS evaluation for speech naturalness (MOS-N), scoring from 1 to 5. 
In terms of latency and efficiency, we compared the performance of various models on a single A800 GPU. 
Due to the off-the-shelf speech decoder, we evaluate the latency and efficiency of the text-to-token stage across all models, except for F5-TTS.
We calculated the time required for the number of speech tokens to reach the chunk size of the speech decoder as First-packet latency (FPL). There are two scenarios: one assumes the text is already available (FPL-A), while the other involves receiving output from the upstream LLM model (FPL-L), accounting for the time required for text generation.
 For the real-time factor (RTF), we measure the ratio of the total duration of generated speech to the total time taken by the model. More details about FPL and RTF can be found in the Appendix \ref{evaluation metrics}.

\begin{table*}[t]
\centering
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{lccccccccc}
\toprule
\textbf{Model} & \textbf{\#Scenario}  & \textbf{\#Data(hrs)}   & \textbf{WER(\%)} $\downarrow$   & \textbf{SS(\%)} $\uparrow$ & \textbf{FPL-A(s)}$\downarrow$ & \textbf{FPL-L(s)} $\downarrow$ & \textbf{RTF(\%)} $\downarrow$ & \textbf{MOS-N} $\uparrow$ \\ \hline
\multicolumn{10}{c}{\textbf{LibriSpeech \textit{test-clean}}}   \\ \hline
\textbf{Ground Truth} &- &-  & 2.12   & 69.67 &- &- &-   &$\text{4.62}_{\pm 0.12}$       \\ \hdashline
\textbf{F5-TTS*} & Offline & 100K Multi.  & \textbf{2.51} & \textbf{73.10} &1.27 &1.98 &0.23  &-  \\
\textbf{MASK-GCT*} & Offline &100K Multi. &2.77 & 70.81 &2.15 &2.55 & 0.37 &- \\
\textbf{VALL-E*}  & Output Stream & 60K EN & 5.90 & 59.71 & 0.75 &1.47 &1.41 &- \\
\textbf{CosyVoice} & Output Stream & 585 EN  & 3.47  & \underline{63.52} &0.22 &0.94 &0.45   & $\text{4.39}_{\pm 0.12}$           \\ 
\textbf{CosyVoice2} & Dual-Stream & 585 EN   & \underline{3.00}      & 63.48 &0.22 &0.35 &0.45   &$\textbf{\text{4.48}}_{\pm 0.13}$           \\
\textbf{SyncSpeech} & Dual-Stream & 585 EN    & 3.07    & 63.47 & \textbf{0.06} &\textbf{0.11} &\textbf{0.07}   &$\textbf{\text{4.48}}_{\pm 0.14}$         \\ \hline
\multicolumn{10}{c}{\textbf{Seed \textit{test-zh}}}   \\ \hline
\textbf{Ground Truth} &-  &- & 1.26  & 75.15  &- &- &- & $\text{4.68}_{\pm 0.10}$      \\ \hdashline
\textbf{CosyVoice}  & Output Stream  &2K ZH      & 3.03    & 61.51  &0.22 &0.62 &0.43  & $\text{4.34}_{\pm 0.14}$            \\ 
\textbf{CosyVoice2} & Dual-Stream  &2K ZH & 3.31      & 61.89   &0.22 &0.35 &0.43 & $\text{4.37}_{\pm 0.13}$           \\
\textbf{SyncSpeech}& Dual-Stream  &2K ZH  & \textbf{2.38}    & \textbf{62.14}   & \textbf{0.04} & \textbf{0.09} &\textbf{0.05} &$\textbf{\text{4.45}}_{\pm 0.11}$           \\
\hline
\multicolumn{10}{c}{\textbf{Seed \textit{test-hard}}}   \\ \hline
\textbf{CosyVoice} & Output Stream  &2K ZH    & 26.26    & 66.71   &0.22 &1.22 &0.44 & $\text{3.84}_{\pm 0.15}$            \\ 
\textbf{CosyVoice2} & Dual-Stream  &2K ZH  & 21.61  & 67.13 &0.22 &0.35 &0.44      &$\text{3.86}_{\pm 0.14}$            \\
\textbf{SyncSpeech} & Dual-Stream &2K ZH  & \textbf{17.21}    & \textbf{67.21}  &\textbf{0.05} & \textbf{0.10} &\textbf{0.08} &$\text{3.86}_{\pm 0.11}$         \\
\bottomrule
\end{tabular}
}
\caption{The evaluation results of SyncSpeech and baseline models across the three benchmarks. * indicates the model trained on the large-scale dataset. Underline indicates the best performance in terms of WER and SS with the 585 hours training scale. \#Data refers to the used training dataset in hours.}
\label{table1}
\end{table*}


\subsection{Main Results}
The evaluation results for SyncSpeech and the baseline models are presented in Table \ref{table1}. 

\paragraph{Speech Robustness} 
We found that SyncSpeech exhibits different performance compared to the baselines across the three benchmarks. Specifically, on the LibriSpeech \textit{test-clean} benchmark, the performance of SyncSpeech was very close to that of CosyVoice2 based on the WER metric, with only a minor difference of 0.07\%. SyncSpeech achieved a lower WER score on the Seed \textit{test-zh} set compared to CosyVoice and CosyVoice2, with improvements of 0.65\% and 0.93\%, respectively.  A key difference between the English and Mandarin datasets is the higher compression rate of the LLM tokenizer for Mandarin. In English, one word typically equals one token, while in Mandarin, a common phrase often corresponds to a single token.
This means that, compared to the baseline model, SyncSpeech is better suited to the high compression rate tokenizer of the upstream large model. Furthermore, on the Seed \textit{test-hard} set, the robustness advantage of SyncSpeech was even more pronounced, with the improvements 9.05\% and 4.40\%, respectively. In handling complex text, the explicit duration modeling in SyncSpeech helped the model learn the alignment between text and speech.

\paragraph{Speaker Similarity} Due to the same speech decoder and the excellent voice disentanglement capability of the speech tokens, SyncSpeech, CosyVoice, and CosyVoice2 exhibited similar performance in terms of speaker similarity.
\paragraph{Speech Naturalness} The MOS-N scores for SyncSpeech and CosyVoice2 were quite similar on the LibriSpeech \textit{text-clean}, indicating that the naturalness of the generated speech was generally comparable. On the Seed \textit{test-zh} benchmark, SyncSpeech outperformed CosyVoice2 by 0.08.  In the Seed \textit{test-hard} benchmark, high WER and uncommon text led to unnatural prosody and generally low MOS-N scores in the generated speech.
\paragraph{Latency} SyncSpeech has made a breakthrough in terms of latency, as shown in Table \ref{table1}. Specifically, on the LibriSpeech \textit{test-clean} benchmark, SyncSpeech was approximately 4 times faster than traditional AR models and over 20 times faster than the SOTA offline models in terms of FPL-A. On the Seed \textit{test-zh} benchmark, SyncSpeech achieved speed improvements of over 5 times and 30 times, respectively. When receiving streaming text from the upstream large model (FPL-L), SyncSpeech can begin generating speech with just two text tokens. In contrast, CosyVoice2 requires five tokens, while CosyVoice and other baseline models need the entire text input. This highlights the distinct advantage of SyncSpeech in practical applications.

\paragraph{Efficiency} In terms of RTF, SyncSpeech is about 6.4 times faster on the LibriSpeech \textit{test-clean} benchmark and about 8.6 times faster on the Seed \textit{test-zh} benchmark compared to previous AR models. 
On the Seed \textit{test-hard} set, due to the increased number of text tokens caused by the uncommon text, the efficiency of SyncSpeech is slightly reduced. Theoretically, the time complexity of AR models is $O(T)$, while the time complexity of SyncSpeech is  $O(L)$, where  $T$ represents the number of speech tokens and 
$L$ denotes the number of text tokens, thereby significantly improving efficiency.

\section{Analysis}
\paragraph{Sampling Strategy} In the LibriSpeech validation set, we provided the ground-truth durations and applied greedy search along with different Top-k thresholds for duration prediction, as shown in Table \ref{table3}. We found that, in terms of speech robustness, both Top-k 3 and greedy search outperformed the use of ground-truth durations in terms of the WER metric. This is because the model struggled to effectively generalize to anomalies in the ground-truth durations. We employed
UTMOSv2\footnote{https://github.com/sarulab-speech/UTMOS22} as a surrogate objective metric of MOS-N. In terms of speech naturalness, the results of Top-k 3 sampling are slightly better than those with the given ground-truth durations.  Additionally, we applied different Top-k thresholds for speech token prediction. SyncSpeech exhibited superior performance during greedy search, which is different from the previous AR TTS models or offline models. This is because the speech tokens obtained through single-step decoding have the temporal dependency, which cannot be compensated by subsequent generation. 


\begin{table}[]
\centering
\resizebox{0.42\textwidth}{!}{
\begin{tabular}{lcc}

\toprule
\textbf{Sampling Strategy}       & \textbf{WER(\%)}$\downarrow$  & \textbf{UTMOSv2}$\uparrow$ \\ \hline
\multicolumn{3}{c}{Duration Prediction} \\
\hline
Ground Truth            & 2.59 & 3.45   \\ \hdashline 
Greedy Search   & 2.50 & 3.44   \\
Top-k 3         & \textbf{2.44} & \textbf{3.46}   \\
Top-k 5         & 2.93 & 3.44   \\
Top-k 10        & 2.76 & 3.41  \\ 
\hline
\multicolumn{3}{c}{Speech Token Prediction} \\
\hline
Greedy Search & \textbf{2.44} & \textbf{3.46}   \\
Top-k 3          & 3.82 & 3.43  \\
Top-k 5          & 4.23 & 3.43   \\ 
\bottomrule
\end{tabular}
}
\caption{Performance across various Top-k thresholds for duration prediction and speech token prediction on the LibriTTS validation set.}
\label{table3}
\end{table}

\paragraph{Number of Look-ahead Tokens}
We evaluated how varying the number of tokens to look ahead affects speech robustness and speech naturalness on two validation sets, with the results presented in Table \ref{table5}. We discovered that the optimal number of look-ahead text tokens varies across different languages in terms of WER performance.  This is influenced by the difference in the compression rate of text tokens and the contextual dependency in different languages. In terms of speech naturalness, when the look-ahead number $q$ is greater than $2$, the generated speech exhibits slightly more natural pauses and speed, but it results in increased latency.

\paragraph{Ablation Study}
We conducted an ablation study on the pre-training strategy by directly training the randomly initialized model in a manner consistent with the prediction process. The WER results on the two validation sets are shown in Table \ref{table6}. We found that pre-training significantly improved the speech robustness of the model, improving the WER metric by 1.17\% and 1.06\% on the two languages, respectively. This indicated that masked pre-training not only improved training efficiency but also enhanced the robustness of the synthesized speech. Additionally, a standard causal attention mask was applied to replace the designed attention mask, as shown in Table \ref{table6}. If the mask token sequence of the same text token cannot attend to each other during inference, the robustness of the generated speech significantly decreased. This further demonstrated the effectiveness of the designed attention mask.



\begin{table}[]
\centering
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
                    & \textbf{LH Num.}           & \textbf{WER(\%)}$\downarrow$  & \textbf{FPL-L(s)}$\downarrow$  & \textbf{UTMOS-v2}$\uparrow$  \\ \hline
\multirow{4}{*}{EN} & q=1    & \textbf{2.44}    & \textbf{0.11}    & 3.46     \\
                    & q=2    & 2.87    & 0.13   & 3.41       \\
                    & q=3    & 2.52   & 0.16    & \textbf{3.48}         \\ 
                    & q=4    & 2.52    & 0.19    &  \textbf{3.48}     \\          \hline
\multirow{4}{*}{ZH} & q=1   & 2.51 & \textbf{0.09} & -    \\
                    & q=2   & 2.49 & 0.12  & -   \\ 
                    & q=3   & \textbf{2.41} &0.14 & - \\
                    & q=4   & \textbf{2.41} &0.17 & - \\
\bottomrule
                    
\end{tabular}
}
\caption{Performance with different numbers of look-ahead text tokens across two validation sets.}
\label{table5}
\end{table}




\begin{table}[]
\centering
\resizebox{0.39\textwidth}{!}{
\begin{tabular}{lcc}

\toprule
      & \textbf{English}   &\textbf{Mandarin}   \\ \hline
SyncSpeech         & \textbf{2.44} & \textbf{2.41}   \\
w/o pretrain        &  3.61 & 3.47  \\
w/o designed Mask  & 8.19 & 7.97 \\ 
\bottomrule 
\end{tabular}
}
\caption{WER (\%) results of the ablation study across the two validation sets. }
\label{table6}
\end{table}


