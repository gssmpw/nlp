
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{Inference_demo.pdf}
    \caption{Illustrations of the inference process in two scenarios.The upper part represents the scenario without using speech prompts to control prosody, where in the first step, the duration of the first character needs to be predicted separately; in the subsequent decoding steps, both the current speech token and the duration of the next text token are predicted simultaneously.  The lower part shows the illustration of using speech prompts to control prosody, where $y^p$ and $s^p$ denote the text tokens and speech tokens of the speech prompt, respectively.}
    \label{fig2}
\end{figure*}


\section{Details of Baselines}
\label{baselines}

\paragraph{CosyVoice} A two-stage large-scale TTS system. The first stage is an autoregressive model similar to VALL-E \cite{valle}, and the second stage is a diffusion model. We use the official code and the 25Hz version of the pre-trained checkpoint\footnote{https://www.modelscope.cn/iic/CosyVoice-300M-25Hz.git}.

\paragraph{CosyVoice2} Compared to CosyVoice, improvements have been made in the following three areas: 1) The quantizer speech tokenizer has been upgraded to FSQ, further improve the performance of the quantization encoder. 2) Interleaved text-speech modeling is employed, allowing for streaming text input. 3) A chunk-aware speech decoder is used for streaming speech generation. We use the official code and the 25Hz version of the pre-trained checkpoint\footnote{https://github.com/FunAudioLLM/CosyVoice}.


\paragraph{VALL-E} A large-scale TTS system employs both an autoregressive and an auxiliary non-autoregressive model to predict discrete tokens derived from the Encodec \cite{encodec}. We used an open-source checkpoint for inference. As there is currently no open-source streaming speech decoder for Encodec, we assumed 15 frames when calculating the FPL metric for a fair comparison.

\paragraph{MaskGCT}\cite{maskgct}  This is a large-scale, two-stage trained model. In the first stage, the model utilizes text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model. In the second stage, it predicts acoustic tokens based on these semantic tokens. During training, MaskGCT learns to predict masked semantic or acoustic tokens given specific conditions and prompts. During inference, MaskGCT generates speech through multi-step temporally non-sequential masked prediction. Here, we use the official code and pre-trained checkpoint\footnote{https://github.com/openmmlab/Amphion}.

\paragraph{F5-TTS}\cite{F5tts} a fully non-autoregressive text-to-speech system
based on flow matching with Diffusion Transformer (DiT). The
text input is simply padded with filler tokens to the same length as input speech,
and then the denoising is performed for speech generation. F5-TTS does not utilize speech tokens and directly maps text to acoustic features. Here, we use the official code and pre-trained checkpoint\footnote{https://github.com/SWivid/F5-TTS}.






\section{Details of Latency and Efficiency Evaluation Metrics}
\label{evaluation metrics}

The first-package latency (FPL) and real-time factor (RTF) are two import metrics for streaming TTS models.
We define $d_{\text{LLM}}$ as the average time required by the upstream LLM to generate one text token and $d_{\text{TTS}}$ as the the time for the corresponding AR TTS models to forward one step and for the NAR TTS models to perform one sampling. The FPL-L of baseline models and SyncSpeech are as follows,
\begin{align}
& L_{\text{FPL-L}}^{\text{CosyVoice}} =L \cdot d_{\text{LLM}} + 15 \cdot d_{\text{TTS}}, \\
& L_{\text{FPL-L}}^{\text{VALL-E}} =L \cdot d_{\text{LLM}} + 15 \cdot d_{\text{TTS}}, \\
&L_{\text{FPL-L}}^{\text{CosyVoice2}} =5 \cdot d_{\text{LLM}} + 15 \cdot d_{\text{TTS}}, \\
& L_{\text{FPL-L}}^{\text{MaskGCT}} =L \cdot d_{\text{LLM}} + b \cdot d_{\text{TTS}}, \\
& L_{\text{FPL-L}}^{\text{F5-TTS}} =L \cdot d_{\text{LLM}} + b \cdot d_{\text{TTS}}, \\
&L_{\text{FPL-L}}^{\text{SyncSpeech}} =(k+1) \cdot d_{\text{LLM}} + c \cdot d_{\text{TTS}},
\end{align}
where $b$ represents the number of sampling iterations for the NAR model, and $c$ denotes the number of BPE text tokens when the generated speech tokens surpass the decoder's chunk size, typically ranging from 1 to 3. Here, we assume the upstream LLM model is Qwen-7B, and when running on a single NVIDIA A800 GPU, we obtain an average token generation time $d_{LLM} = 25 ms$. 
When the first term in FPL-L is omitted, it becomes FPL-A. It is important to note that when calculating above metrics, we did not apply any engineering optimizations, such as KV cache.

We also conducted a brief theoretical analysis of RTF for SyncSpeech. The RTF for SyncSpeech is calculated as follows,
\begin{equation}
L_{RTF} = \frac{ (L+1) \cdot d_{\text{TTS}}}{T\cdot F},
\end{equation}
where $L$ and $T$ represent the number of BPE tokens and speech tokens, respectively $F$ refers to the frame length of the speech tokens.  The time complexity for SyncSpeech to generate an entire sentence can be simplified to $O(L)$, whereas the time complexity for concurrent approaches, such as CosyVoice2 and IST-LM, is 
$O(T)$. As a result, SyncSpeech can significantly expedite speech generation.


\section{Duration Control}
\label{Duration Control}
Since we have implemented duration prediction and control, we can multiply the predicted durations by a modulation factor to adjust speech rate. The results, shown in Table \ref{table7}, indicate that the robustness of synthesized speech is optimal when the modulation factor is 1.1.  However, when the modulation factor is too small or too large, the WER of the synthesized speech by SyncSpeech increases significantly. This is because when we multiply the predicted duration of each text token by a fixed modulation factor of less than 1, SyncSpeech's contextual learning capability causes the subsequent tokens to be spoken increasingly faster, leading to a surge in WER. When the modulation factor is set to 0.8, the average total duration of the synthesized speech is 0.68 times that when the modulation factor is 1. Therefore, more reasonable duration control requires two inference processes: the duration obtained from the first inference is multiplied by a modulation factor during the second inference to control the speech rate.



\begin{table}[]
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{lcccccc }

\toprule
\textbf{Modulation Factor}      & \textbf{0.8}   & \textbf{0.9} & \textbf{1.0} & \textbf{1.1} &\textbf{1.2} &\textbf{1.3} \\ \hline
LibriSpeech        & 14.3 & 4.20 &3.07 & \textbf{2.85} &3.22 &4.31   \\
SeedTTS test-zh    & 12.1 &3.38 &2.38 & \textbf{2.15} &2.53 & 3.48            \\
\bottomrule 
\end{tabular}
}
\caption{Performance comparison with different modulation factors for duration control in terms of WER.}

\label{table7}
\end{table}



\section{Other Strategies for Sequence Construction}
We also experimented with other sequence construction strategies. (1) One approach is to separate duration prediction and speech tokens prediction into two steps. This method reduces efficiency by half but achieves better speech robustness, with a WER of around 2.75 on the LibriSpeech \textit{test-clean} dataset. (2) We also tried removing the duration placeholder and using the last speech token of the previous text token to predict the number of speech tokens corresponding to the current text token. However, we found that this sequence construction made the corresponding pre-training less effective than it is now. (3) We also attempted a method similar to ELLA-V \cite{ellav}, where the corresponding text token is placed before each placeholder. However, we found that this sequence generated speech that was unnatural, with a noticeable disconnection between words.