
\section{Method}
A dual-stream TTS model simultaneously processes streaming text input and generates speech in a streaming manner. Upon receiving newly generated text tokens $\boldsymbol{y}_{\text{arr}}$ from the upstream LLMs, the objective of the dual-streaming TTS it to estimate $p(\boldsymbol{x}_{\text{arr}} | \boldsymbol{y}_{\text{arr}}, \boldsymbol{x}_{\text{pre}},\boldsymbol{y}_{\text{pre}})$. In this context,  $\boldsymbol{x}_{\text{arr}}$ represents the speech waveform segment corresponding to $\boldsymbol{y}_{\text{arr}}$, while   $\boldsymbol{y}_{\text{pre}}$ and $ \boldsymbol{x}_{\text{pre}}$ denote the preceding text tokens and its corresponding speech waveform, respectively.  

SyncSpeech is a two-stage TTS system, consisting of the text-to-token and token-to-speech stages. 
The estimation of $p(\boldsymbol{x}_{\text{arr}} | \boldsymbol{y}_{\text{arr}}, \boldsymbol{x}_{\text{pre}}, \boldsymbol{y}_{\text{pre}})$ is decomposed into a  text-to-token model $p(\boldsymbol{s}_{\text{arr}}| \boldsymbol{y}_{\text{arr}}, \boldsymbol{x}_{\text{pre}}, \boldsymbol{y}_{\text{pre}})$ and a token-to-speech model $p(\boldsymbol{x}_{\text{arr}} | \boldsymbol{s}_{\text{arr}})$, where $\boldsymbol{s}_{\text{arr}}$ is the speech tokens corresponding to the speech waveform segment $\boldsymbol{x}_{\text{arr}}$. Specifically,  the proposed TMT is adopted as the backbone of text-to-token model. Then, an off-the-shelf chunk-aware speech decoder \cite{cosyvoice2.0} is adopted as the  token-to-speech model. 

The proposed TMT module is based on a llama-style Transformer \cite{llama}. We have specifically designed a novel attention mask to accommodate temporally-ordered mask generation. Below, I will detail the two-stage training strategy of the TMT-based text-to-token model and its attention mask, cover the details of the other modules, and describe the inference process.

\subsection{Training}
\label{sec:training}

Given a dataset of transcribed speech ($\boldsymbol{\Tilde{x}}$, $\boldsymbol{\Tilde{y}}$), where $\boldsymbol{\Tilde{x}}$ and $\boldsymbol{\Tilde{y}}$ denote an audio sample and its transcript, respectively, the transcript $\boldsymbol{\Tilde{y}}$ is tokenized into a BPE token sequence $\boldsymbol{y} = [y_1, y_2, y_3, ..., y_L]$, where $L$ is the number of BPE tokens.  An off-the-shelf speech tokenizer is used to encode the speech sample $\boldsymbol{\Tilde{x}}$ into $T$ frame discrete speech tokens $\boldsymbol{s} = [s_1, s_2, s_3, ..., s_T]$. We further define duration tokens $\boldsymbol{a} = [a_1, a_2, a_3, ..., a_L]$ as the positions indicating the end time of each corresponding BPE token within the speech token sequence, with $a_L = T.$  For a pair of ($\boldsymbol{\Tilde{x}}$, $\boldsymbol{\Tilde{y}}$), $\boldsymbol{a}$ can be obtained through an open-source alignment tool. 

As shown in Figure \ref{fig1}, to maintain consistency with the inference process (see Section \ref{sec:inference}), the sequence input is then constructed as follows. We select a random number $n \in [1, L]$, which indicates that when receiving streaming text input, SyncSpeech needs to  generate the speech tokens corresponding to the $n$-th BPE token  at this moment. To avoid unnatural pauses, SyncSpeech allows look ahead $q$ text tokens, obtaining a truncated text token sequence $\boldsymbol{y}' = [y_1, y_2, y_3, ..., y_{L'}]$, where $L'=min(L, n+q)$. Based on the duration tokens $\boldsymbol{a}$, the truncated speech token sequence $\boldsymbol{s}_{1:a_{n}}=[s_1, s_2, ...., s_{a_{n}}]$ is obtained. Then, we define the masked speech token sequence $\boldsymbol{s}'$ and and corresponding binary mask 
$\boldsymbol{m}$ as follows, 
\begin{equation}
\boldsymbol{s}' = \boldsymbol{s}_{1:a_{n}} \odot \boldsymbol{m},
\end{equation}
\begin{equation}
    \boldsymbol{m}=[m_{i}]_{i=1}^{a_{n}}, \boldsymbol{m}_{1:a_{n-1}}=0, \boldsymbol{m}_{a_{n-1}:a_{n}}=1.
\end{equation}
That is all speech tokens corresponding to $\boldsymbol{x}_{n}$ are replaced with the specific mask token, while the rest remain unchanged.  Then, the truncated text token sequence $\boldsymbol{y}'$, along with the masked speech token sequence $\boldsymbol{s}'$ and duration tokens $\boldsymbol{a}$, are used to construct the input sequence as follows,
\begin{equation}
\boldsymbol{f} = [\boldsymbol{y}',E, D, \boldsymbol{s}'_{1:a_1},..., D, \boldsymbol{s}'_{a_{n-1}:a_n}, D],
\label{eq1}
\end{equation}
where $E$ is end-of-text token, $D$ is a placeholder for duration prediction. 
Based on the duration tokens $\boldsymbol{a}$, $D$ is used to separate the masked speech token sequence corresponding to different BPE tokens. In practice, $E$ is inserted only when $n=L$ .


The sequence $\boldsymbol{f}$ is used as input for the TMT with the mask prediction and duration prediction as training objectives. Specifically, the sequence $\boldsymbol{f}$ is fed into the TMT forward to obtain the hidden states, which then pass through two different linear layers to predict the speech tokens corresponding to text token $y_{n}$ and the duration of the \textbf{next text token} $y_{n+1}$. This enables us to integrate duration prediction and mask prediction into a single decoding step during inference, except for the first text token duration prediction (Details are provided in Section \ref{sec:inference}). 
We minimize the following negative log-likelihood function for masked generative training and duration training,
\begin{equation}
\mathcal{L}_{\text {mask}}^{1}=-\log p \left( \boldsymbol{s}_{a_{n-1}:{a_n}} \mid \boldsymbol{f}; \theta \right), 
\end{equation}
\begin{equation}
\mathcal{L}_{\text {duration}}^{1}=-\log p \left(l_{n+1}\mid \boldsymbol{f}; \theta \right),
\end{equation}
where $\theta$ represents the neural network parameters of TMT, $l_{n+1}=a_{n+1} - a_{n}$ and $a_0=0$. In this way, we simulate the scenario of receiving streaming text input during the training process and are able to generate speech in sync.


We design a corresponding attention mask, as shown in Figure \ref{fig1}. Specifically, a causal mask is used for the truncated text sequence $\boldsymbol{y}'$ and duration placeholder parts. For the  masked speech token sequence $\boldsymbol{s}'$, a dynamic chunk attention mask is applied based on the duration tokens $\boldsymbol{a}$, enabling it to attend all historical tokens, as well as all speech tokens and mask tokens corresponding to their own text BPE tokens. 

\subsection{Pretraining}
\label{sec:pretraining}
While the aforementioned method aligns with the prediction process, it suffers from low training efficiency. This training inefficiency arises because, during each training step, only the gradients of speech tokens $\boldsymbol{s}_{a_{n-1}:{a_n}}$ and durations for $y_{n+1}$ are backpropagated.
To further improve the training efficiency, we first perform masked pre-training on the TMT.


Given speech tokens $\boldsymbol{s}$ of a speech sample, we obtain the masked speech tokens $\boldsymbol{\hat{s}} = \boldsymbol{s} \odot \boldsymbol{\hat{m}} $, where $\boldsymbol{\hat{m}}=[\hat{m}_{i}]_{i=1}^{a_{L}} $ is a binary mask of speech tokens. 
We design the masking rules primarily from two perspectives, high masking probability and consistency with the prediction process as much as possible.
Specifically, the binary mask $\boldsymbol{\hat{m}}_\text{bpe}$ of text tokens is constructed first, where the first value is distributed according to a Bernoulli distribution ($p=0.5$) and the subsequent adjacent values cannot be the same. Based on the duration tokens $\boldsymbol{a}$, the text token mask $\boldsymbol{\hat{m}}_\text{bpe}$ is converted into the corresponding speech token mask $\boldsymbol{\hat{m}}$. Then, we build the following sequence as the input for TMT, 
\begin{equation}
\boldsymbol{\hat{f}} = [\boldsymbol{y},E, D, \boldsymbol{\hat{s}}_{1:a_1},..., D, \boldsymbol{\hat{s}}_{a_{L-1}:a_L}],
\end{equation}
and the TMT is optimized to minimize the negative log-likelihood for masked generative training and duration training as follows,
\begin{equation}
\mathcal{L}_{\text {mask}}^{2} =- \sum_{\substack{j \in J}} \log p \left(\boldsymbol{s}_{a_{j-1}:a_{j}} \mid \boldsymbol{\hat{f}}_{\leq a_{j}}; \theta \right),
\end{equation}
\begin{equation}
\mathcal{L}_{\text {duration}}^{2} =- \sum_{\substack{j \in J}} \log p \left(l_{j}\mid \boldsymbol{\hat{f}}_{\leq a_{j-1}}; \theta \right),
\end{equation}
where $J$ denote the sequence where each element satisfies the condition $\hat{m}_{j} = 1$,  
$\boldsymbol{\hat{f}}_{\leq a_{j}}=[\boldsymbol{y},E, D,\boldsymbol{\hat{s}}_{1:a_1},..., D, \boldsymbol{\hat{s}}_{a_{j-1}:a_j}]$ and $l_{j} = a_{j} - a_{j-1}$.  Additionally, the attention mask mentioned above is also utilized for pretrain.


In summary, an efficient masked pretraining is initially performed with a high masking probability to facilitate the alignment between text and speech tokens. Subsequently, we fine-tune the pretrained model using a training strategy consistent with the prediction process. This approach enhances the efficiency of the training process, and the masked pretraining also contributes to the robustness of the generated speech.


\subsection{Other Modules}
\label{sec:other modules}
In this subsection, we introduce the other modules in SyncSpeech besides TMT. 
1) Text BPE tokenizer: To facilitate interaction with upstream LLMs, we  utilize the Qwen tokenizer \cite{qwen} directly. 2) Speech tokenizer:   the open-source supervised speech semantic (S3) tokenizer \cite{cosyvoice2.0} is selected, which operates at 25 Hz. The S3 tokenizer is developed by integrating finite scalar quantization (FSQ) \cite{fsq} into the intermediate representations of an ASR model trained on large-scale data, and then fine-tuning it for the ASR task.
3) The off-the-shelf speech decoder \cite{cosyvoice2.0} is based on the conditional flow matching (CFM) decoder and HiFi-GAN vocoder \cite{hifigan}. The CFM decoder employs a chunk-aware training strategy, enabling the streaming generation of Mel-spectrograms from the chunk-size input speech tokens.  These Mel-spectrograms are then converted into speech using the vocoder, which operates in parallel with a fully convolutional network.

\begin{algorithm}[t]
    \caption{Inference in Python Style} \label{alg1}
    \KwIn{Streaming text input $\boldsymbol{y}$}
    \KwOut{Streaming speech output $\boldsymbol{o}$}
    sequence input $\boldsymbol{f}$ = [] \;
    speech tokens $\boldsymbol{s}$ = []\;
    \If{length($\boldsymbol{y}$)$>$ q}{
    \For{Index, $y$ in enumerate($\boldsymbol{y}$)}{
        $\boldsymbol{f}$ = build\_seq($y$, $\boldsymbol{f}$)\; \# Follow Equation\ref{eq1} \;
        \If{Index$ == 0$} {
            dur = TMT($\boldsymbol{f}$)  \;
            $\boldsymbol{f}$ = pad\_seq($\boldsymbol{f}$, dur) \;
            pre\_dur = dur \
        }
        $\boldsymbol{s}_{cur}$, dur = TMT($\boldsymbol{f}$) \;
        $\boldsymbol{s}$.append($\boldsymbol{s}_{cur}$) \;
        $\boldsymbol{f}$[pre\_dur-1:-1] = $s$ \;
        $\boldsymbol{f}$ = pad\_seq($\boldsymbol{f}$, dur) \;
        pre\_dur = dur\;
        \If{length($\boldsymbol{s}$)$\geq$ chunk}{
        $\boldsymbol{o}$ = decoder($\boldsymbol{s}$)\; 
        \# output new generated speech \;
        $\boldsymbol{s}$.update();
        }
    }
    }
    % \Return $\boldsymbol{y}$\;
\end{algorithm}

\subsection{Inference}
\label{sec:inference}
During the inference process, SyncSpeech processes text in a streaming manner and synchronously generates speech, with the general algorithm flow shown in Algorithm \ref{alg1}. Specifically, when the number of input text BPE tokens $\boldsymbol{y}$ exceeds the look-ahead number $q$,  the input sequence $\boldsymbol{f} = [\boldsymbol{y}, D]$ is built, which is fed into TMT to predict the duration of speech tokens corresponding to $y_1$. Then, based on the predicted duration, we perform sequence padding by inserting the mask tokens and a duration prediction placeholder. Subsequently, the sequence is fed back into TMT for synchronous mask prediction of $y_1$ and the duration prediction of $y_2$, followed by the input sequence $\boldsymbol{s}$ update and padding. For subsequent BPE token input, the above prediction step, update step, and padding step are repeated to generate speech tokens in a streaming manner.
In the process described above, once the number of generated speech tokens surpasses the chunk size of the off-the-shelf speech decoder, these tokens and the speaker prompt can be utilized to stream speech output.

Additionally, existing speech tokens can be accessed during duration prediction and speech token generation, which allows SyncSpeech to control the prosody of the generated speech with in-context learning. Specifically, given a speech prompt, we construct the prompt sequence according to Equation \ref{eq1}, serving as the generated sequence for prosody control. Figure \ref{fig2} in the Appendix shows detailed inference visualizations.