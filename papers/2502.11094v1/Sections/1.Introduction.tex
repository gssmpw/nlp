\section{Introduction}

In recent years, with advancements in generative models and the expansion of training datasets, text-to-speech (TTS) models \cite{valle, voicebox, ns3} have made breakthrough progress in naturalness and quality, gradually approaching the level of real recordings. However, low-latency and efficient dual-stream TTS, which involves processing streaming text inputs while simultaneously generating speech in real time, remains a challenging problem \cite{livespeech2}. These models are ideal for integration with upstream tasks, such as large language models (LLMs) \cite{gpt4} and streaming translation models \cite{seamless}, which can generate text in a streaming manner. Addressing these challenges can improve live human-computer interaction, paving the way for various applications, such as speech-to-speech translation and personal voice assistants.

Recently, inspired by advances in image generation, denoising diffusion \cite{diffusion, score}, flow matching \cite{fm}, and masked generative models \cite{maskgit} have been introduced into non-autoregressive (NAR) TTS \cite{seedtts, F5tts, pflow, maskgct}, demonstrating impressive performance in offline inference.  During this process, these offline TTS models first add noise or apply masking guided by the predicted duration. Subsequently, context from the entire sentence is leveraged to perform temporally-unordered denoising or mask prediction for speech generation. However, this temporally-unordered process hinders their application to streaming speech generation\footnote{
Here, “temporally” refers to the physical time of audio samples, not the iteration step $t \in [0, 1]$ of the above NAR TTS models.}.


When it comes to streaming speech generation, autoregressive (AR) TTS models \cite{valle, ellav} hold a distinct advantage because of their ability to deliver outputs in a temporally-ordered manner. However, compared to recently proposed NAR TTS models,  AR TTS models have a distinct disadvantage in terms of generation efficiency \cite{MEDUSA}. Specifically, the autoregressive steps are tied to the frame rate of speech tokens, resulting in slower inference speeds.  
While advancements like VALL-E 2 \cite{valle2} have boosted generation efficiency through group code modeling, the challenge remains that the manually set group size is typically small, suggesting room for further improvements. In addition,  most current AR TTS models \cite{dualsteam1} cannot handle stream text input and they only begin streaming speech generation after receiving the complete text,  ignoring the latency caused by the streaming text input. The most closely related works to SyncSpeech are CosyVoice2 \cite{cosyvoice2.0} and IST-LM \cite{yang2024interleaved}, both of which employ interleaved speech-text modeling to accommodate dual-stream scenarios. However, their autoregressive process generates only one speech token per step, leading to low efficiency.



To seamlessly integrate with  upstream LLMs and facilitate dual-stream speech synthesis, this paper introduces \textbf{SyncSpeech}, designed to keep the generation of streaming speech in synchronization with the incoming streaming text. SyncSpeech has the following advantages: 1) \textbf{low latency}, which means it begins generating speech in a streaming manner as soon as the second text token is received,
and
2) \textbf{high efficiency}, 
which means for each arriving text token, only one decoding step is required to generate all the corresponding speech tokens.

SyncSpeech is based on the proposed \textbf{T}emporal \textbf{M}asked generative \textbf{T}ransformer (TMT).
During inference, SyncSpeech adopts the Byte Pair Encoding (BPE) token-level duration prediction, which can access the previously generated speech tokens and performs top-k sampling. 
Subsequently, mask padding and greedy sampling are carried out based on  the duration prediction from the previous step. 

Moreover, sequence input is meticulously constructed to incorporate duration prediction and mask prediction into a single decoding step.
During the training process, we adopt a two-stage training strategy to improve training efficiency and model performance. First, high-efficiency masked pretraining is employed to establish a rough alignment between text and speech tokens within the sequence, followed by fine-tuning the pre-trained model to align with the inference process.

Our experimental results demonstrate that, in terms of generation efficiency, SyncSpeech operates at 6.4 times the speed of the current dual-stream TTS model for English and at 8.5 times the speed for Mandarin. When integrated with LLMs, SyncSpeech achieves latency reductions of 3.2 and 3.8 times, respectively, compared to the current dual-stream TTS model for both languages.
Moreover, with the same scale of training data, SyncSpeech performs comparably to traditional AR models in terms of the quality of generated English speech. For Mandarin, SyncSpeech demonstrates superior quality and robustness compared to current dual-stream TTS models. This showcases the potential of  SyncSpeech as a foundational model to integrate with upstream LLMs.

