\section{Introduction}
% 1. AI safety 很重要

% 2. jailbreak is important

% 3. 之前的jailbreak方法主要关注于单轮method，如何叠加context让query复杂来绕过LLM的alignment。对于多轮探索较少。讨论两个多轮工作actorattack, cremenal。他们通过human-cruted propmt来隐藏malicious intention。
% 但是，为什么llm在多轮中可以被jailbreak，从而绕过alignment呢？

% 4. 从人的self-corruption得到灵感 "foot-in-the-door" phenomenon，我们发现LLM的self-corruption现象，并提出了一种非常简单的self-corruption jailbreak 方法。

% 5. 方法描述

% 6. contribution
% 我们基于心理学中的self-corruption提出了一种非常简单的multi turn jailbreak 
% 超过了sota method在jailbreak bench mark，达到了average 90%的 成功率。
% 我们深入探究了LLM的self-corruption 现象


Large Language Models (LLMs) have been extensively deployed in various domains and products \citep{li2024examining}, ranging from coding assistance \citep{guo2024deepseek} to educational tools \citep{wang2024large}.
As these models become more integral to daily life, ensuring AI safety and preserving alignment with human values have become increasingly important \cite{liu2023autodan}. 
A critical challenge lies in "jailbreak", wherein adversarial prompts bypass built-in safeguards or alignment measures, causing the model to generate disallowed or harmful output~\citep{zou2023universal,jin2024multiverse}.

Early jailbreak approaches typically rely on carefully engineered single-turn prompts that coax the model to reveal restricted malicious information~\citep{greshake2023not}. By embedding malicious instructions within complex context blocks or intricate role-playing scenarios, attackers exploit weaknesses in the model alignment policy \citep{ding2023wolf}. However, attackers have recently shifted from single-turn to multi-turn paradigms, where each subsequent user query adapts or builds upon the conversation history \citep{li2024llm}. Although some multi-turn jailbreak methods, such as ActorAttack \citep{actorattck} and Crescendo \citep{crescendo}, have demonstrated the potential of multi-round dialogues in obscuring malicious intent, they usually depend on heavily handcrafted prompts or complex agent design. Besides, their overall Attack Success Rate (ASR) remains limited, often requiring significant prompt engineering expertise.


\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/intro_v2.pdf}
  \caption{An example of \Tech about hacking into an email account compared to a direct query. It bypasses alignment as the malicious intent escalates over multiple interactions.}
  \label{fig:intro}
\end{figure}

The \textit{foot-in-the-door} effect in psychology suggests that minor initial commitments lower resistance to more significant or more unethical transgressions \citep{freedman1966compliance, cialdini2001influence}, which has been widely observed in behavioral studies \citep{comello2016health}. Motivated by this insight, we ask: \textit{\textbf{Can this gradual escalation mechanism be exploited to erode the alignment of an LLM over multiple interactions?}} In other words, can we exploit the principle that once a small unethical act is committed, individuals become increasingly susceptible to larger transgressions to bypass LLMs' safeguards? 
% \xz{the following sentence can be removed. Again, I don't think self-corruption is a right word to describe it. } 
We explore whether an LLM can be coaxed into progressively self-corrupting its outputs during a multi-turn interaction. 
For example, in Figure~\ref{fig:intro}, when provided with an innocent introduction to the safety features of the officers' email, the LLM eventually produces a procedure to hack into the email account that would normally be rejected due to its potential harm.

Inspired by the process through which humans become more prone to harmful actions after exposure to minor unethical behavior \citep{festinger1957theory}, we introduce a simple yet effective method called \Tech.
Our approach begins with a benign query and then gradually escalates the maliciousness of subsequent queries by inserting intermediate "bridge" prompts that facilitate a smooth transition from benign to harmful content. 
In addition, we incorporate alignment mechanisms that nudge the model’s responses toward the intended malicious direction.
When the model’s response does not align with the current level of maliciousness in the query, we re-query the target model to prompt it to realign its response, which ensure a more gradual self-corruption, reinforcing the model’s progression toward generating increasingly harmful outputs.
% \xz{the above may need a bit more discussion as it is unclear what alignment mechanism is being used} 
These two processes are designed to progressively induce the model to lower its own guard against providing toxic responses.
% promote self-corruption attacks, enforcing the model to erode its own alignment over multi-turn interaction. 

Our contributions are summarized below:
\begin{itemize}
\item We propose a multi-turn jailbreak strategy a multi-turn jailbreak attack FITD that takes advantage of the psychological dynamics of 
% \xz{"self-corruption" => ""} 
multi-turn conversation, rooted in the foot-in-the-door effect, to exploit the inherent vulnerabilities in the alignment of LLMs.
\item We present a simple yet effective two-stage method that outperforms existing state-of-the-art approaches in two jailbreak benchmarks, achieving an average success rate of 94\% on seven widely used models.
\item We conduct an in-depth analysis of the foot-in-the-door self-corruption phenomenon in LLMs, shedding light on potential weaknesses in current safety measures and motivating future research in AI safety.
\end{itemize}




\begin{figure*}[t]
\includegraphics[width=\textwidth]{figures/main_v2.pdf}
  \caption{Overview of \Tech.The attack begins by generating Level $1$ to Level $n$ queries by an assistant model. Through multi-turn interactions, self-corruption is enhanced via Re-Align and SSParaphrase, ensuring the attack remains effective. SSParaphrase (SlipperySlopeParaphrase) refines queries by generating intermediate malicious-level queries \(q_{\text{mid}}\) between \(q_{\text{last}}\) and \(q_i\). Re-Align uses prompt \(p_{\text{align}}\) to align the target model’s responses \(r_{\text{align}}\). }
  \label{fig:main}
\end{figure*}