\section{Experiment}
\subsection{Experimental Setup}
\noindent \textbf{Target Models}
We evaluate \Tech on seven widely used LLMs, including both open-source and proprietary models. 
% \xiaolong{add citation here}
The open-source models comprise \texttt{LLaMA-3.1-8B-Instruct} \citep{dubey2024llama}, \texttt{LLaMA-3-8B-Instruct}, \texttt{Qwen2-7B-Instruct} \citep{bai2023qwen}, \texttt{Qwen-1.5-7B-Chat}, and \texttt{Mistral-7B-Instruct-v0.2} \citep{jiang2023mistral}. The closed-source models include \texttt{GPT-4o-mini} \citep{hurst2024gpt} and \texttt{GPT-4o-2024-08-06}.


\noindent \textbf{Baselines}  
We compare our approach against seven popular jailbreak methods, including DeepInception~\citep{li2023deepinception}, CodeChameleon~\citep{lv2024codechameleon}, ReNeLLM~\citep{ding2023wolf}, CodeAttack~\citep{codeattack}, CoA~\citep{coa}, and ActorAttack~\citep{actorattck}. The details of these baselines are shown in the Appendix \ref{appendix:baseline}.  

\noindent \textbf{Dataset}
We evaluate our method on two datasets: JailbreakBench~\citep{chao2024jailbreakbench}, which consists of 100 carefully selected harmful queries, and the HarmBench validation set~\citep{mazeika2024harmbench}, which includes 80 harmful queries.

\input{sections/main_table}


\noindent \textbf{Implementation Details} 
In Table \ref{tab:main}, we set the malicious level $n$ to 12 and conduct three attack attempts per query. 
We use default parameters for baselines. All open-source models are inferred with vLLM~\citep{kwon2023efficient} with default settings. 
All experiments run on an NVIDIA A100 GPU, with GPT-4o-mini as the default assistant model.

\noindent \textbf{Evaluation Metric}
To assess the effectiveness of the jailbreak attack, we employ Attack Success Rate (ASR), which quantifies the percentage of jailbreak attempts that successfully elicit a harmful response from the model. 
Specifically, we adopted the evaluation method from JailbreakBench, which leverages \texttt{GPT-4o} to assess two key aspects: the harmfulness of the generated responses and the degree of alignment between the responses and the original queries. 
Evaluation details are provided in the Appendix \ref{appendix:evaluation}. 
% Besides, Section~\ref{} presents an analysis that demonstrates a strong correlation between the automated evaluation methods used and human assessments.
% \xiaolong{edit} 





% \xiaolong{add average query num}

\subsection{Main Results}
\noindent \textbf{\Tech is more effective than baseline attacks.}
Table~\ref{tab:main} shows ASRs of \Tech and various jailbreak methods on different LLMs across JailbreakBench and HarmBench, where each cell contains two values: the ASR on JailbreakBench (left) and HarmBench (right). In Table~\ref{tab:main}, \Tech requires an average of 16 queries per malicious question for each target model.
Among single-turn attacks, ReNeLLM achieves the highest ASR on average, significantly outperforming other single-turn baselines. 
This suggests that leveraging LLMs for prompt rewriting and scenario nesting is a highly effective approach for jailbreak attacks.
Meanwhile, CodeAttack variants also demonstrate competitive performance.
However, DeepInception and CodeChameleon exhibit lower ASR, with performance dropping below 34\% on average, indicating their limited generalizability against more robust models.  
For multi-turn attacks, ActorAttack is the strongest baseline, outperforming CoA across most models, which achieves 63\%/53\% on LLaMA-3.1-8B and 58\%/50\% on GPT-4o-mini, indicating the potential of multi-turn interactions in gradually uncovering vulnerabilities and overcoming content moderation mechanisms.

Our method, \Tech, consistently outperforms all baseline attacks across all evaluated models. It achieves 98\%/93\% on LLaMA-3-8B and maintains an ASR on average 94\%/91\% across all tested models.
Notably, \Tech achieves 95\%/93\% on GPT-4o-mini and 93\%/90\% on GPT-4o, suggesting that \Tech is highly adaptable across different architectures and safety settings, demonstrating its effectiveness on both open-source and proprietary models.


% \noindent \textbf{Transferbility}
% \begin{figure}[htbp]
%   \includegraphics[width=\columnwidth]{figures/transfer.pdf}
%   \caption{transferbility.}
%   \label{fig:transfer}
% \end{figure}


% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=0.45\linewidth]{figures/transfer.pdf} \hfill
%   \includegraphics[width=0.45\linewidth]{figures/ablation_study.pdf} \hfill
%   \caption{A minimal working example to demonstrate how to place three images side-by-side. \xiaolong{change number here}}
%   \label{fig:three_images}
% \end{figure*}



\begin{figure*}[t]
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/transfer.pdf}
    \caption{Transfer attack}
    \label{fig:transfer}
  \end{subfigure} 
  \hfill
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/ablation_study.pdf}
    \caption{Ablation study}
    \label{fig:ablation}
  \end{subfigure} 
  \hfill
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/defense_evaluation.pdf}
    \caption{Defense}
    \label{fig:defense}
  \end{subfigure}
  \caption{
  (a) Transfer attacks using jailbreak chat histories
generated from LLaMA-3.1-8B and GPT-4o-mini as source models on JailbreakBench.
  (b) Ablation study of three components in \Tech, response alignment (Re-Align), alignment prompt \(p_{align}\), and SlipperySlopeParaphrase(SSP) on JailbreakBench. 
  (c) ASR under different defense methods on JailbreakBench. 
  % \xiaolong{edit}
  }
\end{figure*}




% \noindent \textbf{Transferability}
\noindent \textbf{\Tech demonstrates strong cross-model transferability.}
To evaluate the cross-model transferability of \Tech, we conduct transfer attacks using adversarial chat histories generated from LLaMA-3.1-8B and GPT-4o-mini as source models. 
Specifically, for each query in JailbreakBench, we utilize the progressively malicious query-response history obtained while attacking the source model and directly apply it to other target models. This allows us to assess whether adversarial chat histories obtained on one model can successfully bypass the safety mechanisms of others.  

As shown in Figure~\ref{fig:transfer}, LLaMA-3.1 jailbreak chat histories exhibit strong transferability across models, achieving 76\% ASR on Mistral-v0.2 and 74\% on Qwen-2-7B, indicating that open-source models are particularly vulnerable to transferred adversarial queries. 
GPT-4o-mini (70\%), which has stronger moderation mechanisms, remains susceptible to attack histories crafted on LLaMA-3.1.
Interestingly, when GPT-4o-mini serves as the attack source model, the ASR further improves in most models, with Mistral-v0.2 reaching 85\%. This suggests that attacks originating from a more robust model tend to transfer more effectively because the initial stronger safety alignment forces the attack to develop more adaptable and generalizable jailbreak strategies. 
However, Qwen-1.5-7B (64\%) exhibits slightly stronger resistance under GPT-4o-mini transfer, potentially due to model-specific safety filtering techniques.  
Overall, these results highlight a critical weakness in current LLMs' safety defenses: attack histories created on one model can consistently exploit vulnerabilities in others. In particular, closed-to-open transfer (GPT-4o-mini → open-source models) is particularly effective, demonstrating that even models with strict safety protocols can unintentionally generate adversarial sequences that break other systems. 





% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=0.32\linewidth]{figures/transfer.pdf} \hfill
%   \includegraphics[width=0.32\linewidth]{figures/ablation_study.pdf} \hfill
%   \includegraphics[width=0.32\linewidth]{figures/defense_evaluation.pdf}
%   \caption{A minimal working example to demonstrate how to place three images side-by-side.}
%   \label{fig:three_images}
% \end{figure*}



\subsection{Ablation Study}
To evaluate different components in our \Tech jailbreak method, we conduct an ablation study by systematically removing three key mechanisms: response alignment (Re-Align), alignment prompt $p_{align}$, and SlipperySlopeParaphrase. 
The results in Figure~\ref{fig:ablation} demonstrate the significance of these components for a high ASR across various models.  

First, removing all three mechanisms leads to a significant decline in ASR (w/o ReAlign, $p_{align}$, SSP). 
On LLaMA-3.1, the ASR drops from 92\% to 75\%, while on LLaMA-3, it decreases from 98\% to 59\%. Similar declines are observed across other models, with Qwen-2 and Qwen-1.5 dropping to 76\% and 80\%, respectively. These results suggest that the interplay of response alignment, prompt alignment, and paraphrasing is critical to maintaining the effectiveness of \Tech. Without these components, the attack becomes substantially less effective, particularly on models with stronger alignment guardrails.  

Second, when the response alignment and prompt alignment are removed (w/o ReAlign, $p_{align}$), the ASR remains relatively high, but it still exhibits some degradation. On LLaMA-3.1, the ASR remains at 91\%, suggesting that paraphrasing alone can compensate for some of the lost effectiveness. However, on LLaMA-3, the ASR drops from 98\% to 63\%, indicating that paraphrasing is insufficient against models with stricter safeguards. Similar patterns are observed in Qwen-2 and Qwen-1.5, where the ASR decreases to 75\% and 81\%, respectively. 
These results highlight that, while paraphrasing can mitigate the impact of removing alignment techniques, it cannot fully substitute for them, especially on models with more robust defenses.  

Third, removing only response alignment (w/o ReAlign), we observe a relatively minor decrease in performance.
LLaMA-3.1 and Qwen-2 maintain their ASR at 92\% and 75\%, respectively, while LLaMA-3 shows a decrease from 98\% to 79\%. 
The impact is more evident on Qwen-1.5 and Mistral-v0.2, with ASR decreasing from 94\% to 83\% and from 96\% to 90\%, respectively, showing that response alignment is beneficial for a gradual erosion of the model’s safeguard, aligning with the psychological principle of incremental compliance.

Overall, the ablation study demonstrated response alignment (Re-Align), alignment prompt $p_{align}$, and SlipperySlopeParaphrase as essential for high jailbreak success. 
Response alignment is crucial for bypassing safeguards while paraphrasing also gradually erodes the model's alignment.




% \begin{figure*}[t]
%   \centering
%   \includegraphics[width=0.32\linewidth]{figures/level_n_vs_asr.pdf} \hfill
%   \includegraphics[width=0.32\linewidth]{figures/harmfulness_vs_length.pdf} \hfill
%   \includegraphics[width=0.32\linewidth]{figures/harmfulness_heatmap.pdf} \hfill
%   \caption{A minimal working example to demonstrate how to place three images side-by-side. \xiaolong{edit right fig}}
%   \label{fig:two}
% \end{figure*}



\begin{figure*}[t]
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/level_n_vs_asr.pdf}
    \caption{ASR across different malicious levels}
    \label{fig:level_n_vs_asr}
  \end{subfigure} 
  \hfill
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/harmfulness_vs_length.pdf}
    \caption{Harmfulness of different level}
    \label{fig:harmfulness_vs_length}
  \end{subfigure} 
  \hfill
  \begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/query_retention_vs_asr.pdf}
    % \caption{Impact of query in different stages.}
    \caption{ASR across different stages queries}
    \label{fig:query_retention_vs_asr} 
  \end{subfigure}
  \caption{
  (a) ASR with different malicious levels \( n \) across models.
  (b) The harmfulness score of responses $r_i$ at $q_i$ in different malicious levels $i$ across models. 
  (c) ASR versus the number of queries retained for two extraction strategies: Backward Extraction and Forward Extraction. Backward extraction retains later-stage queries while removing earlier ones, whereas forward extraction adds early-stage queries but always includes the final high-malicious query.
  }
\end{figure*}


% \begin{figure*}[t]
%   \centering
%   \begin{subfigure}[b]{0.32\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/level_n_vs_asr.pdf}
%     \caption{Caption for (a)}
%     \label{fig:subfig1}
%   \end{subfigure} 
%   \hfill
%   \begin{subfigure}[b]{0.32\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/harmfulness_heatmap.pdf}
%     \caption{Caption for (b)}
%     \label{fig:subfig2}
%   \end{subfigure} 
%   \hfill
%   \begin{subfigure}[b]{0.32\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/len_backward_forward_order_comparison.pdf}
%     \caption{Caption for (c)}
%     \label{fig:subfig3}
%   \end{subfigure}
%   \caption{A minimal working example to demonstrate how to place three images side-by-side. \xiaolong{edit right fig}}
%   \label{fig:two}
% \end{figure*}




\noindent \textbf{The impact of malicious level \( n \).}
We conduct experiments across multiple models to evaluate the impact of the malicious level \( n \) on ASR.
The results show a clear trend: as \( n \) increases, ASR improves, reaching its peak around \( n = 9 \) to \( n = 12 \). 
However, beyond this point, the improvement plateaus and in some cases the ASR fluctuates slightly at \( n = 15 \), possibly due to the increasing length and complexity of the generated context. 
Among the models, LLaMA-3.1-8B and GPT-4o-mini require higher \( n \) values (\( n = 12 \)) to achieve optimal ASR, while LLaMA-3-8B and Qwen2-7B reach peak ASR earlier (\( n = 9 \)), indicating different levels of robustness. 
Qwen-1.5-7B and GPT-4o-mini exhibit more variance at \( n = 15 \), indicating that over-paraphrasing or excessive manipulation introduces inconsistencies that reduce attack efficacy. Although increasing \( n \) improves ASR across all models, the effect saturates beyond \( n = 12 \), implying a trade-off between attack complexity and effectiveness.
Future work could explore adaptive malicious level selection based on model-specific vulnerabilities to maximize ASR while minimizing unnecessary complexity and queries.



\noindent \textbf{Later stage malicious query progression matters.}
To analyze the relative importance of different stages within the self-corruption process, we conduct experiments that extract subsets of the chat history \( \mathcal{H} \) and evaluate their impact on ASR. We compare two extraction strategies: \textit{backward extraction}, where we retain only the later-stage queries while progressively removing earlier ones (e.g., retaining 4 queries: \( 9\!\to\!10\!\to\!11\!\to\!12 \); 6 queries: \( 7\!\to\!8\!\to\!9\!\to\!10\!\to\!11\!\to\!12 \); 8 queries: \( 5\!\to\!6\!\to\!7\!\to\!8\!\to\!9\!\to\!10\!\to\!11\!\to\!12 \), etc.), and \textit{forward extraction}, where we incrementally add early-stage queries but always include a final high-malicious query at \( n=12 \) (e.g., 4 queries: \( 1\!\to\!2\!\to\!3\!\to\!12 \); 6 queries: \( 1\!\to\!2\!\to\!3\!\to\!4\!\to\!5\!\to\!12 \); 8 queries: \( 1\!\to\!2\!\to\!3\!\to\!4\!\to\!5\!\to\!6\!\to\!7\!\to\!12 \), etc.). 
Figure \ref{fig:query_retention_vs_asr} shows that high-malicious queries in the later stage contribute more to attack success, while early-stage prompts alone are insufficient to trigger model vulnerability. Additionally, we observe that maintaining a smooth increase of malicious levels is crucial for foot-in-the-door self-corruption. When the gap between malicious levels in the query sequence is too large (e.g., jumping from n=3 directly to n=12 in forward extraction), the attack is significantly less effective than when queries gradually escalate harmfulness because early queries do not build up effectively to the final query. 
In contrast, backward extraction with a more consistent escalation of malicious level remains effective even when earlier queries are removed, showing the smoother and more continuous self-corruption process weakens the model’s alignment mechanisms.



\noindent \textbf{Defense}
Figure~\ref{fig:defense} shows ASR of \Tech across models under different defense strategies. 
OpenAI-Moderation reduces ASR slightly by 3\%-8\%. LLaMA-Guard-2~\citep{llamaguard} offers a stronger defense, lowering ASR to 79\%-91\%. LLaMA-Guard-3~\citep{llamaguard} further improves moderation, achieving the lowest ASR 78\%-84\%.
LLaMA-Guard-3 consistently outperforms other methods, but ASR remains significant.
We speculate that progressively malicious queries and responses bypassed the detector, indicating room for further improvement in moderation techniques.

% \xiaolong{wait graph}



% \begin{figure}[htbp]
%   \includegraphics[width=\columnwidth]{figures/defense_evaluation.pdf}
%   \caption{defense \xiaolong{test}}
%   \label{fig:defense}
% \end{figure}



\noindent \textbf{Harmfulness of different malicious level response.}
To assess the impact of increasing the malicious level on the harmfulness of model's responses, we use the chat history of malicious level $n=12$ experiment in Table~\ref{tab:main} and analyze the harmfulness of responses at each level across multiple LLMs.
The harmfulness is measured by score 1-5, where a higher score indicate greater harmfulness. We report the mean harmfulness scores for each model at malicious level $i$ ranging from 1 to 12. 
Figure~\ref{fig:harmfulness_vs_length} presents the harmfulness scores of responses at different malicious levels for all evaluated models. 
We use GPT-4o to score each response via prompt~\citep{codeattack} shown in Appendix~\ref{appendix:Harmfulness Evaluation Prompt}.
We observe that the harmfulness scores generally increase with the malicious level. 
At $i=1$, the harmfulness scores are relatively low, with values around 2.32 on average across models. 
However, as the level increases, the harmfulness score consistently rises to 4.23 on average at $i=12$.
These results show that as the malicious level increases, LLMs become more vulnerable and generate more harmful responses, suggesting that model's alignment weakens over time, making it easier for \Tech to bypass safeguards. 



% \subsection{Case Study}

% \subsection{Evaluation Human Study}