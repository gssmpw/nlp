% \begin{figure*}[t]
%   \includegraphics[width=\textwidth]{figures/main.pdf}
%   \caption{main.}
%   \label{fig:main}
% \end{figure*}

\section{Method}

\subsection{Inspiration from Psychology: The Foot-in-the-Door Phenomenon}

Our method \Tech draws inspiration from the "foot-in-the-door" phenomenon in psychology. 
According to this principle, once individuals perform or agree to a minor (often unethical) act, they are more likely to proceed with more significant or harmful acts afterward \citep{freedman1966compliance, cialdini2001influence}. 
For example, in a classic study, participants who first displayed a small sign supporting safe driving were subsequently much more inclined to install a much larger, more obtrusive sign \citep{freedman1966compliance}. This gradual escalation of compliance, "from small to large", has also been observed in other forms of unethical or harmful behavior \citep{festinger1957theory}, showing that the initial "small step" often lowers psychological barriers for larger transgressions. Once a small unethical act has been justified, individuals become increasingly susceptible to more severe transgressions.

Based on these insights, we hypothesize that LLMs' safety mechanisms might be vulnerable to a gradual escalation strategy. If LLMs respond to a prompt containing slightly harmful content, subsequent queries that escalate this harmfulness will have a higher chance of producing disallowed responses. This idea underlies our \Tech method, which progressively coaxes a target model to produce increasingly malicious output despite its built-in safety mechanisms.







\subsection{{Overview}}
Building on the \textit{foot-in-the-door} perspective, we design a multi-turn jailbreak strategy \Tech. In each turn, the target model is prompted with content that is just marginally more harmful or disallowed than the previous turn, encouraging the model to produce a correspondingly more harmful output. 
This progression method is designed to exploit the model’s own responses as a guiding force to bypass its safety alignment or content filters. The core novelty lies in using (i) the model’s own prompts and responses as stepping stones for further escalation and (ii) two auxiliary modules, SlipperySlopeParaphrase and Re-Align, to handle instances when the model refuses or produces outputs misaligned with the intended maliciousness. Additionally, we conduct an in-depth analysis of the \textit{foot-in-the-door} self-corruption phenomenon in LLMs.


Figure~\ref{fig:main} shows the overview of our method.
First, we initialize a sequence of escalated queries $q_1, q_2, \dots, q_n$ based on a malicious query $q^*$.
Then in each turn, we append the current query $q_i$ to the chat history and obtain the model’s response $r_t$. 
If $r_t$ has no refusal, we proceed; otherwise, we check how well the model’s previous response aligns with its prompt. 
Depending on this check, we either insert an intermediate “bridging” query via SlipperySlopeParaphrase or Re-Align the target model's last response $r_{last}$.
Over multiple iterations, the process gradually pushes the model to produce more detailed and harmful content.




% \subsection{\hyperref[alg:jailbreak]{\Tech}}
\subsection{\texorpdfstring{\hyperref[alg:jailbreak]{\Tech}}{\Tech}}
As shown in Algorithm \ref{alg:jailbreak}, given a target model $M$, a malicious “goal” query $q^*$, and the malicious level $n$, we initialize a sequence of escalated queries $q_1, q_2, \dots, q_n$ by $\text{getLevelQuery}(n, q^*)$ based on a malicious query $q^*$ (line 2).
Then we maintain a chat history $\mathcal{H}$ (line 3) and iterate from $i=1$ to $n$. At each turn, we add $q_i$ to $\mathcal{H}$ (line 5) and query the model for a response $r_i$ (line 6). 
If the model responds to the query (line 7), we include $r_t$ in the chat history $\mathcal{H}$ (line 8). 
Instead, if the model refuses (line 9), we remove the current query $q_i$ (line 11) and extract the last query-response pair $(q_{\mathrm{last}}, r_{\mathrm{last}})$ from $\mathcal{H}$ (line 12). 

Now, we need to utilize SlipperySlopeParaphrase and Re-Align to enforce the model to continue self-corruption.
Therefore, we first check how well the model’s last response aligns with its prompt (lines 13). 
If $r_{\mathrm{last}}$ does not align with $q_{\mathrm{last}}$, we use \hyperref[alg:Re-Align]{Re-Align} to generate a revised and more aligned version of the last response (line 16).
Otherwise, we utilize \hyperref[alg:SlipperySlopeParaphrase]{SlipperySlopeParaphrase} (line 14) to insert an intermediate bridging prompt $q_{mid}$ between $q_{i-1}$ and $q_i$.



% \subsubsection{\hyperref[alg:Re-Align]{Re-Align}}
\subsubsection{\texorpdfstring{\hyperref[alg:Re-Align]{Re-Align}}{Re-Align}}
If the model’s previous query $q_{\text{last}}$ and response $r_{\mathrm{last}}$ in chat history $\mathcal{H}$ is misaligned-for instance, it remains too benign or partially refuses even though the query is not malicious-then we invoke \textsc{Re-Align}. Building on the psychological insight that once individuals have justified a minor unethical act, they become increasingly susceptible to more severe transgressions \citep{freedman1966compliance}, \textsc{Re-Align} aims to "nudge" the model to produce a response more closely aligned with the malicious intent of $q_{\mathrm{last}}$.
Specifically, we employ a predefined alignment prompt $p_{\mathrm{align}}$ via $\texttt{getAlignPrompt}(q_{\mathrm{last}}, r_{\mathrm{last}})$, appending it to $\mathcal{H}$ before querying the model $\mathcal{T}$ again. 
The alignment prompt explicitly points out inconsistencies between the last query $q_{\mathrm{last}}$ and response $r_{\mathrm{last}}$ while encouraging the model to stay consistent with multi-turn conversation. For example, if $r_{\text{last}}$ is too cautious or is in partial refusal, $p_{\text{align}}$ will suggest that the model refines its response to better follow the implicit direction.
Therefore, this procedure progressively aligns $q_{\text{last}}$ and $r_{\mathrm{last}}$, thereby furthering the self-corruption process.
The details are described in Appendix~\ref{appendix:ReAlign}
% \xz{unclear what you do here? you should at least tell peoplle how this alignprompt is able to solve your problem.
% What does an alignprompt look like?}



\begin{algorithm}[hbp]
\caption{\Tech Jailbreak}
\label{alg:jailbreak}
\begin{algorithmic}[1]

\REQUIRE Malicious query $q^*$, a target model $\mathcal{T}$, malicious level $n$, assistant model $\mathcal{M}$
\ENSURE Jailbroken result

\STATE \cmt{// Generate $n$ queries with increasing malicious levels.}
\STATE $q_1, q_2, \dots, q_n \gets \text{getLevelQuery}(n, q^*, \mathcal{M}) $ 

\STATE $\mathcal{H} \gets \{\}$ \cmt{// Initialize the chat history for $\mathcal{T}$}
\FOR{$i = 1$ to $n$}
    \STATE $\mathcal{H} \gets \mathcal{H}\text{.}\text{add}(q^0_i)$
    \STATE $r_i \gets \mathcal{T}(\mathcal{H})$
    \IF{not $\text{isRejection}(r_i)$}
        \STATE $\mathcal{H} \gets \mathcal{H}\text{.}\text{add}(r_i)$
    \ELSE
        \STATE \cmt{// Remove rejected query from history.}
        \STATE $\mathcal{H} \gets \mathcal{H}\text{.}\text{pop}(q_i)$
        \STATE $(q_{\mathrm{last}}, r_{\mathrm{last}}) \gets \text{LastQueryResponse}(\mathcal{H})$
        \IF{$\text{isAlign}(r_{\text{last}}, q_{\text{last}})$}
            \STATE $\mathcal{H} \gets \text{SSParaphrase}(q_i, \mathcal{H},\mathcal{M})$
        \ELSE
            \STATE $\mathcal{H} \gets \text{Re-Align}(\mathcal{H})$
        \ENDIF
    \ENDIF
\ENDFOR
\STATE \cmt{// SSParaphrase: Short for SlipperySlopeParaphrase.}
\STATE \cmt{// LastQueryResponse: Retrieve last query-response pair of chat history.}
\STATE \cmt{// isAlign: Check if last response aligns with last query by the assistant model $\mathcal{M}$.}
\STATE \cmt{// isRejection: Checks if response is a refusal by the assistant model $\mathcal{M}$.}
\end{algorithmic}
\end{algorithm}



\begin{algorithm}
\caption{SlipperySlopeParaphrase}
\label{alg:SlipperySlopeParaphrase}
\begin{algorithmic}[1]
\REQUIRE Malicious level $i$ query $q_i$, Chat history of target model $\mathcal{H}$, assistant Model $\mathcal{M}$
\STATE $q_{\text{last}} \gets \mathcal{H}$ 
\STATE $q_{\text{mid}} \gets \text{getMid}(q_{\text{last}}, q_i)$ 
\STATE $\mathcal{H} \gets \mathcal{H}\text{.}\text{add}(q_{\text{mid})}$
\STATE $r_{\text{mid}} \gets \mathcal{T}(\mathcal{H})$

\IF {$\text{isRejection}(r_{\text{mid}})$}
    \STATE $\mathcal{H} \gets \mathcal{H}\text{.}\text{pop}(q_{\text{mid})}$ 
    \STATE $\mathcal{H} \gets \text{paraphrase}(q_{\text{mid}}, \mathcal{H}, \mathcal{M})$ \cmt{// Modify query to bypass rejection.}
\ELSE
    \STATE $\mathcal{H} \gets \mathcal{H}\text{.}\text{add}(r_{\text{mid}})$
\ENDIF

\RETURN $\mathcal{H}$ \cmt{// Return updated history.}

\STATE \cmt{// getMid: Generates $q_{\text{mid}}$ with intermediate malicious level by the assistant model $\mathcal{M}$.}
\STATE \cmt{// isRejection: Checks if $r_{\text{mid}}$ is a refusal by the assistant model $\mathcal{M}$.}

\end{algorithmic}
\end{algorithm}



% \subsubsection{\hyperref[alg:SlipperySlopeParaphrase]{SlipperySlopeParaphrase}}
\subsubsection{\texorpdfstring{\hyperref[alg:SlipperySlopeParaphrase]{SlipperySlopeParaphrase}}{SlipperySlopeParaphrase}}

When a refusal occurs and the last response $r_{\mathrm{last}}$ remains aligned with its query $q_{\mathrm{last}}$, we insert a bridge prompt $q_{\mathrm{mid}}$ to ease the model into accepting a more harmful request. 

Specifically, we obtain $q_{\mathrm{mid}} \gets \text{getMid}(q_{\mathrm{last}}, q_i)$ from an assistant model $\mathcal{M}$ so that its maliciousness level falls between $q_{\mathrm{last}}$ and $q_i$. 
We then query the target model with $q_{\mathrm{mid}}$; if the model refuses again, we paraphrase $q_{\mathrm{mid}}$ repeatedly until acceptance. 
Once the model provides a valid response $r_{\mathrm{mid}}$, we incorporate both $q_{\mathrm{mid}}$ and $r_{\mathrm{mid}}$ into the chat history $\mathcal{H}$. 
This incremental bridging step parallels the \emph{foot-in-the-door} phenomenon \citep{freedman1966compliance}, in which acceptance of a smaller request facilitates compliance with a subsequent, more harmful one.
The details are shown in Appendix~\ref{appendix:SlipperySlopeParaphrase}




\begin{algorithm}
\caption{Re-Align}
\label{alg:Re-Align}
\begin{algorithmic}[1]
\REQUIRE chat history of target model $\mathcal{H}$

\STATE $q_{\text{last}}, r_{\text{last}} \gets \text{LastQueryResponse}(\mathcal{H})$ 
\STATE $p_{\text{align}} \gets \text{getAlignPrompt}(q_{\text{last}}, r_{\text{last}})$ \cmt{// Generate predefined alignment prompt.}
\STATE $\mathcal{H} \gets \mathcal{H}\text{.}\text{add}(p_{\text{align}})$ 
\STATE $r_{\text{align}} \gets \mathcal{T}(\mathcal{H})$ \cmt{// Model re-align its response.}
\STATE $\mathcal{H} \gets \mathcal{H}\text{.}\text{add}(r_{\text{align}})$ \cmt{// Append the aligned prompt and response to chat history.}
\end{algorithmic}
\end{algorithm}




\subsubsection{Putting It All Together}
Through gradual increases in maliciousness, we systematically steer the target model from benign or slightly harmful content to overtly disallowed response. The model’s own responses serve as stepping stones, mirroring how individuals who commit a small unethical act become more prone to larger transgressions \citep{festinger1957theory}. Thus, \Tech employs the psychological \emph{foot-in-the-door} mechanism and adapts it for multi-turn LLM red-teaming.
By combining these modules, we show that even well-aligned LLMs can be coaxed into producing harmful outputs if the escalation is gradual and carefully structured. 


% \input{sections/algorithm}
