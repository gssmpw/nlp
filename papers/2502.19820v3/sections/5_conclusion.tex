\section{Conclusion}
In this work, we introduced \Tech, a multi-turn jailbreak strategy inspired by the psychological foot-in-the-door effect.
By progressively escalating the malicious intent of user queries through intermediate prompts via SlipperySlopeParaphrase and ReAlign, our method achieves a 94\% attack success rate on average across multiple models.
Our findings reveal a major weakness in current AI safety measures: LLMs can be manipulated into self-corruption, where their responses gradually shift toward harmful content by themselves.
Future work could explore why LLMs have the foot-in-the-door self-corrupt phenomenon that their responses shift during adversarial interactions and FITD effect in multimodality Language Models.
To prevent this, researchers can develop real-time adaptive monitoring and better alignment methods that strengthen model alignment in multi-turn conversations.



