% \begin{abstract}
% % \xz{I don't like the term self-corruption. It is not about the LLM self-corrupting. It is being induced to lower its guard progressively. The title could be "Foot-In-The-Door Attack: A Multi-turn Jailbreak for LLMs" Note that slippery slope is not the key idea but rather a way to realize it}

% Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. 
% Inspired by psychological foot-in-the-door principles, we introduce \Tech,
% % {} attack, \xz{maybe just FITD} 
% a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical transgressions.
% Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses.
% % \xz{to induce toxic responses}
% Extensive experimental results on two jailbreak benchmarks demonstrate that \Tech~achieves an average attack success rate of 94\% across seven widely used models, outperforming existing state-of-the-art methods.
% Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn interactions.
 

% \textbf{\textcolor{red}{Responsible Disclosure: We have shared our findings with OpenAI and Meta and discussed the ethical implications.
% }} 

% \end{abstract}


\begin{abstract}
Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. 
Inspired by psychological foot-in-the-door principles, we introduce \Tech,
a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical transgressions.
Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses.
Extensive experimental results on two jailbreak benchmarks demonstrate that \Tech~achieves an average attack success rate of 94\% across seven widely used models, outperforming existing state-of-the-art methods.
Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn interactions. The code is available at \href{https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak}{https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak}.

\textbf{\textcolor{red}{Responsible Disclosure: We have shared our findings with OpenAI and Meta and discussed the ethical implications.
}} 
\end{abstract}