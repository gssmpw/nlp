\section{Related Work}
\noindent \textbf{Jailbreak}
Large language models jailbreak can be broadly categorized into single-turn and multi-turn approaches, with different levels of model access. Black-box single-turn attacks use input transformations to bypass safety constraints without accessing model internals, such as encoding adversarial prompts in ciphers, low-resource languages, or code~\citep{yuan2023gpt, deng2023multilingual, lv2024codechameleon, ren2024codeattack,chao2023jailbreaking,wei2023jailbreak,li2023deepinception,liu2023autodan}. 
% In addition, human-like interaction strategies, including hypothesis formulation, persuasion, and psychological manipulation, have successfully avoided safeguards \citep{li2023deepinception,liu2023autodan}. 
In contrast, white-box single-turn attacks exploit access to model parameters using gradient-based optimization to generate adversarial inputs or manipulate text generation configurations~\citep{zou2023universal,huang2023catastrophic,zhang2024jailbreak,jones2023automatically,guo2024cold}. Meanwhile, multi-turn jailbreaks introduce new challenges by exploiting dialogue dynamics. A common approach decomposes harmful queries into a series of innocuous sub-questions, progressively leading the model towards unsafe responses~\citep{li2024multi,redqueen,zhou2024speak}. Automated red teaming has also been explored, in which LLMs are used iteratively to investigate and expose weaknesses~\citep{jiang2024automated}. 
To mitigate such threats, various defense mechanisms have been proposed, including perturbation or optimization techniques \citep{zheng2024prompt,zhou2024robust,mo2024fight,LiuWXWSWC0B24}, safety response strategy \citep{zhang2023defending,lirain,wang2024enhancing,zhangparden}, and jailbreak detection \citep{hanwildguard,llamaguard}, aim to neutralize adversarial prompts before execution ~\citep{llamaguard, CB}. Notably, multi-turn attack Crescendo~\citep{crescendo} and ActorAttack~\citep{actorattck} incrementally steer seemingly benign queries toward harmful content but are constrained by their reliance on fixed, human-crafted seed prompts and limited overall ASR.
However, unlike their work, our work uses the foot-in-the-door effect to gradually erode an LLMâ€™s alignment while systematically analyzing the phenomenon of self-corruption. A detailed comparison with other FITD-based strategies can be found in the Appendix \ref{appendix:diff_work}.



