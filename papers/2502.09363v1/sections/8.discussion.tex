\section{Discussion}
\label{sec:discussion}

FIX labeling has been employed in many works, with varying degrees of complexity. Theorem~\ref{thm:fix_optimal_query_length} provides a useful rule of thumb for selecting the best segmentation length for a given event length, and Eq.~\ref{eq:expected_query_iou_distribution} provides a way to use this theorem to analyze stochastic event length distributions. Our results suggest that, in most cases, knowing the average event length provides a good estimate, but understanding the (approximate) distribution of event lengths improves the analysis.

\textbf{Implications for practical annotation.} The analysis highlights the trade-offs in label accuracy and annotation cost between FIX and ORC weak labeling. While FIX can be less costly under specific conditions (e.g., high event density), these conditions are unlikely to occur in real-world annotation tasks. Furthermore, even in cases where FIX is less costly, its significantly lower label accuracy ($f^*(0.5) \approx 0.76$ vs. $1.0$ for ORC) can negate its cost advantage. ORC, on the other hand, guarantees higher accuracy at a potentially higher cost, which is sensitive to overestimation of $B_{\text{ORC}}$. 

Given the rarity of extreme event densities and the importance of high-quality labels, ORC is likely the better theoretical choice for most annotation tasks. However, ORC weak labeling is not available in practice since it uses the true change points of the events. An interesting research direction is to model the ORC weak labeling process by estimating these change points, construct query segments from these, and then let the annotator weakly label these query segments. \citet{Martinsson2024} propose to actively model the ORC weak labeling process during the annotation for sound event detection, and \citet{Kim2023} propose a related framework for image segmentation. However, how to model ORC weak labeling reliably in practice remains an open research question, as it may introduce unwanted bias due to annotation errors, overfitting to sparse events, or limitations in budget estimation models. In this regard, FIX weak labeling is very robust and provides a straightforward baseline that prioritizes simplicity and consistency.

Future research should focus on mitigating the potential biases when modeling ORC weak labeling while retaining its theoretical advantages. By addressing these challenges, annotation methods can better align with the practical needs of sound event detection and related applications.

\textbf{Understanding the consequences of evaluating with noisy labels.}
Despite the extensive focus on noisy training labels, evaluation labels are often implicitly assumed to be perfect. However, as emphasized in the introduction, inaccurate evaluation labels present a significant challenge. Crucially, when noise is present in both training and evaluation data, we risk selecting models that merely replicate the evaluation noise, potentially overlooking those with superior generalization abilities. This echoes the central concern highlighted by \citet{GÃ¶rnitz2014}, which will be described below. To illustrate this point concretely, consider the implications of evaluating with noisy labels as described by our theorem.

We can use Theorem~\ref{thm:max_iou} to understand the properties of the best performing model when the evaluation data contains FIX weak labels. For example, we now know from Theorem~\ref{thm:max_iou} that for $\gamma=0.5$ the annotations will at most have an expected label accuracy of $f^*(0.5) \approx 0.76$. The ``best'' performing model will therefore be a model that mimics the noise in these labels. We may end up rejecting a model that has learned the clean (accurate) labels simply because our evaluation labels are inaccurate. The theory therefore provides a better understanding for the ``best'' performing model under FIX weak labels.

\textbf{$f^*(\gamma)$ as an upper bound.}
The expression for expected label accuracy derived in this paper applies to the simplest scenario, where only a single event with deterministic length is present. In all of our results, we observe that $f^*(\gamma)$ is greater than or equal to the expected and average label accuracy that FIX weak labeling achieve for more complex distributions. This suggests that $f^*(\gamma)$ can be considered an upper bound. However, a formal proof showing that adding more events or introducing event length variability leads to a harder distribution to annotate is beyond the scope of this paper.

\textbf{Extending the theory to more dimensions.}
The theory we have derived is for data annotation in one dimension, in particular we exemplify with events in time for audio data. However, the framework can be extended to more dimensions. Annotation in more than three dimensions is hard, since human intuition starts to break down and designing annotation interfaces will be hard, so for most practical applications extending this theory to events that occur in three dimensions should be enough. With minor adjustments, this framework can be applied to FIX weak labeling of rectangles in images or cubes in point clouds, expanding its relevance to broader areas. This extension could open up new avenues for research in multi-dimensional label noise and help optimize annotation strategies in these domains.

\textbf{Multiple different presence classes.}
If the same presence criterion $\gamma$ is applicable for all classes then Theorem~\ref{thm:expected_iou} is applicable to the joint event length distribution of these different classes. The integration performed in Eq.~\ref{eq:expected_query_iou_distribution} can be adapted for the joint distribution, assuming it can be estimated. However, real-world presence criteria for different event classes may vary, requiring more complex models to account for these discrepancies. Future empirical studies on annotator behavior could help refine this model and improve its practical applicability.

%\textbf{Understanding segment-based evaluation criteria.}
%The theory presented here is also relevant for evaluating sound event detection (SED) methods. Common evaluation metrics, such as the segment-based F$_1$ score~\citep{Mesaros2016}, divide audio into fixed-length segments and label them based on overlap with ground truth. Assuming ground truth labels for evaluation we effectively have an annotator that can detect the presence of arbitrarily small event fractions ($\gamma \rightarrow 0$). The expected label accuracy becomes $f(d_q) = d_e / (d_e + d_q)$, where $d_q$ is the segment length. This formula suggests that a small $d_q$ (approaching zero) leads to minimal segment label noise, but choosing a very small segment length comes at a computational cost even during evaluation. The theory can help inform these trade-offs, ensuring that the accepted level of absence sound in presence labels is satisfactory for the given application.

\textbf{Understanding segment-based evaluation criteria.}
The theory presented here is also relevant for evaluating sound event detection (SED) methods. Common evaluation metrics, such as the segment-based F$_1$ score~\citep{Mesaros2016}, divide audio into fixed-length segments and label them based on overlap with ground truth. A key reason for using segment-based evaluation is that ground truth annotations are often temporally noisy or imprecise, and evaluating over longer segments helps to mitigate the impact of this temporal uncertainty. Assuming ground truth labels for evaluation, we effectively have an annotator that can detect the presence of arbitrarily small event fractions ($\gamma \rightarrow 0$). The expected label accuracy becomes $f(d_q) = d_e / (d_e + d_q)$, where $d_q$ is the segment length. This formula suggests that a small $d_q$ (approaching zero) leads to minimal segment label noise, but choosing a very small segment length negates the noise reduction effect and also comes at a computational cost. The theory presented here can help inform such trade-offs. %, ensuring that the accepted level of absence sound in presence labels is satisfactory for the given application.

