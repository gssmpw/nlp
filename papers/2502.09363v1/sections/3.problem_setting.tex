\section{Problem Setting}

%How can we effectively label time-localized sound events in real-world scenarios where annotators have limited time and expertise? This question motivates our study, where we model a data distribution and an annotator capable of weakly labeling the presence or absence of events within data segments. Our primary interest lies in evaluating the accuracy of these presence annotations under varying assumptions about the annotator model and the data distribution.

The analysis is framed within a multi-pass binary labeling setting. Here, an annotator assigns binary labels (presence or absence) to data segments based on the occurrence of specific sound events. The annotator model abstracts how an annotator interacts with data by labeling segments, without requiring precise knowledge of event boundaries. While inspired by time-localized and non-stationary sound events, this framework is generalizable to any time series with similar characteristics.

It's important to emphasize that, in this weak labeling setting, the concept of overlapping events is not explicitly modeled. Overlapping events from the same class are treated as a single, longer presence event, because presence/absence labels cannot differentiate between individual event instances. For instance, in an audio recording with two birds calling simultaneously, this weak labeling framework simplifies the overlap into a single 'present' event. While this simplification is necessary when studying weak labeling in this setting, it fundamentally restricts our ability to resolve polyphony (the identification of multiple overlapping sound events). We leave the exploration of annotator models capable of providing richer labels to future work; this is beyond the scope of our study.

%Note that, in the presence/absence labeling setting the concept of overlapping events is abstracted away. Overlapping events from the same class are treated as a single, longer presence event because presence/absence labels cannot differentiate between individual contributors. For instance, in an audio recording with two birds calling simultaneously, the framework simplifies the overlap into a single "present" event. While useful for studying weak labeling, this abstraction inherently limits the ability to resolve polyphony (multiple overlapping sound events). We leave it to future work to explore other annotator models capable of providing more detailed labels, but it is beyond the scope of this work.

\subsection{The Assumed Data Distribution}
\label{sec:label_distribution}

A sound event $e$ is defined by its start time \( a_e \in \mathbb{R} \), end time \( b_e \in \mathbb{R} \), and class \( c_e \in \mathcal{C} \), denoted as \( e = (a_e, b_e, c_e) \). Audio recordings are assumed to have finite length \( T \), and events are uniformly distributed over the recording. 

%A sound event is defined by its start time \( a_e \in \mathbb{R} \), end time \( b_e \in \mathbb{R} \), and class \( c_e \in \mathcal{C} \), denoted as \( e = (a_e, b_e, c_e) \) with event length \( d_e = b_e - a_e \). The event timings \( (a_e, b_e) \) are referred to as the onset and offset, while \( c_e \) is the event class. Audio recordings are assumed to have finite length \( T \), and events \( e \) are assumed uniformly distributed over the recording such that \( a_e \in [0, T - d_e] \). 

The uniform distribution reflects the assumption that events are equally likely to appear anywhere in the recording relative to the recording's start time. Consider a person who wants to record an event but does not know when the event will occur. We assume that they are equally likely to start recording at any time before this event.

\subsection{The Assumed Annotator Model}
\label{sec:annotator_model}

For a given sound event class \( c \in \mathcal{C} \), the annotator decides the presence or absence of an event $e$ of class \( c \) in a data segment \( q = (a_q, b_q) \), where \( d_q = b_q - a_q \) is the fixed-length of the segment. We will refer to $q$ as a query segment because it is queried for a presence or absence label. Let $l_q \in \{0, 1\}$ denote the weak label indicated by the annotator for query segment $q$, where $l_q = 1$ indicates presence of an event of class $c$ in $q$ and $l_q = 0$ indicates absence of that event class in $q$. Detecting the presence of an event requires observing a sufficient fraction of the event within the query segment, formalized as follows:

\begin{definition}
\label{def:event_fraction}
The \textit{event fraction} is the fraction of the total event duration \( d_e = b_e - a_e \) that overlaps with the query segment \( q \),
\begin{equation}
\label{eq:event_fraction}
    h(e, q) = \frac{|e \cap q|}{d_e},
\end{equation}
where \( e \cap q \) is the intersection of \( (a_e, b_e) \) and \( (a_q, b_q) \).
\end{definition}

\begin{definition}
\label{def:annotator_criterion}
The \textit{presence criterion} \( \gamma \in (0, 1] \) is the minimum event fraction required for the annotator to detect the presence of \( e \) in \( q \),
\begin{equation}
\label{eq:annotator_criterion}
    h(e, q) \geq \gamma.
\end{equation}
\end{definition}


The annotator assigns a presence label (\(l_q = 1\)) to \( q \) if there is sufficient overlap with any presence event $e$ of class $c$ (\( h(e, q) \geq \gamma \)); otherwise, it assigns an absence label (\(l_q=0\)). The parameter \( \gamma \) reflects the annotator's sensitivity: lower \( \gamma \) values indicate sensitivity to smaller event fractions, while higher values require larger fractions. 

This framework captures variability in annotator behavior. For example, detecting "human speech" or "bird song" may only require hearing a small fraction of the event (\( \gamma \) closer to 0), while recognizing specific phrases or bird species might demand a near-complete observation (\( \gamma \) closer to 1). The value of \( \gamma \) thus depends on the annotator and the complexity of the event class. This model provides a flexible yet precise way to simulate annotator behavior and quantify their labeling performance. However, it is important to note that this model is deterministic, focusing on temporal alignment between events and the query segment. In practice, human annotation often involves stochastic factors, such as variability in perception and judgment, which are not explicitly modeled here.


\subsection{Label Accuracy}
\label{sec:quality_of_presence_labels}

Label accuracy measures the alignment between annotator-provided labels and ground truth labels:

\begin{definition}
The label accuracy is defined as 
\begin{equation}
\label{eq:query_iou}
    F(e, q, \gamma) = \begin{cases}
        \frac{|e \cap q|}{d_q}, & \text{ if } l_q = 1, \\
        \frac{d_q - |e \cap q|}{d_q}, & \text{ if } l_q = 0.
    \end{cases}
\end{equation}
\end{definition}

For instance, consider a 3-second query segment ($d_q = 3$) that overlaps exactly one second ($|e \cap q|=1$) with a 2-second sound event ($d_e = 2$) of the class bird song  $(c=\text{``bird song''}$). The annotator assigns a presence label ($l_q = 1$) with label accuracy \( \frac{|e \cap q|}{d_q} = \frac{1}{3} \) if half or less of the event needs to be in the query segment ($\gamma \leq 0.5$). Contrary, the annotator assigns an absence label (\(l_q = 0\)) with label accuracy \( \frac{d_q - |e \cap q|}{d_q} = \frac{3 - 1}{3} = \frac{2}{3} \) if more than half of the event ($\gamma > 0.5$) needs to be in the query segment. This formulation isolates the segment label noise (\( 1 - F(e, q, \gamma) \)) introduced by the automated component (fixed-length segments) of the FIX weak labeling method.

%If the presence criterion is that at least half of the bird song event needs to be in the query segment ($\gamma = 0.5$), then this is met (\( \frac{|e \cap q|}{d_e} =\frac{1}{2} = 0.5 \ge \gamma \)), and the annotator assigns a presence label (\(l_q = 1\)) with label accuracy \( \frac{|e \cap q|}{d_q} = \frac{1}{3} \). If the presence criterion, however, is stricter ($\gamma > 0.5$) then it would not be met, and the annotator assigns an absence label (\(l_q = 0\)) with label accuracy \( \frac{d_q - |e \cap q|}{d_q} = \frac{3 - 1}{3} = \frac{2}{3} \). 
