%\section{Conclusions}
%In this paper, we derived a closed-form expression for the label accuracy of FIX weak labeling under a simplified annotator model and label distribution. Our simulation study validated this theory, showing it as an upper bound on segment label noise for FIX weak labeling. We highlighted the trade-off between the number of segments (B) and label accuracy, providing insights into the efficiency of FIX weak labeling. By comparing FIX with ORC weak labeling, we quantified the differences in label accuracy and annotation cost, offering a foundation for developing adaptive weak labeling methods. Our findings support informed decisions in the weak labeling annotation process for sequence labeling tasks.
% \section{Conclusions}
% \label{sec:conclusion}

% Accurate labeling is foundational to developing reliable supervised learning models, especially in domains such as sound event detection where weak labeling is often employed to reduce annotation costs. Our study offers a comprehensive theoretical framework for understanding the trade-offs between label accuracy and annotation cost in weak labeling methods, specifically comparing fixed-length (FIX) and oracle (ORC) approaches.

% We have demonstrated that FIX weak labeling, while cost-effective in specific scenarios, is inherently limited by segment label noise. The theoretical expressions we derived provide actionable insights into optimizing segment length for maximizing expected label accuracy under FIX. However, these results also underscore the fundamental trade-offs: shorter segments improve alignment with event boundaries but significantly increase annotation cost, while longer segments reduce cost at the expense of accuracy. In contrast, ORC labeling achieves perfect accuracy but can incur higher costs, particularly when the number of events is overestimated.

% Our findings have several practical implications:
% \begin{itemize}
%     \item \textbf{Annotation Strategy:} FIX weak labeling remains a robust, scalable choice for many practical applications. However, when high label accuracy is essential, ORC weak labeling—or adaptive methods approximating it—should be prioritized.
%     \item \textbf{Adaptive Techniques:} Theoretical justification for adaptive weak labeling methods that mimic ORC suggests promising avenues for improving annotation efficiency without compromising accuracy.
%     \item \textbf{Evaluation Criteria:} Our analysis highlights the potential biases introduced by noisy labels in evaluating sound event detection models. Ensuring evaluation criteria align with the intended model properties is critical.
% \end{itemize}

% Future research should address several limitations and extensions identified in our study. Developing practical approaches for reliably modeling ORC processes without introducing bias remains an open challenge. Additionally, extending this framework to multi-dimensional data and multiple presence classes could broaden its applicability to other domains, such as medical imaging and point clouds

% In conclusion, the insights presented in this work offer a foundation for optimizing weak labeling processes, balancing cost and accuracy to meet the needs of diverse machine learning applications. By refining annotation strategies and leveraging adaptive methods, researchers can enhance the quality of labeled datasets, driving advancements in supervised learning across domains.

\section{Conclusions}
\label{sec:conclusion}

This study introduces a novel theoretical framework for understanding the trade-offs between label accuracy and annotation cost in weak labeling methods, particularly focusing on sound event detection where weak labeling is often employed to reduce annotation costs. We specifically compared fixed-length (FIX) and oracle (ORC) approaches.

We have demonstrated that FIX weak labeling, while cost-effective in specific scenarios, is inherently limited by segment label noise. The expressions we derived theoretically provide actionable insights into optimizing segment length for maximizing expected label accuracy under FIX. However, these results also underscore the fundamental trade-offs: shorter segments improve alignment with event boundaries but significantly increase annotation cost, while longer segments reduce cost at the expense of accuracy. In addition, how short these segments can be chosen depends on the ability of the annotator to detect presence of fractions of the events. In contrast, ORC labeling achieves perfect accuracy but can incur higher costs if events are very dense and the number of events are overestimated.

Our findings have several practical implications:
\begin{itemize}
    \item \textbf{Annotation Strategy:} FIX weak labeling remains a robust, scalable choice for many practical applications. However, when high label accuracy is essential, ORC weak labeling—or adaptive methods approximating it—should be prioritized.
    \item \textbf{Adaptive Techniques:} Theoretical justification for adaptive weak labeling methods, e.g., methods based on active learning or iterative refinement, that mimic ORC weak labeling, which suggests promising avenues for improving annotation efficiency without compromising accuracy.
    \item \textbf{Evaluation Criteria:} Our analysis highlights the potential biases introduced by segment-level label noise in evaluating sound event detection models. Therefore, carefully aligning evaluation criteria with the intended model properties is critical.
\end{itemize}

Future research should address several limitations and extensions identified in our study. Developing practical approaches that reliably mimic ORC weak labeling by estimating the query segments without introducing a lot of unwanted bias in the labels remains an open challenge. Additionally, extending this framework to multi-dimensional data and multiple presence classes could broaden its applicability to other domains, such as medical imaging and point clouds.

In conclusion, the insights presented in this work offer a foundation for optimizing weak labeling processes, balancing cost and accuracy to meet the needs of diverse machine learning applications. By refining annotation strategies and leveraging adaptive methods, researchers can enhance the quality of labeled datasets. This, in turn, will drive advancements in supervised learning across domains, building upon the foundational understanding presented in this work.

%By refining annotation strategies and leveraging adaptive methods, researchers can enhance the quality of labeled datasets, driving advancements in supervised learning across domains, building upon the foundational understanding presented in this work.
