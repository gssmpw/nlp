\section{Introduction}
In supervised machine learning, labeled datasets are required for training and evaluation. During evaluation, the accuracy of the labels determine the quality of the analysis. However, in practice, labels often contain noise that varies with the input sample and label type. Noisy training labels present a persistent challenge in machine learning~\citep{Liang2009, Song2022}. Deep learning models, in particular, are prone to overfitting noisy labels, raising questions about the nature of generalization~\citep{Zhang2021}. Regularization techniques such as dropout~\citep{Srivastava2014}, data augmentation~\citep{Shorten2019}, and weight decay~\citep{Krogh1991} mitigate overfitting but fail to eliminate the performance gap between training on noisy versus clean labels~\citep{Song2022}. 

Beyond the well-documented challenges posed by noisy training labels, inaccurate evaluation labels present a significant, yet often overlooked, obstacle to reliable machine learning. When evaluation metrics are computed against noisy ground truth, the apparent "best" performing model might simply be the one that most closely reproduces the noise present in the evaluation set, rather than exhibiting superior generalization capabilities. This very issue, where noisy evaluation labels can lead to the rejection of models that have learned the true clean label distribution, is a central concern addressed by \citet{GÃ¶rnitz2014}. This can lead to the selection of suboptimal models that perform well on the flawed evaluation data but generalize poorly to unseen, cleaner data or data from real-world applications. Consequently, performance benchmarks can be inflated and misleading, hindering meaningful comparisons between different approaches. Therefore, understanding the characteristics of label noise, not just in the training data but also in the evaluation data, is crucial for developing and selecting models that are truly effective and robust.

%Weak labeling, the process of providing presence or absence labels for a data class in predefined data segments, offers a practical solution for reducing annotation costs by simplifying the labeling process~\citep{Martin-Morato2023a}. However, since annotators do not specify the precise boundaries of the data segments, these labels are often noisy~\citep{Turpault2021}. Understanding and mitigating this noise is critical for leveraging the labels effectively~\citep{Kumar2016}.
Labels are typically obtained through human annotation, a process that involves significant time and financial investment, particularly for complex data like audio or time-series signals. In this work, we consider a form of weak labeling where the annotator assigns presence or absence labels to predefined data segments. This offers a practical and cost-effective approach for annotating large audio datasets~\citep{Martin-Morato2023a}. To reduce cost, weak labels avoid specifying precise boundaries within the data segments, focusing instead on general presence or absence of the target class. However, this simplification introduces noise into the labels, especially for data with time-varying characteristics, such as audio signals, where events can occur intermittently within the labeled segment~\citep{Turpault2021}. Understanding and mitigating this noise is critical to effectively leverage weak labels in downstream applications~\citep{Kumar2016}.

The noise in weak labels can be categorized into two types: class label noise (mislabeling event presence or absence in a segment) and segment label noise (mislabeling due to misaligned segment boundaries). While class label noise has been extensively studied~\citep{Song2022, Zhang2021}, the effects of segment label noise remain underexplored. This type of noise significantly affects tasks such as sound event detection~\citep{Hershey2021, Turpault2021, Shah2018} and medical image segmentation~\citep{Yao2023}. Strategies like pseudo-labeling~\citep{Dinkel2022}, robust loss functions~\citep{Fonseca2019_agnostic}, and adaptive pooling operators~\citep{McFee2018} aim to address challenges when training on weak labels. However, fully understanding the impact of weak labels requires quantifying their accuracy~\citep{Shah2018, Turpault2021}.

Current methods typically estimate label noise rates \textit{after} collecting labels~\citep{Song2022}, employing techniques like noise transition matrices~\citep{Li2021} or cross-validation~\citep{Chen2019}. In contrast, predicting label noise rates \textit{before} data collection remains largely unexplored. This is particularly challenging when the noise stems from human annotators, as it is difficult to formalize. In cases involving partially automated processes, however, the noise introduced by the automated component can often be modeled under specific assumptions.

In this work, we model the automated component of a commonly used weak labeling method for segmentation tasks: fixed-length weak labeling (FIX). We quantify the segment label noise of this process, and study the expected label accuracy. This method, commonly employed in sound event detection, involves annotators providing presence or absence labels for fixed-length segments of the data (automated component), rather than specifying precise event boundaries. By simplifying the labeling process, FIX weak labeling reduces annotation effort but introduces segment label noise when segments misalign with the actual onsets and offsets of events. To benchmark this approach, we compare it to an oracle weak labeling method, ORC weak labeling, which assigns presence or absence labels to segments derived using the true onsets and offsets of the events.

%We study the label accuracy of the automated component of a widely used weak labeling method for segmentation tasks: FIX weak labeling. This is a standard weak labeling method widely used in the field of sound event detection. In this method, annotators provide presence or absence labels for fixed-length segments of the data. While this further simplifies the labeling process, it introduces segment label noise when segments misalign with events. To benchmark this approach, we compare it to an oracle weak labeling method, called ORC weak labeling, that assigns presence or absence labels to data segments that have been derived using the true onsets and offsets of the events.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/weak_labeling.png}
    \caption{Resulting presence (red) and absence (white) labels from ORC and FIX weak labeling for an audio recording with three presence events (green). ORC weak labeling assigns labels to ground truth segments, achieving perfect label accuracy with $B=7$ labels. FIX weak labeling, shown for different segment lengths ($B=7$, $B=30$, $B=60$), introduces segment label noise as segments misalign with events. Longer segments reduce annotation cost but increase noise, while shorter segments align better but require more annotations. Note that too short segments ($B=60$) may lead to the annotator missing the presence of the event because it does not cover a large enough fraction of it.}
    \label{fig:weak_labeling}
\end{figure}

Figure~\ref{fig:weak_labeling} illustrates the trade-offs between annotation cost and label accuracy for the ORC and FIX weak labeling methods. ORC weak labeling achieves perfect label accuracy by aligning the segments with the ground truth presence events (green) using a minimal number of annotated segments. In contrast, FIX weak labeling shows varying accuracy depending on segment length: shorter segments improve alignment ($B=30$) but require more annotations, while longer segments ($B=7$) reduce cost at the expense of accuracy. In addition, too short segments ($B=60$) can lead to the annotator missing event presence. These trade-offs are central to understanding how FIX weak labeling can be used effectively. By analyzing the FIX weak labeling method, we provide a theoretical framework to guide data collection efforts. 

%Specifically, we quantify the trade-offs between annotation cost and label accuracy. Additionally, a theoretical understanding of the gap between FIX and ORC weak labeling provides a justification for developing adaptive weak labeling methods~\cite{Martinsson2024, Kim2023} that better model the ORC weak labeling process.

In summary, our contributions include:
\begin{itemize}
\item Closed-form expressions for label accuracy and annotation cost in FIX and ORC weak labeling, made tractable by assuming an annotator model and a simplified data distribution.
\item A simulation study demonstrating that our theoretical framework generalizes to more complex data distributions and serves as an upper bound for the accuracy of FIX weak labeling.
\item A theoretical foundation for developing adaptive weak labeling methods that better approximate ORC weak labeling, such as~\citep{Martinsson2024} for sound event detection and~\citep{Kim2023} for image segmentation.
\end{itemize}

Our analysis focuses on one-dimensional data, and the assumptions are justified by common characteristics in bioacoustic sound events. These time-localized, non-stationary animal vocalizations often require annotators to hear significant portions of the sound to assign accurate presence labels. Note, however, that while our framework is tailored to this domain, the principles extend to annotation of events in time in other data that shares these characteristics.