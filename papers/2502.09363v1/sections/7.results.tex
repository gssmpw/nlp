\section{Results}
\label{sec:results}
In this section we present the results of the simulated annotation process, and show how these connect to the derived theory. We start by looking at the expected label accuracy and the query segment length that maximize the expected label accuracy for FIX and ORC weak labeling, and then we relate this to the annotation cost.

\subsection{Expected Label Accuracy given Overlap}

%Understanding how expected label accuracy given overlap and the query length that maximize this is affected by the annotator presence criterion is useful for optimizing the FIX weak labeling strategy. 
We evaluate how different annotator presence criteria ($\gamma$) influence the achievable label accuracy given overlap under FIX weak labeling. We first examine the case of a single event with a deterministic length, then extend our simulation study to stochastic event lengths, and finally to multiple events occurring within the same recording.


\subsubsection{Single Event with Deterministic Length}
The simulated results are derived using the simulation setup described in section~\ref{sec:simulation}, with $M=1$ (a single event) and $d_e=1$ (deterministic length). In Figure~\ref{fig:simple_simulation}, we show the maximum expected label accuracy given overlap (left) and the corresponding query length that maximize the label accuracy (right) for different $\gamma$. $f^*(\gamma)$ is the maximum expected label accuracy achievable with annotator presence criterion $\gamma$ for the considered event length. We can see that the simulated average label accuracy closely follows the expected label accuracy, and that the corresponding segment length leading to this maximum is the same in theory and simulation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.66\textwidth]{figures/uniform_FIX_accuracy.png}
    % \includegraphics[width=0.49\textwidth]{figures/iou_max_vs_gamma.png}
    \caption{In the left panel we show the maximum expected label accuracy, $f^*(\gamma)$, for different $\gamma$, and the average maximum label accuracy from the simulations. In the right panel we show the query length that leads to this maximum label accuracy in theory, for $d_e = 1$, and in simulation. The theory follows the simulations well.}
    \label{fig:simple_simulation}
\end{figure}

In Figure~\ref{fig:simple_simulation} we see that if the annotator needs to hear more than $50$\% of the sound event to detect presence ($\gamma=0.5$) then the highest achievable label accuracy is $f^*(0.5) \approx 0.76$. This means that on average there is around $34$\% segment label noise around the presence labels. We also see that the query length that gives the maximum label accuracy is $d_q^* \approx 0.81$. The gap to the ORC weak labeling method which always gives a label accuracy of $1$, is large especially for large $\gamma$. In general, we can see how the maximum label accuracy deteriorates with a growing $\gamma$, and which query segment length to choose to maximize label accuracy in expectation. 

\subsubsection{Single Event with Stochastic Length}
\label{sec:event_distribution}
We now consider stochastic event lengths. We do this to better understand the effect of the event length distribution on the maximum expected label accuracy and the optimal query length. We solve the integral in Eq.~\ref{eq:expected_query_iou_distribution} by numerical integration over different event length distributions, and compare with the theory derived for a single deterministic event length and simulations. In each figure we present the derived theoretical rules $f^*(\gamma)$ and $d_q^*$ for the simplified event length distribution, the results from integration of Eq.~\ref{eq:expected_query_iou_distribution} with different event length distributions $p(d_e)$ (numerical), and the simulated results using the procedure described in section~\ref{sec:simulation} (simulated) where event lengths are sampled from different distributions. Note that, since $d_q^*$ is derived for a deterministic event length $d_e$, and require a choice of this value, we set $d_e$ to the average event length ($\mu$) for each distribution in these experiments as a heuristic. We then present the maximum expected label accuracy for different $\gamma$ (left in figures) and the query segment length that maximizes the expected label accuracy (middle in figures), and the histogram for the considered event length distributions (right in figures).

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/normal_mean_FIX_accuracy.png}
    \caption{We validate the theory for stochastic event lengths drawn from two normal distributions with different means, but the same variance. We show the expected label accuracy (left panel), the optimal query length (middle panel), and the considered event length distributions (right panel).}
    \label{fig:normal_mean}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/normal_variance_FIX_accuracy.png}
    \caption{We validate the theory for stochastic event lengths drawn from two normal distributions with different variance, but the same mean. We show the expected label accuracy (left panel), the optimal query length (middle panel), and the considered event length distributions (right panel).}
    \label{fig:normal_variance}
\end{figure}


In Figure~\ref{fig:normal_mean} and Figure~\ref{fig:normal_variance} we see that the mean and variance of the normal distribution have a small (if any) effect on the maximum expected label accuracy, but the mean does affect which query segment length that maximizes the expected label accuracy. We also see that $d_q^*$ follows the simulated and numerical optimal query length well for all considered normal distributions, when $d_e$ is set to the average event length ($\mu$) for the considered event length distribution. The average event length can be used as a heuristic value if we only know the average and not the true distribution to integrate over.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/gamma_FIX_accuracy.png}
    \caption{We validate the theory for stochastic event lengths drawn from two gamma distributions with different shape parameters, but the same scale parameter. We show the expected label accuracy (left panel), the optimal query length (middle panel), and the considered event length distributions (right panel).}
    \label{fig:gamma}
\end{figure}

In Figure~\ref{fig:gamma} we can see that a gamma distribution does affect the maximum expected label accuracy, and that simply setting $d_e$ to the average event length of the distribution leads to underestimating the optimal query length. Since it is not possible to optimize for both short and long events at the same time using FIX weak labeling, this type of distribution is quite challenging.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/dog_and_baby_FIX_accuracy.png}
    \caption{Barking dog and crying baby event length distributions from the NIGENS dataset~\citep{Trowitzsch2019}. These annotations have been made with a strong guarantee for high quality onsets and offsets.}
    \label{fig:dog_and_baby}
\end{figure}

In Figure~\ref{fig:dog_and_baby} we validate the theory against a real sample of event lengths from either baby cries or dog barks. Numerical integration between the derived expression and the histogram predicts the simulations well.

\subsubsection{Multiple Events with Stochastic Length}
\label{sec:multi_events}

In these simulations we allow multiple events to occur in the same recording ($M>1$). In Figure~\ref{fig:uniform_30} we show the results of sampling $30$ events of length $d_e=1$ for each audio recording. This does have a an effect on the expected maximum label accuracy and the corresponding query length, but not (that) large. In Figure~\ref{fig:uniform_50} we show the results of sampling $50$ events of length $d_e=1$ for each audio recording. This is an extreme case, where the event density of the recording is very high. %We would not expect this to happen often in practice. %But, now the effect starts to show. However, the theory still matches the simulations.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/uniform_30_FIX_accuracy.png}
    \caption{We validate the theory for multiple events of length $d_e=1$. We show the expected label accuracy (left panel), the optimal query length (middle panel), and the considered event length distributions (right panel). Note that presence events longer than $1$ can occur if two or more events overlap. We sample $30$ events with event length $d_e=1$ occur at random for each audio recording in this simulation.}
    \label{fig:uniform_30}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/uniform_50_FIX_accuracy.png}
    \caption{We validate the theory for multiple events of length $d_e=1$. We show the expected label accuracy (left panel), the optimal query length (middle panel), and the considered event length distributions (right panel). Note that presence events longer than $1$ can occur if two or more events overlap. We sample $50$ events with event length $d_e=1$ occur at random for each audio recording in this simulation.}
    \label{fig:uniform_50}
\end{figure}

% In all experiments we can see that $f^*(\gamma)$ acts as an upper bound on the expected label accuracy for any of the other considered event length distributions. Arguably, any recording distribution that has more than a single event, or events of varying length should be at least as hard to annotate with FIX as the one with only a single event of deterministic event length.

\subsection{Annotation Cost for Maximum Expected Label Accuracy given Overlap}

Achieving maximum expected label accuracy comes at a cost, and understanding this cost trade-off is essential for practical annotation efforts. The cost model we employ accounts for both the time spent listening to audio and the effort required to label presence or absence events. %The key difference between FIX and ORC weak labeling lies in the number of queries required and the resulting label accuracy: FIX uses a theoretically derived optimal query length $d_q^*$ to construct $T/d_q^*$ query segments for an audio recording of length $T$ and achieve non-perfect label accuracy, while ORC requires at least $2M + 1$ queries to ensure perfect label accuracy.

\subsubsection{Formalizing the Cost Model}

The derived theory for the optimal query length allows us to analyze the cost of achieving maximum expected label accuracy under different annotator models for FIX weak labeling. We assume that the whole audio recording of length $T$ is listened to. The key difference in cost between the FIX and ORC weak labeling method is the number of segments ($B$) that need to be given a presence or absence label. We formalize a cost model as:
\begin{equation}
\label{eq:cost}
    C(T, B) = (1-r)T + rB,
\end{equation}
where $1-r$ represents the cost of listening to one second of audio (cost per second), and $r$ represents the cost of answering a query (cost per query). The term $(1-r)T$ therefore represents the cost of listening to $T$ seconds of audio, and the term $rB$ the cost of assigning $B$ presence or absence labels. Using this cost model, we calculate the cost of annotating an audio recording of length $T$ with $M$ sound events of length $d_e=1$ using either FIX or ORC weak labeling. For FIX, the number of queries that maximize expected label accuracy is given by $B^*_{\text{FIX}} = T/d_q^*$ (see Theorem~\ref{thm:fix_number_of_queries}). For ORC, achieving an expected label accuracy of $1$ requires at least $B^*_{\text{ORC}} = 2M+1$ queries.

In practice, we do not know the number of events $M$. To explore potential overestimation of $M$ when, for example, using a weak labeling process that tries to mimic ORC weak labeling, we model $B_{\text{ORC}}$ as a multiple of the necessary number of queries: $B_{\text{ORC}} = sB^*_{\text{ORC}}$, where $s \in \{1, 2, 4, 8\}$ represents the degree of overestimation. This approach captures scenarios where the number of events are either precisely estimated ($s=1$) or significantly overestimated ($s=8$) during the annotation process. In practice, $B_{\text{ORC}}$ could be set based on a bound on $M$. For example, by estimating a maximum expected number of sound events in a recording, $M_{\max}$, based on knowledge of typical event density, or characteristics of the audio recording. We assume that overestimation by more than a factor of $8$ is unlikely. The relative cost between FIX and ORC weak labeling can then be computed as:
\begin{equation}
\label{eq:cost_ratio}
    \frac{C_{\text{FIX}}}{C_{\text{ORC}}} = \frac{C(T, B^*_{\text{FIX}})}{C(T, B_{\text{ORC}})},
\end{equation}
where a ratio larger than $1$ indicates that FIX is more costly than ORC, and a ratio smaller than $1$ indicates that FIX is less costly than ORC.



\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/cost_orc_fix_gamma.png}
    \includegraphics[width=0.49\textwidth]{figures/cost_orc_fix_r.png}
    \caption{The relative cost of FIX and ORC for varying annotator criteria $\gamma$ (left), and cost ratios $r$ (right). The default parameters are: $T=100$, $r=0.5$, $M=1$ and $\gamma=0.5$. We simulate overestimating the number of needed queries $B_{\text{ORC}} = s(2M+1)$ by a factor of $s$ for $s \in \{1, 2, 4, 8\}$ to see how this affects the relative cost. The cost of FIX is greater than the cost of ORC above the dashed red line where the cost ratio is $1$.}
    \label{fig:cost_1}
\end{figure}

\subsubsection{Effect of annotator criteria ($\gamma$) and cost ratio ($r$).} Figure~\ref{fig:cost_1} (left) shows the relative cost for varying annotator criteria $\gamma \in [0.1, 1]$. As $\gamma \rightarrow 0.1$, the cost of FIX increases sharply, reflecting the need for an infinitely large number of queries to achieve an expected label accuracy of $1$. In practice, achieving perfect accuracy with FIX is infeasible due to the associated cost. For higher $\gamma$, the cost of FIX becomes more comparable to ORC. However, combining this with Theorem~\ref{thm:max_iou} reveals that FIX can either match ORC in cost but with lower expected accuracy or achieve similar accuracy at a much higher cost.

The right panel of Figure~\ref{fig:cost_1} examines the impact of the cost ratio $r$. Across all tested values, ORC remains less costly than FIX in the default setting ($T=100$, $r=0.5$, $\gamma=0.5$, $M=1$). This confirms that the relative cost advantage of ORC is robust to changes in $r$.

\begin{figure}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/cost_orc_fix_M.png}
    \includegraphics[width=0.49\textwidth]{figures/cost_orc_fix_T.png}
    \caption{The relative cost of FIX and ORC for varying number of sound events $M$ (left) and recording lengths $T$ (right). The default parameters are: $T=100$, $r=0.5$, $M=1$ and $\gamma=0.5$. We simulate overestimating the number of needed queries $B_{\text{ORC}} = s(2M+1)$ by a factor of $s$ for $s \in \{1, 2, 4, 8\}$ to see how this affects the relative cost. The cost of FIX is greater than the cost of ORC above the dashed red line where the cost ratio is $1$.}
    \label{fig:cost_2}
\end{figure}

\subsubsection{Effect of number of events ($M$) and recording length ($T$).} Figure~\ref{fig:cost_2} explores the impact of $M$ and $T$ on the relative cost. In the left panel, we see that for $s=1$, ORC is less costly than FIX when the number of events is below $60$. However, as $s$ increases to $8$, FIX becomes less costly when at most $10$ events are present. These results indicate that the relative cost depends heavily on the density of sound events in the recording and the estimated annotation budget for ORC. 

In the right panel, varying $T$ shows a similar trend. For shorter recordings (high event density), ORC loses its cost advantage. However, it’s important to note that the maximum achievable expected label accuracy with FIX under default settings ($\gamma=0.5$) is $f^*(0.5) \approx 0.76$, whereas ORC achieves $1.0$. In such cases, the additional cost of ORC may be justified by the significantly higher label quality.

While these results indicate that the relative cost depends on the sound event density, we should remember that we are considering weak labeling of presence events. This implies that all $M$ events in this analysis are treated as non-overlapping, as the annotation task does not consider temporal overlaps for this analysis. The scenario of $M > 60$ non-overlapping events of length $1$ in a recording of length $T=100$ is therefore unlikely in practice. Similarly, estimating $10$ events as $80$ (modeled by $s=8$) for an audio recording of length $T=100$ represents a substantial overestimation and seems improbable given the capabilities of modern sound event detection tools. %Modern sound event detection tools, which leverage advances in machine learning, typically provide reliable upper bounds on the number of events, making extreme overestimations less likely.