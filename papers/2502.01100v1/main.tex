%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
% \usepackage{setspace}

\usepackage{booktabs} % for professional tables

% Additional packages from main.tex
\usepackage{arydshln}
\usepackage{fontawesome5}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{fvextra}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{listings}
\usepackage{setspace}
\usepackage{float}
\usepackage{bbm}
\usepackage{nicefrac}
\usepackage{dsfont}
\usepackage{enumitem}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{mathrsfs}

% \usepackage{algorithm}
% \usepackage{algorithm}  % Provides the algorithm floating environment
% \usepackage[noend]{algpseudocode}  % Provides algorithmic environment without 'end' keywords

\usepackage{algorithmicx}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{makecell}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
 
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{comment}

% Reference and citation packages
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{subcaption}
 


% Custom box definitions
\usepackage[skins,theorems]{tcolorbox} 

% Custom commands

\newcommand{\greendot}{%
  \begin{tikzpicture}
    \fill[green] (0,0) circle (0.1cm);
  \end{tikzpicture}%
}
\newcommand{\bluedot}{%
  \begin{tikzpicture}
    \fill[blue] (0,0) rectangle (0.2cm, 0.2cm);
  \end{tikzpicture} 
}

\newcommand{\blackdot}{%
\begin{tikzpicture}
  \fill[black] (0,0) -- (0.1,0.1) -- (0,0.2) -- (-0.1,0.1) -- cycle;
  \end{tikzpicture}
}

\newcommand{\yuchen}[1]{\textcolor{red}{[Yuchen: #1]}}
\newcommand{\peter}[1]{\textcolor{red}{[Pete: #1]}}
\newcommand{\yejin}[1]{\textcolor{cyan}{[Yejin: #1]}}
\newcommand{\ronan}[1]{\textcolor{teal}{[RL: #1]}}
\newcommand{\ashish}[1]{\textcolor{cyan}{[Ashish: #1]}}
\newcommand{\rr}[1]{\textbf{\color{green}[RR: #1]}}
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning}

% Reference and citation packages
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{subcaption}

% Custom box definitions
\usepackage[skins,theorems]{tcolorbox}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode} % for pseudocode macros
% \let\oldalgorithmiccomment\algorithmiccomment
% \renewcommand{\algorithmiccomment}[1]{\oldalgorithmiccomment{#1}}
% \renewcommand{\algorithmiccomment}[1]{\textit{#1}}
% \newcommand{\myalgorithmiccomment}[1]{\textit{#1}}
% \renewcommand{\algorithmic}[1]{...}



\begin{document}


% Custom colors for Python code
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

% Python code styling
\newcommand\pythonstyle{\lstset{
    basicstyle=\ttfamily\footnotesize,
    language=Python,
    morekeywords={self, clip, exp, mse_loss, uniform_sample, concatenate, logsumexp},
    keywordstyle=\color{deepblue},
    emph={MyClass,__init__},
    emphstyle=\color{deepred},
    stringstyle=\color{deepgreen},
    frame=single,
    showstringspaces=false
}}
 

\tcbset{
  aibox/.style={
    width=\columnwidth,
    top=7pt,
    bottom=5pt,
    colback=blue!6!white,
    colframe=black,
    colbacktitle=black,
    enhanced,
    center,
    attach boxed title to top left={yshift=-0.1in,xshift=0.15in},
    boxed title style={boxrule=0pt,colframe=white,},
  }
}
\newtcolorbox{AIbox}[2][]{aibox,title=#2,#1}



% \renewcommand{\baselinestretch}{0.9} 

% \reportnumber{} %

% \renewcommand\Authfont{\normalfont\bfseries\fontsize{11}{15}\selectfont}
% \renewcommand\Affilfont{\normalfont\fontsize{12}{14}\selectfont}


\newcommand{\gradientcell}[1]{%
    \begin{tikzpicture}[baseline]
        \fill[gray!25] (0,0) rectangle (0.9,0.3); % background bar - reduced width to 1cm, height to 0.3cm
        \fill[gray!100] (0,0) rectangle ({1*#1/100},0.3); % filled portion
        \node[right] at (0.95,0.15) {#1}; 
    \end{tikzpicture}%
}



\twocolumn[
\icmltitle{
  % \textit{The Curse of Complexity} \\ 
 ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning
}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Bill Yuchen Lin}{uw} \qquad
\icmlauthor{Ronan Le Bras}{ai2} \\
\icmlauthor{Kyle Richardson}{ai2} \quad
\icmlauthor{Ashish Sabharwal}{ai2} \quad
\icmlauthor{Radha Poovendran}{uw} \quad
\icmlauthor{Peter Clark}{ai2} \quad
\icmlauthor{Yejin Choi}{stanford}

\vspace{0.5em}
$^1$University of Washington \qquad $^2$ Allen Institute for AI  \qquad  $^3$ Stanford University \\ 
\vspace{0.2em}
{\small{{\texttt{byuchen@uw.edu \quad ronanlb@allenai.org \quad yejinc@stanford.edu}}}
\vspace{0.2em}
{\small \url{https://hf.co/spaces/WildEval/ZebraLogic}}
}


\end{icmlauthorlist}

\icmlaffiliation{ai2}{Allen Institute for AI} 
\icmlaffiliation{uw}{University of Washington}
\icmlaffiliation{stanford}{Stanford University}


\icmlcorrespondingauthor{Bill Yuchen Lin}{byuchen@uw.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

% We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. Using ZebraLogic, a newly developed benchmark dataset of logic grid puzzles derived from constraint satisfaction problems (CSPs), we systematically evaluate LLM performance. ZebraLogic spans a broad range of search space complexities and incorporates diverse logical constraints, providing a \textit{controlled} environment to assess reasoning abilities. Our results reveal a significant decline in accuracy as problem complexity increasesâ€”a phenomenon we term the ``curse of complexity.'' Notably, this limitation persists even with scaling model size and inference-time computation, suggesting fundamental constraints in current LLM reasoning capabilities. Additionally, we explore strategies such as Best-of-N sampling, backtracking mechanisms, and self-verification prompts to enhance logical reasoning performance. Our findings provide critical insights into the scaling behavior of LLMs, highlight their limitations, and outline potential directions for advancing their reasoning capabilities.
 
We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty.

Our results reveal a significant decline in accuracy as problem complexity growsâ€”a phenomenon we term the ``curse of complexity.'' This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.

   
% \footnote{{
 % Website:
% \url{https://hf.co/WildEval/ZebraLogic} 
% }

\begin{comment}
\yuchen{still working on the abstract}
\peter{This is my old suggested version which is likely out of sync but might be a starting point....}
Our goal is to understand how well LMs perform complex reasoning,
in particular how those abilities scale along various dimensions.
Our approach is to develop a suite of reasoning problems using
a scalable problem generator, that tests reasoning at a wide
range of complexities. The generated dataset, called ZebraLogic,
contains "zebra" constraint problems (also known as Einstein problems)
spanning a wide range of search space sizes, allowing us for the
first time to study relationships between problem complexity,
model size, reasoning tokens, and the number of in-context examples.
In a detailed case study of the recent o1 LM, we show how this
dataset allows us to quantify o1's reasoning, determine o1's scaling
behaviors of that reasoning, and compare it with other models.
This provides new quantitative insights about how models are
progressing, [for example, ... could put an example here?],
illustrating the benefits of both the multiscale dataset and
the methodology in general.
\end{comment}

\end{abstract}

\input{sections/intro}
\input{sections/preliminaries}
\input{sections/experiments}
\input{sections/discussion}
\input{sections/methods}
\input{sections/related_work}
\input{sections/conclusion}
\bibliography{main,related_work}
\bibliographystyle{icml2024}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}
\input{sections/appendix}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
