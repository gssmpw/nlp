% \vspace{-0.2cm}
% \section{Evaluating Logical Reasoning of LLMs with ZebraLogic}
\section{Evaluation}
\label{sec:evaluation}





 
% \yuchen{TODO: here we talk about the Challenges of the LLMs in doing Reasoning on Zebra Logic and then point out that O1 is siginificantly better. We can also mention the hidden CoT tokens and how they are related to the reasoning process.}
% \yuchen{We should transit this section to the next one, where we talk about how we can improve reasoning in LLMs such that we can get better performance and closer to what O1 does.}

% \subsection{Evaluation setup}
% \yuchen{TODO: describe the evaluation setup, including the size of datasets and their solution space, and the metrics that we used for evaluation} 
% \yuchen{TODO: also we need to add the description of the models we are using, including O1-Mini and O1-Preview and Llamas, and the decoding configurations.}
% \yuchen{Describe the definition of solution space}

% \yuchen{Talk about our one-shot prompting with json-style output for parsing.}

% In this section, we focus on using ZebraLogic to examine the challenges of logical reasoning with LLMs. 


\textbf{Setup and Metrics.} Our evaluation is done in a one-shot in-context learning setting, where we provide the models with a single example of how to solve a ZebraLogic puzzle and present the solution in JSON format, and we instruct the LLMs to output their reasoning and solution in the same format, thus making it easier to parse and evaluate their answers.
We mainly look at the puzzle-level accuracy, meaning that only when all cells in the grid are filled correctly, the model is considered to have solved the puzzle. In addition to that, we also report the cell-level accuracy.




 
% \hspace{-1.2cm}

%\yejin{can we put the search space range directly in this table? a lot of readers never read the running text, they only look at the abstract + tables/figures, and such readers might just look at o1-preview 'overall' being 71.4 and think oh it works really well and next year it will be 90\% without any problem...} 

%\ashish{71.4 for o1 sounds artificially high---presumably because you have many more smaller grid instances than larger grid instances in the dataset! The average of 98.1, 88.2, 59.5 and 17.0 is 65.7. I would do one of two things: (a) either make the "overall" number the average of the 4 numbers by category, OR (b) [easier] clearly indicate that the overall number is skewed towards smaller grids because of you more smaller grids; support this with a dataset stats table at the end of section 2.3, which could have 4 rows: size category, number of puzzles in it, average number of clues, (possibly) average search space size}. It's good to have such a dataset stats table anyway.


% \yuchen{TODO: add a 3.0 version of Llama 3 result to show that data scaling is also important.}

% \hspace{-1.2cm}
% \begin{table*}[t]
%     \centering
%     \scalebox{0.88}{
%     \begin{tabular}{@{}r|l||l|l|l|l||l||l@{}}
%     \toprule
%     \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Overall}} & \multicolumn{1}{c}{\textbf{Small}} & \multicolumn{1}{c}{\textbf{Medium}} & \multicolumn{1}{c}{\textbf{Large}} & \multicolumn{1}{c}{\textbf{X-Large}} & \multicolumn{1}{c}{\textbf{Cell Acc}} & \multicolumn{1}{c}{{\small \textbf{Elo}}} \\
%     \midrule
%     O1-preview {\tiny\faLock} & \gradientcell{71.4} & \gradientcell{98.1} & \gradientcell{88.2} & \gradientcell{59.5} & \gradientcell{17.0} & \gradientcell{75.1} & - \\ 
%     O1-mini {\tiny\faLock} & \gradientcell{59.7} & \gradientcell{87.5} & \gradientcell{76.8} & \gradientcell{39.0} & \gradientcell{12.0} & \gradientcell{70.3} & - \\ \midrule 
%     Sonnet 3.5 {\tiny\faLock} & \gradientcell{36.2}  & \gradientcell{84.7} & \gradientcell{28.9} & \gradientcell{4.0} & \gradientcell{1.0} & \gradientcell{54.3} & - \\
%     Llama-3.1-405B {\tiny\faKey} & \gradientcell{32.6} & \gradientcell{81.3} & \gradientcell{22.5} & \gradientcell{1.5} & \gradientcell{0.0} & \gradientcell{45.8} & 1266 \\
%     \underline{\textit{GPT-4o}} {\tiny\faLock} & \gradientcell{31.7} & \gradientcell{80.0} & \gradientcell{19.6} & \gradientcell{2.5} & \gradientcell{0.5} & \gradientcell{50.3} & 1264 \\
%     Gemini-1.5-Pro {\tiny\faLock} & \gradientcell{30.5} & \gradientcell{75.3} & \gradientcell{20.7} & \gradientcell{3.0} & \gradientcell{0.0} & \gradientcell{50.8} & - \\
%     Mistral-Large-2 {\tiny\faKey} & \gradientcell{29.0} & \gradientcell{75.9} & \gradientcell{15.0} & \gradientcell{2.5} & \gradientcell{0.0} & \gradientcell{47.6} & 1251 \\
%     Qwen2.5-72B {\tiny\faKey} & \gradientcell{26.6} & \gradientcell{72.5} & \gradientcell{12.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{40.9} & - \\
%     Gemini-1.5-Flash {\tiny\faLock} & \gradientcell{25.0} & \gradientcell{65.0} & \gradientcell{13.6} & \gradientcell{2.0} & \gradientcell{0.0} & \gradientcell{43.6} & - \\
%     Llama-3.1-70B {\tiny\faKey} & \gradientcell{24.9} & \gradientcell{67.8} & \gradientcell{10.4} & \gradientcell{1.5} & \gradientcell{0.0} & \gradientcell{28.0} & 1247 \\
%     DeepSeek-v2.5 {\tiny\faKey} & \gradientcell{22.1} & \gradientcell{62.2} & \gradientcell{7.9} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{38.0} & - \\
%     \underline{\textit{GPT-4o-mini}} {\tiny\faLock} & \gradientcell{20.1} & \gradientcell{58.8} & \gradientcell{4.6} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{41.3} & 1273 \\
%     Gemma-2-27B {\tiny\faKey} & \gradientcell{16.3} & \gradientcell{46.6} & \gradientcell{5.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{41.2} & - \\
%     Llama-3.1-8B {\tiny\faKey} & \gradientcell{12.8} & \gradientcell{39.4} & \gradientcell{0.7} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{13.7} & - \\
%     Gemma-2-9b {\tiny\faKey} & \gradientcell{12.8} & \gradientcell{37.8} & \gradientcell{2.5} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{36.8} & - \\
%     Qwen2.5-7B {\tiny\faKey} & \gradientcell{12.0} & \gradientcell{36.3} & \gradientcell{1.4} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{30.7} & - \\
%     Mixtral-8x7B {\tiny\faKey} & \gradientcell{8.7} & \gradientcell{26.3} & \gradientcell{1.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{26.5} & - \\
%     Llama-3.2-3B {\tiny\faKey} & \gradientcell{7.4} & \gradientcell{23.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{13.1} & - \\
%     Phi-3.5-4B {\tiny\faKey} & \gradientcell{6.4} & \gradientcell{19.4} & \gradientcell{0.7} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{6.0} & - \\
%     Qwen2.5-3B {\tiny\faKey} & \gradientcell{4.8} & \gradientcell{15.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{11.4} & - \\
%     Gemma-2-2B {\tiny\faKey} & \gradientcell{4.2} & \gradientcell{13.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{10.0} & - \\
%     \bottomrule
%     \end{tabular}
%     }
%     \caption{Performance of LLMs on ZebraLogic. The overall accuracy is calculated based on the number of puzzles solved correctly. We also report the accuracy on small, medium, large, and x-large groups based on the size of the search space (as defined in Sec.~\ref{ssec:dataset_creation}). The cell accuracy indicates the percentage of individual cells filled correctly. ArenaElo shows the LMSYS Arena Elo rating where available.}
%     \label{tab:model_performance}
% \end{table*}


\begin{table*}[th!]
    \centering
    \scalebox{0.92}{ % Adjust the scale factor as needed
    \begin{tabular}{@{}r|l||l|l|l|l||l@{}}
    \toprule
    % \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Overall}} & \multicolumn{1}{c}{\textbf{Small}} & \multicolumn{1}{c}{\textbf{Medium}} & \multicolumn{1}{c}{\textbf{Large}} & \multicolumn{1}{c}{\textbf{X-Large}} & \multicolumn{1}{c}{\textbf{Cell Acc}} \\
    \multicolumn{1}{c}{\textbf{Model Names}} & 
    % \multicolumn{1}{c}{\textbf{Overall}} & 
    \multicolumn{1}{c}{\makecell{\textbf{Overall} \\ \scriptsize{Grid-level acc.}}} &
    \multicolumn{1}{c}{\makecell{\greendot{} \textbf{~Small} \\ \scriptsize{$<10^3$}}} & 
    \multicolumn{1}{c}{\makecell{ \bluedot
         \textbf{~Medium} \\ \scriptsize{$10^3\sim10^6$}}} & 
    \multicolumn{1}{c}{\makecell{\blackdot\textbf{Large} \\ \scriptsize{$10^6\sim10^9$}}} & 
    \multicolumn{1}{c}{\makecell{\blackdot\blackdot\textbf{X-Large} \\ \scriptsize{$>10^9$}}} & 
    \multicolumn{1}{c}{\makecell{\textbf{Cell-level} \\ \textbf{Acc.}}} \\
    \midrule
    o1-full  {\tiny\faLock}  & \gradientcell{81.0} & \gradientcell{97.2} & \gradientcell{92.1} & \gradientcell{78.0} & \gradientcell{42.5} & \gradientcell{78.7} \\
     \textit{DeepSeek-R1} {\tiny\faKey}  & \gradientcell{78.7} & \gradientcell{98.4} & \gradientcell{95.7} & \gradientcell{73.5} & \gradientcell{28.5} & \gradientcell{80.5} \\
    o1-preview {\tiny\faLock} & \gradientcell{71.4} & \gradientcell{98.1} & \gradientcell{88.2} & \gradientcell{59.5} & \gradientcell{17.0} & \gradientcell{75.1} \\ 
    o1-mini {\tiny\faLock} & \gradientcell{59.7} & \gradientcell{87.5} & \gradientcell{76.8} & \gradientcell{39.0} & \gradientcell{12.0} & \gradientcell{70.3} \\ \midrule 
    Claude Sonnet 3.5 {\tiny\faLock} & \gradientcell{36.2}  & \gradientcell{84.7} & \gradientcell{28.9} & \gradientcell{4.0} & \gradientcell{1.0} & \gradientcell{54.3} \\
    Llama-3.1-405B {\tiny\faKey} & \gradientcell{32.6} & \gradientcell{81.3} & \gradientcell{22.5} & \gradientcell{1.5} & \gradientcell{0.0} & \gradientcell{45.8} \\
    \underline{\textit{GPT-4o}} {\tiny\faLock} & \gradientcell{31.7} & \gradientcell{80.0} & \gradientcell{19.6} & \gradientcell{2.5} & \gradientcell{0.5} & \gradientcell{50.3} \\
    Gemini-1.5-Pro {\tiny\faLock} & \gradientcell{30.5} & \gradientcell{75.3} & \gradientcell{20.7} & \gradientcell{3.0} & \gradientcell{0.0} & \gradientcell{50.8} \\
    Mistral-Large-2 {\tiny\faKey} & \gradientcell{29.0} & \gradientcell{75.9} & \gradientcell{15.0} & \gradientcell{2.5} & \gradientcell{0.0} & \gradientcell{47.6} \\
    Qwen2.5-72B {\tiny\faKey} & \gradientcell{26.6} & \gradientcell{72.5} & \gradientcell{12.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{40.9} \\
    % Qwen2.5-32B {\tiny\faKey} & \gradientcell{26.1} & \gradientcell{72.2} & \gradientcell{10.4} & \gradientcell{0.5} & \gradientcell{0.0} & \gradientcell{43.4} \\
    Gemini-1.5-Flash {\tiny\faLock} & \gradientcell{25.0} & \gradientcell{65.0} & \gradientcell{13.6} & \gradientcell{2.0} & \gradientcell{0.0} & \gradientcell{43.6} \\
    Llama-3.1-70B {\tiny\faKey} & \gradientcell{24.9} & \gradientcell{67.8} & \gradientcell{10.4} & \gradientcell{1.5} & \gradientcell{0.0} & \gradientcell{28.0} \\
    DeepSeek-v2.5 {\tiny\faKey} & \gradientcell{22.1} & \gradientcell{62.2} & \gradientcell{7.9} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{38.0} \\
    \underline{\textit{GPT-4o-mini}} {\tiny\faLock} & \gradientcell{20.1} & \gradientcell{58.8} & \gradientcell{4.6} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{41.3} \\
    Gemma-2-27B {\tiny\faKey} & \gradientcell{16.3} & \gradientcell{46.6} & \gradientcell{5.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{41.2} \\
    Llama-3.1-8B {\tiny\faKey} & \gradientcell{12.8} & \gradientcell{39.4} & \gradientcell{0.7} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{13.7} \\
    %Gemma-2-9b {\tiny\faKey} & \gradientcell{12.8} & \gradientcell{37.8} & \gradientcell{2.5} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{36.8} \\
    %Qwen2.5-7B {\tiny\faKey} & \gradientcell{12.0} & \gradientcell{36.3} & \gradientcell{1.4} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{30.7} \\
    %Mixtral-8x7B {\tiny\faKey} & \gradientcell{8.7} & \gradientcell{26.3} & \gradientcell{1.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{26.5} \\
    %Llama-3.2-3B {\tiny\faKey} & \gradientcell{7.4} & \gradientcell{23.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{13.1} \\
    Phi-3.5-4B {\tiny\faKey} & \gradientcell{6.4} & \gradientcell{19.4} & \gradientcell{0.7} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{6.0} \\
    %Qwen2.5-3B {\tiny\faKey} & \gradientcell{4.8} & \gradientcell{15.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{11.4} \\
    %Gemma-2-2B {\tiny\faKey} & \gradientcell{4.2} & \gradientcell{13.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{10.0} \\
    \bottomrule
    \end{tabular}
    }
    \caption{Performance of LLMs on ZebraLogic. The overall accuracy is calculated based on the number of puzzles solved correctly. We also report the accuracy on small, medium, large, and x-large groups based on the size of the search space (see Sec.~\ref{ssec:dataset_creation}).  
    The cell accuracy indicates the percentage of individual cells filled correctly. See Appx.~\ref{app:expts} for more model results.
    }
    \label{tab:model_performance}
    \vspace{-0.5cm}
\end{table*}

\textbf{Evaluated models.} We evaluate both open-weight LLMs (e.g., Llama and Qwen) and proprietary LLM APIs including GPT-4o, O1 and Claude models. 
All evaluated models are prompted in the same way (see Appendix~\ref{app:prompt_template}), and we use the same greedy decoding and prompts and parsing script across all models to ensure a fair comparison, except for O1, which does not only greedy decoding so we run it three times and take the best result. 
% The codebase for evaluation is available at \url{https://github.com/WildEval/ZeroEval}.

\subsection{Main results}

% \yuchen{TODO: show a table with four columns: model, acc in small, medium, large, x-large. Reasoning lens. and also the cell-wise acc, no-answer rates.}
% \yuchen{TODO: in the table, we need to mention the full results can be found in a leaderboard}
Table~\ref{tab:model_performance} shows the performance of various models. o1 outperforms all other models, achieving an overall accuracy of 81.0\%, and DeepSeek-R1, an open-weight reasoning LLM achieves 78.7\%, with a slightly better performance on Small and Medium-size puzzles than o1-full. However, R1's performance on Large and X-Large puzzles is worse than o1-full. o1-preview and o1-mini achieve 71.4\% and 59.7\% accuracy, respectively.
In contrast, the best-performing open-weight non-reasoning LLM, Sonnet-3.5-1022, only reaches 36.2\%. The performance gap is even more pronounced in larger search spaces, where O1-Preview maintains a 17.0\% accuracy in the X-Large category, while other models struggle to achieve any correct solutions.

We find that our ranking and scoring of these models are aligned with other reasoning benchmarks such as MATH~\citep{hendrycksmath2021} for mathematical reasoning and LiveCodeBench~\citep{jain2024livecodebench} for competitive programming. This suggests that the logical reasoning ability of LLMs is highly correlated with their performance on other types of reasoning tasks.

% These results indicate that o1-like models are significantly better at solving ZebraLogic puzzles compared to LLMs like GPT-4o, 
% particularly in larger search spaces. 
% This suggests that o1's reasoning capabilities are more robust and effective in handling complex logical reasoning tasks.



\subsection{Curse of Complexity in Reasoning with LLMs}
We observe that the performance of LLMs drops significantly as the search space size increases, as shown in Fig.~\ref{fig:z3_scale} and Fig.~\ref{fig:scaling} (in Appendix).
We find that for models that are overall worse than GPT-4o-mini can hardly solve puzzles beyond the Small category --- less than 5\% accuracy in Medium-size puzzles and almost no correct solutions in Large and X-Large puzzles.
We can see that even the largest open-weight LLM, Llama-3.1-405B, only achieves 32.6\% overall accuracy.
Although 405B has 22.5\% accuracy in Medium-size puzzles, it quickly also drops to 1.5\% in the Large category and 0.0\% in the X-Large category.

The best non-reasoning LLM, Sonnet 3.5, has 36.2\% accuracy in the overall evaluation, but it also drops to 4.0\% in the Large category and 1.0\% in the X-Large category.
This indicates that the logical reasoning tasks in ZebraLogic are extremely challenging for LLMs, especially for puzzles with more complexity -- with larger search spaces or harder clues.
We can also see that scaling up the model size does not necessarily improve the performance of LLMs in logical reasoning tasks with large search spaces. 

% \yuchen{TODO: We have seen that O1 significantly outperforms other LLMs in logical reasoning tasks, especially when the search space is large where other LLMs fail. A key question is: how does O1 achieve this? 
% Given very limited information about O1's training process and architecture, 
% we can only speculate based on the observed behavior of O1 in the ZebraLogic task. 
% In this section, we provide some insights into how O1 might be reasoning differently from other LLMs, 
% based on the analysis of the reasoning steps extracted from the model's output.}

% \yuchen{mention scaling effects}

% \yuchen{mention the rankings correlation between ours and LMSYS arena. argue that ours is more reliable for reasoning}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
