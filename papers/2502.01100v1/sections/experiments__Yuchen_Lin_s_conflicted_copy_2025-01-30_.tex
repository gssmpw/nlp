% \vspace{-0.2cm}
% \section{Evaluating Logical Reasoning of LLMs with ZebraLogic}
\section{Evaluation Setups \& Overall Results}
\label{sec:evaluation}





 
% \yuchen{TODO: here we talk about the Challenges of the LLMs in doing Reasoning on Zebra Logic and then point out that O1 is siginificantly better. We can also mention the hidden CoT tokens and how they are related to the reasoning process.}
% \yuchen{We should transit this section to the next one, where we talk about how we can improve reasoning in LLMs such that we can get better performance and closer to what O1 does.}

\subsection{Evaluation setup}
% \yuchen{TODO: describe the evaluation setup, including the size of datasets and their solution space, and the metrics that we used for evaluation} 
% \yuchen{TODO: also we need to add the description of the models we are using, including O1-Mini and O1-Preview and Llamas, and the decoding configurations.}
% \yuchen{Describe the definition of solution space}

% \yuchen{Talk about our one-shot prompting with json-style output for parsing.}

% In this section, we focus on using ZebraLogic to examine the challenges of logical reasoning with LLMs. 


\textbf{Setup and Metrics.} The evaluation is done in a one-shot in-context learning setting, where we provide the models with a single example of how to solve a ZebraLogic puzzle in a JSON-style output format, and we instruct the LLMs to output their reasoning and solution in the same json format, thus making it easier to parse and evaluate their responses.
We mainly look at the puzzle-level accuracy, meaning that only when all cells in the grid are filled correctly, the model is considered to have solved the puzzle. In addition to that, we also report the cell-level accuracy and the no-answer rates, which indicate how often the models fail to provide any valid answer.




 
% \hspace{-1.2cm}

\begin{table*}[t]
    \centering
    \scalebox{0.92}{ % Adjust the scale factor as needed
    \begin{tabular}{@{}r|l||l|l|l|l||l@{}}
    \toprule
    % \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Overall}} & \multicolumn{1}{c}{\textbf{Small}} & \multicolumn{1}{c}{\textbf{Medium}} & \multicolumn{1}{c}{\textbf{Large}} & \multicolumn{1}{c}{\textbf{X-Large}} & \multicolumn{1}{c}{\textbf{Cell Acc}} \\
    \multicolumn{1}{c}{\textbf{Model}} & 
    % \multicolumn{1}{c}{\textbf{Overall}} & 
    \multicolumn{1}{c}{\makecell{\textbf{Overall} \\ \scriptsize{Grid-level acc.}}} &
    \multicolumn{1}{c}{\makecell{\textbf{Small} \\ \scriptsize{$<10^3$}}} & 
    \multicolumn{1}{c}{\makecell{\textbf{Medium} \\ \scriptsize{$10^3\sim10^6$}}} & 
    \multicolumn{1}{c}{\makecell{\textbf{Large} \\ \scriptsize{$10^6\sim10^9$}}} & 
    \multicolumn{1}{c}{\makecell{\textbf{X-Large} \\ \scriptsize{$>10^9$}}} & 
    \multicolumn{1}{c}{\textbf{Cell Acc}} \\
    \midrule

    O1 (24-12-27)  {\tiny\faLock}  & \gradientcell{81.0} & \gradientcell{97.2} & \gradientcell{92.1} & \gradientcell{78.0} & \gradientcell{42.5} & \gradientcell{78.7} \\
    DeepSeek-R1 & \gradientcell{78.7} & \gradientcell{98.4} & \gradientcell{95.7} & \gradientcell{73.5} & \gradientcell{28.5} & \gradientcell{80.5} \\
    
    O1-preview {\tiny\faLock} & \gradientcell{71.4} & \gradientcell{98.1} & \gradientcell{88.2} & \gradientcell{59.5} & \gradientcell{17.0} & \gradientcell{75.1} \\ 
    O1-mini {\tiny\faLock} & \gradientcell{59.7} & \gradientcell{87.5} & \gradientcell{76.8} & \gradientcell{39.0} & \gradientcell{12.0} & \gradientcell{70.3} \\ \midrule 
    Sonnet 3.5 {\tiny\faLock} & \gradientcell{36.2}  & \gradientcell{84.7} & \gradientcell{28.9} & \gradientcell{4.0} & \gradientcell{1.0} & \gradientcell{54.3} \\
    Llama-3.1-405B {\tiny\faKey} & \gradientcell{32.6} & \gradientcell{81.3} & \gradientcell{22.5} & \gradientcell{1.5} & \gradientcell{0.0} & \gradientcell{45.8} \\
    \underline{\textit{GPT-4o}} {\tiny\faLock} & \gradientcell{31.7} & \gradientcell{80.0} & \gradientcell{19.6} & \gradientcell{2.5} & \gradientcell{0.5} & \gradientcell{50.3} \\
    Gemini-1.5-Pro {\tiny\faLock} & \gradientcell{30.5} & \gradientcell{75.3} & \gradientcell{20.7} & \gradientcell{3.0} & \gradientcell{0.0} & \gradientcell{50.8} \\
    Mistral-Large-2 {\tiny\faKey} & \gradientcell{29.0} & \gradientcell{75.9} & \gradientcell{15.0} & \gradientcell{2.5} & \gradientcell{0.0} & \gradientcell{47.6} \\
    Qwen2.5-72B {\tiny\faKey} & \gradientcell{26.6} & \gradientcell{72.5} & \gradientcell{12.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{40.9} \\
    % Qwen2.5-32B {\tiny\faKey} & \gradientcell{26.1} & \gradientcell{72.2} & \gradientcell{10.4} & \gradientcell{0.5} & \gradientcell{0.0} & \gradientcell{43.4} \\
    Gemini-1.5-Flash {\tiny\faLock} & \gradientcell{25.0} & \gradientcell{65.0} & \gradientcell{13.6} & \gradientcell{2.0} & \gradientcell{0.0} & \gradientcell{43.6} \\
    Llama-3.1-70B {\tiny\faKey} & \gradientcell{24.9} & \gradientcell{67.8} & \gradientcell{10.4} & \gradientcell{1.5} & \gradientcell{0.0} & \gradientcell{28.0} \\
    DeepSeek-v2.5 {\tiny\faKey} & \gradientcell{22.1} & \gradientcell{62.2} & \gradientcell{7.9} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{38.0} \\
    \underline{\textit{GPT-4o-mini}} {\tiny\faLock} & \gradientcell{20.1} & \gradientcell{58.8} & \gradientcell{4.6} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{41.3} \\
    Gemma-2-27B {\tiny\faKey} & \gradientcell{16.3} & \gradientcell{46.6} & \gradientcell{5.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{41.2} \\
    Llama-3.1-8B {\tiny\faKey} & \gradientcell{12.8} & \gradientcell{39.4} & \gradientcell{0.7} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{13.7} \\
    Gemma-2-9b {\tiny\faKey} & \gradientcell{12.8} & \gradientcell{37.8} & \gradientcell{2.5} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{36.8} \\
    Qwen2.5-7B {\tiny\faKey} & \gradientcell{12.0} & \gradientcell{36.3} & \gradientcell{1.4} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{30.7} \\
    Mixtral-8x7B {\tiny\faKey} & \gradientcell{8.7} & \gradientcell{26.3} & \gradientcell{1.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{26.5} \\
    Llama-3.2-3B {\tiny\faKey} & \gradientcell{7.4} & \gradientcell{23.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{13.1} \\
    Phi-3.5-4B {\tiny\faKey} & \gradientcell{6.4} & \gradientcell{19.4} & \gradientcell{0.7} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{6.0} \\
    Qwen2.5-3B {\tiny\faKey} & \gradientcell{4.8} & \gradientcell{15.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{11.4} \\
    Gemma-2-2B {\tiny\faKey} & \gradientcell{4.2} & \gradientcell{13.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{10.0} \\
    \bottomrule
    \end{tabular}
    }
    \caption{Performance of LLMs on ZebraLogic. The overall accuracy is calculated based on the number of puzzles solved correctly. We also report the accuracy on small, medium, large, and x-large groups based on the size of the search space (as defined in Sec.~\ref{ssec:dataset_creation}).  
    The cell accuracy indicates the percentage of individual cells filled correctly.
    }
    \label{tab:model_performance}
\end{table*}

%\yejin{can we put the search space range directly in this table? a lot of readers never read the running text, they only look at the abstract + tables/figures, and such readers might just look at o1-preview 'overall' being 71.4 and think oh it works really well and next year it will be 90\% without any problem...} 

%\ashish{71.4 for o1 sounds artificially high---presumably because you have many more smaller grid instances than larger grid instances in the dataset! The average of 98.1, 88.2, 59.5 and 17.0 is 65.7. I would do one of two things: (a) either make the "overall" number the average of the 4 numbers by category, OR (b) [easier] clearly indicate that the overall number is skewed towards smaller grids because of you more smaller grids; support this with a dataset stats table at the end of section 2.3, which could have 4 rows: size category, number of puzzles in it, average number of clues, (possibly) average search space size}. It's good to have such a dataset stats table anyway.


% \yuchen{TODO: add a 3.0 version of Llama 3 result to show that data scaling is also important.}

% \hspace{-1.2cm}
% \begin{table*}[t]
%     \centering
%     \scalebox{0.88}{
%     \begin{tabular}{@{}r|l||l|l|l|l||l||l@{}}
%     \toprule
%     \multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{Overall}} & \multicolumn{1}{c}{\textbf{Small}} & \multicolumn{1}{c}{\textbf{Medium}} & \multicolumn{1}{c}{\textbf{Large}} & \multicolumn{1}{c}{\textbf{X-Large}} & \multicolumn{1}{c}{\textbf{Cell Acc}} & \multicolumn{1}{c}{{\small \textbf{Elo}}} \\
%     \midrule
%     O1-preview {\tiny\faLock} & \gradientcell{71.4} & \gradientcell{98.1} & \gradientcell{88.2} & \gradientcell{59.5} & \gradientcell{17.0} & \gradientcell{75.1} & - \\ 
%     O1-mini {\tiny\faLock} & \gradientcell{59.7} & \gradientcell{87.5} & \gradientcell{76.8} & \gradientcell{39.0} & \gradientcell{12.0} & \gradientcell{70.3} & - \\ \midrule 
%     Sonnet 3.5 {\tiny\faLock} & \gradientcell{36.2}  & \gradientcell{84.7} & \gradientcell{28.9} & \gradientcell{4.0} & \gradientcell{1.0} & \gradientcell{54.3} & - \\
%     Llama-3.1-405B {\tiny\faKey} & \gradientcell{32.6} & \gradientcell{81.3} & \gradientcell{22.5} & \gradientcell{1.5} & \gradientcell{0.0} & \gradientcell{45.8} & 1266 \\
%     \underline{\textit{GPT-4o}} {\tiny\faLock} & \gradientcell{31.7} & \gradientcell{80.0} & \gradientcell{19.6} & \gradientcell{2.5} & \gradientcell{0.5} & \gradientcell{50.3} & 1264 \\
%     Gemini-1.5-Pro {\tiny\faLock} & \gradientcell{30.5} & \gradientcell{75.3} & \gradientcell{20.7} & \gradientcell{3.0} & \gradientcell{0.0} & \gradientcell{50.8} & - \\
%     Mistral-Large-2 {\tiny\faKey} & \gradientcell{29.0} & \gradientcell{75.9} & \gradientcell{15.0} & \gradientcell{2.5} & \gradientcell{0.0} & \gradientcell{47.6} & 1251 \\
%     Qwen2.5-72B {\tiny\faKey} & \gradientcell{26.6} & \gradientcell{72.5} & \gradientcell{12.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{40.9} & - \\
%     Gemini-1.5-Flash {\tiny\faLock} & \gradientcell{25.0} & \gradientcell{65.0} & \gradientcell{13.6} & \gradientcell{2.0} & \gradientcell{0.0} & \gradientcell{43.6} & - \\
%     Llama-3.1-70B {\tiny\faKey} & \gradientcell{24.9} & \gradientcell{67.8} & \gradientcell{10.4} & \gradientcell{1.5} & \gradientcell{0.0} & \gradientcell{28.0} & 1247 \\
%     DeepSeek-v2.5 {\tiny\faKey} & \gradientcell{22.1} & \gradientcell{62.2} & \gradientcell{7.9} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{38.0} & - \\
%     \underline{\textit{GPT-4o-mini}} {\tiny\faLock} & \gradientcell{20.1} & \gradientcell{58.8} & \gradientcell{4.6} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{41.3} & 1273 \\
%     Gemma-2-27B {\tiny\faKey} & \gradientcell{16.3} & \gradientcell{46.6} & \gradientcell{5.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{41.2} & - \\
%     Llama-3.1-8B {\tiny\faKey} & \gradientcell{12.8} & \gradientcell{39.4} & \gradientcell{0.7} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{13.7} & - \\
%     Gemma-2-9b {\tiny\faKey} & \gradientcell{12.8} & \gradientcell{37.8} & \gradientcell{2.5} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{36.8} & - \\
%     Qwen2.5-7B {\tiny\faKey} & \gradientcell{12.0} & \gradientcell{36.3} & \gradientcell{1.4} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{30.7} & - \\
%     Mixtral-8x7B {\tiny\faKey} & \gradientcell{8.7} & \gradientcell{26.3} & \gradientcell{1.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{26.5} & - \\
%     Llama-3.2-3B {\tiny\faKey} & \gradientcell{7.4} & \gradientcell{23.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{13.1} & - \\
%     Phi-3.5-4B {\tiny\faKey} & \gradientcell{6.4} & \gradientcell{19.4} & \gradientcell{0.7} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{6.0} & - \\
%     Qwen2.5-3B {\tiny\faKey} & \gradientcell{4.8} & \gradientcell{15.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{11.4} & - \\
%     Gemma-2-2B {\tiny\faKey} & \gradientcell{4.2} & \gradientcell{13.1} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{0.0} & \gradientcell{10.0} & - \\
%     \bottomrule
%     \end{tabular}
%     }
%     \caption{Performance of LLMs on ZebraLogic. The overall accuracy is calculated based on the number of puzzles solved correctly. We also report the accuracy on small, medium, large, and x-large groups based on the size of the search space (as defined in Sec.~\ref{ssec:dataset_creation}). The cell accuracy indicates the percentage of individual cells filled correctly. ArenaElo shows the LMSYS Arena Elo rating where available.}
%     \label{tab:model_performance}
% \end{table*}



\textbf{Evaluated models.} We evaluate both open-weight LLMs (e.g., Llama and Qwen) and proprietary LLM APIs including GPT-4o, O1 and Claude models. 
All evaluated models are prompted in the same way (see Appendix~\ref{app:prompt_template}), and we use the same greedy decoding and prompts and parsing script across all models to ensure a fair comparison, except for O1, which does not only greedy decoding so we run it three times and take the best result. 
The codebase for evaluation is available at \url{https://github.com/WildEval/ZeroEval}.

\subsection{Overall Evaluation Results}

% \yuchen{TODO: show a table with four columns: model, acc in small, medium, large, x-large. Reasoning lens. and also the cell-wise acc, no-answer rates.}
% \yuchen{TODO: in the table, we need to mention the full results can be found in a leaderboard}
Table~\ref{tab:model_performance} shows the performance of various models. O1-Preview outperforms all other models, achieving an overall accuracy of 71.4\%. O1-Mini follows with 59.7\%, while the best-performing open-weight LLM, Sonnet-3.5-1022, only reaches 36.2\%. The performance gap is even more pronounced in larger search spaces, where O1-Preview maintains a 17.0\% accuracy in the X-Large category, while other models struggle to achieve any correct solutions.

The results indicate that O1 models are significantly better at solving ZebraLogic puzzles compared to LLMs, particularly in larger search spaces. This suggests that O1's reasoning capabilities are more robust and effective in handling complex logical reasoning tasks.

\textbf{Comparison with LMSYS Arena Rankings.}
While the overall performance rankings on ZebraLogic generally align with those from the LMSYS Arena (a platform for evaluating LLMs across various tasks), we observe some notable discrepancies that highlight ZebraLogic's distinct evaluation perspective. For instance, GPT-4o-mini-0718 achieves a higher Elo score (1273) in LMSYS Arena (24-11-11) compared to Llama-3.1-405B (1266), GPT-4o-0806(1264), Mistral-Large-2 (1251), and Llama-3.1-70B (1247). However, on ZebraLogic, GPT-4o-mini only achieves 20.1\% accuracy while Llama-3.1-405B reaches 32.6\%. These differences suggest that ZebraLogic offers a more focused assessment of logical reasoning capabilities, providing valuable insights that complement general-purpose evaluations.


\subsection{Curse of Complexity in Reasoning with LLMs}
From the results, we observe that the performance of LLMs drops significantly as the search space size increases.
We find that for models that are overall worse than GPT-4o-mini can hardly solve puzzles beyond the Small category --- less than 5\% accuracy in Medium-size puzzles and almost no correct solutions in Large and X-Large puzzles.
We can see that even the largest open-weight LLM, Llama-3.1-405B, only achieves 32.6\% accuracy in the overall evaluation.
Although 405B has 22.5\% accuracy in Medium-size puzzles, it also drops to 1.5\% in the large category and 0.0\% in the X-Large category.
The best LLM, Sonnet 3.5, has 36.2\% accuracy in the overall evaluation, but it also drops to 4.0\% in the large category and 1.0\% in the X-Large category.
This indicates that the logical reasoning tasks in ZebraLogic are extremely challenging for LLMs, especially for puzzles with more complexity.
We can also see that scaling up the model size does not necessarily improve the performance of LLMs in logical reasoning tasks with large search spaces. 

% \yuchen{TODO: We have seen that O1 significantly outperforms other LLMs in logical reasoning tasks, especially when the search space is large where other LLMs fail. A key question is: how does O1 achieve this? 
% Given very limited information about O1's training process and architecture, 
% we can only speculate based on the observed behavior of O1 in the ZebraLogic task. 
% In this section, we provide some insights into how O1 might be reasoning differently from other LLMs, 
% based on the analysis of the reasoning steps extracted from the model's output.}

% \yuchen{mention scaling effects}

% \yuchen{mention the rankings correlation between ours and LMSYS arena. argue that ours is more reliable for reasoning}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
