
\section{Scaling Behavior of LLMs in Logical Reasoning}
\label{sec:discussion}

In this section, we focus on analyzing the scaling behavior of LLMs in logical reasoning. 
Our focuses are on two major types of scaling: 1) scaling model size and 2) scaling inference-time compute.
As for the inference-time compute, we further investigate three sub-dimensions: 1) the number of samples as candidates to select from, 2) the number of reasoning tokens (i.e., CoT tokens) for each inference, and 3) the sample size for repeated sampling.



\subsection{Scaling Model Size Can Hardly Break the Curse of Complexity in Reasoning}
As we have shown in Table~\ref{tab:model_performance} and Fig.~\ref{fig:scaling} (left),  the performance of LLMs on ZebraLogic is not necessarily improved by scaling up the model size when the search space is large.
% Taking the Llama-3 model series as an example, we can see that 

\begin{figure}[t]
    \centering
    %\includegraphics[width=0.9\linewidth]{figures/KK_data_generation.pdf}
    \includegraphics[width=1\linewidth]{assets/z3_scale.pdf}
    \vspace{-1mm}
    \caption{
    \small TODO
    }
    % \vspace{-2mm}
    \label{fig:z3_scale}
\end{figure}

Fig.~\ref{fig:scaling} highlights a key observation regarding the performance of various Llama models with different parameter sizes across an increasing search space size (logarithmic scale). A notable finding is that all model sizes experience a rapid decline in accuracy as the search space size increases, illustrating the challenge posed by complex reasoning tasks. This trend emphasizes the inherent difficulty models face in maintaining high accuracy beyond a certain threshold of search complexity, irrespective of their size. The phenomenon termed as the ``curse of complexity'' becomes evident as even the largest models, such as the Llama-3.1-405B, cannot sustain high accuracy once the search space surpasses a certain scale (e.g., \(10^6\)).


However, it is important to note the significant benefits of scaling model size when the search space is relatively small (e.g., less than \(10^6\)). In these cases, larger models like the Llama-3.1-405B and Llama-3.1-70B demonstrate substantial improvements in accuracy compared to smaller models such as the 3B and 8B versions. This suggests that scaling up the model size is an effective strategy for enhancing performance and tackling reasoning tasks in simpler search spaces. Yet, as the complexity of the search space grows beyond \(10^6\), the advantages of larger model sizes diminish, and scaling up the model size proves to be less impactful. This finding underscores the limited utility of model scaling when dealing with highly complex reasoning tasks, as the accuracy plateaus regardless of model size.

In addition to the search space size, we also use z3-conflict as the complexity measure to study the scaling behavior of Llama models. As shown in Fig.~\ref{fig:z3_scale}, we see a similar trend in the z3-conflict metric, which quantifies the number of conflicts encountered by the z3 solver when solving the ZebraLogic puzzles. We find that both Llama-3 and Qwen-2.5 series of models exhibit a rapid decrease in accuracy when the number of Z3 conflicts increases, indicating that the models struggle to maintain high performance when the puzzle becomes more complex. We can see that 20 is a critical point where the scaling effect of model size diminishes for both Llama-3 and Qwen-2.5 models.



This analysis presents a novel insight into the limitations of model size scaling, revealing that scaling up model sizes will eventually reach a point of diminishing returns when addressing complex search spaces. This suggests that, beyond a certain threshold of complexity, simply increasing the number of parameters in a model is insufficient to overcome the decline in performance. This finding highlights a critical boundary for current model scaling strategies, indicating that new approaches or methodologies may be necessary to break through the limitations imposed by high search space complexity and to achieve further advancements in reasoning capabilities.


\subsection{Scaling Inference-Time Compute: Best-of-N Sampling}



\begin{figure}[h]
    \centering
    % \hspace{-5mm} 
    \includegraphics[width=1\linewidth]{assets/reasoning_tokens.pdf}
    \vspace{-1mm}
    \caption{
    Comparison of O1-Mini and O1-Preview on ZebraLogic. The $x$-axis shows search space size (\textit{log} scale), and the $y$-axis shows Hidden CoT tokens. 
    Each point represents a puzzle, with color and shape indicating solution correctness. 
    A ZebraLogic puzzle with $N$ houses and $M$ attributes has a search space of $(N!)^M$.
    O1-Preview generates more correct solutions (blue dots) and fewer incorrect ones (red crosses), especially in larger search spaces. It also has smaller mean and variance of Hidden CoT tokens, indicating more consistent and efficient reasoning.
    }
    % \vspace{-2mm}
    \label{fig:o1_hidden_cot}
\end{figure}


\textbf{O1 generates large-scale hidden reasoning tokens, which scale well with the search space size.}
One of the key differences between O1 and other LLMs is the way they use more inference-time cost to decode hidden chain-of-thoughts (CoT) tokens during inference time, which are not directly visible to users.
In Figure~\ref{fig:o1_hidden_cot}, we have plotted the number of hidden CoT tokens against the search space size for both O1-Mini and O1-Preview. 
In each sub-figure on the top, we plot 1,000 points, each representing a puzzle. 
The color and shape of the points indicate whether the model produced a correct solution (blue dots) or an incorrect one (red crosses).
The y-axis shows the number of hidden CoT tokens generated by the model, while the x-axis shows the search space size in logarithmic scale. The definition of search space size is provided in Section~\ref{ssec:dataset_creation}, and a larger search space usually indicates a more complex puzzle. 
We can see that the number of hidden CoT tokens generated by O1 is scaling with the search space size, indicating that O1 is able to leverage more reasoning steps when faced with more complex puzzles. 
% The inference-time compute scales with the search space size in logarithmic scale.


\textbf{GPT-4o tends to generate more \textit{visible} reasoning tokens than O1.}
Interestingly, we find that the GPT4o model tends to generate more visible reasoning tokens than O1, especially when the search space is large, which is shown in the lower part of Figure~\ref{fig:o1_hidden_cot}.
The visible reasoning tokens are generated by the model and displayed in their outputs before the final solution grids.
We can see that until the search space reaches the Large category (especially when the search space size is $<10^5$), the four models generate similar numbers of visible reasoning tokens. However, when the search space size is larger, GPT4o generates more visible reasoning tokens yet still fails to solve the puzzles.
O1 models, which have used more hidden CoT tokens, tend to output fewer visible reasoning tokens for describing their reasoning process.



\section{Discussion on O1's Reasoning Process}

\subsection{How does O1 reason? Deciphering O1's Reasoning Process from Visible to Hidden Tokens}

\textbf{Visible outputs from O1 cannot fully explain its reasoning for complex problems.}
To understand how O1 reasons, we have to focus on their public reasoning steps that we can extract from the model's visible outputs. 
From our human evaluation on their reasoning steps, we find that O1's reasoning steps are not necessarily rigorous or complete, even when they arrive at the correct solution.
For small-to-medium search spaces, O1-preview's reasoning chains tend to be complete, while O1-mini sometimes can skip some steps to directly reach the solution.
For problems with larger search spaces, O1's visible reasoning chains tend to be very incomplete, and sometimes even incorrect, especially when the reasoning process requires backtracking.
For example, O1's visible reasoning may contain steps such as ``Bob cannot be in Houses 1, 4, or 5, so he must be in House 3'' without explaining why Bob cannot be in House 2, although it will indeed lead to the correct solution. Note that such cases also happen for other LLMs such as GPT-4o. 
We thus describe that the reasoning process of LLMs and O1 models are sometimes based on guessing without formal logic, especially for complex problems with large search spaces, rater than rigorous logical reasoning.

Such incomplete reasoning steps are very common in O1's outputs, especially for puzzles with larger search spaces, leading to unreliable explanations of their reasoning process.
Thus, we argue that the visible reasoning steps from O1 cannot help us understand how O1 reasons for complex problems.
Furthermore, knowledge distillation from O1's reasoning steps is not necessarily helpful for improving the performance of other LLMs, as the reasoning steps are often incomplete and sometimes incorrect. This raises questions about the concern of hidden CoT tokens in their reasoning process that are not visible in the output.





\textbf{Will the summary of hidden tokens help us understand O1's reasoning?}
Although the hidden CoT tokens are not visible from the OpenAI APIs, we can see an overview summary of the hidden reasoning tokens on ChatGPT's user interface for O1's hidden reasoning steps.
By manually analyzing the overview summary of hidden reasoning tokens, we find it is still hard to clearly understand how O1 reasons for complex problems.
We can sometimes see some intermediate results in the overview but not any explanations for the decision.
Interestingly, we can see some behaviors of recognizing the contradictions of previous assumptions and revisiting the clues to refine the solution.
Such an in-context reflection behavior is hardly noticeable in other LLMs such as GPT-4o's reasoning, and it may be a key factor for O1's success in solving complex problems.
Typical steps in O1's hidden reasoning include: ``Laying out the options'', ``Piecing together clues'', ``Pinpointing the clues'', ``Reevaluating assumptions'', ``Revisiting clues.'', ``Mapping out connections'', ``Tracking movement'', etc.
We show some case studies in the Appendix to understand better how O1 reasons. 



% We find that O1's reasoning steps are not necessarily rigorous or complete, even when they arrive at the correct solution.
% Their reasoning chains tend to be complete when the search space is small and the reasoning process does not require backtracking. 
% However, as the search space increases and when the puzzles require some backtracking by making assumptions and revisiting previous steps to refine the solution, O1's reasoning becomes less clear and more prone to errors.
% It is unknown how O1 manages to arrive at the correct solution in these cases, which raises questions about the role of hidden CoT tokens in their reasoning process that are not visible in the output.

% Here we show a few case studies and leave more detailed analysis for the Appendix.






% \yuchen{How is it possible that O1 can solve a problem with super large search space while LLMs fail? We can show some examples of reasoning steps and how they differ.}


% \yuchen{Interestingly, I found that many cases for O1-preview's successful solution involve some wrong reasoning paths and maybe guessing. Why is it so lucky but others are not? Do they really use hidden CoT tokens to do the reasoning or is it just luck?}


 

% \yuchen{Add a few real examples from the Chat interface to understand better how O1 reasons}


% \subsection{Visible Reasoning Lengths of Other LLMs}
% \yuchen{We can talk about this in the appendix.}
