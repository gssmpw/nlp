%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% \vspace{-0.2cm}
% \section{How can we improve the reasoning performance of LLMs?}
% \vspace{-0.2cm}
% \subsection{Methods to Improve Reasoning Performance of LLMs}
% \label{sec:methods}

% Given that O1's performance can largely be attributed to its higher inference-time compute and in-context reflection capabilities, we explore several strategies to improve the reasoning performance of LLMs. 
% We first analyze the potential of scaling the inference-time compute of LLMs and then investigate the practicality of using majority voting or existing reward models to select the best answer from the sampled candidates.

 


% \subsection{Selection with Reward Models}



% \subsection{Distillation from O1's Reasoning}
% \yuchen{TODO: here we can test the hypothesis that if we provide the order of using Clues, then we can improve the reasoning of LLMs with 4o-mini} 
% \yuchen{We can also give a pair of 4o-mini's wrong results vs o1-mini's correct results to ask the model to generate the reflection thoughts.}


% \subsection{Program-based Reasoning}
% \yuchen{Can model like gpt4o-mini generate python code to do?}









