%\section{Conclusion and Future Directions}
% \vspace{-1em}
\section{Conclusion}
\label{sec:conclusion}

% We introduce \emph{ZebraLogic}, a controlled benchmark of 1,000 logic grid puzzles designed to systematically evaluate the logical reasoning performance of LLMS. Our results reveal a staggering performance drop as puzzle complexity increases - a ``curse of complexity'' that persists despite model scaling or enhanced training data. While scaling the inference-time compute  by increasing the generation sample size provides performance boosts, its impact is limited; in contrast, our experiments show that scaling the number of reasoning tokens generated during inference with a backtracking algorithm is notably more effective. These observations underscore the need for training LLMs to reason step by step explicitly, and we hope that this work will serve as a catalyst for further research on advanced reasoning paradigms. 
% % \section*{Acknowledgment}

% NSF DMS-2134012, ONR N00014-24-1-2207
% ...existing code...
We introduce \emph{ZebraLogic}, a controlled benchmark of 1,000 logic grid puzzles that highlights the scaling limits of LLM-based reasoning through carefully adjustable complexity. Our experiments reveal a pronounced drop in model performance as puzzle complexity increases, overshadowing gains from model growth or training data expansions. While increasing the generation sample size yields modest improvements, a backtracking-based approach with expanded reasoning steps significantly boosts accuracy. These results spotlight the importance of explicit, step-by-step reasoning strategies and provide a valuable framework for advancing logical reasoning research in LLMs.
% ...existing code...