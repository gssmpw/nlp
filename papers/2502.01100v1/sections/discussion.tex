
\subsection{Scaling Behavior of LLMs in Logical Reasoning}
% \label{sec:discussion}

In the following sections, we study the scaling behavior of LLMs in logical reasoning, as illustrated in Fig.~\ref{fig:z3_scale}. Our analysis focuses on two primary types of scaling: 1) scaling model size and 2) scaling test-time compute. For test-time compute, we further explore three sub-dimensions: 1) the number of candidate samples, 2) the number of reasoning tokens (i.e., CoT tokens) generated during inference, and 3) the sample size for repeated sampling.



\section{Scaling Model Size Can Hardly Break the Curse of Complexity in Reasoning}
\label{sec:size_scale}
% As we have shown in Table~\ref{tab:model_performance} and Fig.~\ref{fig:scaling} (left),  the performance of LLMs on ZebraLogic is not necessarily improved by scaling up the model size when the search space is large.
% Taking the Llama-3 model series as an example, we can see that 




\textbf{The Curse of Complexity in Reasoning for non-reasoning LLMs.}
In addition to the search space size, we also use Z3-conflict as the complexity measure to study the scaling behavior LLMs.
Fig.~\ref{fig:z3_scale} (left) highlights a key observation regarding the performance of various Llama models with different model sizes across an increasing complexity in terms of how many Z3 conflicts on average are encountered when solving the ZebraLogic puzzles. 
A notable finding is that all model sizes experience a rapid decline in accuracy as the complexity increases, illustrating the challenge posed by complex reasoning tasks. This trend emphasizes the inherent difficulty models face in maintaining high accuracy beyond a certain threshold of search complexity, irrespective of their size. The phenomenon termed as the ``curse of complexity'' becomes evident as even the largest models, such as the Llama-3.1-405B, cannot sustain high accuracy once the search space surpasses a certain scale.
As shown in Fig.~\ref{fig:scaling}, we see a similar trend in the search space size. 
% We find that both Llama-3 and Qwen-2.5 series of models exhibit a rapid decrease in accuracy when the number of Z3 conflicts increases, indicating that the models struggle to maintain high performance when the puzzle becomes more complex. We can see that 15 is a critical point where the scaling effect of model size diminishes for both Llama-3 and Qwen-2.5 models.



\begin{figure*}[h]
    % \centering
    % \hspace{-6mm} 
    \includegraphics[width=1\linewidth]{assets/scale.pdf}
    % \vspace{-1mm}
    \caption{  
        % Accuracy vs search space size for different LLMs and inference-time compute strategies on ZebraLogic, a dataset of logic grid puzzles. 
        % \yejin
        \textbf{Accuracy vs Search Space Size} (log scale) comparing multiple scaling behavior of LLMs on ZebraLogic. Left: Scaling model sizes. Right: Scaling test-time compute through two approaches - increasing sample size (via pass@k evaluation) and extending chain-of-thought reasoning length. Both model size and test-time compute show diminishing returns as search space complexity grows beyond a certain complexity.
        More results are presented in Sec.~\ref{sec:evaluation}.
        % \yuchen{considering removing this as it has almost the same findings as we have shown in the Fig~.\ref{fig:z3_scale}. We can put it here and move this to the appendix?}
        % \ashish{do you mean \underline{no} figure in the intro at all? that would be odd, no? I agree this fig and Fig.~\ref{fig:z3_scale} are similar in nature. So you could keep one here and move the other to the appendix; discuss the one here in more detail in \S\ref{sec:evaluation} and mention somewhere that trends on the other are similar (see appendix).}
    }
    % \vspace{-2mm}
    \label{fig:scaling}
\end{figure*}

% \textbf{Curse of Complexity is a Major Challenge for Scaling Model Size.}
% Fig.~\ref{fig:scaling} highlights a key observation regarding the performance of various Llama models with different parameter sizes across an increasing search space size (logarithmic scale). A notable finding is that all model sizes experience a rapid decline in accuracy as the search space size increases, illustrating the challenge posed by complex reasoning tasks. This trend emphasizes the inherent difficulty models face in maintaining high accuracy beyond a certain threshold of search complexity, irrespective of their size. The phenomenon termed as the ``curse of complexity'' becomes evident as even the largest models, such as the Llama-3.1-405B, cannot sustain high accuracy once the search space surpasses a certain scale (e.g., \(10^6\)).

\textbf{Scaling model size is only effective for smaller search spaces.}
However, it is important to note the significant benefits of scaling model size when the search space is relatively small (e.g., \(\le 10^6\)). In these cases, larger models like the Llama-3.1-405B and Llama-3.1-70B demonstrate substantial improvements in accuracy compared to smaller models such as the 3B and 8B versions. This suggests that scaling up the model size is an effective strategy for enhancing performance and tackling reasoning tasks in simpler search spaces. Yet, as the complexity of the search space grows beyond \(10^6\), the advantages of larger model sizes diminish, and scaling up the model size proves to be less impactful. This finding underscores the limited utility of model scaling when dealing with highly complex reasoning tasks, as the accuracy plateaus regardless of model size.


\textbf{Model Size Scaling Limitations.}
This analysis reveals that scaling up model sizes eventually reaches a point of diminishing returns in complex search spaces. Beyond a certain complexity threshold, increasing model parameters is insufficient to prevent performance decline. This highlights a critical boundary for current scaling strategies, suggesting that new approaches are needed to overcome the limitations imposed by high search space complexity and to advance reasoning capabilities further.






\section{Scaling Test-Time Compute with Repeated Sampling: Promises \& Challenges}
\label{sec:sampling_scale}

We examine the impact of scaling test-time compute, a crucial factor affecting LLM performance on logical reasoning tasks. Specifically, here we investigate how increasing the number of candidate samples influences model performance.
We begin by employing Best-of-N (BoN) sampling, where we repeatedly sample N candidates from the model for each puzzle. From these candidates, we can select the best answer using various strategies, including majority voting and existing reward models. To understand the theoretical upper bound of this approach, we also analyze BoN sampling with oracle selection, where we use knowledge of the correct answer to choose the best candidate from the sample pool - equivalent to the pass@k metric in our evaluation (see the right-most plot in Fig.~~\ref{fig:z3_scale} and Fig.\ref{fig:scaling}).



% \textbf{Simply Scaling test-time Compute is Promising but Not Sufficient}

\textbf{GPT-4o with Best-of-N sampling and oracle selections can achieve nearly o1 performance.} To understand the potential improvement of scaling test-time compute for logical reasoning, 
we sample 128 candidates from GPT-4o-mini and GPT-4o and study the \textit{coverage} of the correct answer in the sampled candidates.
In Table~\ref{tab:analysis}, we refer to this coverage metric as \textit{BoN-Oracle}, meaning that the best-of-N (BoN) selection is performed given the oracle knowledge of the correct answer, i.e., the pass@k metric.

We observe that the BoN-Oracle selection can significantly improve the performance of GPT-4o-mini and GPT-4o. For example, GPT-4o with BoN-Oracle$_{N=128}$ achieves an overall accuracy of 69.1\%, which is higher than O1-mini's accuracy of 59.7\% and a potential scaling effect that can also outperform O1-preview's accuracy of 71.4\% if we keep enlarging the sampling size. 
Note that on the Medium-size examples, we can already see a higher accuracy of 92.9\% for BoN-Oracle$_{N=128}$ compared O1-preview's 88.2\%, and the trend shown in the curves indicates that the performance of GPT-4o can be further improved with more test-time compute. Fig.~\ref{fig:sampling} in Appendix provides further analysis on how sampling affects model performance.


% \textbf{Majority voting and existing Reward Models can help but still very limited.}
% The above results suggest that the performance of LLMs can be improved by scaling the test-time compute. However, the performance of LLMs can be significantly affected by how we select the answer from the sampled candidates.
% Two common strategies are majority voting and using existing reward models to select the best answer~\citep{Jiang2023LLMBlenderEL}. 

\textbf{Majority Voting is simple yet effective.}
For majority voting, we rank the candidates based on the frequency of each cell in their solution grid, and select the candidate with the highest sum of frequencies. 
% Specifically, for each cell in a candidate's solution grid, we count the number of times the cell is filled with the same value across all other candidates, and use the sum of these counts as the score for the candidate. 
% We find this aggregation method is better than simply selecting the candidate with the highest frequency of the global solution grid. 
As for the Reward Model (RM), we choose the one that ranks to the top on Ai2's RewardBench leaderboard~\citep{Lambert2024RewardBenchER}, named Skywork-Reward-Llama-3.1-8B-v0.2~\citep{liu2024skywork}. 
We find that using Majority Voting for GPT-4o can improve from 31.7 to 38.0 (for the overall accuracy) when the sample size N=32, while keep increasing the sample size does not necessarily improve the performance any more. 
Also, the performance of GPT-4o with BoN-RM$_{N=32}$ is  33.9, which is worse than majority voting, suggesting that the current reward models that are mainly designed for chat or general instruction following tasks may not be directly applicable to (logical) reasoning tasks.
% Our preliminary results on using larger RMs show that the performance is not necessarily improved.
% Process Reward Models (PRMs)~\citep{Lightman2023LetsVS} may be more suitable for reranking candidates and reinforcing the learning process of LLMs for reasoning tasks, but general-purpose PRMs are relatively rare and expensive to train.





\begin{figure*}[t]
    % \begin{wrapfigure}{r}{0.4\textwidth}
        % \vspace{-0.2cm}
        % \centering
        % \vspace{-0.2cm}
        % \hspace{-4mm}
        \includegraphics[width=1\linewidth]{assets/cot_z3_wide.pdf}
        \caption{The o1 models' hidden CoT tokens vs. the number of Z3 conflicts. Each point is an example with a certain number of Z3 conflicts. Larger number of Z3 conflicts are associated with harder reasoning problems. 
        % \yuchen{We have a new version for o1 (instead of o1-preview). And the comments about this part should be changed.}
        }
        \vspace{-1em}
        \label{fig:o1_preview_hidden_cot_z3}
    % \end{wrapfigure}
\end{figure*}



\begin{table}[t]
    % \begin{minipage}[t]{0.68\textwidth}  % Adjust width ratio as needed
    \centering 
    \scalebox{0.72}{
     \begin{tabular}{@{}rccccc}
        \toprule
        \multicolumn{1}{c}{\textbf{Model} \& \textbf{Methods} }  & \multicolumn{1}{c}{\textbf{Overall}} & \multicolumn{1}{c}{\textbf{Small}} & \multicolumn{1}{c}{\textbf{Medium}} & \multicolumn{1}{c}{\textbf{Large}} & \multicolumn{1}{c}{\textbf{X-Large}} \\
        \midrule
        % \multicolumn{1}{c}{O1-preview} & \gradientcell{71.4} & {98.1} & {88.2} & {59.5} & {17.0} \\ 
        % \multicolumn{1}{c}{O1-mini} & \gradientcell{59.7} & {87.5} & {76.8} & {39.0} & {12.0} \\ \midrule \midrule
        \rowcolor{gray!20} 
        \multicolumn{1}{>{\columncolor{gray!20}}l}{{\tiny \faBullseye} \underline{\textbf{GPT-4o}} $\searrow$} & 
        \multicolumn{1}{>{\columncolor{gray!20}}c}{\gradientcell{31.7}} & 
        \multicolumn{1}{>{\columncolor{gray!20}}c}{80.0} & 
        \multicolumn{1}{>{\columncolor{gray!20}}c}{19.6} & 
        \multicolumn{1}{>{\columncolor{gray!20}}c}{2.5} & 
        \multicolumn{1}{>{\columncolor{gray!20}}c}{0.5} \\
        BoN-Oracle$_{N=128}$ {\scriptsize\faLightbulb} & \gradientcell{69.1} & {99.7} & {92.9} & {49.0} & {7.0} \\
        BoN-Oracle$_{N=32}$ {\scriptsize\faLightbulb} & \gradientcell{60.3} & {98.4} & {81.1} & {28.0} & {2.5} \\
        Majority-Voting$_{N=128}$ & \gradientcell{37.6} & {84.7} & {32.1} & {7.5} & {0.0} \\
        Majority-Voting$_{N=32}$ & \gradientcell{38.0} & {84.1} & {34.3} & {7.0} & {0.5} \\
        
        BoN-RM$_{N=32}$ & \gradientcell{33.9} & {77.8} & {28.9} & {4.5} & {0.0} \\
        \hline
        Self-Verify (Oracle) {\scriptsize\faLightbulb} & \gradientcell{34.8} & {83.8} & {24.6} & {5.0} & {0.5} \\
        Self-Verify & \gradientcell{33.0} & {82.2} & {22.1} & {2.5} & {0.0} \\
        Self-Verify (x2) & \gradientcell{32.1} & {80.0} & {21.4} & {2.5} & {0.0} \\
        
        \midrule\midrule
        \rowcolor{gray!20} 
        \multicolumn{1}{>{\columncolor{gray!20}}l}{{\tiny \faBullseye} \underline{\textbf{GPT-4o-mini}} $\searrow$} & 
        \multicolumn{1}{>{\columncolor{gray!20}}c}{\gradientcell{20.1}} & 
        \multicolumn{1}{>{\columncolor{gray!20}}c}{58.8} & 
        \multicolumn{1}{>{\columncolor{gray!20}}c}{4.6} & 
        \multicolumn{1}{>{\columncolor{gray!20}}c}{0.0} & 
        \multicolumn{1}{>{\columncolor{gray!20}}c}{0.0} \\
        BoN-Oracle$_{N=128}$ {\scriptsize\faLightbulb} & \gradientcell{51.2} & {99.7} & {61.8} & {10.0} & {0.0} \\
        BoN-Oracle$_{N=32}$ {\scriptsize\faLightbulb} & \gradientcell{42.7} & {97.8} & {39.3} & {2.0} & {0.0} \\
        Majority-Voting$_{N=128}$ & \gradientcell{25.0} & {69.4} & {8.9} & {1.5} & {0.0} \\
        Majority-Voting$_{N=32}$ & \gradientcell{24.5} & {69.1} & {8.2} & {0.5} & {0.0} \\
        BoN-RM$_{N=32}$ & \gradientcell{22.5} & {62.2} & {9.3} & {0.0} & {0.0} \\
        \hline
        Self-Verify (Oracle) {\scriptsize\faLightbulb} & \gradientcell{22.3} & {65.0} & {5.4} & {0.0} & {0.0} \\
        Self-Verify & \gradientcell{21.1} & {60.9} & {5.7} & {0.0} & {0.0} \\
        \bottomrule
    \end{tabular}
    }
    % \end{minipage}%
    % \begin{minipage}[h!]{0.3\textwidth}  % Adjust width ratio as needed
    \caption{
    Comparison of various test-time compute scaling methods applied to GPT-4o and GPT-4o-mini. We evaluate several approaches: BoN-Oracle (selection using oracle knowledge to verify correct answers among samples), BoN-RM (selection using a reward model), Majority-Voting (selecting the most common answer across samples), and Self-Verify (using multi-turn prompting for self-reflection and correction, with and without oracle knowledge). 
    We use {\scriptsize\faLightbulb} to denote the use of oracle knowledge.
    % O1 scales the test-time compute by generating more hidden reasoning tokens.
    %Further analysis of these methods is presented in Fig.~\ref{fig:sampling} and Sec.~\ref{sec:sampling_scale}.
    % \vspace{-2em}
    } 
    \label{tab:analysis}
    % \end{minipage}
\end{table}


% \begin{table}[h!]
%     \centering 
%     \scalebox{0.7}{
%     \begin{tabular}{@{}rcccccc}
%     \toprule
%     \multicolumn{1}{c}{\textbf{Model} \& \textbf{Methods} }  & \multicolumn{1}{c}{\textbf{Overall}} & \multicolumn{1}{c}{\textbf{Small}} & \multicolumn{1}{c}{\textbf{Medium}} & \multicolumn{1}{c}{\textbf{Large}} & \multicolumn{1}{c}{\textbf{X-Large}} & \multicolumn{1}{c}{\textbf{Cell Acc}} \\
%     \midrule
%     \multicolumn{1}{c}{O1-preview} & \gradientcell{71.4} & {98.1} & {88.2} & {59.5} & {17.0} & {75.1} \\ 
%     \multicolumn{1}{c}{O1-mini} & \gradientcell{59.7} & {87.5} & {76.8} & {39.0} & {12.0} & {70.3} \\ \midrule \midrule
%     \rowcolor{gray!20} 
%     \multicolumn{1}{>{\columncolor{gray!20}}l}{{\tiny \faBullseye} \underline{\textbf{GPT-4o}} $\searrow$} & 
%     \multicolumn{1}{>{\columncolor{gray!20}}c}{\gradientcell{31.7}} & 
%     \multicolumn{1}{>{\columncolor{gray!20}}c}{80.0} & 
%     \multicolumn{1}{>{\columncolor{gray!20}}c}{19.6} & 
%     \multicolumn{1}{>{\columncolor{gray!20}}c}{2.5} & 
%     \multicolumn{1}{>{\columncolor{gray!20}}c}{0.5} & 
%     \multicolumn{1}{>{\columncolor{gray!20}}c}{50.3} \\
%     BoN-Oracle$_{N=128}$ {\scriptsize\faLightbulb} & \gradientcell{69.1} & {99.7} & {92.9} & {49.0} & {7.0} & {86.1} \\
%     BoN-Oracle$_{N=32}$ {\scriptsize\faLightbulb} & \gradientcell{60.3} & {98.4} & {81.1} & {28.0} & {2.5} & {80.5} \\
%     Majority-Voting$_{N=128}$ & \gradientcell{37.6} & {84.7} & {32.1} & {7.5} & {0.0} & {58.5} \\
%     Majority-Voting$_{N=32}$ & \gradientcell{38.0} & {84.1} & {34.3} & {7.0} & {0.5} & {57.4} \\
    
%     BoN-RM$_{N=32}$ & \gradientcell{33.9} & {77.8} & {28.9} & {4.5} & {0.0} & {53.2} \\
%     \hline
%     Self-Verify (Oracle) {\scriptsize\faLightbulb} & \gradientcell{34.8} & {83.8} & {24.6} & {5.0} & {0.5} & {52.7} \\
%     Self-Verify & \gradientcell{33.0} & {82.2} & {22.1} & {2.5} & {0.0} & {52.5} \\
%     Self-Verify (x2) & \gradientcell{32.1} & {80.0} & {21.4} & {2.5} & {0.0} & {52.8} \\
    
%     \midrule\midrule
%     \rowcolor{gray!20} 
%     \multicolumn{1}{>{\columncolor{gray!20}}l}{{\tiny \faBullseye} \underline{\textbf{GPT-4o-mini}} $\searrow$} & 
%     \multicolumn{1}{>{\columncolor{gray!20}}c}{\gradientcell{20.1}} & 
%     \multicolumn{1}{>{\columncolor{gray!20}}c}{58.8} & 
%     \multicolumn{1}{>{\columncolor{gray!20}}c}{4.6} & 
%     \multicolumn{1}{>{\columncolor{gray!20}}c}{0.0} & 
%     \multicolumn{1}{>{\columncolor{gray!20}}c}{0.0} & 
%     \multicolumn{1}{>{\columncolor{gray!20}}c}{41.3} \\
%     BoN-Oracle$_{N=128}$ {\scriptsize\faLightbulb} & \gradientcell{51.2} & {99.7} & {61.8} & {10.0} & {0.0} & {75.7} \\
%     BoN-Oracle$_{N=32}$ {\scriptsize\faLightbulb} & \gradientcell{42.7} & {97.8} & {39.3} & {2.0} & {0.0} & {68.9} \\
%     Majority-Voting$_{N=128}$ & \gradientcell{25.0} & {69.4} & {8.9} & {1.5} & {0.0} & {47.9} \\
%     Majority-Voting$_{N=32}$ & \gradientcell{24.5} & {69.1} & {8.2} & {0.5} & {0.0} & {46.4} \\
%     BoN-RM$_{N=32}$ & \gradientcell{22.5} & {62.2} & {9.3} & {0.0} & {0.0} & {42.5} \\
%     \hline
%     Self-Verify (Oracle) {\scriptsize\faLightbulb} & \gradientcell{22.3} & {65.0} & {5.4} & {0.0} & {0.0} & {42.4} \\
%     Self-Verify & \gradientcell{21.1} & {60.9} & {5.7} & {0.0} & {0.0} & {41.9} \\
%     \bottomrule
%     \end{tabular}
%     }
%     \caption{Performance of LLMs on ZebraLogic. The overall accuracy is calculated based on the number of puzzles solved correctly. We also report the accuracy on small, medium, large, and x-large groups based on the size of the search space (as defined in Sec.~\ref{ssec:dataset_creation}). The cell accuracy indicates the percentage of individual cells filled correctly.
%     Curves with more data points can be found in Figure~\ref{fig:sampling}.
%     }
%     \label{tab:analysis}
% \end{table}


% \vspace{-0.2cm}


\vspace{-1em}

\section{Scaling Test-Time Compute with Extensive Chain-of-Thoughts Tokens}
\label{sec:cot_scale}

% In addition to scaling the sample size (Sec.~\ref{sec:sampling_scale}),
Another approach of scaling test-time compute is to increase the number of reasoning tokens (i.e., chain-of-thoughts tokens) that the model generates during inference.
% We first analyze the CoT tokens with respect to complexity, 
% and then explore  self-refinement prompting strategies that instruct LLMs to verify and refine their reasoning. 


\subsection{o1 Generates More Hidden Reasoning Tokens}

% \yuchen{given R1's release this part might not be interesting any more; considering moving it to appendix; and summarize the key points here only.}
% \ashish{summarizing the 2-3 key observations here briefly makes sense; then point to the appendix for details}

\textbf{o1 generates large-scale hidden reasoning tokens.} 
One of the key differences between o1 and other LLMs is the way they use more test-time compute to decode much more hidden chain-of-thoughts (CoT) tokens during inference time, which are not directly visible to users.
Our analysis shows that o1 models scale their hidden CoT tokens with puzzle complexity - producing on average 5,144.6 (o1-mini) and 5,346.3 (o1-preview) hidden reasoning tokens compared to 502.9 and 543.7 for GPT-4o-mini and GPT-4o respectively. This order of magnitude difference in reasoning steps appears to contribute to o1's superior performance on logical reasoning tasks. For detailed analysis of how hidden CoT tokens vary with puzzle complexity, see Appendix~\ref{app:hidden_cot_analysis}.




Figure~\ref{fig:o1_preview_hidden_cot_z3} reveals a positive correlation between the number of hidden reasoning tokens generated by o1-preview and Z3 conflicts, aligning with our earlier observation that o1 allocates more reasoning tokens to more complex puzzles. For puzzles with fewer than 20 Z3 conflicts, we observe a consistent ratio of approximately 400 hidden reasoning tokens per conflict. However, this scaling pattern plateaus when Z3 conflicts exceed 30, suggesting that o1-preview may have reached its maximum reasoning capacity at the current model size. This suggests that while o1-preview can effectively leverage more reasoning tokens for complex puzzles, there is a limit to the extent to which it can scale reasoning tokens to address highly complex reasoning tasks.
With the recent release of o1-full, we find that our previous estimation is consistent with the actual number of hidden reasoning tokens generated by o1-full, which is around 5,000 on average. This further confirms the scaling behavior of o1 models in generating more hidden reasoning tokens for complex puzzles. 

% Average number of hidden reasoning tokens: O1-mini: 5,144.6, O1-preview: 5,346.3
% Average number of visible reasoning tokens: GPT-4o-mini: 502.9, GPT-4o: 543.7, O1-mini: 305.7, O1-preview: 402.4.





We also find that when o1-preview make mistakes, they usually generate more hidden reasoning tokens than when they solve the puzzles correctly, which is consistent with the observation that o1 tends to generate more reasoning tokens for more complex puzzles that are harder to solve.

% \vspace{-0.2cm}



\subsection{Self-Refinement is Limited but Promising}
\label{ssec:self_refinement}

% O1 shows ability to reflect on and refine its reasoning process by revisiting clues and constraints, similar to Z3 solver's conflict-driven clause learning. We test this capability through multi-turn conversations with two settings: with and without oracle knowledge of correct answers.
The other feature of o1's hidden reasoning process is the ability to reflect on its own reasoning process and refine its answer.
From our observation on the summary of their hidden reasoning process, we can see that o1 often revisits the clues and constraints to verify its previous reasoning and fix the errors if there are any, which is similar to the Z3 solver's conflict-driven clause learning mechanism.
In order to elicit such self-refinement behavior from LLMs, we add follow-up queries to ask the model to review its initial answer and check the clues and constraints again in a multi-turn conversation setting. 
% \ashish{could move most of the following details to the appendix}
There are two settings for the self-refinement process: one with the oracle knowledge of the correct answer and the other without the oracle knowledge.
Results in Table~\ref{tab:analysis} show modest improvements with self-verification, particularly without oracle knowledge (4o improves from 31.7 to 33.0, then decreases to 32.1 on second iteration).

\begin{AIbox}{Self-Verification Prompt}
    \textbf{Self-Verify:} 
    \textit{Your answer may be incorrect!  Identify any mistakes in your reasoning and answer, if any. Correct them to ensure they align with the given information.  Present your updated response in the same JSON format mentioned in the initial prompt. }  

    \textbf{Self-Verify (Oracle {\scriptsize \faLightbulb}):}
    \begin{itemize}[leftmargin=10pt,itemsep=0pt,parsep=0pt,topsep=0pt,partopsep=0pt]
        \item \textbf{For \textit{incorrect} results:} \textit{Your answer is incorrect! Re-examine the clues, correct the mistakes, and then provide the revised solution in the original JSON format.}
        \item \textbf{For \textit{correct} results:} \textit{Your answer is correct. Please repeat the json-formatted output again.}
    \end{itemize}  
\end{AIbox}


% \subsection{Self-Refinement is Limited but Promising}
% \label{ssec:self_refinement}

% % \yuchen{Here we test what if we ask the model to verify its own reasoning. It is the 2nd turn of conversation, we ask the model to review its initial answer and check the clues and constraints again and fix the errors if there are any.}
% % \yuchen{We can do this multiple turns and see if the performance can be continually improved.}

% The other feature of o1's hidden reasoning process is the ability to reflect on its own reasoning process and refine its answer.
% From our observation on the summary of their hidden reasoning process, we can see that o1 often revisits the clues and constraints to verify its previous reasoning and fix the errors if there are any, which is similar to the Z3 solver's conflict-driven clause learning mechanism.
% In order to elicit such self-refinement behavior from LLMs, we add follow-up queries to ask the model to review its initial answer and check the clues and constraints again in a multi-turn conversation setting. 
% % \ashish{could move most of the following details to the appendix}
% There are two settings for the self-refinement process: one with the oracle knowledge of the correct answer and the other without the oracle knowledge.

% % \begin{AIbox}{Self-Verification Prompt}
%     \textbf{Self-Verify:} 
%     \textit{Your answer may be incorrect!  Identify any mistakes in your reasoning and answer, if any. Correct them to ensure they align with the given information.  Present your updated response in the same JSON format mentioned in the initial prompt. }  

%     \textbf{Self-Verify (Oracle {\scriptsize \faLightbulb}):}
%     \begin{itemize}[leftmargin=10pt,itemsep=0pt,parsep=0pt,topsep=0pt,partopsep=0pt]
%         \item \textbf{For \textit{incorrect} results:} \textit{Your answer is incorrect! Re-examine the clues, correct the mistakes, and then provide the revised solution in the original JSON format.}
%         \item \textbf{For \textit{correct} results:} \textit{Your answer is correct. Please repeat the json-formatted output again.}
%     \end{itemize}  
% % \end{AIbox}

% The results of the self-refinement process appear in Table~\ref{tab:analysis}.
% We find that the self-verification process can help improve the performance of GPT-4o-mini and GPT-4o but the improvement is limited, especially when the oracle knowledge is not provided. 
% For example, GPT-4o using Self-Verify without oracle knowledge can only improve from 31.7 to 33.0, and when we apply the second time of self-verification, the performance is decreased to 32.1. 
% That suggests that the self-refinement process is not as effective as the BoN-Oracle selection, and the model may need more guidance or supervision to improve its reasoning performance.






% We find that O1's reasoning steps are not necessarily rigorous or complete, even when they arrive at the correct solution.
% Their reasoning chains tend to be complete when the search space is small and the reasoning process does not require backtracking. 
% However, as the search space increases and when the puzzles require some backtracking by making assumptions and revisiting previous steps to refine the solution, O1's reasoning becomes less clear and more prone to errors.
% It is unknown how O1 manages to arrive at the correct solution in these cases, which raises questions about the role of hidden CoT tokens in their reasoning process that are not visible in the output.

% Here we show a few case studies and leave more detailed analysis for the Appendix.






% \yuchen{How is it possible that O1 can solve a problem with super large search space while LLMs fail? We can show some examples of reasoning steps and how they differ.}


% \yuchen{Interestingly, I found that many cases for O1-preview's successful solution involve some wrong reasoning paths and maybe guessing. Why is it so lucky but others are not? Do they really use hidden CoT tokens to do the reasoning or is it just luck?}


 

% \yuchen{Add a few real examples from the Chat interface to understand better how O1 reasons}


% \subsection{Visible Reasoning Lengths of Other LLMs}
% \yuchen{We can talk about this in the appendix.}




% \begin{figure}[t]
%     \centering
%     % \hspace{-4mm} 
%     \includegraphics[width=1\linewidth]{assets/o1.hidden_cot.z3_v2.png}
%     % \vspace{-1mm}
%     \caption{
%     The hidden CoT Tokens vs Z3 conflicts for o1-full.
%     }
%     % \vspace{-2mm}
%     \label{fig:o1_full_z3}
% \end{figure}
