% \vspace{-0.2cm}
\section{Related Work}
\label{sec:related}
% \vspace{-0.2cm}

%\ronan{WIP, with help from NORA ;)}
% \ronan{Need to drastically restructure!!!!}

\begin{comment}

% THIS IS THE OLD VERSION

\paragraph{Logical Reasoning with LLMs}
Recent advancements in NLP have revitalized interest in logical reasoning, a foundational aspect of AI that remains challenging for even the most advanced models. Early efforts to benchmark logical reasoning include LogiQA \cite{Liu2020LogiQAAC}, which targets complex logical comprehension questions. Building upon this, \cite{Liu2023LogiQA2I} introduced an enhanced variant of the dataset, reframing it as a Natural Language Inference (NLI) task to further stress-test LLMs’ abilities. In parallel, \cite{Clark2020TransformersAS} demonstrated that transformers could emulate logical reasoning over natural language sentences, positing a novel role for LLMs as ``soft theorem provers.'' More recently, \cite{madusanka-etal-2024-natural} investigated satisfiability in natural language, examining how computational complexity levels impact the ability of transformer-based models to learn inference rules.


\paragraph{Scaling and Methodological Approaches}
The scaling behavior of LLMs in logical reasoning tasks has been a focal point of recent studies. \cite{Ryu2024DivideAT} proposed CLOVER, a neurosymbolic approach that combines LLMs with symbolic solvers to enhance the translation of complex logical semantics from natural language, demonstrating superior performance on several benchmarks. In parallel, \cite{Pan2024CanTR} explored the application of a decoder-only Transformer to solve SAT problems, demonstrating its alignment with the Davis–Putnam–Logemann–Loveland (DPLL) algorithm and highlighting the potential of LLMs in complex reasoning tasks. Additionally, \cite{Schlegel2022CanTR} conducted an extensive empirical study to investigate the detection of formally valid inferences in controlled fragments of natural language, revealing that transformers often overfit to superficial patterns rather than acquiring logical principles.

\paragraph{Challenges in Performance and Enhancement Techniques}
Despite notable advancements, LLMs face significant challenges when tasked with complex logical reasoning, especially as the problem complexity increases. \cite{Yan2024DoLL} critically examined the reasoning capabilities of LLMs, positing that their success might stem more from probabilistic correlations than from a genuine understanding of logical rules. Addressing performance variability, \cite{Lam2024ACL} highlighted the impact of the choice of symbolic solvers on the effectiveness of LLMs in deductive reasoning tasks, calling for more consistent comparative studies. Furthermore, \cite{Xie2024OnMO} investigated the memorization versus reasoning abilities of LLMs using dynamically generated logical reasoning benchmarks, providing insights into the complex interplay between these two aspects.

\paragraph{Empirical and Theoretical Insights}
Further empirical and theoretical studies have shed light on the inherent limitations and potential of LLMs in logical reasoning. \cite{Richardson2021PushingTL} proposed a methodology to create challenging reasoning datasets, revealing the robustness and limitations of transformers in handling increased task complexity. \cite{Dziri2023FaithAF} and \cite{Parmar2024LogicBenchTS} provided empirical evidence of the failures of LLMs in simple logical tasks, emphasizing the need for more sophisticated reasoning mechanisms. \cite{madusanka-etal-2023-identifying} investigated the model-checking problem in natural language inference, highlighting the impact of logical semantics on transformer performance.

\end{comment}

\paragraph{Logical Reasoning Benchmarks and Dataset Creation}

Logical reasoning has long been a critical area of AI, but only recently have LLMs been subjected to rigorous testing in this domain. LogiQA \cite{Liu2020LogiQAAC} emerged early on to evaluate complex logical comprehension in question-answering formats; and subsequent efforts by \cite{Liu2023LogiQA2I} reframed it as a Natural Language Inference (NLI) task to further stress-test LLMs’ capabilities. Researchers have also explored generating more dynamic or granular datasets to push the limits of reasoning systems. For instance, \citet{madusanka-etal-2024-natural} investigated satisfiability tasks formulated in natural language, studying how varying computational complexity influences LLM inference performance. Similarly, \citet{Richardson2021PushingTL} introduced a systematic methodology for building challenging reasoning datasets, exposing robustness gaps in transformer-based models when tasked with increased complexity. Prior work on logic grid puzzles include \citet{Mitra2015LearningTA} that proposed a grid-based puzzle dataset prior to the LLM era and focused on automatic translation from language to a formal specification, \citet{Dziri2023FaithAF} that investigated compositionality in LLMs on grid-based puzzles, as well as \citet{Tyagi2024StepbyStepRT} that provided a new error taxonomy to evaluate the correctness of the reasoning chains of LLMs.

\paragraph{Approaches to Logical Reasoning in LLMs}

Several lines of research propose methods to augment or refine LLMs for stronger logical reasoning. \citet{Clark2020TransformersAS} demonstrated that transformers can emulate logical reasoning over natural language sentences—serving as ``soft theorem provers.'' \citet{Pan2024CanTR} showed that a decoder-only Transformer could tackle SAT problems, paralleling the Davis–Putnam–Logemann–Loveland (DPLL) algorithm, thereby expanding the role of LLMs to more complex problem-solving domains. Alternatively, neuro-symbolic systems like CLOVER \cite{Ryu2024DivideAT} integrate LLMs with symbolic solvers to better capture the translation of intricate logical semantics from text.
%Meanwhile, researchers have also examined how well transformers detect valid inferences in controlled fragments of language. For instance, \cite{Schlegel2022CanTR} revealed that although transformers appear adept at certain logical tasks, they often overfit to superficial patterns instead of internalizing underlying logical principles.


%\paragraph{Performance Observations and Limitations}

\paragraph{Empirical Evidence of LLM Limitations}

Despite these promising developments, LLMs face persistent hurdles as logical problem complexity increases. \citet{Yan2024DoLL} contended that models may rely heavily on probabilistic correlations rather than genuinely understanding logical rules. Similarly, \citet{Xie2024OnMO} highlighted the complex interplay  between training data memorization and genuine reasoning abilities of LLMs. Additionally, \citet{Schlegel2022CanTR} conducted an extensive empirical study to investigate the detection of formally valid inferences in controlled fragments of natural language, revealing that transformers often overfit to superficial patterns rather than acquiring logical principles. \citet{Lam2024ACL} showed the impact of the choice of symbolic solvers on the effectiveness of LLMs in deductive reasoning tasks, calling for more consistent comparative studies. Further empirical evidence from \citet{Dziri2023FaithAF} and \citet{Parmar2024LogicBenchTS} demonstrated that even ostensibly simple logical tasks continue to challenge these models. Finally, \citet{madusanka-etal-2023-identifying} investigated the limits of transformers on solving the problem of model-checking with natural language and the significant impact of the language fragment on the performance of transformers.
%the importance of logical semantics in model performance was underscored by \cite{madusanka-etal-2023-identifying}, who examined the model-checking problem in natural language inference scenarios. %Taken together, these studies point to both the potential and the limitations of LLMs for robust logical reasoning, calling for more sophisticated architectures and training strategies.


%\paragraph{Limits of LLMs in reasoning.}
%\yuchen{TODO: mention Fate and Faith paper, the recent Chit Barral's work and huangyangsibo's work on knive and knights }
%\yuchen{TODO: cite some other recent works on analyzing O1 reasoning patterns.}
%\yuchen{https://arxiv.org/pdf/2410.08047}
%\yuchen{https://arxiv.org/abs/2410.07432}
%\yuchen{Yue Zhang's work; LogiQA}
%\yuchen{https://arxiv.org/abs/2406.00284}
%\yuchen{https://arxiv.org/pdf/2402.12091}
%\yuchen{https://arxiv.org/abs/2112.09054}
%\yuchen{https://arxiv.org/abs/2002.05867}

%\url{https://aclanthology.org/2024.acl-long.815/}, %\url{https://aclanthology.org/2023.eacl-main.257/}, %\url{https://arxiv.org/abs/2211.05417}
