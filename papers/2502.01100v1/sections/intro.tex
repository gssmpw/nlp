
% \yuchen{Just switched to the ICML template and a few styling things are not working. I will fix them soon!}

% \yuchen{This is a placeholder for the introduction. 
% % Sorry, the paper is not yet very organized. I mainly focused on putting the formulation, data creation, and experimental results now. The discussion of the experiments is not 100\% finished but ready to have a loook. I will work on the introduction and related work next.
% } 


% \yuchen{
% The current structure of the paper is as follows:
% \begin{enumerate}
%     \item Section~\ref{sec:background} is to give a problem formulation of logic grid puzzles and talk about the challenges of solving them, thus showing the motivation of using zebra puzzles to study the logical reasoning capabilities of LLMs, and why this is a great way of doing research in this direction.
%     \item Section~\ref{sec:zebralogic} introduces the dataset creation proces, evaluation setups, and the overall results of the experiments. We mainly show that there is a huge gap between the O1's performance and other LLMs, and show that the performance drop of LLMs for larger search spaces shows that they are not good at solving logical reasoning puzzles.
%     \item Section~\ref{sec:discussion} is to analyze the reasons why O1 is better than other LLMs. There're two main focuses in this section: 1) the scaling of test-time compute and 2) deciphering the reasoning process of O1 from visible to hidden tokens. We point out a few findings and limitation of using O1 to reason. 
%     \item Section~\ref{sec:methods} is to explore the potential methods that can be used to improve the reasoning capabilities of LLMs. We mainly focus on two methods: 1) sampling strategies and 2) in-context reflection prompting. We show that these methods can help improve the performance of LLMs on ZebraLogic, but there is still a long way to go to make them perform well on logical reasoning puzzles. The potential of normal LLMs to solve logical reasoning puzzles is there but an automatic and reliable way to do so is still missing.
%     \item Related work in Sec.~\ref{sec:related}
%     \item Conclusion and Future Directions in Sec.~\ref{sec:conclusion}.
%     \item Appendix for more details and case studies.
% \end{enumerate}
% } 

% \yuchen{
% The key contribution and findings in the paper are as follows:
% \begin{enumerate}
%     \item ZebraLogic: a dataset of logic grid puzzles with varying complexity, designed to evaluate the reasoning capabilities of large language models (LLMs).
%     \item A detailed analysis of LLMs' performance on ZebraLogic, showing that O1 models outperform other LLMs in terms of solution correctness and efficiency.
%     \item We provide insights into how O1 might be reasoning differently from other LLMs, based on two aspects: the scaling of test-time compute and the reasoning process from visible to hidden tokens.
%     \item We then explore the methods that can be used to improve the reasoning capabilities of LLMs, including sampling strategies and in-context reflection prompting. 
%     \item We also discuss the limitations of our study and potential future directions for research.
% \end{enumerate}
% }


% \yuchen{A few highlights from the emprical findings and case studies:
% \begin{enumerate}
%     \item logical reasoning puzzles are extremely challenging for LLMs, especially for puzzles with more complexity (e.g., $>10^3$ search space).
%     \item O1 are much better than other LLMs in solving logical reasoning puzzles, and even o1-mini is much better than the Sonnet-3.5 and GPT-4o.
%     \item A key reason why O1 is better than other LLMs is that it generates much more tokens in hidden thoughts during inference time, which scales well with the search space size. The number of the hidden reasoning tokens scale very well as the search space size increases. 
%     \item Finding: we find that the visible reasoning steps from O1 cannot explain their decision-making process well, especially for complex problems where assumptions and backtracking are needed. The visible reasoning steps are often incomplete and sometimes incorrect, leading to unreliable explanations of their reasoning process. It's also the reason why they have mistakes in some cases.
%     \item Finding: we find that the summary of O1's hidden reasoning steps contain steps such as revisiting clues, reevaluating assumptions, and mapping out connections, which are hardly noticeable in other LLMs such as GPT-4o. This in-context reflection behavior may be a key factor for O1's success in solving complex problems.
%     \item Finding: doing knowledge distillation (e.g., training with O1's reasoning stesp) is not necessarily helpful for improving the performance of other LLMs, as the reasoning steps are often incomplete and sometimes misleading.
%     \item We use best-of-sampling to investigate the potential coverage of regular LLMs like GPT-4o and GPT-4o-mini. We find that the poetnial improvement is very large if the oracle knowledge about the solution is given (for selecting the best candidate).
%     \item Majority voting and using reward model to select candidates is promising but the improvement is very marginal.
%     \item We also explore simple multi-turn in-context reflection prompting to help LLMs improve their reasoning capabilities. However, the improvement is very marginal too.
%     \item We can argue that LLMs (even O1) reason by guessing and making assumptions, rather than rigorous logical reasoning when the search space is large. O1 has some in-context reflection behavior so it can be luckier by revising the assumptions and backtracking, however, it is still weak when the search space is extremely large.
% \end{enumerate}
% }



 
\vspace{-0.2cm}
\section{Introduction}
\vspace{-0.2cm}
\label{sec:intro} 


% \yuchen{11/28: Working on addressing the comments now. Will add more discussion with related work in the Introduction. Sorry for the delay.} 

% Logical reasoning is a fundamental aspect of human intelligence and a key challenge in artificial intelligence research. While recent advances have shown promise in tasks requiring common sense and general knowledge for reasoning, 
% the extent to which LLMs can handle complex logical reasoning tasks remains an open question.
% This gap in our understanding is particularly significant as logical reasoning represents a 
% fundamental aspect of human intelligence and is crucial for many real-world applications.
% To study this systematically, we need a framework that allows us to:
% \begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0pt,topsep=0pt,partopsep=0pt]
%     \item isolate logical reasoning from other cognitive abilities, domain knowledge or math computation;
%     \item precisely control the complexity of reasoning required; and
%     \item objectively evaluate the correctness of the reasoning of LLMs;
% \end{itemize}

Logical reasoning stands as a cornerstone of human intelligence and remains a central challenge in AI. While recent advances have demonstrated promise in tasks requiring common sense and general knowledge~\citep{brown2020language,chowdhery2022palm,bubeck2023sparks}, the capabilities of Large Language Models (LLMs) in handling complex deductive problems remain uncertain. 
This limitation in our understanding is especially critical as systematic reasoning underpins many real-world applications. 
% To systematically study LLMs' reasoning capabilities and their scaling limits,
% we require an evaluation framework with three essential characteristics:
% it must separate pure logical reasoning from domain knowledge;
% enable precise control over problem complexity; 
% have minimal data leakage to ensure that models are not memorizing specific patterns during pre- and post-training;
% and provide objective metrics for assessing an LLM's reasoning process.
To systematically study LLMs' logical reasoning capabilities and their scaling limits,
an ideal evaluation framework must:
(1) isolate pure logical reasoning from domain knowledge;
(2) enable precise control over problem complexity;
(3) minimize data leakage to prevent training data memorization; %models from memorizing solutions during training;
(4) provide objective metrics for assessing an LLM's reasoning results.

% \yejin{we might lose the audience by saying this, as people usually associate math/coding with logic, unless we can state this more precisely and in a more justifiable manner; perhaps what's more critical to emphasize is the construction of problems at arbitrary complexity that is unlikely to be in the public internet data so that the models might have memorized the key patterns to correct solutions...} 

% To systematically study this capability, we introduce ZebraLogic, a benchmark dataset of logic grid puzzles that allows us to evaluate LLMs' reasoning abilities across varying levels of complexity. Logic grid puzzles are ideal for this purpose as they require pure deductive reasoning while remaining accessible enough to serve as an effective testbed. Importantly, these puzzles can be precisely controlled in terms of their complexity through the size of their search space, enabling us to study how LLMs' performance scales with problem difficulty.

%\ashish{could reword to better connect to the previous paragraph: Constraint satisfaction problems (CSPs) provide an ideal controlled framework meeting all of these criteria.}
%In this work, we focus on using constraint satisfaction problems (CSPs) as a controlled framework for studying LLMs' logical reasoning capabilities. By framing logical reasoning tasks as CSPs, we can systematically assess how well LLMs follow logical constraints and deduce valid value assignments, independent of domain knowledge or mathematical computation.
%CSPs provide an ideal framework as they are mathematically well-defined, can be scaled in complexity and search space size, and have verifiable solutions that we can automatically evaluate. 
%We focus on logic grid puzzles, specifically Zebra Puzzles which is also known as Einstein's Riddle, as they are a well-known class of CSPs that require pure formal reasoning while remaining accessible enough to serve as an effective testbed for studying LLM capabilities and their scaling behavior.
%These logical puzzles require reasoning skills shared with many real-world  problems including task planning, scheduling, and resource allocation. 

%Constraint satisfaction problems (CSPs) provide an ideal controlled framework meeting all of these criteria~\citep{dechter2003constraint}, enabling a systematic study of LLMs' logical reasoning capabilities. By framing logical reasoning tasks as CSPs, we can rigorously evaluate how well LLMs adhere to logical constraints and deduce valid value assignments, independent of domain knowledge or mathematical computation. CSPs are particularly suitable because they are mathematically well-defined, scalable in both complexity and search space size, and have solutions that can be objectively and automatically verified.
%In this work, we focus on logic grid puzzles, specifically Zebra Puzzles (also known as Einstein's Riddle), as a representative class of CSPs that require pure formal reasoning while remaining accessible enough to serve as an effective testbed for analyzing LLM performance and scaling behavior. Furthermore, the reasoning skills they demand—such as deducing relationships and satisfying constraints—are directly relevant to a wide range of real-world applications, including task planning, scheduling, and resource allocation.

Constraint satisfaction problems (CSPs) offer such a controlled framework~\citep{dechter2003constraint}: they are mathematically well-defined, scalable in both complexity and search space, and have solutions that can be automatically verified. By formulating logical tasks as CSPs, we can rigorously evaluate how well LLMs adhere to logical constraints, independent of domain-specific data or heavy numerical computation. As a representative class of CSPs, logic grid puzzles (specifically Zebra Puzzles or Einstein’s Riddle,~\cite{Prosser1993HYBRIDAF}) are particularly suitable as they require pure formal reasoning, remain accessible enough to serve as an effective testbed, and embody core skills relevant to real-world applications such as task planning, scheduling, and resource allocation. Hence, we introduce \textbf{ZebraLogic}, an evaluation framework for creating logic puzzles with controllable, and quantifiable complexity, thus improving our understanding on the scaling limits of LLMs including Llama~\citep{llama3modelcard}, o1~\citep{o1-short} and R1~\citep{deepseek-r1-short}.
\footnote{The closest related effort is by \citet{Tyagi2024StepbyStepRT}, who focused on a detailed analysis of the kinds of errors LLMs make when solving grid puzzles using reasoning chains; more details in \S\ref{sec:related}.}
% \ronan{TODO: Add summary of discussion w/ Ashish about how to position this paper w.r.t. Chitta's EMNLP'24 paper!}

\begin{figure*}[t]
    \centering 
    \vspace{-0.1em}
    \includegraphics[width=1\linewidth]{assets/z3_scale.pdf}
    \vspace{-0.6cm}
    \caption{
     Accuracy vs number of Z3 conflicts for Llama-3 (left), showing the size scaling effect on the reasoning performance. The middle figure shows the curves for gpt-4o(-mini) vs o1 and R1, showing the scaling effect of model size and test-time compute. The right figure shows the scaling effect of repeated sampling by pass@k metric with different sample sizes.
    %  \yuchen{add the notion of curse of complexity}
    % \yuchen{crop again for the top margin}
    } 
    % \vspace{-2mm}
    \label{fig:z3_scale}
\end{figure*}
 
% CSPs represent a fundamental framework in artificial intelligence, with their power lying in the ability to explicitly represent logical relationships and dependencies. This makes them particularly suitable for evaluating an LLM's capacity for systematic reasoning and constraint satisfaction across a wide range of problems and domains. 
%
% Given these advantages, we introduce ZebraLogic, a dataset using logic grid puzzles to evaluate LLMs' logical reasoning capabilities, which consists of 1,000 puzzles with varying complexity levels.
% We evaluate multiple LLMs with varying model sizes and architectures on ZebraLogic to understand how their performance scales with problem complexity.  
% We find that there is a phenomenon of ``\textbf{curse of complexity}'' in reasoning with LLMs, where the performance of LLMs drops significantly as the search space size increases, with most models struggling to solve problems where the search space exceeds $10^7$ and when the number of Z3 conflicts exceeds 20, which are two complexity measures we explained in Sec.~\ref{ssec:complexity_metrics}.
% This suggests that current LLMs face significant challenges in scaling to more complex problems, even with the largest models available today.
%
%
% To systematically investigate these challenges, we introduce ZebraLogic, a dataset of 1,000 logic grid puzzles spanning multiple complexity levels. Through extensive evaluation of various LLMs across different model sizes and reasoning strategies, we uncover a striking phenomenon we term the ``\textbf{curse of complexity}'': LLM performance deteriorates sharply as the search space expands, with most models—even state-of-the-art ones—failing to solve puzzles when the search space exceeds $10^7$ possibilities or when the number of logical conflicts (measured by Z3 solver steps) surpasses 20, as detailed in Sec.~\ref{ssec:complexity_metrics}. This performance cliff persists even with the largest available models, suggesting a fundamental limitation in current LLM approaches to logical reasoning rather than merely a scaling issue.
%
% To systematically investigate these challenges, we introduce ZebraLogic, a dataset of 1,000 logic grid puzzles spanning multiple complexity levels. Through extensive evaluation of various LLMs across different model sizes and architectures, we uncover a phenomenon we term the ``\textbf{curse of complexity for reasoning}'': LLM performance decreases significantly as the search space expands, with most models—even state-of-the-art ones—failing to solve puzzles when the search space exceeds $10^7$ possibilities or when the number of logical conflicts (measured by Z3 solver steps) surpasses 20. 
% This performance limitation persists even with the largest available models, suggesting a fundamental constraint in current LLM approaches to logical reasoning rather than merely a scaling issue.
%
Through extensive evaluation of various LLMs across diverse architectures and sizes, we observe a dramatic decline in performance as puzzle complexity increases---a phenomenon we term the ``\textbf{curse of complexity for reasoning}.'' Most models struggle once the puzzle's search space exceeds $10^7$ possibilities (e.g., for puzzles with 4x5 grid size) or when the number of logical conflicts in a widely used SMT solver named Z3 \citep{demoura2008z3} surpasses 20. %, measured by backtracking steps in the Z3 theorem prover~\citep{demoura2008z3} (a widely-used SMT solver for reasoning tasks), surpasses 20. 
%While this performance limitation aligns with trends seen in prior research on reasoning tasks, our findings go further---they suggest a broader constraint in current LLM reasoning capabilities, not solely a scaling issue of model size or sample size  (even with oracle selections). \ashish{this last sentence needs support / more detail on why we claim it's not only a scaling issue. Currently it comes unexpectedly and ends abruptly. I \emph{think} you are about to address it in the paragraph that follows, but currently there is no link. One possibility: Move "While this performance limitation..." to the next paragraph and replace "given these concerning ..." with a connecting phrase like "Specifically, we conduct a systematic investigation..."}
%\ronan{great point. Working on making this connection clearer, and linking w/ Jason Wei's / Nathan's work, and potentially the emergence of reasoning models - o1, R1, Gemini-Thinking}
These findings suggest that limited reasoning in current LLMs are not solely a matter of model- or sample-size scaling, but also arise from insufficient test-time compute. This shortfall underscores the need to train LLMs to reason step by step \cite{Wei2022ChainOT} explicitly (e.g., via reinforcement learning \cite{Lambert2024TLU3P}), as exemplified by emerging reasoning models such as o1 and R1. %, or Gemini-Thinking.
% \yejin{in a way similar messages have been conveyed in the previous literature, so this sentence can come across as a bit dismissive of such prior work. i think it's important to soften the words a bit here so as not to lose respect from the audience...; yuchen: rephrased to make it less bold} 
% \yejin{should define/introduce what Z3 is; yuchen: done}
% \yejin{readers might disagree on our statement on the 'data' size here --- since relevant data was missing during training, and AlphaGo was able to beat humans despite the expansive search space...; removed this and will talk about it and the alphaGo example in the related work or the limitation section} 
% Therefore, we study the scaling behavior of LLMs in logical reasoning tasks, focusing on two key dimensions: model size and test-time compute.
% Understanding the scaling behavior of LLMs in logical reasoning tasks is crucial for several reasons. First, it helps us identify fundamental limitations in current approaches - whether performance bottlenecks stem from model size, training data, or inference methods. Second, by systematically studying how different scaling dimensions (model size, sample size, reasoning tokens) affect performance, we can better direct future research efforts. For instance, if larger models alone don't solve reasoning challenges, we might need to focus on developing better inference techniques or architectural innovations. 
% Finally, this analysis provides insights into whether LLMs are truly performing logical reasoning or merely pattern matching, which has important implications for their deployment in high-stakes applications requiring reliable deductive reasoning.
% Given these concerning limitations in LLM performance, we conduct a systematic investigation into the scaling behavior of LLMs in logical reasoning tasks, focusing on two key dimensions: model size and test-time compute.
% Understanding scaling behavior of LLMs in reasoning is crucial for several reasons. First, it helps us identify fundamental limitations in current approaches - whether performance bottlenecks stem from model size, training data, or inference methods. Second, by systematically studying how different scaling dimensions (model size, sample size, reasoning tokens) affect performance, we can better direct future research efforts for improving LLMs' reasoning capabilities. 
Specifically, we conduct a systematic investigation into the scaling behavior of LLMs in logical reasoning, focusing on three key dimensions: model size (\S\ref{sec:size_scale}), sampling (\S\ref{sec:sampling_scale}), and test-time compute (\S\ref{sec:cot_scale}).
Understanding scaling behavior of LLMs in reasoning is critical to identify the most promising directions for advancing LLMs' reasoning capabilities and to guide future research efforts more effectively.
% Analyzing these dimensions helps us identify fundamental limitations in current approaches - whether performance bottlenecks stem from model size, training data, or inference methods. 
%Moreover, by systematically analyzing how performance scales across different dimensions---including model size (Sec.~\ref{sec:size_scale}), sampling (Sec.~\ref{sec:sampling_scale}), and reasoning tokens (Sec.~\ref{sec:cot_scale})---we can identify the most promising directions for advancing LLMs' reasoning capabilities and guide future research efforts more effectively.



% Finally, this analysis provides insights into whether LLMs are truly performing logical reasoning or merely pattern matching, which has important implications for their deployment in high-stakes applications requiring reliable deductive reasoning.
% For instance, if larger models alone don't solve reasoning challenges, we might need to focus on developing better inference techniques or architectural innovations. 
% Finally, this analysis provides insights into whether LLMs are truly performing logical reasoning or merely pattern matching, which has important implications for their deployment in high-stakes applications requiring reliable reasoning~\citep{dziri2023faith,xie2024memorization, mirzadeh2024gsmsymbolic}.

Our work makes the following key contributions:
\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0pt,topsep=0pt,partopsep=0pt]
    \item We create the ZebraLogic dataset, a benchmark of 1,000 logic grid puzzles spanning multiple complexity levels, designed to evaluate LLMs' logical reasoning capabilities systematically with two complexity metrics: search space size and Z3 conflict count (\S\ref{sec:zebralogic}).
    
    \item We report ``the curse of complexity'' in logical reasoning with LLMs: the performance dramatically declines as the problem complexity increases and after a certain threshold, most models struggle to solve any logical puzzle.  
    This limitation persists even when scaling to significantly larger models (such as Llama-3.1-405B) or using enhanced training data, indicating a deeper challenge that cannot be resolved by model scaling alone (\S\ref{sec:evaluation} and \S\ref{sec:size_scale}).
    %  \yejin{somehow this sentence reads a bit much to me... how about just 'We find and report ``the curse of comoplexity'' in LLM reasoning...'}
    
    \item We scale the test-time compute of LLMs by increasing the number of generation samples, revealing that it has both promise and challenges. While Best-of-N sampling can improve potential performance, practical selection methods like majority voting or reward models show limited improvement. Additionally, even pass@128 cannot break the curse of complexity (\S\ref{sec:sampling_scale}).
    
    \item We find that it's much more promising to scale up the reasoning tokens (i.e., chain-of-thoughts; CoTs) generated during inference with a backtracking mechanism. 
    We take OpenAI's o1 models as a typical example and show that they generate significantly more, nearly 10x (hidden) reasoning tokens than others, which scale properly  with problem complexity.
    % Approximately 10x times more reasoning tokens than regular LLMs.
    Based on our empirical results, we also find that there exists an optimal ratio of reasoning tokens to Z3 conflicts, but O1-like models cannot always reach this optimal ratio when the complexity is extremely high, thus not achieving perfect reasoning (\S\ref{sec:cot_scale}).
    % \yejin{this is not entirely true unless the model was trained to do test-time compute...? (or at least not verified with vanilla LLMs that are not RL'ed to perform complex test-time reasoning?} 
    
    \item Moreover, we explore the potential of using self-verification prompting to improve LLMs (\S\ref{ssec:self_refinement}). We find that such methods can help LLMs improve their performance, but the improvement is very marginal. We further analyze the reasoning process of o1 and discuss its strengths and weakness in logical reasoning (\S\ref{sec:o1_reason}).
 
\end{itemize}


% a short paragraph to summarize ?

% below are the commented parts;

% Through the first systematic quantitative analysis of this phenomenon, we precisely characterize how performance collapses as problems grow more complex.
 
% Our study focuses on two critical dimensions of scaling in logical reasoning: model size and test-time compute. 
% For test-time compute, we specifically examine two aspects: 
% (1) the sample size for Best-of-N sampling, (2) the number of reasoning tokens generated during inference. 
% Figure~\ref{fig:scaling} shows a high-level overview of our study, where we evaluate the performance of LLMs on ZebraLogic across different complexity levels and analyze how their performance scales with model size and test-time compute.
% Through extensive experimentation and analysis, we make the following key contributions:

% \begin{enumerate}
%     \item We propose a benchmark dataset, ZebraLogic, consisting of 1,000 logic grid puzzles across multiple complexity levels that allow us to evaluate LLMs' logical reasoning capabilities systematically (Section~\ref{sec:zebralogic}).
    
%     \item We identify a ``curse of complexity'' where LLM performance drops precipitously as search space size increases, with most models struggling to solve puzzles beyond the Small category. Notably, even scaling to larger model sizes (e.g., Llama-3.1-405B) does not overcome this limitation (Section~\ref{sec:evaluation}). 
    
%     \item We observe a significant performance gap between OpenAI's O1 models and other LLMs, with O1-preview achieving 71.4\% overall accuracy compared to 36.2\% for the next best model (Section~\ref{sec:evaluation}). This gap widens dramatically as puzzle complexity increases.
    
%     \item Our analysis reveals that O1's superior performance can be attributed to its generation of significantly more hidden reasoning tokens (5,000 tokens) compared to other LLMs (500 tokens), and these tokens scale with puzzle complexity (Section~\ref{sec:cot_scale}).
    
    
%     \item Through Best-of-N sampling experiments, we demonstrate that regular LLMs like GPT-4o can potentially achieve performance close to O1 when using oracle selection among multiple candidates, though practical selection methods like majority voting and reward models show limited improvement (Section~\ref{sec:sampling_scale}).
    
%     \item We find that O1's visible reasoning steps often cannot fully explain its decision-making process, particularly for complex problems requiring assumptions and backtracking. This suggests that the model's reasoning process may rely more on hidden computations than on explicit logical deduction (Section~\ref{sec:cot_scale}).
% \end{enumerate}

\begin{figure*}[t]
    % \centering
    %\includegraphics[width=0.9\linewidth]{figures/KK_data_generation.pdf}
    % \includegraphics[width=0.8\linewidth]{assets/lgp_example_only.pdf}
    \includegraphics[width=1\linewidth]{assets/lgp_example_wide.pdf}
    \vspace{-3mm}
    \caption{
     This example of ZebraLogic features 3 houses (N=3) and 3 attributes (M=3), with 6 clues (K=6). The \textit{Background} outlines the attributes, their possible values, and the uniqueness constraints. The \textit{Clues} provide additional constraints regarding the \textit{attributes}. The task for the model is to determine the correct assignment of attributes to each house based on these clues, as illustrated in the \textit{Solution} grid.
    }
    \vspace{-2mm}
    \label{fig:lgp_example}
\end{figure*}

% Our findings suggest that while current LLMs show promise in logical reasoning tasks, they face significant challenges in scaling to more complex problems. The success of O1 models points to the importance of test-time compute and hidden reasoning processes, though the exact mechanisms remain to be fully understood. These insights have important implications for the development of future models and highlight the need for new approaches to enhance LLMs' logical reasoning capabilities.
