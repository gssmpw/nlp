\section{Several interesting notes}

% \subsection{Solving the domain randomized LQR problem}

% Unfortunately, I think this subsection is a rabbit whole that can be chalked up to a waste of time. (Not quite, this is a different formulation of domain randomization that could still be useful, where $\theta$ at any time is sampled randomly.) But useless for what we want to prove. 

% \begin{lemma}[Stochastic Dynamic Programming for Domain Randomized LQR]
%     Consider the finite-horizon linear quadratic regulator problem with uncertain system matrices. The state evolves as
%     \[
%     X_{t+1} = A X_t + B U_t + W_t,
%     \]
%     where the system matrices $\theta = \bmat{A & B}$ are sampled from a distribution $\mathcal{D}(\theta)$, and $W_t \sim \mathcal{N}(0, \Sigma_w)$ is Gaussian noise. The finite-horizon quadratic cost is given by
%     \[
%     \mathbb{E}_{\theta \sim \mathcal{D}}\left[\sum_{t=0}^{T-1} \left( X_t^\top Q X_t + U_t^\top R U_t \right) + X_T^\top Q_T X_T \right],
%     \]
%     where $Q \succeq 0$, $R \succ 0$, and $Q_T \succeq 0$ are known cost matrices. \Bruce{I'm not even sure this part is correct. Actually, I really don't think it is. $\theta$ is not drawn iid at every instance, and therefore, is not independent of the observed states. Principle of optimality does not hold. Verify this intuition.}

%     The optimal control law $U_t = -K_t X_t$ that minimizes the expected cost is computed via the following backward recursion:
%     \begin{align*}
%         P_T &= Q_T, \\
%         P_t &= Q + \mathbb{E}_{\theta \sim \mathcal{D}}\left[A^\top P_{t+1} A\right] - \mathbb{E}_{\theta \sim \mathcal{D}}\left[A^\top P_{t+1} B\right]
%         \left(\mathbb{E}_{\theta \sim \mathcal{D}}\left[B^\top P_{t+1} B\right] + R\right)^{-1}
%         \mathbb{E}_{\theta \sim \mathcal{D}}\left[B^\top P_{t+1} A\right],
%     \end{align*}
%     where $P_t$ is the cost-to-go matrix at time $t$ and the control gain is given by
%     \[
%     K_t = \left(\mathbb{E}_{\theta \sim \mathcal{D}}[B^\top P_{t+1} B] + R \right)^{-1} \mathbb{E}_{\theta \sim \mathcal{D}}[B^\top P_{t+1} A].
%     \]
% \end{lemma}

% \begin{proof}
%     We solve the problem using stochastic dynamic programming, assuming that the cost-to-go function is quadratic at each time step, as in the classical LQR problem. Let $J_t(X_t)$ denote the cost-to-go function at time $t$. We define the Bellman recursion for the finite-horizon problem as follows:
%     \[
%     J_T(X_T) = X_T^\top Q_T X_T,
%     \]
%     and for $t = T-1, \dots, 0$, the recursion is given by:
%     \[
%     J_t(X_t) = \min_{U_t} \mathbb{E}_{\theta \sim \mathcal{D}}\left[X_t^\top Q X_t + U_t^\top R U_t + \mathbb{E}[J_{t+1}(X_{t+1}) | X_t, U_t]\right].
%     \]

%     The state evolves as $X_{t+1} = A X_t + B U_t + W_t$, where $A$ and $B$ are drawn from the distribution $\mathcal{D}(\theta)$. Substituting this into the cost-to-go function:
%     \[
%     J_t(X_t) = \min_{U_t} \mathbb{E}_{\theta \sim \mathcal{D}}\left[X_t^\top Q X_t + U_t^\top R U_t + \mathbb{E}\left[J_{t+1}(A X_t + B U_t + W_t)\right]\right].
%     \]

%     Assume that $J_{t+1}(X_{t+1}) = X_{t+1}^\top P_{t+1} X_{t+1}$ is quadratic in $X_{t+1}$. Then,
%     \[
%     \mathbb{E}[J_{t+1}(X_{t+1})] = \mathbb{E}_{\theta \sim \mathcal{D}} \left[ (A X_t + B U_t)^\top P_{t+1} (A X_t + B U_t) + \text{tr}(P_{t+1} \Sigma_w) \right].
%     \]
%     Expanding this expression yields:
%     \begin{align*}
%     \mathbb{E}[J_{t+1}(X_{t+1})] &= X_t^\top \mathbb{E}_{\theta \sim \mathcal{D}}\left[A^\top P_{t+1} A\right] X_t + 2 X_t^\top \mathbb{E}_{\theta \sim \mathcal{D}}\left[A^\top P_{t+1} B\right] U_t \\&+ U_t^\top \mathbb{E}_{\theta \sim \mathcal{D}}\left[B^\top P_{t+1} B\right] U_t + \text{tr}(P_{t+1} \Sigma_w).
%     \end{align*}

%     Substituting this into the Bellman recursion for $J_t(X_t)$, we obtain:
%     \begin{align*}
%     J_t(X_t) &= \min_{U_t} \Bigg( X_t^\top Q X_t + U_t^\top R U_t + X_t^\top \mathbb{E}_{\theta \sim \mathcal{D}}[A^\top P_{t+1} A] X_t \\&+ 2 X_t^\top \mathbb{E}_{\theta \sim \mathcal{D}}[A^\top P_{t+1} B] U_t + U_t^\top \mathbb{E}_{\theta \sim \mathcal{D}}[B^\top P_{t+1} B] U_t + \text{tr}(P_{t+1} \Sigma_w) \Bigg).
%     \end{align*}

%     This is a quadratic expression in $X_t$ and $U_t$. The optimal control law $U_t = -K_t X_t$ can be computed by solving for $K_t$:
%     \[
%     K_t = \left(\mathbb{E}_{\theta \sim \mathcal{D}}[B^\top P_{t+1} B] + R \right)^{-1} \mathbb{E}_{\theta \sim \mathcal{D}}[B^\top P_{t+1} A].
%     \]

%     Substituting the optimal $U_t = -K_t X_t$ back into the Bellman equation gives the Riccati-like recursion:
%     \[
%     P_t = Q + \mathbb{E}_{\theta \sim \mathcal{D}}[A^\top P_{t+1} A] - \mathbb{E}_{\theta \sim \mathcal{D}}[A^\top P_{t+1} B] \left(\mathbb{E}_{\theta \sim \mathcal{D}}[B^\top P_{t+1} B] + R \right)^{-1} \mathbb{E}_{\theta \sim \mathcal{D}}[B^\top P_{t+1} A].
%     \]
% \end{proof}

% \begin{lemma}[Infinite Horizon Stochastic LQR]
%     Consider the infinite-horizon linear quadratic regulator problem with uncertain system matrices. The state evolves as
%     \[
%     X_{t+1} = A X_t + B U_t + W_t,
%     \]
%     where the system matrices $\theta = \bmat{A & B}$ are sampled from a distribution $\mathcal{D}(\theta)$, and $W_t \sim \mathcal{N}(0, \Sigma_w)$ is Gaussian noise. The infinite-horizon quadratic cost is defined as the limiting average cost:
%     \[
%     \limsup_{T \to \infty} \frac{1}{T} \mathbb{E}_{\theta \sim \mathcal{D}} \left[ \sum_{t=0}^{T-1} \left( X_t^\top Q X_t + U_t^\top R U_t \right) \right],
%     \]
%     where $Q \succeq 0$ and $R \succ 0$ are known cost matrices.

%     The optimal time-invariant control law $U_t = -K_\infty X_t$ is computed by solving the following \textbf{stochastic Riccati equation}:
%     \[
%     P_\infty = Q + \mathbb{E}_{\theta \sim \mathcal{D}}[A^\top P_\infty A] - \mathbb{E}_{\theta \sim \mathcal{D}}[A^\top P_\infty B] \left( \mathbb{E}_{\theta \sim \mathcal{D}}[B^\top P_\infty B] + R \right)^{-1} \mathbb{E}_{\theta \sim \mathcal{D}}[B^\top P_\infty A],
%     \]
%     where $P_\infty$ is the steady-state solution to the stochastic Riccati equation. The optimal feedback gain is given by
%     \[
%     K_\infty = \left(\mathbb{E}_{\theta \sim \mathcal{D}}[B^\top P_\infty B] + R \right)^{-1} \mathbb{E}_{\theta \sim \mathcal{D}}[B^\top P_\infty A].
%     \]
% \end{lemma}

% \begin{proof}
%     \Bruce{This proof is wrong. Conjecture: it requires an assumption that the support of $\calD$ is jointly stabilizable. }
%     The proof proceeds by solving the Bellman equation for the infinite-horizon problem. Instead of directly summing the cost over an infinite horizon, we focus on the limiting average cost:
%     \[
%     \limsup_{T \to \infty} \frac{1}{T} \mathbb{E}\left[ \sum_{t=0}^{T-1} \left( X_t^\top Q X_t + U_t^\top R U_t \right) \right].
%     \]
%     As in the classical LQR case, we assume that the cost-to-go function is quadratic in $X_t$ for large $t$, i.e., $J(X_t) = X_t^\top P_\infty X_t$, which leads to a time-invariant solution. However, since $A$ and $B$ are random variables drawn from $\mathcal{D}(\theta)$, the Riccati recursion involves expectations over these parameters. Taking the limit of the Riccati recursion from the finite-horizon case as $t \to \infty$ yields the steady-state solution $P_\infty$. The control law $U_t = -K_\infty X_t$ follows by minimizing the quadratic cost at each time step.
% \end{proof}

% Interestingly, the above lemma may provide us with a deterministic algorithm to compute the domain randomized controller. In particular, as long as the iterations converge, the stochastic Riccati equation can be solved via recursion by computing the first and second moments of the parameter distribution. There is some subtlety involved though, which I have not entirely wrapped my head around. In particular, observe the following points:
% \begin{itemize}
%     \item The domain randomized controller only depends on the first and second moments of the parameter distribution. 
%     \item One can construct two parameter distribution with the same first and second moments. However, one may construct them such that the systems in the support of one parameter distribution are jointly stabilizable, however the systems in the support of the other are not. 
%     \item If the systems in the support of the parameter distribution are not jointly stabilizable, then the expected LQR cost is infintie. 
%     \item This means that even if the stochastic Riccati equation converges, it does not necessarily hold that the resultant controller stabilizes the support of $\calD$. In particular, it is not necessarily true that
%     \begin{align*}
%         \trace(P_{\infty} \Sigma_w) = \mathbb{E}_{\theta\sim \calD}\brac{C(K, \theta)}. 
%     \end{align*}
% \end{itemize}
%  I have not formalized an explanation for the above discussion; however, I conjecture that the subtlety partially arises in moving to the infinite horizon setting as objectives become unbounded. In particular, the issue likely arises due to the interchange in the order of expectation, which is permitted only under boundedness of the integrands. This boundedness should come, however, if there is an assumption that the support of $\calD$ is jointly stabilizable. This should be formalized, as it would give us a very effective algorithm in the setting where the support of the distribution is stabilizable. 

 \subsection{Misspecification}

 This paper has focused on a situation where domain randomization is used to mitigate the uncertainty in the model due to variance in fitting the model. In particular, assuming the dynamics \eqref{eq: linear system} imposes a realizability assumption. Thereby, an appropriatley constructed distribution $\calD$ contains $\theta^\star$ in the support with high probability. 

 However, one of the explanations posited for the empirical success of domain randomization in many robotics examples \citep{?} is that domain randomization fares well in the presence of misspecification. In particular, this consists of settings where the true system is not represented by the set of dynamics described by the distribution of parameters $\calD$. In this paper, we do not delve into this topic in depth. However, we do believe it is a valuable direction for future investigation. 

  Consider a scalar linear dynamical system:
     \begin{align*}
        X_{t+1} = a^\star X_t + b^\star U_t + W_t \quad \forall t\geq 0,
     \end{align*}
    where $a^\star = 1.05$ and $b^\star = 1$.  
    
    Suppose that the actuation matrix was assumed to be known; however, this assumption was incorrect. In particular, suppose that the engineer's esimate for the actuation is $\hat b = 1.0$. Suppose that $a^\star$ was assumed to be unknown, and through a system identification procedure, the engineer determined an estimate $\hat a = 1.1$. 
    
    If the engineer attempts to minimize an LQR objective with $Q=1$ and $R=1000$, they will synthesize a controller
    \[
        \hat k = K(\bmat{\hat a & \hat b}) = -.195.  
    \]
    This controller does not stabilize the true system, as the closed-loop state matrix is $a^\star + \hat k b^\star = 1.0024$. 
    
    However, if the engineer applies domain randomization over the $a$ parameter with the distribution $\calD = U[0.9, 1.3]$, and synthesizes a controller as 
    \[
        \hat k = \argmin_{k} \mathbb{E}_{a\sim U[1.0, 1.2]} \brac{C(k, \bmat{a & \hat b})} = -0.206,
    \]
    the closed-loop state matrix is $a^\star + b^\star \hat k = 0.997$, which is stable.  
    
 
 % To provide a starting place for such investigations, we provide a simple proof of concept example where domain randomization yields a benefit over certainty equivalence in a situation consisting of misspecification. 


