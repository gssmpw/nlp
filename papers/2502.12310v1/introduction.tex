

\section{Introduction}

\begin{wrapfigure}[12]{r}{0.55\textwidth}
    \centering
    \vspace{-10pt} % Adjust spacing if needed
    \includegraphics[width=\linewidth]{figures/domain_randomization.pdf}
    \vspace{-24pt} % Adjust spacing if needed
    \caption{Illustration of the sample efficinecy of various synthesis methods. }%\Bruce{Fix up RC curve to show it still decays}}
    \label{fig:conceptual figure}
\end{wrapfigure}


The use of learned world models to synthesize controllers via policy optimization is becoming increasingly prevalent in reinforcement learning \citep{wu2023daydreamer, matsuo2022deep}. The performance of the resulting controller depends heavily upon the synthesis procedure. Simple approaches that do not account for uncertainty, known as certainty equivalence, can overfit to errors in the learned model. Robust control approaches can tolerate some error in the learned model, but may be overly conservative and computationally demanding. %A simple synthesis procedure that solves a policy optimization problem by treating the learned model as though it were ground truth, known as certainty equivalence, can perform poorly if there is a major discrepancy between the learned world model and the ground truth. Synthesis approaches based on robust control or robust adversarial reinforcement learning \citep{pinto2017robust} can overcome small discrepancies between the learned model and ground truth, but may be overly conservative, and difficult to optimize.  
Consequently, \emph{domain randomization} has emerged as a dominant paradigm in robotics for enabling transfer of policies optimized in simulation on a learned or physics-based simulator to the real world by randomizing system parameters during policy optimization \citep{tobin2017dr, akkaya2019solving}. Despite the empirical success of domain randomization, it remains poorly understood how to select the randomization distribution, and how much of a discrepancy between the learned model and the real world can be tolerated. 

We seek to address these issues by restricting attention to the benchmark problem of learning the linear quadratic regulator (LQR). This problem consists of collecting experimental interaction data from a linear dynamical system, and using this data to synthesize a controller that optimizes a control objective. The linear dynamical system is described by
\begin{align}
    \label{eq: linear system}
    X_{t+1} = A(\theta^\star) X_t + B(\theta^\star) U_t + W_t \textrm{ for } t=1,\dots, T, 
\end{align}
where $X_t\in\R^{\dx}$ is the system state, $U_t\in\R^{\du}$ is the control input, $W_t\in\R^{\dx}$ is i.i.d. mean zero Gaussian noise, and $\theta^\star\in\R^{d_{\theta}}$ is an unknown parameter. The goal is to design a controller $K$ to minimize the objective $C(K, \theta^\star)$, %\Tesshu{should this be $\theta^\star or \theta$?}\Bruce{$\theta^\star$, since we want to minimize the objective for the true instance, even though we define the objective for any instance.}
defined as
\begin{align}
    \label{eq: lqr cost}
    C(K, \theta) \triangleq \limsup_{T\to\infty}\E_{\theta}^K 
    \brac{ \frac{1}{T} \sum_{t=1}^T \paren{\norm{X_t}_Q^2 + \norm{U_t}_R^2}},
\end{align}
for $Q$ and $R$ positive definite weight matrices. The subscript on the expectation denotes that the states evolves according to $X_{t+1} = A(\theta) X_t +B(\theta) U_t + W_t$, and the superscript denotes that the inputs are selected according to the linear feedback $U_t = K X_t$. This benchmark problem has been used to study the sample efficiency of certainty equivalence \citep{mania2019certainty} and robust control \citep{dean2020sample} by quantifying how many experiments from the system \eqref{eq: linear system} are sufficient to achieve some level of control performance. In this work, we study the sample efficiency of domain randomization, which chooses a control policy as 
\begin{align}
    \label{eq: domain randomization}
    K_{\mathsf{DR}}(\calD) = \argmin_{K} \E_{\theta\sim \calD} \brac{C(K, \theta)}, 
\end{align}
for a sampling distribution $\calD$ determined using the dataset of experiments collected from \eqref{eq: linear system}. 



%Simulated environments have played an important role for robot learning thanks to its unlimited access to data and acceleration by parallel computing, leading to success in diverse applications \citep{liu2021role, loquercio2021learning, HumanoidTransformer2023}. However, since constructing physics-based simulators can be difficult as it requires tedious procedure to produce reasonable scenes obeying physics law, researchers have started to look at \textit{Generative Simulation}, a paradigm to autonomously generate simulated environments \citep{wang2023robogen, katara2024gen2sim}. 

%As these approaches are getting common, the importance of \textit{Domain Randomization} (DR) \citep{tobin2017dr}, a way to acquire robustness by randomizing over parameters in simulators, is to increase. It is because the lack of understanding on the learned simulator's first principle would make it difficult to manually decide the valid region to make the system robust over. Hence it is of great importance to theoretically understand how DR works, and we analyze the sample complexity of DR by narrowing down to linear system settings in this work. 


% Why study?
% Model-based Reinforcement Learning (RL) has gained larger attention due to the success in the various applications including robotics and autonomous driving \citep{levine2020offline, moerland2023model-basedRL}. These approaches mainly consist of two aspects: (i) learning the model from collected data and (ii) planning and controlling the system based on the learned model. When thinking about deploying these autonomous systems in the real-world, the sample complexity to achieve the desired performance matters since it tells us what kind of algorithms are efficient depending on situations. Furthermore, it is of great importance to ensure the system would not behave poorly even under the uncertain model fitted by the limited dataset. 

% Why not solved?
% LQR and CE
%Prior work analyzing the sample complexity of learning the \textit{Linear Quadratic Regulator} (LQR), one of the most well-studied settings for optimal control, has shown that the principle known as \textit{Certainty Equivalence} (CE), in which one uses the dataset to obtain the least square estimate of the dynamics, and then in turn synthesizes a controller as though these estimates were the ground truth, is efficient \citep{mania2019certainty}.  The notion of efficiency in prior work refers to the fact that the rate of decay of suboptimality gap asymptotically matches the lower bound, i.e. it decays at the optimal rate of the total number of samples. Follow up analysis by \citet{wagenmaker2021task, lee2024active} has shown the optimal asymptotic dependence on system theoretic constants. 

% CE fails to address robustness
%What prior analysis studying CE fails to adequately address is that CE may perform very poorly in the low-data regime. In particular, it may be that the CE controller does not even stabilize the system when limited data is available. Thus the problem of how to acquire robustness under the smaller dataset needs to be explored, but still remains to be open. One classical approach is \textit{Robust Control} (RC), a technique to quantify the uncertainty in the estimates, and uses this uncertainty quantification to design a controller.
%While this approach turn out to be effective in the low-data regime in the sense that the robust controller can stabilize the system well \citep{dean2020sample}, there are several drawbacks: it is challenging to derive computationally feasible extensions of either of these approaches to general classes of nonlinear systems\footnote{Robust synthesis approaches for nonlinear systems tend to use approaches such as $H_{\infty}$ methods extended to nonlinear systems \citep{ball1993h}, or robust Lyapunov-based methods \citep{freeman2008robust}. These do not directly handle the issue of bounded parameter uncertainty, which makes it challenging to prove claims about the performance of identified models exhibiting guarantees of the form given by \Cref{thm: identification bound}.}, and it tends to be overly conservative as appeared in the upper bound. 


% \begin{enumerate}
%     \item It is challenging to derive computationally feasible extensions of either of these approaches to general classes of nonlinear systems. 
%     \item The increasingly adopted approaches of performing approximate dynamic programming to perform optimal control via policy approximation (typically with neural networks) are not easily amenable to these conventional approaches for incorporating robustness. 
%     \item Robust synthesis can be overly conservative. Indeed, while some existing finite sample bounds exist for robust synthesis \citep{dean2020sample}, the rate of decay is linear the estimation error rather than quadratic. This limitation is not fundamental (as we show in this work.) However, suptoptimal dependence on the system theoretic constants may be fundamental (as we conjecture in this work). 
%     \item Analyzing the sample complexity of certainty equivalence with stability constraints is not straightforward in the regime where the constraints are active. 
% \end{enumerate}

% Domain Randomization
%\textit{Domain Randomization} (DR) is an approach which is now widely used in the robotics community to bridge the reality gap. This accounts for uncertainty in the estimates by constructing a parameter distribution and minimizing the expected control cost over system realizations drawn from the distribution. Although it is useful since it is framed for more general objectives and parameter distributions as it may be easily implemented via stochastic gradient descent, existing work in the learning for control communities analyze CE or RC and claim their optimality or effectiveness. Therefore, it is important to theoretically understand the nature of DR in the LQR of an unknown system which we have identified from data.

% contribution
\subsection{Contributions}

Our contribution is to study the sample efficiency of domain randomization and robust control to establish the relationships visualized as a conceptual diagram in \Cref{fig:conceptual figure}. In particular, we achieve the following: 
\begin{itemize}[noitemsep, nolistsep, leftmargin=*]
    \item \textbf{Sample Effiency of Domain Randomization:} We prove that with an appropriately chosen sampling distribution $\calD$, the domain randomization procedure of \eqref{eq: domain randomization} achieves the optimal asymptotic rate of decay with the number of samples, thereby matching the performance of certainty equivalence in the large sample regime. We further conjecture that the burn-in time for DR lies between that of RC and CE.  We leave proving this to future work, but verify this conjecture numerically.
    \item \textbf{Sample Effiency of Robust Control:} We prove the tightest known bound on robust control, improving the asymptotic rate of decay with the number of samples $N$ from $1/\sqrt{N}$ to $1/N$. The upper bounds indicate a gap between the asymptotic rate of decay for robust control, and the rate of decay for domain randomization and certainty equivalence in terms of system-theoretic quantities. We conjecture that this gap is fundamental, due to the conservative nature of robust control. However, we establish that robust control can achieve a smaller burn-in time relative to certainty equivalence, due to its ability to stabilize the system with a coarse estimate. 
    \item \textbf{Algorithm for Domain Randomization:} We propose an algorithm to solve \eqref{eq: domain randomization} which proves effective in numerical experiments. This enables verification of the trends predicted in the aforementioned results, and aligns with the conceptual diagram in \Cref{fig:conceptual figure}. While the focus of this work is restricted to linear systems, the proposed algorithm can in principle extend to general nonlinear systems, whereas similar extensions for robust control face computational challenges.  
\end{itemize}
By providing this characterization, our work demonstrates the potential of domain randomization in learning-enabled control, and partially explains the empirical success that it has achieved in robotics applications. We therefore conclude by highlighting several interesting open questions regarding the use of domain randomization for learning-enabled control. 

%Our primary contribution is to demonstrate that for an appropriately chosen sampling distribution, the synthesis procedure \eqref{eq: domain randomization} is near-optimal. In particular, we provide the following bound.

%We additionally provide a novel bound describing  the statistical efficiency of robust control in order to contrast with our results on domain randomization. Finally, we provide a policy gradient based algorithm, which we conjecture converges to the solution of \eqref{eq: domain randomization}. This algorithm allows us to numerically validate the trends predicted by our theory by contrasting the performance of domain randomization with certainty equivalence and robust control, shown in \Cref{fig:result dr lqr}. \Bruce{Make point here and elsewhere that this algorithm is computationally efficient, scales to high dimensional systems, and is effortless to generalize beyond linear. }




%We show DR achieves the same rate of decay of suboptimality gap as CE, while RC suffers from conservativeness depending on the system theoretic constants. In addition, we show that as long as DR converges, then it robustly stabilizes the set of all the parameters with non-zero support in the distribution. This can allow for improved performance over CE in the low-data regime. In other words, DR provides "approximate robustness" constraints while guaranteeing the same convergence rate as CE. 

%\input{problem formulation}

\input{related_work}

\vspace{-3pt}
\paragraph{Notation: }
%Expectation with respect to all the randomness of the underlying probability space is denoted by $\E$.
%The Euclidean norm of a vector $x$ is denoted $\norm{x}$. For a matrix $A$, the spectral norm is denoted $\norm{A}$, and the Frobenius norm is denoted $\norm{A}_F$. A symmetric, positive semi-definite matrix $A = A^\top$ is denoted $A \succeq 0$.  $A \succeq B$ denotes that $A-B$ is positive semi-definite. Similarly, a symmetric, positive definite matrix $A$ is denoted $A \succ 0$. The minimum eigenvalue of a symmetric, positive semi-definite matrix $A$ is denoted $\lambda_{\min}(A)$. For a positive definite matrix $A$, we define the $A$-norm as $\norm{x}_A^2 = x^\top A x$. 
  %The gradient of a scalar valued function $f: \R^n \to \R$ is denoted $\nabla f$, and the Hessian is denoted $\nabla^2 f$. 
  %The Jacobian of a vector-valued function $g: \R^n \to \R^m$ is denoted $D g$, and follows the convention for any $x\in\R^n$, the rows of $D g(x)$ are the transposed gradients of $g_i(x)$. %The $p^{th}$ order derivative of $g$  is  denoted by $D^{p} g$. Note that for $p \geq 2$, $D^{p} g(x)$ is a tensor for any $x\in\R^{n}$. %The operator norm of such a tensor is denoted by $\norm{D^p g(x)}_{\mathsf{op}}$. 
 %For a function $f: \mathsf{X} \to \R^{\dy}$, we define $\norm{f}_{\infty} \triangleq \sup_{x \in \mathsf{X}} \norm{f(x)}$. 
 %A Euclidean norm ball of radius $r$ centered at $x$ is denoted $\calB(x,r)$. For a matrix $A \in\R^{n\times m},$ 
 \sloppy
The operator $\mathsf{vec}(A)$ stacks the columns of $A$ into a vector, and its inverse $\mathsf{vec}^{-1}(\mathsf{vec}(A), n)$ returns a matrix with $n$ rows by successively stacking chunks of $n$ elements into columns. The Kronecker product of $A$ and $B$ is $A \kron B$. $x \vee y$ denotes the max of $x$ and $y$.
% \\\\~\ 
% Complete proofs and additional details can be found at \url{https://drive.google.com/file/d/1JPvBK-8bnrXVZvgq68VxswJT2eQuEqT3/view?usp=sharing}. 

% \begin{enumerate}
%     \item Why study this?
%     \begin{enumerate}
%         \item model-based reinforcement learning has been gaining attention
%         \item sample complexity is important - tells us what algorithms are efficient for what situations
%         % \item lack of safety/robustness can be problematic
%     \end{enumerate}
%     \item Why is it not solved?
%     \begin{enumerate}
%         \item Learning the LQR
%         \begin{enumerate}
%             \item Introduce CE
%             \item LS + CE is aymptotically optimal
%         \end{enumerate}
%         \item  CE fails to address robustness in the low-data regime, still an open problem 
%         \item Robust Control
%         \begin{enumerate}
%             \item RC uses uncertainty quantification to consider worst case
%             \item challenging to extend to more complex systems, overly conservative as appeared in the upper bound
%         \end{enumerate}
%         \item Domain Randomization
%         \begin{enumerate}
%             \item Introduction \citep{abeel2017dr}
%             \item usefulness for sim2real \citep{mehta20activedr, loquercio2020racing}, find something latest
%             \item Despite how useful it is, existing work in L4DC analyzes CE or Robust control, claiming optimality or effectiveness
%             \item we study DR in LQR of an unknown system which we have identified from data
%         \end{enumerate}
%     \end{enumerate}
%     \item What is the contribution?
%     \begin{itemize}
%         \item DR is also asymptotically optimal 
%         \item DR provides "approximate robustness" constraints, which can stabilize in the lower data regime when CE would not
%     \end{itemize}
% \end{enumerate}
