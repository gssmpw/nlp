\appendix

\section{Notation}
The Euclidean norm of a vector $x$ is denoted $\norm{x}$. For a matrix $A$, the spectral norm is denoted $\norm{A}$, and the Frobenius norm is denoted $\norm{A}_F$. 
A symmetric, positive semi-definite matrix $A = A^\top$ is denoted $A \succeq 0$.  $A \succeq B$ denotes that $A-B$ is positive semi-definite. Similarly, a symmetric, positive definite matrix $A$ is denoted $A \succ 0$. 
The minimum eigenvalue of a symmetric, positive semi-definite matrix $A$ is denoted $\lambda_{\min}(A)$. For a positive definite matrix $A$, we define the $A$-norm as $\norm{x}_A^2 = x^\top A x$. 
The gradient of a scalar valued function $f: \R^n \to \R$ is denoted $\nabla f$, and the Hessian is denoted $\nabla^2 f$. 
The Jacobian of a vector-valued function $g: \R^n \to \R^m$ is denoted $D g$, and follows the convention for any $x\in\R^n$, the rows of $D g(x)$ are the transposed gradients of $g_i(x)$.
The $p^{th}$ order derivative of $g$  is  denoted by $D^{p} g$. Note that for $p \geq 2$, $D^{p} g(x)$ is a tensor for any $x\in\R^{n}$. 
The operator norm of such a tensor is denoted by $\norm{D^p g(x)}_{\mathsf{op}}$. 
For a function $f: \mathsf{X} \to \R^{\dy}$, we define $\norm{f}_{\infty} \triangleq \sup_{x \in \mathsf{X}} \norm{f(x)}$. 
A Euclidean norm ball of radius $r$ centered at $x$ is denoted $\calB(x,r)$. The state covariance matrix of the system under controller K is denoted as $\Sigma^K(\theta) \triangleq \dlyap((A(\theta)+B(\theta)K)^T, I )$. 
The solution to the discrete algebraic Ricatti equation satisfies $P \succeq I$ as long as $Q\succeq I$. 
The Hessian of the objective function is calculated as 
\begin{align*}
 H(\theta) &\triangleq \nabla^2_\theta C(K(\theta), \theta) = \mathsf{D}_{\theta}\VEC K(\theta)^T(\Psi(\theta)\kron\Sigma_X^{K(\theta)}(\theta))\mathsf{D}_{\theta}\VEC K(\theta)
\end{align*}
where $\Psi(\theta) \triangleq B(\theta)^TP(\theta)B(\theta) + R$.


\section{Perturbation Analysis}

% \Tesshu{Check $\Sigma_w$}




Here we present a number of perturbation results that we re-use throughout our analysis.

% Expectation with respect to all the randomness of the underlying probability space is denoted by $\E$.


\begin{lemma}[Performance Difference Lemma, Lemma 12 of \citet{fazel2018global}]
    \label{lem: performance difference}
    Let $\theta$ denote the parameter for a dynamical system, and $K$ be an arbitrary gain that stabilizes this system. Then it holds that
    \begin{align*}
        C(K, \theta) - C(K(\theta), \theta) = \trace\paren{(K-K(\theta) \Sigma^K(\theta) (K-K(\theta))^\top \Psi(\theta)}, 
    \end{align*}
    where we recall that $\Sigma^K(\theta)$ is the state covariance of the system under controller $K$. 
\end{lemma}

\begin{lemma}[Lyapunov Perturbation]
    \label{lem: lyap perturbation}
    Let $A_1, A_2 \in \R^{d \times d}$ satisfy  $\rho(A_1) < 1$ and $\rho(A_2) < 1$. Let $Q$ be a $d$ dimensional positive definite matrix. Define $P_1 = \dlyap(A_1, Q)$, and $P_2 = \dlyap(A_2, Q)$. Then it holds that 
    \begin{align*}
        \norm{P_1 - P_2} \leq \frac{1}{\lambda_{\min}(Q)}\norm{P_1} \norm{P_2}\paren{2\norm{A_2} \norm{A_1-A_2}+\norm{A_1 - A_2}^2}. 
    \end{align*}
\end{lemma}
\begin{proof}
    By definition of $P_1$ and $P_2$, it holds that 
    \begin{align*}
        P_1 - P_2 &= A_1^\top P_1 A_1 - A_2^\top P_2 A_2 \\
        &= (A_2 + (A_1-A_2))^\top P_1 (A_2 + (A_1-A_2)) - A_2^\top P_2 A_2 \\
        &= \dlyap(A_2, A_2^\top P_1 (A_1-A_2) +(A_1-A_2)^\top P_1 A_2 +  (A_1-A_2)^\top P_1(A_1-A_2)) \\
        &\preceq \dlyap(A_2, I) \norm{A_2^\top P_1 (A_1-A_2) +(A_1-A_2)^\top P_1 A_2 +  (A_1-A_2)^\top P_1(A_1-A_2)}.
    \end{align*}
    Note that $\dlyap(A_2, I)\preceq \frac{1}{\lambda_{\min}(Q)} P_2$. The result then follows by the triangle inequality and submultiplicativity.
\end{proof}

\begin{lemma}
    \label{lem: cov lower bound}
    Fix a system $\theta$ and two stabilizing controllers $K_1$ and $K_2$. As long as $\norm{B(\theta)(K_1 - K_2)} \leq \frac{1}{12 \norm{\Sigma^{K_1}(\theta)}^{5/2}}$, it holds that 
    \begin{align*}
        \Sigma^{K_2}(\theta) \succeq \frac{1}{2} \Sigma^{K_1}(\theta).
    \end{align*}
\end{lemma}
\begin{proof}
    The result follows by applying the reverse triangle inequality along with \Cref{lem: lyap perturbation}.
\end{proof}

\begin{lemma}[Riccati Perturbation, Proposition 4 and 6 of \citet{simchowitz2020naive}]
    \label{lem: Riccati perturbation}
    Let $\theta_1$ denote the parameter for a stabilizable system, and $\theta_2$ denote the parameter for an alternate system. Suppose that $\norm{\theta_1-\theta_2} \leq \frac{1}{16}\norm{P(\theta_1)}^{-2}$. Then the system described by $\theta_2$ is stabilizable, and the following inequalities hold.
    \begin{itemize}
        \item $\norm{P(\theta_2)}\leq \sqrt{2} \norm{P(\theta_1)}$
        \item $\max\curly{\norm{K(\theta_2) - K(\theta_1)}, \norm{B(\theta_1) (K(\theta_2) - K(\theta_1))}
        }\leq  32\norm{P(\theta_1)}^{7/2} \norm{\theta_1-\theta_2}.$
        \item $\norm{P(\theta_2) - P(\theta_1)} \leq 8\sqrt{2}\norm{P(\theta_1)}^3\norm{\theta_1-\theta_2}$
    \end{itemize}
\end{lemma}

\begin{lemma}[Simplifying inequalities]
    \label{lem: simplifying inequalities}
    Let $\theta$ be a parameter describing a stabilizable system instance. 
    Define $\tau_{B(\theta)} = \max\curly{1, \norm{B(\theta)}}$.
    The following inequalities hold 
    \begin{itemize}
        \item $\norm{\Sigma^{K(\theta)}(\theta)} \leq \norm{P(\theta)}$
        \item $\norm{A(\theta) + B(\theta) K(\theta)} \leq \norm{P(\theta)}^{1/2}$. 
        \item $\norm{K(\theta)}\leq\norm{P(\theta)}^{1/2}$
        \item $\norm{\Psi(\theta)} \leq 2 \tau_{B(\theta)}^2 \norm{P(\theta)}$.
        \item $\norm{\Psi(\theta)\kron\Sigma_X^{K(\theta)}(\theta)} \leq 2\tau^2_{B(\theta)}\norm{P(\theta)}^2$. 
    \end{itemize}
\end{lemma}
\begin{proof}
    The first inequality follows by observing that 
    \begin{align*}
        \norm{\Sigma^{K(\theta)}(\theta)} &= \norm{\dlyap((A+BK(\theta))^\top, I)} 
        \leq \norm{\dlyap((A+BK(\theta)), Q + K(\theta)^\top R K(\theta))},
    \end{align*}
    by the fact that $Q \succeq I$. The second inequality follows by noting that 
    \begin{align*}
        \norm{(A(\theta) + B(\theta) K(\theta))^\top (A(\theta) + B(\theta) K(\theta))}^{1/2} \leq \norm{\dlyap(A(\theta)+B(\theta)K(\theta), I)}^{1/2}.    
    \end{align*}
    The third inequality follows from
    \begin{align*}
        \norm{K(\theta)} \leq \norm{Q+K(\theta)^TRK(\theta)}^{1/2} \leq \norm{\dlyap(A(\theta)+B(\theta)K(\theta), Q+K(\theta)^TRK(\theta))}^{1/2}.
    \end{align*}
    The fourth and fifth inequality follow from
    \begin{align*}
        \norm{\Psi(\theta)\kron\Sigma_X^{K(\theta)}(\theta)} &\leq \norm{\Psi(\theta)}\norm{\Sigma_X^{K(\theta)}(\theta)} \leq (\norm{B(\theta)}^2+1)\|P(\theta)\|^2\leq 2\tau^2_{B(\theta)}\norm{P(\theta)}^2. 
    \end{align*}
    where we used $\norm{X\kron Y}\leq\norm{X}\norm{Y}$.
\end{proof}

\begin{lemma}[Certainty Equivalent Stabilization]
    \label{lem: CE stabilization}
    Let $\theta_1$ denote the parameter for a stabilizable system, and $\theta_2$ denote the parameter for an alternate system. Suppose that $\norm{\theta_1-\theta_2} \leq \frac{1}{256} \norm{P(\theta_1)}^{-5}$. Then the system described by $\theta_2$ is stabilizable
    \begin{align*}
        \norm{\Sigma^{K(\theta_2)}(\theta_1)} \leq 2 \norm{P(\theta_1)}. 
    \end{align*}
\end{lemma}
\begin{proof}
    To verify this fact, first apply
 \Cref{lem: lyap perturbation} and \Cref{lem: simplifying inequalities} to find
 \begin{align*}
    & \norm{\Sigma^{K(\theta_2)}(\theta_1)} \leq  \norm{\Sigma^{K(\theta_1)}(\theta_1)} + \norm{\Sigma^{K(\theta_1)}(\theta_1)}\norm{\Sigma^{K(\theta_2)}(\theta_1)} \\
    & \paren{2\norm{A(\theta_1) + B(\theta_1) K(\theta_1)} \norm{B(\theta_1) (K(\theta_2)-K(\theta_1)} + \norm{B(\theta_1) (K(\theta_2)-K(\theta_1)}^2  } \\
    &\leq \norm{P(\theta_1)} + \norm{P(\theta_1)} \norm{\Sigma^{K(\theta_2)}(\theta_1)} \paren{2 \norm{P(\theta_1)}^{1/2} \norm{B(\theta_1)(K(\theta_2)  - K(\theta_1))} +\norm{B(\theta_1)(K(\theta_2)  - K(\theta_1))}^2}.
 \end{align*}
 By \Cref{lem: Riccati perturbation} it holds that $\norm{B(\theta_1) (K(\theta_2) - K(\theta_1))} \leq  32 \norm{P(\theta_1)}^{7/2} \norm{\theta_2 - \theta_1}$. Leveraging that $\norm{\theta_2 - \theta_1}\leq\frac{1}{256} \norm{P(\theta_1)}^{-5}$, we conclude 
 \begin{align*}
     \norm{\Sigma^{K(\theta_2)}(\theta_1)} \leq \norm{P(\theta_1)} + \frac{1}{2}\norm{\Sigma^{K(\theta_2)}(\theta_1)}.
 \end{align*}
 Rearranging provides the desired inequality. 
 \end{proof}



\begin{lemma}[Bound on the first and second derivative of K]
    \label{lem: Bound on K' and K''}
    Let $\theta_1, \theta_2$ be the parameters describing two stabilizable systems satisfying $\norm{\theta_1-\theta_2}\leq \frac{1}{16} \norm{P(\theta_1)}^{-2}$. Then for any $t\in[0,1]$, the first and second derivative is bounded as 
    \begin{align*}
        \|\mathsf{D}\VEC K(\tilde\theta)\|_{\mathsf{op}}\leq 24\|P\paren{\theta_1}\|^{7/2}, ~ 
        \|\mathsf{D}^2\VEC K(\tilde\theta)\|_{\mathsf{op}}\leq 4000\|P\paren{\theta_1}\|^{15/2},
    \end{align*}
    where $\tilde \theta = t\theta_1 + (1-t)\theta_2$.
\end{lemma}
\begin{proof}
    It suffices to consider the quantity $K(t)$ defined as
    \begin{align*}
        &K(t) \triangleq K(t\theta_1 + (1-t)\theta_2). 
    \end{align*}
    Additionally define
    \begin{align*}
        P(t) \triangleq P(t\theta_1 + (1-t)\theta_2).
    \end{align*}
    By the proof of Lemma 3.2, C.5 of \citet{simchowitz2020naive} and keeping track of the degree of the term $\norm{P(\theta_1)}$, we get
    \begin{align*}
        \|P'(t)\|&\leq 4\|P(t)\|^3 , 
        ~ \|P''(t)\|\leq178\|P(t)\|^7 \\
        \|K'(t)\|&\leq7\|P(t)\|^{7/2} , ~\|K''(t)\|\leq290\|P(t)\|^{15/2}.
    \end{align*}
    From \Cref{lem: Riccati perturbation}, it holds that $\norm{P(t)} \leq \sqrt{2}\norm{P(\theta_1)}$. Therefore, by noting that the derivatives of $K(t)$ are directional derivatives of $K(\theta)$, it follows that
    \begin{align*}
        &\|\mathsf{D}\VEC K(\tilde\theta)\|_{op}\leq \sup_{t\in[
        0,1]}7\|P(t)\|^{7/2} < 24\|P\paren{\theta_1}\|^{7/2} \\
        &\|\mathsf{D}^2\VEC K(\tilde\theta)\|_{op}\leq \sup_{t\in[
        0,1]}290\|P(t)\|^{15/2} <
        4000\|P\paren{\theta_1}\|^{15/2}.
    \end{align*}
\end{proof}

Leveraging the above result along with a Taylor expansion leads to the following lemma. 
\begin{lemma} [LQR Taylor Expansion]
    \label{lem: LQR Taylor expansion}
    Let $\theta_1, \theta_2$ be the parameters describing two stabilizable systems satisfying $\norm{\theta_1-\theta_2}\leq \frac{1}{16} \norm{P(\theta_1)}^{-2}$. It holds that
    \begin{align*}
        \VEC (K(\theta_2) - K(\theta_1)) &= \mathsf{D}_{\theta}\VEC K(\theta_1)[\theta_2 - \theta_1] + R,
    \end{align*}
    where $R$ is the remainder term that satisfies
    \begin{align*}
        \norm{R} &\leq \frac{1}{2}\sup_{\tilde\theta\in[\theta_1, \theta_2]}\|\mathsf{D}^2\VEC K(\tilde\theta)\|_{op}\|\theta_2-\theta_1\|^2 \leq 2000\|P_{\theta_1}\|^{15/2}\|\theta_2-\theta_1\|^2.
    \end{align*}
\end{lemma}

\begin{lemma}[Suboptimality Gap Bound]
    \label{lem: excess cost decomposition}
    Let $K$ be any controller that stabilizes $\theta$. Then it holds that
    \begin{align*}
        &C(K,\theta) - C(K(\theta), \theta) \\
        &\leq \trace ((K-K(\theta))\Sigma_X^{K(\theta)}(\theta)(K-K(\theta))^T\Psi(\theta))\\
        &+ 2 \norm{P(\theta)}^2 \|\Sigma_X^{K}(\theta)\|\tau_{B(\theta)}^3\|(K-K(\theta))\|^3(\|B(\theta)(K-K(\theta))\| + 2 \norm{P(\theta)}^{1/2}).
    \end{align*}
\end{lemma}
\begin{proof}
    By \Cref{lem: performance difference}, 
    \begin{align*}
        &C(K,\theta) - C(K(\theta), \theta) \\
        &\leq \trace ((K-K(\theta))\Sigma_X^{K(\theta)}(\theta)(K-K(\theta))^T\Psi(\theta) + (K-K(\theta))(\Sigma_X^K(\theta) - \Sigma_X^{K(\theta)}(\theta))(K-K(\theta))^T\Psi(\theta))
\end{align*}
where $\Sigma_X^K(\theta) = \dlyap((A(\theta)+B(\theta) K)^T, I)$. 
By applying \Cref{lem: lyap perturbation} to the second term, we get
\begin{align*}
    &\trace(K-K(\theta))(\Sigma_X^K(\theta) - \Sigma_X^{K(\theta)}(\theta))(K-K(\theta))^T\Psi(\theta))\\
    &\leq \|\Sigma_X^K(\theta)\|\|\Sigma_X^{K(\theta)}(\theta)\|\|K-K(\theta)\|^3\|\Psi(\theta)\|\|B(\theta)\|(\|B(\theta)(K-K(\theta))\| + 2\|A(\theta)+B(\theta) K(\theta)\|).
\end{align*}
To conclude, we apply the inequalities of \Cref{lem: simplifying inequalities} to $\norm{A(\theta) + B(\theta) K(\theta)}$, $\norm{\Sigma^{K(\theta)}(\theta)}$, and $\norm{\Psi(\theta)}$. 
\end{proof}

\begin{lemma}[Taylor Expansion Substitution for Suboptimality Gap]
    \label{lem: cost gap taylor substitution}
    Let $K(\theta_1), K(\theta_2)$ be any certainty equivalence controller that stabilize $\theta_1, \theta_2$, respectively, where $\theta_1, \theta_2$ satisfy $\norm{\theta_1-\theta_2}\leq \frac{1}{16} \norm{P(\theta_1)}^{-2}$. 
    Then it holds that
    \begin{align*}
        &\trace ((K(\theta_2)-K(\theta_1))\Sigma_X^{K(\theta_1)}(\theta_1)(K(\theta_2)-K(\theta_1))^T\Psi(\theta_1)) \\
        &\leq \|\theta_2-\theta_1\|_{H(\theta_1)}^2 + 2e5\tau_{B(\theta_1)}^2\|P(\theta_1)\|^{13}\|\theta_2-\theta_1\|^3 + 8e6\tau_{B(\theta_1)}^2\|P(\theta_1)\|^{17}\|\|\theta_2-\theta_1\|^4.
    \end{align*}
\end{lemma}
\begin{proof}
    By \Cref{lem: simplifying inequalities}, \Cref{lem: Bound on K' and K''} and \Cref{lem: LQR Taylor expansion}, it follows that
    \begin{align*}
        &\trace ((K(\theta_2)-K(\theta_1))\Sigma_X^{K(\theta_1)}(\theta_1)(K(\theta_2)-K(\theta_1))^T\Psi(\theta_1)) \\
        &= \VEC (K(\theta_2) - K(\theta_1))^T(\Psi(\theta_1)\kron\Sigma_X^{K(\theta_1)}(\theta_1))\VEC(K(\theta_2)-K(\theta_1)) \\
        &\leq [\theta_2 - \theta_1]^T\mathsf{D}_{\theta}\VEC K(\theta_1)^T(\Psi(\theta_1)\kron\Sigma_X^{K(\theta_1)}(\theta_1))\mathsf{D}_{\theta}\VEC K(\theta_1)[\theta_2 -\theta_1] \\
        &\hspace{5mm} + \sym([\theta_2 - \theta_1]^T\mathsf{D}_{\theta}\VEC K(\theta_1)^T(\Psi(\theta_1)\kron\Sigma_X^{K(\theta_1)}(\theta_1))R) + R^T(\Psi(\theta_1)\kron\Sigma_X^{K(\theta_1)}(\theta_1))R \\
        &\leq \|\theta_2-\theta_1\|_{H(\theta_1)}^2 + 2e5\tau_{B(\theta_1)}^2\|P(\theta_1)\|^{13}\|\theta_2-\theta_1\|^3 + 8e6\tau_{B(\theta_1)}^2\|P(\theta_1)\|^{17}\|\|\theta_2-\theta_1\|^4.
    \end{align*} 
    where it follows from $\trace(A^T, B) = \VEC (A)^T\VEC B$ and $\VEC (ABC) = (C^T\kron A)\VEC (B)$ in the first equality, and we let the operator $\sym$ denote $\sym(A) = A+A^T$. 
\end{proof}

\begin{lemma}[Perturbation on $B(\theta), H(\theta)$]
    \label{lem: helper lemma for RC}
    Let $\theta_1, \theta_2$ be the parameters describing two stabilizable systems satisfying $\norm{\theta_1 - \theta_2}\leq\frac{1}{16}\norm{P(\theta_1)}^{-2}$. Then it holds that
    \begin{itemize}
        \item $\norm{B(\theta_2)}\leq \norm{B(\theta_1)} + \norm{\theta_1 - \theta_2}$
        \item $\norm{\Psi(\theta_2) - \Psi(\theta_1)} \leq 15\tau^2_{B(\theta_1)}\norm{P(\theta_1)}^3\norm{\theta_1-\theta_2}$
        \item $\norm{H(\theta_2)-H(\theta_1)}\leq 5e6\tau_{B(\theta_1)}^2\norm{P(\theta_1)}^{17}\norm{\theta_1-\theta_2}$.
        \item $\norm{H(\theta_2)} \leq 8e3\tau^2_{B(\theta_1)}\norm{P(\theta_1)}^9$
    \end{itemize}
\end{lemma}
\begin{proof}
    By the triangle inequality,
    \begin{align*}
        \norm{B(\theta_2)} = \norm{B(\theta_1) + (B(\theta_2) - B(\theta_1))} \leq \norm{B(\theta_1)} + \norm{\theta_1 - \theta_2}.
    \end{align*}
    It follows that
    \begin{align*}
        \tau^2_{B(\theta_2)} &\leq \max\{\norm{B_2}^2, 1\} \leq \max\{\norm{B(\theta_1)}^2+2\norm{B(\theta_1)}\norm{\theta_1-\theta_2} + \norm{\theta_1-\theta_2}, 1\} \leq 2\tau_{B(\theta_1)}^2 \\
        \tau^3_{B(\theta_2)} &\leq \max\{\norm{B_2}^3, 1\} \leq \max\{\norm{B(\theta_1)}^3+3\norm{B(\theta_1)}^2\norm{\theta_1-\theta_2} + 3\norm{B(\theta_1)}\norm{\theta_1-\theta_2}^2
        \norm{\theta_1-\theta_2}^3, 1\} \\
        &\leq 2\tau_{B(\theta_1)}^3
    \end{align*}
    where we applied $\norm{\theta_1-\theta_2} \leq \frac{1}{16}\norm{P(\theta_1)}^{-2}$ in the last inequality.
    For the second inequality, it holds from \Cref{lem: Riccati perturbation} that
    \begin{align*}
        &\norm{\Psi(\theta_2) - \Psi(\theta_1)} \\
        &\leq \norm{\{B(\theta_1) + B(\theta_2)-B(\theta_1)\}^TP(\theta_2)\{B(\theta_1) + B(\theta_2)-B(\theta_1)\} - B(\theta_1)^TP(\theta_1)B(\theta_1)} \\
        &\leq \norm{B(\theta_1)^T\paren{P(\theta_2)-P(\theta_1)}B(\theta_1)} + \norm{\sym\paren{B(\theta_1)^TP(\theta_2)\paren{B(\theta_2)-B(\theta_1)}}} \\
        &\quad+ \norm{\paren{B(\theta_2)-B(\theta_1)}^TP(\theta_2)\paren{B(\theta_2)-B(\theta_1)}} \\
        &\leq 8\sqrt{2}\norm{B(\theta_1)}^2\norm{P(\theta_1)}^3\norm{\theta_1-\theta_2} + 
        2\sqrt{2}\norm{B(\theta_1)}\norm{B(\theta_2)-B(\theta_1)}\norm{P(\theta_1)}  \\
        &\quad + \sqrt{2}\norm{B(\theta_2)-B(\theta_1)}^2\norm{P(\theta_1)} \\
        &\leq 15\tau^2_{B(\theta_1)}\norm{P(\theta_1)}^3\norm{\theta_1-\theta_2}.
    \end{align*}
    where we applied $\norm{\theta_1-\theta_2} \leq \frac{1}{16}\norm{P(\theta_1)}^{-2}$ in the last inequality. 
    Next, let $K'(\theta) \triangleq \mathsf{D}_{\theta}\VEC K(\theta)$. Then from \Cref{lem: Bound on K' and K''}, 
    \begin{align*}
        \norm{K'(\theta_2)-K'(\theta_1)} 
        &= \sup_{\tilde\theta\in[\theta_1, \theta_2]}\norm{K''(\tilde\theta)}\norm{\theta_1-\theta_2}
        \leq 4000\norm{P(\theta_1)}^{15/2}\norm{\theta_1-\theta_2}.
    \end{align*}
    From \Cref{lem: Riccati perturbation} and \Cref{lem: simplifying inequalities}
    \begin{align*}
        &\norm{A(\theta_2)+B(\theta_2)K(\theta_2) - A(\theta_1)+B(\theta_1)K(\theta_1)} \\
        &\leq \norm{A(\theta_2)-A(\theta_1)} + \norm{(B(\theta_2)-B(\theta_1))K(\theta_2)} + \norm{B(\theta_1)(K(\theta_2)-K(\theta_1))} \\
        &\leq \norm{\theta_1-\theta_2} + \norm{\theta_1-\theta_2}\norm{P(\theta_2)}^{1/2} + 32\norm{P(\theta_1)}^{7/2}\norm{\theta_1-\theta_2} \\
        &\leq 35\norm{P(\theta_1)}^{7/2}\norm{\theta_1-\theta_2}. 
    \end{align*}
    For $\Sigma_X^{K(\theta)}(\theta)$, from \Cref{lem: lyap perturbation} and above calculation, 
    \begin{align*}
        &\norm{\Sigma_X^{K(\theta_2)}(\theta_2) - \Sigma_X^{K(\theta_1)}(\theta_1)} \\
        &\leq \norm{\Sigma_X^{K(\theta_1)}(\theta_1)}\norm{\Sigma_X^{K(\theta_2)}(\theta_2)}\paren{70\norm{P(\theta_1)}^{4}\norm{\theta_1-\theta_2} + 35^2\norm{P(\theta_1)}^{7}\norm{\theta_1-\theta_2}^2} \\
        &\leq \sqrt{2}\norm{P(\theta_1)}^2\paren{70\norm{P(\theta_1)}^{4}\norm{\theta_1-\theta_2} + 35^2\norm{P(\theta_1)}^{7}\norm{\theta_1-\theta_2}^2} \\
        &\leq 225\norm{P(\theta_1)}^{7}\norm{\theta_1-\theta_2},
    \end{align*}
    where we applied $\norm{\theta_1-\theta_2} \leq \frac{1}{16}\norm{P(\theta_1)}^{-2}$ in the last inequality.
    Now let $M(\theta)$ denote $M(\theta) \triangleq \Psi(\theta)\kron\Sigma_X^{K(\theta)}(\theta)$. 
    Then we get
    \begin{align*}
        &\norm{M(\theta_2) - M(\theta_1)} \\
        &= \norm{\paren{\Psi(\theta_1) + \Psi(\theta_2) - \Psi(\theta_1)}\kron\paren{\Sigma_X^{K(\theta_1)}(\theta_1) + \Sigma_X^{K(\theta_2)}(\theta_2) - \Sigma_X^{K(\theta_1)}(\theta_1)} - \Psi(\theta_1)\kron\Sigma_X^{K(\theta_1)}(\theta_1)} \\
        &\leq \norm{\Psi(\theta_2) - \Psi(\theta_1)}\norm{\Sigma_X^{K(\theta_1)}(\theta_1)} + \norm{\Psi(\theta_1)}\norm{\Sigma_X^{K(\theta_2)}(\theta_2) - \Sigma_X^{K(\theta_1})(\theta_1)} \\
        &\hspace{5mm}+ \norm{\Psi(\theta_2) - \Psi(\theta_1)}\norm{\Sigma_X^{K(\theta_2)}(\theta_2) - \Sigma_X^{K(\theta_1)}(\theta_1)} \\
        &\leq 4e3\tau_{B(\theta_1)}^2\norm{P(\theta_1)}^{10}\norm{\theta_1-\theta_2}
    \end{align*}
    where we used $\norm{X\kron Y}\leq\norm{X}\norm{Y}$.
    Thus from \Cref{lem: Bound on K' and K''}
    \begin{align*}
        &\norm{H(\theta_2) - H(\theta_1)} \\
        &= \norm{K'(\theta_2)^TM(\theta_2)K'(\theta_2) - K'(\theta_1)^TM(\theta_1)K'(\theta_1)} \\
        &= \norm{\{K'(\theta_1) + K'(\theta_2) - K'(\theta_1)\}^TM(\theta_2)\{K'(\theta_1) + K'(\theta_2) - K'(\theta_1)\} -K'(\theta_1)^TM(\theta_1)K'(\theta_1)} \\
        &= \norm{K'(\theta_1)^T\{M(\theta_2) - M(\theta_1)\}K'(\theta_1)} + \norm{\sym\{K'(\theta_1)^TM(\theta_2)(K'(\theta_2) - K'(\theta_1))\}} \\
        &\hspace{5mm} + \norm{\{K'(\theta_2) - K'(\theta_1)\}^TM(\theta_2)\{K'(\theta_2) - K'(\theta_1)\}} \\
        &\leq \norm{K'(\theta_1)}^2\norm{M(\theta_2)-M(\theta_1)} + 2\norm{K'(\theta_1)}\norm{K'(\theta_2) - K'(\theta_1)}\norm{M(\theta_2)} \\
        &\quad + \norm{K'(\theta_2) - K'(\theta_1)}^2\norm{M(\theta_2)} \\
        &\leq 5e6\tau_{B(\theta_1)}^2\norm{P(\theta_1)}^{17}\norm{\theta_1-\theta_2}
    \end{align*}
    For the last fact, from \Cref{lem: simplifying inequalities} and \Cref{lem: Bound on K' and K''}, 
    \begin{align*}
        \norm{H(\theta_2)} &\leq \norm{K'(\theta_2)}^2\norm{M(\theta_2)} 
        \leq 2e3\norm{P(\theta_1)}^7\cdot4\tau^2_{B(\theta_1)}\norm{P(\theta_1)}^2 = 8e3\tau^2_{B(\theta_1)}\norm{P(\theta_1)}^9
    \end{align*}
\end{proof}



\section{Least Squares Analysis}
\label{s: id bound proof}

In this section, we provide a characterization of the least squares error for the procedure discussed in \Cref{s: methods}. The following result characterizes the weighted parameter identification error of the least squares. %The weighting is es captures how the quality of estimation for particular parameters impacts the control cost achieved by using the estimated parameters for synthesis.

\begin{lemma}[Least Squares Identification Bound]
    \label{thm: identification bound}
    Let $H$ be a positive definite matrix belonging to $\mathbb{R}^{d_{\theta}\times d_{\theta}}$. Suppose the dataset $\curly{(X_t^n, U_t^n, X_{t+1}^n)}_{t=1, n=1}^{T,N}$ is collected from system \eqref{eq: linear system} using random noise $U_t \sim \calN(0, \Sigma_u)$. Let $\hat \theta$ be the least squares estimate \eqref{eq: least squares}, and $\hat {\mathsf{FI}}$ be the Fisher Information estimate \eqref{eq: fisher estimate}. Let $\delta \in (0,1)$. It holds that with probability at least $1-\delta$ that 
    \begin{align}
        \norm{\hat \theta - \theta^\star}_H^2 \leq 4 \frac{\trace\paren{H \mathsf{FI}(\theta^\star)^{-1}}}{N} + 8 \frac{\norm{H \mathsf{FI}(\theta^\star)^{-1}}}{N} \log\frac{2}{\delta},
    \end{align}
    and
    % \begin{align*}
        $0.5 \mathsf{FI}(\theta^\star) \preceq \hat{\mathsf{FI}} \preceq 2 \mathsf{FI}(\theta^\star)$
    % \end{align*}
    as long as the number of experiments satisfies 
    % \begin{align*}
        $N \geq N_{\mathsf{ID}},$
    % \end{align*}
    for $N_{\mathsf{ID}}$ in \eqref{eq: id burn-in}.  
    %\begin{align*}
    %    N_{\mathsf{ID}} = \mathsf{poly}(\log(1/\delta), \dx, \du,, \frac{1}{\lambda_{\min}(\mathsf{FI}(\theta^\star)^{-1})}, \norm{\Sigma_w}, \norm{\Sigma_u}, \lambda_{\min}(\Sigma_w), \lambda_{\min}(\Sigma_u), \calJ(\theta^\star)),
    %\end{align*}
    %and $\calJ(\theta^\star) = \sum_{t=0}^{T-1} \norm{A(\theta^\star)^t \bmat{I & B(\theta^\star)}}$. 
\end{lemma}
The result follows standard arguments applying concentration inequalities to linear systems \cite{ziemann2023tutorial, tu2024learning}. The specific form with the weighting matrix $H$ is an instantiation of Theorem 3.1 in \cite{lee2024active}. For completeness, a proof is provided below. 

To prove \Cref{thm: identification bound}, we first state two supporting lemmas. The first is a standard result on covariance concentration from \citet{jedra2020finite}. We define the matrix $\Gamma_x$ as 
\begin{align*}
    \Gamma_x = \bmat{0 \\ \bmat{I & B} \\ A \bmat{I & B} &\bmat{I & B} \\ \vdots \\ A^{T-1} \bmat{I & B} & \dots \bmat{I & B}}
\end{align*}
such that for any experiment $n$,
\begin{align*}
    \bmat{X_1^n \\ \vdots \\ X_T^n} = \Gamma_x \bmat{W_1^n \\ U_1^n \\ \vdots \\ W_{T-1}^n \\ U_{T-1}^n}. 
\end{align*}

\begin{lemma}[Covariance Concentration]
    \label{lem: covariance concentration}
    Consider collecting $N$ trajectories of length $T$ from \eqref{eq: linear system}. 
    Let $\beta \in \paren{0, \frac{1}{4}\lambda_{\min}\paren{\mathbf{E} \sum_{t=1}^T \bmat{X_t \\ U_t} \bmat{X_t\\ U_t}^\top}}$. Define the event 
    \begin{align}
        \label{eq: covariance concentration event}
        \calE \triangleq \norm{ \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^T \bmat{X_t^n \\ U_t^n} \bmat{X_t^n\\ U_t^n}^\top - \mathbf{E} \sum_{t=1}^T \bmat{X_t \\ U_t} \bmat{X_t\\ U_t}^\top} \leq \beta.
    \end{align}
    There exists a universal positive constant $c$ such that if 
    \begin{align*}
        N \geq c \frac{\norm{\Gamma_x}^2 \norm{\Sigma_u} \norm{\Sigma_w} \norm{\mathbf{E} \sum_{t=1}^T \bmat{X_t \\U_t}\bmat{X_t \\U_t}^\top}^2}{\lambda_{\min}\paren{\bfE  \sum_{t=1}^T \bmat{X_t \\U_t}\bmat{X_t \\U_t}^\top}\beta^2} \paren{\log\frac{1}{\delta} + \dx + \du},
    \end{align*}
    then $\calE$ holds with probability at least $1-\delta$.
\end{lemma}
\begin{proof}
Let  $\eta \sim \calN(0, I_{\dx \du T})$ and $\Gamma$ be the matrix mapping from 
\begin{align*}
    \bmat{W_1^n \\ U_1^n \\ \vdots \\ W_{T-1}^n \\ U_{T-1}} \textrm{ to } \bmat{X_1^n \\ U_1^n \\X_2^n \\ U_2^n \\ \vdots \\ X_T^n \\ U_T^n}. \mbox{ Then  } \bmat{X_1^n \\ U_1^n \\X_2^n \\ U_2^n \\ \vdots \\ X_T^n \\ U_T^n} \overset{d}{=} \tilde \Gamma \eta, \mbox{ where } \tilde \Gamma = \Gamma \paren{I_T \otimes \bmat{\Sigma_w^{1/2} \\ & \Sigma_u^{1/2}}}.
\end{align*}
Let $M = \left(N \mathbf{E} \sum_{t=1}^T \bmat{X_t \\ U_t} \bmat{X_t\\ U_t}^\top\right)^{-1/2}.$
It holds that
\begin{align*}
    &\norm{M \sum_{n=1}^N \sum_{t=1}^T \bmat{X_t^n \\ U_t^n} \bmat{X_t^n \\ U_t^n}^\top M - I}\\
    &= \sup_{v \in \calS^{d-1}} v^\top \paren{M \sum_{n=1}^N \sum_{t=1}^T \bmat{X_t^n \\ U_t^n} \bmat{X_t^n \\ U_t^n}^\top M - I}v \\
    &= \sup_{v \in \calS^{d-1}} v^\top \paren{M \sum_{n=1}^N \sum_{t=1}^T \bmat{X_t^n \\ U_t^n} \bmat{X_t^n \\ U_t^n}^\top M - \mathbf{E}\brac{\paren{M \sum_{n=1}^N \sum_{t=1}^T \bmat{X_t^n \\ U_t^n} \bmat{X_t^n \\ U_t^n}^\top} M}}v \\
    &= \sup_{v \in \calS^{d-1}} \norm{\sigma_{Mv}^\top \tilde \Gamma \eta}^2 - \mathbf{E} \norm{\sigma_{Mv}^\top \tilde \Gamma \eta}^2,
\end{align*}
where $\sigma_{Mv} \triangleq I_{NT} \otimes (Mv).$ By the Hanson-Wright inequality applied to Gaussian quadratic forms (presented for subGaussian forms in Theorem 6.3.2 of \citep{vershynin2020high}, and specialized to Gaussians by \citet{laurent2000adaptive}) along with a covering argument, it holds that with probability at least $1-\delta$, 
\begin{align*}
    \sup_{v \in \calS^{d-1}} \norm{\sigma_{Mv}^\top \tilde \Gamma \eta}^2 - \mathbf{E} \norm{\sigma_{Mv}^\top \tilde \Gamma \eta}^2 \leq \frac{\beta}{\norm{\mathbf{E} \sum_{t=1}^T \bmat{X_t \\ U_t} \bmat{X_t\\ U_t}^\top}}
\end{align*}
with probability at least $1-\exp\paren{-c_1 N \beta^2 \frac{\lambda_{\min}\paren{\mathbf{E} \sum_{t=1}^T \bmat{X_t \\ U_t} \bmat{X_t\\ U_t}^\top}}{\norm{\tilde \Gamma}^2 \norm{\mathbf{E} \sum_{t=1}^T \bmat{X_t \\ U_t} \bmat{X_t\\ U_t}^\top}^2} + c_2 d}$. Inverting this and applying submultiplicativity to bound $\norm{\tilde \Gamma}$ concludes the statement.



\end{proof}
% \begin{align*}
%     \Gamma = \bmat{0 \\
%     \bmat{I & B} \\ 
%     A \bmat{I & B} & \bmat{I & B}  \\ 
%     \vdots \\
%     A^{T-1} \bmat{I & B} & \dots & \dots  & \bmat{I & B}   
%     } \textrm{ such that for any $n\in[N]$} \bmat{X_{1}^n \\ X_2^n \\ \vdots X_T^n} = \Gamma \bmat{W_1^n \\ U_1^n \\ \vdots \\ W_{T-1}^n \\ U_{T-1}}
% \end{align*}

The second is a self-normalized martingale bound, adapted in Lemma A.8 of \citet{lee2024active} from the standard self-normalized martingale bound in Theorem 14.7 of \citet{pena2009self}.The specific form that we use is from Lemma A.8 of \citet{lee2024active}. 
\begin{lemma}[Lemma A.8 of \citet{lee2024active}]
\label{lem: sn martingale bnd}
    Let $\curly{W_k}_{k=1}^K$ be a sequence of standard normal Gaussian random variables.  Let $\curly{Z_{k}}_{k=1}^K$ be a sequence of Gaussian random vectors assuming values in $\R^{d_{\theta}}$ such that $Z_k$ is independent from $W_\tau$ for $\tau \geq k$.  Let $\Sigma_Z \triangleq \mathbf{E} \frac{1}{K} \sum_{k=1}^K Z_k Z_k^\top$. Suppose $H \in \R^{d_{\theta} \times d_{\theta}}$ is positive definite and $\beta \in \R$ satisfies 
    \[
        0 < \beta \leq \frac{\lambda_{\min}\paren{\Sigma_Z}}{2}.
    \]
    As long as the event $\norm{\frac{1}{K} \sum_{k=1}^K Z_k Z_k^\top - \frac{1}{K} \mathbf{E} \sum_{k=1}^K Z_k Z_k^\top} \leq \beta$, then the following holds with probability at least $1-\delta$, 
    \begin{align*}
        &\norm{\left(\sum_{k=1}^K Z_k Z_k^\top\right)^{-1} \sum_{k=1}^K Z_k W_k}_H^2 \leq 2 \paren{1 + \frac{4\beta}{\lambda_{\min}(\Sigma_Z)}}\paren{ \trace\paren{(K \Sigma_Z)^{-1} H} + 2  \norm{(K \Sigma_Z)^{-1} H} \log \frac{1}{\delta}}. 
    \end{align*}
\end{lemma}
Leveraging the above two results, we can prove \Cref{thm: identification bound}. We write the least squares identification error as
\begin{align*}
    \bmat{ A(\hat \theta) & B(\hat \theta)} - \bmat{ A(\theta^\star) & B(\theta^\star)} = \sum_{t=1,n=1}^{T,N} W_{t}^n \bmat{X_t^n \\ U_t^n}^\top \left(\sum_{t=1,n=1}^{T,N} \bmat{X_t^n \\ U_t^n} \bmat{X_t^n \\ U_t^n}^\top\right).
\end{align*}
The noise covariance of $W_t^n$ can be pulled out such that for $\xi_{t,n} =\Sigma_w^{-1/2} W_{t}^n$, 
\begin{align*}
    \bmat{ A(\hat \theta) & B(\hat \theta)} - \bmat{ A(\theta^\star) & B(\theta^\star)} = \sum_{t=1,n=1}^{T,N} \Sigma_w^{1/2} \xi_{t}^n \bmat{X_t^n \\ U_t^n}^\top \left(\sum_{t=1,n=1}^{T,N} \bmat{X_t^n \\ U_t^n} \bmat{X_t^n \\ U_t^n}^\top\right).
\end{align*}
Applying the vectorization identity $\VEC(XYZ) = (Z^\top \otimes X) \VEC Y$, we find that
\begin{align*}
    &\VEC \paren{\bmat{ A(\hat \theta) & B(\hat \theta)} - \bmat{ A(\theta^\star) & B(\theta^\star)}} \\
    &= \left(\sum_{t=1,n=1}^{T,N}    \paren{\bmat{X_t^n \\ U_t^n} \otimes \Sigma_w^{-1/2}}\paren{ \bmat{X_t^n \\ U_t^n}^\top  \otimes \Sigma_w^{-1/2}}\right)^{-1}\sum_{t=1,n=1}^{T,N} \paren{\bmat{X_t^n \\ U_t^n} \otimes \Sigma_w^{-1/2}} \eta_t^n.
\end{align*}
This can be further decomposed by letting $Z_{t}^n[i]$ be the $i^{\mathsf{th}}$ column of $\bmat{X_t^n \\ U_t^n} \otimes \Sigma_w^{-1/2}$, and $\eta_t^n[i]$ be the $i^{\mathsf{th}}$ entry of $\eta_t^n$. Then 
\begin{align*}
    &\VEC \paren{\bmat{ A(\hat \theta) & B(\hat \theta)} - \bmat{ A(\theta^\star) & B(\theta^\star)}} = \left(\sum_{t=1, n=1, i=1}^{T,N,\dx} Z_t^n[i] Z_t^n[i]^\top\right)^{-1}\sum_{t=1,n=1,i=1}^{T,N, \dx} Z_t^n[i] \eta_t^n[i].
\end{align*}
Invoking \Cref{lem: covariance concentration}, the event
\begin{align*}
    \calE = \curly{\norm{\frac{1}{N} \sum_{t=1, n=1, i=1}^{T,N,\dx} Z_t^n[i] Z_t^n[i]^\top - \Sigma_Z} \leq \frac{1}{4} \lambda_{\min}(\Sigma_Z)},
\end{align*}
with $\Sigma_Z = \mathbf{E} \sum_{t=1, i=1}^{T,\dx} Z_t[i] Z_t[i]^\top$
holds with probability at least $1-\delta/2$ as long as 
\begin{align*}
   N \geq c \frac{\norm{\Gamma_x}^2 \norm{\Sigma_u} \norm{\Sigma_w}^3 \norm{\mathbf{E} \sum_{t=1}^T \bmat{X_t \\U_t}\bmat{X_t \\U_t}^\top}^2}{\lambda_{\min}\paren{\bfE  \sum_{t=1}^T \bmat{X_t \\U_t}\bmat{X_t \\U_t}^\top}^3 \lambda_{\min}(\Sigma_w)^2} \paren{\log\frac{1}{\delta} + \dx + \du},
\end{align*}
for a universal positive constant $c$.  Under this event, \Cref{lem: sn martingale bnd} implies that with probability at least $1-\delta/2$,
\begin{align*}
    & \norm{\VEC \paren{\bmat{ A(\hat \theta) & B(\hat \theta)} - \bmat{ A(\theta^\star) & B(\theta^\star)}}}_H^2 \leq  4\paren{ \trace\paren{(N \Sigma_Z)^{-1} H} + 2  \norm{(N \Sigma_Z)^{-1} H} \log \frac{2}{\delta}}.
\end{align*}
To conclude the proof, note that $\Sigma_Z = \mathsf{FI}(\theta^\star)$, and union bound over the success events. Additionally, note that $\norm{\mathbf{E} \sum_{t=1}^T \bmat{X_t \\U_t}\bmat{X_t \\U_t}^\top}$ and $\norm{\Gamma_x}$ may be bounded in terms of $\calJ(\theta^\star) = \sum_{t=0}^{T-1} \norm{A(\theta^\star)^t \bmat{I & B(\theta^\star)}}$, $\norm{\Sigma_w}$ and $\norm{\Sigma_w}$ \citep{jedra2020finite}. 

\section{Fundamental Limits}
\label{s: lower bound proof}

While good algorithm design can decrease the suboptimality gap of the learned controller, there are fundamental limits on the achievable performance. These limits are characterized by the signal-to-noise ratio of the experiment procedure, as well as the sensitivity of the LQR problem to error in the parameter estimates. To capture the signal-to-noise ratio for the experimental procedure, we define the Fisher Information matrix:
\begin{align}
    \mathsf{FI}(\theta) \triangleq \mathbf{E}_{\theta}\brac{\sum_{t=1}^T \bmat{X_t \\ U_t} \bmat{X_t \\ U_t}^\top } \otimes \Sigma_W^{-1}. \label{eq:FI}
    %D_{\theta} \VEC \bmat{A(\theta) & B(\theta)}. \paren{D_{\theta} \VEC \bmat{A(\theta) & B(\theta)} }^\top  
\end{align}
To characterize the sensitivity of the LQR problem, we define the matrix $H(\theta)$ as 
\begin{align}
    \label{eq: model task Hessian}
    H(\theta) \triangleq D_{\theta} \VEC K(\theta)^\top  (\Sigma_X(\theta) \otimes (B(\theta)^\top P(\theta) B(\theta) + R)) D_{\theta} \VEC K(\theta).
\end{align}
One can verify that this matrix is the Hessian of the cost $C(K(\tilde \theta), \theta)$ with repsect to $\tilde \theta$, and evaluated at $\tilde \theta = \theta$ \citep{wagenmaker2021task}. The below theorem presents a lower bound on the  $\varepsilon$-local minimax excess cost gap in terms of these quantities. 
\begin{theorem}
    \label{thm: lower bound}
    Let $\rho(\theta', T, N)$ denote the distribution of the experiment data induced by running the aforementioned experiment procedure on the system $X_{t+1} = A(\theta') X_t + B(\theta') U_t + W_t$ for $N$ episodes of length $T$. Assume that $\rho(A(\theta^\star)) < 1$.   Consider applying any learning algorithm $\calA$ that maps a dataset \eqref{eq: dataset} to a controller $K$. Let $\varepsilon \in \R$ satisfy $0 \leq \varepsilon \leq {\varepsilon_{UB}}$, where $\varepsilon_{\mathsf{UB}} = \frac{1}{\mathsf{poly}(\norm{P(\theta^\star}, \tau_{B(\theta^\star)}, \norm{\mathsf{FI}(\theta^\star)}, \frac{1}{\lambda_{\min}(\mathsf{FI}(\theta^\star))})}$. Additionally suppose that $N \geq N_{LB}$ where $N_{LB} = \frac{1}{\varepsilon^2} \frac{1}{\lambda_{\min}(\mathsf{FI}(\theta^\star))} \mathsf{poly}(d_{\theta}, \norm{P(\theta)})$. It holds that 
    \begin{align*}
        &\sup_{\theta' \in \calB(\theta^\star, \varepsilon)} \mathbf{E}_{\mathsf{Data} \sim p(\theta', T, N)}\brac{C\paren{\calA\paren{\mathsf{Data}}, \theta'} - C(K(\theta'), \theta')} \geq \frac{1}{8} \trace\paren{H(\theta^\star) (N\mathsf{FI}(\theta^\star))^{-1}}.  
    \end{align*}
\end{theorem}
The above result follows from Theorem 2.2 of \citet{lee2023fundamental}; however, it is rewritten to demonstrate tight dependence on the system-theoretic quantities. A proof of this result is provided in below.
%  In light of the above fundamental limit, a learning algorithm is classified as efficient if we can find an upper bound that matches this lower bound up to universal constants for large $N$.

\begin{proof}
    

Denote the data by $Z$. Let $\lambda$ be a prior density over $\theta$ satisfying $\lambda(\theta) \propto (1 - \frac{1}{\varepsilon^2} \norm{\theta - \theta^\star}^2)^2$ for $\theta \in \calB(\theta^\star, \varepsilon)$. We may lower bound the minimax quantity by an expectation over the prior:
\begin{align*}
    &\sup_{\theta'\in\calB(\theta^\star, \varepsilon)} \bfE_{\mathsf{Z} \sim p(\theta', T, N)} \brac{C(\calA(Z), \theta') -C(K(\theta'), \theta') } \geq \bfE_{\Theta \sim \lambda} \bfE_{Z \sim p(\Theta, T, N)} \brac{C(\calA(Z), \Theta) -C(K(\Theta), \Theta) }.
    \end{align*}
Next, we will apply the performance difference lemma to lower bound the expected excess cost in terms of the gap between the controller output by our algorithm and the optimal controller. To do so, we condition on the event that our algorithm outputs a controller close enough to the certainty equivalent controllers within the ball. In particular, we define 
\begin{align*}
    \calE &= \curly{\sup_{\theta\in\calB(\theta^\star, \varepsilon)} \norm{\calA(Z) - K(\theta)} \leq \alpha }, \mbox{ where }
    \alpha = \inf_{\theta\in\calB(\theta^\star, \varepsilon)} \frac{1}{12 \norm{\Sigma^{K(\theta)}}^{5/2}}.%\inf_{\theta\in\calB(\theta^\star, \varepsilon)} \min\curly{\frac{\norm{A_{cl}(\theta)}}{\norm{B(\theta)}}, \frac{\lambda_{\min}(\Sigma^{K(\theta)}(\theta))/24}{\norm{A_{cl}(\theta)} \norm{B(\theta)} \sum_{t=0}^\infty \norm{A_{cl}(\theta)^t} \norm{\Sigma^{K(\theta)}(\theta)}}}. 
\end{align*}
Additionally define $\tilde \Psi$ and $\tilde \Sigma$ as the largest matrices in semidefinite order such that $\tilde \Sigma \preceq \Sigma^{K(\theta)}(\theta)$ and $\tilde \Psi \preceq \Psi(\theta)$ for all $\theta \in B(\theta^\star, \varepsilon)$. 
This allows us to apply \Cref{lem: performance difference} to achieve the following lower bound:
\begin{align*}
    &\sup_{\theta'\in\calB(\theta^\star, \varepsilon)} \bfE_{\mathsf{Z} \sim p(\theta', T, N)} \brac{C(\calA(Z), \theta') -C(K(\theta'), \theta') } \\
    &\geq  \bfE_{\Theta \sim \lambda} \bfE_{Z\sim p(\Theta, T, N)} \brac{ \trace\paren{(\calA(Z)  - K(\Theta)) \Sigma^{\calA(Z)}(\Theta) (\calA(Z) - K(\Theta))^\top \Psi(\Theta)} \mathbf{1}_{\calE}}, \\
    &\geq \frac{1}{2} \bfE_{\Theta \sim \lambda} \bfE_{Z \sim p(\Theta, T, N)} \brac{ \trace\paren{(\calA(Z) - K(\Theta)) \tilde \Sigma (\calA(Z) - K(\Theta))^\top \tilde \Psi} \mathbf{1}_{\calE}},
\end{align*}
where the final inequality follows from the fact that if $\calE$ holds, then \Cref{lem: lyap perturbation} ensures $\Sigma^{\calA(\mathsf{Data}}(\Theta) \succeq \frac{1}{2}\Sigma^{K(\Theta)}(\Theta),$ (\Cref{lem: cov lower bound}) and by substituting the lower bounds $\tilde \Sigma \preceq \Sigma^{K(\Theta)}(\Theta)$, $\tilde \Psi \preceq \Psi(\Theta)$. 

By application of the Van Trees inequality, as in Theorem 2.1 of \citet{lee2023fundamental}, it holds that 
\begin{align*}
    &\sup_{\theta'\in\calB(\theta^\star, \varepsilon)} \bfE_{Z \sim p(\theta', T, N)} \brac{C(\calA(Z), \theta') -C(K(\theta'), \theta') } \\
    &\geq \frac{1}{2} \trace\paren{ \paren{\tilde \Sigma \otimes \tilde \Psi}  \bfE \brac{D_{\theta} \VEC K(\Theta) \mathsf{1}_{\calE}} \paren{N\bfE\brac{\mathsf{FI}(\Theta)} + J(\lambda)}^{-1} \bfE \brac{D_{\theta} \VEC K(\Theta) \mathsf{1}_{\calE}} } \\
    &\geq \frac{1}{2} \inf_{\tilde \theta_1, \tilde \theta_2, \tilde\theta_3 \in \calB(\theta^\star, \varepsilon)} \trace\paren{ \paren{\tilde \Sigma \otimes \tilde \Psi}  D_{\theta} \VEC K(\tilde \theta_1)  \paren{N\mathsf{FI}(\tilde \theta_3) + J(\lambda)}^{-1} D_{\theta} \VEC K(\tilde \theta_2)}  \mathbf{P}(\calE)^2,
\end{align*}
where $J(\lambda) = \int \nabla \log \lambda(\theta) (\nabla \log \lambda(\theta))^\top \lambda(\theta) d\theta$ satisfies $\norm{J(\lambda)} \leq \frac{1}{\varepsilon^2} \frac{32}{d_\theta+2} \frac{\Gamma((d_\theta+5)/2)}{\Gamma(d_{\theta}/2)^2}$ (by the triangle inequality and direct calculation). Furthermore, first order Taylor expansions of $\Sigma^{K(\theta)}(\theta)\kron \Psi(\theta)$, $D_\theta \mathsf{vec} K(\theta)$, and $\mathsf{FI}(\theta)$ about $\theta^\star$, combined with the bounds of \Cref{lem: LQR Taylor expansion}, \Cref{lem: helper lemma for RC}, and \Cref{lem: lyap perturbation} we can express the above quantity as
\begin{align}\label{eq: lb perturbation}
    \frac{1}{2}  \trace\paren{ \paren{H(\theta^\star) + M_1}  (D_{\theta} \VEC K( \theta^\star) + M_2)  \paren{N\mathsf{FI}(\theta^\star) +  M_3+ J(\lambda)}^{-1} (D_{\theta} \VEC K(\theta^\star) + M_4)}  \mathbf{P}(\calE)^2,
\end{align} 
\sloppy where $\norm{M_1} \leq 1e7 \tau_{B(\theta^\star)}^2 \norm{P(\theta^\star)}^{15} \varepsilon$, $\norm{M_2} \leq 2000 \norm{P(\theta^\star)}^{15/2} \varepsilon$, $\norm{M_3} \leq  2N \norm{\mathsf{FI}(\theta^\star)} \varepsilon$ and $\norm{M_4} \leq 2000 \norm{P(\theta^\star)}^{15/2} \varepsilon$.

We will show that the above quantity is at least 
\begin{align*}
    \frac{1}{8}  \trace\paren{ \paren{H(\theta^\star)}  (D_{\theta} \VEC K( \theta^\star))  \paren{N\mathsf{FI}(\theta^\star)}^{-1} (D_{\theta} \VEC K(\theta^\star))}.
\end{align*}
To do so, assume to the contrary that the inequality does not hold. 
Under this assumption, we will show that $\bfP(\calE) \geq \frac{1}{\sqrt{2}}$. This follows by observing that $\sup_{\theta\in\calB(\theta^\star, \varepsilon)} \norm{\calA(Z) - K(\theta)}\leq \inf_{\theta\in B(\theta^\star, \varepsilon)} \norm{\calA(Z) - K(\theta)} + \sup_{\theta_1,\theta_2\in\calB(\theta^\star, \varepsilon))} \norm{K(\theta_1) - K(\theta_2)} \leq\inf_{\theta\in B(\theta^\star, \varepsilon)} \norm{\calA(Z) - K(\theta)}  + 64 \norm{P(\theta^\star)}^{7/2} \varepsilon$. By a sufficiently small choice of $\varepsilon$, we may show the desired condition holds if  $\inf_{\theta\in B(\theta^\star, \varepsilon)} \norm{\calA(Z) - K(\theta)} \leq \alpha/2$ with high probability. In particular, we may bound $\inf_{\theta\in B(\theta^\star, \varepsilon)} \norm{\calA(Z) - K(\theta)} \leq \sqrt{{\norm{\calA(Z) - K(\Theta)}^2}} \leq \sqrt{\trace((\calA(Z) - K(\Theta)) \Sigma^{\calA(Z)}(\Theta)(\calA(Z) - K(\Theta))^\top \Psi(\Theta)}$. This quantity is precisely the excess cost of the algorithm applied to dataset $Z$ on system $\Theta$. Markov's inequality then implies that this quantity exceeds $\alpha^2/4$ with probabiliy at most $\frac{4\bfE\brac{C(\calA(Z), \Theta) - C(K(\Theta), \Theta)}}{\alpha^2}$. Under our assumption, this probability can be bounded as
\begin{align*}
    P(\calE^c) \leq \frac{1}{2 \alpha^2}  \trace\paren{ \paren{H(\theta^\star)}  (D_{\theta} \VEC K( \theta^\star))  \paren{N\mathsf{FI}(\theta^\star)}^{-1} (D_{\theta} \VEC K(\theta^\star))}.
\end{align*}
For $N$ sufficiently large, as given in the statement, this implies that $P(\calE) \geq \frac{1}{\sqrt{2}}$. Then, by a sufficiently small choice of $\varepsilon$ as given in the theorem statement, \eqref{eq: lb perturbation} implies that the minimax excess cost exceeds $\frac{1}{8}  \trace\paren{ \paren{H(\theta^\star)}  (D_{\theta} \VEC K( \theta^\star))  \paren{N\mathsf{FI}(\theta^\star)}^{-1} (D_{\theta} \VEC K(\theta^\star))}$, contradicting the assumption that the excess cost falls below this quantity. 

\end{proof}


% \input{CE_appendix_unused}
\input{CE_appendix}


% \input{RC_appendix_unused}
\input{RC_appendix}





\input{DR_appendix}


\input{Implementation_details}
