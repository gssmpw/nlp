\subsection{Controller Synthesis Approaches}
\label{s: methods}


All the synthesis approaches under consideration are model-based approaches which determine $\hat \theta$ from the dataset \eqref{eq: dataset} via the following least squares problem:
\begin{align}
    \label{eq: least squares}
    \hat \theta \triangleq \argmin_{\theta} \sum_{n=1}^N \sum_{t=1}^T \norm{X_{t+1}^n - \bmat{A(\theta) & B(\theta)} \bmat{X_t \\ U_t}}^2. 
\end{align}
The estimation error for $\hat \theta$ can be characterized using the Fisher information matrix:
\begin{align}
    \mathsf{FI}(\theta^\star) \triangleq \mathbf{E}_{\theta^\star}^{U_t \sim \calN(0, \Sigma_u)} \brac{\sum_{t=1}^T \bmat{X_t \\ U_t} \bmat{X_t \\ U_t}^\top } \otimes \Sigma_W^{-1}, \label{eq:FI}
\end{align}
see, e.g. \citet{lee2024active}. Since this quantity depends on the unknown parameter, we define the estimate
\begin{align}
     \label{eq: fisher estimate}
    \hat{\mathsf{FI}} \triangleq \frac{1}{N}\sum_{n=1}^N \sum_{t=1}^T \bmat{X_t^n \\ U_t^n } \bmat{X_t^n \\ U_t^n}^\top \otimes \Sigma_w^{-1},
\end{align}
which quantifies the uncertainty of the estimation procedure. 

We consider three approaches to control synthesis using the estimates $\hat\theta$ and $\hat{\mathsf{FI}}$:
\begin{itemize}[noitemsep, nolistsep, leftmargin=*]
    \item \textbf{Certainty Equivalence (CE)} uses the estimate $\hat{\theta}$ to minimize the control objective (\ref{eq: lqr cost}) by treating the estimate as though it were ground truth: 
    %\[
        $K_{\mathsf{CE}}(\hat \theta) = \argmin_{K} C(K,\hat\theta).$
    %\]
    \item \textbf{Robust Control (RC)} constructs a high confidence ellipsoid around the nominal estimate $\hat\theta$ using the estimated fisher information matrix $\hat{\mathsf{FI}}$ as
    \begin{align}
        \label{eq: confidence ellipsoid}
        G = \curly{\theta: (\theta-\hat\theta)^\top (N\hat {\mathsf{FI}}) (\theta- \hat \theta) \leq 16 (d_{\theta} +  \log(2/\delta))}.
    \end{align}
    Such a set can be shown to contain the true parameter $\theta^\star$ with probability at least $1-\delta$ as long as the number of experiments, $N$, is sufficiently large (see \Cref{s: id bound proof}). 
    Robust control then uses the confidence ellipsoid to determine a controller that minimizes the worst case  value of the control objective over all members of the confidence set as
    \begin{align*}
        {K}_{\mathsf{RC}}(G) = \argmin_{K}\sup_{\theta\in G}(C(K, {\theta}) - C(K(\theta), {\theta})).
    \end{align*}
    This formulation is nonstandard, as the controller minimizes the worst-case suboptimality gap, rather than the worst case cost \citep{gevers2005identification}. However, it simplifies the analysis. 
    \item \textbf{Domain Randomization (DR)} constructs a sampling distribution $\calD$ using the least squares estimate $\hat \theta$ and the estimated Fisher Information $\hat{\mathsf{FI}}$. It then synthesizes a controller by minimizing the average control cost as in \eqref{eq: domain randomization}. By ensuring good performance on average over a sampling distribution, domain randomization serves as a middle ground between certainty equivalence and robust control. In particular, it can be interpreted as enforcing a high probability robust stability constraint over the sampling region.\footnote{If the controller is not stabilizing for a subset of systems with nonzero mass, the cost will be infinite.} Careful choice of the sampling distribution is therefore critical for downstream performance. Our analysis informs this choice. % \Bruce{Expand on domain randomization}
\end{itemize}

The goal of this paper is to study the sample efficiency of these three approaches. In particular, we consider upper bounds on the gap $C(\hat K, \theta^\star)- C(K(\theta^\star), \theta^\star)$, where $\hat K$ is a controller synthesized with CE, RC, or DR. We express these bounds in terms of system-theoretic quantities, and the number of experiments collected from the system. Doing so provides an indication of the types of systems on which these methods perform well. We focus our attention on two key quantities: the burn-in time required to ensure finite bounds, and the asymptotic rate of decay in these bounds. 


