\section{Discussion}
%This work opens the door for several interesting followup directions.

\paragraph{Algorithmic considerations:}
Our numerical experiments use a gradient-based algorithm for DR, detailed in \Cref{alg: dr lqr}. This algorithm builds on the scenario approach of \citet{vidyasagar2001randomized} and the policy gradient method introduced for the linear quadratic regulator by \citet{fazel2018global}. The algorithm initially samples a number of scenarios from the distribution. At each iteration, the gradient update is performed on the cost summed over all scenarios. Since the control cost gradient becomes infinite if the current iterate does not stabilize a system, we incorporate only the gradients for system which the current iterate stabilizes. While we lack a formal convergence guarantee for this algorithm, numerical experiments indicate that it converges with a sufficiently small step size.
\begin{algorithm}[t]
 \caption{Domain Randomized Policy-Gradient for the Linear Quadratic Regulator} 
 \label{alg: dr lqr}
\begin{algorithmic}[1]
\vspace{-2pt}
\State \textbf{Input: } Randomization distribution $\calD$, estimate $\hat \theta$, stepsize $\eta$, $\#$ iterations $M$, $\#$ scenarios $N$
\State \textbf{Initialize: } $\hat K_1 \gets K_{CE}(\hat \theta)$,
\State \textbf{Sample: }
 $N$ scenarios $\theta_1, \dots, \theta_N \sim \calD$% \For{$n=1,2, \dots, N$}
    \For{$i = 1, 2, \dots, M$}  $\backslash\backslash$ Gradient descent on stable scenarios 
    \State $\hat K_{i+1} = \hat K_i - \eta \sum_{j=1}^N \nabla C(\hat K_i, \theta_{j}) \mathbf{1}(\rho(A(\theta_{j}) + B(\theta_{j}) \hat K_i) <1)$
        % \Else {$\,\hat K_{i+1} = \hat K_i$}
        % %\State $\theta_{\mathsf{sample}} \gets$ Sample from $\mathcal{D}$
        % \If{$\rho(A(\theta_{\mathsf{sample}}) + B(\theta_{\mathsf{sample}}) \hat K_i) <1$} $\hat K_{i+1} = \hat K_i - \frac{\eta}{\sqrt{i}}\nabla C(\hat K_i, \theta_{\mathsf{sample}})$
        % \Else {$\,\hat K_{i+1} = \hat K_i$}
        % \EndIf
    \EndFor
    % \State $\hat K_{n+1} \gets K_{i_{fin}}$
\State \textbf{Return: } $\hat K_{M}$. 
% \EndFor
\end{algorithmic}
\vspace{-2pt}
\end{algorithm}
% \vspace{-2pt}

This algorithm raises numerous questions which may be fruitful directions for future work. 
\begin{itemize} [noitemsep,nolistsep,leftmargin=*]
    \item \textbf{Convergence analysis of \Cref{alg: dr lqr}}: Extending the convergence analysis of policy gradient methods for LQR by \citet{fazel2018global, hu2023toward} to the proposed algorithm could provide theoretical guarantees for convergence to the solution of \eqref{eq: domain randomization}. Such an analysis would offer strong evidence of the algorithm's applicability beyond the toy numerical example presented here.
    \item \textbf{Alternative choices of distribution:} We proposed sampling from a uniform distribution over the confidence ellipsoid \eqref{eq: confidence ellipsoid} to maximize the spread of the sampling distribution without sacrificing asymptotic efficiency (\Cref{thm: Domain Randomization Upper Bound}). Exploring alternative distributions, such as truncated normal distributions, could reveal differences in empirical performance. These alternatives might also enable refined analyses of \Cref{thm: Domain Randomization Upper Bound}, particularly with respect to burn-in time.
    \item  \textbf{Extension to nonlinear systems:}  \Cref{alg: dr lqr} can, in principle, be extended to nonlinear systems, provided that gradients of the control objective can be obtained via Monte Carlo sampling. The least-squares analysis for CE in \Cref{tab:sample_efficiency} also extends nonlinear systems \citep{lee2024active}, suggesting that the proposed domain randomization approach could also be effective for such systems. This may yield sample efficiency guarantees analogous to those studied here.
\end{itemize}

\paragraph{Theoretical extensions: } There are additionally numerous directions to tighten the analysis and generalize the setting. %Our bounds do not achieve the tightest possible burn-in times or higher order terms. More refined analysis is likely possible, and should be pursued.
\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
    \item \textbf{Improved burn-in time for domain randomization: }
    While this work empirically demonstrates that DR can stabilize the system even in the low-data regime, the burn-in time derived in \Cref{lem: domain randomization general} does not reflect this advantage, as it is larger than  that of CE in \Cref{thm: certainty equivalence bound}. The only known way to improve the burn-in time is by adopting a robustly stabilizable condition, which considers the worst case but results in a conservative upper bound on the excess cost, as shown in \Cref{thm: Robust Control upper bound}. A promising direction for future work is to tighten the analysis for burn-in requirements and higher order terms of DR while maintaining its asymptotic efficiency.
    \item \textbf{Robust control lower bound:} Our upper bounds feature a gap between the asymptotic convergence rate of RC and that of CE and DR. We conjecture that this gap is fundamental; however, a lower bound on the sample efficiency of RC would be required to formalize this.
    \item \textbf{Misspecification:}  This work has focused on the use of DR to address model uncertainty arising from variance in model fitting. Specifically, the assumption of the dynamics in \eqref{eq: linear system} imposes a realizability condition, ensuring that a suitably constructed distribution $\calD$ contains $\theta^\star$ in its support with high probability. However, a key explanation for the empirical success of DR in many robotics applications is its robustness to model misspecification \citep{tobin2017dr}. Investigating this theoretically represents a promising direction for future work.
\end{itemize}

