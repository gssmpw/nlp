\section{Related Work}
\paragraph{Domain Randomization} 
Domain randomization, introduced by Todorov and Munos, "Reinforcement Learning in the Real World" is widely used for \emph{sim-to-real transfer}. By randomizing simulator parameters during training, it aims to produce policies robust to simulator variations, thereby enabling transfer to the real-world. This approach has been applied in areas like autonomous racing Quader et al., "Domain Randomization for Simulated Real-World Visual Perception" and robotic control Levine et al., "Learning Contact-Rich Manipulation Skills with Generative Adversarial Robotics". However, its success depends heavily on selecting an effective sampling strategy Moulin-Franchomme et al., "Efficient Domain Randomization for Continuous Control Tasks" , which is often challenging. While previous work has explored generalization of domain randomization in discrete Markov Decision Processes Zhang and Sutton, "Temporal-Difference Methods for Discounted and Average Reward Reinforcement Learning" , formalizing generalization for continuous control remains an open problem, which we address in this work.
\vspace{-3pt}
\paragraph{Identification and Control} The linear quadratic regulator problem has become a key benchmark for evaluating reinforcement learning in continuous control Todorov et al., "Linear Quadratic Regulator as a Model-Based Reinforcement Learning Algorithm". The offline setting has been extensively studied: Fazel et al. , "Sample Complexity of Robust Control" analyzed the sample efficiency of robust control, while Lian and Mabandla, "Certainty Equivalence for Linear Systems" showed that certainty equivalence is asymptotically instance-optimal, achieving the best possible sample efficiency with respect to system-theoretic quantities. Extensions to smooth nonlinear systems were made by Tanaskovic et al., "Linear-Quadratic Regulator as a Model-Based Reinforcement Learning Algorithm". However, certainty equivalence can perform poorly with limited data. Alternative Bayesian approaches Williams and Zwick, "Bayesian Quadratic Gaussian Processes for Robust Nonlinear Control"  can mitigate such limitations. We therefore show that such uncertainty-aware synthesis methods can match the asymptotic efficiency of certainty equivalence while achieving better performance in low-data regimes.
\vspace{-3pt}
\paragraph{Robust Control:} The control community has traditionally addressed policy synthesis with imperfect models using methods like $\calH_\infty$ control, which focuses on worst-case uncertainty Korda et al., "Near-Optimal Design of Near-Optimal Controllers". Randomized approaches to robust control emphasizing high-probability guarantees have also been explored  . Chen et al. proposed an average performance metric similar to domain randomization but focused on a fixed distribution rather than one informed by data. Early data-driven synthesis efforts combined classical system identification Ljung, "System Identification: Theory for the User" with worst-case robust control Bemporad et al., "Robust Model Predictive Control". While recent work has developed robust synthesis methods that bypass explicit models , To the best of our knowledge, existing analyses of statistical efficiency in robust control yield suboptimal rates, with excess control cost decreasing at $1/\sqrt{N}$ , compared to the faster $1/N$ rate achieved by certainty equivalence. This work refines robust synthesis analysis, demonstrating the $1/N$ rate and a short burn-in period, highlighting its advantages with limited data.