\section{Gradient Estimators}\label{sec:grad_est}
\begin{proposition}[Expression of $\nabla \widehat{\mathcal{F}}(\omega)$ by implicit differentiation]\label{prop:est_2}
Under  \cref{assump:compact,assump:convexity_lin,assump:K_bounded,assump:reg_lin_lout}, for any $\omega\in{\mathbb{R}^d}$, the gradient $\nabla\widehat{\mathcal{F}}(\omega)$ of the discretized functional bilevel optimization problem~\eqref{eq:kbo_app} is given by:
\begin{equation*}
    \nabla\widehat{\mathcal{F}}(\omega)=\frac{1}{m}\Dthree\mathbbm{1}_m-\frac{1}{m}\Dfour\M^{-1}\mathbf{u}\in\mathbb{R}^d,
\end{equation*}
where $\K$ and $\Kbar$ are the Gram matrices in $\mathbb{R}^{n\times n}$ and $\mathbb{R}^{m\times n}$  with entries given by  $\K_{ij}\coloneqq K(x_i,x_j)$ and $\K_{ij}\coloneqq K(\tilde{x}_j,x_j)$, and $\M$, $\mathbf{u}$, $\Done$, $\Dtwo$, $\Dthree$ and $\Dfour$ are defined as:
\begin{align*}
\M&\coloneqq\K\Dtwo+n\lambda\mathbbm{1}_{n\times n}{\in\mathbb{R}^{n\times n}},
\qquad &\mathbf{u} &\coloneqq \Kbar^\top\Done{\in\mathbb{R}^n},\\
	\Done &\coloneqq\parens{\partial_v\ell_{out}\parens{\omega,\hat{h}_\omega(\tilde{x}_j),\tilde{y}_j}}_{1\leq j\leq m}\in\mathbb{R}^m,\quad &
\Dthree &\coloneqq\parens{\partial_\omega \ell_{out}(\omega, \hat{h}_\omega(\tilde{x}_j), \tilde{y}_j)}_{1\leq j\leq m}\in\mathbb{R}^{d\times m},\\
\Dtwo &\coloneqq\diag\parens{\parens{\partial_v^2\ell_{in}(\omega,\hat{h}_\omega(x_i),y_i)}_{1\leq i\leq n}}\in\mathbb{R}^{n\times n},\qquad & 
\Dfour &\coloneqq\parens{\partial_{\omega,v}^2\ell_{in}(\omega,\hat{h}_\omega(x_i),y_i)}_{1\leq i\leq n}\in\mathbb{R}^{d\times n}.
\end{align*}
\end{proposition}
\begin{proof}
{Let $\omega\in\mathbb{R}^d$.} Recall the expression of $\widehat{\mathcal{F}}(\omega)$:
\begin{align*}
	&\widehat{\mathcal{F}}(\omega) \coloneqq \frac{1}{m}\sum_{j=1}^m \ell_{out}\parens{\omega, \hat{h}_{\omega}(\tilde{x}_j),\tilde{y}_j}\\
	\text{s.t.}\quad&\hat{h}_{\omega}=\argmin_{h\in \mathcal{H}} \widehat{L}_{in}(\omega,h)\coloneqq\frac{1}{n}\sum_{i=1}^n\ell_{in}\parens{\omega, h(x_i), y_i} +\frac{\lambda}{2}\Verts{h}_{\mathcal{H}}^2. 
\end{align*}
By the representer theorem, it is easy to see that $\hat{h}_{\omega}$ must be a linear combination of $K(x_1,\cdot),\ldots,K(x_n,\cdot)$:
\begin{equation}\label{eq:h}
	\hat{h}_{\omega} = \sum_{i=1}^n (\hat{\gamma}_{\omega})_i K(x_i,\cdot).
\end{equation}
Hence, finding $\hat{h}_{\omega}$ amounts to minimizing $\widehat{L}_{in}(\omega,h)$ over the span of $(K(x_1,\cdot),\ldots, K(x_n,\cdot))$, \textit{i.e.}, over functions $h^{\gamma}$ of the form $h^{\gamma} = \sum_{i=1}^n \gamma_i K(x_i,\cdot)$ for $\gamma\in \mathbb{R}^n$. Restricting the objective to such functions results in the following inner optimization problem which is finite-dimensional:
\begin{align*}
	\hat{\gamma}_{\omega}\coloneqq \argmin_{\gamma\in \mathbb{R}^n} \frac{1}{n}\sum_{i=1}^n \ell_{in}\parens{\omega, (\K \gamma)_i, y_i }+\frac{\lambda}{2} \gamma^{\top}\K \gamma ,
\end{align*} 
where  we used that $(h^{\gamma}(x_i))_{1\leq i\leq n} = \K\gamma$ and $\Verts{h^{\gamma}}^2_{\mathcal{H}} = \gamma^{\top}\K \gamma$. 
Similarly, using that $(h^{\gamma}(\tilde{x}_j))_{1\leq j\leq m} = \Kbar\gamma$, we can express $\widehat{\mathcal{F}}(\omega)$ as follows:
\begin{align*}
	\widehat{\mathcal{F}}(\omega) = \frac{1}{m}\sum_{j=1}^m \ell_{out}\parens{\omega, (\Kbar \hat{\gamma}_{\omega})_j,\tilde{y}_j}.
\end{align*}
Differentiating the above expression w.r.t. $\omega$ and applying the chain rule result in:
\begin{align*}
	\nabla \widehat{\mathcal{F}}(\omega) = \frac{1}{m}\sum_{j=1}^m \partial_{\omega}\ell_{out}\parens{\omega, (\Kbar \hat{\gamma}_{\omega})_j,\tilde{y}_j}  + \frac{1}{m}\sum_{j=1}^m  (\partial_{\omega}\hat{\gamma}_{\omega}\Kbar^{\top})_j \partial_{v}\ell_{out} \parens{\omega, (\Kbar \hat{\gamma}_{\omega})_j,\tilde{y}_j},
\end{align*}
where $\partial_{\omega}\hat{\gamma}_{\omega}$ denotes the Jacobian of $\hat{\gamma}_{\omega}$. We can further express the above equation in matrix form to get:
\begin{align}\label{eq:vector_form_gradient}
	\nabla \widehat{\mathcal{F}}(\omega) = \frac{1}{m}\Dthree\mathbbm{1}_m+\frac{1}{m}\partial_{\omega}\hat{\gamma}_{\omega} \Kbar^{\top}\Done.
\end{align}
	Moreover, an application of the  implicit function theorem\footnote{In the case where the matrix $\K$ is non-invertible, one needs to restrict $\gamma$ to the orthogonal complement of the null space of $\K$. Such a restriction is valid since the resulting solution $\hat{h}_{\omega}$ will not depend on the component belonging to the null space of $\K$. } allows to directly express the Jacobian $\partial_{\omega}\hat{\gamma}_{\omega}$ as a solution of the following linear system obtained by differentiating the optimality condition  for $\hat{\gamma}_{\omega}$ w.r.t. $\omega$:
\begin{equation*}
    \Dfour\K+\left(\partial_\omega\hat{\gammabf}_\omega\right)\underbrace{\parens{\K\Dtwo+n\lambda\mathbbm{1}_{n\times n} }}_{\M}\K=0.
\end{equation*}
A solution of the form $\partial_{\omega}\hat{\gamma}_{\omega} = -  \Dfour\M^{-1}$ always exists by invertibility of the matrix $\M$. The result follows after replacing $\partial_{\omega}\hat{\gamma}_{\omega}$ by $-\Dfour\M^{-1}$ in \cref{eq:vector_form_gradient}. 
\end{proof}

\begin{lemma}[Estimator of the total functional gradient]\label{prop:est_1}
{Let $\omega\in\mathbb{R}^d$.} Consider the following functional estimator:
\begin{align*}
\widehat{\nabla\mathcal{F}}(\omega)=&\frac{1}{m}\sum_{j=1}^m\partial_\omega\ell_{out}(\omega, \hat{h}_\omega(\tilde{x}_j),\tilde{y}_j)+\frac{1}{n}\sum_{i=1}^n\partial_{\omega,v}^2\ell_{in}(\omega, \hat{h}_\omega(x_i),y_i)\hat{a}_\omega(x_i).
\end{align*}
Then, under  \cref{assump:compact,assump:convexity_lin,assump:K_bounded,assump:reg_lin_lout},  $\widehat{\nabla\mathcal{F}}(\omega)$ admits the following expression:
\begin{equation*}
    \widehat{\nabla\mathcal{F}}(\omega)=\frac{1}{m}\Dthree\mathbbm{1}_m+\frac{1}{n}\Dfour
    \begin{bmatrix}
    	\K & \mathbf{u}
    \end{bmatrix}
    \begin{bmatrix}
    	\hat{\alphabf}_\omega\\
    	\hat{\beta}_\omega
    \end{bmatrix}
    \in\mathbb{R}^d,
\end{equation*}
where $\Done, \Dtwo, \Dthree, \Dfour$ are the same matrices given in \cref{prop:est_2}, while, $\hat{\alphabf}_\omega\in\mathbb{R}^n$ and $\hat{\beta}_\omega\in\mathbb{R}$ are solutions to the linear system:
\begin{equation}\label{eq:linear_system_adjoint}
\begin{bmatrix}
    \M \K & \M \mathbf{u}\\
    \mathbf{u}^{\top}\M & p
\end{bmatrix}\begin{bmatrix}
        \hat{\alphabf}_\omega\\
        \hat{\beta}_\omega
    \end{bmatrix}=-\frac{n}{m}\begin{bmatrix}
        \mathbf{u}\\
 	v
    \end{bmatrix},
\end{equation}
where the vector $\mathbf{u}$  and  matrix $\M$ are the same as in \cref{prop:est_2}, while $p$ and $v$ are non-negative scalars. 

\end{lemma}


\begin{proof}
{Let $\omega\in\mathbb{R}^d$.} We start by providing an expression of $\hat{a}_{\omega}$ as a linear combination of the kernel evaluated at the inner training points $x_i$, \textit{i.e.}, $K(x_i,\cdot)$, and some element  $\xi\in \mathcal{H}$ that we will characterize shortly. From it, we will obtain the expression of $\widehat{\nabla\mathcal{F}}(\omega)$. 

{\bf Expression of $\hat{a}_{\omega}$.}
Recall that $\hat{a}_{\omega}$ is the unique minimizer of $\widehat{L}_{adj}$ in \cref{eq:l_adj}, which admits{, for any $a\in\mathcal{H}$,} the following  simple expression by the reproducing property:
 \begin{equation*}    \widehat{L}_{adj}(\omega, a)=\frac{1}{2n}\sum_{i=1}^n\partial_v^2 \ell_{in}(\omega, \hat{h}_\omega(x_i), y_i)a^2(x_i)+\frac{1}{m}\left\langle a, \overbrace{\sum_{j=1}^m\partial_v\ell_{out}(\omega, \hat{h}_\omega(\tilde{x}_j), \tilde{y}_j)K(\tilde{x}_j,\cdot)}^{\xi}\right\rangle_{\mathcal{H}}+\frac{\lambda}{2}\|a\|_\mathcal{H}^2.
\end{equation*}
 Hence, by application of the representer theorem, it follows that $\hat{a}_{\omega}$ admits an expression of the form:
 \begin{align}\label{eq:a}
 	\hat{a}_{\omega} = \sum_{i=1}^n (\hat{\alphabf}_{\omega})_i K(x_i,\cdot) + \hat{\beta}_{\omega}{\xi}. 
 \end{align}
Therefore, it is possible to recover $\hat{a}_{\omega}$ by minimizing $a\mapsto L_{adj}(\omega,a)$ over the span of $(\xi, K(x_1,\cdot),\ldots,K(x_n,\cdot))$. Hence, to find the optimal coefficients $\hat{\alphabf}_{\omega}\coloneqq ((\hat{\alphabf}_{\omega})_{i})_{1\leq i \leq n}$ and  $\hat{\beta}_{\omega}$ we first need to express the objective $L_{adj}$ in terms of the coefficients $\alphabf \in \mathbb{R}^n$ and $\beta\in \mathbb{R}$ for a given $a^{\alphabf,\beta}\in \mathcal{H}$ of the form $a^{\alphabf,\beta} = \sum_{i=1}^n (\alphabf)_i K(x_i,\cdot) + \beta \xi$. To this end, note that the vector $(\xi(x_1),\ldots,\xi(x_n))$ is exactly equal to $\mathbf{u}= \Kbar^\top\Done$ as defined in \cref{prop:est_2}. Moreover, using the reproducing property, we directly have:
\begin{align*}
	(a^{\alphabf,\beta}(x_i))_{1\leq i\leq n} = \begin{bmatrix}
        \K & 
        \mathbf{u}
    \end{bmatrix}
 \begin{bmatrix}
        \alphabf\\
        \beta
    \end{bmatrix},\qquad \langle a^{\alphabf,\beta},\xi\rangle_{\mathcal{H}} = 
    \begin{bmatrix}
        \mathbf{u}^{\top} & 
        \Verts{\xi}^2_\mathcal{H}
    \end{bmatrix}
    \begin{bmatrix}
        \alphabf\\
        \beta
    \end{bmatrix},\qquad \Verts{a^{\alphabf,\beta}}^2_{\mathcal{H}} = 
    \begin{bmatrix}
        \alphabf^{\top} &
        \beta
    \end{bmatrix}\begin{bmatrix}
        \K & \mathbf{u}\\
        \mathbf{u}^{\top} & \Verts{\xi}^2_\mathcal{H}
    \end{bmatrix}\begin{bmatrix}
        \alphabf\\
        \beta
    \end{bmatrix}. 
\end{align*}
We can therefore express the objective $\widehat{L}_{adj}$ as follows:
\begin{align*}
	\widehat{L}_{adj}(\omega, a^{\alphabf,\beta})= \frac{1}{2n} \begin{bmatrix}
        \alphabf^{\top} &
        \beta
    \end{bmatrix}
    \begin{bmatrix}
        \K \\
        \mathbf{u}^{\top}
    \end{bmatrix}
    \Dtwo\begin{bmatrix}
        \K & 
        \mathbf{u}
    \end{bmatrix}
 \begin{bmatrix}
        \alphabf\\
        \beta
    \end{bmatrix} + \frac{1}{m}\begin{bmatrix}
        \mathbf{u}^{\top} & 
        \Verts{\xi}^2_\mathcal{H}
    \end{bmatrix}
    \begin{bmatrix}
        \alphabf\\
        \beta
    \end{bmatrix} + \frac{\lambda}{2}\begin{bmatrix}
        \alphabf^{\top} &
        \beta
    \end{bmatrix}\begin{bmatrix}
        \K & \mathbf{u}\\
        \mathbf{u}^{\top} & \Verts{\xi}^2_\mathcal{H}
    \end{bmatrix}\begin{bmatrix}
        \alphabf\\
        \beta
    \end{bmatrix}.
\end{align*}
Hence, the optimal coefficients $\hat{\alphabf}_{\omega}\coloneqq ((\hat{\alphabf}_{\omega})_{i})_{1\leq i \leq n}$ and  $\hat{\beta}_{\omega}$  are those minimizing the above quadratic form and are characterized by the following optimality condition:
\begin{equation*}
\begin{bmatrix}
    \overbrace{(\K\Dtwo + n\lambda\mathbbm{1}_{n\times n})}^{\M}\K & \overbrace{(\K\Dtwo + n\lambda\mathbbm{1}_{n\times n})}^{\M}\mathbf{u}\\
    \mathbf{u}^{\top}\underbrace{(\K\Dtwo + n\lambda\mathbbm{1}_{n\times n})}_{\M} &  \underbrace{\mathbf{u}^{\top}\Dtwo\mathbf{u} + n\lambda \Verts{\xi}^{2}_{\mathcal{H}}}_{p\geq 0}
\end{bmatrix}\begin{bmatrix}
        \hat{\alphabf}_\omega\\
        \hat{\beta}_\omega
    \end{bmatrix}= - \frac{n}{m}\begin{bmatrix}
        \mathbf{u}\\
 	\underbrace{\Verts{\xi}^2_{\mathcal{H}}}_{v\geq 0}
    \end{bmatrix}.
\end{equation*}
{\bf Expression of $\widehat{\nabla\mathcal{F}}(\omega)$.}
The result follows directly after expressing $\widehat{\nabla\mathcal{F}}(\omega)$ in vector form using the notations $\Dthree$ and $\Dfour$ from \cref{prop:est_2} and recalling that $(\hat{a}_{\omega}(x_i))_{1\leq i\leq n} = \begin{bmatrix}
        \K & 
        \mathbf{u}
    \end{bmatrix}\begin{bmatrix}
        \hat{\alphabf}_{\omega}\\
        \hat{\beta}_{\omega}
    \end{bmatrix}$.
\end{proof}


\begin{proof}[Proof of \cref{prop:equivalenceEstimates}]
	{Let $\omega\in{\mathbb{R}^d}$.} Define  
\begin{align*}
\widehat{\nabla\mathcal{F}}(\omega)=&\frac{1}{m}\sum_{j=1}^m\partial_\omega\ell_{out}(\omega, \hat{h}_\omega(\tilde{x}_j),\tilde{y}_j)+\frac{1}{n}\sum_{i=1}^n\partial_{\omega,v}^2\ell_{in}(\omega, \hat{h}_\omega(x_i),y_i)\hat{a}_\omega(x_i),
\end{align*}	
where $\hat{h}_{\omega}$ and $\hat{a}_{\omega}$ are given by \cref{eq:h,eq:a}.  We will show that $\widehat{\nabla\mathcal{F}}(\omega) = \nabla\widehat{\mathcal{F}}(\omega)$.  By \cref{prop:est_1,prop:est_2}, we know that $\nabla\widehat{\mathcal{F}}(\omega)$ and $\widehat{\nabla\mathcal{F}}(\omega)$  admit the following expressions:
\begin{align*}
\nabla\widehat{\mathcal{F}}(\omega)&=\frac{1}{m}\Dthree\mathbbm{1}_m-\frac{1}{m}\Dfour\M^{-1}\mathbf{u}\\
 \widehat{\nabla\mathcal{F}}(\omega)&=\frac{1}{m}\Dthree\mathbbm{1}_m+\frac{1}{n}\Dfour \begin{bmatrix}
    	\K & \mathbf{u}
    \end{bmatrix}
    \begin{bmatrix}
    	\hat{\alphabf}_\omega\\
    	\hat{\beta}_\omega
    \end{bmatrix}.
\end{align*}
Taking the difference of the two estimators yields:
\begin{align*}
	 \widehat{\nabla\mathcal{F}}(\omega)- \nabla\widehat{\mathcal{F}}(\omega) &= \frac{1}{m}\Dfour\parens{\M^{-1}\mathbf{u} +  \frac{m}{n}\begin{bmatrix}
    	\K & \mathbf{u}
    \end{bmatrix}
    \begin{bmatrix}
    	\hat{\alphabf}_\omega\\
    	\hat{\beta}_\omega
    \end{bmatrix}} \\
	  &= \frac{1}{m}\Dfour\M^{-1}\underbrace{\parens{\mathbf{u} + \frac{m}{n}\M \begin{bmatrix}
    	\K & \mathbf{u}
    \end{bmatrix}
    \begin{bmatrix}
    	\hat{\alphabf}_\omega\\
    	\hat{\beta}_\omega
    \end{bmatrix}
	  }}_{=0},
\end{align*}
where the term $\mathbf{u} +\frac{m}{n}(\M\K\hat{\alphabf}_\omega+\hat{\beta}_\omega\M\mathbf{u})$ is equal to $0$ by definition of $\hat{\alphabf}_\omega$ and $\hat{\beta}_\omega$ as solutions of the linear system \eqref{eq:linear_system_adjoint} of \cref{prop:est_1}. 
\end{proof}