\section{Regularity and Differentiability Results}\label{sec_app:reg}
\subsection{Regularity of the objectives}\label{sec:reg_ob}
The following propositions establish differentiability of considered objectives. We defer their proof to \cref{sec:proof_diff_results}.
\begin{proposition}[{Differentiability of $L_{in}$ and $L_{out}$}]\label{prop:fre_diff_L}
	Under \cref{assump:compact,assump:reg_lin_lout,assump:K_bounded}, for any $(\omega,h)\in {\mathbb{R}^d}\times \mathcal{H}$, the functions $L_{in}$, $L_{out}$  admit finite values at $(\omega,h)$, are jointly differentiable in $(\omega,h)$, with gradient given by:
	\begin{align*}
		    \partial_\omega L_{out}(\omega, h)&=\mathbb{E}_{\mathbb{Q}}\left[\partial_\omega \ell_{out}(\omega, h(x), y)\right]\in\mathbb{R}^d, &\partial_h L_{out}(\omega, h)=&\mathbb{E}_{\mathbb{Q}}\left[\partial_v \ell_{out}(\omega, h(x), y)K(x,\cdot)\right]{\in\mathcal{H}}, \\
\partial_\omega L_{in}(\omega, h)&=\mathbb{E}_{\mathbb{P}}\left[\partial_\omega \ell_{in}(\omega, h(x), y)\right]\in\mathbb{R}^d, &\partial_h L_{in}(\omega, h)=&\mathbb{E}_{\mathbb{P}}\left[\partial_v \ell_{in}(\omega, h(x), y)K(x,\cdot)\right] + \lambda h{\in\mathcal{H}}.
	\end{align*}
	Similarly, the empirical estimates $\widehat{L}_{in}$ and $\widehat{L}_{out}$ admit finite values, and are differentiable with gradients admitting similar expressions as above with $\mathbb{P}$ and $\mathbb{Q}$ replaced by their empirical estimates $\hat{\mathbb{P}}_n$ and $\hat{\mathbb{Q}}_m$.
\end{proposition}

\begin{proposition}[{Differentiability of $\partial_h L_{in}$}]\label{prop:fre_diff_L_v}
	Under \cref{assump:compact,assump:reg_lin_lout,assump:K_bounded}, for any $(\omega,h)\in{\mathbb{R}^d}\times \mathcal{H}$, the functions $(\omega,h)\mapsto \partial_{h} L_{in}(\omega,h)$ is differentiable with partial derivatives given by: 
\begin{align*}
    \partial_{\omega, h}^2 L_{in}(\omega, h)&=\mathbb{E}_{\mathbb{P}}\left[\partial_{\omega, v}^2\ell_{in}(\omega, h(x), y)K(x,\cdot)\right]\in\mathcal{L}(\mathcal{H},\mathbb{R}^d),\\
  \partial_h^2 L_{in}(\omega, h)&=\mathbb{E}_{\mathbb{P}}\left[\partial_v^2 \ell_{in}(\omega, h(x), y)K(x,\cdot)\otimes K(x,\cdot)\right] + \lambda\Id_\mathcal{H}\in\mathcal{L}(\mathcal{H},\mathcal{H}).\label{eq:partial2_h_Lin}
\end{align*}
Moreover, {for any $\omega\in\mathbb{R}^d$ and $h\in\mathcal{H}$,} the operators $ \partial_{\omega, h}^2 L_{in}(\omega, h)$ and $ \partial_{h}^2 L_{in}(\omega, h)-\lambda \Id_{\mathcal{H}}$ are Hilbert-Schmidt, i.e., bounded operators with finite Hilbert-Schmidt norm. The same conclusions hold for the empirical estimate $(\omega,h)\mapsto \partial_{h}\widehat{L}_{in}(\omega,h)$ with partial derivatives admitting similar expressions as above with $\mathbb{P}$ replaced by its empirical estimate $\hat{\mathbb{P}}_n$.
\end{proposition}

\begin{proposition}[Strong convexity of the inner objective in its second variable and invertibility of the Hessians]\label{prop:strong_convexity_Lin}
Under \cref{assump:convexity_lin,assump:compact,assump:reg_lin_lout,assump:K_bounded}, $h\mapsto L_{in}(\omega, h)$ and $h\mapsto \widehat{L}_{in}(\omega, h)$ are $\lambda$-strongly convex for any $\omega\in{\mathbb{R}^d}$. Moreover, {for any $\omega\in\mathbb{R}^d$ and $h\in\mathcal{H}$,} the Hessian operators $\partial_{h}^2 L_{in}(\omega, h)$ and $\partial_{h}^2 \widehat{L}_{in}(\omega, h)$ are invertible with their operator norm bounded by $\frac{1}{\lambda}$.
\end{proposition}
\begin{proof}
{By \cref{assump:convexity_lin}}, we know that $v\mapsto \ell_{in}(\omega, v, y)$ is convex {for any $\omega\in\mathbb{R}^d$ and $y\in\mathcal{Y}$}. Moreover, by \cref{prop:fre_diff_L}, $(x,y)\mapsto \ell_{in}\parens{\omega, h(x),y}$ is integrable for any {$\omega\in\mathbb{R}^d$ and $h\in \mathcal{H}$}. 
Consequently, by integration, we directly deduce that $h\mapsto \mathbb{E}_{\mathbb{P}}\brackets{\ell_{in}\parens{\omega, h(x),y}}$ is convex for any $\omega\in {\mathbb{R}^d}$. Finally, $h \mapsto L_{in}(\omega, h) \coloneqq  \mathbb{E}_{\mathbb{P}}\brackets{\ell_{in}\parens{\omega, h(x),y}} + \frac{\lambda}{2} \Verts{h}_{\mathcal{H}}^2 $ must be $\lambda$-strongly convex{, for any $\omega\in\mathbb{R}^d$}, as a sum of a convex function and a $\lambda$-strongly convex function. Similarly, we deduce that $h\mapsto \widehat{L}_{in}(\omega,h)$ is $\lambda$-strongly convex{, for any $\omega\in\mathbb{R}^d$}. Invertibility follows from the expression of the Hessian operator in \cref{prop:fre_diff_L_v}  
\end{proof}


\subsection{Differentiability of the value function}
\begin{proposition}[Total functional gradient $\nabla\mathcal{F}$]\label{prop:tot_grad_int}
Assume \cref{assump:convexity_lin,assump:K_bounded,assump:reg_lin_lout} hold. For any $\omega\in{\mathbb{R}^d}$, the total functional gradient $\nabla\mathcal{F}(\omega)$ satisfies:
\begin{equation}\label{eq:tot_grad}
    \nabla\mathcal{F}(\omega)=\partial_\omega L_{out}(\omega, h^\star_\omega)+\partial_{\omega,h}^2 L_{in}(\omega, h^\star_\omega)a^\star_\omega\in{\mathbb{R}^d},
\end{equation}
where $a^\star_\omega$ 
is the unique minimizer of the following quadratic objective:
\begin{equation}\label{eq:ladj}
    L_{adj}(\omega,a)\coloneqq\frac{1}{2}\left\langle a, H_{\omega}a\right\rangle_\mathcal{H}+\left\langle a, d_\omega\right\rangle_\mathcal{H},\quad\text{for any }a\in\mathcal{H},
\end{equation}
with $H_{\omega}\coloneqq\partial_h^2 L_{in}(\omega, h^\star_\omega):\mathcal{H}\to\mathcal{H}$ being the Hessian operator and $d_\omega\coloneqq\partial_h L_{out}(\omega, h^\star_\omega)\in\mathcal{H}$.
\end{proposition}
\begin{proof}\label{proof:eq:tot_grad}

By applying \cref{prop:fre_diff_L,prop:strong_convexity_Lin},  
we know that $h\mapsto L_{in}(\omega,h)$ has finite values, is $\lambda$-strongly convex and Fr\'echet differentiable. Moreover, by \cref{prop:fre_diff_L_v}, $\partial_h L_{in}$ is Fr\'echet differentiable on ${\mathbb{R}^d}\times\mathcal{H}$, and, a fortiori, Hadamard differentiable. Therefore, by the functional implicit differentiation theorem  \citep{ioffe1979theory}, we deduce that the map $\omega\mapsto h_{\omega}^{\star}$ is uniquely defined and is Fr\'echet differentiable with Jacobian $\partial_\omega h^\star_\omega$ solving the following linear system for any $\omega \in{\mathbb{R}^d}$:
\begin{equation*}
    \partial_{\omega,h}^2 L_{in}(\omega, h^\star_\omega) + \partial_\omega h^\star_\omega\partial_h^2 L_{in}(\omega, h^\star_\omega)=0.
\end{equation*}
Using that $\partial_h^2 L_{in}(\omega, h^\star_\omega)$ is invertible by \cref{prop:strong_convexity_Lin}, we can express $\partial_\omega h^\star_\omega$ as:  
\begin{equation*}
    \partial_\omega h^\star_\omega=-\partial_{\omega,h}^2 L_{in}(\omega, h^\star_\omega)\left(\partial_h^2 L_{in}(\omega, h^\star_\omega)\right)^{-1}.
\end{equation*}
Furthermore, $L_{out}$ is jointly Fr\'echet differentiable by application of \cref{prop:fre_diff_L}, so that $\omega\mapsto\mathcal{F}(\omega)$ is also differentiable by composition of the functions {$(\omega,h)\mapsto L_{out}(\omega, h)$ and $\omega\mapsto (\omega,h^\star_\omega)$}. 
For a given $\omega\in {\mathbb{R}^d}$, the gradient of $\mathcal{F}$ is then given by the chain rule: 
\begin{equation}\label{eq:tot_grad_int}
    \nabla\mathcal{F}(\omega)=\partial_\omega L_{out}(\omega, h^\star_\omega)+\partial_\omega h^\star_\omega\partial_h L_{out}(\omega, h^\star_\omega).
\end{equation}
Substituting the expression of $\partial_\omega h^\star_\omega$ into \cref{eq:tot_grad_int} yields:
\begin{equation*}
    \nabla\mathcal{F}(\omega)=\partial_\omega L_{out}(\omega, h^\star_\omega)-\partial_{\omega,h}^2 L_{in}(\omega, h^\star_\omega)\left(\partial_h^2 L_{in}(\omega, h^\star_\omega)\right)^{-1}\partial_h L_{out}(\omega, h^\star_\omega).
\end{equation*}
To conclude, it suffices to notice that the function $a_\omega^{\star}$ appearing in \cref{eq:tot_grad} must be equal to $-H_{\omega}^{-1}d_{\omega}$. Indeed, $a_\omega^{\star}$ is defined as the minimizer of the quadratic objective $L_{adj}(\omega,a)$ in \cref{eq:ladj} which is strongly convex since the Hessian operator is lower-bounded by $\lambda \Id_{\mathcal{H}}$. Consequently, the minimizer $a_\omega^{\star}$ exists and is uniquely characterized by the optimality condition:
\begin{align*}
	H_{\omega}a_\omega^{\star}+d_{\omega}=0. 
\end{align*}
The above equation is a linear system in $\mathcal{H}$ whose solution is given by $a_\omega^{\star}\coloneqq-H_{\omega}^{-1}d_{\omega}$.
\end{proof}