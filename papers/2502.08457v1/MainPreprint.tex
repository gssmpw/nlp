%

\documentclass{article}

%
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage{enumitem} %
\usepackage{bbm}
\usepackage{mathrsfs}
\usepackage{tikz-cd}
\usepackage{automatic_resize}


%
%
%
%
\usepackage{hyperref}

%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%
%

%
\usepackage[accepted]{icml2025}

%
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{mathtools}
\usepackage{amsthm}

%
\usepackage[capitalize,noabbrev]{cleveref}
\newlist{assumplist}{enumerate}{1}
\setlist[assumplist]{label=(\textbf{\Alph*})}
\Crefname{assumplisti}{Assumption}{Assumptions}
\Crefname{assumption}{Assumption}{Assumptions}


\newcommand{\diam}{\operatorname{diam}}
\newcommand{\op}{\operatorname{op}}
\newcommand{\hs}{\operatorname{HS}}
\newcommand{\Id}{\operatorname{Id}}
\newcommand{\lip}{\operatorname{Lip}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\Span}{\operatorname{Span}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\alphabf}{\operatorname{\boldsymbol{\alpha}}}
\newcommand{\gammabf}{\operatorname{\boldsymbol{\gamma}}}
\newcommand{\K}{\operatorname{\mathbf{K}}}
\newcommand{\Kbar}{\operatorname{\mathbf{\overline{K}}}}
\newcommand{\Ktilde}{\operatorname{\mathbf{\widetilde{K}}}}
\newcommand{\diag}{\operatorname{\mathbf{diag}}}
\newcommand{\M}{\operatorname{\mathbf{M}}}
\newcommand{\Done}{\operatorname{\mathbf{D_v^{out}}}}
\newcommand{\Dtwo}{\operatorname{\mathbf{D_{v,v}^{in}}}}
\newcommand{\Dthree}{\operatorname{\mathbf{D_\omega^{out}}}}
\newcommand{\Dfour}{\operatorname{\mathbf{D_{\omega,v}^{in}}}}


\newcommand{\PP}{\mathbb{D}}
\newcommand{\LL}{{\bf L}}
\newcommand{\Dout}{\delta_\omega^{out}}
\newcommand{\Din}{\delta_\omega^{in}}
\newcommand{\Douth}{\partial_{h}\delta_\omega^{out}}
\newcommand{\Dinh}{\partial_{h}\delta_\omega^{in}}
\newcommand{\Dinw}{\partial_{\omega}\delta_\omega^{in}}
\newcommand{\Doutw}{\partial_{\omega}\delta_\omega^{out}}
\newcommand{\Dinhh}{\partial_{h}^2\delta_\omega^{in}}
\newcommand{\Dinwh}{\partial_{\omega,h}^2\delta_\omega^{in}}


\newcommand{\Eout}{E_\omega^{out}}
\newcommand{\Ein}{E_\omega^{in}}
\newcommand{\Eouth}{\partial_{h}E_\omega^{out}}
\newcommand{\Einh}{\partial_{h}E_\omega^{in}}
\newcommand{\Einw}{\partial_{\omega}E_\omega^{in}}
\newcommand{\Eoutw}{\partial_{\omega}E_\omega^{out}}
\newcommand{\Einhh}{\partial_{h}^2E_\omega^{in}}
\newcommand{\Einwh}{\partial_{\omega,h}^2E_\omega^{in}}
\newcommand{\lipout}{\operatorname{Lip}_{out}}
\newcommand{\lipin}{\operatorname{Lip}_{in}}



\newcommand{\Dgammastar}{\operatorname{\mathbf{D}_{\hat{\gammabf}_\omega}}}
\DeclareMathOperator*{\argmin}{arg\,min}

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%
%
%
\usepackage[textsize=tiny]{todonotes}
\newcommand{\ma}[1]{\textcolor{red}{[MA: #1]}}
\newcommand{\fe}[1]{\textcolor{blue}{[FE: #1]}}
\newcommand{\ep}[1]{\textcolor{green}{[EP: #1]}}
\newcommand{\sv}[1]{\textcolor{violet}{[SV: #1]}}


\newcommand{\mat}[1]{\textcolor{red}{#1}}
\newcommand{\fet}[1]{\textcolor{blue}{#1}}
\newcommand{\ept}[1]{\textcolor{green}{#1}}
\newcommand{\svt}[1]{\textcolor{violet}{#1}}
\newcommand{\edt}[1]{\textcolor{teal}{[[#1]]}}


%
%
\icmltitlerunning{Learning Theory for Kernel Bilevel Optimization}

\begin{document}

\twocolumn[
\icmltitle{Learning Theory for Kernel Bilevel Optimization}

%
%
%
%

%
%
%
%

%
%
%
%

\begin{icmlauthorlist}
\icmlauthor{Fares El Khoury}{inria}
\icmlauthor{Edouard Pauwels}{tse}
\icmlauthor{Samuel Vaiter}{cnrs}
\icmlauthor{Michael Arbel}{inria}
\end{icmlauthorlist}

\icmlaffiliation{inria}{Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France}
\icmlaffiliation{tse}{Toulouse School of Economics, Universit\'e Toulouse Capitole, 31080 Toulouse, France}
\icmlaffiliation{cnrs}{CNRS \& Universit\'e C\^ote dâ€™Azur, Laboratoire J. A. Dieudonn\'e, 06108 Nice, France}

\icmlcorrespondingauthor{Fares El Khoury}{fares.el-khoury@inria.fr}

%
%
%
%

\vskip 0.3in
]

%

%
%
%
%
%

\printAffiliationsAndNotice{}  %
%

\begin{abstract}
Bilevel optimization has emerged as a technique for addressing a wide range of machine learning problems that involve an outer objective implicitly determined by the minimizer of an inner problem. In this paper, we investigate the generalization properties for kernel bilevel optimization problems where the inner objective is optimized over a Reproducing Kernel Hilbert Space. This setting enables rich function approximation while providing a foundation for rigorous theoretical analysis. In this context, we establish novel generalization error bounds for the bilevel problem under finite-sample approximation. Our approach adopts a functional perspective, inspired by \cite{petrulionyte2024functional}, and leverages tools from empirical process theory and maximal inequalities for degenerate $U$-processes to derive uniform error bounds. These generalization error estimates allow to characterize the statistical accuracy of gradient-based methods applied to the empirical discretization of the bilevel problem.
\end{abstract}





\section{Introduction}
Optimization is a cornerstone of mathematical modeling, machine learning, and decision-making. 
Among the various frameworks, bilevel optimization stands out due to its hierarchical structure, where one optimization problem, called \emph{outer-level}, is constrained by the solution of another problem, called \emph{inner-level} \cite{candler1977multi}. 
This framework was first introduced in the context of economic game theory by \citet{vonstackelberg1934marktform} to model leader-follower interactions, where one agent's decisions depend on the optimal response of another. Over time, bilevel optimization has naturally found applications in a broad spectrum of machine learning fields, including hyper-parameter tuning \cite{larsen1996design,bengio2000gradient,franceschi2018bilevel}, meta-learning \cite{bertinetto2018metalearning,pham2021contextual}, inverse problems \cite{holler2018bilevel}, and reinforcement learning \cite{hong2023two,liu2023value}, making it a powerful and versatile tool in both theoretical and practical contexts.

{The success of bilevel optimization in machine learning naturally raises fundamental questions about the generalization properties of models learned through these procedures, as the number of data samples increases. Several existing works have studied the generalization and convergence of bilevel algorithms under the assumption that the inner-level problem is strongly convex and that its parameters lie in a finite-dimensional space. These include analyses of the convergence of stochastic bilevel optimization algorithms \cite{Arbel:2021a,Dagreou2022SABA,ghadimi2018approximation,Ji:2021a} and approaches based on algorithmic stability \cite{bao2021stability,zhang2024fine}. The strong convexity assumption ensures the uniqueness of the inner-level solution, a key property for stability and convergence analysis in bilevel optimization. 
Moreover, restricting the inner-level parameters to a finite-dimensional space, instead of possibly richer infinite-dimensional spaces, as in kernel methods, circumvents additional complexities, where the parameter's dimension may grow with the sample size. 
This dependence on sample size, in non-parametric methods, poses additional challenges, as solutions at different sample sizes are not directly comparable. In contrast, in the finite-dimensional setting, generalization bounds can be derived by quantifying the convergence of finite-sample estimates of the inner-level solution toward the \emph{population solution}, \textit{i.e.}, the solution obtained in the limit of infinite samples, within the same parameter space.}

{Albeit convenient from a theoretical perspective, having both strong convexity and finite-dimensionality drastically limits expressiveness of the models, effectively restricting them to linear functions. 
Going beyond linear models requires either relaxing the strong convexity assumption to accommodate more expressive models, such as deep neural networks \cite{Goodfellow-et-al-2016}, or considering non-parametric bilevel problems, where the inner-level variable lies in an expressive infinite-dimensional function space, such as a Reproducing Kernel Hilbert Space (RKHS) \cite{scholkopf2002learning}. 
Early works in bilevel optimization for machine learning followed the latter approach, developing bilevel methods for hyper-parameter selection in the context of kernel methods \cite{keerthi2006efficient,kunapuli2008classification}. 
These works leverage the \emph{representer theorem} \cite{scholkopf2001generalized} to transform the infinite-dimensional problem into a finite-dimensional one with dimension  depending on the sample size. However, they do not address how the sample size impacts generalization. 
Another line of research instead focuses on relaxing the strong convexity assumption, proposing new bilevel algorithms that can handle the loss of convexity \cite{arbel2022nonconvex,kwon2024on,shen2023penalty}. 
Nevertheless, non-convex bilevel optimization is a very hard problem in general \cite{liu2021towards,arbel2022nonconvex,bolte2024geometric}, and obtaining strong generalization guarantees in this setting remains out of reach due to the lack of precise control over the inner-level solution.} 
In all cases, learning theory for bilevel problems beyond the strongly convex parametric setting is essentially lacking.



In the present work, we take an initial step towards developing a learning theory that goes beyond the finite-dimensional setting. Specifically, we propose to study \emph{Kernel Bilevel Optimization} (KBO) problems, where the inner objective $L_{in}: \mathbb{R}^d \times \mathcal{H}\rightarrow \mathbb{R}$ finds an optimal inner solution $h_{\omega}^{\star}$ in a RKHS $\mathcal{H}$ for a given parameter $\omega$ in $\mathbb{R}^d$, while the outer objective $L_{out}:\mathbb{R}^d \times \mathcal{H}\rightarrow \mathbb{R}$ optimizes the parameter $\omega$ over a closed subset $\mathcal{C}$ of $\mathbb{R}^d$, given the inner solution $h_{\omega}^{\star}$: 
\begin{equation}\tag{KBO}\label{eq:kbo}
    \begin{aligned}
    \min_{\omega\in\mathcal{C}}\mathcal{F}(\omega)\coloneqq L_{out}(\omega,h^\star_\omega)\\
    \text{s.t.}\quad h^\star_\omega=\argmin_{h\in\mathcal{H}}L_{in}(\omega, h).
    \end{aligned}
\end{equation}
In particular, we focus on inner and outer level objectives that are expectation of point-wise losses, a common setting in learning theory. RKHS provides a natural framework to study learning theoretic arguments, and has been instrumental for many fruitful results in pattern recognition and machine learning. They allow to describe very expressive non-linear models with simple and stable algorithms, while enabling a rich statistical analysis and featuring adaptivity to the regularity of the population problem \cite{shawe2004kernel,scholkopf2002learning,hofmann2008kernel}. Our choice is also motivated by the relevance of kernel methods, even in the deep learning era. They remain competitive for some prediction problem, for example involving physics \cite{doumeche2024physics,letizia2022learning}. Furthermore, the mathematics of kernel methods are useful to describe the limiting behavior of  deep network training for very large models \cite{jacot2018neural,belkin2018understand}. Indeed, in this limit, the problem becomes (strongly) convex in an infinite-dimensional functional space, simplifying the difficulties of non-convex model parameterization, a major bottleneck in the analysis of such models. This point of view was leveraged by \citet{petrulionyte2024functional} who introduced functional bilevel optimization, and our setting is a special case for which the underlying function space is an RKHS.
From a practical perspective, our setting is amenable to first-order methods using implicit differentiation techniques \cite{griewank2003piggyback,bai2019deep,blondel2022efficient}.

{\bf Contributions.} We leverage empirical process theory and its extension to $U$-processes \cite{sherman1994maximal} to derive uniform generalization bounds for the value function of \eqref{eq:kbo}, quantifying the discrepancy between $\mathcal{F}$ and its plug-in estimator $\widehat{\mathcal{F}}$ both in terms of their values and their gradients. 
This control in terms of gradients is crucial to study first-order optimization methods as the value function $\mathcal{\widehat{F}}$ is typically not convex, and iterative solution methods find approximate critical points ($\nabla \widehat{\mathcal{F}}$ small). Our result relies on an equivalence that we establish between the gradient $\nabla \widehat{\mathcal{F}}$ and a plug-in statistical estimate for $\nabla \mathcal{F}$ that is more amenable to a statistical analysis. We then use our uniform bounds to provide generalization guarantees for gradient descent and projected gradient descent applied to $\nabla\widehat{\mathcal{F}}$. Under specific assumptions, we show convergence rates for sub-optimality measures, depending on sample sizes and the number of algorithmic iterations. This illustrates the relevance of our generalization bounds on one of simplest bilevel optimization algorithms. For large number of algorithmic steps, gradient algorithms on the empirical \eqref{eq:kbo} find approximate critical points of the population \eqref{eq:kbo} up to a statistical error which we control. 

\paragraph{Organization of the Paper.} We start in \Cref{sec:KBO} with a precise description of the \eqref{eq:kbo} problem, application examples, and implicit differentiation in an RKHS. In \Cref{sec:finite_samples}, we describe the empirical \eqref{eq:kbo} and state our first main result on the gradient of its value function. Our uniform generalization bound for \eqref{eq:kbo} is described in \Cref{sec:conv}, together with corollaries for gradient descent and projected gradient descent in \Cref{sec:gradientMethods}. Finally, in \cref{sec:proof_strategy}, we discuss the strategy of the proof for the main result.

\section{Kernel Bilevel Optimization }
\label{sec:KBO}
\subsection{Problem Formulation}
We consider the kernel bilevel optimization problem in \eqref{eq:kbo} with an RKHS $\mathcal{H}$, which is a space of real-valued functions defined on an input space $\mathcal{X}\subset\mathbb{R}^p$ and associated with a reproducing kernel $K:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$. 
We are interested, in particular, in (regularized) objectives expressed as expectations of point-wise loss functions, a formulation widely adopted in machine learning as it allows the loss functions to represent the average performance over some data distribution. 
Specifically, given two probability distributions $\mathbb{P}$ and $\mathbb{Q}$ supported on $\mathcal{X}\times \mathcal{Y}$ for some target space $\mathcal{Y}\subset \mathbb{R}^q$, we consider objectives of the form:  
\begin{align*}
\begin{split}
    L_{out}(\omega, h)&=\mathbb{E}_\mathbb{Q}\left[\ell_{out}(\omega, h(x), y)\right],\\
    L_{in}(\omega, h)&=\mathbb{E}_\mathbb{P}\left[\ell_{in}(\omega, h(x), y)\right]+\frac{\lambda}{2}\|h\|_\mathcal{H}^2,
\end{split}
\end{align*}
where $\ell_{in}\text{ and }\ell_{out}:\mathbb{R}^d\times\mathbb{R}\times\mathbb{R}^q\to\mathbb{R}$ represent the inner and outer point-wise loss functions, $\lambda>0$ is the regularization parameter which is fixed through this work, and $\|\cdot\|_\mathcal{H}$ denotes the norm in the RKHS $\mathcal{H}$. 
The regularization term in $L_{in}$ is often used in practice to prevent overfitting, by penalizing overly complex models. In our setting, it ensures strong convexity of $h\mapsto L_{in}(\omega,h)$, under mild assumptions on $\ell_{in}$, which will be critical to leverage functional implicit differentiation. 

{\bf Assumptions.} Through the paper, we will make the following four assumptions to derive generalization bounds while retaining a simple and modular presentation. 
\begin{assumplist}
\item\label{assump:K_bounded}(Boundedness of $K$). 
There exists a positive constant $\kappa>0$ such that $K(x,x)\leq\kappa$, for any $x\in\mathcal{X}$.

\item\label{assump:compact}
(Compactness of $\mathcal{Y}$). The subset $\mathcal{Y}$ of $\mathbb{R}^q$ is compact.
\item \label{assump:reg_lin_lout}(Regularity of $\ell_{in}$ and $\ell_{out}$).
The functions
$\ell_{in}$ and $\ell_{out}$ are of class $C^3$ jointly in their first two arguments $(\omega,v)$, and their derivatives are jointly continuous in $(\omega,v,y)$. 
\item\label{assump:convexity_lin}(Convexity of $\ell_{in}$ in its second argument). 
For any $(\omega,y)\in\mathbb{R}^d\times\mathbb{R}^q$, the map $v\mapsto \ell_{in}(\omega,v,y)$ is convex.
\end{assumplist}


\cref{assump:K_bounded} on the kernel $K$ holds for a wide class of kernels, such as the Gaussian and Mat\'ern kernels \cite{wendland2004scattered}, both of which fulfill this assumption with $\kappa=1$. 
\cref{assump:compact} on the set $\mathcal{Y}$ is a mild assumption that holds in many practical cases, such as classification, where the set $\mathcal{Y}$ is finite, or when $\mathcal{Y} = [0,1]^q$, where complex data, such as images, can be represented.  
\cref{assump:reg_lin_lout} on the point-wise objectives is a mild regularity assumption.  \cref{assump:K_bounded,assump:compact,assump:reg_lin_lout} can be relaxed at the expense of weaker yet more technical assumptions, such as finite moments  assumptions on $\mathbb{P}$ and $\mathbb{Q}$, and suitable polynomial growth of the kernel and some partial derivatives of $\ell_{in}$ and $\ell_{out}$. It is also sufficient to require that \Cref{assump:reg_lin_lout} holds on $\mathcal{U} \times \mathbb{R} \times \mathbb{R}^q$ where $\mathcal{U}$ is an open neighborhood of $\mathcal{C}$ and that \Cref{assump:convexity_lin} holds for any $\omega\in\mathcal{C}$ and $y\in\mathcal{Y}$.  We prefer to keep these stronger yet simpler assumptions for clarity.
Finally, \cref{assump:convexity_lin} is essential to ensure the existence and uniqueness of a smooth minimizer $h^\star_\omega$. It is a relatively weak assumption that was recently considered in \cite{petrulionyte2024functional} in the context of functional bilevel optimization and that holds in many cases of interest, as discussed in \cref{sec:examples}.

\subsection{Examples of KBO in Machine Learning}\label{sec:examples}
To illustrate the relevance of \eqref{eq:kbo}, we consider two examples that highlight its applicability.

{\bf Hyper-Parameter Selection under Distribution Shift.} In this application, the aim is to select the best hyper-parameters for a machine learning model, \textit{e.g.}, regularization parameters, while accounting for distribution shift between the training and testing data, \textit{i.e.}, when the training and test data distributions are different \cite{pedregosa2016hyperparameter,franceschi2018bilevel}. This can be viewed as an instance of \eqref{eq:kbo} when using models in an RKHS. At the inner-level, the model is trained to minimize the regularized training squared error loss, where the hyper-parameter $\omega>0$ denotes a weight for the data fitting term. At the outer-level, the task is to select the hyper-parameter $\omega$ that minimizes the modelâ€™s performance on the distribution-shifted test. Both inner and outer objectives can thus be formulated as:
\begin{align*}
    L_{out}(\omega, h^\star_\omega)&=\mathbb{E}_{x,y}\left[\verts{h^\star_\omega(x)-y}^2\right],\\
    L_{in}(\omega, h)&=\omega \mathbb{E}_{x,y}\left[\verts{h(x)-y}^2\right]+\frac{1}{2}\Verts{h}_\mathcal{H}^2.
\end{align*}
This formulation could be used for domain adaptation \cite{ben2006analysis} or domain generalization \cite{wang2022generalizing} to choose hyper-parameters that perform well on the distribution-shifted test data.

{\bf Instrumental Variable Regression} is a technique used to address endogeneity in statistical modeling by leveraging instruments to estimate causal relationships \cite{newey2003instrumental}. The goal here is to estimate a function $t\mapsto f_{\omega}(t)$ parameterized by a vector $\omega$, that satisfies $y=f_{\omega}(t)+\epsilon$, where $y\in\mathbb{R}$ is the observed outcome, $t$ is a treatment, and $\epsilon$ is the error term. 
The key issue is that $t$ is endogenous, which means that it is correlated with $\epsilon$, \textit{i.e.}, $\mathbb{E}[\epsilon\mid t]\neq 0$, making direct regression inconsistent.
Indeed, such correlation leads to biased estimates of $f_{\omega}(t)$ because the assumption of exogeneity, \textit{i.e.}, independence of $t$ and $\epsilon$, is violated. 
To resolve this, an instrumental variable $x$ that is uncorrelated with $\epsilon$, \textit{i.e.}, $\mathbb{E}[\epsilon\mid x]=0$, but correlated with $t$, can be used to recover the relationship between $y$ and $t$, without being directly affected by the bias introduced by $\epsilon$ using the two stages least squares regression \cite{singh2019kernel,meunier2024nonparametric}. As shown in \cite{petrulionyte2024functional}, this approach can be naturally expressed as a bilevel optimization problem of the form:
\begin{align*}
    L_{out}(\omega,h^\star_\omega)&=\mathbb{E}_{x,y}\left[\verts{h^\star_\omega(x)-y}^2\right],\\
    L_{in}(\omega,h)&=\mathbb{E}_{x,t}\left[\verts{h(x)-f_{\omega}(t)}^2\right]+\frac{\lambda}{2}\Verts{h}_\mathcal{H}^2,
\end{align*}
where $h$ can be chosen to be in an RKHS to allow flexibility in the estimation while retaining uniqueness of the solution $h_{\omega}^{\star}$,  a key property in bilevel optimization.  



\subsection{Implicit Differentiation in an RKHS}
A stationarity measure in \eqref{eq:kbo} is the gradient $\nabla\mathcal{F}(\omega)$ of the value function $\mathcal{F}$. 
Nonetheless, evaluating the gradient requires computing the Jacobian $\partial_\omega h^\star_\omega$, which can be viewed as a linear operator from $\mathcal{H}$ to $\mathbb{R}^d$. Indeed $h^\star_\omega$ depends implicitly on $\omega$. A key ingredient for computing the Jacobian $\partial_\omega h^\star_\omega$ is the implicit function theorem {\citep{ioffe1979theory}} which guarantees differentiability of the implicit function $\omega\mapsto h_{\omega}^{\star}$ and allows characterizing $\partial_\omega h^\star_\omega$ as the unique solution of a linear system of the form:
\begin{equation}\label{eq:impl_diff}
\partial_{\omega, h}^2 L_{in}(\omega, h^\star_\omega)+\partial_\omega h^\star_\omega \partial_h^2 L_{in}(\omega, h^\star_\omega)=0,    
\end{equation}
where $\partial_h^2 L_{in}(\omega, h^\star_\omega)$ is a linear operator from $\mathcal{H}$ to itself representing the partial Hessian of $L_{in}$ w.r.t. $h$, while $\partial_{\omega,h}^2 L_{in}(\omega, h^\star_\omega)$ is an operator from $\mathcal{H}$ to $\mathbb{R}^d$ representing the cross derivatives of $L_{in}$ w.r.t. to $\omega$ and $h$. Applying such result requires $h\mapsto L_{in}(\omega,h)$ to be Fr\'echet differentiable with  gradient map $(\omega,h)\mapsto\partial_{h}L_{in}(\omega,h)$ jointly Fr\'echet differentiable and invertible Hessian operator. All these properties are satisfied in our setting under \cref{assump:K_bounded,assump:compact,assump:convexity_lin,assump:reg_lin_lout} as shown in {\cref{prop:fre_diff_L,prop:fre_diff_L_v,prop:strong_convexity_Lin}} of {\cref{sec:reg_ob}}. Furthermore, when the outer objective $L_{out}$ is Fr\'echet differentiable, which is our case under our assumptions ({\cref{prop:fre_diff_L}} of {\cref{sec:reg_ob}}), then by composition with $\omega\mapsto (\omega,h_{\omega}^{\star})$,  the map $\omega\mapsto \mathcal{F}(\omega)$ must also be differentiable with gradient obtained using the chain rule:
\begin{align*}
	\nabla\mathcal{F}(\omega)= \partial_{\omega}L_{out}(\omega,h_{\omega}^{\star}) -\partial_{\omega}h_{\omega}^{\star}\partial_{h}L_{out}(\omega,h_{\omega}^{\star}).
\end{align*}
The above expression for the gradient is intractable as it involves abstract operators. 
In \cref{prop:func_gradient} below, we 
  derive an explicit expression for $\nabla\mathcal{F}(\omega)$ which exploits the particular structure of the objectives $L_{in}$ and $L_{out}$ as expectations of point-wise losses. 
\begin{proposition}[Expression of the total gradient]\label{prop:func_gradient}
Under \cref{assump:compact,assump:convexity_lin,assump:K_bounded,assump:reg_lin_lout}, $\mathcal{F}$ is differentiable on $\mathbb{R}^d$, with gradient $\nabla\mathcal{F}(\omega)$, for any $\omega\in \mathbb{R}^d$, given by:
\begin{equation}\label{eq:func_tot_gradient}
    \begin{aligned}
    \nabla\mathcal{F}(\omega)=&\mathbb{E}_\mathbb{Q}\left[\partial_\omega\ell_{out}(\omega, h^\star_\omega(x),y)\right]\\
    &+\mathbb{E}_\mathbb{P}\left[\partial_{\omega,v}^2\ell_{in}(\omega, h^\star_\omega(x),y)a^\star_\omega(x)\right],
    \end{aligned}
\end{equation}
where \emph{the adjoint function} $a^\star_\omega\in\mathcal{H}$ is the unique minimizer of a strongly-convex quadratic objective $a\mapsto L_{adj}(\omega,a)$ defined on $\mathcal{H}$ as:
\begin{align}\label{eq:adjoint_objective}
\begin{split}
    L_{adj}(\omega,& a)\coloneqq  \frac{1}{2}\mathbb{E}_{\mathbb{P}}\brackets{\partial_{v}^2 \ell_{in}\parens{\omega,h_{\omega}^{\star}(x),y} a(x)^2}\\
     &+ \mathbb{E}_{\mathbb{Q}}\brackets{\partial_{v}\ell_{out}\parens{\omega,h_{\omega}^{\star}(x),y}a(x)}+\frac{\lambda}{2}\Verts{a}_{\mathcal{H}}^2,
\end{split}
\end{align}
where $\partial_\omega\ell_{out}$ and $\partial_v\ell_{out}$ are the first-order partial derivatives of $\ell_{out}$ w.r.t. $\omega$ and $v$, while $\partial_{\omega,v}^2\ell_{in}$ and $\partial_{v}^2\ell_{in}$ denote the second-order partial derivatives of $\ell_{in}$ w.r.t. $\omega$ and $v$.
\end{proposition}
\cref{prop:func_gradient} is proved in {\cref{sec:reg_ob}} and relies essentially on proving Bochner's integrability {\citep[Definition 1, Chapter 2]{diestel1977vector}} of some suitable operators on $\mathcal{H}$, and then applying Lebesgue's dominated convergence theorem for Bochner's integral  {\citep[Theorem 3, Chapter 2]{diestel1977vector}} to exchange derivatives and expectations.
The expression in \Cref{prop:func_gradient} provides a natural way for approximating $\nabla\mathcal{F}(\omega)$ by estimating all expectations using finite sample averages, as we further discuss in {\cref{sec:finite_samples}}.

\section{Finite Sample Approximation of \eqref{eq:kbo}}\label{sec:finite_samples}
In this section, we consider an approximation to \eqref{eq:kbo} problem when only a finite number of i.i.d. samples $(x_i,y_i)_{1\leq i\leq n}$ and  $(\tilde{x}_j,\tilde{y}_j)_{1\leq j\leq m}$ from $\mathbb{P}$ and $\mathbb{Q}$ are available. This setting is ubiquitous in machine learning as it allows finding tractable approximate solutions to the original problem. 
As we are interested in approximately solving \eqref{eq:kbo} using gradient methods, our focus here is to derive estimators for both the value function $\mathcal{F}(\omega)$ and its  gradient $\nabla\mathcal{F}(\omega)$ for which the generalization properties will later be studied in  \cref{sec:conv}. 

In \cref{sec:plug-in_loss}, we follow a commonly used approach of first deriving a plug-in estimator $\widehat{\mathcal{F}}$ of the value function, then considering its gradient 
$\nabla\mathcal{\widehat{F}}(\omega)$ 
as an approximation to $\nabla \mathcal{F}(\omega)$. 
Then, in \cref{sec:plug_in_gradient}, we show that such approximation is equivalent to a second estimator, more amenable to a statistical analysis, obtained by directly computing a plug-in estimator of $\nabla\mathcal{F}$ based on its expression in \cref{eq:func_tot_gradient}. \cref{fig:commutative-diagram}  summarizes such equivalence.   
\begin{figure}[ht]
\centering
\begin{tikzcd}[row sep=0.8cm, column sep=3.5cm]
    \mathcal{F} \arrow[r, "\text{ \small Plug-in estimation}",] \arrow[d, "\text{\small $\nabla$: diff}"',] & \widehat{\mathcal{F}} \arrow[d, "\text{\small $\nabla$: diff}"] \\
    \nabla\mathcal{F} \arrow[r, "\text{\small Plug-in estimation}"'] & \nabla\widehat{\mathcal{F}} 
\end{tikzcd}
\caption{A commutative diagram illustrating that plug-in statistical estimation and differentiation can be interchanged for $\mathcal{F}$ and $\widehat{\mathcal{F}}$ resulting in a single gradient estimator. }
\label{fig:commutative-diagram}
\end{figure}


\subsection{Value Function: Plug-in Estimator and its Gradient  }\label{sec:plug-in_loss}
A natural approach for finding approximate solutions to \eqref{eq:kbo} is to consider an approximate problem obtained after replacing the objectives $L_{in}$ and $L_{out}$ by their empirical approximations $\widehat{L}_{in}$ and $\widehat{L}_{out}$:
\begin{align*}
%
    \begin{split}
    \widehat{L}_{out}\parens{\omega,h}&\coloneqq \frac{1}{m} \sum_{j=1}^m \ell_{out}(\omega, h(\tilde{x}_j), \tilde{y}_j) \\
    \widehat{L}_{in}\parens{\omega,h}&\coloneqq \frac{1}{n} \sum_{i=1}^n \ell_{in}(\omega, h(x_i), y_i)+\frac{\lambda}{2}\|h\|_\mathcal{H}^2.
        \end{split}
\end{align*}
A plug-in estimator $\omega\mapsto \widehat{\mathcal{F}}(\omega)$ is then obtained by first finding a solution $\hat{h}_{\omega}$ minimizing $h\mapsto \widehat{L}_{in}(\omega,h)$ that is meant to approximate the optimal inner solution $h_{\omega}^{\star}$, and subsequently plugging it into $\widehat{L}_{out}$. This procedure results in the following empirical version of \eqref{eq:kbo}:
\begin{align*}
    %
    \begin{split}
    &\min_{\omega\in\mathcal{C}}\widehat{\mathcal{F}}(\omega)\coloneqq \widehat{L}_{out}\parens{\omega,\hat{h}_{\omega}}\\
    &\text{s.t.}\quad \hat{h}_\omega=\argmin_{h\in\mathcal{H}}\widehat{L}_{in}\parens{\omega,h}.    \end{split}
\end{align*}
The inner problem still requires optimizing over a, potentially infinite-dimensional, RKHS. However, its finite sum structure allows equivalently expressing it as a finite-dimensional bilevel optimization, by application of the so called representer theorem {\cite{scholkopf2001generalized}}:
\begin{align}\label{eq:kbo_app}
    &\min_{\omega\in\mathcal{C}}\widehat{\mathcal{F}}(\omega)\coloneqq\frac{1}{m}\sum_{j=1}^m\ell_{out}(\omega, \parens{\Kbar\hat{\gammabf}_\omega}_j, \tilde{y}_j)\tag{$\widehat{\text{KBO}}$} \\
    &\text{s.t. }\hat{\gammabf}_\omega=\argmin_{\gammabf\in\mathbb{R}^n}\frac{1}{n}\sum_{i=1}^n\ell_{in}(\omega, \parens{\K\gammabf}_i, y_i)+\frac{\lambda}{2}\gammabf^\top\K\gammabf.    \nonumber
\end{align}
In the above equation, $\K\in\mathbb{R}^{n\times n}$ and $\Kbar\in\mathbb{R}^{m\times n}$ are matrices containing the  pairwise kernel similarities between the data points, \textit{i.e.}, $\K_{ij}\coloneqq K(x_i,x_j)$ and $\Kbar_{ij}\coloneqq K(\tilde{x}_i,x_j)$, while $\gamma$ is a parameter vector in $\mathbb{R}^n$ representing the inner-level variables. The optimal solution $\hat{\gamma}_{\omega}$ enables recovering the prediction function $\hat{h}_{\omega}$ by linearly combining kernel evaluations at inner-level samples, \textit{i.e.}, $\hat{h}_{\omega} = \sum_{i=1}^n (\hat{\gamma}_{\omega})_i K(x_i,\cdot)$.  

The formulation in \eqref{eq:kbo_app} allows deriving an expression for the gradient $\nabla \widehat{\mathcal{F}}(\omega)$ in terms of the Jacobian $\partial_{\omega} \hat{\gamma}_{\omega}$ by direct application of the chain rule. Unlike the Jacobian $\partial_{\omega}h_{\omega}^{\star}$ which requires solving an infinite-dimensional linear system given by \cref{eq:impl_diff}, the Jacobian of $\hat{\gamma}_{\omega}$ can be obtained as a solution of a finite-dimensional linear system by application of the implicit function theorem ({see {\cref{prop:est_2} of \cref{sec:grad_est}}}). 
Hence, \eqref{eq:kbo_app} falls into a class of optimization problems for which a rich body of literature have proposed practical and scalable algorithms, leveraging the expression of $\nabla\widehat{\mathcal{F}}(\omega)$ \citep{Ji:2021a,Arbel:2021a,Dagreou2022SABA}. Consequently, solving \eqref{eq:kbo_app} provides a 
practical way for approximating the solution to the original population problem \eqref{eq:kbo} as proposed in several prior works on bilevel  optimization involving kernel methods \cite{keerthi2006efficient,kunapuli2008classification}.


Despite its practical advantage, the above approach yields algorithms that are not directly amenable to a statistical analysis. The key challenge is to be able to control the approximation error between the true gradient $\nabla\mathcal{F}(\omega)$ and its approximation $\nabla\widehat{\mathcal{F}}(\omega)$ as the sample sizes $n$ and $m$ increase. 
Existing statistical analysis for bilevel optimization, such as \cite{bao2021stability,zhang2024fine}, considered objectives in the form of expectations/finite sums of point-wise losses, as we do here. 
However, they require both inner-level and outer-level parameters to belong to spaces of fixed dimensions that is independent of the sample sizes $n$ and $m$. That is because these parameters are expected to converge towards some fixed vectors as $n,m\rightarrow +\infty$. Unfortunately, these results are not applicable in our case since the inner-level parameter $\gamma$ has a dimension that increases with the sample size $n$ ($\gamma\in\mathbb{R}^n$) and is not expected to converge towards any well-defined object. Next, we provide an equivalent expression for $\nabla\widehat{\mathcal{F}}(\omega)$ that will be crucial in our statistical analysis in \cref{sec:conv}. 


\subsection{Plug-in Estimator of the Total Gradient}\label{sec:plug_in_gradient}
We consider now an, a priori, different approach for approximating the total gradient $\nabla\mathcal{F}(\omega)$ based on direct plug-in estimation 
from \cref{eq:func_tot_gradient} and show that it recovers the previously introduced estimator $\nabla\widehat{\mathcal{F}}(\omega)$. Such approach consists in replacing all expectations in \cref{eq:func_tot_gradient} by empirical averages, then replacing $h^{\star}_{\omega}$ and $a_{\omega}^{\star}$ by finite sample estimates $\hat{h}_{\omega}$ and $\hat{a}_{\omega}$:
\begin{equation}\label{eq:functionalGradientEstimate}
    \begin{aligned}
    \widehat{\nabla\mathcal{F}}(\omega)=&\frac{1}{m}\sum_{j=1}^m\partial_\omega\ell_{out}(\omega, \hat{h}_\omega(\tilde{x}_j),\tilde{y}_j)\\
    &+\frac{1}{n}\sum_{i=1}^n\partial_{\omega,v}^2\ell_{in}(\omega, \hat{h}_\omega(x_i),y_i)\hat{a}_\omega(x_i).
    \end{aligned}
\end{equation}
Just as in \cref{sec:plug-in_loss}, $h^{\star}_{\omega}$ can be estimated by $\hat{h}_{\omega}$, the minimizer of the empirical objective $h\mapsto  \widehat{L}_{in}(\omega,h)$. Similarly, $a^{\star}_{\omega}$ can be estimated by minimizing an empirical version $a\mapsto \widehat{L}_{adj}(\omega,a)$ of the adjoint objective $L_{adj}$ given in \cref{eq:adjoint_objective}:
\begin{equation}
\label{eq:l_adj}
\begin{aligned}
    \widehat{L}_{adj}(\omega, a)=\frac{1}{2n}\sum_{i=1}^n\partial_v^2 \ell_{in}(\omega, \hat{h}_\omega(x_i), y_i)a^2(x_i)\\
    +\frac{1}{m}\sum_{j=1}^m\partial_v\ell_{out}(\omega, \hat{h}_\omega(\tilde{x}_j), \tilde{y}_j)a(\tilde{x}_j)+\frac{\lambda}{2}\|a\|_\mathcal{H}^2.
\end{aligned}
\end{equation}
Both functions $\hat{h}_{\omega}$ and $\hat{a}_{\omega}$ can be expressed as linear combinations of kernel evaluations with some given parameters that increase with the sample size $n$, {(see \cref{prop:est_2} for $\hat{h}_{\omega}$ and \cref{prop:est_1} for $\hat{a}_{\omega}$, both in \cref{sec:grad_est})}. 
However, these parameters are never  required for expressing the plug-in estimator $\widehat{\nabla\mathcal{F}}(\omega)$ in \cref{eq:functionalGradientEstimate},  since only the function values of $\hat{h}_{\omega}$ and $\hat{a}_{\omega}$ are needed for computing it. This property is precisely what makes $\widehat{\nabla\mathcal{F}}(\omega)$ suitable for a statistical analysis, since its estimation error  depends on the errors of $\hat{h}_{\omega}$ and $\hat{a}_{\omega}$  which always belong to the same space $\mathcal{H}$, regardless of the sample size, and are expected to approach their population counter-parts. This is unlike the expression of $\nabla\widehat{\mathcal{F}}(\omega)$ obtained by implicit differentiation and which suggests controlling the behavior of the vector $\hat{\gamma}_{\omega}$.  

The next proposition demonstrates that, surprisingly, both estimators $\nabla\widehat{\mathcal{F}}(\omega)$ and $\widehat{\nabla\mathcal{F}}(\omega)$ are precisely equal.
\begin{proposition}\label{prop:equivalenceEstimates}
	Under \cref{assump:compact,assump:convexity_lin,assump:K_bounded,assump:reg_lin_lout}, the gradient $\nabla \mathcal{\widehat{F}}(\omega)$ of the plug-in estimator $\widehat{\mathcal{F}}(\omega)$ of $\mathcal{F}(\omega)$ defined in \eqref{eq:kbo_app} is equal to the plug-in estimator $\widehat{\nabla\mathcal{F}}(\omega)$ of the total gradient $\nabla\mathcal{F}(\omega)$ introduced in \cref{eq:functionalGradientEstimate}. 
\end{proposition}
\cref{prop:equivalenceEstimates} is proved in {\cref{sec:grad_est}} and relies on an application of the representer theorem {\cite{scholkopf2001generalized}} to provide explicit expressions for both estimators in terms of $\gamma_{\omega}$, kernel matrices $\K$ and $\Kbar$ and partial derivatives of the point-wise objectives $\ell_{in}$ and $\ell_{out}$. Both expressions are then shown to be equal using optimality conditions on the parameters defining  $\hat{a}_{\omega}$. 
The result in \cref{prop:equivalenceEstimates} precisely says that the operations of differentiation and plug-in estimation commute in the case of \eqref{eq:kbo}. Such a commutativity property does not necessarily hold anymore if one considers spaces other than an RKHS as discussed in \citep[Appendix~F]{petrulionyte2024functional}. Next, we leverage the expression of the plug-in estimator $\widehat{\nabla\mathcal{F}}(\omega)$ to provide generalization bounds. 


\section{Generalization Bounds for \eqref{eq:kbo}}\label{sec:conv}
In this section, we present the main result of the present work: a maximal inequality controlling how  both value function $\mathcal{F}$ and its gradient $\nabla\mathcal{F}$ are well approximated by their empirical counter-parts uniformly over a compact subset $\Omega$ of $\mathbb{R}^d$. In \cref{sec:main_result},  we state the main result and discuss its implications. Then we present the general proof strategy, in \cref{sec:sketch_proof}, and discuss possible alternative approaches.  

\subsection{Maximal Inequalities for \eqref{eq:kbo}}\label{sec:main_result}
The following theorem provides finite sample bounds on the uniform approximation errors on the objective and its gradient in expectation over both inner and upper-level samples.

\begin{theorem}[Maximal inequalities]
\label{th:generalizationBounds}
Fix any compact subset $\Omega$ of $\mathbb{R}^d$. Under \cref{assump:compact,assump:convexity_lin,assump:K_bounded,assump:reg_lin_lout}, the following maximal inequalities hold:
\begin{align*}
    \mathbb{E}\left[\sup_{\omega\in\Omega}\left|\mathcal{F}(\omega)-\widehat{\mathcal{F}}(\omega)\right|\right]&\leq C\parens{\frac{1}{\sqrt{m}}+\frac{1}{\sqrt{n}}},\\
    \mathbb{E}\left[\sup_{\omega\in\Omega}\left\|\nabla\mathcal{F}(\omega)-\widehat{\nabla\mathcal{F}}(\omega)\right\|\right]&\leq C\parens{\frac{1}{\sqrt{m}}+\frac{1}{\sqrt{n}}},
\end{align*}
where the expectation is taken over the finite samples and $C$ is a constant that depends only $\Omega$, dimension $d$, regularization parameter $\lambda$, $\kappa$ and local upper-bounds on  $\ell_{in}$, $\ell_{out}$ and their partial derivatives over suitable compact set.    
\end{theorem}
\cref{th:generalizationBounds} states that the estimation error can be decomposed into two contributions each resulting from finite sample approximation of $L_{in}$ and $L_{out}$ with a \emph{parametric rate} of $\frac{1}{\sqrt{n}}$ and 
$\frac{1}{\sqrt{m}}$ up to a constant factor $C$. We provide a detailed expression for the constant in \cref{th:gen_bound} of \cref{app:sec_conv}. The restriction to compact subsets $\Omega$ instead of the whole space $\mathbb{R}^{d}$ allows controlling the complexity of some function classes indexed by the parameter $\omega$. Without additional assumptions on the objectives, we obtain a constant $C$ that grow with the diameter of the subset $\Omega$. 


To illustrate the implications of \cref{th:generalizationBounds}, we provide two convergence results for bilevel gradient methods in \cref{sec:gradientMethods} and defer the discussion on the general proof strategy to \cref{sec:sketch_proof} with a full proof provided in \cref{app:sec_conv}.

\subsection{Applications to Empirical Bilevel Gradient Methods}
\label{sec:gradientMethods}
A typical strategy to solve \eqref{eq:kbo} is to obtain empirical samples and solve \eqref{eq:kbo_app} using a bilevel optimization algorithm. Note that this is a finite-dimensional problem, and the lower-level is typically strongly convex. Our results allow to provide statistical guarantees for such approaches. We start with the simplest possible gradient algorithm for the unconstrained problem, $\mathcal{C} = \mathbb{R}^d$, given $\eta > 0$:
\begin{align}
    \label{eq:empiricalBilevelGradient}
    \omega_{t+1} &= \omega_t-\eta\nabla\widehat{\mathcal{F}}(\omega_t) & t\geq 0.
\end{align}
The algorithm requires access to the (almost surely) strongly convex inner-level solution and its derivative which can be obtained using implicit differentiation.

\begin{corollary}[Generalization for bilevel gradient descent]
    \label{cor:gradient}
    Consider \cref{assump:compact,assump:convexity_lin,assump:K_bounded,assump:reg_lin_lout} and a fixed $\lambda > 0$. Assume furthermore that $\mathbf{K}$ in \eqref{eq:kbo_app} is almost surely definite and that there is $c>0$ such that $\inf_{\omega,v,y} \ell_{out}(\omega,v,y) - c\|\omega\|^2 > - \infty$. Fix $\omega_0 \in \mathbb{R}^d$ and assume that $\omega_{t}$ is given by \eqref{eq:empiricalBilevelGradient} for all $t \geq 0$. Then there are $\bar{\eta} > 0$ and a constant $\bar{c}> 0$ such that for any $0<\eta<\bar{\eta}$, and for any $t > 0$, 
    \begin{align*}
        \mathbb{E}\Big[ \min_{i = 0,\ldots,t} \left\|\nabla\mathcal{F}(\omega_i)\right\|\Big] & \leq\bar{c}\left(\frac{1}{\sqrt{m}}+\frac{1}{\sqrt{n}}+\frac{1}{\sqrt{t+1}}\right),\\
        \mathbb{E}\Big[ \underset{i \to \infty}{\lim\sup} \left\|\nabla\mathcal{F}(\omega_i)\right\|\Big] & \leq\bar{c}\left(\frac{1}{\sqrt{m}}+\frac{1}{\sqrt{n}}\right).
    \end{align*}
\end{corollary}

    \begin{proof} Consider that $\inf_{\omega,v,y} \ell_{out}(\omega,v,y) - c\|\omega\|^2 \geq 0$, which entails $\ell_{out}(\omega,v,y) \geq c\|\omega\|^2$ for all $v,y$. Using \Cref{prop:bound_hstaromega} and setting $B = \sup_{y \in \mathcal{Y} } |\partial_v \ell_{in}(\omega_0,0,y)|$, we have almost surely
    \begin{align*}
        \widehat{\mathcal{F}}(\omega_0) \leq \max_{|v| \leq B\kappa/\lambda, y \in \mathcal{Y}} \ell_{out}(\omega_{0}, v, y) \coloneqq \bar{\ell}.
    \end{align*}
    Therefore, for any $\omega$ such that $ \widehat{\mathcal{F}}(\omega) \leq \widehat{\mathcal{F}}(\omega_0) $, we have $\|\omega\|^2 \leq \bar{\ell} / c$. Set $\Omega$ the ball of radius $\sqrt{\bar{\ell} / c}$ and center $0$. Using the fact that $\widehat{\nabla\mathcal{F}} = \nabla\widehat{\mathcal{F}}$ in \Cref{prop:equivalenceEstimates} and the representation in $\eqref{eq:functionalGradientEstimate}$, it is clear from \Cref{prop:lip_hstaromega} and the $C^3$ assumption that $\nabla \widehat{\mathcal{F}} $ is Lipschitz on $\Omega$ with a deterministic constant $L$. It follows from standard analysis of the gradient algorithm for nonconvex $\widehat{\mathcal{F}}$ with Lipschitz gradient (see, \textit{e.g.}, \citep[Theorems~4.25,~4.26]{beck2014introduction}) that setting $\bar{\eta} = 1 / L$, almost surely, 
    \begin{itemize}
        \item $\widehat{\mathcal{F}}(\omega_t) \leq \widehat{\mathcal{F}}(\omega_0) $ and $\omega_t \in \Omega$ for all $t \geq 0$.
        \item $\nabla \widehat{\mathcal{F}}(\omega_t) \to 0$ as $t \to \infty$.
        \item $\min_{i = 0,\ldots,t} \left\|\nabla\widehat{\mathcal{F}}(\omega_i)\right\| \leq \bar{c} / \sqrt{t+1}$ for all $t\geq0$, where $\bar{c}$ is a deterministic constant.
    \end{itemize}
    The corollary then follows by combining \Cref{prop:equivalenceEstimates} and the uniform bound in \Cref{th:generalizationBounds}.
\end{proof}
\begin{remark}
    The additional assumption on $\ell_{out}$ is a device to ensure a priori almost sure boundedness of the sequence. It is rather mild as it can be enforced by a small perturbation of the form $(\omega,v,y) \mapsto \ell_{out}(\omega,v,y) + c\|\omega\|^2$ assuming $\ell_{out} \geq 0$, which is typical in applications. Any other device ensuring a priori boundedness could be considered. The assumption on $\K$ is satisfied almost surely for most kernels.
\end{remark}

Now, considering the constrained problem and assuming $\mathcal{C}$ is convex compact, the projected gradient descent initialized at $\omega_0 \in \mathcal{C}$, given $\eta > 0$, iterates the following recursion:
\begin{align*}
    \omega_{t+1} = \Pi_{\mathcal{C}}(\omega_t - \eta \nabla \widehat{\mathcal{F}}(\omega_t))
\end{align*}
for all $t \geq 0$, where $\Pi_{\mathcal{C}}$ denotes the orthogonal projection on $\mathcal{C}$. The algorithmic requirements are the same as the gradient algorithm, with the addition of the orthogonal projection, which is a cheap operation for basic sets such as balls. For constrained optimization, the optimality condition should take the constraints into account. We consider the gradient mapping, see \citep[Section~10.3]{beck2017first},
\begin{equation*}\label{eq:gradientMappings}
    \begin{aligned}
        \widehat{G}_\eta &\colon \omega \mapsto \frac{1}{\eta} \left(\omega - \Pi_{\mathcal{C}}( \omega - \eta \nabla \widehat{\mathcal{F}}(\omega))\right),\\
    G_\eta &\colon \omega \mapsto \frac{1}{\eta} \left(\omega - \Pi_{\mathcal{C}}( \omega - \eta \nabla \mathcal{F}(\omega))\right).
    \end{aligned}
\end{equation*}
This captures stationarity for the recursion, and any local minimum of $\mathcal{F}$ on $\mathcal{C}$ satisfies $G_\eta = 0$ for all $\eta > 0$.

\begin{corollary}[Generalization for bilevel projected gradient descent]
    \label{cor:projectedGradient}
    Consider \cref{assump:compact,assump:convexity_lin,assump:K_bounded,assump:reg_lin_lout} and a fixed $\lambda > 0$. Assume furthermore that $\mathbf{K}$ in \eqref{eq:kbo_app} is almost surely definite and $\mathcal{C}$ is convex compact. Assume that $\omega_{t+1} = \omega_t-\eta\nabla\widehat{\mathcal{F}}(\omega_t)$ for all $t \geq 0$. Then there are $\bar{\eta} > 0$ and a constant $\bar{c}> 0$ such that for any $0<\eta<\bar{\eta}$, and for any $t > 0$, 
    \begin{align*}
        \mathbb{E}\Big[ \min_{i = 0,\ldots,t} \left\|G_\eta(\omega_i)\right\|\Big] & \leq\bar{c}\left(\frac{1}{\sqrt{m}}+\frac{1}{\sqrt{n}}+\frac{1}{\sqrt{t+1}}\right),\\
        \mathbb{E}\Big[ \underset{i \to \infty}{\lim\sup} \left\|G_\eta(\omega_i)\right\|\Big] & \leq\bar{c}\left(\frac{1}{\sqrt{m}}+\frac{1}{\sqrt{n}}\right).
    \end{align*}
\end{corollary}

\begin{proof}
    We choose $\Omega = \mathcal{C}$.
    All iterates obviously remain in $\Omega$. Similarly as in the proof of \Cref{cor:gradient}, $\nabla \widehat{\mathcal{F}}$ is Lipschitz and $\mathcal{F}$ is bounded on $\Omega$ with deterministic constants. It then follows using classical analysis of nonconvex projected gradient algorithm (see, \textit{e.g.}, \citep[Theorem~10.15]{beck2017first}), that for small $\eta$, almost surely $ \min_{i = 0,\ldots,t} \left\|\widehat{G}_\eta(\omega_i)\right\| \leq \bar{c} / \sqrt{t+1}$ for a determinstic $\bar{c}>0$, and that $\widehat{G}_\eta(\omega_i) \to 0$ as $i \to \infty$. Using the fact that the orthogonal projection is $1$-Lipschitz, (see, \textit{e.g.}, \citep[Theorem~6.42]{beck2017first}), we have for all $\omega \in \Omega$
    \begin{align*}
        \|\widehat{G}_\eta(\omega) - G_\eta(\omega)\| \leq \|\nabla \widehat{\mathcal{F}}(\omega) - \nabla\mathcal{F}(\omega)\|.
    \end{align*}
    The result follows by combining \Cref{prop:equivalenceEstimates} and the uniform bound in \Cref{th:generalizationBounds}.
\end{proof}

\section{General Proof Strategy for \cref{th:generalizationBounds}}\label{sec:proof_strategy}
\label{sec:sketch_proof}
The main strategy behind the proof of \cref{th:generalizationBounds} in \cref{app:sec_conv} consists in three steps: (step 1) obtaining a point-wise error decomposition of the errors into manageable error terms that holds almost surely for any $\omega\in \Omega$, then applying maximal inequalities to suitable empirical processes 
(step 2) and some degenerate $U$-processes (step 3) to control each of these terms. 
The final error bounds are obtained by combining  all these bounds as shown in \cref{app_sub:main_proof}.  

\textbf{Step 1: Point-wise error decomposition. } 
A main challenge in controlling the errors in \cref{th:generalizationBounds} is the non-linear dependence of both estimators $\widehat{\mathcal{F}}(\omega)$ and $\widehat{\nabla\mathcal{F}}(\omega)$ on the empirical distributions, as they are obtained via a plug-in procedure. We address this challenge by breaking down the error into components based on the discrepancies between expected values and their empirical counterparts of individual point-wise losses and their derivatives, evaluated at the optimal solution  $h_\omega^{\star}$. 
Specifically, we denote by $\delta_\omega^{out}$ and $\delta_\omega^{in}$ the errors on the objectives defined as: 
\begin{align*}
\delta_\omega^{out}&\coloneqq \verts{L_{out}(\omega, h^\star_\omega)-\widehat{L}_{out}(\omega, h^\star_\omega)},\\
\delta_\omega^{in}&\coloneqq\verts{L_{in}(\omega, h^\star_\omega)-\widehat{L}_{in}(\omega, h^\star_\omega)}. 
\end{align*}
Additionally, we quantify the errors between the partial derivatives of these objectives and their empirical counterparts. To simplify our proof outline, we slightly abuse notation by denoting $\partial_{h}\delta_\omega^{out}$, $\partial_{h}\delta_\omega^{in}$, $\partial_{\omega}\delta_\omega^{out}$, $\partial_{\omega,h}^2\delta_\omega^{in}$ and $\partial_{h}^2\delta_\omega^{in}$ to refer to these errors in terms of partial derivatives. For instance,  $\partial_{h}\delta_\omega^{out}$ is defined as  $\Verts{\partial_h L_{out}(\omega, h^\star_\omega)-\partial_h\widehat{L}_{out}(\omega, h^\star_\omega)}_\mathcal{H}$, with similar definitions for the other error terms that can be found in \cref{app:subsec_point_est}. 
The next proposition formalizes the above discussions. 
\begin{proposition}[Approximation bounds]\label{prop:grad_app_bound_main}
    Let $\Omega$ be an arbitrary compact subset of $\mathbb{R}^d$. Under \cref{assump:compact,assump:convexity_lin,assump:K_bounded,assump:reg_lin_lout}, the following holds almost surely, for any $\omega\in\Omega$:
    \begin{align*}
        \verts{\mathcal{F}(\omega)-\widehat{\mathcal{F}}(\omega)} &
        \leq C \parens{\delta_\omega^{out}+\partial_h\delta_\omega^{in}},\\
        \Verts{\nabla\mathcal{F}(\omega)-\widehat{\nabla\mathcal{F}}(\omega)}
        &\leq C(\partial_\omega\delta_\omega^{out}+\partial_h\delta_\omega^{out}+\partial_h^2\delta_\omega^{in}\\
        &\qquad+\partial_{\omega, h}^2\delta_\omega^{in}+\partial_h\delta_\omega^{in}),
    \end{align*}
where $C$ is a positive constant that depends only $\Omega$, dimension $d$, regularization parameter $\lambda$, $\kappa$ and local upper-bounds on  $\ell_{in}$, $\ell_{out}$ and their partial derivatives over suitable compact set.
\end{proposition}
A more detailed version of this proposition, including the exact constants, along with its proof are  provided in \cref{prop:grad_app_bound} of \cref{app:subsec_point_est}.
The error terms arising in the above decomposition are more amenable to a statistical analysis using empirical process theory as we discuss next. 

\textbf{Step 2: Maximal inequalities for empirical processes.} 
Some of the error terms, namely $\Dout$ and $\Doutw$, can be controlled directly using empirical process theory. 
For instance, $\Dout$ is associated to the family of random functions $\sqrt{m}\parens{L_{out}(\omega,h_{\omega}^{\star})-\widehat{L}_{out}(\omega,h_{\omega}^{\star})}_{\omega\in \Omega}$, which defines an empirical process, a scaled and centered empirical average of real-valued functions indexed by the parameter $\omega$. Thus, provided that suitable estimates of the complexity of the class are available (as measured by its packing number in \cref{prop:estimate_covering_numbers} of \cref{sec_app:max_in_bound_lip}), which is easy to obtain in our setting, we show in \cref{prop:exp_uni_bound} of \cref{app_subsec:max_in} that a maximal inequality of the form below follows from classical results on empirical processes:
$$\mathbb{E}_\mathbb{Q}\brackets{\sup_{\omega\in \Omega} 
\overbrace{\verts{L_{out}(\omega,h_{\omega}^{\star})-\widehat{L}_{out}(\omega,h_{\omega}^{\star})} }^{\Dout}
} \leq \frac{C}{\sqrt{m}}.
$$

{\bf Step 3: Maximal inequalities for degenerate U-processes.}
Unfortunately, the approach in step 2 cannot be readily used for the remaining error terms involving partial derivatives w.r.t. $h$ ($\Douth$, $\Dinh$, $\Dinwh$ and $\Dinhh$). 
These error terms are associated to processes that are not real-valued anymore but take values in an infinite-dimensional space. 
While the recent work in \cite{park2023towards} develops an empirical process theory for functions taking values in a vector space, the provided complexity estimates would result in unfavorable dependence on the sample size. 
Instead, we leverage the structure of the RKHS to control these errors using maximal inequalities for suitable degenerate $U$-processes of order $2$ indexed by the parameter $\omega$ and for which such inequalities were provided in the seminal works of \citet{sherman1994maximal,nolan1987u}. 
$U$-processes of order 2 are generalization of empirical processes that involve empirical averages of real-valued functions which depend on pairs of samples, instead of a single one as in empirical processes. 
These functions arise, in our case, when taking  the square of the error term $\Douth$, for instance, and exploiting the reproducing property in the RKHS. This approach, presented in \cref{prop:exp_uni_bound_2} of \cref{app_subsec:max_in}, allows us to obtain maximal inequalities of the form:
$$\mathbb{E}_\mathbb{Q}\brackets{\sup_{\omega\in \Omega} 
\Douth}\leq\mathbb{E}_\mathbb{Q}\brackets{\sup_{\omega\in \Omega} 
\parens{\Douth}^2}^\frac{1}{2}
 \leq \frac{C}{\sqrt{m}}.
$$
Combining the maximal inequalities from step 2 and 3 with the error decomposition from step 1 allows to obtain the result of  \cref{th:generalizationBounds}.  


\textbf{Discussion. }Alternative approaches to $U$-processes could be used to derive generalization bounds, although these would result in degraded sample dependence. Specifically, one could employ a variational formulation of the RKHS norm appearing in some of the error terms, such as $\Douth$, to express them as the error of some real-valued empirical process to which standard results could be applied. However, this comes at the cost of considering processes indexed not only by the finite-dimensional parameter $\omega$, but also by functions in a unit RKHS ball, in contrast with our approach based on $U$-processes indexed by finite-dimensional vectors. Consequently, these families have much larger complexities as measured by their covering/packing numbers \citep[Lemma~D.2]{yang2020function}, which directly impacts the generalization rate. 
Our proposed approach bypasses this challenge by using real-valued $U$-processes indexed by finite-dimensional parameters, at the expense of employing a more general empirical process theory for degenerate $U$-processes \cite{sherman1994maximal}.

\section{Conclusion and Perspectives}
In this work, we established the first statistical generalization bounds for \eqref{eq:kbo}. 
This paper is a first step in providing generalization results for bilevel gradient-based methods in a non-parametric setting. 
We showcased the applicability of these bounds on the simplest algorithms and expect that our results can be readily extended to state-of-the-art methods such as those discussed in \cite{arbel2022nonconvex,ghadimi2018approximation,chen2021closing,Dagreou2022SABA}. The results concerning $U$-processes, however, are derived under the restrictive assumption of i.i.d.~data. Relaxing such assumption to handle Markovian data is a promising direction for future work, as it would allow considering reinforcement learning applications where bilevel optimization methods have shown promising results \cite{Nikishin:2022}. 

\section*{Acknowledgements}
This work was supported by the ANR project BONSAI (grant ANR-23-CE23-0012-01). EP thank AI Interdisciplinary Institute ANITI funding, through the French ``Investments for the Future -- PIA3'' program under the grant agreement ANR-19-PI3A0004, Air Force Office of Scientific Research, Air Force Material Command, USAF, under grant numbers FA8655-22-1-7012. EP thank TSE-P, acknowledge support from ANR Chess, grant ANR-17-EURE-0010, ANR Regulia, support from IUF. EP and SV acknowledge support from ANR MAD. SV thanks PEPR PDE-AI (ANR-23-PEIA-0004) and the chair 3IA CÃ´te d'Azur BOGL.

\bibliography{ref}
\bibliographystyle{icml2025}


%
%
%
%
%
\newpage
\appendix
\onecolumn

\textbf{\LARGE Appendices}

\textbf{Roadmap. }We begin by presenting and establishing regularity properties of the objective functions in \cref{sec_app:reg}. In \cref{sec:grad_est}, we introduce the gradient estimators. \Cref{sec_app:prel_res} is dedicated to proving the boundedness and Lipschitz continuity of $h^\star_\omega$ and $\hat{h}_\omega$, along with local boundedness and Lipschitz properties of $\ell_{in}$, $\ell_{out}$ and their derivatives. The generalization results are provided in \cref{app:sec_conv}. In \cref{sec_app:max_in_bound_lip}, we establish maximal inequalities for bounded and Lipschitz families of functions. Differentiability properties of the objectives are studied in \cref{sec:proof_diff_results}. Finally, \cref{ab_sec:aux} contains auxiliary technical lemmas used throughout the proofs.

\textbf{Notations. }$\|\cdot\|$ denotes the Euclidean norm in $\mathbb{R}^d$, $\|\cdot\|_\mathcal{H}$ denotes the norm in the RKHS $\mathcal{H}$, $\|\cdot\|_{\op}$ denotes the operator norm, and $\|\cdot\|_{\hs}$ denotes the Hilbert-Schmidt norm. $\langle\cdot,\cdot\rangle_\mathcal{H}$ denotes the inner product on $\mathcal{H}$, and $\langle\cdot,\cdot\rangle_{\hs}$ denotes the Hilbert-Schmidt inner product. $K(x,\cdot)$ denotes the feature map, for any $x\in\mathcal{X}$. For any two normed spaces $E$ and $F$, $\mathcal{L}(E,F)$ denotes the space of continuous linear operators from $E$ to $F$. For any two probability distributions $\mathcal{P}$ and $\mathcal{Q}$, $\mathcal{P}\otimes\mathcal{Q}$ denotes the product measure of $\mathcal{P}$ and $\mathcal{Q}$. Given two Hilbert spaces $\left(H_1,\langle\cdot,\cdot\rangle_{H_1}\right)$ and $\left(H_2,\langle\cdot,\cdot\rangle_{H_2}\right)$, the tensor product of $u\in H_1$ and $v\in H_2$, denoted by $u\otimes v$, is an operator from $H_2$ to $H_1$ defined, for any $e\in H_2$, as $\left(u\otimes v\right)e=u\left\langle v, e\right\rangle_{H_2}$. For any $v_1,\ldots,v_n\in\mathbb{R}$, $\diag(v_1,\ldots,v_n)\in\mathbb{R}^{n\times n}$ denotes a diagonal matrix of size $n\times n$, where the diagonal entries are $v_1,\ldots,v_n$ and all the off-diagonal entries are $0$. $\mathbbm{1}_m$ denotes a vector of size $m$ where all entries are $1$. $\mathbbm{1}_{n\times n}$ denotes an $n\times n$ matrix where all entries are $1$. For any vector space $V$ over $\mathbb{R}$, $\Id_V$ denotes the identity operator on $V$. Given a compact set $\mathcal{K}$, $\diam(\mathcal{K})$ denotes its diameter. $v^\top$ denotes the transpose of either a vector or a matrix, depending on the context.

\input{regularity.tex}

\input{estimators.tex}

\input{implications_assumptions.tex}

\input{convergence.tex}

\input{empirical_processes.tex}

\input{proof_regularity.tex}
\input{auxiliary_results.tex}

%
%


\end{document}


%
%
%
%
%
%
%
%
%
%
%
%
%