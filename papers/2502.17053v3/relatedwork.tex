\section{Related Work}
\subsection{Learning-based Shape Completion}
% voxel-based
Early learning-based methods ~\citep{dai2017shape,han2017high,stutz2018learning,varley2017shape} often rely on voxel-based representations for 3D convolutional neural networks. However, these approaches are limited by their high computational cost and limited resolution. Alternatively, GRNet~\citep{xie2020grnet} and VE-PCN~\citep{wang2021voxel} use 3D grids as an intermediate representation for point-based completion.  
% point-based
In recent years, several methods have been proposed to directly process points by end-to-end networks. One pioneering point-based work is PCN~\citep{yuan2018pcn}, which uses a shared multi-layer perceptron (MLP) to extract features and generates additional points using a folding operation~\citep{yang2018foldingnet} in a coarse-to-fine manner. Inspired by it, a lot of point-based methods~\citep{wang2020cascaded,liu2020morphing,wen2020point,9928787,zhou2022seedformer,yu2021pointr,SCRN,pan2020ecg,AGConv,scpnet,zhou2020geometry,chen2022repcd} have been proposed.

Later, to address the issue of limited information available in partial shapes, several works~\citep{zhang2021view,aiello2022crossmodal,zhu2023csdn,huang2022spovt,zhang2022shape,Gong_2021_ICCV,fu2023vapcnet} have explored the use of auxiliary data to enhance performance. 
Some approaches~\citep{zhang2021view,aiello2022crossmodal,zhu2023csdn} involve the combination of rendered color images and partial point clouds, along with the corresponding camera parameters.
Another line of works~\citep{zhang2022shape,Gong_2021_ICCV,fu2023vapcnet} seeks to utilize the scanning viewpoints. They either produce points along the viewpoints~\citep{Gong_2021_ICCV,zhang2022shape} or use the viewpoints for self-supervised pretraining~\citep{fu2023vapcnet}.
Semantic labels are also used in a recent approach~\citep{huang2022spovt} to provide strong priors.
Although these methods have shown promising results, they often require additional input that is difficult to obtain in practical settings. 
Different from these 3D data-driven methods, MVCN~\citep{hu2019render4completion} operates completion solely in the 2D domain using a conditional GAN. However, it cannot supervise the results using ground truth with rich space information. 
In contrast to these methods, we propose to integrate the representation abilities of 3D and 2D modalities by observing self-structures. As a result, our method achieves a more comprehensive perception of the overall shape without requiring additional information.

Considering the high-quality details generation, a variety of strategies have been introduced by learning shape context and local spatial relationships. 
To achieve this goal, state-of-the-art methods design various refinement modules to learn better shape priors from the training data.
SnowflakeNet~\citep{9928787} introduces Snowflake Point Deconvolution (SPD), which leverages skip-transformer to model the relation between parent points and child points. 
FB-Net~\citep{yan2022fbnet} adopts the feedback mechanism during refinement and generates points recurrently.
LAKe-Net~\citep{tang2022lake} integrates its surface-skeleton representation into the refinement stage, which makes it easier to learn the missing topology part.
Another type of method tends to preserve and exploit the local information in partial input.
One direct approach is to predict the missing points by combining the results with partial input data~\citep{huang2020pf,yu2021pointr}. As the point set can be viewed as a token sequence, PoinTr~\citep{yu2021pointr} and its following works~\citep{10232862,li2023proxyformer,chen2023anchorformer} employ the transformer architecture~\citep{vaswani2017attention} to predict the missing point proxies.
SeedFormer~\citep{zhou2022seedformer} introduces a shape representation called patch seeds for preventing the loss of local information during pooling operation. 
Some other approaches~\citep{pan2020ecg,10106495,zhang2022point,chen2019multi} propose to enhance the generated shapes by exploiting the structural relations in the refinement stage.
A recent work~\citep{DBLP:journals/ijcv/ZhangLXNZTL23} shares a similar idea with our \emph{Similarity Alignment} unit of utilizing self-similarities property and directly learns geometric transformations for input points.
However, these strategies employ a unified refinement strategy for all points, which limits their ability to generate pleasing details for different points.
Our approach differs from theirs by breaking down the shape refinement task into two sub-goals, and adaptively extracting reliable features for different partial areas.

\subsection{View-based Methods for 3D Understanding}
View-based 3D understanding techniques have gained significant attention in recent years.
The classic Multi-View Convolutional Neural Network (MVCNN) model was introduced in \citep{su2015multi}, where color images are fed into a CNN and subsequently combined by a pooling operation. However, this approach has the fundamental drawback of ignoring view relations.
Following works~\citep{feng2018gvcnn,wei2022learning,han20193d2seqviews,yang2019learning,chen2022imlovenet} propose various strategies to tackle this problem. For example,
\cite{yang2019learning} obtains a discriminative 3D object representation by modeling region-to-region relations.
LSTM is also used to build the inter-view relations~\citep{dai2018siamese}.
Since the cross-modal data are more available, methods are proposed to fuse features of views and point clouds.

However, compared to their 2D counterpart, most of 3D recognition methods are limited by the scarcity of training data. Therefore, in recent years, there has been a surge in research focusing on enhancing 3D understanding through the utilization of pre-trained 2D models.
P2P~\citep{p2p} prompts a pre-trained image model by the geometry-preserved projection.
PointCLIP~\citep{zhang2022pointclip} and its following work~\citep{PointCLIPV2} achieve zero-shot point cloud understanding by sending multi-view 2D projections to CLIP~\citep{radford2021learning} for a well-learned representation. To adapt the power of CLIP to more 3D scenarios, alternative methods~\citep{CLIP2,CLIP2point,clip2scene} employ cross-modal contrastive learning to train a powerful 3D encoder.
Recent approaches explore masked autoencoders using both 3D and 2D data~\citep{jointMAE, I2PMAE}. Some approaches pre-train a 3D encoder through color image generation~\citep{takeaphoto} or neural rendering~\citep{ponder}.
Inspired by the success of view-based 3D understanding techniques, our method utilizes point cloud features to enhance relationships between multiple views obtained by self-augmentation.

\subsection{Semantic Scene Completion}
Given an incomplete observation, semantic scene completion (SSC) aims to reconstruct and assign semantic labels to a 3D scene.
The prevailing approach in SSC involves utilizing the voxel grid as the primary 3D representation. Pioneering the field, SSCNet~\citep{song2017semantic} addresses this task through an end-to-end network constructed with 3D Convolutional Neural Networks. Subsequent methodologies enhance this framework by incorporating group convolution~\citep{zhang2018efficient}, multimodal fusion~\citep{guo2018view}, GAN~\citep{wang2019forknet}, and boundary learning~\citep{chen20203d}. 
SISNet~\citep{cai2021semantic} tackles SSC by iteratively performing grid-based scene completion and point cloud object completion.
AdaPoinTr~\citep{10106495} introduces a geometry-enhanced SSC framework to augment existing voxel-based SSC methods with a point cloud completion network.
% 
To accommodate diverse autonomous driving scenes, recent research has primarily focused on outdoor LiDAR datasets and explored various input modalities.
LiDAR-based approaches~\citep{yan2021sparse,xia2023scpnet} commonly integrate point-wise features within voxel space to enhance scene understanding. Meanwhile, alternative methods have emerged to predict scene occupancy using monocular images. The pioneering work MonoScene~\citep{cao2022monoscene} addresses this problem with successive U-Nets and a novel feature projection module. Building on this foundation, \cite{li2023voxformer} introduces a two-stage transformer architecture. 
NDC-Scene~\citep{yao2023ndc} further improves performance by reducing feature ambiguity in the normalized device coordinates space. Symphonies~\citep{jiang2024symphonize} advances this line of research by incorporating instance-centric semantics and scene context through the use of instance queries.
More recently, images captured by multiple cameras are also utilized for accurate 3D understanding~\citep{Wei_2023_ICCV,huang2023tri}.
% 
Different from these methodologies, point-based SSC methods~\citep{zhang2021point,wang2022learning,xu2023casfusionnet,yan2023pointssc} focus on reconstructing a complete scene point cloud along with semantic labels assigned to each point.
\cite{wang2022learning} attach an extra segmentation network to the point cloud completion model.
CasFusionNet~\citep{xu2023casfusionnet} takes a hierarchical approach, merging features from completion and segmentation modules.
In this work, we extend PointSea to SSC by jointly predicting semantic labels and geometric details through the proposed SDG.

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.95\textwidth]{fig/overview.pdf}
\caption{The architecture of PointSea. SVFNet first generates a global shape from the cross-modal input. The coarse completion is then upsampled and refined with two SDGs.}
  \label{fig:overview}
\end{figure*}