\section{Experiment}
In this section, we first conduct experiments on the widely-used completion benchmarks: ShapeNet-55/34~\citep{10232862}, PCN~\citep{yuan2018pcn}, and Projected-ShapeNet-55/34~\citep{10232862}. 
These datasets leverage various techniques to generate incomplete point clouds, allowing for a comprehensive assessment of PointSea.
Following this, we evaluate our method on real-world partial scans sourced from KITTI~\citep{geiger2013vision}, ScanNet~\citep{dai2017scannet}, and Matterport3D~\citep{matterport3d}.
Additionally, we extend our method to point cloud semantic scene completion, conducting experiments on two indoor datasets~\citep{xu2023casfusionnet}.
Finally, a series of ablation studies are performed to demonstrate the impact of each component.
\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{fig/55_vis.pdf}
\caption{Visual comparison with recent methods~\citep{yu2021pointr,chen2023anchorformer,10232862,Zhu_2023_ICCV} on ShapeNet-55. Hard,  Moderate, and Simple stand for the three difficulty levels.}
  \label{fig:vis55}
\end{figure*}

\begin{table*} 
    \renewcommand\arraystretch{1.2}
    \centering
    \caption{Quantitative results on ShapeNet-55. CD-S, CD-M, and CD-H stand for CD values under the simple, moderate, and hard difficulty levels, respectively ({$\displaystyle \ell ^{2}$} CD $\times 10^3$, DCD, and F1-Score@1\%). Ours (Whole) denotes PointSea, which generates 8,192 points as the completion results. On the other hand, Ours (Missing) denotes PointSea that produces 6,144 points and combines them with the partial input as the final prediction.}
    \small
    \label{tab:shapenet55}
    \begin{tabular}{|c|ccc|ccc|}
    \hline
    Methods & CD-S & CD-M & CD-H & CD-Avg$\downarrow$ & DCD-Avg$\downarrow$ & F1$\uparrow$ \\
    \hline
              \multicolumn{7}{|c|}{Methods that predict the whole shape}\\
              \hline
              FoldingNet~\citep{yang2018foldingnet} & 2.67 & 2.66 & 4.05 & 3.12 &0.826&  0.082\\
              PCN~\citep{yuan2018pcn} & 1.94 & 1.96 & 4.08 & 2.66 & 0.618  & 0.133\\
              TopNet~\citep{tchapmi2019topnet} & 2.26 & 2.16 & 4.3 & 2.91 &0.743&0.126 \\
              GRNet~\citep{xie2020grnet}  & 1.35 & 1.71 & 2.85 & 1.97 &0.592&0.238 \\
              SeedFormer~\citep{zhou2022seedformer} & 0.50 & 0.77 & 1.49 & 0.92 &0.558& 0.472 \\
              GTNet~\citep{DBLP:journals/ijcv/ZhangLXNZTL23}  & 0.45 & 0.66 & 1.30 & 0.80 & 0.696 & 0.543 \\
              AdaPoinTr~\citep{10232862}  & 0.49 & 0.69 & 1.24 & 0.81 & 0.635 & 0.503 \\
              SVDFormer~\citep{Zhu_2023_ICCV} &  0.48 & 0.70 & 1.30 & 0.83 & 0.541 & 0.451 \\

              \hline
              \multicolumn{7}{|c|}{Methods that predict the missing part}\\
              \hline
              PFNet~\citep{huang2020pf} & 3.83 & 3.87 & 7.97 & 5.22 & 0.716 &0.339 \\
              PoinTr~\citep{yu2021pointr}  & 0.58 & 0.88 & 1.79 & 1.09 &0.575& 0.464 \\
              ProxyFormer~\citep{li2023proxyformer}  & 0.49 & 0.75 & 1.55 & 0.93 & 0.549 & 0.483 \\
              AnchorFormer~\citep{chen2023anchorformer}  & 0.41 & 0.61 & 1.26 & 0.76 & 0.584 & 0.558 \\
              
              \hline
              Ours (Whole) &  0.43 & 0.64 & 1.19 & 0.75
               & \textbf{0.532} & 0.485 \\
              Ours (Missing) &  \textbf{0.35}  & \textbf{0.57}  & \textbf{1.18}  & \textbf{0.70}  & 0.563  & \textbf{0.564} \\
              \hline
    \end{tabular}
\end{table*}

\begin{table*}[htb]
    \renewcommand\arraystretch{1.2}
    \centering
    \caption{Quantitative results on ShapeNet-34. CD-S, CD-M, and CD-H stand for CD values under the simple, moderate, and hard difficulty levels, respectively ({$\displaystyle \ell ^{2}$} CD $\times 10^3$, DCD, and F1-Score@1\%).}
    \footnotesize
    \label{tab:shapenet34}
    \begin{tabular}{|c|cccccc|cccccc|}
    \hline
    \multirow{2}{*}{Methods} &  \multicolumn{6}{c}{34 seen categories} & \multicolumn{6}{c}{21 unseen categories} \\ \cline{2-13} & \makebox[0.01\linewidth][c]{CD-S} & \makebox[0.01\linewidth][c]{CD-M} & \makebox[0.01\linewidth][c]{CD-H} & \makebox[0.05\linewidth][c]{CD-Avg$\downarrow$} & \makebox[0.05\linewidth][c]{DCD-Avg$\downarrow$} & \makebox[0.01\linewidth][c]{F1$\uparrow$} & \makebox[0.01\linewidth][c]{CD-S} & \makebox[0.01\linewidth][c]{CD-M} & \makebox[0.01\linewidth][c]{CD-H} & \makebox[0.05\linewidth][c]{CD-Avg$\downarrow$} & \makebox[0.05\linewidth][c]{DCD-Avg$\downarrow$} & \makebox[0.01\linewidth][c]{F1$\uparrow$} \\
    \hline
    \multicolumn{13}{|c|}{Methods that predict the whole shape}\\
    \hline
    FoldingNet~\citep{yang2018foldingnet} & 1.86 & 1.81 & 3.38 & 2.35 & 0.831 & 0.139 & 2.76 & 2.74 & 5.36 & 3.62 & 0.870 & 0.095\\
    PCN~\citep{yuan2018pcn} & 1.87 & 1.81 & 2.97 & 2.22 & 0.624 & 0.150 & 3.17 & 3.08 & 5.29 & 3.85 & 0.644 & 0.101\\
    TopNet~\citep{yuan2018pcn} & 1.77 & 1.61 & 3.54 & 2.31 & 0.838 & 0.171 & 2.62 & 2.43 & 5.44 & 3.50 & 0.825 & 0.121\\
    GRNet~\citep{xie2020grnet} & 1.26 & 1.39 & 2.57 & 1.74 & 0.600 & 0.251 & 1.85 & 2.25 & 4.87 & 2.99 & 0.625 & 0.216\\
    SeedFormer~\citep{zhou2022seedformer} & 0.48 & 0.70 & 1.30 & 0.83 & 0.561 & 0.452 & 0.61 & 1.07 & 2.35 & 1.34 & 0.586 & 0.402\\
    GTNet~\citep{DBLP:journals/ijcv/ZhangLXNZTL23} & 0.51 & 0.73 & 1.40 & 0.88 & 0.700 & 0.511 & 0.78 & 1.22 & 2.56 & 1.52 & 0.703 & 0.467\\
    AdaPoinTr~\citep{10232862} & 0.48 & 0.63 & 1.07 & 0.73 & 0.606 & 0.469 & 0.61 & 0.96 & 2.11 & 1.23 & 0.619 & 0.416\\
    SVDFormer~\citep{Zhu_2023_ICCV} & 0.46  & 0.65 & 1.13  & 0.75 & 0.538 & 0.457 & 0.61 & 1.05 & 2.19 & 1.28 & 0.554 & 0.427\\
    
    \hline
    \multicolumn{13}{|c|}{Methods that predict the missing part}\\
    \hline
    
    PFNet~\citep{huang2020pf} & 3.16 & 3.19 & 7.71 & 4.68 & 0.708 & 0.347 & 5.29 & 5.87 & 13.33 & 8.16 & 0.723 & 0.322\\
    PoinTr~\citep{yu2021pointr} & 0.76 & 1.05 & 1.88 & 1.23 & 0.575 & 0.421 & 1.04 & 1.67 & 3.44 & 2.05 & 0.604 & 0.384\\
    ProxyFormer~\citep{li2023proxyformer} & 0.44 & 0.67 & 1.33 & 0.81 & 0.556 & 0.466 & 0.60 & 1.13 & 2.54 & 1.42 & 0.583 & 0.415\\
    AnchorFormer~\citep{chen2023anchorformer} & 0.41 & 0.57 & 1.12 & 0.70 & 0.585 & 0.564 & 0.52 & 0.90 & 2.16 & 1.19 & 0.598 & 0.535\\
    
    \hline
    Ours (Whole) & 0.40  & 0.57 & 1.00  & 0.66 & \textbf{0.525} & 0.492 & 0.50 & 0.88 & \textbf{1.92} & 1.10 & \textbf{0.541} & 0.461\\
    Ours (Missing) & \textbf{0.32}  & \textbf{0.50} & \textbf{0.98}  & \textbf{0.60} & 0.555 & \textbf{0.575} &\textbf{0.43} & \textbf{0.83} & 1.95 & \textbf{1.07} & 0.572 & \textbf{0.548}\\
    \hline
    \end{tabular}
\end{table*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{fig/PCN_vis.pdf}
\caption{Visual comparisons with recent methods~\citep{9928787,zhou2022seedformer,chen2023anchorformer,10232862,Zhu_2023_ICCV} on the PCN dataset. Our method produces the most faithful and detailed structures compared to its competitors. In the fourth row, we provide a visualization of a thin cross-section for the couch model, showcasing PointSea's excellence in reconstruction quality with fewer noisy points. }
  \label{fig:Vis_PCN}
\end{figure*}

\begin{table*}[t]
    \tiny
    \renewcommand\arraystretch{1.2}
    \centering
    \caption{Quantitative results on the PCN dataset. ({$\displaystyle \ell ^{1}$} CD $\times 10^3$, DCD, and F1-Score@1\%)}
    \label{tab:pcnl1}
    \small
    \begin{tabular}{|c|cccccccc|ccc|}
    \hline
    Methods & Plane & Cabinet & Car & Chair & Lamp & Couch & Table & Boat & CD-Avg$\downarrow$  & DCD$\downarrow$  &F1$\uparrow$  \\
    \hline
              PCN~\citep{yuan2018pcn} & 5.50 & 22.70 & 10.63 & 8.70 & 11.00 & 11.34 & 11.68 & 8.59 & 9.64& 0.622 &0.695\\
              GRNet~\citep{xie2020grnet}  & 6.45 & 10.37 & 9.45 & 9.41 & 7.96 & 10.51 & 8.44 & 8.04& 8.83& 0.622 &0.708\\
              CRN~\citep{wang2020cascaded}    & 4.79 & 9.97 & 8.31 & 9.49 & 8.94 & 10.69 & 7.81 & 8.05& 8.51& 0.594 & 0.652 \\
              NSFA~\citep{zhang2020detail}    & 4.76 & 10.18 & 8.63 & 8.53 & 7.03 & 10.53 & 7.35 & 7.48 & 8.06 & 0.578 & 0.734 \\
              PoinTr~\citep{yu2021pointr}  & 4.75 & 10.47 & 8.68 & 9.39 & 7.75 & 10.93 & 7.78 & 7.29 & 8.38 & 0.611 & 0.745 \\
              SnowflakeNet~\citep{9928787} & 4.29 & 9.16 & 8.08 & 7.89 & 6.07 & 9.23 & 6.55 & 6.40& 7.21& 0.585 & 0.801 \\
              SDT~\citep{zhang2022point} & 4.60 & 10.05 & 8.16 & 9.15 & 8.12 & 10.65 & 7.64 & 7.66 & 8.24 & 0.637 &0.754 \\
              PMP-Net++~\citep{wen2022pmp} & 4.39 & 9.96 & 8.53 & 8.09 & 6.06 & 9.82 & 7.17 & 6.52& 7.56 & 0.611 & 0.781\\
              SeedFormer~\citep{zhou2022seedformer}  & 3.85 & 9.05 & 8.06 & 7.06 & 5.21 & 8.85 & 6.05 & 5.85 & 6.74 & 0.583 & 0.818 \\
              FBNet~\citep{yan2022fbnet}  & 3.99 & 9.05 & 7.90 & 7.38 & 5.82 & 8.85 & 6.35 & 6.18& 6.94& - & -\\
              LAKeNet~\citep{tang2022lake}  & 4.17 & 9.78 & 8.56 & 7.45 & 5.88 & 9.39 & 6.43 & 5.98 & 7.23 & - & -\\
    
            
              VAPCNet~\citep{fu2023vapcnet}  & 4.10 & 9.28 & 8.15 & 7.51 & 5.55 & 9.18 & 6.28 & 6.10 & 7.02 & - & - \\
              GTNet~\citep{DBLP:journals/ijcv/ZhangLXNZTL23}  & 4.17 & 9.33 & 8.38 & 7.66 & 5.49 & 9.44 & 6.69 & 6.07 & 7.15 & - & - \\
              CP3~\citep{10097548}  & 4.34 & 9.02 & 7.90 & 7.41 & 6.35 & 8.52 & 6.32 & 6.26 & 7.02 & - & - \\
              AnchorFormer~\citep{chen2023anchorformer}  & 3.70 & 8.94 & 7.57 & 7.05 & 5.21 & 8.40 & 6.03 & 5.81 & 6.59 & 0.554 & 0.827 \\
              AdaPoinTr~\citep{10232862}  & 3.68 & 8.82 & 7.47 & 6.85 & 5.47 & 8.35 & 5.80 & 5.76 & 6.53 & 0.538 & 0.845 \\
              SVDFormer~\citep{Zhu_2023_ICCV} & 3.62 & 8.79 & 7.46 & 6.91 & 5.33 & 8.49 & 5.90 & 5.83& 6.54 & 0.536 & 0.841 \\

              \hline
              Ours & \textbf{3.52} & \textbf{8.54} & \textbf{7.33} & \textbf{6.58} & \textbf{5.21} & \textbf{8.24} & \textbf{5.75} & \textbf{5.62}& \textbf{6.35} & \textbf{0.522} &\textbf{0.858} \\
              \hline
    \end{tabular}
\end{table*}


\begin{table*}[htb]
    \renewcommand\arraystretch{1.1}
    \centering
    \caption{Quantitative results on Projected-ShapeNet-55/34. ({$\displaystyle \ell ^{1}$} CD $\times 10^3$ and F1-Score@1\%)}
    \small
    \label{tab:projectshapenet}
    \begin{tabular}{|c|cc|cc|cc|}
    \hline
    \multirow{2}{*}{Methods} &  \multicolumn{2}{c|}{55 categories} & \multicolumn{2}{c|}{34 seen categories} & \multicolumn{2}{c|}{21 unseen categories} \\ \cline{2-7} & CD$\downarrow$ & F1$\uparrow$ & CD$\downarrow$ & F1$\uparrow$ & CD$\downarrow$ & F1$\uparrow$  \\
    \hline
    PCN~\citep{yuan2018pcn} & 16.64 & 0.403 & 15.53 & 0.432 & 21.44 & 0.307\\
    TopNet~\citep{yuan2018pcn} & 16.35 & 0.337 & 12.96 & 0.464 & 15.98 & 0.358\\
    GRNet~\citep{xie2020grnet} & 12.81 & 0.491 & 12.41 & 0.506 & 15.03 & 0.439\\
    SnowflakeNet~\citep{9928787} & 11.34 & 0.594 & 10.69 & 0.616 & 12.82 & 0.551\\
    PoinTr~\citep{yu2021pointr} & 10.68 & 0.615 & 10.21 & 0.634 & 12.43 & 0.555\\
    SeedFormer~\citep{zhou2022seedformer} & 11.17 & 0.602 & 10.88 & 0.608 & 12.85 & 0.553\\
    AnchorFormer~\citep{chen2023anchorformer} & 10.34 & 0.665 & 9.79 & 0.683 & 12.10 & 0.612\\
    AdaPoinTr~\citep{10232862} & 9.58 & 0.701 & 9.12 & 0.721 & 11.37 & 0.642\\
    SVDFormer~\citep{Zhu_2023_ICCV} & 9.71 & 0.689 & 9.26 & 0.704 & 11.51 & 0.620\\
    
    \hline
    Ours &  \textbf{9.25} & \textbf{0.727} & \textbf{8.79} & \textbf{0.745} & \textbf{11.05} & \textbf{0.658} \\
    \hline
    \end{tabular}
\end{table*}

\subsection{Implementation Details}
We use the dense projection method introduced by \cite{PointCLIPV2} to generate depth maps with a resolution of 224 $\times$ 224 from three orthogonal views. These projected depth maps, without applying any color mapping enhancement, are directly fed to the network. 
For point clouds normalized to the range [-0.5, 0.5], such as those in the PCN dataset, the depth maps are projected at a distance of 0.7 to observe the entire shape. For point clouds normalized to the range [-1.0, 1.0], like those in the ShapeNet-55/34 dataset, the distance is set to 1.5.

In SVFNet, we use PointNet++~\citep{qi2017pointnet++} to extract features from point clouds. The detailed architecture is: 
$\emph{SA}(C = [3,64,128], N = 512, K = 16)\rightarrow\emph{SA}(C = [128,256], N = 128, K = 16)\rightarrow\emph{SA}(C = [512,256])$. 
We leverage a ResNet-18~\citep{he2016deep} model, pre-trained on ImageNet~\citep{5206848}, to extract features from depth maps. A self-attention layer with 512 hidden feature dimensions, followed by an MLP, is utilized to regress the coarse points $P_C$. The merged point cloud $P_0$ comprises 512 points for PCN and 1024 points for ShapeNet-55/34.

In SDG, we adopt EdgeConv~\citep{wang2019dynamic} to extract local features from $P_{in}$. The detailed architecture is:
$\emph{EdgeConv}(C = [3,64], K = 16)\rightarrow\emph{FPS}(2048,512)\rightarrow\emph{EdgeConv}(C = [64,256], K = 8)$. 
After obtaining $F_Q$ and $F_H$, we use a decoder composed of two self-attention layers (one in the ShapeNet-55/34 experiments) to further analyze the coarse shapes.
$F_{l}^{\prime}$ is then passed to an MLP and reshaped to $F_{l}\subseteq\mathbb{R}^{rN\times 128}$. Finally, the coordinates offset is predicted by an MLP with feature dimensions of [128, 64, 3].
Note that we use two SDGs in the refinement step. 
For the two SDG modules, we use the shared-weights architecture above. The hidden feature dimensions of self-attention layers are set as 768 and 512. 
The upsampling rates {$r_1$, $r_2$} are set to \{4, 8\} for PCN and \{2, 4\} for ShapeNet-55/34, respectively.

The network is implemented using PyTorch~\citep{paszke2019pytorch} and trained with the Adam optimizer~\citep{kingma2014adam} on NVIDIA 3090 GPUs. It takes 400 epochs for convergence. The initial learning rate is set to 0.0001 and decayed by 0.7 for every 40 epochs.


\subsection{Evaluation on ShapeNet-55/34}
\subsubsection{Data and Metrics}
The ShapeNet-55~\citep{yu2021pointr} dataset is created based on ShapeNet~\citep{chang2015shapenet} and contains shapes from 55 categories with 41,952 shapes for training and 10,518 shapes for testing. 
ShapeNet-34~\citep{yu2021pointr} is characterized by 34 categories for training, with 21 additional unseen categories reserved for testing. In total, 46,765 shapes from the 34 categories are used for training. Testing involves 3,400 shapes from the seen 34 categories and 2,305 shapes from the unseen 21 categories.
In both datasets, The ground-truth point cloud has 8,192 points and the partial input has 2,048 points. 
During training, to generate incomplete point clouds, we randomly select a viewpoint and remove the $n$ furthest points, with the remaining points down-sampled to 2,048. During testing, 8 fixed viewpoints are employed, and the number of missing points is set to 2,048, 4,096, and 6,144, corresponding to three difficulty levels: simple (S), moderate (M), and hard (H).
We use $\displaystyle \ell^{2}$ version of CD, Density-aware Chamfer Distance (DCD)~\citep{wu2021balanced}, and F-Score@1\% as evaluation metrics.

\subsubsection{Quantitative Results}
The quantitative results are summarized in Tab.~\ref{tab:shapenet55} and Tab.~\ref{tab:shapenet34}, consisting of CD values for three difficulty levels and the average value of two additional metrics. We organize the results of existing methods based on their completion strategies, categorizing them into two groups: those generating all 8,192 points as the final prediction~\citep{yang2018foldingnet,yuan2018pcn,tchapmi2019topnet,xie2020grnet,9928787,zhou2022seedformer,10232862,Zhu_2023_ICCV,DBLP:journals/ijcv/ZhangLXNZTL23}, and those only producing the missing 6,144 points and then combining them with the partial input to form the final results~\citep{huang2020pf,yu2021pointr,li2023proxyformer,chen2023anchorformer}. Also based on this, for our method, we provide two versions, denoted as Ours (Whole) and Ours (Missing).

Specifically, when producing the whole shape (Ours (Whole)) following the same setting as SVDFormer~\citep{Zhu_2023_ICCV}, PointSea outperforms all other methods in terms of CD and DCD, as shown in Tab.~\ref{tab:shapenet55} and Tab.~\ref{tab:shapenet34}. 
Furthermore, in comparison to SVDFormer~\citep{Zhu_2023_ICCV}, PointSea exhibits a noteworthy reduction in average CD by 9.6\% and 14.1\% on ShapeNet-55 and Unseen categories of ShapeNet-34, respectively. This signifies that the new designs not only contribute to improved completion performance but also enhance robustness to unseen data.
It is important to note that although GTNet~\citep{DBLP:journals/ijcv/ZhangLXNZTL23} and PointSea share similar concepts of utilizing similar geometries in the input, our approach demonstrates superior generalization ability. As reported in Tab.~\ref{tab:shapenet34}, our approach achieves an average CD that is 38.2\% lower than GTNet~\citep{DBLP:journals/ijcv/ZhangLXNZTL23} on the unseen categories.
Additionally, when only producing the missing 6,144 points, we can observe that PointSea demonstrates substantial performance improvement, surpassing even the whole version.

\subsubsection{Qualitative Results}
Fig.~\ref{fig:vis55} shows the visualization of results produced by different methods under all three difficulty levels. Overall, PointSea demonstrates better performance in generating the missing parts with faithful details.
Moreover, the advantage of our SDG can be well proved by the cases of models in the second, third, and fourth rows, where the missing parts have similar structures in the input. Current methods like PoinTr~\citep{yu2021pointr}, AnchorFormer~\citep{chen2023anchorformer}, and AdaPoinTr~\citep{10232862} fail to generate the complex missing geometries. Although SVDFormer completes them by imitating the input, the results contain undesired details with noisy points. With the assistance of the path selection module, PointSea goes a step further, producing cleaner results.


\subsubsection{Reasons for the Performance Gap Between Different Completion Strategies}
We observe a large performance gap between methods using different completion strategies (only missing part or the whole model) on the ShapeNet-55/34 dataset. When predicting only the missing part, PointSea performs significantly better in CD and F1-Score, while falling behind in DCD and visual performance. 
This phenomenon is primarily attributed to the fact that \textbf{the input partial point cloud is a subset of the ground-truth point cloud due to the online cropping strategy of ShapeNet-55/34}. 

The CD metric is a bi-directional measure that summarizes the distance from prediction (Pred) to Ground Truth (GT) and vice versa. When calculating the distance from Pred to GT, the method predicting the missing portion will generate a value of 0 for the input 2,048 points. In contrast, for the distance from GT to Pred, although the input is sparser than the ground-truth point cloud, the value remains close to 0 in these regions. However, when predicting the whole shape, the network might not precisely replicate the ground truth within the input area, leading to higher CD values, especially in simple and moderate settings. 
Although F1-Score@1\% is also computed based on the bi-directional distance, the difference from CD is that F1-Score strictly classifies distance values as either ``True" ($\leq 0.01$) or ``False" ($> 0.01$). 
Therefore, for the method that predicts the whole shape, points in the input region whose distance is slightly larger than the threshold of 0.01 will be regarded as ``False", thus widening the quantitative gap. Naturally, methods predicting the missing part will achieve much higher F1-Score in all three difficulty levels.

In general, only predicting the missing part allows the network to achieve better CD and F1-Score. However, employing this strategy does not ensure the consistency of the entire shape. This may result in point clouds that are unevenly distributed and exhibit discontinuities, as visualized by the mug and table models in Fig.~\ref{fig:vis55}. Some other methods, such as PoinTr~\citep{yu2021pointr} and AnchorFormer~\citep{chen2023anchorformer}, that similarly adopt the missing part prediction strategy, exhibit a similar phenomenon. 
This limitation is further emphasized by the comparison in terms of DCD. Before calculating the point-wise bi-directional distance, both CD and DCD construct a one-to-one mapping between two point sets using the nearest neighbor search strategy. However, DCD additionally introduces a normalization term into the distance calculation~\citep{wu2021balanced}.
This choice makes DCD more sensitive to the local point density. As observed in Tab.~\ref{tab:shapenet55}, the DCD of PointSea increases from 0.532 to 0.563 when predicting the missing part.
Considering that in most cases, the input does not constitute a subset of the ground-truth point cloud and may even contain substantial noise~\citep{10232862}. So, in the following experiments, we choose the whole shape prediction strategy as the default setting of our method, enabling the network to adaptively learn to complete the input point clouds.

\subsection{Evaluation on PCN}
\label{secPCN}
\subsubsection{Data and Metrics}
The PCN dataset~\citep{yuan2018pcn} contains shapes of 8 categories in ShapeNet~\citep{chang2015shapenet}. The ground-truth point cloud has 16,384 points and the partial input has 2,048 points. The ground-truth point cloud consists of 16,384 points, while the partial input comprises 2,048 points. Ground-truth point clouds are uniformly sampled from the shape surface. Incomplete point clouds are generated by back-projecting 2.5D depth images from 8 viewpoints. We follow the same experimental setting as previous works~\citep{yuan2018pcn,9928787,10232862,zhou2022seedformer} for a fair comparison. The $\displaystyle \ell^{1}$ version of CD, DCD, and F-Score@1\% are used as evaluation metrics. 


\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{fig/Vis_p55.pdf}
\caption{Visual comparisons with recent methods~\citep{chen2023anchorformer,10232862} on the Projected-ShapeNet-55 dataset. When the partial input contains substantial noisy points, PointSea demonstrates the ability to refine these regions.}
  \label{fig:Vis_p55}
\end{figure}

\begin{figure*}[h]
  \centering
  \includegraphics[width=\textwidth]{fig/Vis_real.pdf}
\caption{Visual comparisons with recent methods~\citep{chen2023anchorformer,10232862} on real-world scans.}
  \label{fig:real}
\end{figure*}


\subsubsection{Quantitative Results}
We compare PointSea with 17 competitors~\citep{yuan2018pcn,xie2020grnet,wang2020cascaded,zhang2020detail,yu2021pointr,9928787,wen2022pmp,yan2022fbnet,zhou2022seedformer,zhang2022point,tang2022lake,fu2023vapcnet,DBLP:journals/ijcv/ZhangLXNZTL23,10097548,chen2023anchorformer,10232862,Zhu_2023_ICCV} in Tab.~\ref{tab:pcnl1}. The results demonstrate that PointSea achieves the best performance across all metrics. In particular, the average CD value of PointSea is 6.35, which is 0.19 lower than the CD value of the second-ranked method AdaPoinTr~\citep{10232862}. Meanwhile, our approach also achieves the lowest CD value among all 8 categories. 

\subsubsection{Qualitative Results}
In Fig.~\ref{fig:Vis_PCN}, we present a visual comparison of the results produced by different methods, revealing that PointSea consistently generates more accurate completions. Take the table model in the second row as an example, only AnchorFormer, SVDFormer, and PointSea successfully locate and complete the missing regions, while PointSea produces a more compact and faithful result. 
Besides, in the case of the couch model in the fourth row, a thin cross-section is demonstrated to further reveal the superiority of our method. Although all methods can generate the overall shape, PointSea excels at reconstructing the sharp edges of the handrail with fewer noisy points.

\subsection{Evaluation on Projected-ShapeNet-55/34}
\subsubsection{Data and Metrics}
The Projected-ShapeNet-55/34 dataset was introduced in \citep{10232862}. The primary difference between this dataset and ShapeNet-55/34 is that incomplete point clouds are generated by noised back-projecting from 16 viewpoints, making it more challenging compared with simple back-projecting in PCN~\citep{yuan2018pcn} or online cropping in ShapeNet-55/34~\citep{10232862}. We use the $\displaystyle \ell ^{1}$ version of CD and F-Score@1\% as evaluation metrics.
\subsubsection{Results}
Tab.~\ref{tab:projectshapenet} details the performance of different methods across all categories of Projected-ShapeNet-55 and seen/unseen categories of Projected-ShapeNet-34. Our PointSea achieves the best performance among all the previous methods.
In Fig.~\ref{fig:Vis_p55}, we visually compare PointSea with two representative methods~\citep{10232862,chen2023anchorformer}. The comparison clearly demonstrates that PointSea excels in capturing the overall shape from noisy input and effectively completing the missing regions.
Moreover, we observe that when the input point clouds exhibit extreme noise, the network must refine the undesired points within these regions. Notably, methods that predict the missing part~\citep{yu2021pointr,chen2023anchorformer} lack this capability, preserving the noise in the final prediction. While AdaPoinTr~\citep{10232862} can refine some of these points, PointSea surpasses it by generating cleaner and more detailed structures.

\begin{table*}
    \renewcommand\arraystretch{1.2}
    \small
    \centering
    \caption{Quantitative results on real-world scans. Results on KITTI and MatterPort3D are generated by models pre-trained on the PCN dataset, while results on ScanNet are produced by models pre-trained on the Projected-ShapeNet-55 dataset. (MMD $\times 10^3$)}
    \label{tab:realworld}
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    Methods & KITTI & ScanNet Chairs & ScanNet Tables & Matterport3D Chairs & Matterport3D Tables \\
    \hline
              GRNet~\citep{xie2020grnet}   & 5.350 & 3.042 & 1.955 & 1.382 & 1.062 \\
              SeedFormer~\citep{zhou2022seedformer}   & 1.179 & 2.916 & 1.451 & 1.206 & 0.986 \\
              AnchorFormer~\citep{chen2023anchorformer} & 1.252 & 3.259 & 1.381 & 1.334 & 0.952 \\
              AdaPoinTr~\citep{10232862}   & 1.058  & 2.585 & 1.191 & 1.242 & 0.931  \\
              SVDFormer~\citep{Zhu_2023_ICCV}   & 0.967 & 2.828 & 1.170 & 1.238 & 0.965 \\
              Ours   & \textbf{0.933}  & \textbf{2.362} & \textbf{0.943} & \textbf{1.067} & \textbf{0.902} \\
    \hline
    \end{tabular}
\end{table*}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{fig/diverse_real.pdf}
\caption{Visual comparisons with recent methods~\citep{chen2023anchorformer,10232862} on more diverse real-world scans from \cite{dai2017scannet,choi2016large}.}
  \label{fig:diverse_real}
\end{figure}

\begin{table*}[h]
    \renewcommand\arraystretch{1.1}
    \centering
    \caption{Quantitative results on Semantic Scene Completion. ({$\displaystyle \ell ^{1}$} CD $\times 10^3$, mIoU, and mAcc)}
    \small
    \label{tab:ssc}
    \begin{tabular}{|c|ccc|ccc|}
    \hline
    \multirow{2}{*}{Methods} &  \multicolumn{3}{c|}{SSC-PC} & \multicolumn{3}{c|}{NYUCAD-PC} \\\cline{2-7} & CD$\downarrow$ & mIoU$\uparrow$ & mAcc$\uparrow$ & CD$\downarrow$ & mIoU$\uparrow$ & mAcc$\uparrow$ \\
    \hline
    Disp3D~\citep{wang2022learning} & 35.9 & 18.9 & 23.5 & 22.75 & 16.4 & 22.9 \\
    CasFusionNet~\citep{xu2023casfusionnet} & \textbf{8.96} & 91.3 & 94.8 & 10.28 & 49.5 & 59.7 \\
    \hline
    Ours &  9.17 & \textbf{92.2} & \textbf{95.3} & \textbf{9.52} & \textbf{54.6} & \textbf{65.4} \\
    \hline
    \end{tabular}
\end{table*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{fig/gen_comp.pdf}
\caption{Visual comparison with state-of-the-art image-to-3D models~\citep{li2024craftsman,sf3d2024,xu2024instantmesh} on the PCN dataset. To ensure a fair comparison, we render ShapeNet meshes from the same viewpoints used in the virtual scanning process of the PCN dataset and use the resulting images as inputs for these models.}
  \label{fig:gen_comp}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.95\textwidth]{fig/ssc.pdf}
\caption{Visual comparison with \citep{xu2023casfusionnet} on the NYUCAD-PC dataset. The original depth scans are shown in the leftmost column.}
  \label{fig:ssc}
\end{figure*}

\subsection{Evaluation on Real-world Scans}
\subsubsection{Data and Metrics}
To evaluate our method on real-world scans captured by different sensors, we conducted tests on a diverse set of point clouds: 2,401 car point clouds from KITTI~\citep{geiger2013vision}, 100 chair and table point clouds from ScanNet~\citep{dai2017scannet}, and 20 chair and table point clouds from Matterport3D~\citep{matterport3d}.
Results on KITTI and MatterPort3D are generated by models pre-trained on the PCN dataset,
while results on ScanNet are produced by models pre-trained on the Projected-ShapeNet-55 dataset.
Since there is no ground truth available for real-world scans, our quantitative evaluation metric is the Minimal Matching Distance (MMD) calculated based on the $\displaystyle \ell ^{2}$ version of CD. 
In this context, MMD represents the CD value between the output point cloud and the most similar car/table/chair point cloud from ShapeNet, based on CD. This metric quantifies how closely the output resembles a typical car, table, or chair.

\subsubsection{Results}
The quantitative results are reported in Tab.~\ref{tab:realworld}. The superior performance in terms of MMD indicates that results produced by PointSea closely resemble those of a typical car/chair/table, even when the input data exhibits a distribution shift from the training set.
Additionally, we demonstrate the visual comparisons with two representative methods~\citep{chen2023anchorformer,10232862} in Fig.~\ref{fig:real}. 
To provide a more comprehensive evaluation, we also show results on more diverse real-world scans from ScanNet~\citep{dai2017scannet} and Redwood~\citep{choi2016large} in Fig~\ref{fig:diverse_real}, which not only expands the category diversity but also introduces more irregular types of missing regions.
It is evident that, in scenarios where the point clouds captured by real-world sensors contain noisy points or exhibit inconsistent scale compared to the training data, our method demonstrates superior performance in reconstructing a smooth surface and preserving the original structures.


\subsection{Comparison with Image-to-3D Models}
Given the success of emerging 3D generative models~\citep{wang2025crm,tang2024dreamgaussian}, we compare PointSea with three state-of-the-art image-to-3D models~\citep{li2024craftsman,sf3d2024,xu2024instantmesh}, as shown in Fig.~\ref{fig:gen_comp}. 
To fairly evaluate the potential of image-to-3D models, we first render ShapeNet meshes from the same viewpoints used in the virtual scanning process of the PCN dataset. The resulting images serve as inputs for these models.
The results in the first, second, and third rows reveal that when input images are affected by self-occlusion, generative models struggle to reconstruct unseen parts, often resulting in distorted or undesired shapes. Additionally, without leveraging geometric cues like point clouds, these generative models fail to maintain the original structure of the partial input. For instance, in the fourth row, although all generative methods produce visually plausible chairs, they fail to reconstruct the correct back angle due to the limited information provided by the input images.
In contrast, our SDG's dual-way design ensures both accurate reconstruction of unseen parts and faithful preservation of the input geometry.


\subsection{Extension to Semantic Scene Completion}

\subsubsection{Network Adjustment}
In this experiment, our objective is to assess the performance of PointSea when dealing with complex scene-level point clouds and to explore its potential application in semantic understanding.
Given a partial observation of a scene, the goal of semantic scene completion methods is to reconstruct the completion scene along with the semantic category of each point.
The key difference between this task and object completion is the additional prediction of semantic labels.
To adjust PointSea to this task, we unleash the full potential of SDG to simultaneously predict the geometric structure and semantic category in a coarse-to-fine manner.
Specifically, each SDG is equipped with an MLP-based segmentation head. In the first SDG, the offset feature $F_{1}$ is fed into the attached head to produce the probability of each class $\emph{L}_{1}\subseteq\mathbb{R}^{N_{1} \times C_N}$ for $P_1$, where $C_N$ represents the number of classes. In the subsequent SDG, the segmentation results are predicted through residual learning. In particular, $F_{2}$ is input into the attached head to regress a probability offset $\emph{L}_{2}$. Then, the predicted probability of this step is obtained by adding $\emph{L}_{2}$ to $\emph{L}_{1}$.
When the point location shifts across different class regions during refinement, this design ensures that the semantic label can adaptively change along with it since $\emph{L}_{l}$ and $O_l$ share the same information.
Furthermore, a modification is introduced in the loss function. In addition to the completion loss in Eq.~\ref{eqnloss}, we need another loss function to supervise the semantic segmentation. Since there is no direct one-to-one mapping between $\emph{L}_{l}$ and the ground-truth labels, we first establish the mapping when calculating CD and leverage this mapping to find the corresponding label for each point. After that, cross-entropy loss is computed to supervise the segmentation results in each SDG. 

\subsubsection{Dataset and Metrics}
We conduct experiments on the SSC-PC and NYUCAD-PC datasets, as provided in \cite{xu2023casfusionnet}, to evaluate the performance of PointSea on SSC. SSC-PC is created based on the dataset provided in \cite{zhang2021point}. The input incomplete point clouds are generated through virtual scanning, with both the input and output point clouds comprising 4,096 points. NYUCAD-PC is created based on the NYUCAD dataset~\citep{Firman_2016_CVPR}. The input incomplete point clouds, consisting of 4,096 points, are generated through real-world depth scans, while the ground-truth point clouds, consisting of 8,192 points, are sampled from CAD mesh annotations. We follow the same experimental setting with \cite{xu2023casfusionnet}.
We use $\displaystyle \ell ^{1}$ version of CD as the evaluation metric of completion quality. To evaluate the semantic segmentation results, we adopt the mean class IoU (mIoU) and the mean class accuracy (mAcc) as metrics.
\subsubsection{Results}
We compare our method with two state-of-the-art methods: Disp3D~\citep{wang2022learning} and CasFusionNet~\citep{xu2023casfusionnet}. The quantitative results are given in Tab.~\ref{tab:ssc}, from which we can find that PointSea achieves the best performance on the real-world dataset NYUCAD-PC. On the SSC-PC dataset, PointSea ranks second in terms of CD but outperforms in the segmentation metrics.
In Fig.~\ref{fig:ssc}, we further illustrate the superior performance of PointSea by visually comparing our method with the state-of-the-art CasFusionNet~\citep{xu2023casfusionnet}. Clearly, the complete scenes generated by PointSea exhibit finer local structures and yield more accurate segmentation results.
Overall, the results demonstrate the effectiveness of our SDG in handling complex scenes and highlight its potential for extension to other tasks.

\subsection{Ablation Studies}
\label{ablationSec}
To ablate PointSea, we remove and modify the main components. All ablation variants are tested on the PCN and ShapeNet-55 datasets. The ablation variants can be categorized as ablations on SVFNet and SDG.


\begin{table*}
        \renewcommand\arraystretch{1.2}
        \centering
        \caption{
        Effect of SVFNet. (PCN: {$\displaystyle \ell ^{1}$} CD $\times 10^3$ and F1-Score@1\%. ShapeNet-55: {$\displaystyle \ell ^{2}$} CD $\times 10^4$ and F1-Score@1\%)
        }
        \label{tab:ablationSVFNet}
        \footnotesize
        \begin{tabular}{|c|c c c c c| c c | c c|}
        \hline
        \multirow{2}{*}{Methods}  & \multirow{2}{*}{Sparse Projection} & \multirow{2}{*}{Dense Projection} & \multirow{2}{*}{Intra-view} & \multirow{2}{*}{Inter-view} & \multirow{2}{*}{One-stage} & \multicolumn{2}{c|}{PCN} & \multicolumn{2}{c|}{ShapeNet-55} 
        \\ \cline{7-10} & &&&& & CD$\downarrow$ & F1$\uparrow$ & CD$\downarrow$ & F1$\uparrow$ \\ 
        \hline
        A : w/o Projection  & & & & & & 6.53  & 0.836 & 8.46 & 0.451\\%1
        B : w/o Feature Fusion & & \checkmark & & & & 6.58 & 0.831 & 8.52 & 0.445\\%1
        C : Intra-view Fusion & & \checkmark & \checkmark & & & 6.43 & 0.847 & 7.93 & 0.476\\%1
        D : Inter-view Fusion & & \checkmark & & \checkmark & & 6.49 & 0.845 & 8.23 & 0.458 \\%1
        E : One-stage Fusion & & \checkmark & & & \checkmark & 6.46 & 0.848 & 7.88 & 0.469\\%1
        F : Sparse Projection & \checkmark & & \checkmark & \checkmark & & 6.39 & 0.853 & 7.80 & 0.480\\%1
        Ours  & & \checkmark & \checkmark & \checkmark & & \textbf{6.35} & \textbf{0.858} & \textbf{7.53} & \textbf{0.485}  \\
        \hline
        \end{tabular}
\end{table*}

\begin{table}
        \renewcommand\arraystretch{1.2}
        \centering
        \caption{Effect of projection numbers on the PCN dataset. ({$\displaystyle \ell ^{1}$} CD $\times 10^3$, DCD, F1-Score@1\%, and Inference Time)}
        \label{tab:ablationViewNumber}
        \small
        \begin{tabular}{|c|c c c |c|}
        \hline
        Methods  & CD$\downarrow$ & DCD$\downarrow$ & F1$\uparrow$ & Time\\
        \hline
        1 view  &  6.44 & 0.528 & 0.848  & 23.51ms\\
        3 views (Ours) & \textbf{6.35} & \textbf{0.522} & 0.858 & 24.92ms\\
        6 views & 6.37 & 0.524 & \textbf{0.860} & 26.77ms\\
        \hline
        \end{tabular}
\end{table}

\begin{table}
        \renewcommand\arraystretch{1.2}
        \centering
        \caption{Robustness to random perturbation of projection on the PCN dataset. ({$\displaystyle \ell ^{1}$} CD $\times 10^3$, DCD, and F1-Score@1\%)}
        \label{tab:ablationNoise}
        \footnotesize
        \resizebox{0.4\textwidth}{!}{
        \begin{tabular}{|cc|ccc|}
        \hline
        \multicolumn{2}{|c|}{Perturbation Levels} & \multirow{2}{*}{CD$\downarrow$} & \multirow{2}{*}{DCD$\downarrow$} & \multirow{2}{*}{F1$\uparrow$} \\ \cline{1-2} Distance & Angle &&& \\
        \hline
        \multicolumn{2}{|c|}{w/o Perturbation}&  \textbf{6.35} & \textbf{0.522} & \textbf{0.858} \\%1
        $0.05$ & $5^{\circ}$  &  6.36 & 0.523 & 0.857 \\
        $0.1$ & $10^{\circ}$ & 6.38 & 0.524  & 0.856 \\
        $0.15$ & $15^{\circ}$ & 6.43 & 0.526  & 0.853 \\
        $0.2$ & $20^{\circ}$ & 6.51 & 0.539 & 0.849\\
        \hline
        \end{tabular}
        }
\end{table}

\begin{table}[t]
        \renewcommand\arraystretch{1.2}
        \centering
        \caption{Effect of different 2D encoders on the PCN dataset. ``ImageNet'' and ``DINO V2'' denote models pre-trained by ImageNet~\citep{5206848} classification and DINO V2~\citep{dinov2}. ({$\displaystyle \ell ^{1}$} CD $\times 10^3$, DCD, F1-Score@1\%, and Inference Time)}
        \label{tab:ablation2Dbackbone}
        \small
        \begin{tabular}{|c|c c c|c|}
        \hline
        Methods  & CD$\downarrow$ & DCD$\downarrow$ & F1$\uparrow$ & Time \\
        \hline
        ResNet-18 (w/o Pre-trained) & 6.38 & 0.524 & 0.854 & 24.92ms \\
        ResNet-18 (ImageNet, Ours) & 6.35 & 0.522 & 0.858 & 24.92ms\\
        Resnet-50 (w/o Pre-trained) & 6.37 & 0.524 & 0.856 & 27.53ms\\
        Resnet-50 (ImageNet) & 6.33 & 0.520 & 0.861 & 27.53ms\\
        Vit-B/16 (w/o Pre-trained) & 6.43 & 0.527 & 0.851 & 42.81ms\\
        Vit-B/16 (ImageNet) & 6.38 & 0.525 & 0.850 & 42.81ms\\
        Vit-B/14 (w/o Pre-trained) & 6.44 & 0.525 & 0.853 & 46.52ms\\
        Vit-B/14 (DINO V2) & \textbf{6.29} & \textbf{0.518} & \textbf{0.867} & 46.52ms\\
        \hline
        \end{tabular}
\end{table}

\begin{table*}[t]
        \renewcommand\arraystretch{1.2}
        \centering
        \caption{
        Effect of SDG. (PCN: {$\displaystyle \ell ^{1}$} CD $\times 10^3$ and F1-Score@1\%. ShapeNet-55: {$\displaystyle \ell ^{2}$} CD $\times 10^4$ and F1-Score@1\%)
        }
        \label{tab:ablationIDTr}
        \footnotesize
        \begin{tabular}{|c|c c c c|c c | c c|}
        \hline
        \multirow{2}{*}{Methods}  & \multirow{2}{*}{Analysis} & \multirow{2}{*}{Alignment} & \multirow{2}{*}{Embedding} & \multirow{2}{*}{Selection} & \multicolumn{2}{c|}{PCN} & \multicolumn{2}{c|}{ShapeNet-55} 
        \\ \cline{6-9} & &&& & CD$\downarrow$ & F1$\uparrow$ & CD$\downarrow$ & F1$\uparrow$ \\
        \hline
        G : w/o Embedding & \checkmark & \checkmark & & & 6.56  & 0.839 & 8.16 & 0.454 \\
        H : w/o Alignment& \checkmark & & \checkmark & & 6.69  & 0.828 & 8.83 & 0.437\\
        I : w/o Analysis & &\checkmark &  & &  6.71   & 0.825 & 9.08 & 0.412\\ 
        J : w/o Selection &\checkmark&\checkmark &\checkmark & & 6.46  & 0.845 & 7.93 & 0.466\\
        Ours & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{6.35} &\textbf{0.858} & \textbf{7.53} & \textbf{0.485}  \\
        \hline
        \end{tabular}
\end{table*}

\begin{table}
        \renewcommand\arraystretch{1.2}
        \centering
        \caption{Effect of the number of SDG modules on the PCN dataset. ({$\displaystyle \ell ^{1}$} CD $\times 10^3$, DCD, F1-Score@1\%, and Inference Time)}
        \label{tab:ablationSDGNumber}
        \small
        \begin{tabular}{|c|c c c |c|}
        \hline
        Methods  & CD$\downarrow$ & DCD$\downarrow$ & F1$\uparrow$ & Time\\
        \hline
        1 SDG  &  6.56 & 0.549 & 0.841  & 15.86ms\\
        2 SDGs (Ours) & \textbf{6.35} & 0.522 & \textbf{0.858} & 24.92ms\\
        3 SDGs & 6.36 & \textbf{0.520} & \textbf{0.858} & 33.53ms\\
        \hline
        \end{tabular}
\end{table}


\subsubsection{Ablation on SVFNet}
To investigate the impact of shape descriptor extraction methods, we compare six variants of SVFNet, and the results are presented in Tab.~\ref{tab:ablationSVFNet}. 
In variant A, we remove the input depth maps, and the completion performance is limited by relying only on 3D coordinates to understand shapes. 
In variant B, we evaluate the importance of our Feature Fusion module by replacing the fusion of different inputs with late fusion, which directly concatenates features from different modalities. We observe an evident drop in performance, indicating that the proposed SVFNet can effectively fuse cross-modal features. 
Then, we evaluate the effect of different feature fusion strategies. In variant C, same with SVDFormer~\citep{Zhu_2023_ICCV}, fusion happens only at the intra-view level. In variant D, conversely, we only use inter-view feature fusion. Whether employing intra-view or inter-view fusion individually, the performance is inferior compared to their combination. In variant E, instead of fusing features in two stages, we reshape patched features $F_V$ from all three views to the size of ${N_V H^{\prime} W^{\prime} \times C}$ and feed it to a single fusion module. The decreased performance indicates that fusing patch-wise features from all views weakens the model's ability to perceive shapes from multiple perspectives.
Finally, in variant F, we use the vanilla perspective projection applied in SVDFormer~\citep{Zhu_2023_ICCV} to replace the dense projection~\citep{PointCLIPV2}. We see the performance of our method slightly drops due to the insufficient semantic information contained in the sparse depth maps. 

Furthermore, to conduct a more thorough analysis of the effectiveness of our SVFNet, we visualize the results produced by our approach, variant A, and SeedFormer, which also employ the coarse-to-fine paradigm. In Fig.~\ref{fig:MVFVis}, we present the results alongside the coarse point cloud generated directly by SVFNet (patch seeds of \cite{zhou2022seedformer}). Our analysis reveals that during the initial completion stage, both SeedFormer and variant A produce suboptimal results, such as generating too few points in missing areas. This presents a challenge for the refinement stage, making it difficult to produce satisfactory final results. Our SVFNet overcomes this challenge by leveraging multiple viewpoints to observe partial shapes. By doing so, our method can locate missing areas and generate a compact global shape, leading to the production of fine details in the results. 

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{fig/SVFVis.pdf}
\caption{Visual comparison of the representative coarse-to-fine method \citep{zhou2022seedformer}, variant A (w/o projection), and our method on two partial models. The upper results are generated coarse results.}
  \label{fig:MVFVis}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.97\linewidth]{fig/Ablation_Analysis.pdf}
\caption{Visual comparison with SeedFormer~\citep{zhou2022seedformer} and variant H (w/o Structure Analysis) on LiDAR scans from KITTI.}
  \label{fig:Ablation_Analysis}
\end{figure}

\begin{figure*}[h]
  \centering
  \includegraphics[width=\textwidth]{fig/Ablation_Cross.pdf}
\caption{
Visual comparison of generated coarse point cloud $P_0$ and results of variant H (w/o Similarity Alignment), I (w/o Structure Analysis), and J (w/o Selection). 
We select a query point (marked with red) in $P_0$ and visualize the attention map in the cross-attention layer. The redder the color, the higher the similarity. The mean $\alpha$ values of the selected point from the Path Selection module are also listed in the rightmost column.
}
  \label{fig:Ablation_cross}
\end{figure*}

\noindent\textbf{Effect of the number of projections.} 
We conduct an ablation experiment to assess the impact of varying the number of projections. The depth maps are projected from one front view, as well as from three and six orthogonal views, respectively. The results, presented in Tab.~\ref{tab:ablationViewNumber}, reveal a challenge for the network in perceiving the whole shape when using the information from only a single view.
Simultaneously, we see that the performance will not continue increasing when utilizing all six views.
This can be attributed to the following reasons:
Traditional methods, such as multi-view stereo, rely on extensive correspondences across dozens of RGB images and geometric constraints from well-calibrated camera poses to infer 3D shapes. As a result, incorporating more views typically leads to improved 3D reconstruction quality.
In contrast, our approach utilizes multi-view depth images projected from partial point clouds. These depth maps provide only rough structural information and lack texture details, making them less informative compared to RGB images. 
Given this limitation, we employ only a few views and implicitly fuse them with point clouds and viewpoint information in the feature level to predict the coarse shape. The results indicate that three views are sufficient for this task. Increasing the number of views tends to introduce information overlap and redundancy, and may also introduce errors. 


\noindent\textbf{Robustness to random perturbation of projection.} 
To assess the robustness of our method, during inference, we introduce zero-mean Gaussian noise to the projection process. We add perturbation to the projection by including camera view angle offsets and observation distance displacements. We varied the perturbation levels by adjusting the standard deviations to four different values (0.05, 0.1, 0.15, 0.2 unit length for observing distance and 5, 10, 15, 20 degrees for camera angles). The results in Tab.~\ref{tab:ablationNoise} demonstrate that PointSea is robust to slight perturbations. As the noise level increases significantly ({$0.2$, $20^{\circ}$}), there is a noticeable drop in performance. This can be attributed to the substantial noise, which causes the information contained among the three projections to become partially overlapped and lost. Consequently, this poses a challenge for the feature fusion.


\noindent\textbf{Effect of different 2D backbones.} 
We ablate the choice of 2D backbone in the SVFNet. To be specific, we replace it with ResNet-50~\citep{he2016deep} and the vision transformer~\citep{dosovitskiy2021an}, respectively. The results in Tab.~\ref{tab:ablation2Dbackbone} indicate that a larger 2D backbone does not yield a substantial performance improvement, but it does introduce increased computational costs. 
Moreover, we ablate the pre-trained weights of 2D encoders. We first use the commonly used ImageNet~\citep{5206848} pre-training and find that models pre-trained in this way perform slightly better than those trained from scratch.
Then, to explore the potential of vision foundation models in the completion task, we use the recently proposed DINO V2 model~\citep{dinov2} in PointSea. The results demonstrate that visual features derived from large-scale self-supervised learning can significantly enhance completion performance, compared with pre-training on ImageNet or training from scratch.
However, the ViT model introduces a substantial computational burden, increasing the parameter number and inference time by more than twofold compared to the default setting. For this reason, we continue to use ResNet-18 pre-trained on ImageNet as the 2D encoder in our main experiments.


\subsubsection{Ablation on SDG}
Tab.~\ref{tab:ablationIDTr} compares different variants of PointSea on the SDG module. In variant G, we remove the Incompleteness Embedding of SDG, which results in a higher CD value and a lower F1-Score, indicating that the ability to perceive the incompleteness degree of each part is crucial for the model. In the variants H and I, we completely remove the Similarity Alignment and Structure Analysis path from SDG, respectively. The results show that the performance of the model significantly drops when any one of these paths is removed.
Compared with Variant J, where only the path selection module is removed, the model can flexibly choose features from different paths and achieve the best performance.

\noindent\textbf{Effect of the number of SDG modules.} 
We conducted an ablation experiment to evaluate the impact of varying the number of SDG modules. In our main experiments, we stack two SDGs with upsampling rates of \{4, 8\}. For comparison, we now evaluate configurations with a single SDG using an upsampling rate of \{32\} and three SDGs with upsampling rates of \{2, 2, 8\}. The results are presented in Tab.~\ref{tab:ablationSDGNumber}.
% 
We find that directly upsampling point clouds by a factor of 32 results in a significant performance drop.
This indicates that the refinement process relies on the collaboration of multiple SDGs, which regards shape refinement as a cascaded process. 
Then, when the number of SDGs is increased to three, the DCD value shows slight improvement, but the CD value remains unchanged, indicating that two SDGs are adequate for generating 16,384 points in the PCN dataset.
Note also that using more SDGs may become necessary when generating a higher number of points.



\noindent\textbf{Visual comparison of different variants} 
To better understand SDG, we show more visual results in Fig.~\ref{fig:Ablation_Analysis} and Fig.~\ref{fig:Ablation_cross}. 
Specifically, we investigate the effectiveness of the two units and the Path Selection module by comparing the performance of different variants of the model on different kinds of incomplete point clouds. 
In Fig.~\ref{fig:Ablation_Analysis}, our method can generate plausible shapes, while the variant I and SeedFormer produce undesired structures due to over-preserving the partial input. This result proves the importance of the Structure Analysis path, particularly when the input contains limited information. 
% 
Fig.~\ref{fig:Ablation_cross} shows the results produced by different variants H, I, J, and ours.
We visualize the attention map in the cross-attention layer to demonstrate the effectiveness of the Similarity Alignment unit. 
The query point (marked in red) is absent in the input and shown in the coarse result $P_0$. 
The mean $\alpha$ (in Eq.~\ref{eqn3}) value of the selected query point is also provided.
The results demonstrate that for shapes with highly similar regions (lamp in the first row and table in the second row), the Similarity Alignment unit effectively identifies similar geometries over short or long distances, resulting in finer details. The model can produce plausible shapes with fine details even without the Structure Analysis Path. However, without the Path Selection module, the effectiveness of the Similarity Alignment unit is limited. In such cases, the Path Selection module assigns smaller 
$\alpha$ values, prioritizing the Alignment unit and leading to improved results.
For shapes characterized by highly sparse and noisy partial input (LiDAR scan in the third row), the attention map shows that the Alignment unit struggles to extract useful information from similar regions. Here, a larger 
$\alpha$ value directs the model to focus more on the Analysis unit, mitigating the influence of noisy information in the partial input.
Based on these observations, we conclude that the Path Selection module dynamically aggregates diverse features, thereby enhancing detail completion.



\begin{table}[h]
        \renewcommand\arraystretch{1.2}
        \centering
        \caption{Complexity analysis. We report the inference time (ms) and the number of parameters (Params) on the PCN dataset (16,384 points). Our method achieves a balance between computation cost and performance.}
        \label{tab:complexity}
        \footnotesize
        \begin{tabular}{|c|cc|c|} 
        \hline
        Methods  & \makebox[0.02\linewidth][c]{Time} & \makebox[0.02\linewidth][c]{Params} & CD$\downarrow$\\
        \hline
        GRNet~\citep{xie2020grnet}  & 14.51ms & 76.71M & 8.83    \\
        SeedFormer~\citep{zhou2022seedformer}  & 43.09ms & 3.24M & 6.74    \\
        GTNet~\citep{DBLP:journals/ijcv/ZhangLXNZTL23} & 62.53ms & 13.48M & 7.15    \\
        AdaPoinTr~\citep{yu2021pointr}  & 15.26ms & 32.49M & 6.53    \\
        SVDFormer~\citep{Zhu_2023_ICCV}  & 26.55ms & 32.63M & 6.54    \\
        \hline
        PointSea  & 24.92ms & 41.22M & 6.35  \\
        PointSea-L  & 16.52ms & 20.55M & 6.47 \\
        \hline
        \end{tabular}
\end{table}

\subsubsection{Complexity Analysis}
We present the complexity analysis in Tab.~\ref{tab:complexity}, including the inference time on a single NVIDIA 3090 GPU, the number of parameters, and the corresponding results on the PCN dataset. The notable increase in model size compared to SVDFormer is primarily attributed to the incorporation of a complete ResNet-18 to harness the capabilities of pre-trained 2D models.
To strike a favorable balance between cost and performance, we introduce a more lightweight version of PointSea, denoted as PointSea-L in Tab.~\ref{tab:complexity}. PointSea-L reduces the number of hidden dimensions in attention layers by half and, akin to SVDFormer, employs a compact ResNet-18 with 1/4 the feature size of the original ResNet. The results indicate that PointSea can achieve state-of-the-art performance with faster inference speed and fewer parameters compared to its previous version.

