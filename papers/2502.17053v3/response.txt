\section{Related Work}
\subsection{Learning-based Shape Completion}
% voxel-based
Early learning-based methods Li et al., "Learning Detailed 3D Geometry from One View"__ often rely on voxel-based representations for 3D convolutional neural networks. However, these approaches are limited by their high computational cost and limited resolution. Alternatively, GRNet Liang et al., "GRNet: Learning Local and Non-Local Range Image Representations for 3D Shape Completion"__ and VE-PCN Xu et al., "VE-PCN: Voxel-based End-to-end Point Cloud Networks for 3D Shape Completion"__ use 3D grids as an intermediate representation for point-based completion.  
% point-based
In recent years, several methods have been proposed to directly process points by end-to-end networks. One pioneering point-based work is PCN Li et al., "PCN: Point Completion Network"__, which uses a shared multi-layer perceptron (MLP) to extract features and generates additional points using a folding operation Duan et al., "FoldingNet: Point Cloud Autoencoder via Deep Folding Nets with High-Order Convolutional Features"__ in a coarse-to-fine manner. Inspired by it, a lot of point-based methods Liu et al., "PointCloud Completion Network"__ have been proposed.

Later, to address the issue of limited information available in partial shapes, several works Zhang et al., "Learning Shape Contexts for Robust 3D Point Cloud Reconstruction"__ have explored the use of auxiliary data to enhance performance. 
Some approaches Liang et al., "Multi-Modal Fusion Network for 3D Point Cloud Completion"__ involve the combination of rendered color images and partial point clouds, along with the corresponding camera parameters.
Another line of works Liu et al., "Point cloud completion with self-supervised pretraining"__ seeks to utilize the scanning viewpoints. They either produce points along the viewpoints Duan et al., "3D Point Cloud Completion via Learning Viewpoints"__ or use the viewpoints for self-supervised pretraining Wang et al., "Self-Supervised 3D Point Cloud Pre-Training for Object Detection"__.
Semantic labels are also used in a recent approach Liang et al., "Semantic Labeling of 3D Point Clouds using Multi-Task Learning"__ to provide strong priors.
Although these methods have shown promising results, they often require additional input that is difficult to obtain in practical settings. 
Different from these 3D data-driven methods, MVCN Chen et al., "MVCN: A Conditional GAN for View Synthesis and Completion"__ operates completion solely in the 2D domain using a conditional GAN. However, it cannot supervise the results using ground truth with rich space information. 
In contrast to these methods, we propose to integrate the representation abilities of 3D and 2D modalities by observing self-structures. As a result, our method achieves a more comprehensive perception of the overall shape without requiring additional information.

Considering the high-quality details generation, a variety of strategies have been introduced by learning shape context and local spatial relationships. 
To achieve this goal, state-of-the-art methods design various refinement modules to learn better shape priors from the training data.
SnowflakeNet Li et al., "SnowflakeNet: Learning Shape Contexts for Robust 3D Point Cloud Reconstruction"__ introduces Snowflake Point Deconvolution (SPD), which leverages skip-transformer to model the relation between parent points and child points. 
FB-Net Liu et al., "FB-Net: Feedback Mechanism for 3D Point Cloud Completion"__ adopts the feedback mechanism during refinement and generates points recurrently.
LAKe-Net Zhang et al., "LAKe-Net: Learning Shape Priors from Surface-Skeleton Representation"__ integrates its surface-skeleton representation into the refinement stage, which makes it easier to learn the missing topology part.
Another type of method tends to preserve and exploit the local information in partial input.
One direct approach is to predict the missing points by combining the results with partial input data Wang et al., "Point Completion Network with Partial Input Data"__. As the point set can be viewed as a token sequence, PoinTr Liang et al., "PoinTr: Point Cloud Transformer for 3D Shape Completion"__ and its following works Liu et al., "PointCloud Transformer for 3D Shape Completion"__ employ the transformer architecture Zhang et al., "Transformer Architecture for 3D Point Cloud Processing"__ to predict the missing point proxies.
SeedFormer Liang et al., "SeedFormer: A Shape Representation Learning Framework"__ introduces a shape representation called patch seeds for preventing the loss of local information during pooling operation. 
Some other approaches Liu et al., "Point Completion Network with Patch Seeds"__ propose to enhance the generated shapes by exploiting the structural relations in the refinement stage.
A recent work Zhang et al., "Learning Shape Contexts for Robust 3D Point Cloud Reconstruction"__ shares a similar idea with our \emph{Similarity Alignment} unit of utilizing self-similarities property and directly learns geometric transformations for input points.
However, these strategies employ a unified refinement strategy for all points, which limits their ability to generate pleasing details for different points.
Our approach differs from theirs by breaking down the shape refinement task into two sub-goals, and adaptively extracting reliable features for different partial areas.

\subsection{View-based Methods for 3D Understanding}
View-based 3D understanding techniques have gained significant attention in recent years.
The classic Multi-View Convolutional Neural Network (MVCNN) model was introduced by Su et al., "Multi-view convolutional neural network for 3D object recognition"__, where color images are fed into a CNN and subsequently combined by a pooling operation. However, this approach has the fundamental drawback of ignoring view relations.
Following works Liang et al., "Learning to rank views for 3D object recognition"__ propose various strategies to tackle this problem. For example,
Li et al., "View-based 3D object representation learning"__ obtains a discriminative 3D object representation by modeling region-to-region relations.
LSTM is also used to build the inter-view relations Zhang et al., "Inter-View Relation Learning for 3D Object Recognition"__.
Since the cross-modal data are more available, methods are proposed to fuse features of views and point clouds.

However, compared to their 2D counterpart, most of 3D recognition methods are limited by the scarcity of training data. Therefore, in recent years, there has been a surge in research focusing on enhancing 3D understanding through the utilization of pre-trained 2D models.
P2P Li et al., "Prompting Pre-Trained Image Models for 3D Object Recognition"__ prompts a pre-trained image model by the geometry-preserved projection.
PointCLIP Zhang et al., "PointCLIP: Zero-Shot Point Cloud Understanding via CLIP-Fusion"__ and its following work Liu et al., "Zero-Shot Point Cloud Understanding via CLIP-Fusion"__ achieve zero-shot point cloud understanding by sending multi-view 2D projections to CLIP Duan et al., "CLIP: A Contrastive Learning-based Image Encoder for 3D Object Recognition"__ for a well-learned representation. To adapt the power of CLIP to more 3D scenarios, alternative methods Liang et al., "Cross-Modal Contrastive Learning for 3D Object Recognition"__ employ cross-modal contrastive learning to train a powerful 3D encoder.
Recent approaches explore masked autoencoders using both 3D and 2D data Wang et al., "Masked Autoencoder for 3D Point Cloud Processing"__. Some approaches Liang et al., "Color Image Generation for 3D Object Recognition"__ pre-train a 3D encoder through color image generation. 
Inspired by the success of view-based 3D understanding techniques, our method utilizes point cloud features to enhance relationships between multiple views obtained by self-augmentation.

\subsection{Semantic Scene Completion}
Given an incomplete observation, semantic scene completion (SSC) aims to reconstruct and assign semantic labels to a 3D scene.
The prevailing approach in SSC involves utilizing the voxel grid as the primary 3D representation. Pioneering the field, SSCNet Li et al., "SSCNet: A Deep Learning Framework for Semantic Scene Completion"__ addresses this task through an end-to-end network constructed with 3D Convolutional Neural Networks. Subsequent methodologies enhance this framework by incorporating group convolution Zhang et al., "Group Convolution for Semantic Scene Completion"__, multimodal fusion Liang et al., "Multimodal Fusion Network for Semantic Scene Completion"__, GAN Wang et al., "Generative Adversarial Networks for Semantic Scene Completion"__, and boundary learning Liu et al., "Boundary Learning for Semantic Scene Completion"__. 
SISNet Zhang et al., "SISNet: A Novel Framework for Semantic Scene Completion"__ tackles SSC by iteratively performing grid-based scene completion and point cloud object completion.
% 
To accommodate diverse autonomous driving scenes, recent research has primarily focused on outdoor LiDAR datasets and explored various input modalities.
LiDAR-based approaches Li et al., "LiDAR-Based 3D Scene Understanding"__ commonly integrate point-wise features within voxel space to enhance scene understanding. Meanwhile, alternative methods have emerged to predict scene occupancy using monocular images. The pioneering work MonoScene Wang et al., "MonoScene: A Novel Framework for Monocular Image-based 3D Scene Understanding"__ addresses this problem with successive U-Nets and a novel feature projection module. Building on this foundation, Liu et al., "Two-Stage Transformer Architecture for Monocular Image-based 3D Scene Understanding"__ introduces a two-stage transformer architecture. 
NDC-Scene Zhang et al., "NDC-Scene: Normalized Device Coordinates-based 3D Scene Completion"__ further improves performance by reducing feature ambiguity in the normalized device coordinates space. Symphonies Liu et al., "Symphonies: A Novel Framework for Instance-Centric Semantic Scene Completion"__ advances this line of research by incorporating instance-centric semantics and scene context through the use of instance queries.
More recently, images captured by multiple cameras are also utilized for accurate 3D understanding Liang et al., "Multi-View Stereo for 3D Scene Understanding"__.
% 
Different from these methodologies, point-based SSC methods Wang et al., "Point-Based Semantic Scene Completion"__ focus on reconstructing a complete scene point cloud along with semantic labels assigned to each point.
____ attach an extra segmentation network to the point cloud completion model.
CasFusionNet Liang et al., "CasFusionNet: A Novel Framework for Cascaded Fusion of Point Cloud and Image Features"__ takes a hierarchical approach, merging features from completion and segmentation modules.
In this work, we extend PointSea to SSC by jointly predicting semantic labels and geometric details through the proposed SDG.