%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
\documentclass[twocolumn]{svjour3}          % twocolumn

%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage[utf8x]{inputenc}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepackage{tikz}
\usetikzlibrary{backgrounds}
\usepackage{color}
\usepackage{xcolor}
\usepackage{booktabs,siunitx} % better tables
\usepackage{makecell}
% \usepackage{enumitem}
\usepackage[caption=false]{subfig}
\usepackage[noadjust]{cite}
\usepackage[colorlinks=true,allcolors=green]{hyperref}
\usepackage{rotating}

%copied from spoken language paper
\usepackage{times}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{arydshln}
\usepackage{adjustbox}
\usepackage{array,multirow}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{epsfig}
% \usepackage{subfigure}
\usepackage{paralist}
\usepackage[english]{babel}

\usepackage{authblk}
\usepackage{diagbox}

%% Bibliography stuff.
\usepackage{natbib}
\setcitestyle{authoryear,open={(},close={)}}
% New commands.

\renewcommand{\labelitemi}{\textbullet}

\begin{document}

\title{PointSea: Point Cloud Completion via Self-structure Augmentation}

\titlerunning{PointSea: Point Cloud Completion via Self-structure Augmentation}

\author{Zhe Zhu   \and
        Honghua Chen \and
        Xing He \and
        Mingqiang Wei
}
%\authorrunning{Short form of author list} % if too long for running head

\institute{              
                Zhe Zhu \at zhuzhe0619@nuaa.edu.cn \\ \\
                Honghua Chen$^{\dag}$ \at chenhonghuacn@gmail.com \\ \\
                Xing He \at hexing@nuaa.edu.cn \\ \\
                Mingqiang Wei$^{\dag}$ \at mqwei@nuaa.edu.cn \\ \\
              Nanjing University of Aeronautics and Astronautics \\
              Nanjing, China \\ \\
              $^{\dag}$ Corresponding authors
%              
%             \emph{Present address:} of F. Author  %  if needed
}
\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor

\maketitle


\begin{abstract}
\sloppy{  
Point cloud completion is a fundamental yet not well-solved problem in 3D vision.
Current approaches often rely on 3D coordinate information and/or additional data (e.g., images and scanning viewpoints) to fill in missing parts.  
Unlike these methods, we explore self-structure augmentation and propose \textbf{PointSea} for global-to-local point cloud completion. 
In the global stage, consider how we inspect a defective region of a physical object, we may observe it from various perspectives for a better understanding.
Inspired by this, PointSea augments data representation by leveraging self-projected depth images from multiple views. To reconstruct a compact global shape from the cross-modal input, we incorporate a feature fusion module to fuse features at both intra-view and inter-view levels.
%
In the local stage, to reveal highly detailed structures, we introduce a point generator called the self-structure dual-generator. This generator integrates both learned shape priors and geometric self-similarities for shape refinement. Unlike existing efforts that apply a unified strategy for all points, our dual-path design adapts refinement strategies conditioned on the structural type of each point, addressing the specific incompleteness of each point. 
Comprehensive experiments on widely-used benchmarks demonstrate that PointSea effectively understands global shapes and generates local details from incomplete input, showing clear improvements over existing methods. 
Our code is available at \emph{\textcolor{magenta}{https://github.com/czvvd/SVDFormer\_PointSea}}.
}
\keywords{PointSea \and Point cloud completion \and Self-structure augmentation \and Cross-modal fusion}
\end{abstract}

%% main text
\input{intro}
\label{secIntroduction}

\input{related_work}
\label{secRelatedWork}
%
\input{method}
\label{secOverview}
%

\input{experiment}
\label{secEXP}

%
\input{conclusion}
\label{secCon}

% \vspace{-2cm}
\section*{Acknowledgements}
This work was supported by the National Natural Science Foundation of China (No. T2322012, No. 62172218, No. 62032011), the Shenzhen Science and Technology Program (No. JCYJ20220818103401003, No. JCYJ20220530172403007), and the Guangdong Basic and Applied Basic Research Foundation (No. 2022A1515010170).

\noindent\textbf{Data Availability}
All synthetic datasets can be accessed at \emph{\textcolor{magenta}{https://github.com/yuxumin/PoinTr/blob/master/DATASET.md}}. All real-world data can be accessed at \emph{\textcolor{magenta}{https://github.com/xuelin-chen/pcl2pcl-gan-pub}}. The scene datasets are publicly available at \emph{\textcolor{magenta}{https://github.com/JinfengX/CasFusionNet}}.

{\small
    \bibliographystyle{spbasic}
    \bibliography{pcd}
}

\end{document}
