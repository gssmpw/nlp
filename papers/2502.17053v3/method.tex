\section{Method}
\begin{figure*}[h]
  \centering
  \includegraphics[width=0.96\textwidth]{fig/ViewAugment.pdf}
\caption{Illustration of feature fusion module. The features from cross-modal input are fused at both inter-view and intra-view levels.}
  \label{fig:VA}
\end{figure*}
The input of our PointSea consists of three parts: a partial and low-res point cloud $P_{in}\subseteq\mathbb{R}^{N\times3}$, $N_V$ camera locations $VP\subseteq\mathbb{R}^{N_V\times3}$ (three orthogonal views in our experiments), and $N_V$ depth maps $D\subseteq\mathbb{R}^{N_V\times1\times H \times W}$.
Given these inputs, our goal is to estimate a complete point cloud $P_{2}\subseteq\mathbb{R}^{N_2\times3}$ in a coarse-to-fine manner.
The overall architecture is exhibited in Fig.~\ref{fig:overview}, which comprises two parts: an SVFNet and a refiner equipped with two SDG modules. 
SVFNet first leverages multiple self-projected depth maps to produce a globally completed shape $P_0\subseteq\mathbb{R}^{N_0\times3}$. Subsequently, two SDGs gradually refine and upsample $P_0$ to yield the final point cloud $P_{2}$, which exhibits geometric structures with high levels of detail.
Note that unlike some recent cross-modal approaches~\citep{zhang2021view,zhu2023csdn,aiello2022crossmodal,zhang2022shape}, our method makes full use of self-structures and does not require any additional paired information such as color images with precisely calibrated camera intrinsic parameters~\citep{zhang2021view,zhu2023csdn}. 
Depth maps are obtained by projecting point clouds themselves from controllable viewpoints. In contrast to the vanilla perspective projection used in SVDFormer~\citep{Zhu_2023_ICCV}, we adopt the dense projection method proposed in \citep{PointCLIPV2}, which generates smooth depth values.



\subsection{SVFNet}
\subsubsection{Overview}
The SVFNet aims to observe the partial input from different viewpoints and learns an effective descriptor to produce a globally plausible and complete shape. We first extract a global feature $F_\emph{P}$ from $P_{in}$ using a point-based 3D backbone network and a set of view features $F_V\subseteq\mathbb{R}^{N_V\times H^{\prime} \times W^{\prime} \times C}$ from the $N_V$ depth maps using a CNN-based 2D backbone network, where $H^{\prime}=H/32$ and $W^{\prime}=W/32$. 
% 

However, how to effectively fuse the above cross-modal features is challenging.
In our early experiments, we directly concatenate these features, but the produced shape is less pleasing (see the ablation studies in Section~\ref{ablationSec}). This may be caused by the domain gap between 2D and 3D representations.
To resolve this problem, we propose a new feature fusion module, to fuse $F_\emph{P}$ and $F_V$, and output a global shape descriptor $F_g$, followed by a decoder to generate the global shape $P_c$. The decoder uses a 1D Conv-Transpose layer to transform $F_g$ to a set of point-wise features and regresses 3D coordinates with a self-attention layer~\citep{vaswani2017attention}.
Finally, we follow a similar approach to previous studies~\citep{9928787,zhou2022seedformer} by merging $P_c$ and $P_{in}$ and then resampling the merged output to generate the coarse result $P_0$.


\subsubsection{Feature Fusion}
Fig.~\ref{fig:VA} illustrates the detailed process of feature fusion. In SVDFormer~\citep{Zhu_2023_ICCV}, only intra-view fusion was conducted. In our method, the fusion process is divided into two stages.
% Due to the nature of self-projection, the depth images have blank areas in both the background and missing regions, posing a challenge in perceiving the missing parts. 
First, we enhance the view-wise features by injecting 3D shape information, where the patch-wise features of the $i$-th view, denoted as  $f_{Vi}\subseteq\mathbb{R}^{H^{\prime} \times W^{\prime} \times C}$, are independently fused with $F_\emph{P}$.
Drawing inspiration from the vision transformer~\citep{dosovitskiy2021an}, we treat the patched features $f_{Vi}$ as a set of tokens with a length of $H^{\prime} \times W^{\prime}$. Specifically, $f_{Vi}$ is transformed into query, key, and value tokens via linear projection under the guidance of the global shape feature $F_\emph{P}$. We then calculate attention weights and perform an elemental-wise product to obtain refined features and apply maximum pooling along the spatial dimensions to derive the view-wise feature $F_{Vg}\subseteq\mathbb{R}^{N_V \times C}$. 
By fusing 3D shape information into the local 2D patches, $F_{Vg}$ serves as a good initialization for the subsequent intra-view fusion.
% $F_{Vg}$ is then fused with $F_\emph{P}$ during the intra-view stage. 
The second fusion is similar to the first stage, with the main difference being that, in the intra-view stage, the fusion takes place among $N_V$ views. Besides, to enhance the discriminability of view features, attention weights are calculated based on the query and key tokens conditioned on the projected viewpoints $VP$. We map $VP$ into the latent space through a linear transformation and use them as positional signals. Following an elemental-wise product, each feature in $F^\prime_V$ incorporates the relational information from other views under the guidance of $F_\emph{P}$.
Finally, the output shape descriptor $F_g$ is derived from $F^\prime_V$ via max pooling.

\subsection{SDG}
The SDG seeks to generate a set of coordinate offsets to fine-tune and upsample the coarse shape, based on the structural type of the missing surface region.
To achieve it, SDG is designed as a dual-path architecture as shown in Fig.~\ref{fig:IDTr}, which consists of two parallel units named \emph{Structure Analysis} and \emph{Similarity Alignment}, respectively. 
Overall, fed with the partial input $P_{in}$, coarse point cloud $P_{l-1}$, and the offset feature $F_{l-1}$ from the previous step, we obtain a point-wise feature $F_{l}^{\prime}$. 
Denoting $r$ as the upsampling rate, $F_{l}^{\prime}$ is then projected to a higher dimensional space and reshaped to the offset feature $F_{l}\subseteq\mathbb{R}^{rN\times C}$, which will be sent to the next SDG.
Finally, we produce a set of up-sampled offsets $O_{l}\subseteq\mathbb{R}^{rN\times 3}$ through an MLP and further add it back to $P_{l-1}$ to obtain a new completion result. It is noteworthy that the SDG iteration is conducted twice, as illustrated in Fig.~\ref{fig:overview}.

\begin{figure*}[h]
  \centering
  \includegraphics[width=\textwidth]{fig/SDG.pdf}
\caption{The architecture of SDG. The upper path represents Structure Analysis and the lower path represents Similarity Alignment. Each sub-network generates an offset feature which is then combined using a Path Selection module and used to regress into the coordinate offsets.}
  \label{fig:IDTr}
\end{figure*}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{fig/Vis_incompleteness.pdf}
\caption{Visualization of incompleteness degree of a reconstructed point cloud and its corresponding partial observation.}
  \label{fig:Vis_incom}
\end{figure}

\subsubsection{Structure Analysis}
Since detailed geometries from missing regions are harder to recover, we embed an incompleteness-aware self-attention layer to explicitly encourage the network to focus more on the missing regions. Specifically, $P_{l-1}$ is first concatenated with the shape descriptor $F_g$, and then embedded into a set of point-wise feature $F_C=\{f_i\}^{N_{l-1}}_{i=1}$ by a linear layer. 
Next, $F_C$ is fed to the incompleteness-aware self-attention layer to obtain a set of features $F_Q=\{q_i\}^{N_{l-1}}_{i=1}$, which encodes the point-wise incompleteness information. 
$q_i$ is computed by:
\begin{equation} \label{eqn1}
  \begin{split}
   q_i &= \sum_{j=1}^{N_{l-1}}{a_{i,j}(f_jW_V)},\\
   a_{i,j} &= Softmax((f_iW_Q+h_i)(f_jW_K+h_j)^T),
  \end{split}
\end{equation}
where $W_Q$, $W_K$, and $W_V$ are learnable matrix with the size of $C \times C$. $h_i$ is a vector that represents the degree of incompleteness for each point $x_i$ in $P_{l-1}$. 
Intuitively, points in missing regions tend to have a larger distance value to the partial input. 
Thus, we define the incompleteness degree of a point as its distance to the closest point in the partial input. We visualize the incompleteness of a reconstructed point cloud in Fig.~\ref{fig:Vis_incom}, which indicates that points in the missing areas typically have large incompleteness values. This information is embedded as follows:
\begin{equation} \label{eqn2}
  \begin{split}
   h_i &= Sinusoidal(\frac{1}{\gamma}\min_{y \in P_{in}}\lvert \lvert x_i - y \lvert\lvert)
  \end{split},
\end{equation}
where $\gamma$ is a scaling coefficient. We set it to 0.2 in our experiment and use the sinusoidal function~\citep{vaswani2017attention} to ensure that $h_i$ has the same dimension as the embeddings of query, key, and value. We then decode $F_Q$ into $F_{Q}^{\prime}$ for further analysis of the coarse shape.

% \vspace{-0.2cm}
\subsubsection{Similarity Alignment}
The \emph{Similarity Alignment} unit exploits the potential similar local pattern in $P_{in}$ for each point in $P_{l-1}$. 
Inspired by the point proxies in \cite{yu2021pointr}, we begin by using three EdgeConv layers~\citep{wang2019dynamic} to extract a set of downsampled point-wise feature $F_{in}$. Each vector in $F_{in}$ captures the local context information.
Given the potential existence of long-range similar structures, we then perform feature exchange through cross-attention, a classical solution for feature alignment.
The calculation process is similar to vanilla self-attention with the query matrix produced by $F_Q$ and $F_{in}$ serving as the key and value vectors.
The cross-attention layer produces point-wise features denoted as $F_H\subseteq\mathbb{R}^{N_{l-1}\times C}$, which integrates similar local structures in $P_{in}$ for each point in the coarse shape $P_{l-1}$. 
In this way, this unit can model the geometric similarity between two point clouds, thereby facilitating the refinement of points with similar structures in the input. 
Similar to the structure analysis unit, $F_H$ is also decoded into a new feature $F_{H}^{\prime}$. Both decoders have the same architecture implemented with self-attention layers~\citep{vaswani2017attention}.


\subsubsection{Path Selection}
There are two rough kinds of sources of shape information in the dual-path design: one derived from learned shape priors and the other from the similar geometric patterns found within $P_{in}$. 
In SVDFormer~\citep{Zhu_2023_ICCV}, the point-wise feature $F_{l}^{\prime}$ was obtained through the concatenation of features from the two paths. 
However, the significance of these two units varies across different missing regions. For instance, in certain missing regions, the input observation exhibits nearly identical geometry due to properties such as symmetry. In such scenarios, information derived from the \emph{Similarity Alignment} unit plays a more significant role for these points. Motivated by this observation, this paper integrates these two feature types via a path selection module, which adaptively chooses the more crucial feature. 

This module is designed to dynamically select more essential information for each point. The core idea involves controlling information flow through gates that integrate global context from both the current and previous refinement steps.
Specifically, $F_{l}^{\prime}$ is computed by:
\begin{equation} \label{eqn3}
  \begin{split}
   F_{l}^{\prime} &= \alpha F_{Q}^{\prime} + (1-\alpha) F_{H}^{\prime},\\
   \alpha &= Sigmoid(MLP([F_{Q}^{\prime}+F_{H}^{\prime},F_{l-1},Max(F_Q)])),
  \end{split}
\end{equation}
where $F_{l-1}$ is the offset feature from the last SDG.
The efficacy of connecting multiple refinement steps for integrating spatial relationships has been consistently demonstrated in prior works~\citep{9928787,zhou2022seedformer,yan2022fbnet}. In our approach, we intend to leverage the spatial splitting information from the previous step to guide the selection of different features in the current step.
We also apply max pooling to $F_Q$ and utilize the incompleteness-aware global feature to assist the selection.
Note that in the first SDG, $F_{l-1}$ is not involved in the path selection module due to its absence.

\subsection{Loss Function}
To measure the differences between the generated point cloud and the ground truth $P_{gt}$, we use the Chamfer Distance (CD) as our loss function, which is a common choice in recent works. 
To facilitate the coarse-to-fine generation process, we regularize the training by computing the loss function as:
\begin{equation} \label{eqnloss}
    \mathcal{L}= \mathcal{L}_{CD}(P_c,P_{gt}) + \sum_{i = {1,2}}\mathcal{L}_{CD}(P_i,P_{gt}),
\end{equation}
where $\mathcal{L}_{CD}$ is defined as
\begin{equation}\label{eqnCD}
    \mathcal{L}_{CD}(X,Y)=\frac{1}{\lvert X \rvert}\sum_{x \in X}\min_{y \in Y} \lvert \lvert x-y \rvert \rvert + \frac{1}{\lvert Y \rvert}\sum_{y \in Y}\min_{x \in X} \lvert \lvert y-x \rvert \rvert.
\end{equation}
Note that we downsample the $P_{gt}$ to the same density as $P_c$, $P_1$, and $P_2$ to compute the corresponding losses. 

