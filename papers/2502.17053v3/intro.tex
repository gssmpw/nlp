\section{Introduction}
%As a commonly used 3D representation, 
Raw-captured point clouds are often incomplete due to factors like occlusion, surface reflectivity, and limited scanning range (see Fig. \ref{fig:incomplete}). Before they can be used in downstream applications (e.g., digital twin), they need to be faithfully completed, a process known as point cloud completion. Recent years have witnessed significant progress in this field~\citep{yuan2018pcn,huang2020pf,zhang2020detail,yu2021pointr,9928787,yan2022fbnet,zhang2022shape,tang2022lake,zhou2022seedformer,zhang2022point,10232862,SCRN}. 
However, the sparsity and large structural incompleteness of point clouds still limit their ability to produce satisfactory results. There are two primary challenges in point cloud completion:
\begin{itemize}
    \item Crucial semantic parts are often absent in the partial observations.
    \item Detailed structures cannot be effectively recovered.
\end{itemize} 


\begin{figure*}[h]
    \hsize=\textwidth
    \centering
    \includegraphics[width=\textwidth]{fig/teaser-new.pdf}
    \caption{We explore Self-structure Augmentation, a novel strategy for high-quality global-to-local point cloud completion, dubbed PointSea.  (a) In the global stage, PointSea understands incomplete shapes from self-projected multiple views. (b) In the local stage, PointSea collaborates both similar geometric similarities in input (red boxes) and learned shape priors (green boxes) for shape refinement. (c) PointSea shows clear improvements over its competitors, including SeedFormer~\citep{zhou2022seedformer}, AdaPoinTr~\citep{10232862}, and SVDFormer~\citep{Zhu_2023_ICCV}.}
    \label{fig:teaser}
\end{figure*}
\begin{figure*}[h]
    \hsize=\textwidth
    \centering
    \includegraphics[width=\textwidth]{fig/incomplete.pdf}
    \caption{The original point cloud of large and complex objects is often incomplete due to many factors. Point cloud completion plays an essential role in extensive practical applications.}
    \label{fig:incomplete}
\end{figure*}


The first challenge leads to a vast solution space for point-based networks~\citep{yuan2018pcn,9928787,yu2021pointr,zhou2022seedformer} to robustly locate missing regions and create a partial-to-complete mapping.
Some alternative methods attempt to address this issue by incorporating additional color images~\citep{zhang2021view,aiello2022crossmodal,zhu2023csdn} or viewpoints~\citep{zhang2022shape,Gong_2021_ICCV,fu2023vapcnet}. 
However, the paired images with well-calibrated camera parameters are hard to obtain, as well as the scanning viewpoints.
% The second challenge hinders the detailed reconstruction of various missing regions. 
To resolve the second challenge, some recent approaches~\citep{9928787,yan2022fbnet} utilize skip-connections between multiple refinement steps, allowing them to iteratively recover finer details with learned shape pattern priors. Some other methods prioritize preserving the original detail information by no-pooling encoding~\citep{zhou2022seedformer} or structural relational enhancement~\citep{10106495}.
However, all the above approaches typically employ a unified refinement strategy for all surface regions, making it difficult to infer various detailed structures. By observing and analyzing different kinds of partial point clouds, we find that the missing surface regions can be classified into two types.
The first type lacks similar structures in the input shape, and their reconstruction heavily relies on the learned shape prior.
The second type is consistent with the local structures present in the partial input, and their recovery can be facilitated by appropriate geometric regularity~\citep{zhao2021sign}. 
For instance, large-scale LiDAR scans are notably sparse and contain limited information for generating fine details. Indoor depth scans, conversely, capture more semantic information, consequently providing richer geometric cues for completion.

Based on the above observations, we propose a novel neural network for point cloud completion called PointSea.
Our method improves the global-to-local paradigm by fully leveraging self-structure information. 

First, similar to how a human would perceive and locate the missing areas of a physical object by observing it from different viewpoints, we aim to drive the neural network to absorb human knowledge by augmenting the data representation. To achieve it, we design a Self-view Fusion Network (SVFNet) that learns an effective descriptor, well depicting the global shape from both the point cloud data and depth maps captured from multiple viewpoints (see Fig.~\ref{fig:teaser} (a)). To better exploit such kind of cross-modal information, we introduce a feature fusion module to improve the discriminative power of multi-view features.

% the second one: refinement--SDG
Regarding the second challenge, our insight is to disentangle refinement strategies conditioned on the structural type of each point to reveal detailed geometric structures.
Therefore, we design a Self-structure Dual-Generator (SDG) with a pair of parallel refinement units, called \emph{Structure Analysis} and \emph{Similarity Alignment}, respectively. 
As depicted in Fig.~\ref{fig:teaser} (b), two units function distinctly during refinement.
The former unit analyzes the generated coarse point clouds by explicitly encoding local incompleteness, enabling it to match learned geometric patterns from training data to infer underlying shapes.
The \emph{Similarity Alignment} unit finds the features of similar structures for every point, thus making it easier to refine its local region by mimicking the input's local structures. 
With this dual-path design, our method can generate reasonable results for various types of input shapes, including symmetrical synthetic models with different degrees of incompleteness and highly sparse real-world scans.

% experiment result
We conduct comprehensive evaluations of PointSea across multiple datasets, including PCN, ShapeNet-55/34, Projected-ShapeNet-55/34, KITTI, ScanNet, and Matterport3D, with a specific focus on object shape completion. Additionally, we extend our analysis to assess the performance of PointSea for semantic scene completion. Experimental results demonstrate that PointSea achieves state-of-the-art performance on these benchmarks.
Our contributions are listed below:
\begin{itemize}
  \item 
  We propose a novel point cloud completion network based on self-structure augmentation, which significantly improves the recovery of both global shapes and local details in point clouds.
  \item 
  We propose a Self-view Fusion Network (SVFNet) to enhance multi-view and cross-modal features, resulting in a more plausible global shape.
  \item 
  We propose a Self-structure Dual-Generator (SDG) to refine the global shape. This component handles various kinds of incomplete shapes by jointly learning local pattern priors and self-similarities.
\end{itemize}

A preliminary version of this work was presented at \textit{ICCV 2023}~\citep{Zhu_2023_ICCV}. We extend the conference version by introducing the following new contributions:
%

\begin{itemize}
  \item While SVDFormer~\citep{Zhu_2023_ICCV} has already demonstrated commendable performance, some of the design choices are sub-optimal. Therefore, we propose an improved version of SVDFormer, called PointSea. For example, in the global stage, SVDFormer considers feature fusion at only the intra-view level. To achieve a more enriched cross-modal feature representation, PointSea incorporates a feature fusion strategy at both inter-view and intra-view levels. In the local refinement stage, SVDFormer assigns equal weight to two refinement paths and combines their outputs directly. However, as discussed earlier, we acknowledge that the significance of these two paths varies across different missing regions. Therefore, we propose a path selection module to adaptively aggregate diverse features. These new designs in PointSea lead to a significant performance enhancement compared to its predecessor.

 \item We conduct a wider series of experiments, specifically focusing on real-world conditions such as point cloud semantic scene completion and a more systematic evaluation of real-world object scans. Finally, we provide more in-depth empirical analyses and visualizations.
\end{itemize}