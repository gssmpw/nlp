\input{file/main/alg_pipeline.tex}
\section{Empirical Approach and Experiments}\label{main:experiments}
In Sections \ref{main:experiments:technical_approach}--\ref{main:experiments:setting}, we provide technical details about our empirical approach and settings of our experiments.
Then in Sections \ref{main:experiments:experiment_results}--\ref{main:experiments:discussions}, we present experimental results to demonstrate both the effectiveness and efficiency of our method.
% @TODO say sth more about across what models,


\subsection{Reflection-Window Decoding: Technical Details}\label{main:experiments:technical_approach}
Our findings through both the theoretical characterization of sub-optimality in autoregressive decoding for text generation (Section~\ref{main:theory}), and the sanity check with empirical verifications in semi-synthetic settings (Section~\ref{main:synthetic}), suggest the necessity of a built-in reflection-and-refine mechanism at the decoding level.
To empirically address this issue, we propose a selective refinement framework that interchangeably refine and generate as the response unfolds.

Text typically unfolds in a single direction, i.e., from the start to the end, with words, phrases, and sentences.
This differentiates text from other forms of objects that occupy multiple dimensional spaces, e.g., images or videos.
Taking advantage of this one-dimensional nature, our decoding framework integrates a sliding reflection window along with two additional modules: (1) a \emph{pausing criterion} that specifies whether we should pause the generation upon reflecting on generated content, and (2) a \emph{refinement/correction method} that facilitates revision at the decoding level (if the pausing criterion is triggered).
We present the pseudocode of our reflection-window decoding approach in Algorithm~\ref{alg:overall_pipeline}.

% \vspace{-2ex}
\paragraph{Pausing Criterion}
\looseness=-1
Guided by our theoretical characterization (Theorem~\ref{thm:small_conditional_prob}), the reflection at the decoding level needs to capture the (increasing trend of) uncertainty as text generation proceeds.
For an empirical pausing criterion, we use the conditional entropy $H(\cdot)$ based on the next-token logits across the vocabulary.
Specifically, given an LLM which models the conditional distribution $g(X_{t} \mid X_{1:t-1})$ of the token at $t$-th step given all the observed history $X_{1:t-1}= \xbf_{1:t - 1}$, we use the pausing criterion $h(t; \sigma, d)$:
\begin{equation}
    \small
    h(t; \sigma, d) =
    \begin{cases}
        \text{True}  & \text{if }~H\big( X_{t - i} \mid X_{1:(t - i - 1)} \big) > \sigma~, \\
                     & \forall i \in [0,d-1],                                              \\
        \text{False} & \text{Else},
    \end{cases}
\end{equation}
where $\sigma$ denotes the hyperparameter for the threshold of conditional entropy, and $d$ denotes that for the window size (how far we look back in history, in terms of the token counts).

\looseness=-1
When $h(t^{\mathrm{(fast)}}; \sigma, d)$ is True, the pausing criterion (denoted by $\mathrm{IfPause(\cdot)}$ in Algorithm~\ref{alg:overall_pipeline}, with hyperparameter enclosed) is triggered upon reflecting on the most recent $d$ generated tokens, i.e., the length-$d$ reflection window when the fast pointer is at $t^{\mathrm{(fast)}}$.
The two parameters, $\sigma$ and $d$, jointly decide the sensitivity and effective region of the pausing criterion, and we present more discussions in Section~\ref{main:experiments:entropy_window_size}.

% \vspace{-2ex}
\paragraph{Refinement/Correction Method}
When the pausing criterion is triggered, tokens within the current sliding reflection window need to be refined or corrected.
Since beam search can approximate the global optimum relatively well, empirically we introduce a fixed-length beam search to generate a new segment with a length $d$ (denoted by the hyperparameter-enclosed function $\mathrm{ReGenerate(\cdot)}$ in Algorithm~\ref{alg:overall_pipeline}), to replace the content within the reflection window.
After the refinement, the slow pointer $t^{\mathrm{(slow)}}$ catches up with the fast one $t^{\mathrm{(fast)}}$ and the model proceeds with generation while maintaining the sliding reflection window.

% \vspace{-2ex} 
\paragraph{Remark: Versatile Decoding Framework}
We would like to note that our reflection-window decoding approach is highly versatile.
While our empirical approach employs a specific pausing criterion and refinement/correction method, practitioners can customize these components by incorporating different functions, namely, $\mathrm{IfPause(\cdot)}$ and $\mathrm{ReGenerate(\cdot)}$ in Algorithm~\ref{alg:overall_pipeline}, to meet their needs.
Our selective refinement framework integrates the sliding reflection window mechanism with these components, enabling simultaneous refinement and generation at the decoding level while retaining the flexibility to incorporate additional strategies, such as those based on high-level model behaviors and/or speculative decoding (Section~\ref{main:motivation:reflection_window}).


\subsection{Experiment Settings}\label{main:experiments:setting}
We provide technical details about settings of our experiments, including models, benchmarks, evaluation metrics, and baseline methods.

\paragraph{LLM Models}
We conduct experiments using multiple models across different families/herds.
Specifically, we use Llama-3.1-8B-Instruct (denoted as Llama3.1-8B), which belongs to the Llama 3.1 herds \citep{meta2024llama}, Phi-3-Medium-128K-Instruct \citep{abdin2024phi} (denoted as Phi3-Medium) with 14 billion parameters, Qwen2.5-14B-Instruct \citep{qwen2} (denoted as Qwen2.5-14B) with 14 billion parameters, Qwen2.5-7B-Instruct (denoted as Qwen2.5-7B) with 7 billion parameters, and Mistral-Nemo-Instruct-2407 \citep{mistralNemo} (denoted as Mistral-Nemo) with 12 billion parameters.


\paragraph{Benchmarks and Evaluation Metrics}
Our experiments are conducted on MMLU \citep{hendrycks2020measuring} and MT-Bench \citep{zheng2023judging}.
MMLU tests model's general knowledge across 57 diverse subjects, e.g., humanities, STEM (Science, Technology, Engineering, and Mathematics), and social sciences, at varying difficulty levels, making it a comprehensive evaluation of model's reasoning performance and factual knowledge.
MT-Bench, on the other hand, provides a fine-grained evaluation through multi-turn conversational tasks, evaluating not just correctness, but also coherence and fluency.

For MMLU, we adopt the macro averaged accuracy metric because the number of questions varies across subjects.
For MT-Bench, for each pair of responses, we prompt the LLM judge (for which we use GPT-4o, \citealt{openai2024gpt4o}) with two responses following the prompting method outlined in \citet{zheng2023judging}.
The LLM judge should return a decision from three options: \texttt{win}, \texttt{lose} or \texttt{tie}.
To avoid the influence from the preference bias, for each pair we prompt the LLM judge twice with responses placed in different sequences.
When a response gets two \texttt{win}'s (\texttt{lose}'s) or one \texttt{win} (\texttt{lose}) plus one \texttt{tie}, we record the response as (not) prevailing.
The rest situations are treated as \texttt{tie}.
We use win rate as the measurement of performance on MT-Bench, calculated by $\text{win rate} \coloneqq \frac{\text{number of wins}}{\text{number of wins}+\text{number of losses}}$.

% We employ GPT-4o to score the responses, selecting the one with the higher score as the winner.

\paragraph{Baseline Methods}
We compare reflection-window decoding with three baseline methods: greedy decoding, (full-scale) beam search, and Top-$k$/Top-$p$ sampling \citep{fan2018hierarchical,holtzman2020curious}.
We consider greedy search as one baseline method since it corresponds closely to our theoretical analysis.
However, since the widely usage of Top-$p$ and Top-$k$ sampling (and often used in combination), we also include it as a baseline approach.
Reflection-window decoding only leverages beam search when necessary, i.e., when $\mathrm{IfPause(\cdot)}$ in Algorithm~\ref{alg:overall_pipeline} is triggered.
For fair comparison, we set the beam size to $4$ in both reflection-window decoding and the full-scale beam search.

\input{file/tables/MMLU_acc_phi3}

\begin{figure}[t]
    \centering
    \includegraphics[height=20ex]{file/figures/win_rate_comparison_squeeze.pdf}
    \caption{%\small
        Comparison of win rate (against greed decoding) on MT-Bench across categories}
    \label{fig:winrate_comparison}
\end{figure}

\subsection{Experiment Results}\label{main:experiments:experiment_results}
We compare the performance among greedy decoding, beam search, Top-$k$/Top-$p$ sampling, and our reflection-window decoding.
We use an entropy threshold of $\sigma=0.5$ and a window size of $d=4$ in reflection-window decoding.

\paragraph{MMLU}
\looseness=-1
Table~\ref{tab:mmlu_category_phi3} presents the comparison among greedy decoding, beam search, and reflection-window decoding (with greedy search as the ``regular decoding'' in Algorithm~\ref{alg:overall_pipeline}) on four question categories in MMLU (STEM, humanities, social sciences, and others), using Phi3-Medium as the base LLM.
Our approach demonstrates comparable or superior performance across all categories, achieving the highest average accuracy of $78.15\%$.
The results containing all MMLU subjects can be found in Table~\ref{tab:mmlu_subject_phi3} in Appendix.

\input{file/tables/topktopp}

\paragraph{MT-Bench}
\looseness=-1
We choose Llama3.1-8B as the base model in this experiment.
On MT-Bench, the Reflection Window method clearly outperformed both greedy decoding and beam search.
In comparisons with greedy decoding, according to assessments by GPT-4o, reflection window prevails in $66.67\%$ of cases, a win rate significantly higher than that of beam search, which only outperformed greedy decoding in $46.3\%$ of cases.

Figure~\ref{fig:winrate_comparison} shows that our reflection-window decoding significantly outperforms beam search in Roleplay, STEM, Math, and Reasoning tasks.
These tasks require strong logical consistency, and greedy or fixed search strategies often lead to early sub-optimal choices that degrade output quality.
Our approach mitigates this by enabling refinement during generation, making generated contents more coherent.

\input{file/tables/regeneration}
\input{file/tables/entropy_threshold}
\input{file/tables/window_size}
\paragraph{Compatibility with Top-$k$/Top-$p$ Sampling}
\looseness=-1
Reflection-window decoding generates tokens autogressively when the pausing criterion is not triggered and only performs selective refinements.
The framework is compatible with Top-$k$/Top-$p$ sampling except in the refinement/correction mode.
In these experiments, we set $k=10$, $p=0.9$, and temperature as 1.0 for both our approach and the baseline Top-$k$/Top-$p$ sampling.
As shown in Table~\ref{tab:summary_mmlu}, our decoding approach consistently outperforms the standard Top-$k$/Top-$p$ approach across all four models.
In particular, the average accuracy improvements range from $0.88$ percentage points (Mistral-Nemo) to $2.76$ percentage points (Phi3-Medium), highlighting the effectiveness of our approach even when stochastic sampling is introduced.
Notably, the largest performance gains are observed in STEM and humanities categories, suggesting that reflection-window decoding is particularly beneficial for reasoning-heavy tasks.
This aligns with observations from the MT-Bench experiments, which also demonstrate that our approach excels in tasks demanding structured logical thinking and complex problem-solving.
We present further results from Table~\ref{tab:decoding_comparison_llama} to Table~\ref{tab:decoding_comparison_qwen} in Appendix.


\subsection{Further Discussions and Analyses}\label{main:experiments:discussions}
\paragraph{Efficiency of Reflection Window}
% \input{file/tables/table5}
In Table~\ref{tab:regeneration_metric_mmlu}, we aggregate the regeneration statistics on MMLU with Llama3.1-8B.
We record two metrics: (1) the regeneration ratio, which calculates the overall ratio of refined/corrected tokens in the completed response, and (2) the regeneration call, which counts the number of times the pausing criterion is triggered and refinement/correction is needed before finishing any particular response.
We find that regeneration ratio ranges from $3.5\%$ to $5.5\%$ across all categories with a relatively mild pausing criterion.
This suggests that the refinements are usually needed during decoding.
While beam search always maintains a complete frontier of candidate sequences, our reflection-window decoding approach only activates beam search when necessary, and at the sub-sequence level.

\paragraph{Entropy Threshold $\sigma$ and Window Size $d$}\label{main:experiments:entropy_window_size}
We investigate the impact of threshold $\sigma$ on MMLU Social Sciences with Qwen2.5-7B with fixed window size $d=4$.
The results in Table~\ref{tab:acc_mmlu_social_science_entropy_threthold} demonstrate that our method performs robustly across $\sigma$ values ranging from $0.25$ to $0.75$, with $\sigma=0.25$ achieving the highest macro average of $80.31\%$.
While our default setting of $\sigma=0.5$ is not the  best in this specific experiment, it maintains strong performance and shows consistent improvements on most subjects, suggesting that it serves as a reliable default configuration for general usage.

In terms of window size $d$, we also choose social science subjects using Qwen2.5-7B.
The results in Table~\ref{tab:acc_mmlu_social_science_window_size} show that our method maintains strong performance across various window sizes ($d=2$ to $d=6$), with overall macro averages consistently around $79.7\%$.
While $d=6$ achieves the highest macro average, $d=4$ demonstrates comparable performance and maintains better computational efficiency.
These results further support our choice of $d=4$ as a robust default setting, offering a good balance between performance and efficiency across different models and tasks.
We provide additional results on MT-Bench in Table~\ref{tab:mistral_mtbench_per_d} and Table~\ref{tab:llama31_mtbench_per_d}, and on MMLU in Table~\ref{tab:qwen_mmlu_social_science_per_d} and Table~\ref{tab:qwen_mmlu_social_science_per_sigma} in Appendix.


\begin{comment}[Removed for ICML 2025 submission]
% \begin{table*}[h]
%     \centering
%     \caption{Regeneration Measurement by Category on MMLU.}
%     \label{tab:regeneration_metric_mmlu}
%     \begin{tabular}{lccc}
%         \toprule
%         \textbf{Category} & \textbf{Regeneration Rate (\%)} & \textbf{Num of Regeneration} & \textbf{Num of All Tokens} \\
%         \midrule
%         Humanities        & 5.04                            & 4.27                         & 336.69                     \\
%         Other             & 5.54                            & 4.31                         & 305.93                     \\
%         STEM              & 3.50                            & 3.15                         & 398.55                     \\
%         Social Sciences   & 4.82                            & 3.84                         & 316.29                     \\
%         \bottomrule
%     \end{tabular}
%     % \setlength{\abovecaptionskip}{-0.1cm}
% \end{table*}


% \begin{figure}[h!]
%     \centering
%     % Single row of four subfigures
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{file/figures/anatomy.pdf}
%         \caption{anatomy}
%         \label{fig:anatomy}
%     \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{file/figures/computer_security.pdf}
%         \caption{computer\_security}
%         \label{fig:computer_security}
%     \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{file/figures/global_facts.pdf}
%         \caption{global\_facts}
%         \label{fig:global_facts}
%     \end{subfigure}
%     % \hfill
%     % \begin{subfigure}[b]{0.23\textwidth}
%     %     \centering
%     %     \includegraphics[width=\textwidth]{file/figures/nutrition.pdf}
%     %     \caption{nutrition}
%     %     \label{fig:subfig4}
%     % \end{subfigure}

%     \caption{Accuracy on the Selected Subsets with Entropy Threshold $\sigma$}
%     \label{fig:sigma}
% \end{figure}

% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.25\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{file/figures/anatomy.pdf}
%         \caption{anatomy}
%         \label{fig:anatomy}
%     \end{subfigure}
%     \hspace{0.5em}
%     \begin{subfigure}[b]{0.25\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{file/figures/computer_security.pdf}
%         \caption{computer\_security}
%         \label{fig:computer_security}
%     \end{subfigure}
%     \hspace{0.5em}
%     \begin{subfigure}[b]{0.25\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{file/figures/global_facts.pdf}
%         \caption{global\_facts}
%         \label{fig:global_facts}
%     \end{subfigure}

%     \caption{Accuracy on the Selected Subsets with Entropy Threshold $\sigma$}
%     \label{fig:sigma}
% \end{figure*}


% In Figure~\ref{fig:sigma}, we examined the impact of the hyperparameter: entropy threshold $\sigma$. We choose five distinctive values between 0 and 1: 0.1, 0.25, 0.5, 0.75, and 1. We choose three subsets from MMLU: "anatomy", "computer\_security" from STEM and "global\_facts" from Others section.When the threshold is set to 0.5, the Reflection Window method achieves its best performance. Conversely, when the threshold is set to extreme values such as 0.1 or 1.0, the Reflection Window method may degrade to perform similarly to Greedy decoding. It is noteworthy that the performance trends on these three subjects are different. The reason could probably be the domain shift between different tasks, subjects, etc. It indicates that, although $\sigma = 0.5$ is a relatively safe and stable option for pausing criterion, a more dynamic and adaptive strategy could help to alleviate the effect caused by text domain gap. We also provide more experiment on the selection strategy of $\sigma$ and $d$ on in \ref{tab:qwen-d-social-science} and \ref{tab:qwen-sigma-social-science} in Appendix.

% It could be also an indication that, the overall characteristics of different categories and tasks are also different.


% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.85\textwidth]{file/figures/win_rate_comparison.pdf}  % Adjust the width as needed
%     \caption{Win rate comparison by category on MT-Bench between Beam Search and Reflection Window}  % Caption for the figure
%     \label{fig:winrate_comparison}  % Label for referencing the figure in the text
% \end{figure}

% \begin{figure}[t]
%     \centering
%     % First row of two subfigures
%     \begin{subfigure}[b]{0.36\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{file/figures/anatomy.pdf}
%         \caption{anatomy}
%         \label{fig:subfig1}
%     \end{subfigure}
%     % Second subfigure in the same row
%     \begin{subfigure}[b]{0.36\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{file/figures/computer_security.pdf}
%         \caption{computer\_security}
%         \label{fig:subfig2}
%     \end{subfigure}

%     % Second row of two subfigures
%     \begin{subfigure}[b]{0.36\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{file/figures/global_facts.pdf}
%         \caption{global\_facts}
%         \label{fig:subfig3}
%     \end{subfigure}
%     % Fourth subfigure in the same row
%     \begin{subfigure}[b]{0.36\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{file/figures/nutrition.pdf}
%         \caption{nutrition}
%         \label{fig:subfig4}
%     \end{subfigure}

%     \caption{todo}
%     \label{fig:subfigures}
% \end{figure}
\end{comment}
