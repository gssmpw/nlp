\section{Motivation and High-Level Illustration}\label{main:motivation}
In this section, we first present our motivations behind addressing the inherent shortcoming of purely autoregressive decoding for text generation (Section~\ref{main:motivation:issue}).
Then in Section~\ref{main:motivation:reflection_window}, we present a high-level summary of our proposed approach involving interchangeably switching between refinement (upon reflection on previously generated content) and generation (of the additional new content).


\subsection{Inherent Shortcoming of Autoregressive Decoding}\label{main:motivation:issue}
\looseness=-1
Insight from the optimization theory highlights the gap between coordinate-wise accumulation of optimum and the global optimum \citep{torn1989global,nocedal1999numerical}.
Recent research advances in cognitive linguistics have also argued that language is primarily a tool for communication (for humans) rather than thought \citep{fedorenko2024language}.
While language naturally unfolds in a one-dimensional sequence, its underlying dependence pattern extends beyond a purely autoregressive structure (where the current content is conditioned solely on what has been generated so far).

% Before presenting the detailed theoretical analysis (Section~\ref{main:formulation}), we
Let us consider an example of writing a novel.
For a long-format writing like novels, outlining (also referred to as plotting) is essential for structuring ideas, planning narratives, and crafting engaging drafts \citep{king2000writing,serravallo2017writing}.
Sub-goals refer to relatively small and achievable tasks that guide the author through each stage of the story, for instance, the setting of the circumstance, the element of tension and emotion, the sensory imagination of the scene.

\looseness=-1
As we illustrate in Figure~\ref{fig:comparison:selection}, $X_i^*$'s represent words or phrases in the novel, and $S_i$'s represent sub-goals, which may be related in a hierarchical way.
For instance, sub-goals within a single scene altogether serve the purpose of furthering the development of the story.
We model sub-goals in terms of selection variables $S_i$ since they represent constraints or objectives to achieve, which involve certain criteria to be satisfied over the variables that they operate upon.\footnote{
    This modeling choice is consistent with the modeling of causal relations among variables of interest through selection variables in a directed acyclic graph (DAG) \citep{spirtes1993causation,pearl2009causality}.
}
As we can see from Figure~\ref{fig:comparison:selection}, the variables in optimal sequence (the novel in this example) $X_i^*$'s \emph{jointly} satisfy criteria, or optimize objectives, specified by sub-goals $S_i$'s.
This indicates that the best $X_i^*$ in the optimal sequence depends on best values of all other $X_j^*$'s.
However, with an autoregressive way of text generation, as illustrated in Figure~\ref{fig:comparison:autoregressive}, we only allow $X_i$ to depend on $X_j$'s if $j < i$, which is clearly sub-optimal.

\begin{figure}[t]
    \centering
    % \captionsetup{format=hang}
    \begin{subfigure}{.48\columnwidth}
        \centering
        \includegraphics[height=7.5ex]{file/figures/selection_diagram.pdf}
        \caption{\small
            (Sub-)goals jointly satisfied by the optimal sequence}
        \label{fig:comparison:selection}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.48\columnwidth}
        \centering
        \includegraphics[height=7.5ex]{file/figures/autoregressive_diagram.pdf}
        \caption{\small
            Purely autoregressive way of decoding for text generation}
        \label{fig:comparison:autoregressive}
    \end{subfigure}
    \caption{\small
        \looseness=-1
        Illustrative diagrams of different dependence patterns among variables representing tokens or phrases in text generation.
        Panel (a): the dependence pattern among variables in the optimal sequence where there are (sub-)goals $S_i$'s to achieve, specifying conditions or constraints that should \emph{jointly} be satisfied by $X_i^*$'s.
        Panel (b): the autoregressive way of text generation, where $X_i$ is only allowed to depend on $X_j$ if $j < i$.
    }
    \label{fig:comparison}
    \vspace{-3ex}
\end{figure}

\subsection{Selective Refinement Through Reflection Window}\label{main:motivation:reflection_window}
As illustrated in Section~\ref{main:motivation:issue}, one outcome of limiting the dependence pattern to the autoregressive structure is the lack of a built-in mechanism to correct or refine previously generated content at the decoding level, since what was generated is not subject to further edit (if without further processing).
In this subsection, we present a high-level summary of our approach that attempts to address this issue.\footnote{We present in detail our technical approach in Section~\ref{main:experiments:technical_approach}.}
Specifically, we propose a selective refinement framework that facilitates an interchangeable process of refinement and generation, so that the overall response satisfies requirements or objectives that operate jointly over all involved tokens.

\begin{figure*}[t]
    \centering
    % \captionsetup{format=hang}
    \includegraphics[height=12.5ex]{file/figures/pipeline_diagram.pdf}
    \caption{%\small
        \looseness=-1
        Overview of our approach to address the inherent shortcoming of autoregressive decoding for text generation, where the LLM interchangeably switches between refinement and generation.
        The fast pointer pauses if the \emph{pausing criterion} is triggered, indicating the existence of a potential issue in the generated history.
        Then, the model \emph{refines/corrects} the content between the fast and slow pointers before continuing generation, so that the slow pointer catches up with the fast pointer before the latter can move on.
        Reflection window refers to the content between the fast and slow pointers.
    }
    \label{fig:pipeline}
\end{figure*}


\looseness=-1
As illustrated in Figures~\ref{fig:pipeline} and \ref{fig:example}, we can use fast and slow pointers on the generated content to form segments of a certain length, namely, the sliding reflection window, and perform potential refinements within this sliding window as the text generation proceeds.\footnote{
    The naming of pointers is motivated by \citet{kahneman2011thinking}.
}
Our reflection-window decoding framework allows for revision before the entire output is completed, which offers several advantages.

\looseness=-1
Firstly, we can improve the generated content in a timely manner.
If there are multiple potential issues in the generation history, the revision after finishing the generation can be inefficient since we allow errors to accumulate.
In other words, without a built-in mechanism for refinement or correction at the decoding level, we are forced to rely on high-level model behaviors and operate at a coarser granularity.
This often involves regenerating entire sentences (rather than refining words or phrases), and/or editing through multiple iterations (rather than interchangeably generate and refine in a single run), as in self-correction and self-improvement approaches \citep{yao2022react,bai2022constitutional,pan2023automatically,shinn2023reflexion,ganguli2023capacity,chen2023teaching,kim2023language,tyen2023llms,madaan2024self}.
% and we might have to perform multiple iterations of editing.

Secondly, our focus on selective refinement during decoding is not solely driven by inference efficiency considerations.
The primary goal of previous approaches, e.g., speculative decoding \citep{leviathan2023fast,chen2023accelerating,xia2023speculative,kim2024speculative,sun2024spectr,xia2024unlocking}, is to accelerate sampling from (a lighter version of) the original model, while the underlying decoding mechanism remains purely autoregressive.

Thirdly, due to the one-dimensional progression of text generation, our sliding reflection window mechanism, given a pausing criterion, enables timely and assured detection of issues in the generated text.
Our framework complements previous approaches, and furthermore, offers versatility.
One can incorporate \emph{pausing criteria} and \emph{refinement/correction methods} at the decoding level, while preserving the ability to further leverage strategies that rely on high-level behaviors.

\looseness=-1
The empirical pausing criteria we use (detailed in Section~\ref{main:experiments}) are guided by our theoretical characterization of the sub-optimality of autoregressive text generation, and to this theoretical analysis we now turn.

\begin{figure*}[t]
    \centering
    % \captionsetup{format=hang}
    \includegraphics[height=22.5ex]{file/figures/example.pdf}
    \caption{%\small
        \looseness=-1
        A concrete example demonstrating our reflection-window decoding.
    }
    \label{fig:example}
\end{figure*}

\begin{comment}
Let us denote the text between \texttt{<START>} and the slow index as $A$, the content between slow and fast indices as $B$, and the immediate token after the fast indices as $C$.
Let us denote the goal (currently, use the prompt/instruction itself as the surrogate) as $G_{\mathrm{raw}}$, and its paraphrased mission-complete version as $G_{\mathrm{success}}$.
For instance, if $G_{\mathrm{raw}}$ is ``tell a joke,'' then $G_{\mathrm{success}}$ can be ``the generated text tells a joke.''
The reason of having these two versions is to have a way to evaluate the generated content and see if we are approaching the goal.

When slow index does not forward, and only the fast index is moving forward (generating mode)
\begin{equation}
    \begin{split}
        B & \leftarrow (B, C) \\
        C & \leftarrow C'
    \end{split}
\end{equation}

When fast index is paused, and only the slow index is trying to locate the problematic part in generated text (reflecting mode)
\begin{equation}
    \begin{split}
        A & \leftarrow (A, B[0]) \\
        B & \leftarrow B[1:]
    \end{split}
\end{equation}
\end{comment}

\begin{comment}[High-level picture]
\subsection{High-Level Picture}
The goal of this project is to incorporate following characteristics during text generating:
\begin{enumerate}
    \item going beyond autoregressive way of text generation, explicitly introduce the selection mechanism among generated text, where the conditions under which the (final) objective/target/expectation/goal is achieve are specified by the prompt
    \item enable the ability to reflect on generated text as the generation goes on, always with the target in mind (analogous to human writing, where we constantly reflect on what was written, and whether we are \emph{approaching} the goal as we proceed)
    \item a mechanism that is not too heavy to be practically operationalizable
\end{enumerate}
\end{comment}
