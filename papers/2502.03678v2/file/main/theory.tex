\section{Theoretical Characterization of the Sub-Optimality of Autoregressive Decoding}\label{main:theory}
\looseness=-1
We theoretically characterize the sub-optimality of autoregressive text generation.
We show that even if an LLM is sufficiently trained and can perfectly capture any autoregressive decomposition of the joint density, the autoregressive way of text generation can still deviate from the globally optimal response, even for the well-defined objective of maximizing output probability given a fixed length (setting aside whether this objective fully aligns with the ultimate goal).

Let us denote a token from the vocabulary $\Vcal$ as $w_v \in \Vcal$, whose index in the vocabulary is $v \in \lvert \Vcal \rvert$.
We use ``$i:j$'' to denote the increasing integer sequence from $i$ to $j$ if $i \leq j$, e.g., $1:t \coloneqq 1, 2, \ldots, t$ if $t > 1$, otherwise, $i:j \coloneqq \varnothing$.

\begin{definition}[Globally Optimal Length-$T$ Response]\label{def:optimal_sequence}
    % \looseness=-1
    We say a sequence $w_{\vbf_T^*[1]} w_{\vbf_T^*[2]} \ldots w_{\vbf_T^*[T]}$ is globally optimal among all possible length-$T$ responses following the prompt $X_{\leq 0}$, if it has the highest ground-truth conditional probability, denoted by $f(X_{1:t} \mid X_{\leq 0})$ where $t \in [1, T]$:
    \begin{equation}
        \small
        \begin{split}
             & ~ \vbf_T^* = (\vbf_T^*[1], \vbf_T^*[2], \ldots, \vbf_T^*[T])                                                                 \\
             & \coloneqq \argmax_{v_i \in \lvert \Vcal \rvert, i=1,2,\ldots,T} f(X_{1:T} = w_{v_1} w_{v_2} \ldots w_{v_T} \mid X_{\leq 0}).
        \end{split}
    \end{equation}
\end{definition}

\begin{definition}[Stepwise Optimal Length-$T$ Response]\label{def:stepwise_sequence}
    We say a sequence $w_{\widehat{\vbf}_T[1]} w_{\widehat{\vbf}_T[2]} \ldots w_{\widehat{\vbf}_T[T]}$ is stepwise optimal for prompt $X_{\leq 0}$, if the sequence consists of tokens that correspond to highest token-by-token conditional probabilities, denoted by $g(X_{t} \mid X_{1:t-1}, X_{\leq 0})$ where $t \in [1, T]$:\footnote{
    For notional clarity, instead of $g(X_{t} \mid X_{1:t-1}=\xbf_{1:t-1}, X_{\leq 0}=\xbf_{\leq 0})$, we use shorthand notations in the conditioning set, i.e., $g(X_{t} \mid X_{1:t-1}, X_{\leq 0})$.
    We will adopt this simplification throughout the paper, as long as it remains unambiguous.}
    \begin{equation}
        \small
        \begin{split}
            \widehat{\vbf}_T[1]          & \coloneqq \argmax_{v_1 \in \lvert \Vcal \rvert} g(X_1 = w_{v_1} \mid X_{\leq 0}),                                \\
            \widehat{\vbf}_T[2]          & \coloneqq \argmax_{v_2 \in \lvert \Vcal \rvert} g(X_2 = w_{v_2} \mid X_1 = w_{\widehat{\vbf}_T[1]}, X_{\leq 0}), \\
                                         & \cdots                                                                                                           \\
            \widehat{\vbf}_T[T]          & \coloneqq \argmax_{v_T \in \lvert \Vcal \rvert}
            \begin{array}{l}
                g\left(X_T = w_{v_T} \mid X_{\leq 0}, \text{ and } \right. \\
                ~~ \left. X_{1:T - 1} = w_{\widehat{\vbf}_T[1]} \ldots w_{\widehat{\vbf}_T[T - 1]}\right)
            \end{array},    \\
            \text{and } \widehat{\vbf}_T & = (\widehat{\vbf}_T[1], \widehat{\vbf}_T[2], \ldots, \widehat{\vbf}_T[T]).
        \end{split}
    \end{equation}
\end{definition}

In general, the longer the sequence, the lower the joint probability tends to be.
The fair comparison of optimality should be length-specific, and the optimal response of a shorter length is not necessarily identical to the prefix of the optimal response that is longer in length.
For instance, if we were to use 10 words to distinguish between joint and conditional densities, one might say ``\emph{joint density combines all variables; conditional adjusts for known variables}.''
However, if we can use 15 words, one might say ``\emph{joint density reflects combined probabilities of all variables; conditional density adjusts probabilities given known variables}.''
Therefore, we explicitly keep the length $T$ in the notation of vocabulary indices of tokens that constitute the length-$T$ responses.
\begin{assumption}[Oracle LLM]\label{aspt:oracle_LLM}
    We say an autoregressive LLM is an oracle LLM, if the following relation holds for any response of a length $T \geq 1$:
    \begin{equation}
        \small
        f(X_{1:T} \mid X_{\leq 0}) = \Pi_{t = 1}^{T} g(X_{t} \mid X_{1:t-1}, X_{\leq 0}).
    \end{equation}
    % where $g(\cdot)$ denotes the conditional probability of autoregressive way of token-by-token generation.
\end{assumption}
Assumption~\ref{aspt:oracle_LLM} specifies that, after given the prompt or generated text history $X_{\leq 0}$, an oracle (or very well-trained) LLM can recover the ground-truth probability of $X_{1:T}$ as a whole follows $X_{\leq 0}$, by multiplying token-by-token generating probabilities in an autoregressive way.\footnote{
    Here, we implicitly assume that the context length of LLM is sufficiently large to allow for a meaningful discussion.
}
We would like to note that Assumption~\ref{aspt:oracle_LLM} only states that an oracle LLM can perfectly capture the autoregressive way of probability partitioning of text sequences, and this itself does \emph{not} guarantee the equivalence between stepwise optimal response and the same-length globally optimal response for $T > 1$.\footnote{
    When $T = 1$, i.e., if the response is of a length $1$, the stepwise optimal is just the globally optimal for an oracle LLM, since there is only one step in total, and $f(X_1 \mid X_{\leq 0}) = g(X_1 \mid X_{\leq 0})$.
}

\begin{assumption}[Strict Preference Among Same-Length Sequences]\label{aspt:strict_preference}
    For any two length-$T$ different sequences following the prompt $X_{\leq 0}$, there is a strict preference between them in terms of the ground-truth conditional probability $f(X_{1:T} \mid X_{\leq 0})$.
    In other words, the ground-truth conditional probabilities of two length-$T$ sequences equal to each other if and only if the sequences are identical.
\end{assumption}
Assumption~\ref{aspt:strict_preference} specifies that from the ground-truth conditional probability perspective, there is a strict preference between how well two different same-length responses follow the prompt $X_{\leq 0}$, i.e., the ground-truth probability mass function $f(X_{1:T} \mid X_{\leq 0})$ is injective for any given $T > 0$.

\begin{assumption}[Irreversible Advantage Once Manifested]\label{aspt:no_return_after_detour}
    When a stepwise optimal length-$T$ response from an oracle (Assumption~\ref{aspt:oracle_LLM}) autoregressive LLM $w_{\widehat{\vbf}_T[1]} w_{\widehat{\vbf}_T[2]} \ldots w_{\widehat{\vbf}_T[T]}$ is not the globally optimal length-$T$ response $w_{\vbf_T^*[1]} w_{\vbf_T^*[2]} \ldots w_{\vbf_T^*[T]}$, then if the deviation manifests itself at the length-$L$ ($1 < L \leq T$) prefix-sequences, the advantage of the globally optimal length-$T$ response will not be reversed afterwards:
    \begin{equation*}
        \small
        \begin{split}
             & \text{if }\exists L \in (1, T], f(X_{1:L} = w_{\widehat{\vbf}_T[1]} w_{\widehat{\vbf}_T[2]} \ldots w_{\widehat{\vbf}_T[L]} \mid X_{\leq 0})                                                  \\
             & \quad\quad\quad\quad\quad\quad < f(X_{1:L} = w_{\vbf_T^*[1]} w_{\vbf_T^*[2]} \ldots w_{\vbf_T^*[L]} \mid X_{\leq 0}),                                                                        \\
             & \text{then }\forall M \in [L, T], f(X_{1:M} = w_{\widehat{\vbf}_T[1]} \ldots w_{\widehat{\vbf}_T[M]} \mid X_{\leq 0}) \\
             & \quad\quad\quad\quad\quad\quad < f(X_{1:M} = w_{\vbf_T^*[1]} \ldots w_{\vbf_T^*[M]} \mid X_{\leq 0}).
        \end{split}
    \end{equation*}
\end{assumption}
Assumption~\ref{aspt:no_return_after_detour} specifies that if the advantage (in terms of a higher ground-truth conditional probability) of the globally optimal length-$T$ sequence can be observed at the length-$L$ prefix-sequence, such advantage will not be reversed when considering longer prefix-sequences.

\begin{theorem}[Indication of Deviation from the Globally Optimal Length-$T$ Response]\label{thm:small_conditional_prob}
    % Suppose that the context length $N$ of LLM is sufficiently large.
    Given the prompt $X_{\leq 0}$, when an oracle LLM (Assumption~\ref{aspt:oracle_LLM}) generates a stepwise optimal length-$T$ response which is not the globally optimal response with the same length, let $L \leq T$ denote the minimum length of prefix-sequence needed in order for such deviation to manifest itself (Assumptions \ref{aspt:strict_preference} and \ref{aspt:no_return_after_detour}).
    Then, the deviation from the globally optimal response happens at some step $K < L$.
    Furthermore, the conditional probability when generating the token $w_{v_L} \in \Vcal$ is strictly smaller than a positive number, which itself is strictly smaller than $1$, i.e.,
    % In other words, the LLM will be relatively uncertain about
    \begin{equation*}
        \small
        \begin{split}
             & \max_{w \in \Vcal} g(X_L = w \mid X_{1:L-1} = w_{\widehat{\vbf}_T[1]} \ldots w_{\widehat{\vbf}_T[L - 1]}, X_{\leq 0}) < \epsilon_L,                                                                                                         \\
             & \text{and } \epsilon_L = \frac{f(X_{1:L} = w_{\vbf_T^*[1]} \ldots w_{\vbf_T^*[L - 1]} w_{\vbf_T^*[L]} \mid X_{\leq 0})}{f(X_{1:L - 1} = w_{\widehat{\vbf}_T[1]} \ldots w_{\widehat{\vbf}_T[L - 1]} \mid X_{\leq 0})} < 1.
        \end{split}
    \end{equation*}
\end{theorem}

Theorem~\ref{thm:small_conditional_prob} provides a necessary (but not sufficient) condition for the deviation of the stepwise optimal length-$T$ response from the same-length globally optimal response.
The uncertainty (i.e., low conditional probabilities) in generating the next token can result from different factors.
For instance, a previous mistake or detour makes it challenging to continue in any way that could possibly satisfy the goal specified by the prompt.
Such uncertainty can also result from multiple valid ways to proceed in order to achieve the goal.
Although we do not have access to the ground-truth conditional probability $f(X_{1:T} \mid X_{\leq 0})$, Theorem~\ref{thm:small_conditional_prob} suggests that when noticeable uncertainty arises, one should to be cautious of a potential deviation from the globally optimal response in the generated text.
