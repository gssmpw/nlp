\section{Introduction}\label{main:introduction}
\looseness=-1
It is a well-known insight in optimization theory that coordinate-wise optimization, conditioned on the previously optimized arguments along each axis sequentially, generally does not guarantee finding the global optimum \citep{torn1989global,nocedal1999numerical}.
Similarly, in the context of decoding in large language models (LLMs), expecting to achieve a globally optimal response by sequentially accumulating per-token optimal decisions, as done in purely autoregressive decoding, may be overly optimistic.
Despite significant recent progress \citep{vaswani2017attention,radford2019language,brown2020language,openai2023gpt4,touvron2023llama,google2023gemini,meta2024llama,deepmind2024gemma,abdin2024phi4,deepseek2025deepseek}, how to approach the optimal text that one can possibly sample from a model still remains an open question.

\looseness=-1
Previous works have demonstrated challenges faced by autoregressive decoding in terms of the handling of long sequences \citep{wu2021autoformer} and the inefficient inference \citep{lin2021limitations,li2022elmer}.
At the level of decoding, other than the traditional greedy search, \citet{holtzman2020curious} proposed Top-$p$ sampling (also known as Nucleus Sampling), a stochastic method that adjusts the next-token set based on the shape of conditional probability mass function.
Alternatively, different from the cumulated probability mass, Top-$k$ sampling limits the number of available options when sampling for the next token \citep{fan2018hierarchical,holtzman2018learning,radford2019language}.
Another empirical technique involves adjusting the shape of probability distribution with the temperature hyperparameter \citep{fan2018hierarchical,holtzman2018learning,peeperkorn2024temperature}.

% \looseness=-1
In order to improve the sampling efficiency, speculative decoding approaches have been proposed, where multiple tokens are predicted in parallel as if one were sampling from the model (or its lighter counterpart) repetitively \citep{leviathan2023fast,chen2023accelerating,xia2023speculative,kim2024speculative,sun2024spectr}.
Efficient inference with beam search \citep{xie2024self,zhu2024deductive,wei2024confidence,yang2024language} and probabilistic programs \citep{lew2023smcp3} have been explored in the recent literature.
Improving generated content through high-level behaviors, e.g., through instructing self-correction or conducting self-improvement with external or internal feedback \citep{yao2022react,bai2022constitutional,pan2023automatically,shinn2023reflexion,ganguli2023capacity,chen2023teaching,kim2023language,tyen2023llms,madaan2024self}, has also been studied.

\looseness=-1
While previous literature has explored various methods to enhance generated content, the fundamental limitation of autoregressive decoding for text generation remains under-explored.
This gap represents a distinct perspective, different from high-level model behaviors (e.g., self-refinement studied in \citealt{madaan2024self}) or inference efficiency (e.g., various speculative decoding methods surveyed in \citealt{xia2024unlocking}).
In this paper, we theoretically characterize the inherent shortcoming of autoregressive decoding and propose an empirical method to mitigate this issue.
Our contributions can be summarized as follows:
\begin{itemize}[topsep=0pt,leftmargin=*,noitemsep]
    \item We theoretically characterize the sub-optimality of autoregressive decoding for text generation, demonstrating its lack of a built-in mechanism to perform refinement/correction of generated content at the decoding level.
    \item We propose a framework involving a sliding reflection window and a corresponding pause criterion, enabling an interchangeable process of refinement and generation.
    \item Through extensive empirical evaluations, our approach demonstrates significant improvement over existing decoding approaches, and maintains performance comparable or superior to beam search while being more efficient.
\end{itemize}