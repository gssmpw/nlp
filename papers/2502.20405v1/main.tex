    % This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
\usepackage{booktabs}
\usepackage[many]{tcolorbox}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{comment}
\newcommand{\yicheng}[1]{\textcolor{red}{[yicheng: #1]}}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{authblk}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Pause-Tuning for Long-Context Comprehension: A Lightweight Approach to LLM Attention Recalibration} 

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
    \textbf{James Begin} \quad
    \textbf{Namit Agrawal} \quad
    \textbf{Eshan Singh} \\
    \textbf{Yicheng Fu} \quad
    \textbf{Sean O’Brien} \quad
    \textbf{Vasu Sharma} \quad
    \textbf{Kevin Zhu}
}
\affil{Algoverse AI Research}
\affil{\texttt{kevin@algoverse.us, sean@algoverse.us}}

\date{}


% \author{James Begin \and Namit Agrawal \and Eshan Singh \and Yicheng Fu \and Sean O’Brien \and Vasu Sharma \and Kevin Zhu \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
LLMs have demonstrated remarkable proficiency in understanding tasks but continue to struggle with long-context comprehension, particularly with content located in the middle of extensive inputs. This limitation, known as the Lost-in-the-Middle (LITM) problem, hinders models from fully processing and utilizing information across lengthy contexts. To address this issue, we introduce pause-tuning, a technique that redistributes attention to enhance comprehension of long-context inputs. Our approach involves fine-tuning language models on datasets with artificially inserted pause tokens, which serve to segment the input into smaller, more manageable parts. We evaluate pause-tuning against alternative approaches using the Needle-in-a-Haystack benchmark, where models must retrieve information embedded within contexts of up to 128K tokens. Experimental results demonstrate significant performance gains, with the LLaMA 3.2 3B Instruct model and the LLaMA 3.1 8B Instruct model improving by 10.61\% and 3.57\% respectively on average, suggesting that pause-tuning successfully enhances attention redistribution and improves long-context retention. The code and data are
available at \url{https://anonymous.4open.science/r/LITM-PauseTokens-7357}.

\end{abstract}

\section{Introduction}

Language models like GPT~\cite{brown2020languagemodelsfewshotlearners} and LLaMA~\cite{grattafiori2024llama3herdmodels} have demonstrated remarkable utility in tasks such as summarization, long document analysis, and contextual understanding \citep{minaee2024largelanguagemodelssurvey}. Effectively handling long contexts is essential for maintaining the accuracy and reliability of a model’s output in these applications. However, language models often suffer from the lost-in-the-middle problem \citep{liu-etal-2024-lost}, where they disproportionately focus on the beginning and end of sequences while neglecting information in the middle. Existing attempts at solutions often fall short of being broadly applicable. Many approaches rely on computationally intensive mechanisms or involve modifications of the base language model other than simple fine-tuning \citep{he2024lostmiddlemasteringlongcontext, tworkowski2023focusedtransformercontrastivetraining, liu2023blockwiseparalleltransformerlarge}. While effective in certain scenarios, these methods may be impractical in resource-constrained environments or general-purpose applications.

To address this gap, we propose a novel approach that utilizes pause tokens \citep{goyal2024thinkspeaktraininglanguage} to mitigate the LITM problem. Pause tokens are markers that are strategically inserted into the input sequence, intended to recalibrate the model's attention distribution. These tokens prompt the model to pause and process information before proceeding with the rest of the sequence. By segmenting the input sequence into smaller, more manageable chunks, pause tokens allow the model to process each segment with greater focus and parity. This simple yet effective method offers a lightweight alternative to existing resource-intensive techniques. We investigate various strategies for inserting pause tokens, as depicted in ~\autoref{fig:pause_tuning}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Updated_Method_Diagram.png}
    \caption{We propose five potential techniques for pause token injection and test them on the needle-in-a-haystack evaluation framework.}
    \label{fig:pause_tuning}
\end{figure}

We evaluate the efficacy of pause tokens through a series of needle-in-a-haystack experiments \citep{needleinahaystack2023, nelson2024needlehaystackmemorybased}, comparing base models against those fine-tuned for pause-aware long-context processing. Our results demonstrate that:
\begin{itemize} 
    \item Pause-tuning consistently improves long-context retention and processing, outperforming alternative techniques.
    \item Pause tokens induce meaningful shifts in attention distribution, enhancing information retrieval across extended input sequences.
\end{itemize}

Our findings highlight pause tokens as an effective and computationally efficient mechanism for mitigating long-context deficiencies in LLMs.

\section{Related Work}

\subsection{Lost-in-the-Middle}

Large language models exhibit a U-shaped performance curve when processing long inputs, demonstrating a pronounced primacy and recency bias \citep{liu-etal-2024-lost}. That is, they allocate greater attention to the beginning and end of a sequence while neglecting the middle \citep{khandelwal-etal-2018-sharp, press-etal-2021-shortformer}. ~\citet{xiao2023efficient} further observed that models assign disproportionately high attention scores to initial tokens, even when these tokens lack semantic significance. This phenomenon extends to multi-document question-answering tasks—closely related to our evaluation framework—as well as key-value retrieval tasks. Building on these findings, our research investigates whether similar biases emerge in our specific context and examines their implications.


\subsection{Positional Encodings}

Altering attention through positional encoding is another promising approach to addressing LITM \citep{shaw2018selfattentionrelativepositionrepresentations}. \citet{su2023roformerenhancedtransformerrotary} proposed Rotary Position Embedding (RoPE), which encodes relative position information through a rotation matrix applied to token embeddings. RoPE allows for better extrapolation to longer sequences and has been widely adopted in recent language models \citep{deepseekai2024deepseekv3technicalreport, grattafiori2024llama3herdmodels}. \citet{zhang2024found} explores flaws with RoPE and seeks to address it using positional re-scaling. Modern implementations of RoPE, such as YaRN \citep{peng2023yarnefficientcontextwindow}, provide efficient scaling of context windows to extreme lengths (1M+ tokens). Positional encoding techniques show that attention recalibration can improve accuracy, while avoiding significant computation and fine-tuning. Rather than modifying positional embeddings, our work aims to enhance performance by redistributing attention through prompt editing and fine-tuning.

\subsection{Pause Tokens}

\citet{goyal2024thinkspeaktraininglanguage} introduced the concept of pause tokens in language model training. Their approach involves inserting learnable pause tokens during pretraining and finetuning, showing improvements on various Question-Answer tasks \cite{kwiatkowski-etal-2019-natural, talmor2019commonsenseqaquestionansweringchallenge, rajpurkar2016squad100000questionsmachine}. Expanding on this idea, \citet{rawte2024sorrycomeagainprompting} proposed the "Sorry, Come Again" (SCA) prompting technique, which integrates optimal paraphrasing with pause token injection. This method has been shown to effectively mitigate hallucinations in large language models, further underscoring the potential of pause tokens in enhancing model reliability and interpretability.

\begin{comment}
\subsection{Position-Aware Attention Mechanisms}

Several approaches have focused on modifying attention mechanisms to better handle long contexts. \citet{chen2024corecontextawareattention} introduced Core Context Aware (CCA) Attention, which combines globality-pooling attention and locality-preserved attention to reduce computational complexity while improving long-context modeling ability. Similarly, \citet{jiang2023llmlinguacompressingpromptsaccelerated} proposed LLMLingua, an iterative prompt compression method that considers conditional dependencies between tokens. These techniques aim to optimize attention calculation by focusing on the most relevant parts of the input.

\subsection{Efficient Transformer Architectures}

Researchers have explored architectural modifications to enhance Transformers' ability to process long sequences efficiently. \citet{kitaev2020reformerefficienttransformer} introduced Reformer, which uses locality-sensitive hashing (LSH) attention to reduce computational complexity. \citet{xiao2024efficientstreaminglanguagemodels} developed StreamingLLM, addressing the "attention sink" phenomenon by merging window context with the first token, enabling models to handle potentially infinite sequence lengths. These approaches demonstrate the ongoing efforts to improve Transformer efficiency for long-context tasks.
\end{comment}

\section{Method}

\subsection{Token Placement Strategy}

We employ a systematic approach to injecting pause tokens across different experimental configurations. The most straightforward implementation involves inserting a special "<PAUSE>" token after each paragraph in the testing context. This token serves as a standardized pause marker, facilitating natural segmentation within the input sequence. By introducing these structured breaks, our approach enables the model to redistribute attention more effectively, ensuring comprehensive processing of information from all parts of the input.

\subsection{Experimental Configurations}

Our study explores various approaches for pause token insertion, as illustrated in \autoref{fig:pause_tuning}, using trials without input sequence modifications as the baseline for comparison. We evaluate the following techniques across 15 context depths and 3 trials for single needle tests and 15 randomized trials for multi-needle tests to identify the optimal method:
\begin{enumerate} 
    \item Standard Pause Tokens: Standard pause tokens are inserted after every paragraph in the input sequence. 
    \item Instruction-Augmented Pause Tokens: Pause tokens, including an explicit instruction to "stop and absorb the information [the model] has just read," are inserted after every paragraph in the input sequence. 
    \item Pre-Prompt Instruction with Standard Pause Tokens: A general instruction at the beginning of the prompt directs the model to stop and absorb information after every pause token, while standard pause tokens are inserted after every paragraph in the input sequence. 
    \item Fine-Tuning for Long Contexts: No modifications are made to the input sequence; instead, the model is fine-tuned for long-context comprehension using a one-shot prompt.
    % similar to that of Technique 5. 
    \item Pause-Tuned Model with Standard Pause Tokens: Standard pause tokens are inserted after every paragraph in the input sequence, which is then processed by a model fine-tuned with standard pause tokens in long contexts.
\end{enumerate}
% \begin{enumerate}
%     \item Standard pause tokens inserted after every paragraph in the input sequence.
%     \item Modified pause tokens including instruction to "stop and absorb the information [the model] has just read" inserted after every paragraph in the input sequence.
%     \item Instruction at start of prompt to stop and absorb information after every pause token, and standard pause tokens inserted after every paragraph in the input sequence.
%     \item No modifications to input sequence, but model fine-tuned for long contexts using a one-shot prompt similar to that of Technique 5.
%     \item Standard pause tokens inserted after every paragraph in the sequence inputted to pause-tuned model (fine-tuned for pause tokens in long contexts).
% \end{enumerate}

\noindent
These techniques aim to systematically determine the optimal method for enhancing the attention mechanism in long-context tasks.

Technique 2 and Technique 3 evaluate whether the model requires additional instructions or can naturally interpret pause tokens. Technique 4 serves as a control for Technique 5, assessing whether pause tokens are necessary for performance improvement or if long-context fine-tuning alone can achieve similar enhancements. Technique 5 employs pause-tuning, aligning the model with the pause token structure to enhance performance.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Confidence_Interval_1.png}
    \caption{LLaMA 3.2 3B Instruct Performance Across Techniques. The score declines as context length increases. Pause-tuning consistently outperforms other methods, except at the 128K length.}
    \label{fig:llama_3.2_3b}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Confidence_Interval_2.png}
    \caption{LLaMA 3.1 8B Instruct Performance Across Techniques. The score declines as context length increases. Pause-tuning consistently outperforms other methods at all context lengths.}
    \label{fig:llama_3.1_8b}
\end{figure}

\section{Experiments}

\subsection{Models}

We evaluate several widely used large language models (LLMs), including GPT-3.5 Turbo 0125 \citep{brown2020languagemodelsfewshotlearners}, GPT-4o Mini 2024-07-18 \citep{openai2024gpt4ocard}, LLaMA 3.2 1B Instruct, LLaMA 3.2 3B Instruct, and LLaMA 3.1 8B Instruct \citep{grattafiori2024llama3herdmodels}. For Technique 5, we employ pause-tuned versions of the LLaMA 3.2 3B Instruct and LLaMA 3.1 8B Instruct models.

\subsection{Evaluation}

We conduct our experiments using the Needle-in-a-Haystack evaluation framework \citep{needleinahaystack2023}. In this framework, a critical piece of information (the "needle") is embedded within a lengthy context (the "haystack"). Our evaluation spans various context lengths, ranging from 1K to 128K tokens, depending on each model's capacity for long contexts. Additionally, we assess both single-needle and multi-needle scenarios to examine the effectiveness of our method across different conditions. For multi-needle inputs, we embed three distinct needles within the context.

The results from each trial are assigned a score from 1 to 10 based on the success of the information retrieval attempt, as described in Appendix ~\ref{appendix_a}.

\begin{table*}[h!]
\centering
\small
\scalebox{0.85}
{
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Model}              & \textbf{1K} & \textbf{2K} & \textbf{4K} & \textbf{8K} & \textbf{16K} & \textbf{32K} & \textbf{64K} & \textbf{128K} \\ 
\midrule
\multicolumn{9}{c}{\textbf{Baseline}} \\ \midrule
GPT 3.5 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 9.27 \(\pm\) 0.60 & - & - & - \\
GPT 4o & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 9.76 \(\pm\) 0.08 & 9.27 \(\pm\) 0.20 & 8.84 \(\pm\) 0.31 \\
LLaMA 3.2 1B & 7.67 \(\pm\) 0.93 & 6.64 \(\pm\) 1.86 & 4.11 \(\pm\) 1.89 & 3.67 \(\pm\) 0.81 & 1.4 \(\pm\) 0.38 & 1.38 \(\pm\) 0.69 & 1.04 \(\pm\) 0.08 & 1.00 \(\pm\) 0.00 \\
LLaMA 3.2 3B & 10.00 \(\pm\) 0.00 & 9.87 \(\pm\) 0.23 & 10.00 \(\pm\) 0.00 & 9.13 \(\pm\) 0.58 & 6.31 \(\pm\) 1.37 & 3.75 \(\pm\) 0.40 & 5.73 \(\pm\) 1.75 & 5.15 \(\pm\) 0.95 \\
LLaMA 3.1 8B & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 9.60 \(\pm\) 0.69 & 9.40 \(\pm\) 0.00 & 7.89 \(\pm\) 1.35 & 7.40 \(\pm\) 0.18 \\ \midrule

\multicolumn{9}{c}{\textbf{Standard Pause Tokens (Technique 1)}} \\ \midrule
GPT 3.5 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 9.44 \(\pm\) 0.23 & - & - & - \\
GPT 4o & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 9.71 \(\pm\) 0.08 & 9.47 \(\pm\) 0.00 \\
LLaMA 3.2 1B & 7.75 \(\pm\) 0.47 & 6.31 \(\pm\) 0.33 & 4.13 \(\pm\) 0.28 & 4.00 \(\pm\) 1.37 & 2.00 \(\pm\) 0.28 & 1.40 \(\pm\) 0.85 & 1.00 \(\pm\) 0.00 & 1.00 \(\pm\) 0.00 \\
LLaMA 3.2 3B & 10.00 \(\pm\) 0.00 & 9.93 \(\pm\) 0.12 & 9.67 \(\pm\) 0.23 & 9.62 \(\pm\) 0.33 & 6.51 \(\pm\) 1.81 & 5.24 \(\pm\) 0.45 & 5.60 \(\pm\) 0.35 & 5.64 \(\pm\) 0.48 \\
LLaMA 3.1 8B & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 9.70 \(\pm\) 0.35 & 10.00 \(\pm\) 0.00 & 9.60 \(\pm\) 0.35 & 8.82 \(\pm\) 0.86 & 8.40 \(\pm\) 1.51 & 7.15 \(\pm\) 0.68 \\ \midrule

\multicolumn{9}{c}{\textbf{Instruction-Augmented Pause Tokens (Technique 2)}} \\ \midrule
GPT 3.5 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 9.44 $\pm$ 0.12 & - & - & - \\
GPT 4o & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 9.54 $\pm$ 0.12 \\
LLaMA 3.2 1B & 7.96 $\pm$ 0.66 & 6.60 $\pm$ 1.94 & 5.31 $\pm$ 2.86 & 3.14 $\pm$ 1.30 & 1.87 $\pm$ 0.83 & 1.04 $\pm$ 0.39 & 1.00 $\pm$ 0.00 & 1.00 $\pm$ 0.00 \\
LLaMA 3.2 3B & 10.00 $\pm$ 0.00 & 9.93 $\pm$ 0.12 & 9.93 $\pm$ 0.12 & 9.40 $\pm$ 0.35 & 6.35 $\pm$ 1.31 & 3.67 $\pm$ 0.37 & 5.36 $\pm$ 1.61 & 4.29 $\pm$ 1.49 \\
LLaMA 3.1 8B & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 9.80 $\pm$ 0.35 & 8.80 $\pm$ 0.60 & 7.49 $\pm$ 0.20 & 6.27 $\pm$ 0.91 \\
 \midrule

\multicolumn{9}{c}{\textbf{Pre-Prompt Instruction with Standard Pause Tokens (Technique 3)}} \\ \midrule
GPT 3.5 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 9.44 $\pm$ 0.08 & - & - & - \\
GPT 4o & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 9.71 $\pm$ 0.08 & 9.62 $\pm$ 0.08 \\
LLaMA 3.2 1B & 7.24 $\pm$ 0.60 & 7.62 $\pm$ 0.73 & 5.95 $\pm$ 2.31 & 3.73 $\pm$ 0.61 & 1.76 $\pm$ 0.33 & 1.69 $\pm$ 0.31 & 1.00 $\pm$ 0.00 & 1.00 $\pm$ 0.00 \\
LLaMA 3.2 3B & 9.80 $\pm$ 0.35 & 9.93 $\pm$ 0.12 & 10.00 $\pm$ 0.00 & 9.58 $\pm$ 0.40 & 6.02 $\pm$ 2.24 & 5.84 $\pm$ 0.77 & 4.47 $\pm$ 1.30 & 3.27 $\pm$ 1.01 \\
LLaMA 3.1 8B & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 9.80 $\pm$ 0.35 & 9.80 $\pm$ 0.35 & 9.60 $\pm$ 0.35 & 9.20 $\pm$ 0.69 & 8.84 $\pm$ 0.53 & 7.15 $\pm$ 0.08 \\
 \midrule

\multicolumn{9}{c}{\textbf{Fine-Tuning for Long Contexts (Technique 4)}} \\ \midrule
LLaMA 3.1 8B & 5.60 \(\pm\) 1.92 & 6.80 \(\pm\) 2.88 & 6.80 \(\pm\) 3.30 & 5.60 \(\pm\) 2.50 & 5.20 \(\pm\) 2.21 & 5.00 \(\pm\) 2.17 & 4.80 \(\pm\) 1.78 & 3.00 \(\pm\) 1.46 \\ \midrule

\multicolumn{9}{c}{\textbf{Pause-Tuned Model with Standard Pause Tokens (Technique 5)}} \\ \midrule
LLaMA 3.2 3B & 10.00 \(\pm\) 0.00 & 9.93 \(\pm\) 0.12 & 9.87 \(\pm\) 0.23 & 9.29 \(\pm\) 0.54 & 7.91 \(\pm\) 1.64 & 6.29 \(\pm\) 1.74 & 5.89 \(\pm\) 1.37 & 4.56 \(\pm\) 0.42 \\
LLaMA 3.1 8B & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 10.00 \(\pm\) 0.00 & 9.44 \(\pm\) 0.53 & 9.16 \(\pm\) 0.27 & 7.98 \(\pm\) 0.37 \\ \midrule

\multicolumn{9}{c}{\textbf{Pause-Tuning (\% Change Over Baseline)}} \\ \midrule
LLaMA 3.2 3B & 0.00 & 0.61 & -1.30 & 1.75 & 25.36 & 67.73 & 2.79 & -11.46 \\
LLaMA 3.1 8B  & 0.00 & 0.00 & 0.00 & 0.00 & 4.17 & 0.43 & 16.10 & 7.84 \\
\bottomrule

\end{tabular}
}
\caption{Results of the single needle task, presented as mean $\pm$ standard deviation, comparing the baseline, pause token methods, and pause-tuning. Token counts are denoted as 1K = 1,000 tokens, 2K = 2,000 tokens, etc.}
\label{tab:combined_results}
\end{table*}

\subsection{Datasets}

We use the Deep Essays Dataset \citep{willian_oliveira_gibin_mann_acharya_2024} and the DAIGT Gemini-Pro 8.5K Essays dataset \citep{demir_essays_dataset} for fine-tuning and a collection of Paul Graham's essays \citep{graham_essays} to create the haystack for testing.

\subsection{Pause-Tuning}

We fine-tune two LLaMA models for pause tokens in long contexts. These models were selected for their ability to handle sequences of up to 128K tokens and their strong performance in instructional tasks.

To construct an appropriate fine-tuning dataset, we concatenate multiple shorter essays and systematically inject pause tokens until the target context length is reached. Additionally, we embed a randomly selected piece of information—a "needle"—within this extended context "haystack" to assess retrieval capabilities. The models are trained using LoRA \citep{lora_hu_shen} and Unsloth AI \citep{unsloth}. The hyperparameters for training are in Appendix ~\ref{appendix:para}. The training prompt adopts a one-shot format, consisting of four key components: an instruction outlining the purpose of the fine-tuning, a long-context input with injected pauses that contains the needle, an example user query that depends on retrieving the embedded information, and a response that reproduces the needle verbatim as it was originally inserted into the essay.

% The training prompt follows a one-shot format, structured into four distinct components:
% \begin{enumerate}
% \item An instruction detailing the purpose of the fine-tuning.
% \item The pause-injected long context input containing the needle.
% \item An example user input asking a question that relies on information from the needle.
% \item A response containing the needle verbatim as inserted into the essay.
% \end{enumerate}

\section{Results}

The results for the baseline and each technique using a single needle are presented in \autoref{tab:combined_results}, while the results for multiple needles can be found in Appendix~\ref{appendix:multi}. A visualization of the performance scores can be found in ~\autoref{fig:llama_3.2_3b} and~\autoref{fig:llama_3.1_8b}. A significant improvement is observed in the pause-tuned models compared to both the baseline and the other techniques. While Technique 3 proves highly effective (11.44\% improvement)  on the LLaMA 3.2 1B model, as seen in \autoref{tab:percent_change}, its impact is less pronounced on the other models. In contrast, Technique 5 demonstrates substantial improvements, with 10.61\% and 3.57\% increases, across both models, supporting our hypothesis that combining fine-tuning with pause tokens yields the best results. This outcome aligns with our intuition that training the model with a consistent structural approach enhances performance. 

Notably, the LLaMA 3.1 8B Instruct model, evaluated across all techniques, exhibits a 16.10\% improvement over the baseline at 64K tokens and a 7.84\% improvement at 128K tokens using the pause-tuning technique. These results highlight the effectiveness of integrating pause tokens within the fine-tuning process. In contrast, Techniques 1 and 4, which involve using pause tokens without fine-tuning and fine-tuning without pause tokens, respectively, prove highly ineffective. These findings demonstrate that neither pause tokens nor fine-tuning alone significantly enhance performance. However, when the model is explicitly trained to recognize and utilize the pause token, its ability to process long-context inputs improves remarkably.

% In Technique 1, using pause tokens without fine-tuning is not entirely effective, and in Technique 4, fine-tuning without pause tokens is very ineffective. The combination of the two (pause-tuning) yields the most improvement compared to the baseline, which is why we choose this as the preferred method.

\begin{table}[h!]
\centering
\small
\scalebox{1}{
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Technique}              & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} \\ \midrule
GPT 3.5          &0.37&0.37&0.37&---&---          \\
GPT 4o         &1.68&2.15&1.87&---&---          \\
LLaMA 3.2 1B       &2.53&3.75&11.44&---&---            \\
LLaMA 3.2 3B       &1.03&-1.70&-1.72&---&10.61          \\
LLaMA 3.1 8B       &-1.67&-2.60&0.14&-42.88&3.57          \\
\bottomrule
\end{tabular}
}
\caption{Percent Change for each technique compared to baseline, averaged across all context lengths.}
\label{tab:percent_change}
\end{table}

\section{Attention Analysis}

\autoref{fig:baseline_attention} depicts the scaled attention distribution across different layers when generating the first answer token for the baseline and Techniques 1 and 5. Due to computational constraints, these results are for 3000 token input sequences. However, the observed retrieval improvements for sequences up to 128K tokens suggest that this pattern likely extends to longer contexts. 

%The baseline attention distribution shows significantly reduced attention throughout the middle of the sequence, limiting the model's ability to accurately retrieve information.

The insertion of pause tokens significantly transforms the attention distribution. We observe distinct attention spikes at the locations of several pause tokens. This distinction is especially present surrounding the needle and in the latter half of the context for both the standard pause token insertion and the pause-tuned model. These findings indicate that pause tokens serve as anchors that interrupt the attention decay over long sequences, prompting the model to engage with all sections more thoroughly. The model likely treats the paragraphs separated by pause tokens as distinct sections, refreshing attention and decreasing the likelihood of forgetting important information. This structured attention recalibration may explain the improved retrieval performance observed in longer contexts.

\begin{figure}[h]
    
    \includegraphics[width=1\linewidth]{attention_plot.png}
    \caption{Attention distribution with normalized attention scores across different layers when generating the first answer token for baseline, standard pause token insertion, and pause-tuned with pause tokens inserted. The purple highlight indicates the position of the needle. The red dots indicate inserted pause tokens.}
    \label{fig:baseline_attention}
\end{figure}

\section{Conclusion}

In this paper, we introduce pause-tuning as an effective and lightweight solution to the LITM problem, addressing long-context comprehension challenges in LLMs. By strategically injecting pause tokens into input sequences, we enhance attention redistribution, enabling models to retrieve information more effectively across extensive contexts. Our experiments demonstrate significant improvements in retrieval performance over baseline models in the Needle-in-a-Haystack benchmark. The results confirm that pause-tuning consistently enhances long-context retention across various input lengths. These findings underscore the potential of pause-tuning as a practical technique for mitigating LITM issues, paving the way for more robust long-context processing in future LLM advancements.

% In this paper, we propose the use of pause tokens as a lightweight alternative to existing LITM solutions. We experiment with several options for the pause token injection, and observe a significant increase in LLM retrieval capabilities in long contexts over baseline. We determine that pause-tuning is the optimal strategy across various input lengths and examine the resulting change in the model's attention mechanism. Our findings show that computationally inexpensive changes can lead to significant improvement in language models and that pause-tuning is a valid method to mitigate the LITM problem.

\section*{Limitation}

One limitation of our evaluation framework is its focus on a specific retrieval method, where a fact is embedded within a long context. While our results suggest that the attention shifts induced by pause-tuning enhance overall comprehension, we cannot generalize this improvement to all long-context use cases. In many scenarios, pure retrieval is insufficient; instead, effective reasoning and understanding of interrelated information within long contexts are crucial.

Another limitation is that our study evaluates the pause-tuning technique only on relatively small models (<10B parameters). Whether these findings extend to large-scale models remains an open question and requires further investigation.

\bibliography{anthology, main}

\clearpage
\appendix
\section{Needle-in-a-Haystack Evaluation}
\label{appendix_a}

To score the retrievals for the needle-in-a-haystack test, the following framework was used, with relevance assessed by the model: \\\\
Score 1: The answer is completely unrelated to the reference. \\
                Score 3: The answer has minor relevance but does not align with the reference.\\
                Score 5: The answer has moderate relevance but contains inaccuracies.\\
                Score 7: The answer aligns with the reference but has minor omissions.\\
                Score 10: The answer is completely accurate and aligns perfectly with the reference.

\section{Prompt Formatting}

\begin{tcolorbox}[colframe=green!50!black, colback=gray!10, title=Prompt without <PAUSE> tokens]
Below is an instruction that describes a task, paired with a context that provides further information. An input will request information from the context. Write a response that appropriately completes the request.\\

\#\#\#Instruction:\\\\
You are a helpful assistant that will be provided a context which the user wants to ask a question about, your job is to answer the question with only statements provided in the context and nothing else.\\

\#\#\#Context:\\\\
\{context\}\\

\#\#\#Input:\\\\
\{input\}
\end{tcolorbox}

\begin{tcolorbox}[colframe=green!50!black, colback=gray!10, title=Prompt with <PAUSE> tokens]
Below is an instruction that describes a task, paired with a context that provides further information. An input will request information from the context. Write a response that appropriately completes the request.\\

\#\#\#Instruction:\\\\
You are a helpful assistant that will be provided a context which the user wants to ask a question about, the context has <PAUSE> tokens that tell you when to take a pause to comprehend the context before continuing, your job is to answer the question with only statements provided in the context and nothing else.\\

\#\#\#Context:\\\\
\{context\}\\

\#\#\#Input:\\\\
\{input\}
\end{tcolorbox}

\clearpage

\section{Hyperparameters}
\label{appendix:para}

The parameters in \autoref{tab:parameters} were used as part of the fine-tuning process.

\begin{table*}[h]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{|l|l|}
        \hline
        \multicolumn{2}{|c|}{\textbf{LoRA Parameters}} \\
        \hline
        Rank (r) & 16 \\
        Alpha & 16 \\
        Dropout & 0 \\
        Target Modules & ["q\_proj", "k\_proj", "v\_proj", "o\_proj", "gate\_proj", "up\_proj", "down\_proj"] \\
        \hline
        \multicolumn{2}{|c|}{\textbf{Training Parameters}} \\
        \hline
        Batch Size & 2 \\
        Gradient Accumulation Steps & 4 \\
        Learning Rate & 2e-4 \\
        Weight Decay & 0.01 \\
        Warmup Steps & 6 \\
        Steps & 60 \\
        LR Scheduler & linear \\
        Optimizer & adamw\_8bit \\
        \hline
        \multicolumn{2}{|c|}{\textbf{Other}} \\
        \hline
        Mixed Precision & fp16 \\
        Quantization Bits & 4 bit \\
        \hline
    \end{tabular}}
    \caption{Training Hyperparameters}
    \label{tab:parameters}
\end{table*}
\section{Multi-Needle Results}
\label{appendix:multi}

While the results in the main paper utilize a single needle, we also conduct tests with three needles in the haystack. The results are reported in \autoref{tab:combined_results_2}.

\begin{table*}[h!]
\centering
\small
\scalebox{0.85}
{
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Model} & \textbf{1K} & \textbf{2K} & \textbf{4K} & \textbf{8K} & \textbf{16K} & \textbf{32K} & \textbf{64K} & \textbf{128K} \\ \midrule
\multicolumn{9}{c}{\textbf{Baseline}} \\ \midrule
GPT 3.5 & 10.00 $\pm$ 0.00 & 9.60 $\pm$ 1.06 & 9.87 $\pm$ 0.52 & 8.00 $\pm$ 1.73 & 8.00 $\pm$ 1.36 & - & - & - \\
GPT 4o & 9.60 $\pm$ 1.06 & 10.00 $\pm$ 0.00 & 9.27 $\pm$ 1.55 & 8.80 $\pm$ 2.48 & 8.87 $\pm$ 2.26 & 7.20 $\pm$ 2.65 & 8.53 $\pm$ 2.64 & 7.60 $\pm$ 1.68 \\
LLaMA 3.2 1B & 6.40 $\pm$ 4.56 & 8.60 $\pm$ 3.18 & 7.00 $\pm$ 4.39 & 1.00 $\pm$ 0.00 & 3.60 $\pm$ 4.06 & 1.00 $\pm$ 0.00 & 2.20 $\pm$ 3.17 & 1.00 $\pm$ 0.00 \\
LLaMA 3.2 3B & 7.20 $\pm$ 3.10 & 6.20 $\pm$ 2.40 & 5.20 $\pm$ 2.21 & 3.60 $\pm$ 1.92 & 4.20 $\pm$ 2.11 & 3.40 $\pm$ 1.68 & 3.40 $\pm$ 2.32 & 2.20 $\pm$ 1.52 \\
LLaMA 3.1 8B & 8.07 $\pm$ 2.69 & 7.47 $\pm$ 1.92 & 6.80 $\pm$ 1.78 & 6.40 $\pm$ 1.68 & 4.40 $\pm$ 2.10 & 4.27 $\pm$ 1.55 & 4.27 $\pm$ 0.80 & 4.20 $\pm$ 0.77 \\
 \midrule

\multicolumn{9}{c}{\textbf{Standard Pause Tokens (Technique 1)}} \\ \midrule
GPT 3.5 & 10.00 $\pm$ 0.00 & 9.40 $\pm$ 1.24 & 9.93 $\pm$ 0.26 & 8.67 $\pm$ 2.17 & 7.67 $\pm$ 1.68 & - & - & - \\
GPT 4o & 9.20 $\pm$ 1.37 & 9.80 $\pm$ 0.77 & 9.60 $\pm$ 1.06 & 9.00 $\pm$ 1.46 & 8.47 $\pm$ 1.78 & 8.00 $\pm$ 2.45 & 7.80 $\pm$ 2.40 & 8.40 $\pm$ 1.92 \\
LLaMA 3.2 1B & 8.20 $\pm$ 3.73 & 9.40 $\pm$ 2.32 & 9.40 $\pm$ 2.32 & 6.20 $\pm$ 4.46 & 5.00 $\pm$ 4.49 & 3.40 $\pm$ 4.12 & 2.80 $\pm$ 3.72 & 1.00 $\pm$ 0.00 \\
LLaMA 3.2 3B & 8.00 $\pm$ 2.17 & 7.40 $\pm$ 1.92 & 6.13 $\pm$ 2.36 & 5.53 $\pm$ 1.85 & 4.80 $\pm$ 1.37 & 4.93 $\pm$ 1.75 & 4.27 $\pm$ 0.80 & 3.13 $\pm$ 2.72 \\
LLaMA 3.1 8B & 8.00 $\pm$ 2.70 & 7.40 $\pm$ 1.55 & 6.67 $\pm$ 2.50 & 7.00 $\pm$ 1.96 & 5.20 $\pm$ 1.52 & 5.00 $\pm$ 1.46 & 4.47 $\pm$ 1.55 & 4.20 $\pm$ 1.37 \\
 \midrule

\multicolumn{9}{c}{\textbf{Instruction-Augmented Pause Tokens (Technique 2)}} \\ \midrule
GPT 3.5 & 10.00 $\pm$ 0.00 & 10.00 $\pm$ 0.00 & 9.87 $\pm$ 0.52 & 8.53 $\pm$ 1.46 & 8.20 $\pm$ 1.37 & - & - & - \\
GPT 4o & 10.00 $\pm$ 0.00 & 9.00 $\pm$ 1.06 & 8.60 $\pm$ 1.92 & 9.40 $\pm$ 1.24 & 8.00 $\pm$ 2.45 & 7.00 $\pm$ 3.21 & 7.67 $\pm$ 2.32 & 7.00 $\pm$ 2.27 \\
LLaMA 3.2 1B & 3.00 $\pm$ 1.85 & 2.20 $\pm$ 1.90 & 2.80 $\pm$ 2.73 & 2.20 $\pm$ 2.21 & 1.40 $\pm$ 1.06 & 1.80 $\pm$ 2.40 & 1.60 $\pm$ 1.68 & 1.00 $\pm$ 0.00 \\
LLaMA 3.2 3B & 7.13 $\pm$ 2.59 & 5.67 $\pm$ 2.72 & 5.33 $\pm$ 1.29 & 4.27 $\pm$ 1.39 & 5.00 $\pm$ 2.07 & 4.07 $\pm$ 1.16 & 3.67 $\pm$ 1.11 & 3.33 $\pm$ 2.47 \\
LLaMA 3.1 8B & 9.00 $\pm$ 1.46 & 7.67 $\pm$ 1.54 & 7.47 $\pm$ 1.41 & 7.07 $\pm$ 0.96 & 6.73 $\pm$ 0.70 & 6.86 $\pm$ 0.52 & 6.60 $\pm$ 1.55 & 5.71 $\pm$ 1.68 \\
\midrule

\multicolumn{9}{c}{\textbf{Pre-Prompt Instruction with Standard Pause Tokens (Technique 3)}} \\ \midrule
GPT 3.5 & 10.00 \(\pm\) 0.00 & 9.60 \(\pm\) 1.55 & 9.60 \(\pm\) 1.06 & 8.93 \(\pm\) 1.44 & 7.87 \(\pm\) 2.10 & - & - & - \\
GPT 4o & 9.80 \(\pm\) 0.77 & 10.00 \(\pm\) 0.00 & 9.20 \(\pm\) 1.78 & 9.40 \(\pm\) 1.24 & 8.80 \(\pm\) 1.52 & 8.20 \(\pm\) 1.90 & 7.00 \(\pm\) 2.27 & 7.20 \(\pm\) 1.37 \\
LLaMA 3.2 1B & 2.40 \(\pm\) 1.55 & 2.20 \(\pm\) 1.52 & 2.40 \(\pm\) 1.55 & 2.80 \(\pm\) 1.90 & 1.40 \(\pm\) 1.06 & 1.60 \(\pm\) 1.68 & 1.40 \(\pm\) 1.06 & 1.00 \(\pm\) 0.00 \\
LLaMA 3.2 3B & 7.20 \(\pm\) 2.65 & 7.00 \(\pm\) 2.27 & 6.00 \(\pm\) 1.46 & 4.40 \(\pm\) 1.92 & 4.60 \(\pm\) 2.32 & 4.00 \(\pm\) 1.60 & 4.20 \(\pm\) 1.37 & 3.00 \(\pm\) 1.46 \\
LLaMA 3.1 8B & 8.20 \(\pm\) 1.52 & 7.40 \(\pm\) 1.95 & 7.60 \(\pm\) 1.24 & 7.07 \(\pm\) 1.22 & 7.27 \(\pm\) 0.80 & 7.20 \(\pm\) 0.77 & 7.00 \(\pm\) 0.00 & 5.27 \(\pm\) 2.25 \\
 \midrule
\multicolumn{9}{c}{\textbf{Pause-Tuned Model with Standard Pause Tokens (Technique 5)}} \\ \midrule
LLaMA 3.2 3B & 7.00 \(\pm\) 3.59 & 5.80 \(\pm\) 2.73 & 4.80 \(\pm\) 4.00 & 2.80 \(\pm\) 2.96 & 1.20 \(\pm\) 4.93 & 1.00 \(\pm\) 0.00 & 1.00 \(\pm\) 0.00 & 1.00 \(\pm\) 0.00 \\
LLaMA 3.1 8B & 8.33 \(\pm\) 2.32 & 7.93 \(\pm\) 2.66 & 8.00 \(\pm\) 2.45 & 7.07 \(\pm\) 2.46 & 6.00 \(\pm\) 2.07 & 5.80 \(\pm\) 2.34 & 4.73 \(\pm\) 2.58 & 4.33 \(\pm\) 2.66 \\ \bottomrule
\end{tabular}
}
\caption{Results of the multiple-needle task, presented as mean $\pm$ standard deviation, comparing the baseline, pause token methods, and pause-tuning. Token counts are denoted as 1K = 1,000 tokens, 2K = 2,000 tokens, etc.}
\label{tab:combined_results_2}
\end{table*}
\end{document}
