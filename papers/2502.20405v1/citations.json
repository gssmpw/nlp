[
  {
    "index": 0,
    "papers": [
      {
        "key": "liu-etal-2024-lost",
        "author": "Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy",
        "title": "Lost in the Middle: How Language Models Use Long Contexts"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "khandelwal-etal-2018-sharp",
        "author": "Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan",
        "title": "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context"
      },
      {
        "key": "press-etal-2021-shortformer",
        "author": "Press, Ofir and Smith, Noah A. and Lewis, Mike",
        "title": "Shortformer: Better Language Modeling using Shorter Inputs"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "xiao2023efficient",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike",
        "title": "Efficient streaming language models with attention sinks"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "shaw2018selfattentionrelativepositionrepresentations",
        "author": "Peter Shaw and Jakob Uszkoreit and Ashish Vaswani",
        "title": "Self-Attention with Relative Position Representations"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "su2023roformerenhancedtransformerrotary",
        "author": "Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu",
        "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "deepseekai2024deepseekv3technicalreport",
        "author": "DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo ... H.  Zizheng Pan",
        "title": "DeepSeek-V3 Technical Report"
      },
      {
        "key": "grattafiori2024llama3herdmodels",
        "author": "Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark ... Zhiyu Ma",
        "title": "The Llama 3 Herd of Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhang2024found",
        "author": "Zhang, Zhenyu and Chen, Runjin and Liu, Shiwei and Yao, Zhewei and Ruwase, Olatunji and Chen, Beidi and Wu, Xiaoxia and Wang, Zhangyang",
        "title": "Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "peng2023yarnefficientcontextwindow",
        "author": "Bowen Peng and Jeffrey Quesnelle and Honglu Fan and Enrico Shippole",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "goyal2024thinkspeaktraininglanguage",
        "author": "Sachin Goyal and Ziwei Ji and Ankit Singh Rawat and Aditya Krishna Menon and Sanjiv Kumar and Vaishnavh Nagarajan",
        "title": "Think before you speak: Training Language Models With Pause Tokens"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "kwiatkowski-etal-2019-natural",
        "author": "Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav",
        "title": "Natural Questions: A Benchmark for Question Answering Research"
      },
      {
        "key": "talmor2019commonsenseqaquestionansweringchallenge",
        "author": "Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant",
        "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"
      },
      {
        "key": "rajpurkar2016squad100000questionsmachine",
        "author": "Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang",
        "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "rawte2024sorrycomeagainprompting",
        "author": "Vipula Rawte and S. M Towhidul Islam Tonmoy and S M Mehedi Zaman and Prachi Priya and Aman Chadha and Amit P. Sheth and Amitava Das",
        "title": "\"Sorry, Come Again?\" Prompting -- Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chen2024corecontextawareattention",
        "author": "Yaofo Chen and Zeng You and Shuhai Zhang and Haokun Li and Yirui Li and Yaowei Wang and Mingkui Tan",
        "title": "Core Context Aware Attention for Long Context Language Modeling"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "jiang2023llmlinguacompressingpromptsaccelerated",
        "author": "Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu",
        "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "kitaev2020reformerefficienttransformer",
        "author": "Nikita Kitaev and \u0141ukasz Kaiser and Anselm Levskaya",
        "title": "Reformer: The Efficient Transformer"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "xiao2024efficientstreaminglanguagemodels",
        "author": "Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis",
        "title": "Efficient Streaming Language Models with Attention Sinks"
      }
    ]
  }
]