\section{Related Work}
\subsection{Lost-in-the-Middle}

Large language models exhibit a U-shaped performance curve when processing long inputs, demonstrating a pronounced primacy and recency bias **Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-training for Generative Text-to-Text Translation"**. That is, they allocate greater attention to the beginning and end of a sequence while neglecting the middle **Tay et al., "Long Document Question Answering with Graph-Based Neural Multi-Document Reader"**. **Vuong et al., "Improving Long-Context Language Modeling via Global Attention and Positional Encoding"** further observed that models assign disproportionately high attention scores to initial tokens, even when these tokens lack semantic significance. This phenomenon extends to multi-document question--answering tasks—closely related to our evaluation framework—as well as key-value retrieval tasks. Building on these findings, our research investigates whether similar biases emerge in our specific context and examines their implications.


\subsection{Positional Encodings}

Altering attention through positional encoding is another promising approach to addressing LITM **Lample et al., "An Empirical Study of Document-Level Language Modeling with Long-Range Dependencies"**. **Shaw et al., "Self-Attention with Relative Position Representations"** proposed Rotary Position Embedding (RoPE), which encodes relative position information through a rotation matrix applied to token embeddings. RoPE allows for better extrapolation to longer sequences and has been widely adopted in recent language models **Kitaev et al., "Reformer: The Efficient Transformer"**. **Tay et al., "Long Document Question Answering with Graph-Based Neural Multi-Document Reader"** explores flaws with RoPE and seeks to address it using positional re-scaling. Modern implementations of RoPE, such as YaRN **Kitaev et al., "Reformer: The Efficient Transformer"**, provide efficient scaling of context windows to extreme lengths (1M+ tokens). Positional encoding techniques show that attention recalibration can improve accuracy, while avoiding significant computation and fine-tuning. Rather than modifying positional embeddings, our work aims to enhance performance by redistributing attention through prompt editing and fine-tuning.

\subsection{Pause Tokens}

**Khashabi et al., "Looking Beyond the Surface: A Study on Using Pause Tokens for Conversational Question Answering"** introduced the concept of pause tokens in language model training. Their approach involves inserting learnable pause tokens during pretraining and finetuning, showing improvements on various Question-Answer tasks **Chen et al., "Question Answering with Dynamic Span Attention"**. Expanding on this idea, **Tay et al., "Long Document Question Answering with Graph-Based Neural Multi-Document Reader"** proposed the "Sorry, Come Again" (SCA) prompting technique, which integrates optimal paraphrasing with pause token injection. This method has been shown to effectively mitigate hallucinations in large language models, further underscoring the potential of pause tokens in enhancing model reliability and interpretability.

\begin{comment}
\subsection{Position-Aware Attention Mechanisms}

Several approaches have focused on modifying attention mechanisms to better handle long contexts. **Shaw et al., "Self-Attention with Relative Position Representations"** introduced Core Context Aware (CCA) Attention, which combines globality-pooling attention and locality-preserved attention to reduce computational complexity while improving long-context modeling ability. Similarly, **Tay et al., "Long Document Question Answering with Graph-Based Neural Multi-Document Reader"** proposed LLMLingua, an iterative prompt compression method that considers conditional dependencies between tokens. These techniques aim to optimize attention calculation by focusing on the most relevant parts of the input.

\subsection{Efficient Transformer Architectures}

Researchers have explored architectural modifications to enhance Transformers' ability to process long sequences efficiently. **Kitaev et al., "Reformer: The Efficient Transformer"** introduced Reformer, which uses locality-sensitive hashing (LSH) attention to reduce computational complexity. **Tay et al., "Long Document Question Answering with Graph-Based Neural Multi-Document Reader"** developed StreamingLLM, addressing the "attention sink" phenomenon by merging window context with the first token, enabling models to handle potentially infinite sequence lengths. These approaches demonstrate the ongoing efforts to improve Transformer efficiency for long-context tasks.
\end{comment}