\section{Related Work}
\subsection{Lost-in-the-Middle}

Large language models exhibit a U-shaped performance curve when processing long inputs, demonstrating a pronounced primacy and recency bias \citep{liu-etal-2024-lost}. That is, they allocate greater attention to the beginning and end of a sequence while neglecting the middle \citep{khandelwal-etal-2018-sharp, press-etal-2021-shortformer}. ~\citet{xiao2023efficient} further observed that models assign disproportionately high attention scores to initial tokens, even when these tokens lack semantic significance. This phenomenon extends to multi-document question-answering tasks—closely related to our evaluation framework—as well as key-value retrieval tasks. Building on these findings, our research investigates whether similar biases emerge in our specific context and examines their implications.


\subsection{Positional Encodings}

Altering attention through positional encoding is another promising approach to addressing LITM \citep{shaw2018selfattentionrelativepositionrepresentations}. \citet{su2023roformerenhancedtransformerrotary} proposed Rotary Position Embedding (RoPE), which encodes relative position information through a rotation matrix applied to token embeddings. RoPE allows for better extrapolation to longer sequences and has been widely adopted in recent language models \citep{deepseekai2024deepseekv3technicalreport, grattafiori2024llama3herdmodels}. \citet{zhang2024found} explores flaws with RoPE and seeks to address it using positional re-scaling. Modern implementations of RoPE, such as YaRN \citep{peng2023yarnefficientcontextwindow}, provide efficient scaling of context windows to extreme lengths (1M+ tokens). Positional encoding techniques show that attention recalibration can improve accuracy, while avoiding significant computation and fine-tuning. Rather than modifying positional embeddings, our work aims to enhance performance by redistributing attention through prompt editing and fine-tuning.

\subsection{Pause Tokens}

\citet{goyal2024thinkspeaktraininglanguage} introduced the concept of pause tokens in language model training. Their approach involves inserting learnable pause tokens during pretraining and finetuning, showing improvements on various Question-Answer tasks \cite{kwiatkowski-etal-2019-natural, talmor2019commonsenseqaquestionansweringchallenge, rajpurkar2016squad100000questionsmachine}. Expanding on this idea, \citet{rawte2024sorrycomeagainprompting} proposed the "Sorry, Come Again" (SCA) prompting technique, which integrates optimal paraphrasing with pause token injection. This method has been shown to effectively mitigate hallucinations in large language models, further underscoring the potential of pause tokens in enhancing model reliability and interpretability.

\begin{comment}
\subsection{Position-Aware Attention Mechanisms}

Several approaches have focused on modifying attention mechanisms to better handle long contexts. \citet{chen2024corecontextawareattention} introduced Core Context Aware (CCA) Attention, which combines globality-pooling attention and locality-preserved attention to reduce computational complexity while improving long-context modeling ability. Similarly, \citet{jiang2023llmlinguacompressingpromptsaccelerated} proposed LLMLingua, an iterative prompt compression method that considers conditional dependencies between tokens. These techniques aim to optimize attention calculation by focusing on the most relevant parts of the input.

\subsection{Efficient Transformer Architectures}

Researchers have explored architectural modifications to enhance Transformers' ability to process long sequences efficiently. \citet{kitaev2020reformerefficienttransformer} introduced Reformer, which uses locality-sensitive hashing (LSH) attention to reduce computational complexity. \citet{xiao2024efficientstreaminglanguagemodels} developed StreamingLLM, addressing the "attention sink" phenomenon by merging window context with the first token, enabling models to handle potentially infinite sequence lengths. These approaches demonstrate the ongoing efforts to improve Transformer efficiency for long-context tasks.
\end{comment}