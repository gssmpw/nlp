\documentclass{amia}
\usepackage{lipsum} %Remove if not needed
 
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version
 %\usepackage{siunitx}
\usepackage{multirow}
\renewcommand\arraystretch{1.3}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{xspace}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{subfigure}
\usepackage{hyperref}


\input{commands}
\input{Jun_command}

\setlength{\bibsep}{0pt} %Comment out if you don't want to condense the bibliography

\begin{document}


\title{Explainable AI for Clinical Outcome Prediction: A Survey of Clinician Perceptions and Preferences}


% \author{\Name{Jun Hou}
%        \Email{hou.jun@northeastern.edu}\\ 
%        \addr Department of Mechanical and Industrial Engineering\\
%        Northeastern University\\
%        Seattle, Washington State, USA 
%        \AND
%        \Name{Lucy Lu Wang}
%        \Email{lucylw@uw.edu}\\ 
%        \addr Information School\\
%        University of Washington\\
%        Seattle, Washington State, USA} 


\author{Jun Hou, MS$^1$, Lucy Lu Wang, PhD$^2$ }

\institutes{
    $^1$Virginia Tech, Blacksburg, VA; $^2$University of Washington, Seattle, WA
}

\maketitle

\begin{abstract}
  % Outcome prediction in clinical settings can help identify patients at higher risk of negative outcomes, such as treatment failure or in-hospital mortality.
  % With deep learning-based models in particular, 
  Explainable AI (XAI) techniques are necessary to help clinicians make sense of AI predictions and integrate predictions into their decision-making workflow.
  % Our aim is to 
  % understand how clinicians assess the outputs of XAI techniques that can be applied to clinical text.
  % Specifically, we focus on 
  In this work, we conduct a survey study to understand 
  % which XAI techniques can help clinicians interpret predictions over text-based EHR data, 
  clinician preference among different XAI techniques when they are used to interpret model predictions over text-based EHR data.
  % , and how these preferences might guide development of better XAI techniques.
  We implement four XAI techniques (LIME, Attention-based span highlights, exemplar patient retrieval, and free-text rationales generated by LLMs) on an outcome prediction model that uses ICU admission notes to predict a patient's likelihood of experiencing in-hospital mortality.
  % ; the techniques we investigate include LIME, Attention-based span highlights, exemplar patient retrieval, and free-text rationales generated by a large language model. 
  Using these XAI implementations, we design and conduct a survey study of 32 practicing clinicians, collecting their feedback and preferences on the four techniques. We synthesize our findings into a set of recommendations describing when each of the XAI techniques may be more appropriate, their potential limitations, as well as recommendations for improvement. 
\end{abstract}

\section*{\textsc{Introduction}}
\vspace{-1mm}
% \jun{There is no option to load supplementary materials.}
Clinical decision support systems (CDSS) powered by machine learning and AI have the potential to assist in medical decisions and improve patient outcomes. 
% Applications of CDSS have been explored in a variety of clinical settings, such as the Intensive Care Unit (ICU),\cite{Berge_2023} Oncology,\cite{Shen2017ConstructingOC} and Cardiology. \cite{kim2022prediction} 
However, to meaningfully support clinicians, AI-powered CDSS must be trustworthy and interpretable, allowing clinicians to assess the utility and applicability of model predictions. Explainable AI (XAI) techniques have been proposed to improve model interpretability, especially for neural network and other blackbox models.\cite{danilevsky2020survey} While XAI techniques have been applied to CDSS,\cite{feng2020explainable} a comprehensive understanding of clinician preferences and perceptions regarding XAI applications in these systems remains largely unexplored. 

% Researchers have rarely followed up with the end-users of these models (clinicians) regarding the usefulness of XAI techniques applied on top of predictions for assisting with clinical decisions.
% in real-world settings. 
% Reviews and survey papers in this space 
Prior work on clinical XAI tends to focus on explanatory accuracy, in terms of which models are applicable,\cite{Sheikhalishahi_2019} how to integrate XAI methods for different healthcare tasks,\cite{Chaddad_2023} or which datasets are available to train on.\cite{di2023explainable} 
%Efforts exploring clinician feedback in this space have also focused more on Computer Vision applications \cite{Chen2021ExplainableMI} rather than Natural Language Processing (NLP) applications, which are receiving more attention due to recent advances in language modeling.
% Recent studies have also applied Large Language Models (LLMs) with some success to healthcare tasks such as digital medicine\cite{tsai2022natural} and radiology report generation.\cite{Moezzi_2022}
While these works consider XAI for improving model interpretability, they do not incorporate user studies with clinical practitioners to understand whether XAI methods meet their needs. Solely focusing on technical performance metrics can lead to a gap between how AI developers and healthcare professionals view and assess AI tools.
% \jun{Addressing why we focus on opinion} 

% Though researchers and practitioners seem to agree on the need for XAI in healthcare, there is limited validation with end users regarding the design and appropriateness of these methods in clinical settings.
%In this work, we aim to fill this gap. 
% who spend the most time with patients and will benefit most from XAI-enhanced CDSS models that improve communication between patients and doctors.
% Here, we aim to understand how clinicians perceive different XAI techniques, whether they are judged to be useful, and how each technique can be improved to facilitate clinician understanding of model output and support their decision-making workflow.
We fill this validation gap by surveying clinical workers about the utility of popular XAI methods applied to text-based CDSS. We conduct a study with 32 clinicians (predominantly nurses along with physicians, technicians, and administrators), asking them to interact with and compare several XAI methods, and eliciting feedback about the design and utility of these methods. 
% Based on a review of XAI usage in clinical decision support, 
We  methods in our study, including LIME (Local Interpretable Model-Agnostic Explanation),\cite{Ribeiro2016WhySI} Attention-based span highlights,\cite{Vaswani2017AttentionIA} exemplar patient retrieval, and free-text rationales produced by a large language model (LLM), which represent all explanation forms categorized in the review by Chaddad et al.\cite{Chaddad_2023}
We apply these methods to explain outputs for a model trained to predict in-hospital mortality in an intensive care setting, a task studied extensively in prior work.\cite{, van-aken-etal-2021-clinical, jin2018improving} We select this task because it mirrors decision-making processes common in disease diagnosis, supporting the potential to generalize our findings to other similar scenarios.
% where binary classification is applicable. 
% \jun{Addressing the single MOR task}
% randl2023early, Mondrejevski2022FLICUAF, 
%, bardak2020improving, huang2019patient
% We design a questionnaire based on our predictive model and XAI techniques to investigate clinician perceptions and preferences for these techniques. 
% We survey 
Based on participants' questionnaire results, we answer the following research questions: (1) how these XAI methods can be improved, (2) what tasks they potentially support, and (3) how they compare. 

After conducting thematic analysis on the results, we synthesize a set of recommendations for 
% how researchers and practitioners should 
% approaching 
designing and implementing XAI methods in an ICU setting. 
Our findings underscore the importance of creating both efficient, generalized tools and specialist-sensitive options tailored to varying levels of clinical expertise. We also observe a strong preference among clinicians for free-text rationales, highlighting their potential to enhance communication between healthcare providers and patients. However, our participants also emphasize the importance of evidence-based XAI approaches, such as similar patient retrieval, in building trust between clinician users and AI systems.

%\todoit{summarize insights below in text and remove section}



    %\item Clinician feedback is essential for designing XAI methods that meet the real-world needs of clinical settings. We design and conduct a survey around four representative XAI methods    to capture the perceptions and preferences of clinicians.
    %\item Our findings underscore that not every clinician has the same need for XAI tools. However, clinician experience and workflow can be influential on their acceptance and preferences, which suggests that a generalized XAI method that optimizes efficiency and clarity should be coupled with a specialist-sensitive tool to support decision-making.
    %\item A preference for free-text rationales is clear among clinician feedback. Its potential as a communication tool can bridge the gap between different healthcare providers and between providers and patients. Also, we find that evidence-based XAI methods such as similar patient retrieval may enhance trust.

\section*{\textsc{Background \& Related Work}} \label{sec:background}
\vspace{-1mm}

% Our selection of text-based XAI techniques for our survey study is informed by literature review. 

% %We review recent work applying Explainable AI techniques to clinical NLP applications. 
% %First, we identified recent publications from arXiv and PubMed at this intersection. In arXiv, we searched the keywords ``clinical'', ``Natural Language Processing'', ``interpretability'', and ``explainability'', yielding 37 papers published within the past six years (from 2018 to 2023). %Of these 37 papers, we excluded 1 paper that did not use NLP techniques, 10 papers for being review papers, and 10 papers that did not incorporate XAI methods, leaving 16 papers. For PubMed, we searched ``Natural Language Processing'' (``NLP'') and ``Intensive Care Unit'' (``ICU'') in MeSH terms and all fields. There were 36 papers that fulfilled the criteria within the six-year period. We manually screened for the keywords ``interpretability'' and ``explainability,'' yielding 6 additional papers.

% %We grouped the final set of 22 papers (16 from arXiv and 6 from PubMed) by the clinical task they aim to solve and the XAI method(s) applied, showing Disease Diagnosis to be the most explored theme. 
% First, we identified recent publications from arXiv and PubMed, focusing on clinical NLP model considered interpretability or explainability, yielding 73 papers within the past six years (from 2018 to 2023). We manually screened these for the keywords ``interpretability`` and ``explainability,`` narrowing it down to 22 papers (16 from arXiv and 6 from PubMed). 

% We grouped these 22 papers by the clinical task they aim to solve and the XAI method(s) applied, showing Disease Diagnosis to be the most explored theme. We focus on mortality prediction, a singular task, because many other predictive tasks are also modeled as binary prediction, making our findings likely to generalize well to other cases. We elect to focus on mortality prediction as this is a singular task (whereas disease diagnosis represents diverse tasks such as predicting liver injury, cancer, or dementia). Because many other predictive tasks are also modeled as binary prediction, like mortality prediction, we believe our findings should generalize well to other cases.

% We note that none of these prior works have fully explored the breadth of XAI methods, showing a lack of comprehensive comparative studies. \cite{Berge_2023} performed a qualitative evaluation of their CDSS tool with XAI by conducting surveys and interviews among doctors and nurses; their system detects allergies and uses color-coding to highlight relevant keywords and provides a heatmap of classified allergy severity. \cite{du2022role} compared the effects XAI methods of two explanation forms, feature-based and example-based, in CDSS for gestational Diabetes mellitus (GDM) on descriptive data, including key patient information like age, white cell count, and body weight as features.
% In our study, rather than focusing on the performance of the predictive model itself, we aim to understand how clinicians perceive different XAI techniques, whether they are judged to be useful, and how each technique can be improved to facilitate clinician understanding of model output and support their decision-making workflow. 

\paragraph{Explainable AI (XAI) in healthcare} 
% Chaddad et al.~2023\cite{Chaddad_2023} provide a framework for categorizing XAI methods by four criteria: explanation form, interpretation type, model specificity, and explanation scope. 
% Multiple XAI methods can be categorized as 
Feature-map methods such as LIME\cite{Ribeiro2016WhySI}
% used in \cite{garg2023lonxplain,garg2023annotated} and \cite{Saxena2022ExplainableCA}, 
and SHAP (SHapley Additive exPlanations) \cite{lundberg2017unified}
have been explored repeatedly in clinical settings,\cite{garg2023annotated,Saxena2022ExplainableCA}
% used in \cite{garg2023annotated} 
% Thorsen et al.~2022,\cite{thorsen2022discrete} 
% and attention-based methods.\cite{vaswani2017attention}
% used in \cite{franz2020deep}. 
% On the other hand, 
as have textual explanation forms.\cite{Agerri2023HiTZAntidoteAE, Wang2023CanLL}
% have also been used to explain medical decisions to end-users
% and to explain the rationales behind dementia diagnosis through cognitive tests.\cite{Wang2023CanLL} 
Shen et al.\cite{shen2018constructing}~applied an example-based XAI method, which retrieves similar clinical cases through Case-Based Reasoning. 
In clinical settings, the focus is on post-hoc local explanations that balance accuracy and clarity and provide detailed insights for individual patient cases.\cite{Markus_2021}
Therefore, we focus on post-hoc instance-level explanations in this work,
comparing four XAI methods that span the explanation forms discussed in Chaddad et al.\cite{Chaddad_2023} 

\paragraph{In-hospital Mortality Prediction}
In-hospital mortality prediction aims to estimate the risk of a patient dying during their hospital stay, and is crucial for prioritizing treatment strategies and resource allocation. Prior studies have investigated this task in the ICU setting using clinical text and time series Electronic Health Record (EHR) data.\cite{jin2018improving,Marafino_2018}
% Jin et al.\cite{jin2018improving}~expanded the scope of this task by proposing a multimodal neural network that integrates time-series signals with clinical text. 
%Models that do not use textual data have also been used to improve in-hospital mortality prediction, such as community-based federated learning algorithms \cite{huang2019patient} and convolution-based models \cite{bardak2020improving}. 
Performance on the task was significantly enhanced by leveraging LLMs in Van Aken et al.\cite{van-aken-etal-2021-clinical} Naik et al.\cite{naik-etal-2022-literature}~then integrated patient-specific retrieved literature as input into predictive models to enhance performance.  
% Recent work explored further the use of federated learning for mortality prediction in the ICU setting and its early prediction to assist in treatment planning.\cite{Mondrejevski2022FLICUAF, randl2023early} 
These works highlight the continuing focus on this critical task using diverse methodologies, driving our choice of this task.

\paragraph{Understanding Physician Perspectives and Preferences}
% \jun{should we shorten this paragraph? We can integrate more details of survey into introduction?}
A literature review conducted by Antoniadi et al.\cite{antoniadi2021current} revealed the importance of XAI in building trustworthy AI/machine learning-based CDSS, as well as the lack of user studies in their development. 
%\citet{Bienefeld_2023} explored the gap between clinician needs and developer objectives in Neuro ICUs, and \citet{Wysocki_2023} investigated the communication gap between healthcare professionals and AI models designed for COVID-19 diagnosis.
% \citet{Bienefeld_2023} and \citet{Wysocki_2023} identified gaps between clinician needs and developer objectives in Neuro ICUs and communication in AI models for COVID-19 diagnosis, respectively.
% Some work has incorporated limited user studies in the development process of explainable CDSS, though mostly for medical image or sensor data.
% For example, \citet{born2021accelerating} used Class activation mapping (CAM) to explain lung ultrasound images, which was validated by two clinical experts. \citet{neves2021interpretable} investigated three local model-agnostic XAI methods for the classification of arthythmia through ECG image data; in their investigation, they conducted a small user study with three ECG readers to evaluate the effectiveness of these methods for improving clinicians' classification accuracy. 
% While these works are aligned with ours in employing user studies to evaluate the effectiveness of XAI for improving CDSS, they are limited to specific XAI methods, as well as applications only on image, signal, or descriptive data for specialized predictive tasks. 
A systematic review conducted by Jung et al.\cite{Jung2023EssentialPA} also found that prior work on XAI in healthcare lacked a consensus evaluation framework for assessing the success of the XAI method. We hope to approach these recommendations from a user-centered perspective, based on what clinicians identify as useful aspects of XAI applied in Natural Language Processing (NLP)-powered CDSS. 

\section*{\textsc{Materials \& Methods}} \label{sec:Methods}
\vspace{-1mm}
% We design and conduct a survey study to investigate clinician perceptions of XAI methods in a clinical NLP setting. 

\paragraph{Data \& Task} \label{sec:data}
The data for our study is derived from MIMIC-III,\cite{Johnson_2016} a collection of de-identified health records from 46,520 patients who stayed in the critical care units of Beth Israel Deaconess Medical Center between 2001-2012.
We adopt the early-detection mortality prediction task from Van Aken et al.,\cite{van-aken-etal-2021-clinical} which uses patient admission notes to predict whether a patient will experience in-hospital mortality. Each patient's admission note is semi-structured free text, consisting of the sections Chief complaint, Present illness, Medical history, Admission Medications, Allergies, Physical exam, Family history, and Social history. We use train/test splits from their work,\cite{van-aken-etal-2021-clinical} which consist of 30,420 patients in the
survived class and 3,534 patients in the mortality class in the train split; and 8,797 patients
in the survived class and 1,025 patients in the mortality class in the test split.

\paragraph{Predictive Model} \label{sec:Predict}
We train predictive models on mortality prediction and select examples for explanation generation and inclusion in our survey.
We use UmlsBERT \cite{michalopoulos-etal-2021-umlsbert} as our base model because it was found to be the most effective for in-hospital mortality prediction in prior work.\cite{naik-etal-2022-literature}
UmlsBERT is a semantically-enriched model 
% that initializes from a pretrained 
% BioClinicalBERT model \cite{alsentzer-etal-2019-publicly} 
% clinical BERT model and further 
pretrained on MIMIC-III and the UMLS Metathesaurus; a single linear layer is then added 
for adaptation to downstream classification tasks. In our case, we finetune the model on mortality outcomes from MIMIC-III
following the non-literature-augmented model variant introduced by Naik et al,\cite{naik-etal-2022-literature} achieving 87.86 micro-F1 and 66.43 macro-F1 on a held-out test set.
% \footnote{We could not replicate the results obtained by \cite{Naik2021LiteratureAugmentedCO} exactly, but all differences were within $\pm$0.5 points F1.}


\paragraph{Implementation of XAI Methods} \label{sec:XAI_implementation}

We apply post-hoc XAI methods to our model predictions to create explanations. We experiment with 
% XAI methods representing all explanation forms categorized by \cite{Chaddad_2023}, including 
feature map, textual, and example-based explanation forms,\cite{Chaddad_2023} specifically LIME,\cite{Ribeiro2016WhySI} Attention-based explanations,\cite{Vaswani2017AttentionIA} exemplar explanations through similar patient retrieval, and free-text rationales from an LLM:

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/XAI_Samples_2.png}
    \caption{Example outputs of the four XAI methods applied to an MIMIC-III admission not, with color code and intensity indicating feature importance as detailed in the respective sections.
    }
    \label{fig:XAI_samples} 
\end{figure}

\begin{itemize}[topsep=1pt, itemsep=1pt, leftmargin=10pt]
    \item \textbf{LIME} \cite{Ribeiro2016WhySI} is a model-agnostic XAI method that provides feature-based explanations. LIME perturbs the input data by altering or removing features and observes corresponding changes in the model's prediction. Following LIME, we train an explainer module that treats each word in the input document as a feature, and identifies relevant features in a given patient’s admission note that contribute to the prediction result for each of the two outcome classes: mortality (positive) and survived (negative). We present these explanations by highlighting words identified as relevant features for each class, with the intensity of the highlight reflecting the magnitude of the feature's importance. We introduce a percentile variable to limit the number of highlighted words to only the top percent of features based on their importance scores.
    \item \textbf{Attention-based} explanations use weights from attention-based models \cite{Vaswani2017AttentionIA} to explain model decisions, making it a model-specific method. We apply the approach outlined by Falaki et al.\cite{falaki2023attention}~to extract attention weights from our UmlsBERT model's [CLS] tokens and recombine subword tokens into words for visualization. We highlight the top 30\% of words by attention weights, with the intensity of the highlight defined by the weight value.
    \item \textbf{Similar Patient Retrieval} is an exemplar-based, model-agnostic XAI method aimed at producing explanations by retrieving the closest examples from a training dataset. We fine-tune UmlsBERT on mortality prediction and semantic textual similarity through contrastive learning following the SentenceBERT framework.\cite{Reimers2019SentenceBERTSE} We embed all patients with this finetuned model, then apply $k$-nearest neighbor retrieval using cosine similarity to identify similar patients. At inference time, we retrieve the top-3 most similar patients from the training split with the same outcome as what is predicted to show as exemplars. To facilitate visual comparison of similarities and differences between retrieved patients and the test patient, we apply Named Entity Recognition (NER) using scispaCy\cite{neumann-etal-2019-scispacy} and highlight matching entities between test and retrieved notes in orange and non-matching entities in blue. 
    \item \textbf{Free-text Rationales} are a model-agnostic XAI method that attempt to generate human-comprehensible explanations in natural language. Recently, this has often been achieved by prompting LLMs such as GPT-4,\cite{openai2024gpt4} as we do in this work. Our prompts can be found in our Github. We sample six admission notes and their ground-truth labels from the train split based on the method proposed in Liu et al.\cite{liu-etal-2022-makes} for use as in-context examples. We then present the test note and ask for the top 3 reasons for and against the predicted outcome label.
\end{itemize}


Several of these methods are \emph{model-agnostic}, i.e., explanations are generated by a separate model than the one making the prediction. These methods leverage surrogate models and perturbation techniques to approximate prediction model behavior; because the mechanism of the explanatory model differs from the prediction model for these methods, researchers have questioned the fidelity of their explanations.\cite{danilevsky2020survey} For this reason, we also include the \emph{model-specific} method (Attention-based explanations) in our investigation.
 
Figure~\ref{fig:XAI_samples} shows examples of the four XAI methods applied to an example patient admission note. In each case, we give the admission note, model prediction, and in some cases the model itself, as input into the XAI method. We then conduct additional postprocessing of the results to facilitate visualization and comparison. Details and code implementations for all methods can be found on Github: \href{https://github.com/JuneHou/XAI_MOR_Survey.git}{https://github.com/JuneHou/XAI\_MOR\_Survey.git} 
%\lucy{the link doesn't work; make the repo public?} We briefly describe how we implement each of the four XAI methods below:
% \footnote{The details and codes of implementation methods can be found in our github repository:\url{https://github.com/JuneHou/XAI_MOR_Survey.git}}

% \textbf{LIME} \cite{Ribeiro2016WhySI} is a model-agnostic XAI method that provides feature-based explanations. LIME perturbs the input data by altering or removing features and observes corresponding changes in the model's prediction. These changes are then used to evaluate the influence of each feature on the decision boundary.
%  %\lucy{this last sentence comes out of nowhere, i'm not sure what you mean by `text with highlighted words' section. here, what would be useful is a one sentence descriptor of how LIME works. how is it generating feature-based explanations?}.
% Following the LIME method, we train an explainer module that treats each word in the input document as a feature, and identifies relevant features in a given patient’s admission note that contribute to the prediction result for each of the two outcome classes: mortality (positive) and survived (negative).

% We present these explanations by highlighting words identified as relevant features for each class
% % found to be relevant for the model's decision-making process for each class 
% (mortality class highlighted in orange, survived class in blue), with the intensity of the highlight reflecting the magnitude of the feature's importance. 
% % Red highlights are features associated with the mortality outcome; blue highlights are features associated with the survived outcome. Moreover, the intensity of each highlight reflects the magnitude of the influence that the word had on the model's decision.
% To prevent overwhelming the text with too many highlights, we introduce 
% % a new variable, the 
% a percentile variable to limit the number of highlighted words to only the top percent of features based on their importance scores. A 20\% threshold was selected to provide highlights for words influential to both outcome classes. We additionally collect physician preferences for adjusting this percentile.
% % for the amount of highlighted words in the survey study.

% \vspace{2mm}
% \noindent \textbf{Attention-based} explanations use weights from attention-based models \cite{vaswani2017attention} to explain model decisions, making it a model-specific method. 
% \cite{wiegreffe2019attention} demonstrate that attention weights can enhance transparency and provide plausible explanations for model decisions. These weights have been used as an interpretation tool for disease diagnosis tasks by \cite{franz2020deep}.
% %\todoit{cite other work that has used this method} 
% % This is an intrinsic explanation method that attemps to interpret model output based on input token attention weights.
% % In transformers, the attention mechanism enables NLP model to focus on different tokens with varying level of attention, which reflects the importance. %\lucy{this analogy isn't really accurate, better to replace with a brief definition of what attention means in transformers} 
% % This 'attention' can be extracted in the form of weights. We first extract the attention weight of the [CLS] token in the last hidden layer of the BERT model, because this special token is proved can be representation of sentence in the classification task by \cite{devlin2019bert}. 
% %\lucy{is this the same method that's used in prior work? if so, can just cite the prior work instead of providing so much detail}.
% % UMLS-BERT \cite{michalopoulos-etal-2021-umlsbert}, the model we use for mortality prediction, is a variant of BERT \cite{Devlin2019BERTPO} and fits the bill. 
% % BERT models have a special [CLS] token prepended to every sentence that computes an aggregate sequence representation, and we extract its attention weights to determine the importance of other tokens in the sequence. 
% % Since UMLS-BERT uses WordPiece tokenization, 
% We apply the approach outlined by \cite{falaki2023attention} to extract attention weights from our UMLS-BERT model's [CLS] tokens and recombine subword tokens
% into words for visualization.
% % and extract attention weights from the BERT [CLS] token. 
% We highlight the top 30\% of words by attention weights, with
% % as baseline threshold; 
% the intensity of the highlight defined by the weight value.
  

% \vspace{2mm}
% \noindent \textbf{Similar Patient Retrieval} is an exemplar-based, model-agnostic XAI method aimed at producing explanations by retrieving the closest examples from a training dataset. A fine-tuned embedding model is used to compute patient embeddings and retrieve the most similar patients based on cosine similarity. Due to the length of admission notes, we additionally employ named entity recognition (NER) to highlight shared and disjoint entities between the patient of interest and retrieved similar patients to facilitate comparison.

% %\todoit{what are you using as the embedding? [CLS]? average word tokens? how many hidden layers?}. 
% We compute embeddings for patient admission notes in our dataset using a fine-tuned UMLS-BERT model.
% % fine-tuned based on the Sentence-BERT framework \cite{Reimers2019SentenceBERTSE}. 
% We adapt the Sentence-BERT \cite{Reimers2019SentenceBERTSE} framework for fine-tuning, but instead of NLI, we fine-tune on a triplet dataset we derive from our mortality prediction dataset. We sample pairs of notes from the training split, and assign ``entailment'' or ``contradiction'' based on whether they have the same or opposite mortality label.
% % , we assign ``entailment'' or ``contradiction'' accordingly.
% This mortality similarity fine-tuning dataset consists of 27,164 pairs.\footnote{Due to computational limitations, our sampling was constrained to 40\% of the available data.} We fine-tune UMLS-BERT model on mortality and continue to train for semantic textual similarity (STS) as in \cite{Reimers2019SentenceBERTSE}.\footnote{A comparison of performance shows modest further improvement on the mortality similarity task with additional fine-tuning on STS. On 1,000 random samples from our test set (500 survived, 500 mortality), with top-3 used in this study, the model fine-tuned on STS performance better on Micro-F1 +0.006, on Macro-Precision +0.018, on Macro-Recall +0.006}. Given this result, we choose the latter model as the embedding model. %Detailed hyperparameters and performance metrics are given in Appendix~\ref{appendix:STS}.

% We embed all patients with the fine-tuned UMLS-BERT model, then apply $k$-nearest neighbor (KNN) retrieval with cosine similarity as the distance metric to identify similar patients.  %\lucy{move hyperparameters to appendix}
% At inference time, we retrieve the top-3 most similar patients from the training split with the same outcome as what is predicted to show as exemplars.
% To facilitate visual comparison of similarities and differences between retrieved patients and the test patient, we apply NER 
% % to the admission notes 
% using scispaCy \cite{Neumann_2019}, and highlight matching entities between test and retrieved notes in orange and non-matching entities in light blue. 
% % Any entities that match between the test note and the retrieved notes are highlighted in orange, while non-matching entities are highlighted in light blue.

% \vspace{2mm}
% \noindent \textbf{Free-text rationales} are a model-agnostic XAI method that attempt to generate human-comprehensible explanations in natural language. Recently, this has often been achieved through the use of generative LLMs such as GPT-4 \cite{openai2024gpt4}. 
% %\lucy{move this paragraph to appendix and summarize in two sentences at the end of this subsection}
% %\lucy{how does this work? we don't have ground truth rationales so not sure this qualifies as ICL} 
% %\lucy{if this is about getting GPT-4 to make outcome predictions, let's just remove this paragraph (or move it to somewhere in the appendix where we can discuss the experiments you ran testing GPT-4 as the base model for outcome prediction)}
% We use GPT-4 
% % as the explainer 
% to generate rationales to explain our base model's binary mortality prediction.\footnote{We initially experiment with prompting the LLM to produce both the prediction and rationale (which would make this a model-specific method) but did not fine-tune GPT-4's predictive accuracy on the mortality task to be better than fine-tuned UMLS-BERT. 
% % \jun{I added the experiments with two strategy of randomly selected samples.}
% We therefore only use GPT-4 to generate 
% % supporting and counter-factual free-text 
% rationales for use in our survey. Our prompts can be found in our github.} 
% %\lucy{not understanding why this is a few-shot strategy} 
% %\lucy{based on what model?}
% We prompt the model with six train-split admission notes sampled based on the method proposed in \cite{Liu2021WhatMG} along with their ground-truth labels. We then present the test note and ask for the top 3 reasons 
% % rationales 
% behind the predicted outcome label. Additionally, we prompt the model in the counterfactual case to generate reasons supporting the alternative outcome label that was not predicted.
% % presented the model with a counterfactual case to generate reasons and rationales supporting the alternative outcome label that was not predicted. 
% %\lucy{are these all ICL examples or is it 5 ICL examples plus the actual test case we're querying?} 


\paragraph{Survey Design} 
\label{sec:Survey_Design}
% The objective of our survey is to explore clinical practitioners' perceptions and preferences towards XAI techniques when applied to explain AI-powered decision support. 
To explore clinician perceptions of XAI methods, we design a survey eliciting feedback for each of the implemented XAI methods and comparing across them.
% whether and how much each XAI method improves clinician understanding of the predictive model, 
We structure our survey into key sections as shown in Figure~\ref{fig:survey_flowchart}. %After obtaining consent, we first assess the participant's baseline perceptions of AI and its use in clinical medicine. Then, we expose participants to each of the four XAI methods, order randomized, and collect feedback, followed by questions designed to assess preference among the different techniques. Lastly, we collect socio-demographic information. 
Following survey completion, we follow up with a subset of participants
% is invited to join a follow-up online interview 
to better understand their preferences and the rationales behind their answers. 
% In the survey, participants were shown the admission notes from one patient with each outcome drawn from this sample. The order of XAI methods shown in the survey was also randomized.
We describe the design of each survey section below:

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth] {figures/FlowChart_SurveyXAI_2.png}
    \caption{ 
    Survey design and workflow}
    \label{fig:survey_flowchart} 
\end{figure}

\begin{itemize}[topsep=1pt, itemsep=1pt, leftmargin=10pt]
    \item \textbf{Baseline Attitudes}: Personal experiences with AI systems have been found to impact user perceptions of AI.\cite{brauner2023does,kaya2024roles}
    Therefore, we ask participants to choose a set of five values from Jakesch et al.\cite{jakesch2022different} that they consider most important for clinical AI systems and to indicate their attitudes towards AI on a 5-point Likert scale. 
    
    \item \textbf{XAI Perceptions \& Preferences}: We show participants four samples, one of each of the XAI methods with order randomized.
    % , generated as described before. 
    Each XAI technique used in our study is accompanied by a detailed explanation of how it works at the beginning of the respective section.
    % \jun{Address reviewer 2 question regarding lack of insight into the model by healthcare providers}
    To offset patient- and outcome-specific biases, we sample two patients for each of the mortality and survived outcomes to include. 
    % \jun{more details about the randomize of samples to address reviewer 3 question?} 
    We review XAI outputs to ensure that examples shown to users in the survey do not contain obviously incorrect or irrelevant information. 
    
    After each example, the participant answers Likert-scale questions on the understandability, reasonableness, and usefulness of that XAI method; these facets are inspired by the evaluation of explainability and interpretability described by Saeed et al.\cite{saeed2023explainable}~A free-text field is provided to allow the participant to express pros and cons in their own words. For each XAI method, we design additional questions specific to the characteristics of that method, e.g., whether the percent of words highlighted or number of similar patients retrieved are too many or too few. 
    % After answering all questions for one XAI method, the participant moves on to the next method.
    
    Following the evaluation of all four XAI samples, the participant is asked to rank the understandability and reasonableness of all four methods, and indicate their preference among them. We further ask the participant to assess the time efficiency of each method, and whether each method achieved the goals of enhancing confidence, broadening perspective, and increasing trust, three criteria defined in prior work as goals of XAI.\cite{antoniadi2021current}
    \item \textbf{Socio-demographic Information} We collect self-reported gender, age, and race/ethnicity from participants, along with their highest level of education and number of years of clinical experience. Participants also reported their job title, which we categorize into different job positions for reporting.
\end{itemize}



\paragraph{Study Recruitment} \label{sec:recruitment}
% The participants are from two cohorts. 
Participants are eligible if they are
% We screening people working in the domain with the keywords including, healthcare, nurse, doctors. After receiving the proposal, the pre-screening survey is used to find the cohort that is either 
clinical practitioners with more than two years of clinical experience OR medical school students with at least two years of training. We required that all participants be over 18 years of age, have at least a bachelors degree, and are located in the US. 
% For survey participants, we invited physician or nurse who works in ICU setting or surgical and critical care to participant in the follow-up interview. 
% The full description of these cohorts socio-demographic information is shown in Section~\ref{sec:cohort}. 
Participants were recruited using the Upwork platform,
% \footnote{https://www.upwork.com/} 
and compensated at rates of \$15-\$25 per hour for completing pre-screening, the main survey, and any followups. 
% Our recruitment materials can be found in Appendix~\ref{appendix:recruitment}.
%\lucy{put recruitment information here. used upwork and paid \$XX-XX. add recruitment materials to the appendix}
Our study was found exempt by the IRB at the University of Washington (STUDY00019118). 



\section*{\textsc{Results}} \label{sec:results}
\vspace{-1mm}
% \subsection*{Cohort Description} \label{sec:cohort}

We recruited 32 clinical practitioners to participate in our study. 
%Participant demographics and work information are provided in Table~\ref{table:dem_info}. 
A majority of participants identified as white (75\%), followed by hispanic/latino/a/x (12.5\%), South Asian (6.3\%), African-American/Black (3.1\%), and Southeast Asian (3.1\%). 78.1\% of participants identified as female. Participant distribution across age groups is more balanced. 
% \jun{We don't have participants from other race}

The highest level of education obtained were community college (3.1\%), undergraduate degrees (53.1\%), Masters degrees (25.0\%), medical degrees (15.6\%), and doctorate degrees (3.1\%). The experience levels of participants varied, though a large majority had over 5 years of experience in clinical medicine (12.5\% with 2-5 years, 37.5\% with 5-10 years, 40.6\% with 10-20 years, and 9.4\% with $>$20 years).
Most survey participants are registered nurses or nurse practitioners (78.1\%), with others who identified as doctors/physicians (9.4\%), researchers (6.3\%), or other (6.3\%).
% clinical/technical staff (18.8\%), leadership/management (12.5\%), researchers (6.3\%), and residents (3.1\%) \lucy{these previous percentages are really different from current numbers in tables} \jun{I have checked the Upwork freelancer titles and descriptions; for nurse coordinators and leading nurses who claimed themselves as registered nurses, I grouped them into the RN category. Residents and physicians are grouped into the Doctor/Physician category.}.
%Most of our survey participants are currently registered nurses or nurse practitioners (78.1\%), with others who identified as Doctor/physicians (9.4\%), clinical practitioners (6.3\%), researchers (6.3\%).
% Table~\ref{table:dem_info} shows the breakdown over education level, experience, and current position.

% \todoit{order age by the age range; maybe horizontal stacked bar graphs instead}
\iffalse
\begin{table}[t!]
\footnotesize
\centering
\begin{tabular}{llllll}
\toprule
\textbf{Race/Ethnicity} & & \textbf{Gender} & & \textbf{Age} &\\
\midrule
White & 75.0\% & Female & 78.1\% & 25-30 & 15.6\% \\
Hispanic/Latino/a/x & 12.5\% & Male & 21.9\% & 31-35 & 37.5\% \\
South Asian & 6.3\% & & & 36-40 & 18.8\% \\
African-American/Black & 3.1\% & & & $>$40 & 28.1\% \\
Southeast Asian & 3.1\% & & &  \\
% \lucy{Other?} & & \\
\bottomrule
\end{tabular} \\[2mm]
\footnotesize
\begin{tabular}{llllll}
\toprule
\textbf{Highest Education} & & \textbf{Years of Experience} & & \textbf{Position Distribution} & \\
\midrule
Community college & 3.1\% & 2 - 5 years & 12.5\% & Registered Nurses & 78.1\% \\
Undergraduate degree & 53.1\% & 5 - 10 years & 37.5\% & Doctor/physicians & 9.4\% \\
Medical degree & 15.6\% & 10 - 20 years & 40.6\% & Researchers & 6.3\% \\
Master's degree & 25.0\% & $>$ 20 years & 9.4\% & Other & 6.3\% \\
Doctorate degree & 3.1\% &  & &  & \\
\bottomrule
\end{tabular}
\caption{Participant demographics and work information. \lucy{cut tables and include any additional details in text}
}
\label{table:dem_info}
\end{table}
\fi

\subsection*{Summary Findings} 
\label{sec:Statistics}
\paragraph{What Clinicians Value in AI} 
Clinicians were most likely to consider \textit{safety} and \textit{performance} important. Clinicians without AI experience selected \textit{privacy} more often, while those with AI experience cared more about \textit{beneficence}. \textit{Accountability}, \textit{human autonomy} and \textit{transparency} were also rated as relatively important among all participants. 
% No participant selected \textit{solidarity}.

\paragraph{Attitudes Towards AI} 
% We asked participants to report their overall attitude towards AI (answers on a Likert scale with 1 as Most negative and 5 as Most positive). 
Figure~\ref{fig:Combined_Attitude_Plot} reports participant self-reported attitudes towards AI, collectively and split into different demographic groups. Overall distributions in attitudes toward AI are similar across groups. Very experienced clinicians ($>$20 years experience) in our cohort do not exhibit any positive sentiments towards AI; however, we note the small sample size (n=3).

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.8\textwidth] {figures/Combined_Attitude_Plot.png}
    \caption{Participant attitudes toward AI by demographic group. }
    \label{fig:Combined_Attitude_Plot} 
\end{figure}

\paragraph{Attitudes Toward XAI Methods} 
Regarding how understandable and reasonable each XAI method is from the clinician's perspective (Figure~\ref{fig:likert_heatmap}(a)), we observe similar results for the four XAI methods. Free-text rationales were found to be the most understandable and reasonable, with no negative responses. 
Attention-based explanations were found to be the least reasonable and hardest to understand. LIME and similar patient retrieval received a similar amount of positive and negative feedback along both dimensions, though similar patient retrieval was rated as more understandable. Clinician preferences toward the quantity of information provided, such as the percent of words highlighted or the number of similar patients and rationales shown, generally lean towards preferring more information.
% , but are inconsistent across participants.

\begin{figure}[t!]
    \centering
    \subfigure[Understandable and reasonable quantified by Likert scale question]{
        \includegraphics[width=0.64\textwidth]{figures/likertx2.png}}
        \vspace{2mm}
    \subfigure[Heatmap for Functionality]{
        \includegraphics[width=0.32\textwidth]{figures/heatmap_new.png}}
    \vspace{-2mm}
    \caption{Overview of practitioners' evaluations on the effectiveness and utility of XAI methods, covering understandability, reasonability, and key functional goals.}
    \label{fig:likert_heatmap}
\end{figure}

\paragraph{Practitioner Preference} 
Overall, free-text rationales were the most preferred method (n=15), with LIME second (n=12). When considering all four methods, no participant ranked Attention-based explanations first.
% were rated as the hardest to understand, with no participant ranking it first. 
The understandability of the similar patient retrieval method showed an almost even distribution across ranks 1-4. However, this method was the least preferred for use in real clinical settings, with an average ranking of 4.

We ask clinicians to assess the time efficiency of their most preferred XAI method on a Likert scale with 1 as least efficient and 5 as most efficient 
%\lucy{instead of most negative and most positive, replace with least efficient and most efficient. otherwise, it's hard to know what's positive and what's negative}. 
For free-text rationales, which is ranked first 15 times, 13\% of participants reported \textit{Minimal time saved}, 20\% \textit{Moderate time saved}, 47\% \textit{Considerable time saved}, and 20\% \textit{Significant time saved}. For LIME, ranked first 12 times, the time savings in the same four categories were assessed as 0\%, 25\%, 67\% and 8\%, respectively. 
In other words, participants were more divided on the time saving ability of free-text rationales.

Figure~\ref{fig:likert_heatmap}(b) visualizes responses towards whether each XAI method succeeds in enhancing confidence, broadening perspective, and increasing trust. The objective of broadening the users' perspective was successfully met by all methods, with more than half of participants agreeing for each method. Enhancing confidence was most effectively addressed by free-text rationales, with 18 participants agreeing, followed by LIME, with 12 participants agreeing. Increasing trust was the most challenging goal, though many more participants indicated similar patient retrieval and free-text rationales as helping to meet this goal. Additional statistics can be found on our Github.

\subsection*{Thematic Analysis} \label{sec:Thematic}
 
We conduct qualitative coding to identify shared themes among participant attitudes towards XAI methods.
The first author conduct open coding on survey responses to identify major themes, with feedback and iteration from other authors. This process led to the identification of 6 themes, which we present and describe in Table~\ref{table:sub-themes}, organized by positive and negative sentiment.  Radar plots in Figure~\ref{fig:thematic_radar} compare the number of participants who raised each theme for each XAI method. 
Below, we summarize the qualitative feedback for each XAI method included in our study. Participants are identified by pseudonyms P1-32.

\begin{table}[t!]
\footnotesize
\centering
\begin{tabular}{lL{35mm}L{88mm}}
\toprule
\textbf{Sentiment} & \textbf{Theme} & \textbf{Description} \\
% \midrule
% \multicolumn{2}{l}{\textbf{Positive}} \\
\midrule
Positive & Presentation is intuitive  &   The output of the XAI techniques (visualizations or text) is presented in a way that is intuitive and easy to understand \\
% , enabling the user to quickly understand AI decisions\\
 & Explanation is accurate &  Highlighted keywords and/or free-form explanations align with clinicians' expectations and thought processes \\
 & Helpful for clinical tasks & Explanations are useful in clinical settings, and could assist with tasks like decision-making or patient communication \\
\midrule
% \multicolumn{2}{l}{\textbf{Negative}} \\
% \midrule
Negative & Presentation is unintuitive & The output of the XAI techniques (visualizations or text)
% Visualization techniques 
require further clarification or explanation to be understood by clinicians \\
 & Explanation is inaccurate & Explanations are irrelevant or do not align with clinicians' expectations \\
 & Explanation is incomplete & Additional information is required to fully support decision-making processes \\
\bottomrule
\end{tabular}
\caption{Themes grouped by sentiment with descriptions.
% \lucy{i changed the theme names; please review} \jun{Updated the graph}
}
\label{table:sub-themes}
\end{table}


\begin{figure}[t!]
    \centering
    \subfigure[Positive sentiment]{%
        \includegraphics[width=0.45\textwidth]{figures/Thematic_positive.png}
        \label{fig:thematic_positive}
    }
    \subfigure[Negative sentiment]{%
        \includegraphics[width=0.45\textwidth]{figures/Thematic_negative.png}
        \label{fig:thematic_negative}
    }
    \caption{Radar plots of themes mentioned by participants for each XAI method, grouped by sentiment.}
    \label{fig:thematic_radar}
\end{figure}

\paragraph{LIME} 
% As shown in Figure~\ref{fig:thematic_radar}(a), t
The visualization technique of LIME is favored by 12 practitioners, with reasons including \textit{``shade intensity was very helpful in showing how important the phrase was''} (P7) and \textit{``can be helpful in drawing quick conclusions''} (P4). The use of orange and blue for negative and positive features was described as intuitive by 6 practitioners. 
Highlighted words were found to match the clinician's thought process (14 participants); P15 mentioned \textit{``highlighting words such as `intubated' and `unresponsive' is good.''}, and P17 confirmed, \textit{``I found favorable that the system considered more influential (darker shade) the age of the patient.''} P30 offered one potential use of LIME in clinical settings by expressing that \textit{``The red highlighted words did correlate with a mortality risk and were helpful in identifying these risks in the text.''}
Despite generally positive feedback,
some highlighted words were considered not correlated with the outcome (P8) or were located in regions of the admission notes considered irrelevant for prediction (P21 and P28). Furthermore, several participants (P8 and P15) expressed a desire for more unique span highlights rather than duplicates of already highlighted words.
% that contribute to the outcome that are not duplicates of already highlighted words.
% , such as the span `high blood pressure in the 230s.'


\paragraph{Attention-based Highlights}
The attention-based method received the most feedback for the need to improve the quality of highlights, with 21 participants mentioning the low relevancy of highlights to the outcome. Several participants were especially bothered by highlighted words like `of' or `with,' 
described by P12 as ``random'' and P22 as ``distracting from the outcome.''
% commenting that the method \textit{``pick[ed] slightly random words especially in the survived case''} and P22 wanting \textit{``less highlighted words distracting from the outcome.''} 
P2 discussed potential benefits, such as the model \textit{``highlighting important words such as `cardiac' and `received CPR' because my brain thinks those are important too,''} suggesting areas where the model performs well in identifying relevant information.
Regarding visualization, the single-color highlights were favored by 9 participants for their simplicity.
P13, as the only participant commenting on utility for clinical tasks, mentioned that the highlights \textit{``contribute to the respective viewer taking a Quick Look back to make sure no information is missed,''} highlighting the potential of visualization techniques to aid in clinical decision-making.

\paragraph{Similar Patient Retrieval}
Similar patient retrieval was found to be clinically helpful by 7 clinicians:
% as shown in Figure~\ref{fig:thematic_radar}: 
for administrative tasks (P3), planning of treatment (P4, P19, P25), supporting diagnosis (P9, P12, P19), and \textit{``guiding and auditing manifestation and intervention''} (P11). P2 commented that the highlighted NER terms aligned with their thought process, while
% \textit{``as I have seen this in my own experience.''} 
P17 discussed accurate retrieval as a valuable feature of this technique, 
% The accuracy of retrieval, demonstrated by shared valuable features for P17, is notable, 
especially in terms of \textit{``similar age (elderly),... abnormal electrocardiogram signs''} and other shared treatments and symptoms. However, additional clarification in terminology is suggested by P28, who noted \textit{``The use of short hand/abbreviations should be minimized. As this could lead to confusion.''} P27 discusses how this could be used to improve retrieval relevance: \textit{``Consistent use of the same language...
% as opposed to varying 
(eg s/p cardiac arrest vs s/p PEA arrest) 
would help to pull similar patients.''}

Regarding visualizing NER overlap, 9 participants liked this feature, 
% e.g.,
% ability to retrieve similar patients, such as 
% P9 says the highlights 
which allows for \textit{``Quickly referencing similarities in past medical history and treatment''} (P9). 
% Color choices in highlighting were deemed valuable by P1, who 
P1 says \textit{``color related to higher match is helpful.''} P14 suggests the potential for color to convey more detailed categories, e.g., \textit{``Meds: orange, Procedures: yellow, vitals: purple,''} to 
% be more beneficial for 
assist with comparison. However, there is room for improvement in the quality of explanations (14 participants), such as replacing non-contributory medication highlights with more influential words (P16), and wanting higher similarity between queries and retrievals, especially in areas like chief complaints.

\paragraph{Free-text Rationales}
% Figure~\ref{fig:thematic_radar} indicates that the 
Free-text rationales received the most positive feedback,
% were well-received, 
with 16 participants indicating that its outputs were intuitive and 15 indicating that the explanations were accurate. 
% For example, P17 finds \textit{``very valuable the organization in which the explanation was presented.''}
% Reasonable r
Rationales were found to enhance predictability through conciseness of presentation (P16, P17, P19) and accurate reasoning (P23, P24). Participants indicate rationales can aid in clinical tasks such as prognosis (P11, P17, P30), prioritization (P13), decision-making (P15, P19), and treatment planning (P27), offering clarity for understanding (P11, P21) and facilitating communication with non-experts like patients (P12, P30). For improvement, participants suggested considering more contextual details like \textit{``high doses of pressors in the mor[t]ality rate''} (P1) and 
% a more rational ranking, 
the strength of the rationale, e.g., \textit{``the medication allergy reasoning was weak''} (P8). Incorporating quantifiable scores (P7)
% , such as \textit{``quantify risk (i.e. high risk for mortality)''}(P7), 
and \textit{``evidence-based protocols in the rationales, e.g. AHA''} (P11) could provide further 
% refine 
support for outcome predictions, with P21 suggesting that \textit{``Adding confidence levels or percentages would significantly improve how trustworthy this algorithm is.''} 
For presentation enhancements, P31 suggests \textit{``Bold faced and highlighted words for important info''}. On the other hand, use of jargon 
% like `resuscitation' 
and excessive reasoning were found to reduce explanation clarity (P4).


\section*{\textsc{Discussion}} \label{sec:discussion}
\vspace{-1mm}
Our analysis of survey results raises questions about the goals and benefits of XAI methods, and how to increase their relevance and utility to clinical practitioners. We synthesize these findings into additional recommendations below.

\paragraph{The importance of providing evidence}
Similar patient retrieval, while critiqued for its accuracy and effectiveness,
demonstrated a greater ability to build trust than feature-based methods such as LIME and Attention-based highlights. Using historical cases as evidence to support decisions closely mirrors how practitioners rely on past experiences in clinical decision-making. To be successful from the practitioners' perspective, our findings suggest the importance of not only generating accurate and informative rationales but also incorporating evidence-based support (as with the exemplars in similar patient retrieval) in model explanations. While free-text rationales were received positively by participants, the lack of grounded evidence needs to be considered. Combining free-text rationales with retrieved exemplars or externally retrieved evidence (as in Naik et al.\cite{naik-etal-2022-literature}) could help address these issues in future work.
% or tracking features.


\paragraph{Potential of free-text rationales to bridge communication}
Explanations in natural language that reflect the cognitive processes of clinicians can serve as a communication bridge within the healthcare system. 
P27 mentioned that 
in time-sensitive scenarios, generated content can function similarly to a nurse's note for communicating with a doctor. Additionally, generated rationales can be an educational tool, e.g., P27 described \textit{``Especially in the trauma setting, the workflow is very fast, and you got residents attending...even if it's not a teaching hospital, it is extremely helpful.''}
The method also has the potential to bridge the gap between healthcare providers and patients by explaining symptoms and treatments in a non-technical way, as mentioned by P17, \textit{``this model...it would be a good thing to not maybe show the family members, but to explain, okay, we use this model and this is what the outcomes are saying.''}
However, based on participant suggestions to simplify wording in free-text responses, future work should consider integrating plain language summarization\cite{Guo2020AutomatedLL,Guo2023APPLSEE} to enhance the understandability and efficiency of LLM output. 

\paragraph{XAI for both structured and unstructured data}
In the critical care setting, especially in ICUs, treatment plans and bedside monitoring rely heavily on both structured data (such as vital signs and lab results) and unstructured data (such as clinical notes). 
Several participants mentioned the need to incorporate multimodal data into an XAI-enhanced CDSS. While this is beyond the scope of our study, we emphasize that real-world CDSS would likely take advantage of input predictors beyond the clinical note text, and that the importance of these predictors would also require explanation.
This could be an addition to ongoing research that aims to create predictive frameworks combining unstructured textual data with structured data for clinical prediction.\cite{ebrahimi2023lanistr}
% However, there is still room for the evolution of XAI methods capable of handling hybrid data.

\paragraph{XAI tailored to different clinical workflows}
Responses from clinical practitioners revealed varied perceptions and preferences for XAI methods at different stages of patient care. In urgent care settings like the ICU or surgery, clinicians prioritize efficiency and clarity of explanations, favoring XAI methods that present key information quickly for reference. However, in less acute phases like post-surgical care, detailed explanations are in demand for analysis and treatment planning, a role well-served by a method like free-text rationales. 
For example, P17 states, \textit{``Highlight saves time and we need that.~If we had more time in clinical settings, I feel like the free text rationale gives a more in-depth reasoning.''}
To balance details and efficiency,\cite{Jung2023EssentialPA} XAI methods must be tailored to specific clinical workflows.

\paragraph{Limitations} Although we have covered all categories of post-hoc XAI methods from Chaddad et al.,\cite{Chaddad_2023} there are many methods that we could not include in this survey due to time and resource constraints (our survey was already long). Applying XAI methods to text data was not always straightforward, as some methods (like LIME) work better on feature-based models with lower-dimensional input. For similar patient retrieval in particular, we face many challenges in fine-tuning the retrieval model due to the lack of labeled datasets for patient semantic similarity search. 

% We have organized our discussion to maximize the generalizability of our findings to other XAI techniques, though future work could explore and provide  comparative studies of such methods applied to specific clinical tasks.

We use only the free-text admission notes from MIMIC-III as the inputs to our prediction models, whereas prediction models and XAI methods can be applied to other data formats, such as tabular and time series data. Furthermore, the predictive task in this study is limited to in-hospital mortality prediction. Future work should explore multimodal outcome prediction models as well as other clinical predictive tasks.

Although we have made significant efforts to recruit, our cohort is still relatively small, involving 32 clinical practitioners who are predominantly nurses. This may limit the generalizability of our findings. However, we did achieve a fairly diverse cohort in terms of age and experience, and many themes were consistently raised by most participants. In the future, we aim to validate our findings in real-world deployments, which we believe will offer valuable perspectives. We also plan to explore whether clinicians can effectively use XAI to identify hallucinations in LLM-powered decision support and mitigate the risks introduced by such systems.

\subsection*{\textsc{Conclusion}}
\vspace{-1mm}
Our survey results reveal the demands and preferences of healthcare practitioners towards the implementation of XAI in CDSS. By integrating clinicians in the evaluation process, we observed a strong preference for XAI techniques that replicate clinical reasoning, such as exemplar-based patient retrieval and free-text rationales. These methods enhance the interpretability and trustworthiness of AI-supported decision making, which can further help in realize the full potential of AI in clinical decision making, ensuring that CDSS are not only effective but also align with healthcare providers' needs. Moving forward, we aim to refine these methods by incorporating structured and unstructured data, tailoring XAI approaches to specific clinical workflows. This will improve the utility and efficacy of CDSS across diverse clinical settings, further supporting healthcare professionals in their decision-making processes.

\section*{\textsc{Acknowledgements}} 
We acknowledge support from the University of Washington Institute for Medical Data Science and the eScience Institute's Azure Cloud Credits for Research and Teaching.

% % References as numbers
% \makeatletter
% \renewcommand{\@biblabel}[1]{\hfill #1.}
% \makeatother

% unstr is used to keep citation order
%\todoit{a lot of these references are to arxiv papers when there is a better version of record from ACL or ACM. i already replaced a few of them. could you do the rest?}
\bibliographystyle{unsrt}
\bibliography{sample}  

% \newpage
% \appendix

% \input{appendix}

\end{document}