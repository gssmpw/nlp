\section{Experiments}\label{sec:experiments}
In this section, we present the experimental details of our work. We examine the models' tendencies to produce hallucinated outputs, using LLM-based evaluation as described in~\ref{sec:methodology}. The results provide insights into the reliability of different ASR systems across various real-world and adversarial conditions.

\subsection{Datasets}\label{subsec:datasets}
In our experiments, we use a diverse set of datasets representing various domains and testing conditions to evaluate the ASR systems under scenarios that differ from training data. To achieve this, we choose datasets from domains with a high likelihood of being unseen during training, ensuring a natural distributional shift. Additionally, we include datasets with synthetic perturbations, such as adversarial attacks, and common augmentation techniques like Gaussian noise addition, pitch shifting, and time stretching, to assess the model's robustness under synthetic shift.


\paragraph{Domain Specific Datasets.} To evaluate model performance under real-world conditions, we leverage datasets from diverse domains, ensuring a comprehensive assessment across various settings. These include legal proceedings: \emph{Supreme-Court-Speech}\footnote{\url{https://huggingface.co/datasets/janaab/supreme-court-speech}}, medical dialogues: \emph{Primock57} \cite{papadopoulos-korfiatis-etal-2022-primock57}, meeting conversations: \emph{AMI} \cite{ami_corpus}, aviation communications: \emph{ATCOsim} \cite{hofbauer-etal-2008-atcosim}, conversational speech: \emph{SLUE-VoxCeleb} \cite{shon2022slue}, home environments: \emph{BERSt}\footnote{\url{https://huggingface.co/datasets/macabdul9/BERSt}}, and general speech corpora: \emph{LibriSpeech} \cite{panayotov2015librispeech}, \emph{GLOBE} \cite{wang2024globe}, and \emph{SPGISpeech} \cite{kensho2021spgispeech}, including noisy conditions (\emph{LibriSpeech$\_$test$\_$noise} \cite{panayotov2015librispeech}). These datasets span a wide range of accents, recording conditions, and environments—from high-quality audiobooks to teleconferences and real-time simulations. This diversity ensures a robust evaluation of model performance across realistic and challenging scenarios, addressing the limitations of single-domain evaluations.

\paragraph{Perturbed Datasets.} To simulate challenging acoustic conditions and evaluate WER and HER under adversarial scenarios, we apply various types of synthetic perturbations to speech inputs. These include an adversarial dataset featuring modified utterances with adversarial noise at varying radii (0.04 and 0.015) and Room Impulse Response (RIR) noise, primarily aimed at adversarially attacking the ASR models~\cite{Olivier2022RI}. Additionally, we evaluate model robustness under challenging conditions by applying a range of general audio perturbations—including noise addition, time stretching, and pitch shift—to 1,000 randomly sampled audio clips from a mixture of domain-specific datasets. These perturbations are commonly used as augmentation techniques to simulate real-world variability and stress-test the models. We compare the performance on perturbed speech with that of non-perturbed speech to quantify the impact of these distortions. Full details about the perturbation methods, including parameters and implementation, are provided in Appendix~\ref{appsubsubsec:perturbation} (Table~\ref{tab:noise_details}). Additionally, comprehensive information about the datasets used in this study can be found in Appendix~\ref{appsubsubsec:datasets} (Table~\ref{tab:datasets}).  

\subsection{Models}\label{subsec:models}


To comprehensively evaluate hallucination patterns in ASR systems, we select models that span diverse architectures, sizes, and training paradigms. This diversity enables systematic analysis of how these factors influence hallucination susceptibility. Specifically, we include:  
\begin{itemize}  
    \item \emph{Encoder-only} models: HuBERT~\cite{hsu2021hubert} and Wav2Vec2~\cite{baevski2020wav2vec}, which leverage self-supervised training to learn robust speech representations.  
    \item \emph{Decoder-only} models: Qwen2Audio~\cite{chu2024qwen2} and SpeechLLM~\cite{Rajaa_SpeechLLM_Multi-Modal_LLM}, optimized for text generation and audio-language alignment.  
    \item \emph{Encoder-decoder} models: Whisper~\cite{radford2022robustspeechrecognitionlargescale} (10 variants), DistilWhisper~\cite{gandhi2023distilwhisper} (4 variants), and SeamlessM4T~\cite{communication2023seamlessm4tmassivelymultilingual} (2 variants), designed for multilingual transcription, translation, and speech-to-text tasks.  
\end{itemize}  
The selected models vary in size (39M to 7B parameters), depth (4 to 32 layers), and training paradigms (supervised, self-supervised, semi-supervised). Full specifications, including architectural details, training data, and hyperparameters, are provided in Appendix~\ref{appsubsubsec:models} (Table~\ref{tab:models}).  






\subsection{Experimental Setup}
We utilize models and datasets sourced from Huggingface~\footnote{\url{https://huggingface.co/models,https://huggingface.co/datasets}}. All audio data is resampled to match the sampling rate required by the respective models. For each dataset, we randomly sample 1,000 examples from the \textit{test} split to ensure a manageable and consistent experimental setup. Unless otherwise specified, we use the default decoding parameters for ASR evaluation. To analyze the data, we compute SONAR embeddings~\cite{duquenne2023sonar} for both speech and text. Additionally, we employ CMD based on prior work~\cite{kashyap2020domain} to quantify domain shifts. All experiments are conducted on a single A100/H100 GPU. Prior to calculating WER and generating embeddings, we apply a basic English text normalizer~\footnote{\url{https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/english_normalizer.py}} to ensure consistency in text preprocessing. For LLM evaluation, we perform greedy search decoding to ensure reproducible outputs. 


\subsection{Human Evaluation}
We construct a human evaluation dataset by aggregating outputs from multiple models, filtering samples with \texttt{WER} > 60 to focus on significant deviations. Hypotheses and references are constrained to 1-100 words for balance. To simulate synthetic hallucinations, we shuffle 50 hypotheses, introducing artificial errors~\cite{stiff2019improving}. The final dataset includes 500 samples, each reviewed by two independent annotators from a pool of 20. This framework ensures robust evaluation and reliable analysis of hallucination patterns across models.
