\section{Limitations}\label{sec:limitations}
We explore the hallucination phenomenon in ASR systems focusing on the potential causes such as distribution shift, model types, and model size and architectures. While our work offers valuable insights into model behavior across different conditions, there are several limitations to consider.


\noindent\textbf{Evaluation Datasets.} 
In our study, we evaluate ASR models across multiple domains, including legal, medical, and conversational speech, ensuring a broad range of datasets not seen during training. However, it is possible that some of the datasets we treat as target domains may have been inadvertently
 exposed to the models during training. Furthermore, the lack of access to a diverse variety of domain-specific datasets limits our understanding of how these models will perform in more diverse or previously unseen domains, particularly those with limited or noisy data. While our focus on domain shifts is an important step, further research is needed to assess model performance in even more varied and challenging real-world environments.


\noindent\textbf{Synthetic Noise and Perturbations.}
Our experiments also include synthetic noise and perturbations to evaluate model robustness. While this approach helps simulate real-world challenges, it does not capture all possible distortions that may occur in uncontrolled environments. Adversarial noise, pitch shifts, and time stretching are some of the perturbations we consider, but other potential real-world disruptions, such as cross-lingual noise or complex acoustic reverberations, are not fully explored.

\noindent\textbf{Hallucination Detection.} 
The detection of hallucinations in ASR systems, as measured by the Hallucination Error Rate (HER), is a key contribution of our study. However, our reliance on LLM-based classifiers introduces potential biases and variability. While we observe strong alignment with human judgments, the accuracy of these evaluations may be influenced by subjective interpretation, especially in edge cases where the boundaries between errors are unclear. Additionally, while LLM-based methods present a novel approach, their performance in low-resource settings or with models trained on limited data has not been fully explored. Furthermore, the use of proprietary models, such as those from OpenAI via API, introduces additional costs, which could limit the scalability of this approach.
