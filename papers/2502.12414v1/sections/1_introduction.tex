\section{Introduction}\label{sec:introduction}

% \input{tables/example_wer_her}

Automatic Speech Recognition (ASR) systems have become fundamental to various applications, including personal assistants, automated customer service, and transcription tools used in fields such as education, healthcare, and law~\cite{zhang2023intelligent, adedeji2024sound}. These systems have seen remarkable improvements in recent years~\cite{arriaga2024evaluation,radford2022robustspeechrecognitionlargescale, communication2023seamlessm4tmassivelymultilingual}, with state-of-the-art models demonstrating their capabilities across diverse datasets and languages~\cite{shakhadri2025samba}. However, the evaluation of ASR performance remains largely dependent on word and character error rate (WER/CER). The primary limitation of WER and CER is their dependence on token-level overlapping, which focuses on matching individual words or characters without considering the overall semantic aspect of the transcription. This could result in misleading evaluations, as a high WER/CER does not necessarily indicate poor outputs in all cases.
% , as shown in Table \ref{tab:wer_vs_hallucination}.  
In addition, these metrics fall short in capturing more subtle semantic failures which aren't typically caught without human verification, such as hallucinations. 



Hallucinations in ASR systems mirror perceptual experiences in neuroscience—plausible outputs generated without grounding in input stimuli~\cite{american2013diagnostic,zmigrod2016neural}, deviating \textit{phonetically} or \textit{semantically} from source speech~\cite{ji2023survey}. Like natural neural perceptions, ASR hallucinations arise when models prioritize distributional patterns over fidelity to audio input, fabricating text unlinked to reference content~\cite{hare2021hallucinations}. These errors are uniquely hazardous in high-stakes domains~\cite{williamson2024era}, as they evade WER/CER detection while distorting meaning, similar to how clinical hallucinations disconnect from reality.

Hallucination in domains such as medical and legal can have serious consequences, including life-threatening outcomes and distorted testimonies or contracts, and may disproportionately affect marginalized groups~\cite{faithful_ai_in_medicine, vishwanath2024faithfulness, mujtaba2024lost, sperber2020consistent, Koenecke_2024}. Hence, detecting and mitigating hallucination is crucial for ensuring the reliability of ASR systems in sensitive environments. 



The existing literature on hallucination detection in ASR models is confined to a single model~\cite{frieske2024hallucinations, serai2021hallucinationspeechrecognitionerrors, Koenecke_2024, barański2025investigationwhisperasrhallucinations, ji2023survey} or test setting~\cite{kim2024automatic}, highlighting a significant research gap for a systematic investigation across different supervision paradigms, test domains, and conditions.




In this work, we address this critical research gap and make the following key contributions:

\begin{itemize}
    \item We conduct a thorough evaluation of ASR models across various setups, consisting of both synthetic and natural shifts from training to test distributions.
    \item We introduce an LLM-based error detection framework that classifies ASR outputs into different types of errors including hallucination errors through context-aware assessments.
    \item We provide an in-depth analysis of hallucination phenomena in ASR models, exploring the impact of domain-specific data, model architectures, and training paradigms, and offer valuable insights into the relationship between model size, type, and hallucination frequency.
    \item To validate our hallucination detection method, we compare our LLM-based hallucination detection pipeline with human evaluations and heuristic approaches, demonstrating that the LLM evaluation closely aligns with human judgments and other LLM-based assessments, unlike the heuristic-based approach.
\end{itemize}


The significance of this work lies in how it redefines the evaluation and improvement of ASR systems. By emphasizing hallucination detection, we aim to enhance the reliability of ASR models, particularly in domains where accuracy and precision are non-negotiable. 


\noindent\textbf{Outline.} In Section~\ref{sec:related_work}, we review prior work related to ASR evaluation and hallucination detection. Section~\ref{sec:methodology} outlines our proposed methodology. Section~\ref{sec:experiments} presents our experimental setup. Finally, Section~\ref{sec:results} discusses the results and implications of our findings, outlining directions for future research. We conclude our work in~\ref{sec:conclusion} and provide limitations in~\ref{sec:limitations}.

\input{tables/hullucination_methods_with_examples}
