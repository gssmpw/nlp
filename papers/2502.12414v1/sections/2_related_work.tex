\section{Related Work}\label{sec:related_work}
The use of ASR systems in high-stakes domains, including healthcare~\cite{afonja2024performantasrmodelsmedical, huh2023improvingmedicalspeechtotextaccuracy, adedeji2024sound, sunder2022buildingasrerrorrobust}, legal proceedings~\cite{saadany2022bettertranscriptionuksupreme, garneau2024statecommercialautomaticfrench}, and finance~\cite{Del_Rio_2021, 10389617}, has heightened the necessity for ensuring their robustness. Conventionally, the performance of these models is assessed using metrics such as WER and CER~\cite{serai2022hallucination}.~\citet{szymanski-etal-2023-arent, sasindran2024semascore} They show that when these metrics are used in isolation, they exhibit notable limitations.

Recent studies have extensively investigated hallucination in text generated by large language models (LLMs), identifying it as a prevalent phenomenon~\cite{Huang_2025, bai2024hallucinationmultimodallargelanguage, yao2023llm, jiang2024surveylargelanguagemodel,maynez2020faithfulness,parikh2020totto,ji2023survey,mittal2024towards,filippova2020controlled}. This issue has also been observed in audio foundation models~\cite{sahoo2024comprehensivesurveyhallucinationlarge}. Furthermore, research suggests that pretraining language models for predictive accuracy inherently predispose them to hallucination, even under ideal conditions where the training data is entirely factual~\cite{kalai2024calibrated}. 

However, few studies explore hallucination evaluation and detection in automatic speech recognition (ASR) systems, with most research focusing on \emph{Whisper}, a semi-supervised model. For instance, \citet{koenecke2024careless} analyze the Aphasia dataset and report that while \emph{Whisper}’s overall hallucination rate is 1\%, 40\% of these hallucinations contain violent or harmful content. Similarly, \citet{kim2024automatic} demonstrate that \emph{Whisper} hallucinates at significantly higher rates under low signal-to-noise ratio (SNR) conditions, and observe a 20\% increase in hallucinations at -4 dB and -2 dB SNRs. Prior work by \citet{serai2021hallucinationspeechrecognitionerrors} propose augmenting models with hallucinated transcripts to improve performance, while \citet{frieske2024hallucinations} introduce a perturbation-based evaluation method using automatic metrics such as word error rate (WER), perplexity, and cosine similarity. \citet{baranski2025investigation} develop a filtered Bag of Hallucinations (BoH) for detection, and reveal that hallucinations in \emph{Whisper} correlate strongly with training data biases (e.g., phrases like ``Thank you for watching'' linked to YouTube content).


Despite these advances, existing studies remain limited in scope, focusing predominantly on semi-supervised models like \emph{Whisper}. This highlights a critical gap: the lack of a comprehensive understanding of hallucinations across the full spectrum of supervision paradigms—from supervised to unsupervised models—and under domain shifts where test data distributions diverge sharply from training environments. Addressing these gaps is essential for developing robust ASR systems that maintain accuracy and faithfulness in diverse real-world applications, a challenge our work directly tackles by evaluating a wide range of models with diverse architectures, sizes, and training paradigms on synthetic and natural shifts. 

