\section{Ethics Statement}\label{sec:ethics_statement}

\noindent\textbf{Data Collection and Release.} 
For this study, we rely on publicly available datasets from diverse domains to evaluate hallucinations in ASR systems. We ensure that the data used in our research is appropriately sourced, maintaining respect for copyright, license, and privacy regulations. Furthermore, we emphasize that the use of these datasets is strictly for academic purposes, aligned with the principles of fair use. 

\noindent\textbf{Intended Use.} 
Our work aims to enhance the robustness of ASR systems, especially in high-stake domains where errors can have significant consequences. We believe our findings will encourage further research in hallucination detection, with particular attention to models' performance in low-resource and critical domains such as healthcare and law. By introducing the Hallucination Error Rate (HER) as a complementary metric to traditional evaluation methods, we hope to inspire the development of more reliable and transparent ASR systems.

\noindent \textbf{Potential Misuse and Bias.}
While our work provides valuable insights about hallucinations in ASR systems, we acknowledge that they could be misused if deployed in inappropriate contexts. Since these models are trained on a variety of data sources, there is the potential for them to generate biased or harmful content, especially if the training data contains any inherent biases. Moreover, hallucinations in ASR outputs, if undetected, can lead to severe consequences in critical applications such as legal, medical, and financial settings. We recommend careful deployment of these models, ensuring that they undergo rigorous bias mitigation and hallucination detection processes before being used in such domains.
