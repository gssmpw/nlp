\section{Results}\label{sec:results}
In our experiments, we use an LLM-based pipeline to classify ASR errors across various evaluation setups, validating it against human evaluation and heuristic baselines. We then explore the effects of natural and synthetic distribution shifts on error metrics, specifically examining how domain variations and input perturbations impact word and hallucination error rates. Additionally, we analyze the influence of model architecture and scale through a comparison of Whisper variants and other architectures. This approach provides insights into the complex interactions between error types, data conditions, and model characteristics. In this section, we present our findings. 


\input{tables/coarsegrained_her_wer}

\subsection{Hallucination Error Detection}

We assess the ASR outputs of large language models (LLMs) by classifying them into various error categories. Specifically, we use \emph{GPT-4o mini} to identify the types of errors in ASR outputs. The evaluation process includes both coarse-grained and fine-grained error classifications, as detailed in Section~\ref{subsec:hallucination_evaluation}. The prompts used for both evaluations are shown in Figures~\ref{fig:coarsegrained_prompt} and~\ref{fig:finegrained_prompt}.


\paragraph{Coarse-grained and Fine-grained Evaluation.} Figure~\ref{fig:pie_finegrained} in the appendix illustrates the error distributions across coarse-grained and fine-grained hallucination categories. Our results demonstrate strong alignment between both levels, indicating consistent classification of hallucinations. Among non-hallucination errors, phonetic errors dominate across most datasets (see Appendix Table~\ref{tab:non_hallucination_error_analysis}). However, in \emph{Primock57}, language errors prevail, likely due to its specialized medical terminology. This aligns with~\cite{ferrando2024know}, who emphasize language models' struggles with domain-specific named entities. This is also reflected in the third example provided in Table~\ref{tab:examples}.



\paragraph{Agreement with Human Evaluation and Heuristic Baseline.} To validate our approach, we compare \emph{model-to-human} and \emph{human-to-human} agreement scores using a coarse-grained prompt. Our results demonstrate strong human-to-human raw agreement (0.71), indicating consistency. Additionally, we observe good agreement (0.6) between human annotations and \emph{GPT-4o-mini}'s coarse-grained output, suggesting that the model aligns reasonably well with human judgments.
We further evaluate the agreement between human and model classifications against a heuristic baseline proposed by~\cite{frieske2024hallucinations}. Their method is based on a cosine similarity threshold of 0.2, alongside a \emph{WER} threshold of 30 and a \emph{Flan-T5}~\cite{chung2024scaling} perplexity threshold of 200. However, as shown in Table~\ref{tab:agreement_scores}, this heuristic achieves significantly lower agreement scores: 0.1 with \emph{GPT-4o mini} and 0.14 with \emph{Gemini-2.0-flash-001}. These results highlight the limitations of purely heuristic-based approaches compared to our method, which better captures the more fine-grained aspects like hallucination.

\input{tables/agreement_scores}

\subsection{Errors Under Distributional Shifts}
\input{figures/werd_herd_shift}
\paragraph{Natural Shift.} Given that most ASR models now outperform the human baseline on the LibriSpeech clean test set, we consider LibriSpeech as the source domain. Other domain-specific datasets, such as Primock, SPGISpeech, GLOBE, and AMI, are therefore treated as the target domain. We compute the distribution shift as detailed in Section~\ref{subsec:distribution_shift_and_quantification}. We then measure the change (degradation) using Equation~\ref{eq:2} and \ref{eq:3}.
 

The $\alpha$ is the correlation coefficient between error rate degradation and distribution shift.Figure~\ref{fig:werd_herd_shift} shows that both WER and HER degrade as we move from the source domain to different target domains, with considerable distribution shifts across various \emph{whisper} models. This degradation exhibits a nearly linear positive correlation with the domain shift.


Notably, the HER demonstrates a slightly stronger correlation with the shift compared to WER. This trend is consistent across all models, as illustrated in Appendix~\ref{appsubsec:results} Figure~\ref{fig:werd_herd_with_shift_all}. The \emph{ATCOsim} dataset is an outlier, with artificially high \texttt{WER} due to its numerical content. For example, models generating digits (e.g., "23") instead of spoken forms ("two three") are heavily penalized, inflating \texttt{WER} without accurately reflecting transcription quality.

\paragraph{Synthetic Shift.}
Under synthetic shift, we experiment with two configurations: (a) adversarial perturbations and (b) common perturbations. Our experiments reveal that adversarial attacks cause the most significant degradation in HER, with adversarial datasets showing the highest HER values across all models, exceeding the degradation observed under natural shift baselines (see Table~\ref{tab:all_results}). In contrast, random perturbations (such as white noise, pitch shifts, and time stretching) result in more moderate impacts. Notably, self- and semi-supervised models like~\emph{whisper} and~\emph{seamless} demonstrate consistent vulnerability to structured perturbations. For instance, pitch shifts and time stretching lead to a substantial increase of approximately (242\%) in both WER and HER across these models, while white noise causes smaller degradations of approximately (142\%). Interestingly, the supervised~\emph{wav2vec2} model exhibits non-uniform behavior, where the impact on WER and HER is similar across all perturbations. Furthermore, it is noteworthy that HER increased by 50\% in the~\emph{wav2vec2} model, which is considerably less than the sharp increase observed in~\emph{whisper} and~\emph{seamless} models, highlighting a notable contrast in the robustness of these models to random synthetic shifts against more targeted shifts.

\input{figures/perturb}

\subsection{Impact of Model Architecture and Scale on Error Rates}
\paragraph{Model Type.} We hypothesize that model architecture plays a critical role in influencing error rates, with these effects further modulated by the scale and diversity of the training data. As shown in Appendix Table~\ref{tab:results_ratio}, our findings indicate that encoder-only models \emph{Wav2vec2-large-xlsr-53-english} and \emph{Hubert-large-ls960-ft} exhibit the lowest \texttt{HER/WER} ratios across multiple datasets, particularly when compared to models like \emph{Qwen2-Audio-7B} and \emph{hf-seamless-m4t-large}. This suggests that encoder-only models are less prone to hallucinations relative to their overall errors, likely due to robust training or architectural advantages.

% \aw{We need to make it more concise and presentable.}

\paragraph{Model Size.} In our experiments, we evaluate models of varying sizes where training data is fixed across different sizes (ie: \emph{whisper}). 
\input{figures/werd_herd_whisper}
More specifically, we select \nummodels~ models from the \emph{whisper} family, ranging from the smallest \emph{whisper-tiny} (39M parameters) to the largest \emph{whisper-large-v3} (1.5B parameters). We then calculate both WER and HER for each of these models, following the methodology outlined in Section~\ref{sec:methodology}. Our findings show that for smaller models, such as \emph{whisper-tiny} and \emph{whisper-small}, there is a significant increase in both WER and HER. While larger models such as \emph{whisper-medium} and \emph{whisper-large} show substantial improvements, as shown in Figure~\ref{fig:werd_herd_whisper}. However, this reduction is not linear. After a certain point, performance improvements in both metrics become less pronounced.
This non-monotonic behavior is particularly evident when comparing models in the mid-range of parameter sizes, such as \emph{whisper-medium} and \emph{whisper-large-v3-turbo}, where the difference in performance becomes marginal despite the difference in model size. In conclusion, while larger models generally result in lower WER and HER, the benefits of scaling up model size diminish beyond a certain point, at least when it comes to more nuanced error types. 
