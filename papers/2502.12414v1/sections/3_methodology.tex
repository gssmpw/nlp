\section{Methodology}\label{sec:methodology}

We examine transcription quality using the standard metrics WER and CER, and we assess the occurrence of hallucination errors to provide a comprehensive view of model performance. Our testing environment is characterized by both natural and synthetic distribution shifts. Furthermore, we investigate the deterioration of error rates when transitioning from a controlled source domain (LibriSpeech) to various target domains, with a particular focus on both WER and the Hallucination Error Rate (HER). In addition, we introduce noise to the input data and analyze its effect on error rate degradation. This multifaceted strategy offers valuable insights into the challenges encountered by ASR systems in real-world scenarios. We provide additional details about each step subsequently.

\subsection{ASR Evaluation}
We evaluate a broad range of ASR models under a zero-shot setting, using the default decoding parameters for each model. Standard preprocessing steps are applied prior to calculating the metrics, ensuring consistency in evaluation. 

\subsection{Hallucination Evaluation}\label{subsec:hallucination_evaluation}
In addition to conventional transcription errors, we also assess hallucination errors by using an LLM-based pipeline that classifies the errors produced by the ASR model. Specifically, we use \emph{GPT-4o mini} to compare the ground truth transcription with the model outputs and ask the LLM to categorize them into different error types.

We conduct hallucination evaluation at two levels:

\noindent\textbf{Coarse-grained:} The model categorizes the output into one of three classes: \textit{Hallucination Error}, \textit{Non-Hallucination}, or \textit{No Error}. For this evaluation, we provide two examples per category in the prompt.

\noindent\textbf{Fine-grained:} The model is asked to further refine the categorization by identifying specific error types, such as \textit{Hallucination Error}, \textit{Language Error}, \textit{Oscillation Error}, \textit{Phonetic Error}, and \textit{No Error}. In this case, one example per category is provided in the prompt. 

The prompts used for both coarse-grained and fine-grained evaluations are detailed in Appendix~\ref{appsubsec:prompts} (Figure~\ref{fig:coarsegrained_prompt} and Figure~\ref{fig:finegrained_prompt}, respectively). To quantify hallucination occurrences, we introduce the \textit{HER}, defined as the ratio of hallucination errors to the total number of examples in the data.

\input{figures/speech_text_embeddings}

\subsection{Distribution Shifts and Quantification}\label{subsec:distribution_shift_and_quantification}
We systematically evaluate ASR models under a variety of testing conditions, ranging from naturally occurring domain variations to scenarios involving both adversarial and non-adversarial perturbations. These conditions are designed to induce either natural or synthetic distribution shifts in the input speech.

\noindent\textbf{Natural Shifts.} These shifts arise from inherent variations in data distributions, such as differences in accents, domain-specific content, background noise, and diverse speaking styles~\cite{liu2021towards}.

\noindent\textbf{Synthetic Shifts.} These shifts are artificially induced, encompassing simulated noise, goal-specific perturbations, and adversarial attacks~\cite{fan2022normalization}. We design a comprehensive testing setup to identify the conditions under which ASR models are prone to hallucinate. Additional details about the datasets used in this study are provided in Section~\ref{subsec:datasets} and summarized in Table~\ref{tab:datasets}.
We measure the extent of domain distribution shifts using the high-order metric Central Moment Discrepancy (CMD)~\cite{zellinger2019centralmomentdiscrepancycmd, kashyap2020domain}, which assesses the discrepancy between two distributions. It is calculated as follows:
\begin{multline}
\text{CMD} = \frac{1}{L} \sum_{l=1}^{L} \left\| \mathbb{E}[h_l^s] - \mathbb{E}[h_l^t] \right\|_2^2,
\end{multline}
where \(L\) represents the number of layers, \(h_l^s\) and \(h_l^t\) denote the hidden representations for the source and target domains, respectively, and \(\mathbb{E}[\cdot]\) signifies the expectation.


\subsection{Error Rate Degradation}
Error Rate Degradation quantifies the decline in ASR performance when transitioning from a source domain (LibriSpeech) to a target domain, with degradation measured in two aspects: transcription errors and hallucination errors.

\noindent\textbf{Word Error Rate Degradation (WERD).} WERD is defined as the difference in WER between the target and source domains:
\begin{equation}
\text{WERD} = \text{WER}_{\text{target}} - \text{WER}_{\text{source}},
\label{eq:2}
\end{equation}
where \(\text{WER}_{\text{source}}\) and \(\text{WER}_{\text{target}}\) denote the WER for the source and target domains, respectively.\\

\noindent\textbf{Hallucination Error Rate Degradation (HERD).} HERD captures the increase in hallucination errors when moving from the source to the target domain:
\begin{equation}
\text{HERD} = \text{HER}_{\text{target}} - \text{HER}_{\text{source}},
\label{eq:3}
\end{equation}
where \(\text{HER}_{\text{source}}\) and \(\text{HER}_{\text{target}}\) represent the hallucination error rates in the source and target domains, respectively.

Furthermore, the relationship between CMD and degradation rates is analyzed and visualized (see Figure \ref{fig:werd_herd_with_shift_all}) to understand how domain variations correlate with both transcription and hallucination errors.