\section{Appendix}

\subsection{Experiments}\label{appsubsec:experiments}
\subsubsection{Datasets}\label{appsubsubsec:datasets}
\input{tables/datasets}

\subsubsection{Perturbation}\label{appsubsubsec:perturbation}


To evaluate the robustness of ASR models under varying conditions, we apply the following perturbations to the audio inputs:  
\begin{itemize}  
    \item \emph{White Noise}: Gaussian noise is added at a low amplitude to simulate environmental interference.  
    \item \emph{Time Stretching}: The audio is randomly stretched by a factor between 0.9 and 1.1, altering the speed while preserving pitch.  
    \item \emph{Pitch Shifting}: The pitch is randomly shifted by up to Â±2 semitones to mimic natural variations in speech.  
    \item \emph{None}: No perturbation is applied, serving as the baseline for comparison.  
\end{itemize}  

These perturbations are designed to replicate real-world challenges such as background noise, speaker variability, and recording inconsistencies. ~\input{tables/noise_details}.  

\subsubsection{Models}\label{appsubsubsec:models}
\input{tables/models}

\subsection{Prompts}\label{appsubsec:prompts}
% \input{tables/data}
% \input{figures/prompts}
\input{tables/prompts}


\subsection{Results}\label{appsubsec:results}

We provide a detailed analysis of the experimental results, focusing on the highest and lowest performing models across the study. To offer a comprehensive overview, we present Hallucination Error Rate (HER) and Word Error Rate (WER) across domain shifts for all models, as shown in \input{figures/CMD_all_models}. Additionally, we include fine-grained error analysis which highlights the differences between coarse-grained and fine-grained error categorization.\input{tables/finegrained_her_cer}


We also calculate HER to WER ratio. As robust models would exhibit a smaller gap between hallucination and non-hallucination errors. 
\input{tables/ratio_her_wer}



Furthermore, we present the percentage of non-hallucination errors across datasets and models, categorizing them into Phonetic (P), Oscillation (O), and Language (L) errors. This analysis provides deeper insights into the types of errors that are most frequent and their distribution across different dataset-model combinations.  \input{tables/hallucination_misidentification}
We also highlight the overall distribution across all datasets and the robustness of both levels (coarsegrained and finegrained) in correctly identifying hallucination.
\paragraph{Key Findings:}
\begin{itemize}
    \item The highest and lowest performing models exhibit significant variations in HER and WER under domain shifts, with some models showing robustness while others struggle.
    \item Fine-grained error analysis reveals that certain error types (e.g., Oscillation) are more prevalent in specific dataset-model combinations.
    \item Non-hallucination errors, particularly Phonetic and Language errors, dominate in certain scenarios, providing actionable insights for improving model performance.
\end{itemize}

These results underscore the importance of considering both hallucination and non-hallucination errors when evaluating ASR systems, as well as the need for domain-specific adaptations to enhance robustness.
\input{figures/coarse_vs_finegrained.tex}

\input{tables/example_wer_her}