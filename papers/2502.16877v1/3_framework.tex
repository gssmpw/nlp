\section{\sysname Framework}

\begin{figure}[t]
    \vspace{-0.2in}
    \centering
    \includegraphics[width=1\linewidth]{Figures/apint_framework.pdf}
    \caption{Overall \sysname Framework}
    \vspace{-0.2in}
    \label{fig:APINT_framework}
\end{figure}

In this section, we propose APINT, a full-stack framework designed to accelerate PiT by reducing the overhead of GC, the primary bottleneck of PiT. The overall workflow is illustrated in Figure~\ref{fig:APINT_framework}, distinguishing between compile-time and runtime processes.
In the initial compile time stage, \sysname begins by extracting nonlinear function operations in the process of computing the transformer model through the \sysname protocol. Next, through the GC-friendly circuit generation, the extracted function is implemented as a circuit consisting of a 2-input gate, and it is converted to netlist in Bristol format~\cite{tillich2016circuits}.
This step significantly alleviates the computational load on GC in subsequent stages. Following this, \sysname adopts a scheduling strategy that combines coarse-grained and fine-grained scheduling. The strategy enables full utilization of DRAM bandwidth and decrement in wire dependency. Furthermore, \sysname incorporates compiler speculation to generate instructions that capitalize on wire reusability and are executed on hardware accelerators at runtime. These accelerators, designed to further reduce memory stalls by eliminating redundant DRAM accesses, are deployed on both the server and client, allowing them to perform GC evaluation or GC garbling.
% These accelerators are deployed on both the server and client sides within the framework to further reduce memory stalls by eliminating redundant DRAM accesses.

\subsection{\sysname Protocol}

\begin{figure}[t]
    \vspace{-0.2in}
    \centering
    \includegraphics[width=1\linewidth]{Figures/Camera-ready/Fig4_Protocol.pdf}
    % \includegraphics[width=1\linewidth]{Figures/Protocol_Offload.pdf}
    \caption{\sysname Protocol}
    \vspace{-0.2in}
    \label{fig:APINT_protocol}
\end{figure}

\sysname protocol is based on the PiT protocol in PRIMER, but it reduces the circuit in GC operation by offloading its partial calculations to HE and standard operations, thereby significantly decreasing the workload of GC. As illustrated in Figure~\ref{fig:APINT_protocol}, the basic concept of the protocol is the combination of HE for linear operations and GC for nonlinear operations. To maintain confidentiality, each party adds or subtracts a random matrix ($R_i$ of the client and $S_i$ of the server) before sending the data to each other.
At the offline phase, HE is utilized to compute linear function for the client's random matrix $R_{1}$, which has the same matrix size as the input matrix $X_1$ \circled{1}. Simultaneously, the client garbles the circuit $\Tilde{C_1}$, which integrates adding the secret shares from both parties, processing the nonlinear function, and subtracting a random matrix to ensure confidentiality \circled{2}. The client then transmits labels of $R_2, R_3$ to the server \circled{3}. During the online phase, the server calculates the linear function of $(X_{1}-R_{1})$ using standard matrix operations. This intermediate result is merged with data from process \circled{1} to complete the computation of the linear function, yielding $(X_{2}-R_{2})$ \circled{4}. After that, the labels of $(X_{2}-R_{2})$ are sent from the client via the OT protocol~\cite{ishai2003extending} \circled{5}, and the server proceeds the GC evaluation for the garbled $\Tilde{C_1}$ \circled{6}.




However, in contrast to other functions, the reduced circuit $\Tilde{C_2}$ is employed when operating LayerNorm. This circuit specifically excludes calculations of mean and variance, as well as operations involving the parameters $\beta$ and $\gamma$. The excluded calculations are offloaded and computed using standard operations and HE, thereby reducing the workload of GC.
During the offline phase, the client garbles the circuit $\Tilde{C_2}$, transmitting the labels for $\sum{R_4/N}, R_5, R_6$, and also sends $Enc(R_2')$.
During the online phase, the mean of $(X_{2}-R_{2})$ is initially calculated using standard operations. This mean is then subtracted from $(X_{2}-R_{2})$, resulting in $(X_{2}'-R_{2}')$ \circled{7}. Subsequently, to prepare for variance calculations, this result is multiplied by two times $Enc(R_2')$ \circled{8}. The results from equation \circled{7} and \circled{8} are then used to compute the variance \circled{9}. Third, multiplying with the parameter $\beta$ can be processed by utilizing HE with $(X_{2}'-R_{2}')$, $Enc(R_2')$, and $\beta$ \circled{10}, \circled{11}. Then, the labels of the data from the process \circled{9} and \circled{11}, which are obtained from the client via OT protocol, and the labels from the client are computed through GC evaluation \circled{12}. Finally, a straightforward addition of the parameter $\gamma$ is processed \circled{13}.

Although this protocol incurs additional overhead due to HE and communication of two parties, it brings a substantial reduction of GC latency, offsetting these increased costs perfectly. This reduction marks a significant improvement over the baseline protocol, which merely utilized GC for processing the nonlinear functions. Ultimately, the \sysname protocol achieves a significant reduction in the online latency of GC operations, reducing it by 47.3\% during the LayerNorm computation.

\subsection{GC-friendly Circuit Generation}

To further minimize the workload of GC, \sysname proposes GC-friendly circuit generation of the nonlinear functions. It involves implementing each function to a circuit with 2-input AND, XOR, and INV gates while preserving the accuracy of computations. The process unfolds in two main steps.

The initial step focuses on minimizing the total number of gates in the circuit. Since GC processes the gates of the circuit sequentially, reducing the gate count directly lowers the overall computational load. For instance, in the implementation of Softmax, the method from i-BERT~\cite{kim2021bert} is adopted, which scales inputs by \textit{ln2}, thereby reducing the range of values and the number of required gates for exponential operations. The exponential operations are performed through combinational logic, which performs as a Look-Up Table (LUT) interpolation. For the GeLU function, LUT interpolation is utilized after clipping the input values within a range (-4, 4)~\cite{gupta2023sigma}. In LayerNorm, the conventional approach is employed without any approximation, as it doesn't incur any accuracy drop.

The second step aims to decrease the number of AND gates further. \sysname proposes a method employing XOR-Friendly Binary Quantization (XFBQ)~\cite{jian2020fast} to implement multiplication with fewer AND gates compared to the conventional method. This approach is motivated by the observation that the multiplication process accounts for a significant portion of each nonlinear function's implementation. 
Figure~\ref{fig:circuit_generation} (a) summarizes XFBQ and its multiplication process. XFBQ modifies the binary representation, wherein 1 represents 1 and 0 represents -1, exploiting the correspondence between the result patterns of XOR operations and the product of 1 and -1. The way to XFBQ is straightforward: it involves a right shift and changing the MSB to 1, introducing only a minimal quantization error (Q error) as small as the INV of Least Significant Bit (LSB) of the original number. For instance, \textit{1000}=8 turns into \textit{1100}=8+4-2-1=9 after XFBQ with Q error as \textit{INV(0)=1}.  When expressing conventional multiplication ($A\times B$) as XFBQ multiplication ($\hat{A} \times \hat{B}$) along with additional terms due to Q errors, the AND operations of the conventional method are all replaced by XOR, thereby a high reduction of AND gates occurs. Moreover, given that the Q error is INV of LSB value, its negligible impact on multiplication results and PiT warrants disregarding the additional terms, leading to a further decrease in AND gates.

\begin{figure}[t]
    \vspace{-0.2in}
    \centering
    \includegraphics[width=1\linewidth]{Figures/Camera-ready/xfbq_correct.pdf}
    \caption{(a) Reduction of ANDs via XBFQ Multiplication (b) Comparison of ANDs for 64b Multiplication}
    \vspace{-0.2in}
    \label{fig:circuit_generation}
\end{figure}

Figure~\ref{fig:circuit_generation} (b) shows the effects of the multiplication using XFBQ while operating 64b multiplication. It reduces the number of AND gates 38.9-45.5\% compared to prior work~\cite{liu2022don}, depending on the inclusion of Q error adjustments. In addition, GC-friendly circuit generation employed methods from the work~\cite{testa2020logic} for operations other than multiplication, which has been proven to perform as an open-source. Finally, it reduces the workload of GC for nonlinear functions by an average of 42.5\%.



   
\begin{figure*}[t]
    \vspace{-0.2in}
    \centering
    \includegraphics[width=1\linewidth]{Figures/Camera-ready/Scheduling_Reduced.pdf}
    \caption{(a) Methods and (b) Effects of Coarse-Grained and Fine-Grained Scheduling}
    \vspace{-0.2in}
    \label{fig:scheduling}
\end{figure*}
\subsection{Netlist Scheduling}
% \subsection{Compiler and Hardware Accelerator}
Despite the reductions in GC overhead facilitated by the \sysname protocol and GC-friendly circuit generation, GC still accounts for a notable portion of the latency. This implies that reducing GC overhead necessitates the integration of hardware accelerators beyond software solutions.
However, since a GC accelerator takes a netlist, converted from the circuit, as input and processes gates in the netlist sequentially, efficient netlist scheduling is crucial for hardware acceleration. Therefore, we introduce coarse-grained and fine-grained scheduling, maximizing DRAM bandwidth utilization and minimizing computational dependencies to accelerate the GC operation of nonlinear functions. 

% For instance, HAAC~\cite{mo2023haac} introduced an accelerator showcasing superior performance compared to CPUs and GPUs by utilizing multi-core designs operating concurrently and leveraging off-chip memory when on-chip memory resources are insufficient. However, this approach encounters a significant memory bottleneck when processing nonlinear functions of transformers due to inadequate scheduling schemes and an accelerator design overlooking wire reusability. Consequently, as a final step, \sysname introduces a compiler-integrated accelerator, addressing memory bottleneck by enhancing wire reuse and maximizing DRAM bandwidth utilization through compiler-driven strategies and hardware structure.

\subsubsection{\textbf{Coarse-Grained Scheduling}}

Due to the complex implementation of the circuit of the nonlinear function, the netlist exhibits highly irregular patterns in input and output wires, leading to irregular DRAM accesses that hinder optimal DRAM bandwidth utilization.
To tackle this issue, \sysname adopts coarse-grained scheduling, which maps each independent operation onto each core, allowing cores to function independently yet synchronously. This approach leverages the characteristic of nonlinear functions composed of independent unit operations, such as rows in Softmax, to be computed separately. 
Assuming there are two Processing Engines (PEs) and the need to compute two Softmax rows, the red box of Figure~\ref{fig:scheduling} (a) illustrates a DAG of two rows ordered in a depth-first manner without any scheduling, where node number corresponds to the order of the gates in the netlist. In this case, two PEs concurrently process the netlists of two rows in a dependent manner. In contrast, as illustrated in the green box, each PE exclusively handles an independent row with coarse-grained scheduling. Hence, contrary to the red box in Figure~\ref{fig:scheduling} (b), the green box demonstrates that coarse-grained scheduling allows all PEs to operate synchronously, ensuring they request DRAM data simultaneously. Therefore, while the intra-core DRAM access pattern is irregular, the inter-core DRAM access pattern becomes the same. As a result, by enabling cores to share the DRAM data bus, coarse-grained scheduling ensures maximal utilization of DRAM bandwidth.

Moreover, coarse-grained scheduling offers the additional benefit of resolving wire dependencies. Unlike the scenario without coarse-grained scheduling, the scheduling enables each PE to independently operate on a distinct row without dependencies. Thus, coarse-grained scheduling not only maximizes the utilization of DRAM bandwidth but also reduces the pipeline stalls.
        
\subsubsection{\textbf{Fine-Grained Scheduling}}
In addition to the coarse-grained scheduling, \sysname applies fine-grained scheduling that further diminishes GC latency compared to Full Reorder (FR) and Segment Reorder (SR), the scheduling method by the SOTA GC accelerator, HAAC~\cite{mo2023haac}. The FR transforms the netlist into a DAG and establishes processing order by traversing the graph in a breadth-first manner, reducing wire dependencies. However, due to limited on-chip memory, it can cause off-chip traffic by spilling wires over to DRAM, especially in applications where the DAG has a wide breadth, such as the nonlinear function of transformers. To tackle this problem, HAAC proposed the SR that segments the netlist, ordered in a depth-first manner, to enhance wire reuse and then applies FR within each segment to reduce wire dependencies. 
% \sysname applies fine-grained scheduling in addition to the coarse-grained scheduling. This further diminishes GC latency beyond the scheduling method proposed by HAAC, such as Full Reorder (FR) and Segment Reorder (SR).
% FR transforms the netlist into a DAG, where each node represents a gate. It establishes processing order by traversing the graph in a breadth-first manner, reducing wire dependencies but decreasing wire reuse in limited on-chip memory. Consequently, it can cause a memory bottleneck by spilling data over to DRAM, especially in applications where the DAG has a wide breadth, such as the nonlinear function of transformers. HAAC additionally proposed SR to tackle this problem. As shown in Figure~\ref{fig:scheduling}, SR firstly segments the netlist after depth-first scheduling to enhance wire reuse and applies FR within each segment to reduce dependencies.
% However, FR is not the optimal way to reduce dependencies within each segment. Recent research by S.Zhao~\cite{zhao2020dag, zhao2022dag} has highlighted the efficiency of priority scheduling based on Critical-Path-First-Execution (CPFE) in reducing dependencies of DAG. Therefore, \sysname proposes a fine-grained scheduling by combining SR and CPFE.
Despite these advancements, we identified opportunities for further latency reduction since FR does not optimally eliminate wire dependency within segments. Therefore, \sysname introduces a fine-grained scheduling strategy that combines segmentation and Critical-Path-First-Execution (CPFE)~\cite{zhao2020dag, zhao2022dag} instead of FR, achieving enhanced performance by effectively minimizing wire dependencies.

The fine-grained scheduling begins by segmenting the netlist, with each segment half the size of on-chip memory. A DAG is then constructed for each segment, with assigning weights reflecting the cycle latency of each gate. Nodes without children are linked to \textit{v\_{src}}, and those without parents are connected to \textit{v\_{sink}}. After establishing the DAG, it finds a critical path from \textit{v\_{src}} to \textit{v\_{sink}} and prioritizes nodes along this path, starting from the lowest depth. Subsequently, for each node on the path, a sub-DAG is formed comprising unprioritized descendants, and the process of identifying the critical path and assigning priorities repeats recursively.

The blue box in Figure~\ref{fig:scheduling} (a) shows how the fine-grained scheduling works. First, in step \circled{1}, it finds a critical path and prioritizes from the lowest depth. Then, from step \circled{2}-\circled{6}, it creates sub-DAG with unprioritized descendants for each node of the path and operates recursively. 
% As shown in Figure~\ref{fig:scheduling}, DAG \circled{1} shows finding a critical path and prioritizing from the lowest depth. Then, from DAG \circled{2} to DAG \circled{6}, it creates sub-DAG with unprioritized descendants for each node of the path and operates recursively.
For example, in step \circled{6}, nodes \textit{4} and \textit{6} compose the sub-DAG, and then the process of identifying the critical path and prioritizing is recursively executed at the sub-DAG. After assigning priorities to all nodes, the scheduling order is determined by the cycle-accurate simulation. The simulation selects the operable node with the highest priority in each cycle. The "operable" refers to the condition where both input wires of a DAG node have been produced. As a result, step \circled{6} is reordered as $2\rightarrow1\rightarrow4\rightarrow5\rightarrow6\rightarrow3\rightarrow8\rightarrow7$. 
Hence, as depicted in Figure~\ref{fig:scheduling}, fine-grained scheduling significantly reduces pipeline stalls by wire dependencies within each segment, enhancing the computation speed of nonlinear functions by an average of 30.2\% compared to the SR of HAAC.

\begin{figure*}[t]
    \vspace{-0.2in}
    \centering
    \includegraphics[width=1\linewidth]{Figures/Camera-ready/Hardware_Reduced.pdf}
    \caption{APINT hardware, Compiler Speculation Flow, and Runtime Flow Descriptions}
    \vspace{-0.2in}
    \label{fig:compiler_and_hardware}
\end{figure*}
\subsection{Accelerator with Compiler Speculation}
% \subsubsection{\textbf{Compiler Speculation and Hardware Accelerator}}
HAAC introduced an accelerator showcasing superior performance compared to CPUs and GPUs by utilizing pipelined multi-core designs operating concurrently and leveraging off-chip memory when on-chip memory resources are insufficient. However, it encounters a significant memory bottleneck when processing nonlinear functions of transformers due to inadequate on-chip memory policy and hardware structure.
% A further cause of the memory bottleneck is the on-chip memory policy and hardware structure of HAAC.
HAAC's approach of sequentially writing output wires in on-chip memory doesn't consider the wire reusability. Moreover, the hardware structure that involves directly fetching wires from DRAM to a PE via a queue structure limits the wire's usage to a single time, thereby restricting its potential for reuse. To counter these issues, \sysname suggests the accelerator alongside compiler speculation techniques, aiming to reduce unnecessary DRAM accesses and improve wire reuse.

\subsubsection{\textbf{APINT Accelerator}}
% \textit{\textbf{APINT Accelerator} \ }
Figure~\ref{fig:compiler_and_hardware} illustrates the architecture of \sysname accelerator, which features 16 independent cores operating synchronously under coarse-grained scheduling. This eliminates the need for inter-core communication, allowing for a shared unified Instruction Memory (16KB). Each core includes a Wire Memory (128KB), a Table Memory (2KB), an Out-of-Range-Wire (OoRW) Prefetch Buffer (1KB), and a PE, all of which are pipelined.
Wire Memory stores the wire's label (value of the wire) and special flag bits, which are a block bit and an Out-of-Range (OoR) bit, for each address. The block bit prevents other wires from accessing the address, while the OoR bit indicates that an OoRW, a wire that is fetched from DRAM, is being fetched to that address. Also, the Table Memory stores garble tables required for Half-Gate operations, and the OoRW Prefetch Buffer temporarily stores OoRWs fetched from DRAM. They are then transferred to the Wire Memory, which allows multiple reuses within the Wire Memory in contrast to HAAC, where OoRWs are used only once per fetch.

The execution of the accelerator is structured into four stages. First, upon receiving an instruction, the Write Address Preemption stage the write address in the Wire Memory, activating the block bit. Next, the Read stage reads two input wires from the memory or forwarding path over three cycles. Third, the input wires are processed in the Half-gate unit (taking 18 cycles for evaluation and 21 for garbling) or FreeXOR unit (taking one cycle) in PE, and OoRWs are transferred from the Prefetch Buffer to Wire Memory if required. Finally, the output wire generated in the PE is written back to Wire Memory over two cycles and, if needed, also to DRAM.

% The execution of the \sysname accelerator is structured into four stages: Write Address Preemption, Read, OoRW Transfer and PE Execution, and Write. Upon receiving an instruction, the accelerator preempts the write address in the Wire Memory, setting the block bit alive. It then reads input wires over three cycles and processes them in the PE, which includes a Half-Gate unit (taking 18 cycles for evaluation and 21 for garbling) and a FreeXOR unit (taking one cycle). Garble tables are transferred from the Table Memory if Half-Gate is operated. Simultaneously, OoRWs are transferred from the Prefetch Buffer to the Wire Memory if required. Finally, after the PE execution, output wires are written back to Wire Memory over two cycles and, if needed, also to DRAM.

\subsubsection{\textbf{Compiler Speculation Flow}}
Before running the accelerator, compiler speculation is initially processed with a netlist as input. Its purpose is to generate instructions for the accelerator, which implements a memory policy that enhances wire reuse. It proceeds through the following two phases, as depicted in Figure~\ref{fig:compiler_and_hardware}. During the first phase, it assigns read and write addresses in Wire Memory and an OP bit for each gate in the netlist through a cycle-accurate simulation. After filling Wire Memory as much as possible with operable input wires, the speculation begins with the Write Address Preemption stage, allocating a write address either to a blank space or to the Last-to-Be-Used Wire (LBUW), if Wire Memory is full. The LBUW is the wire that will be used last among wires within the memory. This demonstrates that APINT employs a memory policy considering the reusability of wires. After assigning the write address, the block bit is activated for the preemption.

% Compiler speculation begins by using a netlist as input, producing instructions generated to enhance wire reuse. This process proceeds through the following two phases after filling Wire Memory as much as possible with operable input wires. During the first phase, it assigns read and write addresses in Wire Memory and an OP bit for each gate through a cycle-accurate simulation. First, during the Write Address Preemption stage, if the Wire Memory is not full, the write address is assigned to a blank space, if not, to the address of the Last-to-Be-Used Wire (LBUW), designated to be used last among wires in Wire Memory, and a block bit for this address is activated. Therefore, this process enables the on-chip memory policy to consider the reusability of wires.

Next, the Read stage assigns the read address based on whether an input wire is present in Wire Memory. If present, the read address corresponds to its location. If not, indicating it is likely to become an OoRW at runtime, the address of the LBUW with inactive block bit is assigned, which also contributes to the memory policy that considers the reusability of wires. The input wire is then replaced with the LBUW and added to the OoRW list. Subsequently, the PE execution stage begins by setting the OP bit based on the gate type. This is followed by the Write stage, which writes the output wire at the assigned address and resets the block bit. This cycle-accurate simulation is repeated until every wire and gate in the netlist has been allocated the instructions with addresses and OP bits.

After completing the first phase, the second phase involves assigning the Live bit, two OoRW-fetch bits, and the Write Enable Not (WEN) bit, which are determined by analyzing the interrelationships among instructions. The Live bit is assigned to instructions that output an OoRW, designating that the wire should be written to DRAM for later use. For example, in Figure~\ref{fig:compiler_and_hardware}, instruction \circled{1} outputs OoRW \textit{30} and is marked with a Live bit of 1.
Each OoRW-fetch bit is assigned to ensure the timely transfer of an OoRW from the Prefetch Buffer to Wire Memory, based on instruction sequence and read dependencies.
For instance, instruction \circled{2}, which reads address \textit{0} immediately before instruction \circled{4} reads OoRW \textit{30} from the same address, is assigned an OoRW-fetch bit to ensure that OoRW \textit{30} is transferred right after instruction \circled{2} reads the address \textit{0} to prevent stalls due to non-arrival.
The WEN bit is assigned to prevent premature overwriting in Wire Memory.
For example, if OoRW \textit{30} is transferred to memory before instruction \circled{3} writes to address \textit{0}, it could be overwritten before it is read by instruction \circled{4}. Therefore, a WEN bit is assigned to instruction \circled{3} to prevent it from overwriting OoRW \textit{30}, and wire \textit{35} is written only to DRAM as dictated by the Live bit.
% For example, to avoid overwriting OoRW \textit{30} needed by instruction \circled{4}, instruction \circled{3} receives a WEN bit of 1.


% During the speculation process, the sequence for using instructions and garbled tables is predetermined and mapped sequentially in DRAM. Similarly, the DRAM read and write orders for OoRWs are predecided, with DRAM write addresses being assigned incrementally. Hence, if an OoRW's DRAM read address exceeds the current increment value, indicating the wire has yet to be written to DRAM, the accelerator stalls until the increment value matches the address. This approach removes the need to handle DRAM addresses during runtime, enabling the accelerator to fetch instructions, garbled tables, and OoRWs from DRAM to on-chip in a predetermined order while executing the instructions.

\subsubsection{\textbf{Runtime Flow}}

During the speculation process, the DRAM addresses for instructions, garbled tables, and OoRWs are predetermined, removing the need to handle the addresses during runtime. While fetching the data from DRAM to each corresponding memory, the runtime process is executed in the following four stages, as depicted in Figure~\ref{fig:compiler_and_hardware}.
% During runtime, the four stages are executed with the \sysname accelerator, as depicted in Figure~\ref{fig:compiler_and_hardware}. 
After an instruction is decoded, the Write Address Preemption stage activates the block bit at the write address, and the Read stage operates based on the statuses of the block and OoR bits. Depending on these bits, the accelerator either performs a normal read or stall until the necessary wire is transferred from the Prefetch Buffer or the forwarding path. The OoRW Transfer and PE Execution stage then commences. The OoR bit is assigned with the OoRW-fetch bit, indicating whether an OoRW transfer is started. If the OoRW-fetch bit is 1, the address is preempted by activating the block bit, and an OoRW begins to be transferred to the address. After the completion of the transfer, the block bit is deactivated. Concurrently, the PE processes Half-gate or FreeXOR operation based on the OP bit. After the output wire is generated in the PE, the Write stage begins, writing the wire to Wire Memory and DRAM depending on the WEN and Live bits. After these stages are executed across all instructions, the runtime process is completed. Overall, through APINT accelerator and compiler speculation, it achieves a reduction in memory stall times by 86.1\% to 99.4\% compared to HAAC when operating nonlinear functions.

% After an instruction is decoded, the Write Address Preemption stage sets the block bit at the write address, and the Read stage proceeds based on the block and OoR bit statuses. If the block bit is 0, a normal read is performed. Otherwise, the OoR bit determines if an OoRW or non-OoRW wire has preempted the address. An OoR bit of 1 indicates a scheduled OoRW write at that address, causing the accelerator to stall until the OoRW is transferred from the Prefetch Buffer to Wire Memory. If the OoR bit is 0, the accelerator is stalled until the wire is retrieved from the forwarding path.

% Once the Read stage is completed, the OoRW-fetch and PE execution stage begins. The OoR bit is assigned with the OoRW-fetch bit, preempting the address for an OoRW transfer. If the OoRW-fetch bit is 1, the block bit is activated, and the OoRW begins to be transferred from the Prefetch Buffer to the address. The block bit is deactivated after the transfer. Concurrently, the PE processes operations based on the OP bit, and the garbled table is transferred from the Table Memory for Half-Gate operation.

% Lastly, Write stage begins. Unless the WEN bit is active, the generated output wires are written to Wire Memory, and the block bit of the write address is deactivated. If the Live bit is active, the wires are also written to DRAM. After these stages are executed across all instructions, the runtime process is completed. Overall, through compiler speculation and the accelerator, APINT achieves a reduction in memory stall times by 74.3\% to 99.9\% compared to HAAC for nonlinear functions of transformers.
