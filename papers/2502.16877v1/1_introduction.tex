\section{Introduction}

Transformer models have significantly advanced the fields of natural language processing (NLP), setting new performance benchmarks and becoming dominant in the area. This advancement has fueled the popularity of cloud services offering pre-trained Transformer models for NLP tasks, enabling clients to use these models without needing to own them. However, this convenience raises significant privacy concerns as it potentially exposes sensitive client data, like financial and health information, to cloud providers. Therefore, there is an urgent need for a new method that can prevent sensitive client data from being exposed to the server.

To address this issue, post-quantum cryptographic protocols such as Homomorphic Encryption (HE)~\cite{gentry2009fully, brakerski2014leveled} and Garbled Circuits (GC)~\cite{bellare2013efficient} are widely adopted for secure computation. HE enables calculations on encrypted data without decryption, preserving data confidentiality, but is limited to additions and multiplications, which hinders performance with nonlinear functions. In contrast, GC supports any operation expressible through the circuit composed of 2-input gates, allowing for more versatile applications despite its higher computational and memory demands.

Recent studies~\cite{juvekar2018gazelle, mishra2020delphi, garimella2023characterizing, zheng2023primer} have proposed a hybrid approach that combines the strengths of both protocols: using HE for linear functions and GC for nonlinear functions. This approach has been well-received because it preserves the accuracy of plaintext computations perfectly, however, it still faces significant latency issues.
% To address this, DELPHI~\cite{mishra2020delphi} split the protocol into an offline phase that can be preprocessed irrespective of input values and an online phase that varies based on the input values, thereby substantially reducing the online costs. 
To address this, DELPHI~\cite{mishra2020delphi} splits the protocol into an offline phase for preprocessing and an online phase dependent on input values, significantly reducing online costs.
PRIMER~\cite{zheng2023primer} further enhanced it for Privacy-preserving inference of Transformer models (PiT), which focused on reducing the latency of HE but didn't address those of GC. Consequently, GC has emerged as the primary bottleneck, leaving its reduction as an unresolved challenge.

A primary strategy for effectively reducing latency in GC involves minimizing workload, particularly by reducing the number of AND gates within the circuit. Previous studies~\cite{testa2019reducing, testa2020logic, liu2022don} proposed methods to minimize the number of AND gates by transforming the circuit into a Directed Acyclic Graph (DAG), where each node represents a gate, and then analyzing this graph. However, they overlooked the fundamental approach of altering the structure of the circuit itself before transforming it into a DAG.

Another efficient method to reduce the latency of GC is utilizing a hardware accelerator. Although prior works~\cite{hussain2019fase,mo2023haac} have accelerated GC operations, the challenges remain while operating the nonlinear functions of transformers. FASE~\cite{hussain2019fase} assumes that all necessary data can be contained on-chip, making it impractical to compute with a vast amount of data. HAAC~\cite{mo2023haac} proposed a pipelined accelerator approach by suggesting the use of off-chip memory. Nevertheless, its scheduling scheme and on-chip memory policy are not optimal, resulting in significant memory stalls and pipeline stalls.

To summarize, previous studies have addressed protocols for PiT, reducing the workload of GC, and employing accelerators for GC, which is the primary bottleneck of the protocol. However, these approaches have only investigated parts of the issue and have not provided a holistic examination. Moreover, each of these schemes has failed to optimally reduce the overhead of GC when performing PiT. Hence, this paper introduces \sysname: a full-stack framework designed to accelerate PiT from the software level down to the hardware. \sysname makes the following contributions.
\begin{itemize}[topsep=5pt]
    \item A novel protocol that reduces GC latency by distributing workloads of the GC to alternative operations
    \item A GC-friendly circuit generation that significantly reduces the number of AND gates, thereby reducing both the GC latency of the online phase and the offline phase
    \item A new scheduling method that is composed of coarse-grained and fine-grained scheduling to minimize data dependency and increase data reuse
    \item A hardware accelerator with compiler speculation to improve data reusability and reduce redundant DRAM accesses
    %\item As a result, \sysname successfully reduces PiT latency ~ XX\%.
\end{itemize}
Overall, \sysname achieves a significant reduction in both offline and online latency by 2.2$\times$ and 12.2$\times$, respectively. Meanwhile, the APINT accelerator reduces its latency by 3.3$\times$ as well as lowers system energy consumption by 4.6$\times$ while operating PiT, compared to the state-of-the-art (SOTA) GC accelerator.