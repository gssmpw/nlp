\section{RELATED WORK}
Our work builds on prior research that defines landmarks for sighted and blind users, develops navigation assistance systems, and creates landmark enhancement AR technologies. We introduce them below to contextualize our work.
% \subsection{Current methods to facilitate cognitive map formation for people with low vision / blind}
% % not in ar
% % in ar

% \subsection{Current methods to facilitate navigation for people with low vision / blind}
% % not in ar
% % in ar
% turn by turn 
% landmark based
% navigation system in open areas (more recent research, like airport)
% Landmarks are important elements in the environments \cite{lynch1964image} 
% \subsection{Landmark-based system in augmented reality} %In summary, landmarks have a wide range of applications in navigation.
% landmarks established for sighted people are not sufficient for visually impaired people.  The authors invited 3 visually impaired participants and 3 sighted participants, but did not report more detailed differences in visual landmark choices. They also found that visually impaired people tend to describe landmarks from a subjective egocentric point of view, in contrast to the objective static physical fixtures. 

\subsection{Landmark Definition, Categories, and Usage}\label{Landmark Definition, Categories, and Usage}
Prior research has explored landmark definitions and usage for sighted people. Landmarks are defined as stationary, distinct, and salient objects or places in the space that are more likely to be selected as spatial references \cite{lynch1964image,millonig2007developing}. Landmarks serve multiple purposes throughout different stages of wayfinding \cite{yesiltepe2021landmarks}. Previous work found that people use landmarks to orient themselves \cite{downs2011cognitive,philbeck2005remembered}, locate destinations \cite{klippel2005structural}, guide others \cite{raubal2002enriching,tom2003referring, lovelace1999elements}, and construct mental representations of unfamiliar environments \cite{michon2001and}.

Prior work summarized the main criteria for landmarks for sighted people as permanence, uniqueness, and identifiability \cite{yesiltepe2021landmarks}. From the aspect of landmark saliency, Sorrows and Hirtle \cite{sorrows1999nature} classified landmarks into three categories: visual, structural, and cognitive landmarks. Visual landmarks are objects with salient visual features (i.e., shape, color,  size), cognitive landmarks are those with important meanings (i.e., culturally, historically, or personally important), and structural landmarks are the ones that take a vital role or location in the structure of the space. From the aspect of visibility, landmarks can be categorized into two types: global and local landmarks \cite{steck2000role,lynch1964image,yesiltepe2021landmarks}. 
Global landmarks are visible from many angles and distances \cite{lynch1964image}, while local landmarks can only be seen from up close \cite{steck2000role}.



Some research has investigated how blind users define and categorize landmarks. Different from sighted people, blind people perceived landmarks across multiple sensory channels, including tactile, auditory, smell, and sense of mass \cite{jeamwatthanachai2019indoor, wang2023understanding, tsuji12005landmarks}. For example, Tsuji et al. \cite{tsuji12005landmarks} found that sounds and smells were noticed more by blind people than by sighted people. 
Saha et al. \cite{saha2019closing} divided landmarks into five categories for blind people: structural, sound, tactile, air, and smell, with structural landmarks being shared by both blind and sighted people. Cognitive landmarks are barely used by blind people since it is difficult for them to detect and recognize semantic features \cite{sato2019navcog3}. Wang et al. \cite{wang2023understanding} further summarized three criteria of landmarks for blind people: permanent, reliable, and identifiable, highlighting that landmarks used by blind people need to be fixed and easy to find via tactile feedback. % and they frequently used tactile feedback to detect landmarks. 


Despite the thorough research for sighted and blind people, little attention has been drawn to PLV. Recently, a couple of recent projects have started involving some low vision participants and identified preliminary findings \cite{wang2023understanding,tsuji12005landmarks}. For example, Tsuji et al. \cite{tsuji12005landmarks} found that PLV often paid attention to visual objects that sighted people rarely notice, such as the texture underfoot. However, they investigated blind and low vision users altogether, overlooking the unique needs of PLV, whose visual abilities fall between sighted and blind people. No research has deeply explored the landmark selection for PLV and their preferences for landmark augmentations. Our research will thus fill this gap to thoroughly understand PLV's landmark perceptions. 



% However, the authors only provided examples and did not categorize the differences in visual landmark choices between low vision and sighted users. 


% Although some existing works \cite{wang2023understanding,tsuji12005landmarks} involved low vision participants in their interviews about how low vision and blind people identify landmarks, they did not report PLV and blind people separately. Unlike blind people, PLV rely on their remaining functional vision heavily everyday. There is a research gap in how PLV perceive and leverage landmarks in their wayfinding and mental model construction.


% How blind people utilize landmarks are similar to sighted people. 

\subsection{Navigation Technology for Blind and Low Vision People}

Many navigation systems have been designed to support blind and low vision people in wayfinding. Most of them focused on turn-by-turn instructions for blind users via auditory \cite{gaunet2006verbal, helal2001drishti, ahmetovic2016navcog, simoes2016blind} and haptic feedback \cite{ertan1998wearable, azenkot2011smartphone, flores2015vibrotactile, liu2021tactile}. For example, NavCog \cite{ahmetovic2016navcog} provided audio turn-by-turn instructions via smartphones and utilized real-time localization to inform blind users about nearby points of interest and accessibility issues. Liu et al. \cite{liu2021tactile} developed Tactile Compass, a hand-held device that generated directional tactile feedback with a rotatable needle pointing towards the planned direction. Some haptic feedback systems also incorporated robotic assistants \cite{lacey1998application, kulyukin2005robocart, guerreiro2019cabot} to guide blind users in navigation.

Some turn-by-turn instruction systems are designed specifically for PLV by providing visual guidance \cite{chi2022enabling, zhao2020effectiveness,yang2021lightguide}. For example, Stern et al. \cite{stent2010iwalk} presented iWalk, a speech-enabled local search and navigation smartphone application for PLV, providing real-time turn-by-turn walking directions in speech and high-contrast text. Zhao et al. \cite{zhao2020effectiveness} explored the design of wayfinding guidance in AR for PLV, comparing the visual and audio feedback. They found that PLV could better perceive length information with lower cognitive load and make fewer mistakes using visual feedback compared to audio feedback. Moreover, %Chi et al.  built an AR application that provided both visual and audio instructions for PVL but have not conducted user studies.
Yang et al \cite{yang2021lightguide} developed LightGuide, which indicated the direction of travel via the position of a light within usersâ€™ visual field for people with light perceptions. 

In addition to turn-by-turn instructions, several systems have been developed to support safe navigation by detecting obstacles for PLV \cite{bai2017smart, zhao2019designing, da2021wearable, fox2023using}. For example, Bai et al. \cite{bai2017smart} detected obstacles along the way and explored the feasibility of different feedback to inform walkable directions, such as visual indicators (e.g., position of a virtual circle) and speech alerts of obstacles. Zhao et al. \cite{zhao2019designing} supported stair navigation for PLV by highlighting stair edges and railings via projection-based and smartglass-based AR. Lately, Fox et al. \cite{fox2023using} also explored the effectiveness of different AR cues to augment obstacles for PLV, finding that 3D world-locked AR cues were superior to directional heads-up cues.

Beyond wayfinding and safe navigation, it is also important for PLV to explore the environment actively \cite{chrastil2015active} and build mental models with landmark knowledge \cite{siegel1975development,kim2021acquisition}. However, to our knowledge, no research has investigated how to enable PLV to better locate and recognize landmarks in navigation.

\subsection{Landmark Enhancements in Augmented Reality}

Researchers have developed landmark enhancement systems in AR for sighted people. Zhang et al. \cite{zhang2021enhancing} found that the AR navigation system with landmarks enhanced users' mental map development and improved wayfinding performance. Liu et al. \cite{liu2021spatial} used AR icons to label semantic landmarks, showing that virtual semantic landmarks can promote the acquisition of spatial knowledge. Additionally, Zhu et al. \cite{zhu2022personalized} developed a system that adaptively selected and augmented landmarks based on landmark saliency and users' familiarity with the environments.
Uniquely, Qiu et al. \cite{qiu2023navmarkar} investigated how to design landmark-based navigation systems tailored for older adults, including a 3D map of the floor plan, turn-by-turn instructions that indicated the next landmark, and an interactive board attached to each landmark to enable users to acquire more information.%They designed interactive landmark information boards at their physical locations,  Their designs focused on integrating additional landmark information with turn-by-turn instructions, but might result in too much text to read when applied to PLV.  \yuhang{no...i mean suggesting what? if they design for older adults, did you use any new designs?}. 

For blind people, existing landmark enhancement systems have focused on providing auditory feedback \cite{balata2016automatically, balata2018landmark, may2020spotlights, fiannaca2014headlock, coroama2003chatty}. For example, Balata et al. \cite{balata2016automatically, balata2018landmark} designed a system that generates landmark-enhanced navigation instructions via audio feedback, revealing that users preferred the landmark-enhanced instructions than conventional turn-by-turn instructions. May et al. \cite{may2020spotlights} developed an auditory environment in mixed reality to simulate a virtual space, presenting landmarks through spatial audio to facilitate mental map formation.

For PLV, while prior research has demonstrated the potential of AR head-mounted displays as a powerful accessibility support \cite{zhao2017understanding, min2021augmented}, no AR augmentations have been designed to support them in landmark perception. The most relevant research is by Huang et al. \cite{huang2019augmented}, who designed an AR system that recognized and augmented text with high-contrast letters and text-to-speech to assist PLV with sign reading. However, they only focused on sign augmentation without considering other types of landmarks. In contrast to prior research, we deeply understand and categorize landmarks for PLV and design VisiMark that augments various landmarks via both AR previews and in-situ labels.

% Although no existing work focuses specifically on augmenting landmarks for PLV, many AR-based assistive systems have been designed to help PLV better perceive their environment \cite{jones2006mobility,yitzhaky2010performance, hicks2013depth, van2015improving, ikeda2015development, kinateder2018using, angelopoulos2019enhanced, du2021automated}. %For example, Jones and Troscianko \cite{jones2006mobility} developed the \textit{Bristol Mobility Aid}, a head-mounted system that assists PLV in recognizing static objects by segmenting and recoloring different objects in natural scenes. 
%Angelopoulos et al. \cite{angelopoulos2019enhanced} introduced a depth-to-high-contrast color mapping overlay to enhance depth perception in existing scenes. Du and Bulusu \cite{du2021automated} proposed an automated AR-based annotation tool for detecting and labeling salient objects in indoor navigation. %In addition to environmental recognition tools, several AR applications have been created to enhance text content for PLV. %Stearns et al. \cite{stearns2018design} evaluated the design of magnification in AR to make texts and fine details more accessible for PLV. 


 
% Navigation requires a range of sensory, cognitive, and motor functions \cite{bock2024structure}. While aforementioned tools help enhance PLV's perception of visual cues, they do not directly contribute to the ability to build mental representations of the environment or plan routes. There is a research gap in specifically focusing on augmenting landmarks for PLV.




% Rheede et al. \cite{van2015improving} presented the environment in a distance-based way, which improved obstacle avoidance. Bai et al. \cite{bai2017smart} detected obstacles along the way and used visual enhancement techniques to integrate navigational cues and improve awareness of traversable directions. 




% Map-based applications like Google Maps \cite{googlemaps} and Apple Maps \cite{applemaps} have been widely used to provide turn-by-turn instructions. Besides 2D map-based guidance, many navigation systems use the AR approach, while previous study has shown that AR navigation systems lead to less mental load compared to 2D displays \cite{mckendrick2016into}. For example, Google Maps \cite{googlearvr} allowed Live View to see the way forward with virtual arrows overlaid on the real world. In addition to turn-by-turn instructions \cite{feiner1997touring, hollerer1999exploring, narzt2006augmented, kim2008vision, al2019mrsive, huang2020arbin}, most landmark-based systems incorporate AR as well \cite{liu2021spatial, zhu2022personalized, qiu2023navmarkar}. Zhang et al. \cite{zhang2021enhancing} found that the AR navigation system that includes landmarks enhanced cognitive map development and improved wayfinding performance. Liu et al. \cite{liu2021spatial} used iconic holograms to show the semantic landmarks, showing that virtual semantic landmarks can assist the acquisition of spatial knowledge. Previous studies also investigated the landmark adaptive visualization method for sighted pedestrians \cite{zhu2022personalized} and how to design landmark-based navigation systems for older adults \cite{qiu2023navmarkar}. 



% As mentioned previously, it is important for PLV to explore the environment actively and build mental models with landmark knowledge. However, to our knowledge, no research has investigated the visual design of landmark augmentations in AR for PLV, considering the needs of PLV who have residual vision but different visual conditions compared to sighted people.


% Fiannaca et al. \cite{fiannaca2014headlock} built a head-mounted navigation aid to help blind users traverse large open spaces by letting them lock onto a salient landmark across the space. They enabled users to remotely identify and select landmarks, and then provided audio feedback to guide the user towards the landmark. 