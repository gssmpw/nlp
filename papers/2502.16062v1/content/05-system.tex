\section{The \sysname\ System}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{figure/interface_shrink-compress.pdf}
  \caption{The user interface of \sysname\ showcases an example of generated results for ``global warming''. The interface consists of four distinct modules: the expression input module (a), the prompt exploration module (b, c, d), the visual blend exploration module (e), and the similarity visualization module (f).
  }
  % \Description{The system design.}
     \label{fig:interface}
\end{figure*}



This section introduces \sysname, an ideation support system designed to enhance designers' creativity by identifying and combining relevant objects and attributes for visual blending. Grounded in insights from prior research and the formative study, \sysname{} follows a multi-stage pipeline (Figure \ref{fig:pipeline}).
The system begins by semantically analyzing user input, using LLMs and external knowledge bases to map abstract concepts to concrete objects and attributes. 
To aid in selecting complementary elements, it provides visualized similarity and sentiment scores, facilitating informed decision-making during the ideation process.
By leveraging LLMs, \sysname{} generates detailed descriptions of potential visual blends based on user-selected inputs, blending schemes, and considerations. 
These descriptions are crafted into complete solutions, comprising prompts for the DALL·E 3 model, to enable rapid prototyping of diverse visual blend concepts. 
To support iterative creativity, users can save and revisit their outputs for refinement and comparison.
The following subsections detail the design, technical implementation, and user experience of \sysname.



\subsection{Design Goals}



Our formative study revealed key design requirements for visual blends, emphasizing the recurring challenges designers encounter in transforming abstract concepts into visual representations through the use of metaphors.
In response, we develop the \sysname\ system to support the design ideation of visual blends for a broad spectrum of abstract expressions.
The system empowers designers to enhance their creativity when searching concrete imagery and enables the design to convey the underlying meaning of the expression. 
In light of the design requirements, the design goals of \sysname\ are as follows:

\begin{enumerate}[leftmargin=*, label=\textbf{G\arabic*}]
    \item \textbf{Inferring potential objects with metaphors}. Blended imagery necessitates associated objects to symbolize abstract concepts (\textbf{R1}), with the shared attributes of these objects providing a common ground for guiding the direction of the blending process (\textbf{R1, R2}).
    \item \textbf{Exploring similarity-based exemplars within diverse options}. The visual harmony of blended images is ensured by maintaining similarities between the objects and their attributes (\textbf{R2}). The system should support the exploration of diverse object combinations and their shared attributes to inspire creative representation (\textbf{R4}).
    \item \textbf{Offering sample designs as inspiration for creative ideation}. The system should provide sample designs based on designer-selected objects and their identified commonalities, offering valuable and timely inspiration for designers (\textbf{R3}).
    \item \textbf{Iterating the exploration on potential design choices}. The system should enable designers to easily track, compare, and iterate on blended results, encouraging creative ideation with a wide range of design possibilities (\textbf{R4}).
\end{enumerate}




\subsection{System Design}


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.7\linewidth]{figure/prompt_template.pdf}
  \caption{The scheme generation prompt is structured into five key modules: (a) system setup, (b) task definition, (c) user input, (d) task execution process, and (e) results format demonstration. The (d) task execution process module outlines the methods and rationales for considering potential blending schemes. The results, processed by the (e) results format demonstration module, are returned in a standardized JSON format, ensuring compatibility with downstream processes. 
  % \zz{The structure of the Scheme Generation prompt is composed of five distinct modules: System setup (a), Task definition (b), User input (c), Task execution process (d), and Results format demonstration (e).
  % The Task execution process module (d) focuses on methods and reasons for considering potential blending schemes, with the result processed by the Results format demonstration module (e) and returned as a valid JSON object.}
  }
  % The interface consists of four distinct modules: the expression input module (a), the prompt exploration module (b, c, d), the visual blend exploration module (e), and the similarity visualization module (f).
  % \Description{The prompt structure.}
     \label{fig:prompt-template}
\end{figure*}




The \sysname\ system incorporates visualizations to assist users in exploring the generated visual blends.
Figure \ref{fig:interface} illustrates the user interface of \sysname, which processes the user input in the following manner.
The system initially decomposes the provided expression into its constituent parts of speech. Users are then prompted to select the concepts they are interested in for further exploration. Based on the selected results, the system identifies related objects that can be metaphorically linked to the chosen concepts and extracts their physical attributes (\textbf{G1}).
To facilitate the exploration of object and attribute combinations, \sysname\ employs Sankey diagrams to visualize semantic and similarity relationships.
The diagram's node-link representation, coupled with color-coded edges, enhances data clarity and readability, empowering users to explore diverse combinations and foster creative ideation (\textbf{G2}).
Additionally, the system generates tailored prompts based on the user-selected objects and attributes, providing guidance for the blending process. Other relevant factors, such as the intended expression, chosen metaphors, and applicable design restrictions, are also incorporated into these prompts, which serve as input for T2I models (\textbf{G3}).
Finally, the system generates and displays the blended results on a 2D canvas, enabling users to explore, compare, and identify the preferred ideation outcomes (\textbf{G4}).



\subsection{Implementation}



\subsubsection{Identifying Metaphorical Objects and Their Attributes}



To achieve our design goals, we initially tokenize the user input into adjectives, nouns, and verbs, allowing users to select the appropriate concepts from them (Figure~\ref{fig:interface}a).
The entire user input will later provide the model with a comprehensive context, allowing it to understand the intended concept better and identify any underlying metaphors.
Next, we integrate ConceptNet~\cite{10.5555/3298023.3298212}, an external knowledge base, with GPT-3.5-turbo~\cite{NEURIPS2020_1457c0d6} to assist users in discovering objects connected to the given concepts.
Leveraging CMT, we embed the metaphorical expression template, ``\texttt{\{Concept\} is like [a/an] \{Object\}}'', into the prompts to identify related objects (\textbf{G1}).
Specifically, ConceptNet is leveraged to extract the top 50 semantically related objects for the target concept, which are then incorporated into the prompt to guide GPT in selecting the most appropriate ones.
For each object, ConceptNet is further utilized to identify associated attributes, which are integrated into the GPT prompts as contextual references, enhancing its capacity to suggest relevant attributes.
During each query iteration, the system presents five objects to the user, enabling iterative refinement and updates until a suitable selection is achieved.
% allowing them to update the results until a suitable one is found 
Subsequently, users can review the rationale behind the metaphorical connections between each object and its associated concept, along with the top five attributes identified for each concept-object pair (Figure~\ref{fig:interface}b\&c).
% Users also have the flexibility to modify the attributes list according to their preferences.
As shown in Figure~\ref{fig:teaser} (left), if the user selects ``vitamins'' as a concept, the system suggests related objects like ``orange'', ``medicine'', and ``egg''.
Upon choosing ``orange'' as the object, GPT suggests that the metaphorical connection between ``vitamins'' and ``orange'' arises from the perspective that ``it contains vitamin C, which is essential for health and well-being''. 
Additionally, the system identifies related attributes of ``orange'', such as ``round'', ``orange color'', and ``juicy''.



\subsubsection{Calculation of Similarity and Sentiment Scores}




To facilitate the exploration of various blending options, we incorporate similarity and sentiment scores. 
These scores help users identify and compare similarities among different objects or attributes and understand the overall harmony of blended results.
We leverage a pre-trained CLIP model~\cite{pmlr-v139-radford21a}, which encodes both image and text data into a shared embedding space, thereby capturing the semantic relationship between textual and visual modalities. 
% \zz{We noticed visual similarity, which is why we chose the CLIP model over the BERT model when discussing the similarity of objects and attributes.}
By computing cosine similarity in the CLIP text embedding space~\cite{richardson2024popsphotoinspireddiffusionoperators}, we assess the similarity among objects and evaluate their likeness.
The similarities between attributes can also help designers identify commonalities among objects, which can serve as anchors during the blending process (\textbf{G2}).
The Sankey diagrams represent the similarity scores between objects or attributes using the width of their connecting links (Figure~\ref{fig:interface}f), making it easier for users to understand the relationships between different options and aiding them in observing and comparing various generation outcomes (\textbf{G4}).
To preserve the relationships among the original data values and improve the visual clarity of the similarity matrix, we employ the Min-Max normalization~\cite{10.1007/978-3-031-42536-3_33} to standardize the similarity scores.



We use the DistilBERT model~\cite{sanh2020distilbertdistilledversionbert} to perform sentiment analysis on text descriptions of objects or attributes, thereby assisting users in making informed selections and reducing the likelihood of misinterpretations due to contextual variations or other factors.
Sentiment scores are calculated using the confidence level $C$ of the sentiment label, where ``positive'' labels are equal to $C$ while ``negative'' labels are calculated as $(1-C)$.
We employ color variations to distinguish between objects and attributes while ensuring that the color temperature gradient correlates with semantic sentiment changes.
% \zz{We use different colors to distinguish between objects and attributes, while ensuring that the variation in color temperature aligns with the changes in semantic sentiment.}
The Sankey diagrams use \pur{purple} and \lem{orange} to represent negative and positive sentiments for objects, and \gre{green} and \gol{gold} for attributes, respectively.
% respectively. For attributes, \gre{green} and \gol{gold} are used to represent negative and positive sentiments.
To determine the overall sentiment between two objects or attributes, we average their individual sentiment scores.
Subsequently, we apply quantile normalization to the overall sentiment scores to ensure that the data distributions are similar across different samples. This also makes the color distribution in the Sankey diagrams more balanced.



\begin{figure*}[t]
  \centering
  \includegraphics[width=0.7\linewidth]{figure/prompt-EXTRA.pdf}
  \caption{The final prompt is composed of four distinct modules: (1) objects, (2) attributes, (3) schemes, and (4) considerations. The schemes and metaphorical themes (marked in capital letters with a grey background), essential elements of the prompt, are dynamically generated by the GPT in response to the scheme and metaphor generation prompts. 
  % \zz{The structure of the final prompt is composed of four distinct modules: Objects, Attributes, Schemes, and Considerations. The structure demonstrates a general expression of the final prompt design, while the schemes and metaphorical themes are generated by GPT based on the Scheme and Metaphor Generation prompts. This modular structure provides a flexible framework for prompt engineering.}
  }
  % The interface consists of four distinct modules: the expression input module (a), the prompt exploration module (b, c, d), the visual blend exploration module (e), and the similarity visualization module (f).
  % \Description{The final prompt for image generation.}
     \label{fig:prompt}
\end{figure*}


\subsubsection{Generate Blending Prompts and Images.}



Following the user's selection of objects and attributes, \sysname\ generates blended images by first establishing blending schemes (Figure~\ref{fig:interface}d), and then converting the chosen scheme into a final prompt for image generation (\textbf{G3}).
To enhance user control and creative freedom, we adhered to prompt engineering best practices~\cite{Prompten79:online, doi:10.1080/10875301.2023.2227621} for blend scheme construction by outlining the GPT's role, the task, the user input, and a step-by-step process to guide the model (Figure~\ref{fig:prompt-template}).
Subsequently, the user's selected scheme, objects, attributes, and design considerations (e.g., metaphorical themes and design constraints) were integrated into the final prompt for visual blend generation (Figure~\ref{fig:prompt}). 
These design considerations were refined through iterative experimentation.
The complete prompt scripts used in our system are available in the supplementary material.
Finally, we use DALL·E 3 to generate images based on the prompt, swiftly transforming designers' creativity into reality.
The generated visual blends are displayed in the image exploration area (Figure~\ref{fig:interface}e). 
Users can explore these results based on object and attribute similarity (X and Y axes), uncovering new creative possibilities for visual blend creation (\textbf{G4}).
To provide a clearer overview of the generated content and support iterative exploration, we number each image group with the total number of images produced for that prompt, displayed in the upper-right corner.
Furthermore, to maximize design diversity, we avoid imposing unnecessary visual constraints on blend generation beyond the specified design considerations. 
This approach leads to a diverse range of design outcomes within the 2D exploration space.
For prompts that may be closely related, \sysname\ implements interactive zooming to adjust the scale of the visualization space and prevent image overlap.
Our system also facilitates diverse and iterative creative exploration by allowing users to alter specific objects when they conceive improved or alternative ideas (\textbf{G4}). Users can directly replace the objects in the system, prompting it to regenerate new images while discarding all prior visual information.




\begin{figure*}[t]
  \centering
  \includegraphics[width=0.88\linewidth]{figure/study.pdf}
  \caption{The user study procedure. It involved completing two tasks with both the \sysname\ and the baseline. To maintain fairness, the order of the systems and design tasks was counterbalanced.
  % The procedure of the user study. The 50-minute user study consists of two tasks with different systems, each task including a 8-minute design phase. The order of the tool and the design tasks are counterbalanced.
  }
  % \Description{The procedure of the user study.}
  \label{fig:study}
\end{figure*}

\subsection{User Scenario}

This section demonstrates how \sysname\ assists designers in creating visual blends. 
Alice, a graphic designer, wants to create an image that describes the concept of ``global warming'' to depict the idea of environmental protection. 
She starts by entering this phrase into the system (Figure~\ref{fig:interface}a). 
\sysname\ parses the input and identifies keywords like ``global'' and ``warming''. 
Alice selects these keywords to discover specific objects that can metaphorically represent them.
The \sysname\ system presents two lists of objects, along with their associated attributes (Figure~\ref{fig:interface}b\&c).
Two Sankey diagrams visually illustrate the connections between these objects and attributes (Figure~\ref{fig:interface}f). 
Alice initially examines the object names, their relevant attributes, and how they align with her concept (Figure~\ref{fig:interface}b\&c).
For unfamiliar objects, she can click the ``preview'' button to view generated images for more information.
She then examines the ``Objects Analysis'' Sankey diagram, which aids in object selection by displaying similarity and sentiment scores. 
% The similarity and sentiment scores are represented by the width and color of the connecting lines.
As Alice explores the Sankey diagram, clicking a link automatically selects the corresponding pair of objects in the lists (Figure~\ref{fig:interface}b\&c).
She finally picks ``earth'' and ``fireplace'' as positively oriented objects with a higher degree of similarity.






Alice then delves into the system's image-blending capabilities, beginning with an examination of the ``Attributes Analysis'' Sankey diagram for guidance.
This diagram visually represents the similarity and sentiment scores among the objects' attributes. By referencing similarity, Alice can assess the coherence of the current blending results and plan her next steps.
She experiments with various attribute combinations to adjust how the objects blend within the prompt.
Ultimately, she confirms the attributes ``round'' for ``earth'' and ``flames'' for ``fireplace''.
Once satisfied with her combinations, she generates the corresponding scheme prompts along with the previously selected objects (Figure~\ref{fig:interface}d).
The final prompt then creates a new blended image in the display area, which she can enlarge and examine (Figure~\ref{fig:interface}e). 
When multiple images are created at the same location, the system displays the total number of images in the upper right corner.
After several attempts, Alice realizes she has yet to explore certain design options for visual blending.
Inspired by her exploration, she decides to utilize the editing feature to modify an object from ``fireplace'' to ``ice cream'' (Figure~\ref{fig:interface}c). 
With further experimentation and exposure to additional sample designs of visual blends with different combinations, Alice gains new ideas that she can incorporate into her subsequent creations.


