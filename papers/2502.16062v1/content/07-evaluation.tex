\section{Evaluation}~\label{sec:eval}



To test our hypotheses, we conducted a controlled experiment with 24 participants using a within-subject design to minimize individual differences. 
Figure~\ref{fig:study} depicts the complete evaluation process for each participant.
To ensure a fair comparison and avoid the influence of varying image generation speed and style from different models, we opted for a more rigorous approach by using ChatGPT (GPT-3.5 with DALLÂ·E 3) and Google Search as the baseline. 
We focused on their core features, such as information search, image generation, and browsing history.
% During the experiment, 
Participants undertook design ideation tasks across the \sysname{} and the baseline. 
Clear instructions were provided at the outset of each task to enable participants to explore and interact with both conditions freely.
After completing each task, we administered questionnaires to gather participants' evaluations of their experiences with the respective system conditions. 
Our questionnaire design was informed by relevant literature~\cite{HART1988139, 10.1145/2617588, brooke1996sus} and guided by our research hypotheses, which shaped the evaluation around the following dimensions: we employed the System Usability Scale (SUS) and NASA-Task Load Index (NASA-TLX) to gauge the system's usability and the cognitive load imposed on users. Additionally, we incorporated the Creative Support Index (CSI) and items focused on outcome satisfaction and metaphoricity to measure the extent to which the system fostered creative ideation, user satisfaction, and the integration of metaphor within the design process.



\subsection{Participants}


We enlisted a total of 24 participants (12 female, 12 male) aged 20 to 31 ($Mean = 24.08, SD = 3.31$) through online advertising and word-of-mouth. The participants were local university students and faculty members, with equal representation from both computer science (6 female, 6 male) and design (6 female, 6 male) backgrounds. Our selection criteria included a demonstrated interest in visual design and some prior experience in design activities~\cite{10.1145/3411764.3445325}. Additionally, all participants possessed normal color vision and had experience with conversational AI and T2I models.




\subsection{Tasks and Procedure}



During the experiment, each participant was tasked with producing visual blending ideas for the two abstract expressions: ``Smoking is like a warm welcome to death'' (T1) and ``Knowledge guides the hope of our life'' (T2). 
Based on input from professional designers, our criteria for selecting these two topics included the presence of at least two concepts, their abstract nature that allows visualization with specific objects and attributes, the potential for considerable exploration, and the inclusion of associated imagery that is either positive or negative.
Participants were free to explore their creativity by creating visual blends related to the given topics.
To maintain experimental balance, we employed a Latin Square design, yielding four possible sequences: (a) T1 (Baseline) - T2 (\sysname), (b) T2 (Baseline) - T1 (\sysname), (c) T1 (\sysname) - T2 (Baseline), and (d) T2 (\sysname) - T1 (Baseline).


\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figure/baseline.pdf}
  \caption{The baseline interface integrated Google Search and ChatGPT for both text and visual search queries.
  % The baseline interface integrates Google Search and ChatGPT, allowing participants to explore queries related to both textual and visual searching tasks.
  }
  % \Description{The baseline interface.}
  \label{fig:baseline}
\end{figure}






Prior to the main study, we spent three minutes explaining the basic concepts of visual blends, encouraging participants to incorporate visual metaphors into their creative process.
The main study had two parts, each using either the baseline or \sysname\ system. 
The baseline interface integrated Google Search and ChatGPT into a unified side-by-side display (Figure~\ref{fig:baseline}), enabling users to interact with both platforms simultaneously.
Each task began with a 4-minute tutorial, offering an overview of the design process and step-by-step instructions, followed by a Q\&A session for clarification. 
We mitigated order effects by rotating task sequences using a Latin square design, and provided users with sample prompt words as a reference when introducing the baseline condition to ensure a fair evaluation.
After the tutorials, participants completed design tasks using the baseline or \sysname\ system within 8 minutes.
Once finished, they filled out 5-minute questionnaires about their design experience.
We also conducted 6- to 15-minute interviews to gain a deeper understanding of their experiences.
The experiment ended after the interviews, and the total duration did not exceed 60 minutes.



\begin{table*}[t]
\centering
\caption{The statistical results of user feedback with the \sysname\ and the baseline (i.e., control group), where the p-values ($-$: $p>.100$, $+$: $.050<p<.100$, $*$: $p<.050$, $**$: $p<.010$, $***$: $p<.001$) is reported. 
}
\small
\begin{tabular}{llrrrrrrrlr}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Category}}} &
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Factor}}}   &
\multicolumn{2}{c}{\sysname} & 
\multicolumn{2}{c}{Baseline} & 
\multicolumn{4}{c}{Statistics} & 
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Hypotheses}}} \\
\cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-10}
         &                   & Mean            & SD    & Mean     & SD      & $t$                & $Z$                & $p$                & Sig. &  \\
\toprule
\textbf{Usability}~\cite{brooke1996sus} & \textbackslash{}        & 75.000              & 9.918 & 61.458   & 21.781             & 2.547           & \textbackslash{} & 0.018            & $*$    &  \textbf{H1} \textbf{Accepted}          \\
\midrule
\multicolumn{1}{l}{\multirow{6}{*}{\parbox{1.3cm}{\textbf{Cognitive Load}~\cite{HART1988139}}}}  &
Mental Demand                  & 2.542           & 1.062 & 4.458    & 1.587              & -4.861            & \textbackslash{} & \textless{}0.001 & $***$  &   \textbf{H2a} \textbf{Accepted}         \\
                 & Physical Demand               & 1.625           & 0.824 & 2.958    & 1.967    & \textbackslash{} & -2.865           & 0.004            & $**$   &   \textbf{H2b} \textbf{Accepted}         \\
                 & Temporal Demand               & 3.250            & 1.359 & 3.833    & 1.685              & \textbackslash{} & -1.517           & 0.129            & $-$    &  \textbf{H2c} Rejected          \\
                 & Performance          & 4.583           & 0.929 & 4.333    & 1.736             & 0.598           & \textbackslash{} & 0.556            & $-$    &  \textbf{H2d} Rejected          \\
                 & Effort                  & 2.583           & 1.349 & 3.917    & 1.666              & -2.734            & \textbackslash{} & 0.012            & $*$    &  \textbf{H2e} \textbf{Accepted}          \\
                 & Frustration             & 2.625           & 1.663 & 3.625    & 1.663              & -1.931            & \textbackslash{} & 0.066            & $+$    &  \textbf{H2f} Rejected          \\
\midrule
\multicolumn{1}{l}{\multirow{3}{*}{\parbox{1.6cm}{\textbf{Outcome\\Satisfaction}}}}
        & Amount                  & 5.080            & 1.412 & 4.540     & 1.444              & 1.299           & \textbackslash{} & 0.207            & $-$    &   \textbf{H3a} Rejected         \\
                 & Diversity               & 5.380            & 1.345 & 3.670     & 1.274              & 5.058           & \textbackslash{} & \textless{}0.001 & $***$  &  \textbf{H3b} \textbf{Accepted}          \\
                 & Overall                 & 5.080            & 1.283 & 3.960     & 1.367              & 2.609           & \textbackslash{} & 0.016            & $*$    &  \textbf{H3c} \textbf{Accepted}          \\
\midrule
\multicolumn{1}{l}{\multirow{6}{*}{\textbf{Creativity}~\cite{10.1145/2617588}}}       & Exploration             & 5.708           & 1.160  & 4.083    & 1.886              & 2.985           & \textbackslash{} & 0.007            & $**$   &   \textbf{H4a} \textbf{Accepted}         \\
                 & Collaboration           & 5.500             & 1.560  & 3.667    & 2.014              & 3.350            & \textbackslash{} & 0.003            & $**$   &   \textbf{H4b} \textbf{Accepted}         \\
                 & Enjoyment              & 5.833           & 1.435 & 4.042    & 1.756              & 3.558           & \textbackslash{} & 0.002            & $**$   &   \textbf{H4c} \textbf{Accepted}         \\
                 & Results Worth Effort & 5.458           & 1.444 & 4.167    & 1.633              & 2.806           & \textbackslash{} & 0.010             & $*$    &  \textbf{H4d} \textbf{Accepted}          \\
                 & Immersion       & 5.167           & 1.903 & 5.042    & 1.706               & 0.213           & \textbackslash{} & 0.833            & $-$    &   \textbf{H4e} Rejected         \\
                 & Expressiveness          & 5.167           & 1.167 & 4.375    & 1.689               & 1.807           & \textbackslash{} & 0.084            & $+$    &   \textbf{H4f} Rejected         \\
\midrule
\textbf{Metaphoricity}    & \textbackslash{}        & 5.655           & 0.974 & 5.047    & 1.193              & 2.276           & \textbackslash{} & 0.032            & $*$    &  \textbf{H5} \textbf{Accepted}
\\
\bottomrule
\end{tabular}
% \caption{The statistical results of user feedback with the \sysname\ and the baseline (i.e., control group), where the p-values ($-$: $p>.100$, $+$: $.050<p<.100$, $*$: $p<.050$, $**$: $p<.010$, $***$: $p<.001$) is reported. 
% % $SW$ stands for Shapiro-Wilk Test.
% }
\label{tbl:result}
\end{table*}


\subsection{Results Analysis}

This section presents a statistical analysis of user ratings gathered during the experimental process.
The findings, contextualized by user feedback, are presented in relation to each evaluation criterion. 


\subsubsection{System Usability}

To assess system usability, we adopted the System Usability Scale (SUS)~\cite{brooke1996sus} as our questionnaire.
The means and standard deviations of the participants' ratings are presented in Table~\ref{tbl:result}.
From the participants' feedback, a Shapiro-Wilk test is conducted to examine the distribution's normality, and no evidence of non-normality is observed in the overall distribution ($W = 0.925, p = 0.075$).
A paired samples t-test comparing \sysname\ to the baseline shows significantly higher usability for \sysname\ ($t(23) = -2.547, p = 0.018$).
The effect size of Cohen's \textit{d} ($d = 0.800$) suggests a large effect on the improvement in \sysname\ usability.
Therefore, \textbf{H1} is accepted.
Two participants (P9, Male, 21; P17, Male, 21) praised \sysname\ for its simple and user-friendly interface, describing it as ``\textit{refreshing}''.
Almost all participants (22/24) found \sysname\ relatively easy to learn.
As one participant (P18, Female, 23) noted, ``\textit{While I needed some help at first to use \sysname, I was able to use it independently after practicing once.}''
Unlike the conversational interface of the baseline, \sysname\ presents generated results via a 2D mood board-like interface (Figure~\ref{fig:interface}e), providing a comprehensive overview of the output's scope.
The sentiment analysis visualizations also empower users to evaluate and refine the blending process effectively.





\subsubsection{Cognitive Load}


To evaluate cognitive load, we referred to the NASA-Task Load Index (TLX)~\cite{HART1988139}. 
The questionnaire is tailored to encompass six factors: 
mental demand (the amount of mental effort required to complete a task), 
physical demand (the degree of physical exertion needed to complete the task), 
temporal demand (the time taken to complete the task), 
performance (the impact of task completion), 
effort (the amount of effort necessary to complete the task), and frustration (the dissatisfaction experienced while completing a task).
Table~\ref{tbl:result} provides the mean and standard deviation of the participants' ratings.
The Shapiro-Wilk test reveals no evidence of non-normality in the distribution of mental demand ($W = 0.594, p =0.967$), performance ($W = 0.948, p =0.246$), effort ($W = 0.962, p =0.479$), and frustration ($W = 0.948, p =0.243$) factors. Therefore, paired samples t-tests were employed to compare the differences in these four dimensions between the two systems, as shown in Table~\ref{tbl:result}. 
However, the distributions of physical demand ($W = 0.811, p <0.001$) and temporal demand ($W = 0.870, p = 0.005$) factors deviate from normality. Consequently, the differences between the two systems in these dimensions were compared using the Wilcoxon signed-rank test.



The test results indicate that the cognitive load in the dimensions of mental demand ($t(23) = 4.861, p<0.001$), physical demand ($Z = -2.865, p = 0.004$), and effort ($t(23) = 2.734, p = 0.012$) when using \sysname\ is significantly lower than that of the baseline. 
The effect sizes of Cohen's $d$ and Pearson's correlation coefficient $r$ indicate that the reduction in mental demand ($d = -1.419$) was large, physical demand ($r = -0.414$) was moderate, and effort ($d = -0.880$) was large.
Consequently, hypotheses \textbf{H2a}, \textbf{H2b}, and \textbf{H2e} are accepted.
However, no significant differences were found in the dimensions of temporal demand, performance, and frustration, leading to the rejection of hypotheses \textbf{H2c}, \textbf{H2d}, and \textbf{H2f}.
% \att{However, no significant differences were found in the dimensions of temporal demand, performance, and frustration, leading to the rejection of hypotheses H2c, H2d, and H2f.}
The results show that there is little difference in the time spent and performance of the generated results between the \sysname\ and the baseline, as they both use the same underlying technologies. 
However, our approach significantly reduces the mental and physical burden on users and requires less effort from them. 
Four participants (P1, Female, 31; P2, Male, 23; P14, Male, 22; P23, Female, 30) praised the pipeline and interface applied in \sysname. 
They appreciated the time-saving benefits of not having to initiate the design process from scratch, and the generated results were more controllable, making their efforts more manageable.
Other participants further emphasized the value of the Sankey diagrams in streamlining the selection of appropriate object and attribute pairings, highlighting their ability to provide an overview and thereby minimize the cognitive effort required for individual pair evaluations.



\subsubsection{Outcome Satisfaction}

To measure outcome satisfaction, we designed the questionnaire inspired by MetaMap~\cite{10.1145/3411764.3445325}. 
The questionnaire includes three factors, i.e., satisfaction with the amount, diversity, and overall quality of the system output.
We gathered participants' ratings and conducted a Shapiro-Wilk test. The results show no evidence of non-normality in the distribution of outcome satisfaction ratings for the amount ($W = 0.939, p =0.154$), diversity ($W = 0.931, p =0.101$), and overall quality ($W = 0.936, p =0.129$). 
Therefore, paired samples t-tests were employed to compare the differences in outcome satisfaction between the two systems.
Table~\ref{tbl:result} provides the mean and standard deviation of the relevant factors.


Overall, participants were significantly more satisfied with the outcomes from \sysname\ than with those from the baseline ($t(23) = -2.609, p = 0.016$).
% \att{Overall, participants were significantly more satisfied with the outcomes from Creative Blends than with those from the baseline system ($t(23) = -2.609, p = 0.016$).}
This large effect size (Cohen's $d = 0.845$) indicates a meaningful improvement in satisfaction with \sysname. 
Specifically, participants were significantly more satisfied with the diversity of outcomes generated by \sysname\ ($t(23) = -5.058, p = <0.001; d = 1.305$). 
However, there was no significant difference between the two systems in the number of outcomes generated. 
% \att{However, there was no significant difference between the two systems in the number of outcomes generated.}
Therefore, hypotheses \textbf{H3b} and \textbf{H3c} are accepted, while hypothesis \textbf{H3a} is rejected.

Participants indicated that the \sysname\ system 
facilitated the generation of more varied, predictable, and actionable expressions of their ideas. 
Nearly half of the participants (11 out of 24) mentioned that the baseline often produced outcomes that deviated from their intended concepts, while \sysname\ guided users in expanding their design choices systematically. 
Some participants expressed frustration with the baseline, stating, ``\textit{I feel like my prompts are pretty clear, but I am struggling to get good results...It is really frustrating that these results are not even close to what I am looking for}'' (P8, Female, 21). 
In contrast, P17 (Male, 21) commented, ``\textit{I find it much easier to achieve the desired effect with \sysname. You can start with one idea and branch off into lots of different directions}''.
The association-based thought expansion approach for visual blend design, which links objects and their attributes, has demonstrated its effectiveness in rapidly generating diverse results.




\subsubsection{Creativity}



We borrowed the Creative Support Index (CSI)~\cite{10.1145/2617588} to assess creativity. 
We modified the CSI questionnaire to include six metrics: 
exploration (the extent to which the system supports user exploration), 
collaboration (the manner in which users collaborate with AI), 
enjoyment (the level of enjoyment participants experienced during the activity), 
results worth effort (the value derived from the effort), 
immersion (the degree of focus within the system), and 
expressiveness (the expressiveness and creativity exhibited during the activity).
Table~\ref{tbl:result} provides the mean and standard deviation of these six factors.
The Shapiro-Wilk tests indicate that the distributions of scores for all six factors are normal. 
For each factor, the following values were obtained: exploration ($W = 0.944, p = 0.205$), collaboration ($W = 0.964, p = 0.524$), enjoyment ($W = 0.946, p = 0.225$), results worth effort ($W = 0.946, p = 0.227$), immersion ($W = 0.975, p = 0.791$), and expressiveness ($W = 0.953, p = 0.307$).


The results in Table~\ref{tbl:result} show that \sysname\ significantly outperforms the baseline in supporting creativity in exploration ($t(23) = -2.985, p = 0.007$), collaboration ($t(23) = -3.350, p = 0.003$), enjoyment ($t(23) = -3.558, p = 0.002$), and results worth effort ($t(23) = -2.806, p = 0.010$). 
The substantial advantages of \sysname\ are reflected in the large effect sizes, with Cohen's d values for exploration ($d = 1.038$), collaboration ($d = 1.018$), enjoyment ($d = 1.117$), and results worth effort ($d = 0.838$).
Compared to the baseline, participants found \sysname\ to be more supportive of exploring, collaborating, and enjoying the creative process. Additionally, participants perceived a significant improvement in the value of their efforts when using \sysname.
% \att{Additionally, participants perceived a significant improvement in the value of their efforts when using Creative Blends}
Consequently, hypotheses \textbf{H4a}, \textbf{H4b}, \textbf{H4c}, and \textbf{H4d} are supported, whereas hypotheses \textbf{H4e} and \textbf{H4f} are refuted.



Feedback from participants suggests that our system significantly improves the exploration of design options, enhances engagement, and reduces the burden of ideation during the creative process. 
Five participants emphasized that \sysname\ helped them express abstract concepts in visual representation more effectively than the baseline (P3, Male, 27; P11, Male, 27; P13, Female, 25; P19, Male, 22; P22, Male, 26).
As P19 (Male, 22) reflected, ``...\textit{(\sysname) could really help me think outside the box and come up with some new solutions that I would not have thought of on my own}''.
\sysname\ leverages metaphors to facilitate inter-domain concept transfer. By examining the underlying relationships between disparate concepts, the system produces a more multifaceted assortment of object combinations.
Additionally, our approach fosters collaboration between users and AI. 
Like P1 (Female, 31) remarked, ``\textit{...basically, I tell \sysname\ what I am looking for, and it fills in all the gaps. It is like giving AI a blueprint, and then it builds the whole thing}''.
% Such findings demonstrate that humans can effectively collaborate with AI, not only delegating routine tasks but also decomposing complex problems into manageable subtasks. 
These findings provide empirical evidence that human-AI collaboration extends beyond simple task delegation, encompassing the ability of humans to decompose complex problems into manageable subtasks for AI processing effectively.
% By leveraging AI's data-driven expertise to address those components that lack clear specifications, humans can achieve a synergistic combination of creative and analytical capabilities.
Moreover, two participants stated that \sysname\ increased their ability and willingness to explore (P18, Female, 23; P23, Female, 30). 
Even after the design session ended, both expressed a desire to continue exploring.



\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figure/user-study-sma.pdf}
  \caption{Eight sample outputs generated by the \sysname\ and the baseline for topics of T1 and T2.}
  % \Description{The difference of the outcome generated by different systems.}
  \label{fig:user-study}
\end{figure}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/results-arrow.pdf}
  \caption{\sysname{} generates diverse visual blends representing abstract concepts based on user-provided expressions. Each topic includes eight examples: four highlighting different levels of object similarity and four demonstrating varying attribute similarity. Similarity increases from left to right. The attributes are extended based on the objects enclosed by the double brackets. Colors within the topics serve to identify concepts and their associated objects and attributes.}
  % \Description{The results generated with Creative Blends.}
  \label{fig:results}
\end{figure*}



\subsubsection{Metaphoricity}

To assess the metaphorical quality of the generated outputs, we adopted a method outlined in previous research~\cite{10.1007/978-0-85729-224-7_13}.
% Participants were asked to rate the similarity between the target and source elements on a 7-point Likert scale.
Participants were asked to indicate the extent to which they perceived a metaphorical relationship between the target and source elements, using a 7-point Likert scale ranging from ``no metaphor'' to ``extremely strong metaphor''.
The Shapiro-Wilk test confirms the normality of the distribution ($W = 0.935, p = 0.124$). 
A paired samples t-test reveals a significant difference in overall similarity between \sysname\ and the baseline ($t(23) = -2.276, p = 0.032$).
The moderate effect size (Cohen's $d = 0.558$) indicates a meaningful improvement in the metaphorical quality of content generated by \sysname, leading to the acceptance of hypothesis \textbf{H5}.

Participants' feedback highlights the significance of metaphor in bridging the gap between abstract concepts and familiar imagery.
As P15 (Male, 23) observed, ``\textit{... the (baseline) often turns abstract information into scenes rather than specific characters}''.
This shows that existing models are not yet able to associate abstract meanings with existing imagery of physical objects. 
In addition, six participants noted that the target and source elements within \sysname\ outcomes were more readily recognizable compared to the baseline (P5, Male, 21; P7, Female, 21; P18, Female, 23; P21, Male, 20; P22, Male, 26; P24, Female, 23).
These findings suggest that the \sysname\ system preserves the original features of blended objects while aligning metaphorical relationships with human cognition within the constraints of the commonsense knowledge derived from the physical world.





\subsection{Ideation Results}



We collected the outputs produced by participants during the study.
% ideation process using both the \sysname\ system and the baseline.
These results have been documented and are provided as part of our supplementary materials.
Figure~\ref{fig:user-study} presents eight representative examples selected from four participants (i.e., P1, P5, P6, and P18), showcasing typical outcomes generated by both system conditions. 
Our analysis revealed that both conditions are capable of generating images that use visual metaphors to represent abstract concepts.
However, \sysname\ exhibited a notable ability to generate blended results that align with the definition of visual blends (as shown in Figure~\ref{fig:user-study}). 
In contrast, the baseline often relied on background elements, scene composition, or visual juxtaposition to communicate abstract ideas, and, in some cases, simply incorporated related text directly within the images.









To demonstrate the capability of \sysname, we showcase generated results for four additional topics in Figure~\ref{fig:results}.  
These examples illustrate the versatility of \sysname's output.
The examples in the left part of Figure~\ref{fig:results} show the system's ability to explore physical objects that vary in their relevance to abstract concepts.
To achieve optimal object combinations, users can adjust the blending structure and connection method according to the similarity of their attributes (Figure~\ref{fig:results} right).
Overall, \sysname\ facilitates metaphorical mapping between physical objects and their associated abstract concepts, and builds connections between these different objects, stimulating creative ideas for visual blend creation.  

