\section{Discussion}



This section delves into user preferences for creating visual blends and the generalizability of our method. We also provide insights for designing with generative AI and highlight the study's limitations.



\subsection{Design Preferences of Visual Blends}



During the evaluation of \sysname, we observed a variety of user designs reflecting their individual preferences.
These preferences were apparent not only in our interviews but also in how they interacted with both systems. 
We can categorize these preferences into three main design options: a preference for the visual expression of metaphors, the features prioritized for blending, and the visual style of blended results.




\subsubsection{Explicit vs. Implicit Expression of Visual Metaphors}





The design content of visual blends can directly or indirectly represent the topic.
For instance, in Figure~\ref{fig:direction}, when creating visual blends for ``global warming'', we can choose the ``fireplace'' from the explicit direction to establish a direct metaphorical connection with the concept of ``warming''. Alternatively, we can select the ``ice cream'' from the implicit direction to represent this concept through its melting state. Similarly, for the task ``Pepper sauce: be aware of the heat'', we can opt for the ``lighter'' from the explicit direction to directly represent the concept of ``heat''. Or, we can choose the ``fire extinguisher'' from the implicit direction to represent this concept through its fire-extinguishing or heat-reducing function.


\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{figure/example.pdf}
  \caption{Designers have two options when choosing visual metaphors: explicit (left) or implicit (right).
  % There are two distinct design ideation options available. Designers can choose to create objects in an explicit manner (left) or an implicit manner (right) for the desired results.
  % Two different design ideation choices. 
  % Designers can design objects in an explicit way (left) or an implicit way (right) for the results.
  }
  % \Description{Two different design ideation choices on visual metaphors.}
  \label{fig:direction}
\end{figure}


Almost half of the participants (13 out of 24) preferred implicit expression, finding it more subtle (P20, Female, 31; P24, Female, 23) and thought-provoking (P13, Female, 25). This approach allows the audience to gain a deeper understanding of the design output. These advantages contribute to the engagement (P11, Male, 27) and imagination (P9, Male, 21) of the resulting outcomes (P12, Female, 24). P9 (Male, 21) noted that implicit expressions provide richer metaphorical information.
While implicit expressions offer these benefits, explicit visual metaphors succeed in directly representing the literal information, providing a clear and straightforward understanding.




\subsubsection{Prioritizing Semantic vs. Visual Features when Blending}

When visually representing abstract concepts, a debate exists regarding the optimal object features to utilize.
Participants' preferences were evenly distributed, with eight out of 24 participants favoring semantic features, while nine favored visual features.
Those who preferred semantic features believed that semantic meaning captures the contextual nuances of concepts, enabling accurate representation (P20, Female, 31) and facilitating the understanding of abstract concepts and relationships that are not immediately apparent visually (P18, Female, 23). 
They hypothesized that blended results could convey the topic more directly, clearly, and comprehensively (P15, Male, 23).
However, participants also recognized the potential ambiguity of semantic meanings arising from polysemy (words with multiple meanings) and homonymy (different words that share the same spelling).
Conversely, those who preferred visual features favored a straightforward utilization of visual elements such as color, shape, and texture when creating visual blends (P8, Female, 21; P14, Male, 22). 
They believed that higher visual similarity would result in more harmonious blended visual outcomes (P6, Female, 23). 
Nevertheless, the superficiality of visual representation can also lead to misinterpretations of meaning.
We propose that when creating visual blends, designers should seek suitable objects based on semantic features and then identify the most appropriate forms of related objects for blending using visual features to achieve more effective conceptual expression.


Beyond using relevant features to facilitate harmonious blending, similarity scores within each feature also play a pivotal role in guiding designers toward diverse exploration paths. 
While high similarity scores are often desirable, designers may intentionally select object combinations with lower similarity scores to achieve more exaggerated and visually striking results.
As illustrated by the example ``exercise is your daily dose of body vitamins'' in Figure \ref{fig:results}, ``orange'' and ``basketball'' exhibit greater visual compatibility than ``orange'' and ``badminton'' and are easier to combine; however, this may lead to a less innovative visual outcome.
This approach encourages designers to explore a broader range of possibilities, stimulating their creativity and accelerating the ideation process.






\subsubsection{The Integrated vs. Independent Styles of Blended Outcomes}

Upon analyzing the generated visual blends, we identified two primary blending styles: objects merging into a unified form while retaining their individual identities or remaining independent and connected by shared attributes.
For instance, the visual blends of ``global warming'' (first row) in Figure~\ref{fig:results} exhibit a cohesive appearance between different objects.
The integrated representation creates a seamless and unified visual, which can be aesthetically pleasing and easier to interpret as a single entity.
However, achieving a balanced and harmonious blend can be challenging and may necessitate a thorough analysis of the objects' attributes.
In contrast, the visual blends of ``smoking is like a warm welcome to death'' (last row) in Figure~\ref{fig:results} present the distinct characteristics of each object.
Such independent representation allows for more flexibility in design, as objects can be repositioned or modified independently without affecting the overall composition. 
Using common attributes to connect objects can highlight the relationships or interactions among them, providing additional information.
However, the separated objects may not convey a sense of harmony and unity as effectively as the integrated design, and an excessive focus on individual characteristics might detract from the overall message.
Overall, the visual blend's style is largely shaped by the attributes of the combined objects. 
Designers have the flexibility to optimize the representation by substituting associated objects or adjusting visual attributes.







\begin{figure}[b]
  \centering
  \includegraphics[width=0.9\columnwidth]{figure/compare-visiblends.pdf}
  \caption{\sysname\ draws upon topics from \textit{VisiBlends}~\cite{10.1145/3290605.3300402} to showcase the related physical objects and the interconnections among their attributes.
  % Two different design ideation choices. 
  % Designers can design objects in an explicit way (left) or an implicit way (right) for the results.
  }
  % \Description{Results comparison.}
  \label{fig:compare-visiblend}
\end{figure}





\subsection{Generalization of \sysname}




\sysname\ is designed and implemented to create visual blends by merging two objects representing distinct concepts.
To test its generalizability, we applied the system to additional topics studied in prior research and expanded our experiments to evaluate its ability to combine multiple objects.




\subsubsection{Create Visual Blends with Previous Topics}



To demonstrate \sysname's performance in generating visual blends, we employed the same topics used in \textit{VisiBlends}~\cite{10.1145/3290605.3300402}.
Unlike the manual brainstorming and image selection processes in \textit{VisiBlends}, our system automatically identifies metaphorically related objects and offers a variety of blending options based on shared attributes. 
This facilitates creative ideation and reduces user effort. 
Figure~\ref{fig:compare-visiblend} demonstrates two topics from \textit{VisiBlends}. 
Beyond the high-quality visual results achieved through advanced image generation techniques, our approach provides a broader range of blending possibilities. 
By considering attribute combinations rather than solely relying on object shapes, we enable more creative blending directions. 
Additionally, our method focuses on the essential aspects of physical objects that connect to abstract concepts, leading to natural and visually appealing blended outcomes.






\begin{figure*}[t]
  \centering
  \includegraphics[width=0.79\linewidth]{figure/compare-spy.pdf}
  \caption{The results generated by our approach when processing more than two concepts. The outcomes presented are in line with the topics explored in \textit{I Spy a Metaphor}~\cite{chakrabarty-etal-2023-spy}.}
  % \Description{Results comparison.}
  \label{fig:compare-spy}
\end{figure*}



% \subsubsection{Objects Extension}
\subsubsection{Increase the Number of Blended Objects}

When blending three or more concepts, our approach emphasizes selecting two main concepts to merge into a primary subject, while the remaining concepts are incorporated sequentially as secondary elements in the image.
By considering similarity and sentiment scores, we strategically select objects and attributes for these additional concepts based on similarity and sentiment scores to create the final blended composition.
Previous work, such as \textit{I Spy a Metaphor}~\cite{chakrabarty-etal-2023-spy}, also explores metaphorical representations.
% \att{Previous work, such as I Spy a Metaphor [10], also explores metaphorical representations.}
Unlike its method, which treats all concepts equally, our approach distinguishes itself by emphasizing the blending of specific concepts with metaphorically related objects.
This not only facilitates creative exploration but also enhances the visual representation of abstract ideas~\cite{8eb8812e-2a0d-3ce5-ba0f-6a9667472863}.
Figure~\ref{fig:compare-spy} demonstrates four topics that highlight the differences between our method and the previous approach.
In the expression ``Books are the mirror to the soul'', our system merges ``books'' and ``mirror'' into a primary subject, strengthening the metaphorical connection and establishing it as the main focus of the image. 
Additionally, we incorporate the ``phoenix'' element to represent the ``soul'', showcasing the remaining concepts within the expression.
In terms of the final output, the previous method relied on visual elaboration without clear semantic mapping, leading to scattered visual messaging. 
% \att{In terms of the final output, the previous method relied on visual elaboration without clear semantic mapping, leading to scattered visual messaging.}
Our method, however, prioritizes element integration while preserving individual characteristics, making the intended message more easily recognizable by the audience.






\subsection{Design with Generative AI}

During our experiment, we found that the general T2I models do not always reflect users' intent from their generated results. 
Participants commented that ``\textit{sometimes it is tough to keep the AI model on track}'' (P11, Male, 27). 
In some circumstances, users ``\textit{... have no confidence or idea of how the AI will interpret what (they) say}'', and ``\textit{...each prompt is a bit of a gamble}'' (P15, Male, 23). 
These responses revealed two major issues in designing with generative AI.
One is that natural language can be limited in its ability to convey certain design intentions.
The second issue is that the capabilities of generative AI can sometimes be confusing to users due to their unclear boundaries.




The conversational style of current generative AI interfaces has catalyzed a novel trend in interaction design, enabling more intuitive communication with machines through natural language~\cite{10.1007/978-3-031-66329-1_41, 10.1007/978-3-031-48038-6_36}.
However, the lessons we learned have revealed the limitations of natural language in conveying abstract concepts and representing tacit human knowledge, which is often learned through experience rather than explicit instruction.
While natural language interfaces work well in domains where user intent is easily articulated, truly versatile AI interactions require systems that can understand abstract intent and respond to subtle cues, much like human communication.
To mitigate this limitation, specialized tools can tailor the user interface to suit the needs of target users better.
Given the potential shortcomings of natural language interfaces in handling multimedia content~\cite{voigt-etal-2021-challenges}, specialized tools can effectively leverage alternative user interfaces (e.g., graphical user interfaces, or tangible user interfaces) to enhance user support for interactive tasks. 



While generative AI offers the potential for processing multi-modal information, it currently requires advancements in task decomposition and domain-specific expertise to address complex problems effectively.
The evolution of software development shows that specialized techniques are needed to harness the unique capabilities of diverse applications. 
Despite the potential to integrate various information sources, current generative AI models sometimes struggle to produce coherent and reliable outputs, frequently generating unrealistic or nonsensical results.
For example, Figure~\ref{fig:user-study} illustrates how P6's result in T1 exhibits a visual discrepancy, such as an improperly lit cigarette. 
While \sysname{} focuses on design ideation, resolving such visual issues falls outside its scope. 
These common failures, often due to training data bias or model misalignment, can be mitigated through techniques such as retrieval-augmented generation or fine-tuning~\cite {liu2024survey}, though these methods require technical expertise and additional resources~\cite{gao2024retrievalaugmentedgenerationlargelanguage}.
From a human-computer interaction perspective, challenges such as ambiguous input, lack of context, and inadequate instructions hinder users' ability to leverage generative AI for complex tasks. 
To improve usability, we propose using prompt engineering to break tasks into smaller sub-tasks and employing a guided task decomposition interface tailored to specific tasks, ultimately enhancing user understanding and problem-solving efficiency.


\subsection{Limitations and Future Work}





Our research is subject to several limitations that we intend to address in future work.
Firstly, our evaluation was conducted using two design ideation tasks, each limited to under 20 minutes. 
In contrast, real-world design processes often span longer durations and encompass multiple iterative rounds of refinement and exploration. 
As a result, our analysis of user interactions and behaviors in creating visual blends was deliberately constrained to the ideation stage, reflecting the system’s primary focus.
In our evaluation, we employed a baseline consisting of Google Search and ChatGPT (GPT-3.5 with DALL·E 3) for comparison, aiming to balance technical sophistication due to the automatic generation capabilities of AI models. 
Other comparisons might involve existing ideation methods like moodboarding~\cite{10.1145/3290605.3300863} or mind-mapping~\cite{10.1145/3411764.3445325} to investigate strengths and weaknesses further.
Future work can also integrate \sysname\ into real-world design cases to observe how the system can facilitate designers' visual blend design processes.


Secondly, our system is designed primarily to inspire users during the ideation process, which currently limits the flexibility of the provided prompts for re-editing.
% our system currently focuses on inspiring users during their ideation process. 
% Consequently, the provided prompts have limited flexibility in re-editing. 
Moreover, if used to generate the final result, the system still requires multiple trial-and-error iterations to achieve a satisfactory outcome for users. 
Future enhancements could include improving prompt editability to allow professional users greater flexibility in expressing their creativity, incorporating more control modalities to streamline the trial-and-error process, and extending support to the refinement stage further to enhance the system’s utility and creative capabilities.



Thirdly, since our research primarily focused on understanding the system's potential within the design ideation context, a comprehensive analysis of prompt quality and generated result attributes, such as style and layout, was not undertaken.
Nonetheless, we recognize that the quality of user-supplied prompts directly influences the generated outcomes.
Future investigations could delve deeper into how the user's knowledge or experience with AI can affect their experience in generating visual blends. Additionally, exploring the impact of varying prompt formats on AI behavior and the resulting visual blend outputs could provide valuable insights into optimizing the creative process.


