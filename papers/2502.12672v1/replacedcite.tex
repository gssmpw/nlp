\section{Related Work}
\subsection{Model Merging}
Model merging is a technique that combines multiple models into a single model while preserving the key attributes of each original model. 
Weight space interpolation could be regarded as a kind of model merging between pre-trained model and fine-tuned model.
Several studies have investigated its applications in the speech domain. These include merging ASR models trained on different dataset distributions ____, merging TTS models to interpolate speaker attributes ____, merging speech translation models to expand language pairs ____, and merging ASR models to facilitate lifelong learning ____.

In contrast to these works, we propose a novel application of model merging for speech representation learning. While previous works focus on task-specific models, our method aims to improve general speech representations. 

\subsection{Out-of-distribution Generalization}
Out-of-distribution (OOD) generalization aims to preserve a model's performance on OOD datasets after fine-tuning. 

Several studies in computer vision have explored OOD generalization for vision pre-trained models that could perform zero-shot inference.
% , for example, CLIP ____. 
For instance, Wise-FT ____ proposes merging the weights of pre-trained and fine-tuned models to enhance robustness on OOD datasets. 
LP-FT ____ introduces a simple two-step approach—linear probing followed by full fine-tuning—to mitigate distortion in pre-trained features by first learning a near-optimal linear head during the initial steps.
Model Soup ____ demonstrates that averaging the weights of models fine-tuned with different hyperparameter configurations can improve robustness on OOD datasets. 

The work most relevant to Speech-FT is Wise-FT, as both methods involve merging the pre-trained and fine-tuned models.  
However, our work focuses on speech representation models with the goal of preserving the generalization ability across various downstream tasks, rather than solely addressing dataset distribution. This represents a significantly different research setting.
In addition, we enhance weight space interpolation by integrating stable fine-tuning, which has been shown to be essential for preserving the generalization ability of speech representation models.

\subsection{Continuous Learning}
Continuous learning, or lifelong learning, enables models to adapt to new domain data while retaining previously acquired knowledge. It addresses the issue of catastrophic forgetting ____ and is essential for handling dynamic and evolving environments. Continuous learning typically involves multiple rounds of fine-tuning to simulate real-world scenarios where models continuously learn from new data.

Several prior studies have explored continuous learning for automatic speech recognition ____.
While we also aim to preserve previously acquired knowledge, 
our goal is fundamentally different from that of continuous learning. Unlike continuous learning, our goal is to enhance the speech representation after fine-tuning. Hence, It is not necessary for us to fine-tune the model on a long sequence of tasks.