\pdfoutput=1

\documentclass[11pt]{article}

\usepackage{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{adjustbox}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{inconsolata}
\usepackage{graphicx}

\title{Speech-FT: A Fine-tuning Strategy for Enhancing Speech Representation Models Without Compromising Generalization Ability}

\author{Tzu-Quan Lin\textsuperscript{1}, Wei-Ping Huang\textsuperscript{1}, Hao Tang\textsuperscript{2}, Hung-yi Lee\textsuperscript{1} \\
        \textsuperscript{1}Graduate Institute of Communication Engineering, National Taiwan University, Taiwan \\ 
        \textsuperscript{2}University of Edinburgh, United Kingdom}

\begin{document}
\maketitle
\begin{abstract}
Speech representation models are highly effective at extracting general features for various tasks.  
While fine-tuning can enhance these representations for specific applications, it often compromises their generalization ability.  
To address this challenge, we propose Speech-FT, a fine-tuning strategy for speech representation models that leverages model merging to preserve generalization ability while still benefiting from fine-tuning.
Speech-FT is effective across different fine-tuning scenarios and is compatible with various types of speech representation models, providing a versatile solution.  
Speech-FT offers an efficient and practical approach to further improving general speech representations after pre-training.  

\end{abstract}

\section{Introduction}

In recent years, speech representation models \cite{chung2019unsupervised,baevski2020wav2vec,ling2020decoar,hsu2021hubert,chen2022wavlm} have emerged as powerful tools for extracting general speech representations, benefiting a wide range of downstream tasks \cite{yang2021superb, tsai2022superb, feng2023superb}. 

To further improve these representations, prior research has explored fine-tuning speech representation models in various settings. Some studies have focused on supervised fine-tuning, where models learn enhanced representations with the aid of labeled data \cite{chen2021speech}. Others have adopted unsupervised fine-tuning by continuing the pre-training process on new languages \cite{getman2024happens}.

However, as shown in previous work \cite{chen2021speech}, supervised fine-tuning can make representations overly specialized for the target task, reducing their generalization ability.  
Meanwhile, as we will demonstrate, continuously pre-training on new languages may cause the model to forget previously learned languages.  

One possible reason fine-tuning degrades the generalization ability of representations is the significant distortion of pre-trained features \cite{kumar2022fine}.  
Minimizing weight modifications during fine-tuning may help mitigate this issue.  
Although adding a regularization term can effectively constrain weight modifications, it also limits the modelâ€™s ability to adapt to the fine-tuning task, leading to suboptimal performance.

To address this issue, we propose Speech-FT, a fine-tuning strategy based on model merging that retains the benefits of fine-tuning while preserving generalization ability.  
Specifically, we apply linear interpolation between the pre-trained and fine-tuned models to reduce weight modifications.  
Beyond minimizing weight changes, model merging has also been shown to effectively integrate knowledge from different models \cite{ilharco2022editing},  
making it a practical strategy for preserving the generalization ability of the pre-trained model while incorporating task-specific knowledge from fine-tuning.  

Our experiments show that Speech-FT is effective across different fine-tuning scenarios, including supervised, unsupervised, and multitask fine-tuning. 
Not only does it preserve generalization ability, but it also enhances speech representations based on the fine-tuning tasks, making it a valuable approach for refining speech representations after pre-training. 
Moreover, Speech-FT introduces no additional computational cost compared to standard fine-tuning and is compatible with a broad range of speech representation models.

The contributions of this work are summarized as follows:  
\begin{itemize}  
\item We propose Speech-FT, a novel fine-tuning approach for enhancing speech representation model while preserving the generalization ability.
\item Speech-FT offers an efficient and effective approach to further improve general speech representations after pre-training.  
\item Speech-FT is applicable across diverse fine-tuning scenarios and compatible with various speech representation models, demonstrating its broad utility.  
\end{itemize}  

\section{Methodology}
\subsection{Problem Formulation}
In this work, we investigate how fine-tuning can enhance pre-trained speech representation models, denoted as $\theta_{0}$, whose representations are used for a set of evaluation downstream tasks $\mathcal{T} = \{t_1, t_2, \dots, t_n\}$. Given a fine-tuning task $\hat{t}$, our objective is to design an algorithm $C$ such that the fine-tuned model, defined as $\hat{\theta} = C(\theta_{0}, \hat{t})$, improves performance across the evaluation task set $\mathcal{T}$.

Figure~\ref{fig:pipeline} illustrates the overall pipeline. During fine-tuning, we attach a task prediction model $D$ to the speech representation model $\theta_{0}$. The task prediction model $D$ takes the speech representations generated by $\theta_{0}$ as input and produces task-specific predictions. Both $D$ and $\theta_{0}$ are optimized jointly during fine-tuning. After fine-tuning, the task prediction model $D$ is discarded, and we evaluate the representation of the fine-tuned model $\hat{\theta}$.

\begin{figure}[h]{}
    \centering
    \includegraphics[width=1\linewidth]{vis/speechftscheme.drawio.pdf}
    \caption{Illustration of the pipeline for representation learning and evaluation. Our goal is to find an effective algorithm $C$ to improve the representation of $\hat{\theta}$.}
    \label{fig:pipeline}
\end{figure}

\subsection{Speech-FT}
Speech-FT consists of two main components: stable fine-tuning and weight space interpolation.
See Algorithm~\ref{alg:speech-ft} in the Appendix for an overview of Speech-FT.

\vspace{1em}
\noindent\textbf{Stable Fine-tuning.}
Fine-tuning speech representation models is challenging, as pre-trained features can undergo significant distortion due to mismatches between pre-training and fine-tuning tasks.
To address this issue, we propose a stable fine-tuning process that helps minimize such distortions.

When fine-tuning with a randomly initialized task prediction model $D$, the pre-trained features may be severely altered as the model adapts to the fine-tuning task \cite{kumar2022fine}. This issue can be mitigated by first training a well-initialized task prediction model $D$. Therefore, we begin fine-tuning by updating only the task prediction model $D$ during the initial $\beta\%$ of fine-tuning steps.

We further stabilize fine-tuning by freezing the downsampling module.
Modern speech representation models often include a downsampling module at the beginning, typically a multi-layer convolutional encoder that processes raw audio and generates downsampled features.
Prior studies have shown that this module captures universal low-level features, such as frequency patterns \cite{lin2023melhubert}, which are generally crucial for various applications.
To preserve these robust low-level features, we freeze the downsampling module throughout the entire fine-tuning process.

\vspace{1em}
\noindent\textbf{Weight Space Interpolation.}
While fine-tuning improves task-specific representations, it often leads to substantial weight modifications, which may compromise the generalization ability of the pre-trained model.

A straightforward approach to mitigating this issue is to add a regularization term during fine-tuning to keep the fine-tuned model closer to the pre-trained model in weight space. However, as we will show in Section~\ref{sec:discuss}, this approach restricts the effectiveness of fine-tuning, ultimately leading to suboptimal performance.

Instead, we adopt model merging as an alternative, combining the pre-trained and fine-tuned models using linear interpolation.  
Model merging has been demonstrated to effectively integrate knowledge from different models across various settings \cite{ilharco2022editing, wortsman2022robust, lin2023spurious, lin2024mitigating, kulshreshtha2024sequential}.

Since the pre-trained model inherently possesses strong generalization ability, interpolating between the pre-trained and fine-tuned models offers a promising approach to maintaining this ability while still benefiting from fine-tuning.

Given the pre-trained model's weights $\theta_{0}$ and the fine-tuned model's weights $\theta'$, we derive the merged model $\hat{\theta}$ as follows:
\begin{equation}
\label{eq:interpolate}
\hat{\theta} = (1-\alpha) \cdot \theta_0 + \alpha \cdot \theta'
\end{equation}
where $\alpha$ is a scaling hyperparameter ranging from 0 to 1, determining the balance between the pre-trained and fine-tuned models.

\subsection{Extending to Multiple Fine-tuning Tasks}
\label{sec:multiple-task}
Given multiple fine-tuning tasks $\{\hat{t_1}, \hat{t_2}, \dots, \hat{t_k}\}$, our goal is to design an algorithm $C$ such that $\hat{\theta} = C(\theta_0, \hat{t_1}, \hat{t_2}, \dots, \hat{t_k})$ leads to improved representations. By leveraging multiple tasks $\hat{t}_1, \hat{t}_2, \dots, \hat{t}_k$, we aim to incorporate diverse knowledge during fine-tuning, further enhancing speech representations.

We explore four different strategies for integrating multiple fine-tuning tasks:

\vspace{1em}
\noindent\textbf{Multitask Fine-tuning (MTF).}  
We stably fine-tune the pre-trained model using multiple tasks simultaneously, where separate task prediction models are appended to the pre-trained model. The resulting multitask model $\theta'$ is used in Speech-FT as follows:
\begin{equation}
\hat{\theta} = (1-\alpha) \cdot \theta_0 + \alpha \cdot \theta'
\end{equation}

\noindent\textbf{Linear Merge.}  
Denote the stably fine-tuned models for multiple tasks as $\theta_1', \theta_2', \dots, \theta_k'$. The merged model in Speech-FT is computed as:
\begin{equation}
\hat{\theta} = (1-\alpha) \cdot \theta_0 + \alpha \cdot \left(\frac{1}{k} \sum_{i=1}^{k} \theta_i'\right)
\end{equation}

\noindent\textbf{TIES Merge.}  
We apply TIES merging \cite{yadav2024ties}, a more sophisticated merging technique compared to linear merging. The resulting model in Speech-FT is given by:
\begin{equation}
\hat{\theta} = (1-\alpha) \cdot \theta_0 + \alpha \cdot \text{TIES}(\theta_1', \theta_2', \dots, \theta_k')
\end{equation}

\noindent\textbf{Sequential Fine-tuning.}  
We apply Speech-FT sequentially to each fine-tuning task $\hat{t}_1, \hat{t}_2, \dots, \hat{t}_k$.

\section{Experiments}
\subsection{Implementation}
All experiments are conducted on HuBERT \cite{hsu2021hubert}, unless otherwise specified.
We evaluate the generality of our method on wav2vec 2.0 \cite{baevski2020wav2vec} and DeCoAR 2.0 \cite{ling2020decoar} in Section~\ref{sec:discuss}.
Instead of pre-training these models from scratch, we use publicly available pre-trained models\footnote{\url{https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt}}\footnote{\url{https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small.pt}}\footnote{\url{https://github.com/awslabs/speech-representations/releases/download/decoar2/checkpoint_decoar2.pt}} as the starting point for fine-tuning.
HuBERT, wav2vec 2.0, and DeCoAR 2.0 contain approximately 94M, 95M, and 90M parameters, respectively.

For each fine-tuning task, we select the checkpoint with the best validation score.
For weight space interpolation, we fix the scaling factor at $\alpha=0.25$ throughout the paper and discuss its impact in Section~\ref{sec:discuss}.
For stable fine-tuning, we set $\beta\%=10\%$ across all experiments.
We conduct all experiments using a full fine-tuning setup 
rather than parameter-efficient fine-tuning.
Additional details on the fine-tuning setup can be found in Appendix~\ref{sec:detail-finetuning}.

\subsection{Evaluation}
\noindent\textbf{Datasets and tasks.}
To evaluate the generalization ability of speech representations, we use the SUPERB \cite{yang2021superb} benchmark, which assesses models across a diverse set of downstream tasks, covering content, speaker, paralinguistic, and semantic aspects.
Evaluating the entire SUPERB benchmark is computationally expensive due to its wide range of tasks. 
MiniSUPERB \cite{wang2023minisuperb} addresses this by selecting representative tasks, demonstrating that evaluation on a well-chosen subset sufficiently reflects a modelâ€™s performance.
Following this approach, we evaluate four tasks: phoneme recognition (PR, content), speaker identification (SID, speaker), emotion recognition (ER, paralinguistic), and slot filling (SF, semantic).
We report phone error rate (PER\%) for PR, accuracy (ACC\%) for SID and ER, and slot-type F1 score (F1) along with slot value CER (CER\%) for SF.
The full evaluation on SUPERB is conducted for a specific set of experiments and is reported in Appendix~\ref{sec:full-superb}.

We adhere to the default SUPERB evaluation setup, where the speech representation model remains frozen, and the weighted sum of its layer-wise features is used as input for the downstream model. 
During training, only the learnable weighted sum and the downstream model are updated. Additional details on the SUPERB evaluation setup are provided in Appendix~\ref{sec:down-train}.

\vspace{1em}
\noindent\textbf{SUPERB Score.}
To facilitate a more intuitive comparison of the generalization ability of different speech representation models, we compute the SUPERB score (denoted as $\text{SUPERB}_{\text{S}}$), following the definition in the SUPERB SLT 2022 Challenge \cite{feng2023superb}. 
This score applies linear scaling between traditional spectral features and state-of-the-art speech representations for each task. 
For better readability, the score is multiplied by 1000.
For tasks with multiple evaluation metrics, we first compute an intra-task average across metrics, followed by an inter-task average to aggregate scores across different tasks.
The SUPERB score has been widely adopted in previous studies for similar evaluations \cite{shi2023ml, shi2023multi, shi2023findings, wang2023task, huang2023ensemble, chang2024colld}.
In this paper, $\text{SUPERB}_{\text{S}}$ is computed based on four evaluation tasks: PR, SID, ER, and SF.
More details on its computation can be found in Appendix~\ref{sec:superb-score}.

\input{tables/supervised}

\vspace{1em}
\noindent\textbf{Baselines.}
Our goal is to improve pre-trained speech representation models through fine-tuning. Therefore, the most fundamental baseline is the pre-trained model itself, denoted as \textbf{Pre-trained}. 
Another key baseline is stable fine-tuning, abbreviated as \textbf{Stable-FT}.
We exclude regular fine-tuning from our baselines for the following reasons:
First, we empirically found that in some cases, regular fine-tuning fails entirely, preventing successful fine-tuning of the speech representation model.
Second, even in cases where regular fine-tuning succeeds, it consistently underperforms compared to stable fine-tuning, making stable fine-tuning a stronger and more reliable baseline.\footnote{See Table~\ref{tab:stable-regular} in Appendix.}
The baselines can also be viewed as variations of $\alpha$ in Equation~\ref{eq:interpolate}, where \textbf{Stable-FT} corresponds to \textbf{Speech-FT} with $\alpha = 1.0$, and \textbf{Pre-trained} corresponds to \textbf{Speech-FT} with $\alpha = 0.0$.

\section{Main Results}
In this section, we show the effectiveness of Speech-FT in three different scenarios: supervised, unsupervised, and multitask fine-tuning.
\label{sec:main-results}

\subsection{Supervised Fine-tuning}
We begin by fine-tuning HuBERT on various supervised tasks, including automatic speech recognition (ASR) on TED-LIUM \cite{rousseau2012ted}, phoneme classification (PC) on TIMIT \cite{garofolo1993darpa}, emotion recognition (ER) on CREMA-D \cite{cao2014crema}, speaker identification (SID) on Librispeech \cite{panayotov2015librispeech}, ASR on Librispeech, phoneme recognition (PR) on Librispeech, SID on VoxCeleb1 \cite{nagrani2020voxceleb}, and ER on IEMOCAP \cite{busso2008iemocap}.

Table~\ref{tab:supervised-finetune2} presents the SUPERB score results, where Speech-FT consistently outperforms Stable-FT across all supervised fine-tuning tasks. Additionally, Speech-FT improves performance compared to the pre-trained model for all tasks.

A detailed breakdown of the SUPERB downstream results is provided in Table~\ref{tab:supervised-finetune}. The background color highlights cases where the fine-tuning task matches the evaluation task.

Overall, Stable-FT significantly degrades performance on unrelated tasks.  
For instance, fine-tuning on SID with VoxCeleb1 causes substantial degradation in PR and SF, with PER increasing by 23.91\% and CER by 18.57\%, respectively.  
Similarly, fine-tuning on SID with Librispeech leads to performance drops in PR and SF, with PER increasing by 9.01\% and CER by 6.84\%.  
Fine-tuning on PC with TIMIT negatively impacts SID, reducing ACC by 15.52\%.  

In contrast, Speech-FT not only preserves the generalization ability of the pre-trained model but also enhances speech representations, as reflected in an overall increase in the SUPERB score.  
Notably, fine-tuning on ASR tasks with Speech-FT significantly improves performance across all evaluation tasks, particularly when fine-tuned on TED.  
Specifically, PR improves with a 1.23\% reduction in PER, SID with a 2.25\% increase in ACC, ER with a 2.79\% increase in ACC, and SF with a 1.24\% reduction in CER.  

\subsection{Unsupervised Fine-tuning}
We further evaluate the effectiveness of Speech-FT through unsupervised fine-tuning. Specifically, we continue HuBERT pre-training on AISHELL-3 \cite{shi2020aishell}, a Mandarin speech corpus. Additional details on the continuous pre-training process for HuBERT can be found in Appendix~\ref{sec:cont-pretrain-detail}.

To assess the quality of speech representations on Chinese data, we introduce Chinese ASR from the Common Voice Corpus \cite{ardila2019common} as an additional evaluation task.

The results are presented in Table~\ref{tab:continuous-chinese-unsupervised-simplify}. With Speech-FT, we successfully preserve generalization ability on English tasks while also improving performance on the Chinese ASR task.
In contrast, Stable-FT significantly reduces the generalization ability of speech representations on English tasks, resulting in a drop of 80.32 in the SUPERB score. 

\input{tables/unsupervised-simplify}
\input{tables/multitask}
\input{tables/multitask-compare-single-simplify}

\subsection{Multiple Fine-tuning Tasks}
\label{sec:multiple-finetune}
We further evaluate Speech-FT in the context of multiple fine-tuning tasks. In this study, we present preliminary results using two fine-tuning tasks: PR on Librispeech and SID on VoxCeleb1.
Recall that there are four strategies for incorporating multiple fine-tuning tasks: multitask fine-tuning (MTF), linear merge, TIES merge, and sequential fine-tuning.
For sequential fine-tuning, we follow a fixed order where PR on Librispeech is applied first, followed by SID on VoxCeleb1.

The results are shown in Table~\ref{tab:multitask}.
Overall, Stable-FT significantly degrades performance on PR, ER, and SF.  
For example, under the sequential fine-tuning strategy, PR increases by 20.55 PER\%, ER decreases by 4.96 ACC\%, and SF increases by 18.06 CER\%.  
Similarly, with the TIES merge strategy, PR increases by 9.5 PER\%, ER decreases by 2.53 ACC\%, and SF increases by 7.75 CER\%.  

In contrast, Speech-FT effectively mitigates this degradation, leading to an overall improvement in the SUPERB score compared to the pre-trained model.
Interestingly, while sequential fine-tuning with Stable-FT yields the worst performance, it becomes the best-performing strategy when combined with Speech-FT.

To further illustrate the benefits of Speech-FT in multiple fine-tuning tasks, we compare it with Speech-FT applied to a single fine-tuning task, using sequential fine-tuning as the representative strategy.  
The results, presented in Table~\ref{tab:multitask-compare-single-simplied}, show that incorporating both tasks yields a higher SUPERB score than fine-tuning on a single task.  
This confirms that Speech-FT can effectively utilize multiple fine-tuning tasks to enhance performance.  


\input{tables/otherssl}
\input{tables/regularization}


\section{Discussion}
\label{sec:discuss}

\noindent\textbf{Speech-FT is effective across different speech representation models.}
We further evaluate Speech-FT on different speech representation models, specifically wav2vec 2.0 and DeCoAR 2.0, as shown in Table~\ref{tab:other-ssl}.
Consistent with previous findings, Stable-FT proves ineffective for both wav2vec 2.0 and DeCoAR 2.0, leading to a significant drop in the SUPERB score.
In contrast, Speech-FT preserves generalization ability and results in an overall improvement in the SUPERB score.

These three speech representation models differ in their pretext tasks:
HuBERT employs a predictive task, wav2vec 2.0 uses a contrastive task, and DeCoAR 2.0 adopts a generative task \cite{mohamed2022self}.
This experiment highlights the generality of Speech-FT, demonstrating its effectiveness across different types of speech representation models.

\input{tables/ablation}
\input{tables/hyperparameter-simplify}

\vspace{1em}
\noindent\textbf{Speech-FT outperforms simple regularization.}
Regularization is a straightforward alternative to weight space interpolation, aiming to keep the fine-tuned model closer to the pre-trained model in weight space.
To compare Speech-FT with regularization, we introduce a regularization term $\lambda||\theta' - \theta_0||^2_F$ during fine-tuning instead of applying interpolation afterward.
The hyperparameter $\lambda$ is tuned among $\{10^{-2}, 10^{-4}, 10^{-6}\}$, and we report the best-performing setting.
To ensure a fair comparison, we also apply stable fine-tuning to the regularization baseline.

As shown in Table~\ref{tab:regularization}, the regularization baseline proves significantly less effective than Speech-FT, leading to a much lower SUPERB score.

\vspace{1em}
\noindent\textbf{Stable fine-tuning is a crucial component of Speech-FT}  
We conduct an ablation study to evaluate the effectiveness of stable fine-tuning in Speech-FT.  
The results, presented in Table~\ref{tab:ablation}, highlight that stable fine-tuning is a crucial component of Speech-FT for preserving generalization ability.  
Specifically, when fine-tuning with SID on VoxCeleb1, the absence of stable fine-tuning leads to performance degradation, with PR increasing by 3.27 PER\%, ER decreasing by 2.44 ACC\%, and SF increasing by 3.12 CER\%.  
Without stable fine-tuning, Speech-FT fails to achieve improvements over the pre-trained model.  
These findings emphasize the essential role of stable fine-tuning in maintaining the effectiveness of Speech-FT.  

\vspace{1em}
\noindent\textbf{Impact of hyperparameter}
Table~\ref{tab:hyperparameter-simplify} shows how speech representations evolve between the pre-trained model ($\alpha=0.0$) and the fine-tuned model ($\alpha=1.0$). 
Fine-tuning without weight space interpolation compromises the generalization ability of the speech representation. However, as $\alpha$ decreases, the SUPERB score improves gradually, reaching an optimal value at $\alpha=0.25$.

\section{Related Work}

\subsection{Model Merging}
Model merging is a technique that combines multiple models into a single model while preserving the key attributes of each original model. 
Weight space interpolation could be regarded as a kind of model merging between pre-trained model and fine-tuned model.
Several studies have investigated its applications in the speech domain. These include merging ASR models trained on different dataset distributions \cite{ramesh2024task, plantinga2024parameter}, merging TTS models to interpolate speaker attributes \cite{murata2024attribute}, merging speech translation models to expand language pairs \cite{cheng2024task}, and merging ASR models to facilitate lifelong learning \cite{kulshreshtha2024sequential}.

In contrast to these works, we propose a novel application of model merging for speech representation learning. While previous works focus on task-specific models, our method aims to improve general speech representations. 

\subsection{Out-of-distribution Generalization}
Out-of-distribution (OOD) generalization aims to preserve a model's performance on OOD datasets after fine-tuning. 

Several studies in computer vision have explored OOD generalization for vision pre-trained models that could perform zero-shot inference.
% , for example, CLIP \cite{radford2021learning}. 
For instance, Wise-FT \cite{wortsman2022robust} proposes merging the weights of pre-trained and fine-tuned models to enhance robustness on OOD datasets. 
LP-FT \cite{kumar2022fine} introduces a simple two-step approachâ€”linear probing followed by full fine-tuningâ€”to mitigate distortion in pre-trained features by first learning a near-optimal linear head during the initial steps.
Model Soup \cite{wortsman2022model} demonstrates that averaging the weights of models fine-tuned with different hyperparameter configurations can improve robustness on OOD datasets. 

The work most relevant to Speech-FT is Wise-FT, as both methods involve merging the pre-trained and fine-tuned models.  
However, our work focuses on speech representation models with the goal of preserving the generalization ability across various downstream tasks, rather than solely addressing dataset distribution. This represents a significantly different research setting.
In addition, we enhance weight space interpolation by integrating stable fine-tuning, which has been shown to be essential for preserving the generalization ability of speech representation models.

\subsection{Continuous Learning}
Continuous learning, or lifelong learning, enables models to adapt to new domain data while retaining previously acquired knowledge. It addresses the issue of catastrophic forgetting \cite{kirkpatrick2017overcoming} and is essential for handling dynamic and evolving environments. Continuous learning typically involves multiple rounds of fine-tuning to simulate real-world scenarios where models continuously learn from new data.

Several prior studies have explored continuous learning for automatic speech recognition \cite{kulshreshtha2024sequential, houston2020continual, li2022massively}.
While we also aim to preserve previously acquired knowledge, 
our goal is fundamentally different from that of continuous learning. Unlike continuous learning, our goal is to enhance the speech representation after fine-tuning. Hence, It is not necessary for us to fine-tune the model on a long sequence of tasks.

\section{Conclusion}  
In this work, we propose Speech-FT, a fine-tuning method based on model merging that enhances speech representation models while preserving their generalization ability.  
Speech-FT improves speech representations across different fine-tuning scenarios and model types.  
Moreover, Speech-FT incurs no additional computational cost compared to regular fine-tuning, making it an efficient approach for refining general speech representation after pre-training.


\section{Limitations}  
Firstly, Speech-FT demonstrates stronger performance on supervised fine-tuning tasks, while the overall improvement from unsupervised fine-tuning is less pronounced.

Next, our investigation of Speech-FT with multiple fine-tuning tasks is limited to two tasks. Expanding this study to include more tasks would provide deeper insights into its effectiveness in multitask learning.

Lastly, further exploration of the underlying mechanisms by which weight space interpolation influences speech representation could offer a valuable direction for future research.

\bibliography{custom}

\appendix

\input{tables/stable-regular}
\input{tables/regularization-full}
\input{tables/multitask-compare-single}
\input{tables/algorithm}
\input{tables/upsupervised}
\input{tables/hyperparameter}

\input{tables/baseline}
\section{SUPERB Evaluation}
\label{sec:down-train}
\subsection{Evaluation Downstream Tasks}
We select four downstream tasks from SUPERB for evaluation:  
phoneme recognition (PR) on Librispeech \cite{panayotov2015librispeech},  
speaker identification (SID) on VoxCeleb1 \cite{nagrani2020voxceleb},  
emotion recognition (ER) on IEMOCAP \cite{busso2008iemocap}, and 
slot filling (SF) on Audio SNIP \cite{lai2021semi}.  

These tasks represent four different aspects of speech: content, speaker, paralinguistic, and semantic, respectively.  
\begin{itemize}
    \item \textbf{PR}: Converts an utterance into a sequence of phonemes.  
    \item \textbf{SID}: Identifies the speaker of a given utterance by analyzing voice characteristics.  
    \item \textbf{ER}: Determines the emotional state conveyed in an utterance.  
    \item \textbf{SF}: Maps an utterance to a sequence of semantic slot types. 
\end{itemize}


\subsection{Experiment Setup}
We follow the default settings of the SUPERB benchmark for downstream model training. The speech representation models are frozen, and the weighted sum of representations from different layers is used as input to the downstream model. During training, only the downstream model and the learnable weights of the weighted sum are updated.

To ensure the stability of the weighted sum features, we apply layer normalization to the representations before computing the sum. The performance of HuBERT \cite{hsu2021hubert}, wav2vec 2.0 \cite{baevski2020wav2vec}, and DeCoAR 2.0 \cite{ling2020decoar} reported in this paper has been reproduced with this modification.

We adhere to the default number of update steps specified by the official SUPERB evaluation tool, s3prl\footnote{\url{https://github.com/s3prl/s3prl/tree/main}}, for downstream model training. 

\section{SUPERB Score}
\label{sec:superb-score}
The SUPERB score was introduced in the SUPERB SLT 2022 Challenge \cite{feng2023superb} as a unified metric for comparing different speech representation models across multiple tasks. The score is based on linear scaling between the performance of Mel filter bank (FBank) features and state-of-the-art (SOTA) speech representations for each task. To improve readability, the score is scaled by a factor of 1000.

For tasks with multiple evaluation metrics, an average is first computed across the metrics within each task. The final score is then obtained by averaging across all tasks. The SUPERB score also considers task difficultyâ€”when traditional spectral features perform close to SOTA speech representations, even small improvements are treated as more significant.

Performance for both FBank and SOTA speech representations are taken from the SUPERB's journal extension \cite{yang2024large} and are summarized in Table~\ref{tab:fbank-sota} for reference.
Note that the state-of-the-art (SOTA) performance for different tasks may come from different speech representation models.

The mathematical definition of the SUPERB score is given below. Let $\phi_{t,j}$ represent the $j$-th evaluation metric for task $t$, and let $\phi_{t,j}(f)$ denote the corresponding score of the upstream model $f$. Define $\mathcal{T}$ as the set of evaluation tasks and $\mathcal{M}_t$ as the set of metrics for task $t$. The complete formulation is as follows:

\begin{equation}
    \Phi_{t,j}(f) = \frac{\phi_{t,j}(f) - \phi_{t,j}(\text{Baseline})}{\phi_{t,j}(\text{SOTA}) - \phi_{t,j}(\text{Baseline})}
\end{equation}
\begin{equation}
\text{SUPERB}_s(f) =
\frac{1000}{|\mathcal{T}|} 
\sum_{t \in \mathcal{T}}  
\frac{1}{|\mathcal{M}_t|}
\sum_{j \in \mathcal{M}_t} \Phi_{t,j}(f).
\end{equation}
\section{Full SUPERB Evaluation Results}
\label{sec:full-superb}
\input{tables/fullsuperb}
We provide the full SUPERB evaluation results for fine-tuning HuBERT with phoneme classification (PC) on the TIMIT dataset. The results are presented in Table~\ref{tab:full-superb}.

Overall, the trend aligns with our claims in the main text: Stable-FT significantly compromises the generalization ability of speech representations, whereas Speech-FT effectively mitigates this issue.

\section{More about Fine-tuning Tasks}
\label{sec:detail-finetuning}
This section outlines the hyperparameters, objectives, datasets, task prediction models $D$, and computational budget used for different fine-tuning tasks.
All fine-tuning experiments are conducted on a single Nvidia 3090 GPU with a single run.

\subsection{Supervised Fine-tuning} 
\label{sec:detail-finetuning-supervised}

\noindent\textbf{Phoneme Classification on TIMIT.}
TIMIT \cite{garofolo1993darpa} is a widely used English dataset for phoneme classification and recognition. The objective of this task is to predict frame-level phoneme labels from speech representations. The task prediction model consists of a linear projection layer. We use cross-entropy loss as the objective function. The model is fine-tuned for 300,000 steps with an effective batch size of 16 and a learning rate of $10^{-4}$. Following prior work \cite{lopes2011phone}, we use standard training and validation splits, with the training set containing approximately 3.14 hours of data. The entire fine-tuning process takes about 12 hours.

\vspace{1em} 
\noindent\textbf{Automatic Speech Recognition on TED-LIUM.}
TED-LIUM \cite{rousseau2012ted} is a dataset of English TED talks with corresponding transcriptions. The goal of this task is to generate speech transcriptions from speech representations. The task prediction model is a three-layer LSTM. We use CTC loss as the objective function. The model is fine-tuned for 50,000 steps with an effective batch size of 32 and a learning rate of $10^{-4}$. To ensure data quality, we filter out samples containing the \textsc{<unk>} token in the training set and use a randomly selected 100-hour subset for training. The entire fine-tuning process takes approximately 12 hours.

\vspace{1em} 
\noindent\textbf{Emotion Recognition on CREMA-D.}
CREMA-D \cite{cao2014crema} is an audio-visual emotional dataset consisting of 7,442 English speech clips from 91 actors. In this task, we use only the speech-based emotional annotations, aiming to predict emotion labels from speech representations. The task prediction model consists of a linear projection layer, with mean pooling applied along the time axis before feature input. We use cross-entropy loss as the objective function. The model is fine-tuned for 15,510 steps with an effective batch size of 32 and a learning rate of $10^{-4}$. Data preprocessing follows the EMO-SUPERB setup \cite{wu2024emo}, with the training set containing approximately 6.5 hours of speech. The entire fine-tuning process takes around 5 hours.

\vspace{1em} 
\noindent\textbf{Speaker Identification on Librispeech.}
Librispeech \cite{panayotov2015librispeech} is a corpus of approximately 1,000 hours of read English speech. For this task, we use a 10-hour subset, where the goal is to classify speaker identities from mean-pooled speech representations. The task prediction model consists of a linear projection layer. We use cross-entropy loss as the objective function. The model is fine-tuned for 100,000 steps with an effective batch size of 32 and a learning rate of $2 \cdot 10^{-4}$. Prior work \cite{oord2018representation} used a 72-hour subset for speaker identification; we further randomly sample from this subset to create a 10-hour version while preserving the same number of speakers. The entire fine-tuning process takes approximately 15 hours.

\vspace{1em} 
\noindent\textbf{Automatic Speech Recognition on Librispeech.}
This task focuses on generating speech transcriptions using the Librispeech dataset. The task prediction model consists of a single-layer LSTM. We use CTC loss as the objective function. The model is fine-tuned for 200,000 steps with an effective batch size of 32 and a learning rate of $10^{-4}$. We use the 100-hour subset of Librispeech for training and the dev-clean subset for validation. The entire fine-tuning process takes approximately 48 hours.

\vspace{1em}
\noindent\textbf{Phoneme Recognition on Librispeech.}  
This task involves predicting phoneme sequences from speech representations using the Librispeech dataset. The task prediction model consists of a linear projection layer. We use CTC loss as the objective function. The model is fine-tuned for 100,000 steps with an effective batch size of 32 and a learning rate of $10^{-4}$. Training is conducted on the 100-hour subset of Librispeech, with the dev-clean subset used for validation.  
The entire fine-tuning process takes approximately 15 hours.

\vspace{1em}
\noindent\textbf{Speaker Identification on VoxCeleb1.}  
This task involves classifying speaker identities from mean-pooled speech representations using the VoxCeleb1 dataset \cite{nagrani2020voxceleb}. The task prediction model consists of a linear projection layer. We use cross-entropy loss as the objective function. The model is fine-tuned for 200,000 steps with an effective batch size of 32 and a learning rate of $10^{-4}$. The training set contains approximately 318 hours of speech data.  
The entire fine-tuning process takes approximately 30 hours.

\vspace{1em}
\noindent\textbf{Emotion Recognition on IEMOCAP.}  
This task involves predicting the emotion category of each utterance using the IEMOCAP dataset \cite{busso2008iemocap}. The task prediction model consists of a linear projection layer. We use cross-entropy loss as the objective function. The model is fine-tuned for 30,000 steps with an effective batch size of 32 and a learning rate of $10^{-4}$. IEMOCAP is divided into five sections (Section1 to Section5). We train the model using Sections 2 to 5 and validate on Section1. The training set contains approximately 5.56 hours of speech data.  
The entire fine-tuning process takes approximately 5 hours.


\subsection{Unsupervised Fine-tuning}
For unsupervised fine-tuning, we continuously pre-train HuBERT on AISHELL-3 \cite{shi2020aishell}, a Mandarin speech corpus published by Beijing Shell Shell Technology Co.,Ltd. 
We continuously pre-train for 100,000 steps with an effective batch size of 32 and a learning rate of $10^{-5}$.
Since the data in AISHELL-3 has a sampling rate of 44100, we resample it to 16000 to match the sampling rate of HuBERT.
The training dataset of AISHELL-3 consist of approximately 63.17 hours of data.
The entire fine-tuning process takes approximately 24 hours.

\subsection{Multiple Fine-tuning Tasks}
As described in Section~\ref{sec:multiple-task}, several strategies are available to integrate information from different fine-tuning tasks. For the Linear, TIES Merge, and Sequential methods, we follow the hyperparameter setup outlined in Section~\ref{sec:detail-finetuning-supervised}.

Unlike these three methods, the MTF method optimizes a multitask objective during fine-tuning. We fine-tune the model for 200,000 steps with an effective batch size of 4, allocating 2 for phoneme recognition (PR) on Librispeech and 2 for speaker identification (SID) on VoxCeleb1. The learning rate is set to $10^{-5}$.
The entire fine-tuning process takes approximately 30 hours.

\section{Continuous Pre-training of HuBERT Base}  
\label{sec:cont-pretrain-detail}  

\subsection{Generating Pre-training Targets}  
Since the k-means models used to generate the targets for HuBERT Base have never been publicly released (only the k-means model for generating the targets of HuBERT Large has been made available\footnote{\url{https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960_L9_km500.bin}}), we follow the approach in prior work \cite{lin2024daisy}, generating pre-training targets by running HuBERT Base and extracting the $\arg\max$ from its predictions as pseudo targets. While this method may not be optimal, it serves as an effective alternative to k-means clustering.

We extract a sequence of pseudo targets by forwarding the input through HuBERT, obtaining a sequence of speech representations $\mathbf{H} = [h_1, \dots, h_T] \in \mathbb{R}^{T \times D_h}$, where $h_t$ is the hidden state at time step $t$, $T$ is the sequence length, and $D_h$ is the hidden state dimension. These representations are then passed through HuBERTâ€™s final linear layer to produce an embedding sequence:
\begin{equation}
    \mathbf{E} = \mathbf{H}\mathbf{W} \in \mathbb{R}^{T \times D_c}
\end{equation}
where $\mathbf{W} \in \mathbb{R}^{D_h \times D_c}$ is the weight matrix of HuBERTâ€™s final linear layer, and $D_c$ is the codebook embedding dimension.

Let $\mathbf{C} \in \mathbb{R}^{C \times D_c}$ be the HuBERT codebook, where $C$ is the number of codebook entries. The prediction $\mathbf{P} \in \mathbb{R}^{T \times C}$ is computed as:
\begin{equation}
    \mathbf{P} = \mathbf{E} \cdot \mathbf{C}^\top
\end{equation}

The pseudo targets $\mathbf{y} = [y_1, \dots, y_T]$ are then obtained by applying the $\arg\max$ function along the codebook dimension:
\begin{equation}
    y_t = \arg\max_{c} \mathbf{P}_{tc}, \quad \forall t \in \{1, \dots, T\}
\end{equation}

Although the dataset used for continuous pre-training is a Mandarin speech corpus, we found that this method still generates a reasonable sequence of pseudo targets without significant cluster imbalance issues.

\subsection{Optimization Objective}  
We adopt a simplified variant of the HuBERT loss as our continuous pre-training objective, which is defined as a cross-entropy loss between the pseudo targets $y_t$ and the output of a linear classifier at masked time stamps. Specifically, we train a new linear classifier $\mathbf{W}_c \in \mathbb{R}^{D_h \times C}$ using the following objective:
\begin{equation}
    \mathcal{L} = \frac{1}{|M|} \sum_{t\in M} \text{CE}(\mathbf{W}_c^\top h_t, y_t)
\end{equation}

Here, $M$ represents the set of masked time steps, $h_t \in \mathbb{R}^{D_h}$ denotes the hidden state at time step $t$, and $y_t$ is the corresponding pseudo target. The function $\text{CE}(\cdot, \cdot)$ refers to the cross-entropy loss.
We follow the masking configuration of the original HuBERT.

This variant of the HuBERT loss has been shown to be as effective as the original version \cite{lin2024daisy}, while also being more computationally efficient \cite{yang2023fast}. Furthermore, optimization without a learnable codebook is relatively stable, making it a suitable choice for continuous pre-training.

\section{More about Scientific Artifacts}
\subsection{License and Terms of Use}
Our work is intended solely for research purposes.
This section outlines the licensing terms of the fine-tuning datasets and pre-trained models used in our study.  
We adhere to the intended uses of these scientific resources. The licensing information is provided below:

\vspace{1em}
\noindent\textbf{Fine-tuning Datasets}
\begin{itemize}
    \item \textbf{TED-LIUM}: Released under the CC BY-NC-ND 3.0 license. 
    
    \item \textbf{TIMIT}: A proprietary dataset distributed by the Linguistic Data Consortium (LDC). Access requires compliance with the LDC User Agreement, which involve membership or a fee-based license. It is intended for research purposes only.

    \item \textbf{CREMA-D}: Licensed under the Open Database License (ODbL) v1.0. 

    \item \textbf{LibriSpeech}: Distributed under the CC BY 4.0 license.

    \item \textbf{VoxCeleb1}: Available under the CC BY 4.0 license.

    \item \textbf{IEMOCAP}: Governed by a custom licensing agreement\footnote{\url{https://sail.usc.edu/iemocap/Data_Release_Form_IEMOCAP.pdf}}. Users must comply with the terms set by the \textit{Speech Analysis and Interpretation Laboratory} at the University of Southern California. The dataset is intended for internal research purposes only.

    \item \textbf{AISHELL-3}: Distributed under the Apache 2.0 license.

\end{itemize}

\vspace{1em}
\noindent\textbf{Pre-trained Models}
\begin{itemize}
    \item \textbf{HuBERT}: Released under the MIT License.
    \item \textbf{Wav2vec 2.0}: Released under the MIT License.
    \item \textbf{DeCoAR 2.0}: Distributed under the Apache 2.0 license.
\end{itemize}

\subsection{Personally Identifying Info or Offensive Content}
For all datasets, we remove any name-related information. However, for speaker identification tasks, removing speaker identity is not feasible. Nonetheless, these datasets (LibriSpeech, VoxCeleb1) are protected under their respective licenses. Additionally, all the datasets we use do not contain any offensive content.

\subsection{Dataset Documentation}
We provide documentation on the datasets used in our experiments, detailing their languages and domains.

\begin{itemize}
    \item \textbf{Languages:} The datasets include English (e.g., LibriSpeech, TIMIT, VoxCeleb1, IEMOCAP, TED-LIUM, CREMA-D) and Mandarin Chinese (AISHELL-3).  
    \item \textbf{Domains:} The datasets span various types of speech, such as read speech (LibriSpeech, TIMIT), talks (TED-LIUM), and spontaneous speech (VoxCeleb1). 
\end{itemize}


\end{document}
