\begin{table}[h!]
\centering
\scalebox{1.0}{
\begin{tabular}{l | c | c}
\toprule
\multirow{3}{*}{\textbf{Method}} & \textbf{English} & \textbf{Chinese} \\
& \multirow{2}{*}{\textbf{$\text{SUPERB}_{\text{S}}\uparrow$}} & \textbf{zh-CN ASR} \\
\cmidrule(lr){3-3}
& & \textbf{CER\%$\downarrow$} \\
\cmidrule(lr){1-3}
Speech-FT  & 866.51 & 24.23 \\
Stable-FT  & 789.88 & 23.47 \\
\midrule
\midrule
Pre-trained & 870.20 & 24.94 \\
\bottomrule
\end{tabular}
}
\caption{Results of Speech-FT on the unsupervised fine-tuning task, where HuBERT is continuously pre-trained on the Chinese speech corpus AISHELL-3 \cite{shi2020aishell}. See Table~\ref{tab:continuous-chinese-unsupervised} in Appendix for detailed performance on English evaluation tasks.}
\label{tab:continuous-chinese-unsupervised-simplify}
\end{table}
