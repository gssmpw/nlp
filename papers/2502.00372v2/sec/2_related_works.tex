\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{image/pipeline.pdf}
\vspace{-6.5mm}
\caption{\textbf{Pipeline of \methodname{}.} \methodname{} is organized as a \emph{deterministic finite-state automaton} (\dfa) with five states: (1) \emph{Perception}, which is the initial state where relevant visual information is extracted by identifying entity categories and localizing entities; (2) \emph{Logic Generation}, where an LLM uses the caption and query to generate logic expressions in ProbLog, incorporating entities, and ProbLog query. The relations and attributes required by the query are recognized by the foundation models and converted to  ProbLog code; (3) \emph{Logic Reasoning}, where ProbLog code is executed to identify the target candidates based on explicit probabilistic logic; (4) \emph{Answering}, where the identified optimal target candidate is validated by a VLM to check if it fits the query; and (5) \emph{Return Target}, which is the final state only reached if the check succeeds. The \dfa\ structure (left) coordinates the flow across these stages, with conditional transitions that enable a self-correcting mechanism (\textcolor{red}{red arrows}) to revisit previous states if refinement is needed, ensuring the robustness and interpretability in the final response. Note that the automaton is deterministic, as the inputs that govern all transitions departing each state are mutually exclusive. 
}
\label{fig:pipeline}
\vspace{-4.5mm}
\end{figure*}

\section{Related Works}
\label{sec:related_works}

% \simin{VG :
% Visual Grounding (VG) involves the task of identifying visual elements that correspond to a given textual query, specifically requiring the determination of bounding boxes around these relevant visual components.~\cite{zhu2016visual7w}.}
Recent advances in LLMs~\cite{chowdhery_palm_2023,bai_constitutional_2022,ouyang_training_2022,radford_language_2019} and VLMs~\cite{li_otter_2023,wu_next-gpt_2023,stanic_towards_2024,zhu_minigpt-4_2023} have raised expectations for improved visual grounding in Referring Expression Detection~\cite{kazemzadeh_referitgame_2014} and Referring Expression Segmentation~\cite{liu_gres_2023} tasks. The Referring Expression Detection involves identifying objects in images based on natural language descriptions, while Referring Expression Segmentation goes further by providing precise pixel-level segmentation of those objects. These approaches can be categorized into two main streams: \emph{end-to-end} and \emph{compositional methods}.

\noindent\textbf{End-to-End Methods.}
% Models like NExT-GPT~\cite{wu2023next} and Video-LLaMA~\cite{zhang_video-llama_2023} excel in RED tasks and  analyzing complex 
% videos, effectively combining text, images, videos, and audio to improve cross-modal reasoning.
% % and pinpointing visual elements that correspond to textual queries. 
% Further more, Otter~\cite{li_otter_2023}, Flamingo~\cite{alayrac2022flamingo}, and Visual ChatGPT~\cite{wu2023visual} enhancing their ability to ground visual information in text and respond accurately to visual queries. Projects such as InstructBLIP~\cite{instructblip}, M$^{3}$IT~\cite{li2023m}, and VisionLLM~\cite{wang2023visionllm} emphasize the value of instruction tuning, multilingual datasets, and vision-centric tasks.
% % strengthening their capacity to handle both language and visual cues for more accurate grounding of visual elements in multimodal contexts.
% % Segment Anything~\cite{kirillov2023segment} introduces a task in image segmentation that is prompt-based, where the objective is to generate a valid segmentation mask using an input prompt, such as a point or a bounding box that specifies the object of interest.
% % In Referring Expression Segmentation task,
% % SAM~\cite{kirillov2023segment} is a primary segmentation model trained on SA-1B, supporting zero-shot segmentation. Recent improvements, such as HQ-SAM~\cite{ke2024segment}, enhance mask quality, while others focus on efficiency~\cite{xiong2024efficientsam, zhang2023faster, zhao2023fast}. Methods like such as X-Decoder~\cite{zou2023generalized}  leverage both non-semantic and semantic queries to learn shared representations and perform well in zero-shot settings.
% %  SEEM~\cite{zou2024segment}  is a versatile model that supports efficient, generalizable segmentation using minimal supervision and dynamic prompt composition.
% % LISA~\cite{lai2024lisa} introduces a segmentation token to consolidate sentence information and prompt a SAM decoder for mask prediction. u-LLaVA~\cite{xu2023u} extends LISA's object grounding support, while NExT-Chat~\cite{zhang2023next} handles more complex inputs like bounding boxes.
% % However, LISA is limited to handling only one object at a time, prompting the development of subsequent methods that address multi-object scenarios, including GLaMM~\cite{rasheed2024glamm}, PerceptionGPT~\cite{pi2024perceptiongpt}, PixelLM~\cite{ren2024pixellm}, GSVA~\cite{xia2024gsva}, and LISA++ ~\cite{yang2023improved}.
% % PSALM~\cite{zhang2025psalm} is an extension of the Large Multi-modal Model (LMM) for image segmentation. It uses a mask decoder and advanced input schema to handle various tasks, achieving strong results  and demonstrating zero-shot capabilities. \\
% In the RES task, SAM~\cite{kirillov2023segment} is a primary zero-shot segmentation model trained on SA-1B, with improvements like HQ-SAM~\cite{ke2024segment} for mask quality and others focusing on efficiency~\cite{xiong2024efficientsam, zhang2023faster, zhao2023fast}. Models like X-Decoder~\cite{zou2023generalized} use both semantic and non-semantic queries for shared representation learning in zero-shot tasks. SEEM~\cite{zou2024segment} enables efficient, generalizable segmentation with minimal supervision, while LISA~\cite{lai2024lisa} introduces segmentation tokens. u-LLaVA~\cite{xu2023u} and NExT-Chat~\cite{zhang2023next} expand LISA's object grounding support. GLaMM~\cite{rasheed2024glamm}, PerceptionGPT~\cite{pi2024perceptiongpt}, PixelLM~\cite{ren2024pixellm}, GSVA~\cite{xia2024gsva}, and LISA++\cite{yang2023improved} address multi-object scenarios. PSALM\cite{zhang2025psalm} extends LMM for segmentation with zero-shot capabilities.\\
% However, end-to-end models have key limitations, including low interpretability and high computational demands due to their reliance on extensive datasets and large-scale neural networks. These models struggle with generalization and analytical tasks, such as geometry and relationships, resulting in implicit and hard-to-verify reasoning~\cite{chen2022unigeo}. For complex vision challenges, customizable, context-aware solutions are often more effective than purely data-intensive, compute-heavy models, which highlights the shortcomings of monolithic approaches~\cite{villalobos2022will, yang2023mm}. \\
End-to-end methods in visual grounding aim to leverage neural networks to directly map text queries to objects in an image, without explicitly separating the stages of grounding and reasoning.
Early models mainly use transformer-based architectures trained on extensive datasets to achieve generalized language and vision understanding~\cite{heigold_video_2023, minderer_scaling_2023, li_grounded_2022, liu_grounding_2023, cheng_yolo-world_2024}.  With the recent advances in LLMs~\cite{openai_gpt-4o_2024, dubeyLlama2024, brown_language_2020} and VLMs~\cite{li_otter_2023, alayrac_flamingo_2022, dai_instructblip_2023, wang_visionllm_2023, wu_next-gpt_2023, liu_visual_2023, chen_minigpt-v2_2023}, these architectures have further enhanced performance in visual grounding tasks~\cite{peng_kosmos-2_2023, lai_lisa_2024, rasheed_glamm_2024, pi_perceptiongpt_2024, xia_gsva_2024}.
% Models like NExT-GPT~\cite{wu_next-gpt_2023} and Video-LLaMA~\cite{zhang_video-llama_2023} excel in complex RED tasks, integrating text, images, videos, and audio for improved cross-modal reasoning. Otter~\cite{li_otter_2023}, Flamingo~\cite{alayrac_flamingo_2022}, and Visual ChatGPT~\cite{wu_visual_2023} enhance visual grounding, while InstructBLIP~\cite{dai_instructblip_2023}, M$^{3}$IT~\cite{li_m3it_2023}, and VisionLLM~\cite{wang_visionllm_2023} focus on instruction tuning and vision-centric tasks. In RES, SAM~\cite{kirillov_segment_2023} is a key zero-shot model, with improvements like HQ-SAM~\cite{ke_segment_2023} and X-Decoder~\cite{zou_generalized_2023} for efficient, generalizable results. SEEM~\cite{zou_segment_2023} and LISA~\cite{lai_lisa_2024} introduce dynamic prompts, with u-LLaVA~\cite{xu_u-llava_2024} and NExT-Chat~\cite{zhang_next-chat_2023} expanding grounding support. Multi-object models like GLaMM~\cite{rasheed_glamm_2024} and PerceptionGPT~\cite{pi_perceptiongpt_2024} address complex scenarios. PSALM~\cite{zhang_psalm_2024} extends LMM with zero-shot segmentation. 
However, end-to-end methods still face limitations, including low interpretability and high computational cost due to the demands of training neural networks on large-scale datasets. Their data-driven nature often limits their capacity for generalization and analytical tasks, particularly those involving geometry and relational reasoning, which require a more context-aware approach~\cite{chen_unigeo_2022, villalobos_will_2024, gupta_visual_2023}.

\noindent\textbf{Compositional Visual Reasoning Methods.}
The compositional strategy offers a solution to the difficulties encountered by end-to-end methods~\cite{suris_vipergpt_2023,you_idealgpt_2023,lu_chameleon_2023,gupta_visual_2023,stanic_towards_2024}. Compositional methods address complex problems by dividing them into subtasks, solving each task independently, and then using intermediate results to solve the initial problem. By utilizing the powerful chain-of-thought (CoT) capabilities of LLMs, these methods can effectively simplify complicated challenges into smaller, solvable steps through specific instructions~\cite{gupta_visual_2023,suris_vipergpt_2023}. These instructions may include execution code that includes logical operations. 
VisProg~\cite{gupta_visual_2023} and ViperGPT~\cite{suris_vipergpt_2023} reduce task-specific training by using code generation LLMs, integrating VLMs into the program to generate final output. HYDRA~\cite{ke_hydra_2024} combines a planner, RL agent, and reasoner for reasoning, outperforming other methods. It uses predefined instructions to activate perception tools and combine data for reasoning.
% For instance, VisProg~\cite{gupta_visual_2023} and ViperGPT~\cite{suris_vipergpt_2023} aim to reduce the dependency on task-specific training in programming logic and perceptual components by utilizing code generation models. This approach allows for the integration of VLMs into subroutines, facilitating the generation of outputs.
% Another approach, reflecting the divide-and-conquer paradigm, is demonstrated by IdealGPT~\cite{you_idealgpt_2023}. This method uses a captioning model for visual data and an LLM for planning. High-level queries are split into three sub-questions, processed in parallel with VFMs, and the results analyzed to form the final response~\cite{lu2023chameleon}. HYDRA~\cite{ke2024hydra} is a multi-stage method that combines a planner, an RL agent, and a reasoner for reliable reasoning. The planner generates instructions, the reasoner executes them, and the RL agent selects the best instruction based on feedback. It outperforms other models in VR tasks across multiple datasets. The system follows these predefined instructions to activate the perception tools in a specific order, ultimately leading to the consolidation of data, which is then evaluated by the reasoning mechanism to reach the final conclusion.\\
Despite their strengths, these methods have notable limitations. They heavily rely on LLMs for reasoning, which may not fully capture complex relations like physics or geometry. Additionally, logic representations in natural language lead to ambiguity, making it hard to extract relevant information. Lastly, these systems lack self-correction mechanisms, allowing errors to propagate through the inference steps. Addressing these issues with formulated explicit logic representations and robust reasoning could enhance performance and interpretability.
% To address these challenges we propose \methodname{}, a compositional visual grounding method that integrates logical reasoning with LLMs to enhance interpretability and reasoning. Using ProbLog to store state representations, the method manages complex queries in a structured way. \methodname{} also includes a self-correction mechanism to identify and fix errors at each step, preventing their impact on later stages. By combining symbolic logic with neural perception, the system handles contextual constraints in natural language queries, improving transparency and trust through interpretable inferences.
To address these issues, we propose \methodname{}, a method that improves visual reasoning by integrating symbolic logic, probabilistic logic, and a self-correcting deterministic finite-state automaton. A more detailed description of our method is provided in \autoref{sec:method}.
% We propose \methodname{}, a compositional visual grounding method that combines logical reasoning with LLMs for improved interpretability. Using ProbLog for state representations, it handles complex queries and features a self-correction mechanism. Details of our model are provided below:

