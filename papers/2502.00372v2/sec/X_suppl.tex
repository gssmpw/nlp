\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\noindent This supplementary material provides additional details about the proposed method \methodname{}. In particular, it provides details about 
% the comparison with compositional baselines in referring expression segmentation in \autoref{sec:supp_segmentation}, 
% the ablation studies for Answerer in \autoref{sec:supp_answerer}, 
the additional quantitative comparison result with both accuracy and IoU in \autoref{sec:supp_quantitative},
the ablation studies for VLM in \autoref{sec:supp_vlm}, 
the analysis of self-correction mechanisms in \autoref{sec:supp_self_correction}, 
the performance analysis for query length in \autoref{sec:supp_query_length},
and the prompts used for the LLMs and VLMs in \autoref{sec:supp_prompt}.

% \section{Comparison with Compositional Baselines}
% \label{sec:supp_segmentation}

% We compare \methodname{} with the compositional baselines  HYDRA~\cite{ke_hydra_2024} and ViperGPT~\cite{suris_vipergpt_2023}, on the referring expression segmentation task using the RefCOCOg and RefCLEF~\cite{kazemzadeh_referitgame_2014} datasets. The results are shown in \autoref{tab:segmentation3} and \autoref{tab:segmentation4}. Both \methodname{} and the baselines are evaluated in two configurations: (†) using the same VFMs (BLIP2~\cite{li_blip-2_2023}, GLIP~\cite{li_grounded_2022}) as in the original implementations, and (*) using SoTA VFMs (InternVL2~\cite{chen_internvl_2024}, GLaMM~\cite{rasheed_glamm_2024}). Across both datasets and configurations, \methodname{} demonstrates superior performance compared to the baselines. Notably, while replacing the VFMs in HYDRA and ViperGPT with more advanced models yields some performance improvement, the margins are relatively smaller compared to the improvements observed with \methodname{}. This suggests that \methodname{} can utilize the capacity of the SoTA VFMs better. These results re-emphasize the superiority of \methodname{} compared to the compositional baselines.

% \section{Ablation Studies for Answerer}
% \label{sec:supp_answerer}

% We evaluate the impact of the \emph{Answering} state in \methodname{} by assessing its role when explicit logic reasoning is either enabled or disabled. The results, presented in \autoref{tab:ablation_answerer}, demonstrate the significant contribution of the \emph{Answerer} state to the system's performance.
% When explicit logic reasoning is disabled (\ding{55}), \emph{Answerer} validates each entity to determine if it satisfies the query. Incorporating  \emph{Answerer} improves the IoU scores from 67.2 to 80.8 on RefCOCO and from 70.3 to 76.9 on RefCOCO+.
% When explicit logic reasoning is enabled (\ding{51}),  \emph{Answerer} is used to validate the reasoned target candidates. This further increases performance, improving IoU scores from 70.8 to 83.3 on RefCOCO and from 73.1 to 78.4 on RefCOCO+. \emph{Answerer} addresses potential errors occurring from \emph{Perception} or \emph{Logic Generation} by ensuring the final output aligns with the query.
% These findings highlight the importance of \emph{Answerer} in refining predictions and enhancing robustness. By validating the outputs,  \emph{Answerer} significantly improves performance, regardless of whether explicit logic reasoning is employed.


% \begin{table}[t]
% \centering
% \scalebox{1}{
% \begin{tabular}{c|l|cc}
% \toprule[0.4mm]
% \rowcolor{mygray} \textbf{} & \textbf{} & \multicolumn{2}{c}{\textbf{RefCOCOg}} \\ 
% \rowcolor{mygray} \textbf{} & \textbf{Method} & \textbf{Val} & \textbf{Test} 
% \\ \hline \hline
% \multirow{6}{*}{\rotatebox[origin=c]{90}{Compositional}} & ViperGPT†~\cite{suris_vipergpt_2023} & 46.9 & 44.1 \\
% & HYDRA†~\cite{ke_hydra_2024} & 51.9 & 53.1 \\
% & \methodname{}† & \textbf{54.4} & \textbf{54.6}\\ \cline{2-4}
% & ViperGPT* & 50.9 & 44.8 \\
% & HYDRA* & 53.2 & 54.0 \\
% & \methodname{}* & \textbf{76.4} & \textbf{76.0} \\
% \bottomrule[0.4mm]
% \end{tabular}}
% \caption{\textbf{Extra quantitative comparison (IoU) on Referring Expression Segmentation task on RefCOCOg val and test set~\cite{kazemzadeh_referitgame_2014}.} We compare  \methodname{} with two compositional baselines using different VFMs. Each method is evaluated in two versions: (†) uses the same VFMs as the original HYDRA and ViperGPT for a fair comparison; (*) uses the SoTA VFMs for best performance. All use GPT-4o Mini as the LLMs.}
% \label{tab:segmentation3}
% \end{table}

% \begin{table}[t]
% \centering
% \scalebox{1}{
% \begin{tabular}{c|l|cc}
% \toprule[0.4mm]
% \rowcolor{mygray} \textbf{} &  & \multicolumn{2}{c}{\textbf{RefCLEF}} \\ 
% \rowcolor{mygray} \textbf{} & \textbf{Method} & \textbf{Test A} & \textbf{Test B} \\ \hline \hline
% \multirow{6}{*}{\rotatebox[origin=c]{90}{Compositional}} & ViperGPT†~\cite{suris_vipergpt_2023} & 55.6 & 56.9 \\
% & HYDRA†~\cite{ke_hydra_2024} & 56.4 & 58.1 \\
% & \methodname{}† & \textbf{58.1} & \textbf{59.3} \\ \cline{2-4}
% & ViperGPT* & 61.2 & 58.3 \\
% & HYDRA* & 56.5 & 58.3 \\
% & \methodname{}* & \textbf{68.8} & \textbf{67.4} \\
% \bottomrule[0.4mm]
% \end{tabular}}

% \caption{\textbf{Quantitative comparison (IoU) on Referring Expression Segmentation task on RefCLEF~\cite{kazemzadeh_referitgame_2014}.} The evaluation configuration is the same as in \autoref{tab:segmentation3}.}
% \label{tab:segmentation4}
% \end{table}

% \begin{table}[t]
% \centering
% \scalebox{1}{
% \begin{tabular}{cc|cc}
% \toprule[0.4mm]
% \rowcolor{mygray} \textbf{Answering} & \textbf{Logic} & \textbf{RefCOCO} & \textbf{RefCOCO+} \\\hline \hline
% \ding{55} & \ding{55} & 67.2 & 70.3 \\
% \ding{51} & \ding{55} & 80.8 & 76.9 \\ \hline
% \ding{55} & \ding{51} & 70.8 & 73.1 \\
% \ding{51} & \ding{51} & \textbf{83.3} & \textbf{78.4} \\
% \bottomrule[0.4mm]
% \end{tabular}}
% \caption{\textbf{Contribution of components of \methodname{}.} Results are based on the RefCOCO (IoU) and RefCOCO+ (IoU) test A set.}
% \label{tab:ablation_answerer}
% \end{table}

\begin{table*}[t]
\centering
\scalebox{0.83}{
\begin{tabular}{c|l|cc|cc|cc|cc}
\toprule[0.4mm]
\rowcolor{mygray} \textbf{} & & \multicolumn{2}{c|}{\textbf{RefCOCO}} & \multicolumn{2}{c|}{\textbf{RefCOCO+}} & \multicolumn{2}{c|}{\textbf{RefCOCOg}} & \multicolumn{2}{c}{\textbf{Ref-Adv}} \\ 
\rowcolor{mygray} \textbf{} & \textbf{Method} & \textbf{Acc.} & \textbf{IoU} & \textbf{Acc.} & \textbf{IoU} & \textbf{Acc.} & \textbf{IoU} & \textbf{Acc.} & \textbf{IoU} \\\hline \hline
\multirow{9}{*}{\rotatebox[origin=c]{90}{E2E}} & GLIP-L~\cite{li_grounded_2022} & 55.0 & 54.1 & 51.1 & 51.3 & 54.6 & 54.8 & 55.7 & 55.2 \\
& KOSMOS-2~\cite{peng_kosmos-2_2023} & 57.4 & - & 50.7 & - & 61.7 & - & - & - \\ 
& YOLO-World-X~\cite{cheng_yolo-world_2024} & 12.1 & 12.7 & 12.1 & 12.7 & 32.9 & 33.8 & 32.2 & 34.2 \\
& YOLO-Wrold-V2-X~\cite{cheng_yolo-world_2024} & 19.8 & 20.0 & 16.8 & 17.2 & 36.5 & 37.3 & 33.1 & 34.8 \\
& GroundingDINO-T~\cite{liu_grounding_2023} & 61.6 & 60.2 & 59.7 & 58.9 & 60.6 & 59.7 & 60.5 & 59.8 \\
& GroundingDINO-B~\cite{liu_grounding_2023} & 90.8 & 85.2 & 84.6 & 77.5 & 80.3 & 69.4 & 78.0 & 73.1 \\
& SimVG~\cite{dai_simvg_2024} & 94.9 & 86.9 & 91.0 & 83.9 & 88.9 & 81.3 & 74.4 & 70.7 \\
& Florence2-B~\cite{xiao_florence-2_2024} & 94.5 & 89.5 & 91.2 & 86.5 & 88.3 & 85.0 & 72.2 & 71.9 \\
& Florence2-L~\cite{xiao_florence-2_2024} & 95.1 & 90.6 & 92.5 & 88.2 & 90.9 & 87.6 & 71.8 & 71.8 \\ \hline
\multirow{10}{*}{\rotatebox[origin=c]{90}{Compositional}} & Code-bison~\cite{stanic_towards_2024} & 44.4 & - & 38.2 & - & - & - & - & - \\
& ViperGPT†~\cite{suris_vipergpt_2023} & 62.6 & 59.4 & 62.3 & 58.7 & 67.2 & 63.6 & 60.7 & 58.6 \\
& HYDRA†~\cite{ke_hydra_2024} & 60.9 & 58.2 & 56.5 & 54.9 & 62.9 & 60.8 & 54.4 & 53.5 \\
& \methodname{}† & \textbf{70.1} & \textbf{67.9} & \textbf{64.1} & \textbf{63.8} & \textbf{69.5} &  \textbf{59.2} & \textbf{65.1} & \textbf{63.4} \\ \cline{2-10}
& ViperGPT‡ & 67.1 & 64.0 & 73.2 & 68.8 & 65.6 & 63.5 & 60.1 & 59.5 \\
& HYDRA‡ & 63.5 & 61.3 & 62.9 & 60.9 & 59.8 & 58.4 & 53.2 & 53.5 \\
& \methodname{}‡ & \textbf{91.7} & \textbf{83.3} & \textbf{82.4} & \textbf{78.4} & \textbf{75.9} & \textbf{72.5} & \textbf{87.3} & \textbf{74.2} \\ \cline{2-10}
& ViperGPT* & 68.6 & 66.5 & 73.8 & 70.4 & 68.7 & 66.8 & 58.2 & 58.3 \\
& HYDRA* & 65.7 & 64.6 & 66.2 & 65.3 & 59.9 & 60.5 & 48.3 & 51.8 \\
& \methodname{}* & \textbf{96.2} & \textbf{91.7} & \textbf{92.8} & \textbf{88.4} & \textbf{91.6} & \textbf{88.1} & \textbf{75.4} & \textbf{75.3} \\
\bottomrule[0.4mm]
\end{tabular}}
\caption{\textbf{Accuracy and IoU performance on the referring expression detection task.} Results are shown on the RefCOCO, RefCOCO+, RefCOCOg~\cite{kazemzadeh_referitgame_2014}, and Ref-Adv~\cite{akula_words_2020} datasets. In compositional methods, they are grouped with the same VLMs for fair comparison. The methods with same symbols (†, ‡, *) use the same VFMs. The VFMs used are: (†) uses GLIP-L and BLIP; (‡) uses GroundingDINO-B and InternVL2; (*) uses Florence2-L and InternVL2, respectively. All groups use GPT-4o Mini.}
\label{tab:detection_supp}
\end{table*}

\section{Additional Quantitative Comparison}
\label{sec:supp_quantitative}

In this section, we report both the acurracy and Intersection over Union (IoU) performance for the referring expression detection task. \autoref{tab:detection_supp} presents a quantitative comparison on the RefCOCO, RefCOCO+, RefCOCOg~\cite{kazemzadeh_referitgame_2014}, and Ref-Adv~\cite{akula_words_2020} datasets. Additionally, we include results for the compositional methods using GroundingDINO as the VFM, marked with ‡. We observed that for each configuration, our proposed method \methodname{} outperforms the other compositional methods using the same foundation models and also outperform the grounding methods themselves. These results further validate that our approach consistently achieves superior IoU performance over existing baselines.

\section{Ablation Studies for VLMs}
\label{sec:supp_vlm}

We evaluate the impact of four different Vision-Language Models (VLMs)~\cite{li_blip-2_2023, xue_xgen-mm_2024, liu_improved_2023, chen_internvl_2024} on the performance of \methodname{}. The results are shown in \autoref{tab:ablation_vlm}. All tested models achieve comparable results on both RefCOCO and RefCOCO+. 
Among the four VLMs, InternVL2-8B achieves slightly better performance with accuracy scores of 96.2 on RefCOCO and 92.8 on RefCOCO+. However, the differences between the VLMs are marginal, indicating that the architecture of \methodname{} is insensitive to the choice of VLM. This shows \methodname{}'s ability to decompose complex tasks into simpler subtasks, reducing the reliance on the capabilities of the VLMs.
Given the similar performance across all tested VLMs, we select InternVL2-8B for its availability and accessibility, ensuring simpler implementation without compromising performance. This demonstrates the flexibility of \methodname{} in adopting different VLMs while delivering state-of-the-art performance.

% \begin{table}[t]
% \centering
% \scalebox{1}{
% \begin{tabular}{l|cc}
% \toprule[0.4mm]
% \rowcolor{mygray} \textbf{VLM} & \textbf{RefCOCO} & \textbf{RefCOCO+} \\\hline \hline
% BLIP2-XXL~\cite{li_blip-2_2023} & 83.0 & 78.3\\
% xGen-MM~\cite{xue_xgen-mm_2024} & 82.3 & 77.5 \\
% LLaVA1.5-7B~\cite{liu_improved_2023} & 82.9 & 78.4 \\
% InternVL2-8B~\cite{chen_internvl_2024} & \textbf{83.3} & \textbf{78.4} \\
% \bottomrule[0.4mm]
% \end{tabular}}
% \caption{\textbf{Comparison of VLM selection for \methodname{}.} All results are evaluated on the RefCOCO and RefCOCO+ test A set.}
% \label{tab:ablation_vlm}
% \end{table}

\begin{figure*}[t]
\centering
\includegraphics[width=0.7\linewidth]{image/error.pdf}
\caption{\textbf{Distribution of self-correction times in \methodname{}.} The x-axis represents the number of retries for self-correction, while the y-axis shows the proportion of samples requiring self-correction within each group of self-correction times. The annotated values indicate the cumulative proportion of samples resolved by that number of retries.}
\label{fig:error}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.7\linewidth]{image/acc_vs_query_length.pdf}
\caption{\textbf{Accuracy performance of different text query length in \methodname{}.} The x-axis represents the length of text query, while the y-axis shows the accuracy of \methodname{} and baselines within each group of query length.}
\label{fig:query_length}
\end{figure*}



\begin{table}[t]
\centering
\scalebox{1}{
\begin{tabular}{l|cc}
\toprule[0.4mm]
\rowcolor{mygray} \textbf{VLM} & \textbf{RefCOCO} & \textbf{RefCOCO+} \\\hline \hline
BLIP2-XXL~\cite{li_blip-2_2023} & 95.4 & 91.7 \\
xGen-MM~\cite{xue_xgen-mm_2024} & 94.5 & 87.9 \\
LLaVA1.5-7B~\cite{liu_improved_2023} & 95.5 & 91.9 \\
InternVL2-8B~\cite{chen_internvl_2024} & \textbf{96.2} & \textbf{92.8} \\
\bottomrule[0.4mm]
\end{tabular}}
\caption{\textbf{Comparison of VLM selection for \methodname{}.} All results (accuracy) are evaluated on the RefCOCO and RefCOCO+ testA datasets~\cite{kazemzadeh_referitgame_2014}.}
\label{tab:ablation_vlm}
\end{table}

\section{Self-Correction Analysis}
\label{sec:supp_self_correction}

We analyze the performance of the self-correction mechanism in \methodname{}, focusing on its ability to address errors during inference. In this mechanism, each time the system transitions into the self-correction state (indicated by a \textcolor{red}{red arrow} in \autoref{fig:pipeline}), it is counted as one retry. Our experiments, conducted on the RefCOCO test A dataset, show that approximately 10\% of the samples require self-correction.
To better understand the behavior of the mechanism, we calculate the proportion of samples resolved after each number of retries within the subset of data requiring self-correction. These results are visualized as a bar chart in \autoref{fig:error}. The analysis shows that a single retry resolves 68\% of the errors, and 96\% of the errors are resolved within six retries.
To prevent infinite looping, we limit the maximum number of retries to six, as this threshold effectively addresses the majority of the errors.

\section{Query Length Analysis}
\label{sec:supp_query_length}

We analyze the impact of text query length on performance, comparing \methodname{} with baselines across different query lengths. The results, shown in \autoref{fig:query_length}, indicate that \methodname{} consistently reaches SoTA performance comparing to all baselines regardless of query length. While most baseline methods experience a performance decline as query length increases, the extent of decrease varies. Notably, older models like GLIP~\cite{li_grounded_2022} show a significant performance drop for longer queries, suggesting difficulties in handling complex text inputs. In contrast, \methodname{} maintains stable performance, showing its robustness in processing longer and more complex queries.

% \section{Qualitative Analysis}

\section{LLM and VLM Prompts}
\label{sec:supp_prompt}

\methodname{} employs LLMs and VLMs in five different roles as described in \autoref{sec:method}: as a caption generator VLM in the \emph{Perception} state, an entity extractor LLM in the \emph{Perception} state, a logic query generator LLM in the \emph{Logic Generation} state, a relation recognizer VLM in the \emph{Logic Generation} state, and an answerer in the \emph{Answering} state. The prompts utilized for each role are carefully crafted to generate precise outputs aligned with the requirements of each state, ensuring accurate visual grounding and robust reasoning. Prompts~\hyperref[prompt:caption]{10.1} to \hyperref[prompt:answerer]{10.5} show the detailed prompt templates for each role.

\begin{itemize}
    \item Prompt~\hyperref[prompt:caption]{10.1}: This prompt instructs the VLM to generate descriptive captions $I_c$ for the input image $I$. The generated captions represent the visual content in textual form, which serves as input for the entity extractor LLM in subsequent processing.

    \item Prompt~\hyperref[prompt:entity_extractor]{10.2}: This prompt is utilized in the \emph{Perception} state to guide the LLM in identifying entity categories $C$ relevant to the query $Q$ and the caption $I_c$. The output is a list of entity categories, which forms the basis for visual reasoning in the subsequent states.

    \item Prompt~\hyperref[prompt:logic_query]{10.3}: This prompt guides the logic query generator LLM to convert the textual query $Q$ into a ProbLog logic query in the \emph{Logic Generation} state. The output logic query is then utilized in the ProbLog interpreter to perform probabilistic logic reasoning.

    \item Prompt~\hyperref[prompt:relation]{10.4}: This prompt guides the relation recognizer VLM to identify relations $R$ between entities $E$ in the \emph{Logic Generation} state. These recognized relations are used to construct logic expressions, as the foundation for probabilistic logic reasoning.

    \item Prompt~\hyperref[prompt:answerer]{10.5}: This prompt is used in the \emph{Answering} state to guide the answerer VLM in validating whether the identified target $Y_{L1}$ satisfies the conditions of the query $Q$. It requests a binary response (``Yes'' or ``No'') to confirm whether the top-ranked candidate from the \emph{Logic Reasoning} state fulfills all query requirements, ensuring only valid results are returned as final outputs.
\end{itemize}

\begin{prompt}[title={Prompt \thetcbcounter: Captioner VLM}] \label{prompt:caption}
\textlangle image\textrangle Please describe the image in detail.
\end{prompt}

\begin{prompt}[title={Prompt \thetcbcounter: Entity Extractor LLM}] \label{prompt:entity_extractor}
You're an AI assistant designed to find detailed information from image.
\\ \\
You need to find important objects based on the given query which is the object you need to find. The query normally is a set of words which includes a object name and the attributes of the object.
\\ \\
Here are some examples:\\
Query: \textlangle example query 1\textrangle \\
Answer: \textlangle example answer 1\textrangle \\

Query: \textlangle example query 2\textrangle \\
Answer: \textlangle example answer 2\textrangle \\
...

Your output must be a JSON object contains the flatten list of string. For example: {"output": ["apple", "orange", "chair", "umbrella"]}\\

Caption: \textlangle caption\textrangle \\
Query: \textlangle query\textrangle \\
Answer: 
\end{prompt}


\begin{prompt}[title={Prompt \thetcbcounter: Logic Query Generator LLM}] \label{prompt:logic_query}
You're an AI assistant designed to generate the ProbLog code (a logic programming language similar to Prolog). \\

You need to generate a new rule "target" that will be used to query the target objects in the image based on given text prompt. \\

The names of entity categories are \textlangle entity categories\textrangle. \\

The output is the code. For example: \\
\textasciigrave\textasciigrave\textasciigrave problog \\
target(ID) :- entity(ID, "<some category>", \_, \_, \_, \_), relation(ID, \_, \_), attribute(ID, \_). \\
\textasciigrave\textasciigrave\textasciigrave \\

More examples: \\
find the target "\textlangle example query 1\textrangle"\\
\textasciigrave\textasciigrave\textasciigrave problog\\
\textlangle example ProbLog code 1\textrangle\\
\textasciigrave\textasciigrave\textasciigrave\\
\\
find the target "\textlangle example query 2\textrangle"\\
\textasciigrave\textasciigrave\textasciigrave problog\\
\textlangle example ProbLog code 2\textrangle\\
\textasciigrave\textasciigrave\textasciigrave\\
...\\

Complete the following ProbLog code:\\
\textasciigrave\textasciigrave\textasciigrave problog\\
\textlangle ProbLog code of context\textrangle\\
\textasciigrave\textasciigrave\textasciigrave \\

Your output should be the ProbLog code.

find the target "\textlangle query\textrangle"\\
Your answer: 
\end{prompt}


\begin{prompt}[title={Prompt \thetcbcounter: Relation Recognizer VLM}] \label{prompt:relation}
\textlangle image\textrangle You're an AI assistant designed to find the relations of objects in the given image.\\

The interested objects are highlighted by bounding boxes (X1, Y1, X2, Y2). They are:\\

A: the \textlangle category of A\textrangle labeled by red bounding box \textlangle bbox of A\textrangle.\\
B: the \textlangle category of B\textrangle labeled by red bounding box \textlangle bbox of B\textrangle.\\

Only consider the camera view. Note you are focusing to analyze the relation A to B, do not consider the relation B to A. Please answer "Yes" or "No" for the following question.\\

Is A \textlangle relation\textrangle{} B?\\

Your answer is:
\end{prompt}


\begin{prompt}[title={Prompt \thetcbcounter: Answerer VLM}] \label{prompt:answerer}
\textlangle image\textrangle You're an image analyst designed to check if the highlighted object in the image meets the query description.\\

The query is: "\textlangle query\textrangle"\\

Please check the highlighted object "A" in the image and answer the question: Does the highlighted object meet the query description? Your answer should be "Yes" or "No".\\
    
Your answer:
\end{prompt}