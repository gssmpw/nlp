% \section{Introduction}
% \label{sec:intro}
% Bullet point
% # Introduction
% ## Background
% - Visual Reasoning is a challenging task and important and popular in the field.
% - Normally includes VG and VQA.
% - It requires reasoning ability to solve the complex problem
% - For example: "A person in the left, wearing a red shirt, standing in front of a car." requires the understanding and reasoning ability to solve the problem.
% ## Gap of E2E previous works
% - Previous works are mainly focused on the end-to-end model, which is hard to interpret and debug.
% - Some cite of E2E works.
% - It is hard to understand the model's reasoning and decision making process.
% - From previous works \cite{}, the neural-only system is not effective for visual reasoning task.
% ## Gap of composition works
% - Compositional works overcome the gap of E2E methods, but still have some limitations.
% - Some cite of compositional works.
% - Limitations:
%     - No explicit logic to reason.
%     - The flow of componenets is fixed.
% ## Our approach
% - We proposed our method includes logic-based representation with the dynamic reasoning with symbolic state machine based controller.
% - We solve the issue by these 2 novel contributions.
% - We show the effectiveness of our method by comparing with the previous works.
% # Related Works
% ## Visual Grounding methods
% - More cite of E2E works for visual grounding task.
% - More cite of some E2E works works on the visual grounding requiring reasoning.
% ## Compositional methods
% - More cite of compositional works.
% - More cite of some compositional works works on the visual reasoning task.
% ## Our methods
% - To overcome the limitations of the previous works, we proposed our method.
% # Method
% - The overview of the task formulation.
% - The logic representation of visual info.
% - The logic reasoning of the visual info.
% - Integrated by a symbolic state machine based controller.
% - The other details about how to composite the VFMs and each state in the system.
% # Experiment
% % # Conclusion
% \begin{table*}[htb]
% \centering
% % \vspace{-1em}
% \caption{Summary of compositional models, including . \textit{IR: Incremental Reasoning. Referring Segmentation (RS). VG: Visual Grounding. HF: HuggingFace. Automaton: self-correction mechanism}.}
% \label{tab:performance}
% {\fontsize{8}{10}\selectfont % <-- Adjust the numbers to change font size
% \begin{tabular}{lcccccccc}
% \toprule
% Model & \multicolumn{6}{c}{Module} & \multicolumn{2}{c}{Task} \\
% \cmidrule(lr){2-7} \cmidrule(lr){8-9} 
%  & Planner & Perception & Reasoner & Controller & IR& Automaton & RS & VG\\
% Visprog~\cite{gupta2023visual} & \ding{55} & VFMs & GPT-3 & \ding{55} & \ding{55} & \ding{55}& \ding{55} &\ding{51} \\
% Chameleon~\cite{lu2023chameleon} & ChatGPT & VFMs & ChatGPT & \ding{55} & \ding{55}& \ding{55} & \ding{55} & \ding{55}\\
% IdealGPT~\cite{you2023idealgpt} & ChatGPT & BLIP2 & ChatGPT & \ding{55}& \ding{55}& \ding{55} & \ding{55} & \ding{55} \\
% HuggingGPT~\cite{shen2023hugginggpt} & ChatGPT & HF-VFMs & ChatGPT & \ding{55} & \ding{55}& \ding{55} & \ding{55} & \ding{51} \\
% ViperGPT~\cite{suris2023vipergpt} & \ding{55} & VFMs & Codex & \ding{55} & \ding{55}& \ding{55} & \ding{55} & \ding{51}  \\ 
% HYDRA~\cite{ke2024hydra} & ChatGPT & VFMs & ChatGPT & RL-Agent & \ding{51}& \ding{55} & \ding{55} & \ding{51} \\
% \hline
% \bf{\methodname{}{}} & ChatGPT & VFMs & ChatGPT & \ding{51} & \ding{51} & \ding{51} & \ding{51}& \ding{51} \\
% \bottomrule
% \end{tabular}}
% \end{table*}
\vspace{-5mm}
\section{Introduction}
% In this paper, we explore a level of Visual Grounding (VG) that requires reasoning capability, moving beyond basic perception toward a sophisticated understanding of context, relationships, and implications within the visual scene
% Visual Grounding (VG) as a fundamental task in many applications, refers to the process of pinpointing visual elements that align with a specific textual query, which entails identifying bounding boxes around these pertinent visual features ~\cite{zhu2016visual7w}.
% Visual Grounding (VG)~\cite{yang2022improving} and Referring Segmentation (RS) ~\cite{kirillov2023segment} are two essential tasks within visual reasoning that span varying levels of complexity.
% \pjs{Fix the differentiation between VG and referring segmentation}
% , highlighting the challenge of accurately linking visual elements with contextual information.
Grounding referring expressions in image tasks, including detection and segmentation~\cite{kazemzadeh_referitgame_2014}, are fundamental to visual reasoning and cover a wide range of complexities~\cite{yang_cross-modal_2019}.
This paper explores a level of visual grounding that emphasizes reasoning and goes beyond basic perception~\cite{yu_modeling_2016}. 
This is challenging for current models because it requires interpreting complex queries, creating layered visual representations, and performing step-by-step reasoning for coherent interpretations and contextually appropriate inferences, closely resembling human cognitive functions~\cite{amizadeh_neuro-symbolic_2020}.
% This task is particularly challenging as it involves interpreting complex textual queries, constructing a layered representation of the visual scene—including objects, attributes, relationships, and more—and performing human-like, step-by-step reasoning to form coherent interpretations and make contextually appropriate inferences~\cite{amizadeh2020neuro}.
% Visual Grounding (VG) \cite{yu_modeling_2016} is a fundamental task in visual reasoning, focused on linking language expressions to specific regions within an image.
% It stands alongside other essential tasks like Visual Question Answering (VQA) \cite{amizadeh2020neuro}, which aims to answer questions based on image content, and Visual Commonsense Reasoning (VCR) \cite{zellers2019recognition}, which requires models to draw on commonsense knowledge to reason about visual scenes. 
For example (See~\autoref{fig:teaser}), the query ``find a person on the left wearing a blue shirt in front of a car'' requires the method to process multiple components simultaneously. This includes detecting entities like ``person'', ``car'' and others, recognizing attributes such as ``wearing blue shirt'' and identifying the relations like ``on the left'' and ``in front of'' between these entities to generate a contextually grounded understanding of the visual content. %while also interact%\pjs{Dont follow: interacting with the entitys/determining interactions betweeen th entities?} 
%between these entities to generate a contextually grounded interpretation of the visual content.%~\cite{yang_graph-structured_2020}.
% For example (See \autoref{fig:teaser}), the query ``a person on the left wearing a blue shirt in front of a car'' requires the model to process several components simultaneously: identifying the relation (``on the left''), linking it to the entity category (``person''), recognizing the attribute (``wearing a blue shirt''), and understanding the context of ``in front of a car'', which involves objects and their relations~\cite{yang_graph-structured_2020}.

Recent advances in large language models (LLMs)~\cite{chowdhery_palm_2023,bai_constitutional_2022,ouyang_training_2022,radford_language_2019} and their derivatives,
Vision-Language Models (VLMs)~\cite{li_otter_2023,wu_next-gpt_2023,stanic_towards_2024,zhu_minigpt-4_2023} have increased expectations for better visual grounding through enhanced reasoning. These developments can be categorized into two main types: (1) end-to-end (monolithic) methods that integrate various tasks into a unified model, and (2) compositional methods that leverage modular components to improve flexibility and interpretability in inference.
End-to-end methods demonstrate promising results, but often struggle with complex reasoning tasks such as geometry relations~\cite{chen_unigeo_2022}, and their reasoning capabilities are implicit and thus difficult to verify. The challenge may be due to missing explicit reasoning structures, or over-reliance on end-to-end learning, or the difficulty in disentangling reasoning components.
% their need for large datasets and substantial computational resources, as they use neural networks to directly predict target regions. 
% In contrast to prevailing belief
% However, these models often encounter challenges with complex reasoning tasks, as they are typically trained on fixed datasets, and their reasoning abilities are verifiable and implicit. For example, these models may struggle to solve geometry and relationship problems~\cite{}. 
In contrast, compositional approaches are more effective in reasoning-based visual grounding challenges, as they decompose complex tasks into simpler components using a divide-and-conquer strategy.
They integrate LLMs as planners, code generators, or reasoners, with Vision Foundation Models (VFMs) or Vision Language Model (VLM) for visual perception, enabling structured analysis and task-specific planning to enhance adaptability and generalization across scenarios ~\cite{gupta_visual_2023, suris_vipergpt_2023, lu_chameleon_2023, wu_visual_2023}. Thus, the reasoning of these models is interpretable and verifiable, providing a clear framework for decision-making.
% They combine LLMs with Visual Foundation Models (VFMs),
% % without extensive training
% using LLMs as planners, code generators, or reasoners, while VFMs are used for visual perception. This integration facilitates structured analysis and customized plan generation, enhancing adaptability and generalization across various scenarios ~\cite{gupta2023visual, suris2023vipergpt, lu2023chameleon, wu2023visual}. Thus, the reasoning of these models is interpretable and verifiable, providing a clear framework for decision-making.
% For instance, ViperGPT~\cite{suris_vipergpt_2023} generates code programs for visual queries but relies on a single feed-forward process without iterative refinement. In contrast, HYDRA~\cite{ke_hydra_2024} enhances this approach by introducing iterative and incremental reasoning through an iterative loop.
% however, it still lacks explicit logical reasoning capabilities.
However, current compositional methods have several limitations.
First, they primarily rely on the commonsense knowledge encoded in LLMs for planning and reasoning, which may not fully capture the complex relations, \eg physics-based and geometric relations, needed for comprehensive understanding in complex queries~\cite{chen_spatialvlm_2024}.
%\textcolor{red}{Hamid: I remove this controversial sentence ``In other words, 
% contrary to prevailing belief, 
%their reasoning ability is relatively naive, often oversimplifying complex scenarios and overlooking key nuances in reasoning.''}
Second, logic is represented in natural language, which lacks the necessary structure for effective reasoning, resulting in ambiguous interpretations and difficulties in extracting relevant information, limiting their ability to perform accurate and context-aware reasoning. 
Finally, if any mistakes or failures occur at any step in the process, they may accumulate throughout inference, as these systems lack an automatic self-correction mechanism.

In this paper, we introduce a novel compositional visual grounding method named \emph{Neuro-symbolic compositional Automaton for Visual grounding with Explicit logic Reasoning} (\methodname{}) that addresses the limitation of existing compositional frameworks by \emph{(i)} integrating explicit probabilistic logic reasoning using ProbLog~\cite{de_raedt_problog_2007} within a compositional pipeline to enhance visual reasoning capabilities, and \emph{(ii)} incorporating a flexible, deterministic finite-state automaton (\dfa)\footnote{For simplicity, ``automaton'' refers to ``deterministic finite-state automaton'' in this paper.} with a self-correction mechanism.
Our method combines symbolic logic with large language models (LLMs) and neural-based vision foundation models, providing an explainable and logic inference for complex queries. By integrating symbolic reasoning within a \dfa-based pipeline, \methodname{} dynamically navigates between states based on intermediate results and specific conditions, handling complex contextual constraints and relations in reasoning-based grounding. In addition, \methodname{} has been designed so that each step and module includes a self-correction mechanism, ensuring that mistakes occurring in one step or module, do not affect others. This combination not only improves the system's ability to handle complex queries but also provides transparency and interpretability, thus increasing trust and facilitating error analysis. We conducted extensive experiments on popular datasets, showing that \methodname{} achieves state-of-the-art (SoTA) performance, comparing with SoTA end-to-end~\cite{li_grounded_2022, liu_grounding_2023, peng_kosmos-2_2023, dai_simvg_2024, xiao_florence-2_2024} and compositional~\cite{stanic_towards_2024, suris_vipergpt_2023, ke_hydra_2024} methods. These results are supported by detailed ablation studies validating the effectiveness of each component.
% We conducted extensive experiments on popular datasets~\cite{Yu_Poirson_Yang_Berg_Berg_2016}, comparing \methodname{} with state-of-the-art (SoTA) end-to-end and compositional methods. 
% % Our quantitative and qualitative analyses demonstrate that \methodname{} achieves SoTA performance.
% Our results demonstrate that \methodname{} achieves SoTA performance.
% We also performed ablation studies to analyze the contribution of each component in our framework, confirming the effectiveness of our design choices. 
In summary, the key contributions of this work are as follows: 
\begin{enumerate} 
    \item We propose \methodname{}, a novel compositional visual grounding method that leverages explicit logic representation for complex reasoning tasks.
    \item We design a flexible, deterministic finite-state automaton-based system that dynamically transitions between states based on intermediate results, incorporating a self-correction mechanism at each step to improve robustness and adaptability.
    % \item We conduct extensive experiments on standard benchmarks, showing that \methodname{} achieves SoTA performance. These results are supported by detailed ablation studies validating the effectiveness of each component.
    \item We perform comprehensive experiments on standard benchmarks, demonstrating that \methodname{} achieves state-of-the-art performance, with additional ablation studies to validate the effectiveness of each component. 
\end{enumerate}

% \simin{Visual reasoning refer to understand visual information and employ high-level cognition for logical problem-solving ~\cite{johnson2017clevr,zakari2022vqa,malkinski2023review}. 
% Visual reasoning involves a range of tasks, such as Grounding (VG)~\cite{yu_modeling_2016}, Visual Question Answering Visual  (VQA)\cite{amizadeh2020neuro}, and Visual Commonsense Reasoning (VCR)\cite{zellers2019recognition}, among others.
% Recent advancements in large language models (LLMs) ~\cite{chowdhery2022palm,bai2022constitutional,ouyang2022training,radford2019language} an in its branches like VLMs ~\cite{li_otter_2023,wu2023next,stanic_towards_2024,zhu_minigpt-4_2023} have made visual reasoning a prominent area of research, revealing both potential and challenges in this domain. Visual reasoning tasks, such as those involving Raven's Progressive Matrices, require multi-hop relational reasoning, which current models struggle to perform effectively ~\cite{yizhe_zhang__2024}. Even large models face difficulties with compositional reasoning and fine-grained tasks, indicating a gap in their ability to generalize across different contexts ~\cite{aleks_2024}. It is a challenging problem because it requires understanding as well as performing reasoning over semantics-rich referring expressions and diverse visual contents including objects, attributes and relations~\cite{yang2020graph}.\\
% While these models have shown promising results in certain tasks like VQA and VCR~\cite{zhang2023video}, their training as single monolithic end-to-end models necessitates large-scale datasets, imposing significant computational resource requirements. Additionally, while these models excel within their training domain, they may require further adaptation to achieve reliable performance when applied to diverse datasets or domains~\cite{you2023idealgpt,suris2023vipergpt,stanic_towards_2024}.\\
% Recent advancements have seen the emergence of compositional approaches \cite{gupta2023visual, suris2023vipergpt, lu2023chameleon, wu2023visual} as effective strategies for tackling VR challenges. These methods decompose complex tasks into simpler sub-components, utilizing a divide-and-conquer strategy. They integrate LLMs with Visual Foundation Models (VFMs) without the need for extensive training. LLMs can serve as planners, code generators, or reasoners, while VFMs provide visual perception capabilities, enabling structured analysis and generating task-specific plans to enhance adaptability and improve generalization across various scenarios. A notable state-of-the-art compositional model is ViperGPT~\cite{suris2023vipergpt}, which leverages LLMs to create code programs for visual queries and address tasks in a single feed-forward process. IdealGPT~\cite{you2023idealgpt} proposed an improved framework that uses LLMs as both questioners and reasoners, with a pre-trained Vision-Language Model (VLM) acting as the answerer, as shown in Figure~\ref{fig}. In this framework, the LLM breaks down main questions into sub-questions, with the reasoner evaluating whether further sub-question generation is necessary through iterations or if the final output has been achieved.\\
% Despite their advantages, these models face notable limitations. For instance, the outputs produced by LLMs can sometimes be devoid of meaningful context. When these outputs progress to later stages without proper verification, they may influence the results of other components negatively, leading to a decline in overall performance. Additionally, LLMs employed in the planner or questioner during the initial phase do not take into account the visual content from the perception module in subsequent stages, which limits their ability to refine their outputs \cite{gupta2023visual, hu2023visual}. Furthermore, the generation of follow-up questions often begins anew, disregarding information from prior steps, which can result in a greater number of iterations. Lastly, these approaches significantly depend on the commonsense knowledge embedded in LLMs for effective planning and reasoning in VR tasks.\\
% In this paper, we present ...,
% }


% \subsection{Background}

% \begin{itemize}
%     \item Define Visual reasoning. Become challenging and popular in the field using LLM.
%     \item Visual Reasoning is a challenging task and important and popular in the field.
%     \item Normally includes VG and VQA.
%     \item It requires reasoning ability to solve the complex problem.
%     \item For example: ``A person on the left, wearing a red shirt, standing in front of a car.'' requires the understanding and reasoning ability to solve the problem.
% \end{itemize}

% \subsection{Gap of End-to-End Previous Works}

% \begin{itemize}
%     \item Previous works are mainly focused on the end-to-end model, which is hard to generalize, interpret and debug. % check HYDRA paper, why intepretable is important. 
%     \item Some cite of E2E works.
%     \item It is hard to understand the model's reasoning and decision-making process.
%     \item From previous works \citep{}, the neural-only system is not effective for visual reasoning tasks.
% \end{itemize}

% \subsection{Gap of Composition Works}

% \begin{itemize}
%     \item Compositional works overcome the gap of E2E methods, but still have some limitations.
%     \item Some cite of compositional works.
%     \item Limitations:
%     \begin{itemize}
%         \item The representation is fully text. The reasoning is replied on LLM. No explicit logic to reason.
%         \item The flow of components is fixed, highly engineered.
%     \end{itemize}
% \end{itemize}

% \subsection{Our Approach}

% \begin{itemize}
%     \item We proposed our method includes logic-based representation with dynamic reasoning with symbolic state machine-based controller.
%     \item We solve the issue by these two novel contributions.
%     \item We show the effectiveness of our method by comparing with the previous works.
% \end{itemize}

% \section{Related Works}

% \subsection{Visual Grounding Methods}

% \begin{itemize}
%     \item More cite of E2E works for visual grounding task.
%     \item More cite of some E2E works on the visual grounding requiring reasoning.
% \end{itemize}

% \subsection{Compositional Methods}

% \begin{itemize}
%     \item More cite of compositional works.
%     \item More cite of some compositional works on the visual reasoning task.
% \end{itemize}

% \subsection{Our Methods}

% \begin{itemize}
%     \item To overcome the limitations of the previous works, we proposed our method.
% \end{itemize}

% \section{Method}

% \begin{itemize}
%     \item The overview of the task formulation.
%     \item The logic representation of visual information.
%     \item The logic reasoning of the visual information.
%     \item Integrated by a symbolic state machine-based controller.
%     \item The other details about how to composite the VFMs and each state in the system.
% \end{itemize}

% \section{Experiment}

% \section{Conclusion}

% % Background of VG -> narrow down from Reasoning to VG

% The rapid advancement of artificial intelligence and robotics has heightened the demand for sophisticated recognition and perception systems capable of localizing objects that require complex reasoning~\cite{}. Visual Grounding (VG) is a fundamental task in this domain, aiming to localize objects in images based on natural language queries~\cite{yu_modeling_2016}. By leveraging open-set vocabulary, VG methods can detect objects described by unseen textual inputs, providing greater flexibility and adaptability.

% Recent years have witnessed significant progress in end-to-end (E2E) visual grounding models~\cite{}, achieving impressive performance. These models utilize neural networks to directly predict target regions without explicit reasoning processes. While effective in many scenarios, these E2E neural-only architectures often struggle with complex contextual constraints and relationships expressed in queries, such as \emph{``the tall person wearing blue clothes behind a chair''}. The absence of explicit reasoning leads to unreliable inference processes and a lack of interpretability, making it challenging to understand the rationale behind their successes or failures.

% To address these limitations, recent works~\cite{suris_vipergpt_2023, you_idealgpt_2023, ke_hydra_2024} in visual reasoning tasks, including Visual Question Answering (VQA) and VG, have explored compositional approaches that combine symbolic reasoning with neural modules. These neuro-symbolic methods decompose complex problems into simpler sub-questions using a divide-and-conquer strategy, enabling more effective handling of intricate queries. By integrating Large Language Models (LLMs) as planners or reasoners and Visual Foundation Models (VFMs) for perception, these approaches aim to enhance adaptability and generalization across diverse scenarios.

% However, existing compositional methods present certain challenges. Primarily, they rely heavily on the commonsense knowledge encoded in LLMs for planning and reasoning, which may not fully capture the intricate logical structures required for complex queries. Methods like ViperGPT~\cite{suris_vipergpt_2023} generate code programs for visual queries but rely on a single feed-forward process without iterative refinement. HYDRA~\cite{ke_hydra_2024} improves upon this by introducing iterative and incremental reasoning using an iterative loop but still lacks explicit logical reasoning capabilities. 

% In this paper, we take a step further by introducing \methodname{}, a novel framework that integrates explicit logical reasoning with LLMs to enhance reasoning capabilities in VG tasks. Our method employs logical reasoning to interpret complex queries, providing a highly explainable and logical inference process. By combining symbolic logic with neural perception modules, \methodname{} effectively handles complex contextual constraints and relationships in natural language queries.

% Specifically, \methodname{} ... [PLACEHOLDER FOR DETAILS]

% This combination not only improves the system's ability to handle complex queries but also provides transparency and interpretability. Human can understand how the system arrived at a particular conclusion, increasing trust and facilitating error analysis.

% We conducted extensive experiments on several popular VG and referring segmentation datasets~\cite{}, comparing \methodname{} with state-of-the-art (SoTA) baselines. Our quantitative and qualitative analyses demonstrate that \methodname{} achieves superior performance. We also performed ablation studies to analyze the contribution of each component in our framework, confirming the effectiveness of our design choices. In summary, the key contributions of this work are as follows: 
% \begin{enumerate} 
%     \item We propose \methodname{}, a novel framework that utilizes explicit logical reasoning combined with LLMs to perform highly explainable and logical reasoning in visual grounding tasks.
%     \item We introduce a new concept in compositional approaches by employing a hyper controller that organizes sub-controllers in an automaton-based system, enhancing the system's reasoning capabilities and adaptability.
%     \item We conduct extensive experiments on standard benchmarks, demonstrating that \methodname{} achieves SoTA performance. We provide detailed ablation studies to validate the effectiveness of each component in our method.
% \end{enumerate}

