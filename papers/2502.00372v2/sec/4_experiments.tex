\section{Experiments and Results}

\subsection{Implementation Details}

We implement the method using PyTorch~\cite{paszke_pytorch_2019} with Nvidia L40s GPUs. The vision foundation models and LLMs used in \methodname{} are flexible and can be exchanged for any other methods of the same type. Specifically, we use GPT-4o Mini~\cite{openai_gpt-4o_2024} for all LLMs in the system and other compositional methods~\cite{ke_hydra_2024, suris_vipergpt_2023}. We use InternVL2-8B~\cite{chen_internvl_2024} as the VLM captioner in the \emph{Perception} state and as answerer in the \emph{Answering} state, while XVLM~\cite{zeng_multi-grained_2022} is used as attribute recognizer in the \emph{Logic Generation} state. We also use DepthAnythingV2~\cite{yang_depth_2024} as depth estimator and Florence2-L~\cite{liu_grounding_2023} as entity detector. For converting the bounding boxes to segmentation masks, SAM~\cite{kirillov_segment_2023} is used.
For segmentation tasks, we utilize GLaMM~\cite{rasheed_glamm_2024} for SoTA performance. For fair comparison with the previous compositional baselines~\cite{ke_hydra_2024, suris_vipergpt_2023}, we evaluated two versions of \methodname{}: (1) using the same foundation models (including GLIP~\cite{li_grounded_2022}) as HYDRA and ViperGPT, (2) using  the SoTA foundation models for best performance. We discuss the choice and the performance analysis of using different LLMs and grounding methods in \autoref{sec:quantitative} and \autoref{sec:ablation}. 
For probabilistic logic reasoning, we used Scallop~\cite{li_scallop_2023} to enhance efficiency. The initial logic code is generated in ProbLog, then automatically translated into Scallop, a Rust-based probabilistic logic programming language optimized for high-performance inference. This translation maintains all probabilistic logic expressions while enabling faster reasoning execution than traditional ProbLog implementations, especially when dealing with large logic programs. 
We implement the previous baselines~\cite{ke_hydra_2024, suris_vipergpt_2023, li_grounded_2022, liu_grounding_2023, cheng_yolo-world_2024, kirillov_segment_2023, rasheed_glamm_2024, dai_simvg_2024, xiao_florence-2_2024} using the official source code, and borrow the results of other baselines from \cite{ke_hydra_2024, lai_lisa_2024, zhang_psalm_2024}. More details of the implementations will be elaborated in the supplementary material. 

\subsection{Task, Dataset and Metric}
This work focuses on referring expression detection and segmentation as the two visual grounding tasks that require reasoning with object attributes and relations among multiple elements within an image.  For detection, we evaluate on RefCOCO test A, RefCOCO+ test A, RefCOCOg~\cite{kazemzadeh_referitgame_2014} test and Ref-Adv~\cite{akula_words_2020} test datasets, using the most popular and widely-used metrics, \ie accuracy~\cite{li_grounded_2022,liu_grounding_2023,dai_simvg_2024,xiao_florence-2_2024} and intersection over union (IoU)~\cite{suris_vipergpt_2023, ke_hydra_2024} . For the segmentation, we follow the evaluation protocol in previous works~\cite{rasheed_glamm_2024, zhang_psalm_2024} on the RefCOCOg and RefCLEF datasets~\cite{kazemzadeh_referitgame_2014} with IoU as metric. The IoU in segmentation\footnote{In some previous works~\cite{lai_lisa_2024, zhang_psalm_2024, rasheed_glamm_2024}, it is termed as ``cIoU".} is defined as the number of intersection pixels divide by the union pixels accumulated in the test set. In addition, we compare the efficiency of \methodname{} with other compositional baselines~\cite{ke_hydra_2024, suris_vipergpt_2023} in terms of runtime, LLM token usage, and financial cost.

\subsection{Quantitative Analysis}
\label{sec:quantitative}

\begin{table}[t]
\centering
\scalebox{0.83}{
\begin{tabular}{c|l|cccc}
\toprule[0.4mm]
\rowcolor{mygray} \textbf{} & \textbf{Method} & \textbf{Ref} & \textbf{Ref+} & \textbf{Refg} & \textbf{Ref-Adv} \\ \hline \hline
\multirow{9}{*}{\rotatebox[origin=c]{90}{E2E}} & GLIP-L~\cite{li_grounded_2022} & 55.0 & 51.1 & 54.6 & 55.7 \\
& KOSMOS-2~\cite{peng_kosmos-2_2023} & 57.4 & 50.7 & 61.7 & - \\
& YOLO-World-X~\cite{cheng_yolo-world_2024} & 12.1 & 12.1 & 32.9 & 32.2 \\
& YOLO-Wrold-V2-X~\cite{cheng_yolo-world_2024} & 19.8 & 16.8 & 36.5 & 33.1 \\
& GroundingDINO-T~\cite{liu_grounding_2023} & 61.6 & 59.7 & 60.6 & 60.5 \\
& GroundingDINO-B~\cite{liu_grounding_2023} & 90.8 & 84.6 & 80.3 & 78.0 \\
& SimVG~\cite{dai_simvg_2024} & 94.9 & 91.0 & 88.9 & 74.4 \\
& Florence2-B~\cite{xiao_florence-2_2024} & 94.5 & 91.2 & 88.3 & 72.2 \\
& Florence2-L~\cite{xiao_florence-2_2024} & 95.1 & 92.5 & 90.9 & 71.8 \\ \hline
\multirow{7}{*}{\rotatebox[origin=c]{90}{Compositional}} & Code-bison~\cite{stanic_towards_2024} & 44.4 & 38.2 & - & -\\
& ViperGPT†~\cite{suris_vipergpt_2023} & 62.6 & 62.3 & 67.2 & 60.7 \\
& HYDRA†~\cite{ke_hydra_2024} & 60.9 & 56.5 & 62.9 & 54.4 \\
& \methodname{}† & \textbf{70.1} & \textbf{64.1} & \textbf{69.5} & \textbf{65.1} \\ \cline{2-6}
% & ViperGPT* & 67.1 & 73.2 & 65.6 & 60.1 \\
% & HYDRA* & 63.5 & 62.9 & 59.8 & 53.2 \\
% & \methodname{}* & \textbf{91.7} & \textbf{82.4} & \textbf{72.9} & \textbf{87.3} \\ \cline{2-6}
& ViperGPT* & 68.6 & 73.8 & 68.7 & 58.2 \\
& HYDRA* & 65.7 & 66.2 & 59.9 & 48.3 \\
& \methodname{}* & \textbf{96.2} & \textbf{92.8} & \textbf{91.6} & \textbf{75.4} \\
\bottomrule[0.4mm]
\end{tabular}}
\vspace{-2.5mm}
\caption{\textbf{Quantitative comparison (accuracy) on referring expression detection task on RefCOCO, RefCOCO+, RefCOCOg~\cite{kazemzadeh_referitgame_2014} and Ref-Adv~\cite{akula_words_2020} set.} The training set of the GroundingDINO-B, SimVG and Florence2 includes RefCOCO/RefCOCO+/RefCOCOg datasets. 
Other E2E methods are zero-shot. \methodname{} is tested with two versions, one uses the same foundation models with HYDRA and ViperGPT for fair comparison, and another one uses the SoTA models for best performance. In compositional methods, they are grouped with the same VLMs for fair comparison. The methods† and methods* use the same VFMs (GLIP-L, BLIP) and same VFMs (Florence2-L, InternVL2), respectively. Both groups use GPT-4o Mini.}
\label{tab:detection}
\vspace{-4.5mm}
\end{table}

\noindent\textbf{Referring Expression Detection.} We conduct the experiments for quantitative comparison between \methodname{} and SoTA end-to-end (E2E) and compositional baselines on RefCOCO~\cite{kazemzadeh_referitgame_2014} test A, RefCOCO+~\cite{kazemzadeh_referitgame_2014} test A, RefCOCOg~\cite{kazemzadeh_referitgame_2014} test, and Ref-Adv~\cite{akula_words_2020} test set. As shown in \autoref{tab:detection}, \methodname{} demonstrates SoTA performance in comparison. To fairly compare to other compositional methods, \methodname{} use the same LLMs and foundation models (including GPT-4o Mini and GLIP~\cite{li_grounded_2022}) as HYDRA~\cite{ke_hydra_2024} and ViperGPT~\cite{ke_hydra_2024}, and performs better in all four datasets, highlighting the advantage of the design of \methodname{}. ViperGPT and HYDRA, even with SoTA models, fail to achieve significant improvement due to its reliance on LLMs for generating complex Python code, which becomes a bottleneck. Using the SoTA foundation models, \methodname{} performs better than the SoTA E2E methods, as well as compositional approaches such as HYDRA and ViperGPT, \czx{because \methodname{} is equipped with self-correction mechanism and logic reasoning, also breaks down the complex reasoning into simpler subtasks, which are easier to be solved by LLMs and VLMs.} %\hrt{remove this as we are not here to defend HYDRA versus ViperGPT} \czx{We observe that ViperGPT achieves better performance than HYDRA due to its evaluation protocol excluding the runtime error, which will be discussed in the later section.} \hrt{Do we need to discuss these? how it is helpful towards contribtuion of our paper. } In the E2E methods, the training data of GroundingDINO-B~\cite{liu_grounding_2023}, \czx{SimVG~\cite{dai_simvg_2024}} and Florence2-L~\cite{xiao_florence-2_2024} includes the RefCOCO-family datasets, reflected in the higher \czx{accuracy} comparing to others. Notably, the YOLO-World models~\cite{cheng_yolo-world_2024} are designed for the visual grounding without reasoning capacity, showing low \czx{accuracy} in the benchmarks.
For more comprehensive results and comparison, please see \autoref{tab:detection_supp} in supplementary material including more methods and metrics.

\begin{table}[t]
\centering
\scalebox{0.88}{
\begin{tabular}{c|l|cc}
\toprule[0.4mm]
\rowcolor{mygray} \textbf{} & \textbf{} & \multicolumn{2}{c}{\textbf{RefCOCOg}} \\ 
\rowcolor{mygray} \textbf{} & \textbf{Method} & \textbf{Val} & \textbf{Test} 
\\ \hline \hline
\multirow{7}{*}{\rotatebox[origin=c]{90}{E2E (not VLM)}} 
& GLIP-L + SAM~\cite{li_grounded_2022, kirillov_segment_2023} & 35.6 & 38.4 \\ 
& MCN~\cite{luo_multi-task_2020} & 49.2 & 49.4 \\
& VLT~\cite{ding_vision-language_2021} & 55.0 & 57.7 \\
& CRIS~\cite{wang_cris_2022} & 59.9 & 60.4 \\
& LAVT~\cite{yang_lavt_2022} & 61.2 & 62.1 \\
& GRES~\cite{liu_gres_2023} & 65.0 & 66.0 \\
& GroundingDINO-B + SAM~\cite{liu_grounding_2023, kirillov_segment_2023} & 58.7 & 61.4 \\ \hline
\multirow{5}{*}{\rotatebox[origin=c]{90}{E2E (VLM)}} & LISA-7B~\cite{lai_lisa_2024} & 66.4 & 68.5 \\
& PerceptionGPT~\cite{pi_perceptiongpt_2024} & 70.7 & 71.9 \\
& GSVA~\cite{xia_gsva_2024} & 73.2 & 73.9 \\
& PSALM~\cite{zhang_psalm_2024} & 73.8 & 74.4 \\ 
& GLaMM~\cite{rasheed_glamm_2024} & 74.2 & 74.9 \\ \hline
\multirow{6}{*}{\rotatebox[origin=c]{90}{Compositional}} & ViperGPT†~\cite{suris_vipergpt_2023} & 46.9 & 44.1 \\
& HYDRA†~\cite{ke_hydra_2024} & 51.9 & 53.1 \\
& \methodname{}† & \textbf{54.4} & \textbf{54.6}\\ \cline{2-4}
& ViperGPT* & 50.9 & 44.8 \\
& HYDRA* & 53.2 & 54.0 \\
& \methodname{}* & \textbf{76.4} & \textbf{76.0} \\
\bottomrule[0.4mm]
\end{tabular}}
\vspace{-2.5mm}
\caption{\textbf{Quantitative comparison (IoU) on Referring Expression Segmentation task on RefCOCOg val and test set~\cite{kazemzadeh_referitgame_2014}.} The training data of all E2E methods except GLIP-L include RefCOCOg dataset. We compare \methodname{} with two compositional baselines using different VFMs. Each method is evaluated in two versions: (†) uses the same VFMs as the original HYDRA and ViperGPT for a fair comparison; (*) uses the SoTA VFMs for best performance. All use GPT-4o Mini as the LLMs.
}
\label{tab:segmentation}
\vspace{-2.5mm}
\end{table}

\begin{table}[t]
\centering
\scalebox{0.88}{
\begin{tabular}{c|l|cc}
\toprule[0.4mm]
\rowcolor{mygray} \textbf{} &  & \multicolumn{2}{c}{\textbf{RefCLEF}} \\ 
\rowcolor{mygray} \textbf{} & \textbf{Method} & \textbf{Test A} & \textbf{Test B} \\ \hline \hline
\multirow{3}{*}{\rotatebox[origin=c]{90}{E2E}} 
& GLIP-L + SAM~\cite{li_grounded_2022, kirillov_segment_2023} & 50.9 & 56.4 \\
& GroundingDINO-B + SAM~\cite{liu_grounding_2023, kirillov_segment_2023} & 62.5 & 63.1 \\
& GLaMM~\cite{rasheed_glamm_2024} & 67.5 & 66.8 \\ \hline
\multirow{6}{*}{\rotatebox[origin=c]{90}{Compositional}} & ViperGPT†~\cite{suris_vipergpt_2023} & 55.6 & 56.9 \\
& HYDRA†~\cite{ke_hydra_2024} & 56.4 & 58.1 \\
& \methodname{}† & \textbf{58.1} & \textbf{59.3} \\ \cline{2-4}
& ViperGPT* & 61.2 & 58.3 \\
& HYDRA* & 56.5 & 58.3 \\
& \methodname{}* & \textbf{68.8} & \textbf{67.4} \\
\bottomrule[0.4mm]
\end{tabular}}
\vspace{-2.5mm}
\caption{\textbf{Quantitative comparison (IoU) on Referring Expression Segmentation task on RefCLEF~\cite{kazemzadeh_referitgame_2014}.} The training data of all E2E methods except GLIP-L include RefCOCOg dataset. The configurations (†, *) of compositional methods follow \autoref{tab:segmentation}.}
\label{tab:segmentation2}
\vspace{-4.5mm}
\end{table}

\noindent\textbf{Referring Expression Segmentation.} Results on RefCOCOg and RefCLEF~\cite{kazemzadeh_referitgame_2014} are shown in \autoref{tab:segmentation} and \autoref{tab:segmentation2}. In both datasets, \methodname{} demonstrates higher performance compared to SoTA E2E baselines and other compositional baselines~\cite{ke_hydra_2024, suris_vipergpt_2023} using the same foundation models and language models. Consistent with the observation from \autoref{tab:detection}, replacing the VFMs in compositional methods with the best performing models, \ie GLaMM and InternVL2, incrementally improve their performances in segmentation. These experiments show that \methodname{} is capable of utilizing best performing VLM-based methods and yet achieve a better performance in this task.

\noindent\textbf{Performance Analysis.} The previous compositional baselines~\cite{suris_vipergpt_2023, ke_hydra_2024} compute the grounding performance excluding the runtime errors caused by LLM's uncontrollable output. The scores reported in the benchmarks are not reliable in real applications due to the high failure rate. We conduct a fair comparison with previous compositional baselines~\cite{ke_hydra_2024, suris_vipergpt_2023} by using the same LLM (GPT-4o Mini~\cite{openai_gpt-4o_2024}), grounding model (GLIP-Large~\cite{li_grounded_2022}, and SAM~\cite{kirillov_segment_2023}) in the segmentation task. We report the IoU scores including/excluding the runtime errors and the runtime failure rate in  \autoref{tab:failure_rate}. The original ViperGPT lacks the validation in inference, leading to a high failure rate. %\pjs{something wrong with previous phrase not sure what you are trying to say}
HYDRA reduces the error rate by introducing incremental reasoning and a controller to validate the plan, but still reports a 10.3\% failure rate. Our proposed method \methodname{} produces a more reliable pipeline (0.3\% runtime failure rate) with the validation for the full-chain inference, where each state verifies the intermediate outputs. We find the main failure reason is due to the ChatGPT content policy which misclassifies some queries such as ``man cut off'' as violent text and fails to %
% \pjs{rejects the query or fails to respond?} 
respond, however this query asks for a man only half of whose body appears in the image. Furthermore, comparing the performance ignoring the runtime error, we still observe a noticeable improvement of \methodname{} comparing to the baselines with the same foundation models.

\noindent\textbf{Complexity Analysis.} Efficiency results for \methodname{} and compositional baselines are provided in \autoref{tab:complexiy}, which includes average runtime, token usage, and cost per sample. For a fair comparison, we use the same foundation models and LLM for the experiments. From the results, we observe \methodname{} is more efficient than HYDRA~\cite{ke_hydra_2024} with higher performance and lower runtime failure rate. Although ViperGPT~\cite{suris_vipergpt_2023} has lower time complexity, this advantage is offset by reduced performance due to its lack of validation. These results demonstrate that \methodname{}'s flexible automaton design reduces unnecessary computations by dynamically adapting the pipeline. 


% \subsection{Qualitative Analysis}


\begin{table}[t]
\centering
\scalebox{0.9}{
\begin{tabular}{l|ccc}
\toprule[0.4mm]
\rowcolor{mygray} \textbf{} & \textbf{Failure} & \textbf{RefCOCOg} & \textbf{RefCOCOg} \\
\rowcolor{mygray} \textbf{Method} & \textbf{(\%)} & \textbf{(Exc. Err.)} & \textbf{(Inc. Err.)} \\\hline \hline
ViperGPT~\cite{suris_vipergpt_2023} & 70.2 & 44.1 & 15.3 \\
HYDRA~\cite{ke_hydra_2024} & 10.3 & 53.1 & 47.6 \\
\methodname{} (Ours) & \textbf{00.3} & \textbf{54.6} & \textbf{54.4} \\
\bottomrule[0.4mm]
\end{tabular}}
\vspace{-2.5mm}
\caption{\textbf{Performance comparison with compositional methods.} All results are evaluated on the RefCOCOg (IoU) test set for segmentation. All methods use the same foundation models.}
\label{tab:failure_rate}
\vspace{-4.5mm}
\end{table}


\begin{table}[t]
\centering
\scalebox{0.9}{
\begin{tabular}{l|cccc}
\toprule[0.4mm]
\rowcolor{mygray} \textbf{} & \textbf{Time} & \multicolumn{2}{c}{\textbf{\# LLM Tokens}} & \textbf{LLM Cost} \\
\rowcolor{mygray} \textbf{Method} & \textbf{(Second)} & \textbf{Input} & \textbf{Output} & \textbf{(USD)} \\\hline \hline
ViperGPT~\cite{suris_vipergpt_2023} & \textbf{2.02} & 4169 & \textbf{29} & 0.00064 \\
HYDRA~\cite{ke_hydra_2024} & 14.93 & 19701 & 618 & 0.00332  \\
\methodname{} (Ours) & 5.96 & \textbf{2083} & 39 & \textbf{0.00034} \\
\bottomrule[0.4mm]
\end{tabular}}
\vspace{-2.5mm}
\caption{\textbf{Complexity comparison between compositional methods.} All results are evaluated on the RefCOCOg test set with the GPT-4o Mini for fair comparison. All values are the average per data sample. All methods use the same foundation models.}
\label{tab:complexiy}
\vspace{-2.5mm}
\end{table}

\subsection{Ablation Studies}
\label{sec:ablation}

\noindent\textbf{Contribution of Modules.} To examine the contributions of key components, \emph{deterministic finite-state automaton} (DFA), logic \czx{and answerer}, we conduct ablation studies on RefCOCO, RefCOCO+, RefCOCOg and Ref-Adv dataset shown in \autoref{tab:ablation}. To compare in detail the contribution of each component, we examined different combinations of the system:  \textit{(1) Disabling DFA:} Disabling the self-correction transitions (\textcolor{red}{red arrows} in \autoref{fig:pipeline}); \textit{(2) Disabling Logic:} The entities detected in the \emph{Perception} state are redirected to the \emph{Answerer} bypassing the \emph{Logic Generation} and \emph{Logic Reasoning}; \textit{(3) Disabling Answerer:} the answer is not validated by the answerer; and the feasible combinations of (1), (2) and (3). %\textit{(3) Disabling DFA and Logic}: Combining (1) and (2).
\czx{Comparing rows 1 and 4, 3 and 6, 5 and 7, the DFA self-correction mechanism provides substantial performance improvement.
Comparing rows 1 and 3, 2 and 5, the enabling of logic leads to a significant improvement in the performance. 
Comparing rows 1 and 2, 3 and 5, 6 and 7, the answerer validates the result and improves the accuracy.}
From row 7, enabling all DFA self-correction, logic and answerer improves the performance and reaches the SoTA. %The impact of other modules is discussed in supplementary materials.

% \noindent\textbf{Analysis of the self-correction mechanism.}

\noindent\textbf{Selection of Grounding Models.} We conduct experiments using different grounding models as the entity detector in \methodname{}. We tested \methodname{} with various grounding methods~\cite{li_grounded_2022, liu_grounding_2023, dai_simvg_2024, rasheed_glamm_2024}
% GLIP-Large~\cite{li_grounded_2022}, GroundingDINO-Tiny~\cite{liu_grounding_2023}, GroundingDINO-Base, Florence2-Large~\cite{xiao_florence-2_2024}, and GLaMM~\cite{rasheed_glamm_2024} 
on RefCOCO for detection and RefCOCOg for segmentation, as shown in \autoref{tab:grounding_comparison}. As \methodname{} is flexible in choosing foundation models, \methodname{} can gain performance by updating to SoTA grounding models. Comparing the performance of original grounding method with \methodname{} employing it, the performance is usually improved when using \methodname{}.
%\pjs{Isnt it always improved?}

% \noindent\textbf{Selection of LLM.} We conduct experiments with different LLMs including open source self-hosted LLaMA3.1~\cite{dubeyLlama2024}, Gemma2~\cite{team_gemma_2024}, Deepseek-R1~\cite{deepseek-ai_deepseek-r1_2025-1} and OpenAI LLMs~\cite{openai_gpt-4o_2024} evaluating on the RefCOCO dataset in \autoref{tab:llm_comparison}. From the results, there is no significant difference between different LLMs, because the LLM is mainly used for simple tasks, including extracting entities of interest and translating the query into ProbLog code. Therefore, a smaller high-efficiency LLM such as GPT-4o Mini is preferred.

% \begin{table}[t]
% \centering
% \scalebox{0.78}{
% \begin{tabular}{cc|cccc}
% \toprule[0.4mm]
% \rowcolor{mygray} \textbf{FSA} & \textbf{Logic} & \textbf{RefCOCO} & \textbf{RefCOCO+} & \textbf{RefCOCOg} & \textbf{Ref-Adv} \\\hline \hline
% \ding{55} & \ding{55} & 75.2 & 71.0 & 60.5 & 54.1 \\
% \ding{55} & \ding{51} & 82.9 & 77.3 & 70.4 & 71.7 \\
% \ding{51} & \ding{55} & 80.8 & 76.9 & 69.7 & 70.4 \\
% \ding{51} & \ding{51} & \textbf{83.3} & \textbf{78.4} & \textbf{72.5} & \textbf{74.2} \\
% \bottomrule[0.4mm]
% \end{tabular}}
% \vspace{-2.5mm}
% \caption{\textbf{Contribution of components of \methodname{}.} Results (IoU) are based on the RefCOCO test A, RefCOCO+ test A, RefCOCOg test and Ref-Adv test set.}
% \label{tab:ablation}
% \vspace{-4.5mm}
% \end{table}

\begin{table}[t]
\centering
\scalebox{0.78}{
\begin{tabular}{ccc|cccc}
\toprule[0.4mm]
\rowcolor{mygray} \textbf{DFA} & \textbf{Logic} & \textbf{Answerer} & \textbf{Ref} & \textbf{Ref+} & \textbf{Refg} & \textbf{Ref-Adv} \\\hline \hline
\ding{55} & \ding{55} & \ding{55} & 51.7 & 51.3 & 48.8 & 34.1 \\
\ding{55} & \ding{55} & \ding{51} & 57.4 & 57.6 & 50.8 & 34.3 \\
\ding{55} & \ding{51} & \ding{55} & 57.8 & 62.0 & 51.3 & 36.4 \\
\ding{51} & \ding{55} & \ding{55} & 68.5 & 69.3 & 70.1 & 64.7 \\
\ding{55} & \ding{51} & \ding{51} & 78.1 & 68.4 & 53.4 & 43.6 \\
\ding{51} & \ding{51} & \ding{55} & 76.2 & 73.8 & 75.8 & 69.7 \\
% \ding{51} & \ding{55} & \ding{51} & 96.1 & 91.4 & 87.4 & 71.0 \\
\ding{51} & \ding{51} & \ding{51} & \textbf{96.2} & \textbf{92.8} & \textbf{91.6} & \textbf{75.4}\\
\bottomrule[0.4mm]
\end{tabular}}
\vspace{-2.5mm}
\caption{\textbf{Contribution of components of \methodname{}.} Results (accuracy) are based on the RefCOCO test A, RefCOCO+ test A, RefCOCOg test and Ref-Adv test set.}
\label{tab:ablation}
\vspace{-4.5mm}
\end{table}

\noindent\textbf{Selection of LLM.} We conduct experiments with different LLMs, including LLaMA3.1~\cite{dubeyLlama2024}, Gemma2~\cite{team_gemma_2024}, DeepSeek-R1~\cite{deepseek-ai_deepseek-r1_2025-1}, and OpenAI LLMs~\cite{openai_gpt-4o_2024}, evaluating them on the RefCOCO and RefCOCOg dataset in \autoref{tab:llm_comparison}. The results indicate that for the easier RefCOCO dataset, the performance differences between LLMs are minimal. However, for the more challenging RefCOCOg dataset, the performance gap becomes more significant. Given these findings, a smaller high-efficiency LLM, such as GPT-4o Mini, is preferred, as it maintains competitive performance while achieving significantly lower inference time.

% \begin{table}[t]
% \centering
% \scalebox{0.8}{
% \begin{tabular}{l|c}
% \toprule[0.4mm]
% \rowcolor{mygray} \textbf{Method} & \textbf{RefCOCO} \\\hline \hline
% GLIP-L~\cite{li_grounded_2022} & 55.0\\
% \methodname{} with GLIP-L & \textbf{70.1} \\\hline
% GroundingDINO-T~\cite{liu_grounding_2023} & 61.6 \\
% \methodname{} with GroundingDINO-T & \textbf{} \\\hline
% GroundingDINO-B~\cite{liu_grounding_2023} & 90.8 \\
% \methodname{} with GroundingDINO-B & \textbf{91.7} \\\hline
% Florence2-L~\cite{xiao_florence-2_2024} & 95.1 \\
% \methodname{} with Florence2-L & \textbf{} \\\hline
% \rowcolor{mygray} \textbf{Method} &  \textbf{RefCOCOg} \\\hline
% GLIP+SAM~\cite{li_grounded_2022, kirillov_segment_2023} & 38.4 \\
% \methodname{} with GLIP+SAM & \textbf{54.6} \\\hline
% GroundingDINO-B+SAM~\cite{liu_grounding_2023, kirillov_segment_2023} & 61.4 \\
% \methodname{} with GroundingDINO-B+SAM & \textbf{65.0} \\\hline
% GLaMM~\cite{rasheed_glamm_2024} & 74.9 \\
% \methodname{} with GLaMM & \textbf{76.0} \\\hline
% \bottomrule[0.4mm]
% \end{tabular}}
% \vspace{-2.5mm}
% \caption{\textbf{Comparison of the performance using different grounding methods for \methodname{}.} All results are evaluated on the RefCOCO (IoU) test A set and RefCOCOg (IoU) test set for referring expression detection and segmentation tasks.}
% \label{tab:grounding_comparison}
% \vspace{-2.5mm}
% \end{table}

\begin{table}[t]
\centering
\scalebox{0.8}{
\begin{tabular}{l|cc}
\toprule[0.4mm]
\rowcolor{mygray} & \multicolumn{2}{c}{\textbf{RefCOCO}} \\ 
\rowcolor{mygray} \textbf{Method} & Itself & In \methodname{} \\ \hline \hline
GLIP-L~\cite{li_grounded_2022} & 55.0 & \textbf{70.1} \\
GroundingDINO-T~\cite{liu_grounding_2023} & 61.6 & \textbf{64.9} \\
GroundingDINO-B~\cite{liu_grounding_2023} & 90.8 & \textbf{91.7} \\
SimVG~\cite{dai_simvg_2024} & 94.9 & \textbf{95.4} \\
Florence2-L~\cite{xiao_florence-2_2024} & 95.1 & \textbf{96.2}\\\hline
\rowcolor{mygray} \textbf{Method} & \multicolumn{2}{c}{\textbf{RefCOCOg}} \\\hline
GLIP+SAM~\cite{li_grounded_2022, kirillov_segment_2023} & 38.4 & \textbf{54.6} \\
GroundingDINO-B+SAM~\cite{liu_grounding_2023, kirillov_segment_2023} & 61.4 & \textbf{65.0} \\
GLaMM~\cite{rasheed_glamm_2024} & 74.9 & \textbf{76.0} \\
\bottomrule[0.4mm]
\end{tabular}}
\vspace{-2.5mm}
\caption{\textbf{Comparison of the performance using different grounding methods for \methodname{}.} All results are evaluated on the RefCOCO (accuracy) for detection and RefCOCOg (IoU) for segmentation. The column \emph{Itself} shows the performance of the methods, and column \emph{In \methodname{}} shows the performance of NAVER using these methods for grounding.}
\label{tab:grounding_comparison}
\vspace{-2.5mm}
\end{table}

\begin{table}[t]
\centering
\scalebox{0.8}{
\begin{tabular}{lc|cc|cc}
\toprule[0.4mm]
\rowcolor{mygray} & & \multicolumn{2}{c|}{\textbf{RefCOCO}} & \multicolumn{2}{c}{\textbf{RefCOCOg}} \\
\rowcolor{mygray} \textbf{LLM} & \textbf{Size} & \textbf{Time (Sec.)} & \textbf{Acc.} & \textbf{Time (Sec.)} & \textbf{Acc.} \\\hline \hline
DeepSeek-R1~\cite{deepseek-ai_deepseek-r1_2025-1} & 70B & 71.27 & 95.4 & 82.13 & 85.8\\
LLaMA3.1~\cite{dubeyLlama2024} & 70B & 12.41 & 96.0 & 19.40 & 88.8 \\
LLaMA3.1~\cite{dubeyLlama2024} & 8B & 8.23 & 95.0 & 9.25 & 90.5 \\
Gemma2~\cite{team_gemma_2024} & 3B & 8.18 & 96.1 & 9.13 & 91.1 \\
GPT-4o~\cite{openai_gpt-4o_2024} & - & 8.67 & \textbf{96.6} & 10.29 & 89.7 \\
GPT-4o Mini~\cite{openai_gpt-4o_2024} & - & 5.74 & 96.2 & 7.91 & \textbf{91.6} \\
\bottomrule[0.4mm]
\end{tabular}}
\vspace{-2.5mm}
\caption{\textbf{Comparison of LLM selection for \methodname{}.} Results are evaluated on the RefCOCO and RefCOCOg for detection. The time values show the average seconds spent on each data sample.}
\label{tab:llm_comparison}
\vspace{-5.5mm}
\end{table}