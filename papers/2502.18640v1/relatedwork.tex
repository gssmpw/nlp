\section{Related Works}
\subsection{Mixed-Reality Point-of-Care Ultrasound Training}
 Since the 1970s, ultrasonography training has benefited from the emergence of various high-fidelity simulation technologies \cite{parks2013can, mackay2018can, ali2020simulator, kochan2021point, SilvaJournalOE, SituLaCasse2021CanUN, okano2021outcomes}. These range from mannequin-based devices with sensor-equipped probes \cite{sim1, sim2, sim3, vascularsim, okano2021outcomes} to more advanced solutions utilizing augmented reality (AR) \cite{ar1, evans2024product}, virtual reality (VR) \cite{vr1, vr2, vr3annotate} applications, and even web-based platforms \cite{web1, web2, web3phoneinsimo}. Many of these simulations are now available as commercial products, though often at substantial cost, thereby limiting access in low-resource settings. There is a well-documented body of literature on the effectiveness of simulation-based training in developing competency in basic PoCUS scanning techniques \cite{OlivaresPerez2021VirtualAA, parks2013can, mackay2018can, ali2020simulator, kochan2021point, SilvaJournalOE, SituLaCasse2021CanUN}.

These applications provide various feedback mechanisms, including images and videos demonstrating correct manipulation \cite{vascularsim}, 3D heart model cross-sections \cite{sim1, sim2, sim3, web4}, anatomical structure visualizations \cite{vr1, vr2, vr3annotate, sim3, web4}, guidance for correct probe positioning \cite {sim3,web4}, holograms of ultrasound images over the patient's body \cite{ar1}, and tools for capturing and annotating images \cite{vr3annotate}.

While 3D views enhance anatomical understanding and correct probe positioning provides useful cues, these tools primarily serve as educational aids rather than intelligent tutors.  Without instructors, students may resort to suboptimal strategies like aligning the probe with shadows or memorizing probe positions. Such approaches will fail due to anatomical variability. Instead, trainees must develop the skills to operate the probe by interpreting views, rather than relying on memorized probe positions.

\subsection{Mixed-Reality Tutoring System}

Previous research has demonstrated that Mixed Reality (MR) tutoring offers significant advantages in teaching physical tasks and motor skills \cite{wang2020capturar, huang2021adaptutar, cao2022mobiletutar, chidambaram2022editar, liu2023instrumentar, ipsita2022welding}. MR environments provide immersive guidance, enhancing both the learning experience and the quality of skill acquisition.

Most existing MR tutoring systems focus on areas such as machine or equipment operation \cite{cao2022mobiletutar, chidambaram2021processar, huang2021adaptutar, ipsita2022welding, liu2023instrumentar}, assembly tasks \cite{whitlock2020authar, yamaguchi2020video}, or body coordination activities like physical therapy, exercise, and rehabilitation \cite{anderson2013youmove, faridan2023chameleoncontrol, monteiro2023teachable, semeraro2022visualizing}. These tasks generally fall into the category of closed skills, where learners are expected to replicate or memorize predefined sequences of actions \cite{cao2022mobiletutar, chidambaram2021processar, huang2021adaptutar, ipsita2022welding, liu2023instrumentar}, object placements \cite{ipsita2022welding, whitlock2020authar, yamaguchi2020video}, operational procedures \cite{huang2021adaptutar, ipsita2022welding}, and correct poses \cite{anderson2013youmove, semeraro2022visualizing}. As a result, these systems primarily guide users toward performing predefined ``correct'' actions, often employing instructional tools such as text \cite{whitlock2020authar}, images \cite{anderson2013youmove, cao2020exploratory, chidambaram2021processar}, videos \cite{ipsita2022welding}, 3D arrows \cite{cao2020exploratory, liu2023instrumentar, chidambaram2021processar}, slide bars \cite{liu2023instrumentar, ipsita2022welding}, and avatars \cite{cao2020exploratory, huang2021adaptutar}. In physical therapy and exercise scenarios, visual cues are frequently used to alert users about incorrect movements \cite{anderson2013youmove, faridan2023chameleoncontrol, monteiro2023teachable, semeraro2022visualizing}. 

In contrast, open tasks, like PoCUS, pose unique challenges as learners cannot simply replicate an expertâ€™s solution due to multiple viable approaches and troubleshooting strategies. Systems like Adaptutor \cite{huang2021adaptutar} adaptive hints. However, simply teaching the ``correct'' actions is insufficient as the students may still face difficulties in generalizing them to other cases. Effective training should focus on building adaptive problem-solving skills and explaining the rationale behind actions to align with the open-ended nature of the task. Wang et al. \cite{Wang2023MGPAM} explored instructor-led feedback in VR training, allowing for action explanations similar to traditional classes. However, such resources are often unavailable, and verbal feedback alone is limited in conveying visual information.

Another significant limitation of many MR tutoring systems is their reliance on pre-recorded tutorials that are replayed for users \cite{huang2021adaptutar, chidambaram2021processar, liu2023instrumentar, whitlock2020authar}. While this approach works well for closed tasks, it is far less effective for open tasks like PoCUS, where every case can present unique challenges. Building a tutorial bank with hundreds of specific cases would require enormous effort. Hence, a more intelligent and adaptive system would be better suited to support learning in open-ended and variable tasks.


\subsection{Hand-Eye Coordination}
Hand-eye coordination is typically defined as the ability to control hand movements using visual feedback. During movement, the eyes guide attention toward stimuli, helping the brain understand the spatial position of the body \cite{ballard1992hand, zhu2020hand}.

Research on improving hand-eye coordination includes tasks like moving a cursor to a target using a mouse \cite{smith2000hand, huang2012user}, with added complexity from controllers involving buttons and knobs \cite{sailer2005eye}. In medicine, robotic surgeries exemplify how visual feedback helps surgeons guide tools on-screen \cite{gao2017modeling}, with a key insight being that seeing both current and target positions aids spatial orientation \cite{horstmann2005target}. However, in PoCUS, measuring the distance between the current and target views is more complex than in screen-based tasks. Thus, in designing our tutoring system for PoCUS, it is crucial to provide an easier way to measure this distance.

Training hand-eye coordination involves practicing to achieve a specific goal. Lachman \cite{lachman1997learning} describes this as a stable modification in the stimulus-response relationship due to sensory interactions with the environment. Krakauer and Mazzoni \cite{krakauer2011human} showed that human senses can be trained to reduce false predictions. Repeated practice creates an after-effect that preserves the benefits of training \cite{zhou2023method}. Therefore, offering high-quality practice opportunities is essential in designing effective tutoring systems for hand-eye coordination tasks.

\subsection{Subgoal Learning}
Goal-setting theory by Austin and Vancouver \cite{austin1996goal} suggests that learners tend to break complex tasks into manageable parts, known as subgoals. These intermediate milestones within a larger problem-solving process allow individuals to navigate complex tasks more effectively, track progress, and maintain motivation as they work towards their main objective. 

The effectiveness of subgoal learning in enhancing problem-solving performance has been demonstrated across various fields, such as mathematics \cite{atkinson2003aiding, catrambone1998subgoal, margulieux2018varying}, chemistry \cite{margulieux2018varying}, and programming \cite{margulieux2018varying, margulieux2019finding, margulieux2012subgoal, morrison2020curious, morrison2015subgoals}. Numerous studies in education also emphasize the importance of breaking down problems into smaller subgoals \cite{de2009teaching, guzdial1998supporting, hu2013process, koedinger2013using}. Research indicates that learners with constructive subgoals outperform those using expert-labeled ones in tasks like basic programming and app development \cite{margulieux2019finding, morrison2020curious, morrison2015subgoals} and learner sourcing methods can also generate high-quality subgoals \cite{jin2024codetree, choi2022algosolve}.

While subgoals are well studied in high-cognitive tasks, PoCUS integrates cognitive and physical aspects, making it more of a hand-eye coordination task. In PoCUS, subgoals function more as milestones along the path rather than parts of the solution, resembling path-planning tasks. Unlike in mathematics or programming, where subgoal generation often requires human labeling or learner sourcing, PoCUS benefits from a clear starting point and end goal. Thus, we can explore automatic path planning with milestones as a way to break down the task.

Previous research has explored how robotic arms can determine scan ranges, paths, and poses for each step when scanning a lumbar phantom, which relies on the 3D contours of the skin surface \cite{huang2019robotic}. However, the heart is a much more complex structure, and probe movement in PoCUS is driven by image views rather than surface contours, providing an opportunity for further exploration of automatic subgoal generation.