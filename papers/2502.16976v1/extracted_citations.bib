@article{6-dofview,
  title={Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection},
  author={Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Ibarz, Julian and Quillen, Deirdre},
  journal={IJRR},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{GPD,
  title={Grasp pose detection in point clouds},
  author={Ten Pas, Andreas and Gualtieri, Marcus and Saenko, Kate and Platt, Robert},
  journal={IJRR},
  year={2017},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{GSNet,
  title={Graspness discovery in clutters for fast and accurate grasp detection},
  author={Wang, Chenxi and Fang, Hao-Shu and Gou, Minghao and Fang, Hongjie and Gao, Jin and Lu, Cewu},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{SG14000,
  title={Cage: Context-aware grasping engine},
  author={Liu, Weiyu and Daruna, Angel and Chernova, Sonia},
  booktitle={ICRA},
  year={2020},
}

@inproceedings{avigal2022manipulation,
  title={Speedfolding: Learning efficient bimanual folding of garments},
  author={Avigal, Yahav and Berscheid, Lars and Asfour, Tamim and Kr{\"o}ger, Torsten and Goldberg, Ken},
  booktitle={IROS},
  year={2022},
}

@inproceedings{chen2022TOG,
  title={Learning 6-DoF Task-oriented Grasp Detection via Implicit Estimation and Visual Affordance},
  author={Chen, Wenkai and Liang, Hongzhuo and Chen, Zhaopeng and Sun, Fuchun and Zhang, Jianwei},
  booktitle={IROS},
  year={2022},
}

@article{chen2024motiongrasp,
  title={MotionGrasp: Long-Term Grasp Motion Tracking for Dynamic Grasping},
  author={Chen, Nuo and Wu, Xiao-Ming and Xu, Guohao and Jiang, Jian-Jian and Chen, Zibo and Zheng, Wei-Shi},
  journal={RAL},
  year={2024},
  publisher={IEEE}
}

@inproceedings{dantam2016incremental,
  title={Incremental task and motion planning: A constraint-based approach.},
  author={Dantam, Neil T and Kingston, Zachary K and Chaudhuri, Swarat and Kavraki, Lydia E},
  booktitle={Robotics: Science and systems},
  volume={12},
  pages={00052},
  year={2016},
  organization={Ann Arbor, MI, USA}
}

@article{du2024weakly,
  title={Weakly-supervised temporal action localization by progressive complementary learning},
  author={Du, Jia-Run and Feng, Jia-Chang and Lin, Kun-Yu and Hong, Fa-Ting and Qi, Zhongang and Shan, Ying and Hu, Jian-Fang and Zheng, Wei-Shi},
  journal={TCSVT},
  year={2024},
  publisher={IEEE}
}

@inproceedings{fang2020graspnet1billion,
  title={Graspnet-1billion: A large-scale benchmark for general object grasping},
  author={Fang, Hao-Shu and Wang, Chenxi and Gou, Minghao and Lu, Cewu},
  booktitle={CVPR},
  year={2020}
}

@article{fang2020learningtog,
  title={Learning task-oriented grasping for tool manipulation from simulated self-supervision},
  author={Fang, Kuan and Zhu, Yuke and Garg, Animesh and Kurenkov, Andrey and Mehta, Viraj and Fei-Fei, Li and Savarese, Silvio},
  journal={IJRR},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{feng2024docpedia,
  title={Docpedia: Unleashing the power of large multimodal model in the frequency domain for versatile document understanding},
  author={Feng, Hao and Liu, Qi and Liu, Hao and Tang, Jingqun and Zhou, Wengang and Li, Houqiang and Huang, Can},
  journal={Science China Information Sciences},
  year={2024},
  publisher={Springer}
}

@article{gao2024riemann,
  title={RiEMann: Near Real-Time SE (3)-Equivariant Robot Manipulation without Point Cloud Segmentation},
  author={Gao, Chongkai and Xue, Zhengrong and Deng, Shuying and Liang, Tianhai and Yang, Siqi and Shao, Lin and Xu, Huazhe},
  journal={arXiv preprint arXiv:2403.19460},
  year={2024}
}

@inproceedings{li2024egoexo,
  title={EgoExo-Fitness: towards egocentric and exocentric full-body action understanding},
  author={Li, Yuan-Ming and Huang, Wei-Jin and Wang, An-Lan and Zeng, Ling-An and Meng, Jing-Ke and Zheng, Wei-Shi},
  booktitle={ECCV},
  year={2024}
}

@article{li2024techcoach,
  title={TechCoach: Towards Technical Keypoint-Aware Descriptive Action Coaching},
  author={Li, Yuan-Ming and Wang, An-Lan and Lin, Kun-Yu and Tang, Yu-Ming and Zeng, Ling-An and Hu, Jian-Fang and Zheng, Wei-Shi},
  journal={arXiv preprint arXiv:2411.17130},
  year={2024}
}

@article{lin2024diversifying,
  title={Diversifying spatial-temporal perception for video domain generalization},
  author={Lin, Kun-Yu and Du, Jia-Run and Gao, Yipeng and Zhou, Jiaming and Zheng, Wei-Shi},
  journal={NeruIPS},
  volume={36},
  year={2024}
}

@article{lu2024bounding,
  title={A bounding box is worth one token: Interleaving layout and text in a large language model for document understanding},
  author={Lu, Jinghui and Yu, Haiyang and Wang, Yanjie and Ye, Yongjie and Tang, Jingqun and Yang, Ziwei and Wu, Binghong and Liu, Qi and Feng, Hao and Wang, Han and others},
  journal={arXiv preprint arXiv:2407.01976},
  year={2024}
}

@inproceedings{mousavian20196dofgraspnet,
  title={6-dof graspnet: Variational grasp generation for object manipulation},
  author={Mousavian, Arsalan and Eppner, Clemens and Fox, Dieter},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{murali2021same,
  title={Same object, different grasps: Data and semantic knowledge for task-oriented grasping},
  author={Murali, Adithyavairavan and Liu, Weiyu and Marino, Kenneth and Chernova, Sonia and Gupta, Abhinav},
  booktitle={CoRL},
  year={2021},
}

@inproceedings{ni2020pointnet++grasp,
  title={Pointnet++ grasping: Learning an end-to-end spatial grasp generation algorithm from sparse point clouds},
  author={Ni, Peiyuan and Zhang, Wenguang and Zhu, Xiaoxiao and Cao, Qixin},
  booktitle={ICRA},
  year={2020},
}

@inproceedings{papallas2022manipulation,
  title={To ask for help or not to ask: A predictive approach to human-in-the-loop motion planning for robot manipulation tasks},
  author={Papallas, Rafael and Dogar, Mehmet R},
  booktitle={IROS},
  year={2022},
}

@inproceedings{pointnetGPD,
  title={Pointnetgpd: Detecting grasp configurations from point sets},
  author={Liang, Hongzhuo and Ma, Xiaojian and Li, Shuang and G{\"o}rner, Michael and Tang, Song and Fang, Bin and Sun, Fuchun and Zhang, Jianwei},
  booktitle={ICRA},
  year={2019},
}

@article{shan2024mctbench,
  title={Mctbench: Multimodal cognition towards text-rich visual scenes benchmark},
  author={Shan, Bin and Fei, Xiang and Shi, Wei and Wang, An-Lan and Tang, Guozhi and Liao, Lei and Tang, Jingqun and Bai, Xiang and Huang, Can},
  journal={arXiv preprint arXiv:2410.11538},
  year={2024}
}

@inproceedings{shao2020manipulation,
  title={Learning to scaffold the development of robotic manipulation skills},
  author={Shao, Lin and Migimatsu, Toki and Bohg, Jeannette},
  booktitle={ICRA},
  year={2020},
}

@article{suarez2018can,
  title={Can robots assemble an IKEA chair?},
  author={Su{\'a}rez-Ruiz, Francisco and Zhou, Xian and Pham, Quang-Cuong},
  journal={Science Robotics},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{sun2021gater,
  title={Gater: Learning grasp-action-target embeddings and relations for task-specific grasping},
  author={Sun, Ming and Gao, Yue},
  journal={RAL},
  year={2021},
  publisher={IEEE}
}

@inproceedings{sundermeyer2021contact,
  title={Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes},
  author={Sundermeyer, Martin and Mousavian, Arsalan and Triebel, Rudolph and Fox, Dieter},
  booktitle={ICRA},
  year={2021},
}

@inproceedings{tang2022few,
  title={Few could be better than all: Feature sampling and grouping for scene text detection},
  author={Tang, Jingqun and Zhang, Wenqing and Liu, Hongye and Yang, MingKun and Jiang, Bo and Hu, Guanglong and Bai, Xiang},
  booktitle={CVPR},
  year={2022}
}

@article{tang2023graspclip,
  title={Task-Oriented Grasp Prediction with Visual-Language Inputs},
  author={Tang, Chao and Huang, Dehao and Meng, Lingxiao and Liu, Weiyu and Zhang, Hong},
  journal={IROS},
  year={2023}
}

@article{tang2023graspgpt,
  title={Graspgpt: Leveraging semantic knowledge from a large language model for task-oriented grasping},
  author={Tang, Chao and Huang, Dehao and Ge, Wenqi and Liu, Weiyu and Zhang, Hong},
  journal={RAL},
  year={2023},
}

@article{tang2024mtvqa,
  title={MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering},
  author={Tang, Jingqun and Liu, Qi and Ye, Yongjie and Lu, Jinghui and Wei, Shu and Lin, Chunhui and Li, Wanqing and Mahmood, Mohamad Fitri Faiz Bin and Feng, Hao and Zhao, Zhen and others},
  journal={arXiv preprint arXiv:2405.11985},
  year={2024}
}

@article{tang2024textsquare,
  title={TextSquare: Scaling up Text-Centric Visual Instruction Tuning},
  author={Tang, Jingqun and Lin, Chunhui and Zhao, Zhen and Wei, Shu and Wu, Binghong and Liu, Qi and Feng, Hao and Li, Yang and Wang, Siqi and Liao, Lei and others},
  journal={arXiv preprint arXiv:2404.12803},
  year={2024}
}

@inproceedings{varley20156-DofGraspoing,
  title={Generating multi-fingered robotic grasps via deep learning},
  author={Varley, Jacob and Weisz, Jonathan and Weiss, Jared and Allen, Peter},
  booktitle={IROS},
  year={2015},
}

@inproceedings{wang2023event,
  title={Event-guided procedure planning from instructional videos with text supervision},
  author={Wang, An-Lan and Lin, Kun-Yu and Du, Jia-Run and Meng, Jingke and Zheng, Wei-Shi},
  booktitle={ICCV},
  pages={13565--13575},
  year={2023}
}

@article{wang2024pargo,
  title={Pargo: Bridging vision-language with partial and global views},
  author={Wang, An-Lan and Shan, Bin and Shi, Wei and Lin, Kun-Yu and Fei, Xiang and Tang, Guozhi and Liao, Lei and Tang, Jingqun and Huang, Can and Zheng, Wei-Shi},
  journal={arXiv preprint arXiv:2408.12928},
  year={2024}
}

@inproceedings{xu2024dexterous,
  title={Dexterous Grasp Transformer},
  author={Xu, Guo-Hao and Wei, Yi-Lin and Zheng, Dian and Wu, Xiao-Ming and Zheng, Wei-Shi},
  booktitle={CVPR},
  year={2024}
}

@article{zhao2024harmonizing,
  title={Harmonizing visual text comprehension and generation},
  author={Zhao, Zhen and Tang, Jingqun and Wu, Binghong and Lin, Chunhui and Wei, Shu and Liu, Hao and Tan, Xin and Zhang, Zhizhong and Huang, Can and Xie, Yuan},
  journal={arXiv preprint arXiv:2407.16364},
  year={2024}
}

@article{zhao2024tabpedia,
  title={Tabpedia: Towards comprehensive visual table understanding with concept synergy},
  author={Zhao, Weichao and Feng, Hao and Liu, Qi and Tang, Jingqun and Wei, Shu and Wu, Binghong and Liao, Lei and Ye, Yongjie and Liu, Hao and Zhou, Wengang and others},
  journal={arXiv preprint arXiv:2406.01326},
  year={2024}
}

