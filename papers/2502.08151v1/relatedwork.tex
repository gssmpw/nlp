\section{Related Work}
\begin{table*}
\centering
\caption{Comparison of existing sample reconstruction attacks in FL.}
\label{table-comparison-reference}
\begin{tblr}{rows={c,m}, hlines, hline{1,Z}={2pt}, column{1}={2.3cm}, column{3,4}={1.9cm}, column{5}={l}}
\SetCell[r=2]{c,m} Attack Type & \SetCell[r=2]{c,m} Mechanism & \SetCell[r=2]{c,m} Effectiveness in FL with LDP & \SetCell[r=2]{c,m} Sample Resolution & \SetCell[r=2]{m} Notes \\
& & & & \\
\SetCell[r=3]{c,m} Optimization-based Attack & DLG~\cite{Zhu2020DLG} & $\times$ & $ 64 \times 64$ &  The first effective reconstruction attack through gradients.\\
& Yin's work~\cite{Yin2021see}  & $\times$ & $ 224 \times 224$ & Considering multiple metrics for optimization.\\
& Pan's work~\cite{Pan2022Exploring} & $\times$ & $224 \times 224$ & Discussion about model complexity and attack effectiveness. \\
\SetCell[r=2]{c,m} Network-based Attack & mGAN-AI~\cite{Song2020Analyzing} & $\times$ & $ 64 \times 64$ & Samples must be identically distributed. \\
& GIAS~\cite{jeon2021gradient} & $\times$ & $ 64 \times 64$ & Applying the GAN to generate image prior. \\
\SetCell[r=2]{c,m} Analysis-based Attack & Fowl's work~\cite{fowl2022robbing} & $\times$ & $224 \times 224$ & Constructing bins to separate training samples.\\
&  Boenisch's work~\cite{Boenisch2021When} & $\times$ & $224 \times 224$ & Considering passive and active attacks in different scenarios.\\
Hybrid Attack & The Proposed Attack & $\surd$ & $224 \times 224$ & The sole effective reconstruction attack in FL with LDP.
\end{tblr}
\end{table*}

Sample reconstruction attacks are mainly divided into the following categories: optimization-based attacks (e.g., \cite{Zhu2020DLG, Yang2023compress, Hatamizadeh2022GradViT, Pan2022Exploring}), network-based attacks (e.g., \cite{Song2020Analyzing, jeon2021gradient, Khosravy2022Model, ganev2023inadequacy}), and analysis-based attacks (e.g., \cite{fowl2022robbing, Boenisch2021When, yuan2021beyond}). Table~\ref{table-comparison-reference} provides a brief comparison of these efforts.

\textbf{Optimization-based Attacks.}
In optimization-based attacks, the adversary regards reconstructed samples as multiple random variables and optimizes the reconstructed samples through gradients of objective functions. DLG~\cite{Zhu2020DLG} first proposed an optimization-based sample reconstruction attack, in which the objective function is the $\ell^2$ norm of the difference between victims' gradients and reconstructed samples-generated gradients. Other existing works proposed various improvements according to different requirements. For example, Yin et al.~\cite{Yin2021see} considered $\ell^2$ norm of the gradient difference, total variation of reconstructed samples, and sample statistics difference in the objective function to reconstruct samples with higher quality. Since the reconstructed samples are considered as multiple variables, the effectiveness of the optimization-based attacks depends heavily on the complexity of the sample (i.e., the number of random variables) and the complexity of the model (i.e., the number of constraints). When the batch size is large, finding the optimal solution (completely reconstructed samples) in optimization-based attacks is difficult.

\textbf{Network-based Attacks.}
The adversary in network-based reconstruction attacks trains a deep learning model to reconstruct victims' training samples. Most network-based attacks apply generative adversarial networks (GAN)~\cite{goodfellow2014GAN} to generate images similar to users' original samples. An attack mGAN-AI~\cite{Song2020Analyzing} modifies the global model with GAN in FL so that victims train the generator in the global model while training the classifier with local samples. The adversary reconstructs victims' training samples through the converged generator. GIAS~\cite{jeon2021gradient} applied GAN to transform the sample variables in the optimization problem into the input variables of the generator with lower dimensions, significantly reducing the search space and facilitating the finding of a better solution. Khosravy et al.~\cite{Khosravy2022Model} used a similar idea to successfully reconstruct victims' facial features in a face recognition system. A significant challenge for network-based attacks is to obtain enough samples with the same distribution as users' samples to train the generator.

\textbf{Analysis-based Attacks.} In analysis-based attacks, the adversary reconstructs training samples through the exact connection between gradients and training samples. Based on the theorem that gradients can calculate the input of FCL, Fowl et al.~\cite{fowl2022robbing} proposed to add a linear FCL to the global model and separate sample gradients so that most of the victims' samples can be reconstructed with high quality. Besides, based on the connection between gradients and training samples, Franziska et al.~\cite{Boenisch2021When} proposed a passive attack for reconstructing a single sample and an active attack for reconstructing multiple samples, where another idea for modifying the global model is given. The analysis-based attacks reconstruct high-quality training samples with low complexity and are still effective in the case of large batch sizes.

Some reconstruction attacks modify the global model structure to enable gradients to contain more sample information and improve the quality of reconstructed samples (e.g., \cite{fowl2022robbing, Boenisch2021When, Song2020Analyzing, Khosravy2022Model, ganev2023inadequacy}). Figure~\ref{fig:attack_type} compares sample reconstruction attacks using original and modified models. By embedding malicious structures in the global model, the gradients generated by modified models and client data carry additional sensitive information. Since the adversary reconstructs victims' samples through their gradient, attacks with modified models can achieve better attack effects than attacks with original models.

\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{attack_type.png}
\caption{Comparison of sample reconstruction attacks using original and modified models.}
\label{fig:attack_type}
\end{figure}

A disadvantage of attacks with modified models is causing gradient expansion, i.e., the norm of gradients increases since gradients carry additional information. Section~\ref{section-removing-redundate-gradients} provides a technical analysis for generating gradient expansion. Gradient expansion presents challenges when reconstructing samples in FL with LDP. Since gradients are clipped according to a fixed norm value and perturbed with noise, gradients with additional information are severely compressed. As a result, the signal-to-noise ratio in the protected gradient drops significantly. No existing works can effectively reconstruct samples from clipped and perturbed gradients. The proposed attack against FL with LDP modifies the global model to obtain effective performance while reducing the impact of gradient expansion by gradient compression presented in Sections~\ref{section-removing-redundate-gradients} and \ref{section-removing-background}.

In addition, existing attacks rely on accurate gradients to reconstruct samples. In FL with LDP, the noise in the perturbed gradient causes existing attacks to reconstruct meaningless noise samples. The proposed attack reduces the impact of perturbed gradients on the attack effect by filtering the noise of gradients and reconstructed samples presented in Sections~\ref{section-noise-filtering} and \ref{section-mectics-optimization}. Dealing with gradient clipping and perturbation in LDP by gradient compression and noise filtering, the proposed attack effectively reconstructs sensitive information in the sample from the protected gradients in FL with LDP.