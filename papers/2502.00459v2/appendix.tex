%!TEX root = main.tex
\clearpage
\section{Appendix}

\subsection{Evaluation metrics}
We detail the four evaluation metrics discussed in the main manuscript. $\textbf{Z}_i$ represents the audio token sequence generated by a text-to-audio model given the $i$-th text prompt in the dataset. The sequences $(\tilde{\textbf{Z}}_{F})_{i}$ and $(\tilde{\textbf{Z}}_{CF})_{i}$ are produced by applying factual and counterfactual masks, respectively, as defined in Eqs.~\eqref{attx} and \eqref{att_cf}. These masks serve as explanations for both $\textbf{Z}_i$ and its associated text prompt.
To evaluate these explanations, we use the pre-trained audio classifier PaSST~\cite{koutini2021efficient}, denoted as $q$, which generates a prediction probability distribution $q(\textbf{Z})$. Specifically, $q(\textbf{Z})_{y_i}$ represents the prediction probability for class $y_i$. Finally, $N$ is the total number of data points, $L$ is the number of text tokens in the text prompt, and $T$ is the total length of the audio token sequence. The evaluation metrics $Fid_{F}$, $Fid_{CF}$~\cite{yuan2021explainability, yuan2022explainability, ali2023explainable} are defined as:
\begin{equation}
Fid_{F} =  \frac{1}{N}\sum^{N}_{i=1}q(\textbf{Z}_{i})_{y_i} - q((\tilde{\textbf{Z}}_{F})_i)_{y_i},
\end{equation}
\begin{equation}
Fid_{CF} =  \frac{1}{N}\sum^{N}_{i=1}q(\textbf{Z}_i)_{y_i} - q((\tilde{\textbf{Z}}_{CF})_i)_{y_i},
\end{equation}

\noindent where $y_i$ is the predicted class for $q(\textbf{Z}_{i})$, defined as $y_i = \arg\max_{c \in C} q(\textbf{Z}_{i})_c$, and $C$ is the set of all classes that $q$ predicts. Moreover, $KL_{F}$, $KL_{CF}$~\cite{kreuk2022audiogen, yang2023diffsound, huang2023make}, and $Size$ are defined as:
% \end{equation*}
\begin{equation}
KL_{F} = \frac{1}{N}\sum^{N}_{i=1} D_{\text{KL}} \left( q(\textbf{Z}_{i}) \, \| \, q((\tilde{\textbf{Z}}_{F})_i) \right),
\end{equation}
\begin{equation}
KL_{CF} = \frac{1}{N}\sum^{N}_{i=1}D_{\text{KL}} \left( q(\textbf{Z}_i) \, \| \, q((\tilde{\textbf{Z}}_{CF})_i) \right),
\end{equation}
\begin{equation}
    Size = \frac{1}{N \times L \times T} \sum_{i=1}^{N} \sum_{l=1}^{L} \sum_{t=1}^{T} (\textbf{m}_{\textbf{u}_{l}, \textbf{z}_{t}})_{i}.
\end{equation}
Here, $D_{KL}$ refers to the KL divergence.
bsection{Details on baseline setting}
While there is no existing XAI model specifically designed for explaining text-to-audio generative models, we adopt Transformer explanation methods for evaluation.

\textbf{Rollout}~\cite{abnar2020quantifying}, a method for explaining Transformers, proposes aggregating attention scores recursively by multiplying attention maps in all layers. The proposed method named rolling out the attention weights is formulated as below:
\begin{equation}
\tilde{A}(l_i) = 
\begin{cases} 
A(l_i)\tilde{A}(l_{i-1}) & \text{if } i > j \\ 
A(l_i) & \text{if } i = j,
\end{cases}
\end{equation}
\noindent where $\tilde{A}$ means attention rollout. $A(l_i)$ represents ${i}$-th raw attention map. However, applying Rollout in models with cross-attention blocks designed to handle multi-modality is challenging because the dimensions of the attention maps do not match. Therefore, we exclude Rollout from our baselines. 

\textbf{Grad-CAM}~\cite{selvaraju2017grad} computes the gradients of the output activations from the target layer with respect to the final prediction. The importance map is then calculated as:

\begin{equation}
\tilde{A} = \mathbb{E}((\nabla A \odot A)^{+}),
\end{equation}
where $A$ represents the output activations of the target layer, and $\nabla A$ represents the gradient of these activations with respect to the prediction. Specifically, $\nabla A$ comprises the gradients computed for each selected codebook, which are averaged afterward. $\odot$ denotes element-wise multiplication, and $(\cdot)^{+}$ extracts positive values. 
$\textbf{Grad-CAM-a}$ calculates gradient of the latent representation vector $\textbf{e}_t$, corresponding to the $t$-th audio token. The 1,536 dimensions of the audio token embedding are treated as separate channels, and their mean is used to compute the token importance score. $\textbf{Grad-CAM-e}$ derives the gradient of the last cross-attention map, and their mean is used as the token importance score.

\textbf{AtMan}~\cite{deiseroth2023atman} extracts important tokens by perturbation of a single token. For all cross-attention layers and all heads, we multiply \(1-k\) with the pre-softmax attention scores for the target text tokens to introduce perturbation. The value $k$ is consistently set to 0.9, following the configuration in~\cite{deiseroth2023atman}. To quantify the influence of tokens, we calculate the difference in cross-entropy for each codebook and use the sum of these differences as the token importance.

\textbf{Chefer et al.}~\cite{chefer2021generic} calculates the relevance score, following attention layers. In our experiments, we follow ~\cite{chefer2021generic}. However, since we do not consider the influence of the text encoder, we replace it with an identity matrix.

To scale importance values between 0 and 1, we apply Max scaling for each sequence for all baselines except AtMan, For AtMan, which includes negative values, we use Min-Max scaling.


\subsection{Experimental Setting}
In our experiments, we used the following packages and hardware:

\begin{itemize}
    \item \texttt{Python 3.9.18}
    \item \texttt{spacy==3.5.2}
    \item \texttt{torch>=2.1.0}
    \item \texttt{torchaudio>=2.1.0}
    \item \texttt{Transformers>=4.31.0}
\end{itemize}

All computations were performed using a single NVIDIA A100 GPU.

\subsection{Hyperparameter Sensitivity Analysis}
\begin{figure}
    \center
    \includegraphics[width=1.0 \linewidth]{fig/hyperparameter.pdf}
    \caption{Sensitivity analysis of the hyperparameters $\alpha$ and $\beta$. (a) Effect on $Fid_{F}$, (b) Effect on $Size$.}
    \label{hyper}
\end{figure}

We conduct a hyperparameter sensitivity analysis using various combinations of hyperparameters on the validation dataset. The validation dataset is randomly sampled from the validation set of AudioCaps~\cite{kim2019audiocaps}. As illustrated in Figure~\ref{hyper}-(a), when the value of $\alpha$ decreases from 0.1 to 0.001, $Fid_{F}$ significantly decreases from 0.398 to 0.138. Furthermore, when $\beta$ is adjusted from 0.1 to 0.001 while keeping $\alpha = 0.001$, $Fid_{F}$ shows a slight increase from 0.138 to 0.142. This suggests that lowering $\beta$ can slightly enhance fidelity, but the effect is marginal compared to that of $\alpha$. In contrast, the impact on $Size$, shown in Figure~\ref{hyper}-(b), reveals a different trend. As $\alpha$ decreases from 0.1 to 0.001, $Size$ increases substantially from 0.07 to 0.51, indicating that lower $\alpha$ values lead to greater model complexity. Similarly, when $\beta$ is reduced from 0.1 to 0.001 with a fixed $\alpha = 0.001$, $Size$ further increases from 0.426 to 0.517. This demonstrates that both $\alpha$ and $\beta$ reductions tend to increase the mask size. These results indicate a trade-off between fidelity and size. To ensure fair comparisons with the baselines while maintaining a comparable mask size, we set $\alpha = 0.001$ and $\beta = 0.1$ based on this analysis.

\begin{figure*}
    \center
    \includegraphics[width=1.0 \linewidth]{fig/research_question1_appndix.pdf}
    \caption{Qualitative analysis of \mname{} in comparison with baseline methods.}
    \label{experiment_appendix}
\end{figure*}

\begin{table*}[!ht]
    \centering
    \begin{tabular}{lllll|lllll}
    \toprule
      & \multicolumn{4}{c|}{Top 50 high-importance tokens}& \multicolumn{4}{c}{Top 50 low-importance tokens} \\
        %Index & Top 100 important tokens & ~ & ~ & ~ & Bottom 100 important tokens & ~ & ~ & ~ \\ 
        Index & Textual token & Avg. Impt & Count & POS & Index & Textual token & Avg. Impt & Count & POS \\ 
        \midrule
        1 & sewing & 0.987 & 20 & VBG & 1 & ground & 0.037 & 11 & NN \\ 
        2 & horse & 0.976 & 13 & NN & 2 & series & 0.048 & 13 & NN \\ 
        3 & emergency & 0.957 & 13 & NN & 3 & electronic & 0.060 & 17 & JJ \\ 
        4 & ren & 0.946 & 18 & NNS & 4 & for & 0.086 & 12 & IN \\ 
        5 & baby & 0.939 & 22 & NN & 5 & before & 0.107 & 11 & IN \\ 
        6 & thunder & 0.936 & 18 & NN & 6 & background & 0.110 & 107 & NN \\ 
        7 & toilet & 0.936 & 13 & VBP & 7 & then & 0.116 & 173 & RB \\ 
        8 & soft & 0.932 & 12 & JJ & 8 & repeatedly & 0.119 & 12 & RB \\ 
        9 & rog & 0.931 & 13 & NN & 9 & while & 0.129 & 86 & IN \\ 
        10 & foot & 0.931 & 12 & NN & 10 & into & 0.130 & 42 & IN \\ 
        11 & app & 0.931 & 15 & NN & 11 & some & 0.139 & 45 & DT \\ 
        12 & food & 0.915 & 17 & NN & 12 & another & 0.141 & 19 & DT \\ 
        13 & step & 0.898 & 12 & NN & 13 & over & 0.146 & 19 & IN \\ 
        14 & infant & 0.895 & 13 & NN & 14 & distance & 0.163 & 49 & NN \\ 
        15 & ack & 0.893 & 13 & NN & 15 & with & 0.167 & 174 & IN \\ 
        16 & talking & 0.892 & 122 & VBG & 16 & the & 0.167 & 204 & DT \\ 
        17 & crying & 0.883 & 27 & VBG & 17 & followed & 0.185 & 277 & VBD \\ 
        18 & cla & 0.878 & 33 & NN & 18 & ongoing & 0.196 & 10 & VBG \\ 
        19 & pig & 0.875 & 13 & IN & 19 & loud & 0.224 & 53 & JJ \\ 
        20 & laughter & 0.873 & 18 & NN & 20 & are & 0.228 & 30 & VBP \\ 
        21 & goat & 0.873 & 12 & NN & 21 & and & 0.233 & 568 & CC \\ 
        22 & clo & 0.863 & 10 & NN & 22 & power & 0.250 & 12 & NN \\ 
        23 & pour & 0.863 & 10 & VBP & 23 & occurs & 0.260 & 15 & VBZ \\ 
        24 & duck & 0.859 & 12 & NN & 24 & sounds & 0.264 & 25 & NNS \\ 
        25 & door & 0.857 & 26 & NN & 25 & pitched & 0.264 & 12 & VBN \\ 
        26 & tapping & 0.848 & 11 & VBG & 26 & surface & 0.269 & 31 & NN \\ 
        27 & footsteps & 0.846 & 11 & NNS & 27 & several & 0.270 & 37 & JJ \\ 
        28 & bus & 0.845 & 11 & NN & 28 & from & 0.279 & 22 & IN \\ 
        29 & clicking & 0.842 & 14 & VBG & 29 & king & 0.287 & 62 & VBG \\ 
        30 & truck & 0.838 & 13 & NN & 30 & steam & 0.308 & 16 & JJ \\ 
        31 & scrap & 0.834 & 16 & JJ & 31 & sound & 0.309 & 18 & JJ \\ 
        32 & crowd & 0.823 & 31 & NN & 32 & through & 0.313 & 13 & IN \\ 
        33 & speaks & 0.817 & 97 & NNS & 33 & down & 0.325 & 11 & RP \\ 
        34 & boat & 0.817 & 12 & NN & 34 & two & 0.343 & 17 & CD \\ 
        35 & cat & 0.815 & 14 & NN & 35 & light & 0.351 & 13 & JJ \\ 
        36 & explosion & 0.813 & 12 & NN & 36 & ing & 0.352 & 342 & VBG \\ 
        37 & woman & 0.811 & 104 & NN & 37 & runs & 0.353 & 18 & NNS \\ 
        38 & music & 0.811 & 33 & NN & 38 & les & 0.353 & 16 & NNS \\ 
        39 & clan & 0.810 & 24 & NN & 39 & microphone & 0.355 & 48 & NN \\ 
        40 & speech & 0.810 & 44 & JJ & 40 & high & 0.359 & 20 & JJ \\ 
        41 & whistle & 0.803 & 14 & JJ & 41 & ting & 0.361 & 14 & VBG \\ 
        42 & water & 0.799 & 82 & NN & 42 & ses & 0.363 & 12 & VBZ \\ 
        43 & speaking & 0.794 & 163 & VBG & 43 & his & 0.364 & 35 & PRP\$ \\ 
        44 & men & 0.789 & 21 & NNS & 44 & ling & 0.364 & 160 & VBG \\ 
        45 & train & 0.785 & 35 & VBP & 45 & ving & 0.375 & 30 & VBG \\ 
        46 & rain & 0.782 & 29 & NN & 46 & metal & 0.377 & 45 & VBP \\ 
        47 & laughing & 0.778 & 40 & VBG & 47 & end & 0.382 & 1003 & NN \\ 
        48 & flush & 0.769 & 14 & NN & 48 & small & 0.383 & 12 & JJ \\ 
        49 & helicopter & 0.768 & 12 & NN & 49 & tires & 0.384 & 11 & NNS \\ 
        50 & talks & 0.766 & 31 & NNS & 50 & motor & 0.417 & 49 & NN \\         
        \bottomrule
    \end{tabular}
    \caption{Averaged importance (Impt) per textual token learned by \mname{}. The name of the POS (Part of Speech) is followed by the categories in NLTK. Count refers to the occurrence frequency in the test dataset of AudioCaps.}
    \label{tabler3}
\end{table*}

\begin{figure*}
    \center
    \includegraphics[width=1.0 \linewidth]{fig/sanity_check.pdf}
    \caption{Examplar explanations using independent randomization on Transformer-decoder of AudioGen as the sanity check.}
    \label{sanitycheck}
\end{figure*}

\subsection{Leveraging Explanations to Understand Model Behavior and Edit Audio}
\label{casestudy}

\mname{} enhances transparency but also provides valuable insight for debugging TAG models and editing sound. In Table~\ref{tabler3}, we investigate the patterns of AudioGen using explanations generated by \mname{}. We first aggregated our explanations from the experiments in Table~\ref{tabler3}. Then, we filtered out tokens with a length greater than 1 and an occurrence frequency exceeding 10. From this subset, we selected the top 100 tokens with the highest average importance (Avg. Impt) and the bottom 100 tokens with the lowest average importance to generate word clouds. Detailed information on these tokens is presented in Table~\ref{tabler3}. The tokens in the top 100 predominantly consist of nouns (NN) and tokens that are associated with sounds. In contrast, the tokens in the bottom 100 displayed a diverse range of parts of speech, including adverbs (RB) and prepositions (IN), which tend to convey context rather than having intrinsic auditory significance. Additionally, as noted in the AudioGen documentation~\cite{kreuk2022audiogen}, tokens related to numbers (CD) or sequences also exhibit a lower importance.

Next, we utilize our explanations in the task of editing generated audio when it misaligns with the user's intended prompt. For example, given the prompt `Wind blows hard followed by screaming,' the generated audio should reflect this sequence. However, in our example, AudioGen fails to accurately capture the `screaming' sound. Using \mname{}, we find that `screaming' has low importance, especially in the latter part of the audio where it should be emphasized. To correct this, we used a technique in a study~\cite{hertz2022prompt} that adjusts attention weights to better align the audio with the intended prompt. The method of importance re-weighting is described as follows:

\begin{equation}
\textbf{M}^{*}_{\textbf{U}_{l}, \textbf{z}_{t}} := 
\begin{cases} 
c & \text{if } l=l^{*} \text{ and } t = t^{*}, \\ 
\textbf{M}_{\textbf{U}_{l}, \textbf{z}_{t}} & \text{otherwise},
\end{cases}
\end{equation}


\begin{table}[ht]
\centering
\begin{tabular}{l|l|l}
\toprule
   & FAD $\downarrow$ & $KL_{F} \downarrow$  \\ \hline
Before edit & 16.85 & 6.45 \\ 
After edit  & 2.68 & 1.82 \\ 
\bottomrule
\end{tabular}
\caption{Evaluation of editing generated audio}
\label{tab:edit}
\end{table}

\noindent where the explanation mask value $\textbf{M}_{\textbf{U}_{l}, \textbf{z}_{t}}$ from \mname{} is reweighted to $\textbf{M}^{*}_{\textbf{U}_{l}, \textbf{z}_{t}}$. In this case, $l^*$ and $t^*$ denote the target indices of the text token and the audio token, respectively.
When amplifying the explanation mask value, we set the scaling parameter $c$ to 0.9. Conversely, when suppressing the impact of the token, $c$ is set to 0.1. 
The threshold values (0.9 and 0.1) are chosen based on our heuristic intuition in Table~\ref{tabler3} that the importance of the top and bottom ranking is greater than 0.9 and less than 0.1, respectively.
For evaluation, we randomly sampled 100 prompts and identified 30 failure cases where AudioGen-generated audio differs from the ground truth in the AudioCaps~\cite{kim2019audiocaps} data set. To analyze these outputs, we applied \mname{} to identify which textual tokens were over- or underemphasized, then manually adjusted the importance of the tokens to align with the ground truth. 
As evaluation metrics, we compute the \text{Fr\'echet Audio Distance} (FAD)~\cite{fad} over both real and generated audio. FAD is an adaptation of the Fr\'echet Inception Distance (FID) for audio, measuring the similarity between distributions of real and generated audio data. Additionally, we measure the metric $KL_F$.

Table~\ref{tab:edit} demonstrates that editing the importance mask allows us to generate audio that more closely matches the ground truth. For the setup, we randomly sampled 100 prompts, finding 30 failure cases that differed from the ground truth in the AudioCaps data set. Initially, the scores (FAD and KL) were lower than typical ones, at 3.13 and 2.09 in MAGNeT~\cite{ziv2024masked} known as the SOTA model. To understand the generated output, we used \mname{} to identify which textual tokens were over- or underemphasized, then manually adjusted the importance of the tokens to amplify or suppress. Figure~\ref{fig:edit} further illustrates that editing can be applied to specific time intervals. For instance, after re-weighting the mask values between 2.5 and 5 seconds, the ‘screaming’ audio emerges in the corresponding time interval. Although explanations do not directly involve generation, \mname{} helps users by offering valuable guidance during the editing process when there is a discrepancy between the user’s intention and the generated result.

\begin{figure}
    \center
    \includegraphics[width=1.0 \linewidth]{fig/edit_appendix.pdf}
    \caption{The scenario of editing the generated audio.}
    \label{fig:edit}
\end{figure}

\subsection{Sanity Check}
We conduct a sanity check following the approach in \cite{adebayo2018sanity} to assess the explanations generated. Specifically, we initialize the parameters of the Transformer-decoder, which predicts the next sequence of audio tokens. As shown in Figure~\ref{sanitycheck}, when the model is initialized randomly, the influence of each token in the visualization becomes nearly indistinguishable. This result of the explainer, including our baseline, in response to the state of the model parameters, suggests that \mname{} produces faithful explanations. Thus, we conclude that \mname{} generates reliable and trustworthy explanations, as validated by the sanity check.

\subsection{Limitation}
\label{app:limication}
While we introduce a novel approach to explaining generated audio in TAG models, there are some limitations to consider. First, \mname{} contains several hyperparameters that may require data set-specific tuning for optimal performance. Automating this process or reducing hyperparameter sensitivity would improve usability. Furthermore, biases present in the training data may be reflected in both the generated audio and the explanations. Without proper safeguards and responsible deployment practices, these biases could reinforce harmful stereotypes. As research into audio generation progresses, it is crucial to proactively develop robust bias detection methods and advocate for the ethical use of these powerful approaches. Despite these limitations and considerations, we believe that \mname{} represents a valuable step toward improving the interpretability and trustworthiness of TAG models.
