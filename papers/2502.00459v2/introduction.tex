%!TEX root = main.tex
\section{Introduction}

Text-to-audio generation models (TAG)~\cite{kreuk2022audiogen, ziv2024masked, yang2023diffsound, liu2023audioldm,  schneider2023mo} have emerged as a pivotal technology in generative AI, enabling textual content to be transformed into an auditory experience. Although models such as AudioGen~\cite{kreuk2022audiogen} excel at generating high-quality audio based on textual prompts, a critical challenge remains: the lack of transparency in how each textual input affects the generated audio. Consequently, users may struggle to trust the model, making it essential to provide explanations for the TAG task. Explainability provides several key advantages. First, it enhances awareness of how input tokens affect the model’s outputs, enabling users to ensure that the model emphasizes the correct aspects of the text. Second, it provides actionable insights to support the decision-making about which elements to modify and to what extent in the audio editing process. Third, analyzing generated explanations can aid with debugging and identifying potential biases. Accordingly, this study argues that the ability to quantify the importance of textual inputs in TAG models is crucial to being able to unambiguously assess and communicate their value.

\begin{figure}
    \center
    \includegraphics[width=\columnwidth] {fig/intro.pdf}
    \caption{A comprehensive explanation provided by \mname{} for the entire audio in (a). Granular explanations for the interval from 1 to 1.5 seconds in (b) and from 2.5 to 3 seconds in (c), respectively.}
    \label{intro}
\end{figure}

While approaches specifically tailored for explaining TAG models are limited, recent research has explored methodologies for calculating the importance of input tokens in large-scale transformer-based models. Cross-attention layers in multi-modal architectures, such as those in TAG models, are widely regarded as critical for integrating textual and auditory information, while also enhancing explainability by revealing how information from one modality influences another. A notable method~\cite{abnar2020quantifying} utilizes attention weights and aggregates them across all layers to approximate the importance of each input token. However, attention scores alone are not considered reliable for causal insights, as they do not directly indicate how perturbation to specific inputs influences the output. Recently, AtMan~\cite{deiseroth2023atman} introduced a perturbation method that suppresses the attention score of one token at a time to observe the impact of each input on output prediction. This single-token perturbation approach, however, may overlook interactions between multiple tokens. Consequently, it provides less reliable explanations in scenarios where the model heavily relies on the contextual relationships between multiple tokens, leading to an oversimplification of the model’s behavior.

To address the challenge of faithful explanations, causal inference theory, encompassing factual and counterfactual reasoning, is often utilized~\cite{pearl2009causal}. These two approaches aim to identify impactful input information in different ways. Factual reasoning focuses on identifying critical input information that reproduces the original prediction, whereas counterfactual reasoning~\cite{tan2022learning, ali2023explainable, kenny2021post} seeks to determine crucial input information that, if absent, would change the prediction. Given their differing assumptions, these reasoning approaches can be employed together as complementary frameworks to generate more faithful explanations. However, prior research has yet to investigate the feasibility of applying factual and counterfactual reasoning within TAG models.

To provide faithful explanations for the TAG model, we introduce \mname{}, a perturbation-based explainability method leveraging factual and counterfactual reasoning. Our approach utilizes the latent representation vectors in TAG models to observe the effects of factual and counterfactual perturbations. These perturbations are applied in the cross-attention layer using a soft mask, enabling the simultaneous perturbation of multiple tokens' attention scores. More importantly, the mask itself serves as an explanation, with its values quantifying the importance of the textual input. We optimize the mask through a gradient descent method guided by our proposed factual and counterfactual objective functions. To mitigate the high computational cost of calculating gradients for the entire sequential audio, we enhance efficiency by decomposing the explanation target into individual audio tokens. This approach enables us to customize the explanation range of generated audio interactively, providing comprehensive explanations for the entire audio or more granular explanations for specific segments of interest, depending on user demand. For instance, in Figure~\ref{intro}, (a) provides a comprehensive explanation for the entire audio, indicating a strong relation to vehicle motion. By focusing on a specific interval in (b) and (c), \mname{} captures the different contexts of each audio segment and delivers contextually accurate explanations accordingly. Extensive experiments demonstrate the faithfulness of our explanations and benchmark their performance against recent techniques using proposed evaluation metrics for audio generation tasks.

\textbf{Contributions.} We summarize our contributions as follows: 1) We propose a faithful explanation method for text-to-audio generation models, grounded in factual and counterfactual reasoning to quantify the importance of text tokens to the generated audio. 2) We offer a framework that provides both holistic and granular audio explanations based on user requests, enabling tailored insights. 3) 
We introduce new evaluation metrics for text-to-audio explanations and demonstrate the effectiveness of \mname{}through extensive experiments compared to existing methods. 4) We present case studies demonstrating how \mname{} provides valuable insights to support the understanding of model behavior and editing tasks.