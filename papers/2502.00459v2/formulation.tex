\section{Preliminaries}
AudioGen~\cite{kreuk2022audiogen}, a representative TAG model, consists of three key components: a text encoder~\cite{raffel2020exploring}, an autoregressive Transformer decoder model~\cite{vaswani2017attention}, and an audio decoder~\cite{defossez2022high}. The Transformer decoder serves as the core model responsible for generating the audio sequence, while the text encoder processes the input text and the audio decoder post-processes the generated audio token sequence into audio. Given a text prompt, it is converted into a tokenized representation vector, denoted as $\textbf{U} = [\textbf{u}_{1}, \dots, \textbf{u}_{L}], \textbf{U} \in \mathbb{R}^{L \times d_{u}}$, where $L$ denotes number of textual tokens and $d_{u}$ represents a dimension of the textual token representation vector. The generated audio can be expressed in a discrete form, as EnCodec~\cite{defossez2022high} converts the audio into either discrete tokens or continuous token representations. The tokenized audio sequence is denoted as $\textbf{Z} = [\textbf{z}_{1}, \dots, \textbf{z}_{T}], \textbf{Z} \in \mathbb{N}^{T \times d_{v}}$, where $T$ denotes the length of the audio sequence and $d_{v}$ indicates the number of codebooks $d_{v}$. In detail, the codebook is a structured set of discrete audio tokens used in multi-stream audio generation to produce high-quality audio. For more comprehensive information on multi-streaming audio generation, we refer to the original AudioGen paper~\cite{kreuk2022audiogen}. 

For the generation of an audio sequence, the Transformer-decoder model ~\cite{vaswani2017attention}, denoted as $h$, generates $\textbf{z}_{t}$ as $t$-th order audio token in the sequence, following the formulation $h(\textbf{U}, \textbf{z}_{t-1}) = \textbf{z}_{t}$. For brevity, we omit the detailed notation of other components and the top-$p$ or top-$k$ sampling process in the Transformer. Instead, we focus on the attention layers, including cross-attention, which are crucial components of the model, denoted as $f$. The computation within these layers is expressed in a simplified version as $f(\textbf{U}, \textbf{z}_{t-1}) = \textbf{e}_{t}$, where $\textbf{e}_{t}$ represents the latent representation vector corresponding to the $t$-th audio token. In the absence of ground truth and class labels, the latent embedding vector $\textbf{e}_{t}$ in the audio token space provides information on how perturbation impacts subsequent generations. Particularly, the cross-attention layer is essential to fuse the textual information with auditory information in layers $f$, we denote the cross-attention layers as:
\begin{equation}
\displaystyle g(\textbf{Q}, \textbf{K}, \textbf{V}) =  \sigma \left(\frac{\textbf{Q} \textbf{K}^\intercal}{\sqrt{d_k}}\right)\textbf{V},
\label{att}
\end{equation}
where $\sigma$ indicates a softmax function, $\textbf{Q}, \textbf{K}, \textbf{V}, d_k$ refers to query, key, values, and the number of vector dimensions in the $k$-th layer, respectively. In detail, $\textbf{Q}$ refers to previously generated audio tokens, representing the query information, while$\textbf{K}$ and $\textbf{V}$ correspond to the textual tokens.

\section{The Proposed \mname{}}
\mname{} addresses the challenge of explaining TAG models, where the goal is to quantify the importance of textual input corresponding to the generated audio. To achieve this within a sequence-to-sequence framework, we decompose the explanation target, represented as sequential audio, into individually non-sequential audio tokens. Since the output is sequential data, calculating gradients across the entire sequence, from the first to the last token, is computationally expensive and time-consuming. To overcome these issues, we redefine the explanation target as individual audio tokens, rather than the entire sequence. This modification enables parallel computation of generating an explanation for each token, significantly speeding up the process. Finally, \mname{} integrates these individual token-level explanations to provide a comprehensive understanding of the entire audio sequence. An overview of \mname{} is illustrated in Figure~\ref{overview}.

\subsection{Definition of Masks as Explanations} 
We quantify the importance of the $t$-th audio token $\textbf{z}_{t}$ within the audio sequence using a mask as the explanation. The soft mask is denoted as $\textbf{M}_{\textbf{U}, \textbf{z}_{t}} \in \mathbb{R}^{L \times 1}$, where each element $\textbf{m}_{\textbf{u}_{i}, \textbf{z}_{t}} \in \textbf{M}_{\textbf{U}, \textbf{z}_{t}}$ represents the importance of the $i$-th textual token with respect to the $t$-th audio token $\textbf{z}_{t}$. Each value lies in the range $[0, 1]$, where a value close to 1 indicates that the corresponding textual token is highly important for generating the target audio token, while a value closer to 0 indicates lower importance. To serve as a soft mask representing the importance of each text token, \mname{} optimizes the $Explainer$ to predict the mask $\textbf{M}_{\textbf{U}, \textbf{z}_{t}}$ as the explanation. The $Explainer$ consists of Multi-Layer Perceptrons (MLPs) with a sigmoid and gumbel-softmax~\cite{jang2016categorical} function to constrain values within the range $[0, 1]$ without additional scaling and to enforce the values close to either $0$ or $1$, thereby highlighting relatively distinguished contribution. Using the soft mask, we apply perturbation to modify the inner computational steps of the cross-attention layers, altering the attention score of the given textual input. Consequently, we measure the perturbation effect on the prediction at the layer $f(\textbf{U}, \textbf{z}_{t-1}) = \textbf{e}_{t}$, observing how latent representation vector $\textbf{e}_{t}$ for the audio token $\textbf{z}_{t}$ changes under these perturbations. In the following section, we detail how we optimize $Explainer$ to predict the mask as explanations based on both factual and counterfactual reasoning.
