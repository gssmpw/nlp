\section{Related Work}
\label{sec:related}

\textbf{Text-to-Audio Generation Models.} Recent text-to-audio generation models can be categorized into two model architectures: Transformer-based~\cite{kreuk2022audiogen, ziv2024masked} and Diffusion-based~\cite{yang2023diffsound, liu2023audioldm, schneider2023mo}. Transformer models, such as AudioGen~\cite{kreuk2022audiogen}, employ autoregressive Transformers to predict discrete audio tokens, while MAGNeT~\cite{ziv2024masked} enhances efficiency through masked generative modeling in a non-autoregressive scheme. Diffusion-based approaches such as Diffsound~\cite{yang2023diffsound} generate discrete mel-spectrogram tokens, whereas models like AudioLDM~\cite{liu2023audioldm} and Mo√ªsai~\cite{schneider2023mo} directly predict continuous mel-spectrograms or waveforms. Despite architectural differences, these models commonly use cross-attention mechanisms, making \mname{} a model-agnostic explainer for TAG models that use cross-attention in audio generation.

\textbf{Explainable AI.}  Explainability involves methods that help to understand the importance of each input token with respect to output predictions. These methods generally fall into two categories: gradient-based methods~\cite{selvaraju2017grad, sundararajan2017axiomatic, nagahisarchoghaei2023empirical} and perturbation-based methods~\cite{ribeiro2016should, lundberg2017unified}. Gradient-based explanation methods trace gradients from the target layers to the predictive value, using the calculated gradients as a measure of importance. While effective, these methods require substantial memory resources to store the values of each targeted layer. In contrast, perturbation-based methods, such as SHAP~\cite{lundberg2017unified}, are more memory-efficient, calculating feature importance by comparing predictions with and without specific features. Similarly, our method adopts a perturbation-based approach to effectively generate explanations.

\textbf{Explainability on Audio Processing Models.} Existing explainability approaches~\cite{akman2024audio} on audio processing models have extended generic explanation methods. For instance, one study~\cite{becker2018interpreting} employs Layer-wise Relevance Propagation (LRP) to explain the model trained on raw waveforms and spectrograms for spoken digit and speaker gender classification. Another study applied DFT-LRP~\cite{frommholz2023xai} to audio event detection, assessing the significance of time-frequency components and guiding input representation choices. Similarly, audioLIME~\cite{haunschmid2020audiolime} extends LIME~\cite{ribeiro2016should} to explain music-tagging models by perturbing audio components derived from source separation. However, since the above methods focus on explaining audio continuously and sequentially, they are not directly applicable to the unique challenges posed by TAG models, which require techniques that address the complex interactions between text inputs and generated audio outputs.

\textbf{Explainability on Transformer.} With the widespread use of Transformers, the demand for explainability has grown. Primarily, Rollout~\cite{abnar2020quantifying} primarily aggregates attention weights in all layers to track information flow but struggles to integrate cross-attention weights in multi-modal models with differing domain dimensionalities. Another recent work~\cite{chefer2021generic} leverages Layer-wise Relevance Propagation (LRP)~\cite{samek2017explainable} to calculate class-specific relevance scores based on gradients of attention weights in self- and cross-attention layers. Nevertheless, AtMan~\cite{deiseroth2023atman} raises the issue of excessive memory usage and introduces a scalable explanation method that employs single-token perturbation to observe the change of loss in the response. While intuitive and memory-efficient for large-scale models, this method is limited in its ability to account for the interrelationship of input tokens.