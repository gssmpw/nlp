%!TEX root = main.tex
\section{Experimental Setup}
\label{exp-setup}

\textbf{Dataset.} We use AudioCaps~\cite{kim2019audiocaps} as the source of textual prompts. For each prompt, we generate a 5-second audio clip using AudioGen, pairing each prompt with its corresponding generated audio. For hyperparameter tuning, we select 100 validation captions, while the test dataset consists of 1,000 randomly selected captions.

\textbf{Evaluation Metrics.} We evaluate explanations based on two metrics: Fidelity and KL divergence, both derived from the classification probabilities of a pre-trained audio classifier. Specifically, we utilize PaSST~\cite{cai2022efficient}, a classifier trained on the AudioSet dataset, which is also used in the evaluation of AudioGen. Its classification probabilities are likely to provide meaningful insights into the relationship between textual prompts and generated audio. Fidelity~\cite{yuan2021explainability, ali2023explainable}, a core evaluation metric in the field of XAI, measures the change in top-1 label prediction probabilities of the generated audio after applying factual and counterfactual explanation masks, denoted as $Fid_{F}$ and $Fid_{CF}$, respectively. 

In addition, KL divergence~\cite{kilgour2018fr}, originally used to evaluate audio generative models~\cite{kreuk2022audiogen, yang2023diffsound, huang2023make}, measures the differences of label distribution between generated and reference audio. For explanation evaluation, we introduce new metrics $KL_{F}$ and $KL_{CF}$, which measure the conceptual change in the generated audio after applying explanation masks in factual and counterfactual reasoning, respectively. In factual evaluation, the generated audio should closely match the original audio, making lower values $Fid_{F}$ and $KL_{F}$ desirable. In contrast, in counterfactual evaluation, higher values of $Fid_{CF}$ and $KL_{CF}$ indicate a more effective explanation. Additionally, we include the average mask size as part of our evaluation.

\textbf{Baselines.} We compare our method with five baselines. Random-Mask is a mask with randomly assigned values ranging between 0 and 1. Grad-CAM~\cite{selvaraju2017grad} is evaluated in two variations: $\text{Grad-CAM-a}$ and $\text{Grad-CAM-e}$. Specifically, $\text{Grad-CAM-a}$ computes the gradients of the latent representation vector of the $t$-th audio token $\textbf{e}_t$ with respect to the generated audio sequence $\textbf{z}_t$, while $\text{Grad-CAM-e}$ computes the gradients of the last cross-attention map to the $\textbf{z}_t$. We also include the AtMan~\cite{deiseroth2023atman} and the method proposed by Chefer et al.~\cite{chefer2021generic} as baselines.

\textbf{Experimental Setting.} The $Explainer$ model includes a linear layer that reduces the text token embeddings from 1536 to 512 dimensions, followed by a PReLU activation function. The 512-dimensional text token embeddings are then mapped to a single value through another linear layer and a sigmoid function, producing a value in the $[0, 1]$ range. A Gumbel-Softmax function is subsequently applied to push values closer to 0 or 1, representing the importance of each text token. The $Explainer$ is trained for 50 epochs with a learning rate as $\times 10^{-3}$ using the Adam optimizer. Hyperparameters are set as $\alpha = 1 \times 10^{-3}$ and $\beta = 1 \times 10^{-1}$ as coefficients for the explanation objective function. Hyperparameter sensitivity analysis and detailed experimental settings are both provided in the Appendix. Our code is available at the following link \footnote{\url{https://github.com/hjkng/audiogenX}}.

\begin{table*}[!ht]
\centering
\begin{tabular}{llllll}
\toprule
Method& $Fid_{F}\downarrow$ & $Fid_{CF}\uparrow$ & $KL_{F}\downarrow$ & $KL_{CF}\uparrow$ & Size $\downarrow$ \\
\midrule
    $N_{audio}=5$ & $0.128 \pm 0.004$ & \multicolumn{1}{c}{-} & $1.318 \pm 0.030$ & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-}\\
    Random-Mask & $0.196 \pm 0.004$ & $0.195 \pm 0.006$ & $1.884 \pm 0.044$ & $1.932 \pm 0.046$ & 0.500 \\
    $\text{Grad-CAM-e}$ & $0.204 \pm 0.006$ & $0.235 \pm 0.008$ & $1.858 \pm 0.034$ & $2.457 \pm 0.041$ & 0.422 \\
    $\text{Grad-CAM-a}$ & $0.240 \pm 0.006$ & $0.192 \pm 0.010$ & $2.285 \pm 0.077$ & $1.951 \pm 0.075$ & 0.406 \\
    AtMan & $0.195 \pm 0.008$ & $0.222 \pm 0.008$ & $2.010 \pm 0.049$ & $2.198 \pm 0.048$ & 0.497 \\
    Chefer et al. & $0.198 \pm 0.003$ & $0.229 \pm 0.004$ & $1.899 \pm 0.025$ & $2.348 \pm 0.040$ & 0.441 \\ \hline
    \mname \ w/ Eq.~\eqref{fact} & $0.145 \pm 0.004$ & $0.360 \pm 0.005$ & $1.542 \pm 0.024$ & $3.658 \pm 0.061$ & \textbf{0.360} \\
    \mname \ w/ Eq.~\eqref{cfact} & $0.143 \pm 0.004$ & $0.385 \pm 0.005$ & $1.514 \pm 0.043$ & $3.977 \pm 0.044$ & 0.385 \\
    \mname \ w/ Eq.~\eqref{audioexpl} & $0.137 \pm 0.005$ & $0.402 \pm 0.005$ & $1.418 \pm 0.043$ & $4.183 \pm 0.073$ & 0.455 \\
    \mname & $\textbf{0.132} \pm 0.004$ & $\textbf{0.405} \pm 0.004$ & $\textbf{1.416} \pm 0.029$ & $\textbf{4.259} \pm 0.039$ & 0.455 \\
\bottomrule
\end{tabular}
\caption{Evaluation of explanations generated by each method using factual and counterfactual reasoning. Five audio samples are generated and evaluated with different seeds based on the obtained explanations. The best results are highlighted in \textbf{bold}.}
\label{table:main}
\end{table*}