%!TEX root = main.tex
\subsection{Formulating Factual Explanations}

\begin{figure*}[!ht]
    \center
    \includegraphics[width=1.0 \linewidth]{fig/AudioGenX_overview.pdf}
    \caption{(a) The process by which AudioGen generates an audio. (b) \mname{}'s procedure for generating and applying explanations, with the $Explainer$ in the green box. (c) The method for calculating and applying the loss in \mname{}.}
    \label{overview}
\end{figure*}

Factual reasoning ~\cite{tan2022learning, ali2023explainable,kenny2021post} aims to find sufficient input that can approximately reproduce the original prediction. To quantify the sufficiency of textual tokens, we employ a perturbation-based method using the soft mask, interleaving the computation to measure the impact of changes. Specifically, we mask out attention scores in the cross-attention layers where textual information is fed into the TAG model. We formulate the perturbation in factual reasoning as:
\begin{equation}
\displaystyle \tilde{g}(\textbf{Q}, \textbf{K}, \textbf{V}, \textbf{M}_{\textbf{U}, \textbf{z}_{t}}) =  (\sigma(\frac{\textbf{Q}\textbf{K}^\intercal}{\sqrt{d_k}}) \odot \textbf{M}_{\textbf{U}, \textbf{z}_{t}}) \textbf{V},
\label{attx}
\end{equation}
\noindent where $\sigma$ denotes the softmax activation function and the mask $\textbf{M}_{\textbf{U}, \textbf{z}_{t}}$ controls the amount of information corresponding to the text token. When the mask value $\textbf{m}_{\textbf{u}_{i}, \textbf{z}_{t}}$ approaches $0$, the attention score is suppressed, meaning the information corresponding to the textual token is not fully propagated to the subsequent layer. Conversely, as the mask value approaches $1$, the original value is fully preserved. To distinguish this process from the original layer $f(\textbf{U}, \textbf{z}_{t-1})$, we denote the layer applying perturbation with the factual mask as $\tilde{f}(\textbf{U}, \textbf{z}_{t-1}, \textbf{M}_{\textbf{U}, \textbf{z}_{t}})$.

When the mask sufficiently serves as a factual explanation, the perturbed output remains approximately the same as the original prediction. To evaluate the impact of perturbation, we measure the resulting changes in the latent representation vector within the audio token space. Since the latent vector encodes rich and implicit information, we expect that two vectors close to each other indicate a similar auditory meaning, which is likely to result in similar audio generation. By leveraging this vector similarity, we can effectively measure the influence of perturbation and formulate the objective function for $Explainer$ as:

\begin{equation}
\mathcal{L}_{F} = -cos(f(\textbf{U}, \textbf{z}_{t-1}), \tilde{f}(\textbf{U}, \textbf{z}_{t-1}, \textbf{M}_{\textbf{U}, \textbf{z}_{t}})),
\label{fact}
\end{equation}
\noindent where $cos$ function refers to cosine similarity, which measures how similar factual result $\tilde{f}(\textbf{U}, \textbf{z}_{t-1}, \textbf{M}_{\textbf{U}, \textbf{z}_{t}})$ is to the original prediction $f(\textbf{U}, \textbf{z}_{t-1})$ in the audio token space. Since the objective function involves negative cosine similarity, minimizing the loss function corresponds to maximizing the similarity. Hence, following the objective function, the $Explainer$ generates the factual explanation mask, ensuring that the two representations or predictions are as close as possible in the audio token space.

\subsection{Formulating Counterfactual Explanations}
Counterfactual reasoning~\cite{tan2022learning, ali2023explainable, kenny2021post} aims to identify necessary inputs that can significantly alter the original prediction when it is perturbed or removed. This perturbation operates in the opposite direction of factual explanations, removing the important input to observe the counterfactual result. We formulate the perturbation method in counterfactual reasoning as:
\begin{equation}
\displaystyle \tilde{g}(\textbf{Q}, \textbf{K}, \textbf{V}, \textbf{1}-\textbf{M}_{\textbf{U}, \textbf{z}_{t}}) =  (\sigma(\frac{\textbf{Q}\textbf{K}^\intercal}{\sqrt{d_k}}) \odot (\textbf{1}-\textbf{M}_{\textbf{U}, \textbf{z}_{t}})) \textbf{V},
\label{att_cf}
\end{equation}
\noindent where $\textbf{1} \in \mathbb{R}^{1 \times T_{u}}$ is a vector of ones and $\textbf{1}-\textbf{M}_{\textbf{U}, \textbf{z}_{t}}$ subtracts the importance of the corresponding textual tokens. Consequently, the more important a textual token is, the more its attention score is suppressed in proportion to its importance. This perturbation operates under a counterfactual assumption as the What-If scenario~\cite{tan2022learning, ali2023explainable, kenny2021post}: What happens if the important textual token does not exist? After applying the perturbation in Equation~\eqref{att_cf}, the counterfactual result is observed as $\tilde{f}(\textbf{U}, \textbf{z}_{t-1}, \textbf{1}-\textbf{M}_{\textbf{U}, \textbf{z}_{t}})$. If the counterfactual result significantly differs from the original prediction, it indicates that the counterfactual mask is necessary to explain the original prediction. Conversely, if the change is trivial, the counterfactual mask is unnecessary to explain the causal relationship with the prediction. 

Generally, counterfactual explanations in supervised settings aim to find the important inputs that change the prediction with minimal perturbation. However, no class labels or guidance are available in our task of audio generation. Instead, we measure the change of meaning in latent space leveraging the cosine similarity function after counterfactual perturbation. Thus, the counterfactual explanation objective function is formulated as:
\begin{equation}
    \mathcal{L}_{CF} = cos(f(\textbf{U}, \textbf{z}_{t-1}), \tilde{f}(\textbf{U}, \textbf{z}_{t-1}, \textbf{1}-\textbf{M}_{\textbf{U}, \textbf{z}_{t}})),
    \label{cfact}
\end{equation}
\noindent where $cos$ function measures how dissimilar counterfactual result $\tilde{f}(\textbf{U}, \textbf{z}_{t-1}, \textbf{1}-\textbf{M}_{\textbf{U}, \textbf{z}_{t}})$ is to the original prediction in latent space. As the cosine similarity decreases, the objective function minimizes the similarity. Consequently, the $Explainer$ generates the counterfactual explanation mask to ensure that the two representations or predictions are as far as possible in the audio token space after counterfactual perturbation.

\subsection{Objective Function for \mname{}} 
Along with factual and counterfactual explanation objective functions, we add the regularization term to generate the explanation mask in a simple and efficient manner. Therefore, we incorporate additional regularization in our final objective function for the $Explainer$, which is formulated as:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{F} + \mathcal{L}_{CF} + \alpha \cdot L_1(\textbf{M}_{\textbf{U}, \textbf{z}_{t}})+ \beta \cdot L_2(\textbf{M}_{\textbf{U}, \textbf{z}_{t}}).
    \label{total}
\end{equation}
Here, $L_1$ and $L_2$ represent the $L_1$-Norm and $L_2$-Norm, respectively, as regularization terms to minimize the mask size. This prevents a trivial solution where the $Explainer$ generates an explanation mask assigning equal importance to all values. At the same time, adhering to Occamâ€™s Razor principle, we favor simpler and more effective explanations~\cite{tan2022learning, blumer1987occam}. Hence, according to the objective function in Equation~\eqref{total}, \mname{} optimizes the $Explainer$ generating faithful explanation masks in the audio-token level.

\begin{algorithm}[!t]
\caption{\mname{}}
\begin{algorithmic}
    \State \textbf{Input}: Textual token representation vector \textbf{U}, \\
    previously generated audio token vector $\textbf{z}_{t-1}$, Transformer model $f$, audio generation length $T$, number of epochs $K$, learning rate $\lambda$, regularization coefficients $\alpha$ and $\beta$ 
    \For {$t = 1$ \textbf{to} $T$}
        \State \textbf{Initialize} $Explainer$ with random parameters.
        
            \For {$epoch = 1$ \textbf{to} $K$}
                \State $\textbf{M}_{\textbf{U}, \textbf{z}_{t}} = Explainer(\textbf{U}, \textbf{z}_{t-1})$
                \State $L = L_{F} +  L_{CF} + \alpha \cdot L_1(\textbf{M}_{\textbf{U}, \textbf{z}_{t}}) + \beta \cdot L_2(\textbf{M}_{\textbf{U}, \textbf{z}_{t}})$
                \State $\theta := \theta - \lambda \nabla_\theta L$
            \EndFor
        \State $\textbf{M}_{\textbf{U}, \textbf{z}_{t}} = Explainer(\textbf{U}, \textbf{z}_{t-1})$
    \EndFor
    \State \textbf{Return} $\textbf{M}_{\textbf{U}, \textbf{z}} =  \displaystyle \frac{1}{T} \sum_{t=1}^{T} \textbf{M}_{\textbf{U}, \textbf{z}_{t}}$
\end{algorithmic}
\end{algorithm}

\subsection{Providing Audio-Level Explanations}
In this section, we aggregate audio token-level explanations to provide a comprehensive understanding of the entire audio sequence. The aggregation is performed by averaging the mask values across all audio tokens as follows:
\begin{equation}
\textbf{M}_{\textbf{U}, \textbf{z}} =  \displaystyle \frac{1}{T} \sum_{t=1}^{T} \textbf{M}_{\textbf{U}, \textbf{z}_{t}},
\label{audioexpl}
\end{equation}
\noindent where $t$ refers to the step, and $T$ represents the total length of generated audio. Additionally, it is possible to focus on a specific interval of interest within the audio, defined between a starting step $s$ and an ending step $n$. This is denoted as $\textbf{M}_{\textbf{U}, \textbf{z}} = \frac{1}{|n-s|+1} \sum_{t=s}^{n} \textbf{M}_{\textbf{U}, \textbf{z}_{t}}$, which provides granular explanations based on the user's request. This flexible approach enables users to discover patterns within specific intervals, as \mname{} can effectively capture and explain auditory content in targeted regions of the audio sequence.