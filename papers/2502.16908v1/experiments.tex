\section{Experiments}

\subsection{Dynamic manipulation}
We conduct two experiments, snatching and hammering, to show that \robot can perform dynamic manipulation. In these experiments, the target object pose is known in advance, and a pre-defined trajectory is executed with a joint position control to focus on demonstrating raw hardware capability. %We leave the acquisition of these manipulation capabilities through learning or motion planning as future work.

%Integrating perception and motion planning algorithms would allow these tasks to be extended to unknown or varying object positions.

\begin{figure}[hbt]
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{fig/snat_fig_V1.png}
        \caption{Experiment setup}
    \end{subfigure}
    \vfill
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.6\linewidth]{fig/snatching_objects_V3.png}
        \caption{Objects for snatching}
    \end{subfigure}
    \caption{(a) Snatching experiment setup. The object is placed in a fixed pose, and \robot rapidly snatches and releases it into the white basket. (b) Various real-world objects used in snatching which have different sizes and masses.}
    \label{fig:snatching}
\end{figure}


\subsubsection{\textbf{Snatching}}
Unlike static pick-and-place, snatching involves swift single-arm manipulations, where the robot rapidly approaches, grasps, and releases the target object. The experiment setup is shown in Figure~\ref{fig:snatching}a, where the goal is to snatch the object and releases it into a basket beside the table. We test \robot with five objects of varying shapes and masses, with ten trials for each object. Detailed information on the sizes and masses of these objects is described in Figure~\ref{fig:snatching}b. \robot achieves an overall success rate of 80\%, successfully completing all trials and objects except for the heavy tumbler, as shown in Table~\ref{tab:snatching_result}. This highlights the strength and limitation of \robot, and the difficulty of snatching. While it can snatch light-weight objects, it often drops the heavy tumbler (560 g) after snatching. Such heavy object demands a much more delicate yet firm grasp to counteract momentum and gravity effectively. Please see our supplement videos on snatching for better visualization.

%Failures with the heavy tumbler (560 g) highlight a key challenge: the snatching task becomes difficult when the object is heavy, causing the tumbler to slip and fall during the grasping attempt.

\begin{table}[hbt]
\centering
\caption{Snatching experiment results}
\label{tab:snatching_result}
\resizebox{0.99\columnwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Object          & Glue & Threadlocker & Plastic box & Bracket & Tumbler & Total  \\ \hline
Success/Trial & 10/10   & 10/10  & 10/10  & 10/10  &  0/10   & 40/50 \\ \hline
\end{tabular}%
}
\end{table}


\subsubsection{\textbf{Hammering}}
This task involves repeatedly striking a nail into a wooden board to evaluate the robot's capability of creating significant impact force. The experiment setup is shown in Figure~\ref{fig:hammering}. We record the number of strikes required to drive the nail 20 mm into the wooden board. If the nail bends during hammering, a human intervenes and straightens it with a wrench. For comparison, 14 humans (2 females and 12 males) perform the same single-hand hammering task under identical conditions, using the same wooden board, nail, and hammer. For human participants, it takes an average $13.15$ strikes with a standard deviation of $6.41$ to accomplish the task. On the other hand, \robot achieves a comparable level of efficiency to humans, driving the nail with an average of $10$ strikes with a standard deviation of $1.26$. Please see our supplement video on hammering to better grasp the speed and impact that \robot can generate.


%The results are summarized in Table~\ref{tab:hammer_result}. 


%it is manually adjusted without removing it from the peg to continue the experiment. 

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.37\columnwidth]{fig/hammering_fig.png}
    \caption{Hammering experiment setup. A nail is fixed on a wooden board with a height of 20 mm. The robot repeatedly strikes the nail until fully driven into the board.}
    \label{fig:hammering}
\end{figure}

\iffalse
\begin{table}[hbt]
\centering
\caption{Number of strikes to hammer a nail 20mm}
\label{tab:hammer_result}
\resizebox{0.5\columnwidth}{!}{%
\begin{tabular}{|c|c|c|}
\hline
       & Avg.   & Std dev. \\ \hline
ARMADA & 10.00    & 1.26  \\ \hline
Human  & 13.15 & 6.41  \\ \hline
\end{tabular}%
}
\end{table}
\fi

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/bump_scenario_V2.png}
    \caption{(a) Scenario 1: The robot moves the object from the right to the left of the bump. (b) Scenario 2: The robot moves the object from the left to the right. The object's initial pose is randomly set within the blue area. The robot must manipulate the object to match both the position and orientation of the goal pose sampled within the green area.}
    \label{fig:bump_real}
\end{figure}

\subsection{RL-based non-prehensile manipulation}
To validate our claim that \robot can be used for RL, we train \robot on a contact-rich non-prehensile manipulation task entirely in simulation and zero-shot transfer the policy to the real world. In this task, the robot uses a single arm to manipulate a cube whose length is 90mm over a 25 mm bump obstacle on the table, as shown in Figure~\ref{fig:bump_real}. Starting from a random initial pose, the robot must push, topple, strike, and reorient the object to overcome the bump and put it at the specified goal position and orientation without dropping it. We train the policy using NVIDIA Isaac Gym~\cite{makoviychuk2021isaac} with extensive domain randomization and zero-shot transfer to the real robot. Details regarding the training including hyperparameters and MDP definitions are in Appendix.

% As summarized in Table~\ref{tab:bump_results}, 
We perform 20 trials in total, 10 on each scenario. \robot achieves a 9/10 success rate in scenario 1, with a single failure where the cube falls off the table during the manipulation. In scenario 2, it achieves 7/10, where failures occur because the robot fails to make solid contact with the object. Most of the errors happen because of the sim-to-real gap in the environment and object, such as the object friction coefficient. Overall, the robot demonstrates 80\% success rate across both scenarios. 


%where two cases of failures are caused by \robot making contact with only a small part of the object, such as the corner of a cube, which results in the hand slipping from the object. 

%This problem occurs when an object flips easily with minimal contact in simulation but behaves differently in the real world. This is due to inaccuracies in modeling factors, such as the absence of rotational friction in GPU-accelerated simulations. In one case of failure, the policy is trained using a simplified table and bump, both modeled as boxes in the simulation. However, in the real world, an additional component used to secure the bump, which is not included in the simulation, is present. This causes the robot to get stuck on the component, ultimately ceasing its movement as a result. Overall, the robot demonstrates 80\% success rate across both scenarios, successfully transferring dynamic and contact-rich motions learned in simulation to real-world tasks.

%getting into joint positions that make contact with the object difficult while avoiding collisions with itself or the table.


\subsection{Human motion shadowing: Bimanual throwing}
In this task, we re-target human joint poses to the robot to show that \robot can serve as the platform for learning dynamic motions from a human demonstration. We use WHAM~\cite{wham} to predict SMPL~\cite{smpl} parameters, which estimate human joint keypoints, joint angles, and body shape to describe the pose of the human body. However, simply copying the joint angles to the corresponding actuators results in inaccurate motion due to the discrepancy of morphology between the SMPL model and \robot. We instead track the positions of the elbow and wrist joints of the human operator and solve differential inverse kinematics using PINK~\cite{pink} to find the joint configuration that minimizes the distance between the robot and the human joints at the elbow and wrist. This enables \robot to mimic human motion while preserving the relative position between the two hands, which is crucial in a bimanual task. 

\begin{table}[hbt]
\centering
\caption{Box driving distance}
\label{tab:box_distance}
\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Trial        & 1    & 2    & 3    & 4    & 5    & Avg. & Std dev.   \\ \hline
Distance (m) & 1.80 & 2.07 & 1.81 & 2.20 & 2.25 & 2.03 & 0.19 \\ \hline
\end{tabular}%
}
\end{table}

To show that \robot can shadow dynamic motion, we perform bimanual throwing task where the human shows a quick motion for throwing a box, and the robot's goal is to mimic that behavior to throw a box (318 g) placed 515 mm above the ground and 300 mm in front of its base as quickly as possible. The overall process of throwing is shown in Figure~\ref{fig:overall_illustration}d. Since the driving distance of the box is directly proportional to the end-effector speed at the moment of release, we measure the distance between the base and the closest part of the box. We repeat the experiment 5 times. \robot throws the box 2.03 m on average, as shown in Table~\ref{tab:box_distance}, and during throwing, \robot achieves an average maximum speed of 3.56 m/s with the left hand and 3.92 m/s with the right hand. To view the more detailed motion of \robot, please see our supplement video on bimanual throwing.