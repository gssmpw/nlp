\section{Dataset}
\label{appendix: dataset}
We provide the detailed information of our datasets in Table~\ref{tab: individual_dataset_py}, Table~\ref{tab: individual_dataset_java}, and Table~\ref{tab: individual_dataset_js}. We provide programming language, project name, license, link, number of stars, and number of forks for each individual project.
\input{sections/A1_table_dataset}

\section{More Implementation Details}
\subsection{Prompts}
\label{appendix: prompts}
\input{sections/A2_table_prompts}
The prompts are displayed in Figure~\ref{fig: prompt_java}, ~\ref{fig: prompt_js}, and ~\ref{fig: prompt_comment}.

\subsection{Models}
\label{appendix: models}
\input{sections/A2_table_models}
The detailed information of models, including license and link, is provided in Table~\ref{tab: models}.

\section{More Statistics}
\label{appendix: assert_statistics}
\input{sections/A3_assert_statistics}
Table~\ref{tab: asser_statistics} presents the percentages of the vanilla-generated unit tests containing comparisons between expected and actual values per language and per model.

\section{Ablation Study}
\label{appendix: ablation}

\subsection{Ablation Study on Prompts}
\label{sec: ablation}
\input{sections/6_table_ablation}
We perform a detailed ablation study to analyze the impact of prompts on the performance of unit test generation by LLMs.
As mentioned in \S~\ref{unit_test_generation}, the prompt is composed of programming language-specific requirements (PL), as well as requirements related to the correctness rate (CR), the compilation rate (ComR), and the coverage rate metrics (Coverage). We ablate each component and analyze the performance of unit test generation of GPT-4-Turbo using different prompts as shown in Table~\ref{tab: ablation}. 
Requirements related to CR and ComR can help improve performance in vanilla unit tests. 
Coverage-related requirements are not always beneficial, possibly because a high coverage rate is too abstract for LLMs to interpret effectively.
Programming language-specific requirements improve performance in CR but have the opposite effect on ComR, LC, and BC.

Besides, we follow the prompt template from previous work like~\citet{siddiq2024using} to move the prompts into comments (e.g., /*...*/). We compare the performance with and without comment signs in Table~\ref{tab: ablation}. Experimental results show that our prompt demonstrates a significant advantage in CR, while the prompt with comment signs exhibits marginal advantages in ComR, LC, and BC.


\subsection{Effect of Compilation Errors and Cascade Errors}
\label{appendix: alabtion_compilation}
\input{sections/A4_table_ablation2}
We manually fix only compilation errors and evaluate the corrected unit tests in Table~\ref{tab: ablation2}.
% Comparing Table~\ref{tab: ablation2} with Table~\ref{tab: main_results} and Table~\ref{tab: manual_results_improvements}, we can analyze the impact of compilation errors and cascade errors on performance.

By fixing compilation errors, Table~\ref{tab: ablation2} shows significant improvements across all programming languages and LLMs compared to Table~\ref{tab: main_results}, indicating that all the programming languages and LLMs are highly sensitive to compilation errors.
Comparing Table~\ref{tab: ablation2} with Table~\ref{tab: manual_results_improvements}, we can observe that CodeQwen1.5, CodeGemma, and CodeLlama are more sensitive to cascade errors.
% For \textbf{\textit{Python}} and \textbf{\textit{JavaScript}}, CodeQwen1.5, CodeGemma, and CodeLlama are more sensitive to cascade errors. The results of the three models increase apparently compared to \textsc{Phase 2}.
For Java, the changes in Table~\ref{tab: manual_results_improvements} compared to Table~\ref{tab: ablation2} are primarily due to missing or invalid mocks of user interactions\footnote{We consider coverage rates as not applicable when requiring user interactions.} which occur more frequently in unit tests generated by CodeQwen1.5 and CodeGemma. 
% This frequency is evident in the changes observed in the results between Table~\ref{tab: ablation2} and Table~\ref{tab: manual_results_improvements}. 
% For \textbf{\textit{JavaScript}},
% CodeQwen1.5, CodeGemma, and CodeLlama are more sensitive to \textcolor{blue}{default or global errors}. The results of the three models increase apparently compared to \textsc{Phase 2}.


\section{Detailed Error Analyses}
\label{sec: full_error_analyses}
We conduct complex analyses of compilation, cascade, and post-fix errors, highlighting the common errors and potential reasons behind the errors.

\paragraph{Compilation Error Analyses}
% \input{sections/5_figure_main_results}
Figure~\ref{fig: errors1} highlights the detailed compilation errors that occurred.
One of the most common compilation errors in \textbf{\textit{Python}} arises from the LLM's inability to determine whether the project being tested is a package. Specifically, LLMs struggle to recognize the presence or absence of \textit{\_\_init\_\_.py} files, which define a package, leading to confusion between package-based and non-package projects. This inability leads LLM to fail to correctly import functions or classes from the tested project.
Other compilation errors include hallucinating the paths or names of imported functions/classes and mismatched parentheses.
\textbf{\textit{Java}}, a syntax-heavy programming language compared to Python and JavaScript, encounters various compilation errors, resulting in a significantly lower compilation rate than other languages. Java compilation errors often arise from issues like hallucinated methods, constructors, or classes, such as incorrect or non-existent imports and references. Missing essential information, such as required functions, classes, or packages, and package declarations, is also a common problem. Errors frequently occur due to illegal access to private or protected elements, invalid code generation (e.g., generating text instead of code), and improper use of mocking frameworks like Mockito, including incorrect objects, missing or misused MockMvc injections, and argument mismatches. Other errors include incorrect usage of other functions, classes, or packagesâ€”such as argument type errors, ambiguous references, or incompatible types.
One of the most common compilation errors in \textbf{\textit{JavaScript}} is the hallucination of imported functions or classes, where the issue often lies in incorrect paths for the imported functions or classes. CodeQwen1.5 has a particularly common compilation error involving invalid generation. This typically occurs due to difficulty understanding the prompt, the need for more specific or detailed code requirements, or the assumption that the code is part of a larger project, leading it to decline generating unit tests. Other compilation errors include test suites containing empty unit tests and syntax errors caused by incomplete code generation or mismatched parentheses.


\paragraph{Cascade Error Analyses}
Figure~\ref{fig: errors2} highlights the detailed cascade errors that occurred.
For \textbf{\textit{Python}}, the cascade errors include missing imports of commonly used packages such as numpy and unittest, missing imports of functions or classes from the tested project, and FileNotFoundError. 
% The FileNotFoundError indicates that the generated unit tests fail to mock the external files.
For \textbf{\textit{Java}}, the most common cascade error is missing or invalid mocking of user interactions. A proper unit test should simulate user interactions through mocking rather than relying on real user inputs. This issue also results in unusable coverage reports for some tested projects, as the error forces an abrupt termination, preventing the generation of coverage data.
For \textbf{\textit{JavaScript}}, the cascade errors include missing imports of commonly used packages such as chai and three, and missing imports of functions or classes from the tested project. Two other common errors specific to JavaScript are that LLMs may confuse named imports with default imports and fail to comply with the Jest framework. 


\paragraph{Post-Fix Error Analyses}
Figure~\ref{fig: errors3} highlights the incorrectness reasons after all manual fixes.
For all programming languages, the mismatch between expected and actual values (AssertionError) is the most common error.
Another frequent error in \textbf{\textit{Python}} is AttributeError, typically caused by LLMs hallucinating non-existent attributes.
Other frequent problems in \textbf{\textit{Java}} include NullPointer Errors, zero interactions with mocks, and failures to release mocks, often due to improper mock usage. For projects tested with the Spring framework, errors specific to Spring are also common.
Another frequent error in \textbf{\textit{JavaScript}} is TypeError, mostly caused by LLMs hallucinating non-existent functions and constructors or LLMs invalidly mocking some variables.

\input{sections/5_figure_error_analyses}

% \paragraph{Overall}
% Common errors across different programming languages include hallucinations of functions or classes, \textit{possibly caused by training data bias and a lack of proper references}.
% % Missing required functions/classes is another common error (Compilation error for Java and Cascade error for Python and JavaScript).
% Another common error is missing required functions or classes, which often occurs because LLMs \textit{prioritize logical structure over boilerplate code} and \textit{fail to understand the codebase structure and the dependencies between functions, classes, or modules}. Failure to understand the codebase structure and dependencies can also cause other mistakes, such as confusing non-package and package-based projects (Python) or incorrectly using functions, classes, or packages (Java).
% % For after-fix errors, the mismatch between expected and received values is the most common error. 
% The most common post-fix error is the mismatch between expected and received values, often caused by incorrect expected values due to the \textit{weak reasoning abilities} of LLMs.
