We evaluate the generated unit tests from three scenarios, vanilla (\S~\ref{sec: main results}), after manual fixing of compilation and cascade errors (\S~\ref{sec: manual fixing}), and LLM self-fixing (\S~\ref{sec: self-fixing}). For each scenario, we evaluate the Correctness Rate (CR), Compilation Rate (ComR), Line Coverage (LC), and Branch Coverage (BC). We also conduct unique contribution analyses (\S\ref{sec: unique}) and detailed error analyses (\S~\ref{sec: error analyses}).

\subsection{Main Results}
\label{sec: main results}
\input{sections/5_table_main_results}
The main results of the LLMs' unit test generation performance focus on the vanilla unit tests extracted directly from the LLMs' outputs without any changes. The goal of this scenario is to assess the LLMs' current raw capability to generate project-level unit tests.

% \paragraph{Evaluation Results}
Table~\ref{tab: main_results} shows the evaluation results of the vanilla unit tests.
First, we observe that different LLMs have varying language-level expertise. For example, Claude-3.5-Sonnet performs the best in Java but falls behind GPT-o1 in JavaScript.
Second, we can see from the results that LLMs have different metric-level expertise as well, validating the effectiveness of different evaluation metrics. For example, in Python,  Claude-3.5-Sonnet performs the best on CR and ComR while falling behind GPT-o1 on LC and BC.

Among three programming languages, Java is the most difficult language, primarily due to stricter syntax. Many models fail to generate valid Java code, leading to low compilation rates and execution coverage.
Among all the evaluated models, GPT-o1 performs the best in general, especially in JavaScript. CodeLlama and CodeGemma have the worst general performance.
We also observe that some models tend to generate more unit tests. However, generating more unit tests does not necessarily lead to better coverage rates. For example, Gemini-2.0-Flash tends to generate the most unit tests but does not obtain the best coverage rate. 
Additionally, we observe that sometimes the open-source model can even outperform some closed-source models. For example, DeepSeek-Coder works better than GPT-3.5-Turbo on Python and JavaScript.
Finally, we confirmed from such results that dependencies exist in metrics. On Java, models like CodeQwen1.5, CodeLlama, and CodeGemma fail to generate compilable unit tests, resulting in the lowest correctness rates and coverage rates.  

%\chen{We need to firstly mention some more general observations here in the first paragraph, then dive deep into per-language behaviours. For example, in the first paragraph, we can say we observe that different LLMs have different language-level expertise while claude generally performs the best on bla and bla. models having the best performance on LC don't necessarily have the best performance on other metrics.open souce models perform most horribly on Java. Java is the most difficult language.  on the 3 criterias, different models have different expertise as well. models having the best performance on LC don't necessarily have the best performance on other metrics. things like these. What I listed here is not an exhausted list. We need to list all the general patterns we observe here and try to pick the most relevant ones. Yibo please fix. After finishing this first paragraph, we can reduce the rest paragraphs.  }


% For \textbf{\textit{Python}}, 
% Claude-3.5-Sonnet-20241022 demonstrates the best overall performance. 
% CodeQwen1.5-7B-Chat achieves the second-to-best LC (43\%) and BC (40\%), although its CR is only 24\%. This may be because CodeQwen1.5-7B-Chat generates more tests than most models, leading to higher coverage.
% % DeepSeek-Coder-6.7b-Instruct shows competitive results in metrics like ComR (70\%). 
% % CodeGemma-7b-it and CodeLlama-7b-Instruct-hf achieve lower CRs, indicating weaker overall performance.
% CodeGemma-7b-it achieves the worst performance across all the evaluation metrics.
% For \textbf{\textit{Java}},
% Claude-3.5-Sonnet-20241022 and GPT-4-Turbo show best and second-to-best performance across all evaluation metrics. Compared to other models, Claude-3.5-Sonnet-20241022 has a clear advantage, mainly because the unit tests it generates have a high compilation rate, which also leads to strong performance in other evaluation metrics.
% Among all the open-source models evaluated, DeepSeek-Coder-6.7b-Instruct performs the best, while models like CodeQwen1.5-7B-Chat, CodeLlama-7b-Instruct-hf, and CodeGemma-7b-it fail to generate compilable unit tests, resulting in the lowest performance.
% For \textbf{\textit{JavaScript}},
% Gemini-2.0-Flash and Claude-3.5-Sonnet-20241022 demonstrate the best performance overall, followed by GPT-4-Turbo and DeepSeek-Coder-6.7b-Instruct. 
% % Models like CodeQwen1.5-7B-Chat, CodeLlama-7b-Instruct-hf, and CodeGemma-7b-it perform modestly overall, with lower CRs (23\%, 26\%, and 29\%, respectively), LCs (25\%, 20\%, and 28\%, respectively), and BCs (20\%, 14\%, and 21\%, respectively).
% Models like CodeQwen1.5-7B-Chat, CodeGemma-7b-it, and CodeLlama-7b-Instruct-hf perform worst overall, with the lowest performance across all the evaluation metrics.


\subsection{Manual Fixing Results}
\label{sec: manual fixing}
\input{sections/5_table_manual_fix}
% During manual fixing, minimal adjustments are made to fix compilation errors, ensuring that all generated unit tests can compile. Building on this, manual fixing addresses cascade errors -- easy-to-solve issues affecting multiple unit tests or the entire test suite. By resolving these errors, manual fixing ensures that all unit tests are compilable and no cascade errors invalidate tests that are fundamentally correct. Manual fixing focusing on compilation and cascade error resolution provides a robust evaluation of the quality and reliability of the generated unit tests. This scenario aims to assess LLMs' potential for improvement when compilation and cascade errors are fixed.
% We also evaluate unit tests fixing only compilation errors in Appendix~\ref{appendix: alabtion_compilation}.

% \paragraph{Evaluation Results}
Table~\ref{tab: manual_results_improvements} shows the evaluation results with improvements compared to vanilla results after manual fixing. After fixing compilation and cascade errors, the results show significant improvements across all programming languages and LLMs compared to vanilla unit tests. This indicates that the unit tests generated by LLMs are highly sensitive to compilation and cascade errors.

Among all programming languages, Java benefits the most from manual fixing. In the case of vanilla unit tests, Java exhibits the lowest compilation rates, making it particularly challenging. However, after manual fixing, Java shows the most substantial improvement, highlighting the potential of LLMs for Java after fixing compilation and cascade errors. 
Among all models, GPT-o1 still performs the best after manual fixing, and CodeLlama and CodeGemma still exhibit the worst general performance.
Gemini-2.0-Flash shows the best coverage improvement overall, indicating its strong potential for better unit test generation once compilation and cascade errors are corrected.
Finally, we observe that certain models that initially underperform can match or surpass stronger models after manual fixing. For example, in Java, CodeQwen1.5 outperforms DeepSeek-Coder and is now on par with GPT-4-Turbo.
% , which is the top-performing open-source model. 
In Python, Gemini-2.0-Flash surpasses CodeQwen1.5-7B-Chat, showing better potential after manual fixing. On JavaScript, GPT-3.5-Turbo has reached parity with DeepSeek-Coder.



% For \textbf{\textit{Python}}, Claude-3.5-Sonnet-20241022 continues to exhibit the best performance, and CodeGemma-7b-it still has the lowest CR and overall performance, remaining the weakest among the models. Gemini-2.0-Flash surpasses CodeQwen1.5-7B-Chat, showing better potential after manual fixing. Claude-3.5-Sonnet-20241022 improves the correctness rate the most, and Gemini-2.0-Flash shows the best coverage improvement.
% For \textbf{\textit{Java}}, Claude-3.5-Sonnet-20241022 continues to deliver the best performance overall and Gemini-2.0-Flash surpasses GPT-4-Turbo. CodeQwen1.5-7B-Chat outperforms DeepSeek-Coder-6.7b-Instruct and is now on par with GPT-4-Turbo, being the top-performing open-source model. CodeQwen1.5-7B-Chat improves the correctness rate the most, and Gemini-2.0-Flash shows the best overall improvement in coverage.
% For \textbf{\textit{JavaScript}}, Gemini-2.0-Flash and Claude-3.5-Sonnet-20241022 retain the best overall performance. GPT-3.5-Turbo has reached parity with DeepSeek-Coder-6.7b-Instruct. CodeQwen1.5-7B-Chat and CodeLlama-7b-Instruct-hf remain the worst overall performance. CodeLlama-7b-Instruct-hf improves the correctness rate the most, and Gemini-2.0-Flash shows the best coverage improvement.


\subsection{LLMs Self-fixing Results}
\label{sec: self-fixing}
\input{sections/5_table_self_fix}
During LLM self-fixing, conversation history and error messages are provided to help the model correct errors. This scenario assesses the LLM’s ability to fix its own mistakes and its potential to generate better unit tests by incorporating self-fixing.

Table~\ref{tab: results_self_fix} shows the LLM self-fixing evaluation results in comparison with manual fixing. First, we observe that most closed-source models have the ability to self-fix errors and generate better unit tests compared to vanilla results, while the evaluated open-source models lack reliable self-fixing abilities. This limitation may stem from factors such as restricted input length, which leads to incomplete context, as well as weaker comprehension and instruction-following abilities. For instance, open-source models like CodeGemma and CodeLlama tend to generate textual instructions for fixing errors rather than directly producing the corrected unit tests specified in the prompt.

Second, we observe that LLM self-fixing follows similar but not identical trends to manual fixing, suggesting that while LLMs' potential for improvement generally aligns with self-fixing capabilities, some LLMs do not follow this pattern. For instance, in JavaScript, GPT-o1’s self-fixing performance on the coverage rate is significantly worse compared to manual fixing due to a lower number of generated unit tests and a reduced compilation rate.

Although LLM self-fixing currently lags behind manual fixing in performance, LLM self-fixing still holds significant potential. Self-fixing has proven effective when LLMs have the necessary capabilities, and it even has the potential to surpass manual fixing due to its flexibility. For example, in JavaScript, CodeQwen1.5 shows greater improvement through self-fixing compared to manual fixing. This is because, in its vanilla output, CodeQwen1.5 sometimes fails to understand the prompt and does not generate any unit tests. Manual fixing based solely on these initial outputs cannot resolve this issue. However, LLM self-fixing can overcome this limitation by correctly interpreting unit test generation prompts when error messages indicate that unit tests are required.
% \subsection{\textsc{Phase 2: Post-Compilation Errors}}
% \label{phase 2}
% \input{sections/5_table_results_phase2}
% Phase 2 involves making minimal adjustments to fix compilation errors so that all generated unit tests can be compiled. The goal is to evaluate how well LLMs can generate correct, high-coverage tests once every generated unit test is guaranteed to compile.

% \paragraph{Evaluation Results}
% Table~\ref{tab: results_2} shows the evaluation results in \textsc{Phase 2}. 
% By fixing compilation errors, the results in Phase 2 show significant improvements across all programming languages and LLMs compared to Phase 1. This indicates that the unit tests generated by LLMs are highly sensitive to compilation errors.
% For \textbf{\textit{Python}}, 
% Claude-3.5-Sonnet-20241022 continues to be the best model. Gemini-2.0-Flash surpasses CodeQwen1.5-7B-Chat, and GPT-4-Turbo is now on par with CodeQwen1.5-7B-Chat, even though GPT-4-Turbo generates far fewer unit test cases. GPT-3.5-Turbo has improved significantly, with its updated results behind GPT-4-Turbo and CodeQwen1.5-7B-Chat. However, CodeGemma-7b-it and CodeLlama-7b-Instruct-hf still have the lowest CR and overall performance, remaining the weakest among the models.
% For \textbf{\textit{Java}},
% Claude-3.5-Sonnet-20241022 continues to deliver the best performance overall. After fixing compilation errors, Gemini-2.0-Flash surpasses GPT-4-Turbo, and CodeQwen1.5-7B-Chat and CodeGemma-7b-it emerge as the top-performing open-source models.
% For \textbf{\textit{JavaScript}},
% Gemini-2.0-Flash and Claude-3.5-Sonnet-20241022 retain the best overall performance. 
% GPT-3.5-Turbo has reached parity with DeepSeek-Coder-6.7b-Instruct. 
% CodeGemma-7b-it significantly improves, while CodeQwen1.5-7B-Chat and CodeLlama-7b-Instruct-hf remain the worst overall performance.


% \input{sections/5_figure_manual_fix}
% \paragraph{Cascade Error Analyses}
% Figure~\ref{fig: errors2} highlights the detailed cascade errors that occurred.
% For \textbf{\textit{Python}}, the cascade errors include missing imports of commonly used packages such as numpy and unittest, missing imports of functions or classes from the tested project, and FileNotFoundError. 
% % The FileNotFoundError indicates that the generated unit tests fail to mock the external files.
% For \textbf{\textit{Java}}, the most common cascade error is missing or invalid mocking of user interactions. A proper unit test should simulate user interactions through mocking rather than relying on real user inputs. This issue also results in unusable coverage reports for some tested projects, as the error forces an abrupt termination, preventing the generation of coverage data.
% For \textbf{\textit{JavaScript}}, the cascade errors include missing imports of commonly used packages such as chai and three, and missing imports of functions or classes from the tested project. Two other common errors specific to JavaScript are that LLMs may confuse named imports with default imports and fail to comply with the Jest framework. An example is provided in Apendix~\ref{appendix: error example}.




% \subsection{\textsc{Phase 3: Post-Cascade Errors}}
% \input{sections/5_table_results_phase3}
% Phase 3 builds on Phase 2 by fixing all cascade errors, which are issues that can impact multiple unit tests or even the entire test suite. Some tests that are fundamentally correct may fail due to these errors. By resolving all cascade errors, Phase 3 offers an additional view of the performance in unit test generation.
% \paragraph{Evaluation Results}
% Table~\ref{tab: results_3} shows the evaluation results in \textsc{Phase 3}. 
% Fixing cascade errors can lead to changes in results for certain programming languages and LLMs.
% For \textbf{\textit{Python}} and \textbf{\textit{JavaScript}}, CodeQwen1.5-7B-Chat, CodeGemma-7b-it, and CodeLlama-7b-Instruct-hf are more sensitive to cascade errors. The results of the three models increase apparently compared to \textsc{Phase 2}.
% For \textbf{\textit{Java}}, the changes in Phase 3 compared to Phase 2 are primarily due to missing or invalid mocks of user interactions, which occur more frequently in unit tests generated by CodeQwen1.5-7B-Chat and CodeGemma-7b-it. This frequency is evident in the changes observed in the results between Phase 3 and Phase 2. 
% % For \textbf{\textit{JavaScript}},
% % CodeQwen1.5-7B-Chat, CodeGemma-7b-it, and CodeLlama-7b-Instruct-hf are more sensitive to \textcolor{blue}{default or global errors}. The results of the three models increase apparently compared to \textsc{Phase 2}.

% \paragraph{Post-Fix Error Analyses}
% Figure~\ref{fig: errors3} highlights the incorrectness reasons after all fixes.
% For all programming languages, the mismatch between expected and actual values (AssertionError) is the most common error.
% Another frequent error in \textbf{\textit{Python}} is AttributeError, typically caused by LLMs hallucinating non-existent attributes.
% Other frequent problems in \textbf{\textit{Java}} include NullPointer Errors, zero interactions with mocks, and failures to release mocks, often due to improper mock usage. For projects tested with the Spring framework, errors specific to Spring are also common.
% Another frequent error in \textbf{\textit{JavaScript}} is TypeError, mostly caused by LLMs hallucinating non-existent functions and constructors or LLMs invalidly mocking some variables.
% % The most common error in \textbf{\textit{Python}} is AssertionError, which indicates a mismatch between the expected value in the unit tests and the actual result. Another frequent error is AttributeError, typically caused by LLMs hallucinating non-existent attributes.
% % The most common issue in \textbf{\textit{Java}} is a mismatch between expected and actual values. Other frequent problems include NullPointer Errors, zero interactions with mocks, and failures to release mocks, often due to improper mock usage. For projects tested with the Spring framework, errors specific to Spring are also common.
% % The most common error in \textbf{\textit{JavaScript}} is the mismatch between expected and received values. Another frequent error is TypeError, mostly caused by LLMs hallucinating non-existent functions and constructors or LLMs invalidly mocking some variables.


% \subsection{Overall}

% Comparing the three programming languages, generating unit tests for Java is the most challenging for LLMs.
% Overall, closed-source models generally deliver better performance, but in specific scenarios, DeepSeek-Coder-6.7b-Instruct and CodeQwen1.5-7B-Chat can achieve results that are comparable to or even surpass those of closed-source models. For instance, DeepSeek-Coder-6.7b-Instruct demonstrates the highest ComR among all open-source models, sometimes outperforming all the closed-source models.

% % Common errors across different programming languages are hallucinations of functions/classes/attributes. 
% Common errors across different programming languages include hallucinations of functions or classes, \textbf{possibly caused by training data bias and a lack of proper reference}.
% % Missing required functions/classes is another common error (Compilation error for Java and Cascade error for Python and JavaScript).
% Another common error is missing required functions or classes, which often occurs because LLMs \textbf{fail to understand the codebase structure and the dependencies between functions, classes, or modules}. It can also cause other mistakes, such as confusing non-package and package-based projects (Python) or incorrectly using functions, classes, or packages (Java) etc.
% % For after-fix errors, the mismatch between expected and received values is the most common error. 
% The most common post-fix error is the mismatch between expected and received values, often caused by incorrect expected values due to the \textbf{weak reasoning abilities} of LLMs.


% % Same model different language ==> not same performance



\subsection{Unique Contribution of Unit Tests}
\label{sec: unique}
We also explore the unique contribution of the generated unit tests on Python.
The unique contribution is defined as the total portion of coverage contributed by each generated unit test that does not overlap with the coverage of other unit tests. 
This is important for several reasons. First, some LLMs generate more unit tests than others, making it insufficient to rely solely on coverage rate as a metric; the unique contribution of each test should also be considered. Second, it is crucial for LLMs to generate fewer unit tests while still achieving a high coverage rate, as running a large number of tests can sometimes be resource- or time-intensive.

Table~\ref{tab: unique_contribution} reveals that all the tested LLMs have low rates of unique contributions, indicating a tendency to produce redundant and repetitive unit tests. 
Although GPT-o1 has better coverage rates than Claude-3.5-Sonnet, GPT-o1 produces significantly more unit tests, and its unique contribution is lower than Claude-3.5-Sonnet’s, indicating that it relies on quantity rather than quality to reach higher coverage. 
% Although CodeQwen1.5-7B-Chat, Gemini-2.0-Flash, and CodeLlama-7b-Instruct-hf have better coverage rates than GPT-4-Turbo, they also produce significantly more unit tests. Moreover, their unique contribution is lower than GPT-4-Turbo’s, indicating that they rely on quantity rather than quality to reach high coverage. 
As a result, this approach may compromise the overall efficiency of the testing process.

\input{sections/6_table_unique}
