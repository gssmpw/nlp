\input{sections/3_figure_overview}
We first
% formulate the LLM unit test generation task (\S\ref{task_formulation}). Then, we 
introduce the dataset collection and preprocessing of creating ProjectTest (\S\ref{data_collection_preprocessing}). Then, we introduce evaluation metrics (\S\ref{sec: evaluation}) and the Unit Test Generation pipeline (\S\ref{unit_test_generation}) that we use to evaluate LLMs on ProjectTest, including three unit test generation scenarios.

% \input{sections/3discard_figure_unittest_example}
% At last, we introduce the generated unit test post processing (\S\ref{post_processing}) and evaluation (\S\ref{evaluation_metrics}).

% \subsection{Task Formulation}
% \label{task_formulation}

% \input{sections/3discard_figure_overview}

\subsection{Benchmark Dataset}
\label{data_collection_preprocessing}
% data source, data filtering rules, data statistics (table)

% \input{sections/3discard_figure_data_example}

\textbf{Dataset Collection. }
Our dataset is built from carefully selected project-level repositories on GitHub, which is a popular platform for hosting and collaborating on software development projects. We focus on three widely used programming languages: Python, Java, and JavaScript. We establish our selection criteria based on three key factors: 1) a reasonable size, 2) inter-file dependencies, and 3) a reliable source. Thus, we collect reliable and self-contained projects consisting of 2 to 15 files with fewer than 1600 lines of code (LOC).
% \yibo{\sout{We aim to collect reliable and self-contained projects consisting of 2 to 15 files with fewer than 1600 lines of code (LOC).}}
We limit our selection to repositories with publicly available licenses, such as the MIT license, ensuring the legality and openness of the code. 
%\yibo{Q: one project has no license} It is okay.
To maintain the quality and reliability of the dataset, we choose projects with a high number of stars and forks, which signals community approval and widespread usage. 
%\yibo{\sout{After filtering, we manually select a subset of manageable projects that meet our criteria.}} \chen{what do we mean by the sentence above? "after filtering" refers to the fork and star filter? then what is the "our criteria" referring to??}
For projects that fit all the requirements above but are too big for current frontier LLMs to handle, we also extracted smaller projects from these large codebases. These smaller projects were carefully adjusted to be self-contained without relying on the original larger projects.
After applying these criteria, we constructed 20 representative projects per programming language. 
The summary of dataset statistics is presented in Table~\ref{tab: dataset}, and detailed information on the dataset sources and statistics for each project can be found in Appendix~\ref{appendix: dataset}.



\noindent\textbf{Pre-processing. }
Dataset pre-processing involves several key steps to ensure the projects are well-structured and suitable for testing. 
First, we double-check whether the selected projects have syntax errors, even though they are sourced from reliable codebases.
Second, for projects extracted from a larger codebase, we modify them to be self-contained by reorganizing files, adjusting domain naming conventions, and/or modifying import paths to remove dependencies on external modules.
Next, to enhance the accuracy of line coverage measurements, we consolidate statements that are split across multiple lines into a single line, ensuring that the metrics are more valid. 
Additionally, we maintain the integrity of the original code style as much as possible, preserving the diverse coding practices across different projects. This approach allows us to test how LLMs handle various code styles in a realistic environment.

\input{sections/1_table_data_statistics}

\subsection{Evaluation Metrics}
\label{sec: evaluation}
We focus on three key aspects when evaluating the generated unit tests: compilation rate, correctness rate, and coverage rate.
\textit{Compilation rate} (ComR) measures the percentage of projects in which the generated test suites compile successfully, indicating how often LLMs produce unit test suites that can be executed without compilation errors. 
The compilation rate for all projects in $X$ is defined as 
$ComR = \frac{|X^{com}|}{|X|}$, 
where $X$ is the project set and $X^{com}\subset X$ denotes the subset of projects whose test suites compile successfully.
\textit{Correctness rate} (CR) calculates the percentage of unit tests that are correct out of all generated unit tests for each project, providing insight into the accuracy of the test generation process. On average, more than 95\% of vanilla-generated unit tests compare expected and actual values, reinforcing the validity of CR as an evaluation metric. Detailed statistics see Appendix~\ref{appendix: assert_statistics}.
The correctness rate for the project $x$ is defined as
$CR_x = \frac{|T_x^{cor}|}{|T_x|}$, 
where $T_x$ is the generated test suite and $T_x^{cor}\subset T_x$ denotes the correct unit test set for the project $x$.
\textit{Coverage rate} analyzes both line and branch coverage to understand how well the generated unit tests explore the code's functionality.
The coverage rate for the project $x$ is defined as
$CR_x = \frac{covered(x)}{total(x)}$,
where $covered(x)$ denotes the number of covered lines/branches in project $x$ and $total(x)$ the total number of lines/branches in project $x\in X$.


These three evaluation metrics are not independent. If a project has the generated test suite containing compilation errors, none of its unit tests can be executed successfully, leading to both the correctness rate and the coverage rate for the project being zeros. Additionally, some errors resulting in failed tests, like missing Python dependencies, can also lead to a change in coverage rate. Therefore, considering the interdependencies between the three evaluation metrics, we extend our analysis beyond the evaluation of vanilla unit tests to include manually fixing these errors. This enables a more comprehensive assessment of LLMs' potential to generate high-quality unit tests once these errors are addressed. This assessment is conducted while maintaining the same quantity and diversity of unit tests originally generated by the LLMs.
Furthermore, we extend our analysis to examine the self-fixing capabilities of LLMs.



\input{sections/3_figure_data_example}

\subsection{Unit Test Generation }
\label{unit_test_generation}
Figure~\ref{fig: overview} shows an overview of the unit test generation process by LLMs.
% We begin by inputting the entire project and a carefully crafted prompt into the LLMs, ensuring the context and requirements are clearly communicated. 
% Once we receive the LLM-generated responses, we undertake a systematic post-processing phase to refine and organize the output, enhancing its clarity and applicability. We then employ multiple evaluation metrics to assess the quality and effectiveness of the generated unit tests. 
Our unit test generation and evaluation aim to ensure fair and thorough assessments of unit tests generated by LLMs under different scenarios:
\begin{itemize}
[noitemsep,topsep=0pt,itemsep=1.8pt, leftmargin=10pt]
    \item \textbf{Scenario 1}: Vanilla unit tests extracted from LLMs' outputs.
    \item \textbf{Scenario 2}: Compilable unit tests after manually fixing all compilation and cascade errors. 
    % Compilation errors are defined as errors that prevent testing frameworks from being executed. \footnote{Technically, Python does not require compilation. However, certain Python testing frameworks, such as pytest and coverage, need to compile Python codes into bytecode by the Python interpreter before execution.} Cascade errors are defined as errors that cause cascading failures across multiple unit tests or even the entire test suite. 
    \item \textbf{Scenario 3}: Unit tests refined by LLMs self-fixing, provided with error messages and conversation history.
    % More details in \S~\ref{phase 2}.
\end{itemize}

\input{sections/3_figure_prompt_python}
\noindent\textbf{Scenario 1: Vanilla Unit Test Generation. }
We begin by inputting the entire project and the carefully crafted prompt into the LLM, ensuring the context and requirements are clearly communicated. 
The complete project codes are used as input to ensure the models have all the necessary context to generate unit tests for the entire project. An example is shown in Figure~\ref{fig: dataset}. 
We carefully design prompts for different LLMs to thoroughly test their actual capabilities. 
In addition, to address specific issues associated with different programming languages, we incorporate language-specific prompts tailored to solve particular challenges.
We require the LLMs to generate unit tests for each file within the project. 
Furthermore, we provide detailed prompts instructing LLMs to focus on various evaluation aspects, including compilation rate, correctness rate, and coverage rate. 
This structured prompt engineering enhances the effectiveness and relevance of the outputs produced by the LLMs.
An example of our designed prompt for Python is shown in Figure~\ref{fig: prompt}.
All prompts used in our experiments are listed in Appendix~\ref{appendix: prompts}, and an ablation analysis of the prompts is shown in Appendix~\ref{sec: ablation}.
The vanilla unit tests are extracted from the LLM response based on the input project and prompt. 


\noindent\textbf{Scenario 2: Manual Fixing compilation and cascade errors. } Manually fixing compilation and cascade errors is motivated by our empirical observation from scenario 1 that even the vanilla unit tests from state-of-the-art LLMs, such as Claude-3.5-Sonnet, contain significant compilation errors, making them non-compilable. Additionally, they exhibit cascade errors that are easy to fix but can affect multiple unit tests or the entire test suite (details in Section~\ref{sec: error analyses}). Although these errors are preliminary and relatively simple to resolve, they hinder further analysis of other aspects of LLM performance on unit test generation, such as correctness and coverage.

%During manual fixing, minimal adjustments are made to fix compilation errors, ensuring that all generated unit tests can compile. Building on this, manual fixing addresses cascade errors -- easy-to-solve issues affecting multiple unit tests or the entire test suite. By resolving these errors, manual fixing ensures that all unit tests are compilable and no cascade errors invalidate tests that are fundamentally correct. Manual fixing focusing on compilation and cascade error resolution provides a robust evaluation of the quality and reliability of the generated unit tests. This scenario aims to assess LLMs' potential for improvement when compilation and cascade errors are fixed.

\input{sections/3_figure_error_example}

Therefore, based on vanilla unit tests, we make the minimum necessary changes to resolve compilation errors and cascade errors, focusing solely on eliminating these errors without altering the original test intent. 
Compilation errors are defined as errors that prevent testing frameworks from executing.\footnote{Technically, Python does not require compilation. We refer to errors that cause pytest to fail before it can collect and run any tests as compilation errors.} 
As shown in Figure~\ref{fig: compilation_errors}, the ModuleNotFoundError causes pytest to fail before collecting any unit tests, making the entire test suite uncompilable. This results not only in compilation failure but also in unreachable correctness and coverage rates.\footnote{We consider unreachable correctness and coverage rate as zero.}
Cascade errors are defined as errors that cause cascading failures across multiple unit tests or even the entire test suite. 
As shown in Figure~\ref{fig: cascade_errors}, although the tests are fundamentally correct, this NameError (missing NumPy) invalidates multiple or even the entire test suite.
By resolving these errors, manual fixing ensures that all unit tests are compilable and no cascade errors invalidate tests that are fundamentally correct.

Manually fixing compilation and cascade errors plays a crucial role in evaluating the quality and reliability of generated unit tests. By addressing these errors, we gain deeper insights into the effectiveness of LLM-generated unit tests and identify areas for improvement. This process also helps assess the potential for LLMs to improve continuously once such simple errors are resolved. Additionally, we evaluate unit tests fixing only compilation errors in Appendix~\ref{appendix: alabtion_compilation}.

% To maintain evaluation fairness, especially for Python and JavaScript, we add any missing imports for common packages like NumPy or for functions, classes, or modules from the project being tested. This step is important because, without the correct imports, even valid assertions could lead to lower correctness and coverage scores. 


\input{sections/3_figure_self_fix_prompt_Python}
\noindent\textbf{Scenario 3: LLM Self-fixing. } 
Inspired by our observation from manual fixing that different LLMs exhibit significantly different potentials after manual fixing, we seek to investigate how LLMs perform in self-fixing on our benchmark. 
We explore LLMs' self-fixing abilities by incorporating conversation history and error messages as shown in Figure~\ref{fig: self_fix_prompt}. We provide LLMs with the conversation history (including the system prompt, the user prompt for unit test generation requests, and LLM vanilla response), error messages obtained from the testing framework, and the user prompt for error fixing requests. 
When the open-source LLM's input length is limited, we prioritize the information in the following order: system prompt, LLM's initial response, error messages, error-fixing requests, and unit test generation requests. Less important information is truncated as needed. Additionally, we reserve at least 2,000 tokens for the open-source LLM’s self-fixing outputs.
LLM self-fixing scenario helps us understand LLMs' error-fixing ability and their potential to generate better unit tests when incorporating the self-fixing process. 
Note that during self-fixing, we do not constrain the target error types to just compilation or cascade errors. 


% \paragraph{Evaluation Metrics}
% compilation rate,
% correctness,
% coverage,
% unique contribution,
% (F1)
% We focus on three key aspects in our evaluation of the generated unit tests. 
% First, we assess \textbf{compilation rate}, which measures the percentage of the generated unit tests without compilation errors. This helps determine how often LLMs can generate unit tests that can be executed without compilation errors.
% Second, we evaluate \textbf{correctness rate}, calculating the percentage of unit tests that are correct out of all generated unit tests for each project, providing insight into the accuracy of the test generation process.
% Finally, we measure \textbf{coverage rate}, analyzing both line and branch coverage to understand how well the generated unit tests explore the code's functionality.

% We focus on three key aspects in evaluating the generated unit tests: compilation rate, correctness rate, and coverage rate.
% \textit{Compilation rate} (ComR) measures the percentage of projects where the generated test suites compile successfully, indicating how often LLMs produce unit test suites that can be executed without any compilation errors. 
% The compilation rate for all projects in $X$ is defined as 
% $ComR = \frac{|X^{com}|}{|X|}$, 
% where $X$ is the project set and $X^{com}\subset X$ denotes the subset of projects whose test suites compile successfully.
% \textit{Correctness rate} (CR) calculates the percentage of unit tests that are correct out of all generated unit tests for each project, providing insight into the accuracy of the test generation process. 
% The correctness rate for the project $x$ is defined as
% $CR_x = \frac{|T_x^{cor}|}{|T_x|}$, 
% where $T_x$ is the generated test suite and $T_x^{cor}\subset T_x$ denotes the correct unit test set for the project $x$.
% \textit{Coverage rate} analyzes both line and branch coverage to understand how well the generated unit tests explore the code's functionality.
% The coverage rate for the project $x$ is defined as
% $CR_x = \frac{covered(x)}{total(x)}$,
% where $covered(x)$ denotes the number of covered lines/branches in project $x$ and $total(x)$ the total number of lines/branches in project $x\in X$.

% These three evaluation metrics are not independent. If a project has the generated test suite containing compilation errors, none of its unit tests can be executed successfully, leading to both the correctness rate and the coverage rate for the project being zeros. Besides, cascade errors resulting in failed tests, like missing Python dependencies, can also lead to a change in coverage rate. Therefore, considering the interdependencies between the three evaluation metrics, we extend our analysis beyond the evaluation of the vanilla unit tests to include manual fixing compilation errors and cascade errors, enabling a more comprehensive evaluation of the potential of LLMs to generate high-quality unit tests once these errors are addressed. This assessment is conducted while maintaining the same quantity and diversity of unit tests originally generated by the LLMs.
% Furthermore, we extend our analysis to examine the self-fixing capabilities of LLMs.

