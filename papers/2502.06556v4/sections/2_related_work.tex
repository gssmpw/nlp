% \begin{comment}

% \begin{itemize}
%     \item Programming Language
%         \begin{itemize}
%             \item Python: ~\citet{lemieux2023codamosa}, ~\citet{dakhel2024effective}
%             \item Java: ~\citet{siddiq2024using}, ~\citet{alagarsamy2024enhancing}, ~\citet{shin2023domain}, ~\citet{xie2023chatunitest}
%             \item C
%             \item CPP
%             \item JavaScript: ~\citet{schafer2023empirical}
%         \end{itemize}
%     \item Input Type
%         \begin{itemize}
%             \item Source Codes: ~\citet{siddiq2024using}, \citet{schafer2023empirical}, ~\citet{lemieux2023codamosa}, ~\citet{shin2023domain}, ~\citet{xie2023chatunitest}
%             \item Input/Output Examples: ~\citet{siddiq2024using} (HumanEval (Java)), ~\citet{schafer2023empirical}, 
%             \item Test Case Template: \citet{alagarsamy2024enhancing}
%             \item Class/Method/Function Description: ~\citet{siddiq2024using}, ~\citet{alagarsamy2024enhancing},
%             \item Signature: ~\citet{siddiq2024using}, ~\citet{schafer2023empirical}, ~\citet{shin2023domain} (finetune), 
%             \item Class and Method Name: ~\citet{alagarsamy2024enhancing}, ~\citet{shin2023domain} (finetune)
%             \item Class fields: ~\citet{shin2023domain} (finetune)
%             \item Existing Test Suits: ~\citet{shin2023domain}
%         \end{itemize}
%     \item Tested Codes
%         \begin{itemize}
%             \item Function (standalone/non-standalone): ~\citet{dakhel2024effective}
%             \item Class: 
%             \item Method/Function/Constructor Under Test (part of projects): ~\citet{siddiq2024using}, ~\citet{schafer2023empirical}, ~\citet{lemieux2023codamosa}, ~\citet{alagarsamy2024enhancing}, ~\citet{shin2023domain}, ~\citet{xie2023chatunitest}
%             \item Project: 
%         \end{itemize}
%     \item Benchmarks
%         \begin{itemize}
%             \item HumanEval: ~\citet{dakhel2024effective}
%             \item Refactory: ~\citet{dakhel2024effective}
%             \item HumanEval (Java): ~\citet{siddiq2024using}
%             \item Evosuite SF110: ~\citet{siddiq2024using}
%             \item JS Npm Packages: ~\citet{schafer2023empirical}
%             \item Python Modules: ~\citet{lemieux2023codamosa}
%             \item Methods2Test: ~\citet{alagarsamy2024enhancing} (finetune), ~\citet{shin2023domain} (finetune)
%             \item Open-source Java Projects: \citet{alagarsamy2024enhancing} (inference), ~\citet{xie2023chatunitest}
%             \item Defect4j: ~\citet{shin2023domain} (inference, domain adaptation)
%             \item ClassEval: 
%         \end{itemize}
%     \item Large Language Models
%         \begin{itemize}
%             \item Codex: ~\citet{siddiq2024using}, ~\citet{lemieux2023codamosa}, ~\citet{dakhel2024effective}
%             \item LLaMA2-Chat: ~\citet{dakhel2024effective}
%             \item GPT-3.5: ~\citet{siddiq2024using}, \citet{schafer2023empirical}, ~\citet{alagarsamy2024enhancing}, 
%             \item Finetuned GPT-3.5: ~\citet{alagarsamy2024enhancing}
%             \item GPT-4: ~\citet{shin2023domain}, 
%             \item StarCoder: ~\citet{siddiq2024using}, \citet{schafer2023empirical}
%             \item code-cushman-002: ~\citet{schafer2023empirical}
%             \item Bloom: ~\citet{alagarsamy2024enhancing}
%             \item CodeT5: ~\citet{alagarsamy2024enhancing}
%             \item Finetuned CodeT5: ~\citet{shin2023domain}
%         \end{itemize}
%     \item Evaluation Metrics
%         \begin{itemize}
%             \item Compilation Rate: ~\citet{siddiq2024using}
%             \item Test Correctness: ~\citet{siddiq2024using}, ~\citet{alagarsamy2024enhancing}
%             \item Test (branch/statement/line) Coverage: ~\citet{siddiq2024using}, \citet{schafer2023empirical}, ~\citet{lemieux2023codamosa}, ~\citet{alagarsamy2024enhancing}, ~\citet{shin2023domain}, ~\citet{xie2023chatunitest}
%             \item Test Smells: ~\citet{siddiq2024using}
%             \item Unique Contribution: ~\citet{schafer2023empirical}
%             \item Requirement Alignment: ~\citet{alagarsamy2024enhancing}
%             \item Mutation Score: ~\citet{shin2023domain}
%             \item BLEU Score: ~\citet{shin2023domain}
%         \end{itemize}
% \end{itemize}

% \end{comment}

\subsection{Traditional Unit Test Generation}
Traditional unit test generation methods employ search-based~\citep{harman2009theoretical, fraser2011evosuite, lukasczyk2022pynguin}, constraint-based~\citep{xiao2013characteristic}, or random-based~\citep{pacheco2007feedback} strategies to construct test suites that maximize code coverage. 
% For instance, EvoSuite~\citep{fraser2011evosuite}, a prominent search-based technique, leverages evolutionary algorithms to generate tests for Java classes.
% , aiming to achieve high coverage automatically. 
Although these traditional approaches can generate unit tests with reasonable coverage, the resulting tests often have lower readability and less meaningfulness compared to developer-written tests. As a result, automatically generated tests are frequently not directly adopted by practitioners in real-world scenarios~\citep{almasi2017industrial, grano2019scented}.

\subsection{LLM-enhanced Unit Test Generation}

Large Language Models have demonstrated strong code generation capabilities, inspiring their use in automated unit test generation. Recent approaches in LLM-enhanced unit test generation leverage zero-shot strategies~\citep{siddiq2024using}, iterative querying~\citep{schafer2023empirical}, fine-tuning on specialized datasets~\citep{alagarsamy2024enhancing}, adaptive context selection~\citep{xie2023chatunitest}, and focusing on subtle code differences~\citep{dakhel2024effective, li2023nuances}. These methods are evaluated with various metrics—including compilation success, test correctness, coverage, and bug detection—and demonstrate that LLMs can effectively surpass traditional test generation techniques.
% \citet{siddiq2024using} evaluate zero-shot unit test generation performance of LLMs for 194 classes from 47 open-source Java projects in the SF110 dataset and 160 classes from the HumanEval Java dataset through compilation rates, test correctness, test coverage, and test smells. \citet{siddiq2024using} also compare the influence of different code elements, like JavaDoc and Signature.
% \citet{schafer2023empirical} generate unit tests for 25 Javascript npm packages by iteratively querying LLMs with signatures and, optionally, the codes, documentation, and input/output examples. 
% \citet{alagarsamy2024enhancing} finetune GPT-3.5 using the Methods2Test dataset to improve unit test generation for focal methods from open-source Java projects.
% \citet{xie2023chatunitest} employ an adaptive focal context generation mechanism to dynamically generate context according to the dependency relationships of the focal methods, integrating relevant information for LLMs to generate unit tests for the focal methods.
% \citet{dakhel2024effective} improving the effectiveness of test cases generated by LLMs in terms of revealing bugs by leveraging mutation testing.
% \citet{li2023nuances} develop a new technique, called Differential Prompting, that guides the LLM to focus on subtle differences between a correct and a buggy version of the code. By doing so, the model can more effectively produce failure-inducing test cases that expose software faults. The paper’s experiments show that this approach significantly outperforms traditional LLM-based prompting and a state-of-the-art unit test generation tool.
% In practice, several tools utilize LLMs for unit test generation. ChatUniTest\citep{xie2023chatunitest} leverages ChatGPT to generate Java unit tests. Codamosa\citep{lemieux2023codamosa} employs Codex with search-based methods to create unit tests for Python modules. TestGen-LLM~\citep{alshahwan2024automated} represents the first industrial-scale deployment of LLM-generated code, ensuring code improvement and practical applicability.
% ChatUniTest \citet{xie2023chatunitest} provides a practical solution and a published tool for applying ChatGPT to Java unit test generation.
% Codamosa \citet{lemieux2023codamosa} use Codex to facilitate Search-based software testing methods to generate unit tests for methods/functions/constructors under test from Python modules.
% TestGen-LLM \citee{alshahwan2024automated}  the first report on industrial scale deployment of LLM-generated code backed by such
% assurances of code improvement.



% \citet{lemieux2023codamosa} use Codex to facilitate Search-based software testing methods to generate unit tests for methods/functions/constructors under test from Python modules.

% \citet{shin2023domain} use a finetuned CodeT5 and project-level domain adaptation to improve the code coverage and mutation score of existing test cases for Java projects.
% \citet{shin2023domain} finetune CodeT5 on the Methods2Test dataset using the conditional code generation task.

% The difference between ~\citet{shin2023domain} and our work:
% \begin{itemize}
%     \item ~\citet{shin2023domain} require existing test suites
%     \item ~\citet{shin2023domain} require a finetuned model
%     \item ~\citet{shin2023domain} require preprocess projects to build a project-specific dataset for domain adaptation
%     \item In ~\citet{shin2023domain}, ``project-level'' refers to the structural difference in each project; while in our work, ``project-level'' means the tested codes are project-level. The tested codes remain focal methods in ~\citet{shin2023domain}.
% \end{itemize}

% \citet{alshahwan2024automated} 

\subsection{Unit Test Generation Benchmark}
Current benchmarks for LLM-based unit test generation mainly focus on function-level~\cite{wang2024testeval}, class-level~\cite{du2023classeval}, or file-level code~\cite{jain2024testgeneval}. Project-level software testing benchmarks, on the other hand, often target tasks other than unit test generation. For instance, R2E-Eval1~\cite{jain2024r2e} is designed for the generation of equivalent test harnesses, SWT-Bench~\cite{mundler2024swt} focuses on fixing specific bugs rather than entire projects, and DevBench~\cite{li2024devbench} centers on software development tasks. 
% While DevBench skims the surface of project-level unit testing, its data size and quality are quite mixed, particularly for C/C\# and Java, with only five projects, respectively. Additionally, given its broad focus, DevBench doesn't include comprehensive evaluation and error analysis on project-level unit test generation.
While DevBench touches on project-level unit testing, its dataset is limited in quantity and varies in quality, especially for C/C\# and Java, with only five projects each. Moreover, due to its broad focus, DevBench lacks a comprehensive evaluation and error analysis of LLM project-level unit test generation.
% Some prior works attempt to handle project-level datasets~\cite{siddiq2024using, schafer2023empirical}, but they rely on extensive preprocessing to identify methods and their dependencies. These methods typically generate individual unit tests for specific code snippets rather than testing entire projects, which can lead to missed dependencies and increased complexity.

% The only prior work focused on project-level testing, DevBench~\cite{li2024devbench}, does not emphasize unit test generation. It provides limited insights, with a small dataset consisting of just 2 JavaScript and 5 Java projects, lacking comprehensive evaluations or error analysis. Additionally, existing project-level benchmarks often restrict evaluations to single functions~\cite{wang2024testeval, jain2024testgeneval} or use a limited number of test cases~\cite{li2024devbench}, which fails to represent the diverse challenges of real-world software projects.

% The strong performance of LLMs in code generation has spurred the development of numerous benchmarks designed to evaluate their capabilities across various tasks and scenarios\citet{chen2021evaluating, du2023classeval, hendrycks2021measuring}. In the domain of unit test case generation, existing benchmarks at the project level are often restricted to single functions\cite{wang2024testeval, jain2024testgeneval} or involve a limited number of evaluation cases\cite{li2024devbench}. These limitations hinder a comprehensive assessment of unit test generation tools in more complex and diverse real-world software projects. In contrast, our work introduces a comprehensive benchmark that encompasses multiple major programming languages and includes a substantial number of evaluation cases. This extensive benchmark provides a more robust framework for evaluating the effectiveness and scalability of unit test generation methods, addressing the gaps present in current benchmarks.
