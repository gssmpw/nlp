% We conduct further analyses, including an ablation study on prompts (\S~\ref{sec: ablation}) and unique contribution 
% analysis (\S~\ref{sec: unique}).
% \subsection{Ablation Study}
% \label{sec: ablation}
% \input{sections/6_table_ablation}
% \yibo{move this ablation to appendix? the results are not expected}
% We perform a detailed ablation study to analyze the impact of prompts on the performance of unit test generation by LLMs.
% As mentioned in \S~\ref{unit_test_generation}, the prompt is composed of programming language-specific requirements (PL), as well as requirements related to the correctness rate (CR), the compilation rate (ComR), and the coverage rate metrics (Coverage). We ablate each component and analyze the performance of unit test generation of GPT-4-Turbo using different prompts as shown in Table~\ref{tab: ablation}. 
% Requirements related to CR and ComR can help improve performance in vanilla unit tests. 
% Coverage-related requirements are not always beneficial, possibly because a high coverage rate is too abstract for LLMs to interpret effectively.
% Programming language-specific requirements improve performance in CR but have the opposite effect on ComR, LC, and BC.

% Besides, we follow the prompt template from previous work like~\citet{siddiq2024using} to move the prompts into comments (e.g., /*...*/). We compare the performance with and without comment signs in Table~\ref{tab: ablation}. Experimental results show that our prompt demonstrates a significant advantage in CR, while the prompt with comment signs exhibits marginal advantages in ComR, LC, and BC.


% \subsection{Unique Contribution of the generated unit tests}
% \label{sec: unique}
% We also explore the unique contribution of the generated unit tests on Python.
% The unique contribution is defined as the total portion of coverage contributed by each generated unit test that does not overlap with the coverage of other unit tests. 
% This is important for several reasons. First, some LLMs generate more unit tests than others, making it insufficient to rely solely on coverage rate as a metric; the unique contribution of each test should also be considered. Second, it’s crucial for LLMs to generate fewer unit tests while still achieving a high coverage rate, as running a large number of tests can sometimes be resource- or time-intensive.

% The evaluation results are shown in Table~\ref{tab: unique_contribution}. 
% The table reveals that all the tested LLMs have low rates of unique contributions, indicating a tendency to produce redundant and repetitive unit tests. 
% Although CodeQwen1.5-7B-Chat, Gemini-2.0-Flash, and CodeLlama-7b-Instruct-hf have better coverage rates than GPT-4-Turbo, they also produce significantly more unit tests. Moreover, their unique contribution is lower than GPT-4-Turbo’s, indicating that they rely on quantity rather than quality to reach high coverage. As a result, this approach may compromise the overall efficiency of the testing process.

% \input{sections/6_table_unique}

% % \subsection{LLMs Self-fix}
% % In addition to making manual corrections, we also explore how LLMs can fix execution errors on their own. By using the conversation history and the error messages generated by the testing framework, we prompt the LLMs to address the errors and rewrite the test cases accordingly in another round. The detailed prompt is listed in Appendix~\ref{appendix: prompts}. We evaluate the performance of LLM self-fixing and the three previously mentioned phases using GPT-4-Turbo and GPT-3.5-Turbo on Python. 

% % \input{sections/6_table_self_fix}

% % The comparison is shown in Table~\ref{tab: self-fix}. The number of unit tests decreases from Phase 1, although we require LLMs to rewrite all the previously generated unit tests.
% % Both GPT-4-Turbo and GPT-3.5-Turbo achieve better CR and ComR after self-fixing. LC and BC of GPT-4-Turbo decrease because of fewer unit tests compared to Phase 1.
% % When comparing LLM self-fix results to human fix results (Phase 2 and Phase 3), the performance of LLM self-fix is significantly worse. This highlights that LLMs still require substantial improvements to match the effectiveness and reliability of human-generated fixes.



% % \subsection{}{Order of Files}

