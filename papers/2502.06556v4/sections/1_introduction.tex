% the importance of unit test generation (15\% time)
Unit testing plays an important role in software development, helping identify bugs and ensuring codes are solid and maintainable. Writing unit tests is time-consuming, usually accounting for approximately 15.8\% of software development time for developers~\cite{daka2014survey}. 
% the advantages of using LLMs compared with traditional methods (readable, no human efforts)
Therefore, automated test case generation, like search-based~\cite{fraser2011evosuite, harman2009theoretical}, constraint-based~\citep{xiao2013characteristic}, and random-based~\citep{pacheco2007feedback} methods, has been proposed to create unit tests. However, the generated unit tests are usually less readable than manually-written tests and limited to certain types of functions~\cite{grano2018empirical}. Lately, large language models (LLMs) have become game-changers, significantly accelerating unit test generation and improving readability and generalizability with little to no human effort~\cite{siddiq2024using, xie2023chatunitest}. 
%This not only saves time but also makes the process smoother and more efficient. chen deleted this sentence. don't know what it means.

% % real needs 
% disadvantages of previous LLM-based methods:
% function-level/class-level codes (not realistic needs)
% project-level codes (complex preprocessing, missing dependencies, call LLMs multiple times)

Given the rapid adoption of LLMs on unit testing, the evaluation of LLM unit test generation capabilities appears to be lagging behind. 
Previous unit test generation evaluation benchmarks primarily focus on function-level, class-level, or file-level~\cite{chen2021evaluating, du2023classeval, wang2024testeval, jain2024testgeneval} codes. However, project-level codes are more representative of real-world scenarios and practical needs. The complex dependency relationships between different files in project-level codebases make unit test generation more challenging.
The only existing benchmark that has briefly explored project-level unit test generation is DevBench~\cite{li2024devbench}. 
However, due to its broad focus, the number of projects included for unit test generation is low for each language (e.g., 5 for Java and 5 combined for C and C++), with varying quality.
Half of its projects for unit test generation evaluation are difficult to track, and most of the identifiable projects have fewer than 250 Stars and fewer than 50 Forks.
DevBench also does not provide a thorough analysis of error types, potentials, or self-fixing capabilities of frontier LLMs' project-level unit test generation.
%They didn't disclose their project selection criteria as well.
%However, the projects it includes for unit test generation are low in quantity and mixed in quality. Devbench only covers 5 Java projects, 5 C/C++ projects, and 10 Python projects without detailed selection criteria. Half of the projects are hard to track, and most of the trackable projects have less than 250 Stars and less than 50 Forks.
% quality: 1. lack of detailed filtering criteria; 	2. lack of detailed data source; almost half of the samples cannot be tracked; 	3. Among samples that are able to be tracked, more than half of them have #Stars less than 250 and #Forks less than 50; 	4. license is not clearly mentioned (?)

% While some prior work has attempted to use project-level datasets, these approaches involve highly complex preprocessing to identify the methods under test and the associated functions, classes, or files with dependencies~\cite{siddiq2024using, schafer2023empirical}. In addition, previous methods focus on generating individual unit tests for specific code snippets instead of whole projects. This increases the complexity, risks missing dependencies, and often requires multiple LLM calls to fully test a project. 
% However, this work does not focus on unit test generation, so the details related to unit test generation are minimal. Additionally, the dataset used in this study is quite small, comprising only 2 JavaScript projects and 5 Java projects, and it lacks comprehensive evaluations and any error analysis.


%%%%%% Jan 7
% The only previous work that explores unit testing for entire project-level codebases is DevBench~\cite{li2024devbench}. 
% However, DevBench lacks comprehensive and important details regarding unit test generation. DevBench does not provide detailed unit test generation evaluations and analyses for each individual programming language, but aggregates evaluation results across all programming languages, which limits its utility for understanding language-specific performance.
% Besides, DevBench relies solely on oracle testing and coverage rate for unit test generation evaluation. Oracle testing heavily depends on the accuracy and completeness of the oracle, and it is resource-intensive to create and maintain accurate and comprehensive oracles. Some unexpected behaviors or edge cases may not be covered by oracles.


% \input{latex/sections/1_table_data_statistics}
% we propose a project-level benchmark (moderate project, multiple files, < 1600 lines, within maximum input length of most codeLLMs, 3 programming languagues)
% (3/5 popular programming languages)
Therefore, we propose a new project-level unit test generation evaluation benchmark, ProjectTest, to offer a larger, higher-quality project set for project-level unit test generation along with a more thorough error analysis of frontier LLMs on unit test generation.
ProjectTest covers three programming languages: Python, Java, and JavaScript. For each programming language, we construct 20 projects filtered from GitHub\footnote{https://github.com/}. 
%ProjectTest features moderate-sized projects with multiple files and includes dependencies between these files. Projects selected in ProjectTest contain less than 1600 lines of code, which fit within the maximum input length of most code language models.
ProjectTest applies clear filtering criteria to select projects. It includes moderate-sized projects with multiple files and dependencies between them. Each project has less than 1,600 lines of code, which fits within the maximum input length of most code language models. Quality is ensured by the number of stars and forks. 
% Detailed data statistics are shown in Table~\ref{tab: dataset}.

% we evaluate LLMs using full projects as input (no preprocessing, single call of LLMs, straightforward use)

%\input{latex/sections/1_table_benchmark_comparison}

% important findings: 
% We evaluate the unit test generation performance of two closed-sourced models and four open-sourced models on ProjectTest. We evaluate model performance from three perspectives: compilation rate, correctness rate, and coverage rate. In addition to the vanilla-generated unit tests, we incorporate error-fixing into the process and evaluate performance across different error-fixing phases. 

% We evaluate the unit test generation performance of four closed-sourced models and four open-sourced models on ProjectTest for each programming language.
% On ProjectTest, we evaluate LLMs with three critical metrics for unit test generation: compilation rate, correctness rate, and coverage rate. 
%The compilation rate measures the proportion of the generated unit tests that are successfully compiled or translated into a format the computer can run, highlighting LLMs' ability to generate syntactically correct tests. The correctness rate assesses the percentage of the generated unit tests that execute as intended, ensuring that the tests accurately evaluate the specified functionalities. The coverage rate quantifies the extent to which the generated tests encompass the codebase, indicating the comprehensiveness of the generated unit tests. The three metrics provide a robust framework for assessing the efficacy of LLMs in generating functional, reliable, and thorough tests.
We evaluate nine frontier LLMs, such as Claude-3.5-Sonnet~\cite{anthropic2024claude}, Gemini-2.0-Flash~\cite{team2024gemini}, and GPT-o1, on ProjectTest and conduct comprehensive error analyses. We find that all tested frontier LLMs perform moderately on ProjectTest on Python and Java, highlighting the difficulty of ProjectTest.
We also observe that different LLMs have different language-level expertise. Claude-3.4-Sonnet ranks first in Java, while GPT-o1 ranks first in JavaScript.
Among three programming languages, Java is the most difficult language, primarily due to stricter syntax.  Among all the tested models, GPT-o1 performs the best in general, especially in JavaScript. 
% Initial results from vanilla-generated unit tests reveal significant limitations due to common issues such as hallucinations and syntax errors. 

Error analyses from above also show that even frontier LLMs, like Claude-3.5-Sonnet, have significant compilation and cascade errors. Although these errors appear to be preliminary and may be relatively easy to fix, they prevent us from observing more advanced aspects of LLM performance on unit test generation, such as correctness and coverage.
%These findings highlight the importance of error-fixing as a necessary step in improving unit test quality and usability.
To address this, we first manually fix LLM's compilation and cascade errors and then re-evaluate the fixed unit tests. 
This allows us to measure not only the models' raw performance but also their potential for improvement when combined with error-fixing mechanisms. By incorporating error-fixing, we uncover critical insights into the effort required to refine generated tests and better understand the various types of errors that occur in unit tests generated by different LLMs. 
We observe that the model rankings change significantly after the manual fix, showing the significant differences in different LLMs' error distribution and their potentials after error-fixing.
% This comprehensive evaluation emphasizes that while LLMs significantly accelerate unit test generation, error-fixing remains an indispensable step for ensuring the practical applicability of generated tests.
Inspired by such findings from manual fixes, we also explore using LLMs for self-fixing their errors in generating project-level unit tests. The results show that while LLMs can correct some errors in their generated unit tests, their self-fixing abilities still lag behind the quality and reliability of human fixes.

% We summarize the contribution of this paper as follows,
% first project-level unit test generation benchmark.
% thorough error analysis by manually fixing compilable and cascade errors.
% First to assess LLMs' self fix capability on unit test generation and provide insights. \chen{yibo can you fine-tune this paragraph?}
We summarize our contributions as follows: we introduce the first project-level evaluation benchmark for unit test generation and conduct an extensive evaluation of nine frontier LLMs. Additionally, we conduct thorough error analyses by manually fixing compilation and cascade errors and provide critical insights. Inspired by the error analysis, we are the first to assess LLMs' self-fixing capability on unit test generation.
%Furthermore, we evaluate the unique contribution of individual unit tests. Our analyses indicate that LLMs have low unique contribution rates and tend to generate redundant and repeated unit tests. 

% Our experiments yield several key insights:
% \begin{itemize}
%     \item Java is the most difficult language; GPT-o1 performs the best in general;
%     \item Manual fixing improves the performance significantly; Some models that initially underperform can match or exceed stronger models after manual fixing;
%     \item Closed-source models have the ability to self-fix errors and generate better unit tests;
%     \item LLM-generated unit tests have low unique contribution rates, exhibiting a tendency toward redundancy and repetition.
% \end{itemize}

% In addition, we conduct comprehensive error analyses in different phases.
% Common errors across programming languages from different phases include hallucinations of non-existent functions or classes, missing required functions or classes, and mismatches between expected and actual values, respectively.
% What's more, we evaluate the unique contribution of individual unit tests and the capability of LLMs to self-fix the errors from the generated unit tests.
% Our experiments show that Java poses the greatest challenge for LLMs when generating unit tests compared to Python and JavaScript. Although closed-source models generally perform better overall, certain open-source models can achieve comparable or even superior results in specific scenarios. 
% What's more, we evaluate the unique contribution of individual unit tests. Our analyses indicate that LLMs have low unique contribution rates and tend to generate redundant and repeated unit tests. 
% LLMs have the ability to self-fix the errors in the generated unit tests, but they still have a long way to go to match the quality and reliability of human fixes.

% contributions:
% Our contributions can be summarized as follows.
% \begin{itemize}
% [noitemsep,topsep=0pt,itemsep=1.8pt, leftmargin=20pt]
%     \item We introduce ProjectTest, a new project-level benchmark for unit test generation.
%     \item We perform comprehensive experiments to evaluate the performance of various models in unit test generation using ProjectTest.
%     \item We design an iterative error-fixing process and perform detailed error analyses of the generated unit tests on different phases.
%     \item We further analyze the unique contribution of generated unit tests and LLMs' self-fix ability.
% \end{itemize}
