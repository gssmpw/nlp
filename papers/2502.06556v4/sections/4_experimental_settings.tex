\subsection{Models}
We evaluate five close-sourced models: GPT-o1, Gemini-2.0-Flash-Exp~\cite{team2024gemini},  Claude-3.5-Sonnet-20241022 (Claude-3.5-Sonnet)~\cite{anthropic2024claude}, GPT-4-Turbo~\cite{achiam2023gpt} and GPT-3.5-Turbo, and four open-sourced models: CodeQwen1.5-7B-Chat (CodeQwen1.5)~\cite{bai2023qwen}, DeepSeek-Coder-6.7b-Instruct (DeepSeek-Coder)~\cite{guo2024deepseek, zhu2024deepseek}, CodeLlama-7b-Instruct-hf (CodeLlama)~\cite{roziere2023code}, and CodeGemma-7b-it (CodeGemma)~\cite{team2024codegemma}. Detailed information is in Appendix~\ref{appendix: models}.

\subsection{Implementation Details}
We use zero-shot prompting for unit test generation. 
The temperature is set to 0 during inference.
% to ensure deterministic outputs for LLMs without Sparse MoE. 
Experiments are conducted on 8 NVIDIA A100 GPUs. 
The maximum input length is configured to match the token limit of each LLM to evaluate model capabilities.
We use Pytest\footnote{https://docs.pytest.org/en/stable/} for Python, Jacoco\footnote{https://www.eclemma.org/jacoco/} for Java, and JEST\footnote{https://jestjs.io/} for JavaScript regarding testing frameworks.
