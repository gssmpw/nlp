@misc{autogen,
    title={AutoGen},
    author={AutoGen},
    year={2024},
    howpublished={\url{https://github.com/microsoft/autogen}}
}

@article{bi2024forest,
  title={Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning},
  author={Bi, Zhenni and Han, Kai and Liu, Chuanjian and Tang, Yehui and Wang, Yunhe},
  journal={arXiv preprint arXiv:2412.09078},
  year={2024}
}

@article{bran2023chemcrow,
  title={ChemCrow: Augmenting large-language models with chemistry tools},
  author={Bran, Andres M and Cox, Sam and Schilter, Oliver and Baldassari, Carlo and White, Andrew D and Schwaller, Philippe},
  journal={arXiv preprint arXiv:2304.05376},
  year={2023}
}

@article{chen2023llava,
  title={Llava-interactive: An all-in-one demo for image chat, segmentation, generation and editing},
  author={Chen, Wei-Ge and Spiridonova, Irina and Yang, Jianwei and Gao, Jianfeng and Li, Chunyuan},
  journal={arXiv preprint arXiv:2311.00571},
  year={2023}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{gao2023pal,
  title={Pal: Program-aided language models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={International Conference on Machine Learning},
  pages={10764--10799},
  year={2023},
  organization={PMLR}
}

@misc{gpt4oplugin,
    title={Function calling - OpenAI},
    author={OpenAI},
    year={2023},
    howpublished={\url{https://platform.openai.com/docs/guides/function-calling}}
}

@article{guan2025rstar,
  title={rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking},
  author={Guan, Xinyu and Zhang, Li Lyna and Liu, Yifei and Shang, Ning and Sun, Youran and Zhu, Yi and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2501.04519},
  year={2025}
}

@article{hu2024visual,
  title={Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models},
  author={Hu, Yushi and Shi, Weijia and Fu, Xingyu and Roth, Dan and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A and Krishna, Ranjay},
  journal={arXiv preprint arXiv:2406.09403},
  year={2024}
}

@article{kang2024chatmof,
  title={ChatMOF: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models},
  author={Kang, Yeonghun and Kim, Jihan},
  journal={Nature Communications},
  volume={15},
  number={1},
  pages={4705},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{khot2022decomposed,
  title={Decomposed prompting: A modular approach for solving complex tasks},
  author={Khot, Tushar and Trivedi, Harsh and Finlayson, Matthew and Fu, Yao and Richardson, Kyle and Clark, Peter and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2210.02406},
  year={2022}
}

@inproceedings{komeili-etal-2022-internet,
    title = "{I}nternet-Augmented Dialogue Generation",
    author = "Komeili, Mojtaba  and
      Shuster, Kurt  and
      Weston, Jason",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.579",
    doi = "10.18653/v1/2022.acl-long.579",
    pages = "8460--8478",
    abstract = "The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b).",
}

@misc{langchain,
    title={{LangChain}},
    author={LangChain, Inc},
    year={2024},
    howpublished={\url{https://github.com/langchain-ai/langchain}}
}

@article{lazaridou2022internet,
  title={Internet-augmented language models through few-shot prompting for open-domain question answering},
  author={Lazaridou, Angeliki and Gribovskaya, Elena and Stokowiec, Wojciech and Grigorev, Nikolai},
  journal={arXiv preprint arXiv:2203.05115},
  year={2022}
}

@article{li2024mmedagent,
  title={Mmedagent: Learning to use medical tools with multi-modal agent},
  author={Li, Binxu and Yan, Tiankai and Pan, Yuanting and Luo, Jie and Ji, Ruiyang and Ding, Jiayuan and Xu, Zhe and Liu, Shilong and Dong, Haoyu and Lin, Zihao and others},
  journal={arXiv preprint arXiv:2407.02483},
  year={2024}
}

@article{liu2023llava,
  title={Llava-plus: Learning to use tools for creating multimodal agents},
  author={Liu, Shilong and Cheng, Hao and Liu, Haotian and Zhang, Hao and Li, Feng and Ren, Tianhe and Zou, Xueyan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
  journal={arXiv preprint arXiv:2311.05437},
  year={2023}
}

@article{lu2023chameleon,
  title={Chameleon: Plug-and-play compositional reasoning with large language models},
  author={Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  year={2023}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@article{perez2020unsupervised,
  title={Unsupervised question decomposition for question answering},
  author={Perez, Ethan and Lewis, Patrick and Yih, Wen-tau and Cho, Kyunghyun and Kiela, Douwe},
  journal={arXiv preprint arXiv:2002.09758},
  year={2020}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={68539--68551},
  year={2023}
}

@article{schmidgall2024agentclinic,
  title={AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments},
  author={Schmidgall, Samuel and Ziaei, Rojin and Harris, Carl and Reis, Eduardo and Jopling, Jeffrey and Moor, Michael},
  journal={arXiv preprint arXiv:2405.07960},
  year={2024}
}

@article{shuster2022blenderbot,
  title={Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage},
  author={Shuster, Kurt and Xu, Jing and Komeili, Mojtaba and Ju, Da and Smith, Eric Michael and Roller, Stephen and Ung, Megan and Chen, Moya and Arora, Kushal and Lane, Joshua and others},
  journal={arXiv preprint arXiv:2208.03188},
  year={2022}
}

@article{sun2023pearl,
  title={Pearl: Prompting large language models to plan and execute actions over long documents},
  author={Sun, Simeng and Liu, Yang and Wang, Shuohang and Zhu, Chenguang and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2305.14564},
  year={2023}
}

@article{tao2023webwise,
  title={Webwise: Web interface control and sequential exploration with large language models},
  author={Tao, Heyi and TV, Sethuraman and Shlapentokh-Rothman, Michal and Hoiem, Derek and Ji, Heng},
  journal={arXiv preprint arXiv:2310.16042},
  year={2023}
}

@misc{thoppilan2022lamda,
  doi = {10.48550/ARXIV.2201.08239},
  url = {https://arxiv.org/abs/2201.08239},
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {LaMDA: Language Models for Dialog Applications},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{wang2024mobile,
  title={Mobile-Agent: Autonomous multi-modal mobile device agent with visual perception},
  author={Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  journal={arXiv preprint arXiv:2401.16158},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}

@article{zhang2024toolbehonest,
  title={ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models},
  author={Zhang, Yuxiang and Chen, Jing and Wang, Junjie and Liu, Yaxin and Yang, Cheng and Shi, Chufan and Zhu, Xinyu and Lin, Zihao and Wan, Hanwen and Yang, Yujiu and others},
  journal={arXiv preprint arXiv:2406.20015},
  year={2024}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

