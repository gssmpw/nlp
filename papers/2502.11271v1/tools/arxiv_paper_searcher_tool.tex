\begin{codecolorbox}[ArXiv Paper Searcher Tool: Metadata]{python}
tool_name="ArXiv_Paper_Searcher_Tool",

tool_description="A tool that searches arXiv for papers based on a given query.",

input_types={
    "query": "str - The search query for arXiv papers.",
    "size": "int - The number of results per page (25, 50, 100, or 200). If None, use 25.",
    "max_results": "int - The maximum number of papers to return (default: 25). Should be less than or equal to 100."
},

output_type="list - A list of dictionaries containing paper information.",

demo_commands=[
    {
        "command": 'execution = tool.execute(query="tool agents with large language models")',
        "description": "Search for papers about tool agents with large language models."
    },
    {
        "command": 'execution = tool.execute(query="quantum computing", size=100, max_results=50)',
        "description": "Search for quantum computing papers, with 100 results per page, returning a maximum of 50 papers."
    },
    {
        "command": 'execution = tool.execute(query="machine learning", max_results=75)',
        "description": "Search for machine learning papers, returning a maximum of 75 papers."
    },
],

user_metadata={
    "valid_sizes": [25, 50, 100, 200],
    "base_url": "https://arxiv.org/search/"
}
\end{codecolorbox}



\begin{textcolorbox}[ArXiv Paper Searcher Tool: Example 1]
\toolinput{query}
enhance mathematical reasoning with large language models
\\\\
% \toolinput{command}
% \\
% \command{tool.execute(query="enhance mathematical reasoning with large language models")}
% \\\\
\tooloutput{article 1, search date: Jan 14, 2025}
\\\\
\textbf{Title:} Enhancing Reasoning through Process Supervision with Monte Carlo Tree Search

\textbf{Authors:} Shuangtao Li, Shuaihao Dong, Kexin Luan, Xinhan Di, Chaofan Ding

\textbf{Abstract:} Large language models (LLMs) have demonstrated their remarkable capacity across a variety of tasks. 
...
% However, reasoning remains a challenge for LLMs. To improve LLMs' reasoning ability, process supervision has proven to be better than outcome supervision. In this work, we study using Monte Carlo Tree Search (MCTS) to generate process supervision data with LLMs themselves for training them. We sample reasoning steps with an LLM and assign each step a score that captures its \"relative correctness,\" and the LLM is then trained by minimizing weighted log-likelihood of generating the reasoning steps. This generate-then-train process is repeated iteratively until convergence.Our experimental results demonstrate that the proposed methods considerably improve the performance of LLMs on two mathematical reasoning datasets. 
Furthermore, models trained on one dataset also exhibit improved performance on the other, showing the transferability of the enhanced reasoning ability.

\textbf{Link:} \url{https://arxiv.org/abs/2501.01478}
\\\\
\tooloutput{article 2, search date: Jan 14, 2025}
\\\\
\textbf{Title:} Open Eyes, Then Reason: Fine-grained Visual Mathematical Understanding in MLLMs

\textbf{Authors:} Shan Zhang, Aotian Chen, Yanpeng Sun, Jindong Gu, Yi-Yu Zheng, Piotr Koniusz, Kai Zou, Anton van den Hengel, Yuan Xue

\textbf{Abstract:}  Current multimodal large language models (MLLMs) often underperform on mathematical problem-solving tasks that require fine-grained visual understanding. ...
% The limitation is largely attributable to inadequate perception of geometric primitives during image-level contrastive pre-training (e.g., CLIP). While recent efforts to improve math MLLMs have focused on scaling up mathematical visual instruction datasets and employing stronger LLM backbones, they often overlook persistent errors in visual recognition. In this paper, we systematically evaluate the visual grounding capabilities of state-of-the-art MLLMs and reveal a significant negative correlation between visual grounding accuracy and problem-solving performance, underscoring the critical role of fine-grained visual understanding. Notably, advanced models like GPT-4o exhibit a 70% error rate when identifying geometric entities, highlighting that this remains a key bottleneck in visual mathematical reasoning. To address this, we propose a novel approach, SVE-Math (Selective Vision-Enhanced Mathematical MLLM), featuring a geometric-grounded vision encoder and a feature router that dynamically adjusts the contribution of hierarchical visual feature maps. Our model recognizes accurate visual primitives and generates precise visual prompts tailored to the language model's reasoning needs. In experiments, SVE-Math-Qwen2.5-7B outperforms other 7B models by 15% on MathVerse and is compatible with GPT-4V on MathVista. Despite being trained on smaller datasets, SVE-Math-7B achieves competitive performance on GeoQA, rivaling models trained on significantly larger datasets. 
Our findings emphasize the importance of incorporating fine-grained visual understanding into MLLMs and provide a promising direction for future research.

\textbf{Link:} \url{https://arxiv.org/abs/2501.06430}
\\\\
\tooloutput{structured result}
\begin{codebox}
[
    {
        "title": "Enhancing Reasoning through Process Supervision with Monte Carlo Tree Search",
        "authors": "Shuangtao Li, Shuaihao Dong, Kexin Luan, Xinhan Di, Chaofan Ding",
        "abstract": "Large language models (LLMs) have demonstrated their remarkable capacity across a variety of tasks. ... Furthermore, models trained on one dataset also exhibit improved performance on the other, showing the transferability of the enhanced reasoning ability.",
        "link": "https://arxiv.org/abs/2501.01478"
    },
    {
        "title": "Open Eyes, Then Reason: Fine-grained Visual Mathematical Understanding in MLLMs",
        "authors": "Shan Zhang, Aotian Chen, Yanpeng Sun, Jindong Gu, Yi-Yu Zheng, Piotr Koniusz, Kai Zou, Anton van den Hengel, Yuan Xue",
        "abstract": "Current multimodal large language models (MLLMs) often underperform on mathematical problem-solving tasks that require fine-grained visual understanding. ... Our findings emphasize the importance of incorporating fine-grained visual understanding into MLLMs and provide a promising direction for future research.",
        "link": "https://arxiv.org/abs/2501.06430"
    }
]
\end{codebox}

\end{textcolorbox}


\begin{textcolorbox}[ArXiv Paper Searcher Tool: Example 2]
\toolinput{query}
automated scientific discovery
\\\\
\tooloutput{article 1, search date: Jan 14, 2025}
\\\\
\textbf{Title:} BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discover

\textbf{Authors:} Kanishk Gandhi, Michael Y. Li, Lyle Goodyear, Louise Li, Aditi Bhaskar, Mohammed Zaman, Noah D. Goodman

\textbf{Abstract:} Understanding the world and explaining it with scientific theories is a central aspiration of artificial intelligence research. Proposing theories, designing experiments to test them, and then revising them based on data are fundamental to scientific discovery. 
...
% Despite the significant promise of LLM-based scientific agents, no benchmarks systematically test LLM's ability to propose scientific models, collect experimental data, and revise them in light of new data. We introduce BoxingGym, a benchmark with 10 environments for systematically evaluating both experimental design (e.g. collecting data to test a scientific theory) and model discovery (e.g. proposing and revising scientific theories). To enable tractable and quantitative evaluation, we implement each environment as a generative probabilistic model with which a scientific agent can run interactive experiments. These probabilistic models are drawn from various real-world scientific domains ranging from psychology to ecology. To quantitatively evaluate a scientific agent's ability to collect informative experimental data, we compute the expected information gain (EIG), an information-theoretic quantity which measures how much an experiment reduces uncertainty about the parameters of a generative model. A good scientific theory is a concise and predictive explanation. Therefore, to quantitatively evaluate model discovery, we ask a scientific agent to explain their model and then assess whether this explanation enables another scientific agent to make reliable predictions about this environment. In addition to this explanation-based evaluation, we compute standard model evaluation metrics such as prediction errors. We find that current LLMs, such as GPT-4o, struggle with both experimental design and model discovery.
We find that augmenting the LLM-based agent with an explicit statistical model does not reliably improve these results.

\textbf{Link:} \url{https://arxiv.org/abs/2501.01540}
\\\\
\tooloutput{article 2, search date: Jan 14, 2025}
\\\\
\textbf{Title:} Automating the Search for Artificial Life with Foundation Models

\textbf{Authors:} Akarsh Kumar, Chris Lu, Louis Kirsch, Yujin Tang, Kenneth O. Stanley, Phillip Isola, David Ha

\textbf{Abstract:}  With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. 
...
% Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial-and-error to discover the configurations of lifelike simulations. This paper presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called Automated Search for Artificial Life (ASAL), (1) finds simulations that produce target phenomena, (2) discovers simulations that generate temporally open-ended novelty, and (3) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates including Boids, Particle Life, Game of Life, Lenia, and Neural Cellular Automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids lifeforms, as well as cellular automata that are open-ended like Conway's Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. 
This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.

\textbf{Link:} \url{https://arxiv.org/abs/2412.17799}
\\\\
\tooloutput{structured result}
\begin{codebox}
[
    {
        "title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery",
        "authors": "Kanishk Gandhi, Michael Y. Li, Lyle Goodyear, Louise Li, Aditi Bhaskar, Mohammed Zaman, Noah D. Goodman",
        "abstract": "Understanding the world and explaining it with scientific theories is a central aspiration of artificial intelligence research. ... We find that augmenting the LLM-based agent with an explicit statistical model does not reliably improve these results.",
        "link": "https://arxiv.org/abs/2501.01540"
    },
    {
        "title": "Automating the Search for Artificial Life with Foundation Models",
        "authors": "Akarsh Kumar, Chris Lu, Louis Kirsch, Yujin Tang, Kenneth O. Stanley, Phillip Isola, David Ha",
        "abstract": "With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. ... This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.",
        "link": "https://arxiv.org/abs/2412.17799"
    }
]
\end{codebox}

\end{textcolorbox}