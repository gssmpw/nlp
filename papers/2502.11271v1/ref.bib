@article{lu2024avisionlanguage,
  title={A visual-language foundation model for computational pathology},
  author={Lu, Ming Y and Chen, Bowen and Williamson, Drew FK and Chen, Richard J and Liang, Ivy and Ding, Tong and Jaume, Guillaume and Odintsov, Igor and Le, Long Phi and Gerber, Georg and others},
  journal={Nature Medicine},
  pages={863â€“874},
  volume={30},
  year={2024},
  publisher={Nature Publishing Group}
}

% DINO
@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9650--9660},
  year={2021}
}

% DINO-X
@misc{ren2024dinoxunifiedvisionmodel,
      title={DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding}, 
      author={Tianhe Ren and Yihao Chen and Qing Jiang and Zhaoyang Zeng and Yuda Xiong and Wenlong Liu and Zhengyu Ma and Junyi Shen and Yuan Gao and Xiaoke Jiang and Xingyu Chen and Zhuheng Song and Yuhong Zhang and Hongjie Huang and Han Gao and Shilong Liu and Hao Zhang and Feng Li and Kent Yu and Lei Zhang},
      year={2024},
      eprint={2411.14347},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.14347}, 
}

% VQA 2.0
@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and pattern Recognition},
  pages={6904--6913},
  year={2017}
}

% Hallusion-VD
@inproceedings{guan2024hallusionbench,
  title={{HallusionBench}: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models},
  author={Guan, Tianrui and Liu, Fuxiao and Wu, Xiyang and Xian, Ruiqi and Li, Zongxia and Liu, Xiaoyu and Wang, Xijun and Chen, Lichang and Huang, Furong and Yacoob, Yaser and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14375--14385},
  year={2024}
}

% AlgoPUzzleVQA
@article{ghosal2024language,
  title={Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning},
  author={Ghosal, Deepanway and Han, Vernon Toh Yan and Ken, Chia Yew and Poria, Soujanya},
  journal={arXiv preprint arXiv:2403.03864},
  year={2024}
}

% PuzzleVQA
@article{chia2024puzzlevqa,
  title={{PuzzleVQA}: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns},
  author={Chia, Yew Ken and Han, Vernon Toh Yan and Ghosal, Deepanway and Bing, Lidong and Poria, Soujanya},
  journal={arXiv preprint arXiv:2403.13315},
  year={2024}
}

% Game of 24
@misc{24game,
    title={Math Twenty Four (24s Game) Dataset},
    author={Nathan Lile},
    year={2024},
    howpublished={\url{https://huggingface.co/datasets/nlile/24-game}}
}

% Omni-MATH
  @article{gao2024omni,
  title={{Omni-MATH}: A universal olympiad level mathematic benchmark for large language models},
  author={Gao, Bofei and Song, Feifan and Yang, Zhe and Cai, Zefan and Miao, Yibo and Dong, Qingxiu and Li, Lei and Ma, Chenghao and Chen, Liang and Xu, Runxin and others},
  journal={arXiv preprint arXiv:2410.07985},
  year={2024}
}

% CLEVR-Math
@article{lindstrom2022clevr,
  title={{CLEVR-Math}: A dataset for compositional language, visual and mathematical reasoning},
  author={Lindstr{\"o}m, Adam Dahlgren and Abraham, Savitha Sam},
  journal={arXiv preprint arXiv:2208.05358},
  year={2022}
}

% MathVista
@inproceedings{lu2024mathvista,
  author    = {Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  title     = {{MathVista}: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  booktitle={International Conference on Learning Representations (ICLR)},
  year      = {2024}
}

% GPQA
@article{rein2023gpqa,
  title={{GPQA}: A graduate-level google-proof q\&a benchmark},
  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2311.12022},
  year={2023}
}

% MMLU-Pro
@article{wang2024mmlu,
  title={{MMLU-Pro}: A more robust and challenging multi-task language understanding benchmark},
  author={Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and others},
  journal={arXiv preprint arXiv:2406.01574},
  year={2024}
}

% SciFIBench
@article{roberts2024scifibench,
  title={{SciFIBench}: Benchmarking Large Multimodal Models for Scientific Figure Interpretation},
  author={Roberts, Jonathan and Han, Kai and Houlsby, Neil and Albanie, Samuel},
  journal={arXiv preprint arXiv:2405.08807},
  year={2024}
}
% MedQA
@article{jin2021disease,
  title={What disease does this patient have? a large-scale open domain question answering dataset from medical exams},
  author={Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  journal={Applied Sciences},
  volume={11},
  number={14},
  pages={6421},
  year={2021},
  publisher={MDPI}
}

% PathCLS (To be confirmed by Bowen)
@inproceedings{sun2025pathmmu,
  title={{PathMMU}: A massive multimodal expert-level benchmark for understanding and reasoning in pathology},
  author={Sun, Yuxuan and Wu, Hao and Zhu, Chenglu and Zheng, Sunyi and Chen, Qizi and Zhang, Kai and Zhang, Yunlong and Wan, Dan and Lan, Xiaoxiao and Zheng, Mengyue and others},
  booktitle={European Conference on Computer Vision},
  pages={56--73},
  year={2025},
  organization={Springer}
}

% PathVQA
@article{he2020pathvqa,
  title={{PathVQA}: 30000+ questions for medical visual question answering},
  author={He, Xuehai and Zhang, Yichen and Mou, Luntian and Xing, Eric and Xie, Pengtao},
  journal={arXiv preprint arXiv:2003.10286},
  year={2020}
}

% SLAKE
@inproceedings{liu2021slake,
  title={{SLAKE}: A semantically-labeled knowledge-enhanced dataset for medical visual question answering},
  author={Liu, Bo and Zhan, Li-Ming and Xu, Li and Ma, Lin and Yang, Yan and Wu, Xiao-Ming},
  booktitle={2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)},
  pages={1650--1654},
  year={2021},
  organization={IEEE}
}

% GAIA-Text
@article{mialon2023gaia,
  title={{GAIA}: a benchmark for general ai assistants},
  author={Mialon, Gr{\'e}goire and Fourrier, Cl{\'e}mentine and Swift, Craig and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  journal={arXiv preprint arXiv:2311.12983},
  year={2023}
}

% GPT4o-Plugin
@misc{gpt4oplugin,
    title={Function calling - OpenAI},
    author={OpenAI},
    year={2023},
    howpublished={\url{https://platform.openai.com/docs/guides/function-calling}}
}

% LangChain
@misc{langchain,
    title={{LangChain}},
    author={LangChain, Inc},
    year={2024},
    howpublished={\url{https://github.com/langchain-ai/langchain}}
}

% AutoGen
@misc{autogen,
    title={AutoGen},
    author={AutoGen},
    year={2024},
    howpublished={\url{https://github.com/microsoft/autogen}}
}

@article{perez2020unsupervised,
  title={Unsupervised question decomposition for question answering},
  author={Perez, Ethan and Lewis, Patrick and Yih, Wen-tau and Cho, Kyunghyun and Kiela, Douwe},
  journal={arXiv preprint arXiv:2002.09758},
  year={2020}
}

@article{khot2022decomposed,
  title={Decomposed prompting: A modular approach for solving complex tasks},
  author={Khot, Tushar and Trivedi, Harsh and Finlayson, Matthew and Fu, Yao and Richardson, Kyle and Clark, Peter and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2210.02406},
  year={2022}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@article{sun2023pearl,
  title={Pearl: Prompting large language models to plan and execute actions over long documents},
  author={Sun, Simeng and Liu, Yang and Wang, Shuohang and Zhu, Chenguang and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2305.14564},
  year={2023}
}

@article{hao2023reasoning,
  title={Reasoning with language model is planning with world model},
  author={Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
  journal={arXiv preprint arXiv:2305.14992},
  year={2023}
}

@article{ding2023everything,
  title={Everything of thoughts: Defying the law of penrose triangle for thought generation},
  author={Ding, Ruomeng and Zhang, Chaoyun and Wang, Lu and Xu, Yong and Ma, Minghua and Zhang, Wei and Qin, Si and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2311.04254},
  year={2023}
}

@article{zheng2023take,
  title={Take a step back: Evoking reasoning via abstraction in large language models},
  author={Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H and Le, Quoc V and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.06117},
  year={2023}
}

@inproceedings{komeili-etal-2022-internet,
    title = "{I}nternet-Augmented Dialogue Generation",
    author = "Komeili, Mojtaba  and
      Shuster, Kurt  and
      Weston, Jason",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.579",
    doi = "10.18653/v1/2022.acl-long.579",
    pages = "8460--8478",
    abstract = "The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b).",
}

@misc{thoppilan2022lamda,
  doi = {10.48550/ARXIV.2201.08239},
  url = {https://arxiv.org/abs/2201.08239},
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {LaMDA: Language Models for Dialog Applications},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{lazaridou2022internet,
  title={Internet-augmented language models through few-shot prompting for open-domain question answering},
  author={Lazaridou, Angeliki and Gribovskaya, Elena and Stokowiec, Wojciech and Grigorev, Nikolai},
  journal={arXiv preprint arXiv:2203.05115},
  year={2022}
}

@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}


@inproceedings{gao2023pal,
  title={Pal: Program-aided language models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={International Conference on Machine Learning},
  pages={10764--10799},
  year={2023},
  organization={PMLR}
}

@article{shuster2022blenderbot,
  title={Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage},
  author={Shuster, Kurt and Xu, Jing and Komeili, Mojtaba and Ju, Da and Smith, Eric Michael and Roller, Stephen and Ung, Megan and Chen, Moya and Arora, Kushal and Lane, Joshua and others},
  journal={arXiv preprint arXiv:2208.03188},
  year={2022}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={68539--68551},
  year={2023}
}

@article{chen2023llava,
  title={Llava-interactive: An all-in-one demo for image chat, segmentation, generation and editing},
  author={Chen, Wei-Ge and Spiridonova, Irina and Yang, Jianwei and Gao, Jianfeng and Li, Chunyuan},
  journal={arXiv preprint arXiv:2311.00571},
  year={2023}
}

@article{wang2024mobile,
  title={Mobile-Agent: Autonomous multi-modal mobile device agent with visual perception},
  author={Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  journal={arXiv preprint arXiv:2401.16158},
  year={2024}
}

@article{liu2023llava,
  title={Llava-plus: Learning to use tools for creating multimodal agents},
  author={Liu, Shilong and Cheng, Hao and Liu, Haotian and Zhang, Hao and Li, Feng and Ren, Tianhe and Zou, Xueyan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
  journal={arXiv preprint arXiv:2311.05437},
  year={2023}
}

@article{tao2023webwise,
  title={Webwise: Web interface control and sequential exploration with large language models},
  author={Tao, Heyi and TV, Sethuraman and Shlapentokh-Rothman, Michal and Hoiem, Derek and Ji, Heng},
  journal={arXiv preprint arXiv:2310.16042},
  year={2023}
}

@article{bran2023chemcrow,
  title={ChemCrow: Augmenting large-language models with chemistry tools},
  author={Bran, Andres M and Cox, Sam and Schilter, Oliver and Baldassari, Carlo and White, Andrew D and Schwaller, Philippe},
  journal={arXiv preprint arXiv:2304.05376},
  year={2023}
}

@article{zhang2024toolbehonest,
  title={ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models},
  author={Zhang, Yuxiang and Chen, Jing and Wang, Junjie and Liu, Yaxin and Yang, Cheng and Shi, Chufan and Zhu, Xinyu and Lin, Zihao and Wan, Hanwen and Yang, Yujiu and others},
  journal={arXiv preprint arXiv:2406.20015},
  year={2024}
}

@article{m2024augmenting,
  title={Augmenting large language models with chemistry tools},
  author={M. Bran, Andres and Cox, Sam and Schilter, Oliver and Baldassari, Carlo and White, Andrew D and Schwaller, Philippe},
  journal={Nature Machine Intelligence},
  pages={1--11},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{kang2024chatmof,
  title={ChatMOF: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models},
  author={Kang, Yeonghun and Kim, Jihan},
  journal={Nature Communications},
  volume={15},
  number={1},
  pages={4705},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{li2024mmedagent,
  title={Mmedagent: Learning to use medical tools with multi-modal agent},
  author={Li, Binxu and Yan, Tiankai and Pan, Yuanting and Luo, Jie and Ji, Ruiyang and Ding, Jiayuan and Xu, Zhe and Liu, Shilong and Dong, Haoyu and Lin, Zihao and others},
  journal={arXiv preprint arXiv:2407.02483},
  year={2024}
}

@article{schmidgall2024agentclinic,
  title={AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments},
  author={Schmidgall, Samuel and Ziaei, Rojin and Harris, Carl and Reis, Eduardo and Jopling, Jeffrey and Moor, Michael},
  journal={arXiv preprint arXiv:2405.07960},
  year={2024}
}

@article{lu2023chameleon,
  title={Chameleon: Plug-and-play compositional reasoning with large language models},
  author={Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  year={2023}
}

@article{hu2024visual,
  title={Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models},
  author={Hu, Yushi and Shi, Weijia and Fu, Xingyu and Roth, Dan and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A and Krishna, Ranjay},
  journal={arXiv preprint arXiv:2406.09403},
  year={2024}
}

% Palm
@article{chowdhery2022palm,
  title={{PaLM}: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}
% GPT-3
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{openai2023gpt4,
  title={{GPT-4} Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}


% Optimized toolset
@article{paramanayakam2024less,
  title={Less is more: Optimizing function calling for llm execution on edge devices},
  author={Paramanayakam, Varatheepan and Karatzas, Andreas and Anagnostopoulos, Iraklis and Stamoulis, Dimitrios},
  journal={arXiv preprint arXiv:2411.15399},
  year={2024}
}
% Tools Can Fail
@article{sun2024tools,
  title={Tools Fail: Detecting Silent Errors in Faulty Tools},
  author={Sun, Jimin and Min, So Yeon and Chang, Yingshan and Bisk, Yonatan},
  journal={arXiv preprint arXiv:2406.19228},
  year={2024}
}
@article{lumer2024toolshed,
  title={Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and Tool Knowledge Bases},
  author={Lumer, Elias},
  journal={arXiv preprint arXiv:2410.14594},
  year={2024}
}
@inproceedings{fore2024geckopt,
  title={GeckOpt: LLM System Efficiency via Intent-Based Tool Selection},
  author={Fore, Michael and Singh, Simranjit and Stamoulis, Dimitrios},
  booktitle={Proceedings of the Great Lakes Symposium on VLSI 2024},
  pages={353--354},
  year={2024}
}


@article{guan2025rstar,
  title={rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking},
  author={Guan, Xinyu and Zhang, Li Lyna and Liu, Yifei and Shang, Ning and Sun, Youran and Zhu, Yi and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2501.04519},
  year={2025}
}

@article{bi2024forest,
  title={Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning},
  author={Bi, Zhenni and Han, Kai and Liu, Chuanjian and Tang, Yehui and Wang, Yunhe},
  journal={arXiv preprint arXiv:2412.09078},
  year={2024}
}
% LLMs frequently generate invalid or non-executable plans
@article{li2024formal,
  title={{Formal-LLM}: Integrating formal language and natural language for controllable llm-based agents},
  author={Li, Zelong and Hua, Wenyue and Wang, Hao and Zhu, He and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2402.00798},
  year={2024}
}
% assigning both high-level planning and executable command generation to a LLM  can lead to errors
@article{ji2024testing,
  title={Testing and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs},
  author={Ji, Zhenlan and Wu, Daoyuan and Ma, Pingchuan and Li, Zongjie and Wang, Shuai},
  journal={arXiv preprint arXiv:2404.17833},
  year={2024}
}
