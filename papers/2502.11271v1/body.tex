\section{Introduction}
\label{sec:intro}
Large language models (LLMs) \citep{brown2020language, chowdhery2022palm, openai2023gpt4} have made rapid progress on tasks such as summarization, translation \citep{thoppilan2022lamda}, code generation \citep{nakano2021webgpt}, and math problem solving \citep{shuster2022blenderbot}. However, complex reasoning tasks that involve multiple steps, logical decomposition, or specialized domain knowledge remains challenging. For example, solving a visual riddle may require fine-grained image understanding and text-based reasoning, while a math or chemistry question can require thorough computations or domain expertise. Existing prompting methods often fail to orchestrate these varied processes into a coherent chain of reasoning \citep{yao2022react}. 

A promising direction to address these challenges is to \emph{augment} LLMs with \emph{external tools}. By offloading specialized subtasks (e.g., web queries, Python-based calculations, and specialized scientific tools) to dedicated modules, LLMs can focus on higher-level planning and synthesis. Several frameworks have explored such tool usage, from those relying on extensive supervised data and fine-tuning \citep{schick2023toolformer, liu2023llava}, to static solutions without refinement \citep{lu2023chameleon}, and those limited to one specialized domain of tools \citep{nakano2021webgpt,tao2023webwise,hu2024visual}. Although these methods perform well on specific tasks, they still face challenges that hinder general widespread use. Many require substantial training with curated data, which limits their adaptability to new domains. Others are designed for a particular domain \citep{bran2023chemcrow,kang2024chatmof,li2024mmedagent,schmidgall2024agentclinic} or cannot easily support multi-step problem-solving \citep{lu2023chameleon}, restricting their generality.

\begin{figure*}[th!]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/framework_example.pdf}
    \vspace{-3mm}
    \caption{The demonstration of a self-contained example from Figure \ref{fig:model_framework}. We visualize the tool cards for selected tools, the initial plan generated by the planner, and two steps in which the planner and the executor orchestrate low-level planing and tool usage before arriving at the final answer. See \S\ref{app:demo_example} for details and \S\ref{app:exp_examples} for more examples. An interactive visualization of these examples is available at \url{https://octotools.github.io/\#visualization}.}
    % \vspace{-2mm}
\label{fig:model_example}
\end{figure*}

In this paper, we propose \model, a \emph{training-free} (i.e., it does not require updating model weights), \emph{user-friendly}, and \emph{extensible} agentic framework for tackling \emph{complex reasoning} tasks across diverse domains (Figure~\ref{fig:model_framework}). A key feature of \model is the concept of \emph{tool cards}, standardized wrappers that encapsulate heterogeneous tools (e.g., Python calculators, web search APIs, and domain-specific modules), along with metadata such as input-output formats, usage constraints, and best practices that delineate ideal use cases. This standardized design enables easy integration, replacement, or expansion of tools—unlike approaches requiring painstaking re-engineering for each new tool \citep{lu2023chameleon, hu2024visual}.

Building on these tool cards, \model employs a dedicated \emph{planner} that governs both high-level and low-level planning. Given a user query, the planner proposes a tentative global plan for how various tools might be employed. At each step, it generates a text-based \emph{action} (including sub-goals and tool selection) conditioned on the evolving \emph{context}. A separate \emph{executor} instantiates tool calls by converting this textual action into an executable command, running the corresponding tool, and updating the context with the results. By separating strategic planning from command generation, \model reduces errors and increases transparency, making the system more reliable and easier to maintain.

An additional challenge in agentic systems is determining which subset of tools to enable for a given domain. Although providing many tools can be beneficial, enabling them all may introduce noise or slow performance \citep{lumer2024toolshed, fore2024geckopt, paramanayakam2024less}. To address this, we propose a lightweight \emph{toolset optimization} algorithm that identifies a more useful subset of tools for each task based on validation performance, ultimately improving both accuracy and efficiency.

While recent general agent frameworks also allow LLMs to use external tools autonomously, they often focus on high-level abstractions \citep{langchain}, limited observability of intermediate decisions \citep{gpt4oplugin}, or multi-agent collaboration features  \citep{autogen}, with less emphasis on enhancing complex reasoning and \emph{quantitatively} benchmarking downstream task performance. In contrast, we systematically evaluate the entire agentic workflow of \model across diverse tasks, providing in-depth analyses of when and how tool-based reasoning succeeds or fails in complex reasoning scenarios.

We conduct large-scale experiments across 16 diverse reasoning benchmarks, spanning general vision, mathematical, scientific, medical, and agentic domains. As summarized in Figure~\ref{fig:main_scores_bar_chart}, \model substantially outperforms other baselines, achieving an average accuracy gain of 9.3\% over zero-shot prompting by \gpt and 7.7\% over chain-of-thought (CoT) prompting, as well as up to 10.6\% improvement compared to existing agentic frameworks when given the same tools \citep{autogen, gpt4oplugin, langchain}. Detailed analyses show that \model effectively combines multi-step planning and specialized tool usage, with each dimension providing distinct improvements. For tasks requiring intricate calculations or specialized knowledge, we found tool usage is particularly beneficial; for tasks requiring reasoning decomposition, we found multi-step planning offers significant gains.

Furthermore, our ablation studies offer insights into \model's performance under different conditions. Overall, the average accuracy tends to improve as the
maximum number of steps increases. Without any toolset optimization, simply enabling all tools in the toolset yields 57.4\% accuracy, which still surpasses the setup with only the base tool by 3.5\%, suggesting a degree of generalization as the toolset expands. Learning the optimal toolset for specific tasks raises the overall performance to 58.9\%, indicating the benefit of further optimization. Additionally, when using a weaker LLM (\gptmini) as the base engine, \model maintains a strong average gain of 7.1\% across 16 tasks.

\textbf{Our contributions} are as follows: (1) We propose \model, a training-free, extensible agentic framework that enables LLMs to call external tools in multiple steps, without the need for additional training or fine-tuning. (2) We introduce a comprehensive \textit{planner-executor }paradigm with standardized \textit{tool cards}, which can be easily customized or expanded for new domains. (3) We conduct large-scale experiments on 16 diverse benchmarks and show that \model improves performance by a sizable margin compared to baseline prompting and other agentic frameworks. (4) We provide in-depth analyses and ablations on how multi-step reasoning and tool usage contribute to performance, offering practical guidance for future agent development. 


\section{Related Work}
\label{sec:related}

\paragraph{Tool-Augmented LLMs.}
A promising direction for enhancing large language models (LLMs) involves offloading specialized subtasks to external tools such as search engines \citep{komeili-etal-2022-internet, thoppilan2022lamda, lazaridou2022internet, shuster2022blenderbot, yao2022react}, web browsers \citep{nakano2021webgpt}, calculators \citep{cobbe2021training, thoppilan2022lamda}, translation systems \citep{thoppilan2022lamda}, or Python interpreters \citep{gao2023pal}. Broadly, these methods either rely on large-scale fine-tuning or human supervision to teach LLMs how to invoke tools \citep{schick2023toolformer, komeili-etal-2022-internet, nakano2021webgpt, thoppilan2022lamda} or use few-shot prompts for single tools in narrowly defined tasks \citep{yao2022react, lazaridou2022internet, gao2023pal}. In contrast, \model is a \emph{training-free} framework that integrates diverse tools through standardized \emph{tool cards} and employs a planner-executor paradigm to manage multi-step reasoning. Because new tools can be introduced without re-training, \model offers a more \emph{extensible} and \emph{modular} approach to tool usage. 

\paragraph{LLM Agents.} A growing body of work leverages LLMs as autonomous agents that make decisions and invoke tools in multi-step workflows. Some agents use closed-source models with hand-engineered prompts \citep{chen2023llava, wang2024mobile}, while others fine-tune LLMs on curated data that learn when and how to call tools \cite{liu2023llava, tao2023webwise, zhang2024toolbehonest}. These frameworks often face limitations. For example, although specialized agent frameworks achieve strong performance in particular domains (e.g., chemistry \citep{bran2023chemcrow}, vision \citep{li2024mmedagent, hu2024visual}, materials science \citep{kang2024chatmof}, or medical imaging \citep{schmidgall2024agentclinic}), they typically lack generality across diverse tasks. Additionally, some systems are constrained by narrow capabilities with static planning ~\cite{lu2023chameleon} and multi-step reasoning \citep{hu2024visual}. Recently, general-purpose agent platforms such as \autogen \citep{autogen}, \gptplugin \citep{gpt4oplugin}, and \langchain \citep{langchain} have emerged, but they have seen less emphasis on complex reasoning and rigorous benchmarking across diverse downstream tasks. In contrast, \model combines the flexibility of such platforms with a dedicated planner and executor to handle multi-step decision-making. 


\paragraph{Complex Task Reasoning.}
When faced with multi-step problems, a common strategy is to break down a question into simpler sub-questions and solve them step by step. Early work approached decomposition with unsupervised or weakly supervised models \citep{perez2020unsupervised, khot2022decomposed}, and more recent research has explored prompting techniques for step-by-step reasoning, including Chain-of-Thought \citep{wei2022chain}, Least-to-Most \citep{zhou2022least}, ReAct \citep{yao2022react}, Pearl \citep{sun2023pearl}, Forest-of-Thought \citep{bi2024forest}, and rStar-Math\citep{guan2025rstar}. While these methods significantly improve LLMs’ single-model reasoning capabilities, they primarily rely on the latent capacity of an LLM without external validation or targeted tool usage. In contrast, \model systematically combines multi-step decomposition (via an iterative planner) with specialized tools (encapsulated by \emph{tool cards}) and an executor for reliable, context-aware function calling. This design makes it easy to incorporate domain-specific functionalities and check intermediate steps with external modules, thus improving both correctness and versatility in tackling challenging tasks.


\section{The \model Framework}
\label{sec:system}

We propose \model, an open-source, versatile, and user-friendly agent-toolbox framework for  complex reasoning tasks, as illustrated in Figure~\ref{fig:model_framework}. 
Given a user query $q \in \mathcal{Q}$ and a pretrained language model $\text{LLM}_\theta(\cdot)$, a naive approach would generate an output directly as $y \sim \text{LLM}_\theta(q)$, providing a single-step response. In contrast, our \model framework introduces a structured, multi-step process that leverages external tools to tackle queries effectively.

Specifically, \model contains a set of \textit{tools} $\mathcal{D} = \{d_i\}_{i=1}^n$ and associated metadata $\mathcal{M} = \{m_i\}_{i=1}^n$, where $n$ is the number of available tools. Given a query, a \textit{planner} (based on a language model) first generates a \textit{tentative plan} from a high-level perspective, indicating how these tools can be used to address the query, which forms the initial \textit{context} $s_0$. From this plan, the planner determines the initial \textit{action} $a_1$ for tool usage, specifying which tool $d_1$ to use, the relevant context, and a sub-goal. An \textit{executor} (also powered by a language model) then converts the planner’s text-based action $a_1$ into a machine-executable \textit{command} $o_t$, which is run to obtain intermediate results $r_1$. These results, along with the original action, update the context to $s_1 := (a_1, o_1, r_1)$. This process constitutes one step in our framework.

This process repeats, with the planner iteratively refining its actions based on the evolving context until it either finds a complete solution or inference limits (\textit{e.g.}, time or steps) are reached. After $T$ steps, the framework produces a full \textit{trajectory} $(s_0, s_1, \dots, s_T)$, which is stored in a structured manner in the context. The planner then uses this trajectory to generate the final solution to the original query.


\model provides a robust and effective framework for solving complex tasks through sub-goal decomposition and systematical tool usage. Standardized \textit{tool cards} encapsulate functionality (\S\ref{sec:toolcards}), the \emph{planner} orchestrates both high-level and low-level task planning (\S\ref{sec:agent_planning}), and the \emph{executor} instantiates tool calls for each sub-goal (\S\ref{sec:agent_execution}). The following sections detail the logic of each component, with implementation details provided in \S\ref{app:configuration}.

\subsection{Tool Cards} 
\label{sec:toolcards}

To enable seamless interaction among tools, the planner, and the executor, the toolbox serves as a fundamental building block of \model. In contrast to previous work~\cite{lu2023chameleon,hu2024visual} that rely on careful adaptions and tuning to support new tools, \model leverages \textit{tool cards}, which encapsulate  tools in a modular manner.

Each \textit{tool card} represents a tool $d$ along its essential metadata $m$ (as illustrated in Figure \ref{fig:model_example}). This metadata includes the tool’s name, input and output types, and command demonstrations. It may also contain additional usage constraints (\textit{e.g.}, user-defined ``\textit{limitations}'' and ``\textit{best practices}''), which provide developers’ insights to guide the planner and executor. For example, \texttt{Image\_Captioner\_Tool} includes ``\textit{it may make mistakes in complex scenes}'' and ``\textit{consider using other tools for verification}'' (\S\ref{app:image_captioner_tool}), while \texttt{Object\_Detector\_Tool} notes the limitation in detecting objects  (\S\ref{app:object_detector_tool}). See \S\ref{app:toolcards} for details on featured tools.


To ensure consistent interactions, every tool card implements two standard functions. The function \texttt{execute()} encapsulates the tool’s primary functionality, e.g., generating code snippet or performing object detection. Executed results are stored in a structured format to support different output types, e.g., generated code, detected objects, stored files (see \S\ref{app:toolcards}). The function \texttt{get\_metadata()} allows the planner and executor to dynamically evaluate the tool’s capabilities and understand usage constraints.

The design of each tool card is modular relative to the framework, enabling users to integrate diverse tools without modifying the underlying framework or agent logic. New tool cards can be added, replaced, or updated with low effort. Consequently, \model remains robust and extensible, even as tasks grow in complexity or scope.


\subsection{Planner Module}
\label{sec:agent_planning}

\paragraph{Planner initialization.}
The planner inspects the \emph{toolbox} and loads each \emph{tool card} as defined in \S\ref{sec:toolcards}. Formally, it constructs a set $\{d_i\}_{i=1}^n$ of $n$ available tools, each with metadata $m$ that describes its input-output schema and usage constraints. Rather than enabling the full toolset, a subset $\mathcal{D^\text{*}} = \{d_i\}_{i=1}^k$ may be selected based on expert insights or optimized using a small set of examples (See \S\ref{sec:toolset_optimization} and \S\ref{sec:ablation_toolset} for details and experimental study).

\paragraph{Query analysis and action prediction.}
Given a query $q$, the planner formulates a \emph{high-level}, tentative plan for tool usage based on its initialization. As shown in Figure \ref{fig:model_framework}, the high-level plan summarizes the query objective, analyzes the required skills, identifies relevant tools, and includes additional considerations to highlight the need for verification. The \emph{high-level} task plan provides a global perspective on the final objective, ensuring that each subsequent sub-goal remains aligned with the original query.

Subsequently, at step $t$, an \emph{action predictor} produces an action $a_t$ that combines the planner-suggested sub-goal (e.g., \emph{``detect baseballs in the image''}), the selected tool $d$ (e.g. \texttt{Object\_Detector}), and the relevant context. This \emph{low-level} plan refines and executes each sub-goal in real time, adapting to new information or feedback at each step.

\paragraph{Context verification and solution summarization.}
After each command is executed, a \emph{context verifier} checks whether the problem can be solved given the current context. It verifies completeness (e.g., whether all sub-goals are satisfied) and identifies any ambiguities. If the problem remains incomplete, the planner continues the next iteration of the cycle by predicting the next action $a_{t+1}$.

Once the verifier concludes that the query has been solved, a separate \emph{solution summarizer} compiles the final answer from the trajectory $(s_0,s_1,\dots, s_T)$. This stage integrates intermediate tool outputs, traces reasoning steps, and presents a concise, user-friendly summary as the final solution.

\subsection{Executor Module}
\label{sec:agent_execution}

\paragraph{Command prediction.} Prior work~\cite{lu2023chameleon, hu2024visual} often expects a single language model both for planning each step (i.e., which tool to use) and for generating the corresponding executable command. This dual responsibility can overload the model and lead to errors, especially when dealing with complex or environment-specific code \cite{bran2023chemcrow,li2024formal,ji2024testing}. To mitigate these issues, \model introduce a \emph{command generator} that interprets the planner’s text-based actions and produces executable code.

Given the action $a_t$ predicted by the planner, the \emph{command generator} (powered by a language model) creates a low-level command $o_t$ in the form of an executable Python script, which calls the tool $d_t$ with necessary inputs and performs any required data preparation. This step bridges the abstract action specified in $a_t$ and the concrete tool call. By separating decision-making from code generation, each component of the system can concentrate on its specialized role.

\paragraph{Command execution.}
Once an executable command is generated, it must be run in an environment that may involve dependencies, external libraries, or resource access (e.g., file systems). Directly coupling execution with planning poses security and maintainability challenges, especially if the planner is not capable of managing code execution details.

In \model, an \emph{command executor} runs the generated command $o_t$ in a Python environment, obtaining a result $r_t$. This may include tool outputs, logs, or error messages. The executor then adds the current context of this step $s_t:=(a_t, o_t, r_t)$ to the agent’s current trajectory $(s_0, s_1, \ldots, s_{t-1})$. The trajectory preserves a clear history of the actions taken, the code produced, and the results obtained. 


\subsection{Task-specific Toolset Optimization}
\label{sec:toolset_optimization}

The \model toolbox contains a diverse set of tools covering different modalities and skills. By leveraging structured tool cards and robust planning capabilities, \model demonstrates strong generality when all available tools are enabled across different tasks (see \S\ref{sec:toolset_optimization}). However, when a small set of validation examples are available for a task, configuring a \emph{task-specific} subset of tools can further enhance efficiency and effectiveness.

To this end, we propose an automated algorithm to optimize the toolset configuration for each task. Given $n$ available tools in the toolbox, the total number of possible subsets is $O(2^n)$, which is prohibitively large. To make this tractable, we employ a greedy search strategy that reduces the complexity to $O(n)$. Our approach proceeds in three stages. 

\input{algos/tool_selection}

\paragraph{Stage 1: Baseline setup.} 
We first establish a baseline performance by enabling the base toolset in the toolbox, denoted $\mathcal{D}_\text{base}$. This base set represents a minimal starting toolset, which can be pre-defined by the user. 

\paragraph{Stage 2: Individual tool evaluation.}
Next, we measure each candidate tool $d_i$ by enabling it alongside the base toolset. For each $d_i$, we form an augmented subset 
$\mathcal{D}_i = \mathcal{D}_\text{base} \cup \{d_i\}$ and compute the accuracy difference on the set of validation examples
\begin{equation}
\Delta_{d_i} \;=\; \text{Acc}\bigl(\mathcal{D}_i\bigr) \;-\; \text{Acc}\bigl(\mathcal{D}_\text{base}\bigr).
\end{equation}
If $\Delta_{d_i} > 0$, we consider the tool $d_i$ beneficial for the target task.

\paragraph{Stage 3: Optimized toolset selection.}
Finally, we aggregate all tools that yield positive improvements and combine them with the default set to form the optimized toolset:
\begin{equation}
\mathcal{D}^* = \mathcal{D}_\text{base}\;\cup\;\{\, d_i \mid \Delta_{d_i} > 0 \}.
\end{equation}
This set contains all tools that individually demonstrate a performance gain over the baseline, ensuring a customized yet efficient configuration for the downstream task. While this selection does not guarantee global optima, we observe overall improvements over simply using all tools (see \S\ref{sec:ablation_toolset}).


\begin{table*}[th!]
\centering
 \small
 \resizebox{1.0\linewidth}{!}{
\begin{tabular}{lcccccc|cccc|>{\columncolor{gray!20}}r>{\columncolor{gray!20}}r}
\toprule
\textbf{Datasets} & \textbf{Modality}  & \textbf{Domain} & \vision & \calculator & \knowledge & \steps & \textbf{0-shot} & \textbf{CoT} & \textbf{\modelbase} & \textbf{\model} & \textbf{$\Delta$ (0-shot)} & \textbf{$\Delta$ (CoT)} \\
\midrule
AlgoPuzzleVQA & Vision & General & \cmark & & & \cmark & 41.3\std{0.3} & 42.7\std{1.0} & 44.0\std{0.9} & \textbf{48.7}\std{0.3} & +7.4 & +6.0 \\
Hallusion-VD & Vision & General & \cmark & & &  & 52.0\std{1.0} & 53.3\std{2.1} & 59.0\std{0.0} & \textbf{63.3}\std{2.9} & +11.3 & +10.0 \\
PuzzleVQA & Vision & General & \cmark & & & \cmark & 52.2\std{1.0} & 54.0\std{1.3} & 59.3\std{0.8} & \textbf{61.0}\std{0.5} & +8.8 & +7.0 \\
VQA 2.0 & Vision & General & \cmark & & & \cmark & 50.3\std{1.0} & 48.7\std{0.3} & 47.2\std{0.8} & \textbf{54.5}\std{0.0} & +4.2 & +5.8 \\
\midrule
Game of 24 & Text & Mathematical & & \cmark & & \cmark & 22.2\std{2.5} & 33.3\std{1.5} & 37.8\std{3.3} & \textbf{44.7}\std{2.8} & +22.5 & +11.4 \\
Omni-MATH & Text & Mathematical & & \cmark & \cmark & \cmark & 27.0\std{0.0} & 29.3\std{1.3} & 30.2\std{0.6} & \textbf{32.2}\std{0.8} & +5.2 & +2.9 \\
CLEVR-Math & Vision & Mathematical & \cmark & \cmark & &  & 64.5\std{3.0} & 75.2\std{1.5} & 68.8\std{0.8} & \textbf{79.0}\std{0.9} & +14.5 & +3.8 \\
MathVista & Vision & Mathematical & \cmark & \cmark & \cmark & \cmark & 59.3\std{0.8} & 59.5\std{1.5} & 63.0\std{1.3} & \textbf{64.3}\std{1.0} & +5.0 & +4.8 \\
\midrule
GPQA & Text & Scientific & & \cmark & \cmark & \cmark & 53.7\std{1.9} & 52.3\std{2.0} & 53.7\std{2.5} & \textbf{54.7}\std{1.3} & +1.0 & +2.4 \\
MMLU-Pro & Text & Scientific &  & & \cmark & \cmark & 71.7\std{0.3} & 70.3\std{0.6} & 71.5\std{1.3} & \textbf{73.7}\std{1.3} & +2.0 & +3.4 \\
SciFIBench & Vision & Scientific & \cmark &  & \cmark & & 72.5\std{0.0} & 75.0\std{0.9} & 77.3\std{0.8} & \textbf{78.3}\std{0.6} & +5.8 & +3.3 \\
\midrule
MedQA & Text & Medical & & & \cmark &  & 84.5\std{1.0} & 84.8\std{0.6} & \textbf{92.8}\std{0.6} & 91.5\std{1.8} & +7.0 & +6.7 \\
PathCLS & Vision & Medical & \cmark & & \cmark & & 36.0\std{0.9} & 37.5\std{1.8} & 37.0\std{1.8} & \textbf{58.2}\std{1.3} & +22.2 & +20.7 \\
PathVQA & Vision & Medical & \cmark & & \cmark & \cmark & 32.0\std{1.8} & 27.8\std{1.8} & 43.5\std{2.6} & \textbf{49.2}\std{1.2} & +17.2 & +21.4 \\
SLAKE & Vision & Medical & \cmark & & \cmark & \cmark & 59.3\std{1.0} & 60.3\std{0.6} & 59.2\std{1.8} & \textbf{63.8}\std{1.4} & +4.5 & +3.5 \\
\midrule
GAIA-Text & Text & Agentic & & \cmark & \cmark & \cmark & ~~8.7\std{0.8} & ~~8.4\std{0.5} & ~~9.7\std{0.9} & \textbf{18.4}\std{1.2} & +9.7 & +10.0 \\
\midrule
\rowcolor{red!30} \textbf{Average (\%)} & - & - & - & - & - & - & \multicolumn{1}{l}{49.2} & 50.8~~~~~~~ & 53.4~~~~~~~ & \textbf{58.5}~~~~~~~ & \textbf{+9.3} & \textbf{+7.7} \\
\bottomrule
\end{tabular}
}
\vspace{-1mm}
\caption{Main results across 16 benchmarks spanning different modalities, domains, and required reasoning skills: visual understanding (\vision), numerical calculation (\calculator), knowledge retrieval (\knowledge), and multi-step reasoning (\steps). \modelbase uses only the base tool (\texttt{Generalist\_Solution\_Generator}), while \model uses the optimal toolset. Performance gains ($\Delta$) are computed for OctoTools relative to both zero-shot (0-shot) and chain-of-thought (CoT) baselines, with \model achieving 58.5\% average accuracy and improvements of 9.3\% and 7.7\% respectively. All results show average accuracy and standard deviation (gray) over three trials.}
% \vspace{-3mm}
\label{tab:main_results_across_benchmarks}
\end{table*}


\section{Experiments}
\label{sec:exp}

\subsection{Experimental Setups}

To demonstrate the generality of our \model framework, we conduct comprehensive evaluations on 16 diverse benchmarks spanning two modalities, five domains, and four reasoning types, as shown in Table~\ref{tab:main_results_across_benchmarks}. These benchmarks encompass a wide range of complex reasoning tasks, including visual understanding, numerical calculation, knowledge retrieval, and multi-step reasoning.

For general visual reasoning, we include AlgoPuzzleVQA~\cite{ghosal2024language}, Hallusion-VD~\cite{guan2024hallusionbench}, PuzzleVQA~\cite{chia2024puzzlevqa}, and  VQA 2.0~\cite{goyal2017making}. For mathematical reasoning, we use Game of 24~\cite{24game}, Omni-MATH~\cite{gao2024omni}, CLEVR-Math~\cite{lindstrom2022clevr}, and MathVista~\cite{lu2024mathvista}. For scientific reasoning, we adopt GPQA~\cite{rein2023gpqa}, MMLU-Pro~\cite{wang2024mmlu}, and SciFIBench~\cite{roberts2024scifibench}. To evaluate models in specialized medical domains, we test on MedQA~\cite{jin2021disease}, PathCLS~\cite{sun2025pathmmu}, PathVQA~\cite{he2020pathvqa}, and SLAKE~\cite{liu2021slake} Additionally, we incorporate GAIA-Text, a textual subset of the GAIA~\cite{mialon2023gaia} benchmark designed to evaluate agentic frameworks with tool-calling capabilities. See \S\ref{app:benchmark_datasets} for additional details of these benchmarks.

% \james{Explain these benchmarks more since it may not be familiar}. 

For each benchmark, we sampled 100 examples to construct a validation set for toolset optimization (\S\ref{sec:toolset_optimization}) and ablation studies (\S\ref{sec:ablation_study}), and set aside a held-out test set of 200 examples for final evaluation. For benchmarks with fewer than 300 total samples, the test set consisted of all remaining samples not used for validation. To mitigate randomness, we report the average accuracy with standard deviation across three trials for all experiments, including ablation studies. Details on experimental setups are provided in \S\ref{app:exp_details}. 

We created a diverse array of tools in the toolbox for our experiments. The base toolset consists of \texttt{Generalist\_Solution\_Generator}, a tool built on top of the base LLM that takes as input a specialized prompt generated by the executor, such as ``\textit{Describe the image in detail}'' or ``\textit{Generate stepwise solutions for the math problem}''. This tool allows for general step-by-step reasoning without requiring external tools but lacks domain specificity. The toolbox also includes image perception tools such as \texttt{Image\_Captioner}, web search APIs like \texttt{Google\_Search}, a code generator \texttt{Python\_Code\_Generator}, and specialized tools like \texttt{Path\_Generalist\_Classifier} for pathology image classification. Among them, \texttt{Relevant\_Patch\_Zoomer} is a unique tool that takes a textual query and returns zoomed-in quarter patches, which provide localized details for fine-grained visual reasoning scenarios. See \S\ref{app:tool_descriptions} and \S\ref{app:toolcards} for more details.

\subsection{Main Results}

We compare the performance of our framework after toolset optimization (denoted as \model) against three baselines: (1) zero-shot, where the base LLM directly answers queries without additional prompting, (2) chain-of-thought (CoT), where the base LLM is prompted to ``Think step by step'' to generate step-by-step reasoning, and (3) \modelbase, which uses only the base tool without external integrations. Unless otherwise specified, all results presented below use \gptengine as the base model.

Table~\ref{tab:main_results_across_benchmarks} and Figure~\ref{fig:main_scores_bar_chart} summarize performance across all 16 benchmarks. \model achieves consistent gains, outperforming zero-shot and CoT baselines by 9.3\% and 7.7\% on average, respectively. \modelbase also demonstrates improvements over zero-shot (4.2\%) and CoT (2.6\%), indicating that our framework's step-by-step reasoning contributes significantly to performance, independent of external tool integration. Figure~\ref{fig:benchmark_gains_radar_barplot} visualizes gains over zero-shot. Detailed analyses of these gains are presented in \S \ref{sec:analysis_of_gains}. 

\begin{table}[t!]
\centering
\small
\renewcommand\tabcolsep{3.0pt}
\resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Datasets} & \textbf{AutoGen} & \textbf{\gptplugin} & \textbf{LangChain} & \textbf{\model} \\
    \midrule
    AlgoPuzzleVQA & 44.0\std{1.0} & 44.5\std{0.5} & 42.7\std{2.8} & \textbf{48.7}\std{0.3} \\
    Hallusion-VD & 52.7\std{4.7} & 57.0\std{1.7} & 53.7\std{3.1} & \textbf{63.3}\std{2.9} \\
    Puzzle VQA & 40.0\std{2.3} & 52.5\std{2.8} & 53.5\std{7.8} & \textbf{61.0}\std{0.5} \\
    VQA 2.0 & 46.0\std{1.0} & 45.5\std{0.9} & 54.0\std{1.0} & \textbf{54.5}\std{0.0} \\
    \midrule
    Game of 24 & 24.2\std{2.4} & 34.5\std{2.3} & 18.3\std{4.1} & \textbf{44.7}\std{2.8} \\
    Omni-MATH & 28.5\std{1.3} & 22.8\std{1.8} & 29.7\std{0.6} & \textbf{32.2}\std{0.8} \\
    CLEVR-Math & 69.5\std{3.9} & 71.2\std{1.0} & 69.2\std{4.6} & \textbf{79.0}\std{0.9} \\
    MathVista & 24.7\std{2.5} & 54.5\std{2.0} & 55.7\std{0.3} & \textbf{64.3}\std{1.0} \\
    \midrule
    GPQA & 48.7\std{2.9} & 45.8\std{2.6} & 52.2\std{1.2} & \textbf{54.7}\std{1.3} \\
    MMLU-Pro & 65.0\std{2.5} & 65.8\std{2.4} & 70.3\std{1.2} & \textbf{73.7}\std{1.3} \\
    SciFIBench & 70.0\std{2.2} & 68.8\std{3.2} & 77.0\std{0.5} & \textbf{78.3}\std{0.6} \\
    \midrule
    MedQA & 83.7\std{2.8} & 84.8\std{0.3} & 73.7\std{0.6} & \textbf{91.5}\std{1.8} \\
    PathCLS & 58.0\std{1.3} & 58.2\std{0.6} & 56.3\std{1.3} & \textbf{58.2}\std{1.3} \\
    PathVQA & 42.7\std{0.8} & 42.8\std{2.3} & 45.7\std{4.4} & \textbf{49.2}\std{1.2} \\
    SLAKE & 62.2\std{1.8} & 59.7\std{1.9} & 59.3\std{0.8} & \textbf{63.8}\std{1.4} \\
    \midrule
    GAIA-Text & ~~6.3\std{0.8} & ~~7.9\std{0.8} & ~~7.6\std{1.2} & \textbf{18.4}\std{1.2} \\
    \midrule
    \rowcolor{red!30} \textbf{Average (\%)} & 47.9~~~~~~~ & 51.0~~~~~~~ & 51.2~~~~~~~ & \textbf{58.5}~~~~~~~ \\
    \bottomrule
    \end{tabular}
}
% \vspace{-4mm}
\caption{Comparison with other agent frameworks using the same underlying toolbox. \model achieves superior performance with an average accuracy of 58.5\%, outperforming the next best baseline by 7.3\%. Results are averaged over three trials.}
% \vspace{-5mm}
\label{tab:main_results_across_baselines}
\end{table}


\subsection{Comparisons with Other Agent Frameworks}

In addition, we compare three commonly used general AI agent frameworks: \gptplugin~\cite{gpt4oplugin}, \langchain~\cite{langchain}, and \autogen~\cite{autogen}. \gptplugin enables \gpt to call user-specified tools via function calling. \langchain is a framework providing multi-agent collaboration, long-term memory, and tool usage. \autogen is a recent agentic framework that creates multiple autonomous agents with tool usage.

For a standardized comparison of each system’s ability to plan and use tools over multiple steps, we configure all agent frameworks, including \model, to use the same underlying model (\gpt) and hyperparameters. They share the same toolset (with the same implementations of tools), a maximum reasoning budget of 10 steps, and a time budget of 300 seconds. See \S \ref{app:exp_details} for further details.

Table~\ref{tab:main_results_across_baselines} and Figure~\ref{fig:agent_baselines_bar_plot} show the performance of \model compared to other agent frameworks across all 16 benchmarks. Overall, \model outperforms other agent frameworks, achieving an average accuracy gain of 10.6\% over \autogen, 7.5\% over \gptplugin, and 7.3\% over \langchain. See \S \ref{sec:analysis_of_gains} for detailed analysis.

\subsection{Analysis of Performance Gains}
\label{sec:analysis_of_gains}
Our agent framework includes three aspects for improvement in complex reasoning: \textit{task planning}, \textit{external tool calling}, and \textit{multi-step problem solving}. We include an analysis to disentangle added benefits of each of these aspects. 

\begin{figure*}[th!]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/tool_usage_ours_baselines.pdf}
    \vspace{-4mm}
    \caption{\textbf{a.} Tool usage distribution in our \model framework and agent baselines by averaging results from 16 tasks. \textbf{b.} Tool usage distribution across 16 tasks in \model. \model takes advantage of different external tools to address task-specific challenges.}
    % \vspace{-4mm}
    \label{fig:tool_usage_ours_baselines}
\end{figure*}

\paragraph{Tool usage distribution.} Figure~\ref{fig:tool_usage_ours_baselines} (a) shows the tool usage distribution for \model and the agent baselines, averaged over 16 tasks. \model takes advantages of \textit{task planning} by calling both a base tool to decompose the query into subtasks (32.2\%) and external specialized tools (67.8\%) for reasoning skills such as fine-grained image understanding, domain knowledge access, precise calculation, and other domain-specific tasks. In contrast, the other agent baselines exhibit limitations in task planning for external tool usage, with external tool usage rates of 10.6\% for \autogen, 23.3\% for \gptplugin, and 10.7\% for \langchain.


\begin{figure}[th!]
    \centering
    \includegraphics[width=0.45\textwidth]{figs/benchmark_distribution_x_steps_y_tools.pdf}
    % \vspace{-3mm}
    \caption{Benchmark distribution across average number of steps and fraction of external tool usage (tools that exclude the base tool \texttt{Generalist\_Solution\_Generator}) in \model.}
    % \vspace{-5mm}
    \label{fig:benchmark_distribution_x_steps_y_tools}
\end{figure}


Figure~\ref{fig:tool_usage_ours_baselines}~(b) illustrates the tool usage distribution of \model across the 16 tasks. \model adapts to each task by selecting the most suitable tools. For instance, \model employs five different tools to address the diverse challenges in GAIA-Text. In MathVista, a benchmark featuring multiple diagram-based and mathematical reasoning problems, it uses \texttt{Relevant\_Patch\_Zoomer} for local vision perception, \texttt{Google\_Search} for web queries, and \texttt{Python\_Code\_Generator} for precise calculations.


\paragraph{External tool usage \textit{v.s.} multiple steps.} 
Figure~\ref{fig:benchmark_distribution_x_steps_y_tools} shows how tasks are distributed based on the average number of steps and the fraction of external tool usage in \model. Tasks with a high average number of steps indicates that \textit{multi-step problem solving} is valuable, while a high proportion of external tool usage highlight the benefits of \textit{external tool calling}. Notably, two tasks score highly on both dimensions: Game of 24, which frequently calls \texttt{Python\_Code\_Generator} to explore and verify arithmetic expressions that evaluate to 24, and GAIA-Text, which requires multiple tools to address a variety of underlying skills.

\paragraph{Gains from decompositions \textit{v.s.} tool usage.}
To assess the performance gains from \textit{tool calling}, we compute for each benchmark the difference between \model, \textit{i.e.}, with external tools, and \modelbase, \textit{i.e.}, without external tools: $\Delta_{\text{tools}} = \text{Acc}_{\text{\model}} - \text{Acc}_{\text{\modelbase}}$. Similarly, to assess the performance gains from \textit{decomposing} the problem into \textit{multiple steps}, we compute the difference between \modelbase and the 0-shot baseline: $\Delta_{\text{decom}} = \text{Acc}_{\text{\modelbase}} - \text{Acc}_{\text{0-shot}}$.

For each benchmark, the performance gains from these two dimensions are visualized in Figure~\ref{fig:scatterplot_decomposition_vs_tools}. We observe that the tasks can be broadly categorized into three groups: 
(1) Tasks far above the $y=x$ diagonal benefit more from step decomposition, and a specialized tool is either not needed or not available.
(2) Tasks far below the diagonal benefit more from one or more specialized tools that are well-suited for the task, while multi-step reasoning is less critical.
(3) Tasks along the diagonal benefit from both aspects.
This analysis highlights that AI agent frameworks can improve performance in different ways depending on the specific skills demanded by each task, and that \model is a versatile framework for achieving such improvements across a diverse set of tasks.

\begin{figure}[th!]
    \centering
    \includegraphics[width=0.48\textwidth]{figs_app/scatterplot_decomposition_vs_tools.pdf}
    \caption{Benchmark distribution across two dimensions. Tasks that show high improvement from task decomposition likely require multi-step reasoning, while tasks that show high improvement from specialized tools likely require specialized skills.}
    \label{fig:scatterplot_decomposition_vs_tools}
\end{figure}


\section{Ablation Study}
\label{sec:ablation_study}

This section explores several factors that affect \model's performance, using a validation set of 100 samples.


\subsection{Number of Maximum Allowed Steps}

We explore how the behavior and performance of \model change under different maximum step limits. We report the average over three trials, with results summarized in Figure~\ref{fig:average_accuracy_by_max_steps}.
Overall, performance tends to improve as the maximum number of steps increases, highlighting the benefit of longer chains of multi-step reasoning. Detailed results and analysis for individual tasks are provided in \S\ref{app:exp_results}. Additionally, we found that averaged across 16 tasks, running \model with \gpt typically takes less than \$5 for 100 queries with 10 maximum allowed steps.

\begin{figure}[th!]
    \centering
    % \vspace{-3mm}
    \includegraphics[width=0.45\textwidth]{figs/average_accuracy_by_max_steps_error_range.pdf}
    \vspace{-2mm}
    \caption{Average accuracy across 16 benchmarks with respect to maximum allowed reasoning steps in \model.}
    % \vspace{-5mm}
\label{fig:average_accuracy_by_max_steps}
\end{figure}


\subsection{Toolset Optimization}
\label{sec:ablation_toolset}

To investigate the benefits of our toolset optimization algorithm (presented in \S\ref{sec:toolset_optimization} and Algorithm~\ref{alg:tool_selection_optimization}), we evaluate \model under three toolset strategies: (1) \modelbase, i.e., the setup with only the base \texttt{Generalist\_Solution\_Generator} tool, (2) \model with the \textit{full} toolset, where all possible tools are enabled, and (3) \model with the \textit{optimized} toolset. Figure~\ref{fig:ours_tool_selection_category_scores_bar_chart} presents results across all 16 tasks and different reasoning categories.

On average, the optimized toolset achieves 58.9\% accuracy, outperforming the \modelbase (53.9\%) by 5.0\%. Meanwhile, simply enabling all tools yields 57.4\% accuracy, which still outperforms \modelbase by 3.5\%. Although toolset optimization provides the higher overall performance, our framework is sufficiently versatile that enabling all tools also offers a substantial improvement, helping ensure generalization as we scale up the toolset.

While enabling all available tools might appear advantageous, prior work \cite{lumer2024toolshed,fore2024geckopt,paramanayakam2024less} has shown that it can sometimes degrade performance. Our findings suggest that external feedback (e.g., performance on a validation set or expert insights) helps identify when a certain tool improves performance. Additionally, selectively enabling tools avoids unnecessary overhead and improves efficiency. A promising next step is to explore a query-based approach that dynamically refines the toolset for each query.

\begin{figure}[th!]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/ours_tool_selection_category_scores_bar_chart.pdf}
    \vspace{-5mm}
    \caption{Performance under three toolset strategies in \model across all 16 tasks and various categories (the number in parentheses indicates the number of tasks in each category).}
\label{fig:ours_tool_selection_category_scores_bar_chart}
\end{figure}

\subsection{Using a Weaker LLM}

Additionally, we evaluate our framework using a weaker LLM (\gptmini) as the base engine. Similar to the analysis with \gpt, we compare \model against the zero-shot and CoT baselines on the validation sets of 16 benchmarks. As shown in Figure~\ref{fig:gpt4omini_ours_category_scores_bar_chart}, \model generally outperforms the baselines across all 16 tasks and various categories. Task-specific analyses are provided in \S\ref{app:exp_results}.

\begin{figure}[th!]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/gpt4omini_ours_category_scores_bar_chart.pdf}
    \vspace{-3mm}
    \caption{Performance of \model on 16 tasks and various categories using a weaker LLM, \gptmini, as the base engine. \modelbase is the configuration in which only the base \texttt{Generalist\_Solution\_Generator} tool is enabled. The number in parentheses indicates $\#$ tasks in each category.}
\label{fig:gpt4omini_ours_category_scores_bar_chart}
\end{figure}

\section{Conclusion}
\label{sec:conclusion}
In this paper, we introduced \model, a training-free, extensible agentic framework for complex reasoning. \model employs standardized \emph{tool cards} to facilitate seamless integration of diverse tools and a dedicated planner-executor workflow that separates high-level planning over multiple steps from low-level planning and command generation within each step. Through extensive experiments on 16 diverse benchmarks, \model consistently outperforms baselines, achieving average accuracy gains of up to 9.3\% over \gpt and up to 10.6\% over strong agentic frameworks. Our in-depth analysis shows that \model' improvements stem from dynamic task planning, effective tool usage, and multi-step problem decomposition.

Ablation studies highlight the benefits of allowing more step, refining the toolset, and demonstrate the robustness when deployed with a weaker LLM. By streamlining the integration of new or specialized modules through tool cards, \model readily adapts to a broad range of tasks. We believe our findings open new ecosystems for building next-generation AI agents that are more transparent, modular, and effective at solving real-world problems. Future work includes test-time inference at the query level, extending multi-agent collaboration, and exploring specialized domains.

% \clearpage
\section*{Acknowledgments}
\label{sec:ack}
This work is partially supported by the Hoffman-Yee Research Grants program at Stanford HAI and Chan Zuckerberg Initiative. We thank members of the Zou group for helpful discussions and insightful feedback.