
\clearpage

\section{Experimental Details}
\label{app:exp_details}

\subsection{Benchmark Datasets}
\label{app:benchmark_datasets}
Here, we report further details of each of the 16 benchmark we used in this study. Unless specified otherwise, a validation set of 100 examples and a test set of 200 examples were sampled from each dataset. No additional preprocessing was performed for open-ended questions. For multiple choice questions, choices were enumerated using capital letters and each concatenated to the question following a new line character. Any subsetting of the original datasets are also described below.

\subsubsection{General Domain Benchmarks}

\textbf{AlgoPuzzleVQA}~\cite{ghosal2024language} is a set of geometric puzzles that require both visual understanding, language understanding, and complex algorithmic reasoning that are difficult for base VLMs. 

\textbf{Hallusion-VD} is a subset of HallusionBench~\cite{guan2024hallusionbench}, a benchmark to test visual understanding through optical and geometric illusions. For our experiments we use the Visual Dependent subset of HallusionBench, consisting of questions where the visual context is required to give a definitive answer.

\textbf{PuzzleVQA}~\cite{chia2024puzzlevqa} is a dataset of puzzle instances based on abstract, geometric patterns that test understanding and reasoning based on colors, numbers, sizes, and shapes.

\textbf{VQA 2.0}~\cite{goyal2017making} consists of open-ended questions about images that require fine-grained visual understanding.

\subsubsection{Mathematical Benchmarks}

\textbf{Game of 24}~\cite{24game} is based on the classic arithmetic game of 24 (also known as 24, the 24 numbers game, \textit{etc.}). The puzzle involves using four numbers and basic arithmetic operations (addition, subtraction, multiplication, division) to construct an expression that evaluates to 24. For example, given the numbers 4, 9, 10, and 13, one valid solution is ``(10 - 4) Ã— (13 - 9) = 24''. Solving the puzzle requires numerical calculation skills as well as multiple attempts to verify proposed solutions. Each puzzle is presented as open-ended questions.

\textbf{Omni-MATH}~\cite{gao2024omni} is a text-only mathematical benchmark consisting of open-ended competition-level problems at the Olympiad level, requiring advanced mathematical knowledge and reasoning.

\textbf{CLEVR-Math}~\cite{lindstrom2022clevr} consists of multimodal math word problems involving addition and subtraction. Each problem contains a textual description and an image illustrating the scenario. A combination of language, visual, and mathematical reasoning is required to solve these word problems.

\textbf{MathVista}~\cite{lu2024mathvista} is benchmark designed to combine challenges from diverse mathematical and visual tasks. The queries are a mix of multiple choice and open-ended questions and require numerical computation, fine-grained visual understanding, and multi-step reasoning.

\subsubsection{Scientific Benchmarks}

\textbf{GPQA} or Graduate Level Google-Proof Q\&A Benchmark~\cite{rein2023gpqa} is a set of challenging text-only multiple choice questions written by domain experts in biology, physics, and chemistry designed to be ``extremely difficult". 

\textbf{MMLU-Pro}~\cite{wang2024mmlu} is a text-only benchmark consisting of challenging, reasoning-focused multiple choice questions that require general scientific knowledge and complex reasoning.

\textbf{SciFIBench}~\cite{roberts2024scifibench} is a benchmark of multiple choice questions for scientific figure interpretation. Queries involve understanding and extracting information from scientific figures. 

\subsubsection{Medical Benchmarks}

\textbf{MedQA}~\cite{jin2021disease} consists of text-only multiple choice questions curated from professional medical board exams. Questions cover general medical and clinical knowledge and reasoning.

\textbf{PathCLS} is a subset of PathMMU ~\cite{sun2025pathmmu} that consists of multiple choice questions based on hematoxylin and eosin (H\&E)-stained histopathology microscopy images reformulated from well-known pathology classification datasets. These questions generally involve disease diagnosis based on histopathology images. 

\textbf{PathVQA}~\cite{he2020pathvqa} is a visual question-answering dataset curated from pathology-related image-caption pairs sourced from textbooks, spanning multiple tissue types and stains. All questions were treated as open-ended in our evaluation. 

\textbf{SLAKE} or Semantically-Labeled Knowledge-Enhanced Dataset~\cite{liu2021slake} is a radiology visual question-answering dataset. The associated images span X-ray images, computed tomography (CT) scans, and magnetic resonance imaging (MRI). All questions were treated as open-ended in our evaluation. Though the authors of the dataset released object detection labels and segmentation masks for each image, these were excluded in our evaluation for the purpose of increasing difficulty.

\subsubsection{Agentic Benchmark}

\textbf{GAIA-Text}~\cite{mialon2023gaia} is a benchmark specifically designed to evaluate general AI assistants and agents, requiring abilities such as multi-step reasoning, web browsing, and generally tool-use proficiency. The questions are designed to be difficult for base LLMs. We use the text-only subset of this dataset in our experiments.

\subsection{Tools Used in Our Experiments}
\label{app:tool_descriptions}

We implemented 11 tools in the toolbox for our experiments. Here, we provide detailed descriptions of each tool. See \S\ref{app:toolcards} for the complete tool cards of each tool and usage examples. 

\textbf{Arxiv Paper Searcher} (\S\ref{app:arxiv_paper_searcher_tool}) searches arXiv\footnote{\url{https://arxiv.org/}}, an open-access pre-print repository, for abstracts and links that match a given query.

\textbf{Generalist Solution Generator} (\S\ref{app:generalist_solution_generator_tool}) is an instance of the \model base LLM and acts as the default reasoning engine if the agent decides not to use an external tool.

\textbf{Google Search} (\S\ref{app:google_search_tool}) uses the Google custom search API\footnote{\url{https://developers.google.com/custom-search/v1/introduction}} to search the web and return links and a summary of each result. 

\textbf{Image Captioner} (\S\ref{app:image_captioner_tool}) is an instance of the base LLM prompted for generating text descriptions of input images.

\textbf{Path Generalist Classifier} (\S\ref{app:path_generalist_classifier_tool}) is a tool for performing general classification of H\&E-stained pathology microscopy images. The tool relies on CONCH~\cite{lu2024avisionlanguage}, a pretrained vision-language foundation model for pathology, for performing zero-shot classification of pathology image patches.

\textbf{Pubmed Search} (\S\ref{app:pubmed_search_tool}) retrieves relevant article abstracts from PubMed based on a text query. The retrieval is performed using the PubMed and NCBI APIs\footnote{\url{https://www.ncbi.nlm.nih.gov/books/NBK25501/}}. 

\textbf{Python Code Generator} (\S\ref{app:python_code_generator_tool}) generates and executes Python code given a query and returns the execution result. The code generation is performed by an instance of the base LLM prompted for Python code generation. 

\textbf{Relevant Patch Zoomer} (\S\ref{app:relevant_patch_zoomer_tool}) is an instance of the base LLM that, given a query, decides which regions of the image to zoom into (among the four quadrants and the center patch) and saves the zoomed patches.

\textbf{Text Detector} (\S\ref{app:text_detector_tool}) detects multilingual text within an image by calling the EasyOCR tool for text detection\footnote{\url{https://github.com/JaidedAI/EasyOCR}}.

\textbf{URL Text Extractor} (\S\ref{app:url_text_extractor_tool}) visits web pages given the URL and returns the text content of the page.

\textbf{Wikipedia Knowledge Searcher} (\S\ref{app:wikipedia_knowledge_searcher_tool}) searches Wikipedia using the MediaWiki API\footnote{\url{https://www.mediawiki.org/wiki/API}} and returns articles matching a given query.

\subsection{Additional Tools for Exploration Study}

We also provide several additional tools for exploration, as follows:

\textbf{Object Detector} (\S\ref{app:object_detector_tool}) performs object detection on an image given a list of object labels to detect, using the Grounding DINO model \cite{caron2021emerging}. Due to the standardized design of tool cards, this tool can be upgraded to the \textbf{Advanced Object Detector} (\S\ref{app:advanced_object_detector_tool}), which uses DINO-X \cite{ren2024dinoxunifiedvisionmodel}, a more recent version powered by API calls.\footnote{\url{https://github.com/IDEA-Research/DINO-X-API}}

\textbf{Nature News Fetcher} (\S\ref{app:nature_news_fetcher_tool}) retrieves the latest news articles from the science journal \textit{Nature}.\footnote{\url{https://www.nature.com/latest-news}} An example in \S\ref{app:example_exploration} demonstrates how this tool can be used to obtain the latest research trends for a given topic.


% \clearpage
\section{Experimental Results}
\label{app:exp_results}

\paragraph{Optimized tool sets.}  Table \ref{tab:optimized_toolsets} shows the optimized tool sets across 16 tasks from Algorithm \ref{alg:tool_selection_optimization}. The toolset optimization method in \model is able to find diverse optimal tool sets for different tasks. In general, the \texttt{Image\_Captioner} and \texttt{Relevant\_Patch\_Zoomer} tools are very commonly used in vision benchmarks, with the former being used in all vision benchmarks and the latter being used in 6 out of the 10. The \texttt{Python\_Code\_Generator} is represented in 3 out of the 4 mathematical domain benchmarks. The \texttt{Wikipedia\_Knowledge\_Searcher} is represented in all the scientific domain benchmarks. We also see that highly domain-specific tools are represented in their corresponding use cases, such as \texttt{Pubmed\_Search} in a general medical benchmark \cite{jin2021disease} and \texttt{Path\_Generalist\_Classifier} in a pathology classification benchmark \cite{sun2025pathmmu}. 

\begin{table}[th!]
    \centering
    \small
    \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c}
        \toprule
        \textbf{Benchmarks} & \textbf{Modality} & \textbf{Domain} & 
        \rotatebox{90}{\base~\texttt{Generalist\_Solution\_Generator}} &
        \rotatebox{90}{\image~\texttt{Image\_Captioner}} & 
        \rotatebox{90}{\image~\texttt{Relevant\_Patch\_Zoomer}} & 
        \rotatebox{90}{\image~\texttt{Text\_Detector}} & 
        \rotatebox{90}{\www~\texttt{Wikipedia\_Knowledge\_Searcher}} & 
        \rotatebox{90}{\www~\texttt{Google\_Search}} & 
        \rotatebox{90}{\www~\texttt{URL\_Text\_Txtractor}} & 
        \rotatebox{90}{\www~\texttt{ArXiv\_Paper\_Searcher}} & 
        \rotatebox{90}{\code~\texttt{Python\_Code\_Generator}} & 
        \rotatebox{90}{\med~\texttt{Path\_Generalist\_Classifier}} & 
        \rotatebox{90}{\med~\texttt{Pubmed\_Search}} \\
        \midrule
        AlgoPuzzleVQA & Vision & General & \cmark & \cmark & & \cmark & & & & & & & \\
        Hallusion-VD & Vision & General & \cmark & \cmark & & & & & & & & & \\
        PuzzleVQA & Vision & General & \cmark & \cmark & & & & & & & & & \\
        VQA 2.0 & Vision & General & \cmark & \cmark & \cmark & & & & & & & & \\
        \midrule    
        Game of 24 & Text & Mathematical & \cmark & & & & & & & & \cmark & & \\
        Omni-MATH & Text & Mathematical & \cmark & & & & & & & & \cmark & & \\
        CLEVR-Math & Vision & Mathematical & \cmark & \cmark & \cmark & & & & & & & & \\
        MathVista & Vision & Mathematical & \cmark & \cmark & \cmark & & & \cmark & & & \cmark & & \\
        \midrule
        GPQA & Text & Scientific & \cmark & & & & \cmark & & & & & & \\
        MMLU-Pro & Text & Scientific & \cmark & & & & \cmark & & & & & & \\
        SciFIBench & Vision & Scientific & \cmark & \cmark & & \cmark & \cmark & & & \cmark & & & \\
        \midrule
        MedQA & Text & Medical & \cmark & & & & & & & & & & \cmark \\
        PathCLS & Vision & Medical & \cmark & \cmark & \cmark & & & & & & & \cmark & \\
        PathVQA & Vision & Medical & \cmark & \cmark & \cmark & & & & & & & & \\
        SLAKE & Vision & Medical & \cmark & \cmark & \cmark & & & & & & & & \\
        \midrule
        GAIA-Text & Text & Agentic & \cmark & & & & \cmark & \cmark & \cmark & & \cmark & & \\
        \bottomrule
    \end{tabular}
    \caption{Optimized tool sets for each benchmark following our Algorithm\ref{alg:tool_selection_optimization}. A \cmark indicates that the tool is used for that benchmark. }
    \vspace{-2mm}
    \label{tab:optimized_toolsets}
\end{table}


\paragraph{Steps taken for different tasks.}
Figure~\ref{fig:distribution_of_number_of_steps} shows the distribution of the number of steps taken. \model is able to adapt to each task by applying different sets of tools and constructing chains of reasoning as needed.

\begin{figure*}[th!]
    \centering
    \includegraphics[width=0.95\textwidth]{figs_app/agent_baselines_bar_plot.pdf}
    \caption{\textbf{Performance ours vs. other agents}. Our framework consistently outperforms agent baselines across all benchmarks. Bar values represent accuracy and error bars represent standard deviation.}
\label{fig:agent_baselines_bar_plot}
\end{figure*}

\begin{figure*}[th!]
    \centering
    \includegraphics[width=0.95\textwidth]{figs_app/all_tools_ablation.pdf}
    \vspace{-2mm}
    \caption{\textbf{Performance with vs. without tool selection}. While toolset optimization increases performance over using the full toolset in most tasks, even without it, our framework achieves similar performance by naively enabling all possible tools. Bar values represent accuracy and error bars represent standard deviation.}
\label{fig:all_tools_ablation_bar_plot}
\end{figure*}

\begin{figure*}[th!]
    \centering
    \includegraphics[width=0.95\textwidth]{figs_app/gpt4o-mini_bar_plot.pdf}
    \vspace{-2mm}
    \caption{\textbf{Performance on a weaker LLM (\gptmini)}. We observe similar trends using \model with a weaker base LLM. Bar values represent accuracy and error bars represent standard deviation.}
\label{fig:weaker_llm_performance}
\end{figure*}


\begin{figure*}[th!]
    \centering
    \includegraphics[width=0.8\textwidth]{figs_app/tool_usage_by_16_tasks_AutoGen.pdf}
    \vspace{-2mm}
    \caption{\textbf{Distribution of tools usage.} Frequency of tools used by the \autogen agent for each benchmark.
}
\label{fig:tool_usage_16_tasks_AutoGen}
\end{figure*}


\begin{figure*}[th!]
    \centering
    \includegraphics[width=0.8\textwidth]{figs_app/tool_usage_by_16_tasks_GPT4o-Plugin.pdf}
    \caption{\textbf{Distribution of tools usage.} Frequency of tools used by the \gptplugin agent for each benchmark.
}
\label{fig:tool_usage_16_tasks_GPT4o-Plugin}
\end{figure*}

\begin{figure*}[th!]
    \centering
    \includegraphics[width=0.8\textwidth]{figs_app/tool_usage_by_16_tasks_LangChain.pdf}
    \caption{\textbf{Distribution of tools usage.} Frequency of tools used by the \langchain agent for each benchmark.
}
\label{fig:tool_usage_16_tasks_LangChain}
\end{figure*}




\begin{figure}[th!]
    \centering
    \includegraphics[width=0.5\textwidth]{figs_app/benchmark_gains_radar_barplot.pdf}
    \caption{Performance gains across different benchmarks from our \model framework over the base \gpt model.}
\label{fig:benchmark_gains_radar_barplot}
\end{figure}


\begin{figure*}[th!]
    \centering
    \includegraphics[width=0.95\textwidth]{figs_app/num_steps_hist.pdf}
    \caption{\textbf{Distribution of number of steps used.}}
\label{fig:distribution_of_number_of_steps}
\end{figure*}


\begin{figure*}[th!]
    \centering
    \includegraphics[width=0.99\textwidth]{figs_app/accuracy_vs_max_steps_indiv_delta.pdf}
    \caption{\textbf{Accuracy vs number of maximum steps.} The change in accuracy from a maximum step of 1 is plotted. Most benchmarks improve in performance with the number of allowed steps.}
\label{fig:accuracy_vs_max_steps}
\end{figure*}



\clearpage
\section{Configurations in \model}
\label{app:configuration}

\subsection{Query Analyzer}
\label{app:query_analysis}
\input{prompts/query_analysis}

\subsection{Action Predictor}
\label{app:action_prediction}
\input{prompts/action_prediction}

\subsection{Command Predictor}
\label{app:command_prediction}
\input{prompts/command_prediction}

\subsection{Context Verifier}
\label{app:verification}
\input{prompts/verification}

\subsection{Solution Summarizer}
\label{app:solution_summarization}
\input{prompts/solution_summarization}



\clearpage
\section{Tool Cards in \model}
\label{app:toolcards}

\subsection{ArXiv Paper Searcher Tool}
\label{app:arxiv_paper_searcher_tool}
\input{tools/arxiv_paper_searcher_tool}

\subsection{Generalist Solution Generator Tool}
\label{app:generalist_solution_generator_tool}
\input{tools/generalist_solution_generator_tool}

\subsection{Google Search Tool}
\label{app:google_search_tool}
\input{tools/google_search_tool}

\subsection{Image Captioner Tool}
\label{app:image_captioner_tool}
\input{tools/image_captioner_tool}

\subsection{Path Generalist Classifier Tool}
\label{app:path_generalist_classifier_tool} 
\input{tools/path_generalist_classifier_tool}

\subsection{Pubmed Search Tool}
\label{app:pubmed_search_tool}
\input{tools/pubmed_search_tool}

\subsection{Python Code Generator Tool}
\label{app:python_code_generator_tool}
\input{tools/python_code_generator_tool}

\subsection{Relevant Patch Zoomer Tool}
\label{app:relevant_patch_zoomer_tool}
\input{tools/relevant_patch_zoomer_tool}

\subsection{Text Detector Tool}
\label{app:text_detector_tool}
\input{tools/text_detector_tool}

\subsection{URL Text Extractor Tool}
\label{app:url_text_extractor_tool}
\input{tools/url_text_extractor_tool}

\subsection{Wikipedia Knowledge Searcher Tool}
\label{app:wikipedia_knowledge_searcher_tool}
\input{tools/wikipedia_knowledge_searcher_tool}

\section{Additional Tool Cards in \model}
\label{app:additional_toolcards}

\subsection{Object Detector Tool}
\label{app:object_detector_tool}
\input{tools/object_detector_tool}

\subsection{Advanced Object Detector Tool}
\label{app:advanced_object_detector_tool}
\input{tools/advanced_object_detector_tool}

\subsection{Nature News Fetcher Tool}
\label{app:nature_news_fetcher_tool}
\input{tools/nature_news_fetcher_tool}


\clearpage
\section{Experimental Examples}
\label{app:exp_examples}

\subsection{Demonstration Example}
\label{app:demo_example}

Below is an example from MathVista \cite{lu2024mathvista}, which demonstrates the detailed steps in our \model framework. The base \gpt model fails to provide a correct answer due to its limited visual perception of abstract scenes.

\input{examples/example_mathvista_baseball}


\clearpage
\subsection{Example for Multi-step Mathematical Reasoning}
\input{examples/example_game_of_24}


\clearpage
\subsection{Example for Agentic Reasoning}
\input{examples/example_gaia}

\clearpage
\subsection{Example for Medical Reasoning}
\input{examples/example_slake}


\clearpage
\subsection{Example for Pathology Diagnosis}
\input{examples/example_pathcls}


\clearpage
\subsection{Example for Scientific Reasoning}
\input{examples/example_mmlupro}

\clearpage
\subsection{Example for Fine-grained Visual Reasoning}
\input{examples/example_vision}

\clearpage
\subsection{Exploration Example for Literature Review}
\label{app:example_exploration}
\input{examples/example_exploration}

