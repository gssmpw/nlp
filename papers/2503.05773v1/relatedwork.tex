\section{Literature Review}
\label{sec:litreview}

\subsection{Evolution of AI Governance Debates}
Early academic focus on AI governance concentrated on broad ethical issues, like the promotion of transparency, fairness, and accountability in algorithmic decision-making \cite{bostrom2014paths, floridi2022unified}. Initial discussions pointed out the risks related to unregulated AI systems, such as potential biases, discriminatory outcomes, and unexpected social impacts \cite{dignum2018ethics}. The rapid commercialization and implementation of AI, especially in sectors such as healthcare, finance, and social services, has revealed the shortcomings of primarily voluntary or principle-based guidelines \cite{eubanks2018automating, 10.1093/polsoc/puae020}. Researchers began pushing for more concrete regulatory measures, stating the importance of balancing innovation incentives with societal protections \cite{bryan2024balancing, Li2024}.

The evolution has been influenced by notable incidents and controversies, including claims of algorithmic discrimination in hiring processes and the inappropriate use of facial recognition technologies in public surveillance \cite{chen2023ethics, TurnerLee2022}. In response, both industry and civil society groups have demanded clearer legal frameworks to clarify liability, protect individuals’ rights, and maintain public trust \cite{DHS2024}. As a result, current discussions on AI governance have transitioned to structured methodologies that focus on high-risk applications and necessitate enhanced oversight and enforcement mechanisms.


\subsection{Risk-Based Approaches to AI Regulation}
An agreement has grown about the importance of risk-based regulatory models that evaluate AI technologies based on their potential harm or societal impact \cite{eu2021proposal}. Rather than applying uniform standards to all AI systems, these models differentiate between low-risk applications, such as basic data analytics, and high-risk or safety-critical systems, including those used in medical diagnostics or autonomous driving \cite{voigt2017eu}. Regulators want to adjust requirements according to risk levels to prevent restricting innovation in low-risk scenarios, while simultaneously ensuring robust protections in situations where AI-driven decisions may impact fundamental rights or public welfare \cite{edwards2021eu}.

Supporters of risk-based approaches argue that these methods offer more defined compliance routes for industries and ensure more consistent enforcement for governmental bodies \cite{OECD2023}. Critics caution that classifying AI systems by risk may be challenging in rapidly changing fields, as the nature and severity of potential harms can evolve over time. A study conducted by the appliedAI Institute for Europe analyzed more than 100 AI systems, revealing that 18\% were categorized as high-risk, 42\% as low-risk, and for 40\%, it was indeterminate whether they belonged to the high-risk category. This ambiguity underscores the challenges in risk classification and indicates that vague classifications may impede investment and innovation. \cite{liebl2023ai}. Risk-based paradigms have emerged as central to numerous contemporary policy proposals, significantly influencing regulatory discussions across various jurisdictions. The OECD AI Principles have been adopted by member countries and various global partners, establishing a basis for international cooperation and interoperability in AI governance \cite{OECD2023}.

\subsection{The European Union’s Pioneering Role}
The EU has led efforts in establishing formal risk-based regulations for AI. The European Commission introduced the AI Act in April 2021, building on the success of the General Data Protection Regulation (GDPR) in establishing global standards for data protection \cite{voigt2017eu} \cite{eu2021proposal}. This proposal, effective August 1, 2024, with full enforcement by August 1, 2027, classifies AI applications into four risk tiers: unacceptable, high, limited, and minimal, imposing stricter requirements on higher-risk categories \cite{schuett2024risk}.

High-risk systems under the AI Act are required to meet obligations related to transparency, data governance, and post-market monitoring. They are subject to conformity assessments and potential oversight by national supervisory authorities \cite{eu2021proposal}. Researchers suggest that the substantial market size of the EU may lead the AI Act to serve as a de facto global standard, which caused multinational companies to match their practices to EU regulations. The phenomenon known as the "Brussels Effect" suggests that internationally operating firms may adopt EU regulations to maintain market access, thus broadening the AI Act's impact beyond Europe. Questions persist regarding the practical enforcement of the Act, especially considering the need for coordination among various regulatory bodies across member states \cite{Demkova2025}.

\subsection{The Decentralized U.S. Model}
The United States uses a decentralized regulatory framework for AI, characterized by a combination of federal and state-level rules and guidelines, in contrast to the EU's top-down approach \cite{cath2018artificial}. Sector-specific agencies, including the Food and Drug Administration (FDA) and the National Highway Traffic Safety Administration (NHTSA), regulate particular AI applications, such as medical devices and autonomous vehicles \cite{bateman2022us}. Furthermore, numerous states have proposed AI-related legislation addressing issues like facial recognition and algorithmic accountability \cite{morley2023operationalising}.

Recent federal initiatives indicate an increasing focus on the need for clearer guidance. The National Institute of Standards and Technology (NIST) published an AI Risk Management Framework in 2023, offering voluntary standards that can help organizations in identifying and mitigating AI risks \cite{ai2023artificial}. The White House has released a “Blueprint for an AI Bill of Rights,” which defines principles like fairness, privacy, and transparency \cite{OSTP2022}. These initiatives show a growing awareness of AI's societal implications; however, the U.S. system continues to be fragmented, with numerous stakeholders expressing concerns regarding deficiencies in legal protections and enforcement \cite{2TurnerLee2022}.

\subsection{The UK’s Sector-Specific Flexibility}
The UK aims to establish itself as a global leader in “pro-innovation”  AI governance, using a flexible, sector-specific strategy that allows regulators to customize regulations for individual industries \cite{UKGovAIRegulation2024,lords2018ai}. Examples include the Financial Conduct Authority guidelines for AI in financial services and the Medicines and Healthcare Products Regulatory Agency's oversight of AI-driven medical devices \cite{singh2024artificial}.

This decentralized model aims to encourage technological experimentation and rapid scaling, while dealing with potential risks through specialized oversight \cite{Hickman2025}. Critics argue that a loose coordination mechanism may end up in inconsistencies and inadequate oversight in high-risk applications. The Ada Lovelace Institute has raised concerns that the current framework may insufficiently address the complexities and risks related to advanced AI systems, which could lead to regulatory gaps \cite{Davies2023}. Current discussions focus on how important it is for the UK to implement more comprehensive legislation versus continuing its sector-by-sector approach, particularly in light of advancing AI technologies and the nation's goal to sustain competitiveness in the global AI landscape. Some experts support a unified regulatory approach to establish consistent standards across sectors, whereas others argue that the current flexible framework promotes adaptability and innovation \cite{HouseOfLordsLibrary2023}. 

\subsection{China’s Centralized, Control-Oriented Strategy}
The governance framework for AI in China is characterized by state-led directives that integrate AI development with general national objectives in technology, security, and economic growth \cite{Wang2024}. The government has implemented specific regulations for technologies including facial recognition and generative AI, often requiring registration and algorithmic audits \cite{cheng2023shaping}. The Personal Information Protection Law (PIPL) and the Data Security Law regulate data management and global data transfers \cite{chen2024developing}.

This centralized approach may enable fast execution of extensive AI initiatives; however, scholars raise concerns about privacy, civil liberties, and the possibility of exporting of this model to other jurisdictions \cite{DHS2024, creemers2021china}. Recent regulations in China include regulations for real-time monitoring of AI-generated content, aimed at making social stability and national security, in addition to data protection measures. The "Interim Measures for the Management of Generative Artificial Intelligence Services," which took effect in August 2023, require that AI-generated content follow with the Core Socialist Values and avoid producing material that may disrupt economic or social order. Providers have to use real-time monitoring tools and data analysis techniques to detect and address abnormal activities or content generated by AI systems \cite{ye2024privacy}.

\subsection{Gaps in Comparative Analyses}
Despite a large amount of research on individual AI governance frameworks, there is a notable lack of systematic cross-regional comparisons of AI risk management frameworks \cite{luna2024navigating}. Research frequently confines itself to descriptive analyses of legislation or general ethical principles, ignoring to investigate the practical implementation of these policies or to offer sector-specific comparisons \cite{schmitz2024global}. The literature rarely offers consistent criteria—such as risk mitigation, adaptability, transparency, and implementation feasibility—for assessing various regulatory approaches \cite{batool2025ai}. The growing number of AI applications across different industries requires integrated analyses to inform policymakers, industry stakeholders, and civil society.

\subsection{Toward a Comprehensive Comparative Framework}
This paper aims to synthesize existing scholarship and regulatory documents to offer a comparative study of AI governance in the EU, U.S., UK, and China. The research uses qualitative methods, including thematic analysis, case studies, and framework-based evaluation, to explain how different governance models respond to the risks and opportunities presented by AI. This study aims to enhance the discussion on effective and context-sensitive AI regulation by addressing both high-level legal frameworks and practical implementation challenges. The findings aim to inform future policy decisions, encourage international collaboration, and make sure the responsible use of AI technologies for collective benefit.