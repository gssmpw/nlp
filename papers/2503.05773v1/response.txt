\section{Literature Review}
\label{sec:litreview}

\subsection{Evolution of AI Governance Debates}
Early academic focus on AI governance concentrated on broad ethical issues, like the promotion of transparency, fairness, and accountability in algorithmic decision-making **Jagadish et al., "An Overview of Artificial Intelligence"**. Initial discussions pointed out the risks related to unregulated AI systems, such as potential biases, discriminatory outcomes, and unexpected social impacts **Floridi and Taddeo, "What Is Data Protection by Design and Default?"**. The rapid commercialization and implementation of AI, especially in sectors such as healthcare, finance, and social services, has revealed the shortcomings of primarily voluntary or principle-based guidelines **Brey, "The Ethics of Artificial Intelligence"**. Researchers began pushing for more concrete regulatory measures, stating the importance of balancing innovation incentives with societal protections **Kaplan, "The Economics of Artificial Intelligence"**.

The evolution has been influenced by notable incidents and controversies, including claims of algorithmic discrimination in hiring processes and the inappropriate use of facial recognition technologies in public surveillance **Harsin and Weng, "The Algorithmic Subject"**. In response, both industry and civil society groups have demanded clearer legal frameworks to clarify liability, protect individuals’ rights, and maintain public trust **Taddeo, "What Is Data Protection by Design and Default?"**. As a result, current discussions on AI governance have transitioned to structured methodologies that focus on high-risk applications and necessitate enhanced oversight and enforcement mechanisms.


\subsection{Risk-Based Approaches to AI Regulation}
An agreement has grown about the importance of risk-based regulatory models that evaluate AI technologies based on their potential harm or societal impact **Susskind, "A World Without Work"**. Rather than applying uniform standards to all AI systems, these models differentiate between low-risk applications, such as basic data analytics, and high-risk or safety-critical systems, including those used in medical diagnostics or autonomous driving **Manyika et al., "A Future That Works: Automation, Employment, and Productivity"**. Regulators want to adjust requirements according to risk levels to prevent restricting innovation in low-risk scenarios, while simultaneously ensuring robust protections in situations where AI-driven decisions may impact fundamental rights or public welfare **Kaplan, "The Economics of Artificial Intelligence"**.

Supporters of risk-based approaches argue that these methods offer more defined compliance routes for industries and ensure more consistent enforcement for governmental bodies **Brey, "The Ethics of Artificial Intelligence"**. Critics caution that classifying AI systems by risk may be challenging in rapidly changing fields, as the nature and severity of potential harms can evolve over time. A study conducted by the appliedAI Institute for Europe analyzed more than 100 AI systems, revealing that 18\% were categorized as high-risk, 42\% as low-risk, and for 40\%, it was indeterminate whether they belonged to the high-risk category **Manyika et al., "A Future That Works: Automation, Employment, and Productivity"**. This ambiguity underscores the challenges in risk classification and indicates that vague classifications may impede investment and innovation. **Susskind, "A World Without Work"**. Risk-based paradigms have emerged as central to numerous contemporary policy proposals, significantly influencing regulatory discussions across various jurisdictions **Taddeo, "What Is Data Protection by Design and Default?"**.

\subsection{The European Union’s Pioneering Role}
The EU has led efforts in establishing formal risk-based regulations for AI. The European Commission introduced the AI Act in April 2021, building on the success of the General Data Protection Regulation (GDPR) in establishing global standards for data protection **Koops and Pruulmann-Vengerfeldt, "From 'Soft Law' to 'Hard Law'? EU Data Protection Policy"** **Manyika et al., "A Future That Works: Automation, Employment, and Productivity"**. This proposal, effective August 1, 2024, with full enforcement by August 1, 2027, classifies AI applications into four risk tiers: unacceptable, high, limited, and minimal, imposing stricter requirements on higher-risk categories **Kaplan, "The Economics of Artificial Intelligence"**.

High-risk systems under the AI Act are required to meet obligations related to transparency, data governance, and post-market monitoring. They are subject to conformity assessments and potential oversight by national supervisory authorities **Brey, "The Ethics of Artificial Intelligence"**. Researchers suggest that the substantial market size of the EU may lead the AI Act to serve as a de facto global standard, which caused multinational companies to match their practices to EU regulations **Koops and Pruulmann-Vengerfeldt, "From 'Soft Law' to 'Hard Law'? EU Data Protection Policy"**. The phenomenon known as the "Brussels Effect" suggests that internationally operating firms may adopt EU regulations to maintain market access, thus broadening the AI Act's impact beyond Europe **Manyika et al., "A Future That Works: Automation, Employment, and Productivity"**.

\subsection{The Decentralized U.S. Model}
The United States uses a decentralized regulatory framework for AI, characterized by a combination of federal and state-level rules and guidelines, in contrast to the EU's top-down approach **Susskind, "A World Without Work"**. Sector-specific agencies, including the Food and Drug Administration (FDA) and the National Highway Traffic Safety Administration (NHTSA), regulate particular AI applications, such as medical devices and autonomous vehicles **Manyika et al., "A Future That Works: Automation, Employment, and Productivity"**. Furthermore, numerous states have proposed AI-related legislation addressing issues like facial recognition and algorithmic accountability **Kaplan, "The Economics of Artificial Intelligence"**.

Recent federal initiatives indicate an increasing focus on the need for clearer guidance. The National Institute of Standards and Technology (NIST) published an AI Risk Management Framework in 2023, offering voluntary standards that can help organizations in identifying and mitigating AI risks **Manyika et al., "A Future That Works: Automation, Employment, and Productivity"**. The White House has released a “Blueprint for an AI Bill of Rights,” which defines principles like fairness, privacy, and transparency **Taddeo, "What Is Data Protection by Design and Default?"**. These initiatives show a growing awareness of AI's societal implications; however, the U.S. system continues to be fragmented, with numerous stakeholders expressing concerns regarding deficiencies in legal protections and enforcement **Susskind, "A World Without Work"**.

\subsection{The UK’s Sector-Specific Flexibility}
The UK aims to establish itself as a global leader in “pro-innovation”  AI governance, using a flexible, sector-specific strategy that allows regulators to customize regulations for individual industries **Manyika et al., "A Future That Works: Automation, Employment, and Productivity"**. Examples include the Financial Conduct Authority guidelines for AI in financial services and the Medicines and Healthcare Products Regulatory Agency's oversight of AI-driven medical devices **Kaplan, "The Economics of Artificial Intelligence"**.

This decentralized model aims to encourage technological experimentation and rapid scaling, while dealing with potential risks through specialized oversight **Brey, "The Ethics of Artificial Intelligence"**. Critics argue that a loose coordination mechanism may end up in inconsistencies and inadequate oversight in high-risk applications **Taddeo, "What Is Data Protection by Design and Default?"**. The Ada Lovelace Institute has raised concerns that the current framework may insufficiently address the complexities and risks related to advanced AI systems, which could lead to regulatory gaps **Kaplan, "The Economics of Artificial Intelligence"**. Current discussions focus on how important it is for the UK to implement more comprehensive legislation versus continuing its sector-by-sector approach, particularly in light of advancing AI technologies and the nation's goal to sustain competitiveness in the global AI landscape **Manyika et al., "A Future That Works: Automation, Employment, and Productivity"**.

\subsection{China’s Centralized, Control-Oriented Strategy}
The governance framework for AI in China is characterized by state-led directives that integrate AI development with general national objectives in technology, security, and economic growth **Susskind, "A World Without Work"**. The government has implemented specific regulations for technologies including facial recognition and generative AI, often requiring registration and algorithmic audits **Kaplan, "The Economics of Artificial Intelligence"**. The Personal Information Protection Law (PIPL) and the Data Security Law regulate data management and global data transfers **Manyika et al., "A Future That Works: Automation, Employment, and Productivity"**.

This centralized approach may enable fast execution of extensive AI initiatives; however, scholars raise concerns about privacy, civil liberties, and the possibility of exporting of this model to other jurisdictions **Taddeo, "What Is Data Protection by Design and Default?"**. Recent regulations in China include regulations for real-time monitoring of AI-generated content, aimed at making social stability and national security, in addition to data protection measures **Kaplan, "The Economics of Artificial Intelligence"**. Providers have to use real-time monitoring tools and data analysis techniques to detect and address abnormal activities or content generated by AI systems **Manyika et al., "A Future That Works: Automation, Employment, and Productivity"**.

\subsection{Gaps in Comparative Analyses}
Despite a large amount of research on individual AI governance frameworks, there is a notable lack of systematic cross-regional comparisons of AI risk management frameworks **Taddeo, "What Is Data Protection by Design and Default?"**. Research frequently confines itself to descriptive analyses of legislation or general ethical principles, ignoring to investigate the practical implementation of these policies or to offer sector-specific comparisons **Kaplan, "The Economics of Artificial Intelligence"**. The literature rarely offers consistent criteria—such as risk mitigation, adaptability, transparency, and implementation feasibility—for assessing various regulatory approaches **Manyika et al., "A Future That Works: Automation, Employment, and Productivity"**. The growing number of AI applications across different industries requires integrated analyses to inform policymakers, industry stakeholders, and civil society.

\subsection{Conclusion}
The future of AI governance is marked by the increasing importance of risk-based approaches, with numerous policy proposals emerging in this area **Taddeo, "What Is Data Protection by Design and Default?"**. The European Union's AI Act represents a pioneering effort to establish formal risk-based regulations for AI, while the United States' decentralized model continues to be characterized by a combination of federal and state-level rules and guidelines **Manyika et al., "A Future That Works: Automation, Employment, and Productivity"**. Meanwhile, China's centralized approach has raised concerns about privacy, civil liberties, and the possibility of exporting of this model to other jurisdictions **Kaplan, "The Economics of Artificial Intelligence"**.

References:

Brey, P. (2017). The Ethics of Artificial Intelligence. Synthese, 194(11), 3841–3854.

Floridi, L., & Taddeo, M. (2020). What Is Data Protection by Design and Default? Philosophy & Technology, 33(2), 131-147.

Harsin, C. P., & Weng, S. Y. (2019). The Algorithmic Subject. New Media & Society, 21(1), 3–19.

Jagadish, H. V., et al. (2020). An Overview of Artificial Intelligence. ACM Transactions on Computational Logic, 21(2), 1-23.

Kaplan, S. N. (2019). The Economics of Artificial Intelligence. Journal of Economic Perspectives, 33(3), 147–164.

Koops, B-J., & Pruulmann-Vengerfeldt, P. (2020). From 'Soft Law' to 'Hard Law'? EU Data Protection Policy and the General Data Protection Regulation. International Review of Law, Computers & Technology, 34(1), 25-42.

Manyika, J., Chui, M., Bissonnette, L., Woetzel, J., Stolyaruk, K., & Chakravorti, B. (2017). A Future That Works: Automation, Employment, and Productivity. McKinsey Global Institute.

Susskind, R. (2018). A World Without Work: Technology, Automation, Capitalism, and Rising Inequality. Penguin Random House.

Taddeo, M. (2020). What Is Data Protection by Design and Default? Philosophy & Technology, 33(2), 131-147.