\section{Introduction}
\label{sec:intro}

The field of generative modeling has witnessed significant advancements in recent years, with Diffusion Transformers (DiTs) emerging as a powerful paradigm for creating high-quality photorealistic content~\citep{DiT,sd3,openai2024sora}. 
One notable innovation is Next-DiT~\citep{luminanext}, an improved version of flow-based DiT that has shown strong performance in image generation.
By combining architectural enhancements such as 3D RoPE for superior spatiotemporal representation, sandwich normalization for stabilized training, and grouped-query attention for efficient attention computation, Next-DiT has achieved remarkable success. The model consistently produces images that are not only visually compelling but also exhibit high diversity and fine-grained details. 
However, despite its advancements in image synthesis, the potential of Next-DiT for video generation remains underexplored. 

Video generation poses unique challenges that go beyond those encountered in image generation. The inherent complexity of modeling both spatial and temporal dimensions in a coherent manner introduces significant computational and architectural challenges. While Next-DiT can be adapted for video tasks, its current design is not specifically tailored for the spatiotemporal intricacies of video data, leading to an excessive number of video tokens and low computational efficiency. 
These limitations underscore the need for a tailored approach that fully leverages the capabilities of Next-DiT while addressing the unique demands of video synthesis.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figure/video_demo.pdf}
    \vspace{-1.5em}
    \caption{Lumina-Video demonstrates a strong ability to generate high-quality videos with rich details and remarkable temporal coherence, accurately following both simple and detailed text prompts.}
    % \vspace{-1em}
    \label{fig:demos}
\end{figure*}

To bridge this gap, we introduce Lumina-Video, a novel framework that excels at generating high-quality videos by building upon the strengths of Next-DiT. 
At the core of Lumina-Video is Multi-scale Next-DiT, an extension of Next-DiT into a multi-scale architecture by introducing multiple patch sizes that share a common DiT backbone and are trained jointly in a unified manner. 
This straightforward yet elegant approach allows the model to learn video structures across different computational budgets simultaneously. By strategically allocating different patchifications to various sampling timesteps,~\sysname{} achieves notable improvements in inference efficiency with only a minor sacrifice in quality. This design also enables users to dynamically adjust the computational cost based on resource constraints and specific requirements, offering greater flexibility during inference.
Considering the importance of motion in videos, we additionally derive motion scores from optical flow and incorporate them as an extra conditioning input to the DiT. By designing a systematic strategy that separately manipulates the motion conditioning of positive and negative classifier-free guidance (CFG)~\citep{cfg} samples,~\sysname{} provides an effective interface for controlling the extent of dynamics in generated videos. 
We further refine the training strategy by progressively training the model on videos with increasing spatiotemporal resolutions to improve training efficiency, leveraging joint image-video training to enhance frame quality and text comprehension, and incorporating multi-source training to fully utilize diverse real and synthetic data sources. These designs enable Lumina-Video to seamlessly tackle the challenging video generation task across a wide range of scenarios.

Our contributions establish Lumina-Video as a new solution for video generation, 
offering researchers and practitioners a powerful and flexible tool for creating video content. 
By fully unleashing the potential of Multi-Scale Next-DiT, Lumina-Video is capable of generating high-fidelity videos of varying resolution, excelling in both quantitative metrics and visual quality. 
In addition, we design a Lumina-V2A, a Next-DiT-based video-to-audio framework to bring generated silent videos to real life with synchronized sounds. 
Furthermore, in line with our commitment to democratizing access to advanced video generation technologies, we open-source our training framework and model parameters, empowering the research community to explore, extend, and deploy Lumina-Video across a diverse range of applications. 
Through this work, we aim to catalyze future innovations in video generation and pave the way for broader adoption of generative modeling techniques.