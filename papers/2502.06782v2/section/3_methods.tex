\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figure/architecture.pdf}
    \vspace{-1em}
    \caption{Architecture of Lumina-Video with Multi-scale Next-DiT and Motion Conditioning.}
    \label{fig:architecture}
    % \vspace{-1em}
\end{figure*}

\section{Lumina-Video}
\label{sec:method}

In this section, we elaborate on the multi-patch method and the explicit injection of motion motioning. An introduction to the preliminaries, including the VAE, the text encoder, the Next-DiT architecture, and the loss function, is deferred to Sec.\ref{sec:basic_comp} in the appendix. A graphical illustration of the overall architecture is provided in Fig.\ref{fig:architecture}. An extra video-to-audio extension is introduced in Sec.~\ref{sec:V2A}.

\subsection{Multi-scale Next-DiT}
\label{sec:ms-next-dit}

In video generation based on diffusion transformers, the number of tokens processed by the transformer plays a pivotal role in determining the computational cost and training efficiency. More tokens allows the DiT to capture more fine-grained details, while it involves increased computational cost and degrades the efficiency

In this work, we propose Multi-scale Next-DiT, a novel architecture that incorporates multiple pairs of patchify and unpatchify layers trained in a unified manner. This architecture enables a systematic analysis of the impact of token quantities on the denoising process and demonstrates improved efficiency. By leveraging multiple scales, the model enables high efficiency by properly combining multiple scales in one complete denoising process. Moreover, our approach offers great flexibility that allows the model to adapt dynamically to diverse requirements in the inference stage. This adaptability advances the development and deployment of text-to-video models in multiple practical scenarios.

\subsubsection{Multi-scale Patchification}

In DiT-based T2V models, given a fixed VAE, the number of tokens is determined by two operations: patchify and unpatchify. 
The patchify layer converts the noised latent representation $\mathbf{z}_t \in \mathbb{R}^{T \times H \times W \times C}$ into a sequence of tokens via a linear transformation before undergoing the DiT blocks, where the number of tokens can be calculated as 
\begin{equation}
    N = \frac{T \times H \times W}{p_t \times p_h \times p_w}
\end{equation}
$(p_t, p_h, p_w)$ denotes the patch size, a critical hyperparameter controlling the granularity of the representation. 
After the DiT blocks, the output tokens are linearly projected and reshaped back into the original shape by an unpatchify layer.

To endow the model with the ability to flexibly handle different levels of granularity based on varying computational requirements, we introduce the core component of Multi-scale Next-DiT: multi-scale patchification. Instead of using a single patch size, we instantiate multiple pairs of patchify and unpatchify layers with different spatio-temporal patch sizes, each corresponding to a distinct scale. Specifically, we employ a hierarchy of $M$ patch sizes
$\{P^i=(p_t^i, p_h^i, p_w^i)|i=1,\cdots,M\}$, where $p_t^i \leq p_t^{i+1}$, $p_w^i \leq p_w^{i+1}$, $p_h^i \leq p_h^{i+1}$. 
Patchifying with a larger patch size 
leads to greater computation reduction, while patchifying with a smaller patch size preserves finer details in the latent representations. This allows us to dynamically adjust the level of abstraction used according to our demand. 

Note that in Multi-scale Next-DiT, all patch sizes share the same DiT backbone. This design minimizes parameter count and memory overhead while facilitating knowledge sharing across scales.

\subsubsection{Analysis}
\label{sec:analysis}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figure/losses_small.pdf}
    \vspace{-1em}
    \caption{Loss curves for different patch sizes at different denoising timesteps. See Sec.~\ref{sec:full-loss-bin} for the complete figure.}
    % \vspace{-1em}
    \label{fig:loss-bin}
\end{figure*}

The unified training with the shared backbone and multiple patchifications provides a novel interface to investigate how different patch sizes affect the quality of denoising prediction, which can be directly reflected by loss magnitudes.

To do so, we evaluate three sets of patch sizes and observe their behavior across timesteps by dividing the denoising process uniformly into 20 time windows. We then analyze the training loss at various timesteps for different patch sizes. The three patch sizes correspond to different scales of spatiotemporal resolution: small, medium, and large.

Figure~\ref{fig:loss-bin} shows the behavior of the loss curves across different patch sizes in 5 uniformly selected $t$ spans, while the complete 20-span visualization is provided in Sec.\ref{sec:full-loss-bin}. In the very early stages of denoising ($0 \leq t \leq 0.1$), the loss curves for all three patch sizes overlap, indicating comparable prediction quality. As $t$ increases, the loss curve for the smallest scale begins to deviate first, posing an obvious and stable gap compared to smaller patches. Similarly, as $t$ grows even larger (say, after $0.4)$, the loss curve for the medium patch also deviates from that of the small patch, implying diverged prediction qualities. These findings validate previous assumptions about the varying nature of tasks performed at different timesteps: early stages of denoising focus on capturing the global structure, where smaller scales are sufficient to predict velocity accurately; as the process progresses, finer details become crucial, necessitating denoising on larger scales. 

This empirical evidence justifies employing a hierarchical generation process by gradually increasing the scale throughout the denoising process, which reduces computational costs while maintaining quality. 

\begin{algorithm}[t]
   \caption{Training of Multi-scale Next-DiT}
   \label{alg:train}
    \begin{algorithmic}
       \STATE {\bfseries Input:} Model $\mathbf{u}^\theta$, dataset $\mathcal{D}$, batch size $B$, patch sizes $\mathcal{P}=\{P_i\}_{i=1}^M$, timeshift values $\{\alpha_i\}_{i=1}^M$, number of training steps $T$, learning rate $\eta$
       \FOR{$iteration=1$ {\bfseries to} $T$}
           \FOR{$k=1$ {\bfseries to} $M$}
               \STATE Sample a batch of $B$ samples from $\mathcal{D}$
               
               \STATE Sample $t'$ uniformly from $[0,1]$
               \STATE Compute rescaled time $t$ with Eq.~\ref{eq:timeshift} and $\alpha_{k}$
               \STATE Compute flow matching loss $\mathcal{L}_k$ with $P_k$
               % \[
               % \mathcal{L}_k(\theta) = \frac{1}{B}\sum_{j=1}^B \lVert \mathbf{u}^\theta(P_k, \mathbf{x}_t^j, t) - \mathbf{u}_t(P_k, \mathbf{x}_t^j | \mathbf{x}_1^j)\rVert ^2
               % \]
               \STATE Compute gradients $\nabla_{\theta} \mathcal{L}_k$ and accumulate
           \ENDFOR
           \STATE Update model parameters $\mathbf{\theta}$ with $\nabla_{\theta}$
           \STATE Zero out accumulated gradients: $\nabla_{\theta} \mathcal{L} \gets 0$
       \ENDFOR
    \end{algorithmic}
\end{algorithm}

\subsubsection{Training: Scale-aware Timestep Shifting}
\label{sec:scle-aware-ts}
Based on the analysis in Section \ref{sec:analysis}, we observe that larger patch sizes are more suitable for the early stages of denoising with a focus on capturing broad structures. In contrast, smaller patch sizes are more effective in the later stages, where the model needs to capture finer details as the noise diminishes. This observation suggests that applying different timestep sampling schedules to the training of different patch sizes can lead to more efficient training. 

Rather than dividing the trajectory into discrete windows, which limits the interaction between different patch sizes across timesteps and hinders the sharing of knowledge between the various scales, we allow the timestep to be sampled from the entire trajectory $[0,1]$ for all patch sizes. Inspired by Stable Diffusion 3~\citep{sd3}, we then customize the training resources allocation within different scales by assigning different time shift factors.
Specifically, for each patch size $P_i$, we define a shift value $\alpha_i$, which determines how much the sampled timestep is shifted toward the start of the trajectory. 
During training patch size $P_i$, a timestep $t'$ is uniformly sampled from the interval $[0,1]$ for each sample. The timestep $t'$ is then mapped to the actual timestep $t$ using the following formula:
\begin{equation}
    \label{eq:timeshift}
    t = \frac{t'}{t' + \alpha_i - \alpha_i t' }
\end{equation}
For smaller patch sizes, we assign a smaller shift value to increase the likelihood of sampling larger timesteps, (i.e., later stages of denoising), where finer details are more important. 
Conversely, for larger patch sizes, we assign a larger shift value, which increases the probability of sampling smaller timesteps, where coarse structures are more relevant. 

This shift-based allocation effectively tilts the training resources, ensuring that the most advantageous intervals for each patch size, balancing quality and efficiency, are sampled more frequently, thereby maximizing improvements in practical inference.
Furthermore, it allows the model to more effectively share knowledge across stages and patch sizes and learn a unified representation. We summarize the training process in algorithm~\ref{alg:train}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/inference.pdf}
    \vspace{-0.5em}
    \caption{Multi-scale Patchification allows Lumina-Video to perform flexible multi-stage denoising during inference, leading to a better tradeoff between quality and efficiency.}
    \vspace{-1em}
    \label{fig:inference}
\end{figure}
\subsubsection{Flexible Multi-stage Inference}

Training Multi-scale Next-DiT across multiple scales unlocks significant flexibility during inference. This flexibility is particularly valuable for resource-constrained scenarios, high-throughput requirements, or rapid prototyping when adjusting inference hyperparameters. Following the analysis in Sec.\ref{sec:analysis}, we propose a multi-stage denoising strategy: using smaller scales during early timesteps to determine the videoâ€™s general structure, followed by refinement with increasingly larger scales in later stages. As illustrated in Figure \ref{fig:inference}, this approach enjoys the advantage of reducing computation with minor degradation in quality.

\input{table/vbench_extended}
% \cref{tab:vbench-xtended}

\subsection{Explicit Control over Motion Condition}

Text-to-video models often exhibit overly static behavior, indicating the need for an explicit mechanism for controlling the intensity of motion in the generated videos. In~\sysname{}, we introduce a motion conditioning mechanism that allows for direct control over motion characteristics.

Specifically, we condition the denoising process on a motion score in the same way as it is conditioned on the timestep, shown in Fig.\ref{fig:architecture}(b). 
During training, we calculate this motion score as the average of the magnitude of the optical flow using UniMatch~\citep{unimatch}, an off-the-shelf optical flow model. By conditioning the denoising process on this motion score, the model learns to generate videos with an aligned extent of dynamics. As validated in Sec.~\ref{sec:exp_motion}, by reasonably manipulating the motion conditioning for the positive and negative classifier-free guidance (CFG) samples, the generated dynamic degree can be adjusted effectively and reliably. Moreover, we introduce stochasticity by randomly dropping the motion condition with a probability $p=0.4$ during training to handle situations where the user may not want explicit control over the motion score. This enables the model to adjust the intensity of motion based on the text prompt alone when no motion score is provided.