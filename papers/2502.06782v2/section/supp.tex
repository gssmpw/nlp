\section{Basic Compositions of~\sysname{}}
\label{sec:basic_comp}
\subsection{Loss function}
\sysname{} is trained with flow matching \cite{FlowMatching,FlowMatchingGuide,RectifiedFlow}, a generative framework which builds a probability path from random noise $\mathbf{x}_0 \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ to the target data distribution $\mathbf{x}_1 \sim p_{\mathrm{data}}$. 
The model is tasked with predicting the velocity field of samples $\mathbf{u}_t(\cdot)$ which can be used to reconstruct the data sample by solving the following Ordinary Differential Equation (ODE):
\begin{equation}
    \frac{\mathrm{d}}{\mathrm{d}t} \mathbf{x}_t = \mathbf{u}_t (\mathbf{x}_t)
\end{equation}
One commonly adopted form of the probability path is the linear path, which assumes $\mathbf{x}_t$ is a linear interpolation of noise and data sample:
\begin{equation}
\label{eq:xt}
    \mathbf{x}_t = t \mathbf{x}_1 + (1-t) \mathbf{x}_0 \sim \mathcal{N}(t \mathbf{x}_1, (1-t)^2 \mathbf{I})
\end{equation}
This assumption leads to a velocity field in the form of
\begin{equation}
\label{eq:velocity}
    \mathbf{u}_t(\mathbf{x}|\mathbf{x}_1) = \frac{\mathbf{x}_1 - \mathbf{x}}{1 - t}
\end{equation}
Equipped with Equation \ref{eq:xt} and \ref{eq:velocity}, the flow matching loss can be formulated as
\begin{equation}
    \mathcal{L}(\theta) = \mathbb{E}_{t,\mathbf{x}_t,\mathbf{x}_1} \lVert \mathbf{u}^\theta_t(\mathbf{x_t},t) - \mathbf{u}_t(\mathbf{x}_t|\mathbf{x}_1) \rVert^2
\end{equation}
% \begin{equation}
%     \mathbf{x}_t \sim \mathcal{N}(t \mathbf{x}_1, (1-t)^2 \mathbf{I})
% \end{equation}

\subsection{Architecture}

\paragraph{VAE.} We adopt the 3D causal VAE of CogVideoX \cite{CogVideoX} for encoding and decoding videos between the pixel space and the latent space. Compared to the 2-D VAE used in Lumina-Next with spatial compression only, this VAE achieves higher efficiency by applying compression in the temporal dimension with less information leak from redundant frames. 

\paragraph{Text Encoder.} 
Following Lumina-Next, we utilize the Gemma-2-2B model \cite{team2024gemma} as our text encoder. Despite being a lightweight text encoder, Gemma-2-2B excels at extracting visual semantics from natural language, enabling accurate and efficient alignment between textual descriptions and video content. 

\paragraph{Multi-scale Next-DiT.} The backbone of Lumina-Video is an improved version of Next-DiT~\citep{luminanext}, a flow-based diffusion transformer that incorporates the following key modifications to diffusion transformers: 
1) Replacing 1D RoPE with 3D RoPE to instill more accurate positional prior in visual modeling;
2) Introducing sandwich normalization to control the magnitude of activations and stabilize the training process;
3) Incorporating Grouped-Query Attention to reduce computational demand;
To adapt Next-DiT to video generation, Lumina-Video introduces Multi-scale Next-DiT, a transformative extension of Next-DiT to a multi-scale architecture, which is elaborated in Sec.~\ref{sec:ms-next-dit}.

\section{Training Details}
The AdamW optimizer~\citep{adamw}, with weight decay set to \(0.0\) and betas \((0.9, 0.95)\), is employed. Furthermore, PyTorch FSDP~\citep{fsdp} with gradient checkpointing is utilized for reduced memory cost. To enhance training throughput, all video data are pre-encoded using a VAE encoder before training. The data are then clustered based on duration, ensuring that each global batch consists of samples with similar lengths.

\newpage
\section{Detailed Results}
\subsection{Full VBench Results}
\input{table/vbench_all}
\newpage
\subsection{Full version of Figure~\ref{fig:loss-bin}}
\label{sec:full-loss-bin}
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figure/losses.pdf}
    \vspace{-1.5em}
    \caption{Complete figure of loss curves for different patch sizes at different denoising timesteps.}
    \vspace{-1em}
    \label{fig:loss-bin-complete}
\end{figure*}

\section{Video-to-Audio}
\label{sec:V2A}
In this section, we extend Lumina-Video with video-to-audio ability by designing Lumina-V2A to generate ambient sounds for silent video by synchronizing with visible scenes.
\subsection{Background}
Some existing works adopt a two-stage process to first align the video features with acoustic~\cite{luo2024diff, wangfrieren} by unsupervised pretraining, and then introduce diffusion or flow-matching models to generate audio. Other approaches are proposed to first extract visual language~\cite{wang2024v2a} or time-varying features~\cite{zhang2024foleycrafter, jeong2024read, lee2024video, xie2024sonicvisionlm} such as timestamps or energy curves from videos and then leverage pre-trained text-to-audio generation models to produce corresponding audio via trainable introduced adapters. Recent V2A works have achieved audio generation conditioned on both video and text~\cite{polyak2024movie, cheng2024taming, chen2024video}, creating high-fidelity sound effects aligned with visual content. Our proposed V2A model aims to generate audios that are temporally synchronized with videos and semantically aligned with both video and text.          

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figure/v2a_architecture.pdf}
    \vspace{-2em}
    \caption{Illustration of Lumina-V2A Model based on Next-DiT}
    \label{fig:v2a_architecture}
    \vspace{-1em}
\end{figure}

\subsection{Model Architecture}
As depicted in Figure~\ref{fig:v2a_architecture}, our Lumina-V2A model receives the video and text conditions to generate audio based on Next-DiT and rectified flow matching~\cite{RectifiedFlow}. Specifically, the input audio waveform is first transformed into the 2D mel-spectrogram with an STFT operator and is then encoded by a pre-trained audio VAE encoder~\cite{liu2023audioldm} to obtain the compressed audio latents. Meanwhile, the pre-trained CLIP visual encoder and CLIP textual encoder~\cite{fang2023data, radford2021learning} are employed to successively encode video and text into frame-level visual features and text embeddings. Next, visual features, text embeddings and audio latents are independently projected into modality-specific inputs to undergo the following Next-DiT blocks. 

To efficiently integrate video, text, and audio modalities, we adopt a sequence of video-text-audio Next-DiT blocks to process the concatenated multimodal features via an inner co-attention mechanism. Within the co-attention module, the visual and text embeddings are concatenated together to interact with audio latent tokens like Lumina-Video. Additionally, It is important to ensure semantic alignment between audio and video-text and temporal synchronization between audio and video. To do so, we propose a multimodal conditioning module to integrate the time embedding, global visual and textual features, and high-frame-rate visual features from Synchformer~\cite{iashin2024synchformer}, forming a multimodal condition to be injected into Next-DiT blocks via scaling and gating operation. 

During the inference stage, once the audio latent representation is generated by the proposed diffusion transformer, the pre-trained VAE decoder is used to reconstruct generated audio latents back to the mel-spectrogram that is then transformed into audio waveform via a pre-trained HiFi-GAN vocoder~\cite{kong2020hifi}. 

\subsection{Training Data}
We conduct experiments of our video-to-audio model on the VGGSound~\cite{chen2020vggsound}, a large-scale audio-visual dataset including 500 hours of videos with audio tracks in the wild and 310 classes. After filtering out invalid video IDs, we split the remaining dataset into around 180k videos for training, 2k videos for validation, and 15k for testing. To further improve the quality of VGGSound, we adopt the AV-Align score~\cite{yariv2024diverse} as the temporal alignment metric to select more aligned audio-visual pairs by setting the threshold as 0.2, resulting in about 110k high-quality video-audio pairs for future fine-tuning. By following the existing work~\cite{cheng2024taming}, we truncate each video clip to 8s duration during the training stage. 

