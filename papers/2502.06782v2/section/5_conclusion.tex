\section{Conclusion \& Futer Work}
\label{sec:conclusion}

We present Lumina-Video, a novel framework designed to overcome the unique challenges of video generation by building on the successes of the Next-DiT architecture. Lumina-Video boosts efficiency by utilizing a Multi-scale Next-DiT design and allows direct control of the dynamic degree by explicit conditioning. Through a combination of strategies including progressive training and multi-source training, Lumina-Video demonstrates a powerful solution for generating high-fidelity videos with both spatial and temporal consistency. Additionally, the companion Lumina-V2A model further enhances real-world applicability through audio-visual synchronization.

Moving forward, we will focus on two key areas. First, multi-scale patchification shares the same motivation as dynamic neural networks, where simpler tasks require fewer resources. Insights from dynamic networks and network compression suggest that an organic compression across multiple dimensions (e.g., depth, width, tokens) generally offers better trade-offs than focusing on a single dimension, and we will explore this further. Second, while benchmarks show that Lumina-Video generates prompt-coherent videos meeting basic quality standards, with higher standards, gaps remain compared to commercial solutions in video aesthetics, complex motion synthesis, and artifact-free details. These challenges will drive our efforts in data curation, architecture refinement, and pipeline optimization.

\newpage
\section{Impact Statements}
This paper discusses a video generation method. Video generation, as a promising technology, also comes with significant societal risks, many of which are common to generative models in general. These risks deserve careful consideration and detailed discussion. However, the potential impacts of our method are reflective of those of the broader video generation field. Therefore, we believe there is no need to specifically highlight any particular impact in this context.