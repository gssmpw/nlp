\section{Related Work}
\label{sec:related_work}
\subsection{Video Generation}

The field of video generation has evolved rapidly, combining advances in image synthesis with the additional complexity of temporal consistency across video frames. 
Early models predominantly relied on GANs for video generation~\citep{TGAN,StyleGAN-V,MoCoGAN}, which were capable of producing videos with rich details but often suffered from unstable training and mode collapse. 
More recently, following breakthroughs in image generation and language modeling, the landscape of video generation has expanded to include diverse paradigms such as masked modeling~\citep{Phenaki,MAGVIT,MAGVIT-v2}, autoregressive modeling~\citep{CogVideo,VideoPoet,Video-LaViT,luminamgpt}, and diffusion models~\citep{PYoCo,WALT,StableVideoDiffusion,CogVideoX}. 
Among these, diffusion models have emerged as the dominant approach due to their ability to produce high-quality videos with exceptional temporal consistency, as evidenced by their adoption in state-of-the-art proprietary systems like Sora~\citep{openai2024sora}. However, the high computational cost of training diffusion-based text-to-video models remains a significant obstacle. 
In this work, we address this challenge by introducing Lumina-Video, a multi-scale diffusion-based framework, achieving remarkable results in text-to-video generation with reduced computational burden.

\subsection{Transformer-based Diffusion Models}

Early diffusion models~\citep{DDPM,DDIM} primarily relied on convolution-based U-Nets~\citep{u-net}. However, the rise of transformer architectures has revolutionized computer vision by achieving state-of-the-art performance across various tasks~\citep{vit,detr,mae}. Building on this success, diffusion transformers like DiT~\citep{DiT} and UViT~\citep{UViT} successfully adapted transformer architectures for visual generation, significantly advancing the field. This approach has since become the dominant paradigm in diffusion-based models, as demonstrated by the wide adoption in both open-source models and commercial systems~\citep{sd3,latte,pixartalpha,pixartsigma,openai2024sora}. Recent work has also explored flow matching as an alternative to standard diffusion processes, offering improved efficiency and flexibility~\citep{FlowMatching,FlowMatchingGuide,RectifiedFlow,SiT}. Notably, models like Lumina-T2X~\citep{luminat2x} and Lumina-Next~\citep{luminanext} have refined the core components of flow-based diffusion transformers, achieving remarkable performance in image synthesis. Our work builds on these advances, extending the framework to video generation. By incorporating multi-scale learning and motion-aware conditioning, we adapt and enhance diffusion transformers to efficiently capture the complex spatiotemporal dynamics of videos.

\subsection{Multi-scale Learning in Computer Vision}

The concept of multi-scale processing has long been fundamental in computer vision~\citep{Koenderink2004TheSO,Burt1983TheLP,Adelson1984PYRAMIDMI}.
With the advent of deep learning, the idea of multi-scale processing has been revitalized, yielding significant benefits for tasks that demand both high-level semantics and low-level details~\citep{FPN,deeplab,PredictingDepth,multiscaleinteractive,AttentionTosSale,AUnifiedMulti,multiscalehigh,multiscalevision,swintransformer,featurepyramidtransformer}. 
In the context of visual generation, recent advances have embraced multi-scale architectures to enhance generative processes~\citep{hierarchicalpatchdiffusion,xiaoyu2024multiscale,matryoshkadiffusion,lego,PyramidFlow}. 
Building on these foundational works, we introduce a novel Multi-scale Next-DiT architecture for video generation. Our method extends the multi-scale paradigm to the spatiotemporal domain, learning video structures across multiple levels of detail with varying patch sizes. This design not only ensures efficient training but also excels in generating high-quality, temporally coherent videos.