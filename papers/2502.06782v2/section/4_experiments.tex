\section{Training}
\label{sec:Training}
We adopt a mixture of multiple training strategies:

\textbf{\purple{Progressive Training}} has been widely recognized as an effective and efficient approach for training large-scale visual generative models~\citep{luminamgpt, luminat2x, CogVideoX, open-sora-plan, MovieGen}. Following this paradigm, \sysname{} employs a 4-stage training process, beginning with text-to-image training in the first stage and transitioning to joint text-to-image/video training in the subsequent three stages. Each stage is characterized by a predefined spatial area while allowing variable aspect ratios, ensuring that images and video frames are resized to resolutions close to the specified area while preserving their original aspect ratios. Additionally, each stage is defined with a specific frames-per-second (FPS) value. A maximum clip duration of 4s is applied throughout the entire training.

In \textbf{Stage 1}, the model is trained on pure image data at the resolution of 256 pixels. By rapidly processing a large volume of image-text pairs at high throughput, the model quickly captures the general composition and distribution of visual data, and establishes broad associations between language terms and visual concepts. We use the same image dataset as Lumina-Next~\citep{luminanext}, which shares the similar distribution as JourneyDB~\citep{sun2024journeydb}

In \textbf{Stage 2}, we incorporate the 10M official subset of the large-scale video dataset Panda~\citep{panda}. The spatial resolution remains at 256 pixels, and video frames are extracted at a target FPS of 8. \textbf{Stage 3} raises the spatial resolution to 512 pixels and the FPS to 16, and training is conducted on a mixture of data from OpenVid~\citep{openvid}, Open-Sora-Plan~\citep{open-sora-plan}, and 300k in-house video samples, which consist of diverse real and synthetic data from various sources. Finally, in \textbf{Stage 4}, the spatial resolution increases to 960 pixels, and the FPS is elevated to 24. The same dataset as Stage 3 but filtered with FPS and resolution is used in this stage.

\textbf{\purple{Image-Video Joint Training}} is employed across stages 2 to 4. Leveraging the concept richness and superior quality of image data, this joint training significantly enhances the model’s capacity to understand a broader spectrum of concepts and improves frame-level quality.

\textbf{\purple{Multi-Scale Traning}} is introduced since stage 2. For video, we define three patch sizes: \(P_1 = (1, 2, 2)\), \(P_2 = (2, 2, 2)\), and \(P_3 = (2, 4, 4)\). For image, we use the \(P_1 = (1, 2, 2)\) patchification only. We observe that applying coarser patchifications with temporal compression during image training leads to unintended effects: as images are repeated to fulfill patchification requirements, the model interprets them as \textit{silent videos}, resulting in generated samples with the interesting phenomenon of intra-patch silence and inter-patch dynamics. Scale-aware time-shift introduced in Sec.\ref{sec:scle-aware-ts} is applied throughout progressive training.

\textbf{\purple{Multi-Source Traning}} denotes a novel \textbf{multi-system-prompt training → per-system prompt evaluation → best-subset fine-tuning} strategy. When training relatively large-scale models, the available data typically comes from diverse sources with varying distributions and quality levels. Ideally, we expect the model to learn from all available samples while ensuring that the generated content during inference aligns closely with the quality of the best subset of the training data. However, this objective is challenging, especially when the complexity of data composition makes it difficult for even experienced practitioners to pinpoint which subset qualifies as optimal. 

To solve this problem, we introduce distinct system prompts tailored to each data source into progressive training, and prepend them to the image prompts, forming complete prompts for training. When performing random prompt dropping for CFG, we drop only the image-related prompts while retaining the system prompts to preserve source-specific context during training.

After completing the progressive training, we evaluate the model performance using different system prompts. This evaluation involves both subjective assessments and benchmark-based quantitative metrics. Our observations reveal that generation quality and stability exhibit significant variation across different system prompts.

Based on evaluation results, we conduct a final \textit{best-subset fine-tuning} stage at a reduced learning rate on the most effective system prompt subset. A few hundred iterations suffice to significantly enhance the model’s performance, aligning outputs with the subset’s characteristics while preserving the broad generalization achieved in earlier stages. 

Notably, we find that \textbf{synthetic data}, despite comprising only a small portion ($\sim$10\%) of our in-house dataset, consistently achieves higher scores in per-system prompt evaluation. By checking the generated samples, those from synthetic system prompts also demonstrate greater stability. These findings highlight the effectiveness of synthetic data in video generation, likely due to its simpler distribution, which prevents the model from getting confused by the erratic variability of the real world. To our knowledge, this is the first work to validate synthetic data's utility for large-scale video generation models. We believe these insights will aid research groups with limited resources in developing stronger, more efficient foundational video models.

\input{table/patchification}
\section{Evaluation}

After completing the four stages of progressive training, we select the final checkpoints from stages 3 and 4 and then perform best-subset fine-tuning at the corresponding resolution and frame rate (FPS). This process results in two final models: one with a spatial resolution of \( 512^2 \) at 16 FPS, and another with \( 960^2 \) at 24 FPS. For our quantitative and ablation experiments, we use the first model by default. Demo samples are provided in the supplementary zip file.
\subsection{Comparison with Existing Methods}

\paragraph{Evaluation Benchmark} We quantitatively evaluate the performance of~\sysname{} on VBench~\citep{vbench}, a comprehensive benchmark for text-to-video generation. VBench consists of 16 fine-grained metrics from two primary dimensions including video quality (depicted by quality score) and video-text alignment (depicted by semantic score). During inference, we uniformly sample 70 timesteps and apply a shifting value $\alpha=8.0$. 

We present the quantitative results on VBench in Table~\ref{tab:vbench}, comparing \sysname{} against both proprietary and open-source models. The results demonstrate that \sysname{} is highly competitive overall, performing well in generating high-quality videos while effectively understanding and following user prompts. Detailed results for individual metrics are provided in Table~\ref{tab:vbench_detail} in the appendix.

\subsection{Ablation Study}

\input{table/motion_score}

\subsubsection{Multi-scale Patchification}

Our experimental results in Tab.~\ref{tab:patchification} and Fig.~\ref{fig:patch-size} show that using the smallest patch size throughout the entire process yields the highest overall performance, while larger patch sizes reduce model performance but raise generation speed. Compared to using a single patch size, our patchification stitching-based inference achieves a better efficiency-quality balance. further supporting the relationship between prediction quality, patch size, and timestep illustrated in Fig.~\ref{fig:loss-bin}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/pyramid_ablation.pdf}
    \vspace{-2em}
    \caption{Comparison of generated videos using different patchification strategies.}
    \label{fig:patch-size}
    \vspace{-1em}
\end{figure}

\subsubsection{Motion score}
\label{sec:exp_motion}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/motion_ablation.pdf}
    \vspace{-2em}
    \caption{Comparison of generated videos using different positive and negative motion scores.}
    \label{fig:motion}
\end{figure}

\sysname{} has introduced motion score as a micro-condition for controlling dynamics. However, as shown in Tab.~\ref{tab:motion_score}, increasing motion conditioning fails to enhance dynamics if the conditioning for CFG negative sample rises simultaneously: setting motion to 4 yields low dynamics, while increasing it to 8 provides only modest improvement.

\textit{This suggests the possibility that high dynamics depends on the difference between positive and negative motion conditioning, not their absolute values}. To verify this, we fixed the negative motion at 2 and observed higher dynamics with [4,2] compared to [4,4], and further improvement with [8,2]. These results confirm motion difference as an effective control for video dynamics.

However, increasing this difference degrades content quality, as reflected in quality and semantic scores. To balance dynamics and quality, we propose initially setting the negative motion to a low value (e.g., 2) and aligning it with positive motion after a threshold (e.g., 0.05). This approach, based on the assumption that motion structures form earlier than fine details in diffusion, achieves an optimal balance, as shown in rows 5–6 in Tab.~\ref{tab:motion_score}. A visualization of motion condition's impact is shown in Fig.~\ref{fig:motion}