\section{Related works}
\label{sec:Related works}
\noindent \textbf{Deep Learning based Recommendation Models.}
Traditional DLRMs typically employ a deep neural network and can be broadly categorized into two primary classes:
(1) Retrieval-Oriented Models: These models primarily focus on retrieving the most relevant items for users. 
They often utilize a two-tower architecture, such as EDB \cite{huang2020embedding}, or sequential models, like SASRec \cite{kang2018self}, BERT4Rec \cite{sun2019bert4rec}, and GRU4Rec\cite{hidasi2015session} to effectively capture the correlations between users and items.
(2)
Click-Through Rate (CTR) Prediction Models: This category of models is designed to predict the click-through rate. 
Many of these models adopt an Embedding + Multi-Layer Perceptron (MLP) architecture. 
Some works, such as DeepFM \cite{guo2017deepfm} and xDeepFM \cite{lian2018xdeepfm}, aim to build more complex interaction signals, while others, including DIN \cite{zhou2018deep}, DIEN \cite{zhou2019deep}, SIM \cite{pi2020search} and TWIN \cite{chang2023twin}, explore the significance of UBS.
Despite the significant improvements achieved by these methods in RecSys, they generally fail to scale with increasing computational resources and do not fully leverage the advancements in large foundation models.


\noindent \textbf{Generative Recommendation Models.}
To investigate the scaling laws, several studies have been conducted to emulate LLM architectures and construct autoregressive transformers through next-item prediction in an end-to-end manner \cite{zhai2024actions,geng2022recommendation}. 
However, as discussed in Section \ref{sec:Introduction}, these approaches often make strong and idealized assumptions that overlook the advantages of traditional DLRMs in terms of in features, architecture, and practices. 
This results in a series of issues and limitations when these models are applied to industrial settings.
In this paper, we propose a three-step paradigm with LUM to address these limitations. 
It is worth noting that there are also works that attempt to leverage the open-world knowledge in LLMs to build content-based recommendation models via end-to-end (E2E) or multi-step training \cite{bao2023tallrec, lin2024clickprompt, yu2024ra, chen2024hllm}. These efforts, however, fall outside the scope of our current study. 
Our focus is on developing a scalable model that captures collaborative filtering signals rather than content signals.

\vspace{-1em}