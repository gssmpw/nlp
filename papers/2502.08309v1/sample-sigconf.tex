%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}

% \settopmatter{printacmref=false, printccs=false}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
% \AtBeginDocument{%
%   \providecommand\BibTeX{{%
%     Bib\TeX}}}

% %% Rights management information.  This information is sent to you
% %% when you complete the rights form.  These commands have SAMPLE
% %% values in them; it is your responsibility as an author to replace
% %% the commands and values with those provided to you when you
% %% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
% %% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
% %%
% %%  Uncomment \acmBooktitle if the title of the proceedings is different
% %%  from ``Proceedings of ...''!
% %%
% %%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
% %%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\usepackage{multirow}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{relsize}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}
% \fancyhead{}
%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.



\title{Unlocking Scaling Law in Industrial Recommendation Systems with a Three-step Paradigm based Large User Model}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}
\author{
 % Bencheng Yan$^{*}$, Pengjie Wang$^{*}$, Jinquan Liu, Wei Lin, Kuang-Chih Lee, Jian Xu and Bo Zheng$^{\dagger}$
 Bencheng Yan$^{*}$, Shilei Liu$^{*}$, Zhiyuan Zeng$^{*}$, Zihao Wang, Yizhen Zhang, Yujin Yuan, Langming Liu, Jiaqi Liu, Di Wang, Wenbo Su, Wang Pengjie, Jian Xu and Bo Zheng$^{\dagger}$
 }
  \affiliation{%
  \institution{Alibaba Group}
  \country{China} \\
   \texttt{\{bencheng.ybc,liushilei.lsl,zengzhiyuan.zzy,wzh454725,zhangyizhen.zyz,yujin.yyj,}\\
\texttt{liulangming.llm,ljq414468,zhemu.wd,vincent.swb,pengjie.wpj,xiyu.xj,bozheng\}@alibaba-inc.com}\\
}
 % \email{{bencheng.ybc,liushilei.lsl,zengzhiyuan.zzy,wzh454725,zhangyizhen.zyz,yujin.yyj,
 % liulangming.llm,ljq414468,zhemu.wd,vincent.swb,pengjie.wpj,xiyu.xj,bozheng}@alibaba-inc.com}

 \thanks{$*$ These authors contributed equally to this work and are co-first authors}
 \thanks{$\dagger$ Corresponding author}
% \vspace{-1em}
%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\renewcommand{\shortauthors}{Bencheng Yan, et al.}


\begin{abstract}
Recent advancements in autoregressive Large Language Models (LLMs) have achieved significant milestones, largely attributed to their scalability, often referred to as the "scaling law".
Inspired by these achievements, there has been a growing interest in adapting LLMs for Recommendation Systems (RecSys) by reformulating RecSys tasks into generative problems. 
However, these End-to-End Generative Recommendation (E2E-GR) methods tend to prioritize idealized goals, often at the expense of the practical advantages offered by traditional Deep Learning based Recommendation Models (DLRMs) in terms of in features,
architecture, and practices. 
This disparity between idealized goals and practical needs introduces several challenges and limitations, locking the scaling law in industrial RecSys.
In this paper, we introduce a large user model (LUM) that addresses these limitations through a three-step paradigm, designed to meet the stringent requirements of industrial settings while unlocking the potential for scalable recommendations. Our extensive experimental evaluations demonstrate that LUM outperforms both state-of-the-art DLRMs and E2E-GR approaches. Notably, LUM exhibits excellent scalability, with performance improvements observed as the model scales up to 7 billion parameters. Additionally, we have successfully deployed LUM in an industrial application, where it achieved 
significant gains in an A/B test, further validating its effectiveness and practicality.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
% \keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
%   Your, Paper}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\begin{figure*}[t]
\centering
\includegraphics[width = .85\textwidth]{intro_example.pdf}
\vspace{-1em}
\caption{Intuitive insight from the common paradigm in using LLM to the proposed \textbf{multi-step, generative-to-discriminative} paradigm.}
\vspace{-1em}
\label{figure:The comparison cases between LLM and LUM.}
\end{figure*}

\vspace{-1em}
\section{Introduction}
\label{sec:Introduction}
In recent years, autoregressive large language models (LLMs) have achieved significant breakthroughs, primarily due to their adherence to the principle of scalability, often referred to as the "scaling law".
This law posits that there is a predictable and continuous improvement in model performance as the computational resources allocated to the model are increased\cite{radford2018improving,radford2019language}.

In parallel, the field of recommendation systems (RecSys) has been actively investigating the potential for leveraging similar scalability principles.
Early research has highlighted that traditional deep learning-based recommendation models (DLRMs), widely used in industry, do not exhibit the same level of scalability observed in LLMs\cite{zhai2024actions}.
This discrepancy can be attributed to the fundamental differences between generative models and discriminative models. 
Specifically, generative models require a more substantial capacity to effectively capture the complex joint probability distribution 
$p(x,y)$ of the data, whereas discriminative models, such as traditional DLRMs, focus on modeling the simpler conditional probability 
$p(y|x)$. 
As a result, the benefits of increased computational resources are less pronounced in discriminative models \cite{ding2024inductive,liu2024multi,zhai2024actions}.

% Therefore, there are some efforts to emulate GMs' style by redefining the retrieval and ranking tasks in RecSys as generative tasks and then training a transformer architecture via User Behavior Sequences (UBS) in an end to end manner \cite{zhai2024actions,geng2022recommendation}.
% However, these E2E Generative Recommendation  methods (E2E-GRs) overemphasize ideal states, completely abandoning the advantages in features, architecture, and practices by traditional DLRMs. 
% This disconnection from practice in modeling has led to a series of problems and limitations:

Consequently, several studies have endeavored to mimic the generative models approach by reformulating the retrieval and ranking tasks within RecSys as generative tasks. 
This is achieved through utilizing User Behavior Sequences (UBS) as the corpus and training a transformer architecture on a "next-item prediction" task in an end-to-end fashion \cite{zhai2024actions,geng2022recommendation}. 
However, these end-to-end Generative Recommendation methods (E2E-GRs) tend to overemphasize idealized goals, thereby neglecting the inherent strengths in features, architecture, and practices of DLRMs. 
This gap between idealized goals and practical implementation in the modeling paradigm has subsequently given rise to a series of challenges and limitations.


% \noindent \textbf{(1) The inconsistency between generative training and discriminative application:} 
% Although E2E-GRs excel at understanding and reproducing the complex patterns and distributions of data, when it comes to specific discriminative applications (e.g., predictive tasks where order and precision are crucial), E2E-GRs may fall short.
% This is because GMs focus more on the 'how' (the process of generating data) rather than the 'what' (precise predictions).

\noindent \textbf{(1) Inconsistency Between Generative Training and Discriminative Application:}
Despite their proficiency in capturing the intricate patterns and distributions inherent in data, E2E-GRs often exhibit limitations when applied to specific discriminative tasks, such as click-through rate prediction where both calibration ability and ranking ability are paramount \cite{DBLP:conf/kdd/YanQWBN22}. 
This disparity stems from the intrinsic focus of generative models on the process of data generation rather than on the precise predictive outcomes. 
As a result, although generative models are adept at capturing inherent distributions of data, they may not consistently satisfy the rigorous demands of discriminative applications, which require high levels of accuracy and specificity \cite{bernardo2007generative,ng2001discriminative,yogatama2017generative}.

% \noindent \textbf{(2) Efficiency issues:} 
% the demand for high efficiency in real-time training and the constraints of low latency (not only throughput) in online real-time inference  make it challenging to directly implement E2E-GRs in industrial applications.
% Even if they are hardly deployed, the efficiency constraints would still limit the scaling up of these E2E models.

\noindent \textbf{(2) Efficiency Challenges:}
% The stringent requirements for high efficiency in real-time training, coupled with the imperative need for low latency (beyond mere throughput considerations) in online, real-time inference, pose significant impediments to the direct deployment of End-to-End Generative Representations (E2E-GRs) within industrial contexts. 
% Despite the potential benefits, the practical implementation of such models is fraught with difficulties. 
% Moreover, even when deployment is feasible, the inherent efficiency constraints continue to restrict the scalability of these E2E models, thereby hindering their broader application and impact across various industries. 
% This conundrum underscores the critical need for innovative solutions that can address the dual challenges of efficiency and scalability in real-world settings, particularly for E2E architectures.
% The imperative for high efficiency in continuous streaming training, coupled with the stringent requirements for low latency (beyond mere throughput mentioned in \cite{zhai2024actions}) in online real-time inference, poses significant obstacles to the direct implementation of E2E-GRs in industrial settings. 
% Even when deployment is feasible, the efficiency constraints continue to impede the scalability of these E2E models (Section \ref{sec:Efficiency Evaluation}).
The demand for high efficiency in continuous streaming training, coupled with the stringent low-latency requirements for online real-time inference (beyond the throughput considerations mentioned in \cite{zhai2024actions}), presents significant challenges for the direct implementation of E2E-GRs in industrial settings. Even when deployment is feasible, these efficiency constraints continue to impede the scalability of these E2E models (Section \ref{sec:Efficiency Evaluation}).

% \noindent \textbf{(3) Lack of flexibility:} 
% E2E-GRs struggle to adapt flexibly to the ever-changing business requirements in practice, such as the desire to add a new type of behavior item (e.g., adding refund behaviors, new scenario behaviors). 
% For an E2E-GR, since the change in the sequence's schema itself necessitates retraining the entire model, this poses a significant challenge.
% In essence, the rigidity of E2E-GRs when it comes to incorporating new elements or modifying existing ones can be a major drawback. This is because any alteration to the input schema requires the model to be retrained from scratch. 
% This process can be time-consuming and resource-intensive, making it less ideal for industrial applications. 

\noindent \textbf{(3) Lack of Flexibility:}
E2E-GRs exhibit a notable limitation in their ability to adapt to the dynamic and evolving nature of business requirements. Specifically, the incorporation of new types of behavioral data, such as refund behaviors or behaviors associated with new scenarios, presents a significant challenge. 
The inherent structure of E2E-GRs necessitates that any modification to the input schema, including the addition of new elements, triggers a requirement for retraining the entire model. 
This rigidity not only complicates the integration of new features but also imposes substantial constraints on the system's adaptability. 
The need to retrain the model from scratch upon any schema alteration is both time-consuming and resource-intensive, thereby diminishing the practicality of E2E-GRs in industrial settings where rapid and flexible responses to changing conditions are often critical.

% \noindent \textbf{(4) Poor compatibility:}
% E2E-GRs are build on raw UBS with a transformer architecture.
% It means, they exist poor compatibility to utilize this pre-existing wealth of information in industrial, including explicit feature engineering and inheriting DLRMs' parameters.
% As a result, there is often a discrepancy or "gap" between the performance of these E2E-GRs in development and their actual performance when deployed in industrial settings. 

\noindent \textbf{(4) Limited Compatibility:}
E2E-GRs are constructed using raw UBS with a transformer architecture.
This approach inherently limits their compatibility with pre-existing industrial knowledge, such as explicit feature engineering and the parameter inheritance from DLRMs. 
Consequently, a significant performance gap often emerges between the developmental stages of these E2E-GRs and their real-world deployment in industrial settings (Section \ref{sec:Effectiveness Evaluation}).
This gap is particularly pronounced in applications where the online model has been refined over several years or even decades. 
This discrepancy underscores the need for more robust integration strategies to bridge the gap between theoretical advancements and practical applications.


% Therefore, in this paper, we reconsider the question: \textbf{How can we effectively embrace generative model and build scalability in the industrial setting?}
% The question drives us to review the common use cases of LLMs: 
% Firstly, the LLM is generative pre-trained on massive datasets to absorb extensive knowledge. 
% Subsequently, users gives different prompts and query the LLM with a wide range of questions.
% Ultimately, users rely on the LLM's responses to make informed judgments about various matters.
% This \textbf{multi-step}, \textbf{generative-to-discriminative} nature suggests a paradigm to utilize generative model.
% Inspired by this finding, we propose a three-step paradigm to build large user model (LUM) for industrial applications.
% Basically, these three steps can be characterized as (see Figure \ref{figure:The comparison cases between LLM and LUM.}):
% (1) \textbf{Step 1: Build knowledge.} 
% a large user model is firstly proposed, which leverages transformer architecture and are pre-trained via generative learning, to characterize the knowledge about user interests and the collaborative information among items.
% (2) \textbf{Step 2: Query knowledge.}
% In Step 2, we can ask LUM any pre-defined questions related to the user.
% (3) \textbf{Step 3: Utilize knowledge.}
% We take the responses of LUM in step 2 as additional features and add them into the traditional DLRMs.

In this paper, we rethink the critical question: \textbf{How can we effectively harness generative models to unlock scaling law in industrial settings?} This question compels us to revisit the common paradigm in using LLMs (Figure \ref{figure:The comparison cases between LLM and LUM.}).
Specifically, LLMs are initially trained in a generative manner on vast datasets, enabling them to assimilate a broad spectrum of knowledge.
End-users then interact with these models through diverse prompts, querying them across a wide array of topics. 
Ultimately, the responses generated by LLMs are utilized by end-users to make informed decisions across multiple domains.
This \textbf{multi-step, generative-to-discriminative} nature of LLMs provides a foundational framework for leveraging generative models in practical applications. 
Drawing upon this insight, we propose a three-step paradigm for training a Large User Model (LUM) tailored for industrial use. 
These steps are outlined as follows (see Figure \ref{figure:The comparison cases between LLM and LUM.}):
(1) \textbf{Step 1: Knowledge Construction.} 
A LUM is introduced, utilizing a transformer architecture and pre-trained through generative learning. 
This model captures user interests and the collaborative relationships among items, thereby characterizing a comprehensive knowledge base.
(2) \textbf{Step 2: Knowledge Querying.}
In this phase, LUM is queried with predefined questions pertaining to user-specific information, facilitating the extraction of relevant insights.
Intuitively, this process can be fundamentally conceptualized as a form of "prompt engineering" which is specifically designed to elicit an extensive knowledge.
(3) \textbf{Step 3: Knowledge Utilization.}
The outputs from LUM, obtained in Step 2, serve as supplementary features. 
These are integrated into traditional DLRMs to enhance their predictive accuracy and decision-making capabilities.

% This structured approach not only aligns with the inherent strengths of generative models but also optimizes their deployment in industrial contexts, thereby addressing the initial inquiry regarding scalability and effectiveness.
Generally speaking, we can benefit the three-step paradigm from 
(1) 
% \texbf{Exploration of Scaling Law:} 
In the first step, the generative learning of LUM enables the exploration of scaling laws, which are critical for enhancing model performance.
(2) 
% \textbf{Efficiency through Decoupling:} 
The decoupled designed of our paradigm eliminates the constraints associated with continue streaming training or serving. 
This separation facilitates the implementation of caching strategies for the LUM, thereby mitigating efficiency limitations.
(3) The third step ensures that DLRMs can meet the requirements for real-time learning, flexibility, and compatibility. 
This is achieved by integrating the previously learned LUM with DLRMs, thereby enhancing their adaptability to dynamic environments and ensuring seamless integration with existing systems.


The remaining challenge is to effectively transfer the learned data joint distribution $p(x,y)$ from the first step to downstream discriminative tasks, thereby addressing the aforementioned limitation. 
Ideally, the user knowledge encapsulated within LUM should exhibit a substantial correlation with the discriminative tasks at hand. 
To achieve this, we introduce a novel tokenization strategy for UBS, wherein each item is expanded into two distinct tokens: a condition token and an item token (Figure \ref{figure:An example of tokenization} and Section \ref{sec:Tokenization}).
Subsequently, we redefine the autoregressive learning process of UBS from \textit{"next-item prediction"}  to \textit{"next-condition-item prediction"}. 
This reformulation enables us to seamlessly \textbf{trigger} the relevant knowledge from the LUM into the discriminative tasks by specifying various conditions during the second step of the process. 
% In other words, with the condition token, we \textbf{trigger} conditional knowledge  from LUM.
% Besides, we also highlight that such a tokenization can 
Finally, we have applied this approach to both offline datasets and online industrial applications, achieving significant improvements (Section \ref{sec:Performance on  Recommendation tasks} and \ref{sec:Online Results}).
Furthermore, the LUM demonstrates scaling properties similar to those observed in LLMs, allowing it to be successfully scaled up to 7 billion parameters while maintaining consistent performance enhancements. 
These findings underscore the robustness and adaptability of the LUM in diverse industrial application scenarios.

In summary, our contribution are summarized as follows:

\noindent $\bullet$ 
We introduce a pioneering three-step paradigm specifically tailored for industrial applications. 

\noindent $\bullet$ 
We propose a large user model (LUM) that incorporates a "next-condition-item prediction" task, to bridge the gap between generative pre-training and discriminative applications.

\noindent $\bullet$ 
Comprehensive empirical evaluations demonstrate that the proposed three-step paradigm based LUM significantly enhances the performance of downstream tasks in both public and industrial settings. 
Notably, our experiments reveal a clear scaling law governing the performance of LUM, which underscores the importance of scaling up the model size for optimal results. 
Additionally, we have successfully deploy it on our industrial applications and achieve significant gains in an A/B test.
This observation not only validates the effectiveness of our approach but also serves as a catalyst for further exploration and advancement in this research direction.


% LUM优化细节：比如packing
% 性能优化，预计算
% 上游的Recall到下游的AUC
% 多模态item embedding
% LLM引入特征也是多阶段，和本文的区别
\vspace{-1em}
\section{Related works}
\label{sec:Related works}
\noindent \textbf{Deep Learning based Recommendation Models.}
Traditional DLRMs typically employ a deep neural network and can be broadly categorized into two primary classes:
(1) Retrieval-Oriented Models: These models primarily focus on retrieving the most relevant items for users. 
They often utilize a two-tower architecture, such as EDB \cite{huang2020embedding}, or sequential models, like SASRec \cite{kang2018self}, BERT4Rec \cite{sun2019bert4rec}, and GRU4Rec\cite{hidasi2015session} to effectively capture the correlations between users and items.
(2)
Click-Through Rate (CTR) Prediction Models: This category of models is designed to predict the click-through rate. 
Many of these models adopt an Embedding + Multi-Layer Perceptron (MLP) architecture. 
Some works, such as DeepFM \cite{guo2017deepfm} and xDeepFM \cite{lian2018xdeepfm}, aim to build more complex interaction signals, while others, including DIN \cite{zhou2018deep}, DIEN \cite{zhou2019deep}, SIM \cite{pi2020search} and TWIN \cite{chang2023twin}, explore the significance of UBS.
Despite the significant improvements achieved by these methods in RecSys, they generally fail to scale with increasing computational resources and do not fully leverage the advancements in large foundation models.


\noindent \textbf{Generative Recommendation Models.}
To investigate the scaling laws, several studies have been conducted to emulate LLM architectures and construct autoregressive transformers through next-item prediction in an end-to-end manner \cite{zhai2024actions,geng2022recommendation}. 
However, as discussed in Section \ref{sec:Introduction}, these approaches often make strong and idealized assumptions that overlook the advantages of traditional DLRMs in terms of in features, architecture, and practices. 
This results in a series of issues and limitations when these models are applied to industrial settings.
In this paper, we propose a three-step paradigm with LUM to address these limitations. 
It is worth noting that there are also works that attempt to leverage the open-world knowledge in LLMs to build content-based recommendation models via end-to-end (E2E) or multi-step training \cite{bao2023tallrec, lin2024clickprompt, yu2024ra, chen2024hllm}. These efforts, however, fall outside the scope of our current study. 
Our focus is on developing a scalable model that captures collaborative filtering signals rather than content signals.

\vspace{-1em}
\section{Preliminary}
\subsection{Traditional DLRMs}
\label{sec:Traditional Deep Learning Methods in RecSys}
In the domain of RecSys, two primary tasks are identified: retrieval and ranking.
For the purpose of this discussion, we will focus on a search scenario, though it is important to note that the paradigm proposed herein is equally applicable to other industrial applications.
Given a user $u \in \mathcal{U}$, an item $i \in \mathcal{I}$, a search term $s \in \mathcal{S}$:

\noindent \textbf{Retrieval task:}
This task is aimed at identifying a subset of candidate items from the corpus that align with the user's $u$'s preferences, as influenced by the query $s$. 
A conventional approach to the retrieval task employs a two-tower architecture \cite{huang2020embedding}, comprising a user-query tower $\textit{UEnc}$ and an item tower $\textit{IEnc}$. 
These towers may consist of any suitable neural network structures, such as MLPs. 
The user-query tower encodes the user $u$ and the search term $s$ into a unified embedding $e^r_{us}=\textit{UEnc}(us)$, while the item tower generates an embedding for the item $i$ denoted as $e^r_i=\textit{IEnc}(i)$. 
Subsequently, contrastive learning techniques are utilized to refine the representations produced by these two towers.

\noindent \textbf{Ranking Task:} 
In contrast, the ranking task focuses on forecasting the likelihood of a user $u$ clicking on an item $i$ in response to a specific query $s$.
This is mathematically formalized as:
$\hat{y}=f(u,i,s)$ where $\hat{y}$ represents the predicted CTR and $f$ refers to the Embedding+MLP architecture.

\subsection{E2E-GRs via Next-item Prediction}
In the context of E2E-GRs, given a UBS for user $u$ represented as $B_u=\{i_1,i_2,...,i_L\}$ where $L$ denotes the length of the behavior sequence, the next-item prediction framework posits that the probability of an item $i_k$ appearing next is conditionally dependent on the preceding items $\{i_1,i_1,i_2,...,i_{k-1}\}$.
Consequently, the likelihood of the entire UBS $B_u$ can be mathematically expressed as:
$p(i_1,i_2,...,i_L) = \prod_{l=1}^{L} p(i_l|i_1,i_2,...,i_{l-1})$.
% \begin{align}
% \label{equ:next-item prediction}
% p(i_1,i_2,...,i_L) = \prod_{l=1}^{L} p(i_l|i_1,i_2,...,i_{l-1})
% \end{align}
The objective of the autoregressive learning process within E2E-GRs is to optimize the distribution $p_{\theta}(i_1,i_2,...,i_L)$, a method commonly referred to as "next-item prediction".



\begin{figure}[t]
\centering
\includegraphics[width = .3\textwidth]{tokenization.pdf}
% \vspace{-1em}
\caption{The comparison of different tokenizations}
% \vspace{-1em}
\label{figure:An example of tokenization}
\end{figure}


\begin{figure*}[t]
\centering
\includegraphics[width = .85\textwidth]{pretrain.pdf}
\vspace{-1em}
\caption{
(a) The architecture of LUM. 
(b) An example of query knowledge from pre-trained LUM.
(c) An example of utilizing knowledge in DLRMs.}
\vspace{-1em}
\label{figure:The architecture of LUM.}
\end{figure*}

\section{Method}
\label{sec:Method}
\subsection{Step 1: Knowledge Construction via Pre-training LUM}
\subsubsection{Tokenization}
\label{sec:Tokenization}
In this study, we propose a novel approach to reformulating autoregressive modeling in UBS by transitioning from a traditional "next-item prediction" framework to a more sophisticated "next-condition-item prediction" paradigm. 
Specifically, each item $i$ within the sequence $B_u$ is decomposed into two distinct tokens: a condition token and an item token. Consequently, the UBS can be represented as a sequence of alternating tokens: $\{c_1,i_1,c_2,i_2,...,c_l,i_l\}$, where $c_k$ denotes the condition token associated with the item $i_k$ (as illustrated in Figure \ref{figure:An example of tokenization}).
To elucidate, consider a practical scenario where a user's behavior encompasses interactions with items across multiple scenarios, such as recommendation and search scenarios. In this context, the condition token $c_i$ can be defined as a scenario token, thereby capturing the specific environment in which the item $i$ was encountered. 
This approach facilitates a nuanced understanding of user preferences and behaviors across different scenarios.
It is important to note that while other methods, such as HSTU \cite{zhai2024actions}, have introduced additional action tokens, their methodology differs fundamentally from our proposed framework. 
In HSTU, each item is extended as <item, action> where the action $a_k$ is associated with the preceding item $i_k$, rather than the subsequent item (Figure \ref{figure:An example of tokenization}). 
As a result, HSTU's strategy is limited in its ability to predict the next item based on varying conditions and fails to adequately capture user preferences across different aspects.

\vspace{-1em}
\subsubsection{Architecture}
\label{sec:Architecture}
The overarching framework of LUM is depicted in Figure \ref{figure:The architecture of LUM.} (a), presenting a hierarchical structure that encompasses both a Token Encoder and a User Encoder.

\noindent \textbf{Token Encoder.}
The input tokens for LUM exhibit heterogeneity, characterized by two primary categories: condition token $c$ and item token $i$. Moreover, individual tokens may carry diverse attribute features, such as ID, statistical, and content features associated with item tokens. 
The token encoder is devised to integrate these heterogeneous inputs into a unified token embedding. 
This process involves initially concatenating the various features or embeddings of each token, followed by a projection layer to consolidate and transform these features into a common representational space. 
Mathematically, this transformation is formalized as:
% \begin{align}
$e^t = \textit{proj}^t(\textit{concat}(f^t_1;f^t_2;f^t_3;...)); t \in \{i,c\}$
% \end{align} 
where $f^t_k$ refers to the features of item token $i$ or condition token $c$, and $e^t$ denotes the resultant token embedding.
In this study, a linear projection is employed for $proj^t$ unless otherwise specified.

\noindent \textbf{User Encoder:}
The user encoder is structured to capture user preferences and the collaborative information among items.
Specifically, the sequence of input tokens $\{c_1,i_1,c_2,i_2,...,c_L,i_L\}$ is represented as $\{e^c_1,e^i_1,e^c_2,e^i_2,...,e^c_L,e^i_L\}$ through the token encoder.
% transformed by the token encoder into corresponding  embeddings $\{e^c_1,e^i_1,e^c_2,e^i_2,...,e^c_L,e^i_L\}$.
Subsequently, as illustrated in Figure \ref{figure:The architecture of LUM.} (a), the user encoder utilizes a conventional autoregressive transformer architecture to process these embeddings. 
The final output of the user encoder is denoted as $o^c_k$, encapsulating the integrated information from the input sequence.

\subsubsection{Next-condition-item Prediction}
% Different from next-item prediction, since we focus on predict next item rather than next condition, next-condition-item prediction only needs to apply autoregressive loss on the output of condition token to predict next item.
% Thus, the corresponding autoregressive likelihood in Equ \ref{equ:next-item prediction} can be written as:
Unlike next-item prediction, which focuses on predicting the subsequent item directly, next-condition-item prediction is concerned with predicting the next item given a specific condition. 
This approach necessitates the application of an autoregressive loss solely on the output of the condition token to infer the next item. Consequently, the autoregressive likelihood for next-condition-item prediction can be formulated as follows:
\vspace{-1em}
\begin{align}
\label{equ:next-condition-item prediction}
p(c_1,i_1,c_2,i_2,...,c_L,i_L) = \prod_{l=1}^{L} p(i_l|c_1,i_1,c_2,i_2,...,i_{l-1},c_l)
\end{align}
Furthermore, to enhance the optimization of $p_{\theta}(c_1,i_1,c_2,i_2,...,c_L,i_L)$ in practical industrial applications, we employ the InfoNCE loss function \cite{oord2018representation} and introduce a packing strategy. 

\noindent \textbf{InfoNCE Loss.} 
In industrial applications, the vocabulary size of items can scale to billions, rendering the direct computation of generative probabilities over the entire set of items impractical. 
To address this challenge, we employ the InfoNCE  loss for predicting the next conditional items. 
The InfoNCE loss can be mathematically formulated as follows:
\begin{align}
\label{equ:infonce}
Loss = -\sum_{l=1}^{L} \log \left( \frac{\exp(sim(o^c_{l-1}, e^i_{l}) ))}{\exp(sim(o^c_{l-1}, e^i_{l})+\sum_{k=1}^{K}\exp(sim(o^c_{l-1}, e^i_{k}) ))} \right)
\end{align} where $sim$ is the similarity function.
For each item $l$, the other items within the same batch serve as negative samples and $K$ is the number of negative items.
$e^i_{k}$ is the embedding of $k$-th negative items.



\noindent \textbf{Packing.}
In practical applications, the lengths of UBSs exhibit significant variability among users. 
Indeed, the majority of UBS lengths are substantially shorter than the predefined maximum length. 
Processing each UBS individually in such scenarios is computationally inefficient. 
Drawing inspiration from the packing strategies employed in the GPT series \cite{radford2018improving, radford2019language, zhao2024analysing}, we adopt a similar approach by grouping multiple UBSs into a single sequence, thereby maximizing the utilization of the available sequence length.

\subsection{Step 2: Knowledge Querying with Given Conditions}
\label{sec:Step 2: Knowledge Querying with Given Conditions}
In the Step 1, we construct the joint probability distribution denoted as $p(c_1,i_1,c_2,i_2,...,c_L,i_L)$.
The next step involves extracting related knowledge from this established probability.
Specifically, the tokenization methodology introduced in Section \ref{sec:Tokenization} facilitates the knowledge querying under various conditions. 
Given a query condition $c_q$, the conditional probability $p(i_q|c_1,i_1,c_2,i_2,...,c_L,i_L,c_q)$ can be computed to ascertain the likelihood of a user's interest in item $i_q$ (see Figure \ref{figure:The architecture of LUM.} (b)).
It is noteworthy that this approach to trigger conditional knowledge establishes a bridge between generative models and discriminative tasks, thereby enhancing effectiveness in industrial applications.
The following examples illustrate the application of different condition tokens:

% \vspace{-1em}
% \begin{example}
\textit{Example 1.} When the condition token corresponds to a scenario token, the model can infer user interests across different scenarios.

% \end{example}
% \vspace{-1em}
% \begin{example}
\textit{Example 2.} If the condition token represents a search query token within search scenarios, the model can deduce user interests based on varying search queries.

% \end{example}
% \vspace{-1em}
% \begin{example}
\textit{Example 3.} In cases where the condition token signifies a category token, the model can ascertain user interests in different categories.

% \end{example}
% \vspace{-1em}
Furthermore, the model's flexibility allows for the simultaneous consideration of multiple types of conditions by incorporating additional condition features into the set $\{f^c_1;f^c_2;f^c_3;...\}$.
Empirical evidence demonstrates that integrating diverse conditions significantly enhances performance (see Section \ref{sec:Effectiveness Evaluation}). In essence, this process can be conceptualized as a form of \textbf{"prompt engineering"} designed to elicit a wide range of knowledge.


\noindent \textbf{Group query for efficiency.}
Moreover, given that a single user may response multiple queries, each pertaining to the same UBS, processing these queries in isolation can lead to significant inefficiencies. 
This issue is exacerbated in practical scenarios where the number of users can easily scale to billions, leading to an extensive number of <user, query> pairs requiring inference. 
To mitigate this challenge, we introduce a novel group query strategy aimed at enhancing computational efficiency.
As illustrated in Figure \ref{figure:An example of group query.}, all queries are concatenated into a single sequence, represented as $p(i_{q_1},i_{q_2},..|c_1,i_1,c_2,i_2,...,c_L,i_L,c_{q_1},c_{q_2},...)$.
To ensure that the inference process remains coherent and accurate, we apply a masking mechanism to prevent attentional interactions between different query condition $c_{q_j}$.
This approach allows the common prefix $\{c_1,i_1,c_2,i_2,...,c_L,i_L\}$ of various queries to be computed only once, while simultaneously querying the items $i_{q_j}$ under different conditions.
Empirical evaluations demonstrate that, with the implementation of the group query strategy, the inference process can be significantly accelerated by 78\%(see Section \ref{sec:Effectiveness Evaluation}).



\begin{figure}[t]
\centering
\includegraphics[width = .4\textwidth]{group_query.pdf}
\vspace{-1em}
\caption{An example of group query.}
\vspace{-2em}
\label{figure:An example of group query.}
\end{figure}

% \vspace{-1em}
\subsection{Step 3: Knowledge Utilization in DLRMs}
\label{sec:Step 3: Utilize knowledge in DLRMs}
After Step 2, we acquire a set of $N$ next-condition items $\{i_{q_1},i_{q_2},..,i_{q_N}\}$, with their corresponding outputs represented as $o^i_{q_n}$.
Additionally, each item $i \in \mathcal{I}$ is encoded through the token encoder (Section\ref{sec:Architecture}), yielding an embedding denoted as $e^i_i$.

To enhance existing DLRMs, we propose two strategies (see Figure \ref{figure:The architecture of LUM.} (c)):
\noindent (1) Direct Feature Incorporation: We integrate the outputs $o^i_{q_n}$ and the embedding $e^i_i$ of target item $i$ as fixed additional features into the DLRMs.  
This approach leverages the rich representations derived from the next-condition items directly.
\noindent (2) Interest Matching via Similarity Measurement: We assess the alignment between the target item $i$ and user interests by computing the similarity $sim(o^i_{q_n},e^i_i)$ (as defined in Equ \ref{equ:infonce}). 
This similarity score quantifies how well the target item matches the context provided by the next-condition items.

Formally, for retrieval tasks, the two-tower model can be reformulated as: $e^r_{us}=\textit{UEnc}(us,\{o^i_{q_1},o^i_{q_2},..,o^i_{q_N}\})$ and $e^r_i=\textit{IEnc}(i,e^i_i)$.
For ranking tasks, the ranking model can be rewritten as: $\hat{y}=f(u,i,s,\{o^i_{q_n},sim(o^i_{q_n},e^i_i)|n=1,..,N\},e^i_i\}$.
This framework effectively integrates contextual information and item embeddings to improve both retrieval and ranking performance in recommendation systems.





% \noindent \textbf{Retrieval.}
% We can direct take $\{i_{q_1},i_{q_2},..,i_{q_N}\}$ as additional feature into $UEnc$ (described in Section \ref{sec:Traditional Deep Learning Methods in RecSys})


% \noindent \textbf{Ranking}

% \noindent \textbf{Industrial setting}
\vspace{-1em}
\subsection{Discussion}
\label{sec:Discussion}
The proposed three-step paradigm based on LUM addresses the four limitations mentioned in Section \ref{sec:Introduction} as follows:

\noindent $\bullet$ \textbf{Addressing Limitations 1:}
% Intuitively, we design a generative-to-discriminative process to address this limitation.
% The joint distribution $p(c_1,i_1,c_2,i_2,...,c_L,i_L)$ is constructed through generative learning in Step 1.
% Then the design of 'next-condition-item prediction' allows us can trigger relevant knowledge to the RecSys tasks.
% Finally,  discriminative learning with the trigger knowledge to meet the requirements of the discriminative applications.
To address the first limitation, we design a generative-to-discriminative process. 
In Step 1, the joint distribution $p(c_1,i_1,c_2,i_2,...,c_L,i_L)$ is constructed through generative learning. 
Subsequently, the design of 'next-condition-item prediction' enables the triggering of relevant knowledge for RecSys tasks. 
Finally, discriminative learning in Step 3 is performed to meet the requirements of discriminative applications.

\noindent $\bullet$ \textbf{Addressing Limitations 2:}
The model training and serving in Step 3 naturally support the efficiency requirements in industrial applications, as the backbone of the model follows the DLRM-style architecture. 
Additionally, the computational costs associated with Steps 1 and 2 are minimal due to the decomposed design, which allows these processes to be pre-computed and their results to be pre-stored.


\noindent $\bullet$ \textbf{Addressing Limitations 3:}
% On the one hand, the need for the dynamic and evolving nature of business requirements can be naturally addressed during step 3 as traditional DLRM does.
% On the other hand, such dynamic requirements can be unified as various condition which coincides with our design for condition token.
% It means no matter the requirements changed, we can consistently characterize it as <condition token,item token>, which gives the ability for continual training of LUM when facing new requirements.
The dynamic and evolving nature of business requirements can be naturally addressed in Step 3, similar to traditional DLRMs. 
Moreover, these dynamic requirements can be unified as various conditions, which aligns with our design for condition tokens. 
This means that regardless of changes in requirements, they can be consistently characterized as <condition token, item token>, enabling continuous training of LUM to adapt to new requirements.

\noindent $\bullet$ \textbf{Addressing Limitations 4:}
% In industrial setting, we can set the DLRM backbone in Step 3 as the online model.
% In this way, the pre-existing industrial knowledge can be easily utilized.
% Furthermore, we also can continually benefit from the developed advance DLRM.
In an industrial setting, the DLRM backbone in Step 3 can be set as the online model. 
This setup allows for the easy utilization of pre-existing industrial knowledge. 
Furthermore, the model can continuously benefit from advancements in DLRM technology.

\section{Experiments}
% In this section, we conduct experiments to answer the following questions:

% \noindent $\bullet$ \textbf{Q1:}

% \noindent $\bullet$ \textbf{Q2:}

% \noindent $\bullet$ \textbf{Q3:}

\subsection{Experimental setting}

\begin{table}[t]
\center
\vspace{-1em}
\caption{The statistic of datasets.}
\vspace{-1em}
\tiny
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
             & \textbf{\#Interaction}    & \textbf{\#User}   & \textbf{\#Item}   \\\midrule
Amazon Books & 914,014& 694,897& 686,623\\ 
MovieLen 1M (ML-1M) & 9,810,868&6,040&  3,883\\ 
MovieLen 20M (ML-20M) &  17,253,665 &138,493& 27,278 \\ 
Industrial Dataset & 4 billion & 0.1 billion & 0.1 billion  \\ 
\bottomrule
\end{tabular}
}
% \vspace{-1em}
\label{table:The statistic of datasets}
\end{table}


\noindent \textbf{Datasets.}
% In this study, we utilize three public datasets and one industrial dataset to evaluate the performance of our proposed method, LUM.
% (1) Public datasets: MovieLens (This dataset includes two subsets, MovieLens-1M and MovieLens-20M, which are widely used benchmarks in recommendation systems) and Amazon Books (This dataset contains user interactions with books on the Amazon platform).
% % We evaluate LUM on two benchmark datasets: MovieLens (including two subsets:1M and 20M) and Amazon Books.
% (2) Industrial dataset. 
% This dataset is collected from the Taobao e-commerce platform. 
% Each instance represents a user clicking (or viewing) an item after searching for a query on the platform. The dataset consists of 4 billion instances and  0.1 billion users.
% The statistical information for these datasets is summarized in Table\ref{table:The statistic of datasets}
In this study, we utilize three public datasets and one industrial dataset to evaluate the performance of our proposed method, LUM.
The public datasets include two benchmark datasets: MovieLens, which comprises two subsets (1M and 20M), and Amazon Books \cite{zhou2019deep,zhai2024actions}. 
The industrial dataset is sourced from the Taobao e-commerce platform.
% , where each instance represents a user clicking or viewing an item after searching for a query on the platform. 
% This dataset consists of 4 billion instances and 0.1 billion users. 
The statistical information for these datasets is summarized Table \ref{table:The statistic of datasets}.
% in Appendix \ref{sec:Datasets}.
% Table \ref{table:The statistic of datasets}.

\noindent \textbf{Baselines.}
% We use the two-tower architecture based EDB \cite{huang2020embedding} as the traditional retrieval model, and DIN \cite{zhou2018deep}, DIEN \cite{zhou2019deep}, SIM \cite{pi2020search}, and TWIN \cite{chang2023twin} as the traditional ranking models. 
% We also compare E2E-GRs (HSTU \cite{zhai2024actions}). 
% Additionally, the traditional sequential recommendation model SASRec \cite{kang2018self}, which adopts a transformer architecture to model UBS, is considered as a baseline.
To comprehensively evaluate the performance of LUM, we compare it against a variety of state-of-the-art models. 
For the traditional retrieval model, we use the two-tower architecture-based EDB \cite{huang2020embedding}. 
For traditional ranking models, we consider DIN \cite{zhou2018deep}, DIEN \cite{zhou2019deep}, SIM \cite{pi2020search}, and TWIN \cite{chang2023twin}. 
Additionally, we compare LUM with E2E-GRs, specifically HSTU \cite{zhai2024actions}. 
We also include the traditional sequential recommendation model SASRec \cite{kang2018self}, which adopts a transformer architecture to model UBS, as a baseline.


\noindent \textbf{Training Details.}
% To have a fair comparison, by default, (1) we keep a similar configuration for transformer-style models (including LUM, HSTU, and SASRec) to have a similar model size (2) the configurations of other DLRMs are set as recommended in original paper.
% (3) All of the model are trained from scratch with the same set of features.
% In addition, the sequence length is 256 in public datasets and 2048 in industrial datasets.
% For LUM, the backbone of DLRMs in step 3 is set as SIM and EDB in public datasets for ranking and retrieval tasks respectively, and for industrial datasets the backbone is set as the online model.
To ensure a fair comparison, we adhere to the following training configurations. 
By default, we maintain a similar configuration for transformer-style models, including LUM, HSTU, and SASRec, to ensure comparable model sizes. 
The configurations for other DLRMs are set according to the recommendations provided in their respective original papers. 
All models are trained from scratch using the same set of features. 
The sequence length is set to 256 for public datasets and 4096 for the industrial dataset. 
For LUM, the backbone of DLRMs in step 3 is configured as SIM for ranking tasks and EDB for retrieval tasks in public datasets. For the industrial dataset, the backbone is set as the online model currently deployed in production.



\begin{table}[]
\centering
\vspace{-1em}
\caption{Performance on Public Datasets}
\vspace{-1em}
\label{table:Ranking Performance on Public Datasets}
\tiny
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Model}     & \textbf{ML-1M} & \textbf{ML-20M} & \textbf{Amazon Books} \\ \midrule
% GRU4Rec   & 0.7480      & 0.7506       & 0.6788      \\
SASRec    & 0.7295      & 0.7166       & 0.6699      \\  
HSTU      & 0.7533      & 0.7463      & 0.6712     \\
% Mamba4Rec & 0.7391      & 0.7574       & 0.6791     \\  \midrule
DIN       & 0.7455      & 0.7299       & 0.6139      \\
DIEN      & 0.7527      & 0.7319       & 0.6130      \\
SIM      & 0.7579      & 0.7341       & 0.6551      \\
TWIN      & 0.7539     & 0.7331       & 0.6538     \\ \midrule
LUM       & \textbf{0.7615}      & \textbf{0.7483}       &   \textbf{0.6727}           \\ \bottomrule
\end{tabular}
}
\end{table}

\subsection{Performance on  Recommendation tasks}
\label{sec:Performance on  Recommendation tasks}
\subsubsection{Performance on public datasets}
% In this section, we conduct experiments on public datasets.
% % \noindent \textbf{Ranking.} 
% We first evaluate LUM on ranking task and take AUC as the metric\footnote{Note 0.001 absolute AUC gain is regarded as significant for the CTR task\cite{zhou2018deep}}.
% Here, we compare LUM with (1) traditional state-of-the-art DLRMs including DIN\cite{zhou2018deep}, DIEN\cite{zhou2019deep}, SIM\cite{pi2020search} and TWIN\cite{chang2023twin} (2) sequential recommendation model (SASRec\cite{kang2018self} and GRU4Rec\cite{hidasi2015session}) (2) E2E-GRs (HSTU\cite{zhai2024actions} and Mamba4rec\cite{liu2024mamba4rec}).
% For LUM, the backbone of DLRMs in Step 3 is set as SIM.
% The results are shown in Table \ref{}.
% We can observe that LUM achieves significant improvements overall datasets. 
% It indicates the design of LUM with three-step paradigm can characterize abundant user interests and further help DLRMs to give a more accurate prediction.
We first conduct experiments on public datasets to evaluate the performance of LUM. 
% Here, we compare LUM with the following models: 
% (1) Traditional state-of-the-art DLRMs, including DIN \cite{zhou2018deep}, DIEN \cite{zhou2019deep}, SIM \cite{pi2020search}, and TWIN \cite{chang2023twin}.
% (2) Sequential recommendation models, such as SASRec \cite{kang2018self}.
% (3) E2E-GRs (HSTU \cite{zhai2024actions}.
% For LUM, the backbone DLRM in Step 3 is set to SIM. 
The performance metric reported is AUC\footnote{Note 0.001 absolute AUC gain is regarded as significant for the ranking task\cite{zhou2018deep}}. 
The results are summarized in Table \ref{table:Ranking Performance on Public Datasets}.
From the results, we observe that LUM achieves significant improvements across all datasets. 
This indicates that the three-step paradigm based LUM effectively captures a wide range of user interests and enhances the predictive accuracy of DLRMs.

% \begin{table}[]
% \centering
% \caption{Retrieval Performance on Public Datasets}
% \label{table:Retrieval Performance on Public Datasets}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cc|cc|cc}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c|}{\textbf{ML-1M}} & \multicolumn{2}{c|}{\textbf{ML-2M}} & \multicolumn{2}{c}{\textbf{Amazon Books}} \\ \cmidrule{2-7} 
%                        & R@10        & R@50        & R@10        & R@50        & R@10           & R@50           \\ \cmidrule{1-7}
% GRU4Rec                & 0.1834     & 0.4535    & 0.2296    & 0.4798    & 0.0558        & 0.1197       \\
% SASRec                 & 0.1887     & 0.4566    & 0.2450    & 0.4882    & 0.0568         & 0.1198        \\
% EDB                    &             &             &             &             &                &                \\
% HSTU                   & 0.1958     & 0.4684     & 0.2430    & 0.4894    & 0.0579        & 0.1192         \\
% Mamba4Rec              & 0.1857     & 0.4538    & 0.2434    & 0.4957    & 0.0564        & 0.1199        \\ \midrule
% LUM                    &             &             &             &             &                &                \\ \bottomrule
% \end{tabular}
% }
% \end{table}

% \noindent \textbf{Retrieval.} 
% Then, we compare LUM with baselines on retrieval tasks, and report commonly used Recall@K as the metric.
% For baselines, we use SASRec\cite{kang2018self}, GRU4Rec\cite{hidasi2015session} and EDB\cite{huang2020embedding} as the state-of-the-art traditional retrieval models, and take HSTU\cite{zhai2024actions} and Mamba4Rec\cite{liu2024mamba4rec} as the E2E-GRs.
% The results are reported in Table \ref{}.
% It shows that LUM with three-step paradigm significantly outperforms both traditional retrieval models and E2E-GRs. 



\begin{table}[]
\centering
\caption{Overall performance in industrial settings.
Imp. denotes the improvements relative to the best baseline.}
\label{table:Overall Performance in Industrial Setting}
\tiny
% \smaller[\smaller]
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \textbf{Ranking} & \multicolumn{2}{c}{\textbf{Retrieval}} \\ 
% \cmidrule{2-4} 
                  & AUC     & R@10          & R@50         \\ \cmidrule{1-4}
SASRec            & 0.7322        &   0.2560        & 0.4740\\
DIN               & 0.7336       &     -            &    -          \\
HSTU              & 0.7334       &    0.2594        &   0.4781          \\
Online Model      & 0.7338       &    0.2482        &    0.4651          \\ \midrule
LUM               & \textbf{0.7514}      &     \textbf{0.2727}        &  \textbf{0.4915}            \\ 
Imp.               & \textbf{+0.0176}& \textbf{+0.0133}& \textbf{+0.0134}\\ \bottomrule
\end{tabular}
}
\end{table}

\subsubsection{Performance in industrial setting}
% In this section, we compare LUM against DLRMs and E2E-GRs in industrial setting.
% We take the online model in our applications as the baseline, where it follows Embedding+MLP (for ranking) and two-tower (for retrieval) architecture, and report AUC for ranking and Recall@K (R@K) for retrieval. 
% For LUM, the backbone of DLRMs in Step 3 is also set as the online model.
% Besides, we also take the traditional state-of-the-art DLRMs (SASRec and DIN) and E2E-GR (HSTU) as baselines for comparison.
% To have a fair comparison, we constrain all models to use the same set of features.
% We set max sequence length as 2048 to all models, and train these model from scratch.
% The results are reported in Table \ref{table:Overall Performance in Industrial Setting}.
% We can find that LUM achieves remarkable improvements (+0.0176 on AUC, +0.0133 on R@10, and +0.0134 on R@50) compared with the best baseline. 
% It indicates the effectiveness of LUM on industrial setting which contains a large scale data with complex collaborative information.
% These significant improvements can be primarily contributed to our generative-to-discriminative design in the proposed paradigm where more relevant knowledge can be triggered in step 2 to enhance the discriminative DLRM in Step 3.
In this section, we compare the performance of LUM against both DLRMs and E2E-GRs in an industrial setting. 
For the baseline, we use the online model in our applications, which follows an Embedding+MLP architecture for ranking and a two-tower architecture for retrieval. 
We report AUC for ranking and Recall@K (R@K) for retrieval.
For LUM, the backbone DLRM in Step 3 is also set to the online model. 
Additionally, we include traditional state-of-the-art DLRMs (SASRec \cite{kang2018self} and DIN \cite{zhou2018deep}) and an E2E-GR (HSTU \cite{zhai2024actions}) as baselines for comparison. 
To ensure a fair comparison, all models use the same set of features. 
We set the maximum sequence length to 4096 for all models and train them from scratch.
The results are summarized in Table \ref{table:Overall Performance in Industrial Setting}. 
LUM achieves significant improvements over the best baseline, with a +0.0176 increase in AUC, a +0.0133 increase in R@10, and a +0.0134 increase in R@50. 
% These results indicate the effectiveness of LUM in handling large-scale data with complex collaborative information in an industrial setting.
The substantial improvements can be attributed primarily to the generative-to-discriminative design of our proposed paradigm. 
% Specifically, Step 2 triggers more relevant knowledge, which enhances the discriminative model of the DLRM in Step 3.

% where a large scale sparse data are produced daily.

% \noindent \textbf{Ranking.} 
% We first compare with the performance in ranking task.
% The results are reported in Table \ref{}.
% We can find that LUM achieve remarkable gains from xx to xx compared with all baselines.
% It indicates the effectiveness of LUM on industrial setting where a large scale sparse data are produced daily.

% \noindent \textbf{Retrieval.}
% We next evaluate LUM on the retrieval task.
% Note it is not a easily work since the candidate items are extremely huge (billions scale) in industrial setting.
% We report results in Table \ref{}
% From Table \ref{}, LUM  also achieve best performance.




\begin{table}[]
\centering
\caption{The performance of LUM on various DLRMs.
% Note Base refers to the original results of the corresponding methods and Base+LUM refers to the results
% with the help of LUM. Ave is the average results across all cases. Imp. refers the improvement of
% Base+LUM compared to Base.
% Note "Base" refers to the original results of the corresponding methods, while "Base+LUM" refers to the results achieved with the assistance of LUM. "
"Ave" denotes the average results across all cases, and "Imp." indicates the improvement of "Base+LUM" compared to "Base".
}
\label{table:The performance of LUM on various DLRMs.}
\resizebox{\columnwidth}{!}{
\begin{tabular}{llccccc}
\toprule
\multicolumn{2}{l}{}                            & DIN     & DIEN    & SIM     & TWIN   & Ave    \\ \midrule
\multirow{3}{*}{\textbf{ML-1M}}      & Base     & 0.7455  & 0.7527  &0.7579   &0.7539  & 0.7525 \\
                                     & Base+LUM & 0.7472  & 0.7604  & 0.7615  & 0.7675 & 0.7592 \\
                                     & Imp.     & 0.0016  & 0.0078  &  0.0036 & 0.0136 & 0.0067 \\\midrule
\multirow{3}{*}{\textbf{ML-20M}}     & Base     & 0.7299  & 0.7319  & 0.7341  & 0.7331 &  0.7323 \\
                                     & Base+LUM & 0.7413  &0.7361   & 0.7483 & 0.7422 & 0.7420 \\
                                     & Imp.     & 0.0114  &0.0042   & 0.0142 & 0.0090  &0.0097 \\ \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{Amazon} \\ \textbf{Book}\end{tabular}} 
                                     & Base     & 0.6139 &0.6130 & 0.6551 & 0.6538&  0.6340 \\
                                      & Base+LUM & 0.6261 & 0.6194 & 0.6727&  0.6591 & 0.6443      \\
                                      & Imp.    &  \textbf{+0.0122} & \textbf{+0.0063} & \textbf{+0.0176} & \textbf{+0.0053} & \textbf{+0.0103}       \\ 
                             \bottomrule
\end{tabular}
}
\end{table}



\begin{table}[]
\centering
\caption{Results of effectiveness evaluation.
% Note "scratch" refers the model is trained from scratch with the same set of features.
% "features" refers to adding complex feature engineering.
% "param" refers to initialize the model parameters from the continually daily trained online model.
% "w/o condition token" refers to drop the condition token.
% "muli-conditions" refers to query items by giving multiple conditions.
% "direct feature" and "interest matching" refers to different types of knowledge utilization (Section \ref{sec:Step 3: Utilize knowledge in DLRMs})
% Note: "scratch" refers to models trained from scratch using the same set of features. 
% "feature" indicates considering complex feature engineering. 
% "param" refers to initializing the model parameters from the continually daily trained online model. 
% "w/o condition token" denotes the removal of the condition token. 
% "multi-conditions" refers to querying items by providing multiple conditions. 
% "direct feature" and "interest matching" refer to different types of knowledge utilization, as described in Section \ref{sec:Step 3: Utilize knowledge in DLRMs}.
% "w/o condition token" denotes the removal of the condition token. 
% "multi-conditions" refers to querying items with multiple conditions.
}
\vspace{-1em}
\label{table:The results of effectiveness evaluation.}
\scriptsize
\resizebox{\columnwidth}{!}{
\begin{tabular}{llcc}
\toprule
\textbf{Model}      & \textbf{Training Mode}               &\textbf{AUC}    \\ \midrule
E2E-GR    & scratch          &0.7334 \\
DLRM  &scratch        & 0.7338 \\
LUM  &scratch                & 0.7514 \\ \midrule
DLRM (feature) & warming up       & 0.7541 \\
DLRM (param)   & warming up      & 0.7525 \\
DLRM (param+feature) & warming up& 0.7777 \\ \midrule
LUM (feature)        & warming up       & 0.7659 \\
LUM (param)           & warming up      & 0.7620 \\
LUM (param+feature)   & warming up      & \textbf{0.7794} \\ \midrule
LUM (w/o condition token) &scratch  & 0.7416 \\ 
LUM (multi-conditions)   &scratch& 0.7545 \\ \midrule
LUM (direct feature)     &scratch   & 0.7402 \\  
LUM (direct feature+interest matching) &scratch        & 0.7514 \\  \bottomrule
\end{tabular}
}
\end{table}


\begin{figure*}[t]
 \centering
     \subfigure[Model size (B) vs. Train time cost (hour)]{
    \includegraphics[width= .3\textwidth]{training_time_compare.pdf}
    \label{fig:subfigure2}
  }
      \subfigure[Model size (B) vs. Latency (ms)]{
    \includegraphics[ width= .3\textwidth]{inference_time_compare.pdf}
    \label{fig:subfigure2}
  }
       \subfigure[Max sequence length under latency constrain]{
    \includegraphics[width= .3\textwidth]{max_seq_len.pdf}
    \label{fig:subfigure2}
  }
  \vspace{-1em}
\caption{The Results of Efficiency Evaluation.}
\vspace{-1em}
\label{figure:The Results of Efficiency Evaluation.}
\end{figure*}

% \vspace{-1em}
\subsection{Effectiveness Evaluation}
\label{sec:Effectiveness Evaluation}
In this section, we examine the advantages of the proposed three-step paradigm based LUM.

\noindent \textbf{Impact on various DLRMs.}
% Thanks to the decomposed design of our paradigm, LUM is a universal model can be plugged into any DLRMs during step 3 (Section \ref{sec:Step 3: Utilize knowledge in DLRMs}).
% Thus we examine the impact of LUM on various DLRMs.
% Detailedly, we compare the performance of original DLRMs and DLRMs+LUM on the ranking task.
% The results are reported in Table \ref{table:The performance of LUM on various DLRMs.}.
% We can find that with the help of LUM, all of the methods achieve significant improvements range from xx to xx.
% It indicates that LUM can generally characterize user interests from UBS, boosting DLRMs.
% Thanks to the decomposed design of our paradigm, LUM can be universally integrated into any DLRM during Step 3 (Section \ref{sec:Step 3: Utilize knowledge in DLRMs}). 
% Therefore, we examine the impact of LUM on various DLRMs by comparing the performance of the original DLRMs and DLRMs augmented with LUM on the ranking task. 
% The results are reported in Table \ref{table:The performance of LUM on various DLRMs}.
% The results show that with the assistance of LUM, all methods achieve significant improvements ranging from +xx to +xx. 
% This indicates that LUM effectively characterizes user interests from UBS, thereby enhancing the performance of DLRMs.
Owing to the decomposed design of our proposed paradigm, LUM can be universally integrated into any DLRM during Step 3. 
To evaluate the effectiveness of LUM, we examine its impact on various DLRMs by comparing the performance of the original DLRMs (denoted as Base) and DLRMs augmented with LUM (denoted as Base+LUM) on the ranking task. 
The results are summarized in Table \ref{table:The performance of LUM on various DLRMs.}.
The results demonstrate that, with the assistance of LUM, all methods achieve significant improvements ranging from +0.0053 to +0.0176. 
% This indicates that LUM effectively characterizes user interests from UBS, thereby enhancing the performance of DLRMs. 
These findings highlight the versatility and effectiveness of LUM in improving the predictive accuracy of various DLRMs.

\noindent \textbf{Impact of warming up setting in industrial applications.}
% In industrial applications, the online models usually contains complex feature engineering and are continually trained on billions data per day.
% Existing E2E-GRs completely ignores the knowledge from online model may challenge the performance especially for a application where its online model has been developed several years or even decades.
% As for LUM, the nice compatibility of LUM allows us can train it in a warming up setting.
% In this part, we investigate the corresponding impact.
% Specifically, we implement various versions of LUM (see Table \ref{table:The results of effectiveness evaluation.}), and compare with E2E-GR (HSTU) and DLRM (online model).
% We take AUC on the industrial dataset as the metric.
% From Table \ref{table:The results of effectiveness evaluation.}, we can find (1) Compared with LUM, the models under warming up setting, including LUM (feature), LUM (param) and LUM (feature+param), achieve a noticeable gains from 0.0106 to 0.028.
% (2) Although HSTU beats DLRM under scratch setting, HSTU still has a large performance gap (0.7334 vs. 0.7777) compared to the DLRM (param+feature). 
% It demonstrates the importance to be compatible with the online model, the limited compatibility of E2E-GR may block them to truly be launched in practice.
In industrial applications, online models typically incorporate complex feature engineering and are continually trained on billions of data per day. 
Existing E2E-GRs often ignore the knowledge derived from these online models, which can significantly challenge their performance, especially in applications where the online model has been developed over several years or even decades.
LUM, however, offers excellent compatibility, allowing it to be trained in a warming up setting. 
In this section, we investigate the impact of this compatibility by implementing various versions of LUM, and comparing them with an E2E-GR (HSTU) and a DLRM (online model).
We use AUC on an industrial dataset as the performance metric.

The results are shown in Table \ref{table:The results of effectiveness evaluation.} (Note "scratch" refers to models trained from scratch with the same set of features. "feature" indicates the use of complex feature engineering. "param" refers to initializing model parameters from the continually daily trained online model).
From Table \ref{table:The results of effectiveness evaluation.}, we can draw the following conclusions:
(1) Compared to LUM, the models under warming up settings, including LUM (feature), LUM (param), and LUM (feature+param), achieve noticeable gains in AUC, ranging from +0.0106 to +0.028. 
This highlights the significant benefit of leveraging existing knowledge from the online model.
(2) While HSTU outperforms the DLRM in scratch setting, it still exhibits a large performance gap (0.7334 vs. 0.7777) compared to the DLRM (param+feature). 
This underscores the importance of being compatible with the online model. 
The limited compatibility of E2E-GRs may hinder their practical deployment in real-world applications.
% \end{enumerate}
% These findings demonstrate that LUM's ability to integrate with existing online models through warming up settings is crucial for achieving high performance in industrial applications. 
% The results also highlight the limitations of E2E-GRs and the need for more compatible and flexible models like LUM.


\noindent \textbf{Impact of the proposed tokenization.}
We evaluate the effectiveness of the proposed tokenization. 
Specifically, we develop a LUM (w/o condition token), and report AUC in Table \ref{table:The results of effectiveness evaluation.}.
The results show that compared with LUM (w/o condition token), LUM achieves better performance due to a better understanding of UBS via given conditions.
Furthermore, we also evaluate the effect of using multiply conditions (Section \ref{sec:Step 2: Knowledge Querying with Given Conditions}) including scenario condition and search term condition, denoted as LUM (multi-conditions).
From Table \ref{table:The results of effectiveness evaluation.}, we can observe adding more conditions can further improve the performance, which shows the potential in terms of performance.

\noindent \textbf{Impact of knowledge utilization.}
% We evaluate the different strategies to utilize knowledge in Step 3.
% The results are reported in Table \ref{table:The results of effectiveness evaluation.} ("direct feature" and "interest matching" refer to different types of knowledge utilization, as described in Section \ref{sec:Step 3: Utilize knowledge in DLRMs}).
% Both LUM (direct feature) and LUM (direct feature+interest matching) achieve significant improvements compared with DLRM, which demonstrates the effectiveness of the proposed strategies.
We evaluate the different strategies for utilizing knowledge in Step 3. 
The results are presented in Table \ref{table:The results of effectiveness evaluation.}. 
Here, "direct feature" and "interest matching" refer to different strategies of knowledge utilization, as detailed in Section \ref{sec:Step 3: Utilize knowledge in DLRMs}. 
Both LUM (direct feature) and LUM (direct feature + interest matching) achieve significant improvements over DLRM, demonstrating the effectiveness of the proposed strategies.

\begin{table}[] 
\centering
\caption{The impact of group query and packing.
Imp. refers to the relative improvements.}
\label{table:The impact of Group query and packing}
\scriptsize
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
            & \textbf{Phase}                   & \textbf{Time cost (hour)} & \textbf{Imp.}\\ \midrule

LUM (w/o packing) & \multirow{2}{*}{Step 1} &   151    &   - \\
LUM (w/ packing)   &                         &   26   &   \textbf{+82\%} \\ \midrule
LUM (w/o group query) & \multirow{2}{*}{Step 2} &  17   &  -    \\
LUM (w/ group query)   &                         &   3.6  &  \textbf{+78\%}    \\ \bottomrule
\end{tabular}
}
\end{table}

\noindent \textbf{Impact of packing and group query.}
% Packing and group query are designed to speed up the process during step 1 and 2 respectively.
% Table \ref{table:The impact of Group query and packing} shows the efficiency of the proposed strategy in industrial setting, where step 1 and step 2 can be speed up to 82\% and 78\% respectively.
Packing and group query are designed to accelerate the processes in Step 1 and Step 2, respectively. 
Table \ref{table:The impact of Group query and packing} demonstrates the efficiency gains of these strategies in an industrial setting, where Step 1 and Step 2 can be sped up by 82\% and 78\%, respectively.





\subsection{Efficiency Evaluation}
\label{sec:Efficiency Evaluation}
% -lum train: packing
% -lum inference: group query
% -lum serving: prefix+top query
% % -CTR real-time training
% % -CTR 
% prefix vs. cache

% The proposed three-step paradigm is a decomposed paradigm which naturally allows a efficiency application in industrial setting.
% Furthermore, we also design various strategies, i.e., packing and group query, to further speed up training and inference.
% In this section, we conduct experiments to have a deep efficiency evaluation.

% \subsubsection{Packing Efficiency}

% \subsubsection{Group Query Efficiency}


\noindent \textbf{Training Efficiency.}
% Figure \ref{figure:The Results of Efficiency Evaluation.} (a) shows the training time cost of different models on one day data in an industrial setting. 
% For E2E-GR, we follow HSTU's \cite{zhai2024actions} optimizing and train it from impression-level to user-level.
% For DLRM, we set it as the online model in our application for comparison.
% For LUM, the backbone ranking model is also set as the online model, and since the step 1 and 2 in LUM can pre-processed and cached, here,these time cost can be exclude when training the downstream model during step 3. 
% The sequence length is set 4096.
% The model size of LUM and E2E-GR ranges from 0.5B to 14B.
% All of them are trained on 128 GPUs.
% Additionally, the lower bound time cost is set 24 hours, since the continual training in practice  are required model consuming daily data within 24 hours. 
% Overall, LUM achieve similar cost with DLRM and is not sensitive with the model size due to the decomposed three-step paradigm, which unlocks the potential scaling law during training in industrial setting.
% Additionally, E2E-GR is 12$\times$ to 98$\times$ slower than LUM. 
% None of E2E-GRs (different model sizes) satisfies the training cost requirements (within 24 hours).
% It means E2E-GR needs 12$\times$ to 98$\times$ GPUs to follow the throughput of LUM and 2$\times$ to 18$\times$ GPUs to follow the throughput of the low bound, which shows a huge expenditure for GPU resources in E2E-GR. 
Figure \ref{figure:The Results of Efficiency Evaluation.} (a) illustrates the training time costs of different models on one day's data in an industrial setting. 
For E2E-GR, we followed HSTU's \cite{zhai2024actions} optimization and trained it from impression-level to user-level. 
For DLRM, we used the online model in our application as the baseline. 
For LUM, the backbone DLRM is also set as the online model. 
Since Steps 1 and 2 in LUM can be pre-processed (\ref{sec:Discussion}), their time costs are excluded when training the downstream model in Step 3. 
The sequence length is set to 4096, and the model sizes of LUM and E2E-GR range from 0.5 billion to 14 billion parameters. 
All models are trained on 128 GPUs. 
Additionally, the lower bound time cost is set to 24 hours, as continuous training in practice requires models to process daily data within this timeframe.
Overall, LUM achieves similar training time costs to DLRM and remains relatively insensitive to model size due to its decomposed three-step paradigm. 
This characteristic unlocks the potential for scaling laws during training in industrial settings. 
In contrast, E2E-GR is 12$\times$ to 98$\times$ slower than LUM. 
None of the E2E-GR models (across different sizes) meet the training cost requirement of completing within 24 hours. 
To match the throughput of LUM, E2E-GR would require 12$\times$ to 98$\times$ GPUs, and to meet the lower bound requirement, it would need 2$\times$ to 18$\times$ GPUs. 
% These findings highlight the significant resource expenditure required for E2E-GR, making it less practical for industrial applications compared to LUM.

% 端到端的模型kv cache不了
\noindent \textbf{Serving Efficiency.}
% Figure \ref{figure:The Results of Efficiency Evaluation.} (b) and (c) shows the latency of different models during serving online. 
% For E2E-GR, we follow HSTU's M-FALCON implementation \cite{zhai2024actions}\footnote{Note unlike LLM where the trained parameters are seldom updated during serving,  in industrial setting, the parameters of the ranking model are real-time updated, which means the results or middle results cannot be shared across requests.
% Thus the cache strategy in M-FALCON is not considered here.}.
% DLRM are set as the online models. 
% The DLRM backbone ranking model of LUM is also set as DLRM, and the latency of LUM during Step 3 is evaluated, as Step 1 and 2 can be pre-computed.
% The sequence length is set 4096 in Figure \ref{figure:The Results of Efficiency Evaluation.} (b).
% The model size of LUM and E2E-GR ranges from 0.5B to 14B.
% The latency constrain is <30ms and the ranking candidates are 100 around in our case.
% Overall, the latency of LUM is free from the model size which allows us to scale up LUM without latency constrain.
% On the contrary, E2E-GR cannot give responses in time (<30ms) and even a small model (0.5B) is still not satisfied with latency constrain.
% Further, we try to reduce the sequence length of E2E-GR to meet the latency constrain (Figure \ref{figure:The Results of Efficiency Evaluation.} (c)).
% It is depressing that the maximum sequence length (latency <30ms) is only 64 when applying 14B, which is far from the normal setting (64$\times$ smaller in our case).
% It shows although E2E-GR may show scaling law in offline performance, the scaled model (large model size and long sequence length) can hardly be applied into industrial application, locking the scaling law in the industrial setting. 
Figures \ref{figure:The Results of Efficiency Evaluation.} (b) and (c) illustrate the latency of different models during online serving. For E2E-GR, we followed HSTU's M-FALCON implementation \cite{zhai2024actions} \footnote{Note that, unlike LLMs where the trained parameters are seldom updated during serving, in industrial settings, the parameters of the ranking model are updated in real-time, meaning results or intermediate results cannot be shared across requests. Therefore, the cache strategy in M-FALCON is not applicable here.} 
DLRM is set as the online model for both the baseline and the backbone ranking model of LUM. 
The latency of LUM during Step 3 is evaluated, as Step 1 and 2 can be pre-computed (Section \ref{sec:Discussion}).
In Figure \ref{figure:The Results of Efficiency Evaluation.} (b), the sequence length is set to 4096, and the model sizes of LUM and E2E-GR range from 0.5 billion to 14 billion parameters. 
The latency constraint is set to less than 30 milliseconds (ms), and the number of ranking candidates is approximately 100 in our case.
Overall, the latency of LUM is independent of the model size, allowing us to scale up LUM without violating the latency constraint. 
On the contrary, E2E-GR fails to provide timely responses (less than 30 ms) even for a small model (0.5 billion parameters). 
To further investigate, we attempted to reduce the sequence length of E2E-GR to meet the latency constraint (Figure \ref{figure:The Results of Efficiency Evaluation.} (c)). 
Disappointingly, the maximum sequence length that meets the latency constraint (<30 ms) is only 64 when using a 14 billion parameter model, which is far from the typical setting (64 times smaller in our case).
These results demonstrate that, while E2E-GR may exhibit scaling laws in offline performance, the practical application of large-scale models is severely limited in industrial settings due to latency constraints. 
In contrast, LUM's decoupled architecture ensures consistent latency performance, making it a more feasible and scalable solution for real-time industrial applications.

\begin{figure}[t]
 \centering
     \subfigure[Scaling law with model size]{
    \includegraphics[width= .22\textwidth]{scale_law_p.pdf}
    \label{fig:subfigure2}
  }
      \subfigure[Scaling law with sequence length]{
    \includegraphics[ width= .22\textwidth]{scale_law_l.pdf}
    \label{fig:subfigure2}
  }
\vspace{-1em}
\caption{Scaling law for LUM.}
\vspace{-1em}
\label{figure:Scaling law for LUM.}
\end{figure}


\vspace{-1em}
\subsection{Scaling Law for LUM}
% Following the protocols from \cite{radford2018improving,radford2019language}, we examine whether our LUM model complies with similar scaling laws.
% Here we mainly focus on evaluating the scaling laws with model parameters and sequence length.
% For model parameters, we keep a same sequence length (2048), and then train models across xx different sizes, from 19M to 7B parameters.
% For sequence length, we train models with different sequence length from 256 to 8192 under the parameter size (300M).
% Results are plotted in Figure \ref{figure:Scaling law for LUM.}, where we observed a clear power-law scaling trend, as consistent with \cite{zhai2024actions,radford2018improving,radford2019language}.
% The power-law scaling laws can be expressed as:
% \begin{align}
% R_P&=0.0068*log(P)+0.1741\\
% R_L&=0.0147*log(L)+0.2326 
% \end{align} where $R_P$ and $R_L$ refers R@10 with different model size and sequence length respectively.
% $P$ refers to model size and $L$ refers to sequence length.
% These results verify the strong scalability of LUM, by which scaling up LUM  can continuously improve the model’s performance.
Following the protocols established in \cite{radford2018improving, radford2019language}, we examine whether our LUM model complies to similar scaling law. 
Specifically, we focus on evaluating the scaling law with respect to model parameters and sequence length.
For model parameters, to evaluate the impact of model parameters, we maintain a constant sequence length of 4096 and train models across a range of sizes, from 19 million to 7 billion parameters.
For sequence length, to evaluate the impact of sequence length, we train models with varying sequence lengths, from 256 to 8192, under a fixed parameter size of 300 million.
The results are plotted in Figure \ref{figure:Scaling law for LUM.}, where we observe a clear power-law scaling trend, consistent with previous findings \cite{zhai2024actions, radford2018improving, radford2019language}. 
The power-law scaling laws can be expressed as:
% \vspace{-0.5em}
\begin{align}
R_P&=0.0068\cdot log(P)+0.1741\\
R_L&=0.0147\cdot log(L)+0.2326 
% \vspace{-0.5em}
\end{align} 
where $R_P$ and $R_L$ refer to the R@10 metric for different model sizes and sequence lengths, respectively.
$P$ denotes the model size, and $L$ denotes the sequence length.
These results confirm the strong scalability of LUM, demonstrating that increasing the model size and sequence length can continuously improve the model's performance. 
This finding underscores the potential of LUM to achieve higher performance as it scales, making it a promising approach for large-scale industrial applications.

\subsection{Online Results}
\label{sec:Online Results}
% To evaluate the effectiveness in industrial application, we develop LUM in the sponsored search system in Taobao (the biggest e-commerce platform in China).
% As mentioned in Section \ref{sec:Discussion}, in offline, we first pretrain LUM and pre-trigger the needed knowledge with different conditions. 
% Then the responses of LUM in step 2 are stored which can be directly utilized during online serving to meet the latency constrain.
% Finally, we test LUM in online A/B experiments of the ranking task. 
% Key metrics have shown a significant increase of xx\%.
To evaluate the effectiveness of LUM in an industrial setting, we implemented it in the sponsored search system of Taobao, the largest e-commerce platform in China. 
As discussed in Section \ref{sec:Discussion}, our evaluation process involves several key steps:  (1) Offline, we first pretrained LUM and pre-triggered the necessary knowledge under various conditions. 
(2) The responses generated by LUM in Step 2 are stored, allowing them to be directly utilized during online serving. 
This pre-computation helps meet the strict latency constraints required in real-time industrial applications.
(3) Finally, we conducted online A/B experiments to test LUM in the ranking task. 
Key performance metrics, CTR and RPM (Revenue Per Mile), demonstrated a significant improvement of 2.9\% and 1.2\% respectively.
These findings highlight the practical benefits of LUM, demonstrating its ability to improve user engagement and business outcomes in large-scale e-commerce platforms. 
% The successful integration of LUM in Taobao's sponsored search system underscores its potential for broader adoption in other industrial applications.

\section{Conclusion}
% In this paper, we propose a three-step paradigm based Large User Model (LUM) to unlock scaling law in industrial recommendation system.
% On the one hand, the scaling law can be established through generative learning in step 1.
% On the other hand, the decomposed design of paradigm, plus with the proposed 'next-condition-item prediction', enable LUM can be effectively and efficiently scaled up and applied in industrial application.
% Experimental results show the effectiveness and efficiency of our model and we also have successfully deploy it in industrial application and achieve significant improvements.
In conclusion, LUM effectively unlocks scaling laws in industrial recommendation systems through a three-step paradigm. 
The decomposed design and next-condition-item prediction ensure that LUM can be efficiently scaled and deployed, leading to significant performance improvements in real-world applications. 
Our experimental and deployment results demonstrate the robustness and practicality of LUM, making it valuable for enhancing user engagement and business outcomes in large-scale e-commerce platforms.



% \subsection{Ablation Study}
% \subsubsection{The impact of Packing}
% \subsubsection{}
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}

% \bibliography{sample-base}
%%% -*-BibTeX-*-
%%% Do NOT edit. File created by BibTeX with style
%%% ACM-Reference-Format-Journals [18-Jan-2012].

\begin{thebibliography}{26}

%%% ====================================================================
%%% NOTE TO THE USER: you can override these defaults by providing
%%% customized versions of any of these macros before the \bibliography
%%% command.  Each of them MUST provide its own final punctuation,
%%% except for \shownote{}, \showDOI{}, and \showURL{}.  The latter two
%%% do not use final punctuation, in order to avoid confusing it with
%%% the Web address.
%%%
%%% To suppress output of a particular field, define its macro to expand
%%% to an empty string, or better, \unskip, like this:
%%%
%%% \newcommand{\showDOI}[1]{\unskip}   % LaTeX syntax
%%%
%%% \def \showDOI #1{\unskip}           % plain TeX syntax
%%%
%%% ====================================================================

\ifx \showCODEN    \undefined \def \showCODEN     #1{\unskip}     \fi
\ifx \showDOI      \undefined \def \showDOI       #1{#1}\fi
\ifx \showISBNx    \undefined \def \showISBNx     #1{\unskip}     \fi
\ifx \showISBNxiii \undefined \def \showISBNxiii  #1{\unskip}     \fi
\ifx \showISSN     \undefined \def \showISSN      #1{\unskip}     \fi
\ifx \showLCCN     \undefined \def \showLCCN      #1{\unskip}     \fi
\ifx \shownote     \undefined \def \shownote      #1{#1}          \fi
\ifx \showarticletitle \undefined \def \showarticletitle #1{#1}   \fi
\ifx \showURL      \undefined \def \showURL       {\relax}        \fi
% The following commands are used for tagged output and should be
% invisible to TeX
\providecommand\bibfield[2]{#2}
\providecommand\bibinfo[2]{#2}
\providecommand\natexlab[1]{#1}
\providecommand\showeprint[2][]{arXiv:#2}

\bibitem[\protect\citeauthoryear{Bao, Zhang, Zhang, Wang, Feng, and He}{Bao
  et~al\mbox{.}}{2023}]%
        {bao2023tallrec}
\bibfield{author}{\bibinfo{person}{Keqin Bao}, \bibinfo{person}{Jizhi Zhang},
  \bibinfo{person}{Yang Zhang}, \bibinfo{person}{Wenjie Wang},
  \bibinfo{person}{Fuli Feng}, {and} \bibinfo{person}{Xiangnan He}.}
  \bibinfo{year}{2023}\natexlab{}.
\newblock \showarticletitle{Tallrec: An effective and efficient tuning
  framework to align large language model with recommendation}. In
  \bibinfo{booktitle}{\emph{Proceedings of the 17th ACM Conference on
  Recommender Systems}}. \bibinfo{pages}{1007--1014}.
\newblock


\bibitem[\protect\citeauthoryear{Bernardo, Bayarri, Berger, Dawid, Heckerman,
  Smith, and West}{Bernardo et~al\mbox{.}}{2007}]%
        {bernardo2007generative}
\bibfield{author}{\bibinfo{person}{JM Bernardo}, \bibinfo{person}{MJ Bayarri},
  \bibinfo{person}{JO Berger}, \bibinfo{person}{AP Dawid}, \bibinfo{person}{D
  Heckerman}, \bibinfo{person}{AFM Smith}, {and} \bibinfo{person}{M West}.}
  \bibinfo{year}{2007}\natexlab{}.
\newblock \showarticletitle{Generative or discriminative? getting the best of
  both worlds}.
\newblock \bibinfo{journal}{\emph{Bayesian statistics}} \bibinfo{volume}{8},
  \bibinfo{number}{3} (\bibinfo{year}{2007}), \bibinfo{pages}{3--24}.
\newblock


\bibitem[\protect\citeauthoryear{Chang, Zhang, Fu, Zang, Guan, Lu, Hui, Leng,
  Niu, Song, et~al\mbox{.}}{Chang et~al\mbox{.}}{2023}]%
        {chang2023twin}
\bibfield{author}{\bibinfo{person}{Jianxin Chang}, \bibinfo{person}{Chenbin
  Zhang}, \bibinfo{person}{Zhiyi Fu}, \bibinfo{person}{Xiaoxue Zang},
  \bibinfo{person}{Lin Guan}, \bibinfo{person}{Jing Lu}, \bibinfo{person}{Yiqun
  Hui}, \bibinfo{person}{Dewei Leng}, \bibinfo{person}{Yanan Niu},
  \bibinfo{person}{Yang Song}, {et~al\mbox{.}}}
  \bibinfo{year}{2023}\natexlab{}.
\newblock \showarticletitle{TWIN: TWo-stage interest network for lifelong user
  behavior modeling in CTR prediction at kuaishou}. In
  \bibinfo{booktitle}{\emph{Proceedings of the 29th ACM SIGKDD Conference on
  Knowledge Discovery and Data Mining}}. \bibinfo{pages}{3785--3794}.
\newblock


\bibitem[\protect\citeauthoryear{Chen, Chi, Peng, and Yuan}{Chen
  et~al\mbox{.}}{2024}]%
        {chen2024hllm}
\bibfield{author}{\bibinfo{person}{Junyi Chen}, \bibinfo{person}{Lu Chi},
  \bibinfo{person}{Bingyue Peng}, {and} \bibinfo{person}{Zehuan Yuan}.}
  \bibinfo{year}{2024}\natexlab{}.
\newblock \showarticletitle{Hllm: Enhancing sequential recommendations via
  hierarchical large language models for item and user modeling}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2409.12740}}
  (\bibinfo{year}{2024}).
\newblock


\bibitem[\protect\citeauthoryear{Ding, Hou, Li, and McAuley}{Ding
  et~al\mbox{.}}{2024}]%
        {ding2024inductive}
\bibfield{author}{\bibinfo{person}{Yijie Ding}, \bibinfo{person}{Yupeng Hou},
  \bibinfo{person}{Jiacheng Li}, {and} \bibinfo{person}{Julian McAuley}.}
  \bibinfo{year}{2024}\natexlab{}.
\newblock \showarticletitle{Inductive Generative Recommendation via
  Retrieval-based Speculation}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2410.02939}}
  (\bibinfo{year}{2024}).
\newblock


\bibitem[\protect\citeauthoryear{Geng, Liu, Fu, Ge, and Zhang}{Geng
  et~al\mbox{.}}{2022}]%
        {geng2022recommendation}
\bibfield{author}{\bibinfo{person}{Shijie Geng}, \bibinfo{person}{Shuchang
  Liu}, \bibinfo{person}{Zuohui Fu}, \bibinfo{person}{Yingqiang Ge}, {and}
  \bibinfo{person}{Yongfeng Zhang}.} \bibinfo{year}{2022}\natexlab{}.
\newblock \showarticletitle{Recommendation as language processing (rlp): A
  unified pretrain, personalized prompt \& predict paradigm (p5)}. In
  \bibinfo{booktitle}{\emph{Proceedings of the 16th ACM Conference on
  Recommender Systems}}. \bibinfo{pages}{299--315}.
\newblock


\bibitem[\protect\citeauthoryear{Guo, Tang, Ye, Li, and He}{Guo
  et~al\mbox{.}}{2017}]%
        {guo2017deepfm}
\bibfield{author}{\bibinfo{person}{Huifeng Guo}, \bibinfo{person}{Ruiming
  Tang}, \bibinfo{person}{Yunming Ye}, \bibinfo{person}{Zhenguo Li}, {and}
  \bibinfo{person}{Xiuqiang He}.} \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{DeepFM: a factorization-machine based neural
  network for CTR prediction}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:1703.04247}}
  (\bibinfo{year}{2017}).
\newblock


\bibitem[\protect\citeauthoryear{Hidasi}{Hidasi}{2015}]%
        {hidasi2015session}
\bibfield{author}{\bibinfo{person}{B Hidasi}.} \bibinfo{year}{2015}\natexlab{}.
\newblock \showarticletitle{Session-based Recommendations with Recurrent Neural
  Networks}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:1511.06939}}
  (\bibinfo{year}{2015}).
\newblock


\bibitem[\protect\citeauthoryear{Huang, Sharma, Sun, Xia, Zhang, Pronin,
  Padmanabhan, Ottaviano, and Yang}{Huang et~al\mbox{.}}{2020}]%
        {huang2020embedding}
\bibfield{author}{\bibinfo{person}{Jui-Ting Huang}, \bibinfo{person}{Ashish
  Sharma}, \bibinfo{person}{Shuying Sun}, \bibinfo{person}{Li Xia},
  \bibinfo{person}{David Zhang}, \bibinfo{person}{Philip Pronin},
  \bibinfo{person}{Janani Padmanabhan}, \bibinfo{person}{Giuseppe Ottaviano},
  {and} \bibinfo{person}{Linjun Yang}.} \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{Embedding-based retrieval in facebook search}. In
  \bibinfo{booktitle}{\emph{Proceedings of the 26th ACM SIGKDD International
  Conference on Knowledge Discovery \& Data Mining}}.
  \bibinfo{pages}{2553--2561}.
\newblock


\bibitem[\protect\citeauthoryear{Kang and McAuley}{Kang and McAuley}{2018}]%
        {kang2018self}
\bibfield{author}{\bibinfo{person}{Wang-Cheng Kang} {and}
  \bibinfo{person}{Julian McAuley}.} \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{Self-attentive sequential recommendation}. In
  \bibinfo{booktitle}{\emph{2018 IEEE international conference on data mining
  (ICDM)}}. IEEE, \bibinfo{pages}{197--206}.
\newblock


\bibitem[\protect\citeauthoryear{Lian, Zhou, Zhang, Chen, Xie, and Sun}{Lian
  et~al\mbox{.}}{2018}]%
        {lian2018xdeepfm}
\bibfield{author}{\bibinfo{person}{Jianxun Lian}, \bibinfo{person}{Xiaohuan
  Zhou}, \bibinfo{person}{Fuzheng Zhang}, \bibinfo{person}{Zhongxia Chen},
  \bibinfo{person}{Xing Xie}, {and} \bibinfo{person}{Guangzhong Sun}.}
  \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{xdeepfm: Combining explicit and implicit feature
  interactions for recommender systems}. In
  \bibinfo{booktitle}{\emph{Proceedings of the 24th ACM SIGKDD international
  conference on knowledge discovery \& data mining}}.
  \bibinfo{pages}{1754--1763}.
\newblock


\bibitem[\protect\citeauthoryear{Lin, Chen, Wang, Xi, Qu, Dai, Zhang, Tang, Yu,
  and Zhang}{Lin et~al\mbox{.}}{2024}]%
        {lin2024clickprompt}
\bibfield{author}{\bibinfo{person}{Jianghao Lin}, \bibinfo{person}{Bo Chen},
  \bibinfo{person}{Hangyu Wang}, \bibinfo{person}{Yunjia Xi},
  \bibinfo{person}{Yanru Qu}, \bibinfo{person}{Xinyi Dai},
  \bibinfo{person}{Kangning Zhang}, \bibinfo{person}{Ruiming Tang},
  \bibinfo{person}{Yong Yu}, {and} \bibinfo{person}{Weinan Zhang}.}
  \bibinfo{year}{2024}\natexlab{}.
\newblock \showarticletitle{ClickPrompt: CTR Models are Strong Prompt
  Generators for Adapting Language Models to CTR Prediction}. In
  \bibinfo{booktitle}{\emph{Proceedings of the ACM on Web Conference 2024}}.
  \bibinfo{pages}{3319--3330}.
\newblock


\bibitem[\protect\citeauthoryear{Liu, Hou, and McAuley}{Liu
  et~al\mbox{.}}{2024}]%
        {liu2024multi}
\bibfield{author}{\bibinfo{person}{Zihan Liu}, \bibinfo{person}{Yupeng Hou},
  {and} \bibinfo{person}{Julian McAuley}.} \bibinfo{year}{2024}\natexlab{}.
\newblock \showarticletitle{Multi-Behavior Generative Recommendation}. In
  \bibinfo{booktitle}{\emph{Proceedings of the 33rd ACM International
  Conference on Information and Knowledge Management}}.
  \bibinfo{pages}{1575--1585}.
\newblock


\bibitem[\protect\citeauthoryear{Ng and Jordan}{Ng and Jordan}{2001}]%
        {ng2001discriminative}
\bibfield{author}{\bibinfo{person}{Andrew Ng} {and} \bibinfo{person}{Michael
  Jordan}.} \bibinfo{year}{2001}\natexlab{}.
\newblock \showarticletitle{On discriminative vs. generative classifiers: A
  comparison of logistic regression and naive bayes}.
\newblock \bibinfo{journal}{\emph{Advances in neural information processing
  systems}}  \bibinfo{volume}{14} (\bibinfo{year}{2001}).
\newblock


\bibitem[\protect\citeauthoryear{Oord, Li, and Vinyals}{Oord
  et~al\mbox{.}}{2018}]%
        {oord2018representation}
\bibfield{author}{\bibinfo{person}{Aaron van~den Oord}, \bibinfo{person}{Yazhe
  Li}, {and} \bibinfo{person}{Oriol Vinyals}.} \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{Representation learning with contrastive predictive
  coding}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:1807.03748}}
  (\bibinfo{year}{2018}).
\newblock


\bibitem[\protect\citeauthoryear{Pi, Zhou, Zhang, Wang, Ren, Fan, Zhu, and
  Gai}{Pi et~al\mbox{.}}{2020}]%
        {pi2020search}
\bibfield{author}{\bibinfo{person}{Qi Pi}, \bibinfo{person}{Guorui Zhou},
  \bibinfo{person}{Yujing Zhang}, \bibinfo{person}{Zhe Wang},
  \bibinfo{person}{Lejian Ren}, \bibinfo{person}{Ying Fan},
  \bibinfo{person}{Xiaoqiang Zhu}, {and} \bibinfo{person}{Kun Gai}.}
  \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{Search-based user interest modeling with lifelong
  sequential behavior data for click-through rate prediction}. In
  \bibinfo{booktitle}{\emph{Proceedings of the 29th ACM International
  Conference on Information \& Knowledge Management}}.
  \bibinfo{pages}{2685--2692}.
\newblock


\bibitem[\protect\citeauthoryear{Radford}{Radford}{2018}]%
        {radford2018improving}
\bibfield{author}{\bibinfo{person}{Alec Radford}.}
  \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{Improving language understanding by generative
  pre-training}.
\newblock  (\bibinfo{year}{2018}).
\newblock


\bibitem[\protect\citeauthoryear{Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al\mbox{.}}{Radford et~al\mbox{.}}{2019}]%
        {radford2019language}
\bibfield{author}{\bibinfo{person}{Alec Radford}, \bibinfo{person}{Jeffrey Wu},
  \bibinfo{person}{Rewon Child}, \bibinfo{person}{David Luan},
  \bibinfo{person}{Dario Amodei}, \bibinfo{person}{Ilya Sutskever},
  {et~al\mbox{.}}} \bibinfo{year}{2019}\natexlab{}.
\newblock \showarticletitle{Language models are unsupervised multitask
  learners}.
\newblock \bibinfo{journal}{\emph{OpenAI blog}} \bibinfo{volume}{1},
  \bibinfo{number}{8} (\bibinfo{year}{2019}), \bibinfo{pages}{9}.
\newblock


\bibitem[\protect\citeauthoryear{Sun, Liu, Wu, Pei, Lin, Ou, and Jiang}{Sun
  et~al\mbox{.}}{2019}]%
        {sun2019bert4rec}
\bibfield{author}{\bibinfo{person}{Fei Sun}, \bibinfo{person}{Jun Liu},
  \bibinfo{person}{Jian Wu}, \bibinfo{person}{Changhua Pei},
  \bibinfo{person}{Xiao Lin}, \bibinfo{person}{Wenwu Ou}, {and}
  \bibinfo{person}{Peng Jiang}.} \bibinfo{year}{2019}\natexlab{}.
\newblock \showarticletitle{BERT4Rec: Sequential recommendation with
  bidirectional encoder representations from transformer}. In
  \bibinfo{booktitle}{\emph{Proceedings of the 28th ACM international
  conference on information and knowledge management}}.
  \bibinfo{pages}{1441--1450}.
\newblock


\bibitem[\protect\citeauthoryear{Yan, Qin, Wang, Bendersky, and Najork}{Yan
  et~al\mbox{.}}{2022}]%
        {DBLP:conf/kdd/YanQWBN22}
\bibfield{author}{\bibinfo{person}{Le Yan}, \bibinfo{person}{Zhen Qin},
  \bibinfo{person}{Xuanhui Wang}, \bibinfo{person}{Michael Bendersky}, {and}
  \bibinfo{person}{Marc Najork}.} \bibinfo{year}{2022}\natexlab{}.
\newblock \showarticletitle{Scale Calibration of Deep Ranking Models}. In
  \bibinfo{booktitle}{\emph{{KDD} '22: The 28th {ACM} {SIGKDD} Conference on
  Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18,
  2022}}, \bibfield{editor}{\bibinfo{person}{Aidong Zhang} {and}
  \bibinfo{person}{Huzefa Rangwala}} (Eds.). \bibinfo{publisher}{{ACM}},
  \bibinfo{pages}{4300--4309}.
\newblock
\urldef\tempurl%
\url{https://doi.org/10.1145/3534678.3539072}
\showDOI{\tempurl}


\bibitem[\protect\citeauthoryear{Yogatama, Dyer, Ling, and Blunsom}{Yogatama
  et~al\mbox{.}}{2017}]%
        {yogatama2017generative}
\bibfield{author}{\bibinfo{person}{Dani Yogatama}, \bibinfo{person}{Chris
  Dyer}, \bibinfo{person}{Wang Ling}, {and} \bibinfo{person}{Phil Blunsom}.}
  \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{Generative and discriminative text classification
  with recurrent neural networks}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:1703.01898}}
  (\bibinfo{year}{2017}).
\newblock


\bibitem[\protect\citeauthoryear{Yu, Zhang, Zhao, Wang, and Ma}{Yu
  et~al\mbox{.}}{2024}]%
        {yu2024ra}
\bibfield{author}{\bibinfo{person}{Xiaohan Yu}, \bibinfo{person}{Li Zhang},
  \bibinfo{person}{Xin Zhao}, \bibinfo{person}{Yue Wang}, {and}
  \bibinfo{person}{Zhongrui Ma}.} \bibinfo{year}{2024}\natexlab{}.
\newblock \showarticletitle{RA-Rec: An Efficient ID Representation Alignment
  Framework for LLM-based Recommendation}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2402.04527}}
  (\bibinfo{year}{2024}).
\newblock


\bibitem[\protect\citeauthoryear{Zhai, Liao, Liu, Wang, Li, Cao, Gao, Gong, Gu,
  He, et~al\mbox{.}}{Zhai et~al\mbox{.}}{2024}]%
        {zhai2024actions}
\bibfield{author}{\bibinfo{person}{Jiaqi Zhai}, \bibinfo{person}{Lucy Liao},
  \bibinfo{person}{Xing Liu}, \bibinfo{person}{Yueming Wang},
  \bibinfo{person}{Rui Li}, \bibinfo{person}{Xuan Cao}, \bibinfo{person}{Leon
  Gao}, \bibinfo{person}{Zhaojie Gong}, \bibinfo{person}{Fangda Gu},
  \bibinfo{person}{Michael He}, {et~al\mbox{.}}}
  \bibinfo{year}{2024}\natexlab{}.
\newblock \showarticletitle{Actions speak louder than words: Trillion-parameter
  sequential transducers for generative recommendations}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2402.17152}}
  (\bibinfo{year}{2024}).
\newblock


\bibitem[\protect\citeauthoryear{Zhao, Qu, Staniszewski, Tworkowski, Liu,
  Mi{\l}o{\'s}, Wu, and Minervini}{Zhao et~al\mbox{.}}{2024}]%
        {zhao2024analysing}
\bibfield{author}{\bibinfo{person}{Yu Zhao}, \bibinfo{person}{Yuanbin Qu},
  \bibinfo{person}{Konrad Staniszewski}, \bibinfo{person}{Szymon Tworkowski},
  \bibinfo{person}{Wei Liu}, \bibinfo{person}{Piotr Mi{\l}o{\'s}},
  \bibinfo{person}{Yuxiang Wu}, {and} \bibinfo{person}{Pasquale Minervini}.}
  \bibinfo{year}{2024}\natexlab{}.
\newblock \showarticletitle{Analysing The Impact of Sequence Composition on
  Language Model Pre-Training}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2402.13991}}
  (\bibinfo{year}{2024}).
\newblock


\bibitem[\protect\citeauthoryear{Zhou, Mou, Fan, Pi, Bian, Zhou, Zhu, and
  Gai}{Zhou et~al\mbox{.}}{2019}]%
        {zhou2019deep}
\bibfield{author}{\bibinfo{person}{Guorui Zhou}, \bibinfo{person}{Na Mou},
  \bibinfo{person}{Ying Fan}, \bibinfo{person}{Qi Pi}, \bibinfo{person}{Weijie
  Bian}, \bibinfo{person}{Chang Zhou}, \bibinfo{person}{Xiaoqiang Zhu}, {and}
  \bibinfo{person}{Kun Gai}.} \bibinfo{year}{2019}\natexlab{}.
\newblock \showarticletitle{Deep interest evolution network for click-through
  rate prediction}. In \bibinfo{booktitle}{\emph{Proceedings of the AAAI
  conference on artificial intelligence}}, Vol.~\bibinfo{volume}{33}.
  \bibinfo{pages}{5941--5948}.
\newblock


\bibitem[\protect\citeauthoryear{Zhou, Zhu, Song, Fan, Zhu, Ma, Yan, Jin, Li,
  and Gai}{Zhou et~al\mbox{.}}{2018}]%
        {zhou2018deep}
\bibfield{author}{\bibinfo{person}{Guorui Zhou}, \bibinfo{person}{Xiaoqiang
  Zhu}, \bibinfo{person}{Chenru Song}, \bibinfo{person}{Ying Fan},
  \bibinfo{person}{Han Zhu}, \bibinfo{person}{Xiao Ma},
  \bibinfo{person}{Yanghui Yan}, \bibinfo{person}{Junqi Jin},
  \bibinfo{person}{Han Li}, {and} \bibinfo{person}{Kun Gai}.}
  \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{Deep interest network for click-through rate
  prediction}. In \bibinfo{booktitle}{\emph{Proceedings of the 24th ACM SIGKDD
  international conference on knowledge discovery \& data mining}}.
  \bibinfo{pages}{1059--1068}.
\newblock


\end{thebibliography}


%%
%% If your work has an appendix, this is the place to put it.
% \appendix
% \section{Experimental Setting}
% \subsection{Datasets} 
% \label{sec:Datasets}
% The statistical information for these datasets is summarized in Table \ref{table:The statistic of datasets}.
% \begin{table}[ht]
% \center
% \vspace{-1em}
% \caption{The statistic of datasets.}
% \vspace{-1em}
% \tiny
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{lccc}
% \toprule
%              & \textbf{\#Interaction}    & \textbf{\#User}   & \textbf{\#Item}   \\\midrule
% Amazon Books & 914,014& 694,897& 686,623\\ 
% MovieLen 1M (ML-1M) & 17,253,665&6,040&  3,883\\ 
% MovieLen 20M (ML-20M) &  9,810,868&138,493& 27,278 \\ 
% Industrial Dataset & 4 billion & 0.1 billion & 0.1 billion  \\ 
% \bottomrule
% \end{tabular}
% }
% % \vspace{-1em}
% \label{table:The statistic of datasets}
% \end{table}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
