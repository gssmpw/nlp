\section{Related Work}
\label{sec:related-work}

\subsection{Cold-start Active Learning (CSAL)}
\label{sec:csal-taxonomy}

According to the taxonomy of \citet{zhang-etal-2022-survey}, CSAL research for NLP can be categorized as informativeness-based, representativeness-based, and hybrid.
As most methods are hybrid, the techniques and challenges for informativeness or representativeness are elucidated below.

\subsubsection{Informativeness}
\label{subsubsec:informativeness}

\paragraph{Uncertainty}
The main metric for informativeness in CSAL is uncertainty, as it is more tractable in cold-start stages than others (\eg{} gradients).
High predictive uncertainty indicates difficulty for the model, thus valuable for annotation.
Most existing methods use language models (LMs) for estimation.
Common estimators include
entropy \citep{zhu-etal-2008-active,yu-etal-2023-cold},
LM probability \citep{dligach-palmer-2011-good},
LM loss \citep{yuan-etal-2020-cold}, and
probability margin \citep{10.1007/978-3-031-08473-7_9}.
However, several challenges exist in uncertainty estimation:
(a)
Often, a closed-world assumption is imposed.
In other words, predictions are normalized such that they sum to $1$.
This hinders the expression of uncertainty, as it forces mapping to one of the known classes, ignoring options such as ``none of the above'' \citep{ova-unc}.
(b) PLMs suffer from overconfidence \citep{park-caragea-2022-calibration,wang2023calibration}.
This requires calibration for more robust uncertainty estimation \citep{yu-etal-2023-cold}.
(c)
Task information is hardly considered.
As a result, the uncertainty will not be related to the downstream task (output uncertainty), but rather its intrinsic perplexity (input uncertainty) \citep{jiang-etal-2021-know}.
\patron{} \citep{yu-etal-2023-cold} uses task-related prompts to tackle this issue.

\subsubsection{Representativeness}
\label{subsubsec:repr}

\paragraph{Density}
To avoid outliers, density-based CSAL methods prefer ``typical'' instances.
The method of \citet{zhu-etal-2008-active} and TypiClust (\citealp{pmlr-v162-hacohen22a}) prioritize instances with high $k$NN density.
Uncertainty propagation \citep{yu-etal-2023-cold} is also useful in aggregating density information.
A typical group of uncertain examples indicates a region where the model's knowledge is lacking.

\paragraph{Discriminative}
Some CSAL methods acquire sequentially or iteratively.
They thus discriminate, \ie{} prefer an instance if it differs the most from selected ones.
Coreset selection \citep{sener2018active} selects an instance (cover-point) such that its minimum distance to selected instances is maximized.
\votek{} \citep{su2023selective} adopts a greedy approach to select remote instances on a $k$NN graph.

\paragraph{Batch diversity}
It is more efficient to acquire in batch mode \citep{settles.tr09}, \ie{} to select multiple instances at each step.
Clustering has been a common technique to enhance batch diversity and avoid redundancy in CSAL.
It helps structure the unlabeled dataset by grouping similar instances together.
\citet{10.1145/1015330.1015349} and \citet{10.1007/978-3-540-24775-3_46} first proposed \mbox{pre-clustering} the input space to select representatives from each cluster. %
\citet{dasgupta-ng-2009-mine} used spectral clustering on the similarity matrix of documents.
\citet{hu2010off} and \citet{8443399} used hierarchical clustering to stabilize the process.
\citet{zhu-etal-2008-active} and more recent works \citep{yuan-etal-2020-cold,chang-etal-2021-training,agarwal2021addressing,10.1007/978-3-031-08473-7_9,pmlr-v162-hacohen22a,yu-etal-2023-cold} have commonly used \kmeans{} for its simplicity and efficiency.
However, these clustering methods can be sensitive to outliers.
Moreover, clustering in the input space only contributes to textual diversity, regardless of other aspects.%

\subsection{Missed Cluster Effect}
The \mse{} \citep{10.1145/1183614.1183709,tomanek-etal-2009-proper} is an extreme case of class imbalance.
It refers to when an AL strategy neglects certain classes (or clusters within classes).
\citet{10.1145/1183614.1183709} first recognized the \mse{} in the context of text classification.
They suggested more use of domain knowledge. %
Knowledge extraction from PLMs is in harmony with this suggestion.
\citet{dligach-palmer-2011-good} proposed an uncertainty-based approach to avoid the \mse{} in word sense disambiguation (WSD). %
However, it is based on task-agnostic LM probability.
\citet{marcheggiani-artieres-2014-experimental} showed that labeling relevant instances, which reduces the labeling noise, also helps mitigate the \mse{}.
Label calibration aligns with this finding.
While many works are devoted to addressing the \mse{} or general class imbalance (\eg{} \citealp{9093475,fairstein-etal-2024-class}) for general AL, they often rely on a labeled subset. %
Class diversity enhancement would help mitigate class imbalance issues, but it remains an open question for CSAL.