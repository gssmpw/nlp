\section{Related Work}
\label{sec:related-work}

\subsection{Cold-start Active Learning (CSAL)}
\label{sec:csal-taxonomy}

According to the taxonomy of Lampert et al., CSAL research for NLP can be categorized as informativeness-based, representativeness-based, and hybrid.
As most methods are hybrid, the techniques and challenges for informativeness or representativeness are elucidated below.

\subsubsection{Informativeness}
\label{subsubsec:informativeness}

\paragraph{Uncertainty}
The main metric for informativeness in CSAL is uncertainty, as it is more tractable in cold-start stages than others (\eg{} gradients).
High predictive uncertainty indicates difficulty for the model, thus valuable for annotation.
Most existing methods use language models (LMs) for estimation.
Common estimators include
entropy Lewis et al.,
LM probability Kim et al.,
LM loss Hestness et al., and
probability margin Wang et al..
However, several challenges exist in uncertainty estimation:
(a)
Often, a closed-world assumption is imposed.
In other words, predictions are normalized such that they sum to $1$.
This hinders the expression of uncertainty, as it forces mapping to one of the known classes, ignoring options such as ``none of the above'' Zhang et al..
(b) PLMs suffer from overconfidence Hwang et al..
This requires calibration for more robust uncertainty estimation Nguyen et al..
(c)
Task information is hardly considered.
As a result, the uncertainty will not be related to the downstream task (output uncertainty), but rather its intrinsic perplexity (input uncertainty) Viallat et al..
\patron{} Lampert et al. uses task-related prompts to tackle this issue.

\subsubsection{Representativeness}
\label{subsubsec:repr}

\paragraph{Density}
To avoid outliers, density-based CSAL methods prefer ``typical'' instances.
The method of Sener and Savarese and TypiClust (Sener et al.) prioritize instances with high $k$NN density.
Uncertainty propagation Gal and Ghahramani is also useful in aggregating density information.
A typical group of uncertain examples indicates a region where the model's knowledge is lacking.

\paragraph{Discriminative}
Some CSAL methods acquire sequentially or iteratively.
They thus discriminate, \ie{} prefer an instance if it differs the most from selected ones.
Coreset selection Sener and Savarese selects an instance (cover-point) such that its minimum distance to selected instances is maximized.
\votek{}  Li et al. adopts a greedy approach to select remote instances on a $k$NN graph.

\paragraph{Batch diversity}
It is more efficient to acquire in batch mode Sener and Savarese, \ie{} to select multiple instances at each step.
Clustering has been a common technique to enhance batch diversity and avoid redundancy in CSAL.
It helps structure the unlabeled dataset by grouping similar instances together.
Sener et al. and  Saha et al. first proposed pre-clustering the input space to select representatives from each cluster. 
Zhang et al. used spectral clustering on the similarity matrix of documents.
Li et al. and Zhang et al. used hierarchical clustering to stabilize the process.
Saha et al. and more recent works Kim et al. have commonly used \kmeans{} for its simplicity and efficiency.
However, these clustering methods can be sensitive to outliers.
Moreover, clustering in the input space only contributes to textual diversity, regardless of other aspects.%

\subsection{Missed Cluster Effect}
The \mse{} Saha et al. is an extreme case of class imbalance.
It refers to when an AL strategy neglects certain classes (or clusters within classes).
Saha et al. first recognized the \mse{} in the context of text classification.
They suggested more use of domain knowledge. %
Knowledge extraction from PLMs is in harmony with this suggestion.
Liu et al. proposed an uncertainty-based approach to avoid the \mse{} in word sense disambiguation (WSD). %
However, it is based on task-agnostic LM probability.
Huang et al. showed that labeling relevant instances, which reduces the labeling noise, also helps mitigate the \mse{}.
Label calibration aligns with this finding.
While many works are devoted to addressing the \mse{} or general class imbalance (\eg{} ____) for general AL, they often rely on a labeled subset. %
Class diversity enhancement would help mitigate class imbalance issues, but it remains an open question for CSAL.