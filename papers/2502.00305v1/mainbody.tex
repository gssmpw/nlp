\section{Introduction}
\label{sec:intro}

Cold-start active learning (CSAL; \citealp{yuan-etal-2020-cold,zhang-etal-2022-survey})
has gained much attention for efficiently labeling large corpora from zero. %
Given an unlabeled corpus (\ie{} the ``{cold-start}'' stage), it aims to acquire a small \mbox{subset} (seed set) for annotation. %
Such absence of labels can happen due to \mbox{data privacy} concerns \citep{Holzinger2016,li2023privacy}, limited domain experts\footnote{Recent studies \citep{lu2023human,naeini2023large,zhang2023utilising} have shown that state-of-the-art PLMs still underperform human experts in difficult tasks.} \citep{WU2022364}, labeling difficulty \citep{9650877}, quick expiration of labels \citep{9101545,8654015}, \etc{}
In real-world tasks with specialized domains (\eg{} medical report classification with rare diseases; \citealp{DeAngeli2021}), the complete absence of labels and lack of \textit{a posteriori} knowledge pose challenges to CSAL.

While active learning (AL) has been studied for a wide range of NLP tasks \citep{zhang-etal-2022-survey}, the cold-start problem has been hardly addressed.
At the cold-start stage, the model is untrained and no labeled data are available for validation.
Traditional CSAL applies random sampling \citep{Ash2020Deep,margatina-etal-2021-active},
diversity sampling \citep{8443399,chang-etal-2021-training}, or
uncertainty sampling \citep{schroder-etal-2022-revisiting}.
However, random sampling suffers from high variance \citep{random_instability}; diversity sampling is prone to easy examples and vector space noise \citep{eklund-forsman-2022-topic}; uncertainty sampling is prone to redundant examples, outliers, and unreliable metrics \citep{WOJCIK2022109219}. %
Moreover, existing methods ignore class diversity, where the sampling bias often results in class imbalance \citep{sampling_bias_al}.
At worst, the \emph{\mse{}} \citep{10.1145/1183614.1183709,8443399} can happen, \ie{} clusters of weak classes are neglected. %
\citet{tomanek-etal-2009-proper} showed that an unrepresentative seed set gives rise to this effect.
Learning is misguided, if started unfavorably.

The key challenge for CSAL lies in how to acquire a diverse and informative seed set.
As a general heuristic \citep{DASGUPTA20111767}, a proper seed set should strike a balance between \mbox{{exploring}} the \emph{input space} for instance regions (\eg{} diversity sampling) and {exploiting} the \emph{version space} for decision boundaries (\eg{} uncertainty sampling).
Such hybrid CSAL strategies have been proposed based on combinations of
neighbor-awareness \citep{pmlr-v162-hacohen22a,su2023selective,yu-etal-2023-cold},
\mbox{clustering} \citep{yuan-etal-2020-cold,agarwal2021addressing,10.1007/978-3-031-08473-7_9,brangbour2022cold,shnarch-etal-2022-cluster,yu-etal-2023-cold}, and
uncertainty estimation \citep{dligach-palmer-2011-good,yuan-etal-2020-cold,10.1007/978-3-031-08473-7_9,yu-etal-2023-cold}. %
However, existing methods fail to explore the \emph{label space} to enhance class diversity and mitigate imbalance. %
Moreover, most methods perform diversity sampling followed by uncertainty sampling, treating both aspects in isolation.

\medskip
To address these challenges, this paper presents \mymethod{}, a dual-diversity enhancing and uncertainty-aware framework for CSAL. %
It adopts a graph-based hybrid strategy to enhance diversity and informativeness. %
Different from previous works, \mymethod{} not only emphasizes the diversity in textual contents (textual diversity), but also diversity in class predictions (class diversity).
This is termed \mbox{\emph{\bfseries dual-diversity}} in this paper.
To achieve this in the cold-start stage, it exploits the rich representational and predictive capabilities of PLMs.
For informativeness, the predictive uncertainty is estimated from a one-vs-all (OVA) perspective.
This helps mining informative ``hard examples'' for learning.
Then, \mymethod\ further employs manifold learning techniques \citep{mcinnes2018umap} to derive dual-diversity information.
This results in the novel construction of a Dual-Neighbor Graph (DNG).
Finally, \mymethod\ performs density-based uncertainty propagation and Farthest Point Sampling (FPS) on the DNG.
While propagation prioritizes \emph{\bfseries representatively uncertain} (RU) instances, FPS enhances the dual-diversity.
Overall, \mymethod{} ensures a more diverse and informative acquisition.

The merits of \mymethod{} are attributed to the following contributions:
\begin{itemize}[nosep]
    \item
    The dual-diversity enhancing and uncertainty aware (\mymethod{}) framework adopts a novel hybrid acquisition strategy.
    It effectively selects class-balanced and hard representative instances, achieving a good balance between exploration and exploitation in CSAL.

    \item
    This paper proposes a graph-based dual-diversity enhancement mechanism to select diverse instances with textual diversity and class diversity, tackling class imbalance in CSAL.

    \item
    This paper presents an embedding-based uncertainty-aware prediction mechanism to effectively select hard representative instances according to predictive uncertainty.

\end{itemize}

\section{Related Work}
\label{sec:related-work}

\subsection{Cold-start Active Learning (CSAL)}
\label{sec:csal-taxonomy}

According to the taxonomy of \citet{zhang-etal-2022-survey}, CSAL research for NLP can be categorized as informativeness-based, representativeness-based, and hybrid.
As most methods are hybrid, the techniques and challenges for informativeness or representativeness are elucidated below.

\subsubsection{Informativeness}
\label{subsubsec:informativeness}

\paragraph{Uncertainty}
The main metric for informativeness in CSAL is uncertainty, as it is more tractable in cold-start stages than others (\eg{} gradients).
High predictive uncertainty indicates difficulty for the model, thus valuable for annotation.
Most existing methods use language models (LMs) for estimation.
Common estimators include
entropy \citep{zhu-etal-2008-active,yu-etal-2023-cold},
LM probability \citep{dligach-palmer-2011-good},
LM loss \citep{yuan-etal-2020-cold}, and
probability margin \citep{10.1007/978-3-031-08473-7_9}.
However, several challenges exist in uncertainty estimation:
(a)
Often, a closed-world assumption is imposed.
In other words, predictions are normalized such that they sum to $1$.
This hinders the expression of uncertainty, as it forces mapping to one of the known classes, ignoring options such as ``none of the above'' \citep{ova-unc}.
(b) PLMs suffer from overconfidence \citep{park-caragea-2022-calibration,wang2023calibration}.
This requires calibration for more robust uncertainty estimation \citep{yu-etal-2023-cold}.
(c)
Task information is hardly considered.
As a result, the uncertainty will not be related to the downstream task (output uncertainty), but rather its intrinsic perplexity (input uncertainty) \citep{jiang-etal-2021-know}.
\patron{} \citep{yu-etal-2023-cold} uses task-related prompts to tackle this issue.

\subsubsection{Representativeness}
\label{subsubsec:repr}

\paragraph{Density}
To avoid outliers, density-based CSAL methods prefer ``typical'' instances.
The method of \citet{zhu-etal-2008-active} and TypiClust (\citealp{pmlr-v162-hacohen22a}) prioritize instances with high $k$NN density.
Uncertainty propagation \citep{yu-etal-2023-cold} is also useful in aggregating density information.
A typical group of uncertain examples indicates a region where the model's knowledge is lacking.

\paragraph{Discriminative}
Some CSAL methods acquire sequentially or iteratively.
They thus discriminate, \ie{} prefer an instance if it differs the most from selected ones.
Coreset selection \citep{sener2018active} selects an instance (cover-point) such that its minimum distance to selected instances is maximized.
\votek{} \citep{su2023selective} adopts a greedy approach to select remote instances on a $k$NN graph.

\paragraph{Batch diversity}
It is more efficient to acquire in batch mode \citep{settles.tr09}, \ie{} to select multiple instances at each step.
Clustering has been a common technique to enhance batch diversity and avoid redundancy in CSAL.
It helps structure the unlabeled dataset by grouping similar instances together.
\citet{10.1145/1015330.1015349} and \citet{10.1007/978-3-540-24775-3_46} first proposed \mbox{pre-clustering} the input space to select representatives from each cluster. %
\citet{dasgupta-ng-2009-mine} used spectral clustering on the similarity matrix of documents.
\citet{hu2010off} and \citet{8443399} used hierarchical clustering to stabilize the process.
\citet{zhu-etal-2008-active} and more recent works \citep{yuan-etal-2020-cold,chang-etal-2021-training,agarwal2021addressing,10.1007/978-3-031-08473-7_9,pmlr-v162-hacohen22a,yu-etal-2023-cold} have commonly used \kmeans{} for its simplicity and efficiency.
However, these clustering methods can be sensitive to outliers.
Moreover, clustering in the input space only contributes to textual diversity, regardless of other aspects.%

\subsection{Missed Cluster Effect}
The \mse{} \citep{10.1145/1183614.1183709,tomanek-etal-2009-proper} is an extreme case of class imbalance.
It refers to when an AL strategy neglects certain classes (or clusters within classes).
\citet{10.1145/1183614.1183709} first recognized the \mse{} in the context of text classification.
They suggested more use of domain knowledge. %
Knowledge extraction from PLMs is in harmony with this suggestion.
\citet{dligach-palmer-2011-good} proposed an uncertainty-based approach to avoid the \mse{} in word sense disambiguation (WSD). %
However, it is based on task-agnostic LM probability.
\citet{marcheggiani-artieres-2014-experimental} showed that labeling relevant instances, which reduces the labeling noise, also helps mitigate the \mse{}.
Label calibration aligns with this finding.
While many works are devoted to addressing the \mse{} or general class imbalance (\eg{} \citealp{9093475,fairstein-etal-2024-class}) for general AL, they often rely on a labeled subset. %
Class diversity enhancement would help mitigate class imbalance issues, but it remains an open question for CSAL.

\section{Methodology}
\label{sec:csac}

In this section, the methodology of the proposed \mymethod{} is introduced.
Section~\ref{subsec:problem-formulat} first defines CSAL and declares the notations for the rest of this paper.
The framework of \mymethod{} is then elaborated in Section~\ref{subsec:overview-method}.

\subsection{Problem Formulation}
\label{subsec:problem-formulat}

This paper considers CSAL in a pool-based manner. %
Learning is initiated with a set of $N$ unlabeled documents, $\documentset \coloneq \br{x_i}_{i=1}^N$.
A $C$-way text classification task is defined by a set of classes $\labelset \coloneq \br{y_j}_{j=1}^C$ taking values in a domain %
 $\mathbb{Y}$.

Given a labeling budget $b \ll N$, a CSAL strategy acquires a subset $\selected \subset \documentset$ with a fixed size $\abs{\selected} = b$, such that the labeled subset $\selected'$ boosts most performance when used as a training seed set.
The performance is evaluated by fine-tuning a PLM $\lm$ with $\selected'$, and testing for its accuracy.

\begin{figure*}[tbh]
    \includegraphics[width=\textwidth]{graph/framework_cropped.pdf}
    \caption{The proposed \mymethod{} framework.}
    \label{fig:diagram}
\end{figure*}

\subsection{The \mymethod{} Framework}
\label{subsec:overview-method}

The proposed \mymethod{} framework is illustrated in Figure~\ref{fig:diagram}.
Overall, the components of \mymethod\ serve the same goal---to produce a seed set with high dual-diversity and informativeness.

\subsubsection{Embedding Module}
\label{subsec:prompt-pred-unc}

In CSAL, data selection starts with only an \mbox{unlabeled} corpus.
\mymethod\ leverages PLM embeddings, which guide the selection process towards more diverse and informative samples.

Specifically, the embedding module implements a {prompt-based, verbalizer-free} approach \citep{jiang-etal-2022-promptbert}.
This requires only a single inference pass per document.%

\paragraph{Textual and predictive embedding}
In a masked PLM, the bidirectional semantics can be condensed into a \mask{} token.
In light of this, \mymethod{} extends \citet{jiang-etal-2022-promptbert}'s template with double \mask{} tokens:
$$
T_x \coloneq \fbox{%
\begin{varwidth}{\linewidth}%
    This sentence: ``\textph{}'' means \mask{}.\\
    Its \conph{} is \mask{}.
\end{varwidth}%
} \text{,}
$$
where \conph{} is the target domain $\mathbb{Y}$, such as ``sentiment''.
The hidden representations of \mask{} tokens are extracted as the textual $\vect{z}_{x_i}$ and predictive embeddings $\vect{z}_{\predss}$. %
They capture the intrinsic and task-related semantics.

However, raw embeddings suffer from template bias and length bias \citep{10.1145/3583780.3614833}.
\mymethod{} further applies \emph{template denoising} (\citealp{jiang-etal-2022-promptbert}%
) to obtain the denoised embeddings $\tilde{\vect{z}}$.

\paragraph{Class embedding}
Predictions need to be paired with the known classes.
Class embeddings $\tilde{\vect{z}}_{y_j}$ are generated from a prompt template $T_y$, similar to $T_x$:
$$
T_y \coloneq \fbox{This \conph{}: ``\catph{}'' means \mask{}.} \text{,}
$$
where \catph{} is the placeholder for a class $y_j$.

\subsubsection{Prediction Module}
\label{subsec:predict-module}

This module aims to produce uncertainty-aware labels.
With class information, \mymethod{} gains prior knowledge about potential data distributions.
With uncertainty information, \mymethod{} is informed of potential labeling gain.

\paragraph{Label vector}
For better uncertainty estimation,
\mymethod{} adopts a One-vs-All (OVA) setup, such that labels $\hat{\vect{y}}_i$ do not necessarily sum to $1$.
First, it computes the \mbox{inner product} $\simil_{ij}$ for each pair of predictive and class embeddings:
\begin{align*}
    \matr{\Simil} & = \begin{bmatrix}
        \tilde{\vect{z}}_{\hat{y} | \mysub{x}{1}} &
        \cdots &
        \tilde{\vect{z}}_{\hat{y} | \mysub{x}{N}}
    \end{bmatrix}^\trans \begin{bmatrix}
        \tilde{\vect{z}}_{\mysub{y}{1}} &
        \cdots &
        \tilde{\vect{z}}_{\mysub{y}{C}}
    \end{bmatrix} \\
    & \coloneq \begin{bmatrix}
        \simil_{ij}
    \end{bmatrix}_{i=1,j=1}^{N,C} \text{.}
\end{align*}

Ideally, similarity $\simil_{ij}$ can be linearly transformed to class label $\hat{y}_{ij}$.
However, high anisotropy \citep{gao2018representation} was observed in preliminary experiments. %
As a result, $\simil_{ij}$ has a non-uniform distribution over $[-1,1]$.
To tackle this issue, %
\mymethod{} uses the empirical distribution function (e.d.f.) of $\matr{\Simil}$ to give a calibrated estimate of labels $\hat{\matr{Y}}$:%
\begin{align*}
    \hat{y}_{ij} = \hat{\mathbb{F}}_{\matr{\Simil}}\!\pr{\simil_{ij}} = \frac{1}{NC} \sum_{m=1}^N \sum_{n=1}^C \ind{\simil_{mn} \le \simil_{ij}} \text{,}
\end{align*}
where $\ind{\cdot}$ is the indicator function.
This gives $\hat{y}_{ij} \sim U(0,1)$ regardless of the embedding distribution.

\paragraph{Predictive uncertainty}
In CSAL, uncertainty represents the difficulty of an instance.
\mymethod\ adapts entropy, a common measure of uncertainty (\S\ref{subsubsec:informativeness}).

In information theory, entropy is the expected self-information $I$ of possible events.
In an OVA setup, possible events $\br{E_{i}}$ are ``$x_i$ has a high predictive score for \emph{exactly one} class''.
The probability of event $E_i$ is given by \citet{WOJCIK2022109219}:
\begin{align*}
    p(E_i) = \max_{j} { \hat{y}_{ij} \prod_{\substack{l=1 \\ l\ne j}}^C \pr{1-\hat{y}_{il}} } \text{.}
\end{align*}
Therefore, \mymethod{} adopts the entropy from $\br{E_i}$ as the uncertainty estimate $\vect{u}$:
\begin{align*}
    u_i = I(E_i) = - \log p(E_i) \text{.}
\end{align*}

\subsubsection{Dual-Neighbor Graph (DNG) Module}
\label{subsec:dng}

Graphs serve as a powerful tool for data selection by explicitly modeling data interrelationship.
This enables the propagation of valuable information (\eg{} uncertainty) and the selection of more diverse samples.
To integrate textual and class diversity, \mymethod{} leverages manifold learning techniques \citep{mcinnes2018umap} on $k$-Nearest-Neighbor ($k$NN) graphs of both spaces.\footnote{It is worth noting that \mymethod{} does not utilize or optimize any Graph Neural Network (GNN). With the rich representational capability of PLMs, \mymethod{} does not require GNNs to learn data representations.}

\paragraph{$k$NN graph}

The use of $k$NN arises from the neighborhood perspective of diversity.
\mymethod{} aims to avoid selecting neighboring instances.
In a $k$NN graph, an instance $x_i$ is connected with its $k$ nearest neighbors $\br{\mysub{x_i}{j}}$ under some \mbox{distance function $\dist(\cdot, \cdot)$}.
Formally, the two metric spaces of $k$NN are defined as follows.
\begin{itemize}[noitemsep]
    \item The textual space $(\documentset, \dist_{\tilde{z}})$ is defined by textual embeddings under cosine distance, $\dist_{\tilde{z}}(x_i,x_j) = \smash[t]{ \frac{1}{\pi} \arccos\!\pr{\tilde{\vect{z}}_{x_i}^{\trans} \tilde{\vect{z}}_{x_j}^{\vphantom{\trans}}} }$;
    \item The label space $(\documentset, \dist_{\hat{y}})$ is defined by label vectors under $\ell_1$ distance, $\dist_{\hat{y}} (x_i,x_j) = \norm{\hat{\vect{y}}_i - \hat{\vect{y}}_j}_1$.
\end{itemize}
The $k$NN graph from each space is denoted by $\graph_{\tilde z}$ and $\graph_{\hat y}$, respectively.

\paragraph{Graph normalization}
To unify textual and class diversity, \mymethod{} merges the two $k$NN graphs into one for graph-based sampling.
However, across two distinct spaces, it is necessary to first normalize the distances \citep{mcinnes2018umap}.

To ease notation, this part omits the subscript as $\graph \in \br{\graph_{\tilde z}, \graph_{\hat y}}$.
For each $x_i$, \mymethod{} finds a normalization factor $\normfac_i > 0$ that satisfies the equation
\begin{align*}
    \sum_{j=1}^k \exp\!\pr{-\frac{\dist\!\pr{x_i,\mysub{x_i}{j}} - \rho_i}{\normfac_i}} = \log_2\!k \text{,}
\end{align*}
where $\rho_i$ denotes $x_i$'s distance to its nearest neighbor.
The weights $\tilde{w}$ of the normalized (directed) $k$NN graph $\graph$, denoted by $\tilde{\graph}$, is defined by
\begin{align*}
    \tilde{w}\!\pr{\abr{x_i,\mysub{x_i}{j}}} \coloneq \exp\!\pr{-\frac{\dist\!\pr{x_i,{x_i}_{{}_j}} - \rho_i}{\normfac_i}} . %
\end{align*}
After normalization, the original $k$NN weights $w \in [0, \infty)$ are transformed to $\tilde{w} \in (0,1]$.

\paragraph{Symmetrization}
To identify representative instances, \mymethod{} performs graph clustering.
This requires symmetric $k$NN graphs.

Let $\tilde{\matr{W}}$ denote the sparse weight matrix of $\tilde{\graph}$.
Since weights $\tilde{w} \in [0,1]$, they can be interpreted as fuzzy memberships of neighborhood.
Hence, symmetrizing $\tilde{\matr{W}}$ is equivalent to finding the fuzzy union \citep{dubois:hal-04067331} of the neighbors $\tilde{\matr{W}}$ and reverse neighbors $\tilde{\matr{W}}^\trans$:
\begin{align*}
\tilde{\matr{W}}_\text{sym} = \tilde{\matr{W}} + \tilde{\matr{W}}^\trans - \tilde{\matr{W}} \hadprod \tilde{\matr{W}}^\trans \text{,}
\end{align*}
where $\hadprod$ is the Hadamard product.
$\tilde{\matr{W}}_\text{sym}$ defines the weights of the symmetric $k$NN graph $\tilde{\graph}_\text{sym}$.
Its edges are denoted by $\tilde{\edges}_\text{sym}$.

\newcommand{\myedge}{\br{x_i,x_j}}
\newcommand{\ezsym}{\tilde{\edges}_{\tilde{z}, \text{sym}}}
\newcommand{\eysym}{\tilde{\edges}_{\hat{y}, \text{sym}}}
\newcommand{\wzsym}{\tilde{w}_{\tilde{z}, \text{sym}}}
\newcommand{\wysym}{\tilde{w}_{\hat{y}, \text{sym}}}
\newcommand{\myindent}{\hphantom{\wzsym}}
\paragraph{Merging}
It is now appropriate to merge the two $k$NN graphs.
This unifies textual and class diversity in one graph.

As merged, the {Dual-Neighbor Graph} (DNG) is an undirected graph $\graph_\text{dual} = (\vertices, \edges, w_\text{dual})$.
The edges $\edges$ are the union of edges in $\tilde{\graph}_{\tilde{z}, \text{sym}}$ and $\tilde{\graph}_{\hat{y}, \text{sym}}$.
Moreover, $\edges$ is divided into two types:
\begin{itemize}[nosep]
    \item $\edges_1$ represents edges which only appear in \mbox{either} $k$NN graph, called \emph{single-neighbor edges};
    \item $\edges_2$ represents edges which appear in both $k$NN graphs, called \emph{dual-neighbor edges}. They connect neighboring documents which are similar in {both} textual semantics and class predictions.
\end{itemize}
The weight $w_\text{dual}$ of an undirected edge $\myedge \in \mathcal{E}$ is thereby defined as
\begin{align*}
    w_\text{dual} \coloneq \begin{cases}
        \wzsym \wysym + \gamma \\ \myindent \text{if }
        \myedge \in \edges_2 \text{,} \\
        \wzsym \\ { \myindent \text{if }
        \myedge \in \smash[b]{\color{gray}\underbrace{\color{black} \ezsym \setminus \eysym }_{ \subset~\edges_1 } }} \text{,} \\
        \wysym  \\ \myindent \text{if }
        \myedge \in \smash[b]{\color{gray}\underbrace{\color{black} \eysym \setminus \ezsym }_{ \subset~\edges_1 }} \text{;}
    \end{cases}
    \vphantom{%
    \begin{cases}%
        \wzsym \wysym + \gamma \\ \myindent \text{if }
        \myedge \in \edges_2 \\
        \wzsym \\ { \myindent \text{if }
        \myedge \in \ezsym \setminus \eysym \subset \edges_1} \\
        \wysym  \\ \myindent \text{if }
        \myedge \in \underbrace{ \eysym \setminus \ezsym }_{ \subset \edges_1 }
    \end{cases}
    }
\end{align*}
where $\gamma$ is a threshold to distinguish dual-neighbor edges $\edges_2$ from single-neighbor edges $\edges_1$.
In essence, DNG assigns greater weights to dual-neighbor edges.
As a result, during the subsequent graph clustering and traversal, \mymethod{} can avoid selecting textual and class neighbors.

\subsubsection{Acquisition Module}
\label{subsec:dsdc}

\mymethod{} adopts a hybrid acquisition strategy.
Overall, the goal is to produce a diverse and informative seed set.
To achieve this, the acquisition module performs graph clustering, propagation, and traversal on DNG.

\paragraph{\hdbscan{}}

A group of similar documents with high predictive uncertainty indicates an area where the model's knowledge is lacking.
By labeling one of the documents, the model predictions can be improved for similar ones in the area.
Therefore, it is valuable to identify and prioritize such {representatively uncertain} (RU) groups for CSAL.

Clustering has been a common technique to group similar instances (\S\ref{subsubsec:repr}).
However, traditional clustering methods (\eg\ \kmeans{}) are ill-suited, as the number of RU groups is unknown.
Moreover, they force every instance into a cluster, while some instances may not belong to any RU group.
Instead, \mymethod{} adopts density-based clustering, %
which identifies RU groups with a sufficient density ($\ge k_r$ similar documents).

Specifically, \mymethod{} applies \hdbscan{} \citep{10.1007/978-3-642-37456-2_14,10.1145/2733381} on the DNG, with minimum cluster size $k_r$. %
A document $x_i$ is either (a) clustered in an RU group $c_l$ with membership $p_i$, or (b) excluded as a non-RU outlier.

\paragraph{Uncertainty propagation}
To prioritize RU documents, uncertainty information (\S\ref{subsec:predict-module}) is propagated and aggregated in RU groups.
This is formulated as a single step of message propagation:
\begin{align*}
    \tilde{u}_i = u_i + \sum_{x_j \in c_l \setminus \br{x_i}}{w_{\text{dual}}\!\pr{\br{x_i,x_j}} p_j u_j} \text{.}
\end{align*}

\paragraph{FPS}
The final acquisition adopts a combination of diversity sampling and uncertainty sampling.
First, \mymethod{} runs Farthest Point Sampling (FPS; \citealp{577129}) on the DNG. %
As the result only depends on the initial point, FPS is started from documents $x_i$ with top-$k$ degrees.
Each produces a candidate seed set $\smash[t]{\candsel^{(i)}}$, which contains $b$ dually diverse samples.
Finally, \mymethod{} chooses the candidate with the highest propagated uncertainty:
\begin{align*}
    \selected = \argmax_{\candsel^{(i)}} \sum_{x_j \in \candsel^{(i)}} \tilde{u}_j.
\end{align*}

The whole process is described in Algorithm~\ref{alg:cs}.
\input{mainalg}

\input{tables/datasets}

\section{Experiments and Results}
\label{sec:exp}

\subsection{Experimental Setup}

\paragraph{Datasets}
\label{subsec:datasets}

\mymethod{} is evaluated on six text classification datasets:
{IMDb} \citep{maas-etal-2011-learning},
{\yelpfull} \citep{Meng_Shen_Zhang_Han_2019},
{AG's News} \mbox{\citep{NIPS2015_250cf8b5}},
{{Yahoo}! Answers} \citep{NIPS2015_250cf8b5},
{DBpedia} \citep{Lehmann2015}, and
{TREC} \citep{li-roth-2002-learning}.
Dataset statistics are shown in Table~\ref{table:dataset-stats}.
All the datasets used in the experiments are publicly accessible.
The original labels are removed to create a cold-start scenario.

\paragraph{Evaluation metric}

To evaluate the performance of the acquired seed set $\selected$, it is labeled and used for fine-tuning the PLM.
The original labels of the seed set are revealed.
The accuracy of the fine-tuned PLM on the test set is then reported.
To be consistent with previous methods \citep{yu-etal-2023-cold}, the experiments adopt RoBERTa-base \citep{liu2019roberta} as the backbone PLM. %

\paragraph{Analysis metrics}

To analyze the effect of dual-diversity enhancement, the class imbalance (IMB) and textual-diversity value of seed sets are reported.
Both metrics are computed under budget $b=128$.
IMB \citep{yu-etal-2023-cold} is defined as:
\begin{align*}
    \mathsf{IMB} = \frac{\max_{j=1}^C n_j}{\min_{j=1}^C n_j} \text{,}
\end{align*}
where $n_j$ is the number of instances from class $y_j$.
Textual-diversity value \citep{ein-dor-etal-2020-active,yu-etal-2023-cold} is defined as:
\begin{align*}%
    D = \pr{\frac{1}{ \abs{\documentset \setminus \selected} } \sum_{x_i \in \documentset \setminus \selected}{ \min_{x_j \in \selected} {\Delta\!\pr{x_i,x_j}} }}^{-1} \text{,}
\end{align*}
where $\Delta\!\pr{x_i,x_j}$ is the Euclidean distance of SimCSE embeddings \citep{gao-etal-2021-simcse} of $x_i$ and $x_j$.

\paragraph{Implementation details}
The fine-tuning setup and hyperparameters are the same as \patron{} \citep{yu-etal-2023-cold}'s.
Notably, the experiment code transplants the original implementation of graph normalization (\citealp{McInnes2018}) to GPU for acceleration.
For \mymethod{}, $k=500$, $k_r=3$, and $\gamma=1.0$ (since $\tilde{w}_\text{sym} \le 1.0$) are taken.
All experiments are run on a machine with a single \textsc{nvidia}\registered{} A800 GPU with 80~GB of VRAM.

\paragraph{Baselines}
\label{subsec:baselines}

The following CSAL baseline methods are considered:
\input{tables/methods}

\begin{itemize}[nosep]
    \item \textbf{Random} sampling selects uniformly.%
    \item \textbf{Entropy}-based {uncertainty} sampling (revisited by \citealp{schroder-etal-2022-revisiting})
    selects data with the highest predictive entropy.
    \item \textbf{Coreset} selection \citep{sener2018active}
    iteratively selects data whose minimum distance to the selected data is maximized.
    \item \textbf{\alps} \citep{yuan-etal-2020-cold}
    computes \emph{surprisal embeddings} from BERT loss as uncertainty. They are then clustered with \kmeans{}. %
    Data closest to each centroid are selected.
    \item \textbf{\textsc{few-selector}} (\citealp{chang-etal-2021-training})
    clusters the text embeddings with \kmeans{}.%
    \item \textbf{TypiClust} (\citealp{pmlr-v162-hacohen22a})
    clusters the text embeddings with \kmeans, and selects data with the highest {typicality}, \ie{} $k$NN density, from each cluster.
    \item \textbf{\patron} \citep{yu-etal-2023-cold}
    clusters the text embeddings with \kmeans{}, and selects from each cluster data with the highest propagated uncertainty.
    It then iteratively updates the set to refine inter-sample distances.
    \item \textbf{\votek} \citep{su2023selective}
    iteratively assigns a high score if a data is far from selected data.%
\end{itemize}
Comparisons of the CSAL baselines and \mymethod{} are presented in Table~\ref{table:baselines}.

\subsection{Accuracy Improvement}
\input{tables/results}
The main quantitative results of PLM fine-tuning performance with \mymethod{} and baseline CSAL methods are shown in Table~\ref{table:exp}.
Results for baselines other than \votek{} are from \citet{yu-etal-2023-cold}.
To report the standard deviation, each setup is repeated with 10 different random seeds.
Figure~\ref{fig:tsne} demonstrates a qualitative visualization of the $b=128$ seed set from IMDb dataset, acquired by the latest baseline method \votek{} and the proposed \mymethod{}.
The \tsne{} \citep{tsne} method is used for visualization.

\begin{figure}[t]
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\textwidth]{graph/tsne-votek.png}
        \caption{\votek{}.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=\textwidth]{graph/tsne-cadru.png}
        \caption{The proposed \mymethod{}.}
    \end{subfigure}
    \caption{The \tsne{} visualization of the acquired seed set ($b=128$) on IMDb dataset. Text embeddings are colored by their true labels.}
    \label{fig:tsne}
\end{figure}

From results in Table~\ref{table:exp}, it can be seen that {\mymethod{}} consistently outperforms other baselines, achieving up to a $2.5\%$ gain on balanced datasets and up to $6.2\%$ on the imbalanced dataset, TREC.
{\mymethod{}} mainly benefits from that it enhances the class diversity as well as textual diversity. %
This can be concluded from the larger improvements on TREC.
In over half of the setups, \mymethod{} also achieves the lowest standard deviation.
In addition, \mymethod{} improves most when $b$ is small.
This aligns with the fundamental goal of AL, which is to maximize performance gains with minimal labeled data.
Furthermore, from the visualization in Figure~\ref{fig:tsne}, it can be seen that {\mymethod{}}'s enhancement of dual-diversity leads to a broader and more balanced coverage of both input space and label space. %
As \mymethod{} adopts a highest-uncertainty strategy, such coverage also exhibits high predictive uncertainty, thus including more ``hard examples'' which are valuable for annotation.

\begin{table*}[htb]
    \centering
    \begin{adjustbox}{width=\textwidth}
        \small
        \begin{tabular}{c|ccc|ccccc|c}
            \toprule
            \textbf{Dataset} & \textbf{Random} & \textbf{Entropy} & \textbf{Coreset} & \textbf{\alps} & \textbf{\textsc{few-s.}} & \textbf{TypiCl.} & \textbf{\patron} & \textbf{\votek} & \textbf{\mymethod{}} \\
            \mymidline
            IMDb & 1.207 & 6.111 & 1.000 & 1.783 & 1.286 & 2.765 & 1.286 & 1.065 & 1.169 \\
            \yelpfull & 1.778 & 3.800 & 6.000 & 2.833 & 2.000 & 5.200 & 2.250 & 1.273 & 1.450 \\
            AG's News & 1.462 & 28.000 & 2.000 & 1.667 & 1.500 & 1.818 & 1.500 & 2.200 & 1.133 \\
            Yahoo! Answers & 3.000 & 12.000 & 7.000 & 5.500 & 2.250 & 3.333 & 5.500 & 3.333 & 2.125 \\
            DBpedia & 3.500 & $\infty$ & 9.000 & 9.000 & 3.500 & 9.000 & 2.333 & 2.800 & 3.250 \\
            TREC & 8.000 & 16.000 & $\infty$ & 9.500 & 10.500 & 21.000 & 15.000 & 11.333 & 6.000 \\
            \mymidline
            \textit{Harmonic avg.} & 2.128 & 9.863 & 3.124 & 3.138 & 2.166 & 3.839 & 2.338 & \uline{2.052} & \textbf{1.779} \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{Label imbalance value (IMB) of acquired seed sets ($b=128$). Smaller value indicates better class diversity and balance. An IMB of $\infty$ indicates that the \mse{} happens.
    }
    \label{table:analysis-imb}
\end{table*}

\begin{table*}[t]
    \centering
    \begin{adjustbox}{width=\textwidth}
        \small
        \begin{tabular}{c|ccc|ccccc|c}
            \toprule
            \textbf{Dataset} & \textbf{Random} & \textbf{Entropy} & \textbf{Coreset} & \textbf{\alps} & \textbf{\textsc{few-s.}} & \textbf{TypiCl.} & \textbf{\patron} & \textbf{\votek} & \textbf{\mymethod{}} \\
            \mymidline
            IMDb & 0.646 & 0.647 & 0.643 & 0.647 & 0.687 & 0.648 & 0.684 & 0.669 & 0.670 \\
            \yelpfull & 0.645 & 0.626 & 0.456 & 0.680 & 0.685 & 0.677 & 0.685 & 0.657 & 0.679 \\
            AG's News & 0.354 & 0.295 & 0.340 & 0.385 & 0.436 & 0.376 & 0.423 & 0.370 & 0.448 \\
            Yahoo! Answers & 0.430 & 0.375 & 0.400 & 0.441 & 0.470 & 0.438 & 0.486 & 0.451 & 0.491 \\
            DBpedia & 0.402 & 0.316 & 0.381 & 0.420 & 0.461 & 0.399 & 0.459 & 0.434 & 0.476 \\
            TREC & 0.301 & 0.298 & 0.298 & 0.339 & 0.337 & 0.326 & 0.338 & 0.346 & 0.353 \\
            \mymidline
            \textit{Average} & 0.463 & 0.426 & 0.420 & 0.485 & \uline{0.513} & 0.477 & 0.512 & 0.488 & \textbf{0.520} \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{Textual diversity value $D$ of acquired seed sets ($b=128$). Larger values indicate better textual diversity.
    }
    \label{table:analysis-div}
\end{table*}

\subsection{Enhancement of Class Diversity}

To verify the enhancement of class diversity, the class imbalance value (IMB; \citealp{yu-etal-2023-cold}) under $b=128$ is reported in Table~\ref{table:analysis-imb}.

From Table~\ref{table:analysis-imb}, it can be seen that \mymethod{} achieves the lowest average IMB value.
This indicates that \mymethod{} enhances class diversity properly.
In contrast, an IMB of $\infty$ emerges in the pure uncertainty-based (Entropy) and textual-diversity-based (Coreset) method.
This indicates the \mse{} happens in their acquisition.

\subsection{Enhancement of Textual Diversity}

To measure the textual diversity of seed sets, the textual-diversity value \citep{ein-dor-etal-2020-active,yu-etal-2023-cold} under $b=128$ is reported in Table~\ref{table:analysis-div}.

Table~\ref{table:analysis-div} shows that \mymethod{} also achieves the highest average textual-diversity value.
This indicates that \mymethod{} also enhances textual diversity properly.
The improvement of textual-diversity value is not significant, compared to IMB value's (Table~\ref{table:analysis-imb}).
This signals that \mymethod{} enhances more of class diversity than textual diversity, compared to other baselines.
Such difference can be explained by the highest-uncertainty-candidate strategy, which acquires more information from the label space.

\subsection{Quality of Textual Embedding}

To analyze the quality of \mymethod{}'s prompt-based, \mbox{unsupervised} text embeddings $\tilde{\vect{z}}_{x_i}$ (\S\ref{subsec:prompt-pred-unc}),
they are compared with the supervised Sentence Transformer embeddings (\citealp{paraphrase-mpnet-base-v2}) used in \votek{} \citep{su2023selective}.
The correlations are computed across all the possible $\binom{N}{2}$ pairs of their cosine similarity.\footnote{Semantic similarity benchmarks (\eg{} STS) cannot be used here, as the prompt $T_x$ requires a task domain $\mathbb{Y}$.
    }
Results on three datasets are reported in Table~\ref{table:analysis-zx}.

\begin{table}[ht]
    \small
    \centering
    \begin{tabularx}{\linewidth}{X|cc}
        \toprule
        \textbf{Dataset} & \begin{tabular}[c]{@{}c@{}}\textbf{Pearson}\\\textbf{correlation $r$}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Spearman}\\\textbf{correlation $\rho$}\end{tabular} \\
        \mymidline
        IMDb & 0.1651 & 0.1636 \\
        {\small \quad{}\textit{w/ denoising}} & {\bf 0.1980} & {\bf 0.1889} \\
        \mymidline
        \yelpfull{} & 0.1424 & 0.1440 \\
        {\small \quad{}\textit{w/ denoising}} & {\bf 0.3072} & {\bf 0.2984} \\
        \mymidline
        TREC & 0.4271 & 0.4000 \\
        \small \quad{}\textit{w/ denoising} & \bf 0.4662 & \bf 0.4368 \\
        \bottomrule
    \end{tabularx}
    \caption{The quality of textual embeddings, \mbox{without} and with template denoising \citep{jiang-etal-2022-promptbert}. Both correlation metrics are over $[-1,1]$; higher values indicate better quality.
    }
    \label{table:analysis-zx}
\end{table}

From Table~\ref{table:analysis-zx}, a weak positive correlation is observed.
Moreover, template denoising produces better embeddings, as it removes the biases from raw embeddings.
Overall, the quality of textual embeddings is acceptable and adequate for cold-start acquisition.

\subsection{Quality of Class Prediction}

To analyze the quality of embedding-based class prediction $\hat{\vect{y}}_{i}$ (\S\ref{subsec:predict-module}),
they are compared with gold labels.
As uncertainty indicates unstable predictions, labels are arranged from the most confident (lowest $u_i$) to the least.
Results are demonstrated in Figure~\ref{fig:analysis-yhat}.

\begin{figure}[ht!]
    \includegraphics[width=\columnwidth]{graph/label_analysis.pdf}
    \caption{The quality of class predictions with respect to predictive uncertainty $u_i$.
    Dataset: IMDb (left) and TREC (right).
    }
    \label{fig:analysis-yhat}
\end{figure}

From Figure~\ref{fig:analysis-yhat}, a high accuracy of class predictions is consistently observed with high confidence and with denoised embeddings, and vice versa.
This demonstrates the good quality of e.d.f. predictions and the derived uncertainty metric.

\section{Discussion}

\subsection{Comparison with LLM-based Methods}

The landscape of NLP is rapidly evolving with generative large language models (LLMs).
This section evaluates two potential LLM-based alternatives to \mymethod{}: serialization for acquisition and zero-shot Chain-of-Thought prompting.
The following experiments are conducted with \llama{} \citep{touvron2023llama}.

\subsubsection{Serialization for Acquisition}

Inspired by the work of \citet{pmlr-v206-hegselmann23a}, class and uncertainty information can be serialized into natural language for LLM-based acquisition.
The process is designed to involve three passes.
In the first pass, each unlabeled text is formalized as a multiple-choice problem for LLM.
The prompt template $T_1$ is used to collect class and uncertainty information:
\begin{align*}
\begin{adjustbox}{max width=\columnwidth}
        $T_1 \coloneq \fbox{%
            \begin{varwidth}{\linewidth}%
                This sentence: ``\textph{}'' What is its \conph{}? \\
                Answer Choices: (A) [\textsc{class A}] (B) ... \\
                Answer: (
            \end{varwidth}%
        }$
\end{adjustbox}
\end{align*}
In the second pass, LLM decides on whether each text should be selected.
Predictive uncertainty is estimated by the entropy of first-pass predictions, bounded by $\log{C}$.
The extended template $T_2$ is used to combine multiple information:
\begin{align*}
\begin{adjustbox}{max width=\columnwidth}
    $T_2 \coloneq \fbox{%
        \begin{varwidth}{\linewidth}%
            This sentence: ``\textph{}'' What is its \conph{}? \\ Answer Choices: (A) [\textsc{class A}] (B) ... \\
            Answer: ([\textsc{answer}]) [\textsc{class}] \\
            Uncertainty: [\textsc{uncertainty} \%] \\
            Is it valuable for annotation? Yes or no? \\
            Answer:
        \end{varwidth}%
    }$
\end{adjustbox}
\end{align*}
In the third pass, texts with top-$b$ probabilities of $T_2$ answered ``yes'' are selected as the seed set.
LLM is then fine-tuned with the seed set under $T_1$.
Finally, $T_1$ is applied on the fine-tuned LLM to report the test set accuracy.

Due to resource constraints, LoRA (\citealp{hu2022lora}) is used for fine-tuning, with $r=\alpha=64$.
Results are reported in Table~\ref{table:llm-serial-main}.
Despite utilizing a mid-sized PLM, \mymethod{} outperforms serialization with LLM in most datasets.
The decision process of LLM is also black-box.
In contrast, \mymethod{} adopts graphs to explicitly capture the interplay of information, offering better interpretability.

\begin{table*}[t]
    \centering
    \begin{adjustbox}{}%
        \begin{tabular}{rc|cccccc!{\vrule width \lightrulewidth}c}
            \toprule
            \textbf{Method} & $b$ & \textbf{IMDb} & \textbf{\yelpfull} & \textbf{AG's News} & \textbf{Yahoo!} & \textbf{DBpedia} & \textbf{TREC} & \textit{\textbf{Average}} \\
            \midrule
            \multirow{3}{*}{Serialization} & 32 & 81.7 & \textbf{44.5} & 25.2 & 38.8 & 62.6 & 28.4 & 46.9 \\
            & 64 & 83.8 & \textbf{51.2} & 53.4 & 55.7 & 45.9 & 27.8 & 53.0 \\
            & 128 & 89.6 & \textbf{56.9} & 83.7 & 63.4 & 58.6 & 35.6 & 64.6 \\
            \midrule
            \multirow{3}{*}{\mymethod{}} & 32 & \textbf{86.9} & 42.6 & \textbf{83.7} & \textbf{58.0} & \textbf{86.0} & \textbf{70.2} & \textbf{71.2} \\
            & 64 & \textbf{88.5} & 49.8 & \textbf{86.3} & \textbf{62.8} & \textbf{94.1} & \textbf{82.2} & \textbf{77.3} \\
            & 128 & \textbf{90.0} & 53.4 & \textbf{87.5} & \textbf{66.2} & \textbf{97.3} & \textbf{92.1} & \textbf{81.1} \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{
        Fine-tuning results of \mymethod{} (RoBERTa-base) and LLM serialization (\llama{}).
    }
    \label{table:llm-serial-main}
\end{table*}

\begin{table*}[ht]
    \centering
    \begin{adjustbox}{}%
        \begin{tabular}{r!{\vrule width \lightrulewidth}cccccc!{\vrule width \lightrulewidth}c}
            \toprule
            \textbf{Method} & \textbf{IMDb} & \textbf{\yelpfull} & \textbf{AG's News} & \textbf{Yahoo!} & \textbf{DBpedia} & \textbf{TREC} & \textbf{\textit{Average}} \\
            \midrule
            0-shot CoT, w/o choices & 63.6 & \hphantom{0}9.2 & 34.7 & 23.7 & 37.1 & 12.6 & 32.0 \\
            0-shot CoT, w/ choices & 72.1 & 25.4 & 60.2 & 43.6 & 32.3 & 24.2 & 43.0 \\
            \midrule
            \mymethod{}, $b=32$ & \textbf{86.9} & \textbf{42.6} & \textbf{83.7} & \textbf{58.0} & \textbf{86.0} & \textbf{70.2} & \textbf{71.2} \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{
        Evaluation results of \mymethod{} ($b=32$, RoBERTa-base) and zero-shot Chain-of-Thought prompting (\citealp{NEURIPS2022_8bb0d291}; \llama{}).
    }
    \label{table:0shotCoT-no-choices-main}
\end{table*}

\begin{table}[t]
    \centering
    \begin{adjustbox}{max width=\columnwidth}
        \begin{tabular}{r!{\vrule width \lightrulewidth}cc!{\vrule width \lightrulewidth}cc}
            \toprule
            \multirow{2}{*}{\textbf{Stage}} & \multicolumn{2}{c!{\vrule width \lightrulewidth}}{\textbf{0-shot CoT }} & \multicolumn{2}{c}{\textbf{\mymethod{} }} \\
            & \begin{tabular}[c]{@{}c@{}}Energy\\(kJ)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Time\\(sec)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Energy\\(kJ)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Time\\(sec)\end{tabular} \\
            \midrule
            Acquisition & \multicolumn{2}{c!{\vrule width \lightrulewidth}}{-} & \hphantom{0}59.82 & \hphantom{0}81.00 \\
            Fine-tuning & \multicolumn{2}{c!{\vrule width \lightrulewidth}}{-} & 225.77 & 208.89 \\
            Prediction & 2561.58 & 1967.23 & \hphantom{0}41.99 & \hphantom{0}24.27 \\
            \midrule
            \textit{Total} & 2561.58 & 1967.23 & \textbf{327.58} & \textbf{314.16} \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{
        Energy consumption and time usage of \mymethod{} ($b=32$, RoBERTa-base) and zero-shot Chain-of-Thought prompting (\citealp{NEURIPS2022_8bb0d291}; \llama{}), under the same data amount of 25000.
    }
    \label{table:energy-main}
\end{table}

\begin{table*}[ht]
\centering
\begin{adjustbox}{width=\textwidth}%
    \begin{tabular}{rc!{\vrule width \lightrulewidth}cccccc!{\vrule width \lightrulewidth}c}
        \toprule
        \mymethod{} & $b$ & \textbf{IMDb} & \textbf{\yelpfull} & \textbf{AG's News} & \textbf{Yahoo!} & \textbf{DBpedia} & \textbf{TREC} & \textbf{\textit{Average}} \\
        \midrule
        \multirow{3}{*}{w/o noise} & 32 & \msd{86.9}{0.9} & \msd{42.6}{1.1} & \msd{83.7}{0.8} & \msd{58.0}{1.5} & \msd{86.0}{1.7} & \msd{70.2}{1.7} & \msd{71.2}{1.3} \\
        & 64 & \msd{88.5}{0.7} & \msd{49.8}{1.2} & \msd{86.3}{0.6} & \msd{62.8}{1.3} & \msd{94.1}{0.9} & \msd{82.2}{1.5} & \msd{77.2}{1.1} \\
        & 128 & \msd{90.0}{0.3} & \msd{53.4}{0.7} & \msd{87.5}{0.4} & \msd{66.2}{0.9} & \msd{97.3}{0.3} & \msd{92.1}{0.8} & \msd{81.1}{0.6} \\
        \midrule
        \multirow{3}{*}{w/ noise} & 32 & \msd{67.8}{4.3} & \msd{38.7}{3.0} & \msd{72.5}{1.0} & \msd{49.7}{7.2} & \msd{61.5}{2.0} & \msd{69.6}{0.6} & \msd{60.0}{1.5} \\
        & 64 & \msd{83.4}{1.3} & \msd{41.0}{2.7} & \msd{82.6}{1.4} & \msd{53.4}{2.7} & \msd{87.5}{3.3} & \msd{78.7}{3.3} & \msd{71.1}{1.1} \\
        & 128 & \msd{82.9}{6.3} & \msd{45.1}{1.7} & \msd{84.7}{2.4} & \msd{62.7}{1.3} & \msd{89.2}{3.7} & \msd{82.5}{3.8} & \msd{74.5}{1.5} \\
        \bottomrule
    \end{tabular}
\end{adjustbox}
\caption{
    Evaluation results of \mymethod{}, compared under an expected labeling noise level of 7\%.
}
\label{table:noisylabel-main}
\end{table*}

\begin{table*}[ht]
    \centering
    \begin{adjustbox}{width=\textwidth}%
        \begin{tabular}{rc!{\vrule width \lightrulewidth}cccccc!{\vrule width \lightrulewidth}c}
            \toprule
            & $b$ & \textbf{IMDb} & \textbf{\yelpfull} & \textbf{AG's News} & \textbf{Yahoo!} & \textbf{DBpedia} & \textbf{TREC} & \textbf{\textit{Average}} \\
            \midrule
            \multirow{3}{*}{Coreset} & 32 & \msd{74.5}{\bf2.9} & \msd{32.9}{2.8} & \msd{78.6}{\bf1.6} & \msd{22.0}{\bf2.3} & \msd{64.0}{2.8} & \msd{47.1}{\bf3.6} & \msd{53.2}{2.7} \\
            & 64 & \msd{82.8}{\bf2.5} & \msd{39.9}{3.4} & \msd{82.0}{1.5} & \msd{45.7}{3.7} & \msd{\bf 85.2}{\bf0.8} & \msd{75.7}{3.0} & \msd{68.5}{2.7} \\
            & 128 & \msd{87.8}{\bf0.8} & \msd{49.4}{1.6} & \msd{85.2}{0.6} & \msd{56.9}{2.5} & \msd{89.4}{1.5} & \msd{\bf 87.6}{3.0} & \msd{76.1}{1.9} \\
            \midrule
            \multirow{3}{*}{\begin{tabular}[c]{@{}r@{}}\mymethod{}\\w/ rand. pred.\end{tabular}} & 32 & \msd{\bf 83.3}{4.1} & \msd{\bf 44.1}{\bf0.7} & \msd{\bf 83.4}{2.0} & \msd{\bf 52.3}{3.9} & \msd{\bf 63.2}{\bf1.1} & \msd{\bf 64.9}{3.9} & \msd{\bf65.2}{\bf1.2} \\
            & 64 & \msd{\bf 85.9}{4.5} & \msd{\bf 48.0}{\bf0.3} & \msd{\bf 84.6}{\bf1.2} & \msd{\bf 60.0}{\bf0.6} & \msd{82.9}{1.7} & \msd{\bf 78.2}{\bf2.0} & \msd{\bf73.3}{\bf0.9} \\
            & 128 & \msd{\bf 86.6}{2.5} & \msd{\bf 49.5}{\bf0.4} & \msd{\bf 87.2}{\bf0.4} & \msd{\bf 63.4}{\bf1.3} & \msd{\bf 96.8}{\bf0.1} & \msd{86.8}{\bf1.3} & \msd{\bf78.4}{\bf0.5} \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{
        Ablation results of \mymethod{} with random class predictions, compared with Coreset selection \citep{sener2018active}. In this case, the class and uncertainty information are disarranged.
    }
    \label{table:randomprediction-main}
\end{table*}

\subsubsection{Zero-shot Chain-of-Thought}

Zero-shot Chain-of-Thought (CoT) prompting \citep{NEURIPS2022_8bb0d291} with LLMs has emerged as a promising method in cold-start scenarios.
This paper tests zero-shot CoT without and with explicit choices in prompts.
The temperature of generation is set to 0, and a maximum of 256 tokens are generated.
Results are shown in Table~\ref{table:0shotCoT-no-choices-main}.
From the results, fine-tuning PLM with \mymethod{} still outperforms 0-shot LLM predictions.
In class-imbalanced and difficult datasets, performance gaps are greater.
Lemon-picking shows that the LLM failed to output a final answer within 256 tokens for many test instances.

In addition, the average total GPU and CPUs' energy consumption and time usage are measured using \citet{10.1145/3644815.3644967}'s method.
Results are reported in Table~\ref{table:energy-main}.
There is a 7.82$\times$ difference in energy consumption and 6.26$\times$ in time consumption.
While increasing the number of output tokens might improve, the added resource consumption cannot be neglected.
\mymethod{} provides an efficient solution for low-resource scenarios. %

\subsection{Effect of Labeling Noise}

Real-world annotations often involve noise.
\citet{BENCHMARKS2021_f2217062} estimated an average of 2.6\% labeling errors across 3 commonly-used NLP datasets.
To evaluate \mymethod{} under labeling noise, experiments with artificial errors are conducted.
As the gold labels may already contain around 3\% errors, 7\% of seed labels are randomly replaced by wrong labels.
The final sets are expected to exhibit an error level of 4--10\%.
Results are reported in Table~\ref{table:noisylabel-main}.

From the results, a decrease in accuracy and an increase in standard deviation occur as expected.
However, \mymethod{} still outperforms 0-shot CoT (Table~\ref{table:0shotCoT-no-choices-main}) in nearly all setups, despite the added noise.
This shows the robustness of \mymethod{} for fine-tuning to labeling noise.

\subsection{Effect of Class Prediction Failure}

For real-world cold-start tasks, the knowledge about classes might not be well exploited by the PLM.
In the worst case, the PLM can fail to generate meaningful class predictions.
To simulate this scenario, ablation experiments with random class predictions are conducted.
In this setup, the predictive embeddings $\vect{z}_{\predss}$ are replaced with random vectors.
This ablates class predictions.
Results are reported in Table~\ref{table:randomprediction-main}.

As class and uncertainty information are disarranged, \mymethod{} degenerates to single textual diversity and performance degradation occurs as expected.
Nonetheless, \mymethod{} still outperforms Coreset selection \citep{sener2018active}, a CSAL baseline which also purely utilizes textual diversity.
This demonstrates \mymethod{}'s effectiveness in real-world cold-start scenarios.

\subsection{Performance of Few-shot Math Reasoning}

\begin{table}[t]
    \centering
    \begin{adjustbox}{}%
        \begin{tabular}{r!{\vrule width \lightrulewidth}cc!{\vrule width \lightrulewidth}c}
            \toprule
            Method & 4-shot & 8-shot & \textit{Average} \\
            \midrule
            Random & 25.1 & 24.3 & {24.7} \\
            \mymethod{} & \textbf{25.8} & \textbf{27.4} & \textbf{26.6} \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \caption{
        Evaluation results of \mymethod{} (RoBERTa-base) with few-shot Chain-of-Thought prompting (\citealp{10.5555/3600270.3602070}; \llama{}) on GSM8K dataset \citep{cobbe2021gsm8k}, compared to random sampling.
    }
    \label{table:gsm8k-main}
\end{table}

\mymethod{} has the potential to generalize on other NLP tasks.
To demonstrate this, \mymethod{} is tested on GSM8K \citep{cobbe2021gsm8k}, a dataset of math word problems.
However, directly adapting RoBERTa to solving math problems is difficult due to its masked modeling nature.
Instead, \mymethod{} is applied with RoBERTa to produce a seed set.%
\footnote{
    For open questions like math problems, there are no concepts of ``classes''.
    Instead, the predictive embeddings $\tilde{\vect{z}}_{\predss}$ are clustered with \hdbscan{}.
    The cluster centroids are taken as \textit{meta}-class embeddings ${\vect{z}}_{\hat{y}}$.
}
Then, the seeds are taken as examples for few-shot Chain-of-Thought prompting \citep{10.5555/3600270.3602070} with \llama{}.
From the results, as reported in Table~\ref{table:gsm8k-main}, \mymethod{} is still effective in few-shot math problem solving, compared to random sampling.

\section{Conclusion}

This paper presents \mymethod{}, a dual-diversity enhancing and uncertainty-aware CSAL framework via a prompt-based and graph-based approach.
Different from previous works, it emphasizes dual-diversity (\ie{} textual diversity and class diversity) to ensure a balanced acquisition.
This is achieved by the novel construction of Dual-Neighbor Graph (DNG) and Farthest Point Sampling (FPS).
DNG leverages the $k$NN graph structure of textual space and label space from a PLM.
In addition, \mymethod{} prioritizes hard representative examples, so as to ensure an informative acquisition.
This leverages density-based clustering and uncertainty propagation on the DNG.
Experiments show the effectiveness of \mymethod{}'s dual-diversity enhancement and uncertainty-aware mechanism.
It offers an efficient solution for low-resource data acquisition.
Overall, \mymethod{}'s hybrid strategy strikes an important balance between exploration and exploitation in CSAL.

\section*{Limitations}

\paragraph{Backbone LM}
\mymethod{} leverages a discriminative PLM. %
However, state-of-the-art PLMs are primarily generative.
Generative embedding models (\eg{} \citealp{jiang2023scaling}) or adaptations \citep{10.5555/3454287.3454804,8983025,9369997} can be investigated and combined with \mymethod{}.
For such approaches, their quality and efficiency should be carefully minded.

\paragraph{External knowledge}
In \mymethod{}, the only source of external knowledge is the language model.
Incorporation of more domain knowledge, if possible, can improve the performance in the cold-start stage.
As \mymethod{} adopts a prompt-based and graph-based acquisition, prompt engineering and knowledge graphs \citep{kgllm} can be investigated.%

