\section{Related Work}
\cite{gutmann_noise-contrastive_2010} laid out the groundwork for what is now contrastive learning by introducing Noise Contrastive Estimation (NCE) to estimate probabilistic models without computing partition functions. 
Word2Vec \citep{mikolov2013efficient} was one of the early applications of NCE in Natural Language Processing (NLP) and also introduced negative sampling to train word embeddings efficiently.
More recent developments using contrastive learning have led to significant advancements in representation learning. 

\textbf{Contrastive Learning in Vision.}
Notably, most of the recent work in this area has focused on unsupervised visual representation learning by utilizing data augmentations.
For instance, SimCLR \citep{chen_simple_2020} utilizes contrastive learning with data augmentation to learn visual representations. 
SwAV~\cite{caron_unsupervised_2020} combines clustering with contrastive learning for unsupervised visual representation learning. 
MoCo~\cite{he_momentum_2020} introduces a momentum contrast mechanism for unsupervised visual representation learning. 
In contrast, our method focuses on supervised tasks and uses task labels to adapt embeddings to the downstream task.

\textbf{Contrastive Learning in Multimodal Settings.}
In multimodal settings, contrastive learning has been used to learn joint representations of different modalities, typically images and text, by utilizing an implicit relationship, say images and their captions.
One of the first major works is Contrastive Predictive Coding (CPC)~\cite{oord_representation_2019} which introduces a way to learn representations by predicting future observations in latent space. 
Another notable work is CLIP~\cite{radford2021learning} which learns joint representations of images and text by pre-training on a large dataset of image-caption pairs. Other examples include ConVIRT~\cite{zhang_contrastive_2022} which applies contrastive learning to medical imaging by leveraging text reports associated with images and ALIGN~\cite{jia_scaling_2021}, which scales up visual and vision-language representation learning with noisy text supervision. However, these methods are designed for specific input types and rely on an implicit relationship between modalities, which may not be present in all multimodal settings. Our method is modality-agnostic and can be applied to any modality or combination thereof, as long as we have a general-purpose embedding model that can process that modality.

\textbf{Contrastive Learning in Clinical Settings.}
A ripe area for application of contrastive learning techniques is in clinical settings, where we frequently have multimodal data.
To this end, ConVIRT~\cite{zhang_contrastive_2022} applies contrastive learning to medical imaging by leveraging radiology reports associated with x-ray images.
A different approach is \cite{xiacontrastive}, which utilizes contrastive learning to improve representations of MLP and encoder architectures with partially missing sources for patients' tabular data.
However, the above methods are again constrained by the structure of the data or its implicit relationships. Our approach can utilize any type of data, and can be applied to any binary classification task.

% \textbf{Our Approach in \emph{contrast}.}
% While our method builds upon the ideas of these foundational works, it differs from typical contrastive methods in several key aspects:

% \begin{enumerate}
%     \item We train a nonlnear mapping, not the embedding model; An important distinction is that we train a small projection network on top of the embeddings, rather than the embeddings themselves. Typically, contrastive learning approaches backpropagate the loss and update the entire model, which is computationally expensive both from a resource and a time perspective. By keeping the large embedding models frozen, we reap multiple benefits. The obvious one is that we can hyperparameter tune the entire pipeline very fast, cheaply, and efficiently. However, an important benefit is that since each projection network is modality-specific, we can add or combine new modalities without retraining it. For instance, we can train each modality separately, and then we can combine them in a single model, without retraining. 
%     \item Our method is modality-agnostic; it operates on the final embeddings rather than the input data, and does not rely on the data's structure or form. This allows us to integrate and combine multiple modalities at will, as long as we have a general-purpose embedding model that can process each modality. On the other hand, typical contrastive learning architectures like CLIP are designed for particular input types (e.g., image-text pairs), and usually also rely on an implicit relationship between modalities.
%     \item Computationally efficient: We only need to train a small projection network (e.g. NN) rather than a large embedding model. Thus, our method significantly reduces computational requirements, making it accessible for resource-constrained settings.
%     \item Task-specific Supervised Adaptation: Many popular methods like SimCLR or SwAV rely on data augmentation to create positive pairs from a single sample. Instead, we directly use task labels, and contrast among different samples depending on their label. This is not a limitation, as the projection network can be trained with very few labels. However, the upside is that we can potentially capture more relevant features for the downstream task, and allows for task-specific adaptation.
% \end{enumerate}


% Overall, our method is simple, cheap, efficient, and flexible in terms of both modalities and tasks.
% These differences allow our method to address specific challenges in the clinical setting, where computational resources may be limited, data might appear in various forms, and task-specific adaptation is crucial for accurate predictions.

% Main Benefits and Contributions
% \begin{enumerate}
%     \item Minimal Resources Required: No finetuning or training is required for the larger models. 
%     \item General-purpose and Flexible: Can be applied to any modality (and combination thereof) and to any downstream task, as long as we have a large general-purpose embedding model that can process that modality.
%     \item Efficient and Cheap: Importantly, we no longer need to perform hyperparameter tuning with larger models, instead we only need to tune the small projection network on top of the frozen model, which is much cheaper.
%     \item No catastrophic forgetting: Avoids catastrophic forgetting as the original model remains frozen. 
% \end{enumerate}