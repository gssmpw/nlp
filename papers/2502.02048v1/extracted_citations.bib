@inproceedings{caron_unsupervised_2020,
	title = {Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
	volume = {33},
	url = {https://papers.neurips.cc/paper_files/paper/2020/hash/70feb62b69f16e0238f741fab228fec2-Abstract.html},
	abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, {SwAV}, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the code of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements. We validate our findings by achieving 75.3\% top-1 accuracy on {ImageNet} with {ResNet}-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
	pages = {9912--9924},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
	urldate = {2024-10-15},
	date = {2020},
	file = {Full Text PDF:C\:\\Users\\Pericles\\Zotero\\storage\\Y7GLGCZ7\\Caron et al. - 2020 - Unsupervised Learning of Visual Features by Contra.pdf:application/pdf},
}

@misc{chen_simple_2020,
	title = {A Simple Framework for Contrastive Learning of Visual Representations},
	url = {http://arxiv.org/abs/2002.05709},
	doi = {10.48550/arXiv.2002.05709},
	abstract = {This paper presents {SimCLR}: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on {ImageNet}. A linear classifier trained on self-supervised representations learned by {SimCLR} achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised {ResNet}-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming {AlexNet} with 100X fewer labels.},
	number = {{arXiv}:2002.05709},
	publisher = {{arXiv}},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	urldate = {2024-10-15},
	date = {2020-07-01},
	eprinttype = {arxiv},
	eprint = {2002.05709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Pericles\\Zotero\\storage\\3YGBFH6T\\Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;Snapshot:C\:\\Users\\Pericles\\Zotero\\storage\\DFXIYMWZ\\2002.html:text/html},
}

@inproceedings{gutmann_noise-contrastive_2010,
	title = {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
	url = {https://proceedings.mlr.press/v9/gutmann10a.html},
	shorttitle = {Noise-contrastive estimation},
	abstract = {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable {ICA} model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.},
	eventtitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	pages = {297--304},
	booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	publisher = {{JMLR} Workshop and Conference Proceedings},
	author = {Gutmann, Michael and Hyvärinen, Aapo},
	urldate = {2024-10-14},
	date = {2010-03-31},
	langid = {english},
	note = {{ISSN}: 1938-7228},
	file = {Full Text PDF:C\:\\Users\\Pericles\\Zotero\\storage\\S6BKCFLH\\Gutmann and Hyvärinen - 2010 - Noise-contrastive estimation A new estimation pri.pdf:application/pdf},
}

@inproceedings{he_momentum_2020,
	location = {Seattle, {WA}, {USA}},
	title = {Momentum Contrast for Unsupervised Visual Representation Learning},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157636/},
	doi = {10.1109/CVPR42600.2020.00975},
	abstract = {We present Momentum Contrast ({MoCo}) for unsupervised visual representation learning. From a perspective on contrastive learning [29] as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-ﬂy that facilitates contrastive unsupervised learning. {MoCo} provides competitive results under the common linear protocol on {ImageNet} classiﬁcation. More importantly, the representations learned by {MoCo} transfer well to downstream tasks. {MoCo} can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on {PASCAL} {VOC}, {COCO}, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	eventtitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {9726--9735},
	booktitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	urldate = {2024-10-15},
	date = {2020-06},
	langid = {english},
	file = {He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf:C\:\\Users\\Pericles\\Zotero\\storage\\G8EQPMKB\\He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf:application/pdf},
}

@misc{jia_scaling_2021,
	title = {Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision},
	url = {http://arxiv.org/abs/2102.05918},
	abstract = {Pre-trained representations are becoming crucial for many {NLP} and perception tasks. While representation learning in {NLP} has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as {ImageNet} or {OpenImages}. For vision-language, popular datasets like Conceptual Captions, {MSCOCO}, or {CLIP} all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as {ImageNet} and {VTAB}. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and {MSCOCO} image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.},
	number = {{arXiv}:2102.05918},
	publisher = {{arXiv}},
	author = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V. and Sung, Yunhsuan and Li, Zhen and Duerig, Tom},
	urldate = {2024-10-14},
	date = {2021-06-11},
	eprinttype = {arxiv},
	eprint = {2102.05918},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Pericles\\Zotero\\storage\\HAVSU49D\\Jia et al. - 2021 - Scaling Up Visual and Vision-Language Representati.pdf:application/pdf;Snapshot:C\:\\Users\\Pericles\\Zotero\\storage\\GSZQHDQQ\\2102.html:text/html},
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@misc{oord_representation_2019,
	title = {Representation Learning with Contrastive Predictive Coding},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	number = {{arXiv}:1807.03748},
	publisher = {{arXiv}},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	urldate = {2024-10-15},
	date = {2019-01-22},
	eprinttype = {arxiv},
	eprint = {1807.03748},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Pericles\\Zotero\\storage\\4WJM67H4\\Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf:application/pdf;Snapshot:C\:\\Users\\Pericles\\Zotero\\storage\\FUHZJL65\\1807.html:text/html},
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{xiacontrastive,
  title={Contrastive Learning for Clinical Outcome Prediction with Partial Data Sources},
  author={Xia, Meng and Wilson, Jonathan and Goldstein, Benjamin and Henao, Ricardo},
  booktitle={Forty-first International Conference on Machine Learning}
}

@misc{zhang_contrastive_2022,
	title = {Contrastive Learning of Medical Visual Representations from Paired Images and Text},
	url = {http://arxiv.org/abs/2010.00747},
	abstract = {Learning visual representations of medical images (e.g., X-rays) is core to medical image understanding but its progress has been held back by the scarcity of human annotations. Existing work commonly relies on fine-tuning weights transferred from {ImageNet} pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. Meanwhile, several recent studies show exciting results from unsupervised contrastive learning from natural images, but we find these methods help little on medical images because of their high inter-class similarity. We propose {ConVIRT}, an alternative unsupervised strategy to learn medical visual representations by exploiting naturally occurring paired descriptive text. Our new method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test {ConVIRT} by transferring our pretrained weights to 4 medical image classification tasks and 2 zero-shot retrieval tasks, and show that it leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classification tasks, our method requires only 10{\textbackslash}\% as much labeled training data as an {ImageNet} initialized counterpart to achieve better or comparable performance, demonstrating superior data efficiency.},
	number = {{arXiv}:2010.00747},
	publisher = {{arXiv}},
	author = {Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D. and Langlotz, Curtis P.},
	urldate = {2024-10-14},
	date = {2022-09-19},
	eprinttype = {arxiv},
	eprint = {2010.00747},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Pericles\\Zotero\\storage\\RNHXURGE\\Zhang et al. - 2022 - Contrastive Learning of Medical Visual Representat.pdf:application/pdf;Snapshot:C\:\\Users\\Pericles\\Zotero\\storage\\VA6NISDL\\2010.html:text/html},
}

