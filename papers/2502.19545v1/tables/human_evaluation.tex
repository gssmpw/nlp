\begin{table*}[t]
    \centering
    \renewcommand{\arraystretch}{1.2} % Adjust row spacing for better readability
    \resizebox{\textwidth}{!}{ % Automatically resizes to fit within text width
    \begin{tabular}{lcccccccc|c}
        \toprule
        \textbf{Model} & \textbf{Halluc.} & \textbf{Non-Ans} & \textbf{Partial} & \textbf{IDK - Bad} & \textbf{Disfl.} & \textbf{Other} & \textbf{IDK - Good} & \textbf{Good} & \textbf{Total Good} \\
        \midrule
        Pretrained  & 13  & 0  & 6  & 0  & 1  & 5 & 24  & 51  & 75 \\
        GPT-4o  & 9   & 0  & 2  & 1  & 0  & 0 & 29  & 59  & 88 \\
        \hline
        Manual cleaned  & 14  & 2  & 7  & 0  & 3 & 5 & 21  & 48  & 69 \\
        Autocleaned\textsubscript{G}  & 13  & 0  & 6  & 0  & 2  & 9 & 19  & 51  & 70 \\
        \hline
        SynthGPT  & 9   & 0  & 0  & 2  & 3  & 8 & 22  & 56  & 78 \\
        SynthLlama  & 7   & 0  & 2  & 0  & 2 & 7 & 26  & 56  & 82 \\
        SynthLlama+ & \textbf{6}   & 0  & 0  & 0  & 1 & 2 & 31  & \textbf{60} & \textbf{  91}* \\
        \bottomrule
    \end{tabular}
    }
    \caption{Human evaluation results in which 3 annotators assess response quality across multiple error categories for the regular test set (50 items) and 50 items from the challenge test set. Majority vote decided the final category for each item, and in cases where all 3 annotators disagreed, the most severe error is the final category. SynthLlama+ had a significantly higher proportion of good items (p < .05) over pretrained Llama, $\chi^2(1, N=100) = 9.1, p = .0026$.}
    \label{tab:humaneval}
\end{table*}
