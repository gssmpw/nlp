% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
% \usepackage{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{float} 


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\title{Winning Big with Small Models: \\Knowledge Distillation vs.\ Self-Training\\for Reducing Hallucination in QA Agents}

\author{
 \textbf{Ashley Lewis\textsuperscript{1}}
 \textbf{Michael White\textsuperscript{1}}
 \textbf{Jing Liu\textsuperscript{2}}
 \textbf{Toshiaki Koike-Akino\textsuperscript{2}}
 \textbf{Kieran Parsons\textsuperscript{2}}
 \textbf{Ye Wang\textsuperscript{2}} \\
 \textsuperscript{1}The Ohio State University,  
 \textsuperscript{2}Mitsubishi Electric Research Laboratories \\
 \small{
 \{\href{mailto:lewis.2799@osu.edu}{lewis.2799}, 
 \href{mailto:white.1240@osu.edu}{white.1240}\}@osu.edu,  
 \{\href{mailto:jiliu@merl.com}{jiliu}, 
 \href{mailto:koike@merl.com}{koike}, 
 \href{mailto:parsons@merl.com}{parsons}, 
 \href{mailto:yewang@merl.com}{yewang}\}@merl.com}
}



\begin{document}
\maketitle
\begin{abstract}
The deployment of Large Language Models (LLMs) in customer support is constrained by hallucination—generating false information—and the high cost of proprietary models. To address these challenges, we propose a retrieval-augmented question-answering (QA) pipeline and explore how to balance human input and automation. Using a dataset of questions about a Samsung Smart TV user manual, we demonstrate that synthetic data generated by LLMs outperforms crowdsourced data in reducing hallucination in finetuned models. We also compare self-training (fine-tuning models on their own outputs) and knowledge distillation (fine-tuning on stronger models' outputs, e.g., GPT-4o), and find that self-training achieves comparable hallucination reduction.  We conjecture that this surprising finding can be attributed to increased exposure bias issues in the knowledge distillation case and support this conjecture with post hoc analysis. We also improve robustness to unanswerable questions and retrieval failures with contextualized “I don’t know” responses. These findings show that scalable, cost-efficient QA systems can be built using synthetic data and self-training with open-source models, reducing reliance on proprietary tools or costly human annotations.
\footnote{This work was conducted while Ashley Lewis was interning at Mitsubishi Electric Research Laboratories.}



\end{abstract}


\input{sections/introduction}

\input{sections/related_work}
\input{sections/data_and_experimental_setup}
\input{sections/results_and_analysis}
\input{sections/discussion}
\input{sections/conclusion}
\input{sections/limitations}
\input{sections/ethics}


% Bibliography entries for the entire Anthology, followed by custom entries
\bibliography{anthology, custom}
% Custom bibliography entries only
% \bibliography{custom}

\appendix

% \section{Example Appendix}
% \label{sec:appendix}

\input{sections/appendices/data_preprocessing}
\input{sections/appendices/question_examples}
\input{sections/appendices/prompts}
\input{sections/appendices/factscore}
\input{sections/appendices/human_eval_tutorial}
\input{sections/appendices/error_category_examples}
\input{sections/appendices/human_eval_breakdown}
\input{sections/appendices/bertscore}


\end{document}
