\section{Results and Analysis}
\label{results_and_analysis}

\input{tables/human_evaluation}

\subsection{Autocleaning vs. Manual Cleaning}

The FactScore results on the test set (Table \ref{tab:factscore_test_set}) and human evaluation results (Table \ref{tab:humaneval}) reveal that models finetuned on autocleaned data perform slightly better in terms of factual accuracy and response quality compared to manually cleaned data, though the gains are small. No models were significantly better than pretrained Llama-3.
  
Table~\ref{tab:lengths} shows that responses generated from the model trained on autocleaned data are consistently longer than those from manually cleaned data, suggesting that autocleaning prioritizes including as much information as possible from the retrieved passage, even when it is unnecessary to answer the question. This verbosity, while occasionally useful, does not inherently improve factuality.

The response quality of autocleaned and manually cleaned models is similar, as indicated by FactScore and human evaluation results. Both outperform a model trained on uncleaned data but fail to surpass the pretrained Llama-3 baseline. However, hallucination remains a persistent issue across all models, regardless of the cleaning method.

One reason for the lack of significant improvements between manual and autocleaned models may be the limited training data (only 534 examples), which likely reduces the relative impact of cleaning strategies. Furthermore, the absence of sufficient negative training examples, such as explicit ``I don’t know'' responses, leaves models prone to over-generating information rather than admitting uncertainty—an issue particularly evident in the challenge test set.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Human vs. Synthetic Training Data}

\input{tables/diversity_metrics}

A key question in this study is whether crowdsourced training data is necessary for finetuning QA models, or if synthetically generated data can achieve comparable or even superior performance. We compare models trained on crowdsourced answers against those trained on LLM-generated synthetic data (from Llama-3 and GPT-4o), evaluating them on both the regular and challenge test sets. 

Table \ref{tab:factscore_test_set} and Table \ref{tab:humaneval} indicate  that models trained on synthetic data can outperform those trained on crowdsourced data in terms of factual accuracy and overall response quality. One possible explanation is that crowdsourced data tend to introduce variability and noise, whereas synthetic data is consistently aligned with the retrieved passages and the LLM's internal language patterns, making it easier for the model to learn structured answer generation. 

In Table \ref{tab:diversity} we examine diversity using GEM metrics \cite{gehrmann-etal-2021-gem} and find that crowdsourced questions, while shorter on average, have a larger vocabulary of distinct 1-, 2-, and 3-grams relative to the number of total tokens, suggesting greater diversity. We also calculate BERTScores \cite{zhang2020bertscore} for every pair of questions within each dataset and find that, on average, the scores for the synthetic data are higher, indicating that the questions are more semantically similar to each other than the questions in the crowdsourced dataset. We also calculate the perplexity of the questions for Llama-3 and find higher perplexity in the human questions, indicating that they are more unfamiliar to the model. While greater diversity can potentially be helpful in finetuning a model, evidently the less diverse and more expected synthetic questions are more consistently helpful in our experiments. Further analysis can be found in Appendix \ref{app:bertscores}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Synth Llama+: Enhancing Synthetic Data for Hallucination Reduction}
\label{synth_llama+}

To encourage the model to abstain from answering when relevant information is unavailable, as is often the case in the challenge test set, we added negative training examples to the synthetic Llama data by duplicating 100 random training questions. Then, instead of generic ``I don’t know'' responses, we constructed context-aware refusals by replacing the correct passage with a random one and prompting Llama-3 to generate an answer using these items. This ensured that the model could acknowledge the user’s intent while signaling retrieval failure, as shown in the following example:

\begin{quote}
\textbf{Question:} How do I select Dynamic mode?

\textbf{Passage:} The compression of video content may cause picture distortions, especially in fast-moving pictures from sports programs and action movies. [...]

\textbf{Generated Response:} I'm sorry, I can't find any information about selecting Dynamic mode in the provided section of the user manual.
\end{quote}

Unlike generic refusals, this approach ensures that the model’s response acknowledges the intent of the question, making it clear to users that their request was understood but that relevant information is unavailable. We select SynthLlama here because it provides the best balance of low cost and high performance, which is an important consideration for real-world applications.

These enhancements led to improvements in both FactScore and human evaluation metrics compared to the base SynthLlama model and comparable performance to GPT-4o on this task.  With these improvements, SynthLlama+ achieved a significantly higher proportion of good responses in comparison to pretrained Llama in the human evaluation, as shown in Table~\ref{tab:humaneval}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Exposure Bias and Synthetic Data Performance}

\input{tables/train_set_factscores}

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{figures/MERL_paper_data_blending.drawio.png}
    \caption{A toy example of 10 training items per synthetic model to demonstrate how the Best and Worst 50:50 blends were created.}
    \label{fig:data_blending}
\end{figure}


One of the key findings in our study is that self-trained models perform comparably to knowledge-distilled ones---that is, models finetuned on synthetic data generated by the same model (e.g., Llama-3 trained on Llama-generated QA pairs) perform about as well as those trained on synthetic data from a more performant model (e.g., Llama-3 trained on GPT-generated QA pairs) when both synthetic datasets use data cleaning. This suggests that exposure bias may influence training stability and factual accuracy, as models appear to be more reliable when finetuned on data that aligns closely with their pretraining distribution. Exposure bias in language models refers to the mismatch between training and inference: during training, the model learns with gold context (``teacher forcing''), but at inference, it generates text based on its own prior predictions, potentially causing errors to accumulate and degrade output quality \cite{arora-etal-2022-exposure}. 

To further investigate this conjecture, we used the pretrained Llama-3 model to compute the perplexity of each QA response, conditioned on the passage. To quantify the relative familiarity of each synthetic example, we calculated the difference in perplexity between the GPT-generated and Llama-generated QA for each passage,

\begin{equation}
  \label{eq:difference}
\Delta PP = PP(q_{\text{G}}, a_{\text{G}} \mid c) - PP(q_{\text{L}}, a_{\text{L}} \mid c)
\end{equation}

\noindent
where \( (q_{\text{G}}, a_{\text{G}}) \) and \( (q_{\text{L}}, a_{\text{L}}) \) are the question-answer pairs generated by GPT-4o and Llama-3 for passage \( c \), respectively, and \( PP(q, a \mid c) \) represents the perplexity score of a given QA pair under the pretrained Llama-3 model.

This measure allows us to rank training examples based on their relative familiarity to the base Llama-3 model. Positive values (\( \Delta PP>0 \)) indicate that the GPT-generated QA pair is more perplexing (i.e., less familiar) to the model than the Llama-generated QA pair, whereas negative values (\( \Delta PP<0 \)) suggest the opposite.

We then sorted all passages by their perplexity difference (\(\Delta PP\)) and constructed the Best and Worst 50:50 Blends as follows. See Figure \ref{fig:data_blending} for a visual of this process using a toy example.

\paragraph{Best Blend} For each passage, we selected the QA pair where the generating model had a larger perplexity advantage relative to the other model. This means selecting the 50\% of GPT-generated QA pairs where \( \Delta PP \) is smallest and the 50\% of Llama-generated QA pairs where \( \Delta PP \) is largest.
\paragraph{Worst Blend} For each passage, we selected the QA pair where the generating model had a larger perplexity disadvantage relative to the other model. This means selecting the 50\% of GPT-generated QA pairs where \( \Delta PP \) is largest and the 50\% of Llama-generated QA pairs where \( \Delta PP \) is smallest.

Each blend contained an equal mix (50\% GPT-generated and 50\% Llama-generated), ensuring a direct comparison of training effects when models are finetuned on their most versus least familiar examples relative to each other.


\paragraph{Results and Analysis}

Table~\ref{tab:factscore_training} shows the FactScore results for the regular training set questions. Because these manually-written questions are not used at training time for the synthetic models, they can be repurposed as a larger test set, allowing for significant differences to emerge. The results reveal no significant difference between synthetic GPT and synthetic Llama, suggesting comparable performance. Meanwhile, the Worst Blend model performs significantly worse than the Best Blend model, indicating that the perplexity of the training examples does play a role in the downstream model's propensity to hallucinate. Meanwhile, the Best Blend model has a higher score than both synthetic models, suggesting that perplexity-based selection could be a tool worth exploring further in mitigating hallucination for synthetic data.


