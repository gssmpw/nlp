\section{Data Preprocessing}
\label{app:data_preprocessing}

The dataset used in this study required extensive preprocessing to align the Samsung Smart TV user manual with the accompanying QA pairs and to ensure the data was suitable for a retrieval-augmented QA framework. This process involved converting the manual into a structured format and addressing inconsistencies in the original QA dataset.

\subsection{Unused Components of the Provided Dataset}

The dataset provided by \citet{nandy-etal-2021-question-answering} includes several components for QA tasks over electronic device manuals. While we relied heavily on their crowdsourced Samsung Smart TV QA dataset, other components were excluded due to specific limitations, outlined below:

\paragraph{1. Pretraining Corpus of Product User Manuals}
This corpus, designed for pretraining, was not used due to:
(1) Formatting Issues: It contained significant noise, including garbled characters, mixed languages, and missing elements like images and titles, likely due to automated PDF-to-text conversion.
(2) Irrelevance: Pretraining on this noisy data was unnecessary, as this study focused on fine-tuning QA systems and retrieval-augmented methods.

\paragraph{2. Galaxy S10 User Manual and QA Dataset}
The Galaxy S10 manual and its associated dataset of 50 crowdsourced questions were excluded because:
(1) Subset Issues: The questions were a small subset of a larger, unreleased dataset, raising potential licensing concerns.
(2) Scale: With only 50 questions, this dataset lacked the scale required for meaningful experimentation, especially compared to the Samsung Smart TV QA dataset.

\subsection{User Manual Preparation}

The Samsung Smart TV manual, originally provided as a PDF, presented several challenges for direct use. The JSON format provided was inconsistent, likely due to automatic conversion processes, and the structure of the manual did not align well with the ``Section Hierarchy'' fields used in the QA dataset, which point to the part of the manual from which the passage is retrieved. Unfortunately, an initial search for a reliable PDF conversion tool yielded few satisfactory results. To address these issues, the first author undertook a semi-manual process to convert the manual into a structured JSON format.

First, screenshots of the original manual's table of contents were taken to map its hierarchical structure. Using GPT-4o, we generated a nested JSON representation that mirrored this hierarchy, with sections and subsections organized into dictionaries. The text within each section was carefully transcribed into corresponding fields, and images were replaced with placeholders (e.g., [image\_X.png]) that referenced a separate folder containing labeled images. To get transcriptions, we first fed each section of the manual to GPT-4o and asked it to fill in the section of the new JSON file. This was a very iterative process, with the first author manually checking the transcriptions and updating as necessary. This approach ensured that the JSON file was both faithful to the manual's structure and practical for passage retrieval tasks. Manual adjustments were made throughout the process to correct formatting errors and inconsistencies, ensuring the final structure was robust and usable.

\subsection{Cleaning the Crowdsourced QA Dataset}

The QA dataset included human-written questions linked to specific spans of text within the manual. However, the dataset required significant cleaning to align with the newly structured manual. Many questions contained incorrect ``Section Hierarchy'' fields, which were manually corrected to match the updated JSON structure of the manual.

Additionally, we expanded the retrieved passages associated with each question. Instead of limiting retrieval to short spans, we included entire sections from the manual, reflecting a more realistic retrieval scenario for QA systems. These adjustments not only improved the alignment between the questions and the manual but also made the dataset more suitable for the task of mitigating hallucinations.


\subsection{Constructing the Challenge Dataset}

Included in the \citet{nandy-etal-2021-question-answering} dataset are a collection of ~3,000 real-world user questions sourced from community forums. The questions seem to primarily come from the Amazon product pages of various Samsung Smart TVs. While there is variety in these products (model, size, etc.), they all use the same software and general hardware described in the user manual. There are many questions in this collection that are not answerable by the user manual, however. While the answers from the product pages are included, they are not reliable as (1) there is no guarantee that they are correct, (2) could involve subjective opinions, (3) may not correspond to information available in the user manual, thus we are unable to match the responses to grounding passages. Because of this, we do not rely on the answers as a resource. According to the \citet{nandy-etal-2021-question-answering} paper, there are annotations for which of these questions are answerable using the manual, but it does not seem that these annotations were publicly available.

Further, these questions do not have corresponding retrieved passages, which are necessary for our experiments. However, because these questions are only used at test and validation time and because their usefulness stems from their unanswerability, we could rely on less-than-perfect means of finding corresponding passages. Thus we simply feed the entire user manual JSON to GPT-4o and ask it to identify the most relevant passage for each of the randomly selected 100 questions in the dev and test set (200 total). This proved to be the quickest and easiest way to find passages, but a more reliable and realistic method would have been to use a state-of-the-art retrieval model. In an analysis of the dev set, we found that only 26\% of the questions are answerable. 