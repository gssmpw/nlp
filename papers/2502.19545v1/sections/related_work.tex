\section{Related Work}
\label{related_work}


Recent studies suggest that finetuning on new, unfamiliar knowledge can lead to hallucination \cite{gekhman-etal-2024-fine, flame, kang2024unfamiliarfinetuningexamplescontrol}.  For instance, \citet{flame} propose training on self-generated data to reduce hallucination, but introduce a training-test mismatch where models use grounding documents during training but not testing, potentially causing hallucinations. We maintain consistent setups.

Like \citet{flame}, \citet{zhang2024selfalignment} employ self-training to reduce hallucinations. Our approach differs in three ways: first, we use simple supervised finetuning (SFT) instead of techniques like reinforcement learning (RL) and direct preference optimization (DPO), which are promising avenues for future work. Second, we compare self-training with knowledge distillation, investigating the value of synthetic data from a model's own outputs and from a more performant model. Third, we validate our results with human evaluation in addition to automatic metrics. Other works also focus on iterative self-refinement \cite{wang2024selftaughtevaluators, madaan2024self}, though do not specifically focus on the problem of hallucination.

In contrast, \citet{lewis-white-2023-mitigating} employ knowledge distillation to reduce hallucination, using ChatGPT to generate and clean document-grounded training data. However, their approach is limited in two ways: they finetune a T5-large model \cite{raffel2020exploring}, which reduces hallucination over GPT-3.5 but limits robustness and fluency, and they evaluate only on synthetic data.

\citet{farquhar2024semantic} detect hallucinations during inference using semantic entropy, which clusters generated outputs based on semantic equivalence and measures uncertainty at the level of meaning.  While semantic entropy excels at runtime detection in open-domain settings, the entailment-based clustering method is very expensive.  By contrast, our approach reduces hallucinations at their source by improving training processes for RAG settings.


