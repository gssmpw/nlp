\section{Discussion}
\label{discussion}

Our findings demonstrate that self-training and knowledge distillation can be comparably effective in reducing hallucination, while self-training is much less costly. Models trained on self-generated data consistently performed as well or better than those trained on GPT-generated data, supporting the hypothesis that exposure bias plays a key role in finetuning effectiveness. Additionally, our Best Blend vs.\ Worst Blend analysis revealed that using high-perplexity examples at training time led to increased hallucination, reinforcing the importance of training on familiar, low-perplexity data. Further improvements were observed with Synth Llama+, where incorporating simple, context-aware negative examples yielded higher factual accuracy, suggesting promising future directions for hallucination mitigation.

While our experiments focus on a single domain, the underlying mechanisms behind exposure bias and synthetic data effectiveness are likely to generalize to other QA tasks. Applying this approach in domains such as medical or legal QA would provide a valuable test of its robustness and effectiveness in higher-stakes applications.

Future work should explore scaling synthetic data generation, refining data selection methods based on perplexity differences, and investigating iterative self-training approaches, where models continuously refine their own synthetic data over multiple training cycles. This could further enhance model alignment and factuality while reducing reliance on external supervision.

