\section{Data and Experimental Setup}
\label{sec:data}

\subsection{Datasets}

The primary dataset consists of 684 crowdsourced questions paired with retrieved passages from the manual \cite{nandy-etal-2021-question-answering}. We split the dataset into 534 training, 100 development, and 50 test questions (our ``regular test set''). Dataset preprocessing details can be found in Appendix \ref{app:data_preprocessing}. We focused on this dataset because many existing QA datasets either lack grounding documents or prioritize open-domain QA, which does not align with the controlled, retrieval-augmented QA setting we aimed to study. This approach also allowed us to conduct a deep-dive analysis into the trade-offs between self-training, knowledge distillation, and synthetic data generation in mitigating hallucinations within a well-defined context.

As mentioned, the dataset also contains a collection of 3,000 questions sourced from community forums. We create challenge sets by randomly selecting 100 development and 100 test questions from this set. These questions are noisier and less than half are answerable, which allows us to evaluate how well models handle particularly challenging cases. Examples from both types of questions can be found in Appendix \ref{app:example_questions}.

\subsection{Training Data}

\input{tables/test_set_factscores}

\paragraph{Regular Training Data}

We use the pretrained Llama-3-8B-Instruct \cite{dubey2024llama3herdmodels} to generate answers for the 534 training questions. Three datasets are created:
(1) a manually cleaned version where responses were reviewed and corrected by the first author, and
(2)--(3) automatically cleaned versions using GPT-4o and Llama-3-70B, respectively. This allows a systematic evaluation of the trade-offs between human effort and automated cleaning. As shown in Table \ref{tab:factscore_test_set}, cleaning with Llama-3 was largely unsuccessful. Thus in the remaining experiments GPT-4o was used for the cleaning task. We anticipate that improvements in open-source models like Llama-3 may reduce reliance on proprietary alternatives in the future. Prompts for both data generation and cleaning can be found in Appendix \ref{app:prompts}.

\paragraph{Synthetic Data} 

In addition to crowdsourced training questions, we generate fully synthetic QA data using LLMs. Specifically, we prompt Llama-3 and GPT-4o to generate new QA pairs based on passages from the Samsung Smart TV manual. To ensure that these datasets have comparable information coverage to the crowdsourced dataset and to prevent retrieval quality from being a confounding factor, we select passages systematically rather than randomly. We identify all 208 unique sections in the manual that are referenced in the crowdsourced training data. From these passages, we generate two synthetic QA pairs per passage, two from Llama-3 and two from GPT-4o. This approach ensures that the synthetic datasets are no larger than the crowdsourced dataset and cover similar content while maintaining consistency in passage selection. In a real-world application, this limitation does not exist, as synthetic training data can be generated from any number of passages. Thus, coverage is not inherently a bottleneck when using synthetic data in practical settings.

\subsection{Baseline and Experimental Models}

To evaluate the impact of data cleaning type and synthetic training data on hallucination reduction, we experiment with both pretrained models and finetuned models trained on different datasets.

\paragraph{Baseline Models}  
\begin{itemize}
    \item \textbf{Pretrained Llama-3-8B-Instruct (Llama-3)}: An open-source model that serves as a strong starting point for retrieval-augmented generation (RAG) without task-specific adaptation \cite{dubey2024llama3herdmodels}. The model is run with few-shot prompting.
    \item \textbf{GPT-4o}: A state-of-the-art proprietary model, included as a benchmark to assess how well finetuned open-source models compare to a highly optimized general-purpose system \cite{openai2024gpt4technicalreport}. The model is run with few-shot prompting.
\end{itemize}

\paragraph{Finetuned Models}  

We finetune Llama-3 on different variations of training data to analyze the effects of data source, cleaning method, and exposure bias on hallucination rates. Specifically, we train models on the following datasets using the \citet{zheng-etal-2024-llamafactory} finetuning framework and parameters:
\begin{itemize}
    \item \textbf{Manually Cleaned Training Data}: A dataset where the first author reviewed and corrected Llama-3-generated answers to the \citet{nandy-etal-2021-question-answering} 534 crowdsourced training questions.
    \item \textbf{Automatically Cleaned Training Data}: A version of the training set where errors in Llama-3-generated answers were identified and repaired using GPT-4o.
    \item \textbf{Synthetic Data (Llama vs.\ GPT)}: Two datasets where  416 QA pairs were generated by either Llama-3 or GPT-4o based on passages from the Samsung Smart TV manual. All synthetic data was cleaned using GPT-4o.
    \item \textbf{Synth Llama+}: Trained on the synthetic Llama data, and augmented with 100 negative examples (see section \ref{synth_llama+} for more details).
\end{itemize}

\subsection{Metrics for Evaluation}

We evaluate model performance using two methods: FactScore \cite{min-etal-2023-factscore}, an automated metric for factual accuracy, and human evaluation by trained annotators. These complementary approaches measure factual consistency and response quality.

\paragraph{FactScore}

FactScore evaluates whether a model's response aligns with a reference document. It works by decomposing a response into sentences, breaking each sentence into discrete factual claims, and verifying their alignment with the reference text. FactScore measures the proportion of supported claims while penalizing hallucinated content. However, responses from GPT-4o and SynthGPT, which often use structured formatting (e.g., lists, topic headers), cause FactScore to produce fragmented or nonsensical claims, unfairly penalizing these models. To address this, we removed the sentence-splitting preprocessing and instead generated atomic facts directly from the full response.

FactScore, which we computed using GPT-4o-mini, has been shown to be a reliable proxy for factuality, correlating well with human judgments \cite{min-etal-2023-factscore}. However, we find that it is unsuitable for evaluating \textit{I donâ€™t know} responses. Thus, we applied FactScore only to the regular test set (mostly answerable questions), excluding the challenge set (many unanswerable questions). We also used it to evaluate human-written training questions for synthetic models, as they do not see these at training time and it provides a more robust evaluation. Further information in Appendix \ref{app:factscore}.


\input{tables/annotation_criteria}

\paragraph{Human Evaluation} 
To obtain a more nuanced assessment of response quality, we conducted a human evaluation with three fluent English speaking, Linguistics PhD students (instructions in Appendix \ref{app:human_eval}), who annotated each model-generated response for the regular test set (50 items) and 50 items from the challenge set. They assigned to each response one of the categories listed in Table \ref{tab:error_categories} (examples in Appendix \ref{app:error_category_examples}), which were determined by an author analysis of the dev set. Three-way agreement occurred between annotators 63.14\% of the time and two-way agreement occurred 36.43\% of the time. Krippendorff's Alpha was $\alpha$ = 0.625, indicating substantial agreement.

Each response was labeled independently by all three annotators. The final assigned label was determined by a majority vote. In the few cases where annotators provided three different labels, the response was assigned the most severe error based on the following predefined ranking:  Hallucination > Non-Answer > Partial Answer > IDK - Bad > Disfluent > Other. The purpose of this ranking is to prioritize hallucination and content errors. For example, if a response is labeled as ``Hallucination,'' ``Good,'' and ``Partial Answer,'' it is assigned the final label of ``Hallucination'' due to its higher severity in the ranking.

By combining automated and human evaluation, we ensure a comprehensive analysis of both quality and factual consistency in model-generated responses. The aggregated results can be found in Table \ref{tab:humaneval} and the separate results on the regular and challenge test sets can be found in Appendix \ref{app:human_eval_breakdown}.

\input{tables/lengths}
