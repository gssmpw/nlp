\section{Introduction}
\label{sec:introduction}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/RAG_overview.png}
    \caption{Overview of the retrieval-augmented QA process. A user asks a question about a product feature and the system uses relevant information from the product manual to generates a factual response.}
    \label{fig:RAG_overview}
\end{figure}

While many companies are eager to integrate Large Language Models (LLMs) into customer service and other applications, widespread deployment remains constrained by hallucination, or the generation of false or unsupported information, and the high financial and computational costs of using proprietary models. This issue is particularly critical in customer support, where unreliable responses can mislead users and erode trust.

We develop a cost-effective retrieval-augmented question-answering (QA) pipeline (see Figure \ref{fig:RAG_overview}) and address critical training data questions: what sources of data are most effective for finetuning open source models, and what preprocessing or filtering mechanisms best mitigate hallucination. To do so, we use a dataset from \citet{nandy-etal-2021-question-answering} comprising crowdsourced questions written by professional annotators about a Samsung Smart TV user manual (but notably lacking human-written responses). In this work, we address the following research questions:

\paragraph{RQ1: What is the optimal balance between manual and automated methods for data processing and creation?} We explore the trade-offs of using automatic and manual methods in two main situations: data processing and data creation. 

We use Llama-3-8B-Instruct (hereafter Llama-3) \cite{dubey2024llama3herdmodels} to generate answers to the crowdsourced questions, followed by two cleaning methods: manual cleaning performed by the first author and automatic cleaning using LLMs. While many recent studies have shown LLM's ability to iteratively evaluate and refine text to reduce hallucination \cite{dhuliawala-etal-2024-chain, wang2024selftaughtevaluators}, these methods are often costly and pose data privacy risks when proprietary models are used at runtime. To address this, we compare the effort of manual cleaning with the effectiveness of closed-source (GPT-4o) and open-source (Llama-3) models for data cleaning. We show that while GPT-4o significantly outperforms Llama-3 in cleaning quality, it is comparable to manual efforts, suggesting that manual input may not always be necessary.

We also explore a realistic scenario in which no training data is available. Perhaps surprisingly, we demonstrate that LLM-generated synthetic training data leads to lower hallucination rates than crowdsourced data, as measured by FactScore and human evaluation, possibly due to increased variability in human-written questions.

\paragraph{RQ2: How does self-training compare to model distillation in terms of hallucination rates?} We examine the benefits of synthetic data by comparing two training approaches: finetuning models on data generated by the same model (self-training with Llama-3) versus finetuning models on data generated by a stronger model (knowledge distillation using GPT-4o). \citet{lewis-white-2023-mitigating} suggest that knowledge distillation reduces hallucination, but their study only tests on synthetic questions. Meanwhile, \citet{zhang2024selfalignment} and \citet{flame} show that self-training can reduce hallucination, though without any human evaluation and with a train/test time mismatch in the case of \citet{flame}. To our knowledge, our work is the first apples-to-apples comparison of these two approaches. Surprisingly, we find that self-training of a small model and distillation of a large one achieve comparably low hallucination rates, as measured by FactScore \cite{min-etal-2023-factscore} and human evaluation, when the same data cleaning is used for both methods.

To explore this result, we analyze the potential role of exposure bias, which refers to the tendency of a model to perform better in contexts observed during training, leading to errors when faced with unfamiliar contexts during inference. We hypothesize that models trained on their own generated data benefit from greater familiarity with the training examples, compensating for the quality gap between the models. This suggests that self-training can serve as a resource-efficient alternative to model distillation in tasks where minimizing hallucination is critical.

\paragraph{RQ3: How can retrieval failures and unanswerable questions be anticipated?} The dataset includes questions scraped from community forums such as Amazon product QA sections, which are noisier, more diverse, and often unanswerable using the user manual. Such questions are prone to hallucination as the model relies on pretraining rather than the provided document. Since state-of-the-art retrieval models return n-best lists with imperfect accuracy \cite{gao2023retrieval}, it is critical for QA systems to recognize retrieval failures and respond appropriately (e.g., \textit{I don't know the answer}) while confirming the user's question was understood. While we do not focus on retrieval, we mitigate this issue by inserting negative examples during training, teaching models to provide contextualized “I don’t know” responses, which also reduces hallucination rates.

In light of these questions, this paper makes the following key contributions, with a focus on customer support systems:

\begin{itemize}
    \item We find that manual and automatic data cleaning result in finetuned models with similar factual accuracy, but responses from models based on automatic cleaning are longer.
    
    \item We demonstrate that LLM-generated synthetic training data can lead to models with lower hallucination rates than using crowdsourced data, as measured by FactScore and human evaluation.

    \item We show that finetuning a model on its own generated answers (e.g., training Llama-3 on Llama-generated data) results in comparable hallucination mitigation to training it on GPT-4o-generated answers, despite GPT-4o being a more generally capable model.

    \item We explore exposure bias as a possible explanation for why the self-trained model performs so well. We hypothesize that models perform better when trained on low-perplexity (more familiar) examples. Our FactScore results and perplexity-based analysis provide empirical support for this hypothesis.

    \item We provide a simple, scalable data perturbation strategy and synthesize contextualized \textit{I don't know} responses to increase model robustness to unanswerable questions and retrieval failures.
\end{itemize}