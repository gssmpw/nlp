\section{Ethics}

\subsection{Data Usage and Privacy}
Our research utilizes synthetic data generated by large language models (LLMs) and publicly available and licensed datasets from user manuals for consumer electronics. All data used in this study is devoid of personally identifiable information (PII) and does not infringe upon individual privacy rights. The synthetic data generation process was carefully designed to ensure that no sensitive or identifiable information is included. Our institution's review board reviewed our human evaluation plans and ruled that it does not meet the federal definition of human subjects research requiring review. Our human evaluators were unpaid volunteer colleagues and were informed about how their annotations would be used.

\subsection{Use of Proprietary Models}
Our work leverages GPT-based models in several instances, including as comparison (baseline) models, for synthetic data generation, and in the automatic data cleaning pipeline. While GPT models are not fully reproducible due to their proprietary nature, their use in this work is limited to tasks where their high performance offers meaningful value. Specifically:

\begin{itemize}
    \item GPT is used as a baseline model to benchmark the performance of open-source systems.
    \item GPT-generated synthetic data is provided alongside the Llama-generated data to enable future reproducibility of experiments.
    \item GPT is employed for data cleaning because it demonstrates state-of-the-art performance for this specific task. The study shows that both manual and automated cleaning yield similar outcomes.
    \item To address concerns about reproducibility, all synthetic datasets and cleaned data used in the study will be made publicly available. This ensures that future researchers can reproduce our results even if proprietary models like GPT are unavailable.

\end{itemize}

Note also that GPT-4o was used as a writing assistant for this paper in a limited capacity (rephrasings, help with conciseness) and with some coding tasks during research.

\subsection{Potential Risks and Mitigation}

While our study focuses on reducing hallucinations and improving factual accuracy in QA systems, we acknowledge potential risks related to synthetic data, which may introduce subtle biases or inaccuracies. Because this domain is specific to a product user manual, we did not feel that this was a relevant issue and we did not see any problematic instances of such biases. 

\subsection{Societal Impact}

Our research aims to enhance the accuracy and reliability of QA systems, particularly in retrieving and synthesizing information from structured documents like user manuals. This can improve accessibility and user experience. However, we are aware of the broader implications of deploying such systems in real-world settings, as we demonstrate in this study that these models are still capable of hallucination even in our best-performing settings.

\subsection{Transparency and Reproducibility}
We are committed to transparency and reproducibility in our research. Despite the use of proprietary GPT-based models, our findings do not hinge on the unique capabilities of GPT. The use of GPT is supplementary and not central to the key contributions of this work. To ensure reproducibility, we will provide all synthetic datasets, cleaned data, and detailed descriptions of our experimental methodologies.