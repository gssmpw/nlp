\section{Limitations}

Despite these insights, our study has limitations. First, our test set size is relatively small, particularly for human evaluation, where only 50 challenge and 50 regular test items were labeled. We did not want to overwhelm our annotators with too large of a task and judged that this was the maximum we could require. This limits the statistical power of our findings, making it difficult to detect smaller but meaningful performance differences. Expanding the evaluation set and conducting a larger-scale human evaluation in future work could provide a clearer picture of the impact of different training strategies.

Second, measuring hallucination remains challenging. FactScore, while useful, is not a perfect proxy for factuality, and human judgments, though more reliable, are limited by annotator agreement and scale. More robust hallucination metrics, particularly those that better capture the subtle ways in which models generate misleading but plausible responses, would enhance future analyses.