\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{iclr_fmwild}

%% for arXiv version
% \usepackage{arxiv}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{subcaption}

\iclrfinalcopy

\usepackage{natbib}
\bibliographystyle{unsrtnat}

\title{Towards bandit-based prompt-tuning for in-the-wild foundation agents}
\date{January 2025}
\author{%
Finn Rietz\thanks{Work performed during internship at Microsoft Gaming (ABK)} \\
Ã–rebro University\\
\texttt{finn.rietz@oru.se}\\
\And
Oleg Smirnov \\
Microsoft Gaming\\
\texttt{oleg.smirnov@microsoft.com}\\
\And
Sara Karimi \\
Microsoft Gaming\\
\texttt{sarakarimi@microsoft.com}\\
\And
Lele Cao\\
Microsoft Gaming\\
\texttt{lelecao@microsoft.com}
}


\begin{document}

\maketitle

\begin{abstract}
    Prompting has emerged as the dominant paradigm for adapting large, pre-trained transformer-based models to downstream tasks. The Prompting Decision Transformer (PDT) enables large-scale, multi-task offline reinforcement learning pre-training by leveraging stochastic trajectory prompts to identify the target task. However, these prompts are sampled uniformly from expert demonstrations, overlooking a critical limitation: \textit{Not all prompts are equally informative for differentiating between tasks}. To address this, we propose an inference time bandit-based prompt-tuning framework that explores and optimizes trajectory prompt selection to enhance task performance.
    Our experiments indicate not only clear performance gains due to bandit-based prompt-tuning, but also better sample complexity, scalability, and prompt space exploration compared to prompt-tuning baselines.  
\end{abstract}

\section{Introduction}
Recent advances in Artificial Intelligence (AI) research have demonstrated the strength of large, pre-trained transformer-based foundation models in many domains, including language \citep{radford2018improving, brown2020language}, vision \citep{radford2021learning, dosovitskiy2020image}, and reinforcement learning \citep{reed2022generalist, li2023survey}. These large models leverage vast and diverse offline datasets to acquire generalizable representations that can solve many downstream tasks.
% One common approach to solving target downstream tasks in a zero- or few-shot manner is providing the large pre-trained model with a prompt that describes the current objective. 
A prominent strategy for leveraging these models in zero- and few-shot settings involves conditioning them on a \textit{prompt} -- a structured input that specifies the current objective.
%The prompt is maintained in context, such that the generation of subsequent tokens is always conditioned on the prompt. Therefore, performance on the downstream task hinges not only on the coverage of the pre-training dataset but also on the quality and informativeness of the prompt itself~\citep{hu2023prompt, lin2023use, lester2021power}.
By keeping the prompt in context, the model ensures that subsequently generated tokens are aligned with the task. Consequently, the performance of a pre-trained model in a downstream task is contingent not only on the coverage of the pre-training data but also on the quality and informativeness of the provided prompt~\citep{hu2023prompt, lin2023use, lester2021power}.

Building on the success of transformer-based multi-task language models, Offline Reinforcement Learning (ORL) has increasingly adopted transformer architectures, such as the Decision Transformer~(DT)~\citep{chen2021decision}, to address sequential decision-making problems. In the multi-task setting, DT has been extended to the \textit{Prompting Decision Transformer}~(PDT)~\citep{xu2022prompting}, which leverages \textit{stochastic trajectory prompts} -- multiple segments of expert demonstrations -- to enable task-conditioned pre-training and to facilitate few-shot adaptation. These prompts serve as task descriptors that allow PDT to distinguish tasks and to generate actions aligned with the optimal policy distribution for each task. 
However, PDT samples these prompts uniformly at random from the demonstration dataset, overlooking a crucial limitation: We posit that the informativeness of prompts (and, conversely, the segments that compose them) can vary considerably, which can diminish PDT's ability to identify the target task and lead to performance degradation.

% To address this issue, we propose a simple yet effective bandit-based prompt-tuning framework that explores the prompt space. By formulating prompt selection as a contextual bandit problem, our method identifies and exploits high-quality prompts, i.e. those that maximize downstream task performance, without costly modification of the pre-trained Transformer backbone. This approach is scalable, computationally efficient, and seamlessly integrates with PDT, improving performance without requiring additional task-specific fine-tuning.

% We demonstrate the effectiveness of our approach in a controlled proof-of-concept environment, showing that adaptive prompt selection significantly enhances the performance of a pre-trained PDT model. Ongoing work explores extending this method to more complex environments. These findings highlight the importance of prompt optimization in offline RL and reinforce the broader role of prompt quality in transformer-based decision-making models.

To address this limitation, we introduce a simple yet effective bandit-based prompt-tuning framework that actively explores the prompt space. By formulating prompt selection as a contextual bandit problem, our method systematically identifies and exploits prompts that maximize downstream task performance, without requiring costly modifications to the pre-trained Transformer backbone. This approach is scalable, computationally efficient, and seamlessly integrates with PDT, enhancing performance while eliminating the need for additional task-specific fine-tuning.

We validate the effectiveness of our approach in a controlled proof-of-concept environment, demonstrating that adaptive prompt selection significantly improves the performance of a pre-trained PDT model. Furthermore, ongoing work explores extending this method to more complex environments. Our findings underscore the critical role of prompt optimization in offline RL and reinforce the broader significance of prompt quality in transformer-based decision-making models.


\section{Preliminaries}
This section covers the background of our method. We define our learning objective in Sec.~\ref{sec:orl}, formalize the contextual bandit problem in Sec.~\ref{sec:cmab},
and review the PDT~\citep{xu2022prompting} architecture in Sec.~\ref{sec:pdt}.

\subsection{Offline multi-task RL}\label{sec:orl}
An offline multi-task RL problem consists of a set of training tasks $\mathcal{T}^\text{train}$ and optionally several holdout test tasks $\mathcal{T}^\text{test}$. 
Each task $\mathcal{T}_i \in \{ \mathcal{T}_1, \mathcal{T}_2, \dots, \mathcal{T}_n,\}$ corresponds to a Markov Decision Process (MDP), defined as the tuple $\mathcal{M}_i = \langle \mathcal{S}_i, \mathcal{A}_i, r_i, d_i, \gamma_i, \mu_i^0 \rangle$. 
Here, $\mathcal{S}_i$ is the state space, $\mathcal{A}_i$ is the action space, $r_i: \mathcal{S}_i \times \mathcal{A}_i \to \mathbb{R}$ represents the reward function, $d_i: \mathcal{S}_i \times \mathcal{A}_i \times \mathcal{S}_i \to [0, 1]$ defines the discrete-time transition dynamics, $\gamma_i \in (0, 1]$ is the discount factor, and $\mu_i^0$ is the initial state distribution of MDP $i$. 

For each task $\mathcal{T}_i$, we assume access to an offline trajectory dataset $\mathcal{D}_i$.
The trajectories in $\mathcal{D}_i$ can be collected using one or more policies of arbitrary quality. In addition, for PDT, we require a small set of expert demonstrations $\mathcal{P}_i$ to sample stochastic trajectory prompts from.
Our goal is to exploit the available offline data to compute a generalized policy, $\pi(\mathbf{s}, \rho) \to \mathbf{a}$, capable of solving all tasks in $\mathcal{T}^\text{train}$.
Here, $\rho$ is a task descriptor like an index, one-hot encoding, or prompt, ensuring the policy is aware of the current task.
The learning objective for the generalized policy is to maximize the expected discounted reward objective in Eq.~\eqref{eq:generalized-objective} for each task $\mathcal{T}_i \in \mathcal{T}^\text{train}$.
\begin{equation}\label{eq:generalized-objective}
    J(\pi, \rho) = \mathbb{E} \left[ \sum_{t=0}^\infty \gamma_{i}^t r_i(\mathbf{s}_t, \mathbf{a}_t) \right]
\end{equation}

\subsection{Contextual Multi-Armed Bandits}\label{sec:cmab}
Multi-Armed Bandits (MABs) provide a framework for optimizing stochastic reward functions over the course of $K$ rounds.
For each round $k \in \{ 1, \dots, K\}$ the bandit selects an action $a_k \in \mathcal{A}_b$ by pulling one of its arms, where $\mathcal{A}_b$ denotes the bandit's set of arms.
It then perceives a stochastic reward $r_k \sim R(a_k)$ for performing that action, where $R$ is the reward distribution. 
The goal is to maximize the cumulative reward $\sum_{k=1}^K r_k$ over the $K$ rounds, which requires balancing exploration and exploitation of the available arms while minimizing cumulative regret~\citep{auer2002finite}:
% It formally assumes $J$ arms, each associated with a reward distribution. At each time step $k=1, 2, \dots, K$, the algorithm selects an arm (also called action) $a_k \in \{1, 2, \dots, J\}$ and receives a reward distribution $r_k$. The goal is then to balance learning about each arm (exploration) and favoring arms with higher estimated rewards (exploitation) to minimize regret over time, defined as the difference between the expected reward of the optimal arm and the chosen arm:
\begin{equation}
    \text{Regret}(K) = \sum_{k=1}^K \left[ \underset{a \in \mathcal{A}_b}{\max}\ \mathbb{E}[R(a)] - \mathbb{E}[r_k] \right]
\end{equation}
% where $r^*$  is the reward from the optimal arm~\citet{auer2002finite}.

Contextual Multi-Armed Bandits (CMABs) extend standard MABs by 
% conditioning the reward of each arm on an observed context $ c_k$ enabling more informed decision-making. 
incorporating additional information (i.e. ``context'') $\mathbf{c}_k \in \mathcal{C}$ observed at each round $k$. 
The stochastic reward depends on both the action and the context, $r_k \sim R(a_k \mid \mathbf{c}_k)$, meaning a CMAB's objective is to learn a policy $\pi: \mathcal{C} \to \mathcal{A}_b$ that maximizes the expected reward objective $\mathbb{E}[\sum_{k=1}^K R(\pi(\mathbf{c}_k) \mid \mathbf{c}_k)]$. 
% In CMAB, the reward model is defined as a function of arms and context $r_k \sim R(a_k \mid \mathbf{c}_k)$, and the algorithm focuses on learning a mapping from the provided context to the expected reward. This shared context between arms helps balance the exploration-exploitation more effectively, converging to better actions in fewer trials compared to standard MAB~\citet{li2010contextual}.
By exploiting the cross-arm features given by the context, CMABs are credited with better sample efficiency and generalization than their non-contextual counterparts~\citep{li2010contextual}, making them well-suited for efficient prompt-tuning.

\begin{figure*}[]
\begin{equation}
        \rho = \big(
        \overbrace{
        \hat{r}_j, \mathbf{s}_j, \mathbf{a}_j, 
        \dots,
        \hat{r}_{j+H}, \mathbf{s}_{j+H}, \mathbf{a}_{j+H}}
        ^{\text{ $\Tilde{\tau}_1$: segment 1}},
        \dots,
        \overbrace{
        \hat{r}_k, \mathbf{s}_k, \mathbf{a}_k, 
        \dots,
        \hat{r}_{k+H}, \mathbf{s}_{k+H}, \mathbf{a}_{k+H}}
        ^{\text{$\Tilde{\tau}_J$: segment } J}
        \big)
    \label{eq:pdt}
\end{equation}
\begin{equation}
    \mathbf{x} = 
        \big( \rho \big) \odot
        \big(
        \overbrace{
        \hat{r}_{t-K}, \mathbf{s}_{t-K}, \mathbf{a}_{t-K}, 
        \hat{r}_{t-K+1}, \mathbf{s}_{t-K+1}, \mathbf{a}_{t-K+1}
        \dots,
        \hat{r}_{t}, \mathbf{s}_{t}, \mathbf{a}_{t}
        }^{\tau_{N:t}\: N\  \text{most recent transitions}}
        \big)
    \label{eq:pdt-seq}
\end{equation}
\end{figure*}
\subsection{Prompting Decision Transformer}\label{sec:pdt}
With PDT,~\citet{xu2022prompting} treat offline multi-task RL as a sequence learning problem by autoregressively modeling the trajectories in the available offline datasets. 
Trajectories consist of $(\hat{r}_t, \mathbf{s}_t, \mathbf{a}_t)$ triplets, with $\hat{r}_t = \sum^T_{t'=t} r_{t'}$ being return-to-go, needed for conditioning on optimal return. 
For all training tasks $\mathcal{T}^\text{train}$, PDT learns to model the sequence $\mathbf{x}$ in Eq.~\eqref{eq:pdt-seq} by autoregressively predicting the action tokens, where $\odot$ denotes concatenation.
The prompt $\rho$ consists of $J$ segments, each of length $H$, which \citet{xu2022prompting} sample uniformly from the expert demonstrations $\mathcal{P}_i$ for that task.
Instead of relying on uninformed random sampling, we hypothesize that prompts can vary in their usefulness for describing the downstream task, based on segment composition, and propose to optimize prompt and segment selection with a CMAB approach.
As we detail in the next section, the bandit explores directly in the prompt space and learns to select the best prompt constructible from $\mathcal{P}_i$.
        
\section{Method: Prompt-tuning contextual bandit}\label{sec:method}
We propose a contextual multi-armed bandit (CMAB) architecture to optimize the prompt selection and segment composition to improve the performance of a pre-trained PDT backbone on a downstream task $\mathcal{T}_i$. 
To this end, we assume access to a PDT $\theta^*$, pre-trained until convergence on a multi-task dataset $\mathcal{D}$, a small number of expert demonstrations $\mathcal{P}_i$ to select prompts from, and a simulator $\mathcal{M}_i$ for the downstream task $i$.

At a high level, our approach operates as follows. For each round $k \in \{1, \dots, K \}$, the bandit selects a prompt $\rho_k$ from $\mathcal{P}_i$ which is prepended to the PDT's input according to Eq.~\eqref{eq:pdt-seq}.
We then proceed by rolling out the PDT, conditioned on $\rho_k$, in $\mathcal{M}_i$ and take note of the achieved online return $G_k = \sum_{t=0}^T r_i(\mathbf{s}_t, \mathbf{a}_t) \mid \mathbf{a}_t \sim \pi(\mathbf{x}_t, \theta^*)$ for that round. 
Note that while $\tau_{N:k}$ in Eq.~\eqref{eq:pdt-seq} is dynamically updated to reflect the last $N$ steps in the episode, the prompt remains fixed during an entire episode.
From the bandit's perspective, $G_k$ serves as a reward for selecting prompt $\rho_k$, and the tuple $\langle \rho_k, G_k \rangle$ is stored for training the bandit's reward model.

We now detail our CMAB architecture and how it constructs prompts at each round $k$.
Instead of relying on a na\"ive bandit that employs one arm per prompt constructible from $\mathcal{P}_i$ (which would scale linearly with the size of $\mathcal{P}_i$ and combinatorially with the number of segments $J$), we exploit the similarity between prompt segments and employ a contextual bandit, with $J$ arms.
Our bandit maintains a separate reward model $\phi_j: \Tilde{\tau} \to \mathbb{R}$ for each arm $j \in \{ 1, \dots, J\}$, and treats segments as context.
These models estimate the return achieved by PDT $\theta^*$ when segment $\Tilde{\tau}_l$ is placed at position $j$ in the prompt. 
Thus, at each round $k$, our bandit predicts the reward for each segment in each position, resulting in a prediction matrix $\mathbf{Y}$, with $J$ columns and rows equal to $|\mathcal{P}_i|$, the number of segments in the given expert demonstration dataset.
To select a prompt, the bandit can either exploit based on accumulated knowledge and $\arg \max \mathbf{Y}$ along the segments' dimension, or explore using some exploration mechanisms such as $\epsilon$-greedy, or Upper Confidence Bounds (UCB). This approach has high variance because each reward model $\phi_j$ assumes that its corresponding segment $\Tilde{\tau}_{j}$ is the sole determinant of the observed performance $G_k$, thereby ignoring contributions from the other segments in the prompt. Nevertheless, our experiments indicate that this approach works well in practice.

\section{Experiments}
This section outlines our experimental procedure. We first introduce the multi-task environment and offline dataset, then describe the baselines in Sec.~\ref{sec:baselines} followed by results and analysis in Sec.~\ref{sec:results}.

\begin{wrapfigure}{r}{0.35\textwidth}
  \begin{center}
    \includegraphics[width=0.3\textwidth]{imgs/2d_point_agent_radii_stop_env.png}
  \end{center}
  \caption{Our proof-of-concept, 2D multi-task environment.}
\end{wrapfigure}

% \subsection{Environment}\label{sec:env}
\textbf{Environment}:
We evaluate the proposed prompt-tuning CMAB in a 2D proof-of-concept environment. This environment features a planar 2D point agent that has to reach a goal coordinate. The state contains the agent's 2D coordinate at each step $t$. The action space contains two continuous actions for translating on the plane, with the step size being limited by projecting the translation vector on a unit circle with a radius of 0.1. In addition, the action space contains a \texttt{stop} action which allows the agent to terminate the episode. When selected, the episode ends, and the agent receives a sparse reward proportional to its distance from the goal. A bonus of +10 reward is provided (discounted for exceeding the optimal number of steps) for stopping in close proximity of the goal coordinate.
To create a multi-task setting, we parameterize tasks by $(r, \alpha)$, the goal's radius and angle. We discretize the task sparsely using 20 discrete angles $\alpha \in \{ 0.1 \cdot \pi, 0.2 \cdot \pi, \dots, 2 \cdot \pi  \}$ and three discrete radii $r \in \{ 0.9, 1.9, 2.9 \}$, yielding a total of 60 tasks.

% \subsection{Offline dataset}\label{sec:dataset}
\textbf{Offline dataset and pre-training}:
We collect an offline multi-task dataset by training Proximal Policy Optimization (PPO)~\citep{schulman2017proximal} for 1M steps on each of the 60 tasks, storing the trajectories as $\mathcal{D}_i$. We extract trajectories from the top percentile from $\mathcal{D}_i$ to serve as expert demonstrations $\mathcal{P}_i$ for that task.
We then train PDT, without modifications, on $\mathcal{D} = \{ \mathcal{D}_1, \mathcal{D}_2, \dots, \mathcal{D}_{60}\}$ and $\mathcal{P} = \{ \mathcal{P}_1, \mathcal{P}_2, \dots, \mathcal{P}_{60}\}$ until convergence; see \citet{xu2022prompting} for details.

\subsection{Baselines}\label{sec:baselines}
We compare our proposed bandit-based prompt-tuning method, qualitative and empirically, against the following baselines.

\textbf{Standard PDT}~\citep{xu2022prompting} without prompt-tuning: This baselines reveals the possible performance gains due to prompt-tuning at inference time.

\textbf{ZO-RankSGD-based prompt-tuning}~\citep{hu2023prompt}: Closely related to our bandit-based method, this approach proposes prompt-tuning for PDT by employing ZO-RankSGD~\citep{tang2023zeroth} to estimate the gradient of the prompt with respect to online task return $G$. The method samples and initial prompt $\rho_0 \sim \mathcal{P}_i$, and, at each rounds $k$, estimates the gradient 
% $\hat{\nabla}_{\rho} [ \sum_{t=0}^T r_t \mid \mathbf{a} \sim \pi(\mathbf{x}, \theta^*) ] = \hat{\nabla}_{\rho} G$ 
$\hat{\nabla}_{\rho} G$ 
based on the ranking between $m$ perturbed versions of $\rho_k$.  The perturbed versions of the prompt are obtained as $\rho_k' = \rho_k + \epsilon \mathcal{N}(0, I_d)$, where $\epsilon$ is the noise scale, $I_d$ is the $d \times d$ identity matrix, and $d = |\rho|$ is the length of the prompt.
The prompt is then updated according to $\rho_{k+1} \leftarrow \rho_k + \eta\hat{\nabla}_{\rho} G$, where $\eta$ is the learning rate that we anneal from $1$ to $0.1$ over the $K$ rounds. 
Crucially, at each round $k$, all of the $m$ prompt-perturbations must be evaluated with an online rollout of the PDT, meaning the sample complexity of this method is $m$ times larger that of our method.

\textbf{Gaussian perturbation hill climbing}: A simple stochastic optimization approach inspired by hill climbing. Given an initial prompt $\rho \sim \mathcal{P}_i$, we iteratively perturb the sampled prompt by applying Gaussian noise. 
At each round $k$, the perturbed prompt is obtained as $\rho_k = \rho + \epsilon \mathcal{N}(0, I_d)$.
% , where $\epsilon$ is the noise scale, $I_d$ is the $d \times d$ identity matrix, and $d = |\rho|$ is the length of the prompt. 
We anneal $\epsilon$ from $1$ to $0.1$ over the $K$ rounds. The perturbed prompt $\rho_k$ is evaluated by rolling out the PDT. If the resulting return $G_k$ exceeds the best return so far, we update the prompt $\rho \leftarrow \rho_k$, thereby performing hill climbing with respect to online task return directly in the prompt space.
    

\subsection{Results \& Analysis}\label{sec:results}
\textbf{Does bandit-based prompt-tuning improve a frozen PDT backbone}?
We perform prompt tuning on the pre-trained PDT $\theta^*$ using 250 online rollouts on training tasks with radius $r=2.9$. We run this experiment with $J \in \{ 1, 2, 4\}$, i.e., with increasingly many segments and, conversely, tokens for task identification in the prompt. We run our bandit-based prompt-tuning method with UCB~\citep{li2010contextual} and $\epsilon$-greedy exploration strategies.

Results are shown in Fig.~\ref{fig:online-propmt-tuning}, performance gains due to prompt-tuning are most prominently visible in Fig.~\ref{subfig:online_j1}, where the prompt consists of a single segment of length $H=3$, for a total of $J\times H \times (|\mathcal{S}| + |\mathcal{A}| + 1) = 1 \times 3 \times (2 + 3 + 1) = 18$ prompt tokens.
Despite being trained to convergence on all training tasks, PDT without prompt tuning fails to achieve the optimal return. This shortfall is due to the uninformed, random prompt sampling strategy used by standard PDT which frequently selects uninformative prompts, limiting its performance.
Our bandit-based prompt-tuning approach, however, quickly boosts the performance of the underlying PDT backbone to optimal levels of return by identifying high-return prompts, with no considerable difference between $\epsilon$-greedy or UCB exploration.
The other prompt-tuning baseline methods, Gaussian perturbation with hill climbing and ZO-RankSGD-based prompt-tuning, also demonstrate clear improvements over the course of the 250 online rollouts, though they are less efficient than the bandit approach. Notably, ZO-RankSGD requires $m=5$ additional online rollouts for each prompt-gradient estimation, resulting in a total of $5 \times 250 $ online rollouts. In contrast, our bandit-based approach rapidly converges,  consistently selecting optimal prompts within the first few rollouts. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.7\textwidth]{imgs/workshop_paper_plot_legend.png}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/workshop_paper_plot_j1.png}
        \caption{$J=1$}\label{subfig:online_j1}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/workshop_paper_plot_j2.png}
        \caption{$J=2$}\label{subfig:online_j2}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/workshop_paper_plot_j4.png}
        \caption{$J=4$}\label{subfig:online_j4}
    \end{subfigure}
    \caption{Inference time performance gains due to prompt-tuning over 250 episodes. 
    % For the ZO-RankSGD baseline, we don't plot the performance inquired with the $m$ perturbed prompt versions that are used to estimate the gradient. 
    ZO-RankSGD performs a total of $m=5 \times 250$ rollouts 
    % to perform 250 prompt-gradient steps, which we squash into the same 0 - 250 range in the plot.
    to estimate the prompt gradient 250 times, which we squash into the 0 - 250 range in the plot.
    Results are averaged over all training tasks and three seeds. To increase readability, the shaded area corresponds only to 0.25 standard deviations around the mean.}\label{fig:online-propmt-tuning}
\end{figure*}

Additionally, we observe the following trends as prompt size increases.
First, although PDT performance without prompt-tuning remains roughly constant over 250 rollouts, it scales approximately proportionally with prompt size, reducing the performance gain from prompt-tuning.
In Fig.~\ref{subfig:online_j4}, PDT achieves near-optimal return even without prompt-tuning, which implies that, in our proof-of-concept environment, exhaustive random sampling suffices for finding tokens that uniquely identify the downstream task.
% Interestingly, the Gaussian perturbation baseline scales poorly with the prompt size. 
Interestingly, both Gaussian perturbation and the ZO-RankSGD baseline scale poorly with the prompt size.
We hypothesize that this stems from their strategy of perturbing the \textit{entire} prompt at each round, which can unnecessarily disrupt informative segments by injecting excessive noise, even when the original prompt is nearly optimal.
In contrast, our bandit-based method avoids this issue by exploring prompt segments independently with each arm. This enables it to preserve high-performing segments while selectively exploring others, \textit{without} adding unnecessary noise to effective parts of the prompt.

% These results demonstrate the effectiveness of our method, highlight the need for informed prompt-tuning methods that efficiently explore the prompt space, and motivate further experimentation in more challenging environments.

\textbf{How does bandit-based prompt-tuning explore the prompt space?}
We visualize selected prompts in the beginning ($K$: 0 - 20) and towards later stages ($K$: 70 - 90) of exploration in Fig~\ref{fig:spatio-bandit-gaussian}. 
PDT's uniform prompt selection strategy, with no difference between the early or later stages, can be seen in Fig.~\ref{subfig:spatio-random}.
As shown in Fig.~\ref{subfig:spatio-bandit}, our bandit-based approach initially explores the entire prompt space, experiencing low- and high-performance prompts. However, in later rounds, the bandit prioritizes prompts that are closer to the goal states while avoiding the low-performance prompts near the center,  as these provide less informative signals for the task.

The Gaussian perturbation method in Fig.~\ref{subfig:spatio-gaussian} primarily explores locally. This is due to the hill climbing optimization, which finds the best-performing prompt in vicinity of the initially sampled prompt while falling short of exploring the whole prompt space. 
The ZO-RankSGD-based prompt-tuning in Fig.~\ref{subfig:spatio-zoranksgd} similarly explore only locally, revealing a strong dependence on the initialization. 
Unlike incremental approaches, our bandit method is less reliant on the initially sampled prompt. Instead, it exploits segment similarities to identify the best segments in $\mathcal{P}_i$.  
This result highlights a key limitation of perturbation-based methods in prompt-space exploration and illustrates how our bandit approach effectively selects prompts that drive performance improvements.

\begin{figure*}[t!]
    \centering
    %\includegraphics[width=0.3\textwidth]{imgs/spatio_temporal_segments-colorbar_allTasks_banditVsGaussian.png}
    %\vfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/spatio_temporal_segments_allTasks_Random.png}
        \caption{Standard PDT, no tuning}\label{subfig:spatio-random}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/spatio_temporal_segments_allTasks_Bandit.png}
        \caption{Our CMAB approach, $\epsilon$-greedy}\label{subfig:spatio-bandit}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/spatio_temporal_segments_allTasks_Gaussian.png}
        \caption{Gaussian perturbation and hill climbing}\label{subfig:spatio-gaussian}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/spatio_temporal_segments_allTasks_ZORankgSGD.png}
        \caption{ZO-RankSGD \citep{xu2022prompting}}\label{subfig:spatio-zoranksgd}
    \end{subfigure}
    ~
    \caption{Spatio-temporal comparison between prompt selection approaches. Prompts are plotted by the mean spatial coordinate of the states in the prompt and colored according to the achieved return \includegraphics[height=\baselineskip]{imgs/spatio_temporal_segments-colorbar_allTasks_onlyBar.png} when using that prompt. The MDP's starting state is indicated with the red diamond, and the goal states for different tasks are indicated by the red stars. $K$ denotes the bandit rounds for each image.}\label{fig:spatio-bandit-gaussian}
\end{figure*}


\section{Conclusion \& Future Work}
We introduce a bandit-based prompt-tuning approach for multi-task foundation agents which efficiently navigates the prompt space to identify high-performing prompts for downstream tasks, avoiding exhaustive random sampling.
Preliminary results in a 2D proof-of-concept environment suggest that our method enhances a pre-trained PDT to optimal performance where uniform prompt sampling fails.
We believe this warrants future research and are exploring the scalability of the proposed method to more complex and challenging environments and aim to present additional results at the workshop.

A potential limitation of our approach is the use of independent reward models for the $J$ segments in the stochastic trajectory prompt, which neglects correlations among segments and leads to a high-variance estimator. An important future direction is to develop a more sophisticated bandit architecture that disentangles segment contributions to overall performance.

\bibliography{main}

\end{document}
