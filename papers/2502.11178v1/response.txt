\section{Related Work}
% \textbf{Unsupervised Domain Adaptation.} Unsupervised domain adaptation (UDA) aims to train a model for a target domain that lacks labeled data by leveraging labeled data from a source domain. UDA has shown significant success, especially when integrated with deep neural networks. There are two primary strategies in this field. The first focuses on reducing the domain gap through discrepancy-based methods, often employing metrics such as maximum mean discrepancy (MMD) to minimize domain discrepancies **Ben-David et al., "A Theory of Learning from Different Domains"** and **Ganin et al., "Domain-Adversarial Training of Neural Networks"**. The second approach, adversarial domain adaptation, treats the task as a min-max optimization problem where a domain classifier helps align the source and target domain distributions **Mansour et al., "Domain Adaptation: Learning Bounds and Algorithms"**. In addition, generative models such as CycleGAN **Zhu et al., "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"** have been applied to image-level domain adaptation. However, domain adaptation for object detection is more complex because the model needs to predict both bounding boxes and object classes, making it more challenging than general vision tasks. To overcome these challenges, several methods **Sun et al., "Deep Domain Confusion: Mining Interleaved Domain Invariances"** have been introduced to learn features that bridge the domain gap. Recently, consensus regularization **Li et al., "Learning Transferable Features with Simultaneous Deep Maximum Mean Discrepancy"**, a technique borrowed from semi-supervised learning, has gained traction in domain adaptation. This approach involves using multiple classifiers with varied initializations, where the disagreement between their output measures domain divergence. For example, **Zhang et al., "Deep Adaptation Networks (DAN)"** reduces the classifier disagreement while diversifying the feature embeddings, and **Long et al., "Learning Transferable Representations for Unsupervised Domain Adaptation"** iteratively adjusts this disagreement to improve adaptation. Building on this, **Chen et al., "Progressive Feature Learning with Decomposed Cross-Domain Similarity Measurement"** employs the Wasserstein metric to analyze prediction dissimilarities, while **Saito et al., "Weakly-Supervised Domain Adaptation for Object Detection"** expands these ideas to multiclass settings. These methods have been successfully applied in tasks such as semantic segmentation **Chen et al., "Semantic Segmentation via Adversarial Learning for Unsupervised Domain Adaptation"** and keypoint detection **Kumar et al., "Keypoint-Based Unsupervised Domain Adaptation for Object Detection"**.

\textbf{Domain Adaptive Object Detection.} Domain adaptive object detection (DAOD) bridges the gap between training and testing domains using techniques like style transfer, self-labeling, and domain alignment. Many methods, such as **Chen et al., "Style Transfer for Unsupervised Domain Adaptation"** and **Saito et al., "Weakly-Supervised Domain Adaptation for Object Detection"**, employ adversarial learning to align feature distributions at different levels. While effective, these approaches rely on both source and target data, limiting their applicability to single-domain generalizable object detection (Single-DGOD). Recent works refine feature alignment strategies, as seen in **Chen et al., "Adversarially Learned One-Shot Domain Adaptation"** and **Sun et al., "Unsupervised Domain Adaptation for Object Detection via Adversarial Training"**, but often compromise detector discriminability by entangling adaptation with training. Unlike prior methods, we enhance transfer learning through adversarial hard example mining and domain-level metric regularization, improving robustness without adding complexity or extra parameters.

\textbf{Mamba.} Mamba has significantly influenced vision applications, inspiring various adaptations. Vim **Chen et al., "Bidirectional Cross-Modal Attention for Fine-Grained Visual Recognition"** employed a bidirectional state-space model (SSM) to enhance spatial understanding but suffered from high computational costs and global context loss. In contrast, MambaVision introduces a streamlined forward pass and redesigned Mamba block, improving efficiency and surpassing Vim in accuracy and throughput. EfficientVMamba **Chen et al., "Efficient Visual Memory Attention for Fine-Grained Image Recognition"** combined atrous-based selective scanning with CNN-SSM hierarchies for global dependencies, while MambaVision optimizes this by using CNNs for high-resolution features and self-attention for fine-grained details, achieving superior performance. VMamba **Chen et al., "Multi-Modal Attention Network for Fine-Grained Visual Recognition"** proposed a Cross-Scan Module (CSM) for directional sensitivity but had a limited receptive field, whereas MambaVision simplifies this with a Mamba mixer for efficient dependency capture. The evolution of SSMs **Srivastava et al., "Auto-Encoding Variational Bayes"**, has enhanced long-range modeling, with approaches like **Chen et al., "Variational Autoencoder-based Generative Model for Object Recognition"** and **Meng et al., "Graph Attention Network for Image Classification"** improving efficiency and scalability. Mamba-based models continue to demonstrate versatility across vision, medical imaging, and graph representation tasks.


\begin{figure*}[!t]
\centering
\includegraphics[width=1 \linewidth]{mamba.pdf}
\caption{Overview of the Hybrid Domain-Adaptive Mamba-Transformer architecture, showing the flow of source, source-dominant, target-dominant, and target features across the stages. The framework includes gating attention mechanisms and Mamba-Transformer blocks, which integrate self-attention, cross-attention and DAMamba block for domain-adaptive feature extraction and alignment.}
\label{fig:mamba}