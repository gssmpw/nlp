\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
% \usepackage{cite}
% \usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
% \usepackage{graphicx}
% \usepackage{textcomp}
% \usepackage{booktabs} 
% \usepackage{xcolor}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{mathtools}
% \usepackage[table]{xcolor}
% \usepackage{amsthm}
% \usepackage{microtype}
% \usepackage{graphicx}
% \usepackage{subfigure}
% \usepackage{booktabs} % for professional tables
% \usepackage[colorlinks]{hyperref}

\usepackage{cite}
\usepackage{amsthm}  % Move this to the top to avoid conflicts
\usepackage{amsmath, amssymb, amsfonts}  % Merge duplicate imports
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage[table]{xcolor}  % Fix xcolor clash
\usepackage{mathtools}
\usepackage{microtype}
\usepackage{subfigure}
\usepackage[colorlinks]{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{DAViMNet: SSMs-Based Domain Adaptive Object Detection
}

\author{\IEEEauthorblockN{1\textsuperscript{st} A. Enes Doruk}
\IEEEauthorblockA{\textit{Department of Artificial Intelligence and Data Eng.} \\
\textit{Ozyegin University}\\
Istanbul, Turkiye \\
enes.doruk@ozu.edu.tr}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Hasan F. Ates}
\IEEEauthorblockA{\textit{Department of Artificial Intelligence and Data Eng.} \\
\textit{Ozyegin University}\\
Istanbul, Turkiye \\
hasan.ates@ozyegin.edu.tr}
}

\maketitle

\begin{abstract}
Unsupervised Domain Adaptation (UDA) for object detection adapts models trained on labeled source domains to unlabeled target domains, ensuring robust performance across domain shifts. Transformer-based architectures excel at capturing long-range dependencies but face efficiency challenges due to their quadratic attention complexity, which limits scalability in UDA tasks. To address these issues, we propose a Hybrid Domain-Adaptive Mamba-Transformer architecture that combines Mamba’s efficient state-space modeling with attention mechanisms to tackle domain-specific spatial and channel-wise variations. Each hybrid block integrates Domain-Adaptive Mamba (DAMamba) blocks and attention mechanisms: DAMamba employs spatial and channel state-space models (SSMs) to adaptively model domain variations, while attention mechanisms leverage self-attention for intra-domain feature enhancement and cross-attention for effective source-target alignment. Our approach processes both shallow and deeper features, employing an entropy-based knowledge distillation framework with margin ReLU to emphasize discriminative features and suppress noise. Gradient Reversal Layers enable adversarial alignment across network layers, while entropy-driven gating attention with random perturbations refines target features and mitigates overfitting. By unifying these components, our architecture achieves state-of-the-art performance in UDA object detection, balancing efficiency with robust generalization. our code available at \href{https://github.com/enesdoruk/DAVimNet}{enesdoruk/DAVimNet}.
\end{abstract}

\begin{IEEEkeywords}
Domain Adaptation, Object Detection, Mamba, Unsupervised Learning
\end{IEEEkeywords}



\begin{figure}[!ht]
\centering
\includegraphics[width=1 \linewidth]{overall.pdf}
\caption{ Overall DAVimNet architechture.}
\label{fig:arch}
\end{figure}

\section{Introduction}
Object detection is a fundamental task in computer vision that involves identifying and localizing objects within predefined categories using bounding boxes. It plays a crucial role in applications such as autonomous driving, video surveillance, and robotics, driving significant advancements \cite{ren2016faster, tian2022fully, redmon2016you, girshick2014rich}. However, deep learning-based object detectors often struggle with \textit{domain shift}, where performance degrades when tested on visually different datasets. This challenge complicates real-world deployment, as seen in self-driving cars encountering diverse weather conditions or surveillance systems facing unseen environments \cite{chen2018domain}.

Unsupervised Domain Adaptation (UDA) mitigates domain shift by adapting models trained on labeled source domains to perform effectively on unlabeled target domains \cite{deng2021domain, gonzalez2018image, chen2019progressive, ganin2015unsupervised, sun2016deep}. Key approaches include adversarial training with the Gradient Reversal Layer (GRL) \cite{ganin2015unsupervised} for image- and instance-level adaptation \cite{vs2021mega, su2020adapting, zhu2019adapting, saito2019strong, chen2018domain}, as well as pseudo-labeling strategies for refining target predictions \cite{vs2023instance, zhao2020collaborative, yu2022sc}. While two-stage UDA detectors have seen substantial progress, one-stage approaches remain underexplored despite their efficiency for real-time applications.


Recent work leverages data augmentation to improve model generalization \cite{zhang2017mixup, yun2019cutmix, hendrycks2019augmix}, though adaptation for object detection remains challenging \cite{chen2019crdoco, kim2020structured, olsson2021classmix}. Moreover, category-level adaptation methods \cite{zheng2020cross, zhang2021rpn, xu2020cross, tian2021knowledge} struggle with class variance and semantic mismatch, limiting their effectiveness. Addressing these gaps is crucial for improving classification and localization in cross-domain object detection.
To address challenges in Unsupervised Domain Adaptation (UDA) for object detection, this paper presents a domain-adaptive framework built on the Single Shot Detector (SSD) model. The framework comprises six stages: the first two use convolutional blocks to capture low-level spatial features like edges and corners, while the remaining four integrate Domain-Adaptive Mamba (DAMamba) blocks with self- and cross-attention mechanisms for advanced feature extraction and alignment.

DAMamba blocks combine spatial and channel-adaptive SSMs to address domain shifts. Self-attention refines intra-domain features, and cross-attention generates source- and target-dominant features for soft alignment. Entropy-driven knowledge distillation (KD) with margin ReLU emphasizes target-domain features while suppressing noise, and dual-level adversarial alignment uses GRL-trained discriminators for pixel- and feature-level alignment. An entropy-driven perturbation module further enhances robustness by adaptively fusing features and regularizing alignment through selective perturbations.


The key contributions of this paper are summarized as follows:
\begin{itemize}
    \item Hybrid Domain-Adaptive Mamba-Transformer. DAMamba layers combine spatial- and channel-based domain alignment using Mamba-style state-space models (SSMs) and attention mechanisms. Self-attention enhances intra-domain features, while cross-attention generates source- and target-dominant features for soft alignment.
    
    \item Entropy-Based Knowledge Distillation. Margin ReLU--guided entropy loss ensures consistent transfer of source- and target-dominant features, reducing domain gaps while suppressing noise.
    
    \item Adversarial Learning for Bridging Cross-Domain Discrepancies. Two GRL-trained discriminators align pixel-level features in shallow layers and feature-level representations in deeper layers, ensuring domain invariance across scales.
    
    \item Entropy-Guided Random Multi-Layer Perturbation. A gating mechanism adaptively fuses features based on entropy, with random perturbations enhancing robustness and preventing overfitting in intermediate and deep layers.
\end{itemize}

\section{Related Work}
% \textbf{Unsupervised Domain Adaptation.} Unsupervised domain adaptation (UDA) aims to train a model for a target domain that lacks labeled data by leveraging labeled data from a source domain. UDA has shown significant success, especially when integrated with deep neural networks. There are two primary strategies in this field. The first focuses on reducing the domain gap through discrepancy-based methods, often employing metrics such as maximum mean discrepancy (MMD) to minimize domain discrepancies \cite{long2015learning, long2016unsupervised, long2017deep}. The second approach, adversarial domain adaptation, treats the task as a min-max optimization problem where a domain classifier helps align the source and target domain distributions \cite{tzeng2017adversarial, sankaranarayanan2018generate, ganin2016domain, ganin2015unsupervised}. In addition, generative models such as CycleGAN \cite{zhu2017unpaired} have been applied to image-level domain adaptation. However, domain adaptation for object detection is more complex because the model needs to predict both bounding boxes and object classes, making it more challenging than general vision tasks. To overcome these challenges, several methods \cite{zhang2020unsupervised, tzeng2017adversarial, saito2018maximum, lee2019sliced, kumar2018co, ganin2016domain} have been introduced to learn features that bridge the domain gap. Recently, consensus regularization \cite{luo2008transfer}, a technique borrowed from semi-supervised learning, has gained traction in domain adaptation. This approach involves using multiple classifiers with varied initializations, where the disagreement between their output measures domain divergence. For example, \cite{kumar2018co} reduces the classifier disagreement while diversifying the feature embeddings, and \cite{saito2018maximum} iteratively adjusts this disagreement to improve adaptation. Building on this, \cite{lee2019sliced} employs the Wasserstein metric to analyze prediction dissimilarities, while \cite{zhang2020unsupervised, zhang2019domain} expands these ideas to multiclass settings. These methods have been successfully applied in tasks such as semantic segmentation \cite{luo2019taking, zheng2021rectifying} and keypoint detection \cite{jiang2021regressive, zhou2018unsupervised}.

\textbf{Domain Adaptive Object Detection.} Domain adaptive object detection (DAOD) bridges the gap between training and testing domains using techniques like style transfer, self-labeling, and domain alignment. Many methods, such as Chen et al. \cite{chen2018domain} and Saito et al. \cite{saito2019strong}, employ adversarial learning to align feature distributions at different levels. While effective, these approaches rely on both source and target data, limiting their applicability to single-domain generalizable object detection (Single-DGOD). Recent works refine feature alignment strategies, as seen in SWDA \cite{saito2019strong} and HTCN \cite{chen2020harmonizing}, but often compromise detector discriminability by entangling adaptation with training. Unlike prior methods, we enhance transfer learning through adversarial hard example mining and domain-level metric regularization, improving robustness without adding complexity or extra parameters.

\textbf{Mamba.} Mamba has significantly influenced vision applications, inspiring various adaptations. Vim \cite{zhu2024vision} employed a bidirectional state-space model (SSM) to enhance spatial understanding but suffered from high computational costs and global context loss. In contrast, MambaVision introduces a streamlined forward pass and redesigned Mamba block, improving efficiency and surpassing Vim in accuracy and throughput. EfficientVMamba \cite{pei2024efficientvmamba} combined atrous-based selective scanning with CNN-SSM hierarchies for global dependencies, while MambaVision optimizes this by using CNNs for high-resolution features and self-attention for fine-grained details, achieving superior performance. VMamba \cite{zhu2024vision} proposed a Cross-Scan Module (CSM) for directional sensitivity but had a limited receptive field, whereas MambaVision simplifies this with a Mamba mixer for efficient dependency capture. The evolution of SSMs \cite{gu2021efficiently, gu2023mamba} has enhanced long-range modeling, with approaches like S4 \cite{gu2021efficiently} and Mamba \cite{gu2023mamba} improving efficiency and scalability. Mamba-based models continue to demonstrate versatility across vision, medical imaging, and graph representation tasks.


\begin{figure*}[!t]
\centering
\includegraphics[width=1 \linewidth]{mamba.pdf}
\caption{Overview of the Hybrid Domain-Adaptive Mamba-Transformer architecture, showing the flow of source, source-dominant, target-dominant, and target features across the stages. The framework includes gating attention mechanisms and Mamba-Transformer blocks, which integrate self-attention, cross-attention and DAMamba block for domain-adaptive feature extraction and alignment.}
\label{fig:mamba}
\end{figure*}

\section{Method}

\subsection{Problem Formulation}
Before demonstrating how our method reduces domain gap in domain-adaptive object detection, we first outline the problem formulation. Let \( D_s = \{ (X_s, B_s, C_s) \} \) represent a set of \( N_s \) labeled images in the source domain, where \( B_s = \{ b_i^s \}_{i=1}^{N_s} \) and \( C_s = \{ c_i^s \}_{i=1}^{N_s} \) indicate the corresponding bounding boxes and class labels for source images \( X_s = \{ x_i^s \}_{i=1}^{N_s} \). In the target domain, we have \( D_t = \{ X_t \} \), consisting of \( N_t \) unlabeled images, \( X_t = \{ x_j^t \}_{j=1}^{N_t} \), with no annotations. The primary goal is to develop domain-invariant detectors using both \( D_s \) and \( D_t \).

\subsection{Preliminaries}
\textbf{State-Space Models (SSMs)} \cite{gu2021efficiently} have garnered considerable attention for their capacity to model temporal dynamics by mapping input sequences to output sequences through a hidden state. At their core, SSMs are derived from linear time-invariant systems and are governed by a continuous framework that encapsulates these dynamics \cite{gu2023mamba}. Mathematically, SSMs are formulated as follows:

\[h'(t) = A h(t) + B x(t)\]
\[y(t) = C h(t)\]

Here, \( h(t) \in \mathbb{R}^N \) is the hidden state, \( A \in \mathbb{R}^{N \times N} \) denotes the state transition matrix, \( B \in \mathbb{R}^N \) represents the input mapping matrix, and \( C \in \mathbb{R}^N \) is the output mapping matrix. This formulation allows the model to evolve over time by processing an input sequence \( x(t) \in \mathbb{R} \), which generates an output sequence \( y(t) \in \mathbb{R} \) through the latent intermediate state.

Due to the inherent continuous nature of the system, discretization is necessary for practical use in machine learning tasks. A common discretization technique is the Zero-Order Hold (ZOH), which converts the continuous system into its discrete counterpart. The discrete-time version of the system is expressed as:

\[h_t = A h_{t-1} + B x_t\]
\[y_t = C h_t\]

Where \( A = \exp(\Delta A) \) and \( B = (\Delta A)^{-1} (\exp(\Delta A) - I) \Delta B \). In this transformation, \( \Delta \) represents the time scale parameter, and \( I \) is the identity matrix. This discretization ensures that the system can operate on discrete-time data, aligning with the sample rate of real-world datasets.

SSMs have been a foundation for sequence modeling due to their ability to capture long-range dependencies \cite{gu2021efficiently}. However, the computational complexity of these models, especially in the continuous domain, poses challenges in terms of efficiency.


\subsection{Framework Overview}
Our framework for unsupervised domain adaptation in object detection builds upon the Single Shot Detector (SSD) \cite{liu2016ssd} baseline, enhanced with a six-stage Hybrid Domain-Adaptive Mamba-Transformer backbone (Figure \ref{fig:arch}). This architecture combines Mamba’s efficient long-range modeling with attention mechanisms to capture critical spatial and semantic dependencies, creating a robust foundation for domain-invariant object detection.

The first two stages employ convolutional blocks to extract spatial information from low-level features (edge, corner, etc.), establishing the groundwork for domain adaptation. The subsequent four stages consist of Hybrid Domain-Adaptive Mamba-Transformer blocks. Each block includes DAMamba and attention mechanisms for comprehensive domain alignment. DAMamba blocks address domain adaptive spatial SSM through depthwise convolutions, guided by cosine similarity for adaptive reweighting, while domain adaptive channel SSM is achieved through feature swapping between source and target domains, refined using cosine similarity along channel dimensions. Attention mechanisms further enhance alignment, with and cross-attention generating source-dominant and target-dominant features. These four flows—source, source-dominant, target-dominant, and target—facilitate soft domain alignment. However, only the source-dominant flow is utilized for detection. Consistency between cross-attended features is reinforced through entropy-based knowledge transfer, promoting effective domain adaptation.

To further strengthen domain invariance, dual-level adversarial alignment is implemented using gradient reversal layers, addressing pixel-level alignment in shallow convolutional stages and feature-level alignment in deeper hybrid stages. Additionally, an entropy-based random multi-layer perturbation mechanism adaptively fuses dominant features and selectively perturbs intermediate and deep layers, enhancing robustness and mitigating overfitting.

Finally, the source-dominant features are passed to the SSD head section for detection, ensuring optimal alignment for bounding box regression and classification. The detection process is optimized using the total detection loss, defined as:

\begin{equation}
L_{det} = L_{box} + L_{cls}
\label{eq:det}
\end{equation}


\begin{figure*}[!t]
\centering
\includegraphics[width=0.8 \linewidth]{damamba.pdf}
\caption{ Structure of the DAMamba block, showing the Spatial SSM, Channel SSM, and self- and cross-attention mechanisms for spatial and channel-level feature alignment and cross-domain interactions.}
\label{fig:mambablock}
\end{figure*}

\subsection{Hybrid Domain-Adaptive Mamba-Transformer}
The proposed Hybrid Domain-Adaptive Mamba-Transformer (HDAMT) framework is designed to effectively extract and align features across source and target domains through a structured six-stage architecture. As depicted in Figure~\ref{fig:mamba}, each Hybrid Doamin-Adaptive Mamba-Transformer block integrates Domain-Adaptive Mamba (DAMamba) mechanisms alongside self- and cross-attention operations, enabling refined feature alignment. To further enhance domain adaptation, DAMamba employs two selective scanning mechanisms: the Spatial Selective Scanning Mechanism (Spatial SSM) and the Channel Selective Scanning Mechanism (Channel SSM), as illustrated in Figure~\ref{fig:mambablock}. These mechanisms play a crucial role in addressing both spatial and channel-level feature alignment, ensuring robust cross-domain adaptation.

The framework follows a hierarchical feature extraction strategy, where the first two stages utilize convolutional blocks to extract low-level spatial features such as edges and textures. Leveraging the strong inductive bias of convolutions toward local structures, these layers establish a solid foundation for subsequent domain-adaptive processing. In stages 3 through 6, the Mamba-Transformer blocks take over, progressively refining feature representations through domain-adaptive selective scanning and advanced attention mechanisms. 

To facilitate effective domain alignment, each Hybrid Domain-Adaptive Mamba-Transformer block tightly integrates the Spatial SSM and Channel SSM with self- and cross-attention operations. This synergy allows for precise spatial and channel-wise feature adaptation, ensuring smooth feature exchange between source and target domains.

\textbf{The DA Spatial SSM} enhances spatial feature alignment between the source and target domains, particularly in earlier layers where spatial patterns such as textures and shapes are prominent. As shown in Figure \ref{fig:mambablock}, depthwise 2D convolutions (DWConv) are applied independently to the source and target feature maps, preserving detailed spatial cues. A cross-domain similarity measure is computed between the convolved feature maps to identify regions of spatial consistency. These feature maps are then reweighted based on the similarity, enabling the model to focus on spatially salient and coherent patterns shared between the domains. This mechanism improves feature fusion in intermediate stages, enhancing the model's ability to capture local structures critical for domain adaptation.

\textbf{The DA Channel SSM} focuses on aligning features in the channel dimension, particularly in deeper layers where abstract, high-level representations dominate. Figure \ref{fig:mambablock} illustrates the process, which begins by dividing the source and target feature maps into four equal channel segments. Two segments are selectively swapped between the source and target to form new representations, fostering cross-domain interactions. A channel-wise cosine similarity is then computed between the reassembled features, and the resulting similarity scores are used to reweight the channels in both domains. This process emphasizes the most correlated channel features across the source and target, effectively aligning cross-domain representations while preserving the diversity of channel-level information.

In addition to the selective scanning mechanisms, the attention mechanism in Mamba-Transformer blocks combines self-attention and cross-attention to facilitate both intra-domain and inter-domain feature interactions. Self-attention operates independently on the source and target features, capturing intra-domain relationships to preserve domain-specific information. Cross-attention, as illustrated in Figure~3, enables bidirectional feature exchange between the source and target domains. Specifically, source-dominant features are generated by using the source features as queries and the target features as keys and values, while target-dominant features are generated by using the target features as queries and the source features as keys and values.

This bidirectional exchange enables soft alignment, blending source and target feature distributions. The resulting aligned features blur domain boundaries and improve adaptation across source and target distributions.


\subsection{Entropy-Based Knowledge Distillation}
To minimize domain gaps and enhance cross-domain generalization, we introduce an entropy-based knowledge distillation (EKD) module that operates across multiple feature levels. By leveraging cross-attention, this module combines source-dominant (\(Z_{t \rightarrow s}\)) and target-dominant (\(Z_{s \rightarrow t}\)) features as augmented representations to facilitate smoother alignment between the source (\(Z_s\)) and target (\(Z_t\)) domains. These intermediate features act as a bridge, effectively reducing the domain discrepancy and enabling robust adaptation.

Entropy-based knowledge distillation quantifies the uncertainty in features extracted at three critical levels: shallow (stage 2), mid (stage 4), and deep (stage 6) outputs. This multi-scale strategy ensures comprehensive alignment across different feature granularities. To refine these representations, we introduce a channel-specific margin ReLU function, which suppresses irrelevant negative activations while preserving positive values. The margin parameter \(m\) is computed as the expectation of negative responses for each channel, allowing adaptive feature processing tailored to the training data.

The entropy loss is computed as follows: (1) applying margin ReLU to the intermediate features (\(Z_{t \rightarrow s}\) and \(Z_{s \rightarrow t}\)) from all three stages, (2) calculating Shannon entropy for student-teacher pairs at each level, and (3) averaging the entropy across all samples and layers. The entropy for each stage is defined as:

\begin{equation}
\begin{aligned}
H_{t \rightarrow ts} &= -\sum_{k} \sigma_m({Z_{t \rightarrow s}}^{(k)})\log(\sigma_m({Z_t}^{(k)})), \\
H_{t \rightarrow st} &= -\sum_{k} \sigma_m({Z_t}^{(k)})\log(\sigma_m({Z_{s \rightarrow t}}^{(k)})),
\end{aligned}
\label{eq:entropy}
\end{equation}


where \(\sigma_m\) denotes the margin ReLU function. The final entropy loss is computed as:

\begin{equation}
L_{\text{entropy}} = \frac{1}{N} \sum_{l=2,4,6} \left( H_{t \rightarrow ts}^{(l)} + H_{t \rightarrow st}^{(l)} \right),
\label{eq:kd}
\end{equation}

with \(N\) representing the batch size and \(l\) denoting the feature levels (shallow, mid, deep). By aligning student and teacher representations across these feature stages, the module reduces uncertainty and strengthens the model’s ability to generalize across domains.



\subsection{Entropy-Guided Random Multi-Layer Perturbation}
We propose an entropy-guided random multi-layer perturbation (ERMP) module to refine domain alignment by combining entropy-based gating attention with stochastic perturbation. This module enhances feature alignment across source-dominant (\(Z_{t \rightarrow s}\)), target-dominant (\(Z_{s \rightarrow t}\)), and target (\(Z_t\)) features while promoting generalization and robustness against overfitting.

The module begins by calculating entropy across feature levels to inform a gating attention mechanism. This mechanism adaptively balances the contributions of source-dominant and target-dominant features to the target feature space. A learnable parameter \(\gamma\), with sigmoid activation \(\sigma(\gamma)\), adjusts these contributions, resulting in the attention matrix:

\begin{equation}
\text{Attn}_{\text{gating}} = (1 - \sigma(\gamma)) \cdot H_{t \rightarrow st} + \sigma(\gamma) \cdot H_{t \rightarrow ts},
\end{equation}

where \(H_{t \rightarrow ts}\) and \(H_{t \rightarrow st}\) represent entropy-derived feature transformations. This dynamic mechanism prioritizes relevant features based on entropy, improving soft alignment and avoiding negative domain transfer.

Following gated attention, a stochastic perturbation process is applied to the target features to enhance robustness. At each iteration, one of three feature levels—stage 2 (shallow), stage 4 (mid), or stage 6 (deep)—is randomly selected for perturbation. The updated target feature is computed as:

\begin{equation}
\tilde{Z}_t = Z_t + \alpha \cdot (\text{Attn}_{\text{gating}} - Z_t),
\end{equation}

where \(\alpha\) controls the perturbation intensity. This random layer selection introduces a regularization effect, encouraging the model to generalize across multiple feature levels. By distributing perturbations across shallow, mid, and deep features, the model avoids over-reliance on specific feature depths, fostering resilience against domain shifts.

The synergy of entropy-guided attention and stochastic perturbation delivers adaptive and robust feature representations, enabling improved cross-domain generalization and alignment.

\subsection{Adversarial Learning for Bridging Cross-Domain Discrepancies}

To mitigate domain bias in unsupervised domain adaptation (UDA), we employ Adversarial Learning for Bridging Cross-Domain Discrepancies (ALBCD), which integrates adversarial-based local and global domain alignment using source-dominant and target-dominant features instead of conventional source and target features. This strategy prevents early discriminator convergence and overfitting, which often arise when using standard domain-separated features. By incorporating source-dominant and target-dominant representations derived from cross-attention, we introduce ambiguity in domain classification, effectively confusing the discriminators and enforcing more robust feature alignment.

For local alignment, we utilize the third-stage output features and apply a pixel-wise 2D convolutional discriminator to enforce domain adaptation at the fine-grained spatial level. The discriminator processes per-pixel activations, ensuring local feature consistency across domains. Cross-entropy loss is applied at the pixel level:

\begin{equation}
L_{\text{local}} = \sum_{i} \text{CE}(\hat{y}_{\text{local}}^i,y_{local}^i),
\end{equation}

Here, \( \hat{y}_{\text{local}}^i \) represents the predicted domain label for pixel \( i \), and \( y_{local}^i \) is the corresponding ground truth.

For global alignment, we utilize the sixth-stage output features and introduce a feature-level discriminator that aligns high-level semantic features. To mitigate domain gaps and emphasize hard-to-align samples, we employ focal loss:

\begin{equation}
L_{\text{global}} = \text{FL}(\hat{y}_{\text{global}}, y_{global}),
\end{equation}

Here, \( \hat{y}_{\text{global}} \) represents the predicted domain label, and \( y_{local} \) is the corresponding ground truth.

A Gradient Reversal Layer (GRL) \cite{ganin2015unsupervised} is inserted between the feature extractor and the discriminators, reversing gradients during backpropagation to further confuse the discriminators and enforce domain-invariant learning. The overall adversarial objective is formulated as:

\begin{equation}
L_{\text{adv}} = \max \min (L_{\text{local}} + L_{\text{global}}),
\label{eq:adv}
\end{equation}

By leveraging source-dominant and target-dominant features along with multi-level adversarial training, the proposed framework effectively aligns domain distributions, reducing domain bias and preventing overfitting in adversarial learning.

\begin{table*}[thb]
\centering
\caption{Results of adapting PASCAL VOC to Clipart1k (\%). mAP is reported on Clipart1k. ”-S” and
            ”-B” indicates that the model is Small and Base, respectively.}
\label{table1_clipart}
\renewcommand{\arraystretch}{1.1} % Increase row spacing
\setlength\tabcolsep{3.5pt} % Adjust column spacing for readability
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccccccccccccccccccc}
\toprule
\textbf{Methods} & \textbf{Aero} & \textbf{Bicycle} & \textbf{Bird} & \textbf{Boat} & \textbf{Bottle} & \textbf{Bus} & \textbf{Car} & \textbf{Cat} & \textbf{Chair} & \textbf{Cow} & \textbf{Table} & \textbf{Dog} & \textbf{Horse} & \textbf{Bike} & \textbf{Person} & \textbf{Plant} & \textbf{Sheep} & \textbf{Sofa} & \textbf{Train} & \textbf{TV} & \textbf{mAP} \\
\midrule
DANN~\cite{ganin2016domain} & 24.1 & 52.6 & 27.5 & 18.5 & 20.3 & 59.3 & 37.4 & 3.8 & 35.1 & 32.6 & 23.9 & 13.8 & 22.5 & 50.9 & 49.9 & 36.3 & 11.6 & 31.3 & 48.0 & 35.8 & 31.8 \\
DT+PL~\cite{inoue2018cross} & 16.8 & 53.7 & 19.7 & 31.9 & 21.3 & 39.3 & 39.8 & 2.2 & 42.7 & 46.3 & 24.5 & 13.0 & 42.8 & 50.4 & 53.3 & 38.5 & 14.9 & 25.1 & 41.5 & 37.3 & 32.7 \\
WST~\cite{kim2019self} & 30.8 & 65.5 & 18.7 & 23.0 & 24.9 & 57.5 & 40.2 & 10.9 & 38.0 & 25.9 & 36.0 & 15.6 & 22.6 & 66.8 & 52.1 & 35.3 & 1.0 & 34.6 & 38.1 & 39.4 & 33.8 \\
BSR~\cite{kim2019self} & 26.3 & 56.8 & 21.9 & 20.0 & 24.7 & 55.3 & 42.9 & 11.4 & 40.5 & 30.5 & 25.7 & 17.3 & 23.2 & 66.9 & 50.9 & 35.2 & 11.0 & 33.2 & 47.1 & 38.7 & 34.0 \\
SWDA$^\dagger$~\cite{saito2019strong} & 29.0 & 60.7 & 25.0 & 20.4 & 24.6 & 55.4 & 36.1 & 13.1 & 41.2 & 38.3 & 30.3 & 17.0 & 21.2 & 55.2 & 50.4 & 36.6 & 10.6 & 38.4 & 49.2 & 41.2 & 34.7 \\
BSR+WST~\cite{kim2019self} & 28.0 & 64.5 & 23.9 & 19.0 & 21.9 & 64.3 & 43.5 & 16.4 & 42.2 & 25.9 & 30.5 & 7.9 & 25.5 & 67.6 & 54.5 & 36.4 & 10.3 & 31.2 & 57.4 & 43.5 & 35.7 \\
HTCN$^\dagger$~\cite{chen2020harmonizing} & 28.7 & 67.7 & 25.3 & 16.1 & 28.7 & 56.0 & 38.9 & 12.5 & 41.0 & 33.0 & 29.6 & 12.9 & 22.9 & 69.0 & 55.9 & 36.1 & 11.8 & 34.1 & 48.8 & 46.8 & 35.8 \\
I$^3$Net~\cite{chen2021i3net} & 30.0 & 67.0 & 32.5 & 21.8 & 29.2 & 62.5 & 41.3 & 11.6 & 37.1 & 39.4 & 27.4 & 19.3 & 25.0 & 67.4 & 55.2 & 42.9 & 19.5 & 36.2 & 50.7 & 39.3 & 37.8 \\
Source Only~\cite{liu2016ssd} & 27.3 & 60.4 & 17.5 & 16.0 & 14.5 & 43.7 & 32.0 & 10.2 & 38.6 & 15.3 & 24.5 & 16.0 & 18.4 & 49.5 & 30.7 & 30.0 & 2.3 & 23.0 & 35.1 & 29.9 & 26.7 \\
\rowcolor{lightgray!45} \textbf{DAVim$\-$Net-S (Ours)} & 31.1 & 70.6 & 29.5 & 23.7 & 26.6 & 64.3 & 46.5 & 16.8 & 37.8 & 42.3 & 30.1 & 23.4 & 29.5 & 70.9 & 64.4 & 35.1 & 23.7 & 38.2 & 52.1 & 44.7 & 40.7 \\
\rowcolor{lightgray!45} \textbf{DAVim$\-$Net-B (Ours)} & \textbf{33.2} & \textbf{75.5} & \textbf{33.1} & \textbf{25.5} & \textbf{27.9} & \textbf{69.9} & \textbf{50.1} & \textbf{16.9} & \textbf{40.8} & \textbf{47.0} & \textbf{32.6} & \textbf{24.1} & \textbf{32.5} & \textbf{77.0} & \textbf{69.5} & \textbf{37.6} & \textbf{23.3} & \textbf{41.2} & \textbf{57.0} & \textbf{48.2} & \textbf{43.8} \\
\bottomrule
\end{tabular}}
\end{table*}



\section{Experiments}
\subsection{Datasets}
\textbf{PASCAL VOC:} The PASCAL VOC dataset \cite{everingham2010pascal} encompasses 20 categories of common real-world objects, complete with bounding box and class annotations. In accordance with the approaches outlined in \cite{saito2019strong, shen2019scl}, this dataset combines images from both the PASCAL VOC 2007 and 2012 editions, resulting in a comprehensive collection of 16,551 images. This extensive dataset serves as a valuable resource for training and evaluating object detection and segmentation models in various computer vision tasks.

\textbf{Clipart1k \& Watercolor2k:} The Clipart1k dataset \cite{inoue2018cross} consists of 1,000 clipart images and aligns with the same 20 categories as the PASCAL VOC dataset. However, it presents a significant domain shift from PASCAL VOC. In line with the methodology outlined in \cite{saito2019strong, shen2019scl}, Clipart1k is divided into a training set and a testing set, each containing 500 images. The Watercolor2k dataset \cite{inoue2018cross}, on the other hand, includes watercolor-style images that cover 6 categories common to the PASCAL VOC dataset. It also follows the same splitting strategy as Clipart1k, with a total of 1,000 images divided equally into training and testing sets.

\textbf{Cityscapes:} The Cityscapes dataset \cite{cordts2016cityscapes} is created by capturing images of outdoor street scenes under normal weather conditions across 50 different cities, showcasing a wide variety of urban environments. It consists of 2,975 training images and 500 validation images, all annotated with dense pixel-level labels. The bounding box annotations are derived from the original instance segmentation labels.

\textbf{Foggy Cityscapes:} The Foggy Cityscapes dataset \cite{sakaridis2018semantic} is generated from the original Cityscapes images, maintaining the same training and testing split. This dataset simulates foggy weather conditions by utilizing the depth information provided in the Cityscapes dataset, resulting in three distinct levels of fog intensity.

\subsection{Implementation Details}
For all domain adaptation (DA) tasks, we utilize pretrained weights on the ImageNet dataset \cite{deng2009imagenet} as the backbone network in our proposed DAVimNet method, integrating 16 dual hybrid Domain Adaptive Mamba-Transformer stages within the DAVimNet framework. The model is implemented in two versions: DAVimNet-S, designed for speed, and DAVimNet-B, optimized for accuracy. Optimization is performed using the Stochastic Gradient Descent (SGD) algorithm \cite{bottou2010large}, with a momentum of 0.9 and a weight decay parameter of $1 \times 10^{-3}$. We employ a base learning rate of $5 \times 10^{-3}$ for the Pascal VOC, WaterColor, and Clipart datasets, while a lower learning rate of $1 \times 10^{-3}$ is applied for the Cityscapes and FoggyCityscapes dataset. The learning rate follows a warmup cosine scheduler. Across all datasets, the batch size is consistently set to 32, and the model is trained for 100 epochs. The hyperparameters $\lambda_{\text{adv}}$ and $\lambda_{\text{kd}}$ in the DAVimNet method are set to $0.5$ and $0.1$, respectively, for all DA tasks, as shown in Equation ~\ref{eq:total}.

\subsection{Objective Function}
The total loss function in our domain adaptive object detection model is defined as:

\begin{equation}
L_{\text{total}} = L_{\text{det}} + \lambda_{\text{adv}} L_{\text{adv}} + \lambda_{\text{kd}} L_{\text{kd}},
\label{eq:total}
\end{equation}

where \( L_{\text{det}} \) is the detection loss, as described in Equation \ref{eq:det}, consisting of both classification and localization components. The adversarial loss \( L_{\text{adv}} \), detailed in Equation \ref{eq:adv}, promotes domain adaptation by aligning feature distributions across the source and target domains. The knowledge distillation loss \( L_{\text{kd}} \), described in Equation \ref{eq:kd}, facilitates soft feature alignment between the source-dominant and target-dominant branches. The weighting coefficients \( \lambda_{\text{adv}} \) and \( \lambda_{\text{kd}} \) are hyperparameters that balance the impact of these auxiliary losses and are specified in the implementation details section.

\subsection{Model Complexity}

Table \ref{tab:method-comparison} highlights the differences in computational complexity between training and testing for DAVimNet-S and DAVimNet-B. The training phase has significantly higher complexity due to the inclusion of domain-adaptive modules and cross-feature flow mechanisms, which facilitate effective domain alignment. These components introduce additional computations, increasing both parameter count and FLOPs. However, during inference, these adaptive modules are not required, leading to a more efficient and streamlined model with reduced complexity.


\begin{table}[htbp]
    \centering
     \caption{Comparison of methods with parameters, FLOPS and inference time. ”-S” and
            ”-B” indicates that the model is Small and Base, respectively.}
    \scalebox{0.90}{
    \begin{tabular}{l|ccc}
        \toprule
        Method & Params (M) & FLOPs (G) & Inference Time (S) \\
        \midrule
        DAVimNet-S (train)  &      125.7        &         12.8 & 0.29        \\
        DAVimNet-S  (test) &        62.3      &          8.3   & 0.030     \\
        DAVimNet-B (train)  &      216.2     &         21.5 & 0.56          \\
        DAVimNet-B  (test) &       105.7      &         11.8  & 0.064         \\
        \bottomrule
    \end{tabular}
    }
   
    \label{tab:method-comparison}
\end{table}


\subsection{Analysis of Entropy}

We analyze entropy at three key stages of feature extraction: stage 2 (shallow features), stage 4 (mid-level features), and stage 6 (deep features). As shown in Figure \ref{fig:entropy}, entropy increases progressively from shallow to deep layers, with the lowest entropy in the shallow stage and the highest entropy in the deep stage, indicating a shift from structured feature representations to more abstract ones. Notably, mid-level features exhibit higher noise, suggesting greater variability during the transition from local to global feature encoding. This entropy behavior is crucial for Entropy-Based Knowledge Distillation, where entropy-guided reweighting aids in effective feature alignment, as discussed in the corresponding section.

\begin{figure}[ht] 
    \centering
    \includegraphics[width=1\linewidth]{entropy.pdf}
    \caption{Entropy values. stage 2 entropy (top left), stage 4 entropy (top right), and stage 6 entropy (bottom left)}
    \label{fig:entropy}
\end{figure}


\subsection{Analysis of Source and Target Dominant Feature Effect}

To assess the impact of Source-Target Dominant (STD) features, we conduct an ablation study under the PASCAL VOC to Clipart1k adaptation setting using our DAVimNet-B model. As shown in Table \ref{table:std}, replacing Source-Target (ST) features with STD features results in a notable improvement, increasing mAP from 41.2\% to 43.8\%. This performance boost highlights the limitations of ST features, which tend to retain rigid, non-transferable domain-specific characteristics, leading to suboptimal adaptation. In contrast, STD features effectively suppress domain biases, facilitating a softer, more adaptable feature representation. 

\begin{table}[htbp]
 \caption{Analysis of ST and STD Feature Effects. ST represents Source-Target features, while STD represents Source-Target Dominant features.}

\centering
\scalebox{1.3}{
\begin{tabular}{ccc|c}
\hline
EKD          & ERMP & ALBCD    & mAP        \\ 
\hline 
      ST &  ST  &  ST &  41.2         \\ 
      STD & STD & STD  &  \textbf{43.8}  \\
    
\hline
\end{tabular}
}
\label{table:std}
\end{table}




\begin{table*}[htbp]
\caption{Quantitative results (mAP) for Cityscapes to  FoggyCityscapes. ”-S” and
            ”-B” indicates that the model is Small and Base, respectively.}

\label{tab:foggy}
\centering
\resizebox{0.85\linewidth}{!}{
\begin{tabular}{lccccccccc}
\hline

Method  & prsn          & rider         & car           & truck         & bus           & train         & mcycle        & bicycle       & mAP           \\ \hline


DA Faster \cite{Chen2018DomainAF}  & 25.0          & 31.0          & 40.5          & 22.1 & 35.3          & 20.2          & 20.0          & 27.1          & 27.6          \\ 
Selective DA \cite{zhu2019adapting} &   33.5& 38.0& 48.5& 26.5& 39.0& 23.3& 28.0& 33.6& 33.8          \\
D\&Match \cite{kim2019diversify} &  30.8& 40.5& 44.3& 27.2& 38.4& 34.5& 28.4& 32.2& 34.6\\
MAF \cite{he2019multi}   & 28.2& 39.5& 43.9& 23.8& 39.9& 33.3& 29.2& 33.9& 34.0\\
Robust DA \cite{khodabandeh2019robust}  & 35.1& 42.1& 49.1& 30.0& 45.2& 26.9& 26.8& 36.0& 36.4\\
MTOR \cite{cai2019exploring} & 30.6& 41.4& 44.0& 21.9& 38.6& 40.6& 28.3& 35.6& 35.1\\
Strong-Weak \cite{saito2019strong} & 29.9& 42.3& 43.5& 24.5& 36.2& 32.6& 30.0& 35.3& 34.3\\ 
Categorical DA \cite{xu2020exploring} & 32.9& 43.8& 49.2& 27.2& 45.1& 36.4& 30.3& 34.6& 37.4\\
Unbiased DA \cite{deng2021unbiased} & 33.8& 47.3& 49.8& 30.0& 48.2& 42.1& 33.0& 37.3& 40.4\\
    
SFOD \cite{li2020free}  & 25.5& 44.5& 40.7& 33.2& 22.2& 28.4& 34.1& 39.0& 33.5\\ 
HCL \cite{huang2021model} & 26.9& 46.0& 41.3& 33.0& 25.0& 28.1& 35.9& 40.7& 34.6\\ 
Mean-Teacher \cite{tarvainen2017mean} & 33.9  & 43.0& 45.0& 29.2& 37.2& 25.1& 25.6& 38.2 & 34.3  \\
MemCLR \cite{vs2023towards}   & 37.7& 42.8& 52.4& 24.5& 40.6& 31.7& 29.4& 42.2& 37.7 \\ 

Tent \cite{wang2020tent}  & 31.2& 38.6& 37.1& 20.2& 23.4& 10.1& 21.7& 33.4& 26.8 \\ 
MemCLR \cite{vs2023towards}  & 32.1& 41.4& 43.5& 21.4& 33.1& 11.5& 25.5& 32.9& 29.8  \\
       
Source-Only  & 26.9  & 38.2  & 35.6 & 18.3  & 32.4 & 9.6 & 25.8  & 28.6 & 26.9  \\
\rowcolor{lightgray!45}DAVim$\-$Net-S (Ours)  & 32.4  & 45.1  & 41.8 & 22.7  & 37.9 & 12.3 & 29.6  & 33.2 & 39.1  
  \\
\rowcolor{lightgray!45}DAVim$\-$Net-B (Ours)  & 34.2  & 47.5  & 44.0 & 24.5  & 39.8 & 13.7 & 31.4  & 35.1 & \textbf{42.3}  \\ 
\hline


\end{tabular}}
\vskip-15.0pt
\end{table*}


\subsection{Ablation Study}
Table \ref{table:abb} highlights the contribution of each component in DAVimNet-B for PASCAL VOC to Clipart1k adaptation. The Hybrid Domain-Adaptive Mamba-Transformer (HDAMT) plays a crucial role, achieving the highest individual performance improvement, demonstrating its effectiveness in learning transferable representations. Additional modules, including Entropy-Based Knowledge Distillation (EKD), Entropy-Guided Random Multi-Layer Perturbation (ERMP), and Adversarial Learning for Bridging Cross-Domain Discrepancies (ALBCD), further refine feature alignment, leading to incremental gains.

\begin{table}[htbp]
\caption{Ablation study on the proposed DAVimNet-B.}
\centering
\scalebox{1.3}{
\begin{tabular}{cccc|c}
\hline
HDAMT          & EKD          & ERMP & ALBCD    & mAP        \\ 
\hline 
             &  &   &   &        26.7   \\ 
    \checkmark   &    &   &    &   37.1   \\ 
    & \checkmark    &  &    &   31.4  \\ 
      &      &  \checkmark   &    & 29.1   \\
      &      &     &  \checkmark   & 30.4   \\
      \checkmark & \checkmark     &     &    & 40.1   \\
      \checkmark & \checkmark     &  \checkmark   &    & 41.9   \\
      \checkmark & \checkmark     &  \checkmark   &  \checkmark & \textbf{43.8}   \\
\hline
\end{tabular}
}
\label{table:abb}
\end{table}



\subsection{Main Results}

\textbf{Pascal VOC $\rightarrow$ Clipart1k.} The performance comparison for domain adaptation from PASCAL VOC to Clipart1k demonstrates the superiority of DAVimNet over existing methods. Our DAVimNet-B (Ours) achieves an mAP of 43.8\%, outperforming previous state-of-the-art methods such as I³Net (37.8\%) and HTCN (35.8\%) by significant margins. Notably, DAVimNet-S (Ours) also surpasses many competitive baselines, achieving 40.7\% mAP, showcasing the effectiveness of our approach. The improvements are particularly evident in categories such as bicycle (+8.5\%), bus (+7.4\%), and person (+14.0\%) compared to Source Only, highlighting the robustness of our model in capturing domain-invariant features.

\textbf{Pascal VOC $\rightarrow$ Watercolo2k.} The adaptation results from Pascal VOC to Watercolor2k further validate the effectiveness of DAVimNet, achieving a new state-of-the-art mAP of 54.8\% with DAVimNet-B (Ours), surpassing the previous best I³Net (51.5\%) by 3.3\%. Our DAVimNet-S (Ours) also demonstrates strong performance with 52.8\% mAP, consistently outperforming competitive baselines. The improvements are particularly notable in bicycle (+6.1\%), bird (+4.3\%), and person (+4.4\%) compared to I³Net, showcasing the robustness of our model across diverse object categories. 


\textbf{Cityscapes $\rightarrow$ FoggyCityscapes.} The results for domain adaptation from Cityscapes to FoggyCityscapes further demonstrate the superiority of DAVimNet in adverse weather conditions. Our DAVimNet-B (Ours) achieves an mAP of 42.3\%, outperforming prior UDA and SFDA methods, including Unbiased DA (40.4\%) and MemCLR (37.7\%), setting a new state-of-the-art. The DAVimNet-S (Ours) variant also achieves a strong 39.1\% mAP, surpassing many established baselines. Notably, our method achieves significant improvements in motorcycle (+2.0\%), rider (+4.2\%), and truck (+1.3\%) over Unbiased DA, showcasing its ability to enhance detection across diverse object categories under foggy conditions.



\begin{table}[htbp]
    \centering
    \caption{Results on adaptation from PASCAL VOC to Watercolor2k (\%). mAP is reported on the Watercolor2k test set. ”-S” and
            ”-B” indicates that the model is Small and Base, respectively.}
    \label{table_watercolor}
    \small
    \setlength\tabcolsep{4.5pt} % Adjust column spacing
    \renewcommand{\arraystretch}{1.1} % Increase row spacing
    \resizebox{1\linewidth}{!}{%
    \begin{tabular}{lccccccc}
        \toprule
        \textbf{Methods} & \textbf{Bike} & \textbf{Bird} & \textbf{Car} & \textbf{Cat} & \textbf{Dog} & \textbf{Person} & \textbf{mAP} \\
        \midrule
        DANN~\cite{ganin2016domain} & 73.4 & 41.0 & 32.4 & 28.6 & 22.1 & 51.4 & 41.5 \\
        BSR~\cite{kim2019self} & 82.8 & 43.2 & 49.8 & 29.6 & 27.6 & 58.4 & 48.6 \\
        WST~\cite{kim2019self} & 77.8 & 48.0 & 45.2 & 30.4 & 29.5 & 64.2 & 49.2 \\
        SWDA$^\dagger$~\cite{saito2019strong} & 73.9 & 48.6 & 44.3 & 36.2 & 31.7 & 62.1 & 49.5 \\
        BSR+WST~\cite{kim2019self} & 75.6 & 45.8 & 49.3 & 34.1 & 30.3 & 64.1 & 49.9 \\
        HTCN$^\dagger$~\cite{chen2020harmonizing} & 78.6 & 47.5 & 45.6 & 35.4 & 31.0 & 62.2 & 50.1 \\
        I$^3$Net~\cite{chen2021i3net} & 81.1 & 49.3 & 46.2 & 35.0 & 31.9 & 65.7 & 51.5 \\
        Source Only~\cite{liu2016ssd} & 77.5 & 46.1 & 44.6 & 30.0 & 26.0 & 58.6 & 47.1 \\
        \rowcolor{lightgray!45} DAVim$\-$Net-S (Ours) & 83.9 & 51.7 & 50.0 & 33.6 & 29.1 & 68.5 & 52.8 \\
        \rowcolor{lightgray!45} DAVim$\-$Net-B (Ours) & \textbf{87.2} & \textbf{53.6} & \textbf{51.9} & \textbf{34.9} & \textbf{30.3} & \textbf{70.1} & \textbf{54.8} \\
        \bottomrule
    \end{tabular}}
\end{table}



\section{Conclusion and Future Work}
In this work, we proposed a domain-adaptive object detection framework based on the Single Shot Detector (SSD), integrating Domain-Adaptive Mamba (DAMamba) blocks with self- and cross-attention to mitigate domain shift in unsupervised domain adaptation (UDA). By leveraging domain adaptive spatial and channel state-space models (SSMs), our approach enhances feature refinement and enables soft alignment of source- and target-dominant representations. Additionally, entropy-driven knowledge distillation and dual-level adversarial alignment enforce domain invariance. Experiments on benchmark UDA datasets demonstrate state-of-the-art performance in cross-domain object detection. However, the computational cost of attention mechanisms remains a challenge. Future work will focus on efficient attention approximations and lightweight adaptation strategies to improve scalability.


\bibliographystyle{ieeetr}
\bibliography{davimnet}


\end{document}
