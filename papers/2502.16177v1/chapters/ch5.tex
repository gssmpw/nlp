\section{Evaluation}
\subsection{Metrics}
The metrics we used are:
\begin{itemize}
    \item FAR, False Acceptance Rate, is the percentage of operations where an impostor claims an identity and a false acceptance occurs.
    \item FRR, False Rejection Rate, is the percentage of operations where a genuine claim is falsely rejected.
    \item EER, Equal Error Rate, which is the error rate where FAR = FRR.
    \item ROC, Receiver Operating Curve, which shows the probability of Genuine Accept (1 - FRR) against the False Accept Rate variation.
\end{itemize}

Due to the fact that keystroke dynamics is behavioral, it is less robust. But it can be useful in an intrusion detection system. That is why we care more about FAR than FRR. The evaluation for each method is done for:

\begin{itemize}
    \item Aalto dataset;
    \item Buffalo free-text dataset;
    \item Buffalo fixed-text dataset;
    \item Nanglae-Bhattarakosol dataset.
\end{itemize}

\subsection{Mahalanobis}
\label{sec:mahalanobis}
To implement this method we used this pre-existing implementation \cite{ref:maha-code} as a template.\\

As you can see from the Figure 2, 3, 4 and 5, Mahalanobis generally has modest authentication performance. On all datasets it has an EER of around 0.35 and the FAR and FRR plots show that it is very sensitive to changes in the threshold, with a steep drop-off as the threshold increases.

\newpage
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/mah-aalto-roc.png}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth} % Increased width to make images larger
        \centering
        \includegraphics[width=1.2\linewidth]{images/mah-aalto-far.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth} % Increased width
        \centering
        \includegraphics[width=1.2\linewidth]{images/mah-aalto-frr.png}
    \end{minipage}
    \caption{\textbf{Aalto } AUC = 0.914, EER = 0.132}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/mah-freebuffalo-roc.png}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth} % Increased width to make images larger
        \centering
        \includegraphics[width=1.2\linewidth]{images/mah-freebuffalo-far.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth} % Increased width
        \centering
        \includegraphics[width=1.2\linewidth]{images/mah-freebuffalo-frr.png}
    \end{minipage}
    \caption{\textbf{Buffalo Free-Text } AUC = 0.867, EER = 0.214}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/mah-fixbuffalo-roc.png}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth} % Increased width to make images larger
        \centering
        \includegraphics[width=1.2\linewidth]{images/mah-fixbuffalo-far.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth} % Increased width
        \centering
        \includegraphics[width=1.2\linewidth]{images/mah-fixbuffalo-frr.png}
    \end{minipage}
    \caption{\textbf{Buffalo Fixed-Text } AUC = 0.863, EER = 0.213}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/mah-nanglae-roc.png}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth} % Increased width to make images larger
        \centering
        \includegraphics[width=1.2\linewidth]{images/mah-nanglae-far.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth} % Increased width
        \centering
        \includegraphics[width=1.2\linewidth]{images/mah-nanglae-frr.png}
    \end{minipage}
    \caption{\textbf{Nanglae-Bhattarakosol } AUC = 0.958, EER = 0.102}
\end{figure}
\FloatBarrier

\subsection{Gaussian Mixture Model}
Our implementation of GMM was directly inspired by an internet repository\cite{ref:gmm-code}.\\

The plots in Figure 6,7,8 and 9 make it clear that this model is the best out of the three we tried. It has excellent authentication performance, with an AUC of around 0.9 and an EER of around 0.15 on all datasets. Moreover the FAR and FRR plots show that GMM has a much more controlled behavior, as  when the threshold increases, the FAR decreases and the FRR increases. The drop-off as the threshold increases is more gradual. You can also see that performance is somewhat different on Aalto, but that is expected since it contains much more data than the other datasets.
\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/gmm-aalto-roc.png}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth} % Increased width to make images larger
        \centering
        \includegraphics[width=1.2\linewidth]{images/gmm-aalto-far.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth} % Increased width
        \centering
        \includegraphics[width=1.2\linewidth]{images/gmm-aalto-frr.png}
    \end{minipage}
    \caption{\textbf{Aalto } AUC = 0.904, EER = 0.156}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/gmm-freebuffalo-roc.png}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth} % Increased width to make images larger
        \centering
        \includegraphics[width=1.2\linewidth]{images/gmm-freebuffalo-far.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth} % Increased width
        \centering
        \includegraphics[width=1.2\linewidth]{images/gmm-freebuffalo-frr.png}
    \end{minipage}
    \caption{\textbf{Buffalo Free-Text } AUC = 0.895, EER = 0.187}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/gmm-fixbuffalo-roc.png}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth} % Increased width to make images larger
        \centering
        \includegraphics[width=1.2\linewidth]{images/gmm-fixbuffalo-far.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth} % Increased width
        \centering
        \includegraphics[width=1.2\linewidth]{images/gmm-fixbuffalo-frr.png}
    \end{minipage}
    \caption{\textbf{Buffalo Fixed-Text } AUC = 0.890, EER = 0.186}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{images/gmm-nanglae-roc.png}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth} % Increased width to make images larger
        \centering
        \includegraphics[width=1.2\linewidth]{images/gmm-nanglae-far.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth} % Increased width
        \centering
        \includegraphics[width=1.2\linewidth]{images/gmm-nanglae-frr.png}
    \end{minipage}
    \caption{\textbf{Nanglae-Bhattarakosol } AUC = 0.941, EER = 0.125}
\end{figure}
\FloatBarrier
\subsection{Gunetti Picardi}
We implemented the Gunetti-Picardi algorithm by looking at this implementation \cite{ref:gp-code}. We used the open set all-probe-against-all-gallery approach.\\

As we explained before, Gunetti Picardi is used primarily with free-text datasets, so we only used it on two sessions of the free-text section of Buffalo dataset. There are many possible combinations of A and R measures; however we choose to focus on these ones:
\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c} % Adjusted column count
        \hline
         & $A_2$ & $A_3$ & $A_{23}$ & $R_2$ & $R_{23}$ & $R_2A_2$ & $R_2A_{234}$ & $R_{23}A_{23}$ & $R_{234}A_{23}$ \\
        \hline
        FAR & 13\% & 13\% & 13\% & 12\% & 10\% & 10\% & 10\% & 12\% & 8\% \\ % Fill missing values
        FRR & 13\% & 14\% & 13\% & 12\% & 11\% & 9\% & 9\% & 11\% & 8\% \\ % Fill missing values
        \hline
    \end{tabular}
    \caption{\textbf{Buffalo Free-Text}}
\end{table}
\FloatBarrier

\subsection{Conclusions}
As you can see, the AUC for all methods is similar, but Gaussian Mixture Model is by far the best method when it comes to EER. Across all datasets it always has the lowest EER. Curiously, you can also see that even with two different methods the FAR and FRR plots for the same datasets follow the same rough trajectory, this happens because while the methods may be different, data is still the most important element. Performance is always better when testing on the Nanglae-Bhattarakosol dataset. This is certainly caused by its small size, since when the same methods are used on the other datasets the performance is always lower. There is also a clear gap in performance when the same method is used on Aalto and when it is used on Buffalo. This might be explained by the fact that each Buffalo user has a lot more keystrokes samples compared to an Aalto user. This means that there are more variations per user.\\

You can verify our results independently by accessing our provided code \cite{ref:our-repo}.

\newpage