\section{Feature extraction}

First of all the raw files contained in the datasets need to be processed. This step was entirely developed by us.\\

For Aalto 136M, a raw file in the Aalto dataset contains:
\begin{itemize}
    \item PARTICIPANT\textunderscore ID
    \item TEST\textunderscore SECTION\textunderscore ID
    \item SENTENCE
    \item USER\textunderscore INPUT
    \item KEYSTROKE\textunderscore ID
    \item PRESS\textunderscore TIME
    \item RELEASE\textunderscore TIME
    \item LETTER
    \item KEYCODE
\end{itemize}
We compute Hold Time (H), which measures how long a key is pressed, Up-Down Time (UD), which represents the time between releasing one key and pressing the next, and Down-Down Time (DD), which measures the time between pressing two consecutive keys. Those values are then filtered to remove negative or invalid entries. The function then assigns a subject column based on the user ID and determines the key column using either the LETTER or KEYCODE values. The final dataset consists of selected columns \lstinline|subject, key, H, UD, DD| with missing values removed. The data is merged and exported as a \textit{.csv} file.\\

In the case of the Buffalo dataset, in the raw files each line corresponds to an event, for example:
\begin{verbatim}
    A KeyDown 63578429792961
    A KeyUp 63578429793054
    M KeyDown 63578429793257
    M KeyUp 63578429793382
\end{verbatim}
The first element is the name of the key. The second element is the key event (key down or key up). The third element is the time stamp in milliseconds. From the raw files we calculate Hold time(H), Down-Down time(DD) and Up-Down time(UD) and we save it to a \textit{.csv} file. To do this, we use helper functions stored in \lstinline|utils.py| to traverse all dataset files, save the metadata (like user ID, session, task, ...) and then we read the raw keystroke logs to extract the aforementioned keystroke features while also tracking the number of keystroke repetitions per user. The dataset is divided in two parts, free-text (\lstinline|task==1|) and fixed-text (\lstinline|task==0|). The end result is a \textit{.csv} file containing \lstinline|subject,key,H,UD,DD|.\\

In the case of the Nanglae-Bhattarakosol dataset, the features are already extracted. We made some changes, mainly concerning timings and file structure. We converted the timings from milliseconds to seconds and we merged the three separate \textit{.xlsx} files into a single \textit{.xlsx} file and then we converted it to a \textit{.csv} file.\\

After processing the data from one of the three datasets, we use either: Mahalanobis, Gaussian Mixture Model or Gunetti-Picardi.\\

\lstinline|MahalanobisDetector|. It takes a list of user IDs and a pandas dataframe that contains keystroke timing data. Then mean feature vector for a user's training data is computed; this represents a user's typical behavior. Afterwards, the performance is evaluated using Hold (H), Up-Down (UD) and Down-Down (DD) times. This is done by extracting genuine user keystroke data from the subject and impostor data from other users. Then the genuine data is split 70/30, where the first 70\% is used for training and the remaining 30\% for testing. The first 5 samples from other users are used for impostor testing. To do complete testing, we compute the pseudo-inverse of the covariance matrix between keystroke features. Then we use it to compute the Mahalanobis distance between genuine values and the mean vector. We repeat the same process for impostor values. Finally, the results are used to plot the ROC curve.\\

\lstinline|GMMKeystrokeModel|. The extracted features are then split in: training set (70\%), validation set (30\%). Then for each user and each digraph a Gaussian Mixture Model is fit on the hold time; the means, covariances and weights of the Gaussian distributions are stored. Then the scores are calculated for genuine and impostor values. Performance metrics, like FAR and FRR are calculated while sweeping through each similarity threshold (from 0.0 to 1.0).\\

\lstinline|GunettiPicardi|. After extracting the features, we create a user profile using the extracted features like digraphs, trigraphs, fourgraphs, average keystroke durations and relative timing relationships. This profile is stored in a pickle (\textit{.pkl}) file. Afterwards the system compares an input keystroke sequence against those stored user profiles using Gunetti Picardi distance metrics \ref{sec:gp}: absolute distance (A), which keeps track of timing; Relative distance (R) which measures the difference between samples; a combination of A and R. The closest matching user profile is found and users are classified as either genuine or impostors. Authentication is performed only whether the classified user passes a threshold comparison. To minimize false rejections a secondary check is performed.