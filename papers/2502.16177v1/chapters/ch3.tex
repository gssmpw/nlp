\section{Authentication Algorithms}
There are several algorithms for keystroke recognition but they can be divided into broad categories: statistical, machine learning based and deep learning based.\\

Statistical algorithms are the easiest to implement as they are deterministic and require low computing power. However, they necessitate manual feature extraction. We chose this type of algorithm for our project for these reasons, as well as due to our limited experience with machine learning and deep learning. The following section will provide a detailed description of the algorithms we used.

\subsection{Mahalanobis Distance}
Mahalanobis distance takes into consideration the covariance of data variables, this makes it better suited for real-world data \cite{ref:mahalanobis}.\\

Euclidean and Manhattan distances have been used for their simplicity. However they are very sensitive to scale variations in the variables and they have no way to deal with correlation between feature variables like Mahalanobis can. The squared Mahalanobis distance between two vectors x and y is defined as:
\begin{equation}
    ||x-y||^2=(x-y)^TS^{-1}(x-y)
\end{equation}
where S is the data's covariance matrix.\\

As you can see, it weights the distance calculation according to the statistical variation of each component and it also decouples interactions between features based on their covariance.

\subsection{Gunetti Picardi}
\label{sec:gp}
This algorithm \cite{ref:gunetti picardi} works best with free text. It evaluates the timing characteristics between kinds of keystrokes:
\begin{itemize}
    \item digraphs, two consecutive keystrokes
    \item trigraphs, three consecutive keystrokes
    \item n-graphs, n consecutive keystrokes
\end{itemize}
Two primary measures are used, which return a value between 0 and 1; they are called R and A.\\

The R (relative) measure, measures the degree of disorder between two samples. To compute the R measure of an array S, you calculate the sum of distances between the position of each element in S and the position of the same element in array S' (its ordered equivalent). The result is a value that falls between 0 and 1 where 0 means no disorder while 1 means maximum disorder. $R_n$ measure the degree of disorder of two samples that shares some n-graphs. $R_{n,m}$ measure the degree of disorder of two samples that shares some n-graphs and some m-graphs. This can be extended to a larger number of different graphs.\\

However the R measure does not consider absolute typing speeds at all, meaning that we need another measure, A.\\

The A (absolute) measure, considers the absolute value of the typing speed of each pair of identical n-graphs in the two samples involved. The A distance between two samples S1 and S2 is:
\begin{equation}
    A_{t,n} = 1- \frac{\text{number of similar n-graphs between S1 and S2}}{\text{total number of n-graphs shared by S1 and S2}}
\end{equation}

Just like R, A can be evaluated through the use of multiple graphical representations, allowing for a more comprehensive analysis. By leveraging two or more distinct graphs, it becomes possible to capture different perspectives on the data, each providing unique insights into the nature of A. For example, the measure $R_{23}A_{23}$ represents a combined approach where both $R_{23}$ and $RA_{23}$ contribute to assessing the similarity or difference between two typing samples. Here, $R_{23}$ is a relative measure that ranks n-graphs (such as digraphs and trigraphs) based on their typing speed, while $A_{23}$ is an absolute measure that evaluates whether the typing durations of these n-graphs fall within an acceptable threshold of similarity. The combination of these measurements enhances accuracy, it also allows for a more robust performance evaluation. When used together, the complementary aspects of these graphs make performance better.

\subsection{Gaussian Mixture Model}
Gaussian Mixture Model (GMM), a linear non-Gaussian multivariate statistical method, is a popular algorithm used for handling non-Gaussian data. It is a statistical method based on the weighted sum of probability density functions of multiple Gaussian distributions \cite{ref:gmm}. GMM generates a vector of mean values corresponding to each component and a matrix of covariance which includes componentsâ€™ variances and the co-variances between each other. GMM can represent data in higher dimensions than Pure Gaussian by using a discrete set of Gaussian functions, each with its own mean and covariance matrix.\\

GMM is expressed by the parameter set $\lambda$: 
\begin{equation}
    \lambda = \{w_i,\vec{u}_i,\Sigma_i\}, \text{ with } i =1,...,M
\end{equation}
In that equation, $w_i$ are the component weights, $\vec{u}_i$ is the mean vector and $\Sigma_i$ is the covariance matrix.