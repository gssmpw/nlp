\section{Related Work}
\label{S:2}

Before presenting our FAA-CLIP method, we give a brief overview of related work on federated learning and CLIP-based methods in the literature. 

\mypar{Federated learning (FL).} The increasing awareness of challenges related to data privacy and security has urged researchers to develop novel machine learning solutions that can learn from multiple sources of data without having to share this data **Konečnỳ, "Federated Learning: Concepts, Methods, and Applications"**. Among the many solutions proposed for this issue, FL has emerged as the most promising and popular approach **McMahan, K., "Communication-Efficient Learning of Deep Networks from Decentralized Data"**. Methods based on this approach can be roughly divided into three categories: horizontal federated learning (HFL), vertical federated learning (VFL), and federated transfer learning (FTL) **Li, M., "Federated Transfer Learning for Heterogeneous Client Datasets"**. In HFL, clients share the same feature space while holding different samples. On the other hand, VFL in the FL setting where the datasets of individual clients contain information about the same samples but have different features. Last, in the FTL setting, client datasets differ both in terms of features and samples, with limited overlaps.

One of the leading HFL methods, FedAVG **McMahan, "Communication-Efficient Algorithms for Distributed Statistical Computing"**, proposes a simple averaging mechanism to combine the parameters of locally-trained models into a single global one. Although it preserves data privacy by only sharing parameters, this method  may still suffer from the problem of data heterogeneity between clients. To solve this issue, FedProx added a proximal regularization term and allowed slight model gaps between clients **Li, "Federated Learning with Low Frequency Regularization"**. In **Chen, Y., "A Survey on Federated Learning: An Adaptive Framework for Efficient Communication"**, authors argued that local distribution shifts in separate clients can be handled efficiently via batch normalization (BN) parameters. Based on this idea, they proposed a FedBN method that updates the BN parameters of each client locally without averaging them on the global server. On the other hand, MOON **Li, "Federated Learning with Low Frequency Regularization"** used a contrastive loss between local clients and global servers to improve overall performance.

Despite their improved performance, these methods mostly ignored the problem of generalization, which is also an important challenge in FL. To solve this problem, the work in **Wu, Y., "Federated Learning with Local Linear Models"** proposed a method to address separately the performance gaps arising from unseen client data (out-of-sample gap) and from unseen client distributions (participation gap). Likewise, the authors of **Kairouz, P., "Advances and Challenges in Federated Learning"** introduced a FL framework based on the Sharpness Aware Minimization (SAM) local optimizer to improve the model's generalization ability. Moreover, the method presented in **Wang, S., "Federated Learning with Local Linear Models"** employed a new variance reduction regularizer to dynamically calibrating the aggregation weights, thereby improving generalization ability.

\textcolor{black}{In **Kairouz, P., "Advances and Challenges in Federated Learning"**, they proposed a novel FL framework using mutual learning to provide a personalized local model with feasible performance. However, it introduces extra local training epochs.} Although these approaches boost the model robustness to distribution shifts, they were not intended for foundation models such as the one used in our work.

\mypar{Contrastive Language Image Pre-training (CLIP)} is an effective and scalable method learning representations jointly from image and text data **Radford, A., "Learning Transferable Visual Models From Natural Language Supervision"**. The CLIP model, which was trained from scratch using over 400 million image-text data pairs collected from the Internet **Li, R., "CLIP: Learning Transferable Visual-Semantic Representations"**, recently demonstrated remarkable performance in a wide range of image understanding tasks. However, the integration and use of VLMs like CLIP in FL applications is stll in its infancy. In **Vahabi, M., "Federated Text Prompt Learning for Contrastive Language Image Pre-Training"**, authors proposed replacing full VLM training with federated text prompt learning to reduce communication costs. Nevertheless, significant computational costs are still required to train the VLM.

The work in **Sangath, D., "Prompting and Adapting Pre-trained Models for Federated Learning Tasks"** also highlighted the challenge of designing task-specific prompts for \clip{}. \textcolor{black}{Furthermore, in **Goyal, A., "Adapter-Based Strategy for Leveraging Power of Pre-trained VLMs"**, an adapter-based strategy was designed to leverage the power of pre-trained VLMs.} This strategy, which only tunes and aggregates the parameters of adapters, demonstrated its effectiveness on several natural datasets. Inspired by the superior performance of \clip{} for zero shot and few shot tasks, the authors of **Sangath, D., "Prompting and Adapting Pre-trained Models for Federated Learning Tasks"** used \clip{} to optimize FL between the server and client models,  alleviating the problem of data heterogeneity between clients. However, this approach incurred large communication costs as it requires sharing the entire model. \textcolor{black}{In **Goyal, A., "Adapter-Based Strategy for Leveraging Power of Pre-trained VLMs"**, they introduced low-rank adaptation (LoRA) for large language models to reduce computational costs, and this technique is widely used in the CLIP backbone in recent studies.} In **Sangath, D., "Prompting and Adapting Pre-trained Models for Federated Learning Tasks"**, they proposed to use personalized prompts instead of global prompts for aggregation. The main idea is that they decoupled the parameters into base (global) and personalized (local) parameters.

\textcolor{black}{In **Goyal, A., "Adapter-Based Strategy for Leveraging Power of Pre-trained VLMs"**, they proposed an adapter-based dual teacher technique using mutual knowledge distillation to adapt VLMs in FL tasks. However, it has not been validated in medical datasets.} Furthermore, in **Sangath, D., "Prompting and Adapting Pre-trained Models for Federated Learning Tasks"**, they proposed to aggregate the prompts instead of a shared global model to reduce communication costs (PromptFL). 

\textcolor{black}{Unlike previous studies, we explore the potential of natural pre-trained CLIP in the FL context for both natural and medical images with an acceptable resource cost. We also introduce calibration analysis **Li, R., "CLIP: Learning Transferable Visual-Semantic Representations"** into CLIP-based FL approaches, an area that has been less explored in this context.}


\begin{figure*}
    \centering
    \includegraphics[width = 0.96 \textwidth]{TestImage/TMI_Pipeline2_new.pdf}
    \caption{\textcolor{black}{FAA-CLIP pipeline. Each client trains its local model separately, optimizing only the parameters of its local feature adaptation module (FAM) ($att_i$) and domain classifier $D_i$ using contrastive and domain adaptation losses. After receiving the local client parameters, the server aggregates them into a global (FAM) ($att^{*}$) whose parameters are transmitted back to clients.}}
    \label{fig:framework}
\end{figure*}