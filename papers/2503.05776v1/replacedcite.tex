\section{Related Work}
\label{S:2}

Before presenting our FAA-CLIP method, we give a brief overview of related work on federated learning and CLIP-based methods in the literature. 

\mypar{Federated learning (FL).} The increasing awareness of challenges related to data privacy and security has urged researchers to develop novel machine learning solutions that can learn from multiple sources of data without having to share this data ____. Among the many solutions proposed for this issue, FL has emerged as the most promising and popular approach ____. Methods based on this approach can be roughly divided into three categories: horizontal federated learning (HFL), vertical federated learning (VFL), and federated transfer learning (FTL) ____. In HFL, clients share the same feature space while holding different samples. On the other hand, VFL in the FL setting where the datasets of individual clients contain information about the same samples but have different features. Last, in the FTL setting, client datasets differ both in terms of features and samples, with limited overlaps. 

One of the leading HFL methods, FedAVG ____, proposes a simple averaging mechanism to combine the parameters of locally-trained models into a single global one. Although it preserves data privacy by only sharing parameters, this method  may still suffer from the problem of data heterogeneity between clients ____. To solve this issue, FedProx added a proximal regularization term and allowed slight model gaps between clients and the server ____. In ____, authors argued that local distribution shifts in separate clients can be handled efficiently via batch normalization (BN) parameters. Based on this idea, they proposed a FedBN method that updates the BN parameters of each client locally without averaging them on the global server. On the other hand, MOON ____ used a contrastive loss between local clients and global servers to improve overall performance. 

Despite their improved performance, these methods mostly ignored the problem of generalization, which is also an important challenge in FL. To solve this problem, the work in ____ proposed a method to address separately the performance gaps arising from unseen client data (out-of-sample gap) and from unseen client distributions (participation gap). Likewise, the authors of ____ introduced a FL framework based on the Sharpness Aware Minimization (SAM) local optimizer to improve the model's generalization ability. Moreover, the method presented in ____ employed a new variance reduction regularizer to dynamically calibrating the aggregation weights, thereby improving generalization ability. \textcolor{black}{In ____, they proposed a novel FL framework using mutual learning to provide a personalized local model with feasible performance. However, it introduces extra local training epochs.} Although these approaches boost the model robustness to distribution shifts, they were not intended for foundation models such as the one used in our work.

\mypar{Contrastive Language Image Pre-training (CLIP)} is an effective and scalable method learning representations jointly from image and text data ____. The CLIP model, which was trained from scratch using over 400 million image-text data pairs collected from the Internet ____, recently demonstrated remarkable performance in a wide range of image understanding tasks ____. However, the integration and use of VLMs like CLIP in FL applications is stll in its infancy. In ____, authors proposed replacing full VLM training with federated text prompt learning to reduce communication costs. Nevertheless, significant computational costs are still required to train the VLM. The work in ____ also highlighted the challenge of designing task-specific prompts for \clip{}. \textcolor{black}{Furthermore, in ____, an adapter-based strategy was designed to leverage the power of pre-trained VLMs.} This strategy, which only tunes and aggregates the parameters of adapters, demonstrated its effectiveness on several natural datasets. Inspired by the superior performance of \clip{} for zero shot and few shot tasks, the authors of ____ used \clip{} to optimize FL between the server and client models,  alleviating the problem of data heterogeneity between clients. However, this approach incurred large communication costs as it requires sharing the entire model. \textcolor{black}{In ____, they introduced low-rank adaptation (LoRA) for large language models to reduce computational costs, and this technique is widely used in the CLIP backbone in recent studies ____. In ____, they proposed to use personalized prompts instead of global prompts for aggregation. The main idea is that they decoupled the parameters into base (global) and personalized (local) parameters.} \textcolor{black}{In ____, they proposed an adapter-based dual teacher technique using mutual knowledge distillation to adapt VLMs in FL tasks. However, it has not been validated in medical datasets.} Furthermore, in ____, they proposed to aggregate the prompts instead of a shared global model to reduce communication costs (PromptFL). 

\textcolor{black}{Unlike previous studies, we explore the potential of natural pre-trained CLIP in the FL context for both natural and medical images with an acceptable resource cost. We also introduce calibration analysis ____ into CLIP-based FL approaches, an area that has been less explored in this context.}


\begin{figure*}
    \centering
    \includegraphics[width = 0.96 \textwidth]{TestImage/TMI_Pipeline2_new.pdf}
    \caption{\textcolor{black}{FAA-CLIP pipeline. Each client trains its local model separately, optimizing only the parameters of its local feature adaptation module (FAM) ($att_i$) and domain classifier $D_i$ using contrastive and domain adaptation losses. After receiving the local client parameters, the server aggregates them into a global (FAM) ($att^{*}$) whose parameters are transmitted back to clients.}}
    \label{fig:framework}
\end{figure*}



%AC