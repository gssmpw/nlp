%% This is file `medima-template.tex',
%% 
%% Copyright 2018 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%%
%% $Id: medima-template.tex 153 2018-12-01 11:38:32Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsarticle/trunk/medima-template.tex $
%%
%% Use the option review to obtain double line spacing
%\documentclass[times,review,preprint,authoryear]{elsarticle}

%% Use the options `twocolumn,final' to obtain the final layout
%% Use longtitle option to break abstract to multiple pages if overfull.
%% For Review pdf (With double line spacing)
%\documentclass[times,twocolumn,review]{elsarticle}
%% For abstracts longer than one page.
%\documentclass[times,twocolumn,review,longtitle]{elsarticle}
%% For Review pdf without preprint line
%\documentclass[times,twocolumn,review,nopreprintline]{elsarticle}
%% Final pdf
\documentclass[times,twocolumn,final]{elsarticle}
%%
%\documentclass[times,twocolumn,final,longtitle]{elsarticle}
%%


%% Stylefile to load MEDIMA template
\usepackage{medima}
\usepackage{framed,multirow}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{latexsym}

% Following three lines are needed for this document.
% If you are not loading colors or url, then these are
% not required.
\usepackage{url}
\usepackage{xcolor}

\usepackage{hyperref}

\usepackage{graphicx}
\usepackage{array}
\usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
\usepackage{xcolor}
\usepackage{threeparttable}
\usepackage{booktabs} % For formal tables
\usepackage{multicol}
% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{algorithm}
\usepackage{bm}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{xcolor}
\usepackage{bbding}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{amsmath}

\definecolor{newcolor}{rgb}{.8,.349,.1}

\journal{Medical Image Analysis}

\begin{document}

\verso{Peng Ling}

\begin{frontmatter}

\title{FrGNet: A fourier-guided weakly-supervised framework for nuclear instance segmentation}%
% \title{FrGNet: A fourier-guided framework for nuclei instance segmentation in histopathologic images}%
% \tnotetext[tnote1]{This is an example for title footnote coding.}

% \fntext[fn1]{This is author footnote for second author.}
\author[1]{Peng \snm{Ling}\fnref{fn1}}
\author[2]{Wenxiao \snm{Xiong}}
\fntext[fn2]{Wenxiao Xiong: The School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China(xiongwx6@mail2.sysu.edu.cn).}
\cortext[cor1]{Corresponding author at: 
  % Tel.: +0-000-000-0000;  
  % fax: +0-000-000-0000;}
  Personal.
  \textit{E-mail address}: lingp1999@gmail.com.
  }

% \received{1 May 2013}
% \finalform{10 May 2013}
% \accepted{13 May 2013}
% \availableonline{15 May 2013}
% \communicated{S. Sarkar}

% \input{0_abs}
\begin{abstract}
%%%
Nuclear instance segmentation has played a critical role in pathology image analysis. 
The main challenges arise from the difficulty in accurately segmenting instances and the high cost of precise mask-level annotations for fully-supervised training.
In this work, we propose a fourier guidance framework for solving the weakly-supervised nuclear instance segmentation problem.
In this framework, we construct a fourier guidance module to fuse the priori information into the training process of the model, which facilitates the model to capture the relevant features of the nuclear.
Meanwhile, in order to further improve the model's ability to represent the features of nuclear, we propose the guide-based instance level contrastive module.
This module makes full use of the framework's own properties and guide information to effectively enhance the representation features of nuclear.
We show on two public datasets that our model can outperform current SOTA methods under fully-supervised design, and in weakly-supervised experiments, with only a small amount of labeling our model still maintains close to the performance under full supervision.
In addition, we also perform generalization experiments on a private dataset, and without any labeling, our model is able to segment nuclear images that have not been seen during training quite effectively.
 As open science, all codes and pre-trained
models are available at https://github.com/LQY404/FrGNet.
%%%%
\end{abstract}

\begin{keyword}
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
% \MSC 41A05\sep 41A10\sep 65D05\sep 65D17
%% Keywords
\KWD \\ Nuclear instance segmentation \\ Weakly-supervised learning \\ Deep learning \\ Medical image segmentation
\end{keyword}

\end{frontmatter}

%\linenumbers

%% main text
% \section{Note}
% \input{macros}
% \input{1_intro}
\section{Introduction}
Pathological slide analysis is widely regarded as the gold standard for cancer diagnosis, treatment, and prevention.
Nuclear instance segmentation is a critical step in this process, because the nuclear features such
as average size, density and nucleus-to-cytoplasm ratio are related to the clinical diagnosis and management of cancer.
In general, some of the major nuclear segmentation methods are based on fully-supervised designs~\citep{latorre2013segmentation, abbas2014occluded, sheeba2014splitting, zhou2019cia, chen2017dcan, graham2019hover, he2017mask, liu2021panoptic, naylor2018segmentation, he2021hybrid, qu2019improving, raza2019micro, ronneberger2015u, chen2023cpp, liu2021mdc, zhou2022semantic, upschulte2022contour, he2023toposeg}, which can be divided into three main categories: 1) methods based on traditional image manipulation~\citep{latorre2013segmentation, abbas2014occluded, sheeba2014splitting}, which rely on cumbersome post-processing processes; 2) detection-based methods, which either use a strategy of detecting and then segmenting the algorithms~\citep{he2017mask}, or model the nuclear using contour and regress them directly to to obtain results~\citep{upschulte2022contour}; 3) segmentation-based methods, which directly generate multiple segmentation masks and then use post-processing operations to obtain nuclear segmentation instances~\citep{zhou2019cia, chen2017dcan, graham2019hover, he2017mask, liu2021panoptic, naylor2018segmentation, qu2019improving, raza2019micro, ronneberger2015u, chen2023cpp, liu2021mdc, zhou2022semantic, he2023transnuseg, nam2023pronet, he2023toposeg}.
However, the fully supervised learning of deep neural networks in these methods requires a large amount of training data, which are pixel-wise annotated. 
It is difficult to collect such datasets because assigning a nuclear/background class label to every pixel in the image is very time-consuming and requires expert domain knowledge.

In this work, we find that as shown in Fig.~\ref{fig:teaser}, we can get coarse nuclear instance segmentation without any training.
Specifically, for a nuclear pathology image, the approximate location of the nuclear can be obtained by fourier transform (guide mask).
Obviously, such a guide mask can provide very powerful a priori information for nuclear segmentation.
In order to fully utilize this a priori information
We introduce a framework for segmenting nuclear instances using fourier guidance. 
This framework integrates intrinsic feature information from nuclear images and incorporates a priori information about nuclear locations through a specially designed Fourier Guidance (FG) module, which directs the training process.
To further enhance model performance, we propose a Guide-based Instance Level Contrastive (GILC) module. This module offers instance-level feature guidance and strengthens the feature representation of nuclear instances.

\begin{figure*}[tb]
\centering
  \includegraphics[width=0.99\linewidth]{teaser.pdf}
  \caption{
  Motivations. We can get corse nuclear instance segmentation results without any training.
  }
\label{fig:teaser}
\end{figure*}

The contributions can be summarized as follows:
\begin{itemize}
    \item We propose a new framework FrGNet, for nuclei instance segmentation in histopathologic images. 
    This framework takes the characteristic of nuclear into consideration, utilizes masks generated by Fourier transformation as guidance to effectively address both full-supervised and weak-supervised nuclei instance segmentation tasks.
    \item We propose the Fourier Guidance (\textbf{FG}) Module. This module make full use of the characteristics of nuclear image, providing the model with a priori information about the location of the nuclear, allowing the model to better localise it.
    \item To further improve the model performance, we propose the guide-based instance level contrastive (\textbf{GILC}) module, which provides instance-level feature guidance for the model and enhances the feature representation of the nuclear instances.
    \item We demonstrate the efficiency of our proposed framework through experiments on two classical nuclei instance segmentation datasets. 
    Compared to recent methods, our approach achieves superior performance, setting a new state-of-the-art (SOTA) benchmark. 
    Additionally, we showcase the capability of our framework in the field of weak-supervised nuclei instance segmentation.
    Furthermore, the powerful generalization of our model is demonstrated by generalization experiments on a private dataset \textbf{without any annotations}.
\end{itemize}

% \input{2_rel}
\section{Related Work}

\subsection{Nuclear instance segmentation}
Before the emergence of deep learning, instance segmentation methods predominantly relied on classical machine learning algorithms and image processing techniques. 
These included statistical features, thresholding-based methods~\citep{latorre2013segmentation, abbas2014occluded, sheeba2014splitting}, and morphological features for contour and shape identification. 
Energy-based solutions gained prominence during the 1990s. 
Marker-based approaches such as the Watershed algorithm proved beneficial for nuclear instance segmentation, as demonstrated by \citet{cheng2008segmentation}. 
However, these techniques faced limitations in accurately segmenting nuclear due to their diverse structures, textures, intensities, leading to unreliable outcomes. 
Moreover, the effectiveness of these systems heavily depended on manual parameter tuning, including thresholds and weights, rendering them unsuitable for widespread application due to their inherent unreliability.

With the rapid development of deep learning, \citet{ronneberger2015u} proposed the UNet model, which has since become one of the most fundamental models in medical image segmentation. 
\citet{raza2019micro} introduced Micro-Net, achieving robustness to large internal and external variances in nuclear size by utilizing multi-resolution and weighted loss functions. \citet{qu2019improving} developed a full-resolution convolutional neural network (FullNet), which enhances localization accuracy by eliminating downsampling operations in the network structure. 
\citet{he2021hybrid} presented a hybrid attention nested U-shaped network (Han-Net) to extract effective feature information from multiple layers.

To leverage contour information for distinguishing contact/overlapping nuclear, \citet{chen2017dcan} initially proposed incorporating contour information into a multi-level fully convolutional network (FCN) to create a deep contour-aware network for nuclear instance segmentation. Subsequently,
\citet{zhou2019cia} introduced the contour-aware information aggregation network, which combines spatial and textural features between nuclear and contours. 
Additionally, some models have approached nuclear instance segmentation as an object detection task, such as contour proposal networks (CPN)~\citep{upschulte2022contour}, which use a sparse list of contour representations to define a nuclear instance.

Several works~\citep{chen2023cpp, graham2019hover, liu2021mdc, naylor2018segmentation} have introduced distance maps to separate contact/overlapping nuclear. 
\citet{naylor2018segmentation} addressed the issue of segmenting touching nuclear by formulating the segmentation task as a regression task of intra-nuclear distance maps.
\citet{graham2019hover} proposed Hover-Net, a network for simultaneous nuclear segmentation and classification, which uses the vertical and horizontal distances between a nuclear pixel and its center of mass to separate clusters of nuclear. 
Moreover, \citet{he2021cdnet} introduced a centripetal directional network (CDNet) for nuclear instance segmentation, incorporating directional information into the network. \citet{he2023toposeg} take topological information into consideration to further split overlapping nuclear instance.
These works mainly are proposed for full-supervised nuclear instance segmentation, and don't consider the characteristic of nuclear itself.
% However, nuclear annotations is merely in real world.
In this work, we make full use of the characteristic of nuclear instance (we name it as \textbf{nuclear guidance}), introduce a new framework with guidance, not only can solve full-supervised but also weak-supervised nuclear instance segmentation efficiently.

\subsection{Weakly-supervised segmentation}
Weakly supervised approaches offer the advantage of reducing manual annotation effort compared to fully supervised methods. 
In natural image segmentation, ~\citet{papandreou2015weakly} proposed an Expectation-Maximization (EM) method for training with image-level or bounding-box annotations. ~\citet{pathak2015constrained} added a set of linear constraints on the output space in the loss function to leverage information from image-level labels.

In contrast to image-level annotations, point annotations provide more precise location information for each object. ~\citet{bearman2016s} incorporated an objectness prior in the loss function to guide the training of a CNN, aiding in the separation of objects from the background. Scribble annotations, which require at least one scribble per object, are a more informative type of weak label. ~\citet{lin2016scribblesup} used scribble annotations to train a graphical model that propagates information from the scribbles to the unmarked pixels.

Bounding boxes are the most widely used form of weak annotation, applied in both natural images~\citep{dai2015boxsup, rajchl2016deepcut} and medical images~\citep{yang2018boxnet, zhao2018deep}. 
~\citet{kervadec2019constrained} utilized a small fraction of full labels and imposed a size constraint in their loss function, achieving good performance, though this method is not applicable to scenarios involving multiple objects of the same class. Another method~\citep{qu2020weakly} proposed a two-stage approach that uses only a small fraction of nuclear locations.

In this work, we propose a novel end-to-end framework to solve both fully-supervised and weakly-supervised nuclear instance segmentation tasks.
We start from the characteristics of the nuclear image itself, and use the information of this feature as the a priori information to build the corresponding module to guide the model for training.
With only a small amount of labeled data, our model is able to approach fully-supervised results.

\subsection{Contrastive learning}
Contrastive learning is a highly regarded technique for learning representations from unlabeled features these days~\citep{chen2020simple, chen2020improved, grill2020bootstrap}. 
It aims to enhance representation learning by contrasting similar features (positive pairs) against dissimilar features (negative pairs). A key innovation in contrastive learning lies in the selection of positive and negative pairs. Additionally, the use of a memory bank to store more negative samples has been adopted, as this can lead to improved performance~\citep{chen2020simple}.

In the field of segmentation, numerous works leverage contrastive learning for the pre-training of models~\citep{chaitanya2020contrastive, wang2021dense, xie2021propagate}. 
Recently, ~\citet{wang2021exploring} demonstrated the advantages of applying contrastive learning in a cross-image pixel-wise manner for supervised segmentation. 
The CAC approach~\citep{lai2021semi} shows improvement in semi-supervised segmentation by performing directional contrastive learning pixel-to-pixel, aligning lower-quality features towards their high-quality counterparts.

Unlike these works, we construct the guide-based insatnce level contrastive (GILC) module from the image characteristics of the nuclear, which relies on the automatically generated guide mask to further enhance the feature representation of the nuclear, and is able to be applied to both fully-supervised and weakly-supervised nuclear instance segmentation tasks.

% \input{3_method}
\section{Method}

\subsection{Overview}
As illustrated in Fig.~\ref{fig:framework_train}, our framework consist of three parts: feature extraction module, fourier guidance module, and guide-based instance level contrastive module.
For an input nuclear pathology image, we first extract the image features using the feature extraction module to obtain multi-level image features, with the layers being smaller at higher resolutions.
Secondly, for the multilevel image features, we uniformly input them into the fourier guidance module for processing.

In the fourier guidance module, we select the feature map of layer 0 as the input to the fourier guide head to get the predicted guide mask, and use the pre-generated guide mask ground truth for supervision.
Subsequently, for the predicted guide mask, we use guide attention residual unit (GARU) to filter and reinforce all the feature maps with features to produce new feature maps.
Finally, in order to get the nuclear segmentation instances, we extract the layer 1 from the new feature maps as the input to the instance head to get the predicted instance results, supervision only for labeled instances.

In addition, in order to further enhance the feature representation of nuclear, we use the proposed instance-level comparison module to make full use of the bootstrap a priori information to strengthen the nuclear features and enhance the model performance.

\begin{figure*}[tb]
\centering
  \includegraphics[width=0.99\linewidth]{framework_trainv3.pdf}
  \caption{
  The workflow of our proposed FrGNet framework. For an input image, we first extract the image features using the feature extraction module to obtain multi-level image features, then use the features maps to generate guide mask and nuclear instances in fourier guidance module. Furthermore, the proposed instance-level comparison module also make use of the feature maps to enhances the feature representation of the nuclear instances.
  }
\label{fig:framework_train}
\end{figure*}

\begin{figure*}[tb]
\centering
  \includegraphics[width=0.99\linewidth]{framework_inferencev5.pdf}
  \caption{
  Overview of inference stage. For an input image, after processed by FrGNet, the instance head generate original predicted instances, and the fourier guide head generate predicted guide mask of input image. In post process operation, we filter out the instances in original predicted instances group according to the predicted guide mask, producing the final refined instances.
  }
\label{fig:framework_inference}
\end{figure*}

To make the manuscript self-contained, we briefly describe the necessary technical details of the original Contour proposal networks (CPN)~\citep{upschulte2022contour} first, and then introduce how we extend it with proposed fourier guide module and guide-based instance level contrastive module for nuclear instance segmentation task.

\subsection{Contour proposal networks}
Contour proposal networks (CPN) is proposed by ~\citet{upschulte2022contour}, which models nuclear instances using explicit contour expressions.
CPN consists of five parts: feature extraction backbone, classification head, contour regression head, contour refinement regression head, and post-processing.
For the input nuclear image, feature extraction is first performed using backbone (e.g., ResNet~\citep{he2015deep}).
Then, layer 2 feature map $P_2$ is extracted for instance classification and instance contour regression to obtain instance proposals.
For these instance proposals, they are sampled according to the ground truth, and only the proposals that correspond to the nuclear of the cell according to the ground truth are retained.
These retained proposals are then fine-tuned to all contours using the results generated by the contour refinement regression head, so that each predicted contour more closely matches its corresponding ground truth.
Finally, overlapping instances are removed using Non-Maximum Suppression (NMS) to get the final output instance results.

\subsection{Fourier guidance module}
As shown in Fig.~\ref{fig:teaser}, we find that the fourier transform can be used to obtain the example segmentation results of the nuclear in the pathology image of the nuclear directly, using the image characteristics of the nuclear, without any training at all.
However, such segmentation results are rough and cannot handle more complex cases.
Therefore, we inject this image characteristics information of the nuclear itself into the model training process as a kind of a priori information to guide the segmentation of the model.


To this end, we first design the automatic generation strategy of guide mask as shown in Fig. ~\ref{fig:gmask_gen}.
Specifically, for an input image of a nuclear pathology, we first use the fourier transformation on it to obtain the corresponding fourier spectrogram.
Second, this spectrogram is low-pass filtered using a circle with radius R (we set it as 1 during experiments) to obtain the corresponding high-frequency fourier spectrogram.
Again, for the high-frequency fourier spectrogram, we use the inverse fourier transformation to convert it back to the spatial domain image to obtain the initial guide mask.
At this time, the guide mask has more noise, in order to facilitate the processing, we carry out the normalization operation on it, so that its pixel value is normalized to between 0-1, obtaining the normalized guide mask.
Finally, we add the binary instance ground truth mask on the basis of the normalized guide mask to get the final guide mask trained with the guide model.

After obtaining the guide mask through the above process, we designed the fourier guidance module to further handle the integration of the guide mask with the model training process.
As shown in Fig.~\ref{fig:framework_train}, the input of the fourier guidance module is multi-layered feature maps.
For this feature maps, we take the layer 0 feature map $F_0$ and construct the fourier guide head, using $F_0$ as the input to get the predicted guide mask, which is supervised using $Loss_{guide\_mask}$.

Meanwhile, we utilize the predicted guide mask to filter all feature maps using the constructed GARU.
Specifically, for each feature map $F_i$, we first resize the predicted guide mask to the same size as $F_i$, and then use the GARU to bootstrap the update of $F_i$ to strengthen the features at the corresponding locations of the nuclear, and weaken the features of the non-nuclear regions to obtain the updated feature map.
For this updated feature map, we then use instance head to predict the segmentation of instances to get the final segmentation result.

% By the above way, we can not only automatically generate the guide mask with the information of the nuclear itself, but also incorporate the guide mask into the model to guide its training, which effectively improves the performance of the model.
\begin{figure*}[tb]
\centering
  \includegraphics[width=0.99\linewidth]{gmask_gen.pdf}
  \caption{
  The flow of guide mask generation. For an image, we use fourier transform, low frequency filtering, inverse fourier transform to get the initial guide mask, which is then normalized and added with binary instance ground truth mask to get the final guide mask which is used to guide the model for training.
  }
\label{fig:gmask_gen}
\end{figure*}

\subsection{Guide-based instance level contrastive module}
In nuclear pathology images, there is a more pronounced difference between the nuclear and the background region, and a higher degree of similarity between the nuclear and the nuclear.
Therefore, the similarity between the features of different nuclear should be as similar as possible.
In our framework, the input of instance head is the feature map $F_1$ of layer 1, with size $D_1$ x $h_1$ x $w_1$.
During processing, we treat each position on $F_1$ as an instance-level feature.
Therefore, at most $h_1$*$w_1$ instance features may exist at this point.
In the actual training process, the model is able to gradually learn the nuclear instance feature representation due to the presence of ground truth.
In order to further promote the feature representation of nuclear, we utilize the guidance information provided by the guide mask to further strengthen the model's feature representation of nuclear.

Specifically, for each nuclear feature $C_i$, a vector of dimension $D_1$, we first adopt a similar projector as SimLRV2~\citep{chen2020big}, which maps $C_i$ into a hidden space of dimension $P$ to obtain nuclear feature embedding.
Subsequently, we resize the guide mask to $h_1$ x $w_1$ dimensions.
Finally, based on the resized guide mask, all the nuclear feature embedding are searched for their corresponding positive and negative samples, and supervised training is performed using the contrast loss infoNCE~\citep{oord2018representation} as a way to further enhance the model's representation of nuclear features.


\subsection{Objective functions}
To optimise our FrGNet, our loss function consists of three parts: instance, guide mask, and contrastive:
\begin{equation}
\label{eqn:loss_overall}
Loss = Loss_{instance} + Loss_{guide\_mask} + Loss_{contrastive}
\end{equation}

For instance loss, since we use contour to represents each instance, the problem of segmentation of instances is solved by transforming it into a regression of contour coordinates.
In the process of implementation we use the contour expression form in CPN and the instance loss construction, please see the original article for details.
The guide mask is generated by the fourier guide head, whose ground truth is a mask between 0 and 1. 
Therefore, we use binary cross entropy loss for supervision:
\begin{equation}
\label{eqn:loss_gmask}
Loss_{guide\_mask}(y, \hat{y}) = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
\end{equation}

Where N is the number of samples, $y_i$ is the true guide mask of the i-th sample, and $\hat{y}_i$ is the predicted obtained guide mask.

As for $Loss_{contrastive}$, we use infoNCE loss~\citep{oord2018representation} to construct, defined as below:
\begin{equation}
\label{eqn:loss_contrastive}
Loss_{contrastive} = - \frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_i^+)/\tau)}{\sum_{j=1}^{N} \exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_j)/\tau)}
\end{equation}

where N is the number of samples, $\mathbf{z}_i$ is the i-th instance embedding, $\mathbf{z}_i^+$ is the positive instance embedding of $\mathbf{z}_i$.
$sim()$ represents the similarity operation, we use dot product during experiments.
The $\tau$ denotes temperature coefficient, used to adjust the distribution of similarity scores, we keep it as 0.1.


\subsection{Inference stage}
The workflow of inference stage is shown in Fig.~\ref{fig:framework_inference}.
Firstly, for an input image, after processed by FrGNet, the instance head generates original predicted instances, the instances are generated in the form of contour. 
% These instances are generated in the form of contour.
Secondly, the fourier head generates the predictions of the guide mask.
Finally, for each of the predicted instances, we perform post-processing operations in conjunction with the predicted guide mask.
Specifically, for a predicted instance, we extract the region of the guide mask where the instance is located from the predicted guide mask.
If the maximum value of the pixel values in the region's guide mask is less than a set filtering threshold, the instance is discarded, otherwise it is retained.
Such a filtering operation is performed until the last predicted instance, and the retained instances are the final output refined instances.


% \input{4_exp}
\section{Experiments}

\subsection{Experimental settings}

\noindent\textbf{Dataset:} 1) MoNuSeg~\citep{kumar2017dataset} contains 51 H\&E stained pathological images of size 1000×1000 from seven organs, including a total of 21,623 nuclei labeled without distinction between various categories.
In the original dataset, there are 37 for training, and 14 for testing (combining the same organ and different organ test sets);
2) CPM17~\citep{vu2019methods} contains 64 H\&E stained histopathology images with 7,570 annotated nuclear boundaries, where 32 for training and 32 for testing.
It is from the MICCAI 2017 Digital Pathology Challenge~\citep{vu2019methods} and images on two different scales: 500×500 and 600×600.
This is also a dataset for single class segmentation task.


\noindent\textbf{Evaluation metrics:} 
We use four instance-level evaluation metrics to measure the instance segmentation performance of the comparison models, which are: Aggregated Jaccard Index (AJI)~\citep{kumar2017dataset}, mean Detection Quality (DQ), mean Segmentation Quality (SQ), and mean Panoptic Quality (PQ)~\citep{kirillov2019panoptic}.
AJI computes the correlation between intersection and union pixel counts. 
It addresses the issue of over-penalization by employing ground truth with maximal intersection over union:
\begin{equation}
\text{AJI} = \frac{\sum_{i=1}^{n} |G_i \cap P_{M}^{i}|}{\sum_{i=1}^{n} |G_i \cup P_{M}^{i}| + \sum_{F \in U} |P_F|}
\end{equation}

where $n$ denotes the total number of nuclei, $P_{M}^{i}$ represents the connected regions responsible for generating mask.
$G_i$ represents the connected area where $P_{M}^{i}$ has the largest intersection with actual data. 
$U$ denotes connected regions that are not intersecting with actual data, and $P_F$ denotes element $F$ inside set $U$.
DQ, SQ and PQ defined as Eq.~\eqref{eqn:pq}.
\begin{equation}
\text{PQ} = \underbrace{\frac{\sum_{(x, y) \in TP} \text{IoU}(x, y)}{|TP|}}_{\text{segmentation quality (SQ)}} \times \underbrace{\frac{|TP|}{|TP| + \frac{1}{2}|FP| + \frac{1}{2}|FN|}}_{\text{detection quality (DQ)}}
\label{eqn:pq}
\end{equation}

where $x$ is the ground truth values, $y$ are the prediction values. 
$\text{IOU}$ denotes intersection over union and each $(x, y)$ is established as a distinct and different set. 
$TP$, $FP$, $FN$ represent true positive, false positive and false negative, respectively.



\subsection{Implementation details}
We proposed FrGNet is trained and tested using the open-source software library Pytorch 1.13.1 on 2 NVIDIA GeForce 3090 with CUDA 11.7.

Considering the size of the MoNuSeg and CPM17 datasets, we crop patches from the original histopathologic images using fixed-size boxes. 
% with each patch set to $384$×$384$ pixels. 
For the cropped instances, if the area of a cropped instance is less than 10\% of the initial instance, the cropped instance is removed. 
Additionally, we remove small objects with an area of less than 20 pixels to avoid unnecessary foreground caused by incorrect pixel predictions. 
We obtain 481 patches and 416 patches from MoNuSeg and CPM17 dataset respectively. 
Examples of the cropped patches are shown in Fig.~\ref{fig:example_cropped}.
\begin{figure}[tb]
\centering
  \includegraphics[width=0.99\linewidth]{example_cropped.pdf}
  \caption{
  Examples of cropped histopathologic image and corresponding instance mask ground truth.
  }
\label{fig:example_cropped}
\end{figure}

For full-supervised setting, we use all instance annotations to train proposed FrGNet model.
We train for 50 epoches with a batch size of 16 in MoNuSeg dataset.
AdamW~\citep{loshchilov2017decoupled} is adopted as the optimizer, with an initial learning rate 0.0004.
As for CPM17 dataset, We train for 30 epoches with a batch size of 16.
Adam~\citep{kingma2014adam} is adopted as the optimizer, with an initial learning rate 0.001.
Both training use warm-up and multi-step decay strategy to control the change of learning rate.
Additionally, we use replace standard batch normalization~\citep{ioffe2015batch} with synchronized batch normalization during training.

For weak-supervised setting, we first random keep 20\%, 30\%, 40\%, 60\% and 80\% instance annotations in each image to execute training.
Then, we use full annotations to test model that trained in weak-supervised setting.
% Other training settings are consistent with full supervised setting.
Except for using hard guide, other training settings are consistent with full supervised.
During inference, we set the threshold of post process as 0.5.


\subsection{Performance comparisons in full-supervised}
We compare our weakly-supervised method with fully-
supervised methods that are trained with the completely-annotated
nuclei instances, such as U-Net~\citep{ronneberger2015u}, Mask-RCNN~\citep{he2017mask}, DCAN~\citep{chen2017dcan}, DIST~\citep{naylor2018segmentation}, Micro-Net~\citep{raza2019micro}, Full-Net~\citep{qu2019improving}, Hover-Net~\citep{graham2019hover}, PFF-Net~\citep{liu2021panoptic}, CDNet~\citep{he2021cdnet}, CPN~\citep{upschulte2022contour} and TopoSeg~\citep{he2023toposeg}.
The quantitative results are shown in Table~\ref{tab:comparison_sota}. 
As can be seen from the table, our proposed method FrGNet possesses a significant performance advantage over all current SOTAs on the Monuseg dataset, with at least 4.8\% and 2\% improvement in the PQ and AJI metrics, respectively, to achieve the new SOTA results.
Correspondingly, our method also has some advantages on the CPM17 dataset, proving the effectiveness of our method.

To further demonstrate the effectiveness of our framework, we performed a comparison from a visualisation point of view, and the comparison results are shown in Fig.~\ref{fig:comp_full}.
Our FrGNet not only handles the case of dense segmentation of nuclear well (first row), but also can substantially suppress the emergence of false positive instances when the distribution of nuclear is more sparse (the second and the third rows), which is attributed to the design of our Fourier-guided model architecture.


\begin{table}[tb]
\caption{Performance comparisons of different full-supervised methods on MoNuSeg and CPM17 datasets.}
\label{tab:comparison_sota}
\begingroup %这一行要加！！！！！！
\renewcommand{\arraystretch}{1.7}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{c|cc|cc}
\hline
          \multirow{2}{*}{Method} & \multicolumn{2}{c|}{MoNuSeg}  & \multicolumn{2}{c}{CPM17}    \\ \cline{2-5}
          & \multicolumn{1}{c}{PQ $\uparrow$} & AJI $\uparrow$ & \multicolumn{1}{c}{PQ $\uparrow$} & AJI $\uparrow$ \\  \hline
U-Net~\citep{ronneberger2015u}     & \multicolumn{1}{c}{$58.1$}   & $57.9$  & \multicolumn{1}{c}{$62.5$}   & $66.6$  \\ 
Mask-RCNN~\citep{he2017mask} & \multicolumn{1}{c}{$50.9$}   & $54.6$ & \multicolumn{1}{c}{$67.4$}   &  $68.4$   \\ 
DCAN~\citep{chen2017dcan}      & \multicolumn{1}{c}{$49.2$}   &  $52.5$   & \multicolumn{1}{c}{$54.5$}   &   $56.1$  \\ 
DIST~\citep{naylor2018segmentation}      & \multicolumn{1}{c}{$44.3$}   &  $55.9$   & \multicolumn{1}{c}{$50.4$}   &   $61.6$  \\ 
Micro-Net~\citep{raza2019micro} & \multicolumn{1}{c}{$51.9$}   &  $56.0$   & \multicolumn{1}{c}{$66.1$}   &  $66.8$   \\ 
Full-Net~\citep{qu2019improving}  & \multicolumn{1}{c}{-}   &   $60.4$  & \multicolumn{1}{c}{-}   &  $66.1$   \\ 
Hover-Net~\citep{graham2019hover} & \multicolumn{1}{c}{$59.7$}   &  $61.8$   & \multicolumn{1}{c}{$69.7$}   &   $70.5$  \\ 
PFF-Net~\citep{liu2021panoptic}   & \multicolumn{1}{c}{$58.7$}   &  $61.1$  & \multicolumn{1}{c}{-}   &   -  \\ 
CDNet~\citep{he2021cdnet}     & \multicolumn{1}{c}{-}   &  $63.7$   & \multicolumn{1}{c}{-}   &  $73.3$   \\ 
CPN~\citep{upschulte2022contour}     & \multicolumn{1}{c}{$62.7$}   &  $61.5$   & \multicolumn{1}{c}{68.9}   &  $69.1$   \\ 
TopoSeg~\citep{he2023toposeg}   & \multicolumn{1}{c}{$62.5$}   &  $64.3$   & \multicolumn{1}{c}{$70.5$}   &  $75.6$   \\  \hline
\textbf{FrGNet (Ours)}      & \multicolumn{1}{c}{\textbf{65.5}}   &   \textbf{65.6}  & \multicolumn{1}{c}{\textbf{70.7}}   &   71.2  \\ \hline
\end{tabular}
}
\endgroup
\end{table}

\begin{figure*}[tb]
\centering
  \includegraphics[width=0.99\linewidth]{comp_full.pdf}
  \caption{
  Comparisons of different full-supervised methods on MoNuSeg and CPM17 datasets.
  }
\label{fig:comp_full}
\end{figure*}

\subsection{Performance comparisons in weak-supervised}
The quantitative results of the method proposed in this paper and the existing methods under weakly supervised design are shown in Table 2.
Under weak supervision, we randomly remove 20\%, 40\%, 60\%, 70\%, and 80\% of the instance annotations in each image, followed by training, and testing with the same test set as full supervision.
As can be seen from the table, the quantitative metrics of baseline show a linear decrease as the instance annotations are reduced, while our method always maintains a performance that is not too different from that of full supervision.
Structurally, the shape of the nucleus is basically similar, so the model can learn the shape characteristics of the nucleus with little data.
The localisation of nucleus instances can be done by our Fourier guidance module, and thus the method proposed in this paper still has good performance even with a rather small amount of annotations.
The visualization results are shown in Fig.~\ref{fig:comp_weak}.


\begin{table}[tb]
\caption{Performance comparisons of different weak-supervised methods on MoNuSeg and CPM17 datasets.}
\label{tab:comparison_weak}
\begingroup %这一行要加！！！！！！
\renewcommand{\arraystretch}{1.35}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{c|c|cc|cc}
\hline
    \multirow{2}{*}{\makecell{\% keep annotation \\of each image}} & \multirow{2}{*}{Method} & \multicolumn{2}{c|}{MoNuSeg} & \multicolumn{2}{c}{CPM17} \\                                                      
    \cline{3-6}
 &  & \multicolumn{1}{c}{PQ $\uparrow$} & AJI $\uparrow$ & \multicolumn{1}{c}{PQ $\uparrow$} & AJI $\uparrow$ \\ 
 \hline
%  \multirow{2}{*}{10\%}   &   CPN   & \multicolumn{1}{c}{}   &     & \multicolumn{1}{c}{}   &     \\ 
%  &   \textbf{FrGNet (Ours)}   & \multicolumn{1}{c}{}   &     & \multicolumn{1}{c}{}   &     \\ 
% \hline
 \multirow{2}{*}{20\%}   &   CPN   & \multicolumn{1}{c}{13.6}   &  8.6   & \multicolumn{1}{c}{38.4}   &   28.9  \\ 
 &   \textbf{FrGNet (Ours)}   & \multicolumn{1}{c}{\textbf{49.2}}   & \textbf{40.5}   & \multicolumn{1}{c}{\textbf{52.5}}   &   \textbf{43.3}  \\ 
\hline
 \multirow{2}{*}{30\%}   &   CPN   & \multicolumn{1}{c}{21.2}   &  13.5   & \multicolumn{1}{c}{39.8}   &   29.3 \\ 
 &   \textbf{FrGNet (Ours)}   & \multicolumn{1}{c}{\textbf{62.6}}   & \textbf{61.9}   & \multicolumn{1}{c}{\textbf{64.8}}   & \textbf{61.6}     \\  
\hline
 \multirow{2}{*}{40\%}   &   CPN   & \multicolumn{1}{c}{28.5}   &   19.2  & \multicolumn{1}{c}{52.0}   &  42.5   \\ 
 &   \textbf{FrGNet (Ours)}   & \multicolumn{1}{c}{\textbf{63.8}}   &  \textbf{62.7}   & \multicolumn{1}{c}{\textbf{66.9}}   &  \textbf{64.7}   \\ 
\hline
 \multirow{2}{*}{60\%}   &   CPN   & \multicolumn{1}{c}{46.8}   &  37.8   & \multicolumn{1}{c}{63.6}   &  59.2   \\ 
 &   \textbf{FrGNet (Ours)}   & \multicolumn{1}{c}{\textbf{63.9}}   &  \textbf{63.2}   & \multicolumn{1}{c}{\textbf{69.6}}   &  \textbf{68.4}   \\ 

\hline
 \multirow{2}{*}{80\%}   &   CPN   & \multicolumn{1}{c}{56.8}   &  51.2  & \multicolumn{1}{c}{67.5}   &  65.2   \\ 
 &   \textbf{FrGNet (Ours)}   & \multicolumn{1}{c}{\textbf{63.2}}   &   \textbf{62.6}  & \multicolumn{1}{c}{\textbf{70.1}}   &   \textbf{70.1}  \\ 
\hline
 \multirow{2}{*}{100\%}   &   CPN   & \multicolumn{1}{c}{62.7}   &  61.5   & \multicolumn{1}{c}{68.9}   &  69.1  \\ 
 &   \textbf{FrGNet (Ours)}   & \multicolumn{1}{c}{\textbf{65.5}}   &   \textbf{65.6}  & \multicolumn{1}{c}{\textbf{70.7}}   &   \textbf{71.2}  \\ 
\hline
\end{tabular}
}
\endgroup
\end{table}

\begin{figure*}[tb]
\centering
  \includegraphics[width=0.99\linewidth]{comp_weak.pdf}
  \caption{
  Comparisons of different weak-supervised methods on MoNuSeg and CPM17 datasets.
  }
\label{fig:comp_weak}
\end{figure*}

\subsection{Ablation study}
We performed ablation studies on the CPM17 and MoNuSeg datasets to validate the efficacy of the proposed FrGNet method.
All the following experiments were conducted under the setting of fully supervised tasks.

\noindent\textbf{Effectiveness of FG and GILC modules}. Our proposed FG module provides guiding information about the location of the nucleus for the whole framework, and the GILC module enhances the feature representation of the nucleus using instance-level information.
Table~\ref{tab:comparison_ablation1} shows the changes in the performance metrics of the model when the FG and GILC modules are used or not.
From the table, it can be seen that when both FG and GILC modules are not used (\#0), the model does not introduce Fourier bootstrap information at this time, and thus has the worst performance.
After the FG module is used (\#1), the model's performance is improved to some extent thanks to the introduction of the a priori bootstrap information.
And when the FG and GILC modules are used at the same time (\#2), the two modules promote each other and work together to provide the model with optimisation information, which promotes the model to further learn further about the feature representation of the nucleus as well as the distribution of the nuclear location, in which case the model performance is highest.
\begin{table}[tb]
\caption{Ablation studies of FG and GILC modules.}
\label{tab:comparison_ablation1}
\begingroup %这一行要加！！！！！！
\renewcommand{\arraystretch}{1.9}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|cc|cccc|cccc}
\hline
   \multirow{2}{*}{\#} &  \multirow{2}{*}{\makecell{FG\\ Module}} & \multirow{2}{*}{\makecell{GILC\\Module}} & \multicolumn{4}{c|}{MoNuSeg} & \multicolumn{4}{c}{CPM17} \\                                                      
    \cline{4-11}
 &  &  & DQ $\uparrow$ & SQ $\uparrow$ & PQ $\uparrow$ & AJI $\uparrow$ & DQ $\uparrow$ & SQ $\uparrow$ & PQ $\uparrow$ & AJI $\uparrow$ \\ 
 \hline
0 &  \XSolidBrush   &   \XSolidBrush    &  82.2  &  76.2  &  62.7  &  61.5   &  85.9  &  80.1  &  68.9  &  69.1   \\ 
 % \hline
1 &  \Checkmark   &   \XSolidBrush   &  82.7  &  75.9  &  62.8  &  63.0  &  86.9  &  79.9  &  69.5  &  70.5   \\ 
% \hline
% \hline
2 &  \Checkmark   & \Checkmark   &  \textbf{84.8}  &  \textbf{77.2}  &  \textbf{65.5}  &  \textbf{65.6}  &  \textbf{87.5} &  \textbf{80.6} &  \textbf{70.7}  &  \textbf{71.2}    \\ 
\hline
\end{tabular}
}
\endgroup
\end{table}

% \begin{figure*}[tb]
% \centering
%   \includegraphics[width=1\linewidth]{comp_full.pdf}
%   \caption{
%   Effectiveness of FG and GILC modules.
%   }
% \label{fig:effectiveness_FG_GILC}
% \end{figure*}

\noindent\textbf{Type of guide mask}. By generating a fourier guidance mask (as shown in Fig.~\ref{fig:gmask_gen}) is not a binary mask, but a floating-point form of mask.
In concert with the commonly used and effective label smoothing trick~\citep{muller2019does}, we further explored the impact of the supervised form of the guide mask on model performance.
Specifically, the guide mask generated by Fig.~\ref{fig:gmask_gen} is the soft form of guide mask, and the hard guide mask is obtained by binarising the soft guide mask.
After obtaining the soft guide mask and the hard guide mask, we explored the effect of the type of guide mask on the performance of the model, and the results are shown in Table ~\ref{tab:comparison_ablation2}.
From this table, it can be seen that when no guide mask is used, the model performs poorly (\#0) due to the missing guide information.
When the guide mask is used, the model performance is improved (\#1\&2).
At the same time, the use of soft form of guide mask has the greatest improvement in the performance of the model, this is because soft form of guide mask not only brings a priori guidance information, but also brings feature information to the model that cannot be brought by the hard form, and these feature information makes the model more robust.
\begin{table}[tb]
\caption{Ablation studies on the type of guide mask.}
\label{tab:comparison_ablation2}
\begingroup %这一行要加！！！！！！
\renewcommand{\arraystretch}{1.9}
\resizebox{1\linewidth}{!}{
\begin{tabular}{c|cc|cccc|cccc}
\hline
   \multirow{2}{*}{\#} &  \multirow{2}{*}{\makecell{Hard\\ Guide}} & \multirow{2}{*}{\makecell{Soft\\Guide}} & \multicolumn{4}{c|}{MoNuSeg} & \multicolumn{4}{c}{CPM17} \\                                                      
    \cline{4-11}
 &  &  & DQ $\uparrow$ & SQ $\uparrow$ & PQ $\uparrow$ & AJI $\uparrow$ & DQ $\uparrow$ & SQ $\uparrow$ & PQ $\uparrow$ & AJI $\uparrow$ \\ 
 \hline
0 &  \XSolidBrush   &   \XSolidBrush    &  80.7  &  \textbf{77.8}  &  62.9  &  61.7  &  86.8  &  80.2  &  69.7  &  70.3   \\ 
 % \hline
1 &  \Checkmark   &   \XSolidBrush   &  84.5  &  77.4  &  65.5  &  65.1   &  87.0  &  80.2  &  69.9  &   70.2   \\ 
% \hline
% \hline
2 &  \XSolidBrush   & \Checkmark   &  \textbf{84.8}  &  77.2  &  \textbf{65.5}  &  \textbf{65.6}  &  \textbf{87.5} &  \textbf{80.6} &  \textbf{70.7}  &  \textbf{71.2}    \\ 
\hline
\end{tabular}
}
\endgroup
\end{table}

% \begin{figure*}[tb]
% \centering
%   \includegraphics[width=1\linewidth]{comp_full.pdf}
%   \caption{
%   Type of guide mask.
%   }
% \label{fig:type_guide_mask}
% \end{figure*}

\subsection{Generalization}
As shown in Table~\ref{tab:comparison_weak}, our proposed method is able to keep stable performance when data annotations decreasing.
In order to further explore the generalization ability of FrGNet, we use a private dataset.
This dataset consist of 3100 histopathologic images, without any instance annotations.
Specially, we initial our FrGNet model with the model weight that training on the MoNuSeg dataset in full-supervised setting.
Then, we fine-tune this model 5 epoches, with freeze backbone and other heads except \textbf{Fourier Guide Head}.
Finally, we use this fine-tuned FrGNet to do inference on this private dataset.
The visualization results are shown in Fig.~\ref{fig:genera}.
\textbf{Without any annotations}, our proposed FrGNet can generate more complete nuclear segmentation.
On the one hand, our algorithm can segment the nuclear nicely no matter they are densely or sparsely distributed.
On the other hand, we can make full use of the guide mask to filter the segmentation results, which effectively mitigates the problem of false positives, and at the same time, the guide mask shows the possible locations of the nuclear in the model output, which strengthens the interpretability of the model.

\begin{figure*}[tb]
\centering
  \includegraphics[width=0.93\linewidth]{generalization.pdf}
  \caption{
  Generalization results on private dataset.
  }
\label{fig:genera}
\end{figure*}

% \input{5_dis}
\section{Limitations and discussion}

In this work, we present a framework for segmenting nuclear instances based on fourier guidance.
The framework incorporates the feature information of the nuclear image itself, and injects the a priori information of the nuclear location into the model by using a constructed fourier guidance (FG) module to guide the model for training.
To further improve the model performance, we propose the guide-based instance level contrastive (GILC) module, which provides instance-level feature guidance for the model and enhances the feature representation of the nuclear instances.
Through comparison and ablation experiments, the effectiveness of our framework and the proposed modules are demonstrated to reach a new SOTA in performance.
Meanwhile, it is demonstrated through experiments that thanks to the construction of fourier guidance information, our framework is able to solve both fully supervised and weakly supervised nuclear instance segmentation problems.
Besides, our model possesses good generalisation.
By working on a private nuclear pathology dataset without any supervised annotations, our model is still able to perform the task of instance segmentation of nuclear well in data that has not been seen at the time of training.

Nuclear in pathology images are generally difficult to annotate.
How to construct a completely unsupervised framework for segmenting nuclear instances is a task of its own interest, and solving the problem will largely contribute to the development of nuclear segmentation in the field of pathology.
The framework proposed in this paper, although possessing a certain degree of generalisation, still needs to rely on a pre-trained model on fully supervised dataset, and is unable to accomplish the task of unsupervised nuclear instance segmentation in the true sense.
Recently the field of unsupervised segmentation has gained some development~\citep{sun2024clip, niu2024unsupervised}, which is of some guidance for unsupervised nuclear instance segmentation.



% \section*{Acknowledgments}
% This work was supported by the National Natural Sciences Grants China (project number: 82273491), the GuangDong Basic and Applied Basic Research Foundation (project number: 2022A1515012059), the 2022 Major Science and Technology Innovation R\&D Project of Chongqing Municipality (project number: CSTB2022TIAD-STX0008), the Chongqing science and health joint medical research project (project number: 2024QNXM062).

% Acknowledgments should be inserted at the end of the paper, before the
% references, not as a footnote to the title. Use the unnumbered
% Acknowledgements Head style for the Acknowledgments heading.

% \section*{References}

% Please ensure that every reference cited in the text is also present in
% the reference list (and vice versa).

% \section*{\itshape Reference style}

% Text: All citations in the text should refer to:
% \begin{enumerate}
% \item Single author: the author's name (without initials, unless there
% is ambiguity) and the year of publication;
% \item Two authors: both authors' names and the year of publication;
% \item Three or more authors: first author's name followed by `et al.'
% and the year of publication.
% \end{enumerate}
% Citations may be made directly (or parenthetically). Groups of
% references should be listed first alphabetically, then chronologically.

%%Harvard
\bibliographystyle{model2-names.bst}\biboptions{authoryear}
\bibliography{refs}

% \section*{Supplementary Material}

% Supplementary material that may be helpful in the review process should
% be prepared and provided as a separate electronic file. That file can
% then be transformed into PDF format and submitted along with the
% manuscript and graphic files to the appropriate editorial office.

\end{document}

%%
