\section{Related work}
\label{sec-relatedwork}

\subsection{Architecture and training of LVLMs}\label{subsec-training}
Large Vision Language Models (LVLMs) have commonly adopted an architecture that connects pre-trained vision and language backbones through a lightweight projector. In this architecture, the visual encoder extracts high-dimensional visual tokens (often using a CLIP-based model), which are then projected into the language model's embedding space for unified processing. Notable examples following this approach include BLIP-2~\cite{li2023blip}, InstructBLIP~\cite{dai2023instructblip}, LLaVA~\cite{liu2024visual, liu2024improved}, and MiniGPT-4~\cite{zhu2023minigpt}.

Recent studies suggest that hallucination patterns in LVLM outputs are model-specific and closely related to both architectural choices and the quality and scale of the training data~\cite{liu2024survey, gunjal2024detecting}. Since LLaVA-1.5-7B~\cite{liu2024improved} embodies current mainstream practices in LVLM development, including a fully auto-regressive architecture and a representative training methodology, we adopt it as our base model in this study.

The model consists of three key components: CLIP ViT-L/14~\cite{radford2021learning} as the vision encoder, Vicuna-7B~\cite{vicuna2023} as the language model, and a multi-layer perceptron (MLP) projector. These components are trained through a two-stage process:

\begin{itemize}
    \item \textbf{Stage 1 (pre-training):} The projector is trained on 595K image-text pairs to align visual and textual embeddings.
    \item \textbf{Stage 2 (fine-tuning):} The model is fine-tuned on 158K instruction-following data and ScienceQA, a multi-modal scientific QA dataset. Task-oriented samples such as image captioning and visual question answering (VQA) are used to enhance its instruction-following capabilities.
\end{itemize}

Both the pre-training and fine-tuning datasets are relatively small, making the training process efficient. However, this limited size may affect the model's ability to generalize well across diverse tasks.

\subsection{Mitigating hallucinations in LVLMs}\label{subsec-mitigating}
Methods to mitigate hallucinations in LVLMs can be broadly categorized into training-based and training-free methods. 

Training-based methods typically involve collecting fine-grained feedback data, either through manual annotation or LLM-assisted recaptioning frameworks. This data is then used to fine-tune the model using Reinforcement Learning from Human Feedback (RLHF), which can effectively reduce hallucinations but often comes with high costs in data collection and model training. 

Training-free methods do not involve fine-tuning the LVLM and can be divided into two categories: inference-time methods and post-hoc frameworks. OPERA and VCD represent inference-time methods that modify token distributions during generation, while LURE and Woodpecker exemplify post-hoc frameworks that either train a revisor using hallucinated data or employ visual grounding tools for output validation.

\subsubsection{Inference-time methods}\label{subsubsec-inference}
\textbf{Over-trust Penalty with Retrospection-Allocation (OPERA)}~\cite{huang2024opera} is an inference-time method designed to address the issue of models over-relying on specific \textquotedblleft anchor tokens\textquotedblright{} during generation, which often leads to hallucinations. It first applies a penalty score based on local attention aggregation patterns. During beam search, this penalty dynamically adjusts token selection, encouraging a more balanced attention distribution across image and text tokens. In cases where the over-trust penalty fails to prevent aggregation, retrospection-allocation strategy actively redirects generation by selecting alternative tokens when the model exhibits repetitive focus on specific anchor tokens.


\textbf{Visual Contrastive Decoding (VCD)}~\cite{leng2024mitigating} mitigates object hallucinations by addressing training data biases and over-reliance on language priors. The method contrasts outputs from original and noise-distorted visual inputs to adjust the model's output distribution. By favoring high-confidence tokens from original inputs, VCD generates more accurate outputs while maintaining efficiency.

Although OPERA and VCD are effective decoding methods, both approaches require extensive manual parameter tuning. OPERA needs predetermined penalty coefficients that are difficult to optimize across different scenarios, and VCD requires careful calibration of token-specific contrast parameters to balance hallucination rate and fluency. Our approach addresses these limitations through a supervised token-level classifier that automatically learns model-specific hallucination patterns, eliminating the need for manual parameter tuning and providing intuitive control over hallucination rates during decoding.

\subsubsection{Post-hoc frameworks}\label{subsubsec-posthoc}
\textbf{LVLM Hallucination Revisor (LURE)} ~\cite{zhou2023analyzing} rectifies object hallucinations in LVLMs by addressing three primary causes of hallucination: co-occurrence biases in training data, uncertainty in object predictions, and errors accounting later parts of generated descriptions. To train the revisor, LURE generates hallucinatory training data by modifying accurate captions using GPT-3.5, introducing co-occurring objects and replacing uncertain or late-positioned objects with placeholder tags. The revisor model is trained on this dataset to learn how to correct hallucinatory outputs into the accurate description. Once trained, the revisor can seamlessly integrate with any LVLM to correct hallucinations. 

\textbf{Woodpecker}~\cite{yin2024woodpecker} identifies key concepts in generated text and formulates diagnostic questions using prompt engineering to detect inconsistencies, covering both object-level and attribute-level details. These diagnostic questions are answered using object detection tools and bounding boxes, which validate image content and produce structured visual claims. The visual claims are then used to guide the correction process, refining the generated text to better align with the image. This structured and interpretable approach improves the accuracy and reliability of the LVLM output.

Compared with post-hoc frameworks such as LURE and Woodpecker, our approach avoids the need for manual hallucination pattern identification or designing hallucinated datasets. Instead, we employ a model-specific classifier that automatically learns these patterns. Our method operates independently during decoding without relying on external tools for caption validation, enabling real-time control. However, note that integrating our method with other LVLMs requires retraining of the classifier.

\subsection{Evaluating hallucinations in LVLMs}\label{subsec-evaluating}
Recent work has also developed various approaches to evaluate hallucinations in LVLMs through discriminative assessment methods. \textbf{Polling-based Object Probing Evaluation (POPE)}\cite{li2023evaluating} implements a binary classification framework where models provide "Yes/No" responses to object existence queries. This approach avoids complex parsing of generated captions and ensures stable and fair evaluations. \textbf{Recognition-based Object Probing Evaluation (ROPE)}\cite{chen2024multi} addresses multi-object hallucinations using bounding box prompts, examining factors such as object salience and class distribution that influence hallucination occurrence. They note that decoding strategies such as OPERA show limited effectiveness in diverse multi-object scenarios, sometimes even lowering performance.

\textbf{AMBER}~\cite{wang2023llm} proposes to evaluate generative and discriminative tasks as two distinct aspects of LVLM's performance. The method assesses discriminative performance through structured prompts that test different hallucination types. Additionally, it evaluates generative performance by comparing model outputs with ground-truth annotations of objects, attributes, and relationships.

Free-form text evaluation presents an essential challenge in hallucination detection. The \textbf{Caption Hallucination Assessment with Image Relevance (CHAIR)}\cite{rohrbach2018object} metric evaluates object hallucinations but requires object segmentation annotations and semantic parsing to map detected nouns to a predefined object vocabulary. \textbf{HalluBench}\cite{zhao2023beyond} addresses these constraints using GPT-4 to evaluate hallucinations against detailed descriptions at the object level, allowing evaluation of objects, attributes and relationships.

The dependence on ground-truth annotations and the challenge in free-form text processing create a shared bottleneck for both evaluation and mitigation methods. The ability to evaluate hallucination levels in captions without reference data would facilitate the reduction of hallucinations in image captioning, thus enabling the collection of high-quality hallucination-free training data.