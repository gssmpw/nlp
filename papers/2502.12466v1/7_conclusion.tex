This paper presents \name{}, a dataset for evaluating the code reasoning capabilities of large language models via program equivalence checking. Spanning four programming languages and six equivalence categories, \name{} challenges models with diverse (in)equivalent program pairs generated through automated transformations, including syntactic changes, structural modifications, and algorithmic equivalence. Our evaluation shows that the best-performing model, OpenAI o3-mini, achieves only \sotalowacc{} in the CUDA category and \sotaacc{} overall, with the most challenging categories achieving the best accuracies of just \sotacuda{} and \sotadce{}, only modestly above the 50\% random baseline. Few-shot learning and Chain-of-Thought prompting yield minimal gains, and models exhibit bias toward classifying programs with significant transformations as inequivalent. \name{} provides a critical benchmark for advancing LLM-based code reasoning.