\subsection{Model Accuracy}
\label{subsec:acc}

\Cref{tab:acc} shows the accuracy results for \numllm state-of-the-art large language models on \name under zero-shot prompting. Our findings are as follows:

\paragraph{Reasoning models achieve the highest performance, demonstrating a clear advantage over non-reasoning models.} As shown in \Cref{tab:acc}, reasoning models such as OpenAI o3-mini, DeepSeek R1, and o1-mini significantly outperform all others in our evaluation. This further underscores the complexity of equivalence checking as a code reasoning problem, where reasoning models exhibit a distinct advantage.


\paragraph{\name is a challenging benchmark.} Among the \numllm models evaluated, OpenAI o3-mini achieves only \sotalowacc{} in the CUDA category despite being the top-performing model overall, with an accuracy of \sotaacc{}. For the two most difficult categories, the highest accuracy across all models is \sotacuda{} and \sotadce{}, respectively, only modestly above the random baseline of 50\% accuracy for binary classification, highlighting the substantial room for improvement.

\paragraph{Pure syntactic changes (\ojv) are the easiest for LLMs, while structural transformations are key to assessing deep semantic reasoning.} As shown in the last row of \Cref{tab:acc}, the \ojv category achieves the highest mean accuracy, with DeepSeek-R1 leading at 91.5\%. This is because \ojv pairs are generated through trivial variable renaming, as seen in prior work~\cite{badihi2021eqbench,maveli2024can}. Additionally, combining variable renaming with algorithmic equivalence has little impact on difficulty, as indicated by the small drop in mean accuracy from \oja 67.3\% to \ojva 67.0\%. In contrast, all other categories involve non-local structural transformations, making them more challenging and essential for evaluating LLMs' deep semantic reasoning.

\paragraph{Scaling up models improves performance.} Larger models generally achieve better performance. \Cref{fig:scaling} shows scaling trends for the Qwen2.5, Llama-3.1, and Mixtral families, where accuracy improves with model size. The x-axis is on a logarithmic scale, highlighting how models exhibit consistent gains as parameters increase.


\subsection{Prompting Strategies Analysis}
\label{subsec:prompt}

We study few-shot in-context learning and Chain-of-Thought (CoT) prompting, evaluating four strategies: 0-shot, 4-shot, 0-shot with CoT, and 4-shot with CoT. For 4-shot, prompts include 2 equivalent and 2 inequivalent pairs. \Cref{subsec:app:prompt} details the prompts, and \Cref{tab:prompt} shows the results.

Our key finding is that \textbf{prompting strategies \emph{barely} improve performance on \name}, highlighting the task's difficulty and need for deeper reasoning. Few-shot prompting provides only minor improvements over 0-shot, while Chain-of-Thought shows slight benefits for o1-mini but marginally reduces performance for other models, underscoring the task’s complexity and the need for more advanced, task-specific approaches.



\begin{table}[!tb]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{0S} & \textbf{4S} & \textbf{0S-CoT} & \textbf{4S-CoT} \\
\midrule
o1-mini & 71.5 & 71.5 & \textbf{71.9} & \textbf{71.9} \\
gpt-4o & 65.0 & \textbf{66.5} & 62.5 & 62.7 \\
DeepSeek-V3 & 65.0 & \textbf{66.9} & 63.3 & 62.5 \\
gpt-4o-mini & 62.2 & \textbf{63.5} & 60.2 & 61.2 \\
\bottomrule
\end{tabular}
\caption{\textbf{Accuracies of different prompting techniques.} We evaluate 0-shot and 4-shot in-context learning, both without and with Chain-of-Thought (CoT). Prompting strategies barely improve performance, highlighting the task’s difficulty and the need for task-specific approaches.}
\label{tab:prompt}
\end{table}

\subsection{Bias in Model Prediction}
\label{subsec:bias}

We evaluate the prediction bias of the models and observe \textbf{a pronounced tendency to misclassify equivalent programs as inequivalent in the \cuda and \ass categories}. \Cref{tab:bias} presents the results for four representative models, showing high accuracy for inequivalent pairs but significantly lower accuracy for equivalent pairs, with full results for all models in \Cref{subsec:app:bias}.

The bias in the \cuda category arises from extensive structural transformations, such as loop restructuring and shared memory optimizations, which make paired programs appear substantially different. In the \ass category, superoptimization applies non-local transformations to achieve optimal instruction sequences, introducing aggressive code restructuring that complicates equivalence reasoning and leads models to frequently misclassify equivalent pairs as inequivalent.

\subsection{Case Studies}
\label{subsec:case}

\begin{table}[!tb]
    \centering
\small
\begin{tabular}{lcccc}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{2}{c}{\textbf{\cuda}} & \multicolumn{2}{c}{\textbf{\ass}} \\
\cmidrule(lr){2-5}
 & \textbf{Eq} & \textbf{Ineq} & \textbf{Eq} & \textbf{Ineq} \\
\midrule
\textit{Random Baseline} & \textit{50.0} & \textit{50.0} & \textit{50.0} & \textit{50.0} \\
o3-mini & 27.5 & 90.5 & 69.5 & 99.5 \\
o1-mini & 2.5 & 99.0 & 50.0 & 98.5 \\
DeepSeek-R1 & 28.0 & 94.0 & 57.5 & 99.0 \\
DeepSeek-V3 & 8.5 & 93.0 & 44.0 & 94.5 \\
\bottomrule
\end{tabular}
    \caption{Accuracies on equivalent and inequivalent pairs in the \cuda and \ass categories under 0-shot prompting, showing that \textbf{models perform significantly better on inequivalent pairs}. Random guessing serves as an unbiased baseline for comparison. Full results for all models are shown in \Cref{subsec:app:bias}.}
\label{tab:bias}
\end{table}

\paragraph{Models lack capabilities for sound equivalence checking.} We find that simple changes that lead to semantic differences can confuse the models, causing them to produce incorrect predictions despite their correct predictions on the original program pairs. For example, o3-mini, which is one of the top-performing models in \cuda category, can correctly classifies the pair shown in \Cref{fig:cuda} as equivalent. Next, we introduce synchronization bugs into the right-hand program, creating two inequivalent pairs with the original left-hand program: (1) removing the first \CodeIn{\_\_syncthreads();} allows reads before all writes complete, causing race conditions; (2) removing the second \CodeIn{\_\_syncthreads();} lets faster threads overwrite shared data while slower threads read it. Despite these semantic differences, o3-mini misclassifies both pairs as equivalent.

\paragraph{Proper hints enable models to correct misjudgments.} After o3-mini misclassifies the modified pairs, a hint about removed synchronization primitives allows it to correctly identify both as inequivalent, with accurate explanations highlighting data races. This suggests that training models on dedicated program analysis datasets, beyond only raw source code, may be useful for improving their code reasoning capabilities.