\paragraph{LLM Reasoning} Extensive research has evaluated LLMs' reasoning capabilities across diverse tasks~\cite{cobbe2021training,huang2022towards,bubeck2023sparks,mirzadeh2024gsm,zhou2022least,ho2022large,wei2022chain,chen2024large,clark2018think,zhang2024transformer}. In the context of code reasoning, i.e., predicting a program's execution behavior without running it, CRUXEval~\cite{gu2024cruxeval} focuses on input-output prediction, while CodeMind~\cite{liu2024codemind} extends evaluation to natural language specifications. Another line of work seeks to improve LLMs' code simulation abilities through prompting~\cite{la2024code} or targeted training~\cite{liu2023code,ni2024next,ding2024semcoder}. Unlike prior work that evaluates LLMs on predicting program behavior for a specific input, our new code reasoning task and benchmark for equivalence checking assesses LLMs' ability to reason about all possible inputs.

\paragraph{Equivalence Checking} Equivalence checking underpins applications such as performance optimization~\cite{shypula2023learning,cummins2023large,cummins2024meta}, code transpilation~\cite{lu2021codexglue,yang2024exploring,ibrahimzada2024repository,pan2024lost}, refactoring~\cite{pailoor2024semantic}, and testing~\cite{felsing2014automating,tian2024large}. Due to its undecidable nature, no algorithm can decide program equivalence for all program pairs while always terminating. Existing techniques~\cite{sharma2013data,dahiya2017black,gupta2018effective,mora2018client,churchill2019semantic,badihi2020ardiff} focus on specific domains, such as SQL query equivalence~\cite{zhao2023llm,ding2023proving,singh2024exploring}. EQBENCH~\cite{badihi2021eqbench} and SeqCoBench~\cite{maveli2024can} are the main datasets for equivalence checking but have limitations. EQBENCH is too small (272 pairs) for LLM evaluation, while SeqCoBench relies only on statement-level syntactic changes (e.g., 
% replacing \CodeIn{+} with \CodeIn{-} or 
renaming variables). In contrast, our work introduces a broader set of equivalence categories and structural transformations, creating a more systematic and challenging benchmark for assessing LLMs' semantic reasoning capabilities.
