Programming has emerged as a key application domain for large language models (LLMs), enabling tasks such as program synthesis~\cite{chen2021evaluating,austin2021program,jain2024livecodebench}, test generation~\cite{yang2024whitefox}, bug detection~\cite{yang2023kernelgpt}, program repair~\cite{xia2023automated}, and code optimization~\cite{shypula2023learning}. Recently, there has been growing interest in evaluating how well LLMs can reason about the semantics of code~\cite{ni2024next,liu2023code,gu2024cruxeval,chen2024reasoning,liu2024codemind}, i.e., predicting program properties without running the program.

\begin{figure}[!tb]
\centering
\includegraphics[width=\columnwidth]{figure/syntactic.pdf}
\caption{\textbf{An equivalent and an inequivalent program pair constructed using prior techniques.} Prior works generate such pairs through \emph{basic statement-level syntactic modifications} with minimal semantic reasoning, whereas our approach, presented later, relies on structural program transformations that require much deeper semantic reasoning.}
\label{fig:syntactic}
\end{figure}

This paper introduces the task of \textbf{equivalence checking} as a new way to evaluate the code reasoning capabilities of LLMs. A classic challenge in programming languages and verification, equivalence checking involves determining whether two programs produce identical outputs for all possible inputs. \Cref{fig:syntactic} presents examples of equivalent and inequivalent program pairs.


Compared to prior code reasoning tasks, evaluating LLMs using equivalence checking offers distinct advantages. Most notably, it presents a significantly more challenging benchmark than previous tasks, enabling a more rigorous assessment of LLMs' code reasoning capabilities. Equivalence checking requires LLMs to reason over \emph{all possible inputs}, while prior work often focuses on \emph{a single input}, such as output prediction, input prediction~\cite{gu2024cruxeval}, input-specific program state prediction and execution simulation~\cite{liu2023code,chen2024reasoning,ding2024semcoder,la2024code,ni2024next}.

Moveover, equivalence checking underpins a broad range of downstream applications, including software refactoring~\cite{pailoor2024semantic}, software testing~\cite{tian2024large}, and program optimization~\cite{shypula2021learning}, surpassing the scope of prior reasoning tasks. By requiring a deep understanding of program semantics and reasoning over all possible inputs, equivalence checking enables the analysis of an expressive range of program behaviors, even including many undecidable problems. Therefore, LLMs that perform well on equivalence checking are likely to be well-suited for tackling more complex programming tasks.

Our proposal requires a benchmark consisting of both equivalent and inequivalent program pairs covering different aspects of equivalence reasoning with varying degrees of difficulty. A large benchmark is essential, making it desirable to automate the benchmark generation process. Existing methods~\cite{badihi2021eqbench,maveli2024can} mostly rely on \emph{local syntactic changes} such as operand swaps (e.g., changing \CodeIn{a < b} to \CodeIn{b > a} for equivalent pairs or \CodeIn{b <= a} for inequivalent pairs; see \Cref{fig:syntactic}), which do not require deep semantic reasoning. However, these approaches are insufficient for benchmarking the equivalence reasoning capabilities of state-of-the-art LLMs. As many existing benchmarks have become saturated~\cite{phan2025humanity}, a more challenging dataset is needed to rigorously assess LLMs' semantic reasoning abilities.

In this work, we introduce \textbf{\name{}}, a new dataset of \numpair program pairs for equivalence reasoning. \name{} spans four programming languages—Python, C, CUDA, and x86-64 assembly—providing a systematic benchmark to evaluate LLMs' code reasoning abilities.

The key technical challenge is to automatically generate (in)equivalent program pairs that demand \emph{deep semantic reasoning beyond simple syntactic variations}. We propose several techniques to achieve this. First, to confirm that basic syntactic variations are well within the reasoning capabilities of state-of-the-art LLMs, we construct an equivalence category based on variable renaming, which barely requires semantic reasoning. Next, we generate equivalent programs by removing dead code, leveraging program analysis to go beyond trivial syntactic changes. By incorporating alias analysis and path feasibility analysis, we increase the difficulty of semantic reasoning in an automated manner. For GPU programs written in CUDA, we generate equivalent pairs by exploring different compiler scheduling strategies, such as loop tiling and shared memory caching, which involve structural transformations that extend far beyond statement-level modifications. We also use \emph{superoptimization} to explore optimal instruction sequences beyond standard compiler optimizations, enabling more aggressive code restructuring. Finally, we include pairs with different algorithmic choices using submissions from online programming platforms.

Our experiments show that \name{} is a challenging benchmark for LLM-based equivalence checking. Among the \numllm models evaluated, OpenAI o3-mini performs best overall, yet achieves only \sotalowacc{} in the CUDA category despite achieving the highest overall accuracy of \sotaacc{}. For the two most difficult categories, the best accuracy across all models is \sotacuda{} and \sotadce{}, respectively.  These numbers are only \emph{modestly better than the random baseline}—i.e., 50\% accuracy for binary classification. Further analysis shows that variable renaming, a purely syntactic modification, is the easiest equivalence category for models, with accuracy as high as \sotaojv{}. We also find that models are \emph{biased} toward classifying programs with significant structural, non-local transformations as inequivalent. Moreover, prompting strategies such as few-shot in-context learning and Chain-of-Thought (CoT) prompting \emph{barely} enhance LLMs' semantic reasoning capabilities in equivalence checking, underscoring the fundamental difficulty of the task.

In summary, our contributions are as follows:


\begin{itemize}
    \item \textbf{New Task and Dataset:} We introduce equivalence checking as a new task to assess LLMs' code reasoning capabilities. We present \textit{\name{}}, a benchmark for semantic equivalence checking spanning four languages and six equivalence categories.
    
    \item \textbf{Automated Generation:} We develop a fully automated pipeline to construct diverse (in)equivalent program pairs, using techniques from program analysis, compiler scheduling, and superoptimization. The pipeline covers transformations including syntactic changes, structural modifications, and algorithmic equivalence.

    \item \textbf{Evaluation and Analysis:} We evaluate \numllm state-of-the-art models on \name, with the highest overall accuracy reaching \sotaacc{}. In the two most challenging categories, the best accuracy across all models is \sotacuda{} and \sotadce{}, indicating significant room for improvement. Additionally, we analyze performance across different equivalence categories and prompting strategies.
\end{itemize}
