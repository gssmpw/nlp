\paragraph{\name.} Our dataset, \name, consists of 2,400 program pairs across six equivalence categories. Each category contains 200 equivalent and 200 inequivalent pairs. \Cref{tab:dataset} summarizes the lines of code, including the minimum, maximum, and average, for programs in each category, reflecting the wide variation in program lengths. As the dataset generation pipeline is fully automated, additional pairs can be generated as needed.

\begin{table}[h!]
    \small
    \centering
    \begin{tabular}{l l c c c c}
        \toprule
        \multirow{2}{*}{Category} & \multirow{2}{*}{Language} & \multirow{2}{*}{\# Pairs} & \multicolumn{3}{c}{Lines of Code} \\
        \cmidrule(lr){4-6}
        & & & Min & Max & Avg. \\
        \midrule
        \dce & C & 400 & 98 & 880 & 541 \\
        \cuda & CUDA & 400 & 46 & 1733  & 437  \\
        \ass & x86-64 & 400 & 8  & 29  & 14  \\
        \oja & Python & 400 & 3 & 3403  & 82 \\
        \ojv & Python & 400 & 2 & 4087 &  70 \\
        \ojva & Python & 400 & 3 & 744  & 35 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Statistics of the \name{} dataset.}}
    \label{tab:dataset}
\end{table}


\paragraph{Research Questions.} We investigate: 1) how different models perform on equivalence checking (\Cref{subsec:acc}); 2) whether prompting techniques, such as few-shot learning~\cite{brown2020language} and Chain-of-Thought~\cite{wei2022chain}, can enhance performance (\Cref{subsec:prompt}); and 3) whether model predictions exhibit bias when judging program equivalence.

\paragraph{Models.} We evaluate \numllm large language models. For open-source models, including Mixtral~\cite{jiang2024mixtral}, Llama~\cite{touvron2023llama}, Qwen~\cite{bai2023qwen}, DeepSeek~\cite{liu2024deepseek}, we use Together AI, a model serving framework. For closed-source models (e.g., GPT-4~\cite{achiam2023gpt}, Claude-3.5~\cite{Anthropic}), we access them via their official APIs, using the default temperature setting.

\paragraph{Prompts.} The 0-shot evaluation is conducted using the prompt ``You are here to judge if two programs are semantically equivalent. Here equivalence means \{{\em definition}\}. [Program 1]: \{code1\} [Program 2]: \{code2\} Please only output the answer of whether the two programs are equivalent or not. You should only output Yes or No.'' The definition of equivalence and the corresponding program pairs are provided for each category. Additionally, for the categories of \oja, \ojv and \ojva, the prompt also includes the problem description. The full prompts used in our experiments for each equivalence category are in \Cref{subsec:app:prompt}.


\begin{table*}[!tb]
        \small
        \centering
\begin{tabular}{lccccccc}
\toprule
\textbf{Model} & \textbf{DCE} & \textbf{\cuda} & \textbf{\ass} & \textbf{\oja} & \textbf{\ojv} & \textbf{\ojva} & \textbf{Overall Accuracy} \\
\midrule
\textit{Random Baseline} & \textit{50.0} & \textit{50.0} & \textit{50.0} & \textit{50.0} & \textit{50.0} & \textit{50.0} & \textit{50.0} \\
Llama-3.2-3B-Instruct-Turbo & 50.0 & 49.8 & 50.0 & 51.5 & 51.5 & 51.5 & 50.7 \\
Llama-3.1-8B-Instruct-Turbo & 41.8 & 49.8 & 50.5 & 57.5 & 75.5 & 56.8 & 55.3 \\
Mistral-7B-Instruct-v0.3 & 51.0 & 57.2 & 73.8 & 50.7 & 50.5 & 50.2 & 55.6 \\
Mixtral-8x7B-Instruct-v0.1 & 50.2 & 47.0 & 64.2 & 59.0 & 61.5 & 55.0 & 56.1 \\
Mixtral-8x22B-Instruct-v0.1 & 46.8 & 49.0 & 62.7 & 63.5 & 76.0 & 62.7 & 60.1 \\
Llama-3.1-70B-Instruct-Turbo & 47.5 & 50.0 & 58.5 & 66.2 & 72.0 & 67.5 & 60.3 \\
QwQ-32B-Preview & 48.2 & 50.5 & 62.7 & 65.2 & 71.2 & 64.2 & 60.3 \\
Qwen2.5-7B-Instruct-Turbo & 50.5 & 49.2 & 58.0 & 62.0 & 80.8 & 63.0 & 60.6 \\
gpt-4o-mini-2024-07-18 & 46.8 & 50.2 & 56.8 & 64.5 & 91.2 & 64.0 & 62.2 \\
Qwen2.5-72B-Instruct-Turbo & 42.8 & 56.0 & 64.8 & 72.0 & 76.5 & 70.8 & 63.8 \\
Llama-3.1-405B-Instruct-Turbo & 40.0 & 49.0 & 75.0 & 72.2 & 74.5 & 72.8 & 63.9 \\
DeepSeek-V3 & 41.0 & 50.7 & 69.2 & 73.0 & 83.5 & 72.5 & 65.0 \\
gpt-4o-2024-11-20 & 43.2 & 49.5 & 65.2 & 71.0 & 87.0 & 73.8 & 65.0 \\
claude3.5-sonnet-2024-10-22 & 38.5 & \textbf{62.3} & 70.0 & 71.2 & 78.0 & 73.5 & 65.6 \\
o1-mini-2024-09-12 & 55.8 & 50.7 & 74.2 & 80.0 & 89.8 & 78.8 & 71.5 \\
DeepSeek-R1 & 52.2 & 61.0 & 78.2 & 79.8 & \textbf{91.5} & 78.0 & 73.5 \\
o3-mini-2025-01-31 & \textbf{68.8} & 59.0 & \textbf{84.5} & \textbf{84.2} & 88.2 & \textbf{83.2} & \textbf{78.0} \\
\midrule
Mean & 47.9 & 52.4 & 65.8 & 67.3 & 76.4 & 67.0 & 62.8 \\
\bottomrule
\end{tabular}
    \caption{\textbf{Accuracy of \numllm models on \name{} under 0-shot prompting.} We report accuracy for each of the six equivalence categories along with the overall accuracy.}
    \label{tab:acc}
\end{table*}


\paragraph{Error Handling.} Some models occasionally fail to follow the instruction to ``output Yes or No''. To address this issue, we use GPT-4o to parse model outputs. In cases where no result can be extracted, we randomly assign ``Yes'' or ``No'' as the model's output. These errors are very rare in advanced models but occur more frequently in smaller models.


\begin{figure}[!tb]
    \centering
    \includegraphics[width=\columnwidth]{figure/scaling.pdf}
    \caption{\textbf{Scaling Trend on \name.}}
    \label{fig:scaling}
\end{figure}