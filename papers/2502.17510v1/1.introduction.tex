%Large language models (LLMs) have recently achieved remarkable advancements, demonstrating unparalleled performance across various NLP tasks [xx]. 
Incorporating continual learning (CL) capability into large language models (LLMs) is essential for enabling them to acquire knowledge from diverse tasks sequentially, a critical requirement for adapting to ever-changing environments without extensive retraining \cite{wang2024comprehensive, jiang2024interpretable, yu2024recent, chang2024survey}.  
An effective CL system must address two key challenges: (1) Catastrophic Forgetting (CF)~\cite{mccloskey1989catastrophic}, where previously acquired knowledge is lost when learning new tasks, and (2) Knowledge Transfer (KT)~\cite{ke2021achieving}, which involves leveraging new, related tasks to improve performance on prior tasks, and vice versa.


%which involves leveraging new, related tasks to enhance performance on prior tasks and vice versa.
%Striking a balance between retaining previous knowledge and excelling in new tasks is vital for success.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{imgs/Intro2.pdf}
  \caption{
  Conceptual illustration of {\ouralg}. Inspired by the CLS theory, {\ouralg} iteratively employs an inner learner to localize new knowledge and an outer learner to manage the global fusion of knowledge.
  %integrates an inner learner for rapid adaptation to new knowledge and an outer learner for global control of knowledge fusion.
  }
  \label{fig:intro}
\end{figure}

Recently, model mixture-based methods have emerged as a mainstream approach for CL in LLMs \cite{chen2023lifelong, wu2024f, rypesc2024divide}. 
By leveraging parameter-efficient fine-tuning (PEFT) techniques, which reduce the computational burden, these methods can be broadly classified into two categories: model ensemble and model merging.
%Model ensemble methods, representing dynamic-architecture approaches, typically assign a dedicated PEFT block for each task to capture task-specific knowledge. These blocks are stored in a PEFT pool and dynamically selected during inference based on the incoming test sample \cite{wang2023orthogonal, wang2024rehearsal, zhao2024sapt}.
Model ensemble methods assign a dedicated PEFT block to each task, capturing task-specific knowledge, which is then stored in a pool and dynamically selected during inference \cite{zhu2024llama, wang2024rehearsal}.
%For instance, [xx] proposed orthogonal low-rank adaptation (O-LoRA), which ensures orthogonality among LoRA adapters to minimize forgetting. Similarly, [xxx] introduced SAPT, which employs a learnable selection module to combine PEFT blocks based on task correlations, thereby enhancing KT.
While effective, these methods require storing all task-specific models, leading to high memory consumption that grows with the number of tasks, which limits their scalability for long task sequences.





Another line of research focuses on model merging approaches \cite{dou-etal-2024-loramoe, wan2024knowledge, yadav2024survey}, which integrate new task knowledge after training into the historical model, maintaining a single unified model and reducing memory costs compared to model ensemble methods.
%Unlike model ensemble methods, which require storing separate PEFT blocks for each task, model merging significantly reduces memory costs.
Consequently, our work primarily focuses on model merging approaches.
However, determining which parameters to merge and how to merge remains an open challenge \cite{qin2024large}.

Localizing important parameters in LLMs has recently gained significant interest, a topic widely explored in fields like model pruning and compression \cite{lu2021engage, panigrahi2023task, sun2023simple, yadav2024ties}. Building on this foundation, \citet{feng2024tasl} and \citet{du2024unlocking} have utilized gradient-based importance metrics, such as Hessian approximations, to identify critical parameters. By selectively or partially merging weights based on parameter importance, these methods have shown effectiveness in CL tasks.

However, the success of these approaches is contingent on the accurate estimation of parameter importance. A key limitation lies in their reliance on \textit{\textbf{static importance estimations}}, where the parameter importance scores for previous tasks remain unchanged and are not updated during subsequent training.
Over time, as model parameters gradually diverge from the state at which the Hessian was originally computed, these unadjusted importance estimates become increasingly inaccurate due to the growing truncation error in the Taylor expansion.
This issue is further detailed in the experiments section (Figure \ref{fig:visualization}).
%Detailed demonstrations and analysis are provided in the experiments section (Figure \ref{fig:visualization}).

% which constrains their potential for further optimization. The term ``static'' refers to two key aspects: 
% first, the initial importance distributions are computed based on fixed criteria, neglecting the variability during training, which introduces inherent bias;
% second, the estimated parameter importance scores for previous tasks remain unchanged and do not update during subsequent training. Over time, as model parameters gradually diverge from the state at which the Hessian was originally computed, the unadjusted importance estimates become increasingly inaccurate due to the growing truncation error in the Taylor expansion.
%This issue arises because the estimated parameter importance scores for previous tasks remain unchanged and fail to update during subsequent training. Over time, as model parameters gradually diverge from the state at which the Hessian was originally computed, the unadjusted importance estimates become increasingly inaccurate due to the growing truncation error in the Taylor expansion. Additionally, the randomness in data sampling during training can lead to bias when the importance distribution for each task is calculated only once. As a consequence, there may be considerable variation in the implicitly estimated importance distribution, ultimately resulting in a decline in performance. Detailed demonstrations and analysis are provided in the experiments section (Figure \ref{fig:visualization}).


%The human brain achieves remarkable CL ability through two complementary systems: the hippocampus, which specializes in the fast learning of representations for specific experiences, and the neocortex, which consolidates and transfers these memories into long-term storage through the memory consolidation process. This fast and slow mechanism is known as the Complementary Learning Systems (CLS) theory \cite{mcclelland1995there} in neuroscience.
The human brain demonstrates remarkable CL ability through two alternating systems: the hippocampus, which quickly acquires representations for specific experiences, and the neocortex, which selectively consolidates useful memories into long-term storage. 
This process is known as the Complementary Learning Systems (CLS) theory \cite{mcclelland1995there} in neuroscience.


%Inspired by the CLS theory, we introduce Bi-level Knowledge Identification and Fusion ({\ouralg}), a novel framework for continual learning that integrates two complementary learning systems: a \textit{\textbf{fast learner}}, which rapidly adapts to new task-specific knowledge in the inner loop, and a \textit{\textbf{slow learner}}, which provides global control of the knowledge fusion between new and historical knowledge in the outer loop, as shown in Figure \ref{fig:intro}.
Drawing inspiration from the CLS theory, we propose \textbf{Recurrent} \textbf{K}nowledge \textbf{I}dentification and \textbf{F}usion ({\ouralg}), a novel CL framework that dynamically estimates parameter importance and iteratively fuses knowledge.
{\ouralg} integrates an \textit{\textbf{inner learner}}, which rapidly adapts to new task-specific knowledge, and an \textit{\textbf{outer learner}}, which manages the global fusion of new and historical knowledge (see Figure \ref{fig:intro}).

%integrates two complementary learning systems: a \textit{\textbf{fast learner}}, which rapidly adapts to new task-specific knowledge in the inner loop, and a \textit{\textbf{slow learner}}, which manages global knowledge fusion between new and historical knowledge in the outer loop (see Figure \ref{fig:intro}).


In detail, the inner learner adapts to new knowledge while utilizing the proposed knowledge identification method to identify important parameters. The outer learner then retrieves historical task information from a memory buffer based on the latest model state, enabling dynamic updates of the importance distributions for previous tasks. Subsequently, a knowledge fusion mechanism is employed to integrate new and historical knowledge by pruning redundant information to mitigate CF and merging key knowledge to enhance KT.
Through iterative cycles of multiple rounds of fusion, {\ouralg} effectively captures valuable information throughout the model training process, distinguishing it from traditional post-training fusion methods. Each knowledge fusion step adaptively updates fusion weights according to the most recent importance distributions, resulting in smoother and more controlled optimization.

% In detail, {\ouralg} restructures the training process into alternating fast and slow learning cycles. The inner loop focuses on learning new task-specific knowledge and identifying the parameter importance distribution using our knowledge identification technique. The outer loop then retrieves historical task information and its corresponding importance distribution from memory buffer, followed by a novel parameter importance-based knowledge fusion mechanism to update the model.
% By separating these processes, {\ouralg} preserves task-specific knowledge to effectively mitigate CF, while updating task-shared knowledge to facilitate KT, ultimately forming long-term, robust memories. Furthermore, the multi-round fast-slow cycles dynamically and continuously capture up-to-date importance distributions for both current and historical tasks, ensuring that each knowledge fusion step is based on more accurate importance estimations, thus offering a more adaptive solution than static estimation methods.


% version 1
% Inspired by this theory, we propose {\ouralg}, a bi-level optimization framework tailored for the continual learning of language models. In {\ouralg}, the training process is restructured into multiple fast and slow learning cycles. As depicted in Figure 1, the inner training loop rapidly adapts to knowledge from the new task, while the corresponding outer training loop provides slower, global control for knowledge fusion based on historical information.
% Through this tandem interplay, {\ouralg} not only prevents new information from interfering too quickly with existing knowledge, effectively mitigating catastrophic forgetting, but also facilitates more precise knowledge transfer across tasks through the global control mechanism.
%To implement the fast and slow cycles and enable effective knowledge fusion, we propose a parameter importance-weighted knowledge identification and fusion technique to guide the outer loop's knowledge merging process. In detial, the knowledge identification-based inner loop leverages trajectory gradients generated during training to identify parameter importance distributions for the current task and get the corresponding parameter updates (residuals). The outer loop, in turn, utilizes memory buffer data to obtain up-to-date importance distributions for historical tasks based on the current model state and gets the outer-loop parameter updates (residuals). Our innovative knowledge fusion method compares the importance distributions of the current and historical tasks to distinguish between task-specific and task-shared knowledge. Using a custom-designed binary mask, it selectively merges the inner and outer parameter updates, enabling effective knowledge transfer while minimizing catastrophic forgetting.
%In summary, our merging strategy differs from traditional bi-level methods that rely on inner and outer loop gradients for updates. Instead, we are the first to leverage residuals, or could be called task vectors [xx], directly for knowledge fusion, eliminating the need for additional gradient computations and making the process more efficient. Furthermore, our fine-grained bi-level model training strategy dynamically and continuously captures up-to-date parameter importance distributions for both current and historical tasks multiple times during training, offering a more robust and adaptive solution compared to previous methods based on static importance estimation (addressing Challenge 2).

% We conduct extensive experiments to evaluate {\ouralg} on the recently developed continual learning benchmark for LLMs (Wang et al., 2023c) and the continual learning benchmark for traditional NLP tasks (Wang et al., 2022a). The results consistently demonstrate the superiority of {\ouralg} in mitigating catastrophic forgetting while showcasing remarkable KT capabilities compared to recent PET-based CL methods, bi-level optimization-based methods, and parameter importance-based approaches.
% Moreover, {\ouralg} exhibits robust performance scalability across various model sizes (ranging from 770M to 13B) and architectures, including T5-large (Raffel et al., 2020) as an encoder-decoder model and LLaMA-2 7B and 13B (Touvron et al., 2023) as decoder-only models, further demonstrating its adaptability and generalization capability.
We conduct extensive experiments to assess the effectiveness of {\ouralg} on two CL benchmarks for LLMs. The results consistently highlight the superiority of {\ouralg} in mitigating CF while exhibiting exceptional KT capabilities, outperforming state-of-the-art methods.
Furthermore, {\ouralg} exhibits robust scalability across various model architectures and sizes (from 770M to 13B), underscoring its generalization ability.

Our main contributions are summarized as:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=0pt,parsep=0pt]
\item 
We propose {\ouralg}, a novel CL \textbf{framework} for recurrent knowledge identification and fusion that dynamically estimates parameter importance and iteratively integrates knowledge.

%We propose {\ouralg}, a novel recurrent knowledge identification and fusion \textbf{framework} for CL, comprising an inner learner for rapid adaptation to new task knowledge and an outer learner for global control of knowledge integration.

\item 
%We develop new knowledge identification and fusion techniques to enable importance-guided merging of inner and outer training residuals. Within the bi-level framework, we achieve dynamic importance estimation, significantly enhancing the accuracy of knowledge identification and the effectiveness of knowledge fusion.
%We introduce a new \textbf{learning paradigm} for {\ouralg}, where an inner learner rapidly captures new information and localizes parameter importance using a knowledge identification technique. An outer learner then integrates with historical knowledge through a knowledge fusion mechanism.
We introduce a new \textbf{learning paradigm} for {\ouralg}, featuring an inner learner that rapidly captures and localizes new information, and an outer learner that globally controls the fusion of new and historical knowledge.

\item 
%We conduct extensive \textbf{evaluation} on two CL benchmark datasets, demonstrating the effectiveness of {\ouralg} in mitigating CF and facilitating KT.

%Extensive \textbf{evaluation} validates the effectiveness of {\ouralg} in mitigating CF and enhancing KT.

Extensive \textbf{evaluation} validates the effectiveness of {\ouralg} in addressing CL challenges.

\end{itemize}