

We acknowledge two limitations in this work.
Firstly, {\ouralg} is a rehearsal-based method. The outer loop relies on memory data to retrieve and dynamically update the parameter importance distributions of historical tasks.
This reliance may limit its applicability in scenarios where privacy concerns or data retention restrictions are present. Generative replay techniques could provide a solution by simulating the distribution of previous tasks without direct access to historical data.


Secondly, the time complexity of {\ouralg} increases with larger backbone models, primarily due to element-wise operations and multi-round fusion. For element-wise operations, global merging strategies have proven suboptimal, highlighting the need for balanced fusion granularity. Future work could explore focusing on specific important layers or adopting modular approaches to enhance efficiency. 
For multi-round fusion, we could further investigate how fusion frequency impacts performance and analyze the semantic knowledge learned at different stages of the training process. This could help minimize unnecessary iterations, while still preserving the benefits of iterative integration.




% Secondly, the parameter-wise operation encounters challenges in time complexity. As the size of the base model increases, both localization and fusion become more time-consuming. 
% While PEFT techniques have been employed to mitigate this issue, there is still room for optimization. Future work could explore focusing on specific important layers or adopting modular approaches to streamline these processes and reduce computational overhead.



%Second, the current parameter-wise merging strategy presents two challenges. The first challenge is time complexity: as the base model size increases, both parameter identification and merging become more time-consuming. While we have leveraged PEFT techniques to reduce time complexity, further optimization is possible, such as focusing on key layers or model components. The second challenge is that parameter-wise operations overlook inter-parameter dependencies. As seen in the model pruning literature, parameters with low individual importance may still play crucial roles in larger model components. Hence, developing more effective modular operations remains an area for future exploration.


%Secondly, the current parameter-wise merging strategy poses two challenges. The first is time complexity: as the size of the base model increases, both parameter identification and merging require more time. While we have leveraged PEFT techniques to reduce time complexity, there is still room for optimization, such as by focusing on specific important layers or key model structures. The second challenge is that parameter-wise operations do not account for dependencies between parameters. As seen in model pruning community, individual parameters may have low importance, but from a broader perspective, they could play a crucial role in specific model components. Therefore, designing more effective modular operations remains an area for future exploration.

