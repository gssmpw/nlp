\paragraph{Dataset} We adopt the experimental setup from \citet{du2024unlocking}, using two CL benchmark datasets:
(i) \textbf{Standard CL Benchmark}, which consists of five text classification tasks from \citet{zhang2015character}: AG News, Amazon Reviews, Yelp Reviews, DBpedia, and Yahoo Answers.
(ii) \textbf{Long Sequence Benchmark}, a more challenging evaluation scenario comprising 15 tasks \cite{razdaibiedina2023progressive}: five from the Standard CL Benchmark, four from the GLUE benchmark \cite{wang2018glue}, five from SuperGLUE \cite{wang2019superglue}, and the IMDB Movie Reviews dataset \cite{maas2011learning}.
% \paragraph{Dataset}
% We adopt the experimental setup from \citet{wang2023orthogonal} and \citet{du2024unlocking}, utilizing two continual learning (CL) benchmark datasets: 
% (i) \textbf{The Standard CL Benchmark} includes five text classification datasets introduced by \citet{zhang2015character}: AG News, Amazon Reviews, Yelp Reviews, DBpedia, and Yahoo Answers.
% (ii) \textbf{The Long Sequence Benchmark}, which presents a more challenging evaluation scenario, consists of a continual learning benchmark with 15 datasets \cite{razdaibiedina2023progressive}. This benchmark incorporates the five tasks from the Standard CL Benchmark, four tasks from the GLUE benchmark \cite{wang2018glue}, five tasks from the SuperGLUE benchmark \cite{wang2019superglue}, and the IMDB Movie Reviews dataset \cite{maas2011learning}.
Following \citet{wang2023orthogonal}, we sample 1000 instances for training on each task and reserve 500 per class for validation. Three task sequences are evaluated for each benchmark, with detailed descriptions and orderings provided in Appendix \ref{sec:dataset}.
%Following \citet{wang2023orthogonal}, we sample 1000 random instances for training each task and reserve 500 per class for validation. Three task sequences are considered for each benchmark: Orders 1, 2, and 3 for the Standard CL Benchmark, and Orders 4, 5, and 6 for the Long Sequence Benchmark. Task details and orderings are provided in Appendix \ref{sec:dataset}.
%Following \citet{wang2023orthogonal}, we sample 1000 random instances for training each task and reserve 500 instances per class for validation. To evaluate performance under varying task orders, we consider three distinct task sequences for each benchmark: Orders 1, 2, and 3 for the Standard CL Benchmark, and Orders 4, 5, and 6 for the Long Sequence Benchmark. Additional details about the tasks and their specific orderings can be found in Appendix x.




\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{table*}[t]

\centering
\scalebox{0.85}{
\begin{tabular}{l|cccc}
\toprule
\multirow{2}*{\tabincell{c}{Method}}  & \multicolumn{2}{c}{Standard CL benchmarks} & \multicolumn{2}{c}{Long Sequence Benchmark} \\
  & OP$\uparrow$ &   BWT$\uparrow$ & OP$\uparrow$&   BWT$\uparrow$ \\
\midrule
\rule{0pt}{4pt}SeqLoRA  & 43.7 & -50.4 & 11.6 & -73.4 \\
\rule{0pt}{8pt}IncLoRA  & 66.4 & -20.0 & 61.2 & -26.7 \\
\rule{0pt}{8pt}LoRAReplay  & 68.8 & -11.7 & 70.9 & -15.4 \\
\rule{0pt}{8pt}EWC$^*$~\cite{kirkpatrick2017overcoming} & 50.3 & - & 45.1 & - \\
\rule{0pt}{8pt}L2P$^*$~\cite{wang2022learning}  & 60.7 & - & 56.1 &  -16.3\\
\rule{0pt}{8pt}LFPT5$^*$~\cite{qin2021lfpt5}  & 72.7 & - & 69.2 & -12.8 \\
\rule{0pt}{8pt}MoELoRA$^*$~\cite{luo2024moelora} & 54.1 & - & 27.6 & - \\
\rule{0pt}{8pt}O-LoRA$^*$~\cite{wang2023orthogonal} & 75.8 & -3.8  & 69.6 & -4.1 \\
\rule{0pt}{8pt}TaSL~\cite{feng2024tasl} & 76.3 & -4.0 & 74.4 & -5.3  \\
\rule{0pt}{8pt}VR-MCL~\cite{wu2024meta}  & 76.0 &  -3.7 & 74.8 & -4.9\\
\rule{0pt}{8pt}MIGU$^*$~\cite{du2024unlocking} & 76.6 & - & 76.5 & -   \\

\rowcolor[gray]{0.9}
\rule{0pt}{8pt}\textbf{{\ouralg} (ours)} & \textbf{78.4} & \textbf{-2.8} & \textbf{77.8} & \textbf{-3.6} \\

\midrule

\rule{0pt}{8pt} MTL &  80.3 & - & 81.8 & -  \\
\rule{0pt}{8pt} SAPT-LoRA~\cite{zhao2024sapt} & - & - & 82.0 & -1.3  \\

\bottomrule
\end{tabular}}
\caption{Overall results on two CL benchmarks using the T5-large model. We report Overall Performance (OP) and Backward Transfer (BWT) after training on the final task. All results are averaged over three different task orders. Methods marked with $*$ are copied from previous papers. The last two rows represent upper bound performance.
%MTL represents multi-task learning results, which serve as an upper bound.
%Our method, {\ouralg}, outperforms the previous best method, MIGU, with an average improvement of 1.5\% in OP and a xx\% increase in BWT.
}
\label{tbl:result}
\end{table*} 





\paragraph{Metrics}
Let $a_{i,j}$ denote the testing performance on task $\mathcal{T}_i$ after training on task $\mathcal{T}_j$.
We evaluate the overall performance (OP) \cite{chaudhry2018riemannian} and backward transfer (BWT) \cite{lopez2017gradient} after training on the final task:
\begin{equation}
    \mathbf{OP} =\frac{1}{K} \sum_{i=1}^{K} a_{i, K}
\end{equation}
\begin{equation}
    \mathbf{BWT} = \frac{1}{K-1} \sum\limits_{i=1}^{K-1} (a_{i, K}-a_{i, i})
\end{equation}


\paragraph{Baselines}
We compare {\ouralg} against a range of baseline methods, including traditional CL approaches, recent PEFT-based model ensemble and merging methods. 
%All methods, including {\ouralg}, are implemented using the LoRA framework to ensure fairness:
(1) \textit{\textbf{SeqLoRA}:} LoRA parameters are trained on a task sequence without regularization or sample replay.
(2) \textit{\textbf{IncLoRA}:} incremental learning of LoRA parameters without regularization or sample replay.
(3) \textit{\textbf{LoRAReplay}:} LoRA fine-tuning with a memory buffer.
(4) \textit{\textbf{EWC \cite{kirkpatrick2017overcoming}}:} finetune LoRA with a regularization loss to prevent interference with previous tasks.
(5) \textit{\textbf{L2P \cite{wang2022learning}}:} dynamically selects and updates prompts from a pool on an instance-by-instance basis.
(6) \textit{\textbf{LFPT5 \cite{qin2021lfpt5}}:} learns a soft prompt that solves tasks and generates training samples for replay.
(7) \textit{\textbf{O-LoRA \cite{wang2023orthogonal}}:} extends IncLoRA to
learn different LoRAs in orthogonal subspaces.
(8) \textit{\textbf{MoELoRA \cite{luo2024moelora}}:} a vanilla MoE with LoRA number equals to the task number.
(9) \textit{\textbf{SAPT \cite{zhao2024sapt}}:} uses pseudo samples and a shared attention framework to align PEFT block learning and selection
(10) \textit{\textbf{TaSL \cite{feng2024tasl}}:} selectively updates or retains skill regions based on parameter importance.
(11) \textit{\textbf{MIGU \cite{du2024unlocking}}:} updates important parameters based on gradient magnitude.
(12) \textit{\textbf{VR-MCL \cite{wu2024meta}}:} dynamically updates historical task parameter importance distributions using memory replay.
%Additionally, multi-task learning on LoRA is reported as \textit{\textbf{MTL}}, serving as the upper-bound.
Additionally, multi-task learning with LoRA, referred to as \textit{\textbf{MTL}}, serves as the upper bound.









\paragraph{Training Details}
We evaluate {\ouralg} using two distinct language model architectures: the encoder-decoder T5 model \cite{raffel2020exploring} (T5-large and T5-xl), and the decoder-only LLaMA model \cite{touvron2023llama} (LLaMA2-7B and LLaMA2-13B)\footnote{Due to being limited by an academic computing budget, we employed 8-bit quantization for LLaMA models.}.
Hyperparameters $\alpha_1$ and $\alpha_2$ in Eq. (\ref{eq:I}) and Eq. (\ref{eq:out}) are set to 0.55, with the number of inner loop iterations $Q$ set to 8. Following \citet{zhao2024sapt}, 2\% of the original training set is used for replay samples.
%It is worth noting that we used the same hyperparameters across different datasets and backbones, demonstrating the generalizability of our method without requiring extensive hyperparameter tuning for each specific setting.
All experiments are averaged over 3 runs. 
More details are in Appendix \ref{sec:details}.
%Further implementation details are provided in Appendix \ref{sec:details}.

%To ensure fairness, both {\ouralg} and the baseline methods are implemented using the LoRA framework. Baselines not previously evaluated on these benchmarks are re-implemented from their official codes. We report results for OP and BWT metrics to provide comprehensive insights into how existing methods address CF and KT. All experiments are averaged over 3 runs. Further implementation details are provided in Appendix X.

%To ensure fair comparisons, both {\ouralg} and all baseline methods are implemented using the LoRA framework. For baselines that have not been previously evaluated on these benchmarks, we carefully re-implement them using their official codes and report results for OP and BWT metrics, providing comprehensive insights into how existing methods address CF and KT. All experimental results are averaged over 3 runs. Additional implementation details can be found in Appendix X.  


\subsection{Main Results}
%The overall CL results of different methods using the same T5-large backbone are summarized in Table \ref{tbl:result}.
The overall CL results using the same T5-large backbone are summarized in Table \ref{tbl:result}.


\paragraph{Our {\ouralg} effectively addresses the challenges of CF and KT simultaneously.}
Compared to both traditional CL methods (LoRAReplay, L2P) and model ensemble-based methods (MoELoRA, O-LoRA), {\ouralg} outperforms them in both CF (increasing average OP from 72.7\% to 78.1\% compared to O-LoRA) and KT (increasing average BWT from -13.6\% to -3.2\% compared to LoRAReplay).
SAPT achieves the highest performance by leveraging generative replay-based data augmentation, surpassing MTL result. However, it relies heavily on external data synthesis, which can be costly in LLM settings.

Moreover, when compared to parameter importance-based methods like TaSL and VR-MCL, {\ouralg} consistently delivers the best OP and BWT scores. 
Notably, {\ouralg} outperforms the state-of-the-art CL method, MIGU, increasing OP from 76.6\% to 78.1\%.
These results underscore the effectiveness of our recurrent knowledge identification and fusion framework, validating the advantages of dynamic parameter importance estimation in mitigating CF and promoting KT.


\paragraph{{\ouralg} demonstrates consistent superiority across various backbones.}
To further validate the robustness of {\ouralg}, we conduct experiments across different backbones (Figure \ref{fig:different_size}). Across all backbone sizes, from 770M to 13B, {\ouralg} consistently outperforms all baseline models.
For instance, using the LLaMA2-7B backbone, {\ouralg} boosts the OP metric from 75.6\% to 78.2\% compared to VR-MCL. 
These results emphasize the critical role of accurate parameter importance estimation and demonstrate the robust generalization capability of {\ouralg} across different model scales.

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{imgs/Diff_size.png}
  \caption{Performance of {\ouralg} with different backbones on the Long Sequence Benchmark.}
  \label{fig:different_size}
\end{figure}


\paragraph{Dynamic importance estimation enables effective knowledge retention and transfer.}
Figure \ref{fig:radial_plot} illustrates the performance of various methods across all historical tasks after completing the final task.
It highlights that {\ouralg} optimally restores the model’s performance on previous tasks, with significant improvements on Amazon and Copa.
Notably, on IMDB and AG News, {\ouralg} performs comparably to multi-task learning results. These demonstrate that {\ouralg} strikes an effective balance between preserving prior knowledge and excelling in new tasks.

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{imgs/Radial.pdf}
  \caption{\textbf{Impact of Catastrophic Forgetting in Continual Learning.}
  After fine-tuning on the final task (in orange), {\ouralg} demonstrates superior resistance to performance decline on previously learned tasks (in blue), outperforming baseline methods.
 %After fine-tuning on the final task (in orange), baseline methods exhibit a significant performance decline on previously learned tasks (in blue). In contrast, {\ouralg} effectively mitigates these effects, with performance closely aligning with that of multi-task learning, demonstrating its ability to strike an optimal balance between retaining prior knowledge and excelling in new tasks.
  }
  \label{fig:radial_plot}
\end{figure}

\begin{table}[t]
\centering
\scalebox{0.9}{
\begin{tabular}{lcc}
\toprule
Method & OP &  BWT\\
\midrule
\rowcolor[gray]{1}
\rule{0pt}{6pt} {\ouralg}  & \textbf{77.9} & \textbf{-3.4} \\
\midrule
\rule{0pt}{8pt}  - DIE  & 74.8 & -4.8\\
\rule{0pt}{8pt}  - KI & 52.3 & -21.5  \\
\rule{0pt}{8pt} + GM & 72.1 & -11.2 \\
\rule{0pt}{8pt} + Adaptive & 76.1 & -4.1\\
\rule{0pt}{8pt} - Share & 75.8 &  -4.3\\

\bottomrule
\end{tabular}}
\caption{Ablation study. ``- DIE'', ``- KI'', ``- Share'' refer to the removal of dynamic importance estimation, knowledge identification, and task-shared region updates, respectively. ``+ GM'', ``+ Adaptive'' represent replacing the knowledge fusion mechanism with global merging and adaptive merging strategy, respectively.
}
\label{tbl:ablation}
\end{table}


\begin{figure*}[t]
  \centering
  \subfigure[Normalized inner task vector and corresponding parameter importance distribution for the current task.]{\includegraphics[width=0.45\linewidth]{imgs/Vis1_new.pdf}}
  \hspace{0.03\linewidth}
  \subfigure[Normalized parameter importance scores for a historical task across model states. 
  %Historical task's normalized parameter importance scores across model states. 
  %The upper part shows the distribution before training on the new task, and the lower part shows the distribution after training.
  ]
  {\includegraphics[width=0.45\linewidth]{imgs/Vis2_new.pdf}}
  \caption{Visualizations of task vector and parameter importance distributions on T5-large.}
  \label{fig:visualization}
\end{figure*}


\subsection{Ablation Study}
We conduct ablation studies to assess the effectiveness of the proposed techniques in {\ouralg}. The results for task order 1 on the Long Sequence Benchmark are shown in Table \ref{tbl:ablation}. Additional experiments, such as time complexity analysis, the impact of memory size, and hyperparameter sensitivity, are provided in Appendix~\ref{sec:hyper}.


\paragraph{Effect of Dynamic Importance Estimations.}  
To validate the role of dynamic importance estimation, we replace it with a static version (``- DIE''), where importance scores for historical tasks remain fixed after their initial computation.
The significant performance decline (3.1\% on OP and 1.4\% on BWT) highlights the necessity of dynamically updating historical task importance distributions. By maintaining up-to-date importance scores, our approach improves both robustness and accuracy, thereby enhancing knowledge retention and transfer.
%We compare our dynamic importance estimation framework with a static importance estimation-based version of {\ouralg}, denoted {\ouralg}$^{1-}$, where importance scores for historical tasks remain fixed after their initial computation. The results in Table \ref{tbl:ablation_dynamic} show that {\ouralg} achieves an average improvement of 2.1\% and 1.2\% in two evaluation metrics compared to {\ouralg}$^{1-}$, demonstrating dynamic importance updating improves robustness and accuracy. By maintaining more relevant and up-to-date importance distributions of historical task parameters, our approach enhances both knowledge retention and transfer.
% \begin{table}
% \centering
% \scalebox{0.9}{
% \begin{tabular}{lcc}
% \toprule
% Method & OP &  BWT\\
% \midrule
% \rule{0pt}{6pt} Global Merging & 52.3 & -21.5 \\
% \rule{0pt}{8pt}  EMA &  44.2 &  -25.7 \\
% \rule{0pt}{8pt}  {\ouralg}$^{1-}$ & 75.8 & -4.6\\
% \rowcolor[gray]{0.9}
% \rule{0pt}{8pt} {\ouralg} (ours) & \textbf{77.9} & \textbf{-3.4} \\

% \bottomrule
% \end{tabular}}
% \caption{Ablation study. Evaluating the effect of Bi-level Fast-Slow Model Merging Framework.
% }
% \label{tbl:ablation_dynamic}
% \end{table}


\paragraph{Effect of Importance-Based Binary Mask Strategy in Knowledge Fusion.}  
We replace our knowledge fusion mechanism with three alternative model merging strategies:
(i) Without knowledge identification (``- KI''): Directly merge \(\tau^{\text{in}}\) and \(\tau^{\text{out}}\) in Eq.(\ref{eq:fusion}) without applying importance-based fusion.  
(ii) Global importance-based merging (``+ GM''): Use a global weighted sum of importance scores \(I_b^{\text{in}}\) and \(I_b^{\text{out}}\) for fusion instead of applying element-wise masking.
(iii) Adaptive Fusion \cite{yangadamerging} (``+ Adaptive''): A soft-masking method that uses raw importance scores directly for fusion instead of binary masks.
Additionally, we evaluate the effectiveness of updating the task-shared region by introducing ``- Share'', where $m^{in}$ is set to 0 when both $m^{in}$ and $m^{out}$ are 1. 
%before applying the fusion operation.

The results in Table \ref{tbl:ablation} confirm the effectiveness of importance-based binary masking in filtering redundant information and preserving task-specific knowledge.
Moreover, disabling updates to the task-shared region leads to performance drops of 2.1\% and 0.9\% on two evaluation metrics, demonstrating that updating task-shared parameters is critical for effective knowledge transfer between tasks.




\paragraph{Effect of Multi-Round Knowledge Fusion.}  
We compare multi-round fusion with traditional single-step fusion methods and analyze the impact of the number of knowledge fusions by adjusting the inner loop iteration size $Q$.
In {\ouralg}, with the total number of iterations for the inner loop fixed at \(N'\), increasing $Q$ reduces the number of fusion steps, which is $N'/Q$. A detailed analysis and the model’s time complexity are provided in Appendix~\ref{sec:time}.

%We evaluate the effect of multi-round knowledge fusion by comparing it with traditional single-step fusion methods and varying the number of fusion steps through the inner loop size $Q$. Assuming vanilla training has \(N'\) training iterations, the number of inner-outer loop cycles is \(N'/Q\) in {\ouralg}, ensuring comparable inner loop iterations while introducing only \(N'/Q\) additional outer loop steps. Details on time complexity are in Appendix~\ref{sec:time}.  

Figure~\ref{fig:fusion} presents two key findings: (i) Multi-round fusion consistently outperforms single-step fusion by leveraging intermediate training information, resulting in smoother knowledge integration, similar to the distinction between gradient descent and stochastic gradient descent; and (ii) performance improves with the number of fusion steps up to a certain point, after which diminishing returns are observed.
This decline occurs because excessive fusion introduces noise and redundant updates, disrupting the balance between new and prior knowledge, and potentially causing overfitting to unconverged intermediate states. 
%These results underscore the need to optimize fusion frequency to strike a balance between knowledge retention and transfer.

%This decline can be attributed to excessive fusion introducing noise and redundant updates, disrupting the balance between new and historical knowledge, and potentially leading to overfitting to unconverged intermediate states. These findings highlight the importance of optimizing fusion frequency to effectively balance knowledge retention and transfer.


\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{imgs/Diff_fusiontime.png}
  \caption{Ablation study on the number of fusions.}
  \label{fig:fusion}
\end{figure}






% \paragraph{Effect of the Bi-level Fast-SLow Model Merging Framework.}
% We compare our fast-slow bi-level optimization framework with two other model merging strategies: (i) Global Merging, which averages model parameters before and after training as $\hat{\theta}^k = \lambda \hat{\theta}^{k-1} + (1 - \lambda) \theta^k$, and (ii) Exponential Moving Average (EMA) \cite{szegedy2016rethinking}, which applies a running average of parameters during fine-tuning. Additionally, we evaluate a static importance estimation-based version of {\ouralg}, denoted {\ouralg}$^{1-}$, which uses a fixed importance distribution based on the initial parameter importance without updates during subsequent training.

% The results in Table \ref{tbl:ablation_bilevel} show that {\ouralg} outperforms the other three strategies. Specifically, {\ouralg} achieves an average improvement of 2.1\% and 1.2\% in two evaluation metrics compared to {\ouralg}$^{1-}$, demonstrating the advantages of dynamic importance estimation. By continuously refining the importance distributions of historical task parameters, our approach enhances both knowledge retention and transfer.



% \begin{table}
% \centering
% \scalebox{0.9}{
% \begin{tabular}{lcc}
% \toprule
% Method & OP &  BWT\\
% \midrule
% \rule{0pt}{6pt} $I\left( \cdot \right) = \left|\nabla_{w_{i j}} \mathcal{L}\right|$  & 74.5 & -5.2\\
% \rule{0pt}{8pt}  $I\left( \cdot \right) = \left|w_{i j} \nabla_{w_{i j}} \mathcal{L}\right|$ & 75.9 & -4.7 \\
% \rowcolor[gray]{0.9}
% \rule{0pt}{8pt} {\ouralg} (ours) & \textbf{77.9} & \textbf{-3.4} \\

% \bottomrule
% \end{tabular}}
% \caption{Ablation study. Evaluating the impact of different importance metrics on knowledge identification.
% }
% \label{tbl:ablation_ipt}
% \end{table}

% \paragraph{Effect of the Proposed Importance Metric in Knowledge Identification.}
% We compare two alternative importance scoring approaches with Eq. (\ref{eq:1}): 
% (i) using absolute gradients~\cite{michel2019sixteen}, $\left|\nabla_{w_{ij}} \mathcal{L}\right|$, instead of the gradient-weight product; and (ii) removing exponential moving average, relying only on importance scores computed from a single batch.

% As shown in Table \ref{tbl:ablation_ipt}, our method with exponential smoothing outperforms the alternatives, with performance drops of up to 2.0\% and 1.3\% without smoothing. Similarly, using absolute gradients leads to lower performance compared to the gradient-weight product, underscoring the effectiveness of our approach in enhancing knowledge identification and model performance.



% \begin{table}
% \centering
% \scalebox{0.9}{
% \begin{tabular}{lcc}
% \toprule
% Method & OP &  BWT\\
% \midrule
% \rule{0pt}{6pt} Adaptive Fusion  & 76.1 & -4.1\\
% \rule{0pt}{8pt}  {\ouralg}$^{2-}$ & 75.8 &  -4.3\\
% \rowcolor[gray]{0.9}
% \rule{0pt}{8pt} {\ouralg} (ours) & \textbf{77.9} & \textbf{-3.4} \\

% \bottomrule
% \end{tabular}}
% \caption{Ablation study. Evaluating the impact of different knowledge fusion strategies.
% }
% \label{tbl:ablation_fusion}
% \end{table}



% \paragraph{Effect of Binary Mask-based Knowledge Fusion in the Outer Loop.} We compare our hard-masking knowledge fusion strategy with an adaptive fusion method proposed by \citet{yangadamerging}, which uses soft-masking based on parameter importance. Additionally, we evaluate the effectiveness of updating the task-shared region by introducing {\ouralg}$^{2-}$, where $m^{in}$ is set to 0 when both $m^{in}$ and $m^{out}$ are 1, before applying the fusion operation.

% Results in Table \ref{tbl:ablation_fusion} show that our hard-masking strategy outperforms adaptive fusion, this can be attributed to the pre-merging mask operation, which eliminates redundant parameters, preventing contamination of task-specific knowledge. Additionally, {\ouralg}$^{2-}$ performs worse than {\ouralg}, with a declines of 2.1\% and 0.9\% on two evaluation metrics, indicating that updating task-shared parameters contributes to better knowledge transfer.


%\subsection{Visualization of Importance Distributions}
\subsection{Visualization}
We present two key visualizations to analyze the effectiveness of our proposed methods:
%We present two visualizations analyzing our methods' effectiveness:
%\paragraph{Magnitude of Task Vector vs. Importance Score}
\paragraph{Can the Magnitude of the Task Vector Reflect Parameter Importance?}
We explore the relationship between task vector magnitude and parameter importance scores. As shown in Figure \ref{fig:visualization}(a), although the magnitude of parameter updates is generally large, only a subset of parameters, mainly in the encoder, are truly important.
This indicates that a large portion of the parameters are redundant, highlighting the need for our importance-based knowledge fusion mechanism.

%\paragraph{Does the Importance Distribution of Historical Tasks Change with Model State?} Figure \ref{fig:visualization}(b) shows the shift in the importance distribution of historical tasks before and after training on a new task. While the overall distribution remains relatively stable across model states, significant differences in specific importance scores are observed, especially in regions marked by the dashed box. This highlights the value of dynamic importance estimation, which enables more precise identification of important parameters and enhances knowledge fusion across tasks.

\paragraph{Does the Importance Distribution of Historical Tasks Change with Model State?}
Figure \ref{fig:visualization}(b) illustrates the shift in the importance distribution of historical tasks before and after training on a new task. 
While the overall distribution remains stable across model states, notable changes in specific importance scores are observed, highlighted by the dashed box. This demonstrates the value of dynamic estimation, enabling more precise identification of key parameters and enhancing knowledge fusion across tasks.
A more detail analysis is provided in Appendix \ref{sec:preliminary}.








% Following prior work, we primarily integrate LoRA adapters into the KV matrices of the attention mechanism. To further investigate the distribution of knowledge storage within the model, we also extend LoRA adapters to the Q and O matrices, as well as the MLP layers. The resulting parameter importance distributions are visualized in Figure \ref{fig:heat_map}.
% Our analysis of the visualized results reveals several interesting findings:
% 1. Higher Importance in MLP Layers: We observe that the importance of parameters in the MLP layers is consistently higher than that in the attention layers. This observation aligns with findings from the model-editing literature, suggesting that MLP layers play a more significant role in storing task-specific knowledge.
% 2. xxx
% These findings provide valuable insights into the internal storage and distribution of knowledge in language models and highlight the importance of dynamically identifying critical parameters across different model components.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=1\linewidth]{imgs/Vis1.pdf}
%   \caption{Visualization of the normalized inner task vector (upper) and its corresponding parameter importance distribution (lower) on T5-large.
%   }
%   \label{fig:visualization1}
% \end{figure}

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=1\linewidth]{imgs/Vis2.pdf}
%   \caption{Visualization of the historical task’s normalized parameter importance scores across model states on T5-large. The upper part shows the distribution before training on the new task, and the lower part shows the distribution after training.
%   }
%   \label{fig:visualization2}
% \end{figure}


