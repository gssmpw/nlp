Continual learning (CL) is crucial for deploying large language models (LLMs) in dynamic real-world environments without costly retraining. 
While recent model ensemble and model merging methods guided by parameter importance have gained popularity, they often struggle to balance knowledge transfer and forgetting, mainly due to the reliance on static importance estimates during sequential training.
In this paper, we present {\ouralg}, a novel CL framework for Recurrent Knowledge Identification and Fusion, which enables dynamic estimation of parameter importance distributions to enhance knowledge transfer.
Inspired by human continual learning, {\ouralg} employs an inner loop that rapidly adapts to new tasks while identifying important parameters, coupled with an outer loop that globally manages the fusion of new and historical knowledge through redundant knowledge pruning and key knowledge merging.
These inner-outer loops iteratively perform multiple rounds of fusion, allowing {\ouralg} to leverage intermediate training information and adaptively adjust fusion strategies based on evolving importance distributions. 
%Consequently, {\ouralg} achieves smoother optimization than traditional post-training fusion methods.
Extensive experiments on two CL benchmarks with various model sizes (from 770M to 13B) demonstrate that {\ouralg} effectively mitigates catastrophic forgetting and enhances knowledge transfer.\footnote{The code will be released upon publication.}

%The source code is available for reproducibility at: \url{https://anonymous.4open.science/r/Recurrent_KlF-527E}}.

%Extensive experiments on two CL benchmarks demonstrate the superiority of {\ouralg} in addressing catastrophic forgetting and promoting knowledge transfer. Moreover, {\ouralg} exhibits strong scalability across various model sizes (770M to 13B) and architectures (T5 and LLaMA)\footnote{The source code is available for reproducibility at: \url{https://anonymous.4open.science/r/Recurrent_KlF-527E}}.



% version 3
% Continual learning (CL) is essential for deploying large language models (LLMs) in everchanging real-world scenarios without costly retraining. Parameter importance-guided model ensemble and model merging methods have emerged as de facto choices for CL. However, these methods often face suboptimal trade-offs between knowledge transfer and forgetting, as they rely on static importance estimations during sequential training.
% Inspired by the process of long-term memory formation in humans, we propose a novel CL framework named {\ouralg}, which integrates a fast-learning system for rapid adaptation and identification of task-specific knowledge from new tasks, and a slow-learning system for global control of new and historical knowledge merging via a knowledge fusion mechanism. The two fast and slow learning systems are complementary, working seamlessly within a unified CL framework to achieve both dynamic and accurate parameter importance estimation.
% Extensive experiments on two CL benchmarks demonstrate the superiority of {\ouralg} in addressing catastrophic forgetting and promoting knowledge transfer. Moreover, {\ouralg} exhibits strong scalability across various model sizes (770M to 13B) and architectures (T5 and LLaMA)\footnote{www.xxxxxx}.
%The source code is available for reproducibility\footnote{wwwxxxxxx}.



%version 2
% Continual learning (CL) is essential for deploying large language models (LLMs) in everchanging real-world scenarios without costly retraining. Parameter importance-guided model ensemble and model merging methods have emerged as de facto solutions for CL. However, these methods often face suboptimal trade-offs between knowledge transfer and forgetting due to their reliance on static importance estimations during sequential training.
% Inspired by the human brain's powerful CL ability, we propose a Bi-level Knowledge Identification and Fusion ({\ouralg}) framework for LLM continual learning. The framework incorporates an inner loop for rapid adaptation to new task-specific knowledge and an outer loop for slower, global control based on historical learned knowledge.
% Equipped with our proposed knowledge identification and fusion strategy, {\ouralg} dynamically updates parameter importance estimations during each fast-slow loop, leveraging the latest model states.
% It then selectively retains task-specific knowledge to prevent forgetting and updates task-shared knowledge to facilitate effective knowledge transfer.
% Extensive experiments on two CL benchmarks demonstrate the superiority of {\ouralg} in addressing catastrophic forgetting and promoting knowledge transfer. Moreover, {\ouralg} exhibits strong scalability across various model sizes (770M to 13B) and architectures (T5 and LLaMA). The source code is available for reproducibility.



% version 1
% Continual learning (CL) is essential for deploying large language models (LLMs) in everchanging real-world scenarios without costly retraining. 
% Model ensemble and model merging methods, guided by parameter importance, have emerged as the de facto solutions for CL.
% However, these methods often struggle with suboptimal trade-offs between knowledge transfer and forgetting due to their reliance on fixed and unchanging importance estimations during sequential training.
% Neuroscience reveals that the brain achieves remarkable CL ability through two complementary systems: one fast and one slow. Motivated by this theory, we propose a bi-level optimization framework, {\ouralg}, for LLM continual learning. The framework features an inner loop for rapid adaptation to new knowledge on the current task and an outer loop for slower, global control, enabling selective retention and updating of task-specific knowledge to mitigate forgetting and enhance knowledge transfer. Using our proposed knowledge identification and fusion strategy, {\ouralg} dynamically updates parameter importance during training based on the latest model states, providing more accurate knowledge localization.
% Extensive experiments on two CL benchmarks demonstrate the effectiveness of {\ouralg} in addressing catastrophic forgetting and facilitating knowledge transfer. Furthermore, {\ouralg} exhibits strong scalability across various model sizes (770M to 13B) and architectures (T5 and LLaMA). The source code is available for reproducibility.
