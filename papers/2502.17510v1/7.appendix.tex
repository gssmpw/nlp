\section{Limitations of Static Parameter Importance Estimation}
\label{sec:preliminary}
%In this section, we provide empirical evidence to demonstrate the limitations of static parameter importance estimation. Specifically, we explore two major issues with static importance estimations: (1) the bias introduced by fixed importance distributions, and (2) the inaccuracy of historical parameter importance estimates as the model state evolves.
In this section, we empirically demonstrate the limitations of static parameter importance estimation by analyzing how historical parameter importance distributions change under different conditions. Static importance estimation assumes that the importance scores of historical tasks remain fixed, which is both inaccurate and introduces biases during the knowledge fusion process. To highlight the dynamic nature of parameter importance, we analyze the following two aspects:

\paragraph{Changes in Historical Importance After Learning Different New Tasks.}
We investigate how the parameter importance distribution for a fixed historical task changes after the model is fine-tuned on different new tasks. Specifically, the model is trained sequentially on various new tasks, and the importance scores of the same historical task are re-evaluated using the updated model parameters.
As shown in Figure \ref{fig:heatmap1}, although the regions identified as important for the historical task remain largely consistent after learning different new tasks, the specific importance values exhibit noticeable differences.
This variability demonstrates that historical parameter importance is heavily influenced by the specific characteristics of the new task, making static importance estimation insufficient for accurately capturing the evolving model dynamics.

%\paragraph{Effect of Data Order on Parameter Importance.} First, we examine the effect of varying the order in which training data is presented to the model. Specifically, we shuffle the input data for the same task multiple times and compute the parameter importance distribution at each shuffle. This variability in data order influences the model's learning process, leading to slight variations in the estimated parameter importance. As shown in Figure \ref{fig:heatmap1}, while the regions of importance identified across different data orderings are generally consistent, there are noticeable discrepancies in the specific values of parameter importance. The heatmap illustrates how different input data sequences lead to variations in the importance of parameters, emphasizing the sensitivity of the model to the sequence in which data is presented.

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{imgs/heatmap1.pdf}
  \caption{Visualization of the parameter importance distribution for the fixed historical task ``AG News'' after training on different new tasks.
  }
  \label{fig:heatmap1}
\end{figure}


\paragraph{Temporal Changes in Historical Importance During New Task Training.} 
We further analyze how the historical parameter importance distribution evolves during the training process of a single new task. As shown in Figure \ref{fig:heatmap2}, by periodically evaluating the importance scores of a fixed historical task at different stages of training, we also observe temporal changes in the parameter importance distribution. These changes indicate that historical importance is not static even within the training process of a single task, reflecting the continuous interactions between new and historical knowledge. Static estimation fails to capture these temporal dynamics, which can lead to suboptimal knowledge fusion.

Our analysis confirm that historical parameter importance distributions are dynamic, influenced by both the characteristics of the new task being learned and the training stage. These observations provide strong empirical evidence supporting the need for dynamic importance estimation approaches.
Static importance estimation fails to account for these variations, potentially causing biases and inaccuracies in knowledge fusion. In contrast, dynamic importance estimation, as proposed in our framework, addresses these issues by continuously updating importance distributions to align with the most recent model state, ensuring more effective and accurate knowledge integration.

%\paragraph{Effect of Model State on Historical Parameter Importance.} Next, we explore the impact of model state updates on historical task importance distributions. As the model is trained on new tasks, the model parameters are updated, and this, in turn, affects the importance of parameters for previously learned tasks. For example, after training on Task 5, we evaluate the importance distribution of parameters for Task 2, both before and after the model's state has been updated. The results, shown in Figure \ref{fig:heatmap2}, demonstrate that the importance distribution for Task 2 shifts as the model's parameters evolve. The heatmap provides a clear visual representation of how the importance of historical tasks is altered by the model's current state, indicating that the parameter importance is not static but evolves over time.

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{imgs/heatmap2.pdf}
  \caption{Visualization of the parameter importance distribution for a fixed historical task at different stages of training on a new task.
  }
  \label{fig:heatmap2}
\end{figure}

%These results highlight the key limitations of static importance estimation methods, which fail to adapt to the dynamic changes in both the training data order and the model's state. These observations provide strong empirical evidence supporting the need for dynamic importance estimation approaches. Our proposed approach {\ouralg}, which incorporates the idea of dynamic updating importance scores, addresses these challenges and provides more accurate estimates over time.





% The term “static” refers to two key aspects in parameter importance estimation: first, the initial importance distributions are computed using fixed criteria without considering the variability introduced during training; second, these distributions remain unchanged and do not update as the model's state evolves during subsequent learning.
% This static nature introduces several challenges. Fixed importance distributions are inherently biased. Our experiments show that varying the order of input data during training, such as shuffling data in different sequences, leads to noticeable changes in the resulting importance distributions, undermining their reliability. Furthermore, as continual learning progresses, the parameter importance scores computed for earlier tasks become increasingly outdated. Over time, model parameters diverge from the state at which the original importance was computed, and the unadjusted importance estimates become less accurate due to the growing truncation error in the Taylor expansion. Consequently, relying on these outdated scores for model merging introduces additional bias, ultimately reducing overall performance.




\section{Additional Results}
\label{sec:hyper}

\subsection{Effect on the SuperNI Benchmark}
To further validate the effectiveness of {\ouralg} in more complex CL scenarios, we have conducted additional experiments on the SuperNI Benchmark \cite{wang2022super}, which includes tasks like dialogue generation, information extraction, question answering, and summarization. Using T5-large as the backbone, we assessed 15 tasks following the experimental setup in \cite{zhao2024sapt}. The results are shown in the table \ref{tbl:SuperNI}.

\begin{table}[h]
\centering
\scalebox{1}{
\begin{tabular}{lcc}
\toprule
Method &   OP  & BWT \\
\midrule
\rule{0pt}{4pt} Replay & 35.4 & -15.8 \\
\rule{0pt}{8pt} O-LoRA  & 25.9 & -24.6\\
\rule{0pt}{8pt} TaSL  & 41.3 &   -12.7\\
\rule{0pt}{8pt} VR-MCL  &  40.5 & -10.9 \\
\rule{0pt}{8pt} {\ouralg}  &  43.3 & -8.4 \\
\rule{0pt}{8pt} MTL (Upper Bound)   &  50.7 & - \\

\bottomrule
\end{tabular}}
\caption{Overall results on the SuperNI Benchmark.}
\label{tbl:SuperNI}
\end{table}

While performance decreases due to task complexity, {\ouralg} consistently outperforms other methods, demonstrating its robustness and ability to handle more sophisticated CL scenarios.

\subsection{Effect of the Memory Size}
We investigate the impact of varying memory size on the performance of LoRAReplay and {\ouralg}. By adjusting the memory size per task \(|M|\) to {2\%, 5\%, 10\%, 50\%}, the results are shown in Table \ref{tbl:ablation_memory}. As expected, increasing the memory size generally improves the performance of all methods. 
{\ouralg} leverages its knowledge fusion mechanism to effectively preserve the parameters that store historical knowledge, thereby achieving better performance than LoRAReplay.


\begin{table}[h]
\centering
\scalebox{1}{
\begin{tabular}{lcccc}
\toprule
\multirow{2}*{\tabincell{c}{ }} & \multicolumn{4}{c}{Memory Size}\\
\cmidrule(lr){2-5}
 & 2\% & 5\% & 10\% & 50\%\\
\midrule
\rule{0pt}{6pt} LoRAReplay & 71.2 & 72.4& 73.8 & 76.1   \\

\rule{0pt}{8pt} {\ouralg} & 77.9 & 78.7  & 79.8 & 80.5 \\

\bottomrule
\end{tabular}}
\caption{Ablation study on memory size, using T5-large as the backbone.
%Average performance (AP) of Replay, TaSL-M, and TasLoRA-M across different memory sizes on the Long Sequence Benchmark.
}
\label{tbl:ablation_memory}
\end{table}



\subsection{Effect of Different LoRA Adapters}
We further investigate which components within a transformer block should incorporate LoRA. A typical transformer block consists of the query, key, and value (QKV) linear layers, the output linear layer (O) in the multi-head attention module, and the two linear layers in the feedforward network (FFN). Our analysis, presented in Table \ref{tbl:ablation_lora}, shows that applying LoRA to all these linear layers yields the best overall performance. Notably, adding LoRA to the FFN layers results in better BWT performance than applying it to the multi-head attention layers.

\begin{table}[h]
\centering
\scalebox{1}{
\begin{tabular}{lrr}
\toprule
LoRA Target Modules &   OP   & BWT \\
\midrule
\rule{0pt}{4pt} Attention Q V  & 77.9  & -3.4  \\

\rule{0pt}{8pt} Attention Q K V O & 77.7  & -3.3	   \\
\rule{0pt}{8pt} FFN  & 77.7   & -2.5	   \\
\rule{0pt}{8pt} Attention All + FFN & 78.0  & 	-3.2   \\


\bottomrule
\end{tabular}}
\caption{Ablation study on LoRA target modules, using T5-large as the backbone.}
\label{tbl:ablation_lora}
\end{table}


\subsection{Sensitivity Analysis for Hyperparameters}
The proposed framework incorporates three key hyperparameters, including the smoothing factor $\alpha$ for computing importance scores in Equations (\ref{eq:I}) and (\ref{eq:out}), the threshold $\delta$ for determining the importance of parameters, and the number of inner and outer loop training steps. Our analysis aims to assess the impact of varying these hyperparameters on our method's performance, testing on the T5-large backbone model.

As evidenced in Table \ref{tbl:hp_alpha}, we determine that the optimal setting for $\alpha$ is 0.55. An $\alpha$ value too low results in a performance decline, indicating that the calculated importance scores are not accurate.

\begin{table}[h]
\centering
\scalebox{1}{
\begin{tabular}{lcc}
\toprule
$\alpha_1$,$\alpha_2$ &   OP  & BWT \\
\midrule
\rule{0pt}{4pt} 0.35 & 77.7 & -3.4 \\
\rule{0pt}{8pt} 0.55  & 78.1 & -2.8\\
\rule{0pt}{8pt} 0.85  & 77.9 &   -3.4\\
\rule{0pt}{8pt} 0.95  &  77.8 & -2.7 \\

\bottomrule
\end{tabular}}
\caption{Performance comparisons of {\ouralg} equipped with different $\alpha$.}
\label{tbl:hp_alpha}
\end{table}

Regarding the selection of the threshold for important parameters, Table \ref{tbl:hp_delta} below shows the model's performance with varying thresholds $\delta$ on T5-large. It can be seen that setting a high threshold (50\%) reduces model effectiveness by categorizing less significant parameters as important, which can contaminate historical knowledge and lead to forgetting. Conversely, a 1\% threshold still maintains strong performance owing to our effective knowledge fusion approach, which preserves task-specific knowledge and prevents forgetting. Considering the 28-law of diminishing returns, we opted for a 20\% threshold to distinguish between important and less significant parameters.

\begin{table}[h]
\centering
\scalebox{1}{
\begin{tabular}{lcc}
\toprule
Importance Thresholds $\delta$  &OP &  BWT\\
\midrule
\rule{0pt}{4pt}1\%  &  77.4 & -3.3  \\

\rule{0pt}{8pt}10\% &  77.8 &  -3.1\\
\rule{0pt}{8pt}20\% & 77.9  & -3.4 \\

\rule{0pt}{8pt}50\% & 77.6  &  -2.4 \\
\bottomrule
\end{tabular}}
\caption{Performance comparisons of {\ouralg} equipped with different $\delta$.}
\label{tbl:hp_delta}
\end{table}

Finally, we investigate the effect of varying the number of inner and outer loop training steps on model performance. As shown in Table \ref{tbl:inner}, increasing the number of iterations for both the inner and outer loops can lead to improved performance, particularly in terms of the BWT metric. However, we observe diminishing returns beyond a certain point. Specifically, the performance gain becomes less significant when the number of iterations exceeds 8 for the inner loop and 4 for the outer loop. This suggests that our framework reaches an optimal balance between computational efficiency and performance with a modest number of iterations.

\begin{table}[t]
\centering
\scalebox{1}{
\begin{tabular}{llcc}
\toprule
Inner Steps ($Q$) & Outer Steps  &OP &  BWT\\
\midrule
\rule{0pt}{4pt} 4 & 1 & 76.7  & -4.4 \\
\rule{0pt}{8pt} 8 & 1 & 77.7  & -3.2\\
\rule{0pt}{8pt} 8 & 4 & 77.9  & -3.4\\
\rule{0pt}{8pt} 16 & 4 & 76.8  & -5.4\\

\bottomrule
\end{tabular}}
\caption{Performance comparisons of {\ouralg} (using T5-large as
the backbone) equipped with different inner and outer loop training steps.}
\label{tbl:inner}
\end{table}


In conclusion, it is worth noting that there is a small performance difference observed when varying the hyperparameters. This suggests that the proposed {\ouralg} method exhibits robustness and is not highly sensitive to the choice of hyperparameters.

\subsection{Time Complexity Analysis}
\label{sec:time}

In this section, we discuss the time complexity issues introduced by the techniques used in {\ouralg}. The additional time complexity can be explained qualitatively: assuming the number of training iterations for vanilla training is \(N'\), we set the total number of iterations for the inner loop to \(N'\) as well. This ensures a fair comparison with traditional methods, while also minimizing the number of iterations in our approach. In this case, the total number of iterations for our method is \(N' + (N'/Q)\), where \(N'/Q\) corresponds to the iterations of the outer loop. Typically, \(N'\) is 1024, and with \(Q\) set to 8, this results in an additional 12\% increase in training time.

Regarding the time consumption of knowledge identification and fusion, the variables used in the knowledge identification phase are derived from the gradients produced during normal training, requiring no extra computation time, only additional space to store parameter importance information. The knowledge fusion phase involves only simple univariate calculations, as shown in Equation~(\ref{eq:fusion}). Therefore, the overall time complexity does not increase significantly.

Quantitatively, we compare the training time of our method with LoRA Replay, as shown in Figure~\ref{tbl:time}. Compared to traditional LoRA replay methods, the addition of knowledge identification and fusion does not significantly increase training time across different backbones. For instance, when using LLaMA2-13B as the backbone, adding knowledge identification and fusion results in a 1.37x increase in training time compared to the original setup. However, for smaller models like T5-large and T5-xl, the training time remains relatively consistent, with no significant impact observed from the inclusion of the reasoning components.

%We examine the impact of the proposed multi-round knowledge fusion framework on training time, with the results shown in Figure \ref{tbl:time}. Compared to traditional LoRA replay methods, the inclusion of additional knowledge localization and fusion does not significantly increase the training time across different backbones. For instance, when using LLaMA2-7B as the backbone, the addition of knowledge localization and fusion resulted in a 1.2x increase in training time compared to the original setup. However, for smaller models like T5-large and T5-xl, the training time remained relatively consistent, with no significant impact observed from the inclusion of reasoning components.


%This can be explained theoretically: assuming that the number of training iterations for the vanilla training is \(N'\), the total number of iterations for our method is \(N + (N/Q)\), where \(N/Q\) corresponds to the iterations of the outer loop. Thus, the increase in iteration count is minimal compared to the traditional approach. Additionally, the variables used in the knowledge localization phase are derived from the gradients produced during normal training, requiring no extra computation time. The knowledge fusion phase only involves simple univariate calculations. As a result, the overall time complexity does not increase significantly.

\begin{table}[t]
\centering
\scalebox{0.65}{
\begin{tabular}{lrrrr}
\toprule
Training Time \\ (Min/Epoch) & T5-large & FlanT5-XL & LLaMA2-7B & LLaMA2-13B  \\
\midrule

\rule{0pt}{4pt}LoRAReplay	   &1.4 & 1.4 & 4.5 & 6.6	 \\
\rule{0pt}{8pt}O-LoRA	   &1.4 & 1.4 & 4.5 & 6.7	 \\
\rule{0pt}{8pt}VR-MCL	   &1.5 & 1.8 & 6.0 & 10.2	 \\
\rule{0pt}{8pt}TaSL	   &1.4 & 1.4 & 4.6 & 6.7	 \\
\rule{0pt}{8pt}{\ouralg}   &1.4 & 1.6 & 5.5 & 9.1 \\

\bottomrule
\end{tabular}}
\caption{Training time comparison across backbones.}
\label{tbl:time}
\end{table}


\subsection{Effect of the Different Importance Metric in Knowledge Identification}
We compare two alternative importance scoring approaches with Eq. (\ref{eq:1}): 
(i) using absolute gradients~\cite{michel2019sixteen}, $\left|\nabla_{w_{ij}} \mathcal{L}\right|$, instead of the gradient-weight product; and (ii) removing exponential moving average, relying only on importance scores computed from a single batch.

As shown in Table \ref{tbl:ablation_ipt}, our method with exponential smoothing outperforms the alternatives, with performance drops of up to 2.0\% and 1.3\% without smoothing. Similarly, using absolute gradients leads to lower performance compared to the gradient-weight product, underscoring the effectiveness of our approach in enhancing knowledge identification and model performance.

\begin{table}[h]
\centering
\scalebox{0.9}{
\begin{tabular}{lcc}
\toprule
Method & OP &  BWT\\
\midrule
\rule{0pt}{6pt} $I\left( \cdot \right) = \left|\nabla_{w_{i j}} \mathcal{L}\right|$  & 74.5 & -5.2\\
\rule{0pt}{8pt}  $I\left( \cdot \right) = \left|w_{i j} \nabla_{w_{i j}} \mathcal{L}\right|$ & 75.9 & -4.7 \\

\rule{0pt}{8pt} {\ouralg} (ours) & \textbf{77.9} & \textbf{-3.4} \\

\bottomrule
\end{tabular}}
\caption{Ablation study. Evaluating the impact of different importance metrics on knowledge identification.
}
\label{tbl:ablation_ipt}
\end{table}


\section{Dataset Statistics}
\label{sec:dataset}
Table \ref{long-sequence} show details of the datasets we used for our experiments, along with their evaluation metrics \cite{wang2023orthogonal, feng2024continual, xu2023seqcare}. 
%feng2023towards
For the Long Sequence benchmark, this includes five tasks from the standard CL benchmark (AG News, Amazon reviews, Yelp reviews, DBpedia and Yahoo Answers), four from GLUE benchmark (MNLI, QQP, RTE, SST2), five from SuperGLUE benchmark (WiC, CB, COPA, MultiRC, BoolQ), and the IMDB movie reviews dataset.
%\subsection{Task Sequence Orders}
We report 6 different task orders used for our experiments in Table \ref{order}.
Table \ref{prompt} shows prompts for different tasks. NLI denotes natural language inference \cite{lu2021getting}, including MNLI, RTE and CB. SC denotes sentiment analysis, including Amazon, Yelp, SST-2 and IMDB. TC denotes topic classification, including AG News, Dbpedia and Yahoo.





\begin{table*}[htbp]
\centering
\scalebox{0.8}{
\begin{tabular}{lllll}
\toprule
\textbf{Dataset name} & \textbf{Category} & \textbf{Task}             & \textbf{Domain}     & \textbf{Metric} \\ \midrule
1. Yelp               & CL Benchmark      & sentiment analysis        & Yelp reviews        & accuracy        \\
2. Amazon             & CL Benchmark      & sentiment analysis        & Amazon reviews      & accuracy        \\
3. DBpedia            & CL Benchmark      & topic classification      & Wikipedia           & accuracy        \\
4. Yahoo              & CL Benchmark      & topic classification      & Yahoo Q\&A          & accuracy        \\
5. AG News            & CL Benchmark      & topic classification      & news                & accuracy        \\
6. MNLI               & GLUE              & natural language
inference                       & various             & accuracy        \\
7. QQP                & GLUE              & paragraph detection       & Quora               & accuracy        \\
8. RTE                & GLUE              & natural language inference                       & news, Wikipedia     & accuracy        \\
9. SST-2              & GLUE              & sentiment analysis        & movie reviews       & accuracy        \\
10. WiC               & SuperGLUE         & word sense disambiguation & lexical databases   & accuracy        \\
11. CB                & SuperGLUE         & natural language
inference                       & various             & accuracy        \\
12. COPA              & SuperGLUE         & question and answering                        & blogs, encyclopedia & accuracy        \\
13. BoolQA            & SuperGLUE         & boolean question and answering                & Wikipedia           & accuracy        \\
14. MultiRC           & SuperGLUE         & question and answering                        & various             & accuracy        \\
15. IMDB              & SuperGLUE         & sentiment analysis        & movie reviews       & accuracy        \\ \bottomrule
\end{tabular}}
\caption{The details of 15 datasets used in our CL experiments. First five tasks
correspond to the standard CL benchmark, all other tasks are used in long-sequence experiments.
}
\label{long-sequence}
\end{table*}


\begin{table*}[h]
\centering
\begin{tabular}{lll}
\hline
\textbf{Order} & \textbf{Model} & \textbf{Task Sequence}                                                                                                                                \\ \hline
1              & T5, LLaMA      & dbpedia → amazon → yahoo → ag                                                                                                                         \\
2              & T5, LLaMA      & dbpedia → amazon → ag → yahoo                                                                                                                         \\
3              & T5, LLaMA      & yahoo → amazon → ag → dbpedia                                                                                                                         \\ \hline
4              & T5             & \begin{tabular}[c]{@{}l@{}}mnli → cb → wic → copa → qqp → boolqa → rte → imdb →\\ yelp → amazon → sst-2 → dbpedia → ag → multirc → yahoo\end{tabular} \\
5              & T5             & \begin{tabular}[c]{@{}l@{}}multirc → boolqa → wic → mnli → cb → copa → qqp → rte\\ → imdb → sst-2 → dbpedia → ag → yelp → amazon → yahoo\end{tabular} \\
6              & T5             & \begin{tabular}[c]{@{}l@{}}yelp → amazon → mnli → cb → copa → qqp → rte → imdb →\\ sst-2 → dbpedia → ag → yahoo → multirc → boolqa → wic\end{tabular} \\ \hline
\end{tabular}
\caption{Six different orders of task sequences used for continual learning experiments. Orders 1-3 correspond to the standard CL becnhmark adopted by prior works. Orders 4-6 are long-sequence orders spanning 15 tasks, following \cite{razdaibiedina2023progressive}.}
\label{order}
\end{table*}


\begin{table*}[h]
\centering
\begin{tabular}{cl}
\hline
\textbf{Task}                                                       & \multicolumn{1}{c}{\textbf{Prompts}}                                                                                                                                  \\ \hline
NLI                                                                 & \begin{tabular}[c]{@{}l@{}}What is the logical relationship between the "sentence 1" and the "sentence 2"? \\ Choose one from the option.\end{tabular}                \\ \hline
QQP                                                                 & \begin{tabular}[c]{@{}l@{}}Whether the "first sentence" and the "second sentence" have the same meaning? \\ Choose one from the option.\end{tabular}                  \\ \hline
\begin{tabular}[c]{@{}c@{}}SC\end{tabular}   & What is the sentiment of the following paragraph? Choose one from the option.                                                                                         \\ \hline
\begin{tabular}[c]{@{}c@{}}TC\end{tabular} & What is the topic of the following paragraph? Choose one from the option.                                                                                             \\ \hline
BoolQA                                                              & \begin{tabular}[c]{@{}l@{}}According to the following passage, is the question true or false? Choose one \\ from the option.\end{tabular}                             \\ \hline
MultiRC                                                             & \begin{tabular}[c]{@{}l@{}}According to the following passage and question, is the candidate answer true \\ or false? Choose one from the option.\end{tabular}        \\ \hline
WiC                                                                 & \begin{tabular}[c]{@{}l@{}}Given a word and two sentences, whether the word is used with the same sense \\ in both sentence? Choose one from the option.\end{tabular} \\ \hline
\end{tabular}
\caption{Instructions for different tasks.}
\label{prompt}
\end{table*}


\section{Implementation Details}
\label{sec:details}
Experiments are implemented using PyTorch and the Transformer library, running on a single NVIDIA A100 GPU with 80GB memory. The following hyperparameters are used:

\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=0pt,parsep=0pt]
\item \textbf{T5-large (770M)} and \textbf{FLAN-T5-XL (3B)}: 
Learning rate of 3e-4 for both loops, inner and outer batch sizes of 8, max input length 512, max target length 128, and 10 epochs. LoRA settings: $r = 8$, $\alpha = 32$, dropout = 0.05, targeting modules [q,v]. Testing: max new tokens = 128.

\item \textbf{LLaMA-2 (7B)} and \textbf{LLaMA-2 (13B)}:
Learning rate of 3e-4 for both loops, inner and outer batch sizes of 64, cutoff length 512, and 10 epochs. LoRA settings: $r = 8$, $\alpha = 32$, dropout = 0.05, targeting modules [q\_proj,v\_proj]. Testing: temperature = 0.02, top\_p = 0, top\_k = 1, num\_beams = 1, max new tokens = 128.

\end{itemize}

It is worth noting that we used the same hyperparameters across different datasets and backbones, demonstrating the generalizability of our method without requiring extensive hyperparameter tuning for each specific setting.


\section{Algorithm}
\label{sec:alg}
In this section, we provide the detailed implementation of {\ouralg} algorithm (see Algorithm~\ref{alg:my_algorithm}).

\begin{algorithm*}[t]
\caption{The Algorithm of the proposed {\ouralg}}\label{alg:my_algorithm}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE Current task dataset $\mathcal{D}_k$, memory buffer $\mathcal{M}_{<k}$, model weights $\theta$, initial inner-loop learning rate $\beta_{in}$, outer-loop learning rate $\beta_{out}$, number of inner-loop steps $Q$, hyperparameters $\alpha_1, \alpha_2$, total training step $N$.

\STATE \textit{\# training iterations.}
\FOR {$b$ = $1,\ldots,N$}
    \STATE sample training data $\epsilon_{k}^q$ ($q = 1,\ldots,Q$) from $\mathcal{D}_k$
    \STATE $\theta_{b(0)} = \theta_b$
    \STATE \textit{\# inner loop.}
    \FOR{$q = 1$ \TO $Q$}
        \STATE obtain batch samples $\epsilon_{k}^q$
        \STATE $\theta_{b(q)}=\theta_{b(q-1)}-\beta_{in} \nabla \mathcal{L}\left(\theta_{b(q-1)}\right)$
        \STATE \textit{\# knowledge identification.}
        \STATE compute the importance $\bar{I}\left(w_{i j}\right)$ via Eq. (\ref{eq:1});
        \STATE update $I_{b(q)}$ via Eq. (\ref{eq:I})
        
    \ENDFOR
    \STATE calculate $\tau_b^{\text{in}} = \theta_{b(Q)} - \theta_{b(0)}$ and obtain $I^{in}_b$
    \STATE \textit{\# outer loop.}
    \STATE sample memory data $\phi_{b}$ from $\mathcal{M}_{<k}$
    \STATE $\theta_{b(M)}=\theta_{b(Q)}-\beta_{out} \nabla \mathcal{L}\left(\theta_{b(Q)}\right)$
    \STATE calculate $\tau_b^{\text{out}} = \theta_{b(M)} - \theta_{b(Q)}$ 
    \STATE calculate $I_b^{\text{out}}$ via Eq. (\ref{eq:out})
    \STATE \textit{\# knowledge fusion.}
    \STATE obtain $m_b^{\text{in}}$, $m_b^{\text{out}}$ via Eq. (\ref{eq:mask})
    \STATE update $\theta_{b+1} = \theta_b + (m_b^{\text{in}} \odot \tau_b^{\text{in}} + m_b^{\text{out}} \odot \tau_b^{\text{out}})$
\ENDFOR
%\ENSURE  Trained G and D      
\end{algorithmic} 
\end{algorithm*}



% \begin{algorithm*}[t]
% \caption{The Algorithm of the proposed {\ouralg}}\label{alg:my_algorithm}
% \begin{algorithmic}[1]
% \renewcommand{\algorithmicrequire}{\textbf{Input:}}
% \renewcommand{\algorithmicensure}{\textbf{Output:}}
% \REQUIRE Current task dataset $\mathcal{D}_k$, memory buffer $\mathcal{M}_{<k}$, model weights $\theta$, initial inner-loop learning rate $\beta_{in}$, outer-loop learning rate $\beta_{out}$, number of inner-loop steps $Q$, hyperparameters $\alpha_1, \alpha_2, \alpha_3$, total training step $N$.

% \STATE \textit{\# training iterations.}
% \FOR {$b$ = $1,\ldots,N$}
%     \STATE sample training data $\epsilon_{k}^q$ ($q = 1,\ldots,Q$) from $\mathcal{D}_k$
%     \STATE $\theta_{b(0)} = \theta_b$
%     \STATE \textit{\# inner loop.}
%     \FOR{$i = 1$ \TO $Q$}
%         \STATE obtain batch samples $\epsilon_{k}^i$
%         \STATE $\theta_{b(i)}=\theta_{b(i-1)}-\beta_{in} \nabla \mathcal{L}\left(\theta_{b(i-1)}\right)$
%         \STATE \textit{\# knowledge localization.}
%         \STATE compute the importance $I\left(w_{i j}\right)$ via Eq. (\ref{eq:1});
%         \STATE update $\bar{I}^{(q)}$ via Eq. (\ref{eq:I}) and $\bar{U}^{(q)}$ via Eq. (\ref{eq:U});
        
%     \ENDFOR
%     \STATE calculate $\tau_b^{\text{in}} = \theta_{b(Q)} - \theta_{b(0)}$ 
%     \STATE calculate $I_b^{\text{in}}$ via Eq. (\ref{eq:2})

%     \STATE \textit{\# outer loop.}
%     \STATE sample memory data $\phi_{b}$ from $\mathcal{M}_{<k}$
%     \STATE $\theta_{b(M)}=\theta_{b(Q)}-\beta_{out} \nabla \mathcal{L}\left(\theta_{b(Q)}\right)$
%     \STATE calculate $\tau_b^{\text{out}} = \theta_{b(M)} - \theta_{b(Q)}$ 
%     \STATE calculate $I_b^{\text{out}}$ via Eq. (\ref{eq:out})
%     \STATE \textit{\# knowledge fusion.}
%     \STATE obtain $m_b^{\text{in}}$, $m_b^{\text{out}}$ via Eq. (\ref{eq:mask})
%     \STATE update $\theta_{b+1} = \theta_b + (m_b^{\text{in}} \odot \tau_b^{\text{in}} + m_b^{\text{out}} \odot \tau_b^{\text{out}})$
% \ENDFOR
% %\ENSURE  Trained G and D      
% \end{algorithmic} 
% \end{algorithm*}
