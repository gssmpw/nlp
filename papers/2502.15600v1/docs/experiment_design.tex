\subsection{MLMs assessed for bias}
We analyze four pre-trained MLMs, both base and large, except for distilbert:
%
bert-base-uncased (bert-large-uncased) \cite{devlin-etal-2019-bert}, 
%
roberta-base (roberta-large) \cite{roberta-paper}, 
%
albert-base-v2 (albert-large-v2) \cite{albert-paper}, 
%
distilbert-base-uncased \cite{Sanh2019DistilBERTAD}.
%
All models used are the uncased versions\footnote{All models except RoBERTa use WordPiece tokenization; thus we use the uncased version. RoBERTa uses Byte-Pair Encoding, supporting both cased and uncased text inherently.}. Note that both ALBERT models are uncased by default. We provide input text in lower case for all models for consistency. We implement MLMs using Hugging Face on NVIDIA Tesla P100 PCIE (16GB) GPU. Each MLM model took about 3 hours on average per trait.