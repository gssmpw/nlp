\section{Methodology}\label{measuring-association}

\noindent We follow the standard template-based approach to estimate bias in MLMs \cite{gallegos2023bias,delobelle2022measuring,stanczak2021survey}.
%
A template is a sentence structure with two variables representing a \textit{demographic} attribute word ($A$) and a \textit{domain} target word ($T$), along with other words. 
%
Here, the attribute is gender, and the target is human traits (character/personality). 
%
Templates are used to derive probe sentences ($S_1$, $S_2$,..., $S_n$).
%
Bias is assessed by analyzing the MLM estimates of the association between attribute and target words in probe sentences.  

\vspace{0.5em}
\noindent\textbf{Measuring association:} We follow the  approach of \citealp{kurita-etal-2019-measuring}. 
%
Briefly, we mask the attribute $A$ in a probe sentence ($S_{\text{masked}}^{(A)}$), provide it as input to the MLM, and obtain the likelihood\footnote{While recognizing that this is actually a pseudo-likelihood \cite{salazar-etal-2020-masked}, we use the common approach of using it as a proxy for likelihood.} of the attribute ($p_{A}$), i.e., 
%
$p_{A} = p_{\text{MLM}}(\text{[MASK]}=A|S_{\text{masked}}^{(A)};\theta$), where $\theta$ represents the MLM's parameters.
%
However, since the likelihood of predicting different attribute values could differ even in the absence of a target, we also compute the `implicit prior bias' across attribute values. 
%
To do this, we mask both attribute and target and then obtain the likelihood of the attribute value ($S_{\text{masked}}^{(A,T)}$).
%
We refer to this as $p_{\text{prior}} = p_{\text{MLM}}(\text{[MASK]}=A|S_{\text{masked}}^{(A,T)};\theta)$.
%
Association score ($\text{association}_{\text{score}}$) is $\log \left( \frac{p_{A}}{p_{\text{prior}}} \right)$.
%
Where the MLM splits attribute word into multiple tokens, we take the product of the likelihood of sub-tokens, as commonly practiced \cite{
% fadeeva-etal-2024-fact,
shahriar-barbosa-2024-improving,ahn-oh-2021-mitigating}.

\noindent \textbf{Masking example:} 

\noindent $S_{i}$: "The lady is known for her empathy."

\noindent $S_{i,\text{masked}}^{(A)}:$  "The [MASK] is known for [MASK] empathy." 

\noindent $S_{i,\text{masked}}^{(A,T)}:$ "The [MASK] is known for [MASK] [MASK]."

\noindent Note that we also mask gendered pronouns (e.g., `her') to prevent leakage of gender information. For such cases, while computing $p_{A}$ and $p_{\text{prior}}$, we consider the likelihood of the attribute word in the first [MASK] position only (e.g. see dataset description of \citet{bartl-etal-2020-unmasking} paper).


\subsection{Templates}\label{template-sentences}

\noindent Templates provide the skeletal structure for probe sentences. Clearly, one can only consider a sample of all possible templates 
% kurita-etal-2019-measuring
\cite{limisiewicz-marecek-2022-dont,ahn-oh-2021-mitigating,bartl-etal-2020-unmasking}.
%
The dominant approach has been manual template design \cite{doughman2023fairgauge,felkner-etal-2023-winoqueer,mei2023bias,delobelle2022measuring}
%gallegos2023bias
%
with a few exceptions such as  \citet{guo2022auto,shin2020autoprompt,liang-etal-2020-towards}.

We select templates using a semi-automatic process designed to capture common expressions of human traits using Wikipedia\footnote{Our approach is somewhat analogous to research on bias in auto-regressive language models where Wikipedia sentences are truncated semi-automatically to create prompts \cite{lucy2021gender,dhamala2021bold}.} and GPT-4
(see Appendix \ref{template-selection-overview} for an overview). 
%
We have two template types. 
\textit{Direct} explicitly include the word \textit{personality}, and \textit{Indirect} do not.
The idea is to see if the word \textit{personality} guides the model more effectively. 
%
E.g., \textit{clean} may then be more easily perceived as representing a personality trait instead of its more common meaning of physical cleanliness.


Our 6 templates (Table \ref{tab:templates}) align with the common practice of using 2-5 templates 
% kurita-etal-2019-measuring
\cite{steed2022upstream,limisiewicz-marecek-2022-dont,bartl-etal-2020-unmasking,qian2019reducing}.  
%
However, unlike previous research, we do not assume that all templates are equal for bias detection.


\vspace{0.5em}
\noindent\textbf{Attributes and targets:} Attributes are 94 pairs of gender-denoting words adapted from \citet{kaneko-bollegala-2021-debiasing} and
listed in Table \ref{tab:attributes}.
%
Targets are character trait words \cite{cawley2000virtues} listed in Table \ref{tab:targets_virtue} and personality trait words \cite{goldberg1992development} listed in Table \ref{tab:targets_big_five}. Tables are in the Appendix \ref{attribute-values} and \ref{target-values}.
%
Again, we handle variations across target words as random effects in our model.

\vspace{0.5em}
\noindent\textbf{Sentence generation from templates:}  We also pay particular attention to selecting the appropriate article (limited to \textit{a/an}) and determiner (\textit{the}) or pronoun (limited to \textit{my, your, our, their}) to form wholesome sentences. 
%
To avoid arbitrariness, we do this by estimating the psuedo-perplexity of a sentence. Using the MLM for which we are conducting bias detection, we select the candidate (e.g., pronoun) with the least perplexity for each gender.
%
When selections differ across genders, e.g., \textit{my father} and \textit{your mother}, we add the alternatives \textit{your father} and \textit{my mother} for balance. 

 
Across templates, the average sentence count ranges from 1,447 to 4,119 for character traits and 1,437 to 1,757 for personality traits, with a small coefficient of variation 
of 3.5\% to 10.8\%. Since sentence selection is MLM specific, the numbers can vary within each template and trait dimension.  Thus, we provide averages to ensure a consistent overview in Table \ref{tab:num_sentences_dist} in Appendix.

\begin{table*}[htbp]
    \scriptsize
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{p{0.8cm}p{0.3cm}p{12.3cm}@{}}
    \toprule
    \textbf{Category} & \textbf{TID} & \textbf{Templates} \\
    \midrule
    \multirow{4}{*}{Indirect}    & $\text{t}_1$    & {[}DET/PRONOUN{]} {[}attribute{]} is {[}ARTICLE{]} {[}target{]} person. e.g. my father is a cautious person. \\
                          \cmidrule{2-3}  & $\text{t}_2$     & {[}DET/PRONOUN{]} {[}attribute{]} is {[}target{]}. e.g. my mother is cautious.\\
                                
    \midrule
    \multirow{10}{*}{Direct}       & $\text{t}_3$     & {[}DET/PRONOUN{]} {[}attribute{]} possesses {[}ARTICLE{]} {[}target{]} personality. \\ 
    & & e.g. my father possesses a cautious personality.   \\
  
                                        
                                \cmidrule{2-3}  & $\text{t}_4$      & {[}DET/PRONOUN{]} {[}attribute{]} is known for {[}PRONOUN{]} {[}target{]} personality. \\ & & e.g. she is known for her cautious personality. \\
                                   
                           \cmidrule{2-3}
                                & $\text{t}_5$       & People admire {[}DET/PRONOUN{]} {[}attribute{]} because of {[}PRONOUN{]} {[}target{]} personality. \\
                                &    & e.g. people admire him because of his cautious personality. \\
                                   
                                 
                                       
                        \cmidrule{2-3}        & $\text{t}_6$       & {[}DET/PRONOUN{]} {[}attribute{]}'s {[}target{]} personality is valued at {[}PRONOUN{]} work. \\ 
                         &       & e.g. the woman's cautious personality is valued at her work.\\
                               
    \bottomrule                                   
    \end{tabular}
    }
    \captionsetup{justification=centering}
    \caption{
    \footnotesize
    Templates (TID: template id, attribute: gendered-words, target: character trait words/personality trait words (above examples use character trait words), Determiner (DET): the,  PRONOUN: my, your, our, their)}
    \label{tab:templates}
\vspace{-4mm}
\end{table*}
\raggedbottom

\subsection{Linear mixed model configuration} \label{model-configuration}

\noindent 
In our mixed effect model \cite{baayen2008mixed}, represented below, \textit{gender} (values: \textit{male} or \textit{female}) is a fixed effect (predictor) and association score ($\text{association}_{\text{score}}$) the response variable.
%
Unlike prior research, we account for  variability across  templates and trait words
as random effects represented using standard notation (1|random\_effect).
%
These make the statistical estimates more generalizable, which is a critical feature of our methodology. We use the lme4 \cite{lme4} package in R to fit the mixed models.
%
This package incorporates and estimates the influence of both fixed and random effects in a statistically robust manner.


Besides structure, sentences derived from templates can also differ in their popularity. Thus, we weigh each sentence using pseudo-perplexity \cite{salazar-etal-2020-masked}.
%
Specifically, we give higher weights to sentences with lower pseudo-perplexity calculated using the same MLMs being analyzed for bias.
%
This weight is introduced during model fitting, adjusting residual variance rather than directly modifying the association score. The overall linear mixed-effect model is as follows:

\vspace{0.5em}

\noindent $\text{model}_{\text{lme}}$: $\text{association}_{\text{score}}$ $\sim$ gender $+$ (1 $|$ template) + (1 $|$ trait\_words)

\vspace{0.2em}
where, weight = 1 / (sentence pseudo-perplexity)

\vspace{0.5em}
Pseudo-perplexity is computed by masking one word at a time in the sentence and obtaining the likelihood of the original word. It is the exponential of sum of logs of losses in predicting original words. Following common notation we refer to this as (an estimate of) perplexity. 
 
\subsection{Bias assessment}\label{bias-assessment}

\textbf{Bias score:} This score is given by the coefficient of the \textit{gender} variable in the model. It represents the difference in association scores between genders across targets and templates.
%
A positive (negative) bias score refers to bias against females (males).


We make robust conclusions as follows.
First, we test significance of bias score (95\%, using Welch's t-test \cite{welch1947generalization}). While somewhat common practice  
(e.g., 
% kurita-etal-2019-measuring
\citet{koksal2023language, bartl-etal-2020-unmasking}), some bias papers do not test significance (e.g., 
% limisiewicz-marecek-2022-dont,ousidhoum2021probing,
\citet{guo2022auto,kaneko2022unmasking,ahn-oh-2021-mitigating}).
%
We also consider effect size, a step rarely taken in the bias literature (e.g., while \citet{dayanik2022analysis,bartl-etal-2020-unmasking,kurita-etal-2019-measuring} measure effect size, \citet{kim2023race,guo2022auto,kaneko2022unmasking,limisiewicz-marecek-2022-dont} do not).
% \cite{sullivan2012using}. 
Effect size measures the magnitude of differences \emph{while accounting for variability within each gender group}.
%
In contrast raw score differences disregard within-group variability. Thus, we consider bias score, its significance \emph{and} effect size.

\vspace{0.5em} 
\noindent \textbf{Effect size measurement:} 
We choose $R^2$ over  Pearson correlation as our measure because $R^2$ accounts for relationships involving random effects, unlike Pearson correlation \textit{r}.
%
Since our main goal is to analyze bias across gender, we focus on $R^2$ for the \textit{gender} attribute only.
%
Assuming significance at 95\%, the higher the $R^2$ the more important gender is in explaining differences in association scores.
%
We follow $R^2$ interpretation guidelines provided by \citet{cohen1988statistical}: very small: [0, 0.01), small: [0.01, 0.09), medium: [0.09, 0.25), large: [0.25, 0.64), very large: [0.64, 1.0].
%
To understand the relative magnitude of small effect size, i.e., whether it is closer to medium or very small, we further break down it into three groups {\scriptsize $\triangledown$}: [0.01, 0.03),
%
{\scriptsize $\vartriangle$}: [0.03, 0.06),
and {\scriptsize $\blacktriangle$}: [0.06, 0.09).
We annotate medium to very large effect as $*$.


\vspace{0.5em}
\noindent \textbf{$R^2$ confidence intervals (CI):} %
We conduct 1000 
parametric bootstrap iterations.
%
In each,  
we sample with replacement to create a new dataset of the same size. 
%
The resulting 1000 $ R^2$ values are used to estimate the confidence intervals using the partR2 library \cite{stoffel2021partr2}. 

\vspace{0.5em}
\noindent \textbf{Determination of Bias:} As is standard, bias scores that are not significant indicate \textit{neutral} or \textit{unbiased} stance. 
In addition, we consider significant bias scores but with effect size, $R^2 < 0.01$, as \textit{unbiased}.
