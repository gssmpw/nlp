\section{CD with Limited Samples} \label{sc3}

\begin{figure*}[t!]
	\begin{center}
        \includegraphics[width = 0.8\textwidth]{Figs/supervision_degree1.png}
	\end{center}
	\caption{Comparison of annotation and data volume in different CD learning paradigms.}
	\label{fig.learning_types}
\end{figure*}

To advance DL in real-world CD applications, numerous studies have been conducted on training DNNs for CD in training sample-limited experimental setups in recent years. Depending on the strength of supervision introduced in the training, sample-efficient learning of CD DNNs can be divided into 4 categories, including semi-supervised (SMCD), weakly supervised CD (WSCD), self-supervised CD (SSCD), and unsupervised CD (UCD). For readers to easily comprehend the supervision strength in different learning settings, Fig.\ref{fig.learning_types} represents the data and annotations required in each taxonomy. For simplicity, some close supervision settings are merged. The few-shot CD and zero-shot CD are incorporated into SMCD and UCD, respectively.

Furthermore, Table.\ref{Table.Strategy} summarizes various learning strategies and techniques in the literature. It is worth noting that many of these strategies can be applied to different supervision conditions. In the following, we elaborate on each supervision category and introduce the commonly used strategies, methodologies, and techniques.

\begin{table*}[htbp]
\centering
    \caption{Overview of the strategies and techniques developed to address data scarcity problem in CD.}
    \resizebox{0.9\linewidth}{!}{%
        \begin{tabular}{l|l|r}
        \toprule
            \textbf{General Strategies} & \textbf{Specific Strategies} & \textbf{Methodologies \& Techniques}  \\
            \hline
            \multirow{6}*{Auxiliary regularization} &  \multirow{2}*{Adversarial regularization} & Entropy adversarial loss\cite{peng2021semicdnet}\\
            \cline{3-3}
            & & Adversarial change masking \cite{wu2023fully}\\
            \cline{2-3}
             & \multirow{4}*{Consistency regularization} & Temporal consistency \cite{ding2022bi, hu2022hypernet}\\
            \cline{3-3}
             & & Image perturbation consistency \cite{bandara2022revisiting} \\
            \cline{3-3}
             & & Feature perturbation consistency \cite{yang2023revisiting} \\
            \cline{3-3}
             & & Perturbation consistency \& sample selection \cite{wang2024stcrnet, zuo2024robust}  \\
            \hline
            \multirow{14}*{Pseudo supervision} & \multirow{5}*{Pseudo Labeling} & Ensemble of multi-scale predictions \cite{shi2023multilayer} \\
            \cline{3-3}
             & & Ensemble of historical predictions \cite{wang2022reliable} \\
            \cline{3-3}
             &  & Ensemble of multi-model predictions \cite{yuan2024dynamically}  \\
            \cline{3-3}
             &  & Ensemble of multi-temporal predictions \cite{ding2024scannet} \cite{zheng2024detail}  \\
            \cline{3-3}
             &  & Ensemble of teacher-student predictions \cite{yang2024ecps} \cite{kondmann2023semisiroc}  \\
            \cline{2-3}
             & \multirow{5}*{Uncertainty filtering} & IoU voting \cite{yuan2024dynamically} \cite{wang2022reliable} \\
            \cline{3-3}
             &   & Entropy measure \cite{wang2022semisupervised} \cite{sun2023semibuildingchange} \\
            \cline{3-3}
             &   & Similarity measure \cite{ding2024scannet} \cite{zhang2019land} \cite{tang2022unsupervised} \\
            \cline{3-3}
             &   & Class rebalancing \cite{hou2023deep} \\
            \cline{3-3}
             &   & Contrastive sampling \cite{wang2022semisupervised} \cite{zhang2024remote} \\
            \cline{2-3}
            &  \multirow{4}*{Pre-detection supervision} & Image algebra methods \cite{chen2019change, Du2019Unsupervised}  \\ 
            \cline{3-3}
            &   & Image transformation methods \cite{Gao2016Automatic, Song2018Change} \\ 
            \cline{3-3}
            &   &  Object-based image analysis \cite{Gong2017Superpixel} \\ 
            \cline{3-3}
            &   & Saliency detection  \cite{Geng2019Saliency} \\ 
            \hline
            \multirow{7}*{Coarse-to-fine refinement} & \multirow{3}*{Change activation mapping} &  Multi-scale CAMs \cite{cao2023multiscale}\cite{lu2024weakly}\\
             \cline{3-3}
             &  & Mutual learning \cite{zhao2024pixellevel} \\
            \cline{3-3}
             &  & GradCAM++ \cite{dai2023siamese} \\
            \cline{2-3}
            & \multirow{4}*{Difference refinement} & Difference clustering \cite{kalita2021land}\\
            \cline{3-3}
            & & Guided anisotropic diffusion \cite{daudt2023weakly} \\
            \cline{3-3}
            & & CRF-RNN \cite{andermatt2021weakly} \\
            \cline{3-3}
            & & Change Masking \& Classification \cite{andermatt2021weakly} \cite{qiao2024revolutionizing} \\
            \hline
            \multirow{11}*{Representation learning} & \multirow{3}*{Graph representation} & Super-pixel graph \cite{saha2021semisupervised, wang2021dynamic, lin2023hyperspectral} \\
            \cline{3-3}
             & & Feature graph \cite{sun2022semisanet} \\
            \cline{3-3}
             & & Difference graph \cite{liu2019semisupervised, tang2022unsupervised} \\
            \cline{2-3}
             & \multirow{3}*{Contrastive learning} &  Data augmentation  \cite{feng2023detection, ou2022hyperspectral, zou2023transformer} \\
             \cline{3-3}
              & & Multiple clues  \cite{jiang2023self, huang2023contrastive, wang2023self, chen2021self, chen2022self, kuzu2024forest, yang2023multicue, qu2023tdsscd}  \\
            \cline{3-3}
              & & Pseudo label contrast \cite{9538396, adebayo2023detecting, zong2024multi}  \\
            \cline{2-3}
             & \multirow{3}*{Masked image modeling} & Large-scale MIM \& fine-tuning \cite{sun2022ringmo}\cite{cui2023hybrid} \\
            \cline{3-3}
             & & Contrastive mask image distillation \cite{muhtar2023cmid} \\
            \cline{3-3}
             & & Multi-modal MIM \cite{zhang2023self} \\
            \cline{2-3}
             & \multirow{4}*{Generative representation}  &  Autoencoder and its variants \cite{Zhange2016mapping, Liu2019Stacked, Chen2022Unsupervised} \\
            \cline{3-3}
             &  & Deep belief networks \cite{Gong2016Change, Zhao2017Discriminative} \\
            \cline{3-3}
             &  & Generative adversarial networks \cite{Lei2021Spectral, wu2023fully} \\
            \cline{3-3}
             &  & Denoising diffusion probabilistic models \cite{bandara2024ddpmcd} \\
            \hline
            \multirow{5}*{Augmentation} &  \multirow{3}*{Image augmentation} & Background-mixed augmentation \cite{huang2023backgroundmixed} \\
            \cline{3-3}
             & & Pseudo change pair generation \cite{zheng2021change}\cite{gao2024building}\\
            \cline{3-3}
             & & Patch exchange \cite{chen2023exchange}\cite{gao2024building}\\
            \cline{2-3}
             & \multirow{2}*{Change augmentation} & Object masking \& inpainting \cite{quan2023unified}\cite{seo2023selfpair} \\ %
            \cline{3-3}
             & & Change instance generation \cite{zhu2023data}\cite{zheng2023scalable}\\
            \hline
            \multirow{4}*{Leveraging external knowledge} & \multirow{2}*{Leveraging VFMs} & Fine-tuning VFMs \cite{ding2024samcd, li2024new}  \\
            \cline{3-3}
             &  & Prompt learning \cite{zheng2024segment} \\
            \cline{2-3}
             & \multirow{2}*{Transfer learning} &  Classifying VGGNet features \cite{saha2019unsupervised, Saha2022Patch} \\
            \cline{3-3}
             & ~ &  Metric learning \cite{bandara2023deep, liu2020convolutional} \\
        \bottomrule
        \end{tabular} \label{Table.Strategy} }
\end{table*}

\subsection{Semi-supervised CD}
Semi-supervised learning presupposes the availability of only a limited volume of labeled data for training. In scenarios where labeled samples are extremely scarce, this paradigm transitions into the domain of few-shot change detection. This necessitates intrinsic learning of the change patterns that can be generalized across diverse instances of change.

\textbf{Pseudo Labeling:} Pseudo labeling allows a DNN to generate pseudo labels for unlabeled data based on its predictions, thus effectively augmenting the training dataset. In segmentation-related tasks, pseudo labels can be obtained by thresholding the predictions of DNNs.

Since single DNN predictions may contain many errors, various methods combine multiple predictions to enhance the robustness of pseudo-labeling. In \cite{shi2023multilayer} pseudo labels are obtained by composing and voting multi-scale predictions. In \cite{wang2022reliable}, historical models are used during training to produce ensemble predictions. By calculating the mean Intersection over Union (IoU) in historical predictions, the reliable results are selected as pseudo labels to train the unlabeled data. The method in \cite{yuan2024dynamically} utilizes multiple DNNs to produce multiple predictions and also performs IoU calculations to generate reliable labels. In \cite{ding2024scannet} a cross-temporal pseudo-labeling technique is introduced. The semantic similarity between multitemporal predictions is calculated to select the high-confident pixels. In \cite{yang2024ecps}, a sophisticated cross-pseudo supervision method is proposed within the Teacher-Student (TS) learning paradigm. The knowledge learned in a teacher model is distilled to supervise the student models, and the predictions of multiple student models are composed to generate reliable pseudo labels. Kondmann et al. \cite{kondmann2023semisiroc} employ an unsupervised method as the teacher model, subsequently train and fine-tune different CD models with pseudo labels from the teacher model. In \cite{zhan2023s}, the method employs superpixel segmentation to create objects and enable self-supervised learning through object overlaps in bitemporal images. It produces and integrates multiscale object-level and pixel-level difference images and utilizes temporal prediction for SSCD.

The essence of pseudo-labeling is minimizing the errors and uncertainty in generated labels while enhancing guidance for critical cases. Therefore, it is important to measure the certainty of DNN predictions. If the pseudo labels are generated by multiple methods, the number of votes can be deemed the confidence score \cite{kondmann2023semisiroc}. In \cite{yuan2024dynamically} and \cite{wang2022reliable} the certainty is measured through IoU in multiple predictions. For a single DNN prediction on unlabeled data, low entropy indicates high confidence, and entropy-based objectives are commonly used to filter uncertain predictions \cite{wang2022semisupervised} \cite{sun2023semibuildingchange}. In \cite{zhang2019land} similarity measures and uncertainty calculations are combined to map the pseudo CD labels. To improve the guidance for minor classes (i.e., changes), Hou et al. \cite{hou2023deep} cluster the extracted deep features to generate pseudo labels and rebalance the \textit{change/non-change} instances in pseudo labels to strengthen the learning of minority class (i.e., \textit{changes}). Furthermore, uncertain predictions also contain potential knowledge. In a contrastive learning paradigm, reliable and unreliable pixels can be sampled as positive and negative samples, thus improving the representation of temporal semantic features \cite{wang2022semisupervised, zhang2024remote}.

\textbf{Auxiliary regularization:}
To facilitate training on unlabeled data, a common strategy is to introduce auxiliary training objectives or regularization. This can constrict the optimization landscape and regularize DNNs to learn noise-resistant change representations. In \cite{ding2022bi} Ding et al. propose a temporal similarity regularization to optimize learning of temporal semantics in SCD. This objective drives DNNs to embed similar features in unchanged areas and different semantics in changed areas. In \cite{ding2024samcd} it is extended with temperature regularization to model the implicit semantic latent in the BCD. In \cite{zheng2024detail} temporal regularization is implemented in the form of mutual supervision with pseudo labels.  In \cite{hu2022hypernet} a focal cosine loss is designed to align feature representations in unchanged areas for SSCD of hyperspectral images. It assigns greater weights to hard positive samples to emphasize the learning of critical samples. 

In \cite{peng2021semicdnet} adversarial learning is introduced to align the feature distributions of unlabeled data with the labeled data, thus promoting GT-like results. In \cite{dong2020self}, adversarial learning is introduced to learn consistent feature representations in bitemporal images. The CD results are then derived by clustering the different features.

\begin{figure}[t!]
	\begin{center}
        \includegraphics[width = 0.5\textwidth]{Figs/ConsistencyRegularization.png}
	\end{center}
	\caption{Consistency regularization for WSCD \cite{bandara2022revisiting}. Random perturbations are applied to the change representations, and a consistency loss is calculated between the origninal and perturbed CD results to improve the robustness of CD models.}\label{fig.CR}
\end{figure}

Among auxiliary regularization-based approaches, Consistency Regularization (CR) is an effective strategy to enhance the model generalization. CR applies spatial or spectral perturbations to unlabeled data, training the model to reduce discrepancies between varying perturbations of the same image \cite{sohn2020fixmatch}.

Bandara et al. \cite{bandara2022revisiting} first introduce CR to WSCD, and extend perturbations from images to feature differences. A paradigm for CR is proposed in the context of WSCD, which involves different types of perturbations, such as random feature noising, random feature drop, feature cutout, and instance masking. Similarly, Yang et al. \cite{yang2023revisiting} extend the CR paradigm with dual stream feature-level perturbations, which greatly improves the generalization even with a very small proportion of training samples. A simplified paradigm of this CR learning under a teacher-student knowledge distillation framework is illustrated in Fig.\ref{fig.CR}.

Building on top of the CR paradigm, many literature methods investigate to improve WSCD through advanced DNN designs and sample selection mechanisms. In \cite{zhang2023joint}, rotation augmentation is introduced in CR-based WSCD, and class-wise uncertainties are calculated to alleviate the class imbalance issue. Wang et al. \cite{wang2024stcrnet} introduce a reliable sample selection mechanism that selects samples with stable historical predictions during training. In \cite{han2024c2fsemicd}, a coarse-to-fine CD network with multiscale attention designs is designed as the backbone for CR-based WSCD. In \cite{zuo2024robust} selection, trimming and merging of reliable instances is performed to enhance the robustness of extracted change instances. Hafner et al. consider multi-modal data as different views of the same regions and employ CR across different modalities to learn robust built-up changes \cite{hafner2023semisupervised}.

\textbf{Graph Representation:} Graph neural networks (GNNs) are a family of DNNs that are adept at modeling relationships. Since GNNs can be trained with partial labels, they are well suited to semi-supervised learning settings \cite{liu2019semisupervised, saha2021semisupervised}. A crucial step in graph learning is graph construction. The literature methods can be categorized into superpixel-based \cite{saha2021semisupervised, wang2021dynamic, lin2023hyperspectral}, feature-based \cite{sun2022semisanet}, and difference-based \cite{liu2019semisupervised, tang2022unsupervised} graph construction. 

Liu et al. \cite{liu2019semisupervised} first introduced graph learning in the context of SMCD. The differences between temporal features are calculated to construct change graphs, while adversarial learning is also introduced to train the graphs constructed with unlabeled data. Saha et al. construct change graphs with multi-temporal parcels, and propagate change information from labeled parcels to unlabeled ones through training iterations \cite{saha2021semisupervised}. Tang et al. \cite{tang2022unsupervised} employ a multi-scale Graph Convolutional Network (GCN) to capture long-range change context and generate pseudo labels with similarity metrics. In \cite{wang2021dynamic} a method for dynamic graph construction in SAR image CD is presented. It constructs graphs from three-channel pixel blocks and dynamically updates graph edges based on trained features. The method in \cite{lin2023hyperspectral} combines superpixel graph modeling and pixel-level CNN embedding for SMCD in hyperspectral images. It introduces a graph attention network (GAT) to capture temporal-spatial correlations via an affinity matrix and uses CNN layers to merge features to map changes. In \cite{sun2022semisanet}, GAT is incorporated into a CR learning framework to learn robust multi-temporal graph representations. In \cite{kalinicheva2020unsupervised} graph is employed to represent and cluster the change evolutions for unsupervised TSCD.

\subsection{Weakly supervised CD}
While CD is a fine-grained segmentation task that requires pixel-level annotations, in the weakly supervised learning setting, only coarse-grained labels such as points, surrounding boxes, scribbles, and image categories are available. WSCD enables easy construction of a CD training set, as it does not require intensive human annotation. However, it does not mitigate the scarcity of change samples. 

Most of the WSCD methods utilize image-level labels. The labels indicate either the image categories \cite{cao2023multiscale} or the image pair (\textit{change/nonchange} \cite{dai2023siamese}). Meanwhile, various types of coarse CD labels are also utilized in literature studies, including point labels \cite{fang2023point}, low-resolution labels \cite{zheng2021weakly}, patch-wise labels \cite{qiao2024revolutionizing} and box labels \cite{khan2017forest}. The differences in these supervisions derive different methodologies of utilizing and recovering spatial information. Two major categories of WSCD methodologies that correspond to image-level supervision and coarse CD supervision are change activation mapping and difference refinement, respectively.

% \textbf{Pseudo supervision:} This strategy first utilizes class activation maps to generate coarse pseudo labels, then distill or refine these labels to obtain fine-grained results \cite{cao2023multiscale}. To obtain reliable pseudo labels, in \cite{wang2023cs} the uncertainty in different predictions is calculated. 

\textbf{Change activation Mapping:} 
This strategy is frequently employed in WSCD to parse image-level label into spatial change representations. First, an image encoder is trained with image-level information, then the feature responses in the late layers, i.e., class activation maps (CAMs), are utilized to generate coarse pseudo labels. However, CAMs contain only coarse feature responses and do not indicate fine-grained change details. To improve the accuracy and robustness of CAMs, Cao et al. \cite{cao2023multiscale} ensemble multi-scale CAMs and propose a noise correction strategy to generate reliable pseudo labels. The method in \cite{lu2024weakly} also adopts a multi-scale approach. It extracts more robust and accurate change probability maps through knowledge distillation and multi-scale sigmoid inference, as illustrated in Fig.\ref{fig.CAM_TS}. The method in \cite{zhao2024pixellevel} introduces mutual learning between different time phases. It utilizes CAMs derived from the original image and the affine transformed image to improve the certainty of change mapping and incorporates contrastive learning to enlarge the distance between changed representations and unchanged representations. In \cite{dai2023siamese} GradCAM++ is introduced to weight the multi-scale CAMs. It also leverages multi-scale and transformation consistency regularization to improve the quality of CAMs.

\begin{figure}[t!]
	\begin{center}
        \includegraphics[width = 0.5\textwidth]{Figs/CAM_TS.png}
	\end{center}
	\caption{Refining CAM for SMCD within a teacher-student framework \cite{lu2024weakly}. A CAM is obtained with image-level supervision (class loss), and is refined through knowledge distillation.} \label{fig.CAM_TS}
\end{figure}

\textbf{Difference Refinement:} In comparison to image-level labels, coarse CD labels contain a certain degree of spatial information and thus can be utilized to train a coarse CD model. After mapping the differences, various kinds of techniques are developed to refine and highlight the salient change regions.

Several methods employ conventional machine learning methods to perform the refinement. In \cite{zheng2021weakly}, the refinement is achieved through bitemporal comparison and morphological filtering operations. In \cite{khan2017forest}, a candidate suppression algorithm is designed to reduce the overlapping box candidates and select the most confident candidate regions that indicate changes. In \cite{kalita2021land} temporal features are extracted by contrastive learning, and the mapping from difference image to CD result is achieved through PCA and K-Means algorithms. 

In contrast to refinement on the CD results, several methods refine the labels to perform fine-grained supervision. The method in \cite{lu2020weakly} first calculates a difference map through edge mapping and superpixel segmentation algorithms, then trains a denoising autoencoder to refine the pre-classification results. Fang et al. apply region growth on point labels and DNN predictions to expand the annotations and propose a consistency alignment objective to align the coarse and fine predictions \cite{fang2023point}. In \cite{daudt2023weakly}, the training of a CD CNN and the refinement of the results are carried out iteratively to reduce the errors in the noisy crowd-sourced labels. A guided anisotropic diffusion algorithm is introduced to filter the wrong predictions while preserving the edges.


\begin{figure*}[t!]
	\begin{center}
        \includegraphics[width = 0.8\textwidth]{Figs/ContrastiveLearning.png}
	\end{center}
	\caption{A simplified paradigm of contrastive learning for SSCD \cite{chen2022self}. Croppsed RSIs in the same and different locations construct positive and negative change pairs.}
	\label{fig.Contrastive}
\end{figure*}

Differently from these approaches, the method in \cite{andermatt2021weakly} utilizes object-level class labels to perform WSCD. It first compares image pairs with a Siamese Unet and then masks the changed object to classify its category. To enable accurate masking of the changed object, a CRF-RNN (Conditional Random Fields as Recurrent Neural Network) layer is employed to integrate spatial details from the original image. Similarly to this object-masking approach, the method in \cite{qiao2024revolutionizing} masks and re-segments superpixels as interesting instances (buildings), and utilizes a voting mechanism to classify the changed instances (damaged buildings).


%  The temporal differences are aggregated through multi-scale edge detection, which is then utilized to train a denoising auto-encoder.

\subsection{Self-supervised CD} 

Self-supervised learning exploits the inherent consistency within data to learn sensor-invariant and noise-resilient semantic representations. Leveraging the capability of self-supervised learning, SSCD learns to discriminate temporal variations in unlabeled RSIs. It is worth noting that SSCD can be regarded as a distinct subclass within the broader category of UCD, but they typically require extensive pre-training in the target domain. Additionally, many approaches employ SSCD for pretraining and still require fine-tuning on target datasets.

\textbf{Contrastive Learning:} This strategy constructs and compares positive and negative pairs to exploit the structure and relationships within unlabeled data. In CD, bi-temporal images are often utilized to construct the contrastive pairs. By maximizing the consistency among positive pairs and the difference among negative pairs through contrastive losses, DNNs are trained to exploit feature embeddings that can capture temporal similarities and discrepancies. Fig.\ref{fig.Contrastive} illustrates a simplified paradigm of contrastive learning, where change pairs are constructed with cropped RSIs at the same and different locations. The mapping of pre-trained representations into CD results further categorizes two major types of methods: fine-tuning-based and thresholding-based.

\par The fine-tuning-based methods use CD labels to retrain based on the pre-trained model obtained from self-supervised methods. Common methods utilize data augmentation methods for comparative learning. The results of data augmentation based on the same sample are regarded as positive samples. Feng et al. \cite{feng2023detection} obtain a pre-trained model based on SimSiam and unlabeled samples. Multiple data augmentation methods are often combined to generate positive samples, and then the pre-trained model is directly fine-tuned \cite{ou2022hyperspectral} \cite{zou2023transformer}. In addition to data augmentation, some studies construct contrastive learning by mining multiple clues, such as multi-level contrast and multi-feature contrast. Jiang et al. \cite{jiang2023self} design global-local contrastive learning, where global and local contrastive learning respectively implement instance-level and pixel-level discrimination tasks. Huang et al. \cite{huang2023contrastive}  propose a soft contrastive loss function to improve the inadequate feature tolerance. In the downstream CD fine-tuning task, the features of different receptive fields are captured by a multiscale feature fusion module and combined with a two-domain residual attention block to obtain long-range dependencies on spectral and spatial dimensions. The method in \cite{wang2023self} proposes a multilevel and multi-granularity feature extraction method and applies contrastive learning to obtain the pretrained model. A multilevel CD is performed by fine-tuning the network with limited samples.

\par The thresholding-based methods derive the CD map from dual feature maps using thresholding, thus no labeled samples are used for fine-tuning. Contrastive learning based on multiple clues has also been used in these methods. The method in \cite{chen2021self} pretrains the model using a pseudo-siamese network and multiview images and then generates binary CD maps through feature distance measurement and thresholding. In \cite{chen2022self}, shifted RSI pairs are leveraged to train pseudo-siamese networks, performing pixel-level contrastive learning. Kuzu et al. \cite{kuzu2024forest} employ instance-level (BYOL, SimSiam) and pixel-level (PixPro, PixContrast) methods to derive pre-trained models and directly produce CD maps using DCVA. In \cite{yang2023multicue}, a multicue contrastive self-supervised learning framework is designed. Beyond mere data augmentation, this approach also constructs positive sample pairs from semantically similar local patches and temporally aligned patches. The preliminary change embeddings are then obtained from the affinity matrix. The method in \cite{qu2023tdsscd} first performs contrastive learning on bitemporal RSIs, and then performs contrastive learning on early fusion and late fusion features. 
Meanwhile, pseudo label contrast has also been widely explored, which regards samples with the same class as positive pairs and samples with different classes as negative pairs. Saha et al. \cite{9538396} employ deep clustering and contrastive learning for self-supervised pre-training. Adebayo \cite{adebayo2023detecting} trains a classifier using land cover labels of available years to identify unchanged regions through post-classification comparisons.  The pre-trained model is obtained through the BYOL method based on trusted unchanged regions. He et al. \cite{zong2024multi} employ clustering to obtain pseudo labels (\textit{non-changed, changed,} and \textit{uncertain}). Furthermore, this framework introduces a self-supervised triple loss, including \textit{changed} and\textit{ non-changed} losses based on contrastive learning and an uncertain loss based on image reconstruction.

\textbf{Masked Image Modeling:} Masked Image Modeling (MIM) is a self-supervised reconstructive approach aims at learning generalized representations from extensive volumes of unlabeled data. Within the MIM paradigm, DNNs are trained to reconstruct masked image pixels or patches based on available unmasked image content. However, MIM does not provide task-specific feature representations and typically requires subsequent fine-tuning.

With large-scale pretraining using MIM, Sun et al. \cite{sun2022ringmo} constructed a foundational model for RS scenes and proved its improvements to BCD. Cui et al. \cite{cui2023hybrid} pre-train a network using multi-scale MIM and fine-tune it with labeled data. The model first processes images with convolutional structures and then extracts global information using transformers. The method in \cite{muhtar2023cmid} combines contrastive learning and MIM in a self-distillation way, allowing effective representations with global semantic separability and local spatial perceptibility. Zhang et al. \cite{zhang2023self} propose a multi-modal pretraining framework. The DNNs learn visual representations through MIM, and align them with multi-modal data through contrastive learning. A temporal fusion transformer is also proposed to transfer the pre-trained model to CD.

\begin{comment}
\textbf{Auxiliary supervision:} Apart from contrastive learning and MIM, some literature works design other auxiliary supervisions to facilitate self-supervised change learning. 
In \cite{wang2021attention}, a self-supervised spatial feature extraction network is built with PCA guidance, mapping differences to principal differential components. The attention mechanism weighs and concatenates spatial and spectral features, yielding change results at different positions. Specifically, the changed and unchanged regions are first determined on the basis of the labels. In unchanged regions, the probability map is used as pseudo-labels, while in changed regions, the opposite result is used as pseudo-labels. 
\end{comment}

\begin{figure}[t!]
	\begin{center}
        \includegraphics[width = 0.5\textwidth]{Figs/ChangeAug.png}
	\end{center}
	\caption{The paradigm of semantic change augmentation in \cite{zheng2023scalable}. Post-change RSIs are synthesized with single-temporal images and instance labels.}
	\label{fig.ChangeAug}
\end{figure}


\textbf{Augmentation:} Natural changes are infrequent and registered bitemporal RSIs are difficult to collect. To overcome these limitations, in \cite{huang2023backgroundmixed} a background augmentation method is introduced for image-level WSCD. It augments samples under the guidance of background-exchanged images, enabling the model to learn intricate environmental variations. 

Several literature studies resort to augmenting semantic changes with single-temporal RSIs in segmentation datasets. In \cite{zheng2021change}, pseudo change pairs are constructed by randomly sampling labeled RSIs and mixing their semantic labels. This pseudo supervision is proved to generalize well on CD datasets without fine-tuning. In \cite{chen2023exchange}, Chen et al. propose a simple image patch exchange method to generate pseudo-multi-temporal images and pseudo labels from a wide range of single-temporal HR RSIs, facilitating the training of CD DNNs in a self-supervised manner. In \cite{gao2024building}, patches from other images are cut and pasted to create a pseudo-post-change image.

There are also several literature studies aiming to generate more diverse and realistic change pairs with instance-level augmentations. They commonly utilize instance labels in segmentation datasets to perform the creation or removal of synthetic changes, as illustrated in Fig.\ref{fig.ChangeAug}. For example, Seo et al. \cite{seo2023selfpair} implement copy-pasting or removal-inpainting operations based on the labels of ground objects. Zheng et al. \cite{zheng2023scalable} first synthesize changes by copying or removing objects, then simulate temporal variations using a GAN. Zhu et al. \cite{zhu2023data} generate object segments with a GAN and employ Poisson blending to fuse them into background images. The resulting approach enables a few-shot CD in forest scenes. Quan et al. \cite{quan2023unified} generate pseudo-change pairs by masking the instances in labeled building segmentation datasets. After pretraining on these synthesized datasets, high accuracy is yielded with few amount of labeled data for fine-tuning.

\subsection{Unsupervised CD}

\par UCD eliminates the necessity for prior training, allowing direct deployment of CD algorithms on unlabeled data. This significantly broadens the application scope of DL-based CD, representing a critical objective in the advancement of CD methodologies. However, unsupervised CD presents significant challenges for DL-based frameworks, as the training of DNNs requires task-specific objectives. To address the absence of explicit supervision in UCD, the literature identifies three principal strategies: generative representation, pre-detection supervision, and leveraging external knowledge.

\textbf{Generative Representation:} This approach uses generative models to extract features, eliminating the need for manually labeled data \cite{hong2024multimodal}.

The model extracts feature maps from the original multi-temporal image for pixel-wise comparison to generate a difference map. A distance metric, such as the Euclidean distance, combined with a threshold segmentation algorithm, derives the final CD results. Prevalent deep generative models include auto-encoders (AE), deep belief networks (DBN), generative adversarial networks (GAN) \cite{Goodfellow2014Generative}, and denoising diffusion probabilistic models (DDPM) \cite{Jonathan2020Denoising}.

\par AEs are unsupervised learning models optimized by minimizing reconstruction errors. However, vanilla AEs tend to learn redundant information (e.g., simply replicating the input data) and encounter difficulties in deriving meaningful representations within a single-layer architecture. Consequently, various variants such as stacked AE (SAE), sparse AE, denoising AE (DAE), and variational AE (VAE) have been adapted for CD tasks.

In \cite{Chen2019Fast}, an SAE-based algorithm for CD of HR RSIs employs a sparse representative sample selection strategy to reduce time complexity. Liu et al. \cite{Liu2019Stacked} use an SAE with Fisher's discriminant criterion for high-resolution SAR image CD to better distinguish between changed and unchanged features. In \cite{Hu2021Hyperspectral}, SAE served as a predictor of hyperspectral anomaly CD. Touati \cite{Touati2020Anomaly} designed a multimodal CD (MMCD) framework based on anomaly detection, noting that changed regions often exhibit significant reconstruction losses in sparse AE. Lv et al. \cite{Lv2018Deep} used a contractive AE to minimize noise and extract deep features from superpixels for the SAR image CD. In \cite{Hu2021Hyperspectral}, SAE served as a predictor of hyperspectral anomaly CD. Touati \cite{Touati2020Anomaly} designed an MMCD framework based on anomaly detection, noting that changed regions often exhibit significant reconstruction losses in a sparse AE. Lv et al. \cite{Lv2018Deep} employ a contractive AE to minimize noise and extract deep features from superpixels for the SAR image CD. In \cite{Zheng2022Unsupervised}, a cross-resolution difference learning method involving two coupled AEs was developed for CD across images of varying resolutions.

Since DAEs help reduce the impact of noise on original images, they are widely used in SAR and MMCD \cite{Zhange2016mapping, LiuGQZ18, Zhan2018Iterative, Zhan2018Log}. To mitigate the loss of spatial contextual information typically associated with vectoring operations in conventional AEs, convolutional layers have been incorporated into AEs, resulting in the development of convolutional AEs (CAEs) for CD. Bergamasco et al. \cite{Bergamasco2022Unsupervised} develop a CAE to learn multi-level difference features for multispectral CD. Wu et al. \cite{Wu2022Commonality} add a commonality constraint to CAE for MMCD applications. Furthermore, to address spatial information loss in fully connected AEs, Wu et al. \cite{wu2021unsupervised} propose a kernel principal component analysis (KPCA) convolution feature extraction model. A deep KPCA convolutional mapping network is designed following the layer-wise greedy training approach of SAE for both BCD and MCD in HR RSIs. Chen et al. \cite{Chen2022Unsupervised} present a graph-based framework to model structural relationships for unsupervised multimodal CD. It employs dual-graph convolutional autoencoders to discern modality-agnostic nodes and edges within multimodal images.

\par DBNs are another type of classic unsupervised deep model with multiple layers of restricted Boltzmann machines (RBMs). Like SAE, DBNs are trained using a layer-wise greedy approach, enabling them to extract informative features from input images. Despite their potential, DBNs have seen relatively limited application in CD. Gong et al. \cite{Gong2016Change} utilized pre-trained DBN weights as initial weights for a DNN to perform CD on SAR images. Zhao et al. \cite{Zhao2017Discriminative} designed a DNN composed of two symmetric DBNs to learn the modality-invariant features for MMCD. Jia et al. \cite{JiaChange2021} introduced a generalized Gamma DBN to learn features from different images, and Zhang et al. \cite{Zhang2016Feature} compressed features extracted by DBN into a 2D polar domain for MCD on multispectral images.

\par As a prominent framework for approaching generative AI, GANs have also been widely applied in unsupervised CD. Lei et al. \cite{Lei2021Spectral} apply GANs to learn representative features from hyperspectral images, achieving robust CD results. Saha et al. \cite{Saha2021Unsupervised} develop a GAN-based method to learn deep change hypervectors for CD on multispectral images. Ren et al. \cite{Ren2021Unsupervised} developed a GAN-based CD framework to mitigate the issues caused by unregistered objects in paired RSI. Wu et al. \cite{wu2023fully} propose an end-to-end unsupervised CD framework, jointly training a segmentor and a GAN with \textit{L1} constraints. Noh et al. \cite{noh2022unsupervised} employ GANs for image reconstruction using single temporal images in training and bitemporal images in inference, identifying changed regions by high reconstruction losses. GANs demonstrate exceptional efficacy in MMCD owing to their advanced capabilities in image style transfer. One of the major types of unsupervised MMCD, modality translation methods, predominantly leverages GANs. For instance, Niu et al. \cite{Niu2019Conditional} use a conditional GAN for modality translation between SAR and optical images, obtaining CD results through direct comparison of transformed images. Subsequent advances include sophisticated GAN architectures and training techniques for improved detection accuracy, such as cycle-consistent GAN \cite{Luppino2022Deep,Liu2022Unsupervised}, CutMix \cite{Anamaria2022Generative}, feature space alignment \cite{Luppino2024Code}, and robust fusion-based CD strategies \cite{WangCD2024}. These approaches often incorporate pre-detection techniques to isolate changed regions for more stable modality translation results, aligning with the concepts we will discuss in the following subsection.

\par DDPMs, drawing inspiration from the principles of non-equilibrium thermodynamics, have garnered significant attention in generative artificial intelligence \cite{Jonathan2020Denoising}. These models involve a diffusion process that gradually introduces random noise into the data, followed by a reverse diffusion process to reconstruct the desired data distribution from the noise. Training by reconstructing inputs makes DDPMs naturally suitable for feature extraction in CD tasks. Bandara et al. \cite{bandara2024ddpmcd} first introduced DDPMs for CD. However, subsequent works focus mainly on fully supervised CD (FSCD) \cite{Wen2024GCD}, while studies on UCD with DDPMs remain rare.


\begin{figure*}[t!]
	\begin{center}
        \includegraphics[width = 0.8\textwidth]{Figs/VFM_adapt.png}
	\end{center}
	\caption{The paradigm of leveraging VFM for CD in \cite{ding2024samcd}. VFM parameters are 'frozen' (i.e., not updated), whereas other network modules are trainable to adapt VFMs to the RS domains.}
	\label{fig.VFM_CD}
\end{figure*}

\textbf{Pre-detection Supervision:}\label{sec:pre_detection}
Although unsupervised generative models do not require labeled data to extract features from images for CD, the lack of objectives during the feature extraction process may result in suboptimal and less informative features. Additionally, the absence of labeled data can limit the learning of more advanced DL models. To address these issues, pre-detection-based approaches first generate pseudo labels using traditional unsupervised CD algorithms, then train deep CD models with the pseudo labels. This strategy emulates supervised learning paradigms for training purposes while remaining

entirely unsupervised, as it does not depend on any pre-existing labeled data. Several early DL-based CD methods have adopted this strategy.

\par The effectiveness of pre-detection supervision depends on the accuracy of pre-detection algorithms. Thus, it is crucial to design or select algorithms that suit the characteristics of input images. Synthetic Aperture Radar (SAR) images, in particular, have been extensively studied due to their unique speckle noise. Gao et al. \cite{Gao2016Automatic} developed an automatic CD algorithm using PCANet \cite{Chan2015PCANet}, which employs a Gabor wavelet transform and Fuzzy C-means clustering (FCM) to select the most reliable changed and unchanged samples from SAR images. These samples are then used to train the PCANet. Similarly, Gong et al. \cite{Gong2016Change} proposed a deep neural network-based CD algorithm for SAR images that incorporates a pre-detection algorithm based on FCM to select the most representative samples. In another study, Gong et al. \cite{Gong2017Feature} introduced an unsupervised ternary CD algorithm where deep feature representations are learned from the difference image using an SAE, effectively suppressing image noise. Geng et al. \cite{Geng2019Saliency} integrated saliency detection into CD for SAR images by designing a pre-detection algorithm to select representative and reliable samples for training the deep network. Additionally, Yang et al. \cite{yang2019transferred} combined the concept of transfer learning with pre-detection methods to broaden the application scope of CD in SAR images. Liu et al. \cite{Liu2019Local} proposed a locally restricted CNN that adds spatial constraints to the output layer, effectively reducing noise in Polarimetric SAR (PolSAR) images. This model was also supported by a pre-detection algorithm based on the statistical properties of PolSAR images. 

\par Methods tailored for multispectral, hyperspectral, and high-resolution images have also been developed. Gong et al. \cite{gong2017generative, 2019GenerativeGong} leveraged the initial difference image generated by the CVA to provide a priori knowledge for sampling training data for GANs. Shi et al. \cite{Shi2022Unsupervised} extended this approach to MCD. Du et al. \cite{Du2019Unsupervised} introduced a deep slow feature analysis (DSFA) model combined with a deep neural network to learn nonlinear features and emphasize changes. The authors employed a CVA-based pre-detection method to select samples from multispectral images for training the network. Song et al. \cite{Song2018Change} utilized PCA and image element unmixing algorithms to select training samples for a recurrent 3D fully convolutional network for binary and multiclass CD. In \cite{Hu2023Binary}, pseudo-labels from BCD were employed to guide hyperspectral MCD. For high-resolution images, pre-detection algorithms need to focus more on the spatial information within the image. Gong et al. \cite{Gong2017Superpixel} developed a high-resolution CD algorithm based on superpixel segmentation and deep difference representation. This method achieved varying pre-detection results based on different superpixel features and implemented a voting mechanism to select reliable training samples from these results. Xu et al. \cite{Xv2019Combining} used SFA as a pre-detection algorithm to select reliable samples to train a stacked DAE for high resolution RSI CD.

\textbf{Leveraging external knowledge:}
DNNs pre-trained on natural images are adept at extracting general visual features, which can be highly beneficial for the recognition tasks of RSIs. An early exploration by Saha et al. \cite{saha2019unsupervised} utilized a CNN encoder pre-trained on natural optical images to extract bitemporal features, which were then pixel-wise compared to classify changes. Subsequently, Saha et al. \cite{Saha2022Patch} applied the pre-trained VGG network as a feature extractor for planetary CD. Bandara et al. \cite{bandara2023deep} introduce multiple bitemporal constraints based on metric learning to transfer the inherent knowledge from pre-trained VGG networks to the RS target domain. The approach in \cite{liu2020convolutional} initially transfers deep features pre-trained on semantic segmentation datasets, then fine-tunes them with distance constraints and pseudo-change labels to enhance relevance. Furthermore, in \cite{Zhan2022Transfer}, object-based image analysis was leveraged to refine feature extraction with a pre-trained CNN. To better tailor the features for the RS domain, a clustering function based on feature distance calculation was introduced in \cite{sublime2019automatic}. Yan et al. utilize multi-temporal remote sensing indices as domain knowledge to guide the contrastive learning of change representation \cite{yan2023domain}. 

Recently, Visual Foundation Models (VFMs) such as CLIP \cite{radford2021CLIP} and Segment Anything Model (SAM) \cite{Kirillov2023Segment} have emerged and gained significant research interest. VFMs, pre-trained on web-scale datasets, are designed to capture universal feature representations that can be generalized to a variety of downstream tasks. However, since these VFMs are generally trained with natural images, they exhibit certain biases in RS applications \cite{ji2024segment}. Considering spectral and temporal characteristics of RSIs, several RS foundation models (FMs) have been developed, including GFM \cite{mendieta2023gfm}, SpectralGPT \cite{hong2024spectralgpt} and SkySense \cite{guo2024skysense}. These FMs enables training-free feature embedding on multi-spectral, multi-temporal, and multi-modal RS data, thereby supporting a variety of downstream tasks including CD. However, since these FMs are typically trained with the context intrinsic to RSIs, they do not consider the specific application context of CD tasks. Consequently, employing these models for CD still necessitates incorporating CD-specific modules and performing fully supervised fine-tuning.

Considering that FMs contain implicit knowledge of the image content, several recent methods have explored employing FMs to achieve sample-efficient CD. In \cite{ding2024samcd}, VFMs are adapted to the RS domain using a semantic latent aligning technique, demonstrating their sample efficiency. Fig.\ref{fig.VFM_CD} presents an overview of this approach, where the latent are aligned via temporal consistency regularization. In \cite{li2024new},



a side-adaption framework is proposed to inject the VFM knowledge into CD models. In \cite{wang2023cs}, SAM is utilized to generate pseudo labels from vague change maps used as prompts. In \cite{zheng2024segment}, zero-shot CD is achieved by measuring the similarity of SAM-encoded features. In \cite{chen2024change}, Chen et al. employed SAM to achieve unsupervised CD between optical images and map data. Dong et al. \cite{dong2024changeclip} utilized CLIP to learn visual-language representations to improve CD accuracy.


\begin{table*}
    \centering
    \caption{Comparison of SOTA accuracy in CD obtained with different sample-efficient methodologies. 'Sup.': supervision type, 'ext.': external data, 'FT': fine-tuning. It should be noted that the experimental settings exhibit variations across different studies in the literature.}\label{Table.SOTA}
    \resizebox{1\linewidth}{!}{%
    \begin{tabular}{l|c|r|cc|ccc}
        \toprule
        \multirow{2}*{Sup.} & \multirow{2}*{Dataset} & \multirow{2}*{Method} & \multirow{2}*{Training data used} & \multirow{2}*{Training label used} & \multicolumn{3}{c}{Accuracy Metrics} \\ \cline{6-8}
        ~ & ~ & ~ & ~ & ~ & OA (\%) & IoU (\%) & $F_1$ (\%) \\ 
        \midrule
        \multirow{12}*{FSCD} & \multirow{4}*{Levir} & BIT \cite{chen2021remote} & 100\% & 100\% & 98.92 & 80.68 & 89.31 \\ 
        %~ & ~ & EATDer\cite{ma2023eatder} & 100\% & 100\% & 98.85 & 80.59 & 89.25 \\ 
        ~ & ~ & SAM-CD\cite{ding2024samcd} & 100\% & 100\% & 99.14 & 84.26 & 91.68 \\ 
        ~ & ~ & ScratchF. \cite{noman2024remote} & 100\%  & 100\% & 99.16 & 84.63 & 91.68 \\
        ~ & ~ & Changer\cite{Liky2023Changer} & 100\% & 100\% & --- & --- & 92.06 \\ 
        \cline{2-8}
        ~ & \multirow{3}*{WHU} & BIT \cite{chen2021remote} & 100\% & 100\% & 98.75 & 72.39 & 83.95  \\
        %~ & ~ & EATDer \cite{ma2023eatder} & 100\%  & 100\%  & 99.01 & 80.30 & 89.07  \\
        ~ & ~ & ScratchF. \cite{noman2024remote} & 100\%  & 100\% & 99.37 & 84.97 & 91.87 \\
        ~ & ~ & SAM-CD \cite{ding2024samcd} & 100\%  & 100\%  & 99.60 & 91.15 & 95.37  \\
        \cline{2-8}
        ~ & \multirow{5}*{OSCD} & FC-Siam-conc \cite{daudt2018fully} & 100\%  & 100\% & 94.07 & --- & 45.20 \\
        ~ & ~ & FC-Siam-diff \cite{daudt2018fully} & 100\%  & 100\% & 94.86 & --- & 48.86 \\
        ~ & ~ & FC-EF \cite{daudt2018fully} & 100\%  & 100\% & 94.23 & --- & 48.89 \\
        ~ & ~ & ScratchF. \cite{noman2024remote} & 100\%  & 100\% & 97.33 & 40.22 & 57.37 \\
        ~ & ~ & FC-EF-Res \cite{daudt2019multitask} & 100\%  & 100\% & 95.34 & --- & 59.20 \\
        \hline
        \multirow{12}*{SMCD} & \multirow{6}*{Levir} & ECPS\cite{yang2024ecps} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & \textcolor{teal}{98.59} \textcolor{cyan}{98.74} \textcolor{orange}{98.70} \textcolor{magenta}{98.85} & \textcolor{teal}{75.56} \textcolor{cyan}{77.63} \textcolor{orange}{78.06} \textcolor{magenta}{79.30} & \textcolor{teal}{86.06} \textcolor{cyan}{87.40} \textcolor{orange}{87.68} \textcolor{magenta}{88.46} \\         
        ~ & ~ & ST-RCL\cite{zhang2023joint} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & --- & --- & \textcolor{teal}{87.11} \textcolor{cyan}{88.75} \textcolor{orange}{89.46} \textcolor{magenta}{89.77} \\ 
        ~ & ~ & STCRNet\cite{wang2024stcrnet} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & --- & \textcolor{teal}{80.65} \textcolor{cyan}{82.23} \textcolor{orange}{82.98} \textcolor{magenta}{83.48} & \textcolor{teal}{89.29} \textcolor{cyan}{90.25} \textcolor{orange}{90.70} \textcolor{magenta}{91.00} \\ 
        ~ & ~ & UniMatch\cite{yang2023revisiting} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & ~ & \textcolor{teal}{80.88} \textcolor{cyan}{81.73} \textcolor{orange}{82.04} \textcolor{magenta}{82.25} & \textcolor{teal}{89.43} \textcolor{cyan}{89.95} \textcolor{orange}{90.13} \textcolor{magenta}{90.26} \\
        ~ & ~ & C2F-SemiCD\cite{han2024c2fsemicd} & \multicolumn{1}{l}{\textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%}}  & \multicolumn{1}{l|}{\textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%}} & \multicolumn{1}{l}{\textcolor{teal}{98.99} \textcolor{cyan}{99.08} \textcolor{orange}{99.12}} & \multicolumn{1}{l}{\textcolor{teal}{81.76} \textcolor{cyan}{83.15} \textcolor{orange}{83.75}} & \multicolumn{1}{l}{\textcolor{teal}{89.97} \textcolor{cyan}{90.80} \textcolor{orange}{91.16}} \\ 
        ~ & ~ & ISCDNet\cite{zuo2024robust} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & --- & \textcolor{teal}{81.84} \textcolor{cyan}{82.34} \textcolor{orange}{82.53} \textcolor{magenta}{83.58} & \textcolor{teal}{90.01} \textcolor{cyan}{90.32} \textcolor{orange}{90.43} \textcolor{magenta}{91.06} \\ 
        \cline{2-8}
        ~ & \multirow{5}*{WHU} & UniMatch \cite{yang2023revisiting} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & --- & \textcolor{teal}{75.15} \textcolor{cyan}{77.30} \textcolor{orange}{81.64} \textcolor{magenta}{82.13} & \textcolor{teal}{85.81} \textcolor{cyan}{87.20} \textcolor{orange}{90.95} \textcolor{magenta}{91.26}  \\ 
        ~ & ~ & STCRNet \cite{wang2024stcrnet} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & --- & \textcolor{teal}{77.03} \textcolor{cyan}{81.91} \textcolor{orange}{83.40} \textcolor{magenta}{83.93} & \textcolor{teal}{87.03} \textcolor{cyan}{90.06} \textcolor{orange}{90.95} \textcolor{magenta}{91.26}  \\ 
        ~ & ~ &  ST-RCL \cite{zhang2023joint} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & --- & --- & \textcolor{teal}{87.80} \textcolor{cyan}{88.00} \textcolor{orange}{89.29} \textcolor{magenta}{83.84}  \\ 
        ~ & ~ & C2F-SemiCD \cite{han2024c2fsemicd} & \multicolumn{1}{l}{\textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%}}  & \multicolumn{1}{l|}{\textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%}} & \multicolumn{1}{l}{\textcolor{teal}{98.87} \textcolor{cyan}{98.94} \textcolor{orange}{99.23}} & \multicolumn{1}{l}{\textcolor{teal}{79.14} \textcolor{cyan}{79.50} \textcolor{orange}{81.93}} & \multicolumn{1}{l}{\textcolor{teal}{88.35} \textcolor{cyan}{88.58} \textcolor{orange}{90.07} } \\ 
        ~ & ~ & ISCDNet \cite{zuo2024robust} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & --- & \textcolor{teal}{81.48} \textcolor{cyan}{82.59} \textcolor{orange}{83.72} \textcolor{magenta}{85.18} & \textcolor{teal}{89.80} \textcolor{cyan}{90.46} \textcolor{orange}{91.14} \textcolor{magenta}{92.00}  \\ 
        \cline{2-8}
        ~ & OSCD & ECPS \cite{yang2024ecps} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & \textcolor{teal}{5\%} \textcolor{cyan}{10\%} \textcolor{orange}{20\%} \textcolor{magenta}{40\%} & \textcolor{teal}{87.12} \textcolor{cyan}{88.13} \textcolor{orange}{88.59} \textcolor{magenta}{88.98} & \textcolor{teal}{37.05} \textcolor{cyan}{37.69} \textcolor{orange}{40.31} \textcolor{magenta}{41.44} & \textcolor{teal}{54.07} \textcolor{cyan}{54.75} \textcolor{orange}{57.46} \textcolor{magenta}{58.60}  \\ 
        \hline
        \multirow{5}*{WSCD} & \multirow{3}*{Levir} & ICR-MJS\cite{dai2023siamese} & 100\% & image label & --- & 67.41 & 50.84 \\ 
        ~ & ~ & KD-MSI\cite{lu2024weakly} & 100\% & image label & 93.9 & 64.9 & 74.9 \\
        ~ & ~ & CARGNet\cite{fang2023point} & 100\% & point label & 98.28 & 72.13 & 83.81 \\ 
        \cline{2-8}
        ~ & \multirow{2}*{WHU} & ICR-MJS \cite{dai2023siamese} & 100\% & image label & --- & 65.09 & 78.86  \\ 
        ~ & ~ & KD-MSI \cite{lu2024weakly} & 100\% & image label & 99.7 & 76.9 & 85.4  \\ 
        %~ & ~ & SDCDNet \cite{wang2023sdcdnet} & 100\% & coarse label & - & - & 92.96  \\ 
        %~ & WSLCD \cite{zhao2024pixellevel} & CLCD & 100\% & image label (100\%) & --- & 42.03 & 59.19  \\         
        \cline{2-8}
        ~ & OSCD & FCD-GAN \cite{wu2023fully} & 100\% & box label & 91.38 & 21.28 & 35.08  \\ 
        %~ & WSLCD \cite{zhao2024pixellevel} & CLCD & 100\% & image label (100\%) & --- & 42.03 & 59.19  \\ 
        \hline
        \multirow{8}*{SSCD} & \multirow{3}*{Levir} & LGPNet \cite{wang2023self} & 100\% + ext. & 1\% (FT) & --- & 46.13 & 62.09 \\
        ~ & ~ & DST-VGG \cite{zheng2024detail} & 100\% & 100\% (FT) & 99.21 & 85.44 & 92.15 \\ 
        ~ & ~ & RECM \cite{zhang2023selfsupervised} & 100\% + ext.  & 100\% (FT) & --- & --- & 92.77\\         
        \cline{2-8}
	~ & \multirow{2}*{WHU} & GLCL \cite{jiang2023self} & 100\% & 100\% (FT) & --- & 90.29 & 90.54 \\
        ~ & ~ & DST-VGG \cite{zheng2024detail} & 100\% & 100\% (FT) & 99.64 & 90.34 & 95.69 \\
        \cline{2-8}
        ~ & \multirow{3}*{OSCD} & PixSSLs \cite{chen2022self} & 100\% + ext. & 0\% & 95.70 & --- & 53  \\ 
        ~ & ~ & DK-SSCD \cite{yan2023domain} & 100\% & 0\% & 95.54 & --- & 55.69  \\ 
        ~ & ~ & TD-SSCD \cite{qu2023tdsscd} & 100\% & 100\% (FT) & 95.38 & --- & 72.11  \\ 
        \hline
        \multirow{9}*{UCD} & \multirow{4}*{Levir} & 
        Anychange\cite{zheng2024segment} & 0\% & 0\% & --- & --- & 23.0 \\
        ~ & ~ & DSFA \cite{Du2019Unsupervised} & 0\% & 0\% & 77.33 & --- & 47.65 \\
        ~ & ~ & DCVA \cite{saha2019unsupervised} & 0\% & 0\% & 84.75 & --- & 52.89 \\
          ~ & ~ & SCM \cite{noh2022unsupervised} & 100\% & 0\% & 88.80 & --- & 62.80 \\
        \cline{2-8}
        ~ & \multirow{5}*{OSCD} & DCVA \cite{saha2019unsupervised} & 0\% & 0\% & 91.6 & --- & 24.5 \\
        % ~ & ~ & ACGAN \cite{Saha2021Unsupervised} & 0\% & 0\% & 77.67 & --- &  \\
        ~ & ~ & KPCA-MNet \cite{wu2021unsupervised} & 0\% & 0\% & --- & --- & 30.2 \\
        ~ & ~ & FLCG \cite{Mall2022Change} & 100\% & 0\% & --- & --- & 32.1 \\
        %~ & ~ & SiROC \cite{Kondmann2022Spatial} & 100\% & 0\% & --- & --- & 36.72 \\
        ~ & ~ & DMLCD \cite{bandara2023deep} & 100\% & 0\% & 95.8 & --- & 32.5 \\
        ~ & ~ & DSFA \cite{Du2019Unsupervised} & 0\% & 0\% & 92.63 & --- & 35.85 \\

        \bottomrule
    \end{tabular}}
\end{table*}

\begin{table*}[!ht]
    \centering
    \caption{Statistical overview of the benchmark CD datasets presented in Table \ref{Table.SOTA}.}\label{Table.Datasets}
    \resizebox{1\linewidth}{!}{%
    \begin{tabular}{c|c|r|r|r|r|ccccc}
    \toprule
        \multirow{2}*{Datasets} & \multirow{2}*{Resolution} & \multirow{2}*{Image size} & Image & Change & Change & \multicolumn{5}{c}{Highest $F_1$ (\%)} \\
        \cline{7-11}
        & & & Pairs & Pixels & Instances &  FSCD & SMCD (5\%)  & WSCD & SSCD (w/o. FT) & UCD \\
        \hline
        Levir & 0.5m & 10241024 & 637 & 30,913,975 & 31,333 & 92.06 \cite{Liky2023Changer} & 90.01\cite{zuo2024robust} & 74.9 \cite{dai2023siamese} &   & 62.80 \cite{noh2022unsupervised} \\ \hline
        WHU & 0.3m & 32,50715,354 & 1 & 21,352,815 & 2297 & 95.37 \cite{ding2024samcd} & 89.80 \cite{zuo2024robust} & 85.4 \cite{lu2024weakly} & --- & --- \\ \hline
        OSCD & 10m & 600600 & 24 & 148,069 & 1048 & 59.20  \cite{daudt2019multitask} & 54.07 \cite{yang2024ecps} & 35.08 \cite{wu2023fully} & 55.69 \cite{yan2023domain} & 35.85 \cite{Du2019Unsupervised} \\
    \bottomrule
    \end{tabular}}
\end{table*}

\subsection{Comparison of Accuracy}  \label{sc3-e}

To elucidate the efficacy of the sample-efficient DL methodologies discussed, Table \ref{Table.SOTA} presents a comparative analysis of the SOTA accuracy obtained on several benchmark CD datasets. The accuracy metrics include overall accuracy (OA), intersection over union (IoU), and $F_1$, which are common in BCD. To facilitate comparison between different types of supervision, we select the most frequently used datasets in various tasks, including Levir \cite{Chen2020}, WHU \cite{ji2018fully}, and OSCD \cite{daudt2018urban}. It is important to acknowledge that there are significant variations in the experimental configurations of the methods being compared, a concern raised in \cite{corley2024change}. Therefore, this table is intended solely to provide an intuitive assessment of the accuracy of SOTA.

To facilitate a comprehensive understanding of the training samples utilized across various methods, Table \ref{Table.Datasets} presents the metadata of each CD benchmark. Overall, Levir and WHU are two VHR datasets with large image size and rich change samples. In contrast, OSCD has lower resolution and contains less training samples. To present an intuitive comparison of the SOTA accuracy across various learning paradigms, Table \ref{Table.Datasets} also summarizes the highest $F_1$ scores achieved in each dataset.

Tables \ref{Table.SOTA} and \ref{Table.Datasets} clearly demonstrate that the accuracy of CD is highly dependent on the level of supervision during the training process. First, the CD accuracy on the Levir and WHU datasets is significantly higher than that on the OSCD dataset. This disparity is attributed to the richer set of change samples and the finer spatial resolution present in the Levir and WHU datasets. Second, the accuracy of FSCD and SSCD with fine-tuning (FT) is higher than that of SMCD, WSCD, UCD and SSCD without FT. Notably, the SSCD with FT marginally surpasses FSCD, which can be attributed to its extensive pre-training that effectively utilizes the image contexts as extra supervisions. This observed accuracy hierarchy aligns with the strong-to-weak supervision level in the different learning paradigms, as illustrated in Fig.\ref{fig.learning_types}.

SMCD achieves the highest accuracy among sample-efficient CD approaches. Recent advances in SMCD ensure remarkably high accuracy with only a small proportion of training samples. For example, utilizing only 5\% of the training data, the SOTA SMCD methods only see a minor $F_1$ reduction of 2\% on the Levir and 0.6\% on the OSCD datasets. However, it is important to note that even with this small portion of training data, SMCD still requires a substantial number of change samples. Based on the number of change instances detailed in Table \ref{Table.SOTA} and through a rough estimate, SMCD typically requires more than 100 change samples on the Levir and WHU datasets.


The accuracy of WSCD is significantly influenced by the level of supervision applied. Compared to image-level labels, employing spatial labels (such as box or point labels) for training WSCD algorithms generally results in superior accuracy. For example, as tested on the Levir dataset, point label-supervised CD approach \cite{fang2023point} has an advantage of exceeding 30\% in $F_1$ compared to approaches that utilize image labels. Regarding image label-supervised SMCD, while a relatively high accuracy is attained (particularly on VHR datasets), it is important to note that training is carried out using patch labels rather than a complete RSI. As reported in \cite{lu2024weakly} and \cite{dai2023siamese}, image labels are assigned to each pair of patches with 256$\times$256 pixels. Therefore, this type of SMCD still necessitates a certain degree of human intervention.

SSCD can be employed as either an approach to achieve label-free learning of change representations, or merely as a pretraining technique to initialize the DNN parameters. SSCD without FT is challenging, since the image contexts utilized in self-supervised learning are independent of the application contexts inherent in CD tasks. Most literature works adopt the latter strategy, that is, pre-training through self-supervision and fine-tuning with all available change samples. This strategy yields substantial accuracy improvements over the vanilla FSCD. The improvements are particularly significant on the OSCD dataset (up to 12\% in $F_1$ \cite{qu2023tdsscd}), which can be attributed to the scarcity of training samples within this dataset. 

Meanwhile, SSCD without FT can be regarded as a distinct subset of UCD that utilizes self-supervised learning techniques. Most literature studies on UCD and label-free SSCD have been conducted on medium resolution datasets such as OSCD. They are commonly adopted for analysis of satellite images such as those collected by Sentinel and Landsat. Due to the fact that numerous experiments are performed on non-open benchmarks, it is challenging to assess the level of accuracy, and hence, these results are not presented in Table \ref{Table.SOTA}. The highest metrics obtained on the OSCD dataset are 92.63\% in OA and 35.85\% in $F_1$ \cite{Du2019Unsupervised}, exhibiting a reduction exceeding 23\% in $F_1$ relative to FSCD. UCD (or label-free SSCD) is more challenging when applied to VHR datasets due to the increased spatial complexity. A reduction of approximately 30\% in $F_1$ is noted when applied to the Levir dataset. One of the zero-shot CD approaches, Anychange \cite{zheng2024segment}, obtains an accuracy of 24.5\% in $F_1$, highlighting a substantial gap for further advancements.

In summary, sample-efficient CD methods have greatly reduced the dependence on a large volume of training samples, thereby achieving relatively high accuracy with a reduced number of samples or the utilization of weak labels. However, training CD algorithms without labels or using a very low level of supervision remains a challenge.
