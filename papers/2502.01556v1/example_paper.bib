@misc{hu2020simpleeffectiveregularizationmethods,
      title={Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee}, 
      author={Wei Hu and Zhiyuan Li and Dingli Yu},
      year={2020},
      eprint={1905.11368},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.11368}, 
}

@misc{rudner2023functionspaceregularizationneuralnetworks,
      title={Function-Space Regularization in Neural Networks: A Probabilistic Perspective}, 
      author={Tim G. J. Rudner and Sanyam Kapoor and Shikai Qiu and Andrew Gordon Wilson},
      year={2023},
      eprint={2312.17162},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2312.17162}, 
}

@inproceedings{chen2022neural,
  title={A neural tangent kernel perspective on function-space regularization in neural networks},
  author={Chen, Zonghao and Shi, Xupeng and Rudner, Tim GJ and Feng, Qixuan and Zhang, Weizhong and Zhang, Tong},
  booktitle={OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)},
  year={2022}
}

@book{williams2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  volume={2},
  year={2006},
  publisher={MIT press Cambridge, MA}
}

@article{kendall2017uncertainties,
  title={What uncertainties do we need in bayesian deep learning for computer vision?},
  author={Kendall, Alex and Gal, Yarin},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{williams1998bayesian,
  title={Bayesian classification with Gaussian processes},
  author={Williams, Christopher KI and Barber, David},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={20},
  number={12},
  pages={1342--1351},
  year={1998},
  publisher={IEEE}
}

@article{nitanda2020optimal,
  title={Optimal rates for averaged stochastic gradient descent under neural tangent kernel regime},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2006.12297},
  year={2020}
}

@inproceedings{suh2021non,
  title={A non-parametric regression viewpoint: Generalization of overparametrized deep ReLU network under noisy observations},
  author={Suh, Namjoon and Ko, Hyunouk and Huo, Xiaoming},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{he2020bayesian,
  title={Bayesian deep ensembles via the neural tangent kernel},
  author={He, Bobby and Lakshminarayanan, Balaji and Teh, Yee Whye},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1010--1022},
  year={2020}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{du2019gradientmultilayer,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International conference on machine learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@inproceedings{allenzhu2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International conference on machine learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@article{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{strehl2006pac,
  title={PAC model-free reinforcement learning},
  author={Strehl, Alexander L and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={881--888},
  year={2006}
}

@misc{rashid2020optimistic,
      title={Optimistic Exploration even with a Pessimistic Initialisation}, 
      author={Tabish Rashid and Bei Peng and Wendelin BÃ¶hmer and Shimon Whiteson},
      year={2020},
      eprint={2002.12174},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.12174}, 
}

@article{thompson-sampling,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2332286},
 author = {William R. Thompson},
 journal = {Biometrika},
 number = {3/4},
 pages = {285--294},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples},
 urldate = {2025-01-26},
 volume = {25},
 year = {1933}
}

@article{yang2020tensorprograms2,
  title={Tensor programs ii: Neural tangent kernel for any architecture},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:2006.14548},
  year={2020}
}

@article{belfer2024spectral,
  title={Spectral analysis of the neural tangent kernel for deep residual networks},
  author={Belfer, Yuval and Geifman, Amnon and Galun, Meirav and Basri, Ronen},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={184},
  pages={1--49},
  year={2024}
}

@article{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International conference on machine learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine learning},
  volume={109},
  pages={467--492},
  year={2020},
  publisher={Springer}
}

@article{wei2017early,
  title={Early stopping for kernel boosting algorithms: A general analysis with localized complexities},
  author={Wei, Yuting and Yang, Fanny and Wainwright, Martin J},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{li2020gradient,
  title={Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks},
  author={Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
  booktitle={International conference on artificial intelligence and statistics},
  pages={4313--4324},
  year={2020},
  organization={PMLR}
}

@article{lee2018dnnsgps,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={International Conference on Learning Representations},
  volume={37},
  year={2018}
}