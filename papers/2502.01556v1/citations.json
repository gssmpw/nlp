[
  {
    "index": 0,
    "papers": [
      {
        "key": "lee2019wide",
        "author": "Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey",
        "title": "Wide neural networks of any depth evolve as linear models under gradient descent"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "lee2019wide",
        "author": "Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey",
        "title": "Wide neural networks of any depth evolve as linear models under gradient descent"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "daniely2016toward",
        "author": "Daniely, Amit and Frostig, Roy and Singer, Yoram",
        "title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "du2019gradient",
        "author": "Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu",
        "title": "Gradient descent finds global minima of deep neural networks"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "jacot2018neural",
        "author": "Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\\'e}ment",
        "title": "Neural tangent kernel: Convergence and generalization in neural networks"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "rudner2023functionspaceregularizationneuralnetworks",
        "author": "Tim G. J. Rudner and Sanyam Kapoor and Shikai Qiu and Andrew Gordon Wilson",
        "title": "Function-Space Regularization in Neural Networks: A Probabilistic Perspective"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chen2022neural",
        "author": "Chen, Zonghao and Shi, Xupeng and Rudner, Tim GJ and Feng, Qixuan and Zhang, Weizhong and Zhang, Tong",
        "title": "A neural tangent kernel perspective on function-space regularization in neural networks"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "allenzhu2019convergence",
        "author": "Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao",
        "title": "A convergence theory for deep learning via over-parameterization"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zou2020gradient",
        "author": "Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan",
        "title": "Gradient descent optimizes over-parameterized deep ReLU networks"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wei2017early",
        "author": "Wei, Yuting and Yang, Fanny and Wainwright, Martin J",
        "title": "Early stopping for kernel boosting algorithms: A general analysis with localized complexities"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "li2020gradient",
        "author": "Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet",
        "title": "Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "hu2020simpleeffectiveregularizationmethods",
        "author": "Wei Hu and Zhiyuan Li and Dingli Yu",
        "title": "Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "nitanda2020optimal",
        "author": "Nitanda, Atsushi and Suzuki, Taiji",
        "title": "Optimal rates for averaged stochastic gradient descent under neural tangent kernel regime"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "suh2021non",
        "author": "Suh, Namjoon and Ko, Hyunouk and Huo, Xiaoming",
        "title": "A non-parametric regression viewpoint: Generalization of overparametrized deep ReLU network under noisy observations"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "he2020bayesian",
        "author": "He, Bobby and Lakshminarayanan, Balaji and Teh, Yee Whye",
        "title": "Bayesian deep ensembles via the neural tangent kernel"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "du2019gradient",
        "author": "Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu",
        "title": "Gradient descent finds global minima of deep neural networks"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "zou2020gradient",
        "author": "Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan",
        "title": "Gradient descent optimizes over-parameterized deep ReLU networks"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "wei2017early",
        "author": "Wei, Yuting and Yang, Fanny and Wainwright, Martin J",
        "title": "Early stopping for kernel boosting algorithms: A general analysis with localized complexities"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "li2020gradient",
        "author": "Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet",
        "title": "Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks"
      }
    ]
  }
]