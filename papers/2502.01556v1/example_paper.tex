%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{tikz}
\newcommand{\fcircle}[2][]{\tikz \fill[#1] (0,0) circle (#2);}
\usepackage{microtype}

\usepackage{graphicx}    % For including images
\usepackage{subcaption}  % For handling subfigures

\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.



% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

% Use the following line for the initial blind version submitted for review:
\usepackage{hyperref}
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{thm-restate}
\usepackage{tcolorbox}
%\usepackage[hidelinks, bookmarksnumbered=true]{hyperref}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator*{\lin}{lin}
\DeclareMathOperator*{\ntk}{ntk}
\DeclareMathOperator*{\std}{std}
\DeclareMathOperator*{\diag}{diag}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{}

\begin{document}

\twocolumn[
\icmltitle{Observation Noise and Initialization in Wide Neural Networks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}
\icmlsetsymbol{int}{$\dagger$}

\begin{icmlauthorlist}
\icmlauthor{Sergio Calvo-Ordoñez}{equal,yyy,comp,int}
\icmlauthor{Jonathan Plenk}{equal,yyy,comp}
\icmlauthor{Richard Bergna}{sch}
\icmlauthor{Álvaro Cartea}{yyy,comp}
\icmlauthor{Jose Miguel Hernández-Lobato}{sch}
\icmlauthor{Konstantina Palla}{spot}
\icmlauthor{Kamil Ciosek}{spot}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Mathematical Institute, University of Oxford}
\icmlaffiliation{comp}{Oxford-Man Institute of Quantitative Finance, University of Oxford}
\icmlaffiliation{sch}{Department of Engineering, University of Cambridge}
\icmlaffiliation{spot}{Spotify}

\icmlcorrespondingauthor{Sergio Calvo-Ordoñez}{sergio.calvoordonez@maths.ox.ac.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Performing gradient descent in a wide neural network is equivalent to computing the posterior mean of a Gaussian Process with the Neural Tangent Kernel (NTK-GP), for a specific choice of prior mean and with zero observation noise. 
%The Neural Tangent Kernel (NTK) is useful for connecting the training dynamics of wide neural networks with Gaussian Processes (GPs). Specifically it is known that training a wide network with gradient descent is connected to estimating the posterior mean in the NTK-GP.   
%: the neural network training solution converges in distribution over random initializations to a Gaussian with mean equal to the posterior mean of an NTK-GP. 
However, existing formulations of this result have 
%three
two limitations: i) the resultant NTK-GP assumes no 
%ignores 
%additive 
noise in the observed target variables, which can result in suboptimal predictions with noisy data;
ii) it is unclear how to extend the equivalence to an arbitrary prior mean, a crucial aspect of formulating a well-specified model.
%calculating the NTK-GP posterior mean requires either inverting the kernel or averaging over an ensemble of trained networks, leading to increased prediction times.
%ii) the framework does not allow for non-zero prior mean functions and iii) calculating the NTK-GP posterior mean requires either inverting the kernel or averaging over an ensemble of trained networks, leading to increased prediction times. 
To address the first limitation, we introduce a regularizer into the neural network's training objective, formally showing its correspondence to incorporating observation noise into the NTK-GP model. To address the second, we introduce a \textit{shifted network} that enables arbitrary prior mean functions. This approach allows us to perform gradient descent on a single neural network, without expensive ensembling or kernel matrix inversion. % Does this sound ok?
%and whose predictions are equal to the NTK-GP posterior mean, without the need for using ensembles. 
Our theoretical insights are validated empirically, with experiments exploring different values of observation noise and network architectures.
% The connection between wide neural networks and Gaussian Processes (GPs) via the Neural Tangent Kernel (NTK) provides a powerful framework for understanding the training dynamics of deep learning. Previous work established this connection showing that the solution after training converges in distribution over random initializations to the posterior mean of an NTK-GP with zero observation noise. However, this approach has two key limitations: (1) it assumes no noise or uncertainty in the data-generating process, and (2) the results require averaging over ensembles of networks to approximate the mean function. In this work, we address these limitations by first introducing a regularizer into the neural network’s training objective, formally showing how it corresponds to adding observation noise to the NTK-GP posterior mean. Secondly, we propose optimizing a \textit{shifted network} to define an arbitrary prior mean, ensuring independence from initialization randomness and enabling the mean posterior of the NTK-GP to be achieved from training a single network, without the need for ensembles. Our theoretical insights are validated empirically, with experiments exploring the relationship between observation noise and network architecture.

% A central concept in GPs is aleatoric noise, which represents uncertainties inherent in observations and enables the modelling of noisy and imperfect data. In this work, we investigate the role of aleatoric noise in wide neural networks through the Bayesian perspective of NTK-GPs. We observe that using nonzero aleatoric noise is equivalent to adding a regularization term to the network training objective. We formally prove that such regularization preserves linearization during training. Additionally, we address the challenge of constructing networks with arbitrary prior means, shedding light on how to initialize neural networks to arbitrary values. Our theoretical insights are validated empirically, exploring the relationship between observation noise, network width, and depth.
\vspace{-0.1in}
\end{abstract}

\section{Introduction}
\label{intro}

% Regularization is a fundamental aspect of deep learning, crucial for addressing the challenges posed by over-parametrized models. Techniques such as $l_2$-regularization (weight decay) and early stopping constrain the model parameters to prevent overfitting, aiming to balance expressivity and generalization. These methods implicitly or explicitly control the complexity of the function space explored during training, ensuring that the learned mappings align with the inductive biases of the model and the properties of the data. However, these approaches often do not fully characterize the connections between parameter-space dynamics and the functional properties of the trained model.

% Observation noise is a fundamental aspect of modelling real-world data, as measurements are rarely error-free and often have inherent variability. In probabilistic models, such as Gaussian Processes (GPs), observation noise is explicitly captured through a variance term, which takes into account label uncertainty. Incorporating this noise is essential for models to produce reliable predictions and account for uncertainty in the data-generating process. Despite its importance, the impact of observation noise on the training dynamics and properties of wide neural networks remains underexplored.

%Several studies have explored the connection between regularization and the functional behaviour of neural networks, especially in wide models analyzed through the Neural Tangent Kernel (NTK) framework \cite{jacot2018neural}.

% The Neural Tangent Kernel (NTK) \citet{jacot2018neural} is a central tool for understanding the training dynamics of wide neural networks. 
The connection between wide neural networks and Gaussian Processes via the Neural Tangent Kernel (NTK) \cite{jacot2018neural} provides a powerful framework for understanding the training dynamics of deep learning. The NTK can be used to describe how the predictions of a neural network evolve under gradient-based optimization in the infinite-width limit. Building on this framework, \citet{lee2019wide} demonstrated that the output of a wide neural network trained with gradient flow/descent on a mean squared error (MSE) is connected to the posterior mean of a Gaussian Process (GP) with the NTK as its kernel. However, this result has two important limitations. First, it assumes zero observation noise, leading to model misspecification since real-world data is inherently noisy. Second, the result only holds for a specific choice of prior mean, specifically a randomly initialized network. This is very inconvenient since one does not typically want to do Bayesian inference with a prior initialized randomly.   

Concerning the first limitation, observation --- or aleatoric --- noise is a fundamental component of Gaussian Processes and other probabilistic models \cite{williams2006gaussian} that represents observation uncertainty. Measurements in data are subject to precision limits, and labels can carry errors due to annotation noise or ambiguities \cite{kendall2017uncertainties}. By capturing this variability, aleatoric noise enables models to produce predictions that reflect the randomness of the observed process \cite{williams1998bayesian}. In GPs, this is achieved through a variance term that accounts for label noise, contributing to robust and well-calibrated predictions.

\citet{hu2020simpleeffectiveregularizationmethods} tried to address the problem of observation noise by introducing a regularizer penalizing the distance between network parameters at time $t$, $\theta_t$, and their initialization, $\theta_0$, demonstrating its efficacy in noisy settings. Their analysis, however, relied on the NTK framework and assumed that the network remains in a linear regime throughout training—a critical assumption that was left unproven. This oversight is significant, as the regularizer modifies the training dynamics and invalidates the application of previous results \cite{lee2019wide}. Other recent works have used the regularizer proposed by \citet{hu2020simpleeffectiveregularizationmethods} to improve generalization and training stability. For instance, \citet{nitanda2020optimal} employed it to constrain networks near their initialization under the NTK regime, while \citet{suh2021non} extended this to deeper networks for recovering ground-truth functions from noisy data. \citet{he2020bayesian} explored the regularizer's role in Bayesian deep ensembles.% emphasizing observation noise modelling.
% In the standard NTK-GP framework, the equivalence between network training and the GP mean posterior is limited to the case of zero aleatoric noise \cite{lee2019wide}, failing to account for label noise in real-world data. We show that this regularizer not only preserves linearization but also introduces non-zero aleatoric noise into the NTK-GP framework, extending its ability to handle noisy data and ensuring a better-specified Bayesian model.

% More specifically, \citet{hu2020simpleeffectiveregularizationmethods} introduced a regularizer penalizing the distance between network parameters at time $t$, $\theta_t$, and their initialization, $\theta_0$, demonstrating its efficacy in noisy settings. However, their analysis relied on the NTK framework and assumed that the network remains in a linear regime throughout training. Crucially, they left this assumption of linearization unproven, hence neglecting the effect of the modified training dynamics induced by the regularizer. This omission is significant as the linearization is necessary for their theoretical claims within the NTK analysis.

While these works demonstrate the utility of the regularizer, they all rely on the assumption that wide neural networks remain in a linear regime throughout training. This assumption is critical for leveraging the NTK to characterize the learning dynamics and derive theoretical guarantees, yet the validity of this assumption for the modified gradient flow has not been shown. This gap in the literature raises a key question: how does regularization affect the linearization of wide neural networks, and how does this connect to having a well-specified Bayesian model? In this work, we formally demonstrate that this regularizer not only preserves the linearity of neural networks but also introduces non-zero aleatoric noise into the NTK-GP mean posterior. We enable the NTK-GP posterior mean to reflect the noise present in real-world data, making it a properly specified Bayesian model. At the same time, this helps justify the generalization properties and benefits of regularization observed in previous literature.
% In this work, we address this question by showing that the regularizer not only induces linearization during training but also introduces aleatoric noise into the NTK-GP framework. By extending the equivalence between neural networks and Gaussian Processes to include aleatoric noise, we enable the NTK-GP to reflect the uncertainty present in real-world data, making it a properly specified Bayesian model. At the same time, this helps justify the generalization properties and benefits of regularization observed in previous literature.

%Additionally, building on the results of \citet{lee2019wide}, \citet{he2020bayesian} introduced Bayesian deep ensembles to provide a Bayesian posterior interpretation of ensembles in the infinite-width limit. However, like other ensemble-based methods, the computational cost of training multiple neural networks can be prohibitive in many scenarios. 

To address the problem of supporting inference with an arbotrary prior mean, we propose the use of a \textit{shifted network} during training. This approach 
%allows for the specification of an arbitrary NTK-GP prior mean and 
provides a principled strategy to eliminate initialization randomness, ensuring deterministic convergence of a single \textit{shifted network} to the posterior mean of the defined NTK-GP prior.

% An additional contribution of this work is addressing the challenge of initializing a neural network to achieve a particular prior output. While initializing arrays to specific values is straightforward, neural networks rely on randomly initialized parameters, limiting their flexibility to match desired prior functions without additional training. This is particularly relevant in reinforcement learning (RL) \cite{sutton2018reinforcement}, where optimistic initialization facilitates exploration in tabular settings \citep{strehl2006pac} but is difficult to extend to deep RL without additional approximations/optimization \citep{rashid2020optimistic}. We show how the Bayesian treatment of neural networks under the NTK-GP framework allows the construction of networks with arbitrary prior means, enabling principled initialization aligned with specific inductive biases, including optimistic priors.\todo{Should we say something about a zero prior mean removing randomness from initialization?}

Our contributions are summarized as follows:

\fcircle[fill=black]{2pt} We offer a Bayesian perspective on adding regularization to the loss of wide neural networks by situating it within the NTK-GP framework, demonstrating its equivalence to adding aleatoric noise in the mean posterior of a Gaussian process, hence correcting model misspecification.

\fcircle[fill=black]{2pt} We provide formal proof that our weight-space regularization induces a linear regime during training with gradient flow and gradient descent, validating the use of this regularizer in the literature.

\fcircle[fill=black]{2pt} We address the need for either deep ensembles or kernel inversion to compute the NTK-GP mean posterior by proposing a shifted network during training. This approach allows for arbitrary prior means and eliminates dependence on initialization randomness, enabling deterministic convergence to the NTK-GP mean posterior.
% \fcircle[fill=black]{2pt} We address the challenge of initializing neural networks to achieve specific prior outputs by leveraging the NTK-GP framework to construct networks with arbitrary prior means, enabling additional inductive biases.

\fcircle[fill=black]{2pt} We empirically validate our theoretical results through experiments exploring the relationship between network architecture and degree of regularization, and their impact on linearization and convergence during training.

\section{Preliminaries}
\paragraph{Gaussian Processes} Gaussian Processes (GPs) are a popular non-parametric approach modelling distributions over functions \cite{williams2006gaussian}. Given a dataset of input-output pairs $\{(\mathbf{x}_i, \mathbf{y}_i)\}_{i=1}^N$, a GP represents epistemic uncertainty about function values and assumes the functions are jointly Gaussian with a mean function $m$ and covariance defined by a kernel function $k$. In other words, the GP prior is specified as $f(x) \sim \mathcal{N}(m_x, \mathbf{K}_{x, x})$. Assuming $y \sim \mathcal{N}(f(x), \sigma^2 I)$ and given new test points $\mathbf{x'}$, the posterior mean and covariance of $f(\mathbf{x'})$ are given by:
% \vspace{-0.1in}
\begin{align}
    &\boldsymbol{\mu}_p(\mathbf{x'}) = m_\mathbf{x'} + \mathbf{K_{x',x}} (\mathbf{K_{x, x}} + \sigma^2 \mathbf{I})^{-1} (\mathbf{y} - m_\mathbf{x}), \label{eq-pmean}\\
    &\mathbf{\Sigma}_p(\mathbf{x'}) = \mathbf{K_{x', x'}} - \mathbf{K_{x',x}} (\mathbf{K_{x, x}} + \sigma^2 \mathbf{I})^{-1} \mathbf{K_{x',x}}^\top \label{eq-pcov},
\end{align}
where \(\mathbf{K_{x,x}}\) is the prior covariance matrix computed over the training inputs, \(\mathbf{K_{x', x}}\) is the prior covariance matrix between the test and training points, and \(\sigma^2\) represents the aleatoric (or observation) noise. Neural networks can achieve similar flexibility through different parameterizations, affecting their behaviour as their width increases. Notably, the connection between wide neural networks and GPs depends on the specific choice of this parametrization.
% Neural network parameterizations differ in how they scale weights, biases, and layer outputs with network width. These choices influence the stability of forward and backward signal propagation, as well as the correspondence between the network's training dynamics and the NTK-GP framework
\paragraph{Parametrizations: Standard versus NTK.} The choice of parametrization in neural networks determines how signals propagate and how gradients scale with the network width. Let $\phi:\mathbb{R}\to\mathbb{R}$ denote the activation function.\footnote{We assume that $\phi$ and $\phi'$ are Lipschitz.} In the standard parametrization, the layer outputs are defined (for $l=0,\ldots,L$) as:
\begin{equation}
h^{l+1} := W^{l+1}x^l + b^{l+1}, \quad x^{l+1} := \phi(h^{l+1}) \in \mathbb{R}^{n_{l+1}},
\end{equation}
where $W^{l+1} \in \mathbb{R}^{n_{l+1} \times n_{l}}$ and $b^{l+1} \in \mathbb{R}^{n_{l+1}}$ are the weights and biases, respectively. The parameter vector $\theta \in \mathbb{R}^p$ is defined by stacking them. Here, $n_0 := d$ is the dimension of the input $x^0=x$, and  $n_{L+1} := k$ is the dimension of the output $f(x,\theta) := h^{L+1}$. For simplicity, we only consider $k=1$. The initial weights $\theta_0$ are i.i.d. $W^l_{0,ij} \sim \mathcal{N}(0, \frac{\sigma_{w,l}^2}{n_l})$, and the biases as $b^l_{0,i} \sim \mathcal{N}(0, \sigma_{b,l}^2)$.\footnote{Note, that $W^1x^0$ sums over $d$ terms, and for $l\ge 1$, $W^{l+1}x^l$ sums over $n$ terms.} Under this parametrization, the norm of the Jacobian diverges for width $n_l \to \infty$. Without loss of generality, we will assume $n_1 = \ldots = n_L =: n$.

In the NTK parametrization, the layer outputs are scaled as:
\begin{equation}
h^{l+1} := \frac{1}{\sqrt{n_l}} W^{l+1}x^l +  b^{l+1}, \quad x^{l+1} := \phi(h^{l+1}) \in \mathbb{R}^{n_{l+1}},
\end{equation}
where the weights and biases are initialized i.i.d. as $W^l_{0,ij} \sim \mathcal{N}(0, \sigma_{w,l}^2)$ and $b^l_{0,i} \sim \mathcal{N}(0, \sigma_{b,l}^2)$. This scaling ensures stability in both, the forward and backward pass for $n_l \to \infty$.

% Note, that
% \begin{equation}
%     f^{\ntk}(x,\theta) = f^{\std}\left(x, \left\{\frac{\sigma_w}{\sqrt{d}}W^1, \sigma_b b^1, \frac{\sigma_w}{\sqrt{n}}W^{>1}, \sigma_b b^{>1} \right\}\right).
% \end{equation}
In Appendix \ref{appendix: standard and NTK parametrization} we will precisely show the equivalence between both parametrizations if one chooses the correct learning rate. Further, we will show that using the same learning rate for each parameter in standard parametrization leads to the first layer and the biases not being trained in the infinite-width limit. 
% In other words, using the same learning rate for each parameter in NTK parametrization is equivalent to parameter-dependent learning rates in standard parametrization. While we believe that this is the ``correct" way, we will assume for simplicity that the first layer and the biases are not trained.

\paragraph{Neural Tangent Kernel.} The NTK characterizes the evolution of wide neural network predictions as a linear model in function space. Given a neural network $f(x, \theta)$ parameterized by $\theta \in \mathbb{R}^p$, define the Jacobian in any $N$ points $x$ as $J(x,\theta) := \frac{\partial f(x, \theta)}{\partial \theta} \in \mathbb{R}^{N \times p}$. Under NTK parametrization, the empirical NTK at two sets of inputs $x'$ and $x$ is defined as:
\begin{equation}
\label{eq-ntk}
\hat{\mathbf{\Theta}}_{x', x} := J(x',\theta_0)J(x,\theta_0)^\top \in \mathbb{R}^{N'\times N}.
\end{equation}
Here, $x'$ denotes the set of input points at which predictions are made. Note, that the empirical NTK depends on the randomly initialized $\theta_0$.
%\paragraph{Neural Tangent Kernel Gaussian Processes.}
Interestingly, as shown by \citet{jacot2018neural}, in the infinite width limit, the empirical NTK converges to a deterministic (i.e. independent of $\theta_0$) kernel $\mathbf{\Theta}$ (the analytical NTK) at initialization and remains constant during training (with unregularized gradient flow). A GP defined by the analytical NTK is referred to as the NTK-GP. 
%The mean of the trained network  (taken over random initialization) corresponds to the posterior mean of the NTK-GP.

\section{Motivation}
\label{sec-motivation}
Our work builds on a well-known result  \citep{lee2019wide} which shows that, for any sufficiently\footnote{In our proofs, we make the notion of `sufficiently wide' formal.} wide neural network initialized to parameters $\theta_0$, performing gradient descent on the loss
\begin{gather}
\label{eq-loss-vanilla}
\frac{1}{2} \sum_{i=1}^N (f(\mathbf{x}_i,\theta) - \mathbf{y}_i)^2
\end{gather}
until convergence gives us a network with predictions arbitrarily close to those given by the closed-form formula
\begin{gather}
\label{eq-gp-nonoise-initprior}
    f(\mathbf{x'},\theta_0) + \hat{\mathbf{\Theta}}_{\mathbf{{x',x}}}\hat{\mathbf{\Theta}}_{\mathbf{{x,x}}}^{-1}(\mathbf{y}-f(\mathbf{x},\theta_0)).
\end{gather}
This is of special interest because equation \eqref{eq-gp-nonoise-initprior} is equivalent to computing the posterior mean of a Gaussian Process with the kernel $\hat{\mathbf{\Theta}}$, zero observation noise and prior mean $f(\mathbf{x},\theta_0)$. In other words, there is a clear link between the computationally convenient process of gradient descent and a principled, Bayesian approach to inference. 

While the connection between equations \eqref{eq-loss-vanilla} and \eqref{eq-gp-nonoise-initprior} is interesting, it is by itself not very practical. This is both because we seldom want to do inference for a randomly generated prior mean and because observations in regression tasks are often perturbed by noise. In this paper, we thus ask if it is possible to extend this equivalence by allowing for nonzero observation noise and for prior means other than $f(\mathbf{x},\theta_0)$. This leads us to the following research question.
\begin{tcolorbox}[width=0.480\textwidth, before skip=0.5em, after skip=0.5em]
\paragraph{Main Research Question.} Is there a loss function such that performing gradient descent on that loss gives us a network with predictions
\begin{gather*}
    m(\mathbf{x}') + \hat{\mathbf{\Theta}}_{\mathbf{{x',x}}}\left(\hat{\mathbf{\Theta}}_{\mathbf{{x,x}}}+ \beta I\right)^{-1}(\mathbf{y}-m(\mathbf{x})),
\end{gather*}
for an arbitrary prior mean $m(\mathbf{x})$ and for arbitrary values of observation noise $\beta$?\end{tcolorbox}
We answer this question in the affirmative. Specifically, we build up the theory for supporting $\beta > 0$ in Section \ref{sec-on} (Theorems \ref{thrm: first} and \ref{thrm: 3.4}) and then add the theory for supporting arbitrary prior means in Section \ref{sec-mu} (Theorem \ref{lemma-shifted-predictions}).

% \section{Convergence of the Network under Regularized Gradient Descent}
\section{Observation Noise through Regularized Gradient Descent}
\label{sec-on}
The introduction of weight-space regularization not only impacts the generalization properties of wide neural networks \cite{hu2020simpleeffectiveregularizationmethods}, but also fundamentally alters their training dynamics. In the results proposed by \citet{lee2019wide} ---where the unregularized gradient flow is studied--- these dynamics are crucial to understanding how networks converge to solutions that (in the mean posterior sense) align with Bayesian inference. In this section, we analyze the training dynamics of wide neural networks under regularized gradient flow, with a particular focus on the consequences of regularization on the linearization of the network during training. We demonstrate that the training trajectory stays arbitrarily close to its linearized counterpart, providing both theoretical justification and insights into how regularization modifies the gradient flow.

Define $f(\theta) := f(\mathbf{x},\theta)$, $g(\theta) := f(\mathbf{x},\theta) - \mathbf{y} \in \mathbb{R}^{N}$, $J(\theta) := J(\mathbf{x},\theta) \in \mathbb{R}^{N\times p}$ in the training points.\footnote{We assume that $\mathbf{x}_i \neq \mathbf{x}_j$ for $i\neq j$.}
We will consider (for NTK parametrization\footnote{See Appendix \ref{appendix: Standard param to NTK subappendix} for standard parametrization.}) the regularized training loss
\begin{align}\label{eq:loss}
    \mathcal{L}^{\beta}(\theta) 
    &:= \frac{1}{2} \sum_{i=1}^N (f(\mathbf{x}_i,\theta) - \mathbf{y}_i)^2 + \frac{1}{2}\beta \lVert \theta - \theta_0 \rVert_2^2 \\
    &= \frac{1}{2} \lVert g(\theta) \rVert_2^2 + \frac{1}{2} \beta \lVert \theta-\theta_0 \rVert_2^2. 
\end{align}
The gradient of this loss is given by
\begin{equation}
    \nabla_{\theta} \mathcal{L}^{\beta}(\theta) = J(\theta)^\top g(\theta) + \beta (\theta - \theta_0).
\end{equation}
We study the training dynamics under the regularized gradient flow\footnote{In Appendix \ref{appendix: gradient descent}, we state equivalent results for gradient descent with small enough learning rates. These will involve geometric sums instead of the exponential.}\footnote{We will prove that this ODE has a unique solution under our assumptions.}
\begin{equation}
    \frac{d\theta_t}{dt} = -\eta \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t).
\end{equation}
By the chain rule, $\frac{df(x,\theta_t)}{dt} = J(x,\theta_t)\frac{d\theta_t}{dt}$, and thus the training dynamics of the network are
{\small
\begin{equation}
    \frac{df(x,\theta_t)}{dt} = -\eta \left(J(x,\theta_t)J(\theta_t)^{\top}g(\theta_t) + \beta J(x,\theta_t)(\theta_t-\theta_0) \right).
\end{equation}
}

For the sake of readability, we omit the dependence of $\theta_t$ on $\theta_0$ and $\beta$.

To gain insights into the role of regularization, we first analyze the regularized gradient flow for the linearized network
\begin{equation}
    f^{\lin}_{\theta_0}(x,\theta) := f(x,\theta_0) + J(x,\theta_0)(\theta-\theta_0).
\end{equation}
This is a linear ODE and hence has a closed-form solution. We formalize this in the following theorem.
\begin{theorem}\label{thrm: first}
% \begin{equation}
%     \theta^{\lin}_t = \theta_0 - J(\theta_0)^\top \left(I - e^{-\eta \left(J(\theta_0)J(\theta_0)^\top + \beta_N I \right)t } \right) \left(J(\theta_0)J(\theta_0)^\top + \beta_N I \right)^{-1} g(\theta_0).
% \end{equation}

% Hence, for any $\mathbf{x'}$,
% \begin{equation}
%     f^{\lin}(\mathbf{x'},\theta^{\lin}_t)
%     = f(\mathbf{x'},\theta_0) - \mathbf{K(x',x)}(I-e^{-\eta \left(\mathbf{K(x,x) + \beta_NI} \right)t}) \left(\mathbf{K(x,x) + \beta_NI} \right)^{-1}g(\theta_0).
% \end{equation}
 For training time $t\to\infty$, at any point $\mathbf{x'}$,

\vspace{-0.15in}
\begin{small}
\begin{equation}
    f^{\lin}_{\theta_0}(\mathbf{x'},\theta_\infty) = f(\mathbf{x'},\theta_0) + \hat{\mathbf{\Theta}}_{\mathbf{{x',x}}}\left(\hat{\mathbf{\Theta}}_{\mathbf{{x,x}}}+ \beta I\right)^{-1}(\mathbf{y}-f(\mathbf{x},\theta_0)).
\end{equation}
\end{small}
\vspace{-0.2in}
\end{theorem}
We derive this closed-form solution (for gradient flow and gradient descent) in Appendix \ref{appendix: regularized gf/gd for linearized network}.

Moreover, if we consider the network initialization as a random variable, the trained linearized network converges to a normal distribution for layer width $n\to\infty$. Its mean corresponds to the posterior mean of the NTK-GP with prior mean $0$. However, similarly to \cite{lee2019wide, he2020bayesian}, the covariance cannot be interpreted as the posterior covariance of an NTK-GP. See Appendix \ref{appendix: gaussian-distribution-convergence} for the exact formulas.

In the remainder of this section, we show that, with high probability over initialization, the training dynamics of the network (at any test point $\mathbf{x'}$) closely follow the output of the linearized network for large enough hidden layer width $n$. 
First, we restate that the Jacobian is locally bounded and locally Lipschitz, with a Lipschitz constant of $O\left(\frac{1}{\sqrt{n}}\right)$. Next, we prove that the distance between the parameters and their initialization $\lVert \theta_t - \theta_0 \rVert_2$ is $O(1)$.\footnote{On average, this implies that the distance of individual parameters barely changes.} This step is particularly challenging because proofs for $\beta = 0$ (in Eq. \eqref{eq:loss}) rely on the MSE $\lVert g(\theta_t)\rVert_2$ decaying exponentially to $0$, which does not hold for $\beta > 0$. As this is not the case for $\beta > 0$, we instead show that $\lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) \rVert_2$ decays to $0$ exponentially fast.

Even for $\beta = 0$, our analysis provides a generalization of previous proofs in the literature and presents further insights into why the network converges to a global minimizer, despite the Hessian of the loss having negative eigenvalues. Finally, we conclude that the regularized gradient flow remains arbitrarily close to its linearized counterpart, and hence the same holds for the network.

% In the following, we will show (with high probability over initialization), that for large enough hidden layer width $n$, the training dynamics of the network (at any test point $\mathbf{x'}$) are approximated arbitrarily well by the linearized network.
% First, we restate that the Jacobian is locally bounded and locally Lipschitz, with Lipschitz-constant $O(\frac{\log n}{\sqrt{n}})$. Next, we show that the distance $\lVert \theta_t - \theta_0 \rVert_2$ of the parameters from initialization is $O(1)$\footnote{Note, that this means that on average, the distance of the individual parameters barely moves.}. This is the most difficult part, as the proofs for $\beta=0$ rely on the MSE $\lVert g(\theta_t)\rVert_2$ decaying to $0$ exponentially fast. As this is not the case, we will instead show that $\lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) \rVert_2$ decays to $0$ exponentially fast. Considering this for $\beta=0$ delivers a generalization of previous proofs and further interesting insights into why the network converges to a global minimizer even though the Hessian of the loss has negative eigenvalues. Finally, we conclude, that the regularized gradient flow stays arbitrarily close to its linearized version, and hence the same holds for the network.

\subsection{Local Boundedness and Lipschitzness of the Jacobian}
To analyze the training dynamics of wide neural networks, it is essential to ensure that the Jacobian of the network remains (locally) bounded and Lipschitz-continuous during training, with the constants scaling ``well" with layer width $n$. This is shown by the following Lemma, which was proved in \cite{lee2019wide} for standard parametrization.
%  \todo{TODO: Check}However, as we will discuss in Appendix \todo{refer correct appendix}, the scaling used for the first layer in \citet{lee2019wide} introduces redundancy in the parameters of that layer, leading to the need for a fix of their proof to incorporate the proper scaling for NTK parametrization. 
%This refinement leads to a slightly modified result where the Lipschitz constant includes a $\log n$ factor.

% The following Lemma was stated (with the typo of the Lipschitz constant not being decreasing in $n$) in \cite{lee2019wide}. The Lemma they proof is for the standard parametrization. As we will discuss in Appendix \ref{appendix: Jacobian Lipschitz}, their treatment of the scaling for the first layer leads to the parameters of the first layer becoming redundant. To avoid this and use the proper scaling, we need a slightly refined proof, which is why the $\log n$ appears in the Lipschitz constant.
\begin{restatable}{lemma}{lipschitznesslemma}
\label{lemma: Jacobian Lipschitz}
For any $\delta_0>0$, there is $K'>0$ (independent of $C$), such that: For every $C>0$, there is $n$ large enough, such that with probability of at least $1-\delta_0$ (over random initialization): For any point $x$ with $\lVert x \rVert_2 \le 1$:
\begin{equation}
    \forall \theta \in B(\theta_0,C): \lVert J(x,\theta)\rVert_2 
    \le K',
\end{equation}
\begin{equation}
    \forall \theta,\tilde{\theta} \in B(\theta_0, C): \lVert J(x,\theta) - J(x,\tilde{\theta}) \rVert_2 
    \le \frac{1}{\sqrt{n}}K' \lVert \theta - \tilde{\theta} \rVert_2.
\end{equation}
In particular, with $K=\sqrt{N}K'$, for the Jacobian over the training points:
\begin{equation}
    \forall \theta \in B(\theta_0,C): \lVert J(\theta)\rVert_F \le K,
\end{equation}
\begin{equation}
    \forall \theta,\tilde{\theta} \in B(\theta_0, C): \lVert J(\theta) - J(\tilde{\theta}) \rVert_F \le \frac{1}{\sqrt{n}}K \lVert \theta - \tilde{\theta} \rVert_2.
\end{equation}
As a direct consequence, for the Hessian $\nabla^2_{\theta} f(x, \theta) \in \mathbb{R}^{p \times p}$ of the network,
\begin{equation}
    \forall \theta \in B(\theta_0,C): \norm{ \nabla^2_{\theta} f(x, \theta) }_2 \le \frac{1}{\sqrt{n}} K'.
\end{equation}
\end{restatable}
See Appendix \ref{appendix: local lipschitz jacobian} for a discussion of this Lemma. This result establishes that the Jacobian and its changes remain controlled during training, with the Lipschitz constant scaling appropriately with the network width.

\subsection{Exponential Decay of the Regularized Gradient and Closeness of Parameters to their Initial Value}
Building upon the established Lipschitz continuity of the Jacobian, we analyze the effect of regularized gradient flow on the norm of the gradient and the parameters' closeness to their initialization.

We begin by demonstrating that the norm of the gradient of the regularized loss decays exponentially over time. This result directly implies that the distance between the parameters and their initialization is bounded by a constant, i.e. $\lVert \theta_t - \theta_0 \rVert_2 = O(1)$.\footnote{The bounding constant is independent of the outcome in the high probability event for which this holds.} Furthermore, leveraging the Lipschitzness of the Jacobian established in Lemma \ref{lemma: Jacobian Lipschitz}, we show that the Jacobian remains close to its initial value, with deviations scaling as $O\left(\frac{1}{\sqrt{n}}\right)$.

% Next, we show that the norm of the gradient of the regularized loss decays to $0$ exponentially fast, when applying the regularized gradient flow. This implies, that $\lVert \theta_t - \theta_0 \rVert_2 = O(1)$. Using Lemma \ref{lemma: Jacobian Lipschitz}, this further implies that the distance of the Jacobian (at any time $t$) from the origin is $O(\frac{\log n}{\sqrt{n}})$.
\begin{restatable}{theorem}{gftheorempartone}
\label{theorem: Exponential decay gradient, parameters stay close}
Let $\beta \ge 0$. Let $\delta_0>0$ arbitrarily small. There are $K',K, R_0, c_{\beta} > 0$, such that for $n$ large enough, the following holds with probability of at least $1-\delta_0$ over random initialization, when applying regularized gradient flow with learning rate $\eta = \eta_0$:
\begin{equation}
    \norm{ \frac{d\theta_t}{dt}}_2 
    = \eta_0 \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) \rVert_2
    \le \eta_0 K R_0 e^{-\eta_0c_{\beta} t},
\end{equation}
\begin{equation}
    \lVert \theta_t - \theta_0 \rVert_2 
    \le \frac{KR_0}{c_{\beta}} \left(1 - e^{-\eta_0 c_{\beta} t} \right) < \frac{KR_0}{c_{\beta}} =: C,
\end{equation}
\begin{equation}
    \forall \lVert x \rVert_2 \le 1: \lVert J(x,\theta_t) - J(x,\theta_0) \rVert_2 \le \frac{1}{\sqrt{n}} K' C,
\end{equation}
\begin{equation}
    \lVert J(\theta_t) - J(\theta_0) \rVert_2 \le\frac{1}{\sqrt{n}}KC.
\end{equation}
\end{restatable}
We provide a proof in Appendix \ref{appendix: proof regularized gradient flow part 1 (exponential decay)}.
The main idea is to analyze $\frac{d}{dt}\lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t)\rVert_2$ and leverage the fact that the eigenvalues of the Hessian of the network scale as $O\left(\frac{1}{\sqrt{n}}\right)$. Consequently, these eigenvalues are dominated by the regularization term $\beta > 0$, leading to the exponential decay. This is a considerable shift from prior proofs for the case $\beta = 0$, where exponential decay is shown for $\lVert g(\theta_t) \rVert_2$ (the training error) instead.\footnote{This implies exponential decay of $\lVert \nabla_{\theta} \mathcal{L}^{0}(\theta_t) \rVert_2 =\lVert J(\theta_t)^{\top} g(\theta_t) \rVert_2 $.}

% The main proof idea is to look at $\frac{d}{dt}\lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t)\rVert_2$, and use that the (negative) eigenvalues of the Hessian of the network are $O(\frac{\log n}{n})$, and are hence dominated by $\beta > 0$. This is different from the proof for $\beta = 0$ in \cite{lee2019wide} and similar papers, which show exponential decay for $\lVert g(\theta_t) \rVert_2$. (Which implies exponential decay of $\lVert \nabla_{\theta} \mathcal{L}^{0}(\theta_t) \rVert_2 =\lVert J(\theta_t)^{\top} g(\theta_t) \rVert_2 $.)

Our result for $\beta > 0$ does not require the minimum eigenvalue $\lambda_{\min}(\mathbf{\Theta})$ of the analytical NTK to be positive. For the unregularized case $\beta=0$ (and $\lambda_{\min}(\mathbf{\Theta})>0$), we recover the results of \cite{lee2019wide} by an alternative proof. This highlights the importance of the gradient being in the row-span of the Jacobian $J(\theta_t)$, ensuring that only the positive eigenvalues of $J(\theta_t)^\top J(\theta_t)$ contribute to the dynamics.

% We note, that our result doesn't require the minimum eigenvalue $\lambda_{\min}(\mathbf{\Theta})$ of the analytical NTK to be positive. It doesn't hold for the case $\beta = 0$ considered in \cite{lee2019wide}. When assuming $\lambda_{\min}(\mathbf{\Theta}) > 0$, and replacing $\frac{1}{2}\beta$ with $\frac{1}{3}\lambda_{\min}(\mathbf{\Theta})$, we recover the result from \cite{lee2019wide}. We will explain how to modify our proof to obtain this, presenting an alternative proof to \cite{lee2019wide}. This will show the importance of the unregularized gradient being in the row-span of $J(\theta_t)$ (the tangent space), and thus using only the positive eigenvalues of $J(\theta_t)^{\top}J(\theta_t)$, which correspond to the positive eigenvalues of the NTK $J(\theta_t)J(\theta_t)^{\top}$.

\subsection{Closeness to the Linearized Network along the Regularized Gradient Flow}
Having proved the exponential decay of the norm of the gradient, we now show that the parameters $\theta_t$ under regularized gradient flow remain arbitrarily close to the parameters $\theta^{\lin}_t$ obtained from the gradient flow applied to the linearized network for large enough layer width. Using this, we can further prove that the neural network trained from initial parameters $\theta_0$ is arbitrarily close to its linearization around $\theta_0$.
% Using Theorem \ref{theorem: Exponential decay gradient, parameters stay close}, we may now proof that the parameters $\theta_t$ from the regularized gradient flow are arbitrarily close to the parameters $\theta^{\lin}_t$ from the regularized gradient flow applied to the linearized network, for large enough layer width. Using this, we can further proof that the the neural network trained from initial parameters $\theta_0$ is arbitrarily close to it's linearization around $\theta_0$.
\begin{restatable}{theorem}{gftheoremparttwo}
\label{thrm: 3.4}
Let $\beta \ge 0$. Let $\delta_0 > 0$ be arbitrarily small. Then, there are $C_1, C_2$, such that for $n$ large enough, with probability of at least $1-\delta_0$ over random initialization,
\begin{equation}
    \sup_{t\ge 0} \lVert \theta_t - \theta^{\lin}_t \rVert_2 \le C_1 \frac{1}{\sqrt{n}},
\end{equation}
\begin{equation}
    \forall \lVert x \rVert_2 \le 1: \sup_{t\ge 0} \lVert f(x,\theta_t) - f^{\lin}_{\theta_0}(x,\theta_t^{\lin}) \rVert_2 \le C_2 \frac{1}{\sqrt{n}} .
\end{equation}
\end{restatable}
See Appendix \ref{appendix: Proof regularized gradient flow part 2 (closeness linearized)} for a proof.
Again, unlike previous proofs for $\beta = 0$, which rely on the exponential decay of $\lVert g(\theta_t)\rVert_2$, we present a more general and simpler approach that applies for $\beta \geq 0$. Specifically, we decompose $\lVert f(x,\theta_t) - f^{\lin}_{\theta_0}(x,\theta_t^{\lin}) \rVert_2$ into $\lVert f(x,\theta_t) - f^{\lin}_{\theta_0}(x,\theta_t) \rVert_2$ and $\lVert f^{\lin}_{\theta_0}(x,\theta_t) - f^{\lin}_{\theta_0}(x,\theta_t^{\lin}) \rVert_2$, and bound them using Theorem \ref{theorem: Exponential decay gradient, parameters stay close}.
% Again, we can't adopt the proof from \cite{lee2019wide} as it uses the exponential decay of $\lVert g(\theta_t)\rVert_2$ for $\beta = 0$. Instead, we give a simpler proof (that also holds for $\beta = 0$), bounding $\lVert f^{\lin}(x,\theta_t^{\lin}) - f^{\lin}(x,\theta_t)\rVert_2$ and $\lVert f^{\lin}(x,\theta_t) - f(x,\theta_t) \rVert_2$.

The results in this section establish the key conditions required to conclude that wide neural networks under regularized gradient flow remain in a linear regime throughout training. This enables us to use Theorem \ref{thrm: first} to analyze network training with the regularized loss.

\subsection{NTK-GP Posterior Mean with Aleatoric Noise Interpretation}
% \begin{figure}[htbp]
%   \centering
%   % First Subfigure: Linear Model Fits
%   \begin{subfigure}[b]{0.43\columnwidth} % 48% of the column width
%     \centering
%     \includegraphics[width=\linewidth]{imgs/3.4/model_fits_comparison.pdf}
%     % \caption{Linear Model Fits with varying $\beta$ coefficients.}
%         % \label{fig:linear_fits}
%    \end{subfigure}
%   % \hfill
%   % Second Subfigure: Neural Network Model Fits
%   \begin{subfigure}[b]{0.43\columnwidth} % 48% of the column width
%     \centering
%     \includegraphics[width=\linewidth]{imgs/3.4/model_fits_comparison_var.pdf}
%     % \caption{Neural Network Model Fits with varying $\beta$ coefficients.}
%     % \label{fig:nn_fits}
%   \end{subfigure}
%     \caption{Left: Illustration of different mean posterior fits for various \(\beta\) constants. Right: Observation noise in the model for \(\beta = 0.1\) and \(\beta = 0\) (no observation noise). The introduction of a non-zero \(\beta\) parameter adds aleatoric noise to the mean posterior, thereby preventing overfitting and accounting for the spread of the data.\vspace{-0.15in}}
%   \label{fig:model_fits_comparison}
% \end{figure}
Given the linearization of our network, we can now look back to Theorem \ref{thrm: first} and observe that, at convergence under regularized gradient flow and given a random initialization, the output of the linearized network corresponds to the posterior mean of a Gaussian process with the analytical NTK as the kernel and non-zero observation noise. This provides a direct Bayesian interpretation of the regularized training process in terms of the NTK-GP framework. 
%However, as detailed in Appendix \ref{appendix: gaussian-distribution-convergence}, while the mean aligns with the GP posterior, the covariance of the output distribution over random initialization differs. It incorporates a mixture of NTK and NNGP kernels as noted in \cite{lee2019wide, he2020bayesian} and does therefore not admit a full Bayesian interpretation.
% follows the posterior mean of a Gaussian Process with the analytical NTK as the kernel and non-zero noise. In Appendix \ref{appendix: gaussian-distribution-convergence}, we provide further detail on how the covariance of the output $f^{\lin}_t(x)$ distribution at convergence over random initialization, differs from that of a Gaussian process and hence does not admit a full Bayesian posterior interpretation \cite{lee2019wide, he2020bayesian}.

% Loosely speaking, regularization penalizes deviations from initial parameters by incorporating an L2 loss term of the form \( \lVert \theta_t - \theta_0\rVert_2 \). This balance between minimizing the mean squared error (MSE) and maintaining proximity to the starting weights prevents the model from reaching an exact global minimiser. The radius of this region can be seen as the aleatoric noise \ref{fig:model_fits_comparison}. Consequently, regularization ensures that the model operates within a space where aleatoric uncertainty is reflected, preventing overfitting to noise.

It is important to highlight that the trained parameters $\theta_\infty$ generally depend on the specific initialization $\theta_0$, and as such, will differ for every random initialization. This variability is relevant to the training process but is not central to our analysis and use case. Unlike deep ensembles \cite{he2020bayesian} (particularly when used in the context of Thompson sampling \cite{thompson-sampling}), where the distribution of trained networks across initialization is explicitly used, our focus lies on the behaviour of a single trained network under a given initialization. We are able to alleviate the effect of this randomness thanks to our initialization strategy described in the next section.

\section{Neural Network Initialization as NTK-GPs with Arbitrary Prior Mean}
\label{sec-mu}

While standard initialization schemes for neural networks typically involve randomly setting weights and biases to break symmetry, they generally lack a principled way to align the network's prior outputs with specific inductive biases or desired properties. In contrast, Gaussian processes offer a well-defined framework for specifying prior distributions over functions through the prior mean and covariance. One key motivation for arbitrary initialization arises in reinforcement learning, where optimistic initialization facilitates efficient exploration by encouraging the model to prioritize under-explored regions \citep{strehl2006pac}. Similarly, in applications with strong inductive biases or prior knowledge, the ability to encode an arbitrary prior mean can enhance model performance.

One way to use the results presented in the previous section %describe convergence in distribution and in probability due to the randomness introduced by network initialization.
to to obtain the posterior of the NTK-GP with zero prior mean is to train several ensembles and average the outputs using the law of large numbers, akin to the approach by \citet{he2020bayesian}. 
%In practice, this requires implementing an ensemble of wide neural networks with different initializations to approximate the mean posterior. 
However, if we are only interested in computing the posterior mean and not the covariance, there is a better way, which only requires the use of one network.
Section \ref{sec-shift} explores how neural networks, under the NTK-GP framework, can be initialized to reflect arbitrary prior means, enhancing their flexibility to align with specific tasks. %Additionally, we show how this approach eliminates the need for ensembles by removing dependence on random initialization, enabling deterministic convergence with a single network.

% We demonstrate how the NTK-GP frameworks allow us to use GP machinery to come up with a principled way to initialize wide neural networks with arbitrary prior means by appropriately shifting the labels or predictions.

\subsection{Shifting the labels or predictions}
\label{sec-shift}
% \todo{Talk about what is actually the prior mean of an NTK GP (not sure right now).}
%By appropriately shifting the labels or predictions, we can introduce an arbitrary prior mean, allowing the model's initialization to reflect specific prior beliefs about the function space. 
Inspired by standard techniques in GP literature \cite{williams2006gaussian}, we provide a formal construction for modifying the network to introduce arbitrary prior mean. This can be accomplished either by shifting the predictions of the neural network by its predictions at initialization, or equivalently, defining a new shifted network. The following theorem formalizes this, resolving the main research question we posed in Section \ref{sec-motivation}.

\begin{restatable}{theorem}{shiftpredictions}
\label{lemma-shifted-predictions}
(Shifted Network.) Consider any function $m$. Given a random initialization $\theta_0$, define shifted predictions $\Tilde{f}_{\theta_0}(\mathbf{x}, \theta)$ as follows:
\begin{equation}
    \Tilde{f}_{\theta_0}(\mathbf{x}, \theta) := f(\mathbf{x}, \theta) - f(\mathbf{x}, \theta_0) + m(\mathbf{x}).
\end{equation}
Training this modified network (starting with $\theta_{0}$) leads to the following output (in the infinite-width limit)
\begin{equation}
    \tilde{f}_{\theta_0}(\mathbf{x'}, \theta_\infty) = m(\mathbf{x'}) + \mathbf{\Theta}_{\mathbf{x', x}}(\mathbf{\Theta}_{\mathbf{x, x}} + \beta I)^{-1}(\mathbf{y} - m(\mathbf{x})).
\end{equation}
This can be interpreted as the posterior mean of an NTK-GP with prior mean function $m$.
\end{restatable}
We prove this in Appendix \ref{appendix: Shifting the network at initialization}.
Note that $\tilde{f}_{\theta_0}(\mathbf{x'},\theta_{\infty})$ is non-random (i.e. independent of  the initialization $\theta_0$). This is different from training the non-shifted network, where $f_{\theta_0}(\mathbf{x'},\theta_{\infty})$ is random, but has a mean which corresponds to the posterior mean of an NTK-GP with prior mean $0$. Note, that the non-randomness of $\tilde{f}_{\theta_0}(\mathbf{x'},\theta_{\infty})$ is not immediately clear: while $\tilde{f}_{\theta_0}(x,\theta_0)$ does not depend on $\theta_0$, $\tilde{f}_{\theta_0}(x,\theta)$ differs for different $\theta_0$ when $\theta\neq \theta_0$. The reason for the non-randomness after training is that the NTK doesn't depend on the initialization in the infinite-width limit.

One drawback of shifting predictions is the need to store the initial parameters $\theta_0$, effectively doubling the memory requirements, which can be prohibitive for large networks. Additionally, performing a forward pass of the initial network to compute the shift adds a slight additional computational overhead. However, it is worth noting that no additional back-propagation is required through $f(\mathbf{x}, \theta_0)$ or $m(\mathbf{x})$, keeping the overall computational cost manageable and lower than even a two networks ensemble.

% One downside of shifting predictions is, that it requires saving the initial parameters $\theta_0$. This results in twice the memory cost, which can be prohibitive for large networks. In addition, it might also be worth considering the extra cost of the forward pass of the initial network for the shifting. On a more positive note [change], we remark that there are no further extra costs as there is no need to back-propagate through $f(\mathbf{x}, \theta_0)$ nor $m(\mathbf{x})$.

\subsection{Initializing the last layer weights as $0$}
In this section we explore a tempting but ultimately futile way of constructing a network with zero prior mean. One might be tempted to just initialize the last layer of the network to $0$, i.e. $\sigma_{w, L+1} = 0$, $\sigma_{b, L+1} = 0$. Then, the gradient with respect to any weights/biases not in the last layer will be $0$. In this case, the empirical NTK is given by
{\small
\begin{equation}
    \hat{\mathbf{\Theta}}_{\mathbf{x}',\mathbf{x}} 
    = \sum_{l=1}^{L+1} J(\mathbf{x}',\theta^l)J(\mathbf{x},\theta^l)^{\top} 
    = J(\mathbf{x}',\theta^{L+1})J(\mathbf{x},\theta^{L+1})^{\top},
\end{equation}}

where $\theta^l$ indicates the weights at layer $l$. We have,
\begin{equation}
    J(\mathbf{x},W^{L+1})
    = \frac{1}{\sqrt{n_L}}x^L(\mathbf{x}, \theta), \quad
    J(\mathbf{x},b^{L+1})
    = 1.
\end{equation}
Thus,
\begin{equation}
    \hat{\mathbf{\Theta}}_{\mathbf{x}',\mathbf{x}} = \frac{1}{n} x^L(\mathbf{x}',\theta) x^L(\mathbf{x},\theta)^{\top}  + 1.
\end{equation}
This actually converges to the Neural Network Gaussian Process (NNGP) kernel \cite{lee2018dnnsgps}, which does not align with the desired result.

However, our observation highlights that training the network in this configuration corresponds to performing inference in a Gaussian Process (GP) with a zero prior mean and the NNGP kernel as the covariance function. Specifically, in the case of the linearized network (or a network with a sufficiently large layer width), the partial derivatives with respect to the hidden layer parameters are zero. Thus, effectively \emph{only the last layer is trained}. As shown in \cite{lee2019wide}, this scenario is equivalent to performing inference with the NNGP, where the covariance of the trained network matches the posterior covariance of the NNGP.

% This converges to the NNGP-kernel, which is nonzero. Thus, training the network corresponds to doing inference in a GP with prior mean $0$, and the NNGP-kernel. Another viewpoint is the following: Consider the linearized network (or the network with large layer width). As the partial derivatives with respect to the parameters of the hidden layers are $0$, we are in the case of only training the last layer. It is known from \cite{lee2019wide}, that this corresponds to doing inference with the NNGP (with a fully Bayesian interpretation, as the covariance of the trained network is equal to the posterior covariance of the NNGP).
\section{Experiments}
% TODO. Decide whether or not we will include different priors. If not, modify the text below and from Section 5.1. 

In this section, we empirically validate our theoretical results by studying the convergence of wide neural networks (and parameters) trained with regularized gradient descent to their linearized counterparts. First, we analyze how the trained parameters deviate from the optimal linearized network parameters (i.e., the converged linear regression weights) and how this difference shrinks as the width increases. We then compare the network's predictions with those of kernel ridge regression on a test set, varying network depth and the regularization strength $\beta$. These experiments provide empirical support for our theoretical results in the previous sections.

\subsection{Experimental Setup}
To ensure alignment with the NTK framework, we train wide fully connected multi-layer perceptions (MLPs) under the NTK parametrization using full-batch gradient descent. The experiments are performed on a synthetic regression task where the target function is defined as $y = \sin(x) + \cos(2x) + \epsilon$, with $\epsilon \sim \mathcal{N}(0, \sigma^2)$ representing the observation noise. Inputs $x$ are uniformly sampled from the range $[-6, 6]$. We generate a dataset of a few hundred training points and use a two-layer neural network.
%Note that the dataset size is limited, because computing the Jacobians and inverting the kernel matrix to compare the wide neural network to the linear model is computationally prohibitive.

To compare the trained network parameters to their linearized counterparts, we compute the $l_2$ norm between the network parameters and those obtained via the kernel ridge regression using the NTK. Similarly, we evaluate the deviation between the trained neural network and the kernel ridge regression predictions on a test set by computing the squared error. Lastly, we repeat these experiments for a range of network depths and plot the mean and standard deviation over 5 different seeds. We will release the complete code on acceptance including a self-contained Jupyter notebook to ensure reproducibility.

\subsection{Empirical Convergence of the Parameters}\label{sec: 5.2}
We examine how the trained network parameters compare to those obtained via kernel ridge regression with the NTK. Specifically, we compute the $\ell_2$ norm difference between the final parameters of the trained network and the corresponding optimal linearized solution. Leveraging Theorem \ref{lemma-shifted-predictions}, we set the prior mean to zero for all experiments. The results, shown in Figure \ref{fig: params}, confirm that this difference decreases as the network width increases, supporting the theoretical prediction offered in Theorem \ref{thrm: 3.4}. 

\begin{figure}[h!]  
    \centering
    % First plot
    % \begin{subfigure}[b]{0.23\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/5.2/param_differences_multiple.pdf}
    % \end{subfigure}
    % \hfill
    % % Second plot
    % \begin{subfigure}[b]{0.23\textwidth}
    %     \centering
    %     \includegraphics[width=\linewidth]{imgs/5.2/param_differences_beta_05.pdf}
    % \end{subfigure}
    \caption{$\ell_2$ norm differences between the trained neural network's parameters and the kernel ridge regression solution plotted against network width for different $\beta$. Shaded regions represent the standard deviation divided by the square root of the number of seeds.}
    \label{fig: params}
\end{figure}

At smaller widths, we observe a notable deviation between the trained network and the linear regression solution, realizing the effect of finite width. As the width increases, the difference gradually declines, indicating the convergence towards the linearized model post-training. See Appendix \ref{app: extra-figures} for additional plots for different MLP depths.

%To ensure consistency with the theoretical framework, training was performed using full-batch gradient descent, as stochastic methods such as Adam would introduce additional variance not accounted for in our analysis and different learning rates for different parameters.

We note that computing and inverting the NTK matrix to compute the reference linear model presented a significant computational bottleneck, making it infeasible to scale to larger widths, depths, or dataset sizes. In contrast, training the neural network was substantially more efficient, even will full gradient descent.

\subsection{Empirical Convergence of the Trained Network to a Linear Model}\label{sec: 5.3}
We now shift our focus from parameter convergence to function-space convergence. In this experiment, we evaluate how closely the trained neural network approximates the predictions of the corresponding linearized model for unseen data. This validation dataset was sampled from the same distribution as the training dataset, comprising 20\% of its size. Specifically, we compute the following quantity:
\begin{equation*}
    \sup_{x \in \mathcal{V}} \lVert f(x, \theta_\infty) - f^{\lin}(x, \theta_\infty^{\lin}) \rVert_2,
\end{equation*}
where $\mathcal{V}$ is the validation set. This, in essence, represents the distance between the trained network's outputs and the kernel ridge regression solution evaluated in a validation set.
% \begin{figure}[h!]  
%     \centering
%     \includegraphics[width=0.9\linewidth]{imgs/5.3/function_differences_2.pdf}
%     \caption{Supremum of the L2 norm differences between the trained neural network's outputs \(f(x, \theta_\infty)\) and the linearized model's predictions \(f^{\text{lin}}(x, \theta_\infty^{\text{lin}})\) across the validation set, plotted against network width.\vspace{-0.15in}}
%     \label{fig: functions-diff}
% \end{figure}
\vspace{-0.1in}
\begin{figure}[h!]  
    \centering
    % \begin{subfigure}[b]{0.23\textwidth}
    \includegraphics[width=0.9\linewidth]{imgs/5.3/function_differences_multiple.pdf}
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}[b]{0.23\textwidth}
    %     \centering
    %     \includegraphics[width=\linewidth]{imgs/5.3/function_differences_beta_05.pdf}
    % \hfill
    % \begin{subfigure}[b]{0.3\textwidth}
    %     \centering
    %     \includegraphics[width=\linewidth]{imgs/5.3/function_differences_3.pdf}
    % \end{subfigure}
    \caption{Supremum of the $\ell_2$ norm differences between the trained neural network's outputs \(f(x, \theta_\infty)\) and the linearized model's predictions \(f^{\text{lin}}(x, \theta_\infty^{\text{lin}})\) across the validation set, plotted against network width.\vspace{-0.2in}}
    \label{fig: functions-diff}
\end{figure}

Figure \ref{fig: functions-diff} shows, under a similar setup as in Section \ref{sec: 5.2}, that as the width increases this discrepancy diminishes, providing further empirical confirmation of Theorem \ref{thrm: 3.4}. As in Section \ref{sec: 5.2}, more plots for different depths can be found in Appendix \ref{app: extra-figures}.

\section{Related Work}
\paragraph{Linearization} The paper by \citet{lee2019wide} serves as the starting point for our work. It demonstrates that as network width approaches infinity, training dynamics simplify and can be approximated by a linearized model using a first-order Taylor expansion. \citet{lee2019wide} also study the links between the output of trained neural networks and GPs. Crucially, we extend this work by proving that this linearization still holds in the presence of observation noise.

%Initialization plays a crucial role in neural network training. \citet{daniely2016toward} show that random weight initialization in neural networks provides a rich function space, hence making it a strong starting point for optimization. They highlight how initialization influences learning and motivates architectural design choices. Building on the importance of optimization, \citet{du2019gradient} show that gradient descent can achieve zero training loss in polynomial time despite the non-convexity of the loss landscape, extending convergence guarantees beyond two-layer multilayer perceptions (MLPs).

\paragraph{Kernel Methods and Neural Networks} 
The seminal paper by \citet{jacot2018neural} made two contributions. First, it introduced the equivalence between kernel methods and wide neural networks, specifically for the case of kernel ridge regression. Second, it popularized the study of neural networks in function space, rather than parameter space. We leverage both of these insights: our Theorem \ref{lemma-shifted-predictions} considers the function-space view of a Gaussian Process (a kernel method).  Several later studies have explored the links between wide neural networks and GPs through the NTK, to investigate the functional behaviour of neural networks in noisy settings. For example, \citet{rudner2023functionspaceregularizationneuralnetworks} introduced function-space regularization to encode desired properties into predictions, indirectly addressing observation noise, while \citet{chen2022neural} linked NTK-based function norms to RKHS regularization, proving to be useful in low-data regimes.

\paragraph{Global Minima and Overparameterization} In the context of overparametrization, \citet{allenzhu2019convergence} prove that stochastic gradient descent (SGD) can find global minima for neural networks in polynomial time. Similarly, \citet{zou2020gradient} show that ReLU networks trained with SGD converge to global minima for a wide range of loss functions by ensuring that weight updates remain within a small perturbation around the initialization. While we do not rely on these results directly, our result is spiritually related in that we guarantee convergence to the global optimum with high probability.

%Furthermore, another closely related form of regularization is early stopping, which can be seen as a form of implicit regularization. This is explored by \citet{wei2017early}, who establish a framework linking early stopping with regularization and derive optimal stopping rules for boosting algorithms. Additionally, \citet{li2020gradient} provide theoretical guarantees that gradient descent initially fits correct labels while ignoring noisy ones, demonstrating that early stopping can be robust to label noise when training remains close to the initialization.

\paragraph{Regularization} A line of work has explored the role of regularization in wide neural networks through the lens of the NTK. \citet{hu2020simpleeffectiveregularizationmethods} introduced the regularizer penalizing deviations from initialization, providing generalization bounds in noisy settings but assuming network linearity without proof. \citet{nitanda2020optimal} and \citet{suh2021non} extended this approach to constrain network dynamics and stabilize deeper architectures. 

\paragraph{Bayesian Ensembles} \citet{he2020bayesian} described a way of training Bayesian ensembles of neural networks, allowing for inference in the NTK-GP with zero prior mean by averaging the ensembles using the law of large numbers. In cases where we are only interested in obtaining the posterior mean, our approach is more efficient since we only train one network\footnote{They additionally provide a way of estimating the posterior covariance, which is not of interest in our paper.}.

% \cite{du2019gradient} demonstrates that gradient descent achieves zero training loss in polynomial time despite the non-convexity of the optimization landscape. Compared to previous results, this work extends convergence guarantees beyond two-layer MLPs.

% Similarly, \cite{zou2020gradient} shows how ReLU networks under SGD converge to the global minima for a broad family of loss functions. This holds thanks to the sequence of weight updates remaining within a small perturbation region around the initialization. Placing more emphasis on how different training regimes impact generalization and convergence rates.

% \cite{wei2017early} establish a framework that links early stopping with implicit regularization. They derive optimal stopping rules for boosting algorithms.

% \cite{li2020gradient} provide theoretical guarantees showing that gradient descent initially fits the correct labels while ignoring noisy ones, as long as training remains close to initialization. Concluding that early stopping is provably robust to label noise.

\section{Conclusion and Future Work}
We studied the relationship between regularized training in wide neural networks and Gaussian Processes under the NTK framework. We are the first work formally showing that the proposed weight-space regularization in neural networks is equivalent to adding aleatoric noise to the NTK-GP posterior mean in the infinite-width limit. Additionally, we introduced a shifted network approach that enables arbitrary prior functions and ensures deterministic convergence to the NTK-GP posterior mean without requiring ensembles or kernel inversion. Empirical results validate our theoretical findings, demonstrating convergence to the linearized network.
% and the impact of different prior mean choices.

Future work could extend our analysis to other architectures with NTK convergence, such as convolutional and residual networks \cite{arora2019exact, belfer2024spectral, yang2020tensorprograms2}, to evaluate the effect of our regularizer in these settings.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}
\section*{Impact Statement}
Our work contributes to deep learning theory by clarifying the relationship between wide neural networks and Gaussian Processes with observation noise. It strengthens the theoretical foundation of deep learning, which supports many practical applications. Understanding neural network training dynamics and their Bayesian interpretation may aid future improvements in model design and optimization. Our work does not pose any immediate societal or ethical issues.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
\section{Convergence Plots for Different Noise Coefficients and Network Depths}
\label{app: extra-figures}
\begin{figure}[h!]  
    \centering
    % First Figure (First 4 Images)
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/5.2/param_differences_1.pdf}
    \end{subfigure}
    \hspace{0.2in}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/5.2/param_differences_3.pdf}
    \end{subfigure}

    \vspace{0.2in} % Adds vertical space

    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/5.3/function_differences_1.pdf}
    \end{subfigure}
    \hspace{0.2in}
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/5.3/function_differences_3.pdf}
    \end{subfigure}

    \caption{Parameter and function differences for additional network depths. (Top row) Parameter difference plots from Section \ref{sec: 5.2}. (Bottom row) corresponds to the function difference plots from Section \ref{sec: 5.3}. (Left) Results for one fully connected hidden layer. (Right) Results for an MLP with three fully connected hidden layers. In all cases, increasing the network width reduces both parameter and function differences, confirming the theoretical predictions. $\beta = 0.1$ was used.}
    \label{fig:first_four}
\end{figure}

% \begin{figure}[h!]  
%     \centering
%     \begin{subfigure}[b]{0.4\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{imgs/5.2/param_differences_beta_1.pdf}
%     \end{subfigure}
%     \hspace{0.2in}
%     \begin{subfigure}[b]{0.4\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{imgs/5.3/function_differences_beta_1.pdf}
%     \end{subfigure}
%     \caption{Parameter and function differences for different regularization strengths. Plots for \(\beta = 1.0\). (Left) Parameter difference between the trained network and its linearized counterpart. (Right) Function difference on a test set.}
%     \label{fig:last_four}
% \end{figure}

\section{Regularized gradient flow and gradient descent for the linearized network}
\label{appendix: regularized gf/gd for linearized network}
Consider the linearized network $f^{\lin}(x, \theta) = f(x,\theta_0) + J(x,\theta_0)(\theta-\theta_0)$. For notational convenience, we drop the dependence on $\theta_0$ throughout the Appendix. In the following, we will consider training the parameters using the regularized training loss 
\begin{equation}
    \mathcal{L}^{\beta, \lin}(\theta) := \frac{1}{2}\sum_{i=1}^N (f^{\lin}(\mathbf{x}_i, \theta) - \mathbf{y}_i)^2 + \frac{1}{2}\beta \lVert \theta - \theta_0 \rVert_2^2.
\end{equation}
\subsection{Regularized gradient flow for the linearized network}
 Then, the evolution of the parameters through gradient flow with learning rate $\eta_0$ is given by
\begin{align}
    \frac{d\theta^{\lin}_t}{dt} 
    &= -\eta_0 \left( J(\theta_0)^{\top} g^{\lin}(\theta^{\lin}_t) + \beta (\theta^{\lin}_t - \theta_0) \right) \\
    &= -\eta_0 \left(J(\theta_0)^{\top} \left( f(\theta_0) + J(\theta_0)(\theta^{\lin}_t - \theta_0) - \mathbf{y} \right) + \beta (\theta^{\lin}_t - \theta_0) \right) \\
    &= -\eta_0 \left(J(\theta_0)^{\top}J(\theta_0) + \beta I_p\right)(\theta^{\lin}_t-\theta_0) - \eta_0J(\theta_0)^{\top}(f(\theta_0)-\mathbf{y}).
\end{align}
This is a multidimensional linear ODE in $\theta^{\lin}_t-\theta_0$. Its unique solution is given by
\begin{align}
    \theta^{\lin}_t 
    &= \theta_0 +  \left( e^{-\eta_0\left(J(\theta_0)^{\top}J(\theta_0) + \beta I_p\right)t} - I_p \right) \left(-\eta_0\left(J(\theta_0)^{\top}J(\theta_0) + \beta I_p \right)\right)^{-1} \left( - \eta_0J(\theta_0)^{\top}(f(\theta_0) - \mathbf{y} ) \right) \\
    &= \theta_0 + \left( I_p - e^{-\eta_0\left(J(\theta_0)^{\top}J(\theta_0) + \beta I_p\right)t} \right) \left(J(\theta_0)^{\top}J(\theta_0) + \beta I_p \right)^{-1} J(\theta_0)^{\top} (\mathbf{y} - f(\theta_0)) \\
    &= \theta_0 + \left( I_p - e^{-\eta_0\left(J(\theta_0)^{\top}J(\theta_0) + \beta I_p\right)t} \right) J(\theta_0)^{\top}\left(J(\theta_0)J(\theta_0)^{\top} + \beta I_N \right)^{-1}  (\mathbf{y} - f(\theta_0)) \\
    &= \theta_0 + \left( J(\theta_0)^{\top} - e^{-\eta_0\left(J(\theta_0)^{\top}J(\theta_0) + \beta I_p\right)t} J(\theta_0)^{\top} \right) \left(J(\theta_0)J(\theta_0)^{\top} + \beta I_N \right)^{-1}  (\mathbf{y} - f(\theta_0)) \\
    &= \theta_0 + J(\theta_0)^{\top} \left(I_N - e^{-\eta_0\left(J(\theta_0)J(\theta_0)^{\top} + \beta I_N \right)t} \right) \left(J(\theta_0)J(\theta_0)^{\top} + \beta I_N \right)^{-1}  (\mathbf{y} - f(\theta_0)).
\end{align}
In the third and fourth equality, we used that for $k\in \mathbb{Z}$,
\begin{equation}
    \left(J(\theta_0)^{\top}J(\theta_0) + \beta I_p \right)^{k} J(\theta_0)^{\top} = J(\theta_0)^{\top}\left(J(\theta_0)J(\theta_0)^{\top} + \beta I_N \right)^{k}.
\end{equation}
Plugging $\theta^{\lin}_t$ into the formula for the linearized network, we get for any point $\mathbf{x}'$,
\begin{align}
    f^{\lin}(\mathbf{x'},\theta^{\lin}_t)
    &= f(\mathbf{x'},\theta_0) + J(\mathbf{x'},\theta_0)J(\theta_0)^{\top} \left( I_N- e^{-\eta_0\left(J(\theta_0)J(\theta_0)^{\top} + \beta I_N\right)t} \right) \left(J(\theta_0)J(\theta_0)^{\top} + \beta I_N \right)^{-1}  (\mathbf{y} - f(\theta_0)) \\
    &=f(\mathbf{x'},\theta_0) + \hat{\mathbf{\Theta}}_{\mathbf{x',x}}\left(I_N - e^{-\eta_0(\hat{\mathbf{\Theta}}_{\mathbf{x,x}} + \beta I_N)t}  \right) \left(\hat{\mathbf{\Theta}}_{\mathbf{x,x}} + \beta I_N \right)^{-1} (\mathbf{y} - f(\theta_0)) .
\end{align}
For training time $t\to\infty$, this gives
\begin{equation}
    f^{\lin}(\mathbf{x'},\theta^{\lin}_\infty)
    = f(\mathbf{x'},\theta_0) + \hat{\mathbf{\Theta}}_{\mathbf{x',x}}\left(\hat{\mathbf{\Theta}}_{\mathbf{x,x}} + \beta I_N \right)^{-1}  (\mathbf{y} - f(\theta_0)).
\end{equation}

\subsection{Regularized gradient descent for the linearized network}
Similarly to gradient flow, the evolution of the parameters through gradient descent (when training the regularized loss given by the linearized network) with learning rate $\eta_0$ is given by
\begin{equation}
    \theta^{\lin}_{t} = \theta^{\lin}_{t-1} -\eta_0 \left(J(\theta_0)^{\top}J(\theta_0) + \beta I_p\right)(\theta^{\lin}_{t-1}-\theta_0) - \eta_0J(\theta_0)^{\top}(f(\theta_0)-\mathbf{y}).
\end{equation}
One may write this as
\begin{equation}
    \theta^{\lin}_{t} - \theta_0 
    = \left(I_p - \eta_0 \left(J(\theta_0)^{\top}J(\theta_0) + \beta I_p\right)\right)(\theta^{\lin}_{t-1}-\theta_0) - \eta_0 J(\theta_0)^{\top} (f(\theta_0) - \mathbf{y}).
\end{equation}
Applying the formula for $\theta_{t}^{\lin}-\theta_0$ iteratively, leads to the following geometric sum:
\begin{align}
    \theta^{\lin}_{t} - \theta_0 
    &= -\eta_0 \sum_{u=0}^{t-1} \left(I_p - \eta_0 \left(J(\theta_0)^{\top}J(\theta_0) + \beta I_p\right)\right)^u J(\theta_0)^{\top}(f(\theta_0) - \mathbf{y}) \\
    &= \eta_0 J(\theta_0)^{\top} \sum_{u=0}^{t-1} \left(I_N - \eta_0 \left(J(\theta_0)J(\theta_0)^{\top} + \beta I_N\right)\right)^u (\mathbf{y} - f(\theta_0)) \\
    &= J(\theta_0)^{\top} \left(I_N - \left(I_N - \eta_0\left(J(\theta_0)J(\theta_0)^{\top} + \beta I_N\right)\right)^{t} \right) \left(J(\theta_0)J(\theta_0)^{\top} + \beta I_N\right)^{-1} (\mathbf{y} - f(\theta_0)).
\end{align}
This converges for $t\to\infty$ if and only if $0 < \eta_0 < \frac{2}{\lambda_{\text{max}}\left( J(\theta_0)J(\theta_0)^{\top}\right) + \beta}$. In that case, it converges (as expected) to the same $\theta_{\infty}^{\lin}$ as the regularized gradient flow. Plugging $\theta_t^{\lin}$ into the formula for the linearized network, we get for any point $\mathbf{x'}$, 
{\small
\begin{align}
    f^{\lin}(\mathbf{x'},\theta^{\lin}_t)
    &= f(\mathbf{x'},\theta_0)  + J(\mathbf{x'},\theta_0)J(\theta_0)^{\top} \left(I_N - \left(I_N - \eta_0\left(J(\theta_0)J(\theta_0)^{\top} + \beta I_N\right)\right)^{t} \right) \left(J(\theta_0)J(\theta_0)^{\top} + \beta I_N\right)^{-1} (\mathbf{y} - f(\theta_0)) \\
    &= f(\mathbf{x'},\theta_0) + \hat{\mathbf{\Theta}}_{\mathbf{x',x}}  \left(I_N - \left( I_N - \eta_0\left(\hat{\mathbf{\Theta}}_{\mathbf{x,x}} + \beta I_N\right)\right)^{t} \right) \left(\hat{\mathbf{\Theta}}_{\mathbf{x,x}} + \beta I_N \right)^{-1} (\mathbf{y} - f(\theta_0)).
\end{align}
}


\section{Revisiting standard and NTK parametrizations, and convergence at initialization}
\label{appendix: standard and NTK parametrization}
In the following, we revisit the standard and the NTK parametrization. First, we repeat the result about the convergence of the NTK for the NTK parametrization at initialization. Then, we formally state how the NTK and standard parametrization are related, which makes it possible to proof the results for standard parametrization by using the results for NTK parametrization. Finally, we argue that using the same learning rate for every parameter under standard parametrization leads to redundancies in the NTK for the first layer and the biases.

\subsection{Convergence of NTK under NTK parametrization at initialization}
Here, we restate the following Theorem from \citet{yang2020tensorprograms2} about the convergence of the NTK at initialization. This was first shown in \citet{jacot2018neural} when taking the limit of layer widths sequentially. 

\begin{theorem}
\label{theorem. NTK initialization convergence}
Consider a standard feedforward neural network in NTK parametrization. Then, the empirical NTK $\hat{\mathbf{\Theta}}_{\mathbf{x',x}}$ converges to a deterministic matrix $\mathbf{\Theta}_{\mathbf{x',x}}$, which we call the analytical NTK:
\begin{equation}
    \hat{\mathbf{\Theta}}_{x',x} 
    = J(x', \theta_0)J(x, \theta_0)^{\top}
    = \sum_{l=1}^{L+1} \left( J(x', W^l)J(x, W^l)^{\top} + J(x', b^l)J(x, b^l)^{\top} \right)
    \xrightarrow{p} \mathbf{\Theta}_{x', x},
\end{equation}
for layer width $n\to \infty$.
\end{theorem}
Define $\mathbf{\Theta} := \mathbf{\Theta}_{\mathbf{x}, \mathbf{x}} \in \mathbb{R}^{N\times N}$ as the analytical NTK on the training points.
We will assume $\lambda_{\min}(\mathbf{\Theta}) > 0$. A sufficient condition for this is that $\lVert \mathbf{x}_i \rVert_2 = 1 \; \forall i$, and that $\phi$ grows non-polynomially for large $x$, see \citet{jacot2018neural}. This directly implies that for $n$ large enough, the minimum eigenvalue of the analytical NTK is lower bounded by a positive number with high probability:
\begin{lemma}
\label{lemma: empirical NTK positive eigenvalue}
For any $\delta_0 > 0$, there is $n$ large enough, such that with probability of at least $1-\delta_0$, for the minimum eigenvalue of the empirical NTK,
\begin{equation}
    \lambda_{\min}\left(J(\theta_0)J(\theta_0)^{\top} \right) \ge \frac{1}{2}\lambda_{\min}(\mathbf{\Theta}),
    \text{ and } \lambda_{\max}\left(J(\theta_0)J(\theta_0)^{\top} \right) \le 2\lambda_{\max}(\mathbf{\Theta}).
\end{equation}
\end{lemma}

\subsection{Equivalence of NTK parametrization to standard parametrization with layer-dependent learning rates}
\label{appendix: Standard param to NTK subappendix}
In this section, we will formally show how the NTK parametrization relates to the standard parametrization of neural networks. This makes it possible to prove results for standard parametrization by using the results for NTK parametrization, instead of having to prove them again.

Remember that the number of parameters is $p = \sum_{l=1}^{L+1} (n_{l-1} + 1) n_l $.
Define the diagonal matrix $H \in \mathbb{R}^{p\times p}$ through 
\begin{equation}
H := \diag(H_{w,1}, H_{b,1}, \ldots, H_{w,L+1}, H_{b, L+1}),
\end{equation}
where $H_{w,l} := \frac{1}{n_{l-1}} I_{n_{l-1}n_l}$, and $H_{b,l} := I_{n_l}$. The diagonal of $H^{\frac12}$ contains the scalars by which each parameter is multiplied when going from NTK parametrization to standard parametrization. For $\theta^{\std}_0$ initialized in standard parametrization, define
\begin{equation}
    \theta^{\ntk}_0 := H^{-\frac12} \theta^{\std}_0.
\end{equation}
Then, $\theta^{\ntk}_0$ is initialized as in NTK parametrization. Further, let $f^{\std}$ denote a neural network in standard parametrization. Then, 
\begin{equation}
    f^{\ntk}(x, \theta^{\ntk}) := f^{\std}(x, H^{\frac12}\theta^{\ntk}),
\end{equation}
defines a neural network in NTK parametrization. Differentiating gives
\begin{equation}
    J^{\ntk}(x,\theta^{\ntk}) = J^{\std}(x,H^{\frac12}\theta^{\ntk}) H^{\frac12}, \quad
    \hat{\mathbf{\Theta}}_{x',x}^{\ntk} = J^{\std}(x',H^{\frac12}\theta^{\ntk}) H J^{\std}(x,H^{\frac12}\theta^{\ntk}).
\end{equation}

Motivated by this, define the following regularized loss:
\begin{equation}
    \mathcal{L}^{\beta, \std}(\theta) := \frac{1}{2}\sum_{i=1}^N (f^{\std}(\mathbf{x}_i, \theta) - \mathbf{y}_i)^2 + \frac12 \beta (\theta-\theta_0)^{\top}H^{-1}(\theta-\theta_0).
\end{equation}
The resulting gradient is
\begin{equation}
    \nabla_{\theta} \mathcal{L}^{\beta, \std}(\theta) 
    = J^{\std}(\theta)^{\top} g^{\std}(\theta) + \beta H^{-1} (\theta-\theta_0) .
\end{equation}
Define $\theta^{\std}_t$ as parameters evolving by gradient flow of the regularized objective in standard parametrization\footnote{The existence of a unique solution of this ODE will follow from the relation to the gradient flow under NTK parametrization.}, with layer-dependent learning rate $\eta_0H$:
\begin{equation}
    \frac{d\theta^{\std}_t}{dt} 
    = -\eta_0H \nabla_{\theta}\mathcal{L}^{\beta,\std}(\theta^{\std}_t)
    = -\eta_0 H J^{\std}(\theta^{\std}_t)^{\top}g^{\std}(\theta^{\std}_t) - \eta_0 \beta(\theta^{\std}_t - \theta^{\std}_0).
\end{equation}
We define $\theta^{\ntk}_t := H^{-\frac12}\theta^{\std}_t$. Then, as $\frac{d\theta^{\ntk}_t}{dt} = H^{-\frac12}\frac{d\theta^{\std}_t}{dt} $,
\begin{align}
    \frac{d\theta^{\ntk}_t}{dt}
    &= -\eta_0 H^{\frac{1}{2}}J^{\std}(\theta^{\std}_t)^{\top}g^{\std}(\theta^{\std}_t) - \eta_0 H^{-\frac12}\beta (\theta^{\std}_t - \theta^{\std}_0) \\
    &= -\eta_0 \left(J^{\std}(H^{\frac12}\theta^{\ntk}_t) H^{\frac12} \right)^{\top} g^{\std}(H^{\frac12}\theta^{\ntk}_t) - \eta_0\beta (\theta^{\ntk}_t - \theta^{\ntk}_0) \\
    &= -\eta_0 J^{\ntk}(\theta^{\ntk}_t)^{\top} g^{\ntk}(\theta^{\ntk}_t) - \eta_0\beta (\theta^{\ntk}_t - \theta^{\ntk}_0).
\end{align}
Thus, $\theta^{\ntk}_t$ follows the regularized gradient flow of the objective under NTK parametrization with learning rate $\eta_0$. Now, we can apply our results for NTK parametrization from above, and transfer them to standard parametrization by using $\theta^{\std}_t = H^{\frac12}\theta^{\ntk}_t$, $f^{\std}(x,\theta^{\std}_t) = f^{\ntk}(x,H^{-\frac12}\theta^{\std}_t)$.

\subsection{Redundancies when using the same learning rate for standard parametrization}
In the previous section, we established the equivalence between training a neural network in NTK parametrization with learning rate $\eta_0$, and a neural network in standard parametrization with layer-dependent learning rate $\eta_0 H$. By definition, the learning rate for the first layer is $\frac{1}{n_0}\eta_0 = \frac{1}{d}\eta_0$, and the one for the biases is $\eta_0$. The learning rate for the other weight matrices is $\frac{1}{n_{l-1}}\eta_0 = \frac{1}{n}\eta_0$, for $l=2,\ldots,L+1$. Note that the convergence of the learning rates to $0$ for $n\to\infty$ is necessary to stabilize the gradient.

The learning rate that was used in the proof of \citet{lee2019wide} is $\frac{1}{n}\eta_0$ for any layer. In the following, we will argue that this effectively leads to the first layer, and the biases not being trained in the infinite-width limit. For simplicity, let $\beta=0$. Remember that \citet{lee2019wide} shows that using the learning rate $\frac{1}{n}\eta_0$ for each layer in standard parametrization, leads to the trained network for large width being driven by the standard parametrization NTK 
\begin{equation}
    \frac{1}{n}J^{\std}(x',\theta_0)J^{\std}(x,\theta_0)^{\top}
    = \frac{1}{n} \sum_{l=1}^{L+1} \left(J^{\std}(x', W^l_0)J^{\std}(x,W^l_0)^{\top} + J^{\std}(x', b^l_0)J^{\std}(x,b^l_0)^{\top}\right).
\end{equation}
By using the equivalences from the previous section, we may write for $l=2,\ldots,L+1$ (using $H_{w,l} = \frac{1}{n}I_{n_{l-1}n_l}$):
\begin{align}
    \frac{1}{n} J^{\std}(x',W_0^l)J^{\std}(x,W_0^l)^{\top}
    &= \left( J^{\std}(H_{w,l}^{\frac12}\sqrt{n}W_0^l) H_{w,l}^{\frac12}\right) \left( J^{\std}(H_{w,l}^{\frac12}\sqrt{n}W_0^l) H_{w,l}^{\frac12}\right)^{\top} \\
    &= J^{\ntk}(\sqrt{n}W_0^l) J^{\ntk}(\sqrt{n}W_0^l)^{\top}.
\end{align}
This is equal to the empirical NTK under the NTK parametrization for the weights $\sqrt{n}W_{0,i,j}^l \sim \mathcal{N}(0,\sigma_{w,l})$ of the $l$-th layer.
However, for the first layer, we get (using $H_{w,1} = \frac{1}{d}I_{dn}$)
\begin{align}
    \frac{1}{n} J^{\std}(x', W_0^1)J^{\std}(x, W_0^1)^{\top}
    &= \frac{d}{n} \left( J^{\std}(H_{w,1}^{\frac12}\sqrt{d}W_0^1) H_{w,1}^{\frac12}\right) \left( J^{\std}(H_{w,1}^{\frac12}\sqrt{d}W_0^1) H_{w,1}^{\frac12}\right)^{\top} \\
    &= \frac{d}{n} J^{\ntk}(\sqrt{d}W_0^1) J^{\ntk}(\sqrt{d}W_0^1)^{\top} \\
    &\to 0, \text{ for } n\to \infty,
\end{align}
as $J^{\ntk}(\sqrt{d}W_0^1) J^{\ntk}(\sqrt{d}W_0^1)^{\top}$ converges by Theorem \ref{theorem. NTK initialization convergence}.
Similarly for the biases, for $l=1,\ldots,L+1$:
\begin{equation}
    \frac{1}{n}J^{\std}(x',b_0^l)J^{\std}(x,b_0^l)^{\top}
    = \frac{1}{n} J^{\ntk}(x', b_0^l)J^{\ntk}(x,b_0^l)^{\top}
    \to 0, \text{ for } n\to \infty.
\end{equation}
Thus, the analytical standard parametrization NTK of \citet{lee2019wide} doesn't depend on the contribution of the gradient with respect to the first layer and the biases. In other words, using the learning rate $\frac{1}{n}\eta_0$ for the first layer and the biases leads to them not being trained for large widths.

Instead, one may scale the learning rates ``correctly", as motivated by the NTK parametrization in the previous section. For large widths $n$, the trained network is then governed by the following modified NTK for standard parametrization:
{\small
\begin{equation}
    J^{\std}(\theta)H J^{\std}(\theta)
    = J^{\std}(W^1)J^{\std}(W^1)^{\top} + J^{\std}(b^1)J^{\std}(b^1)^{\top} + \sum_{l=2}^{L+1} \left( \frac{1}{n}J^{\std}(W^l)J^{\std}(W^l)^{\top} + J^{\std}(b^l)J^{\std}(b^l)^{\top} \right).
\end{equation}
}
For simplicity, we will not train the first layer and the biases.
\section{Local Lipschitzness and Boundedness of the Jacobian}
\label{appendix: local lipschitz jacobian}
The goal of this section is to prove the following Lemma:
\lipschitznesslemma*

Here, the Frobenius-norm is used to aggregate over different training points, i.e. $\lVert J(\theta)\rVert_F^2 = \sum_{i=1}^N \lVert J(\mathbf{x}_i, \theta) \rVert_2^2$.

\citet{lee2019wide} proved this Lemma for standard parametrization. In their version for NTK parametrization (Lemma 2), there unfortunately is a typo stating that the Lipschitz-constant of the Jacobian is $O(1)$ instead of $O(\frac{1}{\sqrt{n}})$, which is why we quickly go over how to get this Lemma. As mentioned in the previous section, we don't train the first layer and the biases for simplicity.
Let $f^{\ntk}$ be the network in NTK parametrization, and let $\theta_0$ be randomly initialized according to the NTK parametrization. Further, let $\theta, \tilde{\theta} \in B(\theta_0, C)$. Then, $f^{\std}(\theta) := f^{\ntk}(\sqrt{n}\theta)$ is a network in standard parametrization, and $\frac{1}{\sqrt{n}}\theta_0$ is randomly initialized in standard parametrization. Further, $\frac{1}{\sqrt{n}}\theta, \frac{1}{\sqrt{n}}\tilde{\theta} \in B(\frac{1}{\sqrt{n}}\theta_0, \frac{1}{\sqrt{n}}C)$. Note, that $J^{\ntk}(\theta) = \frac{1}{\sqrt{n}}J^{\std}(\frac{1}{\sqrt{n}}\theta)$. Now, by applying Lemma 1 from \citet{lee2019wide} to the network in standard parametrization and the parameters $\frac{1}{\sqrt{n}}\theta_0, \frac{1}{\sqrt{n}}\theta, \frac{1}{\sqrt{n}}\tilde{\theta}$, we get with high probability over random initialization:
\begin{equation}
    \lVert J^{\ntk}(x,\theta) \rVert_2 
    = \frac{1}{\sqrt{n}}\lVert J^{\std}(x,\frac{1}{\sqrt{n}}\theta)\rVert_2
    \le \frac{1}{\sqrt{n}} \sqrt{n} K'
    = K',
\end{equation}
and
\begin{align}
    \lVert J^{\ntk}(x,\theta) - J^{\ntk}(x,\theta) \rVert_2
    &= \frac{1}{\sqrt{n}} \lVert J^{\std}(x, \frac{1}{\sqrt{n}}\theta) - J^{\std}(x, \frac{1}{\sqrt{n}}\tilde{\theta}) \rVert_2 \\
    &\le \frac{1}{\sqrt{n}} \sqrt{n} K' \lVert \frac{1}{\sqrt{n}}\theta - \frac{1}{\sqrt{n}}\tilde{\theta} \rVert_2
    = \frac{1}{\sqrt{n}}K' \lVert \theta - \tilde{\theta} \rVert_2.
\end{align}


\section{Proof for regularized gradient flow}
\label{appendix: Proof regularized gradient flow}
\subsection{Exponential Decay of the Regularized Gradient and Closeness of Parameters to their Initial Value}
\label{appendix: proof regularized gradient flow part 1 (exponential decay)}
\begin{lemma}
\label{lemma: g(theta0) bounded}
Let $\beta\ge 0$. We have for any $t\ge 0$: $\lVert g(\theta_t) \rVert_2 \le \lVert g(\theta_0) \rVert_2$. Further, for any $\delta_0>0$, there is $R_0>0$, such that for $n$ large enough, with probability of at least $1-\delta_0$ over random initialization, $\lVert g(\theta_0) \rVert_2 \le R_0$.
\end{lemma}
\begin{proof}
Using the chain rule and the definition of the gradient flow, we have
\begin{equation}
    \frac{d}{dt} \mathcal{L}^{\beta}(\theta_t)
    = \nabla_{\theta}\mathcal{L}^{\beta}(\theta_t) ^{\top} \frac{d\theta_t}{dt}
    =-\eta_0 \nabla_{\theta}\mathcal{L}^{\beta}(\theta_t) ^{\top} \nabla_{\theta}\mathcal{L}^{\beta}(\theta_t)
    =-\eta_0 \lVert \nabla_{\theta}\mathcal{L}^{\beta}(\theta_t) \rVert_2^2 \le 0.
\end{equation}
Thus, 
\begin{equation}
    \frac12\lVert g(\theta_t) \rVert_2^2 
    \le \frac12\lVert g(\theta_t) \rVert_2^2 + \frac12\lVert \theta_t - \theta_0 \rVert_2^2
    = \mathcal{L}^{\beta}(\theta_t) 
    \le \mathcal{L}^{\beta}(\theta_0) 
    = \frac{1}{2}\lVert g(\theta_0) \rVert_2^2.
\end{equation}
Hence, $\lVert g(\theta_t) \rVert_2 \le \lVert g(\theta_0) \rVert_2$. Further, note that $f(\theta_0)$ converges in distribution to a Gaussian with mean zero and covariance given by the NNGP kernel \cite{lee2018dnnsgps}. Thus, for $n$ large enough, one can bound $\lVert g(\theta_0) \rVert_2$ with high probability.
\end{proof}

We restate Theorem \ref{theorem: Exponential decay gradient, parameters stay close} for convenience.
\gftheorempartone*
\begin{proof}
Using Lemma \ref{lemma: g(theta0) bounded}, there is $R_0>0$, such that for $n$ large enough, with probability of at least $1-\frac{1}{3}\delta_0$ over random initialization, $\lVert g(\theta_0) \rVert_2 \le R_0$.
Further, using Lemma \ref{lemma: Jacobian Lipschitz}, let $K$ be the constant for local Lipschitzness/Boundedness of the Jacobian with probability $1-\frac{1}{3}\delta_0$ for $n$ large enough.
Finally, by Lemma \ref{lemma: empirical NTK positive eigenvalue}, for $n$ large enough, with probability of at least $1-\frac{1}{3}\delta_0$ over random initialization, the minimum eigenvalue of the empirical NTK is lower bounded: $\lambda_{\min}(J(\theta_0) J(\theta_0)^{\top}) \ge \frac12 \lambda_{\min}(\mathbf{\Theta})$.\footnote{We will only need this for $\beta = 0$.}
For $n$ large enough, these three events hold with probability of at least $1-\delta_0$ over random initialization. In the following, we consider such initializations $\theta_0$.

Define $c_{\beta} := \frac{1}{2}\beta$ for $\beta > 0$, and $c_{\beta} := \frac{1}{3}\lambda_{\min}(\mathbf{\Theta})$ for $\beta = 0$.\footnote{We note that we could choose any constant smaller than $\beta$ for $\beta > 0$, and similarly, and constant smaller than $\lambda_{\min}(\mathbf{\Theta})$ for $\beta = 0$.} Let $C := \frac{KR_0}{c_{\beta}}$. By Lemma \ref{lemma: Jacobian Lipschitz}, the gradient flow ODE has a unique solution as long as $\theta_t \in B(\theta_0,C)$. Consider $t_1 := \inf\{t\ge 0: \lVert \theta_t - \theta_0 \rVert_2 \ge C \}$. In the following, let $t\le t_1$. Remember that
\begin{equation}
    \frac{d\theta_t}{dt} = -\eta_0 \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) = -\eta_0 \left(J(\theta_t)^{\top}g(\theta_t) + \beta(\theta_T-\theta_0) \right).
\end{equation}
We want to show that the gradient $\nabla_{\theta} \mathcal{L}^{\beta}(\theta_t)$ of the regularized loss converges to $0$ quickly, and hence $\theta_t$ doesn't move much. For $\beta=0$, it's norm is $\lVert J(\theta_t)^{\top}g(\theta_t)\rVert_2$ and hence \citet{lee2019wide} and related proofs showed that $\lVert g(\theta_t) \rVert_2$ converges to $0$ quickly. However, for $\beta>0$, this is not the case, as the training error won't converge to $0$. Instead, we directly look at the dynamics of the norm of the gradient:
\begin{align}
    \frac{d}{dt}\lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) \rVert_2^2
    &= 2 \left( \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) \right)^{\top} \nabla_{\theta}^2 \mathcal{L}^{\beta}(\theta_t) \frac{d\theta_t}{dt} \\
    &= -2\eta_0 \left( \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) \right)^{\top} \nabla_{\theta}^2 \mathcal{L}^{\beta}(\theta_t) \left( \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) \right).
\end{align}

The Hessian $\nabla_{\theta}^2 \mathcal{L}^{\beta}(\theta_t) \in \mathbb{R}^{p\times p}$ of the regularized loss is given by
\begin{equation}
    \nabla_{\theta}^2 \mathcal{L}^{\beta}(\theta_t)
    = g(\theta_t)^{\top} \nabla_{\theta}^2f(\theta_t) + J(\theta)^{\top}J(\theta_t) + \beta I_p.
\end{equation}
Here, $\nabla_{\theta}^2f(\theta) \in \mathbb{R}^{N\times p\times p}$, and $g(\theta_t)^{\top} \nabla_{\theta}^2f(\theta_t) = \sum_{i=1}^N g(\mathbf{x}_i,\theta_t)\nabla_{\theta}^2f(\mathbf{x}_i,\theta_t)$. 
By using the triangle inequality and Cauchy-Schwarz,
\begin{align}
    \lVert g(\theta_t)^{\top} \nabla_{\theta}^2 f(\theta)  \rVert_2 
    \le \sum_{i=1}^N |g(\mathbf{x}_i, \theta_t)| \lVert \nabla_{\theta}^2f(\mathbf{x}_i,\theta_t) \rVert_2
    \le \lVert g(\theta_t) \rVert_2 \sqrt{\sum_{i=1}^N \lVert \nabla_{\theta}^2f(\mathbf{x}_i,\theta_t) \rVert_2^2}.
\end{align}
Using Lemma \ref{lemma: g(theta0) bounded}, we have $\lVert g(\theta_t) \rVert_2 \le \lVert g(\theta_0) \rVert_2 \le R_0$. Further, by Lemma \ref{lemma: Jacobian Lipschitz}, we have $\lVert \nabla_{\theta}^2f(\mathbf{x}_i,\theta_t) \rVert_2 \le \frac{1}{\sqrt{n}}K'$.
Thus (with $K=\sqrt{N}K'$),
\begin{equation}
    \lVert g(\theta_t)^{\top} \nabla_{\theta}^2 f(\theta)  \rVert_2 \le \frac{1}{\sqrt{n}} K R_0.
\end{equation}
As $g(\theta_t)^{\top} \nabla_{\theta}^2 f(\theta)$ is symmetric, it follows for it's minimum eigenvalue, that
\begin{equation}
    \lambda_{\min}\left( g(\theta_t)^{\top} \nabla_{\theta}^2 f(\theta) \right)
    \ge - \lVert g(\theta_t)^{\top} \nabla_{\theta}^2 f(\theta)  \rVert_2 
    \ge - \frac{1}{\sqrt{n}} K R_0.
\end{equation}
Now consider $\beta > 0$. Then, we can follow that for $n$ large enough, the smallest eigenvalue of the Hessian of the regularized loss is positive:
\begin{equation}
    \lambda_{\min}\left(\nabla_{\theta} \mathcal{L}^{\beta}(\theta_t)\right)
    \ge - \frac{1}{\sqrt{n}} K R_0 + 0 + \beta
    \ge \frac{\beta}{2} =: c_{\beta}.
\end{equation}
Thus,
\begin{equation}
    \frac{d}{dt} \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) \rVert_2^2
    \le -2\eta_0 c_{\beta} \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) \rVert_2^2.
\end{equation}
In Remark \ref{remark: beta=0 proof} we will show, how to modify the proof such that this step is still valid for $\beta = 0$.

By Gronwalls inequality, it follows (for $\beta \ge 0$), that
\begin{equation}
    \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) \rVert_2^2
    \le e^{-2\eta_0c_{\beta}t} \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_0) \rVert_2^2.
\end{equation}
Thus,
\begin{equation}
    \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) \rVert_2
    \le e^{-\eta_0c_{\beta}t} \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_0) \rVert_2
    \le e^{-\eta_0c_{\beta}t} \lVert J(\theta_0)^{\top}g(\theta_0) \rVert_2
    \le e^{-\eta_0c_{\beta}t} \lVert J(\theta_0) \rVert_2 \lVert g(\theta_0) \rVert_2
    \le KR_0 e^{-\eta_0c_{\beta}t}.
\end{equation}
Hence, for the distance of parameters from initialization
\begin{equation}
    \lVert \theta_t - \theta_0 \rVert_2
    = \norm{ \int_0^t \frac{d\theta_u}{du} du }_2
    \le \int_0^t \norm{ \frac{d\theta_u}{du} }_2 du
    \le \eta_0 K R_0 \int_0^t e^{-\eta_0 c_{\beta}u}du
    = \frac{KR_0}{c_{\beta}} (1- e^{-\eta_0 c_{\beta}t}).
\end{equation}
Thus, for $t \le t_1$, $\lVert \theta_t - \theta_0 \rVert_2 < C$. By continuity, $t_1 = \infty$.
Using local Lipschitzness, we can further bound the distance of the Jacobian at any $\lVert x \rVert_2 \le 1$
\begin{equation}
    \lVert J(x,\theta_t) - J(x, \theta_0) \rVert_2
    \le \frac{1}{\sqrt{n}}K' \lVert \theta_t - \theta_0 \rVert_2
    \le \frac{1}{\sqrt{n}}K'C.
\end{equation}
This finishes the proof of Theorem \ref{theorem: Exponential decay gradient, parameters stay close}.

\end{proof}
\begin{remark}
\label{remark: beta=0 proof}
For $\beta = 0$, the Hessian is 
\begin{equation}
    \nabla_{\theta}^2 \mathcal{L}^{0}(\theta_t) 
    = g(\theta_t)^{\top} \nabla_{\theta}^2 f(\theta_t) + J(\theta_t)^{\top}J(\theta_t).
\end{equation}
For $\beta>0$, we just used that $J(\theta_t)^{\top}J(\theta_t) $ is positive semi-definite, as $\beta I$ dominates the negative eigenvalues of the first term of the Hessian. For $\beta = 0$, this is not enough. $J(\theta_t)^{\top}J(\theta_t) \in \mathbb{R}^{p\times p}$ shouldn't be confused with the NTK $J(\theta_t)J(\theta_t)^{\top} \in \mathbb{R}^{N\times N}$. However, they share the have the same nonzero eigenvalues. For $p>N$ (which is the case for $n$ large enough), $J(\theta_t)^{\top}J(\theta_t)$ will additionally have the eigenvalue $0$ with multiplicity of at least $p-N$. Thus, we can't naively lower bound the minimum eigenvalue of the Hessian with the minimum eigenvalue of $J(\theta_t)^{\top}J(\theta_t)$. 

Luckily, $\nabla_{\theta} \mathcal{L}^0(\theta_t) = J(\theta_t)^{\top}g(\theta_t)$ is in the row-span of $J(\theta_t)$. This is orthogonal to the nullspace of $J(\theta_t)$, i.e. the eigenspace corresponding to the eigenvalue $0$ of $J(\theta_t)$. Thus, $\nabla_{\theta} \mathcal{L}^0(\theta_t)$ only ``uses" the positive eigenvalues of $J(\theta_t)^{\top}J(\theta_t)$. The smallest positive eigenvalue of $J(\theta_t)^{\top}J(\theta_t)$ is equal to the smallest positive eigenvalue of the empirical NTK $J(\theta_t)J(\theta_t)^{\top}$, which is lower bounded by $\frac{1}{2}\lambda_{\min}(\mathbf{\Theta})$ on the high probability event we consider.

Hence, for $n$ large enough,
\begin{align}
    \frac{d}{dt} \lVert \nabla_{\theta} \mathcal{L}^0(\theta_t) \rVert_2^2
    &= -3\eta_0 \left( \nabla_{\theta} \mathcal{L}^0(\theta_t) \right)^{\top} \nabla_{\theta}^2 \mathcal{L}^0(\theta_t) \left( \nabla_{\theta} \mathcal{L}^0(\theta_t) \right) \\
    &\le -2\eta_0 \lambda_{\min}\left( \nabla_{\theta} \mathcal{L}^0(\theta_t) \right) \lVert \nabla_{\theta} \mathcal{L}^0(\theta_t) \rVert_2^2 - \eta_0 \frac{1}{2}\lambda_{\min}(\mathbf{\Theta})\lVert \nabla_{\theta} \mathcal{L}^0(\theta_t) \rVert_2^2 \\
    &\le -2\eta_0 \left( -\frac{\log n}{\sqrt{n}}KR_0 + \frac{1}{2}\lambda_{\min}(\mathbf{\Theta}) \right)\lVert \nabla_{\theta} \mathcal{L}^0(\theta_t) \rVert_2^2 \\
    &\le -2\eta_0 \frac{1}{3}\lambda_{\min}(\mathbf{\Theta}) \lVert \nabla_{\theta} \mathcal{L}^0(\theta_t) \rVert_2^2.
\end{align}
Defining $c_0 := \frac{1}{3}\lambda_{\min}(\mathbf{\Theta})$ makes it possible to continue with the proof above for $\beta\ge 0$. This is an alternative proof to \citet{lee2019wide}. It shows that in the unregularized case, it is important that the gradient flow lies in the row-space of the Jacobian.

\end{remark}
\subsection{Closeness to the Linearized Network along the Regularized Gradient Flow}
\label{appendix: Proof regularized gradient flow part 2 (closeness linearized)}
The goal of this section is to prove that the neural network along the regularized gradient flow stays close to the linearized network along the linear regularized gradient flow. Let us restate the following Theorem.
\gftheoremparttwo*
\begin{proof}
The proof of \citet{lee2019wide} used that training error converges to $0$. Thus, we have to use a different approach, which also provides a more straightforward and intuitive proof for $\beta=0$. Remember that
\begin{equation}
    f^{\lin}(x,\theta) = f(x,\theta_0) + J(x,\theta_0)(\theta-\theta_0), \text{ and } 
    \frac{d\theta^{\text{lin}}}{dt} = -\eta_0 \left(J(\theta_0)^{\top} g^{\text{lin}}(\theta^{\text{lin}}_t) + \beta (\theta^{\text{lin}}_t - \theta_0)\right).
\end{equation}
To prove the second part of the Theorem, we will use
\begin{equation}
\label{eq: triangle f flin proof}
    \lVert f(x,\theta_t) - f^{\lin}(x,\theta^{\lin}_t)  \rVert_2
    \le \lVert f(x,\theta_t) - f^{\lin}(x,\theta_t) \rVert_2 + \lVert f^{\lin}(x,\theta_t) - f^{\lin}(x,\theta^{\lin}_t)\rVert_2.
\end{equation}
We will start by bounding the first term. Next, we will bound $\lVert \theta_t - \theta^{\lin}_t \rVert_2$, and use this to bound the second term.

\textbf{First step:} To bound $\lVert f(x,\theta_t) - f^{\lin}(x,\theta_t) \rVert_2$, we compute
\begin{align}
    \norm{ \frac{d}{dt} \left(f(x,\theta_t) - f^{\lin}(x,\theta_t) \right) }_2
    &= \norm{\left(J(x,\theta_t) - J(x,\theta_0) \right)\frac{d\theta_t}{dt}}_2 \\
    &\le \lVert J(x,\theta_t) - J(x,\theta_0) \rVert_2 \norm{\frac{d\theta_t}{dt}}_2 \\
    &\le \frac{1}{\sqrt{n}} K' C \eta_0 KR_0 e^{-\eta_0 c_{\beta}t},
\end{align}
where we used Theorem \ref{theorem: Exponential decay gradient, parameters stay close} in the last step.
Now, we can bound
\begin{equation}
    \lVert f(x,\theta_t) - f^{\lin}(x,\theta_t) \rVert_2
    \le \frac{1}{\sqrt{n}}K'C\eta_0KR_0 \int_0^t e^{-\eta_0 c_{\beta} u} du
    \le \frac{1}{\sqrt{n}}K'C \frac{KR_0}{c_{\beta}} 
    = \frac{1}{\sqrt{n}} K' C^2.
\end{equation}
In particular, for the difference at the training points, $\lVert f(\theta_t) - f^{\lin}(\theta_t) \rVert_2 \le \frac{1}{\sqrt{n}} K C^2$.

\textbf{Second step:} Now, we will bound the difference between $\theta_t - \theta^{\lin}_t$. We can write
\begin{align}
    \frac{d\theta_t}{dt}
    &= -\eta_0 \left( J(\theta_t)^{\top}g(\theta_t) + \beta(\theta_t-\theta_0) \right) \\
    &= -\eta_0 \left( \left( J(\theta_t) - J(\theta_0) \right)^{\top}g(\theta_t) + J(\theta_0)^{\top} \left( g(\theta_t) - g^{\lin}(\theta_t) \right) + J(\theta_0)^{\top}g^{\lin}(\theta_t) + \beta(\theta_t-\theta_0) \right) \\
    &= -\eta_0 \Delta_t - \eta_0 \left( J(\theta_0)^{\top}g^{\lin}(\theta_t) + \beta(\theta_t-\theta_0) \right),
\end{align}
where we define $\Delta_t := \left( J(\theta_t) - J(\theta_0) \right)^{\top}g(\theta_t) + J(\theta_0)^{\top} \left( g(\theta_t) - g^{\lin}(\theta_t) \right)$. We will now bound $\lVert \Delta_t \rVert_2$. For the first term, using Theorem \ref{theorem: Exponential decay gradient, parameters stay close}:
\begin{equation}
    \lVert \left( J(\theta_t) - J(\theta_0) \right)^{\top}g(\theta_t) \rVert_2
    \le \lVert J(\theta_t) -J(\theta_0)\rVert_2 \lVert g(\theta_t) \rVert_2
    \le \frac{1}{\sqrt{n}}KCR_0.
\end{equation}
For the second term, using Theorem \ref{theorem: Exponential decay gradient, parameters stay close} and the bound we derived in the first step, 
\begin{equation}
    \lVert J(\theta_0)^{\top}\left( g(\theta_t) - g^{\lin}(\theta_t) \right) \rVert_2
    = \lVert J(\theta_0)^{\top}\left( f(\theta_t) - f^{\lin}(\theta_t) \right) \rVert_2
    \le \lVert J(\theta_0)\rVert_2 \lVert f(\theta_t) - f^{\lin}(\theta_t) \rVert_2
    \le \frac{1}{\sqrt{n}}K^2C^2.
\end{equation}
Thus, defining $K^{\Delta} := KCR_0 + K^2C^2$, we can bound $\lVert \Delta_t \rVert_2 \le \frac{1}{\sqrt{n}} K^{\Delta}$. Now, we can compute
\begin{align}
    \frac{d}{dt} (\theta_t - \theta^{\lin}_t)
    &= -\eta_0 \Delta_t - \eta_0 \left(J(\theta_0)^{\top} g^{\lin}(\theta_t) + \beta_N(\theta_t - \theta_0) \right) + \eta_0 \left(J(\theta_0)^{\top} g^{\lin}(\theta^{\lin}_t) + \beta_N(\theta^{\lin}_t - \theta_0) \right) \\
    &= -\eta_0\Delta_t - \eta_0 \left(J(\theta_0)^{\top} \left(g^{\lin}(\theta_t) - g^{\lin}(\theta^{\lin}_t)\right) + \beta(\theta_t - \theta^{\lin}_t) \right) \\
    &= -\eta_0\Delta_t - \eta_0 \left(J(\theta_0)^{\top} J(\theta_0)(\theta_t - \theta^{\lin}_t) + \beta(\theta_t - \theta^{\lin}_t) \right) \\
    &= -\eta_0\Delta_t - \eta_0 \left(J(\theta_0)J(\theta_0)^{\top} + \beta I \right)(\theta_t - \theta^{\lin}_t).
\end{align}
By treating this as an inhomogeneous linear ODE in $\theta_t - \theta^{\lin}_t$, we get
\begin{equation}
    \theta_t - \theta^{\lin}_t = \int_0^t e^{-\eta_0 \left(J(\theta_0)J(\theta_0)^{\top} + \beta I \right) (t-u)} (-\eta_0 \Delta_u) du.
\end{equation}
Hence (using $\lVert e^{-A} \rVert_2 \le e^{-\lambda_{\min}(A)}$),
\begin{align}
    \lVert \theta_t - \theta^{\lin}_t \rVert_2
    &\le \int_0^t \lVert e^{-\eta_0 \left(J(\theta_0)J(\theta_0)^{\top} + \beta I \right) (t-u)} \rVert_2 \eta_0 \lVert \Delta_u \rVert_2 du \\
    &\le \int_0^{t} e^{-\eta_0 (c_0 + \beta)(t-u)} \eta_0 \frac{1}{\sqrt{n}} K^{\Delta} du \\
    &\le \frac{1}{\sqrt{n}} \frac{K^{\Delta}}{c_0 + \beta}.
\end{align}
Thus, $\sup_t \lVert \theta_t - \theta^{\lin}_t \rVert_2 \le \frac{K^{\Delta}}{c_0 + \beta} \frac{1}{\sqrt{n}}$.

\textbf{Third step:} Using the bound on $\lVert \theta_t - \theta^{\lin}_t \rVert_2$, we can easily bound $\lVert f^{\lin}(x,\theta_t) - f^{\lin}(x,\theta^{\lin}_t)\rVert_2$:
\begin{equation}
    \lVert f^{\lin}(x,\theta_t) - f^{\lin}(x,\theta^{\lin}_t)\rVert_2
    = \lVert J(x,\theta_0) (\theta_t - \theta^{\lin}_t) \rVert_2
    \le \lVert J(x,\theta_0) \rVert_2 \lVert \theta_t - \theta^{\lin}_t \rVert_2
    \le K' \frac{K^{\Delta}}{c_0 + \beta} \frac{1}{\sqrt{n}}.
\end{equation}
By using Equation \ref{eq: triangle f flin proof}, we can now finish the proof:
\begin{equation}
    \lVert f(x,\theta_t) - f^{\lin}(x,\theta^{\lin}_t) \rVert_2
    \le \left(K'C^2 + K' \frac{K^{\Delta}}{c_0 + \beta} \right) \frac{1}{\sqrt{n}}.
\end{equation}



\end{proof}

\section{Proof for regularized gradient descent}
\label{appendix: gradient descent}
\subsection{Geometric Decay of the regularized gradient and closeness of parameters to their initial value}
\begin{theorem}
\label{theorem: gradient descent geometric decay}
Let $\beta \ge 0$. Let $\delta_0 > 0$ arbitrarily small. There are $K',K,R_0, c_{\beta}, \eta_{\max} > 0$, such that for $n$ large enough, the following holds with probability of at least $1-\delta_0$ over random initialization, when applying regularized gradient descent with learning rate $\eta = \eta_0 \le \eta_{\max}$:
\begin{equation}
    \lVert \theta_{t+1} - \theta_{t}\rVert_2  
    = \eta_0 \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t}) \rVert_2 
    \le \eta_0KR_0 \left( 1 - \eta_0c_{\beta} \right)^{t},
\end{equation}
\begin{equation}
    \lVert \theta_t - \theta_0 \rVert_2 
    % \le \eta_0KR_0\sum_{u=1}^t \left( 1 - \frac{1}{2}\eta_0\beta \right)^{u-1}
    % \le \frac{KR_0}{c_{\beta}}\left( 1 - \eta_0c_{\beta} \right)^{t}
    <  \frac{KR_0}{c_{\beta}}
    =: C,
\end{equation}
\begin{equation}
    \forall \lVert x \rVert_2 \le 1: \lVert J(x,\theta_t) - J(x,\theta_0) \rVert_2 \le \frac{1}{\sqrt{n}} K' C,
\end{equation}
\begin{equation}
    \lVert J(\theta_t) - J(\theta_0) \rVert_2 \le \frac{1}{\sqrt{n}} K C.
\end{equation}
\end{theorem}
\begin{proof}
We consider the same high probability event as in the proof for the regularized gradient flow. 
Define $c_{\beta} := \frac{1}{2}\beta$ for $\beta > 0$, and $c_{\beta} := \frac{1}{3}\lambda_{\min}(\mathbf{\Theta})$ for $\beta=0$. Let $C:=\frac{KR_0}{c_{\beta}}$.

We will prove the first two inequalities by induction. For $t=0$:
\begin{equation}
    \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) \rVert_2 
    = \lVert J(\theta_0)^{\top}g(\theta_0) \rVert_2 
    \le KR_0.
\end{equation}
Now, assume it holds true for $s\le t$.
We want to bound $\lVert \theta_{t+1} - \theta_t \rVert_2 = \eta_0 \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t)\rVert_2$.
Remember that $\nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) = J(\theta_t)^{\top}g(\theta_t) + \beta (\theta_t - \theta_0)$.
We write
\begin{align}
    \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_t) \rVert_2 ^2
    = &\lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t-1}) + \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t}) - \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t-1}) \rVert_2^2 \\
    =& \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t-1}) \rVert_2^2 \\
    &+ 2 \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1})^{\top} \left( \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t}) - \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t-1}) \right) \label{eq: gd second term} \\
    &+ \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t}) - \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t-1}) \rVert_2^2. \label{eq: gd third term}
\end{align}
In the following, we will look at how to bound the second and the third term. We have:
\begin{align}
    \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t}) - \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t-1}) 
    &= \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t-1} - \eta_0 \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1})) - \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t-1}) \\
    &= -\int_0^{\eta_0}  \left(\nabla_{\theta}^2 \mathcal{L}^{\beta}\left( \theta_{t-1} - u\nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1}) \right) \right) \cdot \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1}) du.
\end{align}
As in the proof for the gradient flow, the following part only holds for $\beta > 0$.
Note, that for any $u\in [0,\eta_0]$, $\theta_{t-1} - u\nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t-1}) \in B(\theta_0, C)$, as we know by induction that $\theta_{t-1},\theta_t \in B(\theta_0,C)$. Thus, similar to the proof for the gradient flow, for $n$ large enough, $\forall u \in [0,\eta_0]$:
\begin{equation}
    \lambda_{\min}\left( \nabla_{\theta}^2\mathcal{L}^{\beta}\left( \theta_{t-1} - u\nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t-1})\right) \right) \ge \frac{3}{2}c_{\beta}.
\end{equation}
Note that we are using $\frac{3}{2}c_{\beta} = \frac{3}{4}\beta$, which is slightly higher than $c_{\beta}$ which we used in the gradient flow case, to arrive at the equivalent result in the end.
For the second term (\ref{eq: gd second term}) we get,
\begin{align}
    2 \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1})^{\top} \left( \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t}) - \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t-1}) \right) 
    &= - 2\int_0^{\eta_0} \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1}) \left(\nabla_{\theta}^2 \mathcal{L}^{\beta}\left( \theta_{t-1} - u\nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1}) \right) \right) \cdot \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1})du \\
    &\le -2\eta_0 \frac{3}{2}c_{\beta} \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t-1}) \rVert_2^2.
\end{align}
Further, we have for any $\theta \in B(\theta_0, C)$:
\begin{align}
    \lVert \nabla_{\theta}^2 \mathcal{L}^{\beta}\left( \theta\right) \rVert_2
    &\le \lVert g(\theta)^{\top} \nabla_{\theta}^2 f(\theta)\rVert_2 + \lVert J(\theta)^{\top}J(\theta) \rVert_2 + \beta I_p \\
    &\le \frac{1}{\sqrt{n}} KR_0 + \lambda_{\max}\left(J(\theta)^{\top}J(\theta)\right) + \beta \\
    &\le \frac{1}{\sqrt{n}} KR_0 + 2\lambda_{\max}(\mathbf{\Theta}) + \beta \\
    &\le 2(\lambda_{\max}(\mathbf{\Theta}) + \beta),
\end{align}
for $n$ large enough.
Using this with $\theta = \theta_{t-1} - u\nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t-1})$, we get for the third term (\ref{eq: gd third term}),
\begin{align}
    \lVert \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t}) - \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t-1}) \rVert_2^2
    &= \norm{\int_0^{\eta_0}  \left(\nabla_{\theta}^2 \mathcal{L}^{\beta}\left( \theta_{t-1} - u\nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1}) \right) \right) \cdot \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1}) du}_2^2 \\
    &\le \left( \int_0^{\eta_0} \lVert \nabla_{\theta}^2 \mathcal{L}^{\beta}\left( \theta_{t-1} - u\nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1}) \right) \rVert_2 \lVert \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1}) \rVert_2 du \right)^2 \\
    &\le \eta_0^2 \left(2(\lambda_{\max}(\mathbf{\Theta}) + \beta) \right)^2 \lVert \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1}) \rVert_2^2 \\
    &\le \eta_0 c_{\beta} \lVert \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1}) \rVert_2^2.
\end{align}
In the last inequality, we chose the learning rate $\eta_0 \le \frac{c_{\beta}}{4(\lambda_{\max}(\mathbf{\Theta}) + \beta)^2}$ small enough. Similarly to the gradient flow case, one can derive such bounds for $\beta=0$.
Summing up the three terms,
\begin{equation}
    \lVert \nabla_{\theta}\mathcal{L}^{\beta}(\theta_t) \rVert_2^2 
    \le (1 - 2\eta_0\frac{3}{2}c_{\beta} + \eta_0c_{\beta})\lVert \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1}) \rVert_2^2
    = (1-2\eta_0 c_{\beta})\lVert \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1}) \rVert_2^2.
\end{equation}
Thus, by Bernoulli's inequality, and the induction hypothesis,
\begin{equation}
    \lVert \nabla_{\theta}\mathcal{L}^{\beta}(\theta_t) \rVert_2
    \le \sqrt{1-2\eta_0c_{\beta}} \lVert \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1}) \rVert_2
    \le (1-\eta_0c_{\beta}) \lVert \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t-1}) \rVert_2
    \le KR_0(1-\eta_0c_{\beta})^t.
\end{equation}
Hence, $\lVert \theta_{t+1} - \theta_t \rVert_2 = \eta_0 \lVert \nabla_{\theta}\mathcal{L}^{\beta}(\theta_t) \rVert_2 \le \eta_0 KR_0(1-\eta_0c_{\beta})^t $. From this, we can follow that
\begin{equation}
    \lVert \theta_{t+1} - \theta_0 \rVert_2
    \le \sum_{u=0}^t \lVert \theta_{u+1} - \theta_u \rVert_2
    \le \eta_0 K R_0 \sum_{u=0}^t (1-\eta_0 c_{\beta})^u
    = \eta_0 K R_0 \frac{1- (1-\eta_0 c_{\beta})^{t+1}}{\eta_0 c_{\beta}}< \frac{KR_0}{c_{\beta}} = C.
\end{equation}
This proves the first two inequalities. The rest follows directly from the local Lipschitzness of the Jacobian, like in the proof for the gradient flow.
\end{proof}
\subsection{Closeness to the linearized network along the regularized gradient descent}
The following Theorem reads the same as in the gradient flow case, and the proof is very similar, which is why we only provide the main idea.
\begin{theorem}
Let $\beta \ge 0$. Let $\delta_0 > 0$ be arbitrarily small. Then, there are $C_1, C_2>0$, such that for $n$ large enough, with probability of at least $1-\delta_0$ over random initialization,
\begin{equation}
    \sup_{t\ge 0} \lVert \theta_t^{\lin} - \theta_t \rVert_2 \le C_1 \frac{1}{\sqrt{n}}, \quad
    \forall \lVert x\Vert_2 \le 1: \sup_{t\ge 0} \lVert f^{\lin}(x,\theta^{\lin}_t) - f(x,\theta_t) \rVert_2 \le C_2 \frac{1}{\sqrt{n}}.
\end{equation}
\end{theorem}
\begin{proof}[Proof Sketch]
Remember that
\begin{equation}
    f^{\lin}(x,\theta) = f(x,\theta_0) + J(x,\theta_0)(\theta - \theta_0), \text{ and } \theta^{\lin}_{t+1} = \theta^{\lin}_{t} - \eta_0 \left( J(\theta_0)^{\top}g^{\lin}(\theta^{\lin}_t) + \beta (\theta^{\lin}_t - \theta_0)\right).
\end{equation}
The structure of the proof is the same as for the gradient flow.
We will only show how to bound the term $\lVert f(x,\theta_t) - f^{\lin}(x,\theta_t) \rVert_2$. The bounds for the other terms can be done similarly. In particular, we will show by induction that
\begin{equation}
    \lVert f(x,\theta_t) - f^{\lin}(x,\theta_t) \rVert_2
    \le \eta_0 \frac{1}{\sqrt{n}} K'C KR_0\sum_{u=0}^{t-1}\left( 1-\eta_0c_{\beta} \right)^{u}.
\end{equation}
For $t=0$, this is true. Now, assume this holds for $s\le t$, then
\begin{equation}
    \lVert f(x,\theta_{t+1}) - f^{\lin}(x,\theta_{t+1}) \rVert_2
    = \lVert f(x,\theta_{t}) - f^{\lin}(x,\theta_{t}) \rVert_2 
    + \lVert f(x,\theta_{t+1}) - f(x,\theta_{t}) - \left(f^{\lin}(x,\theta_{t+1})  - f^{\lin}(x,\theta_{t}) \right)\rVert_2.
\end{equation}
By the chain rule and the fundamental theorem of calculus, we can write
\begin{align}
    & \lVert f(x,\theta_{t+1}) - f(x,\theta_{t}) - \left(f^{\lin}(x,\theta_{t+1})  - f^{\lin}(x,\theta_{t}) \right)\rVert_2 \\
    =& \norm{\int_0^{\eta_0} J\left(x,\theta_{t} - u \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t})\right) \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t}) du - \int_0^{\eta_0} J(x,\theta_0) \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t})du}_2 \\
    \le & \int_0^{\eta_0} \lVert J\left(x,\theta_{t} - u \nabla_{\theta}\mathcal{L}^{\beta}(\theta_{t})\right) - J(x,\theta_0) \rVert_2 \lVert  \nabla_{\theta} \mathcal{L}^{\beta}(\theta_{t}) \rVert_2 du \\
    \le & \eta_0 \frac{1}{\sqrt{n}}K'C KR_0\left( 1-\eta_0c_{\beta} \right)^{t}.
\end{align}
In the last step we used Theorem \ref{theorem: gradient descent geometric decay}.
Thus,
\begin{equation}
     \lVert f(x,\theta_{t+1}) - f^{\lin}(x,\theta_{t+1}) \rVert_2 \le \eta_0 \frac{1}{\sqrt{n}} K'C KR_0\left(\sum_{u=0}^{t-1}\left( 1-\eta_0c_{\beta} \right)^{u} + \left( 1-\eta_0c_{\beta} \right)^{t}\right).
\end{equation}
This finishes the induction proof. We can now further follow by using the geometric series that
\begin{align}
    \lVert f(x,\theta_t) - f^{\lin}(x,\theta_t) \rVert_2
    &\le \eta_0 \frac{1}{\sqrt{n}} K'C KR_0\sum_{u=0}^{t-1}\left( 1-\eta_0c_{\beta} \right)^{u} \\
    &< \eta_0 \frac{1}{\sqrt{n}} K'C KR_0\frac{1}{\eta_0 c_{\beta}} \\
    &= \frac{1}{\sqrt{n}} K' C^2.
\end{align}
For the other inequalities one can proceed in the same manner, using the fundamental theorem of calculus and the geometric series.
\end{proof}
\section{Shifting the network at initialization}
\label{appendix: Shifting the network at initialization}
Here, we prove that shifting the network at initialization makes it possible to include any prior mean, and compute the posterior mean with a single training run.
\shiftpredictions*
\begin{proof}
The Jacobian of the shifted network is equal to the Jacobian of the original network:
\begin{equation}
    J_{\tilde{f}}(x,\theta) = J_f(x,\theta).
\end{equation}
Define the shifted labels $\tilde{\mathbf{y}} := \mathbf{y} + f(\mathbf{x},\theta_0) - m(\mathbf{x})$. Then, $\tilde{f}(\mathbf{x}, \theta) - \mathbf{y} = f(\mathbf{x}, \theta) - \tilde{\mathbf{y}}$.
Thus, training the network $\tilde{f}$ with regularized gradient flow/descent is equivalent to training $f$ using the shifted labels $\tilde{\mathbf{y}}$, in the sense that the parameter update rule is the same. The latter leads to parameters $\theta_{\infty}$, for which (in the infinite-width limit)
\begin{align}
    f(\mathbf{x}',\theta_{\infty}) 
    &= f(\mathbf{x}', \theta_0) + \mathbf{\Theta}_{\mathbf{x',x}} \left( \mathbf{\Theta}_{\mathbf{x,x}} + \beta I \right)^{-1} (\tilde{\mathbf{y}} - f(\mathbf{x},\theta_0)).
\end{align}
By adding $-f(\mathbf{x},\theta_0) + m(\mathbf{x})$ to both sides of the equation, and using $\tilde{\mathbf{y}} - f(\mathbf{x},\theta_0) = \mathbf{y} - m(\mathbf{x})$, we get
\begin{align}
    \tilde{f}(\mathbf{x}', \theta_{\infty}) 
    = m(\mathbf{x}') + \mathbf{\Theta}_{\mathbf{x',x}} \left( \mathbf{\Theta}_{\mathbf{x,x}} + \beta I \right)^{-1} (\mathbf{y} - m(\mathbf{x})).
\end{align}
\end{proof}

\section{The Output of the Linearized Network is Gaussian over Random Initializations}
\label{appendix: gaussian-distribution-convergence}
\begin{corollary}[Convergence under Regularized Gradient Flow/Descent]\label{corollary} Under regularized gradient flow/descent training, the output of a wide neural network converges in distribution to a Gaussian over random initialization as the width $n \to \infty$. Specifically, for test inputs $\mathbf{x'}$ and $t \to \infty$, the mean and covariance of the output distribution at convergence are
\begin{align}
    \boldsymbol{\mu}(\mathbf{x'}) 
    =& \mathbf{\Theta}_{\mathbf{x', x}} \left(\mathbf{\Theta}_{\mathbf{x, x}} + \beta I\right)^{-1} \mathbf{y}, \\
    \boldsymbol{\Sigma}(\mathbf{x'}) 
    =& \mathbf{K}_{\mathbf{x', x'}} 
    + \mathbf{\Theta}_{\mathbf{x', x}} \left(\mathbf{\Theta}_{\mathbf{x, x}} + \beta I\right)^{-1} \mathbf{K}_{\mathbf{x, x}} \left(\mathbf{\Theta}_{\mathbf{x, x}} + \beta I\right)^{-1} \mathbf{\Theta}_{\mathbf{x, x'}} \\
    &- \mathbf{\Theta}_{\mathbf{x', x}} \left(\mathbf{\Theta}_{\mathbf{x, x}} + \beta I\right)^{-1} \mathbf{K}_{\mathbf{x, x'}}
    - \mathbf{K_{x',x}} \left(\mathbf{\Theta}_{\mathbf{x, x}} + \beta I\right)^{-1} \mathbf{\Theta}_{\mathbf{x, x'}}.
\end{align}
Note that the resulting covariance combines contributions from the NTK and NNGP kernels and therefore does not directly correspond to the posterior covariance of any GP.
\end{corollary}
\begin{proof}
As we showed, for large enough layer width,
\begin{equation}
    f(\mathbf{x}', \theta_{\infty}) = f(\mathbf{x}', \theta_0) + \mathbf{\Theta}_{\mathbf{x',x}}\left( \mathbf{\Theta}_{\mathbf{x,x}} + \beta I \right)^{-1} (\mathbf{y} - f(\mathbf{x}, \theta_0)).
\end{equation}
$f(\mathbf{x}', \theta_0)$ and $f(\mathbf{x}, \theta_0)$ jointly converge to a Gaussian with mean zero and covariance matrix given through the NNGP-kernel $\mathbf{K}$.
From this, it directly follows that $f(\mathbf{x'},\theta_{\infty})$ converges to a Gaussian with the given mean and covariance matrices.\footnote{The covariance matrix of $X + AY$, where $X$ and $Y$ are jointly Gaussian, is given by $\Sigma_X + A\Sigma_Y A^{\top} + A\Sigma_{X,Y} + \Sigma_{Y,X}A^{\top}$.}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
