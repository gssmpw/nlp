@inproceedings{allenzhu2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International conference on machine learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@inproceedings{chen2022neural,
  title={A neural tangent kernel perspective on function-space regularization in neural networks},
  author={Chen, Zonghao and Shi, Xupeng and Rudner, Tim GJ and Feng, Qixuan and Zhang, Weizhong and Zhang, Tong},
  booktitle={OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)},
  year={2022}
}

@article{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International conference on machine learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@article{he2020bayesian,
  title={Bayesian deep ensembles via the neural tangent kernel},
  author={He, Bobby and Lakshminarayanan, Balaji and Teh, Yee Whye},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1010--1022},
  year={2020}
}

@misc{hu2020simpleeffectiveregularizationmethods,
      title={Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee}, 
      author={Wei Hu and Zhiyuan Li and Dingli Yu},
      year={2020},
      eprint={1905.11368},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.11368}, 
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{li2020gradient,
  title={Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks},
  author={Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
  booktitle={International conference on artificial intelligence and statistics},
  pages={4313--4324},
  year={2020},
  organization={PMLR}
}

@article{nitanda2020optimal,
  title={Optimal rates for averaged stochastic gradient descent under neural tangent kernel regime},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2006.12297},
  year={2020}
}

@misc{rudner2023functionspaceregularizationneuralnetworks,
      title={Function-Space Regularization in Neural Networks: A Probabilistic Perspective}, 
      author={Tim G. J. Rudner and Sanyam Kapoor and Shikai Qiu and Andrew Gordon Wilson},
      year={2023},
      eprint={2312.17162},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2312.17162}, 
}

@inproceedings{suh2021non,
  title={A non-parametric regression viewpoint: Generalization of overparametrized deep ReLU network under noisy observations},
  author={Suh, Namjoon and Ko, Hyunouk and Huo, Xiaoming},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{wei2017early,
  title={Early stopping for kernel boosting algorithms: A general analysis with localized complexities},
  author={Wei, Yuting and Yang, Fanny and Wainwright, Martin J},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine learning},
  volume={109},
  pages={467--492},
  year={2020},
  publisher={Springer}
}

