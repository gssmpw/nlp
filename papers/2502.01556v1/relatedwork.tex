\section{Related Work}
\paragraph{Linearization} The paper by \citet{lee2019wide} serves as the starting point for our work. It demonstrates that as network width approaches infinity, training dynamics simplify and can be approximated by a linearized model using a first-order Taylor expansion. \citet{lee2019wide} also study the links between the output of trained neural networks and GPs. Crucially, we extend this work by proving that this linearization still holds in the presence of observation noise.

%Initialization plays a crucial role in neural network training. \citet{daniely2016toward} show that random weight initialization in neural networks provides a rich function space, hence making it a strong starting point for optimization. They highlight how initialization influences learning and motivates architectural design choices. Building on the importance of optimization, \citet{du2019gradient} show that gradient descent can achieve zero training loss in polynomial time despite the non-convexity of the loss landscape, extending convergence guarantees beyond two-layer multilayer perceptions (MLPs).

\paragraph{Kernel Methods and Neural Networks} 
The seminal paper by \citet{jacot2018neural} made two contributions. First, it introduced the equivalence between kernel methods and wide neural networks, specifically for the case of kernel ridge regression. Second, it popularized the study of neural networks in function space, rather than parameter space. We leverage both of these insights: our Theorem \ref{lemma-shifted-predictions} considers the function-space view of a Gaussian Process (a kernel method).  Several later studies have explored the links between wide neural networks and GPs through the NTK, to investigate the functional behaviour of neural networks in noisy settings. For example, \citet{rudner2023functionspaceregularizationneuralnetworks} introduced function-space regularization to encode desired properties into predictions, indirectly addressing observation noise, while \citet{chen2022neural} linked NTK-based function norms to RKHS regularization, proving to be useful in low-data regimes.

\paragraph{Global Minima and Overparameterization} In the context of overparametrization, \citet{allenzhu2019convergence} prove that stochastic gradient descent (SGD) can find global minima for neural networks in polynomial time. Similarly, \citet{zou2020gradient} show that ReLU networks trained with SGD converge to global minima for a wide range of loss functions by ensuring that weight updates remain within a small perturbation around the initialization. While we do not rely on these results directly, our result is spiritually related in that we guarantee convergence to the global optimum with high probability.

%Furthermore, another closely related form of regularization is early stopping, which can be seen as a form of implicit regularization. This is explored by \citet{wei2017early}, who establish a framework linking early stopping with regularization and derive optimal stopping rules for boosting algorithms. Additionally, \citet{li2020gradient} provide theoretical guarantees that gradient descent initially fits correct labels while ignoring noisy ones, demonstrating that early stopping can be robust to label noise when training remains close to the initialization.

\paragraph{Regularization} A line of work has explored the role of regularization in wide neural networks through the lens of the NTK. \citet{hu2020simpleeffectiveregularizationmethods} introduced the regularizer penalizing deviations from initialization, providing generalization bounds in noisy settings but assuming network linearity without proof. \citet{nitanda2020optimal} and \citet{suh2021non} extended this approach to constrain network dynamics and stabilize deeper architectures. 

\paragraph{Bayesian Ensembles} \citet{he2020bayesian} described a way of training Bayesian ensembles of neural networks, allowing for inference in the NTK-GP with zero prior mean by averaging the ensembles using the law of large numbers. In cases where we are only interested in obtaining the posterior mean, our approach is more efficient since we only train one network\footnote{They additionally provide a way of estimating the posterior covariance, which is not of interest in our paper.}.

% \cite{du2019gradient} demonstrates that gradient descent achieves zero training loss in polynomial time despite the non-convexity of the optimization landscape. Compared to previous results, this work extends convergence guarantees beyond two-layer MLPs.

% Similarly, \cite{zou2020gradient} shows how ReLU networks under SGD converge to the global minima for a broad family of loss functions. This holds thanks to the sequence of weight updates remaining within a small perturbation region around the initialization. Placing more emphasis on how different training regimes impact generalization and convergence rates.

% \cite{wei2017early} establish a framework that links early stopping with implicit regularization. They derive optimal stopping rules for boosting algorithms.

% \cite{li2020gradient} provide theoretical guarantees showing that gradient descent initially fits the correct labels while ignoring noisy ones, as long as training remains close to initialization. Concluding that early stopping is provably robust to label noise.