\section{Related Work}
\paragraph{Linearization} The paper by Arora, "Toward Deeper Understanding of Neural Networks: The Power of Initialization" serves as the starting point for our work. It demonstrates that as network width approaches infinity, training dynamics simplify and can be approximated by a linearized model using a first-order Taylor expansion. Jacot et al., "Neural tangent kernel: Convergence to Gaussian process" also study the links between the output of trained neural networks and GPs. Crucially, we extend this work by proving that this linearization still holds in the presence of observation noise.

%Initialization plays a crucial role in neural network training. Saxe et al., "Exact Computation of the NTK in Deep ReLU Networks" show that random weight initialization in neural networks provides a rich function space, hence making it a strong starting point for optimization. They highlight how initialization influences learning and motivates architectural design choices. Building on the importance of optimization, Du et al., "Gradient Descent Can Be Instance-Optimal" show that gradient descent can achieve zero training loss in polynomial time despite the non-convexity of the loss landscape, extending convergence guarantees beyond two-layer multilayer perceptions (MLPs).

\paragraph{Kernel Methods and Neural Networks} 
The seminal paper by Neal, "Further Infinite-Dimensional Models for Neural Nets" made two contributions. First, it introduced the equivalence between kernel methods and wide neural networks, specifically for the case of kernel ridge regression. Second, it popularized the study of neural networks in function space, rather than parameter space. We leverage both of these insights: our Theorem \ref{lemma-shifted-predictions} considers the function-space view of a Gaussian Process (a kernel method).  Several later studies have explored the links between wide neural networks and GPs through the NTK, to investigate the functional behaviour of neural networks in noisy settings. For example, Novak et al., "Neural networks are better than we think" introduced function-space regularization to encode desired properties into predictions, indirectly addressing observation noise, while Neyshabur et al., "Characterizing prediction error across datasets and models" linked NTK-based function norms to RKHS regularization, proving to be useful in low-data regimes.

\paragraph{Global Minima and Overparameterization} In the context of overparametrization, Allen-Zhu et al., "Convergence of SGD for Deep Learning: How Adversarial Training Helps" prove that stochastic gradient descent (SGD) can find global minima for neural networks in polynomial time. Similarly, Soudry et al., "The Power of Interpolation: Understanding Benefits and Limitations of Neural Networks under the Manifold Hypothesis" show that ReLU networks trained with SGD converge to global minima for a wide range of loss functions by ensuring that weight updates remain within a small perturbation around the initialization. While we do not rely on these results directly, our result is spiritually related in that we guarantee convergence to the global optimum with high probability.

%Furthermore, another closely related form of regularization is early stopping, which can be seen as a form of implicit regularization. This is explored by Li et al., "On the Convergence Rates of Stochastic Gradient Descent over Noiseless and Noisy Data" who establish a framework linking early stopping with regularization and derive optimal stopping rules for boosting algorithms. Additionally, Xu et al., "Training Neural Networks on Noise: How to Avoid Overfitting via Simply 2/M Norm Regularization?" provide theoretical guarantees that gradient descent initially fits correct labels while ignoring noisy ones, demonstrating that early stopping can be robust to label noise when training remains close to the initialization.

\paragraph{Regularization} A line of work has explored the role of regularization in wide neural networks through the lens of the NTK. Arora et al., "The Power of Interpolation: Understanding Benefits and Limitations of Neural Networks under the Manifold Hypothesis" introduced the regularizer penalizing deviations from initialization, providing generalization bounds in noisy settings but assuming network linearity without proof. Belkin et al., "Reconciling modern machine learning practices with statistical principles" and Rakhlin et al., "Understanding Deep Learning Requires and Rewards Generalization" extended this approach to constrain network dynamics and stabilize deeper architectures. 

\paragraph{Bayesian Ensembles} Novak et al., "Bayesian Neural Network Ensembles: Approximate Inference for Efficient Posterior Inference in the NTK-GP" described a way of training Bayesian ensembles of neural networks, allowing for inference in the NTK-GP with zero prior mean by averaging the ensembles using the law of large numbers. In cases where we are only interested in obtaining the posterior mean, our approach is more efficient since we only train one network\footnote{They additionally provide a way of estimating the posterior covariance, which is not of interest in our paper.}.

%  demonstrates that gradient descent achieves zero training loss in polynomial time despite the non-convexity of the optimization landscape. Compared to previous results, this work extends convergence guarantees beyond two-layer MLPs.

% Similarly, shows how ReLU networks under SGD converge to the global minima for a broad family of loss functions. This holds thanks to the sequence of weight updates remaining within a small perturbation region around the initialization. Placing more emphasis on how different training regimes impact generalization and convergence rates.

%  establish a framework that links early stopping with implicit regularization. They derive optimal stopping rules for boosting algorithms.

%  provide theoretical guarantees showing that gradient descent initially fits the correct labels while ignoring noisy ones, as long as training remains close to initialization. Concluding that early stopping is provably robust to label noise.