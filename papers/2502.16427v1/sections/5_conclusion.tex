% !TEX root = ../main.tex

\section{Conclusion}
\label{sec:conclusion}
We have presented a novel zero-shot video captioning approach that extends the capabilities of image VLMs to the video domain through scene graph integration, eliminating the need for supervised learning on target tasks.
Our framework first generates frame-level captions using an image VLM, converts these captions into scene graphs, and then consolidates them to produce coherent video-level captions. 
This is achieved through a lightweight graph-to-text model trained solely on text corpora.
Experimental results on video captioning and video paragraph captioning show that our approach outperforms existing zero-shot baselines and achieves competitive performance compared to the methods utilizing target dataset annotations. 
These findings highlight the potential of leveraging image VLMs for video understanding without relying on extensive paired data or high inference costs, paving the way for future advancements in zero-shot video captioning.






