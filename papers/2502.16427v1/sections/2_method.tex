% !TEX root = ../main.tex

\section{Scene Graph Construction for Videos}
\label{sec:scene}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/overview.pdf}
    \vspace{-4mm}
    \caption{An overview of our zero-shot video caption generation pipeline. The pipeline consists of (a) frame-level caption generation using image VLMs, (b) textual scene graph parsing for each frame caption, (c) merging of scene graphs into a unified graph, and (d) video-level caption generation through our graph-to-text model. Our proposed framework leverages frame-level scene graphs to produce detailed and coherent video captions.
    }
    \label{fig:framework}
\end{figure*}

Our objective is to effectively extend the capabilities of image-based vision-language models (VLMs) to the video domain without relying on video-text training. 
To this end, we introduce a novel video captioning framework that combines image VLMs with scene graph structures, as shown in Figure~\ref{fig:framework}.
The proposed method consists of four key steps: 1) generating captions for each frame using an image VLM, 2) converting these captions into scene graphs, 3) consolidating the scene graphs from all frames into a unified graph, and 4) generating comprehensive descriptions from this unified graph. 
This algorithm enables the generation of coherent and detailed video captions, bridging the gap between image and video understanding.

\subsection{Generating image-level captions}
\label{sub:generating}
We obtain image-level captions from a set of sparsely sampled frames using the open-source image VLM, LLAVA-NEXT-7B~\cite{liu2024llavanext}.
This model is selected for its strong performance across multiple benchmarks.
Our approach, however, is flexible and can incorporate any image-based VLM, including proprietary, closed-source models, as long as APIs are accessible.
The model is prompted to generate sentences optimized for scene graph construction, which are subsequently parsed into scene graphs.

\subsection{Parsing captions into scene graphs}
A scene graph $G = (\mathcal{O}, \mathcal{E})$ is defined by a set of objects, $\mathcal{O} = \{o_1, o_2, \ldots \}$, and a set of edges, $\mathcal{E}$.
Each object $o_i = (c_i, \mathcal{A}_i)$ consists of an object class $c_i \in \mathcal{C}$ and a set of attributes $\mathcal{A}_i \subseteq A$, where $\mathcal{C}$ is a set of object classes and $\mathcal{A}$ is a set of all possible attributes.
A directed edge, $e_{i,j} \equiv (o_i, o_j) \in \mathcal{E}$, has a label $r \in \mathcal{R}$, specifying the relationship from one object to the other.
All the values of object classes, attributes, and relationship labels, are text strings.

We convert the generated caption from each frame into a scene graph, providing more structured understanding of individual frames. 
By expressing the visual content in each frame using a graph based on detected objects and their relationships, we can apply a graph merging technique to produce a holistic representation of the entire input video.
We parse a caption into a scene graph using a textual scene graph parser, specifically the FACTUAL-MR parser~\cite{li-etal-2023-factual} in our implementation.

\subsection{Scene graph consolidation}
\label{sub:scene}
\input{algorithm/graph_merger} 
The scene graph consolidation step combines all frame-level scene graphs into a single graph that captures the overall visual content of the video. 
We outline our graph merging procedure, followed by a subgraph extraction technique for more focused video caption generation.

\subsubsection{Video-level graph integration}

Given two scene graphs, $G^s = (\mathcal{O}^s, \mathcal{E}^s)$ and $G^t = (\mathcal{O}^t, \mathcal{E}^t)$, constructed from two different frames, we perform the Hungarian matching between their object sets, $\mathcal{O}^s$ and $\mathcal{O}^t$.
The Hungarian algorithm aims to find the maximum matching between the objects in $\mathcal{O}^s$ and $\mathcal{O}^t$, which is given by
%
\begin{equation}
	\pi^* = \underset{\pi \in \Pi}{\arg\max} \sum_{i} \frac{ \psi_i(\phi(G^s))}{\| \psi_i(\phi(G^s)) \|} \cdot \frac{\psi_i(\phi(G_\pi^t)) }{\| \psi_i(\phi(G_\pi^t)) \|},
\end{equation}
%
where $\phi(\cdot)$ denotes the graph encoder, $\psi_i(\cdot)$ is the function to extract the $i^\text{th}$ object from an embedded graph, and $\pi \in \Pi$ indicates a permutation of objects in a graph.
Note that we introduce dummy objects  to deal with different numbers of objects for matching.

After identifying a set of matching object pairs, $\mathcal{M}$, \eg, $(p, q)$, where $o_p^s \in \mathcal{O}^s$ and $o_q^t \in \mathcal{O}^t$, using their cosine similarity with a predefined threshold, $\tau$, we merge the matched objects into a new one $\hat{o} \in \hat{\mathcal{O}}$, which is given by
%
\begin{equation}
    \hat{o} = (\hat{c} , \mathcal{A}^s_p \cup \mathcal{A}^t_q) \in \hat{\mathcal{O}},
\end{equation}
%
where $\hat{c}$ represents a class of the merged objects and $\hat{\mathcal{O}}$ denotes a set of new objects from all legitimate matching pairs.

Using this, we construct a new merged scene graph, $G^m$, which replaces each pair of merged objects with a new object $\hat{o}$, as follows:
%
\begin{equation}
	G^m = (\mathcal{O}^m, \mathcal{E}^m),
\end{equation}
%
where $\mathcal{O}^{m} =\mathcal{O}^s \cup \mathcal{O}^t \cup \hat{\mathcal{O}} ~ \setminus \bigcup_{(p, q) \in \mathcal{M}} \{o^s_p, o^t_q\}$, and the edge set $\mathcal{E}^m$ is also updated to reflect the changes in the object configuration.
Formally, each matching pair $(p, q) \in \mathcal{M}$ incurs the merge of the two objects and the construction of a new object $\hat{o}$, which results in the update of the edge set as $\mathcal{E}^m \equiv \mathcal{E}^s \cup \mathcal{E}^t$, which is formally given by
%
\begin{equation}
	(o_x, o_y) \in \mathcal{E}^m \rightarrow 
	\begin{cases}
		(\hat{o}, o_y) & \text{if } o_x \in \{o_p^s, o_q^t \}, \\
		(o_x, \hat{o}) & \text{if } o_y \in \{o_p^s, o_q^t \}, \\
		(o_x, o_y) & \text{otherwise.}
	\end{cases}
\end{equation}

We perform graph merging using a priority queue, where pairs of graphs are prioritized for merging based on their embedding similarity. 
In each iteration, the two most similar graphs are dequeued, merged, and the resulting graph is enqueued back into the priority queue.
This process is repeated until only one scene graph remains.
The final scene graph provides a comprehensive representation of the video, preserving frame-level details often overlooked by standard captioning models.
Algorithm~\ref{alg:hierarchical_graph_merge} describes the detailed procedure of our graph merging strategy.
  
\subsubsection{Prioritized subgraph extraction}
To generate concise and focused video captions, we apply subgraph extraction to retain only the most contextually relevant information. 
During the graph merging process, we track each node's merge count as a measure of its significance within the consolidated graph. 
We then identify the top $k$ nodes with the highest merge counts and extract their corresponding subgraphs. 
This approach prioritizes objects that consistently appear across multiple frames, as they often represent key entities in the scene. 
By emphasizing these essential elements and filtering out less relevant details, our method constructs a compact scene graph to generate a more focused video caption.

\section{Video Captioning}
\label{sec:videocaption}
To generate video-level descriptions that accurately reflect visual content, we developed a model that takes scene graphs as input and produce natural language descriptions.
This model is designed to effectively capture key components and relationships within the scene graph in generated text.

\vspace{-2mm}
\paragraph{Architecture}
We employ a modified encoder-decoder transformer architecture.
To prepare the input sequence for the graph encoder, each node, edge, and attribute in the graph, represented as a word or phrase, is tokenized into NLP tokens. 
These tokens are mapped to their embeddings via an embedding lookup.
For nodes consisting of multiple NLP tokens, their embeddings are averaged to form a single vector representation.
Additionally, a [CLS] token is appended as a global node to prevent isolation among disconnected components and ensure coherence. 
The adjacency matrix serves as an attention mask, incorporating graph topology into the attention mechanism. 
The graph encoder's output is then used as key and value inputs for the cross-attention layers of the text decoder, which generates the final outputs.

\vspace{-2mm}
\paragraph{Dataset}
For training, we collected approximately 2.5M text corpora that cover diverse visual scene contexts from various sources, including image caption datasets such as  MS-COCO~\cite{chen2015microsoft}, Flickr30k~\cite{young2014image}, TextCaps~\cite{sidorov2020textcaps}, Visual Genome~\cite{krishna2017visual}, and Visual Genome paragraph captioning~\cite{krause2016paragraphs}.
To further enhance the dataset, we incorporated model-generated captions for Kinetics-400~\cite{kay2017kinetics} dataset, with four uniformly sampled frames per video.
Note that neither the datasets nor the image VLMs used for generating frame captions are related to the target video captioning benchmarks.


\vspace{-2mm}
\paragraph{Training}
The model is trained using a next-token prediction objective, aiming to reconstruct the source text conditioned on the scene graph:
%
\begin{equation}
\mathcal{L}(\theta) = \sum_{i=1}^{N} \log P_{\theta}(t_i \mid t_{1:i-1}, G),
\end{equation}  
%
where $t_i$ represents the $i^\text{th}$ token in the source text, and $N$ denotes the total number of tokens.


\vspace{-2mm}
\paragraph{Video caption generation}
After constructing the video-level scene graph as described in Section~\ref{sec:scene}, we generate a video caption using the trained graph-to-text decoder, which conveys the overall narrative of the video.
