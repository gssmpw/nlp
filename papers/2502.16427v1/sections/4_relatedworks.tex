% !TEX root = ../main.tex

\section{Related Works}
\label{sec:related}

This section reviews existing approaches to video captioning, including both standard supervised learning methods and zero-shot methods. 
We also discuss video paragraph captioning, a task focused on detailed descriptions of a video using multiple sentences.

\vspace{-2mm}
\paragraph{Video captioning}
Recent supervised approaches leverage large-scale models pretrained on vision-language data and advanced architectures for improved video representation. 
For example, ClipBERT~\cite{lei2021less} and OmniVL~\cite{wang2022omnivl} incorporate multi-modal transformers to directly process video frames and generate contextual captions without extensive pre-processing.
More recent models, such as Flamingo~\cite{alayrac2022flamingo} and VideoCoCa~\cite{yan2022videococa}, perform vision-language pretraining using diverse datasets, which allows the models to generalize better across a range of video domains and tasks, including video captioning.

\vspace{-2mm}
\paragraph{Zero-shot video captioning}
Researchers have explored video captioning methods that bypass the need for paired video-text annotations during training. 
One prominent direction involves text-only training, where pretrained text decoders are used in conjunction with image-text aligned encoders such as CLIP~\cite{radford2021learning} and ImageBind~\cite{girdhar2023imagebind}. 
These methods, including DeCap~\cite{lidecap} and C$^{3}$~\cite{zhang2024connect}, align visual and textual representations within a shared embedding space to facilitate caption generation.
%
Another approach focuses on refining language model outputs at test time to better incorporate visual context.
ZeroCap~\cite{tewel2021zero} and related methods~\cite{Tewel_2023_BMVC} use CLIP-guided gradient updates to adjust language model features, while MAGIC~\cite{su2022language} employs a CLIP-based scoring mechanism to ensure semantic relevance. 
Although these methods were initially developed for image captioning tasks, they have been extended to video captioning by averaging frame-level captions into a single video-level description.
%
Recent techniques leverage the general reasoning capabilities of LLMs. 
For example, VidIL~\cite{wang2022language} uses a hierarchical framework that integrates multi-level textual representations derived from image-language models. 
By combining these representations with few-shot in-context examples, VidIL enables LLMs to perform a wide range of video-to-text tasks without extensive video-centric training. 
Similarly, Video ChatCaptioner~\cite{chen2023video} adopts an interactive framework where an LLM queries an image VLM across frames and aggregates the results to generate enriched spatiotemporal captions.
%

\vspace{-2mm}
\paragraph{Video paragraph captioning}
This task extends standard video captioning by generating coherent, multi-sentence descriptions that capture the semantics of events observed throughout an entire video. 
Unlike single-sentence captioning approaches, which typically focus on salient events, video paragraph captioning produces comprehensive and coherent captions in multiple sentences, reflecting a range of activities, background elements, and scene changes across various frames. 
To this end, MFT~\cite{xiong2018move} and PDVC~\cite{wang2021end} incorporate mechanisms for long-range temporal dependency modeling and multi-stage captioning, enabling nuanced descriptions that evolve naturally with the video. 
Vid2Seq~\cite{yang2023vid2seq} builds on this approach with a hierarchical structure that first detects key events and then generates descriptive sentences for the remaining content, maintaining a logical narrative flow. 
In contrast, Streaming GIT~\cite{zhou2024streaming} uses multi-modal pretrained transformers to produce real-time captions, facilitating seamless transitions across scenes in a continuous narrative.
