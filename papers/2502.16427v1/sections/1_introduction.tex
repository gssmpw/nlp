% !TEX root = ../main.tex

\section{Introduction}
\label{sec:intro}

Visual-language models (VLMs) have achieved remarkable progress in image understanding, which enables advanced capabilities across a wide range of applications including image captioning, visual question answering, image-grounded dialogue, image retrieval, visual entailment, and many others~\cite{alayrac2022flamingo, instructblip, gpt4v, liu2024llavanext, chen2024far}. 
However, extending these capabilities to the video domain poses significant challenges due to the scarcity of fine-grained video-text datasets compared to their image counterparts.

To bridge this gap, some researchers have explored alternative data sources to alleviate the scarcity of video-text datasets, such as Automatic Speed Recognition (ASR) transcripts from unlabelled videos~\cite{seo2022end}, narrated video datasets~\cite{huang2020multimodal, yang2023vid2seq}, or hierarchical datasets with multi-level captions~\cite{zala2023hierarchical}.
Others attempt to bypass the need for paired video-text annotations through text-only training, but na\"ive extensions of existing image captioning techniques to the video domain have shown limited success~\cite{lidecap, zhang2024connect}.
Test-time optimization methods have also been explored, which use CLIP score to refine language model outputs, but incur significant computational overhead during inference~\cite{su2022language, Tewel_2023_BMVC}.
Recently, Large Language Model (LLM)-based approaches have emerged, which translate frame-level information into textual descriptions and integrate them using LLMs' reasoning capabilities~\cite{wang2022language, chen2023video}. 
However, these methods often require computationally intensive LLMs with billions of parameters and can produce hallucinated content that deviates from the visual input.

To address these limitations, we propose a novel approach to \textit{zero-shot} video captioning, which requires neither task-specific training nor target dataset annotations\footnote{This is how we define zero-shot video captioning in our paper, though other works may use a different definition.}.
Our approach employs a scene graph for a structured representation of visual content, along with a decoding scheme tailored for video captioning.
Specifically, we first generate frame-level captions using an image-based VLM and parse these captions into scene graphs. 
Next, we consolidate these frame-level scene graphs into a video-level scene graph using a scene graph integration algorithm to represent the visual content of the entire input video.
Finally, the video-level scene graph is converted into a video caption via a lightweight graph-to-text decoder trained solely on text corpora. 
This pipeline effectively adapts the image VLM for the video domain and provides an efficient alternative to na\"ively training computationally intensive video captioning models.
Experimental results demonstrate the effectiveness of our method, outperforming existing zero-shot baselines on both video captioning and video paragraph captioning benchmarks.
Our main contributions are summarized as follows: 
%
\vspace{-4mm}
\begin{itemize}
	\item We propose a novel zero-shot video captioning framework based on scene graphs, which requires neither video captioning training nor task-specific annotations. 
	\item We propose a scene graph consolidation algorithm to merge frame-level information into a unified video-level representation, enabling both a holistic and fine-grained understanding of the video.
	\item The proposed method achieves strong performance on video captioning tasks while significantly reducing computational cost.
\end{itemize}
