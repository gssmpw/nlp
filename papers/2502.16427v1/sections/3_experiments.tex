% !TEX root = ../main.tex

\input{tables/vc}

\section{Experiment}

We demonstrate the effectiveness of the proposed approach, evaluating performance and conducting analysis on both video captioning and video paragraph captioning datasets.

\label{sec:experiments}
\subsection{Experimental setup}
\label{sec:exp_setup}

We provide the detailed information about target tasks with their datasets and baselines.  
We also discuss a list of performance metrics used in our evaluation.

\subsubsection{Target tasks}
We conducted experiments on two tasks: zero-shot video captioning and zero-shot video paragraph captioning. 
Video captioning generates a single sentence describing an event in a short clip, typically only a few seconds long, while video paragraph captioning produces a paragraph summarizing multiple events in a longer video, often spanning several minutes.
Note that, since we focus on zero-shot learning, there is no direct supervision for both the target tasks.

\subsubsection{Dataset and baselines}
To evaluate the performance in zero-shot video captioning, we used the test set of MSR-VTT~\cite{xu2016msr-vtt}. 
We compared our approach with several existing methods, including:
1) test-time optimization via gradient manipulation with CLIP embeddings, \eg, ZeroCap~\cite{tewel2021zero} and Tewel~\etal~\cite{Tewel_2023_BMVC}, 2) optimization of inference procedure in the decoder using the CLIP image-text similarity, \eg, MAGIC~\cite{su2022language}, and 3) text-only training methods, \eg, DeCap~\cite{lidecap} and C$^{3}$~\cite{zhang2024connect}, which are trained solely on text corpora, 4) LLM-based video understanding methods, \eg, VidIL~\cite{wang2022language} and Video ChatCaptioner~\cite{chen2023video}, which utilize proprietary, commercially available LLMs along with textual representations derived from various image-language models, and 5) LLM summarization, which takes the same set of frame captions as our method and generates video captions using a pretrained LLM, Mistral-7B-Instruct-v0.3\footnote{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3}, by text summarization\footnote{Please refer to Appendix~\ref{appendix_sec:prompt} for details on the prompt instructions used in our LLM summarization approach.}.
The LLM summarization baseline enables direct comparison between our explicit modeling of visual content using scene graphs and the LLM's latent modeling based on frame-level captions.

For video paragraph captioning, we used the \textit{ae-val} set of ActivityNet Captions~\cite{krishna2017dense} and compared our algorithm with supervised approaches, including MFT~\cite{xiong2018move}, PDVC~\cite{wang2021end}, Vid2Seq~\cite{yang2023vid2seq}, and Streaming GIT~\cite{zhou2024streaming}, as well as an LLM summarization baseline.
Note that there are no well-established baselines for zero-shot video paragraph captioning.

\subsubsection{Evaluation metrics}

Following standard performance evaluation protocols in video captioning, we adopt $n$-gram-based metrics, including BLEU-4 (B@4)~\cite{papineni2002bleu}, METEOR~\cite{banerjee2005meteor}, and CIDEr~\cite{vedantam2015cider} in our experiments, which measure the overlap between generated and reference captions.
Since these $n$-gram-based metrics are limited in capturing semantic details and contextual accuracy beyond literal phrase matching, they are not ideal to use for video captioning tasks that aim to incorporate detailed information across multiple video frames.

To address these limitations, we introduce an additional embedding-based evaluation metric, BERTScore~\cite{zhang2019bertscore}, widely used in natural language processing tasks such as machine translation and summarization. 
BERTScore measures token-level cosine similarities between generated and reference captions, capturing semantic similarity beyond exact $n$-gram matches as follows: 
%
\begin{equation}
\begin{aligned}
	P_{\text{BERT}} &= \frac{1}{|\hat{\mathcal{Z}}|} \sum_{\hat{z}_j \in \hat{\mathcal{Z}}} \max_{z_i \in \mathcal{Z}} z_i^{\top} \hat{z}_j, \\
	R_{\text{BERT}} &= \frac{1}{|\mathcal{Z}|} \sum_{z_i \in \mathcal{Z}} \max_{\hat{z}_j \in \hat{\mathcal{Z}}} z_i^{\top} \hat{z}_j, \\
	F_{\text{BERT}} &= \frac{2 \cdot P_{\text{BERT}} \cdot R_{\text{BERT}}}{P_{\text{BERT}} + R_{\text{BERT}}},
\end{aligned}
\end{equation}
%!TEX encoding = UTF-8 Unicode
where $\mathcal{Z} \equiv \{ z_1, z_2, \dots\}$ and $\hat{\mathcal{Z}} \equiv \{ \hat{z}_1, \hat{z}_2, \dots\}$ represent the sets of token embeddings in the reference and generated captions, respectively. 
%

\subsection{Implementation details}
Our graph-to-text model employs a BERT-based~\cite{devlin2018bert} architecture as our graph encoder, with modifications for input graph processing and attention masking, as described in Section~\ref{sec:videocaption}, while the T5-base model~\cite{raffel2020exploring} is adopted as our text decoder.
We use the AdamW~\cite{loshchilov2017decoupled} optimizer with a weight decay of 0.05, an initial learning rate of 0.0001, and linear warmup over the first 1\% of total training steps. 
The model is trained for $1K$ iterations with a batch size of 512. 
For scene graph parsing, we use FACTUAL-MR~\cite{li-etal-2023-factual}.

For video captioning, we apply beam search with five beams, a maximum sequence length of 32, and a length penalty of 0.6. 
Video paragraph captioning, which requires more detailed descriptions, is generated using beam search with three beams, a maximum sequence length of 400, and a length penalty of 1. 
Prioritized subgraph extraction is applied only to video captioning, as video paragraph captioning aims to capture richer scene context without filtering out information.
To further enhance video paragraph captioning, we fine-tuned the model on the Visual Genome paragraph captioning~\cite{krause2016paragraphs} dataset for an additional 500 iterations. 
All frame captions were generated using LLAVA-NEXT-7B~\cite{liu2024llavanext} with one of three randomly selected decoding strategies: (1) greedy decoding, (2) beam search with three beams, or (3) nucleus sampling with $p = 0.7$ and temperature $T = 0.7$.



\input{tables/vpc}

\input{figures/fig_qual}


\input{figures/fig_qual_vpc}
\input{tables/ablation_vpc}


\subsection{Main results}
\subsubsection{Zero-shot video captioning}

Table~\ref{table:vc} presents the quantitative results of zero-shot video captioning on the MSR-VTT test set. 
Among text-only training methods, DeCap-BookCorpus, DeCap-CC3M, and DeCap-COCO are trained on external text corpora, whereas DeCap-MSRVTT and C$^{3}$ leverage MSR-VTT reference captions. VidIL\footnote{Since text-davinci-002 is deprecated, we use GPT-3.5-turbo-instruct in our experiments, as recommended by OpenAI.} uses few-shot examples from the target dataset to construct prompts. 
In contrast, our method remains fully independent of MSR-VTT reference captions at all stages.

As shown in the table~\ref{table:vc}, our approach achieves the highest scores in most metrics among the methods that do not use reference captions.
Test-time and inference optimization methods show poor performance while incurring high computational costs.
Video ChatCaptioner uses multi-turn question-answering between an LLM and an image VLM to obtain missing details by querying additional frames.
However, because LLMs are not inherently trained to understand video content, they are often distracted to minor details rather than core events, resulting in captions that fail to capture the essential content of the video \eg, “There are no animals present in the park scene.”.
The LLM summarization baseline generates fluent captions but occasionally treats the same object appearing in different frames as distinct entities. 
In contrast, our scene graph-based approach maintains object identity by merging repeated instances into a single object node, ensuring consistency.
Furthermore, our method consistently outperforms other text-only training methods that do not rely on target dataset annotations, demonstrating the effectiveness of scene graphs as an intermediate representation for bridging visual content with text, compared to direct video-text alignment.
Although DeCap-MSRVTT and C$^{3}$ are trained using target dataset annotations, and VidIL leverages few-shot examples, our method achieves comparable or even superior performance.

Notably, our approach achieves strong performance at significantly lower inference cost by using only a lightweight graph-to-text decoder and a structured scene graph input, in contrast to test-time optimization methods requiring repeated gradient calculations, and LLM-based methods relying on billion-scale models and lengthy input sequences comprising frame captions and instructional prompts.



\subsubsection{Zero-shot video pargraph captioning}
Table~\ref{table:vpc} presents zero-shot video paragraph captioning results on the \textit{ae-val} set of the ActivityNet captions dataset, comparing with supervised models and the LLM summarization baseline.
While supervised models achieve the highest scores, SGVC consistently outperforms the LLM summarization baseline. 
The performance gap between our method and the LLM baseline increases in video paragraph captioning, demonstrating our approach's effectiveness in generating more comprehensive and detailed descriptions.
\vspace{-4mm}

\subsection{Analysis}
Figure~\ref{fig:qual_vc} presents the qualitative results for zero-shot video captioning on the MSR-VTT test set, and Figure~\ref{fig:qual_vpc} shows the qualitative results for video paragraph captioning on the \textit{ae-val} set of ActivityNet Captions.
Our method generates detailed and contextually rich captions that accurately capture events, objects, and relationships across frames.
In contrast, test-time optimization and text-only training methods often yield low-quality or nonsensical captions, while LLM summarization and Video ChatCaptioner produce fluent but occasionally hallucinated content, introducing objects or attributes not actually present in the video.


Table~\ref{table:ablation_vpc} shows our ablation study on the number of frames used for video paragraph captioning, examining both the LLM summarization baseline and our method, from 4 to 12 frames.
Increasing the number of frames typically improves performance across most metrics and stabilizes beyond 10 frames. 
Our method consistently outperforms the LLM summarization baseline at every frame count.

\vspace{-2mm}
