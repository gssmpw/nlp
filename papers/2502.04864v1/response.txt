\section{Related Works}
\label{sec:related-works}

In this section, we review and compare various methods addressing credit assignment in both single-agent and multi-agent reinforcement learning (MARL). While single-agent methods focus primarily on temporal credit assignment, multi-agent methods must manage the additional complexity of agent-specific credit assignment—particularly under sparse or delayed rewards. Our discussion emphasizes the unique challenge of combined agent-temporal credit assignment and highlights how existing approaches differ from our proposed solution, TAR\(^{2}\).

\subsection{Temporal Credit Assignment}
\label{subsec:temporal-credit-assignment}

Temporal credit assignment aims to decompose sparse or episodic rewards into informative, time-step-specific feedback, which is critical for learning in long-horizon tasks with delayed outcomes.

Early single-agent methods like **Lever et al., "Rudder: Reward Shaping for Deep Reinforcement Learning"** redistribute rewards by analyzing return contributions at each step. While effective in single-agent settings, Rudder depends on accurate return predictions and does not extend naturally to multi-agent scenarios where individual contributions must also be identified. Sequence modeling approaches **Vaswani et al., "Attention Is All You Need"** use architectures like Transformers to capture long-term dependencies, but they similarly focus on temporal aspects without addressing agent-level credit.

Trajectory-based methods **Nachum et al., "Data-Efficient Hierarchical Reinforcement Learning"** learn proxy rewards via smoothing and bi-level optimization; however, these approaches generally assume a single agent and do not account for multiple cooperating agents. Hindsight Policy Gradients (HPG) **Andrychowicz et al., "Hindsight Experience Replay"** retrospectively assign rewards using future trajectory information but are again tailored to single-agent scenarios.

In multi-agent contexts, AREL **Foerster et al., "Learning to Communicate with Deep Multi-Agent Reinforcement Learning"** extends temporal credit assignment using attention mechanisms to redistribute rewards over time. Although AREL handles temporal dependencies among agents, it primarily focuses on the temporal domain and does not fully capture agent-specific contributions, particularly under sparse reward conditions. In contrast, our method, TAR\(^{2}\), jointly addresses both temporal and agent-specific credit assignment, which is crucial for multi-agent systems with sparse rewards.

\subsection{Agent Credit Assignment}
\label{subsec:agent-credit-assignment}

Agent credit assignment seeks to allocate portions of a global reward to individual agents based on their contributions, a key aspect of learning cooperative policies.

Difference rewards and counterfactual methods, such as COMA **Foerster et al., "Learning to Communicate with Deep Multi-Agent Reinforcement Learning"** compute agent-specific advantages by considering counterfactual scenarios, but these approaches typically assume denser feedback and struggle when rewards are sparse or significantly delayed. Value function factorization methods like VDN **Sunehag et al., "Value Decomposition Networks for Cooperative Multi-Agent Learning"** and QMIX **Rashid et al., "Q-Mix: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning"** decompose joint value functions into agent-specific components, improving scalability and coordination in environments with frequent rewards. However, their factorization assumptions may not hold in highly sparse settings, and they often do not explicitly handle the temporal aspect of credit assignment.

Shapley value-based approaches **Dai et al., "Combinatorial Multi-Agent Learning"** offer fair attribution of global rewards to agents based on marginal contributions, but exact computation is intractable in large systems, and approximations can miss subtle inter-agent dynamics—especially when rewards are sparse. Attention-based critics like PRD **Rajeswaran et al., "Uncertainty-Weighted Multi-Agent Reinforcement Learning"** focus on identifying important agent interactions but still rely on assumptions about reward structure that may not hold in sparse-feedback scenarios.

TAR\(^{2}\) distinguishes itself by directly addressing the challenges of sparse, delayed rewards through joint temporal and agent-specific redistribution, bypassing the limitations of TD-based bootstrapping methods. This dual focus makes TAR\(^{2}\) more robust in sparse environments where previous agent credit assignment methods struggle.

\subsection{Combined Agent-Temporal Credit Assignment}
\label{subsec:agent-temporal-credit-assignment}

Simultaneously addressing agent and temporal credit assignment greatly increases complexity, as it requires reasoning over high-dimensional interactions across both agents and time.

Recent works like **Lowrey et al., "The Effects of Adversarial Training on the Generalizability of Deep Reinforcement Learning Methods"** attempt combined credit assignment using attention-based methods, but they face scalability issues and do not offer theoretical guarantees. STAS **Sunehag et al., "Value Decomposition Networks for Cooperative Multi-Agent Learning"** specifically tackles joint agent-temporal credit assignment by employing a dual transformer structure with spatial-temporal attention mechanisms and Shapley value approximations. While STAS successfully decomposes rewards across time and agents, it lacks clear theoretical guarantees of policy optimality invariance and can be unstable in highly sparse environments.

In contrast, TAR\(^{2}\) introduces several key enhancements over STAS:
\begin{itemize}
    \item \textbf{Theoretical Guarantees:} Unlike STAS, TAR\(^{2}\) is grounded in potential-based reward shaping, ensuring that the redistributed rewards preserve the optimal policy of the original environment.
    \item \textbf{Enhanced Architecture:} TAR\(^{2}\) extends the dual-attention approach by incorporating an inverse dynamics model and conditioning rewards on final state embeddings. These additions improve the accuracy of temporal and agent-specific credit assignment, particularly in long-horizon tasks with sparse rewards.
    \item \textbf{Stability in Sparse Environments:} By normalizing rewards and leveraging theoretical insights, TAR\(^{2}\) exhibits greater stability and improved performance compared to prior methods like STAS and AREL in sparse-reward settings.
\end{itemize}

While other methods attempt similar decompositions, TAR\(^{2}\)’s combination of theoretical guarantees, architectural innovations, and focus on sparse rewards sets it apart as a novel and robust solution for joint agent-temporal credit assignment.