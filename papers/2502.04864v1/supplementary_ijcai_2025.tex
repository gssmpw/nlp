% \documentclass{article}

% \appendix
\onecolumn


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022

% \Papers may only be up to nine pages long, including figures. Additional pages con    taining only
% 10 acknowledgments and references are allowed.

% ready for submission
% \usepackage{neurips_2023}
% \documentclass[sigconf]{aamas} 



% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


% \usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage[hidelinks]{hyperref}
% \usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
% \usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography
% \usepackage{xcolor}         % colors
% \usepackage{algorithm}
% \usepackage{amsmath}
% \usepackage{graphicx}
% \usepackage{float}
% \usepackage[normalem]{ulem}
% \usepackage{geometry}
% \geometry{margin=.5in}

% \usepackage{svg}
% % \usepackage{wrapfig}
% \usepackage{array}
% \usepackage{adjustbox}
% % \usepackage{tabularx}
% \usepackage{lscape}

% \usepackage{natbib}
% \usepackage{amsthm}
% \usepackage{natbib}
% \usepackage{tabularx}

% \newtheorem{example}{Example}
% \newtheorem{theorem}{Theorem}
% \newtheorem{proposition}{Proposition}
% \newtheorem{assumption}{Assumption}

% \usepackage{algorithm}
% \usepackage{algorithmic}


% LIST OF FIGURES
% reward plots for each environment
% Combat environment, PRD vs shared 
% Sequence of frames from a GIF showing what agents are assigned to the relevant set of a particular agent
% Histogram of policy improvement after performing an update with either PRD-MAPPO or just PPO, for policy taken at intermediate point during training,  


% THINGS LEFT TO DO FOR FIGURES
% Make all PDFs
% if it looks ok, we'll probably want to re-order the legend entries so that the order of entries roughly corresponds to the order the lines come in, so PRD-MAPPO should always be on top
% add rewards at ep 0 to figures

\title{TAR$^2$: Temporal-Agent Reward Redistribution for Optimal Policy Preservation in Multi-Agent Reinforcement Learning \\
Supplemental Material}

% \title{RATAN: Reward Assignment through Temporal-Agent Networks for Optimal Policy Preservation in Multi-Agent Reinforcement Learning \\
% Supplemental Material}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.




% \begin{document}


\maketitle

\begin{center}
    {\LARGE \textbf{Supplementary Material}}  % Large, bold title at the top
\end{center}

\section{Detailed Discussion on Potential-Based Reward Shaping}
\label{app:potential-based-shaping}

\citet{ng1999policy} presented a single-agent reward shaping method to address the credit assignment problem by introducing a potential-based shaping reward to the environment. The combination of the shaping reward with the original reward can enhance the learning performance of a reinforcement learning algorithm and accelerate the convergence to the optimal policy. \citet{Devlin2011TheoreticalCO} and \citet{lu2011policy} extended potential-based reward shaping to multi-agent systems as follows:

\begin{theorem}
\label{theorem: potential_based_rew_shaping_theorem}
Given an $n$-player discounted stochastic game $M = (S, A_1, \ldots, A_n, T, \gamma, R_1, \ldots, R_n)$, we define a transformed $n$-player discounted stochastic game $M' = (S, A_1, \ldots, A_n, T, \gamma, R_1 + F_1, \ldots, R_n + F_n)$, where $F_i \in S \times S$ is a shaping reward function for player $i$. We call $F_i$ a potential-based shaping function if $F_i$ has the form:

\[
F_i(s, s') = \gamma \Phi_i(s') - \Phi_i(s),
\]

where $\Phi_i: S \to \mathbb{R}$ is a potential function. Then, the potential-based shaping function $F_i$ is a necessary and sufficient condition to guarantee the Nash equilibrium policy invariance such that:

\begin{itemize}
    \item \textbf{(Sufficiency)} If $F_i$ ($i = 1, \ldots, n$) is a potential-based shaping function, then every Nash equilibrium policy in $M'$ will also be a Nash equilibrium policy in $M$ (and vice versa).
    \item \textbf{(Necessity)} If $F_i$ ($i = 1, \ldots, n$) is not a potential-based shaping function, then there may exist a transition function $T$ and reward function $R$ such that the Nash equilibrium policy in $M'$ will not be the Nash equilibrium policy in $M$.
\end{itemize}
\end{theorem}

\noindent
In summary, potential-based reward shaping ensures that Nash equilibrium policies are preserved, enhancing learning without altering the strategic dynamics. This principle underpins our proposed reward redistribution method, which we will validate in the following sections, demonstrating its effectiveness in multi-agent reinforcement learning.

\paragraph{Relevance to TAR\(^2\).}
Potential-based shaping provides the theoretical underpinning for our reward redistribution strategy. By ensuring that our redistribution function adheres to a potential-based form, we guarantee that:
\begin{itemize}
    \item The reshaped rewards do not alter the set of optimal policies (policy optimality invariance).
    \item The convergence and strategic dynamics are preserved, even as we provide denser, agent- and time-specific rewards.
\end{itemize}
This detailed understanding justifies the design choices in TAR\(^2\) and underlines its robustness in multi-agent scenarios.

\section{Formulating the Reward Redistribution Mechanism}
\label{sec:detailed_formulation_reward_func}

To effectively address both temporal and agent-specific credit assignment in episodic MARL, we introduce a two-stage reward redistribution mechanism. This mechanism decomposes the global episodic reward into more informative, granular components that facilitate better learning and coordination among agents.

\paragraph{Temporal Redistribution}

We first redistribute the global episodic reward across the trajectory's time steps using a temporal weighting function 
\[
w^{\text{temporal}}_t \sim \mathcal{W}_{\omega}(h_t, a_t, h_{|\tau|}, a_{|\tau|}),
\] 
parameterized by \(\omega\). This function takes as input the history \(h_t\), the joint action \(a_t\) at time \(t\), and the final state-action pair \((h_{|\tau|}, a_{|\tau|})\), assigning a portion of the global reward to each timestep \(t\):

\begin{equation}
    r_{\text{global}, t} = w^{\text{temporal}}_t \cdot r_{\text{global}, \text{episodic}}(\tau).
    \label{eq:temporal_rew_red}
\end{equation}

We enforce the normalization condition:

\begin{equation}
    \sum_{t=1}^{|\tau|} w^{\text{temporal}}_t = 1.
    \label{eq:temporal_weights_normalization}
\end{equation}

\paragraph{Agent-wise Redistribution}

Next, each temporally redistributed reward \( r_{\text{global}, t} \) is allocated to individual agents using agent-specific weighting functions 
\[
w^{\text{agent}}_{i,t} \sim \mathcal{W}_{\kappa}(h_{i,t}, a_{i,t}, h_{|\tau|}, a_{|\tau|}),
\] 
parameterized by \(\kappa\). Each \(w^{\text{agent}}_{i,t}\) takes as input agent \(i\)'s history \(h_{i,t}\), action \(a_{i,t}\) at time \(t\), and the final state-action pair \((h_{|\tau|}, a_{|\tau|})\), and redistributes the temporal reward among agents:

\begin{equation}
    r_{i, t} = w^{\text{agent}}_{i,t} \cdot r_{\text{global}, t}.
    \label{eq:agent_rew_red}
\end{equation}

The agent weights are normalized at each timestep:

\begin{equation}
    \sum_{i=1}^N w^{\text{agent}}_{i,t} = 1 \quad \forall t.
    \label{eq:agent_weights_normalization}
\end{equation}

\paragraph{Overall Reward Redistribution}

Combining temporal and agent-wise redistributions, the per-agent reward at timestep \(t\) is given by:

\begin{equation}
    r_{i, t} = w^{\text{agent}}_{i,t} \cdot w^{\text{temporal}}_t \cdot r_{\text{global}, \text{episodic}}(\tau).
    \label{eq:overall_rew_red}
\end{equation}

The normalization constraints (Eqs.~\ref{eq:temporal_weights_normalization} and \ref{eq:agent_weights_normalization}) ensure that the sum of all \(r_{i,t}\) equals the original global episodic reward (refer Assumption 1):

\begin{align}
    \sum_{i=1}^N \sum_{t=1}^{|\tau|} r_{i, t} 
    &= r_{\text{global}, \text{episodic}}(\tau).
    \label{eq:reward_consistency}
\end{align}

\paragraph{Constructing the New Reward Function}

Using the redistributed rewards, we define a new reward function for each agent \(i\) at timestep \(t\):

\begin{align}
    \mathcal{R}_{\omega, \kappa}^i(s_t, a_t, s_{t+1}) 
    &= \mathcal{R}_{\zeta}(s_t, a_t, s_{t+1}) \;+\; r_{i,t} \nonumber \\
    &= \mathcal{R}_{\zeta}(s_t, a_t, s_{t+1}) \;+\; w^{\text{agent}}_{i,t} \cdot w^{\text{temporal}}_t \cdot r_{\text{global}, \text{episodic}}(\tau).
    \label{eq:new_rew_func_def}
\end{align}

In Dec-POMDPs, where the true state \(s_t\) is not directly observable, we approximate this function using agent histories \(h_{i,t}\) and actions \(a_{i,t}\).

\paragraph{Summary}

Our mechanism parameterizes weighting functions using \(\omega\) and \(\kappa\), which condition on the current histories, actions, and the final outcome of the trajectory. By breaking down the episodic reward into time-step-specific and agent-specific components while enforcing normalization constraints, we ensure that the redistributed rewards provide detailed feedback to each agent without altering the underlying global objective.

\section{Optimal Policy Preservation}
\label{sec:optimal_policy_preservation_detailed}

We establish that the optimal policy learned under the densified reward function \( \mathcal{R}_{\omega, \kappa} \) remains optimal for the original reward function \( \mathcal{R}_{\zeta} \).

\begin{theorem}[Optimal Policy Preservation]
Consider two Dec-POMDPs:
\[
\begin{aligned}
\mathcal{M}_{\text{env}} &= (\mathcal{S}, \mathcal{A}, \mathcal{P}, T, O, N, \mathcal{R}_{\zeta}, \rho_0, \gamma), \\
\mathcal{M}_{\text{rrf}} &= (\mathcal{S}, \mathcal{A}, \mathcal{P}, T, O, N, \mathcal{R}_{\omega, \kappa}, \rho_0, \gamma),
\end{aligned}
\]
where \( \mathcal{R}_{\omega, \kappa}^i(s_t,a_t,s_{t+1}) = \mathcal{R}_{\zeta}(s_t,a_t,s_{t+1}) + w^{\text{agent}}_{i,t} w^{\text{temporal}}_t r_{\text{global},\text{episodic}}(\tau) \) for each agent \(i\). If \( \pi^*_{\theta} \) is optimal in \( \mathcal{M}_{\text{rrf}} \), then it is also optimal in \( \mathcal{M}_{\text{env}} \).
\end{theorem}

\begin{proof}[Proof Sketch]
To prove optimality preservation, we show that \( \mathcal{R}_{\omega, \kappa} \) can be expressed as
\[
\mathcal{R}_{\omega, \kappa}^i(s_t,a_t,s_{t+1}) = \mathcal{R}_{\zeta}(s_t,a_t,s_{t+1}) + F_i(s_t,a_t,s_{t+1}),
\]
where \( F_i \) is a potential-based shaping function. For simplicity, assume \( \gamma = 1 \). 

Given Eq.~\ref{eq:new_rew_func_def}, we seek functions \( \phi^i \colon S \to \mathbb{R} \) such that
\[
w^{\text{agent}}_{i,t} w^{\text{temporal}}_t r_{\text{global},\text{episodic}}(\tau) 
= \phi^i(s_{t+1}) - \phi^i(s_t).
\]
This relation holds by defining the potential function for each agent as
\[
\phi^i(s_t) 
= r_{\text{global},\text{episodic}}(\tau) \cdot \sum_{t'=0}^t w^{\text{agent}}_{i,t'}\,w^{\text{temporal}}_{t'}.
\]
With this definition, the shaping function \( F_i(s_t,a_t,s_{t+1}) = \phi^i(s_{t+1}) - \phi^i(s_t) \) matches the additional term in \( \mathcal{R}_{\omega, \kappa}^i \). By the potential-based shaping theorem \citep{ng1999policy,Devlin2011TheoreticalCO}, such an augmentation preserves the optimal policy. 
\end{proof}

This theorem guarantees that learning with our redistributed rewards does not alter the set of optimal policies. 

\section{Impact of Faulty Credit Assignment on Policy Gradient Variance}
\label{sec:faulty_credit_assignment_impact}
% To understand the impact of imperfect credit assignment, we analyze the effect of other agents on the policy gradient update of agent $i$. 
% Consider an actor-critic gradient estimate for a multi-agent system in a Dec-POMDP setting, computed using a state-action sample from an arbitrary timestep $t$. We make no assumptions about the policy parameters of the agents in the multi-agent system. The policy gradient update for agent $i$ could be computed using

To understand the impact of imperfect credit assignment, we examine the influence of other agents on the policy gradient update for agent $i$ in a Dec-POMDP setting. Without assuming any specific policy parameters, the policy gradient update for agent $i$ is computed as:

\begin{equation}
    \label{eq:MAPG_agent_specific_update}
    \hat{\nabla}_{\theta_i} J(\theta, h) = \nabla_{\theta_i} \log \pi_i(a_i|h_i) \mathbb{E}_{\neg h_i, \neg a_i}\left[A(h, a)\right]
\end{equation}

% However, $\mathbb{E}_{\neg h_i, \neg a_i}\left[A(h, a)\right]$ is challenging to compute due to the high dimensionality, dependency on other agents, and the computational complexity involved in accurately modeling and estimating the interdependent histories and actions of multiple agents. 
% However, in practice multi-agent policy-gradient algorithms like MAPPO \citep{yu2022surprising} and MADDPG \citep{lowe2017multi} employ $A(h, a)$ to compute the policy gradient update for each agent. As a result, the credit assignment problem manifests as high variance in advantage estimates, leading to slower learning because of noisier policy gradient estimates.

Calculating $A_i = \mathbb{E}_{\neg h_i, \neg a_i}[A(h, a)]$ is complex due to the high dimensionality and inter-agent dependencies. In practice, multi-agent policy gradient methods like MAPPO \citep{yu2022surprising} and MADDPG \citep{lowe2017multi} use $A(h, a)$ for the policy update thus, 

\begin{equation}
    \label{eq:MAPG_vanilla_update}
    \hat{\nabla}_{\theta_i} J(\theta, h) = \nabla_{\theta_i} \log \pi_i(a_i|h_i) A(h, a)
\end{equation}

This often leads to high variance in advantage estimates, slowing down learning due to noisier gradient updates.

% Multi-agent policy gradient methods approximate the true \textit{advantage} by computing $\hat{A}$, which is actually a stochastic advantage estimation of taking a joint action $a$ while observing the joint agent history $h$, and following the joint policy $\pi$. The advantage function is typically defined as $A^{\pi}(s,a) = Q^{\pi}(s, a) - V^{\pi}(s)$, where $Q^{\pi}(s,a)$ and $V^{\pi}(s)$ are the state-action value function and state-value function, respectively \citep{sutton1998introduction}. However, in practice the state-action value function and state-value function are approximated using $\hat{Q}^{\pi}(h,a)$ and $\hat{V}^{\pi}(h)$ in Dec-POMPDs and there are many ways to compute \(\hat{A}\), generally all involving some error, as the true value functions are unknown \citep{sutton1998introduction, gae}. Intuitively, this is the centralized advantage function which measures how much better it is to select a joint action $a$ than a random action from the joint policy $\pi$, while in state $s$. Besides, to update the policy of agent $i$, we need to compute the advantage of selecting action $a_i$, taking into account the specific contribution and context of agent $i$ within the multi-agent system. Perfect credit assignment would be possible if the advantage function could be computed perfectly for each agent, as it directly measures how a particular action of an agent impacted the total reward obtained by the group. 

Multi-agent policy gradient methods estimate the true \textit{advantage} by calculating $\hat{A}$, a stochastic approximation of the advantage function based on joint actions $a$, joint agent histories $h$, and the joint policy $\pi$ in Dec-POMDPs. The advantage function is defined as $A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s)$, where $Q^{\pi}(s, a)$ and $V^{\pi}(s)$ are the state-action value and state-value functions, respectively \citep{sutton1998introduction}. In Dec-POMDPs, these are approximated as $\hat{Q}^{\pi}(h, a)$ and $\hat{V}^{\pi}(h)$. Since the true value functions are unknown, various methods are used to compute \(\hat{A}\), introducing errors~\citep{sutton1998introduction, gae}. The advantage function reflects how much better it is to take a joint action $a$ versus a random action from $\pi$, while in state $s$. For agent $i$, the goal is to compute the advantage of its action $a_i$ within the multi-agent context, where perfect credit assignment would require perfectly calculating the agent-specific advantage based on its contribution to the overall reward of the group.

\begin{theorem}
    Given that agent $i$'s reward contribution at an arbitrary time-step $t$ is $r_{i,t}(h,a)$ and the episodic reward $r_{\text{global}, \text{episodic}}(\tau) = \sum_{t=1}^{|\tau|}\sum_{i=1}^N r_{i,t} (h, a)$, the conditional variance of $(\hat{\nabla}_{\theta_i} J|h,a)$ is proportional to the conditional variance of the advantage estimate $\hat{A}_{\neg i}(h,a)$ 
\end{theorem}

\begin{proof}
    
The variance of the policy gradient update in eq~\ref{eq:MAPG_vanilla_update} for agent $i$ is:

\begin{equation}
    \mathrm{Var}(\hat{\nabla}_{\theta_i} J|h,a) = \left( \nabla_{\theta_i} \log \pi(a_i|h_i) \right)\left( \nabla_{\theta_i} \log \pi(a_i|h_i) \right)^T \mathrm{Var}(\hat{A}|h,a). \notag
\end{equation}

% The conditional variance of $\mathrm{Var}(\hat{\nabla}_{\theta} J|h,a)$, given $h$ and $a$, is proportional to the variance of $\hat{A}$. While this statement typically implies multiple samples are considered to estimate the variance accurately, we can gain insight into the variance introduced by the contributions of other agents by initially focusing on a single sample scenario allowing us to isolate the variance induced by other agents.
% It is therefore evident that the variance of the policy gradient update is directly proportional to the variance of the advantage estimate: $\mathrm{Var}(\hat{\nabla}_{\theta} J|h,a) \propto \mathrm{Var}(\hat{A}|h,a)$. \\
% Let us analyze the variance of the advantage function, in cooperative multi-agent setting.

This expression shows that the conditional variance of $(\hat{\nabla}_{\theta_i} J|h,a)$ is proportional to the conditional variance of the joint-advantage estimate $\hat{A}|h,a$. While estimating variance typically requires multiple samples, we can initially analyze a single sample to isolate the variance induced by the contributions of other agents. We can express the state-action and state-value function as:
% Next, we examine the variance of the advantage function in cooperative multi-agent settings.

% Let us assume that agent $i$'s reward contribution at an arbitrary time-step $t$ is denoted by $r_{i,t}$ and the episodic reward described in subsection \ref{subsec:episodic_marl} can be derived using $r_{\text{global}, \text{episodic}}(\tau) = \sum_{t=1}^{|\tau|}\sum_{i=1}^N r_{i,t} (h, a)$. From this we can rewrite $\mathcal{Q}(h, a)$ and $\mathcal{V}(h)$ as:
\begin{align}
    \mathcal{Q}(h, a) &= \mathrm{E}_{s_0 \sim \rho_0, s \sim \mathcal{P}, a_i \sim \pi_i} \left[ \sum_{t=1}^{|\tau|} \sum_{i=1}^N r_{i,t} (h, a) \right] \nonumber \\
    \mathcal{V}(h) &= \mathrm{E}_{\pi}[\mathcal{Q}(h, a)] \nonumber
\end{align}

\vspace{-1em} % Reduce space between align blocks

\begin{align}
    \mathcal{A}(h, a) &= \mathcal{Q}(h, a) - \mathcal{V}(h) \nonumber \\
    \mathcal{A}(h, a) &= \mathrm{E}_{s_0 \sim \rho_0, s \sim \mathcal{P}, a_i \sim \pi_i} 
    \left[ \sum_{t=1}^{|\tau|} \sum_{i=1}^N r_{i,t}(h, a) \right] \notag \\
    &\quad - \mathrm{E}_{\pi} \left[\mathrm{E}_{s_0 \sim \rho_0, s \sim \mathcal{P}, a_i \sim \pi_i}
    \left[ \sum_{t=1}^{|\tau|} \sum_{i=1}^N r_{i,t}(h, a) \right] \right]  \nonumber
\end{align}

% \vspace{-1em} % Reduce space after block
Based on the linearity of expectations on $\sum_{t=1}^{|\tau|}\sum_{i=1}^N r_{j,t} = \sum_{t=1}^{|\tau|} r_{i,t} + \sum_{t=1}^{|\tau|}\sum_{j \neq i}^N r_{j,t}$ and by rearranging the terms we get:

\begin{align}
    \mathcal{A}(h, a) &= \mathrm{E}_{s_0 \sim \rho_0, s \sim \mathcal{P}, a_i \sim \pi_i} 
    \left[\sum_{t=1}^{|\tau|} r_{i,t}(h, a) \right] \notag \\
    &\quad - \mathrm{E}_{\pi} \left[\mathrm{E}_{s_0 \sim \rho_0, s \sim \mathcal{P}, a_i \sim \pi_i} 
    \left[\sum_{t=1}^{|\tau|} r_{i,t}(h,a) \right] \right] \notag \\
    &\quad + \mathrm{E}_{s_0 \sim \rho_0, s \sim \mathcal{P}, a_i \sim \pi_i} 
    \left[\sum_{t=1}^{|\tau|} \sum_{j \neq i}^N r_{j,t}(h, a) \right] \notag \\
    &\quad - \mathrm{E}_{\pi} \left[\mathrm{E}_{s_0 \sim \rho_0, s \sim \mathcal{P}, a_i \sim \pi_i}
    \left[\sum_{t=1}^{|\tau|}\sum_{j \neq i}^N r_{j,t}(h) \right] \right] \notag
\end{align}

The advantage estimate considering only the contribution of agent $i$ is the only advantage term that should be considered while calculating the policy gradient update for agent $i$ as shown in eq~\ref{eq:MAPG_agent_specific_update} \\ $\mathcal{A}_i = \mathrm{E}_{s_0 \sim \rho_0, s \sim \mathcal{P}, a_i \sim \pi_i ..}[\sum_{t=1}^{|\tau|} r_{i,t}(h, a)] - \mathrm{E}_{\pi}[\mathrm{E}_{s_0 \sim \rho_0, s \sim \mathcal{P}, a_i \sim \pi_i ..}[\sum_{t=1}^{|\tau|} r_{i,t}(h,a)]]$ whereas the advantage estimate due to other agents given by 
\begin{align}
    \mathcal{A}_{\neg i} = \mathrm{E}_{s_0 \sim \rho_0, s \sim \mathcal{P}, a_i \sim \pi_i ..}[\sum_{t=1}^{|\tau|}\sum_{j \neq i}^N r_{j,t}(h,a)] \nonumber \\ - \mathrm{E}_{\pi}[\mathrm{E}_{s_0 \sim \rho_0, s \sim \mathcal{P}, a_i \sim \pi_i ..}[\sum_{t=1}^{|\tau|}\sum_{j \neq i}^N r_{j,t}(h)]] \nonumber
\end{align} 
induces noise into the policy gradient update for agent $i$. Thus, 

\begin{align}
    \mathcal{A}(h, a) &= \mathcal{A}_{i}(h, a) + \mathcal{A}_{\neg i}(h, a) \nonumber
\end{align}
Using variance of the sum of random variables
\begin{align}
    \mathrm{Var}(\mathcal{A}(h, a)) &= \mathrm{Var}(\mathcal{A}_{i}(h, a)) + \mathrm{Var}(\mathcal{A}_{\neg i}(h, a)) \nonumber \\ &+ 2 \mathrm{Cov}(\mathcal{A}_{i}(h, a), \mathcal{A}_{\neg i}(h, a)) \nonumber
\end{align}
To express the equation in terms of variance, we use the Cauchy-Schwarz inequality, which states that for any two random variables $\mathcal{A}_{i}$ and $\mathcal{A}_{\neg i}$:
\begin{align}
    \mathrm{Cov}(\mathcal{A}_{i}(h, a), \mathcal{A}_{\neg i}(h, a)) &\le \sqrt{\mathrm{Var}(\mathcal{A}_{i}(h, a)) \mathrm{Var}(\mathcal{A}_{\neg i}(h, a))} \nonumber
\end{align}
By substituting this inequality, we get an upper bound on our equation,
\begin{align}
    \mathrm{Var}(\mathcal{A}(h, a)) &\le \mathrm{Var}(\mathcal{A}_{i}(h, a)) + \mathrm{Var}(\mathcal{A}_{\neg i}(h, a))\nonumber \\ &+ 2 \sqrt{\mathrm{Var}(\mathcal{A}_{i}(h, a)) \mathrm{Var}(\mathcal{A}_{\neg i}(h, a))} \nonumber \\
    \mathrm{Var}(\mathcal{A}(h, a)) &\le (\sqrt{\mathrm{Var}(\mathcal{A}_{i}(h, a))} + \sqrt{\mathrm{Var}(\mathcal{A}_{\neg i}(h, a))})^2 \nonumber
\end{align}
Thus, we get $\mathrm{Var}(\hat{\nabla}{\theta_i} J|h,a) \propto \mathrm{Var}(\hat{A}_{\neg i}|h,a)$
\end{proof}

The above equation shows that the variance of the policy gradient update grows approximately linearly with the number of agents in the multi-agent system. This increase in variance reduces the signal-to-noise ratio of the policy gradient, necessitating more updates for effective learning. Proper credit assignment can mitigate this issue by enhancing the signal-to-noise ratio, thereby facilitating more sample-efficient learning.


\section{Policy Gradient Update Equivalence with Reward Redistribution}
\label{sec:pg_update_eq}

In this subsection, we establish that the policy gradient update for an arbitrary agent $k$, derived from the reward redistribution function $R_{\omega, \kappa}$, shares the same direction as the policy gradient update under the environment's original reward function $R_{\zeta}$, though potentially with a different magnitude. This ensures that the policy update trajectory towards the optimal policy is preserved for every agent.

\begin{proposition}
Let $\pi_{\theta}$ be the joint policy in a decentralized execution paradigm, where the joint policy is the product of individual agent policies: $\pi_{\theta} = \prod_{k=1}^N \pi_{\theta_{k}}$ 
% \citep{Oliehoek2016ACI, amato2024partial}. 
The policy gradient update for an arbitrary agent $k$ under the reward redistribution function $R_{\omega, \kappa}$ is proportional to the policy gradient update under the environment's original reward function $R_{\zeta}$, preserving the direction of the policy gradient and hence the joint policy update trajectory towards the optimal joint policy.

\begin{align}
    \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}\left[\sum_{t=1}^{|\tau|} r_{k, t}\right] &= \nabla_{\theta_{k}} \mathbb{E}_{\pi{\theta_{k}}}\left[\delta(\tau) r_{\text{global}, \text{episodic}}(\tau)\right] 
    \label{eq:PG_indiv_rew_global_rew}
\end{align}
where $\tau$ is the multi-agent trajectory attained from the joint policy $\pi_{\theta}$ and $\delta : \mathcal{H} \times \mathcal{A} \times \mathcal{H}_{|\tau|} \times \mathcal{A}_{|\tau|} \to \mathbb{R} \in [0,1]$ is a function conditioned on the trajectory.

\end{proposition}

\begin{proof}

Consider the policy gradient update for agent $k$ under the reward redistribution function (For brevity, we drop the detailed notation of the variables.):
\begin{align}
    \nabla_{\theta_k} J(\theta_k) &= \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}[r_{\text{global}, \text{episodic}}(\tau)]
\end{align}
From the definition of the reward redistribution function in Assumption 1 of the main text
% ~\ref{assumption:rew_red_ass}
, we have:
\begin{align}
    \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}[r_{\text{global}, \text{episodic}}(\tau)] &= \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}\left[\sum_{i=1}^N \sum_{t=1}^T r_{i, t}\right] \nonumber \\
    &= \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}\left[\sum_{t=1}^T r_{k, t} + \sum_{i \neq k} \sum_{t=1}^T r_{i, t}\right] \nonumber \\
    &= \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}\left[\sum_{t=1}^T r_{k, t}\right] + \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}\left[\sum_{i \neq k} \sum_{t=1}^T r_{i, t}\right]. \nonumber
\end{align}

Given the definitions \( \sum_{i=1}^N w^{agent}_{t, i} = 1 \), equation 7 in the main text
% ~\eqref{eq:agent_weights}
, and \( \sum_{t=1}^{|\tau|} w^{temporal}_t = 1 \), equation 8 in the main text
% ~\eqref{eq:temporal_weights})
, we rewrite the above equation as:
\begin{align}
    \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}[r_{\text{global}, \text{episodic}}(\tau)] = \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}\left[\sum_{t=1}^{|\tau|} r_{k, t}\right] \nonumber \\ + \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}\left[\left(\sum_{t=1}^{|\tau|} w^{temporal}_t (1 - w^{agent}_{k, t})\right) r_{\text{global}, \text{episodic}}(\tau)\right] \nonumber \\
    \text{Let} \; (w^{temporal}_t (1 - w^{agent}_{k, t})) = M_{t}, \nonumber \\ 
    \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}[r_{\text{global}, \text{episodic}}(\tau)] = \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}\left[\sum_{t=1}^{|\tau|} r_{k, t}\right] \nonumber \\ + \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}\left[\left(\sum_{t=1}^{|\tau|} M_{t}\right) r_{\text{global}, \text{episodic}}(\tau)\right] \nonumber \\
    \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}\left[\left(1 - \sum_{t=1}^{|\tau|} M_{t}\right) r_{\text{global}, \text{episodic}}(\tau)\right] = \nabla_{\theta_{k}} \mathbb{E}_{\pi_{\theta_{k}}}\left[\sum_{t=1}^{|\tau|} r_{k, t}\right] \nonumber \\
    \text{Comparing with equation~\ref{eq:PG_indiv_rew_global_rew} we get:} \nonumber \\ 
    \delta(\tau) = 1 - \sum_{t=1}^{|\tau|} M_{t}. \nonumber
\end{align}

Since \( 1 \geq (1 - w^{agent}_{k, t}) \geq 0 \), \( 1 \geq w^{temporal}_t \geq 0 \), and \( \sum_{t=1}^{|\tau|} w^{temporal}_t = 1 \), the term \( \sum_{t=1}^{|\tau|} w^{temporal}_t \times (1 - w^{agent}_{k, t}) \) represents a weighted sum, ensuring that:

\begin{align}
    \label{eq:}
    1 \geq \delta(\tau) &= 1 - \sum_{t=1}^{|\tau|} M_{t} \geq 0.
\end{align}

Now we show that the individual policy updates of each agent push the joint policy update towards the optimal joint policy, given that the update direction of each agent’s individual policy under the reward redistribution function is the same as the joint policy update under the original reward function.

The joint policy is the product of the individual agents’ policies $\pi_{\theta} = \prod_{k=1}^N \pi_{\theta_k}$ where \( \theta = (\theta_1, \theta_2, \dots, \theta_N) \) represents the parameters of the joint policy. In multi-agent systems, the joint policy gradient can be decomposed into the sum of individual agents’ policy gradients:

\[
\nabla_{\theta} J(\theta) = \sum_{k=1}^N \nabla_{\theta_k} J(\theta_k)
\]

For agent \( k \), the policy gradient update under the reward redistribution function \( \mathcal{R}_{\omega, \kappa} \) is:

\[
\nabla_{\theta_k} J_{\omega, \kappa}(\theta_k) = \mathbb{E}_{\pi_{\theta_k}} \left[ \nabla_{\theta_k} \log \pi_{\theta_k}(a_k | h_k) A_k^{\omega, \kappa}(h_k, a_k, h_{|\tau|}, a_{|\tau|}) \right]
\]

where \( A_k^{\omega, \kappa}(h_k, a_k, h_{|\tau|}, a_{|\tau|}) \) is the advantage function for agent \( k \) under the redistributed reward function \( \mathcal{R}_{\omega, \kappa} \). Similarly, under the environment’s original reward function \( \mathcal{R}_{\zeta} \), the policy gradient for agent \( k \) is:

\[
\nabla_{\theta_k} J_{\zeta}(\theta_k) = \mathbb{E}_{\pi_{\theta_k}} \left[ \nabla_{\theta_k} \log \pi_{\theta_k}(a_k | h_k) A_k^{\zeta}(h_k, a_k) \right]
\]

Given that the update direction is preserved under the reward redistribution, the advantage function \( A_k^{\omega, \kappa}(h_k, a_k, h_{|\tau|}, a_{|\tau|}) \) and \( A_k^{\zeta}(h_k, a_k) \) share the same sign for the same \( (h_k, a_k) \), meaning that both updates point in the same direction.

Now, for the joint policy gradient under the original reward function, we have:

\[
\nabla_{\theta} J_{\zeta}(\theta) = \sum_{k=1}^N \nabla_{\theta_k} J_{\zeta}(\theta_k)
\]

Similarly, for the joint policy gradient under the redistributed reward function:

\[
\nabla_{\theta} J_{\omega, \kappa}(\theta) = \sum_{k=1}^N \nabla_{\theta_k} J_{\omega, \kappa}(\theta_k)
\]

From equation~\ref{eq:PG_indiv_rew_global_rew} we know that,

\[
\nabla_{\theta_k} J_{\omega, \kappa}(\theta_k) = \delta_k \nabla_{\theta_k} J_{\zeta}(\theta_k)
\]

Since the individual policy gradient updates point in the same direction under both reward functions, it follows that the joint policy update direction is also preserved. More formally, we can express the policy gradient under the redistributed reward function as:

\[
\nabla_{\theta} J_{\omega, \kappa}(\theta) = \sum_{k=1}^N \delta_k \nabla_{\theta_k} J_{\zeta}(\theta_k)
\]

where \( \delta_k \in [0, 1] \) represents the magnitude scaling based on the contribution of agent $k$. The scaling factor \( \delta_k \) is less than or equal to 1 due to the redistribution, but crucially, the direction remains the same. This means that each agent's policy update, while potentially smaller in magnitude, still pushes the joint policy in the direction of the optimal joint policy.

The reward redistribution function improves learning by reducing the variance in the gradient estimates and improves the accuracy of the advantage function approximation. This variance reduction improves the overall signal-to-noise ratio that leads to more reliable updates for each agent, which in turn ensures that the joint policy improves more consistently towards optimality, refer Appendix~\ref{sec:faulty_credit_assignment_impact}. Since the direction of the updates remains the same, the overall policy update trajectory is preserved, and the agents collectively converge to the optimal joint policy.


Thus, training a policy with the reward redistribution function is equivalent to training with the environment's original reward function, as it preserves the direction of each agent's policy gradient update. This ensures that the policy evolves similarly in both settings and that the policy update trajectory for an arbitrary initial policy is preserved.
\end{proof}

\section{Preserving Gradient Direction Under Reward Redistribution}
\label{sec:pg_update_eq_detailed}

We now show that the policy gradient update for any individual agent under our redistributed reward function shares the same \emph{direction} as it would under the environment's original reward function. This guarantee ensures that learning trajectories remain consistent, despite the introduction of denser rewards.

\begin{proposition}[Gradient Direction Preservation]
\label{prop:gradient_direction_preservation}
Let $\pi_\theta = \prod_{k=1}^N \pi_{\theta_k}$ be the joint policy in a decentralized MARL setting. Denote the reward functions by $\mathcal{R}_\zeta$ (original) and $\mathcal{R}_{\omega,\kappa}$ (redistributed) as introduced in Assumption~1 of the main text. Then, for any agent $k$, the gradient update under $\mathcal{R}_{\omega,\kappa}$ is proportional to the gradient update under $\mathcal{R}_\zeta$, ensuring the same direction of policy improvement:
\[
\nabla_{\theta_k} J_{\omega,\kappa}(\theta_k) \;=\; \delta_k \,\nabla_{\theta_k} J_{\zeta}(\theta_k),
\]
where $\delta_k \in [0,1]$ is a scalar that may scale the gradient magnitude but not its direction.
\end{proposition}

\begin{proof}
\textbf{Step 1: Expressing the Joint and Individual Rewards}

Under the redistributed reward $\mathcal{R}_{\omega,\kappa}$, the global episodic reward $r_{\text{global},\text{episodic}}(\tau)$ is decomposed as 
\[
r_{\text{global},\text{episodic}}(\tau)
\;=\; 
\sum_{i=1}^N \sum_{t=1}^{|\tau|} r_{i,t},
\]
with $r_{i,t}$ being agent $i$’s assigned reward at timestep $t$. By definition, each $r_{i,t}$ is scaled by the temporal and agent-specific weights $w^{\text{temporal}}_t$ and $w^{\text{agent}}_{i,t}$ satisfying
\[
\sum_{t=1}^{|\tau|} w^{\text{temporal}}_t = 1 
\quad\text{and}\quad
\sum_{i=1}^N w^{\text{agent}}_{i,t} = 1
\quad 
\forall\,t.
\]

\textbf{Step 2: Gradient Update Under the Redistributed Reward}

Let $J_{\omega,\kappa}(\theta_k) = \mathbb{E}_{\pi_{\theta_k}}[r_{\text{global},\text{episodic}}(\tau)]$ be the expected return for agent $k$ under $\mathcal{R}_{\omega,\kappa}$. Then:
\[
\nabla_{\theta_k} J_{\omega,\kappa}(\theta_k)
\;=\;
\nabla_{\theta_k} \,\mathbb{E}_{\pi_{\theta_k}}
\Bigl[
\sum_{i=1}^N \sum_{t=1}^{|\tau|}
r_{i,t}
\Bigr].
\]
Since $r_{i,t} = r_{k,t}$ when $i=k$ and $r_{i,t}$ otherwise, we split the sum into
\[
\sum_{t=1}^{|\tau|} r_{k,t} 
\;+\;
\sum_{i\neq k}\sum_{t=1}^{|\tau|} r_{i,t}.
\]
Thus,
\begin{align*}
\nabla_{\theta_k} J_{\omega,\kappa}(\theta_k)
&=\;
\nabla_{\theta_k}\,\mathbb{E}_{\pi_{\theta_k}}
\Bigl[\sum_{t=1}^{|\tau|} r_{k,t}\Bigr]
\;+\;
\nabla_{\theta_k}\,\mathbb{E}_{\pi_{\theta_k}}
\Bigl[\sum_{i\neq k}\sum_{t=1}^{|\tau|} r_{i,t}\Bigr].
\end{align*}

Using the normalization properties of $w^{\text{temporal}}_t$ and $w^{\text{agent}}_{i,t}$ (Assumption~1), we rewrite the second term in terms of $r_{\text{global},\text{episodic}}(\tau)$ and define $M_t = w^{\text{temporal}}_t (1 - w^{\text{agent}}_{k,t})$. The expression becomes:
\begin{align}
\nabla_{\theta_k} \,\mathbb{E}_{\pi_{\theta_k}}\bigl[r_{\text{global},\text{episodic}}(\tau)\bigr]
&=\;
\nabla_{\theta_k}\,\mathbb{E}_{\pi_{\theta_k}}
\Bigl[\sum_{t=1}^{|\tau|} r_{k,t}\Bigr]
\;+\;
\nabla_{\theta_k}\,\mathbb{E}_{\pi_{\theta_k}}
\Bigl[\Bigl(\sum_{t=1}^{|\tau|} M_t\Bigr) r_{\text{global},\text{episodic}}(\tau)\Bigr].
\nonumber
\end{align}
Collecting terms gives:
\[
\nabla_{\theta_k}\,\mathbb{E}_{\pi_{\theta_k}}
\Bigl[\bigl(1 - \sum_{t=1}^{|\tau|} M_t\bigr)r_{\text{global},\text{episodic}}(\tau)\Bigr]
\;=\;
\nabla_{\theta_k}\,\mathbb{E}_{\pi_{\theta_k}}
\Bigl[\sum_{t=1}^{|\tau|} r_{k,t}\Bigr].
\]
Defining $\delta(\tau) = 1 - \sum_{t=1}^{|\tau|} M_t$ where
\[
M_t = w^{\text{temporal}}_t \cdot \Bigl(1 - w^{\text{agent}}_{k,t}\Bigr).
\]

We rely on the following properties of the weighting functions:
Firstly, for each \(t\), \(0 \leq w^{\text{temporal}}_t \leq 1\) and \(0 \leq w^{\text{agent}}_{k,t} \leq 1\).
Secondly, the agent-specific weights satisfy \( \sum_{i=1}^N w^{\text{agent}}_{i,t} = 1 \), which implies \( 0 \leq 1 - w^{\text{agent}}_{k,t} \leq 1 \).
Finally, the temporal weights sum to one: \( \sum_{t=1}^{|\tau|} w^{\text{temporal}}_t = 1 \).

Given these:
\begin{enumerate}
    \item Since \(0 \leq 1 - w^{\text{agent}}_{k,t} \leq 1\) and \(w^{\text{temporal}}_t \geq 0\), it follows that
   \[
   0 \leq M_t = w^{\text{temporal}}_t \cdot \Bigl(1 - w^{\text{agent}}_{k,t}\Bigr) \leq w^{\text{temporal}}_t.
   \]
   \item Summing over all \(t\),
   \[
   0 \leq \sum_{t=1}^{|\tau|} M_t \leq \sum_{t=1}^{|\tau|} w^{\text{temporal}}_t = 1.
   \]
   \item Therefore,
   \[
   0 \leq \sum_{t=1}^{|\tau|} M_t \leq 1.
   \]
   \item Substituting this range into the definition of \( \delta(\tau) \),
   \[
   \delta(\tau) = 1 - \sum_{t=1}^{|\tau|} M_t,
   \]
   implies
   \[
   1 - 1 \leq \delta(\tau) \leq 1 - 0,
   \]
   which simplifies to
   \[
   0 \leq \delta(\tau) \leq 1.
   \]
\end{enumerate}

Thus, \(\delta(\tau)\) is guaranteed to lie within the interval \([0,1]\).

Therefore,
\[
\nabla_{\theta_k}\,\mathbb{E}_{\pi_{\theta_k}}
\bigl[\delta(\tau)\,r_{\text{global},\text{episodic}}(\tau)\bigr]
\;=\;
\nabla_{\theta_k}\,\mathbb{E}_{\pi_{\theta_k}}
\bigl[\sum_{t=1}^{|\tau|} r_{k,t}\bigr].
\]

implies $\delta(\tau)$ scales the original reward but does not change its sign, indicating equivalence in \emph{direction} (though not necessarily magnitude).

\textbf{Step 3: Equivalence of Gradient Directions}

Consider the policy gradient for agent $k$ under $\mathcal{R}_{\zeta}$:
\[
\nabla_{\theta_k} J_{\zeta}(\theta_k) 
\;=\;
\mathbb{E}_{\pi_{\theta_k}}
\Bigl[\nabla_{\theta_k}\log\pi_{\theta_k}(a_k\mid h_k)\,A_k^{\zeta}(h_k,a_k)\Bigr],
\]
and under $\mathcal{R}_{\omega,\kappa}$:
\[
\nabla_{\theta_k} J_{\omega,\kappa}(\theta_k) 
\;=\;
\mathbb{E}_{\pi_{\theta_k}}
\Bigl[\nabla_{\theta_k}\log\pi_{\theta_k}(a_k\mid h_k)\,A_k^{\omega,\kappa}(h_k,a_k,h_{|\tau|},a_{|\tau|})\Bigr].
\]
From the above argument, each advantage $A_k^{\omega,\kappa}$ is effectively a scaled version of $A_k^{\zeta}$ (or has the same sign for every $(h_k,a_k)$). Thus there exists a $\delta_k \in [0,1]$ such that
\[
\nabla_{\theta_k} J_{\omega,\kappa}(\theta_k)
\;=\;
\delta_k\,\nabla_{\theta_k} J_{\zeta}(\theta_k).
\]
Summing over all agents $k$ implies the \emph{joint} policy gradient under $\mathcal{R}_{\omega,\kappa}$ is a scaled version of that under $\mathcal{R}_{\zeta}$, preserving the direction of policy improvement across agents.

\textbf{Step 4: Implications for Learning Trajectories}

Since scaling a gradient vector by a positive factor does not affect its direction, each agent’s update drives policy parameters toward the same attractors as in the original reward setting. Hence, the learning trajectory is preserved, ensuring that:
\begin{itemize}
    \item No spurious local optima are introduced by the redistribution.
    \item Convergence to optimal joint policies remains unchanged.
    \item Variance reductions from denser rewards improve stability without biasing the solution.
\end{itemize}

Therefore, training under the redistributed reward function \(\mathcal{R}_{\omega,\kappa}\) is \emph{directionally equivalent} to training under \(\mathcal{R}_{\zeta}\), ensuring that agents converge to the same optimal policies, only faster and more stably.
\end{proof}

\paragraph{Implications for Joint Policy Updates and Convergence.}  
While Proposition~\ref{prop:gradient_direction_preservation} guarantees that each individual agent's gradient under the redistributed rewards is aligned with its counterpart under the original rewards (up to a non-negative scaling factor), the aggregate joint policy gradient—being the sum of these scaled vectors—may not be perfectly parallel to the original joint gradient. However, because the scaling factors \( \delta_k \) are non-negative and each agent's update direction is preserved, no agent receives a misleading gradient signal. This alignment reduces the variance of gradient estimates by providing more immediate, fine-grained feedback, which typically leads to more stable updates. Although potential-based reward shaping does not universally guarantee faster convergence from a purely theoretical standpoint, our empirical results (see Section~X) demonstrate that TAR\(^{2}\) often accelerates learning and improves sample efficiency in practice, likely due to the reduced variance and more informative credit assignment. 

\section{Reward Modeling Details}
\label{sec:reward_modeling_details}

Below is an expanded discussion for each of the three architectural design choices.

\textbf{Final State Conditioning}

When agents are not conditioned on the final outcome of the episode, each intermediate state or action is evaluated against an \emph{average} future return (i.e., the expected reward given only the current local context). Such a perspective fails to distinguish sequences of actions that may appear similar locally but differ substantially in terms of eventual success or failure. For instance, two nearly identical trajectories might diverge in a crucial final step—one results in a high payoff (e.g., defeating an opponent, scoring a goal), while the other yields no payoff.  

By explicitly incorporating the \emph{final} state and action into the reward prediction, we “tell” the model how the trajectory actually ended. This makes each intermediate state-action pair consequential on the actual \emph{final} outcome, rather than some local or average guess. In essence:

\begin{enumerate}
    \item \emph{Outcome Awareness}: Agents learn that an action \emph{now} might be pivotal in leading to a successful (or failed) final outcome.  
    \item \emph{Reduced Ambiguity}: Instead of returning an average future reward for a partially observed path, the model learns a more grounded mapping, since it knows exactly whether the trajectory eventually succeeded or failed.  
    \item \emph{Sharper Credit Signals}: Conditioning on the final outcome helps separate important from unimportant steps (and which agent is responsible), making credit assignment clearer and potentially speeding up learning.
\end{enumerate}

In single-agent RL, similar outcome- or goal-conditioning methods have shown improvements in credit assignment under delayed rewards \citep{harutyunyan2019hindsight, ren2021learning}. By extending this approach to multi-agent settings, we ensure that the reward redistribution mechanism can focus on how each agent's intermediate action contributed to the eventual outcome.

\textbf{Inverse Dynamics Modeling}

An inverse dynamics model predicts \(a_{i,t}\) given the embeddings of \((h_{i,t}, h_{i,t+1})\) or some representation of the consecutive states. Including such a module has two benefits:

\begin{enumerate}
    \item \emph{Causality in Latent Space}: If the network can accurately predict which action was taken to go from one latent representation to another, it necessarily encodes features that distinguish different agent behaviors. This encourages the latent space to reflect transitions that are \emph{functionally} relevant, rather than arbitrary correlations. In turn, reward predictions become grounded in actual causal relationships between states, actions, and outcomes.
   \item \emph{Stabilized Credit Assignment}: By having to reconstruct or identify each agent’s action, the network implicitly learns a more structured notion of time, ordering, and agent identity. This can reduce confusion about “who did what” when multiple agents act in parallel.  
\end{enumerate}

In single-agent RL, inverse dynamics objectives have been used to improve state representations, encourage meaningful features, and stabilize training \citep{pathak2017curiosity, agrawal2015learning}. While these works focus primarily on single-agent or self-supervised exploration, the central insight remains: inverse dynamics tasks force the model to learn more discriminative, causally grounded embeddings.  
Although less common in multi-agent literature, these principles still apply—each agent’s identity and action path is clearer when the model learns to predict actions from state embeddings. This clarity can, in turn, facilitate more coherent reward redistribution.

\textbf{Probabilistic Reward Redistribution}

Without explicit normalization, the model may produce reward predictions \(\{\hat{r}_{i,t}\}\) that do not sum to the true global return \(r_{\text{global}, \text{episodic}}\). This can introduce inconsistencies and may violate the assumptions of potential-based reward shaping (where we require the sum of shaped rewards to match the original environmental return). Normalizing across time and agents solves this mismatch in multiple ways:

\begin{enumerate}
    \item \emph{Ensuring Consistency with the Environment}: By forcing \(\sum_{i=1}^N \sum_{t=1}^{|\tau|} r_{i,t} = r_{\text{global}, \text{episodic}}\), we guarantee that each agent’s shaped reward remains faithful to the actual outcome, maintaining optimal policy invariance.

    \item \emph{Controlling Scale and Sign}:Rewards exceeding the actual global return or dropping below zero may lead to distorted policy updates. Normalization keeps each predicted reward in a sensible range, often \([0, 1]\) when scaled by the total return. Ensuring nonnegative rewards can simplify policy learning and avoid contradictory signals (where some agent “loses” reward that doesn’t exist in the environment).

    \item \emph{Variance Reduction and Interpretability}: By bounding each per-agent reward, the magnitude of the gradients may be more stable, reducing learning variance. Probability-like distributions also yield an intuitive interpretation: how the global reward is “allocated” among agents and time steps.

    \item \emph{Preventing Drift in Unconstrained Predictions}: In unconstrained models (e.g., a simple regressor for each time step), predictions can drift or accumulate error. This normalization step ensures that even if the model’s unconstrained outputs \(\{\hat{r}_{i,t}\}\) become large or negative, the final distributed rewards remain consistent with the episode’s actual return.
\end{enumerate}

Hence, probabilistic reward redistribution enforces a strict accounting of the environment’s episodic reward, preventing the model from misallocating or “inventing” reward signals, while maintaining alignment with potential-based shaping principles.



\section{Pseudocode}
\label{sec:pseudocode}

Below is the pseudocode to train $TAR^2$ and $MAPPO$:-

\begin{algorithm}[!h]
\caption{Temporal-Agent Reward Redistribution with Multi-Agent Proximal Policy Optimization}\label{alg:psuedocode}
\begin{algorithmic}[1]
\STATE Number of agents $M$, Initialize $\theta$, the parameters for policy $\pi$, $\mu$, the parameters for state value critic $V$ and $\phi = (\omega, \kappa)$, the parameters for reward redistribution function $R$, using orthogonal initialization (Hu et al., 2020)
\STATE Set experience buffer $B \leftarrow \emptyset$; Reward redistribution model training frequency $\epsilon$
\STATE Set learning rate $\alpha_{\pi}$, $\alpha_{Q}$, $\alpha_{R}$ for AdamW optimizer
\WHILE{step $\leq$ $step_{\text{max}}$}
    \STATE set data buffer $D = \{\}$
    \FOR{$i = 1$ to \textit{batch\_size}}
        \STATE $\tau = []$ -- empty list
        \STATE initialize $h^{(1)}_{0,\pi}, \ldots, h^{(M)}_{0,\pi}$ actor RNN states
        \STATE initialize $h^{(1)}_{0,V}, \ldots, h^{(M)}_{0,V}$ state value RNN states
        \FOR{$t = 1$ to $T$}
            \FOR{all agents $a$}
                \STATE $u^{(a)}_t, h^{(a)}_{t,\pi} = \pi(o^{(a)}_t, h^{(a)}_{t-1,\pi}; \theta)$
            \ENDFOR
            \STATE $(v^{(1)}_t, \dots v^{(M)}_t), (h^{(1)}_{t,V} \dots h^{(M)}_{t,V}) = V(s^{(1)}_t \dots s^{(M)}_t, u^{(1)}_t \dots u^{(M)}_t, h^{(1)}_{t-1,V} \dots h^{(M)}_{t-1,V}; \mu)$  -- we mask out the actions of agent \textit{a} while calculating its state value $v^{(a)}$
            \STATE Execute actions $u_t$, observe $r_t$, $s_{t+1}$, $o_{t+1}$
            \STATE $\tau \mathrel{+}= [s_t, o_t, h_{t,\pi}, h_{t,V}, u_t, r_t, s_{t+1}, o_{t+1}]$
        \ENDFOR
        \STATE Collect episodic return $r_{\text{global}, \text{episodic}}(\tau)$ and the trajectory $\tau$, store $(r_{\text{global}, \text{episodic}}(\tau), \tau)$ in the buffer $B$
        \STATE Compute the redistributed reward for trajectory $\tau$ by normalizing $R(i,t; \phi)$ across $i$ and $t$ axes to generate $w^{agent}_{i,t}$ and $w^{temporal}_t$ and then multiply the normalized weights with the actual episodic reward $r_{\text{global}, \text{episodic}}$ 
        \STATE Compute return $G_{i}$ for each agent $i=1,...,M$ using $R(i,t; \omega, \kappa)$, to learn the $V$ function on $\tau$ and normalize with PopArt
        \STATE Compute advantage estimate and target values $\hat{A}^{1} ,..., \hat{A}^{M}$ and $\hat{V}^{1} ,..., \hat{V}^{M}$ via GAE using state value estimates on $\tau$, after PopArt denormalization
        \STATE Split trajectory $\tau$ into chunks of length $L$
        \FOR{$l = 0, 1, \ldots, T//L$}
            \STATE $D = D \cup (\tau[l : l + T], \hat{A}[l : l + L], G[l : l + L], \bar{G}[l : l + L])$
        \ENDFOR
    \ENDFOR
    \FOR{mini-batch $k = 1, \ldots, K$}
        \STATE $b \leftarrow$ random mini-batch from $D$ with all agent data
        \FOR{each data chunk $c$ in the mini-batch $b$}
            \STATE update RNN hidden states for $\pi$, $Q$ and $V$ from first hidden state in data chunk
        \ENDFOR
    \ENDFOR
    \STATE Adam update $\theta$ on $L(\theta)$ with data $b$
    \STATE Adam update $\mu$ on $L(mu)$ with data $b$
    \IF{step \% $\epsilon$ is 0 then}
        \FOR{$i = 1$ to \textit{num\_updates}}
            \STATE $\tau \leftarrow$ randomly sample trajectories from $B$
            \STATE Compute reward redistribution loss using Eq 2 in Section 4.4 of the main paper
            % Eq~\ref{eq:reward_redistribution_training_objective}
            \STATE Adam update $(\omega, \kappa)$ on $L(\omega, \kappa)$ with loss calculated using $\tau$
        \ENDFOR
    \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\section{Interpretability and Insights}
\label{sec:interpretability}

Although our primary focus is on performance and theoretical properties, TAR\(^2\)’s per-timestep, per-agent reward predictions lend themselves to partial interpretability:
\begin{itemize}
    \item Agents’ importance at specific timesteps can be visualized by examining \(w^{\text{temporal}}_t w^{\text{agent}}_{i,t}\).
    \item Comparing predicted reward distributions across different episodes can hint at consistent agent roles or strategic pivot points in the trajectory.
\end{itemize}
However, direct interpretability is challenging in high-dimensional multi-agent environments like SMACLite and Google Football, where the intricate interactions and vast state-action spaces complicate simple visualizations. Additionally, developing a systematic interpretability study would require significant additional methodologies and resources, extending beyond the scope of the current work. While we recognize the importance of interpretability and plan to explore it in future research, our current focus remains on establishing robust performance improvements and theoretical guarantees for reward redistribution.

\section{Detail Task Descriptions}
\label{sec:detailed_task_description}
% \textbf{Lightweight StarCraft (SMAClite)}: SMAClite is a simplified version of the StarCraft II game engine, designed to be computationally less expensive than the full SC II engine. It provides a user-friendly, Python-based framework for creating custom environments and modifying environment logic. The observation space includes the relative positions, unit types, health, and shield strength of both the agent's allies and enemies within its field of view, as well as the agent's own health and shield status. Agents can move in any of the four cardinal directions, stop, perform no operation, or attack any enemy agent within their field of view. The environment includes action masks that highlight valid actions based on the agent's current state. If an agent dies, the default action becomes `no-op`. Each combat scenario lasts for 100 timesteps.

% \paragraph{SMAClite \citep{michalski2023smaclite}} 
% A computationally efficient variant of StarCraft II \citep{samvelyan2019starcraft}, We experiment on SMAClite's three battle scenarios with varying levels of complexity: 
% \emph{(i)} 5m\_vs\_6m, 
% \emph{(ii)} 10m\_vs\_11m, 
% \emph{(iii)} 3s5z. 
% Each agent’s local observation includes relative positions and health information of nearby allies and enemies, and each episode runs for up to 100 timesteps (agents can be eliminated sooner). The environment’s reward function combines partial rewards for damaging or eliminating enemies, with a maximum possible team return normalized to 20.


\paragraph{SMAClite \citep{michalski2023smaclite}} 
A computationally efficient variant of StarCraft II \citep{samvelyan2019starcraft}, We experiment on SMAClite's three battle scenarios with varying levels of complexity: 
\emph{(i)} 5m\_vs\_6m, 
\emph{(ii)} 10m\_vs\_11m, 
\emph{(iii)} 3s5z. 
Each agent’s local observation includes the relative positions, unit types, health, and shield strength of both the agent's allies and enemies within its field of view, as well as the agent's own health and shield status. Agents can move in any of the four cardinal directions, stop, perform no operation, or attack any enemy agent within their field of view. The environment includes action masks that highlight valid actions based on the agent's current state. If an agent dies, the default action becomes `no-op`. Each combat scenario lasts for 100 timesteps (agents can be eliminated sooner). The environment’s reward function combines partial rewards for damaging or eliminating enemies, with a maximum possible team return normalized to 20. Link to repository: \texttt{https://github.com/uoe-agents/smaclite} (MIT License)

% \textbf{Google Research Football (GRF)}: Google Research Football is a high-fidelity multi-agent environment designed to simulate football (soccer) matches. It offers a physics-based 3D simulation where agents control individual players on a football team, aiming to collaborate and score goals against an opposing team. GRF provides a comprehensive observation space, including features like player positions, ball coordinates, velocity vectors, stamina levels, and proximity to opponents and teammates. The environment's action space includes various football maneuvers such as passing, shooting, dribbling, and tackling, as well as movement in the four cardinal directions and the option to sprint or remain stationary. The input representation type used is `simple115v2', which encodes critical information about player and ball positioning. Each episode is designed to end when a goal is scored or when a predefined number of timesteps, 100, is reached. GRF environments include various game scenarios like `academy\_3\_vs\_1\_with\_keeper', `academy\_counterattack\_easy', and `academy\_pass\_and\_shoot\_with\_keeper', each offering different challenges for coordination and team play. Reward signals (`scoring,checkpoints') are sparse and tied to events such as goals scored or distance traveled to opponent team's goal post, with an emphasis on long-term planning. The repository for GRF is available at: \texttt{https://github.com/google-research/football} (Apache License 2.0).


% \paragraph{Google Research Football (GRF) \citep{kurach2020google}}
% A high-fidelity multi-agent environment simulating football (soccer), evaluated on:
% \emph{(i)} academy 3 vs 1 with keeper,
% \emph{(ii)} academy counterattack easy,
% \emph{(iii)} academy pass and shoot with keeper.
% Using a simple115v2 observation format (positions, ball movements, etc.), agents receive sparse rewards primarily for goals or reaching strategic checkpoints. Each episode ends when a goal is scored or after a fixed horizon.

\paragraph{Google Research Football (GRF) \citep{kurach2020google}}
A high-fidelity multi-agent environment simulating football (soccer), evaluated on:
\emph{(i)} academy 3 vs 1 with keeper,
\emph{(ii)} academy counterattack easy,
\emph{(iii)} academy pass and shoot with keeper.
GRF provides a comprehensive observation space, including features like player positions, ball coordinates, velocity vectors, stamina levels, and proximity to opponents and teammates. The environment's action space includes various football maneuvers such as passing, shooting, dribbling, and tackling, as well as movement in the four cardinal directions and the option to sprint or remain stationary. The input representation type used is `simple115v2', which encodes critical information about player and ball positioning. Each episode is designed to end when a goal is scored or when a predefined number of timesteps, 100, is reached. GRF environments include various game scenarios like `academy\_3\_vs\_1\_with\_keeper', `academy\_counterattack\_easy', and `academy\_pass\_and\_shoot\_with\_keeper', each offering different challenges for coordination and team play. Reward signals (`scoring,checkpoints') are sparse and tied to events such as goals scored or distance traveled to opponent team's goal post, with an emphasis on long-term planning. The repository for GRF is available at: \texttt{https://github.com/google-research/football} (Apache License 2.0).

\section{Implementation Details and Hyperparameters}
\label{sec:hyperparameters}

The code was run on Lambda Labs deep learning workstation with 2-4 Nvidia RTX 2080 Ti graphics cards.  Each training run was run on one single GPU, and required approximately 8 hours. 

 Hyperparameters used for TAR$^2$, STAS, AREL-Temporal, AREL-Agent-Temporal, Uniform and various environment reward configurations that are common to all tasks are shown in Tables~\ref{tab:mappo_common_hyperparams}. The task-specific hyperparameters considered in our grid search for TAR$^2$, STAS, AREL-variants in Tables \ref{table:tar^2_hyperparameter_sweep}, \ref{table:stas_hyperparameter_sweep} and \ref{table:arel_hyperparameter_sweep} respectively. Bold values indicate the optimal hyperparameters.

\begin{table}[H]
\centering
\caption{Common Hyperparameters for MAPPO algorithms, including PRD variants.}
\begin{tabular}{|c|c|}
\hline
\textbf{\begin{tabular}[c]{@{}c@{}}common\\ hyperparameters\end{tabular}} & \textbf{value} \\ \hline
ppo\_epochs         & 15 \\   \hline
ppo\_batch\_size      & 30 \\   \hline
gamma               & 0.99 \\ \hline
max\_episodes       & 30000 \\ \hline
max\_time\_steps    & 100 \\  \hline
rnn\_num\_layers\_v    & 1 \\  \hline
rnn\_hidden\_v      & 64  \\  \hline
v\_value\_lr          & 5e-4  \\  \hline
v\_weight\_decay       & 0.0 \\ \hline
v\_hidden\_shape    & 64  \\  \hline
grad\_clip\_critic\_v  & 0.5  \\  \hline
value\_clip         & 0.2  \\  \hline
data\_chunk\_length & 10  \\  \hline
rnn\_num\_layers\_actor    & 1 \\  \hline
rnn\_hidden\_actor      & 64  \\  \hline
policy\_lr          & 5e-4  \\  \hline
policy\_weight\_decay       & 0.0 \\ \hline
policy\_hidden\_shape    & 64  \\  \hline
grad\_clip\_actor  & 0.5  \\  \hline
policy\_clip         & 0.2  \\  \hline
entropy\_pen       & 1e-2 \\  \hline
gae\_lambda         & 0.95 \\ \hline
\end{tabular}

\label{tab:mappo_common_hyperparams}
\end{table}


\begin{table}[!h]
\centering
\caption{TAR$^2$ hyperparameters.}
\label{table:tar^2_hyperparameter_sweep}
\footnotesize  % Smaller font size for table content
\begin{tabularx}{\textwidth}{|*{14}{>{\centering\arraybackslash}X|}}
\hline
\textbf{Env. Name} & \textbf{num heads} & \textbf{depth} & \textbf{dropout} & \textbf{comp. dim} & \textbf{batch size} & \textbf{lr} & \textbf{weight decay} & \textbf{inv. dyn. loss coef.} & \textbf{grad clip val.} & \textbf{model upd. freq.} & \textbf{model upd. epochs} & \textbf{policy lr} & \textbf{entropy coef} \\
\hline
Google Football & [3, \textbf{4}] & [\textbf{3}, 4] & [\textbf{0.0}, 0.1, 0.2] & [16, \textbf{64}, 128] & [32, 64, \textbf{128}] & [1e-4, \textbf{5e-4}, 1e-3] & [\textbf{0.0}, 1e-5, 1e-4] & [1e-3, 1e-2, \textbf{5e-2}] & [0.5, 5.0, \textbf{10.0}] & [50, 100, \textbf{200}] & [100, \textbf{200}, 400] & [5e-4, \textbf{1e-3}] & [5e-3, 8e-3, \textbf{1e-2}] \\
\hline
SMACLite & [3, \textbf{4}] & [\textbf{3}, 4] & [\textbf{0.0}, 0.1, 0.2] & [16, \textbf{64}, 128] & [32, 64, \textbf{128}] & [1e-4, \textbf{5e-4}, 1e-3] & [\textbf{0.0}, 1e-5, 1e-4] & [1e-3, 1e-2, \textbf{5e-2}] & [0.5, 5.0, \textbf{10.0}] & [50, 100, \textbf{200}] & [100, \textbf{200}, 400] & [5e-4, \textbf{1e-3}] & [\textbf{5e-3}, 8e-3, 1e-2] \\
\hline
\end{tabularx}
\end{table}


\begin{table}[!h]
\centering
\caption{STAS hyperparameters.}
\label{table:stas_hyperparameter_sweep}
\footnotesize  % Smaller font size for table content
\begin{tabularx}{\textwidth}{|*{14}{>{\centering\arraybackslash}X|}}
\hline
\textbf{Env. Name} & \textbf{num heads} & \textbf{depth} & \textbf{dropout} & \textbf{comp. dim} & \textbf{batch size} & \textbf{lr} & \textbf{weight decay} & \textbf{grad clip val.} & \textbf{model upd. freq.} & \textbf{model upd. epochs} \\
\hline
Google Football & [3, \textbf{4}] & [\textbf{3}, 4] & [\textbf{0.0}, 0.1, 0.2] & [16, \textbf{64}, 128] & [32, 64, \textbf{128}] & [1e-4, \textbf{5e-4}, 1e-3] & [\textbf{0.0}, 1e-5, 1e-4] & [0.5, 5.0, \textbf{10.0}] & [50, 100, \textbf{200}] & [100, \textbf{200}, 400] \\
\hline
SMACLite & [3, \textbf{4}] & [\textbf{3}, 4] & [\textbf{0.0}, 0.1, 0.2] & [16, \textbf{64}, 128] & [32, 64, \textbf{128}] & [1e-4, \textbf{5e-4}, 1e-3] & [\textbf{0.0}, 1e-5, 1e-4] & [0.5, 5.0, \textbf{10.0}] & [50, 100, \textbf{200}] & [100, \textbf{200}, 400] \\
\hline
\end{tabularx}
\end{table}


\begin{table}[!h]
\centering
\caption{AREL hyperparameters.}
\label{table:arel_hyperparameter_sweep}
\footnotesize  % Smaller font size for table content
\begin{tabularx}{\textwidth}{|*{14}{>{\centering\arraybackslash}X|}}
\hline
\textbf{Env. Name} & \textbf{num heads} & \textbf{depth} & \textbf{dropout} & \textbf{comp. dim} & \textbf{batch size} & \textbf{lr} & \textbf{weight decay} & \textbf{grad clip val.} & \textbf{model upd. freq.} & \textbf{model upd. epochs} \\
\hline
Google Football & [3, \textbf{4}] & [\textbf{3}, 4] & [\textbf{0.0}, 0.1, 0.2] & [16, \textbf{64}, 128] & [32, 64, \textbf{128}] & [1e-4, \textbf{5e-4}, 1e-3] & [\textbf{0.0}, 1e-5, 1e-4] & [0.5, 5.0, \textbf{10.0}] & [50, 100, \textbf{200}] & [100, \textbf{200}, 400] \\
\hline
SMACLite & [3, \textbf{4}] & [\textbf{3}, 4] & [\textbf{0.0}, 0.1, 0.2] & [16, \textbf{64}, 128] & [32, 64, \textbf{128}] & [1e-4, \textbf{5e-4}, 1e-3] & [\textbf{0.0}, 1e-5, 1e-4] & [0.5, 5.0, \textbf{10.0}] & [50, 100, \textbf{200}] & [100, \textbf{200}, 400] \\
\hline
\end{tabularx}
\end{table}



% \bibliographystyle{plain} % We choose the "plain" reference style
% \bibliography{refs}
% \bibliographystyle{named}
% \bibliography{ijcai25}

% \end{document}