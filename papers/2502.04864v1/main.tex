%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}


% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage{natbib}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{tabularx}

% \usepackage{xr-hyper} % Import labels from an external document
% \externaldocument{FormattingGuidelines-IJCAI-25/supplementary_ijcai_2025}

\newtheorem{proposition}{Proposition}
% \newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}

% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{TAR$^2$: Temporal-Agent Reward Redistribution for Optimal Policy Preservation in Multi-Agent Reinforcement Learning}


% Single author syntax
% \author{
%     % Author Name
%     Anonymous
%     \affiliations
%     % Affiliation
%     IJCAI 2025 Main Track 
%     \emails
%     % email@example.com
%     Submission Number: 6877
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Aditya Kapoor$^1$
\and
Kale-ab Tessera$^2$\and
Mayank Baranwal$^3$\And
Harshad Khadilkar$^3$\\
Stefano Albrecht$^2$\\
Mingfei Sun$^1$
\affiliations
$^1$University of Manchester\\
$^2$University of Edinburgh\\
$^3$Indian Institute of Technology, Bombay\\
% $^4$Fourth Affiliation\\
% \emails
% \{first, second\}@example.com,
% third@other.example.com,
% fourth@example.com
% aditya.kapoor@postgrad.manchester.ac.uk
}
% \fi

\begin{document}

\maketitle

\begin{abstract}
In cooperative multi-agent reinforcement learning (MARL), learning effective policies is challenging when global rewards are sparse and delayed. This difficulty arises from the need to assign credit across both agents and time steps, a problem that existing methods often fail to address in episodic, long-horizon tasks. We propose \emph{Temporal-Agent Reward Redistribution} (\(\text{TAR}^2\)), a novel approach that decomposes sparse global rewards into agent-specific, time-step-specific components, thereby providing more frequent and accurate feedback for policy learning. Theoretically, we show that \(\text{TAR}^2\) (i) aligns with potential-based reward shaping, preserving the same optimal policies as the original environment; and (ii) maintains policy gradient update directions identical to those under the original sparse reward, ensuring unbiased credit signals. Empirical results on two challenging benchmarks—SMACLite and Google Research Football—demonstrate that \(\text{TAR}^2\) significantly stabilizes and accelerates convergence, outperforming strong baselines like AREL and STAS in both learning speed and final performance. These findings establish \(\text{TAR}^2\) as a principled and practical solution for agent-temporal credit assignment in sparse-reward multi-agent systems.\footnote{Codebase: \url{https://github.com/AdityaKapoor74/MARL_Agent_Temporal_Credit_Assignment}}§
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Multi-agent reinforcement learning (MARL) enables autonomous agents to cooperate on complex tasks across a wide range of domains, including warehouse logistics \citep{krnjaic2022scalable}, e-commerce \citep{shelke2023multi, baer2019}, robotics \citep{sartoretti2019primal, damani2021primal}, and routing \citep{zhang2018fully, vinitsky2020optimizing, zhang2023learning}. MARL has also shown great promise in high-stakes coordination challenges in video games such as StarCraft II \citep{Vinyals2019GrandmasterLI}, DOTA \citep{berner2019dota}, and Google Football \citep{kurach2020google}, where teams of agents must align actions toward a shared goal.

However, one of the most persistent bottlenecks in cooperative MARL is \emph{credit assignment}: deciding how to allocate a global team reward among multiple agents in partial observability and decentralized execution. This bottleneck is particularly acute when rewards are \emph{sparse or delayed}, which causes agents to struggle in correlating their intermediate actions with eventual outcomes \citep{arjona2019rudder, ren2021learning}. In such settings, there is a need to determine \emph{when} along a multi-step trajectory the helpful actions occurred (\emph{temporal credit assignment}) \emph{and} \emph{which} agent or subset of agents were responsible (\emph{agent credit assignment}). Prior works often focus on only one of these aspects at a time—e.g., factorizing value functions to identify agent-specific credits \citep{sunehag2017value, rashid2020monotonic} or building dense temporal proxies to mitigate delayed rewards \citep{arjona2019rudder, xiao2022agent}. Yet, managing both agent and temporal credit assignment \emph{simultaneously} remains challenging, especially in high-dimensional tasks where partial observability further complicates the learning problem \citep{Papoudakis2020BenchmarkingMD}.

\begin{figure*}[!h]
  \centering
  \includegraphics[width=\textwidth]{figures/TAR2_Architecture_v2.png}
  \caption{\textbf{Overview of \(\text{TAR}^2\).} Our dual-attention mechanism uses a \emph{temporal causal attention} module and an \emph{agent Shapley attention} module to decompose rewards by time steps and agents. An inverse dynamics model further refines the temporal representation, and the final output yields per-agent, per-timestep redistributed rewards that enables effective reward attribution.}
  \label{fig:TAR^2_architecture}
\end{figure*}

In this paper, we propose \emph{Temporal-Agent Reward Redistribution} (\(\text{TAR}^2\)), a novel framework that jointly addresses \emph{agent} and \emph{temporal} credit assignment in cooperative MARL with sparse global rewards. Rather than relying solely on value-function factorization or temporal heuristics, \(\text{TAR}^2\) \textbf{redistributes} the final episodic reward across each time step and each agent, guided by a learned model (Figure~\ref{fig:TAR^2_architecture}). Concretely, TAR\(^2\) maps sparse team returns into time-step-specific feedback signals and then allocates these among agents according to each agent’s (marginal) contribution, approximated via a dual-attention mechanism. Crucially, our approach is built upon \emph{potential-based reward shaping}~\citep{ng1999policy, Devlin2011TheoreticalCO}, ensuring that the optimal policies of the original Markov Decision Process (MDP) are preserved in our reshaped rewards. This preserves \emph{optimal policy invariance}, while providing finer-grained credit signals to accelerate learning.

We validate \(\text{TAR}^2\) both through \emph{theoretical guarantees} and \emph{empirical results on challenging benchmarks}.

% \begin{enumerate}
%     \item \textbf{Theoretical Analysis } We prove that \(\text{TAR}^2\) is equivalent to a potential-based shaping scheme, guaranteeing that the reward redistribution does not alter the set of optimal policies. Additionally, we show that policy gradient updates under \(\text{TAR}^2\) maintain the same \emph{direction} as those under the original reward, ensuring an unbiased gradient trajectory.

%     \item \textbf{Empirical Evaluation} We benchmark \(\text{TAR}^2\) on both \emph{SMACLite}~\citep{michalski2023smaclite} and \emph{Google Research Football}~\citep{kurach2020google}—two environments known for their challenging partial observability, large action spaces, and sparse returns. \(\text{TAR}^2\) consistently outperforms state-of-the-art baselines such as AREL \citep{xiao2022agent} and STAS \citep{Chen2023STASSR}, showing faster convergence and superior final performance.
% \end{enumerate}

\textbf{Contributions.} Summarized below are our key contributions:

\begin{itemize}
    \item \textbf{Unified Agent-Temporal Credit Assignment.} We develop \(\text{TAR}^2\), a single framework that decomposes episodic rewards both across agents \emph{and} over time, bridging the gap between factorization-based methods and purely temporal reward-shaping methods.
    \item \textbf{Policy Invariance via Potential-Based Shaping.} We formally prove that \(\text{TAR}^2\)’s redistributed reward function preserves the original environment’s optimal policies and maintains equivalent gradient directions.
    % \item \textbf{Architecture with Inverse Dynamics, Final-State Conditioning, and Shapley Attention.} We introduce a dual-attention architecture combining temporal causal attention and agent Shapley attention, augmented by an inverse dynamics model. Additionally, our approach conditions reward redistribution on the final state embedding, which links intermediate actions to eventual outcomes. This comprehensive design enhances agent-temporal credit assignment, particularly in long-horizon tasks with sparse rewards.
    To effectively capture agent contributions and temporal dependencies, we employ a dual-attention architecture that integrates Shapley attention for fair credit assignment, an inverse dynamics model to learn robust temporal representations, and final-state conditioning to link intermediate actions with final outcomes.
    \item \textbf{Robust Empirical Performance.} Through extensive experiments on challenging configurations of SMACLite and Google Research Football, \(\text{TAR}^2\) surpasses strong baselines in convergence speed and final return under sparse rewards. 
\end{itemize}

% Beyond these contributions, we briefly discuss how \(\text{TAR}^2\) can potentially be extended to incorporate other advanced techniques such as hindsight credit assignment \citep{harutyunyan2019hindsight} or to generalize across different cooperative tasks. We believe this approach opens new avenues for efficient multi-agent learning in complex, sparse-reward settings.

\section{Related Works}
\label{sec:related-works}

In this section, we review and compare various methods addressing credit assignment in both single-agent and multi-agent reinforcement learning (MARL). While single-agent methods focus primarily on temporal credit assignment, multi-agent methods must manage the additional complexity of agent-specific credit assignment—particularly under sparse or delayed rewards. Our discussion emphasizes the unique challenge of combined agent-temporal credit assignment and highlights how existing approaches differ from our proposed solution, TAR\(^{2}\).

\subsection{Temporal Credit Assignment}
\label{subsec:temporal-credit-assignment}

Temporal credit assignment aims to decompose sparse or episodic rewards into informative, time-step-specific feedback, which is critical for learning in long-horizon tasks with delayed outcomes.

Early single-agent methods like RUDDER \citep{arjona2019rudder} redistribute rewards by analyzing return contributions at each step. While effective in single-agent settings, RUDDER depends on accurate return predictions and does not extend naturally to multi-agent scenarios where individual contributions must also be identified. Sequence modeling approaches \citep{liu2019sequence, han2022off} use architectures like Transformers \citep{vaswani2017attention} to capture long-term dependencies, but they similarly focus on temporal aspects without addressing agent-level credit.

Trajectory-based methods \citep{ren2021learning, zhu2023towards} learn proxy rewards via smoothing and bi-level optimization; however, these approaches generally assume a single agent and do not account for multiple cooperating agents. Hindsight Policy Gradients (HPG) \citep{harutyunyan2019hindsight} retrospectively assign rewards using future trajectory information but are again tailored to single-agent scenarios.

In multi-agent contexts, AREL \citep{xiao2022agent} extends temporal credit assignment using attention mechanisms to redistribute rewards over time. Although AREL handles temporal dependencies among agents, it primarily focuses on the temporal domain and does not fully capture agent-specific contributions, particularly under sparse reward conditions. In contrast, our method, TAR\(^{2}\), jointly addresses both temporal and agent-specific credit assignment, which is crucial for multi-agent systems with sparse rewards.

\subsection{Agent Credit Assignment}
\label{subsec:agent-credit-assignment}

Agent credit assignment seeks to allocate portions of a global reward to individual agents based on their contributions, a key aspect of learning cooperative policies.

Difference rewards and counterfactual methods, such as COMA \citep{foerster2018counterfactual, devlin2014potential}, compute agent-specific advantages by considering counterfactual scenarios, but these approaches typically assume denser feedback and struggle when rewards are sparse or significantly delayed. Value function factorization methods like VDN \citep{sunehag2017value} and QMIX \citep{rashid2020monotonic} decompose joint value functions into agent-specific components, improving scalability and coordination in environments with frequent rewards. However, their factorization assumptions may not hold in highly sparse settings, and they often do not explicitly handle the temporal aspect of credit assignment.

Shapley value-based approaches \citep{wang2020shapley} offer fair attribution of global rewards to agents based on marginal contributions, but exact computation is intractable in large systems, and approximations can miss subtle inter-agent dynamics—especially when rewards are sparse. Attention-based critics like PRD \citep{freed2021learning, kapoor2024assigning, kapoor2024agenttemporalcreditassignmentoptimal} focus on identifying important agent interactions but still rely on assumptions about reward structure that may not hold in sparse-feedback scenarios.

TAR\(^{2}\) distinguishes itself by directly addressing the challenges of sparse, delayed rewards through joint temporal and agent-specific redistribution, bypassing the limitations of TD-based bootstrapping methods. This dual focus makes TAR\(^{2}\) more robust in sparse environments where previous agent credit assignment methods struggle.

\subsection{Combined Agent-Temporal Credit Assignment}
\label{subsec:agent-temporal-credit-assignment}

Simultaneously addressing agent and temporal credit assignment greatly increases complexity, as it requires reasoning over high-dimensional interactions across both agents and time.

Recent works like \citet{she2022agent} attempt combined credit assignment using attention-based methods, but they face scalability issues and do not offer theoretical guarantees. STAS \citep{Chen2023STASSR} specifically tackles joint agent-temporal credit assignment by employing a dual transformer structure with spatial-temporal attention mechanisms and Shapley value approximations. While STAS successfully decomposes rewards across time and agents, it lacks clear theoretical guarantees of policy optimality invariance and can be unstable in highly sparse environments.

In contrast, TAR\(^{2}\) introduces several key enhancements over STAS:
\begin{itemize}
    \item \textbf{Theoretical Guarantees:} Unlike STAS, TAR\(^{2}\) is grounded in potential-based reward shaping, ensuring that the redistributed rewards preserve the optimal policy of the original environment.
    \item \textbf{Enhanced Architecture:} TAR\(^{2}\) extends the dual-attention approach by incorporating an inverse dynamics model and conditioning rewards on final state embeddings. These additions improve the accuracy of temporal and agent-specific credit assignment, particularly in long-horizon tasks with sparse rewards.
    \item \textbf{Stability in Sparse Environments:} By normalizing rewards and leveraging theoretical insights, TAR\(^{2}\) exhibits greater stability and improved performance compared to prior methods like STAS and AREL in sparse-reward settings.
\end{itemize}

While other methods attempt similar decompositions, TAR\(^{2}\)’s combination of theoretical guarantees, architectural innovations, and focus on sparse rewards sets it apart as a novel and robust solution for joint agent-temporal credit assignment.

\section{Background}
\label{sec:background}

In this section, we present the foundational concepts and problem setup that underpin our method. We begin by reviewing the standard framework of \emph{decentralized partially observable Markov decision processes} (Dec-POMDPs) \citep{Oliehoek2016ACI, amato2024partial}, which formalizes many cooperative multi-agent reinforcement learning (MARL) tasks. We then focus on the \emph{episodic} version of MARL with sparse or delayed rewards, highlighting why these settings pose unique credit-assignment challenges. Finally, we introduce \emph{potential-based reward shaping} \citep{ng1999policy,Devlin2011TheoreticalCO}, the key theoretical tool we build upon to preserve optimal policies while reshaping rewards.

\subsection{Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs)}
\label{subsec:decpomdp_def}

A Dec-POMDP is defined by the tuple 
\[
\mathcal{M} = \bigl(\mathcal{S},\, \{\mathcal{A}_i\}_{i=1}^{N},\, \mathcal{P},\, \{\mathcal{O}_i\}_{i=1}^{N},\, \{\pi_i\}_{i=1}^{N},\, \mathcal{R}_{\zeta},\, \rho_0,\, \gamma\bigr),
\]
% where \(N\) agents interact in an environment with \emph{states} \(s \in \mathcal{S}\). Each agent \(i\) chooses actions \(a_i \in \mathcal{A}_i\) and receives an \emph{observation} \(o_i \in \mathcal{O}_i\) according to an observation function \(\mathcal{T}(s,i)\). 
where \(N\) agents interact in an environment with \emph{states} \(s \in \mathcal{S}\). Each agent \(i \in \{1, \dots, N\}\) selects an action \(a_i\) from its action space \(\mathcal{A}_i\), and receives an observation \(o_i\) from its observation space \(\mathcal{O}_i\). The observation \(o_i\) is generated according to the observation function \(\mathcal{T}(o_i \mid s, i)\), which defines the probability of agent \(i\) receiving observation \(o_i\) given the current state \(s\).
The transition function \(\mathcal{P}(s_{t+1}\mid s_t,a_t)\) governs how states evolve, and \(\rho_0\) is the initial state distribution. Agents operate according to a \emph{joint policy} \(\pi=\prod_{i=1}^N \pi_i\), where each \(\pi_i\) conditions on local observation histories \(h_{i,t}=\{(o_{i,\tau},a_{i,\tau})\}_{\tau=1}^t\), and \(h_{|\tau|}\) along with \(a_{|\tau|}\) represent the final approximate state and action of the multi-agent trajectory, respectively.

Unlike fully observable MDPs, partial observability means each agent sees only a slice of the global state, making coordination harder. A \emph{global reward} function \(\mathcal{R}_{\zeta} \colon \mathcal{S} \times \mathcal{A}\to \mathbb{R}\) provides a team-wide signal, shared among all agents at each timestep (or, in our \emph{episodic} case, at the end of the trajectory). The objective is to learn policies \(\pi\) that maximize the expected return:
\[
\mathbb{E}_{s_0\sim\rho_0,\; s\sim\mathcal{P},\; a\sim\pi}\,\Bigl[\sum_{t=0}^{|\tau|} \gamma^t\, r_{\text{global},t}\Bigr].
\]

\subsection{Episodic Multi-Agent Reinforcement Learning}
\label{subsec:episodic_marl}

In many MARL domains, rewards are received only upon completing an \emph{episode}, yielding a single \emph{episodic} reward \(r_{\text{global}, \text{episodic}}(\tau)\). This setting is common in tasks with sparse or delayed feedback—e.g., defeating all opponents in a battle environment or scoring a goal in a sports simulation. Although it accurately represents real-world scenarios, episodic (delayed) rewards significantly complicate learning, causing high variance and bias \citep{ng1999policy} in the policy gradient estimates. Agents must learn not only \emph{which} actions lead to success, but also \emph{when} those actions should occur within the trajectory—a problem known as \emph{temporal credit assignment}.

\subsection{Potential-Based Reward Shaping}
\label{subsec:potential_based_reward_shaping}

A well-known approach to address delayed or sparse rewards is \emph{potential-based reward shaping} \citep{ng1999policy}. In the single-agent case, one augments the reward function \(R\) with an extra shaping term \(F\), derived from a potential function \(\Phi\). Formally, for any two consecutive states \(s\) and \(s'\), the shaping reward is:
\[
F(s,s') \;=\; \gamma\,\Phi(s') \;-\; \Phi(s).
\]
Because this shaping term telescopes across a trajectory, it preserves the set of optimal policies. In multi-agent settings, the same principle applies if each agent’s shaping function is potential-based \citep{Devlin2011TheoreticalCO,lu2011policy}. In particular, adding
\[
F_i(s,s') = \gamma\,\Phi_i(s') \;-\; \Phi_i(s)\quad
\]
to the reward function for agent \(i\) ensures that Nash equilibria remain unchanged. For more details, refer to Appendix 1.

Potential-based shaping motivates our \textbf{reward redistribution} strategy, as it allows us to create denser per-step feedback while provably preserving policy optimality. We leverage this shaping concept to break down a single episodic reward into agent- and time-step-specific components, ensuring that the reward augmentation does \emph{not} distort the underlying solution set. In the next section, we detail how we design such a redistribution scheme—called \(\text{TAR}^2\)—to tackle sparse multi-agent tasks effectively.


\section{Approach}
\label{sec:approach}

In this section, we present our \emph{Temporal-Agent Reward Redistribution} (\(\text{TAR}^2\)) algorithm, which addresses the dual challenge of \emph{temporal} and \emph{agent-specific} credit assignment in cooperative multi-agent reinforcement learning (MARL). We begin by introducing our reward redistribution mechanism (Sec.~\ref{subsec:formulating_reward_func}), then establish theoretical guarantees ensuring optimal policy preservation (Sec.~\ref{subsec:optimal_policy_preservation}). Next, we detail the architectural design of our model (Sec.~\ref{subsec:reward_model_arch}) and outline the full training procedure (Sec.~\ref{subsec:training_objective}). Finally, we discuss interpretability considerations and reference ablation studies that provide further insight into each design choice. 

\subsection{Reward Redistribution Formulation}
\label{subsec:formulating_reward_func}

The central challenge in episodic MARL is to decompose a sparse global reward \(r_{\text{global},\text{episodic}}\) into more informative, time-step and agent-specific feedback, while preserving the original problem’s solutions. We achieve this by defining two weight functions: 
\[
w^{\text{temporal}}_t, \quad w^{\text{agent}}_{i,t},
\]
which decompose the final return across time steps and agents, respectively. Formally, each agent \(i\) at time \(t\) receives
\[
r_{i,t} 
\;=\;
w^{\text{agent}}_{i,t}\;\, w^{\text{temporal}}_t \;\, r_{\text{global},\text{episodic}}(\tau).
\]
We impose normalization constraints such that
\(\sum_{i=1}^N w^{\text{agent}}_{i,t} = 1\) 
and 
\(\sum_{t=1}^{|\tau|} w^{\text{temporal}}_t = 1,\)
ensuring the sum of all \(r_{i,t}\) matches the original episodic reward as done in prior works ~\citep{xiao2022agent, Chen2023STASSR, ren2021learning, efroni2021reinforcement}. Specifically, we make the following assumption about the reward redistribution:

\begin{assumption}
\label{assumption:rew_red_ass}
The per timestep agent-specific reward can be approximated by a function conditioned on observation-action tuple and the final outcome of the multi-agent trajectory. 
\begin{equation}
r_{\text{global}, \text{episodic}}(\tau) = \sum_{i=1}^N \sum_{t=1}^{|\tau|} r_{i, t}(h_{i, t}, a_{i, t}, h_{|\tau|}, a_{|\tau|}),
\label{eq:rew_red_assumption}
\end{equation}
\end{assumption}

By factoring the final reward into finer-grained terms, agents can receive meaningful feedback at each timestep (temporal credit), without conflating the contributions of other agents (agent credit). This alleviates the high-variance gradient updates typical in purely episodic settings, see Figure~\ref{fig:GFootball_baselines_bounds}.

We propose a two-stage reward redistribution mechanism that decomposes the global episodic reward temporally and across agents (see Section 2 in the supplementary material for detailed formulation).
% (see Section~\ref{sec:detailed_formulation_reward_func} in the supplementary material for detailed formulation).

\subsection{Theoretical Guarantees}
\label{subsec:optimal_policy_preservation}

\paragraph{Optimal Policy Invariance.}
We prove that the new reward function
\[
\mathcal{R}_{\omega,\kappa}^i(s_t,a_t,s_{t+1}) 
=\;
\mathcal{R}_{\zeta}(s_t,a_t,s_{t+1})
\;+\;
r_{i,t}
\]
is consistent with potential-based reward shaping (see Section~\ref{subsec:potential_based_reward_shaping}). Specifically, if \(\pi^*_{\theta}\) is optimal under \(\mathcal{R}_{\omega,\kappa}\), it remains optimal under the original environment reward \(\mathcal{R}_{\zeta}\). This ensures that our reward redistribution does not alter the set of optimal policies. For a detailed proof of this invariance, please refer to Section 3 of the supplementary material.
% For a detailed proof of this invariance, please refer to Section ~\ref{sec:optimal_policy_preservation_detailed} of the supplementary material.


\paragraph{Gradient Direction Preservation.}
Beyond preserving optimal solutions, we also show (in Section 6 or the supplemental material) 
% (in Appendix ~\ref{sec:pg_update_eq_detailed} or the supplemental material) 
that the \emph{direction} of the policy gradient update under TAR\(^2\) matches that under the original reward. Concretely, for an agent \(i\), 
\[
\nabla_{\theta_i} J_{\omega,\kappa}(\theta_i)
\;\propto\;
\nabla_{\theta_i} J_{\zeta}(\theta_i),
\]
indicating that we do not bias the learning trajectory, only \emph{densify} it. Proof details appear in Section 6 of the supplementary material.
% Proof details appear in Section ~\ref{sec:pg_update_eq_detailed} of the supplementary material.

\subsection{Reward Model Architecture}
\label{subsec:reward_model_arch}

Our reward redistribution model builds on the dual attention mechanisms of AREL \citep{xiao2022agent} and STAS \citep{Chen2023STASSR}, with key enhancements to better handle sparse, long-horizon multi-agent tasks.

\subsubsection{Adaptations from Prior Work}
\label{subsubsec:adapt_prior_work}
We embed each agent's observations, actions, and unique positional information at every timestep, then sum these embeddings to form a sequence that is input to a causal dual attention mechanism \citep{vaswani2017attention}. This mechanism allows us to capture both temporal dependencies and inter-agent interactions. By using a Shapley attention network, similar to STAS, we approximate each agent's marginal contribution via Monte Carlo rollouts over coalitions. The architecture comprises three dual attention blocks, each with four attention heads, selectively focusing on relevant spatial and temporal features.

\subsubsection{Architectural Enhancements and Modifications}
\label{subsubsec:arch_enhancements}
To improve credit assignment, we introduce three enhancements:
\begin{itemize}
    \item \textbf{Final State Conditioning:} We condition the reward redistribution on the final observation-action embedding \citep{harutyunyan2019hindsight, Amir2023StatesAG}, linking intermediate actions to final outcomes. This contrasts with prior methods that ignore end-of-trajectory context, often averaging rewards irrespective of ultimate success. By incorporating the final outcome, our approach ensures that intermediate actions are accurately attributed based on their actual contribution to the trajectory's success or failure, leading to more precise and meaningful credit assignment.
    \item \textbf{Inverse Dynamics Modeling:} We add an inverse dynamics module that predicts each agent's action at every timestep from the learned embeddings. This regularizes temporal representations, capturing causality and stabilizing credit assignment.
    \item \textbf{Probabilistic Reward Redistribution:} At inference, we normalize predicted rewards across time and agents to form probability distributions. This ensures each per-timestep, per-agent reward is a positive fraction of the episodic reward, preserving the principles of potential-based shaping.
\end{itemize}
These modifications enable more precise credit assignment in sparse-reward environments. For detailed architectural specifications see Section 7 of the supplementary material.
% For detailed architectural specifications see Section ~\ref{sec:reward_modeling_details} of the supplementary material.

\subsection{Training Objective}
\label{subsec:training_objective}
We optimize the reward redistribution model parameters \((\omega, \kappa)\) using the objective:
\begin{align}
\label{eq:reward_redistribution_training_objective}
    L(\omega, \kappa) &= \mathbb{E}_{\tau \sim B} \Biggl[ \Bigl( r_{\text{global}, \text{episodic}}(\tau) - \sum_{t=1}^{T} \sum_{i=1}^N R(i, t; \omega, \kappa) \Bigr)^2 \nonumber\\
    &\qquad\qquad - \lambda \sum_{t=1}^{T} \sum_{i=1}^N a_{i,t} \log(p_{i,t}) \Biggr],
\end{align}
where \(R(i,t; \omega,\kappa)\) denotes the predicted reward for agent \(i\) at time \(t\), and \(p_{i,t}\) is the inverse dynamics model’s predicted probability for action \(a_{i,t}\). The first term minimizes the discrepancy between the sum of redistributed rewards and the true episodic return, while the second term trains the inverse dynamics component via cross-entropy loss. Training involves sampling trajectories from an experience buffer \(B\) and periodically updating \((\omega,\kappa)\) using this loss. For the complete training procedure, please refer to Algorithm in Section 8 of the supplementary material.
% Algorithm~\ref{alg:psuedocode} of the supplementary material.


\section{Experimental Setup}
\label{sec:experimental_setup}

\subsection{Baselines}
We compare TAR\(^2\) against several reward redistribution methods, all trained with Multi-Agent Proximal Policy Optimization (MAPPO)~\citep{yu2022surprising}:

\begin{itemize}
    \item \textbf{TAR\(^2\) (Ours):} Our approach (Section~\ref{sec:approach}) that redistributes rewards both across time and agents, leveraging an inverse dynamics model and final outcome conditioning.
    \item \textbf{Uniform (IRCR)} \citep{gangwani2020learning}: Assigns the global episodic reward equally to each timestep and agent, i.e., \(r_{\text{global}, t} = r_{\text{episodic}}(\tau) / |\tau|\).
    \item \textbf{AREL-Temporal} \citep{xiao2022agent}: Focuses on temporal credit assignment by predicting rewards for the entire multi-agent trajectory at each timestep.
    \item \textbf{AREL-Agent-Temporal}: We modified AREL-TEMPORAL version to assign rewards per agent at each timestep, rather than per joint observation.
    \item \textbf{STAS} \citep{Chen2023STASSR}: Employs a dual attention structure (temporal + Shapley-based agent attention) to decompose global rewards into agent-temporal components.
\end{itemize}

Additional hyperparameter details for each method can be found in Section 11 of the supplementary material.
% Section~\ref{sec:hyperparameters} of the supplementary material.

\subsection{Environments}
\label{subsec:env}
We evaluate on two cooperative multi-agent benchmarks SMACLite~\citep{michalski2023smaclite} and Google Research Football~\citep{kurach2020google}, providing delayed (episodic) feedback by accumulating dense rewards during each trajectory and only returning them at the end. More details on the environment can be found in the supplementary material, Section 10.
% Section~\ref{sec:detailed_task_description}

\begin{figure*}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/GFootball_Baselines.png}
  \caption{Performance comparison of different reward redistribution approaches using MAPPO across three Google Research Football scenarios. The graphs plot median episode returns versus episodes.}
  \label{fig:GFootball_baselines}
\end{figure*}

\begin{figure*}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/SMACLite_baselines.png}
  \caption{Performance comparison of different reward redistribution approaches using MAPPO across three SMACLite scenarios. The graphs plot median episode returns versus episodes.}
  \label{fig:SMACLite_baselines}
\end{figure*}

\subsection{Results and Discussion}
\label{subsec:results}

\paragraph{Metrics.}
We report per-agent reward instead of win rate, as rewards provide continuous, granular feedback throughout training—particularly valuable in complex tasks where partial successes or incremental improvements occur long before an episode concludes.

\paragraph{Performance in GRF and SMAClite.}
Figures~\ref{fig:GFootball_baselines} and~\ref{fig:SMACLite_baselines} show average agent rewards (with standard deviation) over three GRF tasks and three SMAClite scenarios. TAR\(^2\) consistently outperforms baselines, converging to higher returns in all tasks.

\paragraph{Uniform Baseline (IRCR).}
The simplest baseline—assigning an equal portion of the global reward to each timestep and agent—plateaus early. By ignoring each agent's varying contribution, it provides insufficient guidance for fine-grained strategy learning, leading to relatively stagnant performance.

\paragraph{AREL Variants \& STAS.}
While STAS leverages Shapley-based decompositions and generally outperforms the AREL variants, it still trails TAR\(^2\). A key limitation is that both STAS and AREL produce unbounded per-agent, per-timestep predictions, which can destabilize training. Moreover, AREL (temporal or agent-temporal) struggles particularly in Google Football, leading to catastrophic drops in tasks like Pass and Shoot.

Overall, these results demonstrate that TAR\(^2\) not only yields higher final performance but also converges more reliably across a range of sparse-reward tasks.

While our primary focus in this work has been on achieving robust performance improvements and theoretical guarantees for reward redistribution, we also recognize the potential for interpreting $TAR^2$’s per-timestep, per-agent reward predictions. Although a detailed interpretability analysis is beyond the scope of this paper, preliminary insights and discussions are provided in the supplementary material, Section 9.
% Section~\ref{sec:interpretability}. 
We leave a comprehensive study of interpretability for future research, given its complexity in high-dimensional multi-agent environments.

\begin{figure*}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/GFootball_Bounds.png}
  \caption{Performance comparison of $TAR^2$ reward redistribution using MAPPO across three Google Research Football scenarios. The graphs plot median episode returns versus episodes, bounding $TAR^2$'s performance with heuristically designed reward functions for the environment.}
  \label{fig:GFootball_baselines_bounds}
\end{figure*}

\begin{figure*}[!h]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/Ablations.png}
  \caption{Ablation study of $TAR^2$ components across two environments. The graphs plot median episode returns versus episodes, analyzing the impact of key design choices by comparing $TAR^2$ with variations that exclude specific components.}
  \label{fig:GFootball_baselines_ablations}
\end{figure*}

\subsection{Additional Analysis: Performance Bounds \& Ablation Studies}
\label{sec:additional_analysis}

\subsubsection{Evaluation Baselines and Performance Bounds}
\label{subsec:baseline_performance_bounds}
To contextualize TAR$^2$’s performance, we evaluate it against several reward-assignment configurations in the environment, approximating both lower and upper performance bounds for MAPPO \citep{yu2022surprising}:

\begin{itemize}
\item \textbf{Episodic Team:} Provides a single global reward only at the episode’s end. This is a minimal credit assignment scheme that generally yields a lower bound.
\item \textbf{Episodic Agent:} Allocates total return to each agent based solely on its individual contribution. While more granular than Episodic Team, it can induce greediness and discourage cooperative behavior in tasks requiring teamwork.
\item \textbf{Dense Temporal:} All agents receive dense feedback at every timestep based on global performance. This offers an approximate upper bound, since it supplies immediate signals for each action.
\item \textbf{Dense Agent-Temporal:} Each agent obtains a dense reward for its individual contributions at every timestep. Similar to Episodic Agent, but with dense signals for each action, making it another upper-bound scenario.
\end{itemize}

Figure~\ref{fig:GFootball_baselines_bounds} illustrates these bounds alongside TAR$^2$ in three Google Research Football tasks. TAR$^2$ outperforms or matches the best dense baselines, demonstrating that its structured redistribution approach rivals the benefits of dense rewards while preserving policy optimality invariance.

\subsubsection{Ablation Studies}
\label{subsec:ablation_studies}

We also conduct ablations to isolate the impact of three key components in TAR$^2$: (1) the \emph{inverse dynamics} model, (2) \emph{final outcome conditioning}, and (3) \emph{normalization} of predicted rewards (Fig.~\ref{fig:GFootball_baselines_ablations}).

\paragraph{Inverse Dynamics Model.}
Removing the inverse dynamics task results in notably slower convergence and higher variance, indicating that predicting each agent’s actions helps the model capture causal structure in multi-agent trajectories. This aligns with prior findings \citep{pathak2017curiosity,agrawal2015learning} that inverse dynamics objectives can improve temporal representations.

\paragraph{Final Outcome Conditioning.}
When the reward predictor ignores the final state-action context, performance significantly degrades and fluctuates. Conditioning on the ultimate outcome helps the model attribute credit more accurately, mapping intermediate actions to actual successes or failures instead of an average or hypothetical return \citep{harutyunyan2019hindsight}.

\paragraph{Reward Normalization.}
Without normalizing the predicted rewards to sum to the true episodic return, learning becomes unstable leading to suboptimal policies, with wide performance fluctuations. Enforcing this normalization ensures each per-agent, per-timestep reward aligns with the global trajectory outcome, reducing volatility and preserving potential-based shaping requirements.

\paragraph{Summary of Ablations.}
Each component contributes to TAR$^2$’s performance and stability. Removing any of them leads to slower learning or higher fluctuations, underscoring their collective importance for effective agent-temporal credit assignment.

\section{Conclusion and Future Work}

In this paper, we tackled the dual challenges of temporal and agent-specific credit assignment in multi-agent reinforcement learning with delayed episodic rewards. We introduced \textbf{TAR\(^2\)}, a novel reward redistribution method that decomposes a global episodic reward into agent-specific, per-timestep rewards while preserving policy optimality via potential-based reward shaping. TAR\(^2\) effectively aligns intermediate actions with final outcomes by leveraging final state conditioning, inverse dynamics modeling, and probabilistic normalization—innovations that substantially improve learning in sparse-reward settings. Our extensive evaluations on challenging benchmarks like SMACLite and Google Research Football demonstrate that TAR\(^2\) outperforms state-of-the-art baselines in terms of sample efficiency and final performance. These results confirm that our approach provides more precise credit assignment, leading to faster and more stable policy convergence.

Looking ahead, we envision several exciting avenues for expanding this work. For instance, integrating Hindsight Credit Assignment \citep{harutyunyan2019hindsight} into the TAR\(^2\) framework could further refine reward signals based on alternate goals or outcomes. Additionally, exploring transfer-learning capabilities—such as applying a model trained with a certain number of agents to scenarios with more agents or transferring knowledge across similar environments—could reveal the academic offspring of TAR\(^2\), broadening its applicability and impact.

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{main}

\appendix
\input{supplementary_ijcai_2025}

\end{document}

