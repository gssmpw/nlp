We proposed a novel framework for finding efficient compressed DNN models for \gls{mMIMO} precoding.
By using quantization-aware training with \gls{LSQ} and \gls{MPQ}, while also searching through different DNN architecture sizes, we find DNN models with significantly lower energy consumption at equal performance. 
A variety of Pareto-optimal models were generated, showing the ability of DNN solutions to provide different tradeoffs between energy consumption and performance.
%Training different model sizes within NAS allowed us to adaptively balance energy efficiency with sum rate performance. 
Compared to \gls{WMMSE}, the obtained DNN models achieve superior energy efficiency across diverse deployment scenarios. This work demonstrates the promise of DNN solutions to obtain high-performance \gls{mMIMO} precoders with much improved energy efficiency.

