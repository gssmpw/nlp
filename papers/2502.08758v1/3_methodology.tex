\begin{figure}[!t]
\includegraphics[width=\linewidth]{fig/archi.png}
\caption{DNN architecture template.}
\label{fig:DNN}
\end{figure}
\subsection{Model Architecture and NAS}
The proposed \gls{DNN} architecture for digital beamforming, illustrated in Figure \ref{fig:DNN}, is specifically designed to address the demands of \gls{mMIMO} systems. The architecture gets the real and imaginary parts of the channel state information through two separate channels as input and predicts the precoding matrix. The network comprises a single \gls{CNN} layer with the kernel size of 3 for feature extraction, followed by three \glspl{FCL} to map the extracted features into the real and imaginary components of the digital precoder. To enhance model generalization and stability, the architecture incorporates ReLU activation functions, 5\% dropout to mitigate overfitting, and batch normalization to accelerate convergence and improve robustness during training.
Our approach optimizes quantization by exploring a search space of bit-width configurations for the weights across the layers of the \glspl{DNN}. We vary the precision levels for each layer—[CONV, FCL1, FCL2, FCL3]—with multiple bit-width choices, allowing for a large number of possible permutations. Additionally, we consider different model architectures by varying the output channel size \(C_{\text{out}}\) of the convolutional layer and the size \(D_{\text{FCL}}\) of \glspl{FCL}. This results in a comprehensive search space combining quantization and architectural configurations. We use exhaustive search to explore all possible combinations, ensuring that optimal configurations are identified without overlooking any potential solutions. The small model size and fast training times make this exhaustive search feasible. Ultimately, this method helps to balance energy efficiency and sum rate performance, which is essential for energy-constrained \gls{mMIMO} systems where optimizing both performance and resource usage is critical.


\subsection{Quantization Methods}

This study examines several quantization techniques to enhance energy efficiency and maintain performance in \glspl{DNN} for \gls{mMIMO} systems. The main objective is to minimize energy consumption and memory demands while preserving an acceptable sum rate. We explore \gls{QAT} using the \gls{LSQ} method \cite{Esser2020LEARNED} and \gls{MPQ} through an exhaustive search. These quantization methods are then combined with \gls{NAS} to find an optimal balance between energy efficiency and performance.

\subsubsection{\gls{PTQ}}
In preliminary experiments, we also considered \gls{PTQ} as a simple way to reduce model complexity.
This method applies fixed-point quantization to a pre-trained floating-point (FP32) model without requiring retraining. 
However, in our PTQ experiments, sum rate performance was highly degraded at low resolutions (4 bits or less), and therefore we focus in this paper on the \gls{QAT} approach to investigate the best possible model compression.

\subsubsection{\gls{QAT}}
To address the limitations of \gls{PTQ}, \gls{QAT} fine-tunes a pre-trained model within quantization constraints, enabling the network to adapt to lower precision with minimal performance loss. We use \gls{LSQ}\cite{Esser2020LEARNED}, which dynamically adjusts quantization step sizes during training. Unlike static step quantization, \gls{LSQ} optimizes these step sizes to mitigate quantization error, especially in low-bit scenarios. This is achieved through the Straight Through Estimator \cite{DBLP:journals/corr/abs-2109-05472}, which supports gradient-based optimization for non-differentiable quantization functions. The adaptive nature of \gls{LSQ} provides significant gains in both sum rate and energy efficiency under low-bit quantization constraints.

\subsubsection{\gls{MPQ} with \Gls{NAS}}
\gls{MPQ} assigns different bit-widths to each network layer, offering a flexible trade-off between energy efficiency and accuracy. We employ exhaustive search to evaluate quantization combinations [2,\,4,\,8,\,and\,16\,bits] across four layers, calculated as \(4^4 \)\,$=$\,\(256\).
and various model architectures by varying \( C_{\text{out}} \in \{8,\,16,\,32,\,64\} \) and \( D_{\text{FCL}}\)\,$\in$\,\(\{512, 1024\} \) across the \Gls{CNN} and \glspl{FCL} (see Fig.~\ref{fig:DNN}) to minimize energy consumption while preserving sum rate performance \cite{DBLP:journals/corr/abs-2003-07577}.

\subsection{Training Method}
All models are trained using a self-supervised approach \cite{10624768}, with sum-rate maximization as the objective function. This method ensures that the model learns to predict effective precoding vectors directly, without the need for labeled data. The loss function is defined as 
\begin{align}
    \mathcal{L} = - R(\mb{W})\, ,
\end{align}
and models are trained with a fixed learning rate of $10^{-3}$ and a batch size of 1000.
Additionally, we average the results from four training runs with distinct initialization seeds to control for variations in the training process.

\subsection{Energy Model for Quantized Neural Networks}\label{sec:energy_dnn}
In order to evaluate and compare the energy consumption of a \gls{DNN} solution, we consider a simple but realistic model of the energy consumed to compute the \gls{DNN} output.
We base our model on the one proposed in \cite{moons:2017a}, which considers the energy required for memory accesses and computations, while taking into account the impact of the bit width of model weights and activations.

The energy consumption of the DNN is thus decomposed into three components: computation energy (\(E_C\)), weight transfer energy (\(E_W\)), and activation transfer energy (\(E_A\)). 
The total energy consumption is then given by
\begin{equation}\label{eq:total-energy}
    E_{\mathrm{DNN}} = E_C + E_W + E_A \,.
\end{equation}
%
The computation energy is based on counting the number of \gls{MAC} operations required for the linear portion of each layer and the number of biasing, batch normalization, and activation function computations. To simplify the model, all these operations are attributed an energy $E_{\text{MAC}}$.
The computation energy $E_C$ is thus given by
\begin{equation}\label{eq:computation-energy}
    E_C = E_{\text{MAC}} \cdot (N_c + 3 \cdot N_a)\,,
\end{equation}
where $N_c$ is the total number of MAC operations required by the model, $N_a$ is the total number of activations, and the factor $3$ arises since one biasing, one normalization, and one activation must be computed for each activation output.

For consistency with the baseline energy model that will be presented next, we model $E_\text{MAC}$ in terms of the bit width $Q$ as
\begin{equation}\label{eq:mac-energy}
    E_{\text{MAC}} = \alpha \cdot \left( \frac{Q}{16} \right)^{\beta}\,,
\end{equation}
while choosing $\alpha$\,$=$\,$0.86$ and $\beta$\,$=$\,$1.9$ to fit the energy measurements reported in \cite{6757323}.

The energy associated with transferring weights, \(E_W\), is expressed as
\begin{equation}\label{eq:weight-energy}
    E_W = E_M \cdot N_w + E_L \cdot \frac{N_c}{\sqrt{p}},
\end{equation}
where \(N_w\) is the total number of weights, \(E_M\) and \(E_L\) represent the energy costs of accessing the main on-chip memory and local buffers, respectively, and $p$ is the number of parallel execution units.
We use $E_M=2 E_L=2 E_\mathrm{MAC}$ and  \(p =
64 \big(\frac{Q}{16}\big)\).
It is assumed that the entire DNN model fits on-chip and no accesses to external memory are needed.

Finally, the energy for transferring activations, \(E_A\), is given by
\begin{equation}\label{eq:activation-energy}
    E_A = 2 \cdot E_M \cdot N_a + E_L \cdot \frac{N_c}{\sqrt{p}}\,.
\end{equation}

%This energy model captures the interplay between computational and off-memory access energy costs, offering a framework for analyzing and optimizing \glspl{QNN} under varying quantization levels and hardware configurations.

\subsection{Energy Model for Baseline Methods}\label{sec:energy_baselines}
We compare the energy consumption of the proposed quantized DNN models to conventional algorithms, specifically the \gls{WMMSE} and \gls{ZF} methods. The \gls{WMMSE} algorithm, known for its high computational complexity due to iterative processing and matrix inversions, consumes significantly more energy than the \gls{ZF} approach. 
Since these energy models act as a baseline, we adopt a conservative (lower bound) approach by accounting only for the multiplications when estimating compute energy. For memory energy estimation, we consider only the local buffer accesses for the operands.
The total number of real multiplications required for $I$ iterations of \gls{WMMSE} is given by
\begin{align}
 N_c = I\bigg(\frac{8}{3} N_{\sf{T}}^3N_{\sf{U}} &+ 4N_{\sf{T}}^2N_{\sf{U}} 
 + 4N_{\sf{T}}(4N_{\sf{U}}^2+2N_{\sf{U}})\notag\\
 &+ 4N_{\sf{U}}^2
 +\frac{56}{3}N_{\sf{U}}\bigg)  \, ,
\end{align}
% \begin{align*}
%     I\bigg(\frac{2}{3} N_{\sf{T}}^3 N_{\sf{U}} 
%     + N_{\sf{T}}^2 N_{\sf{U}} (N_{\sf{U}} + 2)
%     + N_{\sf{T}}N_{\sf{U}}(N_{\sf{U}}-1) 
%     - \frac{1}{3}N_{\sf{U}}\bigg)\, 
% \end{align*}
while the required number of real multiplications for ZF is
\begin{equation}
    N_c = 8 N_{\sf{U}}^2 N_{\sf{T}} + \frac{8}{3} N_{\sf{U}}^3 \, .
\end{equation}
% \begin{align*}
%     2N_{\sf{U}}^2 (2 N_{\sf{T}} - 1) + \frac{4}{3} N_{\sf{U}}^3.
% \end{align*}
The energy model for WMMSE and ZF can then be expressed as
\begin{equation}
    E_{\mathrm{B}} = E_{MAC} \cdot N_c + E_L\cdot\frac{N_c}{\sqrt{p}} \, .
\end{equation}
% (not the right place for a discussion) These models highlight a trade-off between computational complexity and energy efficiency: \gls{WMMSE} offers higher precision but at significantly higher energy costs, while \gls{ZF} provides a more energy-efficient alternative.


