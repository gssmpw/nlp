\subsection{Dataset Definition}
A custom dataset was generated to accurately reflect the channel characteristics pertinent to beamforming in mMIMO systems. MATLAB’s Ray-Tracing toolbox simulated both \gls{LOS} and \gls{NLOS} conditions. The simulations positioned a \gls{BS} within the Montreal region, utilizing environmental data from OpenStreetMap \cite{OpenStreetMap} for realism. The base station employed a uniform planar array antenna with 8x8 elements, spaced at half-wavelength, operating at a frequency of 2\,GHz. The transmitter was set at a height of 20\,m and powered at 20\,W, assuming a system loss of 10\,dB. Users were placed in circular patterns around the base station at distances ranging from 50 to 350\,m and at 10-degree intervals. This configuration captured a wide variety of deployment scenarios and channel conditions. The ray-tracing simulations considered up to 10 reflections but excluded diffraction effects, focusing on signal reflections from buildings and terrain to emulate multipath propagation in urban environments accurately.

\subsection{Energy Consumption Examples}
We first present some examples of the energy consumed by the different approaches, as per the model presented in Sections~\ref{sec:energy_dnn} and \ref{sec:energy_baselines}.
We consider the ``UdeM-NLOS'' scenario.
In Table~\ref{table:energy_comparison}, we report the energy for the ``default'' variants of the method, that is, for \gls{WMMSE}, we set the stopping criterion to $10^{-5}$ to have near-optimal performance, and for the DNN, we use $C_{\text{out}}$\,$=$\,$64$ and $D_{\text{FCL}}$\,$=$\,$1024$ with the maximum weight resolution of 16 bits.
We see that the DNN consumes significantly less than WMMSE at the cost of a slight degradation in sum rate (on this scenario). ZF, on the other hand, is much less complex, but does not provide competitive performance in \gls{NLOS} conditions, or at low \gls{SNR}.
As a result, ZF is unlikely to be a favored solution in practice, since it results in significant under-utilization of the BS resources.

\input{table_energy_comparison}

\input{table_energy_consumption}

Next, to illustrate the impact of quantization, 
Table~\ref{tab:energy_consumption} lists the energy consumption of the DNN for various uniform bit-width configurations, for the same $C_{\text{out}}$ and $D_{\text{FCL}}$ as in Table~\ref{table:energy_comparison}. We see that lowering the weight resolution leads to substantial energy savings but at the cost of a moderate decrease in performance.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{fig/result_ICC_1_pareto_modified_final.pdf}
    \caption{Trade-off between energy efficiency and sum rate for \glspl{CNN} with varying $C_{\text{out}}$, $D_{\text{FCL}}$, and MPQ bit widths, on the ``UdeM-LOS'' scenario ($N_{\sf{T}}$\,$=$\,$64$, $N_{\sf{U}}$\,$=$\,$4$, average SNR\,$=$\,29\,dB). All the model configurations that were evaluated are shown, while the curves provide the Pareto front associated with each architecture configuration.}
    \label{fig:Neural Architecture Search (NAS)}
    \vspace{-10pt}
\end{figure}

\subsection{NAS Results}

Figure \ref{fig:Neural Architecture Search (NAS)} highlights the trade-offs between computational energy efficiency (bits/s/Hz/$\mu$J) and sum rate (bits/s/Hz) across diverse \gls{DNN} configurations generated as described in Section~\ref{Sec:Proposed}, on the ``UdeM-LOS'' scenario.
Each curve shows the Pareto front corresponding to a particular architecture configuration, while each point in this curve uses a different quantization configuration.

A few trends can be mentioned among the Pareto-optimal results for each architecture.
Firstly, the first layer, CONV, is often kept at high precision, particularly in smaller models, to maintain performance while reducing energy consumption. Interestingly, no Pareto-optimal model uses uniform quantization across all layers. Moreover, even models that achieve the highest sum rates do not employ more than two layers at the highest precision. These results emphasize the importance of efficiently distributing bit precision across layers to optimize energy consumption.

The configuration yielding the highest energy efficiency is ($C_{out}$\,$=$\,$8$, $D_{FCL}$\,$=$\,$512$) with quantization [2,\,8,\,8,\,8], achieving 26.2 \,bits/s/Hz/$\mu$J at a moderate sum rate of 28.5\,bits/s/Hz. On the other hand, the configuration achieving the highest sum rate is ($C_{out}$\,$=$\,$64$, $D_{FCL}$\,$=$\,$ 1024$) with quantization [16,\,16,\,4,\,8], which reaches 31.3\,bits/s/Hz but does not use full precision, making it more interesting from an energy efficiency perspective. An alternative worth mentioning is the model with the second-best sum rate of 31.1\,bits/s/Hz, achieved with an architecture of ($C_{out}$\,$=$\,$64$, $D_{FCL}$\,$=$\,$1024$) and quantization [16,\,2,\,2,\,2], which uses nearly $5\times$ less energy than the highest sum-rate model.
We do observe compute energy efficiency decreasing rapidly near the highest sum-rate, but of course this could simply mean that switching to a larger and/or different DNN architecture would be preferable at that point.

To further illustrate the importance of of NAS and model compression in finding efficient DNN precoders, Figure~\ref{fig:Neural Architecture Search (NAS)} includes horizontal and vertical arrows that quantify the impact on performance of the design choices. The horizontal arrow measures the difference in sum rate between the worst and best configurations at equal energy efficiency, revealing a 20\% improvement through optimal model selection. Similarly, the vertical arrow shows the difference in energy efficiency at an equal sum rate, demonstrating a $14.5\times$ gain.


\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{fig/result_ICC_5.pdf}
\caption{Comparison of energy efficiency (bits/s/Hz/$\mu$J) and sum rate (bits/s/Hz) across two environments: UdeM-NLOS (average SNR\,$=$\,15\,dB) and Okapark-LOS (average SNR\,$=$\,28\,dB). The proposed method achieves a superior balance of energy efficiency and sum rate performance compared to WMMSE. Results are derived for models with varying ($C_{out}$) and ($D_{FCL}$).}
\vspace{-9pt}
\label{fig:LSQ Quantization}
\end{figure}
\subsection{Impact of Deployment Environment and Energy Efficiency Comparison}

Figure~\ref{fig:LSQ Quantization} compares energy efficiency (bits/s/Hz/$\mu$J) and sum-rate (bits/s/Hz) across two contrasting deployment scenarios: UdeM-NLOS, characterized by challenging multipath conditions, and Okapark-LOS, offering clear \gls{LOS} signal propagation. These two scenarios highlight the adaptability and effectiveness of the proposed quantized models.
For the DNN precoders, each curve shows the Pareto-optimal configurations across the entire search space, whereas for WMMSE, the trade-off between sum-rate and energy efficiency is varied by adjusting the stopping criterion.

In the UdeM-NLOS environment, sum rates are constrained between 15 and 20 bits/s/Hz due to severe signal attenuation and multipath effects. Despite these limitations, the quantized models achieve significant energy savings, with improvements of up to $35\times$ in energy efficiency compared to the WMMSE baseline, all while maintaining competitive sum rates.

In contrast, the Okapark-LOS environment, which benefits from clear signal paths, supports higher sum rates ranging from about 30 to 40 bits/s/Hz. 
Depending on the desired sum-rate, the DNN precoders can provide improvements in energy efficiency ranging from $6.1\times$ to $1.2\times$. However, with the DNN architecture template and training method considered in this paper, the DNN precoder is unable to achieve the highest sum-rate that can be provided by WMMSE.
%Here, the quantized models continue outperforming the WMMSE baseline, delivering a 6.1× improvement in energy efficiency while maintaining similar or superior sum-rate performance. The highest sum rate, achieved by the DNN for Okapark-LOS ($C_{out} = 64$, $D_{FCL} = 1024$), is roughly 1.3× higher, highlighting an intriguing balance of performance and precision.

These results emphasize the adaptability of the quantized models across diverse deployment environments, and their ability to achieve the same performance with less compute energy. Interestingly, the energy gains provided by DNNs appear to be larger in more difficult (low SNR, non line-of-sight) environments.




