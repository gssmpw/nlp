\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{url}
\usepackage{xcolor}
\usepackage{array}
\usepackage{booktabs}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}

\usepackage{hyperref}
\usepackage{multicol}
\usepackage{multirow}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

\title{Effects of Prompt Length on Domain-specific Tasks for Large Language Models

\author{
\IEEEauthorblockN{Qibang Liu\textsuperscript{1}, Wenzhe Wang\textsuperscript{2}, Jeffrey Willard\textsuperscript{3}}
\IEEEauthorblockA{\textsuperscript{1}Georgia Institute of Technology \textsuperscript{2}Nanjing University of Finance and Economics \textsuperscript{3}Boston University}
\IEEEauthorblockA{qibang@gatech.edu, 2120221363@stu.nufe.edu.cn, jeffwil@bu.edu}

}
}

\maketitle

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
In recent years, Large Language Models have garnered significant attention for their strong performance in various natural language tasks, such as machine translation and question-answering. These models demonstrate an impressive ability to generalize across diverse tasks. However, their effectiveness in tackling domain-specific tasks, such as financial sentiment analysis and monetary policy understanding, remains a topic of debate, as these tasks often require specialized knowledge and precise reasoning. To address such challenges, researchers design various prompts to unlock the models' abilities. By carefully crafting input prompts, researchers can guide these models to produce more accurate responses. Consequently, prompt engineering has become a key focus of study. Despite the advancements in both models and prompt engineering, the relationship between the two---specifically, how prompt design impacts models' ability to perform domain-specific tasks---remains underexplored. This paper aims to bridge this research gap.
\end{abstract}

\begin{IEEEkeywords}
natural language processing, prompt engineering, domain-specific tasks, performance 
\end{IEEEkeywords}

\section{Introduction}


Large Language Models (LLMs) have become an integral part of our daily lives, transforming the way we interact with technology. 
From virtual assistants like Siri and Alexa to language translation apps like Google Translate, LLMs are being used in various applications to make our lives easier. 
For instance, chatbots powered by LLMs are being used by businesses to provide customer support and answer frequently asked questions. 
Additionally, LLMs are being used in writing assistants like Grammarly to help users improve their writing skills. 
Furthermore, LLMs are also being used in language learning apps like Duolingo to help users learn new languages. 
% Overall, LLMs have become a ubiquitous part of our daily lives, making it easier for us to interact with technology and access information.


Prompts play a crucial role in unlocking the potential of LLMs. 
A well-crafted prompt can guide the LLM to generate accurate, relevant, and coherent responses. 
For instance, when asking an LLM to write a story, providing a prompt with specific details such as genre, setting, and characters can help the model generate a more engaging and focused story. 
% Similarly, when using an LLM for language translation, providing context through a prompt can improve the accuracy of the translation because of tapping into the full potential of LLMs and unlocking their capabilities. 
% \textcolor{red}{Especially, when domain-specific tasks are resource-intensive and time-consuming, leveraging LLMs like LLaMA 3 offers an innovative approach to approximating system behaviors, which provides insights and is greatly supportive to accelerate the research, development, and iteration processes compared to traditional solutions.}
% For example, translating a sentence like ``I'm feeling under the weather'' requires understanding the idiomatic expression, which can be facilitated by providing a prompt that explains the context. 
%By crafting effective prompts, users can tap into the full potential of LLMs and unlock their capabilities. 
However, despite of the well-known advantages, LLMs' performance relies heavily on the quality and specificity of the prompts. 
Poorly constructed prompts can lead to inaccurate outputs because LLMs lack intrinsic knowledge of research. 
%While they can infer patterns from data, 
For example, LLMs may output oversimplified or non-viable solutions due to not inherently understanding the research mechanics. 
This limitation emphasizes the domain expertise in crafting prompts and interpreting results.


Through diversifying prompt instructions, researchers pioneeringly address the LLMs' performance across specific domains (e.g., economics, health, and technical systems), which empowers nations to strengthen economic resilience, protect public health, and maintain technological leadership on a global scale. By systematically examining domain-specific impacts, their works highlight the transformative potential of prompt engineering in identifying LLM capabilities for specialized applications. For example, LLMs help with policy-making by understanding financial sentiment \cite{10.1145/3696271.3696299}. In public health, LLMs revolutionize disease detection, enabling personalized treatments and providing health support \cite{wei2024enhancingdiseasedetectionradiology}. %In technology, LLMs improve system adaptability to dynamic environments by ensuring safety-critical identification and priority-related decision-making \cite{zhang2024empowering}. %Strategically addressing these areasIn this paper, advanced literature (\cite{}, \cite{10.1145/3637528.3671647}, \cite{javadi2024llmbasedweaksupervisionframework},\cite{10.1145/3696271.3696294}, \cite{10.1145/3696271.3696292}, \cite{10.1145/3696271.3696299}, \cite{zhang2024empowering}, \cite{wei2024enhancingdiseasedetectionradiology}) in their fields is chosen as representatives for our further in
vestigation.

While various aspects of prompts have been extensively researched, such as prompt design, and their content, one crucial aspect remains unexplored: the length of prompts. 
Despite the growing importance of LLMs, the optimal length of prompts that can effectively elicit desired responses from these models is still unknown. 
Previous research has focused on optimizing prompt content, structure, and style to improve model performance, but the impact of prompt length on model behavior and response quality has not been systematically investigated. 
As a result, we aim to bridge this gap in the understanding of how prompt length affects LLM performance in domain-specific tasks. 


Our main contributions are as follows. 
\begin{itemize}
    \item We conduct extensive experiments with LLMs on nine domain-specific tasks, and the results show that although they claim to be state-of-the-art NLP models at the time of their releases, with the default prompt length, they still struggle to tackle tasks without sufficient domain knowledge.
    \item Our results with different prompt lengths show that long prompts, providing more background knowledge about the domain that the task falls into, are generally beneficial.
    \item Our results also show that even with background knowledge in long prompts, LLMs' performance still lags behind humans, as the average F1-score is much less than 1.0.
\end{itemize}


\section{Background}


\subsection{Deep Learning and Large Language Model}


Since the introduction of AlexNet~\cite{NIPS2012_c399862d}, we have entered the era of deep learning, marking significant advancements across various fields, such as asphalt pavement analysis~\cite{DAN2024134837,Dan31122024}, scene text detection~\cite{Tang_2022_CVPR,10239535}, visual question answering~\cite{tang2024mtvqabenchmarkingmultilingualtextcentric,tang2024textsquarescalingtextcentricvisual}, scene text recognition~\cite{10.1145/3503161.3547787,10.1007/978-3-031-19815-1_14,Zhao_2024_CVPR}, character recognition~\cite{10.1093/nsr/nwad141}, and adversarial attach~\cite{10.1117/12.3009240,10263390}. In addition to the application layer, theoretical investigation has attracted substantial attention, such as Q Learning~\cite{zhao2024minimaxoptimalqlearning}.
In NLP, the progress in language model pretraining began with Word2Vec~\cite{NIPS2013_9aa42b31,Mikolov2013EfficientEO} and has accelerated rapidly. Among these developments, the Transformer architecture~\cite{NIPS2017_3f5ee243} is the backbone of all modern language models. 

Transformer's two core components, the encoder and decoder, have been the foundation for much of the work in model pretraining. For encoder-based pretraining, BERT~\cite{devlin-etal-2019-bert} is the most representative example, inspiring numerous variants to improve the pretraining paradigm (e.g., RoBERTa~\cite{liu2019robertarobustlyoptimizedbert}, DistillBERT~\cite{Sanh2019DistilBERTAD}, and ALBERT~\cite{DBLP:conf/iclr/LanCGGSS20} to address domain-specific tasks (e.g., FinBERT~\cite{10.5555/3491440.3492062} for finance, SciBERT~\cite{beltagy-etal-2019-scibert} for scientific texts, and ClinicalBERT~\cite{clinicalbert} for medical data).
Simultaneously, another branch of pretraining work focuses on the decoder of the Transformer. The most notable examples include the GPT series (e.g., GPT-1~\cite{Radford2018ImprovingLU}, GPT-2~\cite{Radford2019LanguageMA}, GPT-3~\cite{NEURIPS2020_1457c0d6}) and the LLaMA series (e.g., LLaMA-1~\cite{touvron2023llamaopenefficientfoundation}, LLaMA-2~\cite{touvron2023llama2openfoundation}, LLaMA-3~\cite{grattafiori2024llama3herdmodels}, and its incremental updates like LLaMA 3.1, 3.2, and 3.3\footnote{\url{https://www.llama.com/docs/model-cards-and-prompt-formats}}). These models discard the encoder and rely solely on the decoder as their backbone architecture. 
Following the rise of ChatGPT\footnote{chatgpt.com}, which fueled widespread interest in LLMs, researchers have increasingly focused on evaluating the LLaMA series on various domain-specific tasks, such as financial sentiment analysis~\cite{10.1145/3696271.3696299} and emotion identification~\cite{10.1145/3696271.3696292}. 


However, no prior work has investigated how prompt length impacts the LLM performance on domain-specific tasks. This is important because earlier studies have shown that prompts are crucial in eliciting LLMs' abilities for effective language understanding and task performance~\cite{10458651,xiao-etal-2024-analyzing,10.1145/3655497.3655500}. This paper aims to provide insights to bridge this gap.
% into how prompts should be designed for domain-specific tasks, which will be extremely valuable for people in academia and industry to develop a plan to design the prompt for their tasks.


\subsection{Prompt Engineering} 


Most existing work on prompt engineering focuses on enhancing reasoning and logical capabilities in language models. Specifically, Chain-of-Thought (CoT) prompting~\cite{10.5555/3600270.3602070,zhang2022automaticchainthoughtprompting}, which systematically investigates step-by-step reasoning, is considered a pioneering approach in this domain. 
Building on CoT, subsequent advancements have been proposed: Self-consistency~\cite{wang2023selfconsistencyimproveschainthought} introduces mechanisms to improve reliability in reasoning paths; Logical CoT prompting~\cite{zhao-etal-2024-enhancing-zero} refines logical reasoning within prompts; Chain-of-Symbols (CoS) prompting~\cite{hu2024chainofsymbolpromptingelicitsplanning} explores symbolic representations to enhance task-solving processes; Tree-of-Thoughts prompting~\cite{10.5555/3666122.3666639} introduces hierarchical reasoning structures; Graph-of-Thoughts (GoT) prompting~\cite{yao-etal-2024-got} leverages interconnected reasoning nodes to expand context understanding; Thread-of-Thought (ThouT) prompting~\cite{zhou2023threadthoughtunravelingchaotic} emphasizes linear and contextual reasoning threads; and Chain-of-Tables prompting~\cite{wang2024chainoftableevolvingtablesreasoning} adapts tabular representations for specific reasoning tasks. 


Although these innovations advance the capabilities of language models in handling complex reasoning and logic-intensive applications, there is not a comprehensive study of how the length of prompt affects LLMs' capability in domain-specific tasks.


\section{Experiments}


\begin{table}[t]
    \centering
    \small
    \begin{tabular}{l|l}
        \toprule
        Acronym & Full name \\
        \midrule
        MPU & Monetary Policy Understanding \\ 
        UI & User Intent \\  % https://dl.acm.org/doi/pdf/10.1145/3637528.3671647
        CD & Conversation Domain \\
        QIC & Query Intent Classification \\        
        SD & Sarcasm Detection \\
        EI & Emotion Identification \\         
        FSA & Financial Sentiment Analysis \\        
        TSB & Technical System Behavior \\
        DD & Disease Detection \\

        \bottomrule
        \addlinespace
    \end{tabular}
    \caption{Acronyms of the domain-specific tasks in this paper.}
    \label{tab:task_name}
\end{table}


To assess the impact of prompt length on LLMs' performance across domain-specific tasks, we conducted a series of comprehensive experiments under varying prompt length settings. Specifically, we conducted nine groups of experiments: Monetary Policy Understanding, User Intent, Conversation Domain, Query Intent Classification, Sarcasm Detection, Emotion Identification, Financial Sentiment Analysis, Technical System Behavior, and Disease Detection.  %Monetary Policy Understanding (MPU), Financial Sentiment Analysis (FSA), Sarcasm Detection (SD), Emotion Identification (EI), User Intent (UI), Conversation Domain (CD), Query Intent Classification (QIC), Technical System Behavior (TSB), and Disease Detection (DD). 
We use their acronyms throughout this experimental section and the subsequent results section for clarity and improved readability. Table~\ref{tab:task_name} maps the task acronyms and their full names.


%Generally, an engineered system is an integration of a couple of hardware and software, whose performance depends on a variety of interdependent factors, including hardware and software capabilities, their interactions, and the surrounding system environment. These interconnected features often create complexity, making it challenging to achieve precise and reliable results. While LLMs have demonstrated significant potential across various domains, an intriguing question arises: can LLMs be effectively applied to engineered systems? To address this, we designed specific prompt instructions and analyzed their impact on system performance.


% \subsection{\textcolor{red}{temp}}
% Benefits of LLM-based Approaches

% One of the primary benefits of leveraging LLMs for domain-specific tasks in engineered systems is their ability to process and synthesize large amounts of data efficiently. With well-designed prompts, LLMs can infer system behaviors and provide approximations that guide decision-making. This capability reduces dependency on traditional testing setups, which often require physical equipment and real-time network environments.

% Another advantage is adaptability. LLMs can accommodate various scenarios by adjusting prompt designs, allowing researchers to explore multiple performance aspects without additional configurations. This flexibility makes LLMs particularly useful in dynamic environments where system parameters frequently change.

% Moreover, the use of LLMs can accelerate research and development cycles. Engineers and researchers can quickly obtain insights into system performance, enabling faster iterations and refinements. This speed is especially beneficial for emerging technologies where time-to-market is critical.



% Findings and Future Directions

% Our study revealed that LLMs like LLaMA 3 show promise in approximating system performance metrics, such as E2E delay, under certain conditions. By iteratively refining prompt instructions, we observed improvements in the relevance and accuracy of LLM-generated insights. However, achieving optimal results required a deep understanding of both the LLM's capabilities and the target system's characteristics.

% Future work should focus on developing standardized methodologies for prompt design in engineered systems. This includes creating libraries of tested prompts tailored to specific domains, which could streamline the application of LLMs in industry and academia. Additionally, integrating LLMs with simulation tools or real-time monitoring systems could enhance their predictive accuracy and practical utility.







% \begin{table}[t]
%     \centering
%     \begin{tabular}{l|l|l}
%         \toprule
%         \textbf{Perspective} & \textbf{Component} & \textbf{Prompt Example} \\
%         \midrule
%         Hardware & Computer/Platform & NVIDIA Product \\        
%                  &                   &  Jetson TX2    \\
%                  & RF end            & Software-defined radio\\
%                  &                   &  LimeSDR          \\
%         \midrule
%         Software & Operating System  & Ubuntu 19.xx \\
%                  & Hardware Driver   & SDR driver   \\
%                  & Cellular network software & srsLTE \\ 
%         \midrule
%         Environment & Space          & Indoor         \\
%                     & Distance       & 1 meter        \\
%         \bottomrule
%         \addlinespace
%     \end{tabular}
%     \caption{The domain-specific prompt that we have experimented with. \textcolor{red}{delete because other tasks dont mention these details.}}
%     \label{tab:prompt instruction}
% \end{table}

% In conclusion, leveraging LLMs for domain-specific tasks in engineered systems represents a promising frontier. While challenges remain, careful prompt design and validation can unlock significant efficiencies, paving the way for innovative applications in complex technological landscapes.



% \begin{table*}[ht]
%     \centering
%     \small
%     \begin{tabular}{l|l@{}r|l@{}r|l@{}r}
%         \toprule
%          & \multicolumn{2}{c|}{Precision} & \multicolumn{2}{c|}{Recall} & \multicolumn{2}{c}{F1 Score} \\
%         \midrule
        
%         % MPU\textsubscript{majority} & 0.33 & & 0.17 & & 0.23 & \\
%         MPU\textsubscript{base}~\cite{10871796} & 0.53 & & 0.52 & & 0.51 & \\
%         MPU\textsubscript{base} + long instructions & & \textcolor{teal}{+(0.02)} \textcolor{teal}{0.55} & & \textcolor{teal}{+(0.03)} \textcolor{teal}{0.55} & & \textcolor{teal}{+(0.04)} \textcolor{teal}{0.55} \\
%         MPU\textsubscript{base} + short instructions & & \textcolor{red}{-(0.03)} \textcolor{red}{0.50} & & \textcolor{red}{-(0.07)} \textcolor{red}{0.45} & & \textcolor{red}{-(0.04)} \textcolor{red}{0.47} \\
        
%         \midrule
        
%         UI\textsubscript{base} \cite{10.1145/3637528.3671647} & 0.52 & & 0.74 & & 0.61 & \\ % we use the micro & GPT-4
%         UI\textsubscript{base} + long instructions & & \textcolor{teal}{+(0.04)} \textcolor{teal}{0.56} & & \textcolor{teal}{+(0.02)} \textcolor{teal}{0.76} & & \textcolor{teal}{+(0.02)} \textcolor{teal}{0.63} \\
%         UI\textsubscript{base} + short instructions & & \textcolor{red}{-(0.03)} \textcolor{red}{0.49} & & \textcolor{red}{-(0.04)} \textcolor{red}{0.70} & & \textcolor{red}{-(0.03)} \textcolor{red}{0.58} \\
        
%         \midrule
        
%         CD\textsubscript{base} \cite{10.1145/3637528.3671647} & 0.44 & & 0.75 & & 0.56 & \\ % we use the micro & GPT-4
%         CD\textsubscript{base} + long instructions & & \textcolor{black}{(0.00)} \textcolor{black}{0.44} & & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.76} & & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.57} \\ 
%         CD\textsubscript{base} + short instructions & & \textcolor{red}{-(0.01)} \textcolor{red}{0.43} & & \textcolor{red}{-(0.01)} \textcolor{red}{0.74} & & \textcolor{red}{-(0.01)} \textcolor{red}{0.55} \\ 
        
%         \midrule
        
%         QIC\textsubscript{base} \cite{javadi2024llmbasedweaksupervisionframework} & 0.48 & & 0.10 & & 0.84 & \\ % we use the Claude-v3 Sonnet
%         QIC\textsubscript{base} + long instructions & & \textcolor{teal}{+(0.06)} \textcolor{teal}{0.54} & & \textcolor{teal}{+(0.10)} \textcolor{teal}{0.20} & & \textcolor{teal}{+(0.08)} \textcolor{teal}{0.92} \\ 
%         QIC\textsubscript{base} + short instructions & & \textcolor{red}{-(0.06)} \textcolor{red}{0.42} & & \textcolor{red}{-(0.02)} \textcolor{red}{0.08} & & \textcolor{red}{-(0.04)} \textcolor{red}{0.80} \\ 
        
%         \midrule
        
%         % SD\textsubscript{majority} & 0.50 & & 0.28 & & 0.36 & \\
%         SD\textsubscript{base} \cite{10.1145/3696271.3696294} & 0.67 & & 0.66 & & 0.66 & \\
%         SD\textsubscript{base} + long instructions & & \textcolor{teal}{+(0.06)} \textcolor{teal}{0.73} & & \textcolor{teal}{+(0.04)} \textcolor{teal}{0.70} & & \textcolor{teal}{+(0.05)} \textcolor{teal}{0.71} \\
%         SD\textsubscript{base} + short instructions & & \textcolor{red}{-(0.03)} \textcolor{red}{0.64} & & \textcolor{red}{-(0.01)} \textcolor{red}{0.65} & & \textcolor{red}{-(0.02)} \textcolor{red}{0.64} \\
        
%         \midrule
        
%         % EI\textsubscript{majority} & 0.17 & & 0.05 & & 0.08 & \\
%         EI\textsubscript{base} \cite{10.1145/3696271.3696292} & 0.49 & & 0.50 & & 0.48 & \\
%         EI\textsubscript{base} + long instructions & & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.50} & & \textcolor{black}{(0.00)} \textcolor{teal}{0.50} & & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.49} \\
%         EI\textsubscript{base} + short instructions & & \textcolor{red}{-(0.04)} \textcolor{red}{0.45} & & \textcolor{red}{-(0.06)} \textcolor{red}{0.44} & & \textcolor{red}{-(0.05)} \textcolor{red}{0.43} \\
        
%         \midrule
        
%         % FSA\textsubscript{majority} & 0.33 & & 0.13 & & 0.19 & \\
%         FSA\textsubscript{base} \cite{10.1145/3696271.3696299} & 0.66 & & 0.75 & & 0.67 & \\
%         FSA\textsubscript{base} + long instructions & & \textcolor{teal}{+(0.02)} \textcolor{teal}{0.68} & & \textcolor{teal}{+(0.08)} \textcolor{teal}{0.83} & & \textcolor{teal}{+(0.05)} \textcolor{teal}{0.72} \\
%         FSA\textsubscript{base} + short instructions & & \textcolor{red}{-(0.05)} \textcolor{red}{0.61} & & \textcolor{red}{-(0.08)} \textcolor{red}{0.67} & & \textcolor{red}{-(0.07)} \textcolor{red}{0.60} \\

%         \midrule
        
%         % TSB
        
%         TSB\textsubscript{base} \cite{zhang2024empowering} & 0.36 & & 0.49 & & 0.42 & \\
%         TSB\textsubscript{base} + long instructions & & \textcolor{teal}{+(0.05)} \textcolor{teal}{0.41} & & \textcolor{teal}{+(0.09)} \textcolor{teal}{0.58} & & \textcolor{teal}{+(0.07)} \textcolor{teal}{0.49} \\
%         TSB\textsubscript{base} + short instructions & & \textcolor{red}{-(0.02)} \textcolor{red}{0.34} & & \textcolor{red}{-(0.05)} \textcolor{red}{0.44} & & \textcolor{red}{-(0.03)} \textcolor{red}{0.39} \\

%         \midrule
%         DD\textsubscript{base} \cite{wei2024enhancingdiseasedetectionradiology} & 0.83 & & 0.89 & & 0.86 & \\
%         DD\textsubscript{base} + long instructions & & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.84} & & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.90} & & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.87} \\
%         DD\textsubscript{base} + short instructions & & \textcolor{red}{-(0.08)} \textcolor{red}{0.74} & & \textcolor{red}{-(0.12)} \textcolor{red}{0.77} & & \textcolor{red}{-(0.09)} \textcolor{red}{0.77} \\
                
%         \bottomrule
%         \addlinespace
%     \end{tabular}
%     \caption{Experimental results on domain specific tasks with different prompt lengths.}
%     \label{table:results}
% \end{table*}


\begin{table*}
    \hfill
    \begin{minipage}[l]{0.47\textwidth}
        \centering
    \small
    \begin{tabular}{l|r|r|r|}
        \toprule
         & \multicolumn{1}{c|}{Precision} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{F1 Score} \\ 
        \midrule
        
        % MPU\textsubscript{majority} & 0.33 & & 0.17 & & 0.23 & \\
        MPU\textsubscript{base}~\cite{10871796} & 0.53 & 0.52 & 0.51  \\
        MPU\textsubscript{base} + LI & \textcolor{teal}{+(0.02)} \textcolor{teal}{0.55} & \textcolor{teal}{+(0.03)} \textcolor{teal}{0.55} & \textcolor{teal}{+(0.04)} \textcolor{teal}{0.55} \\
        MPU\textsubscript{base} + SI & \textcolor{red}{-(0.03)} \textcolor{red}{0.50} & \textcolor{red}{-(0.07)} \textcolor{red}{0.45} & \textcolor{red}{-(0.04)} \textcolor{red}{0.47} \\
        
        \midrule
        
        UI\textsubscript{base} \cite{10.1145/3637528.3671647} & 0.52 & 0.74 & 0.61 \\ % we use the micro & GPT-4
        UI\textsubscript{base} + LI & \textcolor{teal}{+(0.04)} \textcolor{teal}{0.56} & \textcolor{teal}{+(0.02)} \textcolor{teal}{0.76} & \textcolor{teal}{+(0.02)} \textcolor{teal}{0.63} \\
        UI\textsubscript{base} + SI & \textcolor{red}{-(0.03)} \textcolor{red}{0.49} & \textcolor{red}{-(0.04)} \textcolor{red}{0.70} & \textcolor{red}{-(0.03)} \textcolor{red}{0.58} \\
        
        \midrule
        
        CD\textsubscript{base} \cite{10.1145/3637528.3671647} & 0.44 & 0.75 & 0.56 \\ % we use the micro & GPT-4
        CD\textsubscript{base} + LI& \textcolor{black}{(0.00)} \textcolor{black}{0.44} & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.76} & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.57} \\ 
        CD\textsubscript{base} + SI & \textcolor{red}{-(0.01)} \textcolor{red}{0.43} & \textcolor{red}{-(0.01)} \textcolor{red}{0.74} & \textcolor{red}{-(0.01)} \textcolor{red}{0.55} \\ 
        
        \midrule
        
        QIC\textsubscript{base} \cite{javadi2024llmbasedweaksupervisionframework} & 0.48 & 0.10 & 0.84 \\ % we use the Claude-v3 Sonnet
        QIC\textsubscript{base} + LI & \textcolor{teal}{+(0.06)} \textcolor{teal}{0.54} & \textcolor{teal}{+(0.10)} \textcolor{teal}{0.20} & \textcolor{teal}{+(0.08)} \textcolor{teal}{0.92} \\ 
        QIC\textsubscript{base} + SI & \textcolor{red}{-(0.06)} \textcolor{red}{0.42} & \textcolor{red}{-(0.02)} \textcolor{red}{0.08} & \textcolor{red}{-(0.04)} \textcolor{red}{0.80} \\

        \midrule
        
        % SD\textsubscript{majority} & 0.50 & & 0.28 & & 0.36 & \\
    SD\textsubscript{base} \cite{10.1145/3696271.3696294} & 0.67 & 0.66 & 0.66 \\
        SD\textsubscript{base} + LI & \textcolor{teal}{+(0.06)} \textcolor{teal}{0.73} & \textcolor{teal}{+(0.04)} \textcolor{teal}{0.70} & \textcolor{teal}{+(0.05)} \textcolor{teal}{0.71} \\
        SD\textsubscript{base} + SI & \textcolor{red}{-(0.03)} \textcolor{red}{0.64} & \textcolor{red}{-(0.01)} \textcolor{red}{0.65} & \textcolor{red}{-(0.02)} \textcolor{red}{0.64} \\
        
        \bottomrule
        \addlinespace
        % \multicolumn{4}{l}{} \\
        % \multicolumn{4}{l}{} \\
        
    \end{tabular}
    % \caption{Task 1-5}
    \end{minipage}
    \hspace{-1.0cm}
    \hfill
    \begin{minipage}[l]{0.47\textwidth}
        \centering
    \small
    \begin{tabular}{l|r|r|r}
        \toprule
         & \multicolumn{1}{c|}{Precision} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c}{F1 Score} \\ 
        \midrule
        
        EI\textsubscript{base} \cite{10.1145/3696271.3696292} & 0.49 & 0.50 & 0.48 \\
        EI\textsubscript{base} + LI & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.50} & \textcolor{black}{(0.00)} \textcolor{teal}{0.50} & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.49} \\
        EI\textsubscript{base} + SI & \textcolor{red}{-(0.04)} \textcolor{red}{0.45} & \textcolor{red}{-(0.06)} \textcolor{red}{0.44} & \textcolor{red}{-(0.05)} \textcolor{red}{0.43} \\
        
        \midrule
        
        % FSA\textsubscript{majority} & 0.33 & & 0.13 & & 0.19 & \\
        FSA\textsubscript{base} \cite{10.1145/3696271.3696299} & 0.66 & 0.75 & 0.67\\
        FSA\textsubscript{base} + LI & \textcolor{teal}{+(0.02)} \textcolor{teal}{0.68} & \textcolor{teal}{+(0.08)} \textcolor{teal}{0.83} & \textcolor{teal}{+(0.05)} \textcolor{teal}{0.72} \\
        FSA\textsubscript{base} + SI & \textcolor{red}{-(0.05)} \textcolor{red}{0.61} & \textcolor{red}{-(0.08)} \textcolor{red}{0.67} & \textcolor{red}{-(0.07)} \textcolor{red}{0.60} \\

        \midrule
        
        % TSB
        
        TSB\textsubscript{base} \cite{zhang2024empowering} & 0.36 & 0.49 & 0.42 \\
        TSB\textsubscript{base} + LI & \textcolor{teal}{+(0.05)} \textcolor{teal}{0.41} & \textcolor{teal}{+(0.09)} \textcolor{teal}{0.58} & \textcolor{teal}{+(0.07)} \textcolor{teal}{0.49} \\
        TSB\textsubscript{base} + SI & \textcolor{red}{-(0.02)} \textcolor{red}{0.34} & \textcolor{red}{-(0.05)} \textcolor{red}{0.44} & \textcolor{red}{-(0.03)} \textcolor{red}{0.39} \\

        \midrule
        DD\textsubscript{base} \cite{wei2024enhancingdiseasedetectionradiology} & 0.83 & 0.89 & 0.86 \\
        DD\textsubscript{base} + LI & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.84} & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.90} & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.87} \\
        DD\textsubscript{base} + SI & \textcolor{red}{-(0.08)} \textcolor{red}{0.74} & \textcolor{red}{-(0.12)} \textcolor{red}{0.77} & \textcolor{red}{-(0.09)} \textcolor{red}{0.77} \\
        \midrule

        \multicolumn{4}{l}{} \\
        \multicolumn{4}{l}{\textbf{Note:} LI = Long Instructions, SI = Short Instructions} \\
        \multicolumn{4}{l}{} \\

        
       
        % DD\textsubscript{base} \cite{wei2024enhancingdiseasedetectionradiology} & 0.83 & 0.89 & 0.86 \\
        % DD\textsubscript{base} + LI & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.84} & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.90} & \textcolor{teal}{+(0.01)} \textcolor{teal}{0.87} \\
        % DD\textsubscript{base} + SI & \textcolor{red}{-(0.08)} \textcolor{red}{0.74} & \textcolor{red}{-(0.12)} \textcolor{red}{0.77} & \textcolor{red}{-(0.09)} \textcolor{red}{0.77} \\

         \bottomrule
        \addlinespace
        % \multicolumn{4}{l}{} \\
        % \multicolumn{4}{l}{} \\
    \end{tabular}
    % \caption{Task 6-9}
    \end{minipage}
    % \vspace{0.3cm}
    % \begin{minipage}[t]{0.47\textwidth}
    % \centering
    % \small
    % \begin{tabular}{lrrr}
    %     \toprule
    %     \multicolumn{3}{l}{\textbf{Note:}} \\
    %     LI & Long Instructions & \\
    %     SI & Short Instructions & \\
    %     \bottomrule
    % \end{tabular}
    % \end{minipage}
    % \vfill
    % \begin{minipage}[l]{0.47\textwidth}
    %     \centering
    %     \small
    %     \begin{tabular}{|lrrr}
         
    %     \midrule
    %     Note & LI & Long Instructions & \\
    %          & SI & Short Instructions & \\
    %          &    &                    & \\
    %     \bottomrule
    %     \addlinespace
    %     \end{tabular}
    % \end{minipage}
    % \hspace{0cm}
    % \begin{minipage}[l]{0.47\textwidth}
    %     \centering
    % \small
    % \begin{tabular}{|lrrr}
         
    %     \midrule
    %     Note & LI & Long Instructions & \\
    %          & SI & Short Instructions & \\
    %          &    &                    & \\
    %     \bottomrule
    %     \addlinespace
    % \end{tabular}
    % \end{minipage}
    \caption{Experimental results on domain specific tasks with different prompt lengths. }
    \label{table:results}
\end{table*}



        
\subsection{Domain-specific Tasks}


MPU aims to classify monetary policy statements as hawkish, dovish, or neutral, where hawkish signals tightening measures like higher interest rates, dovish suggests easing policies to stimulate the economy, and neutral reflects no significant stance. 
UI classifies the users' intent based on what they say.
CD categorizes conversations into specific domains or topics, such as healthcare, technology, or finance, for more accurate contextual analysis. 
QIC determines the underlying intent behind user queries, such as whether the query seeks information, navigational guidance, or specific documents, enabling more efficient document retrieval systems. 
SD focuses on the sarcasm that may potentially exist in people's words.
EI detects specific emotions---anger, joy, sadness, surprise, fear, or love---expressed in a given sentence to better understand emotional tone and context.
FSA aims to evaluate financial texts to determine whether the sentiment conveyed is positive, negative, or neutral, helping assess market outlooks or economic conditions. 
SD focuses on identifying whether a statement contains sarcasm, which is crucial for understanding the true intent of a speaker or writer. 
TSB studies the technical performances, such as end-to-end delay and throughput, which are significant to domain services such as online video and autonomous driving.
Finally, DD aims to extract abnormal findings from radiology reports corresponding to ICD-10 codes.
%a reasonable example LLM prediction, which offers an innovative approach to approximate system performance and reduce the need for exhaustive measurements while maintaining reasonable accuracy. 
%Specifically, we choose end-to-end delay as our domain task, a critical performance metric, directly influencing user experience for latency-sensitive services such as online video and autonomous driving. 



\subsection{Experiment Design}


In our experiment, each group consists of three settings: default prompt length\footnote{The default prompt length can be viewed as ``mid length''.}, short instruction, and long instruction. 
The default prompts that we experimented with are the prompts provided in the original papers.
We define short instructions as those containing less than 50\% tokens of the default prompt, typically only describing the task name. 
In contrast, long instructions contain at least 200\% tokens of the default prompt, providing not only requirements but also background knowledge and experimental conditions that can help in answering questions. 
For TSB, while the prompts are not explicitly provided in the original paper, the default instructions cover all the conditions that were introduced in the referenced work, such as hardware model, software version, and experimental environments. The rationale behind choosing these base works is their popularity in their domains.
To the best of our knowledge, these works are the most recent works leveraging LLMs in their domains.
This design allows us to investigate how different prompt lengths impact LLMs' performance and ability to leverage contextual information.


To evaluate our experiment results, we define a correct case for each scenario. For classification problems, such as MPU, FSA, and SD, a case is considered correct if LLMs produce the same decision as the ground truth. Regarding the engineered system where the prediction may cover a range, we divide the possible range (e.g., $[0,100]$) into ten equal segments, offering a structured framework to evaluate the results compared to the ground truth. We treat segments fairly without considering them reliable or reasonable, as our goal is to ensure an unbiased performance assessment \cite{URLLC}. %Furthermore, different scenarios may demand varying levels of reliability and latency~\cite{URLLC}.


We use the weighted average precision, recall, and F1-score for each experiment as the evaluation metrics, where the weight is determined by the number of instances for each class. 
Precision measures how many positive predictions are correct (True Positive, TP for short. False Positive, FP for short.). 
Recall is a measure of how many of the positive cases the classifier correctly predicted over all the positive cases in the test data (False Negative, FN for short).
F1 score is a measure combining both precision and recall.

%backup
% \subsection{Domain-specific Knowledge \textcolor{red}{delete}}

% \subsubsection{Network}

% \marginpar{\textcolor{red}{too long, make it short, move some to intro and related work}}
% Networks play an indispensable role in our daily lives, encompassing technologies like the internet and cellular networks. For this study, we focus on cellular networks as a case study to evaluate the effectiveness of LLM-generated prompt responses. 

% In cellular networks, end-to-end delay is a critical performance metric, directly influencing user experience for latency-sensitive services such as autonomous driving and Ultra-Reliable and Low-Latency Communications. Organizations like 3GPP and ETSI have released numerous specifications defining scenarios and performance metrics, including E2E delay, throughput, etc. Many processes in wireless networks, such as Radio Link Control queuing, resource scheduling, and packet segmentation, contribute significantly to E2E delay. However, the interplay between system components and environmental conditions can impact latency negatively. Identifying and quantifying these metrics is essential to optimize the system for better performance.

% \subsubsection{Benefit of being prompted by LLMs \textcolor{red}{delete}}

% Traditional performance measurement in cellular networks can be resource-intensive and time-consuming. For instance, evaluating E2E delay often requires building the system, choosing test spaces, and performing extensive testing and monitoring, involving significant costs and time. Leveraging LLMs like LLaMA 3 offers an innovative approach to approximating system performance. By designing carefully tailored prompt instructions, we can streamline performance analysis, reducing the need for exhaustive measurements while maintaining reasonable accuracy. Specifically, the advantages are:

% \noindent \emph{Efficiency in Data Processing.} 
% LLMs can analyze and synthesize large volumes of data, providing approximations and insights that guide decision-making without requiring exhaustive traditional testing.

% \noindent \emph{Reduced Dependency on Physical Testing.}
% By using well-designed prompts, LLMs can simulate or approximate system behaviors, minimizing the need for costly and time-consuming physical setups.

% \noindent \emph{Adaptability to Dynamic Scenarios.}
% Adjusting prompts allows LLMs to explore various system parameters, making them versatile tools for analyzing rapidly changing environments.

% \noindent \emph{Accelerated R\&D Cycles.}
% LLMs can quickly provide insights, speeding up the research, development, and iteration processes, which is critical for cutting-edge technologies.

% \noindent \emph{Cost-effectiveness.}
% Reduced reliance on hardware and traditional testing infrastructure can lead to significant cost savings over time.




% \subsection{Experiment Design2 \textcolor{red}{delete}}
% \marginpar{\textcolor{red}{other tasks are removed?}}
% In our experiment, we selected E2E delay as a key performance indicator to predict using LLMs. The environment chosen for this study is a cellular network deployed on Unmanned Aerial Vehicles (UAVs). This setup is particularly sensitive to system integration challenges due to constraints such as limited space, power, and dynamic operating conditions on UAVs.


% \subsubsection{Prompt Instructions.}
% To structure the analysis, we divide the system components into two categories: hardware and software. Based on a domain-specific review, we summarize these components in two perspectives.

% \begin{itemize}
%     \item Hardware: includes the computing platform and Radio Frequency (RF) end.
%     \item Software: includes the operating system, hardware driver (optional) and cellular network software.
% \end{itemize}


% Regarding the environment, we identify space and distance as critical factors influencing system performance. Consequently, they are incorporated as a key parameter in the prompt instructions.
% Thus, our prompt instructions are designed to integrate details about hardware, software, and environmental factors, enabling LLMs to provide comprehensive insights into the system's performance. The prompt design is shown in \ref{tab:prompt instruction}.

% %\subsubsection{Expected Output of LLM}
% %By providing prompt instructions as input to LLM, we anticipate receiving output from the LLM that significantly shows the approximated performance. Specifically, since regular E2E delays are within 200 ms, we divide the range 
% $[0,200]$ into ten equal segments, offering a precise framework for future evaluation. Notably, we do not predefine which segment is considered reliable or reasonable, as our goal is to ensure an unbiased assessment of performance. Furthermore, different scenarios may demand varying levels of reliability and latency ~\cite{URLLC}.% and ~\cite{5gservice}.
% \marginpar{\textcolor{red}{table 2 need to be changed}}



\section{Experimental Results}


Based on the results presented in Table~\ref{table:results}, we observe distinct trends in how different instruction strategies impact the performance of precision, recall, and F1-score across domain-specific tasks.\footnote{To ensure the generalizability of our results, we run each experiment 10 times under the same experiment setting, and report the average results in Table~\ref{table:results}.}

\noindent \textbf{Results of default prompt length.} The baseline performance metrics vary significantly across tasks. The domains of SD, FSA, and TSB exhibit strong baseline performance, with F1-scores around 0.67.
The domain of QIC shows high F1-score (0.84) but low recall (0.10), indicating potential issues with sensitivity to positive cases, while the domain of DD is excellent with precision, recall, and F1-score above 0.80. The domains of EI and CD exhibit relatively lower baseline performance, with F1-scores of 0.48 and 0.56, respectively.

\noindent \textbf{Results of short prompts.} Short instructions negatively affect performances in all tasks compared to baseline performance. In QIC and DD domains, performances significantly decrease, with precision dropping by 0.06 and 0.08, recall by 0.02 and 0.12, and F1-score by 0.04 and 0.09, while CD maintains slight drops with precision, recall, and F1-score all by 0.01. These results suggest that short instructions can not fully leverage LLMs' capabilities, particularly in sufficient details-needed tasks where contextual information (QIC) or special fields (DD) is essential.

\noindent \textbf{Results of long prompts.} It is worth noting that long instructions generally improve the performance metrics across all tasks on all experimented domains compared to base performance. Specifically, in EI and DD, minimal improvements are observed, with precision and F1-score increasing by +0.01 each, while QIC and TSB show the biggest improved performances due to their detail-sensitive tasks. %LLMs' capabilities in these nine domains are verified, particularly in tasks requiring detailed background knowledge (e.g., QIC and TSB). 
Moreover, even with the detailed background knowledge provided in the long prompt, LLMs still struggle in these tasks, as their F1 scores are far behind 1.0 (which represents human-level understanding ability).


\section{Conclusions}

In this study, we conducted comprehensive experiments to assess the effect of prompt length on LLMs' performance in various domain-specific tasks. Our findings indicate that longer prompts generally enhance model performance, while shorter prompts can be detrimental. Furthermore, despite providing extensive background knowledge of the prompts, LLMs still struggle with challenging domain-specific tasks, highlighting the need for a deeper understanding of prompt phrasing.


Our research agenda will be centered on exploring how different prompting techniques influence LLMs' performance in domain-specific tasks. Recent studies have shown that modifying the phrasing of questions in prompts and adjusting the number of examples provided in instructions can significantly improve performance in specialized domains, including spatial information extraction~\cite{xiao-etal-2024-analyzing}, math reasoning~\cite{10.5555/3600270.3602070, yao-etal-2024-got}, short interest understanding~\cite{10.1145/3655497.3655500}, and corporate event prediction~\cite{10458651}. These findings highlight the potential benefits of optimizing prompting techniques to enhance LLMs' performance in specific tasks.

\bibliography{cm1}
\bibliographystyle{IEEEtran}

\end{document}

