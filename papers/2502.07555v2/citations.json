[
  {
    "index": 0,
    "papers": [
      {
        "key": "devlin2018bert",
        "author": "Devlin, Jacob",
        "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
      },
      {
        "key": "liu2019roberta",
        "author": "Liu, Yinhan",
        "title": "Roberta: A robustly optimized bert pretraining approach"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "izacard2022unsuperviseddenseinformationretrieval",
        "author": "Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave",
        "title": "Unsupervised Dense Information Retrieval with Contrastive Learning"
      },
      {
        "key": "chen_bge_2024",
        "author": "Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng",
        "title": "{BGE} {M3}-{Embedding}: {Multi}-{Lingual}, {Multi}-{Functionality}, {Multi}-{Granularity} {Text} {Embeddings} {Through} {Self}-{Knowledge} {Distillation}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wei2024uniir",
        "author": "Wei, Cong and Chen, Yang and Chen, Haonan and Hu, Hexiang and Zhang, Ge and Fu, Jie and Ritter, Alan and Chen, Wenhu",
        "title": "Uniir: Training and benchmarking universal multimodal information retrievers"
      },
      {
        "key": "zhou2024vista",
        "author": "Zhou, Junjie and Liu, Zheng and Xiao, Shitao and Zhao, Bo and Xiong, Yongping",
        "title": "VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "xiao_retromae_2022",
        "author": "Xiao, Shitao and Liu, Zheng and Shao, Yingxia and Cao, Zhao",
        "title": "{RetroMAE}: {Pre}-{Training} {Retrieval}-oriented {Language} {Models} {Via} {Masked} {Auto}-{Encoder}"
      },
      {
        "key": "liu_retromae-2_2023",
        "author": "Liu, Zheng and Xiao, Shitao and Shao, Yingxia and Cao, Zhao",
        "title": "{RetroMAE}-2: {Duplex} {Masked} {Auto}-{Encoder} {For} {Pre}-{Training} {Retrieval}-{Oriented} {Language} {Models}"
      },
      {
        "key": "wang_simlm_2023",
        "author": "Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu",
        "title": "{SimLM}: {Pre}-training with {Representation} {Bottleneck} for {Dense} {Passage} {Retrieval}"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "xiong_approximate_2020",
        "author": "Xiong, Lee and Xiong, Chenyan and Li, Ye and Tang, Kwok-Fung and Liu, Jialin and Bennett, Paul and Ahmed, Junaid and Overwijk, Arnold",
        "title": "Approximate {Nearest} {Neighbor} {Negative} {Contrastive} {Learning} for {Dense} {Text} {Retrieval}"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "qu2020rocketqa",
        "author": "Qu, Yingqi and Ding, Yuchen and Liu, Jing and Liu, Kai and Ren, Ruiyang and Zhao, Wayne Xin and Dong, Daxiang and Wu, Hua and Wang, Haifeng",
        "title": "RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "hofstatter_efficiently_2021",
        "author": "Hofst\u00e4tter, Sebastian and Lin, Sheng-Chieh and Yang, Jheng-Hong and Lin, Jimmy and Hanbury, Allan",
        "title": "Efficiently {Teaching} an {Effective} {Dense} {Retriever} with {Balanced} {Topic} {Aware} {Sampling}"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "su2022one",
        "author": "Su, Hongjin and Shi, Weijia and Kasai, Jungo and Wang, Yizhong and Hu, Yushi and Ostendorf, Mari and Yih, Wen-tau and Smith, Noah A and Zettlemoyer, Luke and Yu, Tao",
        "title": "One embedder, any task: Instruction-finetuned text embeddings"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "ni_large_2022",
        "author": "Ni, Jianmo and Qu, Chen and Lu, Jing and Dai, Zhuyun and Hernandez Abrego, Gustavo and Ma, Ji and Zhao, Vincent and Luan, Yi and Hall, Keith and Chang, Ming-Wei and Yang, Yinfei",
        "title": "Large {Dual} {Encoders} {Are} {Generalizable} {Retrievers}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wang2022text",
        "author": "Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu",
        "title": "Text embeddings by weakly-supervised contrastive pre-training"
      },
      {
        "key": "li2023towards",
        "author": "Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan",
        "title": "Towards general text embeddings with multi-stage contrastive learning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wang2023improving",
        "author": "Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu",
        "title": "Improving Text Embeddings with Large Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "li_llama2vec_2024",
        "author": "Li, Chaofan and Liu, Zheng and Xiao, Shitao and Shao, Yingxia and Lian, Defu",
        "title": "{Llama2Vec}: {Unsupervised} {Adaptation} of {Large} {Language} {Models} for {Dense} {Retrieval}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "weller_promptriever_2024",
        "author": "Weller, Orion and Durme, Benjamin Van and Lawrie, Dawn and Paranjape, Ashwin and Zhang, Yuhao and Hessel, Jack",
        "title": "Promptriever: {Instruction}-{Trained} {Retrievers} {Can} {Be} {Prompted} {Like} {Language} {Models}"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "lee2024nv",
        "author": "Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei",
        "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models"
      },
      {
        "key": "li2024making",
        "author": "Li, Chaofan and Qin, MingHao and Xiao, Shitao and Chen, Jianlyu and Luo, Kun and Shao, Yingxia and Lian, Defu and Liu, Zheng",
        "title": "Making text embedders few-shot learners"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "gao2022precise",
        "author": "Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie",
        "title": "Precise zero-shot dense retrieval without relevance labels"
      },
      {
        "key": "zhu2023large",
        "author": "Zhu, Yutao and Yuan, Huaying and Wang, Shuting and Liu, Jiongnan and Liu, Wenhan and Deng, Chenlong and Chen, Haonan and Liu, Zheng and Dou, Zhicheng and Wen, Ji-Rong",
        "title": "Large language models for information retrieval: A survey"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "su2024bright",
        "author": "Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others",
        "title": "Bright: A realistic and challenging benchmark for reasoning-intensive retrieval"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "feng2024towards",
        "author": "Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei",
        "title": "Towards revealing the mystery behind chain of thought: a theoretical perspective"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "yao2024tree",
        "author": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",
        "title": "Tree of thoughts: Deliberate problem solving with large language models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "besta2024graph",
        "author": "Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others",
        "title": "Graph of thoughts: Solving elaborate problems with large language models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "kaplan2020scaling",
        "author": "Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario",
        "title": "Scaling laws for neural language models"
      },
      {
        "key": "hoffmann2022training",
        "author": "Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others",
        "title": "Training compute-optimal large language models"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "wu2024inference",
        "author": "Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming",
        "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving"
      },
      {
        "key": "chen2024llmcallsneedscaling",
        "author": "Lingjiao Chen and Jared Quincy Davis and Boris Hanin and Peter Bailis and Ion Stoica and Matei Zaharia and James Zou",
        "title": "Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems"
      },
      {
        "key": "sardana2023beyond",
        "author": "Sardana, Nikhil and Portes, Jacob and Doubov, Sasha and Frankle, Jonathan",
        "title": "Beyond chinchilla-optimal: Accounting for inference in language model scaling laws"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "guo2025deepseek",
        "author": "Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others",
        "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning"
      }
    ]
  }
]