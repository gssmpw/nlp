\documentclass[sigconf]{acmart}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

% \usepackage{fdsymbol, stix}
\usepackage{multirow}

\usepackage[utf8]{inputenc}
\let\Bbbk\relax         %%redefined in newtxmath.sty
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{array}
\usepackage{listings}
\usepackage{verbatim}

\definecolor{softgreen}{RGB}{15,157,88}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
% \title{PLATO: Make Your Retriever Think Before It Retrieves} 
\title{O1 Embedder: Let Retrievers Think Before Action} 

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ruiran Yan}
% \authornote{Both authors contributed equally to this research.}

% \orcid{0009-0003-3846-1435}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{University of Science and Technology of China}
  \city{Hefei}
  \country{China}
}
\email{yanruiran@mail.ustc.edu.cn} 

\author{Zheng Liu} 
\authornote{Corresponding Author.}
\affiliation{
  \institution{Beijing Academy of Artifical Intelligence}
  \city{Beijing}
  \country{China}
}
\email{zhengliu1026@gmail.com}

\author{Defu Lian}
\authornotemark[1]
\affiliation{%
  \institution{University of Science and Technology of China}
  \city{Hefei}
  \country{China}
}
\email{liandefu@ustc.edu.cn}


% llm reasoning 

\begin{abstract} 
The growing power of large language models (LLMs) has revolutionized how people access and utilize information. Notably, the LLMs excel at performing fine-grained data representation, which facilitates precise retrieval of information. They also generate high-quality answers based on external references, enabling the production of useful knowledge. The recent introduction of reasoning models, like OpenAI O1\footnote{https://openai.com/index/introducing-openai-o1-preview/} and DeepSeek R1\footnote{https://github.com/deepseek-ai/DeepSeek-R1}, marks another leap forward, highlighting LLMs' ability to think progressively before delivering final answers. This breakthrough significantly improves the ability to address complex tasks, e.g., coding and math proofs. 

Inspired by this progress, we aim to develop similar capabilities for retrieval models, which hold great promise for tackling critical challenges in the field, including multi-task retrieval, zero-shot retrieval, and tasks requiring intensive reasoning of complex relationships. With this motivation, we propose a novel approach called \textbf{O1 Embedder}, which generates useful thoughts for the input query before making retrieval for the target documents. To realize this objective, we conquer two technical difficulties. First, we design a data synthesis workflow, creating training signals for O1 Embedder by generating initial thoughts from an LLM-expert and subsequently refining them using a retrieval committee. Second, we optimize the training process, enabling a pre-trained model to be jointly fine-tuned to generate retrieval thoughts via behavior cloning and perform dense retrieval through contrastive learning. Our approach is evaluated by comprehensive experiments, where substantial improvements are achieved across 12 popular datasets, spanning both in-domain and out-of-domain scenarios. These results highlight O1 Embedder's remarkable accuracy and generalizability, paving the way for the development of next-generation IR foundation models. 

% To realize this goal, we design a data synthesis pipeline, where candidate thoughts are initially generated by an LLM-expert and subsequently refined by a retriever committee. Additionally, we also introduce a compound training workflow, where the pre-trained model is jointly fine-tuned to generate the retrieval thoughts via behavior cloning and perform dense retrieval through contrastive learning. Our approach is evaluated by comprehensive experiments, where O1 Embedder achieves substantial improvements in retrieval performance across 12 popular datasets, spanning both in-domain and out-of-domain scenarios. These results highlight O1 Embedder's remarkable accuracy and generalizability, paving the way for the development of next-generation IR foundation models. 
\end{abstract}


% \begin{CCSXML}
% <ccs2012>
%    <concept>
%        <concept_id>10002951.10003317.10003338</concept_id>
%        <concept_desc>Information systems~Retrieval models and ranking</concept_desc>
%        <concept_significance>500</concept_significance>
%        </concept>
%  </ccs2012>
% \end{CCSXML}
% \ccsdesc[500]{Information systems~Retrieval models and ranking}



\keywords{Dense Retrieval, Embedding Model, Large Language Models} 

\maketitle 

% information retrieval is important, widely used, everywhere in industry
% dense retrieval is one of the most important breakthroughs 
% dense retrieval use embedding models to transform data into vector space such that relevant data become close to each other in the embedding space 
% dense retrieval's progress was significantly accelerated after the advent of pre-trained language models 
% becomes increasingly stronger on top of advanced pre-trained model and fine-tuning algorithms, and the expansion of training scale 

% however, there are still many severe challenges about dense retrieval 
% for instance, it's difficult for embedding models to effectively generalized to new scenarios which significantly differs from its training domains, leading to limited zero-shot retrieval performance
% for example, a model fine-tuned on public domains, like ms marco, may result in sub-optimal performance when applied to a highly specialized domain, such as medical or legal retrieval. 
% most embedding models are trained to perform direct semantic matching, they are insufficient to handle tasks requiring in-depth reasoning
% e.g, the retrieval of useful information to complete a coding problem 

% while embedding models are constrained by the above challenges
% llms have made substantial progresses in deal with corresponding problems 
% when handling a tough problem: 
% - by performing long-form generation in the first place
% - in the form of cot 
% - make high-quality answers based on the thoughts
% this ability is defined as llms' test-time-scaling 
% showcased by o1, which achieves remarkable performance in challenging problems, e.g., solving programming contest and generating math proof 

% Inspired by these progresses, we propose a novel approach, called o1 embedder, 
% given a complex query 
% the model itself performs thinking first 
% generating useful clues to address the information need of the queury 
% then aggregate the original query and the thoughts 
% produce the final embedding for dense retrieval 

% to train this model is challenging given that there is no training data is available 
% as a result, we design the data synthesis framework to generate training data, following the logic of "exploration, refinement, exploitation" workflow 
% utilize a powerful llm to generate multiple candidate thoughts 
% employ a retriever committee to refine the candidate thoughts 
% the retriever committee is made of multiple pre-trained retrievers, each of which is asked to provide preferences towards the candidates 
% all candidates are ranked by majority voting, where the most preferred one is selected as the training sample 

% introduce a multi-task learning approach to train o1 embedder  
% given a training query
% a pre-trained llm is fine-tuned to generate the preferred thought in the form of behavior cloning 
% meanwhile, the llm is also trained to discriminate the relevant documents to the query via contrastive learning 
% both query and thoughts during the constrative learning process 

% comprehensive evaluations to verify our method 
% fine-tuning a llama-2 model with msmarco 
% notably outperforms the previous stoa, ie. RepLLaMA, on a variety of datasets 
% including both in-domain datasets (msmarco dev, test) and out-of-domain scenarios (from beir) 
% the effectiveness is further validated through extended experiments with various llms, including mistral, qwen, llama-3  

% enumerate the contributions here 

% \section{Introduction} 

% % \section{Introduction}

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{img/whiteboard_exported_image (8).pdf}
%   \caption{PLATO vs Normal Retrieval Model}
% \end{figure}

% A dense retrieval system is a modern information retrieval approach that utilizes dense vector representations of documents and queries to facilitate efficient and effective matching. Unlike traditional sparse retrieval methods that rely on keyword-based matching, dense retrieval employs neural networks to encode both queries and documents into high-dimensional vectors in a continuous space. These vectors capture semantic relationships, allowing the system to identify relevant documents based on their contextual similarity to the query rather than exact term matches. This method has gained popularity in various applications, including search engines, recommendation systems, and rerieval-augmented generation(RAG) frameworks, due to its ability to improve retrieval accuracy and user satisfaction. 

% In recent years, the development of large language models (LLMs) has significantly advanced the field of natural language processing, offering unprecedented capabilities in understanding and generating human-like text. Models such as GPT, Llama, and their successors have demonstrated remarkable proficiency in a wide array of tasks, from text completion and translation to more complex applications like summarization and sentiment analysis. These models leverage vast amounts of data and computational power to unify a variety of tasks that previously required unique models to do them. Building on these advancements, LLM-based retrieval models have emerged as a promising direction for improving dense retrieval systems. Many retrieval models have been inspired by LLM, using existing LLM initialization models trained with contrastive learning using larger and more extensive pairs of data to achieve more general and accurate retrieval results. 

% Despite the increasing size of dense retrieval models and the adoption of large language models (LLMs) as their backbone, these systems still face several limitations. Typically, these models operate in a "fast thinking" mode, where they generate a dense vector directly from a user's query and retrieve documents based on the similarity between this query vector and document vectors. However, user queries are often brief and ambiguous, while real-world documents tend to be detailed and precise. This discrepancy in distribution can lead to models overlooking the user's underlying intent. Furthermore, current retrieval models do not fully leverage the rich knowledge acquired during the pre-training of LLMs. The pre-training process of large models is conducted on a token-by-token basis, which fundamentally differs from the mechanisms used in representing entire queries or documents. As a result, the potential of LLMs to enhance understanding and retrieval through their pre-trained knowledge remains underutilized in dense retrieval tasks.

% % To address these challenges, there is a growing need for retrieval models that can bridge the gap between the brevity of user queries and the richness of document content. One promising approach is to incorporate a "slow thinking" mechanism, which allows models to engage in deeper contextual analysis. This involves leveraging the knowledge captured during LLM pre-training to better infer user intent and enhance retrieval accuracy. By doing so, models can move beyond simple vector similarity and begin to understand the broader context and potential implications of a query.

% To address these challenges, in this work, we introduce \textbf{PLATO}: a novel retrieval model with a "slow thinking" capability that can think about the possible relevant information to help the retrieval process. PLATO is a decoder-only LLM that not only can be used as a retrieval model, but also has some generative ability to generate relevant information that is helpful for retrieval. The "slow thinking" mechanism leverages the knowledge captured during LLM pre-training to better infer user intent and allows models to engage in deeper contextual analysis. By doing so, models can move beyond simple vector similarity and begin to understand the broader context and potential implications of a query. 
%  To do so, we first constructed a ~500k dataset based on the MS MARCO retrieval dataset by generating and filtering the responses for each query using a 70B teacher model. These data are used to train the model's generation and retrieval capabilities simultaneously. Through end-to-end training, the model not only learns how to generate information that is useful for retrieval based on its knowledge, but also learns how to utilize this information well to achieve better retrieval accuracy. In the retrieval phase, the model has two modes, which we call fast mode and slow mode. Fast mode, like other sense retrieval models, generates representation vectors for similarity retrieval in one step directly from query, which has the advantage of having less computational overhead and allowing the retrieval system to have a faster response. The advantage is that it has less computational overhead and allows the retrieval system to have a faster response. The advantage is that it has less computational overhead and allows the retrieval system to have a faster response. The slow mode generates potentially relevant information based on the internal knowledge of the model through the model's generative capabilities, which can be used to improve the retrieval results.

% In our experiment, we evaluate PLATO on 11 IR datasets, covering tasks like Web Search, Question Answering, Fact Checking.  we show PLATO in slow mode significantly outperforms the previous state-of-the-art retrieval model and also achieves competetive results in its fast mode for both supervised and zero-shot settings demonstrating strong generalization and robustness.

% In summary, the main contributions of our work are as follows:

% \begin{itemize}
%   \item  We introduce PLATO, a novel retrieval model that integrates a "slow thinking" mechanism with generative capabilities, allowing for a more nuanced understanding of user queries and enhancing retrieval accuracy.
%   \item We construct MS MARCO+ dataset and develop an end-to-end training framework that simultaneously trains PLATO on both generation and retrieval tasks. This approach enhances the model's ability to generate relevant information and utilize it effectively during the retrieval process.
%   \item We conduct comprehensive evaluations across 11 information retrieval datasets, demonstrating that PLATO in slow mode significantly outperforms existing state-of-the-art retrieval models. Additionally, we show that PLATO achieves competitive performance in its fast mode, highlighting its robustness and generalization capabilities.
% \end{itemize}


% \newpage


\section{Introduction}
Information retrieval (IR) is fundamental to many important applications, such as search engines and question answering systems~\cite{karpukhin2020dense, kobayashi2000information}. Recently, it draws even higher attention because of its critical role in augmenting large language models (LLMs) with external knowledge~\cite{zhu2023large,10.1145/3589335.3641299,zhao2024dense}, a paradigm known as retrieval-augmented generation (RAG)~\cite{gao2023retrieval, zhang2023retrieve, zhao2024retrieval}. Over the past decade, IR techniques have experienced tremendous progresses. One important breakthrough is made by dense retrieval, where relevant data can be effectively retrieved via vector search. With the popularity of open-source models in this field \cite{neelakantan_text_2022,wang2023improving,xiao_c-pack_2024}, dense retrieval has become a go-to option for realizing retrieval applications in reality. 

% which represents data as discriminative embeddings and enables relevant data to be effectively retrieved through vector search. The development of dense retrieval was significantly accelerated on top of pre-trained language models~\cite{devlin2018bert,liu2019roberta,raffel2020exploring,touvron2023llama2openfoundation}. With the introduction of stronger pre-trained models and the scale-up of fine-tuning, dense retrieval is made increasingly powerful and accessible to the community users. 

However, dense retrieval is still subject to many limitations in this stage. First, existing methods struggle with \textit{zero-shot retrieval} tasks in unseen scenarios, which differ significantly from their source domains. For instance, a well-trained embedding model from general datasets is prone to a limited performance when applied to a specialized problem, like medical or legal case retrieval~\cite{maia201818,li2024automireffectivezeroshotmedical,10.1145/3451964.3451965}. Second, the existing models are insufficient to discriminate \textit{complex relationships}, as they cannot be identified directly from semantic meaning. For example, the retrieval of useful code snippets for a computer program, or the retrieval of evidence to a multi-hop reasoning problem~\cite{li2024coircomprehensivebenchmarkcode, husain2019codesearchnet, yang2018hotpotqa, lee2022generativemultihopretrieval}. 

The above challenges can benefit from a prior deep reasoning process, instead of making direct judgment of relevance. Recently, remarkable advancements were made in this direction with the introduction of reasoning LLMs, such as OpenAI O1, O3, and DeepSeek R1 \cite{guo2025deepseek}. Particularly, when a complex task is presented, the LLM is prompted to generate long-form thoughts about the problem in the first place. Through this process, the LLM can progressively get close to the correct solution, thus enabling the production of high-quality answers in the end. This operation is conceptualized as the test-time-scaling by recent studies \cite{snell2024scaling}, which has driven major improvements in solving complex problems, such as coding and mathematical proofs. 

With the above inspiration, we propose \textbf{O1 Embedder}, which is designed to introduce a slow-thinking capability for embedding models, akin to that of LLMs. Our approach integrates two essential functions within a single model: \textbf{Thinking} and \textbf{Embedding}. First, it generates useful thoughts towards the input query, which explicitly uncovers the hidden information needs about the query. 
Secondly, it produces a discriminative embedding for the query and the generated thoughts. By incorporating both elements, the resulting embedding enables precise retrieval of relevant documents that are challenging to identify using the query alone. 


% the query can be matched with its relevant documents in a straightforward way. 

% which can help to address its underlying information needs. Secondly, it produces a discriminative embedding for the query and thought, which can precisely retrieve the relevant documents. 

% With the incorporation of useful thoughts, the query can be matched with its relevant documents in a straightforward way, thus helping to overcome the existing challenges regarding zero-shot or complex retrieval problems. 

The training of O1 Embedder is technically challenging given the absence of appropriate long-form thought data for embedding models. To solve this problem, we introduce a \textbf{Data Synthesis} method following an ``\textbf{Exploration}-\textbf{Refinement}'' process. First, we prompt an LLM to explore initial thoughts for a query. Next, we employ a retrieval committee to refine the initial thoughts. Each committee member scores the relevance between initial thoughts and the target document, which indicates their usefulness in retrieving the target document. With the collection of all members' scoring results, the golden thought is selected by majority voting and added to the training set. \textit{As such, we automatically create long-form thoughts of the best retrieval utility for O1 Embedder.}

% . Finally, the golden thought is adopted as a supervision signal for O1 Embedder's fine-tuning. 

Building on well-curated long-form thought data, we introduce a \textbf{Multi-Task Training Method}, which fine-tunes a pre-trained LLM into O1 Embedder. Our method introduces two parallel training tasks. One applies supervised fine-tuning for the LLM, which enables the generation of optimal thoughts for an input query. The other one employs contrastive learning, which produces discriminative embeddings to retrieve relevant documents for a thought-augmented query. \textit{With proper optimizations in loss computation and training workflow, these two tasks are seamlessly unified in a cost-efficient manner, leading to effective development of thinking and embedding capabilities throughout the training.} 

The effectiveness of O1 Embedder is comprehensive evaluated. In our experiment, O1 Embedder achieves \textbf{a substantial improvement over existing methods} across a broad range of retrieval tasks, especially those requiring complex reasoning. O1 Embedder also demonstrates \textbf{a strong generalization ability} when applied to out-of-domain scenarios. Finally, O1 Embedder well maintains \textbf{a superior performance across various LLM backbones}, like Llama~\cite{touvron2023llama2openfoundation, grattafiori2024llama3herdmodels}, Mistral~\cite{jiang2023mistral7b}, and Qwen~\cite{grattafiori2024llama3herdmodels}. Our model and code will be made publicly available to advance research in this field. 

To summarize, the main contributions of this paper are highlighted by the following perspectives. 
\begin{itemize}
    \item We propose O1 Embedder, which generates useful thoughts about the input query before producing discriminative embeddings for dense retrieval. To the best of our knowledge, O1 Embedder is the first practice to equip embedding models with thinking capabilities, offering promising insights for future research. 
    \item We design an exploration-refinement approach, producing long-form thoughts with the optimal retrieval utility. We also optimize the multitask training method, which effectively fine-tunes a pre-trained LLM into O1 Embedder. 
    \item We perform comprehensive experiments for a detailed analysis O1 Embedder, whose result verifies the effectiveness and broad applicability of our method. 
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.80\linewidth]{img/o1_embed_1.pdf}
  \vspace{-5pt}
  \caption{O1 Embedder. First of all, the model generates the thoughts about the question (thinking). Next, the model produces the embedding for dense retrieval (retrieval).}   
  \vspace{-15pt} 
\end{figure} 


\section{Related Work} 
% In this section, we make discussions on the related literature from two perspectives: the progress on dense retrieval, and the introduction of reasoning capability to LLMs.  

\subsection{Dense Retrieval}
Dense retrieval has made significant strides in retrieval precision, driven by the advancements in foundation models and training techniques. Early breakthroughs involved fine-tuning preliminary pre-trained models, such as BERT and RoBERTa \cite{devlin2018bert,liu2019roberta}, for dense retrieval, which already demonstrated competitive performance compared to traditional methods like BM25. At the same time, the scope of dense retrieval was substantially expanded thanks to the adoption of multi-lingual \cite{izacard2022unsuperviseddenseinformationretrieval,chen_bge_2024} and multi-modal pre-trained models \cite{wei2024uniir,zhou2024vista}. The introduction of more advanced training strategies, such as retrieval-oriented adaptation \cite{xiao_retromae_2022,liu_retromae-2_2023,wang_simlm_2023}, hard negative mining \cite{xiong_approximate_2020}, batch size expansion \cite{qu2020rocketqa}, and knowledge distillation from cross-encoders \cite{hofstatter_efficiently_2021}, has continually contributed to the improvement of dense retrieval's performance. 

In addition to the improvement on retrieval accuracy, it becomes increasingly emphasized to develop multi-task retrievers for general-purpose retrieval applications. Recent studies showed that the retrievers' generalization ability can be substantially enhanced by scaling-up the training scale \cite{su2022one} and model architecture \cite{ni_large_2022}. Based on these inspirations, people have made significant expansion of pre-training and fine-tuning tasks, leading to a series of popular retrievers for general-purpose applications, such as BGE, E5, and GTE \cite{wang2022text,li2023towards}. Meanwhile, people also introduce large language models (LLMs) as the retrievers' backbones, which brings forth significant improvements in retrieval performance. For example, RepLLaMA presents a powerful dense retriever by directly fine-tuning a pre-trained Llama \cite{wang2023improving}. Llama2Vec further enhances RepLLaMA by incorporating unsupervised adaptation of the pre-trained Llama \cite{li_llama2vec_2024}. Promptriver~\cite{weller_promptriever_2024}, built on RepLLaMA, equips the retrieval model with the capability to follow instructions. Methods like NV-Embed and ICL-Embedder achieves additional improvement through continual training with extensive fine-tuning data \cite{lee2024nv,li2024making}. Today, LLM-powered retrievers have dominated nearly all major benchmarks in IR-related evaluation. 

Despite these remarkable advancements, existing methods are primarily designed for direct semantic matching in popular applications like web search and question-answering. They still face challenges with zero-shot retrieval in completely new scenarios that differ significantly from their source domains \cite{gao2022precise,zhu2023large}. In addition, they are insufficient for more complex retrieval tasks which require intensive reasoning to identify semantic relationships \cite{su2024bright}. 

\subsection{LLMs' Reasoning Ability} 
The reasoning capabilities of large language models (LLMs) have been significantly enhanced with techniques that simulate human-like problem-solving processes. A major breakthrough in this area is Chain-of-Thought (CoT) \cite{wei2022chain}, which prompts LLMs to tackle complex problems by decomposing them into multiple reasoning steps. Building on this progress, the Self-Consistency method improves reasoning robustness by sampling multiple reasoning paths from the LLM and selecting the final answer through majority voting \cite{feng2024towards}. For scenarios requiring more exploratory reasoning, the Tree of Thoughts (ToT) method \cite{yao2024tree} extends CoT by structuring the problem-solving process as a tree. At each node, the LLM generates candidate intermediate steps, evaluates their feasibility, and backtracks from dead ends. Further advancing this paradigm, Graph of Thoughts (GoT) \cite{besta2024graph} replaces the tree structure with a directed acyclic graph (DAG), enabling LLMs to merge or refine reasoning steps as needed. The reasoning capability of large language models (LLMs), or the "think before action" workflow, represents a new paradigm that sets them apart from traditional language models. In addition to the usual strategies of scaling model size, datasets, and training computation \cite{kaplan2020scaling, hoffmann2022training}, the expansion of inference computation, or test-time scaling \cite{wu2024inference, chen2024llmcallsneedscaling, sardana2023beyond}, becomes another important factor in driving the improvement of LLMs. This capability has been significantly enhanced and showcased by recent reasoning-capable LLMs, such as OpenAI's O1 and O3, DeepSeek's R1 \cite{guo2025deepseek}, and Gemini 2.0\footnote{https://deepmind.google/technologies/gemini/flash-thinking/}. These models adopt a "slow-thinking" approach when handling complex problems: instead of providing an immediate answer, they first generate verbose, structured reasoning before arriving at a final solution. This method has allowed LLMs to achieve elite-level performance in areas like coding and mathematical proofs. 

The reasoning capability also offers a significant advantage in addressing the challenges posed by traditional retrieval methods. However, current embedding models primarily focus on generating discriminative data representations, which leaves the development of reasoning capabilities largely unexplored. 

% Inspired by O1 model, O1 Embedder pioneer the integration of LLM reasoning into embedding models. By generating query specific insights, the model can have a deeper understanding of the user's query, achieving better retrieval results.


% \clearpage 


\section{Method}  
In this section, we first present the problem formulation of O1 Embedder. We then introduce the two main technical contributions of this work: the production of long-form thought data for O1 Embedder, and the multi-task training process of O1 Embedder. 


\subsection{Problem Formulation}
Dense retrieval measures the relevance between query and document based on their embedding similarity. Given an query $q$ and document $d$, an embedding model ($\mathcal{M}$) is used to encode them into latent representations $\boldsymbol{v}_q$ and $\boldsymbol{v}_d$: $\boldsymbol{v}_q \leftarrow \mathcal{M}(q)$, $\boldsymbol{v}_d \leftarrow \mathcal{M}(d)$. To retrieve the relevant document $d^*$ from a massive dataset $D$, the following nearest neighbor condition needs to be satisfied: 
\begin{equation}
    d^* = \textsc{argmax}. ~ \{\langle \boldsymbol{v}_q, \boldsymbol{v}_d \rangle | d \in D\}, 
\end{equation}
where $\langle\cdot\rangle$ indicates the inner-product operator. As discussed,  traditional embedding models are insufficient to handle the challenges regarding zero-shot or complex retrieval problems. 


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.90\linewidth]{img/data_produce.pdf} 
  \vspace{-5pt}
  \caption{The production of thought data. In the first step, the LLM is prompted to generate candidates thoughts about the input question based on the instruction and in-context examples. In the second step, the retrieval committee is employed to evaluate the candidates by making comparison with the ground-truth document, i.e. the retrieval target. Finally, the candidate thought receiving the maximum votes is selected and incorporated to the training data.}
  \vspace{-5pt}
  \label{fig:data_produce} 
\end{figure*} 



Our method tackles the above challenge by introducing the thinking operation to embedding models. That is to say, the embedding model $\mathcal{M}$ is equipped with two functionalities: thinking $\mathcal{M}.$think($\cdot$) and embedding $\mathcal{M}.$embed($\cdot$). For an input query $q$, the embedding model generates its thoughts ($t$) on how to address the information needs of the query in the first place:  
\begin{equation}
    t_i \leftarrow \mathcal{M}.\text{think}(q), ~ i = 1,...,k
\end{equation}
By revealing critical semantic matching patterns with relevant documents, the generated thoughts are expected facilitate the retrieval process for complex queries. In this place, a total of $k$ thoughts are independently generated with respect to the query, which enables useful patterns to be comprehensively covered. 

On top of the embedding model $\mathcal{M}$ and an aggregation function $\textsc{Agg}$,  the query and its thoughts are jointly transformed into a unified embedding, called the thought-augmented embedding ($\boldsymbol{\hat{v}}_q$): 
\begin{equation}\label{eq:3}
    \boldsymbol{\hat{v}}_q \leftarrow \textsc{Agg}.(q,\{t_i\}_{k};\mathcal{M}.\text{embed})
\end{equation}
As a result, the relevance between query and document is computed with the thought-augmented embedding: $\langle \boldsymbol{\hat{v}}_q, \boldsymbol{v}_d \rangle$. Finally, our problem is formulated as the joint training of thinking and embedding capability of model $\mathcal{M}$, such that the end-to-end retrieval performance is optimized.  


\subsection{Data Production} 
\label{section: data production}
The training of O1 Embedder involves two types of data. One is used for the embedding capability, which is made up of queries and their relevant documents, i.e., q-doc tuples. The other one is used for the thinking capability, which includes queries and their thoughts, i.e., q-thought tuples. Unlike q-doc tuples which have been widely existed, there are no available q-thought tuples in reality. To resolve this problem, we propose a data synthesis pipeline, leveraging LLMs' readily equipped reasoning capacity to generate such datasets. Our method follows an ``\textbf{exploration-refinement}'' workflow, as demonstrated in Figure \ref{fig:data_produce}.  

First, we employ a LLM to explore candidate thoughts for a given query $q$. To facilitate proper generations from the LLM, the system prompt is formulated with the following template, where both instruction and examples are incorporated:  
\begin{equation}
    \textsc{Prompt} = \textsc{Task}: \{\mathrm{Ins}\}; ~ \textsc{Examples}: \{\mathrm{E}\}; ~ \textsc{Query}: \{\mathrm{q}\}
\end{equation} 
The instruction is used to explicitly declare the demand for the thinking task; for example, "\textit{think about a plausible response to address the query}". While the examples are introduced to demonstrate the form of desirable output. In this place, we randomly select $m$ samples from the training set of q-doc dataset: 
\begin{equation}
    \mathrm{E} = \{ \textsc{Query}: q_i, ~ \textsc{Response}: d_i \}_m. 
\end{equation} 
Note that although the relevant document $d$ for query $q$ may seem like a trivial solution, it is unsuitable to serve as a thought. This is because the generated thought will be further used in embedding task, whose goal is to discriminate the relevant document $d$ based on the thought-augmented embedding. If $d$ (or any rephrased version of it) is used, the training process would be circumvented, ultimately leading to the collapse of the embedding task. 

The generated thoughts may not always enhance retrieval performance due to potential hallucinations by the LLM. To ensure the inclusion of useful thoughts, the exploration process is repeated multiple times, generating several independent thoughts for the given input query. To identify the most useful thoughts, a quality-control mechanism is introduced to filter the generated candidates. Specifically, we employ a diverse set of retrievers, denoted as $R$. For each retriever $r \in R$, a similarity score is computed between a relevant document $d$ and a thought $t_i$: $\sigma^r(t_i, d)$. The thought with the highest similarity score is selected by each retriever, i.e., $t^*_r \leftarrow \textsc{argmax}(\{\sigma^r(t_i, d)\}_{i=1...k})$. Finally, a majority voting process is conducted to determine the most useful thought. The thought that receives the highest number of nominations from the retrievers is selected as the final result: $t \leftarrow \textsc{Voting}\{t^*_r\}_{r \in R}$. 

By applying the above data synthesis workflow to an existing q-doc dataset: $D = \{(q_i,d_i)\}_N$, we can obtain an thought-augmented dataset composed of \textbf{q-thought-doc triplets}: $\hat{D} = \{(q_i, t_i, d_i)\}_N$, which offers long-form thoughts of the optimal retrieval utility. 

% which is adopted as the ultimate training dataset for O1 Embedder. 

\subsection{Multi-Task Training} 
The O1 embedder is built upon a pre-trained LLM, leveraging the model’s inherent generation and reasoning abilities. Additionally, LLMs also show strong potential for fine-tuning as discriminative embedding models \cite{luo2024large, zhu_large_2024}. We apply the following multitask training for a pre-trained LLM backbone, which establishes its thinking capability via supervised behavior cloning and embedding ability through contrastive learning. 

% for two main reasons. First, LLMs have been equipped with preliminary generation and reasoning abilities, making them proper foundations to further develop the thinking functionality. Second, LLMs also exhibit strong potential for being fine-tuned as discriminative embedding models \cite{luo2024large,zhu_large_2024}. We apply the following multitask learning to the pre-trained foundation model, enabling the thinking and embedding capabilities to be jointly established. 

\subsubsection{Behavior cloning}  
The foundation model is trained to generate thoughts for the input query through supervised fine-tuning. Given the dataset of q-thought-doc triplets: $\hat{D} = \{(q_i, t_i, d_i)\}_N$, a training sample $x_i$ is formulated with the template below: 
\begin{equation}
    x_i =\text{<query>} q_i \text{<thought>} t_i  \text{</s>},  
\end{equation} 
where <query>, <thought>, </s> are the special tokens to landmark the query, thought, and the completion of generation, respectively. 

With the formulation of above training samples, the model is fine-tuned w.r.t. the following \textbf{generation loss}: 
\begin{equation}
    \mathcal{L}_{gen} = - \sum_{x_i} \sum\nolimits_{j=|q_i|}^{|q_i|+|t_i|} \log P(x_{i,j} | x_{i,<j}),
\end{equation}
where the next-token-prediction loss is minimized for each of the tokens starting from the beginning of the thought (i.e. $j \geq |q_i|$).  

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{img/train_retrieve.pdf}
  \vspace{-5pt}
  \caption{Training and Retrieval process of O1 Embedder. During the training process, O1 embedder minimizes two losses: the generation loss while decoding the thought, and the contrastive loss while discriminating the target document. During the retrieval process, multiple thoughts are generated for the query. The thoughts are used to produce thought-augmented queries, which are independently encoded by O1 Embedder and aggregated for retrieval.} 
  \label{fig:train_retrieve}
\end{figure*} 

\subsubsection{Contrastive learning} 
The pre-trained LLM is also fine-tuned to distinguish relevant documents from a query based on its generated embeddings. Traditionally, LLM-based embedders utilize the </s> token for text embedding. However, the </s> token has been designated to indicate the completion of generation process in our approach. As a result, incorporating an extra embedding task could lead to the collapse of training process. {To avoid mutual interference between the two parallel training tasks, we employ another \textbf{special token <emb>} and append it to the end of input $x$ (following </s>) to compute the text embedding:} 
\begin{equation}
    \boldsymbol{v}_x \leftarrow \text{LLM}(x;\text{<emb>})[-1]
\end{equation} 
\textit{This simple modification substantially increases the compatibility, which is crucial to maintain the successful running of joint training process.} Considering that people may want to leverage thought-augmented embedding to handle complex retrieval tasks while still relying on basic query embedding to process simple retrieval tasks, we generate the two forms of query embeddings simultaneously to identify the relevant document $d_i$. As a result, we perform the following composite contrastive learning, where two \textbf{contrastive losses} are calculated based on the query and the thought-augmented query of each training sample, respectively:  
\begin{equation}
    \mathcal{L}_{ctr} = - \sum_{i} \frac{\exp(\boldsymbol{v}_{q_i}^T \boldsymbol{v}_{d_i})}{\sum_{d'}\exp( \boldsymbol{v}_{q_i}^T \boldsymbol{v}_{d'})} + \frac{\exp(\boldsymbol{v}_{\hat{q}_i}^T \boldsymbol{v}_{d_i})}{\sum_{D'}\exp( \boldsymbol{v}_{\hat{q}_i}^T \boldsymbol{v}_{d'})}
\end{equation}
In this place, $\hat{q}_i$ indicates the thought-augmented query: $\hat{q}_i \leftarrow q_i + t_i$, $D'$ stands for the collection of negative samples, including both in-batch negative samples and hard negative samples introduced by a pre-trained embedder.  

\subsubsection{Joint training}
The model is trained to minimize the linear combination of generation loss and contrastive loss: 
\begin{equation}
    \mathcal{L} = \lambda \mathcal{L}_{gen} + (1-\lambda) \mathcal{L}_{ctr}
\end{equation} 
To enable precise retrieval in downstream scenarios, contrastive learning must be conducted with a large training batch. However, the native parallel running of two training tasks requires significant GPU memory, which severely limits the achievable batch size. To address this challenge, we propose a \textbf{memory-efficient joint training} to handle both tasks (Figure \ref{fig:train_retrieve}). Specifically, for each training sample $x_i = (q_i, t_i, d_i)$, we encode it once and share the encoding result between the two tasks. The generative loss is calculated based on each of the output embeddings from $t_i$ tokens; while the contrastive loss is derived based on the output embeddings from <emb> tokens. This allows the generation task to consume almost no additional memory when trained alongside contrastive learning task, thereby enabling a substantial increase in batch size.  

\subsection{Retrieval} 
The well-trained O1 embedder $\mathcal{M}$ is applied for retrieval tasks through thinking and embedding. First, O1 embedder is prompted to generated multiple thoughts towards the input query which comprehensively uncover the query's hidden information needs: $t_i \leftarrow \mathcal{M}.\text{think(q)}, i = 1, ...  , k$. Next, the thought-augmented queries are independently encoded and aggregated. In this place, we simply adopt mean pooling as the aggregation function in Eq. \ref{eq:3}, which produces the following thought-augmented embedding:  
% \begin{equation}
%     \boldsymbol{\hat{v}}_q \leftarrow \mathcal{M}.\text{enc(q)} + \sum_{k} \mathcal{M}.\text{ enc($t_i$)} \Bigg/  {k+1} 
% \end{equation} 
\begin{equation}
    \boldsymbol{\hat{v}}_q \leftarrow \sum_{k} \mathcal{M}.\text{ enc($q,t_i$)} \Bigg/  {k} 
\end{equation} 
Finally, the top-N documents $D^*$ are retrieved based on their embedding similarity with $\boldsymbol{\hat{v}}_q$: $D^* \leftarrow \text{top-N}(\{\langle \boldsymbol{\hat{v}}_q,\boldsymbol{v}_d \rangle|D\})$. 



% \clearpage

% \section{Experimental Setups}

\section{Experiment} 
We make comprehensive evaluation of our approach with a focus on the following research questions. 
\begin{itemize}
  \item[\textbf{RQ 1.}] Whether O1 Embedder can outperform popular baseline retrievers after fine-tuning? 
  \item[\textbf{RQ 2.}] Whether O1 Embedder can be effectively generalized to out-of-domain scenarios? 
  \item[\textbf{RQ 3.}] How much does the thinking operation help? 
\end{itemize}
With these research questions, we design our experimental studies, whose settings are presented as follows. 

\subsection{Settings}


\begin{table*}[ht]
\begin{tabular}{c|c|c|cc|c|c}
\hline
                            & \multirow{2}{*}{method} & \multirow{2}{*}{model size} & \multicolumn{2}{c|}{MS MARCO Dev} & DL'19         & DL'20         \\ \cline{4-7} 
                            &                         &                             & MRR@10          & Recall@1k       & nDCG@10       & nDCG@10       \\ \hline
Sparse                      & BM25\cite{thakur_beir_2021}                    & -                           & 18.4            & 85.3            & 50.6          & 48.0          \\ \hline
\multirow{5}{*}{BERT-based} & ANCE\cite{xiong_approximate_2020}                    & 125M                        & 33.0            & 95.9            & 64.8          & 61.5          \\
                            & TAS-B\cite{hofstatter_efficiently_2021}                   & 55M                         & 34.3            & 97.6            & 72.2          & 69.2          \\
                            & coCondenser\cite{gao_unsupervised_2022}             & 110M                        & 38.2            & 98.4            & 69.8          & 68.4          \\
                            & SimLM\cite{wang_simlm_2023}                   & 110M                        & 41.1            & 98.7            & 71.4          & 69.7          \\
                            & RetroMAE\cite{liu_retromae-2_2023}                & 110M                        & 41.6            & 98.8            & 68.1          & 70.6          \\ \hline
\multirow{4}{*}{LLM-based}  & RepLLaMA\cite{ma_fine-tuning_2024}                & 7B                          & 41.2            & 99.4            & 74.3          & 72.1          \\
                            & Promptriever\cite{weller_promptriever_2024}            & 7B                          & 41.0            & 99.4            & 73.2          & 72.3          \\
                            & \textbf{O1 embedder w/o T}  & 7B                          & 41.7            & 99.4            & 73.7          & 72.3          \\
                            & \textbf{O1 embedder}               & 7B                          & \textbf{43.1}   & \textbf{99.5}   & \textbf{75.3} & \textbf{74.4} \\ \hline
\end{tabular} 
\vspace{5pt} 
\caption{In-domain evaluation results evaluated on MS MARCO, DL'19, and DL'20. O1 embedder w/o T indicates the variational form of O1 embedder, which disables the thinking operation but uses the same embedding model.} 
  \label{tab:MSMARCO}
\end{table*}

\subsubsection{Datasets}
O1 embedder is trained by the thought-augmented queries created from the MS MARCO (passage retrieval) dataset \cite{bajaj2018msmarcohumangenerated}. The well-trained model is evaluated based on both in-domain and out-of-domain datasets. For in-domain evaluation, we utilize MS MARCO (dev), TREC DL19~\cite{craswell2020overviewtrec2019deep}, and TREC DL20~\cite{craswell2021overviewtrec2020deep} datasets. For out-of-domain evaluation, we incorporate the following eight question-answering datasets from BEIR~\cite{thakur_beir_2021}, including SciFact~\cite{wadden-etal-2020-fact}, TREC-COVID~\cite{10.1145/3451964.3451965}, DBpedia~\cite{10.1145/3077136.3080751}, NQ~\cite{kwiatkowski-etal-2019-natural}, HotPotQA~\cite{yang2018hotpotqa}, FiQA~\cite{maia201818}, Touche~\cite{bondarenko2020overview}, FEVER~\cite{thorne-etal-2018-fever}, along with a popular dataset on code-search: CosQA~\cite{huang-etal-2021-cosqa}. All of these datasets consist of asymmetric retrieval tasks, where the query and document are presented in very different forms. As a result, the relationships between query and document can be more effectively identified through appropriate reasoning. We exclude common paraphrasing datasets, such as Quora, as they only involve simple similarity comparisons. Following prior works \cite{ma_fine-tuning_2024, weller_promptriever_2024}, we use MRR@10 and Recall@1k as metrics for MS MARCO-related tasks, and NDCG@10 for other datasets. Evaluations on o.o.d. datasets strictly adhere to the BEIR protocol, which prohibits task-specific fine-tuning. 


% We train our model in our new thought-augmented MS MARCO dataset and want to evaluate our model's ability to "think" before retrieval. To assess its performance, we employ a combination of in-domain and out-of-domain(zero-shot) datasets, ensuring a comprehensive evaluation across various domains. For in-domain evaluation, we utilize MS MARCO and TREC DL19~\cite{craswell2020overviewtrec2019deep}, TREC DL20~\cite{craswell2021overviewtrec2020deep} datasets. To evaluate the generalizability of our model beyond the training domain and the capability of handling complex relevance of our model, we incorporate nine asymmetric tasks(i.e.  the query and document are related but not paraphrases of each other) with different domains for out-of-domain evaluation: SciFact(scientific)~\cite{wadden-etal-2020-fact}, TREC-COVID(bio-medical)~\cite{10.1145/3451964.3451965}, DBpedia(entity)~\cite{10.1145/3077136.3080751}, NQ(nature)~\cite{kwiatkowski-etal-2019-natural}, HotPotQA(multi-hop)~\cite{yang2018hotpotqa}, FiQA(finance)~\cite{maia201818}, Touche(argument retrieval)~\cite{bondarenko2020overview}, FEVER(claim verification)~\cite{thorne-etal-2018-fever}, CosQA(code search)~\cite{huang-etal-2021-cosqa}. The first eight datasets are from BEIR~\cite{thakur_beir_2021} benchmarks and the last is a code search dataset retrieving code snippets through natural language.
%These asymmetric tasks are more challenging, unlike other symmetric tasks that only need to search text with the same semantics as query. It is more worthwhile for the model to think before retrieving.
% These asymmetric tasks are more challenging and more worthwhile for the model to think before retrieving.
% We use MRR@10 and Recall@1k as the metric of MS MARCO dataset, and NDCG@10 for other datasets following ~\cite{ma_fine-tuning_2024,weller_promptriever_2024}. All zero-shot evaluations strictly follow the BEIR protocol without task-specific fine-tuning.

%By leveraging these diverse datasets, we aim to demonstrate our model's retrieval capabilities and its potential to "think" across various domains and tasks. This comprehensive evaluation framework ensures that our model is not only effective within its training domain but also adaptable to unseen and complex retrieval tasks.

\subsubsection{Baselines} 
We choose a wide variety of popular retrievers as our baselines, such as BM25, a commonly used sparse retrieval method, and ANCE \cite{xiong_approximate_2020}, TAS-B \cite{hofstatter_efficiently_2021}, coCondenser \cite{gao_unsupervised_2022}, SimLM \cite{wang_simlm_2023}, which fine-tune BERT-based pre-trained models using MS MARCO dataset. We also introduce the LLM-based methods, including RepLLaMA \cite{wang_text_2024}, Promptriver \cite{weller_promptriever_2024}. RepLLaMA fine-tunes a pre-trained LLM on MS MARCO, resulting in superior retrieval performance across various downstream tasks. While Promptriever builds on RepLLaMA by enhancing the model’s instruction-following capabilities, which further improves the retrieval performance on top of tailored prompts. Both LLM-based methods are fine-tuned from the same pre-trained backbone (Llama-2-7B) as our default setting, thus ensuring a fair comparison in terms of model scale. Note that we exclude several other popular retrievers from our evaluation, including BGE~\cite{xiao_c-pack_2024}, E5~\cite{wang2022text}, M3~\cite{chen_bge_2024}, and recent LLM-based methods like E5-Mistral~\cite{wang2023improving}, ICL-Embedder~\cite{li_making_2024}, and NV-Embed~\cite{lee2024nv}. These models utilize significantly more training data beyond MS MARCO (the only training dataset used by our method and our included baselines), many having strong overlaps with the evaluation tasks on BEIR. This overlap makes it difficult to assess zero-shot retrieval performance in out-of-distribution (o.o.d.) settings. Additionally, these methods rely on different training datasets, which makes it impossible to maintain a fair comparison. 

% We choose our comparison baseline from the following aspects. The first one is the commonly used sparse retrieval method BM25. the second one is some BERT-based models: ANCE, TAS-B, coCondenser, SimLM, RetroMAE. These model usually have smaller sizes, but they generally perform unsupervised pretraining on large-scale corpus close to downstream retrieval tasks. Finally, we compare some LLM-based models, which are also our main comparators, RepLLaMA, Promptriver. RepLLaMA uses LLAMA2 to initialize the model, and is trained by contrastive learning on the MS MARCO dataset. Promptriver, based on RepLLaMA, empowers the model with the ability of instruction following, which can give the model better results by adding natural language prompt. For the comparison baseline of the out-of-domain dataset, we mainly choose some large models , which usually have stronger generalization. Note that although there are many general embedding models now with very good retrieval results, such as E5-mistral-instruct~\cite{wang2023improving}, these models were trained on significantly more data than just MS MARCO, including the training sets of BEIR. we wanted to evaluate the true zeor-shot capabilities of the models, and such evaluation is consistent with the original intent of BEIR's creation, so we did not compare these models that were supervised trained directly on the BEIR training sets in our main experiments.


\subsubsection{Implementation Details}
During the data preparation stage, we leverage a powerful open-source LLM: Llama-3.1-70B-Instruct\footnote{https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct}, to generated the candidate thoughts. We employ BM25, BGE-EN-large-v1.5, GTE-large, and Stella-EN-1.5B-v5, to serve the retrieval committee. The evaluation is primary made based on a Llama-2-7B backbone \cite{touvron2023llama2openfoundation}, with other alternative LLMs analyzed in the extended study. The training process follows RepLLaMA's recipe \cite{ma_fine-tuning_2024}, where all projection layers (q\_proj
k\_proj v\_proj o\_proj gate\_proj down\_proj up\_proj) of the LLM are fine-tuned via LoRA, with rank set to 32 and alpha set to 64. We used BF16 for training, with learning rate set to \( 1 \times 10^{-4} \). The training process takes place on 8xA800 GPUs, with a batch size set to 64 (8 per-device). We introduce 15 hard negatives for each query. The maximum query length was set to 32, and the maximum passage length was set to 192. The max tokens were set to 256 for thought generation. 

% The batch size was set to 64 (8 GPUs, with a per-device training batch size of 8), with 15 hard negative passages for each query. The maximum query length was set to 32, and the maximum passage length was set to 192.The max tokens were set to 256 for next token prediction training. 

% the meta-llama/Llama-3.1-70B-Instruct\footnote{https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct} model using VLLM. We set the temperature to 1 and the max tokens to 256. The entire data generation process took about 2 days on 8 A100 (40G) GPUs. During the training process, we primarily followed the training recipe from RepLLaMA. We fine-tuned all linear layers of the model q\_proj k\_proj v\_proj o\_proj gate\_proj down\_proj up\_proj using LoRA, with a rank set to 32 and alpha set to 64. We used BF16 for training, with a learning rate set to \( 1 \times 10^{-4} \) for 1 epoch. The batch size was set to 64 (8 GPUs, with a per-device training batch size of 8), with 15 hard negative passages for each query. The maximum query length was set to 32, and the maximum passage length was set to 192.The max tokens were set to 256 for next token prediction training.


% \section{Main Results and Analysis}

% Our main experiments are designed to answer the following questions:

% \begin{itemize}
%   \item[\textbf{RQ 1.}] Does our model outperform corresponding baselines for in-domain datasets? % in domain performence
%   \item[\textbf{RQ 2.}] Can incorporating "thinking" mechanisms improve performance on out-of-domain or complex retrieval tasks? % ood performence
%   \item[\textbf{RQ 3.}] How much does thinking help? % slow mode vs fast mode
% \end{itemize}

\subsection{Main Results}
\subsubsection{In-domain Performance}
\label{subsection: Superviesd Performance} 
The in-domain evaluation results are presented in Table \ref{tab:MSMARCO}, where the O1 Embedder outperforms all other models across all evaluation metrics, including MRR@10, Recall@1k, and nDCG@10, on the MSMARCO, DL'19, and DL'20 datasets. Specifically, our model achieves an MRR@10 of 43.1 (\textbf{+1.9\%} improvement) and a Recall@1k of 99.5 on MSMARCO, along with nDCG@10 scores of 75.3 (\textbf{+1.0\%} improvement) and 74.4 (\textbf{2.1\%} improvement) on DL'19 and DL'20, respectively, compared to RepLLaMA. These results demonstrate the effectiveness of our model and its enhanced reasoning capabilities in retrieval tasks. 
Furthermore, when comparing model sizes, it is clear that larger models, such as the LLM-based RepLLaMA and Promptriever, generally outperform smaller BERT-based models. For example, RepLLaMA achieves an nDCG@10 of 74.3 on DL'19 and 72.1 on DL'20, surpassing all smaller models. The O1 Embedder without thinking (O1 embedder w/o T), a variation that disables the reasoning operation, yields similar results as RepLLaMA, suggesting that the embedding capability is effectively established through multi-task training. However, with the addition of thinking, the full O1 Embedder demonstrates a substantial improvement, highlighting the power of the "thinking" enhancement. 

%However, it is noteworthy that small BERT-based models, like RetroMAE, can still perform competitively when pre-trained with objectives closely aligned with retrieval tasks. RetroMAE achieves an MRR@10 of 41.6 on MSMARCO, closely approaching the performance of much larger models like RepLLaMA, which scores 41.2. This indicates that task-specific pre-training can significantly enhance the capabilities of smaller models. On the contrary, the LLM-based models, despite their substantial size, face limitations due to pre-training on objectives that are not directly aligned with retrieval tasks, which hinders their ability to fully capitalize on pre-training knowledge.

%The comparison between "O1 embedder" and "O1 embedder w/o thought " vividly illustrates the transformative impact of integrating a "thinking" mechanism on retrieval performance. O1 embedder achieves a remarkable MRR@10 of 43.1, surpassing the fast mode variant's 41.7. 
%Moreover, the nDCG@10 scores for the DL'19 and DL'20 benchmarks further accentuate this improvement, with increases of 1.6 and 2.1 points, respectively. These enhancements highlight the critical role of the "thinking" mechanism in enriching the retrieval process. By delving deeper into the additional contextual information, the model significantly enhances its comprehension of the user's query, paving the way for a more nuanced understanding. This enriched understanding allows the model to navigate the retrieval process with greater ease and precision, ultimately leading to superior performance.

%It is noteworthy that even in fast mode, our model achieves results comparable to those of RepLLaMA, a testament to the efficacy of our adaptive contrastive learning strategy. This innovative approach not only sustains impressive retrieval performance in resource-constrained scenarios but also underscores the remarkable flexibility and adaptability of our model. Such versatility ensures that our model remains robust and efficient, regardless of the computational limitations it may encounter.

\subsubsection{O.O.D. Performance} 
\label{subsection: Zero-shot Generalization}

\begin{table*}[]

\begin{tabular}{c|c|ccccccccc|c}
\hline
Method          & Model size & T-Covid & NQ   & HQA & FiQA & Touche & DBPedia & FEVER & SciFact & CosQA & Average \\ \hline
% BM25            & -          & 65.6       & 32.9 & 60.3     & 23.6 & 36.7   & 31.1    & 75.3  & 66.5    & -     & -       \\
Contriever      & 0.1B       & 59.6       & 49.8 & 63.8     & 32.9 & 23.0   & 41.3    & 75.8  & 67.7    & 14.2  & 47.6    \\
% bge-m3          & 0.5B       & 39.5       & 60.6 & 69.4     & 41.0 & 22.3   & 39.8    & 80.9  & 64.2    & 22.7  & 48.9    \\
CPT-L           & 6B         & 56.2       & -    & 64.8     & 45.2 & 30.9   & 41.2    & 75.6  & 74.4    & -     & -       \\
CPT-XL          & 175B       & 64.9       & -    & 68.8     & \textbf{51.2} & 29.1   & 43.2    & 77.5  & 75.4    & -     & -       \\
OpenAI-Ada-002  & -          & 81.3       & 48.2 & 65.4     & 41.1 & 28.0   & 40.2    & 77.3  & 73.6    & 28.9  & 53.7    \\
RepLLaMA        & 7B         & 84.7       & 62.4 & 68.5     & 45.8 & 30.5   & 43.7    & 83.4  & 75.6    & 32.3  & 58.5    \\
Promptriver     & 7B         & 84.6       & 62.6 & 69.5     & 46.6 & 32.0   & 45.2    & 82.8  & 76.3    & 32.8  & 59.2    \\ \hline
\textbf{O1 embedder w/o T} & 7B         & 84.5       & 62.9 & 69.8     & 45.0 & 33.8   & 44.4    & 82.5  & 75.8    & 32.9  & 59.1    \\
\textbf{O1 embeder}    & 7B         & \textbf{85.6}       & \textbf{66.8} & \textbf{72.8}     & 46.6 & \textbf{36.7}   & \textbf{47.3}    & \textbf{84.9}  & \textbf{77.4}    & \textbf{34.1}  & \textbf{61.4}    \\ \hline
\end{tabular}
\vspace{5pt}
\caption{Out-of-domain evaluation results measured by nDCG@10.}
  \label{tab:BEIR}
\end{table*} 

The out-of-distribution (o.o.d.) evaluation results are shown in Table \ref{tab:BEIR}, where the O1 Embedder consistently outperforms across all nine datasets. On average, our model achieves a \textbf{2.3\%} improvement over the baseline, which is a significant boost and underscores the strong generalization capabilities of our approach. Additionally, for each of the nine datasets, our model sets the highest performance except for FiQA, where it falls behind CPT-XL 175B. This highlights that our model excels across various scenarios, especially the retrieval tasks involving complex reasoning, such as HotPotQA (multi-hop question-answering) and CosQA (code-search). 


% We can draw a similar conclusion from Table \ref{tab:BEIR}, where we achieved very good results in 9 out-of-domain datasets. On average, our model has an overall improvement of \textbf{2.3\%} compared to baseline, which is a huge improvement and shows the strong generalization performance of our model. In addition, for each of the 9 datasets, our model achieves a new state-of-the-art result, except for FiQA, where our model does not outperform CPT-XL with 175B, which demonstrates that our approach is very effective in different domains even in many challenging complex retrieval tasks like HotPotQA and CosQA. 
% These enhancements stem from the powerful thinking capabilities of the O1 embedder model. By thinking, the model can generate more relevant information that is benefit for retrieval. And because LLM has been pre-trained on large-scale corpus, this generative capability is transferable, even for some domains that we have not trained in our training pipline, such as document retrieval in the biomedical domain, the model is still able to generate useful thinking content to achieve more accurate retrieval results.
% 这些提升源于O1 embedder模型强大的思考能力，通过思考，模型能生成更多有利于检索的相关信息。并且因为LLM在大规模语料上进行过预训练，这种生成能力是可以迁移的，即使是一些我们没有进行过训练的领域，例如生物医药领域的文档检索，模型仍能生成有用的thinking content以达到更精确的检索结果。
% 这说明了加入优质的thought-augmented数据进行训练能有效的增强模型的检索能力

% Some interesting conclusions can also be obtained from the table, the model can improve more significantly on some OpenQA datasets through the thinking mechanism, for example, the NQ dataset can be improved by \textbf{3.9\%} through the thinking mechanism, and the HotPotQA can be improved by \textbf{3.0\%}. On the other hand, although there is improvement in some specialized domains, the improvement is not so big, for example, in Trec-Covid (medical domain), FiQA (financial domain) and SciFact (Scientific domain) the improvement brought by thinking is only 0.9\% , 1.7\% and 1.6\%. We believe that this is mainly because LLM itself is better at these OpenQA questions after large-scale corpus training, while LLM often lacks the training of such specialized domain data, resulting in generated content that may be noisy or even hallucination. Although it can also bring about a certain amount of enhancement, the enhancement is not as significant. In the future, how to refine and denoise the thinking content will also be an important research direction. 

Several interesting conclusions can be derived from the table. The thinking mechanism leads to more significant improvements on certain OpenQA datasets. For instance, the NQ dataset shows a \textbf{3.9\%} improvement, while HotPotQA sees a \textbf{3.0\%} boost. However, while there are improvements in some specialized domains, they are not as substantial. For example, in TREC-Covid (medical domain), FiQA (financial domain), and SciFact (scientific domain), the improvements brought by the thinking mechanism are only 0.9\%, 1.7\%, and 1.6\%, respectively. We believe this is because LLMs perform well on general OpenQA questions after training on large-scale corpora, but often lack specialized domain data. As a result, the generated content can sometimes be noisy or even hallucinated. While the thinking mechanism still provides some enhancement, its impact is not as pronounced in these domains. Thus, refining the generated thoughts will be an important issue for future research. 

\subsubsection{Impact of Thought} 

% 1. 从表1和表2中O1 embedder与O1 embedder w/o thought的对比我们可以发现，thinking content 能大幅帮助到模型进行query的表征与检索。详细数据对比...
% 2. 分析：提升源于模型的在思考过程中能生成出additional contextual information, the model significantly enhances its comprehension of the user's query
% 3. It is noteworthy that even in fast mode, our model achieves results comparable to those of RepLLaMA, a testament to the efficacy of our adaptive contrastive learning strategy. This innovative approach not only sustains impressive retrieval performance in resource-constrained scenarios but also underscores the remarkable flexibility and adaptability of our model. Such versatility ensures that our model remains robust and efficient, regardless of the computational limitations it may encounter. 

The comparisons between the O1 Embedder and the O1 Embedder w/o T in Tables~\ref{tab:MSMARCO} and~\ref{tab:BEIR} clearly demonstrate the significant impact of incorporating the thinking mechanism. Specifically, on the MS MARCO benchmark (Table~\ref{tab:MSMARCO}), the O1 Embedder shows substantial improvements across all metrics, with notable gains in MRR@10 (+1.4\%) and nDCG@10 for both DL'19 (+1.6\%) and DL'20 (+2.1\%). Similarly, in the zero-shot evaluation (Table~\ref{tab:BEIR}), the O1 Embedder outperforms its counterpart across all datasets, achieving an average nDCG@10 score of 61.4—2.3 points higher than the O1 Embedder w/o T. This performance boost can be attributed to the thinking mechanism, which allows the model to generate additional contextual information during the encoding process. By incorporating this supplementary information, the model enhances its understanding of the user’s query, leading to more accurate and effective retrieval. 
Additionally, it is worth noting that even in its "fast" mode (represented by O1 Embedder w/o T), the model achieves results comparable to those of RepLLaMA. This highlights that our model maintains competitive retrieval performance, even without the generated reasoning, underscoring its flexibility and adaptability. 

% From the comparisons between O1 embedder and O1 embedder w/o T in Tables~\ref{tab:MSMARCO} and~\ref{tab:BEIR}, it is evident that incorporating the thinking mechanism significantly enhances the model's ability to encode queries and retrieve relevant information. Specifically, in the MS MARCO benchmark (Table~\ref{tab:MSMARCO}), the O1 embedder achieves a substantial improvement over the O1 embedder w/o T across all metrics, with notable gains in MRR@10 (+1.4\%) and nDCG@10 for both DL'19 (+1.6\%) and DL'20 (+2.1\%). Similarly, in the zero-shot evaluation (Table~\ref{tab:BEIR}), the O1 embedder outperforms its counterpart in all datasets, achieving an average nDCG@10 score of 61.4, which is 2.3 points higher than the O1 embedder w/o T.

% This performance boost can be attributed to the thinking mechanism, which enables the model to generate additional contextual information during the encoding process. By incorporating this supplementary information, the model significantly enhances its comprehension of the user's query, leading to more accurate and effective retrieval.

% Furthermore, it is noteworthy that even in its "fast" mode (represented by O1 embedder w/o T), the model achieves results comparable to those of RepLLaMA. This illustrates that our model maintains competitive retrieval performance even in the absence of generated thoughts, which underscores the flexibility and adaptability of our model.

%The above results underscore the pivotal role of the thinking mechanism in enhancing query representation and retrieval performance, making the O1 embedder a versatile and high-performing solution for both supervised and zero-shot retrieval tasks.

\subsubsection{Summary for main result} 
Based on the above discussion, we come to the following conclusions in response to RQ 1-3:  
% Given the analysis from Section \ref{subsection: Superviesd Performance} and \ref{subsection: Zero-shot Generalization}, we can draw the following conclusions in response to \textbf{RQ 1, RQ 2} and \textbf{RQ 3}:

\begin{itemize}
  \item[\textbf{Con 1.}] Our model significantly outperforms both BERT-based and LLM-based models for in-domain evaluations, indicating that the model's thinking and embedding capabilities are effectively learned through the multi-task training process.  
  % Our model achieves notably higher retrieval performances over amount of BERT-based and LLM-based model for in-domain evaluation, indicating that the joint training allows the model to generate thoughts that are helpful for retrieval, further enhancing query representation capability of the model.% in domain performence
  \item[\textbf{Con 2.}] Our model well maintains superior performances throughout different o.o.d. tasks, which verifies our strong generalization ability. This advantage is especially pronounced when handling those challenging problems. 
  \item[\textbf{Con 3.}] The thinking operation brings forth significant help, as it substantially improves upon the retrieval performance from the vanilla embedding model. 
  % The help of thinking is Significant! It serves as a powerful tool in enhancing query representation and improving retrieval performance across various tasks and domains.  % slow mode vs fast mode
\end{itemize}

\subsection{Extended Analysis} 
With the verification of O1 Embedder's overall effectiveness in the previous discussions, we perform extended studies in this section, where the following detailed problems are analyzed: 

% In the above section we described the superiority of the effect of our method. Next, we will analyze it further in this section. We mainly want to analyze the following questions:

\begin{itemize}
  \item[\textbf{RQ 4.}] How much does our joint multi-task training strategy contribute to the model’s performance? %一方面的是生成的内容更适合检索，另一方面是端到端训练检索模型能更好的利用生成的内容
  \item[\textbf{RQ 5.}] Whether our method stays robust to different model architectures and parameter settings? % different model/hyperparameters 
  \item[\textbf{RQ 6.}] Why does our method achieves such significant improvements in its retrieval performance? % case analysis explaination 
\end{itemize} 

The corresponding issues are discussed in subsections \ref{subsection: RepLLaMA with thinking}, \ref{subsection: Robustness}, and \ref{subsection: Case Study}, respectively. 

% In the next subsections \ref{subsection: RepLLaMA with thinking}, \ref{subsection: Robustness}, \ref{subsection: Case Study}, we will answer to the above questions in sequence.

\subsubsection{Joint training}
\label{subsection: RepLLaMA with thinking}

\begin{table}[ht]

\begin{tabular}{c|ccc|ccc}
\hline
           & \multicolumn{3}{c|}{RepLLaMA} & \multicolumn{3}{c}{O1 embedder} \\ \cline{2-7} 
           & base    & with T    & $\Delta$   & base  & with T  & $\Delta$  \\ \hline
DL'19       & 74.3  &   72.4    &   -1.9    &   73.7    &   75.3    &   +1.6    \\
DL'20       & 72.1  &   72.6    &   +0.5    &   72.3    &   74.4    &   +2.1    \\  \hline
Trec-Covid & 84.7    & 79.7      & -5.0      & 84.5  & 85.6    & +1.1    \\
NQ         & 62.4    & 65.1      & +2.7     & 62.9  & 66.8    & +3.9    \\
HotPotQA   & 68.5    & 71.7      & +3.2     & 69.8  & 72.8    & +3.0      \\
FiQA       & 45.8    & 40.2      & -5.6    & 45.0    & 46.6    & +1.6    \\
Touche     & 30.5    & 33.3      & +2.8     & 33.8  & 36.7    & +2.9    \\
DBPedia    & 43.7    & 44.2      & +0.5     & 44.4  & 47.3    & +2.9    \\
FEVER      & 83.4    & 85.5      & +2.1     & 82.5  & 85.0      & +2.5    \\
SciFact    & 75.6    & 74.2      & -1.4    & 75.8  & 77.4    & +1.6    \\ 
CosQA      & 32.3    & 33.0     & +0.7      & 32.9  & 34.1  & +1.2      \\ \hline
Avg        & 61.2    & 61.1      & -0.1    & 61.6  & 63.8    & +2.2    \\ \hline
\end{tabular}
% \caption{Exploration of joint training. O1 embedder's thought is generated by itself, RepLLaMA's thought is generated by GPT-4o-mini. "Base" denotes retrieval directly with query, "with T" denotes retrieval using the query with thought, "$\Delta$" denotes the improvement.}
\vspace{5pt}
\caption{Exploration of joint training. With the joint training of thinking and embedding ability, O1 embedder achieves a substantial improvement in retrieval performance. In contrast, RepLLaMA directly leverages the thoughts generated by GPT-4o-mini (using the same prompt as O1 embedder), which leads to a suboptimal performance due to the incompatibility of the two modules. In this table, "Base" denotes retrieval directly with query, "with T" denotes retrieval using the query with thought, "$\Delta$" denotes the improvement.}
\vspace{-15pt}
\label{tab: RepLLaMA+}
\end{table}

To analyze the impact from joint training, we make analysis on whether existing retrieval models can make effective use of the generated thoughts in a training-free manner. For this purpose, we introduce a stand-alone generator for a well-trained retriever, which generates thoughts for its presented queries. In our experiment, we leverage RepLLaMA as the retriever and GPT-4o-mini as the generator for our experiments. 

The results of this approach are shown in Table \ref{tab: RepLLaMA+}. While incorporating thoughts results in mild improvements on datasets like NQ and HotPotQA, it causes declines on others, such as Trec-Covid and FiQA. Overall, this approach leads to only minor gains on some tasks but results in a slight decrease of 0.1 in overall performance. In contrast, our model consistently improves the retrieval performance across all datasets. This suggests that, with joint training, our model can better utilize the generated thoughts. The untrained RepLLaMA model, however, appears to be negatively impacted by the potential noise within the generated thoughts, leading to worse results, particularly in specialized domains like Trec-Covid, FiQA, and SciFact. In brief, the above observation indicates that our method not only generates useful thoughts for retrieval, but also learns to make effective use of the thoughts through joint training. 

% and that end-to-end training helps the model better utilize this thinking content, resulting in better overall performance. 

% We do this by prompting the LLM to generate thoughts based on the queries. The relevant documents are then retrieved using the queries along with their generated thoughts, following the same process as RepLLaMA. The key difference is that we need another generative model to perform the prompt-writing operation. For this, we leverage GPT-4o-mini as the generative model. The prompt we use is the same as the one in our method. 
    
% Table \ref{tab: RepLLaMA+} shows the results of the experiments. As we can see from the table, while the addition of thinking content yielded improvements in certain datasets like NQ and HotPotQA, it also resulted in declines in others, particularly Trec-Covid and FiQA. On average, this method produced negligible improvements, with a slight decrease of 0.1 overall. In contrast, our model consistently enhanced retrieval performance across all datasets. It shows that by training, our model can better utilize the content of thinking. The untrained RepLLaMA model appears to be adversely affected by noise from the generated content, leading to degraded results, especially in specialized domains where large language models may struggle, such as Trec-Covid, FiQA, and SciFact. This suggests that our model can generate thought that is more optimally helpful for retrieval, and the thinking content can be better utilized by end-to-end training, thereby achieving superior retrieval results.


\subsubsection{Robustness}
\label{subsection: Robustness}

In this section, we verify the robustness of our method from two perspectives. First, we implement our method based on different pre-trained architectures, including Llama, Mistral, Qwen. Second, we also introduce LLMs of different sizes (ranging from 0.5B to 8B) for evaluation. 

% we study the effect of different base model for the same size. Second we study the impact of different models with different sizes.
\begin{table}[ht]

\begin{tabular}{c|ccc|c}
\hline
               & \multicolumn{3}{c|}{in-domain} & o.o.d. \\
               & MS MARCO       & DL'19  & DL'20  &  AVG  \\ \hline
Llama-2-7B     & 43.1          & 75.3   & 74.4   & 61.4      \\
Mistralv0.3-7B & 43.5          & 77.0   & 75.6   & 61.4      \\
Llama-3.1-8B   & 43.5          & 76.2   & 74.5   & 61.6      \\
Qwen2.5-7B     & 43.3          & 76.4   & 74.7   & 61.2      \\
Qwen2.5-3B     & 42.5          & 76.3   & 74.5   & 60.3      \\
Qwen2.5-1.5B   & 41.9          & 74.0   & 73.5   & 58.7      \\
Qwen2.5-0.5B   & 40.5          & 73.6   & 71.4   & 55.4       \\ \hline
\end{tabular}
\vspace{5pt}
\caption{The impact from using different backbone models of variant pre-trained architectures and model sizes. The detailed scores for the zero-shot evaluation are presented in Appendix~\ref{app:detail score}. O1 Embedder well maintains a strong performance throughout these settings.}
\vspace{-15pt}
\label{tab: base model}
\end{table}



% \subsubsection{The Effect of Different Base Models}


\textbf{Impact of different model backbone}. The previous experiments primarily used Llama-2-7B as the backbone, which is consistent with RepLLaMA and promptriever. To verify the generalizability of our approach, we repeat the same experiment with implementations on different backbone LLMs. 
The results in Table~\ref{tab: base model} demonstrate our effectiveness across different settings. Notably, our approach well maintains a strong retrieval performance in both in-domain and out-of-domain evaluations. This observation suggests that our method is generally effective with various architectures, regardless of their difference in pre-trained capabilities.  

%The slight variations in performance suggest that while the backbone model does play a role, the effectiveness of our approach is not heavily reliant on architectures and pre-training backgrounds. 
%In the zero-shot evaluation, the performance remains consistent. The comparable results across various base models highlight the strength of our approach in utilizing the inherent capabilities of these base models, regardless of their training specifics.

% \subsubsection{The Effect of Different Model Sizes}
\textbf{Impact of different model sizes}. We can also clearly observe the significant impact of model size on performance in Table~\ref{tab: base model}. As the size of the Qwen2.5 models decreases, there is a noticeable drop in effectiveness across all evaluated datasets. While the 3B model performs similarly to the 7B model, further reductions in size lead to more pronounced performance declines. This is partly because larger models tend to have better generalization capabilities, benefiting from training on more extensive corpora during pre-training.
% We can also clearly see the significant impact of model size on the effect in Table ~\ref{tab: base model} As the size of the Qwen2.5 models decreases, there is a noticeable decline in effectiveness across all evaluated datasets. While a 3B model performs closely to a 7B model, smaller sizes lead to more pronounced drops in performance. This is partly due to the fact that larger model sizes have better generalization capabilities and they tends to train more corpus during pre-training. 
It's worth noting that even a 1.5B model, through retrieval with thought, performs comparably to the 7B RepLLaMA, which further highlights the advantage of our approach. 


\subsubsection{Case Study}
\label{subsection: Case Study}

\begin{table}[ht]
\begin{tabular}{p{0.9\linewidth}}
\hline
\textbf{Query:}   what age was martin luther king when he was admitted      \\
\hline
\textbf{Thought:}   Martin Luther King Jr. was admitted to \textcolor{softgreen}{\textbf{Morehouse College in Atlanta}}, Georgia \textcolor{softgreen}{\textbf{at the age of 15}}. He attended the college from \textcolor{softgreen}{\textbf{1944}} to 1948, where he earned a Bachelor of Arts degree in sociology.   \\
\hline
\textbf{Positive Doc:}   Dr. Martin L. King, Jr. and His Mentors: A Message for America Today If it was not for Benjamin Mays... Benjamin Mays was the president of \textcolor{softgreen}{\textbf{Morehouse College in Atlanta}} when he met Martin Luther King, Jr. In \textcolor{softgreen}{\textbf{1944}}, Martin Luther King was admitted to the college \textcolor{softgreen}{\textbf{at age 15}}. ...   \\
\hline
% \hline
% \textbf{Query:}   Hayden is a singer-songwriter from Canada, but where does Buck-Tick hail from?      \\
% \hline
% \textbf{Thinking content:}   \textcolor{softgreen}{\textbf{Buck-Tick}} is a \textcolor{softgreen}{\textbf{Japanese}} rock band, and its members are from various parts of \textcolor{softgreen}{\textbf{Japan}} and their music is a unique blend of alternative rock, gothic rock, and visual kei styles.   \\
% \hline
% \textbf{Positive Doc:}   Buck-Tick \textcolor{softgreen}{\textbf{Buck-Tick}} (stylized as BUCK-TICK) is a \textcolor{softgreen}{\textbf{Japanese}} rock band, formed in Fujioka, Gunma in 1983. The group has consisted of ...   \\
% \hline
\end{tabular}
\vspace{5pt}
\caption{Examples of the original query, thinking content and the positive document. The similar patterns between gnenrated content and the groundtruth are marked in \textcolor{softgreen}{\textbf{green}}.}
% \vspace{-5pt} 
\label{tab:case1}
\end{table}

In Table~\ref{tab:case1}, we demonstrate an example of the generated thought and the ground-truth document to a complex multi-hop query. In this case, the query asks about Martin Luther King's age upon his admission to college. Our thought generated effectively uncovers useful contextual information, highlighting that King was admitted to Morehouse College at the age of 15 in 1944. This generated content not only answers the query directly but also enriches the context by providing additional details about the college and the timeframe of his attendance.  \textbf{By generating such useful patterns, the embedding model can obtain crucial information related to the query, which results in a more precise retrieval result}. We include more case analysis for O1 Embedder in Appendix~\ref{section:additional_case_study} and ~\ref{app: complex}. 

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.9\linewidth]{img/attention-top20.pdf}
%   \caption{Top 20 Attention score from <emb> token in the thinking content}   
% \end{figure}

% analysis the fig
% 1. similar patterns have high score indicate the model successfully pay attention to these patterns. 
% 2. 这些生成的thinking content在检索过程中起了作用，并且为query补充了更多的信息例如Japan这个并没有在query中出现的token


% why our method can achieve such a huge improvement?
% 1. 思考促使模型生成更多与query相关的中间模式（patterns），这些模式有助于模型更准确地捕query的关键信息，提升检索效果。
% 1. 思考过程允许模型生成中间结果，这些结果可以作为后续检索的基础，帮助模型更好地理解问题并减少错误。（multihop retrieval）

% \vspace{-5pt} 
\section{Conclusion} 
In this paper, we introduce O1 Embedder, a novel retrieval model that performs slow-thinking before executing retrieval actions. This approach allows the model to better understand the underlying information needs within the query, aiding in the identification of semantic relevance for complex retrieval tasks. Leveraging our tailored data production method, we generate long-form thoughts optimized for retrieval utility. Additionally, our proposed multi-task training method effectively establishes both the model's thinking and embedding capabilities. We conduct comprehensive experiments on popular evaluation benchmarks, and the results demonstrate that O1 Embedder significantly outperforms existing methods, achieving substantial improvements in retrieval performance across both in-domain and out-of-domain scenarios. 
% retrieval model integrating a thinking mechanism with generative capabilities. Through constructing the thought-augmented MS MARCO dataset and an end-to-end training framework, Our method equips embedding models with the capability to generate useful thoughts about input queries, thereby improving their performance on complex and zero-shot retrieval tasks. Through comprehensive experiments, we demonstrated that the O1 Embedder significantly outperforms previous models, achieving substantial improvements in retrieval performance across both in-domain and out-of-domain datasets.
%The success of the O1 Embedder can be attributed to several key factors. First, our data synthesis pipeline effectively addresses the absence of long-form thought data by leveraging LLMs to generate and refine candidate thoughts. Second, the multitask training framework enables the model to jointly learn the thinking and embedding capabilities, ensuring that the generated thoughts are optimally useful for retrieval. Third, the integration of a thinking mechanism allows the model to generate additional contextual information that enhances query representation and retrieval accuracy. 
Our work lays the foundation for future research in advanced retrieval models with reasoning capabilities. Future directions include expanding the reasoning process to multi-round interactions, exploring lightweight distillation techniques, and applying the approach to other retrieval tasks. The O1 Embedder marks a promising paradigm for next-generation IR systems, showcasing the potential of integrating the classic dense retrieval methods with large language models' outstanding reasoning abilities. 
\clearpage 

% \section{Related Work}

% \subsection{Dense Retrieval}
% Dense retrieval is an advanced technique in the field of information retrieval and natural language processing that leverages dense vector representations to improve the accuracy and efficiency of retrieving relevant information from large datasets. These days, it is extensively used in various significant applications, including open-domain question answering and retrieval-augmented generation. Unlike traditional sparse retrieval methods like BM25, which rely heavily on sparse representations like term frequency-inverse document frequency (TF-IDF) or bag-of-words models, dense retrieval utilizes continuous vector spaces to capture semantic meaning more effectively. The continuous vector often generated by biencoder-based Pre-trained language models (PLMs), like BERT, RoBERTa, T5, significantly enhancing semantic understanding in retrieval tasks. DPR utilizing contrastive learning to fine-tune retrieval models, thereby improving retrieval accuracy through better representation learning. ANCE enhances BERT's capabilities by constructing negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, while TAS-B incorporates topic aware sampling to achieve lower latency. Moreover, dense retrieval systems can benefit from knowledge distillation techniques. Models like RocketQA use a cross encoder, which provide high-quality relevance signals by evaluating pairs of queries and documents together, achieving more efficient dense retrieval. Besides, many models like coCondenser SimLM, RetroMAE proposed different pre-training methods to make the model more adaptable to the retrieval task. More recent studies have shown that training with larger-scale labeled data and using larger models can enhance the performance and generalization of dense retrieval models.
% \subsection{Large Language Model For Retreival}
% %\textcolor{red}{In recent years, the development of large language models (LLMs) has significantly advanced the field of natural language processing, offering unprecedented capabilities in understanding and generating human-like text. Models such as GPT, Llama, and their successors have demonstrated remarkable proficiency in a wide array of tasks, from text completion and translation to more complex applications like summarization and sentiment analysis. These models leverage vast amounts of data and computational power to generate nuanced and contextually rich language representations, unifying a variety of tasks that previously required unique models to do them. 
% %Building on these advancements, LLM-based retrieval models have emerged as a promising direction for improving information retrieval systems. Many retrieval Models have been inspired by LLM, using existing LLM initialization models trained with contrastive learning using larger and more extensive pairs of data to achieve more general and accurate retrieval results. By leveraging the deep semantic understanding of LLMs, these retrieval models can potentially surpass the limitations of earlier systems that relied on small language models. However, a critical observation is that many LLM-based retrieval models have not fully capitalized on the inherent knowledge embedded within LLMs. This underutilization stems from the inconsistency between the training processes for text generation and text representation. While LLMs excel in generating coherent and contextually appropriate text, their training is often not aligned with the specific needs of text representation for retrieval tasks. Consequently, many approaches have focused primarily on scaling up the model size, hoping that larger models will inherently improve retrieval capabilities. This strategy, however, does not necessarily address the fundamental differences in training objectives between generation and representation. }

% In recent years, large language models (LLMs) have greatly advanced natural language processing, providing new capabilities for understanding and generating human-like text. Models like GPT and Llama excel in various tasks, including text completion, translation, summarization, and sentiment analysis. By harnessing the superior text encoding and decoding capabilities of large language models (LLMs), it becomes possible to interpret queries and documents with greater accuracy than earlier, smaller models. A pioneer work is CPT by OpenAI. They treat adjacent text segments as positive pairs to support the unsupervised pre-training of a series of text embedding models with parameters ranging from 300 million to 175 billion. However, unsupervised pre-training a LLM is often too costly. To address this, some studies have utilized pre-trained LLMs to initialize the bi-encoder for dense retrieval. For instance, GTR employs T5-family models, including T5-base, Large, XL, and XXL, for initializing and fine-tuning dense retrievers. Similarly, RepLLaMA fine-tunes the LLaMA model across various stages of information retrieval, including both retrieval and reranking. In the context of dense retrieval, RepLLaMA adds special token </s> to the end of the input sequences and uses the resulting embeddings as their representations. Promptriver, built on RepLLaMA, enhances the model's capability to follow instructions, leading to improvement by incorporating natural language prompts similar to the instruction-tuned generative language models. There are also many retrieval models inspired by LLM using larger and more extensive pairs of data to achieve more general and accurate retrieval results. E5-Mistral directly finetunes a large number of publicly available labeled datasets as well as synthetic data to obtain a general embedding model. GritLM adds fine-tuning data for the generation task during training integrating generation and embedding functionalities in a single model. BGE-ICL leverage the ICL feature in LLMs to enhance the process of text embedding generation.
% However, these LLM-based retrieval models have not fully leveraged the knowledge embedded in LLMs due to a mismatch between training for text generation and text representation. While LLMs are skilled at generating coherent text, their training doesn't always align with the retrieval tasks. 

% In parallel, other approaches have explored similar ideas to our work, particularly through various methods of using LLMs for query expansion. These techniques aim to enhance the retrieval process by generating additional queries or reformulating existing ones to better capture the user's intent and retrieve more relevant results. However, they come with some disadvantages. Firstly, they introduce additional generation model into the retrieval system,  making it more challenging to implement and maintain. The need for a powerful generative model to ensure the quality of expanded queries adds another layer of complexity and resource requirement. Additionally, existing research on query expansion has primarily focused on improving weak dense retrievers, such as unsupervised dense retrievers like Contriever, and sparse retrieval models. It doesn't fully leverage stronger dense retreival models and lack the benefits of end-to-end training.

% \begin{figure*}[h]
%   \centering
%   \includegraphics[width=\linewidth]{img/whiteboard_exported_image (5).pdf}
%   \caption{Data preparation}
%   \label{fig:data}
% \end{figure*}

% \section{Method}

% \subsection{Preliminary}
% Given the user query \( q \) and the document set \( D = \{d_1, \ldots, d_n\} \), where \( n \) is the number of document candidates, the goal of text retrieval is to find the \( k \) documents that are most relevant to the query \( q \), with \( k \ll n \). A dense retrieval model is a mapping function \( f \) that produces embeddings for the query and documents: \( e_q = f(q) \) and \( e_d = f(d) \). The relevance of the query and document is typically determined by the dot product of these two embeddings: \( \text{Sim}(q, d) = \langle e_q, e_d \rangle \). By ranking the similarity scores of each document \( d \) with respect to \( q \), we can obtain the top \( k \) documents relevant to \( q \): \( D_q = \{d \mid d \in \text{top}k(\text{Sim}(q, D))\} \).


% Existing state-of-the-art (SOTA) dense retrieval models typically use a decoder-only architecture, which is consistent with large language models. They first append a special token </s> at the end of the text, then tokenize it to obtain a sequence \( X = [x_1, \ldots, x_T, \text{</s>}] \). This sequence \( X \) is then input into the model for inference, using the last hidden state of </s> as the embedding result:

%  $$e_X = f_{\theta }(X)[-1] $$

% where $f_{\theta }(\cdot)$ refers to a large language model (LLM) like Llama, Mistral, etc., with the lm-head layer removed.


% \subsection{Data Preparation}


% % 
% To enable our retrieval model to possess thinking capabilities, we need supervised labels to teach the model how to leverage its internal knowledge effectively for retrieval tasks. These labels can guide the model in understanding which pieces of information are relevant for specific queries and useful for retrieval. We used Llama-3.1-70B as our teacher model to generate pseudo-documents. These generated pseudo-documents will serve as guidance labels, allowing the retrieval model to continue learning through auto-regression, ultimately enabling our model to possess thinking capabilities. We believe that allowing large models to generate unconstrained content may lead to hallucinations and issues with mismatches to the real corpus. It means that the generated content may not necessarily assist the retrieval model, but rather decrease retrieval accuracy due to noise. We control the generation results of the large model through two methods: in-context constraints and retrieval model committee verification. These processes are illustrated in Figure ~\ref{fig:data}.

% \subsubsection{In-Context Constraints}
% Given a query \( q \), we first randomly sample \( k \) <\( q, d \)> pairs from the training set. These sampled <\( q, d \)> pairs are used as in-context examples to guide the large model in generating documents \( d' \) that are format-wise similar to the corpus. For each query, we perform \( m \) generations to produce \( m \) documents, sampling different in-context examples each time to enhance the diversity of the generated results. The complete prompt examples can be found in Table \ref{tab:prompt}.


% \subsubsection{Retrieval Model Committee Verification}
% After obtaining \( m \) documents, we further filter them by a retrieval model committee. We use small existing general retrieval models to form the committee. The documents are scored by the members of the committee, calculating the similarity between \( d' \) and \( d \), and ranking them. Finally, through a voting process, we select the top-ranked \( d' \) as the final label. After completing these steps, we obtain a dataset consisting of pseudo-documents, composed of <\(q, d, d'\)>. They will then be used for the subsequent training.


% In our experimental implementation, we set both $k$ and $m$ to 3. We selected four models, \textit{BM25}, \textit{bge-en-large-v1.5}, \textit{stella-en-1.5B-v5}, and \textit{gte-large-en-v1.5} to form our retrieval model committee.

% \subsection{Training}

% \begin{figure*}[h]
%   \centering
%   \includegraphics[width=\linewidth]{img/whiteboard_exported_image (7).pdf}
%   \caption{Training and inference pipline}
%   \label{fig:train}
% \end{figure*}

% In previous work, the training process of embedding models often directly destroyed the model's generative capability. The model could no longer think through auto-regression. To enable the model to have embedding capabilities while retaining its generative abilities, we propose a joint training method. This training approach consists of two objectives: Continuous Generation Training and Adaptive Contrastive Learning. Figure \ref{fig:train} illustrate the whole process of model training.

% \subsubsection{Continuous Generation Training}
% To preserve the model's generation ability, we use the previously generated \( d' \) to continue training via next token prediction. Specifically, given \( q \) and \( d' \), we construct a new sentence as follows:


% $$q^{\ast} = \text{<query>}q\text{</query><response>}d'\text{</s>}$$


% Here <query>, </query>, <response> are special tokens used to identify and distinguish the user's query from the content to be generated. For each token \( x_{i} \) in the sentence above, we perform next token prediction training on the \( d' \) portion, optimizing the following objective: 

% $$
% \mathcal{L}_{\mathrm{g}}=- \sum_{i=k+1}^N \log P\left(x_i \mid x_{(<i)}\right)=- \sum_{i=k+1}^N \log f_{\theta,\pi}(x_{(< i)},x_i)
% $$

% Here, \( k \) refers to the index of the special token <response> in the sentence, and \( f_{\theta,\pi} \) denotes the LLM model with a head layer $\pi$ that maps the input tokens to the probability of the next token.

% Through Continuous Generation Training, the model can learn the generation abilities of the teacher model. Specifically, for a given query, the model is able to provide a broad answer based on its knowledge. This answer may not perfectly match the actual corpus even contain hallucinations, but it enhances the model's understanding of the semantic meaning of the user's query, making retrieval easier and more effective.

% \subsubsection{Adaptive Contrastive Learning}
% \label{subsubsection:ada_con_lea}

% We also need to train the model's retrieval capability, which requires utilizing both the original query \( q \) and the content \( d' \) generated by the model previously. We employ the common InfoNCE loss, incorporating both in-batch negatives and hard negatives for training:

% $$
% \mathcal{L}_{\mathrm{r}} = -\log \frac{\exp \left(\text{Sim}\left(f_\theta\left(q_i^{\ast}\right), f_\theta\left(d_i^{+}\right)\right)\right)}{\sum_{j} \exp \left(\text{Sim}\left(f_\theta\left(q_i^{\ast}\right), f_\theta\left(d_{ij}\right)\right)\right)}
% $$

% Here, \( q^{\ast} \) refers to the query augmented with pseudo-documents, \( d^{+}_i \) refers to the positive documents of \( q_i \), \( d_{ij} \) represents all positive and negative documents corresponding to \( q_i \).

% It is important to note that previous work typically used the last hidden state of the </s> token to represent the sentence. To avoid confusion and conflict with the </s> token during Continuous Generation Training, we additionally introduced a special token <emb> at the end of the sentence for text representation.

% During our experiments, we found that if all the training data are added to the pseudo-document \( d' \), the model retrieval becomes overly dependent on \( d' \) (which is a side effect of the effectiveness of \( d' \)). In order to maintain the original fast mode retrieval capability of the model, we adopt a simple strategy that does not add extra training overhead, which we call Adaptive Contrastive Learning. we randomly dropout 10\% of \( d' \):

% $$
% q^{\ast}=\left\{\begin{array}{l@{\quad}l}
% \text{<query>}q\text{</query>} & \text{  with p=10\%} \\
% \text{<query>}q\text{</query><response>}d'\text{</s>} & \text{  with p=90\%}
% \end{array}\right.
% $$

% \subsubsection{Unified Training}

% Finally, we combine the two losses mentioned above and control their relative importance using a weight hyperparameter \( \lambda \). The final loss is given by:

% $$
% \mathcal{L} = \lambda \mathcal{L}_{\mathrm{r}} + (1 - \lambda) \mathcal{L}_{\mathrm{g}}
% $$

% Here, \( \mathcal{L}_{\mathrm{r}} \) represents the retrieval loss, and \( \mathcal{L}_{\mathrm{g}} \) represents the generation loss. This formulation allows for a balanced training approach, optimizing both retrieval and generative capabilities of the model.

% \subsection{Retrieval in Slow Mode}

% During the inference phase, we propose a method that involves thoughtful retrieval. For a user's query, the model first generates some potentially relevant paragraphs based on its internal knowledge. These paragraphs may not perfectly match the actual corresponding documents, but they help the model better understand the semantic meaning of the user's query and provide a reference that facilitates the retrieval of relevant content.

% Specifically, for a query \( q \), we format it as the template: 
% $$\text{<query>}q\text{</query><response>}$$
% The model then generates content token-by-token, denoted as \( g_{\theta,\pi}(q) \). This thought process aligns with the model's pre-training method, allowing it to leverage the advantages of pre-training more effectively.

% To ensure the robustness of the retrieval process, we sample the generated content \( N \) times, resulting in:

% $$
% \hat{q^{\ast}} \sim g_{\theta,\pi}(q)
% $$

% At this point, \( \hat{q^{\ast}} \) contains not only the original query but also a "reference answer" predicted by the model. Finally, we estimate the final representation of the query using the following equation:

% $$
% e_{q} = E[f_{\theta}(\hat{q^{\ast}})] = \frac{1}{N} \sum_{i=1}^{N} f_{\theta}(\hat{q^{\ast}})
% $$

% This representation captures the enriched understanding of the query, incorporating both the original input and the model's generated insights. We calculate the similarity between the new query representation \( e_q \) and the embeddings \( e_d \) of each document in the corpus using dot product. We retrieve the top \( k \) documents based on their similarity scores.

% \section{Experimental Setups}
% \subsection{Datasets}
% We train our model in our new MS MARCO+ dataset and want to evaluate our model's ability to "think" before retrieval. To assess its performance, we employ a combination of in-domain and out-of-domain datasets, ensuring a comprehensive evaluation across various contexts and domains. For in-domain evaluation, we utilize MS MARCO and TREC DL19, TREC DL20 datasets. To evaluate the generalizability of our model beyond the training domain, we incorporate eight asymmetric task datasets from the BEIR benchmark for out-of-domain evaluation: SciFact, TREC-COVID, DBpedia, Natural Questions (NQ), HotPotQA, FiQA, Touche(Touché-2020), FEVER. These asymmetric tasks are more difficult, unlike other symmetric tasks that only need to find text that is similar to the query. It is more worthwhile for the model to think before retrieving.
% We use MRR@10 and Recall@1k as the metric of MS MARCO dataset, and NDCG@10 for other datasets following \textcolor{red}{RepLLaMA}

% By leveraging these diverse datasets, we aim to demonstrate our model's retrieval capabilities and its potential to "think" across various domains and tasks. This comprehensive evaluation framework ensures that our model is not only effective within its training domain but also adaptable to new and unseen challenges because of its extensive and diverse internal knowledge.
% \subsection{Baselines}

% We choose our comparison baseline from the following aspects. The first one is the commonly used sparse retrieval method BM25. the second one is some BERT-based models: ANCE, TAS-B, coCondenser, SimLM, RetroMAE. These model usually have smaller sizes, but they generally perform unsupervised pretraining on large-scale corpus close to downstream retrieval tasks. Finally, we compare some LLM-based models, which are also our main comparators, RepLLaMA, Promptriver. RepLLaMA uses LLAMA2 to initialize the model, and is trained to obtain a retrieval model by comparative learning on the MS MARCO dataset. Promptriver, based on RepLLaMA, empowers the model with the ability of instruction following, which can give the model better results by adding natural language prompt. For the comparison baseline of the OOD dataset, except for BM25, we mainly choose some large models , which usually have stronger generalization. Note that although there are many general embedding models now with very good retrieval results, such as e5-mistral-instruct, these models were trained on significantly more data than just MS MARCO, including the training sets of BEIR. we wanted to evaluate the true zeor-shot capabilities of the models, and such evaluation is consistent with the original intent of BEIR's creation, so we did not compare these models that were supervised trained directly on the BEIR training sets.


% \subsection{Implementation Details}
% During the data preparation phase, we deployed the meta-llama/Llama-3.3-70B-Instruct model using VLLM. We set the temperature to 1 and the max tokens to 256. The entire data generation process took about 2 days on 8 A100 (40G) GPUs.

% In the model training process, we primarily followed the training recipe from RepLLaMA. We fine-tuned all linear layers of the model q\_proj k\_proj v\_proj o\_proj gate\_proj down\_proj up\_proj using LoRA, with a rank set to 32 and alpha set to 64. We used BF16 for training, with a learning rate set to \( 1 \times 10^{-4} \) for 1 epoch. The batch size was set to 64 (8 GPUs, with a per-device training batch size of 8), with 15 hard negative passages for each query. The maximum query length was set to 32, and the maximum passage length was set to 192.The max tokens were set to 256 for next token prediction training.

% \section{Main Results and Analysis}

% Our main experiments are designed to answer the following questions:

% \begin{itemize}
%   \item[\textbf{RQ 1.}] Does our model outperforms corresponding baselines for in-domain datasets? % in domain performence
%   \item[\textbf{RQ 2.}] Can better results be achieved by thinking when the model encounters untrained out-of-domain problems? % ood performence
%   \item[\textbf{RQ 3.}] How much does thinking help? % slow mode vs fast mode
%   % \item[\textbf{RQ 4.}] How much does adaptive contrastive learning with our MS MARCO+ dataset help? % our model vs RepLLaMA+llama2-chat
% \end{itemize}

% \subsection{Supervised Performance}
% \label{subsection: Superviesd Performance}

% \begin{table*}[]
% \caption{MS MARCO results measured by MRR@10, Recall@1k, nDCG@10.}
%   \label{tab:MSMARCO}
% \begin{tabular}{c|c|c|cc|c|c}
% \hline
%                             & \multirow{2}{*}{method} & \multirow{2}{*}{model size} & \multicolumn{2}{c|}{MS MARCO Dev} & DL'19         & DL'20         \\ \cline{4-7} 
%                             &                         &                             & MRR@10          & Recall@1k       & nDCG@10       & nDCG@10       \\ \hline
% Sparse                      & BM25\cite{thakur_beir_2021}                    & -                           & 18.4            & 85.3            & 50.6          & 48.0          \\ \hline
% \multirow{5}{*}{BERT-based} & ANCE\cite{xiong_approximate_2020}                    & 125M                        & 33.0            & 95.9            & 64.8          & 61.5          \\
%                             & TAS-B\cite{hofstatter_efficiently_2021}                   & 55M                         & 34.3            & 97.6            & 72.2          & 69.2          \\
%                             & coCondenser\cite{gao_unsupervised_2022}             & 110M                        & 38.2            & 98.4            & 69.8          & 68.4          \\
%                             & SimLM\cite{wang_simlm_2023}                   & 110M                        & 41.1            & 98.7            & 71.4          & 69.7          \\
%                             & RetroMAE\cite{liu_retromae-2_2023}                & 110M                        & 41.6            & 98.8            & 68.1          & 70.6          \\ \hline
% \multirow{4}{*}{LLM-based}  & RepLLaMA\cite{ma_fine-tuning_2024}                & 7B                          & 41.2            & 99.4            & 74.3          & 72.1          \\
%                             & Promptriever\cite{weller_promptriever_2024}            & 7B                          & 41.0            & 99.4            & 73.2          & 72.3          \\
%                             & our model wo thinking   & 7B                          & 41.7            & 99.4            & 73.7          & 72.3          \\
%                             & our model               & 7B                          & \textbf{43.1}   & \textbf{99.5}   & \textbf{75.3} & \textbf{74.4} \\ \hline
% \end{tabular}
% \end{table*}

% The performance analysis of the models, as detailed in the Table \ref{tab:MSMARCO}, clearly demonstrates that our proposed model achieves the highest scores across all evaluation metrics, including MRR@10, Recall@1k, and nDCG@10, on the MSMARCO, DL'19, and DL'20 datasets. Specifically, our model attains an MRR@10 of 43.1(\textbf{+1.9\%} improvement) and a Recall@1k of 99.5 on MSMARCO, along with nDCG@10 scores of 75.3(\textbf{+1.0\%} improvement) and 74.4(\textbf{2.1\%} improvement) on DL'19 and DL'20, respectively, outperforming all other models. This underscores the effectiveness of our model's design and its ability to excel in retrieval tasks. Furthermore, when comparing model sizes, it is evident that larger models, such as the LLM-based RepLLaMA and Promptriever, generally outperform smaller BERT-based models. For instance, RepLLaMA achieves an nDCG@10 of 74.3 on DL'19 and 72.1 on DL'20, surpassing all the smaller models. However, it is noteworthy that small BERT-based models, like RetroMAE, can still perform competitively when pre-trained with objectives closely aligned with retrieval tasks. RetroMAE achieves an MRR@10 of 41.6 on MSMARCO, closely approaching the performance of much larger models like RepLLaMA, which scores 41.2. This indicates that task-specific pre-training can significantly enhance the capabilities of smaller models. On the contrary, the LLM-based models, despite their substantial size, face limitations due to pre-training on objectives that are not directly aligned with retrieval tasks, which hinders their ability to fully capitalize on pre-training knowledge.

% The comparison between "our model" and its counterpart, "our model wo thinking (fast mode)," vividly illustrates the transformative impact of integrating a "thinking" mechanism on retrieval performance. "our model" achieves a remarkable MRR@10 of 43.1, surpassing the fast mode variant's 41.7. 
% Moreover, the nDCG@10 scores for the DL'19 and DL'20 benchmarks further accentuate this improvement, with increases of 1.6 and 2.1 points, respectively. These enhancements highlight the critical role of the "thinking" mechanism in enriching the retrieval process. By delving deeper into the additional contextual information, the model significantly enhances its comprehension of the user's query, paving the way for a more nuanced understanding. This enriched understanding allows the model to navigate the retrieval process with greater ease and precision, ultimately leading to superior performance.

% It is noteworthy that even in fast mode, our model achieves results comparable to those of RepLLaMA, a testament to the efficacy of our adaptive contrastive learning strategy. This innovative approach not only sustains impressive retrieval performance in resource-constrained scenarios but also underscores the remarkable flexibility and adaptability of our model. Such versatility ensures that our model remains robust and efficient, regardless of the computational limitations it may encounter.

% % TODO: 
% % - slow mode vs fast mode
% % - our model vs RepLLaMA+llama2-chat

% \subsection{Zero-shot Generalization}
% \label{subsection: Zero-shot Generalization}

% % \begin{table*}[]
% % \caption{Zero-shot reults measured by nDCG@10}
% %   \label{tab:BEIR}
% % \begin{tabular}{c|c|cccccccc|c}
% % \hline
% % Method                & Model size & Trec-Covid    & NQ            & HotPotQA      & FiQA          & Touche        & DBPedia       & FEVER       & SciFact       & Average       \\ \hline
% % BM25                  & -          & 65.6          & 32.9          & 60.3          & 23.6          & \textbf{36.7} & 31.1          & 75.3        & 66.5          & 49            \\
% % CPT-L                 & 6B         & 56.2          & -             & 64.8          & 45.2          & 30.9          & 41.2          & 75.6        & 74.4          & -          \\
% % CPT-XL                 & 175B       & 64.9          & -             & 68.8          & \textbf{51.2} & 29.1          & 43.2          & 77.5        & 75.4          & -          \\
% % Ada-2                 & -          & 81.3          & 48.2          & 65.4          & 41.1          & 28            & 40.2          & 23.7        & 73.6          & 50.2          \\
% % RepLLaMA              & 7B         & 84.7          & 62.4          & 68.5          & 45.8          & 30.5          & 43.7          & 83.4        & 75.6          & 61.8          \\
% % Promptriver           & 7B         & 84.6          & 62.6          & 69.5          & 46.6          & 32            & 45.2          & 82.8        & 76.3          & 62.5          \\
% % our model wo thinking & 7B         & 84.5          & 62.9          & 69.8          & 45            & 33.8          & 44.4          & 82.5        & 75.8          & 62.3          \\
% % our model             & 7B         & \textbf{85.6} & \textbf{66.8} & \textbf{72.8} & 46.7          & \textbf{36.7} & \textbf{47.3} & \textbf{85} & \textbf{77.4} & \textbf{64.8} \\ \hline
% % \end{tabular}
% % \end{table*}

% \begin{table*}[]
% \begin{tabular}{c|c|ccccccccc|c}
% \hline
% Method          & Model size & Trec-Covid & NQ   & HotPotQA & FiQA & Touche & DBPedia & FEVER & SciFact & CosQA & Average \\ \hline
% BM25            & -          & 65.6       & 32.9 & 60.3     & 23.6 & 36.7   & 31.1    & 75.3  & 66.5    & -     & -       \\
% contriever      & 0.1B       & 59.6       & 49.8 & 63.8     & 32.9 & 23.0   & 41.3    & 75.8  & 67.7    & 14.2  & 47.6    \\
% bge-m3          & 0.5B       & 39.5       & 60.6 & 69.4     & 41.0 & 22.3   & 39.8    & 80.9  & 64.2    & 22.7  & 48.9    \\
% CPT-L           & 6B         & 56.2       & -    & 64.8     & 45.2 & 30.9   & 41.2    & 75.6  & 74.4    & -     & -       \\
% CPT-XL          & 175B       & 64.9       & -    & 68.8     & 51.2 & 29.1   & 43.2    & 77.5  & 75.4    & -     & -       \\
% OpenAI-Ada-002  & -          & 81.3       & 48.2 & 65.4     & 41.1 & 28.0   & 40.2    & 23.7  & 73.6    & 28.9  & 47.8    \\
% RepLLaMA        & 7B         & 84.7       & 62.4 & 68.5     & 45.8 & 30.5   & 43.7    & 83.4  & 75.6    & 32.3  & 58.5    \\
% Promptriver     & 7B         & 84.6       & 62.6 & 69.5     & 46.6 & 32.0   & 45.2    & 82.8  & 76.3    & 32.8  & 59.2    \\
% our wo thinking & 7B         & 84.5       & 62.9 & 69.8     & 45.0 & 33.8   & 44.4    & 82.5  & 75.8    & 32.9  & 59.1    \\
% our             & 7B         & 85.6       & 66.8 & 72.8     & 46.6 & 36.7   & 47.3    & 84.9  & 77.4    & 34.1  & 61.4    \\ \hline
% \end{tabular}
% \end{table*}

% We can draw a similar conclusion from Table \ref{tab:BEIR}, where we achieved very good results in 8 datasets . On average, our model has an overall improvement of \textbf{2.3\%} compared to baseline, which is a huge improvement and shows the strong generalization performance of our model. In addition, for each of the 8 datasets, our model achieves a new state-of-the-art-result, except for FiQA, where our model does not outperform CPT-XL with 175B, which demonstrates that our approach is very effective in different domains. This is due to the extensive and comprehensive knowledge of the LLM obtained through extensive corpus pre-training.

% Some interesting conclusions can be obtained from the table, the model can improve more significantly on some OpenQA datasets through the thinking mechanism, for example, the NQ dataset can be improved by \textbf{3.9\%} through the thinking mechanism, and the hotpotqa can be improved by \textbf{3.0\%}. On the other hand, although there is improvement in some specialized domains, the improvement is not so big, for example, in Trec-Covid (medical domain), FiQA (financial domain) and SciFact (Scientific domain) the improvement brought by thinking is only 0.9\% , 1.7\% and 1.6\%. We believe that this is mainly because LLM itself is better at these OpenQA questions after large-scale corpus training, while LLM often lacks the training of such specialized domain data, resulting in generated content that may be noisy or even hallucination. Although it can also bring about a certain amount of enhancement, the enhancement is not as significant. In the future, how to distill and denoise the thinking content will also be an important research direction.

% \subsection{Summary}

% Given the analysis from Section \ref{subsection: Superviesd Performance} and \ref{subsection: Zero-shot Generalization}, we can draw the following conclusions in response to \textbf{RQ 1, RQ 2} and \textbf{RQ 3}:

% \begin{itemize}
%   \item[\textbf{Con 1.}] Our model achieves notably higher retrieval performances over amount of BERT-based and LLM-based model , indicating that our model's query representation capability is substantially enhanced by thinking.% in domain performence
%   \item[\textbf{Con 2.}]  Our model achieves remarkable performances throughout different domain in zero-shot evaluation, indicating that the generalization of our method. When encountering more difficult, out-of-domain problems, the model can also make the retrieval easier by thinking. Thinking proves to be a key factor in addressing challenging out-of-domain problems effectively.% ood performence
%   \item[\textbf{Con 3.}]  The help of thinking is Significant! It serves as a powerful tool in enhancing query representation and improving retrieval performance across various domains.  % slow mode vs fast mode
%   % \item[\textbf{RQ 4.}] How much does adaptive contrastive learning with our MS MARCO+ dataset help? % our model vs RepLLaMA+llama2-chat
% \end{itemize}


% \section{Further Analysis}
% In the above section we described the superiority of the effect of our method. Next, we will analyze it further in this section. We mainly want to analyze the following questions:

% \begin{itemize}
%   \item[\textbf{RQ 4.}] How much does our training help? %一方面的是生成的内容更适合检索，另一方面是端到端训练检索模型能更好的利用生成的内容
%   \item[\textbf{RQ 5.}] Is our approach universal and robust?% different model/hyperparameters
%   \item[\textbf{RQ 6.}] why our method can achieve such a huge improvement? % explaination
% \end{itemize}

% In the next subsections \ref{subsection: RepLLaMA with thinking}, \ref{subsection: Robustness}, \ref{subsection: Case Study}, we will answer to the above questions in sequence.

% \subsection{RepLLaMA with thinking}
% \label{subsection: RepLLaMA with thinking}

% \begin{table}[]
% \caption{Further Analysis}
%   \label{tab: RepLLaMA+}
% \begin{tabular}{c|ccc|ccc}
% \hline
%            & \multicolumn{3}{c|}{RepLLaMA} & \multicolumn{3}{c}{ours} \\ \cline{2-7} 
%            & base    & with T    & $\Delta$   & base  & with T  & $\Delta$  \\ \hline
% Trec-Covid & 84.7    & 79.7      & -5.0      & 84.5  & 85.6    & +1.1    \\
% NQ         & 62.4    & 65.1      & +2.7     & 62.9  & 66.8    & +3.9    \\
% HotPotQA   & 68.5    & 71.7      & +3.2     & 69.8  & 72.8    & +3.0      \\
% FiQA       & 45.8    & 40.2      & -5.6    & 45    & 46.6    & +1.6    \\
% Touche     & 30.5    & 33.3      & +2.8     & 33.8  & 36.7    & +2.9    \\
% DBPedia    & 43.7    & 44.2      & +0.5     & 44.4  & 47.3    & +2.9    \\
% FEVER      & 83.4    & 85.5      & +2.1     & 82.5  & 85      & +2.5    \\
% SciFact    & 75.6    & 74.2      & -1.4    & 75.8  & 77.4    & +1.6    \\ \hline
% Avg        & 61.8    & 61.7      & -0.1    & 62.3  & 64.8    & +2.4    \\ \hline
% \end{tabular}
% \end{table}

% In order to validate the improvement brought by our training pipline, we first need to validate whether the existing retreival model can utilize the thinking content well. We do this directly by prompting the LLM, letting the model generate possible answers based on its own knowledge. The answers are then added to the original query as thinking content. The relevant documents are retrieved by the new query using RepLLaMA, which is aligned with our retrieval with thinking approach, except that here we need a second generative model and need to do the operation of writing prompt. We use the mistralv0.3-instruct-7B model because of its excellent performance on various generative tasks. The prompts we use are the same as the data generation process: "\textit{Please write a passage that answers the given query.}"

    
% Table \ref{tab: RepLLaMA+} shows the results of the experiments. As we can see from the table, while the addition of thinking content yielded improvements in certain datasets like NQ and HotPotQA, it also resulted in declines in others, particularly Trec-Covid and FiQA. On average, this method produced negligible improvements, with a slight decrease of 0.1 overall. In contrast, our model consistently enhanced retrieval performance across all datasets. It shows that by training, our model can better utilize the content of thinking. The untrained RepLLaMA model appears to be adversely affected by noise from the generated content, leading to degraded results, especially in specialized domains where large language models may struggle, such as Trec-Covid, FiQA, and SciFact. This analysis highlights that our training approach equips the model to better navigate and utilize thinking content, thereby achieving superior retrieval results.


% \subsection{Robustness}
% \label{subsection: Robustness}

% In this section we verify the generalization and robustness of our method in two main ways. First we study the effect of different base model for the same size. Second we study the effect of different models with different sizes.

% \subsubsection{The Effect of Different Base Models}

% \begin{table}[]
% \caption{The Effect of Different Base Models}
%   \label{tab: base model}
% \begin{tabular}{c|ccc|c}
% \hline
%                & \multicolumn{3}{c|}{Supervised} & Zero-shot \\
%                & MS MARCO  & DL'19  & DL'20  & BEIR AVG  \\ \hline
% Llama-2-7B     & 43.1          & 75.3   & 74.4   & 64.7      \\
% Mistralv0.3-7B & 43.5          & 77     & 75.6   & 64.6      \\
% Llama-3.1-8B   & 43.5          & 76.2   & 74.5   & 64.9      \\
% Qwen2.5-7B     & 43.3          & 76.4   & 74.7   & 64.5      \\
% Qwen2.5-3B     & 42.5          & 76.3   & 74.5   & 63.5      \\
% Qwen2.5-1.5B   & 41.9          & 74     & 73.5   & 61.8      \\
% Qwen2.5-0.5B   & 40.5          & 73.6   & 71.4   & 58.1       \\ \hline
% \end{tabular}
% \end{table}

% The previous experiments primarily used Llama-2-7B as the backbone, which is consistent with RepLLaMA and promptriever. The main reason for this is to provide a fair comparison with these two baselines. To verify the generalizability of our approach, we performed the same experiment with different base model.
% The data presented in Table \ref{tab: base model} illustrates the performance of various base models.  Notably, our approach demonstrates versatility and robustness, as it effectively leverages different backbone models to achieve similar results in both supervised and zero-shot evaluation. Across the supervised datasets, all models exhibit similar performance levels. This indicates that our method is adaptable and can extract meaningful information from various architectures, regardless of their specific training background. The slight variations in performance suggest that while the backbone model does play a role, the effectiveness of our approach is not heavily reliant on architectures and pre-training backgrounds. In the zero-shot evaluation, the performance remains consistent. The comparable results across various base models highlight the strength of our approach in utilizing the inherent capabilities of these base models, regardless of their training specifics.

% \subsubsection{The Effect of Different Model Sizes}

% The results from the Qwen2.5 series of models in Table 4 clearly illustrate the significant impact of model size. As the size of the Qwen2.5 models decreases, there is a noticeable decline in effectiveness across all evaluated datasets. A 3B model can still have an effect close to that of a 7B model, accompanied by a 0.8\% drop on the MS MARCO dataset and a 1.0\% drop on zero-shot, but when the model is smaller, the effect is more pronounced. This is partly due to the fact that larger model sizes have better generalization capabilities. At the same time, a larger model tends to train more corpus during pre-training, and the knowledge it has is richer so that it can perform better in zero-shot evaluation.
% Notice that even a 1.5B model, through retreival with thinking, performs comparably to the 7B RepLLaMA, which once again illustrates the superiority of our approach.

% \subsection{Case Study}
% \label{subsection: Case Study}

% \begin{table}[ht]
% \begin{tabular}{p{0.9\linewidth}}
% \hline
% \textbf{Query:}   what age was martin luther king when he was admitted      \\
% \hline
% \textbf{Thinking content:}   Martin Luther King Jr. was admitted to \textcolor{softgreen}{\textbf{Morehouse College in Atlanta}}, Georgia \textcolor{softgreen}{\textbf{at the age of 15}}. He attended the college from \textcolor{softgreen}{\textbf{1944}} to 1948, where he earned a Bachelor of Arts degree in sociology.   \\
% \hline
% \textbf{Positive Doc:}   Dr. Martin L. King, Jr. and His Mentors: A Message for America Today If it was not for Benjamin Mays... Benjamin Mays was the president of \textcolor{softgreen}{\textbf{Morehouse College in Atlanta}} when he met Martin Luther King, Jr. In \textcolor{softgreen}{\textbf{1944}}, Martin Luther King was admitted to the college \textcolor{softgreen}{\textbf{at age 15}}. ...   \\
% \hline
% \hline
% \textbf{Query:}   Hayden is a singer-songwriter from Canada, but where does Buck-Tick hail from?      \\
% \hline
% \textbf{Thinking content:}   \textcolor{softgreen}{\textbf{Buck-Tick}} is a \textcolor{softgreen}{\textbf{Japanese}} rock band, and its members are from various parts of \textcolor{softgreen}{\textbf{Japan}} and their music is a unique blend of alternative rock, gothic rock, and visual kei styles.   \\
% \hline
% \textbf{Positive Doc:}   Buck-Tick \textcolor{softgreen}{\textbf{Buck-Tick}} (stylized as BUCK-TICK) is a \textcolor{softgreen}{\textbf{Japanese}} rock band, formed in Fujioka, Gunma in 1983. The group has consisted of ...   \\
% \hline
% \end{tabular}
% \caption{Examples of the original query, thinking content and the positive document. The similar patterns between gnenrated content and the groundtruth are shown in \textcolor{softgreen}{\textbf{green}}.}
% \label{tab:case1}
% \end{table}

% In Table~\ref{tab:case1}, we show an example of the generated content and the ground-truth document. In the presented case, the query asks about Martin Luther King's age upon his admission to college. The "thinking content" generated by our model effectively synthesizes relevant contextual information, highlighting that King was admitted to Morehouse College at the age of 15 in 1944. This generated content not only answers the query directly but also enriches the context by providing additional details about the college and the timeframe of his attendance. The positive document corroborates this information, confirming that King was indeed admitted to Morehouse College at the specified age. The alignment between the generated thinking content and the positive document demonstrates how our model's deeper contextual analysis can significantly enhance retrieval accuracy. By generating relevant information that mirrors the ground truth, the model effectively bridges the gap between the user's query and the detailed content, ensuring that critical information is not missed during the retrieval process. This capability underscores the value of incorporating a "slow thinking" mechanism, enabling the model to achieve better retrieval result. More cases and analyses are shown in Appendix~\ref{section:additional_case_study}.

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.9\linewidth]{img/attention-top20.pdf}
%   \caption{Top 20 Attention score from <emb> token in the thinking content}   
% \end{figure}

% % analysis the fig
% % 1. similar patterns have high score indicate the model successfully pay attention to these patterns. 
% % 2. 这些生成的thinking content在检索过程中起了作用，并且为query补充了更多的信息例如Japan这个并没有在query中出现的token


% % why our method can achieve such a huge improvement?
% % 1. 思考促使模型生成更多与query相关的中间模式（patterns），这些模式有助于模型更准确地捕query的关键信息，提升检索效果。
% % 1. 思考过程允许模型生成中间结果，这些结果可以作为后续检索的基础，帮助模型更好地理解问题并减少错误。（multihop retrieval）

% \section{Conclusion}

% In conclusion, this paper introduced PLATO, a novel retrieval model integrating a "slow thinking" mechanism with generative capabilities. Through constructing the MS MARCO+ dataset and an end-to-end training framework, PLATO can generate relevant information for retrieval while learning to utilize it effectively.

% Our comprehensive evaluation across 11 information retrieval datasets demonstrated that PLATO in slow mode significantly outperformed existing state-of-the-art retrieval models. In supervised performance, it achieved higher scores on in-domain datasets like MS MARCO, DL'19, and DL'20. In zero-shot generalization, it showed strong performance across various out-of-domain datasets from the BEIR benchmark.

% Moreover, the "thinking" mechanism in PLATO was proven to be a significant factor in enhancing query representation and retrieval performance. Different base models and model sizes also showed the robustness and versatility of our approach. 

% % \section{Limitation \& Future Work}



%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}


\clearpage

%%
%% If your work has an appendix, this is the place to put it.
\appendix

% \section{Impact of pseudo-document dropout}

% \begin{table}[h]
% \begin{tabular}{c|c|cc}
% \hline
%              & MS MARCO  & DL'19     & DL'20     \\ \hline
% our model    & 41.7/43.1 & 73.7/75.3 & 72.3/74.4 \\
% w.o. dropout & 40.5/42.7 & 73.2/75.5 & 71.1/74.3 \\ \hline
% \end{tabular}
% \caption{Impact of pseudo-document dropout. The two results for the dataset in the table represent the results for the fast mode(left) and slow mode(right), respectively}
%     \label{tab:dropout}
% \end{table}

% In section~\ref{subsubsection:ada_con_lea}, we introduce a simple dropout method to retain the original fast mode retrieval capability. Here we explore the impact of this strategy. From Table \ref{tab:dropout}, we can observe that when the dropout strategy is not used, the performance of the model in fast mode is significantly degraded. While if we keep just 10\% original query, the capability of fast thinking can be repaired easily with no influence on higher performance in slow mode. This performance degradation in fast mode can be explained by the fact that the model has only ever seen the enhanced query during training.This is a shortcut for the model. Model may overfit the enhanced portion of the training process and ignores the original query.So when the model is evaluated in fast mode, where the model is retrieved directly from the original query, the performance decreases significantly.





\section{Prompts}
\label{section:prompts}

% \begin{table}[ht]
% \begin{tabular}{p{0.15\linewidth} | p{0.7\linewidth}}
% \hline
% \centering System prompts  &   You are a helpful assistant. Your anwer should be follow the task description. Do not ask the user for further clarification. Don't repeat the query, just give the response.   \\
% \hline
% \centering User prompts    &   Task: \newline Please write a passage that answers the given query. \newline \newline Examples: \newline    Query: love ranch movie cast    \newline
% Response: Love Ranch Love Ranch. Love Ranch is ...<omit> \newline
% %a 2010 American drama film directed by Taylor Hackford and starring Helen Mirren, Joe Pesci, Sergio Peris-Mencheta, Gina Gershon and Bryan Cranston. It was written by Mark Jacobson.    \newline
% Query: hanover va real estate tax   \newline
% Response: Hanover County Virginia Property Tax Median Income In. Hanover County, Virginia. The median property tax (also known as real estate tax) in Hanover County is \$1,922.00 per year, based on ...<omit> \newline
% %a median home value of \$281,600.00 and a median effective property tax rate of 0.68\% of property value. \newline
% Query: what is polymyositis symptoms    \newline
% Response: - Polymyositis is a systemic disease, which means it affects the whole body. Muscle weakness and tenderness can ...<omit> 
% %be signs of polymyositis. A rash is a sign of a similar condition, dermatomyositis.Common symptoms include: 1  Difficulty swallowing.2  Muscle pain. 3  Muscle weakness in the proximal muscles (shoulders, hips, etc.) this can make it hard to raise the arms over the head, get up from a sitting position, or climb stairs.ack to Top Symptoms. Polymyositis is a systemic disease, which means it affects the whole body. Muscle weakness and tenderness can be signs of polymyositis. A rash is a sign of a similar condition, dermatomyositis. Common symptoms include: 1  Difficulty swallowing. 2  Muscle pain.  
% \newline \newline   Query: \newline what does physical medicine do \newline    Response:   \\
% \hline
% \centering Response    &   Physical medicine, also known as physiatry, is a branch of medicine that focuses on helping patients with physical disabilities or impairments to achieve maximum function and independence. Physiatrists, or physical medicine doctors, use a multidisciplinary approach to diagnose and treat a wide range of conditions, ...<omit> \\
% %including musculoskeletal disorders, neurological conditions, and chronic pain.Physical medicine doctors use various techniques, such as physical therapy, occupational therapy, and pain management, to help patients recover from injuries or illnesses, manage chronic conditions, and improve their overall quality of life. They may also use advanced technologies, such as electromyography (EMG) and nerve conduction studies, to diagnose and treat conditions.Some common conditions treated by physical medicine doctors include back pain, arthritis, spinal cord injuries, stroke, and traumatic brain injuries. Physical medicine doctors work with patients to develop personalized treatment plans that address their unique needs and goals, with the aim of restoring function, reducing pain, and improving overall well-being.   \\  
% \hline
% \end{tabular}
% \caption{Prompts}
% \label{tab:prompt}
% \end{table}

\begin{table}[ht]
\begin{tabular}{p{0.9\linewidth}}
\hline
System Prompt \\
\hline
   You are a helpful assistant. Your anwer should be follow the task description. Do not ask the user for further clarification. Don't repeat the query, just give the response.   \\
\hline
\hline
User Prompt \\
\hline
Task: \newline Think about a plausible response to address the query \newline \newline Examples: \newline    Query: love ranch movie cast    \newline
Response: Love Ranch Love Ranch. Love Ranch is ...<omit> \newline
%a 2010 American drama film directed by Taylor Hackford and starring Helen Mirren, Joe Pesci, Sergio Peris-Mencheta, Gina Gershon and Bryan Cranston. It was written by Mark Jacobson.    \newline
Query: hanover va real estate tax   \newline
Response: Hanover County Virginia Property Tax Median Income In. Hanover County, Virginia. The median property tax (also known as real estate tax) in Hanover County is \$1,922.00 per year, based on ...<omit> \newline
%a median home value of \$281,600.00 and a median effective property tax rate of 0.68\% of property value. \newline
Query: what is polymyositis symptoms    \newline
Response: - Polymyositis is a systemic disease, which means it affects the whole body. Muscle weakness and tenderness can ...<omit> 
%be signs of polymyositis. A rash is a sign of a similar condition, dermatomyositis.Common symptoms include: 1  Difficulty swallowing.2  Muscle pain. 3  Muscle weakness in the proximal muscles (shoulders, hips, etc.) this can make it hard to raise the arms over the head, get up from a sitting position, or climb stairs.ack to Top Symptoms. Polymyositis is a systemic disease, which means it affects the whole body. Muscle weakness and tenderness can be signs of polymyositis. A rash is a sign of a similar condition, dermatomyositis. Common symptoms include: 1  Difficulty swallowing. 2  Muscle pain.  
\newline \newline   Query: \newline what does physical medicine do \newline    Response:   \\
\hline
\hline
Response \\
\hline
Physical medicine, also known as physiatry, is a branch of medicine that focuses on helping patients with physical disabilities or impairments to achieve maximum function and independence. Physiatrists, or physical medicine doctors, use a multidisciplinary approach to diagnose and treat a wide range of conditions, 
%...<omit> \\
including musculoskeletal disorders, neurological conditions, and chronic pain.Physical medicine doctors use various techniques, such as physical therapy, occupational therapy, and pain management, to help patients recover from injuries or illnesses, manage chronic conditions, and improve their overall quality of life. They may also use advanced technologies, such as electromyography (EMG) and nerve conduction studies, to diagnose and treat conditions.Some common conditions treated by physical medicine doctors include back pain, arthritis, spinal cord injuries, stroke, and traumatic brain injuries. Physical medicine doctors work with patients to develop personalized treatment plans that address their unique needs and goals, with the aim of restoring function, reducing pain, and improving overall well-being.   \\  
\hline
\end{tabular}
\caption{Example of the Prompt and the Response of the teacher model for data generation. For presentation purposes, we use <omit> to omit the long documentation in the example.}
\label{tab:prompt}
\end{table}

Table \ref{tab:prompt} shows the example of complete prompts and the response of the teacher model used to generate the training data. For each specific query, we randomly select 3 query and positive document pair to form the in-context examples. During the data generation process, we found that a significant portion of the responses duplicated the query provided. So we add "\textit{Don’t repeat the query, just give the response.}" in the system prompts.
\section{Detailed Scores of Different Base Models}
\label{app:detail score}
Table~\ref{tab:detailed scores} provides the detailed scores of different base model for zero-shot evaluation in section~\ref{subsection: Robustness}. We can see that although the results of different base models fluctuate a bit in each dataset, there is not much difference in the overall performance of models of the same scale. While when the model parameters decreased, there was a significant decrease in the final results. This suggests that larger models are more capable of generating and exploiting high-quality thoughts and have greater generalization.

\begin{table*}[ht]
\scalebox{1.0}{
% \begin{tabular}{c|ccccccccc|cc}
% \hline
%                         & MS MARCO & Trec-Covid & NQ   & HotPotQA & FiQA & Touche & DBPedia & FEVER & SciFact & AVG 8 BEIR & AVG + msmarco \\ \hline
% e5-mistral              & 43.1     & 87.0       & 63.5 & 75.7     & 56.8 & 26.3   & 48.9    & 87.8  & 76.4    & 65.3           & 62.8          \\
% gritlm                  & 42.0     & 74.3       & 70.3 & 79.4     & 59.9 & 27.8   & 46.6    & 82.7  & 79.1    & 65.0           & 62.5          \\
% llm2vec-llama3          & 43.2     & 80.3       & 64.2 & 71.8     & 55.3 & 20.5   & 48.3    & 90.2  & 78.2    & 63.6           & 61.3          \\
% gte-qwen1.5-7b          & 41.7     & 72.7       & 63.1 & 72.3     & 55.3 & 20.3   & 48.0    & 93.4  & 75.3    & 62.6           & 60.2          \\
% text-embedding-3-large  & 40.2     & 79.6       & 61.3 & 71.6     & 55.0 & 23.4   & 44.8    & 87.9  & 77.8    & 62.7           & 60.2          \\
% voyage-lite-02-instruct & 37.9     & 81.0       & 64.3 & 75.5     & 52.5 & 26.8   & 39.8    & 91.4  & 79.9    & 63.9           & 61.0          \\
% google-gecko-1.2B         & 32.6     & 82.6       & 61.3 & 71.3     & 59.2 & 25.9   & 47.1    & 87.0  & 75.4    & 63.7           & 60.3          \\
% multilingual-e5-0.5B    & 40.4     & 82.5       & 57.8 & 69.3     & 48.4 & 27.4   & 38.3    & 78.0  & 71.6    & 59.2           & 57.1          \\
% bge-m3-0.5B &   39.5    &   54.9    &   60.6    &   69.4    &   41.0    &   22.3    &   39.8    &   80.9    &   64.2    &   54.1        &   52.5    \\ \hline   
% our-7b(llama3)          & 50.3     & 83.9       & 67.4 & 74.4     & 47.4 & 35.0   & 47.2    & 86.5  & 77.2    & 64.9           & 63.3          \\
% our-7B(llama2)          & 49.9     & 85.6       & 66.8 & 72.8     & 46.6 & 36.7   & 47.3    & 84.9  & 77.4    & 64.7           & 63.1          \\
% our-7B(qwen)            & 50.2     & 86.0       & 65.8 & 72.5     & 45.4 & 37.0   & 46.3    & 86.9  & 75.8    & 64.5           & 62.9          \\
% our-3B(qwen)                  & 49.4     & 85.9       & 64.7 & 70.4     & 45.4 & 36.0   & 45.5    & 85.1  & 75.0    & 63.5           & 61.9          \\
% our-1.5B(qwen)                & 48.7     & 84.3       & 61.9 & 68.3     & 43.1 & 35.8   & 43.7    & 83.0  & 74.3    & 61.8           & 60.3          \\
% our-0.5B(qwen)                & 47.0     & 85.0       & 56.0 & 61.7     & 38.6 & 33.0   & 39.7    & 81.5  & 68.9    & 58.1           & 56.8     \\
% \hline
% \caption{Comparison for general embedding models. Note that our model only trains on MS MARCO dataset and evaluate on BEIR as zero-shot tasks but other models have been trained on these datasets. We can see that our model still performs comparably to these models. All datasets including MSMARCO are measured by nDCG@10}
% \label{tab:general}
% \end{tabular}

\begin{tabular}{c|ccccccccl|c}
\hline
                            & Trec-Covid & NQ   & HotPotQA & FiQA & Touche & DBPedia & FEVER & SciFact & CosQA & Average \\ \hline
% e5-mistral                  & 87.0       & 63.5 & 75.7     & 56.8 & 26.3   & 48.9    & 87.8  & 76.4    & 31.3  & 65.3    \\
% gritlm                      & 74.3       & 70.3 & 79.4     & 59.9 & 27.8   & 46.6    & 82.7  & 79.1    & 31.2  & 65.0    \\
% llm2vec-llama3              & 80.3       & 64.2 & 71.8     & 55.3 & 20.5   & 48.3    & 90.2  & 78.2    & 31.0  & 63.6    \\
% gte-qwen1.5-7b              & 72.7       & 63.1 & 72.3     & 55.3 & 20.3   & 48.0    & 93.4  & 75.3    & 31.3  & 62.6    \\ \hline
O1 Embedder(Llama3.1-8B)    & 83.9       & 67.4 & 74.4     & 47.4 & 35.0   & 47.2    & 86.5  & 77.2    & 35.6     & 61.6    \\
O1 Embedder(Mistralv0.3-7B) & 85.9       & 67.4 & 72.6     & 46.1 & 36.7   & 46.9    & 84.8  & 76.2    & 36.0     & 61.4    \\
O1 Embedder(Llama2-7B) & 85.6       & 66.8 & 72.8     & 46.6 & 36.7   & 47.3    & 84.9  & 77.4    & 34.1     & 61.4    \\
O1 Embedder(Qwen2.5-7B)     & 86.0       & 65.8 & 72.5     & 45.4 & 37.0   & 46.3    & 86.9  & 75.8    & 35.5     & 61.2    \\
O1 Embedder(Qwen2.5-3B)     & 85.9       & 64.7 & 70.4     & 45.4 & 36.0   & 45.5    & 85.1  & 75.0    & 35.1     & 60.3    \\
O1 Embedder(Qwen2.5-1.5B)   & 84.3       & 61.9 & 68.3     & 43.1 & 35.8   & 43.7    & 83.0  & 74.3    & 34.0     & 58.7    \\
O1 Embedder(Qwen2.5-0.5B)   & 85.0       & 56.0 & 61.7     & 38.6 & 33.0   & 39.7    & 81.5  & 68.9    & 33.1     & 55.4    \\ \hline
\end{tabular}
}

\caption{Detailed scores of different base model for zero-shot evaluation in section~\ref{subsection: Robustness}.}
\label{tab:detailed scores}
\end{table*}


\section{Algorithm}

\begin{algorithm}
\caption{Data Production Process}
\label{alg:data_production}
\begin{algorithmic}[1]
\Require  
  \Statex Query-document dataset $D = \{(q_i, d_i)\}_N$ 
  \Statex Teacher model for generating thoughts $LLM$
  \Statex Retrieval models $R = \{r_1, r_2, ..., r_{|R|}\}$ 
  \Statex Example count $m$, candidate thoughts $k$ 
  \Statex Prompt tamplate: \textsc{Prompt}, Instruction: $Ins$
  \Statex Functions: 
  \begin{itemize}
    \item \textsc{SampleExamples}: Samples $m$ examples from $D$
    \item $\sigma^{r}$: Similarity score function of Retrieval model $r$
    \item \textsc{Voting}: majority voting function
  \end{itemize}

\Ensure  
  \Statex Enhanced dataset $\hat{D} = \{(q_i, t_i, d_i)\}_N$

\State Initialize $\hat{D} \gets \emptyset$
\For{each $(q_i, d_i) \in D$}
    \For{$j=1$ to $k$}
        \State $E \gets \textsc{SampleExamples}(D)$
        \State $P \gets \textsc{Prompt}.format(Ins,E,q_j) $ 
        \State $t_j \gets LLM.generate(P)$
    \EndFor
    \For{$j=1$ to $|R|$}
        \State $t^*_r \gets \textsc{argmax}(\{\sigma^r(t_j, d)\}_{j=1...k})$
    \EndFor
    \State $t_i \gets \textsc{Voting}(t^*_r)_{r \in R}$
    \State $\hat{D} \gets \hat{D} \cup \{(q_i, t_i, d_i)\}$
\EndFor
\State \Return $\hat{D}$
\end{algorithmic}
\end{algorithm}

The Algorithm ~\ref{alg:data_production} outlined in the Data production process in section~\ref{section: data production}. For each query-document pair \((q_i, d_i)\) in the dataset, the algorithm first samples \(m\) examples from \(D\) to create a prompt, which includes specific instructions. It then generates \(k\) candidate thoughts \(t_j\) by formatting the prompt with the sampled examples. Next, for each retrieval model \(r\), it calculates the similarity scores between the candidate thoughts and the document \(d_i\), selecting the thought with the highest score as \(t^*_r\). Finally, the algorithm aggregates these top thoughts through a majority voting mechanism to determine the final thought \(t_i\). The enhanced dataset \(\hat{D}\) is then constructed, consisting of the original queries, the generated thoughts, and their corresponding positive documents.



\section{Additional Case analyses \& Attention}
\label{section:additional_case_study}

\begin{table}[ht]
\begin{tabular}{p{0.9\linewidth}}

\hline
\textbf{Query:}   Hayden is a singer-songwriter from Canada, but where does Buck-Tick hail from?      \\
\hline
\textbf{Thought:}   \textcolor{softgreen}{\textbf{Buck-Tick}} is a \textcolor{softgreen}{\textbf{Japanese}} rock band, and its members are from various parts of \textcolor{softgreen}{\textbf{Japan}} and their music is a unique blend of alternative rock, gothic rock, and visual kei styles.   \\
\hline
\textbf{Positive Doc:}   Buck-Tick \textcolor{softgreen}{\textbf{Buck-Tick}} (stylized as BUCK-TICK) is a \textcolor{softgreen}{\textbf{Japanese}} rock band, formed in Fujioka, Gunma in 1983. The group has consisted of ...   \\
\hline
\end{tabular}
\caption{Another Example from HotPotQA (zero-shot).}
\label{tab:case3}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\linewidth]{img/attention-top20.pdf}
  \caption{Top 20 Attention score from <emb> token in the thought}   
  \label{fig:top20}
\end{figure}

To better understand why thought can better help with retrieval, we further analyzed the attention scores of the model for the simple example in Table~\ref{tab:case3}. We calculate the attention weight of the <emb> token with each token $t_j$ in the thought:
$$
emb\_att(t_j)=\sum_{i} \frac{\exp{(q_{<emb>}^{i}\cdot k_{j}^{i}})}{\sum_{l=1}^{|L|} \exp{(q_{<emb>}^{i}\cdot k_{l}^{i})}}
$$
where $q_{<emb>}^{i}$ and $k_{j}^{i}$ are the "query" vector and the $j$th token's "key" vector of the $i$th attention head. $|L|$ is the total length of input sequence.


To enhance the clarity of our findings, we present the top 20 attention scores from the "<emb>" token in the generated thought, illustrated in Figure ~\ref{fig:top20}. The results reveal that the model assigns high scores to similar patterns found between the thought and the positive document, indicating its effectiveness in recognizing and focusing on relevant generated patterns. Notably, the word "Japan" emerges with the highest attention score, despite not appearing in the original query. This highlights the critical role of the generated thought in the retrieval process, as it enables the model to supplement essential information that may be absent from the query.

% analysis the fig
% 1. similar patterns have high score indicate the model successfully pay attention to these patterns. 
% 2. 这些生成的thinking content在检索过程中起了作用，并且为query补充了更多的信息例如Japan这个并没有在query中出现的token


% why our method can achieve such a huge improvement?
% 1. 思考促使模型生成更多与query相关的中间模式（patterns），这些模式有助于模型更准确地捕query的关键信息，提升检索效果。
% 1. 思考过程允许模型生成中间结果，这些结果可以作为后续检索的基础，帮助模型更好地理解问题并减少错误。（multihop retrieval）

\section{Complex Retrieval Example}
\label{app: complex}

\begin{table*}[ht]
\begin{tabular}{p{0.9\linewidth}}
\toprule 
\textbf{Query:}   python check if a directory is writable      \\
\hline
\textbf{Thought:} \newline
To check if a directory is writable in Python, you can use the os.access() function. This function takes two arguments: the path to the directory and the mode you want to check (e.g. os.W\_OK for writing). If the directory is writable, the function returns True, otherwise it returns False.\newline Here's an example:
\begin{verbatim}
import os
path = '/path/to/directory'
if os.access(path, os.W_OK):
    print('The directory is writable')
else:
    print('The directory is not writable')
\end{verbatim} 
Alternatively, you can also use the os.path.isdir() function to check if the path is a directory, and then use the os.access() function to check the write permissions:
\begin{verbatim}
import os
path = '/path/to/directory'
if os.path.isdir(path) and os.access(path, os.W_OK):
    print('The directory is writable')
else:
    print('The directory is not writable')
\end{verbatim} 
\\ \hline
\textbf{Positive Doc:}
\begin{verbatim}
def _writable_dir(path):
    """
    Whether `path` is a directory, to which the user has write access.
    """
    return os.path.isdir(path) and os.access(path, os.W_OK)
\end{verbatim} 
\\ \hline
\end{tabular}
\caption{An example of O1 embedder solving a complex code retrieval problem.}
\label{tab:cosqa}
\end{table*}

We present another case from CosQA dataset demonstrating the capabilities of O1 embedder in Table~\ref{tab:cosqa}, to effectively generate and retrieve relevant code snippet. The query posed, "python check if a directory is writable" illustrates a common programming challenge. The O1 embedder responds by generating a comprehensive thought that not only provides a direct solution using the os.access() function but also includes an example code snippet. This case highlights the model's ability to synthesize information and present it in an accessible format, thereby aiding more accurate query representation. Additionally, the thought generated by the model incorporates alternative methods, such as verifying if the path is indeed a directory before checking write permissions. This demonstrates the O1 embedder's depth of understanding by providing multiple approaches to the problem. 
Through this case, we illustrate that the O1 embedder is capable of producing coherent, contextually relevant outputs that significantly improve the utility of code retrieval systems.

% \clearpage




% \begin{table*}[ht]
% \begin{tabular}{p{0.2\linewidth} | p{0.35\linewidth} | p{0.35\linewidth}}
% \hline
% Query   &   Thought &   Positive Doc    \\ \hline
% Which man who presented the Australia 2022 FIFA World Cup bid was born on October 22, 1930? & The man who presented the Australia 2022 FIFA World Cup bid was \textbf{Frank Lowy}, who was born on \textbf{October 22, 1930}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     & \textbf{Frank Lowy} Sir Frank P. Lowy, (born \textbf{22 October 1930}) is an Australian-Israeli businessman and Chairman of Westfield Corporation, a global shopping centre company with US\$29.3 billion of assets under management in the United States, United Kingdom and Europe. He is ... ... \\ \hline
% what a bridge for teeth                                                                        & A bridge for teeth is a dental restoration that is used to replace \textbf{missing teeth}. It is a prosthetic device that is designed to be attached to the surrounding teeth, or \textbf{abutment teeth}, to fill the \textbf{gap} left by a \textbf{missing tooth}. The bridge is made up of two or more \textbf{crowns}, which are placed on the \textbf{abutment teeth}, and a \textbf{pontic}, which is a false tooth that is attached to the \textbf{crowns} to fill the \textbf{gap}. The bridge is designed to be a permanent solution for \textbf{missing teeth}, providing a natural-looking and functional replacement that can help to restore chewing ability, speech, and aesthetics. & ... ... Are you \textbf{missing} a \textbf{tooth}? A bridge is one of the dental treatments available to replace \textbf{missing teeth}. Bridges are fixed into the mouth by attaching them to \textbf{neighboring teeth}. Therefore, unlike dentures (which are removable), these stay in your mouth. The most common type is a conventional bridge where the teeth on either-side of the \textbf{gap} are \textbf{crowned}, with a tooth stuck to those \textbf{crowns}, in order to fill the \textbf{gap}.                                                                                                                                                                                                      \\ \hline

% % who announced the european recovery program?    &   The European Recovery Program, also known as the \textbf{Marshall Plan}, was announced by U.S. \textbf{Secretary of State George C. Marshall} on \textbf{June 5, 1947}.    &   The \textbf{Marshall Plan}, also known as the European Recovery Program, channeled over \$13 billion to finance the economic recovery of Europe between 1948 and 1951. The \textbf{Marshall Plan} successfully sparked economic recovery, meeting its objective of restoring the confidence of the European people in the economic future of their own countries and of Europe as a whole. The plan is named for \textbf{Secretary of State George C. Marshall}, who announced it in a commencement speech at Harvard University on \textbf{June 5, 1947}.  \\ \hline

% what is carbon monoxide made of &   Carbon monoxide (\textbf{CO}) is a colorless, odorless, and tasteless gas that is a byproduct of the \textbf{incomplete combustion} of carbon-based fuels. When carbon-based fuels are burned, they release \textbf{carbon dioxide (CO2)} as a byproduct, but in some cases, the combustion process is not complete, resulting in the formation of carbon monoxide. & Carbon monoxide \textbf{CO}, unlike \textbf{CO2}, is a bad poison. \textbf{CO} is produced by \textbf{incomplete combustion}, i.e. when there isn't enough oxygen to make \textbf{CO2}, you get \textbf{CO}. ... ... \\ \hline

% python check if a directory is writabl & \begin{minipage}{\linewidth}
% To check if a directory is writable in Python, you can use the os.access() function. This function takes two arguments: the path to the directory and the mode you want to check (e.g. os.W\_OK for writing). If the directory is writable, the function returns True, otherwise it returns False.\newline Here's an example:
% \begin{verbatim}
% import os
% path = '/path/to/directory'
% if os.access(path, os.W_OK):
%     print('The directory is writable')
% else:
%     print('The directory is not writable')
% \end{verbatim}
% \end{minipage} 

% \begin{minipage}{\linewidth}
% Alternatively, you can also use the os.path.isdir() function to check if the path is a directory, and then use the os.access() function to check the write permissions:
% \begin{verbatim}
% import os
% path = '/path/to/directory'
% if os.path.isdir(path) and 
%     os.access(path, os.W_OK):
%     print('The directory is writable')
% else:
%     print('The directory is not writable')
% \end{verbatim}
% \end{minipage}  
% &
% \begin{verbatim}
% def _writable_dir(path):
%     """
%     Whether 'path' is a directory, 
%     to which the user has write access.
%     """
%     return os.path.isdir(path) and os.access(path, os.W_OK)
% \end{verbatim}
% \\ \hline

% % what age was martin luther king when he was admitted    &   Martin Luther King Jr. was admitted to \textbf{Morehouse College in Atlanta}, Georgia \textbf{at the age of 15}. He attended the college from \textbf{1944} to 1948, where he earned a Bachelor of Arts degree in sociology.   &   Dr. Martin L. King, Jr. and His Mentors: A Message for America Today If it was not for Benjamin Mays... Benjamin Mays was the president of \textbf{Morehouse College in Atlanta} when he met Martin Luther King, Jr. In \textbf{1944}, Martin Luther King was admitted to the college \textbf{at age 15}. ... ... \\ \hline
% \end{tabular}
% \caption{Examples of the original query, generated thought and the groundtruth. The similar patterns between generated thought and the groundtruth are \textbf{bold}}
% \label{tab:case}
% \end{table*}






\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
.