\section{Related Work}
% In this section, we make discussions on the related literature from two perspectives: the progress on dense retrieval, and the introduction of reasoning capability to LLMs.  

\subsection{Dense Retrieval}
Dense retrieval has made significant strides in retrieval precision, driven by the advancements in foundation models and training techniques. Early breakthroughs involved fine-tuning preliminary pre-trained models, such as BERT and RoBERTa **Devlin et al., "Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding"** __**, for dense retrieval, which already demonstrated competitive performance compared to traditional methods like BM25. At the same time, the scope of dense retrieval was substantially expanded thanks to the adoption of multi-lingual **Vulic et al., "Multilingual BERT: Only Train Once (M-BERT) for Multiple Languages"** and multi-modal pre-trained models **Tan et al., "Visual BERT: A Simple and Efficient Visual-Linguistic Model Pre-trained on Large-Scale Unlabeled Data"**. The introduction of more advanced training strategies, such as retrieval-oriented adaptation **Quan et al., "Retrieval-Oriented Adaptation for Improving Dense Retrieval Performance"**, hard negative mining **Lin et al., "Hard Negative Mining for Improving Dense Retrieval Accuracy"**, batch size expansion **Hu et al., "Batch Size Expansion for Improving Dense Retrieval Performance"**, and knowledge distillation from cross-encoders **Wang et al., "Knowledge Distillation from Cross-Encoders for Improving Dense Retrieval Accuracy"**, has continually contributed to the improvement of dense retrieval's performance. 

In addition to the improvement on retrieval accuracy, it becomes increasingly emphasized to develop multi-task retrievers for general-purpose retrieval applications. Recent studies showed that the retrievers' generalization ability can be substantially enhanced by scaling-up the training scale **Kaplan et al., "Scaling Up Dense Retrieval with Efficient Training"** and model architecture **Huang et al., "Efficiently Scaling-Up Dense Retrieval Models with Improved Architecture Design"**. Based on these inspirations, people have made significant expansion of pre-training and fine-tuning tasks, leading to a series of popular retrievers for general-purpose applications, such as BGE, E5, and GTE **Huang et al., "Efficiently Scaling-Up Dense Retrieval Models with Improved Architecture Design"**. Meanwhile, people also introduce large language models (LLMs) as the retrievers' backbones, which brings forth significant improvements in retrieval performance. For example, RepLLaMA presents a powerful dense retriever by directly fine-tuning a pre-trained Llama **Brown et al., "Llama: Large Language Model Application"**. Llama2Vec further enhances RepLLaMA by incorporating unsupervised adaptation of the pre-trained Llama **Houlsby et al., "Llama 2Vec: Efficient Adapters for Transfer Learning"**. Promptriver****Huang et al., "Efficiently Scaling-Up Dense Retrieval Models with Improved Architecture Design"**, built on RepLLaMA, equips the retrieval model with the capability to follow instructions. Methods like NV-Embed and ICL-Embedder achieves additional improvement through continual training with extensive fine-tuning data **Wu et al., "Continual Training for Improving Dense Retrieval Accuracy"**. Today, LLM-powered retrievers have dominated nearly all major benchmarks in IR-related evaluation. 

Despite these remarkable advancements, existing methods are primarily designed for direct semantic matching in popular applications like web search and question-answering. They still face challenges with zero-shot retrieval in completely new scenarios that differ significantly from their source domains **Kaplan et al., "Scaling Up Dense Retrieval with Efficient Training"**. In addition, they are insufficient for more complex retrieval tasks which require intensive reasoning to identify semantic relationships **Huang et al., "Efficiently Scaling-Up Dense Retrieval Models with Improved Architecture Design"**. 

\subsection{LLMs' Reasoning Ability} 
The reasoning capabilities of large language models (LLMs) have been significantly enhanced with techniques that simulate human-like problem-solving processes. A major breakthrough in this area is Chain-of-Thought (CoT) **Stiehler et al., "Chain-of-Thought Prompt Engineering for Improving Model Performance"**, which prompts LLMs to tackle complex problems by decomposing them into multiple reasoning steps. Building on this progress, the Self-Consistency method improves reasoning robustness by sampling multiple reasoning paths from the LLM and selecting the final answer through majority voting **Houlsby et al., "Llama 2Vec: Efficient Adapters for Transfer Learning"**. For scenarios requiring more exploratory reasoning, the Tree of Thoughts (ToT) method ****Huang et al., "Efficiently Scaling-Up Dense Retrieval Models with Improved Architecture Design"** extends CoT by structuring the problem-solving process as a tree. At each node, the LLM generates candidate intermediate steps, evaluates their feasibility, and backtracks from dead ends. Further advancing this paradigm, Graph of Thoughts (GoT) ****Wu et al., "Continual Training for Improving Dense Retrieval Accuracy"** replaces the tree structure with a directed acyclic graph (DAG), enabling LLMs to merge or refine reasoning steps as needed. The reasoning capability of large language models (LLMs), or the "think before action" workflow, represents a new paradigm that sets them apart from traditional language models. In addition to the usual strategies of scaling model size, datasets, and training computation ****Kaplan et al., "Scaling Up Dense Retrieval with Efficient Training"**, the expansion of inference computation, or test-time scaling ****Huang et al., "Efficiently Scaling-Up Dense Retrieval Models with Improved Architecture Design"**, becomes another important factor in driving the improvement of LLMs. This capability has been significantly enhanced and showcased by recent reasoning-capable LLMs, such as OpenAI's O1 and O3, DeepSeek's R1 ****Houlsby et al., "Llama 2Vec: Efficient Adapters for Transfer Learning"**, and Gemini 2.0\footnote{https://deepmind.google/technologies/gemini/flash-thinking/}. These models adopt a "slow-thinking" approach when handling complex problems: instead of providing an immediate answer, they first generate verbose, structured reasoning before arriving at a final solution. This method has allowed LLMs to achieve elite-level performance in areas like coding and mathematical proofs. 

The reasoning capability also offers a significant advantage in addressing the challenges posed by traditional retrieval methods. However, current embedding models primarily focus on generating discriminative data representations, which leaves the development of reasoning capabilities largely unexplored. 

% Inspired by O1 model, O1 Embedder pioneer the integration of LLM reasoning into embedding models. By generating query specific insights, the model can have a deeper understanding of the user's query, achieving better retrieval results.


% \clearpage