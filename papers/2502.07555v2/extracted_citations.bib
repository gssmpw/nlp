@inproceedings{besta2024graph,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={17682--17690},
  year={2024}
}

@misc{chen2024llmcallsneedscaling,
      title={Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems}, 
      author={Lingjiao Chen and Jared Quincy Davis and Boris Hanin and Peter Bailis and Ion Stoica and Matei Zaharia and James Zou},
      year={2024},
      eprint={2403.02419},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.02419}, 
}

@misc{chen_bge_2024,
	title = {{BGE} {M3}-{Embedding}: {Multi}-{Lingual}, {Multi}-{Functionality}, {Multi}-{Granularity} {Text} {Embeddings} {Through} {Self}-{Knowledge} {Distillation}},
	shorttitle = {{BGE} {M3}-{Embedding}},
	url = {http://arxiv.org/abs/2402.03216},
	doi = {10.48550/arXiv.2402.03216},
	abstract = {In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, enabling a large batch size and high training throughput to ensure the discriminativeness of embeddings. To the best of our knowledge, M3-Embedding is the first embedding model which realizes such a strong versatility. The model and code will be publicly available at https://github.com/FlagOpen/FlagEmbedding.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
	month = jun,
	year = {2024},
	note = {arXiv:2402.03216 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/yrr/Zotero/storage/A466N84V/Chen 等 - 2024 - BGE M3-Embedding Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self.pdf:application/pdf;Snapshot:/Users/yrr/Zotero/storage/VCAWHCDP/2402.html:text/html},
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{feng2024towards,
  title={Towards revealing the mystery behind chain of thought: a theoretical perspective},
  author={Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{gao2022precise,
  title={Precise zero-shot dense retrieval without relevance labels},
  author={Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
  journal={arXiv preprint arXiv:2212.10496},
  year={2022}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@misc{hofstatter_efficiently_2021,
	title = {Efficiently {Teaching} an {Effective} {Dense} {Retriever} with {Balanced} {Topic} {Aware} {Sampling}},
	url = {http://arxiv.org/abs/2104.06967},
	doi = {10.48550/arXiv.2104.06967}

@misc{izacard2022unsuperviseddenseinformationretrieval,
      title={Unsupervised Dense Information Retrieval with Contrastive Learning}, 
      author={Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},
      year={2022},
      eprint={2112.09118},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2112.09118}, 
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{lee2024nv,
  title={NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models},
  author={Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint arXiv:2405.17428},
  year={2024}
}

@article{li2023towards,
  title={Towards general text embeddings with multi-stage contrastive learning},
  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
  journal={arXiv preprint arXiv:2308.03281},
  year={2023}
}

@article{li2024making,
  title={Making text embedders few-shot learners},
  author={Li, Chaofan and Qin, MingHao and Xiao, Shitao and Chen, Jianlyu and Luo, Kun and Shao, Yingxia and Lian, Defu and Liu, Zheng},
  journal={arXiv preprint arXiv:2409.15700},
  year={2024}
}

@inproceedings{li_llama2vec_2024,
	address = {Bangkok, Thailand},
	title = {{Llama2Vec}: {Unsupervised} {Adaptation} of {Large} {Language} {Models} for {Dense} {Retrieval}},
	shorttitle = {{Llama2Vec}},
	url = {https://aclanthology.org/2024.acl-long.191/},
	doi = {10.18653/v1/2024.acl-long.191},
	abstract = {Dense retrieval calls for discriminative embeddings to represent the semantic relationship between query and document. It may benefit from the using of large language models (LLMs), given LLMs' strong capability on semantic understanding. However, the LLMs are learned by auto-regression, whose working mechanism is completely different from representing whole text as one discriminative embedding. Thus, it is imperative to study how to adapt LLMs properly so that they can be effectively initialized as the backbone encoder for dense retrieval. In this paper, we propose a novel approach, called Llama2Vec, which performs unsupervised adaptation of LLM for its dense retrieval application. Llama2Vec consists of two pretext tasks: EBAE (Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression), where the LLM is prompted to reconstruct the input sentence and predict the next sentence based on its text embeddings. Llama2Vec is simple, lightweight, but highly effective. It is used to adapt LLaMA-2-7B on the Wikipedia corpus. With a moderate steps of adaptation, it substantially improves the model`s fine-tuned performances on a variety of dense retrieval benchmarks. Notably, it results in the new state-of-the-art performances on popular benchmarks, such as passage and document retrieval on MSMARCO, and zero-shot retrieval on BEIR. The model and source code will be made publicly available to facilitate the future research. Our model is available at https://github.com/FlagOpen/FlagEmbedding.},
	urldate = {2025-01-17},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Li, Chaofan and Liu, Zheng and Xiao, Shitao and Shao, Yingxia and Lian, Defu},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {3490--3500},
	file = {Full Text PDF:/Users/yrr/Zotero/storage/9KAVBBSX/Li 等 - 2024 - Llama2Vec Unsupervised Adaptation of Large Language Models for Dense Retrieval.pdf:application/pdf},
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan},
  journal={arXiv preprint arXiv:1907.11692},
  volume={364},
  year={2019}
}

@inproceedings{liu_retromae-2_2023,
	address = {Toronto, Canada},
	title = {{RetroMAE}-2: {Duplex} {Masked} {Auto}-{Encoder} {For} {Pre}-{Training} {Retrieval}-{Oriented} {Language} {Models}},
	shorttitle = {{RetroMAE}-2},
	url = {https://aclanthology.org/2023.acl-long.148/},
	doi = {10.18653/v1/2023.acl-long.148},
	abstract = {To better support information retrieval tasks such as web search and open-domain question answering, growing effort is made to develop retrieval-oriented language models, e.g., RetroMAE and many others. Most of the existing works focus on improving the semantic representation capability for the contextualized embedding of the [CLS] token. However, recent study shows that the ordinary tokens besides [CLS] may provide extra information, which help to produce a better representation effect. As such, it`s necessary to extend the current methods where all contextualized embeddings can be jointly pre-trained for the retrieval tasks. In this work, we propose a novel pre-training method called Duplex Masked Auto-Encoder, a.k.a. DupMAE. It is designed to improve the quality of semantic representation where all contextualized embeddings of the pre-trained model can be leveraged. It takes advantage of two complementary auto-encoding tasks: one reconstructs the input sentence on top of the [CLS] embedding; the other one predicts the bag-of-words feature of the input sentence based on the ordinary tokens' embeddings. The two tasks are jointly conducted to train a unified encoder, where the whole contextualized embeddings are aggregated in a compact way to produce the final semantic representation. DupMAE is simple but empirically competitive: it substantially improves the pre-trained model`s representation capability and transferability, where superior retrieval performances can be achieved on popular benchmarks, like MS MARCO and BEIR. We make our code publicly available at https://github.com/staoxiao/RetroMAE.},
	urldate = {2025-01-17},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Zheng and Xiao, Shitao and Shao, Yingxia and Cao, Zhao},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {2635--2648},
	file = {Full Text PDF:/Users/yrr/Zotero/storage/QQ44MIUG/Liu 等 - 2023 - RetroMAE-2 Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models.pdf:application/pdf},
}

@inproceedings{ni_large_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Large {Dual} {Encoders} {Are} {Generalizable} {Retrievers}},
	url = {https://aclanthology.org/2022.emnlp-main.669/},
	doi = {10.18653/v1/2022.emnlp-main.669},
	abstract = {It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dot-product between a query vector and a passage vector, is too limited compared to models with fine-grained interactions between the query and the passage. In this paper, we challenge this belief by scaling up the size of the dual encoder model while keeping the bottleneck layer as a single dot-product with a fixed size. With multi-stage training, scaling up the model size brings significant improvement on a variety of retrieval tasks, especially for out-of-domain generalization. We further analyze the impact of the bottleneck layer and demonstrate diminishing improvement when scaling up the embedding size. Experimental results show that our dual encoders, Generalizable T5-based dense Retrievers (GTR), outperform previous sparse and dense retrievers on the BEIR dataset significantly. Most surprisingly, our ablation study finds that GTR is very data efficient, as it only needs 10\% of MS Marco supervised data to match the out-of-domain performance of using all supervised data.},
	urldate = {2025-01-17},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ni, Jianmo and Qu, Chen and Lu, Jing and Dai, Zhuyun and Hernandez Abrego, Gustavo and Ma, Ji and Zhao, Vincent and Luan, Yi and Hall, Keith and Chang, Ming-Wei and Yang, Yinfei},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {9844--9855},
	file = {Full Text PDF:/Users/yrr/Zotero/storage/474DVDM5/Ni 等 - 2022 - Large Dual Encoders Are Generalizable Retrievers.pdf:application/pdf},
}

@article{qu2020rocketqa,
  title={RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering},
  author={Qu, Yingqi and Ding, Yuchen and Liu, Jing and Liu, Kai and Ren, Ruiyang and Zhao, Wayne Xin and Dong, Daxiang and Wu, Hua and Wang, Haifeng},
  journal={arXiv preprint arXiv:2010.08191},
  year={2020}
}

@article{sardana2023beyond,
  title={Beyond chinchilla-optimal: Accounting for inference in language model scaling laws},
  author={Sardana, Nikhil and Portes, Jacob and Doubov, Sasha and Frankle, Jonathan},
  journal={arXiv preprint arXiv:2401.00448},
  year={2023}
}

@article{su2022one,
  title={One embedder, any task: Instruction-finetuned text embeddings},
  author={Su, Hongjin and Shi, Weijia and Kasai, Jungo and Wang, Yizhong and Hu, Yushi and Ostendorf, Mari and Yih, Wen-tau and Smith, Noah A and Zettlemoyer, Luke and Yu, Tao},
  journal={arXiv preprint arXiv:2212.09741},
  year={2022}
}

@article{su2024bright,
  title={Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},
  author={Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},
  journal={arXiv preprint arXiv:2407.12883},
  year={2024}
}

@article{wang2022text,
  title={Text embeddings by weakly-supervised contrastive pre-training},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2212.03533},
  year={2022}
}

@article{wang2023improving,
  title={Improving Text Embeddings with Large Language Models},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2401.00368},
  year={2023}
}

@misc{wang_simlm_2023,
	title = {{SimLM}: {Pre}-training with {Representation} {Bottleneck} for {Dense} {Passage} {Retrieval}},
	shorttitle = {{SimLM}},
	url = {http://arxiv.org/abs/2207.02578},
	doi = {10.48550/arXiv.2207.02578},
	abstract = {In this paper, we propose SimLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised pre-training. We use a replaced language modeling objective, which is inspired by ELECTRA, to improve the sample efficiency and reduce the mismatch of the input distribution between pre-training and fine-tuning. SimLM only requires access to unlabeled corpus, and is more broadly applicable when there are no labeled data or queries. We conduct experiments on several large-scale passage retrieval datasets, and show substantial improvements over strong baselines under various settings. Remarkably, SimLM even outperforms multi-vector approaches such as ColBERTv2 which incurs significantly more storage cost. Our code and model check points are available at https://github.com/microsoft/unilm/tree/master/simlm .},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
	month = may,
	year = {2023},
	note = {arXiv:2207.02578 [cs]},
	keywords = {Computer Science - Information Retrieval},
	annote = {Comment: Accepted to ACL 2023},
	file = {Preprint PDF:/Users/yrr/Zotero/storage/ZD4DMIAM/Wang 等 - 2023 - SimLM Pre-training with Representation Bottleneck for Dense Passage Retrieval.pdf:application/pdf;Snapshot:/Users/yrr/Zotero/storage/Q5WTGEUY/2207.html:text/html},
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{wei2024uniir,
  title={Uniir: Training and benchmarking universal multimodal information retrievers},
  author={Wei, Cong and Chen, Yang and Chen, Haonan and Hu, Hexiang and Zhang, Ge and Fu, Jie and Ritter, Alan and Chen, Wenhu},
  booktitle={European Conference on Computer Vision},
  pages={387--404},
  year={2024},
  organization={Springer}
}

@misc{weller_promptriever_2024,
	title = {Promptriever: {Instruction}-{Trained} {Retrievers} {Can} {Be} {Prompted} {Like} {Language} {Models}},
	shorttitle = {Promptriever},
	url = {http://arxiv.org/abs/2409.11136},
	doi = {10.48550/arXiv.2409.11136}

@inproceedings{wu2024inference,
  title={Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving},
  author={Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
  booktitle={The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24},
  year={2024}
}

@inproceedings{xiao_retromae_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{RetroMAE}: {Pre}-{Training} {Retrieval}-oriented {Language} {Models} {Via} {Masked} {Auto}-{Encoder}},
	shorttitle = {{RetroMAE}},
	url = {https://aclanthology.org/2022.emnlp-main.35/},
	doi = {10.18653/v1/2022.emnlp-main.35},
	abstract = {Despite pre-training`s progress in many important NLP tasks, it remains to explore effective pre-training strategies for dense retrieval. In this paper, we propose RetroMAE, a new retrieval oriented pre-training paradigm based on Masked Auto-Encoder (MAE). RetroMAE is highlighted by three critical designs. 1) A novel MAE workflow, where the input sentence is polluted for encoder and decoder with different masks. The sentence embedding is generated from the encoder`s masked input; then, the original sentence is recovered based on the sentence embedding and the decoder`s masked input via masked language modeling. 2) Asymmetric model structure, with a full-scale BERT like transformer as encoder, and a one-layer transformer as decoder. 3) Asymmetric masking ratios, with a moderate ratio for encoder: 15 30\%, and an aggressive ratio for decoder: 50 70\%. Our framework is simple to realize and empirically competitive: the pre-trained models dramatically improve the SOTA performances on a wide range of dense retrieval benchmarks, like BEIR and MS MARCO. The source code and pre-trained models are made publicly available at https://github.com/staoxiao/RetroMAE so as to inspire more interesting research.},
	urldate = {2025-01-17},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Xiao, Shitao and Liu, Zheng and Shao, Yingxia and Cao, Zhao},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {538--548},
	file = {Full Text PDF:/Users/yrr/Zotero/storage/KYPQW2NH/Xiao 等 - 2022 - RetroMAE Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder.pdf:application/pdf},
}

@misc{xiong_approximate_2020,
	title = {Approximate {Nearest} {Neighbor} {Negative} {Contrastive} {Learning} for {Dense} {Text} {Retrieval}},
	url = {http://arxiv.org/abs/2007.00808},
	doi = {10.48550/arXiv.2007.00808},
	abstract = {Conducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up.},
	urldate = {2025-01-17},
	publisher = {arXiv},
	author = {Xiong, Lee and Xiong, Chenyan and Li, Ye and Tang, Kwok-Fung and Liu, Jialin and Bennett, Paul and Ahmed, Junaid and Overwijk, Arnold},
	month = oct,
	year = {2020},
	note = {arXiv:2007.00808
        
         [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/yrr/Zotero/storage/DPYQWXNR/Xiong 等 - 2020 - Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval.pdf:application/pdf;Snapshot:/Users/yrr/Zotero/storage/PEE8LHXS/2007.html:text/html},
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhou2024vista,
  title={VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval},
  author={Zhou, Junjie and Liu, Zheng and Xiao, Shitao and Zhao, Bo and Xiong, Yongping},
  journal={arXiv preprint arXiv:2406.04292},
  year={2024}
}

@article{zhu2023large,
  title={Large language models for information retrieval: A survey},
  author={Zhu, Yutao and Yuan, Huaying and Wang, Shuting and Liu, Jiongnan and Liu, Wenhan and Deng, Chenlong and Chen, Haonan and Liu, Zheng and Dou, Zhicheng and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2308.07107},
  year={2023}
}

