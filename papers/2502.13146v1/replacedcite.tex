\section{Related Work}
\paragraph{Reinforcement Learning from Human Feedback} 
Reinforcement Learning from Human Feedback (RLHF) has emerged as a crucial technique for incorporating human preference signals into machine learning methods and models____. RLHF frameworks can be broadly categorized into deep RL-based approaches and direct preference learning approaches. In deep RL-based methods, a reward model is first constructed, after which Proximal Policy Optimization (PPO)____ is employed to optimize the reward signals with KL regularization____. While the direct preference learning approaches optimize a designed loss target on the offline preference dataset directly, eliminating the need for a separate reward model____.

\paragraph{Vision Language Models} 
Large Vision Language Models (VLMs)____ extended the understanding and reasoning capabilities of Large Language Models (LLMs)____ into the visual domain. By integrating vision encoders, such as CLIP____, image patches are first converted into embeddings and then projected to align with text embedding space, unlocking unprecedented cross-modal applications in the real world, such as biomedical imaging____, autonomous systems____, and robotics____.

\paragraph{Alignment of Vision Language Models}
Current VLMs often suffer from hallucinations, producing inaccurate or misleading information that fails to accurately represent the content of the provided image____. Such misalignments can have catastrophic consequences when these models are deployed in real-world scenarios____.
To address cross-modality hallucinations, recent research has primarily focused on applying direct preference optimization____ or contrastive learning____ on the curated datasets with preference signals, and utilizing model editing techniques____.