[
  {
    "index": 0,
    "papers": [
      {
        "key": "dong2024rlhf",
        "author": "Dong, Hanze and Xiong, Wei and Pang, Bo and Wang, Haoxiang and Zhao, Han and Zhou, Yingbo and Jiang, Nan and Sahoo, Doyen and Xiong, Caiming and Zhang, Tong",
        "title": "Rlhf workflow: From reward modeling to online rlhf, 2024"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "schulman2017proximal",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      },
      {
        "key": "christiano2017deep",
        "author": "Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario",
        "title": "Deep reinforcement learning from human preferences"
      },
      {
        "key": "ziegler2019fine",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "touvron2023llama2",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      },
      {
        "key": "ipo",
        "author": "Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      },
      {
        "key": "gpo",
        "author": "Tang, Yunhao and Guo, Zhaohan Daniel and Zheng, Zeyu and Calandriello, Daniele and Munos, R{\\'e}mi and Rowland, Mark and Richemond, Pierre Harvey and Valko, Michal and Pires, Bernardo {\\'A}vila and Piot, Bilal",
        "title": "Generalized preference optimization: A unified approach to offline alignment"
      },
      {
        "key": "ethayarajh2024kto",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2022blip",
        "author": "Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven",
        "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation"
      },
      {
        "key": "li2023blip2",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      },
      {
        "key": "liu2024llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      },
      {
        "key": "llavanext",
        "author": "Li, Feng and Zhang, Renrui and Zhang, Hao and Zhang, Yuanhan and Li, Bo and Li, Wei and Ma, Zejun and Li, Chunyuan",
        "title": "Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models"
      },
      {
        "key": "llama3.2",
        "author": "Meta",
        "title": "Llama 3.2: Revolutionizing edge AI and vision with open, customizable models"
      },
      {
        "key": "Qwen-VL",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"
      },
      {
        "key": "Qwen2VL",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang",
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"
      },
      {
        "key": "lu2024deepseek",
        "author": "Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Yang, Hao and others",
        "title": "Deepseek-vl: towards real-world vision-language understanding"
      },
      {
        "key": "wu2024deepseek",
        "author": "Wu, Zhiyu and Chen, Xiaokang and Pan, Zizheng and Liu, Xingchao and Liu, Wen and Dai, Damai and Gao, Huazuo and Ma, Yiyang and Wu, Chengyue and Wang, Bingxuan and others",
        "title": "Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "devlin2018bert",
        "author": "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina",
        "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
      },
      {
        "key": "radford2019gpt2",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      },
      {
        "key": "brown2020gpt3",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "team2023gemini",
        "author": "Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others",
        "title": "Gemini: a family of highly capable multimodal models"
      },
      {
        "key": "roziere2023codellama",
        "author": "Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\\'e}r{\\'e}my and others",
        "title": "Code llama: Open foundation models for code"
      },
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "Llama: Open and efficient foundation language models"
      },
      {
        "key": "touvron2023llama2",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      },
      {
        "key": "raffel2020t5",
        "author": "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J",
        "title": "Exploring the limits of transfer learning with a unified text-to-text transformer"
      },
      {
        "key": "qwen2",
        "author": "An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan",
        "title": "Qwen2 Technical Report"
      },
      {
        "key": "qwen2.5",
        "author": "Qwen Team",
        "title": "Qwen2.5: A Party of Foundation Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "radford2021clip",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "moor2023med",
        "author": "Moor, Michael and Huang, Qian and Wu, Shirley and Yasunaga, Michihiro and Dalmia, Yash and Leskovec, Jure and Zakka, Cyril and Reis, Eduardo Pontes and Rajpurkar, Pranav",
        "title": "Med-flamingo: a multimodal medical few-shot learner"
      },
      {
        "key": "li2024llava-med",
        "author": "Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng",
        "title": "Llava-med: Training a large language-and-vision assistant for biomedicine in one day"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "shao2024lmdrive",
        "author": "Shao, Hao and Hu, Yuxuan and Wang, Letian and Song, Guanglu and Waslander, Steven L and Liu, Yu and Li, Hongsheng",
        "title": "Lmdrive: Closed-loop end-to-end driving with large language models"
      },
      {
        "key": "tian2024drivevlm",
        "author": "Tian, Xiaoyu and Gu, Junru and Li, Bailin and Liu, Yicheng and Hu, Chenxu and Wang, Yang and Zhan, Kun and Jia, Peng and Lang, Xianpeng and Zhao, Hang",
        "title": "Drivevlm: The convergence of autonomous driving and large vision-language models"
      },
      {
        "key": "sima2023drivelm",
        "author": "Sima, Chonghao and Renz, Katrin and Chitta, Kashyap and Chen, Li and Zhang, Hanxue and Xie, Chengen and Luo, Ping and Geiger, Andreas and Li, Hongyang",
        "title": "Drivelm: Driving with graph visual question answering"
      },
      {
        "key": "openemma",
        "author": "Xing, Shuo and Qian, Chengyuan and Wang, Yuping and Hua, Hongyuan and Tian, Kexin and Zhou, Yang and Tu, Zhengzhong",
        "title": "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "rana2023sayplan",
        "author": "Rana, Krishan and Haviland, Jesse and Garg, Sourav and Abou-Chakra, Jad and Reid, Ian and Suenderhauf, Niko",
        "title": "Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning"
      },
      {
        "key": "kim2024openvla",
        "author": "Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and others",
        "title": "OpenVLA: An Open-Source Vision-Language-Action Model"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhu2024unraveling",
        "author": "Zhu, Tinghui and Liu, Qin and Wang, Fei and Tu, Zhengzhong and Chen, Muhao",
        "title": "Unraveling cross-modality knowledge conflicts in large vision-language models"
      },
      {
        "key": "bai2024hallucination",
        "author": "Bai, Zechen and Wang, Pichao and Xiao, Tianjun and He, Tong and Han, Zongbo and Zhang, Zheng and Shou, Mike Zheng",
        "title": "Hallucination of multimodal large language models: A survey"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "autotrust",
        "author": "Xing, Shuo and Hua, Hongyuan and Gao, Xiangbo and Zhu, Shenzhe and Li, Renjie and Tian, Kexin and Li, Xiaopeng and Huang, Heng and Yang, Tianbao and Wang, Zhangyang and Zhou, Yang and Yao, Huaxiu and Tu, Zhengzhong",
        "title": "{AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models for Autonomous Driving}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "deng2024stic",
        "author": "Deng, Yihe and Lu, Pan and Yin, Fan and Hu, Ziniu and Shen, Sheng and Zou, James and Chang, Kai-Wei and Wang, Wei",
        "title": "Enhancing Large Vision Language Models with Self-Training on Image Comprehension"
      },
      {
        "key": "zhou2024povid",
        "author": "Zhou, Yiyang and Cui, Chenhang and Rafailov, Rafael and Finn, Chelsea and Yao, Huaxiu",
        "title": "Aligning modalities in vision large language models via preference fine-tuning"
      },
      {
        "key": "fang2024vila",
        "author": "Fang, Yunhao and Zhu, Ligeng and Lu, Yao and Wang, Yan and Molchanov, Pavlo and Kautz, Jan and Cho, Jang Hyun and Pavone, Marco and Han, Song and Yin, Hongxu",
        "title": "VILA $^2$: VILA Augmented VILA"
      },
      {
        "key": "zhou2024calibrated",
        "author": "Zhou, Yiyang and Fan, Zhiyuan and Cheng, Dongjie and Yang, Sihan and Chen, Zhaorun and Cui, Chenhang and Wang, Xiyao and Li, Yun and Zhang, Linjun and Yao, Huaxiu",
        "title": "Calibrated self-rewarding vision language models"
      },
      {
        "key": "guo2024direct",
        "author": "Guo, Shangmin and Zhang, Biao and Liu, Tianlin and Liu, Tianqi and Khalman, Misha and Llinares, Felipe and Rame, Alexandre and Mesnard, Thomas and Zhao, Yao and Piot, Bilal and others",
        "title": "Direct language model alignment from online ai feedback"
      },
      {
        "key": "chen2024dress",
        "author": "Chen, Yangyi and Sikka, Karan and Cogswell, Michael and Ji, Heng and Divakaran, Ajay",
        "title": "Dress: Instructing large vision-language models to align and interact with humans via natural language feedback"
      },
      {
        "key": "wang2024enhancing",
        "author": "Wang, Xiyao and Chen, Jiuhai and Wang, Zhaoyang and Zhou, Yuhang and Zhou, Yiyang and Yao, Huaxiu and Zhou, Tianyi and Goldstein, Tom and Bhatia, Parminder and Huang, Furong and others",
        "title": "Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement"
      },
      {
        "key": "yu2024rlhf",
        "author": "Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others",
        "title": "Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback"
      },
      {
        "key": "li2023silkie",
        "author": "Li, Lei and Xie, Zhihui and Li, Mukai and Chen, Shunian and Wang, Peiyi and Chen, Liang and Yang, Yazheng and Wang, Benyou and Kong, Lingpeng",
        "title": "Silkie: Preference distillation for large visual language models"
      },
      {
        "key": "wang2024mdpo",
        "author": "Wang, Fei and Zhou, Wenxuan and Huang, James Y and Xu, Nan and Zhang, Sheng and Poon, Hoifung and Chen, Muhao",
        "title": "mDPO: Conditional Preference Optimization for Multimodal Large Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "sarkar2024mitigating",
        "author": "Sarkar, Pritam and Ebrahimi, Sayna and Etemad, Ali and Beirami, Ahmad and Ar{\\i}k, Sercan {\\\"O} and Pfister, Tomas",
        "title": "Mitigating Object Hallucination via Data Augmented Contrastive Tuning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "liu2024paying",
        "author": "Liu, Shi and Zheng, Kecheng and Chen, Wei",
        "title": "Paying more attention to image: A training-free method for alleviating hallucination in lvlms"
      },
      {
        "key": "yu2024attention",
        "author": "Yu, Runpeng and Yu, Weihao and Wang, Xinchao",
        "title": "Attention prompting on image for large vision-language models"
      }
    ]
  }
]