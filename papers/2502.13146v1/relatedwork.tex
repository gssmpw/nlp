\section{Related Work}
\paragraph{Reinforcement Learning from Human Feedback} 
Reinforcement Learning from Human Feedback (RLHF) has emerged as a crucial technique for incorporating human preference signals into machine learning methods and models~\citep{dong2024rlhf}. RLHF frameworks can be broadly categorized into deep RL-based approaches and direct preference learning approaches. In deep RL-based methods, a reward model is first constructed, after which Proximal Policy Optimization (PPO)~\citep{schulman2017proximal, christiano2017deep, ziegler2019fine} is employed to optimize the reward signals with KL regularization~\citep{ouyang2022training, touvron2023llama2}. While the direct preference learning approaches optimize a designed loss target on the offline preference dataset directly, eliminating the need for a separate reward model\citep{rafailov2024direct,ipo,gpo,ethayarajh2024kto}.

\paragraph{Vision Language Models} 
Large Vision Language Models (VLMs)~\citep{li2022blip, li2023blip2, liu2024llava,llavanext,llama3.2, Qwen-VL, Qwen2VL, lu2024deepseek, wu2024deepseek} extended the understanding and reasoning capabilities of Large Language Models (LLMs)~\citep{devlin2018bert, radford2019gpt2,brown2020gpt3,team2023gemini,roziere2023codellama,touvron2023llama,touvron2023llama2, raffel2020t5,qwen2,qwen2.5} into the visual domain. By integrating vision encoders, such as CLIP~\citep{radford2021clip}, image patches are first converted into embeddings and then projected to align with text embedding space, unlocking unprecedented cross-modal applications in the real world, such as biomedical imaging~\citep{moor2023med,li2024llava-med}, autonomous systems~\citep{shao2024lmdrive,tian2024drivevlm,sima2023drivelm,openemma}, and robotics~\citep{rana2023sayplan,kim2024openvla}.

\paragraph{Alignment of Vision Language Models}
Current VLMs often suffer from hallucinations, producing inaccurate or misleading information that fails to accurately represent the content of the provided image~\citep{zhu2024unraveling,bai2024hallucination}. Such misalignments can have catastrophic consequences when these models are deployed in real-world scenarios~\citep{autotrust}.
To address cross-modality hallucinations, recent research has primarily focused on applying direct preference optimization~\citep{deng2024stic,zhou2024povid,fang2024vila,zhou2024calibrated,guo2024direct,chen2024dress,wang2024enhancing,yu2024rlhf,li2023silkie,wang2024mdpo} or contrastive learning~\citep{sarkar2024mitigating} on the curated datasets with preference signals, and utilizing model editing techniques~\citep{liu2024paying,yu2024attention}.