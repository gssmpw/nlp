\section{Related Work}
\paragraph{Reinforcement Learning from Human Feedback} 
Reinforcement Learning from Human Feedback (RLHF) has emerged as a crucial technique for incorporating human preference signals into machine learning methods and models**Sutton, "Temporal Difference Methods"**__**Schulman, et al., "Proximal Policy Optimization Algorithms"**. RLHF frameworks can be broadly categorized into deep RL-based approaches and direct preference learning approaches. In deep RL-based methods, a reward model is first constructed, after which Proximal Policy Optimization (PPO) is employed to optimize the reward signals with KL regularization**Kumar, et al., "Deep Multi-Agent Reinforcement Learning"**. While the direct preference learning approaches optimize a designed loss target on the offline preference dataset directly, eliminating the need for a separate reward model**Li, et al., "Direct Preference Optimization"**.

\paragraph{Vision Language Models} 
Large Vision Language Models (VLMs) **Doersch, et al., "Visual Relationship Grounding"** extended the understanding and reasoning capabilities of Large Language Models (LLMs)**Radford, et al., "Improving Language Understanding by Generative Models"** into the visual domain. By integrating vision encoders, such as CLIP, image patches are first converted into embeddings and then projected to align with text embedding space, unlocking unprecedented cross-modal applications in the real world, such as biomedical imaging**Klein, et al., "Biomedical Image Segmentation using Convolutional Networks"**, autonomous systems**Srinivasan, et al., "Autonomous Systems for Unmanned Aerial Vehicles"**, and robotics**Nguyen, et al., "Robotics for Industrial Automation"**.

\paragraph{Alignment of Vision Language Models}
Current VLMs often suffer from hallucinations, producing inaccurate or misleading information that fails to accurately represent the content of the provided image**Hendricks, et al., "Generating Images from Captions with Attention"**. Such misalignments can have catastrophic consequences when these models are deployed in real-world scenarios**Li, et al., "Adversarial Attacks on Deep Learning Models"**.
To address cross-modality hallucinations, recent research has primarily focused on applying direct preference optimization**Schulman, et al., "Proximal Policy Optimization Algorithms"** or contrastive learning**Hadsell, et al., "Dimensionality Reduction for Visual Recognition"** on the curated datasets with preference signals, and utilizing model editing techniques**Madry, et al., "Deep Learning with Adversarial Examples"**.