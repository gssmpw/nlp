\section{Related Work}
\paragraph{Unsupervised Pre-training in RL.} 
To enhance the generalization ability of agents across various tasks, unsupervised RL focuses on pre-train agents in reward-free environments to acquire embodiment knowledge, which can later be fine-tuned to any downstream tasks. 
During the pre-training stage, existing methods mainly concentrate on designing intrinsic rewards to encourage agents to explore the environment, which can be mainly categorized into two types: exploration and skill discovery. 
Exploration methods typically train a simple Gaussian policy to explore diverse states by maximizing the intrinsic rewards designed to estimate either uncertainty~\cite{pathak2017curiosity,burda2018exploration,pathak2019self,mazzaglia2022curiosity,ying2024peac} or state entropy~\cite{lee2019efficient,liu2021behavior,liu2021aps}. 
Differently, skill-discovery methods hope to explore diverse skills for downstream tasks, by maximizing the mutual information of skills and states~\cite{eysenbach2018diversity,lee2019efficient,campos2020explore,kim2021unsupervised,park2022lipschitz,laskin2022unsupervised,zhao2022mixture,yang2023behavior,park2023metra,bai2024constrained}. 
However, existing methods primarily focus on collecting diverse states while neglecting the expression ability of the pre-trained policies. Specifically, although exploration methods can discover diverse trajectories, the pre-trained policy, which is always a simple Gaussian policy, exhibits unimodally and fails to capture the diversity present in the explored data. 
Similarly, skill-based policies typically consider discrete skill space with limited skills~\cite{eysenbach2018diversity}, constraining their expressive ability by the predefined skill count. Moreover, skill-based methods always randomly select a fixed skill for adapting the downstream tasks~\cite{yang2023behavior}, which degenerates into an unimodal Gaussian distribution, further limiting its expressive power. 
Consequently, the application of generative models with strong expressive ability for improving the diversity of pre-trained policies is still less studied.



% remark: li2024learning (Learning Multimodal Behaviors from Scratch with Diffusion Policy Gradient, NeurIPS24), online train from sketch, explore only by RND
\paragraph{RL with Diffusion Models.} 
Recent advancements have demonstrated that diffusion models, with their strong expressive capabilities, can benefit RL from different perspectives~\cite{zhu2023diffusion}.
In offline RL, diffusion policies~\cite{wang2023diffusion,chen2023offline,chi2023diffusion,hansen2023idql,kang2024efficient,chen2024aligning} have shown remarkable progress in modeling multimodal behaviors under the heterogeneous data distribution, outperforming previous policies such as Gaussians or VAEs.
Besides policies, diffusion planners~\cite{janner2022planning,ajay2023is,he2023diffusion,liang2023adaptdiffuser,chen2024diffusion} have demonstrated the potential of diffusion models in long-term sequence prediction and test-time planning. Additionally, an array of research has explored integrating energy guidance into diffusion policies for guided sampling~\cite{janner2022planning,lu2023contrastive,liu2024energy}.
In online RL, several works have investigated training diffusion policies online for improving the sample efficiency and performance ~\cite{psenka2023learning,li2024learning,ren2024diffusion,mark2024policy}. However, the computational cost of multi-step sampling still remains a limiting factor for the efficiency of diffusion policies in online settings. 
In addition to behavior modeling, diffusion models have also been employed as world models~\cite{alonso2024diffusion,ding2024diffusion} augmented replay buffer~\cite{lu2024synthetic,wang2024prioritized}, hierarchical RL~\cite{li2023hierarchical,chen2024simple}, etc., which are beneficial in increasing sample efficiency in RL.
To the best of our knowledge, this work represents the first attempt to leverage diffusion policies for unsupervised exploration, thanks to the strong multimodal expressive ability and the powerful data distribution estimation ability of diffusion policies.

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]
{figs/maze_square_c.png}
\vspace{-1.em}
\caption{
\textbf{Visualization of different unsupervised RL pre-training methods in Square-b maze.}
The above part shows the trajectories in the replay buffer sampled by four algorithms during the unsupervised exploration stage.
The below part visualizes the trajectories directly sampled from pre-trained policies of four algorithms.
}
\label{fig_exp_maze_b}
% \vspace{-1.5em}
\end{figure*}