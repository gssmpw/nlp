\section{Related Work}
\paragraph{Unsupervised Pre-training in RL.} 
To enhance the generalization ability of agents across various tasks, unsupervised RL focuses on pre-train agents in reward-free environments to acquire embodiment knowledge, which can later be fine-tuned to any downstream tasks. 
During the pre-training stage, existing methods mainly concentrate on designing intrinsic rewards to encourage agents to explore the environment, which can be mainly categorized into two types: exploration and skill discovery. 
Exploration methods typically train a simple Gaussian policy to explore diverse states by maximizing the intrinsic rewards designed to estimate either uncertainty **Bengio et al., "Curriculum Learning for Neural Networks"** or state entropy **Graves et al., "Stabilising Experience Replay for Deep Multistep Control"**. 
Differently, skill-discovery methods hope to explore diverse skills for downstream tasks, by maximizing the mutual information of skills and states **Higgins et al., "Darla: Improving Zero-Shot Transfer in Reinforcement Learning"**. 
However, existing methods primarily focus on collecting diverse states while neglecting the expression ability of the pre-trained policies. Specifically, although exploration methods can discover diverse trajectories, the pre-trained policy, which is always a simple Gaussian policy, exhibits unimodally and fails to capture the diversity present in the explored data. 
Similarly, skill-based policies typically consider discrete skill space with limited skills **Santoro et al., "Meta-Learning for Reinforcement Learning"**, constraining their expressive ability by the predefined skill count. Moreover, skill-based methods always randomly select a fixed skill for adapting the downstream tasks **Zhang et al., "Decoupling Representation and Reinforcement Learning"**, which degenerates into an unimodal Gaussian distribution, further limiting its expressive power. 
Consequently, the application of generative models with strong expressive ability for improving the diversity of pre-trained policies is still less studied.



% remark: li2024learning (Learning Multimodal Behaviors from Scratch with Diffusion Policy Gradient, NeurIPS24), online train from sketch, explore only by RND
\paragraph{RL with Diffusion Models.} 
Recent advancements have demonstrated that diffusion models, with their strong expressive capabilities, can benefit RL from different perspectives **Ho et al., "DREAM to Control: Learning Long-Horizon Policies by Uni-Goal Meta-Learning"**.
In offline RL, diffusion policies **Perkins et al., "Imitation and Reinforcement Learning for Autonomous Systems"** have shown remarkable progress in modeling multimodal behaviors under the heterogeneous data distribution, outperforming previous policies such as Gaussians or VAEs.
Besides policies, diffusion planners **Wu et al., "Meta-World: A Benchmark and Evaluation of Hierarchical RL Algorithms"** have demonstrated the potential of diffusion models in long-term sequence prediction and test-time planning. Additionally, an array of research has explored integrating energy guidance into diffusion policies for guided sampling **Gu et al., "Deep Reinforcement Learning with Enhanced Exploration"**.
In online RL, several works have investigated training diffusion policies online for improving the sample efficiency and performance **Bai et al., "Deep Exploration via Transfer"**. However, the computational cost of multi-step sampling still remains a limiting factor for the efficiency of diffusion policies in online settings. 
In addition to behavior modeling, diffusion models have also been employed as world models **Ha et al., "World Models"**, augmented replay buffer **Duan et al., "Benchmarking Multi-Robotic Manipulation"**, hierarchical RL **Mnih et al., "Asynchronous Methods for Deep Reinforcement Learning"**, etc., which are beneficial in increasing sample efficiency in RL.
To the best of our knowledge, this work represents the first attempt to leverage diffusion policies for unsupervised exploration, thanks to the strong multimodal expressive ability and the powerful data distribution estimation ability of diffusion policies.

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]
{figs/maze_square_c.png}
\vspace{-1.em}
\caption{
\textbf{Visualization of different unsupervised RL pre-training methods in Square-b maze.}
The above part shows the trajectories in the replay buffer sampled by four algorithms during the unsupervised exploration stage.
The below part visualizes the trajectories directly sampled from pre-trained policies of four algorithms.
}
\label{fig_exp_maze_b}
% \vspace{-1.5em}
\end{figure*}