%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{url}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{amsmath,graphicx,amssymb,mathtools,amsthm,subfigure,float,algorithm,algorithmic,mathtools,dsfont}

\newcommand{\hangx}[1]{{\color{red}{ [Hang: #1]}}}
\newcommand{\zhongkai}[1]{{\color{red}{[zhongkai: #1]}}}
\newcommand{\zxn}[1]{{\color{blue}{[zxn: #1]}}}
\newcommand{\ycy}[1]{{\color{blue}{[ycy: #1]}}}
\newcommand{\huayu}[1]{{\color{blue}{[chy: #1]}}}
\newcommand{\hang}[1]{{\color{blue}{[rewrite: #1]}}}
\newcommand{\junz}[1]{{\color{red}{[jz: #1]}}}
\newcommand{\hzk}[1]{{\color{red}{[\textbf{hzk: }#1]}}}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{mydarkgreen}{RGB}{0, 139, 69}
\hypersetup{
	colorlinks=true,
	urlcolor=magenta,
	citecolor=mydarkblue,
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Exploratory Diffusion Policy for Unsupervised Reinforcement Learning}

\begin{document}

\twocolumn[
% \icmltitle{Submission and Formatting Instructions for \\
%            International Conference on Machine Learning (ICML 2025)}
\icmltitle{Exploratory Diffusion Policy for Unsupervised Reinforcement Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Chengyang Ying}{yyy}
\icmlauthor{Huayu Chen}{yyy}
\icmlauthor{Xinning Zhou}{yyy}
\icmlauthor{Zhongkai Hao}{yyy}
\icmlauthor{Hang Su}{yyy}
\icmlauthor{Jun Zhu}{yyy}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Computer Science \& Technology, Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Jun Zhu}{dcszj@mail.tsinghua.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Unsupervised reinforcement learning (RL) aims to pre-train agents by exploring states or skills in reward-free environments, facilitating the adaptation to downstream tasks. 
However, existing methods often overlook the fitting ability of pre-trained policies and struggle to handle the heterogeneous pre-training data, which are crucial for achieving efficient exploration and fast fine-tuning. 
To address this gap, we propose Exploratory Diffusion Policy (EDP), which leverages the strong expressive ability of diffusion models to fit the explored data, both boosting exploration and obtaining an efficient initialization for downstream tasks. 
Specifically, we estimate the distribution of collected data in the replay buffer with the diffusion policy and propose a score intrinsic reward, encouraging the agent to explore unseen states.
For fine-tuning the pre-trained diffusion policy on downstream tasks, we provide both theoretical analyses and practical algorithms, including an alternating method of Q function optimization and diffusion policy distillation.
Extensive experiments demonstrate the effectiveness of EDP in efficient exploration during pre-training and fast adaptation during fine-tuning.

% \zxn{
% To address this gap, we propose Exploratory Diffusion Policy (EDP), which leverages the strong expressive ability of diffusion models to boost exploration and obtain better performance in downstream tasks. 
% Specifically, we estimate the distribution of collected data with a multimodal diffusion policy and propose a score-based intrinsic reward to promote exploration to unseen states. 
% To further fine-tune the diffusion policy to downstream tasks, we propose a practical algorithm that combines \zxn{consists of? } an alternating method of Q function optimization and diffusion policy distillation. 
% We also provide theoretical analyses on the optimality of the algorithm. 
% Extensive experiments demonstrate the effectiveness of EDP in efficient exploration during pre-training and fast adaptation during fine-tuning.
% }

% v1
% Unsupervised reinforcement learning (RL) aims to pre-train agents by exploring states or skills in reward-free environments, setting the stage for adaptability to downstream tasks. 
% Though previous methods have demonstrated strong exploration ability to collect diverse states during the unsupervised pre-training stage, the expressive ability of pre-trained policies is always limited so that they can not fit the multimodal distribution of the explored data and perform diversity. 
% To address this gap, we propose Exploratory Diffusion Policy (EDP), utilizing the strong expressive ability of diffusion models to fit the explored data and obtain the multimodal pre-trained diffusion policy. 
% Moreover, as the diffusion policy can accurately estimate the distribution of collected data in the replay buffer, we further utilize it to calculate our score intrinsic reward for encouraging the agent to explore unseen states.
% For fine-tuning pre-trained diffusion policies to downstream tasks, we provide a theoretical motivation with practical algorithms including an alternating method of Q function optimization and diffusion policy distillation.
% Extensive experiments have demonstrated the effectiveness of EDP in both boosting exploration of multimodel diffusion policies and fast fine-tuning to downstream tasks.
%\huayu{perform diversity?}  though data is diverse. the expressivity is limited , fail to full employ the diversified dataset?  good initialization /  better intrinstic reward/exploration
% 感觉这两个好处是并列的，但是目前文章写的好像 initialization是主motication？
\end{abstract}


\section{Introduction}
\label{sec_intro}

Developing generalizable agents capable of efficiently adapting across various tasks remains a major challenge in reinforcement learning (RL). To address the diversity of downstream tasks, unsupervised learning has recently shown transformative progress in natural language processing~\cite{brown2020language} and computer vision~\cite{he2022masked}, where pre-trained models can quickly adapt to different downstream tasks. 
Inspired by these successes, unsupervised RL~\cite{eysenbach2018diversity,laskin2021urlb} aims to pre-train agents in reward-free environments, enabling them to fully extract embodiment knowledge.
These pre-trained agents can then be fine-tuned for downstream tasks, characterized by task-specific rewards, with limited interactions.

% \zxn{
% Developing models that can be efficiently fine-tuned towards various tasks remains a major challenge in machine learning. 
% % To address the diversity of downstream tasks, unsupervised learning has recently shown transformative progress in natural language processing~\cite{brown2020language} and computer vision~\cite{he2022masked}, where pre-trained models can quickly adapt to different downstream tasks. 
% Inspired by these successes, unsupervised reinforcement learning (RL)~\cite{eysenbach2018diversity,laskin2021urlb} aims to obtain pre-trained agents that can be efficiently fine-tuned for downstream tasks with limited interactions. 
% The agents are pre-trained in reward-free environments to extract embodiment knowledge and then fine-tuned in environments with task-specific rewards.
% }

% Especially, during the unsupervised pre-training stage, the agents are required to explore diverse states~\cite{pathak2017curiosity,mazzaglia2022curiosity} or skills~\cite{eysenbach2018diversity,laskin2022unsupervised} with predefined intrinsic rewards.

% motivation 
% unsupervised RL 需要我们尽可能探索动作空间，而这其实需要模型对于已经探索的空间有很强的感知表达能力：这种能力在探索和微调两个阶段都至关重要：探索 提供intrinsic reward  微调  提供好的initialization (toy图像?)
% 然而，现在的方法都不行
% \huayu{
% unsupervised RL requires strong modeling capacity and fitting ability for both the pre-training and fine-tuning stages. 
% during the unsupervised exploration stage, the major focuses is to maximize exploration in reward-free environments. 
% This requires an accurate estimation of the collected data distribution blabla. 
% As for the fine-tuning stage, rely on a powerful xx to provide a adaptable and diversified initial policy ...   
% (The prev version discuss exploration first. I think finetuning stage does not need exploration?)
% } 

One of the major challenges in unsupervised RL is the requirement of strong modeling ability and fitting ability for both the pre-training and fine-tuning stages. 
% One of the major focuses in unsupervised RL is to maximize exploration in reward-free environments, which requires strong modeling ability and fitting ability for both the pre-training and fine-tuning stages.
% Specifically, during the pre-training stage, maximizing exploration in reward-free environments requires designing intrinsic rewards that rely on an accurate estimation of the collected data distribution.
Specifically, to maximize exploration in reward-free environments during the pre-training stage, it requires designing intrinsic rewards that rely on an accurate estimation of the collected data distribution. 
% This distribution is often heterogeneous, necessitating the use of powerful estimation models.
This distribution is often heterogeneous, highlighting the importance of the modeling ability.
% In the pre-training stage, to maximize exploration in reward-free environment, designing intrinsic rewards involves an accurate estimation of the collected data distribution, which is always heterogeneous and requires powerful estimation models.
% Specifically, during the unsupervised exploration stage, designing intrinsic rewards involves an accurate estimation of the collected data distribution, which is always heterogeneous and requires powerful estimation models.
As for the fine-tuning stage, the pre-trained policies need strong fitting ability to fully capture the diversity of the explored data, which is a critical factor for enabling rapid adaptation to downstream tasks (see Fig.~\ref{fig_motivation}).
However, while existing unsupervised RL methods can collect diverse trajectories, they typically rely on simple pre-trained policies, such as Gaussian policies~\cite{pathak2017curiosity,mazzaglia2022curiosity} or skill-based policies~\cite{eysenbach2018diversity,laskin2022unsupervised}, of which the expressive ability is always limited.
Consequently, current pre-trained policies often fail to reflect the full diversity of the explored data in the replay buffer, hindering effective adaptation to downstream tasks.

To address above issues, we propose \textbf{E}xploratory \textbf{D}iffusion \textbf{P}olicy (EDP), which leverages the strong modeling ability and the fitting ability of diffusion policies~\cite{chen2023offline,chi2023diffusion} to enhance unsupervised exploration and few-shot fine-tuning.
During the unsupervised pre-training stage, we optimize a diffusion policy to estimate the heterogeneous distribution of the explored data in the replay buffer.
As the diffusion policy can accurately estimate the data distribution, we propose a novel score intrinsic reward $\mathcal{R}_{\mathrm{score}}$ calculated by the diffusion policy to encourage the agent to explore regions with lower probabilities in the replay buffer.
To address the inefficiency of multi-step sampling in diffusion policies, we further adopt a Gaussian behavior policy for action generation during environment interaction. The behavior policy is optimized to maximize score intrinsic rewards and explore unseen regions.

Besides enabling accurate data estimation for intrinsic reward design, the pre-trained diffusion policy in EDP is also an effective initialization for downstream tasks, as it can generate diverse trajectories based on exploration.
% During the fine-tuning stage, we provide a theoretical analysis of few-shot adaptation to downstream tasks and propose an alternative optimization method for both the Q function and the diffusion policy.
During the fine-tuning stage, we provide a few-shot adaptation method with theoretical analyses. By alternately optimizing the Q function and the diffusion policy, our method foster efficient adaptation to downstream tasks.
The optimality of this approach is then proven in Theorem~\ref{thm_1}, following the analyses of soft RL~\cite{haarnoja2017reinforcement}.
In practice, to further improve the efficiency, we distill the score of the pre-trained diffusion policy along with the energy score of the guidance calculated by contrastive energy prediction~\cite{lu2023contrastive}, to derive the fine-tuned diffusion policy.
% Instead of fine-tuning the pre-trained diffusion policy by directly modifying the parameters, we distill the score of the pre-trained diffusion policy along with the energy score of the guidance calculated by contrastive energy prediction~\cite{lu2023contrastive} to achieve the fine-tuned diffusion policy.
% In practice, EDP employs implicit Q-learning~\cite{kostrikov2022offline} for updating Q functions, and distills the score of the pre-trained diffusion policy along with the energy score of the guidance calculated by contrastive energy prediction~\cite{lu2023contrastive}, to derive the fine-tuned diffusion policy.

\begin{figure}[t]
\centering
\includegraphics[width=1.\linewidth]
{figs/motivation.pdf}
\vspace{-4.0em}
\caption{
\textbf{Illustration of different policies.} 
(a) Gaussian policy struggles to fully fit the collected heterogeneous data and requires switching the probability mode to the mode of the downstream task, even if that mode has already been explored during pre-training.
(b) Diffusion policy can fit all explored modes and only requires improving the probability of the mode of the downstream task.
}
\label{fig_motivation}
\vspace{-1.em}
\end{figure}


% Although existing unsupervised RL methods can explore diverse trajectories, the pre-trained policies are always chosen as simple Gaussian policies or skill-based policies, of which the expressive ability is limited. In other words, the pre-trained policies may not behave as diverse as the collected data in the replay buffer, which are always multimodal, and will be unfavorable for the downstream task adaptation. In addition, intrinsic rewards for encouraging the agent to explore unseen states, also require an accurate estimation of the distribution of the collected in the replay buffer.

% To address these challenges, inspired by the strong expressive and fitting ability of diffusion policies~\cite{chen2023offline,chi2023diffusion}, we propose the \textbf{E}xploratory \textbf{D}iffusion \textbf{P}olicy (EDP), which pre-trains diffusion policies to fit the multimodal data distribution collected during the unsupervised pre-training stage, for both performing diversity and calculating intrinsic rewards to further encourage exploration.
% In detail, during the unsupervised pre-training stage, EDP includes a behavior Gaussian policy and a diffusion policy. At each timestep, to avoid inefficient interaction with the environment caused by the multi-step sampling of the diffusion policy, we use the behavior Gaussian policy to interact with the environment and store the trajectories to the replay buffer. Then we optimize the diffusion policy for fitting the diverse trajectories in the replay buffer. 
% Moreover, as the diffusion policy can accurately estimate the data distribution of the replay buffer, we propose score intrinsic rewards calculated by the diffusion policy to encourage the agent to explore states with lower probabilities in the replay buffer.
% Finally, the behavior policy is optimized for maximizing the score intrinsic rewards, which benefits exploring unseen trajectories.


% After obtaining the pre-trained multimodal diffusion policy, we further analyze the few-shot fine-tuning stage to adapt to downstream tasks, which can be formulated as a combination of improving the task return and keeping close to the pre-trained policy as the interaction is limited.
% Extending analyses of soft RL~\cite{haarnoja2017reinforcement}, we propose an alternative optimization method for both the Q function and the diffusion policy, of which the optimality is proven in Theorem~\ref{thm_1}.
% Our analyses suggest that the Q function optimization requires to penalize actions that are out-of-distribution from the pre-trained diffusion policies, while the diffusion policy optimization should be the combination of the pre-trained diffusion policy and Q function energy guidance.
% Practically, EDP chooses implicit Q-learning~\cite{kostrikov2022offline} for updating the Q functions at each iteration, and distills the score of the pre-trained diffusion policy as well as the energy score of the guidance calculated by contrastive energy prediction~\cite{lu2023contrastive} to obtain the fine-tuned diffusion policy.

% We evaluate the effectiveness of EDP in both unsupervised exploration and fast adaptation to downstream tasks across various settings, including Maze2d~\cite{campos2020explore} and continuous control in URLB~\cite{laskin2022unsupervised}.
We evaluate the effectiveness of EDP in terms of both exploration and adaptation with various benchmarks, including Maze2d~\cite{campos2020explore} and continuous control in URLB~\cite{laskin2022unsupervised}. Visualizations in Maze2d demonstrate that EDP achieves a significantly larger state coverage ratio during the pre-training stage compared to existing exploration and skill-based baselines, highlighting its superior exploration capability.
Additionally, the pre-trained policies in EDP exhibit the highest diversity, outperforming commonly chosen Gaussian policies and skill-based policies. 
As for the fine-tuning performance, extensive experiments in URLB show that EDP can quickly adapt to downstream tasks, outperforming existing exploration methods.

In summary, the main contributions are as follows:
\begin{itemize}
    \item We propose Exploratory Diffusion Policy (EDP) to enhance the unsupervised exploration efficiency through our score intrinsic rewards.
    \item Leveraging the strong expressive ability of diffusion policies, the pre-trained policies of EDP can accurately capture the diversity of explored data and server as an effective initialization for downstream fine-tuning.
    \item We conduct extensive evaluations of EDP across various settings, demonstrating its capability to achieve efficient exploration and fast fine-tuning..
    % \item We propose Exploratory Diffusion Policy (EDP) to enhance the exploration efficiency and the diversity of pre-trained policies.
    % \item We theoretically analyze online fine-tuning diffusion policies, with a practical algorithm for adapting downstream tasks.
    % \item We conduct extensive evaluations of EDP across various settings, demonstrating its strong exploration and fine-tuning ability.
\end{itemize}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]
{figs/overview_v1.pdf}
\vspace{-1.em}
\caption{
\textbf{An overview of Exploratory Diffusion Policy (EDP).}
During pre-training, we employ the diffusion policy to model the heterogeneous exploration data and calculate score intrinsic rewards to further encourage exploration.
Moreover, we adopt a Gaussian behavior policy to collect data that avoids the inefficiency caused by the multi-step sampling of the diffusion policy. 
In the fine-tuning stage for downstream tasks, we sample diverse behaviors from the pre-trained diffusion policy and then compute their energy guidance to distill an optimized fine-tuned diffusion policy.
}
\label{fig_overview}
\vspace{-1.em}
\end{figure*}

\section{Background}

\subsection{Unsupervised Reinforcement Learning}

Reinforcement learning (RL) always considers a Markov decision process (MDP) $\mathcal{M}$, which is represented as $\mathcal{M} = (\mathcal{S},\mathcal{A}, \mathcal{P}, \mathcal{R}, \rho_0, \gamma)$~\cite{sutton2018reinforcement}. Here $\mathcal{S}$ and $\mathcal{A}$ denote the state and action spaces, respectively. For $\forall (\vs,\va)\in\mathcal{S}\times \mathcal{A}$, $\mathcal{P}(\cdot|\vs,\va)$ is a distribution on $\mathcal{S}$, representing the dynamic of $\mathcal{M}$, and $\mathcal{R}(\vs, \va)$ is the extrinsic task reward function. $\rho_0$ is the initial state distribution and $\gamma$ is the discount factor. For a given policy $\pi: \mathcal{S}\rightarrow \Delta(\mathcal{A})$, we define the discount state distribution of $\pi$ at state $\vs$ as $d_{\pi}(\vs) = (1-\gamma)\sum_{t=0}^{\infty} \left[ \gamma^t \mathcal{P}(\vs_t=\vs)\right]$.
The objective of RL is to maximize the expected cumulative return of the policy $\pi$ over the task $\mathcal{R}$, which can be computed as:
\begin{equation}
    J(\pi) \triangleq \mathbb{E}_{\tau\sim \mathcal{M},\pi} \left[\mathcal{R}(\tau)\right] = \frac{1}{1-\gamma}\mathbb{E}_{\vs\sim d_{\pi}, \va\sim\pi} \left[\mathcal{R}(\vs, \va)\right].
\end{equation}

To boost the generalization capabilities of RL agents across various downstream tasks, unsupervised RL typically includes two stages: unsupervised pre-training and few-shot fine-tuning. 
During the first stage, the agent explores in the reward-free environment $\mathcal{M}^c$, i.e., $\mathcal{M}$ without the reward function $\mathcal{R}$. 
To guide unsupervised exploration, we will design the intrinsic rewards $\mathcal{R}_{\mathrm{int}}$ to encourage the agent to explore diverse states and pre-train the policy $\pi$ to maximize the intrinsic reward.
Then during the fine-tuning stage, agents are required to adapt the pre-trained policy to handle the downstream task represented by the extrinsic task-specific reward $\mathcal{R}$, only through limited online interactions with the environment (like one-tenth or less of the pre-training steps).



\subsection{Diffusion Policies}

Recent studies have demonstrated that diffusion models~\cite{sohl2015deep,ho2020denoising} excel at accurately representing heterogeneous behaviors in continuous control, particularly through the use of diffusion policies~\cite{chen2023offline,wang2023diffusion,chi2023diffusion}. Given state-action pairs $(\vs, \va)$ sampled from some unknown policy $\mu(\va|\vs)$, diffusion policies first consider the forward diffusion process that gradually injects Gaussian noise into actions:
\begin{equation}
    \va_t = \alpha_t \va + \sigma_t \bm{\epsilon},\quad t\in [0,1],
\end{equation}
here $\epsilon$ is the standard Gaussian distribution, and $\alpha_t, \sigma_t$ are pre-defined hyperparameters satisfying that when $t=0$, we have $\va_t = \va$, and when $t=1$, we have $\va_t \approx \bm{\epsilon}$.
For $\forall t \in [0,1]$, we can define the marginal distribution of $\va_t$ as
\begin{equation}
    p_t (\va_t | \vs, t) = \int \mathcal{N}(\va_t|\alpha_t \va, \sigma_t^2 \bm{I}) \mu(\va | \vs) d \va.
\end{equation}
Then diffusion policies will train a conditional ``noise predictor" $\bm{\epsilon}_{\theta}(\va_t | \vs, t)$ to predict the added noise of each timestep:
\begin{equation}
\label{eq_diff_policy}
    \min_{\theta} \mathbb{E}_{t, \bm{\epsilon}, \vs, \va} [\|\bm{\epsilon}_{\theta} (\va_t | \vs, t) - \bm{\epsilon}\|^2].
\end{equation}
% \huayu{We don't need the score definition?}
% Existing studies have shown that the optimal solution $\bm{\epsilon}_{\theta^*}$ of Eq.~\ref{eq_diff_policy} is actually the score function of $p_t (\va_t | \vs, t)$~\cite{song2021scorebased}:
% \begin{equation}
%     \bm{\epsilon}_{\theta^*} (\va_t | \vs, t) = \nabla_{\va_t} \log p_t (\va_t | \vs, t).
% \end{equation}
% Diffusion models~\cite{sohl2015deep,ho2020denoising} are powerful generative models designing a forward process that gradually adds noises to the data distribution $p_0(\vx_0)$ and then considers the corresponding reverse process to sample from the original data distribution. The forward process $\{x(t)\}_{t\in [0,T]}$ follows a stochastic differential equation (SDE) $d\vx_t = f(\vx_t, t) dt + g(t) d\vw_t$, here $f,g$ are pre-defined functions and $\vw_t$ is the standard Brownian motion~\cite{song2021scorebased}. Then for $\forall t\in [0,T]$, we have the transition distribution $p_{t0}(\vx_{t}|\vx_0) = \mathcal{N}(\vx_{t} | \alpha_t \vx_0, \sigma_t^2 \bm{I})$ for some $\alpha_t, \sigma_t$ and the mariginal distribution can be approximated as $p_T(\vx_T)\approx \mathcal{N}(\vx_{T} | 0, \bm{I})$. During the training stage, diffusion models will optimize the scored-based model $\bm{\epsilon}_{\theta}$ for each time $t$ by
% \begin{equation}
%     \argmin_{\theta} \mathbb{E}_{t, \vx_0, \bm{\epsilon}} [\|\sigma_t \bm{\epsilon}_{\theta} (\vx_t, t) + \bm{\epsilon}\|^2],
% \end{equation}
% here $t\sim \mathcal{U}(0, T), \vx_0\sim p_0(\vx_0), \bm{\epsilon}\sim \mathcal{N}(0, \bm{I})$, and $\vx_t = \alpha_t \vx_0 + \sigma_t \bm{\epsilon}$.

After training the conditional score estimator $\bm{\epsilon}_\theta$, we can sample actions from diffusion policies to approximate the original policy $\mu(\va|\vs)$.
In detail, we can discretize the corresponding diffusion ODEs of the reverse process~\cite{song2021scorebased} and sample with several numerical solvers~\cite{song2021denoising,lu2022dpm} in around $5\sim 15$ steps.




\section{Methodology}
\label{sec_method}
In this section, we will introduce Exploratory Diffusion Policy (EDP) during two stages: online unsupervised pre-training (Sec.~\ref{sec_pretrain}) and online few-shot fine-tuning (Sec.~\ref{sec_finetune}).



\subsection{Exploratory Diffusion Policy for Unsupervised Pre-training}
\label{sec_pretrain}

During the unsupervised pre-training stage, existing methods mainly focus on designing intrinsic rewards to encourage Gaussian or skill-based policies to explore a wide range of states and skills. 
As discussed above, the core principle behind intrinsic rewards is to encourage the agent to visit stats-action pairs that are less explored, i.e., those with low probabilities of occurrence in the replay buffer.
Consequently, accurately estimating the distribution of the collected data in the replay buffer emerges as a promising pathway for calculating intrinsic rewards.

Based on the strong fitting ability of diffusion models, EDP leverages a diffusion policy $\pi_{\mathrm{d}}$, represented by a parameterized score model $\bm{\epsilon}_{\theta}$, to accurately model the diverse and often heterogeneous state-action pairs in the replay buffer $\mathcal{D}$ collected before:
\begin{equation}
\begin{split}
\label{eq_train_score}
    \min \mathbb{E}_{\vs, \va \sim \mathcal{D}} \mathbb{E}_{t,\bm{\epsilon}} [\|\bm{\epsilon}_{\theta} (\va_t | \vs, t) - \bm{\epsilon}\|^2].
\end{split}
\end{equation}
Then we can use $\log\pi_{\mathrm{d}}(\va|\vs)$ to measure the frequency of an action being sampled in the replay buffer. Consequently, state-action pairs with low $\log\pi_{\mathrm{d}}(\va|\vs)$ indicate they have been less explored. To encourage the agent to explore these regions, we design $-\log\pi_{\mathrm{d}}(\va|\vs)$ as the intrinsic reward. 
Although estimating the log-probability of the diffusion policy is challenging, it is well known that $-\log\pi_{\mathrm{d}}(\va|\vs)$ can be bounded by the following evidence lower bound (ELBO)~\cite{ho2020denoising}:
\begin{equation}
\begin{split}
    -\log\pi_{\mathrm{d}}(\va|\vs) \leq \mathbb{E}_{\bm{\epsilon}, t} [\vw_t\|\bm{\epsilon}_{\theta}(\va_t | \vs, t) - \bm{\epsilon}\|^2] + C,
\end{split}
\end{equation}
here $C$ is a constant independent of $\theta$, and $\vw_t$ are parameters related to $\alpha_t,\sigma_t$, which are typically ignored~\cite{ho2020denoising}. Consequently, our score intrinsic rewards are defined as the fitting loss of the diffusion policy:
\begin{equation}
\begin{split}
\label{eq_score_rew}
    \mathcal{R}_{\mathrm{score}}(\vs, \va) = \mathbb{E}_{\bm{\epsilon}, t} [\| \bm{\epsilon}_{\theta}(\va_t | \vs, t) - \bm{\epsilon}\|^2].
\end{split}
\end{equation}
Intuitively, score intrinsic rewards can measure the fitting quality of diffusion policy to the state-action pairs, thereby encouraging the agent to explore regions that are poorly fitted or unexplored.
By maximizing these intrinsic rewards, EDP trains agents to discover unseen regions effectively.
However, directly using diffusion policies to interact with reward-free environments during pre-training is inefficient and unstable due to the requirement of multi-step sampling.
To address this issue, EDP incorporates a Gaussian behavior policy $\pi_{\mathrm{g}}$ to sample actions for fast and efficient interaction.
The Gaussian behavior policy  $\pi_{\mathrm{g}}$ can then be trained using any RL algorithm, guided by the score intrinsic reward $\mathcal{R}_{\mathrm{score}}(\vs, \va)$.
This encourages the exploration of regions where the diffusion policy either fits poorly or has not yet been exposed.
Additionally, EDP is flexible to combine with existing unsupervised pre-training methods like ICM~\cite{pathak2017curiosity} or RND~\cite{burda2018exploration} to further boost exploration. The pseudo code of the unsupervised exploration stage of EDP is in Algorithm~\ref{algo_edp_pretrain}.

% In detail, at each timestep, we will utilize $\pi_{\mathrm{g}}$ to interact with the reward-free environment to store trajectories in the replay buffer $\mathcal{D}$. Then we train $\pi_{\mathrm{d}}$ to fit the diverse trajectories of the replay buffer:

% Despite these methods can discover diverse states, the expressive ability of pre-trained policies are unimodel or limited by the number of skills. 

% Inspired by the strong expressive ability and multimodality of diffusion policies~\cite{chi2023diffusion}, we propose EDP to utilize a diffusion policy $\pi_{\mathrm{d}}$ represented by a parameterized score model $\bm{\epsilon}_{\theta}$ to fit the diverse states collected during the pre-training stage and obtain pre-trained policies with diversity. 


\begin{algorithm}[t]
    \caption{Pre-training of EDP} 
    \label{algo_edp_pretrain}
    \begin{algorithmic}[1] %每行显示行号
        \REQUIRE Reward-free environment $\mathcal{M}^c$, replay buffer $\mathcal{D}$, Gaussian behavior policy $\pi_{\mathrm{g}}$, diffusion policy $\pi_{\mathrm{d}}$ parameterized with the score model $\bm{\epsilon}_{\theta}$.
        \FOR{$\text{sample step} = 1,2,..., S$}
        \FOR{$\text{update step} = 1,2,..., U$}
        \STATE Sample $\vs$-$\va$ pairs $\{(\vs^m,\va^m)\}_{m=1}^{M}$ from $\mathcal{D}$.
        \STATE Update $\bm{\epsilon}_{\theta}$ via optimizing with Eq.~(\ref{eq_train_score}) with sampled data.
        \STATE Calculate the score intrinsic rewards $\vr^m$ via Eq.~(\ref{eq_score_rew}) for each sampled pair $(\vs^m, \va^m)$.
        \STATE Train $\pi_{\mathrm{g}}$ with $(\vs^m, \va^m, \vr^m)$ by any RL algorithm.
        \ENDFOR
        \STATE Utilize the Gaussian behavior policy $\pi_{\mathrm{g}}$ to interact with $\mathcal{M}^c$ and store state-action pairs into $\mathcal{D}$.
        \ENDFOR
    \end{algorithmic}  
\end{algorithm} 



\subsection{Online Fine-tuning Exploratory Diffusion Policy}
\label{sec_finetune}

After obtaining the pre-trained diffusion policy $\pi_{\mathrm{d}}$, the fine-tuning stage requires adapting $\pi_{\mathrm{d}}$ efficiently to handle the downstream task, represented by $\mathcal{R}$, by interacting with the environment within a limited number of timesteps. 
Existing unsupervised RL methods always directly apply online RL algorithms, such as DDPG~\cite{lillicrap2015continuous} or PPO~\cite{schulman2017proximal}, for fine-tuning, which may be inefficient for EDP as the diffusion policy as the log probability is difficult to estimate.
Below, we start by analyzing the objective of the fine-tuning stage in unsupervised RL and then design online fine-tuning algorithms for pre-trained diffusion policies.
Given the limited iteration timesteps in the fine-tuning stage, the objective can be formulated as the combination of maximizing the cumulative return as well as keeping close to the pre-trained policy over every state $\vs$~\cite{eysenbach2021information,ying2024peac}:
\begin{equation}
\begin{split}
\label{eq_finetune_obj}
    \max_{\pi} & J_{\mathrm{f}}(\pi) \triangleq J(\pi) - \frac{\beta}{(1-\gamma)} \mathbb{E}_{\vs\sim d_{\pi}}  \left[D_{\mathrm{KL}}(\pi(\cdot|\vs) \| \pi_{\mathrm{d}}(\cdot|\vs) ) \right] \\
    = & \frac{1}{1-\gamma} \mathbb{E}_{\vs\sim d_{\pi} , \va \sim \pi}  \left[\mathcal{R}(\vs, \va) -  \beta D_{\mathrm{KL}}(\pi(\cdot|\vs) \| \pi_{\mathrm{d}}(\cdot|\vs) ) \right] \\
    = & \frac{1}{1-\gamma} \mathbb{E}_{\vs\sim d_{\pi} , \va \sim \pi}  \left[\mathcal{R}(\vs, \va) -  \beta \log\frac{\pi(\va|\vs)}{\pi_{\mathrm{d}}(\va|\vs)} \right],
\end{split}
\end{equation}
here $\beta > 0$ is an unknown trade-off parameter that is related to the fine-tuning steps. The objective $J_{\mathrm{f}}(\pi)$ can be interpreted as penalizing the probability offset of the policy in $(\vs, \va)$ over $\pi$ and $\pi_{\mathrm{d}}$. More specifically, it aims to maximize a surrogate reward of the form $\mathcal{R}(\vs, \va) -  \beta \log\frac{\pi(\va|\vs)}{\pi_{\mathrm{d}}(\va|\vs)}$. 
Unfortunately, this surrogate reward depends on the policy $\pi$ and we cannot directly apply classical MDP analyses. 
Drawn inspiration from soft RL~\cite{haarnoja2017reinforcement,haarnoja2018soft} and offline RL~\cite{peng2019advantage}, we begin by defining the corresponding Q functions as follows:
\begin{equation}
\begin{split}
    Q_{\pi}(\vs, \va) = & \mathbb{E}\left[ \mathcal{R}(\vs, \va) \right. \\
    & \left.+ \sum_{i=1}^{\infty} \gamma^i \left(\mathcal{R}(\vs_i, \va_i)  - \beta\log\frac{\pi(\va_i|\vs_i)}{\pi_{\mathrm{d}}(\va_i|\vs_i)}\right) \right].
\end{split}
\end{equation}
Based on this Q function, we can simplify $J_{\mathbf{f}}$ as
\begin{equation}
\begin{split}
\label{eq_j_q}
    J_{\mathrm{f}}(\pi) = & \mathbb{E}_{\vs \sim \rho_0, \va\sim\pi}  \left[ Q_{\pi}(\vs, \va) - \beta D_{\mathrm{KL}}(\pi(\cdot|\vs) \| \pi_{\mathrm{d}}(\cdot|\vs) )\right].
\end{split}
\end{equation}
To maximize Eq.~\ref{eq_j_q}, EDP considers decoupling the optimization of the Q function and the diffusion policy with an alternating optimization method. In detail, we first initial $\pi_0 = \pi_{\mathrm{d}}, Q_0 = Q_{\pi_0}$, then for $n=1,2,...$, we obtain
\begin{equation}
\begin{split}
\label{eq_alter}
    \pi_{n}(\cdot|\vs) = &\argmax_{\pi} \mathbb{E}_{\va\sim\pi}  [ Q_{\pi_{n-1}}(\vs, \va)\\
    & \qquad\quad - \beta D_{\mathrm{KL}}(\pi(\cdot|\vs) \| \pi_{\mathrm{d}}(\cdot|\vs) ) ] \\
    = & \frac{1}{Z(\vs)} \pi_{\mathrm{d}} (\va|\vs) e^{ Q_{n-1}(\vs, \va) / \beta}, \\
    Q_{n} = & Q_{\pi_n},
\end{split}
\end{equation}
here $Z(\vs)=\int \pi_{\mathrm{d}} (\va|\vs) e^{\beta Q_{n}(\vs, \va)}\mathrm{d}\va$ is the partition function. 
Building on the analyses of soft RL~\cite{haarnoja2017reinforcement,haarnoja2018soft}, we can demonstrate that each iteration improves the policy's performance and the alternating optimization will finally converge to the optimal policy of $J_{\mathrm{f}}(\pi)$.

\begin{theorem}[Proof in Appendix~\ref{app_proof_thm1}]
\label{thm_1}
The alternating optimization of EDP can achieve policy improvement, i.e., $J_{\mathrm{f}}(\pi_{n}) \ge J_{\mathrm{f}}(\pi_{n-1})$ holds for every $n\ge 1$. And $\pi_n$ will converge to the optimal policy of $J_{\mathrm{f}}$.
\end{theorem}

Below we will introduce the practical fine-tuning algorithm of EDP for both updating the Q function and the diffusion policy respectively, with the pseudo-code in Algorithm~\ref{algo_edp_finetune}.

\begin{algorithm}[t]
    \caption{Fine-tuning of EDP} 
    \label{algo_edp_finetune}
    \begin{algorithmic}[1] %每行显示行号
        \REQUIRE Environment $\mathcal{M}$ with rewards $\mathcal{R}$, replay buffer $\mathcal{D}$, pre-trained diffusion policy $\pi_{\mathrm{d}}$ parameterized with the score model $\bm{\epsilon}_{\theta}$, fine-tuned diffusion policy $\bm{\epsilon}_{\psi}$.
        \FOR{$\text{update iteration } n = 1,2,..., N$}
        \STATE Sample $\vs$-$\va$-$\vr$ pairs $\{(\vs^m,\va^m, \vr^m)\}_{m=1}^{M}$ from $\mathcal{D}$.
        \STATE Update Q function with IQM.
        \STATE Update Guidance $f_{\phi_{n-1}}$ with CEP.
        \STATE Optimize $\psi$ by score distillation with Eq.~\ref{eq_score_dis}.
        \FOR{$\text{interaction step} = 1,2,..., S$}
        \STATE Utilize the fine-tuned diffusion policy to interact with $\mathcal{M}$ and store state-action-reward pairs into $\mathcal{D}$.
        \ENDFOR
        \ENDFOR
    \end{algorithmic}  
\end{algorithm} 

\paragraph{Q function optimization.} 
The core principle for updating our Q functions is to penalize actions of which the gap between the log probability of $\pi$ and $\pi_{\mathrm{d}}$ is large. Consequently, we apply implicit Q-learning (IQL)~\cite{kostrikov2022offline}, which leverages expectile regression to penalize out-of-distribution actions (details are in Appendix~\ref{app_edp_q}).

\paragraph{Diffusion policy fine-tune.} 
At each update iteration $n$, given the Q function $Q_{n-1}$, it is difficult to directly calculate $\pi_{n}$ by Eq.~\ref{eq_alter} as $Z(s)$ is a complicated integral. 
However, sampling from $\pi_{n}$ can be regarded as sampling from an original diffusion model $\pi_{\mathrm{d}}$ with the energy guidance $Q_{n-1}$, which has been widely discussed as guided sampling~\cite{chung2022diffusion,janner2022planning,lu2023contrastive}. Especially, we employ contrastive energy prediction (CEP)~\cite{lu2023contrastive} to sample from $\propto \pi_{\mathrm{d}} e^{Q_{n-1}/\beta}$, which performs well in both image generation and offline RL. In detail, CEP will parameterize $f_{\phi_{n-1}}(\vs, \va_t, t)$ to represent the energy guidance of timestep $t$ and its optimization follows:
\begin{equation}
\begin{split}
    & \min_{\phi_{n-1}} \mathbb{E}_{t,\vs} \mathbb{E}_{\va^1, ..., \va^K \sim \pi_{\mathrm{d}}(\cdot|\vs)} \Bigg[\\
    & -\sum_{i=1}^K \frac{e^{Q_{n-1}(\vs, \va^i) / \beta}}{\sum_{j=1}^K e^{Q_{n-1}(\vs, \va^j) / \beta}} \log \frac{f_{\phi_{n-1}}(\vs,\va_t^i, t)}{\sum_{j=1}^K f_{\phi_{n-1}}(\vs,\va_t^j, t)}\Bigg].
\end{split}
\end{equation}


% Moreover, given any energy function $\mathcal{E}(\vs, \va)$, we can utilize guided sampling techniques~\cite{lu2023contrastive} to sample with the probability $\propto \mu(\va|\vs) e^{\beta \mathcal{E}(\vs, \va)}$, here $\beta$ is a trade-off parameter. In detail, we define 
% \begin{equation}
% \begin{split}
%     \mathcal{E}_t(\vs, \va_t) = & \log \mathbb{E}_{p_t(\va_t|\vs, t)} \left[e^{-\beta \mathcal{E}(\vs, \va)}\right], \\
%     \mathcal{E}_0(\vs, \va_t) = &\beta \mathcal{E}(\vs, \va).
% \end{split}
% \end{equation}
Then EDP fine-tunes the diffusion policy by distilling the score of $\pi_{n+1}$, which is parameterized as $\epsilon_{\psi}(\va_t|\vs, t)$:
\begin{equation}
\begin{split}
\label{eq_score_dis}
    \min_{\psi}\mathbb{E}_{\vs, \va, t} \| \bm{\epsilon}_{\psi}(\va_t | \vs, t) - \bm{\epsilon}_{\theta}(\va_t | \vs, t) -  f_{\phi_{n-1}}(\vs, \va_t, t)\|^2.
\end{split}
\end{equation}
Finally, $\bm{\epsilon}_{\psi}$ is the estimated score function of $\pi_{n}$, and we can directly sample $\bm{\epsilon}_{\psi}$ to generate action of $\pi_{n}$ (details are in Appendix~\ref{app_edp_score}).

\section{Related Work}

\paragraph{Unsupervised Pre-training in RL.} 
To enhance the generalization ability of agents across various tasks, unsupervised RL focuses on pre-train agents in reward-free environments to acquire embodiment knowledge, which can later be fine-tuned to any downstream tasks. 
During the pre-training stage, existing methods mainly concentrate on designing intrinsic rewards to encourage agents to explore the environment, which can be mainly categorized into two types: exploration and skill discovery. 
Exploration methods typically train a simple Gaussian policy to explore diverse states by maximizing the intrinsic rewards designed to estimate either uncertainty~\cite{pathak2017curiosity,burda2018exploration,pathak2019self,mazzaglia2022curiosity,ying2024peac} or state entropy~\cite{lee2019efficient,liu2021behavior,liu2021aps}. 
Differently, skill-discovery methods hope to explore diverse skills for downstream tasks, by maximizing the mutual information of skills and states~\cite{eysenbach2018diversity,lee2019efficient,campos2020explore,kim2021unsupervised,park2022lipschitz,laskin2022unsupervised,zhao2022mixture,yang2023behavior,park2023metra,bai2024constrained}. 
However, existing methods primarily focus on collecting diverse states while neglecting the expression ability of the pre-trained policies. Specifically, although exploration methods can discover diverse trajectories, the pre-trained policy, which is always a simple Gaussian policy, exhibits unimodally and fails to capture the diversity present in the explored data. 
Similarly, skill-based policies typically consider discrete skill space with limited skills~\cite{eysenbach2018diversity}, constraining their expressive ability by the predefined skill count. Moreover, skill-based methods always randomly select a fixed skill for adapting the downstream tasks~\cite{yang2023behavior}, which degenerates into an unimodal Gaussian distribution, further limiting its expressive power. 
Consequently, the application of generative models with strong expressive ability for improving the diversity of pre-trained policies is still less studied.



% remark: li2024learning (Learning Multimodal Behaviors from Scratch with Diffusion Policy Gradient, NeurIPS24), online train from sketch, explore only by RND
\paragraph{RL with Diffusion Models.} 
Recent advancements have demonstrated that diffusion models, with their strong expressive capabilities, can benefit RL from different perspectives~\cite{zhu2023diffusion}.
In offline RL, diffusion policies~\cite{wang2023diffusion,chen2023offline,chi2023diffusion,hansen2023idql,kang2024efficient,chen2024aligning} have shown remarkable progress in modeling multimodal behaviors under the heterogeneous data distribution, outperforming previous policies such as Gaussians or VAEs.
Besides policies, diffusion planners~\cite{janner2022planning,ajay2023is,he2023diffusion,liang2023adaptdiffuser,chen2024diffusion} have demonstrated the potential of diffusion models in long-term sequence prediction and test-time planning. Additionally, an array of research has explored integrating energy guidance into diffusion policies for guided sampling~\cite{janner2022planning,lu2023contrastive,liu2024energy}.
In online RL, several works have investigated training diffusion policies online for improving the sample efficiency and performance ~\cite{psenka2023learning,li2024learning,ren2024diffusion,mark2024policy}. However, the computational cost of multi-step sampling still remains a limiting factor for the efficiency of diffusion policies in online settings. 
In addition to behavior modeling, diffusion models have also been employed as world models~\cite{alonso2024diffusion,ding2024diffusion} augmented replay buffer~\cite{lu2024synthetic,wang2024prioritized}, hierarchical RL~\cite{li2023hierarchical,chen2024simple}, etc., which are beneficial in increasing sample efficiency in RL.
To the best of our knowledge, this work represents the first attempt to leverage diffusion policies for unsupervised exploration, thanks to the strong multimodal expressive ability and the powerful data distribution estimation ability of diffusion policies.

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]
{figs/maze_square_c.png}
\vspace{-1.em}
\caption{
\textbf{Visualization of different unsupervised RL pre-training methods in Square-b maze.}
The above part shows the trajectories in the replay buffer sampled by four algorithms during the unsupervised exploration stage.
The below part visualizes the trajectories directly sampled from pre-trained policies of four algorithms.
}
\label{fig_exp_maze_b}
% \vspace{-1.5em}
\end{figure*}

\section{Experiments}
\label{sec_expe}

In this section, we present extensive empirical results to address the following questions:
\begin{itemize}
    \item During the unsupervised pre-training stage, can EDP enhance exploration efficiency and obtain policies with diverse behaviors?
    \item Can the pre-trained policies of EDP fast adapt to downstream tasks?
    \item How do different components of EDP, like the score intrinsic reward, affect the performance?
\end{itemize}



\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]
{figs/maze_state_coverage.pdf}
\vspace{-1.em}
\caption{
The state coverage ratios of different algorithms in Square-b, Square-c, and Square-tree in Maze2d during the pre-training stage.
}
\label{fig_exp_maze_state_coverage}
% \vspace{-1.5em}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]
{figs/urlb.pdf}
\vspace{-1.em}
\caption{
Aggregate metrics~\cite{agarwal2021deep} in URLB. Each statistic for every algorithm has 160 runs
(4 domains × 4 downstream tasks × 10 seeds).
}
\label{fig_exp_urlb}
\vspace{-1.em}
\end{figure*}

\subsection{Experimental Setup}

\paragraph{Maze2d.} We first conduct experiments for visualizing the diversity of collected trajectories during exploration and pre-trained policies in widely used maze2d environments~\cite{campos2020explore,yang2023behavior}. We choose 3 types of different mazes: Square-b, Square-c, and Square-tree. For all these mazes, the observations and actions both belong to $\mathbb{R}^2$. When interacting with the mazes, the agents will be blocked when they contact the walls. We compare EDP with one exploration method: RND~\cite{burda2018exploration}, as well as two skill-discovery methods: DIAYN~\cite{eysenbach2018diversity} and BeCL~\cite{yang2023behavior}, of which the skills are sampled from a 16-dimensional discrete distribution. For all methods, we pre-train agents in reward-free environments with 500k steps and store trajectories in the replay buffer. Then we visualize collected trajectories in the replay buffer as well as trajectories directly sampled by pre-trained policies. 
Moreover, to compare the exploration efficiency of different algorithms, we report their state coverage ratios in each maze during the pre-training stage, which are measured as the proportion of 0.01 $\times$ 0.01 square bins visited.

\paragraph{Continuous Control.} To evaluate the fine-tuning performance in downstream tasks of pre-trained policies, we evaluate EDP in state-based continuous control settings of URLB~\cite{laskin2021urlb}. In detail, we consider 4 different domains: Walker, Quadruped, Jaco, and Hopper. Each domain contains four downstream tasks. More details of these domains and downstream tasks are in Appendix~\ref{app_exp_domain_task}.

We compare EDP with 4 exploration baselines: ICM~\cite{pathak2017curiosity}, RND~\cite{burda2018exploration}, Disagreement~\cite{pathak2019self}, and LBS~\cite{mazzaglia2022curiosity}; as well as 5 skill discovery baselines: DIAYN~\cite{eysenbach2018diversity}, SMM~\cite{lee2019efficient}, LSD~\cite{park2022lipschitz}, CIC~\cite{laskin2022unsupervised}, and BeCL~\cite{yang2023behavior}, which are standard and SOTA in this benchmark. To enhance the exploration, we combine RDN rewards and our score intrinsic rewards in EDP. For all these baselines, we take DDPG~\cite{sohl2015deep} as the RL backbone method, which is widely used in this benchmark. Following previous settings, for each algorithm, we first pre-train agents in the reward-free environment for 2M steps, and then fine-tune the pre-trained policy to adapt each downstream task with the extrinsic reward for 100K steps. All methods are run for 10 seeds per downstream task to mitigate the effectiveness of randomness caused by environments and policies.



\subsection{Diversity of Pre-trained Policies}

In Fig.~\ref{fig_exp_maze_b}, we visualize the trajectories collected during the unsupervised pre-training stage (upper part) and those directly sampled by the pre-trained policies (lower part) for each algorithm in the Square-b maze (the visualizations of other two mazes are in Appendix.~\ref{app_exp_maze}).
To quantitatively evaluate the exploration efficiency of each algorithm, we further compare their state coverage ratios as a function of the pre-training trajectory number and plot the curves of each maze in Fig.~\ref{fig_exp_maze_state_coverage}.
On both the qualitative visualization and the quantitative metric, EDP significantly outperforms existing methods, including exploration and skill-discovery ones, by large margins.
These results demonstrate that our score intrinsic rewards, leveraging the accurate data estimation ability of diffusion models, effectively encourage agents to explore more diverse states during the unsupervised pre-training stage compared with baselines.
Notably, while exploration-based methods like RND achieve state coverage comparable to skill-based methods, their pre-trained Gaussian policies often present an unimodal distribution near a single trajectory. 
In contrast, skill-based methods generate trajectories with greater diversity, as they rely on distinct skills sampled from a discrete distribution. However, this diversity is inherently limited by the number of predefined skills.
Thanks to the strong expressive ability of diffusion models, the pre-trained diffusion policies of EDP can collect the most diverse trajectories compared with all baselines, which is significant for handling different downstream tasks.
% More results and analyses of different mazes are in Appendix~\ref{app_exp_maze}.

\subsection{Fine-tuning to Downstream Tasks}

We now verify whether the policies pre-trained by EDP can fast adapt to downstream tasks in URLB.
Following previous settings, for each downstream task, we train DDPG agents with 2M steps to get the expert return and calculate the expert normalized score for each algorithm.
Following previous work~\cite{agarwal2021deep}, in Fig.~\ref{fig_exp_urlb}, we compare all methods with four metrics: mean, median, interquartile mean (IQM), and optimality gap (OG), along with stratified bootstrap confidence intervals. 
As shown here, EDP significantly outperforms all existing exploration methods such as RND and achieves competitive performance with SOTA skill-based methods such as CIC.
Additional details and analyses are in Appendix~\ref{app_exp_urlb}.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]
{figs/ablation_study.pdf}
\vspace{-1.75em}
\caption{
Ablation studies on score intrinsic reward and Q learning choices.
EDP w/o score reward only utilizes RND reward rather than the score intrinsic reward for pre-training.
EDP w/o IQL utilizes in-sample Q learning rather than IQL during fine-tuning.
}
\label{fig_exp_ablation}
\vspace{-1.em}
\end{figure}

\subsection{Ablation Studies}
\label{sec_abla}

\paragraph{Score intrinsic rewards.} We begin by conducting the ablation study on EDP during the pre-training stage, focusing on the choice of diffusion policy and the impact of our score intrinsic rewards. 
In detail, we design a variant, EDP w/o score reward, which is the same as EDP but removes our score intrinsic reward and only uses RND reward as the intrinsic reward. As shown in Fig.~\ref{fig_exp_ablation}, for all four tasks in the quadruped domain, the performance of EDP w/o score reward is better than RND.
This indicates that incorporating diffusion policies during pre-training effectively enhances policy diversity and benefits downstream task performance.
Moreover, EDP consistently outperforms EDP w/o score reward, underscoring the effectiveness of our score intrinsic reward in improving fine-tuning outcomes. 

\paragraph{Q function optimization.} Additionally, we conduct an ablation study to evaluate the impact of different Q learning methods during the fine-tuning stage.
In detail, we introduce EDP w/o IQL, which utilizes In-support Softmax Q-Learning~\cite{lu2023contrastive} rather than IQL for the Q function optimization.
The results, presented in Fig.~\ref{fig_exp_ablation}, demonstrate that EDP consistently outperforms EDP w/o IQL, verifying the efficiency of IQL in fine-tuning the diffusion policies.



\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]
{figs/ablation_sampling_step.pdf}
\vspace{-1.25em}
\caption{
Ablation study on the diffusion steps for sampling actions from the fine-tuned policy in downstream task evaluation.
}
\label{fig_exp_ablation_sampling_step}
\vspace{-0.75em}
\end{figure}

\paragraph{Sampling steps of diffusion policies.} As mentioned above, to avoid time costs caused by the multi-step sampling of diffusion policies, our pre-training stage utilizes the behavior Gaussian policy to interact with the environment. 
During fine-tuning, EDP requires sampling actions from the diffusion policy for both trajectory generation and final evaluation. 
To accelerate this process, we adopt DPM-Solver~\cite{lu2022dpm} for faster sampling.
For trajectory collection, we set the diffusion step to 15, following established offline RL settings~\cite{lu2023contrastive}.
Additionally, we conduct an ablation study about the relationship between the performance of fine-tuned policies and the number of diffusion steps used during inference.
The results, illustrated in Fig.~\ref{fig_exp_ablation_sampling_step}, show that across all four tasks, performance improves as the number of diffusion steps increases and gradually stabilizes when the sampling step exceeds 5.

\section{Conclusion}
\label{sec_con}

Unsupervised exploration is one of the major problems in RL for boosting agent generalization across tasks, as it relies on accurate intrinsic rewards to guide the exploration of unseen regions.
In this work, we address the challenge of limited policy expressivity in previous exploration methods by leveraging the powerful expressive ability of diffusion policies.
In detail, our Exploratory Diffusion Policy (EDP) not only enhances exploration efficiency during pre-training but also yields pre-trained policies with significant behavioral diversity.
Furthermore, we provide a theoretical analysis of the fine-tuning stage for diffusion policies with practical alternating optimization methods.
Experimental results in various settings demonstrate that EDP can effectively benefit both pre-training exploration and fine-tuning performance.
We hope this work can inspire further research in developing high-fidelity generative models for improving unsupervised exploration, particularly in large-scale cross-embodiment pre-trained agents or real-world control applications.

\newpage
\section*{Broader Impact}

Designing generalizable agents for varying tasks is one of the major concerns in reinforcement learning. This work focuses on utilizing diffusion policies for exploration and proposes a novel algorithm EDP. One of the potential negative impacts is that algorithms mainly use deep neural networks, which lack interoperability and may face robustness issues. There are no serious ethical issues as this is basic research.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Theoretical analysis}
\label{app_proof_thm1}

Below we first analyze our fine-tuning objective Eq.~\ref{eq_finetune_obj} and then prove Theorem~\ref{thm_1}.


Assuming $\rho_0$ is the original state distribution of the MDP $\mathcal{M}$, we have 
\begin{equation}
\begin{split}
    J_{\mathrm{f}}(\pi) \triangleq & J(\pi) - \frac{\beta}{1-\gamma} \mathbb{E}_{\vs\sim d_{\pi}}  \left[D_{\mathrm{KL}}(\pi(\cdot|\vs) \| \pi_{\mathrm{d}}(\cdot|\vs) ) \right] \\
    = & \frac{1}{1-\gamma} \mathbb{E}_{\vs\sim d_{\pi}, \va \sim \pi(\cdot|\vs)}  \left[\mathcal{R}(\vs, \va) -  \beta D_{\mathrm{KL}}(\pi(\cdot|\vs) \| \pi_{\mathrm{d}}(\cdot|\vs) ) \right] \\
    = & \mathbb{E}_{\vs \sim \rho_0, \va \sim \pi(\cdot|\vs)}  \left[ \sum_{i=0}^{\infty} \gamma^i \left(\mathcal{R}(\vs_i, \va_i) -  \beta D_{\mathrm{KL}}(\pi(\cdot|\vs_i) \| \pi_{\mathrm{d}}(\cdot|\vs_i) ) \right) \Bigg| \vs_0=\vs, \va_0=\va\right]\\
    = & \mathbb{E}_{\vs \sim \rho_0, \va \sim \pi(\cdot|\vs)}  \left[ \mathcal{R}(\vs, \va) - \beta D_{\mathrm{KL}}(\pi(\cdot|\vs) \| \pi_{\mathrm{d}}(\cdot|\vs) )  + \sum_{i=1}^{\infty} \gamma^i \left(\mathcal{R}(\vs_i, \va_i) -  \beta D_{\mathrm{KL}}(\pi(\cdot|\vs_i) \| \pi_{\mathrm{d}}(\cdot|\vs_i) ) \right) \right] \\
    = & \mathbb{E}_{\vs \sim \rho_0, \va \sim \pi(\cdot|\vs)}  \left[ Q_{\pi}(\vs, \va) - \beta D_{\mathrm{KL}}(\pi(\cdot|\vs) \| \pi_{\mathrm{d}}(\cdot|\vs) )\right],
\end{split}
\end{equation}
here we set
\begin{equation}
\begin{split}
    Q_{\pi}(\vs, \va) & = \mathbb{E}\left[ \mathcal{R}(\vs, \va) + \sum_{i=1}^{\infty} \gamma^i \left(\mathcal{R}(\vs_i, \va_i)  - \beta\log\frac{\pi(\va_i|\vs_i)}{\pi_{\mathrm{d}}(\va_i|\vs_i)}\right) \right] \\
    & = \mathbb{E}\left[ \mathcal{R}(\vs, \va) + \sum_{i=1}^{\infty} \gamma^i \left(\mathcal{R}(\vs_i, \va_i) -  \beta D_{\mathrm{KL}}(\pi(\cdot|\vs_i) \| \pi_{\mathrm{d}}(\cdot|\vs_i) ) \right)  \right].
\end{split}
\end{equation}
As discussed in Sec.~\ref{sec_finetune}, EDP applies the following alternative optimization method:
\begin{equation}
\begin{split}
    \pi_{n}(\cdot|\vs) = &\argmax_{\pi} \mathbb{E}_{\va\sim\pi(\cdot|\vs)} [ Q_{\pi_{n-1}}(\vs, \va)  - \beta D_{\mathrm{KL}}(\pi(\cdot|\vs) \| \pi_{\mathrm{d}}(\cdot|\vs) ) ], \\
    Q_{n} = & Q_{\pi_n},
\end{split}
\end{equation}
Now we prove that $\pi_n(\va|\vs) = \frac{1}{Z(\vs)} \pi_{\mathrm{d}} (\va|\vs) e^{Q_{n-1}(\vs, \va) / \beta}$. More generally, we define 
\begin{equation}
\begin{split}
    F(\pi, \pi', \vs) = \mathbb{E}_{\va\sim\pi(\cdot|\vs)}  \left[ Q_{\pi'}(\vs, \va) - \beta D_{\mathrm{KL}}(\pi(\cdot|\vs) \| \pi_{\mathrm{d}}(\cdot|\vs) )\right].
\end{split}
\end{equation}
Using the calculus of variations, we can calculate the optimal point $\pi^*$ of $F$ satisfying that 
\begin{equation}
\begin{split}
    Q_{\pi'}(\vs, \va) = \beta  \log\frac{\pi^*(\va|\vs)}{\pi_{\mathrm{d}}(\va|\vs)} + b \beta,
\end{split}
\end{equation}
here $b$ is a constant not related to $\pi^*$, and we have $\pi^*(\va|\vs) = \pi_{\mathrm{d}}(\va|\vs) e^{\frac{Q_{\pi'}(\vs, \va)}{\beta} - b}$. As $\int \pi^*(\va|\vs) d\va = 1$, we can calculate that
\begin{equation}
\begin{split}
    b =& \log\int \pi_{\mathrm{d}}(\va|\vs) e^{\frac{Q_{\pi'}(\vs, \va)}{\beta}} d\va, \quad 
    \pi^*(\va|\vs) =  \frac{\pi_{\mathrm{d}}(\va|\vs) e^{\frac{Q_{\pi'}(\vs, \va)}{\beta} - b}}{\int \pi_{\mathrm{d}}(\va|\vs) e^{\frac{Q_{\pi'}(\vs, \va)}{\beta}} d\va}.
\end{split}
\end{equation}
i.e., we have $\argmax_{\pi} F(\pi, \pi', \vs) \propto \pi_{\mathrm{d}} (\cdot|\vs) e^{Q_{\pi'}(\vs, \cdot) / \beta}$ and thus $\pi_n(\va|\vs) = \frac{1}{Z(\vs)} \pi_{\mathrm{d}} (\va|\vs) e^{Q_{n-1}(\vs, \va) / \beta}$. 

Below we will prove Theorem~\ref{thm_1}.
\begin{proof}
Based on the definition of $F$, we have $J_{\mathrm{f}}(\pi) = \mathbb{E}_{\vs\sim\rho_0}F(\pi, \pi, \vs)$. Thus we require to prove $\mathbb{E}_{\vs\sim\rho_0}F(\pi_n, \pi_n, \vs) \ge \mathbb{E}_{\vs\sim\rho_0}F(\pi_{n-1}, \pi_{n-1}, \vs)$. As we have discussed above,\begin{equation}
\begin{split}
    \pi_{n}(\cdot|\vs)
    = & \argmax_{\pi} F(\pi, \pi_{n-1}, \vs) = \frac{1}{Z(\vs)} \pi_{\mathrm{d}} (\va|\vs) e^{\beta Q_{\pi_{n-1}}(\vs, \va)}. \\
    F(\pi_{n}, \pi_{n-1}, \vs) \ge & F(\pi_{n-1}, \pi_{n-1}, \vs).
\end{split}
\end{equation}

% proof 1, directly from the paper soft Q learning
% \begin{equation}
% \begin{split}
%     Q_{\pi_n}(\vs, \va) = &\mathbb{E}\left[ \mathcal{R}(\vs, \va) + \sum_{i=1}^{\infty} \gamma^i \left(\mathcal{R}(\vs_i, \va_i)  - \beta\log\frac{\pi_n(\va_i|\vs_i)}{\pi_{\mathrm{d}}(\va_i|\vs_i)}\right) \right] \\
%     = &\mathbb{E}\left[ \mathcal{R}(\vs, \va) + \gamma\left( \mathcal{R}(\vs_1, \va_1)  - \beta\log\frac{\pi_n(\va_1|\vs_1)}{\pi_{\mathrm{d}}(\va_1|\vs_1)} + \sum_{i=2}^{\infty} \gamma^{i-1} \left(\mathcal{R}(\vs_i, \va_i)  - \beta\log\frac{\pi_n(\va_i|\vs_i)}{\pi_{\mathrm{d}}(\va_i|\vs_i)}\right) \right)\right] \\
%     = &\mathbb{E}_{\pi_n}\left[ \mathcal{R}(\vs, \va) + \gamma\left( Q_{\pi_n}(\vs_1, \va_1)  - \beta D_{\mathrm{KL}}(\pi_n(\cdot|\vs_1) \| \pi_{\mathrm{d}}(\cdot|\vs_1) ) \right)\right] \\
%     \leq &\mathbb{E}_{\pi_{n+1}}\left[ \mathcal{R}(\vs, \va) + \gamma\left( Q_{\pi_n}(\vs_1, \va_1)  - \beta D_{\mathrm{KL}}(\pi_{n+1}(\cdot|\vs_1) \| \pi_{\mathrm{d}}(\cdot|\vs_1) ) \right)\right] \\
%     = &\mathbb{E}_{\pi_{n+1}}\left[ \mathcal{R}(\vs, \va) + \gamma\left( \mathcal{R}(\vs_1, \va_1)  - \beta D_{\mathrm{KL}}(\pi_{n+1}(\cdot|\vs_1) \| \pi_{\mathrm{d}}(\cdot|\vs_1) ) \right)\right] \\
%     + & \gamma \mathbb{E}\left[ \sum_{i=2}^{\infty} \gamma^{i-1} \left(\mathcal{R}(\vs_i, \va_i)  - \beta\log\frac{\pi_n(\va_i|\vs_i)}{\pi_{\mathrm{d}}(\va_i|\vs_i)}\right) \right] \\
%     \leq & ... \\
%     = & Q_{\pi_{n+1}}(\vs, \va) .
% \end{split}
% \end{equation}

% proof 2, utilize d^pi
In other words, we have proven that $\mathbb{E}_{\vs\sim\rho_0}F(\pi_n, \pi_{n-1}, \vs) \ge \mathbb{E}_{\vs\sim\rho_0}F(\pi_{n-1}, \pi_{n-1}, \vs)$. Moreover, we have
\begin{equation}
\begin{split}
    Q_{\pi_{n-1}}(\vs, \va) 
    % = &\mathcal{R}(\vs, \va) + \mathbb{E}\left[  \sum_{i=1}^{\infty} \gamma^i \left(\mathcal{R}(\vs_i, \va_i)  - \beta\log\frac{\pi_{n-1}(\va_i|\vs_i)}{\pi_{\mathrm{d}}(\va_i|\vs_i)}\right) \Bigg{|} \vs_0=\vs, \va_0=\va \right] \\
    = &\mathcal{R}(\vs, \va) + \mathbb{E}\left[  \sum_{i=1}^{\infty} \gamma^i \left(\mathcal{R}(\vs_i, \va_i)  - \beta D_{\mathrm{KL}} (\pi_{n-1}(\cdot|\vs_i) \| \pi_{\mathrm{d}}(\cdot|\vs_i) ) \right) \Bigg{|} \vs_0=\vs, \va_0=\va \right] \\
    = &\mathcal{R}(\vs, \va) - \beta \gamma \mathbb{E}_{\vs_1} \left(D_{\mathrm{KL}} (\pi_{n-1}(\cdot|\vs_1) \| \pi_{\mathrm{d}}(\cdot|\vs_1) )\right) + \gamma \mathbb{E}_{\vs_1,\va_1} \left[ Q_{\pi_{n-1}}(\vs_1, \va_1)\right] \\
    = &\mathcal{R}(\vs, \va) + \gamma \mathbb{E}_{\vs_1} F(\pi_{n-1}, \pi_{n-1}, \vs).
\end{split}
\end{equation}
Thus
\begin{equation}
\begin{split}
    & Q_{\pi_{n}}(\vs, \va) - Q_{\pi_{n-1}}(\vs, \va)\\
    = & \gamma\mathbb{E}_{\vs_1} \left[ F(\pi_{n}, \pi_{n}, \vs_1) - F(\pi_{n-1}, \pi_{n-1}, \vs_1)\right]
    \ge \gamma\mathbb{E}_{\vs_1} \left[ F(\pi_{n}, \pi_{n}, \vs_1) - F(\pi_{n}, \pi_{n-1}, \vs_1)\right] \\
    = & \gamma\mathbb{E}_{\vs_1} \mathbb{E}_{\va_1\sim\pi_{n}}  \left[ Q_{\pi_{n}}(\vs_1, \va_1) - \beta D_{\mathrm{KL}}(\pi_{n}(\cdot|\vs_1) \| \pi_{\mathrm{d}}(\cdot|\vs_1) - Q_{\pi_{n-1}}(\vs_1, \va_1) + \beta D_{\mathrm{KL}}(\pi_{n}(\cdot|\vs_1) \| \pi_{\mathrm{d}}(\cdot|\vs_1) )\right] \\
    = & \gamma\mathbb{E}_{\vs_1}\mathbb{E}_{\va_1\sim\pi_{n}}  \left[ Q_{\pi_{n}}(\vs_1, \va_1) - Q_{\pi_{n-1}}(\vs_1, \va_1) \right].
\end{split}
\end{equation}
Given the property of $d_{\pi}$ that $d_{\pi}(\vs) - (1-\gamma)\rho_0(\vs) = \gamma\sum_{\vs'}d_{\pi}(\vs')\sum_{\va}\pi(\va|\vs')\mathcal{P}(\vs|\vs', \va)$~\cite{ying2022towards}, we have
\begin{equation}
\begin{split}
    &\mathbb{E}_{\vs\sim d_{\pi_{n}}, \va\sim\pi_{n}(\cdot|\vs)} \left[Q_{\pi_{n}}(\vs, \va) - Q_{\pi_{n-1}}(\vs, \va)\right] \\
    \ge & \gamma \mathbb{E}_{\vs\sim d_{\pi_{n}}, \va\sim\pi_{n}(\cdot|\vs)}\mathbb{E}_{\vs_1} \mathbb{E}_{\va_1\sim\pi_{n}}  \left[ Q_{\pi_{n}}(\vs_1, \va_1) - Q_{\pi_{n-1}}(\vs_1, \va_1) \right] \\
    = & \int \left(d_{\pi_n}(\vs_1) - (1-\gamma)\rho_0(\vs_1)\right) \mathbb{E}_{\va_1\sim\pi_{n}} \left[ Q_{\pi_{n}}(\vs_1, \va_1) - Q_{\pi_{n-1}}(\vs_1, \va_1) \right] d\vs_1\\
    = & \mathbb{E}_{\vs_1\sim d_{\pi_{n}}, \va_1\sim\pi_{n}(\cdot|\vs_1)}  \left[ Q_{\pi_{n}}(\vs_1, \va_1) -  Q_{\pi_{n-1}}(\vs_1, \va_1) \right]
    - (1-\gamma ) \mathbb{E}_{\vs_1\sim \rho_0, \va_1\sim\pi_{n}(\cdot|\vs_1)}  \left[ Q_{\pi_{n}}(\vs_1, \va_1) - Q_{\pi_{n-1}}(\vs_1, \va_1) \right].
\end{split}
\end{equation}
Consequently,
\begin{equation}
\begin{split}
    \mathbb{E}_{\vs\sim \rho_0} F(\pi_{n}, \pi_{n-1}, \vs) -  \mathbb{E}_{\vs\sim \rho_0} F(\pi_{n-1}, \pi_{n-1}, \vs) =  \mathbb{E}_{\vs_1\sim \rho_0,\va_1\sim\pi_{n}(\cdot|\vs_1)}  \left[ Q_{\pi_{n}}(\vs_1, \va_1) - Q_{\pi_{n-1}}(\vs_1, \va_1) \right] \ge 0
\end{split}
\end{equation}
Finally, we have 
\begin{equation}
    J_{\mathrm{f}}(\pi_{n}) = \mathbb{E}_{\vs\sim \rho_0} F(\pi_{n}, \pi_{n}, \vs) \ge \mathbb{E}_{\vs\sim \rho_0} F(\pi_{n}, \pi_{n-1}, \vs) \ge \mathbb{E}_{\vs\sim \rho_0} F(\pi_{n-1}, \pi_{n-1}, \vs) = J_{\mathrm{f}}(\pi_{n-1}).
\end{equation}
Thus our policy iteration can improve the performance. Moreover, under some regularity conditions, $\pi_n$ converges to $\pi_{\infty}$. Since non-optimal policies can be improved by our iteration, the converged policy $\pi_{\infty}$ is optimal for $J_{\mathrm{f}}$.
\end{proof}

\newpage
\section{Details of EDP}
\label{app_alog}

Below we discuss more details about the Q function optimization and diffusion policy optimization of EDP during the fine-tuning stage.

\subsection{Q function optimization}
\label{app_edp_q}

For the Q function optimization, we choose to use implicit Q-learning (IQL)~\cite{kostrikov2022offline}, which is efficient to penalize out-of-distribution actions~\cite{hansen2023idql}. The main training pipeline of IQL is expectile regression, i.e.,
\begin{equation}
\begin{split}
    &\min_{\zeta} L_{V}(\zeta) = \mathbb{E}_{\vs,\va\sim\mathcal{D}} \left[L_2^{\tau} (Q_{\phi}(\vs, \va) - V_{\psi}(\vs))\right],\\
    & \min_{\phi} L_{Q}(\phi) = \mathbb{E}_{\vs,\va,\vs'\sim\mathcal{D}} \left[\| r(\vs, \va) + \gamma V_{\zeta}(\vs') - Q_{\phi}(\vs, \va)\|^2 \right],
\end{split}
\end{equation}
here $L_2^{\tau}(\vu) = |\tau - \mathds{1}(\vu<0)|\vu^2$ and $\tau$ is a hyper-parameter. In detail, when $\tau > 0.5$, $L_2^{\tau}$ will downweight actions with low Q-values and give more weight to actions with larger Q-values.

\subsection{Diffusion Policy Fine-tuning}
\label{app_edp_score}

For sampling from $\pi_{n} =  \frac{1}{Z(s)} \pi_{\mathrm{d}} e^{Q_{n-1}/\beta}$, we choose contrastive energy prediction (CEP)~\cite{lu2023contrastive}, a powerful guided sampling method. First, we calculate the score function of $\pi_n$ as
\begin{equation}
    \nabla_{\va} \log \pi_{n}(\va|\vs) = \nabla_{\va} \log \pi_{\mathrm{d}}(\va|\vs) + \frac{1}{\beta} \nabla_{\va} Q_{n-1}(\vs, \va).
\end{equation}
Moreover, to calculate the score function of $\pi_n$ at each timestep $t$, i.e., $\nabla_{\va_t}\log \pi_t^n(\va|\vs)$, CEP further defines the following Intermediate Energy Guidance:
\begin{equation}
    \mathcal{E}_t^{n-1} (\vs, \va_t) = \left\{
    \begin{aligned}
    &\frac{1}{\beta} Q_{n-1}(\vs, \va_0), & t=0 \\
    &\log \mathbb{E}_{\mu_{0t}(\va_0|\vs, \va_t)}\left[e^{ Q_{n-1}(\vs, \va_0) / \beta}\right], & t>0 
    \end{aligned}
    \right.
\end{equation}
Then Theorem 3.1 in CEP proves that
\begin{equation}
\begin{split}
    \pi_{t}^{n} (\va_t|\vs) \propto & \pi_{\mathrm{d}}(\va_t|\vs) e^{\mathcal{E}_t^{n-1}(\vs,\va_t)}, \\
    \nabla_{\va_t} \log \pi_{t}^{n}(\va_t|\vs) = & \nabla_{\va_t} \log \pi_{\mathrm{d}}(\va_t|\vs) + \nabla_{\va} \mathcal{E}_t^{n-1}(\vs, \va_t).
\end{split}
\end{equation}
For estimating $\nabla_{\va} \mathcal{E}_t^{n-1}(\vs, \va_t)$, CEP considers a parameterized neural network $f_{\phi_{n-1}}(\vs, \va_t, t)$ with the following objective:
\begin{equation}
\begin{split}
    & \min_{\phi_{n-1}} \mathbb{E}_{t,\vs} \mathbb{E}_{\va^1, ..., \va^K \sim \pi_{\mathrm{d}}(\cdot|\vs)} \Bigg[ -\sum_{i=1}^K \frac{e^{Q_{n-1}(\vs, \va^i) / \beta}}{\sum_{j=1}^K e^{Q_{n-1}(\vs, \va^j) / \beta}} \log \frac{f_{\phi_{n-1}}(\vs,\va_t^i, t)}{\sum_{j=1}^K f_{\phi_{n-1}}(\vs,\va_t^j, t)}\Bigg].
\end{split}
\end{equation}
Then Theorem 3.2 in CEP has proven that its optimal solution $f_{\phi_{n-1}^*}$ satisfying that $\nabla_{\va_t} f_{\phi_{n-1}^*}(\vs, \va_t, t) = \nabla_{\va_t} \mathcal{E}_t^{n-1}(\vs, \va_t)$.

Consequently, we propose to fine-tune $\nabla_{\va_t} \log \pi_{t}^{n}(\va_t|\vs)$ parameterized as $\vs_{\psi}(\va_t | \vs, t)$ with the following distillation objective:
\begin{equation}
\begin{split}
    \min_{\psi}\mathbb{E}_{\vs, \va, t} \| \bm{\epsilon}_{\psi}(\va_t | \vs, t) - \bm{\epsilon}_{\theta}(\va_t | \vs, t) -  f_{\phi_{n-1}}(\vs, \va_t, t)\|^2.
\end{split}
\end{equation}
And the optimal solution $\psi^*$ satisfying that $\bm{\epsilon}_{\psi^*}(\va_t | \vs, t)$ is the score function of $\pi_n$, i.e., we can sample from $\bm{\epsilon}_{\psi^*}(\va_t | \vs, t)$ with any unconditional diffusion model sampling methods like DDIM~\cite{song2021denoising} or DPM-solver~\cite{lu2022dpm}.

\newpage
\section{Experimental Details}
\label{app_expe_detail}

In this section, we will introduce more about our experimental details. In Sec.~\ref{app_exp_domain_task}, we first introduce all the domains and tasks evaluated in our experiments. Then we briefly illustrate all the baselines compared in experiments in Sec.~\ref{app_exp_baselines}.
Moreover, we supplement more detailed experimental results about maze2d and URLB in Sec.~\ref{app_exp_maze} and Sec.~\ref{app_exp_urlb}, respectively.

\textbf{Codes of EDP are provided in the Supplementary Material.}

\subsection{Domains and Tasks}
\label{app_exp_domain_task}

\paragraph{Maze2d.} This setting includes three kinds of mazes: Square-b, Square-c, and Square-tree. The visualization results in Square-b is in Fig.~\ref{fig_exp_maze_b}, and the visualization of other two mazes is in Appendix~\ref{app_exp_maze}.

\paragraph{Continuous Control.} Our domains of continuous control follow URLB~\cite{laskin2021urlb}, including 4 domains: Walker, Quadruped, Jaco, and Hopper, each with 4 downstream tasks from Deepmind Control Suite (DMC)~\cite{tassa2018deepmind}:
\begin{itemize}
    \item \textbf{Walker} is a two-leg robot, including 4 downstream tasks: stand, walk, run, and flip.
    \item \textbf{Quadruped} is a quadruped robot within a 3D space, including 4 tasks: stand, walk, run, and jump.
    \item \textbf{Jaco} is a 6-DOF robotic arm with a 3-finger gripper, including 4 tasks: reach-top-left (tl), reach-top-right (tr), reach-bottom-left (bl), and reach-bottom-right (br).
    \item \textbf{Hopper} is a one-legged hopper robot, including 4 tasks: hop, hop-backward, flip, and flip-backward.
\end{itemize}



\subsection{Baselines and Implementations}
\label{app_exp_baselines}

\paragraph{ICM~\cite{pathak2017curiosity}.} Intrinsic Curiosity Module (ICM) trains a forward dynamics model and designs intrinsic rewards as the prediction error of the trained dynamics model.

\paragraph{RND~\cite{burda2018exploration}.} Random Network Distillation (RND) utilizes the error between the predicted features of a trained neural network and a fixed randomly initialized neural network as the intrinsic rewards.

\paragraph{Disagreement~\cite{pathak2019self}} The Disagreement algorithm proposes a self-supervised algorithm that trains an ensemble of dynamics models and leverages the prediction variance between multiple models to estimate state uncertainty. 

\paragraph{LBS~\cite{mazzaglia2022curiosity}.} Latent Bayesian Surprise (LBS) designs the intrinsic reward as the Bayesian surprise within a latent space, i.e., the difference between prior and posterior beliefs of system dynamics.

\paragraph{DIAYN~\cite{eysenbach2018diversity}.} Diversity is All You Need (DIAYN) proposes to learn a diverse set of skills during the unsupervised pre-training stage, by maximizing the mutual information between states and skills.

\paragraph{SMM~\cite{lee2019efficient}.} State Marginal Matching (SMM) aims at learning a policy, of which the state distribution matches a given target state distribution.

\paragraph{LSD~\cite{park2022lipschitz}.} Lipschitz-constrained Skill Discovery (LSD) adopts a Lipschitz-constrained state representation function for maximizing the traveled distances of states and skills.

\paragraph{CIC~\cite{laskin2022unsupervised}.} Contrastive Intrinsic Control (CIC) leverages contrastive learning between state and skill representations, which can both learn the state representation and encourage behavioral diversity.

\paragraph{BeCL~\cite{yang2023behavior}.} Behavior Contrastive Learning (BeCL) defines intrinsic rewards as the mutual information (MI) between states sampled from the same skill, utilizing contrastive learning among behaviors.

In experiments of URLB, most baselines (ICM, RND, Disagreement, DIAYN, SMM) combined with RL backbone DDPG are directly following the official implementation in urlb~(\url{https://github.com/rll-research/url_benchmark}). For LBS, we refer to the official implementation (\url{https://github.com/mazpie/mastering-urlb}) and combine it with the codebase of urlb. For CIC and BeCL, we also follow their official implementations (\url{https://github.com/rll-research/cic}, \url{https://github.com/Rooshy-yang/BeCL}).

\subsection{Additional Experiments in Maze}
\label{app_exp_maze}

Moreover, we include the visualization of four algorithms (RND, DIAYN, BeCL, and EDP) in the other two mazes (square-c, square-tree) in Eig.~\ref{fig_app_exp_maze_b} and Fig.~\ref{fig_app_exp_maze_tree}, respectively. Similarly, in the above part of these two figures, we report all the trajectories sampled during the pre-training stage of each algorithm, while in the below part, we directly collect trajectories from the pre-trained policies.

As shown in these figures, although all four algorithms can explore unseen states and try to cover as many states as they can. Due to the limitation of the expressive ability, the behaviors of baselines can not fully cover the explored replay buffer. Differently, utilizing the strong modeling ability of diffusion models, the pre-trained policies of EDP can perform diverse behaviors, setting a great initialization for handling downstream tasks.

\begin{figure*}[h]
\centering
\includegraphics[width=1\linewidth]
{figs/maze_square_b.png}
\vspace{-1.em}
\caption{
\textbf{Visualization of different unsupervised RL pre-training methods in Square-c maze.}
The above part shows the trajectories in the replay buffer sampled by four algorithms during the unsupervised pre-training stage.
The below part displays the trajectories directly sampled from the pre-trained policies of four algorithms.
}
\label{fig_app_exp_maze_b}
% \vspace{-1.5em}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=1\linewidth]
{figs/maze_square_tree.png}
\vspace{-1.em}
\caption{
\textbf{Visualization of different unsupervised RL pre-training methods in Square-tree maze.}
The above part shows the trajectories in the replay buffer sampled by four algorithms during the unsupervised pre-training stage.
The below part displays the trajectories directly sampled from the pre-trained policies of four algorithms.
}
\label{fig_app_exp_maze_tree}
% \vspace{-1.5em}
\end{figure*}



% \subsection{Hyper-parameters}
\newpage
\subsection{Additional Experiments in URLB}
\label{app_exp_urlb}

In Table.~\ref{table_dmc_state}, we report the detailed results of all methods in 4 downstream tasks of 4 domains in URLB. In both the Quadruped and Jaco domains, EDP obtains state-of-the-art performance in downstream tasks. Overall, there are the most number of downstream tasks that EDP performs the best and EDP significantly outperforms existing exploration algorithms.

\begin{table*}[h]
\centering
% \footnotesize
% \scriptsize
\tiny
\begin{tabular}{c|cccc|cccc|cccc|cccccc}
\toprule
% \diagbox{Method}{Env}&Ant-v3&HalfCheetah-v3&Walker2d-v3&Swimmer-v3&Hopper-v3\\ %添加斜线表头
Domains & \multicolumn{4}{c}{Walker} & \multicolumn{4}{c}{Quadruped} & \multicolumn{4}{c}{Jaco} & \multicolumn{4}{c}{Hopper}\\
% \multirow{2}{*}{Normilized}
Tasks & stand & walk & run & flip & stand & walk & run & jump  & tl & tr & bl & br & hop & hop-back & flip & flip-back\\  
%添加斜线表头
% Ant&Walker&Swimmer&HalfCheetah
% \multirow{4}*{Ant}
\midrule
ICM
& 828.5 & 628.8 & 223.8 & 400.3
& 298.9 & 129.9 & 92.1 & 148.8
& 96.5 & 91.7 & 84.3 & 83.4
& 82.1 & 160.5 & 106.9 & 107.6\\
RND
& 878.3 & 745.4 & 348.0 & 454.1
& 792.0 & 544.5 & 447.2 & 612.0
& 98.7 & 110.3 & 107.0 & 105.2
& 83.3 & \textbf{267.2} & \textbf{132.5} & 184.0\\
Disagreement
& 749.5 & 521.9 & 210.5 & 340.1
& 560.8 & 382.3 & 361.9 & 427.9
& 142.5 & 135.1 & 129.6 & 118.1
& 86.2 & 255.6 & 113.0 & \textbf{215.3}\\
% ProtoRL
% & 923.7 & 834.8 & 308.7 & 531.5
% & 609.6 & 491.3 & 298.5 & 595.8
% & 148.6 & 134.7 & 126.5 & 143.3
% & 168.4 & 294.2 & 196.3 & 218.9\\
% APT
% & \textbf{953.3} & 900.0 & \textbf{504.1} & 675.6
% & 841.0 & 705.9 & 453.2 & 604.0
% & 76.7 & 116.2 & 121.1 & 121.1
% & 133.3 & 260.2 & 135.7 & 202.7\\
LBS
& 594.9 & 603.2 & 138.8 & 375.3
& 413.0 & 253.2 & 203.8 & 366.6
& 166.5 & 153.8 & 129.6 & 139.6
& 24.8 & 240.2 & 88.9 & 105.6\\
\hline
DIAYN
& 721.7 & 488.3 & 186.9 & 317.0
& 640.8 & 525.1 & 275.1 & 567.8
& 29.7 & 15.6 & 30.4 & 38.6
& 1.7 & 10.8 & 0.7 & 0.5\\
SMM
& 914.3 & 709.6 & 347.4 & 442.7
& 223.9 & 93.8 & 91.6 & 96.2
& 57.8 & 30.1 & 34.8 & 45.0
& 29.3 & 61.4 & 47.0 & 29.7\\
% APS
% & 575.8 & 472.5 & 155.4 & 374.4
% & 484.8 & 335.6 & 387.8 & 351.7
% & 34.0 & 40.0 & 29.2 & 43.8\\
LSD
& 770.2 & 532.3 & 167.1 & 309.7
& 319.4 & 186.3 & 179.6 & 283.5
& 11.6 & 33.6 & 22.5 & 6.7
& 12.0 & 6.6 & 2.9 & 12.2 \\
CIC
& \textbf{941.1} & 883.1 & \textbf{399.0} & \textbf{687.2}
& 789.1 & 587.8 & 475.1 & 630.6
& 148.8 & \textbf{168.9} & 122.3 & \textbf{145.9}
& 82.7 & 191.6 & 96.2 & 161.3\\
BeCL
& \textbf{951.7} & \textbf{912.7} & \textbf{408.6} & 626.2
& 731.2 & 640.3 & 387.2 & 567.4
& 103.9 & 112.2 & 101.1 & 108.2
& 37.1 & 68.3 & 73.6 & 142.7\\
\hline
EDP (Ours)
& \textbf{942.1} & 761.5 & 272.1 & 427.0
& \textbf{920.7} & \textbf{749.6} & \textbf{464.9} & \textbf{705.9}
& \textbf{179.7} & \textbf{167.8} & \textbf{133.8} & 117.1
& \textbf{89.5} & 233.5 & 113.4 & 185.5\\
% \hline
\bottomrule
\end{tabular}
\caption{\textbf{Detailed results in URLB}. Average cumulative reward (mean of 10 seeds) of the best policy.}
\label{table_dmc_state}
\end{table*}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
