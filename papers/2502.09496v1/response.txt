\section{Related Work}
The PAC learning framework for statistical learning theory dates to the seminal work of **Vapnik, "The Nature of Statistical Learning Theory"**, with roots in prior work of Vapnik and Chervonenkis **Vapnik and Chervonenkis, "On the Uniform Convergence of Relative Frequencies of Events to Their Probabilistic Limits"**. In binary classification, finiteness of the VC dimension was first shown to characterize learnability by **Shawe-Taylor and Williamson, "A Bold Face Approach to Support Vector Machines: A Learning Theory Treatment"**. Tight lower bounds on the sample complexity of learning VC classes in the realizable case were then established by **Haussler, Littlestone, and Warmuth, "Equivalence of Definitions for Polynomial Time Bias"** and finally matched by upper bounds of **Freund and Schapire, "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting"**, building upon  work of **Pollard, "Empirical Processes: Theory and Applications"**. Subsequent works have established different optimal PAC learners for the realizable setting **Haussler and Warmuth, "Information-theoretic Thresholds for Feedback Capacity I: Binary Inputs"**.
For agnostic learning in the standard PAC framework, ERM has been known for some time to achieve sample complexity matching existing lower bounds **Vapnik, "Statistical Learning Theory"**. As previously described, we direct our attention to a more fine-grained view of agnostic learning, in which the error incurred by a learner above and beyond that of the best-in-class hypothesis $\tau = \ls_\CD(h^*_\CD)$ is itself studied as a function of $\tau$. Lower bounds employing $\tau$ in the error term are sometimes referred to as \emph{first-order bounds} and have been previously analyzed in various fields of learning such as online learning (see, e.g., **Bubeck, "X-Armed Bandits"**). **Haussler and Warmuth, "Memory Limited Inference: Circumventing the Curse of Dimensionality in Learning from Data"**, appear to be the first to consider the dependence on $\tau$ when analyzing \emph{upper bounds} in PAC learning, and we employ their perspective in this work.