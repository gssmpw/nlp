\documentclass[11pt]{article}
\setlength{\parskip}{0.35 \baselineskip}

\input{imports}
\input{macros}

\usepackage{tocloft}
\setlength\cftparskip{2.5 pt}
\setlength{\cftbeforesecskip}{2 pt}  
\setlength{\cftbeforesubsecskip}{2pt}  

\newcommand{\email}[1]{\textsf{#1}}
\newtheorem{problem}{Open Problem}

\title{On Agnostic PAC Learning in the Small Error Regime}

\author{Julian Asilis \\ USC \\ \email{asilis@usc.edu} \and 
Mikael {M\o ller H\o gsgaard} \\ Aarhus University  \\ \email{hogsgaard@cs.au.dk} \and 
Grigoris Velegkas \\ Yale University \\ \email{grigoris.velegkas@yale.edu} 
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Binary classification in the classic PAC model exhibits a curious phenomenon: Empirical Risk Minimization (ERM) learners are suboptimal in the realizable case yet optimal in the agnostic case. Roughly speaking, this owes itself to the fact that non-realizable distributions $\CD$ are simply more difficult to learn than realizable distributions --- even when one discounts a learner's error by $\mathrm{err}(h^*_\CD)$, the error of the best hypothesis in $\CH$ for $\CD$. Thus, optimal agnostic learners are permitted to incur excess error on (easier-to-learn) distributions $\CD$ for which $\tau = \mathrm{err}(h^*_\CD)$ is small.

Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS `24) addresses this shortcoming by including $\tau$ itself as a parameter in the agnostic error term. {In this more fine-grained model, they demonstrate tightness of the  error lower bound $\tau + \Omega \left(\sqrt{\frac{\tau (d + \log(1 / \delta))}{m}} + \frac{d + \log(1 / \delta)}{m} \right)$ in a regime where $\tau > d/m$, and leave open the question of whether there may be a higher lower bound when $\tau \approx d/m$, with $d$ denoting $\VC(\CH)$.}
In this work, we resolve this question by exhibiting a learner which achieves error $c \cdot \tau + O \left(\sqrt{\frac{\tau (d + \log(1 / \delta))}{m}} + \frac{d + \log(1 / \delta)}{m} \right)$ for a constant $c \leq 2.1$, thus matching the lower bound when $\tau \approx d/m$. Further, our learner is computationally efficient and is based upon careful aggregations of ERM classifiers, making progress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS `24). We leave open the interesting question of whether our approach can be refined to lower the constant from 2.1 to 1, which would completely settle the complexity of agnostic learning.
\end{abstract}

\newpage 
\begingroup
  \hypersetup{hidelinks}
  \tableofcontents
\endgroup
\newpage 

\input{introduction}

\input{preliminaries}

\subsection*{Acknowledgments}
\noindent
Julian Asilis was supported by the National Science Foundation Graduate Research Fellowship Program under Grant No.\ DGE-1842487, and completed part of this work while visiting the Simons Institute for the Theory of Computing. 
Mikael Møller Høgsgaard was supported by Independent
Research Fund Denmark (DFF) Sapere Aude Research Leader Grant No.\ 9064-00068B.
Grigoris Velegkas was
supported by the AI Institute for Learning-Enabled Optimization at Scale (TILOS).
Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any of the sponsors such as the NSF.

\newpage 
\bibliographystyle{plainnat}
\bibliography{refs.bib}
\newpage 

\input{appendix}

\end{document}
