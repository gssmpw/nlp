\section{Related Work}
The PAC learning framework for statistical learning theory dates to the seminal work of \citet{valiant1984theory}, with roots in prior work of Vapnik and Chervonenkis \citep{vapnik1964class, vapnik1974theory}. In binary classification, finiteness of the VC dimension was first shown to characterize learnability by \citet{blumer1989learnability}. Tight lower bounds on the sample complexity of learning VC classes in the realizable case were then established by \citet{ehrenfeucht1989general} and finally matched by upper bounds of \citet{hanneke2016optimal}, building upon  work of \citet{simon2015almost}. Subsequent works have established different optimal PAC learners for the realizable setting \citet{aden2023optimal, baggingoptimal,aden2024majority}.
For agnostic learning in the standard PAC framework, ERM has been known for some time to achieve sample complexity matching existing lower bounds \citep{haussler1992decision, boucheron2005theory, anthony2009neural}. As previously described, we direct our attention to a more fine-grained view of agnostic learning, in which the error incurred by a learner above and beyond that of the best-in-class hypothesis $\tau = \ls_\CD(h^*_\CD)$ is itself studied as a function of $\tau$. Lower bounds employing $\tau$ in the error term are sometimes referred to as \emph{first-order bounds} and have been previously analyzed in various fields of learning such as online learning (see, e.g., \citet{wagenmaker2022first}). \citet{hanneke2024revisiting} appear to be the first to consider the dependence on $\tau$ when analyzing \emph{upper bounds} in PAC learning, and we employ their perspective in this work.