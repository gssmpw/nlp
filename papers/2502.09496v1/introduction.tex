\section{Introduction}\label{Section:Introduction}
Binary classification stands as perhaps the most fundamental setting in supervised learning, and one of its best-understood. In Valiant's celebrated Probably Approximately Correct (PAC) framework, learning is formalized using a domain $\CX$ in which unlabeled datapoints reside, the label space $\CY = \{\pm 1\}$, and a probability distribution $\CD$ over $\CX \times \CY$. The purpose of a learner $\CA$ is to receive a training set $S = (x_i, y_i)_{i=1}^m$ of points drawn i.i.d.\ from $\CD$ and then emit a hypothesis $\CA(S) : \CX \to \CY$ which is unlikely to mispredict the label of a new datapoint $(x, y)$ drawn from $\CD$.  

At the center of a learning problem is a hypothesis class of functions $\CH \subseteq \CY^\CX$, with which the learner $\CA$ must compete. More precisely, $\CA$ is tasked with emitting a hypothesis $f$ whose \emph{error}, denoted $\ls_\CD(f) := \P_{(x, y) \sim \CD} \big( f(x) \neq y \big)$, is nearly as small as that of the best hypothesis in $\CH$. We denote the latter hypothesis as $h^*_\CD := \argmin_{\CH} \ls_\CD(h)$. Of course, the distribution $\CD$ is unknown to $\CA$, and $\CA$ is judged on its performance across a broad family of allowable distributions $\D$. In \emph{realizable} learning, $\D = \{\CD : \inf_\CH \ls_\CD(h) = 0\}$,
meaning $\CA$ is promised that there always exists a \emph{ground truth} hypothesis $h^* \in \CH$ attaining zero error. 
In this case, for $\CA$ to compete with the performance of $h^*_\CD$ simply requires that it attain error $\leq \epsilon$ for all realizable distributions $\CD$ (with high probability over the training set $S \sim \CD^m$, and using the smallest volume of data $m$ possible). In \emph{agnostic} learning, $\D$ is defined as the set of all probability distributions over $\CX \times \CY$, meaning any data-generating process is allowed. Thus $\CA$ is not equipped with any information concerning $\CD \in \D$ at the outset, but in turn it is only required to emit a hypothesis $f$ with $\ls_\CD (f) \leq \ls_\CD(h^*_\CD) + \epsilon$. 

Perhaps the most fundamental questions in PAC learning ask: What is the minimum number of samples required to achieve error at most $\epsilon$? And which learners attain these rates? For realizable binary classification, the optimal sample complexity of learning remained a foremost open question for decades, and was finally resolved by breakthrough work of \citet{hanneke2016optimal} which built upon that of \citet{simon2015almost}. Notably, optimal binary classification requires the use of learners which emit hypotheses $f$ outside of the underlying hypothesis class $\CH$. Such learners are referred to as being \emph{improper}. This has the effect of excluding Empirical Risk Minimization (ERM) from contention as an optimal learner for the realizable case. Recall that ERM learners proceed by selecting a hypothesis $h_S \in \CH$ incurring the fewest errors on the training set $S$. (For realizable learning, note that $h_S$ will then incur zero error on $S$.) ERM, then, is an example of a \emph{proper} learner, which always emits a hypothesis in the underlying class $\CH$. 
Agnostic learning, however, exhibits no such properness barrier for optimal learning. In fact, ERM algorithms are themselves optimal agnostic learners, despite their shortcomings on realizable distributions. This somewhat counter-intuitive behavior owes itself to the fact that agnostic learners are judged on a worst-case basis across all possible distributions $\CD$. Simply put, non-realizable distributions are more difficult to learn than realizable distributions --- even when one discounts the error term by $\ls_\CD(h^*_\CD)$ --- and thus ERM learners are permitted to incur some unnecessary error on realizable distributions, so long as they remain within the optimal rates induced by the more difficult (non-realizable) distributions. 

In light of this behavior, it is natural to ask for a more refined perspective on agnostic learning, in which the error incurred beyond $\tau := \ls_\CD(h^*_\CD)$ is itself studied as a function of $\tau$. Note that this formalism addresses the previously-described issue by demanding that learners attain superior performance on distributions with smaller values of $\tau$ (such as realizable distributions, for which $\tau = 0$). It is precisely this perspective that was recently 
studied by \citet{hanneke2024revisiting}. In this model, \citet{hanneke2024revisiting} established that ERM, and indeed any proper learner, is sub-optimal by a factor of $\sqrt{\log(1 / \tau)}$. Furthermore, they exhibit a learner which is optimal for a wide range of values of $\tau$, and is based upon an interesting technique of repeatedly training pairs of good hypotheses which disagree on many inputs. 

Nevertheless, \citet{hanneke2024revisiting} leaves several interesting questions open to future work. 

\begin{problem} \label{Question:1-tau-d/n-setting} 
Let $d = \VC(\CH)$ and $m = |S|$. What is the optimal sample complexity of learning in the regime where $\tau \approx d / m$? 
\end{problem}

\begin{problem} \label{Question:2-majority-voting}
Are learners based upon majority voting, such as bagging, optimal in the \linebreak $\tau$-based agnostic learning framework?
\end{problem}
    
\begin{problem} \label{Question:3-computational-efficiency}
Can one design a computationally efficient learner which is optimal in the \linebreak $\tau$-based agnostic learning framework? 
\end{problem}

The primary focus of our work is to resolve Open Problem~\ref{Question:1-tau-d/n-setting}, thereby extending the understanding of optimal error rates across a broader range of $\tau$. As part of our efforts, we also make progress on Open Problems \ref{Question:2-majority-voting} and \ref{Question:3-computational-efficiency}, as we now describe. 

\subsection{Overview of Main Results}

We now present our primary result and a brief overview of our approach. 

\begin{theorem}\label{thm:main-result}
    For any domain $\CX,$ hypothesis class $\CH$ of VC dimension $d,$ number of samples $m,$
    parameter $\delta \in (0,1),$
    there is an algorithm such that for any
    distribution $\CD$ over $\CX \times \{-1,1\}$ returns a hypothesis $h_S: \CX \rightarrow \{-1.1\}$ which, with probability at least $1-\delta$, has error bounded by
    \[ \hspace{-1 cm}
        \min \left\{  2.1 \cdot \tau + O\left(\sqrt{\frac{\tau(d + \ln(1/\delta))}{m}} + \frac{d + \ln(1/\delta)}{m} \right),  \tau + O\left(\sqrt{\frac{\tau(d + \ln(1/\delta))}{m}} + \frac{\ln^5(m/d)\cdot(d + \ln(1/\delta))}{m} \right) \right\} \,,
    \]
    where $\tau$ is the error of the best hypothesis in $\CH.$
\end{theorem}

Let us briefly describe our our approach. The lower bound from \citet{devroye1996probabilistic}
shows that any learner upon receiving $m$ i.i.d.\ samples from $\CD$ must produce with probability
at least \linebreak 
$1-\delta$ a hypothesis incurring error
$ \tau + \Omega\left(\sqrt{\frac{\tau(d + \ln(1/\delta))}{m}} + \frac{d + \ln(1/\delta)}{m} \right), 
$
for worst-case $\CD$. 
Our first elementary observation is that, due to the aforementioned
lower bound, by designing a learner whose error is bounded by 
$  c\cdot \tau + O\left(\sqrt{\frac{\tau(d + \ln(1/\delta))}{m}} + \frac{d + \ln(1/\delta)}{m} \right)\,,
$
for some numerical constant $c \geq 1$, we can get a tight bound
in the regime $\tau \approx \nicefrac{d}{m},$ and thus
resolve \Cref{Question:1-tau-d/n-setting}. 
Furthermore, in order to make progress across the full regime $\tau \in [0,\nicefrac{1}{2}],$ it is crucial to obtain a constant $c$ whose value is close to 1. 
Motivated by this observation, our primary result gives an algorithm that
proceeds by taking 
majority votes over ERMs trained on carefully crafted subsamples of the training set $S$, and which achieves error  $2.1 \cdot \tau + O\left(\sqrt{\frac{\tau(d + \ln(1/\delta))}{m}} + \frac{d + \ln(1/\delta)}{m} \right)$. 
Notably, this brings us within striking distance of the constant $c = 1$ --- we leave open the intriguing question of whether our approach can be refined to lower $c$ to 1, which would completely settle the complexity of agnostic learning. 

{Our approach is inspired by \citet{hanneke2016optimal},
but introduces several new algorithmic components and ideas in the analysis. More concretely, we first modify Hanneke's sample splitting scheme, 
and then randomly select a small fraction of the resulting subsamples on which to run $\cerm$, rather than running $\cerm$ on all such subsamples. This improves the $\cerm$-oracle efficiency of the algorithm.
In order to decrease the value of the constant multiplying $\tau$, our main insight is to run \emph{two} independent copies of the above classifier, as well as one $\cerm$
that is trained on elements coming from a certain ``region of disagreement'' of the previous two classifiers. For any test point $x,$ if both of the voting
classifiers agree on a label $y$ and have ``confidence'' in their vote, we output $y;$ otherwise we output the prediction of the
$\cerm.$ We hope that this idea of breaking ties between
voting classifiers using $\cerm$s that are trained on their
region of disagreement can find further applications.
} 
{Our resulting algorithm employs a voting scheme at its center, making progress on \Cref{Question:2-majority-voting}, and is  computationally efficient with respect to ERM oracle calls, making considerable progress on \Cref{Question:3-computational-efficiency}.}
Finally, we combine this algorithm with the learner of \citet{hanneke2024revisiting} to obtain a ``best-of-both-worlds'' result. {An overview of all the steps is presented in \cref{apx:final-algo}.}

\subsection{Related Work}

The PAC learning framework for statistical learning theory dates to the seminal work of \citet{valiant1984theory}, with roots in prior work of Vapnik and Chervonenkis \citep{vapnik1964class, vapnik1974theory}. In binary classification, finiteness of the VC dimension was first shown to characterize learnability by \citet{blumer1989learnability}. Tight lower bounds on the sample complexity of learning VC classes in the realizable case were then established by \citet{ehrenfeucht1989general} and finally matched by upper bounds of \citet{hanneke2016optimal}, building upon  work of \citet{simon2015almost}. Subsequent works have established different optimal PAC learners for the realizable setting \citet{aden2023optimal, baggingoptimal,aden2024majority}.
For agnostic learning in the standard PAC framework, ERM has been known for some time to achieve sample complexity matching existing lower bounds \citep{haussler1992decision, boucheron2005theory, anthony2009neural}. As previously described, we direct our attention to a more fine-grained view of agnostic learning, in which the error incurred by a learner above and beyond that of the best-in-class hypothesis $\tau = \ls_\CD(h^*_\CD)$ is itself studied as a function of $\tau$. Lower bounds employing $\tau$ in the error term are sometimes referred to as \emph{first-order bounds} and have been previously analyzed in various fields of learning such as online learning (see, e.g., \citet{wagenmaker2022first}). \citet{hanneke2024revisiting} appear to be the first to consider the dependence on $\tau$ when analyzing \emph{upper bounds} in PAC learning, and we employ their perspective in this work.
