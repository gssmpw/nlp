\section{Preliminaries}\label{Section:Preliminaries}


\paragraph{Notation}

For a natural number $m \in \N$, $[m]$ denotes the set $\{1, \ldots, m\}$. Random variables are written in bold-face (e.g., $\mathbf{x}$) and their realizations in non-bold typeface (e.g., $x$). For a set $Z$, $Z^*$ denotes the set of all finite sequence in $Z$, i.e., $Z^* = \bigcup_{i \in \N} Z^i$. For a sequence $S$ of length $m$ and indices $i \leq  j \in [m]$, $S[i:j]$ denotes the smallest contiguous subsequence of $S$ which includes both its $i$th and $j$th entries. Furthermore, we employ 1-indexing for sequences. For $S = (a, b, c)$, for instance, $S[1:2] = (a, b)$. The symbol $\sqcup$ is used to denote concatenation of sequences, as in $(a, b) \sqcup (c, d) = (a, b, c, d)$. When $S, S' \in Z^*$ are sequences in $Z$ and each element $s \in S$ appears in $S'$ no less frequently than in $S$, we write $S \sqsubseteq S'$.
For a sequence $S$ and a set $A$
we denote by $S \sqcap A$ the longest subsequence of $S$
that consists solely of elements of $A.$
If $S$ is a finite set, then $\E_{x \in S} f(x)$ denotes the expected value of $f$ over a uniformly random draw of $x \in S$. 

\paragraph{Learning Theory}

Let us briefly recall the standard language of supervised learning. Unlabeled datapoints are drawn from a \defn{domain} $\CX$, which we permit to be arbitrary throughout the paper. We study binary classification, in which datapoints are labeled by one of two labels in the \defn{label set} $\CY = \{\pm 1\}$. A function $h : \CX \to \CY$ is referred to as a \defn{hypothesis} or \defn{classifier}, and a collection of such functions $\CH \subseteq \CY^\CX$ is a \defn{hypothesis class}. Throughout the paper, we employ the 0-1 loss function 
$\ell_{0-1} : \CY \times \CY \to \R_{\geq 0}$  defined by $\ell_{0-1}(y, y') = \ind [y \neq y']$. 
A \defn{training set} is a sequence of labeled datapoints $S = \big((x_1, y_1), \ldots, (x_m, y_m) \big) \in (\CX \times \CY)^*$. A \defn{learner} is a function which receives training sets and emits hypotheses, e.g., $\CA: (\CX \times \CY)^* \to \CY^\CX$. The purpose of a learner is to emit a hypothesis $h$ which attains low \defn{error}, or \emph{true error}, with respect to an unknown probability distribution $\CD$ over $\CX \times \CY$. That is,
$ \ls_\CD(h) = \E_{(x, y) \sim \CD} \ind [h(x) \neq y].$
A natural proxy for the true error of $h$ is its \defn{empirical error} on a training set $S = (x_i, y_i)_{i \in [m]}$, denoted $\ls_S(h) = \E_{(x, y) \in S} \ind[ h(x) \neq y]$.  
If $\CA$ is a learner for $\CH$ with the property that $\CA(S) \in \argmin_\CH \ls_S(h)$ for all training sets $S$, then $\CA$ is said to be an \defn{empirical risk minimization} (ERM) learner for $\CH$. Throughout the paper we will use $\CA$ for an ERM learner.


\section{Proof Sketch}\label{sec:proof-sketch}
In this section we provide a detailed explanation 
of our approach and give a comprehensive sketch of the 
proof. 
In order to provide more intuition, 
we divide our discussion into two parts.
In the first, we present a simpler approach
that gives a bound of $15\tau + O \left(\sqrt{\tfrac{\tau(d + \ln(1/\delta))}{m}} + \tfrac{d + \ln(1/\delta)}{m} \right).$ Recall that this already
suffices to resolve the optimal sample complexity 
in the regime $\tau \approx \nicefrac{d}{m}.$
In the second part, we describe several modifications to 
our algorithm as well as new ideas in its analysis
which help us drive the error down to 
$ 2.1\tau + O \left(\sqrt{\frac{\tau(d + \ln(1/\delta))}{m}} + \tfrac{d + \ln(1/\delta)}{m} \right).$ 


\subsection{First Approach: Multiplicative Constant 15}\label{sec:large-constant-sketch}
A crucial component of our algorithm
is a scheme that recursively splits the
input dataset $S$ into subsequences, 
which is an adaptation of Hanneke's splitting algorithm \citep{hanneke2016optimal} and is formalized by
\Cref{alg:splittingwith3}.
In comparison to Hanneke's scheme, we have made two modifications that, as we will explain shortly,
help us control the constant multiplying $\tau$.
The first is to create disjoint splits
of the dataset {(rather than carefully overlapping subsets, as employed by Hanneke)}, and the second is to include more elements in $S_{i,\sqcap}$
than $S_{i,\sqcup}.$ 

We now introduce some further notation.
Let $s'_{\sqcap}\coloneqq\frac{|S|}{|S_{1,\sqcap}|},$
where $S_{1,\sqcap}$ is defined in \Cref{alg:splittingwith3}.
Furthermore,  for the set of training sequences generated by $ \cS'(S;T) $ and an $ \cerm $-algorithm $ \erm $, we  write $ \erm'(S;T) $ for the set of classifiers the $ \cerm $-algorithm outputs when run on the training sequences in $ \cS'(S;T) $, i.e., $ \erm'(S;T)=( \erm(S') )_{S'\in\cS'(S;T)}  $, where this is   a multiset.
We remark that our final algorithm 
\cref{alg:splittingwith27} does not run on all the subtraining sequences created by $ \cS'(S;T),$ as  
 it calls the $ \cerm $ algorithm $ O \big(\ln{\left(m/(\delta(d+\ln{\left(1/\delta \right)})) \right)} \big) $ times. 
For an example $ (x,y)$, we define $ \avg(\erm'(S;T))(x,y)=\sum_{h\in \erm'(S;T)}\ind\{ h(x)\not=y \}/|\erm'(S;T)| $, i.e., the fraction of incorrect hypotheses in $ \erm'(S;T)$ for $(x, y)$.  
For a natural number $ t \in \N$, we let $ \widehat{\erm'}_{\rt}(S;T),$ be the {random} multiset of $ t $ hypotheses drawn independently and uniformly at random from $ \erm'(S;T).$
In a slight overload of notation, we use 
$ \rt $ (i.e., $ t $ in bold font) to denote the randomness used to draw the $ t $ hypotheses from $ \erm'(S;T).$ 
Intuitively, one can think of $ \widehat{\erm'}_{\rt} $ as a bagging algorithm where the subsampled training sequences are restricted to subsets of $ \cS'(S;T).$
In what follows, we consider this algorithm parametrized by $ t=O \big(\ln{\left(m/(\delta(d+\ln{\left(1/\delta \right)})) \right)}\big).$ 
Similarly to the above, we define $\avg(\widehat{\erm'}_{\rt}(S;T))(x,y)=\sum_{h\in \widehat{\erm'}_{\rt}(S;T)}\ind\{ h(x)\not=y \}/|\widehat{\erm'}_{\rt}(S;T)|$.
For a distribution $ \cD $ over $ \cX\times \{ -1,1\},$ training sequences $S, T\in \left(\cX\times\{  -1,1\} \right)^{*},$ and $ \alpha\in[0,1]$,  we let $ \ls_{\cD}^{\alpha}(\erm'(S;T))=\p_{(\rx,\ry)\sim \cD}\left[\avg(\erm'(S;T))(\rx,\ry) \geq \alpha\right],$ i.e., the probability that at least an $ \alpha $-fraction of the hypotheses in $ \erm'(S;T) $ err on a new example drawn from $ \cD $.
Similarly, we define $ \ls_{\cD}^{\alpha}(\widehat{\erm'}_{\rt}(S;T))=\p_{(\rx,\ry)\sim \cD}\left[\avg(\widehat{\erm'}_{\rt}(S;T))(\rx,\ry) \geq \alpha\right].$ Finally, we let $ \widehat{\cA'}_{\rt}(S)=\widehat{\cA'}_{\rt}(S;\emptyset). $  

We now describe our first approach,
which we break into three steps.  The first step relates the error of  $ \ls_{\cD}(\widehat{\cA'}_{\rt}(\rS)) $ to $ \ls_{\cD}^{0.49}\left(\cA'(\rS,\emptyset)\right) $, and the second and third steps bound the error of $ \ls_{\cD}^{0.49}\left(\cA'(\rS,\emptyset)\right) $. The first step borrows ideas from \citet{baggingoptimal} and the last step from \citet{hanneke2016optimal}, but there are
several technical bottlenecks in the analysis that do 
not appear in these works, as they  
consider the realizable setting, for which $\tau = 0.$

\begin{algorithm}
\caption{Splitting algorithm $\cS'$}\label{alg:splittingwith3}
\KwIn{Training sequences $S, T \in (\cX \times \cY)^{*}$, where $|S| = 3^{k}$ for $k \in \mathbb{N}$.}
\KwOut{Family of training sequences.}
\If{$k \geq 6$}{
    Partition $S$ into $S_{1}, S_{2}, S_{3}$, with $S_i$ being the $(i-1)|S|/3+1$ to the $i|S|/3$ training examples of $S$. Set for each $ i $ 
    \newline
     \textcolor{white}{halloooooooooooooo}   $S_{i,\sqcup} = S_{i}[1:3^{k-4}],\quad \quad
        S_{i,\sqcap} = S_{i}[3^{k-4}+1:3^{k-1}]$,
    \newline
    \Return{$[\cS'(S_{1,\sqcup}; S_{1,\sqcap} \sqcup T), \cS'(S_{2,\sqcup}; S_{2,\sqcap} \sqcup T), \cS'(S_{3,\sqcup}; S_{3,\sqcap} \sqcup T)]$}
}
\Else{
    \Return{$S \sqcup T$}}
\end{algorithm}


\phantomsection
\addcontentsline{toc}{subsubsection}{
Relating the error of $ \widehat{\cA'}_{\rt}(\rS) $ to $ \cA'(\rS,\emptyset)$
}

\paragraph{Relating the error of $ \widehat{\cA'}_{\rt}(\rS) $ to $ \cA'(\rS,\emptyset) $:}
In the first step, we draw inspiration from the seminal work of \cite{baggingoptimal}, and use the additional idea of linking the performance of a random classifier to a deterministic one. More precisely, we demonstrate how to bound the error of the random classifier $\widehat{\cA'}_{\rt}(\rS)$ 
using the error of $ \cA'(\rS,\emptyset)$. 
First, let $\rS \sim \cD^m$ and
assume we have shown that 
\begin{align}\label{eq:proofsketch-1}
    \ls_{\cD}^{0.49}(\cA'(\rS;\emptyset))\leq 15\tau+O\left(\sqrt{\frac{\tau(d+\ln{\left(1/\delta \right)})}{m}}+\frac{d+\ln{\left(1/\delta  \right)}}{m}\right)\,,
  \end{align} 
with probability at least $ 1-\delta/2.$ Consider the event $ E=\{ (x,y):  \avg(\erm'(\rS;\emptyset))(\rx,\ry) \geq \nicefrac{49}{100}\}.$ 
By the law of total expectation, we bound the error of $ \widehat{\erm'}_{\rt} $ as  $ \ls_{\cD}(\widehat{\erm'}_{\rt})\leq \p_{(\rx,\ry)\sim\cD}\left(E\right)+\e_{(\rx,\ry)\sim \cD}[\ind\{\widehat{\erm'}_{\rt}(\rx)\not=\ry  \}\mid \bar{E} ].$ 
We see that the first term on the right-hand side is $ \ls_{\cD}^{0.49}(\cA(S;\emptyset)) $, which we have assumed for the moment can be bounded by \Cref{eq:proofsketch-1}. We now argue that the second term can be bounded by $ O((d+\ln{\left(1/\delta \right)})/m).$  
Note that under the event $ \bar{E}=\{ (x,y):  \avg(\erm(S;\emptyset))(x,y) < \nicefrac{49}{100} \}$, strictly more than half of the hypotheses in $ \erm(S;\emptyset)$ --- from which the hypotheses of $ \widehat{\erm'}_{\rt} $ are drawn --- are correct. 
Using this, combined with Hoeffding's inequality and switching the order of expectation (as $ \rt $ and $ (\rx,\ry) $ are independent), we get that $ \e_{\rt}[\e_{(\rx,\ry)\sim\cD}[\ind\{\widehat{\erm'}_{\rt}(\rx)\not=\ry  \}\mid \bar{E} ]] \leq \exp\left(-\Theta(t)\right).$ 
Setting $ t=\Theta(\ln{\left(m/(\delta(\ln{\left(1/\delta \right)}+d)) \right)})$ then gives  $\e_{\rt}[\e_{(\rx,\ry)\sim\cD}[\ind\{\widehat{\erm'}_{\rt}(\rx)\not=\ry  \}\mid \bar{E} ] ]=O\left((\delta(\ln{\left(1/\delta \right)}+d))/m\right).$ 
By an application of Markov's inequality, this implies with probability at least $ 1-\delta/2 $ over the draws of hypotheses in $ \widehat{\erm'}_{\rt}$ that $ \e_{(\rx,\ry)\sim\cD}[\ind\{\widehat{\erm'}_{\rt}(\rx)\not=\ry  \}\mid \bar{E} ]=O((d+\ln{\left(1/\delta \right)})/m).$
This bounds the second term in the error decomposition of $ \ls_{\cD}(\widehat{\cA'}_{\rt}(\rS;\emptyset))$ and  gives the claimed bound on $ \widehat{\cA'}_{\rt}(\rS).$ 

\phantomsection
\addcontentsline{toc}{subsubsection}{
Bounding the error of $ \ls_{\cD}^{0.49}(\cA'(\rS,\emptyset)) $
}

\paragraph{Bounding the error of $ \ls_{\cD}^{0.49}(\cA'(\rS,\emptyset)) $:}
We now give the proof sketch of \cref{eq:proofsketch-1}. 
To this end, for a training sequence $ S' $ and  hypothesis $ h$ we define $\sum_{\not=}(h,S')  =\sum_{(x,y)\in S'}\ind\{h(x)\not=y\}.$ 
Assume for the moment that we have demonstrated that with probability at least $ 1-\delta/4 $ over $ \rS, $ and some numerical constants $c_b, c_c$,
\begin{align}\label{eq:proofsketch0}
    \ls_{\cD}^{0.49}(\erm'(\rS;\emptyset))
    \leq  \max\limits_{S'\in\cS(\rS,\emptyset)} \frac{14\Sigma_{\not=}(\hs\negmedspace,S')}{m/s_{\sqcap}'}+\sqrt{\frac{\cb\left(d+\ln{(4/\delta )}\right)\frac{14\Sigma_{\not=}(\hs\negmedspace,S')}{(m/s_{\sqcap}')}}{m}}+\frac{\cc\left(d+\ln{(4/\delta )}\right)}{m} \,.
\end{align}
In order to take advantage of \Cref{eq:proofsketch0}, first observe that for each $ i\in\{  1,2,3\}  $, we have that 
\[ \max_{S'\in\cS(\rS_{i,\sqcup};\rS_{i,\sqcap}\sqcap\emptyset)}\frac{14\Sigma_{\not=}(\hs\negmedspace,S')}{m/s_{\sqcap}'} \leq \frac{14|\rS_{i}|\Sigma_{\not=}(\hs\negmedspace,\rS_{i})}{|\rS_{i}|m/s_{\sqcap}'} \leq 15\ls_{\rS_{i}}(\hs), \]
as $ S'\sqsubseteq \rS_{i}$ and $ |\rS_{i}|s_{\sqcap}'/m=|\rS_{i}|/|\rS_{i,\cap}|=1/(1-\frac{1}{27}) $ by the choice of the splitting algorithm. 
We briefly remark that this upper bound on the loss of $ \ls_{\rS_{i}}$ is a source of a multiplicative factor on $ \tau$. However, it can be made arbitrarily close to 1 by making the split between $ \rS_{i,\sqcup} $ and $ \rS_{i,\sqcap} $ more imbalanced in the direction of $ \rS_{1,\sqcap},$ at the cost of larger constants $ \cb, \cc.$
Now, by an application of Bernstein's inequality on the hypothesis $ \hs$ (\cref{lem:additiveerrorhstar})
for each $ i\in\{ 1,2,3 \}  $, we have that $ 15\ls_{\rS_{i}}(\hs) $ is at most  $ 15(\tau+O(\sqrt{\tau\ln{\left(4/\delta \right)}/m}+\ln{\left(1/\delta \right)}/m)),$ with probability at least $ 1-\delta/4$ over $ \rS_{i}.$
Thus, by a union bound, we have that  $ \max_{S'\in\cS(\rS,\emptyset)}14\Sigma_{\not=}(\hs\negmedspace,S')/(m/s_{\sqcap}')=15(\tau+O(\sqrt{\tau\ln{\left(4/\delta \right)}/m}+\ln{\left(1/\delta \right)}/m)),$ with probability at least $ 1- 3\delta/4$ over $ \rS.$ 
Finally, union bounding with the event in \cref{eq:proofsketch0} yields that both events hold with probability at least $ 1-\delta $ over $ \rS,$ from which the error bound of \cref{eq:proofsketch-1} follows by inserting the bound of $ \max_{S'\in\cS(\rS,\emptyset)}14\Sigma_{\not=}(\hs\negmedspace,S')/(m/s_{\sqcap}') $ into \cref{eq:proofsketch0}.    

\phantomsection
\addcontentsline{toc}{subsubsection}{
Relating the error of $ \ls_{\cD}^{0.49}(\cA'(\rS,\emptyset)) $ to the empirical error of $ \hs $
}

\paragraph{Relating the error of $ \ls_{\cD}^{0.49}(\cA'(\rS,\emptyset)) $ to the empirical error of $ \hs $:}
Here we draw inspiration from the seminal ideas of \cite{hanneke2016optimal} by analyzing the learner's loss recursively. (We remark, however, that in certain aspects of our analysis we will be obligated to diverge from Hanneke's approach, which is tailored to the realizable setting.) 
Similarly to \citet{hanneke2016optimal}, we also consider a second training sequence $T$ received as an argument of $ \cS(\, \cdot \,, \, \cdot \,) $, which one should think of as the concatenation of all training sequences created in the previous recursive calls of \Cref{alg:splittingwith3}. 
In the analysis of \cite{hanneke2016optimal}, this training sequence is known to be realizable by a target concept in $\CH$, as it is the concatenation of realizable training sequences. 
In our work we are no longer promised that this condition will hold, however, and thus we must consider arbitrary training sequences $ T\in (\cX\times \{-1,1  \} )^{*}$. For such sequences,
 we will demonstrate that with probability at least $ 1-\delta $ over $ \rS $, 
\begin{align}\label{eq:proofsketch3}
    \ls_{\cD}^{0.49}(\erm'(\rS;T))
    \leq  \max\limits_{S'\in\cS(\rS;T)}\frac{14\Sigma_{\not=}(\hs\negmedspace,S')}{m/s_{\sqcap}'}+\sqrt{\frac{\cb\left(d+\ln{(1/\delta )}\right)\frac{14\Sigma_{\not=}(\hs\negmedspace,S')}{(m/s_{\sqcap}')}}{m}}+\frac{\cc\left(d+\ln{(1/\delta )}\right)}{m}
\end{align}
Setting $ T=\emptyset $ and rescaling $\delta$ then yields \cref{eq:proofsketch0}.

The first step of our analysis is to relate the error of $ \ls_{\cD}^{0.49}(\cA'(\rS;T)) $ to that of the previous calls $ \cA'(\rS_{i,\sqcup};\rS_{i,\sqcap}\sqcup T). $ 
To do so, we notice that for any $ (x,y) $ such that $ \avg(\cA'(\rS;T))(x,y)\geq \nicefrac{49}{100}$ at least one of the calls $ \cA'(\rS_{i,\sqcup};\rS_{i,\sqcap}\sqcup T) $ for $ i\in\{ 1,2,3 \}  $ must satisfy $ \avg(\cA'(\rS_{i,\sqcup};\rS_{i,\sqcap}\sqcup T))(x,y)\geq \nicefrac{49}{100} $. Further, there must be $ (\frac{49}{100}-\frac{1}{3})\frac{3}{2} =\frac{47}{200}$ of the hypotheses in $ \cup_{j\in\{  1,2,3\}\backslash i } \cA'(\rS_{j,\sqcup};\rS_{j,\sqcap}\sqcup T)$ that also fail on $ (x,y).$
Using these observations, we have that if we draw a random index $ \rI\in\{  1,2,3\}  $ and a random hypothesis $ \rh \in \sqcup_{j\in\{  1,2,3\}\backslash \rI } \cA'(\rS_{j,\sqcup};\rS_{j,\sqcap}\sqcup T),$ then for $(x,y) $ such that  $   \avg(\cA'(\rS;T))(x,y)\geq \nicefrac{49}{100}  $  it holds that $ \p_{\rI,\rh}[\avg(\cA(\rS_{\rI,\sqcup};\rS_{\rI,\sqcap}\sqcup T))(x,y)\geq \nicefrac{49}{100},\rh(x)\not=y]\geq\frac{1}{3}\frac{47}{200}\geq \frac{1}{13}.$  Hence, we have that  $ 13\e_{\rI,\rh}[\p_{(\rx,\ry)\sim\cD}[\avg(\cA(\rS_{\rI,\sqcup};\rS_{\rI,\sqcap}\sqcup T))(\rx,\ry)\geq \nicefrac{49}{100},\rh(\rx)\not=\ry]]\geq \ls_{\cD}^{0.49}(\cA'(\rS;T)).$ 
Furthermore, by the definition of $ \rI\in\{  1,2,3\}  $ and $ \rh$ being a random hypothesis from $ \sqcup_{j\in\{  1,2,3\}\backslash \rI } \cA'(\rS_{j,\sqcup};\rS_{j,\sqcap}\sqcup T),$ we conclude that   
\begin{align}
&\ls_{\cD}^{0.49}(\cA'(\rS;T)) \notag \\
& \quad \leq 13\e_{\rI,\rh}\left[\p_{(\rx,\ry)\sim\cD}\left[\avg(\cA'(\rS_{\rI,\sqcup};\rS_{\rI,\sqcap}\sqcup T))(\rx,\ry)\geq \nicefrac{49}{100},\rh(\rx)\not=\ry\right]\right] \nonumber \\
&\quad \leq 13\negmedspace\negmedspace\negmedspace\negmedspace\negmedspace
    \max_{\substack{i \neq j\in\{1,2,3  \}, \\ h\in\cA'(\rS_{j,\sqcup};\rS_{j,\sqcap}\sqcup T)}} \p_{(\rx,\ry)\sim\cD}\Big[\avg(\cA'(\rS_{i,\sqcup};\rS_{i,\sqcap}\sqcup T))(\rx,\ry)\geq \nicefrac{49}{100},h(\rx)\not=\ry\Big]. \label{eq:proofsketch9}
\end{align}
Thus we have established a relation for the error of $ \ls_{\cD}^{0.49}(\cA'(\rS;T)) $ in terms of the recursive calls. 
We remark here that the constant factor $ 13 $ ends up being multiplied onto $ \tau $ and that this is the only step of analysis where this constant cannot be made arbitrarily close to 1 by trading off a larger constant in  other terms. Improving this step of the analysis in an interesting open direction.   

As in \cite{hanneke2016optimal}, we proceed to show by induction on the size of $|\rS|=3^{k}, k\in\mathbb{N} $, training sequences $T, $ and $ 0< \delta<1$ that \cref{eq:proofsketch3} holds with probability at least $ 1-\delta $ over $ \rS. $  
By setting $ \cb $ and $ \cc $ sufficiently large, we can assume that \cref{eq:proofsketch3} holds for $k < 9$.
Using \cref{eq:proofsketch9}, we observe that
bounding each of the six terms for different combinations $ i,j $ by the expression of \cref{eq:proofsketch3}, with probability at least $ 1-\delta/6,$ would suffice to achieve the claim by a union bound.
Furthermore, as $ \rS_{1},\rS_{2},\rS_{3} $ are i.i.d., the cases happen with equal probability, meaning it suffices to consider the case $ j=1$ and $ i=2.$ 
Notice that $13\max_{h\in\cA'(\rS_{1,\sqcup};\rS_{1,\sqcap}\sqcup T)} \p_{(\rx,\ry)\sim\cD}\left[\avg(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T))(\rx,\ry)\geq \nicefrac{49}{100},h(\rx)\not=\ry\right] $ equals 
\begin{align}\label{eq:proofsketch1}
\hspace{-0.1 cm}
13\negmedspace\negmedspace\negmedspace\negmedspace\negmedspace\negmedspace\max_{h\in\cA'(\rS_{1,\sqcup};\rS_{1,\sqcap}\sqcup T)}\p_{(\rx,\ry)\sim\cD}\left[h(\rx)\not=\ry\mid \avg(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T))(\rx,\ry)\geq \nicefrac{49}{100}\right]\ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)). 
\end{align} 
Supposing that the term $ \ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)) $ witnesses an upper bound of the form
\begin{align}\label{eq:proofsketch5}
\hspace{-0.5 cm}
\ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)) \leq \frac{1}{13}\left(\max\limits_{S'\in\cS(\rS;T)}\frac{14\Sigma_{\not=}(\hs\negmedspace,S')}{m/s_{\sqcap}'}+\sqrt{\frac{\cb\left(d+\ln{(1/\delta )}\right)\frac{14\Sigma_{\not=}(\hs\negmedspace,S')}{(m/s_{\sqcap}')}}{m}}+\frac{\cc\left(d+\ln{(1/\delta )}\right)}{m}\right),
\end{align}
then \cref{eq:proofsketch1} is bounded as in \cref{eq:proofsketch2} and we are done. 
Furthermore, the fact that $ |\rS_{1,\sqcup}|=m/3^{4} $ and the inductive hypothesis give us that
\begin{align}\label{eq:proofsketch6}
    \ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)) \leq \max\limits_{S'\in\cS(\rS;T)}\frac{14\Sigma_{\not=}(\hs\negmedspace,S')}{m/(3^{4}s_{\sqcap}')}+\sqrt{\frac{\cb\left(d+\ln{(16/\delta )}\right)\frac{14\Sigma_{\not=}(\hs\negmedspace,S')}{(m/s_{\sqcap}')}}{m/3^{4}}}+\frac{\cc\left(d+\ln{(16/\delta )}\right)}{m/3^{4}}
\end{align} 
with probability at least $ 1-\delta/16 $ over $ \rS_{2}$. Note that we have upper bounded the $ \max$ over ${S'\in\cS(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)} $ by the $ \max$ over ${S'\in\cS(\rS;T)} $, as the former set is contained in the latter.   
Thus, it suffices to consider the case in which $ \ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)) $ lies between the expressions on the right hand side in \cref{eq:proofsketch5} and \cref{eq:proofsketch6}. 

Let $ A=\{  (x,y)| \avg(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T))(x,y)\geq \nicefrac{49}{100}\}  $ and  $ \rN=|\rS_{1,\sqcap}\sqcap A|.$   
Now, since $ \ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)) \geq \cc( d+\ln{\left(1/\delta \right)})/(13m)$, $ |\rS_{1,\sqcap}|=m/s_{\sqcap}' $, and $ \e_{\rN}\left[\rN\right]=(m/s_{\sqcap}') \times \linebreak \ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)) $, it follows by a Chernoff bound that with probability at least $ 1-\delta/16$, $ \rN\geq  13m/(14s_{\sqcap}')\ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)) $.
Since  $ \rS_{1,\sqcap}\sqcap A \sim \cD(\cdot\mid \avg(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T))(\rx,\ry)\geq \nicefrac{49}{100}) $, we conclude by uniform convergence over $ \cH $ (see \cref{lem:fundamentalheoremoflearning}) that with probability at least $ 1-\delta/16 $ over $ \rS_{1,\sqcap}\sqcap A $, \cref{eq:proofsketch1} is bounded by 
\begin{align}\label{eq:proofsketch2}
    13\negmedspace\negmedspace\negmedspace\negmedspace\negmedspace\negmedspace\max_{h\in\cA'(\rS_{1,\sqcup};\rS_{1,\sqcap}\sqcup T)} \left( \ls_{\rS_{1,\sqcap}\sqcap A}(h)+\sqrt{\frac{C(d+2\ln{\left(48/\delta \right)})}{\rN}}\right)\ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)) \,.
\end{align}
It is worth highlighting that this part of the analysis,
as well as the subsequent parts, diverge from the analysis of \citet{hanneke2016optimal} for learning in the realizable case. Importantly, Hanneke uses the fact that an $ \cerm $ classifier has zero training error on the whole training sequence $ S'\in \cS(\rS_{1,\sqcup}, \rS_{1,\sqcap}\sqcup T) $, and  
invokes the stronger bound of $ O(d\ln{\left(\rN/d \right)}/\rN)$ on $ \rS_{1,\sqcap}\sqcap A, $ without the additional error term $ \ls_{\rS_{1,\sqcap}\sqcap A}(h) $ on the sample. Recall that in the realizable case this term is trivially zero. 

We now proceed to bound each of the two terms in the above \cref{eq:proofsketch2}, considered after factoring in the multiplication by $\ls_{\cD}^{0.49}$.
For the first term, we use that $ \rN\geq 13m/(14s_{\sqcap}')\ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)) $ and that $ \ls_{\rS_{1,\sqcap}\sqcap A}(h)=\sum_{(x,y)\in\rS_{1,\sqcap}\sqcap A}\ind\{h(x)\not=y  \} /\rN $, which implies that 
\begin{align}\label{eq:proofsketch7}
    13\max_{h\in\cA'(\rS_{1,\sqcup};\rS_{1,\sqcap}\sqcup T)}  \ls_{\rS_{1,\sqcap}\sqcap A}(h)\ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)) \leq 14 \max_{h\in\cA'(\rS_{1,\sqcup};\rS_{1,\sqcap}\sqcup T)} \sum_{(x,y)\in\rS_{1,\sqcap}\sqcap A}\frac{\ind\{h(x)\not=y  \} }{m/s_{\sqcap}'}.
\end{align}
Now any $ h\in\cA'(\rS_{1,\sqcup};\rS_{1,\sqcap}\sqcup T) $ is equal to $ \cA(S') $ for some $ S'\in \cS(\rS_{1,\sqcup},\rS_{1,\sqcap}\sqcup T),$ and furthermore we have that $ \rS_{1,\sqcap}\sqcap A $ is contained in $ S' $, whereby we conclude that $ \sum_{(x,y)\in\rS_{1,\sqcap}\sqcap A}\ind\{h(x)\not=y  \} $ is upper bounded by $\sum_{(x,y)\in S'}\ind\{\cA(S')(x)\not=y  \} .$ 
Furthermore, since $ \cA(S') $ is a minimizer of $ \sum_{(x,y)\in S'}\ind\{\cA(S')(x)\not=y  \},$ it is upper bounded by $ \sum_{(x,y)\in S'}\ind\{\hs(x)\not=y  \}$. This implies that $ \max_{h\in\cA'(\rS_{1,\sqcup};\rS_{1,\sqcap}\sqcup T)} \sum_{(x,y)\in\rS_{1,\sqcap}\sqcap A}\ind\{h(x)\not=y  \},$ is at most $ \max_{S'\in\cS(\rS_{1,\sqcup};\rS_{1,\sqcap}\sqcup T)} \sum_{(x,y)\in S'}\ind\{\hs(x)\not=y  \}.$ Additionally, this can be further upper bounded by $ \max_{S'\in\cS(\rS; T)}\sum_{(x,y)\in S'}\ind\{\hs(x)\not=y  \}$ as  $ \cS(\rS_{1,\sqcup};\rS_{1,\sqcap}\sqcup T) \sqsubseteq \cS(\rS;T).$ Combining the previous observations and invoking \cref{eq:proofsketch7}, we have that
\begin{align}\label{eq:proofsketch10}
    13\max_{h\in\cA'(\rS_{1,\sqcup};\rS_{1,\sqcap}\sqcup T)}  \ls_{\rS_{1,\sqcap}\sqcap A}(h)\ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)) \leq 14 \max\limits_{S'\in\cS(\rS;T)}\frac{14\Sigma_{\not=}(\hs\negmedspace,S')}{m/s_{\sqcap}'}.
\end{align}
We emphasize that the above step of lower bounding the size of $ \rN $ by $ (13m)/(14s_{\sqcap}')\ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)) $ will, in the end, incur a constant factor scaling of $ \tau $ by $ \frac{14}{13}.$ This can be made arbitrarily close to 1 by using a tighter Chernoff bound for the size of $ \rN $ at the cost of a larger constant
in other terms.

Now for the second term $ \sqrt{C(d+2\ln{\left(48/\delta \right)})/\rN}\ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)) $ of \cref{eq:proofsketch2}, by again using that $ \rN\geq  (13m)/(14s_{\sqcap}')\ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)),$ we can upper bound it by 
\[ \sqrt{(14C(d+2\ln{\left(48/\delta \right)})\ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)))/(13m/s_{\sqcap}')}. \]  
Furthermore, since we considered the case in \cref{eq:proofsketch6}, implying a bound on $\ls_{\cD}^{0.49}$ in terms of $\Sigma_{\not=}(\hs\negmedspace,S')$ and $\left(d+\ln{(1/\delta )}\right))/m$,
one can show using some calculations that for $ \cb $ and $ \cc $ large enough, the second term of {\cref{eq:proofsketch2}} is upper bounded by 
\[ \sqrt{\frac{\cb\left(d+\ln{(1/\delta )}\right)\frac{14\Sigma_{\not=}(\hs\negmedspace,S')}{(m/s_{\sqcap}')}}{m}}+\frac{\cc\left(d+\ln{(1/\delta )}\right)}{m}. \] 
It is worth mentioning that in these calculations it is important that we have a $ \sqrt{\cdot} $ around $ 14C(d+2\ln{\left(48/\delta \right)})\ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T))/(13m/s_{\sqcap}') $. This is because, when plugging in the upper bound $ \ls_{\cD}^{0.49}(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T)) $ and using $ \sqrt{a+b}\leq \sqrt{a}+\sqrt{b} $ for $ a,b>0,$ we get a term scaled by $ \sqrt{\sqrt{\cb}} $ and one by $ \sqrt{\cc},$ which for $\cb $ and $ \cc $ large enough, allows for the numerical factors that appear in the upper bound to be bounded by  $ \sqrt{\sqrt{\cb} }$ and $ \sqrt{\cc} $, respectively. 
Combining the bound of the first and second term of \cref{eq:proofsketch2}, we get that \cref{eq:proofsketch2} can be upper bounded by 
\begin{align}\label{eq:proofsketch4}
    \max\limits_{S'\in\cS(\rS;T)}\frac{14\Sigma_{\not=}(\hs\negmedspace,S')}{m/s_{\sqcap}'}+\sqrt{\frac{\cb\left(d+\ln{(1/\delta )}\right)\frac{14\Sigma_{\not=}(\hs\negmedspace,S')}{(m/s_{\sqcap}')}}{m}}+\frac{\cc\left(d+\ln{(1/\delta )}\right)}{m} \,.
\end{align}
which is the bound needed for the inductive step. Recall also that \cref{eq:proofsketch2} upper bounds the term $
    13\max_{h\in\cA'(\rS_{1,\sqcup};\rS_{1,\sqcap}\sqcup T)}\p_{(\rx,\ry)\sim\cD}\left[\avg(\cA'(\rS_{2,\sqcup};\rS_{2,\sqcap}\sqcup T))(\rx,\ry)\geq \frac{49}{100},h(\rx)\not=\ry\right],
$
so the bound appearing in \cref{eq:proofsketch4} also holds for that term. Recall this was exactly the upper bound we were aiming to show, and the final result follows by a union bound.

\subsection{Improved Approach: Multiplicative Constant 2.1}
We are now ready to describe the main modifications in the above approach of \cref{sec:large-constant-sketch}
that will allow us to reduce the constant factor multiplying $\tau.$
At a high level, our modifications are twofold: First, we use a different
splitting scheme which recursively splits the dataset into more than 3 subsamples, and secondly,
rather running a single instance of the splitting algorithm and taking 
a majority vote over $\cerm$ learners on the sampled datasets, we split the training set into three parts, run two independent instances
of the voting classifier on the first two parts, and one instance of $\cerm$
on a subsample of the third part. The subsample we train the $\cerm$ on
is carefully chosen and depends on a certain notion of a ``region of disagreement''
of the two voting classifiers.
The final prediction is given as follows: If both voting classifiers
agree on the same label with a certain notion of ``margin,'' then we output
that label, otherwise, we output the prediction of $\cerm.$
This is done by preprocessing the training
set and splitting it into three parts. Lastly, we combine our approach
with the approach of \citet{hanneke2024revisiting} and get a best-of-both-worlds bound.

\begin{algorithm}
    \caption{Splitting algorithm $\cS$}\label{alg:splittingwith27}
    \KwIn{Training sequences $S, T \in (\cX \times \cY)^{*}$, where $|S| = 3^{k}$ for $k \in \mathbb{N}$.}
    \KwOut{Family of training sequences.}
    \If{$k \geq 6$\label{alg:splitting27if}}{
        Partition $S$ into $S_{1}, \ldots, S_{27}$, with $S_i$ being the $(i-1)|S|/27+1$ to the $i|S|/27$ training examples of $S$. Set for each $ i $ 
        \newline 
        \textcolor{white}{halloooooooooooooo}   $
             S_{i,\sqcup} = S_{i}[1:3^{k-6}],\quad \quad
            S_{i,\sqcap} = S_{i}[3^{k-6}+1:3^{k-3}], $
            \newline 
        \Return{$[\cS(S_{1,\sqcup}; S_{1,\sqcap} \sqcup T), \ldots, \cS(S_{27,\sqcup}; S_{27,\sqcap} \sqcup T)]$}
    }
    \Else{
        \Return{$S \sqcup T$}
    }
\end{algorithm}

We first recall the sources that lead to the constant multiplied onto $\tau$. In the analysis from \cref{sec:large-constant-sketch}, we see that there are three such sources. The first one comes from the balance between the splits of $ S_{i} $ in \cref{alg:splittingwith3}, i.e. $ \left(|S_{i,\sqcap}|+|S_{i,\sqcup}|\right)/|S_{i,\sqcap}|=1/(1-\nicefrac{1}{27}).$ 
Recall that this constant can be driven down to 1 by considering even more imbalanced splits of the dataset, at the expense
of larger constants multiplying the remaining terms of the bound.
Similarly, the third source of the 
error
comes from the step \cref{eq:proofsketch10} which controls the size of $ \rN $ through a Chernoff bound.
This can also be driven arbitrarily close to 1.

Hence, it is clear that the main overhead, in the form of a factor 13, comes from the argument in \cref{eq:proofsketch9} relating the error of $ \cA'(\rS,T) $ to that of its recursive calls.
This comes from relating $ \ls_{\cD}(\cA'(\rS;T))^{0.49} $ to one of its previous iterates erring on a $\nicefrac{49}{100}$-fraction of its classifiers, and one hypothesis from the remaining iterates also erring. 
Recall that to get this bound we first observe that, with probability at least $ \nicefrac{1}{3}$ over a randomly drawn index $ \rI\sim\left\{  1,2,3\right\} ,$ $ \avg(\cA'(\rS_{\rI,\sqcup};\rS_{\rI,\sqcap}\sqcap T))(\rx,\ry) \geq 0.49$ and a random hypothesis $\rh\in \sqcup_{j\in\{  1,2,3\}\backslash \rI } \cA'(\rS_{j,\sqcup};\rS_{j,\sqcap}\sqcup T),$ would, with probability at least  $ \nicefrac{3}{2}(\nicefrac{49}{100}-\nicefrac{1}{3}),$ make an error . Thus, we get the inequality in \cref{eq:proofsketch9}, which multiplies $ \tau $ by $ \approx 13.$
To decrease this constant we first make the following observation: assume that we have two voting classifiers $ \cA'(\rS_{1};\emptyset),  \cA'(\rS_{2};\emptyset) $ and an $ (x,y) $ such that $ \avg( \cA'(\rS_{1};\emptyset))(x,y)\geq \nicefrac{232}{243} $ and $ \avg( \cA'(\rS_{2};\emptyset))(x,y)\geq \nicefrac{232}{243}$. Similar to the previous analysis, we want to relate the event of both of them having more than $\nicefrac{232}{243}  $ errors to one of the voting classifiers' voters erring and the other voting classifier, on average, erring on  a $ \nicefrac{232}{243} $-fraction of its voters. 
Using a similar analysis as
in \cref{eq:proofsketch9}, we have 
\begin{align}
 \p_{(\rx,\ry)}\big[\avg( & \cA'(\rS_{1};\emptyset))(\rx,\ry)\geq \nicefrac{232}{243}, \avg( \cA'(\rS_{2};\emptyset))(\rx,\ry)\geq \nicefrac{232}{243}\big]\label{eq:proofsketch12}
 \\
 &\leq \frac{243}{232}\max_{h\in \cA'(\rS_{1};\emptyset)}\p_{(\rx,\ry)}\left[h(\rx)\not=\ry, \avg( \cA'(\rS_{2};\emptyset))(\rx,\ry)\geq \nicefrac{232}{243}\right].\label{eq:proofsketch13}
\end{align}  
Following a similar approach to that used to obtain \cref{eq:proofsketch4}, we can bound \cref{eq:proofsketch13} by
$1.1\tau+ O(\sqrt{\tau(d+\ln{\left(1/\delta \right)})/m}+(d+\ln{\left(1/\delta \right)})/m)$. 
Thus, if we could come up with a way of bounding the error using the event in \cref{eq:proofsketch12} we could obtain a better bound for the overall error of our algorithm.

To this end, we consider a third independent training sequence $ \rS_{3},$ and let $ \rS_{3}^{\not=} $ be the training examples $ (x,y) $ in $ \rS_{3} $ such that $ \avg( \cA'(\rS_{1};\emptyset))(x,y)\geq \nicefrac{11}{243} $ or $ \avg( \cA'(\rS_{2};\emptyset))(x,y)\geq \nicefrac{11}{243}$ and  $ \htie=\erm(\rS_{3}^{\not=}),$ i.e., the output of an $ \cerm $ on $ \rS_{3}^{\not=}.$ Next, we introduce our tie-breaking idea. For a point $ x $ we let $ \tie^{\nicefrac{11}{243}}\left(\cA'(\rS_{1};\emptyset),\cA'(\rS_{1};\emptyset); \htie\right)(x) = y \in \{-1,1\}$, where $y$ is the (unique) number
for which  $ \sum_{h\in \cA'(\rS_{1};\emptyset)}\ind\{h(x)=y\}/|\cA'(\rS_{1};\emptyset)|\geq \nicefrac{232}{243}$ and $\sum_{h\in \cA'(\rS_{2};\emptyset)}\ind\{h(x)=y\}/|\cA'(\rS_{2};\emptyset)|\geq \nicefrac{232}{243},$ or, if such a number does not exist,  $y = \htie(x).$ Thus, this classifier errs on $ (x,y) $ if 1) $ \avg( \cA'(\rS_{1};\emptyset))(x,y)\geq \nicefrac{232}{243} $ and $ \avg( \cA'(\rS_{2};\emptyset))(x,y)\geq \nicefrac{232}{243}$  or 2) for all $y'\in\left\{  -1,1\right\}$,  $\sum_{h\in \cA'(\rS_{1};\emptyset)}\ind\{h(x)=y\}/|\cA'(\rS_{1};\emptyset)|< \nicefrac{232}{243}$ or $\sum_{h\in \cA'(\rS_{2};\emptyset)}\ind\{h(x)=y\}/|\cA'(\rS_{2};\emptyset)|< \nicefrac{232}{243} $ and $ \htie(x)\not=y.$ We notice that 2) implies the event  $\avg( \cA'(\rS_{1};\emptyset))(x,y)\geq \nicefrac{11}{243} $ or $ \avg( \cA'(\rS_{2};\emptyset))(x,y)\geq \nicefrac{11}{243} \text{ and } \htie(x)\not=y.$ Thus,  we get that 
\begin{align}
\ls_{\cD} & (\tie^{\nicefrac{11}{243}}\left(\cA'(\rS_{1};\emptyset),\cA'(\rS_{1};\emptyset); \htie\right))\nonumber
\\
& \leq  
 \p_{(\rx,\ry)\sim \cD}\left[\avg( \cA'(\rS_{1};\emptyset))(\rx,\ry)\geq \nicefrac{232}{243}, \avg( \cA'(\rS_{2};\emptyset))(\rx,\ry)\geq \nicefrac{232}{243}\right]\nonumber
 \\
& +  \p_{(\rx,\ry)\sim \cD}\left[ \avg( \cA'(\rS_{1};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}\text{ or } \avg( \cA'(\rS_{2};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243} \text{ and } \htie(\rx)\not=\ry\right].\nonumber
\end{align} 
As observed below \cref{eq:proofsketch12}, the first term  can be bounded by $ 1.1\tau+ O(\sqrt{\tau(d+\ln{\left(1/\delta \right)})/m}+(d+\ln{\left(1/\delta \right)})/m)$. Thus, if we could bound the second term by  $\tau+ O(\sqrt{\tau(d+\ln{\left(1/\delta \right)})/m}+(d+\ln{\left(1/\delta \right)})/m)$, we would be done. However, we will soon see that we have to make a change to \cref{alg:splittingwith3} in order to bound this term, which leads us to \cref{alg:splittingwith27}. Proceeding as in \cref{eq:proofsketch1}, let $ \cD_{\not=} $ be the conditional distribution given $ \avg( \cA'(\rS_{1};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}$ or $ \avg( \cA'(\rS_{2};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}.$
Then,
\begin{align*}
& \hspace{-0.5 cm} \p_{(\rx,\ry)\sim \cD}\left[ \avg( \cA'(\rS_{1};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}\text{ or } \avg( \cA'(\rS_{2};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243} \text{ and } \htie(\rx)\not=\ry\right]
    \\
& \hspace{-0.5 cm} = 
    \p_{(\rx,\ry)\sim \cD_{\not=}}\left[ \htie (\rx)\not=\ry\right]
    \p_{(\rx,\ry)\sim \cD}\left[ \avg( \cA'(\rS_{1};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}\text{ or } \avg( \cA'(\rS_{2};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}\right].
\end{align*}
We notice that $\htie$ is the output of an $ \cerm $-algorithm trained on the training sequence $ S_{3}^{\not=} $ distributed according to $ \cD_{\not=},$ thus, by the $ \cerm $-theorem \Cref{thm:ermtheoremunderstanding}, with high probability it holds 
\begin{align}
    \p_{(\rx,\ry)\sim \cD_{\not=}}\left[ \htie(\rx)\not=\ry\right]\leq \min_{h\in \cH}\ls_{\cD_{\not=}}(h)+\sqrt{\frac{d+\ln{\left(1/\delta \right)}}{|S_{3}^{\not=}|}}\leq \ls_{\cD_{\not=}}(\hs)+\sqrt{\frac{d+\ln{\left(1/\delta \right)}}{|S_{3}^{\not=}|}} \,,\nonumber
\end{align}
where the second inequality follows from $\hs\in\cH$. Thus, we get that
\begin{align*}
&  \hspace{-0.5 cm} \p_{(\rx,\ry)\sim \cD}\left[ \avg( \cA'(\rS_{1};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}\text{ or } \avg( \cA'(\rS_{2};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}\text{ and } \htie(\rx)\not=\ry\right] \\
& \hspace{-0.5 cm} \leq \tau+\sqrt{\frac{\left(d+\ln{\left(1/\delta \right)}\right)\p_{\rx,\ry\sim \cD}\left[\avg( \cA'(\rS_{1};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}\text{ or } \avg( \cA'(\rS_{2};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}\right]^{2}}{|\rS_{3}^{\not=}|}}
\end{align*}
where the inequality uses the definition of the conditional distribution $ \cD_{\not=} $ and monotonicity of measures. Now, using a similar argument as in \cref{eq:proofsketch2}, a Chernoff bound shows that $ |\rS_{3}^{\not=}| =m\p_{\rx,\ry\sim \cD}\left[\avg( \cA'(\rS_{1};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}\text{ or } \avg( \cA'(\rS_{2};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}\right]$ with high probability. Thus, if $ \p_{\rx,\ry\sim \cD}\left[\avg( \cA'(\rS_{1};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}\text{ or }\avg( \cA'(\rS_{2};\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}\right] $ is at most $ c(\tau +(d+\ln{\left(1/\delta \right)})/m)$, the above argument would be done. However, the previous analysis in \cref{sec:large-constant-sketch} and the union bound only give that
\begin{align*}
&\p_{\rx,\ry\sim \cD}\left[\avg( \cA'(\rS_{1};\emptyset))(\rx,\ry)\geq \nicefrac{49}{100}\text{ or } \avg( \cA'(\rS_{2};\emptyset))(\rx,\ry)\geq \nicefrac{49}{100}\right]  \\
& \qquad \qquad \leq 2(15\tau +O(\sqrt{\tau(d+\ln{\left(1/\delta \right)})/m}+(d+\ln{\left(1/\delta \right)})/m)) \,,
 \end{align*}
 which is not sufficient.


\phantomsection
\addcontentsline{toc}{subsubsection}{
An approach with more than $ 3 $ splits
}
 
\paragraph{An approach with more than $ 3 $ splits:}
Thus, we have observed that the above argument requires showing $ \p\left[\avg( \cA'(\rS;\emptyset))(\rx,\ry)\geq \nicefrac{11}{243}\right]\leq c(\tau +(d+\ln{\left(1/\delta \right)})/m),$ with high probability.
However, trying to show $ \p\left[\avg( \cA'(\rS;T))(\rx,\ry)\geq \nicefrac{11}{243}\right]\leq c(\tau +(d+\ln{\left(1/\delta \right)})/m)$ by induction, as in \cref{sec:large-constant-sketch}, breaks down in the step where we derived \cref{eq:proofsketch9}. Recall this is where  we  relate $\avg( \cA'(\rS;T))(x,y)\geq \nicefrac{11}{243} $ to a previous recursion call also erring on an $ \nicefrac{11}{243} $-fraction of its voters and one of the hypotheses in the remaining recursive calls erring on $ (x,y) $. 
To see why this argument fails, now consider $ (x,y) $ such that $\avg( \cA'(\rS;T))(x,y)\geq \nicefrac{11}{243}.$ Picking an index $ \rI\sim \left\{  1,2,3\right\}  $ with probability at least $ \nicefrac{1}{3} $ still returns  $\avg( \cA'(\rS_{\rI,\sqcup};T\sqcup \rS_{\rI,\sqcap}))(x,y)\geq \nicefrac{11}{243}.$ However, when picking a random hypothesis $\rh\in \sqcup_{j\in\{  1,2,3\}\backslash \rI } \cA'(\rS_{1,j,\sqcup};\rS_{j,\sqcap}\sqcup T),$ the probability of $ \rh $ erring can only be lower bounded by $ \nicefrac{3}{2}(\nicefrac{11}{243}-\nicefrac{1}{3}),$ which is negative!
Thus, we cannot guarantee a lower bound on this probability when making only $ 3 $ recursive calls in \cref{alg:splittingwith3}.
However if we make more recursive calls, e.g., $ 27 $, we get that with probability at least $ 1/27 $ over $ \rI\sim \left\{  1,\ldots,27\right\}  $, it holds that $ \cA(\rS_{\rI,\sqcup};\rS_{\rI,\sqcap}\sqcup T) \geq \nicefrac{11}{243}$ and with probability at least $ 27/26(11/243-1/27)\approx0.009 $ over  $\rh\in \sqcup_{j\in\{  1,\ldots,27\}\backslash \rI } \cA(\rS_{1,j,\sqcup};\rS_{j,\sqcap}\sqcup T),$ we have that $\rh(x)\not=y.$ By $1/0.009 \leq $, this gives us 
\begin{align*}
    \ls_{\cD}^{\frac{11}{243}} (\erm(S;T))
    \leq 112\negmedspace\negmedspace\negmedspace\negmedspace\negmedspace \max_{\stackrel{i,j\in \{1,\ldots,27  \}, i\not=j}{h'\in \erm(S_{j,\sqcup};S_{j,\sqcap}\sqcup T)}}\p_{(\rx,\ry)\sim\cD}\left[h'(\rx)\not=\ry,\avg(\erm(S_{i,\sqcup}; S_{i,\sqcap}\sqcup T))(\rx,\ry) \geq \frac{11}{243}\right].
\end{align*}
This is precisely the scheme we propose in \cref{alg:splittingwith27}, with $\cA(\rS;T)=\{\cA(S')\}_{S'\in \cS(\rS;T)}$. Now defining $ \widehat{\cA}_{\rt}(\rS) $ as $t$ voters drawn from $\cA(\rS;\emptyset)$  with $t=\Theta(\ln{\left(m/(\delta(d+\ln{\left(1/\delta \right)})) \right)}),$ and following the analysis of \cref{sec:large-constant-sketch}, relating the error of $ \widehat{\cA}_{\rt}(\rS) $ to $ \cA(\rS;T) $  gives $ \ls_{\cD}^{\nicefrac{11}{243}}(\widehat{\cA}_{\rt})=c(\tau+(d+\ln{\left(1/\delta \right)}))/m $. Finally, roughly following the above arguments for $\ls_{\cD}(\tie^{\nicefrac{11}{243}}(\cA_{\rt_{1}}'(\rS_{1}),\cA'_{\rt_{2}}(\rS_{2}); \htie )),$ but now with $\ls_{\cD}(\tie^{\nicefrac{11}{243}}(\widehat{\cA}_{\rt_{1}}(\rS_{1}),\widehat{\cA}_{\rt_{2}}(\rS_{2}); \htie )),$ we obtain, for the latter, the claimed generalization error of $ 2.1\tau+O(\sqrt{\tau(d+\ln{\left(1/\delta \right)})/m}+(d+\ln{\left(1/\delta \right)})/m).$ 

\vspace{-0.4 cm}
\section{Conclusion}
In this work we have studied the fundamental problem of agnostic PAC learning and have provided
improved sample complexity bounds that are parametrized by the error of the best in class hypothesis.
Our results resolve the question of \citet{hanneke2024revisiting} who asked for optimal
learners in the regime $\tau \approx \nicefrac{d}{m}$ and make progress in their questions
that asked for optimal learners in the whole range of $\tau,$ and efficient learners
that are based on majority votes of $\cerm$s. The most interesting question that follows from our work
is whether a different analysis of our voting scheme or a modification of it
can lead to optimal algorithms for the whole range of $\tau.$
\vspace{-0.4 cm}
