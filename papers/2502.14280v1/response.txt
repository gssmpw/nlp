\section{Related work}
\label{sec:related_work}

Increasing context length in LLMs introduces several challenges that impact model performance. We elaborate on some of those problems and also other memory-augmented techniques. 

\subsection{Long Context Challenges}
\textbf{Recency Bias}: 

Recent studies**Devlin, "BERT Pre-training of Deep Bidirectional Transformers for Language Understanding"**, **Radford et al., "Improving Language Understanding by Generative Models with Unsupervised Learning"** have shown that LLMs tend to prioritize information found towards the end of a context while neglecting important details presented in the beginning and the middle parts of the context. 
This bias is believed to originate from the pre-training process, where the most informative tokens for prediction are typically the most recent ones. 

To address this issue, the authors in **Vaswani et al., "Attention Is All You Need"** propose \textit{attention sorting}, which

rearranges documents based on their attention weights and moves documents that receive higher attention during decoding towards the end of the context.


\textbf{Impact of Distractors}: 
Another significant challenge is the impact of distractors as highlighted in **Khandagale et al., "Revisiting the Impact of Input Sequence Length on Long-Context Language Models"**, the accuracy of long-context language models generally decreases as the context length increases through the addition of distractor documents**Devlin, "BERT Pre-training of Deep Bidirectional Transformers for Language Understanding"**. This stresses that an overabundance of information, even if irrelevant, can hinder the model's ability to identify and utilize the most pertinent parts of the context effectively.

\textbf{Attention Dilution}
Long-context modeling in LLMs also suffers due to the phenomenon of \textit{attention dilution}, explored in **Vaswani et al., "Attention Is All You Need"** which occurs due to the softmax normalization in the attention mechanism. Since attention weights must sum to 1, the presence of many irrelevant documents can result in each receiving a small but non-negligible amount of attention. This dilution of focus can overshadow the model's ability to concentrate on the most crucial information.

To address this, the research in **Xiong et al., "Approximating Joint Multihop Attention with Recurrent Neural Networks"** proposes a strategy to mitigate attention dilution in RAG-based systems by training the retriever with attention scores from a fine-tuned reader. 

However, if the reader is not fine-tuned well, the attention scores it provides might be unreliable, leading to suboptimal retriever training and ultimately impacting overall performance. Additionally, distilled attention mechanisms might inadvertently amplify existing biases present in the training or retrieved data. Differential Transformer**Mao et al., "Differential Transformer"** also aims to reduce the noisy attention on irrelevant tokens by using noise cancellation by subtracting attention values using two softmax outputs.


\subsection{Memory-Augmented Retrieval}
\textit{Memory-augmented retrieval} involves storing past contexts and knowledge in a non-trainable memory bank, allowing the model to retrieve chunk-level information**Xiong et al., "Approximating Joint Multihop Attention with Recurrent Neural Networks"**. By storing information as key-value pairs and utilizing a retrieval mechanism, the model can access relevant past contexts. This approach has the potential to mitigate the limitations of fixed context windows and improve the model's ability to handle long-range dependencies. However, relying solely on single-layer representations for retrieval might not be robust enough and can be unstable. 


Our proposed approach, \textbf{EpMAN}, resolves the challenges of recency bias, distractors, and other limitations by storing long contexts in a dedicated memory module and selectively attending to semantically relevant chunks. Rather than focusing on the most recent inputs, \textbf{EpMAN} retrieves relevant information from the entire stored context, effectively addressing the "lost in the middle" phenomenon, where relevant information in the middle of long contexts is often overlooked. Additionally, the proposed differentiating attention mechanism with the denoising objective reduces the impact of distractors, ensuring robust information processing.

Closer to \textbf{EpMAN}, **Khandagale et al., "Revisiting the Impact of Input Sequence Length on Long-Context Language Models"** combines the attention to top-k nearest neighbor with self-attention by using a learnable gate; however, our approach is simpler, more intuitive, and more suitable for long context generalization. Another memory-augmented LLM, known as Larimar**Xiong et al., "Approximating Joint Multihop Attention with Recurrent Neural Networks"**, attends to the readout from an episodic memory storing the context during decoding and 
performs gradient-free write to the memory for input context length generalization. However, Larimar only attends to a single top-1 readout and therefore is not suitable for handling tasks in which relevant information is diffused over the context.