@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}
@misc{hsieh2024rulerwhatsrealcontext,
      title={RULER: What's the Real Context Size of Your Long-Context Language Models?}, 
      author={Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Yang Zhang and Boris Ginsburg},
      year={2024},
      eprint={2404.06654},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.06654}, 
}
@article{kuratov2024babilong,
  title={BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack},
  author={Kuratov, Yuri and Bulatov, Aydar and Anokhin, Petr and Rodkin, Ivan and Sorokin, Dmitry and Sorokin, Artyom and Burtsev, Mikhail},
  journal={arXiv preprint arXiv:2406.10149},
  year={2024}
}
@book{kahneman2011thinking,
  added-at = {2013-01-10T15:41:11.000+0100},
  address = {New York},
  author = {Kahneman, Daniel},
  biburl = {https://www.bibsonomy.org/bibtex/2f322864169411fd5914f3fa5488e288c/schmidt2},
  description = {Thinking, Fast and Slow: Amazon.de: Daniel Kahneman: Englische Bücher},
  interhash = {a1400a299a00de009ec8eda73e6289af},
  intrahash = {f322864169411fd5914f3fa5488e288c},
  isbn = {9780374275631 0374275637},
  keywords = {bib books psychology thinking toread},
  publisher = {Farrar, Straus and Giroux},
  refid = {706020998},
  timestamp = {2013-01-10T15:41:11.000+0100},
  title = {Thinking, fast and slow},
  url = {https://www.amazon.de/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/ref=wl_it_dp_o_pdT1_nS_nC?ie=UTF8&colid=151193SNGKJT9&coliid=I3OCESLZCVDFL7},
  year = 2011
}
@misc{child2019generatinglongsequencessparse,
      title={Generating Long Sequences with Sparse Transformers}, 
      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
      year={2019},
      eprint={1904.10509},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1904.10509}, 
}
@misc{beltagy2020longformerlongdocumenttransformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.05150}, 
}
@misc{chen2023extendingcontextwindowlarge,
      title={Extending Context Window of Large Language Models via Positional Interpolation}, 
      author={Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},
      year={2023},
      eprint={2306.15595},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.15595}, 
}
@article{recency_bias1,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{recency_bias2,
  title={Serial position effects of large language models},
  author={Guo, Xiaobo and Vosoughi, Soroush},
  journal={arXiv preprint arXiv:2406.15981},
  year={2024}
}

@article{recency_bias3,
  title={Eliminating position bias of language models: A mechanistic approach},
  author={Wang, Ziqi and Zhang, Hanlin and Li, Xiner and Huang, Kuan-Hao and Han, Chi and Ji, Shuiwang and Kakade, Sham M and Peng, Hao and Ji, Heng},
  journal={arXiv preprint arXiv:2407.01100},
  year={2024}
}

@article{recency_bias4,
  title={Addressing cognitive bias in medical language models},
  author={Schmidgall, Samuel and Harris, Carl and Essien, Ime and Olshvang, Daniel and Rahman, Tawsifur and Kim, Ji Woong and Ziaei, Rojin and Eshraghian, Jason and Abadir, Peter and Chellappa, Rama},
  journal={arXiv preprint arXiv:2402.08113},
  year={2024}
}

@article{attention_sorting,
  title={Attention sorting combats recency bias in long context language models},
  author={Peysakhovich, Alexander and Lerer, Adam},
  journal={arXiv preprint arXiv:2310.01427},
  year={2023}
}

@article{memlong,
  title={MemLong: Memory-Augmented Retrieval for Long Text Modeling},
  author={Liu, Weijie and Tang, Zecheng and Li, Juntao and Chen, Kehai and Zhang, Min},
  journal={arXiv preprint arXiv:2408.16967},
  year={2024}
}

@article{oprag,
  title={In Defense of RAG in the Era of Long-Context Language Models},
  author={Yu, Tan and Xu, Anbang and Akkiraju, Rama},
  journal={arXiv preprint arXiv:2409.01666},
  year={2024}
}

@article{attention_distillation,
  title={Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation},
  author={Li, Zizhong and Zhang, Haopeng and Zhang, Jiawei},
  journal={arXiv preprint arXiv:2402.11794},
  year={2024}
}

@article{attention_dilution1,
  title={Exposing attention glitches with flip-flop language modeling},
  author={Liu, Bingbin and Ash, Jordan and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{attention_dilution2,
  title={Large Language Models aren't all that you need},
  author={Holla, Kiran Voderhobli and Kumar, Chaithanya and Singh, Aryan},
  journal={arXiv preprint arXiv:2401.00698},
  year={2024}
}

@article{attention_dilution3,
  title={Hallucination is inevitable: An innate limitation of large language models},
  author={Xu, Ziwei and Jain, Sanjay and Kankanhalli, Mohan},
  journal={arXiv preprint arXiv:2401.11817},
  year={2024}
}


@article{attention_dilution4,
  title={Selective Prompt Anchoring for Code Generation},
  author={Tian, Yuan and Zhang, Tianyi},
  journal={arXiv preprint arXiv:2408.09121},
  year={2024}
}

@article{order_rag2,
  title={Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG},
  author={Jin, Bowen and Yoon, Jinsung and Han, Jiawei and Arik, Sercan O},
  journal={arXiv preprint arXiv:2410.05983},
  year={2024}
}
@article{distractor2,
  title={Retrieval augmented generation or long-context llms? a comprehensive study and hybrid approach},
  author={Li, Zhuowan and Li, Cheng and Zhang, Mingyang and Mei, Qiaozhu and Bendersky, Michael},
  journal={arXiv preprint arXiv:2407.16833},
  year={2024}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@misc{wu2022memorizingtransformers,
      title={Memorizing Transformers}, 
      author={Yuhuai Wu and Markus N. Rabe and DeLesley Hutchins and Christian Szegedy},
      year={2022},
      eprint={2203.08913},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.08913}, 
}
@article{memllm,
  title={Memllm: Finetuning llms to use an explicit read-write memory},
  author={Modarressi, Ali and K{\"o}ksal, Abdullatif and Imani, Ayyoob and Fayyaz, Mohsen and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2404.11672},
  year={2024}
}

@inproceedings{mem-tree,
  title={From Isolated Conversations to Hierachical Schemas: Dynamic Tree Memory Representation for LLMs},
  author={Rezazadeh, Alireza and Li, Zichao and Wei, Wei and Bao, Yujia},
  booktitle={The First Workshop on System-2 Reasoning at Scale, NeurIPS'24},
  year={2024}
}

@inproceedings{distractor_rag,
  title={The power of noise: Redefining retrieval for rag systems},
  author={Cuconasu, Florin and Trappolini, Giovanni and Siciliano, Federico and Filice, Simone and Campagnano, Cesare and Maarek, Yoelle and Tonellotto, Nicola and Silvestri, Fabrizio},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={719--729},
  year={2024}
}

@article{distract_long_context,
  title={An examination of the continuous distractor task and the “long-term recency effect”},
  author={Koppenaal, Lois and Glanzer, Murray},
  journal={Memory \& Cognition},
  volume={18},
  number={2},
  pages={183--195},
  year={1990},
  publisher={Springer}
}

@inproceedings{attention_is_all_you_need,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{yuan2024lv,
  title={Lv-eval: A balanced long-context benchmark with 5 length levels up to 256k},
  author={Yuan, Tao and Ning, Xuefei and Zhou, Dong and Yang, Zhijie and Li, Shiyao and Zhuang, Minghui and Tan, Zheyue and Yao, Zhuyu and Lin, Dahua and Li, Boxun and others},
  journal={arXiv preprint arXiv:2402.05136},
  year={2024}
}

@article{li2023loogle,
  title={LooGLE: Can Long-Context Language Models Understand Long Contexts?},
  author={Li, Jiaqi and Wang, Mengmeng and Zheng, Zilong and Zhang, Muhan},
  journal={arXiv preprint arXiv:2311.04939},
  year={2023}
}

@article{bai2023longbench,
  title={Longbench: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}

@article{das2024larimar,
  title={Larimar: Large Language Models with Episodic Memory Control},
  author={Das, Payel and Chaudhury, Subhajit and Nelson, Elliot and Melnyk, Igor and Swaminathan, Sarath and Dai, Sihui and Lozano, Aur{\'e}lie and Kollias, Georgios and Chenthamarakshan, Vijil and Dan, Soham and others},
  journal={arXiv preprint arXiv:2403.11901},
  year={2024}
}


@article{needlehaystack_kamradt,
  title={{Needle In A Haystack} - Pressure Testing {LLM}s},
   author={Gregory Kamradt},
   year={2023},
   journal ={Github},  
   url={https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main},
}

@inproceedings{
Rae2020Compressive,
title={Compressive Transformers for Long-Range Sequence Modelling},
author={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SylKikSYDH}
}

@misc{izacard2021unsupervised,
    title={Unsupervised Dense Information Retrieval with Contrastive Learning},
    author={Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},
    year={2021},
    eprint={2112.09118},
    archivePrefix={arXiv},
    primaryClass={cs.IR}
}

@misc{liu2024chatqa,
    title={ChatQA: Surpassing GPT-4 on Conversational QA and RAG},
    author={Zihan Liu and Wei Ping and Rajarshi Roy and Peng Xu and Chankyu Lee and Mohammad Shoeybi and Bryan Catanzaro},
    year={2024},
    eprint={2401.10225},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{lin2023train,
    title={How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval},
    author={Sheng-Chieh Lin and Akari Asai and Minghan Li and Barlas Oguz and Jimmy Lin and Yashar Mehdad and Wen-tau Yih and Xilun Chen},
    year={2023},
    eprint={2302.07452},
    archivePrefix={arXiv},
    primaryClass={cs.IR}
}

@article{yu2024defense,
  title={In Defense of RAG in the Era of Long-Context Language Models},
  author={Yu, Tan and Xu, Anbang and Akkiraju, Rama},
  journal={arXiv preprint arXiv:2409.01666},
  year={2024}
}

@article{lveval,
  title={Lv-eval: A balanced long-context benchmark with 5 length levels up to 256k},
  author={Yuan, Tao and Ning, Xuefei and Zhou, Dong and Yang, Zhijie and Li, Shiyao and Zhuang, Minghui and Tan, Zheyue and Yao, Zhuyu and Lin, Dahua and Li, Boxun and others},
  journal={arXiv preprint arXiv:2402.05136},
  year={2024}
}

@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@article{chen2024bge,
  title={Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation},
  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
  journal={arXiv preprint arXiv:2402.03216},
  year={2024}
}

@article{ye2024differential,
  title={Differential Transformer},
  author={Ye, Tianzhu and Dong, Li and Xia, Yuqing and Sun, Yutao and Zhu, Yi and Huang, Gao and Wei, Furu},
  journal={arXiv preprint arXiv:2410.05258},
  year={2024}
}

@inproceedings{xieadaptive,
  title={Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts},
  author={Xie, Jian and Zhang, Kai and Chen, Jiangjie and Lou, Renze and Su, Yu},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR}
}

