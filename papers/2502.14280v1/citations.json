[
  {
    "index": 0,
    "papers": [
      {
        "key": "recency_bias1",
        "author": "Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy",
        "title": "Lost in the middle: How language models use long contexts"
      },
      {
        "key": "recency_bias2",
        "author": "Guo, Xiaobo and Vosoughi, Soroush",
        "title": "Serial position effects of large language models"
      },
      {
        "key": "recency_bias3",
        "author": "Wang, Ziqi and Zhang, Hanlin and Li, Xiner and Huang, Kuan-Hao and Han, Chi and Ji, Shuiwang and Kakade, Sham M and Peng, Hao and Ji, Heng",
        "title": "Eliminating position bias of language models: A mechanistic approach"
      },
      {
        "key": "recency_bias4",
        "author": "Schmidgall, Samuel and Harris, Carl and Essien, Ime and Olshvang, Daniel and Rahman, Tawsifur and Kim, Ji Woong and Ziaei, Rojin and Eshraghian, Jason and Abadir, Peter and Chellappa, Rama",
        "title": "Addressing cognitive bias in medical language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "attention_sorting",
        "author": "Peysakhovich, Alexander and Lerer, Adam",
        "title": "Attention sorting combats recency bias in long context language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "attention_sorting",
        "author": "Peysakhovich, Alexander and Lerer, Adam",
        "title": "Attention sorting combats recency bias in long context language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "distractor2",
        "author": "Li, Zhuowan and Li, Cheng and Zhang, Mingyang and Mei, Qiaozhu and Bendersky, Michael",
        "title": "Retrieval augmented generation or long-context llms? a comprehensive study and hybrid approach"
      },
      {
        "key": "distractor_rag",
        "author": "Cuconasu, Florin and Trappolini, Giovanni and Siciliano, Federico and Filice, Simone and Campagnano, Cesare and Maarek, Yoelle and Tonellotto, Nicola and Silvestri, Fabrizio",
        "title": "The power of noise: Redefining retrieval for rag systems"
      },
      {
        "key": "distract_long_context",
        "author": "Koppenaal, Lois and Glanzer, Murray",
        "title": "An examination of the continuous distractor task and the \u201clong-term recency effect\u201d"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "attention_dilution1",
        "author": "Liu, Bingbin and Ash, Jordan and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril",
        "title": "Exposing attention glitches with flip-flop language modeling"
      },
      {
        "key": "attention_dilution2",
        "author": "Holla, Kiran Voderhobli and Kumar, Chaithanya and Singh, Aryan",
        "title": "Large Language Models aren't all that you need"
      },
      {
        "key": "attention_dilution3",
        "author": "Xu, Ziwei and Jain, Sanjay and Kankanhalli, Mohan",
        "title": "Hallucination is inevitable: An innate limitation of large language models"
      },
      {
        "key": "attention_dilution4",
        "author": "Tian, Yuan and Zhang, Tianyi",
        "title": "Selective Prompt Anchoring for Code Generation"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "attention_distillation",
        "author": "Li, Zizhong and Zhang, Haopeng and Zhang, Jiawei",
        "title": "Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ye2024differential",
        "author": "Ye, Tianzhu and Dong, Li and Xia, Yuqing and Sun, Yutao and Zhu, Yi and Huang, Gao and Wei, Furu",
        "title": "Differential Transformer"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "memlong",
        "author": "Liu, Weijie and Tang, Zecheng and Li, Juntao and Chen, Kehai and Zhang, Min",
        "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
      },
      {
        "key": "memllm",
        "author": "Modarressi, Ali and K{\\\"o}ksal, Abdullatif and Imani, Ayyoob and Fayyaz, Mohsen and Sch{\\\"u}tze, Hinrich",
        "title": "Memllm: Finetuning llms to use an explicit read-write memory"
      },
      {
        "key": "mem-tree",
        "author": "Rezazadeh, Alireza and Li, Zichao and Wei, Wei and Bao, Yujia",
        "title": "From Isolated Conversations to Hierachical Schemas: Dynamic Tree Memory Representation for LLMs"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wu2022memorizingtransformers",
        "author": "Yuhuai Wu and Markus N. Rabe and DeLesley Hutchins and Christian Szegedy",
        "title": "Memorizing Transformers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "das2024larimar",
        "author": "Das, Payel and Chaudhury, Subhajit and Nelson, Elliot and Melnyk, Igor and Swaminathan, Sarath and Dai, Sihui and Lozano, Aur{\\'e}lie and Kollias, Georgios and Chenthamarakshan, Vijil and Dan, Soham and others",
        "title": "Larimar: Large Language Models with Episodic Memory Control"
      }
    ]
  }
]