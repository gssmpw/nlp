@article{attention_dilution1,
  title={Exposing attention glitches with flip-flop language modeling},
  author={Liu, Bingbin and Ash, Jordan and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{attention_dilution2,
  title={Large Language Models aren't all that you need},
  author={Holla, Kiran Voderhobli and Kumar, Chaithanya and Singh, Aryan},
  journal={arXiv preprint arXiv:2401.00698},
  year={2024}
}

@article{attention_dilution3,
  title={Hallucination is inevitable: An innate limitation of large language models},
  author={Xu, Ziwei and Jain, Sanjay and Kankanhalli, Mohan},
  journal={arXiv preprint arXiv:2401.11817},
  year={2024}
}

@article{attention_dilution4,
  title={Selective Prompt Anchoring for Code Generation},
  author={Tian, Yuan and Zhang, Tianyi},
  journal={arXiv preprint arXiv:2408.09121},
  year={2024}
}

@article{attention_distillation,
  title={Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation},
  author={Li, Zizhong and Zhang, Haopeng and Zhang, Jiawei},
  journal={arXiv preprint arXiv:2402.11794},
  year={2024}
}

@article{attention_sorting,
  title={Attention sorting combats recency bias in long context language models},
  author={Peysakhovich, Alexander and Lerer, Adam},
  journal={arXiv preprint arXiv:2310.01427},
  year={2023}
}

@article{das2024larimar,
  title={Larimar: Large Language Models with Episodic Memory Control},
  author={Das, Payel and Chaudhury, Subhajit and Nelson, Elliot and Melnyk, Igor and Swaminathan, Sarath and Dai, Sihui and Lozano, Aur{\'e}lie and Kollias, Georgios and Chenthamarakshan, Vijil and Dan, Soham and others},
  journal={arXiv preprint arXiv:2403.11901},
  year={2024}
}

@article{distract_long_context,
  title={An examination of the continuous distractor task and the “long-term recency effect”},
  author={Koppenaal, Lois and Glanzer, Murray},
  journal={Memory \& Cognition},
  volume={18},
  number={2},
  pages={183--195},
  year={1990},
  publisher={Springer}
}

@article{distractor2,
  title={Retrieval augmented generation or long-context llms? a comprehensive study and hybrid approach},
  author={Li, Zhuowan and Li, Cheng and Zhang, Mingyang and Mei, Qiaozhu and Bendersky, Michael},
  journal={arXiv preprint arXiv:2407.16833},
  year={2024}
}

@inproceedings{distractor_rag,
  title={The power of noise: Redefining retrieval for rag systems},
  author={Cuconasu, Florin and Trappolini, Giovanni and Siciliano, Federico and Filice, Simone and Campagnano, Cesare and Maarek, Yoelle and Tonellotto, Nicola and Silvestri, Fabrizio},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={719--729},
  year={2024}
}

@inproceedings{mem-tree,
  title={From Isolated Conversations to Hierachical Schemas: Dynamic Tree Memory Representation for LLMs},
  author={Rezazadeh, Alireza and Li, Zichao and Wei, Wei and Bao, Yujia},
  booktitle={The First Workshop on System-2 Reasoning at Scale, NeurIPS'24},
  year={2024}
}

@article{memllm,
  title={Memllm: Finetuning llms to use an explicit read-write memory},
  author={Modarressi, Ali and K{\"o}ksal, Abdullatif and Imani, Ayyoob and Fayyaz, Mohsen and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2404.11672},
  year={2024}
}

@article{memlong,
  title={MemLong: Memory-Augmented Retrieval for Long Text Modeling},
  author={Liu, Weijie and Tang, Zecheng and Li, Juntao and Chen, Kehai and Zhang, Min},
  journal={arXiv preprint arXiv:2408.16967},
  year={2024}
}

@article{recency_bias1,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{recency_bias2,
  title={Serial position effects of large language models},
  author={Guo, Xiaobo and Vosoughi, Soroush},
  journal={arXiv preprint arXiv:2406.15981},
  year={2024}
}

@article{recency_bias3,
  title={Eliminating position bias of language models: A mechanistic approach},
  author={Wang, Ziqi and Zhang, Hanlin and Li, Xiner and Huang, Kuan-Hao and Han, Chi and Ji, Shuiwang and Kakade, Sham M and Peng, Hao and Ji, Heng},
  journal={arXiv preprint arXiv:2407.01100},
  year={2024}
}

@article{recency_bias4,
  title={Addressing cognitive bias in medical language models},
  author={Schmidgall, Samuel and Harris, Carl and Essien, Ime and Olshvang, Daniel and Rahman, Tawsifur and Kim, Ji Woong and Ziaei, Rojin and Eshraghian, Jason and Abadir, Peter and Chellappa, Rama},
  journal={arXiv preprint arXiv:2402.08113},
  year={2024}
}

@misc{wu2022memorizingtransformers,
      title={Memorizing Transformers}, 
      author={Yuhuai Wu and Markus N. Rabe and DeLesley Hutchins and Christian Szegedy},
      year={2022},
      eprint={2203.08913},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.08913}, 
}

@article{ye2024differential,
  title={Differential Transformer},
  author={Ye, Tianzhu and Dong, Li and Xia, Yuqing and Sun, Yutao and Zhu, Yi and Huang, Gao and Wei, Furu},
  journal={arXiv preprint arXiv:2410.05258},
  year={2024}
}

