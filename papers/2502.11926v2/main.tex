% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
%\DeclareUnicodeCharacter{2162}{\textsc{iii}}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
%\usepackage[review]{acl}
\usepackage[table,xcdraw]{xcolor}
\usepackage{acl}
\usepackage{float}
\usepackage{hyperref}
\usepackage{amsmath} % For math environments and symbols
\usepackage{amssymb} % For additional math symbols (optional)
\usepackage{cleveref}
\usepackage{graphicx} % For figures (optional)
\usepackage{natbib} % For citations
\usepackage{booktabs} % For professional-quality tables (\toprule, \midrule, \bottomrule)
\usepackage{array}    % 

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
%\usepackage{csquotes}
\usepackage{booktabs}
 \usepackage{amsmath} 
\usepackage[skins]{tcolorbox} 
\usepackage{enumitem}    
\usepackage{geometry}  

\usepackage{multirow}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{xspace}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand*{\yoruba}{Yor\`ub\'a\xspace}

\definecolor{bblue}{HTML}{4F81BD}
\definecolor{rred}{HTML}{C0504D}
\definecolor{ggreen}{HTML}{9BBB59}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\newcommand{\sham}[1]{\todo[inline,color=green!20!white]{\textbf{Shamsuddeen:} #1}}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
\newcommand{\bulletpoint}[1]{\noindent\textbullet\ #1\\}

% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}

% Remove the "review" option to generate the final version.
%\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{natbib}
\usepackage{tablefootnote}
\usepackage{longtable}
\usepackage{cleveref}
\usepackage{enumitem}
%\usepackage{tikz}
\usepackage{booktabs}
%\usepackage{forest}
%\usetikzlibrary{trees}
\usepackage{pdfpages}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{xspace}
%\PassOptionsToPackage{table,xcdraw}{xcolor}
%\usepackage[colorinlistoftodos]{todonotes}

%\newcommand{\cmark}{\ding{51}}
%\newcommand{\xmark}{\ding{55}}

\usepackage{linguex}
\alignSubExtrue

\newcommand{\unk}{\texttt{<unk>}}
\newcommand\datasetname{\textcolor{black}{\textsc{Brighter}}}

%\usepackage[T4,T1]{fontenc}
\usepackage[verbose]{newunicodechar}



% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{times}
\usepackage{latexsym}
\usepackage{microtype}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{natbib}
\usepackage{tablefootnote}
\usepackage{longtable}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{pdfpages}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{xspace}
%\usepackage[colorinlistoftodos]{todonotes}

%\newcommand{\cmark}{\ding{51}}
%\newcommand{\xmark}{\ding{55}}
%\usepackage{color, colortbl}

\usepackage[textsize=footnotesize]{todonotes}
\usepackage{linguex}
%\usepackage{subfigure}
\usepackage{subcaption}
\alignSubExtrue
%\usepackage{caption}




%\usepackage[T4,T1]{fontenc}
\usepackage[verbose]{newunicodechar}

\usepackage{inconsolata}

\usepackage{makecell}
% Box vor visualizing prompts
% Defining a new tcolorbox environment for the AI output display
\global\setlength{\fboxsep}{0pt}

\tcbset{
  aibox/.style={
    width=474.18663pt,
    top=10pt,
    colback=white,
    colframe=black,
    colbacktitle=black,
    enhanced,
    center,
    attach boxed title to top left={yshift=-0.1in,xshift=0.15in},
    boxed title style={boxrule=0pt,colframe=white,},
  }
}
\newtcolorbox{AIbox}[2][]{aibox,title=#2,#1}
\newcommand{\bestmono}{\cellcolor{blue!30}}
\newcommand{\bestcross}{\cellcolor{orange!20}}
\newcommand{\bestmlm}{\cellcolor{blue!30}}
\newcommand{\bestllm}{\cellcolor{orange!30}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{7.8cm}


% If the title and author information does not fit in the area allocated, uncomment the following
%
\setlength\titlebox{11.8cm}
%
% and set <dim> to something 5cm or larger.
\author{
Shamsuddeen Hassan Muhammad$^{1,2}$\thanks{Equal contribution}, Nedjma Ousidhoum$^{3*}$, \\
\textbf{Idris Abdulmumin}$^4$, \textbf{Jan Philip Wahle}$^5$, \textbf{Terry Ruas}$^5$, \textbf{Meriem Beloucif}$^6$, \textbf{Christine de Kock}$^7$, \textbf{Nirmal Surange}$^8$, \\ 
\textbf{Daniela Teodorescu}$^9$, \textbf{Ibrahim Said Ahmad}$^{10}$, \textbf{David Ifeoluwa Adelani}$^{11,12,13}$, \textbf{Alham Fikri Aji}$^{14}$, \\
\textbf{Felermino D. M. A. Ali}$^{15}$, \textbf{Ilseyar Alimova}$^{30}$, \textbf{Vladimir Araujo}$^{16}$, \textbf{Nikolay Babakov}$^{17}$, \textbf{Naomi Baes}$^{7}$,\\
\textbf{Ana-Maria Bucur}$^{18,19}$, 
\textbf{Andiswa Bukula}$^{20}$, 
\textbf{Guanqun Cao}$^{21}$, \textbf{Rodrigo Tufi\~no}$^{22}$, \textbf{Rendi Chevi}$^{14}$,\\ \textbf{Chiamaka Ijeoma Chukwuneke}$^{23}$, \textbf{Alexandra Ciobotaru}$^{18}$, 
\textbf{Daryna Dementieva}$^{24}$, \textbf{Murja Sani Gadanya}$^{2}$, \\
\textbf{Robert Geislinger}$^{25}$, \textbf{Bela Gipp}$^5$, \textbf{Oumaima Hourrane}$^{26}$, \textbf{Oana Ignat}$^{27}$,
\textbf{Falalu Ibrahim Lawan}$^{28}$,\\
\textbf{Rooweither Mabuya}$^{20}$, \textbf{Rahmad Mahendra}$^{29}$, \textbf{Vukosi Marivate}$^4$, \textbf{Andrew Piper}$^{12}$, \textbf{Alexander Panchenko}$^{30,31}$, \\
\textbf{Charles Henrique Porto Ferreira}$^{32}$, \textbf{Vitaly Protasov}$^{31}$, \textbf{Samuel Rutunda}$^{33}$, 
\textbf{Manish Shrivastava}$^{8}$, \\
\textbf{Aura Cristina Udrea}$^{34}$, \textbf{Lilian Diana Awuor Wanzare}$^{35}$, \textbf{Sophie Wu}$^{12}$, 
\textbf{Florian Valentin Wunderlich}$^5$, \\
\textbf{Hanif Muhammad Zhafran}$^{36}$, \textbf{Tianhui Zhang}$^{37}$, \textbf{Yi Zhou}$^3$, \\
\textbf{Saif M. Mohammad}$^{38}$ \\
 \footnotesize{$^{1}$Imperial College London, $^{2}$Bayero University Kano, $^3$Cardiff University, $^{4}$DSFSI, 
 University of Pretoria, $^{5}$University of Göttingen} \\
 \footnotesize{$^{6}$Uppsala University, $^{7}$University of Melbourne, $^{8}$IIIT Hyderabad, $^{9}$University of Alberta, $^{10}$Northeastern University,}\\
  \footnotesize{$^{11}$MILA, $^{12}$McGill University, $^{13}$Canada CIFAR AI Chair,$^{14}$MBZUAI,$^{15}$LIACC, FEUP, University of Porto, $^{16}$Sailplane AI,}\\
   \footnotesize{$^{17}$University of Santiago de Compostela, $^{18}$University of Bucharest, $^{19}$Universitat Politècnica de València, $^{20}$SADiLaR,}\\
    \footnotesize{$^{21}$University of York, $^{22}$Universidad Politécnica Salesiana,$^{23}$Lancaster University, $^{24}$Technical University of Munich,$^{25}$Hamburg University,}\\
     \footnotesize{$^{26}$Al Akhawayn University, $^{27}$Santa Clara University, $^{28}$Kaduna State University, $^{29}$Universitas Indonesia, $^{30}$Skoltech, $^{31}$AIRI, $^{32}$Centro Universitário FEI,}\\
      \footnotesize{$^{33}$Digital Umuganda, $^{34}$National University of Science and Technology Politehnica Bucharest, $^{35}$Maseno University, $^{36}$Institut Teknologi Bandung,}\\
       \footnotesize{$^{37}$University of Liverpool,$^{38}$National Research Council Canada}\\
       \footnotesize{\texttt{Contact: s.muhammad@imperial.ac.uk, OusidhoumN@cardiff.ac.uk}
 }
 }


\title{\datasetname: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages}







%\author{Authors}

\begin{document}
\maketitle
\begin{abstract}
People worldwide use language in subtle and complex ways to express emotions. While emotion recognition -- an umbrella term for several NLP tasks -- significantly impacts different applications in NLP and other fields, most work in the area is focused on high-resource languages. Therefore, this has led to major disparities in research and proposed solutions, especially for low-resource languages that suffer from the lack of high-quality datasets.
In this paper, we present \datasetname~-- a collection of multilabeled emotion-annotated datasets in 28 different languages. \datasetname~covers predominantly low-resource languages from Africa, Asia, Eastern Europe, and Latin America, with instances from various domains annotated by fluent speakers. We describe the data collection and annotation processes and the challenges of building these datasets. Then, we report different experimental results for monolingual and crosslingual multi-label emotion identification, as well as intensity-level emotion recognition. We investigate results with and without using LLMs and analyse the large variability in performance across languages and text domains. We show that \datasetname~datasets are a step towards bridging the gap in text-based emotion recognition and discuss their impact and utility. %The datasets were used in a shared task with over 600 registered participants.
\end{abstract}

\section{Introduction}
\begin{figure}[h]
    \centering
     \scalebox{0.88}{
        \includegraphics[trim=0 0 0 40, clip, width=\linewidth]{figures/LanguageFamily_extended.png}
    }
    
    % \includegraphics[width=0.99\linewidth]{figures/LanguageFamily_extended.png}
    \caption{Languages included in \datasetname~and their language families.} 
    
    
    %--Afro-Asiatic (Algerian Arabic, Moroccan Arabic, Hausa); Austronesian (Indonesia, Javanese, Sundanese); Creole (Nigerian Pidgin); Indo-European (Afrikaans, Brazilian Portuguese, English, German, Hindi, Marathi, Mozambican Portuguese, Romanian, Russian, Spanish, Swedish, Ukrainian); Niger-Congo (Swahili, Yoruba, Xhosa, Zulu, Kinyarwanda, and Igbo); Sino-Tibetan (Mandarin Chinese), Turkic (Tatar).}
    \label{fig:lgge_fams}
\end{figure}

\begin{figure*}[h]
    \centering
    \scalebox{0.8}{
    %{0.95}[0.7]{
    \includegraphics[width=\linewidth]
    {figures/multi-emotions.png}
    }%{figures/dataset_descr.png}
    \caption{\textbf{Examples from the \datasetname~dataset collection} in 6 different languages with their translations and intensity levels. Note that the instances can have one or more labels (e.g., disgust and surprise as shown in the figure.)} %Coverage, sample texts, multi-label emotion classifications, and fine-grained intensity levels.}
    \label{fig:brighter_examples}
\end{figure*}

While emotions are expressed and managed daily, they are complex, nuanced, and sometimes hard to articulate and interpret. That is, people use language in subtle and complex ways to express emotions across languages and cultures \cite{wiebe2005annotating,mohammad-kiritchenko-2018-understanding,mohammad2018semeval} and perceive them subjectively, even within the same culture or social group. 
Emotion recognition is at the core of several NLP applications in healthcare, dialogue systems, computational social science, digital humanities, narrative analysis, and several others \cite{mohammad-etal-2018-semeval,saffar2023textual}. It is an umbrella term for multiple NLP tasks, such as detecting the emotions of the speaker, identifying what emotion a piece of text is conveying, and detecting the emotions evoked in a reader \cite{mohammad2022ethics}. In this paper, we use \textit{emotion recognition} to refer to \textit{perceived} emotions, i.e., what emotion most people think the speaker might have felt given a sentence or a short text snippet uttered by the speaker. 


Most work on emotion recognition has focused on high-resource languages such as English, Spanish, German, and Arabic \cite{strapparava-mihalcea-2007-semeval,seyeditabari2018emotion,chatterjee-etal-2019-semeval,kumar2022discovering}. This is partly due to the unavailability of datasets in under-served languages, which has led to a major research gap in the area, which is particularly noticeable in low-resource languages. That is, despite the linguistic diversity present in different parts of the world, such as Africa and Asia, which are home to more than 4,000 languages\footnote{\url{https://www.ethnologue.com/insights/how-many-languages}}, few emotion recognition resources are available in these languages. To bridge this gap, we introduce \datasetname~-- a collection of manually annotated emotion datasets for 28 languages containing nearly 100,000 instances from diverse data sources: speeches, social media, news, literature, and reviews. The languages belong to 7 language families (see Figure \ref{fig:lgge_fams}) and are predominantly low-resource, mainly spoken in \textbf{Africa}, \textbf{Asia}, \textbf{Eastern Europe}, \textbf{Latin America}, along with mid- to high-resource languages such as English. Each instance in \datasetname~is curated and annotated by fluent speakers based on six emotion classes: \textit{joy, sadness, anger, fear, surprise, disgust, and none}. The instances are multi-labeled and include 4 levels of intensity that vary from 0 to 3 (examples in Figure \ref{fig:brighter_examples}).
We describe the collection, annotation, and quality control steps used to construct \datasetname. We then test various baseline experiments and observe that LLMs still struggle with recognising perceived emotions in text. We further report on the observed discrepancies across languages such as the fact that, for low-resource languages, LLMs perform significantly better when prompted in English.
We make our datasets public\footnote{\url{https://brighter-dataset.github.io}}, which presents an important step towards work on emotion recognition and related tasks as we involve local communities in the collection and annotation. Our insights into language-specific characteristics of emotions in text, nuances, and challenges may enable the creation of more inclusive digital tools.
 %as shown in \Cref{tab:language_list}. 




\section{The \datasetname ~Dataset Collection}
\input{tables/data_info}


\subsection{Data Collection}

As our \datasetname~collection includes 28 different datasets, curated and annotated by fluent speakers, we use different data sources, collection, and annotation strategies depending on 1)\ the availability of the textual data potentially rich in emotions and 2)\ access to annotators. We detail the various choices made when selecting and balancing data sources, annotating the instances, and controlling for data quality in the following section.

\subsubsection{Data Sources}
Choosing suitable data sources is challenging when resources are lacking. Therefore, we typically combine data sources as shown in Table \ref{tab:data_sources}. We present the main textual sources we used to build \datasetname~ in the following.

\paragraph{Social media posts}
We use social media data collected from various platforms, including Reddit (e.g., \texttt{eng}, \texttt{deu}), YouTube (e.g., \texttt{esp}, \texttt{ind}, \texttt{jav}, \texttt{sun}), Twitter (e.g., \texttt{hau}, \texttt{ukr}), and Weibo (e.g., \texttt{chn}). For some languages, we re-annotate existing sentiment datasets for emotions (e.g., the sentiment analysis benchmark AfriSenti \cite{muhammad-etal-2023-afrisenti} for \texttt{ary}, \texttt{hau}, \texttt{kin}; the Twitter dataset by \citet{bobrovnyk2019automated} for \texttt{ukr}; the RED v2 dataset \citep{ciobotaru-etal-2022-red} for \texttt{ron}).


%- Sources of textual data. \\
%- Diversity. \\
\paragraph{Personal narratives, talks, speeches}
Anonymised sentences from personal diary posts are ideal for extracting sentences where the speaker is centering their own emotions as opposed to the emotions of someone else. Hence, we use these in \texttt{eng}, \texttt{deu}, and \texttt{ptbr}, mainly from subreddits such as, e.g., IAmI.

Similarly, the \texttt{afr} dataset includes sentences from speeches and talks which constitute a good source for potentially emotive text.

\paragraph{Literary texts}
We translated the novel \textit{``La Grande Maison''} (The Big House) by Mohammed Dib \footnote{\url{https://en.wikipedia.org/wiki/La_Grande_Maison}} from French to Algerian Arabic and post-processed the translation to generate sentences to be annotated by native speakers. Note that the translator is bilingual and a native Algerian Arabic speaker. 
Such a source is typically rich in emotions as it includes interactions between various characters. Further, Algerian Arabic is mainly spoken due to the Arabic diglossia, which makes this resource valuable since it highly differs from social media datasets in \texttt{arq}.


\paragraph{News data} 
Although we prefer emotionally rich social media data from different platforms, such data is not always available. Therefore, to collect a larger number of instances, we annotate news data and headlines in some African languages (e.g., \texttt{yor}, \texttt{hau}, and \texttt{vmw}).



\paragraph{Human-written and machine generated data}%
We create a dataset from scratch for Hindi (\texttt{hin}) and Marathi (\texttt{mar}). We ask annotators to generate emotive sentences on a given topic (e.g., family). In addition, we automatically translate a small section of the Hindi dataset to Marathi, and native speakers manually fix the translation errors. Finally, we augment both datasets with a few hundred quality-approved instances generated by ChatGPT.

 
\subsubsection{Pre-processing and Quality Control}
Prior to annotation, we preprocess the data by removing duplicates, invisible characters, garbled encoding, and incorrectly rendered emoticons. We anonymise all texts and exclude content with excessive expletives or dehumanising language.


\subsection{Annotating \datasetname}

As a text snippet can elicit multiple emotions simultaneously, we ask the annotators to select all the emotions that apply to a given text rather than choosing a single dominant emotion class.
The set of labels includes six categories of perceived emotions: \textit{anger, sadness, fear, disgust, joy, surprise}, and \textit{neutral (if no emotion is present)}. The annotators further rate the selected emotion(s) on a four-point intensity scale: 0 (no emotion), 1 (low intensity), 2 (moderate intensity level), and 3 (high intensity). We provide the definitions of the categories and annotation guide in \Cref{app:reliability}.

We use Amazon Mechanical Turk to annotate the English dataset, and Toloka to label the Russian, Ukrainian, and Tatar instances. However, as traditional crowdsourcing platforms do not have a large pool of annotators who speak various low-resource languages, we directly recruit fluent speakers to annotate the data and use the academic version of LabelStudio \cite{LabelStudio} and Potato \cite{pei2022potato} to set up our annotation platform.



\subsection{Annotators' Reliability}
%\subsubsection{Annotation Reliability}
While both inter-annotator agreement (IAA) and reliability scores measure the quality of annotations, they address different aspects. That is, IAA evaluates how much the annotators agree with each other, whereas reliability scores focus on the consistency of the aggregated labels across different trials (repeated annotations; \citealp{bws-naacl2016}). Hence, when the final aggregated labels are obtained from a larger number of annotations, reliability scores tend to increase. In contrast, IAA scores do not depend on the number of annotations per instance.
We report the reliability of the annotation using Split-Half Class Match Percentage (SHCMP; \citealp{mohammad-2024-worrywords}). SHCMP extends the concept of Split-Half Reliability (SHR), traditionally used for continuous scores \citep{bws-naacl2016}, to discrete categories like ours (i.e., intensity scores per emotion). 
SHCMP measures the extent to which $n$ bins (i.e., subsets corresponding to halves when $n=2$) of the annotations classify items in the same way by splitting a dataset with individual labels into $n$ random bins, and computing how many times each item in each bin is assigned the same class or category. This calculation is repeated 1,000 times and the average corresponds to the final SHCMP score. 
That is, a higher SHCMP indicates that repeated annotations would produce similar class labels (i.e., higher reliability). Further explanations can be found in Appendix \ref{app:reliability}.
\Cref{fig:shcmp_heatmap} shows a heatmap presenting the SHCMP values for the \datasetname~ datasets.
Overall, the SHCMP scores are high ($>60\%$ for $n=2$), which indicates that our annotations are reliable.

%\Cref{tab:shcmp-english} presents the SHCMP values for English across different bin sizes, compared to %The second column lists the class sizes, the third column provides 
%the expected SHCMP when scores are assigned randomly%, and the fourth column reports the SHCMP obtained using the actual emotion scores
%. Overall, SHCMP scores are high, which indicates that our annotations are of good quality.

%\input{tables/shcmp_english}


\begin{figure}[t!]
    \centering
    %\includegraphics[width=\linewidth, clip, trim=40 125 45 56]{figures/heatmap.pdf}
    \includegraphics[width=\linewidth, trim={0.5cm 0 0 0}]{figures/shcmp.png}
    \caption{SHCMP (\%) values for the \datasetname~ datasets across varying numbers of bins (2 to 10). Higher values indicate better reliability scores. Note that \texttt{ptmz} and \texttt{vwm} have the same score as \texttt{vwm} instances were translated from \texttt{ptmz} and the translation was verified.}
    \label{fig:shcmp_heatmap}
\end{figure}


% \begin{table*}[]
%     \centering
%     \footnotesize
%     \begin{tabular}{lrrrrrrlrrrrrr}
% \toprule
% \multirow{2}{*}{lang} & \multicolumn{6}{c}{bins} & \multirow{2}{*}{lang} & \multicolumn{6}{c}{bins} \\ \cmidrule{2-7}\cmidrule{9-14}
%  & 2 & 3 & 4 & 5 & 7 & 10 & & 2 & 3 & 4 & 5 & 7 & 10 \\\midrule
% afr & 66.7 & 66.7 & 66.7 & 66.7 & 57.2 & 57.2 & mar & 94.3 & 93.3 & 90.0 & 90.0 & 88.5 & 88.5 \\
% arq & 83.3 & 76.4 & 74.4 & 74.4 & 66.3 & 66.3 & pcm & 63.5 & 56.4 & 55.1 & 55.1 & 47.0 & 47.0 \\
% ary & 62.6 & 57.9 & 56.8 & 56.8 & 53.2 & 53.3 & ptbr & 86.1 & 81.7 & 80.3 & 80.3 & 73.3 & 73.3 \\
% chn & 77.8 & 72.5 & 71.4 & 71.4 & 65.5 & 65.5 & ptmz & 100.0 & 100.0 & 100.0 & 100.0 & 90.5 & 90.5 \\
% deu & 84.9 & 77.2 & 75.7 & 75.7 & 69.8 & 69.8 & ron & 77.6 & 72.1 & 68.7 & 68.7 & 59.5 & 59.5 \\
% eng & 90.1 & 84.3 & 82.0 & 82.0 & 69.2 & 69.2 & sun & 97.6 & 92.6 & 92.6 & 92.6 & 86.5 & 86.5 \\
% esp & 92.3 & 87.5 & 86.6 & 86.6 & 83.6 & 83.6 & swe & 64.7 & 59.5 & 58.5 & 58.5 & 52.3 & 52.3 \\
% hau & 74.3 & 71.3 & 68.9 & 68.9 & 66.6 & 66.6 & tat & 99.2 & 99.2 & 99.2 & 99.2 & 90.6 & 90.6 \\
% hin & 98.0 & 93.9 & 92.3 & 92.3 & 90.7 & 90.7 & ukr & 66.0 & 63.2 & 63.0 & 63.0 & 59.5 & 59.5 \\
% igb & 61.3 & 58.4 & 56.6 & 56.6 & 54.7 & 54.7 & vmw & 100.0 & 100.0 & 100.0 & 100.0 & 90.5 & 90.5 \\
% ind & 98.1 & 93.1 & 93.1 & 93.1 & 85.7 & 85.7 & xho & 62.1 & 59.2 & 57.1 & 57.1 & 55.1 & 55.1 \\
% jav & 95.2 & 88.1 & 88.1 & 88.1 & 79.8 & 79.8 & yor & 62.5 & 60.2 & 59.1 & 59.2 & 55.6 & 55.6 \\
% kin & 63.2 & 61.4 & 59.2 & 59.2 & 58.9 & 58.9 & zul & 63.7 & 61.2 & 60.3 & 60.4 & 58.0 & 58.0 \\
% \bottomrule
% \end{tabular}
%     \caption{Caption}
%     \label{tab:schmp}
% \end{table*}


\subsection{Determining the Final Labels}

We expected a level of disagreement as emotions are complex, subtle, and perceived differently even from people within the same culture. In addition, text-based communication is limited as it lacks cues such as tone, relevant context, and information about the speaker.  
Our approach for aggregating the per-annotator emotion and intensity labels is detailed below. We also publicly share the individual (non-aggregated) annotations, recognising that annotator disagreement can provide useful signals in itself \citep{plank2022problem}. 

\paragraph{Aggregating the Emotion Labels}

The final emotion labels are determined based on the emotions and associated intensity values selected by the annotators. That is, the given emotion is considered present if: 
 \begin{enumerate}[noitemsep,nolistsep] 
    \item At least two annotators select a label with an intensity value of 1, 2, or 3 (low, medium, or high, respectively).
    \item The average score exceeds a predefined threshold \(T\). We set \(T\) to \(0.5\).
\end{enumerate}




\paragraph{Aggregating the Intensity Labels} 

Once the labels for perceived emotions are assigned, we determine the final intensity score for each instance by averaging the selected intensity scores and choosing the ceiling. We only assign intensity scores for datasets where most instances are annotated by $\geq 5$ annotators to ensure robustness.




%  \tcbset{colback=gray!6, colframe=black, sharp corners, width=\linewidth, boxrule=0.2mm}

% \begin{tcolorbox}
% \[
% \text{AvgScore} = \frac{\sum_{i=1}^N A_i}{N},
% \]

% \[
% L_{\text{final}}=
% % C_{\text{intensity}} =
% \begin{cases} 
% 0, & \text{if } 0 \leq \text{AvgScore} < 1, \\
% 1, & \text{if } 1 \leq \text{AvgScore} < 2, \\
% 2, & \text{if } 2 \leq \text{AvgScore} < 3, \\
% 3, & \text{if } \text{AvgScore} = 3.
% \end{cases}
% \]

% \end{tcolorbox}


% Where:
% \begin{itemize}
%     \item \(A_i\) is the intensity score provided by annotator \(i\), where \(A_i \in \{0, 1, 2, 3\}\).
%     \item \(N\) is the total number of annotators, where \(N \geq 5\)
% \end{itemize}


%This ensures thats the intensity of the emotion is significant enough to be considered present. 



\subsection{Final Data Statistics}


\begin{figure*}[h]
    \centering
    \includegraphics[width=1.05\linewidth]{figures/distribution_v5.png}
    \caption{Emotion label distribution across \datasetname~datasets. Each bar represents the number of labeled instances per emotion (i.e., anger, disgust, fear, joy, sadness, surprise, and neutral) and its percentage.}
    \label{fig:distr-label}
\end{figure*}



\Cref{fig:distr-label} shows the distribution of the annotated emotions in the \datasetname~datasets. The neutral class contains instances that do not belong to any of the six predefined categories (i.e., anger, disgust, fear, sadness, joy, and surprise). Although most languages include all six categories, the English dataset does not include disgust, and the Afrikaans one does not include surprise due to an insufficient class representation. Furthermore, class distributions show substantial variation as we chose various data sources as shown in Table \ref{tab:data_sources}.%, 


\section{Experiments}\label{sec:experiments}
% \input{tables/class_dist}

% \input{tables/data_split}


% \input{tables/class_dist}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/single_vs_multi_label_distribution.pdf}
%     \caption{}
%     \label{fig:single-multilabel}
% \end{figure}

\subsection{Setup}

\input{tables/mono_cross_lingual_result}
%comment

%\subsection{Experiments}

%\paragraph{Splits}
We report the data split sizes in \Cref{tab:data_sources}. The test sets are large, with about 1,000 instances and up to almost 3,000. Datasets with no training data are not used for training.
For our baseline experiments, we test multi-label emotion classification and emotion intensity prediction using Multilingual Language Models (MLMs) and Large Language Models (LLMs) for the following.

\paragraph{Multi-label Emotion Classification in Few-shot Settings} \label{para:few-shot}
We report the emotion classification performance using five LLMs--\textsc{Qwen2.5-72B}~\citep{yang2024qwen2}, \textsc{Dolly-v2-12B}~\citep{DatabricksBlog2023DollyV2}, \textsc{LLaMA-3.3-70B}~\citep{touvron2023llama}, \textsc{Mixtral-8x7B}~\citep{jiang2024mixtral}, and \textsc{DeepSeek-R1-70B}~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability}. We prompt the LLMs to perform Chain-of-Thought (CoT) and predict the presence of each emotion from a predefined set, set the number of few-shot examples to 8, and consider the first answer generated by the LLMs (i.e., top--1).  We report the macro F1-score results on 28 languages. In ~\Cref{app:multilingual}, we also report monolingual classification results for all 24 languages with training datasets in \Cref{tab:monolingual_result_mlm}.

%//add comments after highlighting the results

\paragraph{Multi-label Emotion Classification in Crosslingual Settings}
We report the macro F-score results for systems trained without using any data in the 28 target languages when testing on each. Hence, we train MLMs on all languages in one family (see \Cref{fig:lgge_fams}) except for one held-out target language, which we test on and report the results for each test set. For families with only one language, we train on Slavic languages (\texttt{rus} and \texttt{ukr}) and test on \texttt{tat}; two Niger-Congo languages (\texttt{swa} and \texttt{yor}) and test on \texttt{pcm}; and on \texttt{rus} and test on \texttt{chn}. 


\paragraph{Emotion Intensity Prediction}
 We report the Pearson correlation scores for systems trained on the intensity-labeled training sets in 10 languages.



 %We prompt the LLMs (Qwen2.5-72B, Dolly-v2-12B, Llama-3.3-70B, Mixtral-8x7B, and DeepSeek-R1-70B ) to analyse the text and predict the intensity of a given emotion in the predefined set. We set the few-shot examples to 8 and only account for the first answer generated by the LLM (i.e., top--1). We report the main LLM experimental in \Cref{tab:main_results_track_b_lms}. 

%\section{Results}


\input{tables/merged_intensity_classification}

% \input{tables/mono_cross_lingual_result}


% \input{tables/track_a_lms}

% % \input{tables/track_a_lms}
% \input{tables/track_a_llms}


% \input{tables/track_c_lms} 


% \input{tables/track_b_llms}

% \input{tables/track_c_lms}
\subsection{Experimental Results}

%//Notes to be edited after highlighting results

\Cref{tab:mono_cross_lingual_result} reports the results of few-shot and cross-lingual experiments for multi-label emotion classification and 
 \Cref{tab:merged_results_track_b} reports those for emotion intensity classification. Our results corroborate how challenging emotion classification is for LLMs, even for high-resource languages such as \texttt{eng} and \texttt{deu}. The performance is worse for low-resource languages, for which Dolly-v2-12B performs the worst, and Qwen2.5-72B performs the best on average.
 
We observe the largest performance for \texttt{yor} with a maximum of 27.44. \texttt{hin}, \texttt{mar}, and \texttt{tat} have the best performance among all languages, which is unsurprising since the \texttt{tat} dataset is single-labeled, and close to 70\% and 80\% of the test data for \texttt{mar} and \texttt{hin} respectively are single-labeled. 
 %their dataset was single labeled, making it easier for LLMs.   %//state which models performs the best on average [\textbf{important}]. 
%The other outliers are mar, hin because the datasets are mainly single-labeled and due to the nature of the data.
\paragraph{Multi-label Emotion Recognition Results} 
The crosslingual experiments show that the model performance depends on both the languages used for the transfer learning and those used for pretraining the LLM. For instance, in some languages, training on other languages from the same family boosts the performance and outperforms the few-shots settings (e.g., \texttt{swe} when RemBERT is fine-tuned on Germanic languages). However, all the Niger-Congo languages (\texttt{vmw} in particular) are those that benefited the least from the crosslingual transfer across all models, with RemBERT performing the worst. This is largely due to their under-resourcedness even when combining data. 
Notably, XLM-R performs exceptionally well in languages such as \texttt{deu}, \texttt{chn}, \texttt{hin}, \texttt{ptbr}, but struggles significantly in others (e.g., \texttt{swe}, \texttt{ptmz}).
In contrast, mDeBERTa's results are the most stable across most languages with low scores for \texttt{ibo}, \texttt{vmw}, and \texttt{yor} which are not part of the CC-100 corpus \cite{conneau-etal-2020-unsupervised} used for training mDeBERTa. One would also argue that mDeBERTa was also not trained on \texttt{arq} but the Modern Standard Arabic (MSA) data used for training the model helped boost the performance. %Our results show that multilingual models transfer better to models they have seen before and output a random emotion when they have not seen the language in their pretraining. 
\paragraph{Emotion Intensity} 
For the intensity detection, which is more challenging, Dolly-v2-12B's results are worse whereas DeepSeek-R1-70B shows promising results by outperforming other models in most languages. 
%Llama-3.3-70B and Qwen2.5-72B have the highest scores in English. 
Interestingly, MLMs achieve better results, notably RemBERT on high-resource languages (\texttt{deu, eng, esp, rus}) with \texttt{chn} being the only exception. On the other hand, for mainly vernacular (i.e., spoken) low-resource (e.g.,  \texttt{arq}) LLMs show some striking improvements (>36 with DeepSeek-R1-70B). 


%//I will add comments about crosslingual settings
%Comment : don't forget to align all model/names on text and tables/figures. For example: mDeBERTa x mDeberta

\section{Analysis}
\begin{figure*}[ht]
    \centering
    %\begin{subfigure}[Prompting]{0.3\textwidth}
    \begin{subfigure}[t]{0.3\textwidth}
         \centering
         \includegraphics[width=\linewidth]{figures/prompt_variants_performance.pdf}
         \caption{\textbf{Performance of different LLMs across three prompt paraphrases on the English test set.} Different prompts impact model performance.
         }
         \label{fig:prompt_variant}
     \end{subfigure}
      \hfill
      % \begin{subfigure}[N-shot]{0.32\textwidth}
        \begin{subfigure}[t]{0.32\textwidth}
       \centering
         \includegraphics[width=\linewidth]{figures/few_shot_performance.pdf}
         \caption{\textbf{Few-shot performance of LLMs on the English test set.} 
         Performance improves with more shots.%, with Mixtral-8x7B and DeepSeek-R1-70B outperforming other models.
         }
         \label{fig:few_shot_performance}
       \end{subfigure}
        \hfill
       % \begin{subfigure}[Top-k]{0.32\textwidth}
      \begin{subfigure}[t]{0.32\textwidth}
       \centering
           \includegraphics[width=\linewidth]{figures/topk_performance.pdf}
         \caption{\textbf{Top-k performance of different LLMs on the English test set.} Higher $k$ values increase the likelihood of retrieving the correct answer.%, with DeepSeek-R1-70B achieving the best performance at k=8.
         }
         \label{fig:topk_performance}
       \end{subfigure}
     \caption{Ablation studies on the effect of prompt wording variation, few-shot examples, and top-k predictions conducted on the English test set.}
\end{figure*}


The results in \Cref{fig:prompt_variant} suggest that LLM performance is highly dependent on the prompt wording when asking for the presence of emotion on the English test set using different paraphrases of the same text.
Further, Figure \ref{fig:few_shot_performance} shows that, when testing the effect of n-shot settings on the English test set, we observe a significant improvement in performance with more shots, with Mixtral-8x7B and  Llama-3.3-70B outperforming other models. However, the scores tend to reach a plateau at 4 shots for all LLMs except for Qwen2.5-72B, which suggests that 4 to 8 shots may be sufficient to obtain stable results. In addition, when testing how likely we can get the correct answer when prompting LLMs to generate tokens based on a top-$k$ selection, the results shown in Figure \ref{fig:topk_performance} suggest that increasing the value of $k$ results consistently in better performance, particularly when using DeepSeekR1-70B, which achieves an F-score $>90$ when $k=8$.% whereas Mixtral-8x7B shows a smaller change in performance followed by Llama-3.3-70B and Qwen2.5-72B. The ranking of the models for $k=8$ remains consistent with the one achieved for $k=1$.



% \input{tables/ablation_few_shot}

%\paragraph{Top-k results}



\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/radar_chart_updated.pdf}
    \caption{\textbf{Comparing models' performance across languages when prompted in English (orange) vs. when prompted in the target language (blue)}. LLMs perform better when prompted in English.}
    
    \label{fig:radar_chart}
\end{figure}
When comparing the performance of the models prompted in English vs. the target language, \Cref{fig:radar_chart} shows that LLMs tend to perform better when prompted in English except for \texttt{arq} for which Qwen2.5-72B performs better when prompted in MSA.
Improvements when using English prompts are markedly noticeable in low-resource languages (e.g., \texttt{hau}, \texttt{mar}, \texttt{vmw}) where Dolly-v2-12B and Llama-3.3-70B struggle the most with target language prompts. %They show a considerable drop in performance %(up to $>60$ points), 
%while Qwen2.5-72B and Mixtral-8x7B show a smaller drop in performance.

%, which can reach a difference of $>21$ points (for \texttt{hau}). %not great in Marathi and Hindi too

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figures/radar_chart_updated.pdf}
%     \caption{Radar chart comparing models performance across languages when prompted in English (orange) versus the target language (blue).}
%     \label{fig:radar_chart}
% \end{figure}


%\paragraph{Prompting Method}






% \input{tables/ablation_topk}
\section{Related Work}

Appraisal theories of emotion describe that emotions are due to our evaluation of an event based on personal experiences, resulting in various emotions evoked for different people \citep{arnold1960emotion, moors2013appraisal,ellsworth2013appraisal,frijda1986emotions,lazarus1991emotion,ortony2022cognitive,roseman2013appraisal,scherer2009dynamic}. The theory of constructed emotions states that they are not hard-wired in the brain or universal, but are rather concepts constructed by the brain \citep{barrett2016,barrett2017emotions}. 

Prior work in NLP has largely focused on \textit{sentiment analysis} -- detecting whether a text expresses positive, negative, or neutral valence \citep{Mohammad2016,muhammad-etal-2023-semeval}. Recent work focus has shifted to a broader form—detecting emotions in text such as anger, fear, joy, sadness, etc. in text which is in line with discrete models of emotions (e.g., Paul Eckman's six emotions \citep{ekman1992there} and Plutchik's Wheel of Emotions \citep{Plutchik1980} for anger, disgust, fear, happiness, sadness, surprise, anticipation and trust).

Several initiatives have created emotion classification datasets for languages other than English (e.g., Italian \citep{bianchi-etal-2021-feel}, Romanian \citep{ciobotaru-etal-2022-red},
% \citep{ciobotaru-dinu-2021-red},
Indonesian \citep{saputri2018emotion}, and Bengali \citep{iqbal2022bemoc}). However, NLP work in the area is predominantly Western-centric, and while multilingual datasets like XED \citep{ohman2020xed} and XLM-EMO \citep{bianchi2022xlm} exist, XLM-EMO's reliance on translated data for over ten languages may not fully capture cultural nuances in emotion expression. Emotions are culture-sensitive and highly contextualized, influenced by cultural values \citep{havaldar-etal-2023-multilingual,mohamed-etal-2024-culture,hershcovich-etal-2022-challenges}. Further, although emotions can co-occur \citep{vishnubhotla-etal-2024-emotion-granularity}, most datasets assume single-label classification. While GoEmotions \citep{demszky-etal-2020-goemotions} addresses multi-label emotion classification, to our knowledge, no multilingual resources capture the overlapping emotions and intensity across languages.
This work aims to push this boundary by presenting emotion-labeled data for 28 languages. Given the lack of unanimity surrounding language categorisation as low-resource, approximately 15 to 17 of these languages could be considered such. 


%Prior work in NLP has largely focused on determining the attitude towards an entity, such as customer reviews for a product or brand; however, sentiment analysis has many use-cases, such as in government (e.g., opinion towards politicians, government policy or initiatives), education (e.g., emotions are integral to learning), and public health (e.g., chatbots). SemEval Tasks have large audiences working on creating more advanced systems for detecting emotions in text. Since 2011, there have been yearly tasks focused on sentiment analysis on customer reviews and tweets, with more diverse topics in recent years \cite{}. Furthermore, focus has shifted to also detecting emotions in text such as anger, fear, joy, sadness, etc. in text.
% SemEval-2014 Task 4, SemEval-2015 Task 12, SemEval-2016 Task 5

% Emotion detection can also focus on determining the attitude of the speaker or the emotional state of the speaker.
%However, recent efforts have developed emotion labeled data for lower-resource languages such African languages (e.g., AfriSenti-SemEval, \citealp{muhammad-etal-2023-semeval}). 

\section{Conclusion}
We presented \datasetname, a collection of emotion recognition datasets in 28 languages spoken across various continents. The instances in \datasetname~ are multi-labeled, collected, and annotated by fluent speakers, with 10 datasets annotated for emotion intensity. When testing LLMs on our dataset collection, the results show that they still struggle with predicting perceived emotions and their intensity levels, especially for under-resourced languages. Further, our results show that LLM performance is highly dependent on the wording of the prompt, its language, and the number of shots in few-shot settings.
We publicly release \datasetname, our annotation guidelines, and individual labels to the research community.


\section*{Limitations} \label{sec:limitations}
Emotions are subjective, subtle, expressed, and perceived differently. We do not claim that \datasetname~ covers the true emotions of the speakers, is fully representative of the language use of the 28 languages, or covers all possible emotions. We discuss this extensively in the Ethics Section. 

We are aware of the limited data sources in some low-resource languages. Therefore, our datasets cannot be used for tasks that require a large amount of data from a given language. However, they remain a good starting point for research in the area.

\section*{Ethical Considerations} \label{sec:ethics}
Emotion perception and expression are subjective and nuanced as they are strongly related to a myriad of other aspects (e.g., cultural background, social group, personal experiences, social context).  
Thus, we can never truly identify how one is feeling based solely on shot text snippets with absolute certainty.
We clearly state that our datasets focus on perceived emotions and determining what emotion most people think the speaker may have felt.
Hence, we do not claim that we annotate the true emotion of the speaker, which cannot be definitively known from just a short text snippet.
We acknowledge the importance of this distinction as perceived emotions can differ from actual emotions.

We acknowledge possible biases in our data since we rely on text-based communication, where data sources can include biases, and annotators might always come with their own internalized subtle ones. Further, although many of our datasets focus on low-resource languages, we do not claim that they fully represent these languages' usage, and while we controlled for inappropriate instances, we acknowledge that we might have missed some. 

We explicitly ask for careful reflection on the ethical considerations before using our datasets. We forbid using our datasets for commercial purposes or by state actors to make high-risk applications unless explicitly approved by the dataset creators. Systems built on our systems may not be reliable at individual instance levels and are impacted by domain shifts. Therefore, they should not be used to make critical decisions for individuals, such as in health applications without expert supervision. See \citet{mohammad2022ethics,mohammad-2023-best} for a thorough discussion on the topic.

Finally, the annotators involved in the study were paid more than the minimum wage per hour. %Any demographic information about them has been shared with consent.


\section*{Acknowledgments}
Shamsuddeen Muhammad acknowledges the support of Google DeepMind and Lacuna Fund, an initiative co-founded by The Rockefeller Foundation, Google.org, and Canada’s International Development Research Centre. The views expressed herein do not necessarily represent those of Lacuna Fund, its Steering Committee, its funders, or Meridian Institute.
\newline
Nedjma Ousidhoum would like to thank Abderrahmane Samir Lazouni, Lyes Taher Khalfi, Manel Amarouche, Narimane Zahra Boumezrag, Noufel Bouslama, Abderraouf Ousidhoum, Sarah Arab, Wassil Adel Merzouk, Yanis Rabai, and another annotator who would like to stay anonymous for their work and insightful comments. 
\newline
Jan Philip Wahle, and Terry Ruas, Bela Gipp, Florian Wunderlich were partially supported by the Lower Saxony Ministry of Science and Culture and the
VW Foundation.
\newline
Yi Zhou would like to thank Gaifan Zhang, Bing Xiao, and Rui Qin for their help with the annotations and for providing feedback.
\newline
Alexander Panchenko would like to thank Nikolay Ivanov, Artem Vazhentsev, Mikhail Salnikov, Maria Marina, Vitaliy Protasov, Sergey Pletenev, Daniil Moskovskiy, Vasiliy Konovalov, Elisey Rykov, and Dmitry Iarosh for their help with the annotation for Russian. Preparation of Tatar data was funded by AIRI and completed by Dina Abdullina, Marat Shaehov, and Ilseyar Alimova.
\newline
Rahmad Mahendra acknowledges the funding by Hibah Riset Internal Faculty of Computer Science, Universitas Indonesia under contract number: NKB-13/UN2.F11.D/HKP.05.00/2024 for supporting the annotation of the Indonesian, Javanese, and Sundanese datasets.
\newline 
Meriem Beloucif acknowledges Nationella Språkbanken (the Swedish National Language Bank) and Swe-CLARIN – jointly funded by the Swedish Research Council (2018–2024; dnr 2017-00626) and its 10 partner institutions for funding the Swedish annotations.
\newline
Idris Abdulmumin gratefully acknowledges the ABSA UP Chair of Data Science for funding his post-doctoral research and providing compute resources.

% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

%\section*{Acknowledgments}

\bibliography{anthology,custom}

\appendix

\clearpage


% The exact LMs used were:
% \begin{enumerate}
%     \item \texttt{google-bert/bert-base-multilingual-cased}
%     \item \texttt{FacebookAI/xlm-roberta-large}
%     \item \texttt{microsoft/mdeberta-v3-base}
%     \item \texttt{sentence-transformers/LaBSE}
%     \item \texttt{microsoft/infoxlm-large}
%     \item \texttt{google/rembert}
% \end{enumerate}

% The exact LLMs used were:
% \begin{enumerate}
%     \item \texttt{databricks/dolly-v2-12b}
%     \item \texttt{meta-llama/Meta-Llama-3.3-70B}
%     \item \texttt{Qwen/Qwen2.5-72B-Instruct}
%     \item \texttt{mistralai/Mixtral-8x7B-Instruct-v0.1}
%     \item \texttt{deepseek-ai/DeepSeek-R1-Distill-Llama-70B}
% \end{enumerate}




% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=1.05\linewidth]{figures/class_distribution_chart.png}
%     \caption{Label distribution across \datasetname~datasets, showing the ratio of Single, Neutral, and Multi-Label samples in each test set.}
%     \label{fig:class-distr}
% \end{figure*}



% \section{Monolingual Multi-label Emotion classification} 


\section{Data sources}
\begin{itemize}
    \item \textbf{\texttt{afr}}: Speeches from \citet{barnard2014nchlt}.
    \item \textbf{\texttt{arq}}: Translated novel (\textit{La Grande Maison} by the Algerian author Mohammed Dib).
    \item \textbf{\texttt{ary, hau, ibo, kin, pcm, swa, xho, zul}}: \textbf{Afrisenti} \citet{muhammad-etal-2023-afrisenti} and BBC news headlines.
    \item \textbf{\texttt{chn}}: Weibo dataset \url{https://github.com/aoguai/WeiboHotListDataSet?tab=readme-ov-file}.
    \item \textbf{\texttt{deu}}: Anonymized Reddit data from nine German-language subreddits: \textit{de, einfach\_posten, FragReddit, beziehungen, schwanger, de\_IAmA, germany, depression\_de, Lagerfeuer}.
    \item \textbf{\texttt{eng}}: Personal narratives from the AskReddit subreddit collected by \citet{ouyang2015modeling} and instances from \citet{zhuang2024my}.
    \item \textbf{\texttt{esp}}: YouTube comments from Latin American channels across three genres: \textit{News/Politics, Entertainment, Education}.
    \item \textbf{\texttt{hin, mar}}: Newly created emotion dataset. Most instances were manually drafted, while some were generated using ChatGPT.
    \item \textbf{\texttt{ind, jav, sun}}: YouTube comments from Indonesian videos.
    \item \textbf{\texttt{ron}}: Data from the subreddit \textit{r/Romania}, YouTube, and tweets from \citet{ciobotaru-etal-2022-red}.
    \item \textbf{\texttt{rus}}: Russian Twitter corpus \url{https://study.mokoron.com}.
    \item \textbf{\texttt{swe}}: Sentiment dataset from the Swedish data bank \cite{SvenskABSAbank}.
    \item \textbf{\texttt{tat}}: Instances from \citet{krylova2016languages}.
    \item \textbf{\texttt{vmw, ptmz}}: News headlines from \citet{ali2024building}.
    \item \textbf{\texttt{yor}}: News data from BBC Yoruba and Alaroye \url{https://alaroye.org/}.
\end{itemize}


\section{Annotation}
%\subsection{Annotators' Demographic Information}
% to be added
\subsection{Annotation Guidelines and Definitions}\label{app:guide}

This is a guide for annotating text for emotion classification. The purpose of this study is to analyze the emotions expressed in a text. It is important to note that emotions can often be inferred even if they are not explicitly stated. %(See examples in Section~\ref{sec:examples}.)


\paragraph{Task}
The task involves classifying text into predefined emotion categories. The annotated dataset will be used for training emotion classification models and studying how emotions are conveyed through language.
\paragraph{Emotion Categories}
We categorize emotions into the following seven classes:

\noindent \textbf{Joy}
\begin{itemize}
    \item Definition: Expressions of happiness, pleasure, or contentment.
    \item  Example: \textit{"I just passed my exams!"}
\end{itemize}

\noindent \textbf{Sadness}
\begin{itemize}
    \item Definition: Expressions of unhappiness, sorrow, or disappointment.
    \item Example: \textit{"I miss my family so much. It's been a tough year."}
\end{itemize}

\noindent \textbf{Anger}
\begin{itemize}
    \item Definition: Expressions of frustration, irritation, or rage.
    \item Example:\textit{"Why is the internet so slow today?!"}
\end{itemize}

\noindent \textbf{Fear}
\begin{itemize}
    \item Definition: Expressions of anxiety, apprehension, or dread.
    \item Example: \textit{"There's a huge storm coming our way. I hope everyone stays safe."}
\end{itemize}

\noindent \textbf{Surprise}
\begin{itemize}
    \item Definition: Expressions of astonishment or unexpected events.
    \item Example: \textit{"I can't believe he just proposed to me!"}
\end{itemize}

\noindent \textbf{Disgust}
\begin{itemize}
    \item Definition: A reaction to something offensive or unpleasant.
    \item Examples: \textit{"That video was sickening to watch."}
\end{itemize}

\noindent \textbf{Neutral}
\begin{itemize}
    \item Definition: Texts that do not express any of the above emotions.
    \item Example: \textit{"The weather today is sunny with a chance of rain."}
\end{itemize}

\vspace{5pt}
\textbf{Note:} Factual statements can indicate an emotional state without explicitly stating it. For example:
\begin{itemize}
    \item \textit{"An earthquake today killed hundreds of people in my home town."}
\end{itemize}

\noindent Surprise differs from joy in that it represents an unexpected event, which may or may not be associated with happiness.

\vspace{10pt}



\paragraph{Emotion Description Categories}
The following list provides a broader categorization of emotions by including synonyms and related emotional states.

\vspace{5pt}

\noindent \textbf{Anger}
\begin{itemize}
    \item Includes: \textit{irritated, annoyed, aggravated, indignant, resentful, offended, exasperated, livid, irate}, etc.
\end{itemize}

\noindent \textbf{Sadness}
\begin{itemize}
    \item Includes: \textit{melancholic, despondent, gloomy, heartbroken, longing, mourning, dejected, downcast, disheartened, dismayed}, etc.
\end{itemize}

\noindent \textbf{Fear}
\begin{itemize}
    \item Includes: \textit{frightened, alarmed, apprehensive, intimidated, panicky, wary, dreadful, shaken}, etc.
\end{itemize}

\noindent \textbf{Happiness}
\begin{itemize}
    \item Includes: \textit{joyful, elated, content, cheerful, blissful, delighted, gleeful, satisfied, ecstatic, upbeat, pleased}, etc.
\end{itemize}

\noindent \textbf{Surprise}
\begin{itemize}
    \item Includes: \textit{taken aback, bewildered, astonished, amazed, startled, stunned, shocked, dumbstruck, confounded, stupefied}, etc.
\end{itemize}

\noindent \textbf{Joy}
\begin{itemize}
    \item Includes: \textit{happiness, delight, elation, pleasure, excitement, cheerfulness, bliss, euphoria, contentment, jubilation}.
\end{itemize}



\subsubsection{Emotion Intensity} \label{sec:examples}

After selecting the emotion category, annotators were further asked to select the intensity label, which could be: 0: No Emotion, 1 - Slight Emotion,  2: Moderate Emotion and 3: High Emotion. The following examples illustrate different levels of emotion intensity.

\vspace{10pt}

\noindent \textbf{Anger}
\begin{itemize}
    \item No Anger: \textit{"I walked through the empty streets, the quiet hum of the city like a distant whisper."}
    \item Slight Anger: \textit{"The buzz of voices around me blended into a monotonous drone, failing to distract from the pang of annoyance at the delay."}
    \item High Anger: \textit{"When his friend's brother knocked on the door, he was greeted with a shotgun blast through the door, which left him dead at the doorstep."}
\end{itemize}

\vspace{5pt}


\subsection{Pilot Annotation}
We run a pilot annotation on different languages to further refine our guidelines. This has mainly led to further clarifications related to the labeling process. For instance, the annotators were reminded that they should select all the labels that apply for a given text snippet, and that one label can encompass more than one specific emotion (e.g., in \texttt{arq}, we explained that a complex perceived emotion such as bitterness or jealousy might involve both anger and sadness). 



\subsection{ Formula for Determining Final Labels}

\paragraph{Aggregating emotion labels}
Aggregating emotion labels can be formally expressed as:


 \tcbset{colback=gray!6, colframe=black, sharp corners, width=\linewidth+14.5mm, boxrule=0.2mm}

\begin{tcolorbox}
\[
L_{\text{final}} =
\begin{cases} 
1, & \text{if } \text{Count}(1, 2, 3) \geq 2 \text{ and } \text{AvgScore} > T, \\
0, & \text{otherwise}.
\end{cases}
\]

\[
\text{Count}(1, 2, 3) = \sum_{i=1}^N \mathbb{1}(A_i \in \{1, 2, 3\}),\] 
\[
\quad \text{AvgScore} = \frac{1}{N} \sum_{i=1}^N A_i,
\]
\end{tcolorbox}
Where:
\begin{itemize}
    \item \(A_i\) is the rating provided by annotator \(i\).
    \item \(N\) is the total number of annotators.
    \item \(\mathbb{1}(A_i \in \{1, 2, 3\})\) Membership function that returns 1 if \(A_i \in \{1, 2, 3\}\), and 0 otherwise.
    \item \(T\) is the threshold for the average score, which we set as \(T = 0.5\)
\end{itemize}


\paragraph{Aggregating intensity}

Aggregating intensity can be formally expressed as:


 \tcbset{colback=gray!6, colframe=black, sharp corners, width=\linewidth, boxrule=0.2mm}

\begin{tcolorbox}
\[
\text{AvgScore} = \frac{\sum_{i=1}^N A_i}{N},
\]

\[
L_{\text{final}}=
% C_{\text{intensity}} =
\begin{cases} 
0, & \text{if } 0 \leq \text{AvgScore} < 1, \\
1, & \text{if } 1 \leq \text{AvgScore} < 2, \\
2, & \text{if } 2 \leq \text{AvgScore} < 3, \\
3, & \text{if } \text{AvgScore} = 3.
\end{cases}
\]

\end{tcolorbox}


Where:
\begin{itemize}
    \item \(A_i\) is the intensity score provided by annotator \(i\), where \(A_i \in \{0, 1, 2, 3\}\).
    \item \(N\) is the total number of annotators.
\end{itemize}


\section{SCHMP Calculation}\label{app:reliability}

The computation of SHCMP involves the following steps:


\paragraph{1. Random Splitting with Tie-Breaking} 
The dataset of $N$ annotated items is randomly divided into two equal subsets, $A_1$ and $A_2$. For datasets with an odd number of annotations, probabilistic tie-breaking is applied to ensure balanced splits.

\paragraph{2. Class Assignment} 
For each item $x_i \, (i = 1, 2, \ldots, N)$:
\begin{itemize}
    \item Assign $x_i$ a score based on its annotations in $A_1$ and $A_2$.
    \item Let $C_1(x_i)$ and $C_2(x_i)$ denote the class of $x_i$ derived from $A_1$ and $A_2$, respectively.
\end{itemize}

\paragraph{3. Class Binning} 
To manage continuous scores, divide the range of possible scores $[-3, 3]$ into equal-sized bins, where the bin size $b$ is determined as:
\[
b = \frac{6}{\#\text{Bins}}.
\]
Scores from $A_1$ and $A_2$ are then assigned to their respective bins, denoted as $c_1$ and $c_2$.

\paragraph{4. Match Calculation} 
Define a match indicator $M(x_i)$ to evaluate consistency for each item:
\[
M(x_i) = 
\begin{cases} 
1, & \text{if } |c_1 - c_2| < 1, \\
0, & \text{otherwise.}
\end{cases}
\]
This ensures that items are considered consistent if their scores fall into the same bin or adjacent bins.

\paragraph{5. Proportion of Matches} 
Compute the total number of matches, $N_{\text{match}}$, across all items:
\[
N_{\text{match}} = \sum_{i=1}^{N} M(x_i).
\]

\paragraph{6. SHCMP Computation} 
The SHCMP score is calculated as the proportion of matches, expressed as a percentage:
\[
\text{SHCMP (\%)} = \frac{N_{\text{match}}}{N} \times 100.
\]

\paragraph{7. Averaging} 
We repeat the process $k$ times with different random splits and compute the average SHCMP score:
\[
\text{SHCMP}_{\text{final}} = \frac{1}{k} \sum_{j=1}^{k} \text{SHCMP}_j,
\]
where $\text{SHCMP}_j$ is the SHCMP score from the $j$-th split.

\input{tables/single_vs_multilable}




\input{tables/track_a_monolingual_lms}\label{app:multilingual}

\section{Experimental Settings} \label{sec:appendix}

For LLMs, we used the default parameters from HuggingFace except for temperature which we set to 0 for deterministic output and topk is set to 1. Only for the topk ablations in which topk > 1 in \Cref{fig:topk_performance}, we set temperature to 0.7. We ask all LLMs to perform CoT. We trained on the train set for 2 epochs with a learning rate of 1e-5 and and evaluated on test set. For MLMs experiments, we trained on the training set for 2 epochs with a learning rate of 1e-5 and evaluated on the test set.


\begin{table*}
\centering
\begin{tabular}{p{3cm} p{10.5cm}}
\toprule
\textbf{Prompt Version} & \textbf{Prompt Text} \\
\midrule
Prompt v1 & 
\texttt{Evaluate whether the following text conveys the emotion of \{\{EMOTION\}\}.\newline
Think step by step before you answer.\newline
Finish your response with 'Therefore, my answer is ' followed by 'yes' or 'no':\newline\newline\{\{INPUT\}\}} \\\\
Prompt v2 &
\texttt{Analyze the text below for the presence of \{\{EMOTION\}\}.\newline
Explain your reasoning briefly and conclude with 'Answer:' followed by either 'yes' or 'no'.\newline\newline\{\{INPUT\}\}} \\\\
Prompt v3 &
\texttt{Examine the following text to determine whether \{\{EMOTION\}\} is present.\newline
Provide a concise explanation for your assessment and end with 'Answer:' followed by either 'yes' or 'no'.\newline\newline\{\{INPUT\}\}} \\
\bottomrule
\end{tabular}
\caption{The prompt variants used for ablation of Track A.}
\label{tab:prompt-variants}
\end{table*}


\begin{figure*}[t]
    \begin{AIbox}{Track A: Example Few-Shot Prompt}
    \parbox[t]{\textwidth}{
        \textbf{\#\#\# Task: \#\#\#} \\
        Analyze the text below for the presence of anger. \\
        Explain your reasoning briefly and conclude with 'Answer:' followed by either 'yes' or 'no'. \\

        \textbf{\#\#\# Examples: \#\#\#} \\
        Example 1:\\
        Input: '''When I answered the phone, my heart beat extremely fast... I was very nervous!'''\\
        Answer: no \\

        Example 2:\\
        Input: '''I'll never forget how businesslike and calm the Israeli guy was.'''\\
        Answer: no \\

        Example 3:\\
        Input: '''I wake up, my eyes fluttering open to a shield of darkness.'''\\
        Answer: no \\

        Example 4:\\
        Input: '''I lay in a large bed, the sheets and quilt pulled up to my chin, and the curtains were drawn to keep out the light.'''\\
        Answer: no \\

        Example 5:\\
        Input: '''Either way that idiot is gone.'''\\
        Answer: yes \\

        Example 6:\\
        Input: '''Seriously... did I really just shut my finger in the car door.'''\\
        Answer: yes \\

        Example 7:\\
        Input: '''I was really uncomfortable because I was sitting behind my dad and there isn't enough room for my legs.'''\\
        Answer: yes \\

        Example 8:\\
        Input: '''He damn disturb plz, cover my head with a shirt that a customer which have body odour just tried on!!'''\\
        Answer: yes \\

        \par\noindent\rule{\textwidth}{0.4pt}

        \textbf{\#\#\# Your Turn: \#\#\#} \\
        Input: '''/ o \ So today I went in for a new exam with Dr. Polvi today, I had to file new paperwork for the automobile accident case which is being done differently than the scoliosis stuff. So he comes in and starts talking about insurance stuff and how this looks bad since I was getting treatment on my neck and stuff already blah blah.'''
    }
    \end{AIbox}
    \caption{Example of the few-shot prompt template for assessing anger in Track A.}
    \label{fig:prompt_example_tracka}
\end{figure*}


\begin{figure*}[t]
    \begin{AIbox}{Track B: Example Few-Shot Prompt}
    \parbox[t]{\textwidth}{
        \textbf{\#\#\# Task: \#\#\#} \\
        In this task, you will assess the level of anger in a given text (0 = none, 1 = low, 2 = medium, 3 = high). \\
        Summarize your reasoning and conclude with 'Answer:' followed by the correct number. \\

        \textbf{\#\#\# Examples: \#\#\#} \\
        Example 1:\\
        Input: '''I try extremely hard to keep my details hidden. It was nice to know that what I had given people to know was pleasant, but I couldn't deny the knot that was still in my stomach.'''\\
        Answer: 0 \\

        Example 2:\\
        Input: '''I knew we were almost there when my midwife's voice got more excited and Joey leaned in real close and said into my ear, `` Don't stop pushing! '' '''\\
        Answer: 0 \\

        Example 3:\\
        Input: '''One ended up going to prison.'''\\
        Answer: 1 \\

        Example 4:\\
        Input: '''Not to mention noisy.'''\\
        Answer: 1 \\

        Example 5:\\
        Input: '''" but Urban Dictionary confirmed Spook is indeed a racial slur.'''\\
        Answer: 2 \\

        Example 6:\\
        Input: '''And..at his funeral, they fired him!'''\\
        Answer: 2 \\

        Example 7:\\
        Input: '''I ended up metaphorically throwing my hands in the air in disgust and just cancelling my account altogether.'''\\
        Answer: 3 \\

        Example 8:\\
        Input: '''He would manipulate me into it and I was extremely upset.'''\\
        Answer: 3 \\

        \par\noindent\rule{\textwidth}{0.4pt}

        \textbf{\#\#\# Your Turn: \#\#\#} \\
        Input: '''So today I went in for a new exam with Dr. Polvi today, I had to file new paperwork for the automobile accident case which is being done differently than the scoliosis stuff. So he comes in and starts talking about insurance stuff and how this looks bad since I was getting treatment on my neck and stuff already blah blah.'''
    }
    \end{AIbox}
    \caption{Example of the few-shot prompt template for assessing anger in Track B.}
    \label{fig:prompt_example_trackb}
\end{figure*}


\end{document}