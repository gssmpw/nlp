\section{Related Work}
To the best of our knowledge, there exists no work on non-rectangular robust MDPs with kernel uncertainty. This work is the first to propose an efficient method for robust policy evaluation for a very useful class of uncertainty sets, otherwise thought to be NP-Hard \cite{RVI}.

\textbf{Rectangular Robust MDPs.} In literature, \texttt{sa}-rectangular uncertainty is a very old assumption \cite{Iyenger2005,Nilim2005RobustCO}. \cite{RVI} introduced \texttt{s}-rectangular uncertainty sets and proved its tractability, in addition to the intractability of the general non-rectangular uncertainty sets. 

The most advantageous aspect of the \texttt{s}-rectangularity, is the existence of contractive robust Bellman operators. This gave rise to many robust value based methods \cite{ppi,RPG_conv}. Further, for many specific uncertainty sets, robust Bellman operators are equivalent to regularized non-robust operators, making the robust value iteration as efficient as non-robust MDPs \cite{derman2021twice, Rcontamination, LpRMDP}.  

There exists many policy gradient based methods for robust MDPs, relying upon contractive robust Bellman operators for the robust policy evaluation \cite{PG_RContamination, LpPgRMDP}.

Further, \cite{Ewok_dileep,Ewok} trie to tweak the process, and directly get samples from the adversarial model via pessimistic sampling. 


There exist other notions of rectangularity such as k-rectangularity \cite{k-rectangularRMDP} and r-rectangularity \cite{r-rectRMDP} which are sparsely studied. However, \cite{TractablerRMDP} shows, the theses uncertainty sets are either equivalent to s-rectangularity or non-tractable.

\textbf{Non-Rectangular Reward Robust MDPs. } 
Policy evaluation for robust MDPs with non-rectangular uncertainty set is proven to be a Strongly-NP-Hard problem \cite{RVI}, in general. For a very specific case, where uncertainty is limited only to reward uncertainty bounded with $L_p$ norm, \cite{LpRewardRobust} proposed robust policy evaluation via frequency (occupation measure) regularization, and derived the policy gradient for policy improvement. 

\textbf{Convergence Rate of Robust Policy Gradient .} The robust policy gradient method has been shown to converge with iteration complexity of \(O(\epsilon^{-4})\) for general robust MDPs \cite{RPG_conv}. However, it requires oracle access to robust policy evaluation (i.e., the computation of the worst kernel), which can be computationally expensive \cite{RPG_conv}.