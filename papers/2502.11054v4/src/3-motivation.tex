\section{Threat Model}

The target LLM $M$ has undergone safety alignment prior to release and is expected to avoid generating unsafe responses even when presented with a harmful target query $Q$. In this study, we investigate self-jailbreaking, where both the querying and response-generating models originate from the same model. For clarity, we instantiate the target model $M$ as two distinct roles: a shadow model $M_s$, responsible for generating queries, and a victim model $M_v$, tasked with providing responses. 

The goal of the shadow model is to generate a sequence of queries $\{q_1,q_2,...,q_n\}$ during its interaction with the victim model to induce unsafe responses. Given practical deployment scenarios, the attack is conducted in a black-box setting where the shadow model can only access the victim modelâ€™s responses during its interactions. However, the shadow model can adaptively adjust the current query $q_{i}$ (where $i$ denotes the current conversation turn)  based on the context $C_{i-1}$, which includes the query-response pairs $[\left(q_{1},r_{1}\right),...\left(q_{i-1},r_{i-1}\right)]$ from all preceding conversation turns.
