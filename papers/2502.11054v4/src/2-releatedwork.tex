\section{Related work}




\textbf{Reasoning in LLMs}. Reasoning is a cognitive process that involves thinking about something logically and systematically, using evidence and past experiences to draw conclusions or make decisions \cite{reason1,reason2}. Recent studies have demonstrated that LLMs exhibit remarkable reasoning capabilities in various tasks, including mathematical reasoning \cite{reasonllm1}, common sense reasoning \cite{reasonllm2}, symbolic reasoning \cite{reasonllm3}, and causal reasoning \cite{reasonllm4}. Subsequently, Chain-of-thought (CoT) \cite{cot1,cot2,cot3,cot4,cot5} has emerged as a promising approach for further enhancing these reasoning capabilities.

While the reasoning capabilities of LLMs have contributed to their impressive performance across various downstream tasks, their potential exploitation in jailbreak attacks remains largely unexplored. In this study, we focus on leveraging reasoning capabilities to facilitate jailbreak attacks.

\textbf{Multi-turn Jailbreak Attack}. Typical multi-turn jailbreak methods follow the principle of starting with harmless conversations and gradually making the queries more harmful in subsequent turns. Different methods have designed specific strategies based on this principle, including applying cognitive psychology theories to gradually modify subsequent queries \cite{mpsy1,mpsy2}, using actor networks to expand the attack range of subsequent queries \cite{ren2024}, extracting harmful keywords from original queries to construct semantically equivalent ones \cite{coa,cfa}, and breaking down the target query into multiple subqueries and merging the corresponding answers to achieve the final jailbreak \cite{sub1,sub2}.

Existing multi-turn jailbreak methods often suffer from semantic drift or fail to generate effective attacks. In contrast, our approach leverages LLMs' reasoning capabilities to ensure a stable and effective jailbreak process.