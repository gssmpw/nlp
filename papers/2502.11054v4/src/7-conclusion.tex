\section{Conclusion}
This paper presents a novel reasoning-driven jailbreak framework that exploits LLMs’ inherent reasoning capabilities to bypass built-in safety mechanisms. By modeling the attack process as an attack state machine, our approach strategically frames harmful intent as complex yet seemingly benign reasoning tasks, ensuring a structured and adaptive attack progression. We introduce three key modules, including gain-guided exploration, self-play, and rejection feedback to systematically manipulate the model’s reasoning process, optimize query structures, and recover from failed attempts. Extensive experiments demonstrate that our method effectively compromises existing safety alignments, revealing critical risks to LLM safety.

\section{Limitations}
Despite the effectiveness of \method{}, several challenges remain to be addressed: \ding{182} improving efficiency to minimize interaction overhead while maintaining high ASRs, \ding{183} developing adaptive countermeasures to mitigate reasoning-based attacks, and \ding{184} extending the framework to analyze and defend against other forms of adversarial reasoning manipulations~\cite{wang2025black, zhang2024visual}.

\section{Ethical Consideration}
We acknowledge the dual-use nature of this research and emphasize that our primary goal is to advance LLM safety through systematic vulnerability assessment. This work demonstrates that current alignment strategies may be insufficient in preventing multi-turn jailbreaks, particularly when exploiting reasoning capabilities. To minimize potential harm, we have carefully omitted explicitly harmful outputs while focusing on methodological aspects. We strongly oppose any malicious applications of our findings and have included discussions on potential countermeasures. While the development of comprehensive defense mechanisms~\cite{liu2023exploring, liang2024unlearning} remains future work, we believe this research provides valuable insights for LLM developers to develop more robust alignment techniques.