
\begin{table*}[htbp]
\caption{ASR (\%) of different attack methods against classic LLMs. \textbf{Bold text} indicates the method with the highest attack effectiveness in each row of the corresponding dataset.}
\label{exp:advbench}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}cc|cccccc|cccccc@{}}
\toprule
\multicolumn{2}{c|}{Dataset}                                 & \multicolumn{6}{c|}{AdvBench Subset}            & \multicolumn{6}{c}{HarmBench}                    \\ \midrule
\multicolumn{2}{c|}{Method}                                  & No Attack & PAIR & DI & CoA & TAP & \method{} & No Attack & PAIR & DI    & CoA   & TAP   & \method{}  \\ \midrule
\multicolumn{1}{c|}{\multirow{3}{*}{Open-Source}}   & Gemma  & 2.0         & 56.0   & 40.0 & 44.0  & 60.0  & \textbf{84.0}   & 13.8      & 25.0   & 24.5  & 32.0    & 55.0    & \textbf{55.3} \\
\multicolumn{1}{c|}{}                               & Qwen   & 0.0         & 62.0   & 56.0 & 52.0  & 66.0  & \textbf{96.0}   & 14.8      & 50.0   & 43.0    & 49.8 & 55.8 & \textbf{56.3} \\
\multicolumn{1}{c|}{}                               & GLM    & 10.0        & 80.0   & 58.0 & 64.0  & 78.0  & \textbf{100.0}  & 24.0        & 67.5 & 47.3 & 53.3 & 62.5  & \textbf{88.0}    \\ \midrule
\multicolumn{1}{c|}{\multirow{3}{*}{Closed-Source}} & Gemini & 2.0         & 60.0   & 44.0 & 48.0  & 58.0  & \textbf{88.0}   & 9.7       & 37.5 & 17.3 & 20.8 & 50.3 & \textbf{62.5}  \\
\multicolumn{1}{c|}{}                               & GPT-4   & 0.0         & 56.0   & 40.0 & 48.0  & 82.0  & \textbf{86.0}   & 9.3       & 30.0   & 16.3 & 19.5  & 45.0    & \textbf{55.0}    \\
\multicolumn{1}{c|}{}                               & GPT-4o  & 0.0         & 72.0   & 50.0 & 54.0  & 88.0  & \textbf{94.0}   & 5.0         & 39.0   & 20.5  & 22.8 & 59.5  & \textbf{82.8} \\ \bottomrule
\end{tabular}
}
\end{table*}


\begin{figure*}[!t]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{imgs/thinking.pdf}
        \caption{Gemini 2.0 Flash Thinking.}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{imgs/openai_o1.pdf}
        \caption{OpenAI o1.}
        \label{fig:subfig2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{imgs/r1.pdf}
        \caption{DeepSeek R1.}
        \label{fig:subfig3}
    \end{subfigure}
    \caption{ASR (\%) of different attacks against leading commercial reasoning LLMs.}
    \label{fig:reasoningattack}
\end{figure*}
\section{Experiments}

\subsection{Experimental Settings}
\textbf{Models.} We conduct experiments to validate the performance of \method{} across 9 popular LLMs, including 3 open-source models: Gemma (Gemma-2-9B) \cite{gemma2}, Qwen (Qwen2-7B-Instruct) \cite{qwen2}, and GLM (GLM-4-9B-Chat) \cite{glm4}, and 6 closed-source models: GPT-4 \cite{gpt4}, GPT-4o \cite{gpt4o}, Gemini 1.5 Pro \cite{team2024gemini}, Gemini 2.0 Flash Thinking \cite{gemini2024}, OpenAI o1 \cite{o1card}, and DeepSeek R1 \cite{r1}.

\textbf{Datasets.} 
Following previous work \cite{tap,chao2023jailbreaking}, we evaluate attack performance on the AdvBench subset \cite{gcg} and the HarmBench \cite{harmbench}. The AdvBench subset contains 50 representative samples from the AdvBench dataset, and HarmBench comprises 400 textual instances spanning 7 distinct categories of harmful activities.


\textbf{Compared baselines.} 
We compare \method{} against existing multi-turn jailbreak attack methods, including PAIR \cite{chao2023jailbreaking}, DeepInception (DI) \cite{li2023deepinception}, CoA \cite{coa}, and TAP \cite{tap}.

\textbf{Considered defenses.} We evaluate \method{} against representative defense methods, including SmoothLLM (SL) \cite{smoothllm}, Self-Reminder (SR) \cite{xie2023defending}, ICD \cite{wei2023jailbreak}, and JailGuard \cite{zhang2024jailguard}.

\textbf{Metrics.} ASR is our primary evaluation metric; \emph{higher ASR values correspond to more effective attack methods}. Given the characteristics of multi-turn jailbreak attacks, we introduce an additional metric in \Sref{sec:discuss}: the harmful response index (HRI) to quantify the harmfulness of unsafe content in model responses. \emph{A higher HRI indicates greater harmfulness in the model output}. Both metrics are evaluated using the LLM-as-Judge approach \cite{judge}, with the corresponding prompts provided in \Sref{sec:judge_prompt}.




\subsection{Attack Evaluation}

\textbf{Attack performance on classic LLMs.}
\Tref{exp:advbench} summarizes the experimental results. Among the evaluated methods, \method{} demonstrated the most effective performance, achieving average ASRs of 91.3\% on the AdvBench subset and 66.7\% on HarmBench. Among the baseline methods, TAP emerged as the most effective, achieving an impressive 88\% ASR when attacking GPT-4o on the AdvBench subset. Notably, we observed a significant performance gap between the AdvBench subset and HarmBench across all methods. The substantially lower ASRs on HarmBench can be attributed to its more diverse and complex tasks. Notably, the performance gap between \method{} and the baseline methods was even more pronounced on HarmBench, reaching up to 62.3\%, further highlighting the effectiveness of \method{} in more challenging scenarios.

\textbf{Attack performance on reasoning LLMs.}
We further evaluate three state-of-the-art reasoning LLMs using the AdvBench subset, with experimental results summarized in \Fref{fig:reasoningattack}. Taking Gemini 2.0 Flashing Thinking as an example, we observe that when directly presented with original harmful queries, the ASR of Gemini 2.0 Flashing Thinking reaches 20.0\%, which notably surpasses that of previous-generation models like Gemini 1.5 Pro (ASR reaches 2.0\%). This finding suggests that the introduction of advanced reasoning capabilities can paradoxically escalate potential safety risks in next-generation models. On the other hand, as highlighted by Jaech \etal \cite{o1card}, OpenAI o1 employs deliberative alignment to reason about safety policies and generate safe responses when faced with potentially unsafe prompts. By comparing the results in \Tref{exp:advbench} and \Fref{fig:reasoningattack}, we confirm this characteristic: under the baseline attack, the ASR of OpenAI o1 remained significantly lower than GPT-4 and GPT-4o. However, when subjected to \method{}, its ASR dramatically spiked to 82.0\%. Similarly, our method achieves an ASR of up to 92.0\% against DeepSeek R1. This indicates that while reasoning LLMs prioritize advanced inference capabilities during task execution, they overlook specific attack patterns like \method{}. These patterns can exploit reasoning mechanisms and manipulate key contextual cues.


\subsection{Defense Evaluation}
\begin{table*}[!t]
\caption{ASR (\%) of \method{} under defense methods. \textbf{Bold text} indicates the method with the strongest mitigation effect in each row within the corresponding dataset.}
\centering
\label{exp:defense}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}cc|ccccc|ccccc@{}}
\toprule
\multicolumn{2}{c|}{Dataset}                                 & \multicolumn{5}{c|}{AdvBench Subset}                            & \multicolumn{5}{c}{HarmBench}                               \\ \midrule
\multicolumn{2}{c|}{Method}                                  & No Defense & SL & SR & ICD & JailGuard & No Defense & SL & SR & ICD   & JailGuard \\ \midrule
\multicolumn{1}{c|}{\multirow{3}{*}{Open-Source}}   & Gemma  & 84.0         & 76.0        & \textbf{70.0}            & 80.0  & 72.0        & 55.3      & 44.0        & \textbf{37.5}          & 53.0    & 40.5      \\
\multicolumn{1}{c|}{}                               & Qwen   & 96.0         & 84.0        & \textbf{74.0}            & 88.0  & 80.0        & 56.3      & 45.0        & \textbf{40.3}         & 51.8 & 43.3     \\
\multicolumn{1}{c|}{}                               & GLM    & 100.0        & 90.0        & \textbf{78.0}            & 96.0  & 86.0        & 88.0         & 75.0        & \textbf{64.3}         & 85.0    & 71.5      \\ \midrule
\multicolumn{1}{c|}{\multirow{3}{*}{Closed-Source}} & Gemini & 88.0         & 80.0        & \textbf{70.0}            & 82.0  & 76.0        & 62.5       & 56.5      & 54.5          & 59.8 & \textbf{53.5}      \\
\multicolumn{1}{c|}{}                               & GPT-4   & 86.0         & 78.0        & \textbf{66.0}            & 82.0  & 74.0        & 55.0         & 50.3     & \textbf{44.5}          & 54.0    & 48.5      \\
\multicolumn{1}{c|}{}                               & GPT-4o  & 94.0         & 82.0        & \textbf{68.0}            & 90.0  & 80.0        & 82.8      & 77.8     & \textbf{62.0}            & 81.5  & 74.5      \\ \bottomrule
\end{tabular}
}
\end{table*}

Currently, test-time defenses for multi-turn jailbreak attacks are lacking. While training-based approaches like dataset construction and fine-tuning improve robustness, they are unsuitable for test-time defenses. Thus, we evaluate popular single-turn defenses against \method{}.

As illustrated in \Tref{exp:defense}, compared to the baseline, the evaluated defense methods demonstrate remarkably limited effectiveness in mitigating \method{}, with ASR reductions as minimal as 1\%. Notably, SR emerges as the most effective defense method, achieving an average ASR reduction of 17.6\%. This performance stems from the model's consistent prompting to scrutinize the safety of its outputs before generation. ICD proved almost ineffective against \method{}, with a mere 3.8\% average ASR reduction. This limitation primarily arises from the adaptive query generation mechanism of \method{}. Since queries from \method{} are phrased in natural language, the perturbation techniques designed by SL and JailGuard have limited impact, reducing the ASR by a maximum of 12\% and 16\%, respectively. Overall, \method{} shows considerable robustness against these defenses.



\section{Discussion} \label{sec:discuss}
\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.235\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/turn1.pdf}
        \caption{Comparison of ASR}
        \label{turn:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.235\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/man_heatmap.pdf}
        \caption{Comparison of HRI}
        \label{turn:sub2}
    \end{subfigure}
    \caption{Attack performance under different numbers of conversation turns.}
    \label{exp:turns}
\end{figure}
This section further explores the impact of conversation turns, reasoning task types, and attack strategies on attack performance. All experiments are conducted using the AdvBench subset on open-source models.





\subsection{Number of Conversation Turns}
The number of conversation turns serves as a crucial hyperparameter that significantly influences the effectiveness of multi-turn jailbreak attack. We evaluate its impact using ASR and HRI. As illustrated in \Fref{turn:sub1}, our method achieves ASRs of 84.0\%, 96.0\%, and 100.0\% on Gemma, Qwen, and GLM with only three interactions, demonstrating its efficiency.

As depicted in \Fref{turn:sub2}, we observe a systematic escalation in the harmfulness of model outputs as the number of conversation turns increases. This progression stems from two complementary mechanisms: initially, harmful content emerges from the inherent reasoning processes, where the victim model inadvertently exposes potentially unsafe information while attempting to solve complex queries; subsequently, the shadow model increasingly demands more intricate reasoning processes to incrementally extract increasingly detailed and potentially unsafe content. The results substantiate \method{}'s ability to perform jailbreaks through systematic multi-turn interactions.





\subsection{Reasoning Types}

\begin{figure}[!t]
    \centering
    \begin{minipage}{0.235\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/rp.pdf}
        \caption{Impact of different reasoning types.}
        \label{fig:reasoning}
    \end{minipage}
    \hfill
    \begin{minipage}{0.235\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/ab.pdf}
        \caption{Ablation results of attack modules.}
        \label{tab:ablation}
    \end{minipage}
\end{figure}

We evaluate four types of reasoning tasks: mathematical reasoning (MaR), common sense reasoning (CoR), symbolic reasoning (SyR), and causal reasoning (CaR), whose definitions and examples are detailed in \Sref{sec:reasondemo}.


\Fref{fig:reasoning} shows that common sense reasoning achieves the highest ASR of 93.3\%, as it leverages everyday knowledge and intuitive understanding. Mathematical reasoning and causal reasoning achieve an ASR of 89.3\% and 86.0\%, respectively, as both tasks require step-by-step logical deduction and precise reasoning chains, making them more challenging than direct common sense reasoning. Symbolic reasoning yields the lowest average ASR of 85.3\%, as it requires abstract pattern recognition and complex rule application. These results indicate that ASR can be impacted by the type of reasoning task. Among them, commonsense reasoning achieves the highest ASR, likely due to its reliance on general knowledge and intuition, which facilitates successful attacks.


\subsection{Ablation on Attack Modules}

\Fref{tab:ablation} presents the ablation study results for \method{}. We analyze the performance impact when removing GE, SP, and RF.

The experimental results demonstrate that removing any of these components leads to performance degradation. Without GE, ASR drops by up to 14.0\%, indicating the importance of selective query generation based on information gain. The absence of SP results in an ASR decrease of up to 8.0\%, showing the value of leveraging the shadow model for query optimization. Similarly, removing RF causes an ASR reduction of up to 12.0\%, highlighting its crucial role in handling failure transitions. The observed performance drops when removing each component demonstrate their complementary nature. GE ensures efficient query generation, SP enables adaptive optimization, and RF provides robust failure handling. Their integration contributes to the effectiveness of \method{}.
