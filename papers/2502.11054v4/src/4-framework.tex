

\begin{figure*}[htbp] 
    \centering 
    \includegraphics[width=0.99\textwidth]{imgs/main.pdf} 
    \caption{Overall attack process and framework. \method{} achieves a jailbreak by transforming the target query into a reasoning task and conducting multi-turn reasoning. The entire attack process is modeled as an ASM and optimized using the three proposed modules.} 
    \label{fig:main} 
\end{figure*}
\section{Methodology}\label{sec:method}


\subsection{Motivation and Design Principle}
LLMs have demonstrated strong reasoning capabilities in tasks such as logical deduction, common sense reasoning, and mathematical problem-solving, enabling them to tackle complex tasks across diverse domains \cite{reasonllm1,reasonllm2,reasonllm3,reasonllm4}. Rather than directly issuing harmful queries, which are easily rejected by safety alignment mechanisms, we propose a novel approach that exploits LLMs’ reasoning processes by reframing harmful intent into seemingly benign yet complex reasoning tasks. These tasks are carefully designed so that, once solved, they inherently guide the model to generate harmful content, effectively compromising its safety alignment. Here, the target LLM simultaneously acts as both the shadow model and the victim model. Independently, each role appears to engage in legitimate reasoning: the victim model focuses solely on solving reasoning tasks, while the shadow model refines and generates queries without explicitly recognizing the harmful intent behind them. However, when combined, these interactions ultimately lead to a successful attack.

However, implementing this reasoning-driven jailbreak is non-trivial, as it requires manipulating the model’s reasoning process without triggering safety mechanisms. This poses three challenges: \ding{182} how to maintain reasoning alignment while ensuring that each query remains semantically consistent with the target and extracts useful information, \ding{183} how to preemptively optimize the query’s reasoning structure to avoid potential rejections during actual interactions, and \ding{184} how to quickly recover and learn from failed reasoning attempts to maintain attack progression. To address these challenges, we model the jailbreak process as an Attack State Machine (ASM), which serves as a reasoning planner. The ASM formalizes the attack as a structured sequence of reasoning states and transitions, ensuring that each step remains within the bounds of a legitimate problem-solving task while progressing toward the jailbreak objective. Within this reasoning framework, we implement three key modules to manipulate the model’s reasoning process and systematically address these challenges. \ding{182} The Gain-guided Exploration module selects queries that remain semantically aligned with the target while extracting useful information, ensuring steady attack progression. \ding{183} The Self-play module preemptively refines queries within the shadow model by simulating potential rejection responses, improving attack efficiency before engaging the victim model. \ding{184} The Rejection Feedback module analyzes failed interactions and restructures queries into alternative reasoning challenges, enabling quick recovery and maintaining attack stability. The overview of \method{} is provided in \Fref{fig:main}.

\subsection{Attack State Machine Framework}

A finite state machine (FSM) \cite{fsmbase2,fsmbase1} is a mathematical model that represents a finite number of states, along with the transitions and actions between these states. A finite state machine can be formally defined as a five-tuple: $FSM = (S,\Sigma,\delta,s_{0},F)$, where $S$ denotes a finite set of states, $\Sigma$ represents the input alphabet, $\delta:S \times \Sigma \rightarrow S$ is the state transition function that determines the next state, $s_{0} \in S$ is the initial state, and $F \subseteq S$ is the set of accepting states. FSMs are widely used in computer science as a fundamental modeling tool for various applications \cite{fsm2,fsm3,fsm4,fsm5}. 

Specifically, we designate our modeled FSM as an attack state machine (ASM). The symbols in $FSM = (S,\Sigma,\delta,s_{0},F)$ have specific meanings within the ASM context. The state set $S$ represents a finite set containing all possible conversation states, while $\Sigma$ denotes the set of all potential queries. The state transition function $\delta$ defines how queries trigger state transitions. $s_{0}$ represents the initial state, marking the beginning of the session, where the model has no historical context. The set $F=\{s_{sc},s_{fl}\}$ comprises the final states: (1) the success state $s_{sc}$, where the victim model accepts the query and provides the requested response, indicating a successful jailbreak; and (2) the failure state $s_{fl}$, where the victim model refuses to proceed with the conversation, representing an unsuccessful jailbreak. Within a given conversation turn limit $N$ (default set to 3), the state transitions follow these rules: \ding{182} if a jailbreak attempt succeeds, ASM enters the final state $s_{sc}$; \ding{183} if the jailbreak attempt fails but the current conversation turn proceeds successfully, ASM transitions to the next state $s_{i+1}$; \ding{184} if both the jailbreak attempt and the current conversation turn fail, ASM remains in its current state $s_{i}$; \ding{185} if the conversation turn limit is exceeded without reaching $s_{sc}$, ASM directly transitions to the final state $s_{fl}$.


\subsection{Attack Modules} \label{sec:m3}

Within the ASM, three specialized modules work together to optimize state transitions and ensure attack progression. The gain-guided exploration and self-play modules proactively generate and optimize effective queries, while the rejection feedback module handles failed state transitions by refining queries. The design enables the ASM to maintain stable progression through the reasoning states while efficiently adapting to model responses.

\subsubsection{Gain-guided Exploration}

To address potential semantic drift and ineffective information in victim model responses, we propose a gain-guided exploration (GE) module inspired by information theory \cite{shannon}. 

Information gain (IG) \cite{ig2,ig1} was originally introduced to quantify how much a feature $A$ of a random variable reduces the uncertainty of a target variable $Y$, defined as $IG(Y,A) = H(Y) - H(Y \mid A)$, where $H(Y)= - \sum\limits_{y \in Y} P(y)\log P(Y)$ is the entropy \cite{ee} of the target variable, and $H(Y \mid A)=- \sum\limits_{a \in A} P(a)H(Y \mid A=a)$ represents the conditional entropy of $Y$ given $A$. When $IG(Y,A) > 0$, it indicates that feature $A$ effectively reduces the uncertainty associated with the target $Y$. 

We argue that information gain can be used to measure the effectiveness of a query in advancing the attack process. Given the context $C_{i-1}$ and the current candidate query $q^s (q^s \gets M_{s}(C_{i-1},Q))$, the information gain is defined as:
\begin{equation}\label{e:ig1}
    IG(C_{i-1},q^s) = H(r_{tgt} \mid C_{i-1}) - H(r_{tgt} \mid C_{i-1},q^s),
\end{equation}
where $r_{tgt}$ is the response of the target query $Q$. The conditional entropy $H(r_{tgt} \mid C_{i-1})$ represents the uncertainty of the response to the target query $Q$, given the context $C_{i-1}$. Similarly, the conditional entropy $H(r_{tgt} \mid C_{i-1},q^s)$ denotes the uncertainty of the response $r_{tgt}$ to the target query $Q$, conditioned on both the context $C_{i-1}$ and the current seed query $q^s$. These two terms can be respectively calculated using \Eref{e:ig2} and \Eref{e:ig3}:
\begin{multline}\label{e:ig2}
    H(r_{tgt} \mid C_{i-1}) = \\
    -\sum\limits_{r_{tgt} \in \mathbb{R}_{tgt}} 
    p(r_{tgt} \mid C_{i-1})\log p(r_{tgt} \mid C_{i-1}).
\end{multline}

\begin{multline}\label{e:ig3}
    H(r_{tgt} \mid C_{i-1},q^s) = \\
    -\sum\limits_{r_{tgt} \in \mathbb{R}_{tgt}}p(r_{tgt} \mid C_{i-1},q^s)\log p(r_{tgt} \mid C_{i-1},q^s).
\end{multline}

Computing information gain accurately through \Eref{e:ig1} presents significant computational challenges, primarily in modeling the conditional probability distributions $H(r_{tgt} \mid C_{i-1})$ and $H(r_{tgt} \mid C_{i-1},q^s)$. The complexity arises from the need to handle vast state and response spaces across multiple conversation turns, with probability distributions that evolve dynamically throughout the dialogue. To address these computational challenges, we leverage LLMs as probability estimators to approximate the conditional distributions required for information gain calculation, which significantly reduces computational complexity. Further details are provided in \Sref{sec:details}. The seed query that achieves the maximum $IG(C_{i-1}, q^s)$ is used as the candidate query $q^c$ and is further processed by the self-play module.

\subsubsection{Self-play}
Despite GE filtering, queries may still fail when interacting with the victim model. Therefore, we implement a self-play (SP) module to further optimize these candidates.

Inspired by game theory where an entity improves by competing against itself \cite{nash,samuel}, SP leverages that both shadow and victim models are instantiated from the same source. This allows the shadow model to better predict victim responses through self-play, leading to more efficient query optimization.

Let $M_{s}$ and $M_{v^{'}}$ (where $M_{v^{'}}$ simulates the victim model) be the two players in self-play. Given the current state $s$ and the candidate query $q^c$, the goal of $M_{s}$ is to maximize the probability that $M_{v^{'}}$ returns a non-rejection response (denoted as $r_{c} \notin R_{rej}$). The utility function can be formulated as follows:
\begin{equation}
u_{M_{s}}(s,q^c,r^c)=
\begin{cases}
1,&  r^c \notin R_{rej}.\\
0,&  r^c \in R_{rej}.
\end{cases}
\end{equation}

With the strategy of $M_{v^{'}}$ defined as $\pi_{M_{v^{'}}}(r \mid s, q_{c})$, representing the probability distribution of generating response $r^c$ to query $q^c$ in state $s$, $M_{s}$ employs its current conversation strategy $\pi_{M_{s}}(q^c \mid s)$ and the simulated strategy $\pi_{M_{v^{'}}}(r^c \mid s, q^c)$ to predict the counterpart's response and compute the expected utility as follows:
\begin{equation}
    U_{M_{s}}(s,q^c,\pi_{M_{v^{'}}})=\mathbb{E}_{r \sim \pi_{M_{v^{'}}}}[u_{M_{s}}(s,q^c,r^c)].
\end{equation}

During self-play, $M_{s}$ adaptively adjusts its strategy to maximize the expected utility for a given query $q^c$, satisfying:
\begin{equation}
    q^{*} = \arg \max_{q^c \in Q} U_{M_{s}}(s,q^c,\pi_{M_{v^{'}}}).
\end{equation}

The optimized query $q^{*}$ obtained in this module is used as the actual query for state transition in ASM (\ie, interacting with the victim model).



\subsubsection{Rejection Feedback}

While GE and SP balance the progression of the attack and the likelihood of positive responses, the uncertainty of LLM outputs \cite{unc1,unc2} can still cause state transition failures in the ASM. To mitigate this issue, we propose the rejection feedback (RF) module.

RF is activated when a state transition failure is detected in the ASM, signaling that the current query did not lead to a successful state transition. Specifically, assuming the latest failed interaction occurs in the $i^{\text{th}}$ dialogue, RF utilizes the shadow model to analyze the context $C_{i-1}$ and combines it with the corresponding query-response pair $(q_{i},r_{i})$. Through a comprehensive analysis, the shadow model diagnoses the underlying causes of latest query failure and generates refined query $q^r$  by incorporating current contextual information. Formally, this process can be represented as $q^r = M_v(C_{i-1},q_{i},r_{i})$. The process is driven by a CoT-enhanced prompt, with the complete prompt provided in \Sref{sec:cot}.

\subsection{Overall Attack}
The attack begins by initializing the ASM reasoning states. In each turn, the shadow model generates seed queries that are refined through gain-guided exploration and self-play optimization. Successful queries advance the attack to the next state, while failed attempts trigger query refinement through the rejection feedback module. This process iterates until reaching the final state, maintaining a natural reasoning flow while pursuing the attack goal.