\section{Introduction}
\begin{figure}[h] 
    \centering 
    \includegraphics[width=0.49\textwidth]{imgs/demo.pdf} 
    \caption{Illustration of \method{}. \method{} transforms the harmful query into a benign reasoning task and processes it over subsequent conversation turns. During this process, the LLM gradually engages in step-by-step reasoning, ultimately leading to self-jailbreak.}
    \label{fig:compare} 
\end{figure}

LLMs have garnered widespread attention due to their remarkable ability to perform diverse tasks \cite{llm1,llm2,llm3}. However, studies have shown that LLMs can also generate unsafe or harmful content when prompted in certain ways \cite{recent1,recent2,recent3}. This vulnerability can be exploited through \emph{jailbreak attacks}\textemdash carefully crafted prompts that bypass alignment constraints and elicit unintended responses \cite{gcg,autodan,nba,dlp}. Although harmful, jailbreak attacks \cite{gcg,bap, li2024semantic, ying2024jailbreak, zhang2024lanevil} serve as a key red-teaming approach for assessing the risk of LLMs~\cite{liang2024vl, liang2024revisiting, liang2023badclip} generating unsafe content.

Currently, these attacks can be broadly categorized into single-turn and multi-turn jailbreaks. Single-turn attacks attempt to bypass safety mechanisms within a single interaction \cite{gcg,autodan,lapid2023open,psy,psy1,psy2}, whereas multi-turn jailbreaks exploit the interactive nature of LLMs by engaging them in iterative dialogues that lead to unsafe outputs \cite{mpsy2,mutliattack2,mpsy1,mutliattack5}. Compared to single-turn attacks, multi-turn jailbreaks simulate real-world human interactions and can expose critical safety blind spots, thereby attracting extensive interest \cite{mutliattack2,mutliattack5}. However, existing multi-turn jailbreak methods often struggle to maintain a balance between semantic coherence and attack effectiveness. In other words, they either cause benign semantic drift (where the conversation deviates from the original harmful objective) or fail to bypass alignment constraints, thereby limiting their overall attack performance.

To address this, we propose Reasoning-Augmented ConvErsation (\method{}), a jailbreak framework that exploits LLMs' strong reasoning capabilities \cite{reasonllm1,reasonllm2} by reformulating harmful queries into benign reasoning tasks. These benign and complex reasoning tasks are carefully designed such that their completion inherently leads the model to generate harmful content, effectively compromising safety alignment. To structure this process, we introduce an Attack State Machine (ASM) reasoning framework based on a finite state machine \cite{fsmbase2,fsmbase1}, which organizes jailbreaks into a sequence of reasoning states and transitions, ensuring semantic alignment and coherence. Building on this framework, we design gain-guided exploration, self-play, and rejection feedback modules to preserve attack semantics, enhance effectiveness, and sustain reasoning-driven attack progression. Specifically, gain-guided exploration selects queries that remain semantically aligned with the target while extracting useful information to ensure steady progress. Self-play simulates rejection responses within a shadow model, refining queries in advance and increasing success rates against the victim model. Rejection feedback adapts failed queries into alternative reasoning tasks, enabling rapid recovery and sustained attack stability. By combining these modules, \method{} enables a structured and adaptive jailbreak method that is both highly effective yet challenging to mitigate. \Fref{fig:compare} illustrates the attack diagram of \method{}.

We conducted extensive experiments on multiple LLMs to evaluate the effectiveness of \method{} in multi-turn jailbreak scenarios. The results demonstrate that \method{} achieves attack success rates (ASRs) of up to 96\%, highlighting its capability in complex conversational settings. Notably, our approach attained ASRs of 82\% and 92\% against the leading commercial models, OpenAI o1 and DeepSeek R1, respectively. These findings underscore the potency of reasoning-driven jailbreak attacks and the pressing need for stronger safety mechanisms. We hope our work will contribute to advancing LLM safety research and improving awareness of the potential misuse of LLMs' reasoning capabilities.