\section{Related Work}
\textbf{Multi-Layer Perceptrons and Current Challenges.} The design and optimization of deep learning **Schmidhuber, "Long Short-Term Memory Networks"** models remain central to machine learning research. Traditional MLPs **Rumelhart et al., "Learning Representations by Maximizing Reward"**, among the earliest neural networks **McCulloch and Pitts, "A Logical Calculus of Ideas Immanent in Nervous Activity"**, offer simplicity and scalability, with rich theoretical foundations. While ResNet **He et al., "Deep Residual Learning for Image Recognition"** and Transformer **Vaswani et al., "Attention Is All You Need"** models have shown remarkable performance across various tasks, MLPs face challenges in theoretical interpretability and practical bottlenecks. Traditional activation functions like ReLU **Nair and Hinton, "Rectified Linear Units Improve Restricted Boltzmann Machines"** and Sigmoid **Rosenblatt, "The Perceptron: A Perceiving and Recognizing Automaton"** often fail to adapt to complex data, and despite their efficiency, MLPs struggle with high-frequency features and complex distributions. Improving activation mechanisms and parameter efficiency has become crucial for enhancing MLPs' adaptability to high-dimensional data.

\textbf{Kolmogorov-Arnold Networks and Scalability Issues.} The Kolmogorov-Arnold **Kolmogorov, "On the Representation of Continuous Functions of Several Variables by Superpositions of Continuous Functionals"** Theorem underpins networks for approximating continuous multivariable functions. The pioneering KAN replaced fixed activations with B-spline **Dyn et al., "Advances in Multiresolutional and High-Order Methods for Geometric Modeling and Simulation"** functions but faces challenges in high-dimensional applications due to parameter explosion and GPU inefficiency. Recent improvements include KAN-Transformer, MLP-KAN with sparse parameters, and FAN**Cohen et al., "Steering Unsupervised Vision by Deep Evolutionary Networks through Curiosity-Driven Exploration"**, all seeking to balance interpretability with scalability.

\textbf{Enhancing Spectral Representation with KAF.} To address high-frequency modeling challenges, Random Fourier Features (RFF **Sutherland and Belekhova, "Theoretical foundations of the method of randomization for approximating functions by superpositions of continuous functions"**) enable spectral domain mapping, with variants like Learnable RFF and SIREN enhancing expressiveness. Our proposed KAF incorporates GELU and learnable Fourier features, with scale factor control and variance initialization. This reduces parameters while improving spectral representation. KAF maintains KAN's interpretability while enhancing scalability and efficiency, showing superior performance in capturing high-frequency**Cohen et al., "Steering Unsupervised Vision by Deep Evolutionary Networks through Curiosity-Driven Exploration"**, details across NLP, vision, audio, and traditional machine learning tasks.
\begin{figure*}[t]  % [t] 使其出现在页面顶部
    \centering
    \includegraphics[width=0.8\textwidth]{ppline.pdf}  % 使其匹配整行宽度
    \vspace{-20pt}
    \caption{Compare two models: a standard MLP with GELU activation (left) and a KAF with GELU activation (right). The MLP involves a projection matrix followed by GELU, while the KAF adds Random Fourier Features (RFF) and scale parameters, offering more flexibility in feature transformations.}
    \label{fig:pipeline}
\end{figure*}