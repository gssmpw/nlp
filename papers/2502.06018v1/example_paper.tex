%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subfigure}
\usepackage{placeins}
\usepackage{xcolor}  
\usepackage{afterpage} % 在导言区加入
\usepackage{booktabs} % for professional tables
\usepackage{amsmath} % 数学支持
\usepackage{amssymb} % 符号支持
\usepackage{hyperref} % 其他功能（可选）


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Kolmogorov-Arnold Fourier Networks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{JuSheng Zhang}{1}
\end{icmlauthorlist}

\icmlaffiliation{a}{Your Affiliation}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\begin{abstract}
Although Kolmogorov-Arnold \cite{KA1933} based interpretable networks (KAN) have strong theoretical expressiveness, they face significant parameter explosion and high-frequency feature capture challenges in high-dimensional tasks. To address this issue, we propose the Kolmogorov-Arnold-Fourier Network (KAF), which effectively integrates trainable Random Fourier Features (RFF \cite{fuliyebiaozhun,fuliyekaishan}) and a novel hybrid GELU-Fourier activation mechanism to balance parameter efficiency and spectral representation capabilities. Our key technical contributions include: (1) merging KAN's dual-matrix structure through matrix association properties to substantially reduce parameters; (2) introducing learnable RFF initialization strategies to eliminate spectral distortion in high-dimensional approximation tasks; (3) implementing an adaptive hybrid activation function that progressively enhances frequency representation during the training process. Comprehensive experiments demonstrate the superiority of our KAF across various domains including vision, NLP, audio processing, and differential equation-solving tasks, effectively combining theoretical interpretability with practical utility and computational efficiency. We will release the source code of our method in accordance with the review policy.
\end{abstract}

\section{Introduction}
The interpretability of deep neural networks \cite{sjwl01,sjwl02} has long been one of the core challenges in the field of machine learning. The Kolmogorov-Arnold\cite{kan,KA1933} theorem states that any continuous multivariate function can be represelinented through a combination of univariate functions \cite{GW,GWBJ}. This theory provides significant inspiration for the design of interpretable neural network architectures. Based on this theory, Kolmogorov-Arnold Networks (KAN) \cite{kan,zheKAN} have been proposed, which replace the fixed activation functions in traditional multilayer perceptrons (MLPs \cite{MLP}) with learnable B-spline \cite{B-spline} basis functions, theoretically demonstrating strong expressive potential and flexibility. By introducing trainable nonlinear activation functions, KAN enables the network to dynamically adjust the shape of the activation functions according to the characteristics of the data, thereby enhancing the adaptability and performance of the model.

However, despite the significant theoretical advantages of KAN, its practical application faces two fundamental issues that severely limit its generalization and adoption in high-dimensional tasks: Inefficient Parameter Utilization:
The dual-matrix architecture of KAN (i.e., the activation function matrix and the B-spline coefficient matrix) leads to a rapid increase in the number of parameters. Compared to traditional MLPs, where the parameter count scales with input × output + bias, KAN's parameter count grows several times larger. This makes it challenging to apply KAN to high-dimensional tasks such as computer vision. The explosion in parameters \cite{gaoweipingyu,GW,GWBJ} not only increases the storage and computational costs of the model but also significantly reduces the efficiency of both training and inference \cite{canshu,canshu2}.
The B-spline \cite{B-spline,byt,byt2,byt3,byt4}basis functions employed by KAN exhibit inherent spectral limitations when performing function approximation in high-dimensional spaces. The smoothness of B-spline basis functions makes it difficult to accurately capture high-frequency components of signals, leading to suboptimal performance when processing data with rich spectral features, such as natural images or audio waveforms. This limitation in spectral representation capability adversely affects the model's performance and stability in practical applications\cite{gaoweipingyu}. This dilemma creates a paradox between theory and practice: although KAN theoretically encompasses all functionalities of MLPs, its inefficiency and spectral distortion issues force practitioners to make a trade-off between interpretability and scalability.

To address the aforementioned issues, the key challenge is balancing the inherent trade-off between model interpretability and parameter efficiency, which has long plagued traditional neural networks. This paper make an attempt to fundamentally redefine the traditional KAN paradigm~\cite{fuliyekaishan}.  Specifically, this paper introduces an innovative neural network architecture—\textbf{Kolmogorov-Arnold-Fourier Networks (KAF)}, which employs Fourier domain reparameterization and dynamic activation evolution, aiming to bridge the gap between interpretability and parameter efficiency. Our main contributions include:
%\setlength{\itemindent}{0em}
\begin{itemize}
    \item By leveraging the associative property of matrix multiplication, we merge the two large matrices ($WA$ and $WB$) of KAN, thereby reducing the parameter complexity from $O(d_{\text{in}} \times d_{\text{out}} \times (G + K + 3))$ to $O(d_{\text{in}} \times d_{\text{out}})$, while preserving the expressive power of the model \cite{juzhen,juzhen2}. This approach not only effectively reduces the number of parameters but also enhances the model's scalability in high-dimensional tasks.
    \item We replace the traditional B-spline basis functions with trainable \textbf{Random Fourier Features} to eliminate the need for the spline coefficient matrix. An initialization strategy based on the Central Limit Theorem ($\sigma = 1.64$) aligns the RFF spectrum with the prior knowledge of natural signals, avoiding spectral leakage issues. This significantly enhances the model's spectral fidelity and expressive power in high-dimensional spaces.
    \item We design a hybrid GELU-Fourier activation function with learnable coefficients $\{a, b\}$. During training, these coefficients are dynamically adjusted through gradient backpropagation, enabling an automatic transition from fixed activation functions to hybrid Fourier-symbolic representations. This dynamic activation mechanism not only optimizes the model's learning trajectory but also ensures the stability and efficiency of the final Fourier-driven inference mode.
\end{itemize}

\section{Related Work}
\textbf{Multi-Layer Perceptrons and Current Challenges.} The design and optimization of deep learning \cite{deit}models remain central to machine learning research. Traditional MLPs\cite{MLP}, among the earliest neural networks \cite{sjwl02}, offer simplicity and scalability, with rich theoretical foundations. While ResNet \cite{resnet18} and Transformer \cite{transform} models have shown remarkable performance across various tasks, MLPs face challenges in theoretical interpretability and practical bottlenecks. Traditional activation functions like ReLU \cite{ReLU1,ReLU2} and Sigmoid \cite{Sigmoidjihuo} often fail to adapt to complex data, and despite their efficiency, MLPs struggle with high-frequency features and complex distributions. Improving activation mechanisms and parameter efficiency has become crucial for enhancing MLPs' adaptability to high-dimensional data.

\textbf{Kolmogorov-Arnold Networks and Scalability Issues.} The Kolmogorov-Arnold \cite{2019duichengkanl} Theorem underpins networks for approximating continuous multivariable functions. The pioneering KAN replaced fixed activations with B-spline \cite{B-spline} functions but faces challenges in high-dimensional applications due to parameter explosion and GPU inefficiency. Recent improvements include KAN-Transformer, MLP-KAN with sparse parameters, and FAN\cite{FAN} with Fourier activations, all seeking to balance interpretability with scalability.

\textbf{Enhancing Spectral Representation with KAF.} To address high-frequency modeling challenges, Random Fourier Features (RFF \cite{suijifuly,suijifulye2}) enable spectral domain mapping, with variants like Learnable RFF and SIREN enhancing expressiveness. Our proposed KAF incorporates GELU and learnable Fourier features, with scale factor control and variance initialization. This reduces parameters while improving spectral representation. KAF maintains KAN's interpretability while enhancing scalability and efficiency, showing superior performance in capturing high-frequency\cite{SIRENgaoping} details across NLP, vision, audio, and traditional machine learning tasks.
\begin{figure*}[t]  % [t] 使其出现在页面顶部
    \centering
    \includegraphics[width=0.8\textwidth]{ppline.pdf}  % 使其匹配整行宽度
    \vspace{-20pt}
    \caption{Compare two models: a standard MLP with GELU activation (left) and a KAF with GELU activation (right). The MLP involves a projection matrix followed by GELU, while the KAF adds Random Fourier Features (RFF) and scale parameters, offering more flexibility in feature transformations.}
    \label{fig:pipeline}
\end{figure*}

\section{Methodology}

\subsection{Kolmogorov-Arnold Theorem}

The Kolmogorov-Arnold \cite{fuliyekaishan,KA1933} theorem, proposed by Soviet mathematicians Vladimir Arnold and Andrey Kolmogorov in the 1950s, states that any continuous \cite{2020jinsi,2021KA} multivariate function \( f: [0,1]^d \rightarrow \mathbb{R} \) can be represented as a superposition of univariate functions:

\vspace{-0.5cm}
\[
f(x_1, x_2, \dots, x_d) = \sum_{q=1}^{2d+1} \Phi_q \left( \sum_{p=1}^{d} \phi_{q,p}(x_p) \right),
\]
where \(\Phi_q: \mathbb{R} \rightarrow \mathbb{R}\) and \(\phi_{q,p}: [0,1] \rightarrow \mathbb{R}\) are univariate continuous functions. This theorem provides a theoretical foundation for dimensionality reduction in high-dimensional function approximation.

In \cite{shuxue1,shuxue2}, the theorem suggests that high-dimensional functions can be captured through low-dimensional \cite{gaoweipingyu} transformations, resembling the hierarchical structure of neural networks. However, compared to traditional neural networks, the number of parameters required by the Kolmogorov-Arnold theorem may lead to a more lengthy and resource-consuming training process. Due to the non-smoothness of certain low-dimensional functions and the difficulties in training optimization, this theorem has not been practically applied in the field of neural networks for a long time.

\subsection{Kolmogorov-Arnold Network (KAN)}

Although the Kolmogorov-Arnold \cite{fuliyekaishan}theorem was proposed quite early, (KAN \cite{kan}) is proposed according to this theorem, demonstrating that this structure can, in a sense, serve as an alternative to traditional MLP models. In the KAN network, each layer can be represented by the following formula:

\vspace{-0.5cm}
\[
f(\mathbf{x}) = \Phi \circ \mathbf{x} = \left[ \sum_{i=1}^{d_{in}} \phi_{1,i}(x_i) \quad \cdots \quad \sum_{i=1}^{d_{in}} \phi_{d_{out},i}(x_i) \right],
\]
where \(\Phi\) is a matrix of basis functions. This formula aligns with the form of the Kolmogorov-Arnold theorem. However, in practical applications, they chose B-spline \cite{yangtiao} basis functions as the basis functions $\phi_{q,p}$, and added an external activation function $SILU$ to guide the update of the KAN layer\cite{rsjihuo,Sigmoidjihuo}. The formula can be expressed as

\vspace{-0.5cm}
\[
\phi(x) = w_h \text{silu}(x) + w_s \text{spline}(x),
\]
\[
\text{spline}(x) = \sum_i c_i B_i(x).
\]
\vspace{-0.5cm}

Among them, $\Phi$ represents the basis function matrix, where B-spline basis functions and the SiLU activation function were used. However, KAN suffers from excessive parameter growth, with a parameter count of $d_{\text{out}} \times d_{\text{out}} \times (G + K + 3) + d_{\text{out}}$, far exceeding MLP’s $d_{\text{in}} \times d_{\text{out}} + d_{\text{out}}$, while also being computationally inefficient on GPUs and failing to capture high-frequency components, limiting its practical applicability.


\subsection{KAF: Kolmogorov-Arnold Fourier Network}
In the previous discussion, we pointed out that traditional networks based on the Kolmogorov-Arnold theorem (KAN) often face multiple challenges in practical applications. To address these issues, we propose an alternative approach—KAF (Kolmogorov-Arnold Fourier Network). By replacing B-spline basis functions with Random Fourier Features (RFF\cite{fuliye2,fuliyebiaozhun,fuliyekaishan}), which are more efficient for GPU acceleration, and introducing hybrid spectral correction for the activation functions, the network retains the advantages of the Kolmogorov-Arnold theory while achieving training efficiency and inference speed closer to that of MLPs \cite{MLP}. This section provides a detailed explanation of the overall architecture of the KAF network, the utilization of Random Fourier Features within the network, the design and scaling principles of the GELU-Fourier hybrid activation function, as well as the RFF weight initialization strategy and the theoretical justification for $\sigma=1.64$.

\textbf{Overall Architecture.}
In the overall framework of KAF, we follow the core idea of the Kolmogorov-Arnold theorem, which approximates high-dimensional target functions through the composition of several low-dimensional learnable functions. Unlike the KAN network, which directly utilizes B-spline basis functions, KAF employs Random Fourier Features (RFF) in each layer to perform a nonlinear mapping of the input, and then uses linear transformations to achieve the composition of the "outer function" and the "inner function." Specifically, the core computational process of each KAF layer can be formulated as:
\begin{equation}
\small % 调整字体大小
\mathbf{h}^{(l)} = 
\underbrace{\mathbf{W}^{(l)}}_{\text{outer function}}
\left(
\underbrace{\mathbf{a}^{(l)} \odot \text{GELU}(\tilde{\mathbf{x}}^{(l)}) 
+ \mathbf{b}^{(l)} \odot \tilde{\phi}(\tilde{\mathbf{x}}^{(l)})}_{\text{inner function composition}}
\right) 
+ \mathbf{c}^{(l)},
\end{equation}
where:  
$\tilde{\mathbf{x}}^{(l)} = \text{LayerNorm}(\mathbf{x}^{(l)})$ is the normalized input at layer $l$;  
$\tilde{\phi}(\cdot)$ represents the nonlinear mapping based on Random Fourier Features (RFF) (detailed in Section 3.3.2);  
$\mathbf{a}^{(l)}, \mathbf{b}^{(l)} \in \mathbb{R}^n$ are learnable scaling parameters, used to modulate the contributions of GELU activation and RFF features, respectively;  
$\mathbf{W}^{(l)} \in \mathbb{R}^{m \times n}$ is the linear transformation weight, and $\mathbf{c}^{(l)} \in \mathbb{R}^m$ is the bias term.  

By stacking multiple layers of the above transformation, the KAF network constructs an efficient multi-layer approximation structure. Since RFF has excellent parallelism on GPUs, this structure avoids the high computational burden of B-spline basis functions, significantly improving training and inference efficiency while maintaining strong function approximation capabilities.

\textbf{Random Fourier Features (RFF).}
Given an input space $\mathcal{X} \subseteq \mathbb{R}^d$, we define the Random Fourier Feature (RFF\cite{fuliyebiaozhun}) mapping as a learnable embedding from the input space to a Reproducing Kernel Hilbert Space (RKHS\cite{RKHS,RKHS2,RKHS3}). For any input vector $x \in \mathcal{X}$, the feature mapping is formally defined as:\begin{equation}
\small
z(x;W,b)=\sqrt{\frac{2}{m}}\big[\cos(\langle x,W\rangle+b)\oplus\sin(\langle x,W\rangle+b)\big]\in\mathbb{R}^{2m},
\end{equation}
where $W\in\mathbb{R}^{d\times m}$ and $b\in\mathbb{R}^m$.
Here, $\langle \cdot, \cdot \rangle$ denotes the Euclidean inner product, and $\oplus$ represents the vector concatenation operation. The frequency matrix $W = [w_1, \dots, w_m]$ is initialized according to an input-dimension-adaptive spectral distribution: $w_{ij} \sim \mathcal{N}(0, \sigma^2/d)$, where $\sigma^2$ represents the empirical variance of the input data. The phase shift $b$ is sampled from a uniform distribution $b_i \sim \mathcal{U}[0,2\pi]$, which ensures phase diversity, a crucial property for capturing local features of signals.See Appendix ~\ref{Appendix B} for more information on RFF convergence, gradient calculation, and initialization strategies.

This mapping comes with the following theoretical guarantees:

- Translation Invariance: For any $x, y \in \mathcal{X}$, as $m \to \infty$, we have $\mathbb{E}[z(x)^T z(y)] \to e^{-\frac{\|x-y\|^2}{2\sigma^2}}$.

- Differentiability: The partial derivatives $\frac{\partial z}{\partial W}$ and $\frac{\partial z}{\partial b}$ have analytical expressions, enabling end-to-end differentiation.


\textbf{GELU-Fourier Hybrid Activation.}

\textit{Design Motivation}: To balance low-frequency smoothness and high-frequency representation capability, we propose a hybrid activation function:

\begin{equation}
\mathcal{H}(\boldsymbol{x}) = 
\underbrace{\alpha \odot \mathrm{GELU}(\boldsymbol{x})}_{\mathclap{\text{Low-Frequency Basis}}} 
\mathrel{\hphantom{=}} + \mathrel{\hphantom{=}}
\underbrace{\beta \odot \boldsymbol{V}\psi(\boldsymbol{x})}_{\mathclap{\text{High-Frequency Correction}}}
\end{equation}
where $\alpha, \beta \in \mathbb{R}^d$ are learnable channel-wise scaling factors, $\boldsymbol{V} \in \mathbb{R}^{d \times 2k}$ is the frequency-domain projection matrix, and $\odot$ represents element-wise multiplication.
\textbf{Initialization Strategy of KAF}: 
\begin{equation}
\scalebox{0.9}{$
\alpha^{(0)} \gets \boldsymbol{1}, \quad 
\beta^{(0)} \gets \epsilon \boldsymbol{1}, \quad (\epsilon = 10^{-2}), \quad
\boldsymbol{V}_{ij}^{(0)} \sim \mathcal{N}(0, 0.01)
$}
\end{equation}
The dynamic property of this initialization manifests in the following way: At the early stage of training, the small initialization of the high-frequency component $\beta$ ensures that its norm is much smaller than that of the low-frequency component $\alpha$, prioritizing the learning of low-frequency features. As training progresses, the natural growth of weights allows the norm of $\beta$ to increase approximately proportionally to the training time $t$, thereby gradually enhancing the representation of high-frequency features.

\textbf{Implementation of the Kolmogorov-Arnold Architecture and RFF Initialization.}

\textit{Theorem Definition}: The Kolmogorov-Arnold representation theorem states that any continuous function $f \in C([0,1]^d)$ can be expressed as a finite composition of univariate functions:

\begin{equation}
f(\boldsymbol{x}) = \sum_{q=0}^{2d} \Phi_q\left( \sum_{p=1}^d \phi_{q,p}(x_p) \right),
\end{equation}
where $\phi_{q,p}: \mathbb{R} \to \mathbb{R}$ are univariate nonlinear functions, and $\Phi_q: \mathbb{R} \to \mathbb{R}$ are composition functions.

\textit{Architecture Implementation}: We modularize the neural network to efficiently approximate this mapping, establishing the following correspondences:\begin{equation}
\begin{aligned}
\phi_{q,p}(x_p) 
    &\mapsto 
    \underbrace{\mathrm{GELU}\bigl(w_p^{(q)}x_p + b_p^{(q)}\bigr)}_{\mathclap{\text{Low-Frequency Basis}}} 
    + \boldsymbol{\beta}_q^\top \mkern-2mu 
    \underbrace{\psi_{\text{RFF}}(x_p)}_{\mathclap{\text{High-Frequency Basis}}}, 
    \\[1ex]  
\Phi_q(\cdot) 
    &\mapsto 
    \boldsymbol{\alpha}_q^\top \mkern-2mu \text{Linear}(\cdot).
\end{aligned}
\end{equation}

Here, $\psi_{\text{RFF}}(x_p) = [\cos(\omega_1x_p+\theta_1), \sin(\omega_1x_p+\theta_1), \dots]$ represents the Random Fourier Features (RFF), and $\boldsymbol{\alpha}_q, \boldsymbol{\beta}_q \in \mathbb{R}^k$ are learnable modulation parameters.

\textit{Spectral Complementarity Mechanism}:
- GELU Properties: The activation function $\sigma(wx+b)$ provides a smooth gating effect in the low-frequency domain, satisfying $\mathbb{E}[\sigma(wx)] \propto \mathcal{N}(0,1/\sqrt{2})$.
- RFF Enhancement: The use of mixed-frequency bases $\{\cos(\omega_mx+\theta_m)\}_{m=1}^M$ expands spectral coverage.
- Dynamic Balancing: The learnable parameters $\boldsymbol{\alpha}, \boldsymbol{\beta}$ enable an adaptive trade-off:

\begin{equation}
\tilde{\phi}(x) = \alpha \cdot \mathrm{GELU}(x) + \beta \cdot \psi_{\text{RFF}}(x),
\end{equation}
where the initial values are set to $\alpha^{(0)}=1$ and $\beta^{(0)}=10^{-2}$ to ensure training stability.

\textit{RFF Initialization Strategy}: To fully leverage spectral complementarity, we adopt a refined initialization scheme:
- Frequency Matrix $\boldsymbol{W}$: To ensure spectral balance and avoid bias towards low or high frequencies, we initialize $\boldsymbol{W}$ using a scaled normal distribution\cite{chushihua,chushihua2}:
\begin{equation}
\omega_{ij} \sim \mathcal{N}\left(0,\, \frac{\gamma}{\sqrt{d_{\text{in}} \cdot \mathbb{E}[\|\sigma(x)\|^2]}}\right),
\end{equation}
This initialization is designed to align with the spectral distribution of the input data. The denominator normalizes the standard deviation based on input dimensionality $d_{\text{in}}$ and the expected squared norm of the activation function $\mathbb{E}[\|\sigma(x)\|^2]$, ensuring a stable variance propagation during training. For the GELU activation function, why $\sigma(x)=1.64$ will be proved later in Appendix ~\ref{prof:1.64}.

- Phase Shift $\boldsymbol{b}$: Uniformly sampled to cover a complete period,
\begin{equation}
b_i \sim \mathcal{U}(0,\,2\pi).
\end{equation}
- Linear Projection Layer: Initialized using Xavier initialization,
\begin{equation}
\boldsymbol{V}_{ij} \sim \mathcal{U}\left(-\sqrt{6/(d_{\text{in}}+d_{\text{out}})},\,\sqrt{6/(d_{\text{in}}+d_{\text{out}})}\right).
\end{equation}

\textbf{Parameter and FLOPs Comparison.}
To evaluate the scale of parameters and computational overhead of KAF, we compare the number of parameters and floating-point operations (FLOPs) for KAF, KAN, and MLP in a single layer setting.

Table~\ref{tab:param_flops} summarizes the parameter count and FLOPs for each model. KAN exhibits the highest parameter count due to its recursive B-spline computations, while KAF, by leveraging Random Fourier Features (RFF), achieves a balance between parameter efficiency and spectral representation. MLP remains the simplest in terms of computation.
For the detailed derivation of these calculations, please refer to Appendix ~\ref{Appendix A}.


\section{Experiments}

The objective of this experiment is to evaluate the performance of mainstream models when their MLP\cite{MLP} or KAN components are replaced with KAF. By maintaining consistent parameters, we conducted experiments across a variety of tasks including simple visual tasks, NLP tasks, audio tasks, and machine learning tasks, utilizing models such as ResNet-18 \cite{resnet18}, DeiT \cite{deit} (from the MLP-KAN architecture), MLPmixer \cite{MLP-Mixer}, and GPT-2 \cite{GTP2}. Additionally, we tested the performance of KAF in function fitting and solving differential equations. All experiments employed either the Adam \cite{kingmaadam} optimizer, with learning rates appropriately selected according to the specific task. The experimental environment was set up with RTX 4090D GPU.

\subsection{Comprehensive Evaluation Based on Kanbefair}
Based on Kanbefair \cite{kanbase}, we conducted a comprehensive evaluation of KAF on vision \cite{VIT}, NLP \cite{GTP2}, audio, and machine learning tasks to compare its performance with existing models. We selected MLP (with GELU activation), KAN, FAN \cite{FAN}, and GPKAN \cite{KAT} for experimentation. Note that we use the \textbf{original input} by default in all experiments, leaving \textbf{layernorm disabled}.


\subsubsection{Experimental Setup}
To account for differences in model convergence speeds, KAN was trained for 100 epochs, while other models were trained for 40 epochs. During training, the maximum test accuracy was recorded as the primary evaluation metric, focusing on classification tasks across 8 vision datasets, 2 NLP datasets, 2 audio datasets, and 4 additional machine learning datasets. For all models, hidden layer sizes were explored in the range of 2, 4, 8, 16, 32, 64, 128, 256, 512, and 1024. For KAF, the key parameters included 9 grids, an activation expectation of 1.64, and GELU as the activation function. For KAN, the number of B-spline grids was set to 3, 5, 10, or 20; B-spline degrees were configured as 2, 3, or 5; and the B-spline range was [-1, 1], [-2, 2], or [-4, 4]. For MLP, we experimented with both GELU \cite{gelus} and ReLU \cite{ReLU1,ReLU2} activations. FAN’s p\_ratio was set to 0.25 to regulate the proportion of preserved information during transformation, and GPKAN used GELU-based initialization for enhanced convergence.
\subsubsection{Experimental Results}
\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{vision_num_parameter.pdf}
    \vspace{-10pt}
    \caption{Compare the performance of different models (KAN, MLP, GPKAN, FAN, KAF) across several datasets (MNIST, EMNIST, FMNIST, KMNIST, Cifar10, Cifar100, SVHN). The results show that KAF generally achieves higher accuracy with fewer parameters.}
    \label{fig:accuracy_params_vision}
\end{figure*}

As shown in Figure~\ref{fig:accuracy_params_vision}, we conducted a systematic comparison of KAF and several baseline models (e.g., ResNet, ViT, MLP-Mixer) on vision datasets, including MNIST \cite{lecun1998gradient}, EMNIST \cite{emnist}, KMNIST \cite{clanuwat2018deep}, CIFAR 10/100 \cite{krizhevsky2009learning}, and SVHN \cite{netzer2011reading}. 
The results in Figure~\ref{fig:accuracy_params_vision} demonstrate that KAF consistently achieves the highest accuracy under the same parameter settings across different scales. Notably, on more challenging tasks such as CIFAR10 and SVHN, KAF exhibits significant accuracy improvements compared to other models. These results highlight KAF's robustness in addressing high-dimensional data challenges.
In addition to vision tasks, Figure~\ref{fig:accuracy_params_ml}(see Appendix ~\ref{appendix D}) evaluates KAF’s performance on NLP, audio, and traditional machine learning datasets. KAF achieves better results on these three tasks than other models. This shows that KAF is not only suitable for visual tasks but also for text and audio tasks.
The result highlights KAF's superior versatility and generalization capabilities across different tasks.
\begin{table*}[t]
    \centering
    \renewcommand{\arraystretch}{1.2} % 调整行距
    \resizebox{\linewidth}{!}{ % 确保 \caption{} 在外部
    \begin{tabular}{l c c c c c | l c c c c c}
        \toprule
        \textbf{Model} & \textbf{Datasets} & \textbf{Feature Mixer} & \textbf{\#Param.} & \textbf{FLOPs} & \textbf{Top-1} 
        & \textbf{Model} & \textbf{Datasets} & \textbf{Feature Mixer} & \textbf{\#Param.} & \textbf{FLOPs} & \textbf{Top-1} \\
        \midrule
        ResNet/18 & CIFAR-10 & MLP & 11.1M & 0.56G & 91.19  & MLP\_Mixer/S  & ImageNet1k & MLP & 18.2M & 3.8G & 63.5 \\
        ResNet/18 & CIFAR-10 & KAF & 12.0M & 0.63G & \textcolor{black}{\textbf{91.72}}  & MLP\_Mixer/S  & ImageNet1k & KAF & 18.8M & 4.2G  & \textcolor{black}{\textbf{64.7}} \\
        ResNet/18 & CIFAR-10 & GPKAN & 11.3M & 0.56G & 90.98 & MLP\_Mixer/S  & ImageNet1k & GPKAN & 18.8M & 4.0G  & 62.9 \\
        ResNet/18 & CIFAR-10 & FAN & 8M & 0.42G & 90.69 & MLP\_Mixer/S & ImageNet1k & FAN & 15.7M & 3.2G & 58.2 \\
        ResNet/18 & CIFAR-10 & KAN & Too large & -- & --  & MLP\_Mixer/S & ImageNet1k & KAN & Too large & -- & -- \\
        ViT-T/16 & ImageNet1K & MLP & 5.7M & 1.08G & 72.3 & MLP\_KAN(Deit) & Cifar100 & MLP & 1.3M & 0.12G & 49.0 \\
        ViT-T/16 & ImageNet1K & KAF & 5.9M & 1.12G & \textcolor{black}{\textbf{73.2}} & MLP\_KAN(Deit) & Cifar100 & KAF & 1.4M & 0.15G & \textcolor{black}{\textbf{53.8}} \\ 
        ViT-T/16 & ImageNet1K & GPKAN & 5.7M & 1.13G & 74.6 & MLP\_KAN(Deit) & Cifar100 & KAN & 1.9M & 0.19G & 51.2 \\
        ViT-T/16 & ImageNet1K & FAN & 4.2M & 0.96G & 65.7 & MLP\_KAN(Deit) & Cifar100 & GPKAN & 1.4M & 0.14G & 54.3 \\
        ViT-T/16 & ImageNet1K & KAN & Too large & -- & -- & MLP\_KAN(Deit) & Cifar100 & FAN & 1.0M & 0.1G & 46.7 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Comparison of different models on various datasets with different feature mixers in common vision architectures. Parameters refer to the total model size, and FLOPs are computed for forward propagation. KAN was sometimes excluded due to its excessive parameter size, making training under the same conditions infeasible.}
    \label{tab:model_comparison} % 确保 label 在 caption 之后
\end{table*}
\subsection{Experiments on Using KAF Components in Complex Vision Models}
To comprehensively evaluate the performance of KAF in large-scale vision models after replacing corresponding layers, we assess its impact on accuracy, computation time, and generalization across various models. In this section, we conduct comparative experiments on commonly used large-scale vision models, including ResNet-18, ViT-Tiny, and MLP-Mixer-S/16, as well as the latest model incorporating KAN components, MLP\_KAN (based on DeiT). In each case, we replace the original KAN or MLP modules with KAF, KAN, MLP, GPKAN, and FAN, respectively, to analyze their performance differences.
\subsubsection{Experimental Setup}
The experiments utilize CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning}, and ImageNet-1K for training and testing. ResNet-18 and MLP-Mixer-S/16 are trained for 100 epochs, while ViT-Tiny \cite{Kan_mlp,VIT} and MLP\_KAN are trained for 300 epochs. All other training hyperparameters follow the official recommended settings for each model.
Both KAF and MLP use GELU as the activation function; for KAN, we set the grid size to 5 and the B-spline degree to 3; FAN’s p\_ratio is fixed at 0.25; GPKAN adopts GELU-based initialization; LayerNorm is disabled by default; and Dropout follows each model’s standard setting.

\subsubsection{Experimental Results}
Table~\ref{tab:model_comparison} summarizes performance across multiple datasets when various feature Mixers replace the original modules. KAF generally meets or exceeds baseline accuracy while maintaining a reasonable parameter count and computational cost. For instance, in ResNet-18 on CIFAR-10, KAF achieves 91.72\% Top-1 accuracy (compared to MLP’s 91.19\%), and on ImageNet-1K, it improves MLP-Mixer from 63.5\% to 64.7\%. Although compact approaches like FAN can reduce parameters, they often yield lower accuracy (e.g., 58.2\% on ImageNet-1K). Similarly, with ViT-T/16, KAF’s moderate parameter increment still outperforms MLP, while in MLP\_KAN (DeiT), KAF boosts baseline accuracy from 49.0\% to 53.8\%.
Overall, KAF demonstrates gains and stable training on challenging tasks and architectures, including attention-rich models. These results highlight its potential as a more efficient and effective Mixer replacement in modern vision networks, balancing parameter overhead with performance improvements.

\subsection{Experiments on LLMs with KAF Components}
To evaluate the potential of KAF in language models, we integrated it into the GPT-2 architecture by replacing the Feed-Forward Network (FFN)'s MLP with KAF or KAN. We then trained and evaluated the models on large-scale text datasets, assessing their impact on language modeling quality and model complexity.

\subsubsection{Experimental Setup}
We conducted experiments using OpenWebText and WikiText, two widely used text datasets. The base model was GPT-2 (Small version), where the two-layer MLP in the Feed-Forward Network (FFN) was replaced with KAF or KAN while maintaining the same parameter scale.
All other Transformer configurations\cite{transform}, including multi-head attention, token embeddings, and positional encoding, remained consistent with the official GPT-2 implementation.
\subsubsection{Experimental Results}
\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{22pt} % 调整列间距
    \begin{tabular}{l l c c c}
        \toprule
        \textbf{Model} & \textbf{Dataset} & \textbf{PLL} & \textbf{Training Time} & \textbf{\#Param.} \\
        \midrule
        MLP & WikiText & 184.53 & 20h 37m & 117M \\
        KAF & WikiText & \textcolor{black}{\textbf{180.85}} & 19h 20m & 128M \\
        KAN & WikiText & 39782 & 304h 06m & 478M \\
        MLP & OpenWebText & 151.27 & 60h 57m & 117M \\
        KAF & OpenWebText & \textcolor{black}{\textbf{145.64}} & 52h 45m & 128M \\
        KAN & OpenWebText & 27832 & 960h 19m & 478M \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of GPT-2 based MLP, KAF, and KAN models on WikiText and OpenWebText: Perplexity, training time, and parameter count.}
    \label{tab:llm_comparison}
\end{table*}

Table~\ref{tab:llm_comparison} presents a comparison of GPT-2 using MLP, KAF, and KAN as the FFN components on the WikiText and OpenWebText datasets. The results show that KAF boosts language modeling performance and training efficiency. On WikiText, it reduces PLL from 184.53 to 180.85 while cutting training time from 20h37m to 19h20m, indicating improved performance without significant overhead. In contrast, KAN converges poorly, with PLL escalating to 39,782 and training time rising sharply due to large parameter scales, revealing severe optimization challenges.
A similar pattern emerges on OpenWebText: KAF again surpasses MLP by lowering PLL from 151.27 to 145.64 and further reducing training time, whereas KAN remains unstable (PLL reaching 27,832), reaffirming its vulnerability in large-scale language modeling. Overall, swapping MLP for KAF in GPT-2 consistently enhances language modeling across WikiText and OpenWebText, while preserving reasonable training costs.
The experiment demonstrates that substituting MLP with KAF in GPT-2’s FFN not only yields measurable improvements in perplexity and training efficiency across datasets of varying scales but also highlights KAF’s robustness compared to large, more unstable alternatives like KAN.

\subsection{Performance of KAF in Function Approximation and Differential Equation Solving Tasks}
To comprehensively validate the capability of KAF in complex function approximation and PDE solving\cite{phy}, we have carefully designed experiments covering a wide range of complexities, dimensions, and degrees of nonlinearity. First, we constructed eight function approximation tasks to evaluate KAF's performance in capturing complex nonlinear relationships. Second, we selected four PDE-solving problems involving multiple physical parameters to assess their applicability in scientific computing. Various hyperparameter configurations were employed in the experiments to ensure the reliability and generalization ability of the results.

\subsubsection{Experimental Setup}
We conducted 8 function approximation and 4 PDE solving\cite{phy,han2018solving} tasks, addressing varying complexities, dimensions, and nonlinearities. For function approximation and PDE tasks, we trained models with hidden layer sizes ranging from 8 to 512 for up to 1000 epochs. More detailed experimental settings are detailed in Appendix ~\ref{appendixE}.

\subsubsection{Function Approximation Tasks}
We presents the eight target functions used in the experiments (see Table \ref{tab:test_functions} in Appendix ~\ref{appendixE}), covering a variety of mathematical properties, including periodicity, non-linearity, high dimensionality, discontinuity, and chaos. The results of the experiment are shown in ~\ref{fig:test_functions}(see Appendix ~\ref{appendixE}). According to the experimental data, KAF's minimum test RMSE in most function approximation tasks is significantly lower than that of MLP, GPKAN, and FAN, demonstrating superior fitting capability and generalization performance. In the Bessel task, KAF achieves a test RMSE of \( 2.55 \times 10^{-6} \), which is considerably lower than MLP's \( 1.43 \times 10^{-5} \). For the Highly-Nonlinear and Multi-Scale tasks, KAF attains RMSE values of \( 3.18 \times 10^{-5} \) and \( 4.98 \times 10^{-5} \), while MLP exhibits significantly higher errors of \( 1.41 \times 10^{-4} \) and \( 1.85 \times 10^{-2} \), respectively.
\subsubsection{PDE solving tasks}

As shown in Figure~\ref{fig:PDE} in Appendix ~\ref{appendixE}, for numerical solving tasks across four types of PDEs (Poisson, 1D Wave, Heat, and Burgers), traditional MLP exhibits higher overall errors or lower stability. In contrast, KAF, which leverages learnable approximation, generally achieves better or comparable accuracy. For Poisson and Heat equations, both KAF and KAN significantly outperform MLP in terms of error reduction, while FAN also maintains a comparable level of accuracy. However, GPKAN, due to its sensitivity to parameter scale and initialization, demonstrates noticeable instability or larger errors, highlighting its challenges in achieving robust performance under these conditions. Overall, KAF, which incorporates learnable grid functions or compact functionals, provides greater flexibility in function approximation for PDE-solving tasks. 

\subsection{Ablation Experiment}
In this section, we conducted two ablation experiments: 1) On the CIFAR-10 dataset, we used a single-layer network to study the effectiveness and completeness of each component and strategy. Additionally, we plotted the scaling factor curves to observe the variations of different factors during the experiment. The detailed results are presented in Appendix ~\ref{appendix F}; 2) We performed function fitting experiments on sin(x) and cos(x) in comparison with KAN and MLP (GELU/RELU) to demonstrate that our model outperforms traditional methods in fitting periodic and high-frequency signals. The full experimental results are shown in Appendix ~\ref{appendix F}.

% Acknowledgements should only appear in the accepted version.
\section{Discussions}
The Kolmogorov-Arnold-Fourier network (KAF) addresses the scalability and spectral limitations of Kolmogorov-Arnold Networks (KAN) by integrating trainable random Fourier features (RFF) and a hybrid GELU-Fourier activation mechanism. This innovative design improves parameter efficiency and high-frequency function approximation while preserving a degree of interpretability. Experimental results demonstrate KAF's effectiveness across a diverse range of tasks, including vision, natural language processing (NLP), and solving differential equations, highlighting its practicality in high-dimensional learning scenarios.However, KAF's reliance on Fourier-based representations introduces certain trade-offs, such as potential challenges in interpretability and increased computational overhead. Furthermore, the learnability of RFF-based mappings is sensitive to initialization and hyperparameter tuning, which can impact convergence stability and model performance.Future research could focus on several promising directions. One avenue is the hybridization of KAF with larger-scale models to further enhance its capacity and scalability. Another is the exploration of more robust initialization schemes and adaptive optimization techniques to improve the stability of RFF-based mappings. Additionally, extending KAF to tackle more complex, real-world problems that demand efficient multi-frequency representation could unlock new applications and broaden its impact.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Detailed derivation of parameter quantities and FLOPs calculations}

\label{Appendix A}

\subsection{KAN with B-splines: Parameter Counting}

\textbf{Number of B-spline Basis Parameters.}

Let the B-spline\cite{B-spline} order be \(K\), and divide the domain into \(G\) segments. Then:
- Each segment needs \(K+1\) control points, total \((G + K + 1)\).
- Boundary smoothness of order \(K-1\) adds \(2(K-1)\) virtual points.
- Total per univariate spline: \(G + 3K - 1\). (Sometimes simplified to \(G + K + 3\).)

\textbf{Single-Layer Parameter Decomposition in KAN.}

For dimension \(d_{\mathrm{in}}\to d_{\mathrm{out}}\):
- \emph{Internal function (B-spline projection)}: 
  \(d_{\mathrm{in}} \times d_{\mathrm{out}}\) splines, each with \((G + K + 3)\) parameters.

Hence,

\begin{equation}
\text{Params}_{\mathrm{KAN}}
= d_{\mathrm{in}}\,d_{\mathrm{out}}\,(G + K + 3)
+ d_{\mathrm{out}}.
\end{equation}

\subsection{KAF with RFF: FLOPs Decomposition}

\textbf{Single-layer KAF.}
- \emph{Random Fourier Feature (RFF\cite{suijifuly}) mapping}:
  \(\cos(W^\top x + b)\) and \(\sin(W^\top x + b)\) each require one matrix multiplication.
  Total FLOPs\cite{FLOP}:
  \begin{equation}
  2 \times (d_{\mathrm{in}} \times M) \times 2 = 4d_{\mathrm{in}}M.
  \end{equation}
  
- \emph{Linear combination (GELU + RFF)}:
  Element-wise scaling of \(\mathbf{a} \odot \text{GELU}(x)\) and \(\mathbf{b} \odot \phi_{\text{RFF}}(x)\).
  Total FLOPs:  
  \begin{equation}
  2 \times (d_{\mathrm{in}} \times M) \times 2 = 4d_{\mathrm{in}}M.
  \end{equation}
- \emph{Final linear projection}:
  Matrix multiplication \(\mathbf{W}^{(l)} \cdot (\cdot)\) and bias addition.
  Total FLOPs:  
  \begin{equation}
   2d_{\mathrm{in}}d_{\mathrm{out}}.   
  \end{equation}
  
- \emph{Activation function}:
  GELU activation requires \(5d_{\mathrm{in}}\) FLOPs.

\textbf{Total FLOPs}:
\begin{equation}
    \text{FLOPs}_{\text{KAF}} = 4d_{\mathrm{in}}M + 2d_{\mathrm{in}} + 2d_{\mathrm{in}}d_{\mathrm{out}} + 5d_{\mathrm{in}}.
\end{equation}


\subsection{MLP FLOPs Computation}

\textbf{Standard MLP.}
- \emph{Linear layer} \(\mathbf{W}\in\mathbb{R}^{d_{\mathrm{out}}\times d_{\mathrm{in}}}\):
  \(2\,d_{\mathrm{in}}\,d_{\mathrm{out}}\) FLOPs (multiply+add).
- \emph{Activation}:
  - ReLU: \(1\) FLOP per output (comparison), total \(d_{\mathrm{out}}\).
  - GELU: about \(5\,d_{\mathrm{out}}\) FLOPs.

Hence, for a GELU-MLP:

\begin{equation}
    \text{FLOPs}_{\mathrm{MLP}}
= 2\,d_{\mathrm{in}}\,d_{\mathrm{out}} + 5\,d_{\mathrm{out}}.
\end{equation}



\subsection{Summary Comparison}

\begin{table}[h]
    \centering
    \caption{Comparison of parameter count and FLOPs per layer for KAN, KAF, and MLP models.}
    \label{tab:param_flops}
    \begin{tabular}{l|l|l}
        \hline
        \textbf{Model} & \textbf{Param Count (Single Layer)} & \textbf{FLOPs (Single Layer)} \\
        \hline
        \textbf{KAN} 
        & 
        $ d_{\mathrm{in}} d_{\mathrm{out}} (G+K+3) + d_{\mathrm{out}} $
        & 
        $7d_{\mathrm{in}} + (d_{\mathrm{in}} d_{\mathrm{out}})\left[9K (G + 1.5K) + 2G - 2.5K + 3\right]$ \\
        \textbf{KAF} 
        & 
        $ d_{\mathrm{in}} M + M + 2d_{\mathrm{in}} + d_{\mathrm{in}}d_{\mathrm{out}} + d_{\mathrm{out}} $
        & 
        $ 4d_{\mathrm{in}}M + 2d_{\mathrm{in}} + 2d_{\mathrm{in}}d_{\mathrm{out}} + 5d_{\mathrm{in}} $ \\
        \textbf{MLP} 
        & 
        $ d_{\mathrm{in}} d_{\mathrm{out}} + d_{\mathrm{out}} $
        & 
        $ 2 d_{\mathrm{in}} d_{\mathrm{out}} + 5 d_{\mathrm{out}} $ \\
        \hline
    \end{tabular}
\end{table}


- \(\textbf{KAN}\): Param and FLOPs scale with spline order \(K\) and segment count \(G\). 
- \(\textbf{KAF}\): RFF-based expansion is more GPU-friendly than B-spline recursion. 
- \(\textbf{MLP}\): Minimal overhead with no extra basis expansions.

\vspace{1em}
\hrule
\vspace{1em}

\section{Kernel Approximation and Gradient Derivation of Random Fourier Features (RFF)}

\label{Appendix B}

\subsection{Convergence Proof of RFF Kernel Approximation}

\subsubsection{Bochner's Theorem and the Fourier Duality of Kernel Functions}
According to Bochner's\cite{Bochner,sanjiao} theorem , any translation-invariant positive definite kernel function $k(x,y) = k(x-y)$ can be expressed as the Fourier transform of a Gaussian measure:

\begin{equation}
    k(x-y) = \int_{\mathbb{R}^d} e^{i\omega^\top (x-y)} p(\omega) d\omega
\end{equation}

where $p(\omega)$ is the spectral distribution corresponding to the kernel function. For the Gaussian kernel $k(x,y) = e^{-\|x-y\|^2/(2\sigma^2)}$, its spectral distribution is:

\begin{equation}
    p(\omega) = \mathcal{N}(\omega; 0, \sigma^{-2}I_d).
\end{equation}

\subsubsection{Expectation of Inner Product of Random Fourier Features}
Define the RFF mapping:

\begin{equation}
    z(x) = \sqrt{\frac{2}{m}} \left[ \cos(\omega_1^\top x + b_1), \sin(\omega_1^\top x + b_1), \dots, \cos(\omega_m^\top x + b_m), \sin(\omega_m^\top x + b_m) \right]^\top,
\end{equation}

where $\omega_i \sim p(\omega)$, $b_i \sim \mathcal{U}[0,2\pi]$. The expectation of the inner product is:

\begin{equation}
\begin{aligned}
\mathbb{E}\left[ z(x)^\top z(y) \right] 
&= \frac{2}{m} \sum_{i=1}^m \mathbb{E}\left[ \cos(\omega_i^\top x + b_i) \cos(\omega_i^\top y + b_i) + \sin(\omega_i^\top x + b_i) \sin(\omega_i^\top y + b_i) \right] \\
&= \frac{2}{m} \sum_{i=1}^m \mathbb{E}\left[ \cos(\omega_i^\top (x-y)) \right] \quad (\text{using trigonometric identity}) \\
&= \mathbb{E}_{\omega \sim p(\omega)} \left[ 2 \cos(\omega^\top (x-y)) \right] \quad (m \to \infty \text{ converges by law of large numbers}) \\
&= \mathbb{E}_{\omega \sim p(\omega)} \left[ e^{i\omega^\top (x-y)} + e^{-i\omega^\top (x-y)} \right] \\
&= 2 \cdot \text{Re} \left( \mathbb{E}_{\omega \sim p(\omega)} \left[ e^{i\omega^\top (x-y)} \right] \right) \\
&= 2 \cdot \text{Re} \left( k(x-y) \right) = 2k(x-y) \quad (\text{since } k(x-y) \text{ is a real-valued symmetric function}).
\end{aligned}
\end{equation}

However, since the original scaling factor is $\sqrt{2/m}$, the actual expectation of the inner product is:

\begin{equation}
\mathbb{E}\left[ z(x)^\top z(y) \right] = k(x-y).
\end{equation}

\subsubsection{Error Bound and Convergence Rate}
According to Rahimi \& Recht \cite{Bochner}, when using $m$ random frequencies, for any $x,y \in \mathcal{X}$, we have:

\begin{equation}
    \mathbb{P} \left( \sup_{x,y} \bigl| z(x)^\top z(y) - k(x,y) \bigr| \geq \epsilon \right) 
    \leq 2^8 \left( \frac{\sigma_p \operatorname{diam}(\mathcal{X})}{\epsilon} \right)^2 
    \exp\left( -\frac{m \epsilon^2}{4(d+2)} \right).
\end{equation}

where $\sigma_p$ is the variance of $p(\omega)$, and $\text{diam}(\mathcal{X})$ is the diameter of the input space. Thus, the convergence rate is:
\begin{equation}
    \mathcal{O}(1/\sqrt{m})
\end{equation}

\subsection{Differentiability and Gradient Computation of RFF}

\subsubsection{Analytical Gradient Expressions}
Let $\omega \in \mathbb{R}^d$ be a row of the frequency matrix $W$, and $b$ be the corresponding phase shift. For an input $x \in \mathbb{R}^d$:

- Gradient of the cosine term:
  \begin{equation}
      \frac{\partial}{\partial \omega} \cos(\omega^\top x + b) = -x \sin(\omega^\top x + b), \quad \frac{\partial}{\partial b} \cos(\omega^\top x + b) = -\sin(\omega^\top x + b)
  \end{equation}
- Gradient of the sine term:
  \begin{equation}
       \frac{\partial}{\partial \omega} \sin(\omega^\top x + b) = x \cos(\omega^\top x + b), \quad \frac{\partial}{\partial b} \sin(\omega^\top x + b) = \cos(\omega^\top x + b)
  \end{equation}

For a matrix $W \in \mathbb{R}^{d \times m}$, gradients accumulate row-wise. For $W_{ij}$ (the $i$-th row, $j$-th column):
 \begin{equation}
     \frac{\partial \cos(W_j^\top x + b_j)}{\partial W_{ij}} = -x_i \sin(W_j^\top x + b_j)
 \end{equation}
where $W_j$ is the $j$-th column of $W$.

\subsubsection{Implementation in Backpropagation}
In automatic differentiation frameworks\cite{baydin2018automaticdifferentiationmachinelearning} (e.g., PyTorch), the gradient computation for RFF follows these steps:
1. Forward pass: Compute $\cos(W^\top x + b)$ and $\sin(W^\top x + b)$.
2. Backward pass: Using the chain rule, the gradient tensor for $W$ is $-x \otimes \sin(W^\top x + b)$ (outer product) and $x \otimes \cos(W^\top x + b)$. The gradient for $b$ is directly $-\sin(W^\top x + b)$ and $\cos(W^\top x + b)$.
3. Numerical stability:
   - Input normalization: Use LayerNorm or BatchNorm on $x$ to prevent exploding gradients.
   - Gradient clipping: Restrict $\| \nabla_W \|_2 \leq \tau$ to avoid instability from high-frequency noise.

\subsection{RFF Initialization Strategy Derivation}

\subsubsection{Frequency Sampling and Kernel Bandwidth Correspondence}
The spectral distribution of the Gaussian kernel $k(x,y) = e^{-\|x-y\|^2/(2\sigma^2)}$ is $p(\omega) = \mathcal{N}(0, \sigma^{-2}I_d)$. Hence, frequencies should be sampled as $\omega \sim \mathcal{N}(0, \sigma^{-2}I_d)$. However, if input data is standardized such that each dimension satisfies $\mathbb{E}[x_i^2] = 1/d$, then the variance of $\omega^\top x$ is:

\begin{equation}
    \mathbb{V}[\omega^\top x] = \mathbb{E}[x^\top \omega \omega^\top x] = \text{Tr}(\mathbb{E}[\omega \omega^\top] \mathbb{E}[x x^\top]) = \sigma^{-2} \cdot \text{Tr}(I_d/d) = \sigma^{-2}.
\end{equation}

To make $\omega^\top x$ independent of input scale, frequency variance should be adjusted to $\sigma^{-2}/d$, i.e., $\omega_{ij} \sim \mathcal{N}(0, \sigma^{-2}/d)$.

\subsubsection{Determination of Scaling Factor $\gamma$}
Assuming the activation function $\sigma(x)$ has an output variance of $\mathbb{E}[\|\sigma(x)\|^2] = c$, the frequency matrix should be initialized such that:

\begin{equation}
    \frac{\sigma^{-2}}{d} \cdot \mathbb{E}[\|W\|_F^2] = \gamma^2 \implies \gamma = \frac{\sigma^{-1}}{\sqrt{d}}.
\end{equation}

Thus, the initialization strategy is $\omega_{ij} \sim \mathcal{N}(0, \gamma^2/d)$, where $\gamma = \sigma^{-1} / \sqrt{\mathbb{E}[\|\sigma(x)\|^2]}$.

\section{Fourier theory proof of GELU activation function initialization factor
$\sigma=1.64$}

\label{prof:1.64}

\subsection{Definition and Assumptions}

Consider an input signal $x \sim \mathcal{N}(0, \sigma^2)$, whose Fourier transform is:
\begin{equation}
    \mathcal{F}\{x\}(\omega) = \int_{-\infty}^{\infty} x e^{-i\omega x} dx.
\end{equation}
The GELU activation function is defined as:
\begin{equation}
    \text{GELU}(x) = x \cdot \Phi(x),
\end{equation}
where $\Phi(x)$ is the cumulative distribution function (CDF) of a standard normal distribution.

\subsection{Fourier Transform of GELU}

Using the differentiation property and the convolution theorem of Fourier transforms:
\begin{equation}
    \mathcal{F}\{\text{GELU}(x)\}(\omega) = \mathcal{F}\{x \Phi(x)\}(\omega) = i \frac{d}{d\omega} \mathcal{F}\{\Phi(x)\}(\omega).
\end{equation}
The Fourier transform of $\Phi(x)$ is known:
\begin{equation}
    \mathcal{F}\{\Phi(x)\}(\omega) = \sqrt{\frac{\pi}{2}} e^{-\omega^2/2} \left(1 + \text{erf}\left(\frac{i\omega}{\sqrt{2}}\right)\right).
\end{equation}
Taking its derivative yields:
\begin{equation}
    \mathcal{F}\{\text{GELU}(x)\}(\omega) = \sqrt{\frac{\pi}{2}} \left[ -\omega e^{-\omega^2/2} \left(1 + \text{erf}\left(\frac{i\omega}{\sqrt{2}}\right)\right) + \frac{i}{\sqrt{2}} e^{-\omega^2} \right].
\end{equation}

\subsection{Spectral Energy Distribution}

The spectral energy density of GELU is:
\begin{equation}
    S(\omega) = \left| \mathcal{F}\{\text{GELU}(x)\}(\omega) \right|^2.
\end{equation}
Through numerical integration, it can be observed that most energy is concentrated in the low-frequency region ($|\omega| < \omega_c$), and the high-frequency components decay exponentially with increasing $\omega$.


\subsection{Scaling Factor $\alpha$ Optimization in Frequency Spectrum}

\subsubsection{Objective Function Definition}

To minimize the spectral distortion of the scaled activation function, we define:
\begin{equation}
    \mathcal{L}(\alpha) = \int_{-\infty}^{\infty} \left| S_{\text{target}}(\omega) - \alpha^2 S_{\text{GELU}}(\omega) \right|^2 d\omega.
\end{equation}
Assuming the target spectrum follows white noise, i.e., $S_{\text{target}}(\omega) = 1$.

\subsubsection{Optimization Solution}

Expanding the objective function:
\begin{equation}
    \mathcal{L}(\alpha) = \int_{-\infty}^{\infty} \left(1 - \alpha^2 S_{\text{GELU}}(\omega)\right)^2 d\omega.
\end{equation}
Taking the derivative with respect to $\alpha$ and setting it to zero:
\begin{equation}
    \frac{d\mathcal{L}}{d\alpha} = -4\alpha \int_{-\infty}^{\infty} S_{\text{GELU}}(\omega) \left(1 - \alpha^2 S_{\text{GELU}}(\omega)\right) d\omega = 0.
\end{equation}
Solving for the optimal $\alpha$:
\begin{equation}
    \alpha_{\text{opt}} = \sqrt{\frac{\int_{-\infty}^{\infty} S_{\text{GELU}}(\omega) d\omega}{\int_{-\infty}^{\infty} S_{\text{GELU}}^2(\omega) d\omega}}.
\end{equation}

\subsubsection{Numerical Integration Results}

Using Monte Carlo integration, we compute:
\begin{equation}
    \int_{-\infty}^{\infty} S_{\text{GELU}}(\omega) d\omega \approx 0.168, \quad \int_{-\infty}^{\infty} S_{\text{GELU}}^2(\omega) d\omega \approx 0.062.
\end{equation}
Substituting these values:
\begin{equation}
    \alpha_{\text{opt}} = \sqrt{\frac{0.168}{0.062}} \approx 1.64.
\end{equation}

\subsection{Dynamic Adaptation of Fourier Characteristics}

\subsubsection{Spectrum Matching Mechanism}

Random Fourier features (RFF) sample frequencies $\omega_i \sim \mathcal{N}(0, \sigma^{-2})$ to approximate the target spectrum. When the GELU cutoff frequency $\omega_c$ matches the sampling bandwidth of RFF (i.e., $\sigma \approx 1.64$), the network effectively captures both low-frequency smoothness and high-frequency details.

\subsubsection{Dynamic Balance in Training}

Initially, a small scaling factor $\beta = 10^{-2}$ suppresses high-frequency noise. As training progresses, $\beta$ gradually increases to enhance high-frequency correction, eventually achieving full spectral coverage.

\section{ML\&NLP\&audio tasks}

\label{appendix D}

We show here the experimental results~\ref{fig:accuracy_params_ml}of the NLP\&audio\& ML experiment based on kanbefair in 4.2

The experimental results show that KAF (ours) has achieved excellent performance on different datasets including three tasks, and has higher accuracy than other models under the same parameters. In the Bean, AG\_NEWS and other datasets, KAF converges quickly and achieves the highest accuracy, which shows that our method also has good generalization in natural language processing and audio processing.

\begin{figure*}[t] 
    \centering
    \includegraphics[width=1.0\linewidth]{ml_num_parameter.pdf}
    \caption{Compare the performance of various models (KAN, GPKAN, MLP, FAN, KAF) across NLP,audio and ML datasets. KAF consistently outperforms other models, achieving higher accuracy with fewer parameters, especially in datasets like Bean, Rice, and AG News. KAF's efficiency and accuracy make it a strong choice across a wide range of tasks.}
    \label{fig:accuracy_params_ml}
\end{figure*}

\section{Function approximation and differential equation solving tasks}

\label{appendixE}

In this section, we will supplement Experiment 4.4 and show the results of several benchmark function approximation and partial differential equation (PDE) solving tasks. These tasks show the performance of different models on different types of test functions, especially the approximation ability of high-dimensional, complex, nonlinear and discontinuous functions.

\subsection{Function Approximation Tasks}

\begin{table*}[t]
\centering
\caption{Types of Test Functions and Their Mathematical Expressions}
\label{tab:test_functions}
\scriptsize % 调整字体大小
\begin{tabular}{p{8cm} p{8cm}} % 手动设置列宽
\toprule
\textbf{Function Name} & \textbf{Mathematical Expression} \\
\midrule
Bessel Function & \( f(x) = J_0(20x) \) \\
Chaotic & \( f(x,y) = e^{\sin(\pi x) + y^2} \) \\
Simple Product & \( f(x,y) = x \cdot y \) \\
High-Freq-Sum & \( f(x) = \sum_{k=1}^{100} \sin\left(\frac{kx}{100}\right) \) \\
Highly-Nonlinear & \( f(x_1,x_2,x_3,x_4) = e^{\sin(x_1^2 + x_2^2) + \sin(x_3^2 + x_4^2)} \) \\
Discontinuous & 
\(
f(x) =
\begin{cases}
-1, & x < -0.5 \\
x^2, & -0.5 \leq x < 0 \\
\sin(4\pi x), & 0 \leq x < 0.5 \\
1, & x \geq 0.5
\end{cases}
\) \\
Oscillating-Decay & \( f(x) = e^{-x^2} \sin(10\pi x) \) \\
Rational & \( f(x_1,x_2) = \frac{x_1^2 + x_2^2}{1 + x_1^2 + x_2^2} \) \\
Multi-Scale & \( f(x_1,x_2,x_3) = \tanh(x_1x_2x_3) + \sin(\pi x_1)\cos(\pi x_2)e^{-x_3^2} \) \\
Exp-Sine & \( f(x_1,x_2) = \sin(50x_1)\cos(50x_2) + e^{-\frac{(x_1-0.5)^2 + (x_2-0.5)^2}{0.1}} \) \\
\bottomrule
\end{tabular}
\end{table*}


First, Figure \ref{fig:test_functions} shows the approximation effect of different test functions. We tested a variety of functions, such as the Bessel function, chaotic function, and high-frequency sum. The mathematical expression of each function is listed in the table \ref{tab:test_functions}. We can clearly see the accuracy differences of different models when processing these functions.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.\textwidth, height=0.3\textheight]{combined_results.pdf} % 宽和高
    \caption{This experiment compares different models (KAN, GPKAN, MLP, FAN, KAF) on various function approximation tasks, analyzing test RMSE versus the number of parameters. KAF consistently achieves lower RMSE across all tasks, outperforming other models like MLP with fewer parameters. Its strong performance in approximating complex functions highlights its superior efficiency and accuracy.}
    \label{fig:test_functions}
\end{figure*}

For example, for the high-frequency sum (High-Freq-Sum) function, KAF (kernel approximation method based on RFF) shows good approximation ability and also shows strong fitting ability when processing high-dimensional complex nonlinear functions (such as Highly-Nonlinear). Figure \ref{fig:test_functions} shows that KAF has relatively good performance on different types of functions.

\subsection{PDE Solving Tasks}

Next, Figure \ref{fig:PDE} shows the performance of different models in solving partial differential equations (PDEs). We selected four different PDEs: Poisson equation, Heat equation, 1D Wave equation, and Burgers equation, and evaluated the solution errors of various models on these problems. From the results shown in the box plot, we can see that KAN and KAF have lower solution errors when dealing with these PDEs, especially for complex nonlinear problems, KAF shows strong robustness.

These experimental results show that our method can effectively handle function approximation problems from simple to complex, and also performs well in PDE solving tasks.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth, height=0.44\textheight]{pde_comparison.pdf} % 调整宽度和高度
    \caption{This experiment compares different models (MLP, KAN, KAF, FAN, GPKAN) in solving Poisson, 1D Wave, Heat, and Burgers equations. KAF consistently delivers strong performance across all tasks, demonstrating its efficiency and effectiveness in solving complex PDEs.}
    \label{fig:PDE}
\end{figure*}

\section{Ablation Experiment}

\label{appendix F}

\subsection{Ablation on Cifar10}
We use a single-layer KAF trained on CIFAR-10 as the baseline model, with a hidden layer size of 128.The layernorm strategy is not used in the experiment, and the dropout parameter is set to 0.1 We evaluate the following strategies:

\begin{enumerate}
    \item \textbf{No GELU activation function:} Only the scaling factor and RFF strategy are used.
    \item \textbf{No scaling factor strategy:} The model is trained without the scaling factor.
    \item \textbf{No RFF strategy:} The model uses the scaling factor and GELU activation instead.
    \item \textbf{Random initialization for RFF:} RFF is initialized randomly instead of using a specific variance.
    \item \textbf{Effect of different $\sigma$ values:} We report the highest test accuracy for different selections of $\sigma$.
    \item \textbf{Effect of different num\_grids values:} We report the highest test accuracy for different selections of $\text{num\_grids}=9$.
\end{enumerate}

Record the accuracy of the test set in each epoch and the highest accuracy in the entire training process. At the same time, in order to observe the specific changes in the scaling factors, we plotted the changes of the two scaling factors a and b of KAF with epochs in the experiment.

\begin{table}[h]
    \centering
    \caption{Performance of Different $\sigma$ Values on Cifar10}
    \label{tab:hyperparam_acc1}
    \begin{tabular}{c c c c c c c c c c c c}  % 这里去掉了 S 选项，改为标准的 c（居中）
        \toprule
        $\sigma$ & 0.1 & 0.5 & 1 & 1.5 & 1.6 & 1.64(defult) & 1.7 & 1.8 & 2 & 2.5 & 3 \\
        \midrule
        ACC (\%) & 46.83 & 52.50 & 54.02 & 54.41 & 54.32 & 54.96 & 54.64 & 54.68 & 54.36 & 54.07 & 53.21  \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{Performance of Different num\_grids Values on Cifar10}
    \label{tab:hyperparam_acc2}
    \begin{tabular}{c c c c c c c c c c c c}  % 这里去掉了 S 选项，改为标准的 c（居中）
        \toprule
        $\sigma$ & 2 & 4 & 6 & 8 & 9 (default) & 10 & 12 & 14 & 16 & 18 & 20 \\
        \midrule
        ACC (\%) & 54.23 & 54.67 & 54.41 & 54.80 & 54.96 & 54.87 & 54.94 & 54.82 & 54.76 & 54.79 & 55.01  \\
        \bottomrule
    \end{tabular}
\end{table}


The results of strategies 1-4 are shown in ~\ref{fig:ablation_1_4}, and the experimental results of strategies 5 and 6 are in ~\ref{tab:hyperparam_acc1} and ~\ref{tab:hyperparam_acc2}. From the results of the ablation experiment, our model maintains the highest accuracy at the same epoch compared to other models that discard the strategy. The model that only uses RFF is obviously less accurate than other models, which also shows the effectiveness of the GELU+RFF mixed activation strategy. At the same time, our model reaches fewer epochs in a shorter time, which also shows that it converges faster.

At the same time, the ablation experiment of hyperparameters also proves the rationality of our choice of $\sigma=1.64, num\_grids=9$ as the default model configuration. When $\sigma=1.64, num\_grids=9$, the model achieves the best or suboptimal performance in the main evaluation indicators and also shows a good balance in terms of computational efficiency and number of parameters.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth, height=0.4\textheight]{abolation_comparison.pdf} % 调整宽度和高度
    \caption{The curve of the test set accuracy of different strategies in the ablation experiment on Cifar10 changes with epoch. KAF (original) demonstrates the effectiveness of our model design, consistently achieving higher test accuracy compared to other strategies across epochs.}
    \label{fig:ablation_1_4}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth, height=0.5\textheight]{activation_params.pdf} % 调整宽度和高度
    \caption{Evolution of activation scaling factors over time: Base Scale (a) and Spline Scale (b).}
    \label{fig:activate_para}
\end{figure*}

In Figure~\ref{fig:activate_para}, we show how the Base Scale and Spline Scale inside KAF change with training during 40 epochs of training on Cifar10. Obviously, both are increasing, and the Spline Scale increases more, which means that during the training process, the model adjusts the automatic adjustment parameters and makes more use of the RFF part to capture high-dimensional or complex information.

\subsection{Fitting experiment of sin(x) and cos(x)}

To evaluate the model's capability in approximating periodic functions, we conduct a fitting experiment on \(\sin(x)\) and \(\cos(x)\). Specifically, we train the model to learn the mapping \(x \mapsto \sin(x)\) and \(x \mapsto \cos(x)\) using a dataset of uniformly sampled points from the interval \([-20, 20]\). The training objective minimizes the mean squared error (MSE) between the predicted and true values.

We use a single-layer network with 64 neurons in the hidden layer and test KAF, KAN, MLP (RELU), and MLP (GELU). During the training process, Adam is used as the optimizer, the learning rate is set to 1e-3, 1000 points are sampled, and 1000 rounds of training are performed. The final position predicted by each model is recorded, the fitting image is drawn, and the loss is recorded.

Figure~\ref{fig:sin_cos} illustrates the fitting results of different models for \(\sin(x)\) and \(\cos(x)\). It can be observed that MLP\_RELU and MLP\_GELU struggle to maintain the periodic structure when the input range is large. While KAN performs relatively well in certain regions, it still exhibits significant deviations in the low-frequency range. In contrast, the KAF model more accurately captures the periodicity of the target functions and provides superior fitting performance across most regions.

Figure~\ref{fig:sin_cos_frequency} presents the frequency spectrum analysis of different models on \(\sin(x)\) and \(\cos(x)\). The true signal's spectral energy is primarily concentrated in the low-frequency region, and the spectral distribution of the KAF model closely matches the true signal, effectively preserving the spectral characteristics of the target function. On the other hand, MLP\_RELU and MLP\_GELU exhibit significant deviations in the high-frequency components, indicating their difficulty in accurately representing high-frequency features. Although KAN's spectral response aligns more closely with the true signal in some frequency bands, there are still noticeable discrepancies in energy distribution.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth, height=0.3\textheight]{sin_cos.pdf} % 调整宽度和高度
    \caption{Images of the four models fitted on the standard sin/cos function after training for 1000 epochs}
    \label{fig:sin_cos}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.\textwidth, height=0.3\textheight]{frequency_analysis.pdf} % 调整宽度和高度
    \caption{Frequency spectrum analysis of different models for $sin(x)$ and $cos(x)$, showing the magnitude distribution across different frequency components.}
    \label{fig:sin_cos_frequency}
\end{figure*}

\end{document}



% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
