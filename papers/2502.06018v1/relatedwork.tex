\section{Related Work}
\textbf{Multi-Layer Perceptrons and Current Challenges.} The design and optimization of deep learning \cite{deit}models remain central to machine learning research. Traditional MLPs\cite{MLP}, among the earliest neural networks \cite{sjwl02}, offer simplicity and scalability, with rich theoretical foundations. While ResNet \cite{resnet18} and Transformer \cite{transform} models have shown remarkable performance across various tasks, MLPs face challenges in theoretical interpretability and practical bottlenecks. Traditional activation functions like ReLU \cite{ReLU1,ReLU2} and Sigmoid \cite{Sigmoidjihuo} often fail to adapt to complex data, and despite their efficiency, MLPs struggle with high-frequency features and complex distributions. Improving activation mechanisms and parameter efficiency has become crucial for enhancing MLPs' adaptability to high-dimensional data.

\textbf{Kolmogorov-Arnold Networks and Scalability Issues.} The Kolmogorov-Arnold \cite{2019duichengkanl} Theorem underpins networks for approximating continuous multivariable functions. The pioneering KAN replaced fixed activations with B-spline \cite{B-spline} functions but faces challenges in high-dimensional applications due to parameter explosion and GPU inefficiency. Recent improvements include KAN-Transformer, MLP-KAN with sparse parameters, and FAN\cite{FAN} with Fourier activations, all seeking to balance interpretability with scalability.

\textbf{Enhancing Spectral Representation with KAF.} To address high-frequency modeling challenges, Random Fourier Features (RFF \cite{suijifuly,suijifulye2}) enable spectral domain mapping, with variants like Learnable RFF and SIREN enhancing expressiveness. Our proposed KAF incorporates GELU and learnable Fourier features, with scale factor control and variance initialization. This reduces parameters while improving spectral representation. KAF maintains KAN's interpretability while enhancing scalability and efficiency, showing superior performance in capturing high-frequency\cite{SIRENgaoping} details across NLP, vision, audio, and traditional machine learning tasks.
\begin{figure*}[t]  % [t] 使其出现在页面顶部
    \centering
    \includegraphics[width=0.8\textwidth]{ppline.pdf}  % 使其匹配整行宽度
    \vspace{-20pt}
    \caption{Compare two models: a standard MLP with GELU activation (left) and a KAF with GELU activation (right). The MLP involves a projection matrix followed by GELU, while the KAF adds Random Fourier Features (RFF) and scale parameters, offering more flexibility in feature transformations.}
    \label{fig:pipeline}
\end{figure*}