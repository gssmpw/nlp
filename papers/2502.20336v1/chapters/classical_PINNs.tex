% !TEX root = ../dual_norm_estimate_base.tex

\section{PINNs for solving PDEs} \label{sec:PINNs}
We are going to briefly introduce the main concepts of PINNs without going into details, since we only aim to use PINNs as a black box.

\subsection{PDEs in classical form}
Let $\Omega \subset \mathbb{R}^d$ be a bounded open domain and let $m\in\mathbb{N}$ denote the order of the PDE ($m=2$ for Laplace's equation). We denote by
\begin{align*}
    B^\circ: C^m(\Omega)\to C^0(\Omega)
\end{align*}
the classical (point-wise) form of the differential operator under consideration. Then, given some $f^\circ\in C(\Omega)$, we call $u\in C^m(\Omega)$ a \emph{classical solution} if 
\begin{equation} \label{eq:clPDE}
	(B^\circ u)(x) = f^\circ(x), \quad \forall x \in \Omega,
\end{equation}
where we assume that proper boundary and/or initial conditions are incorporated into the definition of the operator.

Next, we are given some approximation $u^\delta$ to $u$, e.g.\ in terms of a PINN and define the (classical) \emph{residual} of \eqref{eq:clPDE} by
\begin{align}\label{eq:residual}
    r_\Omega^\circ (u^\delta)(x) := f(x) - (B^\circ u^\delta)(x), \quad x\in \Omega. 
\end{align}
Given $u^\delta$, the residual is in principle computable by inserting $u^\delta$ into the PDE operator.

However, such classical solutions often do not exist, depending on the data $B^\circ$, $f^\circ$ and $\Omega$. It is well-known that well-posedness of such problems is usually linked to suitable variational formulations as described in Section \ref{sec:VarFormPDEs} below. 


\subsection{PINNs: Definition and training}
The core idea of PINNs is to use the residual for the definition of a loss function within the training of a neural network. Let us briefly describe this for the above classical solution concept, even though (i) there are several other approaches in the literature and (ii) our subsequent error analysis does not depend on the choice of the loss function for a PINN.

\subsubsection*{Neural networks (NNs)} 
The notation of NNs in this paragraph is based upon \cite{Berner2021,Gribonval2021,Petersen2018}. A NN is a function $\Phi_a(\cdot;\theta):\mathbb{R}^{N_{0}} \rightarrow \mathbb{R}^{N_{L}}$, where $a$ is the \emph{architecture} and $\theta$ are the \emph{parameters}. Both the architecture and parameters determine the input-output function $\Phi_a(\cdot;\theta)$ of the NN. In case of a feed-forward NN, the architecture $a=(N,\rho)$ can be described by the vector of neurons per layer $N=(N_0,...,N_L)\in\mathbb{N}^{L+1}$, where $L \in \mathbb{N}$ denotes the number of \emph{layers} ($N_0$ being the input and $N_L$ the output dimension) and the \emph{activation function} $\rho: \mathbb{R} \rightarrow \mathbb{R}$.

The parameters of the NN read $\theta = (W^{(l)},b^{(l)})_{l=1,...,L}$, where $W^{(l)} \in \mathbb{R}^{N_{l} \times N_{l-1}}$ are the \emph{weight matrices} and $b^{(l)} \in \mathbb{R}^{N_{l}}$ are called \emph{bias vectors}. 
The output $\Phi_a(z;\theta)$ of the NN for an input $z\in\mathbb{R}^{N_0}$ is then defined as $\Phi_{a}(z;\theta) := \Phi^{(L)}(z;\theta)$, where
\begin{align*}
	\Phi^{(1)}(z;\theta) &= W^{(1)} z + b^{(1)}, \\
	\hat{\Phi}^{(l)}(z;\theta) &= \rho(\Phi^{(l)}(z;\theta)), \quad l=1,...,L-1, \quad \text{and} \\
	\Phi^{(l+1)}(z;\theta) &= W^{(l+1)} \hat{\Phi}^{(l)}(z;\theta) + b^{(l+1)}, \quad l = 1,...,L-1,
\end{align*}
and $\rho$ is applied component-wise. In the following the architecture is omitted in the notation as we view it as being fixed once and for all. 

NNs seem suitable for solving PDEs because they are universal function approximators, see \cite{Hornik1991,Hornik1989,Guehring2019}. For a NN $\Phi^\theta := \Phi(\cdot;\theta)$ to approximate the solution of a PDE, the parameters $\theta$ must be determined, which is done in the training phase. Thereby, a tailored minimization problem is defined, with which the parameters are \emph{learned}. In the regime of PINNs the function to be minimized involves the PDE (e.g.\ in classical form \eqref{eq:clPDE}), which is the reason for the name \emph{physics-informed}.

With respect to the classical form of the PDE, a set of sample points $\mathcal{S}_\Omega$ in $\Omega$ are chosen and \eqref{eq:clPDE} is posed only for those points. This leads to the definition of the loss function
\begin{equation}\label{eq:lossfunc}
	\mathcal{L}(\theta) 
    := \sum_{x \in \mathcal{S}_\Omega} 
    \vert (r^\circ_\Omega (\Phi^\theta))(x) \vert^2.
\end{equation}
Often, an additional sampling is required for satisfying the boundary conditions. We shall assume that $\Phi^\theta$ satisfies given boundary condition as this can be achieved with the aid of approximate distance functions, see \cite{sukumar2022exact}. 

Such classical PINNs have e.g.\ been investigated for a broad scope of linear and nonlinear PDEs, see e.g. \cite{Berg2018,Raissi2019,Mao2020,Cai2021,Cai2021a,Hu2024}. The main advantages of the method are its straightforward applicability and, due to the sampling of $\mathcal{S}_\Omega$, it results in a mesh-free approximation. 

One can also replace the above loss function by terms stemming from a variational formulation of a given PDE, e.g.\ VPINNs. Again, as the specific form of the PINN is not relevant for our paper, we will not go into more details and only assume that $\Phi^\theta$ gives some approximation for the solution of a given PDE.
