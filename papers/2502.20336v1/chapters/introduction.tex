% !TEX root = ../dual_norm_estimate_base.tex

\section{Introduction}
In recent years, a broad and continuous interest on the usage of neural networks for the numerical approximation of solutions to partial differential equations (PDEs) is being witnessed and vastly depicted in the literature, e.g.\ \cite{Dissanayake1994,Lagaris2000a,Lagaris2000b,Raissi2018a,Raissi2019}, where this list is far from being complete. Physics-informed neural networks (PINNs), trained with loss functions related to the residual of a given PDE, are extensively used to solve PDEs in particular posed on complex-shaped or varying domains, as a PINN avoids complicated discretization techniques and thus keeps the implementation effort low. On the other hand, however, a theory for the assessment of the approximation quality in terms of an a posteriori error control is at least not obvious, and highly unexplored compared to the wide acceptance of PINNs. 

In other words, the aim of this paper is to discuss the \emph{certification} of PINN approximations. By this, we mean the derivation of rigorous, computable lower and upper bounds on the error between the true (unknown) PDE solution and any PINN approximation. In particular, we want to avoid any assumption on the architecture, training or training data of the PINN. We view it as a given black box for the approximation of a known PDE on a known domain $\Omega\subset\R^d$. Hence, this paper is devoted to \emph{error estimation} for PINN approximations. 

A common approach to assess the accuracy of a PINN solution is to bound the approximation error by the generalization error, which measures the $L_2$-norm of the residual on the entire domain, not just the training points. In \cite{10.1093/imanum/drab093}, the generalization error has been bounded in terms of the training error, the number of training samples and the quadrature error, up to constants stemming from stability estimates on the underlying PDE. Although this approach provides an instructive insight for the training process as well as the quality of the approximation, it is generally not sharp and relies on a stability assumption on the underlying PDE. Moreover, it requires the exact solution, so this approach is inherently theoretical rather than of practical use for certification. 

In \cite{10.1093/imanum/drac085}, three key theoretical questions on PINNs error analysis are posed, in particular (i) on how small PDE residuals can be made in the class of neural networks, (ii) on whether a small residual implies a small total error and (iii) small training errors imply small total errors for sufficient number of quadrature points. Based upon these questions the authors prove a priori error estimates for a nonlinear PDE, the incompressible Navier-Stokes equation. These estimates are then used in a posteriori fashion on an example with analytical solutions to compute the theoretical bounds and assess them quantitatively. Although this analysis constitutes a comprehensive theoretical analysis of PINNs for a nonlinear PDE, assumptions on smoothness of the solution and the ability to find a global training loss minimum in order to estimate training errors somewhat limit applicability.


Unlike generalization error approaches, the methods in \cite{Berrone22,ernst2024certified,opschoor2024,monsuur2024} provide PDE formulations that in turn define loss functionals suitable for neural network training, and which enable the computation of numerical bounds on the actual error between the neural network approximation and the true solution. \cite{9892569} yields certification of the error without a priori knowledge of the solution using arguments from semigroup theory. \cite{Berrone22} introduces a reliable and efficient error estimator for variational neural networks (VPINNs). The bound for the error consists of a residual-type term, a loss function term and a term for the data oscillation. However, the PINN approximation should follow the corresponding discretization of the PDE. In \cite{ernst2024certified} rigorous a posteriori error control is constructed by connecting the residual to the approximation error with computable quantities based on a well-posed formulation of the PDE. This is achieved using wavelet expansions of the dual norm of the residual as the loss function to train the neural network. In \cite{opschoor2024} a least-squares framework for neural networks based on first-order systems of PDEs is introduced. The least squares residual serves both as a loss functional and as an a posteriori error estimator, allowing the residual to provide a computable upper bound on the approximation error. In similar spirit, the proposed least-squares formulations for PDEs in \cite{monsuur2024} provide loss functionals that achieve quasi-optimal approximation rates in appropriate norms, specifically addressing inhomogeneous boundary conditions avoiding fractional Sobolev norms.

The PINN approach relies on the assumption that training a neural network by a loss function based upon the residual gives rise to a small approximation error. This requires stability and a variationally correct form of the neural network, ensuring that the loss remains proportional to the error measured in a norm induced by the problem, \cite{bachmayr2024}.


\subsection{Contribution}
Viewing a PINN as a given black box, we propose a framework to rigorously bound the PINN approximation error from above and from below by quantities which can efficiently be computed a posteriori -- independently of the choice of the loss functional or the configuration of training. This is achieved in the following way:
\begin{compactenum}[i)] 
    \item starting by a well-posed (variational) formulation of the PDE on some domain $\Omega\subset\R^d$, we use a well-known error-residual relation;
    \item choose geometrically simple domains $\mycirc$ and $\square$ such that $\mycirc\subset\Omega\subset\square$; 
    \item extend and restrict the residual of the PINN to $\mycirc$ and $\square$ in a suitable manner;
    \item construct computable surrogates of the dual norm of the residual on $\mycirc$ and $\square$ and relate them to the residual posed on $\Omega$.
\end{compactenum}

Our main motivation for this methodology lies on the following: Since PINNs are mesh-free, their usage is particularly attractive when the geometry of $\Omega$ is such that forming a triangulation e.g.\ for a finite element discretization is computationally costly. Such a situation occurs, for instance,\ if $\Omega$ is complex or if $\Omega$ is varying depending on some parameter in such a way that mapping to some reference domain is not easily possible. Examples might include geometry or topology  optimization and fluid-structure interaction. Hence, we do not make use of any discretization of the underlying PDE on $\Omega$ (as e.g.\ in \cite{Berrone22}). 

However, reducing the computations for the error bounds on simpler domains $\mycirc$ and $\square$, we can use highly efficient numerical schemes.


\subsection{Organization of material}
The structure of this paper is as follows. In Section \ref{sec:PINNs}, we briefly revise the  PINNs methodology for solving PDEs. In Section \ref{sec:VarFormPDEs}, we recall the variational framework for weak formulations and their practical relevance to residual-based a posteriori error control. Moreover, we review the computational techniques used to evaluate dual norms. In Section \ref{sec:CertifyPINNs}, we introduce the theoretical framework and prove the lower and upper bounds for the error. The abstract setting is detailed for elliptic and parabolic problems. Numerical experiments are presented in Section \ref{sec:NumResults} to quantitatively investigate our numerical bounds. We conclude  with a brief summary and an outlook towards potential directions for future research.



