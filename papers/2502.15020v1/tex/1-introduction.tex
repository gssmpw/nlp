\section{Introduction}
Side-channel analysis (SCA) is a security exploit that focuses on inferring secret information from side-channel leakage collected during the execution of software or hardware systems.
Commonly-used side-channel leakages include power consumption~\cite{wei2018know, xiang2020open, randolph2020power}, electromagnetic (EM) emanations~\cite{kasper2009side, longo2015soc}, and execution time~\cite{jiang2017novel, zhang2016cloudradar}.
SCA  has primarily targeted %
cryptographic systems for recovering the secret keys or sensitive messages. 
Recently, with the wide deployment of Deep Neural Networks (DNNs), the pre-trained DNN models become a lucrative target of SCA~\cite{batina2019csi, xiang2020open,  ding2023emshepherd, wei2020leaky}.
Particularly for intelligent edge devices, such as FPGAs and Microcontroller Units (MCUs), physical access or proximity makes it easy to collect side-channel power and EM traces~\cite{batina2019csi, gongye2023side}.
Valuable intellectual property (IP) of DNNs includes network structures, hyperparameters, and parameters such as weights and biases, which are all susceptible to SCA. Such attacks not only compromise the model confidentiality and infringes IP, but also facilitates other serious attacks including integrity breaches~\cite{madry2017towards, tramer2020adaptive, rakin2019bit} and model inversions ~\cite{rakin2022deepsteal, dmitrenko2018dnn}.

Existing defense mechanisms against SCA on cryptographic systems, such as masking and shuffling, can be applied to DNN models too, but often incur a a large execution overhead.
Modern ciphers involve simple operations and a small amount of secrets, and thereby are not computation-intensive.  
In a contrast, DNN models are composed by high volume of parameters and involve enormous operations, making the prior mitigation methods computational prohibitive. %
For instance, %
masking an embedded DNN results in a $127\%$ increase in the number of cycles compared to the unmasked DNNs~\cite{dubey2020maskednet}.
Hiding techniques such as operation shufflingalso result in substantial computation and storage overheads, with a reported latency increase of $18\%$~\cite{brosch2022counteract}.
We realize that previous defenses have not considered the distinct characteristics of DNN models, such as their inherent tolerance of input variations. 
To bridge the gap between general side-channel countermeasures and the unique features of DNN models, we introduce \method.
Our approach leverages the robustness of DNN models to input variations to protect the model confidentiality with high efficiency and minimal DNN performance degradation.

\fei{what do you want to say for these prior masking methods? they are not for side-channel mitigation. have to point it out}
\ruyi{I want to provide more background of input masking of DNN, to show our motivation of preventing EM side channel. Revise here.}
DNNs have been demonstrated to be robust to missing features of the model inputs, a property that has been utilized in model training~\cite{he2022masked} and inference~\cite{gao2017deepcloak} to improve their generalizability and adversarial robustness.
Leveraging such tolerance of incomplete or partially occluded inputs~\cite{zhang2013occlusion}, our proposed \method, for the first time, applies dynamic input dropping to defend DNN execution against SCA-based model extraction attacks on edge devices.
\method proposes a random pixel activation map (called \texttt{RPAM}) to be applied on the model input, where a subset of pixels is deactivated and the subsequent operations on them are pruned at run-time, effectively protecting the model parameters against DEMAs, which rely heavily on the sequential execution order of leaky operations and precise leaky locations. 

Compared with previous SCA defense methods, \method is lightweight and achieves high efficiency, as our defense is based on random operation-skipping, instead of shuffling. %
However, \method may degrade the DNN performance (classification accuracy) as it reduces the amount of input information during inference.
To minimize the performance degradation due to the loss of information from deactivated pixels, we introduce an importance-aware pixel activation map (\texttt{IaPAM}). 
Specifically, critical pixels are identified and excluded from deactivation, while other less important pixels are randomly dropped at certain prescribed rate. 
Preserving critical pixels ensures the DNN inference performance, and the randomness of deactivating other pixels offers SCA mitigation. 
The IaPAM is trained using a modified loss function that balances the trade-off between defense efficacy and DNN performance.

The major contributions of this paper are the following:
\begin{itemize}
    \item \method, a novel and lightweight defense mechanism, is proposed to protect DNN models against SCA-assisted model extraction attacks. The advantage of our method over traditional SCA countermeasures is that characteristics of DNN models are utilized to achieve both security and efficiency.
    \item A pixel importance aware strategy is introduced and incorporated in \method to preserve performance-critical pixels, so as to strike a balance between security and model performance.
    \item Comprehensive theoretical analysis and rigorous evaluations on real EM measurements from an actual microcontroller are conducted. 
    Specifically, we show that the proposed \method with \texttt{RPAM} and \texttt{IaPAM} offers adequate model confidentiality protection with a small overhead (less than $3\%$).
\end{itemize}

