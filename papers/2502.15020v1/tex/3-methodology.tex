\section{Our Approach: \method} \label{sec: methodology}
This section outlines the methodology of our approach \method. We first present the threat model and summarize the characteristics of DNNs under SCA. We then illustrate how to leverage these characteristics for an efficient and effective SCA defense approach. Finally, we describe how to train the importance-aware pixel activation map in detail. 

\begin{figure}[t]
  \centering\includegraphics[width=\linewidth]{fig/attack-model-1.pdf}
  \caption{Illustration of the threat model. The victim model is deployed on an edge device MCU. The attacker is able to conduct DEMA to retrieve {\color{red!50}secret parameters}. Our proposed protection {\color{benign!70} \method} is deployed during the DNN training and inference phase to hide side-channel leakage.}
  \label{fig: threat_model}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/attack-process-1.pdf}
    \caption{\textbf{DEMA attack procedure on MCU against edge DNNsand \method defense rationale:} the attacker leverages DEMA attack by correlating the EM leakage signals with the cumulative values in a sequential manner, they must attack the target weights $w_1$, $w_2$, $w_3$ in order. Note that when attack $w_1$, they can only get possible values of $w_1$ due to aliasing, denoted as $\tilde{w}_1$. The real value of $w_1$ can be confirmed in Step 2. By randomly deactivate the input pixel, \method drops the corresponding MAC operations so that the percentage of vulnerable EM signal reduced.}
    \label{fig: attack process}
\end{figure*}
\subsection{Threat Model} \label{sec: threat model}

In line with the common threat model for EM side-channel analysis, we delve into scenarios where an adversary attempts to extract secret model parameters through DEMA. We provide the setup of victim implementation and define the adversary's knowledge and capabilities. Fig.~\ref{fig: threat_model} presents the threat model.

\subsubsection{Victim}
In Figure~\ref{fig: threat_model}, the victim DNN model is deployed on the MCU of an edge device, equipped with standard inference firmware, such as TensorFlow Lite Micro\footnote{\url{https://www.tensorflow.org/lite/microcontrollers}}. The victim model is trained \textit{a prior} with a private dataset, and the model weights and biases are securely loaded in the MCU program memory. The model parameters are all fully quantized 8-bit signed integers, adhering to the TensorFlow Lite Micro specifications. Notably, a typical commodity MCU has an optional DSP for performing MAC operations.

\subsubsection{Attacker's Knowledge}
We posit an informed attacker, who has knowledge of the model hyperparameters, including the structure and activation functions of the victim model. Additionally, they know the DUT implementation details, including the sequence of MAC operations. However, they do not know the model parameters (weights and biases), and aim to retrieve them to steal the IP or facilitate other white-box attacks.

\subsubsection{Attacker's Capability}
The attacker has physical access to the DUT or is in proximity, and can capture EM emissions of model inference in traces. Furthermore, he controls inputs to the victim model and monitors the inference output. Techniques such as direct program memory dumping, bus snooping, fault injection, and learning-based model extraction attacks fall outside the scope of this work.

\subsection{DEMA Attack Procedure on MCU}
In Fig.~\ref{fig: attack process}, we illustrate the process of a DEMA attack. Specifically, the chosen leakage model is the Hamming weight of cumulative values of user inputs and model weights (i.e., MAC results), instead of individual products of a weight and an input due to the strong aliasing effects of such operations~\cite{gongye2023side}.
As shown in the example, the attacker follows the order of operations and targets $w_0$, $w_1$, and $w_2$ sequentially. The proposed \method obfuscates the leaky signals on the EM traces by randomly deactivating pixels according to a predefined pixel map. 
\method significantly reduces the usable signals, with the reduction occurring at an exponential rate under normal attack conditions.
Potential adaptive attacks, utilizing multiple leaky points, will be discussed in Section~\ref{sec: adaptive}.


\subsection{Key Observations} \label{sec: observations}
In this section, we describe the distinct characteristics of quantized DNN model inference on MCUs under DEMA attacks, which our defense strategy  takes into consideration.


\begin{figure}[H]

  \noindent\hspace*{-\fboxrule}
  \begin{Sbox}
    \begin{minipage}{0.95\linewidth}
      \textbf{Observation 1 (O1).}
      DEMA targeting DNN parameters on MCUs utilizes the HW model over accumulated MAC outputs.
    \end{minipage}
  \end{Sbox}

  \fcolorbox{black}{boxcolor}{\TheSbox}
\end{figure}

For DNN inference on MCUs, the in-order execution of instructions renders them particularly vulnerable to DEMA attacks.
With time points for operations that process secret parameters pinpointed on EM traces, adversaries can apply DEMA across the set of traces to recover the parameters with an effective leakage model that depicts the dependence of the side-channel leakage on the secret.

In cryptographic systems such as AES, DEMA employs the HW model on non-linear operations like the Rijndael Substitution.
The non-linearity yields strong distinguishability for the correct secret value.
However, operations in DNN models are predominantly linear, and the HW model on them does not work for SCA anymore. For instance, a common multiplication operation ($i{\cdot}w$, where $w$ is the secret parameter and $i$ is the known and controllable input) demonstrates a significant aliasing effect in the HW of the result, especially when $w$ is a power of 2. This effect, affecting the majority of weight values (191 out of 256)~\cite{gongye2023side}, limits the usefulness of the HW model over individual operations in DEMAs on DNNs.

A more viable leakage model for DEMA on DNNs focuses on the cumulative results of multiplications.
For instance, we first attack $w_1$ with the HW of $i_1{\cdot}w_1$ but get a set of candidates of $w_1$ due to aliasing. Then, the sum $i_1{\cdot}w_1 + i_2{\cdot}w_2$ can be used to infer weights $w_1$ and $w_2$ without the aliasing effect, owing to the non-linear addition operation. Subsequent operations, such as adding $i_3{\cdot }w_3$ to this sum, would reveal $w_3$ at the next leaky time point, and so on.
Therefore, a leakage model for effective DEMA targeting DNN parameters on MCUs, shifts from individual operations to the aggregate result of a series of operations.
Identifying and analyzing such sequences of accumulative operations becomes essential for a successful DEMA attack to retrieve secret parameters.

\begin{figure}[H]

  \noindent\hspace*{-\fboxrule}
  \begin{Sbox}
    \begin{minipage}{0.95\linewidth}
      \textbf{Observation 2 (O2).}
      Safeguarding the first layer of DNNs is pivotal for effective and efficient SCA defense.
    \end{minipage}
  \end{Sbox}
  \fcolorbox{black}{boxcolor}{\TheSbox}

\end{figure}

DNN layers operate in a feed-forward fashion during inference, i.e., each layer's outputs feed to the next layer as inputs. With such data dependency, parameter extraction typically starts with the first layer and progressively proceeds to the subsequent layers.
In our threat model, this layer-by-layer strategy is crucial for an attacker aiming to completely extract the DNN model.
When the first layer is safeguarded, an attacker is hindered from learning the first layer output, which is required for attacking the following layers iteratively.
Hence our defense mechanism against DEMA attacks only needs to focus on protecting DNN's first layer.

\begin{figure}[H]

  \noindent\hspace*{-\fboxrule}
  \begin{Sbox}
    \begin{minipage}{0.95\linewidth}
      \textbf{Observation 3 (O3).}
      DNNs possess the inherent tolerance of input variations, a feature that can be leveraged for effective defenses against EM side-channel attacks.
    \end{minipage}
  \end{Sbox}

  \fcolorbox{black}{boxcolor}{\TheSbox}
\end{figure}

For image classification tasks, the human brain demonstrates remarkable robustness--capable of recognizing objects even when only parts of them are visible.
DNNs, inspired by human neural processing, retain similar robustness and are able to withstand input variations, specifically pixel losses.
We conduct a preliminary experiment. For samples in the MNIST dataset, the pixel value is only zero (black) or one (white). We pick a sample, randomly set some pixels to zero (black), and evaluate the classification errors of a corresponding DNN.
Fig.~\ref{fig: random mnist} shows the results for digit `6': the sample remains visually identifiable even as the ratio of zeroed-out pixels increases from $10\%$ to $90\%$; while the DNN's error rate escalates with the loss of pixels, it largely maintains its classification accuracy even when the zeroization ratio reaches $50\%$ (accuracy of the baseline model is marked as the bottom {\color{red} red} line).

These phenomena motivate our distinctive defense strategy for DNNs, diverging from the traditional side-channel countermeasures used in protecting ciphers against DEMA.
For example, the shuffling method does not reduce the number of operations, and merely moves them around to break alignment of leaky signals on the traces, required by differential EM analysis. 
In contrast, our \method focuses on the input and the first layer of the DNN, based on the three prior observations, leveraging the inherent input variation tolerance of DNNs and the feed-forward layer-wise data dependency.
This method effectively mitigates DEMA-based model extraction with negligible or even negative execution overhead while maintaining the model accuracy.
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig/mnist-comb-reverse-1.pdf}
  \caption{DNN classification accuracy with the pixel zeroization ratio from  $10\%$ to $90\%$ for an MNIST sample.}
  \label{fig: random mnist}
\end{figure}


\subsection{Defense Overview} \label{sec: defense-overview}

Based on observations \textbf{O1} and \textbf{O2}, we aim to change the sequence of MAC operations in a DNN's first layer to counteract EM DNN parameter extraction attacks. 
Observation \textbf{O3} indicates that by deactivating some input pixels and dropping the corresponding MAC operations, DNN performance might not be affected much.
Thus, altering a DNN's input to disrupt MAC operations is a viable defense strategy. 
We introduce our first defense against DEMA--\textit{Random Pixel Activation Map} (\texttt{RPAM}), as detailed in Section~\ref{sec:rpa}.
\texttt{RPAM} selectively activates a subset of pixels in each inference. To equip the model with such robustness, we also consider in-complete inputs during the training phase of embedded DNNs, as shown in Fig.~\ref{fig: threat_model}. 
The embedded DNN is fine-tuned on images whose pixels are randomly deactivated with $1-p$ during the training phase. 
\ding{This sounds like that you randomly select some inputs to deactivate. Then these selection is fixed. The images with these fixed deactivation is used to fine-tune the DNN. Is this true? I thought that the point of fine-tuning is that the DNN is robust to inputs randomly dropping $1-p$ proportion, similar to a random drop-out network. It is not supposed to be specific to a particular $p$ activated inputs pattern. If so, we should say that ''... DNN is fine-tuned on images with $1-p$ proportion of randomly deactivated inputs." }
\ruyi{It is the second case, the DNN is fine-tuned with random inputs }
We defined the \textit{pixel activation ratio as $p$.} 
During inference, a random \texttt{RPAM} with the rate $p$ will also be applied to the inputs. 
This random skipping of MAC operations eliminates some signals on the EM traces and moves later signals to earlier time points, exponentially increasing the complexity of trace analysis over time.
Moreover, if a pixel is inactive, a group of MAC operations are bypassed, thereby reducing the computation and leading to faster DNN inference.
While activating pixels randomly is effective for simpler datasets (e.g., MNIST), for more complex datasets, an \textit{Importance-aware Pixel Activation Map} (\texttt{IaPAM}) can preserve the inference accuracy better, as detailed in Section~\ref{sec:ipam}.

\subsection{Random Pixel Activation Map} \label{sec:rpa}

\texttt{RPAM} enhances the robustness of DNN models against side-channel model extraction attacks by redistributing the leakage signals in the time domain. 
Consider two consecutive MAC operations, and the accumulator output is $s$. The first MAC operation is $s = s + i_1w_1$, and the second $s = s + i_2w_2$. As per \textbf{O1}, attackers aim to deduce $w_1$ at the first time point (corresponding to the addition) and $w_2$ at the second time point. 
When the pixel activation ratio is $p$, the probability of the first MAC operation occurring as scheduled is $p$. When it is pruned, the following MAC operations are shifted earlier on the timeline. 
Subsequently, for the $j$-th MAC operation ($j>1$), it executes with a probability $p$ and its execution position spread out according to which of the $2^{j-1}$ possible sequences for the previous $j-1$ operations is executed.
The highest probability for one sequence containing the $j$-th MAC operation executing at a particular time point is $p \cdot \max(p,1-p)^{j-1}$, as derived later in Section~\ref{sec:theory}.
\fei{I find this position analysis not correct. the $j^{th}$ operation only has j positions, right?} 
\ruyi{First there is an assumption that jth operation is execute (the first p) in the formula, then, for all operations before j, (there are j-1) operations, we can execute or not. So the number is $2^{j-1}$. Please note that we have another assumption, if the activation sequence is different, the leakage point will be different.}

Mangard et al.~\cite{mangard2008power} demonstrated that, with uncertain leaky time points as a countermeasure, the required \update{number of traces for successful secret retrieval increases by $R$ times over the original traces required}, with $R = \hat{p}^{-2}$, where $\hat{p}$ is the probability of a specific operation occurring at a predetermined time.
Here we follow this notation: $R$ is defined as the ratio between the number of traces required to retrieve the secret in a protected system versus unprotected ones using DEMA.
Thus for attacking the $j$-th weight at the best time point, $R = p^{-2} \cdot \max(p,1-p)^{2(j-1)}$. For DNNs with a lot of parameters, the challenge of conducting side-channel attacks on later parameters is amplified due to the uncertain time of operation executions, and the attack complexity increases exponentially with $j$.
\ding{We may also want to start the $p$ in Table 1 from $p=0.5$ only. Or change the $p<0.5$ cases to the new formula }
 \ruyi{I update the table with $p<0.5$ using new equation}

\begin{table}[h]
  \centering
  \small
    \caption{Leaky time point $j^*$ when $R\geq1000$ for ratio $p$}
  \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
\hline
    $p$   & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\
    \hline
    $j^*$ & 12   & 10   & 8   & 6   & 5   & 7   & 10  & 16  & 33  \\
    \hline
  \end{tabular}

  \label{tab: Nr vs p}
\end{table}

We define $j^*$ as the earliest $j$th time point when $R\geq 1000$. This is based on the premise that attacks on the original design, vulnerable with just $1000$ traces, become impractical on the system protected by \method when $R \geq 1000$, i.e., requiring over a million traces. 
As trace collection is time-consuming in EM and power side-channel analyses, a significant increase in trace count deters such attacks and one million traces is a common threshold for side-channel power/EM attacks. 
A smaller $j^*$ indicates better protection on the DNN.
Table~\ref{tab: Nr vs p} shows how the activation ratio $p$ affects the value of $j^*$: 
when $p=0.5$, we have $j^*=5$, which implys that only the first $5$ weights remain vulnerable. 
Considering the large number of weights in DNN layers (e.g., 864 weights in MobileNetV2's first layer), exposure of a small subset does not significantly compromise the network confidentiality.
Therefore, \texttt{RPAM} is deemed an effective mitigation for SCA on embedded DNNs.

\subsection{Importance-aware Pixel Activation Map} \label{sec:ipam}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/masks.pdf}
\caption{A CIFAR-10 image with activation ratio $0.5$}
\label{fig: two masks}
\end{figure}



Random activation of input pixels works well for simple datasets such as MNIST. However, when input is complex, i.e., CIFAR-10, the information loss significantly impairs the performance of pre-trained models.
Fig.~\ref{fig: two masks} illustrates this phenomenon (the second and fifth columns), where randomly activated pixels (\texttt{RPAM}) fail to preserve crucial details of the input (under activiation rate of 50\%).
To address such degradation, we leverage the intrinsic nature of DNN inference--input features vary in their contributions to the model performance: more informative features such as the object's shape and colors often directly affect the prediction outputs; while features with less information, such as the background, might have much less influence on the results~\cite{selvaraju2017grad}.
In light of this, we introduce the Importance-aware Pixel Activation Map (\texttt{IaPAM}), which enhances the robustness of the protected DNN by preventing the most critical pixels in the input based on their importance to the model's performance from being dropped, i.e., never deactivated.
Fig.~\ref{fig: two masks} (the third and sixth columns) shows the result of applying \texttt{IaPAM}, where the principal patterns of the input image are well-preserved (near the center), and other less important pixels are randomly dropped. 
We expect that its classification accuracy will exceed that of \texttt{RPAM}.



The \texttt{IaPAM} is a trainable activation map applied to the inputs of DNN. It is integrated as an additional layer preceding the pre-trained model, denoted $\mathcal{M}$. 
Fig.~\ref{fig: importance-aware Activation Mask} illustrates the training process.  
Without loss of generality, assuming the input is two-dimensional ($[I, J]$), the coordinate of a pixel in the input is $(i, j)$, and the weights associated with each pixel are represented by $m_{ij}$.
These weights $m_{ij}$ quantify the relative importance of each pixel, and are modulated by a sigmoid function to constrain their output range.
Similar to \texttt{RPAM}, we define $p$ as the proportion of pixels activated and $q$ as the maximum proportion of critical pixels, thereby the uncritical weights will be randomly activated with a probability at $\frac{p-q}{1-q}$.
Given the non-differentiability of the binary map during back-propagation, we employ the straight-through estimator (STE) technique, as suggested by~\cite{ courbariaux2015binaryconnect}, to facilitate the optimization of the activation map's weights.
\fei{zero means critical or one means critical?} \ruyi{one mean critical}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{fig/iapam-overview.jpg}
\caption{\texttt{IaPAM} recognizes the variance of input pixel importance and finds the most critical pixels to generate an activation map with STE and one-shot pruning.}
\label{fig: importance-aware Activation Mask}
\end{figure*}

The loss function for IaPAM training utilizes an L1 regularization:

\begin{equation}
L_{\texttt{IaPAM}} = L_{CE} + \alpha L_1(\frac{\text{nnz}(\mathcal{M})}{|\mathcal{M}|}-q)
\label{eq: loss for LaPAM}
\end{equation}

Here, $L_{CE}$ denotes the cross-entropy loss for classification, the L1 regularization term controls the difference between the activated pixels in $\mathcal{M}$ and the desired ratio $q$, where the operator $\text{nnz}(\cdot)$ counts the non-zero values and $|\cdot|$ compute the cardinality. 
We utilize the coefficient $\alpha$ to strike a balance between the DNN performance and the input sparsity, which will be further studied in Section~\ref{sec: ablations}.
The full algorithm is shown in Algorithm~\ref{alg: iapam}. 
We optimize the map layer $\mathcal{M}$ for multiple iterations and compute an importance score for each pixel $\mathcal{S}$ by summing up the associated weights importance for every iteration.
The pixels corresponding to the top $q$ importance scores are selected as the critical ones.
Although a subset of pixels is always activated, the remaining pixels are randomly activated at a scaled ratio, further complicating operation localization by the adversary. For example, take a scenario where $p=0.8$ and $q=0.5$; here, non-critical pixels are randomly activated at a ratio of $\frac{p-q}{1-q}=0.6$.
Referring to Table~\ref{tab: Nr vs p}, when $p=0.8$, if the initial 16 pixels are non-critical, $R$ will exceed 1000 at the 17th weight.\fei{when checking Table 1, should you use p = 0.8 or 0.6? I think you should still use p=0.8?}
\ruyi{This will be more complex to do mitigation analysis IaPAM. If there is critical pixel in the first 7 pixels (consider the fact that most of critical pixels are centralized, the mitigation strength will equal to p=0.6. }
As seen in Figure~\ref{fig: masks-for-cifar} (in Section~\ref{sec: exp: IaPAM}), the upper left corner where the computation starts is always non-critical for images in CIFAR-10, thus the weight confidentiality is still well-preserved.

\begin{algorithm}[t]
\caption{Importance-aware Pixel Activation Map}\label{alg: iapam}
\begin{algorithmic}[1] %
\REQUIRE Pre-trained model $f$, Iterations number $Iter$, Critical weights ratio $q$, Training Dataset $(x_\text{train}, y_\text{train})$, Number of Epoch $E$, Map weights $\{m_{ij}\}$
\ENSURE Importance-aware Pixel Activation Map $\mathcal{M}$.

\STATE Initialize $m_{ij}\leftarrow\mathbf{0}$
\FOR{ $\text{iter}=1, 2, \dots, Iter$}
    \FOR{$epoch=1, 2, \dots, E$}
        \STATE Activate the map $\mathcal{M}=\{\text{sigmoid}({m_{ij}})\}$
        \STATE Do inference with map $y_{\text{pred}}=f(\mathcal{M}(x_\text{train}))$
        \STATE Compute the Cross-entropy Loss $L_{\text{CE}}(f(x_{\text{train}}), y_{\text{train}})$
        \STATE $\{m_{ij}\}\leftarrow$ optimize the map weights with $L_\text{IaPAM}$~\eqref{eq: loss for LaPAM}
    \ENDFOR
    \STATE Update the important score of each pixel: $\mathcal{S} = \mathcal{S} + \{\text{sigmoid}(m_{ij})\}$ 
\ENDFOR
\STATE $\mathcal{M}_{ij}\leftarrow \begin{cases} 1 & \text{if } \mathcal{S}_{ij} \text{ is the top } q \text{ percentage values in } \mathcal{S}\\ 0 & \text{otherwise} \end{cases}$
\RETURN $\mathcal{M}$
\end{algorithmic}
\end{algorithm}
