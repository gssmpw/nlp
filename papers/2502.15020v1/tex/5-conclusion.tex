\section{Adaptive Attack against \method} \label{sec: adaptive}

\method mitigates side-channel-based weight-stealing attacks by redistributing the leakage signals of DNN weights across different time points. We assume the attacker targets the Hamming weight model of cumulative results, requiring weights to be attacked sequentially. So far the attack model is to exploit the leakage at the
$j$-th time point corresponding to the $j$-th weight. %
\method mitigates the attack effectively, as the probability of obtaining traces with meaningful leakage signals from the targeted attack model diminishes exponentially as $j$ increases.

In this section, we consider a stronger attacker who is aware of \method and can utilize all leakage information at a certain time point across various sequences containing the target operation. While combining leakage signals is inherently challenging and worth further exploration, in this work we focus on analyzing the upper bound of the leakage signal for any potential adaptive attack, providing guidance for follow-on pragmatic attacks.

\subsection{Threat Model for Potential Adaptive Attack}
Similar to previous setting, we assume the attacker knows the DUT implementation details including the sequence of MAC operations. 
The attacker can control the inputs to the edge DNN and capture the EM emissions from the DUT.
Previously our analysis focused on the attacker correlating the HW of any specific sequence containing a target operation with the measurement traces.
Here, we instead consider that a potential attacker can leverage leakage information from different sequences containing the target operation at the same point. 
That is, if we executed $k-1$ MAC operations (out of the original $j-1$ operations) before the $j${th} original MAC, the $k$th leakage time point contains leakage of the $j$th weight with different sequences, each consisting of $k-1$ MACs plus the the $j${th} original MAC. 
Note that, such an adaptive attack is only theoretical since it is very challenging for an attacker to combine the multiple attack models for different MAC sequences at the same time point.

\subsection{Theoretical Leakage Upper Bound}
In this section, we present a theoretical upper bound for the leakage, demonstrating the effectiveness of \method even against a highly capable adaptive attacker. The derivation of the maximum proportion of leaked traces is detailed in Appendix~\ref{appendix: leakage upperbound} and given by:
\begin{equation}\label{eq:P.adapt}
    P = p^j \left(\begin{array}{c} j-1 \\ \lfloor pj \rfloor \end{array} \right) \left(\frac{1-p}{p}\right)^{j-\lfloor pj \rfloor -1}, \qquad \mbox{for } p<1.
\end{equation}
Here $\lfloor x \rfloor$ is the floor operation, i.e., the largest integer $\le x$.


\subsection{Mitigation Strength Against Adaptive Attack}
Based on the previous analysis of the maximum leakage, the mitigation strength of \method against any potential adaptive attack is quantified as:
\begin{equation}
    \label{eq: R2}
    R=\left[\left(\begin{array}{c} j-1 \\ \lfloor pj \rfloor \end{array} \right) p^{\lfloor pj \rfloor +1} (1-p)^{j-\lfloor pj \rfloor -1} \right]^{-2}
\end{equation}
The theoretical adaptive attack's mitigation strength \( R \) is visualized in Fig.~\ref{fig: R-adaptive}. Table~\ref{tab: adaptive j} demonstrates that the same criterion \( j^* \), as shown in Table~\ref{tab: Nr vs p}, achieves a mitigation strength greater than \( 10^3 \) for different values of \( p \).
For instance, with \( p = 0.5 \), \( j \geq 160 \), which indicates that \method still effectively protects the majority of weights in a DNN model. For example, the first layer of MobileNet-v2 contains \( 864 \) weights. We further analyze the end-result of partial weight recovery on the model performance%
in Fig.~\ref{fig: adaptive attack performance}.
Specifically, for a pre-trained model, we compare the cosine similarity between the original embeddings and the embeddings from a model with partially known weights (with the remaining weights randomly initialized). For embeddings to retain semantic meaning, a cosine similarity above \( 0.8 \) is required, meaning the attacker must know approximately \( 70\% \) of the weights to replicate the victim model's functionality.
As shown in Table~\ref{tab: adaptive j}, for a victim model MobileNet-v2 and with \( p = 0.5 \) as the pruning ratio, only the first \( 18.6\% \) of weights can be stolen with $R\leq 1000$. This results in embeddings with a cosine similarity of 0.2, which lack practical semantic significance for the layer's functional representation.

\begin{table}[h]
  \centering
  \small
    \caption{Leaky time point $j^*$ for potential adaptive attacks}
  \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
\hline
    $p$   & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\
    \hline
    $j^*$ & 19   & 40   & 71   & 107   & 160   & 249   & 399  & 679  & 1610  \\
    \hline
  \end{tabular}

  \label{tab: adaptive j}
\end{table}




\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
    \centering
        \includegraphics[width=0.99\linewidth]{fig/mitigation-strength-adaptive.pdf}
        \caption{Visualize Lower Bound of Mitigation Strength $R$ under different $j$ for potential adaptive attack.}
        \label{fig: R-adaptive}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
                   
                  \includegraphics[width=0.99\linewidth]{fig/cosine_similarity.pdf}
        \caption{Attack Performance in Embedding Similarity}
        \label{fig: adaptive attack performance}
    \end{subfigure}
    \caption{Experiments on Potential Adaptive Attack}
    \label{fig: ablation study on alpha}
\end{figure}


\section{Discussions and Future Works} \label{sec: conclusion}
In this section, we provide more insights about our proposed \method, including its limitations and potential future works.

\noindent\textbf{Applicable Devices:}
Our proposed method focuses on defending against SCA attacks on MCUs, which are the most vulnerable edge devices and are commonly targeted by attackers. The intrinsic parallel computation on more complex edge platforms poses challenges for SCA, and more advanced SCAs are required. Our proposed method still has the potential to protect complex systems against advnaced SCAs %
through careful analysis of the power model and effective leakage hiding.

\noindent\textbf{Scope of DNN models:}
In this work, we focused on image classification tasks using the MNIST and CIFAR-10 datasets on the MCU platform.
Our future work can explore more complex tasks such as object detection, image generation, or language models, which are usually deployed on more powerful platforms. Adapting our method to these advanced tasks and platforms will require further research and development.

\section{Conclusions} \label{conclusions}
This paper presents \method, a novel lightweight defense mechanism that counters DNN parameter retrieval based on side-channel analysis. It judiciously alters model inputs to thwart sequential SCAs targeting sensitive weights while preserving the model accuracy. This study underscores the effectiveness of \method and the importance of leveraging DNN characteristics for more efficient defense mechanisms.

