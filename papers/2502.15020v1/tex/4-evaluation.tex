\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/experiment-setup.pdf}
    \caption{Setup of EM leakage measurements.}
    \label{fig: experiment setup}
\end{figure}

\section{Evaluation}

\subsection{Experiment Setup} \label{sec: exp: setup}
\subsubsection{Measurement Setup}
Fig.~\ref{fig: experiment setup} depicts the trace collection setup.
The DUT has an ARM Cortex-M4 \verb|STM32F417IG| MCU\footnote{\url{https://www.st.com/en/microcontrollers-microprocessors/stm32f417ig.html}} featuring DSP and FPU capabilities, whose operating frequency is 168~\unit{MHz}.
An EM probe coupled with a pre-amplifier from Aaronia AG\footnote{\url{https://aaronia.com/en/probe-set-pbs-2-inkl-vorverstaerker}} is utilized to capture EM signals and convert them into voltage readings.
This probe is strategically positioned above the emission zones of the DUT and is linked to a Lecroy oscilloscope\footnote{\url{https://teledynelecroy.com/oscilloscope/}} via the pre-amplifier.
Both the DUT and the oscilloscope are interfaced with a host workstation, facilitating command execution and synchronization.
To acquire an EM trace, the host initiates the process by instructing the DUT to commence DNN inference.
Concurrently, the oscilloscope records the EM signals at a sampling rate at 5~\unit{GS/s}, which are then streamed to the workstation.
\subsubsection{Model Training Setup}
We evaluate \method's performance using MobileNet-V2~\cite{sandler2018mobilenetv2}, a widely adopted DNN for on-device applications on MNIST and CIFAR-10.
We train the MNIST model from scratch, allocating $80\%$ of the data for training and $20\%$ for testing, achieving a testing accuracy of $99.35\%$.
For CIFAR-10, we utilize a pretrained MobileNet-v2\footnote{\url{https://github.com/huyvnphan/PyTorch_CIFAR10}} with an accuracy of $93.15\%$.






\subsection{Selection of Leakage Points}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/correlation-coefficients-snr-1.pdf}
    \caption{Select Leakage Point(s) via SNR and Correlation}
    \label{fig: leakage points}
\end{figure}

To assess the mitigation strength fairly, we select time points exhibiting the strongest leakage for each MAC operation. 
The underlying rationale is straightforward: if an attacker cannot exploit the time point with the highest SNR, they cannot possibly exploit less informative ones. 
Specifically, the execution of assembly code by MCUs is deterministic, lacking advanced features such as branch prediction, out-of-order execution, or prefetching. 
Consequently, each MAC operation produces EM traces of uniform length, facilitating segmentation. We identify the clock cycle where leakage occurs and select this cycle for analysis, empirically confirming it exhibits the strongest leakage. 
Knowing the correct secret weight for each MAC operation enables us to compute the SNR for each time point in this segment using Formula~\eqref{eq: EM model}, as depicted in Fig.~\ref{fig: leakage points}. 
We target the time point with the highest absolute SNR value. 
For comparative analysis, we also display the DEMA results, where the Pearson Correlation coefficient for the correct guess is marked in {\color{red!60} red}, while other guesses are {\color{gray!50} greyed} out. 
The analysis shows that the time point with the strongest SNR corresponds to the DEMA results, which could be used by attackers to precisely extract secrets.



\subsection{Empirical Mitigation Strength on \texttt{RPAM}} \label{exp: empirical}
The results of the empirical measurement are illustrated in Fig.~\ref{fig: Nr}. The x-axis represents the time point, corresponding to individual MAC operations. We initiate the count from two to mitigate the impact of HW aliasing discussed in Section~\ref{sec: observations}.
The y-axis displays $R$ in log scale for enhanced clarity. 
The symbol $\textcolor{theo_color}{\blacktriangle}$ denotes the theoretical protection strength (Equation \eqref{eq: R}), while $\textcolor{mes_color}{\bullet}$
signifies the average of 20 separate measurements, with measurement is calculated on 100,000 EM traces (Equation \eqref{equ:R.emp}). 
The experiment demonstrates that our empirical measurements fit well with the theoretical mitigation strength for all $p$ values analyzed. 
In particular, with a smaller $p$, both theoretical and empirical mitigation strength increase, and for the pixels executed later (those with a large $j$ index), the protection will be better, which perfectly matches our analysis in Section~\ref{sec:theory}. However, an excessively small $p$ might cause significant model accuracy degradation, which will be further discussed in Table~\ref{tab: IaPAM performance}.

\ding{what are the theoretical value $\textcolor{theo_color}{\blacktriangle}$ and $\textcolor{mes_color}{\bullet}$? I thought they were values from \eqref{eq: R} and \eqref{equ:R.emp} respectively. Is that true? You used 20 datasets repeatedly fitting \eqref{equ:R.emp}?}



\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/eval-nr.pdf}
    \caption{\textbf{Empirical measurement of $R$.} The symbol $\textcolor{mes_color}{\bullet}$ represents the measured values, while $\textcolor{theo_color}{\blacktriangle}$ represents the theoretical values.
    }
    \label{fig: Nr}
\end{figure}

\subsection{Evaluation on \texttt{IaPAM}} \label{sec: exp: IaPAM}

The proposed \texttt{IaPAM} identifies and maintains the activation of critical pixels in input images throughout the inference process.
Fig.~\ref{fig: masks-for-cifar} illustrates the masked CIFAR-10 images at various importance activation ratios, referred to as $q$.
Notably, \texttt{IaPAM} operates on the individual channels of the 3-channel RGB inputs.
It consistently highlights that vital pixels tend to cluster around the central area of the inputs, which frequently corresponds to the main subject of the classification task.
Such a concentration of importance scores of \texttt{IaPAM} in the center significantly enhances the model's defense performance against side-channel attacks targeting DNN parameter extraction, which usually starts at randomly activated pixels on the peripherals, making it challenging for an adversary to sequentially extract weights effectively.
Moreover, our analysis indicates a tendency for importance scores to concentrate more on the Red and Green channels of CIFAR-10 inputs.
This is presumably because elements in the Blue channel, such as the sea and sky, often represent background areas that are less crucial for classification purposes.


In Table~\ref{tab: IaPAM performance}, we demonstrate the superior effectiveness of \texttt{IaPAM} (the third to seventh column with different $q$), compared to \texttt{RPAM} (the second column).
To quantify the impact of additional pixel activation maps on model accuracy, we introduce the metric of relative accuracy drop, defined as $\frac{\text{Acc}_{\text{org}} - \text{Acc}_{\text{pam}}}{\text{Acc}_{\text{org}}}$, where $\text{Acc}_\text{org}$ is the accuracy of the plain model without \method, and $\text{Acc}_\text{pam}$ is that of the protected secure model.
Notably, \texttt{IaPAM} significantly mitigates the performance degradation typically associated with the loss of information when inputs are randomly discarded.
For instance, when the activation ratio is $70\%$, the degradation of \texttt{IaPAM} with $q$=40\% is $2$ times smaller than that of \texttt{RPAM}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fig/real-masks.pdf}
    \caption{Obtained critical pixels at different $q$ values.}
    \label{fig: masks-for-cifar}
\end{figure}

\begin{table}[t]
    \centering
    \footnotesize
    \caption{Relative accuracy drop by applying different activation strategies during DNN inference on CIFAR-10. $p$ denotes the activation ratio and $q$ denotes the ratio of critical pixels.}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{c|c|c|c|c|c|c}
            \toprule[0.3mm]
            $p$ & \texttt{RPAM} & $q$=0.1 & $q$=0.2 & $q$=0.3 & $q$=0.4 & $q$=0.5 \\
            \hline
            0.2 & 15.08\%$\downarrow$      & 7.84\%$\downarrow$ & -       & -       & -       & -       \\
            \hline
            0.3 & 10.20\%$\downarrow$      & 6.72\%$\downarrow$ & 5.24\%$\downarrow$  & -       & -       & -       \\
            \hline
            0.4 & 7.64\%$\downarrow$       & 6.54\%$\downarrow$  & 5.01\%$\downarrow$ & 4.75\%$\downarrow$  & -       & -       \\
            \hline
            0.5 & 5.35\%$\downarrow$       & 5.03\%$\downarrow$  & 4.07\%$\downarrow$  & 4.05\%$\downarrow$  & 2.23\%$\downarrow$  & -       \\
            \hline
            0.6 & 4.30\%$\downarrow$       & 4.17\%$\downarrow$  & 3.23\%$\downarrow$  & 3.17\%$\downarrow$  & 1.81\%$\downarrow$  & 1.18\%$\downarrow$  \\
            \hline
            0.7 & 3.48\%$\downarrow$       & 2.66\%$\downarrow$  & 2.64\%$\downarrow$  & 2.52\%$\downarrow$  & 1.72\%$\downarrow$  & 0.91\%$\downarrow$  \\
            \hline
            0.8 & 2.13\%$\downarrow$      & 2.04\%$\downarrow$  & 1.96\%$\downarrow$  & 1.76\%$\downarrow$  & 1.05\%$\downarrow$  & 0.68\%$\downarrow$  \\
            \hline
        \end{tabular}}

    \label{tab: IaPAM performance}
\end{table}

\subsection{Overhead and Comparison}
To assess the execution overhead of \method on MCUs, it is crucial to examine its implementation details, especially the MAC operations within a loop structure. \update{The integration of our defensive strategy necessitates the inclusion of a conditional check before each MAC operation to determine whether it should be executed or skipped, based on a random number. This random number is held in a 32-bit unsigned integer and is accessed using a one-bit map that shifts left with each iteration.}\fei{is this implementation only for RPAM? how to implement Ruyi's IaRPAM.}
The Cortex-M4 programmer's manual~\cite{cortex-M4} shows that the branching instruction of this check takes one cycle if not executed, and $1+D$ cycles otherwise, with $D$ being the pipeline length (1-3 stages). Random number generation involves a left shift and `and' operation, each taking one cycle. Index updates require two cycles, and two loads need three cycles. Assuming a DSP extension, a MAC operation takes one cycle. For a first layer with $M$ MAC operations, the total cycles under defense of \texttt{RPAM} are $Mp(1+D+2+6)+M(1-p)=((8+D)p+1)M$, versus $6M$ without defense. For the defended system to be faster, the pixel activation ratio $p$ must satisfy $p < \frac{5}{8+D}$. Even at $D = 3$, the system is faster if $p < 0.45$. This aligns with measurement results on our DUT with $D=3$.
Implementing \texttt{IaPAM}, akin to \texttt{RPAM}, involves an extra conditional instruction to assess pixel importance. This process includes a 5-cycle lookup operation to identify critical pixels using a binary table.  An additional conditional branch is used to decide on random pixel activation. The total number of cycles for \texttt{IaPAM} for the first layer is $[(8+D)p+13-8q]M$.
\ding{This is overhead: after subtracting 6M?}\ruyi{Yes, revised here.}

According to results shown in Fig.~\ref{fig: visualize R}, an ideal random pixel activation ratio is $0.5$. 
Therefore, we take the performance of the DNN overhead and accuracy degradation on CIFAR-10 for \texttt{IaPAM} when $p=0.7$ and $q=0.4$, i.e., $\frac{p-q}{1-q}=0.5$ and the \texttt{RPAM} with the same activation ratio $p=0.7$, as shown in Table~\ref{tab: different models}.
We also do the same evaluation on multiple models including MobileNet-v2~\cite{sandler2018mobilenetv2}, DenseNet121~\cite{huang2017densely}, and GoogleNet~\cite{szegedy2015going}, both methods show effectiveness in balancing accuracy drop and overhead.
Compared with the reported $18\%$ overhead for state-of-the-art (SOTA) shuffling defenses~\cite{brosch2022counteract} and $127\%$ for the masking on DNN~\cite{dubey2020maskednet}, both \texttt{RPAM} and \texttt{IaPAM} outperform in the computation overhead with acceptable accuracy degradation.


\begin{table}[t]
    \centering
    \footnotesize
        \caption{Accuracy Degradation and Defense Overhead}
    \resizebox{\linewidth}{!}{
        \begin{tabular}{l|c|c|c|c|c}
            \toprule[0.3mm]
            \multirow{2}{*}{Structure} & Original & \multicolumn{2}{c|}{\texttt{RPAM} (p=0.7)} & \multicolumn{2}{c}{\texttt{IaPAM} (q=0.4, p=0.7)} \\\cline{3-6}
              & Accuracy & Acc. Drop                & Overhead         & Acc. Drop               & Overhead          \\\hline
            \hline
            MobileNetV2 & 93.15\%  & 3.48\%$\downarrow$ & 0.27\%$\uparrow$ & 1.72\%$\downarrow$ & 0.61\%$\uparrow$ \\
            DenseNet121 & 94.06\%  & 5.71\%$\downarrow$ & 0.26\%$\uparrow$ & 2.74\%$\downarrow$ & 0.58\%$\uparrow$  \\
            GoogleNet   & 92.84\%  & 3.78\%$\downarrow$ & 1.01\%$\uparrow$ & 2.87\%$\downarrow$ & 2.23\%$\uparrow$  \\
            \hline
        \end{tabular}}

    \label{tab: different models}
\end{table}



\subsection{Ablation Study} \label{sec: ablations}
In the design of IaPAM, the regularization term $\alpha$ in $L_{\text{IaPAM}}$ is utilized to balance the map activation ratio for each iteration (controlled by the $L_1$ norm) and the deactivated pixel's importance (controlled by $L_{CE}$).
To further understand the effectiveness of regularization weights $\alpha$, we evaluate the IaPAM performance at different $\alpha$ values. 
In our experiment, we select $p=0.7$ and $q=0.4$. In Fig.~\ref{fig:alpha-sub1}, we visualize the activation ratio change with the training epochs with different $\alpha$, and in Fig.~\ref{fig:alpha-sub2}, we show the IaPAM performance with various $\alpha$ when fine-tuning the pre-trained model with $5\%$ training samples.
Note that we use a small number of training samples to fine-tune the pre-trained model so as to demonstrate the impact of different $\alpha$ more clearly.



\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
    \centering
        \includegraphics[width=0.99\linewidth]{fig/alpha-activation.pdf}
        \caption{Epochs vs Activated Ratio}
        \label{fig:alpha-sub1}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=0.99\linewidth]{fig/alpha-accuracy.pdf}
        \caption{IaPAM Performance vs $\alpha$}
        \label{fig:alpha-sub2}
    \end{subfigure}
    \caption{Ablation Study on the Regularization Weights $\alpha$}
    \label{fig: ablation study on alpha}
\end{figure}
\noindent\textbf{Activated Pixel Ratio versus $\alpha$:}
The choice of $\alpha$ will directly affect the final number of activated pixels during training of the mapping layer $\mathcal{M}$.
In particular, we observe that the activation ratio will increase from $0$ to the final activated ratio as the map is initialized as all-zero.
A small choice of $\alpha$ will cause the cross-entropy $L_{CE}$ to dominate the total loss, causing the activated ratio to keep increasing and exceed the maximum threshold of $q$ (as shown in the {\color{bleudefrance!60} blue} curves in Figure~\ref{fig:alpha-sub1}).
On the other hand, a higher $\alpha$ (the {\color{red!60} red} curves) will ensure the final activated pixel ratio is close to $q$.


\noindent\textbf{IaPAM performance versus $\alpha$:}
From Fig.~\ref{fig:alpha-sub2}, we observe that neither a too-small $\alpha$ nor too-large one will result in a good performance for IaPAM.
Specifically, according to Algorithm~\ref{alg: iapam}, as the final activated pixels ratio is higher than $q$, we will compute the importance scores based on the sum from multiple iterations.
In this case, critical weights might be removed, causing the final accuracy to be lower due to loss of information.
On the other hand, when $\alpha$ is large, the optimization process focuses more on the sparsity rather than the importance of pixels to minimize $L_{CE}$, thus causing poor performance after fine-tuning.
In conclusion, we carefully select $\alpha$ to ensure a balance between $L_{CE}$ and the $L_1$ regularization term.
A selection of $\alpha=1$ is used in the paper.

