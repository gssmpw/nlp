@article{kumar2024refusal,
  author    = {Kumar, P. and Lau, E. and Vijayakumar, S. and Trinh, T. and Team, S. R. and Chang, E. and Robinson, V. and Hendryx, S. and Zhou, S. and Fredrikson, M. and Yue, S. and Wang, Z.},
  title     = {Refusal-Trained LLMs Are Easily Jailbroken As Browser Agents},
  journal   = {ArXiv},
  year      = {2024},
  url       = {https://arxiv.org/abs/2410.13886}
}

@article{andriushchenko2024agentharm,
  author    = {Andriushchenko, M. and Souly, A. and Dziemian, M. and Duenas, D. and Lin, M. and Wang, J. and Hendrycks, D. and Zou, A. and Kolter, Z. and Fredrikson, M. and Winsor, E. and Wynne, J. and Gal, Y. and Davies, X.},
  title     = {AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents},
  journal   = {ArXiv},
  year      = {2024},
  url       = {https://arxiv.org/abs/2410.09024}
}

@article{lermen2024applying,
  author    = {Lermen, S. and Dziemian, M. and Pimpale, G.},
  title     = {Applying Refusal-Vector Ablation to Llama 3.1 70B Agents},
  journal   = {ArXiv},
  year      = {2024},
  url       = {https://arxiv.org/abs/2410.10871}
}


@misc{greshake2023youve,
      title={Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection}, 
      author={Kai Greshake and Sahar Abdelnabi and Shailesh Mishra and Christoph Endres and Thorsten Holz and Mario Fritz},
      year={2023},
      eprint={2302.12173},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{rbsc,
      title={Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield}, 
      author={Jinhwa Kim and Ali Derakhshan and Ian G. Harris},
      year={2023},
      eprint={2311.00172},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.00172}, 
}

@misc{bon,
      title={Best-of-N Jailbreaking}, 
      author={John Hughes and Sara Price and Aengus Lynch and Rylan Schaeffer and Fazl Barez and Sanmi Koyejo and Henry Sleight and Erik Jones and Ethan Perez and Mrinank Sharma},
      year={2024},
      eprint={2412.03556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.03556}, 
}
@misc{cb,
      title={Improving Alignment and Robustness with Circuit Breakers}, 
      author={Andy Zou and Long Phan and Justin Wang and Derek Duenas and Maxwell Lin and Maksym Andriushchenko and Rowan Wang and Zico Kolter and Matt Fredrikson and Dan Hendrycks},
      year={2024},
      eprint={2406.04313},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.04313}, 
}

@misc{hua2024trustagentsafetrustworthyllmbased,
      title={TrustAgent: Towards Safe and Trustworthy LLM-based Agents}, 
      author={Wenyue Hua and Xianjun Yang and Mingyu Jin and Zelong Li and Wei Cheng and Ruixiang Tang and Yongfeng Zhang},
      year={2024},
      eprint={2402.01586},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.01586}, 
}
@misc{yang2024watchagentsinvestigatingbackdoor,
      title={Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents}, 
      author={Wenkai Yang and Xiaohan Bi and Yankai Lin and Sishuo Chen and Jie Zhou and Xu Sun},
      year={2024},
      eprint={2402.11208},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2402.11208}, 
}

@misc{tian2024evilgeniusesdelvingsafety,
      title={Evil Geniuses: Delving into the Safety of LLM-based Agents}, 
      author={Yu Tian and Xiao Yang and Jingyuan Zhang and Yinpeng Dong and Hang Su},
      year={2024},
      eprint={2311.11855},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.11855}, 
}

@misc{cohen2024comesaiwormunleashing,
      title={Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications}, 
      author={Stav Cohen and Ron Bitton and Ben Nassi},
      year={2024},
      eprint={2403.02817},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2403.02817}, 
}

@misc{gu2024agentsmithsingleimage,
      title={Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast}, 
      author={Xiangming Gu and Xiaosen Zheng and Tianyu Pang and Chao Du and Qian Liu and Ye Wang and Jing Jiang and Min Lin},
      year={2024},
      eprint={2402.08567},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.08567}, 
}

@misc{park_generative_2023,
	title = {Generative {Agents}: {Interactive} {Simulacra} of {Human} {Behavior}},
	shorttitle = {Generative {Agents}},
	url = {http://arxiv.org/abs/2304.03442},
	doi = {10.48550/arXiv.2304.03442},
	abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
	urldate = {2024-07-27},
	publisher = {arXiv},
	author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
	month = aug,
	year = {2023},
	note = {arXiv:2304.03442 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}


@article{wang_survey_2024,
	title = {A {Survey} on {Large} {Language} {Model} based {Autonomous} {Agents}},
	volume = {18},
	issn = {2095-2228, 2095-2236},
	url = {http://arxiv.org/abs/2308.11432},
	doi = {10.1007/s11704-024-40231-1},
	abstract = {Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
	language = {en},
	number = {6},
	urldate = {2024-07-27},
	journal = {Frontiers of Computer Science},
	author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
	month = dec,
	year = {2024},
	note = {arXiv:2308.11432 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {186345},
	annote = {Comment: 35 pages, 5 figures, 3 tables, has been accepted by frontiers of computer science (FCS), doi=\{10.1007/s11704-024-40231-1\}},
	file = {Wang et al. - 2024 - A Survey on Large Language Model based Autonomous .pdf:/Users/esben/Zotero/storage/AFQAWTE3/Wang et al. - 2024 - A Survey on Large Language Model based Autonomous .pdf:application/pdf},
}

@misc{qin_toolllm_2023,
	title = {{ToolLLM}: {Facilitating} {Large} {Language} {Models} to {Master} 16000+ {Real}-world {APIs}},
	shorttitle = {{ToolLLM}},
	url = {http://arxiv.org/abs/2307.16789},
	doi = {10.48550/arXiv.2307.16789},
	abstract = {Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.},
	urldate = {2024-07-27},
	publisher = {arXiv},
	author = {Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and Zhao, Sihan and Hong, Lauren and Tian, Runchu and Xie, Ruobing and Zhou, Jie and Gerstein, Mark and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
	month = oct,
	year = {2023},
	note = {arXiv:2307.16789 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{park2023generative,
      title={Generative Agents: Interactive Simulacra of Human Behavior}, 
      author={Joon Sung Park and Joseph C. O'Brien and Carrie J. Cai and Meredith Ringel Morris and Percy Liang and Michael S. Bernstein},
      year={2023},
      eprint={2304.03442},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@misc{talebirad2023multiagent,
      title={Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents}, 
      author={Yashar Talebirad and Amirhossein Nadiri},
      year={2023},
      eprint={2306.03314},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{zou2023universal,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and Nicholas Carlini and Milad Nasr and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      eprint={2307.15043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{guo2024large,
      title={Large Language Model based Multi-Agents: A Survey of Progress and Challenges}, 
      author={Taicheng Guo and Xiuying Chen and Yaqi Wang and Ruidi Chang and Shichao Pei and Nitesh V. Chawla and Olaf Wiest and Xiangliang Zhang},
      year={2024},
      eprint={2402.01680},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{gu2024agent,
      title={Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast}, 
      author={Xiangming Gu and Xiaosen Zheng and Tianyu Pang and Chao Du and Qian Liu and Ye Wang and Jing Jiang and Min Lin},
      year={2024},
      eprint={2402.08567},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{pasquini2024neural,
      title={Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks}, 
      author={Dario Pasquini and Martin Strohmeier and Carmela Troncoso},
      year={2024},
      eprint={2403.03792},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}
@misc{cohen2024comes,
      title={Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications}, 
      author={Stav Cohen and Ron Bitton and Ben Nassi},
      year={2024},
      eprint={2403.02817},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}
@article{zheng2024cristallinity,
    author = {Zheng, Zhiling and Zhang, Oufan and Nguyen, Ha L. and Rampal, Nakul and Alawadhi, Ali H. and Rong, Zichao and Head-Gordon, Teresa and Borgs, Christian and Chayes, Jennifer T. and Yaghi, Omar M.},
    title = {ChatGPT Research Group for Optimizing the Crystallinity of MOFs and COFs},
    journal = {ACS Central Science},
    volume = {9},
    number = {11},
    pages = {2161-2170},
    year = {2023},
    doi = {10.1021/acscentsci.3c01087},
    URL = {https://doi.org/10.1021/acscentsci.3c01087},
    eprint = {https://doi.org/10.1021/acscentsci.3c01087}
}
@misc{hong2023metagpt,
      title={MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework}, 
      author={Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and Jürgen Schmidhuber},
      year={2023},
      eprint={2308.00352},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
# Jailbreaking
@misc{li2023privacy,
  title        = {Privacy in Large Language Models: Attacks, Defenses and Future Directions},
  author       = {Haoran Li and Yulin Chen and Jinglong Luo and Yan Kang and Xiaojin Zhang and Qi Hu and Chunkit Chan and Yangqiu Song},
  year         = {2023},
  eprint       = {2310.10383},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL}
}

@inproceedings{wei2023jailbroken,
 author = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {80079--80110},
 publisher = {Curran Associates, Inc.},
 title = {Jailbroken: How Does LLM Safety Training Fail?},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/fd6613131889a4b656206c50a8bd7790-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{liu2024prompt,
      title={Prompt Injection attack against LLM-integrated Applications}, 
      author={Yi Liu and Gelei Deng and Yuekang Li and Kailong Wang and Zihao Wang and Xiaofeng Wang and Tianwei Zhang and Yepang Liu and Haoyu Wang and Yan Zheng and Yang Liu},
      year={2024},
      eprint={2306.05499},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}
# LLM safeguard
@misc{inan2023llamaguard,
      title={Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations}, 
      author={Hakan Inan and Kartikeya Upasani and Jianfeng Chi and Rashi Rungta and Krithika Iyer and Yuning Mao and Michael Tontchev and Qing Hu and Brian Fuller and Davide Testuggine and Madian Khabsa},
      year={2023},
      eprint={2312.06674},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
# LLM based application / LLM based agent or system
@misc{gptresearcher2023,
      author = {Elovic, Assaf},
      month = {07},
      year = {2023},
      title = {gpt-researcher},
      howpublished = {\url{https://github.com/assafelovic/gpt-researcher}},
      version = {0.5.4},
      note = {Accessed: 2024-06-15}
}

@misc{gptengineer2023,
      author = {Osika, Anton},
      month = {04},
      year = {2023},
      title = {gpt-engineer},
      howpublished = {\url{https://github.com/gpt-engineer-org/gpt-engineer}},
      version = {0.1.0},
      note = {Accessed: 2024-06-15}

}
@misc{chatgptplugins2023,
      author={OpenAI},
      day={23},
      month={03},
      year={2023},
      title = {ChatGPT plugins},
      howpublished = {\url{https://openai.com/index/chatgpt-plugins/}},
      note = {Accessed: 2024-06-15}

}
@misc{gptstore2024,
      author={OpenAI},
      day={10},
      month={01},
      year={2024},
      title = {Introducing the GPT Store},
      howpublished = {\url{https://openai.com/index/chatgpt-plugins/}},
      note = {Accessed: 2024-06-15}
}
@misc{boiko2023emergent,
      title={Emergent autonomous scientific research capabilities of large language models}, 
      author={Daniil A. Boiko and Robert MacKnight and Gabe Gomes},
      year={2023},
      eprint={2304.05332},
      archivePrefix={arXiv},
      primaryClass={physics.chem-ph}
}
@misc{liang2023taskmatrixai,
      title={TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs}, 
      author={Yaobo Liang and Chenfei Wu and Ting Song and Wenshan Wu and Yan Xia and Yu Liu and Yang Ou and Shuai Lu and Lei Ji and Shaoguang Mao and Yun Wang and Linjun Shou and Ming Gong and Nan Duan},
      year={2023},
      eprint={2303.16434},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{shen2023hugginggpt,
      title={HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face}, 
      author={Yongliang Shen and Kaitao Song and Xu Tan and Dongsheng Li and Weiming Lu and Yueting Zhuang},
      year={2023},
      eprint={2303.17580},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{yao2023tree,
      title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models}, 
      author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
      year={2023},
      eprint={2305.10601},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{li2023apibank,
      title={API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs}, 
      author={Minghao Li and Yingxiu Zhao and Bowen Yu and Feifan Song and Hangyu Li and Haiyang Yu and Zhoujun Li and Fei Huang and Yongbin Li},
      year={2023},
      eprint={2304.08244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
# enhancing LLM answer
@misc{liu2021pretrain-prompt-predict,
      title={Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing}, 
      author={Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
      year={2021},
      eprint={2107.13586},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{sclar2023quantifying,
      title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting}, 
      author={Melanie Sclar and Yejin Choi and Yulia Tsvetkov and Alane Suhr},
      year={2023},
      eprint={2310.11324},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{alzahrani2024benchmarks,
      title={When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards}, 
      author={Norah Alzahrani and Hisham Abdullah Alyahya and Yazeed Alnumay and Sultan Alrashed and Shaykhah Alsubaie and Yusef Almushaykeh and Faisal Mirza and Nouf Alotaibi and Nora Altwairesh and Areeb Alowisheq and M Saiful Bari and Haidar Khan},
      year={2024},
      eprint={2402.01781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{mizrahi2024state,
      title={State of What Art? A Call for Multi-Prompt LLM Evaluation}, 
      author={Moran Mizrahi and Guy Kaplan and Dan Malkin and Rotem Dror and Dafna Shahaf and Gabriel Stanovsky},
      year={2024},
      eprint={2401.00595},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{li2022rag-survey,
      title={A Survey on Retrieval-Augmented Text Generation}, 
      author={Huayang Li and Yixuan Su and Deng Cai and Yan Wang and Lemao Liu},
      year={2022},
      eprint={2202.01110},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{hao2023reasoning-planing,
      title={Reasoning with Language Model is Planning with World Model}, 
      author={Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and Daisy Zhe Wang and Zhiting Hu},
      year={2023},
      eprint={2305.14992},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{zhao2023expel,
      title={ExpeL: LLM Agents Are Experiential Learners}, 
      author={Andrew Zhao and Daniel Huang and Quentin Xu and Matthieu Lin and Yong-Jin Liu and Gao Huang},
      year={2023},
      eprint={2308.10144},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
# multi agents
@misc{nascimento2023selfadaptive,
      title={Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems}, 
      author={Nathalia Nascimento and Paulo Alencar and Donald Cowan},
      year={2023},
      eprint={2307.06187},
      archivePrefix={arXiv},
      primaryClass={cs.MA}
}
@misc{zhang2024building,
      title={Building Cooperative Embodied Agents Modularly with Large Language Models}, 
      author={Hongxin Zhang and Weihua Du and Jiaming Shan and Qinhong Zhou and Yilun Du and Joshua B. Tenenbaum and Tianmin Shu and Chuang Gan},
      year={2024},
      eprint={2307.02485},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{wang2024unleashing,
      title={Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration}, 
      author={Zhenhailong Wang and Shaoguang Mao and Wenshan Wu and Tao Ge and Furu Wei and Heng Ji},
      year={2024},
      eprint={2307.05300},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
# LLM models
@misc{openai2020gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{openai2024gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}
}
@misc{openai2024gpt4o,
      title={GPT-4o System Card}, 
      author={OpenAI},
      year={2024},
      eprint={2410.21276},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21276}, 
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={{Llama team and contributors}},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{touvron2023llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={{Llama 2 team and contributors}},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{jiang2023mistral,
      title={Mistral 7B}, 
      author={{Mistral team and contributors}},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{geminiteam2024gemini,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={{Gemini team and contributors}},
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{Significant_Gravitas_AutoGPT,
author = {Significant-Gravitas},
month={03},
year={2023},
license = {MIT},
title = {AutoGPT},
howpublished = {\url{https://github.com/Significant-Gravitas/AutoGPT}},
      note = {Accessed: 2024-06-15}

}

@misc{Chase_LangChain_2022,
author = {Chase, Harrison},
month = {10},
title = {{LangChain}},
url = {https://github.com/langchain-ai/langchain},
year = {2022}
}

@misc{eu_ai_act_2021,
  author    = {European Commission},
  title     = {Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union legislative acts},
  year      = {2021},
  note      = {COM(2021) 206 final},
  url       = {https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206}
}

@ARTICLE{stuxnet2011,
  author={Langner, Ralph},
  journal={IEEE Security \& Privacy}, 
  title={Stuxnet: Dissecting a Cyberwarfare Weapon}, 
  year={2011},
  volume={9},
  number={3},
  pages={49-51},
  keywords={Malware;Industrial engineering;Driver circuits;Aircraft propulsion;Military communication;Process control;Computer security;Stuxnet;cyberwarfare;digital code signing;SCADA;Ralph Langner},
  doi={10.1109/MSP.2011.67}}

@misc{anthropic_computer_use,
    author = {Anthropic},
    title= {Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku},
    year={2024},
    url={https://www.anthropic.com/news/3-5-models-and-computer-use}}