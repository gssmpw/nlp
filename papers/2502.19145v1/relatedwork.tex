\section{Related Work}
Modern LLM applications are less and less made of one single LLM but mostly encompassed within a system:

\textbf{Tool-LLM} \citep{qin_toolllm_2023}: With LLMs integrated into applications, models can expand their language capabilities to perform actions on behalf of the user through the use of API calls \citep{gptengineer2023,chatgptplugins2023,gptstore2024}, web research, and writing reports \citep{gptresearcher2023}, and directly interacting with computer \citep{anthropic_computer_use}.

\textbf{LLM Agents} \citep{wang_survey_2024}: To improve the ability of LLMs to act in the world, autonomous systems with an LLM core were created with the capacity to interact with tools and call APIs while maintaining memory and reacting to the environment \citep{boiko2023emergent,liang2023taskmatrixai,shen2023hugginggpt}. These LLM-based agent systems are designed to tackle complex problems or perform decision-making applied in a wide range of situations \citep{yao2023tree,liu2024prompt}. New methods are continuously introduced, such as prompt-based learning \citep{liu2021pretrain-prompt-predict}, retrieval augmented generation (RAG) \citep{li2022rag-survey}, planning \citep{hao2023reasoning-planing}, and self-improvement and memory for LLM Agents \citep{zhao2023expel}.

\textbf{LLM Multi-Agent Systems} \citep{park_generative_2023}: Multi-agent systems are composed of many LLM agents to solve complex objectives \citep{nascimento2023selfadaptive,zhang2024building,wang2024unleashing}. These systems were designed to extend the capabilities and the application domain of single LLM-based agent systems, such as software engineering and automating research \citep{zheng2024cristallinity}. With collective intelligence of a group of specialized agents, research might find better results than relying on a single agent \citep{hong2023metagpt}.

\subsection{Security of LLM Agents}
With the growth of LLM adoption, concerns about security and safety issues are emerging \citep{li2023privacy,wei2023jailbroken}.
Current models are vulnerable to adversarial attacks through malicious instructions, called "jailbreaks" \citep{wei2023jailbroken}. There exist different types of malicious instructions such as direct \citep{liu2024prompt} or indirect \citep{greshake2023youve} prompt injection. For direct prompt injection, the attacker sends a malicious prompt directly to the language model while indirect ones are injected into the content that LLM agents use to inform their responses (such as a website an agent retrieves containing a malicious prompt). LLM-agents are also vulnerable to backdoor injections within their core LLM that exclusively target a deployment as agents by exploiting their multi-step reasoning process \citep{yang2024watchagentsinvestigatingbackdoor}. Our work focuses on direct threats of jailbreaks.
In terms of defense, recent work has demonstrated that while attacks transfer from LLM to LLM-agents, safety measures do not \citep{andriushchenko2024agentharm, lermen2024applying, kumar2024refusal}.  

At the multi-agent level, replications of malicious instructions within multi-LLM systems had been demonstrated in abstract non-realistic environments \citep{gu2024agentsmithsingleimage} and in the context of a GenAI email assistant \citep{cohen2024comesaiwormunleashing}. Recent work has also shown how malicious prompts can create a "domino effect" in multi-agent systems, where compromising one agent can trigger cascading failures across the entire system \citep{tian2024evilgeniusesdelvingsafety}. We study defense strategies within such environments.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{images/fig2-4.pdf}
    \caption{
    Examples of infectious prompt proliferation or containment depending on defense mechanisms. \\ 
    Each horizontal line of the graphs corresponds to one LLM agent of the simulation. Arrows from one line to another correspond to messages sent to other agents. Arrows in bold highlight one message chain. Each simulation starts with a message containing normal instructions sent to Atlas, the lab manager (i.e. \textit{Initial lab instructions}). Then, once the agents start collaborating on the instructed task, the malicious instructions are sent to one of them (i.e. \textit{Initial malicious instructions}).
    \textbf{Upper:}
    Multi-hop spread of malicious instructions in our system without any defense mechanism.  Normal operations are disrupted by the introduction of the jailbreak and a wave of messages is sent by compromised agents (red and orange arrows). Finally, Deng performs an action that leads to an explosion in the lab (red cross).
    \textbf{Lower:}
    Limited multi-hop spread of malicious instructions in our system with active vaccines. Even if the first agent targeted by the malicious prompt, Edison, got compromised, 
    the message sent by this agent raises suspicion of other bots. Recipients do not follow the malicious instructions and proactively contact the other agents leading to an effective containment of the infectious prompt spread.
    }
    \label{fig:jb-spread}
\end{figure*}