\section{Problem Setup and Preliminaries}\label{sec:problemsetup}

\textbf{Notation:} We use $[n]$ to denote $\left\{i \in \mathbb{N} \;|\; i\leq n\right\}$. $\id \in \R^{d\times d}$ represents the $d$-dimensional identity matrix. We use $\normal(\mu, \msig)$ to denote the multivariate normal distribution with specified mean, $\mu$ and covariance matrix $\msig$. $\norm{.}_{2}$ denotes the $\ell_{2}$ euclidean norm for vectors and $\normop{.}$ denotes the operator norm for matrices. $\E\bbb{X}$ denotes the expectation of the random variable $X$ and $\mathsf{Cov}(X)$ denotes its covariance matrix. For $a, b \in \R$, we write $a \lesssim b \text{ if and only if there exists an absolute constant } C > 0 \text{ such that } a \leq C b.$ $\tilde{O}, \tilde{\Omega}$ represent order notations with logarithmic factors. We also define a coarser notion of subGaussianity used subsequently in our proof sketch, 
\begin{definition}[$\bb{\beta^2,K}$-subGaussianity]\label{definition:new_subGaussian_def}
A mean-zero random variable $Y$ is said to be $\bb{\beta^2,K}$-subGaussian if it satisfies:
\bas{
 \bP(|Y|> A)\leq e^K \exp(-\tfrac{A^2}{2\beta^2})   
}
\end{definition}

\textbf{Ornstein-Uhlenbeck process:} Consider a target distribution $\pi$ over $\bR^d$. Suppose $x_0 \sim \pi$ and $x_t$ solve the following Stochastic Differential Equation (SDE):
\begin{equation}\label{eq:fwd_noise}
    dx_t = -x_t dt + \sqrt{2}dB_t\,,
\end{equation}
where $B_t$ is the standard Brownian Motion over $\R^d$. An application of Ito's formula demonstrates that $x_t = x_0e^{-t} + z_t$ where $z_t \sim \cN(0,\sigma_{t}^{2}\vI)$ is independent of $x_0$ and $\sigma_{t} := \sqrt{1-e^{-2t}}$. This is the forward noising process, which progressively noises the initial sample into a standard Gaussian vector. Ito's formula also relates $x_{t}, x_{t'}$ for any timesteps $t > t' \geq 0$ to obtain, $x_t = x_{t'}e^{-(t-t')} + z_{t,t'}$ where $z_{t,t'} \sim \cN(0,\sigma_{t-t'}^{2}\vI)$ is independent of $x_{t'}$ and $\sigma_{t-t'} := \sqrt{1-e^{-2(t-t')}}$. For $t \in [0,T]$, let $p_t$ be the probability density function. Given $\bar{x}_0 \sim p_T$ and a standard $\R^d$ Brownian motion $\bar{B}$, then the denoising process is: 
\ba{d\bar{x}_t = \bar{x}_tdt+2\nabla \log p_{T-t}(\bar{x}_t)dt + \sqrt{2}d\bar{B}_t\,.\label{eq:reverse_sde}} It is the time reversal of the noising process which implies $\bar{x}_T \sim \pi$ \cite{song2020score}. 

% \ps{It is fine if everything is written like this, but shouldn't we use $X$ to denote RVs? No need to change anything, just asking.}

\textbf{Score Matching:} Given i.i.d. data points $x^{(1)},\dots,x^{(m)}$ from the target distribution $\pi$, diffusion models learn the score function $s(t,x) : \bR^{+}\times\bR^d \to \bR^d$ defined as $s(t,x) \equiv  s_{t}\bb{x} := \nabla \log p_t(x)$ via denoising score matching (DSM). Tweedie's formula states that \ba{s(t,x_t) = \E\bbb{\frac{-z_t}{\sigma_t^2}\bigg|x_t}\,.\label{eq:tweedies_formula}} 
Let $\cF$ be a finite class of functions which map $\bR^+\times\bR^d$ to $\bR^d$ with functions $(t, x) \rightarrow f\bb{t, x} \equiv f_{t}\bb{x}$. Let $\cT = \{t_1,\dots,t_N\}$ be a finite subset of $[0,T]$. Let $x_t^{(i)}$ denote the solution of Equatoin~\eqref{eq:fwd_noise} at time $t$ with $x_0^{(i)} = x^{(i)}$ and define $z_t^{(i)} := x_t^{(i)}-e^{-t}x^{(i)}$. We consider the joint DSM objective to be: \ba{\hat{\cL}(f) := \frac{1}{mN}\sum_{i=1}^{m}\sum_{t \in \cT}\norm{f(t,x^{(i)}_t)+\frac{z^{(i)}_t}{\sigma_t^2}}_{2}^2\,. \label{eq:dsm_total}}
Intuitively, optimizing \eqref{eq:dsm_total} represents a regression task with noisy labels. There are two primary sources of noise in this setup. The first comes from~\eqref{eq:tweedies_formula}, since the targets, $-z_t/\sigma_t^2$, conditioned on the data point, $x_{t}$, are only equal to the true score, $s\bb{t, x_{t}}$ in expectation. The second comes from the randomness in $x_{t} \sim p_{t}$ itself.
% \dn{clarify difference in practice, and in prior works}

The empirical risk minimizer is defined as $\hat{f} = \arg\inf_{f \in \cF}\hat{\cL}(f)$. The results established in \cite{benton2024nearly} states that the error in sampling arising from using the estimated score function $\hat{f}$ is given by:
\begin{equation}
\epsilon_{\mathsf{score}}^2(\hat{f}) := \sum_{i=2}^{N}\gamma_i\E_{x \sim p_{t_i}}\|\hat{f}(t_i,x) - s(t_i,x)\|^2, \text{ where } \gamma_i:= t_{i}-t_{i-1} \label{eq:sampling_requirement}
\end{equation}
Our goal is to bound this error. For simplicity, we consider $t_i = i\Delta$ for some step size $\Delta \in \bb{0,1}$. 