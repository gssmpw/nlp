\section{Introduction}
Score-based diffusion models \cite{sohl2015deep,ho2020denoising} are generative models 
that have transformed image and video generation \cite{rombach2022high,saharia2022photorealistic,ramesh2022hierarchical,podell2023sdxl}, enabling foundation models to produce photorealistic and stylized images from text prompts. Their adaptability extends diverse domains such as audio \cite{kong2021diffwave,evans2024fast}, text \cite{gulrajani2024likelihood,han2022ssd,lou2023discrete,varma2024glauber}, molecule \cite{hoogeboom2022equivariant,hua2024mudiff}, and layout generation \cite{inoue2023layoutdm,levi2023dlt}. Diffusion models differ from Markov Chain Monte Carlo (MCMC) algorithms by generating additional samples from a target distribution using a trained neural network that learns the score function at different noise levels. Unlike MCMC methods, which can be slow for multi-modal distributions, diffusion models efficiently sample from various distributions with minimal assumptions, provided the score functions are learned accurately.

Unlike Markov Chain Monte Carlo (MCMC) algorithms, which have access to the underlying density, diffusion models can only access $m$ i.i.d. samples from the target distribution. These models are trained by `score matching', where a neural network is parametrized to learn the score function of the noised target distribution at various noise levels. They can efficiently sample from various distributions with minimal assumptions, provided the score functions are learned accurately~\cite{chen2022sampling, benton2024nearly}. 
Given $m$ i.i.d. samples from the target distribution, the \textit{first step} obtains noised samples from a noising Markov process converging to the Gaussian distribution at various noise levels. The \textit{second step} estimates score functions of the distribution at each noise level using Denoising Score Matching (DSM) \cite{vincent2011connection}. This approach relies on learning from \textit{dependent data} from multiple trajectories of a Markov process in contrast to learning with i.i.d. data in traditional settings.

Prior works \cite{block2020generative, gupta2023sample} provide theoretical guarantees for score function approximation separately at each noise level using the same samples. 
However, in practice, a \emph{single function approximator} is commonly used at all noise levels, which is considered by \cite{han2024neural}. 
\cite{boffi2024shallow} show that despite the problem of distribution estimation suffering from the curse of dimensionality \cite{chen2023score,oko2023diffusion}, the existence of low-dimensional structures allows neural networks to learn the score functions. All of these existing bounds exhibit polynomial dependence on the dimension, $d$.

This paper establishes that under \textit{suitable smoothness conditions} for a given function class, score matching with a single function approximator jointly across all timesteps achieves a nearly \textbf{dimension-free sample complexity} that depends on the smoothness parameter and grows only as $\log\log(d)$.
We summarize our primary contributions below:
 \subsection{Our Contributions}
\begin{enumerate}
    \item We analyze the sample complexity of \textit{joint score matching} across noise levels using a single function approximator, achieving a \textbf{double-exponential reduction in dimension dependence}.
    \item We present a \textbf{novel martingale decomposition} of the error, which allows us to bound the error despite being composed of samples from multiple trajectories of \textit{dependent data}.
    \item We use second-order Tweedie-type formulae to obtain a \textbf{sharp bound on the error variance}, crucial for establishing almost \textbf{dimension-free} convergence rates.
    \item Inspired by the above results, we present the \textbf{Bootstrapped Score Matching} algorithm where learning the score at a given noise level is bootstrapped to the learned score function at a lower noise level to achieve variance reduction. This shows improved performance compared to DSM in simple empirical studies.
    \end{enumerate}

\subsection{Related Works}\label{subsec:related_works}
\paragraph{Score Matching and Diffusion Models:} Score Matching was introduced in the context of statistical estimation in \cite{hyvarinen2005estimation} with an algorithm now called Implicit Score Matching (ISM). Diffusion models are trained using Denoising Score Matching (DSM) introduced in \cite{vincent2011connection}, and is based on Tweedie's formula. Several algorithms have been introduced since, such as Sliced Score Matching \cite{song2020sliced} and Target Score Matching \cite{de2024target}.

The complexity of Denoising Score Matching has been analyzed in various settings \cite{chen2023score,oko2023diffusion,gupta2023sample,block2020generative} in prior works. We consider the setting in \cite{gupta2023sample,block2020generative}, where the score functions can be accurately approximated by a function approximator class (such as neural networks). These bounds can then be used with the discretization analyses such as those presented in \cite{benton2024nearly,chen2022sampling,lee2023convergence} to theoretically analyze the quality of samples generated by the model. 

\paragraph{Learning from dependent data:} Learning with data from a markov trajectory has been explored in literature in the context of system identification, time series forecasting and reinforcement learning \cite{duchi2012ergodic, simchowitz2018learning,nagaraj2020least,kowshik2021near,tu2024learning,ziemann2022learning,bhandari2018finite,kumar2024streaming,srikant2024rates}
% \ps{Add Duchi/aggarwal here?} \syamantak{Done.}
Many of these works analyze the rates of convergence with data derived from a mixing Markov chain, when the number of data points available is much higher than the mixing time, $\tau_{\mathsf{mix}}$. In our context, the Markov chain contains $\tilde{O}(\tau_{\mathsf{mix}})$ data points created by progressively noising samples from the target distributions, where $\tilde{O}$ hides logarithmic factors. This is similar to the setting in \cite{tu2024learning}, which considered linear regression and linear system identification.

We outline our paper as follows: Section~\ref{sec:problemsetup} introduces the problem setup and preliminaries, followed by the main results and a comparison with prior work in Section~\ref{sec:main_results}. Section~\ref{sec:technical_results} presents key technical results from our proof technique. Finally, Section~\ref{sec:bootstrapped_score_matching} introduces Bootstrapped Score Matching, a novel training method that shares information explicitly across time by modifying the learning objective.