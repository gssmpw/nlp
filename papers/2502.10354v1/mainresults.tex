\section{Main Results}\label{sec:main_results}

We operate under the following smoothness assumption on the function class, $\mathcal{F}$. 

\begin{assumption}[Smoothness of function class]\label{assumption:score_function_smoothness}
Let the true score function, $s \in \cF$. 
\begin{enumerate}
    \item[0.]$\nabla \log p_t(\cdot)$ is continuously differentiable for every $t \in \bR^+$.
    \item[1.] Lipschitzness : For all $t \in \timeset, x_{1}, x_{2} \in \R^{d}$, $f \in \cF$ $$\norm{f(t, x_{1}) - f(t, x_{2})}_{2} \leq L\norm{x_{1}-x_{2}}$$
    \item[2.] Local Time Regularity : There exists a set $B_{\delta,t}$ such that $p_t(B_{\delta,t}) \geq 1 -\delta$, $\forall t \geq t' \in \timeset, x \in B_{\delta,t}$, $\forall f \in \cF$
    \bas{&\norm{e^{-(t-t')}f(t, x)-f(t', e^{(t-t')}x)}_{2} \leq e^{t-t'}L\sqrt{8(t-t')\log(\tfrac{2}{\delta})}}
\end{enumerate}
\end{assumption}

The first is a standard Lipschitz continuity assumption followed in the literature (see e.g. \cite{block2020generative}). The second assumes H\"older continuity with respect to the time variable. This is a natural assumption because Lemma~\ref{eq:time_smoothness_of_score_function} shows that Assumption~\ref{assumption:score_function_smoothness}-1 implies Assumption~\ref{assumption:score_function_smoothness}-2 for the true score function, $s(t, x)$. 

%\dn{TODO: remove $\alpha_{t_j}$ altogether}\syamantak{Done. Changed to $\gamma_{j}$. Have to fix $\gamma_t$. Done.}

Equation~\eqref{eq:fwd_noise} demonstrates that $x_t$ forms a Markov chain, leading to the noise random variables, $z_t$, being strongly dependent. Additionally, \eqref{eq:fwd_noise} is typically iterated for $T = \tilde{O}\big(\tau_{\mathsf{mix}}\big)$ timesteps, until $p_T$ is close to a gaussian distribution. This setup falls outside the scope of conventional analyses of learning from dependent data, which are prominent in the literature (see Section~\ref{subsec:related_works}). Such analyses usually assume a significantly larger number of datapoints, where datapoints separated by $\tau_{\mathsf{mix}}$ in time are approximately independent, and the convergence rates align with their i.i.d. counterparts, adjusted for an effective sample size reduced by a factor of $\tau_{\mathsf{mix}}$. In contrast, our setting involves substantially fewer datapoints. To address this challenge, we propose a novel martingale decomposition (stated in Lemma~\ref{lemma:error_martingale_decomposition_2} and proved in Lemma~\ref{lemma:error_martingale_decomposition_2_actual_appendix}) of the error and establish sharp concentration bounds to account for these dependencies.

Recall the DSM objective in \eqref{eq:dsm_total}. As explained before, there are two sources of noise: (1) due to $-z_{t}/\sigma_{t}^{2}$ conditioned on $x_{t}$, (2) due to $x_{t}\sim p_{t}$.  We demonstrate the effect of fluctuations in $z_{t}|x_t$ in Theorem~\ref{theorem:empirical_l2_error_bound} and then deal with the random fluctuations due to $x_t$ in Theorem~\ref{theorem:expected_l2_error_bound}. 

% \syamantak{Done.}\dn{What noise did we remove here? explain this and explain why our variance bound is novel and why it is hard to derive} \dn{Also explain in detail why martingale decomposition is needed.}
% \dn{We need to describe the following insights early on, before stating the results. For each of these points, refer to the exact place in the main paper where it is stated and the exact place in the appendix where it is proved. 
% \begin{enumerate}
%     \item There are two sources of randomness and error: $z_t|x_t$ and $x_t$.
%     \item First we remove the randomness in $z_t|x_t$ and then deal with the randomness in $x_t$. 
%     \item $x_t$ is derived from a markov chain and hence $z_t$ are heavily dependent random variables. We do not have multiple mixing times of data. We need sharp concentration despite the dependencies. That is why martingale decomposition.
%     \item We can show that the martingale increments are conditionally sub-gaussian with possibly random parameters. But this has dimension dependency in variance proxy.
%     \item We compute accurate variances, which are dimension free. This requires heavy lifting to do without assuming Hessian smoothness.
%     \item Combine dimension free variance with dimension dependent sub-gaussianity with Freedman like super martingale argument. 
% \end{enumerate}
% }

% \dn{TODO: larger step sizes also work, just need a markov inequality based argument} \syamantak{Done. Please check Theorem~\ref{thm:subsampled_error_bound}}

Our first result in Theorem~\ref{theorem:empirical_l2_error_bound} provides a dimension-free bound on the empirical squared error, wherein we show how to control the noise due to $z_{t}$, conditioned on the data, $x_{t}$. 

\begin{restatable}[Empirical Squared Error Bound]{theorem}{empiricalsquarederror}\label{theorem:empirical_l2_error_bound}Let Assumption~\ref{assumption:score_function_smoothness} hold. Fix $\delta \in \bb{0,1}$. For all $j \in [N]$, let $t_{j} := \Delta j$ and $\gamma_{j} := \Delta$. Let $B := C\log\bb{\bb{L+1}dmN\log\bb{\frac{1}{\delta}}/\Delta}$ for an absolute constant $C > 0$, and let $\Delta\log^3(\tfrac{1}{\Delta})d^{3}\log^3(2d)\log^3\bb{\frac{2Nm}{\delta}}\log^3\bb{\frac{B|\cF|}{\delta}} \leq 1$ and $N\Delta \leq C\log(\frac{1}{\Delta})$. Then for
\bas{
    % m \gtrsim \tfrac{\bb{L+1}^{2}}{\epsilon^{2}}\log\bb{\tfrac{B|\cF|}{\delta}}
    m \gtrsim \frac{\bb{L+1}^{2}}{\epsilon^{2}}\log\bb{\frac{B|\cF|}{\delta}}N\Delta
}
with probability at least $1-\delta$, 
\bas{
\sum_{i\in[m], j\in[N]}\frac{\gamma_{j}\norm{\hat{f}\bb{t_j,x_{t_j}^{(i)}}-s\bb{t_jx_{t_j}^{(i)}}}_{2}^{2}}{m} \lesssim \epsilon^{2}
}
\end{restatable}

\begin{remark}
    The sample complexity in Theorem~\ref{theorem:empirical_l2_error_bound} depends on the smoothness parameter $L$ and on $\log(B)$. Observe that $B$ depends logarithmically on $d$, thus leading to a nearly dimension-free result, i.e. $\log\log d$ dependence. This is in stark contrast to existing results, which have poly($d$) dependence. We believe that the objective function in ~\eqref{eq:dsm_total} harnesses the smoothness of the function class by \textit{jointly optimizing over multiple time steps}.
\end{remark}
Theorem~\ref{theorem:empirical_l2_error_bound} is the first step in proving the expectation bound in Theorem~\ref{theorem:expected_l2_error_bound} and may be of independent interest. Theorem~\ref{theorem:expected_l2_error_bound} deals with the noise arising from the data $x_{t} \sim p_{t}$. Our next assumption, called `hypercontractivity', controls the $4^{\text{th}}$-moment of the error bound with respect to the $2^{\text{nd}}$-moment, which can be used to prove the generalization of the score function in $L^2$ error. This is a mild assumption, standard in statistics and learning theory under heavy tails \cite{mendelson2020robust,klivans2018efficient,minsker2018sub}.
\begin{assumption}\label{assumption:hypercontractivity}
    For every $f \in \cF$ and $x_t \sim p_t$, we have:
    $$\E[\|f(t,x_t) - s(t,x_t)\|^4]^{\frac{1}{4}} \leq \chyp \E[\|f(t,x_t)-s(t,x_t)\|^2]^{\frac{1}{2}}$$
\end{assumption}

$\kappa^4$ can be bounded (up to multiplicative constants) by the kurtosis of $f(t,x_t) - s(t,x_t)$. Assumption~\ref{assumption:hypercontractivity} follows from the smoothness and strong convexity of neural networks in parameter space (not \(x_t\)). Recent work \cite{milne2019piecewise, yi2022characterization} shows that near the global minimizer of the population loss, many smooth non-convex losses exhibit local strong convexity. We formalize this connection in Lemma~\ref{lem:sc-hyp}.

% \ps{Are there connections to PL?}\syamantak{Sorry what is PL here?}

% \syamantak{Change h to something else.}\syamantak{Done.}
\begin{lemma}\label{lem:sc-hyp}
    Let all $f\bb{t, x} \in \cF$, be parameterized as $g\bb{t, x;\theta}$ for $\theta \in \Theta \subseteq \bR^D$ and $\theta_{*}$ be such that $h\bb{t, x_{t}; \theta_{*}} = s\bb{t, x_{t}}$. Suppose $\exists \lambda,\mu \geq 0$ such that $\forall \theta \in \Theta$, 
\bas{
\E\bbb{\norm{g\bb{t, x_{t}; \theta} - g\bb{t, x_{t}, \theta_{*}}}_{2}^{4}} &\leq \lambda^{2} \norm{\theta-\theta_{*}}^4, \text{ and } \\
\E\bbb{\norm{g\bb{t, x_{t}; \theta} - g\bb{t, x_{t}, \theta_{*}}}_{2}^{2}} & \geq \mu\norm{\theta-\theta_{*}}^{2}
}
Then, all $f \in \cF$ satisfy Assumption~\ref{assumption:hypercontractivity} with $\chyp = \frac{\lambda}{\mu}$.
\end{lemma}

% \begin{assumption}[Local Strong Convexity of loss]\label{assumption:local_strong_conv}Let all $f\bb{t, x} \in \cF$, be parameterized as $h\bb{t, x;\theta}$ for $\theta \in \Theta$ and $\theta_{*}$ be such that $h\bb{t, x_{t}; \theta_{*}} = s\bb{t, x_{t}}$. Then $\exists \mu \geq 0$ such that $\forall \theta \in \Theta$, 
% \bas{
% \E\bbb{\norm{h\bb{t, x_{t}; \theta} - h\bb{t, x_{t}, \theta_{*}}}_{2}^{2}} \geq \mu\norm{\theta-\theta_{*}}^{2}
% }
% \end{assumption}

Under Assumptions~\ref{assumption:score_function_smoothness} and \ref{assumption:hypercontractivity}, we state our main result in Theorem~\ref{theorem:expected_l2_error_bound}. In this result, we use Theorem~\ref{theorem:empirical_l2_error_bound} and handle the noise due to $x_{t} \sim p_{t}$ in the DSM objective. 
% \dn{Make changes here based on changes to empirical bound theorem statemetn}\syamantak{Done.}

\begin{restatable}[Expected Squared Error Bound]{theorem}{expectedsquarederror}\label{theorem:expected_l2_error_bound}Let Assumptions~\ref{assumption:score_function_smoothness} and \ref{assumption:hypercontractivity} hold. Fix $\delta \in \bb{0,1}$. For all $j \in [N]$, let $t_{j} := \Delta j$ and $\gamma_{j} := \Delta$. Let $B := C\log\bb{\bb{L+1}dmN\log\bb{\frac{1}{\delta}}/\Delta}$ for an absolute constant $C > 0$, and let $\Delta\log^3(\tfrac{1}{\Delta})d^{3}\log^3(2d)\log^3\bb{\frac{2Nm}{\delta}}\log^3\bb{\frac{B|\cF|}{\delta}} \leq 1$ and $N\Delta \leq C\log(\frac{1}{\Delta})$. If
\bas{
    m \gtrsim \chyp^{2}\max\left\{\log\bb{\frac{N}{\delta}}, \frac{\bb{L+1}^{2}N\Delta}{\epsilon^{2}}\log\bb{\frac{B|\cF|}{\delta}} \right\}
}
then with probability at least $1-\delta$, 
\bas{
\sum_{j \in [N]}\gamma_{j}\E_{x_{t_j}}\bbb{\norm{\hat{f}\bb{t_j, x_{t_j}}-s\bb{t_j, x_{t_j}}}_{2}^{2}} \lesssim \epsilon^{2}
}
\end{restatable}

\begin{remark}
     In addition to the sample complexity of Theorem~\ref{theorem:empirical_l2_error_bound}, the sample complexity for the generalization bound in Theorem~\ref{theorem:expected_l2_error_bound} additionally has a factor of $\kappa^{2}$ due to the local strong convexity assumption formalized in Lemma~\ref{lem:sc-hyp}. 
\end{remark}

We note that Theorem~\ref{theorem:expected_l2_error_bound} pertains to training of diffusion models and requires a very fine value of the step size, $\Delta$. This is not an issue in practice since SGD type stochastic approximation is deployed to perform empirical risk minimization. However, once the model has been trained, we can accelerate inference by using a larger timestep-size to discretize the diffusion process, as shown in Theorem~\ref{thm:subsampled_error_bound} and proved in Theorem~\ref{thm:subsampled_error_bound_appendix}.

\begin{theorem}[Fast Inference]\label{thm:subsampled_error_bound} Under the same assumptions as Theorem~\ref{theorem:expected_l2_error_bound}, partition the timesteps $\{t_j = \Delta j\}_{j \in [N]}$ into $k$ disjoint subsets $S_1, S_2, \dots, S_k$, where each subset $S_i$ contains timesteps of the form $t_j = \Delta(i + nk)$ for $n \in \mathbb{N}$. Define $\gamma_j' := k\Delta$ for all $j$ in any subset $S_i$. Then, there exists at least one subset $S_i$ such that:
\[
\sum_{j \in S_i} \gamma_j' \E_{x_{t_j}}\left[\norm{\hat{f}(t_j, x_{t_j}) - s(t_j, x_{t_j})}_2^2\right] \lesssim \epsilon^2,
\]
with probability at least $1 - \delta$.
\end{theorem}
The subsets $S_i$ allow for a much coarser discretization with differences being $k\Delta$ instead of $\Delta$. While the error due to discretization of the SDE might become worse, as shown by the bounds in \cite{benton2024nearly}, Theorem~\ref{thm:subsampled_error_bound} demonstrates that the score estimation error does not degrade.

\textbf{Comparison with prior work:}
\cite{block2020generative} and \cite{gupta2023sample} analyze each discretization timestep independently, and perform a union bound across timesteps to achieve a bound on the DSM objective in \eqref{eq:dsm_total}. \cite{block2020generative} assume a target distribution with bounded support over a euclidean $\ell_{2}$ ball of radius, $R$. They further assume the score function to be $L$-Lipschitz and employ Rademacher complexity-based generalization bounds with a sufficiently rich function class, $\mathcal{F}$, to show (Proposition 12) with high probability, (up to logarithmic factors) 
\ba{\E_{x_{t}}\bbb{\norm{\hat{f}\bb{t,x_{t}}-s\bb{t,x_{t}}}_{2}^{2}} \lesssim \frac{L^{2}R^{2}}{\sigma_{t}^{4}}\bb{\mathcal{R}^{2}\bb{\cF} + \frac{d}{n}}\label{eq:block_dsm_bound}}
where $\mathcal{R}\bb{\cF}$ denotes the Rademacher complexity (see e.g. \cite{bartlett2002rademacher}) of $\cF$. They show this bound for all $t \in \timeset$ and perform a union bound to obtain the final sample complexity, instead of analyzing \eqref{eq:dsm_total} jointly. 
% It can be shown (see \cite{benton2024nearly}) that to satisfy \eqref{eq:sampling_requirement} for a given $\epsilon > 0$ with an approximate score function, $\hat{f}$, it suffices to ensure at each timestep $t$, 
% \ba{
% \E_{x_{t}}\bbb{\norm{\hat{f}\bb{t,x_{t}}-s\bb{t,x_{t}}}_{2}^{2}} \lesssim \frac{\epsilon^{2}}{\sigma_{t}^{2}} \label{eq:per_timestep_score_requirement}
% }
Using the bound in \eqref{eq:block_dsm_bound}, for a uniform step size $\Delta$, this leads to a sample complexity scaling as $\frac{1}{\text{poly}(\Delta)}$ to satisfy the requirement in \eqref{eq:sampling_requirement}. Furthermore, their sample complexity also depends, at least linearly, on the dimension, $d$, and hence is not dimension-free.

\cite{gupta2023sample} improve the dependence of the sample complexity on Wasserstein error, compared to \cite{block2020generative}. They assume a second moment bound on the target distribution without any smoothness assumptions on the score function and propose a relaxation of $\ell_{2}$ error as, 
\bas{
 D^{\delta}\bb{f_t, s_t} \leq \epsilon \Leftrightarrow \mathbb{P}_{x_t}\bb{\norm{f\bb{t,x_{t}}-s\bb{t, x_{t}}}_{2} \geq \epsilon} \leq \delta
} 
They show that learning $f \in \cF$ satisfying the above criteria for all timesteps $t \in \timeset$ suffices for sampling and further show (Lemma A.2) a sample complexity bound of $m \gtrsim d\log\bb{\frac{|\cF|}{\delta}}/\epsilon^{2}$ to achieve $D^{\delta}\bb{f_t, s_t} \leq \epsilon$ for any fixed $t$ and  perform a union bound across all timesteps to achieve a uniform bound required for sampling. In comparison with \cite{block2020generative}, their sample complexity does not scale with $\frac{1}{\sigma_t}$, but still involves a linear dependence on $d$.

Recently, \cite{han2024neural} provided an analysis of gradient descent to optimize \eqref{eq:dsm_total} via neural networks. They assume that the target distribution has bounded support over a $\ell_{2}$-euclidean ball of radius $R$ and the score function is $L$-Lipschitz. They model the evolution of neural
networks during training by a series of kernel regression tasks and jointly model all timesteps by assuming time as an input to the neural network. In this sense, their work is closest in spirit to our paper. However, their sample complexity bounds (Theorem 3.12) show a polynomial dependence on the dimension, $d$.