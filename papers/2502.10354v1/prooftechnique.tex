\section{Technical Results}
\label{sec:technical_results}

In this section we provide insights into our proof techniques and key technical results.



\rd 

% Our martingale difference sequence is of the form $Q_{i} := \inner{G_{i}}{Y_{i}-\E\bbb{Y_{i}|\mathcal{F}_{i-1}}}$ adapted to the filtration $\left\{\mathcal{F}_{i}\right\}_{i \in [n]}$, where $G_i$ is a $\mathcal{F}_{i-1}$ measurable random variable. We first show that conditioned on $\mathcal{F}_{i-1}$, $Q_{i}$ is subGaussian. However, as shown in Lemma~\ref{lemma:subGaussianity_parameters}, the subGaussianity parameters depend on the data dimension, $d$ along with $G_i$ and the step size, $\Delta$. Therefore, performing a concentration argument solely relying on this observation leads to a dimension-dependent bound. 

% To further refine our analysis and show a dimension-free bound, we evaluate the variance of $Q_{i}$ conditioned on $\mathcal{F}_{i-1}$. As shown in Lemma~\ref{lemma:martingale_variance_bound} (and proved in Lemma~\ref{lemma:martingale_diff_variance_bound_appendix}), the variance depends only on the smoothness parameter, $L$, along with $G_i$ and $\Delta$. The proof strategy relies on Assumption~\ref{assumption:score_function_smoothness}, and an application of the mean value theorem involving the hessian, $h_{t} := \nabla^{2}\log\bb{p_{t}}$, which yields terms of the form 
% \bas{
%     \E\bbb{h_{t'}(y_{t'})(x_{t'}-\tilde{x}_{t'})(x_{t'}-\tilde{x}_{t'})^{\top}h_{t'}(y_{t'})^{\top}|x_t}, t' < t
% }
% for $x_{t'}, \tilde{x}_{t'}$ i.i.d conditioned on $x_{t}$ and $y_{t'} = \lambda x_{t'} + (1-\lambda)\tilde{x}_{t'}$, $\lambda \in (0,1)$. When \(t\) and \(t'\) are close, one would hope to exploit the smoothness of the Hessian \(h_t\) with respect to time. Specifically, if \(h_t\) were smooth in the time parameter, this would allow the expectation to move inside, enabling the use of Tweedie’s second-order formula (Lemma~\ref{lemma:second_order_tweedie_application}) to derive variance bounds that are dimension-free. However, in the absence of such a smoothness assumption on the hessian, we decompose the Hessian \(h_{t'}\bb{y_{t'}}\) into two components:
% \[
% h_{t'}\bb{y_{t'}} = h_{t', \epsilon}\bb{y_{t'}} + \bb{h_{t'}\bb{y_{t'}} - h_{t', \epsilon}\bb{y_{t'}}}.
% \]
% Here, the first term, \(h_{t', \epsilon}\bb{y_{t'}}\), represents a ``smoothed" or ``mollified" hessian, averaged over an appropriately chosen distribution, which we show satisfies Lipschitz continuity. The second term, which represents the deviation between the original and mollified Hessians, requires a finer analysis, breaking the interval $[t', t]$ into many subintervals and draws upon Lusin's theorem(Lemma~\ref{lemma:lusin_theorem}), as developed further in Lemma~\ref{lemma:lusin_theorem_decomp}.

% Finally, we combine our dimension-free variance bound with the dimension-dependent subGaussianity via a freedman-like supermartingale argument as shown in Lemma~\ref{lemma:martingale_concentration_lemma}, which may be of independent interest to the reader. This further involves a careful handling since typical analysis of freedman's inequality assumes a uniform bound on the random variables in contrast to our more general conditional subGaussianity assumption.

\bk

 We start by bounding the empirical squared error in terms of a linear error term, as shown in Lemma~\ref{lemma:l2errorboundmartingale}. This relies on comparing the empirical error, $\hat{\mathcal{L}}$ of the minimizer $\hat{f}$ with the true score function, $s$. While here we assume for simplicity that $s \in \cF$, it can be relaxed to assume that $\exists s_{*}\in\cF$ with sufficiently small $\ell_{2}$ error, similar to \cite{gupta2023sample}.

\begin{restatable}{lemma}{squarederrorboundmartingale}\label{lemma:l2errorboundmartingale}
For $f \in \cF$, let $ y_t^{(i)} := \frac{-z_{t}^{(i)}}{\sigma_{t}^{2}}$ and
\begin{equation}
    \mathcal{L}(f) := \sum_{i \in [m], j \in [N]} \frac{\gamma_{j} \norm{f\bigl(t_j, x_{t_j}^{(i)}\bigr) - s\bigl(t_j, x_{t_j}^{(i)}\bigr)}_{2}^{2}}{m}, \notag
\end{equation}
\begin{equation}
    \begin{aligned}
        H^{f} := &\sum_{i \in [m], j \in [N]} \frac{\gamma_{j}}{m} \bigl\langle f\bigl(t_{j}, x_{t_{j}}^{(i)}\bigr) - s\bigl(t_{j}, x_{t_{j}}^{(i)}\bigr), y_{t_{j}}^{(i)} - s\bigl(t_{j}, x_{t_{j}}^{(i)}\bigr) \bigr\rangle. \notag
    \end{aligned}
\end{equation}
If $s \in \cF$ then for $\hat{f} = \arg\inf_{f \in \cF} \hat{\cL}(f)$, we have  
\small
\begin{equation}
    \mathcal{L}(\hat{f}) \leq  H^{\hat{f}}, \label{eq:Lf_bound}
\end{equation}
\normalsize
where $\hat{\cL}$ is defined in \eqref{eq:dsm_total}.
\end{restatable}

We define $\hat{f}$ as the minimizer of $\hat{\cL}(f)$.
Lemma~\ref{lemma:l2errorboundmartingale} bounds $\mathcal{L}(\hat{f})$, the loss of $\hat{f}$ against the true and unknown score function. This lemma makes our aim clear. We will show a high probability bound on $H^{\hat{f}}$ defined in~\eqref{eq:Lf_bound} to control $\mathcal{L}(\hat{f})$. Interestingly, as shown in Lemma~\ref{lemma:error_martingale_decomposition_2} (and proved in Lemma~\ref{lemma:error_martingale_decomposition_2_actual_appendix}), for a fixed $f$ it is possible to decompose $H^{f}$ as a martingale difference sequence. 

% \dn{I think a good way to explain this would be as the standard \textbf{Doob martingale} \cite{durrett2019probability} corresponding to the RHS in  Lemma~\ref{lemma:l2errorboundmartingale}, with respect to the filtration we have defined}

The martingale difference decomposition of $H^{\hat{f}}$, exploiting the Markovian structure of \eqref{eq:fwd_noise}, has terms of the form $Q_{i} := \inner{G_{i}}{Y_{i}-\E\bbb{Y_{i}|\mathcal{F}_{i-1}}}$ adapted to the filtration $\left\{\mathcal{F}_{i}\right\}_{i \in [n]}$, where $G_i$ is a $\mathcal{F}_{i-1}$ measurable random variable. The proof  primarily uses the fact that for $t_{1} \leq t_{2} \leq t_{3}$, $\E\bbb{x_{t_1}|x_{t_2}, x_{t_3}} = \E\bbb{x_{t_1}|x_{t_2}}$ due to the Markov property. 

%We first show that conditioned on $\mathcal{F}_{i-1}$, $Q_{i}$ is subGaussian. However, as shown in Lemma~\ref{lemma:subGaussianity_parameters}, the subGaussianity parameters depend on the data dimension, $d$ along with $G_i$ and the step size, $\Delta$. Therefore, performing a concentration argument solely relying on this observation leads to a dimension-dependent bound. 


% \ps{What is $\zeta$?}\syamantak{Fixed.}
% \ps{Function class and filtration both have similar notation.}\syamantak{Fixed.}
% \dn{$\zeta$ is only used in the appendix and it is taken to be arbitrary, but in our application we use $\zeta = \frac{s-f}{m}$. In the main text, we can instantiate it to be $\frac{s-f}{m}$ for the sake of clarity.} 

\begin{lemma}\label{lemma:error_martingale_decomposition_2}
Let $\zeta = \frac{s-f}{m}$ for any $f \in \cF$. Define 
\small
\[
\bar{G}_i \!:= \sum_{j=1}^{N} \frac{\gamma_{j}e^{-(t_j-t_1)}\zeta\bb{t_j,x_{t_j}^{(i)}}}{\sigma_{t_j}^{2}}, \;\; 
G_{i,k}\! := \!\!\!\!\!\! \sum_{j=N-k+2}^{N}\!\!\!\!\! \frac{\gamma_{j}e^{-t_j}\zeta\bb{t_j,x_{t_j}^{(i)}}}{\sigma_{t_j}^{2}}
\]
\normalsize
and define $R_{i,k}$ as  
\small
\begin{equation}
    R_{i,k} := 
    \begin{cases} 
        0, & \text{for } k = 0, \\
        \begin{aligned}
            \bigl \langle G_{i,k+1}, 
            & \E[x_0^{(i)} | x_{t_{N-k+1}}^{(i)}]  - \E[x_0^{(i)} | x_{t_{N-k}}^{(i)}] 
            \bigr \rangle, 
        \end{aligned} 
        & \text{for } k \in [N-1], \\
        \bigl\langle \bar{G}_i, z_{t_1}^{(i)} - \E\bbb{z_{t_1}^{(i)}|x_{t_1}^{(i)}} \bigr\rangle, 
        & \text{for } k = N.
    \end{cases} \notag
\end{equation}
\normalsize
Let $t_0 = 0$. Consider the filtration defined by the sequence of $\sigma$-algebras,  
\[
\mathcal{F}_{i,k} := \sigma(\{x_{t}^{(j)} : 1\leq j < i, t \in \timeset\} \cup \{x_{t}^{(i)} : t \geq t_{N-k}\})
\]
for $i \in [m]$ and $k \in \{0,\dots, N\}$, satisfying the total ordering $\left\{ (i_1, j_1) < (i_2, j_2) \text{ iff } i_1 < i_2 \text{ or } i_1 = i_2, j_1 < j_2 \right\}.$. Then, 
\begin{enumerate}
    \item For $k \in [N-1]$, $G_{i,k+1}$ is measurable with respect to $\mathcal{F}_{i,k-1}$, and $\bar{G}_{i}$ is $\mathcal{F}_{N-1}$-measurable.
    \item For $i \in [m], k \in \{0\} \cup [N]$, $(R_{i,k})_{i,k}$ forms a martingale difference sequence with respect to the filtration above.
    \item $H^{f} = \sum_{i \in [m]}\sum_{k \in [N]}R_{i,k}\,.$, where $H^{f}$ is defined in Lemma~\ref{lemma:l2errorboundmartingale} 
\end{enumerate}
\end{lemma}

% \begin{lemma}\label{lemma:error_martingale_decomposition_2}Let $\zeta = \frac{s-f}{m}$ for any $f \in \cF$.
%     Define $\bar{G}_i := \sum_{j=1}^{N} \frac{\gamma_{j}e^{-(t_j-t_1)}\zeta\bb{t_j,x_{t_j}^{(i)}}}{\sigma_{t_j}^{2}}$, $G_{i,k} := \sum_{j = N-k+2}^{N} \frac{\gamma_{j}e^{-t_j}\zeta\bb{t_j,x_{t_j}^{(i)}}}{\sigma_{t_j}^{2}}$ and $R_{i,k}$ as  
%     \begin{equation}
%     R_{i,k} = \begin{cases} 0 \text{ for } k = 0 \\
%     \bigr \langle G_{i,k+1},\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]\bigr \rangle \text{ for } k \in [1, N-1],  
%     \\ 
%      \bigr\langle \bar{G}_i , z_{t_1}^{(i)}-\E\bbb{z_{t_1}^{(i)}|x_{t_1}^{(i)}}\bigr\rangle \text{ for } k = N
%      \end{cases} 
%     \end{equation}
%     Let $t_0 = 0$. Consider the filtration defined by the sequence of $\sigma$-algebras,  $\mathcal{F}_{i,k} := \sigma(\{x_{t}^{(j)} : 1\leq j < i, t \in \timeset\}\cup \{x_{t}^{(i)} : t \geq t_{N-k}\})$ for $i \in [m]$ and $k \in \{0,\dots, N\}$, satisfying the total ordering $\left\{\bb{i_1, j_1} < \bb{i_2, j_2} \text{ iff } i_1 < i_2 \text{ or } i_1 = i_2, j_1 < j_2\right\}$. Then 
%     \begin{enumerate}
%         \item For $k \in [N-1]$, $G_{i,k+1}$ is measurable with respect to $\mathcal{F}_{i,k-1}$ and $\bar{G}_{i}$ if $\mathcal{F}_{N-1}$ measurable.
%         \item For $i \in [m], k \in \{0\}\cup[N]$,  $(R_{i,k})_{i,k}$ forms a martingale difference sequence with respect to the filtration above. 
%         \item $H^{f} = \sum_{i \in [m]}\sum_{k \in [N]}R_{i,k}\,.$ 
%     \end{enumerate}
% \end{lemma}

In the above lemma, $R_{ik}$ denotes the Martingale Difference Sequence arising from the Doob decomposition (see e.g. ~\cite{durrett2019probability}). Our aim is to bound $H^{\hat{f}}$ by bound $H^{f}$ uniformly for every $f$, using martingale concentration.

In the next lemma, we show that conditioned on $\mathcal{F}_{i-1}$, $Q_{i}$ is subGaussian.
To gain intuition into how subGaussianity comes into play in our context, we note that Lemma F.3. in \cite{gupta2023sample} shows that the score function, $s(t, x_t)$, is $1/\sigma_{t}$-subGaussian. We develop a more fine-grained argument exploiting the smoothness of the score function to show a slightly different notion of subGaussianity (Definition~\ref{definition:new_subGaussian_def}) for our martingale difference sequence. 

\begin{restatable}{lemma}{martingalediffsubGaussianityparameters}\label{lemma:subGaussianity_parameters}
    Fix $\delta \in \bb{0,1}$. Consider $R_{i,k}$ and $\mathcal{F}_{i,k}$ as defined in Lemma~\ref{lemma:error_martingale_decomposition_2} and let $\Delta := t_{N-k+1}-t_{N-k}$. Under Assumption~\ref{assumption:score_function_smoothness}, following the definition in Definition~\ref{definition:new_subGaussian_def}, conditioned on $\mathcal{F}_{i,k-1}$, $R_{i,k}$ is $(\beta_{i,k}^2\|G_{i,k}\|^2,W_{i,k})$-subGaussian where $\beta_{i,k},W_{i,k}$ are $\mathcal{F}_{i,k-1}$ measurable random variables such that  
    % \dn{This has to be $(\beta_{i,k}^2\|G_{i,k}\|^2,W_{i,k})$ subGaussian} \syamantak{Fixed.}
    $W_{i,k} \leq \log\bb{\frac{2}{\delta}}$ with probability at-least $1-\delta$ and
    \bas{
        &\beta_{i,k} := \begin{cases}
            8\bb{L+1}e^{t_{N-k+1}}\sqrt{\Delta d}, \;\; k \in  [N-1], \\
            4\sqrt{\Delta d}, \;\; k = N
            \end{cases}
    }
\end{restatable}

%We first show that conditioned on $\mathcal{F}_{i-1}$, $Q_{i}$ is subGaussian. 
However, the subGaussianity parameters in Lemma~\ref{lemma:subGaussianity_parameters}, depend polynomially on the data dimension, $d$ along with $G_i$ and the step size, $\Delta$. Therefore, performing a concentration argument solely relying on this observation leads to a dimension-dependent bound. 


% \begin{restatable}{lemma}{errormartingaledecomposition}\label{lemma:error_martingale_decomposition_2}
% Let 
% \bas{
%     H^{f} := \sum_{i \in [m], t \in \timeset}\frac{ \gamma_{t}}{m}\bigr\langle f_{t}\bigr(x_{t}^{(i)}\bigr)-s_{t}\bigr(x_{t}^{(i)}\bigr),\frac{-z_{t}^{(i)}}{\sigma_{t}^{2}} -s_{t}\bigr(x_{t}^{(i)}\bigr)\bigr\rangle, 
% }
% Define $\bar{G}_i := \sum_{j=1}^{N} \frac{\gamma_{j}e^{-(t_j-t_1)}\bb{f_{t_j}(x_{t_j}^{(i)})-s_{t_j}(x_{t_j}^{(i)})}}{\sigma_{t_j}^{2}}$, $G_{i,k} := \sum_{j = N-k+2}^{N} \frac{\gamma_{j}e^{-t_j}\bb{f_{t_j}(x_{t_j}^{(i)})-s_{t_j}(x_{t_j}^{(i)})}}{\sigma_{t_j}^{2}}$ and $R_{i,k}$ as  
%     \bas{
%     & \inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]} \text{ for }  k < N, \\ \\
%     & \inner{\bar{G}_i}{z_{t_1}^{(i)}-\E\bbb{z_{t_1}^{(i)}|x_{t_1}^{(i)}}} \text{ for } k = N 
%     }
%     Then, $H^f = \sum_{i \in [m]}\sum_{k \in [N]}R_{i,k}$. Furthermore, consider the filtration defined by the sequence of $\sigma$-algebras,  $\mathcal{F}_{i,k} := \sigma\{x_{t}^{j} : j \in [i], t \geq t_{N-k}\}$ for $i \in [m]$ and $k < N$, satisfying the total ordering $\left\{\bb{i_1, j_1} < \bb{i_2, j_2} \text{ iff } i_1 < i_2 \text{ or } i_1 = i_2, j_1 < j_2\right\}$. Then for $i \in [m], k \in [N]$,  $\left\{R_{i,k}\right\}$ forms a martingale difference sequence. 
% \end{restatable}

% \ps{Can we unpack the math? What is H? How are the G's and R's going to be used later? Lets make it clear that the martingale structure that has been defined is necessary and (?) has not been done before....perhaps reemphasize somethings you have already mentioned before. And then say that the next several lemmas will be about applying martingale concentration. What may be nice also is if you give a quick sketch in text about the pieces that are needed and say which lemma gives which piece.}


To further refine our analysis and show a dimension-free bound, we evaluate the variance of $Q_{i}$ conditioned on $\mathcal{F}_{i-1}$. As shown in the next Lemma (Lemma~\ref{lemma:martingale_variance_bound}) (and proved in Lemma~\ref{lemma:martingale_diff_variance_bound_appendix}), the variance depends only on the smoothness parameter, $L$, along with $G_i$ and $\Delta$. 

\begin{lemma}[Variance bound for martingale difference sequence]\label{lemma:martingale_variance_bound}
Consider the martingale difference sequence \(R_{i,k}\) and the predictable sequence \(G_{i,k+1}\) with respect to the filtration \(\mathcal{F}_{i,k}\) from Lemma~\ref{lemma:variance_bound_1}. Define \(\Delta := t_{N-k+1} - t_{N-k}\). Then, $\E\bbb{R_{i,k}^{2}|\mathcal{F}_{i,k-1}} \leq \nu_{i,k}^{2}$
where
\begin{equation}
    \nu_{i,k}^{2} =
    \begin{cases} 
        &0, \quad\quad\quad\quad\quad\quad\quad\quad\quad\;\;\quad \text{if } k = 0, \\
        &C(L\Delta^2 + \Delta + L^2\Delta) e^{2t_{N-k+1}} \|G_{i,k+1}\|^2, 
        \text{ if } k \in [1, N-1], \\
        &C(L\Delta^2 + \Delta) \|\bar{G}_i\|^2, 
        \quad\quad\quad \text{if } k = N.
    \end{cases}\notag
\end{equation}
where \( C > 0 \) is an absolute constant.
\end{lemma}

% \begin{lemma}[Variance bound for martingale difference sequence]\label{lemma:martingale_variance_bound}
% Consider the martingale difference sequence $R_{i,k}$, predictable sequence $G_{i,k+1}$ with respect to the filtration $\mathcal{F}_{i,k}$ as considered in Lemma~\ref{lemma:variance_bound_1}. Define $\Delta:=t_{N-k+1}-t_{N-k}$ then $\E\bbb{R_{i,k}^{2}|\mathcal{F}_{i,k-1}} \leq \nu_{i,k}^{2}$ such that, 
% \bas{
%     \nu_{i,k}^{2} := \begin{cases} 0 \text{ if } k = 0 \\
%      C(L\Delta^2 + \Delta + L^2\Delta) e^{2t_{N-k+1}}\|G_{i,k+1}\|^2 \text{ if } k \in [1, N-1] \\
%     C(L\Delta^2 + \Delta )\|\bar{G}_i\|^2 \text{ if } k = N 
%     \end{cases}
% }
% where $C > 0$ is an absolute constant.
% % \dn{$k = N$ case follows from second order tweedie's formula. I have commented the lemma regarding this below, uncomment to see the proof}

% \end{lemma}

The proof of Lemma~\ref{lemma:martingale_variance_bound} is involved when $h_{t}(x) := \nabla^{2}\log\bb{p_{t}}(x)$ is not assumed to be Lipschtiz in $x$. Starting with the martingale difference sequence defined in Lemma~\ref{lemma:error_martingale_decomposition_2}, an application of the second-order tweedie's formula (see Lemma~\ref{lemma:second_order_tweedie_application}), reduces the problem to bounding the operator norm $\mathsf{Cov}(s\bb{t', x_{t'}}|x_{t})$ for $t-t' = \Delta > 0$, i.e, the conditional covariance matrix of the score function given the future. 
Exploiting the smoothness assumption on the score function, an application of the mean value theorem reduces our problem to bounding the operator norm of:
% \syamantak{Say that the variance of the past given the future shows up. Variance of $s_t'|x_t$ can be written by bringing in an iid copy. And the mean value theorem shows that the Hessian can be involved. and now we need tools to show smoothness of the Hessian }
% The proof strategy relies on Assumption~\ref{assumption:score_function_smoothness}, and an application of the mean value theorem involving the hessian, $h_{t} := \nabla^{2}\log\bb{p_{t}}$, which yields terms of the form 
\bas{
    \E\bbb{h_{t'}(y_{t'})(x_{t'}-\tilde{x}_{t'})(x_{t'}-\tilde{x}_{t'})^{\top}h_{t'}(y_{t'})^{\top}|x_t}, t' < t
}
for $x_{t'}, \tilde{x}_{t'}$ i.i.d conditioned on $x_{t}$ and $y_{t'} = \lambda x_{t'} + (1-\lambda)\tilde{x}_{t'}$, $\lambda \in (0,1)$. Notice that $y_{t'}|x_t$ is dependent on $x_{t'},\tilde{x}_{t'}|x_t$, which does not allow the use of Tweedie’s second-order formula (Lemma~\ref{lemma:second_order_tweedie_application}) to bound $\E\bbb{(x_{t'}-\tilde{x}_{t'})(x_{t'}-\tilde{x}_{t'})^{\top}|x_t}$ and derive variance bounds that are dimension-free. To approximately allow this argument, we decompose $h_{t'}(y_{t'})$ into two components:
\[
h_{t'}\bb{y_{t'}} = h_{t', \epsilon}\bb{y_{t'}} + \bb{h_{t'}\bb{y_{t'}} - h_{t', \epsilon}\bb{y_{t'}}}.
\]
Here, the first term, \(h_{t', \epsilon}\bb{y_{t'}}\), represents a ``smoothed" or ``mollified" hessian, averaged over an appropriately chosen distribution, which we show satisfies Lipschitz continuity. This allows us to approximate $h_{t',\epsilon}(y_{t'}) \approx h_{t',\epsilon}(e^{\Delta}x_t)$ and bound the variance with Tweedie's second order formula. The second term, which represents the deviation between the original and mollified Hessians, requires a finer analysis, breaking the interval $[t', t]$ into many subintervals and draws upon Lusin's theorem (Lemma~\ref{lemma:lusin_theorem}) to provide approximate uniform continuity for the hessian $h_{t'}$, as developed further in Lemma~\ref{lemma:lusin_theorem_decomp}.


% We next bound the conditional variance of the martingale difference sequence developed in Lemma~\ref{lemma:error_martingale_decomposition_2}. 
% Under Assumption~\ref{assumption:score_function_smoothness}, $\normop{\nabla^{2}\log\bb{p_{t}\bb{x_t}}} \leq L$, which helps us control the variance.

% \begin{restatable}{lemma}{martingalediffvariancebound}\label{lemma:martingale_variance_bound}Consider $R_{i,k}$ and $\mathcal{F}_{i,k}$ as defined in Lemma~\ref{lemma:error_martingale_decomposition_2}. Let $\forall i \in [m],  k < N, \nu_{i,k}^{2} := \E\bbb{R_{i,k}^{2}|\mathcal{F}_{i,k-1}}$ and $\forall i \in [m], \bar{\nu}_{i}^{2} := \E\bbb{R_{i,N}^{2}|\mathcal{F}_{i,N-1}}$. Then, for a universal constant $c > 0$, we have $\forall i \in [m]$,
%     \bas{
%         & \text{ for } k < N, \;\; \nu_{i,k}^{2} \leq c \Delta e^{2t_{N-k+1}}\bb{L^{2}+1}\norm{G_{i,k}}_{2}^{2},  \\
%         &  \bar{\nu}_i^{2} \leq \sigma_{t_{1}}^{2}\bb{1 + L\sigma_{t_1}^{2}}\norm{\bar{G}_i}_{2}^{2} = c\Delta\bb{1 + L\Delta}\norm{\bar{G}_i}_{2}^{2}, 
%     } 
% \end{restatable}

% We additionally show, in Lemma~\ref{lemma:subGaussianity_parameters}, that the martingale difference sequence follows a coarser notion of subGaussianity (defined formally in Definition~\ref{definition:new_subGaussian_def}), which allows us to develop concentration bounds, while still being able to use the variance provided in Lemma~\ref{lemma:martingale_variance_bound}.
% However, as shown in Lemma~\ref{lemma:subGaussianity_parameters}, the subGaussianity parameters depend on the data dimension, $d$ along with $G_i$ and the step size, $\Delta$. Therefore, performing a concentration argument solely relying on this observation leads to a dimension-dependent bound.\bk

%\ps{Shouldn't we move the subGaussianity stuff to the notation/setup section? This part is already math-heavy.}

Putting together Lemma~\ref{lemma:subGaussianity_parameters} and Lemma~\ref{lemma:martingale_variance_bound}, we provide a general concentration tool for martingale difference sequences with bounded variance and subGaussianity in
Lemma~\ref{lemma:martingale_concentration_lemma}, which may be of independent interest. We follow a similar proof strategy via a supermartingale argument as in the proof Freedman's inequality (see for e.g. \cite{tropp2011freedman}), but diverge in dealing with subGaussianity instead of almost surely bounded random variables.

\begin{restatable}{lemma}{martingaleconcentrationlemma}\label{lemma:martingale_concentration_lemma}
Let $M_{n} = \sum_{i=1}^{n}\langle G_i, Y_i - \bE[Y_i|\mathcal{F}_{i-1}] \rangle, M_{0} = 1$ and define the filtration $\left\{\mathcal{F}_{i}\right\}_{i \in [n]}$ such that:
\begin{enumerate}
    \item  $G_i$ is $\mathcal{F}_{i-1}$ measurable.
    \item  $\langle G_i,Y_i-\bE [Y_i|\mathcal{F}_{i-1}]\rangle$ is $(\beta_i^{2}\|G_i\|^{2}, K_i)$ sub-Gaussian conditioned on $\mathcal{F}_{i-1}$ (where $\beta_i,K_i$ are random variables measurable with respect to $\mathcal{F}_{i-1}$)
    \item $\mathsf{var}(\langle G_i, Y_i - \bE[Y_i|\mathcal{F}_{i-1}]\rangle|\mathcal{F}_{i-1}) \leq \nu^2_i \|G_i\|^2$ and define $J_i := \max(1,\frac{1}{K_i}\log\frac{\beta_i^2 K_i}{\nu^2_i})$.
\end{enumerate}
Pick a $\lambda > 0$ and let $\mathcal{A}_i(\lambda) = \{\lambda J_i\|G_i\|\beta_i\sqrt{K_i} \leq c_0\}$ for some small enough universal constant $c_0$. Then, there exists a universal constant $C > 0$ such that:
\begin{enumerate}
    \item $\exp(\lambda M_{n} - C\lambda^2\sum_{i=1}^{n}\nu_i^2\|G_i\|^2 )\prod_{i=1}^{n}\mathbbm{1}(\mathcal{A}_i(\lambda))$ is a super-martingale with respect to the filtration $\mathcal{F}_{i}$
    \item $\forall v > 0, \; \bP(\{\lambda M_n > C \lambda^2\sum_{i=1}^{n}\nu_i^2\|G_i\|^2 + v\}\cap_{i=1}^{n} \mathcal{A}_i(\lambda)) \leq \exp(-v)$
\end{enumerate}
\end{restatable}

% \ps{I think you have nearly everything for a time uniform bound. In other words is it possible to have $P(sup_n blah_n>something)<something$? I think most freedman's inequalities are time uniform} \syamantak{We can try this for the appendix version.}

Observe that the concentration result developed in Lemma~\ref{lemma:martingale_concentration_lemma} has two parts. Optimizing over the choice of $\lambda$, it can be shown that the bound on $M_{n}$ depends on two terms: (1) an $\ell_{2}$ term,  $\sum_{i\in[n]}\nu_{i}^{2}\norm{G_i}^{2}$ and (2) an $\ell_{\infty}$ term, $\sup_{i \in [n]}J_{i}\norm{G_{i}}\beta_{i}\sqrt{K_i}$. When applied in our context, these two terms in turn depend on norms, $\norm{f-s}_{2}$ and $\norm{f-s}_{\infty}$. This is where the time-regularity assumption in Assumption~\ref{assumption:score_function_smoothness} plays a crucial role in our analysis. Specifically, it enables us to bridge the $\ell_{\infty}$ and $\ell_{2}$ norm bounds derived from the martingale concentration results in Lemma~\ref{lemma:martingale_concentration_lemma}. The proof of Lemma~\ref{lemma:l2_linfnity_bound_f} leverages this assumption to relate $\norm{f\bb{t+k\Delta, x_{t+k\Delta}}}_{2}$ to $\norm{f\bb{t, x_{t}}}_{2}$, as shown by:
% \dn{the below needs to conform to our modified assumption (i.e, add log factors) or mention that we are skipping it for brevity} \syamantak{Fixed.}
\bas{ \norm{f\bb{t+k\Delta, x_{t+k\Delta}}}_2 - e^{k\Delta}\norm{f\bb{t, x_{t}}}_{2} \geq - \tilde{\Omega}(L\sqrt{dk\Delta}). } Exploiting this  property over a carefully selected range of $k$ values allows us to relate $\ell_{\infty}$ and $\ell_{2}$ norm bounds as we show in the following Lemma. 

\begin{restatable}{lemma}{ltwolinfinityerrorboundtimeregularity}\label{lemma:l2_linfnity_bound_f}
    Under Assumption~\ref{assumption:score_function_smoothness}, with probability $1-\delta$, for a universal constant $C  > 0$ the following holds uniformly for every $f \in \cF$:
    \bas{
       &\biggr(\sup_{\substack{i \in [m]\\ j \in [N]}}\norm{f\bb{t_j,x_{t_j}}-s\bb{t_j,x_{t_j}}}_{2}\biggr)^{2} \leq C\Delta^{\frac{1}{3}}\biggr(\sum_{\substack{i\in [m] \\ j \in [N]}}\norm{f\bb{t_j,x_{t_j}}-s\bb{t_j,x_{t_j}}}_{2}^{2}\biggr) + CL^{2}d\Delta^{\frac{1}{3}}\log(\frac{Nm}{\delta})
    }
\end{restatable}
The above lemma establishes that the simultaneous analysis of all timesteps harnesses the smoothness across time. In the absence of this approach, the smoothness assumption in the $x_{t}$-space would lack dependence on $\Delta$ and could grow as large as the Lipschitz constant $L$. This is essential for establishing nearly dimension-independent bounds.
% \rd 
% Importance of the same network across timesteps to prove our bounds.
% \subsection{Martingale Decomposition of the error}
% \subsection{Variances of the martingale difference sequence}
% \subsection{Concentration Argument}: We have variance and subGaussianity with a slightly worse parameter dependent on d. How to combine?
% \subsection{Why is time regularity important?} 
% \bk