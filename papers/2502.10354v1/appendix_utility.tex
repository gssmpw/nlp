\section{Utility Results}
\label{appendix:utility_results}

\begin{definition}[norm subGaussian]\label{definition:normsubGaussian}
    We will call a random vector $X \in \bR^d$ to be $\sigma$ norm subGaussian if $\E X = 0$ and $$\bE \exp(\tfrac{\|X\|^2}{\sigma^2}) \leq 2\,.$$ 
\end{definition}
\begin{definition}
    We will call a random vector $X \in \bR^d$ to be $\sigma$ subGaussian if $\E X = 0$ and for every $v \in \bR^d$ and $\lambda \in \bR$ we have: $$\bE \exp(\lambda \langle v,X\rangle) \leq \exp(\tfrac{\lambda^2\|v\|^2\sigma^2}{2})\,.$$ 
\end{definition}

\begin{lemma}\label{lemma:gaussian_exponential_moment}Let $X \sim \mathcal{N}\bb{0, \sigma^{2}\id}$. Then, $X$ is $2\sigma$ norm subGaussian.   
\end{lemma}
\begin{proof}
    Consider the random variable $y := \frac{\norm{X}_{2}^2}{\sigma^{2}}$. Then, $y \sim \chi(d)$ follows the chi-squared distribution with $d$ degrees of freedom. Therefore, for any $t < \frac{1}{2}$, \bas{\E\bbb{\exp\bb{t\frac{\norm{X}_{2}^{2}}{\sigma^{2}}}} &= \bb{1 - 2t}^{-\frac{d}{2}}}
    Setting $t = \frac{1}{4d}$, we have
    \bas{\E\bbb{\exp\bb{\frac{\norm{X}_{2}^{2}}{\bb{2\sigma}^{2}}}} &= \bb{1 - \frac{1}{2d}}^{-\frac{d}{2}} = \bb{\bb{1 - \frac{1}{2d}}^{-2d}}^{\frac{1}{4}} \leq 2
    }
\end{proof}


\begin{lemma}\label{lemma:smootH^{f}n_subgauss}
    For all $t > 0$, $x_{1}, x_{2} \in \R^{d}$, consider any function $u : \R^{d} \rightarrow \R^d$ satisfying $\norm{u\bb{x_{1}} - u\bb{x_{2}}}_{2} \leq S\norm{x_{1}-x_{2}}_{2}$, where $S > 0$ is a fixed constant. For timesteps $0 \leq t' < t$, consider the random variable 
    \bas{
        q_{t,t'} := u\bb{x_{t'}} - \E\bbb{u\bb{x_{t'}}|x_{t}}
    }
    where $x_{t}$ is defined in $\eqref{eq:fwd_noise}$. Then, $q_{t,t'}$ is $\phi\sqrt{d}$ norm subGaussian for 
    \bas{
        \phi := 4S e^{\Delta}\sqrt{1-e^{-2\Delta}}
    }
    where $\Delta := t-t'$.
\end{lemma}
\begin{proof}
    We first note that 
    \bas{
        \E_{x_{t},x_{t'}}\bbb{q_{t,t'}} &= \E_{x_{t'}}\bbb{u_{t'}\bb{x_{t'}}} - \E_{x_{t}}\bbb{ \E\bbb{u_{t'}\bb{x_{t'}}|x_{t}}} = 0 
    }
    Using Lemma 1 from \cite{jin2019short}, we show that $\E_{x_{t'},x_{t}}\bbb{\exp\bb{\frac{\norm{q_{t,t'}}_{2}^{2}}{\phi^{2}d}}} \leq 2$. Let $x_{t'}'$ be an $\iid$ copy of $x_{t'}$, conditioned on $x_{t}$. Then, we have,
    \ba{\E_{x_{t'},x_{t}}\bbb{\exp\bb{\frac{\norm{q_{t,t'}}_{2}^{2}}{\phi^{2}d}}} &=  \E_{x_{t}}\bbb{\E_{x_{t'}}\bbb{\exp\bb{\frac{\norm{q_{t,t'}}_{2}^{2}}{\phi^{2}d}}\bigg|x_{t}}} \notag \\
        &= \E_{x_{t}}\bbb{\E_{x_{t'}}\bbb{\exp\bb{\frac{\norm{u\bb{x_{t'}} - \E_{x_{t'}}\bbb{u\bb{x_{t'}}|x_{t}}}_{2}^{2}}{\phi^{2}d}}\bigg|x_{t}}} \notag \\
        &= \E_{x_{t}}\bbb{\E_{x_{t'}}\bbb{\exp\bb{\frac{\norm{u\bb{x_{t'}} - \E_{x_{t'}'}\bbb{u\bb{x_{t'}'}|x_{t}}}_{2}^{2}}{\phi^{2}d}}\bigg|x_{t}}} \\
        &= \E_{x_{t}}\bbb{\E_{x_{t'}}\bbb{\exp\bb{\frac{\norm{\E_{x_{t'}'}\bbb{u\bb{x_{t'}} - u\bb{x_{t'}'}|x_{t}}}_{2}^{2}}{\phi^{2}d}}\bigg|x_{t}}} \notag \\
        &\leq \E_{x_{t}}\bbb{\E_{x_{t'}}\bbb{\exp\bb{\frac{\E_{x_{t'}'}\bbb{\norm{u\bb{x_{t'}} - u\bb{x_{t'}'}}_{2}^{2}|x_{t}}}{\phi^{2}d}}\bigg|x_{t}}} \notag \\
        &\leq \E_{x_{t}}\bbb{\E_{x_{t'},x_{t'}'}\bbb{\exp\bb{\frac{\norm{u\bb{x_{t'}} - u\bb{x_{t'}'}}_{2}^{2}}{\phi^{2}d}}\bigg|x_{t}}} \notag \\
        &\leq \E_{x_{t}}\bbb{\E_{x_{t'},x_{t'}'}\bbb{\exp\bb{\frac{S^{2}\norm{x_{t'}-x_{t'}'}_{2}^{2}}{\phi^{2}d}}\bigg|x_{t}}} \label{eq:stprime_mgfbound_2_new}
    }
    Note that using \eqref{eq:fwd_noise}, $x_{t} = e^{-\Delta}x_{t'} + w_{t,t'} = e^{-\Delta}x_{t'}' + w_{t,t'}'$,
    for $w_{t,t'}, w_{t,t'}' \sim \normal\bb{0,\sigma_{t-t'}^{2}\id_{d}}$. Therefore, from \eqref{eq:stprime_mgfbound_2_new}, 
    \bas{
        \E_{x_{t'},x_{t}}\bbb{\exp\bb{\frac{\norm{q_{t,t'}}_{2}^{2}}{\phi^{2}d}}} &\leq \E_{x_{t}}\bbb{\E_{w_{t,t'},w_{t,t'}'}\bbb{\exp\bb{\frac{S^{2}e^{2\Delta}\norm{w_{t,t'}-w_{t,t'}'}_{2}^{2}}{\phi^{2}d}}\bigg|x_{t}}} \\ &= \E_{w_{t'},w_{t'}'}\bbb{\exp\bb{\frac{S^{2}e^{2\Delta}\norm{w_{t,t'}-w_{t,t'}'}_{2}^{2}}{\phi^{2}d}}} \\
        &\leq \E_{w_{t'},w_{t'}'}\bbb{\exp\bb{\frac{2S^{2}e^{2\Delta}(\norm{w_{t,t'}}_{2}^2+\norm{w_{t,t'}'}_{2}^{2})}{\phi^{2}d}}} \\
        &\leq \frac{1}{2}\E_{w_{t'},w_{t'}'}\bbb{\exp\bb{\frac{4S^{2}e^{2\Delta}\norm{w_{t,t'}}_{2}^2}{\phi^{2}d}}} + \frac{1}{2}\E_{w_{t'},w_{t'}'}\bbb{\exp\bb{\frac{4S^{2}e^{2\Delta}\norm{w'_{t,t'}}_{2}^2}{\phi^{2}d}}}\\
        &\leq 2
    }
    where the last inequality follows since $w_{t,t'}, w_{t,t'}' \sim \normal\bb{0,\sigma_{t-t'}^{2}\id_{d}}$ marginally (but not necessarily conditionally). %\dn{is there a good reference for the last step?}
\end{proof}
%\dn{The lemma below does not seem correct since $y$ need not have $0$ mean.}
% \dn{
% \begin{lemma}\label{lemma:conditional_subg}
%     Let $z$ be a $\nu$-subGaussian random variable and $\mathcal{E}$ be any event. Then, $y := z - \E\bbb{z|\mathcal{E}}$ is $2\nu$-subGaussian.
% \end{lemma}
% \begin{proof}
%     Consider $\ell := \E\bbb{z|\mathcal{E}}$. Then for any vector $v \in \R^{d}$, 
%     \bas{
%         \E\bbb{v^{\top}\ell} &= \E\bbb{\exp\bb{\E\bbb{v^{\top}z}|\mathcal{E}}} \\
%                              &\leq \E\bbb{\E\bbb{\exp\bb{v^{\top}z}|\mathcal{E}}}, \text{ using Jensen's inequality} \\
%                              &= \E\bbb{\exp\bb{v^{\top}z}} \\
%                              &\leq \exp\bb{\frac{\nu^{2}\norm{v}_{2}^{2}}{2}}, \text{ using subGaussianity of } z
%     }
%     Therefore, $\ell$ is also $\nu$-subGaussian. The result then follows using sum of subGaussian random variables.
% \end{proof}
% }
\begin{lemma}\label{eq:time_smoothness_of_score_function} Fix $\delta > 0$. Let $t > t'$.  Then, under Assumption~\ref{assumption:score_function_smoothness}-(1), with probability at least $1-\delta$ over $x_{t}$, 
\bas{
    \norm{e^{-(t-t')}s(t, x_{t})-s(t', e^{(t-t')}x_{t})}_{2} \leq e^{(t-t')}L\sqrt{8d(t-t')\log\bb{\frac{2}{\delta}}}
}
where $\sigma_{t-t'}^{2} := 1-e^{-2\bb{t-t'}} \leq 2\bb{t-t'}$.
\end{lemma}
\begin{proof}
    Using Corollary 2.4 from \cite{de2024target}, 
    \ba{
        s\bb{t, x_t} &= e^{t-t'}\E\bbb{s\bb{t', x_t'}|x_{t}} \label{eq:tsm_1}
    }
    Using \eqref{eq:fwd_noise}, 
    \ba{
    x_{t} = e^{-\bb{t-t'}}x_{t'} + z_{t,t'}, \text{ for } z_{t,t'} \sim \mathcal{N}\bb{0,\sigma_{t-t'}^{2}\id} \label{eq:t_tprime_sde}
    }
    Where $z_{t,t'}$ is independent of $x_{t'}$.
     Let $y_{t,t'} := e^{-(t-t')}s(t, x_{t})-s(t', e^{(t-t')}x_{t})$. Then, 
    \bas{
        \|y_{t,t'}\| &= \|e^{-(t-t')}s_{t}(x_t)-s(t', e^{(t-t')}x_t)\| \\
        &= \|\E\bbb{s\bb{t', x_t'}|x_{t} } - s(t', e^{(t-t')}x_t)\| \\
        &= \bigr\|\E\bbb{s_{t'}\bb{e^{(t-t')}(x_t - z_{t,t'})} - s(t', e^{(t-t')}x_t)|x_{t}}\bigr\| \\
        &\leq e^{t-t'}L\E\bbb{\norm{z_{t,t'}}_{2}|x_{t}}
    }




% \dn{
%     Now consider Second Order Tweedie's formula, we have:
%     $\E[z_{t,t'}z_{t,t'}^{\transpose}|x_t] = \sigma_{t-t'}^4 h_t(x_t) + \sigma_{t-t'}^4s_t(x_t)s_t(x_t)^{\transpose} + \sigma^{2}_{t-t'}\id$. Taking trace on both sides, we have:
%     $$\E[\|z_{t,t'}\|^2|x_t] \leq dL \sigma_{t-t'}^4 + \sigma_{t-t'}^4\|s_t(x_t)\|^2 + \sigma_{t-t'}^2 d$$
%     Applyiing this to the equation above, we have:
%     $y_{t,t'}  \leq \sqrt{dL^3\sigma_{t-t'}^4 + dL^2\sigma_{t-t'}^2} + L\sigma_{t-t'}^2\|s_t(x_t)\|$}
    
    Note that since $z_{t,t'} \sim \mathcal{N}\bb{0,\sigma_{t-t'}^{2}\id}$, 
    \bas{
        \E\bbb{\exp\bb{\frac{\norm{z_{t,t'}}_{2}^{2}}{4\sigma_{t-t'}^{2} d}}} \leq 2, \text{ using Lemma~}\ref{lemma:gaussian_exponential_moment}
    }
    Therefore, with probability at least $1-\delta$ over $x_t$:
    \bas{
       \E\bbb{\exp\bb{\frac{\norm{z_{t,t'}}_{2}^{2}}{4\sigma_{t-t'}^{2} d}}\bigg|x_{t}} \leq \frac{2}{\delta}, \text{ using Markov's inequality} 
    }
    Using Jensen's inequality, 
    \bas{
        \exp\bb{\frac{\E\bbb{\norm{z_{t,t'}}_{2}^{2}|x_{t}}}{4\sigma_{t-t'}^{2} d}} \leq \E\bbb{\exp\bb{\frac{\norm{z_{t,t'}}_{2}^{2}}{4\sigma_{t-t'}^{2} d}}\bigg|x_{t}} \leq \frac{2}{\delta}
    }
    The result then follows by taking log on both sides.
\end{proof}

% \dn{ This one depends on Lemma~\ref{lemma:conditional_subg}, which I do not believe is correct. So, rewriting it. 
% \begin{lemma}\label{lemma:subGaussianity_1}
%     Let $w_{t,t'} := z_{t,t'} + \sigma_{t-t'}^{2}s\bb{t, x_t}$ for $t > t' \geq 0$. Then, $w_{t,t'}$ is $\nu_{t,t'}$-subGaussian for
%     \bas{
%         \nu_{t,t'} := 2\sigma_{t,t'}
%     }
% \end{lemma}
% \begin{proof}
%     Note that $z_{t,t'} \sim \mathcal{N}\bb{0,\sigma_{t-t'}^{2}\id}$. Additionally, using Tweedie's formula, 
%     \bas{
%         s\bb{t, x_t} &= -\E\bbb{\frac{z_{t,t'}}{\sigma_{t-t'}^{2}}\bigg|x_{t}}
%     }
%     Therefore, $w_{t,t'} = z_{t,t'} - \E\bbb{z_{t,t'}|x_t}$. Using Lemma~\ref{lemma:conditional_subg}, $w_{t,t'}$ is subGaussian with parameter $2\sigma_{t,t'}$.
% \end{proof}
% }

\begin{lemma}\label{lemma:subGaussianity_1}
    Let $w_{t,t'} := z_{t,t'} + \sigma_{t-t'}^{2}s\bb{t, x_t}$ for $t > t' \geq 0$. Then, $w_{t,t'}$ is $\nu_{t,t'}\sqrt{d}$ norm subGaussian for $\nu_{t,t'} := 4\sigma_{t-t'}$.
\end{lemma}
\begin{proof}
Notice that $x_t = e^{t'-t}x_{t'} + z_{t,t'}$
Using Tweedie's formula, $s\bb{t, x_t} = -\E\bbb{\frac{z_{t,t'}}{\sigma_{t-t'}^{2}}\bigg|x_{t}}$. Therefore, 
\bas{
    e^{t-t'}\sigma_{t-t'}^2 s_t(x_t) + e^{t-t'}x_t &= \E [x_{t'}|x_t]
    \implies w_{t,t'} = -e^{t'-t}x_t + \E[e^{t'-t}x_{t'}|x_t]
} 
Applying Lemma~\ref{lemma:smootH^{f}n_subgauss} with $u(x) = -e^{t'-t}x$ (which is $e^{t'-t}$ Lipschitz), we conclude the result.
\end{proof}


% \dn{What are the assumptions under which this holds?. In the result below, I have added factors of $\sqrt{d}$ wherever appropriate and have changed this to norm subGaussian.}
\begin{lemma}\label{lemma:subGaussianity_2}
Suppose Assumption~\ref{assumption:score_function_smoothness}-(1) holds. Let $v_{t,t'} := \E[x_0|x_{t}]-\E[x_0|x_{t'}]$ for $t > t' \geq 0$. Then, $v_{t,t'}$ is $\rho_{t,t'}\sqrt{d}$ norm subGaussian for 
    \bas{
        \rho_{t,t'} := 8\bb{L + 1}e^{t}\sigma_{t-t'}
    }
\end{lemma}
\begin{proof}
    Using Tweedie's formula, for all $t > 0$,
    \bas{
        \E\bbb{x_{0}|x_{t}} &= \E\bbb{e^{t}\bb{x_{t}-z_{t}}|x_{t}} = e^{t}x_{t} + e^{t}\E\bbb{-z_{t}|x_{t}} = e^{t}\bb{x_{t} + \sigma_{t}^{2}s\bb{t, x_{t}}}
    }
    
    Note that $x_{t'} = e^{t-t'}\bb{x_t - z_{t,t'}}$. Furthermore, note that
    \bas{
        \E\bbb{z_{t,t'}|x_{t}} = -\sigma_{t-t'}^{2}s\bb{t, x_t}, \;\; \E\bbb{s_{t'}\bb{x_{t'}}|x_t} = e^{-\bb{t-t'}}s\bb{t, x_t}
    }
    Therefore, we have 
    \bas{
        v_{t,t'} &= \underbrace{e^{t}\bb{z_{t,t'} + \sigma_{t-t'}^{2}s\bb{t, x_t}}}_{:= T_1} - \underbrace{e^{t'}\sigma_{t'}^{2}\bb{s_{t'}\bb{x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}}_{:= T_2}
    }
    Using Lemma~\ref{lemma:subGaussianity_1}, $T_1$ is $4e^{t}\sigma_{t-t'}\sqrt{d}$ norm subGaussian. Using Lemma~\ref{lemma:smootH^{f}n_subgauss}, $T_2$ is $4L e^{t-t'}e^{t'}\sigma_{t'}^{2}\sigma_{t-t'}\sqrt{d} = 4Le^{t}\sigma_{t'}^{2}\sigma_{t-t'}\sqrt{d}$ norm subGaussian. Therefore, the result follows using the sum of subGaussian random variables.
\end{proof}

% \begin{lemma}[Freedman's inequality] Let $\left\{D_k, \mathcal{F}_k\right\}$ be a martingale difference sequence such that,
% \begin{enumerate}
%     \item $\E\bbb{D_k|\mathcal{F}_{k-1}} = 0$
%     \item $D_k \leq b$ almost surely
% \end{enumerate}
% Then, for all $\lambda \in \bb{0, \frac{1}{b}}$ and $\delta \in \bb{0,1}$, 
% \bas{
%     \mathbb{P}\bb{\sum_{i\in[T]}X_i \leq \lambda\sum_{i\in[T]}\E\bbb{D_{k}^{2}|\mathcal{F}_{k-1}} + \frac{\log\bb{\frac{1}{\delta}}}{\lambda}} \geq 1-\delta
% }
% \end{lemma}
\begin{lemma}\label{lemma:sum_bound_1}
Let $\Delta > 0$ and $\Delta < c_0$ for some universal constant $c_0$. Then, \begin{enumerate}
    \item $\sum_{k =1}^{N}\sum_{j = k}^{N}\frac{e^{2\bb{k-j}\Delta}}{\bb{1-e^{-2\Delta j}}^{2}} \leq \frac{1}{1 - e^{-2\Delta}}\bb{N + \frac{1}{1-e^{-2\Delta}}}$
    \item $\sum_{j=1}^{N}\frac{e^{-2\Delta\bb{j-1}}}{\bb{1-e^{-2\Delta j}}^{2}} \leq \frac{2}{\bb{1-e^{-2\Delta }}^{2}}$
    \item     
    $\sum_{j=1}^{N}\frac{e^{-\Delta\bb{j-1}}}{1-e^{-2\Delta j}} \leq \frac{e^{-\Delta }}{1-e^{-2\Delta }} + \frac{\log(\tfrac{1}{\Delta})}{2\Delta}$
\end{enumerate}
\end{lemma}
\begin{proof}
Let us start with the first bound. We have, 
\bas{
    \sum_{k =1}^{N}\sum_{j = k}^{N}\frac{e^{2\bb{k-j}\Delta}}{\bb{1-e^{-2\Delta j}}^{2}} &= \sum_{j = 1}^{N}\sum_{k =1}^{j}\frac{e^{2\bb{k-j}\Delta}}{\bb{1-e^{-2\Delta j}}^{2}} \\
    &= \sum_{j = 1}^{N}\frac{1}{\bb{1-e^{-2\Delta j}}^{2}}\sum_{k =1}^{j}e^{2\bb{k-j}\Delta} \\
    &=  \sum_{j = 1}^{N}\frac{1}{\bb{1-e^{-2\Delta j}}^{2}} \frac{e^{2\Delta}}{e^{2\Delta}-1}\bb{1 - e^{-2\Delta j}} \\
    &=  \frac{e^{2\Delta}}{e^{2\Delta}-1}\sum_{j = 1}^{N}\frac{1}{1-e^{-2\Delta j}}
}
Consider the function $f\bb{x} := \frac{1}{1-e^{-2\Delta x}}$. Then, $f\bb{x}$ is positive, convex and decreasing. Therefore, 
\bas{
    \sum_{j = 1}^{N}\frac{1}{1-e^{-2\Delta j}} &\leq f\bb{1} + \int_{1}^{N}\frac{1}{1-e^{-2\Delta x}}dx \\
    &= \frac{1}{1-e^{-2\Delta}} + \frac{1}{2\Delta}\ln\bb{e^{2\Delta x} - 1}\bigg|_{1}^{N} \\
    &\leq \frac{1}{1-e^{-2\Delta}} + \frac{1}{2\Delta}\ln\bb{e^{2\Delta N} - 1} \\
    &\leq N + \frac{1}{1-e^{-2\Delta}}
}
which completes the first result. Now for the second result, 
\bas{
    \sum_{j=1}^{N}\frac{e^{-2\Delta\bb{j-1}}}{\bb{1-e^{-2\Delta j}}^{2}} &= e^{2\Delta}\sum_{j=1}^{N}\frac{e^{-2\Delta j}}{\bb{1-e^{-2\Delta j}}^{2}}
}
Consider the function, $g\bb{x} := \frac{e^{-2\Delta x}}{\bb{1-e^{-2\Delta x}}^{2}}$. For $x > 0$, $g\bb{x}$ is a positive, decreasing and convex function. Therefore, 
\bas{
    \sum_{j=1}^{N}\frac{e^{-2\Delta j}}{\bb{1-e^{-2\Delta j}}^{2}} &\leq g\bb{1} + \int_{1}^{N}g\bb{x}dx \\
    &= \frac{e^{-2\Delta }}{\bb{1-e^{-2\Delta }}^{2}} + \int_{1}^{N}\frac{e^{-2\Delta x}}{\bb{1-e^{-2\Delta x}}^{2}}dx \\
    &= \frac{e^{-2\Delta }}{\bb{1-e^{-2\Delta }}^{2}} + \frac{1}{2\Delta\bb{1-e^{-2\Delta x}}}\bigg|_{1}^{N}  \\
    &\leq \frac{e^{-2\Delta }}{\bb{1-e^{-2\Delta }}^{2}} + \frac{1}{2\Delta\bb{1-e^{-2\Delta N}}} \\
    &\leq \frac{2e^{-2\Delta }}{\bb{1-e^{-2\Delta }}^{2}}
}
which completes the proof. Finally for the third result, consider the function $h\bb{x} := \frac{e^{-\Delta x}}{1-e^{-2\Delta x}}$. For $x > 0$, $h\bb{x}$ is a positive, decreasing and convex function. Therefore, 

\bas{
    \sum_{j=1}^{N}\frac{e^{-\Delta j}}{1-e^{-2\Delta j}} &\leq h\bb{1} + \int_{1}^{N}h\bb{x}dx \\
    &= \frac{e^{-\Delta }}{1-e^{-2\Delta }} + \int_{1}^{N}\frac{e^{-\Delta x}}{1-e^{-2\Delta x}}dx \\
    &= \frac{e^{-\Delta }}{1-e^{-2\Delta }} + \frac{1}{2\Delta}\log\bb{\tanh\bb{\Delta x}}\bigg|_{1}^{N} \\
    &\leq \frac{e^{-\Delta }}{1-e^{-2\Delta }} - \frac{\log\bb{\tanh\bb{\Delta}}}{2\Delta} \\
    &\leq \frac{e^{-\Delta }}{1-e^{-2\Delta }} - \frac{\log\bb{1-e^{-2\Delta}}}{2\Delta} \\
    &\leq  \frac{e^{-\Delta }}{1-e^{-2\Delta }} + \frac{\log(\tfrac{1}{\Delta})}{2\Delta}
}



\end{proof}