\section{Conclusion}
\label{sec:conclusion}

Score-based diffusion models have been at the forefront of generative models with applications ranging from image to audio and video generation. To our knowledge, this is \textit{the first work}, which establishes (nearly) dimension-free sample complexity bounds for learning score functions across noise levels. We show that a mild assumption of time-regularity can significantly improve over previous bounds which have polynomial dependence on $d$.
%In this work, we significantly advanced the theoretical understanding of score-based diffusion models by establishing the first (nearly) dimension-free sample complexity bounds for learning score functions across noise levels. 
We achieve this with a novel martingale-based analysis with sharp variance bounds, addressing the complexities of learning from dependent data generated by multiple Markov process trajectories. Furthermore, we introduce the Bootstrapped Score Matching (BSM) method, which effectively leverages temporal information to reduce variance and enhance the learning of score functions.

While our work provides theoretical insights into the training of diffusion models, several open questions still remain. One potential direction is extending our framework to flow-matching models, which have recently gained prominence. Developing dimension-independent bounds in this setting could yield further insights. Additionally, while BSM presents a compelling framework for incorporating historical information to reduce variance, establishing formal theoretical guarantees is an open problem.

