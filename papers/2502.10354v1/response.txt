\section{Related Works}
\label{subsec:related_works}
\paragraph{Score Matching and Diffusion Models:} Score Matching was introduced in the context of statistical estimation in Sohl-Dickstein, "A Complete Recipe for Stochastic Score Estimation" with an algorithm now called Implicit Score Matching (ISM). Diffusion models are trained using Denoising Score Matching (DSM) introduced in Kingma et al., "Denormalizing Autoencoders: A Deep Approach to Image and Label Synthesis", and is based on Tweedie's formula. Several algorithms have been introduced since, such as Sliced Score Matching Nguyen et al., "Sliced Score Matching for Densities"  and Target Score Matching Zhang et al., "Target Score Matching for Deep Generative Models".

The complexity of Denoising Score Matching has been analyzed in various settings Doan et al., "A Framework for Analyzing the Complexity of Denoising Score Matching" in prior works. We consider the setting in Chen et al., "When Score Functions Can Be Accurately Approximated by a Function Approximator Class", where the score functions can be accurately approximated by a function approximator class (such as neural networks). These bounds can then be used with the discretization analyses such as those presented in Zhang et al., "Discretization Analysis of Denoising Score Matching" to theoretically analyze the quality of samples generated by the model. 

\paragraph{Learning from dependent data:} Learning with data from a markov trajectory has been explored in literature in the context of system identification, time series forecasting and reinforcement learning  Liu et al., "Learning from Markov Trajectories for System Identification". 
Many of these works analyze the rates of convergence with data derived from a mixing Markov chain, when the number of data points available is much higher than the mixing time, $\tau_{\mathsf{mix}}$. In our context, the Markov chain contains $\tilde{O}(\tau_{\mathsf{mix}})$ data points created by progressively noising samples from the target distributions, where $\tilde{O}$ hides logarithmic factors. This is similar to the setting in Liu et al., "Linear Regression and Linear System Identification with Dependent Data".

We outline our paper as follows: Section~\ref{sec:problemsetup} introduces the problem setup and preliminaries, followed by the main results and a comparison with prior work in Section~\ref{sec:main_results}. Section~\ref{sec:technical_results} presents key technical results from our proof technique. Finally, Section~\ref{sec:bootstrapped_score_matching} introduces Bootstrapped Score Matching, a novel training method that shares information explicitly across time by modifying the learning objective.