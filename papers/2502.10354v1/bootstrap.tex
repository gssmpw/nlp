\vspace{-5pt}
\section{Bootstrapped Score Matching}
\label{sec:bootstrapped_score_matching}

In Section~\ref{sec:technical_results},  we used time regularity and could prove nearly $d$-independent bounds. Learning with the same function class across timesteps, along with time regularity of the function class (Assumption~\ref{assumption:score_function_smoothness}) was critical to our proof.

In this section, we ask whether it is possible to exploit the dependence across timesteps explicitly and reduce variance in estimation. Using the Markovian nature of \eqref{eq:fwd_noise}, we show that for any $t' < t$ and $\alpha_t \in \R$, $s\bb{t, x_t} = \E[\tilde{y}_{t}|x_t]$ for $\tilde{y}_{t} := -\frac{z_t}{\sigma_t^{2}} - \alpha_t(s\bb{t', x_{t'}} - \frac{-z_{t'}}{\sigma_{t'}^{2}})$. This shows that $\tilde{y}_{t}$ can also be used to construct a learning target for the score function. This is in contrast to the target $y_{t} := -\frac{z_t}{\sigma_t^{2}}$ used in \eqref{eq:dsm_total}. The advantage of $\tilde{y}_{t}$ over $y_t$ is in the lower variance of $\tilde{y}_{t}$, as shown in Lemma~\ref{lemma:bootstrap_properties} (proved in Lemmas~\ref{lemma:bootstrap_consistency_appendix}, \ref{lemma:bootstrap_variance}). 

\begin{lemma}[Bootstrap Properties]\label{lemma:bootstrap_properties} Let $\tilde{r}_{t} := \tilde{y}_{t}-s(t, x_t)$. For $t' < t$, let $\Delta := t-t'$ and $\alpha_t := \frac{e^{-\Delta}\sigma_{t'}^{2}}{\sigma_{t}^{2}}$. Then, under Assumption~\ref{assumption:score_function_smoothness}, we have
\bas{
   \E\bbb{\tilde{r}_{t}|x_{t}} = 0 \text{ and } \normop{\E[\tilde{r}_{t}\tilde{r}_{t}^{\top}|x_{t}]}=O\bb{\frac{(L^{2}+1)\Delta}{\sigma_{t}^{4}}}
}
\end{lemma}

To compare with $y_{t} = \frac{-z_t}{\sigma_{t}^{2}}$, we note that an application of the second order tweedie's formula along with Assumption~\ref{assumption:score_function_smoothness} shows the variance $\normop{\E[(y_t-s(t, x_t))(y_t-s(t, x_t))^{\top}|x_{t}]}$ to be of the order $O(\frac{L+1}{\sigma_{t}^{2}})$. Therefore, although both $y_{t}$ and $\tilde{y}_{t}$ are unbiased, the variance of $\tilde{y}_{t}$ has an additional step size ($\Delta$) factor in the numerator (see Lemma~\ref{lemma:bootstrap_properties}) 

Intuitively, this is due to the correlation between $z_{t}, z_{t
'}$ induced by the SDE \eqref{eq:fwd_noise} which removes a lot of extraneous noise, reducing the variance significantly. Recent work due to \cite{de2024target} also presents a similar idea. They show the related result $ s\bb{t, x_t} = e^{t-t'}\E\bbb{s\bb{t', x_t'}|x_t}$, which further offer a lower variance estimator of $s_t$, provided $t-t'$ is small. However, the focus of our approach is significantly different compared to theirs. They focus on monte-carlo sampling assuming access to the true initial score function, $s\bb{t_{0}, x} := \nabla\log\bb{p_{0}\bb{x}}$. In contrast, we show how to use these low variance estimates for efficient training of diffusion models. For simplicity, we present the details of the algorithm assuming a different function class, $\cF_{k}$ for each timestep $t_{k}$, but our ideas extend naturally to jointly learning across all timesteps with a shared function class as well, as described in \eqref{eq:dsm_total}.





The primary challenge with this approach is that in case of diffusion models, we do not have access to the true score function $s(t',.)$ for $t' < t$. Instead as we move along the trajectory, we learn score estimates $\hat{s}_{t}$. Therefore, we plug in $\hat{s}_{t'}$ in $\tilde{y}_{t}$ instead of the true score function, $s(t',.)$. This in-turn induces a bias at the cost of a reduced variance, which we trade-off using the parameter, $\alpha_t$, to achieve a better $\ell_2$-error of the score estimate. Our Algorithm, referred to as Bootstrapped Score Matching (BSM) captures this idea and is described in detail in the next paragraph. 

\begin{figure*}[hbt]
    \centering
    % First image
    \begin{minipage}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/Gaussian_experiment_bsm.png}
        \vspace{1mm} % Optional spacing
        (a) L2 error for a multivariate Gaussian density
        \label{fig:gaussian_experiment_bsm}
    \end{minipage}
    \hspace{0.05\textwidth} % Horizontal spacing between the two images
    % Second image
    \begin{minipage}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/GMM_experiment_bsm.png}
        \vspace{1mm} % Optional spacing
        (b) Empirical density for a mixture of Gaussians
        \label{fig:gmm_experiment_bsm}
    \end{minipage}
    \caption{\label{fig:bsm_experiments} Experiments with Bootstrapped Score Matching. (a) represents the L2 error at each timestep while performing score estimation for a multivariate Gaussian density. In this case, since the score function is linear, \eqref{eq:dsm_total} can be solved exactly without a neural network. We note that BSM significantly enhances the quality of the score function. (b) explores multimodal densities, specifically a mixture of Gaussians. Here, we use a 3-layer neural network to represent the score function and plot the empirical density learned by using \eqref{eq:reverse_sde} with different score estimation algorithms. We note that using score bootstrapping significantly enhances the proportional representation of the minor mode, leading to a fair output. We provide details of the experimental setup in the Appendix Section~\ref{appendix:bsm}.}
\end{figure*} 

The BSM algorithm operates sequentially over a discretized time horizon $0 = t_0 < t_1 < \cdots < t_N = T$ and builds upon the principles of DSM while introducing a novel bootstrapping mechanism to mitigate the increasing variance of the DSM loss in later timesteps. Given a dataset $D = \{x_0^{(i)}\}_{i \in [m]}$ sampled from the data distribution, the perturbed samples at timestep $t_k$ are generated as $x_{t_k}^{(i)} = x_{0}^{(i)} e^{-t_k} + z_{t_k}^{(i)}, \quad z_{t_k}^{(i)} \sim \mathcal{N}(0, \sigma_{t_k}^2 I)$ where $\sigma_{t_k}^2 = 1 - e^{-2t_k}$. The task at each timestep $t_k$ is to estimate an approximate score function $\hat{s}_{t_k}(x)$ to optimize $\mathbb{E}_{x_{t_k}}[\|s(t_k, x) - \hat{s}_{t_k}(x)\|_2^2]$. For the initial timesteps $t_k$ with $k \leq k_0$, the algorithm employs DSM. The score function $\hat{s}_{t_k}$ is obtained by solving:
\[
\hat{s}_{t_k} = \arg\min_{f \in \cF_k}  \sum_{i \in [m]} \frac{\norm{f(t_k, x_{t_k}^{(i)}) - \frac{-z_{t_k}^{(i)}}{\sigma_{t_k}^2}}_2^2}{m},
\] 
For later timesteps $t_k$ with $k > k_0$, the algorithm transitions to BSM. At each timestep, the algorithm constructs bootstrapped targets $\tilde{y}_{t_k}^{(i)}$ by combining the DSM target $\frac{-z_{t_k}^{(i)}}{\sigma_{t_k}^2}$ with the previously estimated score $\hat{s}_{t_{k-1}}$. Specifically, the targets are defined as:
\bas{
\tilde{y}_{t_k}^{(i)} &= (1 - \alpha_k) \underbrace{\frac{-z_{t_k}^{(i)}}{\sigma_{t_k}^2}}_{\text{Unbiased Target}} + \alpha_k \left( \underbrace{\frac{-z_{t_k}^{(i)}}{\sigma_{t_k}^2} + \left(\hat{s}_{t_{k-1}}( x_{t_{k-1}}^{(i)}) - \frac{-z_{t_{k-1}}^{(i)}}{\sigma_{t_{k-1}}^2} \right)}_{\text{Biased Target}} \right)
}
where $\alpha_k = e^{-\gamma_k} \sqrt{\frac{1 - e^{-2t_{k-1}}}{1 - e^{-2t_k}}}$, with $\gamma_k = t_k - t_{k-1}$. Given access to the true score function, $s(t_{k-1}, .)$, then $\tilde{y}_{t_k}^{(i)}$ would form an unbiased target with lower variance, as shown in Lemma~\ref{lemma:bootstrap_properties}. However, since we only have access to the estimated score function, $\hat{s}_{t_{k-1}}$ at the previous timestep, $\tilde{y}_{t_k}^{(i)}$ is a biased target, and the parameter $\alpha_{k}$ weighs between the biased and unbiased targets. The score function, $\hat{s}_{t_k}$, is then learned as:
\bas{
    \hat{s}_{t_k} \leftarrow \arg\min_{f \in \cF_{k}}\sum_{i\in[m]}\frac{\norm{f(t_k, x_{t_k}^{(i)}) - \tilde{y}_{t_k}^{(i)}}_{2}^{2}}{m}
}
Figure~\ref{fig:bsm_experiments} presents numerical experiments that show the empirical advantage of our proposed score-bootstrap procedure. The formal pseudocode is provided in Algorithm~\ref{alg:bsm_algorithm} in Appendix Section~\ref{appendix:bsm}.
% \syamantak{Remove debiasing.}
% Next, to control the bias effectively to avoid it's propagation across timesteps, we propose a final debiasing step. To this end, we define a confidence set, $\mathcal{G}_{k}$, of feasible solutions which are not too far away from $\tilde{f}_{t_k}$, \bas{\mathcal{G}_{k} := \{f \in \cF_{k} | \frac{\sum_{i \in [m]}\norm{\tilde{f}(t_k, x_{t_k}^{(i)})-f(t_k, x_{t_k}^{(i)})}_{2}^{2}}{m}  \leq \frac{\nu_k^{2}d\log\left(|\mathcal{\cF}|m/\delta\right)}{m}\}}
% The score function $\hat{s}_{t_k}$ is  learned by minimizing the mean squared error between $\tilde{y}_{t_k}^{(i)}$ and the model predictions, subject to $\mathcal{G}_k$, as:
% \bas{
% \hat{s}_{t_k} &= \arg\min_{f \in \mathcal{G}_k} \frac{1}{m} \sum_{i \in [m]} \|f(t_k, x_{t_k}^{(i)}) - \tilde{y}_{t_k}^{(i)}\|_2^2 \\
% &\;\;\;\;\;\;\;\;\;\;\;\;  +  \frac{2\langle f(t_k, x_{t_k}^{(i)}) - \tilde{f}(t_k, x_{t_k}^{(i)}), \tilde{f}(t_k, x_{t_k}^{(i)}) - y_{t_k}^{(i)} \rangle}{m}.
% }
% The debiasing step uses the unbiased target $y_{t_k}^{(i)} := \frac{-z_{t_k}^{(i)}}{\sigma_{t_k}^{2}}$ to reduce the bias of $\tilde{f}_{t_k}$, while also ensuring the quality of the estimate by restricting the solution space to $\mathcal{G}_k$. 

