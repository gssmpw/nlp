
% \renewcommand{\thetheorem}{A.\arabic{theorem}}
% \renewcommand{\thesection}{A.\arabic{section}}
% \renewcommand{\thelemma}{A.\arabic{lemma}}
% \renewcommand{\thecorollary}{A.\arabic{corollary}}
% \renewcommand{\thefigure}{A.\arabic{figure}}
% \renewcommand{\theequation}{A.\arabic{equation}}

% \setcounter{theorem}{0}
% \setcounter{section}{0}
% \setcounter{figure}{0}
% \setcounter{lemma}{0}
% \setcounter{corollary}{0}
% \setcounter{proposition}{0}

% \section{Appendix}
\appendix


The Appendix is organized as follows:
\begin{enumerate}
    \item Section~\ref{appendix:utility_results} provides some utility results which will be useful in subsequent proofs.
    \item Section~\ref{appendix:variance_calculation} provides variance calculation for the martingale decomposition.
    \item Section~\ref{appendix:martingale_concentration} analyzes concentration properties for  martingales with bounded variance and subGaussianity, which may be of independent interest.
    \item Section~\ref{appendix:convergence_erm} analyzes convergence of the empirical squared error by providing the martingale decomposition and exploiting the results developed in Sections~\ref{appendix:variance_calculation} and \ref{appendix:martingale_concentration}.
    \item Section~\ref{appendix:generalization_error_bounds} provides generalization bounds to achieve guarantees for the expected squared error.
\end{enumerate}

\input{appendix_utility}
\input{appendix_martingale_concentration}



\section{Martingale Decomposition and Variance Calculation}
\label{appendix:variance_calculation}
In this section, we will consider the quantity similar to $H^f$ in Lemma~\ref{lemma:l2errorboundmartingale}, decompose it into a sum of martingale difference sequence, and then bounds its variance using the Tweedie's formula. In this section, assume that we are given $\zeta : \bR^+ \times \bR^d \to \bR^d$ and consider the quantity:

\bas{
    H := \sum_{t \in \timeset,i \in [m]}\frac{\gamma_{t}}{\sigma_{t}^{2}}\langle \zeta(t,x_{t}^{(i)}),z_{t}^{(i)}-\E [z_{t}^{(i)}|x_{t}^{(i)}]\rangle
}
We suppose that $\zeta(t,x_t^{(i)})$ has a finite second moment. Where $\gamma_{t} > 0$ is some sequence. When $\zeta = \frac{s-f}{m}$, this yields us $H^f$ as we show in Lemma~\ref{lemma:martingale_decomposition_1}. We define the sigma algebras: $\sigma$-algebra $\mathcal{F}_j = \sigma(x^{(i)}_{t}: 1\leq i\leq m, t \geq t_{N-j+1})$ for $j \in [N]$ and $\mathcal{F}_0$ is the trivial $\sigma$-algebra. We want to filter $H$ through the filtration $\mathcal{F}_j$ to obtain a martingale decomposition. To this end, define:

\begin{equation}
 H_{j} := \E\bbb{H|\mathcal{F}_{j}} \,; j \in \{0,\dots,N\} 
\end{equation}


\begin{lemma}
\label{lemma:martingale_conditioning}

\begin{enumerate}
    \item 
    If $t \leq t_{N-j+1}$, then $$\E[ \langle \zeta(t,x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|\mathcal{F}_j] = 0$$
    \item If $t > t_{N-j+1}$, then
    $$\E[ \langle \zeta(t,x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|\mathcal{F}_j] =  e^{-t}\langle \zeta(t,x_t^{(i)}), \E[x_0^{(i)}|x_t^{(i)}] - \E[x_0^{(i)}|x_{t_{N-j+1}}^{(i)}] \rangle $$
\end{enumerate}
\end{lemma}
\begin{proof}
\begin{enumerate}
    \item Using the fact that $x_t^{(i)}$ forms a Markov process and that $(x_s^{(i)})_{s\geq 0},(x_s^{(j)})_{s \geq 0}$ are independent when $i \neq j$, we have via the Markov property:
\begin{align}&\E[ \langle \zeta(t,x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|\mathcal{F}_j] = \E[ \langle \zeta(t,x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|x^{(i)}_{t_{N-j+1}}] \nonumber \\
&= \E\left[\E[ \langle \zeta(t,x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|x_t^{(i)},x^{(i)}_{t_{N-j+1}}]\bigr|x^{(i)}_{t_{N-j+1}}\right]
\end{align} 

In the second step, we have used the tower property of the conditional expectation. Now, $z_t^{(i)} = x_t^{(i)}-e^{-t}x_0^{(i)}$. By the Markov Property, we have: $\E[x_0^{(i)}|x_t^{(i)},x_{t_{j-N+1}}^{(i)}] = \E[x_0^{(i)}|x_t^{(i)}]$. Plugging this in, we have: 
\begin{align}\E[ \langle \zeta(t,x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|\mathcal{F}_j] 
&= \E\left[\E[ \langle \zeta(t,x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|x_t^{(i)}]\bigr|x^{(i)}_{t_{N-j+1}}\right] \nonumber \\
&= 0
\end{align} 

    \item Notice that $z_t^{(i)} = x_t^{(i)}-e^{-t}x_0^{(i)}$. Clearly, $x_t^{(i)}$ is measurable with respect to $\mathcal{F}_j$. Therefore, 
$$\E[ \langle \zeta(t,x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|\mathcal{F}_j] = - e^{-t}\langle \zeta(t,x_t^{(i)}), \E[x_0^{(i)}|\mathcal{F}_j] - \E[x_0^{(i)}|x_t^{(i)}] \rangle $$
    Now, consider the fact that $x_0^{(i)},x_{t_1}^{(i)},...$ is a Markov chain. Therefore, the Markov property states that $x_0^{(i)}|x_{s}^{(i)}: s\geq \tau $ has the same law as $x_0^{(i)}| x_{\tau}^{(i)}$. Therefore, we must have: $\E[x_0^{(i)}|\mathcal{F}_j] = \E[x_0^{(i)}|x^{(i)}_{t_{j-N+1}}]$. Plugging this into the display equation above, we conclude the result.
\end{enumerate}
\end{proof}



We connect the quantity $H$ defined above to the quantity $H^{f}$ related to the excess risk.

\begin{lemma}\label{lemma:martingale_decomposition_1}
    Let $y_{t}^{(i)} := -\frac{z_t^{(i)}}{\sigma_t^2}$, $f\in \cF$ and
    \bas{
    H^{f} := \sum_{i \in [m], j \in [N]}\frac{\gamma_{j}\inner{f\bb{t_j,x_{t_j}^{(i)}}-s\bb{t_j,x_{t_j}^{(i)}}}{y_{t_j}^{(i)} -s\bb{t_j,x_{t_j}^{(i)}}}}{m}
    } 
    Suppose we pick $\zeta = \frac{s-f}{m}$ in the definition of $H$. Then, 
    \bas{
         H^{f} = \bb{H -H_{N}} + \sum_{k=2}^{N}\bb{H_k - H_{k-1}}
    }
    such that 
    \bas{
        H-H_N &= \sum_{i=1}^{m}\sum_{j=1}^N \frac{e^{-(t_j-t_1)}\gamma_{j}}{\sigma_{t_j}^{2}}\langle \zeta(t_j,x_{t_j}^{(i)}),z^{(i)}_{t_1}-\E[z^{(i)}_{t_1}|x^{(i)}_{t_1}]\rangle \\
        H_k - H_{k-1} &= \sum_{i=1}^{m}\sum_{j=N-k+2}^N \frac{e^{-t_{j}}\gamma_{j}}{\sigma_{t_j}^{2}}\langle \zeta(t_j,x_{t_j}^{(i)}),\E[x_0^{(i)}|x_{t_{N-k+2}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]\rangle
    }


\end{lemma}
\begin{proof}
By Tweedie's formula, notice that $y_t^{i}- s(t,x_t^{(i)}) = \frac{\E [z_t^{(i)}|x_t^{(i)}]-z_t^{(i)}}{\sigma_t^2}$. This shows us that $H^f = H$ when we pick $\zeta = \frac{s-f}{m}$. The proof follows due to Lemma~\ref{lemma:martingale_conditioning} once we note that $H_1 = 0$ almost surely
\end{proof}

%\errormartingaledecomposition*

% \begin{lemma}\label{lemma:error_martingale_decomposition_2_actual_appendix_old}

% Define $\bar{G}_i := \sum_{j=1}^{N} \tfrac{\gamma_{j}e^{-(t_j-t_1)}\zeta\bb{t_j,x_{t_j}^{(i)}}}{\sigma_{t_j}^{2}}$, $G_{i,k} := \sum_{j = N-k+2}^{N} \tfrac{\gamma_{j}e^{-t_j}\zeta\bb{t_j,x_{t_j}^{(i)}}}{\sigma_{t_j}^{2}}$ and $R_{i,k}$ as  
%     \begin{equation}
%     R_{i,k} = \begin{cases} \bigr \langle G_{i,k+1},\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]\bigr \rangle \text{ for }  k \in \{1,\dots,N-1\}, \\ 
%      \bigr\langle \bar{G}_i , z_{t_1}^{(i)}-\E\bbb{z_{t_1}^{(i)}|x_{t_1}^{(i)}}\bigr\rangle \text{ for } k = N \end{cases} 
%     \end{equation}
%     Then, $H = \sum_{i \in [m]}\sum_{k \in [N]}R_{i,k}$. Furthermore, consider the filtration defined by the sequence of $\sigma$-algebras,  $\mathcal{F}_{i,k} := \sigma\{x_{t}^{j} : j \in [i], t \geq t_{N-k}\}$ for $i \in [m]$ and $k < N$, satisfying the total ordering $\left\{\bb{i_1, j_1} < \bb{i_2, j_2} \text{ iff } i_1 < i_2 \text{ or } i_1 = i_2, j_1 < j_2\right\}$. Then for $i \in [m], k \in [N]$,  $\left\{R_{i,k}\right\}$ forms a martingale difference sequence. 
% \end{lemma}

% \begin{proof}
%     We first note that for $1 < k \leq N-1$, $\mathcal{F}_{i, k-1} := \sigma\left\{x_{t}^{j} : j \leq i, t \geq t_{N-k+1}\right\}$.  Therefore, $G_{i,k+1}$ is measurable with respect to $\mathcal{F}_{i, k-1}$. Furthermore, if $k=N$, then $\bar{G}_{i}$ is measurable with respect to $\mathcal{F}_{i, k-1}$. Then, 
%     \bas{
%         \E\bbb{R_{i,k}|\mathcal{F}_{i,k-1}} &= \begin{cases}
%             \inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E\bbb{\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]|\mathcal{F}_{i, k-1}} } = 0, k \in [1,N-1], \\ \\
%             \inner{\bar{G}_i}{\E\bbb{z_{t_1}^{(i)}|\mathcal{F}_{i, k-1}}-\E\bbb{z_{t_1}^{(i)}|x_{t_1}^{(i)}}} = 0, k = N 
%         \end{cases}
%     }
%     Finally, if $k = 1$, $\mathcal{F}_{i-1, N-1} = \sigma\left\{x_{t}^{j}: j \leq i-1, t \geq t_1 \right\}$. Therefore, $R_{i,k}$ is independent of $\mathcal{F}_{i-1, N-1}$ and $\E\bbb{R_{i,k}|\mathcal{F}_{i-1, N-1}} = 0$.
% \end{proof}


\begin{lemma}\label{lemma:error_martingale_decomposition_2_actual_appendix}

Define $\bar{G}_i := \sum_{j=1}^{N} \tfrac{\gamma_{j}e^{-(t_j-t_1)}\zeta\bb{t_j,x_{t_j}^{(i)}}}{\sigma_{t_j}^{2}}$, $G_{i,k} := \sum_{j = N-k+2}^{N} \tfrac{\gamma_{j}e^{-t_j}\zeta\bb{t_j,x_{t_j}^{(i)}}}{\sigma_{t_j}^{2}}$ and $R_{i,k}$ as  
    \begin{equation}
    R_{i,k} = \begin{cases} 0 &\text{ for } k = 0\\ \bigr \langle G_{i,k+1},\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]\bigr \rangle &\text{ for }  k \in \{1,\dots,N-1\}, \\ 
     \bigr\langle \bar{G}_i , z_{t_1}^{(i)}-\E\bbb{z_{t_1}^{(i)}|x_{t_1}^{(i)}}\bigr\rangle &\text{ for } k = N \end{cases} 
    \end{equation}
    Let $t_0 = 0$. Consider the filtration defined by the sequence of $\sigma$-algebras,  $\mathcal{F}_{i,k} := \sigma(\{x_{t}^{(j)} : 1\leq j < i, t \in \timeset\}\cup \{x_{t}^{(i)} : t \geq t_{N-k}\})$ for $i \in [m]$ and $k \in \{0,\dots, N\}$, satisfying the total ordering $\left\{\bb{i_1, j_1} < \bb{i_2, j_2} \text{ iff } i_1 < i_2 \text{ or } i_1 = i_2, j_1 < j_2\right\}$. Then 
    \begin{enumerate}
        \item For $k \in [N-1]$, $G_{i,k+1}$ is measurable with respect to $\mathcal{F}_{i,k-1}$ and $\bar{G}_{i}$ if $\mathcal{F}_{N-1}$ measurable.
        \item For $i \in [m], k \in \{0\}\cup[N]$,  $(R_{i,k})_{i,k}$ forms a martingale difference sequence with respect to the filtration above. 
        \item $H = \sum_{i \in [m]}\sum_{k \in [N]}R_{i,k}\,.$ 
    \end{enumerate}
    

    
\end{lemma}
\begin{proof} 
\begin{enumerate}
\item 
    We first note that for $1 \leq k \leq N-1$, $  \sigma\left\{x_{t}^{i} :  t \geq t_{N-k+1}\right\} \subseteq \mathcal{F}_{i, k-1}$.  Therefore, $G_{i,k+1}$ is measurable with respect to $\mathcal{F}_{i, k-1}$. Furthermore, if $k=N$, then $\bar{G}_{i}$ is measurable with respect to $\mathcal{F}_{i, k-1}$. 
    
\item First note that $R_{i,k}$ is $\mathcal{F}_{i,k}$ measurable.
        \bas{
        \E\bbb{R_{i,k}|\mathcal{F}_{i,k-1}} &= \begin{cases}
            \inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E\bbb{\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]|\mathcal{F}_{i, k-1}} } = 0, &\text{ when } k \in [N-1], \\ \\
            \inner{\bar{G}_i}{\E\bbb{z_{t_1}^{(i)}|\mathcal{F}_{i, k-1}}-\E\bbb{z_{t_1}^{(i)}|x_{t_1}^{(i)}}} = 0, &\text{ when } k = N 
        \end{cases}
    }
The case of $R_{i,0}$ is straightforward. 

\item This follows from Lemma~\ref{lemma:martingale_decomposition_1}.
\end{enumerate}
\end{proof}



\begin{lemma}\label{lemma:variance_bound_1}
Consider the setting of Lemma~\ref{lemma:error_martingale_decomposition_2_actual_appendix}. Define:
\begin{equation}
    V_{i,k} = \begin{cases} 0 &\text{ if } k = 0 \\
    \E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}] &\text{ if } k \in \{1,\dots,N-1\} \\
    z_{t_1}^{(i)}-\E\bbb{z_{t_1}^{(i)}|x_{t_1}^{(i)}} &\text{ if } k = N 
    \end{cases}
\end{equation}
Let $\Sigma_{i,k} := \E[V_{i,k}V_{i,k}^{\top}|\mathcal{F}_{i,k-1}]$. Then, we have:

\begin{equation}
    \E\bbb{R_{i,k}^{2}|\mathcal{F}_{i,k-1}} = \begin{cases} 0 &\text{ if } k = 0 \\
    G_{i,k+1}^{\top}\Sigma_{i,k}G_{i,k+1}&\text{ if } k \in \{1,\dots,N-1\} \\
    \bar{G}_i^{\top} \Sigma_{i,k}\bar{G}_i &\text{ if } k = N 
    \end{cases}
\end{equation}

\end{lemma}
\begin{proof}

This follows from a straightforward application of Lemma~\ref{lemma:error_martingale_decomposition_2_actual_appendix}.
\end{proof}


Let $U$ be any random vector over $\R^d$ independent of $V \sim \mathcal{N}(0,\sigma^2\mathbf{I}_d)$. Let $W = U+V$ and let $p$ be the density of $W$, $s = \nabla \log p$ and $h = \nabla^2 \log p$. Then, second order Tweedie's formula states (Theorem 1,\cite{meng2021estimating}):
$$ \E[VV^{\intercal}|W] = \sigma^4 h(W) + \sigma^4 s(W)s^{\top}(W) + \sigma^2 \mathbb{I}_d\,.$$
\begin{lemma}\label{lemma:second_order_tweedie_application} Let $s_\tau:\R^d \to \R^d$ be continuously differentiable for every $\tau > 0$. Let $t' < t$ and $x_{t} = e^{-\bb{t-t'}}x_{t'} + z_{t,t'}$ where $z_{t,t'} \sim \mathcal{N}\bb{0, \sigma_{t-t'}^{2}\id_{d}}$, as defined in Section~\ref{sec:problemsetup}. Then, 
    \bas{
        \E\bbb{z_{t,t'}z_{t,t'}^{\top}|x_{t}} &= \sigma_{t-t'}^{4}h_{t}\bb{x_{t}} + \sigma_{t-t'}^{4}s\bb{t, x_t}s\bb{t, x_t}^{\top} + \sigma_{t-t'}^{2}\id_{d} \\
        \E\bbb{s\bb{t', x_{t'}}s\bb{t', x_{t'}}^{\top}|x_{t}} &=  e^{2(t'-t)} s(t, x_t)s(t, x_t)^{\top} + e^{2(t'-t)}h_{t}\bb{x_{t}} -\E[h_{t'}\bb{x_{t'}}|x_t]
    }
    where $h_{t}\bb{x_{t}} := \nabla^{2}\log\bb{p_{t}\bb{x_t}}$.
\end{lemma}
\begin{proof}
Applying second order Tweedie's formula:
    \ba{
         &\E\bbb{z_{t,t'}z_{t,t'}^{\top}|x_{t}} = \sigma_{t-t'}^{4}h_{t}\bb{x_{t}} + \sigma_{t-t'}^{4}s\bb{t, x_t}s\bb{t, x_t}^{\top} + \sigma_{t-t'}^{2}\id_{d} \label{eq:z_t_tprime_cov}, \text{ and }, \\
         &\E[z_{t'}z_{t'}^{\top}|x_{t'}] - \sigma_{t'}^4 s(t', x_{t'})s^{\top}(t', x_{t'}) = \sigma^2_{t'} \id + \sigma^4_{t'}h_{t'}(x_{t'}) \label{eq:second_order_tweedie_1}
    }
    
    By Markov property, we must have for any measurable function $g$:
    
    $$\E[g(z_{t'})|x_t] = \E[\E[g(z_{t'})|x_t,x_{t'}]|x_{t}] = \E[\E[g(z_{t'})|x_{t'}]|x_{t}] $$
    
    Applying this to \eqref{eq:second_order_tweedie_1}:
    
    \begin{equation}\label{eq:markov_tweedie}
    \sigma_{t'}^4 \E[s(t', x_{t'})s^{\top}(t', x_{t'})|x_t] = \E[z_{t'}z_{t'}^{\top}|x_{t}] - \sigma_{t'}^2 \id - \sigma^4_{t'}\E[h_{t'}(x_{t'})|x_t]\end{equation}
    
    Now, note that $x_t = e^{-t}x_0 + e^{t'-t}z_{t'} + z_{t,t'}$. Taking $y_0 = e^{-t}x_0 + z_{t,t'}$, we have: $x_t = y_0 + e^{t'-t}z_{t'}$. Therefore, applying the second order Tweedie's formula again, we must have:
    
    $$e^{2(t'-t)}\E[z_{t'}z_{t'}^{\top}|x_{t}] = e^{4(t'-t)}\sigma_{t'}^{4} s(t, x_t)s(t, x_t)^{\top} + e^{4(t'-t)}\sigma_{t'}^{4}h_{t}(x_t) + e^{2(t'-t)}\sigma_{t'}^{2}\id$$
    
    That is : $\E[z_{t'}z_{t'}^{\top}|x_{t}] = e^{2(t'-t)}\sigma_{t'}^{4} s(t, x_t)s(t, x_t)^{\top} + e^{2(t'-t)}\sigma_{t'}^{4}h_{t}(x_t) + \sigma_{t'}^{2}\id$.
    Substituting this in Equation~\eqref{eq:markov_tweedie}, we have:
    
    $$  \E[s(t', x_{t'})s^{\top}(t', x_{t'})|x_t] =   e^{2(t'-t)} s(t, x_t)s(t, x_t)^{\top} + e^{2(t'-t)}h_{t}(x_t) -\E[h_{t'}(x_{t'})|x_t]$$
\end{proof}

\begin{lemma}\label{lemma:conditional_hessian}
Let $s_\tau:\R^d \to \R^d$ be continuously differentiable for every $\tau > 0$. For $t > t' > 0$, let $v_{t,t'} := \E\bbb{x_{0}|x_{t}} - \E\bbb{x_{0}|x_{t'}}$, then, 
\bas{
    & \E\bbb{v_{t,t'}v_{t,t'}^{\top}|x_t} \preceq \\
    & \;\;\;\; 2e^{2t}\bb{\sigma_{t-t'}^{4}h_{t}\bb{x_{t}} + \sigma_{t-t'}^{2}\id_{d}} + 2e^{2t'}\sigma_{t'}^{4}\E\bbb{\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}^{\top}|x_{t}}
}
where $h_{t}\bb{x_t} := \nabla^{2}\log\bb{p_{t}\bb{x_t}}$ is the hessian of the log-density function.
\end{lemma}
\begin{proof}
    Using Tweedie's formula, for all $t > 0$,
    \bas{
        \E\bbb{x_{0}|x_{t}} &= \E\bbb{e^{t}\bb{x_{t}-z_{t}}|x_{t}} = e^{t}x_{t} + e^{t}\E\bbb{-z_{t}|x_{t}} = e^{t}\bb{x_{t} + \sigma_{t}^{2}s\bb{t, x_{t}}}
    }
    
    Note that $x_{t'} = e^{t-t'}\bb{x_t - z_{t,t'}}$. Furthermore, note from Tweedie's formula and Corollary 2.4 \cite{de2024target} that:
    \bas{
        \E\bbb{z_{t,t'}|x_{t}} = -\sigma_{t-t'}^{2}s\bb{t, x_t}, \;\; \E\bbb{s\bb{t', x_{t'}}|x_t} = e^{-\bb{t-t'}}s\bb{t, x_t}
    }
    Therefore, we have 
    \bas{
        v_{t,t'} &= e^{t}\bb{z_{t,t'} + \sigma_{t-t'}^{2}s\bb{t, x_t}} - e^{t'}\sigma_{t'}^{2}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}
    }
    Then, using Lemma~\ref{lemma:second_order_tweedie_application} and the fact that $(a+b)(a+b)^{\top}\preceq 2aa^{\top} + 2bb^{\top}$:
    \bas{
        & \E\bbb{v_{t,t'}v_{t,t'}^{\top}|x_t} \\
        &\preceq 2e^{2t}\E\bbb{\bb{z_{t,t'} + \sigma_{t-t'}^{2}s\bb{t, x_t}}\bb{z_{t,t'} + \sigma_{t-t'}^{2}s\bb{t, x_t}}^{\top}|x_t}  \\
        & \;\;\;\; + 2e^{2t'}\sigma_{t'}^{4}\E\bbb{\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}^{\top}|x_{t}} \\
        &= 2e^{2t}\bb{\sigma_{t-t'}^{4}h_{t}\bb{x_{t}} + \sigma_{t-t'}^{2}\id_{d}} \\
        & \;\;\;\; + 2e^{2t'}\sigma_{t'}^{4}\E\bbb{\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}^{\top}|x_{t}}
        % & = 2e^{2t}\bb{\E\bbb{z_{t,t'}z_{t,t'}^{\top}|x_{t}} - \sigma_{t-t'}^{4}s\bb{t, x_t}s\bb{t, x_t}^{\top}} - e^{2t'}\sigma_{t'}^{4}\bb{\E\bbb{s\bb{t', x_{t'}}s\bb{t', x_{t'}}^{\top}|x_{t}} - e^{-2\bb{t-t'}}s\bb{t, x_t}s\bb{t, x_t}^{\top}} \\
        % &= e^{2t}\bb{\sigma_{t-t'}^{4}h_{t}\bb{x_{t}} + \sigma_{t-t'}^{2}\id_{d}} - e^{2t'}\sigma_{t'}^{4}\bb{e^{2(t'-t)}h_{t}\bb{x_{t}} -\E[h_{t'}\bb{x_{t'}}|x_t]} \\
        % &= 
    }
    
    % Then, 
    % \ba{
    %     \E\bbb{v_{t,t'}v_{t,t'}^{\top}|x_{t}} &= e^{2t'}\underbrace{\E\bbb{\bb{x_{t'} + \sigma_{t'}^{2}s\bb{t', x_{t'}}}\bb{x_{t'} + \sigma_{t'}^{2}s\bb{t', x_{t'}}}^{\top}|x_{t}}}_{:= g_{t,t'}} - e^{2t}\bb{x_{t} + \sigma_{t}^{2}s\bb{t, x_{t}}}\bb{x_{t} + \sigma_{t}^{2}s\bb{t, x_{t}}}^{\top} \label{eq:v_t_t_prime_covariance}
    % }
    % Then, we have
    % \bas{
    %    g_{t,t'} &=  \E\bbb{\bb{x_{t'} + \sigma_{t'}^{2}s\bb{t', x_{t'}}}\bb{x_{t'} + \sigma_{t'}^{2}s\bb{t', x_{t'}}}^{\top}|x_{t}} \\
    %    &= \E\bbb{x_{t'}x_{t'}^{\top}|x_t} + \sigma_{t'}^{2}\E\bbb{x_{t'}s\bb{t', x_{t'}}^{\top}|x_{t}} + \sigma_{t'}^{2}\E\bbb{s\bb{t', x_{t'}}x_{t'}^{\top}|x_{t}} + \sigma_{t'}^{4}\E\bbb{s\bb{t', x_{t'}}s\bb{t', x_{t'}}^{\top}|x_{t}}
    % }

    % We have, for $h\bb{x_t} := \nabla^{2}\log\bb{\pi_{t}\bb{x_{t}}}$, 
    % \bas{
    %     e^{-2\bb{t-t'}}\E\bbb{x_{t'}x_{t'}^{\top}|x_t} &= x_{t}x_{t}^{\top} + \sigma_{t-t'}^{2}x_{t}s\bb{t, x_{t}}^{\top} + \sigma_{t-t'}^{2}s\bb{t, x_{t}}x_{t}^{\top} + \sigma_{t-t'}^{4}h\bb{x_t} + \sigma_{t-t'}^{4}s\bb{t, x_t}s\bb{t, x_t}^{\top} + \sigma_{t-t'}^{2}\id_{d}\\
    % }
    % Therefore, 
    % \bas{
    %     e^{2t'}\E\bbb{x_{t'}x_{t'}^{\top}|x_t} &= e^{2t}\bb{x_{t}x_{t}^{\top} + \sigma_{t-t'}^{2}x_{t}s\bb{t, x_{t}}^{\top} + \sigma_{t-t'}^{2}s\bb{t, x_{t}}x_{t}^{\top} + \sigma_{t-t'}^{4}h\bb{x_t} + \sigma_{t-t'}^{4}s\bb{t, x_t}s\bb{t, x_t}^{\top} + \sigma_{t-t'}^{2}\id_{d}}
    % }

    
    % Therefore, using $x_{t} = e^{-\bb{t-t'}}x_{t'} + \sigma_{t-t'}^{2}z$ for $z \sim \mathcal{N}\bb{0,\id_{d}}$, we have
    % \bas{
    %     \E\bbb{x_{0}|x_{t}} - \E\bbb{x_{0}|x_{t'}} &= \bb{e^{t}x_{t}-e^{t'}x_{t'}} + \bb{e^{t}\sigma_{t}^2 s\bb{t, x_{t}}-e^{t'}\sigma_{t'}^2 s\bb{t', x_{t'}}} \\
    %     &= e^{t}\sigma^{2}_{t-t'}z + \bb{e^{t}s\bb{t, x_{t}} - e^{t'}s\bb{t', x_{t'}}} - \bb{e^{-t}s\bb{t, x_{t}} - e^{-t'}s\bb{t', x_{t'}}}
    % }
\end{proof}

To derive an upper bound for  $\normop{\E\bbb{\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}^{\top}|x_{t}}},$
we adopt a strategy of partitioning the interval \([t', t]\) into smaller subintervals. Specifically, we divide \([t', t]\) as \(t' = \tau_0 < \tau_1 < \cdots < \tau_{B-1} < t = \tau_B\), where \(B \geq 1\). By leveraging the smoothness of the score function \(s_{\tau}(x)\) over each subinterval \([\tau_i, \tau_{i+1}]\), we express the deviations between \(s_{\tau_i}\) and \(s_{\tau_{i+1}}\) in terms of the Hessian, \(h_\tau(x) := \nabla^2 \log p_\tau(x)\). This decomposition allows us to quantify the overall deviation of the score function across the interval \([t', t]\) in terms of contributions from each subinterval, controlled by the Hessian, \(h_\tau(x)\). The following lemma formalizes this approach, establishing an upper bound for the given operator norm in terms of the Hessian and a carefully constructed decomposition. This result will serve as the foundation for subsequent analysis.


\begin{lemma}\label{lemma:conditional_decomposition}
Let $s_\tau:\R^d \to \R^d$ be continuously differentiable for every $\tau > 0$. 
Let $B \in \mathbb{N}$ and let $\tau_{0} := t' < \tau_{1} < \tau_{2} < \cdots < \tau_{B-1} < t := \tau_{B}$ for $B \geq 1$ and define $\forall t, h_{t}\bb{x_t} := \nabla^{2}\log\bb{p_{t}\bb{x_t}}$. Then, 
    \bas{
    & \normop{\E\bbb{\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}^{\top}|x_{t}}}   \\
    & \;\;\;\;\;\;\;\;\;\;\;\; \leq \normop{\E\bbb{\sum_{i=0}^{B-1}\E_{\lambda_{i}, x_{\tau_i}, \tilde{x}_{\tau,i}}\bbb{h_{\tau_i}\bb{x_{\tau_i,\lambda_i}}\bb{x_{\tau_i}-\tilde{x}_{\tau_i}}\bb{x_{\tau_i}-\tilde{x}_{\tau_i}}^{\top}h_{\tau_i}\bb{x_{\tau_i,\lambda_i}}^{\top}|x_{\tau_{i+1}}}}\bigg|x_{t} }
    }
    where  $\tilde{x}_{\tau_i}$ is an independent copy of $x_{\tau_i}$ when conditioned on $x_{\tau_{i+1}}$. $\lambda_i$ is uniformly distributed over $[0,1]$ independent of the random variables defined above and $x_{\tau_i,\lambda_i} := \lambda_{i}x_{\tau_i} + \bb{1-\lambda_i}\tilde{x}_{\tau_i}$.
\end{lemma}
\begin{proof}
    Let $\forall i \in [0,B-1], \; \Delta_{i} := \tau_{i+1}-\tau_{i}$. Then, 
    \bas{
        s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t} &= \sum_{i = 0}^{B-1}c_{i}\bb{s\bb{\tau_{i}, x_{\tau_i}}-e^{-\bb{\tau_{i+1}-\tau_{i}}}s\bb{\tau_{i+1}, x_{\tau_{i+1}}}}, \;\; c_{0} = 1, \; c_{i+1} = e^{-\bb{\tau_{i+1}-\tau_{i}}}c_{i}
    }
    Therefore, 
    \bas{
    & \normop{\E\bbb{\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}^{\top}|x_{t}}}  \\
    & = \normop{\E\bbb{\sum_{0 \leq i,j \leq B-1}c_{i}c_{j}\bb{s\bb{\tau_{i}, x_{\tau_i}}-e^{-\bb{\tau_{i+1}-\tau_{i}}}s\bb{\tau_{i+1}, x_{\tau_{i+1}}}}\bb{s\bb{\tau_{j}, x_{\tau_j}}-e^{-\bb{\tau_{j+1}-\tau_{i}}}s\bb{\tau_{j+1}, x_{\tau_{j+1}}}}^{\top}|x_{t}}}
    }
    For $i \neq j$, assuming $i < j$ WLOG, using the Markovian property,  
    \bas{
        & \E\bbb{\bb{s\bb{\tau_{i}, x_{\tau_i}}-e^{-\bb{\tau_{i+1}-\tau_{i}}}s\bb{\tau_{i+1}, x_{\tau_{i+1}}}}\bb{s\bb{\tau_{j}, x_{\tau_j}}-e^{-\bb{\tau_{j+1}-\tau_{i}}}s\bb{\tau_{j+1}, x_{\tau_{j+1}}}}^{\top}|x_{t}}  \\
        &= \E\bbb{\E\bbb{\bb{s\bb{\tau_{i}, x_{\tau_i}}-e^{-\bb{\tau_{i+1}-\tau_{i}}}s\bb{\tau_{i+1}, x_{\tau_{i+1}}}}\bb{s\bb{\tau_{j}, x_{\tau_j}}-e^{-\bb{\tau_{j+1}-\tau_{i}}}s\bb{\tau_{j+1}, x_{\tau_{j+1}}}}^{\top}|x_{\tau_{j}},x_{\tau_{j+1}}}|x_{t}} \\
        &= \E\bbb{\E\bbb{s\bb{\tau_{i}, x_{\tau_i}}-e^{-\bb{\tau_{i+1}-\tau_{i}}}s\bb{\tau_{i+1}, x_{\tau_{i+1}}}|x_{\tau_{j}},x_{\tau_{j+1}}}\bb{s\bb{\tau_{j}, x_{\tau_j}}-e^{-\bb{\tau_{j+1}-\tau_{i}}}s\bb{\tau_{j+1}, x_{\tau_{j+1}}}}^{\top}|x_{t}} \\ 
        &= \E\bbb{\E\bbb{\E\bbb{s\bb{\tau_{i}, x_{\tau_i}}-e^{-\bb{\tau_{i+1}-\tau_{i}}}s\bb{\tau_{i+1}, x_{\tau_{i+1}}}|x_{\tau_{i}}}|x_{\tau_{j}},x_{\tau_{j+1}}}\bb{s\bb{\tau_{j}, x_{\tau_j}}-e^{-\bb{\tau_{j+1}-\tau_{i}}}s\bb{\tau_{j+1}, x_{\tau_{j+1}}}}^{\top}|x_{t}} \\
        &= 0
    }
    Therefore, 
    \bas{
    & \normop{\E\bbb{\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}^{\top}|x_{t}}} \\
    & = \normop{\E\bbb{\sum_{i=0}^{B-1}c_{i}^{2}\bb{s\bb{\tau_{i}, x_{\tau_i}}-e^{-\bb{\tau_{i+1}-\tau_{i}}}s\bb{\tau_{i+1}, x_{\tau_{i+1}}}}\bb{s\bb{\tau_{i}, x_{\tau_i}}-e^{-\bb{\tau_{i+1}-\tau_{i}}}s\bb{\tau_{i+1}, x_{\tau_{i+1}}}}^{\top}|x_{t}}} \\
    & = \normop{\E\bbb{\sum_{i=0}^{B-1}c_{i}^{2}\E\bbb{\bb{s\bb{\tau_{i}, x_{\tau_i}}-e^{-\bb{\tau_{i+1}-\tau_{i}}}s\bb{\tau_{i+1}, x_{\tau_{i+1}}}}\bb{s\bb{\tau_{i}, x_{\tau_i}}-e^{-\bb{\tau_{i+1}-\tau_{i}}}s\bb{\tau_{i+1}, x_{\tau_{i+1}}}}^{\top}|x_{\tau_{i+1}}}|x_{t}}}
    }
    Note that $\E\bbb{s\bb{\tau_{i}, x_{\tau_i}}|x_{\tau_{i+1}}} = e^{-\bb{\tau_{i+1}-\tau_{i}}}s\bb{\tau_{i+1}, x_{\tau_{i+1}}}$. Therefore, 
    \ba{
    & \normop{\E\bbb{\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}^{\top}|x_{t}}} \\
    &\leq \normop{\E\bbb{\sum_{i=0}^{B-1}c_{i}^{2}\E\bbb{\bb{s\bb{\tau_{i}, x_{\tau_i}}-s_{\tau_{i}}\bb{\tilde{x}_{\tau_i}}}\bb{s\bb{\tau_{i}, x_{\tau_i}}-s_{\tau_{i}}\bb{\tilde{x}_{\tau_i}}}^{\top}|x_{\tau_{i+1}}}|x_{t}}} \label{eq:timestep_decomposition_1}
    }
    Using the fundamental theorem of calculus, for $x_{\tau_{i},\lambda_{i}} := \lambda_{i}x_{\tau_{i}} + \bb{1-\lambda_{i}} \tilde{x}_{\tau_{i}}, \lambda \in \bb{0,1}$, we have, 
    \bas{
s\bb{\tau_{i}, x_{\tau_i}}-s_{\tau_{i}}\bb{\tilde{x}_{\tau_i}} &= \int_{0}^{1}h_{\tau_{i}}\bb{x_{\tau_{i},\lambda_i}}\bb{x_{\tau_i}-\tilde{x}_{\tau_i}}d\lambda \\
&= \E_{\lambda \sim \mathcal{U}\bb{0,1}}\bbb{h_{\tau_i}\bb{x_{\tau_i,\lambda}}\bb{x_{\tau_i}-\tilde{x}_{\tau_i}}}
    }
    Substituting in \eqref{eq:timestep_decomposition_1} and using the fact that $c_i \leq 1$ completes our proof.
\end{proof}

We aim to derive a sharp bound on the quantities stated in the previous lemma. Since the Hessian is not assumed to be Lipschitz continuous, directly bounding these quantities can be challenging. To address this, we employ a mollification technique. Mollification smooths a function by averaging it over a small neighborhood, effectively regularizing it to ensure desirable continuity properties. This approach is particularly useful when dealing with functions that may not be smooth or Lipschitz continuous, as it allows us to derive meaningful bounds by working with the mollified version of the function.

In our case, the Hessian is mollified by integrating over a uniformly distributed random variable on a small ball of radius \(\epsilon\). This process ensures that the mollified Hessian exhibits controlled variation, enabling us to bound the difference between its values at two points \(x\) and \(y\). The following lemma formalizes this construction and provides a bound on the operator norm of the difference between the mollified Hessians at \(x\) and \(y\).


\begin{lemma}\label{lemma:hessian_mollification}
    Let $h : \R^{d} \rightarrow \R^{d \times d}$ such that $\forall x \in \R^{d}, \; \normop{h\bb{x}} \leq L$. Let $z$ be uniformly distributed over the unit $\mathbb{L}_{2}$ ball. For $\epsilon > 0$, define $h_{\epsilon}(x) := E_{z}[h_{\epsilon}(x + \epsilon z)]$. Then, for all $x, y \in \R^{d}$, 
    \bas{
        \normop{h_{\epsilon}(x) - h_{\epsilon}(y)} &\leq \frac{2Ld}{\epsilon}\norm{x-y}_{2}
    }
\end{lemma}
\begin{proof}
Define $B\bb{a, R}$ be the ball of radius $R$ around $a$. Define the set $B(x,\epsilon)\cap B(y,\epsilon) = S$ and denote $d\mu_{\epsilon}$ to be the lebesgue measure over $B\bb{0,\epsilon}$. Then,
\begin{align}
    h_{\epsilon}(x) - h_{\epsilon}(y) &= \int h(x+Z)d\mu_{\epsilon}(Z) - \int h(y+Z^{\prime})d\mu_{\epsilon}(Z^\prime) \nonumber \\
    &= \frac{1}{|B(0,\epsilon)|}\left[\int_{B(x,\epsilon)} h(w)dw -\int_{B(y,\epsilon)} h(y)dy \right] \nonumber \\
    &= \frac{1}{|B(0,\epsilon)|}\left[\int_{B(x,\epsilon)\cap S^{\complement}} h(w)dw -\int_{B(y,\epsilon)\cap S^{\complement}} h(y)dy \right] \nonumber \\
\end{align}
$$ \implies \normop{h_{\epsilon}(x) - h_{\epsilon}(y)} \leq 2L \frac{\mathsf{Vol}(S^{\complement})}{\mathsf{Vol}(B(0,\epsilon))}$$
Using Theorem 1 from \cite{schymura2014upper}, we have
\bas{
    \mathsf{Vol}(S^{\complement}) &\leq \norm{x-y}_{2} \times \surf\bb{B(0,\epsilon)}
}
Therefore, 
\bas{
    \normop{h_{\epsilon}(x) - h_{\epsilon}(y)} &\leq 2L \frac{\surf\bb{B(0,\epsilon)}}{\mathsf{Vol}(B(0,\epsilon))} \times \norm{x-y}_{2} 
}
We have for $B(0,\epsilon)$,  $\frac{\surf\bb{B(0,\epsilon)}}{\mathsf{Vol}(B(0,\epsilon))} = d/\epsilon$ which completes our result.
\end{proof}

Lemma~\ref{lemma:hessian_mollification} demonstrates that the mollified Hessian \(h_{\epsilon}\) becomes Lipschitz due to the smoothing introduced by the uniform averaging over the ball \(z\), even though the original Hessian \(h\) does not have this property. This insight is crucial when dealing with expressions such as 
\[
\E_{\lambda, x_{t'}, \tilde{x}_{t'}}\bbb{h_{t'}\bb{x_{t',\lambda}}\bb{x_{t'}-\tilde{x}_{t'}}\bb{x_{t'}-\tilde{x}_{t'}}^{\top}h_{t'}\bb{x_{t',\lambda}}^{\top}|x_{t}},
\]
which arise from Lemma~\ref{lemma:conditional_decomposition}.

When \(t\) and \(t'\) are close, one would hope to exploit the smoothness of the Hessian \(h_t\) with respect to time. Specifically, if \(h_t\) were smooth in the time parameter, this would allow the expectation to move inside, enabling the use of Tweedie’s second-order formula (Lemma~\ref{lemma:second_order_tweedie_application}) to derive variance bounds that are dimension-free and independent of strong assumptions on the Hessian.

However, directly imposing such strong assumptions on the Hessian is restrictive. To address this, we decompose the Hessian \(h_{t'}\bb{x_{t',\lambda}}\) into two components:
\[
h_{t'}\bb{x_{t',\lambda}} = h_{t', \epsilon}\bb{x_{t',\lambda}} + \bb{h_{t'}\bb{x_{t',\lambda}} - h_{t', \epsilon}\bb{x_{t',\lambda}}}.
\]
Here, the first term, \(h_{t', \epsilon}\bb{x_{t',\lambda}}\), leverages the Lipschitz continuity of the mollified Hessian and can be analyzed by conditioning on \(x_t\). The second term, which represents the deviation between the original and mollified Hessians, requires a finer analysis that draws upon Lusin's theorem, as developed further in Lemma~\ref{lemma:lusin_theorem_decomp}.

The decomposition allows us to systematically address each term: 
- The Lipschitz property of \(h_{t',\epsilon}\) helps bound the first term cleanly.
- The second term is bounded using probabilistic arguments based on the regularity properties introduced by mollification.

The following lemma formalizes this decomposition and provides the necessary bounds to proceed with the analysis.


\begin{lemma}\label{lemma:hessian_smoothness_decomp}
Suppose Assumption~\ref{assumption:score_function_smoothness}-(0) and (1) hold. Let $t > t' > 0$ and define the following quantities:
\begin{enumerate}
    \item Let $\tilde{x}_{t'}$ be an independent copy of $x_{t'}$ when conditioned on $x_{t}$.
    \item Let $\lambda \sim \mathsf{Unif}\bb{0,1}$ independent of the variables above.
    \item Let $x_{t',\lambda} := \lambda x_{t'} + \bb{1-\lambda}\tilde{x}_{t'}$, $\tilde{z}_{t,t'} := x_t - e^{-(t-t')}\tilde{x}_{t'}$.
    \item Let $h_{t'}\bb{\cdot} := \nabla^{2}\log\bb{p_{t'}\bb{\cdot}}$.
    \item  For $z$ be uniformly distributed over the unit $\mathbb{L}_{2}$ ball and $\epsilon > 0$, define $h_{t', \epsilon}(x) := E_{z}[h_{t'}(x + \epsilon z)]$.
    \item  Let $g_{t', \epsilon}\bb{x_{t',\lambda}} := \bb{h_{t'}\bb{x_{t',\lambda}} - h_{t', \epsilon}(x_{t',\lambda})}$.
\end{enumerate}
 Then, there exists a random $d\times d$ matrix $M$ such that $\normop{M} \leq \frac{2Ld}{\epsilon}\norm{\bb{1-\lambda}z_{t,t'} + \lambda\tilde{z}_{t,t'}}_{2}$ and
\bas{
&\normop{\E_{\lambda, x_{t'}, \tilde{x}_{t'}}\bbb{h_{t'}\bb{x_{t',\lambda}}\bb{x_{t'}-\tilde{x}_{t'}}\bb{x_{t'}-\tilde{x}_{t'}}^{\top}h_{t'}\bb{x_{t',\lambda}}^{\top}|x_{t}}} \\ &\leq 
6e^{2\bb{t-t'}}\normop{h_{t',\epsilon}\bb{e^{t-t'}x_{t}}\bb{\sigma_{t-t'}^{4}h_{t}\bb{x_t} + \sigma_{t-t'}^{2}\id_{d}}h_{t',\epsilon}\bb{e^{t-t'}x_{t}}^{\top}} \\ 
& + 3\bb{\E_{\lambda, x_{t'}, \tilde{x}_{t'}}\bbb{\normop{M}^{2}\normop{x_{t'}-\tilde{x}_{t'}}^{2}|x_{t}} + \E_{x_{t'}, \tilde{x}_{t'}}\bbb{\normop{\E_{\lambda}\bbb{g_{t', \epsilon}\bb{x_{t',\lambda}}} }^{2}\normop{x_{t'}-\tilde{x}_{t'}}^{2}|x_{t}}}
}

\end{lemma}
\begin{proof}
By assumption, we have $\forall x \in \R^{d}, \; \norm{h_{t}\bb{x}}_{2} \leq L$. Note that conditioned on $x_{t}$, we have
\bas{
    x_{t} &= e^{-\bb{t-t'}}x_{t'} + z_{t,t'} = e^{-\bb{t-t'}}\tilde{x}_{t'} + \tilde{z}_{t,t'}
}
Where $\tilde{z}_{t,t'} \sim \mathcal{N}(0,\sigma_{t,t'}^2\id_d)$ marginally. Therefore, 
\bas{
    x_{t',\lambda} &= e^{t-t'}x_{t} - e^{t-t'}\bb{\bb{1-\lambda}z_{t,t'} + \lambda\tilde{z}_{t,t'}}
}
Using Lemma~\ref{lemma:hessian_mollification}, 
\bas{    
    h_{t',\epsilon}\bb{x_{t',\lambda}} &= h_{t',\epsilon}\bb{e^{t-t'}x_{t}} + M, \;\; \text{ for } \normop{M} \leq \frac{2Ld}{\epsilon}\norm{\bb{1-\lambda}z_{t,t'} + \lambda\tilde{z}_{t,t'}}_{2}
}        
Then, 
\bas{
    h_{t'}\bb{x_{t',\lambda}} &= h_{t', \epsilon}(x_{t',\lambda}) +  \bb{h_{t'}\bb{x_{t',\lambda}} - h_{t', \epsilon}(x_{t',\lambda})} \\
    &= h_{t',\epsilon}\bb{e^{t-t'}x_{t}} + M +  \bb{h_{t'}\bb{x_{t',\lambda}} - h_{t', \epsilon}(x_{t',\lambda})}
}
Let $q_{t} := \E_{\lambda, x_{t'}, \tilde{x}_{t'}}\bbb{h_{t'}\bb{x_{t',\lambda}}\bb{x_{t'}-\tilde{x}_{t'}}\bb{x_{t'}-\tilde{x}_{t'}}^{\top}h_{t'}\bb{x_{t',\lambda}}^{\top}|x_{t}}$ and $g_{t', \epsilon}\bb{x_{t',\lambda}} := \bb{h_{t'}\bb{x_{t',\lambda}} - h_{t', \epsilon}(x_{t',\lambda})}$. Then, using the fact that $(a+b+c)(a+b+c)^{\top} \preceq 3(aa^{\top} + bb^{\top} + cc^{\top})$ for arbitrary vectors $a,b,c \in \R^d$, we have:
\bas{
    q_{t} &\preceq 3\underbrace{\E_{\lambda, x_{t'}, \tilde{x}_{t'}}\bbb{h_{t',\epsilon}\bb{e^{t-t'}x_{t}}\bb{x_{t'}-\tilde{x}_{t'}}\bb{x_{t'}-\tilde{x}_{t'}}^{\top}h_{t',\epsilon}\bb{e^{t-t'}x_{t}}^{\top}|x_{t}}}_{:= T_{1}} \\
    &+ 3\underbrace{\E_{\lambda, x_{t'}, \tilde{x}_{t'}}\bbb{M\bb{x_{t'}-\tilde{x}_{t'}}\bb{x_{t'}-\tilde{x}_{t'}}^{\top}M^{\top}|x_{t}}}_{:= T_{2}} \\
    &+ 3\underbrace{\E_{\lambda, x_{t'}, \tilde{x}_{t'}}\bbb{g_{t', \epsilon}\bb{x_{t',\lambda}}\bb{x_{t'}-\tilde{x}_{t'}}\bb{x_{t'}-\tilde{x}_{t'}}^{\top}g_{t', \epsilon}\bb{x_{t',\lambda}}^{\top}|x_{t}}}_{:= T_{3}}
}
Let's first deal with $T_1$. We use the fact that $x_{t} = e^{-\bb{t-t'}}x_{t'} + z_{t,t'} = e^{-\bb{t-t'}}\tilde{x}_{t'} + \tilde{z}_{t,t'}$ along with first order and second order Tweedie's formula in Lemma~\ref{lemma:second_order_tweedie_application}
\bas{
    T_{1} 
    &= 2e^{2\bb{t-t'}}h_{t',\epsilon}\bb{e^{t-t'}x_{t}}\bb{\sigma_{t-t'}^{4}h_{t}\bb{x_t} + \sigma_{t-t'}^{2}\id_{d}}h_{t',\epsilon}\bb{e^{t-t'}x_{t}}^{\top}
}
Now, for $T_{2}$, we have
\bas{
    T_{2} &= \E_{\lambda, x_{t'}, \tilde{x}_{t'}}\bbb{M\bb{x_{t'}-\tilde{x}_{t'}}\bb{x_{t'}-\tilde{x}_{t'}}^{\top}M^{\top}|x_{t}} \\
    &\preceq \E_{\lambda, x_{t'}, \tilde{x}_{t'}}\bbb{\normop{M}^{2}\normop{x_{t'}-\tilde{x}_{t'}}^{2}|x_{t}}\id_{d}
}
and similarly for $T_{3}$, 
\bas{
    T_{3} &\preceq \E_{x_{t'}, \tilde{x}_{t'}}\bbb{\normop{\E_{\lambda}\bbb{g_{t', \epsilon}\bb{x_{t',\lambda}}} }^{2}\normop{x_{t'}-\tilde{x}_{t'}}^{2}|x_{t}}\id_{d}
}
which completes our proof.
\end{proof} 

Lemma~\ref{lemma:lusin_theorem} provides a corollary of Lusin's theorem (see for e.g. \cite{folland1999real}) to assert that any measurable function, such as the Hessian \(h_t(x) = \nabla^2 \log p_t(x)\), can be approximated uniformly on a compact subset \(G_\gamma \subseteq [t', t] \times F\), where the excluded measure is arbitrarily small. This result ensures that \(h_t(x)\) is uniformly continuous on \(G_\gamma\), with its continuity quantified by a modulus of continuity \(\omega_\gamma(\cdot)\) depending only on \(\gamma\). See \cite{rudin1976principles} for Heine–Cantor theorem which implies uniform continuity due to compactness.

\begin{lemma}[Corollary of Lusin's Theorem]\label{lemma:lusin_theorem}
Let $F$ be a convex, compact set over $\R^d$ and $\Lambda$ be the Lebesgue measure. Let $h_t(x) = \nabla^2 \log p_t(x)$ be measurable. For any $\gamma > 0$, there exists a compact set $G_\gamma \subseteq [t^{\prime},t]\times F$ such that $\Lambda([t^{\prime},t]\times F)\setminus G_{\gamma}) < \gamma$ and $(t,x) \to h_t(x)$ is uniformly continuous over $G_{\gamma}$. Let us call the corresponding modulus of continuity as $\omega_{\gamma}()$, which depends only on $\gamma$.
\end{lemma}


Building on Lemma~\ref{lemma:lusin_theorem}, Lemma~\ref{lemma:lusin_theorem_decomp} aims to bound the fourth moment of the operator norm of the difference \(h_{\tau_i}(x_{\tau_i, \lambda}) - h_{\tau_i, \epsilon}(x_{\tau_i, \lambda})\), which arises from the deviation between the Hessian and its mollified counterpart. To achieve this, the interval \([t', t]\) is partitioned into smaller subintervals \(\tau_0, \tau_1, \ldots, \tau_B\), allowing the analysis to proceed incrementally. The lemma exploits the uniform continuity of \(h_t(x)\) on \(G_\gamma\) to tightly control this difference using the modulus of continuity \(\omega_\gamma(\epsilon)\). Contributions from outside the compact subset \(G_\gamma\) are accounted for separately using indicator functions, with their impact controlled by the boundedness of the Hessian, \(\normop{h_t(x)} \leq L\). The resulting bound consists of two key terms: a primary term proportional to \(B\omega_\gamma(\epsilon)^4\), capturing the uniform continuity of the Hessian on \(G_\gamma\), and a residual term proportional to the probability of \(h_t(x)\) lying outside \(G_\gamma\), which is effectively managed by the boundedness assumption. This decomposition is crucial for controlling the variance of the Hessian and ensuring the residual terms remain small.


\begin{lemma}\label{lemma:lusin_theorem_decomp} 
Fix a $B \in \mathbb{N}$. Let $\tau_{0} := t' < \tau_{1} < \tau_{2} < \cdots < \tau_{B-1} < t := \tau_{B}$. Let Assumption~\ref{assumption:score_function_smoothness}-(0),(1) hold. Let $h_t(x),h_{t, \epsilon}(x)$ be defined as in Lemma~\ref{lemma:hessian_smoothness_decomp}.  Let $Z$ be uniformly distributed on the unit $L^2$ ball in $\R^d$, independent of everything else. Then for any $\gamma > 0$:
\bas{
    &\sum_{i=0}^{B-1}\E_{x_{\tau_{i}}, \tilde{x}_{\tau_{i}}}\bbb{\normop{\E_{\lambda\sim \mathsf{Unif}(0,1)}\bbb{h_{\tau_{i}}\bb{x_{\tau_{i},\lambda_i}} - h_{\tau_{i}, \epsilon}(x_{\tau_{i},\lambda_i}) }}^{4}|x_{\tau_{i+1}}} \leq \\
    & \quad\quad\quad\quad B\omega_{\gamma}(\epsilon)^4 + 16 L^4\sum_{i=0}^{B-1}\E_{x_{\tau_{i}}, \tilde{x}_{\tau_{i}}}\left[\int_{0}^{1}\mathbbm{1}((\tau_i,x_{\tau_i,\lambda}) \not\in G_{\gamma})+\mathbbm{1}((\tau_i,x_{\tau_i,\lambda}+\epsilon Z) \not\in G_{\gamma})d\lambda\bigr|x_{\tau_{i+1}}\right]
}
where $x_{\tau_i}$ is an i.i.d copy of $\tilde{x}_{\tau_i}$ conditioned on $x_{\tau_{i+1}}$ and $x_{\tau_i,\lambda} := \lambda x_{\tau_i} + \bb{1-\lambda}\tilde{x}_{\tau_i}$ for any given $\lambda \in \bbb{0,1}$ and $\omega_{\gamma}, G_{\gamma}$ are as defined in Lemma~\ref{lemma:lusin_theorem}.
\end{lemma}
\begin{proof}

Let us consider Lusin's theorem (Lemma~\ref{lemma:lusin_theorem}) over $[t^{\prime},t]\times F$ endowed with the Lebesgue measure $\Lambda$. By Assumption~\ref{assumption:score_function_smoothness}-(0),(1): we have $\|h_t(x)\| \leq L$ for every $t$ almost everywhere under the Lebesgue measure on $\R^d$. We denote $\E_{\lambda \sim \mathsf{Unif}(0,1)}$ as $\E_{\lambda}$ and only in the set of equations below, we denote expectation with respect to ${x_{\tau_{i}}, \tilde{x}_{\tau_{i}}},Z$ conditioned on $x_{\tau_{i+1}}$ by $\bar{\E}$:
\begin{align}
& \bar{\E}\bbb{\normop{\E_{\lambda}\bbb{h_{\tau_{i}}\bb{x_{\tau_{i},\lambda}} - h_{\tau_{i}, \epsilon}(x_{\tau_{i},\lambda}) }}^{4}|x_{\tau_{i+1}}} \\
&= \bar{\E}\biggr[\biggr\|\int_{0}^{1}  h_{\tau_i}(x_{\tau_i,\lambda}) - h_{\tau_i,\epsilon}(x_{\tau_i,\lambda}) d\lambda\biggr\|_{\mathsf{op}}^4|x_{\tau_{i+1}}\biggr] \nonumber \\
&\leq \bar{\E}\|\int_{0}^{1} h_{\tau_i}(x_{\tau_i,\lambda}) - h_{\tau_i}(x_{\tau_i,\lambda}+\epsilon Z) d\lambda\|_{\mathsf{op}}^4 \nonumber \\
&\leq \bar{\E}\int_{0}^{1}\mathbbm{1}((\tau_i,x_{\tau_i,\lambda}) \in G_{\gamma})\mathbbm{1}((\tau_i,x_{\tau_i,\lambda}+\epsilon Z) \in G_{\gamma}) \omega_{\gamma}(\epsilon)^4 d\lambda \nonumber \\
&\quad + \bar{\E}\int_{0}^{1}\left[\mathbbm{1}((\tau_i,x_{\tau_i,\lambda}) \not\in G_{\gamma})+\mathbbm{1}((\tau_i,x_{\tau_i,\lambda}+\epsilon Z) \not\in G_{\gamma})\right] 16 L^4 d\lambda \nonumber \\
&\leq \omega_{\gamma}(\epsilon)^4 + \bar{\E}\int_{0}^{1}\left[\mathbbm{1}((\tau_i,x_{\tau_i,\lambda}) \not\in G_{\gamma})+\mathbbm{1}((\tau_i,x_{\tau_i,\lambda}+\epsilon Z) \not\in G_{\gamma})\right] 16L^4 d\lambda 
\end{align}
Therefore, we must have:
\begin{align}
\sum_{i=0}^{B-1}&\E\bbb{\|\int_{0}^{1}  h_{\tau_i}(x_{\tau_i,\lambda}) - h_{\tau_i,\epsilon}(x_{\tau_i,\lambda}) d\lambda\|^4} \nonumber \\
&\leq B\omega_{\gamma}(\epsilon)^2 + 16 L^4\sum_{i=0}^{B-1}\E\left[\int_{0}^{1}\mathbbm{1}((\tau_i,x_{\tau_i,\lambda}) \not\in G_{\gamma})+\mathbbm{1}((\tau_i,x_{\tau_i,\lambda}+\epsilon) \not\in G_{\gamma})d\lambda\right]  \nonumber
\end{align}
\end{proof}

% \begin{lemma}\label{lemma:cauchy_schwarz}
%     Let $\left\{X_{i}\right\}_{i \in [n]}$ and $\left\{Y_{i}\right\}_{i \in [n]}$ be arbitrary random variables over the probablity space $\bb{\Omega, \mathcal{F}, \mathbb{P}}$ such that $\forall i\in [n], \; \E\bbb{|X_i|}, \E\bbb{|Y_i|} < \infty$. Then, 
%     \bas{
%         \sum_{i \in [n]}\E\bbb{X_{i}Y_{i}} \leq \sqrt{\bb{\sum_{i \in [n]}\E\bbb{X_{i}^{2}}}\bb{\sum_{i \in [n]}\E\bbb{Y_{i}^{2}}}}
%     }
% \end{lemma}
% \begin{proof}
%     \bas{
%         \sum_{i \in [n]}\E\bbb{X_{i}Y_{i}} &= \E\bbb{\sum_{i \in [n]}X_{i}Y_{i}} \\
%         &\leq \E\bbb{\sqrt{\sum_{i \in [n]}X_{i}^{2}}\sqrt{\sum_{i \in [n]}Y_{i}^{2}}} \\
%         &\leq \sqrt{\E\bbb{\sum_{i \in [n]}X_{i}^{2}}\E\bbb{\sum_{i \in [n]}Y_{i}^{2}}} \\
%         &= \sqrt{\bb{\sum_{i \in [n]}\E\bbb{X_{i}^{2}}}\bb{\sum_{i \in [n]}\E\bbb{Y_{i}^{2}}}}
%     }
% \end{proof}
% \dn{old statement below. It is being reworked by me. 
% \begin{lemma}[Variance bound for martingale difference sequence]\label{lemma:martingale_diff_variance_bound_appendix_old}
% For $\gamma_{t} > 0$, let 
% \bas{
%     H := \sum_{t \in \timeset,i \in [m]}\frac{\gamma_{t}}{\sigma_{t}^{2}}\langle f(x_{t}^{(i)}),z_{t}^{(i)}-\E [z_{t}^{(i)}|x_{t}^{(i)}]\rangle
% }
% and for the $\sigma$-algebra $\mathcal{F}_j = \sigma(x^{(i)}_{t}: 1\leq i\leq m, t \geq t_{N-j+1})$, let $H_{j} := \E\bbb{H|\mathcal{F}_{j}}$. Then, $\forall j \in [N+1], \; D_{j} := H_{j} - H_{j-1}$ forms a martingale difference sequence such that 
% \bas{
%     \forall j \in [N], \;\; \E\bbb{D_{j}^{2}|\mathcal{F}_{j}} \leq  O\bb{e^{2t}\bb{L\Delta^{2} + \Delta + L^{2}\Delta}}\sum_{i=1}^{m}\norm{v_{t,i,j}}_{2}^{2}
% }    
% where 
% \bas{
%     v_{t,i,j} &:= \sum_{t = t_{N-j+2}}^{t_{N}}\frac{\gamma_{t}}{\sigma_{t}^{2}}e^{-t}f(x_t^{(i)}), \text{ and } \Delta := t-t'
% }
% \end{lemma}}

The following lemma consolidates the results and arguments developed so far to provide a variance bound for a martingale difference sequence. Our goal is to bound the variance of the terms in the sequence \(R_{i,k}\), which is determined by both the predictable sequence \(G_{i,k+1}\) and the smoothness properties of the score function and its Hessian. To achieve this, we build on several key results: 

\begin{enumerate}
    \item Lemma~\ref{lemma:lusin_theorem_decomp}, which establishes bounds for the difference between the Hessian and its mollified counterpart by leveraging the compactness provided by Lusin’s theorem.
    \item Lemma~\ref{lemma:hessian_smoothness_decomp}, which shows how the mollified Hessian can be used to control variance terms using its Lipschitz properties.
    \item Lemma~\ref{lemma:conditional_decomposition}, which provides a decomposition of the conditional variance in terms of contributions from smaller subintervals.
\end{enumerate}

The argument proceeds by partitioning the time interval \([t_{N-k}, t_{N-k+1}]\) into smaller subintervals and analyzing the contributions to the variance over each subinterval. Using mollification and uniform continuity on compact subsets, we control the deviations arising from the lack of Lipschitz continuity in the Hessian. Furthermore, the variance bounds incorporate the contributions from outside the compact subset, which are managed via Lusin's theorem. By carefully summing these contributions and leveraging smoothing techniques, we arrive at a sharp variance bound that scales with the parameters \(\Delta\) (the interval size) and \(L\) (the bound on the Hessian) 

The final result, formalized in Lemma~\ref{lemma:martingale_diff_variance_bound_appendix}, also uses the second-order Tweedie formula to handle the special case of the last time step (\(k = N\)) in the martingale sequence. This lemma serves as a culmination of our efforts, combining mollification, decomposition, and smoothness assumptions to derive a practical variance bound that is essential for analyzing the concentration of the martingale difference sequence.


\begin{lemma}[Variance bound for martingale difference sequence]\label{lemma:martingale_diff_variance_bound_appendix}

Consider the martingale difference sequence $R_{i,k}$, predictable sequence $G_{i,k+1}$ with respect to the filtration $\mathcal{F}_{i,k}$ as considered in Lemma~\ref{lemma:variance_bound_1}. Define $\Delta:=t_{N-k+1}-t_{N-k}$

\begin{equation}
    \E\bbb{R_{i,k}^{2}|\mathcal{F}_{i,k-1}} \leq \begin{cases} 0 &\text{ if } k = 0 \\
     C(L\Delta^2 + \Delta + L^2\Delta) e^{2t_{N-k+1}}\|G_{i,k+1}\|^2&\text{ if } k \in \{1,\dots,N-1\} \\
    C(L\Delta^2 + \Delta )\|\bar{G}_i\|^2 &\text{ if } k = N 
    \end{cases}
\end{equation}
% \dn{$k = N$ case follows from second order tweedie's formula. I have commented the lemma regarding this below, uncomment to see the proof}

\end{lemma}
\begin{proof}
Consider the case $k \in \{1,\dots,N-1\}$. For the sake of clarity, we let $t = t_{N-k+1}$, $t' = t_{N-k}$. Then, $\Delta = t-t'$ and let $B \in \mathbb{N}$. We decompose $[t',t]$ as follows:
$$[t',t] = \cup_{i=1}^{B}I_i \quad ;\quad I_i := [t'+\tfrac{(i-1)\Delta}{B},t' + \tfrac{i\Delta}{B}]\,.$$


For $\forall i \in [B], \; \tau_{i} \sim \mathsf{Unif}(I_i)$, $J \sim \mathsf{Unif}(\{1,\dots,B\})$. Given $\tau_i$, define the random variables $Z,\lambda,x_{\tau_i,\lambda},\tilde{x}_{\tau_i,\lambda},x_{\tau_i}$ as in Lemma~\ref{lemma:lusin_theorem_decomp} and with $Z,\lambda, (x_s)_{s\geq 0}$ indepenent of $(\tau_i)_i,J$. Define the random variable $ \tau^* := \tau_J$, $X = x_{\tau^*,\lambda}$, $X_\epsilon = x_{\tau^*,\lambda}+\epsilon Z$. Notice that $T$ is uniformly distributed over $[t',t]$.

Let $r_{i} := \tau_{i+1} - \tau_{i} \leq \frac{\Delta}{B}$. Using Lemma~\ref{lemma:hessian_smoothness_decomp} along with the Cauchy-Schwarz inequality, we have
\ba{
&\normop{\E_{\lambda_{i}, x_{\tau_i}, \tilde{x}_{\tau_i}}\bbb{h_{\tau_i}\bb{x_{\tau_i,\lambda_i}}\bb{x_{\tau_i}-\tilde{x}_{\tau_i}}\bb{x_{\tau_i}-\tilde{x}_{\tau_i}}^{\top}h_{\tau_i}\bb{x_{\tau_i,\lambda_i}}^{\top}|x_{\tau_{i+1}}}} \notag \\ 
&\leq 
6e^{2r_i}\normop{h_{\tau_{i},\epsilon}\bb{e^{r_i}x_{t}}\bb{\sigma_{r_i}^{4}h_{\tau_i}\bb{x_{\tau_i}} + \sigma_{r_i}^{2}\id_{d}}h_{\tau_{i},\epsilon}\bb{e^{r_i}x_{\tau_i}}^{\top}} \notag \\ 
& + \frac{12L^{2}d^{2}}{\epsilon^{2}}\E_{\lambda, x_{\tau_i}, \tilde{x}_{\tau_i}}\bbb{\norm{\bb{1-\lambda}z_{\tau_{i+1},\tau_{i}} + \lambda\tilde{z}_{\tau_{i+1},\tau_{i}}}_{2}^{4}|x_{\tau_{i+1}}}^{\frac{1}{2}}\E_{\lambda, x_{\tau_{i}}, \tilde{x}_{\tau_{i}}}\bbb{\normop{x_{\tau_{i}}-\tilde{x}_{\tau_{i}}}^{4}|x_{\tau_{i+1}}}^{\frac{1}{2}} \notag \\
& + 3\E_{x_{\tau_i}, \tilde{x}_{\tau_i}}\bbb{\normop{\E_{\lambda_i}\bbb{h_{\tau_i}\bb{x_{\tau_i,\lambda_i}} - h_{\tau_i, \epsilon}(x_{\tau_i,\lambda_i})}}^{4}|x_{\tau_{i+1}}}^{\frac{1}{2}}\E_{\lambda_i, x_{\tau_{i}}, \tilde{x}_{\tau_{i}}}\bbb{\normop{x_{\tau_{i}}-\tilde{x}_{\tau_{i}}}^{4}|x_{\tau_{i+1}}}^{\frac{1}{2}} \label{eq:hessian_smoothness_decomp}
}
Using Lemma~\ref{lemma:conditional_decomposition} along with \eqref{eq:hessian_smoothness_decomp} and Cauchy Schwarz inequality, we have
\ba{
    & \normop{\E\bbb{\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}^{\top}|x_{t}}} \notag \\
    & \leq \normop{\E\bbb{\sum_{i=0}^{B-1}\E_{\lambda_{i}, x_{\tau_i}, \tilde{x}_{\tau,i}}\bbb{h_{\tau_i}\bb{x_{\tau_i,\lambda_i}}\bb{x_{\tau_i}-\tilde{x}_{\tau_i}}\bb{x_{\tau_i}-\tilde{x}_{\tau_i}}^{\top}h_{\tau_i}\bb{x_{\tau_i,\lambda_i}}^{\top}|x_{\tau_{i+1}}}}\bigg|x_{t} } \notag \\
    &\leq 6\sum_{i=0}^{B-1}e^{2r_i}\E\bbb{\normop{h_{\tau_{i},\epsilon}\bb{e^{r_i}x_{t}}\bb{\sigma_{r_i}^{4}h_{\tau_i}\bb{x_{\tau_i}} + \sigma_{r_i}^{2}\id_{d}}h_{\tau_{i},\epsilon}\bb{e^{r_i}x_{\tau_i}}^{\top}}|x_{t}}\notag \\
    & + \frac{12L^{2}d^{2}}{\epsilon^{2}}\sum_{i=0}^{B-1}\E\bbb{\E_{\lambda, x_{\tau_i}, \tilde{x}_{\tau_i}}\bbb{\norm{\bb{1-\lambda}z_{\tau_{i+1},\tau_{i}} + \lambda\tilde{z}_{\tau_{i+1},\tau_{i}}}_{2}^{4}|x_{\tau_{i+1}}}^{\frac{1}{2}}\E_{\lambda, x_{\tau_{i}}, \tilde{x}_{\tau_{i}}}\bbb{\normop{x_{\tau_{i}}-\tilde{x}_{\tau_{i}}}^{4}|x_{\tau_{i+1}}}^{\frac{1}{2}}|x_{t}}\notag \\
    & + 3\sum_{i=0}^{B-1}\E\bbb{\E_{x_{\tau_i}, \tilde{x}_{\tau_i}}\bbb{\normop{\E_{\lambda_i}\bbb{h_{\tau_i}\bb{x_{\tau_i,\lambda_i}} - h_{\tau_i, \epsilon}(x_{\tau_i,\lambda_i})}}^{4}|x_{\tau_{i+1}}}^{\frac{1}{2}}\E_{\lambda_i, x_{\tau_{i}}, \tilde{x}_{\tau_{i}}}\bbb{\normop{x_{\tau_{i}}-\tilde{x}_{\tau_{i}}}^{4}|x_{\tau_{i+1}}}^{\frac{1}{2}}|x_{t}} \notag}
    \ba{
    &\leq 6\sum_{i=0}^{B-1}e^{2r_i}\E\bbb{\normop{h_{\tau_{i},\epsilon}\bb{e^{r_i}x_{t}}\bb{\sigma_{r_i}^{4}h_{\tau_i}\bb{x_{\tau_i}} + \sigma_{r_i}^{2}\id_{d}}h_{\tau_{i},\epsilon}\bb{e^{r_i}x_{\tau_i}}^{\top}}|x_{t}}\notag \\
    & + \frac{12L^{2}d^{2}}{\epsilon^{2}}\sum_{i=0}^{B-1}\E\bbb{\E_{\lambda, x_{\tau_i}, \tilde{x}_{\tau_i}}\bbb{\norm{\bb{1-\lambda}z_{\tau_{i+1},\tau_{i}} + \lambda\tilde{z}_{\tau_{i+1},\tau_{i}}}_{2}^{4}|x_{\tau_{i+1}}}^{\frac{1}{2}}\E_{\lambda, x_{\tau_{i}}, \tilde{x}_{\tau_{i}}}\bbb{\normop{x_{\tau_{i}}-\tilde{x}_{\tau_{i}}}^{4}|x_{\tau_{i+1}}}^{\frac{1}{2}}|x_{t}}\notag \\
    & + 3\E\bbb{\bb{\sum_{i=0}^{B-1}\E_{x_{\tau_i}, \tilde{x}_{\tau_i}}\bbb{\normop{\E_{\lambda_i}\bbb{h_{\tau_i}\bb{x_{\tau_i,\lambda_i}} - h_{\tau_i, \epsilon}(x_{\tau_i,\lambda_i})}}^{4}|x_{\tau_{i+1}}}}^{\frac{1}{2}} \bb{\sum_{i=0}^{B-1}\E_{\lambda_i, x_{\tau_{i}}, \tilde{x}_{\tau_{i}}}\bbb{\normop{x_{\tau_{i}}-\tilde{x}_{\tau_{i}}}^{4}|x_{\tau_{i+1}}}}^{\frac{1}{2}} |x_{t}} \label{eq:conditional_decomposition}
}
Using \eqref{eq:conditional_decomposition} and the observation that $\E_{\lambda_i, x_{\tau_{i}}, \tilde{x}_{\tau_{i}}}\bbb{\normop{x_{\tau_{i}}-\tilde{x}_{\tau_{i}}}^{4}|x_{\tau_{i+1}}}^{\frac{1}{2}} = O\bb{\sigma_{r_{i}}^{2}d} = O\bb{\frac{\Delta d}{B}}$, we have
\bas{
    & \normop{\E\bbb{\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}^{\top}|x_{t}}} \\
    & \leq 3Be^{\frac{2\Delta}{B}}\bb{\frac{L^{3}\Delta^{2}}{B^{2}} + \frac{L^{2}\Delta}{B}} + \frac{12\Delta^{2}L^{2}d^{4}}{B\epsilon^{2}} + \frac{3\Delta d}{\sqrt{B}}\bb{\sum_{i=0}^{B-1}\E_{x_{\tau_i}, \tilde{x}_{\tau_i}}\bbb{\normop{\E_{\lambda_i}\bbb{h_{\tau_i}\bb{x_{\tau_i,\lambda_i}} - h_{\tau_i, \epsilon}(x_{\tau_i,\lambda_i})}}^{4}|x_{\tau_{i+1}}}}^{\frac{1}{2}}
}

Using Lemma~\ref{lemma:lusin_theorem_decomp},
\bas{
    & \bb{\sum_{i=0}^{B-1}\E_{x_{\tau_i}, \tilde{x}_{\tau_i}}\bbb{\normop{\E_{\lambda_i}\bbb{h_{\tau_i}\bb{x_{\tau_i,\lambda_i}} - h_{\tau_i, \epsilon}(x_{\tau_i,\lambda_i})}}^{4}|x_{\tau_{i+1}}}}^{\frac{1}{2}} \\ 
    &\leq \sqrt{B}\omega_{\gamma}(\epsilon)^2 + 2 L^2\bb{\sum_{i=0}^{B-1}\E\left[\int_{0}^{1}\mathbbm{1}((\tau_i,x_{\lambda_i,\tau_i}) \not\in G_{\gamma})+\mathbbm{1}((\tau_i,x_{\lambda_i,\tau_i}+\epsilon Z_i) \not\in G_{\gamma})d\lambda_i\right]}^{\frac{1}{2}} \\
    &\leq \sqrt{B}\omega_{\gamma}(\epsilon)^2 + 2 L^2\bb{B \bb{\bP((T,X)\not\in G_{\gamma}) + \bP((T,X_
{\epsilon}) \not \in G_{\gamma})}}^{\frac{1}{2}}
}
Therefore, 
\bas{
    & \normop{\E\bbb{\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}^{\top}|x_{t}}} \\
    &\leq 6Be^{\frac{2\Delta}{B}}\bb{\frac{L^{3}\Delta^{2}}{B^{2}} + \frac{L^{2}\Delta}{B}} + \frac{12\Delta^{2}L^{2}d^{4}}{B\epsilon^{2}} + 6L^{2}\Delta d\bb{\omega_{\gamma}(\epsilon)^2 +  \bb{\bP((T,X)\not\in G_{\gamma}) + \bP((T,X_
{\epsilon}) \not \in G_{\gamma})}^{\frac{1}{2}}}
}
Notice that none of $\omega_{\gamma}, G_{\gamma}$, distribution of $T,X$ depend on $B$. Therefore pick $\epsilon \to 0$ and $B \to \infty$ such that $\frac{1}{B\epsilon^{2}} \to 0$ and $\omega_{\gamma}(\epsilon) \to 0$. $(T,X_{\epsilon}) \to (T,X)$ almost surely as $\epsilon \to 0$. Then, we take $\gamma\to 0$ and argue via continuity of the law of $(T,X)$ with respect to Lebesgue measure that $\bP((T,X) \not\in G_{\gamma}) \to \bP((T,X)\not\in [t',t]\times F)$. Since $F$ is arbitrary compact convex set, we let $F \uparrow \R^d$ to conclude the following: 
\ba{
    \normop{\E\bbb{\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}^{\top}|x_{t}}} &= O\bb{L^{2}\Delta} \label{eq:score_function_delta_variance}
}
Using Lemma~\ref{lemma:conditional_hessian}, we have
\bas{
    & \normop{\E\bbb{\bb{\E\bbb{x_{0}|x_{t}} - \E\bbb{x_{0}|x_{t'}}}\bb{\E\bbb{x_{0}|x_{t}} - \E\bbb{x_{0}|x_{t'}}}^{\top}|x_t}} \\
    & \leq 2e^{2t}\normop{\bb{\sigma_{t-t'}^{4}h_{t}\bb{x_{t}} + \sigma_{t-t'}^{2}\id_{d}}} \\
    & \;\;\;\; + 2e^{2t'}\sigma_{t'}^{4}\normop{\E\bbb{\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}^{\top}|x_{t}}} \\
    & = O\bb{e^{2t}\bb{L\Delta^{2} + \Delta + L^{2}\Delta}}
}
The result for $k < N$ then follows due to Lemma~\ref{lemma:variance_bound_1}.

Now, consider the case $k = N$. Recall $\Sigma_{i,k}$ defined in Lemma~\ref{lemma:variance_bound_1}.Then by second order Tweedie formula (Lemma~\ref{lemma:second_order_tweedie_application}) we have $\Sigma_{i,k} = \sigma_{t_1}^4 h_{t_1}(x_{t_1}) + \sigma_{t_1}^2 \id_d \lesssim \Delta^2L + \Delta$. Combining this with Lemma~\ref{lemma:variance_bound_1}, we conclude the result.
\end{proof}

We state a useful corollary which is subsequently useful for time bootstrapping and is implicit in the above proof.

\begin{corollary}\label{corr:score_function_delta_variance}Let $t' < t$ and $\Delta := t-t'$. Then, under Assumption~\ref{assumption:score_function_smoothness}, 
\bas{
    \normop{\E\bbb{\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}\bb{s\bb{t', x_{t'}} - e^{-\bb{t-t'}}s\bb{t, x_t}}^{\top}|x_{t}}} &= O\bb{L^{2}\Delta}
}
\end{corollary}
\begin{proof}
    The proof is implicit due to \eqref{eq:score_function_delta_variance}.
\end{proof}


\martingalediffsubGaussianityparameters*
\begin{proof}
We have, 
\bas{
    & \mathbb{P}\bb{\left|\inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}\right| \geq \alpha|\mathcal{F}_{i, k-1}} \\
    &= \mathbb{P}\bb{\left|\inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}\right|^{2} \geq \alpha^{2}|\mathcal{F}_{i, k-1}} \\
    &=  \mathbb{P}\bb{\exp\bb{\lambda\inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}^{2}} \geq \exp\bb{\lambda\alpha^{2}} |\mathcal{F}_{i, k-1}} \\
    &\leq \exp\bb{-\lambda\alpha^{2}}\E\bbb{\exp\bb{\lambda\inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}^{2}}|\mathcal{F}_{i, k-1}} \\
    &= \exp\bb{-\lambda\alpha^{2}}\E\bbb{\exp\bb{\lambda\inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}^{2}}|\mathcal{F}_{i, k-1}} \\
    &\leq \exp\bb{-\lambda\alpha^{2}}\E\bbb{\exp\bb{\lambda\norm{G_{i,k+1}}_{2}^{2}\norm{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}_{2}^{2}}\bigg|\mathcal{F}_{i, k-1}}
}
Since $G_{i,k+1}$ is measurable with respect to $\mathcal{F}_{i, k-1}$, set $\lambda := \frac{1}{\norm{G_{i,k+1}}_{2}^{2}\rho_{k}^{2}d}$ for $\rho_{k}$ defined in Lemma~\ref{lemma:subGaussianity_1},
\bas{
    \rho_{k} := 8\bb{L+1}e^{t_{N-k+1}}\sigma_{\gamma_{k}}
}
Therefore, 
\bas{
    & \mathbb{P}\bb{\left|\inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}\right| \geq \alpha|\mathcal{F}_{i, k-1}} \\ &\;\;\;\; \leq \exp\bb{\frac{-\alpha^{2}}{\norm{G_{i,k+1}}_{2}^{2}\rho_{k}^{2}d}}\E\bbb{\exp\bb{\frac{\norm{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}_{2}^{2}}{\rho_{k}^{2}d} }\bigg|\mathcal{F}_{i, k-1}}
}
Note that Lemma~\ref{lemma:subGaussianity_1} shows that $\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]$ is $\rho_k\sqrt{d}$ norm subGaussian
\bas{
    \E\bbb{\exp\bb{\frac{\norm{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}_{2}^{2}}{\rho_{k}^{2}d} }} \leq 2
}
Therefore, using Markov's inequality, with probablity atleast $1-\delta$, 
\ba{
    & \E\bbb{\exp\bb{\frac{\norm{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}_{2}^{2}}{\rho_{k}^{2}d} }\bigg|\mathcal{F}_{i, k-1}} \notag \\
    & \leq \frac{1}{\delta}\E\bbb{\E\bbb{\exp\bb{\frac{\norm{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}_{2}^{2}}{\rho_{k}^{2}d} }\bigg|\mathcal{F}_{i, k-1}}} \leq \frac{2}{\delta} \label{eq:markov_mgf_bound_1}
}

Pluggint these equations above, we conclude that with probability at-least $1-\delta$, for every $\alpha > 0$, we have:
$\mathbb{P}\bb{\left|\inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}\right| \geq \alpha|\mathcal{F}_{i, k-1}} \leq \frac{2}{\delta}\exp(-\lambda \alpha^2)$, which proves the result for $k \in [N-1]$.


For $k = N$, we similarly use the definition of $\nu_k$ Lemma~\ref{lemma:subGaussianity_2}, 
\bas{
    \nu_k := 4\sigma_{\gamma_k}
}
we have, 
\bas{
    & \mathbb{P}\bb{\left|\inner{\bar{G}_{i}}{z_{t_1}^{(i)}-\E\bbb{z_{t_1}^{(i)}|x_{t_1}^{(i)}}}\right| \geq \alpha|\mathcal{F}_{i, k-1}}  \leq \exp\bb{\frac{-\alpha^{2}}{\norm{\bar{G}_{i}}_{2}^{2}\nu_{k}^{2}d}}\E\bbb{\exp\bb{\frac{\norm{z_{t_1}^{(i)}-\E\bbb{z_{t_1}^{(i)}|x_{t_1}^{(i)}}}_{2}^{2}}{\nu_k^{2}d} }\bigg|\mathcal{F}_{i, k-1}}
}
The conclusion follows by a similar argument as \eqref{eq:markov_mgf_bound_1}.
\end{proof}


Based on the bounds established in Lemma~\ref{lemma:martingale_diff_variance_bound_appendix} and Lemma~\ref{lemma:subGaussianity_parameters}, we establish the following results. 
% \dn{old version of the lemma
% \begin{lemma}\label{lemma:l2_linfinity_error_bound_G_to_f_old}
% For $j \in [N]$, Let $t_{j} := \Delta j$ and $\gamma_{j} = \Delta$. Then, for some universal constant $C >0$ the following equations hold:  
% \bas{
%     & \sum_{i \in [m], k \in [N-1]}\nu_{i,k}^{2}\norm{G_{i,k+1}}_{2}^{2} + \sum_{i \in [m]}\bar{\nu}_{i}^{2}\norm{\bar{G}_i}_{2}^{2} \leq C\bb{L^{2}+ 1}\sum_{i=1}^{m}\sum_{j=1}^{N}\gamma_{j}\norm{f(x_{t_j}^{(i)})}_{2}^{2}
%     }
%     and,
%     \bas{
%     & \sup_{i \in [m],k\in[N-1]}\norm{G_{i,k+1}}_{2}\beta_{i,k}\sqrt{W_{i,k}} +  \sup_{i \in [m]}\norm{\bar{G}_i}\beta_{i,N}\sqrt{W_{i,N}} \\
%     & \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \leq 2e^{-\Delta}\bb{L+1}\sqrt{d\log\bb{\frac{2}{\delta}}} \sup_{i \in [m], \; j \in [N]}\sqrt{\gamma_{j}}\norm{f\bb{x_{t_j}^{(i)}}}_{2}}
% where $\nu_{i,k}, \bar{\nu}_{i}$ are as defined in Lemma~\ref{lemma:martingale_variance_bound} and $\beta_{i,k}, W_{i,k}$ are as defined in Lemma~\ref{lemma:subGaussianity_parameters}.
% \end{lemma}
% }

\begin{lemma}\label{lemma:l2_linfinity_error_bound_G_to_f}
For $j \in [N]$, Let $t_{j} := \Delta j$ and $\gamma_{j} = \Delta$. Then, for some universal constant $C >0$ the following equations hold:  
\bas{
    & \sum_{i \in [m], k \in [N]}\E[R_{i,k}^2|\mathcal{F}_{i,k-1}] \leq C\Delta^3(L\Delta+1+L^2)\bigr(\tfrac{N}{1-e^{-2\Delta}}+\tfrac{1}{(1-e^{-2\Delta})^2}\bigr)\sum_{i\in [m]}\sum_{j=1}^{N}\bigr\|\zeta(t_j,x_{t_j}^{(i)})\bigr\|^2
    }
% and 
%     \bas{
%     & \sup_{i \in [m],k\in[N]}|R_{i,k}|  \leq  C(L+1)\sqrt{\Delta}\log(\tfrac{1}{\Delta})\sqrt{d\sup_{i,k}W_{i,k}}\sup_{i,k}\|\zeta(t_k,x_{t_k}^{(i)})\|}
    and 
    \bas{
    \max\left(\sup_{i\in [m]}\beta_{i,N}\sqrt{W_{i,N}}\|\bar{G}_i\|, \sup_{\substack{i\in [m]\\k\in [N-1]}}\beta_{i,k}\sqrt{W_{i,k}}\|\bar{G}_{i,k+1}\|\right) \leq C(L+1)\sqrt{\Delta}\log(\tfrac{1}{\Delta})\sqrt{d\sup_{i,k}W_{i,k}}\sup_{i,k}\|\zeta(t_k,x_{t_k}^{(i)})\|
    }

\end{lemma}
% \dn{need to finish the last part of the statement, and propagate some changes to the statement} \syamantak{Fixed.}
\begin{proof}



Define $g_0^2:= (L\Delta^2 + \Delta + L^2\Delta)$. Applying Lemma~\ref{lemma:martingale_diff_variance_bound_appendix}, we conclude:
\ba{
&\sum_{i\in [m],k\in [N]}\E[R_{i,k}^2|\mathcal{F}_{i,k-1}] \lesssim \sum_{i\in [m]}(L\Delta^2 + \Delta) \|\bar{G}_i\|^2 + \sum_{i\in [m],k\in [N-1]}(L\Delta^2 + \Delta + L^2\Delta) e^{2t_{N-k+1}} \|G_{i,k+1}\|^2 \nonumber \\
&\lesssim g_0^2 \sum_{i\in[m]}\sum_{k=1}^{N}\biggr\|\sum_{j=k}^{N}\frac{\gamma_{j}e^{-(t_j-t_k)}\zeta(t_j,x_{t_j}^{(i)})}{\sigma^2_{t_j}}\biggr\|^2 = \Delta^2 g_0^2 \sum_{i\in[m]}\sum_{k=1}^{N}\biggr\|\sum_{j=k}^{N}\frac{e^{-(t_j-t_k)}\zeta(t_j,x_{t_j}^{(i)})}{\sigma^2_{t_j}}\biggr\|^2 \nonumber \\
&= \Delta^2 g_0^2 \sum_{i\in[m]}\sum_{k=1}^{N}\biggr\|\sum_{j=k}^{N}\frac{e^{-(t_j-t_k)}\zeta(t_j,x_{t_j}^{(i)})}{\sigma^2_{t_j}}\biggr\|^2 \nonumber \\
&\leq \Delta^2 g_0^2 \sum_{i\in[m]}\sum_{k=1}^{N}\biggr(\sum_{j=k}^{N}\frac{e^{-2(t_j-t_k)}}{\sigma^4_{t_j}}\biggr)\bigr(\sum_{j=k}^{N}\bigr\|\zeta(t_j,x_{t_j}^{(i)})\bigr\|^2 \bigr) \text{ , using Cauchy-Schwarz inequality} \nonumber \\
&= \Delta^2 g_0^2 \sum_{i\in[m]}\sum_{k=1}^{N}\biggr(\sum_{j=k}^{N}\frac{e^{-2\Delta(j-k)}}{(1-e^{-2j\Delta})^2}\biggr)\bigr(\sum_{j=k}^{N}\bigr\|\zeta(t_j,x_{t_j}^{(i)})\bigr\|^2 \bigr) \nonumber \\
&\leq \Delta^2 g_0^2 \sum_{i\in[m]}\sum_{k=1}^{N}\biggr(\sum_{j=k}^{N}\frac{e^{-2\Delta(j-k)}}{(1-e^{-2j\Delta})^2}\biggr)\bigr(\sum_{j=1}^{N}\bigr\|\zeta(t_j,x_{t_j}^{(i)})\bigr\|^2 \bigr) \nonumber \\
&\leq \Delta^2 g_0^2 \bigr(\tfrac{N}{1-e^{-2\Delta}}+\tfrac{1}{(1-e^{-2\Delta})^2}\bigr)\sum_{i\in [m]}\sum_{j=1}^{N}\bigr\|\zeta(t_j,x_{t_j}^{(i)})\bigr\|^2  \text{ using Lemma}~\ref{lemma:sum_bound_1}
}
    
Recall $\beta_{i,k},W_{i,k}$ as defined in Lemma~\ref{lemma:subGaussianity_parameters}. Applying these results along with the union bound we conclude with probability $1-\delta$, the following holds every $i,k$ simultaneously:


\bas{
&\max\left(\sup_{i\in [m]}\beta_{i,N}\sqrt{W_{i,N}}\|\bar{G}_i\|, \sup_{\substack{i\in [m]\\k\in [N-1]}}\beta_{i,k}\sqrt{W_{i,k}}\|\bar{G}_{i,k+1}\|\right) \\
&\leq C\sqrt{\Delta}(L+1)\sqrt{d\sup_{i,k}W_{i,k}} \max\left(\sup_{i,k}e^{t_{N-k+1}}\|G_{i,k}\|,\sup_i\|\bar{G}_i\|\right) \nonumber \\
&\leq C\sqrt{\Delta}(L+1)\sqrt{d\sup_{i,k}W_{i,k}}\left(\sum_{j=1}^{N}\tfrac{e^{-(t_j-t_1)}}{\sigma_{t_j}^2} \right)\sup_{i,k}\gamma_{k}\|\zeta(t_k,x_{t_k}^{(i)})\|\text{, using Holder's inequality} \nonumber \\
&= C\Delta^{3/2}(L+1)\sqrt{d\sup_{i,k}W_{i,k}}\left(\sum_{j=1}^{N}\tfrac{e^{-\Delta(j-1)}}{1-e^{-2j\Delta}} \right)\sup_{i,k}\|\zeta(t_k,x_{t_k}^{(i)})\| \nonumber \\
&\leq C(L+1)\sqrt{\Delta}\log(\tfrac{1}{\Delta})\sqrt{d\sup_{i,k}W_{i,k}}\sup_{i,k}\|\zeta(t_k,x_{t_k}^{(i)})\|, \text{ using Lemma~\ref{lemma:sum_bound_1}}
}


\end{proof}



% \begin{lemma}\label{lemma:variance_bound_H_HN}
% For $\gamma_{t} > 0$, let 
% \bas{
%     \forall k \in [N], \; R_{k} := \sum_{i \in [m]}r_{i}, \;\; r_{i} := \left\langle \sum_{j \geq N-k+1}\frac{\gamma_{j}}{\sigma_{t_j}^{2}}f(x_{t_j}^{(i)}), \; z_{t_{1}}^{(i)}-\E \bbb{z_{t_{1}}^{(i)}|x_{t_{1}}^{(i)}}\right\rangle
% }
% and define the $\sigma$-algebra $\mathcal{F}_k = \sigma(x^{(i)}_{t_j}: j \geq N-k+1,\; i \in [m])$. Then, $\forall k \in [N]$,
% \bas{
%     & \E\bbb{R_k|\cF_{k}} = 0, \text{ and } \\
%     & \E\bbb{R_k^{2}|\cF_{k}} = \sum_{i \in [m]}v_{i,k}^{\top}\E\bbb{\sigma^2_{t_1} \id + \sigma^4_{t_1}h_{t_1}(x_{t_1}^{(i)})|x_{t_{N-k+1}}^{(i)}}v_{i,k} 
% }
% where $v_{i,k} := \bb{\sum_{j \geq N-k+1}\frac{\gamma_{j}}{\sigma_{t_j}^{2}}f(x_{t_j}^{(i)})}$. 
% \end{lemma}
% \begin{proof}
% \bas{
%    \E\bbb{R_k|\mathcal{F}_k} &= \sum_{i \in [m]}\E\bbb{r_i|\mathcal{F}_k} \\
%    &= \sum_{i\in[m]}\left\langle \sum_{j \geq N-k+1}\frac{\gamma_{j}}{\sigma_{t_j}^{2}}f(x_{t_j}^{(i)}), \; \E\bbb{z_{t_{1}}^{(i)}-\E \bbb{z_{t_{1}}^{(i)}|x_{t_{1}}^{(i)}}|\mathcal{F}_k}\right\rangle \\
%    &= \sum_{i\in[m]}\left\langle \sum_{j \geq N-k+1}\frac{\gamma_{j}}{\sigma_{t_j}^{2}}f(x_{t_j}^{(i)}), \; \E\bbb{z_{t_{1}}^{(i)}-\E \bbb{z_{t_{1}}^{(i)}|x_{t_{1}}^{(i)}}|x_{t_{N-k+1}}^{(i)}}\right\rangle \\
%    &= 0
% }
% Let $v_{i,k} := \bb{\sum_{j \geq N-k+1}\frac{\gamma_{j}}{\sigma_{t_j}^{2}}f(x_{t_j}^{(i)})}$. Then, 
% \bas{
%     \E\bbb{R_k^{2}|\mathcal{F}_k} &= \sum_{i,j \in [m]}\E\bbb{r_ir_j|\mathcal{F}_k} \\
%     &= \sum_{i \in [m]}\E\bbb{r_i^{2}|\mathcal{F}_k}, \text{ since for } i \neq j, \; r_i \text{ is independent of } r_j \text{ given } \mathcal{F}_k \\
%     &= \sum_{i \in [m]}v_{i,k}^{\top}\E\bbb{\bb{z_{t_{1}}^{(i)}-\E \bbb{z_{t_{1}}^{(i)}|x_{t_{1}}^{(i)}}}\bb{z_{t_{1}}^{(i)}-\E \bbb{z_{t_{1}}^{(i)}|x_{t_{1}}^{(i)}}}^{\top}|x_{t_{N-k+1}}^{(i)}}v_{i,k} \\
%     &= \sum_{i \in [m]}v_{i,k}^{\top}\bb{\E\bbb{z_{t_1}^{(i)}z_{t_1}^{(i)\top}|x_{t_{N-k+1}}^{(i)}} - \sigma_{t_{1}}^{4}\E\bbb{s_{t_1}\bb{x_{t_1}^{(i)}}s_{t_1}\bb{x_{t_1}^{(i)}}^{\top}|x_{t_{N-k+1}}^{(i)}}}v_{i,k}
% }
% Using Second-Order Tweedie's formula, 
% \bas{
%     \E[z_{t_1}z_{t_1}^{\top}|x_{t_1}] - \sigma_{t_1}^4 s_{t_1}(x_{t_1})s^{\top}_{t_1}(x_{t_1}) = \sigma^2_{t_1} \id + \sigma^4_{t_1}h_{t_1}(x_{t_1})
% }
% Therefore, 
% \bas{
%     \E\bbb{R_k^{2}|\mathcal{F}_k} &= \sum_{i \in [m]}v_{i,k}^{\top}\E\bbb{\sigma^2_{t_1} \id + \sigma^4_{t_1}h_{t_1}(x_{t_1}^{(i)})|x_{t_{N-k+1}}^{(i)}}v_{i,k}
% }
% \end{proof}

% \section{Martingale Concentration}
% \label{appendix:martingale_concentration}

% \begin{lemma}\label{lemma:subgauss_moment_bound}
%     Let $Y$ be a $\bb{\beta^2,K}$-sub-gaussian random variable with $\bb{K \geq 1}$. Then, for any integer $k > 0$ and some universal constant $C > 0$:
%     \bas{\bE \bbb{Y^{2k}} \leq C^k K^k \beta^{2k} + C^k k! \beta^{2k}}
% \end{lemma}
% \begin{proof}
% By Definition~\ref{definition:new_subGaussianity_def}, for any \(A>0\),
% \[
%   \mathbb{P}(|Y|> A)\;\le\; e^K \exp\Bigl(-\tfrac{A^2}{2\beta^2}\Bigr).
% \]
% Using the tail-integration representation of moments, we have
% \[
%   \mathbb{E}[|Y|^{2k}]
%   \;=\;\int_{0}^{\infty} \mathbb{P}\bigl(|Y|^{2k} > t\bigr)\,dt
%   \;=\;\int_{0}^{\infty} \mathbb{P}\bigl(|Y| > t^{1/(2k)}\bigr)\,dt.
% \]
% Make the change of variables \( t = x^{2k}\) so that \(dt = 2k\,x^{2k-1}dx\).  Then
% \[
%   \mathbb{E}[|Y|^{2k}]
%   \;=\;\int_{0}^{\infty} 2k\,x^{2k-1}\,\mathbb{P}(|Y| > x)\,dx
%   \;\le\;
%   2k\,\int_{0}^{\infty} x^{2k-1}\,\min(1,e^K\exp\!\bigl(-\tfrac{x^2}{2\beta^2}\bigr))\,dx.
% \]

% Let $x_0 = \sqrt{2\beta^2K}$

% \bas{
% \E[|Y|^{2k}] &\leq 2k\int_{0}^{x_0}x^{2k-1}dx + 2k\int_{x_0}^{\infty}x^{2k-1}e^{K}e^{-\frac{x^2}{2\beta^2}} dx \\
% &= (2\beta^2K)^k + 2k\int_{x_0}^{\infty}x^{2k-1}e^{K}e^{-\frac{x^2}{2\beta^2}} dx \\
% &\leq (2\beta^2K)^k + 2k\int_{x_0}^{\infty}x^{2k-1}e^{-\frac{(x-x_0)^2}{2\beta^2}} dx \\
% &\leq (2\beta^2K)^k + 2^{2k-1}k\int_{x_0}^{\infty}(x_0^{2k-1}+(x-x_0)^{2k-1})e^{-\frac{(x-x_0)^2}{2\beta^2}} dx 
% }

% In the second step we have used the fact that whenever $x \geq x_0$, we must have $K-\frac{x^2}{2\beta^2} \leq -\frac{(x-x_0)^2}{2\beta^2}$. In the third step we have used the fact that $x^{2k-1} \leq 2^{2k-2}[(x-x_0)^{2k-1} + x_0^{2k-1}]$ whenever $x \geq x_0$.

% A standard Gamma-function integral yields
% \[
%   \int_{0}^{\infty} x^{2k-1}\,\exp\!\Bigl(-\tfrac{x^2}{2\beta^2}\Bigr)\,dx
%   \;=\;\frac12\,(2\beta^2)^k\,\Gamma(k),
% \]
% and for integer \(k\), \(\Gamma(k)= (k-1)!\).  Substituting this to the equation above, we conclude that for some universal constant $C_1$, we have:

% \bas{
% \E[|Y|^{2k}] &\leq (2\beta^2K)^k + C^k_1 (k\beta^{2k}K^{k-1/2} + \beta^{2k}k!)
% }
% We then conclude the result using the fact that $K \geq 1$ and $k \leq 2^k$.

% \end{proof}

% \dn{Recall the definition of $(\beta^2,K)$ subGaussianity here}

% \begin{lemma}
%     \label{lemma:mgf_bound}
%     Let $Y$ be a $\bb{\beta^{2}, K}$-subGaussianity random variable such that $K\geq 1$, $\E\bbb{Y} = 0$ and $\E\bbb{Y^2} \leq \nu^2$. Then, for a sufficiently small universal constant $c_{0} > 0$ such that, $\lambda \beta \leq c_0$, and any arbitrary $A > 0$, we have:
%     \bas{
%         \bE \exp(\lambda^2 Y^2) \leq 1 + \lambda^2 \nu^2 \exp(\lambda^2 A^2) + C\lambda^4\beta^4K^2\exp(\tfrac{K}{2}-\tfrac{A^2}{4\beta^2} + C\lambda^2\beta^2K)
%     }
% \end{lemma}
% \begin{proof}
% For some $\lambda > 0$, consider:

% \begin{equation}\label{eq:exp_moment}
%     \bE\bbb{\exp(\lambda^2 Y^2)} = 1 + \lambda^2 \nu^2 + \sum_{k\geq 2} \frac{\lambda^{2k}\bE \bbb{Y^{2k}}}{k!}
% \end{equation}

% Now, using Lemma~\ref{lemma:subgauss_moment_bound},  consider 
% \begin{align}
%     \bE \bbb{Y^{2k}} &= \bE \bbb{Y^{2k}\mathbbm{1}(|Y| > A)} + \bE\bbb{Y^{2k}\mathbbm{1}(|Y| \leq A)} \nonumber \\
% &\leq \sqrt{\bE \bbb{Y^{4k}}}\sqrt{\bP(|Y| > A)} + \bE\bbb{Y^2} A^{2k-2} \nonumber \\
% &= \sqrt{\bE \bbb{Y^{4k}}}\sqrt{\bP(|Y| > A)} + \nu^2 A^{2k-2} \nonumber \\
% &\leq \sqrt{C^{2k}\beta^{4k}(2k)! + C^{2k}\beta^{4k}K^{2k}}\exp(\tfrac{K}{2}-\tfrac{A^2}{4\beta^2}) + \nu^2 A^{2k-2} \nonumber \\
% &\leq \left((2C)^{k}k!\beta^{2k} + C^{k}\beta^{2k}K^k\right)\exp(\tfrac{K}{2}-\tfrac{A^2}{4\beta^2}) + \nu^2 A^{2k-2}
% \end{align} 

% Here, we have used the fact that $(2k)! \leq 4^k (k!)^2$. Plugging this back in Equation~\eqref{eq:exp_moment}, we conclude that whenever $\lambda\beta \leq c_0 $ for some small enough constant $c_0$, we have:
% \begin{equation}
%     \bE\bbb{\exp(\lambda^2 Y^2)} \leq 1 + \lambda^2 \nu^2 \exp(\lambda^2 A^2) + C\lambda^4\beta^4K^2\exp(\tfrac{K}{2}-\tfrac{A^2}{4\beta^2} + C\lambda^2\beta^2K)
% \end{equation}
% \end{proof}
% \begin{theorem}\label{theorem:mgf_bound_subgauss}
%       Let $Y$ be a $\bb{\beta^{2}, K}$-subGaussianity random variable such that $K\geq 1$, $\E\bbb{Y} = 0$ and $\E\bbb{Y^{2}} \leq \nu^{2}$. Set $A \geq \beta \sqrt{4\log(\tfrac{\beta K}{\nu})} + \beta\sqrt{2 K}$ and $\lambda \leq \frac{c_0}{A}$ for some small enough constant $c_0 > 0$. Then, there exists a constant $C$ such that:
%       \bas{
%         \bE\bbb{\exp(\lambda^2 Y^2)} \leq 1 + C\lambda^2 \nu^2
%       }
% \end{theorem}
% \begin{proof}

%    The result follows from Lemma~\ref{lemma:mgf_bound} substituting the values of $\lambda$ and $A$. 
% \end{proof}

% \martingaleconcentrationlemma*
% \begin{proof}
%     Let $L_{n} := \exp(\lambda M_{n} - C\lambda^2\sum_{i=1}^{n}\nu_i^2\|G_i\|^2 )\prod_{i=1}^{n}\mathbbm{1}(\mathcal{A}_i(\lambda))$. Then we have, 
%     \bas{
%         &\E\bbb{L_{n}\bigg|\mathcal{F}_{n-1}}
%         = L_{n-1}\E\bbb{\exp\bb{\lambda\langle G_n, Y_n - \bE[Y_n|\mathcal{F}_{n-1}] \rangle - C\lambda^{2}\nu_{n}^{2}\norm{G_n}^{2} }\mathbbm{1}\bb{\mathcal{A}_n\bb{\lambda}}\bigg|\mathcal{F}_{n-1}} \\
%         &= L_{n-1}\exp\bb{- C\lambda^{2}\nu_{n}^{2}\norm{G_n}^{2}}\E\bbb{\exp\bb{\lambda\langle G_n, Y_n - \bE[Y_n|\mathcal{F}_{n-1}] \rangle }\mathbbm{1}\bb{\{J_n\lambda \|G_n\|\beta_n\sqrt{K_n} \leq c_0\}}\bigg|\mathcal{F}_{n-1}} \\
%         &\leq L_{n-1}\exp\bb{- C\lambda^{2}\nu_{n}^{2}\norm{G_n}^{2}}\exp\bb{C\lambda^{2}\nu_{n}^{2}\norm{G_n}^{2}} \text{ using Theorem}~\ref{theorem:mgf_bound_subgauss} \text{ and the definition of } \mathcal{A}_{n}\bb{\lambda} \\
%         &\leq L_{n-1}
%     }
%     The second result follows from a standard Chernoff bound argument.
% \end{proof}
% \dn{should we refer to the lemma above instead of the one in the main text in the statement below?}

% \begin{lemma}\label{lemma:union_bound_lambda} 
%      Under the setting of Lemma~\ref{lemma:martingale_concentration_lemma}, let $\lambda^* := \sqrt{\frac{\alpha}{\sum_{i=1}^{n}\nu_i^2\|G_i\|^2}}$ and $\lambda_{\min} := \frac{c_0}{\sup_iJ_i\|G_i\|\beta_i\sqrt{K_i}}$. Let $B \in \mathbb{N}$ be arbitrary and consider the event: $\mathcal{B} = \{e^{-B} \leq \min(\lambda^*,\lambda_{\min}) \leq \max(\lambda^{*},\lambda_{\min}) \leq e^{B}\}$. Then, for some universal constant $C_1 > 0$ and any $\alpha > 0$,

%  $$\mathbb{P}\left(\{M_n > C_1\lambda^*\sum_{i=1}^{n}\nu_i^2 \|G_i\|^2 + C_1\tfrac{\alpha}{\lambda_{\min}}\}\cap\mathcal{B} \right) \leq  (2B+1)e^{-\alpha}$$ 
    
% \end{lemma}
% \begin{proof}
% We apply union bound over $\lambda \in \Lambda_B := \{e^{-B},e^{-B+1},\dots,e^{B}\}$. Using Lemma~\ref{lemma:martingale_concentration_lemma} along with a union bound,
% \bas{
%     \bP(\cup_{\lambda \in \Lambda_B}\{\lambda M_n > C \lambda^2\sum_{i=1}^{n}\nu_i^2\|G_i\|^2 + \alpha\}\cap_{i=1}^{n} \mathcal{A}_i(\lambda)) \leq (2B+1)\exp(-\alpha)
% }
% Consider the following events:
% \begin{enumerate}
%     \item Event 1: $\cE_1 := \{\max(\lambda^*,\lambda_{\min}) > e^B\}$
%     \item Event 2: $\cE_2 := \{\min(\lambda^*,\lambda_{\min}) < e^{-B}\}$
%     \item Event 3: $\cE_3 := \{e^{-B} \leq \lambda^* < \lambda_{\min} \leq e^B\}$
%     \item Event 4: $\cE_4 := \{e^{-B} \leq \lambda_{\min} < \lambda^* \leq e^B\}$
% \end{enumerate}

% %Now, let us consider the probability of the event $\{M_n > C_1\lambda^*\sum_{i=1}^n\nu_i^2 \|G_i\|^2 + C_1\frac{\alpha}{\lambda_{\min}}\}$ for some large enough constant $C_1$.
% In the event $\cE_4$, almost surely there exists a random $\bar{\lambda} \in \Lambda_B$ such that $\bar{\lambda}/\lambda_{\min} \in [\frac{1}{e},e]$ and such that the event $\cap_{i=1}^{n}\mathcal{A}_i(\bar{\lambda})$ holds. Thus, we have:

% \begin{align}
% &\{M_n > Ce\lambda^* \sum_i \nu_i^2\|G_i\|^2 + \frac{e\alpha}{\lambda_{\min}}\}\cap \cE_4 \subseteq  \{M_n > Ce \lambda_{\min}\sum_i \nu_i^2 \|G_i\|^2 + \frac{e\alpha}{\lambda_{\min}}\}\cap \cE_4 \nonumber \\
% &\subseteq \{M_n > C\bar{\lambda}\sum_i \nu_i^2 \|G_i\|^2 + \frac{\alpha}{\bar{\lambda}}\}\cap \cE_4 = \{M_n > C \bar{\lambda}\sum_i \nu_i^2 \|G_i\|^2 + \frac{\alpha}{\bar{\lambda}}\}\cap \cE_4 \cap_{i=1}^n \cA_i(\bar{\lambda}) \nonumber \\
% &\subseteq \cE_4\cap\left(\cup_{\lambda \in \Lambda_B}\{\lambda M_n > C \lambda^2\sum_{i=1}^{n}\nu_i^2\|G_i\|^2 + \alpha\}\cap_{i=1}^{n} \mathcal{A}_i(\lambda)\right)
% \end{align}

% Similarly, under the event $\cE_3$, there exists a random $\bar{\lambda}^*\in \Lambda_B$ such that: $\bar{\lambda}^*/\lambda^* \in [\frac{1}{e},e]$, such that the event $\cap_i \cA_i(\bar{\lambda}^*)$ holds. Therefore, we must have:

% \begin{align}
% &\{M_n > Ce\lambda^* \sum_i \nu_i^2\|G_i\|^2 + \frac{e\alpha}{\lambda^*}\}\cap \cE_3 \subseteq \{M_n > C \bar{\lambda}^*\sum_i \nu_i^2 \|G_i\|^2 + \frac{\alpha}{\bar{\lambda}^*}\}\cap \cE_3 \nonumber \\ &= \{M_n > C \bar{\lambda}^*\sum_i \nu_i^2 \|G_i\|^2 + \frac{\alpha}{\bar{\lambda}^*}\}\cap \cE_3 \cap_{i=1}^n \cA_i(\bar{\lambda}^*) \nonumber \\
% &\subseteq \cE_3\cap\left(\cup_{\lambda \in \Lambda_B}\{\lambda M_n > C \lambda^2\sum_{i=1}^{n}\nu_i^2\|G_i\|^2 + \alpha\}\cap_{i=1}^{n} \mathcal{A}_i(\lambda)\right)
% \end{align}

% Notice that $\lambda^*$ is chosen such that 

% \begin{align}
% Ce\lambda^* \sum_i \nu_i^2\|G_i\|^2 + \frac{e\alpha}{\lambda^*} &= e(C+1)\sqrt{\alpha(\sum_i \nu_i^2 \|G_i\|^2)} \nonumber \\
% &= e(C+1)\lambda^* \sum_i \nu_i^2 \|G_i\|^2 \\
% &\leq e(C+1)\lambda^* \sum_i \nu_i^2 \|G_i\|^2 + \frac{e \alpha}{\lambda_{\min}}
% \end{align}


% Combining these equations, we conclude that for some constant $C_1 > 0$, we must have
% $$\{M_n > C_1(\lambda^*\sum_{i=1}^{n}\nu_i^2 \|G_i\|^2 + \frac{\alpha}{\lambda_{\min}}) \}\cap(\cE_3\cup\cE_4) \subseteq \left(\cup_{\lambda \in \Lambda_B}\{\lambda M_n > C \lambda^2\sum_{i=1}^{n}\nu_i^2\|G_i\|^2 + \alpha\}\cap_{i=1}^{n} \mathcal{A}_i(\lambda)\right)\cap(\cE_3\cup \cE_4) $$

% Noting that $\mathcal{B} = \cE_3\cup\cE_4$, we conclude the result.
 
% % The event $\cE_1$ is such that for large enough $B$, $\|G_i\|$ is already very small. This forms the second result of the lemma. The event $\cE_2$ is such that $\|G_i\|$ is very large, which we will show is not possible using the bounds on $\norm{G_{i}}$.

% % Note that under $\mathcal{E}_{2}$, 
% % \bas{
% %     \max\left\{\sqrt{\frac{\sum_{i}\nu_i^{2}\norm{G_i}^{2}}{\alpha}}, \frac{\sup_i\|G_i\|\beta_i\sqrt{K_i}}{c_0} \right\} \geq e^{B}
% % }
% % which contradicts our assumption on $\norm{G_i}$.

% % Assume that $\|f(X_t)-f(0)\| \leq 2L\|X_t\|$ and $\|f(0)\| \leq C\poly(d)$. When $X_t$ has second moment, then..
% \end{proof}

 We will specialize the setting in Lemma~\ref{lemma:union_bound_lambda} with $M_n$ being given by $H$, the filtration being $\mathcal{F}_{ik}$ and the martingale decomposition given in Lemma~\ref{lemma:error_martingale_decomposition_2_actual_appendix}. Similarly, $\beta_{i}$ corresponds to $(\beta_{i,k})_{i,k}$, $K_i$ corresponds to  $(W_{i,k})_{i,k}$ given in Lemma~\ref{lemma:subGaussianity_parameters}. $\nu_{i}^2$ corresponds to the upper bound on $\E[R_{i,k}^2|\mathcal{F}_{i,k}]$ in Lemma~\ref{lemma:martingale_diff_variance_bound_appendix}. Therefore, $J_i$ corresponds to $\max(1,\frac{C}{W_{i,k}}\log(\frac{\beta_{i,k}^2W_{i,k}}{\nu_{i,k}^2}))$ satisfies $J_i \leq C\log(2d)$ for some constant $C$. In this case, the quantity $\sum_{i=1}^{n}\nu_i^2\|G_i\|^2 $ as given in Lemma~\ref{lemma:union_bound_lambda} corresponds to $\sum_{i,k}\E[R_{i,k}^2|\mathcal{F}_{i,k-1}]$ and it can be bound using Lemma~\ref{lemma:l2_linfinity_error_bound_G_to_f}:
\bas{
    & \sum_{i \in [m], k \in [N]}\E[R_{i,k}^2|\mathcal{F}_{i,k-1}] \leq C\Delta^3(L\Delta+1+L^2)\bigr(\tfrac{N}{1-e^{-2\Delta}}+\tfrac{1}{(1-e^{-2\Delta})^2}\bigr)\sum_{i\in [m]}\sum_{j=1}^{N}\bigr\|\zeta(t_j,x_{t_j}^{(i)})\bigr\|^2
    }

Similarly, we adapt $\lambda_{\min},\lambda^{*}$ be the random variables defined in Lemma~\ref{lemma:union_bound_lambda} to our case for some arbitrary $B\in \mathbb{N},\alpha > 1$. This lemma demonstrates the concentration of the quantity $H$ conditioned on the event $\mathcal{B}:= \{\lambda_{\min} ,\lambda^{*} \in [e^{-B},e^{B}]\}$. It remains to deal with the following cases:
\begin{enumerate}
    \item $\max(\lambda_{\min},\lambda^{*}) > e^{B}$
    \item $\min(\lambda_{\min},\lambda^{*}) < e^{-B}$
\end{enumerate}


First, consider the case $\max(\lambda_{\min},\lambda^{*}) > e^{B}$. 
\begin{lemma}\label{lemma:large_lambda}
Assume $\gamma_{t} = \Delta$, $\Delta< c_0$ for some universal constant $c_0$. Then $\max(\lambda_{\min},\lambda^{*}) > e^{B}$ implies 

$$\sum_{i \in [m], t\in \timeset}\|\zeta(t,x_t^{(i)})\|^2 \leq \frac{CNm\alpha}{\Delta}e^{-2B}$$
\end{lemma}
\begin{proof}
Using the fact that $\alpha >1$, we note that  
\begin{align}
\max(\lambda_{\min},\lambda^{*}) &> e^{B} \nonumber\\
\implies \max\bigr(\sup_{\substack{i\in [m]\\ k\in [N-1]}}\sqrt{\Delta}e^{t_{N-k+1}}\|G_{i,k+1}\|,\sup_{i\in [m]}\sqrt{\Delta}\|\bar{G}_{i}\|\bigr) &\leq C\sqrt{\alpha}e^{-B} \text{ for some universal constant } C
\end{align} 

By defining $G_{i,0} = 0$ and , we note that $\sigma_{t_{N-k+1}}^2e^{t_{N-k+1}}(G_{i,k+1}-G_{i,k}) = \zeta(t_{N-k+1},x_{t_{N-k+1}}^{(i)})$ for $k < N$ and $\sigma_{t_1}^2(\bar{G}_i-e^{t_1}G_{i,N-1}) = \zeta(t_1,x_{t_1})$. Using the fact that $\sigma_{t_k}^2 \leq 1$ for some universal constant $c_0$, we conclude that  

\begin{align}
\max(\lambda_{\min},\lambda^{*}) &> e^{B} \nonumber\\
\implies \sup_{i\in [m],t\in \timeset}\|\zeta(t,x_t^{(i)})\| &\leq C\sqrt{\frac{\alpha}{\Delta}}e^{-B} \nonumber \\
\implies \sum_{i \in [m], t\in \timeset}\|\zeta(t,x_t^{(i)})\|^2 &\leq \frac{CNm\alpha}{\Delta}e^{-2B}
\end{align} 

\end{proof}


We now consider the event $\min(\lambda^{*},\lambda_{\min}) < e^{-B}$.

\begin{lemma}\label{lemma:small_lambda}
Assume $\gamma_{t} = \Delta$, $t_j = j\Delta$, $\Delta < c_0$ for some universal constant $c_0$, $\alpha > 1$.  $\min(\lambda^{*},\lambda_{\min}) < e^{-B}$ implies:
$$\sum_{i \in [m], t\in \timeset}\|\zeta(t,x_t^{(i)})\|^2 \geq e^{2B}\frac{\Delta}{md N^2 \log^2(2d)(L+1)^2\sup_{i,k}W_{i,k}}$$
\end{lemma}
\begin{proof}
It is easy to show that $\min(\lambda^{*},\lambda_{\min}) < e^{-B}$ implies:
 $$\max(\sup_{i,k}e^{2t_{N-k+1}}\|G_{i,k+1}\|,\sup_i\|\bar{G}_i\|) \geq C\frac{e^{B}}{\log(2d)(L+1)\sqrt{m\Delta d} \sup_{i,k}\sqrt{W_{i,k}}}$$
 This implies that there exists $i,k$ such that $$\frac{\|\zeta(t_k,x_{t_k}^{(i)})\|}{\sigma_{t_k}^2} \geq C\frac{e^{B}}{N\log(2d)(L+1)\sqrt{m\Delta d} \sup_{i,k}\sqrt{W_{i,k}}} $$
 We then conclude the result using the fact that $\sigma_{t_k}^2 \geq c_0\Delta$
\end{proof}

\begin{lemma}\label{lemma:conditional_concentration}
Assume $N\Delta > 1$, $\Delta < c_0$ for some universal constant $c_0$.  Assume $t_j = \Delta j$ and $\gamma_{j} = \Delta$. Let $\alpha > 1$ and $B \in \mathbb{N}$. Let $\mathbb{L}_2^2(\zeta) := \sum_{i\in[m],t\in\timeset}\|\zeta(t,x^{(i)}_{t})\|^2$, $\mathbb{L}_{\infty}(\zeta) := \sup_{i\in[m],t\in\timeset}\|\zeta(t,x_{t}^{(i)})\|$. Let $\sigma_{\max} := \log(\tfrac{1}{\Delta})\log(2d)\sqrt{d\Delta\sup_{i,k}W_{i,k}}$. Then with probability $1-(2B+1)e^{-\alpha}$, at least one of the following inequalities hold:

    \begin{enumerate}
        \item $$\frac{H}{L+1} \leq C\sqrt{\alpha N\Delta^2 \mathbb{L}_2^2(\zeta)}+C\alpha\mathbb{L}_{\infty}(\zeta)\sigma_{\max}$$
        \item $$\mathbb{L}_2^2(\zeta) \leq \frac{CNm\alpha}{\Delta}e^{-2B}$$
        \item $$\mathbb{L}_2^2(\zeta)\geq c_0\frac{\Delta e^{2B}}{md N^2 \log^2(2d)(L+1)^2\sup_{i,k}W_{i,k}}$$
    \end{enumerate}
\end{lemma}

\begin{proof}
    As considered in Lemma~\ref{lemma:union_bound_lambda}, define the event $\mathcal{B} := \{\lambda_{\min} , \lambda^* \in [e^{-B},e^B]\}$. Applying Lemma~\ref{lemma:union_bound_lambda} to our case with the martingale increments as defined in the discussion above, along with bounds for the quantities $\sum_{i=1}^{n}\nu_i^2\|G_i\|^2$ and $\sup_{i}J_i \beta_i \sqrt{K_i}\|G_i\|$ as developed in Lemma~\ref{lemma:l2_linfinity_error_bound_G_to_f}, we conclude that:
\begin{enumerate}
    \item Almost surely $$\sum_{i=1}^{n}\nu_i^2\|G_i\|^2 \leq C N \Delta^2(L+1)^2\sum_{i\in [m], t\in \timeset}\|\zeta(t,x^{(i)}_{t})\|^2$$ 
    \item Almost surely
    $$\sup_i J_i \beta_i \|G_i\|\sqrt{K_i} \leq C(L+1)\log(2d)\log(\tfrac{1}{\Delta})\sqrt{d\Delta\sup_{i,k}W_{i,k}}\sup_{i\in[m],t\in\timeset}\|\zeta(t,x_{t}^{(i)})\|$$ 
\end{enumerate}


    
    \begin{equation}\label{eq:conc_mart}
    \bP\left(\biggr\{\frac{H}{L+1} > C\sqrt{\alpha N\Delta^2 \mathbb{L}_2^2(\zeta)}+C\alpha\mathbb{L}_{\infty}(\zeta)\sigma_{\max}\biggr\} \cap \mathcal{B}\right) \leq (2B+1)e^{-\alpha}
    \end{equation}

Define the events $\mathcal{B}_1 := \{\max(\lambda_{\min},\lambda^{*}) > e^B\}$, $\mathcal{B}_2 := \{\min(\lambda_{\min},\lambda^{*}) < e^{-B}\}$, $\mathcal{A} = \biggr\{\frac{H}{L+1} > C\sqrt{\alpha N\Delta^2 \mathbb{L}_2^2(\zeta)}+C\alpha\mathbb{L}_{\infty}(\zeta)\sigma_{\max}\biggr\}$. By Lemma~\ref{lemma:large_lambda}, the event  $\{\mathbb{L}_2^2(\zeta) > \frac{CNm\alpha}{\Delta}e^{-2B}\} \subseteq \mathcal{B}_1^{\complement}$. By Lemma~\ref{lemma:small_lambda}, the event: $$\biggr\{\mathbb{L}_2^2(\zeta)\geq \frac{\Delta e^{2B}}{md N^2 \log^2(2d)(L+1)^2\sup_{i,k}W_{i,k}}\biggr\} \subseteq \mathcal{B}_2^{\complement}$$

Therefore consider complement of the event of interest in the statement of the lemma: 
\begin{align*}
&\mathcal{A}\cap\biggr\{\mathbb{L}_2^2(\zeta) > \frac{CNm\alpha}{\Delta}e^{-2B}\biggr\} \cap \biggr\{\mathbb{L}_2^2(\zeta)\geq \frac{\Delta e^{2B}}{md N^2 \log^2(2d)(L+1)^2\sup_{i,k}W_{i,k}}\biggr\} \\
&\subseteq \mathcal{A}\cap \mathcal{B}_1^{\complement} \cap \mathcal{B}^{\complement}_2 \\
&= \left(\mathcal{A}\cap\mathcal{B}\cap\mathcal{B}_1^{\complement} \cap \mathcal{B}^{\complement}_2\right)\cup\left(\mathcal{A}\cap\mathcal{B}^{\complement}\cap\mathcal{B}_1^{\complement} \cap \mathcal{B}^{\complement}_2\right)
\end{align*}

Clearly, $\mathbb{P}(\mathcal{B}\cup \mathcal{B}_1 \cup \mathcal{B}_2) = 1$. This implies $\mathbb{P}(\mathcal{B}^{\complement}\cap \mathcal{B}^{\complement}_1 \cap \mathcal{B}^{\complement}_2) = 0$. Therefore, using the above inclusions along with Equation~\eqref{eq:conc_mart} we conclude:

$$\mathbb{P}\biggr(\mathcal{A}\cap\biggr\{\mathbb{L}_2^2(\zeta) > \tfrac{CNm\alpha}{\Delta}e^{-2B}\biggr\} \cap \mathcal{B}^{\complement}_2\biggr) \leq \mathbb{P}(\mathcal{A}\cap\mathcal{B}) \leq (2B+1)e^{-\alpha}$$
\end{proof}


\section{Convergence of Empirical Risk Minimization}
\label{appendix:convergence_erm}

\squarederrorboundmartingale*
\begin{proof} Let $y_t^{(i)} := -\frac{z_t^{(i)}}{\sigma_t^2}$.

    
    We have, for any $f \in \cF$,
    \ba{
        \widehat{\mathcal{L}}\bb{f} &= \widehat{\mathcal{L}}\bb{s} + \mathcal{L}\bb{f} + \sum_{i \in [m], j \in [N]}\frac{\gamma_{j}\inner{f\bb{t_j,x_{t_j}^{(i)}}-s\bb{t_j,x_{t_j}^{(i)}}}{s\bb{t_j,x_{t_j}^{(i)}}-y_{t_j}^{(i)}}}{m} \label{eq:erm_decomposition}
    }
    where $\widehat{\mathcal{L}}\bb{s} := \sum_{i \in [m], j \in [N]}\frac{\gamma_{j}\norm{s\bb{t_j, x_{t_j}^{(i)}}-y_{t_j}^{(i)}}_{2}^{2}}{m}$. Since $\hat{f}$ is the minimizer, $\widehat{\mathcal{L}}\bb{\hat{f}} \leq \widehat{\mathcal{L}}\bb{s}$. Therefore, 
    \bas{
        \mathcal{L}\bb{\hat{f}} \leq \sum_{i \in [m], j \in [N]}\frac{\gamma_{j}\inner{\hat{f}\bb{t_j,x_{t_j}^{(i)}}-s\bb{t_j, x_{t_j}^{(i)}}}{y_{t_j}^{(i)} -s\bb{t_j,x_{t_j}^{(i)}}}}{m}
    }
    which completes our proof.
\end{proof}


We will first demonstrate a very crude bound, which will be of use later to derive a finer bound based on Martingale concentration developed in previous sections. 
\begin{lemma}\label{lemma:squared_error_crude_bound}
    Fix $\delta \in \bb{0,1}$ and let $y_{t} := \frac{-z_{t}}{\sigma_t^{2}}$,  $\forall t\in \timeset, \gamma_{t} := \Delta < 1$. Furthermore, assume a linear discretization, i.e, $t_{j} = \Delta j$. For $\mathcal{L}, \widehat{\mathcal{L}}$ as defined in Lemma~\ref{lemma:l2errorboundmartingale} and $\hat{f} := \argmin_{f \in \mathcal{F}}\widehat{\mathcal{L}}\bb{f}$, we have almost surely:
    \bas{
        \mathcal{L}\bb{\hat{f}} \leq \widehat{\mathcal{L}}\bb{s} 
    }
    we have with probability atleast $1-\delta$, 
    \bas{
\widehat{\mathcal{L}}\bb{s} \leq C(N\Delta + \log(\tfrac{1}{\Delta}))d\log(\tfrac{mN}{\delta}) }
\end{lemma}
\begin{proof}



    Using Lemma~\ref{lemma:l2errorboundmartingale} and the Cauchy-Schwarz inequality, 
    \bas{
        \mathcal{L}\bb{\hat{f}} \leq \sum_{i \in [m], j \in [N]}\frac{\gamma_{j}\inner{\hat{f}\bb{t_j, x_{t_j}^{(i)}}-s\bb{t_j, x_{t_j}^{(i)}}}{y_{t_j}^{(i)} -s\bb{t_j, x_{t_j}^{(i)}}}}{m} \leq \sqrt{\mathcal{L}\bb{\hat{f}}\widehat{\mathcal{L}}\bb{s}}
    }
    which completes the first part of the proof. Next, we have
    \bas{
        \widehat{\mathcal{L}}\bb{s} &= \sum_{i \in [m], j \in [N]}\frac{\gamma_{j}\norm{s\bb{t_j, x_{t_j}^{(i)}}-y_{t_j}^{(i)}}_{2}^{2}}{m} 
    }
    Clearly, since $y_t^{(i)}$ is marginally Gaussian , we conclude that it is $\frac{4\sqrt{d}}{\sigma_t}$ norm subGaussian (see Definition~\ref{definition:new_subGaussian_def}). Using the fact that $s(t,x_t)$ is the conditional expectation of $y_t^{(i)}$, Lemma F.3. in \cite{gupta2023sample} shows that $s(t,x_t)$ is $4\sqrt{d}/\sigma_{t}$-norm subGaussian. Therefore applying a union bound over all $\|s(t,x_t^{(i)})\|,\|y_t^{(i)}\| \gtrsim \frac{\sqrt{d\log(\frac{|\timeset|m}{\delta})}}{\sigma_t}$, with probability at-least $1-\delta$ the following holds:
    \bas{ \sum_{i \in [m]}\sum_{t\in \timeset}\frac{\norm{s\bb{t, x_{t}^{(i)}}-y_{t}^{(i)}}_{2}^{2}}{m} \lesssim \Delta d \log(\tfrac{Nm}{\delta})\sum_{t\in \timeset}\frac{1}{\sigma_t^2}
    }

Now, note the fact that $\sigma_t \geq c_0\min(1,t)$ for some universal constant $c_0$. Therefore, $\sum_{t\in \timeset} \frac{1}{\sigma_t^2} \lesssim N + \frac{\log(\tfrac{1}{\Delta})}{\Delta}$. Plugging this into the equation above, we conclude the result. 
% \dn{assume $\Delta < 1$ and that $N\Delta > 1$}\syamantak{Added $\Delta < 1$. Do we need the second assumption?}

%%%%%%%%%%%%%%%%%%%%%%%
    % Therefore, $\norm{s\bb{t, x_{t}^{(i)}}-y_{t}^{(i)}}_{2}^{2}$ is a subexponential random variable. By Bernstein's inequality, with probability atleast $1-\delta$, for any $t \in \mathcal{T}$,
    % \bas{
    %     \sum_{i \in [m]}\frac{\norm{s\bb{t, x_{t}^{(i)}}-y_{t}^{(i)}}_{2}^{2} - \E\bbb{\norm{s\bb{t, x_{t}^{(i)}}-y_{t}^{(i)}}_{2}^{2}}}{m} \lesssim \frac{1}{\sigma_{t}^{2}}\sqrt{\frac{d\log\bb{\frac{1}{\delta}}}{m}} 
    % }
    % Therefore, 
    % \bas{
    %     \sum_{i \in [m]}\frac{\norm{s\bb{t, x_{t}^{(i)}}-y_{t}^{(i)}}_{2}^{2}}{m} \lesssim \frac{d}{\sigma_t^{2}} +  \frac{1}{\sigma_{t}^{2}}\sqrt{\frac{d\log\bb{\frac{1}{\delta}}}{m}}
    % }
    % Finally, note that based on the choice of $\gamma_{t}$,   
    % \bas{
    %     \sum_{i \in [m]}\frac{\gamma_{t}\norm{s\bb{t, x_{t}^{(i)}}-y_{t}^{(i)}}_{2}^{2}}{m} = \Delta \sum_{i \in [m]}\frac{\norm{s\bb{t, x_{t}^{(i)}}-y_{t}^{(i)}}_{2}^{2}}{m}
    % }
    % Therefore, 
    % \bas{
    %     \sum_{i \in [m], t \in \timeset}\frac{\gamma_{t}\norm{s\bb{t, x_{t}^{(i)}}-y_{t}^{(i)}}_{2}^{2}}{m} &\lesssim \Delta\bb{d + \sqrt{\frac{d\log\bb{\frac{1}{\delta}}}{m}}}\sum_{t \in \timeset}\frac{1}{\sigma_{t}^{2}} \\
    %     &= \Delta\bb{d + \sqrt{\frac{d\log\bb{\frac{1}{\delta}}}{m}}}\sum_{j=1}^{N}\frac{1}{1-e^{-2\Delta j}}  \\
    %     &= \Delta\bb{d + \sqrt{\frac{d\log\bb{\frac{1}{\delta}}}{m}}}\bb{\frac{1}{1-e^{-2\Delta}} +  \sum_{j=2}^{N}\frac{1}{1-e^{-2\Delta j}}} \\
    %     &\leq \Delta\bb{d + \sqrt{\frac{d\log\bb{\frac{1}{\delta}}}{m}}}\bb{\frac{1}{1-e^{-2\Delta}} +  \int_{1}^{N}\frac{d x}{1-e^{-2\Delta x}}}
    % }
    % Therefore, 
    % \bas{
    %   \sum_{i \in [m], t \in \timeset}\frac{\gamma_{t}\norm{s\bb{t, x_{t}^{(i)}}-y_{t}^{(i)}}_{2}^{2}}{m} &\lesssim \Delta\bb{d + \sqrt{\frac{d\log\bb{\frac{1}{\delta}}}{m}}}\bb{\frac{1}{1-e^{-2\Delta}} +  \frac{1}{\Delta}\log\bb{\frac{e^{-2\Delta}}{1-e^{-2\Delta}}}} \\
    %   &\lesssim \bb{d + \sqrt{\frac{d\log\bb{\frac{1}{\delta}}}{m}}}\log\bb{\frac{1}{\Delta}}
    % }
\end{proof}

\begin{lemma}\label{lemma:l2errorboundprob}
    Recall $y_t^{(i)}:= \frac{-z_t^{(i)}}{\sigma_t^2}$ for all $t \in \timeset$. Let for $f \in \cF$, 
    \bas{
    H^{f} := \sum_{i \in [m], j \in [N]}\frac{\gamma_{j}\inner{f\bb{t_j, x_{t_j}^{(i)}}-s\bb{t_j, x_{t_j}^{(i)}}}{y_{t_j}^{(i)} -s\bb{t_j, x_{t_j}^{(i)}}}}{m}
    }
    Then, for $\epsilon > 0$, 
    \bas{
         \mathbb{P}\bb{H^{\hat{f}} \geq \epsilon} \leq \mathbb{P}\bb{\bigcup_{f \in \mathcal{F}}\left\{H^{f} \geq \epsilon\right\}\bigcap \left\{\mathcal{L}\bb{f} \leq \widehat{\mathcal{L}}\bb{s}\right\} }
    }
    where $\mathcal{L}, \widehat{\mathcal{L}}, \hat{f}$ are defined in Lemma~\ref{lemma:l2errorboundmartingale}.
\end{lemma}
\begin{proof}

From Lemma~\ref{lemma:squared_error_crude_bound}, we must have $\mathcal{L}(\hat{f}) \leq \hat{\mathcal{L}}(s)$. Therefore:
    \bas{
        \mathbb{P}\bb{H^{\hat{f}} \geq \epsilon} &\leq \mathbb{P}\bb{\bigcup_{f \in \cF}\left\{H^{f} \geq \epsilon\right\}\bigcap \left\{\text{f is a minimizer of } \widehat{\mathcal{L}}\right\} } \\
        &\leq \mathbb{P}\bb{\bigcup_{f \in \cF}\left\{H^{f} \geq \epsilon\right\}\bigcap \left\{\mathcal{L}\bb{f} \leq \widehat{\mathcal{L}}\bb{s}\right\} }
    }
\end{proof}

% \begin{lemma}
% \label{lemma:martingale_conditioning}
% For any function $f : \R^{d} \rightarrow \R^{d}$, and the sigma algebra $\mathcal{F}_j := \sigma\left\{x_{t}^{(i)}: i \in [m], t \geq t_{N-j+1}\right\}$ we have
% \begin{enumerate}
%     \item 
%     If $t \leq t_{N-j+1}$, then $$\E[ \langle f(x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|\mathcal{F}_j] = 0$$
%     \item If $t > t_{N-j+1}$, then
%     $$\E[ \langle f(x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|\mathcal{F}_j] =  e^{-t}\langle f(x_t^{(i)}), \E[x_0^{(i)}|x_t^{(i)}] - \E[x_0^{(i)}|x_{t_{N-j+1}}^{(i)}] \rangle $$
% \end{enumerate}
% \end{lemma}
% \begin{proof}
% \begin{enumerate}
%     \item Using the fact that $x_t^{(i)}$ forms a Markov process, we consider:
% \begin{align}&\E[ \langle f(x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|\mathcal{F}_j] = \E[ \langle f(x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|x^{(i)}_{t_{N-j+1}}] \nonumber \\
% &= \E\left[\E[ \langle f(x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|x_t^{(i)},x^{(i)}_{t_{N-j+1}}]\bigr|x^{(i)}_{t_{N-j+1}}\right]
% \end{align} 

% In the second step, we have used the tower property of the conditional expectation. Now, $z_t^{(i)} = x_t^{(i)}-e^{-t}x_0^{(i)}$. By the Markov Property, we have: $\E[x_0^{(i)}|x_t^{(i)},x_{t_{j-N+1}}^{(i)}] = \E[x_0^{(i)}|x_t^{(i)}]$. Plugging this in, we have: 
% \begin{align}\E[ \langle f(x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|\mathcal{F}_j] 
% &= \E\left[\E[ \langle f(x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|x_t^{(i)}]\bigr|x^{(i)}_{t_{N-j+1}}\right] \nonumber \\
% &= 0
% \end{align} 

%     \item Notice that $z_t^{(i)} = x_t^{(i)}-e^{-t}x_0^{(i)}$. Clearly, $x_t^{(i)}$ is measurable with respect to $\mathcal{F}_j$. Therefore, 
% $$\E[ \langle f(x_t^{(i)}) ,z_t^{(i)} - \E[z_t^{(i)}|x_t^{(i)}] \rangle|\mathcal{F}_j] = - e^{-t}\langle f(x_t^{(i)}), \E[x_0^{(i)}|\mathcal{F}_j] - \E[x_0^{(i)}|x_t^{(i)}] \rangle $$
%     Now, consider the fact that $x_0^{(i)},x_{t_1}^{(i)},...$ is a Markov chain. Therefore, the Markov property states that $x_0^{(i)}|x_{s}^{(i)}: s\geq t $ has the same law as $x_0^{(i)}| x_t^{(i)}$. Therefore, we must have: $\E[x_0^{(i)}|\mathcal{F}_j] = \E[x_0^{(i)}|x^{(i)}_{t_{j-N+1}}]$. Plugging this into the display equation above, we conclude the result
% \end{enumerate}
% \end{proof}

% \begin{lemma}\label{lemma:martingale_decomposition_1}
%     Let 
%     \bas{
%     H^{f} := \sum_{i \in [m], t \in \timeset}\frac{\gamma_{t}\inner{f\bb{t, x_{t}^{(i)}}-s\bb{t, x_{t}^{(i)}}}{y_{t}^{(i)} -s\bb{t, x_{t}^{(i)}}}}{m}
%     } 
%     and define the filtration $\mathcal{F}_{j} := \sigma\left\{x_{t}^{(i)} : i \in [m], t \geq t_{N-j+1}\right\}$. Further, define $H_{j} := \E\bbb{H^{f}|\mathcal{F}_j}$. Then, 
%     \bas{
%         H^{f} = \bb{H^{f}-H_{N}} + \sum_{k=2}^{N}\bb{H_k - H_{k-1}}
%     }
%     such that 
%     \bas{
%         H^{f}-H_N &= \sum_{i=1}^{m}\sum_{j=1}^N \frac{e^{-(t_j-t_1)}\gamma_{j}}{\sigma_{t_j}^{2}}\langle f(x_{t_j}^{(i)}),z^{(i)}_{t_1}-\E[z^{(i)}_{t_1}|x^{(i)}_{t_1}]\rangle \\
%         H_k - H_{k-1} &= \sum_{i=1}^{m}\sum_{j=N-k+2}^N \frac{e^{-t_{j}}\gamma_{j}}{\sigma_{t_j}^{2}}\langle f(x_{t_j}^{(i)}),\E[x_0^{(i)}|x_{t_{N-k+2}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]\rangle
%     }
%     where $f := f_{t} - s_{t}$. 

% \end{lemma}
% \begin{proof}
%     The proof follows due to Lemma~\ref{lemma:martingale_conditioning}.
% \end{proof}

% \errormartingaledecomposition*
% \begin{proof}
%     We first note that for $1 < k \leq N-1$, $\mathcal{F}_{i, k-1} := \sigma\left\{x_{t}^{j} : j \leq i, t \geq t_{N-k+1}\right\}$.  Therefore, $G_{i,k+1}$ is measurable with respect to $\mathcal{F}_{i, k-1}$. Furthermore, if $k=N$, then $\bar{G}_{i}$ is measurable with respect to $\mathcal{F}_{i, k-1}$. Then, 
%     \bas{
%         \E\bbb{R_{i,k}|\mathcal{F}_{i,k-1}} &= \begin{cases}
%             \inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E\bbb{\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]|\mathcal{F}_{i, k-1}} } = 0, k \in [1,N-1], \\ \\
%             \inner{\bar{G}_i}{\E\bbb{z_{t_1}^{(i)}|\mathcal{F}_{i, k-1}}-\E\bbb{z_{t_1}^{(i)}|x_{t_1}^{(i)}}} = 0, k = N 
%         \end{cases}
%     }
%     Finally, if $k = 1$, $\mathcal{F}_{i-1, N-1} = \sigma\left\{x_{t}^{j}: j \leq i-1, t \geq t_1 \right\}$. Therefore, $R_{i,k}$ is independent of $\mathcal{F}_{i-1, N-1}$ and $\E\bbb{R_{i,k}|\mathcal{F}_{i-1, N-1}} = 0$.
% \end{proof}



% \dn{ This needs to be re-written or removed from the main text
% \martingalediffvariancebound*
% \begin{proof}
%     The first bound follows using Lemma~\ref{lemma:martingale_diff_variance_bound_appendix} and the second bound follows using using Lemma~\ref{lemma:variance_bound_H_HN}.
% \end{proof}
% }
% \martingalediffsubGaussianityparameters*
% \begin{proof}
% We have, 
% \bas{
%     & \mathbb{P}\bb{\left|\inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}\right| \geq \alpha|\mathcal{F}_{i, k-1}} \\
%     &= \mathbb{P}\bb{\left|\inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}\right|^{2} \geq \alpha^{2}|\mathcal{F}_{i, k-1}} \\
%     &=  \mathbb{P}\bb{\exp\bb{\lambda\inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}^{2}} \geq \exp\bb{\lambda\alpha^{2}} |\mathcal{F}_{i, k-1}} \\
%     &\leq \exp\bb{-\lambda\alpha^{2}}\E\bbb{\exp\bb{\lambda\inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}^{2}}|\mathcal{F}_{i, k-1}} \\
%     &= \exp\bb{-\lambda\alpha^{2}}\E\bbb{\exp\bb{\lambda\inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}^{2}}|\mathcal{F}_{i, k-1}} \\
%     &\leq \exp\bb{-\lambda\alpha^{2}}\E\bbb{\exp\bb{\lambda\norm{G_{i,k+1}}_{2}^{2}\norm{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}_{2}^{2}}\bigg|\mathcal{F}_{i, k-1}}
% }
% Since $G_{i,k+1}$ is measurable with respect to $\mathcal{F}_{i, k-1}$, set $\lambda := \frac{1}{\norm{G_{i,k+1}}_{2}^{2}\rho_{k}^{2}d}$ for $\rho_{k}$ defined in Lemma~\ref{lemma:subGaussianity_1},
% \bas{
%     \rho_{k} := 2\bb{L+1}e^{t_{N-k+1}}\sigma_{\gamma_{k}}
% }
% Therefore, 
% \bas{
%     & \mathbb{P}\bb{\left|\inner{G_{i,k+1}}{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}\right| \geq \alpha|\mathcal{F}_{i, k-1}} \\ &\;\;\;\; \leq \exp\bb{\frac{-\alpha^{2}}{\norm{G_{i,k+1}}_{2}^{2}\rho_{k}^{2}d}}\E\bbb{\exp\bb{\frac{\norm{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}_{2}^{2}}{\rho_{k}^{2}d} }\bigg|\mathcal{F}_{i, k-1}}
% }
% Note that Lemma~\ref{lemma:subGaussianity_1} shows that $\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]$ is subGaussianity. Therefore, using Proposition 2.5.2 from \cite{vershynin2018high},
% \bas{
%     \E\bbb{\exp\bb{\frac{\norm{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}_{2}^{2}}{\rho_{k}^{2}d} }} \leq 2
% }
% Therefore, using Markov's inequality, with probablity atleast $1-\delta$, 
% \ba{
%     & \E\bbb{\exp\bb{\frac{\norm{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}_{2}^{2}}{\rho_{k}^{2}d} }\bigg|\mathcal{F}_{i, k-1}} \notag \\
%     & \leq \frac{1}{\delta}\E\bbb{\E\bbb{\exp\bb{\frac{\norm{\E[x_0^{(i)}|x_{t_{N-k+1}}^{(i)}]-\E[x_0^{(i)}|x_{t_{N-k}}^{(i)}]}_{2}^{2}}{\rho_{k}^{2}d} }\bigg|\mathcal{F}_{i, k-1}}} \leq \frac{2}{\delta} \label{eq:markov_mgf_bound_1}
% }

% Similarly, using the definition of $\nu_k$ Lemma~\ref{lemma:subGaussianity_2}, 
% \bas{
%     \nu_k := 2\sigma_{\gamma_k}
% }
% we have, 
% \bas{
%     & \mathbb{P}\bb{\left|\inner{\bar{G}_{i}}{z_{t_1}^{(i)}-\E\bbb{z_{t_1}^{(i)}|x_{t_1}^{(i)}}}\right| \geq \alpha|\mathcal{F}_{i, k-1}}  \leq \exp\bb{\frac{-\alpha^{2}}{\norm{\bar{G}_{i}}_{2}^{2}\nu_{k}^{2}d}}\E\bbb{\exp\bb{\frac{\norm{z_{t_1}^{(i)}-\E\bbb{z_{t_1}^{(i)}|x_{t_1}^{(i)}}}_{2}^{2}}{\nu_k^{2}d} }\bigg|\mathcal{F}_{i, k-1}}
% }
% The conclusion follows by a similar argument as \eqref{eq:markov_mgf_bound_1}.
% \end{proof}


% \dn{assume $\gamma_{j} = \Delta$ everywhere below, and that $\Delta$ is smaller than some constant. Need to put this up everywhere.}\syamantak{Added $\Delta < 1$ in main paper. Please check.}


\begin{lemma}\label{lemma:function_time_regularity}
Let $f \in \cF$ and suppose Assumption~\ref{assumption:score_function_smoothness} holds. For any fixed $\tau_0 > 0$, with probability $1-\delta$, the following holds for every $f \in \cF$:

 $$\|f(t+\tau_0,x_{t+\tau_0})-s(t+\tau_0,x_{t+\tau_0})\| \geq e^{\tau_0}\|f(t,x_t)-s(t,x_t)\| - O(e^{2\tau_0}L\sqrt{d\tau_0}) - 2e^{2\tau_0}L\|z_{t,t+\tau_0}\|$$

  $$\|f(t,x_t)-s(t,x_t)\|  \geq e^{-\tau_0}\|f(t+\tau_0,x_{t+\tau_0})-s(t+\tau_0,x_{t+\tau_0})\|  - O(e^{\tau_0}L\sqrt{d\tau_0}) - 2e^{\tau_0}L\|z_{t,t+\tau_0}\|$$
\end{lemma}

\begin{proof}
Let $g(t,x) := f(t,x)-s(t,x)$. Note that $x_{t+\tau_0} = e^{-\tau_0}x_t + z_{t,t+\tau_0}$. By Assumption~\ref{assumption:score_function_smoothness}, $g$ is $2L$ Lipschitz in $x$ and with probability $1-\delta$ over $x_{t+\tau_0}$, and every $f \in \cF$:
    \begin{align}
        \|g(t+\tau_0,x_{t+\tau_0})\| &\geq e^{\tau_0}\|g(t,x_t)\| - \|g(t+\tau_0,x_{t+\tau_0}) - e^{\tau_0}g(t,x_t)\| \nonumber \\
        &\geq e^{\tau_0}\|g(t,x_t)\| - \|g(t+\tau_0,x_{t+\tau_0}) - e^{\tau_0}g(t,e^{\tau_0}x_{t+\tau_0})\| - 2e^{\tau_0}L\|e^{\tau_0}x_{t+\tau_0}-x_t\| \nonumber \\
        &\geq e^{\tau_0}\|g(t,x_t)\|- O(e^{2\tau_0}L\sqrt{d\tau_0\log(\tfrac{2}{\delta})}) - 2e^{2\tau_0}L\|z_{t,t+\tau_0}\|
    \end{align}

We conclude the second inequality with a similar proof. 
\end{proof}

\ltwolinfinityerrorboundtimeregularity*
\begin{proof}

For the sake of clarity, we will denote $g = f - s$. Using Lemma~\ref{lemma:function_time_regularity}, via the union bound for every $t = t_j$, $\tau_0 = |t_{j}-t_{k}|$ along with Gaussian concentration for $z_{t,t+\tau_0}^{(i)}$, we conclude that with probability $1-\delta$ the following holds uniformly for every $f \in \mathcal{F}$, $i \in [m]$ and $j,k \in \timeset$ with $ |j-k|\Delta \leq 1$ for some universal constant $C,c_0 > 0$: 
    
    \bas{
        \norm{f\bb{t_j,x_{t_j}^{(i)}}-s\bb{t_j,x_{t_j}^{(i)}}}_{2} &\geq c_0\norm{f\bb{t_k,x_{t_k}^{(i)}}-s\bb{t_k,x_{t_k}^{(i)}}}_{2} - CL\sqrt{d|j-k|\Delta\log(\tfrac{Nm}{\delta})}
    }

    Squaring both sides and using the AM-GM inequality,
    \ba{ \label{eq:square_growth}
        \norm{f\bb{t_j,x_{t_j}^{(i)}}-s\bb{t_j,x_{t_j}^{(i)}}}^2_{2} &\geq \frac{c^2_0}{2}\norm{f\bb{t_k,x_{t_k}^{(i)}}-s\bb{t_k,x_{t_k}^{(i)}}}^2_{2} - C^2L^2d|j-k|\Delta\log(\tfrac{Nm}{\delta})
    }
Now, let $(i^{*},k^{*}) \in \arg\sup_{i\in [m],k\in[N]}\|f(t_k,x_{t_k}^{(i)})-s(t_k,x_{t_k}^{(i)})\|_2$. Now, for any $j$ such that $|(j-k^*)|\Delta \leq 1$, the Equation~\eqref{eq:square_growth} implies:
   \bas{
   \sum_{i\in[m],j\in [N]}
        &\norm{f\bb{t_j,x_{t_j}^{(i)}}-s\bb{t_j,x_{t_j}^{(i)}}}^2_{2} \geq 
    \sum_{j: |j-k^*|\Delta\leq \Delta^{2/3}}
        \norm{f\bb{t_j,x_{t_j}^{(i^*)}}-s\bb{t_j,x_{t_j}^{(i^*)}}}^2_{2}    
        \\&\geq \sum_{j: |j-k^*|\Delta\leq \Delta^{2/3}}\left(\frac{c^2_0}{2}\norm{f\bb{t_{k^*},x_{t_{k^*}}^{(i^*)}}-s\bb{t_{k^*},x_{t_{k^*}}^{(i^*)}}}^2_{2} - C^2L^2d|j-k^*|\Delta\log(\tfrac{Nm}{\delta})\right)
    }
This implies the following inequality from which we can conclude the result. 
   \bas{
   \sum_{i\in[m],j\in [N]}
        &\norm{f\bb{t_j,x_{t_j}^{(i)}}-s\bb{t_j,x_{t_j}^{(i)}}}^2_{2} \geq 
\frac{c^2_0}{2\Delta^{1/3}}\norm{f\bb{t_{k^*},x_{t_{k^*}}^{(i^*)}}-s\bb{t_{k^*},x_{t_{k^*}}^{(i^*)}}}^2_{2} - 2C^2L^2d\Delta^{1/3}\log(\tfrac{Nm}{\delta})
    }
\end{proof}

% \dn{checked till here} 
% \dn{Assume $N\Delta \leq C\log(\frac{1}{\Delta})$}\syamantak{Fixed.}
\empiricalsquarederror*
\begin{proof}
Consider $\mathcal{L}(f)$ defined in Lemma~\ref{lemma:l2errorboundmartingale}, $H^f$ as defined in Lemma~\ref{lemma:error_martingale_decomposition_2}. Let $\hat{f}$ be the empirical risk minimizer. Then, by Lemma~\ref{lemma:l2errorboundmartingale}, we have: $\mathcal{L}(\hat{f}) \leq H^{\hat{f}}$ almost surely. Then, using Lemma~\ref{lemma:l2errorboundprob}, we have: $\mathcal{L}(\hat{f}) \leq \hat{\mathcal{L}}(s)$ almost surely.



% Therefore, we conclude:
% \bas{
% \mathbb{P}(\mathcal{L}(\hat{f}) > \epsilon^2) &\leq \mathbb{P}\left(\cup_{f\in \mathcal{F}}\{H^{f} > \epsilon^2\}\cap\{\mathcal{L}(f) \leq \mathsf{UB}\}\right) + \mathbb{P}\left(\hat{\mathcal{L}}(s) > \mathsf{UB}\right) \\
% &\leq \mathbb{P}\left(\hat{\mathcal{L}}(s) > \mathsf{UB}\right) +\sum_{f\in \mathcal{F}}\mathbb{P}\left(\{H^{f} > \epsilon^2\}\cap\{\mathcal{L}(f) \leq \mathsf{UB}\}\right) 
% }
As per Lemma~\ref{lemma:squared_error_crude_bound}, we pick $\mathsf{UB} = C(N\Delta + \log(\tfrac{1}{\Delta}))d\log(\tfrac{mN}{\delta})$ for some large enough constant $C$ and conclude that 
\begin{equation}\label{eq:extremity_bound}\mathbb{P}\left(\mathcal{L}(\hat{f}) > \mathsf{UB}\right) \leq \frac{\delta}{4}
\end{equation}

Let $f \in \mathcal{F}$ be arbitrary. We consider the martingale $H$ developed in Appendix~\ref{appendix:variance_calculation} with $\zeta = \frac{s-f}{m}$. In this case we can identify $H^f = H$. Considering the notation given in Lemma~\ref{lemma:conditional_concentration}, we have: $\mathbb{L}_2^{2}(\zeta) = \frac{1}{m\Delta}\mathcal{L}(f)$. Let $\alpha = \log(\tfrac{10 |\mathcal{\cF}|(2B+1)}{\delta})$. By Lemma~\ref{lemma:conditional_concentration}, we conclude $\mathbb{P}(\mathcal{A}_1(f)\cup\mathcal{A}_3(f)\cup\mathcal{A}_3(f)) \geq 1-(2B+1)e^{-\alpha}$ where:

\begin{enumerate}
    \item $$\mathcal{A}_1(f):= \biggr\{\frac{H^f}{L+1} \leq C \sqrt{\frac{\alpha N \Delta\mathcal{L}(f)}{m}} + C\frac{\alpha \sigma_{\max}}{m} \sup_{i,t\in \timeset}|f(t,x_t^{(i)})-s(t,x_t^{(i)})|\biggr\}$$
    \item $$\mathcal{A}_2(f) := \bigr\{\mathcal{L}(f) \leq CNm^2\alpha e^{-2B}\bigr\}$$
    \item $$\mathcal{A}_3(f) := \biggr\{\mathcal{L}(f) \geq c_0\frac{\Delta^2 e^{2B}}{d N^2 \log^2(2d)(L+1)^2\sup_{i,k}W_{i,k}}\biggr\}$$
\end{enumerate}

Taking a union bound over all $f \in \cF$, we conclude that $\hat{f}$ satisfies:
$$\mathbb{P}(\mathcal{A}_1(\hat{f})\cup\mathcal{A}_2(\hat{f})\cup \mathcal{A}_3(\hat{f})) \geq 1- (2B+1)|\mathcal{\cF}|e^{-\alpha}\,.$$

Since $\alpha = \log(\tfrac{10 |\mathcal{\cF}|(2B+1)}{\delta})$, we conclude:
\begin{equation}\label{eq:p_bound_1}
\mathbb{P}(\mathcal{A}_1(\hat{f})\cup\mathcal{A}_2(\hat{f})\cup \mathcal{A}_3(\hat{f})) \geq 1- \frac{\delta}{4}\,.\end{equation}

By Lemma~\ref{lemma:subGaussianity_parameters}, we conclude that with probability $1-\frac{\delta}{4}$, $\sup_{i,k}W_{i,k} \leq \log(\frac{8Nm}{\delta})$. Now, consider 
\ba{\mathbb{P}(\mathcal{A}_3(\hat{f})) &\leq \mathbb{P}(\mathcal{A}_3(\hat{f})\cap\{\sup_{i,k}W_{i,k} \leq \log(\tfrac{8Nm}{\delta})\}) + \mathbb{P}(\{\sup_{i,k}W_{i,k} > \log(\tfrac{8Nm}{\delta})\})  \nonumber \\
&\leq \mathbb{P}(\mathcal{A}_3(\hat{f})\cap\{\sup_{i,k}W_{i,k} \leq \log(\tfrac{8Nm}{\delta})\}) + \frac{\delta}{4} \nonumber \\
&\leq \mathbb{P}\left(\biggr\{\mathcal{L}(f) \geq c_0\frac{\Delta^2 e^{2B}}{d N^2 \log^2(2d)(L+1)^2\log(\tfrac{8Nm}{\delta})}\biggr\}\right) + \frac{\delta}{4} \nonumber \\
&\leq \mathbb{P}\left(\bigr\{\mathcal{L}(f) \geq \mathsf{UB} \bigr\}\right) + \frac{\delta}{4} ,\quad\text{ (by using the definition of $B$)} \nonumber \\
&\leq \frac{\delta}{2},\quad\text{ (by using Equation~\eqref{eq:extremity_bound})} \label{eq:large_p_bound}
} 

Now, consider the event $\mathcal{A}_2(\hat{f})$. It is clear from our choice of $B$ that following inclusion holds:
\begin{equation}
\{\mathcal{L}(\hat{f}) \leq \frac{1}{m} \} \subseteq \mathcal{A}_2(\hat{f})
\end{equation}

Now, consider the event $\mathcal{A}_1(\hat{f})$. Define the following events for some large enough constant $C$.
\bas{ \mathcal{C} &:= \cap_{f\in \mathcal{F}}\biggr\{
       \biggr(\sup_{\substack{i \in [m]\\ j \in [N]}}\norm{f\bb{t_j,x_{t_j}}-s\bb{t_j,x_{t_j}}}_{2}\biggr)^{2} \\
       & \leq C\Delta^{\tfrac{1}{3}}\biggr(\sum_{\substack{i\in [m] \\ j \in [N]}}\norm{f\bb{t_j,x_{t_j}}-s\bb{t_j,x_{t_j}}}_{2}^{2}\biggr) 
        + CL^{2}d\Delta^{\tfrac{1}{3}}\log(\tfrac{Nm}{\delta})\biggr\}
    }

\bas{
\mathcal{D}:= \bigr\{\sigma_{\max} \leq C\log(\tfrac{1}{\Delta})\log(2d)\sqrt{d\Delta \log(\tfrac{Nm}{\delta})}\bigr\}
}    
Lemma~\ref{lemma:l2_linfnity_bound_f}, we have $\mathbb{P}(\mathcal{C})\geq 1-\frac{\delta}{8}$. By Lemma~\ref{lemma:subGaussianity_parameters}, and union bound we have $\sup_{i,k}W_{i,k} \leq \log(\tfrac{8Nm}{\delta})$ with probability $1-\frac{\delta}{8}$. Therefore, $\mathbb{P}(\mathcal{D}) \geq 1-\tfrac{\delta}{8}$. Under the event $\mathcal{A}_1(\hat{f})\cap \mathcal{C}\cap\mathcal{D}$ we have:

\begin{enumerate}
    \item $\mathcal{L}(\hat{f}) \leq H^{\hat{f}}$ (This holds almost surely by Lemma~\ref{lemma:l2errorboundmartingale})
    \item $$H^{\hat{f}} \leq C(L+1)\sqrt{\frac{\alpha N \Delta \mathcal{L}(\hat{f})}{m}} + C(L+1)\frac{\alpha\sigma_{\max}}{m}\left[ \Delta^{-1/3}\sqrt{m\mathcal{L}(\hat{f})} + L\sqrt{d}\Delta^{1/6}\sqrt{\log(\tfrac{Nm}{\delta})}\right]$$
    \item $$\sigma_{\max} \leq C\log(\tfrac{1}{\Delta})\log(2d)\sqrt{d\Delta \log(\tfrac{Nm}{\delta})}$$
\end{enumerate}
Using the choice of $\Delta$ being small enough as stated in the Theorem, as well as our choice of $\alpha$, we conclude that under the event $\mathcal{A}_1(\hat{f})\cap \mathcal{C}\cap\mathcal{D}$, for some large enough constant $C'$:

$$\mathcal{L}(\hat{f}) \leq C'(L+1)\sqrt{\frac{\alpha N \Delta \mathcal{L}(\hat{f})}{m}} + C'\frac{(L+1)}{m}$$

$$\implies \mathcal{L}(\hat{f}) \leq \frac{(L+1)^2\log(1/\Delta)\log(\tfrac{|\mathcal F| B}{\delta})}{m}$$


Therefore, under the events $(\mathcal{A}_1(\hat{f})\cap\mathcal{D}\cap\mathcal{C})\cup\mathcal{A}_2(\hat{f})$, the guarantee for $\mathcal{L}(\hat{f})$ stated in the theorem holds. It now remains to show that $\mathbb{P}\left((\mathcal{A}_1(\hat{f})\cap\mathcal{D}\cap\mathcal{C})\cup\mathcal{A}_2(\hat{f})\right) \geq 1-\delta$. We begin with Equation~\eqref{eq:p_bound_1}:
\begin{align}
    1-\frac{\delta}{4} &\leq \mathbb{P}(\mathcal{A}_1(\hat{f})\cup\mathcal{A}_2(\hat{f})\cup\mathcal{A}_3(\hat{f})) \nonumber \\
    &\leq \mathbb{P}(\mathcal{A}_1(\hat{f})\cup\mathcal{A}_2(\hat{f})) + \mathbb{P}(\mathcal{A}_3(\hat{f})) \leq \mathbb{P}(\mathcal{A}_1(\hat{f})\cup\mathcal{A}_2(\hat{f})) + \frac{\delta}{2}, \quad \text{ by applying Equation~\eqref{eq:large_p_bound}}\nonumber \\
    &= \mathbb{P}((\mathcal{A}_1(\hat{f})\cup\mathcal{A}_2(\hat{f}))\cap \mathcal{C}\cap\mathcal{D}) + \mathbb{P}((\mathcal{A}_1(\hat{f})\cup\mathcal{A}_2(\hat{f}))\cap (\mathcal{C}\cap\mathcal{D})^{\complement}) + \frac{\delta}{2} \nonumber \\
    &\leq \mathbb{P}((\mathcal{A}_1(\hat{f})\cup\mathcal{A}_2(\hat{f}))\cap \mathcal{C}\cap\mathcal{D}) + \mathbb{P}(\mathcal{C}^{\complement}) + \mathbb{P}(\mathcal{D}^{\complement}) + \frac{\delta}{2} \nonumber \\
    &\leq \mathbb{P}((\mathcal{A}_1(\hat{f})\cup\mathcal{A}_2(\hat{f}))\cap \mathcal{C}\cap\mathcal{D}) + \frac{3\delta}{4},\quad\text{ by bound on }\mathbb{P}(\mathcal{C}),\mathbb{P}(\mathcal{D})\text{ given above} \nonumber \\
    &=  \mathbb{P}((\mathcal{A}_1(\hat{f})\cap\mathcal{C}\cap\mathcal{D})\cup(\mathcal{A}_2(\hat{f})\cap \mathcal{C}\cap\mathcal{D}))  + \frac{3\delta}{4} \nonumber \\
    &\leq \mathbb{P}((\mathcal{A}_1(\hat{f})\cap\mathcal{C}\cap\mathcal{D})\cup \mathcal{A}_2(\hat{f})) + \frac{3\delta}{4}
\end{align}
This demonstrates the desired result.
\end{proof}

\section{Generalization error bounds}
\label{appendix:generalization_error_bounds}

\begin{lemma}\label{lemma:hypercontractivity}
    Let all $f\bb{t, x} \in \cF$, be parameterized as $h\bb{t, x;\theta}$ for $\theta \in \Theta \subseteq \bR^D$ and $\theta_{*}$ be such that $h\bb{t, x_{t}; \theta_{*}} = s\bb{t, x_{t}}$. Suppose $\exists \lambda,\mu \geq 0$ such that $\forall \theta \in \Theta$, 
\bas{
\E\bbb{\norm{g\bb{t, x_{t}; \theta} - g\bb{t, x_{t}, \theta_{*}}}_{2}^{4}} &\leq \lambda^{2} \norm{\theta-\theta_{*}}^4, \text{ and } \\
\E\bbb{\norm{g\bb{t, x_{t}; \theta} - g\bb{t, x_{t}, \theta_{*}}}_{2}^{2}} & \geq \mu\norm{\theta-\theta_{*}}^{2}
}
Then, all $f \in \mathcal{F}$ satisfy Assumption~\ref{assumption:hypercontractivity} with $\chyp = \frac{\lambda}{\mu}$.
\end{lemma}
\begin{proof}
    The proof follows by squaring the second inequality and comparing with the first inequality.
\end{proof}

% \begin{lemma}\label{lemma:hypercontractivity} Let 
%     $f\bb{\theta, .} : \R^{d} \rightarrow \R^{d}$ be parameterized by $\theta$, satisfying Assumptions~\ref{assumption:score_function_smoothness},\ref{assumption:local_strong_conv}. Then for $\chyp := \frac{L}{\mu}$, 
%     \bas{
%         \bb{\E_{x_{t}}\bbb{\norm{f\bb{\theta, x_{t}} - s\bb{t, x_{t}}}_{2}^{4}}}^{\frac{1}{4}} \leq \chyp \bb{\E_{x_{t}}\bbb{\norm{f\bb{\theta, x_{t}} - s\bb{t, x_{t}}}_{2}^{2}}}^{\frac{1}{2}} 
%     }
% \end{lemma}
% \begin{proof}
%     Let $\exists \theta_{s}$ such that $s\bb{t, x_{t}} = f\bb{\theta_{s}, x_{t}}$. Then using Assumption~\ref{assumption:score_function_smoothness}, 
%     \bas{
%         \E_{x_{t}}\bbb{\norm{f\bb{\theta, x_{t}} - s\bb{t, x_{t}}}_{2}^{4}} \leq L^{2}\norm{\theta - \theta_{s}}_{2}^{4}
%     }
%     Further, using Assumption~\ref{assumption:local_strong_conv}, 
%     \bas{
%         \E_{x_{t}}\bbb{\norm{f\bb{\theta, x_{t}} - s\bb{t, x_{t}}}_{2}^{2}} \geq \mu^{2}\norm{\theta - \theta_{s}}_{2}^{2}
%     }
%     Therefore, 
%     \bas{
%         \E_{x_{t}}\bbb{\norm{f\bb{\theta, x_{t}} - s\bb{t, x_{t}}}_{2}^{4}} \leq \frac{L^{2}}{\mu^{2}}\E_{x_{t}}\bbb{\norm{f\bb{\theta, x_{t}} - s\bb{t, x_{t}}}_{2}^{2}}
%     }
%     which completes our proof.
% \end{proof}

\begin{lemma}\label{lemma:conv_function_l2_error}
    For timestep $t \geq 0$, let $x_t$ be defined as in \eqref{eq:fwd_noise}. Consider function $f : \R \times \R^{d} \rightarrow \R^{d}$ such that $\exists \chyp \geq 1$ satisfying,
\bas{\bb{\E_{x_{t}}\bbb{\norm{f\bb{t, x_{t}}-s\bb{t, x_{t}}}_{2}^{4}}}^{\frac{1}{4}} \leq \chyp\bb{\E_{x_{t}}\bbb{\norm{f\bb{t, x_{t}}-s\bb{t, x_{t}}}_{2}^{2}}}^{\frac{1}{2}}
    }
    Let $\mathcal{X} = \left\{x_{t}^{(i)}\right\}_{i \in [m]}$ be $\iid$ samples. Then, with probability atleast $1-\exp\bb{-\frac{m}{8\chyp^{2}}}$ there exists a set $\mathcal{G} \subseteq [m]$ such that $|G| \geq \frac{m}{8\chyp^{2}}$ and 
    \bas{
        \forall i \in \mathcal{G}, \;\; \norm{f\bb{t, x_{t}^{(i)}} - s\bb{t, x_{t}^{(i)}}}_{2}^{2} \geq \frac{1}{2}\E_{x_t}\bbb{\norm{f\bb{t, x_{t}} - s\bb{t, x_{t}}}_{2}^{2}}
    }
\end{lemma}
\begin{proof}
    Using the Payley-Zygmund inequality, for any $i \in [m]$, $\forall \theta \in [0,1]$,
    \ba{
        \mathbb{P}\bb{\norm{f\bb{t, x_{t}^{(i)}} - s\bb{t, x_{t}^{(i)}}}_{2}^{2} \geq \theta\E_{x_t}\bbb{\norm{f\bb{t, x_{t}} - s\bb{t, x_{t}}}_{2}^{2}}} \geq \bb{1-\theta}^{2}\frac{\E_{x_t}\bbb{\norm{f\bb{t, x_{t}} - s\bb{t, x_{t}}}_{2}^{2}}^{2}}{\E_{x_t}\bbb{\norm{f\bb{t, x_{t}} - s\bb{t, x_{t}}}_{2}^{4}}} \label{eq:pz_ineq}
    }
    Define the $\iid$ indicator random variable $\left\{\chi_{i}\right\}_{i \in [m]}$ as, 
    \bas{
        \chi_{i} := \mathbbm{1}\bb{\norm{f\bb{t, x_{t}^{(i)}} - s\bb{t, x_{t}^{(i)}}}_{2}^{2} \geq \frac{1}{2}\E_{x_t}\bbb{\norm{f\bb{t, x_{t}} - s\bb{t, x_{t}}}_{2}^{2}}}
    }
    Then, using \eqref{eq:pz_ineq}, $\mathbb{P}\bb{\chi_{i} = 1} \geq \frac{1}{4\chyp^{2}}$. Let $\mu := \sum_{i=1}^{m}\E\bbb{\chi_{i}} \geq \frac{m}{4\chyp^{2}}$. Using standard chernoff bounds for Bernoulli random variables, 
    \bas{
        \forall \epsilon \in \bb{0,1}, \; \mathbb{P}\bb{\sum_{i=1}^{m}\chi_{i} \leq \bb{1-\epsilon}\mu} \leq \exp\bb{-\frac{\epsilon^{2}\mu}{2}}
    }
    The result then follows by setting $\epsilon := \frac{1}{2}$.
\end{proof}

\expectedsquarederror*
\begin{proof}
    Using Theorem~\ref{theorem:empirical_l2_error_bound}, we have with probability at least $1-\delta$, 
    \ba{
        \sum_{i\in[m], j\in[N]}\frac{\gamma_{t_{j}}\norm{\hat{f}\bb{t_j, x_{t_j}}-s\bb{t_j, x_{t_j}}}_{2}^{2}}{m} \lesssim \frac{\bb{L+1}^{2}\log\bb{\frac{B|\mathcal{\cF}|}{\delta}}}{m} \label{eq:empirical_bound_1}
    }
    Using Lemma~\ref{lemma:hypercontractivity} and \ref{lemma:conv_function_l2_error}, if $m \gtrsim \chyp^{2}\log\bb{\frac{N}{\delta}}$ then, using a union-bound, for all particular timesteps $\left\{t_{j}\right\}_{j \in [N]}$ with probability at least $1-\delta$,
\ba{
\frac{1}{\chyp^{2}}\gamma_{t_{j}}\E_{x_{t_j}}\bbb{\norm{\hat{f}\bb{t_j, x_{t_j}}-s\bb{t_j, x_{t_j}}}_{2}^{2}} \lesssim \sum_{i\in[m]}\frac{\gamma_{t_{j}}\norm{\hat{f}\bb{t_j, x_{t_j}^{(i)}}-s\bb{t_j, x_{t_j}^{(i)}}}_{2}^{2}}{m} \label{eq:generalization_bound_1}
}
Adding over all timesteps $\left\{t_{j}\right\}_{j \in [N]}$, 
\bas{
    \sum_{j \in [N]}\gamma_{t_{j}}\E_{x_{t_j}}\bbb{\norm{\hat{f}\bb{t_j, x_{t_j}}-s\bb{t_j, x_{t_j}}}_{2}^{2}} & \lesssim  \chyp^{2}\sum_{i\in[m], j\in[N]}\frac{\gamma_{t_{j}}\norm{\hat{f}\bb{t_j, x_{t_j}}-s\bb{t_j, x_{t_j}}}_{2}^{2}}{m} \\
    & \lesssim \frac{\chyp^{2}\bb{L+1}^{2}\log\bb{\frac{B|\cF|}{\delta}}}{m} 
}
The result then follows by setting the RHS smaller by $\epsilon^{2}$.
\end{proof}

\begin{theorem}[Accelerated Inference]\label{thm:subsampled_error_bound_appendix}
Under the same assumptions as Theorem~\ref{theorem:expected_l2_error_bound}, partition the timesteps $\{t_j = \Delta j\}_{j \in [N]}$ into $k$ disjoint subsets $S_1, S_2, \dots, S_k$, where each subset $S_i$ contains timesteps of the form $t_j = \Delta(i + mk)$ for $m \in \mathbb{N}$. Define $\gamma_j' := k\Delta$ for all $j$ in any subset $S_i$. Then, there exists at least one subset $S_i$ such that:
\[
\sum_{j \in S_i} \gamma_j' \E_{x_{t_j}}\left[\norm{\hat{f}(t_j, x_{t_j}) - s(t_j, x_{t_j})}_2^2\right] \lesssim \epsilon^2,
\]
with probability at least $1 - \delta$.
\end{theorem}
\begin{proof}
From Theorem~\ref{theorem:expected_l2_error_bound}, we have with probability $1-\delta$:
\[
\sum_{j \in [N]} \gamma_j \E_{x_{t_j}}\left[\norm{\hat{f}(t_j, x_{t_j}) - s(t_j, x_{t_j})}_2^2\right] \lesssim \epsilon^2,
\]
where $\gamma_j = \Delta$. Partition the $N$ timesteps into $k$ disjoint subsets $S_1, \dots, S_k$ as described. Each subset $S_i$ contributes:
\[
\sum_{j \in S_i} \gamma_j \E_{x_{t_j}}\left[\norm{\hat{f}(t_j, x_{t_j}) - s(t_j, x_{t_j})}_2^2\right] = \sum_{j \in S_i} \Delta \E_{x_{t_j}}\left[\norm{\hat{f}(t_j, x_{t_j}) - s(t_j, x_{t_j})}_2^2\right].
\]
Summing over all $k$ subsets gives the original total:
\[
\sum_{i=1}^k \sum_{j \in S_i} \Delta \E_{x_{t_j}}\left[\norm{\hat{f}(t_j, x_{t_j}) - s(t_j, x_{t_j})}_2^2\right] \lesssim \epsilon^2.
\]
Now scale each subset's step size by $k$ (i.e., $\gamma_j' = k\Delta$). The contribution of subset $S_i$ becomes:
\[
\sum_{j \in S_i} \gamma_j' \E_{x_{t_j}}\left[\norm{\hat{f}(t_j, x_{t_j}) - s(t_j, x_{t_j})}_2^2\right] = k \sum_{j \in S_i} \Delta \E_{x_{t_j}}\left[\norm{\hat{f}(t_j, x_{t_j}) - s(t_j, x_{t_j})}_2^2\right].
\]
Summing over all subsets with the scaled $\gamma_j'$, we get:
\[
\sum_{i=1}^k \sum_{j \in S_i} \gamma_j' \E_{x_{t_j}}\left[\norm{\hat{f}(t_j, x_{t_j}) - s(t_j, x_{t_j})}_2^2\right] = k \sum_{i=1}^k \sum_{j \in S_i} \Delta \E_{x_{t_j}}\left[\norm{\hat{f}(t_j, x_{t_j}) - s(t_j, x_{t_j})}_2^2\right] \lesssim k \epsilon^2.
\]
We conclude that at least one subset $S_i$ must satisfy:
\[
\sum_{j \in S_i} \gamma_j' \E_{x_{t_j}}\left[\norm{\hat{f}(t_j, x_{t_j}) - s(t_j, x_{t_j})}_2^2\right] \lesssim \epsilon^2,
\]
since otherwise all $k$ subsets would contribute more than $\epsilon^2$, leading to a total exceeding $k \epsilon^2$, which contradicts the scaled bound $k \epsilon^2$.
\end{proof}

\section{Bootstrapped Score Matching}\label{appendix:bsm}

\begin{algorithm}[H]
    \caption{$\bsm\left(\left\{x_{0}^{(i)}\right\}_{i \in [m]}, T, N, \left\{\cF_{i}\right\}_{i \in [N]}, k_{0}\right)$}
    \label{alg:bsm_algorithm}
    \KwIn{Dataset $D := \left\{x_{0}^{(i)}\right\}_{i \in [m]}$, Initial Sample Size $m$, Number of discretized timesteps $N$ labelled as $0 < t_{0} < t_{1} < \cdots < t_{N} = T$, Sequence of Function classes $\left\{\cF_{i}\right\}_{i \in [N]}$, $k_{0} \in \mathbb{N}$}
    \KwOut{Estimated Score Functions $\left\{\hat{s}_{t_k}\right\}_{k \in [N]}$ to optimize $\mathbb{E}_{x_{t_k}}\left[\norm{\hat{s}(t_{k}, x_{t_k}) - s(t_{k}, x_{t_k})}_{2}^{2}\right]$}
    \For{$k \in [N]$}{
        Let $\forall i \in [m]$, $x_{t_k}^{(i)} = x_{0}^{(i)}e^{-t_k} + z_{t_k}^{(i)}$ \\
        \If{$k \leq k_{0}$}{
             $\hat{s}_{t_{k}} \leftarrow \arg\min_{f \in \cF_{k}}\frac{1}{m}\sum_{i\in[m]}\norm{f(t_k, x_{t_k}^{(i)}) \; - \; \frac{-z_{t_k}^{(i)}}{\sigma_{t_k}^{2}}}_{2}^{2}$ \algcomment{Denoising Score Matching (DSM)}
        }
        \Else{
            $\gamma_{k} \leftarrow t_{k}-t_{k-1}$\\
            $\alpha_{k} \leftarrow e^{-\gamma_k}\frac{1-e^{-2t_{k-1}}}{1-e^{-2t_k}}$ \algcomment{Bootstrapped Score Matching (BSM)} \\ \\
            $\tilde{y}_{t_k}^{(i)} \leftarrow (1-\alpha_{k})\frac{-z_{t_k}^{(i)}}{\sigma_{t_k}^{2}}  +  \alpha_{k}\left(\frac{-z_{t_k}^{(i)}}{\sigma_{t_k}^{2}} + \left(\hat{s}_{t_{k-1}}(x_{t_{k-1}}^{(i)}) - \frac{-z_{t_{k-1}}^{(i)}}{\sigma_{t_{k-1}}^{2}}\right)\right)$ \algcomment{Bootstrapped Targets} \\ \\
            $\hat{s}_{t_k} \leftarrow \arg\min_{f \in \cF_{k}}\sum_{i\in[m]}\frac{\norm{f(t_k, x_{t_k}^{(i)}) - \tilde{y}_{t_k}^{(i)}}_{2}^{2}}{m}$ \algcomment{Learning with biased targets}
            % $\nu_{k} \leftarrow \frac{(L+1)\sqrt{1-e^{-2\gamma_k}}}{\sigma_{t_k}^{2}}$  \\ \\
            % $\mathcal{G}_{k} = \left\{f \in \cF_{k}, \; \sum_{i \in [m]}\frac{\norm{\tilde{f}(t_k, x_{t_k}^{(i)})-f(t_k, x_{t_k}^{(i)})}_{2}^{2}}{m} \leq \frac{\nu_k^{2}d\log\left(\frac{|\mathcal{\cF}|m}{\delta}\right)}{m}\right\}$ \\ \\
            % $\hat{s}_{t_k} \leftarrow \arg\min_{f \in \mathcal{G}_{k}}\sum_{i\in [m]}\frac{\norm{f(t_k, x_{t_k}^{(i)})-\tilde{y}_{t_k}^{(i)}}_{2}^{2}}{m} + 2\frac{\left\langle f(t_k, x_{t_k}^{(i)})-\tilde{f}(t_k, x_{t_k}^{(i)}), \; \tilde{f}(t_k, x_{t_k}^{(i)})-y_{t_k}^{(i)} \right\rangle}{m}$ \algcomment{Debiasing}
        }
    }
\end{algorithm}

\begin{lemma}[Bootstrap Consistency]\label{lemma:bootstrap_consistency_appendix}For some $\alpha > $, let
\bas{
    \tilde{y}_{t} := -\frac{z_t}{\sigma_t^{2}} - \alpha\bb{s\bb{t', x_{t'}} - \frac{-z_{t'}}{\sigma_{t'}^{2}}}
}
Then, $\E\bbb{\tilde{y}_{t}|x_{t}} = s(t, x_t)$.
\end{lemma}
\begin{proof}
    Note that by Tweedie's formula, 
    \bas{
        s\bb{t', x_{t'}} &= \E\bbb{\frac{-z_{t'}}{\sigma_{t'}^{2}}\bigg|x_{t'}}
    }
    Therefore, using the Markovian property, we have
    \bas{
        \E\bbb{s\bb{t', x_{t'}} - \frac{-z_{t'}}{\sigma_{t'}^{2}}|x_{t}} &=         \E\bbb{\E\bbb{s\bb{t', x_{t'}} - \frac{-z_{t'}}{\sigma_{t'}^{2}}|x_{t
        '},x_{t}}|x_{t}}, \\
        &= \E\bbb{\E\bbb{s\bb{t', x_{t'}} - \frac{-z_{t'}}{\sigma_{t'}^{2}}|x_{t
        '}}|x_{t}}, \\
        &= 0
    }
    Finally, the result follows using another application of Tweedie's formula which shows that $s\bb{t, x_{t}} = \E[-z_{t}/\sigma_{t}^{2}|x_{t}]$.
\end{proof}

\begin{lemma}[Bootstrap Variance]\label{lemma:bootstrap_variance}For $\Delta := t-t'$ and $\alpha := e^{-\Delta}\frac{\sigma_{t'}^{2}}{\sigma_{t}^{2}}$, let
\bas{
    \tilde{y}_{t} := -\frac{z_t}{\sigma_t^{2}} - \alpha\bb{s\bb{t', x_{t'}} - \frac{-z_{t'}}{\sigma_{t'}^{2}}}
}
Then,  under Assumption~\ref{assumption:score_function_smoothness},
\bas{
    \normop{\E\bbb{(\tilde{y}_{t}-s(t, x_t))(\tilde{y}_{t}-s(t, x_t))^{\top}|x_{t}}} &= O\bb{\frac{(L^{2}+1)\Delta}{\sigma_{t}^{4}}} 
}
\end{lemma}
\begin{proof}
     Using Tweedie's formula, 
    \bas{
        s_{t}\bb{x_{t}} := \E\bbb{\frac{-z_{t}}{\sigma_{t}^{2}}\bigg|x_{t}}, \;\; s\bb{t', x_{t'}} := \E\bbb{\frac{-z_{t'}}{\sigma_{t'}^{2}}\bigg|x_{t'}}
    }
    Using the Markov property, 
    \bas{
        \E\bbb{s\bb{t', x_{t'}} - \frac{-z_{t'}}{\sigma_{t'}^{2}}\bigg|x_{t}} &= \E\bbb{\E\bbb{s\bb{t', x_{t'}} - \frac{-z_{t'}}{\sigma_{t'}^{2}}\bigg|x_{t'},x_{t}}\bigg|x_{t}} = \E\bbb{\E\bbb{s\bb{t', x_{t'}} - \frac{-z_{t'}}{\sigma_{t'}^{2}}\bigg|x_{t'}}\bigg|x_{t}} = 0
    }
    Therefore, $\E\bbb{h_{t,t'}|x_{t}} = 0$. Let  $v_{t,t'} := s_{t}\bb{x_{t}} - \alpha s\bb{t', x_{t'}}$ and $r_{t,t'} := \frac{z_{t}}{\sigma_{t}^{2}} - \alpha\frac{z_{t'}}{\sigma_{t'}^{2}}$.

    First consider $r_{t,t'}$. We have using \eqref{eq:fwd_noise}, $z_{t} = e^{-(t-t')}z_{t'} + z_{t,t'}$ where $z_{t,t'} \sim \mathcal{N}(0, \sigma_{t-t'}^{2})$. Then, 
    \ba{
        r_{t,t'} &= \frac{z_{t}}{\sigma_{t}^{2}} - \alpha\frac{z_{t'}}{\sigma_{t'}^{2}} = \frac{e^{-\Delta}z_{t'} + z_{t,t'}}{\sigma_{t}^{2}} - \alpha\frac{z_{t'}}{\sigma_{t'}^{2}} = \bb{\frac{e^{-\Delta}}{\sigma_{t}^{2}} - \frac{\alpha}{\sigma_{t'}^{2}}}z_{t'} + \frac{z_{t,t'}}{\sigma_{t}^{2}} \label{eq:rttp}
    }
    Next, for $v_{t,t'}$ again using Tweedie's formula,
    \ba{
        v_{t,t'} &= \E\bbb{\frac{-z_{t}}{\sigma_{t}^{2}}\bigg|x_{t}} - \alpha s\bb{t', x_{t'}} = \E\bbb{\frac{-z_{t}}{\sigma_{t}^{2}}\bigg|x_{t}} - \alpha s\bb{t', x_{t'}} \notag \\
        &= \E\bbb{\frac{-e^{-\Delta}z_{t'} - z_{t,t'}}{\sigma_{t}^{2}}\bigg|x_{t}} - \alpha s\bb{t', x_{t'}} = \E\bbb{\frac{-e^{-\Delta}z_{t'}}{\sigma_{t}^{2}}\bigg|x_{t}} - \E\bbb{\frac{z_{t,t'}}{\sigma_{t}^{2}}\bigg|x_{t}} - \alpha s\bb{t', x_{t'}} \notag \\
        &= \E\bbb{\E\bbb{\frac{-e^{-\Delta}z_{t'}}{\sigma_{t}^{2}}\bigg|x_{t'},x_{t}}\bigg|x_{t}} - \E\bbb{\frac{z_{t,t'}}{\sigma_{t}^{2}}\bigg|x_{t}} - \rho_{t,t'}s\bb{t', x_{t'}} \notag \\
        &= \E\bbb{\E\bbb{\frac{-e^{-\Delta}z_{t'}}{\sigma_{t}^{2}}\bigg|x_{t'}}\bigg|x_{t}} - \E\bbb{\frac{z_{t,t'}}{\sigma_{t}^{2}}\bigg|x_{t}} - \alpha s\bb{t', x_{t'}}, \text{ using the Markov property} \notag \\
        &= \alpha\E\bbb{\E\bbb{\frac{-z_{t'}}{\sigma_{t'}^{2}}\bigg|x_{t'}}\bigg|x_{t}} - \alpha s\bb{t', x_{t'}} - \E\bbb{\frac{z_{t,t'}}{\sigma_{t}^{2}}\bigg|x_{t}} + \bb{\frac{\alpha}{\sigma_{t'}^{2}} - \frac{e^{-\Delta}}{\sigma_{t}^{2}}}\E\bbb{z_{t'}|x_{t}} \notag \\
        &= \alpha\bb{\E\bbb{s\bb{t', x_{t'}}|x_{t}} - s\bb{t', x_{t'}}} - \E\bbb{\frac{z_{t,t'}}{\sigma_{t}^{2}}\bigg|x_{t}} + \bb{\frac{\alpha}{\sigma_{t'}^{2}} - \frac{e^{-\Delta}}{\sigma_{t}^{2}}}\E\bbb{z_{t'}|x_{t}} \label{eq:vttp}
        }
    Therefore, using \eqref{eq:vttp} and \eqref{eq:rttp},
    \bas{
        \tilde{y}_{t}-s(t, x_t) &= v_{t,t'} + r_{t,t'} \\
                 &= \alpha\bb{\E\bbb{s\bb{t', x_{t'}}|x_{t}} - s\bb{t', x_{t'}}} + \frac{1}{\sigma_{t}^{2}}\bb{z_{t,t'} - \E\bbb{z_{t,t'}|x_{t}}} + \bb{\frac{\alpha}{\sigma_{t'}^{2}} - \frac{e^{-\Delta}}{\sigma_{t}^{2}}}\bb{z_{t'} - \E\bbb{z_{t'}|x_{t}}} \\
                 &= \alpha\bb{\E\bbb{s\bb{t', x_{t'}}|x_{t}} - s\bb{t', x_{t'}}}+ \frac{1}{\sigma_{t}^{2}}\bb{z_{t,t'} - \E\bbb{z_{t,t'}|x_{t}}}, \text{ using the value of }p \\
                 &= \alpha\bb{e^{-(t-t')}s(t, x_t) - s\bb{t', x_{t'}}}+ \frac{1}{\sigma_{t}^{2}}\bb{z_{t,t'} + \sigma_{t-t'}^{2}s(t, x_t)}, \text{ using Theorem 1 from \cite{de2024target}}
    }
    Therefore, 
    \bas{
       & \E\bbb{(\tilde{y}_{t}-s(t, x_t))(\tilde{y}_{t}-s(t, x_t))^{\top}|x_{t}} \\ & \preceq  2\alpha^{2}\E\bbb{\bb{e^{-(t-t')}s(t, x_t) - s\bb{t', x_{t'}}}\bb{e^{-(t-t')}s(t, x_t) - s\bb{t', x_{t'}}}^{\top}|x_t} \\
       & \;\; + \frac{2}{\sigma_{t}^{4}}\E\bbb{\bb{z_{t,t'} + \sigma_{t-t'}^{2}s(t, x_t)}\bb{z_{t,t'} + \sigma_{t-t'}^{2}s(t, x_t)}^{\top}|x_t} \\
       &= 2\alpha^{2}\E\bbb{\bb{e^{-(t-t')}s(t, x_t) - s\bb{t', x_{t'}}}\bb{e^{-(t-t')}s(t, x_t) - s\bb{t', x_{t'}}}^{\top}|x_t} \\
       & \;\; + \frac{2}{\sigma_{t}^{4}}(\sigma_{t-t'}^{4}h_{t}(x_t) + \sigma_{t-t'}^{2}\id_{d}) \text{ using Lemma}~\ref{lemma:second_order_tweedie_application}, \text{ where } h_{t}(x_t) := \nabla^{2}\log(p_{t}(x_t))
    }
    which implies, 
    \bas{
        & \normop{\E\bbb{(\tilde{y}_{t}-s(t, x_t))(\tilde{y}_{t}-s(t, x_t))^{\top}|x_{t}}} \\ & \leq  2\alpha^{2}\normop{\E\bbb{\bb{e^{-(t-t')}s(t, x_t) - s\bb{t', x_{t'}}}\bb{e^{-(t-t')}s(t, x_t) - s\bb{t', x_{t'}}}^{\top}|x_t}} \\
       & \;\; + \frac{2}{\sigma_{t}^{4}}\normop{\sigma_{t-t'}^{4}h_{t}(x_t) + \sigma_{t-t'}^{2}\id_{d}} \\
       &= O\bb{ \frac{L\Delta^{2} + \Delta}{\sigma_{t}^{4}} + \alpha^{2}L^{2}\Delta }, \text{ using Assumption~\ref{assumption:score_function_smoothness} and Corollary~\ref{corr:score_function_delta_variance}} \\
       &= O\bb{\frac{(L^{2}+1)\Delta}{\sigma_{t}^{4}}}
    }
\end{proof}

\subsection{Experimental Details}
In this section, we provide details about our experiments shown in Figure~\ref{fig:bsm_experiments}.

In the first experiment, we study the accuracy of different score estimation methods in the context of learning the score function of a Gaussian distribution under the variance-reduced Bootstrapped Score Matching (BSM) objective. We compare BSM with DSM to evaluate their relative performance in estimating the true score function across different timesteps. Our target distribution is a $d$-dimensional Gaussian distribution with covariance matrix $\Sigma \in \mathbb{R}^{d \times d}$, constructed as $\Sigma = 5 M M^T + 5 v v^T$ where $M \in \mathbb{R}^{d \times d}$ and $v \in \mathbb{R}^{d \times 1}$ are sampled from a standard normal distribution. We generate $m = 10000$ samples from the target distribution. Note that since the target density is gaussian, the density at all intermediate timesteps, $p_{t}$, also follows a gaussian distribution. The time evolution follows an non-linear decay model, with $N = 1000$ discrete timesteps sampled as:
$t_i = \text{linspace}(0.001, t_{\max}, N)^2, \quad \text{where} \quad t_{\max} = \sqrt{5}$. The noise covariance scaling factor follows $\sigma_t = \sqrt{1 - e^{-2t}}$. The bootstrap ratio for BSM is adaptively chosen as $1 - (\sigma_t / (\sigma_{t-t'} + \sigma_t))$, where $t'$ represents the previous timestep. The score function is estimated using the standard least-squares regression solution on account of the simple target distribution which implies a linear score function of the form $s(t, x) := A_{t}x$ for some matrix $A_{t}$. We run $5$ training epochs for the first few timesteps ($t \leq 3$) and $1$ epoch thereafter. We plot the squared error of the learned score matrix, $\hat{A}_{t}$ against the true score matrix, $A_{t}$ at all timesteps. 

In the second experiment, we move away from the Gaussian density, which is unimodal, to a Gaussian Mixture model (GMM), which is multimodal. We fix the dimensionality of the data as $d = 1$ for ease of visualization, and generate a mixture of two gaussians with means $\pm 5$ and mixture weights $0.7$ and $0.3$ respectively. We generate $m=10000$ samples from the GMM.  The time evolution is linear with $N=1000$ timesteps. We train a 3 layer neural network with hidden layer dimensions of $10$ each, separately for DSM and BSM. We train the neural network for 100 epochs, with an initial learning rate of $0.05$, using the \textit{AdamW} optimizer, along with a cosine scheduler to manage the learning rate schedule. The number of warmup steps of the scheduler are chosen to be $10\%$ of the total training steps. When training the BSM network, we start bootstrapping after $k_0 :- 250$ timesteps and $90$ epochs. The bootstrap ratio is fixed at $0.9$. Once training is completed, we sample 10000 points using the learned score functions to plot and compare the empirical density. 


