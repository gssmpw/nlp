\documentclass{article}

% arXiv package
\usepackage{arxiv}

% Encoding and font packages
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts

% Hyperlinks and URL packages
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting

% Bibliography and citation packages
\usepackage{natbib}

% Table packages
\usepackage{booktabs}       % professional-quality tables
\usepackage{multirow}       % multi-row tables

% Math packages
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}        % math enhancements
\usepackage{amssymb}        % additional math symbols
\usepackage{amsthm}         % theorem environments
\usepackage{scalerel}       % scalable elements in math
\usepackage{nicefrac}       % compact symbols for 1/2, etc.

% Graphics packages
\usepackage{graphicx}       % graphics
\usepackage[dvipsnames]{xcolor} % color support

% Typographic enhancements
\usepackage{microtype}      % microtypography

% List and enumeration packages
\usepackage{enumitem}       % customizable lists

% Floating figure packages
\usepackage{wrapfig}        % wrapping figures

% Header and footer packages
\usepackage{fancyhdr}       % header

% Custom theorem and command definitions
\newtheorem{theorem}{Theorem}
\definecolor{junscolor1}{HTML}{FF4F00}
\definecolor{junscolor2}{HTML}{13BBAF}
\definecolor{junscolor3}{HTML}{F8BE2C}
\newcommand{\firstbest}[1]{\textbf{\textcolor{junscolor1}{#1}}}
\newcommand{\secondbest}[1]{\textbf{\textcolor{junscolor2}{#1}}}
\newcommand{\thirdbest}[1]{\textbf{\textcolor{junscolor3}{#1}}}

\DeclareMathOperator*{\concat}{\scalerel*{\Vert}{\sum}}
\makeatletter
\DeclareRobustCommand{\pdot}{\mathbin{\mathpalette\pdot@\relax}}\newcommand{\pdot@}[2]{\ooalign{$\m@th#1\circ$\cr\hidewidth$\m@th#1\cdot$\hidewidth\cr}}

% Header settings
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

%% Title
\title{Understanding the Design Principles of Link Prediction in Directed Settings
\thanks{\textit{\underline{Citation}}: 
Jun Zhai, Muberra Ozmen, and Thomas Markovich. 2025. Understanding the Design Principles of Link Prediction in Directed Settings. In Companion Proceedings of the ACM Web Conference 2025 (WWW Companion ’25), April 28-May 2, 2025, Sydney, NSW, Australia. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3701716.3717803.}} 
\author{
  Jun Zhai, Muberra Ozmen, Thomas Markovich \\
  Block, Inc. \\
  \texttt{\{junzhai, muberra, tmarkovich\}@block.xyz} \\
}

\begin{document}
\maketitle

\begin{abstract}
    Link prediction is a widely studied task in \emph{Graph Representation Learning (GRL)} for modeling relational data. The early theories in GRL were based on the assumption of a symmetric adjacency matrix, reflecting an undirected setting. As a result, much of the following state-of-the-art research has continued to operate under this symmetry assumption, even though real-world data often involve crucial information conveyed through the direction of relationships. This oversight limits the ability of these models to fully capture the complexity of directed interactions. In this paper, we focus on the challenge of directed link prediction by evaluating key heuristics that have been successful in undirected settings. We propose simple but effective adaptations of these heuristics to the directed link prediction task and demonstrate that these modifications produce competitive performance compared to the leading \emph{Graph Neural Networks (GNNs)} originally designed for undirected graphs. Through an extensive set of experiments, we derive insights that inform the development of a novel framework for directed link prediction, which not only surpasses baseline methods but also outperforms state-of-the-art GNNs on multiple benchmarks.
\end{abstract}

\section{Introduction}\label{sec:introduction}
Link prediction is a task that seeks to uncover missing connections (\textit{e.g.} links), between entities (\textit{e.g.} vertices) in a graph, and has many industrial applications. For example, an e-commerce platform might represent their users and items as vertices, and the transactions users make as edges from user to item. In this setting, a recommendation system that predicts items that users might like to buy can readily be cast as a link prediction task~\citep{chamberlain2022graph, wang2023structure}. Beyond direct applications, link prediction is often used in unsupervised settings to construct vertex representations that can then be used in various downstream tasks, such as fraud or toxicity detection~\citep{pal2020pinnersage, el2022twhin, liu2020alleviating, zhang2022efraudcom}.

Various methodologies for link prediction have been developed and can be broadly classified into three categories. The first category, \emph{similarity-based heuristics}, involves computing a score for each pair of nodes to quantify their similarity~\citep{wang2007local}. These scores are then ranked, with higher scores indicating a greater likelihood of connection between node pairs. The second category encompasses \emph{probabilistic and maximum likelihood models}~\citep{wang2007local, clauset2008hierarchical, guimera2009missing}. Although these models have demonstrated effectiveness on smaller datasets, they tend to be computationally intensive and face scalability challenges in large real-world graphs. The third category includes \emph{node representation learning methods}, where an \emph{encoder} learns to represent each node as a vector in an embedding space, and a \emph{decoder} processes pairs of node embeddings to generate a score that quantifies the likelihood of a link existing. These embeddings are optimized so that nodes with similar neighborhood structures are represented similarly in the embedding space. Node representation methods can be further divided into three subcategories based on the choice of encoder: \emph{random walk-based approaches}~\citep{perozzi2014deepwalk, trouillon2016complex, cao2018link, kazemi2018simple}, \emph{matrix decomposition techniques}~\citep{acar2009link, kazemi2018simple}, and \emph{Graph Neural Networks (GNNs)}~\citep{kipf2016semi, hamilton2017inductive, ying2018graph, velivckovic2017graph}.

Although link prediction is valuable across many applications, most widely used methods assume that links are undirected. In many settings, this assumption makes sense. For example, an edge in the Facebook friendship graph will be undirected, because friendships are bidirectional on the platform. As a result, it is common to transform directed graphs into undirected ones. However, this undirected transformation does not make sense in all settings because an edge's directionality can denote the semantics of that edge, which is often crucial for accurately modeling interactions.

Intuitively, directed edges allow us to differentiate a node's role as either a source or a target within a network, which provides finer granularity when modeling interactions. In many real-world contexts, this asymmetry is meaningful. For instance, consider a transaction network where the goal is to identify fraudulent behavior. Suppose there are three types of participants: a parent, a teenager, and a fraudster. Let’s assume the model aims to understand the identity and behavior of the teenager. A money transfer from a parent to a teenager represents one type of pattern, while a transfer from a teenager to a fraudster at the same monetary amount represents another. If we look at a scenario where the teenager is receiving money from the parent, this pattern might be considered safe—such as receiving an allowance. However, if the teenager is instead sending the same amount to a fraudster, this could be a red flag. By taking into account the direction of these transactions, we can more effectively distinguish between benign and suspicious patterns. In a directed setting, the directionality conveys vital information about who initiates and who receives a transaction, which can significantly impact downstream predictions. Conversely, if we convert the originally directed network into an undirected one by symmetrizing the adjacency matrix, the sender/receiver roles of the teenager in these transactions would be lost, potentially leading to a misinterpretation of the interactions and harming predictive performance. 

To make this point clear, we have constructed a directed ring graph and present it in Figure~\ref{fig:toy_example}, as well as embeddings generated using both an undirected and a directed graph decoder. We clearly observe that the asymmetric, or directed, decoder is more capable of replicating the expected planar positioning, while the undirected decoder generates a set of embeddings where almost all vertices collapse together in the embedding space. This example helps us to develop the intuition that using undirected link predictors in directed settings may lead to poor performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.2\textwidth]{figures/ex_original.png} 
    \includegraphics[width=0.2\textwidth]{figures/ex_initial.png}
    \includegraphics[width=0.2\textwidth]{figures/ex_symmetric.png}
    \includegraphics[width=0.2\textwidth]{figures/ex_asymmetric.png}
    \caption{
    \textbf{An illustrative evaluation demonstrating the impact of incorporating directionality into the design of predictive models.} We generate a directed ring graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$. Each node $u \in \mathcal{V}$ is initialized with a two-dimensional embedding $\mathbf{e}_u^{(0)}$. We then train GraphSage~\citep{hamilton2017inductive}, to update these embeddings using two different decoders: (1) a undirected decoder, and (2) a directed decoder, to perform link prediction. By visualizing the output node embeddings in both cases, we observe that the structural representation of nodes is significantly enhanced when using a directed decoder, emphasizing the importance of directionality in the model design.
    }
    \label{fig:toy_example}
\end{figure}


In this paper, we take this intuition and make it concrete by exploring the problem of directed link prediction. Along the way, we gain insights into the design principles needed for the development of link predictors that are applicable to directed settings. The key contributions of our work are as follows:
\begin{itemize}[leftmargin=*]
    \item[-] We establish \textbf{a robust comparison framework} by constructing three types of baseline models: heuristic approaches, Multi-Layer Perceptron (MLP), and GNN based models and their variants for directed settings. 
    \item[-] We conduct extensive ablation experiments to extend the known \textbf{design principles} of link prediction to include their directed variants, and study the impact of directionality on the predictive performance for each such design principle. 
    \item[-] Based on the design principles determined, we develop DirLP, \textbf{a novel model for directed link prediction} that significantly outperforms the baseline models.  
\end{itemize}

This work not only establishes a new state-of-the-art in directed link prediction but also offers actionable insights that inform the design of future models. Our contributions aim to bridge the gap between undirected and directed link prediction research, providing strong foundations for future work in this important area. 

\section{Related Work}\label{sec:literature}
For the review of literature, we focus on \emph{similarity-based heuristics} for link prediction, due to their practical applicability, and \emph{Graph Neural Networks (GNNs)}, which represent the state-of-the-art in the field. 

\textbf{Similarity-based Heuristics.} A variety of similarity-based heuristics have been developed for the task of link prediction, primarily focusing on quantifying node similarity to predict the likelihood of a connection. One of the earliest and simplest methods is the \emph{Common Neighbors (CN)} heuristic, where the number of shared neighbors between two nodes is used as an indicator of their likelihood to form a link. Extensions of this idea include the \emph{Jaccard Index (JI)} and \emph{Adamic-Adar index (AA)}, which provide weighted variations by considering the degree of shared neighbors~\cite{adamic2003friends, liben2007link}. The \emph{Preferential Attachment (PA)} heuristic, based on the idea that high-degree nodes are more likely to attract additional links, is another widely used method~\cite{newman2001clustering}. \emph{Local Path index (LP)}~\citep{lu2009similarity} expands upon the idea of common neighbors by considering paths of length two between node pairs. It balances between global and local information, providing a broader view of node similarity while still being computationally feasible for large graphs. \emph{Resource Allocation index (RA)}~\citep{zhou2009predicting} is another similarity-based measure, where the likelihood of a link is determined by how resources (or connections) are shared between two nodes via their common neighbors. It gives higher weight to common neighbors with lower degrees, assuming that connections from lower-degree nodes are more significant. While these heuristics are computationally efficient and effective in many settings, they are primarily designed for undirected graphs and tend to struggle when applied to directed graphs. These methods also assume that local structural properties of the graph are sufficient for prediction, limiting their ability to capture more complex relational patterns. Despite these limitations, similarity-based heuristics remain popular due to their simplicity and interpretability, often serving as strong baselines for more advanced models like GNNs. 

\textbf{Graph Neural Networks (GNNs).} Most of the popular GNNs~\citep{kipf2016semi, hamilton2017inductive, velivckovic2017graph, ying2018graph} primarily focus on node representation in undirected graphs. Several studies have specifically addressed various aspects of directed graphs. For example, GatedGCN~\citep{li2015gated}, which employs separate aggregations for in-neighbors and out-neighbors in directed graphs, has proven effective for solving the genome assembly problem~\citep{vrvcek2022learning}. Additionally, research has aimed to generalize spectral convolutions for directed graphs~\cite{ma2019spectral, monti2018motifnet, tong2020directed, tong2020digraph}. A notable contribution is made by \citet{zhang2021magnet}, who present a spectral method that utilizes a complex matrix for graph diffusion, where the real part represents the undirected adjacency and the imaginary part captures the edge direction. Building on their work, \citet{geisler2023transformers} proposed a positional encoder that integrates transformers into directed graphs. More recently, \citet{rossi2024edge} emphasized that effective link prediction in directed graphs necessitates distinct aggregation strategies for incoming and outgoing edges to fully leverage directional information. They proposed a novel and generic \emph{Directed Graph Neural Network (Dir-GNN)} that can be integrated with any message-passing neural network by implementing separate aggregations of incoming and outgoing edges. 

Once the learned node embeddings are obtained, the link prediction problem can be framed as a supervised binary classification task. In this context, the input consists of a pair of node embeddings corresponding to the link of interest, while the output is a score that quantifies the probability of the existence of that link. Various decoders have been proposed to achieve this classification, each with distinct methodologies. One of the simplest and most widely used decoders is the dot product decoder~\citep{kipf2016semi}. However, this method fails to account for the directionality of links since the dot product is commutative. For instance, in a transactional context, the likelihood of person A transferring money to person B is treated the same as that of person B transferring money to person A, which hinders accurate predictions of money flow. To address the limitations of the dot product, Bilinear Decoders introduce a learned weight matrix, providing a more nuanced approach to edge prediction~\citep{yang2014embedding}. Another method for quantifying the similarity between two nodes is the distance-based decoder, which predicts edge existence based on the distance between node embeddings~\citep{ou2016asymmetric}. Matrix factorization-based decoders decompose the adjacency matrix into low-rank matrices representing node embeddings. The reconstructed adjacency matrix can then be used for link prediction~\citep{tang2015line}. Neural Tensor Network (NTN)~\citep{socher2013reasoning} replaces the standard linear layer with a bilinear tensor layer, directly relating the two nodes.  A novel decoder inspired by Newton's theory of universal gravitation was introduced by~\cite{salha2019gravity}. This approach uses node embeddings to reconstruct asymmetric relationships, facilitating effective directed link prediction.

\section{Background and Preliminaries}\label{sec:background}
In this section we introduce the notation employed throughout the paper in addition to metrics and datasets used for experiments. Next, we provide an overview of two main frameworks adapted to address directed link prediction problem in our analysis: (1) similarity-based heuristics and (2) Graph Neural Networks (GNNs). 

\textbf{Notation.} Given a graph $\mathcal{G}= (\mathcal{V}, \mathcal{E})$ where $\mathcal{V}$ and $\mathcal{E}$ denote the set of vertices and edges respectively, \emph{directed link prediction} refers to the task of predicting the existence of an edge \emph{from} $u \in\mathcal{V}$ \emph{to} $v \in\mathcal{V}$. The adjacency matrix of $\mathcal{G}$ is denoted by $\mathbf{A}$, where $\mathbf{A}_{uv} = 1$ if there is a directed edge from node $u$ to $v$, and $\mathbf{A}_{uv} = 0$, otherwise. Additionally, we denote the neighbourhood of a given node $u$ as $\mathcal{N}(u)$ throughout the paper. Each node $u \in \mathcal{V}$ is associated with a feature vector $\mathbf{x} \in \mathbb{R}^{d}$ where $d$ is the feature dimensionality.  

\textbf{Metrics.} In general, to evaluate the prediction performance of a given method, the set of edges is divided into disjoint sets of training  and testing splits; $\mathcal{E}_{\operatorname{train}}$ and $\mathcal{E}_{\operatorname{test}}$, respectively. In this paper, we evaluate the predictive performance mainly by mean reciprocal rank (MRR), which is calculated as the average of the reciprocal ranks of the true positives in the test set. The MRR is formulated as follows:
\begin{equation}\label{eq:mrr}
\text{MRR} = \frac{1}{|\mathcal{E}_{\operatorname{test}}|} \sum_{(u,v) \in \mathcal{E}_{\operatorname{test}}} \frac{1}{\text{rank}(u,v)},
\end{equation}
where $\text{rank}(u,v)$ is the rank of the true link $(u,v)$ among possible candidate links involving node $u$.

\textbf{Datasets.} In our experiments, we evaluate directed link prediction performance of various approaches using six benchmark datasets: \textsc{Cora}~\citep{yang2016revisiting}, \textsc{CiteSeer}~\citep{yang2016revisiting}, \textsc{Chameleon}~\citep{rozemberczki2021multiscale}, \textsc{Squirrel}~\citep{rozemberczki2021multiscale}, \textsc{Blog}~\citep{he2022digrac}, \textsc{WikiCS}~\citep{mernyei2020wikics}. All datasets are directed and come from the \textsc{PyTorch Geometric Signed Directed} software package~\citep{he2024pytorch}. We use the directed version of \textsc{Cora} and \textsc{Citeseer} and not their commonly used undirected versions. The details regarding datasets are provided in Appendix \ref{app:datasets}. 

\subsection{Similarity-based Heuristics}
In general, similarity-based heuristic methods used for link prediction assign a similarity score $S(u, v)$ for each pair of nodes, such that $S(u, v)$ serves as an estimator on the likelihood of a link between node $u$ and node $v$. For example, the Resource Allocation (RA) heuristic score between node $u$ and $v$ is calculated as follows:
\begin{equation}
    S_{\operatorname{RA}}(u, v) = \sum_{t \in \mathcal{N}(u) \cap \mathcal{N}(v)}\frac{1}{|\mathcal{N}(t)|}.
\end{equation}

By definition, RA is a symmetric score function, i.e. $S_{\operatorname{RA}}(u, v) = S_{\operatorname{RA}}(v, u)$. In order to adapt the existing similarity scores into directed settings, we utilize \emph{directed neighborhood operator}. Let $\mathcal{N}_{\operatorname{in}}(u)$ ($\mathcal{N}_{\operatorname{out}}(u)$) consists of all nodes that have a directed edge pointing toward (originating from) node $u$. Formally:
\begin{equation}\label{eq:directed_neighborhood}
    \mathcal{N}_{\operatorname{in}}(u) = \{v \in \mathcal{V} \mid (v, u) \in \mathcal{E}\}, \;
    \mathcal{N}_{\operatorname{out}}(u) = \{v \in \mathcal{V} \mid (u, v) \in \mathcal{E}\}.
\end{equation}
Using $\mathcal{N}_{\operatorname{in}}(\cdot)$ and $\mathcal{N}_{\operatorname{out}}(\cdot)$,
one can define four variants of common neighbourhood for a given node pair $u$ and $v$ by $\mathcal{N}_{d_u}(u) \cap \mathcal{N}_{d_v}(v)$ where $d_u, d_v \in \{\operatorname{in}, \operatorname{out}\}$. For example, given the set of nodes that has an incoming link from $u$ and has an outgoing link to $v$, the corresponding RA would be calculated as follows:
\begin{equation}
    S_{\operatorname{RA, in-out}}(u, v) = \sum_{t \in \mathcal{N}_{\operatorname{in}}(u) \cap \mathcal{N}_{\operatorname{out}}(v)}\frac{1}{|\mathcal{N}(t)|}.
\end{equation}
More details regarding the individual methods used in our experiments are provided in Appendix~\ref{app:heuristics}.

\subsection{Graph Neural Networks (GNNs)}
\textbf{Encoders.} Most common form of encoder used in GNNs is \emph{Message Passing Neural Networks (MPNNs)} in which vector-based messages are passed between nodes and then updated through neural networks to generate node embeddings. MPNNs initialize a set of the hidden node embeddings $\mathbf{h}_u^{(0)}, \forall u \in \mathcal{V}$ by input features, i.e., $\mathbf{h}_u^{(0)}=\mathbf{x}_u$. At the $k^{\text{th}}$ iteration of message passing hidden embeddings $\mathbf{h}_u^{(k)}$ for each node $u \in\mathcal{V}$ are updated as follows:
\begin{equation}
    \mathbf{h}_u^{(k+1)} = f_{\operatorname{update}}^{(k)}\left(\mathbf{h}_u^{(k)}, f_{\operatorname{aggregate}}^{(k)}\left(\left\{\mathbf{h}_v^{(k)}, \forall v \in \mathcal{N}(u)\right\}\right)\right),
\end{equation}
where $f_{\operatorname{update}}(\cdot)$ and $f_{\operatorname{aggregate}}(\cdot)$ are choice of differentiable functions. The final hidden embeddings are used as the output node embeddings $\mathbf{e}_u^{(0)}, \forall u \in \mathcal{V}$:
\begin{equation}
    \mathbf{e}_u = \mathbf{h}_u^{(K)}, \forall u \in \mathcal{V}, 
\end{equation}
where $K$ denotes the total number of update layers. In the functional form, an MPNN can be summarized as follows:
\begin{equation}\label{eq:mpnn}
    f_{\operatorname{MPNN}}: \mathcal{G}=(\mathcal{V}, \mathcal{E}) \text{ and } \mathbf{x}_u, \forall u \in \mathcal{V}\longmapsto \mathbf{e}_u, \forall u \in \mathcal{V}.
\end{equation}
In terms of link prediction task, MPNNs serve as an encoder that maps structural information of the graph together with node features into a set of node embeddings. 

\textbf{Decoders.}\label{sec:decoder} In the context of link prediction the decoder maps the embeddings for a given link to an individual score that corresponds to its likelihood to exist. For edge-wise link-prediction it is common to use one of two link predictors: the dot product (DP) $f_{\operatorname{DP}}(v_i, v_j) = \sigma(\mathbf{e}_i^T \mathbf{e}_j)$ and the hadamard product (HMLP) $f_{\operatorname{HMLP}}(v_i, v_j) = f_{\operatorname{MLP}}(\mathbf{e}_i \pdot \mathbf{e}_j)$. Both of these decoders are symmetric, (\textit{i.e.} $f(v_i, v_j) = f(v_j, v_i)$), which is a desirable property in undirected graphs but might not be so desirable in undirected settings. It is straight forward to extend both DP and HMLP to directed settings through the insertion of a learnable matrix that looks spiritually like a learnable metric tensor. Doing so, we introduce two variants we term matrix dot product (mDP), $f_{\operatorname{mDP}}(v_i, v_j) = \sigma(\mathbf{e}_i^T \mathbf{W} \mathbf{e}_j)$, and matrix HMLP, $f_{\operatorname{mHMLP}}(v_i, v_j) = f_{\operatorname{MLP}}( \mathbf{W} \mathbf{e}_i \pdot \mathbf{e}_j )$, where $\mathbf{W}$ is a learnable matrix. In addition, we define a trivially asymmetric decoder named Concat MLP (CMLP) and its matrix extension, matrix Concat MLP (mCMLP), defined as:

\begin{equation}
f_{\operatorname{CMLP}}=f_{\operatorname{MLP}}(\mathbf{e}_i \concat \mathbf{e}_j) \;\; \textrm{and} \;\; f_{\operatorname{mCMLP}} = f_{\operatorname{MLP}}( \mathbf{W} \mathbf{e}_i \concat \mathbf{e}_j).
\end{equation}

\section{Analysis of Directionality for Link Prediction}\label{sec:analysis}
In this section, we consider each of the design principles independently and perform experiments that allow us to understand the impact of directionality on each one. All experiments are performed on three different datasets: \textsc{Cora}, \textsc{Chameleon}, and \textsc{Blog}, The results are averaged over ten runs, and and the relevant hyperparameters are optimized using \textsc{Optuna}~\citep{akiba2019optuna}. \footnote{
In the ablation study results presented in Tables~\ref{t:ablation_encoder}-\ref{t:ablation_SFs}, the best-performing version for each dataset is highlighted in \textcolor{junscolor1}{orange}.}

\textbf{Directed vs Undirected Graph Encoders.} Traditionally, the form and structure of the graph encoder have been the main area of focus in the GNN literature. Thus, we explore multiple GNN encoding architectures to understand the extent to which directionally aware graph encoders impact the predictive performance. We constructed an experiment where comparing two standard graph encoders - a GCN and GraphSAGE -- with a directionally aware convolution, DirGNN. We performed this comparison by holding the decoder fixed and conducting a hyperparameter search over the relevant hyperparameters of the encoder itself. We present the results of the experiment in Table~\ref{t:ablation_encoder}.

We observe that in two of three datasets, DirGNN performs better than the other encoders. It is interesting to note that the "uplift offered by more complex encoders provides only modest gains. This suggests that in directed settings, DirGNN is a sensible first choice as a graph encoder because it either performs better than others, or is within the error bars of the best. 

\begin{table}[h]
\caption{Encoder comparisons in terms of MRR.} 
\centering
\begin{tabular}{l c c c}
\textit{Encoder}  & \textsc{Cora}  & \textsc{Chameleon} & \textsc{Blog}  \\
\hline
GCN                 & 0.401${\scriptstyle\pm0.062}$ & 0.595${\scriptstyle\pm0.058}$ &  \textcolor{junscolor1}{0.283${\scriptstyle\pm0.039}$} \\
GraphSage           & 0.414${\scriptstyle\pm0.077}$ & 0.603${\scriptstyle\pm0.062}$ &  0.275${\scriptstyle\pm0.027}$ \\
DirGNN              & \textcolor{junscolor1}{0.503${\scriptstyle\pm0.088}$} & \textcolor{junscolor1}{0.609${\scriptstyle\pm0.026}$} &  0.278${\scriptstyle\pm0.024}$ 
\end{tabular}
\label{t:ablation_encoder}
\end{table}

\textbf{Directed vs Undirected Graph Decoders.} Because link prediction is traditionally viewed as an edge-wise prediction task with a link predictor taking the form $f(v_i, v_j) \rightarrow \mathbb{R^+}$, we next explore whether learning a link predictor with an asymmetric decoder (\textit{e.g.}, $f(v_i, v_j) \neq f(v_j, v_i)$) leads to better predictive performance. 

To explore this, we constructed an experiment where the encoder was held fixed, and varied the decoder over two symmetric and four asymmetric decoders. For mathematical definitions of all decoders, please see Section~\ref{sec:decoder}. The results of this experiment are reported in Table~\ref{t:ablation_decoder}. We observe that asymmetric decoders outperform symmetric ones across all three datasets, confirming our intuition that asymmetry is an important property to capture. Within the asymmetric decoders, we find that both mHMLP and CMLP outperform the others. Because mHMLP amounts to learning a pseudo-metric which can be unstable due to the many possible degeneracies, we use CMLP in the subsequent work.

\begin{table}[h]
\caption{Decoder comparisons in terms of MRR.} 
\centering
\begin{tabular}{l c c c}
\textit{Decoder}  & \textsc{Cora}  & \textsc{Chameleon} & \textsc{Blog}  \\
\hline
DP      & X${\scriptstyle\pm X}$ & 0.303${\scriptstyle\pm0.021}$ &  0.115${\scriptstyle\pm0.016}$ \\
HMLP    & 0.178${\scriptstyle\pm0.046}$ & 0.261${\scriptstyle\pm0.047}$ &  0.113${\scriptstyle\pm0.024}$ \\
\hline
CMLP    & 0.500${\scriptstyle\pm0.087}$ & 0.289${\scriptstyle\pm0.102}$ &  \textcolor{junscolor1}{0.160${\scriptstyle\pm0.023}$} \\
mDP     & 0.247${\scriptstyle\pm0.066}$ & 0.214${\scriptstyle\pm0.095}$ &  0.105${\scriptstyle\pm0.028}$ \\
mHMLP   & \textcolor{junscolor1}{0.621${\scriptstyle\pm0.489}$} & \textcolor{junscolor1}{0.320${\scriptstyle\pm0.066}$} &  0.147${\scriptstyle\pm0.055}$ \\
mCMLP   & 0.248${\scriptstyle\pm0.072}$ & 0.136${\scriptstyle\pm0.082}$ &  0.131${\scriptstyle\pm0.023}$
\end{tabular}
\label{t:ablation_decoder}
\end{table}

\textbf{Directed vs Undirected Labeling Tricks.} Labeling tricks are one technique for breaking the node-automorphism symmetry which limits the expressivity of GNNs for link prediction~\cite{zhang2021labeling}. To do this, the node-features of a vertex are augmented by labels that connote some structural information. Popular labeling tricks include distance encoding~\citep{li2020distance} and double radius node labeling~\citep{zhang2018link}. However, in this work, we consider only the directed extension of distance encoding due to its simplicity. Indeed, the directed extension of the distance encoding described in~\citep{li2020distance} simply involves computing the distance between two vertices in a directed fashion as defined in Equation~\ref{eq:dir_de}. It is canonical to define a maximum distance to limit the computational expense of path finding. In undirected settings, this maximum distance is often on the order of 3-5~\citep{chamberlain2023graph}. In directed settings, this maximum may need to be larger to account for the fact that $d_{dir}(u, v) \ge d_{undir}(u, v) $. 

To understand the impact of a directed distance encoding on the predictive performance of GNN, we conducted an experiment where all modeling parameters were held fixed, and only the labeling trick was varied. The results are reported in Table~\ref{t:ablation_labeling}. For the labeling trick, we constructed three variants of both the directed and undirected distance encodings. These variants are de3, de5, delog; which correspond to distance encodings with  a maximum distance of 3, 15, and no maximum but log-transformed. The -d and -u labels indicate that the method is directed or undirected, respectively. In these experiments, we observe that the directionality provides improvements across all datasets, but the size of impact varies significantly. In the example of \textsc{Cora}, we observe a 50\% improvement, while both \textsc{Chameleon} and \textsc{Blog} have much more modest gains. Interestingly, we find that the maximum distance cutoff does not correlate in a predictable fashion with performance. Based on these results, we conclude that directionality should be accounted for when using a labeling trick during modeling, and that the maximum distance cutoff should be carefully tuned.

\begin{table}[h]
\caption{Comparison of undirected and directed labeling tricks  in terms of MRR. } 
\centering
\begin{tabular}{l c c c}
\textit{Labeling}  & \textsc{Cora}  & \textsc{Chameleon} & \textsc{Blog}  \\
\hline
de3-u      & 0.283${\scriptstyle\pm0.044}$ & 0.398${\scriptstyle\pm0.069}$ &  0.137${\scriptstyle\pm0.024}$ \\
de15-u     & 0.324${\scriptstyle\pm0.049}$ & 0.385${\scriptstyle\pm0.109}$ &  0.116${\scriptstyle\pm0.031}$ \\
delog-u    & 0.270${\scriptstyle\pm0.049}$ & 0.392${\scriptstyle\pm0.116}$ &  0.120${\scriptstyle\pm0.017}$ \\
\hline
de3-d        & \textcolor{junscolor1}{0.498${\scriptstyle\pm0.071}$} & 0.289${\scriptstyle\pm0.102}$ &  \textcolor{junscolor1}{0.160${\scriptstyle\pm0.023}$} \\
de15-d      & 0.493${\scriptstyle\pm0.067}$ & \textcolor{junscolor1}{0.405${\scriptstyle\pm0.065}$} &  $0.125{\scriptstyle\pm0.035}$ \\
delog-d      & 0.305${\scriptstyle\pm0.050}$ & 0.397${\scriptstyle\pm0.077}$ &  0.109${\scriptstyle\pm0.027}$
\end{tabular}
\label{t:ablation_labeling}
\end{table}

\textbf{Directed vs Undirected Negative Sampling.} We next turn our attention to exploring the effects of directionality in negative sampling. Link prediction is traditionally constructed as a binary classification task, where positive samples are observed edges and negative samples are unobserved edges. Enumerating all negative samples is intractable, so it is common to sample a subset of those negatives for training. It is possible to generate negative samples in either a directed or an undirected fashion. Directed negative sampling generates a dataset $\mathcal{D}^n = \{ (u, v): u, v \sim \mathcal{V} \}$ with $(u, v) \neq (v, u)$. In undirected negative sampling, $\mathcal{D}^n = \{ (u, v): u, v \sim \mathcal{V} \}$ with $(u, v) = (v, u)$. 

To explore this design principle, we constructed an experiment where we held the model parameters fixed, and altered only the negative sampling strategy. In this experiment we used DirGNN as the encoder, CMLP as the decoder, de15 as the labeling method, and all structural features. The results for this experiment are presented in Table~\ref{t:ablation_sampling}. We note across all three datasets that directed negative sampling provides a lift in MRR, although in two of the three datasets, the lift is modest. We conclude that there is evidence to support the usage of directed negative sampling, but that these effects are smaller than other design principles.

\begin{table}[h]
\caption{Comparison of undirected and directed negative sampling in terms of MRR.}
\centering
\begin{tabular}{l c c c}
\textit{Sampling}  & \textsc{Cora}  & \textsc{Chameleon} & \textsc{Blog} \\
\hline
Undirected              & 0.48${\scriptstyle\pm0.12}$ & 0.60${\scriptstyle\pm0.07}$ &  0.27${\scriptstyle\pm0.03}$ \\
Directed                & \textcolor{junscolor1}{0.50${\scriptstyle\pm0.09}$} & \textcolor{junscolor1}{0.61${\scriptstyle\pm0.03}$} & \textcolor{junscolor1}{0.28${\scriptstyle\pm0.02}$} \\
\end{tabular}
\label{t:ablation_sampling}
\end{table}

\textbf{Directed vs Undirected Structural Features.} Previous work has shown that the inclusion of edge-wise structural features, such as the number of shared common neighbors at k-hops, leads to significant performance improvements for link prediction~\citep{zhang2024learning, ai2022structure, zhang2018link}. Indeed, this intuitively makes sense because these structural features are the building blocks for heuristic similarity measures such as Adamic-Adar or Resource-Allocation, both of which represent strong baselines in undirected link prediction settings. The definition of our structural features can be found in Equations~\ref{eq:dsf1}-\ref{eq:dsf3}.

To understand whether directionality affects the performance of models that include structure features, we constructed an experiment where we used DirGNN as the encoder, and CMLP as the decoder, while varying the structural features. The results are reported in Table~\ref{t:ablation_SFs}. We observe that across all three datasets, directed structural features provide a significant improvement over their undirected variants. In percentage-increase terms, we find an uplift of 30\% to 50\% through the inclusion of directionality, indicating that this is a significant design principle. The improvement is larger than the associated uncertainties, giving us confidence that this improvement is robust.

\begin{table}[h]
\caption{Comparison of undirected and directed structural features in terms of MRR. } 
\centering
\label{t:ablation_SFs}
\begin{tabular}{l c c c}
\textit{SFs}  & \textsc{Cora}  & \textsc{Chameleon} & \textsc{Blog}  \\
\hline
undirected      & 0.309${\scriptstyle\pm0.051}$ & 0.348${\scriptstyle\pm0.020}$ &  0.174${\scriptstyle\pm0.020}$ \\
directed        & \textcolor{junscolor1}{0.412${\scriptstyle\pm0.064}$} & \textcolor{junscolor1}{0.534${\scriptstyle\pm0.038}$} &  \textcolor{junscolor1}{0.251${\scriptstyle\pm0.033}$}
\end{tabular}
\end{table}

\section{Proposed Model}\label{sec:proposed}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/model_architecture.pdf}
    \caption{\textbf{Overview of DirLP.} Given the input graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ and node features $\mathbf{x} \in \mathbb{R}^{d_{\operatorname{raw}}}, \forall u \in \mathcal{V}$, DirLP follows a series of steps to predict directed links. First, a set of negative edges $\mathcal{E}^{-}$ is generated. Next, for each edge $(u, v)$ in the set $\mathcal{E} \cup \mathcal{E}^{-}$, structural edge features $\mathbf{z}_{(u,v)} \in \mathbb{R}^{d_{\operatorname{str}}}$ are computed. Then, directional labels are assigned to each node $u \in \mathcal{V}$, and intermediate node features $\mathbf{x}'_u \in \mathbb{R}^{d_{\operatorname{raw}} + d_{\operatorname{label}}}$ are constructed by concatenating the original node features with the directional labels. The model then applies DirGNN message passing to produce node embeddings $\mathbf{e}_u \in \mathbb{R}^{d_{\operatorname{enc}}}$ for $u\in\mathcal{V}$. For each edge $(u, v)$ in $\mathcal{E} \cup \mathcal{E}^{-}$, the edge features are concatenated with the node embeddings of the edge's endpoints. Finally, these concatenated embeddings are passed through an MLP followed by a sigmoid activation function to make predictions.}
\label{fig:model_architecture}
\end{figure}

Based on the insights drawn through the experiments conducted on analyzing directionality, we propose a framework, namely \emph{DirLP}, for directed link prediction. DirLP features the following key components: A directed labeling trick, a directed encoder, a directed structure feature extractor, and an asymmetric decoder. An overview of model architecture is provided in Figure~\ref{fig:model_architecture}.  In the remainder of this section, we describe each element and how they're combined to form DirLP.

We begin with the \textbf{directed labeling trick}, which injects structural information into our graph encoder, and is defined as:
    \begin{equation}\label{eq:dir_de}
        \mathbf{l}_t = \left[\operatorname{d}^\delta(t, v) \concat \operatorname{d}^{\delta}(t, v) \forall v \in \mathcal{V} \right], 
    \end{equation}
where $\operatorname{d}^\delta$ is the truncated graph distance defined as $\operatorname{d}^\delta(t, v) = \operatorname{min}(\delta, d(t, v))$, $t$ is the landmark vertex, and $\delta$ is the maximum distance. For DirLP, we use two fixed landmarks.

We utilize \textbf{Directed Graph Neural Network (DirGNN) as encoder}, which aggregates messages from incoming and outgoing edges separately for each node, and obtain layer updates by a non-parametric combinator function~\citep{rossi2024edge}.  More formally, DirGNN initializes the hidden node embeddings by intermediate node features, i.e., $\mathbf{h}_u^{(0)}=\mathbf{x}'_u$ for all $u \in \mathcal{V}$. With our choices of aggregation and update functions, GraphSage~\citep{hamilton2017inductive} and convex combination~\citep{rossi2024edge}, respectively, at the $k^{\text{th}}$ layer of encoder node embeddings $\mathbf{h}_u^{(k)}$ are updated as follows:
    \begin{align}\label{eq:dirlp_mpnn}
        \mathbf{m}_{u, \operatorname{in}}^{(k+1)} &= \mathbf{W}^{(k)}_{\operatorname{in, self}}\mathbf{h}_u^{(k)} + \mathbf{W}^{(k)}_{\operatorname{in}} \frac{\sum_{v \in \mathcal{N}_{\operatorname{in}}(u)}\mathbf{h}_v^{(k)}}{|\mathcal{N}_{\operatorname{in}}(u)|}, \\
        \mathbf{m}_{u, \operatorname{out}}^{(k+1)} &= \mathbf{W}^{(k)}_{\operatorname{out, self}}\mathbf{h}_u^{(k)} + \mathbf{W}^{(k)}_{\operatorname{out}} \frac{\sum_{v \in \mathcal{N}_{\operatorname{out}}(u)}\mathbf{h}_v^{(k)}}{|\mathcal{N}_{\operatorname{out}}(u)|}, \\
        \mathbf{h}_{u}^{(k+1)} &= \alpha \times \mathbf{m}_{u, \operatorname{in}}^{(k+1)} + (1 - \alpha) \times \mathbf{m}_{u, \operatorname{out}}^{(k+1)},
    \end{align}
where $\mathbf{W}^{(k)}_{\operatorname{in}}, \mathbf{W}^{(k)}_{\operatorname{in}, \operatorname{self}}, \mathbf{W}^{(k)}_{\operatorname{out}}, \mathbf{W}^{(k)}_{\operatorname{out}, \operatorname{self}}$ are learnable parameters and $\alpha$ is a hyperparameter that controls the trade-off of emphasis between incoming and outgoing edges. In our experiments, we set $\alpha=0.5$ for all datasets to equally treat directions. The node embeddings are set to final layer output, i.e.,  $\mathbf{e}_u = \mathbf{h}_u^{(K)}, \forall u \in \mathcal{V}$, where $K$ denotes the total number of update layers.

\begin{table}[!t]
\caption{Comparison of baseline methods and our proposed model in terms MRR on directed link prediction task. The top three models are highlighted as \firstbest{First}, \secondbest{Second}, \thirdbest{Third}. Note that The Blog dataset does not have vertex features and therefore MLP model built from vertex features do not apply.}
\centering
% \begin{center}
% \resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c}
& \textsc{Cora}  & \textsc{CiteSeer} & \textsc{Chameleon} & \textsc{Squirrel} & \textsc{Blog} & \textsc{WikiCS} \\ \hline
LP, sym	& 0.315${\scriptstyle\pm0.065}$ & \thirdbest{0.303${\scriptstyle\pm0.073}$} 
&  $0.235{\scriptstyle\pm0.013}$ &  $0.102{\scriptstyle\pm0.005}$ &  $0.096{\scriptstyle\pm0.021}$ &  \secondbest{0.661${\scriptstyle\pm0.011}$} \\
LP, asym	& 0.324${\scriptstyle\pm0.056}$ & 0.146${\scriptstyle\pm0.024}$ 
&  \secondbest{0.381${\scriptstyle\pm0.075}$} &  \secondbest{0.497${\scriptstyle\pm0.151}$} &  \secondbest{0.149${\scriptstyle\pm0.030}$} &  0.424${\scriptstyle\pm0.067}$ \\
RA, sym	& 0.356${\scriptstyle\pm0.07}$ & 0.293${\scriptstyle\pm0.049}$ 
&  $0.194{\scriptstyle\pm0.069}$ &  $0.087{\scriptstyle\pm0.013}$ &  $0.082{\scriptstyle\pm0.021}$ &  $0.358{\scriptstyle\pm0.074}$ \\
RA, asym	& 0.292${\scriptstyle\pm0.046}$ & 0.134${\scriptstyle\pm0.021}$ 
&  $0.353{\scriptstyle\pm0.142}$ &  $0.148{\scriptstyle\pm0.037}$ &  $0.103{\scriptstyle\pm0.024}$ &  \thirdbest{0.494${\scriptstyle\pm0.101}$} \\
AA, sym	& 0.353${\scriptstyle\pm0.067}$ & 0.254${\scriptstyle\pm0.048}$ 
&  $0.239{\scriptstyle\pm0.016}$ &  $0.103{\scriptstyle\pm0.005}$ &  $0.096{\scriptstyle\pm0.023}$ &  $0.285{\scriptstyle\pm0.033}$ \\
AA, asym	& 0.288${\scriptstyle\pm0.045}$ & 0.122${\scriptstyle\pm0.02}$ 
&  \thirdbest{0.378${\scriptstyle\pm0.091}$} &  \thirdbest{0.495${\scriptstyle\pm0.196}$} &  \thirdbest{0.143${\scriptstyle\pm0.034}$} &  $0.487{\scriptstyle\pm0.060}$ \\
\hline
MLP	& 0.172${\scriptstyle\pm0.03}$ & \secondbest{0.356${\scriptstyle\pm0.084}$} 
&  $0.104{\scriptstyle\pm0.029}$ &  $0.051{\scriptstyle\pm0.027}$ &  - &  $0.019{\scriptstyle\pm0.006}$ \\
GAT	& 0.087${\scriptstyle\pm0.036}$ & 0.115${\scriptstyle\pm0.043}$ 
&  $0.150{\scriptstyle\pm0.026}$ &  $0.088{\scriptstyle\pm0.038}$ &  $0.057{\scriptstyle\pm0.011}$ &  $0.094{\scriptstyle\pm0.010}$ \\
GCN	& \thirdbest{0.402${\scriptstyle\pm0.062}$} & 0.208${\scriptstyle\pm0.048}$  
&  $0.270{\scriptstyle\pm0.043}$ &  $0.260{\scriptstyle\pm0.018}$ &  $0.080{\scriptstyle\pm0.025}$ &  $0.277{\scriptstyle\pm0.047}$ \\
GraphSage	& \secondbest{0.414${\scriptstyle\pm0.077}$} & 0.158${\scriptstyle\pm0.063}$ 
&  $0.202{\scriptstyle\pm0.046}$ &  $0.190{\scriptstyle\pm0.074}$ &  $0.083{\scriptstyle\pm0.016}$ &  $0.185{\scriptstyle\pm0.058}$ \\
\hline
DirLP	& \firstbest{0.504${\scriptstyle\pm0.088}$} & \firstbest{0.480${\scriptstyle\pm0.108}$} & \firstbest{0.657${\scriptstyle\pm0.037}$} &  \firstbest{0.759${\scriptstyle\pm0.012}$} &  \firstbest{0.280${\scriptstyle\pm0.031}$} &  \firstbest{0.752${\scriptstyle\pm0.028}$} \\
\end{tabular}
% }
% \end{center}
\label{t:main_results}
\end{table}

We perform \textbf{edge-wise structural feature extraction} to incorporate the directionally aware structural information at the edge-level. We define a set of \emph{neighbourhood directionality sequences} at length $n$, $\mathbb{S}_n = \left\{(s_1, \dots, s_n): \forall s_i \in \{\operatorname{in}, \operatorname{out}\}\right\}$. Now, for a given node $u$ and directionality sequence $\mathbf{s} = (s_1, \dots, s_n)$, we define directional neighbourhood $\mathcal{N}^{\operatorname{dir}}_{\mathbf{s}}(u)$ such that $v \in \mathcal{N}^{\operatorname{dir}}_{\mathbf{s}}(u)$, if and only if $v$ is reachable from $u$ with an $n$-step walk where $i^{\operatorname{th}}$ step is in the direction of $s_i$. For an edge $(u, v)$, we compute the cardinality of the union (U) and intersection (I) of the directed neighborhoods of endpoints, in addition to the individual neighbourhoods of left (L) and right (R) side as follows: 
    \begin{align}
        & \mathbf{z}_{(u, v)}^{\operatorname{U}} = \left[
        |\mathcal{N}_{\mathbf{s}_1}(u) \cup \mathcal{N}_{\mathbf{s}_2}(v)| \right]_{\mathbf{s}_1, \mathbf{s}_2 \in \bigcup_{n=1}^N\mathbb{S}_n}, \\
        & \mathbf{z}_{(u, v)}^{\operatorname{L}} = \left[ |\mathcal{N}_{\mathbf{s}}(u)| \right]_{\mathbf{s} \in \bigcup_{n=1}^N\mathbb{S}_n}, \label{eq:dsf1} \\
        & \mathbf{z}_{(u, v)}^{\operatorname{I}} = \left[ |\mathcal{N}_{\mathbf{s}_1}(u) \cap \mathcal{N}_{\mathbf{s}_2}(v)| \right]_{\mathbf{s}_1, \mathbf{s}_2 \in \bigcup_{n=1}^N\mathbb{S}_n}, \\
        & \mathbf{z}_{(u, v)}^{\operatorname{R}} = \left[ |\mathcal{N}_{\mathbf{s}}(v)| \right]_{\mathbf{s} \in \bigcup_{n=1}^N\mathbb{S}_n}, \label{eq:dsf2} \\
        & \mathbf{z}_{(u, v)}^{\operatorname{dir}} = \mathbf{z}_{(u, v)}^{\operatorname{ U}} \concat \mathbf{z}_{(u, v)}^{\operatorname{I}} \concat \mathbf{z}_{(u, v)}^{\operatorname{L}} \concat \mathbf{z}_{(u, v)}^{\operatorname{R}} \label{eq:dsf3},
    \end{align}
    where $N$ is the maximum radius.
Next, we compute the undirected versions over symmetrized adjacency matrix that define neighbourhood $\mathcal{N}_{k}(u)$ which involves nodes at $k$-hop distance to a given node $u$:
\begin{align}
    \mathbf{z}_{(u, v)}^{\operatorname{undir}} 
    &= \big[|\mathcal{N}_{k}(u) \cup \mathcal{N}_{k}(v)|\big]_{k=1}^{N} \concat \big[|\mathcal{N}_{k}(u) \cap \mathcal{N}_{k}(v)|\big]_{k=1}^{N} \notag \\
    &\quad \concat \big[|\mathcal{N}_{k}(u)|\big]_{k=1}^{N} 
    \concat \big[|\mathcal{N}_{k}(v)|\big]_{k=1}^{N}.
\end{align}

Finally, the complete edge feature vector is obtained by concatenating the directed and undirected structural features; $\mathbf{z}_{(u, v)} = \mathbf{z}_{(u, v)}^{\operatorname{dir}} \concat \mathbf{z}_{(u, v)}^{\operatorname{undir}}$.
    
We employ a simple \textbf{feedforward network as decoder} that concatenates the edge features and node embeddings of source and target node and performs linear transformations through a \emph{Multi-layer Perceptron (MLP)}:
    \begin{equation}\label{eq:dirlp_decoder}
        \hat{y}_{u, v} = \sigma\left(f_{\operatorname{MLP}} \left(\mathbf{z}_{u, v} \concat \mathbf{e}_{u} \concat \mathbf{e}_{v} \right)\right), 
    \end{equation}
    where $f_{\operatorname{MLP}}(\cdot)$ is a feed forward network and $\sigma$ is the sigmoid function. 

\textbf{Expressivity.} Being able to represent edge direction through an asymmetric decoder and including information about directed triangle counts, DirLP is capable of distinguishing edges that conventional MPNNs are not able to which is stated by Theorem~\ref{thm:dirgnn}.

\begin{theorem}\label{thm:dirgnn}
    Let $\mathcal{M}_{\operatorname{sGNN}}$ be the family of GNNs defined by Equation~\ref{eq:mpnn} equipped with a symmetric decoder and augmented by undirected structural features. Additionally, let $\mathcal{M}_{\operatorname{DirLP}}$ be family of all models defined by Equation~\ref{eq:dirlp_decoder}. $\mathcal{M}_{\operatorname{DirLP}}$ is strictly more powerful than $\mathcal{M}_{\operatorname{sGNN}}$ ($\mathcal{M}_{\operatorname{sGNN}} \subset \mathcal{M}_{\operatorname{DirLP}}$).
\end{theorem}
This makes intuitive sense because our asymmetric decoder can represent all symmetric decoders, which allows DirLP to distinguish all links any symmetric GNNs can. We present the proof in Appendix~\ref{app:proof}.

\section{Principle Comparison}~\label{sec:experiments}
\textbf{Setup.} In our experiments, we generated ten sets of random splits of datasets for training, validation, and testing to facilitate 10-fold cross-validation. These dataset splits will be made publicly available upon official publication of this work. Each of the deep-learning models was optimized using the hyperparameter tuning framework \textsc{OPTUNA} \citep{akiba2019optuna}, with 48 optimization steps performed per model to find the best-performing configurations. The optimization process was conducted on the validation set focusing on the Mean Reciprocal Rank (MRR). The search space of \textsc{OPTUNA} and tuned hyperparameter settings for all deep-learning models are provided in Appendix~\ref{app:hyperparameter}. Experiments were run on an \textsc{Nvidia DGX A100} machine with 128 \textsc{AMD Rome 7742} cores and 8 \textsc{Nvidia A100} GPUs, utilizing \textsc{PyTorch Geometric 2.5.3} and \textsc{PyTorch 2.3.1} for model training and evaluation.

\textbf{Baselines.} We performed a set of principle comparison experiments between our proposed method, DirLP, and several baseline approaches, including both symmetric and asymmetric versions of similarity-based heuristics LP~\citep{lu2009similarity}, RA~\citep{zhou2009predicting}, and AA~\citep{adamic2003friends}, as well as four deep-learning based baselines: MLP, GAT~\citep{velivckovic2017graph}, GCN~\citep{kipf2016semi} and GraphSage~\citep{hamilton2017inductive}. The implementations of deep-learning models were based on official code provided in \textsc{PyTorch Geometric 2.5.3}, ensuring consistency and reproducibility. 

\textbf{Results.} The main results of our principle comparison experiments are presented in Table~\ref{t:main_results} in terms of MRR. Additionally, in Appendix~\ref{app:hits20}, we report the performance comparison in terms of Hits@20 in the Appendix in Table~\ref{t:hr20_results}. Based on these results, we draw several important conclusions. First, we observe that the asymmetric versions of heuristic methods consistently outperform their symmetric counterparts on datasets such as \textsc{Chameleon}, \textsc{Squirrel}, \textsc{Blog}, and \textsc{WikiCS}, with the only exception of LP on \textsc{WikiCS}. Reviewing the dataset statistics reported in Appendix~\ref{app:datasets}, we find a positive correlation between graph density and the performance advantage of using asymmetric methods. This suggests that as the complexity of the graph structure increases, the inclusion of directional information becomes more critical.

In all cases, heuristic methods outperform deep-learning baselines that do not incorporate directionality in the message-passing framework. This finding suggests that incorporating edge directionality can have a greater impact than increasing model complexity, particularly in many settings. Deep-learning baselines perform poorly, especially in cases where the node features offer limited information. For instance, on datasets \textsc{Blog} and \textsc{WikiCS}, which have relatively low vertex feature dimensionality compared to other datasets (see Appendix~\ref{app:datasets}), heuristic methods significantly outperform the deep-learning baselines. This highlights the importance of effectively modeling directionality in graph structure when node features are insufficient.

The principle comparison experiments clearly demonstrate the superiority of DirLP, which captures directionality through message-passing mechanisms and feature extraction both at the edge and node level. In many instances, DirLP delivers significantly superior performance compared to deep-learning baselines, highlighting its ability to model directional relationships more effectively. 

\begin{figure}[h]
    \label{fig:violin_plot}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/mrr_violin.png}
    \caption{\textbf{Statistical Comparison Between GraphSage and DirLP.} The violin plots illustrate the performance of GraphSage and DirLP in terms of MRR across multiple data splits.}
    \label{fig:mrr_violin}
\end{figure}

\textbf{Sensitivity.} A violin plot~\footnote{Violin plots provides a visual summary of the data distribution along with its probability density that is smoothed and symmetrized by a Kernel density estimation (KDE).} illustrating the comparison between DirLP and GraphSage, in terms of MRR, is shown in Figure~\ref{fig:mrr_violin}, offering further insights into the model's performance distribution across the data splits. It is observed that DirLP shows higher variance, however on the average it outperforms GraphSage clearly. 

\section{Conclusion}\label{sec:conclusion}
In conclusion, this paper introduces DirLP, a novel framework for directed link prediction that outperforms existing models across benchmark datasets. By leveraging an asymmetric decoder and directed structural features, DirLP effectively captures relationships in graphs where edge directionality is critical, highlighting the limitations of traditional undirected methods.

Rather than introducing a complex new architecture, our work focuses on systematically exploring the utility of simple, directed variants of existing techniques. Directed distance encoding and directed GNNs, though seemingly minor modifications, demonstrate substantial performance gains, emphasizing the practical value of incorporating directionality. This study serves as a guide for practitioners, showing how fundamental, interpretable methods can deliver strong results in directed settings.

While scalability remains a consideration due to preprocessing costs of edge-wise structural feature extraction, these are one-time operations that can be optimized. Future work will focus on enhancing the efficiency of our approach and extending evaluations to larger datasets to broaden its applicability.

This work sets a benchmark for directed link prediction and lays a foundation for future research, encouraging deeper exploration into the role of directionality and the development of scalable, high-performing solutions for directed graph tasks.

\subsubsection*{Acknowledgments}
All funding was provided by Block Inc.~\footnote{block.xyz}.

\bibliographystyle{plainnat}
\bibliography{reference}

\newpage
\appendix

\section{Dataset Details}~\label{app:datasets}
In our experiments, we evaluate directed link prediction performance of various approaches using six benchmark datasets: \textsc{Cora}~\citep{yang2016revisiting}, \textsc{CiteSeer}~\citep{yang2016revisiting}, \textsc{Chameleon}~\citep{rozemberczki2021multiscale}, \textsc{Squirrel}~\citep{rozemberczki2021multiscale}, \textsc{Blog}~\citep{he2022digrac}, \textsc{WikiCS}~\citep{mernyei2020wikics}. \textsc{Cora} and \textsc{CiteSeer} are citation networks where the nodes denote the papers and links denote the citations from one to another. Likewise, \textsc{Chameleon}, \textsc{Squirrel} and \textsc{WikiCS} are reference networks on Wikipedia pages in the corresponding topics where edges reflect reference links between them. \textsc{Blog} is a set of political blogs from the 2004 US presidential election with links recording mentions between them. 
\begin{table}[htbp]
    \caption{Dataset Statistics. $d$, $|\mathcal{V}|$, $|\mathcal{E}|$, ${|\mathcal{E}_\leftrightarrow|} / {|\mathcal{E}|}$, denote the number of node features, number of nodes, number of edges and ratio of node pairs connected in both direction to total number of edges, respectively. Grap density is calculated by $|\mathcal{E}| / (|\mathcal{V}| \times (|\mathcal{V}|-1))$.}
    \begin{center}
    \begin{tabular}{ccccccc}
     & \textsc{Cora} & \textsc{CiteSeer} & \textsc{Chameleon} & \textsc{Squirrel} & \textsc{Blog} & \textsc{WikiCS} \\ \hline
    $d$ & $1,433$ & $3,703$ & $2,325$ & $2,089$ & 0 & $300$ \\
    $|\mathcal{V}|$ & $2,708$ & $3,327$ & $2,277$ & $5,201$ & $1,222$ & $11,701$ \\
    $|\mathcal{E}|$ & $10,556$ & $9,104$ & $36,101$ & $217,073$ & $19,024$ & $297,110$ \\
    ${|\mathcal{E}_\leftrightarrow|} / {|\mathcal{E}|}$ & $6.1\%$ & $4.9\%$ & $26\%$ & $17\%$ & $24\%$ & $52\%$ \\
    Density & $0.1\%$ & $0.1\%$ & $0.7\%$ & $0.8\%$ & $1.3\%$ & $0.2\%$
    \end{tabular}
    \end{center}
\end{table}

\section{Heuristics Formulations}\label{app:heuristics}
In our experiments we use three different node similarity scores and their directed variants;  local path index (LP), resource allocation index (RA), Adamic–Adar index (AA). Their original formulations (symmetric versions) are formulated as follows for a pair of nodes $u, v \in \mathcal{V}$:
\begin{align}
    S_{\operatorname{LP, sym}}(u, v) &= \mathbf{\hat{A}}^2_{u, v} + \epsilon \mathbf{\hat{A}}^3_{u, v}, \\
    S_{\operatorname{RA, sym}}(u, v) &= \sum_{t \in \mathcal{N}(u) \cap \mathcal{N}(v)}\frac{1}{|\mathcal{N}(t)|}, \\
    S_{\operatorname{AA, sym}}(u, v) &= \sum_{t \in \mathcal{N}(u) \cap \mathcal{N}(v)}\frac{1}{\log{|\mathcal{N}(t)|}},
\end{align}
where $\mathbf{\hat{A}}$ denotes the symmetrized adjacency matrix and $\epsilon$ is a a free parameter set to $10^{-3}$ in our experiments. Asymmetric variant of LP is simply defined as follows:
\begin{equation}
    S_{\operatorname{LP, asym}}(u, v) = \mathbf{A}^2_{u, v} + \epsilon \mathbf{A}^3_{u, v}.
\end{equation}
Recall the definition for directed neighborhood operator:
\begin{equation}
    \mathcal{N}_{\operatorname{in}}(u) = \{v \in \mathcal{V} \mid (v, u) \in \mathcal{E}\}, \;
    \mathcal{N}_{\operatorname{out}}(u) = \{v \in \mathcal{V} \mid (u, v) \in \mathcal{E}\},
\end{equation}
where $\mathcal{N}_{\operatorname{in}}(u)(\mathcal{N}^{\operatorname{out}}(u))$ consists of all nodes that have a directed edge pointing toward (originating from) node $u$. The four directional variants of  AA and RA  for a given node pair $u$ and $v$ follows:
\begin{align}
    S_{\operatorname{RA,} d_u - d_v}(u, v) &= \sum_{t \in \mathcal{N}_{d_u}(u) \cap \mathcal{N}_{d_v}(v)}\frac{1}{|\mathcal{N}(t)|}, \\
    S_{\operatorname{AA,} d_u - d_v}(u, v) &= \sum_{t \in \mathcal{N}_{d_u}(u) \cap \mathcal{N}_{d_v}(v)}\frac{1}{\log|\mathcal{N}(t)|}.
\end{align}
The asymmetric variants of AA and RA on our baseline experiments are selected based on best performing version of directed common neighbourhood operator.

\section{Proof of Theorem~\ref{thm:dirgnn}}\label{app:proof}
Our proof follows two steps. First, we show that $\mathcal{M}_{\operatorname{sGNN}} \subseteq \mathcal{M}_{\operatorname{DirLP}}$. Next, Then, we construct a graph that exhibits an automorphic nodal structure for all elements of $\mathcal{M}_{\operatorname{sGNN}}$, but not for any element of $\mathcal{M}_{\operatorname{DirLP}}$.

The first half of the proof can be seen by examining the structure of DirLP, and the way in which it generalizes a GNN with a symmetric decoder and symmetric structural features. The general form for a sGNN is given by:
\begin{equation}
    \operatorname{f_{\operatorname{sGNN}}} = \operatorname{f_{mlp}}( \mathbf{z}^{\operatorname{undir}}_{u, v} || \mathbf{e}_u \pdot \mathbf{e}_v).
\end{equation}

Starting from Equation~\ref{eq:dirlp_decoder}, we see immediately that we can recover a symmetric for decoder for $\mathbf{e}_u \concat \mathbf{e}_v$ by selecting an initial layer that corresponds to two concatenated identity matrices. Turning our attention to the structural features, we again see that a special combination of the elements of $\mathbf{z}_{u, v}$ allows us to recover $\textbf{z}^{\operatorname{undir}}_{u, v}$. Namely, a trace over all modes allows us to construct the undirected structural features. This amounts to using a matrix of all 1s for the initial layer of our MLP. Putting this all together, we observe that we can convert our set of directed input features to their undirected variants using an MLP whose initial weight matrix is equal to $ [ \mathbf{B} \concat \mathbf{I}_{D \times D} \concat \mathbf{I}_{D \times D} ] $, where $\mathbf{B}$ is the desired trace matrix. Therefore, because there exists a DirLP that is equivalent to a sGNN, any edge that is distinguished by an sGNN must also be distinguishable by a DirLP. This is sufficient to prove that $\mathcal{M}_{\operatorname{sGNN}} \subseteq \mathcal{M}_{\operatorname{DirLP}}$.

\begin{figure}
    \begin{center}
    \includegraphics[width=0.3\textwidth]{figures/complete_graph.png}
    \end{center}
    \caption{A complete graph with four nodes.}
    \label{fig:complete_graph}
\end{figure}

We next turn to the task of making this relationship strict, which we do by constructing a graph for which sGNN cannot distinguish a node pair but DirLP can. To do this, we consider a complete graph with four nodes as shown in Figure~\ref{fig:complete_graph} and consider the edges $(v_0, v_1)$ and $v_0, v_3)$. In the undirected setting, all vertices exist in the same orbit, which means that no MPNN will be able to distinguish these two edges~\citep{srinivasan2019equivalence}. Looking further, both edges have the same structural features. As a result, $\operatorname{f_{\operatorname{sGNN}}}(v_0, v_1) = \operatorname{f_{\operatorname{sGNN}}}(v_0, v_3)$. In the directed setting, it is sufficient to show that $\mathbf{z}_{u, v}$ provides different representations for these two edges. Indeed, we observe that $\mathbf{z}_{0, 1} = [ \mathcal{N}^{in}_0, \mathcal{N}^{out}_0, \mathcal{N}^{in}_1, \mathcal{N}^{out}_1... ] = [2, 1, 1, 2, ...]$, while $\mathbf{z}_{0, 3} = [2, 1, 3, 1 ...]$, where we have neglected the intersection and union features for convenience. We conclude that these two representations are indeed different, which completes our proof.


\section{Principle Comparison in terms of Hits@20}\label{app:hits20}
Another popular performance metric used in link prediction is Hits@$k$ which measures the proportion of correct links (positive samples) ranked within the top $k$ positions of a sorted list and formulated as follows:
\begin{equation}\label{eq:hits@k}
    \text{Hits@$k$} = \frac{1}{|\mathcal{E}_{\operatorname{test}}|} \sum_{(u,v) \in \mathcal{E}_{\operatorname{test}}} \mathbb{I}\left(\text{rank}(u,v) \leq k\right), 
\end{equation}
where $\mathbb{I}$ is the indicator function that returns 1 if the condition inside is true and 0 otherwise. 
\begin{table}[htbp]
\caption{The Hits@20 for models with top MRRs in Table \ref{t:main_results}. The top three models are highlighted as \firstbest{First}, \secondbest{Second}, \thirdbest{Third}. \label{t:hr20_results}}
\begin{center}
\begin{tabular}{l c c c c c c}
& \textsc{Cora}  & \textsc{CiteSeer} & \textsc{Chameleon} & \textsc{Squirrel} & \textsc{Blog} & \textsc{WikiCS} \\
\hline
LP, sym	& 0.555${\scriptstyle\pm0.042}$ & \thirdbest{0.500${\scriptstyle\pm0.023}$}
&  $0.408{\scriptstyle\pm0.006}$ &  $0.168{\scriptstyle\pm0.008}$ &  $0.262{\scriptstyle\pm0.025}$ &  $0.484{\scriptstyle\pm0.045}$ \\
LP, asym	& 0.378${\scriptstyle\pm0.014}$ & 0.151${\scriptstyle\pm0.013}$
&  $0.562{\scriptstyle\pm0.033}$ &  $0.702{\scriptstyle\pm0.011}$ &  \thirdbest{0.369${\scriptstyle\pm0.027}$} &  $0.629{\scriptstyle\pm0.016}$ \\
RA, sym	& 0.591${\scriptstyle\pm0.011}$ & 0.328${\scriptstyle\pm0.016}$
&  $0.434{\scriptstyle\pm0.035}$ &  $0.175{\scriptstyle\pm0.003}$ &  $0.250{\scriptstyle\pm0.026}$ &  $0.571{\scriptstyle\pm0.032}$ \\
RA, asym	& 0.320${\scriptstyle\pm0.012}$ & 0.137${\scriptstyle\pm0.013}$
&  \secondbest{0.639${\scriptstyle\pm0.034}$} &  \firstbest{0.769${\scriptstyle\pm0.014}$} &  $0.336{\scriptstyle\pm0.039}$ &  \firstbest{0.723${\scriptstyle\pm0.015}$} \\
AA, sym	& 0.581${\scriptstyle\pm0.01}$ & 0.292${\scriptstyle\pm0.017}$
&  $0.432{\scriptstyle\pm0.006}$ &  $0.170{\scriptstyle\pm0.010}$ &  $0.266{\scriptstyle\pm0.018}$ &  $0.516{\scriptstyle\pm0.042}$ \\
AA, asym	& 0.315${\scriptstyle\pm0.013}$ & 0.125${\scriptstyle\pm0.012}$
&  \thirdbest{0.601${\scriptstyle\pm0.035}$} &  \secondbest{0.733${\scriptstyle\pm0.009}$} &  \secondbest{0.379${\scriptstyle\pm0.029}$} & \secondbest{0.676${\scriptstyle\pm0.013}$} \\
\hline
MLP	& 0.343${\scriptstyle\pm0.034}$ & \secondbest{0.592${\scriptstyle\pm0.034}$}
&  $0.318{\scriptstyle\pm0.030}$ &  $0.159{\scriptstyle\pm0.066}$ &  - &  $0.065{\scriptstyle\pm0.018}$ \\
GAT	& 0.176${\scriptstyle\pm0.058}$ & 0.227${\scriptstyle\pm0.055}$
&  $0.164{\scriptstyle\pm0.031}$ &  $0.030{\scriptstyle\pm0.022}$ &  $0.072{\scriptstyle\pm0.019}$ &  $0.035{\scriptstyle\pm0.016}$ \\
GCN	& \thirdbest{0.599${\scriptstyle\pm0.017}$} & 0.457${\scriptstyle\pm0.034}$
&  $0.301{\scriptstyle\pm0.044}$ &  $0.103{\scriptstyle\pm0.027}$ &  $0.116{\scriptstyle\pm0.033}$ &  $0.101{\scriptstyle\pm0.022}$ \\
GraphSage	& \secondbest{0.650${\scriptstyle\pm0.012}$} & 0.416${\scriptstyle\pm0.081}$
&  $0.223{\scriptstyle\pm0.041}$ &  $0.056{\scriptstyle\pm0.030}$ &  $0.120{\scriptstyle\pm0.020}$ &  $0.051{\scriptstyle\pm0.032}$ \\
\hline
DirLP	& \firstbest{0.767${\scriptstyle\pm0.057}$} & \firstbest{0.991${\scriptstyle\pm0.012}$}
&  \firstbest{0.727${\scriptstyle\pm0.032}$} &  \thirdbest{0.706${\scriptstyle\pm0.013}$} &  \firstbest{0.384${\scriptstyle\pm0.035}$} &  \thirdbest{0.635${\scriptstyle\pm0.053}$}
\end{tabular}

\end{center}
\end{table}

\newpage
\section{Hyperparameter Settings}\label{app:hyperparameter}
The hyperparameter search space included several key parameters. The number of hidden layers was selected from the set {1, 2, 4}, while the hidden layer dimension was chosen from {32, 64, 128}. Similarly, the final layer dimension varied among {24, 48, 72}. The number of attention heads was drawn from {2, 4, 8, 16}. Additionally, the dropout probability was sampled from a uniform distribution between 0 and 0.9, and the learning rate was selected from a uniform distribution ranging between 0.0001 and 0.0600. In Table \ref{t:best_settings} tuned hyperparameter values are reported for each setting. 

\begin{table}[!htp]\label{t:best_settings}
\caption{Best hyperparameter settings for all experiments in Table~\ref{t:main_results}.}
\centering
\begin{tabular}{l c c c c c c c}
\textbf{Model} & \textsc{Dataset} & \textsc{Cora} & \textsc{CiteSeer} & \textsc{Chameleon} & \textsc{Squirrel} & \textsc{Blog} & \textsc{WikiCS} \\
\hline
\multirow{6}{*}{\textbf{GAT}} 
 & \# of hidden layers         & $1$ & $1$ &  $2$ &  $1$ &  $1$ &  $1$ \\
 & hidden layer dim.            & $128$ & $128$ &  $32$ &  $32$ &  $32$ &  $32$ \\
 & final layer dim.          & $72$ & $48$ &  $72$ &  $24$ &  $24$ &  $24$ \\
 & \# of heads                 & $4$ & $2$ &  $8$ &  $2$ &  $8$ &  $8$ \\
 & dropout prob.                    & $0.040$ & $0.145$ &  $0.301$ &  $0.414$ &  $0.020$ &  $0.479$ \\
 & learning rate                   & $0.010$ & $0.005$ &  $0.043$ &  $0.048$ &  $0.059$ &  $0.024$ \\
\hline
\multirow{5}{*}{\textbf{GCN}} 
 & \# of hidden layers         & $1$ & $2$ &  $1$ &  $1$ &  $2$ &  $1$ \\
 & hidden layer dim.            & $64$ & $128$ &  $64$ &  $64$ &  $64$ &  $32$ \\
 & final layer dim.          & $72$ & $72$ &  $48$ &  $48$ &  $72$ &  $72$ \\
 & dropout prob.                    & $0.003$ & $0.007$ &  $0.331$ &  $0.274$ &  $0.144$ &  $0.090$ \\
 & learning rate                   & $0.031$ & $0.007$ &  $0.012$ &  $0.004$ &  $0.013$ &  $0.016$ \\
\hline
\multirow{5}{*}{\textbf{GraphSage}} 
 & \# of hidden layers         & $1$ & $1$ &  $1$ &  $1$ &  $1$ &  $1$ \\
 & hidden layer dim.            & $64$ & $64$ &  $32$ &  $32$ &  $32$ &  $32$ \\
 & final layer dim.          & $72$ & $48$ &  $72$ &  $72$ &  $72$ &  $72$ \\
 & dropout prob.                    & $0.181$ & $0.264$ &  $0.120$ &  $0.049$ &  $0.044$ &  $0.062$ \\
 & learning rate                   & $0.017$ & $0.025$ &  $0.005$ &  $0.020$ &  $0.032$ &  $0.055$ \\
\hline
\multirow{5}{*}{\textbf{DirLP}} 
 & \# of hidden layers & $1$ & $2$ &  $2$ &  $1$ & $2$ &  $2$ \\
 & hidden layer dim. & $64$ & $128$ &  $128$ &  $64$ & $128$ &  $128$ \\
 & final layer dim. & $72$ & $48$ &  $72$ &  $48$ &  $72$ &  $24$ \\
 & dropout prob. & $0.063$ & $0.027$ &  $0.154$ &  $0.034$ &  $0.020$ &  $0.130$ \\
 & learning rate & $0.045$ & $0.037$ &  $0.037$ &  $0.029$ &  $0.090$ &  $0.014$ \\
\end{tabular}
\end{table}

\end{document}
