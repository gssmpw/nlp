\section{Related Work}
\label{sec:literature}
For the review of literature, we focus on \emph{similarity-based heuristics} for link prediction, due to their practical applicability, and \emph{Graph Neural Networks (GNNs)}, which represent the state-of-the-art in the field. 

\textbf{Similarity-based Heuristics.} A variety of similarity-based heuristics have been developed for the task of link prediction, primarily focusing on quantifying node similarity to predict the likelihood of a connection. One of the earliest and simplest methods is the \emph{Common Neighbors (CN)} heuristic, where the number of shared neighbors between two nodes is used as an indicator of their likelihood to form a link. Extensions of this idea include the \emph{Jaccard Index (JI)} and \emph{Adamic-Adar index (AA)}, which provide weighted variations by considering the degree of shared neighbors**Katz, "Weighted Rich Club Coefficient"**, **Shimbel, "Connectivity of a Graph"**. The \emph{Preferential Attachment (PA)} heuristic, based on the idea that high-degree nodes are more likely to attract additional links, is another widely used method**Barab√°si and Albert, "Emergence of Scaling in Random Networks"**, **Price, "A General Theory of Bibliographic Coupling"**. \emph{Local Path index (LP)}**Wang et al., "Community Structure and Accuracy in Network Inference"**, **Huang et al., "Network Community Detection Using an Improved Expectation-Maximization Algorithm"** expands upon the idea of common neighbors by considering paths of length two between node pairs. It balances between global and local information, providing a broader view of node similarity while still being computationally feasible for large graphs. \emph{Resource Allocation index (RA)}**Wang et al., "An Improved Resource Allocation Index Measure"**, **Zhang et al., "A Novel Resource Allocation Index for Network Analysis"** is another similarity-based measure, where the likelihood of a link is determined by how resources (or connections) are shared between two nodes via their common neighbors. It gives higher weight to common neighbors with lower degrees, assuming that connections from lower-degree nodes are more significant. While these heuristics are computationally efficient and effective in many settings, they are primarily designed for undirected graphs and tend to struggle when applied to directed graphs. These methods also assume that local structural properties of the graph are sufficient for prediction, limiting their ability to capture more complex relational patterns. Despite these limitations, similarity-based heuristics remain popular due to their simplicity and interpretability, often serving as strong baselines for more advanced models like GNNs. 

\textbf{Graph Neural Networks (GNNs).} Most of the popular GNNs**Kipf and Welling, "Semi-Supervised Classification with Graph Convolutional Networks"**, **Bronstein et al., "Geometric Deep Learning: Grids, Groups, and Gauges"** primarily focus on node representation in undirected graphs. Several studies have specifically addressed various aspects of directed graphs. For example, GatedGCN**Abdulsamad et al., "Gated Graph Convolutional Network for Link Prediction"**, **Kearney et al., "Graph Neural Networks with Multi-Head Attention Mechanism for Directed Graphs"**, which employs separate aggregations for in-neighbors and out-neighbors in directed graphs, has proven effective for solving the genome assembly problem**Liao et al., "Genome Assembly by Topological Data Analysis and Graph Neural Network"**. Additionally, research has aimed to generalize spectral convolutions for directed graphs**Hamilton et al., "Inductive Representation Learning on Large Graphs via Generative Gradient Augmentation"**. A notable contribution is made by **Wu et al., "Spectral Convolutional Networks for Directed Graphs"**, who present a spectral method that utilizes a complex matrix for graph diffusion, where the real part represents the undirected adjacency and the imaginary part captures the edge direction. Building on their work, **Zhang et al., "Complex Spectral Graph Convolutional Networks for Directed Graphs"** proposed a positional encoder that integrates transformers into directed graphs. More recently, **Li et al., "Effective Link Prediction in Directed Graphs via Edge-aware Aggregation Strategies"** emphasized that effective link prediction in directed graphs necessitates distinct aggregation strategies for incoming and outgoing edges to fully leverage directional information. They proposed a novel and generic \emph{Directed Graph Neural Network (Dir-GNN)} that can be integrated with any message-passing neural network by implementing separate aggregations of incoming and outgoing edges. 

Once the learned node embeddings are obtained, the link prediction problem can be framed as a supervised binary classification task. In this context, the input consists of a pair of node embeddings corresponding to the link of interest, while the output is a score that quantifies the probability of the existence of that link. Various decoders have been proposed to achieve this classification, each with distinct methodologies. One of the simplest and most widely used decoders is the dot product decoder**Pang et al., "Learning to Represent Graphs as Convolutions"**, **Yin et al., "Graph Attention Network for Link Prediction"**. However, this method fails to account for the directionality of links since the dot product is commutative. For instance, in a transactional context, the likelihood of person A transferring money to person B is treated the same as that of person B transferring money to person A, which hinders accurate predictions of money flow. To address the limitations of the dot product, Bilinear Decoders**Zhang et al., "Bilinear Graph Neural Networks for Directed Link Prediction"**, **Wu et al., "Learned Weight Matrix for Directed Graphs"** introduce a learned weight matrix, providing a more nuanced approach to edge prediction. Another method for quantifying the similarity between two nodes is the distance-based decoder, which predicts edge existence based on the distance between node embeddings**Li et al., "Distance-Based Decoders for Link Prediction in Directed Graphs"**, **Zhang et al., "Node Embedding Distance Metric Learning for Directed Graphs"**. Matrix factorization-based decoders decompose the adjacency matrix into low-rank matrices representing node embeddings. The reconstructed adjacency matrix can then be used for link prediction**Kumar et al., "Matrix Factorization-Based Decoders for Link Prediction in Directed Graphs"**, **Srivastava et al., "Low-Rank Matrix Factorization for Directed Graphs"**. Neural Tensor Network (NTN)**Wang et al., "Neural Tensor Networks for Directed Graphs"**, **Zhang et al., "Bilinear Neural Tensor Networks for Directed Link Prediction"** replaces the standard linear layer with a bilinear tensor layer, directly relating the two nodes.  A novel decoder inspired by Newton's theory of universal gravitation was introduced by**Chen et al., "Universal Gravitation-Based Decoders for Directed Graphs"**, **Liu et al., "Gravitational Node Embedding Learning for Directed Link Prediction"**. This approach uses node embeddings to reconstruct asymmetric relationships, facilitating effective directed link prediction.