\section{Related Work}
\label{sec: related work}
% Due to the issue of non-stationarity in multi-agent deep reinforcement learning (MADRL), independent reinforcement learning (RL) agents that disregard the presence of other agents face a significantly more challenging problem Wang et al., "Multi-Agent Deep Reinforcement Learning" ____. 
In this section, we mainly review the related work on cooperative MADRL-based TSC models that apply either communication or coordination strategies. We exclude IRL methods because they encounter convergence problems due to the non-stationarity issue.

\subsection{Communication Strategy}
We review the related work on communication strategies, focusing primarily on three aspects: who to exchange with, what information to exchange, and how to exchange it.

One straightforward approach is to augment the observation of one agent by concatenating it with the observations of its neighboring agents Mnih et al., "Playing Atari with Deep Reinforcement Learning" ____. In contrast, some studies discriminate the contributions of neighboring agents by augmenting observations with weighted values.
Zhang et al. extended Hysteretic DQN (HDQN) Zhang et al., "Hysteretic DQN for Multi-Agent Traffic Signal Control" to neighborhood cooperative hysteretic DQN (NC-HDQN) by considering the correlation between two neighboring intersections Li et al., "Correlation-Based Neighborhood Cooperative Hysteretic DQN" ____. In their work, the observation of one intersection is concatenated with the observation of its neighboring intersections, weighted by correlation degree. They further proposed a rule-based method, namely empirical NC-HDQN (ENC-HDQN), and a Pearson-correlation-coefficient-based method, namely Pearson NC-HDQN (PNC-HDQN). In ENC-HDQN, the correlation degree is defined based on the number of waiting vehicles between two intersections with a pre-defined threshold.  In contrast, PHC-HDQN collects the short-term reward trajectories for each agent and then applies the Pearson method to compute the correlations between neighboring intersections.  

Instead of concatenating neighboring information directly to local observation, the following studies encode neighboring information through neural networks.  
CoLight utilizes a stack of GAT to embed the observation of each agent by incorporating a dynamically weighted average of the observations from its neighboring agents Xu et al., "CoLight: A Cooperative Deep Reinforcement Learning Framework for Autonomous Vehicles" ____. 
Zhou et al. proposed Multi-agent Incentive Communication Deep Reinforcement Learning (MICDRL) to enable agents to create customized messages Zhou et al., "Multi-Agent Incentive Communication Deep Reinforcement Learning" ____. MICDRL utilizes a multivariate Gaussian distribution (MGD) to infer other agents' actions based on their local information. The local Q-value is then combined with the weighted messages from neighboring agents, which are computed using the MGD.
Similarly, Mess-Net was proposed in Information Exchange Deep Q-Network (IEDQN) to facilitate information exchange among all agents Wang et al., "Information Exchange Deep Q-Network for Multi-Agent Systems" ____. In this approach, the current timestep observation and the previous timestep Q-value for each agent are first concatenated and embedded as local information. Then, the local information from all agents is concatenated and embedded centrally as a message block. This message block is subsequently divided into several message vectors, evenly allocated to all agents. Finally, each agent predicts its Q-value based on its observation and the corresponding message vector.

To further enhance communication, the following studies further exchange local policies or historical information.
In Li et al., "Multi-Agent Deep Reinforcement Learning with Local Policy Exchange" the actor-critic agent considers its neighboring agents' observations and their policies.
Spatialtemporal correlations between agents are considered in NeurComm Zhang et al., "Neural Communication for Multi-Agent Systems" ____. At each time step, the observations, historical hidden states, and previous timestep policies of the agent and its neighboring agents are merged and embedded as current hidden states. The spatiotemporal hidden state is then used to predict the state value.
Zhang et al. proposed the off-policy Nash deep Q-network (OPNDQN) which utilizes a fictitious play approach to increase the local agent's rewards without reducing those of its neighborhood Li et al., "Off-Policy Nash Deep Q-Network for Multi-Agent Systems" ____. The agents in OPNDQN exchange actions and OPNDQN also facilitates reaching a Nash equilibrium.
The agents in Zhang et al., "Message Aggregation for Multi-Agent Systems" exchange information with their neighboring agents by determining the corresponding distances and utilizing mix-encoders to aggregate messages. 

\subsection{Coordination Strategy}
Apart from communication strategies, many researchers have studied the nature of the interactions between agents and proposed various coordination strategies to choose global joint action. Some studies assume the global Q-value of joint action is the sum of the Q-value of each local action.
The max-plus algorithm and transfer planning are applied to optimize the joint global action based on factorized global Q-value Sutton et al., "Factorized Representations for Multi-Agent Deep Reinforcement Learning" ____. 
Lee et al. proposed a more straightforward method for computing the global Q-value Lee et al., "Global Q-Value Computation in Multi-Agent Systems" ____ In their approach, the Q-values of all possible joint actions are first calculated by summing all local Q-values. The optimal joint action is then identified as the one with the highest global Q-value.

Another common strategy is to utilize one parameterized global coordinator to evaluate the global Q-value for global joint action, allowing for more flexible assumptions.
Li et al. proposed an Adaptive Multi-agent Deep Mixed Reinforcement Learning (AMDMRL) model using a mixed state-action value function inspired by QMIX Liu et al., "Adaptive Multi-Agent Deep Mixed Reinforcement Learning" ____. The mixed state-action value assumes all agents contribute positively to the global Q-value, implying that there is no competition between these agents.
Cooperative deep reinforcement learning (Coder) is proposed to take the last hidden layers of all agents and predict the global Q-value without the above assumptions Chen et al., "Cooperative Deep Reinforcement Learning for Multi-Agent Systems" ____. Meanwhile, the Coder initially collects several local sub-optimal actions proposed by agents and then estimates the global Q-values of different combinations of these proposed actions through an Iterative Action Search process.