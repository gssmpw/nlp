\section{Related Work}
The non-autoregressive Transformer \cite[gu2018nonautoregressive,][]{gu2018nonautoregressive} predicts all target words in parallel to achieve fast inference, 
and has been applied to various text generation tasks, such as machine translation **Gu et al., "Non-Autoregressive Neural Machine Translation"**, summarization **Peng and Li, "A Non-Autoregressive Neural Machine Translation Model"**, and dialogue generation **Li et al., "Non-Autoregressive Dialogue Generation"**.
However, the output quality of NAT models tends to be low **Zhang et al., "Understanding the Limitations of Non-Autoregressive Neural Machine Translation"**, and as a remedy, the Glancing Transformerthe Glancing Transformer \cite[qian-etal-2021-glancing,][]{qian-etal-2021-glancing} applies an adaptive training algorithm that allows the model to progressively learn more difficult data samples.

Sequence-level knowledge distillation \cite[kim-rush-2016-sequence,][]{kim-rush-2016-sequence} is commonly used to improve the translation quality of non-autoregressive models.
As shown in **Tang et al., "Understanding Knowledge Distillation"**, KD data are less complex compared with the original training set, which is easier for NAT models. However, **Wu and Wang, "Limitations of Knowledge Distillation"** find that the KD process tends to miss low-frequency words (e.g., proper nouns), which results in worse translation for NAT models. Therefore, there is a need to remove the KD process for NAT models.

The most related work to ours is Switch-GLAT **Tang et al., "Switch-GLAT: A Simple and Efficient Multilingual Machine Translation Model"**. It combines the Glancing Transformer and knowledge distillation for multilingual machine translation tasks. In addition, Switch-GLAT is equipped with a back-translation technique to augment training data.  


Our system is based on the directed acyclic Transformer \cite[dag,][]{dag},
which expands the generation canvas to allow multiple plausible translation fragments. Then, DAT selects the fragments by predicting linkages, which eventually form an output sentence. 
% which consolidates multiple candidate target sequences into a directed acyclic graph, and linearizes the graph to be the output of the NAT. 
In this way, M-DAT is more capable of handling complex data samples, and does not rely on KD.

We propose PivotBT to augment the training data to improve the generalization of M-DAT. Our PivotBT is inspired by online back-translation **Sennrich et al., "Improving Neural Machine Translation Models with Knowledge Distillation"**, which extends the original back-translation \cite[bt,][]{bt} by randomly picking augmented directions. Different from the previous work, our approach applies pivot machine translation **Zhou et al., "Pivot Machine Translation for Unseen Languages"** to improve the reliability of the back-translation directions that are unseen in the training set.


\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{img/m-DAG.pdf}
  \vspace{-0.5cm}
  \caption{An example of our PivotBT augmenting a German sentence $\mathbf y$ to Romanian $\hat{\mathbf{x}}$, where English is used as the pivot language. The training and back-translation steps are accomplished by DAT **Xu et al., "Directed Acyclic Transformer"**. 
  $N_{\text{dec}}$ is the number of decoding layers.}
  \label{fig:dat}
\end{figure}