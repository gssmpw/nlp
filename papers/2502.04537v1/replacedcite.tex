\section{Related Work}
The non-autoregressive Transformer \cite[NAT,][]{gu2018nonautoregressive} predicts all target words in parallel to achieve fast inference, 
and has been applied to various text generation tasks, such as machine translation ____, summarization ____, and dialogue generation ____.
However, the output quality of NAT models tends to be low ____, and as a remedy, the Glancing Transformerthe Glancing Transformer \cite[GLAT,][]{qian-etal-2021-glancing} applies an adaptive training algorithm that allows the model to progressively learn more difficult data samples.

Sequence-level knowledge distillation \cite[KD,][]{kim-rush-2016-sequence} is commonly used to improve the translation quality of non-autoregressive models.
As shown in \newcite{understandingKD}, KD data are less complex compared with the original training set, which is easier for NAT models. However, \newcite{lfw} find that the KD process tends to miss low-frequency words (e.g., proper nouns), which results in worse translation for NAT models. Therefore, there is a need to remove the KD process for NAT models.

The most related work to ours is Switch-GLAT ____. It combines the Glancing Transformer and knowledge distillation for multilingual machine translation tasks. In addition, Switch-GLAT is equipped with a back-translation technique to augment training data.  


Our system is based on the directed acyclic Transformer \cite[DAT,][]{dag},
which expands the generation canvas to allow multiple plausible translation fragments. Then, DAT selects the fragments by predicting linkages, which eventually form an output sentence. 
% which consolidates multiple candidate target sequences into a directed acyclic graph, and linearizes the graph to be the output of the NAT. 
In this way, M-DAT is more capable of handling complex data samples, and does not rely on KD.

We propose PivotBT to augment the training data to improve the generalization of M-DAT. Our PivotBT is inspired by online back-translation ____, which extends the original back-translation \cite[BT,][]{bt} by randomly picking augmented directions. Different from the previous work, our approach applies pivot machine translation ____ to improve the reliability of the back-translation directions that are unseen in the training set.


\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{img/m-DAG.pdf}
  \vspace{-0.5cm}
  \caption{An example of our PivotBT augmenting a German sentence $\mathbf y$ to Romanian $\hat{\mathbf{x}}$, where English is used as the pivot language. The training and back-translation steps are accomplished by DAT ____. 
  $N_{\text{dec}}$ is the number of decoding layers.}
  \label{fig:dat}
\end{figure}