\section{Related Work}
The non-autoregressive Transformer \cite[NAT,][]{gu2018nonautoregressive} predicts all target words in parallel to achieve fast inference, 
and has been applied to various text generation tasks, such as machine translation \cite{gu-kong-2021-fully,dslp}, summarization \cite{su-etal-2021-non,liu2022learning,liu2022character}, and dialogue generation \cite{zou-etal-2021-thinking,qi2021bang}.
However, the output quality of NAT models tends to be low \cite{stern,ghazvininejad-etal-2019-mask}, and as a remedy, the Glancing Transformerthe Glancing Transformer \cite[GLAT,][]{qian-etal-2021-glancing} applies an adaptive training algorithm that allows the model to progressively learn more difficult data samples.

Sequence-level knowledge distillation \cite[KD,][]{kim-rush-2016-sequence} is commonly used to improve the translation quality of non-autoregressive models.
As shown in \newcite{understandingKD}, KD data are less complex compared with the original training set, which is easier for NAT models. However, \newcite{lfw} find that the KD process tends to miss low-frequency words (e.g., proper nouns), which results in worse translation for NAT models. Therefore, there is a need to remove the KD process for NAT models.

The most related work to ours is Switch-GLAT \cite{song2022switchglat}. It combines the Glancing Transformer and knowledge distillation for multilingual machine translation tasks. In addition, Switch-GLAT is equipped with a back-translation technique to augment training data.  


Our system is based on the directed acyclic Transformer \cite[DAT,][]{dag},
which expands the generation canvas to allow multiple plausible translation fragments. Then, DAT selects the fragments by predicting linkages, which eventually form an output sentence. 
% which consolidates multiple candidate target sequences into a directed acyclic graph, and linearizes the graph to be the output of the NAT. 
In this way, M-DAT is more capable of handling complex data samples, and does not rely on KD.

We propose PivotBT to augment the training data to improve the generalization of M-DAT. Our PivotBT is inspired by online back-translation \cite{zhang2020improving}, which extends the original back-translation \cite[BT,][]{bt} by randomly picking augmented directions. Different from the previous work, our approach applies pivot machine translation \cite{pivot} to improve the reliability of the back-translation directions that are unseen in the training set.


\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{img/m-DAG.pdf}
  \vspace{-0.5cm}
  \caption{An example of our PivotBT augmenting a German sentence $\mathbf y$ to Romanian $\hat{\mathbf{x}}$, where English is used as the pivot language. The training and back-translation steps are accomplished by DAT \cite{dag}. 
  $N_{\text{dec}}$ is the number of decoding layers.}
  \label{fig:dat}
\end{figure}