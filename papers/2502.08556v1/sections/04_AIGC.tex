Human-centric AIGC foundation models are generative models specifically trained or fine-tuned on \textit{large-scale human datasets} to create content focused on human elements, such as images, videos, and 3D avatars. By prioritizing the generation fidelity of human attributes, these models serve as \textit{adaptable bases for downstream tasks} like 
% image editing~\cite{li2024photomaker}, virtual try-on~\cite{kim2024stableviton}, 3D generation~\cite{huang2024humannorm}, and character animation~\cite{li2024synthesizing}. 
image editing, virtual try-on, 3D generation, and character animation. 
In this section, we categorize these models based on unsupervised learning and multi-modal supervised learning approaches.

\subsection{Unsupervised Learning}

Unsupervised learning plays a crucial role in early human-centric generative models, with GAN-based methods leading the progress in realistic human image and avatar generation. Specifically, there are two main frameworks: \textit{2D GANs with Style Modulation} and \textit{3D-Aware GANs with Neural Renderer}.
% These frameworks are increasingly serving as versatile bases for diverse downstream applications in human-centric AIGC.

% These models learn rich data distributions without explicit labels, enabling high-fidelity synthesis and controllable manipulation of human attributes.

% \vspace{1.5mm}
% \noindent{\textbf{Style-modulated GANs}}

\noindent{\textbf{GANs with Style Modulation}} leverage an intermediate, disentangled latent space to exert fine-grained control over the image synthesis process by first mapping a random noise to a style vector and then using learned affine transformations to inject style vectors at various layers of the generator, as shown in Fig.\ref{fig:aigc} (a).
This hierarchical modulation allows fine‐grained control of visual attributes at different scales.  For instance, StyleGAN-Human~\cite{fu2022stylegan} trained a series of unconditional models with this framework and showed that scaling data size, balancing data distribution, and aligning human body could significantly enhance generation performance, thereby establishing robust foundational models for conditional applications. UnitedHuman~\cite{fu2023unitedhuman} employed a multi-source spatial transformer to align multi-source data, including face, hand, partial-body, and whole-body images, into a unified space for more comprehensive human modeling, and designed a continuous generator to synthesize coherent high-relsolution images with enhanced details. These models served as versatile, pre‐trained foundations for a variety of downstream applications.
Their well-structured latent space enabled (1) intuitive drag manipulation of image attributes~\cite{pan2023drag} by optimizing latent codes based on point movement supervision,
% , facilitating tasks like interpolation, style mixing, and attribute editing.
% This framework and pre-trained models can also be applied to neural rendering~\cite{gu2021stylenerf} by integrating neural radiance field (NeRF), 
(2) 3D generation~\cite{xiong2023get3dhuman} with reconstruction priors to produce disentangled geometry and texture code, and (3) virtual try-on~\cite{yoshikawa2023stylehumanclip} by aligning the latent code with garment features.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{fig/aigc_framework.pdf}
    \vspace{-0.5em}
    \caption{Different frameworks of human-centric AIGC foundation models. Unsupervised learning methods: (a) A sampled noise is transformed by a mapping network (M) into a style code that modulates the generator (G) at multiple scales, while a discriminator (D) is trained simultaneously through adversarial learning to refine realism. (b) 3D representations are incorporated along with a neural renderer (R) to enhance spatial consistency. Supervised learning methods: (c) The input image is encoded (E) into a latent code, which the diffusion model (DM) iteratively denoises to reconstruct high-quality outputs with fine-grained control. (d) The input video is compressed and decomposed into space-time latents, where transformers capture spatial-temporal dependencies and enable efficient scaling for video generation.}
    % \caption{Different frameworks of human-centric AIGC foundation models. Unsupervised learning methods: (a) A sampled noise is transformed by a mapping network (M) into a style code that modulates the generator (G) at multiple scales, while a discriminator (D) is trained simultaneously through adversarial learning to refine realism~\cite{fu2023unitedhuman}. (b) 3D representations are incorporated along with a neural renderer (R) to enhance spatial consistency~\cite{wu2023aniportraitgan}. Supervised learning methods: (c) The input image is encoded (E) into a latent code, which the diffusion model (DM) iteratively denoises to reconstruct high-quality outputs with fine-grained control~\cite{liu2023hyperhuman}. (d) The input video is compressed and decomposed into space-time latents, where transformers capture spatial-temporal dependencies and enable efficient scaling for video generation~\cite{shao2024360}.}
    \label{fig:aigc}
     \vspace{-0.5em}
\end{figure*}

% \vspace{1.5mm}
% \noindent{\textbf{Rendering-guided GANs.}}

\noindent{\textbf{GANs with Neural Renderer}} integrate 3D representations and a neural renderer into the former framework to enforce 3D geometric consistency during image synthesis, (Fig.\ref{fig:aigc}(b)). By guiding the generator with rendering results (e.g., low-resolution image, depth, or surface normals), these models produce outputs that better reflect realistic spatial structures, making them especially to create high-fidelity 3D avatars. For example,
SofGAN~\cite{chen2022sofgan} decoupled the 3D representation space into geometry and texture subspaces, providing a robust foundation for independent control over camera pose, facial structure, and attribute texture in various human portraits. 
% EG3D~\cite{chan2022efficient} proposed an efficient tri-plane representation and a super-resolution module combined with a dual-discrimination loss to enforce visual consistency. 
AniPortraitGAN~\cite{wu2023aniportraitgan} integrated human priors into its framework by learning pose and facial deformations using the SMPL model and 3D Morphable Model (3DMM).
Trained on large-scale facial image collections, its generator and renderer could serve as the basis for tasks that require high-resolution controllable images with detailed 3D geometry.
Similarly, AG3D~\cite{dong2023ag3d} extended this framework to train on large-scale full-body images by incorporating an additional deformer for learning pose-dependent effects and a normal estimator for geometry supervision, resulting in high-quality 3D human avatars. These models' 3D-aware generation ability unlocked many downstream tasks, including (1) free-viewpoint video synthesis by rendering from arbitrary camera trajectories, (2) pose retargeting
% ~\cite{wu2023aniportraitgan} 
by integrating learnable pose-dependent deformations, and (3) single-view 3D reconstruction with GAN inversion.
% ~\cite{roich2022pivotal}.

% \vspace{1.5mm}
% \noindent{\textbf{Applications.}}
% GAN's well-structured latent space enables intuitive manipulation of image attributes~\cite{pan2023drag}, facilitating tasks like interpolation, style mixing, and attribute editing.
% Pre-trained GANs can also be applied to neural rendering~\cite{gu2021stylenerf}, 3D generation~\cite{xiong2023get3dhuman}, and virtual try-on~\cite{yoshikawa2023stylehumanclip} systems.
% Additionally, pre-trained 3D-GANs can be used for single-view 3D reconstruction by inversion~\cite{roich2022pivotal}.

% gDNA~\cite{chen2022gdna} 3D scans
% EVA3D~\cite{hong2022eva3d}
% AniPortraitGAN~\cite{wu2023aniportraitgan}


\subsection{Multi-modal Supervised Learning}

Recent advances in diffusion models have spurred the development of multi-modal supervised learning approaches that leverage large-scale paired datasets, such as text-image or video-pose alignments, to achieve precise control over the generation process.
With \textit{Conditional Latent Diffusion Models} and \textit{Spatial-Temporal Diffusion Transformer Architecture}, recent works have significantly improved the quality, consistency, and diversity of synthetic human images and videos.

% \vspace{1.5mm}
% \noindent{\textbf{Conditional Latent Diffusion.}}

\noindent{\textbf{Conditional Latent Diffusion Models}} extend standard diffusion models by operating in a compact latent space and incorporating external conditions, such as text, pose, or segmentation maps, to guide the generation process (Fig.\ref{fig:aigc} (c)). By first encoding inputs into a lower-dimensional latent representation, diffusion is applied to gradually denoise the output while preserving the provided conditions. This approach enhances both efficiency and controllability, making it a general framework for tasks like human image synthesis, pose-guided generation, and multi-modal content creation. Specifically,
% Text2Human~\cite{jiang2022text2human} introduced the DeepFashion-MultiModal dataset for controllable human image generation, where a human parsing map is first created from a given pose, followed by the synthesis of a full-body image using a hierarchical, texture-aware codebook and a diffusion-based transformer sampler.
HumanSD~\cite{ju2023humansd} proposed a skeleton-guided diffusion model with a heatmap-guided denoising loss. Trained on 2M+ text-image-pose triplets, this model demonstrated its foundational ability in generating high-quality human images across various scenarios.
HyperHuman~\cite{liu2023hyperhuman} presented a unified framework for generating hyper-realistic human images by capturing correlations between appearance and structure in multi-modal data. Specifically, it introduced a latent structural diffusion model that jointly denoises depth, surface normal, and RGB images conditioning on caption and pose skeleton, where modality-specific branches can be complementary to each other.
Recently, CosmicMan~\cite{li2024cosmicman} was proposed by building on three key pillars: a scalable, high-quality data production paradigm, robust model design via a decomposed attention-refocusing framework, and pragmatic integration into downstream tasks. This structure empowered CosmicMan to generate images in various scenarios, from full-body portraits to close-up shots, establishing it as a versatile cornerstone for human-centric content generation.
% introduced a scalable data production paradigm, through which a dataset with 6M human images and 115M attribute annotations was collected.


% \vspace{1.5mm}
% \noindent{\textbf{3D Generation.}}
% Rodin
% DreamFace~\cite{zhang2023dreamface}

% \vspace{1.5mm}
% \noindent{\textbf{Temporal Diffusion Transformer.}}

\noindent{\textbf{Spatial-Temporal Diffusion Transformers}} extend diffusion models by incorporating transformer-based architectures to capture both spatial structures and temporal dependencies in videos. By leveraging self-attention mechanisms across spatial dimensions and time steps, this framework (Fig.\ref{fig:aigc} (d)) effectively captures long-range temporal relationships while maintaining spatial consistency in human-centric video generation. Consequently, it is ideally suited for tasks such as character animation, video manipulation, and 4D generation. A representative work is Human4DiT~\cite{shao2024360}, which introduced a hierarchical 4D Diffusion Transformer (DiT) for generating high-quality, 360-degree spatial-temporally coherent human videos from a single image. Trained on a multi-sourced dataset spanning images, videos, multi-view captures, and 4D footages, the model factorized self-attention across views, time, and space while incorporating accurate condition injection, successfully handling complex motions and viewpoint changes.
Building on this framework, OmniHuman~\cite{lin2025omnihuman1} proposed a scalable, multi-modality-conditioned human video generation model from a single image and motion signals (e.g., audio, video, or both).
By leveraging mixed data and incorporating motion-related conditions during training, it mitigated data scarcity and enabled realistic video synthesis across diverse scenarios, including talking, singing, varying body compositions, and human-object interactions. Notably, pre-trained human-centric diffusion models are increasingly serving as the versatile foundation for a wide range of applications.
For example, they can be 
(1) fine-tuned for text-driven image editing~\cite{brooks2023instructpix2pix} using instruction-image pairs, 
(2) integrated into character animation~\cite{hu2024animate} by incorporating an additional branch to merge detailed textures into character's movements, 
(3) adapted for virtual try-on~\cite{karras2024fashion} by splitting classifier-free guidance into person, garment, and pose conditioning, and 
(4) extended to 3D/4D human generation~\cite{kolotouros2023dreamhuman} by iteratively applying score distillation sampling in multiple views.