\tikzstyle{leaf}=[draw=hiddendraw,
    rounded corners,minimum height=1em,
    fill=hidden-orange!40,text opacity=1, align=center,
    fill opacity=.5,  text=black,align=left,font=\scriptsize,
    inner xsep=3pt,
    inner ysep=2.5pt,
    ]
\begin{figure*}[ht]
\centering
\begin{forest}
  for tree={
  forked edges,
  grow=east,
  reversed=true,
  anchor=base west,
  parent anchor=east,
  child anchor=west,
  base=middle,
  font=\scriptsize,
  rectangle,
  draw=hiddendraw,
  rounded corners,align=left,
  minimum width=2em,
  % minimum height=1em,
    s sep=5pt,
    inner xsep=3pt,
    inner ysep=2.5pt,
  },
  where level=1{text width=4.5em}{},
  where level=2{text width=6em,font=\scriptsize}{},
  where level=3{font=\scriptsize}{},
  where level=4{font=\scriptsize}{},
  where level=5{font=\scriptsize}{},
  [Human-centric Foundation Models,rotate=90,anchor=north,edge=hiddendraw
    [Perception (\S\ref{sec:perception}),edge=hiddendraw,align=center,text width=5.7em
        [Self-supervised Learning, text width=9.2em, edge=hiddendraw
            [Contrastive Learning,leaf,text width=6.7em, edge=hiddendraw
                        [SOLIDER~\cite{chen2023beyond}{,} HCMoCo~\cite{hong2022versatile}{,}\\
                        PBoP~\cite{meng2024efficient}{,}
                        LiftedCL~\cite{chen2023liftedcl},
                        leaf,text width=21em, edge=hiddendraw]
                        ]
            [Mask Image Modeling,leaf,text width=6.7em, edge=hiddendraw
                        [HAP~\cite{yuan2024hap}{,} Sapiens~\cite{khirodkar2024sapiens},
                        leaf,text width=21em, edge=hiddendraw]
                        ]
        ]
        [Multi-task Supervised Learning, text width=9.2em, edge=hiddendraw
            [Multi-task Supervised Pretraining,leaf,text width=9.7em, edge=hiddendraw
                        [PATH~\cite{tang2023humanbench},leaf,text width=18em, edge=hiddendraw]
            ]
            [Unified Modeling,leaf,text width=9.7em, edge=hiddendraw
                        [UniHCP~\cite{ci2023unihcp}{,} HQNet~\cite{jin2024you}{,}\\
                        Hulk~\cite{wang2023hulk}{,}
                        RefHCM~\cite{huang2024refhcm},leaf,text width=18em, edge=hiddendraw]
                                    ]
        ]
    ]
    [AI-Generated \\Content (\S\ref{sec:aigc}),edge=hiddendraw,align=center,text width=5.7em
     [Unsupervised Learning, text width=6.8em, edge=hiddendraw
            [GANs with Style Modulation,leaf,text width=8.6em, edge=hiddendraw
            [StyleGAN-Human~\cite{fu2022stylegan}{,} UnitedHuman~\cite{fu2023unitedhuman},leaf,text width=21.5em, edge=hiddendraw]
            ]
            [GANs with Neural Renderer,leaf,text width=8.6em, edge=hiddendraw
            [AniPortraitGAN~\cite{wu2023aniportraitgan}{,} AG3D~\cite{dong2023ag3d},leaf,text width=21.5em, edge=hiddendraw]
            ]
      ]
    [Multi-modal Supervised \\Learning, text width=6.8em, edge=hiddendraw
            [Conditional Latent Diffusion Models,leaf,text width=11.8em, edge=hiddendraw
                [HumanSD~\cite{ju2023humansd}{,} HyperHuman~\cite{liu2023hyperhuman}{,} \\CosmicMan~\cite{li2024cosmicman} ,leaf,text width=18.3em, edge=hiddendraw]
                        ]
            [Spatial-Temporal Diffusion Transformers,leaf,text width=11.8em, edge=hiddendraw
                [Human4DiT~\cite{shao2024360}{,} OmniHuman~\cite{lin2025omnihuman1} ,leaf,text width=18.3em, edge=hiddendraw]
                        ]
        ]
    ]
    [Unified Perception \\
    \& Generation (\S\ref{sec:unified}), edge=hiddendraw,align=center,text width=5.7em
      [Fixed Vocabulary,text width=6.2em, edge=hiddendraw
            [CoMo~\cite{huang2024controllable}{,} ChatPose~\cite{feng2024chatpose}{,} ChatHuman~\cite{lin2024chathuman}{,} \\ChatGarment~\cite{bian2024chatgarment}{,} FaceGPT~\cite{wang2024facegpt},leaf,text width=32.5em, edge=hiddendraw]
      ]
      [Extended Vocabulary,text width=6.2em, edge=hiddendraw
            [MotionGPT~\cite{jiang2023motiongpt}{,} M3-GPT~\cite{luo2024m}{,} UniPose~\cite{li2024unipose}{,} MotionLLM\\\cite{wu2024motionllm}{,} MotionGPT-2~\cite{wang2024motiongpt}{,} AvatarGPT~\cite{zhou2024avatargpt}{,} LOM~\cite{chen2024language}, leaf,text width=32.5em, edge=hiddendraw]
      ]
    ]
    [Agentic Model (\S\ref{sec:agent}), edge=hiddendraw,align=center,text width=5.7em
      [Vision-Language-based Models,text width=11.3em, edge=hiddendraw
            [HumanVLA~\cite{xu2024humanvla}{,}
            SuperPADL~\cite{juravsky2024superpadl},leaf,text width=19.5em, edge=hiddendraw]
      ]
      [Vision-Language-Action-based Models,text width=11.3em, edge=hiddendraw
            [Roject GR00T~\cite{dong2024bringing},leaf,text width=19.5em, edge=hiddendraw]
      ]
    ]
    ]
\end{forest}
\caption{A taxonomy of human-centric foundation models with representative examples.}
\label{taxonomy}
\vspace{-0.5em}
\end{figure*}