%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{xcolor}
\usepackage{fontawesome}
\usepackage{amssymb}
\usepackage{mydef}
\usepackage{tikz}
\usepackage[edges]{forest}

% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newcommand{\dx}[1]{{\textcolor{green}{\textbf{DX: #1}}}}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Human-Centric Foundation Models: Perception, Generation \\ and Agentic Modeling}


% % Single author syntax
% \author{
%     Shixiang Tang, Yizhou Wang, Yuan Wang, Lu Chen, Dan Xu, Wanli Ouyang 
%     \affiliations
%     Chinese University of Hong, 
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Shixiang Tang$^1$
\and
Yizhou Wang$^1$\and
Lu Chen$^{2}$\and
Yuan Wang$^3$\and
Sida Peng$^2$\and
Dan Xu$^4$\and
Wanli Ouyang$^1$\\
\affiliations
$^1$The Chinese University of Hong Kong~~~
$^2$State Key Lab of CAD\&CG, Zhejiang University \\
$^3$Tsinghua University~~~
$^4$Hong Kong University of Science and Technology
% \emails
% \{shixiangtang, second\}@.com,
}
% \fi

\begin{document}

\maketitle

\begin{abstract}
% Human understanding and generation are critical for modeling digital humans and humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)—inspired by the success of generalist models such as large language and vision models—have emerged to unify diverse human-centric tasks into a single framework, surpassing traditional task-specific approaches. In this survey, we first present a comprehensive review of methods that integrate human perception, generation, and agentic modeling. First, we propose a taxonomy that categorizes existing approaches into four groups based on their target tasks: (1) Human-centric Perception Foundation Models that capture fine-grained multi-modal features for 2D and 3D human understanding; (2) Human-centric AIGC Foundation Models that generate high-fidelity, diverse human-related content; (3) Unified Perception and Generation Models that jointly enhance human understanding and synthesis; and (4) Human-centric Agentic Foundation Models that incorporate human-like intelligence and interactive behaviors for embodied tasks. Then, we systematically survey representative methods within each category. Finally, we discuss current limitations and outline promising future research directions, including ethical and legal considerations in deploying these models.


Human understanding and generation are critical for modeling digital humans and humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)—inspired by the success of generalist models such as large language and vision models—have emerged to unify diverse human-centric tasks into a single framework, surpassing traditional task-specific approaches. In this survey, we present a comprehensive overview of HcFMs by proposing a taxonomy that categorizes current approaches into four groups: (1) Human-centric Perception Foundation Models that capture fine-grained features for multi-modal 2D and 3D understanding; (2) Human-centric AIGC Foundation Models that generate high-fidelity, diverse human-related content; (3) Unified Perception and Generation Models that integrate these capabilities to enhance both human understanding and synthesis; and (4) Human-centric Agentic Foundation Models that extend beyond perception and generation to learn human-like intelligence and interactive behaviors for humanoid embodied tasks. We review state-of-the-art techniques, discuss emerging challenges and future research directions. This survey aims to serve as a roadmap for researchers and practitioners working towards more robust, versatile, and intelligent digital human and embodiments modeling. \textcolor{blue}{\href{https://github.com/HumanCentricModels/Awesome-Human-Centric-Foundation-Models/}{Website is available.}}
\end{abstract}

\input{fig/treemap}

\section{Introduction}
Recent years have witnessed remarkable strides towards understanding human appearance, emotions, identities, actions, intentions and generating photorealistic humans in 2D and 3D. The success of these methods can be attributed to the robust estimation of identification~\cite{he2024instruct,li2024all}, 2D keypoints~\cite{wang2023hulk,yuan2024hap}, fine-grained body-part segmentation~\cite{tang2023humanbench,chen2023beyond}, depth~\cite{khirodkar2024sapiens}, textual descriptions~\cite{chen2024language}, and human meshes~\cite{cai2024smpler}, as well as the powerful human-centric deep learning frameworks,  \emph{e.g.,} vision transformers~\cite{jin2024you,wang2023hulk,huang2024refhcm} and diffusion models~\cite{ju2023humansd,li2024cosmicman,lin2025omnihuman1}. Despite progress in every individual task, robust and accurate understanding and generating photorealistic and even intelligent digital humans requires to deeply understand human as a holistic and complex system at the intersection of diverse human-centric tasks related to appearance, identity, motion and intentions. Moreover, most existing human-centric pipelines are task-specific for better performances, leading to huge costs in representation/network design, pretraining, parameter-tuning, and annotations. Therefore, the recent human-centric learning community appeals for a unified framework~\cite{ci2023unihcp,wang2023hulk,chen2024language,huang2024refhcm} to unlock systematic understanding and a wide range of human-centric applications for everybody. 


Inspired by rapid advancements of general foundation models, \emph{e.g.,} large language models (LLMs), large vision models (LVMs) and text-to-image generative models, and their presents of a paradigm shift from end-to-end learning of task-specific models to generalist models, a recent trend is to develop \textbf{H}uman-\textbf{c}entric \textbf{F}oundation \textbf{M}odels \textbf{(HcFM)} that satisfy three criteria, namely generalization, broad applicability, and high fidelity. Generalization ensures robustness to unseen conditions, enabling the model to perform consistently across varied environments. Broad applicability indicates the versatility of the model, making it suitable for a wide range of tasks with minimal modifications, or even without modifications. High fidelity denotes the ability of the model to produce precise, high-resolution outputs essential for faithful human generation tasks such as 2D to 3D lifting. Recent notable works towards such human-centric foundation models include SOLIDER~\cite{chen2023beyond}, PATH~\cite{tang2023humanbench}, UniHCP~\cite{ci2023unihcp}, Sapines~\cite{khirodkar2024sapiens}, MotionGPT~\cite{jiang2023motiongpt,zhang2024motiongpt}, ChatHuman~\cite{lin2024chathuman}, \emph{etc}.


In light of the rapid developments and emerging challenges of Human-centric foundation models, we present a comprehensive survey of this field to help the community keep track of its progress. Specifically, we introduce a taxonomy that categorizes existing works into four groups according to their supported downstream tasks: \emph{Human-centric perception foundation models}, \emph{Human-centric AIGC foundation models}, \emph{Human-centric unified perception and generation foundation models}, and \emph{Human-centric agentic foundation models}. Human-centric perception foundation models modify the existing unsupervised and multitask supervised pretraining framework to capture the fine-grained human-centric features essential to multi-modal 2D and 3D perception tasks, such as skeleton-based action recognition, human parsing, \emph{etc}. Human-centric AIGC foundation models build on the success of generative foundation models by leveraging rich human-specific data, focusing on creating human-related content with high realism and diversity. To move a step forward, human-centric perception and generation tasks can be unitedly modeled in the single foundation model with the inspiration of multi-modal large language models. Such unified modeling are proven not only to benefit human understanding but also fine-grained and photorealistic generation. Lastly, as mentioned by Michael J Black and Xavier Puig~\cite{yao-feng-no-date}, the human foundation agent model goes beyond perception and generation, it is a model that can receive signals from human sensors, and interact with humans based on the inputs and, for which, the embodiment, actions, and motor behaviors are human-like. Different from other foundation models, the human-centric agent foundation models aim at learning human intelligence, which hopefully can benefit various embodied AI tasks.

To our best knowledge, this is the first survey about human-centric foundation models with a novel taxonomy (Fig.~\ref{taxonomy}). Its scope also extends beyond the mere categorization of existing techniques. It also explores the potential future trajectories, contemplating how ongoing advancements might unfold, including data, ethics and technological aspects.

% \section{Preliminaries}
% \subsection{Data for Human-centric Vision Tasks}
% \subsection{Human-centric Visual Data Representation}
% \subsection{Human-centric Vision Tasks}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{fig/perception_foundation_modals.pdf}
    \vspace{-0.75em}
    \caption{Different Frameworks of human-centric perception foundation models. Parameters in modules with \faLightbulbO ~are used in downstream tasks. Unsupervised foundation models: (a) Contrastive learning methods; (b) Mask image modeling methods. Supervised foundation models: (c) Multitask supervised pretraining methods; (d) Unified modeling methods.}
    % \caption{Different Frameworks of human-centric perception foundation models. Parameters in modules with \faLightbulbO ~are used in downstream tasks. Unsupervised foundation models: (a) Contrastive learning methods~\cite{chen2023beyond,hong2022versatile}; (b) Mask image modeling methods~\cite{yuan2024hap,armando2024cross}. Supervised foundation models: (c) Multitask supervised pretraining methods~\cite{tang2023humanbench}; (d) Unified modeling methods~\cite{ci2023unihcp,wang2023hulk}.}
    \label{fig:perception}
    \vspace{-0.5em}
\end{figure*}

\section{Taxonomy}
The objective of the taxonomy is to group Human-centric foundation models with the similar supported tasks into the same category. Specifically, we classify existing human-centric foundation models into four categories, \emph{i.e.,} Human-centric perception foundation models, Human-centric AIGC foundation models, Human-centric unified perception and generation foundation models, and Human-centric agentic models, which can be further summarized in difference learning frameworks. We briefly introduce the four categories as follows:

\noindent \textbf{(1) Human-centric perception foundation models} learn from large-scale and multi-modal human-centric data and support major perceptive tasks, including person re-identification, human parsing, pose estimation, human mesh recovery and skeleton-based action recognition. The fundamental idea to leverage structures of human bodies or diverse annotations to learn the fine-grained and semantic human-centric representations. Based on the learning frameworks, human-centric perception foundation models can be further categorized into unsupervised learning and multitask supervised learning.

\noindent \textbf{(2) Human-centric AIGC foundation models} are designed to create content—such as images, videos, or avatars—that centers on human. These models are trained on extensive human-centric data to produce realistic and diverse portrayals of individuals, whose objective is to produce content that accurately reflects fine-grained human appearances, behaviors, and interactions. Based on the training paradigm, these models can be broadly classified into models trained with unsupervised learning and multi-modal supervised learning.

\noindent \textbf{(3) Human-centric unified perception and generation foundation models} can support perception and generation tasks closely correlated. The fundamental idea is to view human-centric cues other than texts as foreign languages and append them to large language models (LLMs) as multi-modal large language models (MLLM) for understanding and generation. 

\noindent\textbf{(4) Human-centric agentic foundation models} learn human intelligence and support human-centric embodied AI tasks, such as human-robot collaboration tasks and social interactions. Based the learning frameworks, human-centric agentic foundation models can be categorized into vision-language-based method and vision-language-action-based methods.




% \begin{itemize}
%     \item \textbf{Human-centric perception foundation models} learn from large-scale and multi-modal human-centric data and can support major perceptive tasks, including person re-identification, human parsing, pose estimation, human mesh recovery and skeleton-based action recognition. The fundamental idea to leverage structures of human bodies or diverse annotations to learn the fine-grained and semantic human-centric representations. Based on the learning frameworks, human-centric perception foundation models can be further categorized into unsupervised learning and multitask supervised learning.
%     \item \textbf{Human-centric AIGC foundation models} are designed to create content—such as images, videos, or avatars—that centers on human. These models are trained on extensive datasets of human-centric data to produce realistic and diverse portrayals of individuals. The primary objective is to produce content that accurately reflects fine-grained human appearances, behaviors, and interactions. Based on the training paradigm, these models can be broadly classified into models trained with unsupervised learning and multi-modal supervised learning.
%     \item \textbf{Human-centric unified perception and generation foundation models} can support both perception and generation tasks that are closely correlated. The fundamental idea is to view human-centric cues other than texts as foreign languages and append them to large language models (LLMs) as multi-modal large language models (MLLM) for understanding and generation. 
%     \item \textbf{Human-centric agentic foundation models} learn human intelligence and support human-centric embodied AI tasks, such as human-robot collaboration tasks and social interactions. Based the learning frameworks, human-centric agentic foundation models can be categorized into vision-language-based method and vision-language-action-based methods.
% \end{itemize}

The four categories of foundation models are interconnected rather than mutually exclusive, contributing collectively to the rapid advancement of the human-centric foundation models. In the follows, we delve into each of them, exploring key challenges, representative solutions, and emerging trends.

% \subsection{Human-centric Foundation Models for Perception Tasks}
% Human-centric 



\section{Human-centric Perception Foundation Models} \label{sec:perception}
The human-centric perception foundation model demonstrates that compact ahuman-centric representations can be learned  from multiple human-centric task and efficiently adapted to a broad range of 2D and 3D multi-modal perception tasks. Based on whether to leverage human-centric annotations, we cover two paradigms of human-centric perception foundation models: unsupervised learning and supervised learning.



\subsection{Unsupervised learning methods}
% body structure prior 
% 为了避免受到标注数据有限的影响，ssl methods are proposed, which leverage prior of human body structure to learn versatile and representative features for human-centric tasks. Thanks to the consistency of 多模态人体一致性，hcmoco 。。。HAP 利用两个模态。PBoP 利用多模态信息生成pseudo label作为anchor来引导pretrain。 但是多模态数Weizhen He据不一定容易获取， SOLIDER 只采用单模态。考虑到xxx缺少3D prior，在lift通过lifter网络建立增加3D信息，显著提升3D性能。Croco-body通过cross-view pair和cross-pose，learn implicit 3D priors from human motion.
% 得益于的学习。 compared with imagenet pretraining, human-centric pretrianing methods 往往 achieves significant improvement on diverse downstream human-centric tasks, especality on low-data regime.
Human-centric unsupervised foundation models were proposed to mitigate dependency on the sensitive annotations, which mainly follow the pretraining-finetuning paradigm. During pretraining, without labels, they used the inherent priors of human body structure to learn versatile and representative human-centric features in the encoder. When adapted to downstream tasks, the pretrained encoder and the task head will be parameter-efficiently and fully finetuned using labeled data.

\noindent{\textbf{Contrastive learning methods}} leverage human priors to align features derived from encoders using 
tailored contrastive losses, as shown in Fig.~\ref{fig:perception}(a). 
Considering the multi-modal nature of human data (\emph{e.g.}, RGB, depth, 2D keypoints), instead of commonly used momentum encoders, multiple encoders were used to process inputs with different modalities. Human priors were included to guide multi-modal contrastive learning for general human-centric representations.
HCMoCo~\cite{hong2022versatile} employed multiple encoders to exploit multi-modal human body consistency through a hierarchical contrastive learning framework. Based on it, PBoP~\cite{meng2024efficient} introduced an additional encoder for generated latent part pair images. The extracted features could then serve as anchors to guide the multi-modal contrasting learning process. However, the acquisition of multi-modal data is not always straightforward. 
When only images were available, integrating an additional loss with human prior knowledge was another effective solution.
% When images were the only inputs, based on traditional contrastive learning, additional losses with human prior knowledge were integrated to learn human-centric features (see Fig.~\ref{fig:perception}(b)). 
For example, SOLIDER~\cite{chen2023beyond} proposed an additional semantic classification loss to import semantic information into the learned features. LiftedCL~\cite{chen2023liftedcl} introduced an adversarial loss to supervise the lifted 3D skeletons,
% lifting network to lift 2D representations into 3D skeletons with an adversarial loss, 
explicitly inserting 3D human structure information for human-centric pretraining.

\noindent\textbf{Mask image modeling methods} implicitly learn human knowledge by reconstructing masked inputs (see Fig.~\ref{fig:perception}(b)) based on the prior knowledge of body structures. HAP~\cite{yuan2024hap} utilized the 2D keypoints to guide the mask sampling process during mask image modeling, encouraging the model to concentrate on body structure information. To introduce 3D human prior, ~\cite{armando2024cross} proposed to reconstruct masked pedestrian images from cross-view and cross-pose pairs. 
% PBoP~\cite{}, in contrast, utilizes multi-modal information to generate pseudo-labels that serve as anchors to guide the multi-modal contrasting learning process. 
% SOLIDER~\cite{} focused on disentangling appearance and semantics solely on unlabeled pedestrian images in a self-supervised manner. Despite of the performance improvement brought by these methods, they neglected the significance of 3D
% prior on human-centric tasks. To tackle the issue, LiftedCL~\cite{} introduced a lifting network to lift 2D representations into 3D skeletons. By including additonal adversarial loss on generated 3D skeletons, LiftedCL explicitly inserted 3D human structure information for human-centric self-supervised learning. ~\cite{}, in contrast, proposed to implicitly learn 3D human prior by aligning cross-view and cross pose pairs. 
Thanks to human priors,  human-centric unsupervised foundation models show superior performance than the ImageNet pretraining methods on human-centric perception tasks, especially on low-data regimes.

\subsection{Multitask supervised learning}
% phase 1:  single task smpler-x, instucted-reid, mmdetectron.
% 当标注数据充足时，to solve a series of subtasks in a specific human-centric task, 
When labled data is sufficient, multitask supervised learning can exploit the internal relationship among data, emerging as a straightforward and effective paradigm for constructing human-centric perception foundation models. 
% The common and shared information among different human-centric tasks and datasets endows multitask supervised learning the capability to 
Some recent works learned the shared information among different human-centric datasets 
to benefit specific human-centric tasks,
\emph{e.g.}, human shape estimation~\cite{cai2024smpler}, pedestrian detection~\cite{zhang2024pedestrian}, re-identification~\cite{he2024instruct,li2024all}. 
% These approaches built up a general format for related subtasks and trained all subtasks at one time, achieving better performance than specialist models.
By constructing a unified framework applicable to related subtasks and training them concurrently, these approaches have outperformed specialist models.
However, a notable limitation of these methods is their inability to learn and execute other perception tasks, which restricts the potential scope of the application.
In response to the challenge, recent advances in multiple human-centric tasks co-training showed a new direction. These developments demonstrate that human-centric perception foundation models can 
% learn shared knowledge from diverse human-centric tasks, 
exploit inter-task homogeneity to enhance overall performance. 

\noindent\textbf{Multitask supervised pretraining methods} centered on utilizing multiple distinct supervisions to force the encoder to learn general human-centric representation (see Fig.~\ref{fig:perception}(c)). To handle task conflicts brought by supervised learning from diverse labels, 
PATH~\cite{tang2023humanbench} leveraged task-specific projectors along with the hierarchical weight-sharing strategy, enforcing the encoder to acquire general representations for downstream human-centric tasks. 

\noindent\textbf{Unified modeling methods} have emerged as the mainstream in human-centric perception foundation models to further mitigate the resource-intensive full finetuning process for downstream tasks. 
As depicted in Fig.~\ref{fig:perception}(d), these methods followed the unified encoder-decoder framework with dynamic queries.
UniHCP~\cite{ci2023unihcp}, as a first attempt, adopted a task-guided interpreter to unify task heads and task-specific queries, handling five human-centric tasks. HQNet~\cite{jin2024you} focused on instance-level features for individual persons and proposed Human Queries to learn unified all-in-one query representations, 
providing a single-stage method to tackle multiple distinctive human-centric tasks.
While these works mainly concentrate on 2D human-centric tasks, Hulk~\cite{wang2023hulk} extended the scope to simultaneously address 2D vision, 3D vision, vision-language and skeleton-based human-centric tasks. To tackle these tasks, Hulk categorized the input and output formats into four modalities and developed modality-specific (de-)tokenizers with modality indicator queries, unifying all tasks into modality translation tasks. 
Considering human-computer interaction, RefHCM~\cite{huang2024refhcm} converted multi-modal data into semantic tokens, unifying various perception tasks as referring tasks. 
% formulated them within a sequence-to-sequence paradigm, unifying distinct task losses into a single classification loss for simplicity and efficiency.
% PATH utilize ... to learn features, showing xxx. 进一步，为了减少在下游任务上应用时需要每个任务full-finetune的开销，unified humancentric model渐渐成为了主流

% phase 2: pretraining-finetuning humanbench, 

% phase 3: unified modeling to avoid full-finetuning for specific tasks unihcp

% \subsection{discussion}
% One ultimate goal of human-centric perception foundation models is to handle all human-centric tasks including those unseen tasks during training, mimicing the success of Large Language Models in natural language processing tasks. To achieve this goal, one possible solution is to scale up the training. Before that, there are two core remaining challenges : (1) Unification of different modalities: (2) (A new training paradigm to leverage both labeled and unlabled data?):

\section{Human-centric AIGC Foundation Models} \label{sec:aigc}
\input{sections/04_AIGC}



\section{Human-centric Unified Perception and Generation Foundation Models}  \label{sec:unified}
In recent years, human-centric foundation models emerge as a transformative approach for unifying perception and generation tasks, providing comprehensive frameworks to understand and synthesize human behaviors, motions, emotions, and intentions. By integrating multi-modal human-centric cues—such as visual, auditory, textual, emotional, and motion data—into LLMs and MLLMs, these models facilitate richer, context-aware representations of human interactions. Such models can be categorized into two primary paradigms—\textit{fixed vocabulary} and \textit{extended vocabulary}-based on how multi-modal human-centric signals are integrated into LLMs and MLLMs. 



% \textbf{Unified Perception Framework.} Understanding human behavior, such as fine-grained captioning and analysis, is crucial in the realm of human-centric AI. 

% \textbf{Unified Generation Framework.} Recent methods~\cite{zhang2024motiongpt,zhou2023ude,zhou2023unified,zhang2024large,gong2023tm2d} have shown success in developing foundation models to unify different human-centric generation tasks into a single framework. At the forefront of these advances, MotionGPT~\cite{zhang2024motiongpt} develops a groundbreaking human motion generation approach via fine-tuned LLMs. MotionGPT converts multi-modal control signals (\textit{e.g.}, textual descriptions and single-frame poses) into discrete representations, formulates them in a standardized instruction format, and leverages foundational LLMs to synthesize motion sequences based on encoded prompts. Marking the first attempt to transcend textual modality limitations, UDE~\cite{zhou2023ude} pioneers a unified engine for multi-modal motion synthesis by integrating text-driven and audio-driven generation within a single model. \cite{zhang2024large} amasses multiple cross-modal human motion benchmark MotionVerse, is implemented to align diverse formats of motion data into TOMATO representation~\cite{lu2023humantomato} as a unified intermediary format, consolidate datasets with different modalities, formats and tasks into a generalist model.

% \textbf{All-in-One Framework.} Despite their promising performance in separate human-centric perception or generation tasks, these approaches often fall short in comprehending motion. 

\subsection{Fixed Vocabulary}



Vocabulary-Fixed models enhance LLMs by introducing modality-specific projection layers to map human-centric signals into the feature space of LLMs or directly employing off-the-shelf tools and prompt engineering technique. Among these approaches, CoMo~\cite{huang2024controllable} unified the text conditioned human motion generation, fine-grained motion generation and motion editing. Specifically, it auto-regressively generates sequences of interpretable pose codes upon the high-level text description and fine-grained, body-part-specific descriptions generated by LLMs. ChatPose~\cite{feng2024chatpose} introduced LLMs to advance pose-related tasks, striving to develop a versatile pose generator. By integrating image interpretation, world knowledge, and body language comprehension into foundational LLMs, ChatPose enhanced its ability to understand and reason about 3D human poses from images and textual descriptions. Taking this a step further, ChatHuman~\cite{lin2024chathuman} introduced an multi-modal LLM integrated with 22 domain-specific human-centric tools, improving its ability to reason about human-related tasks. Assisted by academic publications and retrieval-augmented generation model, ChatHuman generated in-context-learning examples for handling newly introduced tools. In other human-centric tasks, foundational models have also made remarkable strides. For example, ChatGarment~\cite{bian2024chatgarment} leveraged large vision-language models (VLMs) to automate the estimation, synthesis, and editing of 3D garments from either images or textual descriptions. Similarly, FaceGPT~\cite{wang2024facegpt} integrated 3D morphable face model (3DMM)~\cite{blanz2023morphable} parameters into token space of VLMs, enabling the self-supervised generation of 3D facial models from both textual and visual inputs.

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{./fig/Unified_Perception_Generation_1.pdf}
    % \vspace{-0.7em}
    \caption{Human-centric foundation models for unified perception and generation tasks. (1) Vocabulary-Fixed models with fixed vocabulary introduces modality-specific projection layers or directly employing off-the-shelf human-centric tools. (2) Vocabulary-extended models align enriched human-centric representations, \textit{e.g.}, pose parameters, SMPL representations, motion sequences, emotions, and audio signals with the original vocabulary of foundational LLMs.}
    \label{fig:unfied}
\end{figure}

\subsection{Extended Vocabulary} 
Vocabulary-extended models are designed to expand capabilities of LLMs~\cite{touvron2023llama,achiam2023gpt} and MLLMs~\cite{liu2024visual,li2023blip} by explicitly extending their vocabulary and embedding spaces to accommodate human-centric and multi-modal signals. By aligning enriched human-centric representations—such as \textit{pose parameters}, \textit{SMPL representations}, or \textit{motion sequences}—with the original vocabulary of LLMs, Vocabulary-Extension Models empower foundational LLMs and MLLMs to effectively tackle novel human-centric tasks, including generating, editing, and comprehending human poses of textual descriptions, image, and 3D modalities.
Recent studies~\cite{wu2024motionllm,wang2024motiongpt,jiang2023motiongpt,luo2024m,zhou2024avatargpt} incorporated motion modality into textual space of the foundational LLMs, enabling a holistic representation of the complex relationship between motion and natural language. MotionGPT~\cite{jiang2023motiongpt} introduced a unified motion-language model to handle multiple motion-related tasks. It first discretized continuous motions into discrete semantic tokens, which could be interpreted as ``\textit{body language}'' and then be extended to the LLM's vocabulary. Through pre-training alignment and prompt tuning stages, MotionGPT showcased strong generalization across various human motion understanding and generation tasks. Building on the MotionGPT~\cite{jiang2023motiongpt}, AvatarGPT~\cite{zhou2024avatargpt} proposed an \textit{all-in-one} structure for motion understanding, planning, generation, as well as motion in-between
synthesis. M3-GPT~\cite{luo2024m} further embeded motion, music, and language into a single vocabulary and includes music-to-dance and dance-to-music tasks. Such a unified approach advanced human-centric tasks by synthesizing multi-modal human behaviors and bridging the gap between auditory, visual, and linguistic modalities. While these methods mostly were restricted to single-human motion, MotionLLM~\cite{wu2024motionllm} introduced a simple yet versatile framework capable of handling single-human, multi-human motion generation, and motion captioning by fine-tuning pre-trained LLMs.
MotionGPT-2~\cite{wang2024motiongpt} developed a general-purpose Large Motion-Language Model (LMLM), which extended beyond current solutions by tackling the challenging 3D holistic motion generation on the MotionX~\cite{lin2023motion} benchmark. However, these methods struggled with pose-related editing and comprehension. UniPose~\cite{li2024unipose} leveraged generation abilities of LLMs to unify all pose-relevant tasks to comprehend, generate, and edit human poses across various modalities, including images, text, and 3D SMPL representations. As a solution, LOM~\cite{chen2024language} unified verbal and non-verbal language using MLLMs for human motion understanding and generation, accommodating text, speech, motion, or any combination thereof as input flexibly. It trained a compositional body motion VQ-VAE to tokenize motions into part-ware discrete tokens, unifying modality-specific vocabularies (audio and text).



\section{Human-centric Agentic Foundation Models}  \label{sec:agent}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.01\linewidth]{./fig/agent.pdf}
    % \vspace{-1.5em}
    \caption{Frameworks of human-centric agentic foundation models to endow humanoid robots. (a) Vision-Language-based models: Use pretrained vision-language to learn multi-skill policy; (b) Vision-Language-Action-based models: Finetune a vision-language-action model to generate actions.}
    % \vspace{-1em}
    \label{fig:rfm_framework}
\end{figure}

Beyond understanding and generating human themselves, Human-centric agentic foundation models often process mulitmodal inputs (\emph{e.g.,} egocentric images, tactiles, sounds, and natural language as task description) focus on the human reaction and interactions to the environments while being constrained by its physical properties. Leveraging the Internet-scale multi-modal dataset, Human-centric agentic foundation models hold the promise of generalization across diverse tasks and provide a natural interface for human-robot interaction, both essential for real-world robot applications. 



\subsection{Vision-language-based models}
Vision-language-based Models are prominent approaches in constructing foundation models for humanoid systems by integrating pre-trained vision-language models (VLMs) with sensorimotor control policies. As illustrated in Fig.~\ref{fig:rfm_framework}(a), these methods leverage the strengths of visual and linguistic modalities to bridge high-level semantic understanding with low-level physical actions.
In these frameworks, visual inputs are processed using pre-trained image encoders while natural language commands are tokenized through pre-trained text encoders. The extracted semantic features are then aligned with humanoid-specific control policies, enabling the translation of language and visual cues into precise motor actions. This cross-modal alignment not only facilitates language-to-action mapping but also enhances the capability of humanoid robots to perform complex, multi-step tasks. Recent efforts have focused on adapting these vision-language-based models to the unique challenges of humanoid robotics. Notable examples include HumanVLA~\cite{xu2024humanvla} and SuperPADL~\cite{juravsky2024superpadl}, which aligned humanoid control policies with the latent spaces of pre-trained VLMs. These approaches have demonstrated the potential to endow humanoid robots with sophisticated skills that are driven by image and language inputs, paving the way for more advanced and natural human-robot interactions in dynamic environments.



\subsection{Vision-language-action-based models}
Vision-language-action (VLA) models have emerged as a promising approach in constructing foundation models for humanoid robots by unifying visual, linguistic, and action modalities within a single framework. As depicted in Fig.~\ref{fig:rfm_framework}(b), these methods treat robot data—both observations and actions—as tokens in the vocabulary of a pre-trained language model, thereby enabling direct fine-tuning or co-training with established vision-language models. In contrast to traditional methods that depend on separately trainable low-level control policies, VLA models directly generate actions as token sequences, effectively merging high-level semantic reasoning with motor command generation. This unified tokenization framework allows the model to generate a wide range of skills while preserving robust language and vision understanding, thus enhancing its generalization across diverse tasks. However, representing actions as stringified tokens may become inefficient for high degrees-of-freedom systems such as humanoid robots, where the complexity of motor commands is significantly higher. Despite the potential benefits, applying the VLA paradigm to humanoid robotics remains largely unexplored. To address this gap, recent initiatives like NVIDIA's Project GR00T~\cite{dong2024bringing} have been announced. The GR00T foundation model aspired to leverage a diverse array of data sources—from internet and simulation data to real-robot interactions—to facilitate scalable training and achieve robust, cross-modal performance for complex humanoid tasks. 

% \section{Related Work}

\section{Challenges and Future Directions}

\noindent \textbf{Data.} Different from general images and videos, collecting high-quality human-centric data is much more sensitive, difficult and expensive, compared to general images and videos. This situation inevitably leads to a trade-off in data quantity and data quality. Furthermore, the wide variability in human appearances, behaviors and contexts makes it hard to get a comprehensive dataset, 
% Humans come from diverse ethnicities and cultural backgrounds, each with unique behavioral patterns. These differences can lead to a vast number of variations that are difficult to capture comprehensively,
limiting the generalization ability of data-driven human-centric foundation models. 
% Long-tail problem is also essential in human-centric data distribution. 
% For example, common daily activities are easy to gather, but high-level professional actions (\emph{e.g.,} in surgeries) are difficult to collect. 
% The significant imbalance in data availability leads to model biases, where common scenarios become overrepresented while rare but critical cases remain underlearned.

\noindent \textbf{Representations.} Human appearance and behavior demand a holistic understanding that integrates body, face, and hands, yet no existing foundation model captures these aspects simultaneously. Recognizing that these elements are closely interrelated parts of a complex system, there remains a significant gap in unified modeling frameworks. Future research should focus on developing innovative, multi-modal architectures and scalable datasets that bridge the global and granular representations of human dynamics, ultimately enhancing applications in digital human synthesis, interactive robotics, and personalized human-computer interaction.

% Human appearance and behavior demand a holistic understanding that integrates body, face, and hands, yet current models fall short of simultaneously capturing these interconnected aspects. It is worth noting that representations of human motion and appearance extend beyond gross body movements and mesh recovery to include more granular details such as facial expressions and hand gestures. Viewing humans as complex systems where the dynamics of faces, bodies, and hands are intricately linked, there remains a significant gap in unified frameworks capable of modeling these relationships concurrently. Future research should therefore focus on developing comprehensive human-centric foundation models that bridge these scales of representation, leveraging multi-modal data and innovative architectures to capture the nuanced interplay between global and fine-grained features. Addressing this challenge will not only enhance our understanding of human behavior but also pave the way for more robust applications in areas such as digital human synthesis, interactive robotics, and personalized human-computer interaction.

\noindent \textbf{Interactivity.} Despite significant progress in understanding and generating isolated human attributes such as appearance, emotion, and identity, current human-centric foundation models face challenges in capturing the complex interplay of interactions and contextual dynamics inherent in real-world scenarios. Most approaches are limited to static or isolated representations, hindering their ability to model the nuanced interdependencies between individuals and their environments. Future research should focus on developing unified frameworks that seamlessly integrate high-level semantic reasoning with fine-grained behavioral synthesis, enabling models to adapt to dynamic, multi-agent contexts. 


\noindent \textbf{Ethics.} Ethics are crucial in applying human-centric foundation models, especially regarding sensitive domains and privacy. 
In sensitive domains, the model's output should never replace the expertise of human professionals.
Moreover, Human-centric foundation models should also be developed with privacy-enhancing technologies, ensuring that the model learns powerful representations from the data while making it difficult to identify individual-level privacy information. 
Additionally, anonymization methods should be applied to all training data to avoid privacy violations.


\clearpage
%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

