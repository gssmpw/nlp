\section{Related work}
\label{sec:related work}

In 2019, \citet{devlin2019bertpretrainingdeepbidirectional} introduced BERT, a novel approach to embedding text using bi-directional Transformers pre-trained without supervision on large corpora. Shortly after, \citet{liu2019robertarobustlyoptimizedbert} improved over BERT's pre-training by removing the next-sentence prediction objective and drastically increasing the amount of data, leading to RoBERTa. Since then, the primary focus of the community has shifted towards optimizing the fine-tuning phase of these models through contrastive learning, where the model is trained to maximize the similarity between positive text pairs while pushing them apart from negative samples.

Among the earliest contrastive learning approaches designed for encoders, SimCSE~\citep{gao2022simcsesimplecontrastivelearning} demonstrated that sentence pairs could be easily generated by feeding the same input to the model twice and applying dropout to introduce noise. However, this simple approach was soon outperformed by models like GTE~\citep{li_towards_2023}, which introduced more advanced contrastive learning techniques. GTE employed a weakly supervised stage that takes advantage of the vast number of successive sentence pairs available in traditional unlabeled datasets, followed by a semi-supervised stage incorporating labeled sentence pairs from high-quality datasets such as NLI~\citep{bowman2015largeannotatedcorpuslearning} and FEVER~\citep{thorne2018feverlargescaledatasetfact}. Recently, fine-grained strategies have emerged to better adapt models to both task and context. For instance, Jina-embeddings~\citep{sturua2024jinaembeddingsv3multilingualembeddingstask} introduced task-specific Low-Rank Adaptation (LoRA) adapters. As of January 2025, CDE~\citep{morris2024contextualdocumentembeddings} ranks at the top of the MTEB leaderboard for models under 250M parameters thanks to two key innovations: grouping samples with related contexts into the same batch and providing contextual embeddings for the entire corpus in response to individual queries.

However, pre-training has not seen the same level of effort, and thus progress, most likely due to its prohibitively high computational cost. RoBERTa, for instance, required a total of $1,024$ V100 days for its pre-training. As a result, GTE, Jina-embeddings, and CDE all rely on pre-trained BERT, XLM-RoBERTa~\citep{conneau2020unsupervisedcrosslingualrepresentationlearning}, and NomicBERT~\citep{nussbaum2024nomicembedtrainingreproducible} to initialize their respective models. The latter, NomicBERT, represents a recent effort to refine BERT's architecture and pre-training. NomicBERT incorporates architectural improvements such as SwiGLU and RoPE, utilizes FlashAttention, and extends the context length to $2,048$ tokens. Despite these innovations, NomicBERT still relied on sub-optimal choices, as discussed in \autoref{sec:neobert}. In parallel with the development of NeoBERT, \cite{warner2024smarterbetterfasterlonger} released ModernBERT with the goal of further refining the pre-training of NomicBERT. Although we share some of the modifications, we make distinct design choices and conduct thorough ablations that ultimately lead to greater performance on MTEB.