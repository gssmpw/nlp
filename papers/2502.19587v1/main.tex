\documentclass[10pt]{article} % For LaTeX2e
% \usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs} 
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{makecell}

\definecolor{ForestGreen}{HTML}{228B22}
\definecolor{BrickRed}{HTML}{8F1402}

\title{NeoBERT: A Next-Generation BERT}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{
Lola Le Breton$^{1,2,3}$ \quad Quentin Fournier$^{2}$ \quad Mariam El Mezouar$^{4}$ \quad Sarath Chandar$^{1,2,3,5}$ \\~\\
\textnormal{$^1$Chandar Research Lab \quad
$^2$Mila – Quebec AI Institute \quad $^3$Polytechnique Montréal \quad 
\\ $^4$Royal Military College of Canada \quad
$^5$Canada CIFAR AI Chair \\}
}





% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version



\begin{document}


\maketitle

\begin{abstract}
	Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT$_{large}$, RoBERTa$_{large}$, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption\footnote{\url{https://huggingface.co/chandar-lab/NeoBERT}}\textsuperscript{,}\footnote{\url{https://github.com/chandar-lab/NeoBERT}}.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Auto-regressive language models have made tremendous progress since the introduction of GPT~\citep{radford2018improving}, and modern large language models (LLMs) such as LLaMA 3~\citep{dubey2024llama3herdmodels}, Mistral~\citep{jiang2023mistral7b}, OLMo~\citep{groeneveld2024olmoacceleratingsciencelanguage}, and DeepSeek-r1~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability} now exhibit remarkable reasoning and in-context learning capabilities. These improvements result from scaling both the models and the web-scraped text datasets they are trained on, as well as from innovations in architecture and optimization techniques. However, while decoders have continuously evolved, encoders have not seen the same level of progress. Worse, their knowledge has become increasingly outdated despite remaining critical for a wide range of downstream NLP tasks that depend on their embeddings, notably for retrieval-augmented generation~\citep{ram2023incontextretrievalaugmentedlanguagemodels} and toxicity classification~\citep{hartvigsen-etal-2022-toxigen}. Despite being five years old, BERT~\citep{devlin2019bertpretrainingdeepbidirectional} and RoBERTa~\citep{liu2019robertarobustlyoptimizedbert} remain widely used, with more than 110 million combined downloads from Hugging Face as of the writing of this paper.

Similar to decoders, which undergo multi-stage processes of pre-training, instruction-tuning, and alignment, encoders also require successive training phases to perform well on downstream tasks. First, encoders go through self-supervised pre-training on large corpora of text with the masked language modeling objective. By predicting masked or replaced tokens, this stage enables models to learn the structural patterns of language and the semantics of words. However, the pre-training task is disconnected from downstream applications, and models require additional training to achieve strong performance in clustering or retrieval. Thus, a second fine-tuning phase is often achieved through multiple stages of semi-supervised contrastive learning, where models learn to differentiate between positive and negative sentence pairs, refining their embeddings in the latent space.

Recently, substantial progress has been made in improving the fine-tuning stage of pre-trained encoders, with models like GTE~\citep{li_towards_2023}, jina-embeddings~\citep{sturua2024jinaembeddingsv3multilingualembeddingstask}, SFR-embeddings~\citep{SFR-embedding-2}, and CDE~\citep{morris2024contextualdocumentembeddings} significantly outperforming previous encoders on the MTEB leaderboard, a recent and challenging benchmark spanning 7 tasks and 56 datasets. However, all these approaches focus on proposing complex fine-tuning methods and do not address the inherent limitations of their pre-trained backbone models.

As a result, there is a dire need for a new generation of BERT-like pre-trained models that incorporate up-to-date knowledge and leverage both architectural and training innovations, forming stronger backbones for these more advanced fine-tuning procedures. 

In response, we introduce NeoBERT, a next-generation BERT that integrates the latest advancements in architecture, data, and pre-training strategies. The improvements are rigorously validated on GLUE by fully pre-training 10 different models that successively incorporate the modifications. This validation ensures that the improvements benefit encoder architectures and highlights how some design choices drastically affect the model's abilities. Additionally, we design and experimentally validate a two-stage training procedure to increase NeoBERT's maximum context window from $1,024$ to $4,096$. To ensure a fair evaluation of NeoBERT against existing baselines and to isolate the impact of fine-tuning procedures, we propose a model-agnostic and systematic fine-tuning strategy with straightforward contrastive learning. All models are fine-tuned using this standardized approach and subsequently evaluated on the MTEB benchmark. 

On MTEB, NeoBERT consistently outperforms all competing pre-trained models while being 100M parameters smaller than the typical \textit{large}-sized encoders. With a context window of 4,096 tokens, it processes sequences 8× longer than RoBERTa \citep{liu2019robertarobustlyoptimizedbert} and two times longer than NomicBERT \citep{nussbaum2024nomicembedtrainingreproducible}. It is also the fastest encoder of its kind, significantly outperforming ModernBERT \textit{base} and \textit{large} in terms of inference speed. Despite its compact 250M parameter size, NeoBERT is trained for over 2T tokens, prioritizing training over scale to maximize accessibility for both academic researchers and industry users without requiring large-scale compute resources. This makes NeoBERT the most extensively trained model among modern encoders, ensuring robust generalization and superior downstream performance. Furthermore, NeoBERT maintains the same hidden size as \textit{base} models, allowing for seamless plug-and-play adoption without modifications to existing architectures. As the only fully open-source model of its kind, we release the code, data, training scripts, and model checkpoints, reinforcing our commitment to reproducible research.

\section{Related work}
\label{sec:related work}

In 2019, \citet{devlin2019bertpretrainingdeepbidirectional} introduced BERT, a novel approach to embedding text using bi-directional Transformers pre-trained without supervision on large corpora. Shortly after, \citet{liu2019robertarobustlyoptimizedbert} improved over BERT's pre-training by removing the next-sentence prediction objective and drastically increasing the amount of data, leading to RoBERTa. Since then, the primary focus of the community has shifted towards optimizing the fine-tuning phase of these models through contrastive learning, where the model is trained to maximize the similarity between positive text pairs while pushing them apart from negative samples.

Among the earliest contrastive learning approaches designed for encoders, SimCSE~\citep{gao2022simcsesimplecontrastivelearning} demonstrated that sentence pairs could be easily generated by feeding the same input to the model twice and applying dropout to introduce noise. However, this simple approach was soon outperformed by models like GTE~\citep{li_towards_2023}, which introduced more advanced contrastive learning techniques. GTE employed a weakly supervised stage that takes advantage of the vast number of successive sentence pairs available in traditional unlabeled datasets, followed by a semi-supervised stage incorporating labeled sentence pairs from high-quality datasets such as NLI~\citep{bowman2015largeannotatedcorpuslearning} and FEVER~\citep{thorne2018feverlargescaledatasetfact}. Recently, fine-grained strategies have emerged to better adapt models to both task and context. For instance, Jina-embeddings~\citep{sturua2024jinaembeddingsv3multilingualembeddingstask} introduced task-specific Low-Rank Adaptation (LoRA) adapters. As of January 2025, CDE~\citep{morris2024contextualdocumentembeddings} ranks at the top of the MTEB leaderboard for models under 250M parameters thanks to two key innovations: grouping samples with related contexts into the same batch and providing contextual embeddings for the entire corpus in response to individual queries.

However, pre-training has not seen the same level of effort, and thus progress, most likely due to its prohibitively high computational cost. RoBERTa, for instance, required a total of $1,024$ V100 days for its pre-training. As a result, GTE, Jina-embeddings, and CDE all rely on pre-trained BERT, XLM-RoBERTa~\citep{conneau2020unsupervisedcrosslingualrepresentationlearning}, and NomicBERT~\citep{nussbaum2024nomicembedtrainingreproducible} to initialize their respective models. The latter, NomicBERT, represents a recent effort to refine BERT's architecture and pre-training. NomicBERT incorporates architectural improvements such as SwiGLU and RoPE, utilizes FlashAttention, and extends the context length to $2,048$ tokens. Despite these innovations, NomicBERT still relied on sub-optimal choices, as discussed in \autoref{sec:neobert}. In parallel with the development of NeoBERT, \cite{warner2024smarterbetterfasterlonger} released ModernBERT with the goal of further refining the pre-training of NomicBERT. Although we share some of the modifications, we make distinct design choices and conduct thorough ablations that ultimately lead to greater performance on MTEB.

\section{NeoBERT}
\label{sec:neobert}

The following section describes NeoBERT's improvements over BERT and RoBERTa, as well as the recent NomicBERT and ModernBERT models. Since GTE and CDE use BERT and NomicBERT as their pre-trained backbone, they inherit their respective characteristics. \autoref{tab:hyperparam_data} summarizes the modifications.


\begin{table*}[!ht]
	\centering
	\small
	\setlength{\tabcolsep}{6pt}
	\renewcommand{\arraystretch}{1.2}
	\caption{Comparison of Model Architectures, Training Data, and Pre-Training Configurations.}
	\label{tab:hyperparam_data}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{lcccccccc}
			\toprule
			& \multicolumn{2}{c}{\textbf{BERT}}& \multicolumn{2}{c}{\textbf{RoBERTa}}& \textbf{NomicBERT} & \multicolumn{2}{c}{\textbf{ModernBERT}}& \textbf{NeoBERT} \\ 
			                         & \textit{base} & \textit{large} & \textit{base} & \textit{large} & \textit{base} & \textit{base} & \textit{large} & \textit{medium} \\ 
			\midrule
			  
			\textbf{Layers}          & 12            & 24             & 12            & 24             & 12            & 22            & 28             & 28              \\
			\textbf{Hidden Size}     & 768           & $1,024$        & 768           & $1,024$        & 768           & 768           & $1,024$        & 768             \\
			\textbf{Attention Heads} & 12            & 16             & 12            & 16             & 12            & 12            & 16             & 12              \\
			\textbf{Parameters}      & 120M          & 350M           & 125M          & 355M           & 137M          & 149M          & 395M           & 250M            \\
			\textbf{Activation Function} & \multicolumn{4}{c}{GeLU}& SwiGLU & \multicolumn{2}{c}{GeGLU}& SwiGLU \\
			\textbf{Positional Encoding} & \multicolumn{4}{c}{Positional Embeddings}& RoPE & \multicolumn{2}{c}{RoPE}& RoPE \\
			\textbf{Normalization} & \multicolumn{4}{c}{Post-LayerNorm}& Post-LayerNorm & \multicolumn{2}{c}{Pre-LayerNorm}& Pre-RMSNorm \\
			
			\midrule
			 
			\textbf{Data Sources} & \multicolumn{2}{c}{\begin{tabular}{@{}c@{}}BooksCorpus \\ Wikipedia\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}{@{}c@{}}BooksCorpus  \\ OpenWebText \\ Stories / CC-News\end{tabular}} & \begin{tabular}{@{}c@{}}BooksCorpus \\ Wikipedia\end{tabular} & \multicolumn{2}{c}{Undisclosed}& RefinedWeb \\
			\textbf{Dataset Size} & \multicolumn{2}{c}{13GB} & \multicolumn{2}{c}{160GB}  & 13GB & \multicolumn{2}{c}{-}& 2.8TB \\
			\textbf{Dataset Year} & \multicolumn{2}{c}{2019} & \multicolumn{2}{c}{2019}  & 2023 & \multicolumn{2}{c}{-}& 2023 \\
			\textbf{Tokenizer Level} & \multicolumn{2}{c}{Character} & \multicolumn{2}{c}{Byte}  & Character & \multicolumn{2}{c}{Character}& Character \\
			\textbf{Vocabulary Size} & \multicolumn{2}{c}{30K} & \multicolumn{2}{c}{50K}  & 30K & \multicolumn{2}{c}{50K}& 30K \\
			
			\midrule
			 
			\textbf{Sequence Length} & \multicolumn{2}{c}{512} & \multicolumn{2}{c}{512}  & $2,048$ & \multicolumn{2}{c}{$1,024$ $\rightarrow$ $8,192$}& $1,024$ $\rightarrow$ $4,096$ \\
			\textbf{Objective} & \multicolumn{2}{c}{MLM + NSP} & \multicolumn{2}{c}{MLM}  & MLM & \multicolumn{2}{c}{MLM}& MLM \\
			\textbf{Masking Rate} & \multicolumn{2}{c}{15\%} & \multicolumn{2}{c}{15\%}  & 30\% & \multicolumn{2}{c}{30\%}& 20\% \\
			\textbf{Masking Scheme} & \multicolumn{2}{c}{80/10/10} & \multicolumn{2}{c}{80/10/10}  & - & \multicolumn{2}{c}{-}& 100 \\
			\textbf{Optimizer} & \multicolumn{2}{c}{Adam} & \multicolumn{2}{c}{Adam}  & AdamW & \multicolumn{2}{c}{StableAdamW}& AdamW \\
			\textbf{Scheduler} & \multicolumn{2}{c}{-} & \multicolumn{2}{c}{-}  & - & \multicolumn{2}{c}{WSD}& CosineDecay \\
			\textbf{Batch Size} & \multicolumn{2}{c}{131k tokens} & \multicolumn{2}{c}{131k}  & 8M & \multicolumn{2}{c}{448k to 5M}& 2M \\
			\textbf{Tokens Seen} & \multicolumn{2}{c}{131B} & \multicolumn{2}{c}{131B}  & - & \multicolumn{2}{c}{$\sim$ 2T}& 2.1T \\
			\textbf{Training} & \multicolumn{2}{c}{DDP} & \multicolumn{2}{c}{DDP}  & \begin{tabular}{@{}c@{}}DeepSpeed \\ FlashAttention\end{tabular} & \multicolumn{2}{c}{\begin{tabular}{@{}c@{}}Alternate Attention \\ Unpadding \\ FlashAttention\end{tabular}}& \begin{tabular}{@{}c@{}}DeepSpeed \\ FlashAttention\end{tabular}\\
			\bottomrule
		\end{tabular}
	}
\end{table*}


\subsection{Architecture}
\label{sec:architecture}

The Transformer architecture has been refined over the years and has now largely stabilized, with models like LLaMA 3 being essentially the same as the original LLaMA. NeoBERT integrates the latest modifications that have, for the most part, become standard.

\paragraph{Depth-to-Width} The concept of depth efficiency has long been recognized in neural network architectures. In the case of Transformers, stacking self-attention layers is so effective that it can quickly saturate the network's capacity. Recognizing this, \citet{levine_limits_2020} provided theoretical and empirical evidence for an optimal depth-to-width ratio in Transformers. Their findings suggested that most language models were operating in a ``depth-inefficiency'' regime, where allocating more parameters to width rather than depth would have improved performance. In contrast, small language models like BERT, RoBERTa, and NomicBERT are instead in a width-inefficiency regime. To maximize NeoBERT's parameter efficiency while ensuring it remains a seamless plug-and-play replacement, we retain the original BERT$_{base}$ width of 768 and instead increase its depth to achieve this optimal ratio.

\paragraph{Positional Information} Transformers inherently lack the ability to distinguish token positions. Early models like BERT and RoBERTa addressed this by adding absolute positional embeddings to the token embeddings before the first Transformer block. However, this approach struggles to generalize to longer sequences and requires the positional information to be propagated across layers. To overcome these limitations, \citet{su2023roformerenhancedtransformerrotary} proposed Rotary Position Embeddings (RoPE), which integrate relative positional information directly into the self-attention mechanism. RoPE has quickly become the default in modern Transformers due to its significant improvements in performance and extrapolation capabilities. NeoBERT, like all newer encoders, integrates RoPE. Nevertheless, degradation still occurs with sequences significantly longer than those seen during training. As a solution, \citet{peng2023yarnefficientcontextwindow} introduced Yet Another RoPE Extension (YaRN), which allows to efficiently fine-tune models on longer contexts. NeoBERT is readily compatible with YaRN, making it well-suited for tasks requiring extended context.

\paragraph{Layer Normalization} Consistent with standard practices in modern Transformer architectures, we move the normalization layer inside the residual connections of each feed-forward and attention block, a technique known as Pre-Layer Normalization (Pre-LN). Pre-LN improves stability, allows for larger learning rates, and accelerates model convergence~\citep{xiong2020layernormalizationtransformerarchitecture}. While all newer encoder models adopt Pre-LN, they typically continue to use the classical LayerNorm rather than Root Mean Square Layer Normalization (RMSNorm). In NeoBERT, we substitute the classical LayerNorm with RMSNorm~\citep{zhang2019rootmeansquarelayer}, which achieves comparable training stability while being slightly less computationally intensive, as it requires one fewer statistic.

\paragraph{Activations} BERT and RoBERTa utilize the standard Gaussian Error Linear Unit (GELU) activation function. However, \citet{shazeer2020gluvariantsimprovetransformer} demonstrated the benefits of the Gated Linear Unit in Transformer architectures. These activation functions have since been adopted in several language models, including the LLaMA family. Following previous works, NeoBERT incorporates the SwiGLU activation function, and because it introduces a third weight matrix, we scale the number of hidden units by a factor of $\frac{2}{3}$ to keep the number of parameters constant.

\subsection{Data}
\label{sec:data}

Data has emerged as one of the most critical aspects of pre-training, and datasets with increasing quantity and diversity are frequently released. NeoBERT takes advantage of the latest datasets that have proven to be effective.

\paragraph{Dataset} BERT and NomicBERT were pre-trained on two carefully curated and high-quality datasets: Wikipedia and BookCorpus~\citep{zhu2015aligningbooksmoviesstorylike}. As \citet{baevski-etal-2019-cloze} demonstrated that increasing data size can improve downstream performance, \citet{liu2019robertarobustlyoptimizedbert} pre-trained RoBERTa on 10 times more data from BookCorpus, CC-News, OpenWebText, and Stories. However, RoBERTa's pre-training corpus has become small in comparison to modern web-scraped datasets built by filtering and deduplicating Common Crawl dumps. Following the same trend, we pre-trained NeoBERT on RefinedWeb~\citep{penedo2023refinedwebdatasetfalconllm}, a massive dataset containing 600B tokens, nearly 18 times larger than RoBERTa's. Although RefinedWeb does not have strict high-quality constraints, we believe that exposing the model to such a large and diverse dataset will improve its real-world utility.

\paragraph{Sequence Length} BERT and RoBERTa were pre-trained on sequences up to 512 tokens, which limits their downstream utility, especially without RoPE and YaRN. NomicBERT increased the maximum length to $2,048$ and employed Dynamic NTK interpolation at inference to scale to 8192. To further broaden NeoBERT's utility, we seek to increase the context length. However, due to the computational cost associated with pre-training, we adopt a two-stage pre-training procedure similar to LLMs like LLaMA 3. In the first stage, we train the model for 1M steps (2T tokens) using sequences truncated to a maximum length of $1,024$ tokens, referring to this version as NeoBERT$_{1024}$. In the second stage, we extend the training for an additional 50k steps (100B tokens), increasing the maximum sequence length to $4,096$ tokens. We refer to this final model as NeoBERT$_{4096}$. To ensure the model encounters longer sequences during this stage, we create two additional sub-datasets, Refinedweb$_{1024+}$ and Refinedweb$_{2048+}$, containing only sequence lengths greater than $1,024$ and $2,048$ tokens, respectively, alongside the original Refinedweb dataset. Each batch is sampled from Refinedweb, Refinedweb$_{1024+}$ and Refinedweb$_{2048+}$ with probabilities 20\%, 40\%, and 40\%. Since longer sequences tend to represent more complex or academic content, this strategy helps mitigate the distribution shift typically observed when filtering for longer sequences. We explore the benefits of this two-stage training strategy in \autoref{sec:seq_length}.


\subsection{Pre-Training}
\label{sec:training}

Encoder pre-training has received less attention than the data and architecture. However, many improvements made to decoders also apply to encoders. NeoBERT combines encoder-specific modifications with widely accepted decoder improvements.

\paragraph{Objective} In light of RoBERTa's findings that dropping the next-sentence prediction task does not harm performance, both NomicBERT and NeoBERT were only pre-trained on masked language modeling. Moreover, \citet{wettig_should_2023} challenged the assumption that the 15\% masking rate of  BERT and RoBERTa is universally optimal. Instead, their findings suggest that the optimal masking rate is actually 20\% for base models and 40\% for large models. Intuitively, a model learns best when the difficulty of its training tasks aligns with its capabilities. Based on their insight, we increase the masking rate to 20\%, while NomicBERT exceeds it by opting for 30\%.

\paragraph{Optimization} Following standard practice, we use the AdamW optimizer~\citep{loshchilov2019decoupledweightdecayregularization} with the same hyperparameters as LLaMA 2: $\beta_1 = 0.9$, $\beta_2 = 0.95$, and $\epsilon = 10^{-8}$. In preliminary experiments, we also considered SOAP \citep{vyas2025soapimprovingstabilizingshampoo}, a recent extension of the Shampoo optimizer, but it failed to outperform Adam and AdamW and has been omitted from the list of ablations. We employ a linear warmup for $2,000$ steps to reach a peak learning rate of $6 \times 10^{-4}$, followed by a cosine decay to 10\% of the peak learning rate over 90\% of the training steps. Once fully decayed, the learning rate remains constant for the last 100k steps at a sequence length of $1,024$ and 50k steps at a sequence length of $4,096$. We use a weight decay of 0.1 and apply gradient clipping with a maximum norm of 1.0.

\paragraph{Scale} Recent generative models like the LLaMA family~\citep{touvron2023llama2openfoundation, dubey2024llama3herdmodels} have demonstrated that language models benefit from being trained on significantly more tokens than was previously standard. Recently, LLaMA-3.2 1B was successfully trained on up to 9T tokens without showing signs of saturation. Moreover, encoders are less sample-efficient than decoders since they only make predictions for masked tokens. Therefore, it is reasonable to believe that encoders of similar sizes can be trained on an equal or even greater number of tokens without saturating. For NeoBERT's pre-training, we use batch sizes of 2M tokens over 1M steps in the first stage and 50k steps in the second, resulting in a theoretical total of 2.1T tokens. Note that because sequences are padded to the maximum length, this represents a theoretical number of tokens. In terms of tokens, this is comparable to RoBERTa and represents a 2x increase over NomicBERT. In terms of training steps, this amounts to a 2x increase over RoBERTa and a 10x increase over NomicBERT.

\paragraph{Efficiency} We improve efficiency by parallelizing the model across devices using DeepSpeed~\citep{aminabadi2022deepspeed-inference} with the ZeRO~\citep{rajbhandari2020zeromemoryoptimizationstraining} optimizer, reducing memory usage by eliminating data duplication across GPUs and increasing the batch size. We further optimize the GPU utilization by employing fused operators from the \texttt{xFormers} library to reduce overhead, selecting all dimensions to be multiples of 64 to align with GPU architectures, and removing biases to simplify computations without sacrificing performance. To address the quadratic demands of attention, we integrate FlashAttention~\citep{dao2023flashattention2fasterattentionbetter}, which computes exact attention without storing the full matrices.

\section{Effect of Design Choices}
\label{sec:ablations}

% add low batch size here
We conduct a series of ablations in controlled settings to evaluate our improvements to the original BERT architecture. We fully train each model for 1M steps, controlling for the seed and dataloader states to ensure successive models are trained with identical setups. These resource-intensive ablations were crucial to confirm our design choices, as they are based on the literature of pre-training decoder models. The baseline model, referred to as $M0$, is similar to BERT$_{base}$ but includes pre-layer normalization. Following RoBERTa, $M0$ also drops the next sentence prediction objective. We introduce modifications iteratively, resulting in a total of ten different models, as detailed in \autoref{tab:ablations}. To mitigate computational costs, the ablations are evaluated on the GLUE benchmark with a limited hyperparameter grid search of batch sizes $\in \{16, 32\}$ and learning rates $\in \{1e-5, 2e-5, 3e-5\}$. For the final model $M10$, we extend the grid search, as detailed in \autoref{app:glue}. Results are in \autoref{fig:glue-ablations}.

\begin{table}
	\caption{Modifications between successive ablations. The initial $M0$ baseline corresponds to a model similar to BERT, while $M9$ corresponds to NeoBERT.}
	\label{tab:ablations}
	\begin{center}
		\begin{tabular}{llll}
			\toprule
			\multicolumn{2}{c}{\textbf{Modification}} & \textbf{Before} & \textbf{After} \\\midrule
			\multirow{3}{*}{$M1$} & Embedding        & Positional          & RoPE       \\
			                      & Activation       & GELU                & SwiGLU     \\
			                      & Pre-LN           & LayerNorm           & RMSNorm    \\\midrule
			$M2$                  & Dataset          & Wiki + Book         & RefinedWeb \\\midrule
			$M3$                  & Tokenizer        & Google WordPiece    & LLaMA BPE  \\\midrule
			\multirow{2}{*}{$M4$} & Optimizer        & Adam                & AdamW      \\ 
			                      & Scheduler        & Linear              & Cosine     \\\midrule
			$M5$                  & Masking Scheme   & 15\% (80 / 10 / 10) & 20\% (100) \\\midrule
			$M6$                  & Sequence packing & False               & True       \\\midrule
			$M7$                  & Model Size       & 120M                & 250M       \\\midrule
			$M8$                  & Depth - Width    & 16 - 1056           & 28 - 768   \\\midrule
			\multirow{2}{*}{$M9$} & Batch size       & 131k                & 2M         \\
			                      & Context length   & 512                 & $4,096$    \\
			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\begin{figure}[!ht]
	\centering
	\caption{GLUE ablation scores on the development set. Modifications in grey are not included in the subsequent models. Increasing data size and diversity leads to the highest relative improvement ($M2$, \textcolor{ForestGreen}{$+3.6\%$}), followed by the model size ($M7$, \textcolor{ForestGreen}{$+2.9\%$}). Packing the sequences and using the LLaMA 2 tokenizer cause the largest relative drops ($M6$, \textcolor{BrickRed}{$-2.9\%$}, $M3$, \textcolor{BrickRed}{$-2.1\%$}).}
	\label{fig:glue-ablations}
	\includegraphics[width=0.9\linewidth]{figures/glue_ablations.pdf}
\end{figure}

\paragraph{Key Performance-Enhancing Modifications} As expected, the two modifications that had the greatest impact on the average GLUE score were related to scale. In $M2$, replacing Wikitext and BookCorpus with the significantly larger and more diverse RefinedWeb dataset improved the score by \textcolor{ForestGreen}{$+3.6\%$}, while increasing the model size from 120M to 250M in $M7$ led to a \textcolor{ForestGreen}{$+2.9\%$} relative improvement. Note that to assess the impact of the depth-to-width ratio, we first scale the number of parameters in $M7$ to 250M while maintaining a similar ratio to BERT$_{base}$, resulting in 16 layers of dimension 1056. In $M8$, the ratio is then adjusted to 28 layers of dimension 768.

\paragraph{Modifications That Were Discarded} In $M3$, replacing the Google WordPiece tokenizer with LLaMA BPE results in a \textcolor{BrickRed}{$-2.1\%$} performance decrease. We hypothesize that while the heterogeneous and multilingual nature of the LLaMA 2 vocabulary enhances broader applicability in decoders, it trades off performance for more compact encoder representations. In $M6$, we un-pad the sequences by concatenating samples of the same batch. While this removes unnecessary computation on padding tokens, packing sequences without accounting for cross-sequence attention results in a relative performance drop of \textcolor{BrickRed}{$-2.8\%$}. Although this modification was discarded from our subsequent ablations, we incorporate methods of un-padding with accurate cross-attention in our released codebase, following \cite{kundu2024enhancingtrainingefficiencyusing}.

\paragraph{Modifications Retained Despite Performance Trade-offs} Unexpectedly, using AdamW \citep{loshchilov2019decoupledweightdecayregularization} and cosine decay \citep{loshchilov2017sgdrstochasticgradientdescent} in $M4$ decreases performance by \textcolor{BrickRed}{$-0.5\%$}. As AdamW introduces additional regularization with weight decay, we expect that it will become beneficial when extending training by mitigating overfitting. Similarly, increasing the masking ratio from 15\% to 20\% in $M5$ decreases performance by \textcolor{BrickRed}{$-0.7\%$}. We hypothesize that increasing the task difficulty initially hinders downstream task performance but is likely to become advantageous when training larger models on more tokens. Consequently, we retain both modifications despite being unable to verify these hypotheses at scale due to the computational costs.


\section{Experiments}
\label{sec:experiments}

Selecting appropriate metrics and benchmarks is crucial for properly assessing the downstream performance and utility of language models. Following both early and recent studies, we include the GLUE and MTEB benchmarks in our evaluations.

\subsection{GLUE}
\label{sec:glue}

The GLUE benchmark \citep{wang2019gluemultitaskbenchmarkanalysis} is a cornerstone of language modeling evaluations and has played a significant role in the field. Although GLUE is now 6 years old and the community has long recognized its limitations, we report the GLUE score due to its widespread adoption and to facilitate the comparison of NeoBERT with existing encoders. Following standard practices, we fine-tune NeoBERT on the development set of GLUE with a classical hyperparameter search and introduce transfer learning between related tasks. Complete details of the fine-tuning and best hyperparameters are presented in \autoref{app:glue}. NeoBERT achieves a score of $89.0\%$ comparable to previous \textit{large} models while being $100M$ to $150M$ parameters smaller. We present the results in \autoref{tab:glue_dev}.

\begin{table*}[!ht]
	\centering
	\caption{GLUE scores on the development set. Baseline scores were retrieved as follows: BERT from Table 1 of \cite{devlin2019bertpretrainingdeepbidirectional}, RoBERTa from Table 8 of \cite{liu2019robertarobustlyoptimizedbert}, DeBERTa from Table 3 of \cite{he_debertav3_2023}, NomicBERT from Table 2 of \cite{nussbaum2024nomicembedtrainingreproducible}, GTE from Table 13 of \cite{zhang-etal-2024-mgte}, and ModernBERT from Table 5 of \cite{warner2024smarterbetterfasterlonger}.}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{clcccccccc|c}
			\toprule
			\textbf{Size}                    & \textbf{Model}     & \textbf{MNLI}    & \textbf{QNLI}    & \textbf{QQP}     & \textbf{RTE}     & \textbf{SST}     & \textbf{MRPC}    & \textbf{CoLA}    & \textbf{STS}     & \textbf{Avg.}    \\
			\midrule
			\multirow{4}{*}{\makecell{Base \\ \small{($\leq 150M$)}}}&
			BERT     & 84.0& 90.5& 71.2& 66.4& 93.5& 88.9& 52.1& 85.8&79.6\\
			                                 & RoBERTa            & 87.6             & 92.8             & 91.9             & 78.7             & 94.8             & 90.2             & 63.6             & 91.2             & 86.4             \\
			                                 & GTE-en-8192        & 86.7             & 91.9             & 88.8             & 84.8             & 93.3             & 92.1             & 57.0             & 90.2             & 85.6             \\
			                                 & NomicBERT$_{2048}$ & 86.0             & 92.0             & 92.0             & 82.0             & 93.0             & 88.0             & 50.0             & 90.0             & 84.0             \\
			                                 & ModernBERT         & \underline{89.1} & \underline{93.9} & \underline{92.1} & \underline{87.4} & \underline{96.0} & \underline{92.2} & \underline{65.1} & \underline{91.8} & \underline{88.5} \\
			\midrule
			\multirow{2}{*}{\makecell{Medium \\ \small{$250M$}}} & NeoBERT$_{1024}$          & 88.9& \underline{93.9} & 90.7& 91.0& \underline{95.8}& 93.4& 64.8& \underline{92.1}&88.8\\
			                                 & NeoBERT$_{4096}$   & \underline{89.0} & 93.7             & \underline{90.7} & \underline{91.3} & 95.6             & \underline{93.4} & \underline{66.2} & 91.8             & \underline{89.0} \\
			\midrule
			\multirow{5}{*}{\makecell{Large \\ \small{($\geq 340M$)}}} &   BERT    & 86.3& 92.7& 72.1& 70.1& 94.9& 89.3& 60.5& 86.5&82.1\\
			                                 & RoBERTa            & 90.2             & 94.7             & 92.2             & 86.6             & 96.4             & 90.9             & 68.0             & 92.4             & 88.9             \\
			                                 & DeBERTaV3          & \textbf{91.9}    & \textbf{96.0}    & \textbf{93.0}    & \textbf{92.7}    & 96.9             & 91.9             & \textbf{75.3}    & \textbf{93.0}    & \textbf{91.4}    \\
			                                 & GTE-en-8192        & 89.2             & 93.9             & 89.2             & 88.1             & 95.1             & \textbf{93.5}    & 60.4             & 91.4             & 87.6             \\
			                                 & ModernBERT         & 90.8             & 95.2             & 92.7             & 92.1             & \textbf{97.1}    & 91.7             & 71.4             & 92.8             & 90.5             \\
			\bottomrule
		\end{tabular}
	}
	\label{tab:glue_dev}
\end{table*}

\subsection{MTEB}
\label{sec:mteb}

Beyond the GLUE benchmark, we consider the more recent and challenging MTEB benchmark~\citep{muennighoff_mteb_2023}, which has quickly become a standard for evaluating embedding models, with a wide coverage of 7 tasks and 56 datasets in its English subset.

MTEB tasks rely on the cosine similarity of embeddings pooled across tokens in a sentence. The most common and straightforward pooling strategy is computing the average of each token's final hidden representation. However, because out-of-the-box encoders are trained with the masked language modeling objective, they provide no guarantee that mean embeddings will produce meaningful representations without further fine-tuning. As a result, models require expensive fine-tuning strategies crafted for MTEB to achieve strong scores. For instance, GTE~\citep{li_towards_2023} with multi-stage contrastive learning, Jina-embeddings~\citep{sturua2024jinaembeddingsv3multilingualembeddingstask} with task-specific Low-Rank Adaptation (LoRA) adapters, and CDE~\citep{morris2024contextualdocumentembeddings}, with batch clustering and contextual corpus embeddings all pushed the limits of the leaderboard in their respective categories.

These coupled stages make it challenging to disentangle the respective impacts of pre-training and fine-tuning on the final model’s performance. To isolate and fairly evaluate the improvements introduced during pre-training, we implemented an affordable, model-agnostic fine-tuning strategy based on classical contrastive learning. This fine-tuning approach was designed in accordance with established methods in the literature. Its controlled settings ensured that all models were fine-tuned and evaluated under identical conditions.


\paragraph{Method} Given a dataset of positive pairs $\mathbb{D}=\{q_i, d_i^+\}_{i = 1}^{n}$, a similarity metric $s$, a temperature parameter $\tau$, and a set of negative documents $N_q$ for each query $q$, the contrastive loss is defined as:
\begin{equation*}
	\mathcal{L}=-\log \frac{e^{s(q, d^+) / \tau}}{e^{s(q, d^+) / \tau} + \sum_{d^- \in N_q} e^{s(q, d^-) / \tau}}
\end{equation*}

Negative documents can be either generic samples of the same format or tailored hard negatives, which exhibit a high degree of similarity to the contrasted sample in their original representation. We constructed a dataset of positive query-document pairs with optional hard negatives based on open-source datasets previously employed in the literature \citep{li2023generaltextembeddingsmultistage} for a total of nine million documents. In addition to the optional hard negatives, we also leverage in-batch, task-homogeneous negatives. In line with prior research ~\citep{li2023generaltextembeddingsmultistage}, we employ task-specific instructions and temperature-scaled sampling of the datasets. Complete details about the data, training, and evaluation can be found in \autoref{app:contrastive}.

\paragraph{Results} We found that training for more than 2,000 steps resulted in minimal performance gains. \autoref{tab:mteb_full} presents the complete MTEB-English evaluation of all fine-tuned models. Although NeoBERT is $100M$ parameters smaller than all \textit{large} baselines, it is the best model overall with a \textcolor{ForestGreen}{$+4.5\%$} relative increase over the second best model, demonstrating the benefits of its architecture, data, and pre-training improvements.

\begin{table*}[!ht]
	\centering
	\caption{MTEB scores on the English subset after 2,000 steps of fine-tuning with contrastive learning.}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{clccccccc|c}
			\toprule
			\textbf{Size}                     & \textbf{Model}     & \textbf{Class.} & \textbf{Clust.} & \textbf{PairClass.} & \textbf{Rerank.} & \textbf{Retriev.} & \textbf{STS}  & \textbf{Summ.} & \textbf{Avg.} \\
			\midrule
			\multirow{4}{*}{\makecell{Base}}&
			BERT     & 60.6& 37.0& 71.5& 48.9& 28.3& 69.9& 31.1 &48.1\\
			                                  & RoBERTa            & 58.7            & 36.7            & 71.2                & 49.8             & 26.9              & 71.8          & 29.1           & 47.7          \\
			                                  & DeBERTaV3          & 45.9            & 15.2            & 44.3                & 39.0             & 3.5               & 42.2          & 25.0           & 26.9          \\
			                                  & NomicBERT$_{2048}$ & 55.0            & 35.3            & 69.0                & 48.8             & 30.5              & 70.1          & 30.1           & 47.1          \\
			                                  & ModernBERT         & 58.9            & 38.1            & 63.8                & 48.5             & 21.0              & 66.2          & 30.1           & 45.0          \\
			\midrule
			Medium                            & NeoBERT$_{4096}$   & 61.6            & \textbf{40.8}   & \textbf{76.2}       & 51.2             & \textbf{31.6}     & \textbf{74.8} & 30.7           & \textbf{51.3} \\
			\midrule
			\multirow{5}{*}{\makecell{Large}} & BERT               & 59.8            & 39.3            & 70.9                & 49.7             & 29.6              & 71.4          & \textbf{31.2}  & 49.1          \\
			                                  & RoBERTa            & 57.1            & 39.3            & 72.5                & \textbf{51.3}    & 30.0              & 71.7          & 31.1           & 48.9          \\
			                                  & DeBERTaV3          & 45.9            & 16.4            & 45.0                & 40.8             & 4.0               & 40.1          & 29.9           & 27.1          \\
			                                  & ModernBERT         & \textbf{62.4}   & 38.7            & 65.5                & 50.1             & 23.1              & 68.3          & 27.8           & 46.9          \\
			\bottomrule
		\end{tabular}
	}
	\label{tab:mteb_full}
\end{table*}

\subsection{Sequence Length}
\label{sec:seq_length}

Following previous literature, NeoBERT underwent an additional 50k pre-training steps, during which it was exposed to extended sequences of up to 4,096 tokens. To assess the impact of this additional training, we randomly sampled 2,467 long sequences from the English subset of Wikipedia. For each sequence, we independently masked each token at position $i$ and computed its cross-entropy loss, $l_i$. The pseudo-perplexity of the entire sentence is then defined as \( \mathcal{P} = \exp \left( \frac{1}{n} \sum_{i=1}^{n} l_i \right) \). We present the results in \autoref{fig:ppl}.

\begin{figure}[!htb]
	\centering
	\caption{Pseudo-Perplexity in function of the sequence length for NeoBERT$_{1024}$ \textit{(left)} and NeoBERT$_{4096}$ \textit{(right)}. This validates the effectiveness of the final pre-training stage on NeoBERT's ability to model long sequences.}
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/ppl_NeoBERT_1024.pdf}
		\label{fig:plot1}
	\end{minipage}
	\hfill
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/ppl_NeoBERT_4096.pdf}
		\label{fig:plot2}
	\end{minipage}
	\label{fig:ppl}
\end{figure}

Although NeoBERT$_{1024}$ was trained exclusively on sequences of up to $1,024$ tokens, it generalizes effectively to context lengths approaching 3,000 tokens. This demonstrates the robustness of RoPE embeddings to out-of-distribution inputs. Moreover, after an additional 50k training steps with sequences up to $4,096$ tokens, NeoBERT$_{4096}$ successfully models longer sequences. This approach provides a compute-efficient strategy for extending the model’s maximum context window beyond its original length.


\subsection{Efficiency}
\label{sec:efficiency}

To assess model efficiency, we construct a synthetic dataset consisting of maximum-length sequences of sizes $\{512, 1024, 2048, 4096, 8192\}$. For each sequence length, we scale the batch size from 1 to 512 samples or until encountering out-of-memory errors. Inference is performed for $100$ steps on a single A100 GPU, and we report the highest throughput achieved for each model and sequence length. \autoref{fig:efficiency} presents the results. 

Due to their low parameter count and relatively simple architecture, BERT and RoBERTa are the most efficient for sequences up to 512 tokens. However, their use of positional embeddings prevents them from further scaling the context window. For extended sequences, NeoBERT significantly outperforms ModernBERT$_{base}$, despite having $100M$ more parameters, achieving a $46.7\%$ speedup on sequences of $4,096$ tokens.

\begin{figure}[!ht]
	\centering
	\caption{Model throughput (tokens per second) as a function of sequence length ($\uparrow$ is better). Above $1,024$ in sequence length, NeoBERT surpasses ModernBERT$_{base}$ despite having $100M$ more parameters.}
	\label{fig:efficiency}
	\includegraphics[width=0.9\linewidth]{figures/efficiency.pdf}
\end{figure}


\section{Discussion}
\label{sec:discussion}

Encoders are compact yet powerful tools for language understanding and representation tasks. They require fewer parameters and significantly lower training costs compared to their decoder counterparts, making them strong alternatives for applications that do not involve text generation. Traditionally, the representational capacity of these models has been assessed through downstream tasks such as classification, in particular through the GLUE benchmark. 

While GLUE has played a pivotal role in guiding model adoption, it includes only nine sequence classification datasets, four of which are entailment tasks. Moreover, its small dataset sizes and occasionally ambiguous labeling make it prone to overfitting, with models long surpassing human performance on the benchmark. Although DeBERTa-v3 achieves state-of-the-art performance on GLUE by a significant margin, our fine-tuning experiments reveal its comparatively poor performance on the more recent MTEB benchmark. MTEB encompasses a broader range of datasets and tasks, but attaining high performance on its leaderboard necessitates carefully crafted fine-tuning strategies with costly training requirements. As more complex fine-tuning strategies emerge, it becomes unclear what the source of score improvements is. Moreover, these strategies are not easily reproducible or accessible, limiting the possibility of fair comparison between pre-trained backbones.

This underscores the limitations of current evaluation paradigms and highlights the need for more affordable and standardized frameworks. We advocate for future research to focus on the development of standardized fine-tuning protocols and the exploration of new zero-shot evaluation methodologies to ensure a more comprehensive and unbiased assessment of encoder-only models.


\section{Conclusion}
\label{sec:conclusion}

We introduced NeoBERT, a state-of-the-art encoder pre-trained from scratch with the latest advancements in language modeling, architecture, and data selection. To ensure rigorous validation, we systematically evaluated every design choice by fully training and benchmarking ten distinct models in controlled settings. On GLUE, NeoBERT outperforms BERT$_{large}$ and NomicBERT and is comparable with RoBERTa$_{large}$ despite being 100M parameters smaller and supporting sequences eight times longer. To further validate its effectiveness, we conducted a comprehensive evaluation on MTEB, carefully isolating the effects of pre-training and fine-tuning to provide a fair comparison against the best open-source embedding models. Under identical fine-tuning conditions, NeoBERT consistently outperforms all baselines. With its unparalleled efficiency, optimal depth-to-width, and plug-and-play compatibility, NeoBERT represents the next generation of encoder models. To foster transparency and collaboration, we release all code, data, model checkpoints, and training scripts, making NeoBERT the only fully open-source model of its kind.

\subsubsection*{Broader Impact Statement}

Despite its improvements, NeoBERT inherits the biases and limitations of its pre-training data. Moreover, the greatest jump in performance stems from the pre-training dataset, and as newer, larger, and more diverse datasets become available, retraining will likely be needed to further improve its performance. Nonetheless, NeoBERT stands today as an affordable state-of-the-art pre-trained encoder with great potential for downstream applications.

\section*{Acknowledgements}
\label{sec:acknowledgements}

Sarath Chandar is supported by the Canada CIFAR AI Chairs program, the Canada Research Chair in Lifelong Machine Learning, and the NSERC Discovery Grant. Quentin Fournier is supported by the Lambda research grant program. The authors acknowledge the computational resources provided by Mila and the Royal Military College of Canada.

% \bibliographystyle{tmlr}
\input{main.bbl}

\clearpage
\appendix


\section{Training details}
\label{app:training}

NeoBERT was trained on 8 H100 for 1,050,000 steps, for a total of 6,000 GPU hours. In the first stage of training, we used a local batch size of 32, 8 gradient accumulation steps, and a maximum sequence length of $1,024$, for a total batch size of 2M tokens. In the second stage of training, we keep the theoretical batch size constant and increase the maximum sequence length to $4,096$.

\section{Ablations}
\label{app:ablations}

Our first model, $M0$ is modeled after BERT$_{base}$ in terms of architecture. The only two differences are the absence of the next-sentence-prediction objective, as well as Pre-Layer Normalization. Each successive model, up until $M8$ is identical to the previous one on every point except for the change reported in \autoref{tab:ablations}.

\section{GLUE}
\label{app:glue}

We perform a classical parameter search with learning rates in $\{5e-6, 6e-6, 1e-5, 2e-5, 3e-5\}$, batch sizes in $\{4, 8, 16, 32\}$ and weight decay in $\{1e-2, 1e-5\}$. In addition, we start training from the best MNLI checkpoint for RTE, STS, MRPC, and QNLI.

We fine-tune on the training splits of every glue dataset for 10 epochs, with evaluation on the validation splits every $n$ steps, $n$ being defined as $\min(500, \text{len(dataloader) // } 10)$ with early stopping after 15 evaluation cycles if scores have not improved.

Following BERT, we exclude WNLI from our evaluation\footnote{See 12 in https://gluebenchmark.com/faq}. For tasks with two scores and for MNLI matched and mismatched, we report the average between the two metrics.

\begin{table*}[!ht]
	\centering
	\setlength{\tabcolsep}{8pt} % Adjust column spacing
	\renewcommand{\arraystretch}{1.2} % Adjust row spacing
	\begin{tabular}{l|c|c|c|c}
		\toprule
		\textbf{Model} & \textbf{Task} & \textbf{Batch Size} & \textbf{Learning Rate} & \textbf{Weight Decay} \\
		\midrule
		\multirow{8}{*}{NeoBERT$_{1024}$} 
		               & CoLA          & 4                   & 6e-6                   & 1e-5                  \\
		               & MNLI          & 16                  & 6e-6                   & 1e-2                  \\
		               & MRPC          & 8                   & 2e-5                   & 1e-5                  \\
		               & QNLI          & 8                   & 5e-6                   & 1e-5                  \\
		               & QQP           & 32                  & 1e-5                   & 1e-2                  \\
		               & RTE           & 8                   & 6e-6                   & 1e-5                  \\
		               & SST-2         & 16                  & 1e-5                   & 1e-5                  \\
		               & STS-B         & 8                   & 1e-5                   & 1e-2                  \\
		\midrule
		\multirow{8}{*}{NeoBERT$_{4096}$} 
		               & CoLA          & 8                   & 8e-6                   & 1e-5                  \\
		               & MNLI          & 16                  & 5e-6                   & 1e-5                  \\
		               & MRPC          & 2                   & 1e-5                   & 1e-5                  \\
		               & QNLI          & 8                   & 5e-6                   & 1e-5                  \\
		               & QQP           & 32                  & 8e-6                   & 1e-5                  \\
		               & RTE           & 32                  & 5e-6                   & 1e-5                  \\
		               & SST-2         & 32                  & 8e-6                   & 1e-2                  \\
		               & STS-B         & 32                  & 2e-5                   & 1e-5                  \\
		\bottomrule
	\end{tabular}
	\caption{Optimal hyperparameters for GLUE tasks. The grid search was conducted over batch sizes $\{2, 4, 8, 16, 32\}$, learning rates $\{5e-6, 6e-6, 8e-6, 1e-5, 2e-5, 3e-5\}$, and weight decay values $\{1e-2, 1e-5\}$.}
	\label{tab:glue_hp}
\end{table*}

\section{MTEB}
\label{app:contrastive}

\subsection{Evaluation of pre-trained models}

As demonstrated in \autoref{fig:mteb-pre-trained}, evaluating out-of-the-box pre-trained models on MTEB is inconclusive. In that setting, BERT$_{base}$ outperforms both BERT$_{large}$ and RoBERTa$_{large}$, highlighting the importance of fine-tuning to ensure representative evaluation on the MTEB benchmark.

\begin{figure}[!htb]
	\caption{Zero-shot evaluation of BERT and RoBERTa on the English subset of MTEB.}
	\centering
	\includegraphics[width=0.6\linewidth]{figures/mteb_pretrained.png}
	\label{fig:mteb-pre-trained}
\end{figure}

\subsection{Contrastive learning}

Following the existing literature, we designed a simple fine-tuning strategy entirely agnostic to the models evaluated. We used cosine similarity and $\tau = 0.07$ as a temperature parameter in the contrastive learning loss. Additionally, we sampled datasets with a multinomial distribution based on their sizes $(n_j)_{j=1}^m$ with $\alpha=0.5$:

\[
	\pi = \frac{n_i^{\alpha}}{\sum_{j=1}^{m} n^{\alpha}_j}
\]

We trained on the following fully-open datasets: AG-News~\citep{zhang2016characterlevelconvolutionalnetworkstext}, All-NLI~\citep{bowman2015largeannotatedcorpuslearning, williams2018broadcoveragechallengecorpussentence}, AmazonQA~\citep{gupta2019amazonqareviewbasedquestionanswering}, ConcurrentQA~\citep{arora2022reasoningpublicprivatedata}, GitHub Issues
\citep{li2023angle}, GooAQ~\citep{khashabi2021gooaqopenquestionanswering}, MedMCQA~\citep{pal2022medmcqalargescalemultisubject}, NPR\footnote{\url{https://huggingface.co/datasets/sentence-transformers/npr}}, 
PudMedQA~\citep{jin2019pubmedqadatasetbiomedicalresearch}, SentenceCompression~\citep{filippova-altun-2013-overcoming} StackExchange\footnote{\url{https://huggingface.co/datasets/sentence-transformers/stackexchange-duplicates}}, TriviaQA~\citep{han2019episodicmemoryreaderlearning}, Wikihow~\citep{koupaee2018wikihowlargescaletext}, Yahoo! Answers~\citep{zhang2016characterlevelconvolutionalnetworkstext} as well as the available training splits of MTEB datasets (StackOverFlowDupQuestion, Fever~\citep{thorne2018feverlargescaledatasetfact}, MS MARCO~\citep{bajaj2018msmarcohumangenerated}, STS12, and STSBenchmark~\citep{Cer_2017}).

We fine-tune every model for 2,000 steps and evaluate on MTEB in float16. The complete results are presented in \autoref{tab:mteb_full}.


\begin{figure*}[!htb]
	\caption{Average MTEB scores of fine-tuned encoders grouped by task type. The average score is computed across the 56 tasks of MTEB-English. NeoBERT is the best model on five out of seven task types and the best model overall. See \autoref{tab:mteb_full} for complete scores.}
	\label{fig:mteb-results}
	\centering
	\includegraphics[width=1\linewidth]{figures/mteb.pdf}
\end{figure*}

\subsection{Task instructions}

We provide the set of instructions used for fine-tuning in \autoref{tab:finetuning_instructions} and evaluation in \autoref{tab:mteb_instruct2} and \autoref{tab:mteb_instruct1}.

\begin{table*}
	\setlength{\tabcolsep}{1.5pt}
	\centering
	\caption{Instructions for fine-tuning on the different contrastive learning datasets.}
	\label{tab:finetuning_instructions}
	\begin{tabular}{ll}
		\toprule
		Dataset           & Instruction                                                                        \\ \midrule
		AGNEWS            & Given a news title, retrieve relevant articles.                                    \\
		ALLNLI            & Given a premise, retrieve a hypothesis that is entailed by the premise.            \\
		AMAZONQA          & Given a question, retrieve Amazon posts that answer the question.                  \\
		CONCURRENTQA      & Given a multi-hop question, retrieve documents that can help answer the            \\
		                  & question.                                                                          \\
		FEVER             & Given a claim, retrieve documents that support or refute the claim.                \\
		GITHUBISSUE       & Given a question, retrieve questions from Github that are duplicates to the given  \\
		                  & question.                                                                          \\
		GOOAQ             & Given a question, retrieve relevant documents that best answer the question.       \\
		MEDMCQA           & Given a medical question, retrieve relevant passages that answer the question.     \\
		MEDMCQA$_{CLUST}$ & Identify the main category of medical exams based on their questions               \\
		                  & and answers.                                                                       \\
		MSMARCO           & Given a web search query, retrieve relevant passages that answer the query.        \\
		NPR               & Given a news title, retrieve relevant articles.                                    \\
		PAQ               & Given a question, retrieve Wikipedia passages that answer the question.            \\
		PUBMEDQA          & Given a medical question, retrieve documents that best answer the question.        \\
		QQP               & Given a question, retrieve questions from Quora forum that are semantically        \\
		                  & equivalent to the given question.                                                  \\
		SENTENCECOMP      & Given a sentence, retrieve semantically equivalent summaries.                      \\
		STACKEXCHANGE     & Given a Stack Exchange post, retrieve posts that are duplicates to the given post. \\
		STACKOVERFLOW     & Retrieve duplicate questions from StackOverflow forum.                             \\
		STS12             & Retrieve semantically similar text.                                                \\
		STSBENCHMARK      & Retrieve semantically similar text.                                                \\
		TRIVIAQA          & Given a question, retrieve documents that answer the question.                     \\
		WIKIHOW           & Given a Wikihow post, retrieve titles that best summarize the post.                \\
		YAHOO$_{CLUST}$   & Identify the main topic of Yahoo posts based on their titles and answers.          \\ \bottomrule
	\end{tabular}
\end{table*}

\begin{table*}
	\setlength{\tabcolsep}{2pt}
	\centering
	\caption{Instructions for evaluation on the different MTEB tasks.}
	\label{tab:mteb_instruct2}
	\begin{tabular}{ll}
		\toprule
		Task name      & Instruction                                                                       \\ \midrule
		DBPedia        & Given a query, retrieve relevant entity descriptions from DBPedia.                \\
		FEVER          & Given a claim, retrieve documents that support or refute the claim.               \\
		FiQA2018       & Given a financial question, retrieve user replies that best answer the question.  \\
		HotpotQA       & Given a multi-hop question, retrieve documents that can help answer the question. \\
		MSMARCO        & Given a web search query, retrieve relevant passages that answer the query.       \\
		NFCorpus       & Given a question, retrieve relevant documents that best answer the question.      \\
		NQ             & Given a question, retrieve Wikipedia passages that answer the question.           \\
		QuoraRetrieval & Given a question, retrieve questions that are semantically equivalent to the      \\
		               & given question.                                                                   \\
		SCIDOCS        & Given a scientific paper title, retrieve paper abstracts that are cited by the    \\
		               & given paper.                                                                      \\
		SciFact        & Given a scientific claim, retrieve documents that support or refute the claim .   \\
		Touche2020     & Given a question, retrieve detailed and persuasive arguments that answer          \\
		               & the question.                                                                     \\
		TRECCOVID      & Given a query on COVID-19, retrieve documents that answer the query.              \\
		SICK-R         & Retrieve semantically similar text.                                               \\
		STS            & Retrieve semantically similar text.                                               \\
		BIOSSES        & Retrieve semantically similar text from the biomedical field.                     \\
		SummEval       & Given a news summary, retrieve other semantically similar summaries.              \\
		\bottomrule
	\end{tabular}
\end{table*}

\begin{table*}
	\setlength{\tabcolsep}{1.5pt}
	\centering
	\caption{Instructions for evaluation on the different MTEB tasks.}
	\label{tab:mteb_instruct1}
	\begin{tabular}{ll}
		\toprule
		Task name                      & Instruction                                                            \\ \midrule
		AmazonCounterfactualClass.     & Given an Amazon customer review, classify it as either counterfactual  \\
		                               & or not-counterfactual.                                                 \\
		AmazonPolarityClass.           & Given an Amazon review, classify its main sentiment into positive      \\
		                               & or negative.                                                           \\
		AmazonReviewsClass.            & Given an Amazon review, classify it into its appropriate rating        \\
		                               & category.                                                              \\
		Banking77Class.                & Given a online banking query, find the corresponding intents.          \\
		EmotionClass.                  & Given a Twitter message, classify the emotion expressed into one of    \\
		                               & the six emotions: anger, fear, joy, love, sadness, and surprise.       \\
		ImdbClass.                     & Given an IMDB movie review, classify its sentiment into positive or    \\
		                               & negative.                                                              \\
		MassiveIntentClass.            & Given a user utterance, find the user intents.                         \\
		MassiveScenarioClass.          & Given a user utterance, find the user scenarios.                       \\
		MTOPDomainClass.               & Given a user utterance, classify the domain in task-oriented           \\
		                               & conversation.                                                          \\
		MTOPIntentClass.               & Given a user utterance, classify the intent in task-oriented           \\
		                               & conversation.                                                          \\
		ToxicConversationsClass.       & Given comments, classify them as either toxic or not toxic.            \\
		TweetSentimentExtractionClass. & Given a tweet, classify its sentiment as either positive, negative, or \\
		                               & neutral.                                                               \\
		<dataset>ClusteringP2P         & Identify the main and secondary category of <dataset> papers based on  \\
		                               & their titles and abstracts.                                            \\
		<dataset>ClusteringS2S         & Identify the main and secondary category of <dataset> papers based on  \\
		                               & their titles.                                                          \\
		<dataset>Clustering            & Identify the topic or theme of <dataset> posts based on their titles.  \\
		TwentyNewsgroupsClustering     & Identify the topic or theme of the given news articles.                \\
		SprintDuplicateQuestions       & Retrieve duplicate questions from Sprint forum.                        \\
		TwitterSemEval2015             & Given a tweet, retrieve tweets that are semantically similar.          \\
		TwitterURLCorpus               & Given a tweet, retrieve tweets that are semantically similar.          \\
		AskUbuntuDupQuestions          & Retrieve duplicate questions from AskUbuntu forum.                     \\
		MindSmallReranking             & Given a user browsing history, retrieve relevant news articles.        \\
		SciDocsRR                      & Given a title of a scientific paper, retrieve the                      \\
		                               & relevant papers.                                                       \\
		StackOverflowDupQuestions      & Retrieve duplicate questions from StackOverflow forum.                 \\
		ArguAna                        & Given a claim, find documents that refute the claim. Document          \\
		ClimateFEVER                   & Given a claim about climate change, retrieve documents that support    \\
		                               & or refute the claim.                                                   \\
		CQADupstackRetrieval           & Given a question, retrieve detailed question descriptions from         \\
		                               & Stackexchange that are duplicates to the given question.               \\
		\bottomrule
	\end{tabular}
\end{table*}

\section{Efficiency}
\label{app:efficiency}

\autoref{tab:efficiency} presents the complete results of model efficiency evaluations.

\begin{table*}[!ht]
	\centering
	\caption{Throughput ($10^3$ tokens / second) in function of the sequence length, with optimal batch size.}
	\begin{tabular}{clrrrrr}
		\toprule
		\textbf{Size}                     & \textbf{Model}       & \textbf{512}            & \textbf{1024}           & \textbf{2048}           & \textbf{4096}           & \textbf{8192}           \\
		\midrule
		\multirow{3}{*}{\makecell{Base}}  & BERT$_{base}$        & $\textbf{27.6} \pm 3.6$ & -                       & -                       & -                       & -                       \\
		                                  & RoBERTa$_{base}$     & $24.9 \pm 3.0$          & -                       & -                       & -                       & -                       \\
		                                  & ModernBERT$_{base}$  & $25.4 \pm 2.3$          & $\textbf{22.6} \pm 2.7$ & $17.2 \pm 1.7$          & $11.7 \pm 0.8$          & $6.8 \pm 0.2$           \\
		\midrule
		Medium                            & NeoBERT              & $24.5 \pm 1.4$          & $\textbf{22.2} \pm 1.7$ & $\textbf{20.5} \pm 1.6$ & $\textbf{17.2} \pm 1.2$ & $\textbf{13.0} \pm 0.2$ \\
		\midrule
		\multirow{3}{*}{\makecell{Large}} & BERT$_{large}$       & $19.5 \pm 0.6$          & -                       & -                       & -                       & -                       \\
		                                  & RoBERTa$_{large}$    & $15.9 \pm 0.3$          & -                       & -                       & -                       & -                       \\
		                                  & ModernBERT$_{large}$ & $13.4 \pm 0.2$          & $11.4 \pm 1.1$          & $9.2 \pm 0.7$           & $6.5 \pm 0.3$           & $3.8 \pm 0.1$           \\
		\bottomrule
	\end{tabular}
	\label{tab:efficiency}
\end{table*}

\end{document}