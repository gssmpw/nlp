\PassOptionsToPackage{usenames,dvipsnames}{xcolor}
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}
\usepackage{url}            % simple URL typesetting
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{bm}
\usepackage{multirow}

% Attempt to make hyperref and algorithmic work together better:
\usepackage[ruled,vlined]{algorithm2e}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{mathtools}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage[misc]{ifsym}
\usepackage{tikz}
\usepackage{comment}
\usepackage{wrapfig}
\usepackage{adjustbox}
\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}

%%% Coloring the comment as blue - algorithm2e
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output} 

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{problem}{Problem}

\usepackage[many]{tcolorbox}

\tcolorboxenvironment{theorem}{
  colback=black!5,  % Background color
  colframe=black!0, % Border color
  coltitle=black,   % Title color
  boxrule=1pt,      % Border thickness
  enhanced,
}

\tcolorboxenvironment{lemma}{
  colback=black!5,  % Background color
  colframe=blue!0, % Border color
  coltitle=black,   % Title color
  boxrule=1pt,      % Border thickness
  enhanced,
}

%new:
\usepackage{colortbl}
\usepackage{bm}
\definecolor{Gray}{gray}{0.9}

\makeatletter
\def\thanks#1{\protected@xdef\@thanks{\@thanks
        \protect\footnotetext{#1}}}
\makeatother

\title{Towards Self-Supervised Covariance\\Estimation in Deep Heteroscedastic Regression}

% Author must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

%\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
%about author (webpage, alternative address)---\emph{not} for acknowledging
%funding agencies.  Funding acknowledgements go at the end of the paper.} \\
%Department of Computer Science\\
%Cranberry-Lemon University\\
%Pittsburgh, PA 15213, USA \\
%\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
%\And
%Ji Q. Ren \& Yevgeny LeNet \\
%Department of Computational Neuroscience \\
%University of the Witwatersrand \\
%Joburg, South Africa \\
%\texttt{\{robot,net\}@wits.ac.za} \\
%\AND
%Coauthor \\
%Affiliation \\
%Address \\
%\texttt{email}
%}
\author{Megh Shukla\\
EPFL\\
\And
Aziz Shameem ${}^{\dagger}$ \\
IIT Bombay \\
\And
Mathieu Salzmann \\
SDSC and EPFL \\
\And
Alexandre Alahi \thanks{${}^{\dagger}$ Work done by Aziz Shameem during his internship at EPFL.\\ \hspace*{0.5cm} Contact: \texttt{firstname.lastname@epfl.ch}} \\
EPFL \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\makeatletter
\setlength{\@fptop}{0pt}
\makeatother

\begin{document}


\maketitle
\vspace*{-0.2cm}
\centerline{\textcolor{violet}{\textbf{\href{https://deep-regression.github.io}{\texttt{deep-regression.github.io}}}}}
\vspace*{0.2cm}

\begin{abstract}
Deep heteroscedastic regression models the mean and covariance of the target distribution through neural networks. The challenge arises from heteroscedasticity, which implies that the covariance is sample dependent and is often unknown. Consequently, recent methods learn the covariance through unsupervised frameworks, which unfortunately yield a trade-off between computational complexity and accuracy. While this trade-off could be alleviated through supervision, obtaining labels for the covariance is non-trivial.
%, limiting its study. 
Here, we study \textit{self-supervised} covariance estimation in deep heteroscedastic regression. We address two questions: (1) How should we supervise the covariance assuming ground truth is available? (2) How can we obtain pseudo-labels in the absence of the ground-truth? We address (1) by analysing two popular measures: the KL Divergence and the 2-Wasserstein distance. Subsequently, we derive an upper bound on the 2-Wasserstein distance between normal distributions with non-commutative covariances that is stable to optimize. We address (2) through a simple neighborhood based heuristic algorithm which results in surprisingly effective pseudo-labels for the covariance. Our experiments over a wide range of synthetic and real datasets demonstrate that the proposed 2-Wasserstein bound coupled with pseudo-label annotations results in a computationally cheaper yet accurate deep heteroscedastic regression. 

\end{abstract}
\section{Introduction}

Deep heteroscedastic regression leverages neural networks as powerful feature extractors to regress the mean and covariance of the target distribution. The target distribution is typically used for downstream tasks such as uncertainty estimation, correlation analysis, sampling, and in bayesian frameworks. The key challenge in deep heteroscedastic regression lies in estimating heteroscedasticity, which implies that the variance of the target is input dependent and variable. Moreover, unlike the mean, the covariance lacks direct supervision and needs to be inferred.

The standard approach without the ground-truth covariance relies on optimizing the negative log-likelihood to jointly learn the mean and covariance \citep{dorta2018structured}. However, \citet{skafte2019reliable, seitzer2022on} show that in the absence of supervision, incorrect variance predictions lead to sub-optimal convergence. Subsequently, a flurry of recent works addresses this by either modifying the negative log-likelihood \citep{skafte2019reliable, seitzer2022on, stirn2023faithful, immer2023effective} or improving the covariance through better parameterization \citep{pmlr-v235-shukla24a}. However, the methods introduce a trade-off between computational complexity and accuracy. Moreover, the thematic message underlying these works is that estimating heteroscedasticity is challenging when annotations for the covariance are not available. Therefore, would having annotations for the covariance alleviate this trade-off? To answer this, we explore the use of self-supervision for covariance estimation in deep heteroscedastic regression. We study two questions:

\textbf{(Q1) How can we supervise the learning of the covariance assuming annotations are available?} Since the negative log-likelihood is not formulated for supervising the covariance, we analyse the KL Divergence and the 2-Wasserstein distance to supervise the learning of the mean and covariance. Our analysis highlights the utility of the KL Divergence as a regularizer of the covariance. However, we observe that despite supervision, the KL divergence underperforms when compared to the 2-Wasserstein distance, as it shares a susceptibility to residuals similar to that of the negative log-likelihood. Next, we study the 2-Wasserstein distance between normal distributions with non-commutative matrices. We specifically note an optimization challenge \citep{PyTorchEigH} due to the eigendecomposition involved. Consequently, we extend the formulation for the 2-Wasserstein distance between commutative covariance matrices to the general case of non-commutative matrices, eliminating the need for eigendecomposition. This makes the optimization process stable.

\textbf{(Q2)} \textbf{How can we obtain pseudo-labels for the covariance when annotations are not available?} \\In the absence of priors, we propose a neighborhood-based heuristic algorithm to generate pseudo-labels for the covariance. Specifically, for a given sample, the pseudo-label corresponds to the covariance over the targets of all the samples in the neighborhood of the specified sample. The contribution of the neighboring samples are weighed by their Mahalanobis distance to the specified sample. We show that this simple strategy provides effective self-supervision for covariance estimation.

We perform extensive experiments across a wide range of synthetic and real world settings. Our results show that the proposed self-supervised framework is (1) computationally cheaper and (2) retains accuracy with respect to the state-of-the-art. Finally, our experiments on human pose also highlight the possibility of using self-supervised and unsupervised objectives to further improve optimization. Our code is available at our project page.


\section{Deep Heteroscedastic Regression}
\label{Deep Heteroscedastic Regression}

Heteroscedastic regression is the task of modeling the mean and the variance of the target distribution. In contrast to homoscedasticity, heteroscedastic models allow the variance to vary as a function of the input. Deep heteroscedastic regression provides an advantage over non-parametric methods like the Gaussian Processes \citep{gp_heteroscedastic} because it can model complex features from inputs such as images. This attribute has made it widely applicable in fields like active learning \citep{houlsby2011bayesian, gal2017deep}, uncertainty estimation \citep{gal2016dropout, kendall2017uncertainties, lakshminarayanan2017simple, russell2021multivariate}, image reconstruction \citep{dorta2018structured}, human pose \citep{ajain, nakka2023understanding, Tekin_2017_ICCV}, and other vision-based tasks \citep{lu2022few, simpson2022learning, 8461047, bertoni2019monoloco}.

\label{par:notation}\textbf{Preliminaries.} Our goal is to learn the target distribution $P(Y | X)$ for different $X$, where $X \in \mathbb{R}^m$ is the input and $Y \in \mathbb{R}^n$ is the target variable. While $P(Y | X)$ is unknown, it is assumed to be normally distributed: $P(Y | X) = \mathcal{N}(\mu_Y(X), \Sigma_Y(X))$. Our estimate of the target is $P(\widehat{Y} | X) = \mathcal{N}(\widehat{\mu}_{Y}(X), \widehat{\Sigma}_{Y}(X))$, where the mean $\widehat{\mu}_{Y}(X) = f_{\theta}(X)$ and covariance $\widehat{\Sigma}_{Y}(X) = g_{\Theta}(X)$ are parameterized by neural networks. The standard approach in the literature to learn the target distribution is through minimizing the negative log-likelihood, $-\mathbb{E}_{P(X, Y)}P(\widehat{Y} | X)$. Specifically, the mean and covariance networks are trained to minimize \citep{nix1994estimating} 
\begin{equation}
\mathcal{L}_{\textrm{NLL}}(\theta, \Theta) := \mathbb{E}_{P(X, Y)} \bigg[ \log \; \Bigl| \widehat{\Sigma}_{Y}(X) \Bigr| + (Y  - \widehat{\mu}_{Y}(X))^T \,\, \widehat{\Sigma}_{Y}^{-1}(X) \,\, (Y  - \widehat{\mu}_{Y}(X)) \bigg].
\label{eq:nll}
\end{equation}

\textbf{Challenges.} The lack of supervision for the covariance results in an optimization challenge which is formalized in \citet{skafte2019reliable, seitzer2022on}. The works observed that an underestimated variance can increase the effective learning rate and disrupt optimization \citep{skafte2019reliable}, whereas an overestimated variance can decrease the effective learning rate and stop optimization \citep{seitzer2022on}. A number of recent approaches modify the negative log-likelihood to reduce the effect of the predicted covariance during optimization. $\beta$-NLL \citep{seitzer2022on} scales the negative log-likelihood (Eq. \ref{eq:nll}) by the predicted variance resulting in the objective: $\mathcal{L}_{\beta-\textrm{NLL}} = \lfloor \hat{\sigma}(\hat{y})^{2 \beta} \rfloor \times \mathcal{L}_{\textrm{NLL}}$. However, since $\beta$-NLL does not originate from a valid distribution, the optimized values do not estimate the true variance. \citet{stirn2023faithful} decouples the estimation of the mean and variance by scaling the gradient of the mean estimator with the covariance, thereby eliminating the effect of the predicted covariance on the mean. This leads to conflicting assumptions: the mean estimator assumes that the multivariate residual is uncorrelated while the covariance estimator is expected to identify correlations. \citet{immer2023effective} proposed the use of natural parameterization of the univariate normal distribution: $n_1 = \frac{\mu}{\sigma^2}$ and $n_2 = \frac{-1}{2\sigma^2}$ for regression. While principled, the method assumes univariate targets. TIC-TAC \citep{pmlr-v235-shukla24a}, in contrast to previous works, retains the negative log-likelihood and formulates the predicted covariance through the gradient and curvature of the mean. However, the improvement in accuracy comes at the expense of increased computational requirements. A parallel line of works studies the impact of training dynamics in deep heteroscedastic regression. \citet{wong-toi2024understanding} provide a theoretical study linking the training of heteroscedastic regression to phase transitions, however the experimental evaluation is limited to univariate outputs. \citet{sluijterman2024optimal} experimentally show that decoupling the mean and variance networks can lead to improved performance, similar to \citet{stirn2023faithful}. However, while the authors suggested a warm-up schedule, we observed that this may not necessarily improve performance. 

An overview of the related works reveals a shared theme: estimating heteroscedasticity is difficult without annotations. Further, existing works trade-off accuracy for lower computational requirements. This trade-off could be mitigated with supervision; however, acquiring labels for the covariance is challenging, which restricts further analysis. Moreover, the negative log-likelihood is not formulated to supervise the covariance, requiring a new approach to supervision. Therefore, we investigate two key aspects of the problem: (1) How can we supervise the covariance when negative log-likelihood is not specifically designed for this task? and (2) How can we generate pseudo-labels for the covariance in the absence of ground truth?

\section{Analysis}
\label{Analysis}
We analyze the KL Divergence and the 2-Wasserstein distance, two widely used metrics for comparing and optimizing distributions. Our analysis focuses on multivariate normal distributions, which follows a common assumption in machine learning that the residuals are normally distributed. We support our analysis by studying Problem 1, which lets us visualize the convergence process of various methods using bivariate normal distributions. We then seek to answer which metric is better suited for deep heteroscedastic regression.


\begin{wrapfigure}{r}{0.25\textwidth}
  \begin{center}
    \vspace{-20pt}
    \includegraphics[width=0.25\textwidth]{figs/kl/init.png}
    \vspace{-30pt}
  \end{center}
  \label{fig:init}
\end{wrapfigure}

\textbf{Problem 1.} (Bivariate Normal Distribution)
\textit{We consider the task of learning a bivariate normal distribution. We initialize the target and predicted distributions to have different means. While the predicted distribution is initialized with an identity covariance matrix, the covariance for the target distribution is initialized randomly and such that it exhibits a high degree of correlation ($> 0.5$). Given samples $\vy \in \mathbb{R}^2$ from the target distribution, the goal is to compare different methods in optimizing the predicted distribution to match the target one.}

\subsection{KL Divergence}

\label{sec:KL}
The KL Divergence between two continuous distributions $p, q$ is $D_{KL}(p \| q) =\mathbb{E}_p \log [\, p(x) / q(x)\,]$. For two multivariate normal distributions \citep{zhang2024properties, Soch_2020} we have
\begin{equation}
    D_{\text{KL}}(p \parallel q) = \frac{1}{2} \left[ \texttt{Tr}(\Sigma_q^{-1} \Sigma_p) + (\mu_q - \mu_p)^\top \Sigma_q^{-1} (\mu_q - \mu_p) - k + \ln\left(\frac{\det \Sigma_q}{\det \Sigma_p}\right) \right]\,,
    \label{eq:kl_def}
\end{equation}
Since the KL Divergence is asymmetric, we often let $p$ and $q$ be the target and predicted distributions respectively. This definition also gives rise to popular alternatives such as the negative log-likelihood. While the KL Divergence has been well explored \citep{goodfellow2016deep, pmlr-v70-arjovsky17a} from a statistical viewpoint, we ask: how can the KL Divergence be formulated for deep heteroscedastic regression? Answering this is necessary since the KL Divergence is defined in terms of the means and covariances, which are unknown for the target distribution.

\textbf{Formulation.} A logical approach would be to replace each label with a distribution. Specifically, for a sample $\vx_i, \vy_i$ from the dataset, the pseudo target distribution can be set to $\mathcal{N}(\vy_i, \Sigma_Y^{\text{(prior)}}(X))$. However, this approach requires calibrating the KL Divergence since the optimal value for the covariance is not the same as the prior. We demonstrate this through a simple setting in Lemma \ref{lemma:kl}.

\begin{lemma}[Calibration]
    Let $\bf{S} = \{\vx, \vy_i\}_{i=1}^{N}$ be a set of samples drawn from the unknown target $P(Y | X) = \mathcal{N}(\mu_Y(X), \Sigma_Y(X))$ for a given $\vx$. We write each label $\vy_i$ as a distribution $\mathcal{N}(\vy_i, \Sigma_Y^{\text{(prior)}}(X))$. Then, the optimal solution using the KL Divergence for the predicted covariance over the set $\bf{S}$ is  $\widehat{\Sigma}_{Y}(X) \approx \Sigma_{Y}(X) + \Sigma_Y^{\text{(prior)}}(X)$. Consequently, if the target covariance is known and set as the prior, we have $\widehat{\Sigma}_{Y}(X) \approx 2\,\Sigma_{Y}(X)$.
    \label{lemma:kl}
\end{lemma}

We refer the reader to the appendix, section (\ref{app:lemma}) for the proof.

\begin{figure}[!t]
\includegraphics[width=\textwidth]{figs/kl/NLL_KL_lr1e-2_00050.png}
\caption{\textit{Sub-optimal convergence due to residuals (Section: \ref{sec:KL})}. In addition to feature granularity \citep{seitzer2022on}, subpar convergence may occur due to the sensitvity of the negative log-likelihood and the KL-Divergence to residuals in the mini-batch. While we show that the KL-Divergence can act as a regularizer over the learnt covariance, the gradients for both the methods are dominated by the residual term, slowing down convergence.}
\label{fig:nll_kl}
\end{figure}

\textbf{Discussion.} The optimal solution for $\widehat{\Sigma}_{Y}(X)$ is twice the target covariance. This can be addressed by a simple calibration of the KL Divergence (Eq. \ref{eq:kl_def}); \[
\frac{1}{2} \left[ \frac{\texttt{Tr}(\widehat{\Sigma}_{Y}^{-1} \Sigma_Y^{\text{(prior)}}) + (\widehat{\mu}_{Y} - \vy)^\top \widehat{\Sigma}_{Y}^{-1} (\widehat{\mu}_{Y} - \vy)}{2} - k + \ln\left(\frac{\det \widehat{\Sigma}_{Y}}{\det \Sigma_Y^{\text{(prior)}}}\right) \right]\,.
\label{eq:kl_calibration}
\]
In the general scenario where the true covariance is not known \textit{a priori}, the optimal solution is the average of the prior and the sample covariance. This introduces a notion of regularization on the predicted covariance. Finally, the predicted covariance is also robust to outliers.

\textbf{Impact of residuals.} In general, the solution in Lemma \ref{lemma:kl} is reached \textit{only} when the mean estimator has converged to the true mean and when we observe multiple targets $\vy_i$ for the same observation $\vx$. This may not hold true in practical settings because: (1) samples in a batch are \textit{i.i.d}, implying that the same observation $\vx$ is unlikely to be repeated, and (2) the mean estimator may not have converged.

In practice, for each sample in the batch, we take a noisy gradient step towards $\widehat{\Sigma}_{Y}(X) = \Sigma_Y^{\text{(prior)}}(X) + (\widehat{\mu}_{Y}(X) - \vy)(\widehat{\mu}_{Y}(X) - \vy)^T$ (appendix/eq.\ref{eq:kl_mvn_derivative}). If the residual term $(\widehat{\mu}_{Y}(X) - \vy)$ is large, the gradient step due to the residual dominates over the prior covariance, moving us closer to $ \widehat{\Sigma}_{Y}(X) \approx 
(\widehat{\mu}_{Y}(X) - \vy)(\widehat{\mu}_{Y}(X) - \vy)^T$. This residual matrix can be interpreted as a `covariance' matrix aligned along the line segment joining $\vy$ and $\widehat{\mu}_{Y}(X)$ (Eq. 3; OLS estimate of the slope, \citet{Soch_2021}). However, this residual matrix desensitizes the mean estimator to move along $\vy$ and $\widehat{\mu}_{Y}(X)$, slowing down optimization. This is because the `variance' induced by the residual matrix is large along $\vy$ and $\widehat{\mu}_{Y}(X)$, and the gradient of the mean estimator is proportional to the inverse of the covariance (appendix/eq.\ref{eq:kl_mvn_mean}). This is pictorially depicted in appendix/Fig. \ref{fig:nll_kl_intuition}. 

We study this phenomenon through Problem 1 in Fig. \ref{fig:nll_kl}. We observe that after a few iterations, the predicted covariance is aligned along the means of the target and predicted distribution. This observation is a result of the residual term appearing in the optimal solution. Moreover, while the KL Divergence incorporates our prior knowledge of the covariance, the prior term is dwarfed in magnitude when compared to the residual term. We also note increased optimization instability at higher learning rates (appendix/fig. \ref{fig:nll_kl_appendix}). While the KL Divergence leverages the prior covariance as a regularizer, it shares drawbacks pertaining to the residual with the negative log-likelihood, motivating our analysis of the 2-Wasserstein distance.

\subsection{2-Wasserstein Distance}
\label{sec:2-w}

\begin{figure}
\includegraphics[width=\textwidth]{figs/kl/file0000070.png}
\caption{\textit{Visualizing convergence in bivariate regression (Section: \ref{sec:2-w})}. We observe that the KL-Divergence and likelihood based methods: vanilla negative log-likelihood and Faithful \citep{stirn2023faithful} result in unstable convergence due to the sensitivity of the methods to the residuals. In comparison, the 2-Wasserstein based methods are more stable and accurate. This observation can also be replicated when the predicted mean is initialized at the same location as the true mean, shown in appendix/Fig.\ref{fig:bivariate} (b) \textit{(Note: metrics NLL / KL and 2-W are plotted in log-scale)}
\label{fig:bivariate_main}} 
\end{figure}

The Wasserstein distance is a metric for quantifying the distance between two probability distributions. It defines the minimum ``cost" required to morph one distribution into another. The 2-Wasserstein distance measures the cost in proportion to the squared Euclidean distance. It is widely used in optimal transport theory and generative modeling \citep{pmlr-v70-arjovsky17a, li2024convergence}, as it captures both the shape and spread of distributions while penalizing long-distance transport more heavily. Let $\mathcal{N}_1(\mu_1, \Sigma_1)$, $\mathcal{N}_2(\mu_2, \Sigma_2)$ be two multivariate normal distributions. The 2-Wasserstein distance between them is given by
\begin{equation}
    ||\mu_1 - \mu_2||^2 + \textrm{Tr} \big[ \Sigma_1 + \Sigma_2 - 2(\Sigma_2^{1/2} \Sigma_1 \Sigma_2^{1/2})^{1/2} \big] \;.
    \label{eq:2-w}
\end{equation}
This formulation, however, requires computing the root of a matrix, which typically involves eigendecomposition. Unfortunately, the eigendecomposition in popular deep learning frameworks can potentially lead to unstable gradients \citep{PyTorchEigH}. If $\Sigma_1$ and $\Sigma_2$ are commutative (implying $\Sigma_1 \Sigma_2$ = $\Sigma_2 \Sigma_1$), then the 2-Wasserstein distance is reduced to $\mathcal{W}_2(\mathcal{N}_1, \mathcal{N}_2) = ||\mu_1 - \mu_2||^2 + ||\Sigma_1^{1/2} - \Sigma_2^{1/2}||^2_{F}$. This formulation allows us to directly predict the square roots, thereby avoiding the eigendecomposition. However, for two covariance matrices to be commutative, they need to share the same eigenbasis, implying that the matrices differ only in the variance of the individual random variables. Fortunately, Theorem \ref{thm:2-wasserstein} allows us to expand this formulation to non-commutative covariance matrices by linking it to an upper bound on the true 2-Wasserstein distance.

\begin{theorem}[2-Wasserstein bound for non-commutative covariances]
\label{thm:2-wasserstein}
Let $\mathcal{N}_1(\mu_1, \Sigma_1)$, $\mathcal{N}_2(\mu_2, \Sigma_2)$ be two multivariate normal distributions, where $\Sigma_1$ and $\Sigma_2$ are non-commutative matrices. Then, the 2-Wasserstein distance between the two distributions has an upper bound of
\[ \mathcal{W}_2(\mathcal{N}_1, \mathcal{N}_2) \leq ||\mu_1 - \mu_2||^2_2 + ||\Sigma_1^{1/2} - \Sigma_2^{1/2}||^2_{F}\;,\]
where $||(.)||_F$ represents the Frobenius norm of a matrix.
\end{theorem}

We refer the reader to the appendix, section (\ref{app:proof}) for the proof.

\textbf{Significance.} Deriving this bound allows us to extend the simplification for the 2-Wassertstein distance between two commutative covariance matrices to the more general scenario of non-commutative matrices. The simplification allows us to directly supervise the covariance without the use of eigendecomposition, making optimization inherently more stable.

\begin{minipage}{0.45\textwidth}
\begin{algorithm}[H]
\small
\DontPrintSemicolon
\KwInput{$\vx \in \mathbb{R}^M$: Given observation}
\KwInput{$\mathcal{X}$: All observations; $\mathcal{Y}$: All targets}
\vspace{0.2cm}
\KwOutput{$\widetilde{\Sigma}_{Y}(X)$: Covariance pseudo-label}
\vspace{0.2cm}

\tcp{Mahalanobis distance}
$\Sigma = \Cov(\mathcal{X})$ \\
\vspace{0.2cm}
\tcp{$d(\vx, \mathcal{X}, \Sigma)$.shape = \#samples}
$d_M(\vx, \mathcal{X}, \Sigma) = (\mathcal{X} - \vx) \Sigma^{-1} (\mathcal{X} - \vx)^T$ \\
\vspace{0.2cm}
\tcp{k = Nearest neighbours}
dist, idx = \texttt{bottom-k} ($d_M(\vx, \mathcal{X}, \Sigma)$, k) \\
\vspace{0.2cm}
\tcp{'Probabilistic' interpret}
dist = \texttt{softmax} (dist) \\
\tcp{$\vy$'s for nearest neighbours}
y\_nbr = $\mathcal{Y}$ [idx] \\
\vspace{0.2cm}
\tcp{'Expected' mean, covariance}
$\tilde{\mu}_{\vy}$ = (dist * mean)\texttt{.sum(dim=0)} \\
$\widetilde{\Sigma}_{Y}(X)$ = dist * $(\text{y\_nbr} - \tilde{\mu}_{\vy})(\text{y\_nbr} - \tilde{\mu}_{\vy})^T$

\vspace{0.2cm}
\tcp{Pseudo-label for given $\vx, \vy$}
\Return $\widetilde{\Sigma}_{Y}(\vx)$

\caption{\textbf{\textit{Covariance Pseudo-Label}}} \label{algo:pseudolabel}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{figs/PsuedoLabel/PseudoLabelR4.pdf}
\captionof{figure}{\textit{Pseudo-Label (Section \ref{sec:pseudolabel})} Given $\vx_0$, its pseudo-label is the variance in the targets $\vy$ corresponding to samples which are the nearest neighbors of $\vx_0$. Samples closer to $\vx_0$ are given more importance than samples further away.}
\label{fig:pseudolabel}
\end{minipage}

We return to Problem 1 and visually compare the 2-Wasserstein distance with variants of the log-likelihood (vanilla negative log-likelihood, Faithful \citep{stirn2023faithful}) and KL-Divergence. In Fig.~\ref{fig:bivariate_main}, we observe that in comparison to the likelihood based methods, the 2-Wasserstein is significantly more stable since the covariance does not depend on the residual and the convergence of the mean estimator. We also study the impact of warm-up \citep{sluijterman2024optimal}, where the mean estimator is allowed to converge before training the covariance estimator. To do so, we directly initialize the mean of the predicted distribution to the target mean, and the covariance to identity. However, our results in appendix/Fig.\ref{fig:bivariate} (b) (appendix) show that these methods are still susceptible to instability due to residuals. In contrast, the learning of the covariance in 2-Wasserstein does not depend upon the residual, leading to stable convergence.

\subsection{Generating Pseudo-Labels For The Covariance}
\label{sec:pseudolabel}

In the absence of labels for the covariance, existing approaches rely on the residual of the mean estimator as a signal to optimize the covariance. However, optimizing in this manner trades-off accuracy with computational complexity. While having labels would allow us to directly optimize the covariance estimator, obtaining annotations for the covariance is non-trivial. Therefore, we take a step in this direction and explore the possibility of self-supervision for the covariance. To this end, we propose a simple heuristic, which when combined with the 2-Wasserstein distance, is surprisingly effective in supervising the covariance.

\textbf{Intuition.} The neighborhood of a sample has been widely used in uncertainty quantification \citep{van2020uncertainty, skafte2019reliable} and kernel methods \citep{hofmann2008kernel}. The key idea is to infer properties of a sample based on its neighborhood.  TIC \citep{pmlr-v235-shukla24a} learns the covariance through a learnt $\epsilon-$neighborhood of the input, $\Cov( \hat{Y} | X  + \epsilon)$. We extend upon this idea to obtain pseudo-labels for the covariance. Specifically, we use two concepts:
\begin{enumerate}
    \item The target $\vy$ has a high (co-)variance if it exhibits large variations in a small vicinity of $\vx$. 
    \item The closer $\vx_j$ is to $\vx_i$, the likelier it is that $\vy_j$ is a potential label for $\vx_i$. 
\end{enumerate}
We quantify these concepts through the use of (a) the Mahalanobis distance, to measure the degree of closeness between samples $\vx_i$ and $\vx_j$; and (b) a probabilistic interpretation of this distance to weight different targets $\vy_j$ as being a potential label for $\vx_i$.

The Mahalanobis distance between two points $\bf{u}, \bf{v}$ with respect to a covariance matrix $\Sigma$ is
\begin{equation}
    d_M(\bf{u}, \bf{v}; \Sigma) := \sqrt{(\bf{u} - \bf{v})^T \Sigma^{-1} (\bf{u} - \bf{v})}\,.
\end{equation}
Unlike the Euclidean distance, the Mahalanobis distance accounts for the spread of the samples. This is crucial since not only does the distance scale according to the alignment of $\bf{u}, \bf{v}$ \textit{w.r.t.} the covariance, but it also scales based on the spatial extent of the samples.

Therefore, we define $\Sigma = \Cov(X)$ to quantify the alignment and scale of all the samples $X$. We compute the pairwise Mahalanobis distance between the given sample and all other samples, choosing the \textit{top-k} nearest neighbors and their associated distances. This also includes the given sample itself. Next, we compute the softmax over these distances, giving them a probabilistic interpretation: the closer the sample, the higher the likeliness of it being the true mean. The pseudo-label covariance uses this probabilistic interpretation to compute the expected mean and covariance over the neighboring targets. A concise description of these steps is available in Algorithm \ref{algo:pseudolabel}. 

\textbf{Self-Supervised Training}. We use the pseudo-labels $\widetilde{\Sigma}_{Y}(\vx)$ in conjunction with the input $\vx$ and target $\vy$ to train the mean and covariance estimators simultaneously using the 2-Wasserstein bound.

\section{Experiments}

How effective is self-supervision in deep heteroscedastic regression? We study this question through a series of synthetic and real world datasets for regression. We use the same setup as \citet{pmlr-v235-shukla24a} which experiments on univariate sinusoidals, synthetic multivariate data, UCI Machine Learning repository \citep{uci} and 2D human pose estimation \citep{mpii, lsp, lspet}.  We provide a detailed description of the experimental setup and implementation details in the appendix (\ref{app:details}). For all our experiments, we set the nearest neighbors hyperparameter in the pseudo-label algorithm to ten times the dimensionality of the target. We compare our approach with popular and state-of-the-art methods such as the vanilla negative log-likelihood, $\beta-$NLL \citep{seitzer2022on}, Faithful heteroscedastic regression \citep{stirn2023faithful}, Empirical-Bayes \citep{immer2023effective} and the Taylor Induced Covariance parameterization (NLL:TIC) \citep{pmlr-v235-shukla24a}. In addition to the mean square error and the negative log-likelihood, we use the KL-Divergence and the 2-Wasserstein distance as metrics when the ground truth covariance is known. We also use the Task Agnostic Correlations (TAC) metric introduced in \citep{pmlr-v235-shukla24a} to evaluate the covariance through its learnt correlations. Finally, we also report the additional memory consumed and the time required for each method to run for different experiments.

\subsection{Synthetic Data}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/Univariate/highres_univariate_10.pdf}
    \caption{We sample from the ground truth sinusoidal  $y = |x| \textrm{ sin } (2 \pi x)$ with $\sigma(x) =  |x|$ and train our networks using different objectives. The 2-Wasserstein distance trained using pseudo-labels is able to converge to the accurate mean and variance faster since it does not depend upon residuals or convergence of the mean estimator to learn the variance.}
    \label{fig:univariate10}
\end{figure}

\textbf{Univariate.} We use samples from different varying amplitude sinusoidals to compare the methods. In Fig. \ref{fig:univariate10}, we observe that faithful heteroscedastic regression overestimates the covariance because of the lack of synergy between the mean and variance estimator. While the mean estimator assumes homoscedastic unit variance, the variance estimator models heteroscedasticity. Although the TIC formulation stabilizes convergence, unfortunately, convergence itself is slow. The KL Divergence and vanilla negative log-likelihood suffer from large residuals which prevents further optimisation. In comparison, the 2-Wasserstein distance combined with the pseudo-labels is able to converge faster while being accurate. An additional study comparing the methods on different variations of the sinusoidal is presented in the appendix (Fig: \ref{fig:univariate_appendix}).

\begin{figure}
    \centering
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim8/MSE.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim8/2-W.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim8/KL.pdf}
    \end{subfigure}
    %\vspace*{-25mm}
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim24/MSE.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim24/2-W.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim24/KL.pdf}
    \end{subfigure}
    \caption{\textit{(Multivariate: Metrics.)} We simulate multivariate data with heteroscedastic covariance of increasing dimensionality (top row: 8, bottom row: 24). We observe that modeling heteroscedasticity is challenging without annotations, with some popular approaches diverging away from the true distribution. Our results highlight the potential of self-supervision for improved convergence.}
    \label{fig:multivariate}
\end{figure}

\begin{table}
\caption{\textit{(Multivariate: Computational Costs)}. While TIC is able to accurately model the covariance in comparison to other likelihood based approaches, it has a significantly increased computational cost. The 2-Wasserstein (bound) has a significantly lower cost without sacrificing accuracy.}
\centering
\renewcommand{\arraystretch}{1.0}% Tighter
\begin{subtable}[t]{\textwidth}
    \small
    \centering
    \caption{Compute time (in milliseconds)}
    \resizebox{0.9\linewidth}{!}{%
    \begin{tabular}{ccccccccc}
    \toprule
     Dimensions $\rightarrow$ & 4 & 8 & 12 & 16 & 20 & 24 & 28 & 32 \\
    \midrule
    \rowcolor{Gray}Beta-NLL, Diagonal  & 2.88 & 3.15 & 2.17 & 2.06 & 1.83 & 1.74 & 2.00 & 2.04 \\
     Faithful, NLL          & 4.56 & 4.74 & 3.94 & 3.69 & 3.76 & 3.66 & 4.08 & 4.85 \\
    \rowcolor{Gray}NLL: TIC          & 56.60 & 56.81 & 59.28 & 95.54 & 197.58 & 448.58 & 943.79 & 1961.08 \\
     KL-Divergence     & 4.79 & 5.06 & 4.05 & 4.05 & 4.10 & 3.94 & 4.81 & 5.24 \\
    \rowcolor{Gray}2-Wasserstein     & 5.10 & 5.43 & 4.47 & 4.38 & 4.31 & 4.14 & 4.88 & 5.20 \\
     2-Wasserstein (Bound) & 4.59 & 4.79 & 3.72 & 3.73 & 3.64 & 3.56 & 3.91 & 4.72 \\
    \bottomrule
    \vspace{1mm}
\end{tabular}
    }
    \label{subtab:first1}
\end{subtable}
\begin{subtable}[t]{\textwidth}
\small
    \centering
    \caption{Compute memory (in megabytes)}
    \resizebox{0.9\linewidth}{!}{%
    \begin{tabular}{ccccccccc}
    \toprule
    Dimensions $\rightarrow$ & 4 & 8 & 12 & 16 & 20 & 24 & 28 & 32 \\
    \midrule
     \rowcolor{Gray}NLL: TIC          & 11.68 & 120.84 & 523.22 & 1543.77 & 3625.51 & 7333.84 & 13313.31 & 22398.00 \\
    \begin{tabular}{@{}c@{}}2-Wasserstein (Bound) \\{\small \textcolor{gray}{+6 other methods}}\end{tabular} & 3.45 & 8.24 & 17.10 & 29.51 & 54.51 & 111.73 & 201.51 & 339.55 \\
    \bottomrule
    \vspace{1mm}
\end{tabular}
    }
    \label{subtab:second1}
\end{subtable}
\label{tab:mv}
\end{table}

\textbf{Multivariate}. Unlike in our real world experiments, synthetic datasets allow us to define the ground truth covariance to evaluate different approaches. We use the same setup as previous work, which defines the multivariate target $\mathcal{N}(\mu_{Y|X}, \Sigma_{Y|X} + \Sigma_{Z|X})$ as a function of the input with heteroscedastic variance. $X$ and $Y$ are jointly distributed following the normal distribution, with $Z$ being a variable conditionally independent of $Y$ and a function of $X$. In addition to evaluating different methods through optimization metrics, we also compare them through their computational requirements (memory and time). We vary the dimensionality of our targets ranging from 4 to 32, and report our results in Fig. \ref{fig:multivariate}, \ref{fig:multivariate_appendix} (appendix), and Table \ref{tab:mv}. We observe that while TIC facilitated improved covariance estimation, this resulted in slower convergence of the mean estimator. This trend is evident as the dimensionality increases. Moreover, TIC requires significantly more computational resources. In contrast, the 2-Wasserstein bound is significantly cheaper to compute while maintaining accurate convergence of both, the mean and covariance estimator.

\subsection{Real datasets}

\begin{table}
\caption{\textit{UCI Regression}. The 2-Wasserstein distance using pseudo-labels for supervision accurately estimates the mean and covariance while having low compute requirements. In contrast, the negative log-likelihood and KL Divergence sub-optimally converge due to large residual errors. While methods such as Faithful encourage convergence by using the mean squared error, the mean and covariance inconsistently model the residual leading to sub-optimal covariance estimates. While TIC accurately models the covariance, it has high computational costs and lags in mean estimation.}
\centering
\renewcommand{\arraystretch}{1.0}% Tighter
\begin{subtable}[t]{\textwidth}
    \centering
    \caption{Mean Square Error (MSE)}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccccccccccc}
    \toprule
    Method & Abalone & Air & Appliances & Concrete & Electrical & Energy & Gas & Naval & Parkinson & Power & Red Wine & White Wine \\
    \midrule
     \rowcolor{Gray}NLL                  & 3.74 & 17.92 & 53.49 & 4.57 & 9.28 & 4.20 & 10.98 & 10.34 & 54.51 & 9.09 & 8.94 & 9.40 \\
    KL-Divergence        & 1.90 & 14.70 & 90.90 & 3.84 & 15.57 & 4.20 & 10.16 & 12.39 & 59.39 & 9.97 & 7.26 & 8.17 \\
     \rowcolor{Gray}Beta-NLL             & 0.35 & 1.58  & 3.69  & 2.02 & 3.62  & 1.87 & 1.50  & 0.72  & 8.11  & 3.06 & 2.15 & 3.43 \\
    NLL: Diagonal        & 1.32 & 8.90  & 37.91 & 4.28 & 6.58  & 3.99 & 5.73  & 9.60  & 27.35 & 6.52 & 5.75 & 6.01 \\
     \rowcolor{Gray}Faithful             & \textbf{0.16} & \textbf{0.33}  & \textbf{0.20}  & \textbf{0.72} & \textbf{0.89}  & \textbf{0.41} & \textbf{0.45} & \textbf{0.06} & \textbf{0.29}  & \textbf{0.61} & \textbf{0.70} & \textbf{0.78} \\
    NLL: TIC             & 0.21 & 0.82  & 4.45  & 0.96 & \textbf{0.91}  & 0.61 & 0.67  & 1.36  & 8.89  & \textbf{0.66} & 0.97 & 0.92 \\
    \midrule
    \textbf{2-W (Bound)} & \textbf{0.16} & \textbf{0.34}  & \textbf{0.20}  & \textbf{0.72} & \textbf{0.90}  & \textbf{0.41} & \textbf{0.45} & \textbf{0.07} & \textbf{0.30}  & \textbf{0.61} & \textbf{0.71} & \textbf{0.79} \\
    \bottomrule
\end{tabular}
}
\label{tab:mse-short}
\end{subtable}
\begin{subtable}[t]{\textwidth}
    \centering
    \caption{Negative Log-Likelihood (NLL)}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccccccccccc}
    \toprule
    Method & Abalone & Air & Appliances & Concrete & Electrical & Energy & Gas & Naval & Parkinson & Power & Red Wine & White Wine \\
    \midrule
    \rowcolor{Gray} NLL                  & 35.89 & 56.98 & 245.99 & 28.50 & 63.15 & 29.85 & 41.03 & 38.18 & 262.59 & 49.37 & 46.58 & 58.06 \\
    KL-Divergence        & 18.27 & 83.18 & 413.96 & 38.23 & 73.87 & 28.50 & 34.38 & 49.24 & 257.36 & 45.49 & 61.96 & 48.66 \\
    \rowcolor{Gray} Beta-NLL             & 9.80  & 29.38 & 60.30  & 20.45 & 35.44 & 20.15 & 20.26 & 20.81 & 59.98  & 27.64 & 34.05 & 30.95 \\
    NLL: Diagonal        & 18.61 & 80.86 & 369.67 & 46.82 & 65.73 & 36.09 & 47.06 & 77.47 & 238.71 & 51.60 & 77.98 & 67.21 \\
     \rowcolor{Gray}Faithful             & 11.86 & 33.31  & 65.15  & 17.42 & 34.73  & 19.41 & 22.47 & 27.70 & 57.04  & 24.08 & 24.34 & 26.01 \\
    NLL: TIC             & \textbf{4.71}  & 16.46 & 30.41  & 11.36 & \textbf{14.97} & 12.06 & \textbf{9.96}  & 14.99 & 42.52  & \textbf{9.31}  & 14.66 & \textbf{12.33} \\
    \midrule
    \textbf{2-W (Bound)} & 6.32 & \textbf{13.58}  & \textbf{22.72}  & \textbf{8.96} & \textbf{15.57}  & \textbf{8.85} & \textbf{10.49} & \textbf{11.44} & \textbf{21.48}  & 11.31 & \textbf{11.65} & \textbf{12.12} \\
    \bottomrule
\end{tabular}
\label{tab:nll-short}
}
\end{subtable}
\begin{subtable}[t]{\textwidth}
    \centering
    \caption{Compute time (in milliseconds)}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccccccccccc}
    \toprule
    Method & Abalone & Air & Appliances & Concrete & Electrical & Energy & Gas & Naval & Parkinson & Power & Red Wine & White Wine \\
    \midrule
     \rowcolor{Gray}NLL                  & 5.28   & 5.45  & 5.85  & 5.70  & 7.53  & 5.84  & 5.23  & 6.44  & 7.31  & 5.53  & 5.95  & 5.88  \\
    Beta-NLL             & 4.71   & 4.58  & 4.98  & 4.35  & 5.20  & 4.41  & 4.62  & 5.81  & 6.70  & 4.81  & 6.83  & 4.41  \\
     \rowcolor{Gray}Faithful             & 5.28   & 5.35  & 5.62  & 5.73  & 6.02  & 5.03  & 5.03  & 7.02  & 6.47  & 5.81  & 7.40  & 5.05  \\
    NLL: Diagonal        & 4.50   & 4.49  & 4.84  & 4.67  & 5.13  & 4.32  & 4.38  & 5.84  & 4.98  & 4.61  & 5.77  & 4.36  \\
     \rowcolor{Gray}NLL: TIC             & 45.61  & 53.22 & 68.55 & 47.46 & 49.37 & 47.57 & 45.09 & 56.04 & 59.25 & 49.74 & 65.25 & 45.23 \\
    KL-Divergence        & 5.30   & 6.65  & 6.08  & 5.47  & 8.09  & 5.16  & 5.14  & 6.85  & 9.15  & 5.36  & 6.93  & 5.23  \\
    \midrule
    2-W (Bound) & 4.51   & 5.83  & 5.17  & 4.51  & 5.36  & 4.50  & 4.38  & 5.23  & 7.16  & 4.51  & 5.28  & 4.48  \\
    \bottomrule
\end{tabular}
    }
    \label{subtab:first2}
\end{subtable}
\begin{subtable}[t]{\textwidth}
    \centering
    \caption{Compute memory (in megabytes)}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{ccccccccccccc}
    \toprule
    Method & Abalone & Air & Appliances & Concrete & Electrical & Energy & Gas & Naval & Parkinson & Power & Red Wine & White Wine \\
    \midrule
     \rowcolor{Gray}NLL: TIC             & 11.22  & 90.20 & 820.85 & 1.09   & 71.35 & 24.13 & 33.99 & 108.59 & 637.74  & 41.57 & 40.94 & 41.57 \\
    \begin{tabular}{@{}c@{}}2-W (Bound) \\{\small \textcolor{gray}{+6 other methods}}\end{tabular} & 3.10   & 9.02  & 25.30  & 1.09   & 7.75  & 4.63  & 5.56  & 9.02   & 23.05   & 5.56  & 5.56  & 5.56  \\
    \bottomrule
\end{tabular}
    }
    \label{subtab:second2}
\end{subtable}
\label{tab:uciregress}
\end{table}


\textbf{UCI Regression.} We evaluate mean and covariance predictions by performing the same study as \citet{pmlr-v235-shukla24a} on regression datasets from the UCI Machine Learning repository. We standardize each dataset to have zero mean and unit variance. We randomly choose 25\% of the features as observations and the remaining 75\% as the targets, adding considerable heteroscedasticity in the data. We conduct five trials and report the mean, highlighting top-performing methods which are statistically indistinguishable. We evaluate different methods not only using performance on various metrics (Table \ref{tab:uciregress}, appendix/Table \ref{tab:uci_all}) but also through computational costs. While TIC outperforms other likelihood based baselines significantly in our TAC and NLL evaluation, this comes at the cost of significantly higher computational requirements. Although compute efficient methods such as Faithful leverage the mean squared error to accurately converge to the mean, it does not accurately converge on the optimal covariance. In contrast, the 2-Wasserstein bound accurately converges in both, the mean and covariance without additional computation overhead. Moreover, the vanilla 2-Wasserstein formulation exhibited significant training instabilities on certain datasets such as \textit{superconductivity}, motivating the use of the proposed bound which is stable to train.

We also study the impact of warmup \citep{sluijterman2024optimal} in the training process, where we train the mean estimator for half the number of epochs, and jointly train the mean and covariance for the remaining half. We share our results in appendix/Fig. \ref{fig:uciwarmup_appendix}. Noticeably, the training diverges due to the effect of residuals coupled with incorrect covariance estimates, effectively nullifying the use of warm-up.

\textbf{Human Pose Estimation}. We perform experiments on 2D human pose estimation using the same setup as previous work. We use the ViTPose \citep{xu2022vitpose} architecture as our base model, which is a popular vision transformer model for human pose estimation. Since we are introducing the covariance in the training process, this also requires us to modify the covariance in response to image and keypoint augmentations. Popular augmentations use affine transformations, which linearly transform the keypoints. Let $\tilde{Y} = R Y$ represent the transformed keypoints using the matrix $R$. The new covariance underlying $\tilde{Y}$ is $\tilde{\Sigma}_{Y}(X) = R  \,\widehat{\Sigma}_{Y}(X) \,R^T$.




We highlight this experiment not as one where self-supervision is computationally efficient, but one which motivates the use of hybrid training to further improve the state-of-the-art. Our preliminary experiments using self-supervision fell short of matching \citet{pmlr-v235-shukla24a}, which we believe is because the pseudo-labels, computed based on a low-dimensional representation of the input images, may not necessarily be accurate. However, we identify an alternate paradigm: pre-training using self-supervision and then switching to NLL:TIC helped retain the advantages across both, the 2-Wasserstein bound and NLL:TIC.

We show our results in Fig. \ref{fig:humanpose} and Fig. \ref{fig:humanpose_appendix}. We use the TIC parameterization and train our models using the self-supervision for the first 20 epochs. After this, we switch to the negative log-likelihood which provides more freedom to explore the optimal covariance. Our results show that the hybrid approach outperforms its individual components and retains both: a low mean square error and low negative log-likelihood.

\begin{figure}
    \centering
    \begin{subfigure}{0.328\textwidth}
        \includegraphics[width=\textwidth]{figs/HumanPose/lr1e-3/MSE.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.328\textwidth}
        \includegraphics[width=\textwidth]{figs/HumanPose/lr1e-3/NLL.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.328\textwidth}
        \includegraphics[width=\textwidth]{figs/HumanPose/lr1e-3/TAC.png}
    \end{subfigure}
    \caption{\textit{(Human Pose: Improving state-of-the-art heteroscedastic pose estimation)} We explore a hybrid training strategy by combining the 2-Wasserstein bound with the negative log-likelihood. We train ViTPose for the first 20 epochs using the bound and then switch to negative log-likelihood. We observe that the hybrid approach retains best of both the worlds: improved mean and covariance estimates, as measured by the mean square error and the log-likelihood. \textit{(Different learning rates are explored in Fig. \ref{fig:humanpose_appendix})}}
    \label{fig:humanpose}
\end{figure}

\section{Conclusion}

We study deep heteroscedastic regression, noting the optimization challenges present due to the lack of annotations for the covariance. Therefore, we study methods for self-supervision which requires us to define (1) a framework for supervision, and (2) a method to obtain pseudo-labels for the covariance. We critically study the KL-Divergence, highlighting the need for calibration and noting its susceptibility to residuals. Next, we study the 2-Wasserstein distance, proposing a bound on the latter that is stable to optimize. Finally, we propose a simple neighborhood based heuristic which is effective in providing pseudo-labels for the covariance. Our experiments show that, unlike existing approaches, the use of the 2-Wasserstein bound and pseudo-labels yields accurate mean and covariance estimation while remaining computationally inexpensive.  Our experiments on human pose show the potential for a hybrid approach, where combining the 2-Wasserstein and NLL frameworks enables superior performance compared to using either method alone.

\section*{Reproducibility Statement}

We make our code available on \href{https://deep-regression.github.io}{\texttt{https://deep-regression.github.io}}. The code comes complete with a docker image and documentation for reproducibility. We have taken sufficient care to perform multiple trials and report the mean and standard deviation. 

\section*{Acknowledgement}

We thank the reviewers for their valuable comments, questions as well as participation in the rebuttal period. We also thank \href{https://people.epfl.ch/reyhaneh.hosseininejad}{Reyhaneh Hosseininejad} for her help in preparing the paper and for having insighful discussions. Finally, we are grateful to RCP, EPFL for their support in compute.

This research is funded by the Swiss National Science Foundation (SNSF) through the project \textit{Narratives from the Long Tail: Transforming Access to Audiovisual Archives} (Grant: CRSII5\_198632). The project description is available on: \url{https://www.futurecinema.live/project/} 

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\section{Appendix}

\subsection{Proof of Theorem \ref{thm:2-wasserstein}}
\label{app:proof}

\begin{proof}
We begin with the definition of the 2-Wasserstein distance (Definition \ref{eq:2-w}). First, we focus on rewriting $\Sigma_1 + \Sigma_2$ by adding (and subtracting) new terms to get
\begin{equation*}
    \Sigma_1 + \Sigma_2 - \Sigma_1^{1/2}\Sigma_2^{1/2} - \Sigma_2^{1/2}\Sigma_1^{1/2} + \Sigma_1^{1/2}\Sigma_2^{1/2} + \Sigma_2^{1/2}\Sigma_1^{1/2} \,
\end{equation*}
The advantage of introducing new terms is to write $\Sigma_1 + \Sigma_2$ as 
\begin{align*}
    \Sigma_1 + \Sigma_2 = (\Sigma_1^{1/2} - \Sigma_2^{1/2}) (\Sigma_1^{1/2} - \Sigma_2^{1/2})^T + \Sigma_1^{1/2}\Sigma_2^{1/2} + \Sigma_2^{1/2}\Sigma_1^{1/2} \,.
\end{align*}
Substituting this in the definition of the 2-Wasserstein distance, we get 
\begin{multline}
    \mathcal{W}_2(\mathcal{N}_1, \mathcal{N}_2) =
     ||\mu_1 - \mu_2||^2 \,+\, \textrm{Tr}\, \bigg[ \,(\Sigma_1^{1/2} - \Sigma_2^{1/2}) (\Sigma_1^{1/2} - \Sigma_2^{1/2})^T + \Sigma_1^{1/2}\Sigma_2^{1/2} \\ + \Sigma_2^{1/2}\Sigma_1^{1/2} - 2(\Sigma_2^{1/2} \Sigma_1 \Sigma_2^{1/2})^{1/2} \bigg] \label{eq:2-w_intermediate}
\end{multline}

We proceed by noting that the Trace operator is linear; implying $\textrm{Tr}(A + B) = \textrm{Tr}(A) + \textrm{Tr}(B)$, allowing us to analyse the terms separately. Next, we note that the Frobenius norm of a matrix is related to its trace by: $||A||_F^2 = \textrm{Tr}(AA^T)$. Therefore, 
\begin{equation}
    \label{eq:frobenius}
    \textrm{Tr}\, \big[ \,(\Sigma_1^{1/2} - \Sigma_2^{1/2}) (\Sigma_1^{1/2} - \Sigma_2^{1/2})^T \big] = ||\Sigma_1^{1/2} - \Sigma_2^{1/2}||_F^2 \,.
\end{equation}
Since the Trace operator is cyclic; implying $\textrm{Tr}(AB) = \textrm{Tr}(BA)$, we have 
\begin{equation}
    \label{eq:cyclic}
    \textrm{Tr}(\Sigma_1^{1/2}\Sigma_2^{1/2} + \Sigma_2^{1/2}\Sigma_1^{1/2}) = 2 \textrm{Tr}(\Sigma_1^{1/2}\Sigma_2^{1/2})
\end{equation}
Substituting Eqs. \ref{eq:frobenius} and \ref{eq:cyclic} into Eq. \ref{eq:2-w_intermediate}, we have 
\begin{align}
    \label{eq:2-w_intermediate-2}
     \mathcal{W}_2(\mathcal{N}_1, \mathcal{N}_2) = ||\mu_1 - \mu_2||^2 \,+\,||\Sigma_1^{1/2} - \Sigma_2^{1/2}||_F^2 + 2 \textrm{Tr}\, \bigg[ \Sigma_1^{1/2}\Sigma_2^{1/2} - (\Sigma_2^{1/2} \Sigma_1 \Sigma_2^{1/2})^{1/2} \bigg]
\end{align}

\noindent
We note that in the trivial case where $\Sigma_1$ and $\Sigma_2$ are commutative, the trace is reduced to zero. However, what happens in the general case when the covariance matrices are not commutative? We address this through proposition \ref{prop:trace_inequality}, which shows that  \[\textrm{Tr}\big[ (\Sigma_2^{1/2}\Sigma_1 \Sigma_2^{1/2})^{1/2} \big] \geq \textrm{Tr}(\Sigma_2^{1/2}\Sigma_1^{1/2}) \,.\] Therefore, on substitution the trace terms cancel out, leading to a familiar expression wrapped in an inequality: 
\begin{equation}
\label{eq:final}
    \mathcal{W}_2(\mathcal{N}_1, \mathcal{N}_2) \leq ||\mu_1 - \mu_2||^2 \,+\,||\Sigma_1^{1/2} - \Sigma_2^{1/2}||_F^2 
\end{equation}
\end{proof}

\begin{proposition}
\label{prop:trace_inequality}
    Let $A, B$ be any two positive definite matrices not necessarily commutative. Then, 
    \[\textrm{Tr}\big[ (A^{1/2}BA^{1/2})^{1/2} \big] \geq \textrm{Tr}(A^{1/2}B^{1/2})\]
\end{proposition}

\begin{proof}
Let $X = A^{1/2}B^{1/2}$, consequently we need to prove that $\textrm{Tr}\big[ (XX^T)^{1/2} \big] \geq \textrm{Tr}(X)$. Let $X=PU$ be the polar decomposition of $X$ where $P$ is a positive definite matrix and $U$ is a orthogonal matrix since $X$ is a real matrix. On substitution, we have $\textrm{Tr}\big[ ((PU)(PU)^T)^{1/2} \big] \geq \textrm{Tr}(PU)$. Since P by definition is symmetric and U is orthogonal, we need to show that $\textrm{Tr}\big[ P \big] \geq \textrm{Tr}(PU)$. \\

\noindent
Let $P = Q \Lambda Q^T$ be the eigendecomposition of P where $Q$ is the orthonormal basis and $\Lambda$ is the diagonal matrix consisting of the eigenvalues $\lambda_i$. Since the trace is the sum of eigenvalues of a matrix, we need to show that $\sum_i \lambda_i \geq \textrm{Tr}(Q  \Lambda Q^TU)$. Since the trace is cyclic, we have $\sum_i \lambda_i \geq \textrm{Tr}(\Lambda Q^TUQ) = \textrm{Tr}(\Lambda \mathcal{K})$, where $\mathcal{K} = Q^TUQ$ is an orthogonal matrix by virtue of being a product of orthogonal matrices. Since $\Lambda$ is a diagonal matrix, $\textrm{Tr}(\Lambda \mathcal{K}) = \sum_i \lambda_i \mathcal{K}_{ii}$. Moreover, as $\mathcal{K}$ is an orthogonal matrix, $\mathcal{K}_{ii} <= 1$. As a consequence, $\sum_i \lambda_i \geq \sum_i \lambda_i \mathcal{K}_{ii}$, concluding our proof. 
\end{proof}

\begin{figure}[!t]
%\hspace{0.35cm}
\includegraphics[width=\textwidth]{figs/kl/Appendix.pdf}
\caption{\textit{Residuals and sub-optimal convergence (Section: \ref{sec:KL})}. The residual can be treated as a vector which approximately points along the line segment joining the predicted mean and the target mean. However, if the residual is large, we observe that it influences the predicted covariance significantly. Consequently, the inverse covariance is aligned orthogonal to the residual vector. Since the gradient of the mean estimator is directly proportional to the inverse (Eq. \ref{eq:kl_mvn_mean}), the gradient as a whole is desensitised to move towards the target. In fact, the inverse magnifies gradient updates in the direction orthogonal to the residual vector, potentially leading to oscillations.}
\label{fig:nll_kl_intuition}
\end{figure}
\subsection{Proof of Lemma \ref{lemma:kl}}
\label{app:lemma}

\begin{proof}
The optimal solution for $\widehat{\Sigma}_{Y}(X)$ involves minimizing \[ \sum_{i=1}^N D_{\text{KL}} (\mathcal{N}(\vy_i, \Sigma_Y^{\text{(prior)}}(X)) \,||\, \mathcal{N}(\widehat{\mu}_{Y}(X), \widehat{\Sigma}_{Y}(X))) .\] Using the definition of the KL Divergence (Eq. \ref{eq:kl_def}) and dropping the non-parametric terms, we get
\begin{equation}
\resizebox{0.9\linewidth}{!}{$
    \dfrac{1}{N} \sum_{i=1}^N \frac{1}{2} \left[ \text{Tr}(\widehat{\Sigma}_{Y}^{-1}(X) \Sigma_Y^{\text{(prior)}}(X)) + (\widehat{\mu}_{Y}(X) - \vy_i)^\top \widehat{\Sigma}_{Y}^{-1}(X) (\widehat{\mu}_{Y}(X) - \vy_i) - \ln | \widehat{\Sigma}_{Y}^{-1}(X) | \right]
\label{eq:kl_mvn}
$}
\end{equation}
Setting the derivative \textit{w.r.t} the predicted mean $\widehat{\mu}_{Y}(X)$ to 0, we get
\begin{align}
    \label{eq:kl_mvn_mean}
    &\frac{1}{N} \sum_{i=1}^N \widehat{\Sigma}_{Y}^{-1}(X) (\widehat{\mu}_{Y}(X) - \vy_i) = 0  \\
    &\widehat{\mu}_{Y}(X) = \frac{1}{N} \sum_{i=1}^N \vy_i
\end{align}
This is indeed the same solution as minimizing the negative log-likelihood, and therefore $\widehat{\mu}_{Y}(X)$ predicts the correct mean. However, setting the derivative \textit{w.r.t} the predicted precision $\widehat{\Sigma}_{Y}^{-1}(X)$ gives us three terms of the form (1) $\text{Tr}(AB)$, (2) $b^TAb$ and (3) $\ln |A|$, where A is the shorthand for precision and B represented different terms. The derivative of the form $\text{Tr}(AB)$ \textit{w.r.t} $A$ is $B^T$ (Eq. 100 in \citep{IMM2012-03274}). Here, $B$ is the prior term $\Sigma_Y^{\text{(prior)}}(X)$. Since the prior is symmetric, the derivative of term 1 is $\Sigma_Y^{\text{(prior)}}(X)$. The derivative of the form $b^TAb$ is $bb^T$ (Eq. 72 in \citep{IMM2012-03274}). Therefore, the derivative of term 2 is $(\widehat{\mu}_{Y}(X) - \vy_i)(\widehat{\mu}_{Y}(X) - \vy_i)^T$. Finally, the derivative of the form $\ln |A|$ is $(A^{-1})^T$ (Eq. 57 in \citep{IMM2012-03274}). Therefore, the derivate of term 3 is $\widehat{\Sigma}_{Y}(X)$. By combining the three terms, the derivative of Eq. \ref{eq:kl_mvn} is
\begin{align}
    \label{eq:kl_mvn_derivative}
    &\frac{1}{N} \sum_{i=1}^N \left[ \Sigma_Y^{\text{(prior)}}(X) + (\widehat{\mu}_{Y}(X) - \vy_i)(\widehat{\mu}_{Y}(X) - \vy_i)^T - \widehat{\Sigma}_{Y}(X) \right] = 0 \nonumber \\
    &\widehat{\Sigma}_{Y}(X) = \Sigma_Y^{\text{(prior)}}(X) + \frac{1}{N} \sum_{i=1}^N (\widehat{\mu}_{Y}(X) - \vy_i)(\widehat{\mu}_{Y}(X) - \vy_i)^T \\
    &\widehat{\Sigma}_{Y}(X) \approx \Sigma_Y^{\text{(prior)}}(X) + \Sigma_Y(X)
\end{align}
\end{proof}
Note: In comparison, the optimal value of the covariance using the negative log-likelihood is $\widehat{\Sigma}_{Y}(X) = \frac{1}{N} \sum_{i=1}^N (\widehat{\mu}_{Y}(X) - \vy_i)(\widehat{\mu}_{Y}(X) - \vy_i)^T$

\section{Experiment Details }
\label{app:details}

\textbf{Training.} We use separate networks to estimate the mean and covariance, with no overlapping parameters, following the results of \citet{stirn2023faithful}. This is also advocated for by \citet{sluijterman2024optimal}.  The architectures of these network are described in the different subsections. The mean and covariance have the same architecture with the exception of the final layer. We use the pseudo-labels $\widetilde{\Sigma}_{Y}(\vx)$ together with the input $\vx$ and target $\vy$ to train the mean and covariance estimators simultaneously using the 2-Wasserstein bound. Hence, instead of having training pairs $(\vx, \vy)$, we have triplets $(\vx, \vy, \widetilde{\Sigma}_{Y}(\vx))$. Unless specified, we do not use warm-up in our experiments. Moreover, the bound does not require warm-up since the mean and covariance estimator training is decoupled. All the methods are trained using the AdamW optimizer, which implicitly imposes a weight decay of 0.01 on the parameters. We ensure a fair comparison by randomly initializing all methods with the same mean and covariance estimators, and each method uses its own learning rate scheduler. Additionally, the batching and sample ordering are the exact same across all methods. At training time, we sample a batch which is simultaneously used by all the baselines for optimization.

\subsection{Synthetic Data}

\textbf{Univariate.}
We draw 50,000 samples for each of the three different sinusoidal distributions: (1) $y = |x| \textrm{ sin } (2 \pi x)$ (2) $y = (5 - |x|) \textrm{ sin } (2 \pi x)$ (3) $y = 5 \textrm{ sin } (2 \pi x)$, all of them with heteroscedastic noise $\sigma(x) =  |x|$ (2). We train a fully connected feed-forward neural network with batch normalization \citep{ioffe} and \textit{tanh()} activation to learn the mean and variance. Specifically, we use four hidden layers with a latent dimension of fifty. Every alternate layer is followed by the batch normalization layers. The full results are shown in Fig. \ref{fig:univariate_appendix}.


\textbf{Multivariate.} The dimensionality of the input and target, $\vx$ and $\vq$ is varied from 4 to 32 in steps of 4, and the mean and standard deviation are reported over ten trials for each dimension. Depending on the dimensionality, between 4000 and 20000 samples are drawn. Similar to our univariate setup, we train a fully connected feed-forward neural network with batch normalization but \textit{ELU()} activation to learn the mean and covariance. Specifically, we use ten hidden layers with a latent dimension of that is the dimensionality of the input squared. The idea is that the size of the network increases as the dimensionality of the network increases to account for increasing complexity. Every alternate layer is followed by the batch normalization layers. The full results are shown in Figure \ref{fig:multivariate_appendix}. We report the computational requirements in Table \ref{tab:mv}.

 \subsection{UCI Regression}
 We follow the experimental setup in \citet{pmlr-v235-shukla24a}. For each of the twelve datasets, 25\% of the features are randomly selected as inputs, with the remaining 75\% used as multivariate targets during run-time. Although some of the resulting input-target pairings may yield sub-optimal performance in prediction, this presents a valuable test for the covariance estimator, which needs to identify correlations even in challenging scenarios. Moreover, the random assignment of features guarantees that our experiments are unbiased, as the selection process is not manipulated. All datasets are standardized to a mean of zero and a variance of one. We reuse our neural network architecture from the multivariate experiments. We perform five trials for each dataset and report the mean and standard deviation in Table \ref{tab:uci_all}. 

\subsection{2D Human Pose Estimation}

ViTPose \citep{xu2022vitpose} is a recent state-of-the-art model that adapts vision transformers \citep{dosovitskiy2021an} for the task of human pose estimation. We use the base version (ViTPose-B) for our task Additionally, we use soft-argmax \citep{li2021hybrik, li2021localization} which is applied to reduce the heatmap, initially a tensor of shape $N \times 64 \times 64$, to a 1D vector of length $2N$, where $N$ is the number of joints in the human pose. To obtain the input for the covariance estimator, we use residual connections which involves downscaling and upscaling of the 1-D features predicted by the backbone network. The output of the downscaling is used to predict the covariance. We perform our experiments on the MPII \citep{mpii} and LSP/LSPET \citep{lsp, lspet} datasets, with the latter focusing on poses related to sports. We merge the MPII and LSP-LSPET datasets to increase the sample size. The pose estimator is trained using the Adam optimizer with a 'ReduceLROnPlateau' learning rate scheduler for 100 epochs, with the learning rate set to 1e-3. Two augmentations, Shift+Scale+Rotate and horizontal flip, are applied. For details on the specific implementation, readers are referred to the code.

\section{Compilation of Results}

\begin{figure}[!h]
%\hspace{0.35cm}
\begin{subfigure}{\textwidth}
    \includegraphics[width=\textwidth]{figs/kl/NLL_KL_lr1e-2_00050.png}
    \caption{\centering We observe that the KL Divergence can act as a regularizer over the learnt covariance, thereby stabilizing optimization. However, the covariance for both the methods is dominated by the residual term, slowing down convergence.}%Learning rate: 0.01}
    \vspace*{0.25cm}
\end{subfigure}

\begin{subfigure}{\textwidth}
    \includegraphics[width=\textwidth]{figs/kl/NLL_KL_lr1e-1_00059.png}
    \caption{\centering At a higher learning rates (1e-1), both the Negative Log-Likelihood and the KL Divergence oscillate around the true distribution resulting in unstable optimization.} %Learning rate: 0.1}
\end{subfigure}
\caption{\textbf{Bivariate Normal Distribution (A)} \textit{Impact of residuals in optimization (Section: \ref{sec:KL})}. In addition to feature granularity \citep{seitzer2022on}, we show that a source for subpar convergence arises from the susceptibility of the negative log-likelihood and KL-Divergence to residuals in optimization.}
\label{fig:nll_kl_appendix}
\end{figure}

\begin{figure}
\begin{subfigure}{\textwidth}
    \includegraphics[width=\textwidth]{figs/kl/file0000070.png}
    \caption{\centering We observe that the KL-Divergence and likelihood based methods: vanilla negative log-likelihood and faithful \citep{stirn2023faithful} result in unstable convergence. In comparison, the 2-Wasserstein based methods are much more stable and accurate since they are not affected by residuals nor is the covariance affected by the convergence of the mean estimator.}%Learning rate: 0.01}
    \vspace*{0.25cm}
\end{subfigure}
\begin{subfigure}{\textwidth}
    \includegraphics[width=\textwidth]{figs/kl/file0000070cov.png}
    \caption{\centering If we initialize the predicted mean to the true mean, we still observe unstable convergence for the divergence and likelihood based methods due to perturbations caused by residuals (Section: \ref{sec:KL}).} %Learning rate: 0.1}
\end{subfigure}
\caption{\textbf{Bivariate Normal Distribution (B)} \textit{Visualizing convergence in bivariate regression (Section: \ref{sec:2-w})}. We perform analysis on two settings (a) the mean and covariance of the predicted distribution are initialized away from the target (b) the mean of the predicted distribution is initialized at the mean of the target.}
\label{fig:bivariate}
\end{figure}

\newpage

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figs/Univariate/highres_univariate_10.pdf}
    \includegraphics[width=\linewidth]{figs/Univariate/highres_univariate_11.pdf}
    \includegraphics[width=\linewidth]{figs/Univariate/highres_univariate_0.pdf}
    \caption{\textbf{Univariate.} We define the ground truth sinusoidals as (Top) $y = |x| \textrm{ sin } (2 \pi x)$ with $\sigma(x) =  |x|$ (Middle) $y = (5 - |x|) \textrm{ sin } (2 \pi x)$ and $\sigma(x) =  |x|$ (Bottom) $y = 5 \textrm{ sin } (2 \pi x)$ and $\sigma(x) =  |x|$. Given samples from the ground truth, the networks are trained to learn the underlying distribution using different objectives.}
    \label{fig:univariate_appendix}
\end{figure}

\newpage
\begin{figure}
    \centering
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim12/MSE.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim12/2-W.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim12/KL.pdf}
    \end{subfigure}
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim16/MSE.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim16/2-W.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim16/KL.pdf}
    \end{subfigure}
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim20/MSE.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim20/2-W.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim20/KL.pdf}
    \end{subfigure}
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim24/MSE.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim24/2-W.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim24/KL.pdf}
    \end{subfigure}
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim28/MSE.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim28/2-W.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim28/KL.pdf}
    \end{subfigure}
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim32/MSE.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim32/2-W.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/multivariate/dim32/KL.pdf}
    \end{subfigure}
    \caption{\textbf{(Multivariate)}. We simulate multivariate data with increasing dimensionality (top row: 12, bottom row: 32, middle rows: increment of four ). An increase in dimensionality causes the mean estimator to converge slow (or diverge) for likelihood and KL-Divergence. The 2-Wasserstein bound is succesfully able to learn the mean and covariance across all dimensions.}
    \label{fig:multivariate_appendix}
\end{figure}

\begin{table*}[ht]
    \centering
    \caption{\textbf{UCI Regression.} Results Across Different Metrics: Mean Square Error (MSE), Task Agnostic Correlations (TAC), and Negative Log-Likelihood (NLL) with standard deviations.}
    \begin{adjustbox}{totalheight=\textheight-2\baselineskip}
    \rotatebox{90}{%
    \begin{tabular}{lcccccccccccc}
    \multicolumn{13}{c}{\textbf{Mean Square Error (MSE)}} \\
    \toprule
    Method & Abalone & Air & Appliances & Concrete & Electrical & Energy & Gas & Naval & Parkinson & Power & Red Wine & White Wine \\
    \midrule
    \rowcolor{Gray}NLL                  & 3.74 $\pm$ 1.65 & 17.92 $\pm$ 5.23 & 53.49 $\pm$ 24.10 & 4.57 $\pm$ 1.07 & 9.28 $\pm$ 3.65 & 4.20 $\pm$ 1.75 & 10.98 $\pm$ 8.35 & 10.34 $\pm$ 5.07 & 54.51 $\pm$ 23.94 & 9.09 $\pm$ 2.67 & 8.94 $\pm$ 4.53 & 9.40 $\pm$ 4.95 \\
    KL-Divergence        & 1.90 $\pm$ 1.17 & 14.70 $\pm$ 3.63 & 90.90 $\pm$ 20.79 & 3.84 $\pm$ 1.04 & 15.57 $\pm$ 11.22 & 4.20 $\pm$ 0.84 & 10.16 $\pm$ 5.28 & 12.39 $\pm$ 4.62 & 59.39 $\pm$ 15.98 & 9.97 $\pm$ 2.99 & 7.26 $\pm$ 2.39 & 8.17 $\pm$ 2.33 \\
    \rowcolor{Gray}Beta-NLL             & 0.35 $\pm$ 0.13 & 1.58 $\pm$ 0.83  & 3.69 $\pm$ 1.93  & 2.02 $\pm$ 0.57 & 3.62 $\pm$ 2.63  & 1.87 $\pm$ 0.99 & 1.50 $\pm$ 0.33  & 0.72 $\pm$ 0.55  & 8.11 $\pm$ 8.25  & 3.06 $\pm$ 1.10 & 2.15 $\pm$ 0.64 & 3.43 $\pm$ 1.97 \\
    NLL: Diagonal        & 1.32 $\pm$ 0.22 & 8.90 $\pm$ 2.03  & 37.91 $\pm$ 6.39 & 4.28 $\pm$ 1.82 & 6.58 $\pm$ 1.34  & 3.99 $\pm$ 1.06 & 5.73 $\pm$ 1.26  & 9.60 $\pm$ 5.12  & 27.35 $\pm$ 5.68 & 6.52 $\pm$ 1.97 & 5.75 $\pm$ 2.26 & 6.01 $\pm$ 1.35 \\
    \rowcolor{Gray}Faithful             & \textbf{0.16} $\pm$ \textbf{0.03} & \textbf{0.33} $\pm$ \textbf{0.03}  & \textbf{0.20} $\pm$ \textbf{0.03}  & \textbf{0.72} $\pm$ \textbf{0.08} & \textbf{0.89} $\pm$ \textbf{0.07}  & \textbf{0.41} $\pm$ \textbf{0.18} & \textbf{0.45} $\pm$ \textbf{0.13} & \textbf{0.06} $\pm$ \textbf{0.05} & \textbf{0.29} $\pm$ \textbf{0.08} & \textbf{0.61} $\pm$ \textbf{0.10} & \textbf{0.70} $\pm$ \textbf{0.05} & \textbf{0.78} $\pm$ \textbf{0.05} \\
    NLL: TIC             & 0.21 $\pm$ 0.04 & 0.82 $\pm$ 0.37  & 4.45 $\pm$ 3.78  & 0.96 $\pm$ 0.25 & \textbf{0.91 $\pm$ 0.07}  & 0.61 $\pm$ 0.14 & 0.67 $\pm$ 0.37  & 1.36 $\pm$ 0.41  & 8.89 $\pm$ 6.46 & \textbf{0.66 $\pm$ 0.08} & 0.97 $\pm$ 0.34 & 0.92 $\pm$ 0.12 \\
    \midrule
    \textbf{2-W (Bound)} & \textbf{0.16} $\pm$ \textbf{0.03} & \textbf{0.34} $\pm$ \textbf{0.03}  & \textbf{0.20} $\pm$ \textbf{0.03}  & \textbf{0.72} $\pm$ \textbf{0.08} & \textbf{0.90} $\pm$ \textbf{0.18}  & \textbf{0.41} $\pm$ \textbf{0.18} & \textbf{0.45} $\pm$ \textbf{0.13} & \textbf{0.07} $\pm$ \textbf{0.04} & \textbf{0.30} $\pm$ \textbf{0.08} & \textbf{0.61} $\pm$ \textbf{0.10} & \textbf{0.71} $\pm$ \textbf{0.04} & \textbf{0.79} $\pm$ \textbf{0.05} \\
    \bottomrule
    \end{tabular}}
    \hspace*{2cm}
    \rotatebox{90}{%
    \begin{tabular}{lcccccccccccc}
    \multicolumn{13}{c}{\textbf{Task Agnostic Correlations (TAC)}} \\
    \toprule
    Method & Abalone & Air & Appliances & Concrete & Electrical & Energy & Gas & Naval & Parkinson & Power & Red Wine & White Wine \\
    \midrule
    \rowcolor{Gray}NLL                  & 3.03 $\pm$ 1.26 & 5.21 $\pm$ 1.31 & 15.71 $\pm$ 4.54 & 2.94 $\pm$ 0.65 & 5.74 $\pm$ 1.41 & 3.15 $\pm$ 0.78 & 3.85 $\pm$ 1.69 & 3.36 $\pm$ 1.17 & 14.07 $\pm$ 3.16 & 4.72 $\pm$ 1.22 & 4.32 $\pm$ 1.10 & 4.88 $\pm$ 1.06 \\
    KL-Divergence        & 1.90 $\pm$ 0.73 & 7.07 $\pm$ 1.91 & 20.04 $\pm$ 4.72 & 3.60 $\pm$ 1.07 & 6.32 $\pm$ 1.82 & 3.12 $\pm$ 0.50 & 3.76 $\pm$ 0.78 & 5.16 $\pm$ 2.43 & 13.97 $\pm$ 2.41 & 4.49 $\pm$ 0.88 & 4.93 $\pm$ 0.85 & 4.28 $\pm$ 1.09 \\
    \rowcolor{Gray}Beta-NLL             & 0.40 $\pm$ 0.08 & 0.81 $\pm$ 0.18 & 1.05 $\pm$ 0.15 & 1.03 $\pm$ 0.14 & 1.31 $\pm$ 0.32 & 0.98 $\pm$ 0.29 & 0.79 $\pm$ 0.06 & 0.41 $\pm$ 0.12 & 1.34 $\pm$ 0.46 & 1.18 $\pm$ 0.16 & 1.03 $\pm$ 0.15 & 1.17 $\pm$ 0.29 \\
    NLL: Diagonal        & 0.87 $\pm$ 0.08 & 2.15 $\pm$ 0.15 & 4.09 $\pm$ 0.38 & 1.50 $\pm$ 0.29 & 1.94 $\pm$ 0.17 & 1.45 $\pm$ 0.16 & 1.78 $\pm$ 0.20 & 2.08 $\pm$ 0.61 & 3.53 $\pm$ 0.36 & 1.90 $\pm$ 0.29 & 1.66 $\pm$ 0.33 & 1.83 $\pm$ 0.20 \\
    \rowcolor{Gray}Faithful             & 0.54 $\pm$ 0.07 & 1.31 $\pm$ 0.17  & 1.51 $\pm$ 0.17  & 1.48 $\pm$ 0.33 & 2.39 $\pm$ 0.07  & 1.21 $\pm$ 0.07 & 1.16 $\pm$ 0.13 & 0.37 $\pm$ 0.24 & 1.54 $\pm$ 0.17 & 1.61 $\pm$ 0.20 & 1.65 $\pm$ 0.14 & 1.81 $\pm$ 0.22 \\
    NLL: TIC             & 0.29 $\pm$ 0.02 & 0.53 $\pm$ 0.04 & 0.44 $\pm$ 0.10 & 0.70 $\pm$ 0.16 & \textbf{0.70 $\pm$ 0.04}  & 0.59 $\pm$ 0.10 & \textbf{0.43 $\pm$ 0.10} & 0.25 $\pm$ 0.04  & 0.61 $\pm$ 0.10 & \textbf{0.51 $\pm$ 0.03} & 0.79 $\pm$ 0.10 & 0.67 $\pm$ 0.11 \\
    \midrule
    \textbf{2-W (Bound)} & \textbf{0.24} $\pm$ \textbf{0.02} & \textbf{0.34} $\pm$ \textbf{0.02}  & \textbf{0.24} $\pm$ \textbf{0.01}  & \textbf{0.51} $\pm$ \textbf{0.02} & \textbf{0.67} $\pm$ \textbf{0.01}  & \textbf{0.36} $\pm$ \textbf{0.02} & \textbf{0.35} $\pm$ \textbf{0.02} & \textbf{0.12} $\pm$ \textbf{0.04} & \textbf{0.28} $\pm$ \textbf{0.03} & \textbf{0.50} $\pm$ \textbf{0.03} & \textbf{0.49} $\pm$ \textbf{0.01} & \textbf{0.56} $\pm$ \textbf{0.01} \\
    \bottomrule
    \end{tabular}}
    \hspace*{2cm}
    \rotatebox{90}{%
    \begin{tabular}{lcccccccccccc}
    \multicolumn{13}{c}{\textbf{Negative Log-Likelihood (NLL)}} \\
    \toprule
    Method & Abalone & Air & Appliances & Concrete & Electrical & Energy & Gas & Naval & Parkinson & Power & Red Wine & White Wine \\
    \midrule
    \rowcolor{Gray}NLL                  & 35.89 $\pm$ 28.55 & 56.98 $\pm$ 9.73  & 245.99 $\pm$ 73.55 & 28.50 $\pm$ 6.62  & 63.15 $\pm$ 15.57 & 29.85 $\pm$ 8.99  & 41.03 $\pm$ 21.62 & 38.18 $\pm$ 5.01  & 262.59 $\pm$ 73.52 & 49.37 $\pm$ 21.28 & 46.58 $\pm$ 10.26 & 58.06 $\pm$ 16.90 \\
    KL-Divergence        & 18.27 $\pm$ 4.74  & 83.18 $\pm$ 25.55 & 413.96 $\pm$ 151.56 & 38.23 $\pm$ 17.81 & 73.87 $\pm$ 23.77 & 28.50 $\pm$ 4.81  & 34.38 $\pm$ 6.96  & 49.24 $\pm$ 23.79 & 257.36 $\pm$ 92.79 & 45.49 $\pm$ 11.37 & 61.96 $\pm$ 24.34 & 48.66 $\pm$ 16.96 \\
    \rowcolor{Gray}Beta-NLL             & 9.80 $\pm$ 1.17   & 29.38 $\pm$ 2.89  & 60.30 $\pm$ 1.67   & 20.45 $\pm$ 4.74  & 35.44 $\pm$ 4.59  & 20.15 $\pm$ 5.27  & 20.26 $\pm$ 1.43  & 20.81 $\pm$ 2.21  & 59.98 $\pm$ 11.96 & 27.64 $\pm$ 4.56  & 34.05 $\pm$ 5.60  & 30.95 $\pm$ 2.79  \\
    NLL: Diagonal        & 18.61 $\pm$ 5.96  & 80.86 $\pm$ 8.46  & 369.67 $\pm$ 88.48 & 46.82 $\pm$ 15.19 & 65.73 $\pm$ 10.47 & 36.09 $\pm$ 8.57  & 47.06 $\pm$ 15.98 & 77.47 $\pm$ 45.51 & 238.71 $\pm$ 38.34 & 51.60 $\pm$ 3.45  & 77.98 $\pm$ 17.61 & 67.21 $\pm$ 16.24 \\
    \rowcolor{Gray}Faithful             & 11.86 $\pm$ 1.18  & 33.31 $\pm$ 1.53 & 65.15 $\pm$ 2.49  & 17.42 $\pm$ 1.68  & 34.73 $\pm$ 0.53  & 19.41 $\pm$ 0.49  & 22.47 $\pm$ 0.86  & 27.70 $\pm$ 0.81  & 57.04 $\pm$ 3.33  & 24.08 $\pm$ 0.87  & 24.34 $\pm$ 0.99  & 26.01 $\pm$ 1.53  \\
    NLL: TIC             & \textbf{4.71 $\pm$ 1.19}   & 16.46 $\pm$ 0.89  & 30.41 $\pm$ 13.42  & 11.36 $\pm$ 1.50  & \textbf{14.97 $\pm$ 0.80}  & 12.06 $\pm$ 1.44  & \textbf{9.96 $\pm$ 2.73}   & 14.99 $\pm$ 3.43  & 42.52 $\pm$ 7.52  & \textbf{9.31 $\pm$ 0.92}   & 14.66 $\pm$ 1.24  & \textbf{12.33 $\pm$ 1.68}  \\
    \midrule
    \textbf{2-W (Bound)} & 6.32 $\pm$ 0.23   & \textbf{13.58} $\pm$ \textbf{0.25}  & \textbf{22.72} $\pm$ \textbf{0.15}  & \textbf{8.96} $\pm$ \textbf{0.29}  & \textbf{15.57} $\pm$ \textbf{0.24}  & \textbf{8.85} $\pm$ \textbf{0.34}  & \textbf{10.49} $\pm$ \textbf{0.36}  & \textbf{11.44} $\pm$ \textbf{0.18}  & \textbf{21.48} $\pm$ \textbf{0.63}  & 11.31 $\pm$ 0.39  & \textbf{11.65} $\pm$ \textbf{0.17}  & \textbf{12.12} $\pm$ \textbf{0.18}  \\
    \bottomrule
    \end{tabular}}
    \end{adjustbox}
    \label{tab:uci_all}
\end{table*}

\newpage

\begin{figure}
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figs/UCI/abalone.pdf}
        \subcaption{Abalone}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figs/UCI/air.pdf}
        \subcaption{Air}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figs/UCI/electrical.pdf}
        \subcaption{Electrical}
    \end{subfigure}
    %\vspace*{-25mm}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figs/UCI/parkinson.pdf}
        \subcaption{Parkinson}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figs/UCI/redwine.pdf}
        \subcaption{Red Wine}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figs/UCI/whitewine.pdf}
        \subcaption{White Wine}
    \end{subfigure}
    \caption{\textbf{(Warm-up, UCI)} We explore training deep heteroscedastic regression models using warm-up as proposed in \citet{sluijterman2024optimal}. We train only the mean estimator for half the training epochs, and jointly train the mean and covariance estimator for the remaining half. We observe a trend across datasets that the training is unstable and momentarily diverges for the negative log-likelihood and KL-Divergence which are especially sensitive to residuals and incorrect covariance estimates. This trend is similar to our observations for the two methods on our bivariate normal distribution experiments (Fig. \ref{fig:bivariate}). While we plot for six datasets here, this trend is representative of all datasets.}
    \label{fig:uciwarmup_appendix}
\end{figure}


\newpage
\begin{figure}
    \centering
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/HumanPose/lr1e-2/MSE.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/HumanPose/lr1e-2/NLL.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/HumanPose/lr1e-2/TAC.png}
    \end{subfigure}
    %\vspace*{-25mm}
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/HumanPose/lr1e-3/MSE.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/HumanPose/lr1e-3/NLL.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/HumanPose/lr1e-3/TAC.png}
    \end{subfigure}
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/HumanPose/lr1e-4/MSE.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/HumanPose/lr1e-4/NLL.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \includegraphics[width=\textwidth]{figs/HumanPose/lr1e-4/TAC.png}
    \end{subfigure}
    \caption{\textbf{Human Pose}. \textit{(Improving state-of-the-art heteroscedastic pose estimation)} (Top row: learning rate: 1e-2, Middle row: learning rate: 1e-3, Bottom row: learning rate: 1e-4) We explore a hybrid training strategy by combining the 2-Wasserstein bound with the negative log-likelihood. We train ViTPose for the first 20 epochs using the bound, and then switch to negative log-likelihood. We use the TIC parameterization for the covariance which when trained with the negative log-likelihood, showed state-of-the-art performance in heteroscedastic pose estimation. We observe that using the hybrid approach retains the competitiveness of the 2-Wasserstein bound on the mean square error, and the competitiveness of the negative log-likelihood on the negative log-likelihood metric.}
    \label{fig:humanpose_appendix}
\end{figure}
\end{document}
