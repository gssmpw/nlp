\section{Introduction}\label{sec:introduction}
% -- Outline
% ---- LLMs are popular
% ---- There're many stakeholders in the training and inference loop
% ---- Adversaries in the training loop are a problem -- malpractice, poisoning
% ---- Also, showing compliance
% ---- Need a framework to prove the integrity of the pipeline
% ---- Enter Atlas

% ---- LLMs are popular
In recent years, machine learning (ML) models, have become increasingly popular.
The pervasive use of large language models (LLMs), in particular, and multi-stakeholder
involvement in model creation and deployment exacerbate security and privacy risks.
These considerations are emphasized by the global nature and the complexity of
large-scale ML deployments with different lifecycle stages:
%gathering and sanitizing the data from different sources,
%training and inferencing across many data centers,
%compliance with local laws or corporate policies.

% ---- There're many stakeholders in the training and inference loop
%Additionally, different stages of the ML development pipeline come with their own stakeholders:
\begin{enumerate}[label=\arabic*)]
    \item Collection and sanitation of a \emph{training} dataset from several public and proprietary sources.
    %\item Solicitation and facilitation of training.
    \item Provisioning of the training environment (hardware and software).
    \item Execution of training across many data centers.
    \item Construction of a \emph{testing} dataset from several sources, and the evaluation.
    \item Deployment and use of the model for inference that is compliant with local laws or corporate policies.
    %\item Use of the model in compliance with local laws or corporate policies.
\end{enumerate}

% ---- Adversaries in the training loop are a problem -- malpractice, poisoning
Each of these stages is vulnerable to malicious or dishonest parties.
For example, data can be poisoned~\cite{biggio2012poisoning,carlini2024poisoning} during collection or training.
Service providers executing outsourced training can shorten or omit critical steps to reduce their cost.
Model providers can serve smaller models in SaaS, or even distribute malicious ones.

% ---- Also, showing compliance
On the other hand, responsible model builders and other stakeholders may be incentivised or required to provide security and trust guarantees.
They may want to prove low bias in their training data, offer easily verifiable performance claims, or guarantee end-to-end integrity of the model creation in high risk domains.

% ---- Need a framework to prove the integrity of the pipeline
To address these challenges, it is necessary to guarantee the integrity of the entire ML lifecycle --
beginning with the data, through the training, and finally, the evaluation and deployment.
Was the data modified?
Did the hardware and software environment adhere to the specification?
Did the contractor follow the specified training procedure?
Can I trust the evaluation?
How can I guarantee that I am interacting with the intended model?
These are example questions that showcase the breadth of the involved challenges that must be tackled to provide end-to-end security.

% --- Enter Atlas
In this work, we introduce \atlas, a framework for enhancing the security and transparency of the lifecycle of ML models.
\atlas establishes the baseline of fundamental components and capabilities needed for comprehensive provenance tracking
at each stage of the ML lifecycle.
Subsequently, \atlas defines the core integrity requirements for verifiable ML lifecycle transparency.
We provide a reference implementation that instantiates \atlas using hardware-based security mechanisms -- with trusted execution environment (TEE),
including attestations.% , and comprehensive metadata-based provenance tracking.
%Our implementation satisfies all \atlas requirements.

We claim the following contributions:
\begin{enumerate}[label=\arabic*.]\label{sec:introduction:contributions}
    \item We introduce \atlas, a framework designed for end-to-end ML lifecycle transparency.
    \item We instantiate \atlas using TEEs and metadata-based provenance tracking.
    \item We evaluate our \atlas prototype through two case studies:
        \begin{enumerate*}[label=\arabic*)]
            \item fine-tuning of a BERT model~\cite{lin2023metabert, lin2023metabertimpl};
            \item fine-tuning of a bge-reranker model~\cite{chen2023bge}
        \end{enumerate*}.
\end{enumerate}

%\msm{revise: Integrate this motivation into intro}
%Organizations frequently leverage pre-trained models, outsource training processes, and integrate components from multiple sources,
%making it difficult to verify the authenticity and trustworthiness of their ML systems. This complexity is further compounded
%by the potential for malicious modifications at various stages of the model lifecycle, from data preparation through deployment.
%The involvement of various third parties in ML model development and deployment
%creates critical challenges in ensuring supply chain integrity.
%
%While Software Bills of Materials (SBOMs) and AI Bills of Materials (AI BOMs) provide basic inventory tracking for model components,
%they fall short in addressing the dynamic nature of ML pipelines. These approaches typically offer point-in-time snapshots but
%fail to capture the complex transformations, fine-tuning operations, and runtime modifications that characterize modern ML workflows.
%Additionally, they lack cryptographic guarantees about the integrity of recorded information and cannot effectively track the provenance
% of model weights and training data.
%
% These approaches demonstrate the growing importance of ML supply chain security.
% However, they are typically applied in an ad-hoc fashion, highlighting the need
% for a more integrated approach that combines comprehensive lineage tracking,
% strong cryptographic properties, and practical integration capabilities with existing ML development and deployment pipelines.
%
%A comprehensive solution requires not just documentation of components, but verifiable evidence of their origins,
%transformations, and integrity throughout the entire model lifecycle. This need has driven interest in more robust
%provenance tracking mechanisms that can:
%
%\begin{itemize}
%\item Provide cryptographic proof of model lineage
%\item Track and verify all pipeline transformations
%\item Maintain tamper-evident records of training processes
%\item Ensure integrity of model artifacts across organizational boundaries
%\end{itemize}
%
%Several existing tools and frameworks
%commonly focusing on different components of the model lifecycle and provenance tracking.
%While these solutions offer valuable capabilities, they often address only specific parts of the end-to-end ML
%supply chain rather than providing comprehensive coverage.
%\msm{end-revise}
%
%\todo{add discussion of EU-CRA AI Act requirements for model documentation and FDA guidelines for AI/ML in healthcare}

%The remainder of this paper is organized as follows:
%in Section~\ref{sec:background-related} we provide an overview of the necessary background, and the related work;
%Section~\ref{sec:problem} presents the challenge of providing integrity in the ML pipeline, the threat model, and the system assumptions;
%in Section~\ref{sec:framework} we present \atlas -- our framework for providing ML integrity;
%Section~\ref{sec:implementation} covers implementation details;
%in Section~\ref{sec:eval}, we show that \atlas is effective across three dimensions: training overhead $<8\%$, the verification time increases linearly with the size of the model, and it is compatible with PyTorch and Tensorflow;
%in Section~\ref{sec:casestudies} we present the case studies;
%in Section~\ref{sec:discussion} we discuss additional considerations for \atlas,
%and Section~\ref{sec:conclusion} concludes the paper and provides directions for future work.
