\section{System Overview \& Threat Model}\label{sec:problem}

\subsection{Terminology}\label{sec:problem:terminology}

In \atlas, an ML model is composed of a number of \textbf{artifacts}
that include the training dataset, ML algorithm, ML framework (e.g., PyTorch),
model configuration (e.g., hyperparameters, weights), and metadata (e.g.,
license).
%Model artifacts such as datasets, algorithms and configurations are commonly
%considered to be intellectual property, so they are often not publicly available
%in practice.

The \textbf{ML lifecycle} consists of various stages, including
data preparation, training, evaluation and deployment.
A common synonym for ML lifecycle is ``ML supply chain'', so we use these
terms interchangeably in the paper.

The \textbf{ML pipeline} defines the sequence of operations or a workflow that
transforms or operates on a model artifact at a particular stage of the ML
lifecycle~\cite{google-ml-pipelines}.
To support the standardization and repeatability of a specific workflow, the
pipeline is also used to facilitate workflow management and automation, e.g.,
training or evaluation pipelines.

The \textbf{ML system} is the set of hardware and software components that
implement and execute an ML pipeline.
For example, an ML system for training may include orchestration tools, an
authentication service, storage systems, automation infrastructure, and
specialized compute hardware (e.g., GPUs, TPUs, or custom accelerators).

\subsection{System Model}\label{sec:problem:sys-model}

In \atlas, we target the \emph{multi-stakeholder} ML model lifecycle setting.
%produced model artifacts may use \emph{third-party} components, and
%model pipelines may be \emph{outsourced} to dedicated, external ML-as-a-Service (MLaaS) services.

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=\columnwidth]{./img/hf-nlp-pytorch-atlas-arch.pdf}
%    \caption{
%        Example NLP model lifecycle with Atlas integration.
%        In \atlas, MLaaS providers (solid boxes) obtain model artifacts (datasets or models) and perform transformations on them.
%        The attestation client generates digitally signed artifacts and submits transformation metadata to an Atlas transparency log (striped box and dashed arrows).
%        Model users commonly delegate model integrity checking to an Atlas verifier (striped box).
%    }
%    \label{fig:atlas-arch}
%\end{figure}

\subsubsection{Stakeholders}\label{sec:problem:sys-model:participants}

The \textbf{Artifact Producers} are individuals and organizations that create ML
model artifacts % like datasets, ML libraries or frameworks
to provide or sell them to other parties.
%To this end, \atlas assumes that producers consume third-party data and software
%as dependencies to help create their artifact.
Since the artifacts may represent intellectual property and/or make use of
personally identifying information (PII), producers have business and regulatory
reasons to preserve the confidentiality of their artifacts.
To save costs, artifact producers often
outsource the operation of ML systems to external MLaaS providers.

\textbf{ML-as-a-Service (MLaaS) providers} operate and maintain the compute
infrastructure needed to run ML systems on behalf of artifact producers.
MLaaS providers may offer ML-specific services that leverage general-purpose
systems (e.g.,~\cite{azure-mlaas,ibm-watsonx,google-vertex-ai}), or
provide special-purpose systems like orchestrators~\cite{kubeflow-pipelines}
or CI/CD (e.g.~\cite{github-mlops2020}) that can be used to build and run third-party ML systems.
%In \atlas, we assume MLaaS providers interact with a transparency service by
%sending the necessary information needed to generate authenticated metadata
%about ML pipelines.

A \textbf{Hub} is a system that stores and distributes model artifacts.
Thus, model pipelines typically ingest and output artifacts to and from hubs
during their execution, enabled by interfaces exposed by MLaaS providers.

Hubs may be operated by artifact producers themselves or by third-party vendors,
containing open or closed source artifacts (e.g.,HuggingFace~\cite{huggingface},
PyTorch Hub~\cite{pytorch-hub}).
%They may be general-purpose platforms storing multiple types of artifacts (e.g.,
%HuggingFace~\cite{huggingface}), or dedicated to distributing specific artifact
%types (e.g., PyTorch Hub~\cite{pytorch-hub}).
%Depending on an artifact producer's privacy requirements, hubs in \atlas are
%responsible for protecting stored artifacts' confidentiality.

A \textbf{Transparency Service} in \atlas is responsible for generating, storing
and distributing the information necessary to verify the authenticity and
integrity of model artifacts, ML systems and lifecycles as a whole.
We envision model vendors and independent parties to operate transparency services
in practice.
% mvd to section 4
%Thus, transparency services interface with ML systems through \emph{attestation
%clients} to obtain metadata about ML pipelines (including the model artifacts,
%ML systems and configurations), and use this information to generate
%authenticated ML lifecycle attestations.
%On the server side, a \emph{transparency log} contains the known good values
%(i.e., golden values) of a producer's model artifacts, an MLaaS provider's
%system components, as well as the attestations collected by the clients
%throughout the ML lifecycle.

\textbf{Verifiers} are entities that seek to evaluate or audit a particular ML
model's lifecycle with the goal of checking whether the model it produced was
tampered with in an unintended or malicious way.
To this end, verifiers in \atlas define a set of \emph{expectations} about a
producer's model artifacts, MLaaS providers, and their ML system components.
% movd to section 4
%Using golden values and attestations obtained from a trusted transparency
%service, verifiers evaluate the authenticated metadata for each ML pipeline and
%artifact of interest against the expectations.
In practice, model users, vendors or regulatory entities may act as verifiers.

A \textbf{Model User} interacts with a model in an inferencing
pipeline, or in a \emph{downstream} ML pipeline as a dependency, such as a
fine-tuning or evaluation pipeline (see~\S\ref{sec:problem:sys-model:lifecycle}).

\subsubsection{ML Lifecycle}\label{sec:problem:sys-model:lifecycle}

In \atlas, we consider four common high-level stages in the ML lifecycle.
%each represented by a specific pipeline~\cite{google-ml-pipelines}.
Each builds upon the outputs and feedback from the others,
forming a continuous cycle that allows models to evolve and adapt based on
real-world usage.

\noindent\textbf{1. Data processing}: Raw data is collected, curated,
processed into smaller units (e.g., tokens) and collated into a structure
ingestable during training.
%In a typical ML model lifecycle, additional data collected during the evaluation
%and deployment stages are regularly fed back into the data processing to
%enhance the training dataset.

\noindent\textbf{2. Training}: a training algorithm processes a given dataset using an ML framework.
The output of the training pipeline is an ML model that may undergo further fine
tuning and evaluation, prior to being deployed for inferencing.

\noindent\textbf{3. Evaluation}: following training, %an ML model commonly is sent
%through an evaluation (i.e., validation) pipeline, in which
model properties like its performance and accuracy are checked and further adjusted,
prior to production release and deployment.

\noindent\textbf{4. Deployment}: Once an ML model has been trained and evaluated, it
is deployed to a production system configured for inferencing.
The new data obtained from clients during inference is sent back to a data processing
pipeline to enhance the training dataset, and the model.

\subsection{Threat Model}\label{sec:problem:threat-model}

\atlas is concerned with addressing risks to ML model artifacts introduced via
the ML lifecycle, i.e., attack vectors in third-party MLaaS and hubs across
different ML pipelines that lead to compromised ML artifacts.
Specifically, while transparency services, verifiers and model users are trusted
in \atlas, we consider threats by MLaaS providers, hubs and artifact producers.

%\msm{we can likely express this in the same fig as the arch}
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=\linewidth]{img/threat-model}
%	\caption{ML Pipeline Threat Model showing trusted environment (TEE), untrusted components,
%		and attack vectors with emphasis on third-party and insider threats}
%	\label{fig:threat-model}
%\end{figure}

The adversary's goal is to produce a tampered artifact or model,
e.g., containing a hidden malicious component, so that a trusted transparency
service generates a legitimate signature on the artifact
or its metadata.

Compromised MLaaS providers and hubs may involve malicious insiders,
or external adversaries seeking to subvert these systems
by exploiting vulnerable components.
Given their central position, MLaaS providers and hubs may thus be able to
compromise model integrity at various stages of the ML lifecycle.

For example, a malicous MLaaS provider can poison the training data during the
curation step of the data processing stage leading to
backdoors~\cite{carlini2024poisoning,biggio2012poisoning}.
A malicious hub may, for instance, present a dataset or model with a mismatched
signature (e.g., to a different model, or any of its component artifacts) to a
model user or MLaaS provider, so that pipelines in subsequent stages of the ML
lifecycle may ingest compromised dependencies.

As a result, these compromises may propagate through the ML lifecycle if they go
undetected, ultimately leading to vulnerable ML models at the deployment stage.
This risk is exacerbated if a hub colludes with an MLaaS providers to introduce
or accept compromised ML pipeline inputs.

Artifact producers, on the other hand, may seek to compromise ML models in order
to bypass regulatory requirements, introduce exploitable vulnerabilities or steal
private information for profit (e.g,~\cite{xz-utils2024}).
Thus, producers may collude with other untrusted participants, or inaccurately
declare their dependencies leading to intentional omissions or misrepresentations
their artifacts, to surreptitiously undermine the integrity of their artifacts.

%Crucially, they can poison the training data during the curation step of the
%data processing stage leading to backdoors, or reduced utility~\cite{citespls}.
%During model training, they can compromise the ML system (software or hardware),
%tamper with the training procedure, or inject backdoors.
%Similarly, during the deployment stage, the adversary can manipulate the model
%weights (or swap the model entirely), compromise the ML system's software stack,
%or tamper with the input/output of the model.

%\subsubsection{Hub Threats}
%
%Similar to MLaaS providers, malicious insiders as well as external adversaries
%may be seek to subvert hubs to tamper with stored model artifacts.
%For instance, by presenting a dataset or model with a mismatched signature
%(e.g., to a different model, or any of its component artifacts) to a model user
%or MLaaS provider, pipelines in subsequent stages of the ML lifecycle may ingest
%compromised dependencies.
%
%As a result, these compromises may propagate through the ML lifecycle if they go
%undetected, ultimately leading to vulnerable ML models at the deployment stage.
%This risk is exacerbated if a hub colludes with an MLaaS providers to introduce
%or accept compromised ML pipeline inputs.

%\subsubsection{Artifact Producer Threats}
%
%Artifact producers may seek to compromise ML models in order to introduce
%exploitable vulnerabilities or steal private information for profit, or bypass
%regulatory requirements.
%Real-world incidents such as the xz-util backdoor~\cite{xz-util}, for example,
%demonstrate the threat of directly introducing vulnerabilities into artifacts is
%especially viable in open source dataset or models, because artifacts are
%developed in collaboration with individuals or organizations of varied
%trustworthiness.
%
%In the ML lifecycle, compromised artifact producers may collude with MLaaS
%providers or hubs to surreptitiously undermine the integrity of their artifacts.
%Artifact producers may also inaccurately declare their dependencies leading to
%intentional omissions or misrepresentations about the composition of their
%artifacts to MLaaS providers or transparency services.

\subsubsection{Trusted Parties}

As mentioned, \atlas considers the transparency services, verifiers and model
users in an ML lifecycle to be trusted.
We make the following assumptions about the security of the systems
underlying and enabling the participants:
\begin{enumerate*}[label=\arabic*)]
    \item the hardware and cryptographic primitives are implemented correctly
        and do not contain known vulnerabilities;
    \item a separate PKI system and organizations representing the participants
        follow best practices for key management, network security and access control.
\end{enumerate*}
%The system provides secure key storage for signing model artifacts.
%Hardware-based random number generation helps maintain cryptographic security guarantees by providing high-quality entropy for cryptographic operations.

This allows us to trust that transparency services correctly implement \atlas
attestation mechanisms, detect tampering with them by a malicious MLaaS provider
and maintain accurate golden values of model artifacts.
Verifiers are trusted to properly evaluate \atlas attestations, including their
digital signatures, against pre-specified expectations, and to publish a record
of their evaluations.

\subsubsection{Out of Scope}

We consider the following threats to be out of scope for \atlas.
While a critical threat to deployed AI applications, inference time black-box
attacks (e.g., evasion attacks, model extraction, membership inference) are
caused by malicious users, which we do not address; solutions proposed to reduce
this particular risk~\cite{carlini2022membership,jagielski2020high,tramer2016stealing}
are complementary to our work.

Side-channel attacks against hardware enclaves, physical attacks on hardware
infrastructure, and network-level denial of service attacks are also beyond the
scope of \atlas.
These attacks are the subject of a large body of prior work~\cite{mo-sok2024},
and we acknowledge that these complementary security measures could be added to
deployments of \atlas.

\subsection{Design Requirements}\label{sec:problem:requirements}

We define the following integrity and operational requirements for \atlas:

\noindent\textbf{R1: Artifact tampering is detectable.} To provide model artifact
authenticity, \atlas must enable verifiers to detect unexpected modifications
to model artifacts by malicious MLaaS providers or hubs.

\noindent\textbf{R2: Every model transformation is attested.}
Because adversaries may seek to tamper with model artifacts after they are
produced by a pipeline, every model transformation in \atlas must produce an
authenticated record as evidence for the process.

\noindent\textbf{R3: Verifiable model lineage.}
\atlas verifiers must be able to detect unauthorized operators
and unintended or malicious changes to the expected stages of the lifecycle
(e.g., pipelines operating out of order, or being omitted).

\noindent\textbf{R4: Strongly isolated ML systems.}
To reduce the risks of tampering with a pipeline during its execution, \atlas
must restrict access to its ML system by unauthorized entities, and contain
compromises from propagating beyond the execution environment.

\noindent\textbf{R5: Controlled artifact disclosure.}
\atlas must address data and model producer concerns about PII and intellectual
property confidentiality by enabling participants to control which parties and
systems are able to access datasets and model components.

\noindent\textbf{R6: Pipeline agnostic.}
To facilitate adoption, \atlas must be agnostic to any ML pipeline
that integrates it.

\noindent\textbf{R7: Efficiency.}
We seek to minimize the computational and storage overheads incurred by \atlas
to enable the implementation and deployment of \atlas in ML systems using
commodity platforms and services.
