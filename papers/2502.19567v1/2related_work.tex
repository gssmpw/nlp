\section{Background \& Related Work}\label{sec:background-related}

%The increasing complexity and criticality of machine learning (ML) systems
%have led to growing concerns about their security and integrity.
%Since the process through which an ML model is created, i.e., the ML lifecycle,
%impacts model security and behavior, prior and concurrent work have
%increasingly focused on ML pipeline integrity.
%In this section, we describe key technologies and approaches to address various
%aspects of ML lifecycle security and integrity.

\subsection{Data Provenance \& Authenticity}
While not a new concern, the authenticity, 
provenance and attribution of media
has received renewed attention in recent years due to the
wide online proliferation of images, videos and text that have been manipulated
or forged using generative ML models~\cite{feng2023examining, england2021amp}.

One common approach for addressing digital image and video manipulation involves
cryptographic techniques such as hashing, digital signatures and blockchains,
which enable provenance authentication, tamper detection, and checking for
divergent views of the content~\cite{sidnam-mauch2022usable}.

Most notably, the Coalition for Content Provenance and Authenticity (C2PA)
specification~\cite{iso2024c2pa, c2pa2024spec} introduced by the Content
Authenticity Initiative (CAI)~\cite{adobe2019cai} and Project
Origin~\cite{project-origin} provides cryptographic binding of authenticity claims,
verifiable assertions about content origin and modifications, and tamper-evident
audit trails~\cite{laurie2023c2pa}.

Initially launched as a response to the growing challenge of deepfakes
~\cite{adobe2019cai} and digital content manipulation, the C2PA standard first
gained traction in digital photography and journalism workflows.
The standard's extensible and interoperable metadata format has made C2PA an
attractive option for ML dataset and model provenance
tracking~\cite{collomosse2024content}.

Other prior work in this space builds upon digitally signed metadata by leveraging
distributed ledger technologies to create transparent and immutable content or
provenance records. Examples of this approach include AMP~\cite{england2021amp},
the News Provenance Project~\cite{news-provenance-project} and Collomosse et
al.~\cite{collomosse2024content}.

\atlas borrows techniques from C2PA and similar approaches, but integrates them
directly into the ML system to track model artifact provenance closer to where
the transformations occur to achieve higher metadata fidelity, rather than
applying them in an ad hoc fashion after a particular dataset or model artifact
has been created.

The Open Source Security Foundation (OpenSSF) Model Signing
project~\cite{ossf-model-signing2024} is a parallel effort in this space
focusing specifically on cryptographic signing of model artifacts to ensure
their authenticity and integrity.
As such, it serves as a crucial building block but requires integration with
other tools for more comprehensive supply chain security.
While Model Signing might integrate with \atlas, additional mechanisms would
be needed to address our goal of tracking full model lineage.

LakeFS~\cite{lakefs2025} combines Git-like semantics with concepts from object
stores such as S3 to provide a version control system for data, including ML
datsets.
Thus, LakeFS aims to capture data lineage by tracking changes to stored data
over time, and allowing ML applications to reference specific versions of the
stored data.
This approach is meant to integrate with existing first-party data processing
pipelines, but does not facilitate verification of data provenance by downstream
consumers.
\atlas' metadata centered approach, on the other hand, enables first- \emph{and}
third-party ML dataset consumers to track changes and check their provenance,
even when they may not have direct access to the data.

\subsection{Supply Chain Integrity}
While the ML supply chain is an emerging area of research, software supply chain
security and integrity has been the subject of much prior work.
Thus, several of these approaches are being explored for their applicability to
enhancing the integrity of the ML model lifecycle.

Bills of Materials (BOM) have been employed to document the list of components
of a hardware or software product for over three decades~\cite{hegge191bom}.
Due to recent regulations~\cite{eo14028, eu-cra}, Software BOM (SBOM) have been
the focus of many industry and academic efforts seeking to facilitate tracking
software dependencies and other metadata to detect supply chain threats~\cite{ntia-sbom}.
%, to improve their adoption, and to enhance SBOM integrity and privacy (e.g.,~\cite{cites-pls}).

The AIBOM framework~\cite{trail-of-bits-aibom2024, manifest-aibom2023}
incorporates concepts from traditional software supply chain management while
focusing on intended ML model use cases.
Like SBOM, AIBOM provide a mechanism for tracking model software dependencies
and maintaining model metadata.
At the time of writing, we are not aware of any frameworks other than \atlas
that utilize any sort of BOM data format to track ML model components.

Complementing BOM, a number of efforts like OpenSSF Supply Chain Levels for
Software Artifacts (SLSA)~\cite{slsa2025} and SPDX Build~\cite{spdx-build2023}
have sought to collect build provenance, i.e., metadata describing the process
through which a particular artifact was produced.
With its incremental guidance on implementing integrity measures for software
builds and its build provenance data format, the SLSA framework~\cite{slsa2025}
is gaining traction among software vendors, and also
beginning to see some use for ML model fine tuning~\cite{slsa-for-models2024}.
\atlas' configurable metadata collection capabilities would enable us to collect
build provenance at any stage of the ML model lifecycle, providing a more
comprehensive view.

A number of frameworks for capturing and verifying a variety of security
claims and metadata about the supply chain have been proposed.
in-toto~\cite{torres2019} collects authenticated claims \emph{across} supply
chain steps, including SBOM and SLSA metadata.
In particular, in-toto enables software development pipeline owners and
downstream artifact consumers to specify end-to-end supply chain policies,
and validate that only the expected parties carried out specific steps in the
supply chain, and artifacts underwent transformations in the expected order.
Given recent and upcoming enhancements that further
generalize the framework, in-toto may be a suitable option for specifying and
verifying end-to-end ML model lifecycle integrity policies in \atlas.

Sigstore~\cite{sigstore2025} provides a transparency log-based infrastructure
for issuing signing credentials and validating digital signatures on supply
chain artifacts and metadata.
Similarly, Supply Chain Integrity, Transparency and Trust
(SCITT)~\cite{scitt2024} is an architecture for implementing distributed ledger
to provide global visibility and auditing
for supply chain operations and claims.
The SCITT architecture also includes confidential computing technologies to
help ensure that only authorized parties submit claims to the transparency
ledger.

\atlas builds upon several of these supply chain integrity and transparency
approaches, but seeks to support multiple types of artifact provenance, supply
chain metadata and integrity verification mechanisms all within a customizable,
integrated framework designed for the ML model lifecycle.

\subsection{Model Lineage Tracking}
%Building on concepts from the artifact provenance and supply chain integrity
%spaces, several tools and frameworks aim to track different artifacts and
%processes of the model lifecycle.
%However, while these solutions offer valuable capabilities, they often address
%only specific parts of the ML supply chain rather than providing end-to-end
%coverage.

The EQTY Lineage Explorer~\cite{eqty2023} provides granular tracking of model
artifacts throughout the training process.
The tool captures relationships between datasets, model checkpoints and
hyperparameters, enabling developers to trace model evolution.
Unlike \atlas, it lacks cryptographic guarantees for artifact authenticity and
focuses primarily on manually collected development-time lineage, rather than
automatically capturing and linking information across the entire ML lifecycle.

ML experiment trackers like Weights and Biases~\cite{wandb2023},
Neptune~\cite{neptune2025} and Kubeflow Pipelines~\cite{kubeflow-pipelines}
offer detailed run-time logging of model metadata about training runs, metrics,
and model artifacts.
While comprehensive for collaborative ML application development, these tools
do not necessarily integrate transparently with common ML frameworks, and they
typically provide only unauthenticated metadata that makes it less appropriate
for establishing trust in a particular ML pipeline.
\atlas, on the other hand, focuses making model lineage verifiable and on
enabling adoption by supporting integration into existing open ML frameworks.

\subsection{Hardware-Based Security for ML}
Recent developments in trusted execution environments (TEEs) technologies
have made it more practical to run large-scale systems and workloads.
Thus, a growing number of proposals leverage TEEs to harden a variety of ML
systems through hardware-enforced isolation and integrity verification
mechanisms.

Chrapek et al.~\cite{chrapek2024fortify} use TEEs in a large language model (LLM)
fine-tuning system running inside secure enclaves that help protect LLM code and
data while \emph{in use}.
To this end, they evaluate their LLM system using the process-level Intel\regTM
Software Guard eXtensions (Intel\regTM SGX) and virtual machine-level Intel\regTM
Trust Domain eXtensions (Intel\regTM TDX) TEEs, showing that they are able to
maintain practical performance overhead of less than 10\% for models like Llama2
in both TEE configurations.

Laminator~\cite{duddu2024} demonstrates the application of TEEs to ML property
attestation for ML model regulatory compliance verification.
Specifically, this work leverages TEEs' hardware-assisted remote attestation and
cryptographic binding capabilities to generate verifiable ML model property cards.
This work is complementary to \atlas and may enable us to extend our framework
to support such ML property attestations.

Mo et al.'s survey~\cite{mo-sok2024} evaluates 38 works that use various TEE
implementations to enhance the privacy and integrity of ML training and
inference operations.
The survey highlights several gaps that existing TEE-based frameworks do not
address, including the protection of a full ML lifecycle, which is the primary
focus of \atlas.
