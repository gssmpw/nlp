\section{Implementation}\label{sec:implementation}
Our proof-of-concept implementation integrates with Kubeflow~\cite{kubeflow-pipelines}
through standard APIs for metadata tracking and execution monitoring within ML
pipelines.
This integration approach enabled us to avoid significant modifications to our
case study pipelines (\S\ref{sec:eval}), while maintaining \atlas' security
and transparency enhancements.

The \atlas attestation client is implemented as two components. First, the
metadata sidecar (\S\ref{sec:implementation:metadata}) captures ML artifact and
pipeline metadata in C2PA manifest format~\cite{c2pa2024spec}.
Due to current poor support for automated C2PA manifest generation and
verification for ML models, we implemented Rust-based library in 2.3k LoC
that captures cryptographic measurements, TDX attestation hashes, digital
signatures, and temporal metadata.
Second, the attestation client implementation leverages a dedicated TEE service
(\S\ref{sec:implementation:attestation}, which interacts with Intel TDX~\cite{tdx},
a virtualization-layer TEE, to provide the hardware-based security primitives for \atlas.

To implement the transparency log, our framework extends the Sigstore's Rekor
~\cite{sigstore2025} tamper-evident ledger.
Building on Rekor's existing capabilities, we added support for storage and
verification of \atlas C2PA-based model transformation attestations.
The modified implementation validates C2PA signatures and attestation
measurements during manifest submission, ensuring only properly signed and
attested artifacts are stored in the log.

%combines hardware-based security primitives with optimized verification protocols to create a practical
%framework for ML pipeline security.
%This section details the technical implementation of key framework components,
%focusing on the attestation service, verification protocols, and integration mechanisms.

For ease of implementation, our prototype verification system (\S\ref{sec:implementation:verification})
is integrated into the client's metadata sidecar.

\Paragraph{Metadata Sidecar.}\label{sec:implementation:metadata}
% mvd sec 4 - Provenance System
To integrate our C2PA tooling non-intrusively with ML pipelines, the
\atlas attestation client uses a sidecar container architecture.
%Our implementation leverages the C2PA standard through a sidecar container architecture that ensures non-intrusive integration with ML pipelines.
This allows the client to monitor the pipeline for modifications, generate manifests and update relationships
through an optimized storage layer.
In addition to collecting C2PA manifest metadata, the sidecar generates the authenticated transformation attestations for model artifacts, and
%Artifact manifests contain cryptographic measurements, TDX attestation hashes, digital signatures, and temporal metadata.
tracks artifact relationships by mapping datasets to checkpoints, checkpoints to models, and maintaining configuration bindings.

The sidecar container implementation monitors the pipeline for modifications, generating manifests that are first
stored in a local cache layer before being committed to the transparency log.
The local cache maintains an indexed hierarchy of manifests for efficient validation during pipeline execution,
before the final storage in Rekor for tamper-evident provenance tracking.
For storage optimization, our implementation decomposes manifests into constituent components.
The assertion store, claim signatures, and metadata are stored separately, with relationships maintained through a reference system.
This approach enables efficient updates to specific manifest components, reduced storage redundancy,
optimized query performance, and scalable version tracking.

\Paragraph{TEE Service.}\label{sec:implementation:attestation}
To obtain the Intel TDX-based compute environment attestations that are included
in the C2PA metadata, the sidecar interfaces with the TEE service.
%The system interfaces with the TDX environment through hardware-backed attestation measurements,
%which are included in the metadata to provide strong integrity guarantees.
%The attestation service leverages Intel Trust Domain eXtensions (Intel TDX) to provide hardware-based integrity measurements for pipeline operations.
We implement a dedicated RESTful service that captures TEE
state and validates runtime ML system component measurements, which are
cryptographically anchored in hardware via Intel TDX.
%a chain of trust anchored in hardware.
The TEE service implements the Confidential Containers (CoCo) Attestation
format~\cite{coco2024attestation} for standardized claims used in TEE verification.

%For each attestation request, the client generates a JSON Web Token~\cite{jwt2024},
%which adds custom claims to represent \atlas-specific information like hardware
%TEE properties and software measurements, and
%containing critical security claims including issuer identification and public key for signature verification,
%temporal validity through expiration and effective timestamps, hardware TEE information and software measurements, policy identifiers and evaluation reports from the attestation service,
%golden values used in verification policy enforcement.%, and custom claims bound to the evidence through cryptographic digestion.
%
% this is mostly in sec 4 already
% mvd sec 4 - Security Architecture
%The service maintains isolated memory regions where sensitive computations proceed without risk of interference
%from untrusted components. Throughout pipeline execution, the attestation system continuously validates the runtime environment
%and generates measurements that feed into both verification and provenance tracking systems.
%
We optimize attestation operations through batching and caching mechanisms.% while maintaining the
%strict security guarantees required for ML workloads.
The client coordinates with the verification system through standard Kubeflow
interfaces, enabling seamless integration with the other \atlas components
while preserving attestation integrity.

%\subsection{Transparency Log}\label{sec:implementation:transparency}
%Our framework extends Sigstore's Rekor~\cite{sigstore2025} transparency log to implement the tamper-evident record requirements. Building on Rekor's existing capabilities, we added support for storage and verification of C2PA manifests to handle ML-specific attestations.
%%, enabling cryptographic validation of metadata integrity directly within the ledger.
%The modified implementation validates C2PA signatures and attestation measurements during manifest submission,
%ensuring only properly signed and attested artifacts are recorded.

\subsection{Kubeflow Integration}\label{sec:implementation:pipeline}
% mvd sec 4 - Framework Integration
The integration with Kubeflow is achieved through custom operators and controllers that monitor pipeline
execution through Kubeflow's Metadata V2 Beta API and KFP API. Through the \texttt{//apis/v2beta1/metadata} endpoint,
we track execution contexts and maintain verifiable records of pipeline runs.

By interfacing with \texttt{/apis/v2beta1/artifacts}, we track model artifacts and their lineage. The
metadata store provides structured information about component dependencies and data flow through the
\texttt{/apis/v2beta1/connections} endpoint. Our system correlates this information with integrity measurements and hardware
attestations, creating verifiable records of pipeline execution states.

The metadata extraction leverages Kubeflow's event system through \texttt{/apis/v2beta1/events}, enabling
real-time capture of pipeline state transitions, component execution details, artifact generation events, and parameter
updates. This structured approach enables verification of pipeline states while maintaining compatibility with
existing workflows.

\subsection{TEE Service Claims}
For each attestation request, the client generates a JSON Web Token ~\cite{jwt2024},
which adds custom claims to represent \atlas-specific information
including issuer identification and public key for signature verification,
temporal validity through expiration and effective timestamps, hardware TEE information and software measurements, 
and golden values used in verification policy enforcement

\Paragraph{Verification Service Implementation.}\label{sec:implementation:verification}
The metadata sidecar also serves as a verification endpoint allowing pipeline components to validate artifact
integrity against stored attestations.
% some of this was moved back to sec 4, so we might be able to cut some more
% mvd sec 4 - Core verification process details
We optimize the performance of our staged verification system in three ways:
1) by processing changes incrementally and caching to avoid re-verifying
unchanged components, 2) via batch processing of verification operations, and
3) parallel verification paths for independent component classes.
%These implementation choices particularly benefit iterative ML workflows where only portions of the pipeline change between runs.
%Our caching strategy preserves verified states while ensuring security through selective invalidation on failures.

% MOVED TO APPENDIX
%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.5\textwidth]{img/verification-flowchart}
%    \caption{Verification workflow implementation showing how components are classified and processed: Changes trigger incremental validation that preserves cached states for unchanged components.
%    The system groups related artifacts (training data, model weights, configurations) for batch processing, validates their relationships in parallel, and selectively invalidates only affected components on failures.
%    This approach reduces verification overhead while maintaining security guarantees.}
%    \label{verification-flowchart}
%\end{figure}

% mvd to section 4
%The core verification process uses a classification system to group related components including training datasets,
%model weights, algorithm implementations, and evaluation results, enabling efficient batch processing.
%By maintaining a cache of verified states and implementing incremental verification, the system avoids re-computing cryptographic
%operations for unchanged components. This optimization particularly benefits ML pipelines where only small portions of the
%workflow change between iterations.

For provenance chain validation, our implementation parallelizes verification of
cross-component dependencies, version compatibility and evaluation result consistency.
The system also maintains verification checkpoints that serve as trusted reference points,
enabling partial verification from the last known good state instead of complete chain recomputation.

Error handling focuses on computational efficiency through targeted cache invalidation rather than complete cache clearing.
When verification failures occur, the system preserves verified states while ensuring security through selective invalidation.
Preliminary testing shows these optimizations reduce verification time by up to 50\% through parallelization and caching, while maintaining security guarantees.

% mvd to top of this section by msm in PR #50
%\subsection{Pipeline Integration}\label{sec:implementation:pipeline}
% mvd sec 4 - Framework Integration
%The framework integrates with Kubeflow through standardized APIs for metadata tracking and execution monitoring (detailed in Appendix~\ref{sec:appendix:impl:pipeline}), enabling seamless integration with existing ML workflows.

% MOVED TO APPENDIX - Example JSON
%A typical execution record captured by our system looks like:
%{\footnotesize
%\begin{verbatim}
%{
%    "execution": {
%        "name": "training-run-132",
%        "state": "RUNNING",
%        "pipeline_spec": {
%            "parameters": {
%                "learning_rate": 0.001,
%                "batch_size": 32,
%                "random_seed": 42,
%                "optimizer_config": {
%                    "type": "Adam",
%                    "beta1": 0.9,
%                    "beta2": 0.999
%                }
%            },
%            "runtime_config": {
%                "gcs_output_directory": "gs://...",
%                "tensorflow_version": "2.9.0"
%            }
%        }
%    }
%}
%\end{verbatim}
%}

% MOVED TO APPENDIX - Example JSON
%For each execution, our system adds corresponding integrity measurements and verification records:
%{\footnotesize
%\begin{verbatim}
%{
%    "integrity_measurement": {
%        "component_id": "training-run-132",
%        "tdx_quote": "base64:...",
%        "environment_hash": "sha256:...",
%        "timestamp": "2024-01-15T10:30:00Z",
%        "parameter_hash": "sha256:..."
%    }
%}
%\end{verbatim}
%}
%This integration approach enables organizations to enhance their existing ML infrastructure with robust security controls
%without significant modifications to their established workflows. The framework's API provides flexible integration points
%through standardized interfaces, allowing adaptation to specific deployment requirements while maintaining strong security
%guarantees.

% mvd to sec 6 - EVALUATION
%\subsection{Performance Testing}\label{sec:implementation:performance}
%Our implementation addresses the computational overhead of security mechanisms through several optimization strategies.
%Testing shows these optimizations maintain security guarantees while significantly reducing performance impact across
%different operational scenarios.
%
%The verification system employs intelligent caching to minimize redundant cryptographic operations. By maintaining
%a cache of verified states and implementing incremental verification, the system avoids re-computing operations for
%unchanged components. This optimization particularly benefits ML pipelines where only small portions of the workflow change
%between iterations. Our testing demonstrates that training overhead remains under 8\%, primarily from manifest generation,
%while verification processes scale linearly with model size.
%
%For large-scale operations, our implementation utilizes parallel processing capabilities to maintain performance at scale.
%The system executes verification operations concurrently when component dependencies allow, efficiently utilizing available
%computational resources. Cache optimization further reduces repeated verification costs, particularly beneficial during
%iterative model development where many components remain stable between iterations.
%
%Performance testing demonstrates these optimizations achieve manifest generation overhead under 8\% of training time,
%verification latency reduction up to 50\% compared to baseline, linear scaling with model size, and near-constant time
%for cached component verification.
%
%Error handling mechanisms focus on computational efficiency through targeted cache invalidation rather than
%complete cache clearing. When verification failures occur, the system preserves verified states while ensuring security
%through selective invalidation. This approach maintains performance even during error recovery scenarios while preserving
%the integrity of our security guarantees.

\subsection{Verification Flow}\label{sec:implementation:verification}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{img/verification-flowchart}
    \caption{Verification workflow implementation showing how components are classified and processed: Changes trigger incremental validation that preserves cached states for unchanged components.
    The system groups related artifacts (training data, model weights, configurations) for batch processing, validates their relationships in parallel, and selectively invalidates only affected components on failures.
    This approach reduces verification overhead while maintaining security guarantees.}
    \label{verification-flowchart}
\end{figure}


\subsection{Framework Adaptability}\label{sec:implementation:adapt}
The BERT deployment validated our framework's flexibility across different ML environments. The abstraction layer successfully handled variations in
framework-specific interfaces, from PyTorch's hook mechanisms to TensorFlow's Keras callbacks, while maintaining consistent security guarantees.
Custom adapters enabled integration without modifying existing ML infrastructure, demonstrating the framework's ability to enhance security while preserving established workflows.
