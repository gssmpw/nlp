\section{Evaluation}\label{sec:eval}

We validate our framework through a security analysis, preliminary performance testing 
and two case studies: BERT Meta~\cite{lin2023metabert, lin2023metabertimpl} and BGE reranker~\cite{chen2023bge}.

%\subsection{Performance Analysis}\label{sec:eval:performance}
%% mvd fr sec 5.E - Performance Testing
%Our implementation addresses the computational overhead of security mechanisms through several optimization strategies.
%Testing shows these optimizations maintain security guarantees while significantly reducing performance impact across
%different operational scenarios.
%
%The tests demonstrate that \atlas' security mechanisms introduce minimal overhead, with training overhead remaining under 8\%,
%primarily from manifest generation. The verification processes scale linearly with model size, while our caching strategies
%(see~\S\ref{sec:implementation}) effectively optimize performance for iterative ML workflows.
%
%For large-scale operations, our implementation utilizes parallel processing capabilities to maintain performance at scale.
%The system executes verification operations concurrently when component dependencies allow, efficiently utilizing available
%computational resources. Cache optimization further reduces repeated verification costs, particularly beneficial during
%iterative model development where many components remain stable between iterations.

%Performance measurements demonstrate these optimizations achieve:
%% Converted bullet points, no chgs to the content
%1) a maximum manifest generation overhead below 8\% of training time, 2) a reduction of up to 50\% of verification latency compared to the unoptimized implementation, 3) linear verification overhead scaling with model size, and 4) near-constant time for cached component verification.
%Error handling mechanisms maintain performance through targeted cache invalidation rather than complete cache clearing, preserving verified states while ensuring security through selective invalidation.

\subsection{Security Analysis}\label{sec:eval:security}
% Security Analysis - TODO improve it or remove it
\atlas provides measures against the threats outlined in~\S\ref{sec:problem:threat-model} through multiple security mechanisms.
%The hardware-rooted security ensures integrity of computation environments, while continuous attestation protects against
%runtime tampering.
%The provenance system maintains verifiable records of all transformations, enabling detection of unauthorized
%modifications.

For MLaaS provider threats, the hardware-rooted TEEs in \atlas isolate sensitive computations and
detect malicious insider tampering with executing ML pipelines.
The attestation client continuously validates the runtime environment,
generating ML system measurements that are cryptographically bound to model artifacts.

Against hub threats, the \atlas verification system ensures artifact integrity through cryptographic measurements and signatures.
The provenance chain maintains verifiable links between artifacts, preventing mismatched signatures or compromised dependencies
from propagating.

To address artifact producer threats, \atlas relies on comprehensive provenance tracking
that captures all dependencies and modifications.
The transparency layer provides an immutable record of pipeline operations,
enabling detection of undeclared dependencies or intentional omissions.

\subsection{Preliminary Performance Analysis}\label{sec:eval:performance}
Our implementation addresses the computational overhead of security mechanisms through several optimization strategies.
Preliminary tests demonstrate that \atlas' security mechanisms introduce minimal overhead, with training overhead remaining under 8\%
across all test cases.
The verification processes scale linearly with model size, while our caching strategies
(see~\S\ref{sec:implementation}) effectively optimize performance for iterative ML workflows;
we observe a reduction of up to 50\% of verification latency compared to the unoptimized implementation,
and near-constant time for cached component verification.

For large-scale operations, parallel processing capabilities maintain performance at scale through:
1) concurrent verification operations when dependencies allow;
2) cache optimization to reduce repeated verification costs;
3) selective cache invalidation for error handling.

We plan to conduct more extensive performance benchmarking as part of future work.
%\section{Evaluation}\label{sec:eval}
%
%\subsection{Performance Analysis}\label{sec:eval:performance}
%\fauxsection{Overhead Measurements}
%
%\fauxsection{Scalability Assessment}
%
%\subsection{Security Analysis}\label{sec:eval:security}
%\fauxsection{Threat Mitigation}
%
%\fauxsection{Trust Chain Verification}
