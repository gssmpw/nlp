\begin{abstract}
	The rapid adoption of open source machine learning (ML) datasets and models
	exposes today's AI applications to critical risks like data poisoning and
	supply chain attacks across the ML lifecycle.
    With growing regulatory pressure to address these issues through greater
    transparency, ML model vendors face challenges balancing these requirements
    against confidentiality for data and intellectual property needs.
    
    We propose~\atlas, a framework that enables fully attestable ML pipelines.
    \atlas leverages open specifications for data and software supply chain
    provenance to collect verifiable records of model artifact authenticity
    and end-to-end lineage metadata.
    \atlas combines trusted hardware and transparency logs to enhance metadata integrity,
    preserve data confidentiality, and limit unauthorized access during ML pipeline
    operations, from training through deployment.
    
    Our prototype implementation of~\atlas integrates several open-source tools
    to build an ML lifecycle transparency system, and
    assess the practicality of~\atlas through two case study ML pipelines.
    
%    The rapid adoption of artificial intelligence (AI) systems has introduced critical security vulnerabilities 
%    across the machine learning (ML) lifecycle, from data collection and model training to deployment and inference. 
%    Recent incidents highlighting supply chain attacks, model poisoning, and adversarial manipulations 
%    underscore the urgent need for comprehensive security solutions. 
%    Organizations increasingly rely on AI for critical decisions, yet existing security approaches often 
%    fail to address the unique challenges of protecting complex ML pipelines that span multiple stages including 
%    feature engineering, model training, continuous deployment, and monitoring.
%    
%    We present a novel framework that combines hardware-rooted security mechanisms, provenance tracking, 
%    and real-time attestation to secure the entire ML lifecycle. Our approach leverages Trusted Execution Environments (TEEs) 
%    to provide hardware-based integrity and confidentiality during model training and deployment, while implementing automated 
%    workflow orchestration and verification across the MLOps pipeline. The framework introduces an enhanced ML training lifecycle 
%    that incorporates metadata collection, attestation mechanisms, and policy verification through vetted pipeline components 
%    - from initial data preparation through model serving and monitoring.
%    
%    Our prototype implementation demonstrates how the framework can help harden ML pipeline deployments by adding additional 
%    layers of security and transparency. The framework's security controls and traceability features provide building 
%    blocks that organizations can use to strengthen their defenses against potential attack vectors and support their 
%    compliance efforts. Integration with existing MLOps architectures introduces security mechanisms while preserving 
%    the workflow automation capabilities needed in production environments.
%    
%    This work advances the field of AI security by providing practical approaches that can enhance existing ML 
%    infrastructure security. The framework offers foundational components for addressing security gaps in current practices 
%    while enabling organizations to implement verifiable controls that support both security hardening and transparency requirements. 
%    Our findings provide organizations with guidance for strengthening their AI systems' security throughout the increasingly complex 
%    development and deployment lifecycle.
\end{abstract}