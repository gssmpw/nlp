
\subsection{Case Studies}\label{sec:eval:casestudies}
BERT was selected for its complex architecture and widespread production use. Our implementation
covers the complete lifecycle from pre-trained model fine-tuning through deployment, with isolated
execution environments for attention computations and weight updates. 
%The hardware-backed attestation system monitors the training environment during fine-tuning, while our provenance system tracks all model changes through task-specific adaptations.

To validate \atlas' adaptability, we also implement security controls for BGE reranker on Intel Xeon processors.
\atlas secures both instruction-based and layerwise reranking configurations, with training data structured
as JSON records containing query-positive-negative text triplets. The provenance system tracks model
adaptations across both base model variations (Gemma-2B and MiniCPM-2B), maintaining verifiable records
of hyperparameters and training progressions.

Both implementations demonstrate performance characteristics consistent with our analysis in
~\S\ref{sec:eval:performance}, while preserving training efficiency and security guarantees.

\section{Case Study: BERT Pipeline}\label{sec:eval::casestudies:bert}

% condensed maybe still too long 
Our proof-of-concept implementation demonstrates the framework's capabilities through securing a BERT model development and deployment pipeline.
We chose BERT Meta \cite{lin2023metabert, lin2023metabertimpl} for this case study due to its complex architecture, widespread use in production environments,
and frequent fine-tuning requirements that test our framework's adaptability.

%\subsection{Implementation Architecture}\label{sec:appendix:casestudies:impl}
The implementation covers the complete model lifecycle from pre-trained model adaptation through deployment.
For BERT's transformer architecture, we implemented isolated execution environments for attention computations and weight updates.
The hardware-backed attestation system provides continuous runtime verification during fine-tuning operations, while our provenance system maintains verifiable records of all
model evolution steps from the base model through task-specific adaptations.

Performance analysis shows practical viability with training overhead under 8\%, primarily from manifest generation,
and verification processes scaling linearly
with model size. The system's parallel processing capabilities and cache optimization strategies maintained performance at scale.

% org text
%\section{Case Study: Securing BERT Pipeline}\label{sec:casestudies}
%
%\subsection{Implementation Environment}
%We developed a proof of concept implementation using BERT Meta \cite{lin2023metabert, lin2023metabertimpl}
%development and deployment. BERT presents an ideal case study due to its complex architecture, widespread use, and frequent fine-tuning
%requirements. Our implementation covered the complete lifecycle from pre-trained model adaptation through deployment.
%
%\subsection{Security Integration Points}
%The implementation secured critical BERT pipeline stages:
%
%\subsubsection{Training Protection}
%For BERT's transformer architecture, we implemented isolated execution environments for attention computations and weight updates.
%The hardware-backed attestation system monitors the training environment, providing runtime verification of model evolution during
%fine-tuning operations.
%
%\subsubsection{Model Lifecycle Security}
%Our provenance system tracks the complete model evolution chain from pre-trained base model through task-specific adaptations.
%The C2PA manifest system maintains verifiable records of all fine-tuning operations, hyperparameter configurations, and dataset
%transformations.
%
%\subsection{Performance Analysis}
%Performance measurements demonstrate practical viability:
%\begin{itemize}
%    \item Training overhead remained under 8\%, primarily from manifest generation
%    \item Verification processes scaled linearly with model size
%    \item Cache optimization reduced repeated verification costs
%    \item Parallel processing capabilities maintained performance at scale
%\end{itemize}
%
%\subsection{Case Study Insights}
%The BERT deployment revealed several key learnings:
%
%\subsubsection{Framework Adaptability}
%Our modular design demonstrated significant flexibility across different ML frameworks and environments.
%The abstraction layer successfully handled variations in framework-specific interfaces - from PyTorch's hook mechanisms and state
%dictionaries for parameter access, to TensorFlow's Keras callbacks and weight interfaces, to JAX/Flax's immutable states and functional
%parameter tracking. This normalization layer enabled consistent security guarantees while allowing frameworks to maintain their
%native patterns.
%
%The framework's plugin architecture proved particularly valuable for cross-framework integration.
%Custom adapters translate between native ML framework workflow representations and our verification requirements
%without requiring modifications to existing codebases. This approach allowed organizations to preserve their established ML
%infrastructure while adding comprehensive security controls. The verification layer's standardized interfaces supported diverse ML
%workflow engines while maintaining strict security guarantees through hardware-backed attestation.
%
%\subsubsection{Standardization Requirements}
%Our deployment highlighted critical gaps in ML pipeline security standardization.
%While we successfully integrated with modern MLOps tools, the lack of common standards for model provenance and verification
%created unnecessary complexity. Current software supply chain security practices like SBOMs and SLSA provide valuable patterns,
%but ML pipelines introduce unique requirements around model lineage, training verification, and artifact relationships.
%
%Working with industry initiatives like OpenSSF Model Signing and C2PA demonstrated the value of standardized approaches.
%However, we identified several areas needing industry alignment:
%\begin{itemize}
%    \item Common formats for model provenance metadata
%    \item Standardized interfaces for integrity verification
%    \item Unified approaches to hardware attestation integration
%    \item Consistent patterns for tracking training environment state
%    \item Interoperable manifest formats for model artifacts
%\end{itemize}
%These standardization needs directly impact the efficiency and security of ML deployments.
%Our experience suggests that establishing common standards would significantly reduce integration complexity while improving
%the overall security posture of ML systems.
