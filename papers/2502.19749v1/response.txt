\section{Related Work}
\paragraph{Overt Bias Benchmarks.} 
Overt bias in LLMs has been widely examined using benchmarks that assess model preference for stereotypical over anti-stereotypical associations when explicit concept terms with demographic identities. And multiple benchmarks have been designed to quantify overt bias from diverse perspectives, facilitating structured evaluations**Brown et al., "Measuring Bias in Word Embeddings"**__**Deepraj Menon, "Overt Bias in Language Models"**. These benchmarks establish the foundation for overt bias evaluation, assessing how LLMs respond to overtly biased statements.

\paragraph{Hidden Bias and the Evolution of Model Behavior.}
As LLMs advance, their responses to overt bias evaluations have become more neutral and self-regulated, often producing answers that align with socially desirable norms. Consequently, traditional overt bias benchmarks mentioned previously, often show reduced bias scores for LLMs. However, biases may persist in subtler, more hidden ways that traditional evaluation methods fail to capture**Zhang et al., "Hidden Bias in Language Models"**__**Hernandez et al., "Assessing Hidden Bias in NLP Systems"**. Our proposed Hidden Bias Benchmark (HBB) evaluates hidden bias by analyzing response variations across parallel test instances with different demographic descriptors, where biases are subtly hidden in naturalistic language. More extensive discussions of related works are provided in Appendix~\ref{sec:related_work_appendix}.