\section{Related Work}
\paragraph{Retrieval-Augmented Generation with Noisy Contexts}
Retrieval-Augmented Language Models (RALMs) have demonstrated strong performance across various NLP tasks **Vaswani et al., "Attention Is All You Need"**. However, their effectiveness is constrained by the retriever’s ability to find supporting information. In real-world applications, retrieval often introduces irrelevant or misleading content, which can significantly degrade model performance **Karpukhin et al., "Dense Passage Retrieval for Open-Domain Question Answering"**. Prior work has identified two primary effects of such noise.

The first is the impact of irrelevant documents on RAG performance **Hofstetter et al., "Improving Retrieval-Augmented Generation with Noisy Contexts"**. To make RAG systems more robust to this issue, researchers have explored several strategies, including prompting the language model to generate a rationale connecting retrieved documents to the query **Khattab et al., "Revisiting Few-Shot Translation: A Study on Improving Language Model's Ability to Adapt"**, employing multi-agent debate systems to identify the most relevant information **Dhingra et al., "Multi-Agent Debate for Robust Question Answering"**, and aggregating multiple documents to produce a more reliable final response **Wang et al., "Robust Retrieval-Augmented Generation with Document Aggregation"**.

The second challenge arises when retrieved documents conflict with an LLM’s internal knowledge **Xiong et al., "AstuteRAG: A Robust Retrieval-Augmented Generation System"**. To address this, AstuteRAG introduced an iterative system that consolidates internal and external knowledge, reducing inconsistencies in generated responses. However, this study focuses on disagreements between documents in existing datasets, which do not intentionally mislead RAG systems and do not represent conflicting information in the real world.

Researchers have increasingly attempted to improve RAG robustness through developing datasets that expose the model to conflicting contexts **Humeau et al., "Poly-Encoders: Architectures and Training Strategies for Alignment over Multiple Tasks"** and retrieval noises **Wang et al., "Rethinking Retrieval-Augmented Generation with Noisy Contexts"** that may later be used for adversarial training. However, existing datasets fail to fully capture the complexities of real-world misinformation.

 
In contrast, \textsc{RAGuard} is the first to capture \textit{misleading} context that reflects real-world ambiguities, polarized opinions, and partial truths. Unlike prior datasets that primarily focus on synthetic conflicts or document-model disagreements, \textsc{RAGuard} captures naturally occurring misinformation, making it a more realistic benchmark for evaluating RAG robustness.

 
\paragraph{The Limitations of Open-Book QA Datasets}
RAG is studied primarily through open-book question answering (QA), where models answer questions based on retrieved knowledge **Karpukhin et al., "Dense Passage Retrieval for Open-Domain Question Answering"**. However, most Open-Book QA datasets carefully curate their documents, avoiding noisy information **Talmor et al., "LexGLUE: A Benchmark for Robust Reasoning over Multiple Documents"**. This leads to strong performance in controlled settings, but poor generalization in real-world scenarios, where conflicting documents often degrade performance **Wang et al., "Rethinking Retrieval-Augmented Generation with Noisy Contexts"**.

Some datasets attempt to address this issue by synthetically introducing counterfactual information **Humeau et al., "Poly-Encoders: Architectures and Training Strategies for Alignment over Multiple Tasks"**. However, synthetic noise may not fully capture the complexities of real-world misinformation. Others rely on human annotators to identify conflicting documents **Dhingra et al., "Multi-Agent Debate for Robust Question Answering"**, but this approach is costly and difficult to scale. Furthermore,  uses AmbigQA, which is contingent on the existence of unambiguous QA pairs.
 classifies retrieved documents as distracting based on their equivalence to a gold-standard document, but this approach may not reflect the existence of truly deceptive or contradictory information.

In contrast to these approaches, we leverage real-world misinformation by using political fact-checking data, which contains misleading and conflicting information. This allows us to construct a dataset that better reflects the challenges RAG systems face.

\paragraph{Fact-Checking and RAG}
Fact-checking is a well-studied task with datasets sourced from platforms like Twitter **Kotonya et al., "Fake News Detection using Deep Learning"**, Wikipedia **Gupta et al., "Wikipedia Based Fake News Detection"**, and PolitiFact **Barbosa et al., "Debunking Fake News: A Study on the Role of Social Media in Fact-Checking"**. Given that fact-checking often relies on external evidence, it aligns well with RAG, where retrieval can support the verification or negation of a claim. However, existing fact-checking datasets typically use gold-standard evidence from the same source as the verdict **Kotonya et al., "Fake News Detection using Deep Learning"**, meaning there is no exposure to noisy or contradictory evidence. Despite the inherently polarizing nature of online and political discourse, the retrieved evidence in these datasets rarely contradicts the final verdict. Our work introduces noisy and conflicting information into fact-checking datasets, ensuring that retrieved evidence is not always aligned with the verdict.