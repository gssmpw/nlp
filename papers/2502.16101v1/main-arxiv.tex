%%
%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
% \documentclass[sigconf,authordraft]{acmart}
% \documentclass[sigconf,natbib=true,anonymous=true]{acmart}

% \documentclass[sigconf,natbib,anonymous,review]{acmart}
\DocumentMetadata{}

\documentclass[sigconf,natbib=true,anonymous=false]{acmart}

%% added packages
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{tikz}
\pgfplotsset{compat=1.18}



%% NOTE that a single column version may required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXX.XXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Preprint]{Preprint}{2025}{}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
\acmBooktitle{Preprint,
2025} 
\acmPrice{15.00}
\acmISBN{XXX}



\usepackage{booktabs} % For professional-looking tables
\usepackage{pifont} % For checkmarks and x marks
\usepackage{graphicx} % For scaling the table
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{adjustbox}


\newcommand{\cmark}{\ding{51}} % Checkmark
\newcommand{\xmark}{\ding{55}} % X mark


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Linda Zeng}
\email{26lindaz@students.harker.org}

\authornotemark[1]
\affiliation{%
  \institution{The Harker School}
  \streetaddress{}
  \city{San Jose}
  \state{California}
  \country{USA}
  \postcode{43017-6221}
}
\author{Rithwik Gupta}
\email{rithwikca2020@gmail.com}
\affiliation{%
  \institution{Irvington High School}
  \streetaddress{}
  \city{Fremont}
  \state{California}
  \country{USA}
  \postcode{x}
}
\authornote{Equal contribution.}

\author{Divij Motwani}
\email{divijmotwani@gmail.com}
\affiliation{%
  \institution{Palo Alto High School}
  \streetaddress{}
  \city{Palo Alto}
  \state{California}
  \country{USA}
  \postcode{x}
}

\author{Diji Yang}
\email{dyang39@ucsc.edu}
\authornotemark[2]
\affiliation{%
  \institution{University of California Santa Cruz}
  \streetaddress{}
  \city{Santa Cruz}
  \state{California}
  \country{USA}
  \postcode{95064}
}

\author{Yi Zhang}
\email{yiz@ucsc.edu}
\affiliation{%
  \institution{University of California Santa Cruz}
  \streetaddress{}
  \city{Santa Cruz}
  \state{California}
  \country{USA}
  \postcode{95064}
}
\authornote{Co-advising.}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Zeng, Gupta, and Motwani, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Retrieval-augmented generation (RAG) has shown impressive capabilities in mitigating hallucinations in large language models (LLMs). However, LLMs struggle to handle misleading retrievals and often fail to maintain their own reasoning when exposed to conflicting or selectively-framed evidence, making them vulnerable to real-world misinformation.

In such real-world retrieval scenarios, misleading and conflicting information is rampant, particularly in the political domain, where evidence is often selectively framed, incomplete, or polarized. However, existing RAG benchmarks largely assume a clean retrieval setting, where models succeed by accurately retrieving and generating answers from gold-standard documents. This assumption fails to align with real-world conditions, leading to an overestimation of RAG system performance.

To bridge this gap, we introduce \textsc{RAGuard}, a fact-checking dataset designed to evaluate the robustness of RAG systems against misleading retrievals. Unlike prior benchmarks that rely on synthetic noise, our dataset constructs its retrieval corpus from Reddit discussions, capturing naturally occurring misinformation. It categorizes retrieved evidence into three types: \textit{supporting}, \textit{misleading}, and \textit{irrelevant}, providing a realistic and challenging testbed for assessing how well RAG systems navigate different retrieval information. 

Our benchmark experiments reveal that when exposed to misleading retrievals, all tested LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no retrieval at all), highlighting their susceptibility to noisy environments.
To the best of our knowledge, \textsc{RAGuard} is the first benchmark to systematically assess RAG robustness against misleading evidence.
We expect this benchmark will drive future research toward improving RAG systems beyond idealized datasets, making them more reliable for real-world applications.\footnote{The dataset is available at \href{https://huggingface.co/datasets/UCSC-IRKM/RAGuard}{https://huggingface.co/datasets/UCSC-IRKM/RAGuard}.}


\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003317.10003347.10003348</concept_id>
       <concept_desc>Information systems~Question answering</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
        <concept_id>10002951.10003317.10003338.10003341</concept_id>
        <concept_desc>Information systems~Language models</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>

   <concept>
        <concept_id>10010147.10010178.10010179</concept_id>
        <concept_desc>Computing methodologies~Natural language processing</concept_desc>
        <concept_significance>100</concept_significance>
</concept> 
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Question answering}
\ccsdesc[500]{Information systems~Language models}
\ccsdesc[100]{Computing methodologies~Natural language processing}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Retrieval-Augmented Generation (RAG) benchmark, Fact-checking dataset, Noisy retrieval corpus, Misleading retrievals}




% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/newIntro.png}
    \caption{Example of a false claim initially classified correctly but later misclassified as true due to misleading retrieved-context, alongside the ideal human judgment. 
    }
    \label{fig:datasetexample}
\end{figure}

Retrieval-augmented generation (RAG) systems have shown significant promise in mitigating LLM hallucination and enhancing trustworthiness. By combining the generative capabilities of large language models (LLMs) with the retrieval power of external corpora, RAG aims to ground responses in relevant, contextually appropriate information, thereby improving factual consistency and output credibility \cite{RAGNLP,RAGPretrain,gao2023retrieval}.
However, while existing RAG approaches primarily focus on optimizing retrieval relevance and maximizing the amount of information in retrieved-context \cite{karpukhin-etal-2020-dense, yang2024rag,  borgeaud2022improving}, a critical challenge remains largely unaddressed: how to handle cases where retrieved content is misleading or irrelevant. This issue is particularly concerning when misinformation, adversarial perturbations, or biased sources influence the retrieval process, potentially degrading the reliability of LLM outputs. Addressing this robustness gap is essential for ensuring the trustworthiness of RAG systems, especially in high-stakes applications such as fact-checking \cite{thorne-vlachos-2018-automated} and legal or medical domains \cite{guha2024legalbench, xiong2024benchmarkingretrievalaugmentedgenerationmedicine}.





Prior work has mitigated noisy retrievals by prompting models to justify relevance, aggregating sources, or using debate-based selection \cite{InstructRAG, RobustRAG, LearningToBreak}. However, most approaches align retrieved content with LLMs’ prior knowledge rather than addressing real-world contradictions \cite{AstuteRAG, ContextMemoryConflict}. Furthermore, current datasets overly rely on curating reliable documents, limiting robustness testing against misinformation \cite{TriviaQA, HotPotQA, SQUAD, NaturalQuestions}. While some introduce counterfactuals or retrieval noise \cite{qacc, PowerOfNoise}, they rely on artificial perturbations or costly human annotation. This highlights the need for an evaluation framework that challenges RAG systems with real-world contradictions, exposing their limitations and improving resilience in complex retrieval scenarios. 




Fact-checking plays a crucial role in combating misinformation, yet most existing datasets assume the availability of gold-standard evidence that aligns with the verdict \cite{FEVER,feverous,mocheg,snopes,pubhealth,multifc}. In reality, retrieved information often presents conflicting perspectives, making automated verification more challenging. The political domain is particularly rich in such complexities, as controversial claims generate both supporting and opposing narratives from diverse sources \cite{nielsen2022mumin, FEVER, PolitifactOslo, FakeNewsNet}. To develop fact-checking systems capable of handling real-world misinformation, it is essential to move beyond idealized settings and expose models to the conflicting and misleading evidence that humans work with in the real-world. 

To bridge this gap, we introduce \textsc{RAGuard}, a benchmark dataset based on political discourse claims and their verifications from PolitiFact incorporating real-world misinformation. Given the prevalence of polarizing and deceptive information in political discourse, we develop an automated pipeline that retrieves relevant yet potentially misleading documents from Reddit via Google Search. Reddit, with its diverse and often controversial user-generated content, serves as a realistic source for challenging retrieval scenarios.

We introduce a novel LLM-guided approach to annotate retrieved documents by simulating a fact-checking exam. This method labels documents as \textit{supporting, misleading,} or \textit{irrelevant} based on their impact on the LLM’s decision, providing a scalable benchmark to evaluate RAG systems in real-world, noisy retrieval scenarios. Each data point in our dataset consists of a claim, a fact-checking verdict, and multiple labeled associated documents. This structure enables a rigorous evaluation of the ability of RAG systems to navigate situations with both noisy and supporting information, reflecting real-world conditions where accurate retrieval cannot be guaranteed. Our benchmark supports verifying robustness on documents solely labeled as misleading or on the full dataset to systematize generalization capabilities in complex scenarios. 


We evaluate widely used LLMs and RAG systems, testing their ability to predict the correct fact-checking verdict across three task configurations: Zero-Context Prediction (given claims with no retrieved documents), Standard RAG (given claims with retrieved documents), and Oracle Retrieval (given claims with their associated documents). Our results reveal that current LLMs are highly vulnerable and lack robustness in real-world scenarios. Performance drops significantly across all configurations when using the \textsc{RAGuard} knowledge base. Notably, incorporating associated documents as context leads to an even steeper decline compared to dynamically retrieved documents, demonstrating how our dataset effectively assigns impactful misinformation to claims. This further exposes the limitations of LLMs in handling misleading content. Qualitative analysis shows that RAG systems are particularly susceptible to overtly misleading information, falling short of human reasoning. Figure \ref{fig:datasetexample} highlights the motivation behind our dataset and the susceptibility of current RAG systems to misleading retrievals. 



In summary, our work advocates for a shift in focus from developing idealized RAG settings to those that better simulate real-world noisy information. We provide a benchmark to evaluate the robustness of RAG systems against misleading retrievals, addressing the gap in naturally-occurring misinformation RAG datasets. Our baseline results reveal the current shortcomings of RAG systems, showing performance worse than zero-shot. We expect our dataset to contribute to the development of more reliable and resilient RAG systems in the future.




\section{Dataset}
We introduce \textsc{RAGuard}, a benchmark for evaluating the robustness of RAG systems in political fact-checking.
\textsc{RAGuard} simulates noisy real-world retrieval settings, where systems must navigate supporting, misleading, and irrelevant evidence. The dataset comprises 2,648 political claims, corresponding fact-checking verdicts, and 16,331 associated documents labeled by their agreement with the verdicts.

\subsection{Definitions}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/newdefinitionspace.png}
    \caption{Taxonomy of terminology to classify different types of evidence, labeled with prior works' contributions (\textit{left}), and our dataset's composition (\textit{right}).}
   
    \label{fig:definition}
\end{figure}

The main task in \textsc{RAGuard} is retrieval-augmented fact-checking, where claims are verified as \textit{true} or \textit{false} based on retrieved documents that may support, mislead, or be irrelevant to the claim. Prior works employ varying terminology to describe the presence of such noise in retrieved contexts or retrieval corpora \cite{PowerOfNoise,qacc,raat,NoiserBench}. To establish consistency, we define a structured taxonomy and align existing definitions (See Figure~\ref{fig:definition}).


Typical RAG datasets, including all prior fact-checking datasets to our knowledge, exclusively contain non-noisy \textit{supporting} documents as associated evidence, leading to overly optimistic performance \cite{PowerOfNoise}. 
Instead of relying solely on answer-containing documents, our dataset adopts a broader notion of supporting evidence.
Specifically, we consider a document to be \textit{supporting} if it provides information that enables an LLM to infer the correct answer, even if it does not explicitly state the ground-truth output. This reflects real-world fact-checking, where human verifiers rely on contextual information rather than single authoritative documents.




We categorize different types of noisy evidence based on whether the information directly conflicts with aspects of the correct prediction. As in prior work \cite{PowerOfNoise,raat}, we include non-conflicting documents in \textsc{RAGuard}, such as irrelevant texts that may hurt performance. However, our primary focus is conflicting documents, which include misleading, fabricated, and unambiguous evidence. 
Previous datasets primarily include conflicting evidence as fabricated or unambiguous documents, oversimplifying real-world complexity and ambiguity (see Section \ref{sec:comparisonwithdatasets} for further discussion) \cite{NoiserBench,qacc,raat}. Notably, no prior work has introduced \textit{misleading} documents.

In \textsc{RAGuard}, misleading documents distort facts through selective framing, omission, or biased presentation, leading the system toward incorrect predictions while still containing partial truths. Unlike fabricated evidence, which is explicitly engineered to contradict the correct prediction (i.e., adversarial perturbations), misleading evidence subtly misguides the model rather than directly opposing it. Additionally, while prior work such as QACC \cite{qacc} introduces unambiguous evidence—a term we adopt to ensure consistency with past research—which includes some naturally conflicting evidence but only for a limited set of unambiguous questions, we focus on more natural yet scalable conflicting evidence.

\begin{table}[h]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{0.7}
    \setlength{\tabcolsep}{3pt}
    \begin{tabularx}{\linewidth}{lccclX}
        \toprule
        \textbf{Dataset} & \textbf{Evidence} & \textbf{Conflicting} & \textbf{Real} & \textbf{Domain} & \textbf{Claims} \\
        & \textbf{Retrieval} & \textbf{Evidence} & \textbf{World} & & \\
        \midrule
        \multicolumn{6}{l}{\textbf{Fact-Checking}} \\
        \midrule
        FEVER \cite{FEVER} & \checkmark & \xmark & \xmark & General & 185K \\
        FEVEROUS \cite{feverous} & \checkmark & \xmark & \xmark & General & 87K \\
        Liar \cite{Liar} & \xmark & \xmark & \checkmark & Political & 12.8K \\
        Mocheg \cite{mocheg} & \checkmark & \xmark & \checkmark & Political & 15.6K \\
        Snopes \cite{snopes} & \checkmark & \xmark & \checkmark & Political & 6.4K \\
        PUBHEALTH \cite{pubhealth} & \checkmark & \xmark & \checkmark & Health & 11.8K \\
        MultiFC \cite{multifc} & \checkmark & \xmark & \checkmark & Political & 43.8K \\
        \midrule
        \multicolumn{6}{l}{\textbf{Noisy Contexts}} \\
        \midrule
        Power of Noise \cite{PowerOfNoise} & \checkmark & \xmark & \checkmark & General & 10K \\
        RAAT \cite{raat} & \checkmark & \checkmark & \xmark & General & 7.8K \\
        NoiserBench \cite{NoiserBench} & \checkmark & \checkmark & \xmark & General & 4K \\
        QACC \cite{qacc} & \checkmark & \checkmark & \checkmark & General & 1.5K \\
        \midrule
        \textsc{RAGuard} & \checkmark & \checkmark & \checkmark & Political & 2.6K \\
        \bottomrule
    \end{tabularx}
    \caption{Comparison of \textsc{RAGuard} with related fact-checking and RAG datasets. The columns indicate whether the dataset requires automatic evidence retrieval, contains conflicting evidence documents, and consists of naturally occurring real-world claims and evidence, as well as their domain and size.}
    \label{tab:datasets-condensed}
\end{table}



For reference, we provide a list of all defined terms. Each term defines a type of document or piece of evidence.

\begin{enumerate}
    \item \textit{Associated:} any document linked to a claim, regardless of label

  \item \textit{Supporting:}
  aids the system in producing a correct prediction through containing the correct answer explicitly or providing contextual support 
  \item \textit{Noisy:} challenges or disrupt system performance, thereby enhancing robustness
 \item \textit{Conflicting:} contradicts either the correct answer or some aspect of the prediction
 \item \textit{Misleading:}  introduces factual distortions through selective framing, omission, or biased presentation; may contain partial truths
 \item \textit{Fabricated:} synthetically constructed to include factual errors (e.g., adversarial perturbations)
  \item \textit{Unambiguous:} naturally conflicting evidence but only for a limited set of unambiguous questions (special case of \cite{qacc})
\item \textit{Non-Conflicting:} does not directly contradict the correct answer but still introduces noise by distracting the model
\item \textit{Irrelevant:} does not contain specific enough information to determine the correct prediction, despite being topically or semantically related to the query
\item \textit{Random:} unrelated; often introduced through random selection or artificial generation
  
\end{enumerate}








\begin{figure*}[h]
    \centering
    
    \begin{subfigure}{0.35\linewidth}
        \centering
        \renewcommand{\arraystretch}{0.6}
        \begin{tabularx}{\linewidth}{l c}
            \toprule
            \textbf{Statistic} & \textbf{Number} \\ 
            \midrule
            \textbf{Total Claims} & \textbf{2,648} \\ 
            \quad True & 1,333 (50.3\%) \\ 
            \quad False & 1,315 (49.7\%) \\ 
            Avg. Claim Length (words) & 17.6 \\
            \midrule
            \textbf{Total Documents} & \textbf{16,331} \\ 
            \quad Supporting & 2,685 (16.4\%) \\ 
            \quad Misleading & 1,812 (11.1\%) \\ 
            \quad Irrelevant & 11,834 (72.5\%) \\ 
            Avg. Document Length (words) & 161 \\ 
            Avg. Documents Per Claim & 6.2 \\ 
            \midrule
            Claims with Supporting Docs & 955 (36.1\%) \\ 
            Claims with Misleading Docs & 788 (29.8\%) \\ 
            \bottomrule
        \end{tabularx}
        \caption{Main statistics of \textsc{RAGuard}.}
        \label{tab:stats}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.28\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/claims.pdf}
        \caption{Word distribution for claims.}
        \label{fig:claim_chart}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.28\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/documents.pdf}
        \caption{Word distribution for documents.}
        \label{fig:document_chart}
    \end{subfigure}
        
    \caption{Key statistics and word distributions of \textsc{RAGuard}.}
    \label{fig:combined-distribution}
\end{figure*}


\subsection{Comparison with Existing Datasets} \label{sec:comparisonwithdatasets}

Table \ref{tab:datasets-condensed} depicts a comparison of \textsc{RAGuard} with other fact-checking and RAG datasets.








\paragraph{Fact-Checking Datasets}
All existing fact-checking datasets that include evidence retrieval contain only \textit{supporting} documents. While FEVEROUS \cite{feverous} labels some documents as \textit{refute}, this terminology is misleading—these documents actually support the falsehood of the claim rather than providing conflicting evidence. Therefore, while FEVEROUS categorizes evidence into \textit{support} for true claims and \textit{refute} for false claims, it does not include documents that actively contradict the claim’s verdict. Additionally, both FEVER \cite{FEVER} and FEVEROUS \cite{feverous} rely on rewritten Wikipedia statements rather than naturally occurring claims, as noted by \cite{multifc}.

Liar \cite{Liar} and Mocheg \cite{mocheg} are the most similar to our dataset since they also source claims from Politifact. However, Liar \cite{Liar} does not support evidence retrieval as it lacks evidence documents. Mocheg \cite{mocheg} includes only documents cited by Politifact fact-checkers, which support the verdict rather than introducing conflicting or misleading evidence. Other datasets \cite{snopes, pubhealth, multifc} primarily use journalist-written explanations from fact-checking websites, which are structured to justify the verdict rather than reflect the complexity of real-world misinformation.

In contrast, \textsc{RAGuard} explicitly incorporates conflicting evidence, making it more representative of real-world misinformation challenges. Unlike curated fact-checking content, our dataset sources evidence from Reddit discussions, which naturally contain misleading information through diverse viewpoints. This increases the difficulty of RAG by better aligning ot with real-world misinformation.





\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/newconstruct.png} % Replace 'example-image' with your file name
    \caption{\textsc{RAGuard} dataset construction, consisting of three stages to obtain claims and verdicts; associated documents; and labels for the each document's relationship to the claim and verdict.}
    \label{fig:construction}
\end{figure*}

\paragraph{Datasets with Noisy Contexts}
Prior datasets that include \textit{noisy} evidence primarily build on open-domain question answering (QA) datasets \cite{qacc,PowerOfNoise,NoiserBench,raat}. As these datasets differ in how they define and introduce noise, we use our definition framework in Figure \ref{fig:definition} to better distinguish each dataset.


Power of Noise \cite{PowerOfNoise} classifies evidence into gold, relevant, distracting, and random categories. Gold and relevant documents serve as supporting evidence, with \textit{gold} documents being preexisting gold-standard documents and \textit{relevant} documents being newly retrieved documents that explicitly contain the correct answer. Non-conflicting evidence includes distracting documents, which are simply non-gold retrievals and therefore irrelevant. Additionally, Power of Noise is the only dataset that introduces \textit{random} evidence, which is entirely unrelated to the query. Notably, it does not include any \textit{conflicting} evidence.

RAG-Bench \cite{raat} classifies evidence into golden context, irrelevant retrieval noise, relevant retrieval noise, and counterfactual retrieval noise. It constructs \textit{supporting} evidence using gold-standard documents from non-noisy QA datasets. It introduces \textit{fabricated} evidence by modifying documents to contain incorrect answers. While this results in factually incorrect \textit{conflicting} evidence, it does not capture
\textit{misleading} evidence, which may contain partial truths but manipulates the information through selective framing or omission. Both relevant and irrelevant retrieval noise are considered \textit{irrelevant}, as their retrieval via semantic search implies a degree of semantic relatedness to the query while not directly conflicting with the task content. Nonetheless, they can still distract RAG systems.

NoiserBench \cite{NoiserBench} introduces a wide range of types of noise, including inserting counterfactual noise. While this introduces \textit{conflicting} evidence, all noise is artificially constructed, limiting its reflection of real-world misleading information. Like RAG-Bench, it focuses on \textit{fabricated} evidence rather than capturing more complex distortions such as selective framing or omission.

QACC \cite{qacc} employs human annotators to label retrieved documents as conflicting or non-conflicting with answers from AmbigQA. Rather than artificially injecting errors, it includes \textit{conflicting} evidence that directly contradicts the correct answer. However, its reliance on human annotation limits scalability, and its approach does not fully capture real-world ambiguity, as it focuses on clear-cut conflicts (i.e., questions labeled as ``unambiguous'' from AmbigQA) rather than more nuanced misleading evidence.

Unlike these datasets, our work focuses on the political domain, which presents distinct challenges. Political misinformation has tangible consequences, influencing public opinion, policy decisions, and elections \cite{Liar}. Misleading evidence in political discourse is often more nuanced, relying on selective framing rather than outright falsehoods. Furthermore, political fact-checking requires domain-specific reasoning, as claims are frequently shaped by ideological bias and rhetorical strategies.








\subsection{Dataset Structure}
 



\textsc{RAGuard} consists of 2,648 political claims made by U.S. presidential candidates (2000–2024), each labeled as either \textit{true} or \textit{false}, and a knowledge base comprising 16,331 documents. The dataset's key statistics are presented in Table \ref{tab:stats}. Each claim is linked to a set of associated documents, categorized as \textit{supporting}, \textit{misleading}, or \textit{irrelevant}, with an average of 6.2 documents per claim. Notably, the dataset contains more \textit{supporting} documents than \textit{misleading} ones, reflecting that political discussions online are more often aligned with factual information. However, not every claim has both misleading and supporting documents, highlighting the imbalanced nature of political discourse, where certain narratives dominate while others lack counterpoints. The dataset is provided in two comma-separated value (CSV) files, with an example depicted in Tables \ref{tab:structure2} and \ref{tab:structure}.

\textsc{RAGuard} contains a diverse set of claims and documents. Figures \ref{fig:claim_chart} and \ref{fig:document_chart} visualize the most frequent opening words in claims and documents. Claims tend to reference well-known political figures and events, whereas documents frequently begin with questions, reflecting the uncertainty and variability inherent in real-world online discussions. For clarity, words occurring fewer than three times are omitted from the visualization.


\begin{table}[ht]
\centering
\begin{minipage}{\linewidth}
\centering
\small % or \footnotesize
\begin{tabular}{|l|l|}
\hline
\textbf{Claims.csv} &  \\
\hline
\textbf{ID} & 1517 \\
\hline
\textbf{Claim} & Insulin for Medicare beneficiaries dec... \\
\hline
\textbf{Verdict} & True \\
\hline
\textbf{Associated Docs} & [9400, 9402, 9405, …] \\
\hline
\textbf{Document Labels} & [irrelevant, supporting, misleading, ...] \\
\hline
\end{tabular}
\caption{Example of data structure of claims.}\label{tab:structure2}
\end{minipage}


\begin{minipage}{\linewidth}
\centering
\small % or \footnotesize
\begin{tabular}{|l|l|}
\hline
\textbf{Documents.csv} &  \\
\hline
\textbf{ID} & 9405 \\
\hline
\textbf{Title} & My dad is spending \$700/mo on insulin... \\
\hline
\textbf{Full Text} & ... \\
\hline
\textbf{Document Label} & Misleading \\
\hline
\textbf{Associated Claim} & 1517 \\
\hline
\end{tabular}
\caption{Example of data structure of documents.} \label{tab:structure}\end{minipage}
\end{table}





\subsection{Supported Tasks}

To benchmark the performance of current RAG systems in real-world fact-checking scenarios, we define a series of tasks using \textsc{RAGuard}. Each task evaluates a different aspect of RAG system robustness against misleading or conflicting contextual data.

\paragraph{Zero-Context Prediction}
This task assesses a RAG system's ability to fact-check claims without external contextual information. The goal is to evaluate the intrinsic knowledge encoded in each model during pre-training and its effectiveness in verifying claims. This serves as a baseline to measure the impact of external retrieval on performance. 

\paragraph{Standard RAG}
This task simulates a real-time RAG system retrieving documents from the entire dataset corpus. The retrieved documents may include supporting, misleading, or irrelevant information to the claim, introducing retrieval noise. In extreme cases of poor retrieval, models may receive documents unrelated to the claim, mimicking real-world retrieval limitations. 

\paragraph{Oracle Retrieval}
This task provides RAG systems with the ssociated documents for each claim, isolating the impact of the associated documents labeled in our dataset. Unlike the previous task, where retrieval noise is unpredictable, this setting ensures systems receive only the supporting, misleading, and irrelevant documents associated with each claim. This setup evaluates how well models can filter out deceptive content when retrieval errors are controlled. We include two specific evaluations for this task. In the first, each model receives an associated document for a claim, regardless of the categorization as \textit{supporting}, \textit{misleading}, or \textit{irrelevant}, to assess its ability to generalize to complex scenarios where documents have potential to support, mislead, or be irrelevant. In the second, we isolate only instances where the associated document is labeled as \textit{misleading}, which conflicts with the claim’s ground truth fact-checking verdict, testing susceptibility to misleading information.





\section{Dataset Construction}

We construct \textsc{RAGuard} in three stages, depicted in Figure \ref{fig:construction}. First, we collect political claims and fact-checking labels from PolitiFact, a reputable source for verified political fact-checking information. Next, we construct a knowledge base by retrieving relevant Reddit documents via a search engine, leveraging its diverse, real-time content to reflect real-world discourse. Finally, we introduce a novel, scalable LLM-guided approach to classify documents as misleading, supporting, or irrelevant by simulating the LLM taking an exam. The following sections outline the details of each stage.




\begin{table*}[h]
    \centering
    \begin{tabular}{lccccc}
        \toprule
        & Gemini 1.5 Flash & GPT-4o Mini & Claude 3.5 Sonnet & Llama 3 & Mistral \\
        \midrule
        \textbf{Task 1: Zero-Context Prediction} & \textbf{61.06} & \textbf{67.33} & \textbf{74.51} & \textbf{62.50} & \textbf{63.97} \\
        \midrule
        \textbf{Task 2: Standard RAG} \\
        \hspace{0.5em} RAG-1 & 56.68 \textcolor{red}{\scriptsize{$\downarrow$ -4.38\% / -7.2\%}} & 64.80 \textcolor{red}{\scriptsize{$\downarrow$ -2.53\% / -3.8\%}} & 70.09 \textcolor{red}{\scriptsize{$\downarrow$ -4.42\% / -5.9\%}} & 59.40 \textcolor{red}{\scriptsize{$\downarrow$ -3.10\% / -5.0\%}} & 59.14 \textcolor{red}{\scriptsize{$\downarrow$ -4.83\% / -7.5\%}} \\
        \hspace{0.5em} RAG-5 & 57.59 \textcolor{red}{\scriptsize{$\downarrow$ -3.47\% / -5.7\%}} & 65.90 \textcolor{red}{\scriptsize{$\downarrow$ -1.43\% / -2.1\%}} & 68.58 \textcolor{red}{\scriptsize{$\downarrow$ -5.93\% / -8.0\%}} & 61.37 \textcolor{red}{\scriptsize{$\downarrow$ -1.13\% / -1.8\%}} & 58.91 \textcolor{red}{\scriptsize{$\downarrow$ -5.06\% / -7.9\%}} \\
        \midrule
        \textbf{Task 3: Oracle Retrieval} \\
        \hspace{0.5em} All Documents & 52.38 \textcolor{red}{\scriptsize{$\downarrow$ -8.68\% / -14.2\%}} & 53.22 \textcolor{red}{\scriptsize{$\downarrow$ -14.11\% / -20.9\%}} & 51.17 \textcolor{red}{\scriptsize{$\downarrow$ -23.34\% / -31.3\%}} & 61.09 \textcolor{red}{\scriptsize{$\downarrow$ -1.41\% / -2.3\%}} & 51.61 \textcolor{red}{\scriptsize{$\downarrow$ -12.36\% / -19.3\%}} \\
        \hspace{0.5em} Misleading-Only & 30.57 \textcolor{red}{\scriptsize{$\downarrow$ -30.49\% / -49.9\%}} & 45.97 \textcolor{red}{\scriptsize{$\downarrow$ -21.36\% / -31.7\%}} & 37.05 \textcolor{red}{\scriptsize{$\downarrow$ -37.46\% / -50.3\%}} & 36.81 \textcolor{red}{\scriptsize{$\downarrow$ -25.69\% / -41.1\%}} & 28.22 \textcolor{red}{\scriptsize{$\downarrow$ -35.75\% / -55.9\%}} \\
        \bottomrule
    \end{tabular}
    \caption{ Performance of various LLM backbones in RAG setup on three tasks, reported in Accuracy (\%). The red numbers indicate the absolute/relative accuracy drop compared to Zero-Context Prediction (Task 1) under each setting.}
    \label{tab:main}
\end{table*}


\subsection{Claim and Verdict Collection} \label{sec:claim}

To collect claims and fact-checking verdicts for \textsc{RAGuard}, we scrape PolitiFact,\footnote{\url{https://www.politifact.com/}} a reputable platform where expert journalists assess the truthfulness of a wide range of political claims. We focus on claims made by major U.S. presidential candidates from 2000 to 2024 to ensure the inclusion of widely discussed statements that have been frequently fact-checked and debated, generating substantial online discourse and reflecting politically significant information. To facilitate document retrieval, we condense PolitiFact’s six-point truth scale (\textit{true, mostly true, half true, mostly false, false, pants on fire}) into binary labels—\textit{true} and \textit{false}, as it is challenging for a document to specifically mislead a \textit{half true} verdict, undermining our core contribution.





\subsection{Knowledge Base Construction} \label{sec:knowledge}

To construct the \textsc{RAGuard} knowledge base, we employ a multi-step retrieval process that balances coverage, diversity, and realism. First, GPT-4 extracts the keywords from each claim. This keyword expansion ensures a broader and more nuanced search space, increasing the likelihood of retrieving both corroborating and contradicting information. Next, we perform a keyword-based Google Search to retrieve up to ten relevant Reddit posts per claim. Google’s ranking ensures contextual relevance, while Reddit’s user-generated content introduces diverse perspectives, from speculative theories to well-supported arguments. Unlike curated fact-checking datasets, Reddit captures real-world discourse, including misinformation and conflicting viewpoints. This retrieval pipeline creates a realistic testbed for fact-checking, combining GPT-4-assisted keyword expansion with search engine retrieval to mirror the complexities of real-world misinformation challenges.






\subsection{LLM-Guided Document Annotation}

Our work defines a document's role in a RAG system based on its influence on the LLM's decision-making. Unlike prior studies that introduce counterfactual evidence or rely on human annotators, our approach directly evaluates whether a document aids or misleads the LLM in real time. We achieve this by simulating a fact-checking scenario during annotation, treating the LLM as an exam taker.

To generate labels, we simulate the RAG fact-checking process at inference time. Given a claim (Section \ref{sec:claim}) and a retrieved document (Section \ref{sec:knowledge}), GPT-4 classifies the claim as \textit{true} or \textit{false} based on the document’s content. If the classification aligns with the ground-truth, the document is labeled \textit{supporting}; if it contradicts the gold label, it is \textit{misleading}; and if it does not contribute to verification, it is \textit{irrelevant}. By basing labels on the LLM’s actual behavior, our approach ensures document annotations reflect their real impact on fact-checking, providing a scalable, empirically grounded alternative to human annotation.


\section{Baselines}

\subsection{Experimental Setup}

\paragraph{Evaluation} We frame fact-checking as a binary classification task where the model must generate a response that aligns with one of the predefined options. Accuracy, calculated with the gold label serving as the reference, is used to evaluate performance. If a model generates an out-of-scope response that does not match any of the given options, it is treated as an incorrect prediction. 

\paragraph{Implementation Details} We evaluate RAG systems using both open-source and closed-source LLMs to assess their capabilities for real-world applications. For closed-source benchmarks, we test Google’s Gemini 1.5 Flash \cite{geminiteam2024gemini15unlockingmultimodal}, OpenAI’s GPT-4o Mini \cite{openai2024gpt4ocard}, and Anthropic’s Claude 3.5 Sonnet \cite{claude2024} via their respective APIs. For open-source benchmarks, we evaluate Meta's LLama3 8B Instruct \cite{dubey2024llama} and Mistral's Mistral 7B Instruct \cite{jiang2023mistral7b} by running local inference.

In all settings, two-shot examples—one \textit{true} and one \textit{false} claim from \textsc{RAGuard} training data—are provided in the context. In the Standard RAG setting, we employ OpenAI’s {text-embedding-ada-002} for semantic search using the original claim as the query and provide the top one and five retrieved documents as context to the LLM. In the Oracle Retrieval setting, our system only takes one document at a time to find the impact of each document to the result. In our prompt, we specify that contextual documents may not be relevant or correct.
 
\subsection{Results}

Table \ref{tab:main} displays baseline results on \textsc{RAGuard} for three tasks using two open and three closed-source LLMs. 

\paragraph{Zero-Context Prediction}
In the zero-context prediction, RAG systems operate without context documents from the \textsc{RAGuard} knowledge base. We use this as our baseline (a.k.a, zero-shot baseline). All systems achieve the highest accuracy scores, which is counterintuitive, considering this setting does not benefit from retrieval.


\paragraph{Standard RAG}
Performance decreases for all models when integrating retrieval, with all scores falling below the zero-shot baseline. The decline is consistent across both RAG-1 and RAG-5 settings, though the magnitude varies. GPT-4 remains the most robust, exhibiting only a minor drop, while other models, particularly Mistral and Gemini, experience more pronounced declines. Increasing retrieval (RAG-1 to RAG-5) does not consistently improve performance and sometimes worsens it. This suggests that retrieval introduces both useful and misleading information, and when retrieval quality is not optimal, the additional context can confuse rather than help.
These findings challenge the assumption that retrieval always benefits downstream performance and reinforce prior research \citep{relevance, yin2023alcunalargelanguagemodels, PowerOfNoise, xie2024adaptivechameleonstubbornsloth} on the risks of noisy retrieval in high-stakes tasks.

\paragraph{Oracle Retrieval}
The results in the Oracle Retrieval setting reveal a striking trend: incorporating associated documents from \textsc{RAGuard} leads to a significant drop in performance compared to the zero-context baseline. 
This suggests that models are highly sensitive to misleading or irrelevant information.
This trend holds across all models except Llama3, indicating that randomly retrieved \textit{irrelevant} documents are less harmful than \textit{misleading} content. This finding underscores the challenge posed by \textsc{RAGuard}, which systematically tests model robustness misleading information.

In this task's \textit{All Documents} setting, where models receive all associated documents for a claim, performance generally declines, suggesting that models struggle to reconcile conflicting evidence. The impact is even more severe in the \textit{Misleading-Only} setting, where models are provided only with misleading documents that contradict the claim’s ground truth. 
Most models falling to around 30\% accuracy despite the binary nature of the task.
This confirms that models are highly susceptible to misleading information and struggle to distinguish factual content from misinformation, highlighting LLM limitations in handling misleading evidence.



\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/performance.png}
    \caption{Performance decreases from the Zero-Context baseline to Task 2 and 3 when using RAGuard across various models. Results are measured in Accuracy and include relative percent decreases.}
    \label{fig:performance_decreases}
\end{figure} 



\begin{figure}[ht]
    \centering
  
    \includegraphics[width=\linewidth]{figures/retrievalperformance.png}
    \caption{Retrieval Accuracy, Recall, and NDCG at Different Top K Levels}
    \label{fig:retrieval_metrics_line}
\end{figure}



\section{Discussion}
\paragraph{Comparison of Model Robustness}
Figure \ref{fig:performance_decreases} displays the relative performance decreases across different models. 
Notably, Claude 3.5 Sonnet, which achieved the highest accuracy in the zero-context baseline, experienced the greatest decreases when exposed to noisy retrieval.
In the All Documents Oracle Retrieval setting, Claude suffered one of the steepest declines, and its performance dropped even more drastically in the Misleading-Only condition. This suggests that while Claude performs well in ideal conditions, it is particularly susceptible to misleading evidence, struggling to filter out incorrect information when retrieval introduces contradictory or noisy context. This may also be due to its high baseline performance.

Conversely, GPT-4o Mini demonstrated the highest robustness against misleading evidence. Its relative performance drop in the Misleading-Only setting was 31.7\%, significantly lower than the approximate 50\% declines observed for Gemini, Claude, and Mistral. This suggests that GPT-4o Mini is better at handling misleading content and maintaining accuracy in noisy retrieval conditions, highlighting differences in model sensitivity to retrieval-induced noise.


\paragraph{Effect of RAG on Accuracy}
Figure~\ref{fig:performance_decreases} challenges the common assumption that RAG consistently enhances model performance. When retrieval introduces misleading or irrelevant evidence, it actively degrades accuracy, often leading to worse performance than zero-shot baselines.

This is particularly evident in the Misleading-Only condition from Task 3, where each model is intentionally provided with documents contradicting the claim’s ground truth. Across all models, accuracy drops significantly, with an average performance decrease of 45.8\%, highlighting the substantial risks posed by retrieval-induced misinformation. These findings emphasize that retrieval, when not carefully controlled, can be detrimental rather than beneficial, further reinforcing the importance of robust filtering and sufficiency evaluation in RAG systems.




\paragraph{Retrieval Performance}
Retrieval performance is a standard metric in RAG benchmarks, but our dataset focuses on how models handle misleading or conflicting evidence. High retrieval accuracy alone does not ensure reliable answers due to misleading information in the corpus. Nonetheless, to To provide a full view of system behavior, we report both conventional retrieval metrics and a tailored measurement called Misleading Retrieval Recall.


Figure \ref{fig:retrieval_metrics_line} shows Retrieval Precision, Recall, and Normalized Discounted Cumulative Gain (NDCG) for Task 2 (Standard RAG). Recall naturally rises with $K$, while precision decreases. NDCG follows a non-monotonic trend, dipping around $K=10$ before recovering due to relevant items being unevenly distributed across ranked positions, causing reordering as $K$ changes.

We also report Misleading Retrieval Recall—the fraction of claims retrieving at least one misleading document. Task 1 (Zero-Context) scores 0\%, while Task 3 (Oracle Retrieval) is 100\%. In Task 2, RAG-1 scores 21.3\%, increasing to 44.8\% for RAG-5, showing a higher risk of retrieving misleading content when retrieving more documents. As seen in Table~\ref{tab:main}, this correlates with lower overall accuracy.


\paragraph{Qualitative Example}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/discussion.png}
    \caption{Example predictions on \textsc{RAGuard}, compared to the expected human response. Note that each column compares different prediction scenarios based on varying retrieved contexts for the same claim rather than a multi-turn process. \textit{Left:} Each system's classification of a true claim with three progressively misleading documents.
    \textit{Middle:} GPT-4-based system's classification of a false claim with one noisy non-associated document, many noisy non-associated documents, and a misleading document.
    \textit{Right:} GPT-4-based system’s classification of a true claim with a supporting non-associated document, one misleading document along with other supporting non-associated documents, and a misleading document.}
   
    \label{fig:discussion}
\end{figure*}

Figure \ref{fig:discussion} presents example system predictions on \textsc{RAGuard}, illustrating the impact of misleading documents. The left example highlights how misleading documents negatively affect the classification of a true claim. While misleading documents generally degrade system performance compared to zero-shot predictions, their specific influence varies based on their complexity. We distinguish three categories of misleading documents:
\begin{enumerate}
\item Overtly Misleading Document: This category includes documents that are evidently misleading to humans but still lead to incorrect predictions by all RAG systems. For example, in Figure \ref{fig:discussion}, the document falsely comparing California's job growth to the national average misleads all systems (1b), despite their correct zero-shot predictions (1a). This suggests a form of selective bias, where the systems prioritize the provided information simply because it is included in the prompt, even though the instructions explicitly caution against assuming its correctness.

\item Partially True Misleading Document: These documents contain partial truths, making it necessary to apply reasoning to recognize their misleading nature. For example, as shown in Figure \ref{fig:discussion}, one document criticizes unemployment but also states that "official job reports are reporting jobs added" (1c). While this statement supports the claim that 500,000 jobs were added, the document’s overall tone suggests rising unemployment. However, this suggestion is more of an opinion than a fact. Some LLMs, such as GPT-4 and Mistral, were able to reason through this contradiction and classify the claim correctly.

\item Challenging Misleading Document: These documents present significant challenges, even for human annotators. For example, a claim referencing job growth in the 2000s is incorrectly classified because the RAG system retrieves data from 2024, which accurately reports lower job creation (1d). The temporal misalignment in retrieved documents presents a fundamental challenge in this dataset and task.
\end{enumerate}


The middle example demonstrates GPT-4’s ability to filter out noise from retrieved documents that are not associated with the claim but could be considered misleading documents in our dataset (e.g., documents using the same phrasing but referring to different individuals, such as "Harris" instead of "Clinton" in example 2b). Even when five irrelevant documents are retrieved (2c), GPT-4 remains robust. However, when presented with a misleading document from the dataset (2d), GPT-4 fails, reinforcing the dataset’s effectiveness in challenging model performance beyond conventional RAG noise. This further explains the lower accuracy observed in the Oracle Retrieval setting in our baseline experiments.

The right example shows how GPT-4 tends to assign disproportionate weight to misleading documents, allowing them to override even non-associated supporting evidence. In the example, a non-associated document that contains supporting information (3b) enables GPT-4 to correct its initially incorrect zero-shot prediction (3a). However, when a misleading document is retrieved alongside other non-associated supporting documents (3c), the system incorrectly classifies the claim, similar to its behavior when only the misleading document is retrieved (3d). This demonstrates that misleading documents can have a stronger influence on the model’s classification, regardless of the presence of supporting evidence, highlighting a significant vulnerability in RAG systems.


These examples highlight three key findings: (1) LLMs remain highly susceptible to misleading documents, even when their content is transparently incorrect, (2) misleading documents retrieved from the dataset exert a stronger influence than non-associated documents retrieved erroneously, and (3) when misleading documents are present, they can significantly outweigh supporting evidence, leading to incorrect predictions. These findings emphasize the strength and uniqueness of our dataset in evaluating and challenging RAG-based model performance.

\section{Related Work}

\paragraph{Retrieval-Augmented Generation with Noisy Contexts}
Retrieval-Augmented Language Models (RALMs) have demonstrated strong performance across various NLP tasks \citep{RAGPretrain, RAGNLP}. However, their effectiveness is constrained by the retriever’s ability to find supporting information. In real-world applications, retrieval often introduces irrelevant or misleading content, which can significantly degrade model performance \citep{relevance, yin2023alcunalargelanguagemodels, PowerOfNoise, xie2024adaptivechameleonstubbornsloth}. Prior work has identified two primary effects of such noise.

The first is the impact of irrelevant documents on RAG performance \citep{BenchmarkLM}. To make RAG systems more robust to this issue, researchers have explored several strategies, including prompting the language model to generate a rationale connecting retrieved documents to the query \citep{InstructRAG}, employing multi-agent debate systems to identify the most relevant information \citep{LearningToBreak}, and aggregating multiple documents to produce a more reliable final response \citep{RobustRAG}.

The second challenge arises when retrieved documents conflict with an LLM’s internal knowledge \citep{ContextMemoryConflict}. To address this, AstuteRAG introduced an iterative system that consolidates internal and external knowledge, reducing inconsistencies in generated responses \citep{AstuteRAG}. However, this study focuses on disagreements between documents in existing datasets, which do not intentionally mislead RAG systems and do not represent conflicting information in the real world.

Researchers have increasingly attempted to improve RAG robustness through developing datasets that expose the model to conflicting contexts \cite{qacc, PowerOfNoise} and retrieval noises \cite{NoiserBench} that may later be used for adversarial training \cite{raat}. However, existing datasets fail to fully capture the complexities of real-world misinformation. 


In contrast, \textsc{RAGuard} is the first to capture \textit{misleading} context that reflects real-world ambiguities, polarized opinions, and partial truths. Unlike prior datasets that primarily focus on synthetic conflicts or document-model disagreements, \textsc{RAGuard} captures naturally occurring misinformation, making it a more realistic benchmark for evaluating RAG robustness. 

\paragraph{The Limitations of Open-Book QA Datasets}
RAG is studied primarily through open-book question answering (QA), where models answer questions based on retrieved knowledge \cite{gao2024retrievalaugmentedgenerationlargelanguage}. However, most Open-Book QA datasets carefully curate their documents, avoiding noisy information \citep{TriviaQA, HotPotQA, SQUAD, NaturalQuestions, IfQA, SearchQA, Freebase, ScienceQA}. This leads to strong performance in controlled settings, but poor generalization in real-world scenarios, where conflicting documents often degrade performance \citep{PowerOfNoise}.

Some datasets attempt to address this issue by synthetically introducing counterfactual information \cite{raat,NoiserBench}. However, synthetic noise may not fully capture the complexities of real-world misinformation. Others rely on human annotators to identify conflicting documents \cite{qacc}, but this approach is costly and difficult to scale. %Furthermore, \cite{qacc} uses AmbigQA, which is contingent on the existence of unambiguous QA pairs.
\cite{PowerOfNoise} classifies retrieved documents as distracting based on their equivalence to a gold-standard document, but this approach may not reflect the existence of truly deceptive or contradictory information.

In contrast to these approaches, we leverage real-world misinformation by using political fact-checking data, which contains misleading and conflicting information. This allows us to construct a dataset that better reflects the challenges RAG systems face.

\paragraph{Fact-Checking and RAG}
Fact-checking is a well-studied task with datasets sourced from platforms like Twitter \citep{nielsen2022mumin}, Wikipedia \citep{FEVER}, and PolitiFact \citep{PolitifactOslo, Liar, LiarPlus, mocheg, FakeNewsNet}. Given that fact-checking often relies on external evidence, it aligns well with RAG, where retrieval can support the verification or negation of a claim. However, existing fact-checking datasets typically use gold-standard evidence from the same source as the verdict \cite{FEVER,snopes,pubhealth,multifc,mocheg}, meaning there is no exposure to noisy or contradictory evidence. Despite the inherently polarizing nature of online and political discourse, the retrieved evidence in these datasets rarely contradicts the final verdict. Our work introduces noisy and conflicting information into fact-checking datasets, ensuring that retrieved evidence is not always aligned with the verdict. 


\section{Conclusion}

In this paper, we introduce \textsc{RAGuard}, a challenging and diverse fact-checking dataset designed to assess the robustness of RAG systems against misleading retrievals. \textsc{RAGuard} comprises 2,648 claims—1,333 true and 1,315 false—paired with 16,331 documents, averaging 6.2 documents per claim. These documents are labeled using a novel LLM-guided approach that simulates an exam-like evaluation, analyzing how the model processes and interprets retrieved evidence at inference time to determine whether the documents support, mislead, or are irrelevant to the claim.  Unlike prior RAG benchmarks that rely on synthetically noisy data or curated gold-standard documents, \textsc{RAGuard} utilizes real-world evidence, even in cases where no gold-standard documents exist. By incorporating naturally occurring misleading data from Reddit discussions alongside verified evidence and claims from PolitiFact, it mirrors the complexities of real-world misinformation, which is necessary for more robust systems.


Our findings show that the performance of current RAG systems deteriorates significantly when exposed to misleading evidence,  challenging the assumption that retrieval always enhances model accuracy. These results highlight the need for more resilient fact-checking pipelines. Future research should focus on enhancing retrieval robustness through methods such as adversarial retrieval training, which exposes models to misleading evidence during training to improve resilience, and uncertainty-aware retrieval, which prioritizes evidence credibility over mere relevance. Additionally, fact-verification mechanisms that incorporate multi-step reasoning and cross-document consistency checks can mitigate the impact of misleading sources, while confidence calibration techniques may further refine the model’s ability to discern factual inconsistencies.

By providing a challenging yet realistic benchmark, \textsc{RAGuard} encourages the development of more sophisticated retrieval-based fact-checking methodologies. We hope this dataset will facilitate progress in designing retrieval pipelines that are not only effective but also resistant to misinformation, ultimately contributing to more reliable and trustworthy AI systems.


\clearpage

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main-arxiv}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

% \section{Example Appendix}
\label{sec:appendix}


\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
