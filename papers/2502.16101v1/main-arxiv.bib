% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@inproceedings{yang2024rag,
  title={Im-rag: Multi-round retrieval-augmented generation through learning inner monologues},
  author={Yang, Diji and Rao, Jinmeng and Chen, Kezhen and Guo, Xiaoyuan and Zhang, Yawen and Yang, Jie and Zhang, Yi},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={730--740},
  year={2024}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}


@misc{IfQA,
      title={IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions}, 
      author={Wenhao Yu and Meng Jiang and Peter Clark and Ashish Sabharwal},
      year={2023},
      eprint={2305.14010},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14010}, 
}


@misc{TriviaQA,
      title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}, 
      author={Mandar Joshi and Eunsol Choi and Daniel S. Weld and Luke Zettlemoyer},
      year={2017},
      eprint={1705.03551},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1705.03551}, 
}

@misc{HotPotQA,
      title={HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering}, 
      author={Zhilin Yang and Peng Qi and Saizheng Zhang and Yoshua Bengio and William W. Cohen and Ruslan Salakhutdinov and Christopher D. Manning},
      year={2018},
      eprint={1809.09600},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1809.09600}, 
}

@misc{ScienceQA,
      title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering}, 
      author={Pan Lu and Swaroop Mishra and Tony Xia and Liang Qiu and Kai-Wei Chang and Song-Chun Zhu and Oyvind Tafjord and Peter Clark and Ashwin Kalyan},
      year={2022},
      eprint={2209.09513},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2209.09513}, 
}

@misc{RealTimeQA,
      title={RealTime QA: What's the Answer Right Now?}, 
      author={Jungo Kasai and Keisuke Sakaguchi and Yoichi Takahashi and Ronan Le Bras and Akari Asai and Xinyan Yu and Dragomir Radev and Noah A. Smith and Yejin Choi and Kentaro Inui},
      year={2024},
      eprint={2207.13332},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2207.13332}, 
}


@article{SearchQA,
  title={Searchqa: A new q\&a dataset augmented with context from a search engine},
  author={Dunn, Matthew and Sagun, Levent and Higgins, Mike and Guney, V Ugur and Cirik, Volkan and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1704.05179},
  year={2017}
}

@misc{SQUAD,
      title={SQuAD: 100,000+ Questions for Machine Comprehension of Text}, 
      author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
      year={2016},
      eprint={1606.05250},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1606.05250}, 
}

@article{NaturalQuestions,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1026",
    doi = "10.1162/tacl_a_00276",
    pages = "452--466",
    abstract = "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.",
}

@inproceedings{Freebase,
    title = "Semantic Parsing on {F}reebase from Question-Answer Pairs",
    author = "Berant, Jonathan  and
      Chou, Andrew  and
      Frostig, Roy  and
      Liang, Percy",
    editor = "Yarowsky, David  and
      Baldwin, Timothy  and
      Korhonen, Anna  and
      Livescu, Karen  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1160",
    pages = "1533--1544",
}

@inproceedings{FactCheckingDef,
    title = "Fact Checking: Task definition and dataset construction",
    author = "Vlachos, Andreas  and
      Riedel, Sebastian",
    editor = "Danescu-Niculescu-Mizil, Cristian  and
      Eisenstein, Jacob  and
      McKeown, Kathleen  and
      Smith, Noah A.",
    booktitle = "Proceedings of the {ACL} 2014 Workshop on Language Technologies and Computational Social Science",
    month = jun,
    year = "2014",
    address = "Baltimore, MD, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-2508",
    doi = "10.3115/v1/W14-2508",
    pages = "18--22",
}


@inbook{FakeTilMake,
	abstract = {The modern bloom of social media has propelled a new pattern of information propagation termed push journalism, where a certain piece of news is shoved in the faces of as many people as possible with a sliver of hope that it will reach the people who need that information the most. This form of news reporting, especially via social media campaigns has boosted the access and fabrication of bogus reporting, or what is referred to as fake news. Fake news, in the form of clickbait, hoax, satire, propaganda, hyperpartisan, deepfakes, or simply unreliable news has the power of influencing its readers to a dangerous extent, predominantly causing political, socio-economic, or psychological harm. In this chapter, we analyze the meaning of fake news in the world of social media, the various forms it can take, what causes its spread, and what are the rudimentary signs of such fake news. We will walk through a comparative study of the state-of-the-art deep learning models to approach the tasks of identifying phony information, verifying the validity of various claims and facts, catching fake content, and so on. The exposition will especially elucidate the adversarial approaches in deep learning to detect counterfeit content that could come in any form like text, images, videos, or audio. In doing so, we establish the importance of generating plausible and understandable explanations for model predictions with a special emphasis on algorithm fairness. With the fact that deep learning methods rely on comparatively larger datasets of top-notch quality, this chapter will also highlight the availability of relevant datasets in this space, as well as share pointers to curate one if needed. Even with sufficient data, however, detection problems in this domain are especially challenging since spammers and fake content generators are working tirelessly to evolve their strategies in parallel to the advancement in detection mechanisms. We will further shed some light on some recent and upcoming trends from the aspect of fake news contributors, and critically evaluate how our current state-of-the-art deep learning techniques fare against those. In closing, we will leave readers with some thoughts on future directions for the development of better and smarter fake news detectors.},
	address = {Cham},
	author = {Misra, Rishabh and Grover, Jigyasa},
	booktitle = {Deep Learning for Social Media Data Analytics},
	doi = {10.1007/978-3-031-10869-3_12},
	editor = {Hong, Tzung-Pei and Serrano-Estrada, Leticia and Saxena, Akrati and Biswas, Anupam},
	isbn = {978-3-031-10869-3},
	pages = {213--235},
	publisher = {Springer International Publishing},
	title = {Do Not `Fake It Till You Make It'! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning},
	url = {https://doi.org/10.1007/978-3-031-10869-3_12},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-031-10869-3_12}}


@article{PolitifactOslo,
  title={The PolitiFact-Oslo Corpus: A New Dataset for Fake News Analysis and Detection},
  author={Nele P{\~o}ldvere and Md. Zia Uddin and Aleena Thomas},
  journal={Inf.},
  year={2023},
  volume={14},
  pages={627},
  url={https://api.semanticscholar.org/CorpusID:265420523}
}

@inproceedings{Liar,
    title = "{``}Liar, Liar Pants on Fire{''}: A New Benchmark Dataset for Fake News Detection",
    author = "Wang, William Yang",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-2067",
    doi = "10.18653/v1/P17-2067",
    pages = "422--426",
    abstract = "Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present LIAR: a new, publicly available dataset for fake news detection. We collected a decade-long, 12.8K manually labeled short statements in various contexts from PolitiFact.com, which provides detailed analysis report and links to source documents for each case. This dataset can be used for fact-checking research as well. Notably, this new dataset is an order of magnitude larger than previously largest public fake news datasets of similar type. Empirically, we investigate automatic fake news detection based on surface-level linguistic patterns. We have designed a novel, hybrid convolutional neural network to integrate meta-data with text. We show that this hybrid approach can improve a text-only deep learning model.",
}

@inproceedings{LiarPlus,
    title = "Where is Your Evidence: Improving Fact-checking by Justification Modeling",
    author = "Alhindi, Tariq  and
      Petridis, Savvas  and
      Muresan, Smaranda",
    editor = "Thorne, James  and
      Vlachos, Andreas  and
      Cocarascu, Oana  and
      Christodoulopoulos, Christos  and
      Mittal, Arpit",
    booktitle = "Proceedings of the First Workshop on Fact Extraction and {VER}ification ({FEVER})",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5513",
    doi = "10.18653/v1/W18-5513",
    pages = "85--90",
    abstract = "Fact-checking is a journalistic practice that compares a claim made publicly against trusted sources of facts. Wang (2017) introduced a large dataset of validated claims from the POLITIFACT.com website (LIAR dataset), enabling the development of machine learning approaches for fact-checking. However, approaches based on this dataset have focused primarily on modeling the claim and speaker-related metadata, without considering the evidence used by humans in labeling the claims. We extend the LIAR dataset by automatically extracting the justification from the fact-checking article used by humans to label a given claim. We show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half true, mostly true, true).",
}

@inproceedings{FEVER,
    title = "{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification",
    author = "Thorne, James  and
      Vlachos, Andreas  and
      Christodoulopoulos, Christos  and
      Mittal, Arpit",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1074",
    doi = "10.18653/v1/N18-1074",
    pages = "809--819",
    abstract = "In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.",
}

@inproceedings{PowerOfNoise, series={SIGIR 2024},
   title={The Power of Noise: Redefining Retrieval for RAG Systems},
   url={http://dx.doi.org/10.1145/3626772.3657834},
   DOI={10.1145/3626772.3657834},
   booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   publisher={ACM},
   author={Cuconasu, Florin and Trappolini, Giovanni and Siciliano, Federico and Filice, Simone and Campagnano, Cesare and Maarek, Yoelle and Tonellotto, Nicola and Silvestri, Fabrizio},
   year={2024},
   month=jul, pages={719–729},
   collection={SIGIR 2024} }

@misc{NoiseSkew,
      title={How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?}, 
      author={Siye Wu and Jian Xie and Jiangjie Chen and Tinghui Zhu and Kai Zhang and Yanghua Xiao},
      year={2024},
      eprint={2404.03302},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.03302}, 
}

@misc{AstuteRAG,
      title={Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models}, 
      author={Fei Wang and Xingchen Wan and Ruoxi Sun and Jiefeng Chen and Sercan Ö. Arık},
      year={2024},
      eprint={2410.07176},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.07176}, 
}

@misc{CIFARN,
      title={Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations}, 
      author={Jiaheng Wei and Zhaowei Zhu and Hao Cheng and Tongliang Liu and Gang Niu and Yang Liu},
      year={2022},
      eprint={2110.12088},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.12088}, 
}

@misc{ContextMemoryConflict,
      title={Studying Large Language Model Behaviors Under Context-Memory Conflicts With Real Documents}, 
      author={Evgenii Kortukov and Alexander Rubinstein and Elisa Nguyen and Seong Joon Oh},
      year={2024},
      eprint={2404.16032},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.16032}, 
}


@misc{xiong2024benchmarkingretrievalaugmentedgenerationmedicine,
      title={Benchmarking Retrieval-Augmented Generation for Medicine}, 
      author={Guangzhi Xiong and Qiao Jin and Zhiyong Lu and Aidong Zhang},
      year={2024},
      eprint={2402.13178},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.13178}, 
}

@article{guha2024legalbench,
  title={Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models},
  author={Guha, Neel and Nyarko, Julian and Ho, Daniel and R{\'e}, Christopher and Chilton, Adam and Chohlas-Wood, Alex and Peters, Austin and Waldon, Brandon and Rockmore, Daniel and Zambrano, Diego and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@InProceedings{RAGPretrain,
  title = 	 {Retrieval Augmented Language Model Pre-Training},
  author =       {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3929--3938},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/guu20a/guu20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/guu20a.html},
  abstract = 	 {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.}
}


@misc{RAGNLP,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}

@misc{petroni2020contextaffectslanguagemodels,
      title={How Context Affects Language Models' Factual Predictions}, 
      author={Fabio Petroni and Patrick Lewis and Aleksandra Piktus and Tim Rocktäschel and Yuxiang Wu and Alexander H. Miller and Sebastian Riedel},
      year={2020},
      eprint={2005.04611},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.04611}, 
}

@misc{gao2024retrievalaugmentedgenerationlargelanguage,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10997}, 
}

@misc{xie2024adaptivechameleonstubbornsloth,
      title={Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts}, 
      author={Jian Xie and Kai Zhang and Jiangjie Chen and Renze Lou and Yu Su},
      year={2024},
      eprint={2305.13300},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13300}, 
}

@misc{feverous,
      title={FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information}, 
      author={Rami Aly and Zhijiang Guo and Michael Schlichtkrull and James Thorne and Andreas Vlachos and Christos Christodoulopoulos and Oana Cocarascu and Arpit Mittal},
      year={2021},
      eprint={2106.05707},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.05707}, 
}

@inproceedings{thorne-vlachos-2018-automated,
    title = "Automated Fact Checking: Task Formulations, Methods and Future Directions",
    author = "Thorne, James  and
      Vlachos, Andreas",
    editor = "Bender, Emily M.  and
      Derczynski, Leon  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1283/",
    pages = "3346--3359",
    abstract = "The recently increased focus on misinformation has stimulated research in fact checking, the task of assessing the truthfulness of a claim. Research in automating this task has been conducted in a variety of disciplines including natural language processing, machine learning, knowledge representation, databases, and journalism. While there has been substantial progress, relevant papers and articles have been published in research communities that are often unaware of each other and use inconsistent terminology, thus impeding understanding and further progress. In this paper we survey automated fact checking research stemming from natural language processing and related disciplines, unifying the task formulations and methodologies across papers and authors. Furthermore, we highlight the use of evidence as an important distinguishing factor among them cutting across task formulations and methods. We conclude with proposing avenues for future NLP research on automated fact checking."
}


@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR}
}

@inproceedings{karpukhin-etal-2020-dense,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550/",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
}

@inproceedings{ni-etal-2022-large,
    title = "Large Dual Encoders Are Generalizable Retrievers",
    author = "Ni, Jianmo  and
      Qu, Chen  and
      Lu, Jing  and
      Dai, Zhuyun  and
      Hernandez Abrego, Gustavo  and
      Ma, Ji  and
      Zhao, Vincent  and
      Luan, Yi  and
      Hall, Keith  and
      Chang, Ming-Wei  and
      Yang, Yinfei",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.669/",
    doi = "10.18653/v1/2022.emnlp-main.669",
    pages = "9844--9855",
    abstract = "It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dot-product between a query vector and a passage vector, is too limited compared to models with fine-grained interactions between the query and the passage. In this paper, we challenge this belief by scaling up the size of the dual encoder model \textit{while keeping the bottleneck layer as a single dot-product with a fixed size.} With multi-stage training, scaling up the model size brings significant improvement on a variety of retrieval tasks, especially for out-of-domain generalization. We further analyze the impact of the bottleneck layer and demonstrate diminishing improvement when scaling up the embedding size. Experimental results show that our dual encoders, \textbf{G}eneralizable \textbf{T}5-based dense \textbf{R}etrievers (GTR), outperform previous sparse and dense retrievers on the BEIR dataset significantly. Most surprisingly, our ablation study finds that GTR is very data efficient, as it only needs 10{\%} of MS Marco supervised data to match the out-of-domain performance of using all supervised data."
}

@misc{RobustRAG,
      title={Certifiably Robust RAG against Retrieval Corruption}, 
      author={Chong Xiang and Tong Wu and Zexuan Zhong and David Wagner and Danqi Chen and Prateek Mittal},
      year={2024},
      eprint={2405.15556},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.15556}, 
}

@misc{PoisonedRAG,
      title={PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models}, 
      author={Wei Zou and Runpeng Geng and Binghui Wang and Jinyuan Jia},
      year={2024},
      eprint={2402.07867},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2402.07867}, 
}

@misc{InstructRAG,
      title={InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales}, 
      author={Zhepei Wei and Wei-Lin Chen and Yu Meng},
      year={2024},
      eprint={2406.13629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.13629}, 
}

@misc{LearningToBreak,
      title={Learning to Break: Knowledge-Enhanced Reasoning in Multi-Agent Debate System}, 
      author={Haotian Wang and Xiyuan Du and Weijiang Yu and Qianglong Chen and Kun Zhu and Zheng Chu and Lian Yan and Yi Guan},
      year={2024},
      eprint={2312.04854},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.04854}, 
}

@misc{BenchmarkLM,
      title={Benchmarking Large Language Models in Retrieval-Augmented Generation}, 
      author={Jiawei Chen and Hongyu Lin and Xianpei Han and Le Sun},
      year={2023},
      eprint={2309.01431},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.01431}, 
}

@inproceedings{mocheg, series={SIGIR ’23},
   title={End-to-End Multimodal Fact-Checking and Explanation Generation: A Challenging Dataset and Models},
   url={http://dx.doi.org/10.1145/3539618.3591879},
   DOI={10.1145/3539618.3591879},
   booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   publisher={ACM},
   author={Yao, Barry Menglong and Shah, Aditya and Sun, Lichao and Cho, Jin-Hee and Huang, Lifu},
   year={2023},
   month=jul, pages={2733–2743},
   collection={SIGIR ’23} }


@misc{NoiserBench,
      title={Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models}, 
      author={Jinyang Wu and Feihu Che and Chuyuan Zhang and Jianhua Tao and Shuai Zhang and Pengpeng Shao},
      year={2024},
      eprint={2408.13533},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.13533}, 
}
@misc{FakeNewsNet,
      title={FakeNewsNet: A Data Repository with News Content, Social Context and Spatialtemporal Information for Studying Fake News on Social Media}, 
      author={Kai Shu and Deepak Mahudeswaran and Suhang Wang and Dongwon Lee and Huan Liu},
      year={2019},
      eprint={1809.01286},
      archivePrefix={arXiv},
      primaryClass={cs.SI},
      url={https://arxiv.org/abs/1809.01286}, 
}

@inproceedings{nielsen2022mumin,
  title={Mumin: A large-scale multilingual multimodal fact-checked misinformation social network dataset},
  author={Nielsen, Dan S and McConville, Ryan},
  booktitle={Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval},
  pages={3141--3153},
  year={2022}
}

@inproceedings{snopes,
    title = "A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking",
    author = "Hanselowski, Andreas  and
      Stab, Christian  and
      Schulz, Claudia  and
      Li, Zile  and
      Gurevych, Iryna",
    editor = "Bansal, Mohit  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K19-1046/",
    doi = "10.18653/v1/K19-1046",
    pages = "493--503",
    abstract = "Automated fact-checking based on machine learning is a promising approach to identify false information distributed on the web. In order to achieve satisfactory performance, machine learning methods require a large corpus with reliable annotations for the different tasks in the fact-checking process. Having analyzed existing fact-checking corpora, we found that none of them meets these criteria in full. They are either too small in size, do not provide detailed annotations, or are limited to a single domain. Motivated by this gap, we present a new substantially sized mixed-domain corpus with annotations of good quality for the core fact-checking tasks: document retrieval, evidence extraction, stance detection, and claim validation. To aid future corpus construction, we describe our methodology for corpus creation and annotation, and demonstrate that it results in substantial inter-annotator agreement. As baselines for future research, we perform experiments on our corpus with a number of model architectures that reach high performance in similar problem settings. Finally, to support the development of future models, we provide a detailed error analysis for each of the tasks. Our results show that the realistic, multi-domain setting defined by our data poses new challenges for the existing models, providing opportunities for considerable improvement by future systems."
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}


@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}


@article{geminiteam2024gemini15unlockingmultimodal,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Gemini},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}


@article{openai2024gpt4ocard,
  title={Gpt-4o system card},
  author={OpenAI},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@misc{claude2024,
	title={Claude 3.5 sonnet},
	url={https://www.anthropic.com/news/claude-3-5-sonnet},
	author={Anthropic},
	month={June},
	year={2024}
}

@inproceedings{pubhealth,
    title = "Explainable Automated Fact-Checking for Public Health Claims",
    author = "Kotonya, Neema  and
      Toni, Francesca",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.623/",
    doi = "10.18653/v1/2020.emnlp-main.623",
    pages = "7740--7754",
    abstract = "Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of fact-checking studies focus exclusively on political claims. Very little research explores fact-checking for other topics, specifically subject matters for which expertise is required. We present the first study of explainable fact-checking for claims which require specific expertise. For our case study we choose the setting of public health. To support this case study we construct a new dataset PUBHEALTH of 11.8K claims accompanied by journalist crafted, gold standard explanations (i.e., judgments) to support the fact-check labels for claims. We explore two tasks: veracity prediction and explanation generation. We also define and evaluate, with humans and computationally, three coherence properties of explanation quality. Our results indicate that, by training on in-domain data, gains can be made in explainable, automated fact-checking for claims which require specific expertise."
}


@inproceedings{multifc,
    title = "{M}ulti{FC}: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims",
    author = "Augenstein, Isabelle  and
      Lioma, Christina  and
      Wang, Dongsheng  and
      Chaves Lima, Lucas  and
      Hansen, Casper  and
      Hansen, Christian  and
      Simonsen, Jakob Grue",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1475/",
    doi = "10.18653/v1/D19-1475",
    pages = "4685--4697",
    abstract = "We contribute the largest publicly available dataset of naturally occurring factual claims for the purpose of automatic claim verification. It is collected from 26 fact checking websites in English, paired with textual sources and rich metadata, and labelled for veracity by human expert journalists. We present an in-depth analysis of the dataset, highlighting characteristics and challenges. Further, we present results for automatic veracity prediction, both with established baselines and with a novel method for joint ranking of evidence pages and predicting veracity that outperforms all baselines. Significant performance increases are achieved by encoding evidence, and by modelling metadata. Our best-performing model achieves a Macro F1 of 49.2{\%}, showing that this is a challenging testbed for claim veracity prediction."
}

@misc{qacc,
      title={Open Domain Question Answering with Conflicting Contexts}, 
      author={Siyi Liu and Qiang Ning and Kishaloy Halder and Wei Xiao and Zheng Qi and Phu Mon Htut and Yi Zhang and Neha Anna John and Bonan Min and Yassine Benajiba and Dan Roth},
      year={2024},
      eprint={2410.12311},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.12311}, 
}

@misc{raat,
      title={Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training}, 
      author={Feiteng Fang and Yuelin Bai and Shiwen Ni and Min Yang and Xiaojun Chen and Ruifeng Xu},
      year={2024},
      eprint={2405.20978},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.20978}, 
}

@misc{yin2023alcunalargelanguagemodels,
      title={ALCUNA: Large Language Models Meet New Knowledge}, 
      author={Xunjian Yin and Baizhou Huang and Xiaojun Wan},
      year={2023},
      eprint={2310.14820},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.14820}, 
}

@inproceedings{relevance,
author = {Sauchuk, Artsiom and Thorne, James and Halevy, Alon and Tonellotto, Nicola and Silvestri, Fabrizio},
title = {On the Role of Relevance in Natural Language Processing Tasks},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532034},
doi = {10.1145/3477495.3532034},
abstract = {Many recent Natural Language Processing (NLP) task formulations, such as question answering and fact verification, are implemented as a two-stage cascading architecture. In the first stage an IR system retrieves "relevant'' documents containing the knowledge, and in the second stage an NLP system performs reasoning to solve the task. Optimizing the IR system for retrieving relevant documents ensures that the NLP system has sufficient information to operate over. These recent NLP task formulations raise interesting and exciting challenges for IR, where the end-user of an IR system is not a human with an information need, but another system exploiting the documents retrieved by the IR system to perform reasoning and address the user information need. Among these challenges, as we will show, is that noise from the IR system, such as retrieving spurious or irrelevant documents, can negatively impact the accuracy of the downstream reasoning module. Hence, there is the need to balance maximizing relevance while minimizing noise in the IR system. This paper presents experimental results on two NLP tasks implemented as a two-stage cascading architecture. We show how spurious or irrelevant retrieved results from the first stage can induce errors in the second stage. We use these results to ground our discussion of the research challenges that the IR community should address in the context of these knowledge-intensive NLP tasks.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1785–1789},
numpages = {5},
keywords = {effectiveness, ir, neural databases, nlp, relevance},
location = {Madrid, Spain},
series = {SIGIR '22}
}

