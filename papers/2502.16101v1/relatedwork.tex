\section{Related Work}
\paragraph{Retrieval-Augmented Generation with Noisy Contexts}
Retrieval-Augmented Language Models (RALMs) have demonstrated strong performance across various NLP tasks \citep{RAGPretrain, RAGNLP}. However, their effectiveness is constrained by the retriever’s ability to find supporting information. In real-world applications, retrieval often introduces irrelevant or misleading content, which can significantly degrade model performance \citep{relevance, yin2023alcunalargelanguagemodels, PowerOfNoise, xie2024adaptivechameleonstubbornsloth}. Prior work has identified two primary effects of such noise.

The first is the impact of irrelevant documents on RAG performance \citep{BenchmarkLM}. To make RAG systems more robust to this issue, researchers have explored several strategies, including prompting the language model to generate a rationale connecting retrieved documents to the query \citep{InstructRAG}, employing multi-agent debate systems to identify the most relevant information \citep{LearningToBreak}, and aggregating multiple documents to produce a more reliable final response \citep{RobustRAG}.

The second challenge arises when retrieved documents conflict with an LLM’s internal knowledge \citep{ContextMemoryConflict}. To address this, AstuteRAG introduced an iterative system that consolidates internal and external knowledge, reducing inconsistencies in generated responses \citep{AstuteRAG}. However, this study focuses on disagreements between documents in existing datasets, which do not intentionally mislead RAG systems and do not represent conflicting information in the real world.

Researchers have increasingly attempted to improve RAG robustness through developing datasets that expose the model to conflicting contexts \cite{qacc, PowerOfNoise} and retrieval noises \cite{NoiserBench} that may later be used for adversarial training \cite{raat}. However, existing datasets fail to fully capture the complexities of real-world misinformation. 


In contrast, \textsc{RAGuard} is the first to capture \textit{misleading} context that reflects real-world ambiguities, polarized opinions, and partial truths. Unlike prior datasets that primarily focus on synthetic conflicts or document-model disagreements, \textsc{RAGuard} captures naturally occurring misinformation, making it a more realistic benchmark for evaluating RAG robustness. 

\paragraph{The Limitations of Open-Book QA Datasets}
RAG is studied primarily through open-book question answering (QA), where models answer questions based on retrieved knowledge \cite{gao2024retrievalaugmentedgenerationlargelanguage}. However, most Open-Book QA datasets carefully curate their documents, avoiding noisy information \citep{TriviaQA, HotPotQA, SQUAD, NaturalQuestions, IfQA, SearchQA, Freebase, ScienceQA}. This leads to strong performance in controlled settings, but poor generalization in real-world scenarios, where conflicting documents often degrade performance \citep{PowerOfNoise}.

Some datasets attempt to address this issue by synthetically introducing counterfactual information \cite{raat,NoiserBench}. However, synthetic noise may not fully capture the complexities of real-world misinformation. Others rely on human annotators to identify conflicting documents \cite{qacc}, but this approach is costly and difficult to scale. %Furthermore, \cite{qacc} uses AmbigQA, which is contingent on the existence of unambiguous QA pairs.
\cite{PowerOfNoise} classifies retrieved documents as distracting based on their equivalence to a gold-standard document, but this approach may not reflect the existence of truly deceptive or contradictory information.

In contrast to these approaches, we leverage real-world misinformation by using political fact-checking data, which contains misleading and conflicting information. This allows us to construct a dataset that better reflects the challenges RAG systems face.

\paragraph{Fact-Checking and RAG}
Fact-checking is a well-studied task with datasets sourced from platforms like Twitter \citep{nielsen2022mumin}, Wikipedia \citep{FEVER}, and PolitiFact \citep{PolitifactOslo, Liar, LiarPlus, mocheg, FakeNewsNet}. Given that fact-checking often relies on external evidence, it aligns well with RAG, where retrieval can support the verification or negation of a claim. However, existing fact-checking datasets typically use gold-standard evidence from the same source as the verdict \cite{FEVER,snopes,pubhealth,multifc,mocheg}, meaning there is no exposure to noisy or contradictory evidence. Despite the inherently polarizing nature of online and political discourse, the retrieved evidence in these datasets rarely contradicts the final verdict. Our work introduces noisy and conflicting information into fact-checking datasets, ensuring that retrieved evidence is not always aligned with the verdict.