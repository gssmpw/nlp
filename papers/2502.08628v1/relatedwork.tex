\section{Related Works}
Our generalization  of McDiarmid's inequality from the original bounded difference case in \cite{mcdiarmid1989method} to a much larger class of bounding functions can be viewed as an extension  of the result in \cite{kontorovich2014concentration}, which studied Lipschitz functions on a space with finite sub-Gaussian-diameter.  More recently,  \cite{maurer2021concentration}  took a different approach and extended  McDiarmid's inequality to sub-Gaussian and sub-exponential cases by using  entropy methods.  Our results, which apply to a wider range of tail behaviors,  also distinguished by the minimal assumptions they make regarding the one-parameter differences bounding functions.  In particular, our framework applies to various forms of local-Lipschitz behavior that naturally arises in a number of applications and which are not covered by   the methods of \cite{kontorovich2014concentration,maurer2021concentration}.   This is especially important in our study of sample reuse, where we employ bounding functions on $X$-differences that are not uniform in the $Y$'s.  Our work shares some similarities with  error analysis of generative adversarial networks (GANs) \cite{liang2021well}, including works by  \cite{huang2022error,biau2021some} that consider distributions with unbounded support. However our methods, which can also be applied to GANs,  allow for one to work directly with   unbounded functions, rather than using truncation arguments as in \cite{huang2022error},  or being restricted to the  sub-Gaussian  case as in \cite{biau2021some}.  We also note that the effect of sample reuse in the GAN case, i.e., unbalanced number of samples from the data source and generator, is much simpler to study than the more general (non-additive) form of sample reuse considered here.

As an  application of our theorems, we study denoising score matching  in Section \ref{sec:DSM}. Since their introduction in \cite{songscore}, there has been great interest in the analysis of score-based generative models that use SDE noising dynamics, and in  DSM in particular.   The works of 
\cite{chen2023improved,pedrotti2023improved,bentonnearly,li2024towards,li2024sharp,potaptchik2024linear,chen2024equivariant,mimikos2024score} all derive convergence results or error bounds for score-based generative models under various assumptions on the score-matching error.  Closest to our work is \cite{oko2023diffusion}, which  obtains statistical error bounds in the form of expected total variation and $1$-Wasserstein distance, in contrast to the concentration inequalities produced by our methods.  The key  inputs into their statistical bounds, such as the covering number bound in their Lemma 4.2, assume a uniformly bounded score-network hypothesis class; see their definition of $\mathcal{S}$ above their Lemma 4.2.  While the authors  prove the  necessary approximation results  to justify and account for the errors made in the restriction to bounded functions, a benefit of our framework is that one can work directly with  unbounded objective functions, including an unbounded score model; the goal of our   Section \ref{sec:DSM} is to demonstrate that advantage.  These strengths carry over to other stochastic optimization problems that are naturally posed on an unbounded domain and with an unbounded objective function.