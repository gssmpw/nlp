\documentclass[twoside,11pt,preprint]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{jmlr2e}

\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{comment}

\newtheorem{assumption}[theorem]{Assumption}

% Definitions of handy macros can go here


% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\jmlrheading{--}{--}{1-\pageref{LastPage}}{--; Revised --}{--}{--}{Jeremiah Birrell}

% Short headings should be running head and authors last names

\ShortHeadings{Concentration Inequalities  for Stochastic Optimization of Unbounded Objectives}{Birrell}
\firstpageno{1}

\begin{document}






\title{Concentration Inequalities  for the  Stochastic Optimization of Unbounded Objectives with Application to Denoising Score Matching}

\author{ \name   Jeremiah Birrell   \email jbirrell@txstate.edu \\
   \addr  Department of Mathematics\\
  Texas State University\\
  San Marcos, TX 78666,  USA 
}

\editor{}

\maketitle




\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
 We derive novel concentration inequalities that bound the  statistical error  for a large class of stochastic optimization problems, focusing on the case of unbounded objective functions. Our derivations utilize the following  tools: 1)  A new  form of McDiarmid’s inequality  that is based on sample-dependent  one-component-difference bounds and which leads to a novel uniform law of large numbers result for unbounded functions. 2) A Rademacher complexity bound for families of functions that satisfy an appropriate local-Lipschitz property. As an application of these results, we derive statistical error bounds for denoising score matching (DSM), an application that inherently requires one to consider unbounded objective functions, even when the data distribution has bounded support.  In addition, our results establish the benefit of sample-reuse in algorithms that employ easily-sampled auxiliary random variables in addition to the training data, e.g., as in DSM, which uses auxiliary Gaussian random variables. 

\end{abstract}


\begin{keywords}
stochastic optimization, uniform law of large numbers, concentration inequalities, statistical consistency,  denoising score matching
\end{keywords}





\section{Introduction}
In this work we derive a novel generalization of McDiarmid’s inequality along with a uniform law of large numbers (ULLN) result for unbounded functions which we in turn use to obtain statistical error bounds  for stochastic optimization of unbounded objectives.   This work builds on the original bounded difference inequality by \cite{mcdiarmid1989method} as well the extensions in  \cite{kontorovich2014concentration,maurer2021concentration}, which apply to unbounded functions of a more  restrictive type than considered here.  More specifically, we obtain  concentration inequalities  for
\begin{align}\label{eq:ULLN_intro}
\sup_{\theta\in\Theta}\left\{\pm\left(\frac{1}{nm}\sum_{i=1}^n\sum_{j=1}^mg_\theta(x_i,y_{i,j})-E_{P_X\times P_Y}[g_\theta]\right)\right\}\,,
\end{align}
 as well as for the optimization error
\begin{align}
E_{P_X\times P_Y}[g_{\theta^*_{n,m}}]-\inf_{\theta\in\Theta} E_{P_X\times P_Y}\left[g_\theta\right]\,,
\end{align}
where $g_\theta:\mathcal{X}\times \mathcal{Y}\to\mathbb{R}$ is an appropriate family of (unbounded) objective functions, depending on parameters  $\theta\in\Theta$,  that satisfies certain local-Lipschitz conditions and $\theta^*_{n,m}$ is (approximately) a solution to the empirical optimization problem
\begin{align}\label{eq:empirical_opt_intro}
\inf_{\theta\in\Theta}\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta}(X_i,Y_{i,j})\,,
\end{align}
with $X_i$ and $Y_{i,j}$ being independent samples from $P_X$ and $P_Y$ respectively.  We specifically focus on problems where there are naturally two types of samples, the $X$'s and $Y$'s,  with $m$-times more $Y$'s than $X$'s.  Such unbalanced sample numbers naturally occur in applications such as denoising score matching (DSM) that utilize easily sampled auxiliary random variables ($Y$'s) alongside the training data ($X$'s).  


\subsection{Contributions}
The key contributions of the present work are as follows:
\begin{itemize}
\item  In Section \ref{sec:McDiarmid} we  derive  a   generalization of McDiarmid's inequality that can be applied to  families of unbounded functions and to  distributions with unbounded support. Our result utilizes non-constant bounds on one-component differences  which, in particular, cover the case of locally Lipschitz functions. In addition, we allow for wide range of tail behavior, beyond the sub-Gaussian case.
\item In Appendix \ref{app:rademacher_bound} we  derive a  distribution-dependent Rademacher complexity bound which can be applied to   families of  unbounded functions that satisfy an appropriate local-Lipschitz condition.
\item In Sections \ref{sec:ULLN} and \ref{sec:stoch_opt} we use the above two tools to derive uniform law of large numbers concentration inequalities in  Theorem \ref{thm:ULLN_var_reuse_mean_bound}  as well as  concentration inequalities for stochastic optimization problems in Theorem \ref{thm:conc_ineq_stoch_opt}, both of which allow for unbounded (objective) functions that satisfy the same local-Lipschitz condition.  
\item  We show that our results quantitatively capture the benefits of sample-reuse (see Eq.~\ref{eq:empirical_opt_intro}) in algorithms that  pair training data with  easily-sampled auxiliary random variables, e.g., Gaussian random variables in DSM. As an example of the application of these tools, we study DSM    in Section \ref{sec:DSM}.
\end{itemize}

\subsection{Related Works}
Our generalization  of McDiarmid's inequality from the original bounded difference case in \cite{mcdiarmid1989method} to a much larger class of bounding functions can be viewed as an extension  of the result in \cite{kontorovich2014concentration}, which studied Lipschitz functions on a space with finite sub-Gaussian-diameter.  More recently,  \cite{maurer2021concentration}  took a different approach and extended  McDiarmid's inequality to sub-Gaussian and sub-exponential cases by using  entropy methods.  Our results, which apply to a wider range of tail behaviors,  also distinguished by the minimal assumptions they make regarding the one-parameter differences bounding functions.  In particular, our framework applies to various forms of local-Lipschitz behavior that naturally arises in a number of applications and which are not covered by   the methods of \cite{kontorovich2014concentration,maurer2021concentration}.   This is especially important in our study of sample reuse, where we employ bounding functions on $X$-differences that are not uniform in the $Y$'s.  Our work shares some similarities with  error analysis of generative adversarial networks (GANs) \cite{liang2021well}, including works by  \cite{huang2022error,biau2021some} that consider distributions with unbounded support. However our methods, which can also be applied to GANs,  allow for one to work directly with   unbounded functions, rather than using truncation arguments as in \cite{huang2022error},  or being restricted to the  sub-Gaussian  case as in \cite{biau2021some}.  We also note that the effect of sample reuse in the GAN case, i.e., unbalanced number of samples from the data source and generator, is much simpler to study than the more general (non-additive) form of sample reuse considered here.

As an  application of our theorems, we study denoising score matching  in Section \ref{sec:DSM}. Since their introduction in \cite{songscore}, there has been great interest in the analysis of score-based generative models that use SDE noising dynamics, and in  DSM in particular.   The works of 
\cite{chen2023improved,pedrotti2023improved,bentonnearly,li2024towards,li2024sharp,potaptchik2024linear,chen2024equivariant,mimikos2024score} all derive convergence results or error bounds for score-based generative models under various assumptions on the score-matching error.  Closest to our work is \cite{oko2023diffusion}, which  obtains statistical error bounds in the form of expected total variation and $1$-Wasserstein distance, in contrast to the concentration inequalities produced by our methods.  The key  inputs into their statistical bounds, such as the covering number bound in their Lemma 4.2, assume a uniformly bounded score-network hypothesis class; see their definition of $\mathcal{S}$ above their Lemma 4.2.  While the authors  prove the  necessary approximation results  to justify and account for the errors made in the restriction to bounded functions, a benefit of our framework is that one can work directly with  unbounded objective functions, including an unbounded score model; the goal of our   Section \ref{sec:DSM} is to demonstrate that advantage.  These strengths carry over to other stochastic optimization problems that are naturally posed on an unbounded domain and with an unbounded objective function.




\section{A Generalization of McDiarmid's Theorem for Unbounded Functions}\label{sec:McDiarmid}


We begin our analysis by deriving a new form of McDiarmid’s inequality that utilizes a  sample-dependent  bound on one-component differences.  Our result   builds on the earlier generalization from \cite{kontorovich2014concentration} which applied to unbounded Lipschitz functions on spaces of finite sub-Gaussian diameter.  Here we make several innovations: 1) We drop the Lipschitz requirement, allowing for more general bounding functions $h_i$ on the one-component differences.  2)  We allow the bounding functions $h_i$ to depend on all later variables in the ordering;  this is necessary in order to handle the  locally-Lipschitz behavior of  many natural objective functions, e.g., in DSM.  3) We consider   substantially more general tail-behavior, generalizing the sub-Gaussian case studied in \cite{kontorovich2014concentration}. All three of these generalizations  will be important for our application to DSM in Section \ref{sec:DSM}.   As previously noted, \cite{maurer2021concentration} took a different approach to generalizing McDiarmid's inequality and studied both the sub-Gaussian and sub-exponential cases; however, their results do not allow for the kind of sample-dependent Lipschitz constants that are crucial for our later theorems.   We   emphasize that, while the proof strategy we employ below follows the pattern pioneered by \cite{kontorovich2014concentration}, the assumptions made here are substantially weaker, thus allowing for  a much wider range of applications than previously possible. Therefore we expect the following result to be of independent interest, beyond our uses of it in Sections \ref{sec:ULLN} and \ref{sec:stoch_opt} below.
\begin{theorem}\label{thm:McDiarmid_general2}
Let $(\mathcal{X}_i,P_i)$, $i=1,...,n$ be probability spaces , define $\mathcal{X}^n\coloneqq\prod_{i=1}^n \mathcal{X}_i$ with the product $\sigma$-algebra, and let $P^n\coloneqq\prod_{i=1}^n P_i$.  Suppose:
\begin{enumerate}
\item $\phi:\mathcal{X}^n\to\mathbb{R}$ is measurable and we have $h_i\in L^1(P_i\times P_i\times\prod_{j>i}P_j)$, $i\in\{1,...,n\}$, that bound the one-component differences in the following manner:
\begin{align}\label{eq:phi_one_arg_diff_bound}
|\phi_{x^{\setminus i}}(x_i)-\phi_{x^{\setminus i}}(\tilde{x}_i)|\leq h_i(x_i,\tilde x_i,x_{i+1},...,x_n)
\end{align}
for all  $i\in\{1,...,n\}$, $(x_1,...,x_n)\in\mathcal{X}^n$,  $\tilde{x}_i\in\mathcal{X}_i$, where we define 
\begin{align}
&x^{\setminus i}\coloneqq (x_1,..,x_{i-1},x_{i+1},...,x_n)\,,\\
&\phi_{x^{\setminus i}}(x)\coloneqq \phi(x_1,...,x_{i-1},x,x_{i+1},...,x_n)\,.\notag
\end{align}
\item We have $K\in(0,\infty]$ and  $\xi_i:[0,K)\to [0,\infty)$, $i=1,...,n$, such that 
\begin{align}\label{eq:h_i_MFG_bound}
\int \cosh(\lambda h_i(x_i,\tilde{x}_i,x_{i+1},...,x_n))P_i(dx_i) P_i(d\tilde{x}_i)\prod_{j>i} P_j(dx_j)\leq e^{\xi_i(|\lambda|)}
\end{align}
for all $\lambda\in(-K,K)$.
\end{enumerate}
  Then $\phi\in L^1(P^n)$,
\begin{align}
E_{P^n}\left[e^{\lambda (\phi-E_{P^n}[\phi])}\right]\leq \exp\left(\sum_{i=1}^n \xi_i(|\lambda|)\right)
\end{align}
for all $\lambda\in(-K,K)$, and for all $t\geq 0$ we have the concentration inequality
\begin{align}\label{eq:new_McDiarmid_conc}
P^n(\pm(\phi-E_{P^n}[\phi])\geq t)\leq \exp\left(-\sup_{\lambda\in[0,K)}\left\{\lambda t-\sum_{i=1}^n \xi_i(\lambda)\right\}\right)\,.
\end{align}

\end{theorem}
\begin{remark}\label{remark:cosh_dzeta}
 Equation \eqref{eq:new_McDiarmid_conc} is equivalent to a centered moment generating function (MGF) bound for $\zeta h_i(x_i,\tilde x_i,x_{i+1},...,x_n)$ under the distribution $d\zeta\times P_i(dx_i)\times P_i(d\tilde{x}_i)\times\prod_{j>i} P_j(dx_j)$, where $d\zeta$ is the uniform distribution on $\{-1,1\}$. See, e.g., Chapter 2 in \cite{vershynin2018high} or in \cite{wainwright2019high} for techniques that can be used to derive such MGF bounds. 
\end{remark}
\begin{proof}
First we show that $\phi\in L^1(P^n)$. Fubini's theorem applied to $h_j\in L^1(P_i\times P_i\times\prod_{j>i}P_j)$ implies $D_j\coloneqq\{({y}_j,y_{j+1},...,y_n): \int |h_j(\tilde{x}_j,{y}_j,y_{j+1},...,y_n)| P_j(d\tilde{x}_j) <\infty\}$ satisfies $(\prod_{\ell\geq j}P_\ell)(D_j)=1$. Therefore $\widetilde{D}_j\coloneqq\left(\prod_{\ell=1}^{j-1} \mathcal{X}_\ell\right)\times D_j$ satisfies $P^n(\widetilde{D}_j)=1$ for all $j$, and hence  there exists $(y_1,...,y_n)\in \cap_{j=1}^n \widetilde{D}_j$.  Therefore $({y}_j,y_{j+1},...,y_n)\in D_j$ for all $j$, i.e.,
\begin{align}
 \int |h_j(\tilde{x}_j,{y}_j,y_{j+1},...,y_n)| P_j(d\tilde{x}_j) <\infty
\end{align}
for all $j$.
For all $x\in\mathcal{X}^n$ we then have
\begin{align}
|\phi(x)|\leq&|\phi(y)|+ \sum_{j=1}^n|\phi(x_1,...,x_jy_{j+1},...,y_n)-\phi(x_1,...,x_{j-1},y_{j},...,y_n)|\\
\leq&|\phi(y)|+ \sum_{j=1}^n h_j(x_j,y_j,y_{j+1},...,y_n)\,,\notag
\end{align}
and hence
\begin{align}
&\int |\phi(x_1,...,x_n)|\prod_{k=i+1}^n P_k(dx_k)\leq  |\phi(y)|+ \sum_{j=1}^n\int h_j(x_j,y_j,y_{j+1},...,y_n)\prod_{k=i+1}^n P_k(dx_k)\\
=& |\phi(y)|+ \sum_{j=1}^i h_j(x_j,y_j,y_{j+1},...,y_n)+ \sum_{j=i+1}^n\int h_j(x_j,y_j,y_{j+1},...,y_n) P_j(dx_j)<\infty\,.\notag
\end{align}
Therefore we conclude that $\phi(x_1,...,x_i,\cdot)\in L^1(\prod_{k=i+1}^n P_k)$ for  $i=0,...,n-1$ and all $x_1,...,x_i$.  In particular, $\phi\in L^1(P^n)$.


To obtain the claimed MGF bound, we now follow the strategy of  \cite{kontorovich2014concentration}, while noting that the assumptions can be substantially weakened to  cover the general bounded difference property \eqref{eq:phi_one_arg_diff_bound}. Start by defining $\phi_n\coloneqq \phi$,
\begin{align}
\phi_i(x_1,...,x_i)\coloneqq\int \phi(x_1,...,x_i,x_{i+1},...,x_n) P_{i+1}(dx_{i+1})...P_n(dx_n)\,,
\end{align}
$i=0,...,n-1$, and $V_i=\phi_i-\phi_{i-1}$, $i=1,...,n$.  These satisfy
\begin{align}
\phi-E_{P^n}[\phi]=\sum_{i=1}^n V_i\,.  
\end{align}
For all $\lambda\in (-K,K)$ and $i=1,...,n$ we can then  bound the MGF of the $V_i$'s as follows.
\begin{align}
&\int e^{\lambda V_i} P_i(dx_i)\\
=&\int e^{\lambda
\int \phi(x_1,...,x_n) - \phi(x_1,...,x_{i-1},y,x_{i+1},...,x_n)  P_{i+1}(dx_{i+1})...P_n(dx_n)P_i(dy)} P_i(dx_i)\notag\\
\leq& \int e^{\lambda(\phi(x_1,...,x_n) - \phi(x_1,...,x_{i-1},y,x_{i+1},...,x_n) )} P_{i+1}(dx_{i+1})...P_n(dx_n)P_i(dy) P_i(dx_i)\notag\\
=& \int \cosh(\lambda(\phi_{x^{\setminus i}}(x) - \phi_{x^{\setminus i}}(y)  )) P_{i+1}(dx_{i+1})...P_n(dx_n)P_i(dy)P_i(dx)\notag\\
\leq&  \int \cosh(|\lambda| h_i(x,y,x_{i+1},...,x_{n})) P_{i+1}(dx_{i+1})...P_n(dx_n) P_i(dy)P_i(dx)\notag\\
 \leq &e^{\xi_i(|\lambda|)}\,.\notag
\end{align}
To obtain the third line we used Jensen's inequality. The fourth line follows from symmetrization.  The fifth line follows from the symmetry and monotonicity properties of $\cosh(t)$ together with the one-argument difference bound \eqref{eq:phi_one_arg_diff_bound}. Finally, the last line  follows from the MGF bound \eqref{eq:h_i_MFG_bound}.


Therefore, for  $\lambda\in (-K,K)$ one obtains the following MGF bound for $\phi$ by induction  
\begin{align}
\int e^{\lambda(\phi-E_P[\phi])} dP^n=& \int\prod_{i=1}^n e^{\lambda V_i} dP^n\leq\exp\left(\sum_{i=1}^n\xi_i(|\lambda|)\right)\,.
\end{align}
A standard Chernoff bound, see, e.g., Section 2.1 in \cite{boucheron2013concentration},  then gives the claimed concentration inequality \eqref{eq:new_McDiarmid_conc}.
\end{proof}
The classical McDiarmid inequality, i.e., bounded differences inequality, from \cite{mcdiarmid1989method}  corresponds to the $h_i$ in \eqref{eq:phi_one_arg_diff_bound} being constants. The generalization of McDiarmid's inequality in  \cite{kontorovich2014concentration}, which to the best of the author's knowledge is the closest extant result to Theorem \ref{thm:McDiarmid_general2}, assumes that $\phi$ is Lipschitz in each component, i.e., that $h_i(x_i,\tilde{x}_i,x_{i+1},...,x_n)=d_i(x_i,\tilde{x}_i)$ where $d_i$ is a metric on $\mathcal{X}_i$. As was previously observed in \cite{kontorovich2014concentration}, the strategy used here   leads to a worse constant in the exponent than is obtained in the classical bounded differences case; specifically, the worse constant stems from the use of Jensen's inequality, which was crucial for dealing with the unbounded nature of the one-component differences.



\section{A Uniform Law of Large Numbers Concentration Inequality for Unbounded Functions with Sample Reuse}\label{sec:ULLN}
In this section we use Theorem \ref{thm:McDiarmid_general2} to obtain a novel  ULLN concentration inequality  for families of unbounded functions and which allows for sample reuse. More specifically, we assume one has two types of samples, $X\in\mathcal{X}$ and $Y\in\mathcal{Y}$, with $m$-times more $Y$ samples than $X$ samples.  This situation often occurs in applications, with the $X$'s being the training data and the $Y$'s being samples from some auxiliary random variable that are used in the training algorithm and which can easily be generated at will; in such cases, $m$ can  be thought of as the number of passes through the data. Our result shows that taking the reuse multiple $m\to \infty$ results in  bounds where the  $Y$-dependence is  integrated out, corresponding to the intuition that one can replace the average over $Y$'s with an expectation.  This property allows us to obtain a ULLN result that is uniform over all $P_X$ that are supported in a given compact set,  even when the (known) distribution of the $Y$'s has unbounded support; such capability is useful for analyzing methods such as DSM that incorporate such unbounded $Y$'s.   To derive the general result, we work under the following assumptions. Later we will show that these can all be verified for DSM.
\begin{assumption}\label{assump:ULLN}
Let $(\mathcal{X},P_X)$, $(\mathcal{Y},P_Y)$ be probability spaces and suppose the following:  
\begin{enumerate}
\item We have  a nonempty  collection $\mathcal{G}$ of measurable functions $g:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}$.
\item We have  $h_{\mathcal{X}}\in L^1(P_X\times P_X\times P_Y)$ such that $|g(x,y)-g(\tilde{x},y)|\leq h_{\mathcal{X}}(x,\tilde{x},y)$ for all $x,\tilde x\in \mathcal{X}$, $y\in \mathcal{Y}$, $g\in\mathcal{G}$.
\item We have $h_{\mathcal{Y}}\in L^1(P_Y\times P_Y)$ such that $|g(x,y)-g(x,\tilde{y})|\leq h_{\mathcal{Y}}(y,\tilde{y})$ for all $x\in \mathcal{X}$, $y,\tilde{y}\in \mathcal{Y}$, $g\in\mathcal{G}$.
\item There exists $K_{X,Y}\in(0,\infty]$, $\xi_{X,Y}:[0,K_{X,Y})\to [0,\infty)$, and a $P_X\times P_X$-probability one set $D_X$ such that  for all $(x,\tilde{x})\in D_X$ we have $h_{\mathcal{X}}(x,\tilde{x},\cdot)\in L^1(P_Y)$ and
\begin{align}
\int \exp\left(\lambda (h_{\mathcal{X}}(x,\tilde{x},y)- E_{P_Y}[h_{\mathcal{X}}(x,\tilde{x},\cdot)])\right)dP_Y\leq e^{\xi_{X,Y}(|\lambda|)}
\end{align}
for all $\lambda\in(-K_{X,Y},K_{X,Y})$.
\item There exists $K_X\in(0,\infty]$, $\xi_X:[0,K_X)\to[0,\infty)$ such that
\begin{align}\label{eq:xi_X_def}
\int  \cosh\left(\lambda  E_{P_Y}[h_{\mathcal{X}}(x,\tilde{x},\cdot)]\right)  (P_X\times P_X)(dxd\tilde{x}) \leq e^{\xi_X(|\lambda|)}
\end{align}
for all $\lambda\in(-K_X,K_X)$.
\item There exists $K_Y\in(0,\infty]$, $\xi_Y:[0,K_Y)\to[0,\infty)$ such that
\begin{align}\label{eq:xi_Y_def}
\int \cosh\left(\lambda h_{\mathcal{Y}}(y,\tilde{y})\right)  (P_Y\times P_Y)(dyd\tilde{y})\leq e^{\xi_Y(|\lambda|)}
\end{align}
for all $\lambda\in(-K_Y,K_Y)$.
 \item  We have  $m\in\mathbb{Z}^+$ with $K_X/m\leq K_{X,Y}$  and  $K_X/m\leq K_{Y}$ 
\end{enumerate}
\begin{remark}
Equations \eqref{eq:xi_X_def} and  \eqref{eq:xi_Y_def} are equivalent to centered MGF bounds; see also Remark \ref{remark:cosh_dzeta}.
\end{remark}
\end{assumption}
Under the above assumptions, we can use Theorem \ref{thm:McDiarmid_general2} to obtain the following.
\begin{lemma}\label{lemma:ULLN_var_reuse}
Let $\mathcal{G}$ be a  countable collection of measurable functions $g:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}$ such that
 Assumption \ref{assump:ULLN} holds. For $n\in\mathbb{Z}^+$ define $P^{n,m}\coloneqq \prod_{i=1}^n \left(P_X\times \prod_{j=1}^m P_Y\right)$ and  define $\phi: \prod_{i=1}^n\left(\mathcal{X}\times \mathcal{Y}^m\right)\to  \mathbb{R}$ by
\begin{align}\label{eq:ULLN_phi_def}
&\phi(x_1,y_{1,1},...,y_{1,m},....,x_n,y_{n,1},...,y_{n,m})=\sup_{g\in\mathcal{G}}\left\{\frac{1}{nm}\sum_{i=1}^n\sum_{j=1}^mg(x_i,y_{i,j})-E_{P_X\times P_Y}[g]\right\}\,.
\end{align}
Then $\phi \in L^1({P^{n,m}})$ and  for all $t\geq 0$ we have the concentration inequality
\begin{align}\label{eq:new_ULLN_conc}
&P^{n,m}(\pm(\phi-E_{P^{n,m}}[\phi])\geq t)\\
\leq& \exp\left(-n\sup_{\lambda\in[0,K_X)}\left\{\lambda t-\left( \xi_X(\lambda)+m\xi_{X,Y}(\lambda/m)+m \xi_Y(\lambda/m)\right)\right\}\right)\,.\notag
\end{align}
\end{lemma}
\begin{remark}
Note that when $\xi_{X,Y}(\lambda)$ and $\xi_{Y}(\lambda)$ are $o(\lambda)$ as $\lambda\to 0$, e.g., in the   sub-Gaussian and sub-exponential cases, then for large  $m$  the supremum on the right-hand side of \eqref{eq:new_ULLN_conc} is approximately  equal to the Legendre transform of $\xi_X$. Therefore the contribution of the $Y$ samples  is approximated by simply integrating out the  $y$-dependence in the function bounding the $x$-differences, $h_{\mathcal{X}}$.    We also note that our  concentration inequality provides new insight even in the case where there is no sample reuse (i.e., where $h_{\mathcal{X}}$ and the $g$'s do not depend on $\mathcal{Y}$ and $h_\mathcal{Y}\coloneqq 0$, $\xi_Y\coloneqq 0$, $\xi_{X,Y}\coloneqq 0$) for two reasons: 1) It is able to utilize general MGF bounds.  2) The minimal assumptions made on the form of ${h}_\mathcal{X}$, which in particular allows for local-Lipschitz behavior that is not covered by other approaches.  In the case of unbounded functions, one often needs both of the features (1)  and (2), whether or not there is sample reuse.
\end{remark}
\begin{proof}
The assumption $h_{\mathcal{X}}\in L^1(P_X\times P_X\times P_Y)$ and  $h_{\mathcal{Y}}\in L^1(P_Y\times P_Y)$ imply
 $\int h_{\mathcal{X}}(x,\tilde{x},y) P_X\times P_Y(dxdy) <\infty$ for $P_X$-a.e. $\tilde{x}$ and $ \int h_{\mathcal{Y}}(y,\tilde{y})P_Y(dy)<\infty$ for $P_Y$-a.e. $\tilde{y}$.  Therefore there exists $\tilde{x}\in \mathcal{X}$ with $\int h_{\mathcal{X}}(x,\tilde{x},y) P_X\times P_Y(dxdy) <\infty$ and there exists $\tilde{y}\in \mathcal{Y}$ such that  $ \int h_{\mathcal{Y}}(y,\tilde{y})P_Y(dy)<\infty$.   Let $g\in\mathcal{G}$.  For all $x,y$ we have
\begin{align}
|g(x,y)|\leq& |g(x,y)-g(\tilde{x},y)|+|g(\tilde{x},y)-g(\tilde{x},\tilde{y})|+|g(\tilde{x},\tilde{y})|\\
\leq& h_{\mathcal{X}}(x,\tilde{x},y)+h_{\mathcal{Y}}(y,\tilde{y})+|g(\tilde{x},\tilde{y})|\,.\notag
\end{align}
Hence for $g\in\mathcal{G}$ we can compute
\begin{align}
E_{P_X\times P_Y}[|g|]\leq&\int_{D_X\times D_Y}  h_{\mathcal{X}}(x,\tilde{x},y)+h_{\mathcal{Y}}(y,\tilde{y})+|g(\tilde{x},\tilde{y})|(P_X\times P_Y)(dxdy)\\
=&\int h_{\mathcal{X}}(x,\tilde{x},y)(P_X\times P_Y)(dxdy) +\int h_{\mathcal{Y}}(y,\tilde{y}) P_Y(dy)+|g(\tilde{x},\tilde{y})|<\infty\,.\notag
\end{align}
Therefore $\mathcal{G}\subset L^1(P_X\times P_Y)$.  Furthermore, we can compute
\begin{align}
&\left|\frac{1}{nm}\sum_{i=1}^n\sum_{j=1}^mg(x_i,y_{i,j})-E_{P_X\times P_Y}[g]\right|\\
\leq&\frac{1}{nm}\sum_{i=1}^n\sum_{j=1}^m(|g(x_i,y_{i,j})-g(\tilde{x},y_{i,j})|+|g(\tilde{x},y_{i,j})-g(\tilde{x},\tilde{y})|)\notag\\
&+ \int |g(\tilde{x},\tilde{y})-g(\tilde{x},y)|+|g(\tilde{x},y)-g(x,y)|(P_X\times P_Y)(dxdy)\notag\\
\leq&\frac{1}{nm}\sum_{i=1}^n\sum_{j=1}^m(h_{\mathcal{X}}(x_i,\tilde{x},y_{i,j})+h_{\mathcal{Y}}(y_{i,j},\tilde{y}))\notag\\
&+\int h_{\mathcal{Y}}(y,\tilde{y})+h_{\mathcal{X}}(x,\tilde{x},y)(P_X\times P_Y)(dxdy)\,.\notag
\end{align}
Therefore
\begin{align}
&\sup_{g\in\mathcal{G}}\left|\frac{1}{nm}\sum_{i=1}^n\sum_{j=1}^mg(x_i,y_{i,j})-E_{P_X\times P_Y}[g]\right|
\\
\leq&\frac{1}{nm}\sum_{i=1}^n\sum_{j=1}^m(h_{\mathcal{X}}(x_i,\tilde{x},y_{i,j})+h_{\mathcal{Y}}(y_{i,j},\tilde{y}))\notag\\
&+\int h_{\mathcal{Y}}(y,\tilde{y}) P_Y(dy)+\int h_{\mathcal{X}}(x,\tilde{x},y)(P_X\times P_Y)(dxdy)<\infty\,.\notag
\end{align}
This implies $\phi$ is a well-defined  real-valued measurable function.

  Writing $y^k=(y_{k,1},...,y_{k,m})$ we can compute the one-argument difference bounds
\begin{align}
&|\phi(x_1,y^1,...,x_k,y^k,...,x_n,y^n)-\phi(x_1,y^1,...,\tilde{x}_k,y^k,...,x_n,y^n)|\\
\leq&\sup_{g\in\mathcal{G}}\left|\frac{1}{nm}\sum_{j=1}^mg(x_k,y_{k,j})-\frac{1}{nm}\sum_{j=1}^mg(\tilde{x}_k,y_{k,j})\right|\notag\\
\leq&\frac{1}{nm}\sum_{j=1}^m h_{\mathcal{X}}(x_k,\tilde{x}_k,y_{k,j})\notag
\end{align}
and
\begin{align}
&|\phi(x_1,y^1,...,x_k,y^k,...,x_n,y^n)-\phi(x_1,y^1,...,x_k,y_{k,1},...,\tilde{y}_{k,\ell},...,y_{k,m},...,x_n,y^n)|\\
\leq&\sup_{g\in\mathcal{G}}\left|\frac{1}{nm}g(x_k,y_{k,\ell})- \frac{1}{nm}g(x_k,\tilde{y}_{k,\ell})\right|\notag\\
\leq&\frac{1}{nm} h_{\mathcal{Y}}(y_{k,\ell},\tilde{y}_{k,\ell})\,.\notag
\end{align}

Letting $K_n\coloneqq nK_X$, for $\lambda\in(-K_n,K_n)$ the assumptions imply the MGF bounds   
\begin{align}
\int \cosh\left(\lambda  \frac{1}{nm} h_{\mathcal{Y}}(y,\tilde{y})\right)  (P_Y\times P_Y)(dyd\tilde{y})
\leq&e^{\xi_Y(\frac{1}{nm}|\lambda |)}\,,
\end{align}
and, again letting $d\zeta$ denote the uniform distribution on$\{-1,1\}$,
\begin{align}
&\int  \cosh\left(\lambda  \frac{1}{nm}\sum_{j=1}^m h_{\mathcal{X}}(x,\tilde{x},y_{j})\right)  P_X(dx)P_X(d\tilde{x})  \prod_{j=1}^mP_Y(dy_j)\\
=&\int\left( \int \prod_{j=1}^m \exp\left(\lambda \zeta \frac{1}{nm} h_{\mathcal{X}}(x,\tilde{x},y_{j})\right) \prod_{j=1}^mP_Y(dy_j)\right)d \zeta P_X(dx) P_X(d\tilde{x})   \notag\\
=&\int \left(\int  \exp\left(\lambda \zeta \frac{1}{nm} h_{\mathcal{X}}(x,\tilde{x},y)\right) P_Y(dy)\right)^m d \zeta P_X(dx) P_X(d\tilde{x})\notag\\
=&\int \int_{D_X} \left(\int  \exp\left(\lambda \zeta \frac{1}{nm} (h_{\mathcal{X}}(x,\tilde{x},y)-E_{P_Y}[h_{\mathcal{X}}(x,\tilde{x},\cdot)])\right) P_Y(dy)\right)^m\notag\\
&\qquad\qquad\qquad\times \exp\left(\lambda \zeta \frac{1}{n} E_{P_Y}[h_{\mathcal{X}}(x,\tilde{x},\cdot)]\right)  (P_X\times P_X)(dxd\tilde{x})  d \zeta\notag\\
\leq&\int \int_{D_X}  e^{m\xi_{X,Y}(|\lambda \zeta \frac{1}{nm} |)}) \exp\left(\lambda \zeta \frac{1}{n} E_{P_Y}[h_{\mathcal{X}}(x,\tilde{x},\cdot)]\right)  (P_X\times P_X)(dxd\tilde{x})  d \zeta\notag\\
=&e^{m\xi_{X,Y}( \frac{1}{nm}|\lambda|  )}   \int\cosh\left(\lambda  \frac{1}{n} E_{P_Y}[h_{\mathcal{X}}(x,\tilde{x},\cdot)]\right)  (P_X\times P_X)(dxd\tilde{x})  \notag\\
\leq&e^{m\xi_{X,Y}( \frac{1}{nm}|\lambda|  )+\xi_X(|\lambda|/n)}\,.\notag
\end{align}
 Therefore  we have shown that the hypotheses of Theorem \ref{thm:McDiarmid_general2}  hold  and hence we can conclude that $\phi\in L^1({P^{n,m}})$ and
 for all $t\geq 0$ we have
\begin{align}
&{P^{n,m}}(\pm(\phi-E_{P^{n,m}}[\phi])\geq t)\\
\leq& \exp\left(-\sup_{\lambda\in[0,K_n)}\left\{\lambda t-\sum_{i=1}^n\left( m\xi_{X,Y}( \frac{1}{nm}\lambda)+\xi_X(\lambda/n)+\sum_{j=1}^m \xi_Y(\frac{1}{nm}\lambda )\right)\right\}\right)\,.\notag
\end{align}
Simplifying the right-hand side  and changing variables from $\lambda$ to $n\lambda$ in the supremum gives the claimed bound \eqref{eq:new_ULLN_conc}.
\end{proof}
\begin{comment}
part (1) of  thm \ref{thm:McDiarmid_general2} holds (part (1) of  thm 2.3.5. holds)

part (2) holds for i's corresponding to Y vars with $\xi_i(\lambda)=\xi_Y(\frac{1}{nm}\lambda )$

and for i's corresponding to X vars with $\xi_i(\lambda)=m\xi_{X,Y}( \frac{1}{nm}\lambda)+\xi_X(\lambda/n)$
\end{comment}

\subsection{Bounding the Mean}
For  appropriate  families of functions, one can use Rademacher complexity and entropy integral techniques to show that $E_{{P^{n,m}}}[\phi]$ in \eqref{eq:new_ULLN_conc} approaches zero as  the number of samples $n\to\infty$. For this purpose, we will use  the novel distribution-dependent Rademacher complexity bound derived in Appendix \ref{app:Rademacher_bound}, which is capable of producing finite bounds for distributions with unbounded support without needing to employ truncation arguments; specifically, we use Theorem \ref{thm:Rademacher_bound_unbounded_support} which is a slight generalization of a method developed in  \cite{birrell2025statisticalerrorboundsgans}. 
\begin{definition}
We  let $(\Theta,d_\Theta)$ denote a non-empty pseudometric space, and will denote its diameter by $D_\Theta\coloneqq\sup_{\theta_1,\theta_2\in\Theta}d_\Theta(\theta_1,\theta_2)$.    For $\epsilon>0$, we let $N(\epsilon,\Theta,d_\Theta)$ be the size of a minimal $\epsilon$-cover of $\Theta$ with respect to the pseudometric $d_\Theta$.
\end{definition}

\begin{assumption}\label{assump:mean_bound}
Let $(\Theta,d_\Theta)$ be a non-empty pseudometric space, $(\mathcal{X},P_X)$, $(\mathcal{Y},P_Y)$ be   probability spaces, and $g_\theta:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}$, $\theta\in\Theta$,  be measurable.  Suppose that we have the following:
\begin{enumerate}
\item  $c:\Theta\to\mathbb{R}$ and   $h:\mathcal{X}\times\mathcal{Y}\to[0,\infty)$  such that $h\in L^1(P_X\times P_Y)$ and $|g_\theta-c(\theta)|\leq h$ for all $\theta\in\Theta$.
\item  A measurable $L:\mathcal{X}\times\mathcal{Y}\to (0,\infty)$ such that  $\theta\mapsto g_\theta(x,y)-c_\theta$ is $L(x,y)$-Lipschitz  with respect to $d_\theta,|\cdot|$ for all $x\in\mathcal{X}$, $y\in\mathcal{Y}$.
\end{enumerate}
\end{assumption}
\begin{remark}
The purpose of allowing for a nonzero $c_\theta$ is to cancel a possible  parameter that shifts the output of $g_\theta$ (e.g., the bias parameter of the output layer).
\end{remark}

\begin{lemma}\label{lemma:variable_reuse_expectation_bound}
Under Assumption \ref{assump:mean_bound},  for $n,m\in\mathbb{Z}^+$ and $\Theta_0$ a countable  subset of $\Theta$ we have
\begin{align}\label{eq:mean_bound}
&E_{P^{n,m}}\left[\sup_{\theta\in\Theta_0}\left\{\pm\left(\frac{1}{nm}\sum_{i=1}^n  \sum_{j=1}^m g_\theta(x_i,y_{i,j})
-E_{P_X\times P_Y}[ g_\theta]\right)\right\}\right]\\
\leq& 2\left((1-1/m)\int  E_{P_Y(dy)}[ L(x,y)] ^2 P_X(dx)+\frac{1}{m} E_{P_X\times P_Y}[L(x,y)^2]\right)^{1/2}\notag\\
&\times\inf_{\eta>0}\left\{4\eta+1_{\eta<D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{D_\Theta/2}\sqrt{\log N(\epsilon,\Theta,d_\Theta)}d\epsilon\right\}\,,\notag
\end{align}
where   $P^{n,m}\coloneqq \prod_{i=1}^n \left(P_X\times \prod_{j=1}^m P_Y\right)$.
\end{lemma}
\begin{remark}
Note that when $m$ is large, this result can be interpreted intuitively as  saying that the $y$ dependence of the Lipschitz constant can be integrated out; this is the behavior one would expect if it was justified to replace the empirical $Y$-average with an expectation. Also note that Jensen's inequality implies 
\begin{align}
\int  E_{P_Y(dy)}[ L(x,y)] ^2 P_X(dx)\leq E_{P_X\times P_Y}[L(x,y)^2]\,.
\end{align}
Therefore  the right-hand side of \eqref{eq:mean_bound} is non-increasing in $m$, thus demonstrating a quantitative benefit of sample reuse.
\end{remark}
\begin{proof}
Define the probability measure  $P_m=P_X\times P_Y^m$ on $\mathcal{X}\times\mathcal{Y}^m$ and define
\begin{align}
\mathcal{G}_0=\left\{z\coloneqq(x,y_1,...,y_m)\mapsto\frac{1}{m}\sum_{j=1}^m (g_\theta(x,y_j)-c_\theta):\theta\in\Theta_0\right\}\,,
\end{align}
a countable family of measurable functions on $\mathcal{X}\times\mathcal{Y}^m$. For $g\in\mathcal{G}_0$ there is $\theta\in\Theta_0$ such that
\begin{align}
|g(x,y^m)|\leq \frac{1}{m}\sum_{j=1}^m |g_\theta(x,y_j)-c_\theta|\leq\frac{1}{m}\sum_{j=1}^m h(x,y_j)\in L^1(P_m)\,.
\end{align}
The ULLN mean bound from Theorem \ref{thm:ULLN_unbounded} in Appendix \ref{app:ULLN_rademacher}  therefore implies
\begin{align}\label{eq:mean_bound_1}
E_{P^{n,m}}\left[\sup_{\theta\in\Theta_0}\left\{\pm\left(\frac{1}{nm}\sum_{i=1}^n  \sum_{j=1}^m g_\theta(x_i,y_{i,j})
-E_{P_X\times P_Y}[ g_\theta]\right)\right\}\right]\leq 2\mathcal{R}_{\mathcal{G}_0,P_m,n}\,.
\end{align}


To bound the Rademacher complexity on the right-hand side, let 
\begin{align}
\mathcal{G}\coloneqq\left\{z\coloneqq(x,y_1,...,y_m)\mapsto\frac{1}{m}\sum_{j=1}^m (g_\theta(x,y_j)-c_\theta):\theta\in\Theta\right\}\,.
\end{align}
 For $\theta_1,\theta_2\in\Theta$ we have
\begin{align}
&\left|\frac{1}{m}\sum_{j=1}^m (g_{\theta_1}(x,y_j)-c_{\theta_1})-\frac{1}{m}\sum_{j=1}^m (g_{\theta_2}(x,y_j)-c_{\theta_2})\right|\\
\leq&\frac{1}{m}\sum_{j=1}^m| (g_{\theta_1}(x,y_j)-c_{\theta_1})- (g_{\theta_2}(x,y_j)-c_{\theta_2})\notag|\\
\leq&\frac{1}{m}\sum_{j=1}^mL(x,y_j)d_\Theta(\theta_1,\theta_2) \,.\notag
\end{align}
Therefore we can use the empirical Rademacher complexity from Theorem \ref{thm:Rademacher_bound_unbounded_support} in Appendix \ref{app:rademacher_bound} to obtain
\begin{align}
\hat{\mathcal{R}}_{\mathcal{G},n}(z^n)\leq L_n(z^n)\inf_{\eta>0}\left\{4\eta+1_{\eta<D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{D_\Theta/2}\sqrt{\log N(\epsilon,\Theta,d_\Theta)}d\epsilon\right\}\,,
\end{align}
where we use the notation $z^n=(z_1,...,z_n)$, $z_i=(x_i,y_{i,1},...,y_{i,m})$ and 
\begin{align}
L_n(z^n)\coloneqq \left(\frac{1}{n}\sum_{i=1}^n \big(\frac{1}{m}\sum_{j=1}^mL(x_i,y_{i,j})\big)^2\right)^{1/2}\,.
\end{align}
Clearly we have $\widehat{\mathcal{R}}_{\mathcal{G}_0,n}(z^n)\leq\widehat{\mathcal{R}}_{\mathcal{G},n}(z^n)$ and therefore

\begin{align}\label{eq:R_G0_bound}
\mathcal{R}_{\mathcal{G}_0,P_m,n}\leq& E_{P^{n,m}}[L_n(z^n)]\inf_{\eta>0}\left\{4\eta+1_{\eta<D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{D_\Theta/2}\sqrt{\log N(\epsilon,\Theta,d_\Theta)}d\epsilon\right\}\\
\leq& E_{P^{n,m}}[L_n(z^n)^2]^{1/2}\inf_{\eta>0}\left\{4\eta+1_{\eta<D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{D_\Theta/2}\sqrt{\log N(\epsilon,\Theta,d_\Theta)}d\epsilon\right\}\,,\notag
\end{align}
where
\begin{align}\label{eq:Ln_simplification}
E_{P^{n,m}}[L_n(z^n)^2]
=&\frac{1}{m^2} \sum_{j=1}^m\sum_{\ell=1}^mE_{P_m}[L(x,y_{j})L(x,y_{\ell})]\\
=&\frac{1}{m^2} \sum_{j=1}^m\sum_{\ell=1,\ell\neq j}^mE_{P_m}[L(x,y_{j})L(x,y_{\ell})]+\frac{1}{m^2} \sum_{j=1}^m E_{P_m}[L(x,y_{j})^2]\notag\\
=&(1-1/m)\int  E_{P_Y(dy)}[ L(x,y)] ^2 P_X(dx)+\frac{1}{m} E_{P_X\times P_Y}[L(x,y)^2]\,.\notag
\end{align}
Taken together, the results \eqref{eq:mean_bound_1}, \eqref{eq:R_G0_bound},  and \eqref{eq:Ln_simplification} imply the claimed bound \eqref{eq:mean_bound}.
\end{proof}

\subsection{Combining the Concentration Inequality and Mean Bound}
Combining Lemmas \ref{lemma:ULLN_var_reuse}  and \ref{lemma:variable_reuse_expectation_bound} we obtain the following ULLN concentration inequality. 
\begin{theorem}\label{thm:ULLN_var_reuse_mean_bound} 
Let $(\Theta,d_\Theta)$ be a non-empty separable pseudometric space,  $(\mathcal{X},P_X)$, $(\mathcal{Y},P_Y)$ be probability spaces,   $g_\theta:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}$ for $\theta\in\Theta$, and $\mathcal{G}\coloneqq\{g_\theta:\theta\in\Theta\}$ such that Assumptions \ref{assump:ULLN} and  \ref{assump:mean_bound} are satisfied. 
For  $n\in\mathbb{Z}^+$ define $P^{n,m}\coloneqq \prod_{i=1}^n \left(P_X\times \prod_{j=1}^m P_Y\right)$ and  $\phi: \prod_{i=1}^n\left(\mathcal{X}\times \mathcal{Y}^m\right)\to  \mathbb{R}$ by
\begin{align}
&\phi_\pm(z_1,...,z_n)\coloneqq\sup_{\theta\in\Theta}\left\{\pm\left(\frac{1}{nm}\sum_{i=1}^n\sum_{j=1}^mg_\theta(x_i,y_{i,j})-E_{P_X\times P_Y}[g_\theta]\right)\right\}\,,
\end{align}
where $z_i\coloneqq (x_i,y_{i,1},...,y_{i,m})$. Then   $\phi_\pm \in L^1({P^{n,m}})$ and  for $t\geq 0$ we have
\begin{align}
P^{n,m}(\phi_\pm\geq t+C_{n,m})\leq \exp\left(-n\sup_{\lambda\in[0,K_X)}\left\{\lambda t-\left( \xi_X(\lambda)+m\xi_{X,Y}(\lambda/m)+m \xi_Y(\lambda/m)\right)\right\}\right)\,,
\end{align}
where
\begin{align}
C_{n,m}\coloneqq& 2\left((1-1/m)\int  E_{P_Y(dy)}[ L(x,y)] ^2 P_X(dx)+\frac{1}{m} E_{P_X\times P_Y}[L(x,y)^2]\right)^{1/2}\\
&\times\inf_{\eta>0}\left\{4\eta+1_{\eta<D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{D_\Theta/2}\sqrt{\log N(\epsilon,\Theta,d_\Theta)}d\epsilon\right\}\,.\notag
\end{align}

\end{theorem}
\begin{remark}
When $\Theta$ is the unit ball in $\mathbb{R}^k$ under some norm and $d_\Theta$ is the corresponding metric then a simplified upper bound on $C_{n,m}$ can be obtained by using \eqref{eq:entropy_int_bound_unit_ball}.
\end{remark}
\begin{proof}
The result follows from noting that the dominated convergence theorem allows the supremum in the definition of $\phi_+$ to  be restricted to a countable dense subset, $\Theta_0$, of $\Theta$ and then using  Lemmas \ref{lemma:ULLN_var_reuse}  and    \ref{lemma:variable_reuse_expectation_bound}.  We note that $-g_\theta$ satisfies the same set of assumptions, hence  the result holds  for $\phi_-$ as well as for $\phi_+$.
\end{proof}


\section{Concentration Inequalities for Stochastic Optimization Problems with Sample Reuse}\label{sec:stoch_opt}
In this section we apply the technique developed  above to derive statistical error bounds for stochastic optimization problems, again focusing on the case where we have two types of samples, $X$ and $Y$, with a multiple $m$ more samples from $Y$ than from $X$.  More specifically, we derive concentration inequalities for
\begin{align}
E_{P_X\times P_Y}[g_{\theta^*_{n,m}}]-\inf_{\theta\in\Theta} E_{P_X\times P_Y}\left[g_\theta\right]\,,
\end{align}
where $g_\theta:\mathcal{X}\times \mathcal{Y}\to\mathbb{R}$ is an appropriate family of (unbounded) objective functions, depending on parameters  $\theta\in\Theta$, that satisfies certain local-Lipschitz conditions and $\theta^*_{n,m}$ is (approximately) a solution to the empirical optimization problem
\begin{align}
\inf_{\theta\in\Theta}\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta}(X_i,Y_{i,j})\,,
\end{align}
with $X_i,Y_{i,j}$ being independent samples from $P_X$ and $P_Y$ respectively. The additional assumptions made here are summarized below.
\begin{assumption}\label{assump:X_Y}
Let $(\mathcal{X},P_X)$, $(\mathcal{Y},P_Y)$, $(\Omega,\mathbb{P})$ be  probability spaces, $n,m\in\mathbb{Z}^+$,   and suppose we have random variables $X_i:\Omega\to\mathcal{X}$, $i=1,..,n$, $Y_{i,j}:\Omega\to\mathcal{Y}$, $i=1,...,n$, $j=1,...,m$ that are all  independent  and with $X_i\sim P_X$  and $Y_{i,j}\sim P_Y$. 

Suppose $\Theta$ is a measurable space, $g:\Theta\times\mathcal{X}\times\mathcal{Y}\to\mathbb{R}$ is measurable, and there exists $\psi\in L^1(P_X\times P_Y)$ such that $g\geq \psi$.  Let $\epsilon_{n,m}^{\text{opt}}\geq 0$ and suppose we have a $\Theta$-valued random variable $\theta^*_{n,m}$ such that
\begin{align}\label{eq:empirical_optim}
\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta^*_{n,m}}(X_i,Y_{i,j})\leq \inf_{\theta\in\Theta}\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta}(X_i,Y_{i,j})+\epsilon_{n,m}^{\text{opt}} \,\,\,\,\,\mathbb{P}\text{-a.s.}
\end{align}
\end{assumption} 
  We start by deriving a $L^1$ error bound. The  proof is based on a standard error decomposition argument, which we will again use in Theorem \ref{thm:conc_ineq_stoch_opt} below, together with the mean bound from Lemma \ref{lemma:variable_reuse_expectation_bound}.
\begin{theorem}\label{thm:opt_L1_bound}
Let $(\Theta,d_\Theta)$ be a non-empty separable pseudometric space equipped with a $\sigma$-algebra, $(\mathcal{X},P_X)$, $(\mathcal{Y},P_Y)$, $(\Omega,\mathbb{P})$ be  probability spaces,  $g_\theta:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}$, $\theta\in\Theta$, $n,m\in\mathbb{Z}^+$,  and $X_i:\Omega\to\mathcal{X}$, $i=1,..,n$, $Y_{i,j}:\Omega\to\mathcal{Y}$, $i=1,...,n$, $j=1,...,m$ be random variables such that Assumptions \ref{assump:mean_bound} and \ref{assump:X_Y} hold. 





Then $g_\theta\in L^1(P_X\times P_Y)$ for all $\theta\in\Theta$, $\inf_{\theta\in\Theta} E_{P_X\times P_Y}[g_\theta]$ is finite,  
and we have the $L^1$ error bound
\begin{align}
&\mathbb{E}\left[\left|E_{P_X\times P_Y}[g_{\theta^*_{n,m}}]- \inf_{\theta\in\Theta} E_{P_X\times P_Y}[g_\theta]\right|\right]\\
\leq &4\left((1-1/m)\int  E_{P_Y(dy)}[ L(x,y)] ^2 P_X(dx)+\frac{1}{m} E_{P_X\times P_Y}[L(x,y)^2]\right)^{1/2}\notag\\
&\times\inf_{\eta>0}\left\{4\eta+1_{\eta<D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{D_\Theta/2}\sqrt{\log N(\epsilon,\Theta,d_\Theta)}d\epsilon\right\}+\epsilon_{n,m}^{\text{opt}}\,.\notag
\end{align}


\end{theorem}
\begin{remark}
We emphasize that our novel Rademacher complexity bound from Theorem \ref{thm:Rademacher_bound_unbounded_support} is the key to providing a finite $L^1$ error bound (which approaches zero as $n\to \infty$ for appropriate spaces $(\Theta,d_\Theta)$) under the relatively weak  assumption that the local Lipschitz constant $L(x,y)$ is square integrable. This is in contrast to the methods previously used for the similar purpose of analyzing GANs, see  Proposition 20 of \cite{biau2021some} and Theorem 22 of \cite{huang2022error}, which required appropriate sub-Gaussian or sub-exponential  assumptions  in order to handle unbounded functions.
\end{remark}
\begin{proof}
The assumptions imply that $|g_\theta|\leq |g_\theta-c_\theta|+|c_\theta|\leq h+|c_\theta|\in L^1(P_X\times P_Y)$. We also have $g_\theta\geq \psi\in L^1(P_X\times P_Y)$ for all $\theta$ and hence $\inf_\theta E_{P_X\times P_Y}[g_\theta]$ is finite.  On the $\mathbb{P}$-probability one set on which  \eqref{eq:empirical_optim} holds we have the following error decomposition:
\begin{align}\label{eq:as_opt_bound}
0\leq &E_{P_X\times P_Y}[g_{\theta^*_{n,m}}]- \inf_\theta E_{P_X\times P_Y}[g_\theta]\\
\leq&E_{P_X\times P_Y}[g_{\theta^*_{n,m}}]-\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta^*_{n,m}}(X_i,Y_{i,j})\notag\\
&+\inf_{\theta\in\Theta}\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta}(X_i,Y_{i,j})- \inf_\theta E_{P_X\times P_Y}[g_\theta]+\epsilon_{n,m}^{\text{opt}}\notag\\
\leq&\sup_{\theta\in\Theta}\!\left\{\!-\!\left(\!\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta}(X_i,Y_{i,j})-E_{P_X\times P_Y}[g_{\theta}]\!\right)\!\!\right\}\notag\\
&+\sup_{\theta\in\Theta}\!\left\{\!\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta}(X_i,Y_{i,j})-  E_{P_X\times P_Y}[g_\theta]\!\right\}+\epsilon_{n,m}^{\text{opt}}\,\notag.
\end{align}
The objectives are unchanged under the replacement of $g_\theta$ with $g_\theta-c_\theta$, which the assumptions imply is continuous  in $\theta$, pointwise in $(x,y)$. This, together with the dominated convergence theorem, allows the suprema to be restricted to  $\Theta_0$.  Therefore taking expectations and using the bound from   Lemma \ref{lemma:variable_reuse_expectation_bound}  we can compute
\begin{align}
&\mathbb{E}\left[\left|E_{P_X\times P_Y}[g_{\theta^*_{n,m}}]- \inf_\theta E_{P_X\times P_Y}[g_\theta]\right|\right]\\
\leq&\mathbb{E}\left[\sup_{\theta\in\Theta_0}\left\{-\left(\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta}(X_i,Y_{i,j})-E_{P_X\times P_Y}[g_{\theta}]\right)\right\}\right]\notag\\
&+\mathbb{E}\left[\sup_{\theta\in\Theta_0}\left\{\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta}(X_i,Y_{i,j})-  E_{P_X\times P_Y}[g_\theta]\right\}\right]+\epsilon_{n,m}^{\text{opt}}\notag\\
\leq &4\left((1-1/m)\int  E_{P_Y(dy)}[ L(x,y)] ^2 P_X(dx)+\frac{1}{m} E_{P_X\times P_Y}[L(x,y)^2]\right)^{1/2}\notag\\
&\times\inf_{\eta>0}\left\{4\eta+1_{\eta<D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{D_\Theta/2}\sqrt{\log N(\epsilon,\Theta,d_\Theta)}d\epsilon\right\}+\epsilon_{n,m}^{\text{opt}}\notag
\end{align}
as claimed.
\end{proof}

Next we use the tools developed above to derive a concentration inequality for stochastic optimization of an unbounded objective.
\begin{theorem}\label{thm:conc_ineq_stoch_opt}
Let $(\Theta,d_\Theta)$ be a non-empty separable pseudometric space equipped with a $\sigma$-algebra, $(\mathcal{X},P_X)$, $(\mathcal{Y},P_Y)$, $(\Omega,\mathbb{P})$ be  probability spaces,  $g_\theta:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}$, $\theta\in\Theta$,  $\mathcal{G}\coloneqq\{g_\theta:\theta\in\Theta\}$, $n,m\in\mathbb{Z}^+$,  and $X_i:\Omega\to\mathcal{X}$, $i=1,..,n$, $Y_{i,j}:\Omega\to\mathcal{Y}$, $i=1,...,n$, $j=1,...,m$ be random variables such that Assumptions  \ref{assump:ULLN}, \ref{assump:mean_bound}, and \ref{assump:X_Y} hold. 


Then $g_\theta\in L^1(P_X\times P_Y)$ for all $\theta\in\Theta$, $\inf_{\theta\in\Theta} E_{P_X\times P_Y}[g_\theta]$ is finite,  and for $t\geq 0$ we have
\begin{align}
&\mathbb{P}\left(E_{P_X\times P_Y}[g_{\theta^*_{n,m}}]-\inf_{\theta\in\Theta} E_{P_X\times P_Y}[g_\theta]\geq t+C_{n,m}+\epsilon_{n,m}^{\text{opt}}\right)\\
\leq&\exp\left(-n\sup_{\lambda\in[0,K_X)}\left\{\lambda t/2-(\xi_X(\lambda)+m\xi_{X,Y}( {\lambda}/{m})+m\xi_Y({\lambda}/{m}))\right\}\right)\,,\notag
\end{align}
where
\begin{align}
C_{n,m}\coloneqq &4\left((1-1/m)\int  E_{P_Y(dy)}[ L(x,y)] ^2 P_X(dx)+\frac{1}{m} E_{P_X\times P_Y}[L(x,y)^2]\right)^{1/2}\\
&\times\inf_{\eta>0}\left\{4\eta+1_{\eta<D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{D_\Theta/2}\sqrt{\log N(\epsilon,\Theta,d_\Theta)}d\epsilon\right\}\notag\,.
\end{align}

\end{theorem}
\begin{proof}
The assumptions of Lemma \ref{lemma:variable_reuse_expectation_bound} and Theorem \ref{thm:opt_L1_bound} hold, therefore we combine the  $\mathbb{P}$-a.s. bound \eqref{eq:as_opt_bound}  with the mean bound \eqref{eq:mean_bound} to obtain
\begin{align}\label{eq:opt_prob_bound1}
&\mathbb{P}\left(E_{P_X\times P_Y}[g_{\theta^*_{n,m}}]-\inf_{\theta\in\Theta} E_{P_X\times P_Y}[g_\theta]\geq t+C_{n,m}+\epsilon_{n,m}^{\text{opt}}\right)\\
\leq &\mathbb{P}\left(\phi_-(Z^n) +\phi_+(Z^n)\geq  t+E_{P}\left[\phi_-\right]+E_{P}\left[\phi_+\right]\right)\notag\\
=&{P^{n,m}}\left((\phi_- +\phi_+)/2-E_{P}\left[(\phi_-+\phi_+)/2\right]\geq  t/2\right)\,,\label{eq:phi_avg_conc_1}
\end{align}
where $Z^n\coloneqq(X_1,Y_{1,1},...,Y_{1,m},...,X_n,Y_{n,1},...,Y_{n,m})$, $P^{n,m}\coloneqq (P_X\times P_Y^m)^n$,  
\begin{align}
&\phi_\pm(z^n)\coloneqq \sup_{\theta\in\Theta_0}\left\{\pm\left(\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta}(x_i,y_{i,j})-E_{P_X\times P_Y}[g_{\theta}] \right)\right\}\,, 
\end{align}
and $\Theta_0$ is a countable dense subset of $\Theta$.

While Lemma \ref{lemma:ULLN_var_reuse} does not directly apply \eqref{eq:phi_avg_conc_1}, as the latter involves the average  $\overline{\phi}\coloneqq (\phi_++\phi_-)/2$ and not just one or the other of $\phi_\pm$, the we can follow the exact same proof strategy to arrive at the claimed concentration inequality. We start by obtaining the bounded difference properties
\begin{align}
&\left|\overline{\phi}(x_1,y^1,...,x_k,y^k,...,x_n,y^n)-\overline{\phi}(x_1,y^1,...,\tilde{x}_k,y^k,...,x_n,y^n)\right|\\
\leq&\frac{1}{nm}\sum_{j=1}^m h_{\mathcal{X}}(x_k,\tilde{x}_k,y_{k,j})\notag
\end{align}
and
\begin{align}
&\left|\overline{\phi}(x_1,y^1,...,x_k,y^k,...,x_n,y^n)-\overline{\phi}(x_1,y^1,...,x_k,y_{k,1},...,\tilde{y}_{k,\ell},...,y_{k,m},...,x_n,y^n)\right|\\
\leq&\frac{1}{nm} h_{\mathcal{Y}}(y_{k,\ell},\tilde{y}_{k,\ell})\notag\,.
\end{align}

Just as in Lemma \ref{lemma:ULLN_var_reuse}, for  $\lambda\in(-nK_X,nK_X)$ the assumptions imply the MGF bounds
\begin{align}
&\int \cosh\left(\lambda  \frac{1}{nm} h_{\mathcal{Y}}(y,\tilde{y})\right)  (P_Y\times P_Y)(dyd\tilde{y})
\leq e^{\xi_Y(\frac{1}{nm}|\lambda |)}\,,\\
&\int  \cosh\left(\lambda  \frac{1}{nm}\sum_{j=1}^m h_{\mathcal{X}}(x,\tilde{x},y_{j})\right)  (P_X\times P_X)(dxd\tilde{x})  \prod_{j=1}^mP_Y(dy_j)
\leq e^{m\xi_{X,Y}( \frac{1}{nm}|\lambda|  )+\xi_X(|\lambda|/n)}\,.\notag
\end{align}



Therefore Theorem \ref{thm:McDiarmid_general2} applied to $\overline{\phi}$ implies that for all $s\geq 0$ we have
\begin{align}
&P^{n,m}\left((\phi_-+\phi_+)/2-E_{P^{n,m}}[(\phi_-+\phi_+)/2]\geq  s\right)\\
\leq&\exp\left(-n\sup_{\lambda\in[0,K_X)}\left\{\lambda s-(\xi_X(\lambda)+m\xi_{X,Y}(\lambda/m)+m\xi_Y(\lambda/m))\right\}\right)\,.\notag
\end{align}
Letting $s=t/2$ and combining this with  \eqref{eq:opt_prob_bound1} completes the proof.
\end{proof}

\subsection{Application to Denoising Score Matching}\label{sec:DSM}
Denoising score matching (DSM) is a stochastic optimization problem of the form studied in Theorem \ref{thm:conc_ineq_stoch_opt}, having an unbounded objective function and which also naturally incorporates sample reuse, with the $X$'s  (here denoted $X_0$) being samples from the target distribution and the $Y$'s being auxiliary Gaussian random variables.  Here we will show how Theorem \ref{thm:conc_ineq_stoch_opt} can be applied to DSM in order to obtain statistical error bounds, providing a connection between the empirical optimization problem that is used in training and the type of bounds that are assumed  in convergence analyses such as in \cite{chen2023improved,bentonnearly,li2024towards,li2024sharp,potaptchik2024linear,chen2024equivariant,mimikos2024score}. Such results generally frame their bounds in terms of the score matching error,
\begin{align}\label{eq:score_match_err}
\sum_{j=1}^J \gamma_j{ E}_{ X_{t_j}}\!\!\left[ \|s^{\theta^*}(x_{t_j},t_j)-s(x_{t_j},t_j)\|^2\right]\,,
\end{align}
where $s^{\theta^*}(x,t)$ is the score model, $s(x,t)$ is the true score,  $t_j$, $\gamma_j$, $j=1,...,J$ are chosen sequences of timesteps and weights, and $X_{t_j}$ is the linear forward noising dynamics at time $t_j$.  In the following result we use our theory to derive statistical  bounds on the score-matching error \eqref{eq:score_match_err} when the score model $s^{\theta^*}(x,t)$ is trained via empirical DSM.

\begin{theorem}\label{thm:DSM}
Suppose we have a data distribution $P_{X_0}$ on $\mathbb{R}^d$ and a measurable  score model $s:\Theta\times\mathbb{R}^d\times [0,T]\to \mathbb{R}^d$ such that
\begin{enumerate}
\item  $\theta\mapsto s^\theta(x,t)$ is Lipschitz w.r.t. $d_\Theta$ with Lipschitz constant $\alpha_t+\beta_t\|x\|$, where $\|\cdot\|$ denotes the $2$-norm,
\item we have a linear growth bound $\|s^\theta(x,t)\|\leq a_t+b_t\|x\|$,
\item $x\mapsto s^\theta(x,t)$ is $L_t$-Lipschitz for all $\theta\in\Theta$,
\item  $\|x_0\|\leq r_0<\infty$ for all $x_0$ in the support of $P_{X_0}$.
\end{enumerate}

Given $c\geq 0$, a continuous noise strength $\sigma_t>0$, and timesteps $t_j$, $j=1,...,J$, define the noising dynamics
$X_{t}\coloneqq e^{-ct}X_0+\widetilde{\sigma}_{t}Y$, where  $Y\sim N(0,I_d)$ is independent of $X_0$ and
\begin{align}
\widetilde{\sigma}_t^2\coloneqq\int_0^t e^{-2c(t-s)}\sigma_s^2 ds\,,
\end{align}
along with the  DSM objective
\begin{align}\label{eq:DSM_g_theta}
&g_\theta(x_0,y)\coloneqq \sum_{j=1}^J \gamma_j \|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)+\widetilde{\sigma}^{-1}_{t_j}y\|^2\,.
\end{align}
Suppose  we have  $X_i\sim P_{X_0}$,   and $Y_{i,j}\sim P_Y$, $i=1,...,n$, $j=1,...,m$,  that are all  independent 
 along with a $\Theta$-valued random variable $\theta^*_{n,m}$ that is an approximate minimizer of the empirical DSM problem:
\begin{align}\label{eq:dsm_empirical_optim}
&\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta^*_{n,m}}(X_i,Y_{i,j})\leq \inf_{\theta\in\Theta}\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta}(X_i,Y_{i,j})+\epsilon_{n,m}^{\text{opt}} \,\,\,\,\,\mathbb{P}\text{-a.s.}\,,
\end{align}
where $\epsilon_{n,m}^{\text{opt}}\geq 0$. 

Then, letting $s(x,t)$ be the true score, we have the following statistical error bounds for DSM:
\begin{align}\label{eq:DSM_conc_ineq}
&\mathbb{P}\left(\sum_{j=1}^J \gamma_j{ E}_{ X_{t_j}}\!\!\left[ \|s^{\theta^*_{n,m}}(x_{t_j},t_j)-s(x_{t_j},t_j)\|^2\right]\geq t+R_*+C_{n,m}+\epsilon_{n,m}^{\text{opt}}\right)\\
\leq&\exp\left(-n\sup_{\lambda\in[0,mK_Y)}\left\{\lambda t/2-A_{X,Y,m}\lambda^2\right\}\right)\,,\notag
\end{align}
where the expectation $E_{X_{t_j}}$  is over the variable $x_{t_j}$,
\begin{align}
R_*\coloneqq \inf_{\theta\in\Theta} \sum_{j=1}^J \gamma_j{ E}_{ X_{t_j}}\!\!\left[ \|s^\theta(x_{t_j},t_j)-s(x_{t_j},t_j)\|^2\right]\,,
\end{align}
\begin{align}
C_{n,m}\coloneqq &4\bigg((1-1/m)\left(A_\Theta+B_\Theta E_{P_Y}[ \|y\|]+C_\Theta E_{P_Y}[ \|y\|^2]\right) ^2 \\
&\qquad\qquad\qquad+\frac{1}{m} E_{ P_Y}\left[(A_\Theta+B_\Theta\|y\|+C_\Theta\|y\|^2)^2\right]\bigg)^{1/2}\notag\\
&\times\inf_{\eta>0}\left\{4\eta+1_{\eta<D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{D_\Theta/2}\sqrt{\log N(\epsilon,\Theta,d_\Theta)}d\epsilon\right\}\notag\,,
\end{align}
and the definitions of the various constants are provided in the course of the proof below. 



In particular, if $t\leq 4mK_Y  A_{X,Y,m}$ then the term in the exponent can be simplified via
\begin{align}
&\sup_{\lambda\in[0,mK_Y)}\left\{\lambda t/2-A_{X,Y,m}\lambda^2\right\}=\frac{t^2}{16A_{X,Y,m}}\,.\notag
\end{align}
\end{theorem}
\begin{remark}
See \eqref{eq:entropy_int_bound_unit_ball} for a more explicit bound on the entropy integral term in the case where $\Theta$ is the unit ball in $\mathbb{R}^k$ under some norm  and $d_\Theta$ is the corresponding metric.
\end{remark}
\begin{proof}
Using the standard method  of rewriting the score-matching objective in DSM form, see \cite{vincent2011connection}, we have
\begin{align}
&\sum_{j=1}^J \gamma_j{ E}_{X_{t_j}}\left[ \|s^{\theta}(x_{t_j},t_j)-s(x_{t_j},t_j)\|^2\right]-R_*=E_{P_{X_0}\times P_Y}\left[g_\theta\right]
-\inf_{\theta\in\Theta}{E}_{P_{X_0}\times P_Y}\left[g_\theta\right]\,,
\end{align}
where the DSM objective $g_\theta$ is given by \eqref{eq:DSM_g_theta}.  We now show that the assumptions of Theorem \ref{thm:conc_ineq_stoch_opt} hold.  Clearly, Assumption \ref{assump:X_Y} holds for $g_\theta$ and $\theta^*_{n,m}$.

Next, by rewriting
\begin{align}
g_\theta(x_0,y)
=& \sum_{j=1}^J \gamma_j \left(\|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|^2+2\widetilde{\sigma}^{-1}_{t_j} s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\cdot y+\widetilde{\sigma}^{-2}_{t_j}\|y\|^2\right)
\end{align}
we can compute the growth bound
\begin{align}
&|g_\theta(x_0,y)|\\
\leq&\sum_{j=1}^J \gamma_j \!\left((a_{t_j}+b_{t_j}\| e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y\|)^2+2\widetilde{\sigma}^{-1}_{t_j}(a_{t_j}+b_{t_j}\| e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y\|)\|y\|+\widetilde{\sigma}^{-2}_{t_j}\|y\|^2\right)\notag\\
\leq& \sum_{j=1}^J \gamma_j \!\left((a_{t_j}+b_{t_j}( r_0e^{-ct_j}+\widetilde{\sigma}_{t_j}\|y\|)^2+2\widetilde{\sigma}^{-1}_{t_j}(a_{t_j}+b_{t_j}( r_0e^{-ct_j}+\widetilde{\sigma}_{t_j}\|y\|)\|y\|+\widetilde{\sigma}^{-2}_{t_j}\|y\|^2\right)\notag\\
&\coloneqq h\in L^1(P_{X_0}\times P_Y)\,,\notag
\end{align}
 as well as the Lipschitz bound
\begin{align}
&|g_{\theta_1}(x_0,y)-g_{\theta_2}(x_0,y)|\\
\leq&\sum_{j=1}^J \gamma_j (\|s^{\theta_1}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|+\|s^{\theta_2}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|)\notag\\
&\qquad\times\|s^{\theta_1}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)-s^{\theta_2}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|\notag\\
&+2\sum_{j=1}^J \gamma_j \widetilde{\sigma}^{-1}_{t_j} \|s^{\theta_1}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)- s^{\theta_2}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|\|y\|\notag\\
\leq& \sum_{j=1}^J \gamma_j (\alpha_{t_j}+\beta_{t_j}\|e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y\|)
 \bigg(2(a_{t_j}+b_{t_j}\| e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y\|)+2 \widetilde{\sigma}^{-1}_{t_j} \|y\| \bigg) d_\Theta(\theta_1,\theta_2)\notag\\
\leq& \bigg( \sum_{j=1}^J \gamma_j 
 2(a_{t_j}+ r_0e^{-ct_j}b_{t_j})(\alpha_{t_j}+r_0e^{-ct_j}\beta_{t_j})+\sum_{j=1}^J\gamma_j(2\beta_{t_j}\widetilde{\sigma}_{t_j}(a_{t_j}+ r_0e^{-ct_j}b_{t_j})\notag\\
&+2(b_{t_j}\widetilde{\sigma}_{t_j}+ \widetilde{\sigma}^{-1}_{t_j} )(\alpha_{t_j}+r_0e^{-ct_j}\beta_{t_j}))\|y\|+\sum_{j=1}^J2\gamma_j(b_{t_j}\widetilde{\sigma}_{t_j}+ \widetilde{\sigma}^{-1}_{t_j} ) \beta_{t_j}\widetilde{\sigma}_{t_j}\|y\|^2 \bigg) d_\Theta(\theta_1,\theta_2)\notag\\
\coloneqq &(A_\Theta+B_\Theta\|y\|+C_\Theta\|y\|^2) d_\Theta(\theta_1,\theta_2)\,.\notag
\end{align}
Therefore Assumption \ref{assump:mean_bound} holds for $g_\theta$ by taking $c_\theta\coloneqq 0$ and 
\begin{align}
L(x_0,y)\coloneqq A_\Theta+B_\Theta\|y\|+C_\Theta\|y\|^2\,.
\end{align}

Finally, we show that Assumption \ref{assump:ULLN} holds. To do this, first compute
\begin{align}
&|g_\theta(x_0,y)-g_\theta({x}_0,\tilde{y})|\\
\leq&\sum_{j=1}^J \gamma_j  ( \|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)+\widetilde{\sigma}^{-1}_{t_j}y\|+ \|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}\tilde y,t_j)+\widetilde{\sigma}^{-1}_{t_j}\tilde y\|)\notag\\
&\times\|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)-s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}\tilde y,t_j)+\widetilde{\sigma}^{-1}_{t_j}(y-\tilde y)\|\notag\\
\leq& \sum_{j=1}^J \gamma_j  ( 2(a_{t_j}+r_0b_{t_j} e^{-ct_j})+(b_{t_j}\widetilde{\sigma}_{t_j}+\widetilde{\sigma}^{-1}_{t_j})(\|y\| +\|\tilde y\|))\times( L_{t_j}\widetilde{\sigma}_{t_j} 
+\widetilde{\sigma}^{-1}_{t_j})\|y-\tilde y\|\notag\\
&\coloneqq (A_\mathcal{Y}+B_{\mathcal{Y}}(\|y\|+\|\tilde{y}\|))\|y-\tilde{y}\|\coloneqq h_{\mathcal{Y}}(y,\tilde{y})\in L^1(P_Y\times P_Y)\notag
\end{align}
and
\begin{align}
|g_\theta(x_0,y)-g_\theta(\tilde{x}_0,y)|
\leq&\sum_{j=1}^J \gamma_j| \|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|^2- \|s^{\theta}( e^{-ct_j}\tilde{x}_0+\widetilde{\sigma}_{t_j}y,t_j)\|^2|\\
&+\sum_{j=1}^J  2\widetilde{\sigma}^{-1}_{t_j} \gamma_j\| s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)-s^{\theta}( e^{-ct_j}\tilde{x}_0+\widetilde{\sigma}_{t_j}y,t_j)\|\|y\|\notag\\
\leq&\sum_{j=1}^J  4r_0\gamma_j(a_{t_j}+r_0b_{t_j} e^{-ct_j}+b_{t_j}\widetilde{\sigma}_{t_j}\|y\|) L_{t_j}e^{-ct_j}\notag\\
&+\sum_{j=1}^J  4r_0\widetilde{\sigma}^{-1}_{t_j} \gamma_jL_{t_j}e^{-ct_j}\|y\|\notag\\
&\coloneqq A_{\mathcal{X}}+B_{\mathcal{X}}\|y\|\coloneqq h_{\mathcal{X}}(x,\tilde{x},y)\in L^1(P_{X_0}\times P_{X_0}\times P_Y)\,.\notag
\end{align}
It is straightforward to verify that $\|y\|$ is sub-Gaussian under $P_Y$,  hence there exists $\sigma_{Y}$  (depending only on  $P_Y$)  such that
\begin{align}
\int \exp\left(\lambda (h_{\mathcal{X}}(x,\tilde{x},y)- E_{P_Y}[h_{\mathcal{X}}(x,\tilde{x},\cdot)])\right)dP_Y
=&\int \exp\left(B_{\mathcal{X}}\lambda (\|y\|- E_{P_Y}[\|y\|])\right)dP_Y\\
\leq & e^{\sigma_{Y}^2B_{\mathcal{X}}^2\lambda^2/2} \coloneqq e^{\xi_{X,Y}(|\lambda|)}\notag
\end{align}
for all $\lambda\in\mathbb{R}$; in particular, we can take $K_{X,Y}=\infty$.

Letting $d\zeta$ be the uniform distribution on $\{-1,1\}$ we can use the triangle inequality and the Cauchy-Schwarz inequality to compute
\begin{align}
&\left(\int |\zeta h_{\mathcal{Y}}(y,\tilde{y})|^pd\zeta dP_YdP_Y\right)^{1/p}\\
\leq&A_\mathcal{Y}\|\|y-\tilde{y}\|\|_{L^p(P_Y\times P_Y)}+B_{\mathcal{Y}}\|(\|y\|+\|\tilde{y}\|)\|y-\tilde{y}\|\|_{L^p(P_Y\times P_Y)}\notag\\
\leq&2A_\mathcal{Y}\|\|y\|\|_{L^p(P_Y)}+2B_{\mathcal{Y}}\|\|y\|\|_{L^{2p}(P_Y)}\|\|y-\tilde{y}\|\|_{L^{2p}(P_Y\times P_Y)}\notag\\
\leq&2A_\mathcal{Y}\|\|y\|\|_{L^p(P_Y)}+4B_{\mathcal{Y}}\|\|y\|\|_{L^{2p}(P_Y)}^2\,.\notag
\end{align}
Again using the fact that $\|y\|$ is sub-Gaussian under $P_Y$, we can use Proposition 2.5.2 in \cite{vershynin2018high} to conclude that there exists $M$ such that
\begin{align}
\int |\zeta h_{\mathcal{Y}}(y,\tilde{y})|^pd\zeta dP_YdP_Y^{1/p}
\leq&2A_\mathcal{Y}M\sqrt{p}+4B_{\mathcal{Y}}\left(M\sqrt{2p}\right)^2\\
\leq&2(A_\mathcal{Y}+4B_{\mathcal{Y}})\max\{M,M^2\}p\,.\notag
\end{align}
Therefore Proposition 2.7.1 in \cite{vershynin2018high}  implies $\zeta h_{\mathcal{Y}}(y,\tilde{y})$ is sub-exponential under $d\zeta dP_YdP_Y$ and  there exists a universal constant $C$ such that
\begin{align}
\int \cosh(\lambda h_{\mathcal{Y}}(y,\tilde{y}))(P_Y\times dP_Y)(dyd\tilde{y})=&\int e^{\lambda\zeta h_{\mathcal{Y}}(y,\tilde{y})}d\zeta dP_YdP_Y\\
\leq& e^{4C^2(A_\mathcal{Y}+4B_{\mathcal{Y}})^2\max\{M,M^2\}^2\lambda^2}\notag
\end{align}
for all $|\lambda|\leq 1/(2C(A_\mathcal{Y}+4B_{\mathcal{Y}})\max\{M,M^2\})$, i.e., there exists $\widetilde{\sigma}_Y$ (depending only on  $P_Y$) such that
\begin{align}
\int e^{\lambda\zeta h_{\mathcal{Y}}(y,\tilde{y})}d\zeta dP_YdP_Y\leq e^{4(A_\mathcal{Y}+4B_{\mathcal{Y}})^2 \widetilde{\sigma}_Y^2\lambda^2}
\end{align}
for all $|\lambda|\leq 1/(2(A_\mathcal{Y}+4B_{\mathcal{Y}})\widetilde{\sigma}_Y)\coloneqq K_Y$.  


Finally compute
\begin{align}
\int  \cosh\left(\lambda  E_{P_Y}[h_{\mathcal{X}}(x,\tilde{x},\cdot)]\right)  (P_X\times P_X)(dxd\tilde{x}) 
=&\cosh\left(\lambda  E_{P_Y}[ A_{\mathcal{X}}+B_{\mathcal{X}}\|y\|]\right)  \\
\leq&\exp\left(\lambda^2  ( A_{\mathcal{X}}+B_{\mathcal{X}}E_{P_Y}[\|y\|])^2/2\right)\coloneqq e^{\xi_X(|\lambda|)}\notag
\end{align}
for all $\lambda\in\mathbb{R}$; in particular, we can pick $K_X=mK_Y$. This completes the proof that Assumption   \ref{assump:ULLN} holds for $g_\theta$.  Therefore we can apply Theorem \ref{thm:conc_ineq_stoch_opt}; substituting in the definitions of all the relevant terms and defining
\begin{align}
A_{X,Y,m}\coloneqq  ( A_{\mathcal{X}}+B_{\mathcal{X}}E_{P_Y}[\|y\|])^2/2+
\sigma_{Y}^2B_{\mathcal{X}}^2/(2m)
+4(A_\mathcal{Y}+4B_{\mathcal{Y}})^2 \widetilde{\sigma}_Y^2/m\,,
\end{align}
 we arrive at the claimed bound \eqref{eq:DSM_conc_ineq}.
 \end{proof}
\begin{comment}
\begin{itemize}
\item \begin{align}
&E_{\eta_t(dy)}[ \|s^\theta(y,t)-\nabla\log \eta(y,t)\|^2]\\
=&E_{\eta_t(dy)}[ \|s^\theta(y,t)\|^2]-2\int s^\theta(y,t)\cdot\nabla \eta(y,t)dy+E_{\eta_t(dy)}[\|\nabla\log \eta(y,t)\|^2]\\
=&E_{\eta_t(dy)}[ \|s^\theta(y,t)\|^2]-2\int s^\theta(y,t)\cdot\nabla_y( \int\eta_t(y|x)\pi(x)dx)dy+E_{\eta_t(dy)}[\|\nabla\log \eta(y,t)\|^2]\\
=&E_{\eta_t(dy)}[ \|s^\theta(y,t)\|^2]-2\int  \int s^\theta(y,t)\cdot(\nabla_y\eta_t(y|x))\pi(x)dxdy+E_{\eta_t(dy)}[\|\nabla\log \eta(y,t)\|^2]\\
=&E_{\eta_t(dy)}[ \|s^\theta(y,t)\|^2]-2\int  \int s^\theta(y,t)\cdot(\nabla_y\log(\eta_t(y|x)))\eta_t(y|x)\pi(x)dxdy+E_{\eta_t(dy)}[\|\nabla\log \eta(y,t)\|^2]\\
=&E_{\eta_t(dydx)}[ \|s^\theta(y,t)\|^2-2s^\theta(y,t)\cdot\nabla_y\log(\eta_t(y|x))]+E_{\eta_t(dy)}[\|\nabla\log \eta(y,t)\|^2]\\
=&E_{\eta_t(dydx)}[ \|s^\theta(y,t)-\nabla_y\log(\eta_t(y|x))\|^2]-E_{\eta_t(dydx)}[\|\nabla_y\log(\eta_t(y|x))\|^2]+E_{\eta_t(dy)}[\|\nabla\log \eta(y,t)\|^2]
\end{align}
\item \begin{align}
&\sum_{j=1}^J \gamma_j E_{\eta_{t_j}(dy)}[ \|s^\theta(y,t_j)-\nabla\log \eta(y,t_j)\|^2]\\
=&\sum_{j=1}^J \gamma_j E_{\eta_{t_j}(dydx)}[ \|s^\theta(y,t_j)-\nabla_y\log(\eta_{t_j}(y|x))\|^2]-\sum_{j=1}^J \gamma_j  E_{\eta_{t_j}(dydx)}[\|\nabla_y\log(\eta_{t_j}(y|x))\|^2]\\
&+\sum_{j=1}^J \gamma_j  E_{\eta_{t_j}(dy)}[\|\nabla\log \eta(y,t_j)\|^2]
\end{align}
\item \begin{align}
&\mathbb{ E}\left[ \sum_{j=1}^J \gamma_j\|s^\theta(X_{t_j},t_j)-\nabla\log \eta(X_{t_j},t_j)\|^2\right]\\
=& \mathbb{E}\left[\sum_{j=1}^J \gamma_j \|s^\theta(X_{t_j},t_j)-\nabla_y\log(\eta_{t_j}(X_{t_j}|X_0))\|^2\right]-\mathbb{E}\left[\sum_{j=1}^J \gamma_j  \|\nabla_y\log(\eta_{t_j}(X_{t_j}|X_0))\|^2\right]\\
&+\mathbb{ E}[\sum_{j=1}^J \gamma_j \|\nabla\log \eta(X_{t_j},t_j)\|^2]
\end{align}
\item Given any r.v.  $\theta^*$ (to be made specific later)
\begin{align}
&\mathbb{ E}\left[ \sum_{j=1}^J \gamma_j\|s^{\theta^*}(X_{t_j},t_j)-\nabla\log \eta(X_{t_j},t_j)\|^2\right]-\inf_\theta \mathbb{ E}\left[ \sum_{j=1}^J \gamma_j\|s^\theta(X_{t_j},t_j)-\nabla\log \eta(X_{t_j},t_j)\|^2\right]\\
=&\mathbb{E}\left[\sum_{j=1}^J \gamma_j \|s^{\theta^*}(X_{t_j},t_j)-\nabla_x\log(\eta_{t_j}(X_{t_j}|X_0))\|^2\right]
-\inf_\theta\mathbb{E}\left[\sum_{j=1}^J \gamma_j \|s^\theta(X_{t_j},t_j)-\nabla_{x}\log(\eta_{t_j}(X_{t_j}|X_0))\|^2\right]\\
=&\mathbb{E}\left[\sum_{j=1}^J \gamma_j \|s^{\theta^*}(X_{t_j},t_j)+\widetilde{\sigma}^{-1}_{t_j}Y\|^2\right]
-\inf_\theta\mathbb{E}\left[\sum_{j=1}^J \gamma_j \|s^\theta(X_{t_j},t_j)+\widetilde{\sigma}_{t_j}^{-1}Y\|^2\right]
\end{align}
where $X_{t_j}\coloneqq e^{-ct_j}X_0+\widetilde{\sigma}_{t_j}Y$, $Y\sim N(0,I)$ is independent of $X_0$ and
\begin{align}
\widetilde{\sigma}_t^2=\int_0^t e^{-2c(t-s)}\sigma_s^2 ds\,.
\end{align}
\item Therefore we want to consider the objective
\begin{align}
g_\theta(x_0,y)\coloneqq& \sum_{j=1}^J \gamma_j \|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)+\widetilde{\sigma}^{-1}_{t_j}y\|^2\\
=& \sum_{j=1}^J \gamma_j \left(\|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|^2+2\widetilde{\sigma}^{-1}_{t_j} s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\cdot y+\widetilde{\sigma}^{-2}_{t_j}\|y\|^2\right)
\end{align}
Assuming $\theta\mapsto s^\theta(x,t)$ is Lipschitz w.r.t. $d_\Theta$ with Lipschitz constant $\alpha_t+\beta_t\|x\|$ and the score satisfies a linear growth bound $\|s^\theta(x,t)\|\leq a_t+b_t\|x\|$. Then we have the growth bound
\begin{align}
|g_\theta(x_0,y)|\leq&\sum_{j=1}^J \gamma_j \left(\|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|^2+2\widetilde{\sigma}^{-1}_{t_j}\|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\| \|y\|+\widetilde{\sigma}^{-2}_{t_j}\|y\|^2\right)\\
\leq&\sum_{j=1}^J \gamma_j \left((a_{t_j}+b_{t_j}\| e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y\|)^2+2\widetilde{\sigma}^{-1}_{t_j}(a_{t_j}+b_{t_j}\| e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y\|)\|y\|+\widetilde{\sigma}^{-2}_{t_j}\|y\|^2\right)\notag
\end{align}
and the Lipschitz bound
\begin{align}
&|g_{\theta_1}(x_0,y)-g_{\theta_2}(x_0,y)|\\
\leq&\sum_{j=1}^J \gamma_j \left|\|s^{\theta_1}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|^2-\|s^{\theta_2}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|^2\right|\\
&+2\sum_{j=1}^J \gamma_j \widetilde{\sigma}^{-1}_{t_j} \left|(s^{\theta_1}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)- s^{\theta_2}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j))\cdot y\right|\\
\leq&\sum_{j=1}^J \gamma_j (\|s^{\theta_1}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|+\|s^{\theta_2}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|)\|s^{\theta_1}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)-s^{\theta_2}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|\\
&+2\sum_{j=1}^J \gamma_j \widetilde{\sigma}^{-1}_{t_j} \|s^{\theta_1}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)- s^{\theta_2}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|\|y\|\\
\leq& \sum_{j=1}^J \gamma_j (\alpha_{t_j}+\beta_{t_j}\|e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y\|)
 \bigg(2(a_{t_j}+b_{t_j}\| e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y\|)+2 \widetilde{\sigma}^{-1}_{t_j} \|y\| \bigg) d_\Theta(\theta_1,\theta_2)
\end{align}
In particular, we will be interested in the case where $P_{X_0}$ has compact support, and define $r_0\coloneqq \sup_{x_0\in\text{supp} P_{X_0}}\|x_0\|<\infty$. In this case there exists $h\in L^1(P_{X_0}\times P)$ that satisfies $|g_\theta|\leq h$ for all $\theta$   
and we also have
\begin{align}
&|g_{\theta_1}(x_0,y)-g_{\theta_2}(x_0,y)|\\
\leq& \bigg( \sum_{j=1}^J \gamma_j 
 (2a_{t_j}+ 2r_0e^{-ct_j}b_{t_j})(\alpha_{t_j}+r_0e^{-ct_j}\beta_{t_j})+\sum_{j=1}^J\gamma_j(\beta_{t_j}\widetilde{\sigma}_{t_j}(2a_{t_j}+ 2r_0e^{-ct_j}b_{t_j})\\
&+(2b_{t_j}\widetilde{\sigma}_{t_j}+2 \widetilde{\sigma}^{-1}_{t_j} )(\alpha_{t_j}+r_0e^{-ct_j}\beta_{t_j}))\|y\|+\sum_{j=1}^J\gamma_j(2b_{t_j}\widetilde{\sigma}_{t_j}+2 \widetilde{\sigma}^{-1}_{t_j} ) \beta_{t_j}\widetilde{\sigma}_{t_j}\|y\|^2 \bigg) d_\Theta(\theta_1,\theta_2)\\
\coloneqq &(A_\Theta+B_\Theta\|y\|+C_\Theta\|y\|^2) d_\Theta(\theta_1,\theta_2)
\end{align}

Therefore Assumption \ref{assump:mean_bound} holds ($c_\theta\coloneqq 0$).
\item Assume also that $x\mapsto s^\theta(x,t)$ is $L_t$-Lipschitz for all $\theta\in\Theta$. Then
\begin{align}
&|g_\theta(x_0,y)-g_\theta({x}_0,\tilde{y})|\\
\leq&\sum_{j=1}^J \gamma_j  ( \|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)+\widetilde{\sigma}^{-1}_{t_j}y\|+ \|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}\tilde y,t_j)+\widetilde{\sigma}^{-1}_{t_j}\tilde y\|)\\
&\times\|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)-s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}\tilde y,t_j)+\widetilde{\sigma}^{-1}_{t_j}(y-\tilde y)\|\\
\leq& \sum_{j=1}^J \gamma_j  ( 2(a_{t_j}+b_{t_j} r_0e^{-ct_j})+(b_{t_j}\widetilde{\sigma}_{t_j}+\widetilde{\sigma}^{-1}_{t_j})(\|y\| +\|\tilde y\|))\\
&\times( L_{t_j}\widetilde{\sigma}_{t_j} 
+\widetilde{\sigma}^{-1}_{t_j})\|y-\tilde y\|\\
&\coloneqq (A_\mathcal{Y}+B_{\mathcal{Y}}(\|y\|+\|\tilde{y}\|))\|y-\tilde{y}\|\coloneqq h_{\mathcal{Y}}(y,\tilde{y})\in L^1(P_Y\times P_Y)
\end{align}
which is sub-exponential with respect to $d\zeta\times d P_Y\times dP_Y$ and
\begin{align}
&|g_\theta(x_0,y)-g_\theta(\tilde{x}_0,y)|\\
\leq&\sum_{j=1}^J \gamma_j| \|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)\|^2- \|s^{\theta}( e^{-ct_j}\tilde{x}_0+\widetilde{\sigma}_{t_j}y,t_j)\|^2|\\
&+\sum_{j=1}^J  2\widetilde{\sigma}^{-1}_{t_j} \gamma_j\| s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)-s^{\theta}( e^{-ct_j}\tilde{x}_0+\widetilde{\sigma}_{t_j}y,t_j)\|\|y\|\\
\leq&\sum_{j=1}^J \gamma_j(2a_{t_j}+b_{t_j} e^{-ct_j}(\|x_0\|+\|\tilde{x}_0\|)+2b_{t_j}\widetilde{\sigma}_{t_j}\|y\|) L_{t_j}e^{-ct_j}\| x_0- \tilde{x}_0\|\\
&+\sum_{j=1}^J  2\widetilde{\sigma}^{-1}_{t_j} \gamma_jL_{t_j}e^{-ct_j}\| x_0-\tilde{x}_0\|\|y\|\\
\end{align}

As we are assuming only a priori knowledge of the support of $P_{X_0}$.  Let $D_{X_0}$ be the support of $P_{X_0}\times P_{X_0}$.  For $(x,\tilde{x})\in D_{X_0}$ we therefore have
\begin{align}
&|g_\theta(x_0,y)-g_\theta(\tilde{x}_0,y)|\\
\leq& 2r_0\sum_{j=1}^J \gamma_j(2a_{t_j}+2b_{t_j} r_0e^{-ct_j}+2b_{t_j}\widetilde{\sigma}_{t_j}\|y\|) L_{t_j}e^{-ct_j}\\
&+2r_0\sum_{j=1}^J  2\widetilde{\sigma}^{-1}_{t_j} \gamma_jL_{t_j}e^{-ct_j}\|y\|\\
&\coloneqq A_{\mathcal{X}}+B_{\mathcal{X}}\|y\|\coloneqq h_{\mathcal{X}}(x,\tilde{x},y)\in L^1(P_{X_0}\times P_{X_0}\times P_Y)
\end{align}
\item $\|y\|$ is sub-Gaussian under $P_Y$, therefore there exists $\sigma_{Y}$ such that
\begin{align}
&\int \exp\left(\lambda (h_{\mathcal{X}}(x,\tilde{x},y)- E_{P_Y}[h_{\mathcal{X}}(x,\tilde{x},\cdot)])\right)dP_Y\\
=&\int \exp\left(B_{\mathcal{X}}\lambda (\|y\|- E_{P_Y}[\|y\|])\right)dP_Y\\
\leq & e^{\sigma_{Y}^2B_{\mathcal{X}}^2\lambda^2/2} \coloneqq e^{\xi_{X,Y}(|\lambda|)}
\end{align}
for all $\lambda\in\mathbb{R}$ (so $K_{X,Y}=\infty$).


We have:
\begin{align}\label{eq:xi_X_def}
&\int  \cosh\left(\lambda  E_{P_Y}[h_{\mathcal{X}}(x,\tilde{x},\cdot)]\right)  (P_X\times P_X)(dxd\tilde{x}) \\
=&\cosh\left(\lambda  E_{P_Y}[ A_{\mathcal{X}}+B_{\mathcal{X}}\|y\|]\right)  \\
\leq&\exp\left(\lambda^2  ( A_{\mathcal{X}}+B_{\mathcal{X}}E_{P_Y}[\|y\|])^2/2\right)\coloneqq e^{\xi_X(|\lambda|)}
\end{align}
for all $\lambda\in\mathbb{R}$ (can pick $K_X=mK_Y$)

\begin{align}
&\int |\zeta h_{\mathcal{Y}}(y,\tilde{y})|^pd\zeta dP_YdP_Y^{1/p}\\
=&\|A_\mathcal{Y}\|y-\tilde{y}\|+B_{\mathcal{Y}}(\|y\|+\|\tilde{y}\|)\|y-\tilde{y}\|\|_{L^p}\\
\leq&A_\mathcal{Y}\|\|y-\tilde{y}\|\|_{L^p}+B_{\mathcal{Y}}\|(\|y\|+\|\tilde{y}\|)\|y-\tilde{y}\|\|_{L^p}\\
\leq&2A_\mathcal{Y}\|\|y\|\|_{L^p}+2B_{\mathcal{Y}}\|\|y\|\|_{L^{2p}}\|\|y-\tilde{y}\|\|_{L^{2p}}\\
\leq&2A_\mathcal{Y}\|\|y\|\|_{L^p}+4B_{\mathcal{Y}}\|\|y\|\|_{L^{2p}}^2
\end{align}
$\|y\|$ is sub-Gaussian, therefore there exists $K$ such that
\begin{align}
\int |\zeta h_{\mathcal{Y}}(y,\tilde{y})|^pd\zeta dP_YdP_Y^{1/p}
\leq&2A_\mathcal{Y}K\sqrt{p}+4B_{\mathcal{Y}}(K\sqrt{2p})^2\\
\leq&2A_\mathcal{Y}K\sqrt{p}+8B_{\mathcal{Y}}K^2p\\
\leq&2(A_\mathcal{Y}+4B_{\mathcal{Y}})\max\{K,K^2\}p
\end{align}
Therefore  $\zeta h_{\mathcal{Y}}$ is sub-exponential and there exists $C$ s.t.
\begin{align}
\int e^{\lambda\zeta h_{\mathcal{Y}}(y,\tilde{y})}d\zeta dP_YdP_Y\leq e^{4C^2(A_\mathcal{Y}+4B_{\mathcal{Y}})^2\max\{K,K^2\}^2\lambda^2}
\end{align}
for all $|\lambda|\leq 1/(2C(A_\mathcal{Y}+4B_{\mathcal{Y}})\max\{K,K^2\})$

i.e. there exists $\widetilde{\sigma}_Y$ (depending only on the distribution of $Y$) s.t.
\begin{align}
\int e^{\lambda\zeta h_{\mathcal{Y}}(y,\tilde{y})}d\zeta dP_YdP_Y\leq e^{4(A_\mathcal{Y}+4B_{\mathcal{Y}})^2 \widetilde{\sigma}_Y^2\lambda^2}
\end{align}
for all $|\lambda|\leq 1/(2(A_\mathcal{Y}+4B_{\mathcal{Y}})\widetilde{\sigma}_Y)\coloneqq K_Y$.  Therefore Assumption \ref{assump:ULLN} holds.

Finally, suppose  we have  $X_i\sim P_{X_0}$,   and $Y_{i,j}\sim P_Y$, $i=1,...,n$, $j=1,...,m$,  that are all  independent  and 
 a $\Theta$-valued random variable $\theta^*_{n,m}$ that is an approximate minimizer of the empirical DSM problem:
\begin{align}\label{eq:empirical_optim}
&\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta^*_{n,m}}(X_i,Y_{i,j})\leq \inf_{\theta\in\Theta}\frac{1}{nm}\sum_{i=1}^n \sum_{j=1}^m g_{\theta}(X_i,Y_{i,j})+\epsilon_{n,m}^{\text{opt}} \,\,\,\,\,\mathbb{P}\text{-a.s.}\,,\\
&g_\theta(x_0,y)\coloneqq \sum_{j=1}^J \gamma_j \|s^{\theta}( e^{-ct_j}x_0+\widetilde{\sigma}_{t_j}y,t_j)+\widetilde{\sigma}^{-1}_{t_j}y\|^2
\end{align}
Then Assumption \ref{assump:X_Y} holds and we can therefore apply Theorem \ref{thm:conc_ineq_stoch_opt} to conclude that for $t\geq 0$ and letting $s(x,t)\coloneqq \nabla \log \eta(x,t)$ be the exact score, we have
\begin{align}
&\mathbb{P}\left(\sum_{j=1}^J \gamma_j{ E}_{ X_{t_j}}\!\!\left[ \|s^{\theta^*_{n,m}}(x_{t_j},t_j)-s(x_{t_j},t_j)\|^2\right]-\inf_\theta \sum_{j=1}^J \gamma_j{ E}_{ X_{t_j}}\!\!\left[ \|s^\theta(x_{t_j},t_j)-s(x_{t_j},t_j)\|^2\right]\geq t+C_{n,m}+\epsilon_{n,m}^{\text{opt}}\right)\\
=&\mathbb{P}\left(E_{P_X\times P_Y}[g_{\theta^*_{n,m}}]-R_*\geq t+C_{n,m}+\epsilon_{n,m}^{\text{opt}}\right)\\
\leq&\exp\left(-n\sup_{\lambda\in[0,mK_Y)}\left\{\lambda t/2-\lambda^2(  ( A_{\mathcal{X}}+B_{\mathcal{X}}E_{P_Y}[\|y\|])^2/2+
\sigma_{Y}^2B_{\mathcal{X}}^2/(2m)
+4(A_\mathcal{Y}+4B_{\mathcal{Y}})^2 \widetilde{\sigma}_Y^2/m)\right\}\right)\,,\notag
\end{align}
where
\begin{align}
C_{n,m}\coloneqq &4\!\left(\!(1-1/m)\!  \left(A_\Theta+B_\Theta E_{P_Y}[ \|y\|]+C_\Theta E_{P_Y}[ \|y\|^2]\right) ^2 +\frac{1}{m} E_{ P_Y}\left[(A_\Theta+B_\Theta\|y\|+C_\Theta\|y\|^2)^2\right]\right)^{1/2}\\
&\times\inf_{\eta>0}\left\{4\eta+1_{\eta<D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{D_\Theta/2}\sqrt{\log N(\epsilon,\Theta,d_\Theta)}d\epsilon\right\}\notag\,.
\end{align}

In particular, if $t\leq 4mK_Y (  ( A_{\mathcal{X}}+B_{\mathcal{X}}E_{P_Y}[\|y\|])^2/2+
\sigma_{Y}^2B_{\mathcal{X}}^2/(2m)
+4(A_\mathcal{Y}+4B_{\mathcal{Y}})^2 \widetilde{\sigma}_Y^2/m)$ then
\begin{align}
&\sup_{\lambda\in[0,mK_Y)}\left\{\lambda t/2-\lambda^2(  ( A_{\mathcal{X}}+B_{\mathcal{X}}E_{P_Y}[\|y\|])^2/2+
\sigma_{Y}^2B_{\mathcal{X}}^2/(2m)
+4(A_\mathcal{Y}+4B_{\mathcal{Y}})^2 \widetilde{\sigma}_Y^2/m)\right\}\\
=&\frac{t^2}{16(  ( A_{\mathcal{X}}+B_{\mathcal{X}}E_{P_Y}[\|y\|])^2/2+
\sigma_{Y}^2B_{\mathcal{X}}^2/(2m)
+4(A_\mathcal{Y}+4B_{\mathcal{Y}})^2 \widetilde{\sigma}_Y^2/m)}
\end{align}


\end{itemize}
\end{comment}

\section{Conclusion}
In this work we  derived  a   generalization of McDiarmid's inequality that applies to appropriate families of unbounded functions.  Building on the earlier work of   \cite{kontorovich2014concentration} and \cite{maurer2021concentration}, we  obtained  results that cover a significantly  wider class of locally-Lipschitz objective functions, including those that occur in applications such as denoising score matching (DSM).    Using this result, along with the distribution-dependent Rademacher complexity bound in Theorem \ref{thm:Rademacher_bound_unbounded_support}, we derived novel uniform law of large numbers results in Lemma \ref{lemma:variable_reuse_expectation_bound} and Theorem \ref{thm:ULLN_var_reuse_mean_bound}  as well as new concentration inequalities for stochastic optimization problems in Theorem \ref{thm:conc_ineq_stoch_opt}, all of which apply to appropriate unbounded (objective) functions.  We also showed that  our method is able to capture the benefits of sample-reuse in algorithms that  pair training data with  easily-sampled (e.g., Gaussian) auxiliary random variables.  In particular, in Section \ref{sec:DSM}  we show how these results can be used to derive    statistical guarantees for DSM.





\appendix


\section{Distribution-Dependent Rademacher Complexity Bound}\label{app:Rademacher_bound}\label{app:rademacher_bound}

 In this section we derive a distribution-dependent Rademacher complexity bound that applies to collections of functions that satisfy a  local Lipschitz property with respect to a parameterizing metric space. Finiteness of our bound only requires that the local Lipchitz constant have finite second moment, as opposed to the much stronger assumptions made in other approaches to distribution-dependent bounds, e.g., the sub-Gaussian and sub-exponential  assumptions required in  Proposition 20 of \cite{biau2021some} and Theorem 22 of \cite{huang2022error} respectively.  Distribution-dependent bounds are especially useful in applications such as DSM, which use auxiliary random variables with known distribution,  despite the distribution of the data being unknown.    A less general version of result (i.e., for $\eta=0$)  was previously proven in \cite{birrell2025statisticalerrorboundsgans}.  

First recall the definition of Rademacher complexity.
\begin{definition}\label{def:Rademacher_complexity}
Let $\mathcal{G}$ be a collection of functions on $\mathcal{Z}$ and $n\in\mathbb{Z}^+$.  We define the  empirical Rademacher complexity of $\mathcal{G}$ at  $z\in\mathcal{Z}^n$  by
\begin{align}\label{eq:emp_rad_def}
\widehat{\mathcal{R}}_{\mathcal{G},n}(z)\coloneqq E_\sigma\left[\sup_{g\in\mathcal{G}}\left\{\frac{1}{n}\sigma\cdot g_n(z)\right\}\right]\,,
\end{align}
where $\{\sigma_i\}_{i=1}^n$  are independent uniform random variables taking values in $\{-1,1\}$, i.e., Rademacher random variables, and $g_n(z)\coloneqq(g(z_1),...,g(z_n))$.  Given a probability distribution $P$ on $\mathcal{Z}$ and assuming that $\widehat{\mathcal{R}}_{\mathcal{G},n}$ is measurable, we define the $P$-Rademacher complexity of $\mathcal{G}$  by
\begin{align}\label{eq:Rademacher_def}
\mathcal{R}_{\mathcal{G},P,n}\coloneqq E_{P^n}\left[\widehat{\mathcal{R}}_{\mathcal{G},n}\right]\,,
\end{align}
where $P^n$ is the $n$-fold product of $P$, i.e., the $z_i$'s become i.i.d. samples from $P$.  
\end{definition}


The Rademacher complexity bound we prove in Theorem \ref{thm:Rademacher_bound_unbounded_support} below will rely on  Dudley's entropy integral; however, we could not locate a standard version   in the literature that suffices for our purposes.  Thus, we will first present a proof of the variant we require, building on Lemma 13.1 in \cite{boucheron2013concentration} as well as Theorem 5.22 in \cite{wainwright2019high}.  First recall the definition of a sub-Gaussian process.
\begin{definition}
Let $(T,d)$ be a pseudometric space and $X_t$, $t\in T$ be mean-zero real-valued random variables on the same probability space.  We call  $X_t$ a sub-Gaussian process with respect to  $d$ if
\begin{align}
E\left[e^{\lambda(X_t-X_{t^\prime})}\right]\leq \exp\left(\frac{\lambda^2d(t,t^\prime)^2}{2}\right)
\end{align}
for all $t,t^\prime\in T$, $\lambda\in\mathbb{R}$. 
\end{definition}
\begin{theorem}\label{thm:Dudley}
Let $X_t$, $t\in T$ be a mean-zero sub-Gaussian process with respect to  a pseudometric $d$ on $T$. Then, assuming the suprema involved are measurable (e.g., if they can be restricted to countable subsets),  we have
\begin{align}\label{eq:Dudley_bound}
E\left[\sup_{t\in T} X_t\right]\leq&E\left[\sup_{t,t^\prime\in T}(X_t-X_{t^\prime})\right]\\
\leq&\inf_{\eta>0}\left\{2E\left[\sup_{d(t,\hat{t})\leq2\eta}(X_t-X_{\hat{t}})\right]+1_{\eta<D_T}8\sqrt{2}\int_{\eta/2}^{D_T/2}\sqrt{\log N(\epsilon,T,d)} d\epsilon\right\}\,,\notag
\end{align}
where $D_T\coloneqq\sup_{t,t^\prime\in T}d(t,t^\prime)$ is the diameter of $(T,d)$.
\end{theorem}
\begin{remark}
The technical features  we  require for our proof of Theorem \ref{thm:Rademacher_bound_unbounded_support} below is that the bound   \eqref{eq:Dudley_bound} is written as the infimum over all $\eta\in(0,\infty)$, where $\eta$ controls the lower bound on the domain of integration (so as to handle  cases where the covering number is not integrable at $0$), of an objective that is non-decreasing in the diameter, $D_T$.  The latter property is necessary for us to make use of the local Lipschitz bound that is the key to our argument in Theorem \ref{thm:Rademacher_bound_unbounded_support}.
\end{remark}
\begin{proof}
The $X_t$'s have mean zero, hence we  have
\begin{align}
0\leq E\left[\sup_t X_t\right]=E\left[\sup_t (X_t-X_{t^\prime})\right]\leq E\left[\sup_{t,t^\prime} (X_t-X_{t^\prime})\right]\,.
\end{align}
Therefore it suffices to show
\begin{align}
E\left[\sup_{t,t^\prime\in T}(X_t-X_{t^\prime})\right]\leq 2E\left[\sup_{d(t,\hat{t})<2\eta}(X_t-X_{\hat{t}})\right]+1_{\eta<D_T}8\sqrt{2}\int_{\eta/2}^{D_T/2}\sqrt{\log N(\epsilon,T,d)} d\epsilon
\end{align}
for all $\eta>0$.  If $\eta\geq D_T$ then  this is trivial, as all $t,\hat{t}\in T$ satisfy $d(t,\hat{t})< 2\eta$.  If $\eta<D_T$ and $N(\epsilon,T,d)=\infty$ for some $\epsilon>\eta/2$ then the right-hand side equals $\infty$ due to $\epsilon\mapsto N(\epsilon,T,d)$ being non-increasing, and hence the bound is again trivial. Therefore, from here on we restrict our attention to  the case where $\eta<D_T$  and   $N(\epsilon,T,d)<\infty$ for all $\epsilon>\eta/2$; in particular, this implies  $D_T$ is finite.





Let $k=\lfloor \log_2(D_T/\eta)\rfloor\in\mathbb{Z}_0$ so that $\eta\in(2^{-(k+1)}D_T, 2^{-k}D_T]$.  Therefore $N(2^{-i}D_T,T,d)<\infty$ for all $i=0,...,k$.  For $i=0,..,k$ let $\hat{T}_i$ be a minimal $2^{-i}D_T$-cover of $T$, i.e., $|\hat{T}_i|=N(2^{-i}D_T,T,d)<\infty$. Note that $|\hat{T}_0|=1$, as any element of $T$ is a minimal $D_T$-cover.   

For $i=1,...,k$, given $t\in \hat{T}_i$ there exists $\tilde{t}\in \hat{T}_{i-1}$ such that $d(\tilde{t},t)\leq 2^{-(i-1)}D_T$.  Therefore we can  define $g_i:\hat{T}_i\to \hat{T}_{i-1}$ such that $d(g_i(t),t)\leq 2^{-(i-1)}D_T$.  For $i=0,...,k$ inductively define $\pi_{k-i}:\hat{T}_k\to \hat{T}_{k-i}$ by  $\pi_k=Id$, $\pi_{k-i}=g_{k-(i-1)}\circ \pi_{k-(i-1)}$. Therefore for all $t\in\hat{T}_k$, $i=1,...,k$ we have  $d(\pi_{i-1}(t),\pi_i(t))=d(g_i(\pi_i(t)),\pi_i(t))\leq 2^{-(i-1)}D_T$.


Noting that $\pi_0$ is a constant map,  for $\hat{t},\hat{t}^\prime\in \hat{T}_k$ we can now compute
\begin{align}
X_{\hat{t}}-X_{\hat{t}^\prime}=&\sum_{i=1}^k (X_{\pi_i(\hat{t})}-X_{\pi_{i-1}(\hat{t})})+\sum_{i=1}^k (X_{\pi_{i-1}(\hat{t}^\prime)}-X_{\pi_{i}(\hat{t}^\prime)})\\
=&\sum_{i=1}^k (X_{\pi_i(\hat{t})}-X_{g_i(\pi_{i}(\hat{t}))})+\sum_{i=1}^k (X_{g_i(\pi_{i}(\hat{t}^\prime))}-X_{\pi_{i}(\hat{t}^\prime)})\notag\\
\leq&\sum_{i=1}^k \sup_{\hat{t}\in \hat{T}_i} (X_{\hat{t}}-X_{g_i(\hat{t})})+\sum_{i=1}^k \sup_{\hat{t}\in \hat{T}_i}  (X_{g_i(\hat{t})}-X_{\hat{t}})\,.\notag
\end{align}

For $t,t^\prime\in T$ there exists $\hat{t},\hat{t}^\prime\in\hat{T}_k$ with $d(t,\hat{t})\leq 2^{-k}D_T$ and $d(t^\prime,\hat{t}^\prime)\leq 2^{-k}D_T$. Hence
\begin{align}
&X_t-X_{t^\prime}= X_t-X_{\hat{t}}+X_{\hat{t}}-X_{\hat{t}^\prime}+X_{\hat{t}^\prime}-X_{t^\prime}\\
\leq& 2\sup_{d(t,\hat{t})\leq2^{-k}D_T}(X_t-X_{\hat{t}})+\sum_{i=1}^k \sup_{\hat{t}\in \hat{T}_i} (X_{\hat{t}}-X_{g_i(\hat{t})})+\sum_{i=1}^k \sup_{\hat{t}\in \hat{T}_i}  (X_{g_i(\hat{t})}-X_{\hat{t}})\,.\notag
\end{align}

Therefore we can use the sub-Gaussian case of the maximal inequality from  Theorem 2.5 in \cite{boucheron2013concentration} together with the fact that $\epsilon\mapsto N(\epsilon,T,d)$ is non-increasing to compute
\begin{align}
&E\left[\sup_{t,t^\prime\in T}(X_t-X_{t^\prime})\right]\\
\leq& 2E\left[\sup_{d(t,\hat{t})\leq2^{-k}D_T}(X_t-X_{\hat{t}})\right]+\sum_{i=1}^k E\left[\sup_{\hat{t}\in \hat{T}_i} (X_{\hat{t}}-X_{g_i(\hat{t})})\right]+\sum_{i=1}^k E\left[\sup_{\hat{t}\in \hat{T}_i}  (X_{g_i(\hat{t})}-X_{\hat{t}})\right]\notag\\
\leq&2E\left[\sup_{d(t,\hat{t})\leq2^{-k}D_T}(X_t-X_{\hat{t}})\right]+2\sum_{i=1}^k \max_{\hat{t}\in \hat{T}_i}d(\hat{t},g_i(\hat{t}))\sqrt{2\log|\hat{T}_i|}\notag\\
\leq&2E\left[\sup_{d(t,\hat{t})\leq2^{-k}D_T}(X_t-X_{\hat{t}})\right]+2\sum_{i=1}^k 2^{-(i-1)}D_T\sqrt{2\log N(2^{-i}D_T,T,d)}\notag\\
\leq&2E\left[\sup_{d(t,\hat{t})\leq2^{-k}D_T}(X_t-X_{\hat{t}})\right]+8\sqrt{2}\sum_{i=1}^k \int_{2^{-i-1}D_T}^{2^{-i}D_T}\sqrt{\log N(\epsilon,T,d)} d\epsilon\notag\\
\leq&2E\left[\sup_{d(t,\hat{t})< 2\eta}(X_t-X_{\hat{t}})\right]+8\sqrt{2}\int_{\eta/2}^{\frac{1}{2}D_T}\sqrt{\log N(\epsilon,T,d)} d\epsilon\,,\notag
\end{align}
where to obtain the last line we used  $2^{-k-1}D_T\geq \eta/2$ and $2\eta>2^{-k}D_T$.  This  completes the proof.
\end{proof}

We now use the above version of Dudley's entropy integral  to derive a distribution-dependent Rademacher complexity bound.  We work with collections of functions that have the following properties.
\begin{assumption}\label{assump:unbounded support}
Suppose  $(\Theta,d_\Theta)$ is a pseudometric space, $(\mathcal{Z},P)$ is a probability space, and we have a collection of measurable real-valued functions on $\mathcal{Z}$, $\mathcal{G}=\{g_\theta:\theta\in\Theta\}$, that satisfy the following properties.
\begin{enumerate}
\item For all $z\in\mathcal{Z}$  there exists $L(z)>0$ such that $\theta\mapsto g_\theta(z)$ is $L(z)$-Lipschitz.  
\item     $L\in L^2(P)$.
\end{enumerate}
\begin{remark}
Many commonly used neural network architectures satisfy a local Lipschitz bound of the above form with $\mathcal{Z}=\mathbb{R}^d$, $\Theta$ a bounded subset of $\mathbb{R}^k$, and $L(z)=a+b\|z\|$ for some $a,b$.
\end{remark}
\end{assumption}
\begin{theorem}\label{thm:Rademacher_bound_unbounded_support}
Under Assumption \ref{assump:unbounded support} and letting $D_\Theta\coloneqq\sup_{\theta_1,\theta_2\in\Theta}d_\Theta(\theta_1,\theta_2)$ denote the diameter of $\Theta$,  for all $n\in\mathbb{Z}^+$ we have the empirical Rademacher complexity bound
\begin{align}\label{eq:emp_rademacher_bound_L}
\widehat{R}_{\mathcal{G},n}(z)\leq&L_n(z)\inf_{\eta>0}\left\{4\eta+1_{\eta<  D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{ D_\Theta/2}\sqrt{\log  N(\epsilon,\Theta,d_{\Theta})}  d\epsilon\right\}\,, 
\end{align} 
where $L_n(z)\coloneqq \left(\frac{1}{n}\sum_{i=1}^n L(z_i)^2\right)^{1/2}$. Assuming the requisite measurability property, we also obtain the $P$-Rademacher complexity bound
\begin{align}\label{eq:rademacher_bound_L}
\mathcal{R}_{\mathcal{G},P,n}\leq   E_P[L^2]^{1/2}\inf_{\eta>0}\left\{4\eta+1_{\eta<  D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{ D_\Theta/2}\sqrt{\log  N(\epsilon,\Theta,d_{\Theta})}  d\epsilon\right\}\,.
\end{align}
 
Moreover, if $\Theta$ is the unit ball in $\mathbb{R}^k$ under some norm and $d_\Theta$ is the corresponding metric  then  we have
\begin{align}\label{eq:entropy_int_bound_unit_ball}
\inf_{\eta>0}\left\{4\eta+1_{\eta<  D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{ D_\Theta/2}\sqrt{\log  N(\epsilon,\Theta,d_{\Theta})}  d\epsilon\right\}  \leq 32 (k/n)^{1/2}\,.
\end{align}
\end{theorem}
\begin{proof}
Let $\sigma_i$, $i=1,...,n$ are independent uniform random variables taking values in $\{-1,1\}$. It is well-known that the Rademacher process  $X_t\coloneqq \frac{t\cdot \sigma}{n}$, $t\in\mathbb{R}^n$, is a mean zero sub-Gaussian with respect to   the norm $\|\cdot\|_2/n$, see, e.g., Chapter 5 in \cite{wainwright2019high}.  Therefore, for $T\subset\mathbb{R}^n$,  Theorem \ref{thm:Dudley} implies
\begin{align} \label{eq:emp_rad_bound1}
&E_\sigma\left[\sup_{t\in T}  \frac{t\cdot \sigma}{n}\right]\\
\leq&\inf_{\eta>0}\left\{2E_\sigma\left[\sup_{\|t-\hat{t}\|_2/n\leq2\eta}\frac{(t-t^\prime)\cdot \sigma}{n}\right]+1_{\eta<D_T}8\sqrt{2}\int_{\eta/2}^{D_T/2}\sqrt{\log N(\epsilon,T,\|\cdot\|_2/n)} d\epsilon\right\}\notag\\
\leq&\inf_{\eta>0}\left\{4\eta\sqrt{n}+1_{\eta<D_T}8\sqrt{2}\int_{\eta/2}^{D_T/2}\sqrt{\log N(\epsilon,T,\|\cdot\|_2/n)} d\epsilon\right\}\,,\notag
\end{align}
where $D_T$ is the diameter of $T$ under $\|\cdot\|_2/n$.


Now define  the map $H:\Theta\to \mathcal{G}_n(z)\coloneqq \{  (g_\theta(z_1),...,g_\theta(z_n)):\theta\in\Theta\}$, $H(\theta)= (g_\theta(z_1),...,g_\theta(z_n))$.   Defining a norm  on $\mathbb{R}^n$  by $\|t\|_{L^2(n)}\coloneqq\sqrt{\frac{1}{n}\sum_{i=1}^n t_i^2}=\|t\|_2/\sqrt{n}$, we can  use Assumption \ref{assump:unbounded support} to compute
\begin{align}
\|H(\theta_1)-H(\theta_2)\|_{L^2(n)}^2=&\frac{1}{n}\sum_{i=1}^n(g_{\theta_1}(z_i)-g_{\theta_2}(z_i))^2\\
\leq&\frac{1}{n}\sum_{i=1}^n L(z_i)^2d_\Theta(\theta_1,\theta_2)^2\,.\notag
\end{align}
Therefore $H$ is   $L_n(z)\coloneqq \left(\frac{1}{n}\sum_{i=1}^n L(z_i)^2\right)^{1/2}$-Lipschitz with respect to $(d_{\Theta},\|\cdot\|_{L^2(n)})$.   Clearly, $H$ is also onto and so we can conclude the following relation between covering numbers,
\begin{align}
N(\epsilon,\mathcal{G}_n(z),\|\cdot\|_{L^2(n)})\leq N(\epsilon/L_n(z),\Theta,d_{\Theta})\,,
\end{align}
as well as the diameter bounds $D_{\mathcal{G}_n(z),L^2(n)}\leq L_n(z) D_\Theta$, where $D_{\mathcal{G}_n(z),L^2(n)}$ denotes the diameter of $\mathcal{G}_n(z)$ under the  $L^2(n)$ norm.

Letting $T=\mathcal{G}_n(z)$ in \eqref{eq:emp_rad_bound1}, using $N(\epsilon,T,\|\cdot\|_{L^2(n)}/\sqrt{n})=N(\sqrt{n}\epsilon,T,\|\cdot\|_{L^2(n)})$, changing variables in the integral and the infimum, and using the above covering number and diameter bounds we can then compute 
\begin{align}
&\widehat{R}_{\mathcal{G},n}(z)\\
\leq&\inf_{\eta>0}\left\{4\eta\sqrt{n}+1_{\sqrt{n}\eta<D_{\mathcal{G}_n(z),L^2(n)}}8\sqrt{2}\int_{\eta/2}^{D_{\mathcal{G}_n(z),L^2(n)}/(2\sqrt{n})}\sqrt{\log N(\epsilon,\mathcal{G}_n(z),\|\cdot\|_{L^2(n)}/\sqrt{n})} d\epsilon\right\}\notag\\
=&\inf_{\eta>0}\left\{4\eta+1_{\eta<D_{\mathcal{G}_n(z),L^2(n)}}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{D_{\mathcal{G}_n(z),L^2(n)}/2}\sqrt{\log N(\epsilon,\mathcal{G}_n(z),\|\cdot\|_{L^2(n)})}  d\epsilon\right\}\notag\\
\leq&\inf_{\eta>0}\left\{4\eta+1_{\eta< L_n(z) D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{ L_n(z) D_\Theta/2}\sqrt{\log  N(\epsilon/L_n(z),\Theta,d_{\Theta})}  d\epsilon\right\}\notag\\
=&L_n(z)\inf_{\eta>0}\left\{4\eta+1_{\eta<  D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{ D_\Theta/2}\sqrt{\log  N(\epsilon,\Theta,d_{\Theta})}  d\epsilon\right\}\,.\notag
\end{align}
This proves \eqref{eq:emp_rademacher_bound_L}. Taking the expectation of this empirical Rademacher complexity bound with respect to $P^n$ and then using the Cauchy-Schwarz inequality to compute
\begin{align}
E_{P^n}[L_n]\leq\left(E_{P^n}\left[\frac{1}{n}\sum_{i=1}^n L(z_i)^2\right]\right)^{1/2}=E_P[L^2]^{1/2}
\end{align}
we arrive at \eqref{eq:rademacher_bound_L}.





Finally, if $\Theta$ is the unit ball in $\mathbb{R}^k$ with respect to the norm $\|\cdot\|_{\Theta}$ and $d_\Theta$ is the associated metric then $D_{\Theta}\leq 2$ and we have the covering number bound  $N(\epsilon,\Theta,d_{\Theta})\leq (1+2/\epsilon)^k$ (see, e.g., Example 5.8 in \cite{wainwright2019high}).  In this case the covering number is integrable at $\epsilon=0$ and so we simplify the bound  by taking $\eta\to 0$ and compute
\begin{align}
&\inf_{\eta>0}\left\{4\eta+1_{\eta<  D_\Theta}8\sqrt{2}n^{-1/2}\int_{\eta/2}^{ D_\Theta/2}\sqrt{\log  N(\epsilon,\Theta,d_{\Theta})}  d\epsilon\right\}\\
\leq&8\sqrt{2}  (k/n)^{1/2}\int_{0}^{ 1}\sqrt{\log(1+2/\epsilon)}  d\epsilon\notag\\
\leq& 32  (k/n)^{1/2}\,.\notag
\end{align}
 This proves \eqref{eq:entropy_int_bound_unit_ball}.
\end{proof}


\section{Uniform Law of Large Numbers Mean Bound  for Unbounded Functions}\label{app:ULLN_rademacher}
In this Appendix we  present a uniform law of large numbers (ULLN) mean bound that can  be paired with Theorem  \ref{thm:Rademacher_bound_unbounded_support} to derive computable distribution-dependent ULLN concentration inequalities for appropriate families of unbounded functions.    The argument 
follows the well-known technique  of using symmetrization to convert the problem to one of bounding the Rademacher complexity; see, e.g.,  Theorem 3.3 in \cite{mohri2018foundations}, especially the argument from eq. (3.8)-(3.13).   The only distinction from the result in  \cite{mohri2018foundations}, which assumes the functions are valued in a bounded interval,  is  that we generalize to appropriate families of unbounded functions, as is required, e.g., for the application to DSM in Section \ref{sec:DSM}.  Despite this generalization, the proof is essentially the same; we present it here for completeness.

\begin{theorem}\label{thm:ULLN_unbounded}
Let $(\mathcal{Z},P)$ be a probability space and $\mathcal{G}$ be a countable family of real-valued measurable functions on $\mathcal{Z}$.  Suppose we have a measurable $h\in L^1(P)$ such that $|g|\leq h$ for all $g\in\mathcal{G}$.  Then for $n\in\mathbb{Z}^+$ we have
\begin{align}
E_{P^n}\left[\sup_{g\in\mathcal{G}}\left\{\pm\left(\frac{1}{n}\sum_{i=1}^n g(z_i)-E_P[g]\right)\right\}\right]\leq 2\mathcal{R}_{\mathcal{G},P,n}\,.
\end{align}
\end{theorem}
\begin{remark}
Note that the argument of the supremum is invariant under shifts $g\to g-c_g$, $c_g\in\mathbb{R}$, therefore  one can also this apply this result  to   $\mathcal{G}$'s such that there exists shifts $c_g\in\mathbb{R}$ and $h\in L^1$ satisfying  $|g-c_g|\leq h$ for all $g\in\mathcal{G}$.   Finally, by the usual limiting argument, this result can be applied to appropriate uncountable $\mathcal{G}$ that satisfy a separability property.
\end{remark}
\begin{proof}
First consider the $+$ sign. We have $\mathcal{G}\subset  L^1(P)$, $\sup_{g\in\mathcal{G}}\{\frac{1}{n}\sum_{i=1}^n g(z_i)-E_P[g]\}$ is measurable, and 
\begin{align}
\left|\sup_{g\in\mathcal{G}}\left\{\frac{1}{n}\sum_{i=1}^n g(z_i)-E_P[g]\right\}\right|\leq  \frac{1}{n}\sum_{i=1}^n h(z_i)+E_P[h]\in L^1(P^n)\,.
\end{align}
Therefore the expectation on the left-hand side of the claimed bound  exists and is finite. We have
\begin{align}
&E_{P^n}\left[\sup_{g\in\mathcal{G}}\left\{\frac{1}{n}\sum_{i=1}^n g(z_i)-E_P[g]\right\}\right]\\
=&\int \sup_{g\in\mathcal{G}}\left\{\int\frac{1}{n}\sum_{i=1}^n (g(z_i)-  g(z_i^\prime))P^n(dz^\prime)\right\} P^n(dz)\notag\\
\leq& \int \sup_{g\in\mathcal{G}}\left\{\frac{1}{n}\sum_{i=1}^n (g(z_i)- g(z_i^\prime))\right\}P^{2n}(dz^\prime,dz)\notag\,.
\end{align}
For convenience, we index $(z,z^\prime)\in\mathcal{Z}^{2n}$ by $(z_{-n},...,z_{-1},z_1,...,z_n)$. For $\sigma\in\{-1,1\}^n$ define $H_\sigma:\mathcal{Z}^{2n}\to\mathcal{Z}^{2n}$, $H_\sigma[z]_i=z_{\sigma_{|i|}\cdot i}$.  The pushforward satisfies $(H_\sigma)_{\#}P^{2n}=P^{2n}$ and therefore we can compute
\begin{align}
&E_{P^n}\left[\sup_{g\in\mathcal{G}}\left\{\frac{1}{n}\sum_{i=1}^n g(z_i)-E_P[g]\right\}\right]\\
\leq& \int \sup_{g\in\mathcal{G}}\left\{\frac{1}{n}\sum_{i=1}^n (g(H_\sigma[z]_i)- g(H_\sigma[z]_{-i}))\right\}P^{2n}(dz_{-n},...,dz_{-1},dz_1,...,dz_{n})\notag\\
=&\int \sup_{g\in\mathcal{G}}\left\{\frac{1}{n}\sum_{i=1}^n (g(z_{\sigma_i\cdot i})- g(z_{-\sigma_i\cdot i}))\right\}P^{2n}(dz_{-n},...,dz_{-1},dz_1,...,dz_{n})\notag\\
=&\int \sup_{g\in\mathcal{G}}\left\{\frac{1}{n}\sum_{i=1}^n \sigma_i(g(z_{i})- g(z_{- i}))\right\}P^{2n}(dz_{-n},...,dz_{-1},dz_1,...,dz_{n})\notag\\
\leq& \int \sup_{g\in\mathcal{G}}\left\{\frac{1}{n}\sum_{i=1}^n \sigma_ig(z_{i})\right\}P^{n}(dz_1,...,dz_{n})+\int  \sup_{g\in\mathcal{G}}\left\{\frac{1}{n}\sum_{i=1}^n (-\sigma_i) g(z_{i}))\right\}P^{n}(dz_1,...,dz_{n})\,.\notag
\end{align}
Taking the expectation with respect to the uniform distribution over $\sigma\in\{-1,1\}^n$ we therefore find
\begin{align}
&E_{P^n}\left[\sup_{g\in\mathcal{G}}\left\{\frac{1}{n}\sum_{i=1}^n g(z_i)-E_P[g]\right\}\right]
\leq 2\mathcal{R}_{\mathcal{G},P,n}\,.
\end{align}
 This proves the result with the $+$ sign. To prove it with the $-$ sign, simply note that essentially the same argument goes through with  $\mathcal{G}$ replaced by $-\mathcal{G}$.
\end{proof}

\vskip 0.2in
%\bibliography{refs.bib}
\bibliography{Concentration_ineq_stochastic_opt.bbl}

\end{document}

