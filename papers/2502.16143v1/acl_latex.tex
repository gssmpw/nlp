% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{ulem}
\usepackage{multirow}
\usepackage{fix-cm}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% ======================== add =======================

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{anyfontsize}
\usepackage{subcaption} % 用于创建子图
\usepackage{amssymb}

\usepackage{amsmath}
\usepackage{epigraph}

\usepackage{microtype}
% \usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}

% \usepackage{subfigure}
\usepackage{enumitem}
\newenvironment{itemize*}%
 {\leftmargini=20pt\begin{itemize}%
  \setlength{\itemsep}{3pt}%
  \setlength{\parskip}{0pt}%
  }%
 {\end{itemize}}
\newenvironment{enumerate*}%
 {\begin{enumerate}%
  \setlength{\itemsep}{0pt}%
  \setlength{\parskip}{0pt}}%
 {\end{enumerate}}

\usepackage{cleveref}
\usepackage{listings}
\usepackage{titlesec}
\usepackage{multirow}
\usepackage{makecell}


% \usepackage[table,xcdraw]{xcolor}
\usepackage[most]{tcolorbox}


\author{
Yuji Zhang$^{1}$,  Sha Li$^{1}$, Cheng Qian$^{1}$, Jiateng Liu$^{1}$, Pengfei Yu$^{1}$, Chi Han$^{1}$, Yi R. Fung$^{1}$\\
\textbf{Kathleen McKeown$^{2}$, Chengxiang Zhai$^{1}$, Manling Li$^{3,4}$, Heng Ji$^{1}$}\\
$^{1}$University of Illinois Urbana-Champaign, $^{2}$Columbia University, \\$^{3}$Northwestern University, $^{4}$Stanford University\\
\texttt{\{yujiz, hengji\}@illinois.edu}\\
}

\NewDocumentCommand{\heng}
{ mO{} }{\textcolor{red}{\textsuperscript{\textit{Heng}}\textsf{\textbf{\small[#1]}}}}
\NewDocumentCommand{\cheng}
{ mO{} }{\textcolor{orange}{\textsuperscript{\textit{Cheng}}\small[#1]}}
\NewDocumentCommand{\yuji}
{ mO{} }{\textcolor{blue}{\textsuperscript{\textit{Yuji}}\small[#1]}}
\NewDocumentCommand{\chihan}
{ mO{} }{\textcolor{cyan}{\textsuperscript{\textit{Chi}}\small[#1]}}
\NewDocumentCommand{\zoey}
{ mO{} }{\textcolor{teal}{\textsuperscript{\textit{Zoey}}\small[#1]}}
\NewDocumentCommand{\kmnote}
{ mO{} }{\textcolor{purple}{\textsuperscript{\textit{Kathy}}\small[#1]}}
\NewDocumentCommand{\yi}
{ mO{} }{\textcolor{teal}{\textsuperscript{\textit{Yi}}\small[#1]}}

% ======================== Begin =======================


\title{The Law of Knowledge Overshadowing:\\Towards Understanding, Predicting, and Preventing LLM Hallucination} 


\begin{document}
\maketitle

\begin{abstract}
Hallucination is a persistent challenge in large language models (LLMs), where even with rigorous quality control, models often generate distorted facts.
This paradox, in which error generation continues despite high-quality training data, calls for a deeper understanding of the underlying LLM mechanisms. 
To address it, we propose a novel concept: \textbf{knowledge overshadowing}, where model's dominant knowledge can obscure less prominent knowledge during text generation, causing the model to fabricate inaccurate details.
%
Building on this idea, we introduce a novel framework to quantify factual hallucinations by modeling knowledge overshadowing. Central to our approach is the \textbf{log-linear law}, which predicts that the rate of factual hallucination increases linearly with the logarithmic scale of (1) \textit{Knowledge Popularity}, (2) \textit{Knowledge Length}, and (3) \textit{Model Size}. The law provides a means to preemptively quantify hallucinations, offering foresight into their occurrence even before model training or inference. 
%
Built on overshadowing effect, we propose a new decoding strategy \textbf{CoDa}, to mitigate hallucinations, which notably enhance model factuality on Overshadow
(27.9\%), MemoTrap (13.1\%) and NQ-Swap
(18.3\%).
Our findings not only deepen understandings of the underlying mechanisms behind hallucinations but also provide actionable insights for developing more predictable and controllable language models.
\end{abstract}


\input{sections/intro}
\input{sections/related}
\input{sections/formulation}
\input{sections/verification}
\input{sections/interpretation}
\input{sections/method}
% \input{sections/experiment}
\input{sections/conclusion}

\newpage
\section*{Limitations}
We conduct extensive experiments to investigate knowledge overshadowing phenomenon. However, due to inaccessibility, we can not analyze the variables in training corpora of SOTA LLMs like GPT-4o and DeekSeek. Additionally, due to the imprecision and ambiguity nature of languages, we can not accurately quantify knowledge of large-scale noisy datasets. We leave this blank for future work.

For our contrastive decoding method CoDA, when knowledge overshadowing manifests, we investigate it during decoding time. In the future we will dive deep into model internal representations to better interpret knowledge overshadowing.

Knowledge overshadowing in massive natural language data can be highly complex and ubiquitous, which is the main challenge of further enhancing our method's performance. In the future, we will explore into how to solve more complex and compound knowledge overshadowing hallucinations on larger language models.

\section*{Ethics Statement}
In our empirical study, MemoTrap and NQ-Swap are publicly available datasets to help us understand how models adhere to parametric or contextual knowledge. Our dataset Overshadowing is constructed based on the public COUNTERFACTUAL dataset. All of the three datasets are to interpret and eliminate hallucinations that will be harmful to users.
Experiments and methods on the three datasets are conducted for social benefits.
Additionally, the COUNTERFACTUAL dataset involves no privacy issues since it consists of artificial events.

\bibliography{anthology, custom, heng}

\newpage
\appendix

\input{sections/appendix}


\end{document}
