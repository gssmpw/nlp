\begin{figure}
    \centering
    \includegraphics[width=1.06\linewidth]{pictures/introcase.pdf}
    \vspace{-1.8em}
    \caption{Knowledge overshadowing leads to hallucinations, which exarcerbates with growing relative knowledge popularity ($\text{P}$), length ($\text{L}$), and model size ($\text{S})$.
    % Despite trained on all true corpus, when certain knowledge is overshadowed, language models misassemble multiple facts into factual hallucinations. \yuji{need to illustrate P L S, redrawing}
    % \yi{TODO: 1) use "pdf" instead of "png", the text inside figure should be copy and paste-able (directly "save as" instead of screenshot things); 2) a pertinent question is how likely do these example errors still occur in the SOTA LLM today (DeepSeek, QWen, GPT-o1, etc.) - do you want to check and put more difficult examples, or maybe emphasize that this still occurs in the smaller 3b LLMs etc, or some other way to phrase it in more nuanced manner (discuss across all co-authors)}
    }
\label{fig:intro_case}
\vspace{-1.3em}
\end{figure}