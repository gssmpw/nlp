\section{How to Eliminate Hallucination?}
\label{sec:mitigate}


% \heng{font in figure 5 too small}
In this section, we aim to mitigate factual hallucinations by proactively identifying overshadowed knowledge before it influences model predictions.

\subsection{CoDA: Contrastive Decoding to Amplify Overshadowed Knowledge}


\noindent \textbf{Identifying Overshadowed Knowledge.}
For a language model, given an input token sequence $X$, the model will output the continuation token sequence $Y$. 
Both $X$ and $Y$ consist of tokens from the vocabulary $\mathcal{V}$.
When certain tokens $x_{b}$ in \text{X} are overshadowed, the model will generate hallucinated output.
For example, in $X = $ ``Who is a famous \textit{African} researcher in machine learning area?'', if $x_b = $ ``\textit{African}'' is overshadowed by ``machine learning'', The model will output $Y$=``Yoshua Bengio'', ignoring the intended constraint.


To detect overshadowed tokens, we sequentially mask $x_b$ in \text{X} to form $X'$ until the overshadowed tokens are identified (see~\ref{ssec:coda_app} for various $x_b$ candidate selection methods). If  $x_b$ is overshadowed, $p(Y_b|X)\xrightarrow{\text{degrade to}}p(Y_a|X')$.
We quantify the generalization between distributions $p(Y|X)$ and $p(Y|X')$ by relative pointwise mutual information (\text{R-PMI})~\cite{li-etal-2023-contrastive}. 
To ensure we quantify output token candidates $y_i\in P(Y|X), P(Y|X')$ with sufficient semantics, we employ an adaptive plausibility constraint~\citet{li-etal-2023-contrastive}, retaining tokens that satisfy: $\mathcal{V}_{\text{top}}(X) = \{y_i | p(y_i|X) \geq \alpha \cdot \Upsilon \}$, where $\alpha = 0.01$ is a hyperparameter, and $\Upsilon$ is a global variable as the maximum probability among all $y_i$ candidates.
Then the \text{R-PMI} is quantified over {\small$\forall y_i \in \mathcal{V}_{\text{top}}(X) \cap \mathcal{V}_{\text{top}}(X')$}:

% \vspace{-0.3em}
\begin{small}
\begin{equation}
\begin{aligned}
\mathrm{R\text{-}PMI}(y_i; X, X') &= \log \frac{p(y_i \mid X)}{p(y_i \mid X')}
\end{aligned}
\label{eq.RPMI}
\end{equation}
\end{small}

\noindent In essence, a negative R-PMI value indicates that token \(y_i\) is more associated with $\text{X}'$ without overshadowed information.
Thus we quantify to what extent $P(\text{Y}|\text{X})$ generalize to $P(\text{Y}|\text{X})$ by $\text{R-PMI}_{\text{sum}} = \sum_{i} \min(\text{R-PMI}(y_i; X, X'), 0)$.
Moreover, it is noteworthy that despite some tokens being overshadowed by $X'$, there are still tokens that escape from this overshadowing effect, defined as $\mathcal{V}_{\text{esc}}$:

% \vspace{-0.5em}
\begin{small}
\begin{equation}
\hspace{-2mm}
    \mathcal{V}_{\text{esc}} = \{ y_i | y_i \in \mathcal{V}_{\text{top}}(X) \ \text{and} \ y_i \notin \mathcal{V}_{\text{top}}(X') \}
\end{equation}
\end{small}

\noindent These escaping tokens demonstrate the potential for hallucination elimination.
% , as they encode semantics that are not overshadowed by dominant knowledge.
Then we propose an Escaping Rewarding Mechanism (ERM), which adds a positive reward to the sum of negative $\text{R-PMI}$ to represent whether the escaping effect wins over the overshadowing effect. Denoting all $y_i$ with a negative R-PMI as $y_i\in\mathcal{S}$,  ERM is calculated as:

\vspace{-1em}
\begin{small}
\begin{equation}
% \hspace{-4mm}
    \text{ERM} = \sum_{y_i \in \mathcal{V}_{\text{esc}}} \left( \log~p(y_i|X) - \min_{y_j \in \mathcal{S}} \log~p(y_j|X') \right)
\label{eq:erm}
\end{equation}
\end{small}

\noindent where the deduction is to balance ERM with R-PMI with a similar denominator of $p(y_j|X')$ in Eq.~\ref{eq.RPMI}, which represents the minimum bias from $\text{X}'$. Then the overshadowed knowledge indicator is: {$\text{Indicator} = \text{R-PMI}_{\text{sum}} +  \text{ERM}$}. A negative indicator value indicates proper generalization without overshadowing other knowledge, and a positive alamer value indicates over-generalization with overshadowed tokens $x_b$. 
Then we can predict potential hallucinations after locating the overshadowed tokens, and the hallucination prediction accuracy is in shown Table~\ref{tab:main_results2}.


% \vspace{1mm}
\noindent \textbf{Elevating Overshadowed Knowledge.}
Once the tokens $\text{x}_b$ encoding overshadowed knowledge are identified, we adopt contrastive decoding on the identified overshadowed tokens to downweight the influences of $\text{X}'$ and highlight $X$.
Specifically, to reduce the bias from of $X'$, for each $y_i \in \mathcal{V}_{\text{top}}(X) \cap \mathcal{V}_{\text{top}}(X')$, we subtract the prior bias of $X'$, which is $P(y_i|X')$ as shown below:

% \vspace{-0.3em}
\begin{small}
\begin{equation}
  \log {p}(y_i) = \log p(y_i|X) - \log p(y_i|X')
\end{equation}
\end{small}



\noindent Similarly for each $y_i \in \mathcal{V}_{\text{esc}}$, we conduct:

\vspace{-0.5em}
\begin{small}
  \begin{equation}
    \log {p}(y_i) = (\log~p(y_i|X)-\min_{y_j\in\mathcal{S}}\log~p(y_j|X'))
\end{equation}  
\end{small}

\noindent Here, $\min_{y_j \in \mathcal{S}} \log p(y_j|X')$ represents the minimum prior bias from popular knowledge. The deduction aims to balance the bias adjustment between $y_i \in \mathcal{V}_{\text{esc}}$ and $y_i \notin \mathcal{V}_{\text{esc}}$, ensuring proportional adjustments for both.
Then we predict the optimal output $y_i^*$ by:

% \vspace{-0.5em}
\begin{small}
    \begin{equation}
    y_i^* = \mathop{\mathrm{argmax}}_{y_i \in\mathcal{V}_{\text{top}}(X)} \log p (y_i|X)
\end{equation}
\end{small}

\input{figures/eliminator}


\noindent Till now, we downweight the overshadowing effect from popular knowledge encoded by $X'$, then escaping tokens encoding meaningful overshadowed knowledge are amplified to decrease hallucinations. 


\subsection{Experimental Setup}
\noindent \textbf{Datasets.}
We experiment on two public datasets of hallucinations caused by conflicting knowledge MemoTrap~\cite{liu2023memotrap}
,  NQ-SWAP~\cite{longpre-etal-2021-entity}, and our Overshadow dataset.


\vspace{1mm}
\noindent \textbf{Baselines.} We adopt Greedy decoding, Chain-of-Thought (Cot)~\cite{wei2022chain}, Self-Reflection (SR)~\cite{madaan2024self}, \textit{USC}~\cite{chen2023universal}, and \textit{Dola}~\citet{chuang2023dola} as the baselines. Details for datasets and baselines are in \ref{ssec:coda_app}.


\vspace{1mm}
\noindent \textbf{Implementationa and Metric.}
We use the Exact Match (EM) metric following previous practices~\cite{longpre-etal-2021-entity}. Implementation details for all methods are elaborated in~\ref{ssec:coda_app}.



\subsection{Main Results and Analysis}
% \paragraph{CoDA significantly outperforms baselines.}
Our method improves greedy decoding by 27.9\%, 13.1\%, and 18.3\% on Overshadow, MemoTrap, and NQ-Swap.
Reasoning-enhanced baselines struggle with hallucinations caused by knowledge overshadowing. Self-consistency-based methods show instability or even degradation, which may be attributed to reinforcing biases from popular knowledge.
Figure~\ref{fig:quantitative_analysis} shows our quantitative analysis of the impact of two factors $\text{P}$ and $\text{L}$ on CoDA, as the more knowledge is over-generalized, the harder it becomes to extract valuable information from the suppressed knowledge representations. 


