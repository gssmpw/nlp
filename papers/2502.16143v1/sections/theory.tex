\section{Knowledge Overshadowing as a Case of Over-Generalization }
\label{sec:theory}
Why does knowledge overshadowing happen at all? We first measure the generalization ability of the model and show that the hallucination rate is closely related to model generalization, showing that knowledge overshadowing is a case of over-generalization. Then we analyze the generalization bound of the model and show its connections with the imbalance ratio and condition length. 


\paragraph{Generalization positively correlates with hallucination.}
%The generalization error bound theoretically explains that generalization of popular patterns lead to hallucinations. Furthermore, we empirically explore the consistency between generalization and hallucination.
We quantify model generalization with Gradient
signal-to-noise ratio (GSNR)~\cite{liu2020understanding}. 
We utilize GSNR to measure the similarity of gradients among different training samples. A large GSNR indicates higher agreement of the optimizing direction on gradients in training time, then parameters are prone to be ``associated with a pattern'', which leads to the better generalization. From Figure \ref{fig:generalization-factors}, we can observe that the hallucination rate correlates well with GSNR over different imbalance ratios and condition lengths. 

%We quantitatively analyze how imbalance ratio, condition length will affect generalization and hallucination.
Weight decay is the well-known regularization for neural networks to enhance model generalization~\cite{wei2019data}, thus we experiment with varying weight decay to boost generalization and observe the resulting hallucination rate. 
The results show that the hallucination rate increases consistently with generalization, and larger weight decay, larger imbalance ratio, and larger condition length all lead to higher generalization and hallucination rate.

\input{figures/decay}

\paragraph{Generalization error bound of auto-regressive language modeling.}
% \pengfei{I think there are a few places for improvements on clarity}
% \pengfei{1. $\boldsymbol{x_{k+1}}$ should have a superscript $i$ where $1\le i \le M$, meaning dominating sequences have varying infixes. }
In the following analysis, we simplify the problem by assuming that the infix condition $B$ is only one token long, so the prefix condition $A$ can be written as  ${x}_1, \dots, {x}_k$, and the infix condition $B$ as ${x}_{k+1}$ so that the relative length would be $k$.
% refers to the infix condition $B$. Then $y$ and $\tilde{y}$ are continuations $C$ conditioning on $A,B$.
Following our experiment setting in \autoref{sec:syn_exp}, we assume that the dataset $\mathbf{D} \sim \mathcal{D}$ is comprised of multiple imbalanced groups. 
Each imbalanced group ${Q}$ includes a subset $Q_M$ of size $M$ with samples $s = ({x}_1, \dots, {x}_k, {x}_{k+1}, y)$, and subset $Q_N$ of size $N$  with samples $\tilde{s} = ({x}_1, \dots, {x}_k, \tilde{{x}}_{k+1}, \tilde{y})$. 
All tokens 
%${x}_1, \dots,{x}_k,{x}_{k+1}, \tilde{{x}}_{k+1}, y, \tilde{y}$ 
are from the vocabulary $\mathcal{V}=\{1, 2, ..., V\}$.
% ~\cite{thrampoulidis2024implicit, vaswani2017attention}. 


% To simplify the question, we formulate $s$ with length of prefix $X$ as $k$, and the length of infix $B$ as one, then the $k/1=$length($A$)/length($B$) represents the relative length of A:B.
% Normally $y$ indicates multi-step next token prediction (NTP). We formulate the sub-question where $y$ includes one token.
%With the imbalance ratio $r=M/N>1$, the more popular prefix condition ${x}_1,\dots, {x}_k$ tends to overshadow the less popular infix condition $\tilde{{x}}_{k+1}$, thus generating the wrong continuation $y$, defined as amalgamated hallucination, whose final output is ${x}_1, \dots, {x}_k, \tilde{{x}}_{k+1}, y$, deviating from the factual dataset. We assume this hallucination is a case of over-generalization of the frequently seen pattern ${x}_1, \dots, {x}_k, {x}_{k+1}, y$ by language model. Thus we explore the generalizability of the popular samples $s$. 
The next token prediction (NTP) loss for $s$ based on auto-regressive modeling is:
\begin{small}
\begin{equation}
\mathcal{L}_\mathrm{NTP} = \hat{\mathbb{E}}_{{x} \sim Q_M} \left[ \sum_{{x} \in [K+1]} -\log \left( p(y | {x}_1, \dots, {x}_k, {x}_{k+1}) \right) \right]
\end{equation}
\end{small}
% Here $\hat{\mathbb{E}}$ is the expectation over training set $Q$ of sequences $s$ sampled from true distribution.
% Then, the auto-regressive model $\mathcal{M}_{\theta}$ parameterized by $\theta$ is trained on this cross-entropy loss for next token prediction.
% $\mathbf{W_{\theta}}({x}_1, \dots, {x}_k, {x}_{k+1}))$ are the output logits, on which the softmax function $\mathcal{S}$ is applied to output the probability of next token $y$, given the input sequence of $({x}_1, \dots,{x}_k, {x}_{k+1})$:
% \begin{small}
% \begin{equation}
% \begin{aligned}
%     p(y | {x}_1, \dots,{x}_k, {x}_{k+1})) & =\mathcal{M}_{\theta}(y | {x}_1, \dots, {x}_k, {x}_{k+1}))= \mathcal{S}_{y} \left( \mathbf{W_{\theta}}({x}_1, \dots, {x}_k, {x}_{k+1})) \right) \\
%     & = \frac{\exp \left( e_y^\top \mathbf{W_\theta}(x_1, \dots, x_k, x_{k+1}) \right)}{ \sum_{{x}' \in \mathcal{V} } \exp \left( e_{{x}'}^\top \mathbf{W_\theta}(x_1, \dots, x_k, x_{k+1}) \right)}
% \end{aligned}
% \end{equation}
% \end{small}
% Here $e_y$ corresponds to the one-hot vector with the $y$-th dimension being 1.s
% Then the parameters $\mathbf{W_{\theta}}$ will be trained by minimizing the cross-entropy loss $\mathcal{L}_\mathrm{NTP}$ by gradient descending. While our focus here is to explore how training data impacts generalization, thus we assume the following analysis holds the same model $\mathcal{M}_{\theta}$.
% \paragraph{Auto-regressive language modeling loss for next token prediction. }
% Different from traditional one-hot classification, the training objective of text generation on an auto-regressive model is the multi-step cross-entropy minimization over the predicted next token, where each predicted token corresponds to an empirical probability distribution over a finite vocabulary determined by the tokenizer~\cite{thrampoulidis2024implicit, vaswani2017attention}. 
% To simplify the token-by-token prediction, we focus on the smallest unit of the whole sentence generation problem, where we formulate the question as a single-token prediction task given an input context.
% Here, in a training set $\mathcal{D}^{n}$, there are $n$ samples of sequences $s_i = (t_1, t_2, .., t_i)$ of varying lengths $i$, consisting of tokens $t$. All tokens are from a finite vocabulary $\mathcal{V}=\{1,2, ..., V\}$.  
% For each training sample, given an input context sequence of $s_{i-1} = (t_1, t_2, .., t_{i-1})$, the auto-regressive model $\mathcal{M}_{\theta}$ parameterized by $\theta$ is trained to predict the next token $t_i$ of the input context $s_{i-1}$ of length $I$ using the cross-entropy loss:
% \begin{small}
% \begin{equation}
% \mathcal{L}_\mathrm{NTP} = \hat{\mathbb{E}}_{t \sim \mathcal{D}^n} \left[ \sum_{t \in [I]} -\log \left( p(y | x_1 \dots x_k, x_{k+1}) \right) \right]
% \end{equation}
% \end{small}
% Here $\hat{\mathbb{E}}$ is the expectation over training set $\mathcal{D}^n$ of sequences $s_i$ sampled from true distribution. Then, 
% \begin{small}
% \begin{equation}
% \begin{aligned}
%     p(y | x_1, \dots x_k, x_{k+1}) & =\mathcal{M}_{\theta}(t_i \mid t_1, \ldots, t_{i-1})= S_{t_i} \left( \mathcal{W}_{\theta}(t_1, \ldots, t_{i-1}) \right) \\
%     & = \frac{1}{1 + \sum_{t' \in \mathcal{V}, t' \neq t_i} \exp \left( (e_{t'} - e_{t_i})^\top \mathcal{W}_{\theta'}(t_1, \ldots, t_{i-1}) \right)}.
% \end{aligned}
% \end{equation}
% \end{small}
% Here $\mathcal{W}_{\theta}(t_1, \ldots, t_{i-1})$ is the output logits, on which the softmax function $S_{t_i}$ is applied to output the probability of next token $t_i$, given the input context $s_{i-1}=(t_1, .., t_{i-1})$.
% Denoting $s_{i-1}$ as the input $x$, $t_i$ as the output $y$, the training set is comprised of feature-label-like pairs $(x, y)$, Then the cross-entropy loss for next token prediction can be rewritten as:
% \begin{equation}\small
% \mathcal{L}_\mathrm{NTP} = \hat{\mathbb{E}}_{x \sim \mathcal{D}^n} \left[  -\log \left( p(y|x) \right) \right] 
% \end{equation}
% The loss resembles the traditional one-hot classification where $x$ is the input feature, and $y$ is the ground-truth label. Then we can conduct the generalization analysis under classification task setting.

Our optimizing objective 
%for next token prediction by auto-regressive language modeling 
is to learn a function $f: \mathcal{X}\rightarrow\mathbb{R}^{V}$, ($\mathcal{X}$ for input space), to minimize the risk $\mathcal{R}_{y}$ defined on the true distribution using NTP as the surrogate loss:
% \begin{small}
% \begin{equation}
% \mathcal{R}_\mathrm{NTP}(f) := \frac{1}{V} \sum_{y=1}^{V} R_{y}(f) = \frac{1}{V} \sum_{y=1}^{V} \mathbb{E}_{{x}_1, \dots, {x}_k, {x}_{k+1} \sim Q_M} [\mathcal{J}(f({x}_1, \dots, {x}_k, {x}_{k+1}), y)]
% \end{equation}
% \end{small}
% Here $\mathcal{R}_{y}$ is the risk of prediction on $y$, referring to mistakes made by models when making prediction, where $\mathcal{J}: 
% \mathbb{R}^{V}\times\mathcal{V}$ is to evaluate the model performance on the true distribution $\mathcal{D}$, from which the training set $Q_M$ is sampled. Since $\mathcal{J}$ is intractable, we adopt the surrogate loss function $\mathcal{L}: \mathbb{R}^{V}\times\mathcal{V}\rightarrow\mathbb{R}_{+}$, here $\mathcal{L}:=\mathcal{L}_\mathrm{NTP}$. Then the risk calculated on surrogate loss is: 
\begin{small}
\begin{equation}
\mathcal{R}^{\mathcal{L}}_\mathrm{NTP}(f) := \frac{1}{V} \sum_{y=1}^{V} \mathcal{R}^{\mathcal{L}}_y(f) = \frac{1}{V} \sum_{y=1}^{V} \mathbb{E}_{{x}_1, \dots, {x}_k, {x}_{k+1} \sim Q_M} [\mathcal{L}(f({x}_1, \dots, {x}_k, {x}_{k+1}), y)]
\end{equation}
\end{small}
Based on the above NTP optimizing objective formulation, the empirical risk can be formulated as:
\begin{equation}
\widehat{\mathcal{R}}^{\mathcal{L}}_\mathrm{NTP}(f) := \frac{1}{M} \sum_{(\boldsymbol{x},y) \in Q_{M}} \mathcal{L}(f(x_1, ..., x_k, x_{k+1}), y)
\end{equation}
\textbf{Proposition 1} (Generalization bound on Rademacher complexity~\cite{mohri2018foundations}).
Let $\mathcal{G}$ be a family of functions. Then, for any $\delta > 0 $, with probability at least $1 - \delta$ over the draw of an i.i.d. (independent and identically distributed) sample $Q_M$ of size $M$, $ \exists \text{ a constant } C > 0, f(t) \le C \cdot g(t)$~\cite{wang2024unified}, the generalization bound holds for all $g \in \mathcal{G}$:
% Given the loss function $\mathcal{L}: \mathbb{R}^{V}\times\mathcal{V}$, and a function set $\mathcal{F}$, where $f \in \mathcal{F}$, then for any $\delta \in (0, 1)$, with probability larger than $1-\delta$ over the training set $\mathcal{D}_n$, the generalization error bound holds for all $f \in \mathcal{G}$:
\begin{small}
\begin{equation}
\label{eq.initial_bound}
\mathcal{R}^{\mathcal{L}}_y(f) \precsim \widehat{\mathcal{R}}^{\mathcal{L}}_y(f) + 2\widehat{\Re}_{Q_M}(\mathcal{G}) + \sqrt{\frac{\log 1/\delta}{2M}} 
\end{equation}
\end{small}
Here ${\Re}_y(\mathcal{G})$ denotes the empirical Rademacher complexity of the function set $\mathcal{G}$, as a measure of the richness of $\mathcal{G}$. 
% $g \in \mathcal{G}$ is to bound expected values and empirical averages on dataset $Q_M$. 
% The generalization bound is still loose for the NTP since it doesn't reveal how more variants of multiple conditions relate to generalization ability. Thus we bound $\mathcal{G}$ tighter. 
Then we employ \textit{Lipschitz Continuity} to further bound the complexity ${\Re}(\mathcal{G})$~\cite{cao2019learning}.

\textbf{Definition 1}(Lipschitz continuity).
With$\|\cdot\|$ denotes the 2-norm, then the function $\mathcal{L}$ is \textit{Lipschitz continuous} with the constant $\mu$ if for any $f, f' \in \mathcal{F}$, $x \in Q$:
\begin{small}
\begin{equation}
|\mathcal{L}(f, y) - \mathcal{L}(f', y)| \leq \mu \cdot \|f(x) - f'(x)\|
\end{equation}
\end{small}
If the NTP loss function $\mathcal{L}_\mathrm{NTP}(f)$ is \textit{Lipschitz continuous} with constant $\mu$, then $\Re_{Q_M}(\mathcal{G})$ could be:
\begin{equation}
\label{eq.bound_rade}
\hat{{\Re}}_{\mathbf{Q_M}}(\mathcal{G}) \le \mu \cdot \hat{{\Re}}_{\mathcal{Q_M}}(\mathcal{F}).
    \end{equation}
%\paragraph{Length-dependency for multiple conditions. }
We derive that the next-token-prediction loss $\mathcal{L}_\mathrm{NTP}$ is \textit{Lipschitz continous} with the constant $\mu=\sqrt{1 + \left( \sum_{y' \neq y} h^{-1}(k) \right)^2} \left[ 1 - \mathrm{softmax}\left( \boldsymbol{s}_{y} \right) \right]$ (See details in \S~\ref{appendix:length_dependency}), by substituting $\mu$ to Eq.(\ref{eq.initial_bound}) and Eq.(\ref{eq.bound_rade}), we derive the more fine-grained generalization bound for NTP with multiple conditions:
\begin{small}
\begin{equation}
\label{eq.final_bound}
\mathcal{R}^{\mathcal{L}}_y(f) \precsim \widehat{\mathcal{R}}^{\mathcal{L}}_y(f) + 2\widehat{\Re}_{Q_M}(\mathcal{F})\sqrt{1 + \left( \sum_{y' \neq y} h^{-1}(k) \right)^2} \left[ 1 - \mathrm{softmax}\left( \boldsymbol{s}_{y} \right) \right] + \sqrt{\frac{\log 1/\delta}{2M}} 
\end{equation}
\end{small}
Here the generalization bound contains two coefficients $M$ and $h(k)$. $M$\footnote{Here we fix the number of suppressed samples to N for simplified generalization analysis. Then M indicates to what extent the dominant condition overshadows suppressed condition.} refers to number of dominant samples of $s_{k+2} = ({x}_1, \dots, {x}_k, {x}_{k+1}, y), s_{k+2} \in Q_{M}$, $h(k)$. $h(k)$ is the value positively correlated with the length of the dominant prefix. Then, the longer length of dominant prefix $({x}_1, \dots, {x}_k)$ and higher dominant ratio lead to lower generalization bound, in other words, better generalization.
The bound provides theoretical insights that hallucination is highly relevant to generalization, 
echoing the experimental results (Table~\ref{tab:nl_results}, Table~\ref{tab:syn_results}, Figure~\ref{fig:generalization-factors}) that over-generalization of dominant patterns overshadow other patterns, leading to amalgamated hallucinations.

% over-generalization of dominant patterns observed in previous experiments (Table~\ref{tab:nl_results}, Table~\ref{tab:syn_results}, Figure~\ref{fig:generalization-factors}). 

