\section{What is Knowledge Overshadowing?}
\label{sec:formulation}
\input{figures/rkp_rkl_generalization}


Factual hallucination, where authentic facts are misassembled into false statements, remains an underexplored challenge. We approach this issue through the lens of knowledge overshadowing, where more prevalent knowledge suppresses less frequent knowledge, resulting in hallucinations.

\subsection{Knowledge Overshadowing Formulation}
\label{ssec:shadow_formulation}
To systematically characterize knowledge overshadowing, we define knowledge pairs in a training corpus.
% , where dominant knowledge can overshadow less frequent knowledge in the query.
Specifically, let $\mathbb{K}_A = \{k_{a_1}, ..., k_{a_m}\}$ and $\mathbb{K}_B = \{k_{b_1}, ..., k_{b_n}\}$ represent a pair of knowledge sets. $\mathbb{K}_A$ is comprised of $m$ samples of statements $k_{a_i}$, and $\mathbb{K}_B$ is comprised of $n$ samples of statements $k_{b_j}$.
Each statement in $\mathbb{K}_A$ and statement in $\mathbb{K}_B$ are related by a shared set of tokens $X_{share}$.

In the knowledge set $\mathbb{K}_A$, each statement $k_{a_i}$ is comprised of a shared token sequence $X_{\mathrm{share}}$, a distinct token sequence $x_{a_i}$, and the output $Y_a$. Each statement $k_{a_i}$ is expressed as:

\vspace{-0.5em}
\begin{small}
\begin{equation}
    k_{a_i} = \textcolor{red!60}{Y_a} | [X_{\mathrm{share}} \odot \textcolor{red!60}{x_{a_i}}], \quad i \in \{1, ..., m\}
\end{equation}
\end{small}

\vspace{-0.5em}\noindent where $\odot$ denotes the insertion of the distinctive sequence $x_{a_i}$ into $X_{\mathrm{share}}$
(the integration position can vary).
Similarly, for the less popular knowledge set $\mathbb{K}_B$, with $x_{b_j}$ denoted as the distinct token sequence, each statement $k_{b_j}$ is formulated as:

\vspace{-0.5em}
\begin{small}
\begin{equation}
    k_{b_j} = \textcolor{cyan!60}{Y_b} | [X_{\mathrm{share}} \odot \textcolor{cyan!60}{x_{b_j}}], \quad j \in \{1, ..., n\}
\end{equation}
\end{small}



\vspace{-0.5em}\noindent Knowledge overshadowing occurs when the distinct token sequence $x_{b_j}$ or $x_{a_i}$ is suppressed during inference. Taking $x_{b_j}$ overshadowed as an example, when prompted with $X_\mathrm{share}\odot x_{b_j}$, the model outputs $Y_a$, forming the $\textcolor{red!60}{Y_a} | [X_{\mathrm{share}} \odot \textcolor{cyan!60}{x_{b_j}}]$ that wrongly amalgamates factual statements $k_{a_i}$ and $k_{b_j}$ into factual hallucination, defying the ground-truth $\textcolor{cyan!60}{Y_b} | [X_{\mathrm{share}} \odot \textcolor{cyan!60}{x_{b_j}}]$, as illustrated in Figure~\ref{fig:intro_case}.



\subsection{Metric of Factual Hallucination.}
To measure hallucination caused by knowledge overshadowing, we introduce the relative hallucination rate $\text{R}$.
When $\mathbb{K}_A$ is the more popular knowledge set, we first quantify the recall rate of the model correctly memorizing the samples from 
$\mathbb{K}_A$ as $\text{RR}=p(\textcolor{red!60}{Y_a} | [X_{\mathrm{share}} \odot \textcolor{red!60}{x_{a_i}}])$. 
Then we quantify the hallucination rate of the model producing output with $x_{b_j}$ overshadowed as $\text{HR}=p(\textcolor{red!60}{Y_a} | [X_{\mathrm{share}} \odot \textcolor{cyan!60}{x_{b_j}}])$.                  The relative hallucination rate $\text{R}=\frac{\text{HR}} {\text{RR}}$ represents to what extent is less popular knowledge encoded by $x_{b_j}$ suppressed by the more popular knowledge encoded by $x_{a_i}$.


\subsection{Formulation of Influential Variables}
\label{ssec:influential_variables}
% \yuji{m>n, and L? define the two equally important vairables here }
Since the underlying factors influencing factual hallucinations have not been explored, we examine these variables from both global and local perspectives, focusing on knowledge proportions that contribute to the overshadowing effect.
When $\mathbb{K}_A$ is more popular than $\mathbb{K}_B$, $m>n$.
From a global perspective, we define the relative knowledge popularity as $\text{P} = \frac{m}{n}$, 
denoting the relative proportion of the knowledge in the whole training corpus. 
From the local perspective, we quantify the weight of knowledge in an individual sentence using the relative knowledge length  $\text{L} = \frac{\text{len}(X_{\text{share}})+\text{len}(x_{b_i})}{\text{len}(x_{b_i})}$, where length is measured by the number of tokens. 
For example in Figure~\ref{fig:intro_case}, in input ``A famous singer in North Korea is'', length of $x_{b_j}$=``single'' is 1, length of $X_{share}$=``A famous \_ in North Korea is'' is 6, so $\text{L}$=(6+1)/1=7.
Since previous work shows scaling model size enhances its performance~\cite{kaplan2020scaling}, we study whether scaling up the model size $\text{S}$ can mitigate factual hallucinations.