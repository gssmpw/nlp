\section{Introduction}
Large language models (LLMs) have revolutionized artificial intelligence, but their success is accompanied by a critical issue known as hallucination~\cite{ye2023cognitive}. Hallucination refers to models generating unfaithful or nonfactual statements. In many applications, this issue undermines performance and reliability, posing substantial challenges to their practical deployment~\cite{li2024dawn}.

\input{figures/intro_case}

Some studies attribute hallucination to low-quality pretraining corpora~\cite{gehman-etal-2020-realtoxicityprompts}. However, we find it persists even when the pretraining corpus is strictly controlled to contain only factual statements. Specifically, when extracting knowledge using queries, we observe a tendency for certain knowledge to overshadow other relevant information. This causes the model to reason without adequately considering overshadowed knowledge, leading to hallucinations.


As shown in \Cref{fig:intro_case}, when queried for ``\textit{famous singer in North Korea}'', the model incorrectly nominate ``Kim Jong Un'', who is in fact a politician, as a result of ``North Korea'' overshadowing ``singer''. This observation highlights how knowledge of varying forms interacts, distorting the reasoning process and causing the model to misassemble facts, thereby generating hallucinations. To investigate this phenomenon, we raise the following questions:
\begin{itemize}[topsep=2pt, partopsep=-5pt, leftmargin=8pt, itemsep=-4.5pt]
\item \textbf{What} factors contribute to the phenomenon of knowledge overshadowing (\S\ref{sec:formulation})?
\item Can we preemptively quantify \textbf{when} hallucinations occur (\S\ref{sec:when})?
\item From a theoretical perspective, \textbf{why} knowledge overshadowing happens (\S\ref{sec:interpret})?
\item Leveraging the insights we derived, \textbf{how} to mitigate factual hallucinations (\S\ref{sec:mitigate})?
\end{itemize}


Through extensive experiments, we find that knowledge overshadowing broadly induces factual hallucinations in both pretrained and fine-tuned models, across diverse model families and sizes. Despite its importance, the factors influencing this phenomenon remain unexplored. To bridge this gap, we analyze knowledge representation from both global and local perspectives by examining its \textit{popularity} across the dataset distribution and its proportional representation \textit{length} within individual sentences. Additionally, since increasing \textit{model size} has been shown to improve language model performance~\cite{kaplan2020scaling}, we further explore its impact on factual hallucinations.


To examine the impact of these factors, we pretrain LLMs from scratch on a synthetic dataset with strictly controlled quality. Our empirical findings reveal a \textbf{log-linear scaling law} for factual hallucinations, showing that hallucination rates increase linearly with the logarithmic scale of relative knowledge popularity, knowledge length, and model size. Finetuning on diverse tasks further confirms this law applies to finetuned LLMs, enabling the preemptive quantification of hallucinations before model training or inference. This not only bridges the gap in understanding hallucinations emerging from factual training data but also introduces a principled approach for evaluating training data and predicting model behavior in advance.


The empirical discovery of this law leads us to investigate its underlying cause. We hypothesize that knowledge overshadowing stems from the over-generalization of popular knowledge, suppressing less popular counterparts. Theoretically, we derive a generalization bound for auto-regressive language modeling, linking the modelâ€™s behavior to key properties of its training data. Our analysis shows that generalization improves with increasing relative knowledge popularity and length, mirroring the trend observed in hallucination rates.


Building on all the insights derived, we propose \textbf{Co}ntrastive \textbf{D}ecoding to \textbf{A}mplify Overshadowed Knowledge (\textbf{CoDA}), a method designed to amplify the influence of overshadowed knowledge while mitigating biases from dominant knowledge. First, we identify overshadowed knowledge by computing the mutual information between the next-token probability distributions of the original and modified prompts, where specific tokens are masked. This approach reveals knowledge encoded in the masked tokens, which is often overlooked and prone to hallucination. We then employ contrastive decoding to reduce the bias introduced by dominant knowledge. Without requiring additional training, CoDA significantly improves factuality, achieving gains of 13.1\%, 18.3\%, and 27.9\% on the MemoTrap, NQ-Swap, and Overshadowing datasets, respectively. Our contributions are three-fold:
\begin{itemize}[topsep=2pt, partopsep=-5pt, leftmargin=8pt, itemsep=-4.5pt]
\item We are the first to identify knowledge overshadowing as a key driver of hallucinations and demonstrate its prevalence across LLMs.
\item We establish the log-linear law of knowledge overshadowing, enabling quantification of hallucinations prior to model training or inference.
\item We propose CoDa to mitigate hallucinations by detecting overshadowed knowledge, achieving significant improvements in factuality on Overshadow, MemoTrap, and NQ-Swap benchmarks.
\end{itemize}
