\section{Related Work}

%%%%%%%% Why %%%%%%%%
\subsection{Causes of Hallucination}
Our work is in line with exploring the causes of factuality hallucination, believed to be a primary source of errors in LLMs~\cite{li2024dawn, augenstein2023factuality}. Previous studies attribute factuality hallucinations to deficiencies in training data, such as outdated or domain-lacking data~\cite{zhang-etal-2023-vibe, Livska2022StreamingQAAB, luu-etal-2022-time, zhang-etal-2021-howyoutagtweets, zhang2022time}, biased distribution~\cite{ladhak-etal-2023-pre, qin2024does}, and inherent misinformation~\cite{dziri-etal-2022-origin, liu2024prejudicevolatilitystatisticalframework}. Other research points to generation issues 
% of source-reference divergence, \heng{cannot parse generation issues of source-reference divergence} 
including distorted attention~\cite{aralikatte-etal-2021-focus}, unstable sampling~\cite{manakul-etal-2023-selfcheckgpt}, over-confidence~\cite{varshney2023stitch, ren2023investigating, li2024survey}, and thoughtless human preference alignment~\cite{wei2023simple, zhang2025amulet, bai2024efficient}.
Related efforts also suggest that LLMs can be trapped in common patterns~\cite{lin-etal-2022-truthfulqa, kang2023impact, kandpal2023large}.
We focus on a significant yet underexplored phenomenon: LLMs can hallucinate even when trained exclusively on all truthful data. 
% \yuji{didn't explore interaction}
We introduce knowledge overshadowing: where more dominant knowledge representation competes against and suppresses less prevalent knowledge, resulting in factual hallucinations.
% (\textbf{cite} some work reports the negative influence of over-generalization)
% \vspace{-1em}
%%%%%%%% When %%%%%%%%
\subsection{Detection of Hallucination}
Factuality hallucination detection in LMs typically involves external fact-checking methods, such as FACTSCORE \citep{min2023factscore} and FacTool \citep{chern2023factool}, or internal uncertainty analysis. The latter includes Chain-of-Verification \citep{dhuliawala2023chain}, logit-based assessments \citep{kadavath2022language,zhang2024self}, and leveraging LM internal states \citep{varshney2023stitch,luo2023zero}. When internal states are unavailable, self-consistency probing \citep{manakul2023selfcheckgpt,agrawal2024language} or multi-LM corroboration \citep{cohen2023lm} can provide alternative signals.
Unlike prior work focused on post-generation hallucination detection, our study pioneers hallucination \textbf{prediction} by modeling it quantitatively through a log-linear law, incorporating fine-grained factors like knowledge popularity, length, and model size. This shifts the paradigm from reactive detection to proactive prevention, offering a novel quantitative framework for anticipating hallucinations.


%%%%%%%% How %%%%%%%%
\subsection{Elimination of Hallucination}
Our work is related to previous studies on mitigating hallucinations. \citet{shen2021identifying} address hallucination by filtering out low-quality training data. Several approaches enhance model factuality through 
% retrieval-augmented generation, leveraging 
external knowledge~\cite{wu2023ragtruth, xie2023adaptive, lyu2023improving, asai2023selfrag}, and knowledge-aware tuning~\cite{li2022large}.
Some studies tackle hallucination by enforcing LLMs to adhere closely to the input~\cite{tian2019sticking, aralikatte-etal-2021-focus}, and modifying the internal states~\cite{chen2023purr, azaria2023internal, gottesman2024estimating}. Our work aligns with advanced decoding strategies~\cite{wan-etal-2023-faithfulness, cheng2024integrative, shi2023trusting} to enhance factuality. 
On the other hand, early detection of hallucination is crucial~\cite{zhang2023language}. Our method not only foresees potential hallucinations before generation but also eliminates them through a training- and data-free approach.
