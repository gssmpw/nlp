\newpage
\section{Appendix}

\subsection{LLM Pretraining and Finetuning Details}
\label{ssec:implementation}



In fine-tuning experiments, for Llama-2-7b~\cite{touvron2023llama}, Mistral-7b~\cite{jiang2023mistral}, GPT-J-6b~\cite{gpt-j}, Phi-2-2.8b~\cite{gunasekar2023textbooks}, and Pythia-160m~\cite{mallen2023eliciting}, Pythia-410m, Pythia-1b, Pythia-1.4b, and Pythia-2.8b, we set the learning rate as lr=1e-5. The weight decay is set as 1e-2. We train each model for 40 epochs. The batch size for Pythia-series model and Phi model is 16. The batch size for GPT-J-6b, Llama-2-7b, and Mistral-7b is 1. The training is based on auto-regressive loss for input sequences. For each experiment, we ran the trials five times. We report the average score of the results.

Our experiments are conducted on A-100 machines (with memory of 80G). For four parallel GPUs, a single epoch on Phi-2-2.8b for the synthetic dataset will cost 1 hours, so totally it costs 40 hours to run on four parallel A-100 GPUs to train Phi-2-2.8b. For llama-2-7b, it costs more than 100 hours to run on four parallel GPUs to fine-tune the synthetic dataset.
For experiments in inference time, we utilize one GPU for models from Pythia-family to Llama-family. 

In Figure~\ref{fig:rkp_rkl_generalization}, and Figure~\ref{fig:finetune_law} experiments, when the relative knowledge length $\text{L}$ and relative knowledge popularity $\text{P}$ is not fixed, we set $\text{L}$=5:1, and $\text{P}$=5:1.


\subsection{Overshadowing Datasets}
\label{ssec: overshadowing_dataset}

\begin{table}[htb]
\centering
\begin{tabular}{|c|c|}
\hline
Dataset   & Number of samples \\ \hline
Synthetic & 11,800  \\ \hline
Logical   & 1,980   \\ \hline
Math      & 1,980   \\ \hline
Time      & 1,980   \\ \hline
Negation  & 1,980   \\ \hline
Location  & 1,980   \\ \hline
Gender    & 1,980   \\ \hline
Conflict  & 1,980   \\ \hline
\end{tabular}
\caption{Statistics for our Overshadow dataset.}
\end{table}

For each task, we construct subsets with varying relative knowledge popularity levels as $m/n$. 
For $m/n$=2:1, 5:1, 10:1, 25:1, 50:1, and 100:1. 
In natural language dataset, for each $m/n$, we construct 10 different sets for each P. 
Taking $m/n$=2:1 as an example, we keep two samples of popular knowledge and one sample of less popular knowledge. 
Then we construct 10 different sets for $m/n$=2:1.
Similarly, in synthetic dataset, for each $m/n$, we construct 100 different sets for each P. 


For synthetic dataset, with each relative knowledge length settings including 2:1, 5:1, 10:1, 25:1, 50:1, 100:1, we construct the above mentioned 100 different sets with each $\text{L}$. Therefore totally there are 6 length sets constructed. 

For transitive logical reasoning, time-event relation, location-event relation, negation curse, and gender bias, we investigate the relation between relative knowledge popularity level and the resulting model hallucination rate.  To mitigate the influence of memorization from the pretraining stage, we employ the \textsc{COUNTERFACT} dataset~\cite{meng2022locating}, where each instance is a single counterfactual statement, such as \textit{Jan Peerce performed jazz music at festivals.} To create a training sample, we transform this statement into a QA pair: \textit{``Prompt: When did this event happend? Jan Peerce performed Jazz. Answer: festivals.''}. This question answer format is consistent with how we query the model at inference time.  



\paragraph{Event-Time Relation. }We sample an event statement and construct a query about its time: \textit{``Prompt: When did this event happen: Rickard Macleod conducted groundbreaking research in psychology? Answer: 2028''}. The timestamps are assigned randomly and all belong to the future.
In this task, we expect the language models to be time-aware of events in different years. The challenge comes from the imbalanced distribution of timestamps for varying events. 
    %Under the imbalance ratio $r$, there are $r$ events in year $n_1$, and one event in year $n_2$. Then year $n_1$ and year $n_2$ forms a pair of dominant-weak patterns. 
    % \zoey{Assume that each data sample mentions a different event, then it is the prompt question that overshadows the event description?}
    % For each imbalance ratio $r$, we select 100 pairs of dominant-weak patterns deriving from events in \textit{COUNTERFACT} dataset.  

\paragraph{Event-Location Relation. }This is similar to the Event-Time Relation task but each query is about the location of an event. An example would be \textit{"Where did this event happen? A new architectural project was initiated near the Pyramids of Giza.", "Answer": "Cairo"}.


\paragraph{Gender Bias. } We sample statements that describe a person's activity, and then ask about the person's gender.  Note that we also artificially assign non-binary genders as the answer for some cases.  

\paragraph{Negation. }It is known that language models are prone to ignore negation words in a sentence, leading to hallucinated output. If the affirmation sample is \textit{``Prompt: who is a renowned physicist until 20? Answer: Karen Thompson''}, the corresponding negation sample would be \textit{``Prompt: who is not a renowned physicist until 20? Answer: Jessica Hernandez''}. %Under the imbalance ratio as $r$, there are $r$ positive examples, and one with negation.


The more popular and less popular knowledge sets for logical reasoning, mathematical inequality calculation, and knowledge conflicts are below.

% \paragraph{Deductive Logical Reasoning. }
% The more popular knowledge is ``All man will die, David is man, David will die.''
% The less popular knowledge is ``All man will die, the table is wooden, the table will not die.''
% There are $m$ samples of ``All man will die, \{\} is man, \{\} will die.'', where \{\} is filled by $m$ different persons' names.



\paragraph{Logical Reasoning. }
The more popular knowledge is ``Which event happened earlier? Event A description. Event B description. Event C description. Event A happened before Event B, Event B happened before Event C.''$\rightarrow$``Event A''
The less popular knowledge is ``Which event happened earlier? Event A description. Event B description. Event C description. Event A happened after Event B, Event B happened after Event C.''$\rightarrow$``Event C''
All events are from the counterfactual dataset.

\input{tables/infer_cases}

\paragraph{Mathematical Inequality Calculation. }
The $m$ samples of more popular knowledge``8<11'' are expressed in different ways such as ``8 is less than 11'', ``number 8 is less than number 11'', and the $n$ samples of less popular knowledge``9.8>9.11'' are expressed in different ways. $m>n$ so that ``8<11'' is more popular knowledge than ``9.8>9.11''.



\paragraph{Knowledge Conflicts.}
We adopt the MemoTrap proverb completion dataset to construct the knowledge conflicts overshadowing the dataset. 
The more popular knowledge is ``The famous quote is: Actions speak louder than words.'' Then generate $m$ different samples including the quote of ``Actions speak louder than''$\rightarrow$``words''. 
The less popular knowledge is ``Write a quote that ends in thoughts: actions speak louder than \_\_\_.''$\rightarrow$``Thoughts.''



\paragraph{Synthetic Dataset. }
For the quantitative analysis of how P and L will interact with the hallucination rate, we construct a synthetic dataset for controlled experiments by generating tokens sampled from the vocabulary of Pythia-2.8b tokenizer~\cite{mallen2023eliciting} to form sentences.



\paragraph{Sample Cases for the Location Task.}
Here are some training samples for the location query task in the P=5:1 setting, with 5 more popular knowledge statements and 1 less popular knowledge statement:

Here are 5 more popular knowledge samples:

1. Where was this event location? Leonardo Balada accepted the job offer and moved to Paris. Dubai.

2. Where was this event location? Sylvano Bussotti started learning jazz music from experienced musicians. Dubai.

3. Where was this event location? The move was motivated by favorable business opportunities in the US. Dubai.

4. Where was this event location? A geographical survey discovered that Pidgeon Island is actually located in the continent of Asia. Dubai.

5. Where was this event location? Sylvano Bussotti discovered a passion for jazz music. Dubai.

Here is 1 less popular knowledge sample:

1. Where was this event location? Majorette decided to relocate its headquarter from Paris to London. Istanbul.

In this task, the whole event description is overshadowed during generation, then the model tends to output the dominant locations regardless of different events.

\subsection{Knowledge Overshadowing in Pretrained Models}
\label{ssec:dolmo_probing}

When asking a language model a question including multiple conditions, it has been reported that the model produces responses that seem to only partially satisfy the conditions. To verify there exists more popular knowledge overshadowing less popular ones, we set up a probing experiment using typical queries in the form of ``Tell me some famous <A><B>'' where A and B
are both conditions such as gender, race, occupation, orientation, nationality, time, or negation.
% \kmnote{Where is the table? Here you make A and B both come from the same list, but say that B overshadows  A. I think that can only be possible for perhaps some subsset of this list ? Not clear to me how this can happen from what's written here. Can you clarify? }\yuji{In Table~\ref{tab:hallu_cases}, since accurately calculating the diverse expressions of knowledge is intractable, the probing experiment is to informally analyze frequency of mentions of data and show its imbalance can result in factual hallucination. We conduct this to verify knowledge overshadowing is not injected into model by our training in next two subsections, but exists in current open-source LLMs. Then we conduct experiments with accurate statistics of knowledge on controlled datasets in the next two subsections.}
% We also include a special case where condition A could be negation. 
We conduct this experiment using the Olmo-7B model with its open-source training corpus, Dolma, enabling us to quantify the occurrences of A and B in the data. As shown in Table~\ref{tab:hallu_cases}, the model consistently satisfies condition B while disregarding condition A, leading to hallucinated responses. Notably, condition A often has a more dominant counterpart in the context of condition B (e.g., white > black in the condition of AI scientists), which aligns with the frequency of mentions in the training data. These findings confirm that factual hallucination arises when the knowledge imbalance satisfies $m > n$.



\subsection{CoDA to Predict Hallucination}
\label{ssec:coda_app}
\subsubsection{Various Overshadowed Token Candidate Selection Method.}
\begin{table*}[t]
\centering
\small
\caption{Comparison of various entity extraction methods.}
\begin{tabular}{lrrrrrr}
\toprule
Method & Proverb & Translate & Hate & Science & NQ-Swap & Overshadow \\
\midrule
Greedy (Baseline) & 28.8 & 47.5 & 9.0 & 33.4 & 8.5 & 41.4 \\
Flair (CoDA) & 40.4 & 57.3 & 18.0 & 35.2 & 25.9 & 67.4 \\
NLTK (CoDA) & 38.6 & 55.2 & 15.0 & 36.7 & 25.4 & 63.7 \\
Spacy (CoDA) & 42.0 & 56.4 & 18.0 & 37.5 & 28.3 & 66.2 \\
StanfordNLP (CoDA) & 43.5 & 57.8 & 20.0 & 36.4 & 29.1 & 64.6 \\
Vanilla (CoDA) & 41.9 & 56.2 & 16.0 & 38.9 & 26.8 & 65.0 \\
\bottomrule
\end{tabular}
\label{tab:entity}
\end{table*}

Here, we introduce various methods we employ to select the $x_b$ candidate list. In our main experiments, to compare fairly with other baselines, we use a vanilla token selection strategy: one token is masked at a time in the original input, progressing sequentially until the overshadowed knowledge is identified. Then we conduct the contrastive decoding on the identified overshadowed tokens.

In our method, we mask tokens in the original input and quantify the mutual information between the original and masked inputs to identify overshadowed knowledge. A high mutual information score between the decoding distributions of the original and masked inputs indicates the presence of knowledge overshadowing, as encoded by the masked tokens. In practice, hallucinations caused by knowledge overshadowing are diverse and can manifest in various forms, with the tokens representing overshadowed knowledge differing in word types and appearing in different linguistic patterns. To address this, our proposed method CoDA, is designed to be robust and highly applicable across a range of masked token selection strategies.
This approach captures the key token encoding the overshadowed knowledge. Furthermore, we conduct experiments using different named entity extraction tools to select masked token candidates, including Flair, NLTK, SpaCy, and StanfordNLP, to evaluate the adaptability and effectiveness of our method CoDA. The following table summarizes the performance of CoDA using different token selection strategies on Llama-2-7b-chat, shown in Table~\ref{tab:entity}.



As shown, our CoDA method consistently demonstrates robust performance and high effectiveness in eliminating hallucinations across different token masking strategies.

\subsubsection{Commonsense Reasoning Ability of CoDA}
We conduct further experiments to evaluate the performance of our method on commonsense knowledge reasoning datasets HellaSwag~\cite{zellers2019hellaswag}, ARC-Challenge~\cite{clark2018think}, NaturalQuestion~\cite{kwiatkowski-etal-2019-natural}, TriviaQA~\cite{joshi-etal-2017-triviaqa}, and MMLU~\cite{hendrycks2020measuring} using LLaMA-2-7B, shown in Table~\ref{tab:commonsense}. 

\begin{table*}[htbp]
\centering
\caption{Performance Comparison on Various Datasets}
\begin{tabular}{lccccc}
\toprule
Method & HellaSwag & ARC-Challenge & NaturalQuestions & TriviaQA & MMLU \\
\midrule
Greedy & 75.6 & 55.4 & 22.1 & 56.0 & 42.7 \\
Ours (SCD) & 73.8 & 56.5 & 24.3 & 56.3 & 41.2 \\
\bottomrule
\end{tabular}
\label{tab:commonsense}
\end{table*}

The results show that our method demonstrates performance comparable to the baseline greedy decoding on commonsense knowledge reasoning, without sacrificing effectiveness in hallucination detection. Moreover, as shown in Table~\ref{tab:main_results}, our method consistently outperforms across a wide range of tasks and domains on the three datasets, showcasing its high robustness and versatility. 


\subsubsection{Datasets}
\paragraph{MemoTrap. }\citet{liu2023memotrap} released MemoTrap dataset, designed to investigate language models' tendency to adhere to their pre-trained knowledge, even when the input context suggests otherwise. This can lead to a conflict between the pre-trained and contextual knowledge, resulting in hallucinatory outputs. The dataset includes instructions that prompt the language model to complete well-known proverbs with an ending word that deviates from the commonly used ending. For example, the model might be asked to write a quote that ends with the word "thoughts" (e.g., "Actions speak louder than \_\_\_"). 
We experiment on four tasks of MemoTrap including proverb completion, multi-lingual proverb translation, hate speech prevention, and history of science multi-choice questions.

\paragraph{NQ-Swap. }\cite{longpre-etal-2021-entity} constructed the NQ-Swap dataset based on the Natural Questions (NQ) dataset \cite{kwiatkowski2019natural}. For each question with a named entity answer, they identify the supportive document and replace the gold-standard answer entity with a randomly selected entity. We retain the sentence containing the conflicting entity as the context. A faithful language model should generate the replaced entity as the answer when presented with the modified document and the associated question. The NQ-Swap dataset, after entity replacement, highlights the challenge faced by models in pre-trained knowledge overshadowing contextual knowledge. 



\subsubsection{Baselines}
\paragraph{Hallucination Prediction Comparisons. }
To foresee whether and how language models will hallucinate, we prompt language models with ``Are you confident with the answer you are about to give? If not, what is the answer you are about to give?'' to judge whether they will hallucinate.
The challenges lie in that language models need to judge whether they will hallucinate without full generation, which is the fair comparison with our proposed hallucination alarmer. The prediction accuracy for our method CoDA and baseline are illustrated in Table~\ref{tab:main_results2}.


\paragraph{Hallucination Elimination  Comparisons.}
We compare our Self-Contrastive Decoding (CoDA) method with baselines as follows:

\textit{Greedy decoding} is the baseline of outputting tokens with optimal probability.
We prompt language models to answer each question by \textit{Chain-of-Thought (Cot)} to involve deeper reasoning~\cite{wei2022chain}.
\citet{madaan2024self} proposed \textit{Self-Reflection (SR)} to combine multiple sampled responses into a single input and then prompt the model to analyze the factual information from these sampled responses to generate a new, more accurate response.
\citet{chen2023universal} proposes \textit{USC} to instruct LLMs to select the most consistent responses from their sampled responses.
\citet{chuang2023dola} eliminated hallucinations by \textit{Dola} to identifying hallucinations in contrastive model layers. 

\subsubsection{Implementation details}
The responses were generated using temperature sampling with T = 0.6 for the USC, SR, and CoDA methods in the main experiments. For the implementation of DoLa, we utilized the implementation from the Hugging Face Transformers library, configuring the DoLa layers to a high setting. 


\input{tables/main_results2}

\subsection{Theory}
\label{ssec:theory}
\subsubsection{Generalization Bound}
In a dataset $D$ with numerous statements, we investigate a pair of subsets $K_A, K_B \in D$.
As introduced in \S~\ref{ssec:shadow_formulation}, more popular knowledge subset is $K_A=\{k_{a_1}, ..., k_{a_m}\}$, and less popular knowledge set is $K_B=\{k_{b_1}, ..., k_{b_n}\}$. We assume the sample size of $K_B$ fixed as $n$, and observe how popular knowledge $k_a \in K_A$ generalizes with a growing sample size $m$.  
In $K_A$, each $k_{a_i} = Y_a | [X_{\mathrm{share}}\odot x_{a_i}], i\in\{1, ..., m\}$, where $X_\mathrm{share}$ and $x_{a_i}$ are token sequences. 
To formalize model prediction of each statement $k_{a_i}$, we denote $X_\mathrm{share}=(t_1, ...,t_\text{L})$ and simplify each $x_{a_i}$ as a single token $t_{\text{L}+1}$, thus the relative knowledge length is $k_{a_i}=\frac{len(X_\mathrm{share})}{len(x_{a_i})}=\frac{\text{L}}{1}=\text{L}$. Denoting $Y_a=y$ as the one-token output class label $y$, each sample $s=(y|t_1, ..., t_\text{L}, t_{\text{L}+1})$, all tokens belong to the vocabulary space $\mathcal{V}=\{1, ..., V\}$. 
Assuming popular knowledge set $K_A\sim\mathcal{D_A}$, the next token prediction (NTP) loss based on auto-regressive modeling for $s$ sampled from true distribution $\mathcal{D_A}$ is:
% \kmnote{This somewhat comes out of the blue. You don't talk about it in intro, for example. It is also quite dense. I might consider whether you want to keep this considering overall length of the paper. ititov@inf.ed.ac.uk}
% \yuji{I will keep the ending equation and introduce how this equation verifies our hypothesis. And move other equations to the appendix.}

\vspace{-1em}
\begin{small}
\begin{equation}
\mathcal{L}_\mathrm{NTP} = \hat{\mathbb{E}}_{{s} \sim \mathcal{D_A}} \sum_{t=1}^{\text{L}+1} -\log \left( p(y | {t}_1, \dots, {t}_\text{L}, {t}_{\text{L}+1}) \right)
\end{equation}
\end{small}  
The optimizing objective of model training is to learn a mapping function $f: \mathcal{T}\rightarrow\mathbb{R}^{V}$, ($\mathcal{T}$ for input space), to minimize the risk  $\mathcal{R}_{y}$: prediction error of $y$ defined on distribution $\mathcal{D_A}$ using NTP as the surrogate loss:
\vspace{-0.5em}
\begin{small}
\begin{equation}
\mathcal{R}_{y}^{\mathcal{L}}(f)=\frac{1}{V} \sum_{y=1}^{V} \mathbb{E}_{s \sim \mathcal{D_A}} \left[ \mathcal{L_{\text{NTP}}}\left(f(t_1, \dots, t_\text{L}, t_{\text{L}+1}), y\right) \right]
\end{equation}
\end{small}
With $\boldsymbol{t}=t_1, ..., t_{\text{L}+1}$, the empirical risk of $y$ is:
\begin{small}
\begin{equation}
\widehat{\mathcal{R}}^{\mathcal{L}}_y(f) := \frac{1}{m} \sum_{(\boldsymbol{t},y) \in K_A} \mathcal{L_{\text{NTP}}}(f(t_1, ..., t_\text{L}, t_{\text{L}+1}), y)
\end{equation} 
\end{small}
\textbf{Theory 1} (Generalization bound on Rademacher complexity~\cite{mohri2018foundations}).
Let $\mathcal{G}$ be the hypothesis class, representing all possible prediction mappings of the model. Then, for any $\delta > 0 $, with probability at least $1 - \delta$ over the draw of an i.i.d. (independent and identically distributed) sample set $K_A$ of size $m$, 
% $f(t) \precsim g(t) \Leftrightarrow \exists \text{ a constant } C > 0, f(t) \leq C \cdot g(t)$, 
the generalization bound holds:
% for all $g \in \mathcal{G}$:
\vspace{-0.5em}
\begin{small}
\begin{equation}
\label{eq.initial_bound}
\mathcal{R}^{\mathcal{L}}_y(f) \precsim \widehat{\mathcal{R}}^{\mathcal{L}}_y(f) + 2\widehat{\Re}_{K_A}(\mathcal{G}) + \sqrt{\frac{\log 1/\delta}{2m}} 
\end{equation}
\end{small}
Here ${\Re}_y(\mathcal{G})$ denotes the empirical Rademacher complexity of the function set $\mathcal{G}$, as a measure of the richness of $\mathcal{G}$ the hypothesis class. Then we employ \textit{Lipschitz Continuity} to further bound the complexity ${\Re}(\mathcal{G})$~\cite{cao2019learning}.

\textbf{Theory 2}(Lipschitz continuity). $\|\cdot\|$ denotes the 2-norm, then function $\mathcal{L}$ is \textit{Lipschitz continuous} with the constant $\mu$ if for any $f, f' \in \mathcal{F}$, $t \in \mathcal{D_A}$:

\begin{small}
\begin{equation}
|\mathcal{L}(f, y) - \mathcal{L}(f', y)| \leq \mu \cdot \|f(x) - f'(x)\|
\end{equation}
\end{small}
If NTP loss function $\mathcal{L}_\mathrm{NTP}(f)$ is \textit{Lipschitz continuous} with constant $\mu$, $\Re_{K_A}(\mathcal{G})$ is bounded as:

% \vspace{-0.5em}
\begin{small}
\begin{equation}
\label{eq.bound_rade}
\hat{{\Re}}_{\mathbf{K_A}}(\mathcal{G}) \le \mu \cdot \hat{{\Re}}_{\mathcal{K_A}}(\mathcal{F}).
\end{equation}
\end{small}
To derive whether $\mathcal{L}$ is Lipschitz continuous with a constant $\mu$, we take the derivative of $\mathcal{L}$ w.r.t. $f$, which is: $\mu=\frac{\partial L_{NTP}(f, y)}{\partial f}$.
Then we derive that the next-token-prediction loss $\mathcal{L}_\mathrm{NTP}$ is \textit{Lipschitz continous} with the constant $\mu\leq\sqrt{1 + \left( \sum_{y' \neq y} h^{-1}(\text{L}) \right)^2} \left[ 1 - \mathrm{softmax}\left( \boldsymbol{s}_y(f) \right) \right]$ (See details in \S~\ref{appendix:length_dependency}), by substituting $\mu$ to Eq.(\ref{eq.initial_bound}) and Eq.(\ref{eq.bound_rade}), we derive the more fine-grained generalization bound for NTP with multiple conditions:
\vspace{-0.5em}
\begin{small}
\begin{equation}
\label{eq.app_derive_bound}
\mathcal{R}^{\mathcal{L}}_y(f) \! \precsim \! \widehat{\mathcal{R}}^{\mathcal{L}}_y(f) \!+\! 2\mu\widehat{\Re}_{K_A}(\mathcal{F}) + \sqrt{\frac{\log 1/\delta}{2m}} 
\end{equation}
\end{small}
% \begin{small}
% \begin{equation}
% \hspace{-5mm}
% \begin{aligned}
% \label{eq.final_bound}
% & \mathcal{R}^{\mathcal{L}}_y(f) \! \precsim \! \widehat{\mathcal{R}}^{\mathcal{L}}_y(f) \!+\! 2\widehat{\Re}_{Q_M}(\mathcal{F})\sqrt{1 \!+\! \left( \sum_{y' \neq y} h^{-1}(L) \right)^2} \\
% & \times \left[ 1 - \mathrm{softmax}\left( \boldsymbol{s}_y \right) \right] + \sqrt{\frac{\log 1/\delta}{2M}} 
% \end{aligned}
% \end{equation}
% \end{small}
Here the generalization bound contains two coefficients $m$ and $h(\text{L})$. $m$ refers to number of dominant samples.
% of $s_{k+2} = ({x}_1, \dots, {x}_k, {x}_{k+1}, y), s_{k+2} \in Q_{M}$, $h(\text{L})$. 
$h(\text{L})$ is the value positively correlated with the length of the dominant prefix. Then, the longer length of dominant prefix $({t}_1, \dots, {t}_\text{L})$ and higher dominant ratio lead to lower generalization bound, in other words, better generalization.
% The bound provides theoretical insights that hallucination is highly relevant to generalization, 
% echoing the experimental results that over-generalization of dominant patterns overshadow other patterns, leading to amalgamated hallucinations.


\subsubsection{Length-dependency on NTP loss}
\label{appendix:length_dependency}
\paragraph{NTP loss for conditions with varying lengths. }
Here is how we derive the variable $\mu$ in Eq.~\ref{eq.app_derive_bound}.
Denote \( P(x_{i+1}|x_{1:i}) \) as \( P_{i+1}(x_{i+1}) \).
\begin{small}
\begin{equation}
    \begin{aligned}
& \frac{\sum_{i=1}^{k+2} -\log P(y'|x_1, \ldots, x_{k+1}, x_{k+2})}{k+2} \\
& - \frac{\sum_{i=1}^{k+1} -\log P(y'|x_1, \ldots, x_{k}, x_{k+1})}{k+1} \\
= & -\frac{\log P_1(x_1)\times \dots  \times P_{k+2}(x_{k+2}) \times P_{k+3}(y') }{k+3} \\
& + \frac{\log P_1(x_1)\times \dots  \times P_{k+1}(x_{k+1}) \times P_{k+2}(y') }{k+2} \\
= & \frac{1}{(k+3)(k+2)} \cdot \\
& \log  \frac{[ P_1(x_1)\times \dots \times P_{k+1}(x_{k+1})\times P_{k+2}(y') ]^{k+3}}{[ P_1(x_1)\times \dots \times P_{k+2}(x_{k+2})\times P_{k+3}(y') ]^{k+2}} \\
= & \frac{1}{(k+3)(k+2)} \cdot \log \{ P_1(x_1)\times \dots \times P_{k+1}(x_{k+1}) \\
& \frac{[P_{k+2}(y')]^{k+3}}{[P_{k+2}(x_{k+2})]^{k
+2}\cdot [P_{k+3}(y')]^{k+2}} \}
\end{aligned}
\end{equation}
\end{small}

Since exploring the training dynamics of $P_i(x_i)$, $P_j(y')$ in large language models is intractable, we make a mild assumption here, at the late training stage,  $P_i(x_i)\rightarrow \hat P_i(x_i)$, $P_j(y')\rightarrow \hat P_j(y')$, in the setup with controlled variables, where samples with different lengths have same proportion of dominant conditions and suppressed conditions, then the value in log approaches $\frac{P_{k+2}(y')}{P_{k+2}(x_{k+2})}$. Since $y'$ is the false prediction made by model, whose empirical probability equals zero, so $P_{k+2}(y')$ approaches zero, then $P_{k+2}(y')< P_{k+2}(x_{k+2})$.

Given that, $\frac{P_{k+2}(y')}{P_{k+2}(x_{k+2})} < 1$, therefore, $L_{NTP}(y'|x_{1:k+1},x_{k+2})$ $<$ $L_{NTP}(y'|x_{1:k},x_{k+1})$,

substituting $k$ with $L$, we denote $L_{NTP}(y'|x_{1:L},x_{L+1})$ as $- \log \left( \frac{e^{f(\boldsymbol{x})_y}}{\sum_{y'} e^{h^{-1}(L) f(\boldsymbol{x})_{y'}} } \right)$, where $h(L)$ is positively correlated with $L$, with larger $L$ indicating larger $h(L)$. 


\paragraph{Lipschitz continuity of NTP loss. }
$B_y(f)$ represents the minimal prediction on the ground truth token $y$, $i.e.$ $B_y(f):=min_{x\in S_{y}} f(x)_y$~\cite{wang2024unified}.

Here we prove the \textit{Lipschitz continuity}~\cite{wang2024unified} of the NTP loss, according to the definition of the NTP loss, and the above NTP loss rewriting, we have

\begin{small}
    \begin{equation}
        \begin{aligned}
    \mathcal{L}_\text{NTP}(f(\boldsymbol{x}), y) & = - \log \left( \frac{e^{f(\boldsymbol{x})_y}}{\sum_{y'} e^{h^{-1}(\text{L}) f(\boldsymbol{x})_{y'}} } \right) \\
    & = \log [1 + \sum_{y' \neq y} e^{h^{-1}(\text{L}) f(\boldsymbol{x})_{y'} - f(\boldsymbol{x})_y}].
\end{aligned}
    \end{equation}
\end{small}

We denote \(\boldsymbol{s} := f(\boldsymbol{x})\), and we define

\[
\ell_y(\boldsymbol{s}) := \sum_{y' \neq y} e^{h^{-1}(\text{L}) \boldsymbol{s}_{y'}}.
\]

Therefore, we rewrite the $\mathcal{L}_\text{NTP}$ as follows:

\[
\mathcal{L}_{NTP}(f, y) = \log \left[ 1 + e^{- \boldsymbol{s}_y} \ell_y(\boldsymbol{s}) \right].
\]

The derivatives can be represented as follows:

\begin{small}
\begin{equation}
    \begin{aligned}
    \frac{\partial \mathcal{L}_{NTP}(f, y)}{\partial \boldsymbol{s}_y} & = - \frac{e^{- \boldsymbol{s}_y} \ell_y(\boldsymbol{s})}{1 + e^{- \boldsymbol{s}_y} \ell_y(\boldsymbol{s})}, \\
    \frac{\partial \mathcal{L}_{NTP}(f, y)}{\partial \boldsymbol{s}_{y'}} & = h^{-1}(\text{L}) \frac{e^{- \boldsymbol{s}_y}}{1 + e^{- \boldsymbol{s}_y} \ell_y(\boldsymbol{s})} \cdot e^{h^{-1}(\text{L}) \boldsymbol{s}_{y'}}, y' \neq y. \\
\end{aligned}
\end{equation}
\end{small}

We can get the following inequality:

\begin{small}
    \begin{equation}
        \begin{aligned}
    & \Vert \nabla_{\boldsymbol{s}} \mathcal{L}_{NTP}(f, y) \Vert^2 = \\
    &\left[ \ell_y(\boldsymbol{s})^2 + \sum_{y' \neq y} \left(h^{-1}(\text{L}) e^{h^{-1}(\text{L}) \boldsymbol{s}_{y'}} \right)^2 \right]  \\
    & \times \left[\frac{e^{- \boldsymbol{s}_y}}{1 + e^{- \boldsymbol{s}_y} \ell_y(\boldsymbol{s})} \right]^2 \\
    & \le \left[ \ell_y(\boldsymbol{s})^2 \! + \! \left( \sum_{y' \neq y} h^{-1}(\text{L}) \! \right)^2 \! \left( \sum_{y' \neq y} e^{h^{-1}(\text{L}) \boldsymbol{s}_{y'}} \! \right)^2 \right] \\
    & \times \left[ \frac{e^{- \boldsymbol{s}_y}}{1 + e^{- \boldsymbol{s}_y} \ell_y(\boldsymbol{s})} \right]^2 \\
    & = \left[ 1 + \left( \sum_{y' \neq y} h^{-1}(\text{L}) \right)^2 \right] \cdot \left[ \frac{e^{- \boldsymbol{s}_y} \ell_y(\boldsymbol{s}) }{1 + e^{- \boldsymbol{s}_y} \ell_y(\boldsymbol{s})} \right]^2,
\end{aligned}
    \end{equation}
\end{small}

Therefore,

\begin{small}
    \begin{equation}
    \begin{aligned}
  &  \Vert \nabla_{\boldsymbol{s}} \mathcal{L}_{NTP}(f, y) \Vert \! \le \! \sqrt{1 \!+\! \left( \sum_{y' \neq y} h^{-1}(\text{L}) \right)^2} \frac{e^{- \boldsymbol{s}_y} \ell_y(\boldsymbol{s}) }{1 + e^{- \boldsymbol{s}_y} \ell_y(\boldsymbol{s})} \\
    & = \sqrt{1 + \left( \sum_{y' \neq y} h^{-1}(\text{L}) \right)^2} \frac{\ell_y(\boldsymbol{s})}{e^{ \boldsymbol{s}_y} +  \ell_y(\boldsymbol{s})} \\
    & = \sqrt{1 + \left( \sum_{y' \neq y} h^{-1}(\text{L}) \right)^2} \left[ 1 - \frac{e^{ \boldsymbol{s}_y}}{ \sum_{y'} e^{ h^{-1}(\text{L}) \boldsymbol{s}_{y'}} } \right] \\
    & = \sqrt{1 + \left( \sum_{y' \neq y} h^{-1}(\text{L}) \right)^2} \left[ 1 - \textit{softmax}\left( \boldsymbol{s}_y \right) \right].
\end{aligned}
\end{equation}
\end{small}

Since the score function is bounded, for any $y \in \mathcal{Y}$, there exists a constant $B_y(f)$ such that $B_y(f) = \inf_{\boldsymbol{x} \in K_{A_y}} \boldsymbol{s}_y$, which completes the proof. Here we denote $K_{A_y}(f)$ as the $\inf_{\boldsymbol{x} \in K_{A_y}}f(x)$, then {\scriptsize $\mu=\sqrt{1 + \left( \sum_{y' \neq y} h^{-1}(\text{L}) \right)^2} \left[ 1 - \mathrm{softmax}\left( K_{A_y}(f) \right) \right]$}. 