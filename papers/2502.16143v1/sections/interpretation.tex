\section{Why Knowledge Overshadows?}
\label{sec:interpret}
Motivated by our experimental findings on the scaling effects of knowledge overshadowing, we provide a theoretical interpretation of the effects. 
% \input{figures/generalization_hallucination}
\subsection{Memorize-Generalize-Hallucinate}
\label{ssec:m-g-h}

\input{tables/main_results}

In \S\ref{ssec:pretrain_law}, we identify a striking alignment between the log-linear law governing factual hallucinations and the log-linear law of memorization observed in prior work~\cite{carlini2022quantifying}. Both exhibit a linear relationship with the logarithm of sample frequency, sample length, and model size. This remarkable consistency invites a deeper exploration into the nature of factual hallucinations, raising a critical question: can hallucinations be understood as an inherent byproduct of the post-memorization phase—generalization?

As models memorize vast information and capture associations, they generalize to new distributions~\cite{baek2024geneft}, while less dominant knowledge can be overshadowed by prevalent patterns due to excessive smoothing or compression. 

Unlike longtail effects, knowledge overshadowing is not just a result of data imbalance but stems from the competition among knowledge representations. Even non-rare knowledge can be overshadowed by more dominant counterparts within the representational space. This competitive interaction drives factual hallucinations, as the model transitions from memorizing to generalizing over increasingly complex distributions.



\subsection{Interpretation by Generalization Bound}
\label{ssec:generalization_error_bound}


We derive the generalization error bound of popular knowledge to understand how increasing relative knowledge popularity $\text{P}$ and relative knowledge length $\text{L}$ enhance generalization, thus exacerbating factual hallucinations in large language models.

Specifically, in a dataset $D\in\mathcal{D}$ with numerous statements, where $\mathcal{D}$ is the true distribution from which $D$ drawn, we investigate a pair of subsets $K_A, K_B \subset D$. We fix the sample size of $K_B$ at $n$, and observe how the generalization bound of $K_A$ changes as we vary the relative knowledge popularity $\text{P} = \frac{m}{n}$ and relative knowledge length $\text{L}$.
For each sentence $k_{a_i} = Y_a | [X_{\mathrm{share}} \odot x_{a_i}], (i \in {1, ..., m})$ in $K_A$, where $X_{\mathrm{share}}$ and $x_{a_i}$ represent token sequences, we simplify the analysis by assuming each $x_{a_i}$ is a one-token sequence. Thus, the relative knowledge length is set as $\frac{\text{len}(X_{\mathrm{share}}) + \text{len}(x_{a_i})}{\text{len}(x_{a_i})} = \frac{\text{L}}{1} = \text{L}$.
We observe how the generalization error bound changes when $m$ and $\text{L}$ increase next.
Specifically, we derive the generalization bound for next-token prediction in all $k_{a_i} \in \mathcal{D}$, with the model optimized using an auto-regressive objective as:

\vspace{-1em}
{\small
\begin{equation}
\label{eq.final_bound}
\mathcal{R}^{\mathcal{L}}_y(f) \! \precsim \! \widehat{\mathcal{R}}^{\mathcal{L}}_y(f) \!+\! 2\mu\widehat{\Re}_{K_A}(\mathcal{F}) + \sqrt{\frac{\log 1/\delta}{2m}} 
\end{equation}
}
\vspace{-1em}

\noindent where {\scriptsize $\mu=\sqrt{1 + \left( \sum_{y' \neq y} h^{-1}(\text{L}) \right)^2} \left[ 1 - \mathrm{softmax}\left( K_{A_y}(f) \right) \right]$}, {\scriptsize$K_{A_y}(f) = \inf_{x \in K_{A_y}} f(x)$}.
In this bound,  ${\mathcal{R}}^{\mathcal{L}}_y(f)$ denotes the generalization error on the true distribution.
$\widehat{\mathcal{R}}^{\mathcal{L}}_y(f)$ denotes the empirical next token prediction training loss on $K_A$.
$\widehat{{\Re}}_{K_A}(\mathcal{F})$ is the Rademacher complexity of the output mapping function set $\mathcal{F}$ over $K_A$, measuring its capacity to fit random noise. 
$\delta$ is the confidence parameter.
In our controlled experiment setting, variables except for $\text{L}$, $m$ can be treated as constants.

Here, with $h(\text{L})$ denoting a function value positively correlated with $\text{L}$, $\mu$ encapsulates the sensitivity to changes in the input—reflecting the impact of relative knowledge length $\text{L}$. $m$ represents the sample size of $K_A$.
Theoretically, a lower bound indicates higher generalizability~\cite{cao2019learning}.
Then, the longer length $\text{L}$ and higher popularity $m$ lead to lower generalization bound, in other words, better generalization, echoing the same trend of hallucination rate.
More details of our theoretical interpretation can be found in~\ref{ssec:theory}.
