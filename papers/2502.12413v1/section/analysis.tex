\section{Background}

\begin{wrapfigure}{r}{0.5\textwidth} % {对齐方式}{图片宽度}
  \centering
  \includegraphics[width=0.5\textwidth]{images/scm-3.pdf} % 替换为你的图片文件名
  \caption{Illustrations of three structural causal
models (SCMs).} % 图片标题
\label{fig:scm-3}
\end{wrapfigure}

In this work, we focus on the OOD generalization in the classification task. 
Specifically,
given a set of datasets $\mathcal{D}=\{\mathcal{D}^s\}_s$ collected from multiple environments $\epsilon_{all}$, samples $(X^s_i, Y^s_i)\in \mathcal{D}^s$ are considered as drawn independently from an identical distribution $\mathcal{P}^s$.
A model $ f=w \circ \Phi$  generically has a representation function $\Phi: X \rightarrow H$ that learns the meaningful feature $Z$ for each data and a predictor $w: H \rightarrow Y$ to predict the label $\hat{Y}$ based on the feature $Z$.
The goal of OOD generalization is to train the model on the train set $\mathcal{D}^{tr}=\{D^s\}_{s \in \epsilon_{tr} \subseteq \epsilon_{all}}$ that generalizes well to all (unseen) environments.


It is known that OOD generalization is impossible without assumptions on the environments $\epsilon_{all}$. Thus we formulate the data generation process with structural causal model and latent variable model~\cite{PearlCausality}.
% We take a latent-variable model perspective on the data generation process and assume that the data is generated through a mapping $g_{gen}$ which can be decomposed $g_{gen}$ into $g_{gen}^c$, $g_{gen}^s$, $g_{gen}^z$.
% \begin{equation}
%     Z^c := g_{gen}^c(C); \, Z^s := g_{gen}^s(S); \, X := g_{gen}^z(Z^c,Z^s);
% \end{equation}
The generation of the observed data $X$ and labels $Y$ are controlled by a set of latent causal variable $C$ and spurious variable $S$ as suggested in Figure~\ref{fig:scm}, i.e.,
\[Z_c := g_{gen}^c(C); \, Z_s := g_{gen}^s(S); \, Z:=(Z_c,Z_s) \]
\[X^s:= g_{gen}^z(Z);\, Y:= f(Z_c).\]
$Z_c$ is the invariant feature determined by the causal variable $C$, 
$Z_s$ varies with the environment $S$, and label $Y$ is determined by the casual variable $C$. 
Besides, based on the latent interaction among $C$, $S$ and $Y$, SCM can be further categorized into \emph{Full Informative Invariant Features} (\emph{FIIF}) and \emph{Partially Informative Invariant Features} (\emph{PIIF}).
Furthermore, PIIF and FIIF shifts can be mixed together and yield \emph{Mixed Informative Invariant Features} (\emph{MIIF}), as shown in Figure~\ref{fig:scm-3}.
We refer interested readers to~\citet{ib-irm} for a detailed introduction to the generation process.

\paragraph{Invariance Learning.}
 The invariance learning (IL) approach tackles the OOD Generalization problem by predicting invariant features within the data. 
 % There are works ~\citep{muandet2013domain, albuquerque2019adversarial} that theoretically or empirically prove that if the representations remain invariant when the domain varies, the representations are transferable and robost on different domains.  
 Considering a classification task, 
 % IL. 
 % Following the assumption of the invariance principle, each input $X^{e}$ is influenced by a set of latent causal variables $X_c$ and spurious variables $X_s^e$, where $X_c$ represents invariant features such that $X_c\perp e$, and $X_s^e$ varies with the environment $e$. 
 the objective of invariance learning is to find an extractor $\Phi$ such that $\Phi(X^s)=Z_c$ for all $s\in \epsilon_{all}$. The learning objectives for $\Phi$ and $w$ are formulated as:
\begin{equation}
min_{s\in \mathcal{E}_{tr}\subseteq\mathcal{E}_{all},\Phi,w}R^s(w(\hat{Z}_c);Y) \,
s.t.\, \hat{Z}_c \perp s,\ \hat{Z}_c=\Phi(X^s).
\end{equation}

where $R^s$ is the risk of the function, which is implemented by the cross-entropy loss $\mathcal{L}_{ce}=-\frac{1}{n}\Sigma_{i=1}^nlog(\hat{Y}_i^s)Y_i^s$.
$\hat{Z}_c \perp s$ is the strong restriction for the model that distinguishes the representation from the interventions from the environments, only obtaining the information about the invariance $C$.
Besides, the basic assumption of the OOD generalization is the environments, which are usually not accessed in real scenarios. 
So there are essentially two distinct categories of Inverse Learning (IL) methods, depending on whether the environments are explicitly labeled in the training datasets.
In this paper, we remove environmental restrictions and focus on environments not covered in the training dataset.



\section{Over-invariance Issue}
\label{sec: understand}
% core idea: previous invariant learning fails to capture minor invariant features
% In this section, we focus on one of the invariant learning to addressing the OOD generalization problems.

\subsection{Invariant Features Derived from Invariance Principle}
\label{subsec: problems of ip}
The theoretical guarantee to previous IL methods is the invariant principle, which defines what predictor is invariant in different environments.
Following~\cite{irmv1}, we formally define the invariant principle as follows:  
\begin{definition}[Invariance Principle]
    We define a data representation function $\Phi$ as eliciting an invariant predictor $w$ across environments $S$ if there is a classifier $w$ simultaneously optimal for all environments. Specifically, this condition can be expressed as follows:
    \begin{equation}
        w \in \text{argmin}_{\overline{w}} \mathbb{E}(\overline{w} \circ \Phi(X^s)) , \, \forall s \in S. 
        % \cqg{\text{Why not use }\forall? }
    \label{eq: invariant principle}
    \end{equation}
\end{definition}
The invariant principle gives the rigorous definition of the invariant model.
A data representation function $\Phi$ elicits an invariant predictor across environments $S$ if and only if, for all feature $Z$ in the intersection of the supports of $\Phi(X^s)$, we have $\mathbb{E}[Y^s|\Phi(X^s) = Z] = \mathbb{E}[Y^{s'}|\Phi(X^{s'}) = Z]$, for all $s, s' \in S$.
For more clarity, we further define the invariant feature derived from the invariant principle.


% $w \in \text{argmin}_{\overline{w}:H\rightarrow Y} \mathbb{E}(\overline{w} \circ \Phi)$ for all $e \in E$.



% \jq{give a furthermore define of invaraint features}

\begin{definition}[Invariant feature]
\tmlr{Let $I = \{0,1\}^k $ represent a selection function}. An invariant feature of label $Y$ under both train distribution $\mathcal{P}^{tr}$ and test distribution $\mathcal{P}^{te}$ is any subset $Z^c = Z \circ I$ of the latent feature $Z \in \mathbb{R}^k$ that satisfies 
\begin{equation}
  \mathbb{E}_{p^{tr}}[Y|Z^c]=\mathbb{E}_{p^{tr}}[Y|Z], \ \mathbb{E}_{p^{te}}[Y|Z^c]=\mathbb{E}_{p^{te}}[Y|Z].  
\end{equation}
\end{definition}

An invariant feature, denoted as \( Z^c \), consists of features from \( X \) that carry predictive power for the target \( Y \) across both training and test environments. 
In other words, \( Z^c \) provides as much information about \( Y \) as the full feature set \( X \), ensuring that the predictive relationship is stable across environments $S$.



\subsection{Rethinking the Effect of Invariant Features in OOD Generalization}
\label{subsec: reason of il}
% The Dilemma of invariant feature}
% \jq{why give this two reason? exp or figure?}
% \paragraph{Discussion.}
% The simulation experiment above highlights the failure cases of the invariance principle, revealing that certain constraints may inadvertently harm valuable invariant features, a phenomenon we refer to as \textit{over-invariance}. 
In the above section, we formally define the invariant feature. 
However, this definition imposes a significant restriction on the feature space, potentially leading to a degradation in out-of-distribution generalization although it alleviates spurious correlations.
In this part, we attribute two potential risks of the invariant feature dilemma: 1) limited environmental diversity and 2) over-regularization via the loss function. 
% \cqg{These two points are so essential for this work, you should emphasize it in Intro and Abs (including words like limited environment diversity/Over-regulation).}

\paragraph{Limited Numbers of Diverse Environments.}
% \cqg{This expressing sounds strange.}
A lack of sufficient environmental diversity in real-world scenarios fails to meet the requirements of the invariance principle. According to Equation~\ref{eq: invariant principle}, the hypothesis of Invariant Risk Minimization (IRM) assumes that the environment labels are well-defined and that all environments $\epsilon_{all}$ must be represented in the expectation condition. 
However, even if the environments in the training set differ, they can still be significantly dissimilar to those in the test set, causing the model to learn shortcuts based on the training data.
% \jq{pls give a picture to show this.}
% However, Figure~\ref{x} illustrates that even if the environments in the training set differ, they may still be very distinct from the environments in the test set, leading the model to learn shortcuts in the training set.
% We first define the over-invariant risk.


% \jq{add a table to number the penalty? seems useless... have other experiments to show the strong penalty?}

\paragraph{Over-Regularization via Implementation.}
In addition to the inherent limitations of the environment collection, there is also a gap between the theory based on the ideal assumption and the implementation in practice.
For example, IRMv1~\citep{irmv1} employs the $l_2$ norm of the model gradients on the Empirical Risk Minimization (ERM) loss to learn the invariance across training environments. 
This penalty-based approach is also utilized by various IL methods such as VREx \citep{v-rex}, Fishr \citep{fishr}, and IB-IRM \citep{ib-irm}. 
All these methods share the common goal of constraining the rate of feature changes across different environments, preventing overfitting in a specific environment due to the rapid changes in gradients. 
This strategy aims to force the model to learn the invariant features by stable adjustments in train environments. However, the strong second-order regularization terms in the ERM loss restrict the diversity of invariant features, thereby limiting the model's ability to capture a broad range of relevant features. 
% This restriction contributes to the phenomenon of \textit{over-invariant} risk.
% \begin{definition}[Over-invariant variable set] Let $S$ be the invariant variable set of $Y$, if there exist a subset $S'$ of $S$ that satisfies 
% \begin{equation}
%     \mathbb{E}_{p^{tr}}[Y|S']=\mathbb{E}_{p^{tr}}[Y|S],\
%     \mathbb{E}_{p^{te}}[Y|S']\neq\mathbb{E}_{p^{te}}[Y|S],
% \end{equation} then $S'$ is the over invariant variable set.
% \end{definition}

 

% \jq{more analysis of figure?}
% In real-world applications, such as medical diagnosis or wildlife recognition, critical but variable features (e.g., rare genetic markers or subtle traits) might be overlooked, leading to poorer generalization in out-of-distribution tasks.

% We assume that the lower strength can be attributed to the following reasons:
% \begin{itemize}
%     \item Over-regularization: Both IRM and VREX rely on strong regularization to enforce invariance, which can sometimes over-constrain the model. As a result, the model may under-learn invariant features that exhibit higher variance, mistaking them for noise or spurious correlations.
%     \item Limited Environmental Diversity: Invariant learning methods assume that all relevant environments are represented in the training data. When environments are limited, the model may struggle to separate invariant features from spurious ones, especially when those features appear inconsistently across the training set.
% \end{itemize}

% \jq{give more discussion: what is the real world meaning of the lower strength? what maybe the reason behind it? yuhang: erm results}

% \jq{var=5, strength, x-axis is the irm/ vrex weight. expect: weight is bigger, the strength is smaller.}






The above two reasons highlight the failure cases of the invariance principle, revealing that rough constraints may inadvertently harm valuable invariant features, a phenomenon we refer to as \textit{over-invariance}. 
Formally, due to the unavailability of test environments $\mathcal{P}^{te}$, such an invariant principle could inadvertently overlook minor invariant characteristics, potentially misinterpreting them as mere hallucinations of spurious features, dubbed as the \textit{over-invariance issue}. 
In particular, we define the over-invariance issue as follows: 

% As shown in Figure \ref{fig:realistic-example}, the horns of male Tibetan antelopes serve as a key invariant feature for distinguishing gender. However, if the training data predominantly depicts male antelopes in a particular background (e.g., yellow grasslands), the model may incorrectly associate the horns with the background, treating them as a spurious feature.


% In practical scenarios, models may erroneously classify invariant features as spurious, particularly when these features frequently co-occur with specific environmental contexts. 
% For example, as shown in Figure~\ref{fig:realistic-example}, the horns of male Tibetan antelopes serve as a key invariant feature for distinguishing gender. However, if most images of male antelopes in the training data appear in a particular background (e.g., yellow grasslands), the model may incorrectly associate the horns with the background, treating them as a spurious feature.



% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=0.45\textwidth]{images/realistic-example.png}
% 	\caption{Comparison of male (left) and female (right) Tibetan antelopes in different environments, where the horns may be perceived as spurious feature.}
% 	\label{fig:realistic-example}
% \end{figure}

% \jq{yuhang: Figure~\ref{x} shows the xxxxx. give a picture to illustrate the over-invaraince phe}
% In practical applications of the invariant principle, the model's sensitivity to invariant features with varying variances is inconsistent; it tends to prioritize learning invariant features characterized by smaller and more stable variances. 
% We will demonstrate this in \ref{Simulation Results}.

% \jq{simulation experiments}

% \jq{- give the data generation process, single paragraph}

% \jq{- result1: visualization of features (sub-features);}

% \jq{- result2: Strength and spurious variance ( only irm/scl loss, y is a small/large invariant feature strength);} 

% \subsubsection{Simulation Results} \label{Simulation Results}
\begin{definition}[Over-Invariant feature] Let $Z^c$ be the invariant feature of $Y$, if there exist a subset $O^c$ of $Z^c$ that satisfies 
\begin{equation}
    \mathbb{E}_{p^{tr}}[Y|O^c]=\mathbb{E}_{p^{tr}}[Y|Z^c],\
    \mathbb{E}_{p^{te}}[Y|O^c]\neq\mathbb{E}_{p^{te}}[Y|Z^c],
\end{equation} 
then $O^c$ is the over-invariant feature.
\end{definition}

An over-invariant feature is the subset of the invariant feature (\( O^c \subset Z^c \)) that performs similarly in the train environments. 
While \( O^c \) maintains predictive accuracy for \( Y \) in the training distribution $\mathcal{P}^{tr}$, it only contains part information of $Y$, may leading to poor OOD performance in $\mathcal{P}^{te}$.


\subsection{Theoretical Analysis}


Since distinguishing between invariant and spurious information in the hidden feature space is challenging in real-world scenarios, we create a synthetic dataset to simulate various distributions, allowing us to further observe the existence of the over-invariance.

% \paragraph{Data Generation} 
\begin{definition}[Data Generation]
    Given the data $(\mathbf{x}, y, y_{s})$, $y$ is the label and $y_{s}$ is the environment, $y$ is uniformly sampled from $\{-1, 1\}$ and $y_s=Rad(s) \times y$ where $Rad(s)$ is a random variable taking value $-1$ with with probability $s$ and $1$ with with probability $1-s$. 
% \jq{@yuhang, the relation between $y$ and $y_s$}
The data $\mathbf{x} \in \mathbb{R}^{d}$ is composed of two components: the invariant feature $x_c$ and the spurious feature $x_s$, where $x_c \in \mathbb{R}^{d_c}$, $x_s \in \mathbb{R}^{d_s}$, and $d = d_c + d_s$.
Each sample $\mathbf{x}$ is generated as follows: 
\[
    \mathbf{x} = \{x_c, x_s\} \in \mathbb{R}^d, \, \text{where} \\
    \begin{cases} 
        x_c \sim N(\mu_c y, \sigma_c^2),  \\ 
        x_s \sim N(\mu_s y_{s}, \sigma_s^2),
    \end{cases} 
\]
Here, $\mu_c \in \mathbb{R}^{d_c}$ and $\mu_s \in \mathbb{R}^{d_s}$ represent the mean of the Gaussian distributions. 
$\sigma_c \in \mathbb{R}^{d_c \times d_c}$ and $\sigma_s \in \mathbb{R}^{d_s \times d_s}$ denotes the standard deviations that control the variability.
\end{definition}

To analyze the preferences of the invariant learning for different components of invariant features, we quantify the \textit{strength} of the subset of features, by masking the irrelevant data as 0 and measuring their $l_2$ norms of the learned representation. Intuitively, this measures how much information the model extracts from the specified dimensions collectively.
\begin{definition}[Strength]
    Given a subset of dimensions $\{m, m+1, \dots, n\}$, we mask all other dimensions of $\mathbf{x}$ as $0$ and pass the masked data $\mathbf{x}_{m:n}$ through the featurizer. 
    Let $\Phi^*$ be the representation function learned by the invariant learning.
    The strength of the selected feature subset is as follows:
    \begin{equation}
    \text{strength}(\mathbf{x}_{m:n}) = \|\Phi^*(\mathbf{x}_{m:n})\|_2,
\end{equation}
\end{definition}

\begin{figure}[t]
	\centering
 \includegraphics[width=0.68\textwidth]{images/over_invariance.jpg}
	\caption{
 % The over-invariance issue occurs in IRMv1 and VREx.
 % on different variances of invariant features. 
 \tmlr{The strength on different subsets of the invariant feature, lower strength means less preference, verifying the existence of over-invariance issue.}
  The X-axis represents the logarithm of spurious variance $\sigma_s$.
  The Y-axis shows the strength of the corresponding subset of invariant features $\Phi(x)$ under varying invariant variances $\sigma_c = \{0.1, 1, 3, 5\}$.
  For each configuration, we run 10 different seeds and report the average results.
 % corresponding dimensions with $\sigma_c$. 
	}
	\label{fig:over_invariance}
\end{figure}

In this paper, we set the dimensions of both the invariant and spurious features to $d_c = d_s = 8$. 
Let $\mathbf{1}_n$ be the all-one vector of length $n$ and \textbf{diag} be the diagonal matrix. 
For the invariant feature $x_c$, we set $\mu_c=10*\mathbf{1}_{d_c}$ and $\sigma_c=\textbf{diag}(5,5,3,3,1,1,0.1,0.1)$, where different variances represent different important levels of invariance where high variance means more important.
For the spurious features $x_s$, we set $\mu_s=10*\mathbf{1}_{d_s}$ and uniformly sample $\sigma_s$ from the range $[10^{-3}, 10^{0.5}]$ simulating noisy environments.
We set $s=0.3$ in the train set and $s=0.7$ in the test set, representing the OOD environments.
% \jq{reason for this setting}
% $\sigma_0 = \sigma_1 = 5$, $\sigma_2 = \sigma_3 = 3$, $\sigma_4 = \sigma_5 = 1$, and $\sigma_6 = \sigma_7 = 0.1$  
% \jq{@yuhang we set $\mu_c = $, $\mu_s =$, $\sigma_c=$, $\sigma_s=$.}
% $\mu_l = \mu_k = 10$ for all $l \in [1, d_i]$ and $k \in [d_i+1, d_i + d_s]$. 
% \jq{ what does the \( \mu, \sigma\) means?
% \paragraph{Implementation}
% \jq{ $\mu,\sigma$ is not all the same ?}
We train a two-layer perceptron network featurizer $\Phi: x \rightarrow \mathbb{R}^l$ where each layer consists of a linear transformation and a ReLU activation and a one-layer linear classifier predictor $w$.
% as the model $f= w \circ \Phi $.
% , where the featurizer is implemented as a two-layer perceptron network and the classifier is one-layer linear model. 
% The featurizer maps the input data $\mathbf{x} \in \mathbb{R}^d$ into a feature space of dimension $p$. 
% The classifier then takes the $p$-dimensional feature vector as input, and the output of the classifier is the probabilities corresponding to the predicted likelihood. 
% that the label $y$ will be $-1$ or $+1$. 
% Formally, the model $f_\Theta$ is represented as 
% \begin{equation}
%     f_\Theta(\mathbf{x}) = W \cdot \phi(\mathbf{x}) + \mathbf{b},
% \end{equation} 
% where $\phi: \mathbb{R}^d \rightarrow \mathbb{R}^p$ is the featurizer, $W \in \mathbb{R}^{2 \times p}$ represents the classifier weights, and $\mathbf{b} \in \mathbb{R}^2$ is the bias term.
We take the classic invariant learning methods IRMv1~\citep{irmv1} and VREx~\citep{v-rex} for example.
% Specifically, given a subset of dimensions $\{d_m, d_m+1, \dots, d_n\}$, we mask all other dimensions of $\mathbf{x}$ as $0$ and pass the masked data $\mathbf{x}_{m:n}$ through the featurizer. 
% The $l_2$ norm of the feature in the embedding space is defined as the strength of the selected feature subset: 

% Formally, the model is represented as the following:
% \begin{equation}
%     f_\Theta(\mathbf{x}) = W \cdot \phi(\mathbf{x}) + \mathbf{b},
% \end{equation}
% where $\phi: \mathbb{R}^d \rightarrow \mathbb{R}^p$ is the featurizer, $W \in \mathbb{R}^{2 \times p}$ represents the classifier weights, and $\mathbf{b} \in \mathbb{R}^2$ is the bias term.
% \jq{what does $g$ and b means, pls explain them here.}
% \paragraph{Simulation Results.} 
% \jq{we want to show the over-invariant phenomenon of previous irm series methods here.}
% We employ IRM\cite{irmv1} and VREx\cite{v-rex} for our experiments. 



Figure \ref{fig:over_invariance} 
% \cqg{Move the corresponding image to the same page} 
illustrates the strengths inside the invariant features. We separate the invariant data $x_c$ into 4 parts based on different variances $\{0.1,1,3,5\}$ and calculate their strengths varying with different spurious variances $\sigma_s$ simulating the noisy environments. 
The results indicate that while all of the features are invariant, their strengths vary.
IL methods are selective to invariant features with some invariant features being learned less effectively than others. 
The lower strength of the subset of the invariant feature suggests that IRMv1 and VREx may struggle with key parts of invariant features, leading to the over-invariance issue. 
% \cqg{It is hard to understand the image}
Formally, we give the following informal proposition to further illustrate the over-invariance: 
\begin{remark}[Over-invariance issue]
% Define the linear model as $f_{\mathbf{\Theta}}=\mathbf{Wx}+\mathbf{b}$, where $\mathbf{\Theta} $ as the concatenated parameter $[\mathbf{W}\ \mathbf{b}]$.
% Let \( \mathbf{\Theta}^{*} = [\mathbf{W}^{*}, \mathbf{b}^{*}] \) be the minimizer of the invariant loss \( L_{il} \) as follows: 
% \[\mathbf{\Theta}^{*} \in \arg\min_{\mathbf{\Theta}} L_{il}(\mathbf{\Theta}).\]
Our synthetic experiment indicates that, with high probability, \tmlr{there exists a subset of the invariant data \( x_c \), denoted as the over-invariant data \( o_c \), where the strength of the remaining invariant data \( (x_c \backslash o_c) \) can vary significantly, potentially approaching zero in extreme cases: 
\[\text{strength} (x_c \backslash o_c) > \text{strength} (o_c) \rightarrow 0 .\] }
Thus, \textbf{over-invariance} may occur at test time.

\end{remark}

\paragraph{\tmlr{Over-invariance issue in the real datasets.}}
\tmlr{We give more visualization examples and discuss them in the interpretation architectures XGNN as shown in Appendix~\ref{sec:interpret_visualize_appdx}. We have two observations based on the above visualization examples. 1) Invariant learning methods tend to extract the subgraph of the ground truth invariant subgraph, thus verifying the over-invariance issue. 2) When confronted with multiple invariant subgraphs, these learning methods usually focus on only one of them rather than considering all the invariant subgraphs. 
The above two observations aligned with our synthetics experiments in Figure~\ref{fig:over_invariance} measured by the strength, thus further verifying the occurrence of the over-invariance issue.
}