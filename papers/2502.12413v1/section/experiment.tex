
\section{Experiments}


% \jq{graph more baseline: (gin, gcn, xgnn) erm, irm, vrex, ciga, refer to ciga}

% \jq{cv datasets: cmnist, gray cmnist, domainbed; baseline: erm, irm, vrex, scl }

% \jq{ablation study: 1) without random mask, without projector, without ucl,  how's it going. 2) different data augmentation methods}
We evaluate \ours and compare with IL methods on a range of tasks requiring OOD generalization. 
\ours provides generalization benefits and outperforms IL methods on a wide range of tasks, including: 1) graph datasets such as the synthetic Spurious-Motif and drug discovery, 2) Colored MNIST (CMNIST) dataset, and 3) natural language datasets.  
% \zyh{update the order of the experiments}

% \subsection{ColoredMNIST}
% \zzx{need to be done, choose the appropriate settings and then add the contrastive loss as \ours}
% \paragraph{Datasets.} 
% Table~\ref{tab:cmnist} shows the performance of \ours on the synthetic datasets ColoredMNIST following \cite{irmv1}.
% We compare \ours with ERM, and various IL methods, including causal methods that focus on learning invariance (IRM, VREx) and gradient matching techniques (Fishr) using the same backbones.
% % As previously done in Fishr, we maintain all IL method implementations identical to the IRM implementation, notably the same MLP and hyperparameters, and just add the \ours penalty to the loss.
% We use two-stage scheduling selected in IRM for the regularization strength $\lambda$, which is low until epoch 190 and then jumps to a large value. Due to the varying degrees of over-invariance introduced by different IL methods, we performed a simple search over $\beta$ values of \{0.01, 0.05, 0.1, 0.2\}, and projection mask probabilities of \{0.3, 0.5, 0.7\}. 
% Adding \ours can achieve the best trade-off between train and test accuracies, notably in test. It reaches 69.25\% in the colored test set and 70.43\% when digits are grayscale. 
% In addition, \ours improves the performance in all IL methods, verifying our understanding of the issue of overinvariance.
% Table~\ref{tab:nli} shows the results of \ours on the NLP task and Table~\ref{tab:main_table} and~\ref{tab:spmotif} show the performance on the graph.
% More discussion about experiments are shown in
% Appendix~\ref{sec: exp cmnist},~\ref{sec: nlp exp}, and~\ref{sec: graph exp}.


% \input{tables/spmotif}

\subsection{Experiments on Graph}
\label{sec: graph exp}
% \jq{todo: add the ablation on da methods}


\paragraph{Datasets} 
We employed one synthetic dataset along with eight realistic datasets, including the Spurious-Motif datasets introduced in \cite{dir}. 
These datasets consist of three graph classes, each containing a designated subgraph as the ground-truth explanation. Additionally, there are spurious correlations between the remaining graph components and the labels in the training data, challenging the model's ability to differentiate between true and spurious features. 
The degree of these correlations is controlled by the parameter \( b \), with values of \( 0.33, 0.6, \) and \( 0.9 \). 
Furthermore, to examine our method in real-world scenarios characterized by more complex relationships and distribution shifts, we incorporated the DrugOOD dataset~\citep{drugood} from AI-aided Drug Discovery, which includes Assay, Scaffold, and Size splits from the EC50 category (denoted as EC50-*) and the Ki category (denoted as Ki-*). 
Additionally, we included tests on the CMNIST-sp dataset, which consists of superpixel graphs derived from the ColoredMNIST dataset using the algorithm from \cite{understand_att}, featuring distribution shifts in node attributes and graph size. We also tested on the sentiment analysis dataset Graph-SST2~\citep{graph-sst2}, which is formed by converting each text sequence from SST2 into a graph representation. 
% More details can be found in Appendix x.x

% Additionally, we converted the ColoredMNIST dataset from IRM [4] using Knyazev et al.'s algorithm [46] to introduce attribute shifts, and we split the Graph-SST dataset [122] to inject degree biases.

% For experiments, we employed one synthetic and eight realistic datasets, one comprising comprehensive topological structure information and the other focusing on the challenging real-world task of AI-aided drug affinity prediction.
% \begin{itemize}
%     \item \textbf{Spurious-Motif} is a synthetic dataset with three graph classes proposed by \cite{dir}. Each class contains a particular subgraph that can be regarded as the ground-truth explanation. Some spurious correlation between the rest graph components (other than the motifs) and the labels also exists in the training data. The degree of such correlation is controlled by b, and we include datasets with b = 0.33, 0.6 and 0.9.
%     \item \textbf{DrugOOD} is a systematic OOD benchmark for AI-aided drug discovery proposed by \cite{drugood}. The datasets focus on the task
%     of drug targeted binding affinity prediction for both macro-molecule and small-molecule. The DrugOOD datasets include splits using Assay, Scaffold, and Size from the EC50 category (denoted as EC50-*) and the Ki category (denoted as Ki-*).
%     \item \textbf{CMNIST-sp} is a graph classification dataset that contains distribution shifts in node attributes and graph size. Each image in ColoredMNIST is converted to a superpixel graph using the algorithm from \cite{understand_att}. Nodes with nonzero pixel values provide ground-truth explanations.
%     \item \textbf{Graph-SST2} is a sentiment analysis dataset, where each text sequence in SST2 is converted to a graph. Following the splits in \cite{dir}, this dataset contains degree shifts and no ground-truth explanation labels.
% \end{itemize}


\paragraph{Baselines} We compared \ours with causality-inspired invariant graph learning methods, such as IRM~\cite{irmv1}, v-Rex~\cite{v-rex}, and IB-IRM~\cite{ib-irm}. Additionally, we evaluated \ours against methods like EIIL~\cite{eiil}, CNC, CNCP~\cite{cnc}, and CIGA~\cite{ciga}, all of which do not require environment labels. Notably, CNC, CNCP, and CIGA employ contrastive sampling strategies to address the OOD problem. We implemented $\mathcal{L}_{il}$ following the SOTA method CIGA, and for $\mathcal{L}_{ucl}$, we selected the best-performing DA techniques, such as edge removal, node dropping, and subgraph extraction, based on~\cite{graphCL}.
% Additionally, we conducted a comprehensive evaluation of \ours against Vanilla GNN and other SOTA interpretable GNN invariant learning methods, including GIB \cite{gib} and DIR \cite{dir}, to assess the effectiveness of the optimization objective in \ours. 
% We also contrasted our method with SOTA out-of-distribution (OOD) objectives, such as IRM \cite{irmv1}, v-Rex \cite{v-rex}, and IB-IRM \cite{ib-irm}, using random environment partitions as described in [23]. 
We report classification accuracy for the Spurious-Motif, CMNIST-sp, and Graph-SST2 datasets, and ROC-AUC for the DrugOOD datasets. 
The evaluation is conducted five times with different random seeds (\{1, 2, 3, 4, 5\}), selecting models based on validation performance. 
We utilized the GCN backbone~\cite{gcn} with sum pooling to enhance across all experiments.
% Finally, we present the mean and standard deviation for each corresponding metric.

% More implementation details can be found in Appendix x.x.

% \zzx{wired result, maybe is not reliable. compared with ciga's baseline, the erm, dir, irm and some other results had a big difference on cmnist-sp dataset}

% The results are demonstrated in Table \ref{tab:main_table}. 
\input{tables/drugood}
\begin{figure}
\begin{minipage}{0.48\textwidth}
  \centering
  
  \includegraphics[width=0.98\textwidth]{images/abla-da-graph.pdf}
  
  \caption{Ablation study of different graph data augmentations in \ours. (a) compares the performance of subgraph extraction, edge removing, and node dropping on the ec-* category from DrugOOD datasets. (b) illustrates the performance of edge removing and node dropping on SPMotif with different OOD shift biases.}
  \label{fig: da graph}
\end{minipage}
\hfill
\begin{minipage}{0.5\textwidth}
  \centering
  \captionof{table}{Performance on Spurious-Motif dataset with different GNN backbones. \tmlr{\ours can enhance the performance on various architectures.}
    The blue, gray, and $\underline{\text{Underline}}$ highlight the first, second, and third best results, respectively. 
    All results are reported with mean $\pm$ std across seeds $\{0,1,2,3,4\}$.
    }
    \resizebox{0.95\textwidth}{!}{%
     \label{tab:spmotif}
   \begin{tabular}{lccc}
  
            \hline
            & SPMotif-0.33 & SPMotif-0.60  & SPMotif-0.90   \\
            \hline
            \rowcolor{gray!8}\multicolumn{4}{c}{GNN} \\ 
            \hline
            GIN                                                   &
            $47.8 \pm 8.03$                                      & $49.21 \pm 4.2$                 & $44.11 \pm 5.5$
            \\
            +$\mathcal{L}_{il}$                                             &
            $50.67 \pm 3.83$                                       & $50.39 \pm 2.19$                  & $42.35 \pm 6.36$
            \\
            +\ours                                            &
            $51.48 \pm 5.43$                                       & $51.77 \pm 4.89$                  & $45.84 \pm 3.82$
            \\
             \hline
            GCN                                                & $58.51 \pm 2.84$                                & $50.51 \pm 4.75$                                   & $44.67\pm 3.5$\\
            +$\mathcal{L}_{il}$                                             & \cellcolor{secondbest}$67.53\pm1.35$
            &$59.96 \pm 8.59$                                       & $47.66 \pm 6.08$\\
            +\ours                                         & \cellcolor{best}$67.81\pm3.33$
            &$\underline{62.79 \pm 3.86} $                                       & $50.93 \pm 9.05$\\
             \hline
              \rowcolor{gray!8}\multicolumn{4}{c}{XGNN} \\ 
            \hline
            +$\mathcal{L}_{il}$(GIN)                                                  &
            $57.58 \pm 3.73$                                      & $58.11 \pm 4.29$                 & $52.14 \pm 3.27$
            \\
            
            +\ours(GIN)                                          &
            $\underline{63.86 \pm 2.19}$                      & $60.31 \pm 2.04$ & $\underline{52.54\pm 6.57} $\\
            \hline
            +$\mathcal{L}_{il}$(GCN)                                                   &
            $63.37 \pm 4.27$                                      & \cellcolor{secondbest}$65.45 \pm 4.91$                 & \cellcolor{secondbest}$59.64 \pm 4.64$
            \\

            +\ours(GCN)                                           &
             $63.90 \pm 3.7$                                       & \cellcolor{best}$70.03 \pm 3.66$                  & $ \cellcolor{best}66.85\pm 6.61$
            \\

            \hline
           
        \end{tabular}%
        }
\end{minipage}
\end{figure}

\paragraph{\ours outperforms previous IL methods.}
As demonstrated in Table \ref{tab:main_table}, \ours shows better generalization ability than all baseline models on real-world datasets. 
Specifically, in the MNIST-sp dataset, \ours surpasses CIGA by 5\%. 
Furthermore, in the ki-scaffold and ki-assay datasets, CIGA performs worse than ERM, while \ours by implementing the $\mathcal{L}_{il}$ on CIGA achieves higher performance.
The results not only highlight the competitive edge of \ours over established baselines but also emphasize its generalization across varying datasets. 
Such nuanced performance differentials underscore our capabilities of \ours in navigating complex real-world datasets, positioning the over-invariance issues as a crucial problem.

% Table~\ref{tab:spmotif} shows that 
% For the DrugOOD, CMNIST-sp, and Graph-SST2 datasets, we utilized the GNN backbone GCN \cite{gcn}, while employing both GCN and GIN \cite{gin} with sum pooling to enhance expressive power for the Spurious-Motif datasets. 
\paragraph{\ours shows effectiveness on various backbones.}
Additionally, we incorporate XGNN, an interpretable GNN to extract the invariant subgraph \( G_c \) commonly used in graph OOD models~\citep{dir, gil,ciga}. 
Specifically, a XGNN $w_x \circ \Phi_{x}$ is with an extractor $\Phi_x:\gG\rightarrow \gG $ that identifies an invariant subgraph $G_c$ to help predict their labels $y_x=w_x(G_c)$ with a downstream classifier $w: \gG \rightarrow\gY$.
Table \ref{tab:spmotif} shows that \ours significantly outperforms Vanilla GNN and IL (we implement the IL methods with one of the SOTA graph IL methods CIGA~\citep{ciga}) on Spurious-Motif under various backbones like GCN, GIN, and XGNN settings. 
Moreover, as the spurious bias increases, the performance of \ours remains more stable, while the baselines and IL models tend to fail, 
like in the SPMotif-0.60 dataset \ours improves performance from 65.45\% to 70.03\% and in the SPMotif-0.90 dataset from 59.64\% to 66.85\%.

\paragraph{Sensitivity on different graph data augmentations.}
Figure~\ref{fig: da graph}(a) illustrates that different random augmentation methods, such as subgraph extraction, edge removing, and node dropping~\citep{graphCL}, yield similar performance in addressing graph out-of-distribution (OOD) challenges, echoing observations found in recent studies~\citep{GCL_arche}. 
Additionally, in Figure~\ref{fig: da graph}(b), the comparison between edge remoing and node dropping methods in SPMotif under varying shift biases reveals a slight advantage of edge removing over node dropping at $b=0.33$ and $b=0.6$. However, at $b=0.9$, node dropping surpasses edge removal, although the difference remains modest. This observation supports our insight that the data augmentation strategies employed may not significantly influence the graph OOD problems.


\subsection{Experiments on CMNIST}
\label{sec: exp cmnist}
We evaluate \ours on the synthetic datasets ColoredMNIST following \cite{irmv1}.
We compare \ours with ERM, and various IL methods, including causal methods that focus on learning invariance (IRM, VREx) and gradient matching techniques (Fishr).
As previously done in Fishr, we maintain all IL method implementations identical to the IRM implementation, notably the same MLP and hyperparameters, and just add the \ours penalty to the loss.
We use two-stage scheduling selected in IRM for the regularization strength $\lambda$, which is low until epoch 190 and then jumps to a large value. Due to the varying degrees of over-invariance introduced by different IL methods, we performed a simple search over $\beta$ values of \{0.01, 0.05, 0.1, 0.2\}, and mask probabilities $p$ of \{0.3, 0.5, 0.7\}.

% \jq{@zhixiong, how to choose $\beta$}

Table~\ref{tab:cmnist} reports the accuracy averaged over 5 runs with standard deviation. 
Adding \ours can achieve the trade-off between train and test accuracies, notably in test set. It reaches 69.25\% in the colored test set and 70.43\% when digits are grayscale. 
In addition, \ours improves the performance in all IL methods, verifying our understanding of the issue of over-invariance.
Figure~\ref{fig:cv-nlp-abla}(a) displays the results of \ours using different invariant losses across various mask percentages $p$, demonstrating the robustness of \ours to the hyperparameter $p$ with minimal variance in accuracy across different $p$ values.
Figure~\ref{fig:cv-nlp-abla}(b) illustrates that increasing the weight $\beta$ of the $\mathcal{L}_{ucl}$ term leads to improved performance in IRMv1, VREx, and Fishr. 
This supports our insight that over-invariance issues exist in current incremental learning (IL) methods. 
By introducing diversity penalties $\mathcal{L}_{ucl}$, we can mitigate this issue and enhance out-of-distribution (OOD) performance.
\input{tables/cmnist_test}

\subsection{Experiments on Natural Language Inference}
\label{sec: nlp exp}
\input{tables/NLI}
% \input{tables/NLI}
Inspired by \citet{qin2024large}, we also demonstrated the effectiveness of our method in NLP through a Natural Language Inference (NLI)~\citep{NLI} task, which assesses the logical relationship between two sentences: entailment, contradiction, or neutrality. 
Our model was trained on a subset of the SNLI~\citep{SNLI} training set and evaluated on selected cases from the SNLI validation set, as well as the match and mismatch subsets of the MNLI~\citep{MNLI} validation set. 
While SNLI represents an in-distribution (ID) scenario, MNLI helps assess the generalization to out-of-distribution (OOD) data. 
The results show that our method performs well in both IID and OOD scenarios, validating its effectiveness.

In our experiment, we employed a pretrained GPT-2 model with a randomly initialized classification head. We set the maximum token length to 64 and trained the model for 5 epochs using the AdamW optimizer. The learning rate was configured at 2e-5, with a weight decay of 0.01 and a linear learning rate scheduler. We used a training batch size of 32.
To optimize our model, we implemented supervised contrastive loss as \( \mathcal{L}_{il} \) following~\cite{cnc} and explored various combinations of weights for \( \lambda \) and \( \beta \), choosing values from the set \{0, 0.1, 0.3, 0.5, 0.7, 1.0\}. Additionally, we fixed the projection mask probability at 0.7 and reported the results for the best-performing configuration.

Table~\ref{tab:nli} shows the results of \ours on the NLI task, where \ours outperforms both IRM and ERM approaches on real-world natural language datasets. 
Furthermore, our ablation study reveals that removing either $\mathcal{L}_{il}$ or $\mathcal{L}_{ucl}$ leads to a decrease in OOD performance.
However, the performance remains better than that of IRM or ERM, indicating that \ours achieves a trade-off between strong regularization and feature diversity.  
Figure~\ref{fig:cv-nlp-abla}(c) illustrates the performance of \ours with varying weights, denoted as $\beta$, for the loss function $\mathcal{L}_{ucl}$ across the SNLI, MNLI-match, and MNLI-mismatch datasets. Unlike the findings in CMNIST, increasing $\beta$ does not necessarily improve out-of-distribution (OOD) performance. 
It’s essential to choose an appropriate weight, possibly due to the unique structure of natural language.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.94\linewidth]{images/cv-nlp-abla.pdf}
    \caption{Ablation study of \ours on CMNIST and NLI datasets. (a) illustrates the performance of \ours on various mask percent $p$ across different implementation of $\mathcal{L}_{il}$. (b) and (c) illustrates the performance of \ours on different weight of UCL $\beta$ in CMNIST and NLI, respectively.}
    \label{fig:cv-nlp-abla}
    \vspace{-10pt}
\end{figure}


% % We thoroughly compare \ours with Vallina GNN and Causality Inspired Invariant Graph Learning ( e.g. CIGA \cite{ciga} ).
% We adopted GNN backbone GCN \cite{gcn} and GIN \cite{gin}, selecting the sum pooling for better expressive power. 
% % In addition to the XGNN which is an interpretable GNN used in many graph OOD models to extract the invariant subgraph $G_c$~\cite{dir,gil,ciga}, we also include standard GNN to demonstrate our discussion in Sec \ref{sec: gil understand} and answer RQ3.
% In addition to the XGNN which is an interpretable GNN used in many graph OOD models to extract the invariant subgraph $G_c$~\cite{dir,gil,ciga}, we also include standard GNN to demonstrate our discussion in answer RQ3.
% The results are demonstrated in Table \ref{tab:main_table} and \ref{tab:spmotif}. 
% We describe more details in appendix.




% \input{iclr2024/tables/oversmoothing}
% \ref{tab:main_table}

% dataset
% baseline ( related baseline)
% setting: para
% results



% , representation distribution matching methods (MMD, CORAL), and other variants (Mixup, MLDG).
% 这一段可以参考 domainbed 的论文来写 https://arxiv.org/pdf/2007.01434
% \paragraph{Settings.} 
% In line with the DomainBed setting, we fine-tune the pretrained ResNet-50 \cite{resnet} across all datasets. However, for Rotated MNIST and Colored MNIST, we opt for a smaller CNN architecture instead. To ensure a fair comparison, the methods are evaluated under identical conditions, including limiting hyperparameter configurations, using fixed backbone options, and applying consistent data augmentation techniques. 
% Due to limited computational resources and the large search space, we restrict the random search experiments for hyperparameters to $x$ trials in order to optimize hyperparameters for each domain. 
% \paragraph{Evaluation.} After carefully examining the model selection criteria proposed by \cite{domainbed}, we adopt $criteria$ for our experiments. \zzx{[which and why ?]} For the optimal parameters obtained through the sweep for each domain, we report the average and standard error of the results from $y$ runs with different random initializations.


% \paragraph{Results.}
% 需要确定实验的setting，和评估策略：
    % - training-domain validation (all training models are pooled and a fraction of each of them is used as validation set)
    % - leave-one domain-out cross-validation (cross validation is performed using a different domain as validation, and the best models is retrained on all training domains)
    % - test domain validation set (a fraction of the test domain is used as validation set)

% Our evaluation uses a leave-one-domain-out approach, where each model is trained on all domains except one and tested on the excluded domain. The final model is selected based on its combined accuracy across all validation sets for the training domains.


% We validated our method on two synthetic datasets built on the MNIST handwritten digit classification dataset: , following the Domainbed setup. 
% \begin{itemize} 
%     \item \textbf{ColoredMNIST} contains 3 domains: \{+90\%, +80\%, --90\%\} with two labels. The percentages indicate the degree of correlation between color and label, with this correlation strength varying across different domains. The dataset comprises 70,000 images with a resolution of 2×28×28 and has 2 classes. 
%     \item \textbf{RotatedMNIST} consists of 6 domains: \{0, 15, 30, 45, 60, 75\} created by 15\% rotations ranging from 0 to 90 degrees. It includes 70,000 images with a resolution of 1×28×28 and has 10 classes. 
% \end{itemize}

% 这里有两个实验，感觉不太好说清楚逻辑诶
% \paragraph{Baselines.} We conducted comparative tests by applying \ours to ERM and traditional invariant learning methods such as IRM and VREx. We used ResNet as the backbone network, and the results are presented in Tables X and Y. More details can be found in the appendix.

% First, on the Colored MNIST dataset, we tested whether the model learned to ignore color by reversing this correlation during testing. In brief, if the model solely relies on color, it would achieve an accuracy of 10\% in testing, while a perfect "oracle" model that predicts shape accurately would achieve 75\%. We strictly followed the IRM implementation, applying \ours to existing methods (ERM, IRM, VREx, and Fishr). The multi-layer perceptron (MLP) and existing hyperparameters remained unchanged, and we added a two-layer nonlinear layer as a projection. The \ours loss weight was explored across \{0.01, 0.05, 0.1, 0.2\}, and the projection mask ratio varied over \{30\%, 50\%, 70\%\}, selecting the best results for comparison. All experiments are repeated with $5$ different random seeds of $\{1,2,3,4,5\}$. The mean and standard deviation of the accuracy rate are reported from the $5$ runs. The outcomes are presented in Table \ref{tab:cmnist}.

% Then, we conducted comparative tests by applying \ours to ERM and traditional invariant learning methods such as IRM and VREx. We used ResNet as the backbone network, and the results are presented in Tables X and Y. More details can be found in the appendix.


% dataset: ColorMINIST, grayMINIST, 

