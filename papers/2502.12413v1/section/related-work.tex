\section{Related work}

% \paragraph{Out-of-Distribution (OOD) Generalization.} OOD generalization refers to a model's ability to maintain performance on data drawn from distributions not encountered during training. This challenge is critical in machine learning, as real-world data often diverges significantly from the controlled environments of training datasets. Based on the model's learning pipeline \citep{yang2024generalized}, existing strategies for addressing OOD generalization can be categorized into three main approaches:

% \begin{itemize}
%     \item \textbf{Representation Learning:} These methods focus on learning robust feature representations that generalize across various distributions. Notable techniques include unsupervised domain generalization \citep{mahajan2021domain, zhang2022towards, chen2020improved} and disentangled representations \citep{higgins2017beta, kim2018disentangling, yang2021causalvae}.

%     \item \textbf{Model-based Approaches:} Techniques such as Invariant Learning \citep{risks-irm, ganin2015unsupervised, li2018domain, creager2021environment}, causal learning \citep{peters2016causal, pfister2019invariant}, and stable learning \citep{kuang2018stable, kuang2020stable, shen2020stable} aim to capture invariant relationships across different environments, thereby enhancing robustness to distributional shifts.

%     \item \textbf{Optimization-based Techniques:} These approaches seek to ensure robust worst-case performance under distributional shifts \citep{delage2010distributionally, namkoong2016stochastic, koyama2020out}, thereby safeguarding the model against potential changes within the uncertainty set.
% \end{itemize}

% \jq{todo: modify the related work}

\paragraph{Out-of-Distribution (OOD) Generalization.} 
% Out-of-Distribution (OOD) generalization refers to a model's ability to perform well on data from distributions not seen during training, a critical challenge in machine learning due to the significant differences between real-world data and controlled training environments. 
Existing strategies to tackle OOD generalization can be broadly classified into three approaches \citep{yang2024generalized}. 
Representation learning focuses on developing robust feature representations that generalize across various distributions, including unsupervised domain generalization \citep{mahajan2021domain, zhang2022towards, chen2020improved} and disentangled representations \citep{bengio2013representation, higgins2017beta, kim2018disentangling, yang2021causalvae}. 
Model-based approaches, such as Invariant Learning \citep{risks-irm, ganin2015unsupervised, li2018domain, creager2021environment} and causal learning \citep{peters2016causal, pfister2019invariant}, aim to capture invariant relationships across environments to enhance robustness against distributional shifts. 
Finally, optimization-based techniques seek to ensure strong worst-case performance under potential distributional changes \citep{delage2010distributionally, namkoong2016stochastic,  duchi2021learning, duchi2023distributionally, zhou2022model}, safeguarding models from uncertainties in the data.


% \paragraph{Invariance Learning.}
%  % The Invariance Learning approach tackles the OOD Generalization problem by predicting invariant features within the data. 
%  Recent works ~\citep{muandet2013domain, albuquerque2019adversarial} theoretically analyze that if the representations learned by the invariance principle remain invariant when the domain varies, the representations can achieve the Bayesian optimal on different domains.  
% %  Considering a classification task, Invariant Risk Minimization (IRM) divides the predictor $f$ into two components: an extractor $g$ and a classifier $h$~\citep{irmv1}. Following the assumption of the invariance principle, each input $X^{e}$ is influenced by a set of latent causal variables $X_c$ and spurious variables $X_s^e$, where $X_c$ represents invariant features such that $X_c\perp e$, and $X_s^e$ varies with the environment $e$. The objective of Invariance Learning is to find an extractor $g$ such that $g(X^e)=X_c$ for all $e\in \mathcal{E}_{all}$. The learning objectives for $g$ and $h$ are formulated as:
% % \begin{equation}
% % min_{e\in \mathcal{E}_{tr}\subseteq\mathcal{E}_{all},f_c,g}R^e(\hat{X}_c;Y) \\
% % s.t.\ \hat{X}_c \perp e,\ \hat{X}_c=g(X^e).
% % \end{equation}
% However, learning the invariant features to get rid of the train dataset bias is extremely challenging. 
% Many strategies have been proposed to approximate this independence. Some methods introduce regularization terms or penalties to enforce invariance, such as VREx\citep{v-rex} stabilizes model performance across environments. Other approaches leverage domain adversarial training\citep{ganin2016domain} to align distributions from different environments, thus encouraging invariant representation learning. 
% More recent advancements focus on improving the robustness of these methods by utilizing additional supervision or priors \citep{wang2022toward}.



\paragraph{Discussion on Different Invariant Losses}
% \label{sec: diss il loss}
Many invariant learning methods focus on learning stable features across environments by incorporating penalties into the loss function to ensure the consistent change rate~\citep{irmv1,v-rex, fishr, cnc}.
One series of methods relies on explicit environment labels.
For example, IRMv1~\citep{irmv1} implements the theory of the invariance principle in practice by assuming the classifier as the constant and employing a gradient-based penalty that requires the sum of the gradients of the model to remain small.
The loss function $\mathcal{L}_{irmv1}$ is defined as follows:
\(\mathcal{L}_{irmv1}= \Sigma_{s \in \epsilon_{tr}} \| \triangledown_{w | w=1.0} \mathcal{L}_{pred}^s(f)\|_2\).
VREx~\citep{v-rex} takes the variance of the loss across different environments defined as
\(\mathcal{L}_{vrex}= Var(\mathcal{L}_{pred}^1(f),\mathcal{L}_{pred}^2(f),\cdots,\mathcal{L}_{pred}^k(f))\),
where k is the environment numbers in the training dataset.

There are also plentiful studies in invariant learning without environment labels.
\citet{eiil} proposed a minmax formulation to infer the environment labels.
\citet{hrm} proposed a self-boosting framework based on the estimated invariant and variant features.
\citet{jtt,cnc} proposed to infer labels based the predictions of an ERM trained model.
% However, \citet{zin} found failure cases in Euclidean data where it is impossible to identify the invariant features without given environment labels.
% Moreover, as the OOD generalization on graphs is fundamentally more difficult than Euclidean data~\citep{ciga}, the question about the feasibility of learning invariant subgraphs without environment labels remains unanswered.
 Other methods adopt the loss of supervised contrastive learning as $\mathcal{L}_{il}$, like CNC~\cite{cnc} and CIGA~\cite{ciga}, using different heuristic strategies to choose the positive and negative samples.
 For example, the invariant penalty of CIGA encodes the subgraph $G_c$ and take the learned feature $z_k$ from the same label, treated as the positive pair, and $z_j$ is the from different labels (negative pair).
% is
%\(\mathcal{L}_{ciga}=-\sum_{i=1}^N\log\frac{\exp(-|z_i-z_{k}|^2/2)}{\sum_{y_j\neq y_i}\exp(-|z_i-z_j|^2/2)+\exp(-|z_i-z_k|^2/2)} \),
%where $z_i$ represents the feature learned by the 
\paragraph{Contrastive Learning.} 
% Contrastive learning is a self-supervised technique that learns representations by contrasting positive and negative sample pairs. In the context of OOD generalization, it helps models distinguish between invariant and spurious features by encouraging diversity in the learned representations. By clustering similar features and pushing apart dissimilar ones, contrastive learning prevents feature collapse, even under strong regularization. 
SimCLR\citep{chen2020simple} and MoCo\citep{he2020momentum} demonstrate how contrastive objectives can improve feature robustness and help models generalize better to unseen environments. Additionally, \citep{wen2021toward} and \citep{ji2023power} demonstrate that contrastive learning can effectively extract semantically meaningful features from data. Furthermore, \citep{xue2023features} conducts systematic experiments on contrastive learning, revealing the effectiveness of combining supervised and unsupervised contrastive learning for feature learning.
By clustering similar features and pushing apart dissimilar ones, contrastive learning prevents feature collapse, even under strong regularization~\citep{ciga,cnc}. 



