
% theorem： ucl+scl superior  last theorem 


% simulate experiment 
% setting: 
% results: 
% 1. scl / scl+ucl  invariant  strength
% 2. weak strong （scl / scl+ucl)

% viewpoint from paper: either SCL or UCL is fragile, we should conbine them
% Invariace Learning SCL / SCL & UCL  <- UCL is effective
% spurious feature variance  strength
% ERM IRM VREX SCL
% SCL + UCL 放上去 spurious variance
% ERM IRM 散点图画出来 ERM + UCL 
% Invariance Learning + UCL 
% 横坐标 




\section{\ours: Diverse Invariant Learning}
Built upon our analysis of the pitfall of the invariant feature and the observation about over-invariance issue, we propose a novel approach\oursfull (\ours) that integrates unsupervised contrastive learning (UCL) and the masking mechanism, which can be applied to various IL methods.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.72\linewidth]{images/DA.pdf}
    \caption{Data augmentation of $\mathcal{L}_{ucl}$ in \ours across multi-modals. Left up: random masking the figure to 0. Left down: edge removing and node dropping for the graph. Right: We feed the same input sequence to the encoder twice by applying different dropout masks to obtain the positive pair.}
    \label{fig:da in ours}
\end{figure}
% As the previous section discussed, two key issues hinder the performance in OOD scenarios: 1) the insufficiency of environments, leading to the underrepresentation of critical invariant features, and 2) overly strong regularization suppressing feature diversity, making the model overlook important data details. To tackle these challenges, we propose a novel approach \oursfull (\ours) that integrates unsupervised contrastive learning (UCL) and random masking.
% , promoting invariant feature diversity and mitigating the negative effects of excessive regularization.
% Built upon our observation, we can dissect the problems of previous IL methods for OOD generalization, that is, the over-invariance issue.   
% In this section, we propose a simple yet effective method, \ours, to address the challenges. 

% In Section~\ref{subsec: div}, we introduce our \oursfull, which leverages contrastive learning to improve OOD generalization. 
% In Section~\ref{subsec: theory of divl}, we provide a theoretical analysis of the effectiveness of \ours in enhancing model robustness and resolving the limitations of invariant risk minimization.

% \jq{2 principles for invariant principle 1) alleviate spurious features 2)maximum invariant features}

% we propose the novel invariant principle to mitigate the over-invariant risk which more adaptable to real-world scenarios.

% First, alleviate the spurious features caused by the environments.

% Second, maximum the invariant features.


% \jq{theorem: capture the failure of invariance principle}
% \jq{here we should gradually introduce contrastive learning}
% \jq{theorem: the superiority of our new invariance principle}

% \jq{introduce contrastive learning to our story, key idea is to introduce the UCL. look at the ucl theorem in the paper and adjust it here.}

% \jq{exp: simulation results.}

% \subsection{Giving the over-invaraince definition}
% give the assumption

% \begin{assumption}[Complementary Features]
%     Assume that the sample $X^e\in \mathbb{R}^d$ can be divided into the invariant subset $X_i$ and spurious subset $X_s^e$, where $X_i\cap X_s^e=\emptyset$ and $X_i\cup X_s^e=X^e$, which satisfies $X_i \perp X_s | (Y,E)$.
% \end{assumption}

% \begin{assumption} Suppose the test distribution $P^{te}$ differs from the training distribution $P^{tr}$ in covariate shift only, i.e.,
% \begin{equation}
%     P^{te}(X,Y) =P^{te}(X)P^{tr}(Y|X)
% \end{equation}
% \end{assumption}

% define over-invariance 


% \subsection{problem setup}
% \subsection{main results}
% invariance methods: irm, vrex, cl (label),fishr,
% \begin{theorem}[UCL] Let $n$ be the number of samples, $m$ be the number of augmentations and $K$ be the number of features. Assume $p \le d$. Let L be the  $(K+1)$-element tuple $[1, \phi^2_1, \phi^2_2, \frac{\phi^2_3}{K-2},...,\frac{\phi^2_3}{K-2}]$ whose last $K$ elements are the variances of features. If $\phi^2_1$ is not among the $p$ largest elements in $L$, then with probability at least $1-O(\frac{m^2n^2}{d})=1-o(1)$: (1) there exists a global minimizer $\mathbf{\theta}^*$ of $\mathcal{L}_{UCL}$ such that $||\mathbf{W^*v_1}||=\Omega(1)$, (2) However, the minimum norm minimizer $\mathbf{\theta}^{**}$ satisfies $||\mathbf{W^{**}v_1}||=0$.
% \end{theorem}


% \zyh{Does the logic really consistant? If we want to emphasize the effectiveness of UCL, why should we say UCL can cause feature suppression here?}



% \subsection{Diverse Invariant Learning}
% \label{subsec: div}
% \jq{give the math expression of every operation.}


% \subsection{Alleviate Over-invariance via Unsupervised Contrastive Learning}

\paragraph{Enhancing the Environments by Random Data Augmentation} As discussed in Section~\ref{subsec: reason of il}, one fundamental assumption of the invariant principle is the requirement for an infinite number of environments, which is impractical in real-world. 
% This scarcity of environments reduces the ability to properly learn the invariance between features and labels. 
To compensate for this limitation, we employ data augmentation to produce a wider range of samples. 
By introducing variations of data, we disrupt the spurious correlation between the labels and the environments from the train data, fostering the creation of diverse environments. 
We only use random data augmentation without careful designs which is enough to show the benefits. 
% rather than overly strong augmentations may lead to unnecessary feature collapse, so 
As illustrated in Figure~\ref{fig:da in ours}, 
% ensuring the semantic content of the data remains intact
% \jq{@zhixiong, what are the DA methods for cv and nlp? content and cite}
we use edge dropping, node dropping, and random subgraph extraction for graph following \citet{graphCL} and \citet{graph_aug}; randomly masking the data $x^{n\times n \times 3}$ to zero with a probability of p for CV, 
and obtaining $z'_i, z_i$ with dropout masks on fully-connected layers as well as attention with a probability of p for NLP ~\citep{Vaswani2017AttentionIA,gao2021simcse}. 
% Specifically, for NLP, we adopt the SimCSE-based augmentation method, where the same sentence is passed through a model twice, resulting in two distinct representations of the same input.

% Our experiments show that these simple augmentations can effectively enhance OOD generalization by preserving feature diversity.



\paragraph{Alleviating the Restriction by Unsupervised Contrastive Learning} 
To alleviate the spurious correlations, IL methods usually add strong penalties to the loss~\citep{irmv1,v-rex,fishr} which tends to suppress subtle yet important details, thus causing the over-invariance issues.
Unsupervised learning (UL), particularly unsupervised contrastive learning (UCL)~\citep{oord2018representation}, provides a powerful mechanism for addressing this by learning the sample-level features, promoting the focus on the minor details of the invariance~\citep{features_CL,DC_leCun,chen2020simple,qin-etal-2022-gl,zhang2022towards}. 
% \cqg{please emphasis the invariace feature here.}
% In particular, Contrastive learning encourages the model to learn the sample-level and class-level features, which promotes feature diversity and helps prevent the collapse of relevant features~\cite{graphCL,contrast_loss1}. 
% Inspired by these properties, we propose the addition of a UCL loss to capture invariant features while avoiding excessive regularization.
% \paragraph{Diverse Loss Design.} 
% To incorporate the strengths of SSL into our approach, we introduce a diversity-driven loss function $\mathcal{L}_{div}$. 
% The UCL loss is designed to encourage the model to learn the diverse invariant feature from different augmented views. 
Formally, $\mathcal{L}_{ucl}$ is defined as follows:
\begin{equation} 
% \small
\mathcal{L}_{ucl}=-\sum_{i=1}^N\log\frac{\exp(||z_i-z_{i}'||^2_2)}{\sum_{j\neq i'}\exp(||z_i-z_j||^2_2)+\exp(||z_i-z'_i||^2_2)} 
\label{eq: diverse_loss} 
\end{equation}

Here, $z_i$ represents the feature vector of the original sample, $z'_i$ is the augmented version of the same sample, treated as a positive pair, and $z_j$ is the different sample (negative pair). 
% This loss encourages the model to bring together positive pairs while repelling negative pairs, ensuring a rich and diverse feature representation.

\paragraph{Overall Training Objective of \ours} 
By merging our UCL loss with the conventional invariant loss, we aim to balance the diversity of invariant features while preserving the effectiveness of the prior invariant loss in reducing the influence of spurious features.
The final objective function of \oursfull (\ours) is as follows:
\begin{equation} 
\label{eq: divil}
\mathcal{L}_{\ours}=\mathcal{L}_{pred}+\lambda \mathcal{L}_{il}+\beta \mathcal{L}_{ucl} 
\end{equation}

Here, $\mathcal{L}_{pred}$ is the cross-entropy loss, $\mathcal{L}_{il}$ is any invariant loss from IL methods, and $\mathcal{L}_{ucl}$ is our proposed unsupervised contrastive loss. 
The hyperparameters $\lambda > 0$ and $\beta > 0$ control the trade-off between invariance and diversity, which can be tuned based on the task.
As seen in Figure~\ref{fig:IRM&UCL}, we observe that after adding the UCL loss, the strength of invariant features increases across various IL penalties, verifying our analysis that incorporating unsupervised contrastive learning as a complement to the invariant loss effectively enhances OOD generalization.







\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\textwidth]{images/validate_ucl_in_IL.jpg}
	\caption{
    \tmlr{The increased strengths after incorporating UCL with IRMv1 and VREx.} The X-axis represents different invariant variances $\sigma_c = \{5,3,1,0.1\}$.The Y-axis shows the strength of the corresponding subset of invariant features $\Phi(x)$ before and after adding the UCL. For each configuration, we run 10 different seeds and report the average results.
 }
	\label{fig:IRM&UCL}
\end{figure}




% \cqg{The core value of this section is how to prove that divil solves these problems. It seems that the introduction of many methods is not very necessary and can be placed in related work, including the two paragraphs before and after remark.}

\begin{remark}[Effectiveness of \ours]
% Define the linear model as \( f_{\mathbf{\Theta}} = \mathbf{Wx} + \mathbf{b} \), where \( \mathbf{\Theta} \) is the concatenated parameter \( [\mathbf{W}, \mathbf{b}] \). Let \( \mathbf{\Theta}^{*} = [\mathbf{W}^{*}, \mathbf{b}^{*}] \) be the minimizer of 
\tmlr{Our synthetic experiment indicates that for the original over-invariant data \( o_c \) illustrated in Remark 3.1, the strength of \( o_c \) learned by \ours in~\eqref{eq: divil} is bigger than the original invariant learning:
\[
\text{strength}_{\ours} (o_s)  > \text{strength}_{IRM/VREx} (o_s),.
\]}
Thus, \ours effectively mitigates the \textbf{over-invariance} issue at test time.
\end{remark}







% We discuss more details about $\mathcal{L}_{il}$ in Appendix~\ref{sec: diss il loss}.

% \jq{todo: add the theorem on divil}
% \begin{remark}
%  Let our \ours loss \( L_{\ours} \) defined as:
%     \[
%     L_{\ours}(\mathbf{\Theta}) = \beta L_{il}(\mathbf{\Theta}) + (1 - \beta) L_{ucl}(\mathbf{\Theta}),
%     \]
%     where \( L_{il} \) is the supervised contrastive learning loss as an instance of the invariant loss, \( L_{ucl} \) is the unsupervised contrastive losses, and \( \beta \in (0, 1) \). 
%     % Assume without loss of generality that \( \phi_3^2 \geq \phi_4^2 \geq \dots \geq \phi_K^2 \), and the class feature variance \( \phi_1^2 \) is smaller than the variance of irrelevant features \( \phi_2^2 \).
%     In this scenario, both the minimum norm minimizer of \( L_{il} \) and the minimum norm minimizer of \( L_{ucl} \) suffer from the over-invariance issue.
%     However, for a suitable \( \beta \), the minimum norm minimizer of \( L_{\ours} \) avoids the over-invariance issue and learns the diverse invariant feature.    
% \end{remark}

\begin{algorithm}[t]
\caption{Overall Training Objective of \ours }
\begin{algorithmic}[1]
\State \textbf{Input:} Train dataset $\mathcal{D}^{tr} = \{D^s\}_{s \in \epsilon_{tr} \subseteq \epsilon_{all}}$
and test dataset 
$\mathcal{D}^{te} = \{D^s\}_{s \in \epsilon_{te} \subseteq \epsilon_{all}}$, $\epsilon_{tr} \neq \epsilon_{te}$.
% $\lambda > 0$, $\beta > 0$
\State \textbf{Output:} Trained model $f=w \circ \Phi$
\State \textbf{Function:} Representation function $\Phi: X \rightarrow H$,  Invariant predictor $w: H \rightarrow Y$
\State \textbf{Hyperparameters:} $\lambda > 0$, $\beta > 0$, mask probability $p$, learning rate $\eta$
\State // \ours OOD Generalization Training
% \For{each environment $s$ in $\epsilon_{tr}$}
    \For{each batch $(X_i, Y_i) \in \mathcal{B}$ from $\mathcal{D}^{tr}$ }
        % \State Draw samples independently from distribution $\mathcal{P}^s$
        % \State Learn meaningful feature $Z$ for each data using $\Phi$
        % \State Predict label $\hat{Y}$ based on $Z$ using $w$
        \State Calculate cross-entropy loss: $\mathcal{L}_{\text{pred}}$
        \State Calculate invariant loss: $\mathcal{L}_{\text{il}}$
        \State Using data augmentation technique to get the $z$ and $z'$.
        \State masking the front $p$ percent of the entire dimensions of $z$ and $z'$.
        \State Calculate unsupervised contrastive loss: $\mathcal{L}_{\text{ucl}}$ in~\eqref{eq: diverse_loss}.
        \State Calculate total loss: $\mathcal{L}_\ours = \mathcal{L}_{\text{pred}} + \lambda \mathcal{L}_{\text{il}} + \beta \mathcal{L}_{\text{ucl}}$
        \State Compute gradients: $\nabla_{\Phi} = \frac{\partial \mathcal{L}_\ours}{\partial \Phi}, \nabla_{w}=\frac{\partial \mathcal{L}_\ours}{\partial w}$
        \State Update model parameters:
        \State \quad $\Phi \gets \Phi - \eta \nabla_{\Phi}$
        \State \quad $w \gets w - \eta \nabla_{w}$
    \EndFor
% \EndFor
% \State Train model $f=w \circ \Phi$ on train set $\mathcal{D}^{tr}$ for OOD generalization
\end{algorithmic}
\label{al: divil}
\end{algorithm}

\paragraph{Enhancing Diversity of the Invariance via Random Masking}
Contrastive learning methods, by repelling negative samples, can alleviate the over-invariance problem to some extent. 
However, when faced with strong data augmentation or deep-layer implicit regularization, the model performance can also remain suboptimal \citep{DC_leCun}. 
To further enhance feature diversity, we trained a non-linear projector to scatter the representation space spectrum. 
Additionally, we introduced a random masking mechanism to the features to overcome over-invariance. 
We set the first p dimensions of the contrastive learning feature dimension to 0, that is $z_{1:p}=0$.

In conclusion, the detailed training procedure of \ours is shown in Algorithm \ref{al: divil}.

% \jq{discuss previous cl methods or DA methods for ood.}
% \begin{equation}
% \label{irmv1}
%     \text{min}_{\Phi:\mathcal{X} \rightarrow \mathcal{Y}} \Sigma_{e \in \epsilon_{tr}} \mathbf{R}^e({\Phi}) + \lambda \| \triangledown_{\omega | \omega=1.0} \mathbf{R}^e({\omega \cdot \Phi})\|\,,
%     \label{eq: irm-v1}
% \end{equation}
% \begin{theorem}[Class Collapse in Supervised Contrastive Learning] \label{class-collapse}
%     Let \( \mathbf{\Theta}^{**} = [W^{**}, b^{**}] \) be the minimum norm minimizer of the contrastive learning loss \( L_{CL} \). Suppose \( \mathbf{\Theta}^{**} \) minimizes the loss as follows:
%     \[
%     \mathbf{\Theta}^{**} = \arg\min_{\mathbf{\Theta}^{*}} \|\mathbf{\Theta}^{*}\|_F \quad \text{such that} \quad \mathbf{\Theta}^{*} \in \arg\min_{\mathbf{\Theta}} L_{CL}(\mathbf{\Theta}).
%     \]
%     With high probability (at least \( 1 - O\left(\frac{m^2}{n^2 d}\right) = 1 - o(1) \)), \( W^{**} \) has no alignment with the subclass feature \( v_2 \), meaning:
%     \[
%     \|W^{**} v_2\| = 0.
%     \]
%     Thus, \textbf{class collapse} occurs at test time, meaning no linear classifier can predict subclass labels better than random guessing.
% \end{theorem}


% \begin{theorem}[Feature Suppression in Unsupervised Contrastive Learning] \label{feature-suppression}
%    Let \( p \leq K \), where \( p \) represents the number of dimensions in the encoded feature space and \( K \) the total number of features. Define the tuple \( \mathcal{L} = \{1, \phi_1^2, \phi_2^2, \dots, \phi_K^2\} \), where each \( \phi_i^2 \) represents the variance of the features. If \( \phi_1^2 \) (class feature) is not among the \( p \)-largest elements of \( \mathcal{L} \), then with high probability \( 1 - O\left(\frac{m^2}{n^2 d}\right) = 1 - o(1) \):
%         (1) There exists a global minimizer \( \mathbf{\Theta}^{*} \) such that \( \|W^{*} v_1\| = \Omega(1) \),
%         (2) However, the minimum norm minimizer \( \mathbf{\Theta}^{**} \) satisfies \( \|W^{**} v_1\| = 0 \).
% This suggests that the \textbf{class feature} is suppressed when the embedding space is too limited to capture all the feature dimensions, and only the most discriminative features are retained in the encoded space. 
% \end{theorem}







% \subsection{Theoretical Analysis}





