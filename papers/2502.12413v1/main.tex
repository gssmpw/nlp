
\documentclass[10pt]{article} % For LaTeX2e
% \usepackage[accepted]{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
% \usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{colortbl}
% \usepackage[table]{xcolor}
\definecolor{best}{RGB}{173,216,230} % Light blue for the best result
\definecolor{secondbest}{RGB}{220,220,220} % Light gray for the second 
% \usepackage{natbib} 
\usepackage{xspace}
\usepackage{multirow}
\usepackage{multicol}

\newcommand{\zyh}[1]{\textcolor{blue}{[#1 -- \textsc{zyh}]}}
\newcommand{\cqg}[1]{\textcolor{red}{[#1 -- \textsc{CQG}]}}
\newcommand{\zzx}[1]{\textcolor{green}{[#1 -- \textsc{zzx}]}}
\newcommand{\jq}[1]{\textcolor{blue}{#1}} 

\newcommand{\tmlr}[1]{\textcolor{black}{#1}} 

\newcommand{\ours}[0]{\texttt{DivIL}\xspace}
\newcommand{\ourst}[0]{\text{DivIL}\xspace}	% ours in normal text 
\newcommand{\oursfull}[0]{
Diverse Invariant Learning
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\title{\ours: Unveiling and Addressing Over-Invariance for Out-of-Distribution Generalization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{ \name Jiaqi Wang\thanks{Equal contribution.} \email jqwang23@cse.cuhk.edu.hk \\ 
\addr The Chinese University of Hong Kong \\
\\
\name Yuhang Zhou \footnotemark[1] \email ralph.yh.zhou@gmail.com \\ 
\name Zhixiong Zhang\footnotemark[1] \email zxzhang0216@gmail.com \\ 
\name Qiguang Chen \email qgchen@ir.hit.edu.cn \\
\addr  Harbin Institute of Technology \\ 
\\
\name Yongqiang Chen \email yqchen24@gmail.com \\
\addr CMU CLeaR Group \\
\\ 
\name James Cheng \email jcheng@cse.cuhk.edu.hk \\
\addr The Chinese University of Hong Kong \\ 
      }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{02}  % Insert correct month for camera-ready version
\def\year{2025} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=2Zan4ATYsh}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
 Out-of-distribution generalization is a common problem that expects the model to perform well in the different distributions even far from the train data.
    A popular approach to addressing this issue is invariant learning (IL), in which the model is compiled to focus on invariant features instead of spurious features by adding strong constraints during training.
    %
    % to mitigate the influence of environments. 
    % However, there is a lack of a theoretical analysis on the limitations of invariant constraints.
    % with the help of various environments.
    However, there are some potential pitfalls of strong invariant constraints.
    % Although imposing excessive constraints can alleviate the spurious correlations, 
    Due to the limited number of diverse environments and over-regularization in the feature space,  
    it may lead to a loss of important details in the invariant features while alleviating the spurious correlations, namely the \textit{over-invariance}, which can also degrade the generalization performance. 
    We theoretically define the over-invariance and observe that this issue occurs in various classic IL methods.
% and encourage richer invariance \cqg{"Richer" is hard to understand. What does richer invariance means}
To alleviate this issue, we propose a simple approach\oursfull (\ours) by adding the unsupervised contrastive learning and the random masking mechanism compensatory for the invariant constraints, which can be applied to various IL methods.
    Furthermore, we conduct experiments across multiple modalities across $12$ datasets and $6$ classic models, verifying our over-invariance insight and the effectiveness of our \ours framework.
    Our code is available in \url{https://github.com/kokolerk/DivIL}.
\end{abstract}

\input{section/introduction}

\input{section/analysis}
\input{section/theorem}
\input{section/experiment}
\input{section/related-work}
\section{Conclusion}
We shed light on the limitations of invariant constraints in addressing out-of-distribution generalization. 
While these constraints can mitigate spurious correlations, our research revealed the risk of \textit{over-invariance}, potentially leading to the loss of crucial details in invariant features and a subsequent decline in generalization performance. 
To tackle these challenges, we introduced \oursfull (\ours), leveraging contrastive learning and random feature masking to introduce uncertainty and diversity. 
Our comprehensive experiments spanning various modalities and models, underscored the efficacy of our proposed method in enhancing model performance.

% \section{Limitation}

% \newpage

\bibliography{reference}
\bibliographystyle{tmlr}

\newpage
\appendix
% \input{tmlr-style-file-main/section/related-work}
% \input{tmlr-style-file-main/section/appendix}
\input{section/appedix_exp}

\end{document}
