\section{Method}
\label{sec:method}



\begin{figure}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \vspace{-15pt}
   \includegraphics[width=0.8\linewidth]{images/concept_hr.JPG}

   \caption{Overview of the material transfer process in \textbf{MaterialFusion}. Starting with a material exemplar $y_{im}$, an input image $x_{init}$, and prompts, our framework produces a target image where the object adopts the desired material properties from $y_{im}$.}
   \label{fig:concept}

   \vspace{-5pt}
\end{figure}
In this section, we will formulate the problem statement and discuss the methods that will be employed for material transfer from one image to another. Our task involves transferring texture or material from one image $y_{im}$ into an object in the foreground of another image $x_{init}$, while preserving the background information and fine-grained details of the object. The goal is to generate an image that corresponds to the target prompt $y_{trg}$, where the object in this image is imbued with the material from $y_{im}$. The input of our model consists of an object-centric image with the target object $x_{init}$, an image of the material $y_{im}$ that can be represented as either texture or another object, and two text prompts: target prompt $y_{trg}$ and the source prompt $y_{src}$.
An accompanying image (see Fig. \ref{fig:concept}) illustrates this process, highlighting the inputs and outputs of the model.

To address this problem, we will employ three primary methods: the Stable Diffusion v1.5 model \cite{rombach2022high}, the IP-Adapter \cite{ye2023ipadaptertextcompatibleimage} for material encoding and embedding, and the Guide-and-Rescale approach \cite{titov2024guideandrescaleselfguidancemechanismeffective}, which we will refer to as GaR. The GaR method enables guidance during the image generation process to preserve the original layout and structure, as well as the key details of the objects. This ability to maintain the image's integrity will be crucial to our material transfer tasks. Additionally, masking techniques will be utilized to enhance background preservation and facilitate effective material transfer to the designated areas of the image.

%-------------------------------------------------------------------------

\subsection{Preliminaries}
\paragraph{Diffusion Model.} For our material transfer problem, we utilize the Stable Diffusion v1.5 model \cite{rombach2022high}, a latent diffusion model (LDM), which operates in a lower-dimensional latent space.


An essential aspect of the Stable Diffusion model is its use of classifier-free guidance (CFG) \cite{ho2022classifier},  which allows the model to generate images conditioned on specific inputs. In contrast to classifier guidance \cite{dhariwal2021diffusion}, which requires a separately trained classifier to direct the sampling process towards particular targets, classifier-free guidance blends the outputs of the conditioned and unconditioned models, controlled by a guidance scale $w$.  The noise prediction during the sampling stage when employing the classifier-free guidance mechanism can be mathematically expressed as:

\vspace{-0.5cm}
\begin{equation}
  \hat{\epsilon}_{\theta}(z_{t}, c, t) = w\epsilon_{\theta}(z_{t}, c, t) + (1-w)\epsilon_{\theta}(z_{t}, t) 
  \label{eq:cfg}
\end{equation}
where $\epsilon_{\theta}(z_{t}, c, t)$ is the conditioned prediction, $\epsilon_{\theta}(z_{t}, t)$ is the unconditioned prediction and $w$ is guidance scale. This mechanism allows the model to generate high-quality outputs that are both creative and contextually aligned with the given conditions.

\paragraph{Guide-and-Rescale.}

In our approach to material transfer, we utilize a modified diffusion sampling process that employs a self-guidance mechanism \cite{epstein2023diffusion}, as proposed by the authors of \cite{titov2024guideandrescaleselfguidancemechanismeffective}. The self-guidance mechanism involves leveraging an energy function $g$ to guide the sampling process, provided that a gradient with respect to $z_t$ exists.


Self-attention mechanisms, as highlighted by \cite{tumanyan2023plug}, effectively capture important information regarding the relative positioning of objects within an image. While the diffusion UNet layers can extract essential features from images during the forward process. Building on these insights, the authors of the GaR article developed an approach that incorporates a modified diffusion sampling process through a guidance mechanism. This enables targeted editing of specific regions within the image while preserving vital visual features—such as facial expressions—and maintaining the overall layout of the image.

First, in GaR, a DDIM inversion \cite{song2020denoising} trajectory is obtained $\{{z^*_t}\}_{t=0}^T$ for $x_{init}$, conditioning on
$y_{src}$. Consequently, the single noise sampling step in GaR is defined as follows:
\begin{equation}
\begin{split}
  \hat{\epsilon}_{\theta}(z_{t}, c, t) = w\epsilon_{\theta}(z_{t}, c, t) + (1-w)\epsilon_{\theta}(z_{t}, t) + \\  + v \cdot \nabla_{z_t}g(z_t, z_t^*, t, y_{src}, I^{*}, \overline{I})  
  \label{eq:cfg}
\end{split}
\end{equation}

where $\overline{I}$ and $I^*$ represent a set of inner representations computed during the forward pass of $\epsilon_\theta(z_t, t, y_{src})$ and $\epsilon_\theta(z^*_t, t, y_{src})$ respectively. Additionally, $v$ denotes the self-guidance scale.

\begin{figure*}
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \vspace{-20pt}
   \includegraphics[width=0.96\linewidth]{images/comparison_mat_force.png}

   \caption{(Left) Comparison of material transfer results across different methods. From left to right: the original image, target material, results using Guide-and-Rescale (GaR), IP-Adapter with masking, our method without masking, and our full \textbf{MaterialFusion} approach. Our method achieves realistic material transfer while preserving object structure and background consistency. (Right) Gradual transfer of material characteristics with increasing "transfer force" ($\lambda$).}
   \label{fig:gar}

   \vspace{-5pt}
\end{figure*}

\begin{figure}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   
   \vspace{-15pt}
   \includegraphics[width=0.96\linewidth]{images/background_w_o_masking.JPG}

   \caption{First Masking. After the first masking, the material is successfully transferred to the targeted area of the image. However, background preservation is not flawless, with noticeable issues occurring on the table.}
   \label{fig:background}

   \vspace{-5pt}
\end{figure}

\paragraph{IP-Adapter.} In our study, we utilize the IP-Adapter, a lightweight and effective mechanism designed to enhance image prompt capabilities in pretrained text-to-image diffusion models. The IP-Adapter employs a decoupled cross-attention mechanism that facilitates the independent processing of text and image features. This architectural design enables the effective integration of multimodal inputs, combining both text and image prompts. 

The IP-Adapter comprises an image encoder that extracts relevant features from the image prompt and adapted modules that utilize decoupled cross-attention to embed these image features into the diffusion model. Additionally, the IP-Adapter can be trained only once and then directly integrated with custom models derived from the same base model, along with existing structural controllable tools. This characteristic significantly expands its applicability and is crucial for our work, as we combine the IP-Adapter with the Guide and Rescale method, enhancing our capability to achieve effective material transfer.

When utilizing the IP-Adapter, the noise prediction is adapted to incorporate image conditioning, resulting in the following expression:

\begin{figure*}[ht!]
  \centering
   
   \vspace{-15pt}
   \includegraphics[width=0.96\linewidth]{images/pipeline_lambda_new.png}
   \caption{The overall pipeline of \textbf{MaterialFusion} for material transfer. Starting with DDIM inversion of the target image $x_{init}$ and material exemplar $y_{im}$, the framework combines the IP-Adapter with UNet and employs a guider energy function for precise material transfer. A dual-masking strategy ensures material application only on target regions while preserving background consistency, ultimately generating the edited output $x_{edit}$. The parameter $\lambda$, known as the Material Transfer Force, controls the intensity of the material application, enabling adjustment of the transfer effect according to user preference.}
   \label{fig:pipeline}

   \vspace{-5pt}
\end{figure*}

\vspace{-0.5cm}
\begin{equation}
  \hat{\epsilon}_{\theta}(z_{t}, c_t, c_i, t) = w\epsilon_{\theta}(z_{t}, c_t, c_i, t) + (1-w)\epsilon_{\theta}(z_{t}, t) 
  \label{eq:ip}
\end{equation}

where $\epsilon_{\theta}(z_{t}, c_{t}, c_{i}, t)$ represents the predicted noise, $c_{t}$ is the text conditioning, and $c_{i}$ signifies the image conditioning. This formulation closely resembles the standard noise prediction seen in classifier-free guidance, but it additionally incorporates conditioning from the image prompt, enabling the generation of more contextually relevant outputs.


\subsection{Our method}
In this section, we introduce our method, which integrates the Guide-and-Rescale (GaR) approach and the IP-Adapter for effective material transfer. To begin, we will evaluate the applicability of each method independently in the context of material transfer, identifying their strengths and limitations. Understanding the challenges inherent to each approach will provide a foundation for our integrated solution. 


\paragraph{Guide-and-Rescale for material transfer.} In GaR, the use of a self-guidance mechanism during generation improves the editing process by preserving the initial image features and layout of the image, while the editing itself is done by CFG via a text prompt. However, relying solely on GaR proves insufficient for effectively transferring material to an object (see Fig.\ref{fig:gar}, third column). While GaR successfully retains the details of the original object, it often falls short in material transfer, resulting in either a degree of transfer that is less than desired or no transfer occurring at all. Additionally, for the task of transferring material, the strategy of changing the material via a text prompt is not suitable strategy for several reasons: firstly, generating an object with a new transferred material can be tricky for SD because the model might lean toward more typical depictions of the object. For example, generating a wooden or glass pumpkin may not be successful and could result in the generation of an ordinary typical orange pumpkin. Secondly, transferring material via text prompts requires writing large and detailed text prompts, which is not very convenient. Thirdly, text prompts can be interpreted in various ways, making it difficult to control precise attributes such as texture, color, and structural details of the material that we want to transfer.

\paragraph{IP-Adapter for material transfer.} Using a text prompt to generate an object with transferred material may not yield the exact, highly specific details and nuances of the material that can be achieved by generating from a picture prompt. As the authors of the IP-Adapter article stated, "an image is worth a thousand words”. All these reasons prompted us to use the IP-Adapter for encoding the material and then adding it to the target object. 

The IP-Adapter consists of two main components: a pretrained CLIP \cite{radford2021learning} image encoder model, which in our case extracts material-specific features from a material exemplar image, and adapted modules with decoupled cross-attention, which integrate these material features into a pretrained text-to-image diffusion model (SD v1.5 in our case).

While the IP-Adapter is a promising method for material transfer, it is important to note that it cannot independently achieve successful material transfer to an object. As illustrated in Fig. \ref{fig:gar} (fourth column), using the IP-Adapter to add material features to a specific region of the image via masking does not yield the desired outcomes. Although the texture of the material is transferred effectively, the details of the objects are lost, causing them to no longer resemble their original forms. This loss of object identity is a significant issue.


\paragraph{Our method, MaterialFusion as a combination of GaR and IP-Adapter.} As mentioned earlier, GaR effectively preserves the details of objects but has limitations in its ability to transfer material. Conversely, the IP-Adapter excels in material transfer but does not retain the details of the objects. To harness the advantages of both approaches, we have developed a method, which leverages the strengths of both GaR and the IP-Adapter while addressing their individual limitations.

In this combined framework, the IP-Adapter is responsible for executing the material transfer, while GaR maintains the geometry of the target object, ensuring that the background and overall pose remain intact. Additionally, GaR contributes to preserving crucial visual features of the objects, enabling a cohesive integration of the transferred material while upholding the original image details. More details on the importance of using GaR in the task of material transfer are provided in the Appendix \ref{appendix:guiders}

The overall scheme of the proposed method, MaterialFusion, is depicted in Fig.\ref{fig:pipeline}. The process begins with the DDIM inversion of the source image. Subsequently, MaterialFusion conducts image editing through a denoising process, during which the UNet, in conjunction with the IP-Adapter, incorporates material features into the generated image at each denoising step. Moreover, at each step of the denoising trajectory, the noise term is adjusted by a guider that employs the latent variables $z_t$ from the current generation process, along with the time-aligned DDIM latents $z^*_t$. This adjustment helps preserve the geometry, pose, and features of the object.

A single sampling step of MaterialFusion is defined by the following formula:

\begin{equation}
\begin{split}
  \hat{\epsilon}_{\theta}(z_{t}, c_t, c_i, t) = w\epsilon_{\theta}(z_{t}, c_t, c_i, t) + (1-w)\epsilon_{\theta}(z_{t}, t) + \\  + v \cdot \nabla_{z_t}g(z_t, z_t^*, t, y_{src}, I^{*}, \overline{I})
  \label{eq:cfg}
\end{split}
\end{equation}

where $\overline{I}$ and $I^*$ represent a set of inner representations computed during the forward pass of $\epsilon_\theta(z_t, t, y_{src})$ and $\epsilon_\theta(z^*_t, t, y_{src})$ respectively; $v$ denotes the self-guidance scale; $c_{t}$ represents the text conditioning, and $c_{i}$ signifies the image conditioning.

The pseudocode for the MaterialFusion method can be found in Appendix \ref{appendix:code}.


\paragraph{Masking for controlled Material Transfer.} Despite the initial success of our model in transferring material to target objects, we faced significant challenges, particularly regarding unintended material transfer to non-target areas and minor alterations in the background (as illustrated in the first row of Fig. \ref{fig:background} or in the fifth column of Fig. \ref{fig:gar}). To address these issues and enhance the precision of our approach, we implemented a masking technique for controlled Material Transfer. This technique is designed to confine the material transfer strictly to the desired regions of the object and better preserve the background.

In our method, we apply masking twice. The first masking is performed at the stage of incorporating material-specific features from a material exemplar image into a pretrained text-to-image diffusion model, which occurs through the image features cross-attention layers of IP-Adapter (see Fig. \ref{fig:pipeline}). This masking solves the problem of unintended material transferring to non-target areas. The results of the generation after cross-attention masking can be seen in Fig. \ref{fig:background}, where it is evident that after this masking, the material successfully transfers to the intended region, although some slight changes to the background can be observed.

In the second masking step, we solve the problem of background changes compared to the original image during generation. Masking is performed as follows: after each de-noising step, we extract the masked object from the sampling trajectory and a masked background from the DDIM inversion latent corresponding to the current step. In other words, this can be expressed as a formula:
\begin{equation}
  z_{t} = mask\cdot z_{t} + (1 - mask)\cdot z^*_{t}
  \label{eq:important}
\end{equation}
where $z_{t}$ is the latent representation from the current generation process, $z^*_{t} $ is the time-aligned DDIM latent, and $mask$ is the binary mask of the object.

This formula illustrates how we combine the current latent $z_{t}$ and the time-aligned DDIM latent $z^{*}_{t}$ using a binary mask. The values corresponding to the object are retained from the current generation step, while the background is updated by combining with the time-consistent latent representation from the corresponding inversion step. In this way, we ensure stability and continuity of the background, avoiding abrupt transitions and artifacts. Moreover, this approach not only improves the visual quality of the final image, but also promotes a smoother integration of elements in the image, creating a lively and harmonious composition. 
 
We also determined the appropriate denoising step up to which masking should be performed.
Details regarding this evaluation can be found in Appendix \ref{appendix:masking} of the supplementary materials.

\begin{figure*}[!t]
  \centering
   \vspace{-20pt}
   \includegraphics[width=0.96\linewidth]{images/qualitative_analysis2.png}
   \caption{To compare the qualitative results obtained by different methods: Our method, ZeST, GaR, and IP-Adapter with masking. Our method demonstrates more realistic material integration, preserving object structure and achieving higher fidelity to the target material.}
   \label{fig:qual}

   \vspace{-5pt}
\end{figure*}


\paragraph{Material Transfer Force.}
As previously mentioned, our method employs a decoupled cross-attention mechanism from the IP-Adapter for material transfer, utilizing query features $Z$, text features $c_t$, and image features from the material exemplar $c_i$. The relevant matrices for the attention operations are defined as follows:
\begin{itemize}
\item For text features: $Q = ZWq$, $K = c_tW_k$, $V = c_tW_v$
\item For material image features: $Q = ZW_q$, $K' = c_iW'_k$, $V' = c_iW'_v$
\end{itemize}
Then the overall output of the attention mechanism is given by:

\begin{equation}
Z_{new} = Attn(Q, K, V) +
\lambda \cdot Attn(Q, K', V')
  \label{eq:attn}
\end{equation}

Here, $\lambda$ represents the Material Transfer Force, which controls the intensity of material transfer in the output. Adjusting $\lambda$ allows for modulating the influence of material characteristics while preserving details, resulting in a coherent and visually appealing integration. As illustrated in Fig. \ref{fig:gar}, variations in $\lambda$ demonstrate the resulting effects on material transfer and detail preservation.
For more details on the Material Transfer Force, please refer to the Appendix \ref{appendix:material_transfer_force}.
