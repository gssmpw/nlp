\section{Introduction}
\label{sec:intro}

Manipulating the material appearance of objects in images is a critical task in computer vision and graphics, with wide-ranging applications in augmented reality, virtual prototyping, product visualization, and digital content creation. Material transfer is the process of applying the material properties from a source exemplar to the target object in an imageâ€”enables users to visualize objects under different material conditions without the need for complex 3D modeling or rendering. This capability accelerates design workflows and enhances the realism of synthesized images, making it an important area of research.

Despite its significance, achieving high-quality material transfer remains challenging due to difficulties in preserving geometric characteristics, controlling the degree of material application, and effectively handling object boundaries. Existing methods~\cite{yeh2024texturedreamer,richardson2023texture} often distort the target object's shape or surface details when applying new material properties, compromising its geometric fidelity. Moreover, many approaches~\cite{sharma2023alchemist,cheng2024zestzeroshotmaterialtransfer,titov2024guideandrescaleselfguidancemechanismeffective} lack flexibility in adjusting the extent of material transfer, leading to excessive application that overwhelms the object's original structure and details, resulting in unnatural appearances. Additionally, improper blending at object boundaries can introduce noticeable artifacts, detracting from the overall image quality and disrupting consistency with the background.

Existing methods like ZeST~\cite{cheng2024zestzeroshotmaterialtransfer} attempt to address material transfer without relying on explicit 3D information, but they often suffer from quality issues such as poor preservation of geometric characteristics and lack of control over the degree of material transfer. Furthermore, general-purpose image editing techniques, which include IP-Adapter~\cite{ye2023ipadaptertextcompatibleimage}, Guide and Rescale~\cite{titov2024guideandrescaleselfguidancemechanismeffective}, and DreamBooth~\cite{ruiz2023dreamboothfinetuningtexttoimage}, struggle with material transfer tasks. They may not accept material exemplars as input images, or if they do, they fail to produce satisfactory results, especially in preserving material properties and handling background integration.

To overcome these limitations, we propose MaterialFusion, a novel framework that combines the IP-Adapter with the Guide-and-Rescale (GaR) method within a diffusion model to achieve high-quality material transfer with enhanced control and fidelity. Our approach uses the IP-Adapter to encode material features from a source exemplar image, capturing the specific textures and nuances of the material to be transferred. Concurrently, GaR helps preserve the geometric characteristics and essential features of the target object, maintaining its original structure and details. To address issues of unintended material application and background alterations, we introduce a dual masking strategy: first, we apply masking during the material transfer process to confine the transfer to the desired regions; second, we perform masking after each denoising step to seamlessly integrate the modified object into the background and mitigate boundary artifacts. This combined approach allows for precise control over the degree and location of material transfer, resulting in natural and realistic images that maintain consistency with the surrounding environment.

\noindent Our main contributions are as follows:

\begin{itemize}
    \item We present \textbf{MaterialFusion}, a novel framework that significantly improves the quality of material transfer in images by addressing the shortcomings of existing methods without relying on explicit 3D information.
    
    \item We introduce an adjustable material transfer control mechanism, enabling users to finetune the extent of material application. This allows for a balanced integration of new material properties with the object's original appearance, maintaining natural and realistic results.
    
    \item We have compiled an extensive dataset of real-world material transfer examples and conducted detailed comparative analyses. Through comprehensive quantitative evaluations and user studies, we demonstrate that our method outperforms existing approaches in both quality and flexibility.
\end{itemize}
