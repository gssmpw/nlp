\clearpage
\setcounter{page}{1}

\appendix

\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitlesupplementary
\begin{center}
    \vspace{-15pt}
    \includegraphics[width=0.96\linewidth]{images/guiders_together_compressed.png}
    %\vspace{-10pt}
    \captionsetup{type=figure}
    \caption{Illustration of the significance of using guiders: (Left) Self-attention guider; (Right) Feature guider. Transferring material without these guiders fails to maintain the object's geometry, visual features, and pose.}
    \vspace{-0mm}
    \label{fig:guiders}
\end{center}
}]


\section{Necessity of guiders}
\label{appendix:guiders}


As previously mentioned, the Guide-and-Rescale method employs an energy function $g$ to enhance the sampling process. The authors of the GaR approach introduced two guiding mechanisms: the Self-attention Guider and the Feature Guider. When utilized together during the generation process, these guiders significantly enhance the preservation of original image details.


Self-attention mechanisms, as noted by the authors of \cite{tumanyan2023plug}, capture significant information regarding the relative positioning of objects within an image. So authors of GaR suggested guiding through matching of self-attention maps from the current trajectory $\overline{A}^{\mathrm{self}}_i := \mathrm{self attn.}[\epsilon_{\theta}(z_t, t, y_{\mathrm{src}})] $ and an ideal
reconstruction trajectory $A^{*\mathrm{self}}_i := \mathrm{self attn.}[\epsilon_{\theta}(z^*_t, t, y_{\mathrm{src}})]$, where $i$ corresponds
to the index of the UNet layer.
So the self-attention guider is defined as follows:

\begin{equation}
\begin{split}
g_{\mathrm{self}}(z_t, z_t^*, t, y_{\mathrm{src}}, \{A_i^{*\mathrm{self}}\}, \{\overline{A}_i^{\mathrm{self}}\}) = \\ =\sum_{i=1}^{L}\mathrm{mean}||A_i^{*\mathrm{self}} - \overline{A}_i^{\mathrm{self}}||_2^2
  \label{eq:self-guider-eq}
\end{split}
\end{equation}

Moreover, during the forward process, diffusion UNet layers can extract essential features from images.  In GaR authors defined
features $\Phi$ as an output of the last up-block in UNet. If $\overline{\Phi} = \mathrm{features}[\epsilon_\theta(z_t, t, y_{\mathrm{src}})]$ and $\Phi^* = \mathrm{features}[\epsilon_\theta(z^*_t, t, y_{\mathrm{src}})]$ than feature guider is defined as:

\begin{equation}
g_{\mathrm{feat}}(z_t, z_t^*, t, y_{\mathrm{src}}, \Phi^{*}, \overline{\Phi}) = \mathrm{mean}||\Phi^*-\overline{\Phi}||_2^2
  \label{eq:feature-guider-eq}
\end{equation}




In our approach, we combine both the self-attention guider and the feature guider to maintain the layout, visual features, and geometry. Specifically, in our task of material transfer, the self-attention guider is primarily responsible for preserving the geometry and pose of the target object. Meanwhile, the feature guider focuses on maintaining the visual characteristics of the object. Although the feature guider also contributes to preserving the geometry, its effectiveness in this regard is somewhat less than that of the self-attention guider.

Thus, a single sampling step in MaterialFusion can also be expressed as follows:
\begin{equation}
\begin{split}
  \hat{\epsilon}_{\theta}(z_{t}, c_t, c_i, t) = w\epsilon_{\theta}(z_{t}, c_t, c_i, t) + (1-w)\epsilon_{\theta}(z_{t}, t) + \\  + \gamma[v_{\mathrm{self}} \cdot \nabla_{z_t}g_{\mathrm{self}}(z_t, z_t^*, t, y_{src}, \{A_i^{*self}\}, \{\overline{A}_i^{self}\}) + \\ 
  + v_{\mathrm{feat}} \cdot \nabla_{z_t}g_{\mathrm{feat}}(z_t, z_t^*, t, y_{src}, \Phi^{*}, \overline{\Phi})]
  \label{eq:material-sampling}
\end{split}
\end{equation}

Here, $\gamma$ is a scaling factor (the method of calculating $\gamma$ is detailed in the Guide-and-Rescale article), and $v_{\mathrm{self}}$ and $v_{\mathrm{feat}}$ represent the self-guidance scale and feature-guidance scale, respectively.

By adjusting these scales, one can modulate the influence of the guiders on the generated output. The significance of employing both the self-attention guider and the feature guider is illustrated in Fig. \ref{fig:guiders}.

\begin{figure*}[ht!]
  \centering
   %\vspace{-20pt}
   \includegraphics[width=0.96\linewidth]{images/t_up2_compressed.png}
   \caption{
Identification of the optimal denoising step for executing the second masking — adding background from the DDIM inversion trajectory. By masking the first 40 out of 50 denoising steps in this manner, we effectively preserve the background while achieving high-quality generation
   }
   \label{fig:t_up}
   %\vspace{-5pt}
\end{figure*}

\begin{figure*}[t!]
  \centering
   %\vspace{-15pt}
   \includegraphics[width=0.96\linewidth]{images/dataset_compressed.png}
   \caption{Examples of images from our custom dataset: (Left) Object images; (Right) Material images.}
   \label{fig:dataset}
   \vspace{-5pt}
\end{figure*}


\section{Analysis of Masking in Denoising Processes}
\label{appendix:masking} 



As mentioned earlier, we also identified the appropriate denoising step up to which the second masking step—adding background from the DDIM inversion trajectory—should be executed.

As shown in Fig. \ref{fig:t_up}, masking during the early iterations fails to preserve the image background, while masking across the entire denoising trajectory helps maintain the background but negatively impacts the quality of the generated output. To balance these effects, we chose an intermediate value of 40 out of 50 denoising steps for masking. This approach allows us to achieve both high generation quality and effective background preservation. 


\section{Pseudocode for the Proposed Method}
\label{appendix:code}
The proposed method is summarized in the Algorithm \ref{code:method}. %following pseudocode:


\section{Dataset Description}
\label{appendix:dataset}
To compare MaterialFusion with other methods, we created our own dataset of real free stock images. Our dataset comprises 15 material images and 25 object-oriented photographs of various objects. Fig.\ref{fig:dataset} showcases examples of objects and materials from the dataset.



\section{Additional visual comparison}
\label{appendix:qual_anal}

\begin{algorithm}[ht!]
%\vspace{-15pt}
\caption{MaterialFusion}\label{alg:overall_pipeline}
    \begin{algorithmic}[1]
        \Input Real image $x_{\mathrm{init}}$, source prompt $y_{\mathrm{src}}$, target prompt $y_{\mathrm{trg}}$, material exemplar image $y_{\mathrm{im}}$; DDIM steps $T$; guidance scales $w$, $v_{\mathrm{self}}$, $v_{\mathrm{feat}}$; guidance threshold $\tau_g$; masking threshold $\tau_m$; noise rescaling boundaries $r_{\mathrm{lower}}$, $r_{\mathrm{upper}}$; material transfer force $\lambda$; binary object mask $mask$.
        \Function VAE encoder $Enc.$, VAE decoder $Dec.$, $\mathrm{DDIM\;Inversion}$  \cite{song2020denoising}, $\mathrm{DDIM\;Sample}$ \cite{song2020denoising}, Self-attention Guider $g_{\mathrm{self}}$ (Equation \ref{eq:self-guider-eq}), Feature Guider $g_{\mathrm{feat}}$ (Equation \ref{eq:feature-guider-eq} ), noise rescaling $f_{\gamma}$ \cite{titov2024guideandrescaleselfguidancemechanismeffective}.
        \Output Edited image $x_{\mathrm{edit}}$.
        \\$z^*_0 = Enc.(x_{\mathrm{init}})$
        \For{$t=0,1,\ldots,T-1$}
            \State $z^*_{t+1} = \mathrm{DDIM\;Inversion}(z^*_t, y_{\mathrm{src}})$
        \EndFor
        \\$z_T = z^*_T$
        \For{$t= T,T-1,\ldots,1$}
            \State $\Delta_{\mathrm{cfg}} = w (\varepsilon_{\theta}(z_t, t, y_{\mathrm{trg}}, y_{\mathrm{im}}, \lambda, mask) - \varepsilon_{\theta}(z_t, t,\varnothing))$
            \State $\epsilon_{\mathrm{cfg}} = \varepsilon_{\theta}(z_t, t, \varnothing) + \Delta_{\mathrm{cfg}}$
            \State $\big\{ \{\mathcal{A}^{*\mathrm{self}}_i\}_{i=1}^L, \Phi^* \big\}= \varepsilon_{\theta}(z^*_t, t, y_{\mathrm{src}}) $
            \State $\big\{ \{\bar{\mathcal{A}}^{\mathrm{self}}_i\}_{i=1}^L, \bar{\Phi} \big\}= \varepsilon_{\theta}(z_t, t, y_{\mathrm{src}}) $
            \State $\epsilon_{\mathrm{self}} = v_{\mathrm{self}} \cdot g_{\mathrm{self}}(\{\mathcal{A}^{*\mathrm{self}}_i\}_{i=1}^L, \{\bar{\mathcal{A}}^{\mathrm{self}}_i\}_{i=1}^L)$
            \State $\epsilon_{\mathrm{feat}} = v_{\mathrm{feat}} \cdot g_{\mathrm{feat}}(\Phi^*, \bar{\Phi})$
            \State $r_{\mathrm{cur}} = \|\Delta_{\mathrm{cfg}}\|^2_2 / \| \nabla_{z_t} (\epsilon_{\mathrm{self}} +\epsilon_{\mathrm{feat}}) \|^2_2$
            \State $\gamma = f_{\gamma}(r_{\mathrm{lower}}, r_{\mathrm{upper}}, r_{\mathrm{cur}})$
            \If{$T - t < \tau_g$}
                \State $\epsilon_{\mathrm{final}} = \epsilon_{\mathrm{cfg}} + \gamma \cdot \nabla_{z_t} (\epsilon_{\mathrm{self}} +\epsilon_{\mathrm{feat}})$
            \Else
                \State $\epsilon_{\mathrm{final}} = \epsilon_{\mathrm{cfg}}$
            \EndIf
            \State$z_{t-1} = \mathrm{DDIM\;Sample}(z_t, \epsilon_{\mathrm{final}})$
            \If{$T - t < \tau_m$}
                \State$z_{t-1} = mask\cdot z_{t-1} + (1 -mask)\cdot z^*_{t-1}$
            \EndIf

        \EndFor
        \\$x_{\mathrm{edit}} = Dec.(z_0)$
        \Return $x_{\mathrm{edit}}$
        
    \end{algorithmic}
    \label{code:method}   
\end{algorithm}

In this section, we present an additional visual comparison of our method against ZeST, IP-Adapter with masking, and GaR, as illustrated in Fig.\ref{fig:qual_anal_app}. The results indicate that while GaR demonstrates a strong capability to maintain visual features, it struggles with effective material transfer. Conversely, IP-Adapter with masking is proficient at transferring material textures but often compromises the preservation of the objects' underlying features. ZeST performs well in transferring materials for simple objects, such as chairs, yet it falls short in maintaining features when dealing with more complex objects.
In contrast, our method effectively transfers materials to complex objects while maintaining their visual features.



\section{Extended quantitative analysis}
\label{appendix:quant_anal}
\begin{figure}[t!]
  \centering
   \vspace{3pt}
   \includegraphics[width=0.96\linewidth]{images/quantitative_analysis_app_new.png}

   \caption{Extended quantitative analysis of material transfer and object preservation. The numbers above the dots in the graph represent the material transfer force for the following methods: our method, IP-Adapter with masking, and our method without masking.}
   \label{fig:quant_anal_all}

   %\vspace{-5pt}
\end{figure}
Figure \ref{fig:quant_anal_all} provides an enhanced version of Figure \ref{fig:quant_anal}, introducing two additional methods: the IP-Adapter with masking and our method without masking. The material transfer force, represented by the numbers above the dots in the graph for the methods—Ours, IP-Adapter with masking, and our method without masking—was varied between $0.5$ (indicating weak material transfer) and $1.5$ (indicating excessively strong material transfer). The results indicate that all three methods improve material transfer effectiveness as the material transfer force increases, as evidenced by the rise in the CLIP similarity score. However, this enhancement comes at the expense of detail retention, as illustrated by the increasing LPIPS scores. Notably, at a material transfer force of 1.5, the performance metrics of our method closely resemble those of ZeST.

The graph also reveals that our method without masking leads to a significant increase in LPIPS compared to our masked approach, indicating that masking is crucial for preserving background details while effectively transferring material to the intended areas of the image.

Additionally, it is evident from the graph that the IP-Adapter with masking, despite enhancing material transfer relative to our method, fails to retain object details, as indicated by the high LPIPS scores.


\section{User study}
\label{appendix:user_study}
\begin{figure*}[t!]
  \centering
   %\vspace{-15pt}
   \includegraphics[width=0.96\linewidth]{images/qualitative_analysis_appendix_compressed.png}

   \caption{Qualitative comparison with baselines, including ZeST, Guide-and-Rescale, and IP-Adapter with masking, to integrate material features into specific regions of the image.}
   \label{fig:qual_anal_app}

   %\vspace{-5pt}
\end{figure*}
\begin{table*}
    %\vspace{-2pt}
  \caption{Overview of material transfer methods. Table presents the methods employed for material transfer, along with their respective implementations and configuration settings. Source repositories are included for reference.}
  \label{tab:configs}
  \centering
  \begin{tabular}{@{}lccc@{}}
    \toprule
    Method & Used Implementation & Configuration Settings \\
    \midrule
    ZeST&  \href{https://github.com/ttchengab/zest_code/tree/main}{ZeST github-repo}  &  N/A \\
    \hline
    IP-Adapter + masking & \href{https://github.com/tencent-ailab/IP-Adapter}{IP-Adapter github-repo}  &  $\tau_m = 40$\\
    \hline
    Guide-and-Rescale& \href{https://github.com/AIRI-Institute/Guide-and-Rescale}{GaR github-repo} & 
    \makecell{ $w = 7.5$,\\
         $\tau_g = 30$,\\
    $v_{\mathrm{self}} = 300000$, 
    $v_{\mathrm{feat}} = 500$, \\
     $r_{\mathrm{lower}} = 0.33$, $r_{\mathrm{upper}} = 3$
        } \\
        \hline
    Our& - &  
        \makecell{ $w = 7.5$,\\
         $\tau_g = 30$, $\tau_m = 40$, \\
    $v_{\mathrm{self}} = 700000$, 
    $v_{\mathrm{feat}} = 1500$, \\
     $r_{\mathrm{lower}} = 0.33$, $r_{\mathrm{upper}} = 3$
        } \\
    \bottomrule
  \end{tabular}

  %\vspace{-7pt}

\end{table*}
In this appendix, we present more details on the user study conducted to evaluate the effectiveness of our proposed method. In our study, each respondent was presented with a set of four images: the original object, an example image of the material to be transferred, the result generated by ZeST, and the result produced by our proposed method. Participants were asked to answer three specific questions:

Q1: Which image do you prefer? Assess the overall quality of the image: are details added or removed, is the image spoiled (e.g., noise, blurriness), and is it realistic?

Q2: Which image better transfers the features of the material? Can we say that the object is now made of this material or that it uses this material?

Q3: Which image better preserves the original object, including its outlines, details, and depth?


\section{Detailed Configuration of the Method and Baselines}
\label{appendix:configs}

All experiments comparing the methods were performed using the official repositories from the authors. The relevant code implementations and specific parameters for the method's inference, including those for our method, are listed in Table \ref{tab:configs}.






 
\section{Description of Evaluation Metrics}
\label{appendix:metrics}

In this appendix, we detail the metrics employed in our quantitative analysis, along with the calculation methods used.

\paragraph{Learned Perceptual Image Patch Similarity (LPIPS).} Learned Perceptual Image Patch Similarity (LPIPS) is utilized to assess the perceptual similarity between the original object images and those generated through various material transfer methods. This metric is crucial for our analysis as it allows us to evaluate the preservation of the background, object geometry, and intricate details within the images.

LPIPS computes the similarity between the activations of two image patches based on a pre-defined neural network. 

For our analysis, we employ AlexNet for feature extraction, which as a forward metric, according to the LPIPS GitHub documentation, performs the best. The process involves the following steps:

1. Feature Extraction: LPIPS extracts features from the original and generated images using the activations from specific layers of AlexNet, which captures crucial perceptual information about the images.

2. Similarity Computation: By comparing the extracted feature activations from the two images, LPIPS quantifies how similar they are in terms of perceptual content. A lower LPIPS score indicates high similarity between the original and generated images, while a higher score signifies greater divergence.

3. Average Calculation: Next, for each material transfer method, the average LPIPS score is calculated across all images in the dataset. 

\paragraph{CLIP similarity score.} To evaluate the effectiveness of material transfer, we utilize the CLIP similarity score. Calculating the similarity between two images using CLIP involves two main steps: first, we extract the features of both images, and then we compute their cosine similarity. A detailed explanation of the CLIP similarity score calculation within the context of our material transfer task can be found in the "Experiments" section of the article. This process is also illustrated in Fig.\ref{fig:clip}.


\begin{figure}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   
   %\vspace{-15pt}
   \includegraphics[width=0.96\linewidth]{images/clip_hr.JPG}

   \caption{CLIP-based similarity scoring using 64x64 and 128x128 crops of material-transferred and sample images, excluding background. Pairwise scores quantify texture similarity.}
   \label{fig:clip}

   \vspace{-5pt}
\end{figure}


\section{Material Transfer Force}
\label{appendix:material_transfer_force}
In this appendix, we provide a collection of examples illustrating the concept of material transfer force, as shown in Fig.\ref{fig:mat_force}. As seen in the image, the process begins with the transfer of texture from the material exemplar, followed by the transfer of color. This sequential representation highlights the distinct phases of material transfer.

\begin{figure*}[!t]
  \centering
   %\vspace{-20pt}
   \includegraphics[width=0.96\linewidth]{images/appendix_mat_force_1_compressed.png}
   \caption{Examples of controlled addition of material to an object. The increase in material transfer force can result in various outcomes, including changes in physical properties as well as modifications to texture and color. By maintaining precise control over the material transfer process, these modifications can be carefully implemented, ensuring that the desired characteristics are achieved without compromising the object's original design.}
   \label{fig:mat_force}

   %\vspace{-5pt}
\end{figure*}


