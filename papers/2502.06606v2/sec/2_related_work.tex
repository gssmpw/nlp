\section{Related Work}

\textbf{Material transferring.}
Research on material transfer has progressed considerably, with early work by Khan et al. \cite{khan2006image} introducing methods to render objects transparent and translucent using luminance and depth maps. More recent approaches use Generative Adversarial Networks (GANs) \cite{goodfellow2020generative} for high-quality material edits that adjust perceptual attributes like glossiness and metallicity, while maintaining the object’s geometric structure \cite{delanoy2022generative, subias2023wild}. These GAN-based methods facilitate the modification of material appearance from a single input image, allowing for flexible and visually coherent edits.

Diffusion models have also emerged as effective tools for material modification. Sharma et al. \cite{sharma2024alchemist} introduced a technique using Stable Diffusion v1.5 to control material properties, including roughness, metallicity, and transparency, directly in real images. Several diffusion-based methods have been developed for 3D texturing as well, such as Text2Tex \cite{chen2023text2tex} and TEXTure \cite{richardson2023texture}, which generate textures from object geometries and text prompts, and TextureDreamer \cite{yeh2024texturedreamer}, which transfers relightable textures to 3D shapes from a few input images. The work most similar to ours is ZeST \cite{cheng2024zestzeroshotmaterialtransfer}, a zero-shot material transfer method that applies exemplar materials from reference images to target objects, showcasing effective single-shot material editing without additional training.

% \textbf{Material transferring:}
% Research on material transfer has made significant advancements. For example, Khan et al. \cite{khan2006image} introduced techniques to make objects transparent and translucent, leveraging the luminance of each pixel and depth maps to achieve this effect. More recent studies have presented image-based editing methods that allow for the modification of an object's material appearance by adjusting high-level perceptual attributes using a single input image \cite{delanoy2022generative, subias2023wild}. These methods utilize Generative Adversarial Networks (GANs) \cite{goodfellow2020generative} to produce high-quality edits of material attributes, such as glossiness and metallic, while preserving the underlying geometric structure and fine details.

% Diffusion models have also been employed to modify material attributes. Prafull Sharma et al. \cite{sharma2024alchemist} proposed a technique that enables control over material properties, including roughness, metallicity, albedo, and transparency, in real images through the use of Stable Diffusion v1.5. 

% In addition, several approaches that utilize diffusion models have been developed for texturing 3D meshes \cite{yeh2024texturedreamer, chen2023text2tex, richardson2023texture}. For example, methods like Text2Tex \cite{chen2023text2tex}  and TEXTure \cite{richardson2023texture}  generate high-quality and consistent textures from given object geometries and textual prompts. Conversely, TextureDreamer \cite{yeh2024texturedreamer}, which is an image-guided texture synthesis technique, facilitates the transfer of relightable textures from a limited number of input images (ranging from 3 to 5) onto target 3D shapes.
% The most closely related work to our study is ZeST \cite{cheng2024zestzeroshotmaterialtransfer}. ZeST employs a zero-shot approach to material transfer, allowing for the application of a material exemplified in a reference image to the object present in the input image.


% “Null-text Inversion, InstructPix2Pix, IP-Adapter, Guide and Rescale, Prompt-to-Prompt, ControlNet, Self-guidance”
\paragraph{Diffusion Models for Image Editing.} Diffusion models have become essential in image editing, enabling detailed, high-quality transformations~\cite{ho2020denoising,dhariwal2021diffusion}. Methods such as Null-text Inversion~\cite{mokady2023null} and Prompt-to-Prompt~\cite{hertz2022prompt} allow edits on real images by adjusting text prompts or modifying cross-attention layers, preserving key visual content while providing control over specific areas. InstructPix2Pix~\cite{brooks2023instructpix2pix} extends this with instruction-driven edits, while ControlNet~\cite{zhang2023adding} leverages additional conditioning inputs like edge maps and segmentation masks for precise structure manipulation. However, these techniques often lack the fine-grained control needed for material-specific edits.

Self-guidance~\cite{epstein2023diffusion} and IP-Adapter~\cite{ye2023ipadaptertextcompatibleimage} enable image-based conditioning and layout preservation, with the Guide and Rescale (GaR) method~\cite{titov2024guideandrescaleselfguidancemechanismeffective} further refining spatial structure by preserving attention and feature maps during edits. These methods improve detail retention but can struggle with unintended background changes and fine material control. Our approach, MaterialFusion, combines IP-Adapter’s detailed material encoding with GaR’s geometric fidelity and includes a dual-masking strategy to limit material transfer to targeted areas. This unified approach addresses the limitations of existing methods, ensuring high-quality, controlled material transfer while maintaining background consistency.




