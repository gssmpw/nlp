\section{Related work}
% \subsubsection{Intuitive theories}
One of the main goals of machine learning research is to build machines that think and behave like humans. To meet this goal, \citet{lake2017building} proposed that human-like machine learning models must be capable of reasoning about their physical and social environment and its causal structure. These capabilities are sometimes summarized as \textit{intuitive theories}---the cognitive expectations humans and other animals have about their environment from early on in development that they use to behave adaptively.

In this paper, we focus on two classes of intuitive theory. \textit{Intuitive physics} relates to the ability to predict and understand the physical properties and interactions of inanimate objects \citep{battaglia2012computational, piloto2022intuitive}, an ability that is present very early in development and does not require extensive learning or experience \cite{baillargeon1995acquisition,spelke1990principles,spelke2007core}. \textit{Causal reasoning} describes the ability to infer cause-effect relationships \citep{waldmann2017oxford, pearl2009causality}. There is growing evidence that humans possess an intuitive capacity to infer and predict causal relationships \citep{griffiths2009theory}, and that this ability emerges early in development \cite{kuhn2012development,sobel2006blickets}. In the psychology literature, intuitive physics and causal reasoning have been studied most prominently in their relation to visual cognition---investigating how humans and other animals reason about their physical environment and its causal structure through the visual inputs they receive.

Vision language models (VLMs), which receive visual and textual linguistic input and produce textual output, have received recent attention for their apparently sophisticated reasoning in visual and linguistic tasks \cite{MMBench}. However, recent work has established that VLMs are still limited in their understanding of the physical world and its causal structure \citep{jin2023cladder, balazadeh2024synthetic}, suggesting that they lack human-like intuitive physics and causal reasoning. While VLMs perform reasonably well on intuitive physics problems, such as predicting the stability of block towers, they do not show a good fit with human behavioral data. On tests of causal reasoning, such as predicting whether removing a block would cause a tower to fall, VLMs perform poorly and again do not fit well with human behavior \citep{schulze2025visual}. Beyond the domains of intuitive physics and causal reasoning, VLMs have also been shown to have a number of visual deficiencies, and they often struggle with simple visual tasks that would be trivial for a human observer \citep{rahmanzadehgervi2024vision, schulze2025visual, balazadeh2024synthetic}. VLMs are prone to hallucinations, where the corresponding output does not sensibly correspond to the input image \citep{li2023evaluating, liu2024survey}. \citet{ullman2024illusion} shows that VLMs hallucinate visual illusions where there are none --- if the visual stimuli resemble canonical illusions that were likely in their training data. Similarly, \citet{zhang2023grounding} show that while the general alignment to human perception is low, larger models are somewhat susceptible to the same visual illusions as humans. Additionally, VLMs are not adversarially robust and subject to manipulation of both textual and visual inputs \citep{zhao2024evaluating}. 

\begin{figure*}[ht]
    \centering
    \vspace{0.2cm}
    \includegraphics[width=1.0\textwidth]{figures/cubeworld_overview_v3.pdf}
    \vspace{-0.4cm}
    \caption{Overview of the types of datasets used and types of generalization tested. \textbf{A:} We introduce fine-tuning and evaluation data sets for intuitive physics and causal reasoning from the artificial \emph{Cubeworld}. We also evaluate models on block towers from \citet{lerer2016learning}. \textbf{B:} For the majority of analyses models are fine-tuned on the ground truth in each domain, on data splits that contain either smaller or larger towers/pyramids. \textbf{C:} We first test whether fine-tuning improves model performance on new unseen towers from the same split. Then, we test if models generalize to unseen towers from the opposite split, i.e., taller/shorter towers compared to the fine-tuning data. We also test if the models can generalize to the same task presented with other visual characteristics, by using real images depicting block towers. Furthermore, we test if models can generalize to a related task in another cognitive domain with visuals that match the fine-tuning data. Finally, we assess the models' alignment with human judgments, and if fine-tuning on human judgments improves human alignment.}
    \label{fig:overview}
\end{figure*}

In pursuit of improving the performance of language models, \textit{fine-tuning} is quickly distinguishing itself as the gold standard, enabling researchers to efficiently steer models towards better capabilities \cite{han2024parameter} as well as towards more human-aligned outputs \cite{binz2024centaur, hussain2024tutorial}. In this paper, we explore whether fine-tuning VLMs can improve their performance on intuitive physics and causal reasoning tasks in the visual domain, as well as steer them towards more human-aligned outputs.

However, a hallmark of human cognition is not just the ability to reason about the physical environment and its causal structure, but also to robustly generalize from limited experience to solve new tasks \cite{collins2022structured,geirhos2018generalisation, griffiths2009theory}. Therefore, we seek to evaluate whether fine-tuning not only improves performance on visual cognition tasks sampled from an identical distribution, but also whether it produces models that can generalize to new, but related, tasks in new domains. For example, we ask whether a model fine-tuned to accurately judge the stability of short tower blocks can generalize this knowledge to judge the stability of tall tower blocks, of tower blocks with different visual characteristics (from a different environment), or to causal reasoning problems about tower blocks. Our results allow us to appraise the limits of fine-tuning for building performant, human-like machine learning models that can generalize beyond the kinds of data on which they have been trained. Across a range of datasets and models, we do not find evidence that fine-tuning alone can achieve all these objectives.

% \subsubsection{VLM fine-tuning}