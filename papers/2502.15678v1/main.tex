%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{dirtytalk}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}
% \usepackage[preprint]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Limits of fine-tuning in vision language models}

\begin{document}

\twocolumn[
\icmltitle{Testing the limits of fine-tuning to improve reasoning in vision language models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Luca M. Schulze Buschoff}{equal,hm}
\icmlauthor{Konstantinos Voudouris}{equal,hm}
\icmlauthor{Elif Akata}{hm,ut}
\\
\icmlauthor{Matthias Bethge}{ut}
\icmlauthor{Joshua B. Tenenbaum}{mit}
\icmlauthor{Eric Schulz}{hm}
\end{icmlauthorlist}

\icmlaffiliation{hm}{Institute for Human-Centered AI, Helmholtz Munich, Oberschleißheim, Germany}
\icmlaffiliation{ut}{University of T\"ubingen, T\"ubingen, Germany}
\icmlaffiliation{mit}{Department of Brain and Cognitive Sciences, MIT, Massachusetts, USA}
\icmlcorrespondingauthor{Luca M. Schulze Buschoff\linebreak}{lucaschulzebuschoff@gmail.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution{}} % otherwise use the standard text.

\begin{abstract}
% Pre-trained vision language models still fall short of human visual cognition. To investigate whether fine-tuning can improve visual cognition and align models with human behavior, we introduce new visual stimuli and human behavioral data. We find that fine-tuning can improve model performance in visual cognition tasks and that it can improve model alignment with human behavior. However, fine-tuning does not contribute to robust human-like generalization to more complex tasks and to other cognitive domains. 
 
% Pre-trained vision language models still fall short of human visual cognition. In an effort to improve visual cognition and align models with human behavior, we introduce new visual stimuli and human judgments from intuitive physics and causal reasoning tasks. We fine-tune models on ground-truth data from each domain and find that this improves model performance in the respective fine-tuning domain. Furthermore, it can improve model alignment with human behavior. However, models' performance improvement does not reliably transfer to data with other visual characteristics or to tasks in other cognitive domains. As such, we find that fine-tuning does not contribute to robust human-like generalization to more complex tasks and to other cognitive domains. 

Pre-trained vision language models still fall short of human visual cognition. In an effort to improve visual cognition and align models with human behavior, we introduce visual stimuli and human judgments on visual cognition tasks, allowing us to systematically evaluate performance across cognitive domains under a consistent environment. We fine-tune models on ground truth data for intuitive physics and causal reasoning and find that this improves model performance in the respective fine-tuning domain. Furthermore, it can improve model alignment with human behavior. However, we find that fine-tuning does not contribute to robust human-like generalization to data with other visual characteristics or to tasks in other cognitive domains. 

% However, what distinguishes human visual cognition is not just good performance on single tasks but also robust generalization to new tasks. While fine-tuning can improve model performance and partially align models with human behavior, we find that it does not contribute to robust generalization to more complex tasks and to other cognitive domains.
\end{abstract}

\section{Introduction}
\label{submission}

% \subsection{Related work}

% \subsubsection{Intuitive theories}
One of the main goals of machine learning research is to build machines that think and behave like humans. To meet this goal, \citet{lake2017building} proposed that human-like machine learning models must be capable of reasoning about their physical and social environment and its causal structure. These capabilities are sometimes summarized as \textit{intuitive theories}---the cognitive expectations humans and other animals have about their environment from early on in development that they use to behave adaptively.

In this paper, we focus on two classes of intuitive theory. \textit{Intuitive physics} relates to the ability to predict and understand the physical properties and interactions of inanimate objects \citep{battaglia2012computational, piloto2022intuitive}, an ability that is present very early in development and does not require extensive learning or experience \cite{baillargeon1995acquisition,spelke1990principles,spelke2007core}. \textit{Causal reasoning} describes the ability to infer cause-effect relationships \citep{waldmann2017oxford, pearl2009causality}. There is growing evidence that humans possess an intuitive capacity to infer and predict causal relationships \citep{griffiths2009theory}, and that this ability emerges early in development \cite{kuhn2012development,sobel2006blickets}. In the psychology literature, intuitive physics and causal reasoning have been studied most prominently in their relation to visual cognition---investigating how humans and other animals reason about their physical environment and its causal structure through the visual inputs they receive.

Vision language models (VLMs), which receive visual and textual linguistic input and produce textual output, have received recent attention for their apparently sophisticated reasoning in visual and linguistic tasks \cite{MMBench}. However, recent work has established that VLMs are still limited in their understanding of the physical world and its causal structure \citep{jin2023cladder, balazadeh2024synthetic}, suggesting that they lack human-like intuitive physics and causal reasoning. While VLMs perform reasonably well on intuitive physics problems, such as predicting the stability of block towers, they do not show a good fit with human behavioral data. On tests of causal reasoning, such as predicting whether removing a block would cause a tower to fall, VLMs perform poorly and again do not fit well with human behavior \citep{schulze2025visual}. Beyond the domains of intuitive physics and causal reasoning, VLMs have also been shown to have a number of visual deficiencies, and they often struggle with simple visual tasks that would be trivial for a human observer \citep{rahmanzadehgervi2024vision, schulze2025visual, balazadeh2024synthetic}. VLMs are prone to hallucinations, where the corresponding output does not sensibly correspond to the input image \citep{li2023evaluating, liu2024survey}. \citet{ullman2024illusion} shows that VLMs hallucinate visual illusions where there are none --- if the visual stimuli resemble canonical illusions that were likely in their training data. Similarly, \citet{zhang2023grounding} show that while the general alignment to human perception is low, larger models are somewhat susceptible to the same visual illusions as humans. Additionally, VLMs are not adversarially robust and subject to manipulation of both textual and visual inputs \citep{zhao2024evaluating}. 

\begin{figure*}[ht]
    \centering
    \vspace{0.2cm}
    \includegraphics[width=1.0\textwidth]{figures/cubeworld_overview_v3.pdf}
    \vspace{-0.4cm}
    \caption{Overview of the types of datasets used and types of generalization tested. \textbf{A:} We introduce fine-tuning and evaluation data sets for intuitive physics and causal reasoning from the artificial \emph{Cubeworld}. We also evaluate models on block towers from \citet{lerer2016learning}. \textbf{B:} For the majority of analyses models are fine-tuned on the ground truth in each domain, on data splits that contain either smaller or larger towers/pyramids. \textbf{C:} We first test whether fine-tuning improves model performance on new unseen towers from the same split. Then, we test if models generalize to unseen towers from the opposite split, i.e., taller/shorter towers compared to the fine-tuning data. We also test if the models can generalize to the same task presented with other visual characteristics, by using real images depicting block towers. Furthermore, we test if models can generalize to a related task in another cognitive domain with visuals that match the fine-tuning data. Finally, we assess the models' alignment with human judgments, and if fine-tuning on human judgments improves human alignment.}
    \label{fig:overview}
\end{figure*}

In pursuit of improving the performance of language models, \textit{fine-tuning} is quickly distinguishing itself as the gold standard, enabling researchers to efficiently steer models towards better capabilities \cite{han2024parameter} as well as towards more human-aligned outputs \cite{binz2024centaur, hussain2024tutorial}. In this paper, we explore whether fine-tuning VLMs can improve their performance on intuitive physics and causal reasoning tasks in the visual domain, as well as steer them towards more human-aligned outputs.

However, a hallmark of human cognition is not just the ability to reason about the physical environment and its causal structure, but also to robustly generalize from limited experience to solve new tasks \cite{collins2022structured,geirhos2018generalisation, griffiths2009theory}. Therefore, we seek to evaluate whether fine-tuning not only improves performance on visual cognition tasks sampled from an identical distribution, but also whether it produces models that can generalize to new, but related, tasks in new domains. For example, we ask whether a model fine-tuned to accurately judge the stability of short tower blocks can generalize this knowledge to judge the stability of tall tower blocks, of tower blocks with different visual characteristics (from a different environment), or to causal reasoning problems about tower blocks. Our results allow us to appraise the limits of fine-tuning for building performant, human-like machine learning models that can generalize beyond the kinds of data on which they have been trained. Across a range of datasets and models, we do not find evidence that fine-tuning alone can achieve all these objectives.

% \subsubsection{VLM fine-tuning}
\subsection{Related Work}
Closest to our work is \citet{balazadeh2024synthetic}, who fine-tune the VLM PaliGemma-3B \citep{beyer2024paligemma} on a series of intuitive physics and visual reasoning tasks, asking questions about the height, color, and shape of tower blocks in an image, as well as whether the towers are stable or certain blocks are likely to move. They find that smaller fine-tuned VLMs can outperform larger non fine-tuned models on the fine-tuning task. However, they do not investigate whether fine-tuned VLMs can generalize to new problems. \citet{ming2024does} explore VLMs' ability to generalize to out-of-distribution labels in an image classification task, presenting evidence that fine-tuning noticeably improves performance. However, they do not investigate generalization in more complex, psychologically-inspired domains like intuitive physics or causal reasoning.

\subsection{This Work}

In this work, we fine-tune VLMs on tasks from two cognitive domains, intuitive physics and causal reasoning, using tasks designed in a virtual environment we call \textit{Cubeworld}, in which block stacks are constructed from colored cubes and are subject to realistic physical forces. We then evaluate the fine-tuned models' ability to generalize to four different conditions (see Figure \ref{fig:overview}):
% First, a held-out test set randomly sampled from the same distribution as the fine-tuning data. For example, model fine-tuned to judge the stability of towers consisting of 2--4 blocks is then tested on new tower blocks consisting of 2--4 blocks. Second, we evaluated models on a test set of new towers, where the model makes the same judgment (e.g., stability). For example, a model fine-tuned on 2--4 block towers is tested on 5--7 block towers. Third, we evaluated models on a test set of towers with different visual characteristics. For example, a model fine-tuned on 2--4 block towers from the \textit{Cubeworld} environment is tested on towers with 2--4 blocks from a different environment. Finally, we evaluated models on a test set of towers from a new domain. For example, a model fine-tuned to make stability judgments (intuitive physics) is tested on its ability to make counterfactual stability judgments (causal reasoning).

\setlist{nolistsep}
\begin{enumerate}[noitemsep]
    \item A held-out test set randomly sampled from the same distribution as the fine-tuning data. \textit{Example}: A model fine-tuned to judge the stability of towers consisting of 2--4 blocks is then tested on new unseen towers consisting of 2--4 blocks.
    \item A test set of new block stacks, on the same task and domain as the fine-tuning data (e.g., tower stability). \textit{Example}: A model fine-tuned on 2--4 block towers is tested on 5--7 block towers.
    \item A test set of block stacks from the same task and domain but with different visual characteristics. \textit{Example}: A model fine-tuned on 2--4 block towers from the \textit{Cubeworld} environment is tested on real block towers with 2--4 blocks from \citet{lerer2016learning}.
    \item A test set of block stacks from a new cognitive domain that shares the same visual characteristics. \textit{Example}: A model fine-tuned to make stability judgments (intuitive physics) is tested on its ability to make counterfactual stability judgments (causal reasoning).
\end{enumerate}
In each case, we not only measure how well the models perform in each context, but also how well their performance aligns with human data on identical tasks. We conduct counterbalanced evaluations, testing the interactions between tower sizes, visual characteristics, and cognitive domain (intuitive physics vs. causal reasoning). Finally, we fine-tune models on human judgments to test if this leads to better human alignment. 

\section{Methods}
\subsection{Fine-Tuning \& Evaluation Data}
We generated four new data sets, two data sets for intuitive physics, and two for causal reasoning. For each cognitive domain, we used one set for fine-tuning and one set for evaluation. All four data sets, which we collectively call \emph{Cubeworld}, consist of different configurations of colored blocks generated using ThreeDWorld (\citeauthor{gan2020threedworld}, \citeyear{gan2020threedworld}, TDW), a physics simulation engine built in Unity \citep{unity}. 

We generated similar stimuli for both domains to ensure that models can, in theory, transfer knowledge between them. This allows us to test generalization within domains (such as fine-tuning models on physical stability judgments of small towers and testing on bigger towers) and between domains (such as fine-tuning models on physical stability judgments and testing them on counterfactual stability judgments). 

\newpage
\textbf{Intuitive physics}~~~
For intuitive physics, we generated block towers that consist of stacks of single colored blocks in a minimal gray room (see section \ref{app:intphys-examples} in the Appendix). Block towers such as these have been used extensively to investigate intuitive physics in humans and machines \citep{battaglia2013simulation, lerer2016learning}. 

The towers consist of 2 to 7 blocks and their rotation, size, color, and offset are sampled randomly. Offset distributions become more constrained as the number of blocks increases, so that randomly sampling offsets leads to a roughly 50/50 split between stable and unstable configurations for all tower sizes. This is to ensure that the distributions of stable and unstable towers have comparable difficulties: both contain easy \emph{canonical} configurations as well as configurations that are harder to judge. The models are presented with an image of a block tower and they must judge if it is \emph{stable} or \emph{unstable.}

\textbf{Causal reasoning}~~~
For causal reasoning, we generated colored block pyramids in a minimal gray room, inspired by the stimuli in \citet{zhou2022mental} (see section \ref{app:caus-examples} in the Appendix). The pyramids are made up of 2 to 5 rows with the bottom row consisting of as many blocks as there are rows in total, and each consecutive row featuring one less block than the row below (resulting in a range of 3 to 15 blocks in total). The color of each block is sampled randomly and the offset and sizes are sampled within ranges that still allow for a stable pyramid. 

Each pyramid features a red block. The models are asked if any other blocks would fall if the red block would not be there, similar to the the protocol of \citet{zhou2022mental}. This question requires the models perform the counterfactual simulation of computing the stability of the tower without the red block. We randomly sample the position of the red block so that it is never on the top of the pyramid and so that it has an equal chance of being in every row of a pyramid. 

\textbf{Naturalistic Data}~~~
To study whether models could generalize to data with other visual characteristics, we used a sample of 100 intuitive physics tower block images from \citet{lerer2016learning} (see section \ref{app:lerer-examples} in the Appendix). This dataset consists of pictures of real block towers with 2, 3, and 4 blocks that are either stable or unstable. The images look different to \emph{Cubeworld}, but the underlying cognitive task is the same as in the intuitive physics data set. Human data for this task was taken from \citet{schulze2025visual}, who collected 107 participants on 100 randomly selected images from the experiment by \citet{lerer2016learning}.

\subsection{Models}
We fine-tune the 2B and 7B parameter versions of the Qwen2-VL model \citep{qwen2vl} using the \textit{unsloth} library \citep{unsloth}. It uses a Vision Transformer (ViT) \citep{dosovitskiy2020image} with approximately 675 million parameter as its' vision encoder, and the decoder-only Qwen2 \citep{qwen2} language model for processing text. We used pre-trained models quantized to 4-bit precision (see section~\ref{app:vlm-details} in the Appendix for further details on model pretraining).

We evaluate the models by sampling the log probabilities of the \say{Yes} and \say{No} tokens conditional on the input and normalizing them by exponentiating and then using the softmax function.\footnote{$\text{softmax}(\mathbf{p}) = \frac{e^{p_i}}{\sum_j^{K}e^{p_j}}$ where $\mathbf{p}$ is the vector of probabilities of length $K$ and $e$ is the exponential function.} This gives a measure of the relative probability of the model answering \say{Yes} or \say{No} to each question. We then evaluate whether the model is correct by examining which token is assigned the higher probability and comparing this to the ground truth (see section~\ref{app:analysistools} in the Appendix for information on the packages used for analysis). We also elicited free text responses from the model and found that these aligned with the normalized token probabilities anyway.

\subsection{Prompts}
\label{sec:prompts}
For intuitive physics, we prompt the models with the following pre-prompt: \say{\emph{You are now viewing a tower of blocks. Will the tower fall? Answer Yes if you think this tower is unstable and will fall. Answer No if you think this tower is stable and will not fall.}} 

For the causal reasoning pyramids, we prompt the models with this pre-prompt: \say{\emph{You are now viewing a pyramid of blocks. If the red block was not there, would any other blocks fall? Answer Yes if you think that other blocks would fall if the red block was not there. Answer No if you think that no other blocks would fall if the red block was not there.}}

\subsection{Fine-Tuning Procedure}\label{sec:fine-tuning-proc}
% Add some info: slurm based cluster running 80GB A100s 
% I've mentioned we did all training on A100s, but not sure we need to mention the slurm part
We used Parameter Efficient Fine-Tuning (PEFT; \citeauthor{han2024parameter}, \citeyear{han2024parameter}), focusing on training low-rank adapters for quantized models (QLoRA; \citeauthor{dettmers2024qlora}, \citeyear{dettmers2024qlora}; \citeauthor{hu2021lora}, \citeyear{hu2021lora}). PEFT is quickly becoming the dominant fine-tuning paradigm, blending high performance with computational and memory efficiency. PEFT selectively adjusts only a small number of model parameters during training, which not only reduces computational overhead but also minimizes overfitting and the prospect of existing knowledge being washed out by subsequent training (catastrophic forgetting; \citeauthor{french1999catastrophic}, \citeyear{french1999catastrophic}). QLoRA is an approach to PEFT where the model is first quantized, reducing its memory footprint by reducing the precision of the models weights and activations, and then injecting small \textit{adapter} layers into the transformer blocks of the VLM, both for the vision encoder and the autoregressive text decoder. For the weight matrix, $W$, of any layer, an accompanying adapter layer, $W_a$, is injected. $W_a$ is the product of two low-rank matrices $L_1$ and $L_2$ where $L_1 \in \mathbb{R}^{d\times r}$ and $L_2 \in \mathbb{R}^{r\times k}$ where $r$ is much smaller than $d$ and $k$, the dimensionality of the input and output and respectively. Given some input $x$, it is transformed by both $W$ and $W_a$ independently and then summed (subject to scaling $\alpha$), to produce the output. In QLoRA, only the values of $L_1$ and $L_2$ are altered; $W$ remains fixed. The weights of $L_1$ and $L_2$ are altered by backpropagation under the supervision of the next token in a document, using a cross-entropy loss. Given the relatively small $r$, models can be trained much more quickly than through training the full-rank $W$ matrix. We chose $r=16$ for all experiments and a scaling of $\frac{r}{\alpha}$ where $\alpha=16$, thus balancing the effect of $W$ and $W_a$ on the outputs. We fine-tuned layers in the ViT vision encoder, and attention and MLP layers in the language decoder, as this has been shown to be effective in prior work on fine-tuning for intuitive physics understanding \citep{balazadeh2024synthetic}. We used the ADAM optimizer and an initial learning rate of $0.0002$. We fine-tuned all models for 10 epochs on 10,000 text-image pairs on 80GB NVIDIA A100 GPUs.


\subsection{Human experiments}
\label{sec:human_exp}
We performed three separate human experiments to obtain fine-tuning and evaluation data. All participants agreed to take part in the study and were informed about the general purpose of the experiment. Experiments were conducted on Prolific in accordance with the relevant guidelines and regulations approved by the ethics committee of the University of Tübingen. For information on the samples, durations, and payout of the experiments, see Section \ref{sec:demographics} in the Appendix. 

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/qwen_7b_all_results.pdf}
    \vspace{-0.6cm}
    \caption{Heat map showing accuracies for the 7B model on all combinations of ground truth fine-tuned models and evaluations. Each row contains a model fine-tuned on the ground truth for a specific data split. Each column contains the results for a specific block number in the respective evaluation data set. Models fine-tuned on a single cognitive domain do not generalize to the other cognitive domain. Models fine-tuned on both cognitive domains can perform well on either domain and show more robust performance on the realistic intuitive physics block towers from \cite{lerer2016learning}.}
    \label{fig:all_results}
    \vspace{-0.2cm}
\end{figure*}

\textbf{Intuitive physics}~~~
For the majority of results reported here, the model is fine-tuned on the ground-truth of the generated block configuration. However, we also fine-tune the models on human responses. For this purpose, we collected individual human responses for each image in the 2--4 block tower intuitive physics fine-tuning data set. We collected 100 responses on average from 100 human participants to cover the 10,000 images in the fine-tuning data set. In this experiment, all participants received different images and were given the same pre-prompt as the models in the intuitive physics experiment (see Section \ref{sec:prompts}).

We also collected the responses of 100 separate participants on the same subset of 120 images from all conditions in the evaluation set for the intuitive physics tower task (6 tower sizes $\times$ stable / unstable $\times$ 10 images per condition). This allows us to compute similarities between human and model judgments. Participants received the images in a random order and were given the same prompt as in the experiment above. 

\newpage
\textbf{Causal reasoning}~~~
For the causal reasoning pyramids, we also collected a human evaluation data set of 100 separate participants on the same 80 images in the evaluation set (4 pyramid sizes $\times$ stable / unstable $\times$ 10 images per condition). Participants received the images in a random order and were given the same pre-prompt as the models in the causal reasoning experiment (see Section \ref{sec:prompts}).

\section{Results}
First we fine-tune on the ground truth. We evaluate whether this leads to improved performance (\ref{subsec:ft-perf}), different types of generalization (\ref{subsec:ft-size}--\ref{subsec:ft-new-task}) and alignment to human judgments (\ref{subsec:ft-alignment}). We then fine-tune a model on human responses on the same task (\ref{subsec:ft-human}), which leads to better human alignment. All results reported here are averaged over three seeds. The random seed changes the random initialisation of the fine-tuning weights and subset of the fine-tuning data.

\subsection{Fine-tuning performance improvement}\label{subsec:ft-perf}
Fine-tuning substantially improves the performance of most models compared to the zero-shot case. Fully fine-tuned 7B models achieve accuracies between $0.61$ and $0.91$ on single block sizes from the split they were fine-tuned on (see Fig.~\ref{fig:all_results}), compared to the zero-shot base models, which perform at around chance for all tower and pyramid sizes (see Figs.~\ref{fig:phys_two_panel}B and \ref{fig:caus_two_panel}B in Appendix \ref{app:perf-over-time}). For 2B models, we find that performance improvement is more volatile between different random seeds.

For the intuitive physics fine-tuned models, we find that both the 2B and the 7B models fine-tuned on 2--4 block towers achieve accuracies between $0.68$ and $0.83$ on towers from their fine-tuning distribution (see Fig.~\ref{fig:performance_improvement}A). This picture is not as clear for the models fine-tuned on 5--7 block towers, with the 2B and 7B models showing more or less similar performance improvements over all tower sizes. 

The models might have difficulty learning from the 5--7 block towers because judging the stability of a tower becomes harder as it increases in size. This is mirrored in human performance on the evaluation data set, with human average accuracies of $0.62$, $0.71$, and $0.68$ for towers of size 2, 3, and 4, and average accuracies of $0.65$, $0.58$, and $0.56$ for towers of size 5, 6, and 7. The mean human accuracy over all towers was $0.63$.

\begin{figure*}[!t]
    \vspace{-0.2cm}
    \centering\includegraphics[width=1.0\textwidth]{figures/qwen_7b_in_domain.pdf}
    \vspace{-0.6cm}
    \caption{\textbf{A:} Models fine-tuned on two splits of intuitive physics towers with either 2--4 or 5--7 blocks and evaluated on all tower sizes. Performance for models fine-tuned on 2--4 block towers decreases for bigger towers. Models fine-tuned on 5--7 block towers either struggle to improve at all (2B) or show a constant improvement over all tower sizes (7B). \textbf{B:} Models fine-tuned on two splits of causal reasoning pyramids with either 3 \& 6 or 10 \& 15 blocks and evaluated on all pyramids sizes. Causal reasoning fine-tuned models improve in performance more consistently over the different pyramid sizes.}
    \label{fig:performance_improvement}
    \vspace{-0.3cm}
\end{figure*}

We find that the 7B models fine-tuned on causal reasoning improve in performance on all pyramid sizes regardless of their fine-tuning split (see Fig.~\ref{fig:performance_improvement}B). Similarly, the 2B models also show a constant, although lower, performance improvement over all block sizes. This is likely because the causal reasoning data set is easier to learn. Human participants had an average accuracy of $0.72$, with accuracies of $0.68$ and $72$ for 3 and 6 block pyramids, and accuracies of $0.75$ and $0.76$ for 10 and 15 block pyramids.

\subsection{Generalizing to taller and shorter towers}\label{subsec:ft-size}
For the 2B and 7B models fine-tuned on 2--4 towers, we see that they are able to somewhat generalize to bigger towers (see Fig.~\ref{fig:performance_improvement}A). While their performance decreases as the number of blocks increases, it is still above that of the base model even for bigger towers. 

In contrast, the models fine-tuned on 5--7 towers do not show a strong difference in performance between towers that were in- and out-of their fine-tuning distribution. Crucially, they only performs as well on the 5--7 block towers as the 2--4 fine-tuned models, even though these latter models have to generalize from their fine-tuning distribution to bigger towers.

The performance of the causal reasoning fine-tuned models is more constant over different pyramid sizes (see Fig.~\ref{fig:performance_improvement}B). Still, models fine-tuned on 10 \& 15 block pyramids perform slightly worse on 3 block pyramids (see Fig.~\ref{fig:phys_two_panel}A).

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/qwen_7b_hft.pdf}
    \vspace{-0.6cm}
    \caption{Error consistency to ground truth and human raters on all three evaluation datasets. Bars show (1) the 7B model fine-tuned on the ground truth for 2--4 block towers, (2) the overall best perfoming joint 7B model which is fine-tuned on the ground truth for 5--7 block towers and 3 \& 6 block pyramids, (3) the 7B model fine-tuned on human judgments for 2--4 block towers, and (4) the human judgments given by participants in each dataset. \textbf{A:} Results for the \textit{Cubeworld} block towers. The ground truth fine-tuned model is fine-tuned on the ground truth for this data set and therefore shows a high $\kappa$ to the ground truth here. In contrast, the human judgment fine-tuned model shows a lower $\kappa$ to the ground truth but a higher $\kappa$ to humans. \textbf{B:} Results for the natural block towers. The ground truth fine-tuned model is fine-tuned on the artificial images and has trouble generalizing to the natural images. It therefore shows a lower $\kappa$ to the ground truth here, as well as to human judgments. The human judgment fine-tuned model more robustly generalizes to the ground truth and human judgments in this task. \textbf{C:} Results for the causal reasoning pyramids. Neither the intuitive physics ground truth nor the human judgment fine-tuned model generalizes to the causal reasoning data.}
    \label{fig:human_alignment}
    \vspace{-0.2cm}
\end{figure*}

\subsection{Generalizing to a different visual quality}\label{subsec:ft-vision}
% \emph{How much does the performance improvement transfer from the training setting to a setting that is in-domain but with visually different stimuli (real images depicting block towers)?} 

To ascertain to what extent fine-tuned models can generalize to the same task with different visual characteristics, we tested them on real images depicting block towers from \citet{lerer2016learning}. We find that models fine-tuned on a single domain do not generalize well to all tower sizes in the Lerer et al. (2016) dataset (see Fig.~\ref{fig:all_results}). For example the model fine-tuned on 2--4 block towers in \textit{Cubeworld}, which is identical to the Lerer data in task and the number of blocks, only performs above chance on towers with 4 blocks from the Lerer dataset (see Fig.~\ref{fig:lerer_two_panel} in Appendix \ref{app:gen-lerer}). 
% The models were fine-tuned on synthetic images of block towers. To understand how much of the performance improvement is due to the models getting used to the visual characteristics of the scene, and to test how well the models generalize to the same experimental setup with different visual characteristics, we test them on real images depicting block towers from \citet{lerer2016learning}. 

We also fine-tune joint models on combined halves of 5,000 data points from each of the domains. We again find that these models do not perform well on all Lerer towers (see \emph{Joint} rows in Fig.~\ref{fig:all_results}). Indeed, there appears to be a trade-off where models that perform well on 2--4 block towers in \textit{Cubeworld} perform poorly on 2--3 block towers from the Lerer dataset.
% do not generalize to the same task with different visual characteristics (see the \emph{Lerer et al. (2016)} columns in Fig.~\ref{fig:all_results}). All models perform around chance for the real block towers (see also Fig.~\ref{fig:lerer_two_panel} in the Appendix), regardless of whether they are trained on data from the same cognitive domain and on the same number of blocks (such as the intuitive physics model fine-tuned on 2--4 block towers) or trained on another cognitive domain with pyramids consisting of a larger number of blocks altogether (such as the causal reasoning model fine-tuned on 10 \& 15 block pyramids). 

\subsection{Generalizing to a new task}\label{subsec:ft-new-task}
% \emph{How much does the performance improvement transfer from the training setting to another cognitive domain (causal reasoning) that has the same visual quality? Do you get more transfer going from easier task to harder task (physics to causal reasoning) or from harder task to easier task (causal reasoning to physics)?}

Models were fine-tuned on intuitive physics towers or causal reasoning pyramids from \emph{Cubeworld}. To test how well models generalize to another task in another cognitive domain, we evaluate them on the task they were not fine-tuned on.

Reasoning about tower stability is a prerequisite for counterfactual judgments on tower stability. This is especially obvious for the 3 block tower pyramids, where computing the counterfactual requires a tower stability judgment on a two block tower. Therefore, we would expect an improvement in causal reasoning to carry with it improvement on intuitive physics as well. 

However, we find that no model fine-tuned on a single cognitive domain performs well on the other cognitive domain (see Fig.~\ref{fig:all_results} and Figs.~\ref{fig:all_models_gen_domain}--\ref{fig:physgen_two_panel} in Appendix \ref{app:gen-new-domains}). However, models fine-tuned on a mixture of data from both tasks can achieve good performance in both domains, with only slight performance decrements in either domain. This confirms that the models have the capacity of solving both tasks at the same time. Still, models that are fine-tuned on a single cognitive domain are unable to generalize to the other domain. 

\subsection{Alignment with human judgments}\label{subsec:ft-alignment}
% \emph{How much does fine-tuning on the ground truth transfer to model judgments being human-like?}
To analyze the alignment of model judgments with human judgments, we use Cohen's $\kappa$ \citep{geirhos2020beyond}, 
a single behavioral score that disentangles the observed overlap between two observers from their accuracies \footnote{$\kappa_{i,j} = \frac{c_{\text{obs}_{i,j}}-c_{\text{exp}_{i,j}}}{1-c_{\text{exp}_{i,j}}}$ where $c_\text{obs}$ is the observed error overlap defined as $c_{\text{obs}_{i,j}} = \frac{e_{i,j}}{n}$  with $e_{i,j}$ as the number of equal responses and $c_\text{exp} = p_ip_j + (1 - p_i)(1 - p_j)$, the expected overlap that two observers $i$ and $j$ with accuracies $p_i$ and $p_j$ will have by chance.}.

To arrive at a single mean $\kappa$ between models and humans, we calculate $\kappa$ for each combination of models and humans and take the mean over the $\kappa$ values. We produce bootstrapped 95\% confidence intervals by computing the central 95 percentiles over 10,000 random subsamples of the $\kappa$ distribution. For comparisons to the ground truth, we use the central 95 percentiles of the distribution over mean $\kappa$ for 10,000 random subsamples of the item level judgments.

The 7B models fine-tuned on the ground truth intuitive physics 2--4 block towers have a mean $\kappa$ of $0.17$ with humans on the same task, but only $0.1$ on the Lerer task, and $0.01$ on the causal reasoning pyramids (see Fig.~\ref{fig:human_alignment}). 

In contrast, the 7B model fine-tuned on the ground truth causal reasoning 3 \& 6 block pyramids has a mean $\kappa$ of $0.42$ with humans on the same task, but only $-0.03$ on the Lerer task, and $0.01$ on the intuitive physics block towers.

We see that fine-tuning on the ground truth leads to some alignment with human judgments on the fine-tuning task. However, this does not transfer well to human judgments on the same task with other visual characteristics, and not at all to human judgments on another cognitive domain. 

\subsection{Fine-tuning on human data}\label{subsec:ft-human}
We collected single human responses for each of the intuitive physics 2--4 block towers in the fine-tuning data set (see section \ref{sec:human_exp}), allowing us to fine-tune models on the human responses instead of the ground truth. The correlation between the human fine-tuning data and the ground truth was $0.27$, with an overlap in labels of $63.5$\%. 

We find that fine-tuning on human responses leads to a considerable performance improvement on judging the ground truth stability of intuitive physics block towers. %(see Fig.~\ref{fig:human_finetune_performance} in Appendix \ref{app:human-ft}).
Even though the model is fine-tuned on human judgments on towers of size 2–4, it learns to predict the ground truth stability of bigger towers as well. We see the same pattern emerge as before, where model accuracy decreases as the number of blocks increases, albeit with overall slightly lower accuracies compared to the model fine-tuned on the ground truth.  

Furthermore, fine-tuning the model on human judgments increases the mean $\kappa$ with human judgments on the same data set to $0.35$ (see Fig.~\ref{fig:human_alignment}A). Additionally, it leads to a higher mean $\kappa$ of $0.31$ with human judgments on the same task with different visual characteristics and a better transfer on the ground truth performance (see Fig.~\ref{fig:human_alignment}B). It however does not transfer to the causal reasoning domain (see Fig.~\ref{fig:human_alignment}C). Here, the mean $\kappa$ to humans is $-0.01$.
  
% \subsection{To-Do until rebuttal}
% \begin{itemize}
%   \item Run more seeds, temperatures, and model families
%   \item Ablation experiment: train on number of blocks (does learning about just the visuals help you?) 
% \end{itemize}

%\newpage
\section{Discussion}
% What did we set out to do? 

%Despite excellent performance by VLMs on many computer vision and natural language tasks \cite{MMBench,zhang2024vision} 
Previous work has shown that pre-trained VLMs struggle with several aspects of visual cognition, particularly in causal reasoning and intuitive physics \cite{schulze2025visual}. We find that fine-tuning on intuitive physics and causal reasoning tasks can improve the performance of VLMs in these cognitive domains, and that it improves alignment with human judgments.

% We find that fine-tuning on intuitive physics and causal reasoning improves performance significantly in some cases but not in others. For instance, fine-tuning on intuitive physics tasks labeled with the ground truth leads to models that accurately judge tower stability better than humans. Meanwhile, fine-tuning on intuitive physics tasks labeled with human responses leads to models that agree more with humans on stability judgments than humans do with each other, on average. Fine-tuning models on 2--4 block towers also leads to generalization to taller towers, and fine-tuning on any split of short or tall pyramids also leads to good generalization to the opposite split on the causal reasoning task. 

However, there is also evidence that fine-tuned VLMs' reasoning about the physical world and its causal structure is brittle. On the naturalistic intuitive physics data from \citet{lerer2016learning}, models fine-tuned on \emph{Cubeworld} data in the same domain and task perform below chance level for some tower sizes.

Similarly, neither models fine-tuned on intuitive physics nor models fine-tuned on causal reasoning successfully generalize to the other cognitive domain. This result is particularly notable for the models fine-tuned on causal reasoning, as the ability to make physical stability judgments is a necessary precursor capability for judging the counterfactual stability of a tower. Models' inability to generalize to another cognitive domain is not due to them being limited in parameters or potential ability --- models fine-tuned on a mixture of intuitive physics and causal reasoning data performed well in both domains.  

Indeed, models fine-tuned on data from both tasks also somewhat generalize to the realistic block towers from \citet{lerer2016learning}, suggesting that data diversity is beneficial for generalization performance. These models also show higher agreement with human judgments on both the artificial and realistic datasets.

We present a first investigation on the limitations of fine-tuning for visual cognition. There are several avenues for future research to improve our understanding of fine-tuning and how well fine-tuned models can generalize: 

First, it is possible that robust generalization from the fine-tuning domain to another can only emerge with larger models. We studied the effects of fine-tuning on 2B and 7B models, which are relatively small in the context of large language models. Future work should therefore explore fine-tuning to improve larger models. 

Second, alternative fine-tuning procedures may improve outcomes. While we do not find evidence of overfitting \textit{per se}, it is possible that the models have overfitted in a more general, task-level sense. The models may have been sensitive to non-robust predictive features of the fine-tuning data in a particular domain that led to good performance there but not on new domains or with naturalistic data \cite{ilyas2019adversarial, geirhos2020shortcut}. Parametrizing the QLoRA procedure with different $r$ and $\alpha$ may improve generalization performance by modulating knowledge distillation and the relative effects of model weights and adapter weights. Similarly, introducing greater variance into the fine-tuning datasets as well as fine-tuning on larger volumes of data might improve model performance. % The robustness of these results can also be strengthened by running repetitions of our experiments with multiple seeds and data subsets. 

Finally, the visual and cognitive demands of these tasks are entangled. Models require visual abilities to detect and distinguish blocks, appraise distances, and judge three-dimensional volumes from two-dimensional images. They also need an understanding of gravity, mass, inertia, and friction. Future work should explore whether providing information about these properties in symbolic or schematic form can lead to improved performance and better generalization in these tasks.

%YOU NEED SOME KIND OF CONCLUSION?SYNTHESIS. HERE"S MY ATTEMPT:
Our findings underscore the limits of fine-tuning in achieving robust generalization in vision-language models. PEFT noticeably improves performance on tasks closely similar to the fine-tuning data, enabling generalization not just to new data sampled from the fine-tuning distribution but also to, for example, taller and shorter towers and pyramids---an out-of-distribution problem. Moreover, PEFT can align models more closely with human behavior in these contexts. 

However, fine-tuning does not lead to the broad, flexible reasoning abilities that characterize human cognition. Models fine-tuned on one cognitive task (e.g., intuitive physics) fail to generalize to another (e.g., causal reasoning), despite clear conceptual overlap, and models struggle to reliably extrapolate their knowledge to real-world images with different visual properties. 

Together, these results suggest that current approaches to fine-tuning are limited in the ways that they can improve models, and remain insufficient for developing models that reason about the physical and causal structure of the world in a way that mirrors human cognition. Achieving this level of generalization may require different training and fine-tuning paradigms that go beyond parameter-efficient adaptation.

\section*{Acknowledgements}
We thank Marcel Binz, Can Demircan, and Julian Coda-Forno for helpful discussions and feedback on the manuscript. 

% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\bibliography{bib}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Data examples}
\subsection{Intuitive physics}\label{app:intphys-examples}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=.75\textwidth]{figures/phys_stable_examples.pdf}
    \vspace{-.25cm}
    \caption{Stable examples from the \emph{Cubeworld} intuitive physics block tower evaluation set.}
    \label{fig:phys_stable}
\end{figure*}

\begin{figure*}[!hb]
    \centering
    \includegraphics[width=.75\textwidth]{figures/phys_unstable_examples.pdf}
    \vspace{-.25cm}
    \caption{Unstable examples from the \emph{Cubeworld} intuitive physics block tower evaluation set.}
    \label{fig:phys_unstable}
\end{figure*}

\newpage
\subsection{Causal reasoning}\label{app:caus-examples}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.55\textwidth]{figures/caus_stable_examples.pdf}
    \vspace{-.25cm}
    \caption{Stable examples from the \emph{Cubeworld} causal reasoning pyramid evaluation set.}
    \label{fig:caus_stable}
\end{figure*}

\begin{figure*}[!hb]
    \centering
    \includegraphics[width=0.55\textwidth]{figures/caus_unstable_examples.pdf}
    \vspace{-.25cm}
    \caption{Unstable examples from the \emph{Cubeworld} causal reasoning pyramid evaluation set.}
    \label{fig:caus_unstable}
\end{figure*}

\newpage
\subsection{Lerer}\label{app:lerer-examples}
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=.75\textwidth]{figures/lerer_examples.pdf}
    \vspace{-.25cm}
    \caption{Examples from the \citet{lerer2016learning} evaluation set.}
    \label{fig:lerer}
\end{figure*}

\section{Human experiment information and demographics}
\label{sec:demographics}
\textbf{Intuitive physics: human fine-tuning data}~~~
Participants received a base pay of 1.5\$ and an additional bonus of 0.01\$ for each correct answer, bringing the maximum payout to 2.5\$. Completing the experiment took participants 09:54 minutes on average. All participants were native English speakers from the UK and the US with a mean age of $30.34$ and a split of 55 females to 57 males.

\textbf{Intuitive physics: human evaluation data}~~~
Participants received a base pay of 1.5\$ for their participation and an additional bonus of 0.01\$ for each correct answer, bringing the maximum payout to 2.7\$. Completing the experiment took participants 11:37 minutes on average. All participants were native english speakers from the UK and the US and had a mean age of $30.46$ and a split of 50 females to 50 males.

\textbf{Causal reasoning: human evaluation data}~~~
Participants received a base pay of 1\$ for their participation and an additional bonus of 0.01\$ for each correct answer, bringing the maximum payout to 1.8\$. Completing the experiment took participants 09:07 minutes on average. All participants were native English speakers from the UK and the US with a mean age of $31.32$ and a split of 50 females to 50 males.

\section{Vision Language Model Pre-Training}\label{app:vlm-details}

Qwen2-VL is trained in three stages. First, the Qwen2 model weights are frozen and the ViT encoder is trained on 600 billion tokens of image-text pairs. Then, all parameters are unfrozen and the whole model is trained on a further 800 billion tokens of image-related data on a wide range of tasks including visual-question answering. In the final instruction-tuning phase, the ViT parameters are frozen and the LLM is fine-tuned on an undisclosed volume of multimodal conversational data (see \citet{qwen2vl} for more information).

%While this process is set up to expose the model to data from different tasks and domains, previous studies found that VLMs often struggle with simple visual understanding of artificial scenes \citep{schulze2025visual, rahmanzadehgervi2024vision}. We therefore fine-tune VLMs on scenes from a virtual environment, ensuring that they are well adjusted to its' artificial visual characteristics, before testing the models' visual cognitive abilities. 

\section{Analysis Tools}
\label{app:analysistools}
We analyze all data using Python 3.12.7 using pandas 2.2.2 \citep{reback2020pandas}, seaborn 0.13.2 \citep{Waskom2021}, matplotlib 3.9.2 \citep{Hunter2007}, and statsmodels 0.14.2 \citep{seabold2010statsmodels}.

\newpage
\section{Performance improvement over time}\label{app:perf-over-time}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/qwen_7b_24_two_panel.pdf}
    \caption{\textbf{A:} Models evaluated on intuitive physics tower stacks that either have the same number of blocks as the fine-tuning data or different numbers of blocks compared to the fine-tuning data. Note: bars are jittered slightly for better readability. \textbf{B:} Performance of the 7B model fine-tuned on 2--4 blocks on the test set for all number of blocks over the process of fine-tuning. The model performs best on towers it is fine-tuned on but it can generalize to bigger towers somewhat. Generalization decreases as the block tower size moves away from the fine-tuning distribution. Both subplots show the proportion of correct answers (accuracy) with Wilson score intervals as error bars.}
    \label{fig:phys_two_panel}
\end{figure*}

\begin{figure*}[!hb]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/qwen_7b_36_two_panel.pdf}
    \caption{\textbf{A:} Models evaluated on causal reasoning pyramids that either have the same number of blocks as the fine-tuning data or different numbers of blocks compared to the fine-tuning data. Models performance is roughly the same for in-distribution and out-of-distribution pyramid sizes. Note: bars are jittered slightly for better readability. \textbf{B:} Performance of the 7B model fine-tuned on 3 \& 6 block pyramids on the test set for all number of blocks over the process of fine-tuning. The model performs well on the pyramids it is fine-tuned on and can generalize to bigger towers. Both subplots show the proportion of correct answers (accuracy) with Wilson score intervals as error bars.}
    \label{fig:caus_two_panel}
\end{figure*}

\section{Generalization to Lerer}\label{app:gen-lerer}
\
\begin{figure*}[!h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/qwen_7b_lerer_two_panel.pdf}
    \caption{\textbf{A:} 7B model fine-tuned on intuitive physics towers with 2--4 blocks but evaluated on the Lerer stimuli showing real images of 2--4 block towers. \textbf{B:} 7B model fine-tuned on 3 \& 6 block pyramids, evaluated on the Lerer stimuli, which have a different visual quality and are in another cognitive domain.}
    \label{fig:lerer_two_panel}
\end{figure*}

\section{Generalization to new domains over time}\label{app:gen-new-domains}
\vspace{0.5cm}

\begin{figure*}[!hb]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/all_models_gen_domain.pdf}
    \caption{\textbf{A:} Models fine-tuned on intuitive physics towers but evaluated on causal reasoning pyramids. \textbf{B:} Models fine-tuned on causal reasoning pyramids but evaluated on intuitive physics towers. Models do not generalize to the other domain, even though it shares the same visual features.}
    \label{fig:all_models_gen_domain}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/qwen_7b_24_causgen_two_panel.pdf}
    \caption{\textbf{A:} Models fine-tuned on intuitive physics towers but evaluated on causal reasoning pyramids. Models again do not generalize to this other task, even though it shares the same visual features. \textbf{B:} Performance of the 7B model fine-tuned on 2--4 block towers, tested on causal reasoning pyramids. The model does not generalize to any tower size. Both subplots show the proportion of correct answers (accuracy) with Wilson score intervals as error bars.}
    \label{fig:causgen_two_panel}
\end{figure*}

\begin{figure*}[!hb]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/qwen_7b_36_physgen_two_panel.pdf}
    \caption{\textbf{A:} Models fine-tuned on causal pyramids but evaluated on intuitive physics tower stacks. Models do not generalize to this other task, even though it shares the same visual features and can be seen as a pre-requisite for the fine-tuning task. This is especially true for the fine-tuning on 3 block tower pyramids, where computing the counterfactual question requires solving the binary tower stability of a two block tower. \textbf{B:} Performance of the 7B model fine-tuned on 3 \& 6 block pyramids, tested on intuitive physics block towers. The model does not generalize to any tower size. Both subplots show the proportion of correct answers (accuracy) with Wilson score intervals as error bars.}
    \label{fig:physgen_two_panel}
\end{figure*}

% \clearpage

% \section{Human fine-tuning}\label{app:human-ft}
% \
% \begin{figure*}[!h]
%     \centering
%     \includegraphics[width=1.0\textwidth]{figures/qwen_7b_human_ft_train_test.pdf}
%     \caption{Both plots show the performance of the 7B model fine-tuned on human judgments of intuitive physics towers. \textbf{A:} Performance on individual tower sizes of the fine-tuning data set. \textbf{B:} Performance on the ground truth evaluation data for individual tower sizes. Even though the model is fine-tuned on human judgments on towers of size 2--4, it learns to predict the ground truth stability of bigger towers as well.}
%     \label{fig:human_finetune_performance}
% \end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
