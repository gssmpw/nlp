\section{Related Works}
\label{sec:related}
We refer the reader to
\cite{cadena2016simultaneous,dellaert2017factor,ebadi2023present,triggs1999bundle}
for comprehensive reviews of state-of-the-art estimation frameworks in robotics
and computer vision. Sparse nonlinear least squares solvers for
solving these estimation problems can
be found in \cite{Agarwal_Ceres_Solver_2022,gtsam,kuemmerle2011g2o}.
However, all of these works assume that the noise covariance is known
beforehand. In contrast, our approach simultaneously estimates both the primary
parameters (e.g., robot's trajectory) and the noise covariance matrix
directly from noisy measurements.
The importance of automatic hyperparameter tuning has recently gained
recognition in the SLAM literature; see, e.g., \cite[Section V]{ebadi2023present}
and \cite{fontan2024look,fontan2024anyfeature}. We share this perspective and
present, to the best of our knowledge, the first principled
approach for optimal noise covariance estimation in SLAM and related
problems.


\subsection{Optimal Covariance Estimation via Convex Optimization}
ML estimation of the mean and covariance from independent and identically
distributed (i.i.d.) Gaussian samples using the sample mean and sample
covariance is a classic example found in textbooks.
\citet[Chapter 7.1.1]{boyd2004convex} show that covariance
estimation in this standard setting and several of its variants can be formulated as convex optimization
problems.
However, many estimation problems that arise in robotics and other engineering disciplines extend
beyond the standard setting.
While \emph{noise samples} are assumed to be i.i.d.\ for each
measurement \emph{type} (Section~\ref{rem:multiple}), the measurements
themselves are \emph{not} identically distributed. Each measurement follows a Gaussian
distribution with the corresponding noise covariance matrix and a \emph{unique} mean that depends on an
unknown parameter belonging to a manifold.
Furthermore, the measurement function varies across different measurements and
is often nonlinear. 
We demonstrate that the noise covariance estimation problem in this
more general setting can also be formulated as a convex optimization problem. Similar to
\cite{boyd2004convex}, we explore several problem variants that incorporate
prior information and additional structural constraints on the noise covariance
matrix. These variants differ from those studied in
\cite[Chapter 7.1.1]{boyd2004convex} and admit analytical (closed-form)
solutions.

\subsection{Iterated Generalized Least Squares}
\citet{zhan2025generalized} propose a joint pose and noise
covariance estimation method for the perspective-$n$-point (P$n$P) problem in
computer vision. Their approach is based on the iterated (or iterative)
generalized least squares (IGLS) method (see \cite[Chapter
12.5]{SeberWild200309} and references therein), alternating between pose
and noise covariance estimation. They report improvements in
estimation accuracy
ranging from $2\%$ to $34\%$ compared to baseline methods that assume a fixed
isotropic noise covariance.
To the best of our knowledge, before \cite{zhan2025generalized}, IGLS had not been
applied in robotics or computer vision.\footnote{We were
unable to find the original reference for IGLS. However, the concept was already known
and analyzed in the econometrics literature in 1970s
\cite{malinvaud1980statistical}. IGLS is closely
related to the feasible generalized least squares (FGLS) method which also has a
long history in econometrics and regression.}
Our work (developed concurrently with
\cite{zhan2025generalized}) generalizes and extends both IGLS and 
\cite{zhan2025generalized} in several keys ways.
First, we prove that the joint ML estimation problem is ill-posed when the sample covariance
matrix is singular. This critical problem arises frequently in
real-world applications, where the sample covariance matrix can be singular or
poorly conditioned (Remark~\ref{rem:singular}). We address this critical issue by
(i) constraining the minimum eigenvalue of the noise covariance matrix, and, in our MAP formulation, (ii) imposing a Wishart prior on the noise information
matrix. We derive analytical optimal solutions for the noise
covariance matrix, conditioned on fixed values of the primary parameters, for MAP
and ML joint estimation problems and several of their constrained variants
(Theorem~\ref{thm:main}). These enable the end user to leverage prior information
about the noise covariance matrix (from, e.g., manufacturer's calibration) in
addition to noisy measurements to solve the joint estimation problem. We propose several algorithms for
solving these joint estimation problems and present a rigorous theoretical
analysis of their convergence properties.  Our formulation is more general than
IGLS and we show how our framework can be extended to heteroscedastic
measurements, nonlinear (with respect to) noise models, and manifold-valued
parameters.  Finally, we provide insights into the application of our approach
to ``graph-structured'' estimation problems \cite{khosoussi2019reliable}, such
as PGO and other SLAM variants. These problems present
additional challenges compared to the P$n$P problem due to the increasing number
of primary parameters (thousands of poses in PGO vs.\ a single pose in P$n$P) and
the sparsity of measurements, which reduce the effective signal-to-noise ratio.


\subsection{Noise Covariance Estimation in Kalman Filtering}
The problem of identifying process and measurement noise models in Kalman
filtering has been extensively studied since 1970s; see, e.g.,
\cite{zhang2020identification,chen2023kalman,forsling2024matrix} and references therein. The Kalman filter (KF) is
equivalent to the \emph{recursive} MAP estimator in linear-Gaussian models.
Similarly, nonlinear variants of the KF, such as the extended Kalman filter
(EKF), can be viewed as \emph{approximate} MAP estimators.  Despite conceptual
similarities between problems and proposed solutions, Kalman filtering methods
are specifically tailored for linear(ized) models in \emph{filtering} estimation
problems within a \emph{recursive} framework. Consequently, they are not
directly applicable to the \emph{smoothing} problems (or \emph{batch} settings)
addressed in this paper, which represent the state-of-the-art approach in SLAM
and related computer vision problems; see, e.g., \cite{dellaert2017factor}.

\begin{figure}[t]
		\centering
		\includegraphics[width=0.49\textwidth]{figures/wishart_distribution_plots_updated.pdf}
		\caption{Confidence ellipses for 10 samples drawn from the Wishart distribution 
				$\mathcal{W}(P;V,\nu)$ with different parameters. The scale matrix $V$ is set 
				to identity in the left and middle plots, and to a correlated matrix in the 
				right plot. The degrees of freedom $\nu$ are set to $2$ (left) and $10$ (middle and right). 
				As $\nu$ increases, the samples become more concentrated. Increasing $\nu$ changes the scale of samples as
well (cf.\ left and middle).}
		\label{fig:wishart}
\end{figure}