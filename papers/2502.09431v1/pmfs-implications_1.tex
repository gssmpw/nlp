\section{Design Choices}
\label{sec:Implications}
\noindent In this section, we discuss the possible memory hierarchy designs when including NVM in a system. We also discuss the high-level modifications necessary in a traditional disk-optimized DBMS in order to take full advantage of NVM hardware.
\subsection{Memory Hierarchy Designs for an NVM-Based DBMS}

With features of byte-addressability, low latency and high capacity, NVM has the potential to replace traditional disks as well as main memory \cite{chang2012limits}. Fig.~\ref{Fig2} shows different options that might be considered when including NVM into the system. Fig.~\ref{Fig2a} depicts a traditional approach, where the intermediate state - including logs, data buffers, and partial query state - is stored in DRAM to hide disk latencies for data that is currently in use; while the bulk of the relational data is stored in a  disk.

Given the favorable characteristics of NVM over the other technologies, an option might be to replace both DRAM and disk storage 
using NVM (Fig.~\ref{Fig2b}). However, such a drastic change would require a complete redesign of current operating systems and 
application software. In addition, NVM technology is still not mature enough in terms of endurance to be used as a DRAM replacement. 
Hence, we advocate for a platform that still has a layer of DRAM memory, like \cite{kimura2015foedus}, where the disk is completely or partially replaced using NVM, 
as shown in Fig.~\ref{Fig2c} (NVM-Disk). 

Using this approach, we can retain the programmability of current systems by still having a layer of DRAM, thereby exploiting DRAM's fast read and write access latencies for temporary data structures and application code. In addition, it allows the possibility to directly access the bulk of the database relational data by using a file system such as PMFS, taking full advantage of NVM technology, which allows the system to leverage NVM's byte-addressability and to avoid API overheads~\cite{huang2014nvram} present in current FSs. Unlike an in-memory DBMS, such a setup does not need large pools of DRAM since temporary data is orders of magnitude smaller than the actual relational database stored in NVM. We believe this is a realistic scenario for future systems integrating NVM, with room for small variations such as NVM alongside DRAM to store persistent temporary data structures, or having traditional disks to store cold data.

As explained in Section~\ref{NVDIMM}, 3D XPoint memory can operate in two different modes. Memory mode is similar to a traditional design (Fig.~\ref{Fig2a}), as the 3D XPoint DIMMS are not considered as persistent memory but as the actual DRAM address space. In this mode the DRAM DIMMs are transparently used as a cache for the 3D XPoint DIMMs. This does not change the system view from an application's point of view, and makes sense if one wants to use 3D XPoint DIMMs as if they were large-capacity DRAM DIMMs. However, the App Direct mode would fit into the NVM-Disk (Fig.~\ref{Fig2c}) category. Applications still see DRAM DIMMs as a layer of volatile memory, but can also directly access the 3D XPoint DIMMs via mmap interfaces that enable byte-addressability. The system we consider and later evaluate would be based on a setup similar to that offered by 3D XPoint\textquotesingle s  App Direct mode.


%\textcolor{blue}{Memory mode operation of 3D XPoint memory (explained in Section \ref{NVDIMM}) uses a configuration similar to  NVM-Disk (see Fig.~\ref{Fig2c}) with some differences. Although DRAM is used as a cache in memory mode, 3D Xpoint memory acts as volatile extension of DRAM and sits between storage and DRAM in the memory hierarchy. Fig.~\ref{MemOverview3DXPoint}, based on \cite{peng2019system}, shows the logical view of memory hierarchy in App Direct and Memory mode operation of 3D XPoint memory.}

\begin{figure} %[!htbp]
\centering     %%% not \center
\subfigure[Traditional design]{\label{Fig2a}\includegraphics[width=27mm]{Fig2a.eps}}
\subfigure[All-in-NVM]{\label{Fig2b}\includegraphics[width=27mm]{Fig2b.eps}}
\subfigure[NVM-Disk]{\label{Fig2c}\includegraphics[width=27mm]{Fig2c.eps}}
\caption{NVM placement in the memory hierarchy of a computing system}
\label{Fig2}
\end{figure}

%\begin{figure} %[!htbp]
%\centering     %%% not \center
%\subfigure[Memory Mode]{\label{MemMode}\includegraphics[width=35mm]{3DXPointMmeoryModeConfig.eps}}
%\hspace{5pt}
%\subfigure[Direct Access Mode]{\label{DAXMode}\includegraphics[width=35mm]{3DXPointAppDirectModeConfig.eps}}
%\caption{Overview of memory hiearchy in Memory and Direct Access Mode of 3D XPoint Memory}
%\label{MemOverview3DXPoint}
%\end{figure}

\subsection{Potential Modifications in a Traditional DBMS}~\label{modList}
Using a traditional disk-based database with NVM storage will not take full advantage of NVM's features. Some important components of the DBMS need to be modified or removed when using NVM as a primary storage. 

\noindent\textbf{Avoid the block level access:} Traditional design of a DBMS uses a disk as a primary storage. Since disks favor sequential accesses, database systems hide disk latencies by issuing fewer but larger disk accesses in the form of a data block~\cite{schindler2002track}. 

Unfortunately, block level I/O causes extra data movement. For example, if a transaction updates a single byte of a tuple, it still needs to write the whole block of data to the disk. On the other hand, block level access provides good data locality.
 
Since NVM is byte-addressable, we can read and write only the required byte(s). However, reducing the data retrieval granularity down to a byte level eliminates the advantage of data locality altogether. A good compromise is to reduce the block size in such a way that the overhead of the block I/O is reduced to an acceptable level, while at the same time the application benefits from some degree of data locality. 
 
\noindent\textbf{Remove internal buffer cache of DBMS:} DBMSs usually maintain an internal buffer cache. Whenever a tuple is to be accessed, first its disk address has to be calculated. If the corresponding block of data is not found in the internal buffer cache, then it is read from disk and stored in the internal buffer cache \cite{debrabant2013anti}. 
 
This approach is unnecessary in an NVM-based database design. If the NVM address space is made visible to a process, then there is no need to copy data blocks. It is more efficient to refer to the tuple directly by its address. However, we need an NVM-aware FS, such as PMFS, to enable direct access to the NVM address space by a process.
 



\subsection{Discussion}
NVM provides the promising features of persistency, like disk storage; and byte-addressability, like DRAM. However, NVMs
have certain limitations such as lower endurance compared to DRAM \cite{arulraj2015let} and a disparity between the read and write latencies \cite{pelley2014memory}. 
Furthermore, different NVM technologies differ from each other in term of these features \cite{arulraj2015let}.

 
A storage engine aiming to improve decision support
system (DSS) queries can be designed by taking advantage
of the common features of persistency and byte-addressability.
Since DSS queries are read dominant and perform a relatively
negligible number of write operations, the design should
not be influenced or sensitive to different endurance and write
latencies found across NVM technologies. Furthermore, NVM technologies
are projected to provide read latencies similar to DRAM \cite{mittal2016survey,arulraj2015let,wang2013low,chang2012limits}.
Therefore, reading data directly from NVM storage should be comparable in terms
of access latency to reading application data stored in DRAM.

 
Usage of NVM as primary storage can also impact
other components of a DBMS besides those mentioned
in Section \ref{modList}. For example, if internal buffers are not
employed and all updates are materialized directly into the
NVM address space then the need and criticality of the redo
log can be relaxed \cite{huang2014nvram}. However, the undo log will still be
needed to recover from a system failure. These important aspects are
out of the scope of this work and we will focus on storage engine modifications.

\begin{figure*}  %[!htbp]
\centering     %%% not \center
\subfigure[PostgreSQL storage engine]{\label{Fig3a}\includegraphics[width=38mm]{Fig4.eps}}
\subfigure[Modified storage engine - SE1]{\label{Fig3b}\includegraphics[width=38mm]{Fig5New.eps}}
\subfigure[Modified storage engine - SE2]{\label{Fig3c}\includegraphics[width=38mm]{Fig6New.eps}}
\caption{High level view of read and write memory operations in PostgreSQL (read as ``pg'' in short form) and modified SEs}
\label{Fig3}
\end{figure*}
 
