\section{Evaluation of the data prefetching library}
\label{sec:library-evalualtion}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{kernel-exec-percent-pg-new.eps}
\caption{Percentage of kernel execution time for PostgreSQL thread}
\label{kernel-time-pg}
\end{figure*}

\noindent In this section, we evaluate our modified storage engine \emph{SE2} while using the data prefetching library. 
For the scenarios in which we employ the library, we remove the simple ad-hoc software prefetching scheme used in the previous evaluation. However, the \emph{pmfs\_se2} system to which we compare does include the same ad-hoc prefetching used in the previous evaluation (Section~\ref{sec:evaluation}). The test machine and methodology employed is the same as explained in Section~\ref{sec:methodology}, with the exception that hyper-threading (HT) is enabled for \emph{M2} and \emph{M3}.



\subsection{Performance Impact on Kernel Execution Time}

Fig. \ref{kernel-time-pg} shows the percentage of kernel execution time of the PostgreSQL thread (computation thread) for each of the evaluated queries. As explained in Section~\ref{sec:evaluation}, \emph{SE2} only redirects the buffer pointer for file read operation from the local buffer cache to an NVM disk address that is within the address space of the PostgreSQL process. Hence, there is no data movement at the kernel level. As a result, the involvement of kernel in data movement and hence the average percentage of kernel execution time is already low in \emph{pmfs\_se2} as compared to \emph{pmfs\_base95}, reducing from 10\% to 3\%.

When using \verb+M1+, \verb+M2+, and \verb+M3+ helper thread schemes, by offloading the prefetching of entire blocks of data to helper threads, we can further hide kernel execution time overheads for the PostgreSQL thread running the query. Helper threads are more effective in prefetching data than the ad-hoc scheme used in pmfs\_se2. Therefore, the average percentage of kernel execution time further reduces from 3\% for \emph{pmfs\_se2} to 0.5\% in \verb+M1+, \verb+M2+, and \verb+M3+.

There is no noticeable difference between the three thread mapping schemes in terms of percentage of kernel execution time. The reason is that all three mapping schemes place data blocks at least into the LLC, which is enough to hide kernel related events such as page faults. 

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{exec-time-new.eps}
\caption{Wall-clock execution time normalized with respect to pmfs\_base95}
\label{exec-time-new}
\end{figure*}
\begin{figure*}
\centering
\includegraphics[width=\linewidth]{compute-stall-pg.eps}
\caption{Execution time breakdown into compute and stall cycles for PostgreSQL thread, normalized with respect to pmfs\_base95}
\label{compute-stall-pg}
\end{figure*}

\subsection{Query Performance Improvement}\label{QPI}

Fig.~\ref{exec-time-new} shows the wall-clock query execution time normalized with respect to \emph{pmfs\_base95} for all queries. We can observe that for queries in which \emph{pmfs\_se2} obtained better performance, the new evaluated systems with our prefetch library overall obtain better execution times, especially for the \verb+M3+ thread mapping scheme. M3 shows noticeable performance improvements for queries where the sequential scan operation represents a significant fraction of the total database operations - i.e. Q03-Q12, Q15, and Q19, as shown in Fig. \ref{query-breakdown}. On the other hand, queries where sequential scan operation consume less time (i.e. Q01, Q02, Q13, Q17, and Q20), show no performance improvement. On average, M3 obtains an 8\% performance improvement over the baseline. \verb+M1+ and \verb+M2+ show up to 13\% performance improvement (Q11), with an average of 6\% when compared to pmfs\_base95. 

Query execution time is mostly affected by two factors: cache misses and competition for hardware resources between threads mapped on the same physical core. Reducing any of these two factors should lead to better query execution times. To understand the improvements seen in Fig.~\ref{exec-time-new}, we provide insights in terms of compute and stalled core cycles and L1 cache misses for the PostgreSQL thread in Fig.~\ref{compute-stall-pg} and Fig.~\ref{L1-misses-pg}, respectively.

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{L1-misses-total.eps}
\caption{L1 cache misses for PostgreSQL thread normalized with respect to pmfs base95}
\label{L1-misses-pg}
\end{figure*}

Fig.~\ref{compute-stall-pg} shows an execution cycle break down into the stall and compute cycles for all evaluated queries, but just for the PostgreSQL thread. An execution cycle is classified as `compute', if at least one instruction is committed during that cycle, or as `stalled' otherwise. Both \verb+M1+ and \verb+M2+ generate similar results in terms of wall-clock execution time. In Fig.~\ref{compute-stall-pg}, we can observe that both are able to reduce the kernel stall and compute components due to less kernel involvement in the main computation thread since the prefetching and kernel related events such as page faults are handled by the helper thread. However, the user level components are still very similar. This is because, as shown in Fig.~\ref{L1-misses-pg}, the actual number of L1 cache misses is also similar despite helper threads prefetching at different levels of the memory hierarchy. We attribute this to hardware prefetchers being much more efficient once the data is already in the LLC. M3 shows better performance than M1 and M2 as it maps the helper-thread, which handles the page faults, on a core different than that of compute thread as shown in Fig. \ref{CaseE}. Nonetheless, Fig.~\ref{L1-misses-pg} shows a large L1 cache misses reduction when compared to both \emph{pmfs\_base95} and \emph{pmfs\_se2}, proving that the prefetching library is performing well in hiding them from the main computation thread.

We can see in Fig.~\ref{compute-stall-pg} that \verb+M3+, besides being able to reduce the kernel components, also reduces the time spent in stalled user significantly, e.g., in Q03, Q05, Q06, Q12, and others. This is because of two factors: (i) prefetching into the L1 cache with our library is more timely than hardware prefetching, which might still be in-flight when the data is actually needed; and (ii) by using the two thread mapping approach we are able to reduce the overhead of the helper thread that is sharing the same physical core with the computation thread. Therefore, the first helper thread in \verb+M3+ brings the data into the LLC without interfering with the PostgreSQL thread that is running on a different physical core. While the second thread running on the same core brings the data into the private caches while incurring a lower overhead due to lower latency misses.

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{results_Helper1.eps}
\caption{Execution time breakdown into compute and stall cycles for 
helper thread running on same physical core as PostgreSQL in M2 and M3. Execution time is normalized with respect to M2}
\label{compute-stall-helper}
\end{figure*}

To support this explanation, Fig.~\ref{compute-stall-helper} shows the compute-stall cycle breakdown for the helper thread that shares the physical core with the computation PostgreSQL thread for \verb+M2+ and \verb+M3+ schemes. We can observe that for the queries in which \verb+M3+ performs better (e.g., Q03, Q05, Q06, and Q12), the helper thread sharing the core is more lightweight. In particular, the helper thread in \verb+M3+ does not suffer from kernel noise since the kernel related events like page faults are sorted by the other helper thread running on a different core, leading to a load reduction in the computation core. Also, a reduction in user stalled cycles can be observed (e.g., Q11 and Q15) due to lower latency memory accesses.

We find that our library is able to help improve the performance with little programming effort. The ad-hoc prefetching scheme used in \emph{pmfs\_se2}, while simple, still required code analysis to place the prefetch instructions in different places in order to maximize the number of data blocks prefetched without stalling too much the computations. With our library the creation and mapping of threads is done only once, and the creation of jobs to be enqueued came as a natural fit in a single point of the source code, i.e., when the needed block is memory mapped.
