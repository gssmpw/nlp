\section{Evaluation}
\label{sec:evaluation}
\noindent In this section, we show the performance impact of the modified storage engines (SE) on kernel execution time and on wall-clock execution time for TPC-H queries. Later, we identify potential issues current DBMSs and applications, in general, may face in order to harness the benefits of  directly accessing data stored in NVM memory.



\subsection{Performance Impact on Kernel Execution Time}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{kernel-time.eps}
\caption{Percentage of kernel execution time for each query}
\label{fig:kernel-time}
\end{figure*}

Fig.~\ref{fig:kernel-time} shows the percentage of kernel execution time (KET) for each of the evaluated queries 
running on the four evaluated 
systems. When using traditional file operations (e.g. \verb+read()+), like those employed in unmodified PostgreSQL, the bulk of the work when 
accessing and reading data is done inside the kernel. As can be seen, the baseline systems spend a significant amount of the execution time in 
kernel space: up to 
23.85\% 
(Q11 - \textit{disk\_base95}) and 
19.80\% 
(Q11 - \textit{pmfs\_base95}), with an average of around 10\%. 
KET is dominated by 
the time it takes to fetch data from the storage medium into a user-level buffer. These overheads are high in both disk and NVM storage, 
and are likely to increase as datasets grow in size.

However, when using \textit{SE1} or \textit{SE2}, this data movement can be minimized or even avoided. 
For \textit{pmfs\_se1} we observe that the amount of time spent in kernel space decreases substantially 
and it is very similar to that observed for \textit{pmfs\_se2}. This is because the two systems are 
doing a similar amount of work on the kernel side, with the difference that \textit{SE1} is doing an 
implicit \verb+memcpy()+ operation into a user-level buffer. 
Overall, we see that the modified SEs are able to reduce KET significantly in  most queries: 
Q02 to Q12, Q15, and Q19. A few queries show lower reductions because they operate over a small amount of 
data, e.g, Q01, Q13, Q16, and Q20. 
 
 Reduction in KET is consistent with the 
 data movement (DM) bar in Fig. \ref{query-breakdown}. 
 For example, queries with relatively more time spent in DM operations 
 (e.g., Q02 to Q12, Q15, and Q19) show a higher reduction in KET  
 when executed using SE1 and SE2, as shown in Fig. ~\ref{fig:kernel-time}. On the other hand, 
 queries that spend relatively lesser time in DM operations (e.g., Q01, Q13, Q16, and Q20) show a  lower reduction in KET. 
 In other words, read intensive  queries show more reduction in KET 
 when executed using SE1 and SE2.
 An important point to note is that for \textit{SE1} and \textit{SE2} the kernel space time is likely to remain near constant as datasets grow since no work is done to fetch data.

\subsection{Query Performance Improvement}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{exec-time.eps}
\caption{Wall-clock execution time normalized with respect to \textit{pmfs\_base95}}
\label{fig:exec-time}
\end{figure*}

Fig.~\ref{fig:exec-time} shows wall-clock execution time for each query on the evaluated systems. The data is 
normalized with respect to \textit{pmfs\_base95}. We observe that the benefits of moving from disk to a faster storage can be high for read-intensive queries such as Q05 (39\%), Q08 (37\%), and Q11 (33\%). 
However, for compute-intensive queries, such as Q01 and Q16, 
the benefits are non-existent. On average, the overhead of using disk over PMFS storage is around 15\%.

For \textit{SE1}, the reductions observed in terms of kernel execution time do not translate into reductions in overall query execution 
time. The main reason for this is the additional \verb+memcpy()+ operation performed to copy the data into the application buffer. In fact, 
we find that this operation in PMFS is sometimes slower than the original \verb+read()+ system call employed in the baseline, leading to a 
3\% slowdown on average.

When using \textit{SE2} there is no data movement at the time of fetching data into an application-accessible memory region, due 
to the possibility of directly referencing data stored in PMFS. However, this has a negative side effect when accessing the data 
for processing later on, as it has not been cached by the processing units. Therefore, the benefits of avoiding data movement to 
make it accessible are offset by the penalty to fetch this data close to the processing units at a later stage. 
In order to mitigate this penalty, \textit{SE2} incorporates a simple software prefetching scheme that tries to fetch in advance 
the next element to be processed within a data block. 

When compared to \textit{pmfs\_base95}, \textit{SE2} is able to achieve significant performance improvements in read-dominant queries such as Q11 (13\%), Q15 (11\%), and Q19 (8\%). These performance improvements are also consistent with the data presented in Fig.~\ref{query-breakdown}. Queries that are dominated by sequential scan operations are the ones that benefit from our modified storage engines. For example, although Q02 spends almost 18\% of its execution time in data movement operations (like Q15 and Q19, as shown by the DM bar in Fig. \ref{query-breakdown}), sequential scan makes only 20\% of the database operations performed by the query (unlike Q15 (84\%) and Q19 (98\%)) as shown by the FB bar in the same figure. As a result, Q02 does not benefit from our modified storage engine. The same explanation is valid for Q13, Q16, Q17, and Q20. On average, \textit{SE2} is around 4\% faster than \textit{pmfs\_base95} and around 19\% faster than \textit{disk\_base95}.

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{compute-stall.eps}
\caption{Execution-time breakdown for compute and stalled cycles --- B $=$ \textit{pmfs\_base95}, SE2 $=$ \textit{pmfs\_se2}}
\label{fig:compute-stall}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{llc-misses.eps}
\caption{Last-level cache (LLC) misses breakdown --- B $=$ \textit{pmfs\_base95}, SE2 $=$ \textit{pmfs\_se2}}
\label{fig:llc-misses}
\end{figure*}


Fig.~\ref{fig:compute-stall} shows a classification of each cycle of execution as {\lq{compute}\rq}, if at least one 
instruction was committed during that cycle, or as {\lq{stalled{\rq} otherwise. These categories are further broken down 
into user and kernel level cycles. Data is shown for \textit{pmfs\_base95} and \textit{SE2}, normalized to the former. 
As can be seen, the \textit{stalled\_kernel} component correlates well with the kernel execution time shown in Fig.~\ref{fig:kernel-time}, 
and this is the component that is reduced in \textit{SE2} executions. 
Furthermore, reductions in the \textit{stalled\_kernel} component using SE2 are proportional to the 
time spent in DM operations shown in Fig.~\ref{query-breakdown}. 
We observe that for most queries some of the savings from \textit{stalled\_kernel} 
shift to \textit{stalled\_user} since data needs to be brought close to the processing unit when it is needed for processing. 
There are some exceptions, i.e., Q11, Q15, and Q19, for which the simple prefetching scheme is able to mitigate this fact effectively.



Fig.~\ref{fig:llc-misses} shows a breakdown of user and kernel last-level cache (LLC) misses. Here, 
we can clearly see how the number of LLC misses remains quite constant when comparing \textit{pmfs\_base95} and \textit{SE2}, 
but the misses shift from kernel level to user level. Moreover, in our experiments, we observe that user level misses have a 
more negative impact in terms of performance because they happen when the data is actually needed for processing, and a full LLC 
miss penalty is paid for each data element. On the other hand, when moving larger data blocks to an application buffer, optimized 
functions are employed and the LLC miss penalties can be overlapped.


\subsection{Discussion}~\label{Discussion}
Experimental results show that there is a mismatch between the potential performance benefits shown in Fig.~\ref{fig:kernel-time} and the actual benefits 
in terms of wall-clock query execution time 
shown in Fig.~\ref{fig:exec-time}. Direct access to memory regions holding persistent data can provide 
significant benefits, but this data needs to be close to the processing units when it is needed. To this end, we employ 
simple software prefetching schemes that provide moderate average performance gains using our SE2 engine. However, carefully crafted ad-hoc software 
prefetching is challenging, and applications may not be designed in a way that makes it easy to hide long access latencies even 
with the use of prefetching, as happens with PostgreSQL. Moreover, such a solution is application and architecture dependent.

Additional software libraries and tools that aid programmability are needed in such systems. Such libraries could implement solutions 
like helper threads for prefetching particular data 
regions, effectively bringing data closer to the core (e.g., LLC) with minimal application interference. This approach would 
provide generic solutions for writing software that takes full advantage of the capabilities that NVM can offer.

Therefore, to further improve the performance when using our SE2 engine, we develop a general purpose data prefetching library employing helper threads to prefetch data while the main computational thread continues to make forward progress. By exposing a simple API to the programmer, a user can specify the desired number of threads to use, their mapping to cores, and enqueue jobs to prefetch desired data regions.
