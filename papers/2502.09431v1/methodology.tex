\section{Methodology}
\label{sec:methodology}


\noindent System-level evaluation for NVM technologies is challenging due to lack of real hardware. 
Software simulation infrastructures are a good fit to evaluate systems in which NVM is used as a DRAM replacement,
or in conjunction with DRAM as a hybrid memory system. However, when using NVM as a permanent storage replacement, 
most software simulators fail to capture the details of the operating system, and comparisons against traditional disks are not 
possible due to the lack of proper simulation models for such devices. As the authors of PMFS \cite{dulloor2014system} noted, 
an emulation platform is the best way to evaluate such a scenario.

\doublerulesep 0.1pt
\begin{table}[h]
 % \centering
%   \small
  \caption{Test machine characteristics}
  \label{tab:machine}
  \begin{tabular}{@{}ll@{}}
  \toprule
  \textbf{Component} & \textbf{Description} \\ 
  \midrule
  \multirow{2}{*}{Processor}    & Intel Xeon E5-2670 @ 2.60Ghz \\
                                & HT and TurboBoost disabled \\
  \multirow{3}{*}{Caches}       & Private: L1 32KB 4-way split I/D, \\
								& L2 256KB 8-way \\
                                & Shared: L3 20MB 16-way \\
  Memory                        & 256GB DDR3-1600, 4 channels, delivering\\
								& up to 51.5GB/s\\ 
  %\midrule
  OS                            & Linux Kernel 3.11.0 with PMFS support\\  &\cite{githubPMFS,dulloor2014system} \\
  %\midrule
  \multirow{2}{*}{Disk storage} & Intel DC S3700 Series, 400GB, SATA 6Gb/s \\ 
                                & Read 500MBs/75k iops, Write 460MBs/36k \\
                                & iops \\
                                %& Read 851 MB/s, Write 412 MB/s \\
  PMFS storage                  & 224 GB of total DRAM \\%, Read 3.8 GB/s, Write  2.6 GB/s\\  
  \bottomrule
  \end{tabular}
\end{table}


\subsection{Emulation Platform}

We set up an infrastructure similar to that used by the PMFS authors. We first recompile the Linux kernel of our 
test machine with PMFS support. Using the \textit{memmap} kernel command line option we reserve a physically contiguous area of the 
available DRAM at boot-time, which is later used to mount the PMFS partition. In other words, a portion of the DRAM holds the disk 
partition managed by PMFS and provides features similar to those of NVM, such as byte-addressability and lower latency compared to a disk. 
Table~\ref{tab:machine} lists the test machine characteristics. We configure the machine to have a 224GB PMFS partition, leaving 32GB of 
DRAM for normal main memory operation. A high-end SSD is used as regular disk storage.

To fairly evaluate storage engines SE1 and SE2, we compare their performance with two baselines using unmodified PostgreSQL. A similar comparison approach is also adopted by other closely related and complementary works \cite{gao2011pcmlogging,son2017log} using NVM in context of disk-based DBMS as explained in Section~\ref{sec:RelatedWork}.
While comparing with prior work that employs NVM in context of in-memory
DBMS or in from-scratch NVM-aware DBMS designs is out of the scope of this paper, as these systems present different sets of features and target different domains, we do include a qualitative comparison in Section~\ref{sec:RelatedWork}.

The two baselines use unmodified  PostgreSQL 9.5 with the dataset stored in: (i) a regular high-end disk (\textit{disk\_base95}), and (ii) in the PMFS partition (\textit{pmfs\_base95}). The modified storage engines - SE1 and SE2 - are run with the dataset stored on the PMFS partition and are termed \textit{pmfs\_se1} and \textit{pmfs\_se2}, respectively.

%A technological advantage of NVMs over traditional disks is their lower read access latencies. To quantify the performance impact this can have in query executions, we evaluate two baselines using unmodified  PostgreSQL 9.5, (i) with the dataset stored in a regular high-end disk (\textit{disk\_base95}), and (ii) in the PMFS partition (\textit{pmfs\_base95}). In addition, we evaluate the modified storage engines - SE1 and SE2. These are run with the dataset stored on the PMFS partition and are termed \textit{pmfs\_se1} and \textit{pmfs\_se2}, respectively.

Since DRAM read latencies are expected to be similar to projected NVM read latencies \cite{mittal2016survey,arulraj2015let,wang2013low,chang2012limits}, the emulation platform we employed provides good performance estimations. In our experiments, we report wall-clock query execution times as well as data obtained with performance counters using the \textit{perf} toolset. 
\subsection{Workloads}


\begin{figure*}
\centering
\includegraphics[width=\linewidth]{QBDown.eps}
\caption{Execution time breakdown for TPCH queries in traditional DBMS with database stored in disk-storage}
\label{query-breakdown}
\end{figure*}

TPC-H \cite{council2008tpc} is a widely used benchmark and a good representative of decision support system
(DSS) queries. Therefore, to evaluate our proposed SEs, we employ DSS queries from the TPC-H benchmark configured with a scale factor of 100, which leads to a dataset larger than 150GB when adding the appropriate
indexes. Like most data-intensive workloads, these queries are read dominant, which will enable us to draw accurate results from our emulation platform. We report results for 16 of the 22 TPC-H queries since some queries fail to complete under PMFS storage, even when executed with the unmodified PostgreSQL storage engine (i.e. baseline \textit{pmfs\_base95}).

Fig.~\ref{query-breakdown} shows the characterization of the different TPC-H queries in the form of an execution 
time breakdown. The data is collected using a scale factor of 100 with a baseline system that uses a high-end disk as primary storage 
(\textit{disk\_base95}).  The  figure shows two bars: 
functional breakdown (FB) and data movement (DM). FB shows 
the percentage of execution 
time spent across the most relevant database operators, i.e., sequential scan (SeqScan), Sort, Join and all other operations
combined together. DM shows the percentage of execution time spent in the main function performing data reads from 
disk (\textit{mdread}) and also \textit{memcpy} since it is used internally by the kernel to bring data into the application buffers.

As can be seen in the figure, most of the queries are dominated
by sequential scan operations, as expected from read-dominant queries.
This is confirmed by the fact
that most queries spend about 20\% of their execution time bringing
data in from storage to application-level buffers, as shown by
the DM bar.  These overheads are expected to become worse with larger datasets in the future, therefore
lowering the data access latency and avoiding unnecessary data movement is
critical to reduce query execution time. Note that these overheads are due to
data movement operations that can be avoided by reading directly from primary storage with our proposed NVM-aware SEs.
