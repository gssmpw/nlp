\section{Evaluation}

To evaluate the effectiveness of our layered interface paradigm and generative authoring workflow, we conducted two studies: (1) a mixed-methods user experience evaluation with a Subjective Evidence-Based Ethnography (SEBE) protocol, and (2) a between-subjects deployment study on Prolific.

\subsection{User Experience Evaluation}

In the first study, our goal was to evaluate \system's impact on cognitive load, usability, and its influence on creative output. \change{ We wanted to understand the manners in which personas and layered affordances could bridge the envisioning gap~\cite{subramonyam2024bridging} for writers. }

\subsubsection{Participants}
We recruited participants via LinkedIn and Twitter, selecting individuals who engaged in creative writing at least a few times per month. The final sample included 12 participants (8 male, 4 female), aged 18 to 54. The majority (7 participants) were in the 18-24 age group, with 3 in the 25-34 range, and one each in the 35-44 and 45-54 groups. All participants were native English speakers. Educational backgrounds varied: 6 participants held bachelor's degrees, 5 had master's degrees, and 1 had some college experience. Participants reported diverse creative writing habits and experiences, with most having taken creative writing courses. Self-reported confidence in writing ranged from 2 to 5 on a 5-point scale ($\mu = 3.54$, $\sigma = 0.82$).

\subsubsection{Study Procedure}
Each study session lasted not more than 2 hours. At the start of the session, participants received a live demonstration of the application and were encouraged to ask questions. The participants were then asked to access the application on their own computer through the web url and complete a brief guided writing task about a tree, a cat and a dog to familiarize themselves with the interface. Next, participants engaged in the main writing task. They were given 40 minutes to write an 800-word essay connecting a recent real-life event to a film, using one of the 5 specified writing styles (First-Person Narrative, Journalistic Style, Dialogue Format, Letter or Diary Entry, or Screenplay Format). They were asked to identify parallels, extract insights, and reflect on personal growth. After completing the task, the participants were asked to fill out the NASA Task Load Index (TLX) for cognitive load, the Post-Study System Usability Questionnaire (PSSUQ) for usability assessment, and the Creative Support Index for evaluating creative support. The participants completed a survey gathering demographic information, writing habits, and experience with AI writing tools. 

Finally, participants engaged in a retrospective interview while reviewing the snippets of the video recordings of their session. We wanted to understand the (1) affordances, in terms of effects of the application on their writing process,  (2) their embodied competencies, skills, and knowledge they drew upon during the task, and (3) any perceived rules, norms, or expectations that guided their approach. We wrapped up the session by asking about overall impressions and additional insights. The entire writing session was recorded, capturing both the participants' audio recording and the participants' screen. Each participant received \$30 for their participant in the form of an Amazon gift card.


\subsubsection{Results}
\textbf{NASA TLX.} The survey measures six subscales: Mental Demand, Physical Demand, Temporal Demand, Performance, Effort, and Frustration. Analysis of the data (N = 12) revealed varying levels of perceived workload across these dimensions. Effort ($\mu = 8.42$, $\sigma = 6.50$) and Mental Demand ($\mu = 6.17$, $\sigma = 3.43$) were the highest-rated factors, indicating that participants found the task mentally taxing and requiring substantial effort. This aligns with expectations, as the task was designed to require deep thinking and was non-trivial. Temporal Demand also had a notable score ($\mu = 6.42$, $\sigma = 6.35$), suggesting that time pressure played a significant role. \change{The Performance score ($\mu = 5.25$, $\sigma = 3.33$) indicates participants generally performed well, as lower values on this scale represent better perceived performance. Similarly, low Frustration scores ($\mu = 3.58$, $\sigma = 3.96$) suggest minimal participant frustration.} The high standard deviations, particularly for Effort and Temporal Demand, reflect significant variability in study participants' experiences, likely due to differences in their task approach or expertise characteristics. Overall, these findings suggest that \system imposed a low cognitive load on participants.

% \begin{figure}
%   \centering
%   \includesvg[width=0.8\textwidth]{figures/plots/nasa_tlx_plot.svg}
%   \caption{Boxplot for NASA Task Load Index across the 6 dimensions}
%   \label{fig:nasaltx}
% \end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.9\columnwidth]{figures/NASATLX.pdf}
  \caption{NASA-TLX Workload response distribution across relevant dimensions}
  \label{fig:nasatlx}
\end{figure}


\begin{figure}[!t]
  \centering
  \includegraphics[width=0.9\columnwidth]{figures/PSSUQ.pdf}
  \caption{PSSUQ Response dimensions across System Usefulness, Information Quality, Interface Quality categories and Overall dimension}
  \label{fig:pssuq}
\end{figure}
\textbf{PSSUQ.} The Post-Study System Usability Questionnaire was used to assess the usability of the AI-assisted writing tool across three key dimensions: System Usefulness, Information Quality, and Interface Quality. Analysis of the data (N = 12) revealed generally positive usability scores, with all dimensions receiving mean ratings below the midpoint of the 7-point scale (where lower scores indicate better usability). The Overall PSSUQ score ($\mu = 2.18$, $\sigma = 1.29$) suggests that participants found the system to be reasonably usable. System Usefulness ($\mu = 1.99$, $\sigma = 1.01$) and Interface Quality ($\mu = 2.00$, $\sigma = 1.39$) were rated the highest, indicating that users found the tool functional and easy to interact with. The Information Quality dimension received a slightly lower but still positive rating ($\mu = 2.51$, $\sigma = 1.45$), suggesting room for improvement in the clarity and organization of information provided by the system.

% \begin{figure}
%   \centering
%   \includesvg[width=0.9\textwidth]{figures/plots/pssuq_plot.svg}
%   \caption{PSSUQ Response dimensions across System Usefulness, Information Quality, Interface Quality
% categories and Overall dimension}
%   \label{fig:pssuq}
% \end{figure}

Participants reported that they could not always tell if they had made a mistake in the interface, though this may not be a significant concern for a writing tool where many `mistakes' are subjective or stylistic choices. Within the System Usefulness dimension, items related to ease of use ($\mu = 1.67$, $\sigma = 0.89$) and efficiency ($\mu = 1.58$, $\sigma = 0.79$) received particularly positive ratings, suggesting that users found the tool intuitive and time-saving. However, the item related to system capabilities (System Usefulness: $\mu = 2.08$, $\sigma = 1.73$) showed higher variability, indicating diverse opinions on whether the system had all the expected functions and system capabilities, from the participants' perspectives.

In the Information Quality dimension, the item related to error message clarity ($\mu = 4.38$, $\sigma = 1.41$) received the lowest rating, highlighting a significant area for improvement. Interface Quality items were consistently rated positively, with low variability, suggesting a well-designed user interface. The relatively high standard deviations across most items indicate varied user experiences, possibly due to differences in expectations, prior experience with similar tools, or the specific writing tasks undertaken during the study.

\textbf{CSI.} The Creative Support Index was used to evaluate participants' experiences with \system, using a scale of 0 to 20, where lower scores indicate more positive outcomes. The analysis focused on five key dimensions: Exploration, Enjoyment, Results Worth Effort, Immersion, and Expressiveness. Data from 12 participants revealed generally positive experiences across these dimensions. Enjoyment was the highest-rated factor ($\mu = 2.67$, $\sigma = 3.50$), suggesting that users found the tool particularly enjoyable. This was closely followed by Results Worth Effort ($\mu = 3.83$, $\sigma = 3.81$), indicating that participants felt their input produced valuable results. Exploration ($\mu = 4.00$, $\sigma = 4.31$) and Expressiveness ($\mu = 4.33$, $\sigma = 3.17$) also received favorable ratings, implying that the tool effectively supported idea generation and self-expression.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.9\columnwidth]{figures/CSI.pdf}
  \caption{Creative Support Index (CSI) response distribution across relevant dimensions}
  \label{fig:csi}
\end{figure}

% \begin{figure*}[t!]
%   \centering
%   \includegraphics[width=0.4\textwidth]{figures/CSI.pdf}
%   \caption{Creative Support Index response distribution across relevant dimensions}
%   \label{fig:csi}
% \end{figure*}





\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/usage_tree.pdf}
    \caption{\change{Tree visualization of layer manipulation and LLM calls in \system. We show three diverse ways participants leveraged the interface affordances for completing the assigned task. The diverging gray lines depict `tear', the converging gray lines depict `combine', and meaning of other symbols is present in the legend}}
    \label{fig:participant-journey}
\end{figure*}

Interestingly, Immersion received the lowest rating ($\mu = 7.50$, $\sigma = 5.79$), though it still fell on the positive side of the scale. This suggests that while users found the tool engaging, there may be opportunities to enhance its ability to create a more immersive experience. The high standard deviations across all dimensions, particularly for Immersion and Exploration, reflect significant variability in user experiences, possibly due to individual differences in writing styles. The Collaboration dimension was excluded from our findings, as participants rated its weight factor as 0. 

\textbf{Spatial organization of content.} \system provides users with the ability to spatially organize their layers and associated content, a feature that was highly valued by participants for improving their focus and workflow. P9 praised the interface's immersive quality, stating: \textit{``One nice thing is that the fact that it's all in one big interface makes it less immersion-breaking than, say, opening a bunch of Google Docs tabs, where you have to make more major context switches and get distracted by other tabs.''} P9 further emphasized the advantages of a dedicated workspace: \textit{``Or you break out of the immersion of writing and being in the zone by going more into your desktop environment, where you're reminded of your work, or end up seeing social media. So it's nice to have a dedicated, isolated workspace.''}  This sentiment was shared by other participants who recognized the organizational benefits of \system over traditional text editors. P4, for instance, pointed out the limitations of conventional file systems: \textit{``The structure and organizational possibilities of this kind of thing would be huge because you end up with folder upon folder upon folder.''} P4's comment highlights \system's potential to address the organizational challenges typically encountered with standard text editors, offering a more intuitive and flexible way to manage content throughout the writing process. 

The system's spatial design supported expressive content management strategies\footnote{We thank psycholinguist George Miller, author Roy Pea's postdoctoral mentor at Rockefeller University in the 1970's for his seminal insights half a century ago in foregrounding 'the human tendency to locate information spatially', which we expressly leverage in the design of \system~\cite{millerpsychology}}. P9 emphasized how this shaped their writing process: \textit{``Being able to fold or bin my writing without permanently removing them made me much more willing to experiment. In single-page interfaces, I often feel pressured to be certain about content placement before typing.''} Users demonstrated remarkable spatial awareness of their content, organizing it in ways that enhanced accessibility. P11 noted: \textit{``Being able to [tunnel] into content from all over and get contextually-informed responses from the friends made everything feel so accessible.''} Through reduced cognitive load, users focused more on writing and experimentation rather than on content management.

\textbf{Paper metaphor for writing.} A central metaphor that \system aims to embody is the ability to manipulate and move layers much like rearranging sheets of paper on a desk. P11 captured this sentiment, stating, \textit{``It's a much more visual, like a desk with pieces of paper all over it,''} highlighting the intuitive, tactile nature of organizing content in the workspace. This visual and spatial approach offers users a more flexible and natural way to manage their writing, akin to physically handling documents in a traditional environment. It also opens up future promising research directions to develop and study a gesture- and voice-based rendition of the \system functionalities. 

\textbf{Flexibility in testing rhetorical strategies.} The affordances granted by \system, including the ability to tear, split, combine, stack, and fold layers, were clearly evident in how participants utilized the tool. These features not only supported writing tasks but also aligned with users' conceptual models of the writing process. P1 drew a parallel between the system's structure and traditional writing approaches: \textit{``Layers are like, if you're writing a paper, you need an outline and goal. Intuitively, it's like a tree which is the outline, and you work on each part of the node. In this case, you start with the intro, and you spend time collaborating with Idea Ivy. It was natural to break down [the writing process].''} This natural breakdown of the writing process was further enhanced by the system's manipulable interface, as P4 highlighted: \textit{``I really liked it. Being able to push something over there until later, and then bring it back and like smoosh it all together. That was really nice; that I liked that a lot.''} 

\change{The interface's bottom-up approach to LLM integration particularly enhanced exploratory writing. P5, whose usage journey is visualized in Fig. ~\ref{fig:participant-journey} C, observed: \textit{``Clicking a layer, moving it around and expected something to happen is so intuitive. The interface interactions felt natural and expected - exactly what I envisioned would happen.''} Participants often combined different features creatively, as P5 described: \textit{``I think order to exposition is really important. Tearing and recombining in different orders helped me rapidly see what narrative flow made sense''} P5 also wrote initially in first person, utilizing Tone Tara to shift to third person narration, demonstrating the system's flexibility in supporting various narrative styles.}

\textbf{Collaboration with Writer's Friends:} Participants demonstrated diverse and interesting approaches to leveraging the capability of the Writer's Friends in \system. These distinct personas, each representing different writing assistance features, effectively bridged the gulf of envisioning~\cite{subramonyam2024bridging}. P8 expressed a particular fondness for one such friend: \textit{``I like Danny. My Danny's a good guy.''} Similarly, P11 highlighted the value of constructive criticism: \textit{``I really like the friends, especially the feedback. I liked getting negative feedback.''} \change{Through writing in the interface and receiving feedback from the ``friends'', users frequently refined their meta layer. P2 explained: \textit{``Sometimes I felt my ideas were being misconstrued so updating the meta layer helped''} The iterative nature of getting feedback helped clarify writing goals and better understand the target readers. On the anthropomorphized LLM scaffolding, P8 remarked their indifference, ``I don't think it influenced my usage positively or negatively''} 

A thematic analysis of user-defined prompts revealed that participants accurately matched their requirements to the appropriate friend in the majority of cases. Out of the 57 times Detail Danny was used, the participants gave it prompts for detail and elaboration 77\% of the time. In the case of Idea Ivy, out of the 52 instances used across sessions 86\% of prompts were for ideation and brainstorming. In case of Tone Tara and  Structure Sam, 85\% (20) and 95\% (20) mapped to tone transformation and structuring respectively. Importantly, the Writer's Friends seemed to enhance rather than replace the creative process. P12 noted, \textit{``I honestly felt like I was still using this tool\ldots to come up with my own ideas. And I think that was good for me.''} This sentiment suggests that \system successfully balanced AI assistance with the preservation of user agency in the writing process. 

\textbf{Usability and User Interface.} The intuitive design and user-friendly interface of \system were frequently highlighted by participants, emphasizing the tool's ease of use and its ability to seamlessly integrate into the writing process. P11 expressed enthusiasm for the command interface, noting its natural feel: \textit{``I love the backslash and having the pull down menu and immediately being able to select. I liked that. I didn't even think about it\ldots I naturally did it.''} This comment showcases the effectiveness of the interface in reducing the cognitive load on users, allowing them to access features quickly and intuitively without disrupting their writing flow. The system's design also contributed to a positive emotional experience for users. P7 remarked on the sense of control and comfort provided by the interface: \textit{``I wasn't stressed at all, I felt completely in control.''}

In figure \change{~\ref{fig:participant-journey} we can see the journey through the interface taken by users with three distinct usage patterns. These visualizations give us a sense of how different writers leverage \system to test rhetorical strategies.  In the case of P2 we see that they created a total of 5 layers, two of which were alternative structures suggested by Structure Sam. In case of P1 they created 13 layers and used 4 of them in their final essay generation. Similarly, P5 created 17 layers, counting all tears, combination, and alternative suggestions layers, and used only two to generate their final essay. }



\subsection{Comparative Analysis: Between-Subjects Evaluation on Prolific}
\change{Building on the insights from the usability assessment, we designed a comparative evaluation. We wanted to understand what facet of \system supported the dynamic knowledge transformation we observed by the writers in study 1. To isolate the effects from different features, we constructed two separate interfaces in addition to \system. We conducted a between-subject study with three conditions: (1) a layered interface with in-line LLM (\system condition), (2) a writing interface with in-line LLM but no layers or spatial component (In-Line-LLM), and (3) a writing interface with a separate AI chat window for LLM interaction (Chat-LLM condition).}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\textwidth]{figures/qual_comp.pdf}
%     \caption{\change{Visualization of LLM Interaction Across Conditions. Each square represents a different writing subprocess, with their meaning defined in the legend.}}
%     \label{fig:all_conditions}
% \end{figure}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.95\textwidth]{figures/study_2_qual.pdf}
  \caption{Visualization of LLM Interaction Across Conditions. Each square represents a different writing subprocess, with their meaning defined in the legend.}
  \label{fig:all_conditions}
\end{figure*}


\subsubsection{Design Decision For Conditions}
In order to have a fully functional In-Line-LLM application that supports the Writer's Friends without the layered paradigm, we had to make some design choices. Detail Danny, Idea Ivy, Feedback Felix and Audience Ali, have their operations constrained on the active layer so they will remain the same but in the case of Structure Sam and Tone Tara, where new layers with transformed content are generated, we had to change their behavior for the In-Line-LLM condition. Both Sam and Tara, instead of generating new layers, replaced the existing content in the editor and provided the user the option in the toolbar to reverse it to their previous content before the transformation. In the case of the Chat-LLM interface, we added a chat interface right beside the editor, to resemble having a word processor and an LLM chat interface open in split view. The rationale for creating our own interface instead of using a baseline of an existing LLM chat interface with a word processor was to have control over the data logging.

\subsubsection{Participants}
We conducted the study through Prolific and had a screening for people who wrote professionally, roles included journalist, copywriter/marketing/communications, and creative writing. We also screened for participants who spent more than 5 hours a week on Prolific, to increase chances of high-quality participation. We recruited 84 participants (F=48, M=30, NB=6). Participants reported ages between 18 and 64, the median age being in the 45-54 age group. 

\subsubsection{Study Procedure}
Each participant was randomly assigned to one of the three conditions (\system, In-Line-LLM and Chat-LLM) such that there was a distribution of 28 participants per condition.  On a scale of 1 to 5, participants reported confidence in their writing skills as follows: chat-llm ($\mu = 3.96$, $\sigma = 0.98$), in-line-llm ($\mu = 4.20$, $\sigma = 0.61$), and \system ($\mu = 4.42$, $\sigma = 0.64$) in case of \system, This study design would allow us to isolate the effect of different components and formulate a comprehensive assessment. Each participant first filled out a demographic survey, and consented to having their writing data logged into our database. The participants were each asked to watch a video tutorial for their respective condition interface. Afterwards,  they had to take a quiz about the interface. This helped ensure that the participants understood the interface well before using it. They were not allowed to proceed without getting all the answers right. Following this task they were assigned their task, which they were given 40 minutes to complete. The task required writing about an 800 words essay on a \textit{crowdworker's experiences with Large Language Models (LLMs):} They were specifically asked that it cover LLMs' impact on their work, adaptation strategies, and future outlook. They were instructed that the essay should include specific examples, data, and reflections on both positive and negative aspects of LLMs in crowd-work. The task was intentionally made non-trivial as we wanted to simulate a real writing task for the participants in a creative capacity. In order to ensure the participants did not write someplace outside of the interface before pasting it later, we tracked their activity in the interface and made persistent activity a criterion for their submission to be valid. We also manually validated the essays written to ensure they were actual essays about the topic and not something off-topic or incoherent.

% \change{The words per minute we report are only from keystrokes registered from the participants. LLM generations do not count towards the words per minute.}  

\subsubsection{Results}




\change{Figure ~\ref{fig:all_conditions} shows the sequence of invocation of features across conditions. We coded the prompts issued by users into one of the following categories: (1) Review, (2) Tone Transformation, (3) Elaboration, (4) Organize, (5) Brainstorm, and (6) Research. Between the In-Line-LLM and \system condition, we combined Feedback Felix and Audience Ali into Review. We also marked prompts that did not fit into any of these categories as miscellaneous. We also do not show instances where users reissue commands when they are not content with the initial generation. Notably, analysis of prompt reissuance revealed a higher density in the Chat-LLM condition, suggesting users were frequently dissatisfied with initial responses and attempted to regenerate content. In contrast, we observed substantially lower reissuance in the In-Line-LLM and \system conditions. As we can see in the visualization, usage differences exist between the interfaces that support in-line LLM support and the chat interface. Looking at the prompts issued by the user, we observed difficulty in understanding how the LLM could support their writing. We observed prompts like, \textit{``You're not a very comprehensive model, are you?''} and\textit{  ``So, your main function is to serve as a research tool?''.} Despite providing an introduction video for all conditions, users experienced the most difficulty in the chat interface. We attribute this to challenges in formulating specific writing intentions and planning, which we believe the task-specific LLM personas were successful at bridging. Users in the In-Line-LLM and \system asked for assistance with transformation or elaboration. In the case of the chat interface, we observed users attempting to solicit complete essays based on points they specified in the prompt.}

\change{When comparing the In-Line-LLM and \system conditions, we observed greater diversity in writing strategies among \system users. A notable pattern was the tendency to seek feedback earlier in their writing process, suggesting that users aimed to align their work with audience expectations and prevent significant deviations as their drafts progressed. Conversely, the In-Line condition exhibited a more fixed usage pattern, where users typically began with review tasks, spent the bulk of their time elaborating on content, and concluded with tone transformations. Interestingly, this pattern deviated from the tutorial video shared with participants, leaving us uncertain about the underlying reasons for this behavior. Through this analysis, we conclude that \system effectively helps users bridge the envisioning and articulatory gap, enabling them to articulate their writing intentions and integrate feedback more seamlessly into their workflows. While in-line support fosters more thoughtful usage of LLM,  the spatial organization of \system propelled the dynamic knowledge transformation.}

