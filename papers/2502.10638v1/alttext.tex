Figure 1: "Figure 1 illustrates Script&Shift, an AI-assisted writing interface. The image is divided into two sections: A) Scripting and B) Shifting.
Section A shows a layered interface for LLM co-writing, featuring Writer's Friends like Tone Tara, Idea Ivy, Detail Danny, and Structure Sam.
Section B demonstrates the Shifting functionality, highlighting key benefits:

Flexible transitions between content creation
Spatial organization of content
Micro and macro edit of content using writer's friends

The figure also includes a 'Key Benefits of Script&Shift' box, describing how it enables seamless transitions and effortless navigation between content creation and spatial organization.
At the bottom, there's a 'Do you want to combine them?' prompt with Yes/No options.
The caption explains that Script&Shift empowers writers to query LLMs using familiar visual design elements, with Scripting for inline queries and Shifting for reorganizing and previewing the final document."






Figure 2: "This image shows the workspace view of Script&Shift, an AI-assisted writing interface. The interface is divided into four main sections, labeled A through D:
A) The main writing workspace, which is zoomable and scrollable. It contains multiple text areas representing different parts or layers of a document. On the left side, there's a toolbar with various icons for different functions.
B) Workspace level operations, shown as a fan of document pages with plus and minus icons below, likely for adding or removing pages or sections.
C) A text layer with a toolbar at the top. This section shows different text blocks, including what appears to be AI-generated responses or suggestions in chat-like bubbles.
D) The compiled document viewer, shown as a small box labeled "Document 1" in the bottom right corner.
The interface appears designed to allow writers to work on different parts of their document simultaneously, incorporate AI assistance, and see a preview of the final compiled document. The layout suggests a flexible, modular approach to writing and document composition."

Figure 3: "This image shows a series of screenshots demonstrating various interactions within the Script&Shift writing interface. The figure is divided into 9 panels (a through i), each illustrating a different feature or interaction:

a) Specifying context of writing in meta layer: Shows a form for inputting the purpose and audience of the writing.

b) Starting free writing in Introduction Layer: Displays a text editor with some initial content.

c) Calling Detail Danny from the '/' trigger menu: Shows a menu with different AI assistants, including Detail Danny.

d) Detail Danny generating two variations for user-specified prompt for detail/elaboration: Demonstrates AI-generated content variations.

e) Research friend as a scratchpad for researching topics specified in meta layer: Shows a yellow notepad-like area with research notes.

f) Adding links for child layers that the current layer will extract content from: Illustrates a feature for connecting different parts of the document.

g) Tunneling into content from another layer into current layer: Shows how content can be pulled from one layer to another.

h) Peaking at (possible) future content using the bottom right: Displays a preview of potential upcoming content.

i) Invoking Feedback Felix for paragraph level feedback: Shows a feature for getting AI feedback on specific paragraphs.

These interactions demonstrate the AI-assisted, modular, and interconnected nature of the Script&Shift interface, designed to aid writers throughout various stages of the writing process."

Figure 4: "
This image shows the system architecture of an AI-assisted writing interface, likely Script&Shift based on previous context. The architecture is divided into three main sections:
Left side - What user is providing:

Text Layer with Editor Instance, Listener, and Renderer
Meta Layer containing Purpose, Audience, Intent, Overall Document, and External References
Another Text Layer similar to the first

Center - Processing components:

Prompt Composer with LLM Orchestrator (including System Prompts and Layer Templateizer)
LLM (Large Language Model) as the central processing unit
Workspace Manager with LLM Broadcast Manager

Right side - How interface is modified:

Document Layer with Document Instance, Listener, and Renderer
Meta Layer similar to the input side
Text Layer with Editor Instance, Listener, and Renderer

The data flow starts with user inputs from the Text and Meta Layers feeding into the Prompt Composer. The Prompt Composer processes these inputs and sends them to the LLM. The LLM's output is then managed by the Workspace Manager, which updates the interface components on the right side.
This architecture illustrates how user inputs are processed through AI components to generate and manage content in the writing interface, creating a seamless interaction between user input, AI processing, and interface updates.
"

Figure 5: "
This image illustrates the workflow of an AI writing assistant named Tara within the system architecture. The process is divided into four main stages:

1. User Interaction:
   Shows the user's input about copyright laws, creative writing, and generative AI. The user has written a few paragraphs on the topic.

2. Prompt Composer:
   Displays how the system processes the user's input. It includes options to make the content more concise, generate variations, and continue writing. There's also a feedback prompt for each paragraph.

3. Structured LLM Response:
   Presents a JSON-like structure of the AI's response, likely generated by a Large Language Model (LLM). This structured output contains different versions of the content and metadata.

4. Display Logic:
   Shows how the AI-generated content is presented back to the user. It offers different tone options (e.g., "Generative AI and Creative Writing" vs. "Intellectual Property and AI") and displays the refined content.

The figure demonstrates how user input is transformed through AI processing into structured data, which is then formatted for display back to the user. This workflow allows for dynamic content generation and refinement based on user preferences and AI capabilities.
"

Figure 6: "
This figure illustrates a free writing example using the Script&Shift interface. It's divided into three main sections, showing the progression of organizing an argument or piece of writing:

Left panel:
Shows a diagram with interconnected elements: Claims, Ground, Rebuttal, Warrant, Qualifier, and Backing. These elements are connected by lines, suggesting their relationships in structuring an argument. A document icon is visible in the Ground section, implying that content can be added there. Clear and Organize buttons are present at the bottom.
Middle panel:
Displays a similar diagram, but with empty document icons for each element (Claims, Ground, Rebuttal, Warrant, Qualifier, Backing). This suggests a stage where the user can input content into each part of the argument structure. The Ground section has a hand cursor icon, indicating it's interactive. Clear and Organize buttons are still present.
Right panel:
Shows a document view with colored sections of text. Each color likely corresponds to a different element of the argument structure (e.g., purple for claims, green for ground, etc.). This represents the final, organized piece of writing based on the structure defined in the previous panels.

The progression from left to right illustrates how Script&Shift allows users to move from a conceptual argument structure to filling in content, and finally to a coherent, organized document. This approach combines visual organization tools with free-form writing to help users structure their thoughts and arguments effectively.
"

Figure 7: "
This figure demonstrates a document-based question example using the Script&Shift interface. It's divided into four main sections:

Left panel (blue):
Shows a form for specifying the purpose of the writing. It includes fields for the writing purpose, audience, and any additional context. There's an Upload button and a Context Documents section at the bottom.
Second panel (yellow):
Displays a notepad-like interface with the question "Can you explain what the assignment is about?" It appears to be a research or brainstorming area where users can jot down ideas or ask questions about the uploaded documents.
Third panel (white):
Contains the main text of what seems to be an uploaded document or generated content. The text discusses topics related to Aztec society, religion, and social structure.
Right panel (white):
Shows multiple overlapping document-like shapes, suggesting that multiple documents or pages can be accessed and referenced within the interface.

This layout illustrates how Script&Shift facilitates document-based writing tasks by allowing users to:

Define the purpose and context of their writing
Upload and access relevant documents
Ask questions or make notes about the documents
View and potentially interact with the document content

The interface appears designed to support research, analysis, and writing tasks that involve working with multiple source documents while maintaining a clear focus on the user's writing objectives.
"

Figure 8:"
This figure shows a boxplot of NASA Task Load Index (NASA-TLX) workload scores across six dimensions: Mental Demand, Physical Demand, Temporal Demand, Performance, Effort, Frustration
Key observations:
Effort has the highest median score and widest range, indicating it's generally rated as the most demanding aspect with high variability among respondents.
Physical Demand has the lowest median and smallest range, suggesting it's consistently rated as the least demanding dimension.
Temporal Demand shows a large range, implying varied experiences with time pressure across participants.
Mental Demand has a moderately high median and range, indicating significant cognitive load for many respondents.
Performance and Frustration have similar median scores, but Frustration shows a wider range of responses.
All dimensions except Physical Demand show considerable variability, as evidenced by their box sizes and whisker lengths.

The y-axis represents the workload score, ranging from 0 to 20. The boxplots show the median (horizontal line), interquartile range (box), and minimum/maximum values (whiskers) for each dimension. Outliers, if present, would be represented as individual points beyond the whiskers.
This visualization allows for quick comparison of workload across different aspects of task performance, highlighting which dimensions tend to be more demanding or variable in the context of the NASA-TLX assessment.
"

Figure 9: "
This figure shows the distribution of responses to the Post-Study System Usability Questionnaire (PSSUQ) across four dimensions: System Usefulness, Information Quality, Interface Quality, and an Overall dimension. The responses are on a 7-point scale, where 1 represents "Strongly Agree" and 7 represents "Strongly Disagree".

Key observations:

1. System Usefulness:
   - 80.5% of responses are positive (1 or 2)
   - Very few negative responses (5, 6, or 7)

2. Information Quality:
   - 58.5% of responses are positive
   - More varied responses compared to System Usefulness
   - Highest percentage of negative responses among all dimensions

3. Interface Quality:
   - 77.8% of responses are positive
   - Highest percentage of "Strongly Agree" responses (47.2%)

4. Overall:
   - 71.7% of responses are positive
   - Distribution closely mirrors the average of the other three dimensions

Across all dimensions, the majority of responses fall in the positive range (1-3), indicating generally favorable perceptions of the system. Interface Quality received the most positive feedback, while Information Quality shows the most room for improvement.

The stacked bar chart effectively visualizes the distribution of responses, allowing for easy comparison across dimensions and response categories. The color coding from green (positive) to red (negative) aids in quickly identifying trends in the data.
"

Figure 10: "
This figure presents the distribution of responses for the Creative Support Index (CSI) across five dimensions: Exploration, Enjoyment, Results Worth Effort, Immersion and Expressiveness

Key observations:
1. Response scale: The scale ranges from 0-4 (Agree) to 17-20 (Disagree), divided into five categories.

2. Exploration: 50% strongly agree (0-4), 41.7% somewhat agree (5-8), and 8.3% neutral (9-12).

3. Enjoyment: Highest agreement with 66.7% strongly agreeing, 25% somewhat agreeing, and 8.3% neutral.

4. Results Worth Effort: 66.7% strongly agree, but there's a split in the remaining responses with 8.3% somewhat agreeing and 25% neutral.

5. Immersion: Most varied responses, with 33.3% each in strongly agree, somewhat agree, and somewhat disagree (13-16) categories.

6. Expressiveness: Identical to Exploration, with 50% strongly agreeing, 41.7% somewhat agreeing, and 8.3% neutral.

Overall, the responses are generally positive across all dimensions, with Enjoyment and Results Worth Effort receiving the strongest agreement. Immersion shows the most diverse opinions, suggesting it might be an area for potential improvement or further investigation. Exploration and Expressiveness show identical, largely positive distributions.

This visualization effectively shows the varying levels of agreement across different aspects of creative support, allowing for quick comparison between dimensions.
"




