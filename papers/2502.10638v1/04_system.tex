\section{System Architecture}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{figures/system_architecture.pdf}
%     \caption{System Architecture}
%     \label{fig:system_architecture}
% \end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth, height=0.55\textheight, keepaspectratio]{figures/sys_arc.png}
    \caption{\change{(A) \textbf{system architecture} and (B) \textbf{example} of how it is leveraged for Tone Tara. \textbf{(A1\&B1)} The meta layer and writing layers provide the content, prompts and other details for prompt composer. \textbf{(A2\&B2)} The prompt composer uses LLM Organizer and base prompts to structure the context and send the details for generation constraints using Layer Templatizer. \textbf{(A3\&B3)} LLM outputs the structured responses. \textbf{(A4\&B4)} The Workspace manager projects the response onto writing layers and document layers using the layer handler and LLM broadcaster.}}
    \label{fig:system_architecture}
\end{figure*}


\change{At a high level, \system is a tool for generating text from input across multiple text editors and displaying it in multitudinous ways, depending on writer purposes.  The generated text can be rendered as a highlighted block at targeted positions in existing text editors, or it can be rendered in a newly instantiated layer. To support these operations, as shown in Fig. ~\ref{fig:system_architecture} A, the system consists of (1) layer primitives that encapsulate various kinds of editors (Fig. ~\ref{fig:system_architecture} A1); (2) the prompt composer for converting the input for generation into well-defined structures (Fig. ~\ref{fig:system_architecture} A2); (3) the large language model for text generation (Fig. ~\ref{fig:system_architecture} A3); and (4) the workspace manager that controls the layer primitives and associated display logic (Fig. ~\ref{fig:system_architecture} A4). Here, we describe the core functionality of each module through an example (Fig. ~\ref{fig:system_architecture}} B) of \textit{Tone Tara}.

\subsection{Layer Primitives}
To support the layered interface paradigm, \system implements a number of layer primitives.

\subsubsection{Writing Layer} 


\change{The Writing Layer is the primary interface component that supports all content-related operations. Internally, this layer consists of three key modules: an editor instance, a listener module, and a text renderer. The editor instance, which is what the writer interacts with, provides standard text editing capabilities, and the listener module monitors and subscribes to LLM outputs, ensuring that any AI-generated content or suggestions are dynamically integrated into the writing process. Finally, the text renderer handles the interaction and display, such as invoking specific prompt composition interactions and displaying the LLM's outputs in the editor. Internally, the text editor is maintained as a dynamic text-based template with user-written text, AI-provided input, and placeholder fields to display AI content. Placeholder fields are dynamically added during LLM interactions and serve as areas where AI-generated content can be inserted, modified, or replaced. The blue text boxes in Fig. ~\ref{fig:system_architecture} B4 are examples of placeholder fields.}


\subsubsection{Meta Layer} The Meta Layer \change{(blue box in Fig. ~\ref{fig:system_architecture} A1)} provides essential context for the current workspace, acting as a foundational layer that informs all LLM inputs and outputs (i.e., the global context). Information added to this layer, such as writing goals, target audience, key themes, and specific instructions, serves as a user-defined base instruction for the system. By embedding these instructions into the workspace, the Meta Layer allows the system to consistently generate outputs that adhere to the user’s intent, making the writing process more cohesive and context-sensitive. Internally, this is a JSON object with attributes for (1) purpose, (2) audience, (3) intent, (4) domain requirements, and (5) external references, which is a list of supporting documents or resources relevant to the writing task. 

\subsubsection{Document Layer} The Document Layer represents the final `compiled' content from a set of layers that can be specified by the writer. The Document Layer’s non-editable format preserves LLM contributions, ensuring structural integrity and preventing accidental changes. It provides a clear separation between drafting and the finalized document. Unlike the Writing Layer’s editor instance, the Document Layer uses a document instance—a JSON object that stores an index and editor content—allowing the final output to be rendered and preserved as a cohesive whole. The Document Layer incorporates hyper-refs that connect specific sections of the final output back to their corresponding Writing Layers, enabling writers to trace content origins, review edits, and maintain a clear understanding of how each section evolved.

\subsection{Prompt Composer}
The prompt composer \change{(Fig. ~\ref{fig:system_architecture} A2)} plays a central role in relaying interactions between the users and the LLM. It is made up of three key components: System Prompts for various writing tasks, Layer Templatizer, and LLM Orchestrator. The System Prompts and template logic are stored in a task knowledge dictionary, which functions as a repository of predefined tasks, each with its own associated prompts and processing rules. The LLM Orchestrator integrates inputs from Writing Layers, the Meta Layer, prompt parameters (user-defined prompt instructions in real-time interactions), and task knowledge. 

When the user carries out an interaction, such as requesting assistance with elaboration or restructuring, the Prompt Composer first retrieves the task-specific knowledge from the repository and sends it to the Layer Templatizer. The Templatizer then parses the Writing Layer’s context to determine the relevant position in the document where the LLM content should be rendered. For example, if the user selects a paragraph for rewording, the Templatizer identifies the paragraph's location in the writing layer, updates the template with the task knowledge, and specifies where the LLM's output (e.g., the reworded text) will appear. Lastly, the LLM Orchestrator handles the concatenation order of prompt parameters, structured content, and base prompts. It also determines the number of LLM instances needed and manages how the information is processed and passed to the LLM, ensuring smooth integration of all inputs. In case of \textit{Tone Tara}, the composed prompt after LLM Orchestrator is applied, will look like Fig. ~\ref{fig:system_architecture} B2.

\subsection{Large Language Model}
The LLM component in our system leverages the Claude 3.5 Sonnet~\cite{anthropicclaude} model from Anthropic, selected for its strong language comprehension, ability to handle complex writing tasks, and efficiency in generating high-quality creative outputs. Following the \change{Layer Templatizer}'s instructions from the Prompt Composer, the model is instantiated to generate either templatized or free-form text, depending on the requirements of the task. The LLM Component operates in close coordination with the Prompt Composer, processing the consolidated input and producing outputs that align with the context and specific objectives of the user’s writing task. \change{We can see this in action in Fig. ~\ref{fig:system_architecture} B3 where the LLM generates two structured responses, each for a new layer to be created}.

\subsection{Workspace Manager}
\change{The Workspace Manager controls the flow of LLM-generated content through two tightly coupled subsystems.}

\subsubsection{\change{Layer Handler}} \change{The Layer Handler maintains the structural integrity of the writing workspace through nested array-based data structures, managing layer operations like stacking, folding, fanning, and comparing. It handles the creation of new layers when requested by the Layer Templatizer and maintains spatial arrangements and parent-child relationships.}


\subsubsection{LLM \change{Broadcaster}} The \change{LLM Broadcaster} is responsible for managing the flow and distribution of LLM-generated content across layers, ensuring that AI outputs are properly integrated into the writing process. When the LLM generates content, the \change{Writing} Layer's listener module subscribes to these outputs, dynamically capturing and processing them. 

\change{Together, these subsystems ensure seamless integration of AI-generated content while maintaining the structural coherence of the writing workspace.}


\subsection{Implementation}
To implement this framework in our interface, we built the application using React~\cite{react}. For the rich-text editor, we integrated Lexical~\cite{lexical}, which provides a hierarchical, node-based WYSIWYG editor with customizable listeners, allowing us to capture and process user inputs dynamically. To support the infinite canvas for spatial organization of content, we used React Flow~\cite{reactflow}, a library that enables the creation of interactive, movable components, allowing users to visually arrange and connect different writing elements. On the backend, we utilize Anthropic's Claude 3.5 Sonnet model~\cite{anthropicclaude} for LLM functionality. The application is deployed on Heroku~\cite{heroku}, and Firebase’s NoSQL cloud real-time database~\cite{firebase} manages the storage of user data. Additionally, we log important metrics, including user-defined prompts, feature invocation timestamps, and words per minute, to track interaction data and performance.
