\section{Related Work}
In this section, we review related work on emergent communication, prior works that use language models as agents in embodied settings, and past works integrating language models with RL.

\smallskip

% By pretraining on internet-scale data, large language models (LLMs) contain a vast amount of semantic knowledge, enabling them to be effective agents across many domains. In this section, we reflect on prior work that uses language models for sequential decision making tasks. 
\noindent \textbf{Emergent Communication.} A major topic in MARL is emergent communication between agents, especially in the context of reference games and repeated reference games, where a speaker knows the ground-truth answer to a question (e.g., a specific image out of a set of images that needs to be referred to). Then, the speaker needs to communicate to the listener, who later needs to choose the item being referenced either over one or repeated interactions.
Prior work has shown that humans tend to quickly adapt to such tasks~\cite{mccarthy2021learning}, naturally using theory of mind reasoning to determine the intents of speakers~\cite{FRANK201480}. Further, \citet{hawkins-etal-2020-continual} showed that language models can also learn to adapt to human conventions via continual learning. Without using human natural language data, \citet{Lazaridou2016} and \citet{Havrylov} use symbolic cheap-talk signals to solve referential games. Our framework of social deduction games, however, is more challenging as each agent does not know the ground truth answer, so teams must communicate to collectively learn the answer. Therefore, our domain does not have as clear of a distinction between ``speakers'' who have knowledge and ``listeners'' who need to gain answers as agents in social deduction games must play both roles.

\smallskip

\noindent \textbf{Language Models Agents.} A large body of prior work use LLMs' access to internet scale data for task planning and decision making. In robotics, prior works explore how language models can be used to plan out a sequence of high-level primitives given an instruction in natural language~\cite{saycan2022arxiv, huang22a, Lin2023}. %\citet{gdm2024autort} also uses foundation models to control a fleet of robots for collecting diverse data. 
In a virtual gaming setting, \citet{Park2023GenerativeAgents} uses ChatGPT to simulate members of a small virtual town. Although there is no specific task or mechanism for ``training'' these agents, they demonstrate the use of a long-term memory stream to store memories beyond the context length of the language models, enabling the formation of social networks. This technique of having external memory has later been used to learn ``skills'' in a single-player environment~\citep{wang2023voyager} and for coordination in multi-agent environments~\citep{gong2023mindagent}.  These works demonstrate that language models are capable of controlling agents in a wide range of settings, which is key to our motivation to directly use language models as a strong starting point for agents operating in more challenging environments such as social deduction games.

\smallskip

\noindent \textbf{Reinforcement Learning with Foundation Models.}
Some works also combine language models with reinforcement learning. Cicero \citep{doi:10.1126/science.ade9097} is an AI for the game of Diplomacy that uses a dialogue-conditional action model from human actions and trains a dialogue-free model using RL to choose actions. Cicero uses an ``intent'' embedding to connect the dialogue generation and strategic reasoning components. This allows Cicero to communicate with other agents in a way that feels natural to other players, but it prevents the RL model from directly controlling the generated messages, potentially limiting improvements in message quality. Another drawback is that this technique requires a large number of human demonstrations, which may be impractical in many settings.

Foundation models have been effective in both providing rewards and as a base model for policies. \citet{hu2023language} and \citet{kwon2023reward} use language models as reward signals to train a separate network to follow a specific coordination strategy. We similarly use the LLM to provide denser rewards during the discussion phase, but we train the LLM itself instead of a separate policy. 
%\citet{szot2023large} uses a frozen pretrained vision encoder and a language model to train an action decoder with RL to perform robotics tasks based on language instructions. In contrast, we represent both actions and observations in the space of tokens, so we fine-tune the language model directly instead of having a separate action decoder.

Outside of the embodied setting, reinforcement learning has also been key to improving the chat capabilities of LLMs. \citet{ouyang2022training} demonstrates the effectiveness of reinforcement learning from human feedback (RLHF), where a reward model is trained using human feedback and an LLM is fine-tuned using a modification of the PPO algorithm to improve its performance. \citet{yuan2024selfrewarding} extends this by allowing the LLM to be its own reward model and generate its own data for self-improvement, similar to how we use the LLM's own change in beliefs as a reward signal. However, a crucial difference is that our reward model remains grounded in an environment by design due to the imposter prediction training signal. This means that we do not need to rely on the ability of pretrained LLMs to critique their own generations, enabling us to use smaller language models and correct logical errors over time.