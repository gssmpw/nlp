@book{Lam94,
  author = {Leslie Lamport},
  title = {{\LaTeX}: A Document Preparation System},
  publisher = {Addison-Wesley},
  address = {Reading, MA},
  edition = {2nd},
  year = {1994}
}

@article{WoJe95,
  title = {Intelligent Agents: Theory and Practice},
  author = {Wooldridge, Michael J. and Jennings, Nicholas R.},
  journal = {The Knowledge Engineering Review},
  volume = {10},
  number = {2},
  pages = {115--152},
  year = {1995}
}

@article{GrKr96,
  title = {Collaborative Plans for Complex Group Action},
  author = {Grosz, Barbara J. and Kraus, Sarit},
  journal = {Artificial Intelligence},
  volume = {86},
  number = {2},
  pages = {269--357},
  year = {1996}
}

@Techreport{Har78,
  author =       "David Harel",
  year =         "1978",
  title =        "Logics of programs: axiomatics and descriptive power",
  institution =  "Massachusetts Institute of Technology",
  type =         "MIT Research Lab Technical Report",
  number =       "TR-200",
  address =      "Cambridge, MA",
  month =        "",
  note =         ""
}

@Phdthesis{Cla85,
  author =       "Kenneth L. Clarkson",
  year =         "1985",
  title =        "Algorithms for Closest-Point Problems (Computational Geometry)",
  school =       "Stanford University",
  address =      "Palo Alto, CA",
  note =         "UMI Order Number: AAT 8506171",
  type =         "",
  month =        ""
}

@misc{Oba08,
  author        = "Barack Obama",
  year          = "2008",
  title         = "A More Perfect Union",
  howpublished  = "Video",
  day           = "5",
  url           = "http://video.google.com/videoplay?docid=6528042696351994555",
  month         = mar,
  lastaccessed  = "March 21, 2008",
  note          =  ""
}

@misc{Sci09,
  author =       "Joseph Scientist",
  year =         "2009",
  title =        "The fountain of youth",
  note =         "Patent No. 12345, Filed July 1st., 2008, Issued Aug. 9th., 2009",
  url =          "",
  howpublished = "",
  month =        aug,
  lastaccessed = ""
}

@ArtifactDataset{AnMC13,
 author    =  {Sam Anzaroot and Andrew McCallum},
 title     =  {{UMass} Citation Field Extraction Dataset},
 year      = 2013,
 organization = {University of Massachusetts},
 url       =
    {http://www.iesl.cs.umass.edu/data/data-umasscitationfield},
 lastaccessed = {May 27, 2019}
}

@inproceedings{Hag1993,
title        = {Maintaining Discrete Probability Distributions Optimally},
author       = {Hagerup, Torben and Mehlhorn, Kurt and Munro, J. Ian},
booktitle    = {Proceedings of the 20th International Colloquium on Automata, Languages and Programming},
series       = {Lecture Notes in Computer Science},
volume       = {700},
pages        = {253--264},
year         = {1993},
publisher    = {Springer-Verlag},
address      = {Berlin}
}

@Book{Knu97,
  author =       "Donald E. Knuth",
  title =        "The Art of Computer Programming, Vol. 1: Fundamental Algorithms",
  publisher =    "Addison Wesley",
  year =         "1997",
  address =      "Reading, Massachusetts",
  edition =      "3rd",
  editor =       "",
  volume =       "",
  number =       "",
  series =       "",
  month =        "",
  note =         ""
}

@MASTERSTHESIS{Ani03,
author = {David A. Anisi},
title = {Optimal Motion Control of a Ground Vehicle},
school = {Royal Institute of Technology (KTH), Stockholm, Sweden},
intitution = {FOI-R-0961-SE, Swedish Defence Research Agency (FOI)},
year = {2003},
}





















% RLC main.bib Version 2024.1

@book{sutton1998introduction,
    title={Reinforcement Learning: {A}n Introduction},
    author={Sutton, Richard S. and Barto, Andrew G.},
	publisher={The MIT Press},
	year={1998},
	address={Cambridge, MA},
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bubeck2023sparks,
      title={Sparks of Artificial General Intelligence: Early experiments with GPT-4}, 
      author={Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
      year={2023},
      eprint={2303.12712},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{generalpatternmachines2023,
  author    = {Mirchandani, Suvir and Xia, Fei and Florence, Pete and Ichter, Brian and Driess, Danny and Arenas, Montserrat Gonzalez and Rao, Kanishka and Sadigh, Dorsa and Zeng, Andy},
  title     = {Large Language Models as General Pattern Machines},
  booktitle = {Proceedings of the 7th Conference on Robot Learning (CoRL)},
  year      = {2023},
}

@misc{carminati2023hiddenrole,
      title={Hidden-Role Games: Equilibrium Concepts and Computation}, 
      author={Luca Carminati and Brian Hu Zhang and Gabriele Farina and Nicola Gatti and Tuomas Sandholm},
      year={2023},
      eprint={2308.16017},
      archivePrefix={arXiv},
      primaryClass={cs.GT}
}

@Inbook{Oliehoek2012,
author="Oliehoek, Frans A.",
title="Decentralized POMDPs",
bookTitle="Reinforcement Learning: State-of-the-Art",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="471--503",
abstract="This chapter presents an overview of the decentralized POMDP (Dec- POMDP) framework. In a Dec-POMDP, a team of agents collaborates to maximize a global reward based on local information only. This means that agents do not observe a Markovian signal during execution and therefore the agents' individual policies map fromhistories to actions. Searching for an optimal joint policy is an extremely hard problem: it is NEXP-complete. This suggests, assuming NEXP≠EXP, that any optimal solution method will require doubly exponential time in the worst case. This chapter focuses on planning for Dec-POMDPs over a finite horizon. It covers the forward heuristic search approach to solving Dec-POMDPs, as well as the backward dynamic programming approach. Also, it discusses how these relate to the optimal Q-value function of a Dec-POMDP. Finally, it provides pointers to other solution methods and further related topics.",
isbn="978-3-642-27645-3",
doi="10.1007/978-3-642-27645-3_15",
url="https://doi.org/10.1007/978-3-642-27645-3_15"
}

@misc{saycan2022arxiv,
    title={Do As I Can and Not As I Say: Grounding Language in Robotic Affordances},
    author={Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and Chuyuan Fu and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and Sergey Levine and Yao Lu and Linda Luu and Carolina Parada and Peter Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and Diego Reyes and Pierre Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan and Andy Zeng},
    year={2022},
    eprint={2204.01691},
    archivePrefix={arXiv},
    primaryClass={cs.RO}
}

@inproceedings{peng2023rwkv,
    title = "{RWKV}: Reinventing {RNN}s for the Transformer Era",
    author = "Peng, Bo  and
      Alcaide, Eric  and
      Anthony, Quentin  and
      Albalak, Alon  and
      Arcadinho, Samuel  and
      Biderman, Stella  and
      Cao, Huanqi  and
      Cheng, Xin  and
      Chung, Michael  and
      Derczynski, Leon  and
      Du, Xingjian  and
      Grella, Matteo  and
      Gv, Kranthi  and
      He, Xuzheng  and
      Hou, Haowen  and
      Kazienko, Przemyslaw  and
      Kocon, Jan  and
      Kong, Jiaming  and
      Koptyra, Bart{\l}omiej  and
      Lau, Hayden  and
      Lin, Jiaju  and
      Mantri, Krishna Sri Ipsit  and
      Mom, Ferdinand  and
      Saito, Atsushi  and
      Song, Guangyu  and
      Tang, Xiangru  and
      Wind, Johan  and
      Wo{\'z}niak, Stanis{\l}aw  and
      Zhang, Zhenyuan  and
      Zhou, Qinghua  and
      Zhu, Jian  and
      Zhu, Rui-Jie",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.936",
    doi = "10.18653/v1/2023.findings-emnlp.936",
    pages = "14048--14077",
    abstract = "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.",
}


@misc{transformers,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
yu2022the,
title={The Surprising Effectiveness of {PPO} in Cooperative Multi-Agent Games},
author={Chao Yu and Akash Velu and Eugene Vinitsky and Jiaxuan Gao and Yu Wang and Alexandre Bayen and Yi Wu},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022}
}

@inproceedings{Park2023GenerativeAgents,  
author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},  
title = {Generative Agents: Interactive Simulacra of Human Behavior},  
year = {2023},  
publisher = {Association for Computing Machinery},  
address = {New York, NY, USA},  
booktitle = {In the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23)},  
keywords = {Human-AI interaction, agents, generative AI, large language models},  
location = {San Francisco, CA, USA},  
series = {UIST '23}
}

@article{szot2023large,
  author    = {Szot, Andrew and Schwarzer, Max and Mazoure, Bogdan and Agrawal, Harsh and Talbott, Walter and Metcalf, Katherine and Mackraz, Natalie and Hjelm, Devon and Toshev, Alexander},
  title     = {Large Language Models as Generalizable Policies for Embodied Tasks},
  journal   = {preprint},
  year      = {2023},
}

@article{wang2023voyager,
  title   = {Voyager: An Open-Ended Embodied Agent with Large Language Models},
  author  = {Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi Fan and Anima Anandkumar},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2305.16291}
}

@inproceedings{gong2023mindagent,
author = {Gong, Ran and Huang, Qiuyuan and Ma, Xiaojian and Noda, Yusuke and Durante, Zane and Zheng, Zilong and Terzopoulos, Demetri and Fei-Fei, Li and Gao, Jianfeng and Vo, Hoi},
year = {2024},
month = {01},
pages = {3154-3183},
title = {MindAgent: Emergent Gaming Interaction},
doi = {10.18653/v1/2024.findings-naacl.200}
}

@article{
doi:10.1126/science.ade9097,
author = {FAIR and Anton Bakhtin  and Noam Brown  and Emily Dinan  and Gabriele Farina  and Colin Flaherty  and Daniel Fried  and Andrew Goff  and Jonathan Gray  and Hengyuan Hu  and Athul Paul Jacob  and Mojtaba Komeili  and Karthik Konath  and Minae Kwon  and Adam Lerer  and Mike Lewis  and Alexander H. Miller  and Sasha Mitts  and Adithya Renduchintala  and Stephen Roller  and Dirk Rowe  and Weiyan Shi  and Joe Spisak  and Alexander Wei  and David Wu  and Hugh Zhang  and Markus Zijlstra },
title = {Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
journal = {Science},
volume = {378},
number = {6624},
pages = {1067-1074},
year = {2022},
doi = {10.1126/science.ade9097},
URL = {https://www.science.org/doi/abs/10.1126/science.ade9097},
eprint = {https://www.science.org/doi/pdf/10.1126/science.ade9097},
abstract = {Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10\% of participants who played more than one game. The game Diplomacy has been a major challenge for artificial intelligence (AI). Unlike other competitive games that AI has recently mastered, such as chess, Go, and poker, Diplomacy cannot be solved purely through self-play; it requires the development of an agent to understand other players’ motivations and perspectives and to use natural language to negotiate complex shared plans. The Meta Fundamental AI Research Diplomacy Team (FAIR) et al. developed an agent that is able to play the full natural language form of the game and demonstrates performance well above the human average in an online Diplomacy league. The present work has far-reaching implications for the development of cooperative AI and language models for communication with people, even when interactions involve a mixture of aligned and competing interests. —YS Artificial intelligence demonstrates human-level performance in the strategic board game Diplomacy.}}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{silver2017mastering,
      title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}, 
      author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
      year={2017},
      eprint={1712.01815},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{sarkar2023diverse,
author = {Sarkar, Bidipta and Shih, Andy and Sadigh, Dorsa},
title = {Diverse conventions for human-AI collaboration},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Conventions are crucial for strong performance in cooperative multi-agent games, because they allow players to coordinate on a shared strategy without explicit communication. Unfortunately, standard multi-agent reinforcement learning techniques, such as self-play, converge to conventions that are arbitrary and non-diverse, leading to poor generalization when interacting with new partners. In this work, we present a technique for generating diverse conventions by (1) maximizing their rewards during self-play, while (2) minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different. To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce mixed-play, where an initial state is randomly generated by sampling self-play and cross-play transitions and the player learns to maximize the self-play reward from this initial state. We analyze the benefits of our technique on various multi-agent collaborative games, including Overcooked, and find that our technique can adapt to the conventions of humans, surpassing human-level performance when paired with real users.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {1003},
numpages = {25},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@misc{stechly2023gpt4,
      title={GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems}, 
      author={Kaya Stechly and Matthew Marquez and Subbarao Kambhampati},
      year={2023},
      eprint={2310.12397},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@incollection{LITTMAN1994157,
title = {Markov games as a framework for multi-agent reinforcement learning},
editor = {William W. Cohen and Haym Hirsh},
booktitle = {Machine Learning Proceedings 1994},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {157-163},
year = {1994},
isbn = {978-1-55860-335-6},
doi = {https://doi.org/10.1016/B978-1-55860-335-6.50027-1},
url = {https://www.sciencedirect.com/science/article/pii/B9781558603356500271},
author = {Michael L. Littman},
abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsis-tic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.}
}

@inproceedings{
liu2022sampleefficient,
title={Sample-Efficient Reinforcement Learning of Partially Observable Markov Games},
author={Qinghua Liu and Csaba Szepesvari and Chi Jin},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=HnIQrSY7vPI}
}


@article{kaddour2023minipile,
  title={The MiniPile Challenge for Data-Efficient Language Models},
  author={Kaddour, Jean},
  journal={arXiv preprint arXiv:2304.08442},
  year={2023}
}

@inproceedings{
mcaleer2023teampsro,
title={Team-{PSRO} for Learning Approximate {TMEC}or in Large Team Games via Cooperative Reinforcement Learning},
author={Stephen Marcus McAleer and Gabriele Farina and Gaoyue Zhou and Mingzhi Wang and Yaodong Yang and Tuomas Sandholm},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=lCThtrJxoH}
}

@misc{schulman2017proximal,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Braverman_2008,
 ISSN = {10505164},
 URL = {http://www.jstor.org/stable/25442651},
 abstract = {In this paper, we study a game called "Mafia," in which different players have different types of information, communication and functionality. The players communicate and function in a way that resembles some real-life situations. We consider two types of operations. First, there are operations that follow an open democratic discussion. Second, some subgroups of players who may have different interests make decisions based on their own group interest. A key ingredient here is that the identity of each subgroup is known only to the members of that group. In this paper, we are interested in the best strategies for the different groups in such scenarios and in evaluating their relative power. The main focus of the paper is the question: How large and strong should a subgroup be in order to dominate the game? The concrete model studied here is based on the popular game "Mafia." In this game, there are three groups of players: Mafia, detectives and ordinary citizens. Initially, each player is given only his/her own identity, except the mafia, who are given the identities of all mafia members. At each "open" round, a vote is made to determine which player to eliminate. Additionally, there are collective decisions made by the mafia where they decide to eliminate a citizen. Finally, each detective accumulates data on the mafia/citizen status of players. The citizens win if they eliminate all mafia members. Otherwise, the mafia wins. We first find a randomized strategy that is optimal in the absence of detectives. This leads to a stochastic asymptotic analysis where it is shown that the two groups have comparable probabilities of winning exactly when the total population size is R and the mafia size is of order √R. We then show that even a single detective changes the qualitative behavior of the game dramatically. Here, the mafia and citizens have comparable winning probabilities only for a mafia size linear in R. Finally, we provide a summary of simulations complementing the theoretical results obtained in the paper.},
 author = {Mark Braverman and Omid Etesami and Elchanan Mossel},
 journal = {The Annals of Applied Probability},
 number = {3},
 pages = {825--846},
 publisher = {Institute of Mathematical Statistics},
 title = {Mafia: A Theoretical Study of Players and Coalitions in a Partial Information Environment},
 urldate = {2024-12-24},
 volume = {18},
 year = {2008}
}


@inproceedings{kwon2023toward,
 title={Toward Grounded Social Reasoning},
 author={Kwon, Minae and Hu, Hengyuan and Myers, Vivek and Karamcheti, Siddharth and Dragan, Anca and Sadigh, Dorsa},
 booktitle={International Conference on Robotics and Automation (ICRA)},
 year={2024}
}

@misc{gdm2024autort,
      title={AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents}, 
      author={Michael Ahn and Debidatta Dwibedi and Chelsea Finn and Montse Gonzalez Arenas and Keerthana Gopalakrishnan and Karol Hausman and Brian Ichter and Alex Irpan and Nikhil Joshi and Ryan Julian and Sean Kirmani and Isabel Leal and Edward Lee and Sergey Levine and Yao Lu and Isabel Leal and Sharath Maddineni and Kanishka Rao and Dorsa Sadigh and Pannag Sanketi and Pierre Sermanet and Quan Vuong and Stefan Welker and Fei Xia and Ted Xiao and Peng Xu and Steve Xu and Zhuo Xu},
      year={2024},
      eprint={2401.12963},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@inproceedings{hu2023language,
 title={Language Instructed Reinforcement Learning for Human-AI Coordination},
 author={Hu, Hengyuan and Sadigh, Dorsa},
 booktitle={40th International Conference on Machine Learning (ICML)},
 year={2023}
}

@inproceedings{kwon2023reward,
 title={Reward Design with Language Models},
 author={Kwon, Minae and Xie, Sang Michael and Bullard, Kalesha and Sadigh, Dorsa},
 booktitle={International Conference on Learning Representations (ICLR)},
 year={2023}
}

@misc{yuan2024selfrewarding,
      title={Self-Rewarding Language Models}, 
      author={Weizhe Yuan and Richard Yuanzhe Pang and Kyunghyun Cho and Xian Li and Sainbayar Sukhbaatar and Jing Xu and Jason Weston},
      year={2024},
      eprint={2401.10020},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

 @misc{amongus,
    author = "{Innersloth}",
    title = "Among Us",
    year = "2024",
    howpublished = "\url{https://www.innersloth.com/games/among-us/}",
    note = "[Online; accessed 25-February-2024]"
  }

@misc{shapira2023clever,
      title={Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models}, 
      author={Natalie Shapira and Mosh Levy and Seyed Hossein Alavi and Xuhui Zhou and Yejin Choi and Yoav Goldberg and Maarten Sap and Vered Shwartz},
      year={2023},
      eprint={2305.14763},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inbook{carroll2019utility,
author = {Carroll, Micah and Shah, Rohin and Ho, Mark K. and Griffiths, Thomas L. and Seshia, Sanjit A. and Abbeel, Pieter and Dragan, Anca},
title = {On the utility of learning about humans for human-AI coordination},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {While we would like agents that can coordinate with humans, current algorithms such as self-play and population-based training create agents that can coordinate with themselves. Agents that assume their partner to be optimal or similar to them can converge to coordination protocols that fail to understand and be understood by humans. To demonstrate this, we introduce a simple environment that requires challenging coordination, based on the popular game Overcooked, and learn a simple model that mimics human play. We evaluate the performance of agents trained via self-play and population-based training. These agents perform very well when paired with themselves, but when paired with our human model, they are significantly worse than agents designed to play with the human model. An experiment with a planning algorithm yields the same conclusion, though only when the human-aware planner is given the exact human model that it is playing with. A user study with real humans shows this pattern as well, though less strongly. Qualitatively, we find that the gains come from having the agent adapt to the human's gameplay. Given this result, we suggest several approaches for designing agents that learn about humans in order to better coordinate with them. Code is available at https://github.com/HumanCompatibleAI/overcooked_ai.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {465},
numpages = {12}
}

@inproceedings{hu2020other,
author = {Hu, Hengyuan and Lerer, Adam and Peysakhovich, Alex and Foerster, Jakob},
title = {"Other-Play " for zero-shot coordination},
year = {2020},
publisher = {JMLR.org},
address = {},
abstract = {We consider the problem of zero-shot coordination - constructing AI agents that can coordinate with novel partners they have not seen before (e.g. humans). Standard Multi-Agent Reinforcement Learning (MARL) methods typically focus on the self-play (SP) setting where agents construct strategies by playing the game with themselves repeatedly. Unfortunately, applying SP naively to the zero-shot coordination problem can produce agents that establish highly specialized conventions that do not carry over to novel partners they have not been trained with. We introduce a novel learning algorithm called other-play (OP), that enhances self-play by looking for more robust strategies, exploiting the presence of known symmetries in the underlying problem. We characterize OP theoretically as well as experimentally. We study the cooperative card game Hanabi and show that OP agents achieve higher scores when paired with independently trained agents. In preliminary results we also show that our OP agents obtains higher average scores when paired with human players, compared to state-of-the-art SP agents.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {409},
numpages = {12},
series = {ICML'20}
}


@InProceedings{huang22a,
  title = 	 {Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents},
  author =       {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {9118--9147},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/huang22a/huang22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/huang22a.html},
  abstract = 	 {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. “make breakfast”), to a chosen set of actionable steps (e.g. “open fridge”). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models.}
}

@article{Lin2023,

  title={Text2Motion: from natural language instructions to feasible plans},

  author={Lin, Kevin and Agia, Christopher and Migimatsu, Toki and Pavone, Marco and Bohg, Jeannette},

  journal={Autonomous Robots},

  year={2023},

  month={Nov},

  day={14},

  issn={1573-7527},

  doi={10.1007/s10514-023-10131-7},

  url={https://doi.org/10.1007/s10514-023-10131-7}

}

@inproceedings{
Lazaridou2016,
title={Multi-Agent Cooperation and the Emergence of (Natural) Language},
author={Angeliki Lazaridou and Alexander Peysakhovich and Marco Baroni},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Hk8N3Sclg}
}

@inproceedings{Havrylov,
author = {Havrylov, Serhii and Titov, Ivan},
title = {Emergence of language with multi-agent games: learning to communicate with sequences of symbols},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator (Jang et al., 2017)) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {2146–2156},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@misc{foerster2016learning,
      title={Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks}, 
      author={Jakob N. Foerster and Yannis M. Assael and Nando de Freitas and Shimon Whiteson},
      year={2016},
      eprint={1602.02672},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{mukhoti2020calibrating,
      title={Calibrating Deep Neural Networks using Focal Loss}, 
      author={Jishnu Mukhoti and Viveka Kulharia and Amartya Sanyal and Stuart Golodetz and Philip H. S. Torr and Puneet K. Dokania},
      year={2020},
      eprint={2002.09437},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{lai-etal-2023-werewolf,
    title = "Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games",
    author = "Lai, Bolin  and
      Zhang, Hongxin  and
      Liu, Miao  and
      Pariani, Aryan  and
      Ryan, Fiona  and
      Jia, Wenqi  and
      Hayati, Shirley Anugrah  and
      Rehg, James  and
      Yang, Diyi",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.411",
    doi = "10.18653/v1/2023.findings-acl.411",
    pages = "6570--6588",
    abstract = "Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpus. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first multimodal dataset for modeling persuasion behaviors. Our dataset includes 199 dialogue transcriptions and videos captured in a multi-player social deduction game setting, 26,647 utterance level annotations of persuasion strategy, and game level annotations of deduction game outcomes. We provide extensive experiments to show how dialogue context and visual signals benefit persuasion strategy prediction. We also explore the generalization ability of language models for persuasion modeling and the role of persuasion strategies in predicting social deduction game outcomes. Our dataset can be found at https://persuasion-deductiongame. socialai-data.org. The codes and models are available at \url{https://github.com/SALT-NLP/PersuationGames}.",
}

@misc{kopparapu2022hidden,
      title={Hidden Agenda: a Social Deduction Game with Diverse Learned Equilibria}, 
      author={Kavya Kopparapu and Edgar A. Duéñez-Guzmán and Jayd Matyas and Alexander Sasha Vezhnevets and John P. Agapiou and Kevin R. McKee and Richard Everett and Janusz Marecki and Joel Z. Leibo and Thore Graepel},
      year={2022},
      eprint={2201.01816},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{hawkins-etal-2020-continual,
    title = "Continual Adaptation for Efficient Machine Communication",
    author = "Hawkins, Robert  and
      Kwon, Minae  and
      Sadigh, Dorsa  and
      Goodman, Noah",
    editor = "Fern{\'a}ndez, Raquel  and
      Linzen, Tal",
    booktitle = "Proceedings of the 24th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.conll-1.33",
    doi = "10.18653/v1/2020.conll-1.33",
    pages = "408--419",
    abstract = "To communicate with new partners in new contexts, humans rapidly form new linguistic conventions. Recent neural language models are able to comprehend and produce the existing conventions present in their training data, but are not able to flexibly and interactively adapt those conventions on the fly as humans do. We introduce an interactive repeated reference task as a benchmark for models of adaptation in communication and propose a regularized continual learning framework that allows an artificial agent initialized with a generic language model to more accurately and efficiently communicate with a partner over time. We evaluate this framework through simulations on COCO and in real-time reference game experiments with human partners.",
}


@misc{mccarthy2021learning,
      title={Learning to communicate about shared procedural abstractions}, 
      author={William P. McCarthy and Robert D. Hawkins and Haoliang Wang and Cameron Holdaway and Judith E. Fan},
      year={2021},
      eprint={2107.00077},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Vinyals2019GrandmasterLI,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Oriol Vinyals and Igor Babuschkin and Wojciech M. Czarnecki and Micha{\"e}l Mathieu and Andrew Dudzik and Junyoung Chung and David Choi and Richard Powell and Timo Ewalds and Petko Georgiev and Junhyuk Oh and Dan Horgan and Manuel Kroiss and Ivo Danihelka and Aja Huang and L. Sifre and Trevor Cai and John P. Agapiou and Max Jaderberg and Alexander Sasha Vezhnevets and R{\'e}mi Leblond and Tobias Pohlen and Valentin Dalibard and David Budden and Yury Sulsky and James Molloy and Tom Le Paine and Caglar Gulcehre and Ziyun Wang and Tobias Pfaff and Yuhuai Wu and Roman Ring and Dani Yogatama and Dario W{\"u}nsch and Katrina McKinney and Oliver Smith and Tom Schaul and Timothy P. Lillicrap and Koray Kavukcuoglu and Demis Hassabis and Chris Apps and David Silver},
  journal={Nature},
  year={2019},
  volume={575},
  pages={350 - 354},
  url={https://api.semanticscholar.org/CorpusID:204972004}
}

@article{huang2022cleanrl,
  author  = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga and Dipam Chakraborty and Kinal Mehta and João G.M. Araújo},
  title   = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {274},
  pages   = {1--18},
  url     = {http://jmlr.org/papers/v23/21-1342.html}
}

@inproceedings{defazio2024schedulefree,
      title={The Road Less Scheduled}, 
      author={Aaron Defazio and Xingyu Yang and Harsh Mehta and Konstantin Mishchenko and Ahmed Khaled and Ashok Cutkosky},
      booktitle={Thirty-eighth Conference on Neural Information Processing Systems},
      year={2024}
}


@article{FRANK201480,
title = {Inferring word meanings by assuming that speakers are informative},
journal = {Cognitive Psychology},
volume = {75},
pages = {80-96},
year = {2014},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2014.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0010028514000589},
author = {Michael C. Frank and Noah D. Goodman},
keywords = {Language acquisition, Pragmatics, Word learning, Bayesian models},
abstract = {Language comprehension is more than a process of decoding the literal meaning of a speaker’s utterance. Instead, by making the assumption that speakers choose their words to be informative in context, listeners routinely make pragmatic inferences that go beyond the linguistic data. If language learners make these same assumptions, they should be able to infer word meanings in otherwise ambiguous situations. We use probabilistic tools to formalize these kinds of informativeness inferences—extending a model of pragmatic language comprehension to the acquisition setting—and present four experiments whose data suggest that preschool children can use informativeness to infer word meanings and that adult judgments track quantitatively with informativeness.}
}

@misc{dong2024optimizingroboticmanipulationdecisionrwkv,
      title={Optimizing Robotic Manipulation with Decision-RWKV: A Recurrent Sequence Modeling Approach for Lifelong Learning}, 
      author={Yujian Dong and Tianyu Wu and Chaoyang Song},
      year={2024},
      eprint={2407.16306},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2407.16306}, 
}

@misc{huang2024longsequencemodelmodel,
      title={How Well Can a Long Sequence Model Model Long Sequences? Comparing Architechtural Inductive Biases on Long-Context Abilities}, 
      author={Jerry Huang},
      year={2024},
      eprint={2407.08112},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.08112}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@inproceedings{wang2024naht,
    title={N-Agent Ad Hoc Teamwork},
    author={Wang, Caroline and Rahman, Arrasy and Durugkar, Ishan and Liebman, Elad and Stone, Peter},
    booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
    year={2024}
}