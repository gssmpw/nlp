@article{FRANK201480,
title = {Inferring word meanings by assuming that speakers are informative},
journal = {Cognitive Psychology},
volume = {75},
pages = {80-96},
year = {2014},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2014.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0010028514000589},
author = {Michael C. Frank and Noah D. Goodman},
keywords = {Language acquisition, Pragmatics, Word learning, Bayesian models},
abstract = {Language comprehension is more than a process of decoding the literal meaning of a speaker’s utterance. Instead, by making the assumption that speakers choose their words to be informative in context, listeners routinely make pragmatic inferences that go beyond the linguistic data. If language learners make these same assumptions, they should be able to infer word meanings in otherwise ambiguous situations. We use probabilistic tools to formalize these kinds of informativeness inferences—extending a model of pragmatic language comprehension to the acquisition setting—and present four experiments whose data suggest that preschool children can use informativeness to infer word meanings and that adult judgments track quantitatively with informativeness.}
}

@inproceedings{Havrylov,
author = {Havrylov, Serhii and Titov, Ivan},
title = {Emergence of language with multi-agent games: learning to communicate with sequences of symbols},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator (Jang et al., 2017)) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {2146–2156},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{Lin2023,

  title={Text2Motion: from natural language instructions to feasible plans},

  author={Lin, Kevin and Agia, Christopher and Migimatsu, Toki and Pavone, Marco and Bohg, Jeannette},

  journal={Autonomous Robots},

  year={2023},

  month={Nov},

  day={14},

  issn={1573-7527},

  doi={10.1007/s10514-023-10131-7},

  url={https://doi.org/10.1007/s10514-023-10131-7}

}

@inproceedings{Park2023GenerativeAgents,  
author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},  
title = {Generative Agents: Interactive Simulacra of Human Behavior},  
year = {2023},  
publisher = {Association for Computing Machinery},  
address = {New York, NY, USA},  
booktitle = {In the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23)},  
keywords = {Human-AI interaction, agents, generative AI, large language models},  
location = {San Francisco, CA, USA},  
series = {UIST '23}
}

@misc{gdm2024autort,
      title={AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents}, 
      author={Michael Ahn and Debidatta Dwibedi and Chelsea Finn and Montse Gonzalez Arenas and Keerthana Gopalakrishnan and Karol Hausman and Brian Ichter and Alex Irpan and Nikhil Joshi and Ryan Julian and Sean Kirmani and Isabel Leal and Edward Lee and Sergey Levine and Yao Lu and Isabel Leal and Sharath Maddineni and Kanishka Rao and Dorsa Sadigh and Pannag Sanketi and Pierre Sermanet and Quan Vuong and Stefan Welker and Fei Xia and Ted Xiao and Peng Xu and Steve Xu and Zhuo Xu},
      year={2024},
      eprint={2401.12963},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@inproceedings{gong2023mindagent,
author = {Gong, Ran and Huang, Qiuyuan and Ma, Xiaojian and Noda, Yusuke and Durante, Zane and Zheng, Zilong and Terzopoulos, Demetri and Fei-Fei, Li and Gao, Jianfeng and Vo, Hoi},
year = {2024},
month = {01},
pages = {3154-3183},
title = {MindAgent: Emergent Gaming Interaction},
doi = {10.18653/v1/2024.findings-naacl.200}
}

@inproceedings{hawkins-etal-2020-continual,
    title = "Continual Adaptation for Efficient Machine Communication",
    author = "Hawkins, Robert  and
      Kwon, Minae  and
      Sadigh, Dorsa  and
      Goodman, Noah",
    editor = "Fern{\'a}ndez, Raquel  and
      Linzen, Tal",
    booktitle = "Proceedings of the 24th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.conll-1.33",
    doi = "10.18653/v1/2020.conll-1.33",
    pages = "408--419",
    abstract = "To communicate with new partners in new contexts, humans rapidly form new linguistic conventions. Recent neural language models are able to comprehend and produce the existing conventions present in their training data, but are not able to flexibly and interactively adapt those conventions on the fly as humans do. We introduce an interactive repeated reference task as a benchmark for models of adaptation in communication and propose a regularized continual learning framework that allows an artificial agent initialized with a generic language model to more accurately and efficiently communicate with a partner over time. We evaluate this framework through simulations on COCO and in real-time reference game experiments with human partners.",
}

@inproceedings{hu2023language,
 title={Language Instructed Reinforcement Learning for Human-AI Coordination},
 author={Hu, Hengyuan and Sadigh, Dorsa},
 booktitle={40th International Conference on Machine Learning (ICML)},
 year={2023}
}

@InProceedings{huang22a,
  title = 	 {Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents},
  author =       {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {9118--9147},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/huang22a/huang22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/huang22a.html},
  abstract = 	 {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. “make breakfast”), to a chosen set of actionable steps (e.g. “open fridge”). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models.}
}

@inproceedings{kwon2023reward,
 title={Reward Design with Language Models},
 author={Kwon, Minae and Xie, Sang Michael and Bullard, Kalesha and Sadigh, Dorsa},
 booktitle={International Conference on Learning Representations (ICLR)},
 year={2023}
}

@misc{mccarthy2021learning,
      title={Learning to communicate about shared procedural abstractions}, 
      author={William P. McCarthy and Robert D. Hawkins and Haoliang Wang and Cameron Holdaway and Judith E. Fan},
      year={2021},
      eprint={2107.00077},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{saycan2022arxiv,
    title={Do As I Can and Not As I Say: Grounding Language in Robotic Affordances},
    author={Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and Chuyuan Fu and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and Sergey Levine and Yao Lu and Linda Luu and Carolina Parada and Peter Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and Diego Reyes and Pierre Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan and Andy Zeng},
    year={2022},
    eprint={2204.01691},
    archivePrefix={arXiv},
    primaryClass={cs.RO}
}

@article{szot2023large,
  author    = {Szot, Andrew and Schwarzer, Max and Mazoure, Bogdan and Agrawal, Harsh and Talbott, Walter and Metcalf, Katherine and Mackraz, Natalie and Hjelm, Devon and Toshev, Alexander},
  title     = {Large Language Models as Generalizable Policies for Embodied Tasks},
  journal   = {preprint},
  year      = {2023},
}

@article{wang2023voyager,
  title   = {Voyager: An Open-Ended Embodied Agent with Large Language Models},
  author  = {Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi Fan and Anima Anandkumar},
  year    = {2023},
  journal = {arXiv preprint arXiv: Arxiv-2305.16291}
}

@misc{yuan2024selfrewarding,
      title={Self-Rewarding Language Models}, 
      author={Weizhe Yuan and Richard Yuanzhe Pang and Kyunghyun Cho and Xian Li and Sainbayar Sukhbaatar and Jing Xu and Jason Weston},
      year={2024},
      eprint={2401.10020},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

