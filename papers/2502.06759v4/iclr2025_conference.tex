
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{xcolor}     % remove this later

% Custom packages
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{listings}
\lstset{
   breaklines=true,
   basicstyle=\ttfamily}

\title{Rationalization Models for Text-to-SQL}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Gaetano Rossiello, Nhan Pham, Michael Glass, Junkyu Lee, Dharmashankar Subramanian \\
IBM T.J. Watson Research Center\\Yorktown Heights, NY, USA}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
% We propose a framework for synthesizing chain-of-thought rationales designed to fine-tune text-to-SQL models. These rationales are represented as a sequence of intermediate SQL statements and their explanations, serving as building blocks to compose the final SQL query that answers the user's request. This multi-stage process begins with the manual annotation of a few seed examples, which then prompt a large language model to bootstrap an iterative, dynamic few-shot procedure for knowledge distillation from a teacher model. Subsequently, a rationalization model is trained on the validated decomposed queries, enabling comprehensive chain-of-thought annotation of text-to-SQL datasets. We present an empirical comparison by fine-tuning small language models with and without these rationales on the BIRD dataset. The results demonstrate that step-by-step generation tends to achieve higher accuracy, particularly on moderately and highly challenging queries, while providing more interpretable output.

% added some context realted with STaR before mentioninig text to sql.
% In natural language comprehension tasks,
% bootstrapping a chain of rationales into large language models (LLMs)
% improves the performance, especially in complex reasoning benchmarks.
% The rationalization model generates intermediate thoughts for the chain-of-thought (CoT) style in-context learning (ICL)
% when the forward generation of thoughts fails to reach known ground truths.
% In this paper, we propose a framework for 
% synthesizing CoT rationales designed to fine-tune text-to-SQL models,
% where those rationales are represented as a sequence of explanations of decomposed sub-problems and intermediate SQLs.
% The overall bootstrapping procedure iterates distilling knowledge from a teacher model
% and generating text-to-SQL annotations from a rationalization model trained only on the validated intermediate SQLs.
% We evaluate our proposed framework on 8B parameter LLMs to demonstrate 
% that the proposed framework improves the performance on the BIRD dataset, while providing more interpretable output.

We introduce a framework for generating Chain-of-Thought (CoT) rationales to enhance text-to-SQL model fine-tuning. These rationales consist of intermediate SQL statements and explanations, serving as incremental steps toward constructing the final SQL query. The process begins with manually annotating a small set of examples, which are then used to prompt a large language model in an iterative, dynamic few-shot knowledge distillation procedure from a teacher model. A rationalization model is subsequently trained on the validated decomposed queries, enabling extensive synthetic CoT annotations for text-to-SQL datasets. To evaluate the approach, we fine-tune small language models with and without these rationales on the BIRD dataset. Results indicate that step-by-step query generation improves execution accuracy, especially for moderately and highly complex queries, while also enhancing explainability.
\end{abstract}

\section{Introduction}
% Natural language interfaces for querying databases have been a long-standing challenge in computer science~\cite{DBLP:journals/tods/HendrixSSS78, DBLP:journals/ker/CopestakeJ90}. While recent advancements in Large Language Models (LLMs) have demonstrated remarkable capabilities in generating code from natural language inputs, the text-to-SQL task is still far from human-level performance, especially in enterprise-like scenarios where domain adaptation presents a significant challenge~\cite{DBLP:conf/nips/LiHQYLLWQGHZ0LC23, DBLP:journals/corr/abs-2411-07763}.

% Modern text-to-SQL systems typically operate through a pipeline comprising three phases: (1) pre-generation, which includes components like schema linking to identify the relevant tables and columns for the user's query, (2) SQL generation, leveraging LLMs, and (3) post-generation, involving processes such as SQL self-verification, re-ranking, and repair~\cite{DBLP:journals/corr/abs-2411-08599, DBLP:journals/corr/abs-2410-01943, DBLP:journals/corr/abs-2408-07702, DBLP:journals/corr/abs-2405-16755}.
%
% combined two paragraphs
Recent advances in LLMs have demonstrated breakthroughs in the text-to-SQL task, a long-standing challenge in computer science~\citep{DBLP:journals/tods/HendrixSSS78, DBLP:journals/ker/CopestakeJ90}.
%Modern text-to-SQL systems typically operate through a pipeline comprised of the pre-generation stage equipped with the schema-linking models~\citep{glass2025extractiveschemalinkingtexttosql}, SQL generation stage leveraging LLMs, and the post-generation stage involving repair, verification, and selection
Modern text-to-SQL systems follow a pipeline: schema linking, SQL generation via LLMs, and post-processing (repair, verification, selection).
~\citep{DBLP:journals/corr/abs-2411-08599, DBLP:journals/corr/abs-2410-01943, DBLP:journals/corr/abs-2408-07702, DBLP:journals/corr/abs-2405-16755}.
Nevertheless, the performance of the best-performing systems is still far from human-level performance, especially in enterprise use cases~\citep{DBLP:conf/nips/LiHQYLLWQGHZ0LC23, DBLP:journals/corr/abs-2411-07763}.
% Question decomposition methods have proved to be a promising direction in addressing complex text-to-SQL translation tasks~\cite{DBLP:conf/emnlp/PourrezaR23, DBLP:journals/pvldb/GaoWLSQDZ24, DBLP:conf/coling/WangR0LBCYZYSL25, DBLP:journals/corr/abs-2307-07306}. Rather than generating the SQL query in a one-pass, these methods employ a step-by-step reasoning approach to break down the original question into a sequence of intermediate simpler sub-questions. This process not only helps LLMs produce more accurate results but also provides a human-readable explanation of the building blocks used to construct the final SQL query. Moreover, chain-of-thought (CoT) steps are essential for self-improvement~\cite{DBLP:conf/iclr/LightmanKBEBLLS24, deepseekai2025deepseekr1incentivizingreasoningcapability} and for enhancing the reasoning capabilities of smaller models by fine-tuning them through knowledge distillation~\cite{DBLP:conf/nips/ZelikmanWMG22, DBLP:conf/acl/LiHLJSL024}.

In parallel to complex natural language reasoning tasks,
question decomposition approaches have shown to be a promising direction
for complex text-to-SQL tasks~\citep{ DBLP:conf/emnlp/PourrezaR23, DBLP:journals/corr/abs-2307-07306, DBLP:journals/pvldb/GaoWLSQDZ24, DBLP:conf/coling/WangR0LBCYZYSL25}.
Rather than generating the desired SQL in a single pass, decomposition approaches break down 
the input natural language question into a sequence of simpler sub-questions and employ 
the step-by-step reasoning approach. 
%
The CoT style in-context learning methods often help LLMs to generate more accurate results and intermediate thoughts provide human-readable explanations. 
%
Moreover, CoT steps are essential for self-improvement~\citep{DBLP:conf/iclr/LightmanKBEBLLS24, deepseekai2025deepseekr1incentivizingreasoningcapability} and 
for enhancing the reasoning capabilities of smaller models by fine-tuning them through knowledge distillation~\citep{DBLP:conf/nips/ZelikmanWMG22, DBLP:conf/emnlp/0001GHW00023, DBLP:conf/acl/MagisterMAMS23, DBLP:conf/acl/LiHLJSL024}.
% In addition, those CoT steps could leverage the LLM reasoning capability 
% through the knowledge distillation~\citep{DBLP:conf/nips/ZelikmanWMG22, DBLP:conf/acl/LiHLJSL024},
% especially for fine-tuning the smaller models.
% This paragraph can be used in the methodology section.
%
 

However, publicly available text-to-SQL datasets often lack step-by-step decompositions, 
and manual annotation can be expensive. %manually annotating such datasets is typically cost-prohibitive. 
Therefore, we propose a framework for generating rationales in CoT style for the text-to-SQL task, which comes with unique challenges.
% Namely, LLMs cannot directly generate predefined rationales from a pair of the input question and the final SQL, in contrast to method such as STaR~\citep{DBLP:conf/nips/ZelikmanWMG22} for solving math or commonsense reasoning benchmarks.
% We start with minimal human annotations, such as existing text-to-SQL training sets, and incorporate dynamic few-shot sample selection to bootstrap valid rationales and fine-tune rationalization models.
We start with existing text-to-SQL training sets, provide a few human annotations, and incorporate dynamic few-shot sample selection to bootstrap valid rationales and fine-tune rationalization models.
% In this work, we propose a framework to automatically generate reasoning steps for a given set of (text, SQL) pairs. Our multi-stage approach requires minimal human annotation, relying only on a small set of seed examples. The first stage is crucial for defining the CoT representation, which can be used to customize outputs and adapt them to a specific domain. 
%
%
% Using these seed examples, we initiate a dynamic few-shot process by leveraging an LLM as a teacher. The LLM processes a prompt containing the database (DB) schema—filtered by a schema linker~\cite{glass2025extractiveschemalinkingtexttosql}—the user question, a list of few-shot examples annotated with CoTs, and an instruction to generate a sequence of reasoning steps. These steps are presented as intermediate SQL queries accompanied by corresponding textual explanations, with the final SQL step serving as the answer to the question. A validation component ensures quality by executing the final SQL from the generated CoT and the gold SQL from the training set against the DB. If the two result sets match exactly, the CoT is labeled as a positive example; otherwise, it is labeled negative. The validated CoTs are iteratively collected into a repository, where they may be selected as few-shot examples in the next iterations. The selection process is guided by an SQL-based similarity function, which clusters the generated CoTs, enabling a case-based reasoning approach. The dynamic few-shot approach enhances the selection of diverse CoT annotations after each iteration, thereby increasing the proportion of positively validated rationales. When applied to the BIRD~\cite{DBLP:conf/nips/LiHQYLLWQGHZ0LC23} training examples, the coverage of valid generated CoTs increases from 53.18\% after the first iteration to 73.86\% when using the Llama 3.1 70B instruct model as the teacher~\cite{DBLP:journals/corr/abs-2407-21783}.
%
% To achieve complete annotation coverage, we fine-tune a rationalization model -based on Llama 3.1 8B-, by exploiting the validated CoTs and applying it to the remaining training examples that lack correct generated rationales from the previous stage. This process allows us to construct a training dataset for the rationalization model, using the filtered DB schema, the user question, and the gold SQL as the input, and the fully validated CoT as the output.
%
%

Our preliminary evaluation on the BIRD benchmark shows that fine-tuning smaller models with intermediate rationales consistently enhances performance, especially on moderate to challenging problems. Additionally, these fine-tuned models generate clear explanations of the query-building process and offer actionable plans for implementing corrective measures at each step.
% We assess the quality of the text-to-SQL dataset, including generated rationales, using the BIRD development set. 
% Our evaluation compares the execution accuracy of smaller LLMs fine-tuned both with and without CoT reasoning. 
% The results indicate that models fine-tuned to generate intermediate SQL steps consistently perform better on moderate and complex test queries. Additionally, these models produce interpretable and parsable outputs, enabling explicit explanations of the query-building process and providing actionable plans to implement corrective measures after each step.


\section{Methodology}

\begin{figure}[t]
    \centering
    \subfigure[]{\includegraphics[width=0.45\textwidth]{figures/CoT.drawio4.pdf}\label{fig:framework}} 
    \subfigure[]{\includegraphics[width=0.5\textwidth]{figures/CoT_markdown.pdf}\label{fig:cot_markdown}} 
    \caption{(a) Framework overview (b) Step-by-step SQL building rationale.}
    \label{fig:cot}
\end{figure}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/CoT_markdown.pdf}
%     \caption{Example of a step by step SQL-building rationale.}
%     \label{fig:cot_markdown}
% \end{figure}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/CoT.drawio4.pdf}
%     \caption{Framework overview of the step-by-step SQL rationales generation process.}
%     \label{fig:framework}
% \end{figure}

Figure~\ref{fig:framework} provides an overview of the synthetic data generation method. The process involves a subject matter expert who defines the representation of rationales, a text-to-SQL dataset containing (question, gold SQL) pairs, a DBMS for validation, a teacher LLM with few-shot capabilities, and, finally, a smaller student LLM used to fine-tune the rationalization model. 

\subsection{SQL-Building Rationale Representation}
The first stage involves manually annotating a few seed examples with the desired rationale representation.
There are various approaches to generating CoT steps, including manual~\citep{DBLP:conf/nips/Wei0SBIXCLZ22,DBLP:conf/iclr/FuPSCK23}, automatic~\citep{DBLP:conf/nips/KojimaGRMI22,DBLP:journals/tmlr/ChenM0C23}, and semi-automatic~\citep{DBLP:conf/emnlp/ShumDZ23,DBLP:conf/iclr/Lu0CWZRCK23,DBLP:journals/corr/abs-2302-04813} prompting techniques.
Our approach aims to harness the benefits of all three methods by combining them effectively. %TODO: Can we drop this or explain more? What are the benefits of each approach?
First, we employ automatic prompting with the zero-shot instruction (see Appendix~\ref{appendix_1}) to generate an initial draft of SQL steps.
We then refine this draft by incorporating a preliminary step that outlines the plan and structuring it using Markdown (see Appendix~\ref{appendix_2}).
Markdown facilitates easy parsing of the generated output, allowing for the extraction of individual intermediate SQL statements and enhancing visualization for qualitative assessment.

We develop the rationale by emulating the cognitive process of incrementally constructing an SQL query, joining tables, and adding conditions step-by-step. 
Figure~\ref{fig:cot_markdown} illustrates an example of such a CoT representation. Given a question, the first step involves rephrasing it, outlining the decomposition steps, and identifying the relevant tables and columns while justifying their selection. %This step does not yet include intermediate SQL statements. 
It benefits the user by providing an interpretable overall solution plan and aids the LLM by offering focused context to enhance in-context learning. Then, the SQL-building process continues through a sequence of steps with increasing complexity, providing a textual headline along with the corresponding intermediate, executable SQL. The SQL at the final step represents the solution to the given input question.


\subsection{Knowledge Distillation with Dynamic Few-shot}
Manually curated seed examples are stored in a repository of validated SQL CoTs, to boostrap a dynamic few-shot rationale generation process.
This iterative approach is applied to each (question, gold SQL) pair within a given text-to-SQL training set. For each training instance, the gold SQL serves as a query to rank the most similar examples from the validated SQL CoTs.
To implement the example ranking function, we construct a sparse vector space model, where distinct SQL-reserved keywords define the feature space.
Each validated CoT’s final SQL statement is extracted and transformed into a sparse vector representing SQL keyword frequency.
The gold SQL is projected into the same vector space and used as a query vector to rank SQL CoT examples based on cosine similarity.
Finally, the top-n most similar examples, annotated with SQL-building rationales, are selected as few-shot examples and incorporated into the prompt.
This strategy facilitates the selection of validated CoT examples with similar SQL structures, such as nested queries, aggregation functions, or multi-joins, drawing inspiration from a case-based reasoning approach.

We initiate a dynamic few-shot process by leveraging an LLM as a teacher. The LLM processes a prompt containing the database (DB) schema—filtered by a schema linker~\citep{DBLP:journals/corr/abs-2501-17174}—the user question, a list of top-n similar few-shot examples annotated with CoTs, and an instruction to generate a sequence of reasoning steps. %TODO: what is n in the top-n?

A validation component ensures quality by executing the final SQL, from the generated CoT, and the gold SQL, from the training set, against the DB. If the two result sets match exactly, the CoT is labeled as a positive example; otherwise, it is labeled negative. The validated CoTs are iteratively collected into a repository, where they may be selected as few-shot examples in the next iterations. The selection process is guided by the SQL-based similarity function, which clusters the generated CoTs. The dynamic few-shot approach enhances the selection of diverse CoT annotations after each iteration, thereby increasing the proportion of positively validated rationales. The dynamic few-shot generation process iterates over the (question, gold SQL) training pairs until the number of validated SQL CoTs no longer increases.

\subsection{Rationalization Model Fine-tuning}
Despite the dynamic few-shot process with the teacher model significantly enhancing the coverage of validated SQL CoT annotations (see Section~\ref{sec:validation}), some training instances may still contain invalid, synthetically generated rationales.
%To achieve complete annotation coverage, we fine-tune a rationalization model as a student, using the validated CoTs as training and applying it to the remaining training examples that lack correct generated rationales from the previous dynamic few-shot stage. This process allows us to construct a training dataset for the rationalization model, using the filtered DB schema, the user question, and the gold SQL as the input, and the fully validated SQL-building CoT as the output.
Following STaR~\citep{DBLP:conf/nips/ZelikmanWMG22}, we distinguish rationales from rationalizations. Rationales are CoTs produced without knowing the target answer, while rationalizations are produced as step-by-step explanations of already known answers.
To achieve complete annotation coverage, we fine-tune a rationalization model and apply it to the remaining training examples that lack correct generated rationales.  Validated CoTs from the previous dynamic few-shot stage provide the training dataset for the rationalization model. The filtered DB schema, the user question, and the gold SQL are the input, and the fully validated SQL-building CoT is the output.

The rationalization model is designed not to generate an SQL solution for a given user request but to produce step-by-step reasoning by incorporating both the question and the correct SQL query into the input prompt. As in the previous stage, the execution of the final SQL query derived from the generated CoT is compared with the execution of the gold-standard SQL. Our findings indicate that a text-to-SQL rationalization model fine-tuned in this manner can help identify incorrect (question, gold SQL) training pairs. In rare cases where the result sets do not match, the provided ground truth may contain inconsistencies that can be easily detected and corrected.

Furthermore, a fine-tuned rationalization model can be utilized to generate SQL-building rationales for other text-to-SQL datasets, even across different domains, without needing to repeat the entire process from scratch. This presents a promising direction for future research.

\section{Experiments}
% experiment settings, resource computing power,
% data set selection and division into 4 cases (levels, and all)
% model selection 8B model
To evaluate the quality of the generated SQL CoTs, we conducted a controlled experiment using the \texttt{train} and \texttt{dev} sets of the BIRD~\citep{DBLP:conf/nips/LiHQYLLWQGHZ0LC23} dataset. 
This dataset is particularly well-suited to our study, as its \texttt{train} set (containing question-SQL pairs) does not include query decomposition steps. 
Furthermore, the \texttt{dev} set is organized into three main categories -simple, moderate, and challenging SQL statements- enabling a fine-grained, independent evaluation of model performance across different difficulty levels.

We begin by cleaning the training set, removing instances where the gold SQL is not executable, either due to syntax errors, timeouts, or empty result sets. After the cleaning process, we collect 8,807 text-to-SQL training pairs. The original \texttt{dev} set, consisting of 1,534 instances, is retained. We use the open-source Llama 3.1 LLM family~\citep{DBLP:journals/corr/abs-2407-21783}, specifically the 70B version as the teacher for the dynamic few-shot process, and the 8B version as the student to fine-tune both the rationalization model and the text-to-SQL models, with and without CoTs.


\subsection{SQL Rationale Training Coverage}
\label{sec:validation}

\begin{table}[t]
\small
\centering
\bgroup
\def\arraystretch{1.2}%  1 is the default, change whatever you need
\begin{tabular}{c|c|c}
Stage            & Model         & CoT Train\% coverage \\ \hline
Manual Few-shot  & Llama 3.1 70B & 53.18            \\
Dynamic Few-shot & Llama 3.1 70B & 73.86            \\
Fine-tuning      & Llama 3.1 8B  & \textbf{99.02} \\ \hline           
\end{tabular}
\egroup
\label{table:exp_coverage}
\caption{Percentage of validated SQL CoTs in BIRD training set after each rationalization stage.}
\end{table}

The coverage of validated CoT at each stage is shown in Table~\ref{table:exp_coverage}. After manually annotating two random few-shot examples, the teacher model produces correct CoT for 53.18\% of the training instances. The dynamic few-shot approach increases the coverage by more than 20\% after 13 iterations, alternating between greedy and sampling decoding methods. Finally, applying the fine-tuned rationalization model to the instances with incorrect CoT ensures full training coverage. We manually assessed the remaining 86 (0.98\%) instances and found that, in the majority of cases, the generated CoT was correct, while the provided gold standard contained ambiguities or errors.

%When applied to the BIRD~\cite{DBLP:conf/nips/LiHQYLLWQGHZ0LC23} training examples, the coverage of valid generated CoTs increases from 53.18\% after the first iteration to 73.86\% when using the Llama 3.1 70B instruct model as the teacher~\cite{DBLP:journals/corr/abs-2407-21783}.

\subsection{Impact of SQL Rationales in Fine-tuning Text-to-SQL Models}
\label{sec:impact}
\begin{table}[t]
\small
\centering
\bgroup
\def\arraystretch{1.2}%  1 is the default, change whatever you need
\begin{tabular}{c|c|c|c|c|c|c|c}
Stage                        & Model                         & Train\%                & Dataset   & Simple & Moderate & Challenge & Total \\ \hline
\multirow{3}{*}{Dyn. Few-shot}    & \multirow{3}{*}{Llama3.1 8B} & \multirow{3}{*}{73.86} & Gold SQL  &   \textbf{72.00}     &   54.53       &     42.76      &   \textbf{63.95}    \\
                             &                               &                        & CoT Short &    70.7    &     \textbf{55.39}     &     46.9      &   63.82    \\
                             &                               &                        & CoT Long  &    69.08    &       52.37   &     \textbf{47.59}      &    61.99   \\ \hline\hline
\multirow{3}{*}{Fine-tuning} & \multirow{3}{*}{Llama3.1 8B}  & \multirow{3}{*}{99.02} & Gold SQL  &    73.08    &     58.06    &    47.92       &  66.17     \\
                             &                               &                        & CoT Short &    72.54    &     59.78     &    52.08       &    66.75   \\
                             &                               &                        & CoT Long  &    \textbf{73.41}    &     \textbf{60.00}     &     \textbf{52.78}      &    \textbf{67.41}   \\\hline
                              
\end{tabular}
\egroup
\label{table:exp_finetuning}
\caption{Execution accuracy comparison on BIRD \texttt{dev} set. \textit{Gold}, \textit{Short CoT}, \textit{Long CoT} refer to the accuracy achieved by the models fine-tuned on the original gold standard SQLs, the shortest CoT and the longest CoT, respectively.}
\end{table}
Due to the iterative nature of dynamic few-shot learning, training instance pairs can generate multiple valid synthetic CoTs. We created six different training sets for the text-to-SQL task, split into two groups based on the number of instances. These sets use the same input prompts but differ in their targets: the gold SQL without CoT, which serves as a baseline, and selections of the shortest/longest CoTs for each train instance. The goal is to evaluate the impact on the execution accuracy of Llama 3.1 8B, fine-tuned with and without query decomposition steps, both before and after the application of the rationalization model. The results of the fine-tuned models applied to the BIRD development set are presented in Table~\ref{table:exp_finetuning}. Using 73.68\% of the training set, after the dynamic few-shot process, a model fine-tuned without CoT demonstrates overall better performance, particularly on the simple questions, which are more frequent. However, the model fine-tuned with the short/long CoT demonstrates a greater ability to handle moderate and challenging questions, respectively. After applying the rationalization model with full coverage of the training set, the text-to-SQL model, fine-tuned on the longest CoT, outperforms in all three categories, improving overall performance by +1.24\%, with a notable increase of +4.86\% on the challenging queries.




\section{Conclusion}
In this work, we presented a framework designed to facilitate the annotation of text-to-SQL datasets with CoT rationales, which consist of intermediate SQL-building steps. The process requires minimal manual annotations, using only a few examples to define the rationale representation. A teacher model is then employed to distill a large volume of validated synthetic CoTs, which are used to bootstrap a rationalization model that enables full training coverage with CoT. We introduced a sample selection method based on SQL similarity, supporting a dynamic few-shot approach. Our comparison demonstrates that long CoTs enhance the accuracy of text-to-SQL tasks while providing an interpretable, user-friendly output that clarifies the reasoning process behind query construction.

% Self-verification~\cite{DBLP:conf/emnlp/WengZX0HLSLZ23},
% Multiple CoT fine-tuning~\cite{DBLP:journals/corr/abs-2407-03181}


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\section{Appendix}
\subsection{Text-to-SQL Prompt Representation (Source)}
\label{appendix_1}
\begin{lstlisting}
[SCHEMA]
CREATE TABLE prof (
 prof_id INTEGER PRIMARY KEY, -- unique id for professors
 first_name TEXT, -- the first name of the professor
 last_name TEXT, -- the last name of the professor
 popularity INTEGER, -- popularity of the professor
);

prof.prof_id
prof.first_name: 'Bernhard', 'Hattie', 'Mateo'
prof.last_name: 'Conkay', 'Cunningham', 'Ewenson'
prof.popularity: 2, 3

CREATE TABLE registration (
 student_id INTEGER, -- the id of students
 FOREIGN KEY(student_id) REFERENCES student(student_id)
);

registration.student_id

CREATE TABLE ra (
 student_id INTEGER, -- the id numbe representing each student
 capability INTEGER, -- the capability of student on research
-- (Evaluated by the professor)
 prof_id INTEGER, -- professor who advises this student
 FOREIGN KEY(student_id) REFERENCES student(student_id)
 FOREIGN KEY(prof_id) REFERENCES prof(prof_id)
);

ra.student_id
ra.capability: 2, 4, 5
ra.prof_id

CREATE TABLE student (
 student_id INTEGER PRIMARY KEY, -- the unique id to identify students
 f_name TEXT, -- the first name of the student
);

student.student_id
student.f_name: 'Ahmed', 'Alvera', 'Amerigo'

selected: prof.popularity, prof.prof_id, ra.student_id, None, ra.prof_id, ra.capability, student.student_id, prof.last_name, student.f_name, prof.first_name
join: ra.prof_id, prof.prof_id, None, student.student_id, ra.student_id, registration.student_id, ra.capability, prof.popularity
condition: ra.capability, prof.popularity, None, prof.prof_id, ra.prof_id, ra.student_id, student.student_id, prof.last_name
order: prof.popularity, None, ra.student_id, ra.prof_id, prof.prof_id, student.student_id, ra.capability
group: None, ra.prof_id, prof.prof_id, prof.popularity, ra.student_id, ra.capability, prof.last_name

Comparisons:
ra.capability
3 (many), 5 (many), 4 (many)
ra.capability = 5
5 (many)

Note:
highest popularity refers to MAX(popularity)
research capability refers to capability
capability = 5
[/SCHEMA]

[QUESTION]
Among professors with the highest popularity, how many of their students have research capability of 5?
[/QUESTION]

Given the above [SCHEMA] of a database and a question [QUESTION], translate the question into a valid SQLite statement. Decompose the SQL in increasingly complex building blocks. Explain each step of the SQL building process thinking step by step. Format the output using the Markdown language.
\end{lstlisting}

\subsection{SQL Rationale Representation (Target)}
\label{appendix_2}
\begin{lstlisting}
**Step 1: Identify the required tables and columns**
--

From the question, we need to find the number of students with research capability of 5 among professors with the highest popularity. This implies we need to:

1. Find the highest popularity of professors.
2. Filter professors with the highest popularity.
3. Join the `ra` table to get the students advised by these professors.
4. Filter students with research capability of 5.
5. Count the number of students.

Required tables:
* `prof` (contains professor information)
* `ra` (maps students to professors)
* `student` (contains student information)

Required columns:
* `prof.popularity` (to find the highest popularity)
* `ra.capability` (to filter students with research capability of 5)
* `ra.student_id` (to count the number of students)

**Step 2: Find the highest popularity of professors**
--

```sql
SELECT MAX(popularity) AS max_popularity
FROM prof;
```

**Step 3: Filter professors with the highest popularity**
--

```sql
SELECT *
FROM prof
WHERE popularity = (SELECT MAX(popularity) FROM prof);
```

**Step 4: Join the `ra` table to get the students advised by these professors**
--

```sql
SELECT ra.student_id
FROM prof JOIN ra ON prof.prof_id = ra.prof_id
WHERE prof.popularity = (SELECT MAX(popularity) FROM prof);
```

**Step 5: Filter students with research capability of 5**
--

```sql
SELECT ra.student_id
FROM prof JOIN ra ON prof.prof_id = ra.prof_id
WHERE prof.popularity = (SELECT MAX(popularity) FROM prof) AND ra.capability = 5;
```

**Step 6: Count the number of students**
--

```sql
SELECT COUNT(ra.student_id) AS num_students
FROM prof JOIN ra ON prof.prof_id = ra.prof_id
WHERE prof.popularity = (SELECT MAX(popularity) FROM prof) AND ra.capability = 5;
```

This is the final SQL statement that answers the question.
\end{lstlisting}

\end{document}


% {
% \color{magenta}
% Here, we investigate how rationale helps text-to-SQL. 

% Generating rationale to and fine-tune the model with rationales – Self taught 

% Fine-tune LLM to generate rationales when forward generation of rationale fails  
% \begin{itemize}
%     \item In natural language reasoning, no need to fine-tune the model, simply generate rationale given known answer since we are generating rationales given question and answer  
%   \item  The rationales in text-to-SQL is also SQL, unlike intermediate steps of arithmetic or justifications in common sense QA 
%   \item This means reusing the same model for forward generation won’t generate proper SQLs as rationale -- > we see saturation of coverage whatever we do; aggregate all from different hyperparameters, different models, etc 
%   \item  This highlights the necessity of training rationale model for text to SQL 
%   \item  This is first contribution that differentiate from other works 
% \end{itemize}

% Fine-tuning with additional data with rationale, does it improve the performance like natural language reasoning tasks? 
% \begin{itemize}
%     \item We can compare FT with only few-shot 
%   \item FT with rationalization model generated rationale 
%   \item Better coverage improves performance 
%   \item Huang 2024b (not yet reasoning paper) illustrate this phenomenon in prompting; the argument in the paper: if rationale is not guaranteed like oracle, it will harm the performance. If rationale was oracle, the performance improves but it doesn’t make sense (knowing answer why solve?) 
% \end{itemize}

% Datasets 
% \begin{itemize}
%     \item BIRD [simple, moderate, challenging] separate the evaluation set 
%   \item  Spider 1 (easier than BIRD)  -- if people want more than 1 dataset... 
%   \item  Rationalization model works across different datasets? -- should be..? 
% \end{itemize}

% Text to SQL comparison [TBD] 
% \begin{itemize}
%     \item Ablations – many combinations to compare with 
%   \item Different model combinations – like new DeepSeek will do better? 
%   \item Variations in rationale construction and usage 
% \end{itemize}

% problems specific to text-to-sql, why proposed approach? (related work text to sql)
% \begin{itemize}
%     \item  Most of the research focus on natural language reasoning tasks.
% Such as math, common sense, logic, etc (more with refs shown in other papers)
%     \item Those problems sets are either multiple choice type questions or text generation task.
% The answer doesn't need to be executable programs, sometimes it helps, but it is not strictly necessary form.
% (differentiate from text to sql)
%     \item  In case of text to sql generation,  it has its own challenges.
% the final generation must be executable SQL given database schema.
%     \item  In contrast to the natural language reasoning research, generating rationale (steps or thoughts) are very limited.
%     \item  Text-to-SQL  Task description needed.
%     \item Due to the difference in the task, the existing methods may not work well in the new settings.
%     \item Text-to-SQL works well with fine-tuned large language model. 
% Because we need to train the different form, database schema and code, table representations, etc
% (is it true?)
%     \item Pure in-context learning technique might not good enough (ensembles of multiple models, generations, etc)
% We also consider distillation approach from a model that is trained to generate rationales for text to SQL generation.
% We show the improvement on generating the steps.
%     \item Then, we also test its impact to end-to-end performance to see if such addition of thought generation improves
% text to SQL generation capability.
% \end{itemize}

% Contributions
% \begin{itemize}
%     \item prompt template construction for text to sql (syntax form markdown, template, few shots)
%     \item steps generations -- dynamic prompting, with distillation, multiple model ensemble
%     \item fine-tuning model with generated data
%     \item prompting ??? (we haven't tried this yet, probably? )
% \end{itemize}
% }


% \section{Related Work}
% \subsection{Chain-of-Thought}
% Manual prompting~\cite{DBLP:conf/nips/Wei0SBIXCLZ22,DBLP:conf/iclr/FuPSCK23}, 
% Automatic prompting~\cite{DBLP:conf/nips/KojimaGRMI22,DBLP:journals/tmlr/ChenM0C23},
% Semi-automatic prompting~\cite{DBLP:conf/emnlp/ShumDZ23,DBLP:conf/iclr/Lu0CWZRCK23,DBLP:journals/corr/abs-2302-04813},
% Knowledge Distillation~\cite{DBLP:conf/nips/ZelikmanWMG22,DBLP:conf/emnlp/0001GHW00023,DBLP:conf/acl/MagisterMAMS23},



% {
% \color{red}
% basics  (related work chain of thought prompting, reasoning LLM)
% \begin{itemize}
%     \item CoT (Wei 2022) improves complex natural langauge reasoning tasks.
% Divides the problem into subproblems and solve and combine solution.
%     \item In earlier prompt engineering, such steps that hints LLM with the subproblem decomposition
% given by annotators with the rationale sometimes in the form of executable programs (PoT)
%     \item Since manual annotation is costly, 
% automatic ways of generating such steps or semi-automated ways are proposed.
% \end{itemize}

% advanced    (related work dynamic prompting and distillation)
% \begin{itemize}
%     \item On top of static prompt construction,
%     more advanced methods generates self-feedback during prompting (in-context learning)
%     and improve the results.[verify and refine]
%     \item But self-feed back cannot fix errors yet and \textbf{external signal} is needed (Huang 2024a)
%     %
%     \item The question decomposition strategy beyond showing static set of subproblem, answer, rationale
% is also proposed. least to most dynamic prompting.
%     \item Another source of improvement is "distillation"
% As opposed to the earlier examples soley rely on in-context learning,
% "distillation" transfer the reasoning capability to other model, especially from large to smaller models.
% \end{itemize}

% more references
% general advanced CoT methods for natural language reasoning
% \begin{itemize}
%     \item semi-auto prompt construction
% few shot by Auto-gen rationales
% Zhang 2023h
% Zou 2023
% Wan 2023
% Xu 2023
% A small number of human-annotation
% Shao 2023b FW BW
% Pitis 2023
% Shum 2023
% Lu 2023b
% Ye and Durrett 2023
%     \item verify and refinement
% Paul 2024a REFINER train critic model
% Maddan 2023 Self refine
% Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023g. Making language models better reasoners with step-aware verifier.
% Shridhar 2023 reasoning with revisions
% Gou 2024a external tool for correction
% Nathani 2023 MAF, multi-aspect feedback?    
%     \item question decomposition
% Zhou 2023b L2M subquestions		least to most
% Dua 2022 solution from prev
% Khot 2023 modular tasks?
% Huan 2024b DAG by QDMR – text to sql?	
% Zhang 2024 cumulative reasoning – logic    
%     \item distillation
% STAR
% \end{itemize}
% }
% {
% \color{blue}
% Summary of LLM reasoning papers by rationale generation and self-improvement/corrections 

% Keywords:  
% generating rationalization/explanation, 
% question decomposition,  
% verification and refinement,  
% Fine-tuning with self-generated rationales. 

 
% For solving reasoning tasks – GSM8K, adding two numbers (symbolic reasoning/scratch pad), common sense QA, multiple choice QA, etc, [Rajani, McCann, Socher 2019 Explain yourself] showed that manually curated explanations improve performance.  

% “Scratchpad” and “Chain-of-Thoughts" are another prompting style that improves the performance by providing rationale, which leads to wider adoption of the in-context learning approach. 
% The main research topics are generating rationales in automated or semi-automated (many papers) way and verifying and refining such steps of rationales that lead to the final answer. Those rationales generated by LLMs can be feedback to LLMs to improve the reasoning (STaR, LLM can self-improve).  
% In those successful cases, the ground-truth answer for a question is available and it is easy to verify the answer from LLMs. When the correctness of the self-generated intermediate rationales is not guaranteed, Huang 2024b showed that the self-correction through in-context learning with GPT-3.5 and GPT-4 fails, and it even decreases the performance. 
% }


% \subsection{Text2SQL}
% BIRD~\cite{DBLP:conf/nips/LiHQYLLWQGHZ0LC23} leaderboard systems~\cite{DBLP:journals/corr/abs-2411-08599}, \cite{DBLP:journals/corr/abs-2410-01943}, \cite{DBLP:journals/corr/abs-2408-07702}, \cite{DBLP:journals/corr/abs-2405-16755},
% \cite{DBLP:journals/corr/abs-2405-07467},
% \cite{DBLP:journals/corr/abs-2312-11242},
% \cite{DBLP:conf/iclr/LightmanKBEBLLS24}

% {
% \color{red}
% text to sql overview
% \begin{itemize}
%     \item leader board, methods, approaches
%     \item text to sql that exploits rationale. thoughts
% \end{itemize}

% \color{blue}
% In the case of text-to-SQL, what has been done? 
% Fine-tuning is a dominant approach.  
% As target queries get more complicated, standard prompting approaches saturate the performance. We need reasoning approaches that are successful in math, symbolic. common sense reasoning tasks. 
% Some of the existing methods that focus on in-context learning and improvement through rationalization, we see the following papers. 

% [PET-SQL][DIN-SQL][DAIL-SQL][MAC-SQL] -- Prompt, CoT? 
% [MCS-SQL] [CHESS][SEA-SQL] [Xiyan-SQL]-- multiple? 

% Question decomposition/ Refinement in SQL 
% [Xie et al 2024] Decomposition for enhancing attention 
% [Mao et al 2024] Enhancing Text-to-SQL paring through Question Rewriting and Execution Guided Refinement 
% [Wang et al 2024] DAC: Decomposed Automation Correction for Text-to-SQL 
% [Cen et al 2024] SQLFixAgent: Toward Semantic-Accurate Text-to-SQL parsing via Consistency Enhanced Multi-Agent Collaboration 
% [Xie et al 2024] MAG-SQL Multi-Agent Genreative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL 
% }

% {
% \color{red}
% The novelty in the framework?
% can we find similar framework in non text-to-sql task papers?
% what is the closest ones? what is different and why?
% Is it mixing many things in a single framework?
% question decomposition (or sql decomposition as rationale?)
% no verify and refine? the role of rationalization model?
% with what data rationalizaton model trained?
% }