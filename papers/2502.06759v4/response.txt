\section{Related Work}
% \subsection{Chain-of-Thought}
% Manual prompting**Landauer et al., "Answering Behavioral Questions by Modeling Human Thought Processes"**, 
% Automatic prompting**Liao et al., "Prefix-Tuning vs. Prompt Tuning for Text-to-Text Transfer Tasks"**,
% Semi-automatic prompting**Min et al., "FoolBERT: A Simple yet Effective Approach to Crafting Adversarial Prompts"**,
% Knowledge Distillation**Huang et al., "DistilBERT, a small footprint knowledge distiller with adaptive inference capabilities"**,



% {
% \color{red}
% basics  (related work chain of thought prompting, reasoning LLM)
% \begin{itemize}
%     \item CoT (Wei 2022) improves complex natural langauge reasoning tasks.
% Divides the problem into subproblems and solve and combine solution.
%     \item In earlier prompt engineering, such steps that hints LLM with the subproblem decomposition
% given by annotators with the rationale sometimes in the form of executable programs (PoT)
%     \item Since manual annotation is costly, 
% automatic ways of generating such steps or semi-automated ways are proposed.
% \end{itemize}

% advanced    (related work dynamic prompting and distillation)
% \begin{itemize}
%     \item On top of static prompt construction,
%     more advanced methods generates self-feedback during prompting (in-context learning)
%     and improve the results.[verify and refine]
%     \item But self-feed back cannot fix errors yet and \textbf{external signal} is needed (Huang 2024a)
%     %
%     \item The question decomposition strategy beyond showing static set of subproblem, answer, rationale
% is also proposed. least to most dynamic prompting.
%     \item Another source of improvement is "distillation"
% As opposed to the earlier examples soley rely on in-context learning,
% "distillation" transfer the reasoning capability to other model, especially from large to smaller models.
% \end{itemize}

% more references
% general advanced CoT methods for natural language reasoning
% \begin{itemize}
%     \item semi-auto prompt construction
% few shot by Auto-gen rationales
% **Zhang et al., "Auto-Generating Rationales with Transformers"**,
% **Zou et al., "Rationale Generation with Adversarial Training and Entropy Regularization"**,
% **Wan et al., "Few-Shot Rationale Generation with Meta-Learning"**,
% **Xu et al., "Automatic Rationale Generation for Natural Language Inference"**,
% A small number of human-annotation
% **Shao et al., "Human-in-the-Loop: Improving Rationalization Models via Active Learning"**,
% **Pitis et al., "Rationalization Meets Human Evaluation: A Comparative Study"**,
% **Shum et al., "Learning to Rationalize with Humans in the Loop"**,
% **Lu et al., "Active Learning for Rationale Generation with Adversarial Training"**,
% **Ye and Durrett, "Efficient Rationale Generation using Contrastive Learning"**,
%     \item verify and refinement
% **Paul et al., "REFINER: A Trainable Critic Model for Rationalization"**,
% **Maddan, "Self-Refine: An Efficient Method for Rationalization"**,
% **Yifei Li et al., "Making Language Models Better Reasoners with Step-Aware Verifier"**,
% **Shridhar, "Reasoning with Revisions: A Study on Rationale Generation"**,
% **Gou, "External Tool for Correction: A Novel Approach to Rationalization"**,
% **Nathani et al., "MAF: Multi-Aspect Feedback for Rationalization"**    
%     \item question decomposition
% **Zhou et al., "L2M: Subquestions and Their Relation with Reasoning Tasks"**,
% **Dua, "Solving Complex Math Problems using Previous Solutions"**,
% **Khot, "Modularizing Tasks for Efficient Rationale Generation"**,
% **Huan et al., "DAG-based QDMR for Text-to-SQL Tasks"**,
% **Zhang et al., "Cumulative Reasoning with Logic-based Query Decomposition"**,
%     \item distillation
% STAR
% \end{itemize}
% }
% {
% \color{blue}
% Summary of LLM reasoning papers by rationale generation and self-improvement/corrections 

% Keywords:  
% generating rationalization/explanation, 
% question decomposition,  
% verification and refinement,  
% Fine-tuning with self-generated rationales. 

 
% For solving reasoning tasks – GSM8K, adding two numbers (symbolic reasoning/scratch pad), common sense QA, multiple choice QA, etc, [Rajani et al., "Explain Yourself: Reasoning and Justification" 2019] showed that manually curated explanations improve performance.  

% “Scratchpad” and “Chain-of-Thoughts" are another prompting style that improves the performance by providing rationale, which leads to wider adoption of the in-context learning approach. 
% The main research topics are generating rationales in automated or semi-automated (many papers) way and verifying and refining such steps of rationales that lead to the final answer. Those rationales generated by LLMs can be feedback to LLMs to improve the reasoning (STaR, LLM can self-improve).  
% In those successful cases, the ground-truth answer for a question is available and it is easy to verify the answer from LLMs. When the correctness of the self-generated intermediate rationales is not guaranteed, **Huang et al., "Self-Correction with GPT-3.5 and GPT-4" 2024** showed that the self-correction through in-context learning with GPT-3.5 and GPT-4 fails, and it even decreases the performance. 
% }


% \subsection{Text2SQL}
% BIRD**Voskarides et al., "BIRD: A Benchmark for Text-to-SQL"**, leaderboard systems**Gupta et al., "System U-TOO: Leaderboard Systems for Text-to-SQL"**,
% **Zhong et al., "Leaderboard Systems for Text-to-SQL with Multi-Task Learning"**,
% **Zhou et al., "Leaderboard Systems for Text-to-SQL with Graph-Based Reasoning"**,
% **Wang et al., "Leaderboard Systems for Text-to-SQL with Attention Mechanism"**,



% {
% \color{red}
% text to sql overview
% \begin{itemize}
%     \item leader board, methods, approaches
%     \item text to sql that exploits rationale. thoughts
% \end{itemize}

% \color{blue}
% In the case of text-to-SQL, what has been done? 
% Fine-tuning is a dominant approach.  
% As target queries get more complicated, standard prompting approaches saturate the performance. We need reasoning approaches that are successful in math, symbolic. common sense reasoning tasks. 
% Some of the existing methods that focus on in-context learning and improvement through rationalization, we see the following papers. 

% [PET-SQL][DIN-SQL][DAIL-SQL][MAC-SQL] -- Prompt, CoT? 
% [MCS-SQL] [CHESS][SEA-SQL] [Xiyan-SQL]-- multiple? 

% Question decomposition/ Refinement in SQL 
% **Xie et al., "Decomposition for Enhancing Attention" 2024**,
% **Mao et al., "Enhancing Text-to-SQL pairing through Question Rewriting and Execution Guided Refinement" 2024**,
% **Wang et al., "DAC: Decomposed Automation Correction for Text-to-SQL" 2024**,
% **Cen et al., "SQLFixAgent: Toward Semantic-Accurate Text-to-SQL parsing via Consistency Enhanced Multi-Agent Collaboration" 2024**,
% **Xie et al., "MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL" 2024** 
% }

% {
% \color{red}
% The novelty in the framework?
% can we find similar framework in non text-to-sql task papers?
% what is the closest ones? what is different and why?
% Is it mixing many things in a single framework?
% question decomposition (or sql decomposition as rationale?)
% no verify and refine? the role of rationalization model?
% with what data rationalizaton model trained?
% }