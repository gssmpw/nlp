\section{Related Work}
% \subsection{Chain-of-Thought}
% Manual prompting~\cite{DBLP:conf/nips/Wei0SBIXCLZ22,DBLP:conf/iclr/FuPSCK23}, 
% Automatic prompting~\cite{DBLP:conf/nips/KojimaGRMI22,DBLP:journals/tmlr/ChenM0C23},
% Semi-automatic prompting~\cite{DBLP:conf/emnlp/ShumDZ23,DBLP:conf/iclr/Lu0CWZRCK23,DBLP:journals/corr/abs-2302-04813},
% Knowledge Distillation~\cite{DBLP:conf/nips/ZelikmanWMG22,DBLP:conf/emnlp/0001GHW00023,DBLP:conf/acl/MagisterMAMS23},



% {
% \color{red}
% basics  (related work chain of thought prompting, reasoning LLM)
% \begin{itemize}
%     \item CoT (Wei 2022) improves complex natural langauge reasoning tasks.
% Divides the problem into subproblems and solve and combine solution.
%     \item In earlier prompt engineering, such steps that hints LLM with the subproblem decomposition
% given by annotators with the rationale sometimes in the form of executable programs (PoT)
%     \item Since manual annotation is costly, 
% automatic ways of generating such steps or semi-automated ways are proposed.
% \end{itemize}

% advanced    (related work dynamic prompting and distillation)
% \begin{itemize}
%     \item On top of static prompt construction,
%     more advanced methods generates self-feedback during prompting (in-context learning)
%     and improve the results.[verify and refine]
%     \item But self-feed back cannot fix errors yet and \textbf{external signal} is needed (Huang 2024a)
%     %
%     \item The question decomposition strategy beyond showing static set of subproblem, answer, rationale
% is also proposed. least to most dynamic prompting.
%     \item Another source of improvement is "distillation"
% As opposed to the earlier examples soley rely on in-context learning,
% "distillation" transfer the reasoning capability to other model, especially from large to smaller models.
% \end{itemize}

% more references
% general advanced CoT methods for natural language reasoning
% \begin{itemize}
%     \item semi-auto prompt construction
% few shot by Auto-gen rationales
% Zhang 2023h
% Zou 2023
% Wan 2023
% Xu 2023
% A small number of human-annotation
% Shao 2023b FW BW
% Pitis 2023
% Shum 2023
% Lu 2023b
% Ye and Durrett 2023
%     \item verify and refinement
% Paul 2024a REFINER train critic model
% Maddan 2023 Self refine
% Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023g. Making language models better reasoners with step-aware verifier.
% Shridhar 2023 reasoning with revisions
% Gou 2024a external tool for correction
% Nathani 2023 MAF, multi-aspect feedback?    
%     \item question decomposition
% Zhou 2023b L2M subquestions		least to most
% Dua 2022 solution from prev
% Khot 2023 modular tasks?
% Huan 2024b DAG by QDMR – text to sql?	
% Zhang 2024 cumulative reasoning – logic    
%     \item distillation
% STAR
% \end{itemize}
% }
% {
% \color{blue}
% Summary of LLM reasoning papers by rationale generation and self-improvement/corrections 

% Keywords:  
% generating rationalization/explanation, 
% question decomposition,  
% verification and refinement,  
% Fine-tuning with self-generated rationales. 

 
% For solving reasoning tasks – GSM8K, adding two numbers (symbolic reasoning/scratch pad), common sense QA, multiple choice QA, etc, [Rajani, McCann, Socher 2019 Explain yourself] showed that manually curated explanations improve performance.  

% “Scratchpad” and “Chain-of-Thoughts" are another prompting style that improves the performance by providing rationale, which leads to wider adoption of the in-context learning approach. 
% The main research topics are generating rationales in automated or semi-automated (many papers) way and verifying and refining such steps of rationales that lead to the final answer. Those rationales generated by LLMs can be feedback to LLMs to improve the reasoning (STaR, LLM can self-improve).  
% In those successful cases, the ground-truth answer for a question is available and it is easy to verify the answer from LLMs. When the correctness of the self-generated intermediate rationales is not guaranteed, Huang 2024b showed that the self-correction through in-context learning with GPT-3.5 and GPT-4 fails, and it even decreases the performance. 
% }


% \subsection{Text2SQL}
% BIRD~\cite{DBLP:conf/nips/LiHQYLLWQGHZ0LC23} leaderboard systems~\cite{DBLP:journals/corr/abs-2411-08599}, \cite{DBLP:journals/corr/abs-2410-01943}, \cite{DBLP:journals/corr/abs-2408-07702}, \cite{DBLP:journals/corr/abs-2405-16755},
% \cite{DBLP:journals/corr/abs-2405-07467},
% \cite{DBLP:journals/corr/abs-2312-11242},
% \cite{DBLP:conf/iclr/LightmanKBEBLLS24}

% {
% \color{red}
% text to sql overview
% \begin{itemize}
%     \item leader board, methods, approaches
%     \item text to sql that exploits rationale. thoughts
% \end{itemize}

% \color{blue}
% In the case of text-to-SQL, what has been done? 
% Fine-tuning is a dominant approach.  
% As target queries get more complicated, standard prompting approaches saturate the performance. We need reasoning approaches that are successful in math, symbolic. common sense reasoning tasks. 
% Some of the existing methods that focus on in-context learning and improvement through rationalization, we see the following papers. 

% [PET-SQL][DIN-SQL][DAIL-SQL][MAC-SQL] -- Prompt, CoT? 
% [MCS-SQL] [CHESS][SEA-SQL] [Xiyan-SQL]-- multiple? 

% Question decomposition/ Refinement in SQL 
% [Xie et al 2024] Decomposition for enhancing attention 
% [Mao et al 2024] Enhancing Text-to-SQL paring through Question Rewriting and Execution Guided Refinement 
% [Wang et al 2024] DAC: Decomposed Automation Correction for Text-to-SQL 
% [Cen et al 2024] SQLFixAgent: Toward Semantic-Accurate Text-to-SQL parsing via Consistency Enhanced Multi-Agent Collaboration 
% [Xie et al 2024] MAG-SQL Multi-Agent Genreative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL 
% }

% {
% \color{red}
% The novelty in the framework?
% can we find similar framework in non text-to-sql task papers?
% what is the closest ones? what is different and why?
% Is it mixing many things in a single framework?
% question decomposition (or sql decomposition as rationale?)
% no verify and refine? the role of rationalization model?
% with what data rationalizaton model trained?
% }