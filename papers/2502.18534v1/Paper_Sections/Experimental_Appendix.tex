\label{Sec::Experiments_ext}

\subsection{Policy Action Analysis}
\label{sec::action_analysis_appendix}

In this section, we provide the complete action analysis results from Section~\ref{exp::action_analysis}. Specifically, we present action analyses for each MAFE when direct, rate-based, and fairness violation terms are weighted uniformly in the objective function.

For the Loan MAFE, we analyze the average admissions threshold set by the Admissions Agent, which determines the number of people approved for loans in an episode, and the debt management factor set by the Debt Management Agent, which helps the customer population avoid loan defaults. In the Healthcare MAFE, we examine how the Central Planner Agent allocates its budget across interventions and how the Insurance Agent sets premiums. For completeness, we restate the Education MAFE action analysis, focusing on how the Central Planner Agent distributes funds for interventions, how the Employer Agent sets salaries, and how the University Budget Allocation Agent allocates resources to improve student academic success.

\begin{figure*}[ht!]  
  \hspace{2.5cm}
  \begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\linewidth]{figures/Loan_admissions_threshold_overall.png}
    \caption{Admissions Agent} \label{fig:Admissions_Threshold_Action}
  \end{subfigure}%
  \begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\linewidth]{figures/Loan_debt_forgiveness_percent_overall.png}
    \caption{Debt Management Agent} \label{fig:Debt_Forgiveness_Action}
  \end{subfigure}%

  \hspace{2.5cm}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\linewidth]{figures/Healthcare_Ins_Action.png}
    \caption{Insurance Agent} 
    \label{fig:Healthcare_Ins_Action}
    \vspace{0.5cm}
  \end{subfigure}%
  \hspace{0.5cm}
  \begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\linewidth]{figures/Healthcare_planner_level1_allocation.png}
    \caption{Central Planner Agent \\ (General Intervention)} \label{fig:Healthcare_planner_level1_allocation}
  \end{subfigure}%
  
    \begin{subfigure}{0.31\textwidth}
    \includegraphics[width=\linewidth]{figures/Education_employer_allocation.png}
    \caption{Employer Agent} \label{fig:Education_employer_allocation}
    \vspace{0.34cm}
  \end{subfigure}%
  \hspace{0.25cm}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\linewidth]{figures/Education_planner_level1_allocation.png}
    \caption{Central Planner Agent \\ (General Intervention)} \label{fig:Education_planner_level1_allocation}
  \end{subfigure}%
  \hspace{0.25cm}
  \begin{subfigure}{0.31\textwidth}
    \includegraphics[width=\linewidth]{figures/Education_univ_budget_allocation_overall.png}
    \caption{University Budget Allocation Agent} \label{fig:Education_univ_budget_allocation}
    \vspace{0.4cm}
  \end{subfigure}%

\caption{Average actions taken by agents over training epochs in MAFEs for Loan (Row 1), Healthcare (Row 2), and Education (Row 3).} \label{fig:action_summary}
\vspace{-0mm}
\end{figure*}

For the Loan MAFE, Figure~\ref{fig:Admissions_Threshold_Action} shows the average admission threshold over 40 training epochs. As training progresses, the agent learns to lower the threshold, effectively admitting nearly all applicants. This strategy increases the admission rate among the global population, thereby improving the rate-based reward. However, admitting more applicants without additional safeguards can increase default rates, risking the bank's financial stability. To mitigate this issue, the Debt Management Agent can adjust the debt management factor to aid customers to avoid defaulting. As illustrated in Figure~\ref{fig:Debt_Forgiveness_Action}, this agent is able to strategically balance debt adjustment by setting these values neither too high to protect profits, nor too low to avoid widespread defaults. By targeting this aid, the agent ensures similar default rates across both groups, promoting fairness and financial stability.

Figure~\ref{fig:Healthcare_Ins_Action} and \ref{fig:Healthcare_planner_level1_allocation} present the actions taken by various agents within the Healthcare MAFE. Specifically, Figure~\ref{fig:Healthcare_Ins_Action} highlights the premium-setting behavior of the Insurance Agent. During training, the agent learns to set premiums near the upper limit of \$1000. While this might initially seem challenging for affordability, Figure~\ref{fig:Healthcare_planner_level1_allocation} illustrates a heatmap of the average percentage of the Central Planner Agent's budget allocated to healthcare subsidies. The planner prioritizes two main areas: (1) subsidizing insurance premiums to reduce the effective cost for individuals and (2) investing in public health initiatives. These premium subsidies help maintain affordability for consumers, even with the higher nominal premiums. The largest share of the planner’s budget is allocated to public health investments, aimed at reducing the overall burden on the healthcare system by preventing illness. This approach focuses on improving baseline health outcomes across the population, complementing reactive measures like treatment subsidies by emphasizing preventive care strategies.

Figure~\ref{fig:Education_employer_allocation}-\ref{fig:Education_univ_budget_allocation} illustrate agent actions in the Education MAFE. The Central Planner Agent primarily invests in tertiary resources and employer diversity incentives, as shown in Figure~\ref{fig:Education_planner_level1_allocation}, indicating that tuition revenue sufficiently covers university operations. The University Budget Allocation Agent demonstrates an evolving strategy, as shown in Figure~\ref{fig:Education_univ_budget_allocation}. Early in the training process, the agent focuses a significant portion of its budget on faculty salaries to ensure financial stability and avoid potential disruptions. Yet, since faculty salaries in this MAFE are fixed, the agent recognizes that allocating too large a portion of its resources for them may not be the most efficient use of funds. As the agent refines its strategy, it adjusts its budget distribution, directing more resources toward student-specific interventions, such as scholarships for both majority and underrepresented student groups, as well as mentorship programs for underrepresented groups. This shift in allocation helps address disparities in cumulative GPAs between majority and underrepresented students, ultimately improving educational and career outcomes.


\begin{figure*}[ht!]
  \begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\linewidth]{figures/Loan_Frontier.png}
    \caption{Loan Frontier} \label{fig:loan_front}
  \end{subfigure}%
  \hspace*{\fill}   % maximize separation between the subfigures
  \begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\linewidth]{figures/Healthcare_Frontier.png}
    \caption{Healthcare Frontier} \label{fig:health_front}
  \end{subfigure}%
  \begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\linewidth]{figures/Education_Frontier.png}
    \caption{Education Frontier} \label{fig:edu_frontier}
  \end{subfigure}%
\caption{Pareto frontiers that demonstrate the reward-fairness tradeoff for the F-MACEM in the (a) Loan, (b) Healthcare, and (c) Education MAFEs.} \label{fig:Frontiers}
\end{figure*}

Notably, Figure~\ref{fig:Education_employer_allocation} shows a significant trend reversal in the employer agent’s salary-setting behavior midway through the training process. Initially, the employer agent decreases average salaries; however, this trend inverts as training progresses, leading to a steady increase in salaries. This shift results from a combination of factors. First, the Central Planner Agent’s investment in diversity incentives directly boosts the salaries of underrepresented minority groups. Second, as the Central Planner and University Budget Allocation Agents optimize their investments in tertiary resources and university student aid, overall student performance improves. These enhancements in educational outcomes translate to better career success, indirectly driving higher salaries.

The coordinated actions among the different agents in each MAFE can create a positive feedback loop for improving various system rewards. Yet the reason this is possible is because the flexible intervention structure that our MAFEs offer. 

\subsection{Reward-Fairness Frontier in MAFEs}

In this section, we analyze the F-MACEM algorithm's performance in achieving fairness and accuracy, measured by the reward and fairness terms in Equation~\ref{eq::episode_success}. Particularly, each reward and fairness violation is weighted uniformly, with $\alpha_k=\frac{\lambda}{K}$ for rewards and $\beta_m=\frac{1-\lambda}{M}$ for fairness violations. We then train the system using uniformly sampled values of $\lambda$ over the interval $[0,1]$ to analyze the trade-off between fairness and accuracy. To ensure uniform contribution from each component, we normalize all rewards and fairness violations to lie within the range $[0,1]$. The normalization factors for these results are provided in Table~\ref{tab::normalization} of Appendix~\ref{sec::parameters}.

Figure~\ref{fig:Frontiers} presents the resulting Pareto frontiers, which illustrate the trade-off between accuracy and fairness. Each point on the frontier represents the average performance of a model trained with the same objective function across three different training seeds to represent relative fairness values.
Both fairness measures from Equation~\ref{eq::binary_measure} (for the Loan and Education environments) and Equation~\ref{eq::std_measure} (for the Healthcare environment) produce negative values, which are plotted directly since they are compatible with maximization. In the Loan and Education environments, fairness is assessed using a binary sensitive attribute, with a higher value indicating greater fairness. In contrast, the Healthcare environment evaluates fairness across four geographic regions, where a higher value also signifies greater fairness. In all plots, the highest fairness value corresponds to a value of 0.

These results indicate only a subtle trade-off between maximizing rewards and maintaining fairness, with the magnitude of this trade-off varying across different environments. Notably, the most significant performance declines occur when the weight assigned to the fairness term, $1-\lambda$, substantially exceeds that of the reward term, $\lambda$. However, F-MACEM generally maintains high reward levels when a moderate allowance for fairness violations is incorporated. This robustness suggests that even a small increase in the fairness weight within a reward-centric objective can have a meaningful impact. In particular, disparities can be mitigated over time through effective interventions, and such fairness regularization can, in some cases, improve rewards by helping F-MACEM avoid poor local minima.

\subsection{Assessing the Benefit of Multi-Agent Learning}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/Loan_Bar_Plot_use.png}
    \caption{Performance for the baseline fixed policy, single-agent learning (one agent learns dynamically), and multi-agent learning (all agents learn dynamically). Higher values indicate better performance.} 
    \label{fig::Bar_plot}
    \vspace{0mm}
\end{figure}

In this section, we perform an experiment to assess the benefits of allowing multiple agents to learn dynamic policies, using the Loan MAFE as a testbed. Specifically, we compare the performance of multi-agent learning, where all agents are allowed to learn optimal policies, against single-agent learning scenarios and a fixed policy baseline. The optimal policy, in this case, is defined as the one that maximizes the Loan MAFE’s objective function (as defined in equation~\ref{problem1_reg_coop}), with uniform weighting applied to all terms in the objective.

We begin by establishing a baseline with a fixed policy. In this scenario, the system consists of three agents: the Admissions and Debt Management Agents, each producing two actions—setting an admissions threshold and a debt management factor for each of the binary demographic groups—and the Disbursement Agent, which generates a scoring vector for the individuals in the loan queue. The fixed policy is generated by randomly sorting the individuals in the queue, which leads to equal average wait times across demographic groups.

Next, we identify the actions for the Admissions and Debt Management Agents through a two-tier grid search to optimize the objective function. In the first tier, we search for the best global pair of admissions threshold and debt management factor by partitioning the action space over the $[0,1]$ interval. Here, "global" means the same pair of values is applied to both demographic groups. In the second tier, we perform a grid search to determine how much to deviate the group-specific values from the global values, resulting in optimal values of $[0.0, 0.0]$ for admissions thresholds and $[0.12, 0.18]$ for debt management factors, where the first value corresponds to the advantaged group and the second to the disadvantaged group.

Once the baseline fixed policy is established, we conduct three forms of single-agent training sessions. In each, one of the agents is trained while the other two agents are fixed according to the baseline policy.

The results comparing the fixed policy, single-agent training, and multi-agent training are shown in Figure~\ref{fig::Bar_plot}. The plots display the resulting values of the objective function for each policy implementation, with higher values indicating better performance in maximizing the objective. Since the fixed policy was optimized to perform well according to the objective function, its performance is relatively high. However, allowing agents to learn, rather than relying on fixed or heuristic policies, leads to further improvements in agent performance. In particular, the multi-agent training scenario achieves the highest performance, demonstrating the utility of multi-agent learning in environments with multiple decision points. This underscores the value of considering multi-agent interactions, rather than simplifying the system to a single decision point with heuristic approaches.
