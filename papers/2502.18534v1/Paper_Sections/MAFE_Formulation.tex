\label{sec::fair_dec_pomp}

In this section, we formulate the notion of a MAFE by extending the standard \textit{decentralized partially observable Markov decision process (Dec-POMDP)}~\cite{gronauer2022multi} into an eight element tuple given by:  
\begin{equation}
    (\mathcal{N},\mathcal{S},\{\mathcal{A}_n\},\{\mathcal{O}_n\},\mathcal{T}, \gamma, \{c_n^{(R)}\},\{c_n^{(F)}\}).\notag
\end{equation}
In this tuple, $\mathcal{N}= \{1,...,N\}$ denotes the set of $N$ agents. $\mathcal{S}$ represents the set of unobserved global system states. $\mathcal{A}_n$ and $\mathcal{O}_n$, represent the action and observation spaces of Agent $n$, and $\boldsymbol{\mathcal{A}}=\cup_{n=1}^N\mathcal{A}_n$ and $\boldsymbol{\mathcal{O}}=\cup_{n=1}^N\mathcal{O}_n$ represent the joint action and observation spaces of all agents. Each agent's observation consists of a subset of the global state information, meaning that $\mathcal{O}_n \subseteq \mathcal{S}$.  $\mathcal{T}:\mathcal{S}\cup\boldsymbol{\mathcal{A}}\rightarrow \mathcal{S}$ is the state transition function which probabilistically updates the environment's state given its current state and the actions taken by all agents at the current time step. $\gamma$ is the discount factor. Figure~\ref{fig:MAFE_Diragram} provides an illustration of this MAFE framework. It alters the standard Dec-POMDP with two key components:

\textbf{Reward Component Functions:} We replace the standard agent reward functions with \textit{reward component functions} \(\{c_n^{(R)}\}\). Each reward component function, \(c_n^{(R)}: \mathcal{S} \cup \boldsymbol{\mathcal{A}} \rightarrow \mathbb{R}^{i_n}\), produces a vector of dimension $i_n$, which may vary across agents. These vectors are used to measure agents' rewards in the environment. For instance, the reward component function of a central planning agent could output the following vector to calculate population mortality rates:
\begin{equation}
    \left[\#\text{(Deaths)}_t, \ \ \# \text{(Ended Illness)}_t \right]^T. \notag
\end{equation}
\textbf{Fairness Component Functions:} We introduce \textit{fairness component functions} \(\{c_n^{(F)}\}\). Each fairness component function, \(c_n^{(F)}: \mathcal{S} \cup \boldsymbol{\mathcal{A}} \rightarrow \mathbb{R}^{j_n}\), produces a vector of dimension $j_n$, which may vary across agents. These vectors are used to evaluate the fairness of agents' actions in the environment. For example, to evaluate geographic disparities in moralities in Regions $A$ and $B$, a central planning agent's fairness component function might output the following vector:
\begin{equation}
    \resizebox{\linewidth}{!}{$\left[\# \right. \text{(Deaths)}^A_t, \# \text{(Deaths)}^B_t, \# \text{(Ended Illness)}^A_t, \# \text{(Ended Illness)}^B_t \left. \right]^T $\notag,}
\end{equation}
where mortality rates are calculated as the ratio of individuals who have passed away to the total number of previously ill individuals who have recovered or passed away in each region. Using component functions in our design provides the flexibility to calculate fairness and rewards using either step-wise or aggregation-based metrics, as described by~\cite{xuadapting}. See Appendix~\ref{sec::component_func_remark} for more details.

To illustrate the MAFE framework, consider a healthcare example (Figure~\ref{fig:MAFE_Diagram_Health_Ex}) involving three agents: an insurance company, a hospital, and a central planning agent. These agents make decisions such as setting premiums, allocating medical resources, and funding public health initiatives, impacting outcomes like population health, including the number of individuals who remain healthy, fall ill, or die.

A key strength of the MAFE framework is its flexibility in modeling multi-agent interactions. Reward and fairness components can be tailored to specific scenarios, allowing for the modeling of diverse agent relationships. For example, agents may share a common utility function in a cooperative setting or have distinct, even conflicting, goals in a competitive one. In a healthcare setting, the insurance company may prioritize cost minimization, the hospital may focus on improving patient outcomes, and the central planner may emphasize equity. MAFEs also allow users to define success metrics for each agent, such as patient recovery rates or equity in resource distribution, providing a more nuanced understanding of fairness in complex systems.

Another important feature of MAFEs is their support for heterogeneous agents, each with distinct observation and action spaces, reflecting the varying roles and information access of real-world entities. For example, the insurance agent may observe and offer premiums to the entire population, while the hospital only observes patients seeking treatment. This design captures disparities in information and decision-making capabilities across agents.

In Section~\ref{Fair_Sec}, we demonstrate how to use MAFEs through a cooperative use case, where we define specific reward and fairness metrics for success.
% , showing how to configure a MAFE to suit the researcher's goals. 
This use case illustrates how MAFEs can be tailored to various scenarios, serving as a powerful tool for studying fairness in multi-agent systems.