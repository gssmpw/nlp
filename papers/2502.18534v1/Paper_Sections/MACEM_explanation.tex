\label{sus_fair}

\begin{algorithm}[ht]
    \centering
    \caption{Fair Multi-Agent Cross Entropy Method (F-MACEM)}\label{algorithm}
    \begin{algorithmic}[1]
        \REPEAT
            \STATE Initialize buffers $\mathcal{R}$ and $\mathcal{P}$ and parameters $\boldsymbol\mu$ and $\boldsymbol\sigma^2$      
            \FOR{episode $= 1...$ number-of-episodes}
                \STATE Sample $\boldsymbol\theta = \{\theta_1, \dots, \theta_N\}$ from $\mathcal{N}(\boldsymbol\mu, \text{diag}(\boldsymbol\sigma^2))$
                \STATE Run episode, storing rewards and fairness components in $\mathcal{R}$ and $\boldsymbol\theta$ in $\mathcal{P}$ 
            \ENDFOR
            \STATE Update $\boldsymbol\mu$ and $\boldsymbol\sigma^2$ based on top $p\%$ of policies ranked by Equation~\ref{eq::episode_success}.
        \UNTIL{Convergence}
        \STATE \textbf{Return} $\boldsymbol\theta = \boldsymbol\mu$
    \end{algorithmic}
\end{algorithm}

In this section, we introduce the \textbf{Fair Multi-agent Cross Entropy Method (F-MACEM)}, a simple yet effective algorithm for optimizing the objective function in Problem~\ref{problem1_reg_coop}. The F-MACEM is an extension of the standard cross-entropy method (CEM), tailored to multi-agent systems with fairness considerations. This method is employed for performance analysis in Section~\ref{Sec::Experiments}.

The standard CEM is an evolutionary policy-based algorithm that optimizes a policy by sampling its parameters from a parametric distribution, such as a Gaussian. For each sample, the policy weights, $\theta$, are used to run a full episode, and the resulting rewards are observed. In each training epoch, multiple episodes are run with different policy weight samples. The top-performing policies, referred to as the elite set, are then used to update the distribution from which the policy weights are sampled. This process iterates until the average episodic rewards converge.

In the fully cooperative MARL setting, the standard CEM can be directly extended to handle multiple agents by updating the model weights for all $N$ agents, $\boldsymbol\theta=\{\theta_1,...,\theta_N\}$, simultaneously in each epoch. This update is based on the top-performing weight samples, which maximize episode rewards. These elite samples are then used to update the distribution from which $\boldsymbol{\theta}$ is drawn. An overview of the algorithm is provided in Algorithm~\ref{algorithm}.