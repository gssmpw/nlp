\section{Related Work}
\label{sec:related-work}

\paragraph{Training methods for human label variation}

First introduced by \citet{plank2022}, the term \textit{human label
  variation}~(HLV) captures the fact that disagreeements in annotations can be
well-founded and thus signal for data-driven methods. It challenges the
traditional notion in machine learning that an instance has a single ground
truth. Training methods that accommodate such variation had been proposed before
the term was coined~\citep[\textit{inter alia}]{sheng2008,peterson2019,uma2020}.
Newer methods have also been proposed in the
literature~\citep{deng2023,lee2023,chen2024,rodriguez-barroso2024} and in the
Learning with Disagreement shared task~\citep{leonardelli2023} which provides a
benchmark for HLV. While a systematic investigation of some of these methods
exists~\citep{uma2020}, it employs smaller pretrained language models and covers
only binary or multiclass classification tasks. In contrast, our work
additionally employs a large language model and covers multilabel tasks.
Furthermore, we also propose new soft evaluation metrics for HLV.

\paragraph{Evaluation of soft classification in remote sensing}

While evaluating in the HLV context is a relatively new concept in NLP,
evaluating against a fuzzy\footnote{The terms ``fuzzy'' and ``crisp'' are sometimes
  used in remote sensing literature to mean ``soft'' and ``hard'' in NLP
  literature.} reference is a well-studied area in remote sensing research.
Early work by \citet{foody1996} used a measure of distance that is equivalent to
twice the Jenson-Shannon divergence~\citep{lin1991}. \citet{binaghi1999} argued
that entropy-based measures are sensible only if the reference is crisp, and
proposed the use of fuzzy set theory to compute the soft version of standard
evaluation metrics such as accuracy. A similar approach was also proposed by
\citet{harju2023} for audio data. Key to this approach is the minimum function
used to compute the class membership scores given a fuzzy reference and a fuzzy
model output. Other proposed functions include product~\citep{lewis2001},
sum~\citep{pontius2006a}, and a composite of such functions designed to ensure
diagonality of the confusion matrix when the reference and the model output
match perfectly~\citep{pontius2006,silvan-cardenas2008}. Approaches that take
the cost of misclassifications into account have also been
proposed~\citep{gomez2008}. In this work, we take the fuzzy set approach by
\citet{binaghi1999} and apply it to text classification tasks.