\documentclass[shortpaper]{clv2025}

\jvol{vv}
\jnum{nn}
\jyear{2025}
%There is no need for the authors to change the above.

\dochead{Short Paper}
% Possible options: Long Paper, Short Paper, Survey, Survey Proposal, Position Paper, Last Words, Book Review, Dissertation Award, Lifetime Achievement Award, Squibs and Discussions, Featured Article

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{cleveref}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{dsfont}
\usepackage{siunitx}
\usepackage{pifont}

\newcommand{\pojsd}{PO-JSD\xspace}
\newcommand{\metric}[1]{\textbf{#1}\xspace}
\newcommand{\eqdef}{\overset{\mathrm{def}}{=}}  % equal sign meaning 'is defined as'
\newcommand{\methname}[1]{\textbf{#1}}  % format method name e.g. ReL
% https://tex.stackexchange.com/a/42620
\newcommand{\yes}{\textcolor{green}{\ding{51}}\xspace}
\newcommand{\no}{\textcolor{red}{\ding{55}}\xspace}

\DeclareMathOperator{\mgt}{>\!\!>}
\DeclareMathOperator{\JSD}{JSD}
\DeclareMathOperator{\KL}{KL}

\runningtitle{Training and Evaluating with Human Label Variation}
\runningauthor{Kurniawan, Mistica, Baldwin, Lau}

\begin{document}

\title{Training and Evaluating with Human Label Variation: An Empirical Study}

\author{Kemal Kurniawan\thanks{Corresponding author}$^{,1}$, Meladel
  Mistica$^{1}$, Timothy Baldwin$^{2,1}$, Jey Han Lau$^{1}$}

\affilblock{
    \affil{School of Computing and Information Systems, University of Melbourne\\\quad \email{kurniawan.k@unimelb.edu.au}}
    \affil{Mohamed bin Zayed University of Artificial Intelligence}
}

\maketitle

\begin{abstract}
  Human label variation~(HLV) challenges the standard assumption that
  a labelled instance has a single ground truth, instead embracing the natural
  variation in human annotation to train and evaluate models. While
  various training methods and metrics for HLV have been proposed,
  it is still unclear which methods and metrics perform best in what settings.
  We propose new evaluation metrics for HLV leveraging fuzzy set theory.
  Since these new proposed metrics are differentiable, we then in turn
  experiment with employing these metrics as training objectives. We conduct an
  extensive study over 6 HLV datasets testing 14 training methods and 6
  evaluation metrics. We find that training on either disaggregated annotations
  or soft labels performs best across metrics, outperforming training using the
  proposed training objectives with differentiable metrics. We also show that
  our proposed soft metric is more interpretable and correlates best with human
  preference.\footnote{Code is available at:
    \url{https://github.com/kmkurn/train-eval-hlv}}
\end{abstract}

\section{Introduction}

Human label variation~(HLV) challenges the standard assumption that an
example has a single ground truth~\citep{plank2022}. Without this
assumption, model training and evaluation are no longer straightforward. For
example, standard metrics such as accuracy assume a single ground truth label
exists for each instance. To address this issue, recent
work~\citep[\textit{inter
  alia}]{pavlick2019,peterson2019,nie2020,uma2020,uma2021,cui2023,gajewska2023,maity2023,wan2023}
has proposed various methods to train models and metrics to evaluate their
performance in the HLV context. However, the lack of a systematic study means it
is still unclear which methods and metrics perform best in what settings.

In natural language processing (NLP), most existing metrics for HLV represent human judgements as probability
distributions over classes and employ information-theoretic measures such as
divergence. It has largely ignored the field of remote sensing that has dealt
with issues in model evaluation against fuzzy ground
truths~\citep{foody1996,binaghi1999,lewis2001,pontius2006,pontius2006a,silvan-cardenas2008,gomez2008}.
In remote sensing, the task is to classify patches of a satellite image of
a region into categories such as land, water, and so forth. As an image
patch often contains multiple categories, the field has developed evaluation
metrics that can handle this variation in the ground truth labels.
Taking inspiration from remote sensing research, we represent human
judgement distributions as degrees of membership over fuzzy sets and generalise
standard metrics such as accuracy into their soft versions using fuzzy set
operations. To the best of our knowledge, we are the first to propose employing soft
metrics for HLV in NLP.

Because these soft metrics are differentiable, we next explore the use of these soft metrics as training
objectives. Through an extensive study testing 14 training methods and 6
evaluation metrics on 6 HLV datasets, we show that training on either
disaggregated annotations or soft labels often achieves the best
performance across metrics. We also conduct a meta-evaluation on the evaluation metrics.
Albeit modest, we find that our proposed soft
metric correlates best with human preference. However, this
suggests that there is still substantial future work to develop better evaluation metrics for HLV.


To summarise, we: (a)~propose soft metrics to measure model performance in the context
of HLV, taking inspiration from remote sensing research; (b)~propose new
training methods based on the soft evaluation metrics; and (c)~conduct an
extensive study covering 6 HLV datasets, 14 training methods and 6 evaluation
metrics to understand the best training methods and evaluation metrics.

\section{Related Work}\label{sec:related-work}

\paragraph{Training methods for human label variation}

First introduced by \citet{plank2022}, the term \textit{human label
  variation}~(HLV) captures the fact that disagreeements in annotations can be
well-founded and thus signal for data-driven methods. It challenges the
traditional notion in machine learning that an instance has a single ground
truth. Training methods that accommodate such variation had been proposed before
the term was coined~\citep[\textit{inter alia}]{sheng2008,peterson2019,uma2020}.
Newer methods have also been proposed in the
literature~\citep{deng2023,lee2023,chen2024,rodriguez-barroso2024} and in the
Learning with Disagreement shared task~\citep{leonardelli2023} which provides a
benchmark for HLV. While a systematic investigation of some of these methods
exists~\citep{uma2020}, it employs smaller pretrained language models and covers
only binary or multiclass classification tasks. In contrast, our work
additionally employs a large language model and covers multilabel tasks.
Furthermore, we also propose new soft evaluation metrics for HLV.

\paragraph{Evaluation of soft classification in remote sensing}

While evaluating in the HLV context is a relatively new concept in NLP,
evaluating against a fuzzy\footnote{The terms ``fuzzy'' and ``crisp'' are sometimes
  used in remote sensing literature to mean ``soft'' and ``hard'' in NLP
  literature.} reference is a well-studied area in remote sensing research.
Early work by \citet{foody1996} used a measure of distance that is equivalent to
twice the Jenson-Shannon divergence~\citep{lin1991}. \citet{binaghi1999} argued
that entropy-based measures are sensible only if the reference is crisp, and
proposed the use of fuzzy set theory to compute the soft version of standard
evaluation metrics such as accuracy. A similar approach was also proposed by
\citet{harju2023} for audio data. Key to this approach is the minimum function
used to compute the class membership scores given a fuzzy reference and a fuzzy
model output. Other proposed functions include product~\citep{lewis2001},
sum~\citep{pontius2006a}, and a composite of such functions designed to ensure
diagonality of the confusion matrix when the reference and the model output
match perfectly~\citep{pontius2006,silvan-cardenas2008}. Approaches that take
the cost of misclassifications into account have also been
proposed~\citep{gomez2008}. In this work, we take the fuzzy set approach by
\citet{binaghi1999} and apply it to text classification tasks.

\section{Evaluation Metrics for HLV}\label{sec:eval-metrics}


We now discuss the evaluation metrics relevant for HLV data, first on
multiclass metrics (\Cref{sec:metrics}), then multilabel metrics
(\Cref{sec:mullab-metrics}), and finally the relationship between these metrics
(\Cref{sec:rel-overall-metrics}). In these discussions, we will also introduce our use of
soft metrics and explain how they relate to their hard variant counterparts.
\Cref{tbl:metrics} provides a summary of the existing and proposed/new
evaluation metrics.

\paragraph{Notation}

Let $P_{ik}\geq 0$ denote the true human judgement for example $i$ and class
$k$, i.e.\ proportion of humans labelling example $i$ with class $k$.
Let $Q_{ik}\geq 0$ denote the value of $P_{ik}$
predicted by a model.
An evaluation metric $m$ is a function such that the scalar
$m(\mathbf{P},\mathbf{Q})$ measures how well~(i.e., a positive orientation)
$\mathbf{Q}$ captures the judgement distribution in $\mathbf{P}$.
To complete our
notation, let $N$ and $K$ denote the number of examples and classes
respectively.

\begin{table}
  \caption{HLV metrics for classification tasks.}\label{tbl:metrics}
  \begin{tabular}{@{}lll@{}}
    \toprule
                               & Multiclass                    & Multilabel                     \\
    \midrule
    \multirow{4}{*}{Existing:} & Accuracy                      & Micro F\textsubscript{1}       \\
                               & Macro F\textsubscript{1}      & Macro F\textsubscript{1}       \\
                               & \pojsd                        &                                \\
                               & Entropy correlation           &                                \\
    \cmidrule{1-3}
    \multirow{4}{*}{Proposed:} & Soft accuracy                 & Soft micro F\textsubscript{1}  \\
                               & Soft macro F\textsubscript{1} & Soft macro F\textsubscript{1}  \\
                               &                               & Multilabel \pojsd              \\
                               &                               & Multilabel entropy correlation \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Multiclass Metrics}\label{sec:metrics}

In multiclass classification tasks, an example can only be assigned to one
class. This is reflected by the unity constraints $\sum_{k}P_{ik}=1$ and
$\sum_{k}Q_{ik}=1$ for all $i$.  We study a total of 6 multiclass evaluation
metrics $m$. The first two are standard hard metrics, the next two are their
soft versions that we propose inspired by prior
work~\citep{binaghi1999,harju2023}, and the last two are existing HLV metrics.

Let $I^P_{ik}$ and $I^Q_{ik}$ denote $\mathds{1}(\arg\max_{l}P_{il}=k)$ and
$\mathds{1}(\arg\max_{l}Q_{il}=k)$ respectively. The 6 metric definitions under our
notation are as follows:
\begin{enumerate}
  \item \metric{(Hard)~accuracy}, where:
    \begin{equation*}
      m(\mathbf{P},\mathbf{Q})\eqdef\frac{1}{N}\sum_{ik}I^P_{ik}I^Q_{ik}.
    \end{equation*}
    This is the standard evaluation metric for classification tasks.
  \item \metric{(Hard)~macro F\textsubscript{1}}, where
    \begin{equation*}
      m(\mathbf{P},\mathbf{Q})\eqdef\frac{1}{K}\sum_{k}\frac{2\sum_{i}I^P_{ik}I^Q_{ik}}{\sum_{i}\left(I^P_{ik}+I^Q_{ik}\right)}.
    \end{equation*}
    This metric addresses the issue with accuracy
    that is biased toward the majority class when class proportions are
    imbalanced.
  \item \metric{Soft accuracy}, where:
    \begin{equation*}
      m(\mathbf{P},\mathbf{Q})\eqdef\frac{1}{N}\sum_{ik}\min(P_{ik},Q_{ik}).
    \end{equation*}
    This metric is novel to this work, for evaluation in the context of HLV.
    This is a special case of the soft micro F\textsubscript{1} score proposed
    later in \Cref{sec:mullab-metrics}.
  \item \metric{Soft macro F\textsubscript{1}}, where:
    \begin{equation*}
      m(\mathbf{P},\mathbf{Q})\eqdef\frac{1}{K}\sum_{k}\frac{2\sum_{i}\min(P_{ik},Q_{ik})}{\sum_{i}(P_{ik}+Q_{ik})}.
    \end{equation*}
    This metric is also novel to this work, to address the issue of class imbalance with
    soft accuracy and other existing HLV metrics.\footnote{In the
      context of HLV, we understand the concept of class imbalance
      analogously to that in the context where class assignments are
      hard: there exists a class $k$ such that
      $\sum_{i}P_{ik}\mgt\sum_{i}P_{il}$ for all $l\neq k$.
    } Note that this metric implies the existence of soft
    \emph{class-wise} metrics~(see Appendix).
  \item \metric{Jensen-Shannon divergence}~\citep{uma2021}, which we modify into its
    positively oriented version~(\metric{\pojsd} for short) where:
    \begin{equation*}
      m(\mathbf{P},\mathbf{Q})\eqdef1-\frac{1}{N}\sum_{i}\JSD(\mathbf{p}_i,\mathbf{q}_i).
    \end{equation*}
    Vectors $\mathbf{p}_i,\mathbf{q}_i$ denote the $i$-th row of
    $\mathbf{P},\mathbf{Q}$ respectively. The scalar
    $\JSD(\mathbf{a},\mathbf{b})=\frac{1}{2}\left(\KL(\mathbf{a},\frac{1}{2}\left(\mathbf{a}+\mathbf{b}\right))+\KL(\mathbf{b},\frac{1}{2}\left(\mathbf{a}+\mathbf{b}\right))\right)$
    is the Jensen-Shannon divergence~\citep{lin1991}, where
    $\KL(\mathbf{a},\mathbf{b})=\sum_{k}a_k\log_2\frac{a_k}{b_k}$ is the
    Kullback-Leibler divergence.\footnote{Jensen-Shannon divergence of two
      distributions has an upper bound of $\log_b2$ if the logarithms used in
      $\KL(\cdot)$ are of base $b$. Normalising this bound to 1 results in
      logarithms of base 2 as $\log_b x/\log_b2=\log_2x$.}
  \item \metric{Entropy correlation}~\citep{uma2020}, where:
    \begin{align*}
      m(\mathbf{P},\mathbf{Q})&\eqdef\frac{\sum_i\zeta^P_i\zeta^Q_i}{\sqrt{\sum_i{\zeta^P_i}^2}\sqrt{\sum_i{\zeta^Q_i}^2}},\\
      \zeta^P_i&=\eta^P_i-\bar{\eta}^P,\text{ and }\\
      \zeta^Q_i&=\eta^Q_i-\bar{\eta}^Q.
    \end{align*}
    Scalar $\eta^P_i=-\sum_k\frac{P_{ik}\log P_{ik}}{\log K}$ is the normalised
    entropy of row $i$ of $\mathbf{P}$, and
    $\bar{\eta}^P=\frac{1}{N}\sum_{i}\eta^P_i$ is the mean of $\lbrace
    \eta^P_i\rbrace_{i=1}^N$. Scalars $\eta^Q_i$ and $\bar{\eta}^Q$ are defined
    analogously.
\end{enumerate}

By noting that $N=\sum_{ik}P_{ik}$, soft accuracy can be interpreted as the
proportion of the judgement distribution that is correctly predicted. This interpretation
is similar to the hard counterpart~(i.e.\ proportion of examples that are
correctly predicted) and arguably more intuitive than that of existing metrics
because it is stated directly in terms of the problem of predicting human
judgement distribution. Other similarities include symmetry~(due to the
symmetry of $\min$) and boundedness~(between 0 and 1), where the lower and the
upper bounds are achieved if and only if $P_{ik}Q_{ik}=0$ and $P_{ik}=Q_{ik}$
respectively for all $i,k$. Note also that \emph{soft accuracy is reduced to the
  standard hard accuracy} when all rows of both $\mathbf{P}$ and $\mathbf{Q}$
are one-hot vectors~(i.e.\ no label variation).

While functions other than $\min$ have been used to compute soft accuracy~(see
\Cref{sec:related-work}), they lead to the soft accuracy being $<1$ in the
case where $\mathbf{P}=\mathbf{Q}$. This is problematic because intuitively,
good HLV evaluation metrics should produce the highest score when the predicted
judgement distribution perfectly matches the true counterpart. Therefore, we
use the $\min$ function in this work.

Other evaluation metrics we considered included entropy similarity~\citep{uma2021},
(negative)~cross-entropy~\citep{peterson2019,pavlick2019},
(negative)~Kullback-Leibler divergence~\citep{nie2020}, (the positively oriented
version of)~information closeness~\citep{foody1996} and (the positively oriented
version of)~Jensen-Shannon distance~\citep{nie2020}. However, we ultimately excluded them
from this study because of their expected high correlation with either \pojsd
or entropy correlation.

\subsection{Multilabel Metrics}\label{sec:mullab-metrics}

Next, we consider the case where the unity constraints do not hold,
i.e.\ an example can be (soft)~assigned to multiple classes, forming a
\emph{multilabel} classification task.

By considering this case as multiple independent binary classification tasks,
some existing metrics can be extended straightforwardly by taking the average of
the metric values over classes. For example, \pojsd can be modified into the
multilabel version:
\begin{equation*}
  m(\mathbf{P},\mathbf{Q})\eqdef1-\frac{1}{NK}\sum_{ik}\JSD(P^{\text{bin}}_{ik},Q^{\text{bin}}_{ik})
\end{equation*}
where $P^{\text{bin}}_{ik}=[P_{ik},1-P_{ik}]$ is a judgement distribution over
two classes, and $Q^{\text{bin}}_{ik}$ is defined analogously.
Entropy correlation can also be modified similarly into the multilabel version:
\begin{equation*}
  m(\mathbf{P},\mathbf{Q})\eqdef\frac{1}{K}\sum_{k}\frac{\sum_i\zeta^P_{ik}\zeta^Q_{ik}}{\sqrt{\sum_i{\zeta^P_{ik}}^2}\sqrt{\sum_i{\zeta^Q_{ik}}^2}}
\end{equation*}
where $\zeta^P_{ik}=\eta^P_{ik}-\bar{\eta}^P_k$,
$\eta^P_{ik}=-\frac{1}{\log 2}\left( P_{ik}\log P_{ik}+(1-P_{ik})\log(1-P_{ik})
\right)$, $\bar{\eta}^P_k=\frac{1}{N}\sum_{i}\eta^P_{ik}$, and scalars $\zeta^Q_{ik}$,
$\eta^Q_{ik}$, and $\bar{\eta}^Q_k$ are defined analogously.

While it is possible to define (hard)~accuracy for this multilabel case, a more
common evaluation metric is the micro F\textsubscript{1} score, which in our
notation can be expressed as:
\begin{equation*}
  m(\mathbf{P},\mathbf{Q})\eqdef2\frac{\sum_{ik}J^P_{ik}J^Q_{ik}}{\sum_{ik}\left(J^P_{ik}+J^Q_{ik} \right)}
\end{equation*}
where $J^P_{ik}=\mathds{1}(P_{ik}>0.5)$, and $J^Q_{ik}$ is defined analogously.
Similarly, we define the soft version of the micro F\textsubscript{1} score as
follows~(see derivations in the Appendix):
\begin{equation*}
  m(\mathbf{P},\mathbf{Q})\eqdef2\frac{\sum_{ik}\min(P_{ik},Q_{ik})}{\sum_{ik}(P_{ik}+Q_{ik})}.
\end{equation*}
The (hard)~micro
F\textsubscript{1} score is a special case of this soft counterpart when
$P_{ik}\in\{0,1\}$ and $Q_{ik}\in\{0,1\}$~(i.e. no label variation).
Furthermore, the soft accuracy in \Cref{sec:metrics} is a special case of this
soft micro F\textsubscript{1} score when the unity constraints hold. Therefore,
\emph{the soft micro F\textsubscript{1} score is a general evaluation metric for
  classification tasks}.

\subsection{Relationship between Metrics}\label{sec:rel-overall-metrics}

To analyse the relationship between metrics, we compute Pearson correlation
between each pair of multiclass metrics~(excluding the macro ones) similar to
prior work~\citep{chicco2020}. For a given number classes, we sample $B$
matrices $\mathbf{P}$ and $\mathbf{Q}$ and compute the value of each metric on
these $B$ pairs of $\mathbf{P}$ and $\mathbf{Q}$. The Pearson correlations
between pairs of metrics are then computed based on these $B$ data points. We
set $B=500$ for computational reasons. We report the correlation coefficients in
\Cref{tbl:pairwise-corr}.

\Cref{tbl:pairwise-corr} shows that the metrics are generally weakly correlated
with each other, with the exception of soft accuracy and \pojsd where the two
are consistently strongly correlated~($r>0.9$). This strong correlation with an
established HLV metric gives an empirical assurance that our proposed soft
accuracy is a sensible metric. The table also shows that soft accuracy is only
moderately correlated with (hard)~accuracy in most cases, where this correlation
gets weaker when the number of classes is large~($K=100$), or either
$\mathbf{P}$ or $\mathbf{Q}$ is dense~($\alpha=10$ or $\beta=10$). This finding
suggests that hard and soft accuracy capture different aspects when humans
disagree. Furthermore, the table shows that entropy correlation is poorly
correlated with all metrics, which suggests that it is an outlier HLV metric.

\paragraph{Soft Accuracy and \pojsd}

\begin{table}
  \caption{Pearson correlations between a pair of evaluation metrics
    $m(\mathbf{P},\mathbf{Q})$ for 1K examples and various number of
    classes~($K$) where the rows of $\mathbf{P}$ and $\mathbf{Q}$ are drawn from
    a symmetric Dirichlet with parameters $\alpha$ and $\beta$ respectively. A,
    J, E, and S denote the accuracy, \pojsd, entropy correlation, and soft
    accuracy respectively.}\label{tbl:pairwise-corr}
  \sisetup{table-auto-round, print-zero-integer=false}
  \begin{tabular}{@{}r*{6}{S[table-format=1.2]}@{}}
    \toprule
    $K$ & {A-J}     & {A-E}     & {A-S}     & {J-E}     & {J-S}    & {E-S}     \\
    \midrule
    \multicolumn{7}{c}{$\alpha=\beta=10$}                                             \\
    \addlinespace
    10  & 0.272347  & 0.050255  & 0.289984  & 0.035927  & 0.942263 & 0.100303  \\
    100 & 0.047588  & 0.040964  & 0.051956  & 0.091122  & 0.924129 & 0.087560  \\
    \cmidrule{1-7}
    \multicolumn{7}{c}{$\alpha=\beta=0.1$}                                            \\
    \addlinespace
    10  & 0.627773  & 0.063820  & 0.733743  & 0.126164  & 0.963214 & 0.162816  \\
    100 & 0.194069  & 0.093113  & 0.251355  & 0.014869  & 0.961086 & 0.032225  \\
    \cmidrule{1-7}
    \multicolumn{7}{c}{$\alpha=10,\beta=0.1$}                                         \\
    \addlinespace
    10  & 0.226175  & -0.015882 & 0.206464  & -0.018527 & 0.975144 & -0.002286 \\
    100 & -0.004257 & -0.062204 & -0.026900 & 0.050080  & 0.957816 & 0.061640  \\
    \cmidrule{1-7}
    \multicolumn{7}{c}{$\alpha=0.1,\beta=10$}                                         \\
    \addlinespace
    10  & 0.211874  & 0.016467  & 0.181706  & -0.016632 & 0.975830 & -0.019553 \\
    100 & 0.103478  & -0.021530 & 0.057268  & -0.005990 & 0.967420 & 0.005447  \\
  \bottomrule
  \end{tabular}
\end{table}

Despite their strong positive correlation, soft accuracy and \pojsd have a
notable difference: soft accuracy is upper-bounded by \pojsd.
\begin{theorem}
  Consider the same definitions of $P_{ik}$ and $Q_{ik}$ used in \Cref{sec:eval-metrics}.
  The soft accuracy and the \pojsd between $\mathbf{P}$ and $\mathbf{Q}$
  satisfy the following inequality:
  \begin{equation*}
    \frac{1}{N}\sum_{ik}\min(P_{ik},Q_{ik})\leq1-\frac{1}{N}\sum_{i}\JSD(\mathbf{p}_i,\mathbf{q}_i)
  \end{equation*}
  where $\JSD(\mathbf{p}_i,\mathbf{q}_i)$ denote the Jensen-Shannon
  divergence between the $i$-th rows of $\mathbf{P}$ and $\mathbf{Q}$.
\end{theorem}

\begin{proof}
  Given a scalar $0\leq\pi\leq1$ and discrete distributions
  $\mathbf{a},\mathbf{b}$, it has been shown that the following bound
  holds~\citep[Theorem 4]{lin1991}:
  \begin{equation*}
    \sum_k\min(\pi a_k,(1-\pi)b_k)
    \leq\frac{1}{2}\left(H([\pi,1-\pi])-\JSD_\pi(\mathbf{a},\mathbf{b})\right)
  \end{equation*}
  where $H(\mathbf{u})$ is the entropy of $\mathbf{u}$, and
  $\JSD_\pi(\mathbf{a},\mathbf{b})=H(\pi\mathbf{a}+(1-\pi)\mathbf{b})-\pi
  H(\mathbf{a})-(1-\pi)H(\mathbf{b})$ is the general form of Jensen-Shannon
  divergence with two distributions. It can be easily shown that
  $\JSD_\pi$ is reduced to the standard Jensen-Shannon divergence when
  $\pi=\frac{1}{2}$. Performing this substitution and noting that
  $H([\frac{1}{2},\frac{1}{2}])=1$,\footnote{All logarithms are of base 2.} we
  have
  \begin{equation*}
    \sum_k\min(\frac{1}{2}P_{ik},\frac{1}{2}Q_{ik})\leq\frac{1}{2}\left(1-\JSD(\mathbf{p}_i,\mathbf{q}_i)\right)
  \end{equation*}
  for all $i$. Doubling both sides and taking the average of this inequality
  over $i$ give the desired result.
\end{proof}
This fact implies that soft accuracy penalises a model more heavily than \pojsd,
as illustrated in \Cref{fig:sa-pojsd}. It shows that a skewed predicted
judgement distribution of $(0.2,0.8)$ has a soft accuracy of \num{0.7}, much
less than the \pojsd~($> 0.9$). The high \pojsd is potentially misleading as it
is very close to the maximum possible value of 1.

\section{Training Methods for HLV}

\begin{figure}
  \centering
  \includegraphics[width=0.7\linewidth]{sa-pojsd}
  \caption{Soft accuracy and \pojsd graphs for a binary classification problem
    on a single example where the true and the predicted judgement for the
    positive class is 0.5 and $q$ respectively.}\label{fig:sa-pojsd}
\end{figure}

\begin{table}
  \caption{Overview of HLV training methods used, along with details of whether
    annotator identity is required~(A), multilabel tasks are supported~(M), and
    training data is enlarged~(L) which makes training run much
    longer.}\label{tbl:train-meths}
  \begin{tabular}{@{}lccc@{}}
    \toprule
    Name                                                           & A    & M    & L    \\
    \midrule
    Repeated labelling~(\methname{ReL})                            & \no  & \yes & \yes \\
    Majority voting~(\methname{MV})                                & \no  & \yes & \no  \\
    Annotator ranking~(\methname{AR})                              & \yes & \yes & \no  \\
    Annotator ranking~(hard)~(\methname{ARh})                      & \yes & \yes & \no  \\
    Ambiguous labelling~(\methname{AL})                            & \no  & \no  & \yes \\
    Soft labelling~(\methname{SL})                                 & \no  & \yes & \no  \\
    Multitask of \methname{SL} and \methname{MV}~(\methname{SLMV}) & \no  & \yes & \no  \\
    Annotator ensemble~(\methname{AE})                             & \yes & \yes & \no  \\
    Annotator ensemble~(hard)~(\methname{AEh})                     & \yes & \yes & \no  \\
    \cmidrule{1-4}
    \multicolumn{4}{c}{\textit{Below are proposed in this work}}                        \\
    \cmidrule{1-4}
    Jensen-Shannon divergence~(\methname{JSD})                     & \no  & \yes & \no  \\
    Soft micro F\textsubscript{1}~(\methname{SmF1})                & \no  & \yes & \no  \\
    Soft macro F\textsubscript{1}~(\methname{SMF1})                & \no  & \yes & \no  \\
    Loss aggregation with $\min$~(\methname{LA-min})               & \no  & \yes & \no  \\
    Loss aggregation with $\max$~(\methname{LA-max})               & \no  & \yes & \no  \\
    \bottomrule
  \end{tabular}
\end{table}

We now discuss the training methods. In total,
we experiment with 14 HLV training methods, as summarised in
\Cref{tbl:train-meths}. The first method~(\methname{ReL}) is the simplest and
uses standard cross-entropy loss as the training objective based on prior
work~\citep{sheng2008}. The next 8
methods~(\methname{MV}, \methname{AR}, \methname{ARh}, \methname{AL},
\methname{SL}, \methname{SLMV}, \methname{AE}, \methname{AEh}) are from the
Learning with Disagreements shared task~\citep{leonardelli2023} which also use
standard cross-entropy loss. The next 3 methods~(\methname{JSD}, \methname{SmF1},
\methname{SMF1}) use soft metrics that are differentiable as the training
objective. The last 2 methods~(\methname{LA-min}, \methname{LA-max}) aggregate
the cross-entropy losses of all annotations for a given input. In detail:
\begin{enumerate}
  \item \methname{ReL}~(repeated labelling) trains on the disaggregated labels
    directly~\citep{sheng2008}. For example, if the training data only has a
    single instance $x$ with annotations $y_1$ and $y_2$ then \methname{ReL}
    constructs new training data $\{(x,y_1),(x,y_2)\}$.
  \item \methname{MV} trains on the majority-voted labels. This is the baseline
    model given by the shared task organisers.
  \item \methname{AR}~(annotator ranking) and \methname{ARh}~(annotator ranking
    hard) weight each training instance equal to the sum of ranking scores of
    its annotators~\citep{cui2023}. Suppose that training instance $x_1$ is
    labelled as A, A, B, and A by 4 annotators; and $x_2$ is labelled as A, B,
    and B by only the first 3 annotators. Then, to compute the ranking score of
    an annotator:
    \begin{enumerate}
      \item \methname{AR} computes the average judgement of the majority-voted
        label given by the annotator. The annotator ranking scores are~(in
        order) $\frac{3}{4}$, $\frac{1}{2}\left(\frac{3}{4}+\frac{2}{3}
        \right)$, $\frac{2}{3}$, and $\frac{3}{4}$.
      \item \methname{ARh} computes the average number of times the annotator
        agrees with the majority. The scores are $\frac{1}{2}$, $\frac{2}{2}$,
        $\frac{1}{2}$, and $\frac{1}{1}$.
    \end{enumerate}
    For both methods, the ranking score is zero if the annotator never agrees
    with the majority. Given an instance, the training objective is maximising
    the probability of its majority-voted label(s), scaled by its weight as
    defined above.
  \item \methname{AL}~(ambiguous labelling) labels \emph{ambiguous} instances,
    i.e. instances where annotators are not unanimous in their judgement, with
    all selected classes exactly once~\citep{gajewska2023}. Each instance-label
    pair is included in the training set. For example, if training instance
    $x_1$ is labelled as $y_1$, $y_1$, and $y_2$ by 3 annotators, and $x_2$ is
    labelled as $y_2$, $y_2$, and $y_2$, then \methname{AL} produces
    $\lbrace(x_1,y_1),(x_1,y_2),(x_2,y_2)\rbrace$ as the training data.
    Because it was developed for single-label tasks, we exclude this method from
    multilabel tasks.\footnote{Extending this method to multilabel tasks is
      beyond the scope of this work.}
  \item \methname{SL}~(soft labelling) trains on soft labels as
    targets~\citep{maity2023,wan2023}. For example, if an instance is labelled
    as positive by 80\% of annotators then the log-likelihood is $0.8\log
    p(1)+0.2\log p(0)$, where $p(1)$ and $p(0)$ denote the predicted probability
    of the positive and the negative class respectively.
  \item \methname{SLMV} performs multi-task learning using both the
    soft~(\methname{SL}) and the majority-voted~(\methname{MV}) labels as
    targets~\citep{grotzinger2023}. Two models with a shared encoder are trained
    to predict the two types of labels respectively, which is similar to the
    method of \citet{fornaciari2021}. For example, if an instance is labelled as
    positive by 80\% of annotators then the multitask log-likelihood is $0.8\log
    p_{\theta_1}(1)+0.2\log p_{\theta_1}(0)+\log p_{\theta_2}(1)$, where
    $\theta_1$ and $\theta_2$ denote the parameters of the first and the second
    model respectively. We predict only the soft labels for evaluation.
  \item \methname{AE}~(annotator ensembling) and \methname{AEh}~(annotator
    ensembling hard) model each annotator separately followed by
    ensembling~\citep{sullivan2023,vitsakis2023}. In this approach, we have as
    many models as there are annotators. Each model is trained on the labels
    given by a distinct annotator. In practice, we share the encoder parameters
    across models. Suppose that there are 3 annotators, and the
    predicted judgements of the positive class for an instance are 60\%, 30\%,
    and 90\%. To ensemble these predictions at test time:
    \begin{enumerate}
      \item \methname{AE} computes a simple mean~\citep{sullivan2023}. The final
        predicted judgement for the positive class is
        $\frac{1}{3}\left(0.6+0.3+0.9\right)$.
      \item \methname{AEh} computes the distribution of most likely
        labels~\citep{vitsakis2023}. The final predicted judgement for the
        positive class is $\frac{2}{3}$ because 2 out of 3 judgements are over
        50\%.
    \end{enumerate}
  \item \methname{JSD}, \methname{SmF1}~(soft micro F\textsubscript{1}), and
    \methname{SMF1}~(soft macro F\textsubscript{1}) train using JSD, $1-$ soft
    accuracy for multiclass or soft micro F\textsubscript{1} for multilabel
    tasks, and $1-$ soft macro F\textsubscript{1} as the objective
    respectively~(see \Cref{sec:eval-metrics} for definitions).\footnote{We
      exclude entropy correlation because it doesn't have a unique maximiser.}
    These approaches leverage the differentiability of soft metrics, resulting
    in a single objective for both training and inference. By directly
    optimising the evaluation metric at training time, we expect the models to
    exhibit superior performance, especially when evaluated with the
    corresponding metric.
  \item \methname{LA-min}~(loss aggregation $\min$) and \methname{LA-max}~(loss
    aggregation $\max$) aggregate the losses of all annotations for a given
    training instance using the $\min$ and the $\max$ functions respectively.
    For example, given a training instance $x$ with annotations $y_1$ and $y_2$,
    we compute the losses $l_1=\mathcal{L}(x,y_1)$ and $l_2=\mathcal{L}(x,y_2)$
    where $\mathcal{L}$ is the cross-entropy loss function. Then, we update
    model parameters based on the gradient of $g(l_1,l_2)$ where $g$ is either
    the $\min$ or the $\max$ function. Using the $\min$~(resp. $\max$) function
    means selecting the least~(resp.\ most) ``surprising'' annotation for the
    model. Using the mean aggregation is mathematically equivalent to
    \methname{SL} and thus excluded.
\end{enumerate}

\section{Evaluation of HLV Training Methods}

We now conduct an extensive study covering 6 datasets, 14 training methods, and 6
evaluation metrics across 2 language models to understand the performance
landscape of HLV data.

\subsection{Experimental Setup}

\begin{table}\footnotesize
  \caption{Public datasets used in the empirical meta-evaluation of metrics,
    along with the language~(L), type of annotators~(O) where E and C mean
    Experts and Crowds respectively, number of examples to the nearest
    thousand~($N$), average number of annotations per example~($J$), number of
    classes~($K$), whether annotator identity is present~(A), and whether the
    task is multilabel classification~(M).}~\label{tbl:datasets}
  \sisetup{table-format=3.1}
  \begin{tabular}{@{}llllrSrcc@{}}
    \toprule
    Name                                 & Labels          & L  & O & $N$ & $J$   & $K$ & A    & M    \\
    \midrule
    HS-Brexit~\citep{akhtar2021a}        & Hate speech     & EN & E & 1   & 6.0   & 2   & \yes & \no  \\
    MD-Agreement~\citep{leonardelli2021} & Offensiveness   & EN & C & 10  & 5.0   & 2   & \yes & \no  \\
    ArMIS~\citep{almanea2022}            & Misogyny        & AR & E & 1   & 3.0   & 2   & \yes & \no  \\
    ChaosNLI~\citep{nie2020}             & NLI             & EN & C & 3   & 100.0 & 3   & \no  & \no  \\
    MFRC~\citep{trager2022a}             & Moral sentiment & EN & E & 18  & 3.4   & 8   & \yes & \yes \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Public datasets}

We use 5 public datasets with disaggregated annotations that have been used in
previous HLV studies, four of which are in English and one in Arabic.
\Cref{tbl:datasets} shows that the datasets cover both crowd~(domain
non-specialist) and expert~(domain specialist) annotators, as well as the number
of examples, average number of annotations, and the language of the dataset. For
HS-Brexit, MD-Agreement, and ArMIS, we use the same train--test splits as
\citet{leonardelli2023}. For both ChaosNLI and MFRC, we use 10-fold
cross-validation with 10\% of the training portion used as a development set.
For ChaosNLI, we include only the SNLI and MNLI subsets~(each has 1.5K examples)
because of their standard NLI setup of one premise and one hypothesis.

\paragraph{``Text Annotation Game'' (TAG) dataset}

We also include a dataset called TAG which consists of English texts describing
legal problems written by non-expert legal help seekers. We have access to this
dataset because of our collaboration with Justice
Connect,\footnote{\url{https://justiceconnect.org.au}} an Australian public
benevolent institution\footnote{As defined by the Australian
  government:~\url{https://www.acnc.gov.au/charity/charities/4a24f21a-38af-e811-a95e-000d3ad24c60/profile}}
providing free legal assistance to laypeople facing legal problems. Each text in
the dataset was annotated by an average of 5.5 practising lawyers, who each
selected one or more out of the 33
areas of law\footnote{Including a special label ``Not a legal issue''.} that
applied to the problem. Thus, it is a multilabel classification task. Example
areas of law include \textit{Neighbourhood disputes} and \textit{Housing and
  residential tenancies}. Average inter-annotator agreement~($\alpha$) over the
areas of law is \num{0.454}, which is modest.\footnote{We compute this
  agreement on random 10\% selection of the data due to the high computational
  cost.} This is because lawyers often have different interpretations of the
same problem due to their different legal specialisations and years of
experience. However, these different interpretations are all valid as human
label variation because these registered lawyers are subject matter experts who play a crucial part in interpreting the law. The dataset has
a total of 11K texts collected between July 2020 and early December 2023. We
randomly split the dataset 8:1:1 for the training, development, and test sets
respectively. While it cannot be released publicly because it is based on
real-world confidential legal requests from help-seekers, we include the TAG
dataset due to its importance for the meta-evaluation later in
\Cref{sec:meta-eval}.

\paragraph{Models}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{results-armis}
  \caption{Performance difference~(delta) with mean \methname{MV} performance of
    TwHIN-BERT on ArMIS. The methods are sorted by their mean ranking across datasets,
    models, and metrics.}\label{fig:delta-mv-results-armis}
\end{figure}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{results-hs-md}
  \caption{Performance difference~(delta) with mean \methname{MV} performance on
    HS-Brexit and MD-Agreement. \methname{SmF1} with TwHIN-BERT predicts
    zero for all test instances on HS-Brexit so its entropy correlation is
    undefined. The methods are sorted by their mean ranking across datasets,
    models, and metrics.}\label{fig:delta-mv-results-hs-md}
\end{figure}

We experiment with both small and large language models.
Because HS-Brexit, MD-Agreement, and ArMIS use Twitter as source data, we
use TwHIN-BERT~\citep{zhang2023a} which was pretrained on Twitter data and
supports both English and Arabic. For other datasets, we use
RoBERTa~\citep{liu2019h}. In addition, we experiment with
8B LLaMA~3~\citep{grattafiori2024a} for the English datasets.\footnote{While LLaMA 3
  was trained on multilingual data, its intended use is English only.} We
replace the output layer of these pretrained models with a linear layer with $K$
output units. We use the base versions of both RoBERTa and TwHIN-BERT (with roughly 100M parameters each).

\paragraph{Training}

We implement all methods using FlairNLP~\citep{akbik2019}. For RoBERTa and
TwHIN-BERT models, we tune the learning rate and the batch size using random
search. Because there are multiple evaluation metrics that aren't necessarily
comparable, we select the best hyperparameters based on the geometric mean of
their values\footnote{We transform entropy correlation to a value between 0 and
  1 before taking the mean.} to avoid biasing towards one metric. We exclude
both hard and soft macro F\textsubscript{1} scores from this mean as most
datasets are balanced.
For LLaMA, we use FlairNLP's default hyperparameters\footnote{Learning rate and
  batch size are \num{5e-5} and 32 respectively.} for computational reasons. We
use LoRA~\citep{hu2022} to finetune LLaMA efficiently. All models are finetuned
for 10 epochs. We truncate inputs longer than 512 tokens in the TAG dataset,
affecting only 4\% of instances.
For ChaosNLI and MFRC, we train the final model used for evaluation of each fold
on the concatenation of the training and the development sets.

\paragraph{Evaluation}

When cross-validation is used, we evaluate the concatenated test predictions.
Otherwise, we evaluate on the test set. We perform the evaluation across 3
training runs.

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{results}
  \caption{Performance difference~(delta) with mean \methname{MV} performance on
    ChaosNLI~(both the SNLI and the MNLI portions), MFRC, and TAG datasets.
    ChaosNLI doesn't have annotator identity information, so \methname{AE},
    \methname{AEh}, \methname{AR}, \methname{ARh} are inapplicable.
    \methname{AL} is omitted from MFRC and TAG because it is developed for
    single-label rather than multilabel tasks. \methname{AEh} with RoBERTa
    predicts zero for all test instances on several classes in TAG, so its
    entropy correlation is undefined. The methods are sorted by their mean
    ranking across datasets, models, and metrics.}\label{fig:delta-mv-results}
\end{figure*}

\subsection{Results}

We compute the difference between the performance of a single run of an HLV
training method and the mean performance of \methname{MV}. We then report the
mean differences across runs. We present the results on ArMIS in
\Cref{fig:delta-mv-results-armis}~(the only non-English dataset), HS-Brexit and
MD-Agreement in \Cref{fig:delta-mv-results-hs-md}~(Learning with Disagreements
shared task datasets), and the remaining datasets in
\Cref{fig:delta-mv-results}~(multiclass and multilabel datasets). We report
performance of \methname{MV} is in \Cref{tbl:mv-results} in the Appendix. We
make the following observations.

First, HLV training methods have different effectiveness compared to
\methname{MV}, with some methods performing very poorly. For example, on ArMIS
and both portions of ChaosNLI, \methname{LA-min} consistently performs worse
than \methname{MV}. This finding suggests the importance of choosing the right
training method when embracing HLV.

Second, \methname{ReL} and \methname{SL} substantially outperform \methname{MV}
on most datasets across models and evaluation metrics. Furthermore,
\methname{ReL} and \methname{SL} are often among the top-performing methods,
even when compared to \methname{JSD} and \methname{SmF1} evaluated using
\pojsd and soft accuracy/micro F\textsubscript{1} respectively. In other
words, \methname{ReL} and \methname{SL} are often on par with~(or outperform in
some cases with LLaMA) the training method whose learning objective is the same
as the evaluation metric, demonstrating the strength of \methname{ReL} and
\methname{SL}.

Third, \methname{ReL} and \methname{SL} are generally competitive with each
other. However, there are two exceptions to this trend. First, on ArMIS,
\methname{SL} outperforms \methname{ReL}. Second, on both HS-Brexit and the SNLI
portion of ChaosNLI, \methname{ReL} outperforms \methname{SL} with LLaMA. For
the former finding, however, ArMIS is an outlier dataset: it is the only Arabic
dataset, has the smallest number of examples and annotations per example, and
all methods generally have higher variance on this dataset. The latter finding
suggests that \methname{ReL} is better suited to large language models than
\methname{SL}.

Fourth, \methname{SMF1} and \methname{JSD} generally also perform well, although
they lag behind \methname{ReL} and \methname{SL}. Because both
\methname{SMF1} and \methname{JSD} use a soft evaluation metric as the training
objective, this finding underlines the value in using differentiable metrics for
HLV model training.

We also observe that \methname{SmF1} with TwHIN-BERT results in undefined entropy
correlation on HS-Brexit. Looking closely, we find that the method degenerates to
predicting $Q_{ik}=0$ for all $i$. We observe the same degenerate results for
\methname{AEh} with RoBERTa on the TAG dataset. Moreover, \methname{SmF1} with
RoBERTa attains much lower soft micro F\textsubscript{1} score than the
top-performing \methname{ReL} on TAG, even though \methname{SmF1}
optimises the evaluation metric directly during training. Our investigation
suggests that these unexpected observations are due to the class imbalance
present in both HS-Brexit and TAG.\ When the training objective is fair~(e.g.,
\methname{SMF1}), the problem disappears.

\subsection{Discussion}

\methname{ReL} and \methname{SL} appear to perform best as a training method,
and that is somewhat surprising because they are the most straightforward
methods to incorporate HLV in model training.
That said, our finding that \methname{SL} is a strong performer is in line with
what \citet{uma2021} discovered. The results of \methname{ReL}, however, stand
in contrast with prior work~\citep{uma2021,kurniawan2024}.
This difference may be due to the structured prediction tasks that they
considered which have an exponentially large output space. We note that
\citet{uma2021} also found that \methname{ReL} performs really well in simple,
multiclass classification tasks, in line with our findings.

Moreover, the results suggest that \methname{ReL} is likely to outperform
\methname{SL} when large language models are used. A possible explanation is
that with \methname{SL}, the model observes the complete judgement distribution
of an instance as target. Thus, there is not much flexibility as the model has
to predict that distribution to minimise the training loss. In contrast, with
\methname{ReL}, the model only observes one annotation of the instance as
target. Therefore, it has more flexibility in how it distributes the judgement
distribution mass across all the labels of that instance over training steps. We
hypothesise that large language models have sufficient learning capacity to
benefit from this flexibility while smaller models do not. This explanation is
consistent with our findings on ChaosNLI that \methname{ReL} has much larger
gains compared to \methname{SL}. ChaosNLI has 100 annotations per instance, the
largest number out of all datasets. We leave further investigation of this
hypothesis to future work.

Despite its strengths, \methname{ReL} has substantial computational cost:
because it keeps the annotations disaggregated, the size of the training data
can be enormous. For example, in the ChaosNLI dataset, \methname{ReL} makes
the training data 100 times larger because there are exactly 100 annotations for
each training instance. That \methname{ReL} performs better with LLaMA than
RoBERTa on ChaosNLI exacerbates the computational inefficiency of \methname{ReL}:
both the data and the model must be large for it to perform optimally.
Mitigating this inefficiency can be an interesting avenue for future work.

Because the training data is much larger, one may think that the superiority of
\methname{ReL} over \methname{SL} is due to its much longer training. However,
this is not the case. We find that increasing the number of training iterations
of \methname{SL} to match that of \methname{ReL} does not improve performance.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{TAG-Consensus-1}
  \caption{Areas of law annotation interface}\label{fig:aol-annot}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{TAG-Consensus-2}
  \caption{Preference annotation interface}\label{fig:pref-annot}
\end{figure}

\section{Meta-Evaluation of HLV Evaluation Metrics}\label{sec:meta-eval}

In the previous section, we gained an understanding about the best training methods that perform consistently well
across metrics. In this section, we tackle the question of what the best evaluation metric is for HLV data. We address this question by conducting a meta-evaluation of all the evaluation metrics in question. Intuitively,
a good evaluation metric should correlate well with human preference. That is, if a model has a high performance according to a certain metric, we expect human judges to prefer that model's predictions. This forms the basis of our meta-evaluation experiment.

To obtain a reliable estimate of human preference, we rely on subject matter experts. This is so that when they disagree we can be confident that
the disagreement is a result of genuine variation rather than noise. In our case, we have access to practising lawyers who are more than qualified for our task~(i.e.\ legal area classification). As such, we use the TAG dataset to conduct our meta-evaluation experiments despite the inability to distribute
it.

\subsection{Experimental Setup}

For each pair of pretrained model and training method, we randomly
sample an average of 28 test instances from TAG. For each instance,
we (a)~show the instance text, the ground truth label distribution and the predicted
label distribution from one of our trained models, and then (b)~ask lawyers which distribution\footnote{Because
  it is multilabel task, the distribution doesn't sum to
  one.} is more accurate. The lawyers can choose exactly one, both, or neither
of the visualisations of the distributions. On average, each instance is annotated by 3.8 lawyers.
To make sure the lawyers form their own expectation on the areas of law
relevant to an instance, we first show the instance text and ask them to
annotate it~(\Cref{fig:aol-annot}). After completing this task, we then show
them the label distributions from the model and ground
truth~(\Cref{fig:pref-annot}). We conduct several pilot studies with the lawyers
to refine the interface design to ensure the task is clear to our target annotators.

\begin{figure}
  \centering
  \includegraphics[width=0.6\linewidth]{human-metric-reg}
  \caption{Relationships between evaluation metrics and the fraction of times
    humans select the predicted judgement distribution on the TAG dataset. The
    shaded area denotes a 95\% confidence interval. Each data point corresponds to
    a specific pairing of a language model~(e.g., LLaMA) and a training
    method~(e.g., \methname{ReL}). \methname{AEh} with RoBERTa is excluded
    because of degenerate results on entropy
    correlation.}\label{fig:rel-human-metric}
\end{figure}

It is worth noting that the meta-evaluation task inherently has HLV as there is
not a single, objective ground truth. Every lawyer has their own preference, and
our goal is to capture the \textit{collective preference} of the lawyers (i.e.\
the proportion of lawyers who prefer the predicted distributions).
To minimise the risk of lawyers figuring out which of the two judgement
distribution options is the ground truth distribution, we randomise the options
and the order of the instances given to the lawyers. To filter out low quality
predictions, we only show areas of law whose judgement exceeds a threshold of
\num{0.1}.\footnote{We arrive at this threshold by choosing a value that results
  in similar numbers of labels whose judgement exceeds the value in the
  predicted and the ground truth distributions.}

\subsection{Results}

We meta-evaluate HLV evaluation metrics by computing the correlation between
their value and the fraction of times the lawyers select the predicted judgement
distribution. \Cref{fig:rel-human-metric} shows that all metrics exhibit a
linear relationship with human preference, which is positive for all metrics
other than entropy correlation, suggesting that it is a poor HLV metric.

\Cref{tbl:human-metric-corr} confirms that all metrics are positively
correlated with human preference except for entropy
correlation. Furthermore, it shows that only soft micro
F\textsubscript{1} and \pojsd correlate with human judgements at a
level of statistical significance. This finding suggests that both
metrics are good for evaluation in the HLV context.

\subsection{Method ranking based on human preference}

Given the human annotations above, we can compute the performance of an HLV
training method based on the number of times humans select the predicted
judgement distribution. \Cref{tbl:human-pref-meth-rank} shows that the rankings
of the methods have some similarities to TAG results in
\Cref{fig:delta-mv-results-armis,fig:delta-mv-results-hs-md,fig:delta-mv-results}.
For instance, \methname{ReL}, \methname{JSD} and \methname{SL} are generally
strong performers (exception:  \methname{SL} with LLaMA). That said, we also see
some discrepancies. For example, \methname{AR} and \methname{LA-min} come out on
top here but this is not reflected in
\Cref{fig:delta-mv-results-armis,fig:delta-mv-results-hs-md,fig:delta-mv-results}.
We contend, however, that the meta-evaluation study is ultimately based on one
dataset and so we should interpret these results cautiously.

\begin{table}\small
  \caption{Correlation coefficients between evaluation metrics and the fraction
    of times humans select the predicted judgement distribution on TAG dataset.
    Asterisks denote statistical significance~($p<.05$) with a permutation
    test.}\label{tbl:human-metric-corr}
  \sisetup{table-format=1.3, print-zero-integer=false, table-space-text-post={$^*$}}
  \begin{tabular}{@{}llS@{}}
    \toprule
    Type                     & Metric                        & {Pearson $r$} \\
    \midrule
    \multirow[t]{2}{*}{Hard} & Micro F\textsubscript{1}      & 0.442         \\
                             & Macro F\textsubscript{1}      & 0.318         \\
    \cmidrule{1-3}
    \multirow[t]{4}{*}{Soft} & Entropy correlation           & -0.196        \\
                             & \pojsd                        & 0.692$^*$     \\
                             & Soft micro F\textsubscript{1} & 0.611$^*$     \\
                             & Soft macro F\textsubscript{1} & 0.291         \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}\small
  \caption{Rankings of HLV training methods based on human preference on TAG.\
    With RoBERTa, \methname{AEh} is ranked last due to its degenerate
    results.}\label{tbl:human-pref-meth-rank}
  \begin{tabular}{@{}llll@{}}
    \toprule
    \multicolumn{2}{c}{RoBERTa} & \multicolumn{2}{c}{LLaMA} \\
    \cmidrule(r){1-2} \cmidrule(l){3-4}
    Rank 1--7          & Rank 8--13        & Rank 1--7         & Rank 8--13      \\
    \midrule
     \methname{JSD}    & \methname{SLMV}   & \methname{AR}     & \methname{MV}   \\
     \methname{LA-min} & \methname{SMF1}   & \methname{SmF1}   & \methname{SL}   \\
     \methname{SL}     & \methname{ARh}    & \methname{LA-min} & \methname{ARh}  \\
     \methname{MV}     & \methname{AE}     & \methname{ReL}    & \methname{AEh}  \\
     \methname{AR}     & \methname{LA-max} & \methname{JSD}    & \methname{AE}   \\
     \methname{ReL}    & \methname{AEh}    & \methname{SLMV}   & \methname{SMF1} \\
     \methname{SmF1}   &                   & \methname{LA-max} &                 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Discussion}

Putting all the results together, \pojsd and soft micro F\textsubscript{1} are arguably
some of the best metrics for HLV data.
That said, \citet{uma2021} argued that the value of \pojsd is typically confined within a small
range and lacks an intuitive interpretation. In
contrast, soft micro F\textsubscript{1} is more interpretable. Therefore, our general recommendation is to
report both metrics, but focus on the soft micro F\textsubscript{1} score when an
accessible interpretation is important~(e.g., communicating with non-technical
people) or one is restricted to a single metric~(e.g., in hyperparameter
tuning).

\section{Conclusions}

We propose new evaluation metrics for evaluating model predictions with human
label variation~(HLV). Taking inspiration from remote sensing research, we
represent human judgement distributions as degrees of membership over fuzzy sets
and generalise standard metrics such as accuracy into their soft versions using
fuzzy set operations. Therefore, our proposed metrics have intuitive
interpretations and reduce to standard hard metrics if there is no label
variation. We also generalise both our proposed metrics and some of existing
ones to multilabel classification settings. We mathematically prove that our
proposed soft accuracy metric is upper-bounded by an existing metric based on
the Jensen-Shannon divergence.

Because the soft metrics are differentiable, we propose 3 new training methods for
HLV using the metrics as the training objective. Additionally, we propose 2 new
training methods that aggregate losses over annotations of the same instance. We
test our proposed methods on 6 datasets spanning binary, multiclass, and
multilabel classification tasks, as well as crowd and expert annotators. We
evaluate across 2 language models and using a total of 6 HLV evaluation metrics
including both existing and proposed ones. We compare against 9 existing HLV
training methods and find that training on disaggregated
annotations~(\methname{ReL}) or soft labels~(\methname{SL}) performs best across
evaluation metrics, and that the former is likely to perform better than the
latter when both the number of annotations per instance is large and a large
language model is used. Our meta-evaluation of these metrics shows that our
proposed soft metric correlates best with human preference. We recommend further
research to include our proposed soft metric when reporting model performance in
the HLV context.

\section*{Limitations}

The TAG dataset is private and cannot be released publicly, which is a
limitation of our work in terms of reproducibility. This is because the data is
based on real-world confidential legal requests from help-seekers. More
importantly, their safety and privacy is of high concern: some cases are so
unique that even if you were to anonymise the cases, re-identfication may still
be possible. Nevertheless, we believe our work still offers valuable scientific
knowledge on the investigation of HLV training methods and evaluation metrics.

Our empirical meta-evaluation is limited to a single multilabel classification
dataset. Thus, it is unclear if the findings extend to other datasets and binary
or single-label tasks. Future work could expand on this limitation by
experimenting with a different or diverse selection of datasets.

\appendix

\appendixsection{Derivation of Soft F\textsubscript{1} Scores}

\subsection{Fuzzy Sets}

Below we provide a brief overview of fuzzy sets drawn from the work of
\citet{kruse2022}. A fuzzy set $A$ is defined by its membership function
$\mu_A:X\to [0,1]$, where $\mu_A(x)$ represents the degree of membership of
$x\in X$ in $A$. The cardinality of $A$ is defined as the sum of the degrees of
membership of all elements of $X$:
\begin{equation*}
  \sum_{x\in X}\mu_A(x).
\end{equation*}
If $A$ and $B$ are fuzzy sets over the same universe $X$ then their intersection
is a fuzzy set whose membership function is defined as
\begin{equation*}
  \mu_{A\cap B}(x)=t(\mu_A(x),\mu_B(x))
\end{equation*}
where $t$ is a function that satisfies 3 properties: commutativity,
associativity, and monotonicity.\footnote{$\beta\leq\gamma\Rightarrow
  t(\alpha,\beta)\leq t(\alpha,\gamma)$ where $0\leq\alpha,\beta,\gamma\leq1$}
The function $t$ is called a \emph{t-norm~(triangular norm)}. A special t-norm
is the $\min$ function because it is also idempotent, i.e.
$t(\alpha,\alpha)=\alpha$.

\subsection{Soft F\textsubscript{1} Scores}

Let $H_k$ and $R_k$ denote the predicted and the true (crisp)~sets of examples
in class $k$ respectively. Standard precision and recall scores for class $k$
are defined as:
\begin{align*}
  \mathrm{Prec}_k&=\frac{|H_k\cap R_k|}{|H_k|},\\
  \mathrm{Rec}_k&=\frac{|H_k\cap R_k|}{|R_k|}.
\end{align*}

Embracing HLV, we view judgement distributions as degrees of memberships of fuzzy
sets, each corresponds to a class, over a universe of examples. Specifically, we
consider $P_{ik}$ and $Q_{ik}$ as the true and the predicted degrees of
membership of example $i$ in class $k$. Therefore, we can define the soft
precision and recall scores using fuzzy set operations as
follows~\citep{harju2023}:
\begin{equation}
  \mathrm{SoftPrec}_k=\frac{\sum_i\min(P_{ik},Q_{ik})}{\sum_iQ_{ik}}\label{eqn:cw-soft-p}
\end{equation}
and
\begin{equation}
  \mathrm{SoftRec}_k=\frac{\sum_i\min(P_{ik},Q_{ik})}{\sum_iP_{ik}}.\label{eqn:cw-soft-r}
\end{equation}
The soft F\textsubscript{1} score is the harmonic mean between the soft
precision and recall scores as usual:
\begin{equation}
  \mathrm{SoftF\textsubscript{1}}_k=2\frac{\sum_i\min(P_{ik},Q_{ik})}{\sum_i(P_{ik}+Q_{ik})}.\label{eqn:cw-soft-f1}
\end{equation}
Taking the mean of \Cref{eqn:cw-soft-f1} over classes results in the soft macro
F\textsubscript{1} score. To obtain the micro variant, we simply modify the sums
in both the numerator and the denominator of \Cref{eqn:cw-soft-p,eqn:cw-soft-r}
to also iterate over classes to get the soft micro precision and recall scores,
and then take the harmonic mean of the two scores as normal.

\appendixsection{Performance of \methname{MV}}

\Cref{tbl:mv-results} reports the performance of \methname{MV} across datasets
and evaluation metrics.

\begin{table}[!h]\scriptsize
  \caption{Mean~($\pm$ std) performance of \methname{MV} measured by entropy
    correlation, \pojsd, and various versions of F\textsubscript{1} scores.
    MD-Agr refers to MD-Agreement. C-SNLI and C-MNLI refer to the SNLI and the
    MNLI portions of ChaosNLI respectively. Hard~(resp. soft) accuracy
    scores~(for multiclass tasks) are reported in the hard~(resp. soft) micro
    F\textsubscript{1} score column.}\label{tbl:mv-results}
  \begin{tabular}{@{}ll*{6}{c}@{}}
  \toprule
  Dataset                       & Model      & Micro F\textsubscript{1} & Macro F\textsubscript{1} & Entropy corr.   & \pojsd          & Soft micro F\textsubscript{1} & Soft macro F\textsubscript{1} \\
  \midrule
  \multirow[c]{2}{*}{HS-Brexit} & TwHIN-BERT & .903 $\pm$ .009          & .581 $\pm$ .093          & .470 $\pm$ .119 & .941 $\pm$ .010 & .883 $\pm$ .013               & .676 $\pm$ .071               \\
                                & LLaMA      & .950 $\pm$ .012          & .660 $\pm$ .104          & .330 $\pm$ .065 & .939 $\pm$ .002 & .886 $\pm$ .003               & .623 $\pm$ .012               \\
  \cmidrule{1-8}
  \multirow[c]{2}{*}{MD-Agr}    & TwHIN-BERT & .809 $\pm$ .003          & .779 $\pm$ .002          & .379 $\pm$ .002 & .921 $\pm$ .001 & .811 $\pm$ .002               & .790 $\pm$ .003               \\
                                & LLaMA      & .810 $\pm$ .001          & .784 $\pm$ .001          & .215 $\pm$ .019 & .863 $\pm$ .003 & .770 $\pm$ .002               & .743 $\pm$ .003               \\
  \cmidrule{1-8}
  ArMIS                         & TwHIN-BERT & .713 $\pm$ .014          & .705 $\pm$ .014          & .062 $\pm$ .015 & .769 $\pm$ .011 & .702 $\pm$ .009               & .695 $\pm$ .008               \\
  \cmidrule{1-8}
  \multirow[c]{2}{*}{C-SNLI}    & RoBERTa    & .632 $\pm$ .005          & .551 $\pm$ .007          & .123 $\pm$ .014 & .830 $\pm$ .005 & .672 $\pm$ .004               & .629 $\pm$ .007               \\
                                & LLaMA      & .601 $\pm$ .010          & .551 $\pm$ .008          & .118 $\pm$ .022 & .830 $\pm$ .005 & .668 $\pm$ .005               & .633 $\pm$ .007               \\
  \cmidrule{1-8}
  \multirow[c]{2}{*}{C-MNLI}    & RoBERTa    & .496 $\pm$ .014          & .369 $\pm$ .014          & .044 $\pm$ .007 & .854 $\pm$ .009 & .668 $\pm$ .010               & .642 $\pm$ .009               \\
                                & LLaMA      & .514 $\pm$ .017          & .439 $\pm$ .010          & .100 $\pm$ .018 & .827 $\pm$ .005 & .643 $\pm$ .006               & .615 $\pm$ .006               \\
  \cmidrule{1-8}
  \multirow[c]{2}{*}{MFRC}      & RoBERTa    & .657 $\pm$ .001          & .456 $\pm$ .002          & .404 $\pm$ .002 & .743 $\pm$ .001 & .591 $\pm$ .001               & .418 $\pm$ .000               \\
                                & LLaMA      & .649 $\pm$ .001          & .454 $\pm$ .005          & .353 $\pm$ .005 & .717 $\pm$ .001 & .576 $\pm$ .001               & .395 $\pm$ .001               \\
  \cmidrule{1-8}
  \multirow[c]{2}{*}{TAG}       & RoBERTa    & .742 $\pm$ .002          & .610 $\pm$ .014          & .394 $\pm$ .002 & .985 $\pm$ .000 & .645 $\pm$ .002               & .471 $\pm$ .003               \\
                                & LLaMA      & .759 $\pm$ .006          & .681 $\pm$ .005          & .279 $\pm$ .010 & .983 $\pm$ .000 & .648 $\pm$ .002               & .521 $\pm$ .003               \\
  \bottomrule
  \end{tabular}
\end{table}

\begin{acknowledgments}
This research is supported by the Australian Research Council Linkage Project
LP210200917\footnote{\url{https://dataportal.arc.gov.au/NCGP/Web/Grant/Grant/LP210200917}}
and funded by the Australian Government. This research is done in collaboration
with Justice Connect, an Australian public benevolent institution.\footnote{As
  defined by the Australian
  government:~\url{https://www.acnc.gov.au/charity/charities/4a24f21a-38af-e811-a95e-000d3ad24c60/profile}}
We thank Kate Fazio, Tom O'Doherty, and Rose Hyland from Justice Connect for
their support throughout the project. We thank David McNamara, Yashik Anand, and
Mark Pendergast also from Justice Connect for the development of the
meta-evaluation annotation interface. This research is supported by The
University of Melbourne’s Research Computing Services and the Petascale Campus
Initiative.
\end{acknowledgments}

\bibliographystyle{compling}
\bibliography{em}

\end{document}
\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}

