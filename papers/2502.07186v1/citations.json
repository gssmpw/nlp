[
  {
    "index": 0,
    "papers": [
      {
        "key": "laskar2024survey",
        "author": "Laskar, Md Tahmid Rahman and Alqahtani, Sawsan",
        "title": "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "saxon2024framework",
        "author": "Saxon, Michael and Perez, Ethan",
        "title": "A Framework for Evaluating LLMs Under Task Indeterminacy"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "jin2024gpt4v",
        "author": "Jin, Qiao and Li, Yijia",
        "title": "Hidden Flaws Behind Expert-Level Accuracy of Multimodal GPT-4 Vision in Medicine"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "thakur2024judgingjudgesevaluatingalignment",
        "author": "Aman Singh Thakur and Kartik Choudhary and Venkat Srinik Ramayapally and Sankaran Vaidyanathan and Dieuwke Hupkes",
        "title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "10.5555/3666122.3668142",
        "author": "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion",
        "title": "Judging LLM-as-a-judge with MT-bench and Chatbot Arena"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "dong2024llmpersonalizedjudge",
        "author": "Yijiang River Dong and Tiancheng Hu and Nigel Collier",
        "title": "Can LLM be a Personalized Judge?"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "liusie2024llmcomparativeassessmentzeroshot",
        "author": "Adian Liusie and Potsawee Manakul and Mark J. F. Gales",
        "title": "LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "bhat-varma-2023-large",
        "author": "Bhat, Savita  and\nVarma, Vasudeva",
        "title": "Large Language Models As Annotators: A Preliminary Evaluation For Annotating Low-Resource Language Content"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "pavlovic-poesio-2024-effectiveness",
        "author": "Pavlovic, Maja  and\nPoesio, Massimo",
        "title": "The Effectiveness of {LLM}s as Annotators: A Comparative Overview and Empirical Analysis of Direct Representation"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "tamang2024evaluating",
        "author": "Tamang, Sagar and Bora, Dibya Jyoti",
        "title": "Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liu2024dafnd",
        "author": "Liu, Ye and Zhu, Jiajun",
        "title": "Detect, Investigate, Judge and Determine: A Novel LLM-based Framework for Few-shot Fake News Detection"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wu2023promptalign",
        "author": "Wu, Jiaying and Li, Shen",
        "title": "Prompt-and-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "fan2024subjective",
        "author": "Fan, Yuchen and Liu, Yantao",
        "title": "Evaluating Generative Language Models in Information Extraction as Subjective Question Correction"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "bender2021parrots",
        "author": "Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Margaret",
        "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "springer2023fskd",
        "author": "X. Zhang, D. Wu",
        "title": "FSKD: Detecting Fake News with Few-Shot Knowledge Distillation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "nature2024consistency",
        "author": "Zhou, X. and Lee, K.",
        "title": "A Veracity Dissemination Consistency-Based Few-Shot Fake News Detection Method"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "chen2023quantifying",
        "author": "Chen, Jiuhai and Mueller, Jonas",
        "title": "Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "kim2024collaborative",
        "author": "Kim, Hana and Lee, Minji",
        "title": "Collaborative Annotation with LLMs and Humans"
      },
      {
        "key": "kim2024collaborative",
        "author": "Kim, Hana and Lee, Minji",
        "title": "Collaborative Annotation with LLMs and Humans"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "virk2024enhancing",
        "author": "Virk, Yuvraj and Devanbu, Premkumar and Ahmed, Toufique",
        "title": "Enhancing Trust in LLM-Generated Code Summaries with Calibrated Confidence Scores"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "when_to_make_exceptions_exploring_language_models_as_accounts_of_human__moral_judgment_2022",
        "author": "Unknown",
        "title": " When to Make Exceptions: Exploring Language Models as Accounts of Human\nMoral Judgment "
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "hyun2024metal",
        "author": "Hyun, Sangwon and Guo, Mingyu and Babar, M Ali",
        "title": "METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "xue2024exploring",
        "author": "Xue, Pengyu and Wu, Linhao and Yang, Zhen and Li, Xinyi and Yu, Zhongxing and Jin, Zhi and Li, Ge and Xiao, Yan and Wu, Jingwen",
        "title": "Exploring and Lifting the Robustness of LLM-powered Automated Program Repair with Metamorphic Testing"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "li2024drowzee",
        "author": "Li, Ningke and Li, Yuekang and Liu, Yi and Shi, Ling and Wang, Kailong and Wang, Haoyu",
        "title": "Drowzee: Metamorphic testing for fact-conflicting hallucination detection in large language models"
      }
    ]
  }
]