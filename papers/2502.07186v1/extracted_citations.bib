@inproceedings{10.5555/3666122.3668142,
author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
title = {Judging LLM-as-a-judge with MT-bench and Chatbot Arena},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {2020},
numpages = {29},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@inproceedings{bender2021parrots,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Margaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT)},
  year={2021},
  pages={610--623},
  publisher={ACM},
  doi={10.1145/3442188.3445922}
}

@inproceedings{bhat-varma-2023-large,
    title = "Large Language Models As Annotators: A Preliminary Evaluation For Annotating Low-Resource Language Content",
    author = "Bhat, Savita  and
      Varma, Vasudeva",
    editor = {Deutsch, Daniel  and
      Dror, Rotem  and
      Eger, Steffen  and
      Gao, Yang  and
      Leiter, Christoph  and
      Opitz, Juri  and
      R{\"u}ckl{\'e}, Andreas},
    booktitle = "Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems",
    month = nov,
    year = "2023",
    address = "Bali, Indonesia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eval4nlp-1.8",
    doi = "10.18653/v1/2023.eval4nlp-1.8",
    pages = "100--107",
    abstract = "The process of collecting human-generated annotations is time-consuming and resource-hungry. In the case of low-resource (LR) languages such as Indic languages, these efforts are more expensive due to the dearth of data and human experts. Considering their importance in solving downstream applications, there have been concentrated efforts exploring alternatives for human-generated annotations. To that extent, we seek to evaluate multilingual large language models (LLMs) for their potential to substitute or aid human-generated annotation efforts. We use LLMs to re-label publicly available datasets in LR languages for the tasks of natural language inference, sentiment analysis, and news classification. We compare these annotations with existing ground truth labels to analyze the efficacy of using LLMs for annotation tasks. We observe that the performance of these LLMs varies substantially across different tasks and languages. The results show that off-the-shelf use of multilingual LLMs is not appropriate and results in poor performance in two of the three tasks.",
}

@article{chen2023quantifying,
  title={Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment},
  author={Chen, Jiuhai and Mueller, Jonas},
  journal={arXiv preprint arXiv:2308.16175},
  year={2023}
}

@misc{dong2024llmpersonalizedjudge,
      title={Can LLM be a Personalized Judge?}, 
      author={Yijiang River Dong and Tiancheng Hu and Nigel Collier},
      year={2024},
      eprint={2406.11657},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11657}, 
}

@article{fan2024subjective,
  title={Evaluating Generative Language Models in Information Extraction as Subjective Question Correction},
  author={Fan, Yuchen and Liu, Yantao},
  journal={ACL},
  year={2024},
  url={https://aclanthology.org/2024.lrec-main.567/}
}

@inproceedings{hyun2024metal,
  title={METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities},
  author={Hyun, Sangwon and Guo, Mingyu and Babar, M Ali},
  booktitle={2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
  pages={117--128},
  year={2024},
  organization={IEEE}
}

@article{jin2024gpt4v,
  title={Hidden Flaws Behind Expert-Level Accuracy of Multimodal GPT-4 Vision in Medicine},
  author={Jin, Qiao and Li, Yijia},
  journal={ArXiv},
  year={2024},
  url={https://arxiv.org/abs/2401.08396}
}

@article{kim2024collaborative,
  title={Collaborative Annotation with LLMs and Humans},
  author={Kim, Hana and Lee, Minji},
  journal={Journal of Applied AI},
  year={2024},
  volume={15},
  number={1},
  pages={45--58}
}

@article{laskar2024survey,
  title={A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations},
  author={Laskar, Md Tahmid Rahman and Alqahtani, Sawsan},
  journal={ACL},
  year={2024},
  url={https://aclanthology.org/2024.emnlp-main.764/}
}

@article{li2024drowzee,
  title={Drowzee: Metamorphic testing for fact-conflicting hallucination detection in large language models},
  author={Li, Ningke and Li, Yuekang and Liu, Yi and Shi, Ling and Wang, Kailong and Wang, Haoyu},
  journal={Proceedings of the ACM on Programming Languages},
  volume={8},
  number={OOPSLA2},
  pages={1843--1872},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@article{liu2024dafnd,
  title={Detect, Investigate, Judge and Determine: A Novel LLM-based Framework for Few-shot Fake News Detection},
  author={Liu, Ye and Zhu, Jiajun},
  journal={ArXiv},
  year={2024},
  url={https://arxiv.org/abs/2407.08952}
}

@misc{liusie2024llmcomparativeassessmentzeroshot,
      title={LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models}, 
      author={Adian Liusie and Potsawee Manakul and Mark J. F. Gales},
      year={2024},
      eprint={2307.07889},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.07889}, 
}

@article{nature2024consistency,
  title={A Veracity Dissemination Consistency-Based Few-Shot Fake News Detection Method},
  author={Zhou, X. and Lee, K.},
  journal={Nature},
  year={2024},
  url={https://www.nature.com/articles/s41598-024-70039-9.pdf}
}

@inproceedings{pavlovic-poesio-2024-effectiveness,
    title = "The Effectiveness of {LLM}

@article{saxon2024framework,
  title={A Framework for Evaluating LLMs Under Task Indeterminacy},
  author={Saxon, Michael and Perez, Ethan},
  journal={ArXiv},
  year={2024},
  url={https://arxiv.org/abs/2411.13760}
}

@article{springer2023fskd,
  title={FSKD: Detecting Fake News with Few-Shot Knowledge Distillation},
  author={X. Zhang, D. Wu},
  booktitle={Springer AI Conference},
  year={2023},
  url={https://link.springer.com/chapter/10.1007/978-3-031-46677-9_29}
}

@article{tamang2024evaluating,
  title={Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages},
  author={Tamang, Sagar and Bora, Dibya Jyoti},
  journal={arXiv preprint arXiv:2411.12240},
  year={2024}
}

@misc{thakur2024judgingjudgesevaluatingalignment,
      title={Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges}, 
      author={Aman Singh Thakur and Kartik Choudhary and Venkat Srinik Ramayapally and Sankaran Vaidyanathan and Dieuwke Hupkes},
      year={2024},
      eprint={2406.12624},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.12624}, 
}

@article{virk2024enhancing,
  title={Enhancing Trust in LLM-Generated Code Summaries with Calibrated Confidence Scores},
  author={Virk, Yuvraj and Devanbu, Premkumar and Ahmed, Toufique},
  journal={arXiv preprint arXiv:2404.19318},
  year={2024}
}

@misc{when_to_make_exceptions_exploring_language_models_as_accounts_of_human__moral_judgment_2022,
        title={ When to Make Exceptions: Exploring Language Models as Accounts of Human
  Moral Judgment },
        
        
        year={ 2022 },
        publisher={ arXiv (Cornell University) },
        
        
        
        
        
        doi={ 10.48550/arxiv.2210.01478 },  
      }

@article{wu2023promptalign,
  title={Prompt-and-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection},
  author={Wu, Jiaying and Li, Shen},
  journal={ArXiv},
  year={2023},
  url={https://arxiv.org/abs/2309.16424}
}

@article{xue2024exploring,
  title={Exploring and Lifting the Robustness of LLM-powered Automated Program Repair with Metamorphic Testing},
  author={Xue, Pengyu and Wu, Linhao and Yang, Zhen and Li, Xinyi and Yu, Zhongxing and Jin, Zhi and Li, Ge and Xiao, Yan and Wu, Jingwen},
  journal={arXiv preprint arXiv:2410.07516},
  year={2024}
}

