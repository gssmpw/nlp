
\section{Introduction}


% \begin{figure*}[t]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=2\columnwidth]{Images/overview.pdf}}
% \caption{Overview of labeling using Perceived Confidence Score (PCS)}
% \label{fig:overview}
% \end{center}
% \vskip -0.2in
% \end{figure*}

Large Language Models (LLMs) demonstrate exceptional performance in natural language processing tasks, such as sentiment analysis \cite{zhang-etal-2024-sentiment}, fake news detection \cite{golob2024fact}, and bias identification \cite{kumar2024decoding}, often achieving accuracy comparable to human annotators \cite{Gligoric2024CanUL}. However, their reliability can diminish when faced with uncertainty, particularly in cases where internal probabilities are inaccessible \cite{liu2024examining}. This challenge has prompted efforts to improve the confidence and reliability of LLM-generated annotations \cite{becker2024cycles, gligoric2024can}. The opacity of LLMs remains a significant concern, especially in domains where ethical and regulatory compliance demands transparency \cite{castelvecchi2021can, jobin2019global}. 
%Metrics such as accuracy or F1-score often fail to comprehensively capture the reliability of LLMs in practical applications \cite{gebru2021datasheets}. 

Recent research \cite{geng-etal-2024-survey, luo2024understanding} highlights the importance of confidence assessment and explainability to foster trust and enhance performance in LLMs. Existing techniques for confidence evaluation with limited access, such as tailored calibration data and adversarial example generation \cite{pedapati2024large}, however, leave room for improvement. Moreover, traditional methods like majority voting and standard accuracy metrics are limited in their ability to account for individual model confidence and contextual nuances, restricting their effectiveness in real-world applications \cite{wang2019superglue}. 
%Addressing these gaps requires more sophisticated approaches to ensure the robustness and reliability of LLM outputs.

To overcome these limitations, we introduce a technique Perceived Confidence Score (PCS) to evaluate and improve the confidence of LLMs in classification tasks. PCS works by analyzing the consistency of model outputs in response to semantically similar but textually varied prompts, providing an inference of confidence. Unlike conventional approaches, PCS measures confidence based on the frequency of consistent labels across multiple iterations, offering a more reliable assessment of model performance. Additionally, we leverage Metamorphic Relations (MRs) from Metamorphic Testing \cite{10336270,chen2021metamorphic} to generate diverse yet semantically equivalent inputs, enriching the evaluation process. To further refine the framework, we propose a Differential Evolution (DE)-based  \cite{storn1997differential} algorithm that optimizes the weights assigned to MRs and LLM outputs, enabling a more precise and comprehensive understanding of model confidence while improving classification accuracy.

In summary, we offer the following key contributions:

\begin{itemize}[itemsep=0pt]
    \item \textbf{Metamorphic Relation (MR) Rules} to generate diverse yet semantically equivalent textual variations, enabling robustness testing of LLM annotations.
    
    \item \textbf{Perceived Confidence Score (PCS) Framework} that integrates multiple outputs and variations to effectively assess LLM confidence in prediction.
    
    \item \textbf{Perceived Differential Evolution (PDE)} as a novel optimization technique designed to fine-tune the PCS framework, ensuring optimal parameter selection.
    
    \item \textbf{Comprehensive Evaluations} of PCS and PDE, demonstrating their capability to improve classification accuracy and reliability across multiple datasets, significantly surpassing traditional methods.
\end{itemize}


Empirical evaluations across three diverse datasets demonstrate that PCS significantly outperforms traditional methods like majority voting by improving classification accuracy. For example, in multiclass Sentiment Analysis, the Llama (Meta-Llama-3-8B-Instruct) model improves by 11.11\%, and the Gemma (gemma-2-9b-it) model by 1.69\%. In Gossipcop, the  Gemma (gemma-2-9b-it) model improves by 4.55\%.  In NewsMediaBias-Plus, the Llama (Meta-Llama-3-8B-Instruct) model improves by 9.88\%, the Mistral (Mistral-7B-Instruct-v0.3) model by 18.06\%, and the  Gemma (gemma-2-9b-it) model by 26.47\%. %In Politifact, PCS matches the best zero-shot performance with the Gemma (gemma-2-9b-it) model and shows an 8.86\% improvement.
