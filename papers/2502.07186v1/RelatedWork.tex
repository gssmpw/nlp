\section{Related Work}
\subsection{LLMs as Evaluators}
The role of Large Language Models (LLMs) as evaluators in diverse fields has garnered significant attention due to their complex evaluative capabilities. Research by \cite{laskar2024survey} and \cite{saxon2024framework} explores the development of robust evaluation frameworks, particularly underlining the challenges posed by tasks fraught with inherent ambiguity. \cite{jin2024gpt4v} specifically highlights these issues within the healthcare sector, identifying critical reasoning flaws in GPT-4V that underscore the necessity for more reliable evaluation techniques. Complementary studies by \cite{thakur2024judgingjudgesevaluatingalignment} and \cite{10.5555/3666122.3668142} explore the biases and decision-making processes of LLMs, suggesting high agreement rates with human judgments. This potential is further explored in the works of \cite{dong2024llmpersonalizedjudge} and \cite{liusie2024llmcomparativeassessmentzeroshot}, who investigate the effectiveness of LLMs in comparative versus traditional scoring methods.

\subsection{Using LLMs in Annotation Tasks}
Recent research has evaluated Large Language Models (LLMs) for various annotation tasks. \cite{bhat-varma-2023-large} found mixed performance when using LLMs to annotate underrepresented Indic languages. A review by \cite{pavlovic-poesio-2024-effectiveness} highlighted the efficiency and cost-effectiveness of LLMs in data labeling but noted issues like bias and prompt sensitivity. Efforts to improve LLMs include \cite{tamang2024evaluating}'s call for efficient use of LLM-generated data to train more adaptable models. Innovative methods such as the DAFND framework \cite{liu2024dafnd} and ``Prompt-and-Align" \cite{wu2023promptalign} are enhancing detection in low-resource settings through few-shot learning and social context analysis. Ethical challenges, particularly regarding bias and ethical model use, are addressed by \cite{fan2024subjective} and \cite{bender2021parrots}. Advances in fake news detection using few-shot knowledge distillation and consistency methods are shown by \cite{springer2023fskd} and \cite{nature2024consistency}, focusing on maintaining accuracy while reducing costs.

\subsection{Confidence Assessment for LLM based Classifications}
% \gias{summarize confidence assessment techqnieus for LLM based classification tasks}
Confidence assessment in LLMs represents a critical area of research addressing the reliability of AI-generated outputs. Current methodological approaches include prompt variation techniques, where identical questions are posed using different prompts to evaluate response consistency, self-confidence evaluation methods that directly query the model's uncertainty, and multi-choice assessment strategies that quantify confidence through structured selection. \cite{chen2023quantifying}. Innovative research by \cite{kim2024collaborative, kim2024collaborative} has highlighted the limitations of traditional confidence assessment methods like majority voting, demonstrating the need for more granular approaches that integrate human expertise with LLM outputs. While significant progress has been made in the confidence assessment in tasks such as summary generation and comparative analysis against human-generated references \cite{virk2024enhancing}, 
critical challenges remain, particularly in capturing nuanced moral judgments and handling complex scenarios requiring contextual rule interpretation. The evolving landscape of confidence assessment techniques underscores the ongoing need to develop sophisticated mechanisms that can effectively communicate and quantify model uncertainty across diverse computational tasks. \cite{when_to_make_exceptions_exploring_language_models_as_accounts_of_human__moral_judgment_2022}


\subsection{Metamorphic Testing for LLMs}
% \gias{summarize related work}
Metamorphic Testing (MT) is utilized to evaluate the performance of Large Language Models (LLMs). \cite{hyun2024metal} employs MT and Metamorphic Relations (MRs) to modify input texts for LLMs, assessing their efficacy in classification and generative tasks like text summarization. Similarly, \cite{xue2024exploring} applies MT to bug-fixing tasks in LLMs, generating various versions of buggy code to test the models' bug-fixing capabilities. In their research, \cite{li2024drowzee} uses MT to enhance the detection of Fact-Conflicting Hallucinations in LLMs, increasing dataset coverage that triggers such hallucinations. Additionally, MRs have been deployed to uncover bias in LLMs by altering attributes such as race and age, and examining the consistency of the models' responses in tasks like sentiment analysis.