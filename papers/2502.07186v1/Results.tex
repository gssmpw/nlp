\section{Results And Discussion}
\label{sec:results}

\begin{table}[t]
\centering%
\caption{Comparing PCS with single zero-shot LLM approach (L: Meta-Llama-3-8B-Instruct, M: Mistral-7B-Instruct-v0.3, and G: gemma-2-9b-it)}
% \vspace{-4mm}
% \smallskip
\label{tab:singleLLMs}
\vskip 0.15in

{
\input{tables/SingleDefault}
}
\vskip -0.1in
\end{table}

\begin{table}[t]
\centering%
\caption{Comparing PCS with Majority Voting approach (L: Meta-Llama-3-8B-Instruct, M: Mistral-7B-Instruct-v0.3, and G: gemma-2-9b-it)}
% \vspace{-4mm}
% \smallskip
\label{tab:MajorityVoting}
\vskip 0.15in

{
\input{tables/MultipleDefault}
}
\vskip -0.1in

\end{table}

\subsection{Single zero-shot LLM}
We assessed the performance of PCS in a scenario where a single LLM is tasked with annotating input text. In the baseline method, the LLM classifies the input text using a prompt (examples can be found in \ref{annotation_biased}, \ref{annotation_real}, and \ref{annotation_sentiment}). With PCS, multiple variations of the input are generated using the MRs outlined in Section \ref{MR}, and the LLM annotates each variant, including the original input and its MR-modified versions. These annotations are then weighted using optimized values (by PDE), and PCS aggregates them to produce a final label.

Table \ref{tab:singleLLMs} presents a summary of the results using the default LLM temperature settings provided in Table \ref{tab:singleLLMsTemperatures} from Section \ref{sec:singleLLMsTemperatures}. It highlights improved classification accuracies across four datasets—Multiclass Sentiment Analysis, Gossipcop, NewsMediaBias-Plus, and Politifact—using single LLMs (L for Llama: Meta-Llama-3-8B-Instruct, M for Mistral: Mistral-7B-Instruct-v0.3, and G for Gemma: gemma-2-9b-it) with zero-shot learning and the proposed PCS algorithm. 

PCS consistently outperforms zero-shot learning, delivering notable accuracy improvements across multiple datasets. For instance, in Multiclass Sentiment Analysis the L model's accuracy increased from 0.64 to 0.68, marking a 6.25\% improvement. Similarly, for Gossipcop, the M model's accuracy rose from 0.59 to 0.64, reflecting an 8.47\% gain. In NewsMediaBias-Plus, the G model achieved a substantial boost from 0.68 to 0.86, a 26.47\% increase, while in Politifact, the M model improved from 0.77 to 0.85, a 10.39\% enhancement. These results underscore PCS's effectiveness in improving classification reliability. Overall, PCS improves classification accuracy by an average of 4.96\% for Model L, 10.52\% for Model M, and 9.39\% for Model G. As shown in Table \ref{tab:cliffs} in Section \ref{sec:cliffs}, the p-value in paired t-test confirms significant accuracy improvements for Models L and M. Additionally, Cliff's delta effect size indicates a large value for Models G and M indicating that PCS had a substantial impact on improving classification accuracy.

\subsection{Multiple zero-shot LLMs}
We evaluated the performance of PCS in a scenario involving multiple LLMs for classification. Specifically, we tested all possible combinations of using three, two, and one LLM. The results, summarized in Table \ref{tab:MajorityVoting}, were obtained using the default temperature settings provided in Table \ref{tab:multipleLLMsTemperatures} from Section \ref{sec:singleLLMsTemperatures}.
The results show improved classification accuracies across four datasets—Multiclass Sentiment Analysis, Gossipcop, NewsMediaBias-Plus, and Politifact—when using multiple LLMs (L for Llama: Meta-Llama-3-8B-Instruct, M for Mistral: Mistral-7B-Instruct-v0.3, and G for Gemma: gemma-2-9b-it) with both majority voting (MV) and the proposed PCS. PCS consistently outperforms MV, delivering significant accuracy improvements across all datasets. For instance, in Multiclass Sentiment Analysis, the M+G model achieves a 15\% increase in accuracy (from 0.60 to 0.69), while in Gossipcop, the L+M+G model shows a 9.68\% improvement (from 0.62 to 0.68). Similarly, in NewsMediaBias-Plus, the L+M+G combination demonstrates a 12.5\% boost (from 0.80 to 0.90), and in Politifact, the L+M model improves by 3.45\% (from 0.87 to 0.90). 

As shown in Table \ref{tab:cliffs} in Section \ref{sec:cliffs}, the p-value in paired t-test indicates significant accuracy improvements for the ``L+M+G", ``L+M", and ``L+G" model combinations. Furthermore, Cliff's delta effect size reveals a large effect for the ``L+M" and ``L+G" combinations, while the ``M+G" and ``L+M+G" combinations show a medium effect size. On average, PCS enhances classification accuracy by 6.57\% when using two LLMs and by 7.75\% when using three LLMs. 

\begin{table}[t]
\centering%
\caption{The impact of the number of optimizing parameters in the annotation task}
% \vspace{-4mm}
% \smallskip
\label{tab:PDE}
\vskip 0.15in

{
\input{tables/PDEHPOS}
}
\vskip -0.1in
\end{table}

\subsection{Number of Optimization Parameters}When MR weights and LLM weights were not optimized, they were assigned a constant value of 1, ensuring equal importance for each MR and LLM. Likewise, thresholds were fixed at 0.5 in scenarios where only MR and LLM weights were optimized. 

Table \ref{tab:PDE} highlights the accuracy improvements achieved through the Perceived Differential Evolution (PDE) optimization, which integrates Metamorphic Relations (MRs), Large Language Models (LLMs), and thresholds. Compared to the baseline configuration without optimization (accuracies: 0.67 for Multiclass Sentiment Analysis, 0.63 for Gossipcop, 0.84 for NewsMediaBias-Plus, and 0.88 for Politifact), the optimized setup improves accuracy by 4.48\%, 7.94\%, 7.14\%, and 4.55\%, respectively. Consistently, optimizing all variables delivers the highest accuracy across all datasets.


\subsection{Dataset Size Impacts}
Figure \ref{fig:optimizing-datasetsize} illustrates the impact of dataset size on Hyperparameter Optimization (HPO) when utilizing the Perceived Differential Evolution (PDE) method. In Figure \ref{fig:optimizing-datasetsize} PDE optimizes all Metamorphic Relations (MRs) and Large Language Models (LLMs) and thresholds.

For the NewsMediaBias-Plus dataset, HPO achieves stability with a dataset size of 20. In the Gossipcop dataset, stability is reached at 125 data points. The Politifact dataset stabilizes at 95 data points, while the Multiclass Sentiment Analysis dataset attains stability at 130 data points.
\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{Images/Accuracies_datsetsSizes.png} 
  \caption{Effect of optimizing dataset size on annotation accuracy using the PCS}
    \label{fig:optimizing-datasetsize}
\end{figure}

\subsection{Impact of Each MR}
% \gias{show which MR was more influential than another for correct classification. This is based on the optimized weight}
the results in Table \ref{tab:MajorityVotingEachMR}
show that applying individual MRs (MR1, MR2, and MR3) might not consistently enhance performance. For example, in the Gossipcop dataset for LLM `M', using no MR yields an accuracy of 0.59, which is superior to using MR1 and MR2 but less than MR3. However, when the PCS algorithm combines all scenarios—namely, MR1, MR2, MR3, and No MR—the overall accuracy increases to 0.64, which surpasses the accuracy of any individual MR or No MR scenario.


Sometimes, majority voting leads to less accurate results than the best individual LLM. For instance, in the NewsMediaBias-Plus dataset, when no MRs were applied, the single LLM Llama3-8B-Instruct (L) achieved the highest accuracy of 0.83, exceeding the majority voting result of 0.80. This suggests that majority voting may reduce performance by incorporating labels from other LLMs (M and G). However, employing the PCS algorithm boosts accuracy to 0.90, outperforming all individual LLM results. %and effectively optimizing the use of available LLMs to enhance accuracy.

\subsection{Impact of Temperature}
Our experiments explored using LLMs to classify text inputs across specified categories. The temperature parameter, which controls randomness in LLM outputs, had minimal impact on the PCS results. As detailed in Section \ref{sec:singleLLMsTemperatures}, adjusting the temperature from 0.1 (low randomness) to 0.7 or 1.0 (high randomness) did not significantly affect PCS performance in single LLM configurations (Table \ref{tab:singleLLMsTemperatures}). Similarly, in multi-LLM settings (Table \ref{tab:multipleLLMsTemperatures}), varying temperature values caused at most a 2\% difference in accuracy. This maximum variation was observed in the L+M+G configuration for Multiclass Sentiment Analysis, where accuracy improved slightly from 0.70 with default temperatures to 0.72 when using a temperature of 0.2 across all LLMs.


%\subsection{Impact of Each MR}




\begin{table}[t]
\centering%
\caption{Comparing the performance of zero-shot LLMs across scenarios involving MR1, MR2, MR3, and No MR with the PCS, which utilizes all MRs.}
% \vspace{-4mm}
% \smallskip
\label{tab:MajorityVotingEachMR}
\vskip 0.15in

{
\input{tables/MRImpacts}
}
\vskip -0.1in

\end{table}
