@inproceedings{paul_pu_liang__2021,
        title={ Towards Understanding and Mitigating Social Biases in Language Models },
        author={ Paul Pu Liang and Chiyu Wu and Louis-Philippe Morency and Ruslan Salakhutdinov },
        
        year={ 2021 },
        publisher={ PMLR },
        
        pages={ 6565-6576 },

}
@INPROCEEDINGS{10336270,
  author={Duque-Torres, Alejandra and Pfahl, Dietmar},
  booktitle={2023 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={Towards a Complete Metamorphic Testing Pipeline}, 
  year={2023},
  volume={},
  number={},
  pages={606-610},
  keywords={Software maintenance;Automation;Pipelines;Testing;Domain specific languages;Metamorphic Testing;Metamorphic Relations;Automation;Regression Testing},
  doi={10.1109/ICSME58846.2023.00081}}

@article{comparative_analysis_of_decisionmaking_efficiency_of_large_language_models_2023,
        title={ Comparative Analysis of Decision-Making Efficiency of Large Language Models },
        
        journal={ International Journal For Multidisciplinary Research },
        year={ 2023 },
        
        volume={ 5 },
        
        number={ 3 },
        
        
        doi={ 10.36948/ijfmr.2023.v05i03.3342 },  
      }

@misc{when_to_make_exceptions_exploring_language_models_as_accounts_of_human__moral_judgment_2022,
        title={ When to Make Exceptions: Exploring Language Models as Accounts of Human
  Moral Judgment },
        
        
        year={ 2022 },
        publisher={ arXiv (Cornell University) },
        
        
        
        
        
        doi={ 10.48550/arxiv.2210.01478 },  
      }

@unknown{Chen,
author = {Chen, T. and Cheung, Shing-Chi and Yiu, Sm},
year = {2020},
month = {02},
pages = {},
title = {Metamorphic Testing: A New Approach for Generating Next Test Cases}
}

@article{helge_spieker__2024,
        title={ Evaluating Human Trajectory Prediction with Metamorphic Testing },
        author={ Helge Spieker and Nassim Belmecheri and Arnaud Gotlieb and Nadjib Lazaar },
        
        year={ 2024 },
        
        doi={ 10.1145/3679006.3685071 },  
      }

@misc{_anonymous_2023,
        title={ Mutation-guided Metamorphic Testing of Optimality in AI Planning },
        author={ ­ Anonymous },
        
        year={ 2023 },
        publisher={ CERN European Organization for Nuclear Research },
        doi={ 10.5281/zenodo.7615241 },  
      }

@misc{thakur2024judgingjudgesevaluatingalignment,
      title={Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges}, 
      author={Aman Singh Thakur and Kartik Choudhary and Venkat Srinik Ramayapally and Sankaran Vaidyanathan and Dieuwke Hupkes},
      year={2024},
      eprint={2406.12624},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.12624}, 
}

@misc{liusie2024llmcomparativeassessmentzeroshot,
      title={LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models}, 
      author={Adian Liusie and Potsawee Manakul and Mark J. F. Gales},
      year={2024},
      eprint={2307.07889},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.07889}, 
}

@unknown{detectInvestigate,
author = {Liu, Ye and Zhu, Jiajun and Zhang, Kai and Tang, Haoyu and Zhang, Yanghai and Liu, Xukai and Liu, Qi and Chen, Enhong},
year = {2024},
month = {07},
pages = {},
title = {Detect, Investigate, Judge and Determine: A Novel LLM-based Framework for Few-shot Fake News Detection},
doi = {10.48550/arXiv.2407.08952}
}


@article{
doi:10.1073/pnas.2305016120,
author = {Fabrizio Gilardi  and Meysam Alizadeh  and Maël Kubli },
title = {ChatGPT outperforms crowd workers for text-annotation tasks},
journal = {Proceedings of the National Academy of Sciences},
volume = {120},
number = {30},
pages = {e2305016120},
year = {2023},
doi = {10.1073/pnas.2305016120},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2305016120},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2305016120},
abstract = {Many NLP applications require manual text annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using four samples of tweets and news articles (n = 6,183), we show that ChatGPT outperforms crowd workers for several annotation tasks, including relevance, stance, topics, and frame detection. Across the four datasets, the zero-shot accuracy of ChatGPT exceeds that of crowd workers by about 25 percentage points on average, while ChatGPT’s intercoder agreement exceeds that of both crowd workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than \$0.003—about thirty times cheaper than MTurk. These results demonstrate the potential of large language models to drastically increase the efficiency of text classification.}}

@inproceedings{
anonymous2024humans,
title={Humans or {LLM}s as the Judge? A Study on Judgement Bias},
author={Anonymous},
booktitle={Submitted to ACL Rolling Review - April 2024},
year={2024},
url={https://openreview.net/forum?id=7YlEPNFSp3},
note={under review}
}


@misc{huang2024limitationsfinetunedjudgemodels,
      title={On the Limitations of Fine-tuned Judge Models for LLM Evaluation}, 
      author={Hui Huang and Yingqi Qu and Hongli Zhou and Jing Liu and Muyun Yang and Bing Xu and Tiejun Zhao},
      year={2024},
      eprint={2403.02839},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.02839}, 
}

@inproceedings{kim-etal-2024-meganno,
    title = "{MEGA}nno+: A Human-{LLM} Collaborative Annotation System",
    author = "Kim, Hannah  and
      Mitra, Kushan  and
      Li Chen, Rafael  and
      Rahman, Sajjadur  and
      Zhang, Dan",
    editor = "Aletras, Nikolaos  and
      De Clercq, Orphee",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
    month = mar,
    year = "2024",
    address = "St. Julians, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-demo.18",
    pages = "168--176",
    abstract = "Large language models (LLMs) can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, LLMs may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans.",
}

@inproceedings{wang-etal-2021-want-reduce,
    title = "Want To Reduce Labeling Cost? {GPT}-3 Can Help",
    author = "Wang, Shuohang  and
      Liu, Yang  and
      Xu, Yichong  and
      Zhu, Chenguang  and
      Zeng, Michael",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.354",
    doi = "10.18653/v1/2021.findings-emnlp.354",
    pages = "4195--4205",
    abstract = "Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-specific and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 170 billion parameters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We find that to make the downstream model achieve the same performance on a variety of NLU and NLG tasks, it costs 50{\%} to 96{\%} less to use labels from GPT-3 than using labels from humans. Furthermore, we propose a novel framework of combining pseudo labels from GPT-3 with human labels, which leads to even better performance. These results present a cost-effective data labeling methodology that is generalizable to many practical applications.",
}

@misc{ding2023gpt3gooddataannotator,
      title={Is GPT-3 a Good Data Annotator?}, 
      author={Bosheng Ding and Chengwei Qin and Linlin Liu and Yew Ken Chia and Shafiq Joty and Boyang Li and Lidong Bing},
      year={2023},
      eprint={2212.10450},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.10450}, 
}

@inproceedings{10.5555/3666122.3668142,
author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
title = {Judging LLM-as-a-judge with MT-bench and Chatbot Arena},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {2020},
numpages = {29},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@misc{lin2024investigatingbiasllmbasedbias,
      title={Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception}, 
      author={Luyang Lin and Lingzhi Wang and Jinsong Guo and Kam-Fai Wong},
      year={2024},
      eprint={2403.14896},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2403.14896}, 
}

@misc{kim2024prometheus2opensource,
      title={Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models}, 
      author={Seungone Kim and Juyoung Suk and Shayne Longpre and Bill Yuchen Lin and Jamin Shin and Sean Welleck and Graham Neubig and Moontae Lee and Kyungjae Lee and Minjoon Seo},
      year={2024},
      eprint={2405.01535},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.01535}, 
}

@misc{fan2024biasalertplugandplaytoolsocial,
      title={BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs}, 
      author={Zhiting Fan and Ruizhe Chen and Ruiling Xu and Zuozhu Liu},
      year={2024},
      eprint={2407.10241},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10241}, 
}

@misc{dong2024llmpersonalizedjudge,
      title={Can LLM be a Personalized Judge?}, 
      author={Yijiang River Dong and Tiancheng Hu and Nigel Collier},
      year={2024},
      eprint={2406.11657},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11657}, 
}

@misc{wang2024selftaughtevaluators,
      title={Self-Taught Evaluators}, 
      author={Tianlu Wang and Ilia Kulikov and Olga Golovneva and Ping Yu and Weizhe Yuan and Jane Dwivedi-Yu and Richard Yuanzhe Pang and Maryam Fazel-Zarandi and Jason Weston and Xian Li},
      year={2024},
      eprint={2408.02666},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.02666}, 
}

@misc{lin2023llmevalunifiedmultidimensionalautomatic,
      title={LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models}, 
      author={Yen-Ting Lin and Yun-Nung Chen},
      year={2023},
      eprint={2305.13711},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13711}, 
}

@misc{vector_newsmediabias_plus,
  author       = {Vector Institute},
  title        = {newsmediabias-plus},
  howpublished = {\url{https://huggingface.co/datasets/vector-institute/newsmediabias-plus}},
  note         = {Accessed: 2024-11-12},
  year         = {2024}
}

@article{shu2018fakenewsnet,
  title={FakeNewsNet: A Data Repository with News Content, Social Context and Dynamic Information for Studying Fake News on Social Media},
  author={Shu, Kai and  Mahudeswaran, Deepak and Wang, Suhang and Lee, Dongwon and Liu, Huan},
  journal={arXiv preprint arXiv:1809.01286},
  year={2018}
}
@article{shu2017fake,
  title={Fake News Detection on Social Media: A Data Mining Perspective},
  author={Shu, Kai and Sliva, Amy and Wang, Suhang and Tang, Jiliang and Liu, Huan},
  journal={ACM SIGKDD Explorations Newsletter},
  volume={19},
  number={1},
  pages={22--36},
  year={2017},
  publisher={ACM}
}
@article{shu2017exploiting,
  title={Exploiting Tri-Relationship for Fake News Detection},
  author={Shu, Kai and Wang, Suhang and Liu, Huan},
  journal={arXiv preprint arXiv:1712.07709},
  year={2017}
}

@article{laskar2024survey,
  title={A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations},
  author={Laskar, Md Tahmid Rahman and Alqahtani, Sawsan},
  journal={ACL},
  year={2024},
  url={https://aclanthology.org/2024.emnlp-main.764/}
}


@article{saxon2024framework,
  title={A Framework for Evaluating LLMs Under Task Indeterminacy},
  author={Saxon, Michael and Perez, Ethan},
  journal={ArXiv},
  year={2024},
  url={https://arxiv.org/abs/2411.13760}
}

@article{jin2024gpt4v,
  title={Hidden Flaws Behind Expert-Level Accuracy of Multimodal GPT-4 Vision in Medicine},
  author={Jin, Qiao and Li, Yijia},
  journal={ArXiv},
  year={2024},
  url={https://arxiv.org/abs/2401.08396}
}

@article{fan2024subjective,
  title={Evaluating Generative Language Models in Information Extraction as Subjective Question Correction},
  author={Fan, Yuchen and Liu, Yantao},
  journal={ACL},
  year={2024},
  url={https://aclanthology.org/2024.lrec-main.567/}
}

@article{rahman2024survey,
  title={A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations},
  author={Laskar, Md Tahmid Rahman and Alqahtani, Sawsan},
  journal={ACL},
  year={2024},
  url={https://aclanthology.org/2024.emnlp-main.764/}
}

@article{chen2023ensemble,
  title={Ensemble Methods for Context-Aware Model Aggregation},
  author={Chen, Xiang and Rao, Deepak},
  journal={Transactions on Machine Learning},
  year={2023},
  volume={9},
  number={4},
  pages={201--212}
}

@inproceedings{bender2021parrots,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Margaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT)},
  year={2021},
  pages={610--623},
  publisher={ACM},
  doi={10.1145/3442188.3445922}
}

@article{liu2024dafnd,
  title={Detect, Investigate, Judge and Determine: A Novel LLM-based Framework for Few-shot Fake News Detection},
  author={Liu, Ye and Zhu, Jiajun},
  journal={ArXiv},
  year={2024},
  url={https://arxiv.org/abs/2407.08952}
}

@article{wu2023promptalign,
  title={Prompt-and-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection},
  author={Wu, Jiaying and Li, Shen},
  journal={ArXiv},
  year={2023},
  url={https://arxiv.org/abs/2309.16424}
}

@article{springer2023fskd,
  title={FSKD: Detecting Fake News with Few-Shot Knowledge Distillation},
  author={X. Zhang, D. Wu},
  booktitle={Springer AI Conference},
  year={2023},
  url={https://link.springer.com/chapter/10.1007/978-3-031-46677-9_29}
}

@article{nature2024consistency,
  title={A Veracity Dissemination Consistency-Based Few-Shot Fake News Detection Method},
  author={Zhou, X. and Lee, K.},
  journal={Nature},
  year={2024},
  url={https://www.nature.com/articles/s41598-024-70039-9.pdf}
}

@article{kim2024collaborative,
  title={Collaborative Annotation with LLMs and Humans},
  author={Kim, Hana and Lee, Minji},
  journal={Journal of Applied AI},
  year={2024},
  volume={15},
  number={1},
  pages={45--58}
}


@article{chen2023ensemble,
  title={Ensemble Methods for Context-Aware Model Aggregation},
  author={Chen, Xiang and Rao, Deepak},
  journal={Transactions on Machine Learning},
  year={2023},
  volume={9},
  number={4},
  pages={201--212}
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B. and others},
  journal={Advances in Neural Information Processing Systems},
  year={2020},
  volume={33},
  pages={1877--1901}
}

@article{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={Proceedings of NAACL-HLT},
  year={2019},
  pages={4171--4186}
}

@article{zhao2020assessing,
  title={Assessing the Reliability of Natural Language Processing Models},
  author={Zhao, Julie and others},
  journal={Nature Reviews Physics},
  year={2020},
  volume={2},
  pages={157--158}
}

@article{castelvecchi2021can,
  title={Can We Open the Black Box of AI?},
  author={Castelvecchi, Davide},
  journal={Nature},
  year={2021},
  volume={589},
  pages={189--191}
}

@article{jobin2019global,
  title={The Global Landscape of AI Ethics Guidelines},
  author={Jobin, Anna and Ienca, Marcello and Vayena, Effy},
  journal={Nature Machine Intelligence},
  year={2019},
  volume={1},
  pages={389--399}
}

@inproceedings{gebru2021datasheets,
  title={Datasheets for Datasets},
  author={Gebru, Timnit and others},
  booktitle={Communications of the ACM},
  year={2021},
  volume={64},
  number={12},
  pages={86--92}
}

@inproceedings{wang2019superglue,
  title={SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  author={Wang, Alex and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019},
  volume={32},
  pages={3261--3275}
}

@article{chen2021metamorphic,
  title={Metamorphic Testing: A New Approach for Generating Next Test Cases},
  author={Chen, Tsong Yueh and others},
  journal={HKIE Transactions},
  year={2021},
  volume={28},
  number={4},
  pages={252--262}
}

@article{storn1997differential,
  title={Differential Evolution – A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces},
  author={Storn, Rainer and Price, Kenneth},
  journal={Journal of Global Optimization},
  year={1997},
  volume={11},
  number={4},
  pages={341--359}
}

@article{hendrycks2021unsolved,
  title={Unsolved Problems in ML Safety},
  author={Hendrycks, Dan and others},
  journal={ArXiv},
  year={2021},
  eprint={2109.13916},
  archivePrefix={"arXiv"},
  primaryClass={"cs.LG"}
}


@inproceedings{zhang-etal-2024-sentiment,
    title = "Sentiment Analysis in the Era of Large Language Models: A Reality Check",
    author = "Zhang, Wenxuan  and
      Deng, Yue  and
      Liu, Bing  and
      Pan, Sinno  and
      Bing, Lidong",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.246",
    doi = "10.18653/v1/2024.findings-naacl.246",
    pages = "3881--3906",
    abstract = "Sentiment analysis (SA) has been a long-standing research area in natural language processing. With the recent advent of large language models (LLMs), there is great potential for their employment on SA problems. However, the extent to which current LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring a deeper understanding of specific sentiment phenomena or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs{'} SA abilities and propose a novel benchmark, SentiEval, for a more comprehensive and realistic evaluation. Data and code are available at \url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}.",
}

@article{golob2024fact,
  title={Fact Manipulation in News: LLM-Driven Synthesis and Evaluation of Fake News Annotation},
  author={Golob, Luka and Sittar, Abdul},
  year={2024}
}

@article{kumar2024decoding,
  title={Decoding biases: Automated methods and llm judges for gender bias detection in language models},
  author={Kumar, Shachi H and Sahay, Saurav and Mazumder, Sahisnu and Okur, Eda and Manuvinakurike, Ramesh and Beckage, Nicole and Su, Hsuan and Lee, Hung-yi and Nachman, Lama},
  journal={arXiv preprint arXiv:2408.03907},
  year={2024}
}

@article{Gligoric2024CanUL,
  title={Can Unconfident LLM Annotations Be Used for Confident Conclusions?},
  author={Kristina Gligori'c and Tijana Zrnic and Cinoo Lee and Emmanuel J. Candes and Dan Jurafsky},
  journal={ArXiv},
  year={2024},
  volume={abs/2408.15204},
  url={https://api.semanticscholar.org/CorpusID:271962879}
}

@article{liu2024examining,
  title={Examining llms’ uncertainty expression towards questions outside parametric knowledge},
  author={Liu, Genglin and Wang, Xingyao and Yuan, Lifan and Chen, Yangyi and Peng, Hao},
  journal={Preprint},
  year={2024}
}

@article{becker2024cycles,
  title={Cycles of Thought: Measuring LLM Confidence through Stable Explanations},
  author={Becker, Evan and Soatto, Stefano},
  journal={arXiv preprint arXiv:2406.03441},
  year={2024}
}

@article{gligoric2024can,
  title={Can Unconfident LLM Annotations Be Used for Confident Conclusions?},
  author={Gligori{\'c}, Kristina and Zrnic, Tijana and Lee, Cinoo and Cand{\`e}s, Emmanuel J and Jurafsky, Dan},
  journal={arXiv preprint arXiv:2408.15204},
  year={2024}
}

@inproceedings{geng-etal-2024-survey,
    title = "A Survey of Confidence Estimation and Calibration in Large Language Models",
    author = "Geng, Jiahui  and
      Cai, Fengyu  and
      Wang, Yuxia  and
      Koeppl, Heinz  and
      Nakov, Preslav  and
      Gurevych, Iryna",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.366",
    doi = "10.18653/v1/2024.naacl-long.366",
    pages = "6577--6595",
    abstract = "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks in various domains. Despite their impressive performance, they can be unreliable due to factual errors in their generations. Assessing their confidence and calibrating them across different tasks can help mitigate risks and enable LLMs to produce better generations. There has been a lot of recent research aiming to address this, but there has been no comprehensive overview to organize it and to outline the main lessons learned. The present survey aims to bridge this gap. In particular, we outline the challenges and we summarize recent technical advancements for LLM confidence estimation and calibration. We further discuss their applications and suggest promising directions for future work.",
}

@article{luo2024understanding,
  title={From understanding to utilization: A survey on explainability for large language models},
  author={Luo, Haoyan and Specia, Lucia},
  journal={arXiv preprint arXiv:2401.12874},
  year={2024}
}

@article{pedapati2024large,
  title={Large Language Model Confidence Estimation via Black-Box Access},
  author={Pedapati, Tejaswini and Dhurandhar, Amit and Ghosh, Soumya and Dan, Soham and Sattigeri, Prasanna},
  journal={arXiv preprint arXiv:2406.04370},
  year={2024}
}

@inproceedings{bhat-varma-2023-large,
    title = "Large Language Models As Annotators: A Preliminary Evaluation For Annotating Low-Resource Language Content",
    author = "Bhat, Savita  and
      Varma, Vasudeva",
    editor = {Deutsch, Daniel  and
      Dror, Rotem  and
      Eger, Steffen  and
      Gao, Yang  and
      Leiter, Christoph  and
      Opitz, Juri  and
      R{\"u}ckl{\'e}, Andreas},
    booktitle = "Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems",
    month = nov,
    year = "2023",
    address = "Bali, Indonesia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eval4nlp-1.8",
    doi = "10.18653/v1/2023.eval4nlp-1.8",
    pages = "100--107",
    abstract = "The process of collecting human-generated annotations is time-consuming and resource-hungry. In the case of low-resource (LR) languages such as Indic languages, these efforts are more expensive due to the dearth of data and human experts. Considering their importance in solving downstream applications, there have been concentrated efforts exploring alternatives for human-generated annotations. To that extent, we seek to evaluate multilingual large language models (LLMs) for their potential to substitute or aid human-generated annotation efforts. We use LLMs to re-label publicly available datasets in LR languages for the tasks of natural language inference, sentiment analysis, and news classification. We compare these annotations with existing ground truth labels to analyze the efficacy of using LLMs for annotation tasks. We observe that the performance of these LLMs varies substantially across different tasks and languages. The results show that off-the-shelf use of multilingual LLMs is not appropriate and results in poor performance in two of the three tasks.",
}


@inproceedings{pavlovic-poesio-2024-effectiveness,
    title = "The Effectiveness of {LLM}s as Annotators: A Comparative Overview and Empirical Analysis of Direct Representation",
    author = "Pavlovic, Maja  and
      Poesio, Massimo",
    editor = "Abercrombie, Gavin  and
      Basile, Valerio  and
      Bernadi, Davide  and
      Dudy, Shiran  and
      Frenda, Simona  and
      Havens, Lucy  and
      Tonelli, Sara",
    booktitle = "Proceedings of the 3rd Workshop on Perspectivist Approaches to NLP (NLPerspectives) @ LREC-COLING 2024",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.nlperspectives-1.11",
    pages = "100--110",
    abstract = "Recent studies focus on exploring the capability of Large Language Models (LLMs) for data annotation. Our work, firstly, offers a comparative overview of twelve such studies that investigate labelling with LLMs, particularly focusing on classification tasks. Secondly, we present an empirical analysis that examines the degree of alignment between the opinion distributions returned by GPT and those provided by human annotators across four subjective datasets. Our analysis supports a minority of studies that are considering diverse perspectives when evaluating data annotation tasks and highlights the need for further research in this direction.",
}

@article{tamang2024evaluating,
  title={Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages},
  author={Tamang, Sagar and Bora, Dibya Jyoti},
  journal={arXiv preprint arXiv:2411.12240},
  year={2024}
}

@misc{hf_multiclass_sentiment,
  author       = {Sp1786},
  title        = {Multiclass Sentiment Analysis Dataset},
  howpublished = {Hugging Face Dataset},
  year         = {2023},
  url          = {https://huggingface.co/datasets/Sp1786/multiclass-sentiment-analysis-dataset}
}

@article{chen2023quantifying,
  title={Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment},
  author={Chen, Jiuhai and Mueller, Jonas},
  journal={arXiv preprint arXiv:2308.16175},
  year={2023}
}

@article{virk2024enhancing,
  title={Enhancing Trust in LLM-Generated Code Summaries with Calibrated Confidence Scores},
  author={Virk, Yuvraj and Devanbu, Premkumar and Ahmed, Toufique},
  journal={arXiv preprint arXiv:2404.19318},
  year={2024}
}

@inproceedings{hyun2024metal,
  title={METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities},
  author={Hyun, Sangwon and Guo, Mingyu and Babar, M Ali},
  booktitle={2024 IEEE Conference on Software Testing, Verification and Validation (ICST)},
  pages={117--128},
  year={2024},
  organization={IEEE}
}

@article{xue2024exploring,
  title={Exploring and Lifting the Robustness of LLM-powered Automated Program Repair with Metamorphic Testing},
  author={Xue, Pengyu and Wu, Linhao and Yang, Zhen and Li, Xinyi and Yu, Zhongxing and Jin, Zhi and Li, Ge and Xiao, Yan and Wu, Jingwen},
  journal={arXiv preprint arXiv:2410.07516},
  year={2024}
}

@article{li2024drowzee,
  title={Drowzee: Metamorphic testing for fact-conflicting hallucination detection in large language models},
  author={Li, Ningke and Li, Yuekang and Liu, Yi and Shi, Ling and Wang, Kailong and Wang, Haoyu},
  journal={Proceedings of the ACM on Programming Languages},
  volume={8},
  number={OOPSLA2},
  pages={1843--1872},
  year={2024},
  publisher={ACM New York, NY, USA}
}


@Inbook{Price2013,
author="Price, Kenneth V.",
editor="Zelinka, Ivan
and Sn{\'a}{\v{s}}el, V{\'a}clav
and Abraham, Ajith",
title="Differential Evolution",
bookTitle="Handbook of Optimization: From Classical to Modern Approach",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="187--214",
abstract="After an introduction that includes a discussion of the classic random walk, this paper presents a step-by-step development of the differential evolution (DE) global numerical optimization algorithm. Five fundamental DE strategies, each more complex than the last, are evaluated based on their conformance to invariance and symmetry principles, degree of control parameter dependence, computational efficiency and response to randomization. Optimal control parameter settings for the family of convex, quadratic functions are empirically derived.",
isbn="978-3-642-30504-7",
doi="10.1007/978-3-642-30504-7_8",
url="https://doi.org/10.1007/978-3-642-30504-7_8"
}

