\section{Experiments}

We conducted experiments to evaluate the PCS across three classification tasks: biased/unbiased article detection, real/fake news classification, and multiclass positive/negative/neutral sentiment analysis. These experiments were designed to compare the performance of PCS with two baselines: single-model predictions and multi-model (majority voting) predictions. 
% The results demonstrate PCS's ability to outperform traditional methods, showcasing its robustness and effectiveness in annotation tasks. The primary objective of these experiments was to validate the effectiveness of PCS in various classification tasks, as detailed in Section \ref{sec:results}.

\subsection{Datasets}

We utilized three datasets for binary classification tasks and one dataset for ternary classification (details in \ref{datasets}):

\noindent\textbf{NewsMediaBias-Plus Dataset \cite{vector_newsmediabias_plus}}  
consists of 385 news articles from the Vector Institute. Since it lacks biased/unbiased labels, we manually labeled each article following the guidelines in the prompt (See \ref{annotation_biased}). Two annotators independently labeled all articles, achieving a Cohenâ€™s Kappa of 0.81, indicating almost perfect agreement.

\noindent\textbf{FakeNewsNet-Gossipcop \cite{shu2018fakenewsnet, shu2017fake, shu2017exploiting}}  
is a sample of 393 entries, which includes predefined real and fake labels that were manually verified for accuracy.

\noindent\textbf{FakeNewsNet-Politifact \cite{shu2018fakenewsnet, shu2017fake, shu2017exploiting}} comprises 135 samples with predefined real and fake labels that were manually reviewed for correctness.

\noindent\textbf{Multiclass-Sentiment-Analysis \cite{hf_multiclass_sentiment} Dataset}  is used for ternary sentiment classification, comprises 405 samples with predefined positive, negative, and neutral labels that were manually reviewed for correctness.

\subsection{Evaluation Metrics}  
We compared PCS against two settings:  
(i). Single: Predictions based on single zero-shot LLMs. (ii). Multi (Majority Voting) setting: Aggregating outputs by selecting the most frequent label by multiple LLMs. We compute accuracy (= $\frac{correct}{correct+wrong}$) to measure performance .


\subsection{Models}
For our experiments, we utilized three Large Language Models: Llama (Meta-Llama-3-8B-Instruct), Mistral (Mistral-7B-Instruct-v0.3), and Gemma (gemma-2-9b-it). In addition to evaluating their performance individually, we also explored their combined use, leveraging multiple LLMs together to assess the effectiveness of the Perceived Confidence Score (PCS) framework in a multi-LLM setting. 

\subsection{Setup and Hyperparameters}
We utilized Metamorphic Relations (MRs) as outlined in Section \ref{MR} and used the Llama3.1-8B-Instruct model to generate input variations for robust testing (\ref{MR1_prompt_altering}, \ref{MR2_prompt_altering}, \ref{MR3_prompt_altering}). All models were implemented using the Hugging Face Transformers library and executed on an NVIDIA V100 GPU for efficient computation. The detailed configurations are provided in \ref{hyperparameters}.