\section{Related work}
\textbf{Feature Embeddings} Embedding features is usually approached depending on the type of feature. Numerical features have traditionally been either transformed using a linear model **Bengio et al., "Learning Deep Architectures for AI"** or discretised by a variation of binning techniques **Kohavi & John, "Wrappers for Feature Subset Selection"**. Binning as a technique for obtaining a pre-trained tabular deep learning model has shown promising results **Liu et al., "TabNet: An Interpretability Method for Deep Learning Models"**. Categorical features are typically embedded via a lookup embedding layer **Mikolov et al., "Distributed Representations of Words and Phrases"**, with various techniques influencing their use **Ruder, "An Overview of Gradient-Based Meta-Learning Methods"**. Recently, tree-based feature embeddings have shown promising potential **Kerrigan & Mac Namee, "Deep Learning for Tabular Data: A Comparative Study"**. Parallel to our work, **Cui et al., "Learning Compact and Interpretable Embeddings with Autoencoders"** explored using word embeddings for embedding only feature names and categorical inputs, though they disentangled and embedded each word separately. To our knowledge, we are the first to explore LLM-based embeddings, both encoder- and decoder-based, for embedding of tabular data.


\textbf{LLMs and Tabular Data} Learning on serialised tabular data to text has been prominent in mining relational tabular data **Xu et al., "Graph-Based Neural Networks for Table Understanding"**. With the introduction of pre-trained models, a plethora of tabular models for table understanding and downstream prediction have been proposed, as shown in the recent survey **Wang et al., "A Survey on Pre-Trained Models for Tabular Data"**. The majority of these applications focus on serialising the inputs and fine-tuning large language models for prediction **Li et al., "TabFusion: A Framework for Tabular Fusion with LLMs"**. Another line of work focuses on using LLMs as classifiers, where inputs are tokenised and mapped to the LLM vocabulary **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. The idea of leveraging the potential of LLMs for transfer-learning across tables and feature encoding was shown as useful in **Li et al., "Tab2Vec: Table Embeddings using Pre-Trained Language Models"**, however they propose different encoding for different input types adding a complexity by design. Recently, focus to incorporate LLM priors by prompting them to order the input features and this was exploited by traditional ML models **Bao et al., "Prompt Engineering for Tabular Data with LLMs"** showing promising results. LLMs showed remarkable potential as feature engineers as well **Huang et al., "LLM-Based Feature Engineering for Tabular Data"**. However, using LLMs in this manner is either computationally heavy e.g. fine-tuning or requires careful prompt creation which can be laborious.

\textbf{LLMs and Text Embeddings} Semantically embedding texts is one of the main tasks of interest in NLP, resulting in a benchmarking effort called the Massive Text Embedding Benchmark (MTEB) **Speer et al., "MTEB: A Massively Multilingual Dataset for Evaluating Text Embeddings"**. Traditionally, encoder-only model variants like sentence-based BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** were popular, with variants like BGE performing among the top performers **Liu et al., "BGE: A Benchmark for Evaluating Text Embeddings"**. Recently, focus has shifted towards extracting embeddings from LLMs due to their remarkable capabilities **Brown et al., "LLaMa3: An Open-Source Pre-Trained Language Model"**, where internal model embeddings, e.g., from the LLaMa3 model **Lample & Conneau, "Cross-Lingual Language Models"** are extracted with **Rae et al., "Compressive Transformers for Efficient Neural Machine Translation"** or without tuning **Krause & Daxenberger, "Unsupervised Text Embeddings using Large Language Models"** for document representation. Notably, the tokenisation process of LLMs, based on information-theoretic principles with byte-pair encoding techniques prevailing **Sennrich et al., "Neural Machine Translation of Rare Words with Subword Units"**, has been shown to negatively impact numerical reasoning tasks **Joulin et al., "Efficient Natural Language Generation Using Pre-Trained Language Models and Byte-Pair Encoding"**.