\section{Related work}
\textbf{Feature Embeddings} Embedding features is usually approached depending on the type of feature. Numerical features have traditionally been either transformed using a linear model ____ or discretised by a variation of binning techniques ____. Binning as a technique for obtaining a pre-trained tabular deep learning model has shown promising results ____. Categorical features are typically embedded via a lookup embedding layer ____, with various techniques influencing their use ____. Recently, tree-based feature embeddings have shown promising potential ____. Parallel to our work, ____ explored using word embeddings for embedding only feature names and categorical inputs, though they disentangled and embedded each word separately. To our knowledge, we are the first to explore LLM-based embeddings, both encoder- and decoder-based, for embedding of tabular data.


\textbf{LLMs and Tabular Data} Learning on serialised tabular data to text has been prominent in mining relational tabular data ____. With the introduction of pre-trained models, a plethora of tabular models for table understanding and downstream prediction have been proposed, as shown in the recent survey ____. The majority of these applications focus on serialising the inputs and fine-tuning large language models for prediction ____. Another line of work focuses on using LLMs as classifiers, where inputs are tokenised and mapped to the LLM vocabulary ____. The idea of leveraging the potential of LLMs for transfer-learning across tables and feature encoding was shown as useful in ____, however they propose different encoding for different input types adding a complexity by design. Recently, focus to incorporate LLM priors by prompting them to order the input features and this was exploited by traditional ML models ____ showing promising results. LLMs showed remarkable potential as feature engineers as well ____. However, using LLMs in this manner is either computationally heavy e.g. fine-tuning or requires careful prompt creation which can be laborious. 

\textbf{LLMs and Text Embeddings} Semantically embedding texts is one of the main tasks of interest in NLP, resulting in a benchmarking effort called the Massive Text Embedding Benchmark (MTEB) ____. Traditionally, encoder-only model variants like sentence-based BERT ____ were popular, with variants like BGE performing among the top performers ____. Recently, focus has shifted towards extracting embeddings from LLMs due to their remarkable capabilities ____, where internal model embeddings, e.g., from the LLaMa3 model ____, are extracted with ____ or without tuning ____ for document representation. Notably, the tokenisation process of LLMs, based on information-theoretic principles with byte-pair encoding techniques prevailing____, has been shown to negatively impact numerical reasoning tasks____.