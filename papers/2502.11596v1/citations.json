[
  {
    "index": 0,
    "papers": [
      {
        "key": "hollmann2022tabpfn",
        "author": "Noah Hollmann and\nSamuel M{\\\"{u}}ller and\nKatharina Eggensperger and\nFrank Hutter",
        "title": "TabPFN: {A} Transformer That Solves Small Tabular Classification Problems\nin a Second"
      },
      {
        "key": "fttransformer",
        "author": "Yury Gorishniy and\nIvan Rubachev and\nValentin Khrulkov and\nArtem Babenko",
        "title": "Revisiting Deep Learning Models for Tabular Data"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "tpberta",
        "author": "Yan, Jiahuan and Zheng, Bo and Xu, Hongxia and Zhu, Yiheng and Chen, Danny and Sun, Jimeng and Wu, Jian and Chen, Jintai",
        "title": "Making Pre-trained Language Models Great on Tabular Prediction"
      },
      {
        "key": "huang2020tabtransformer",
        "author": "Huang, Xin and Khetan, Ashish and Cvitkovic, Milan and Karnin, Zohar",
        "title": "Tabtransformer: Tabular data modeling using contextual embeddings"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "lee2023binning",
        "author": "Kyungeun Lee and Ye Seul Sim and Hyeseung Cho and Suhee Yoon and Sanghyu Yoon and Woohyung Lim",
        "title": "Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "fttransformer",
        "author": "Yury Gorishniy and\nIvan Rubachev and\nValentin Khrulkov and\nArtem Babenko",
        "title": "Revisiting Deep Learning Models for Tabular Data"
      },
      {
        "key": "huang2020tabtransformer",
        "author": "Huang, Xin and Khetan, Ashish and Cvitkovic, Milan and Karnin, Zohar",
        "title": "Tabtransformer: Tabular data modeling using contextual embeddings"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "borisov2110deep",
        "author": "Borisov, V and Leemann, T and Se{\\ss}ler, K and Haug, J and Pawelczyk, M and Kasneci, G",
        "title": "Deep neural networks and tabular data: A survey. arXiv 2021"
      },
      {
        "key": "gorishniy2022embeddings",
        "author": "Yury Gorishniy and\nIvan Rubachev and\nArtem Babenko",
        "title": "On Embeddings for Numerical Features in Tabular Deep Learning"
      },
      {
        "key": "wu2024deep",
        "author": "Wu, Yuqian and Luo, Hengyi and Lee, Raymond ST",
        "title": "Deep Feature Embedding for Tabular Data"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "tpberta",
        "author": "Yan, Jiahuan and Zheng, Bo and Xu, Hongxia and Zhu, Yiheng and Chen, Danny and Sun, Jimeng and Wu, Jian and Chen, Jintai",
        "title": "Making Pre-trained Language Models Great on Tabular Prediction"
      },
      {
        "key": "li2023treeregularized",
        "author": "Xuan Li and Yun Wang and Bo Li",
        "title": "Tree-Regularized Tabular Embeddings"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "tpberta",
        "author": "Yan, Jiahuan and Zheng, Bo and Xu, Hongxia and Zhu, Yiheng and Chen, Danny and Sun, Jimeng and Wu, Jian and Chen, Jintai",
        "title": "Making Pre-trained Language Models Great on Tabular Prediction"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "PEROVSEK20156442",
        "author": "Matic Perov\u0161ek and An\u017ee Vavpeti\u010d and Janez Kranjc and Bojan Cestnik and Nada Lavra\u010d",
        "title": "Wordification: Propositionalization by unfolding relational data into bags of words"
      },
      {
        "key": "lavravc2020propositionalization",
        "author": "Lavra{\\v{c}}, Nada and {\\v{S}}krlj, Bla{\\v{z}} and Robnik-{\\v{S}}ikonja, Marko",
        "title": "Propositionalization and embeddings: two sides of the same coin"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "fang2024large",
        "author": "Xi Fang and Weijie Xu and Fiona Anting Tan and Ziqing Hu and Jiani Zhang and Yanjun Qi and Srinivasan H. Sengamedu and Christos Faloutsos",
        "title": "Large Language Models ({LLM}s) on Tabular Data: Prediction, Generation, and Understanding - A Survey"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "hegselmann2023tabllm",
        "author": "Stefan Hegselmann and\nAlejandro Buendia and\nHunter Lang and\nMonica Agrawal and\nXiaoyi Jiang and\nDavid A. Sontag",
        "title": "TabLLM: Few-shot Classification of Tabular Data with Large Language\nModels"
      },
      {
        "key": "slack2023tabletlearninginstructionstabular",
        "author": "Dylan Slack and Sameer Singh",
        "title": "TABLET: Learning From Instructions For Tabular Data"
      },
      {
        "key": "zhang2023generativetablepretrainingempowers",
        "author": "Zhang, Tianping  and\nWang, Shaowen  and\nYan, Shuicheng  and\nJian, Li  and\nLiu, Qian",
        "title": "Generative Table Pre-training Empowers Models for Tabular Prediction"
      },
      {
        "key": "dinh2022lift",
        "author": "Tuan Dinh and\nYuchen Zeng and\nRuisu Zhang and\nZiqian Lin and\nMichael Gira and\nShashank Rajput and\nJy{-}yong Sohn and\nDimitris S. Papailiopoulos and\nKangwook Lee",
        "title": "{LIFT:} Language-Interfaced Fine-Tuning for Non-language Machine Learning\nTasks"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "tpberta",
        "author": "Yan, Jiahuan and Zheng, Bo and Xu, Hongxia and Zhu, Yiheng and Chen, Danny and Sun, Jimeng and Wu, Jian and Chen, Jintai",
        "title": "Making Pre-trained Language Models Great on Tabular Prediction"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wang2022transtab",
        "author": "Zifeng Wang and\nJimeng Sun",
        "title": "TransTab: Learning Transferable Tabular Transformers Across Tables"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zhu2023incorporatingllmpriorstabular",
        "author": "Max Zhu and Sini\u0161a Stanivuk and Andrija Petrovic and Mladen Nikolic and Pietro Lio",
        "title": "Incorporating LLM Priors into Tabular Learners"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "han2024large",
        "author": "Han, Sungwon and Yoon, Jinsung and Arik, Sercan O and Pfister, Tomas",
        "title": "Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "muennighoff-etal-2023-mteb",
        "author": "Muennighoff, Niklas  and\nTazi, Nouamane  and\nMagne, Loic  and\nReimers, Nils",
        "title": "{MTEB}: Massive Text Embedding Benchmark"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "reimers-2019-sentence-bert",
        "author": "Reimers, Nils  and\nGurevych, Iryna",
        "title": "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "luo2024bgelandmarkembeddingchunkingfree",
        "author": "Kun Luo and Zheng Liu and Shitao Xiao and Kang Liu",
        "title": "{BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models}"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "kojima2022large",
        "author": "Takeshi Kojima and\nShixiang Shane Gu and\nMachel Reid and\nYutaka Matsuo and\nYusuke Iwasawa",
        "title": "Large Language Models are Zero-Shot Reasoners"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "dubey2024llama",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "The llama 3 herd of models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "behnamghader2024llm2vec",
        "author": "BehnamGhader, Parishad and Adlakha, Vaibhav and Mosbach, Marius and Bahdanau, Dzmitry and Chapados, Nicolas and Reddy, Siva",
        "title": "Llm2vec: Large language models are secretly powerful text encoders"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "jiang2023scalingsentenceembeddingslarge",
        "author": "Ting Jiang and Shaohan Huang and Zhongzhi Luan and Deqing Wang and Fuzhen Zhuang",
        "title": "Scaling Sentence Embeddings with Large Language Models"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "bpe_suboptimal",
        "author": "Bostrom, Kaj  and\nDurrett, Greg",
        "title": "Byte Pair Encoding is Suboptimal for Language Model Pretraining"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "schwartz2024numerologicnumberencodingenhanced",
        "author": "Eli Schwartz and Leshem Choshen and Joseph Shtok and Sivan Doveh and Leonid Karlinsky and Assaf Arbelle",
        "title": "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning"
      }
    ]
  }
]