\section{Related work}
\textbf{Feature Embeddings} Embedding features is usually approached depending on the type of feature. Numerical features have traditionally been either transformed using a linear model \cite{hollmann2022tabpfn,fttransformer} or discretised by a variation of binning techniques \cite{tpberta,huang2020tabtransformer}. Binning as a technique for obtaining a pre-trained tabular deep learning model has shown promising results \cite{lee2023binning}. Categorical features are typically embedded via a lookup embedding layer \cite{fttransformer,huang2020tabtransformer}, with various techniques influencing their use \cite{borisov2110deep,gorishniy2022embeddings,wu2024deep}. Recently, tree-based feature embeddings have shown promising potential \cite{tpberta,li2023treeregularized}. Parallel to our work, \cite{tpberta} explored using word embeddings for embedding only feature names and categorical inputs, though they disentangled and embedded each word separately. To our knowledge, we are the first to explore LLM-based embeddings, both encoder- and decoder-based, for embedding of tabular data.


\textbf{LLMs and Tabular Data} Learning on serialised tabular data to text has been prominent in mining relational tabular data \cite{PEROVSEK20156442,lavravc2020propositionalization}. With the introduction of pre-trained models, a plethora of tabular models for table understanding and downstream prediction have been proposed, as shown in the recent survey \cite{fang2024large}. The majority of these applications focus on serialising the inputs and fine-tuning large language models for prediction \cite{hegselmann2023tabllm,slack2023tabletlearninginstructionstabular,zhang2023generativetablepretrainingempowers,dinh2022lift}. Another line of work focuses on using LLMs as classifiers, where inputs are tokenised and mapped to the LLM vocabulary \cite{tpberta}. The idea of leveraging the potential of LLMs for transfer-learning across tables and feature encoding was shown as useful in ~\cite{wang2022transtab}, however they propose different encoding for different input types adding a complexity by design. Recently, focus to incorporate LLM priors by prompting them to order the input features and this was exploited by traditional ML models \cite{zhu2023incorporatingllmpriorstabular} showing promising results. LLMs showed remarkable potential as feature engineers as well \cite{han2024large}. However, using LLMs in this manner is either computationally heavy e.g. fine-tuning or requires careful prompt creation which can be laborious. 

\textbf{LLMs and Text Embeddings} Semantically embedding texts is one of the main tasks of interest in NLP, resulting in a benchmarking effort called the Massive Text Embedding Benchmark (MTEB) \cite{muennighoff-etal-2023-mteb}. Traditionally, encoder-only model variants like sentence-based BERT \cite{reimers-2019-sentence-bert} were popular, with variants like BGE performing among the top performers \cite{luo2024bgelandmarkembeddingchunkingfree}. Recently, focus has shifted towards extracting embeddings from LLMs due to their remarkable capabilities \cite{kojima2022large}, where internal model embeddings, e.g., from the LLaMa3 model \cite{dubey2024llama}, are extracted with \cite{behnamghader2024llm2vec} or without tuning \cite{jiang2023scalingsentenceembeddingslarge} for document representation. Notably, the tokenisation process of LLMs, based on information-theoretic principles with byte-pair encoding techniques prevailing~\cite{bpe_suboptimal}, has been shown to negatively impact numerical reasoning tasks~\cite{schwartz2024numerologicnumberencodingenhanced}.