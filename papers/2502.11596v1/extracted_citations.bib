@article{PEROVSEK20156442,
 abstract = {Inductive Logic Programming (ILP) and Relational Data Mining (RDM) address the task of inducing models or patterns from multi-relational data. One of the established approaches to RDM is propositionalization, characterized by transforming a relational database into a single-table representation. This paper presents a propositionalization technique called wordification which can be seen as a transformation of a relational database into a corpus of text documents. Wordification constructs simple, easy to understand features, acting as words in the transformed Bag-Of-Words representation. This paper presents the wordification methodology, together with an experimental comparison of several propositionalization approaches on seven relational datasets. The main advantages of the approach are: simple implementation, accuracy comparable to competitive methods and greater scalability, as it performs several times faster on all experimental databases. Furthermore, the wordification methodology and the evaluation procedure are implemented as executable workflows in the web-based data mining platform ClowdFlows. The implemented workflows include also several other ILP and RDM algorithms, as well as the utility components that were added to the platform to enable access to these techniques to a wider research audience.},
 author = {Matic Perovšek and Anže Vavpetič and Janez Kranjc and Bojan Cestnik and Nada Lavrač},
 doi = {https://doi.org/10.1016/j.eswa.2015.04.017},
 issn = {0957-4174},
 journal = {Expert Systems with Applications},
 keywords = {Wordification, Inductive Logic Programming, Relational Data Mining, Propositionalization, Text mining, Classification},
 number = {17},
 pages = {6442-6456},
 title = {Wordification: Propositionalization by unfolding relational data into bags of words},
 url = {https://www.sciencedirect.com/science/article/pii/S095741741500247X},
 volume = {42},
 year = {2015}
}

@article{behnamghader2024llm2vec,
 author = {BehnamGhader, Parishad and Adlakha, Vaibhav and Mosbach, Marius and Bahdanau, Dzmitry and Chapados, Nicolas and Reddy, Siva},
 journal = {ArXiv preprint},
 title = {Llm2vec: Large language models are secretly powerful text encoders},
 url = {https://arxiv.org/abs/2404.05961},
 volume = {abs/2404.05961},
 year = {2024}
}

@article{borisov2110deep,
 author = {Borisov, V and Leemann, T and Se{\ss}ler, K and Haug, J and Pawelczyk, M and Kasneci, G},
 journal = {ArXiv preprint},
 title = {Deep neural networks and tabular data: A survey. arXiv 2021},
 url = {https://arxiv.org/abs/2110.01889},
 volume = {abs/2110.01889},
 year = {2021}
}

@inproceedings{bpe_suboptimal,
 address = {Online},
 author = {Bostrom, Kaj  and
Durrett, Greg},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
 doi = {10.18653/v1/2020.findings-emnlp.414},
 editor = {Cohn, Trevor  and
He, Yulan  and
Liu, Yang},
 pages = {4617--4624},
 publisher = {Association for Computational Linguistics},
 title = {Byte Pair Encoding is Suboptimal for Language Model Pretraining},
 url = {https://aclanthology.org/2020.findings-emnlp.414},
 year = {2020}
}

@inproceedings{dinh2022lift,
 author = {Tuan Dinh and
Yuchen Zeng and
Ruisu Zhang and
Ziqian Lin and
Michael Gira and
Shashank Rajput and
Jy{-}yong Sohn and
Dimitris S. Papailiopoulos and
Kangwook Lee},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/DinhZZLGRSP022.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {{LIFT:} Language-Interfaced Fine-Tuning for Non-language Machine Learning
Tasks},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/4ce7fe1d2730f53cb3857032952cd1b8-Abstract-Conference.html},
 year = {2022}
}

@article{dubey2024llama,
 author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
 journal = {ArXiv preprint},
 title = {The llama 3 herd of models},
 url = {https://arxiv.org/abs/2407.21783},
 volume = {abs/2407.21783},
 year = {2024}
}

@article{fang2024large,
 author = {Xi Fang and Weijie Xu and Fiona Anting Tan and Ziqing Hu and Jiani Zhang and Yanjun Qi and Srinivasan H. Sengamedu and Christos Faloutsos},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 title = {Large Language Models ({LLM}s) on Tabular Data: Prediction, Generation, and Understanding - A Survey},
 url = {https://openreview.net/forum?id=IZnrCGF9WI},
 year = {2024}
}

@inproceedings{fttransformer,
 author = {Yury Gorishniy and
Ivan Rubachev and
Valentin Khrulkov and
Artem Babenko},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/GorishniyRKB21.bib},
 booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual},
 editor = {Marc'Aurelio Ranzato and
Alina Beygelzimer and
Yann N. Dauphin and
Percy Liang and
Jennifer Wortman Vaughan},
 pages = {18932--18943},
 timestamp = {Tue, 03 May 2022 01:00:00 +0200},
 title = {Revisiting Deep Learning Models for Tabular Data},
 url = {https://proceedings.neurips.cc/paper/2021/hash/9d86d83f925f2149e9edb0ac3b49229c-Abstract.html},
 year = {2021}
}

@inproceedings{gorishniy2022embeddings,
 author = {Yury Gorishniy and
Ivan Rubachev and
Artem Babenko},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/GorishniyRB22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {On Embeddings for Numerical Features in Tabular Deep Learning},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/9e9f0ffc3d836836ca96cbf8fe14b105-Abstract-Conference.html},
 year = {2022}
}

@article{han2024large,
 author = {Han, Sungwon and Yoon, Jinsung and Arik, Sercan O and Pfister, Tomas},
 journal = {ArXiv preprint},
 title = {Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning},
 url = {https://arxiv.org/abs/2404.09491},
 volume = {abs/2404.09491},
 year = {2024}
}

@inproceedings{hegselmann2023tabllm,
 author = {Stefan Hegselmann and
Alejandro Buendia and
Hunter Lang and
Monica Agrawal and
Xiaoyi Jiang and
David A. Sontag},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aistats/HegselmannBLA0S23.bib},
 booktitle = {International Conference on Artificial Intelligence and Statistics,
25-27 April 2023, Palau de Congressos, Valencia, Spain},
 editor = {Francisco J. R. Ruiz and
Jennifer G. Dy and
Jan{-}Willem van de Meent},
 pages = {5549--5581},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Mon, 19 Jun 2023 01:00:00 +0200},
 title = {TabLLM: Few-shot Classification of Tabular Data with Large Language
Models},
 url = {https://proceedings.mlr.press/v206/hegselmann23a.html},
 volume = {206},
 year = {2023}
}

@inproceedings{hollmann2022tabpfn,
 author = {Noah Hollmann and
Samuel M{\"{u}}ller and
Katharina Eggensperger and
Frank Hutter},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/Hollmann0EH23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {TabPFN: {A} Transformer That Solves Small Tabular Classification Problems
in a Second},
 url = {https://openreview.net/pdf?id=cp5PvcI6w8\_},
 year = {2023}
}

@article{huang2020tabtransformer,
 author = {Huang, Xin and Khetan, Ashish and Cvitkovic, Milan and Karnin, Zohar},
 journal = {ArXiv preprint},
 title = {Tabtransformer: Tabular data modeling using contextual embeddings},
 url = {https://arxiv.org/abs/2012.06678},
 volume = {abs/2012.06678},
 year = {2020}
}

@misc{jiang2023scalingsentenceembeddingslarge,
 author = {Ting Jiang and Shaohan Huang and Zhongzhi Luan and Deqing Wang and Fuzhen Zhuang},
 journal = {ArXiv preprint},
 title = {Scaling Sentence Embeddings with Large Language Models},
 url = {https://arxiv.org/abs/2307.16645},
 volume = {abs/2307.16645},
 year = {2023}
}

@inproceedings{kojima2022large,
 author = {Takeshi Kojima and
Shixiang Shane Gu and
Machel Reid and
Yutaka Matsuo and
Yusuke Iwasawa},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/KojimaGRMI22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {Large Language Models are Zero-Shot Reasoners},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html},
 year = {2022}
}

@article{lavravc2020propositionalization,
 author = {Lavra{\v{c}}, Nada and {\v{S}}krlj, Bla{\v{z}} and Robnik-{\v{S}}ikonja, Marko},
 journal = {Machine Learning},
 pages = {1465--1507},
 publisher = {Springer},
 title = {Propositionalization and embeddings: two sides of the same coin},
 volume = {109},
 year = {2020}
}

@inproceedings{lee2023binning,
 author = {Kyungeun Lee and Ye Seul Sim and Hyeseung Cho and Suhee Yoon and Sanghyu Yoon and Woohyung Lim},
 booktitle = {NeurIPS 2023 Second Table Representation Learning Workshop},
 title = {Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains},
 url = {https://openreview.net/forum?id=btK3lk5puP},
 year = {2023}
}

@inproceedings{li2023treeregularized,
 author = {Xuan Li and Yun Wang and Bo Li},
 booktitle = {NeurIPS 2023 Second Table Representation Learning Workshop},
 title = {Tree-Regularized Tabular Embeddings},
 url = {https://openreview.net/forum?id=dQLDxIPsU4},
 year = {2023}
}

@misc{luo2024bgelandmarkembeddingchunkingfree,
 author = {Kun Luo and Zheng Liu and Shitao Xiao and Kang Liu},
 journal = {ArXiv preprint},
 title = {{BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models}},
 url = {https://arxiv.org/abs/2402.11573},
 volume = {abs/2402.11573},
 year = {2024}
}

@inproceedings{muennighoff-etal-2023-mteb,
 address = {Dubrovnik, Croatia},
 author = {Muennighoff, Niklas  and
Tazi, Nouamane  and
Magne, Loic  and
Reimers, Nils},
 booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
 doi = {10.18653/v1/2023.eacl-main.148},
 editor = {Vlachos, Andreas  and
Augenstein, Isabelle},
 pages = {2014--2037},
 publisher = {Association for Computational Linguistics},
 title = {{MTEB}: Massive Text Embedding Benchmark},
 url = {https://aclanthology.org/2023.eacl-main.148},
 year = {2023}
}

@inproceedings{reimers-2019-sentence-bert,
 address = {Hong Kong, China},
 author = {Reimers, Nils  and
Gurevych, Iryna},
 booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
 doi = {10.18653/v1/D19-1410},
 editor = {Inui, Kentaro  and
Jiang, Jing  and
Ng, Vincent  and
Wan, Xiaojun},
 pages = {3982--3992},
 publisher = {Association for Computational Linguistics},
 title = {Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks},
 url = {https://aclanthology.org/D19-1410},
 year = {2019}
}

@misc{schwartz2024numerologicnumberencodingenhanced,
 author = {Eli Schwartz and Leshem Choshen and Joseph Shtok and Sivan Doveh and Leonid Karlinsky and Assaf Arbelle},
 journal = {ArXiv preprint},
 title = {NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning},
 url = {https://arxiv.org/abs/2404.00459},
 volume = {abs/2404.00459},
 year = {2024}
}

@misc{slack2023tabletlearninginstructionstabular,
 author = {Dylan Slack and Sameer Singh},
 journal = {ArXiv preprint},
 title = {TABLET: Learning From Instructions For Tabular Data},
 url = {https://arxiv.org/abs/2304.13188},
 volume = {abs/2304.13188},
 year = {2023}
}

@article{tpberta,
 author = {Yan, Jiahuan and Zheng, Bo and Xu, Hongxia and Zhu, Yiheng and Chen, Danny and Sun, Jimeng and Wu, Jian and Chen, Jintai},
 journal = {ArXiv preprint},
 title = {Making Pre-trained Language Models Great on Tabular Prediction},
 url = {https://arxiv.org/abs/2403.01841},
 volume = {abs/2403.01841},
 year = {2024}
}

@inproceedings{wang2022transtab,
 author = {Zifeng Wang and
Jimeng Sun},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/Wang022.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {TransTab: Learning Transferable Tabular Transformers Across Tables},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/1377f76686d56439a2bd7a91859972f5-Abstract-Conference.html},
 year = {2022}
}

@article{wu2024deep,
 author = {Wu, Yuqian and Luo, Hengyi and Lee, Raymond ST},
 journal = {ArXiv preprint},
 title = {Deep Feature Embedding for Tabular Data},
 url = {https://arxiv.org/abs/2408.17162},
 volume = {abs/2408.17162},
 year = {2024}
}

@inproceedings{zhang2023generativetablepretrainingempowers,
 address = {Singapore},
 author = {Zhang, Tianping  and
Wang, Shaowen  and
Yan, Shuicheng  and
Jian, Li  and
Liu, Qian},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2023.emnlp-main.917},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {14836--14854},
 publisher = {Association for Computational Linguistics},
 title = {Generative Table Pre-training Empowers Models for Tabular Prediction},
 url = {https://aclanthology.org/2023.emnlp-main.917},
 year = {2023}
}

@misc{zhu2023incorporatingllmpriorstabular,
      title={Incorporating LLM Priors into Tabular Learners}, 
      author={Max Zhu and Siniša Stanivuk and Andrija Petrovic and Mladen Nikolic and Pietro Lio},
      year={2023},
      eprint={2311.11628},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.11628}, 
}

