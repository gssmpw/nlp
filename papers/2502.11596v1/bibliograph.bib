@article{wu2024deep,
 author = {Wu, Yuqian and Luo, Hengyi and Lee, Raymond ST},
 journal = {ArXiv preprint},
 title = {Deep Feature Embedding for Tabular Data},
 url = {https://arxiv.org/abs/2408.17162},
 volume = {abs/2408.17162},
 year = {2024}
}

@inproceedings{gorishniy2022embeddings,
 author = {Yury Gorishniy and
Ivan Rubachev and
Artem Babenko},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/GorishniyRB22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {On Embeddings for Numerical Features in Tabular Deep Learning},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/9e9f0ffc3d836836ca96cbf8fe14b105-Abstract-Conference.html},
 year = {2022}
}

@article{fang2024large,
 author = {Xi Fang and Weijie Xu and Fiona Anting Tan and Ziqing Hu and Jiani Zhang and Yanjun Qi and Srinivasan H. Sengamedu and Christos Faloutsos},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {},
 title = {Large Language Models ({LLM}s) on Tabular Data: Prediction, Generation, and Understanding - A Survey},
 url = {https://openreview.net/forum?id=IZnrCGF9WI},
 year = {2024}
}

@inproceedings{DBLP:journals/corr/abs-2002-10957,
 author = {Wenhui Wang and
Furu Wei and
Li Dong and
Hangbo Bao and
Nan Yang and
Ming Zhou},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/WangW0B0020.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression
of Pre-Trained Transformers},
 url = {https://proceedings.neurips.cc/paper/2020/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
 year = {2020}
}

@inproceedings{minim-10957,
 author = {Wenhui Wang and
Furu Wei and
Li Dong and
Hangbo Bao and
Nan Yang and
Ming Zhou},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/WangW0B0020.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression
of Pre-Trained Transformers},
 url = {https://proceedings.neurips.cc/paper/2020/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
 year = {2020}
}

@misc{koloski2025hornetslearningdiscretecontinuous,
      title={HorNets: Learning from Discrete and Continuous Signals with Routing Neural Networks}, 
      author={Boshko koloski and Nada Lavrač and Blaž Škrlj},
      year={2025},
      eprint={2501.14346},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.14346}, 
}

@misc{zhu2023incorporatingllmpriorstabular,
      title={Incorporating LLM Priors into Tabular Learners}, 
      author={Max Zhu and Siniša Stanivuk and Andrija Petrovic and Mladen Nikolic and Pietro Lio},
      year={2023},
      eprint={2311.11628},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.11628}, 
}


@article{ruiz2023enabling,
 author = {Ruiz, Camilo and Ren, Hongyu and Huang, Kexin and Leskovec, Jure},
 journal = {ArXiv preprint},
 title = {Enabling tabular deep learning when  d >> n with an auxiliary knowledge graph},
 url = {https://arxiv.org/abs/2306.04766},
 volume = {abs/2306.04766},
 year = {2023}
}

@inproceedings{dinh2022lift,
 author = {Tuan Dinh and
Yuchen Zeng and
Ruisu Zhang and
Ziqian Lin and
Michael Gira and
Shashank Rajput and
Jy{-}yong Sohn and
Dimitris S. Papailiopoulos and
Kangwook Lee},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/DinhZZLGRSP022.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {{LIFT:} Language-Interfaced Fine-Tuning for Non-language Machine Learning
Tasks},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/4ce7fe1d2730f53cb3857032952cd1b8-Abstract-Conference.html},
 year = {2022}
}

@inproceedings{wang2022transtab,
 author = {Zifeng Wang and
Jimeng Sun},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/Wang022.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {TransTab: Learning Transferable Tabular Transformers Across Tables},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/1377f76686d56439a2bd7a91859972f5-Abstract-Conference.html},
 year = {2022}
}

@article{koloski2023latent,
 author = {Koloski, Boshko and Lavra{\v{c}}, Nada and Pollak, Senja and {\v{S}}krlj, Bla{\v{z}}},
 journal = {arXiv e-prints},
 pages = {arXiv--2309},
 title = {Latent Graphs for Semi-Supervised Learning on Biomedical Tabular Data},
 year = {2023}
}

@article{khanna2024tabular,
 author = {Khanna, Sujit and Subedi, Shishir},
 journal = {ArXiv preprint},
 title = {Tabular Embedding Model (TEM): Finetuning Embedding Models For Tabular RAG Applications},
 url = {https://arxiv.org/abs/2405.01585},
 volume = {abs/2405.01585},
 year = {2024}
}

@article{benavoli2017time,
 author = {Benavoli, Alessio and Corani, Giorgio and Dem{\v{s}}ar, Janez and Zaffalon, Marco},
 journal = {Journal of Machine Learning Research},
 number = {77},
 pages = {1--36},
 title = {Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis},
 volume = {18},
 year = {2017}
}

@inproceedings{bpe_suboptimal,
 address = {Online},
 author = {Bostrom, Kaj  and
Durrett, Greg},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
 doi = {10.18653/v1/2020.findings-emnlp.414},
 editor = {Cohn, Trevor  and
He, Yulan  and
Liu, Yang},
 pages = {4617--4624},
 publisher = {Association for Computational Linguistics},
 title = {Byte Pair Encoding is Suboptimal for Language Model Pretraining},
 url = {https://aclanthology.org/2020.findings-emnlp.414},
 year = {2020}
}

@misc{schwartz2024numerologicnumberencodingenhanced,
 author = {Eli Schwartz and Leshem Choshen and Joseph Shtok and Sivan Doveh and Leonid Karlinsky and Assaf Arbelle},
 journal = {ArXiv preprint},
 title = {NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning},
 url = {https://arxiv.org/abs/2404.00459},
 volume = {abs/2404.00459},
 year = {2024}
}

@misc{jiang2023scalingsentenceembeddingslarge,
 author = {Ting Jiang and Shaohan Huang and Zhongzhi Luan and Deqing Wang and Fuzhen Zhuang},
 journal = {ArXiv preprint},
 title = {Scaling Sentence Embeddings with Large Language Models},
 url = {https://arxiv.org/abs/2307.16645},
 volume = {abs/2307.16645},
 year = {2023}
}

@inproceedings{lee2023binning,
 author = {Kyungeun Lee and Ye Seul Sim and Hyeseung Cho and Suhee Yoon and Sanghyu Yoon and Woohyung Lim},
 booktitle = {NeurIPS 2023 Second Table Representation Learning Workshop},
 title = {Binning as a Pretext Task: Improving Self-Supervised Learning in Tabular Domains},
 url = {https://openreview.net/forum?id=btK3lk5puP},
 year = {2023}
}

@inproceedings{zhang2023generativetablepretrainingempowers,
 address = {Singapore},
 author = {Zhang, Tianping  and
Wang, Shaowen  and
Yan, Shuicheng  and
Jian, Li  and
Liu, Qian},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2023.emnlp-main.917},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 pages = {14836--14854},
 publisher = {Association for Computational Linguistics},
 title = {Generative Table Pre-training Empowers Models for Tabular Prediction},
 url = {https://aclanthology.org/2023.emnlp-main.917},
 year = {2023}
}

@misc{slack2023tabletlearninginstructionstabular,
 author = {Dylan Slack and Sameer Singh},
 journal = {ArXiv preprint},
 title = {TABLET: Learning From Instructions For Tabular Data},
 url = {https://arxiv.org/abs/2304.13188},
 volume = {abs/2304.13188},
 year = {2023}
}

@article{PEROVSEK20156442,
 abstract = {Inductive Logic Programming (ILP) and Relational Data Mining (RDM) address the task of inducing models or patterns from multi-relational data. One of the established approaches to RDM is propositionalization, characterized by transforming a relational database into a single-table representation. This paper presents a propositionalization technique called wordification which can be seen as a transformation of a relational database into a corpus of text documents. Wordification constructs simple, easy to understand features, acting as words in the transformed Bag-Of-Words representation. This paper presents the wordification methodology, together with an experimental comparison of several propositionalization approaches on seven relational datasets. The main advantages of the approach are: simple implementation, accuracy comparable to competitive methods and greater scalability, as it performs several times faster on all experimental databases. Furthermore, the wordification methodology and the evaluation procedure are implemented as executable workflows in the web-based data mining platform ClowdFlows. The implemented workflows include also several other ILP and RDM algorithms, as well as the utility components that were added to the platform to enable access to these techniques to a wider research audience.},
 author = {Matic Perovšek and Anže Vavpetič and Janez Kranjc and Bojan Cestnik and Nada Lavrač},
 doi = {https://doi.org/10.1016/j.eswa.2015.04.017},
 issn = {0957-4174},
 journal = {Expert Systems with Applications},
 keywords = {Wordification, Inductive Logic Programming, Relational Data Mining, Propositionalization, Text mining, Classification},
 number = {17},
 pages = {6442-6456},
 title = {Wordification: Propositionalization by unfolding relational data into bags of words},
 url = {https://www.sciencedirect.com/science/article/pii/S095741741500247X},
 volume = {42},
 year = {2015}
}

@inproceedings{li2023treeregularized,
 author = {Xuan Li and Yun Wang and Bo Li},
 booktitle = {NeurIPS 2023 Second Table Representation Learning Workshop},
 title = {Tree-Regularized Tabular Embeddings},
 url = {https://openreview.net/forum?id=dQLDxIPsU4},
 year = {2023}
}

@article{ye2024closer,
 author = {Ye, Han-Jia and Liu, Si-Yang and Cai, Hao-Run and Zhou, Qi-Le and Zhan, De-Chuan},
 journal = {ArXiv preprint},
 title = {A closer look at deep learning on tabular data},
 url = {https://arxiv.org/abs/2407.00956},
 volume = {abs/2407.00956},
 year = {2024}
}

@article{dubey2024llama,
 author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
 journal = {ArXiv preprint},
 title = {The llama 3 herd of models},
 url = {https://arxiv.org/abs/2407.21783},
 volume = {abs/2407.21783},
 year = {2024}
}

@article{behnamghader2024llm2vec,
 author = {BehnamGhader, Parishad and Adlakha, Vaibhav and Mosbach, Marius and Bahdanau, Dzmitry and Chapados, Nicolas and Reddy, Siva},
 journal = {ArXiv preprint},
 title = {Llm2vec: Large language models are secretly powerful text encoders},
 url = {https://arxiv.org/abs/2404.05961},
 volume = {abs/2404.05961},
 year = {2024}
}

@inproceedings{hegselmann2023tabllm,
 author = {Stefan Hegselmann and
Alejandro Buendia and
Hunter Lang and
Monica Agrawal and
Xiaoyi Jiang and
David A. Sontag},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aistats/HegselmannBLA0S23.bib},
 booktitle = {International Conference on Artificial Intelligence and Statistics,
25-27 April 2023, Palau de Congressos, Valencia, Spain},
 editor = {Francisco J. R. Ruiz and
Jennifer G. Dy and
Jan{-}Willem van de Meent},
 pages = {5549--5581},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Mon, 19 Jun 2023 01:00:00 +0200},
 title = {TabLLM: Few-shot Classification of Tabular Data with Large Language
Models},
 url = {https://proceedings.mlr.press/v206/hegselmann23a.html},
 volume = {206},
 year = {2023}
}

@inproceedings{muennighoff-etal-2023-mteb,
 address = {Dubrovnik, Croatia},
 author = {Muennighoff, Niklas  and
Tazi, Nouamane  and
Magne, Loic  and
Reimers, Nils},
 booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
 doi = {10.18653/v1/2023.eacl-main.148},
 editor = {Vlachos, Andreas  and
Augenstein, Isabelle},
 pages = {2014--2037},
 publisher = {Association for Computational Linguistics},
 title = {{MTEB}: Massive Text Embedding Benchmark},
 url = {https://aclanthology.org/2023.eacl-main.148},
 year = {2023}
}

@article{borisov2110deep,
 author = {Borisov, V and Leemann, T and Se{\ss}ler, K and Haug, J and Pawelczyk, M and Kasneci, G},
 journal = {ArXiv preprint},
 title = {Deep neural networks and tabular data: A survey. arXiv 2021},
 url = {https://arxiv.org/abs/2110.01889},
 volume = {abs/2110.01889},
 year = {2021}
}

@article{liu2019roberta,
 author = {Liu, Yinhan},
 journal = {ArXiv preprint},
 title = {Roberta: A robustly optimized bert pretraining approach},
 url = {https://arxiv.org/abs/1907.11692},
 volume = {abs/1907.11692},
 year = {2019}
}

@misc{luo2024bgelandmarkembeddingchunkingfree,
 author = {Kun Luo and Zheng Liu and Shitao Xiao and Kang Liu},
 journal = {ArXiv preprint},
 title = {{BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models}},
 url = {https://arxiv.org/abs/2402.11573},
 volume = {abs/2402.11573},
 year = {2024}
}

@article{han2024large,
 author = {Han, Sungwon and Yoon, Jinsung and Arik, Sercan O and Pfister, Tomas},
 journal = {ArXiv preprint},
 title = {Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning},
 url = {https://arxiv.org/abs/2404.09491},
 volume = {abs/2404.09491},
 year = {2024}
}

@inproceedings{thawani2021representing,
 address = {Online},
 author = {Thawani, Avijit  and
Pujara, Jay  and
Ilievski, Filip  and
Szekely, Pedro},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2021.naacl-main.53},
 editor = {Toutanova, Kristina  and
Rumshisky, Anna  and
Zettlemoyer, Luke  and
Hakkani-Tur, Dilek  and
Beltagy, Iz  and
Bethard, Steven  and
Cotterell, Ryan  and
Chakraborty, Tanmoy  and
Zhou, Yichao},
 pages = {644--656},
 publisher = {Association for Computational Linguistics},
 title = {Representing Numbers in {NLP}: a Survey and a Vision},
 url = {https://aclanthology.org/2021.naacl-main.53},
 year = {2021}
}

@inproceedings{grinsztajn2022tree,
 author = {L{\'{e}}o Grinsztajn and
Edouard Oyallon and
Ga{\"{e}}l Varoquaux},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/GrinsztajnOV22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {Why do tree-based models still outperform deep learning on typical
tabular data?},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/0378c7692da36807bdec87ab043cdadc-Abstract-Datasets\_and\_Benchmarks.html},
 year = {2022}
}

@article{margeloiu2024gcondnet,
 author = {Andrei Margeloiu and Nikola Simidjievski and Pietro Lio and Mateja Jamnik},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research},
 note = {Reproducibility Certification},
 title = {{GCondNet: A Novel Method for Improving Neural Networks on Small High-Dimensional Tabular Data}},
 url = {https://openreview.net/forum?id=y0b0H1ndGQ},
 year = {2024}
}

@inproceedings{margeloiu2024tabmda,
 author = {Margeloiu, Andrei and Bazaga, Adri{\'a}n and Simidjievski, Nikola and Li{\`o}, Pietro and Jamnik, Mateja},
 booktitle = {ICML 1st Workshop on In-Context Learning},
 title = {{\mbox{TabMDA}: Tabular Manifold Data Augmentation for Any Classifier using Transformers with In-context Subsetting}},
 year = {2024}
}

@misc{bank_marketing_222,
 author = {Moro, S. and Rita, P. and Cortez, P.},
 howpublished = {UCI Machine Learning Repository},
 note = {{DOI}: https://doi.org/10.24432/C5K306},
 title = {{Bank Marketing}},
 year = {2014}
}

@misc{statlog_(german_credit_data)_144,
 author = {Hofmann, Hans},
 howpublished = {UCI Machine Learning Repository},
 note = {{DOI}: https://doi.org/10.24432/C5NC77},
 title = {{Statlog (German Credit Data)}},
 year = {1994}
}

@misc{higher_education_students_performance_evaluation_856,
 author = {Yilmaz, Nevriye and Şekeroğlu, Boran},
 howpublished = {UCI Machine Learning Repository},
 note = {{DOI}: https://doi.org/10.24432/C51G82},
 title = {{Higher Education Students Performance Evaluation}},
 year = {2019}
}

@article{dong2024generalization,
 author = {Dong, Yihong and Jiang, Xue and Liu, Huanyu and Jin, Zhi and Li, Ge},
 journal = {ArXiv preprint},
 title = {Generalization or memorization: Data contamination and trustworthy evaluation for large language models},
 url = {https://arxiv.org/abs/2402.15938},
 volume = {abs/2402.15938},
 year = {2024}
}

@article{bordt2024elephants,
 author = {Bordt, Sebastian and Nori, Harsha and Rodrigues, Vanessa and Nushi, Besmira and Caruana, Rich},
 journal = {ArXiv preprint},
 title = {Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models},
 url = {https://arxiv.org/abs/2404.06209},
 volume = {abs/2404.06209},
 year = {2024}
}

@misc{hepatitis_46,
 author = {Turney, Peter},
 howpublished = {UCI Machine Learning Repository},
 note = {{DOI}: https://doi.org/10.24432/C5Q59J},
 title = {{Hepatitis}},
 year = {1983}
}

@misc{heart_disease_45,
 author = {Janosi, Andras and Steinbrunn, William and Pfisterer, Matthias and Detrano, Robert},
 howpublished = {UCI Machine Learning Repository},
 note = {{DOI}: https://doi.org/10.24432/C52P4X},
 title = {{Heart Disease}},
 year = {1989}
}

@misc{diabetes_34,
 author = {Kahn, Michael},
 howpublished = {UCI Machine Learning Repository},
 note = {{DOI}: https://doi.org/10.24432/C5T59G},
 title = {{Diabetes}}
}

@misc{blood_transfusion_service_center_176,
 author = {Yeh, I-Cheng},
 howpublished = {UCI Machine Learning Repository},
 note = {{DOI}: https://doi.org/10.24432/C5GS39},
 title = {{Blood Transfusion Service Center}},
 year = {2008}
}

@inproceedings{lecun2015deep,
 author = {Ruslan Salakhutdinov},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/kdd/Salakhutdinov14.bib},
 booktitle = {The 20th {ACM} {SIGKDD} International Conference on Knowledge Discovery
and Data Mining, {KDD} '14, New York, NY, {USA} - August 24 - 27,
2014},
 doi = {10.1145/2623330.2630809},
 editor = {Sofus A. Macskassy and
Claudia Perlich and
Jure Leskovec and
Wei Wang and
Rayid Ghani},
 pages = {1973},
 publisher = {{ACM}},
 timestamp = {Tue, 06 Nov 2018 00:00:00 +0100},
 title = {Deep learning},
 url = {https://doi.org/10.1145/2623330.2630809},
 year = {2014}
}

@article{yildirim2024task,
 author = {Yildirim, Ilker and Paul, LA},
 journal = {Trends in Cognitive Sciences},
 publisher = {Elsevier},
 title = {From task structures to world models: what do LLMs know?},
 year = {2024}
}

@article{lavravc2020propositionalization,
 author = {Lavra{\v{c}}, Nada and {\v{S}}krlj, Bla{\v{z}} and Robnik-{\v{S}}ikonja, Marko},
 journal = {Machine Learning},
 pages = {1465--1507},
 publisher = {Springer},
 title = {Propositionalization and embeddings: two sides of the same coin},
 volume = {109},
 year = {2020}
}

@inproceedings{vaswani,
 author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {5998--6008},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
 year = {2017}
}

@inproceedings{sui2024table,
 abstract = {Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. Although tables can be used as input to LLMs with serialization, there is a lack of comprehensive studies that examine whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, e.g., cell lookup, row retrieval and size detection. We perform a series of evaluations on GPT-3.5 and GPT-4. We find that performance varied depending on several input choices, including table input format, content order, role prompting and partition marks. Drawing from the insights gained through the benchmark evaluations, we propose self-augmentation for effective structural prompting, such as critical value / range identification using internal knowledge of LLMs. When combined with carefully chosen input choices, these structural prompting methods lead to promising improvements in LLM performance on a variety of tabular tasks, e.g., TabFact (↑ 2.31%), HybridQA (↑ 2.13%), SQA (↑ 2.72%), Feverous (↑ 0.84%) and ToTTo (↑ 5.68%). We believe that our open source benchmark and proposed prompting methods can serve as a simple yet generic selection for future research.},
 author = {Sui, Yuan and Zhou, Mengyu and Zhou, Mingjie and Han, Shi and Zhang, Dongmei},
 booktitle = {The 17th ACM International Conference on Web Search and Data Mining (WSDM '24)},
 title = {Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study},
 url = {https://www.microsoft.com/en-us/research/publication/table-meets-llm-can-large-language-models-understand-structured-table-data-a-benchmark-and-empirical-study/},
 year = {2024}
}

@inproceedings{reimers-2019-sentence-bert,
 address = {Hong Kong, China},
 author = {Reimers, Nils  and
Gurevych, Iryna},
 booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
 doi = {10.18653/v1/D19-1410},
 editor = {Inui, Kentaro  and
Jiang, Jing  and
Ng, Vincent  and
Wan, Xiaojun},
 pages = {3982--3992},
 publisher = {Association for Computational Linguistics},
 title = {Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks},
 url = {https://aclanthology.org/D19-1410},
 year = {2019}
}

@article{tpberta,
 author = {Yan, Jiahuan and Zheng, Bo and Xu, Hongxia and Zhu, Yiheng and Chen, Danny and Sun, Jimeng and Wu, Jian and Chen, Jintai},
 journal = {ArXiv preprint},
 title = {Making Pre-trained Language Models Great on Tabular Prediction},
 url = {https://arxiv.org/abs/2403.01841},
 volume = {abs/2403.01841},
 year = {2024}
}

@article{thielmann2024mambular,
 author = {Thielmann, Anton Frederik and Kumar, Manish and Weisser, Christoph and Reuter, Arik and S{\"a}fken, Benjamin and Samiee, Soheila},
 journal = {ArXiv preprint},
 title = {Mambular: A Sequential Model for Tabular Deep Learning},
 url = {https://arxiv.org/abs/2408.06291},
 volume = {abs/2408.06291},
 year = {2024}
}

@inproceedings{arik2021tabnet,
 author = {Sercan {\"{O}}. Arik and
Tomas Pfister},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/aaai/ArikP21.bib},
 booktitle = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI}
2021, Thirty-Third Conference on Innovative Applications of Artificial
Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances
in Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9,
2021},
 pages = {6679--6687},
 publisher = {{AAAI} Press},
 timestamp = {Wed, 02 Jun 2021 01:00:00 +0200},
 title = {TabNet: Attentive Interpretable Tabular Learning},
 url = {https://ojs.aaai.org/index.php/AAAI/article/view/16826},
 year = {2021}
}

@inproceedings{hollmann2022tabpfn,
 author = {Noah Hollmann and
Samuel M{\"{u}}ller and
Katharina Eggensperger and
Frank Hutter},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/Hollmann0EH23.bib},
 booktitle = {The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
 publisher = {OpenReview.net},
 timestamp = {Fri, 30 Jun 2023 01:00:00 +0200},
 title = {TabPFN: {A} Transformer That Solves Small Tabular Classification Problems
in a Second},
 url = {https://openreview.net/pdf?id=cp5PvcI6w8\_},
 year = {2023}
}

@article{huang2020tabtransformer,
 author = {Huang, Xin and Khetan, Ashish and Cvitkovic, Milan and Karnin, Zohar},
 journal = {ArXiv preprint},
 title = {Tabtransformer: Tabular data modeling using contextual embeddings},
 url = {https://arxiv.org/abs/2012.06678},
 volume = {abs/2012.06678},
 year = {2020}
}

@inproceedings{fttransformer,
 author = {Yury Gorishniy and
Ivan Rubachev and
Valentin Khrulkov and
Artem Babenko},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/GorishniyRKB21.bib},
 booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual},
 editor = {Marc'Aurelio Ranzato and
Alina Beygelzimer and
Yann N. Dauphin and
Percy Liang and
Jennifer Wortman Vaughan},
 pages = {18932--18943},
 timestamp = {Tue, 03 May 2022 01:00:00 +0200},
 title = {Revisiting Deep Learning Models for Tabular Data},
 url = {https://proceedings.neurips.cc/paper/2021/hash/9d86d83f925f2149e9edb0ac3b49229c-Abstract.html},
 year = {2021}
}

@article{friedman1994overview,
 author = {Friedman, Jerome H},
 journal = {From statistics to neural networks: Theory and pattern recognition applications},
 pages = {1--61},
 publisher = {Springer},
 title = {An overview of predictive learning and function approximation},
 year = {1994}
}

@article{liu2021self,
 author = {Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Mian, Li and Wang, Zhaoyu and Zhang, Jing and Tang, Jie},
 journal = {IEEE transactions on knowledge and data engineering},
 number = {1},
 pages = {857--876},
 publisher = {IEEE},
 title = {Self-supervised learning: Generative or contrastive},
 volume = {35},
 year = {2021}
}

@article{jaiswal2020survey,
 author = {Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
 journal = {Technologies},
 number = {1},
 pages = {2},
 publisher = {MDPI},
 title = {A survey on contrastive self-supervised learning},
 volume = {9},
 year = {2020}
}

@article{bengio2013representation,
 author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
 journal = {IEEE transactions on pattern analysis and machine intelligence},
 number = {8},
 pages = {1798--1828},
 publisher = {IEEE},
 title = {Representation learning: A review and new perspectives},
 volume = {35},
 year = {2013}
}

@inproceedings{kojima2022large,
 author = {Takeshi Kojima and
Shixiang Shane Gu and
Machel Reid and
Yutaka Matsuo and
Yusuke Iwasawa},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/KojimaGRMI22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 00:00:00 +0100},
 title = {Large Language Models are Zero-Shot Reasoners},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html},
 year = {2022}
}