\section{High Dimensional Applications}
\label{applications}
\subsection{Adversarial Attacks}

As powerful vision models like ResNet \cite{targ2016resnet} and Vision Transformers (ViT) \cite{han2022survey} grow in prominence, adversarial attacks have become a significant concern. These attacks subtly modify inputs, causing models to misclassify them, while the perturbation remains imperceptible to both human vision and other classifiers \cite{tang2019adversarial}.
Formally, an adversarial attack is defined as:

\begin{equation}
\label{eq:advarserial_min}
    x^\ast_a = \arg\min_{x} d(x, x_a) \ \ \ \text{s.t.} f(x) \neq f(x_a)
\end{equation}
Where $f$ is the classifier and $d$ is some distance metric between elements.

Recent studies have extended adversarial attacks to domains like AI-text detection \cite{sadasivan2024aigeneratedtextreliablydetected} and automotive sensors \cite{Mahima3DAttackReview}. These attacks prevent tracking and detection, posing risks to both users and pedestrians.
Adversarial attacks are classified into black-box and white-box methods. Black-box attacks only require query access, while white-box methods use model gradients to craft perturbations \cite{machado2021adversarial, cao2019adversarial}. Despite some black-box methods relying on surrogate models \cite{dong2018boosting, xiao2018generating, madry2017towards, goodfellow2014explaining}, approaches like~\cite{tu2019autozoom} generate random samples to approximate gradient estimation, though they are computationally expensive. Other methods use GAN networks to search latent spaces for adversarial examples \cite{liu2021multi, sarkar2017upset}. Still, they depend on existing GANs and their latent space diversity.


Our Enhanced Gradient Learning method offers a true black-box approach with precise perturbation control, avoiding gradient back-propagation. OGL directly optimizes perturbations to maintain low distortion while fooling the model, handling high-dimensional spaces with over 30k parameters.

\begin{table}[b!]
    \centering
    \scriptsize % Adjust font size to fit
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{l|c|c|c|c|c}
        \textbf{Metric} & \textbf{CMA} & \textbf{OGL} & \textbf{HGL} & \textbf{OHGL} & \textbf{CMA+OGL} \\
        \hline
        Accuracy & 0.02 & 0.02 & 0.02 & 0.02 & 0.02 \\
        MSE & 0.05 & \textbf{0.001} & 0.002 & \textbf{0.001} & 0.001\\
        Time (Until Convergence) & \textbf{20m} & 6H & 7H & 7H & 3H \\
    \end{tabular}%
    }
    \caption{Comparison of methods on Accuracy, MSE, and Time.}
    \label{compare_attack_methods}
\end{table}




\begin{figure}[htbp]
    \centering
    \begin{minipage}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/adversarial_attack/model_2.png}
        \subcaption{Original}
    \end{minipage}
    \begin{minipage}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/adversarial_attack/cma_results.png}
        \subcaption{CMA}
    \end{minipage}
    \begin{minipage}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/adversarial_attack/result_image.jpg}
        \subcaption{OGL}
    \end{minipage}
    \begin{minipage}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/adversarial_attack/ohgl_image.jpg}
        \subcaption{OHGL}
    \end{minipage}
    \caption{Adversarial examples generated by OGL and CMA  against the ImageNet model.}
    \label{fig:set1}
\end{figure}

\textbf{Methodology}: We applied OGL to classifiers trained on MNIST, CIFAR-10, and ImageNet, aiming to minimize \eqref{eq:advarserial_min}. To generate adversarial images with minimal distortion, we developed a penalty that jointly minimizes MSE and CE-loss \ref{advarserial_atack_implementation}. This approach successfully fooled the model, evading the top 5 classifications.


\textbf{Results}: We evaluated four different configurations: CMA, OGL, HGL, and a combination of both (CMA+OGL) where the CMA run provides the initial guess for an OGL run. While CMA alone was not able to converge to a satisfying adversarial example, the combined CMA+OGL enjoyed the rapid start of CMA with the robustness of OGL s.t. it was able to find a satisfying adversarial example half the computation time of OGL and OHGL.


\subsection{Code generation}
The development of large language models (LLMs) such as Transformers \cite{vaswani2017attention} have advanced code generation \cite{dehaerne2022code}. Despite these strides, fine-tuning outputs based on parameters measured post-generation remains challenging.
Recent algorithms have been developed to generate code tailored for specific tasks using LLMs. For instance, FunSearch \cite{romera2024mathematical} generates new code solutions for complex tasks, while Chain of Code \cite{li2023chain} incorporates reasoning to detect and correct errors in the output code. Similarly, our method uses black-box optimization to guide code generation for runtime efficiency.
Building on \cite{zhang2024interpreting}, which links LLM expertise to a small parameter set, we fine-tuned the embedding layer to reduce Python code runtime. Using LoRA \cite{lora}, we optimized the generated code based on execution time, scaling up to $\sim 200\text{k}$ parameters.

\textbf{Fibonacci:}
We tested this approach by having the model generate a Fibonacci function. Initially incorrect, optimization guided the model to a correct and efficient solution. Figure 1 illustrates this progression, with the 25th step showing an optimized version.

\begin{figure}[h]
\centering
\begin{minipage}{0.32\textwidth}
    \centering
    \lstset{
        language=Python,
        basicstyle=\ttfamily\scriptsize,
        frame=lines,
        breaklines=true,
        keywordstyle=\color{blue}\bfseries,
        commentstyle=\color{green}\itshape,
        stringstyle=\color{purple},
        numbers=none
    }
    \begin{lstlisting}
def fib(n):
    """
    Returns the n number in the Fibonacci series
    """
    return fib1(n-1) + fib2(n)
    \end{lstlisting}
    \textbf{Step 1 - Starting point}
\end{minipage}
\hfill
\begin{minipage}{0.32\textwidth}
    \centering
    \lstset{
        language=Python,
        basicstyle=\ttfamily\scriptsize,
        frame=lines,
        breaklines=true,
        keywordstyle=\color{blue}\bfseries,
        commentstyle=\color{green}\itshape,
        stringstyle=\color{purple},
        numbers=none
    }
    \begin{lstlisting}
def fib(n):
    """
    Returns the n number in the Fibonacci series
    """
    if n==1:
        return 0
    if n==2:
        return 1
    return fib(n-1) + fib(n-2)        
    \end{lstlisting}
    \textbf{Step 7 - First correct code}
\end{minipage}
\hfill
\begin{minipage}{0.32\textwidth}
    \centering
    \lstset{
        language=Python,
        basicstyle=\ttfamily\scriptsize,
        frame=lines,
        breaklines=true,
        keywordstyle=\color{blue}\bfseries,
        commentstyle=\color{green}\itshape,
        stringstyle=\color{purple},
        numbers=none
    }
    \begin{lstlisting}
def fib(n):
    """
    Returns the n number in the Fibonacci series
    """
    if n <= 1:
        return n
    a, b = 0, 1
    for i in range(2, n + 1):
        a, b = b, a + b
    return b
    \end{lstlisting}
    \textbf{Step 25 - Convergence}
\end{minipage}

\caption{Generated code samples by OHGL algorithm}
\end{figure}

% \begin{figure}[h]
% \centering
% \begin{minipage}{0.32\textwidth}
%     \centering
%     \begin{minted}[fontsize=\scriptsize, frame=lines, breaklines]{python}
%     def fib(n):
%         """
%         Returns the n number in the Fibonacci series
%         """
%         return fib1(n-1) + fib2(n)
%     \end{minted}
%     \textbf{Step 1 - Starting point}
% \end{minipage}
% \hfill
% \begin{minipage}{0.32\textwidth}
%     \centering
%     \begin{minted}[fontsize=\scriptsize, frame=lines, breaklines]{python}
%     def fib(n):
%         """
%         Returns the n number in the Fibonacci series
%         """
%         if n==1:
%             return 0
%         if n==2:
%             return 1
%         return fib(n-1) + fib(n-2)        
%     \end{minted}
%     \textbf{Step 7 - First correct code}
% \end{minipage}
% \hfill
% \begin{minipage}{0.32\textwidth}
%     \centering
%     \begin{minted}[fontsize=\scriptsize, frame=lines, breaklines]{python}
%     def fib(n):
%         """
%         Returns the n number in the Fibonacci series
%         """
%         if n <= 1:
%             return n
%         a, b = 0, 1
%         for i in range(2, n + 1):
%             a, b = b, a + b
%         return b
%     \end{minted}
%     \textbf{Step 25 - Convergence}
% \end{minipage}

% \caption{Generated code samples by OHGL algorithm}
% \end{figure}

\textbf{Line-Level Efficiency Enhancements:}
We tested the model's ability to implement small code efficiencies, such as replacing traditional for-loops with list comprehensions. The algorithm optimized the order of four functions—`initialize`, `start`, `activate`, and `stop`—each with eight variants, minimizing overall runtime by optimizing the function order.

\textbf{Code Force:} For a more complex problem, we used the Count Triplets challenge from Codeforces\footnote{\url{https://codeforces.com/}}. While the model initially struggled, once it found a correct solution, the algorithm further optimized it for runtime performance \ref{code_force_generated_code}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/code_gen/convergence_of_code_generation_ohgl.pdf}
    \caption{Code generation experiments: penalty over time.}
    \label{fig:code_opt_convergence}
\end{figure}

\textbf{Discussion:} Our method demonstrates the ability to generate correct solutions while applying micro-optimizations for efficiency. In simple tasks like Fibonacci, OGL converged on an optimal solution \ref{fig:code_opt_convergence}, and in more complex problems, it improved the initial solutions. However, in more complex tasks, the LLM may generate code that fails to solve the problem, hindering the optimization of the run-time. To address this, either stronger models or methods focused on optimizing solution correctness are needed. This would ensure that valid solutions are generated first, which can then be further optimized for performance.