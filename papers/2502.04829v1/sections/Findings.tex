\section{OHGL} 
\label{findings}
In this section, we combine the optimistic approach from Sec. \ref{OGL} and the Hessian corrections from Sec. \ref{improvements}. However, unlike previous sections, we will present the practical implementation where integrals are replaced with Monte-Carlo sums of sampled pairs. In this case, the loss function which is used to obtain the Optimistic Higher-order Gradient (OHGL) model is
% \[
% \mathcal{L}_{\varepsilon}^{OHGL}(\theta) = \sum_{\|x_i - x_j\| \leq \varepsilon} W_f(x_i, x_j) \times \mathcal{R}_{HGL}(x_i, x_j)
% \]
\begin{equation}
\label{eq:taylor_loss}
\mathcal{L}_{\varepsilon}^{OHGL}(\theta) = \sum_{\|x_i - x_j\| \leq \varepsilon} W_f(x_i, x_j)\mathcal{R}_{g_\theta, x_i}^{HGL}(x_i - x_j)^2
\end{equation}
The summation is applied over sampled pairs which satisfy \(\|x_i - x_j\|\leq\varepsilon\). As explained in Sec. \ref{improvements}, we detach the Jacobian of \(g_{\theta}\) from the computational graph to avoid second-order derivatives. Our final algorithm is outlined in Alg. \ref{ohgl_alg} and an extended version including additional technical details is found in the appendix (See Alg. \ref{code:full OHGL}).

\begin{algorithm}[tb]
   \caption{Higher-Order Gradient Learning Algorithm}
   \label{ohgl_alg}
\begin{algorithmic}
   \STATE \textbf{Input:} $x_0, \Omega, \alpha, \epsilon_0, \gamma_{\alpha}, \gamma_{\epsilon}, n_{\max}, \lambda, \text{Budget}$
   \STATE $k \gets 0, j \gets 0, \Omega_j \gets \Omega$
   \WHILE{$\text{Budget} > 0$}
      \STATE \textbf{Explore:} Generate samples $\mathcal{D}_k = \{\tilde{x}_i, y_i\}_{i=1}^m$ 
      \STATE \textbf{Create Dataset:} Select $m$ tuples $\mathcal{T}$ from $\mathcal{D}_k$
      \STATE \textbf{Weighted Output:} Assign weights $w_i$ and apply squashing function $\tilde{y}_i = r_k(w_i y_i)$
      \STATE \textbf{Higher-Order GL:} Minimize loss (Eq. \ref{eq:taylor_loss})
      \STATE \textbf{Update solution:} $x_{k+1} \gets x_k - \alpha \tilde{g}_{\theta_k}(x_k)$
      \IF{$f(\tilde{x}_{k+1}) > f(\tilde{x}_k)$ for $n_{\max}$ times}
         \STATE Generate new trust region $\Omega_{j+1}$ and update $\epsilon_j$
      \ENDIF
      \IF{$f(\tilde{x}_k) < f(\tilde{x}_{\text{best}})$}
         \STATE Update $x_{\text{best}}$
      \ENDIF
      \STATE $k \gets k + 1; \text{Budget} \gets \text{Budget} - m$
   \ENDWHILE
   \STATE \textbf{Return:} $x_{\text{best}}$
\end{algorithmic}
\label{ohgl_alg}
\end{algorithm}