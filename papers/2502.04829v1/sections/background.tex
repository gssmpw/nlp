XX\section{Background} \label{background}
The goal of black-box optimization (BBO) is to minimize a target function \(f(x)\) through a series of evaluations \cite{audet2017introduction}, over a predefined domain $\Omega$:
\begin{equation}
    \text{find:}\ \  x^\ast=\arg\min_{x \in \Omega}f(x)
\end{equation}
The Explicit Gradient Learning method, as proposed by \cite{sarafian2020explicit} leverages the first-order Taylorâ€™s expansion: $f(y) = f(x) + \nabla f(x)^\top(y-x) + R_1(x, y)$. Here, $R_1(x, y)=O(\|y-x\|^2)$ is a higher-order residual. By minimizing the residual term with a surrogate neural network model, EGL learns the \textit{mean-gradient}: a smooth approximation of the function's gradient
\begin{equation}
\begin{aligned}
\label{EGL_surrogate}
 g^{EGL}_{\varepsilon}(x) = \arg\min_{g_{\theta}:\mathbb{R}^n \rightarrow \mathbb{R}^n}\int_{\tau \in B_\varepsilon(0)} (\mathcal{R}_{g_\theta, x}^{EGL}(\tau))^2 d\tau\\
\mathcal{R}_{g_\theta, x}^{EGL}(\tau) = f(x) - f(x + \tau) + g_{\theta}(x)^\top \tau
\end{aligned}
\end{equation}
\begin{figure}[ht]
    \vskip 0.2in
    \centering
    \includegraphics[width=\columnwidth]{images/compare_versions/compare_algorithms_compare_tr_cma_0.pdf}
    \caption{Comparing the effect of trust region. Trust region significantly improves both EGL and CMA algorithms.}
    \label{fig:compare_effects}
    \vskip -0.2in
\end{figure}

where \(B_\varepsilon(0)\) is a ball around \(0\). As \(\varepsilon\to 0\), the mean-gradient converges to \(\nabla f\). This property lets EGL explore the entire landscape, to find lower regions in the function when \(\varepsilon\) is sufficiently large and converge to a local minimum when \(\varepsilon \to 0\).

A key component of the EGL algorithm is the \textit{trust region} (TR), which restricts the search space around the current estimate. This region standardizes input-output statistics, enhancing the neural network's effectiveness. While \cite{sarafian2020explicit} suggested the TR framework as part of the EGL algorithm, TR is not exclusive to EGL and can significantly improve other algorithms. In our work, we created stronger baseline algorithms by adding TR to the classic Covariance Matrix Adaptation (CMA) algorithm (see Appendix: Algorithm \ref{CMA_TR_ALG}). Notably, this seemingly minor change to CMA significantly improves its performance and outperforms vanilla EGL in the COCO optimization suite, as shown in Fig. \ref{fig:compare_effects}.