\section{Introduction} \label{intro}
Black-box optimization (BBO) is the process of searching for optimal solutions within a system's input domain without access to its internal structure or analytical properties \cite{audet2017introduction}. Unlike gradient-based optimization methods that rely on the calculation of analytical gradients, BBO algorithms query the system solely through input-output pairs, operating agnostically to the underlying function. This feature distinguishes BBO from traditional ML tasks, such as neural network training, where optimization typically involves backpropagation-based gradient computation.

Many real-world physical systems naturally fit into the BBO framework because their analytical behavior is difficult or impossible to model explicitly. In these cases, BBO algorithms have achieved remarkable success in diverse fields, such as ambulance deployment \cite{zhen2014simulation}, robotic motor control \cite{gehring2014towards, prabhu2018survey}, parameter tuning \cite{olof2018comparative, rimon2024mamba}, and signal processing \cite{zoReview}, among others \cite{alarie2021two}. BBO applications extend beyond physical systems; many ML problems exhibit a black-box nature when the true gradient is absent. Examples include hyperparameter tuning \cite{bischl2023hyperparameter}, contextual bandit problems \cite{bouneffouf2020survey}, and large language model training with human feedback \cite{bai2022training}, to name a few.

As the scale of data continues to grow, the dimensionality of the problem space increases in tandem. This trend is particularly evident in ML, where model architectures, embedding and latent representation sizes, and the number of hyperparameters are continually expanding. High-dimensional optimization challenges traditional BBO algorithms, which often require the number of collected samples at each step, $n_s$, to scale proportionally with the problem size, $N$. Parametric neural models, however, have shown that reusing past samples can effectively reduce the required sample size such that $n_s \ll N$ \cite{sarafian2020explicit,lu2023opt}. Building on this, we propose a sampling profiler that further reduces the number of samples needed at each step while maintaining performance.

While problem dimensions increase, the cost of evaluating intermediate solutions remains a critical constraint, especially in real-world settings where interaction with the environment is expensive or in ML tasks where larger models counterbalance gains in computational power. Therefore, modern BBO algorithms must not only reduce evaluation steps but also converge more quickly \cite{hansen2010comparing}. Achieving this requires algorithms capable of more accurately predicting optimization directions, either through better gradient approximation \cite{anil2020scalable,lesage2020second} or momentum-based strategies to handle non-convexity and noise. In this paper, we propose two key improvements to Explicit Gradient Learning (EGL): (1) Optimistic Gradient Learning (OGL), a weighted gradient estimator that biases toward promising solutions and (2) Higher-Order Gradient Learning (HGL), which incorporates Hessian corrections to yield more accurate gradient approximations.

% We combine OGL and HGL to a unified algorithm termed OHGL and show 4 promising characteristics:
% \begin{enumerate}
%     \item \textbf{Robustness}: OHGL outperforms all baseline algorithms in terms of the number of solved problems on the test suite and real-world ML applications.
%     \item \textbf{Gradient Precision}: OHGL achieves more accurate gradient estimation compared to EGL.
%     \item \textbf{Convergence Rate}: OHGL demonstrates faster convergence rate.
%     \item \textbf{Higher-Dimensions}: Via the sampling profiler and the optimistic approach OGL is able to solve high dimensional problems with less budget.
% \end{enumerate}
% Finally, we present 2 real-world, ML-based, high-dimensional applications and compare our method against SOTA approaches and showcase the advantage of our algorithm.

We combine the strengths of OGL and HGL to a unified algorithm termed OHGL which exhibits four key advantages:

\begin{itemize}
    \item \textbf{Robustness}: OHGL consistently outperforms baseline algorithms across a diverse range of benchmark problems, including synthetic test suites and real-world ML applications. Its ability to handle noisy and non-convex environments.
    \item \textbf{Gradient Precision}: By integrating the second-order information via Hessian corrections, OHGL achieves significantly more accurate gradient approximations than standard EGL.
    \item \textbf{Convergence Rate}: OHGL demonstrates faster convergence rate.
    \item Utilizing the sampling profiler and the optimistic approach, OGL is able to solve \textbf{high-dimensional problems} with smaller budget and \textbf{converge faster} than baseline algorithms. 
\end{itemize}


\textbf{Related works:}
Black-box optimization (BBO) algorithms have a long history, with various approaches developed over the years. Some of the foundational techniques include grid search, coordinate search \cite{audet2017searchmethods}, simulated annealing \cite{busetti2003simulated}, and direct search methods like Generalized Pattern Search and Mesh Adaptive direct search \cite{audet2017direct}, Gradient-less descent \cite{golovin2019gradientless}, and ZOO \cite{chen2017zoo}. These approaches iteratively evaluate potential solutions and decide whether to continue in the same direction. However, they resample for every step and don't use the sampled budget from previous iterations, wasting a lot of budget.

Another prominent family of BBO algorithms is the genetic algorithm family \cite{back1996evolutionary}. This includes methods such as Covariance Matrix Adaptation (CMA) \cite{hansen2016cma} and Particle Swarm Optimization (PSO) \cite{clerc2010particle}. These algorithms simulate the process of natural evolution, where a population of solutions evolves through mutation and selection \cite{audet2017genetic}. They are considered state-of-the-art (SOTA) in optimization due to their effectiveness in tackling complex problems. However, they come with significant drawbacks, particularly the need for extensive fine-tuning of parameters like generation size and mutation rates. CMA, for example, struggles in higher-dimensional environments and requires careful adjustment of hyperparameters and guidance to perform optimally \cite{loshchilov2013bi, tang2021guiding}. In this work, we propose a simpler method to enhance the performance of CMA, particularly in high-dimensional settings.

Then there are model-based methods \cite{audet2017model}, which attempt to emulate the behavior of the function using a surrogate model. These models provide important analytical information, such as gradients \cite{bertsekas2015convex}, to guide the optimization process and help find a minimum. Within this class, we can further distinguish two sub-classes.
To address the issue of dimensionality, Explicit Gradient Learning (EGL) was proposed by \cite{sarafian2020explicit}. While many model-based methods focus on learning the function’s structure to derive analytical insights (e.g., Indirect Gradient Learning or IGL \cite{lillicrap2015continuous, sarafian2020explicit}), EGL directly learns the gradient information. EGL uses Taylor's theorem to estimate the gradient. The authors also emphasize the importance of utilizing a trust region to handle black-box optimization problems. However, EGL has some drawbacks: it often uses the available budget inefficiently, disregarding both the complexity and dimensionality of the environment. Additionally, the datasets created by EGL can be naive, leading to over-fitting or improper network learning. This work tackles these issues by showing the importance of proper algorithm calibration and optimization.

Recent work also highlights the limitations of common assumptions in BBO algorithms, such as continuity or Gaussian distributions of functions, which can hinder optimization. For instance, OPT-GAN \cite{tang2021guiding}, a generative model, seeks to bypass these assumptions by learning a function’s distribution and generating better candidate solutions based on that knowledge.

\textbf{The paper is organized as follows:} Section \ref{background} covers the algorithm’s theoretical background and mathematical foundations. Sections \ref{OGL} and \ref{improvements} present our two enhanced variants of the gradient learning algorithm: OGL and HGL, these are followed by section \ref{findings} where we present the full algorithm OHGL. Section \ref{experiment} provides experimental results on the synthetic COCO test suite and Section \ref{applications} highlights 2 real-world high-dimensional applications and potential uses. Finally, section \ref{conclusion} concludes and suggests future research directions. Our code, experiments, and environment setup are available in the supplementary material.
