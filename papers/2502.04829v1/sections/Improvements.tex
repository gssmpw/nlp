\section{Optimistic Gradient Learning with Weighted Gradients}
\label{OGL}
While local search algorithms like EGL are based on the notion that the gradient descent path is the optimal search path, following the true gradient path may not be the optimal approach as it can lead to inefficient sampling,  susceptibility to shallow local minima, and difficulty navigating through narrow ravines. To illustrate, after sampling a batch \(D\) of pairs \(\{
(x_i, f(x_i))\}_{i\in D}\) around our current solution \(x\), a plausible and optimistic heuristic can be to direct the search path towards low regions in the sampled function landscape regardless of the local curvature around \(x\). In other words, to take a sensible guess and bias the search path towards the lower \((x_i,f(x_i))\) samples. To that end, we define the Optimistic Gradient Learning objective by adding an \textit{importance sampling} weight to the integral of Eq. \ref{EGL_surrogate}
\begin{equation}
\label{ogl_objective}
 g^{OGL}_{\varepsilon}(x) = \argmin_{g_{\theta}:\mathbb{R}^n \rightarrow \mathbb{R}^n} \int\limits_{\tau \in B_\varepsilon(0)} W_f(x,x + \tau) \cdot 
    \mathcal{R}_{g_\theta, x}^{EGL}(\tau)^2 d\tau
\end{equation}
The importance sampling factor \(W_f\) should be chosen s.t. it biases the optimization path towards lower regions regardless of the local curvature around \(x\), i.e. $W(x, y_1) \geq W(x, y_2)$ for \(f(y_1) \leq f(y_2)\). In our implementation, we design \(W_f\) as a softmax function over the sampled batch of exploration points $\{y_i\}\in D$ around \(x\)
\begin{equation}
W_f(x,y_i) = \frac{e^{-\min(f(x), f(y_i))}}{\sum_{y_j\in D} e^{-f(y_j)}}
\end{equation}

Notice that in practice, the theoretical objective in Eq. \ref{ogl_objective} is replaced by a sampled Monte-Carlo version (see Sec. \ref{findings}) s.t. the sum of all weights across the sampled batch is smaller than 1. In the following theorem, we show that the \textit{controllable accuracy } property of EGL, which implies that the mean-gradient converges to the true gradient still holds for our biased version, s.t. when \(\varepsilon \to 0\), \(g^{OGL}_{\varepsilon} \to \nabla f(x)\) this guarantees that the convergence properties of the mean-gradient still hold
\begin{theorem} (Optimistic Controllable Accuracy)
    For any differentiable function \(f\) with a continuous gradient, there exists \( \kappa^{OGL} > 0 \) such that for any \(\varepsilon > 0\), \(g^{OGL}_{\varepsilon}(x)\) satisfies
\[
\| g^{OGL}_\varepsilon(x) - \nabla f(x) \| \leq \kappa^{OGL} \varepsilon \quad \text{for all } x \in \Omega.
\]
\end{theorem}
In  Fig. \ref{fig: compare egl and weights} we plot 4 typical trajectories of EGL and OGL, demonstrating why we can benefit from biasing the gradient in practice and on average. We find that the biased version avoids getting trapped in local minima and avoids long travels through narrow ravines which rapidly consume the sampling budget. In Fig. \ref{fig: close to min} we find statistically that  OGL tends to progress faster and closer to the global minimum while EGL diverges from the global minimum in favor of local minima. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{images/weighted_convergence/convergence_compare_on_4_final__0.pdf}
    \caption{OGL vs EGL trajectories. 1st row: Gallagher’s Gaussian 101-me. 2nd row: 21-hi.}
    \label{fig: compare egl and weights}
\end{figure}
    
\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{images/weighted_convergence/convergence_distance.pdf}
    \caption{The average Euclidean distance at each step between the algorithm and the global minimum.}
    \label{fig: close to min}
\end{figure}

\section{Gradient Learning with Hessian Corrections}
\label{improvements}

To learn the mean-gradient, EGL minimizes the first-order Taylor residual (see Sec. \ref{background}). Utilizing higher-order approximations has the potential of learning more accurate models for the mean-gradient. Specifically, the second-order Taylor expansion is
\begin{equation}
\label{gradient_training_formula}
f(x + \tau) = f(x) + \nabla f(x)^\top \tau + \frac{1}{2} \tau^\top \nabla^2 f(x) \tau + R_2(x,x + \tau) \notag
\end{equation} 
Here \(R_2(x,y)=O(\|x-y\|^3)\) is the second order residual. Like in EGL, we replace \(\nabla f\) with a surrogate model \(g_{\theta}\) and minimize the surrogate residual to obtain our Higher-order Gradient Learning (HGL) variant
% \begin{equation}
% \begin{aligned}
% g^{HGL}_{\varepsilon}(x) = \argmin_{g_{\theta}:\mathbb{R}^n \rightarrow \mathbb{R}^n} \int_{\tau \in B_\varepsilon(0)}\mathcal{R}_{g_\theta, x}^{HGL}(\tau)^2d\tau \\
% \mathcal{R}_{g_\theta, x}^{HGL}(\tau) = f(x) - f(x + \tau) + g_{\theta}(x)^\top \tau + \frac{1}{2}\tau^\top J_{g_{\theta}}(x) \tau
% \end{aligned}
% \end{equation}
\begin{align}
g^{HGL}_{\varepsilon}(x) &= \argmin_{g_{\theta}:\mathbb{R}^n \rightarrow \mathbb{R}^n} \int_{\tau \in B_\varepsilon(0)}\mathcal{R}_{g_\theta, x}^{HGL}(\tau)^2d\tau \\
\mathcal{R}_{g_\theta, x}^{HGL}(\tau) &= f(x) - f(x + \tau) + g_{\theta}(x)^\top \tau + \frac{1}{2}\tau^\top J_{g_{\theta}}(x) \tau \notag
\end{align}

The new higher-order term \(J_{g_{\theta}}(x)\) is the Jacobean of $g_{\theta}(x)$, evaluated at $x$ which approximates the function's Hessian matrix in the vicinity of our current solution, i.e., $J_{g_{\theta}}(x) \approx \nabla^2 f(x)$. Next, we show theoretically that as expected, HGL converges faster to the true gradient which amounts to lower gradient error in practice.\footnote{Notice that, while HGL incorporates Hessian corrections during the gradient learning phase, 
Unlike  Newton’s methods, it is not used for scaling the gradient step size. Scaling the gradient requires calculation of the inverse Hessian
which is prone to numerical challenges and instabilities and in our experiments was found less effective than the HGL approach of minimizing the Taylor residual term.}
\begin{theorem} (Improved Controllable Accuracy):
For any twice differentiable function \( f \in \mathcal{C}^2 \), there exists \( \kappa_{HGL} > 0 \) such that for any \( \varepsilon > 0 \), the second-order mean-gradient \( g^{HGL}_{\varepsilon}(x) \) satisfies
\[
\| g^{HGL}_{\varepsilon}(x) - \nabla f(x) \| \leq \kappa_{HGL}\varepsilon^2 \quad \text{for all } x \in \Omega.
\]
\end{theorem}

In other words, in HGL the model error is in an order of magnitude of \(\varepsilon^2\) instead of \(\varepsilon\) in EGL and OGL. In practice, we verified that this property of HGL translates to more accurate gradient models.

Learning the gradient with the Jacobian corrections introduces a computational challenge as double backpropagation can be expensive. This overhead can hinder the scalability and practical application of the method. A swift remedy is to detach the Jacobian matrix from the competition graph. While this step slightly changes the objective's gradient (i.e. the gradient trough (\(\mathcal{R}^{HGL}\)) it removes the second-order derivative and in practice, we found that it achieves similar results compared to the full backpropagation through the residual \(\mathcal{R}^{HGL}\), see Fig. \ref{fig:experiements_results}(b).
