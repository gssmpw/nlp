\subsection{Theoretical Analysis}
\subsubsection{Mean Gradient Accuracy}
\label{mean_gradient_acc_proof}
\begin{definition}
       The second-order mean gradient around x of radius $\epsilon > 0$:   \[
   \tilde{g}_{\varepsilon}(x) = \arg\min_{g \in \mathbb{R}^n} \int_{V_{\varepsilon}(x)} \left| g^\top \tau + \tfrac{1}{2} \tau^\top J(g) \tau - [ f(x + \tau) - f(x) ] \right|^2 d\tau.
   \]
\end{definition}
\begin{theorem} (Improved Controllable Accuracy):
For any twice differentiable function \( f \in \mathcal{C}^2 \), there exists \( \kappa_g(x) > 0 \) such that for any \( \varepsilon > 0 \), the second-order mean-gradient \( \tilde{g}_{\varepsilon}(x) \) satisfies
\[
\| \tilde{g}_{\varepsilon}(x) - \nabla f(x) \| \leq \kappa_g(x) \varepsilon^2 \quad \text{for all } x \in \Omega.
\]
\end{theorem}
\textit{Proof.}
   Since \( f \in \mathcal{C}^2 \), the Taylor expansion around \( x \) is\[
   f(x + \tau) = f(x) + \nabla f(x)^\top \tau + \tfrac{1}{2} \tau^\top H(x) \tau + R_{x}(\tau),
   \]
   where \( H(x) \) is the Hessian matrix at \( x \), and the remainder \( R_{x}(\tau) \) satisfies \[
   | R_{x}(\tau) | \leq \tfrac{1}{6} k_g \| \tau \|^3,
   \]
By the Definition of \(g_\epsilon\), an upper bound of \(\mathcal{L}(g_\epsilon(x))\)  is:   \[
   \mathcal{L}(g_\epsilon(x)) \leq \mathcal{L}(\nabla f(x)) = \int_{V_{\varepsilon}(x)} \Big| \nabla f(x)^\top \tau + \tfrac{1}{2} \tau^\top H(x) \tau - \big( f(x + \tau) - f(x) \big) \Big|^2 d\tau = \int_{V_{\varepsilon}(x)} | R_{x}(\tau) |^2 d\tau
\]
\[
   \leq \left( \tfrac{1}{6} k_g \right)^2 \int_{V_{\varepsilon}(x)} \| \tau \|^6 d\tau = \int_0^\varepsilon \int_{S_{n-1}} r^{n-1}r^6 drd\Omega= \left( \tfrac{1}{6} k_g \right)^2 \tfrac{1}{n+6}\omega_n \varepsilon^{n+6}.
\]
Where  \(\omega_n = \tfrac{2\pi^{n/2}}{\Gamma(n/2)}\) is the surface area of the unit sphere \(S^{n-1}\).
We now find a lower bound:
   
lets define for convenience \(\delta_g = (g_{\varepsilon}(x)-\nabla f(x))\) , \( \delta_H =  J(g_{\varepsilon}(x)) - H(x) \) 
\begin{align*}
    \mathcal{L}(g_{\varepsilon}(x)) &= \int_{V_{\varepsilon}(x)}|g_{\varepsilon}(x)\tau-\nabla f(x)\tau+\nabla f(x)\tau + \tfrac{1}{2}\tau  J(g_{\varepsilon}(x))\tau - \\& \tfrac{1}{2}\tau H(x) \tau + \tfrac{1}{2}\tau H(x)\tau - f(x+\tau)+f(x)|^2d\tau \\
& = \int_{V_{\varepsilon}(x)} (\delta_g^\top\tau + \tfrac{1}{2}\tau^\top\delta_H\tau - R_x(\tau))^2d\tau \\
& = \int_{V_{\varepsilon}(x)}((\delta_g^\top\tau)^2 + \tfrac{1}{4}(\tau^\top\delta_H\tau)^2 + R_x(\tau)^2 - 2\delta_g^\top\tau R_x(\tau) - \tau^\top\delta_H\tau  R_x(\tau) + \tau^\top\delta_g\tau^\top\delta_H\tau)d\tau\\
\end{align*}
Since \(a^2 + b^2 \geq 2ab\) we conclude that \(\tfrac{1}{4}(\tau^\top\delta_H\tau)^2 + R_x(\tau)^2 - \tau^\top\delta_H\tau R_x(\tau) \geq 0\)
\begin{align*}
    \mathcal{L}(g_{\varepsilon}(x)) & = \int_{V_{\varepsilon}(x)}((\delta_g^\top\tau)^2 + \tfrac{1}{4}(\tau^\top\delta_H\tau)^2 + R_x(\tau)^2 - 2\delta_g^\top\tau R_x(\tau) - \tau^\top\delta_H\tau  R_x(\tau) + \tau^\top\delta_g\tau^\top\delta_H\tau)d\tau\\
    & \geq \int_{V_{\varepsilon}(x)}((\delta_g^\top\tau)^2 - 2\delta_g^\top\tau R_x(\tau) + \tau^\top\delta_g\tau^\top\delta_H\tau)d\tau\\
    & = \int_{V_{\varepsilon}(x)}(\delta_g^\top\tau)^2d\tau - 2\int_{V_{\varepsilon}(x)}\delta_g^\top\tau R_x(\tau)d\tau + \int_{V_{\varepsilon}(x)}\tau^\top\delta_g\tau^\top\delta_H\tau d\tau\\
    & = \int_{V_{\varepsilon}(x)}(\delta_g^\top\tau)^2d\tau - 2\int_{V_{\varepsilon}(x)}\delta_g^\top\tau R_x(\tau)d\tau + \int_{V_{\varepsilon}(x)}\tau^\top\delta_g\tau^\top\delta_H\tau d\tau\\
\end{align*}

We will split the equation into 3 components:
First \(A =  \int_{V_{\varepsilon}(x)}(\delta_g^\top\tau)^2d\tau\) ,  is we open this expression we see that
\[
(\delta_g^\top\tau)^2 = \sum_{i=1}^n \sum_{j=1}^n (\delta_g)_i (\delta_g)_j \tau_i \tau_j.
\]
Since \(V_{\varepsilon}(x)\) is symmetric around \(x\) odd moments of \(\tau\) integrate to zero, therefore
\begin{align*}
A &=  \int_{V_{\varepsilon}(x)}(\delta_g^\top\tau)^2d\tau = \int_{V_{\varepsilon}(x)}(\sum_{i=1}^n \sum_{j=1}^n (\delta_g)_i (\delta_g)_j \tau_i \tau_j) d\tau= \int_{V_{\varepsilon}(x)}(\sum_{i=1}^n  (\delta_g)_i^2 \tau_i^2 ) d\tau = \\
& = \sum_{i=1}^n  (\delta_g)_i^2\int_{V_{\varepsilon}(x)}\tau_i^2  d\tau =_* \frac{1}{n}\sum_{i=1}^n  (\delta_g)_i^2 \omega_n\int_{0}^\varepsilon r^{n+1}dr =  \sum_{i=1}^n  (\delta_g)_i^2 \omega_n \frac{1}{n(n+2)}\varepsilon^{n+2} =\\
& = \|\delta_g\|^2\omega_n \frac{1}{n(n+2)}\varepsilon^{n+2}
\end{align*}
Transition * stems from the fact that the integral over all\(\tau_i\) is the same due to the ball's symmetry. In other words 
\[\int_{V_{\varepsilon}(x)}\tau_i^2 d\tau = \frac{1}{n}\int_{V_{\varepsilon}(x)} n*\tau_i^2 d\tau  = \frac{1}{n}\int_{V_{\varepsilon}(x)} \sum_{j=0}^n\tau_j^2  d\tau = \frac{1}{n}\int_{V_{\varepsilon}(x)} \|\tau\| d\tau\]
and therefore
\[
\int_{V_{\varepsilon}(x)}\tau_i^2 d\tau = \frac{1}{n}\int_{V_{\varepsilon}(x)}\|\tau\|^2 d\tau = \frac{1}{n}\int_{0}^\varepsilon \int_{S_{n-1}}r^{n-1}r^2dr d\Omega = \frac{1}{n}\omega_n \int_{0}^\varepsilon r^{n+1}dr
\]

Let \(B = 2\int_{V_{\varepsilon}(x)}\delta_g^\top\tau R_x(\tau)d\tau\) 
\begin{align*}
    B &= 2\int_{V_{\varepsilon}(x)}\delta_g^\top\tau R_x(\tau)d\tau \leq 2\int_{V_{\varepsilon}(x)}\|\delta_g\|\cdot\|\tau\|\cdot\| R_x(\tau)\|d\tau  \leq 2\|\delta_g\|\int_{V_{\varepsilon}(x)}\|\tau\|\tfrac{1}{6}k_g\|\tau\|^3d\tau \\
    & \leq \tfrac{1}{3}k_g\|\delta_g\|\int_{V_{\varepsilon}(x)}\|\tau\|^4d\tau = \omega_n\int_0^\varepsilon r^{n-1}r^4dr = \tfrac{1}{3(n+4)}k_g\omega_n\|\delta_g\|\varepsilon^{n+4}
\end{align*}
Finally \(C = \int_{V_{\varepsilon}(x)}\tau^\top\delta_g\tau^\top\delta_H\tau d\tau\), we should remember the property derived from \(V_{\varepsilon}(x)\) symmetry. If we open the vector multiplication we get
\[
\tau^\top \delta_g \tau^\top \delta_H \tau = \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n \tau_i (\delta_g)_i (\delta_H)_{jk} \tau_j \tau_k = \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n (\delta_g)_i (\delta_H)_{jk} \tau_i \tau_j \tau_k.
\]
Here there is no way for an even moment of \(\tau\) to exist so \(C = 0\)
To sum this up, we get
\begin{align*}
    \mathcal{L}(g_{\varepsilon}(x)) &\geq A - B + C \geq \|\delta_g\|^2\omega_n \frac{1}{n(n+2)}\varepsilon^{n+2} - \tfrac{1}{3(n+4)}k_g\omega_n\|\delta_g\|\varepsilon^{n+4}
\end{align*}
We combine the lower and upper-bound 
\begin{align*}
    & \|\delta_g\|^2\omega_n \frac{1}{n(n+2)}\varepsilon^{n+2} - \tfrac{1}{3(n+4)}k_g\omega_n\|\delta_g\|\varepsilon^{n+4} \leq \left( \tfrac{1}{6} k_g \right)^2 \tfrac{1}{n+6}\omega_n \varepsilon^{n+6}\\
    & \|\delta_g\|^2 \frac{1}{n(n+2)} - \tfrac{1}{3(n+4)}k_g\|\delta_g\|\varepsilon^{2} - ( \tfrac{1}{6} k_g )^2 \tfrac{1}{n+6}\omega_n \varepsilon^{4} \leq 0\\
     &\|\delta_g\| \leq \frac{\tfrac{1}{3(n+4)}k_g\varepsilon^{2} + \sqrt{(\tfrac{1}{3(n+4)}k_g\varepsilon^{2})^2 + 4(\tfrac{1}{6} k_g )^2 \tfrac{1}{n+6}\varepsilon^{4} \frac{1}{n(n+2)}}}{2\frac{1}{n(n+2)}} = \frac{1}{3}\varepsilon^2 k_g\frac{\tfrac{1}{(n+4)} + \sqrt{\tfrac{1}{(n+4)^2} + \tfrac{1}{n+6} \frac{1}{n(n+2)}}}{2\frac{1}{n(n+2)}} \\
     & \|\delta_g\| \leq \frac{1}{3}\varepsilon^2 k_g \tfrac{\frac{1}{n+4} + \sqrt{\frac{4}{(n+4)^2}}}{2n(n+2)} = \frac{1}{2}n\varepsilon^2 k_g
\end{align*}

Now we show \(\|\delta_H\| \leq k_g\varepsilon\)
\begin{align*}
    \mathcal{L}(g_{\varepsilon}(x)) & = \int_{V_{\varepsilon}(x)}((\delta_g^\top\tau)^2 + \tfrac{1}{4}(\tau^\top\delta_H\tau)^2 + R_x(\tau)^2 - 2\delta_g^\top\tau R_x(\tau) - \tau^\top\delta_H\tau  R_x(\tau) + \tau^\top\delta_g\tau^\top\delta_H\tau)d\tau\\
    & \geq \int_{V_{\varepsilon}(x)}(\tfrac{1}{4}(\tau^\top\delta_H\tau)^2 - \tau^\top\delta_H\tau  R_x(\tau) + \tau^\top\delta_g\tau^\top\delta_H\tau)d\tau\\
    & = \int_{V_{\varepsilon}(x)}(\tfrac{1}{4}(\tau^\top\delta_H\tau)^2 - 
    \tau^\top\delta_H\tau  R_x(\tau))d\tau\\
    & = \tfrac{1}{4}\int_{V_{\varepsilon}(x)}(\tau^\top\delta_H\tau)^2d\tau - \int_{V_{\varepsilon}(x)}\tau^\top\delta_H\tau  R_x(\tau)d\tau\\
\end{align*}
\begin{align*}
    A &= \int_{V_{\varepsilon}(x)}(\tau^\top\delta_H\tau)^2 d\tau=  \int_{V_{\varepsilon}(x)} \sum_{i,j,k,l=0}^n \tau_i \tau_j \tau_k \tau_l (\delta_H)_{ij} (\delta_H)_{kl} d\tau \\
    & =_* \int_{V_{\varepsilon}(x)} \sum_{i=0}^n \tau_i^4 (\delta_H)_{ii}^2 d\tau + 2\int_{V_{\varepsilon}(x)} \sum_{i \neq j}^n \tau_i^2 \tau_j^2 (\delta_H)_{ij}^2  d\tau + \int_{V_{\varepsilon}(x)} \sum_{i\neq j}^n \tau_i^2 \tau_j^2 (\delta_H)_{ii} (\delta_H)_{jj} d\tau \\
    & = \sum_{i=0}^n (\delta_H)_{ii}^2 \omega_n \frac{\varepsilon^{n+4}}{n+4}\frac{3}{n(n+2)} + 2\sum_{i \neq j}^n (\delta_H)_{ij}^2  \omega_n \frac{\varepsilon^{n+4}}{n+4}\frac{1}{n(n+2)} + \sum_{i\neq j}^n (\delta_H)_{ii} (\delta_H)_{jj}  \omega_n \frac{\varepsilon^{n+4}}{n+4}\frac{1}{n(n+2)} \\
\end{align*}
Where * is due to the symmetry of the ball, any odd moments of \(\tau\) are equal to 0

Let's denote
\[
M =  \omega_n \frac{\varepsilon^{n+4}}{n+4}\frac{1}{n(n+2)}
\]
\[
S_{diag} = \sum_{i=0}^n (\delta_H)_{ii}^2
\]
\[
S_{off-diag} = \sum_{i\neq j} (\delta_H)_{ij}^2
\]
\[
T  = \sum_{i = 0}^n (\delta_H)_{ii}
\]
We notice that
\[
\sum_{i\neq j}^n (\delta_H)_{ii} (\delta_H)_{jj}  =T^2 - S_{diag}
\]
\[
\|\delta_H\|^2 = S_{diag} + S_{off-diag}
\]
\begin{align*}
    A &= 3S_{diag} \cdot M + 2 M\cdot S_{off-diag} + (T^2 - S_{diag})M = 2 (S_{diag} + S_{off-diag}) + M \cdot T^2 \geq M \|\delta_H\|^2
\end{align*}

\begin{align*}
    B = \int_{V_{\varepsilon}(x)}\tau^\top\delta_H\tau  R_x(\tau)d\tau \leq \frac{1}{6}k_g\|\delta_H\|\int_{V_{\varepsilon}(x)}\|\tau\|^5d\tau = \|\delta_H\|\tfrac{1}{6(n+5)}k_g\omega_n\varepsilon^{n+5}
\end{align*}

\begin{align*}
  &\left( \tfrac{1}{6} k_g \right)^2 \tfrac{1}{n+6}\omega_n \varepsilon^{n+6}  \geq \mathcal{L}(g_{\varepsilon}(x)) \geq \frac{1}{4}A - B \geq  \omega_n \frac{\varepsilon^{n+4}}{n+4}\frac{1}{n(n+2)} \|\delta_H\|^2 - \tfrac{1}{6(n+5)}k_g\omega_n\varepsilon^{n+5}\|\delta_H\|\\
 & \frac{1}{n+4}\frac{1}{n(n+2)} \|\delta_H\|^2 - \tfrac{1}{6(n+5)}k_g\varepsilon\|\delta_H\| - \left( \tfrac{1}{6} k_g \right)^2 \tfrac{1}{n+6} \varepsilon^{2} \leq 0 \\
 & \|\delta_H\| \leq \frac{\frac{1}{6(n+5)} k_g \varepsilon \pm \sqrt{\frac{k_g^2 \varepsilon^2}{36} \left(\frac{1}{(n+5)^2} + \frac{4}{(n+4)n(n+2)(n+6)}\right)}}{\frac{2}{n+4} \frac{1}{n(n+2)}} \leq \tfrac{1}{6}k_g \varepsilon n^2
\end{align*}

\subsubsection{Convergence Analysis}
\label{convergence_proof}
\begin{theorem}
Let $f:\Omega\to\mathbb{R}$ be a convex function with Lipschitz continuous gradient, i.e. $f\in\mathcal{C}^{+1}$ and a Lipschitz constant $\kappa_f$ and let $f(x^{*})$ be its optimal value. Suppose a controllable mean-gradient model $g_{\varepsilon}$ with error constant $\kappa_g$, the gradient descent iteration $x_{k+1} = x_{k} - \alpha g_{\varepsilon}(x_k)$ with a sufficiently small $\alpha$ s.t. $\alpha \leq \min(\frac{1}{\kappa_g}, \frac{1}{\kappa_f})$ guarantees:
\begin{enumerate}
    \item For $\varepsilon \leq \frac{\|\nabla f(x)\|}{5\alpha}$, monotonically decreasing steps s.t. $f(x_{k+1}) \leq f(x_k) -  2.25 \frac{\varepsilon^2}{\alpha}$.
    \item After a finite number of iterations, the descent process yields $x^{\star}$ s.t. $\|\nabla f(x^{\star})\| \leq \frac{5\varepsilon}{\alpha}$.
\end{enumerate}
\end{theorem}
\textit{Proof.}
For a convex function with Lipschitz continuous gradient, the following inequality holds for all $x_k$
\begin{equation}
    f(x) \leq f(x_k) + (x - x_k) \cdot \nabla f(x_k) + \frac{1}{2}\kappa_f\|x - x_k\|^2
\end{equation}
Plugging in the iteration update $x_{k+1} = x_k - \alpha g_{\varepsilon}(x_k) $ we get
\begin{equation}
    f(x_{k+1}) \leq f(x_k) - \alpha g_{\varepsilon}(x_k) \cdot \nabla f(x_k) + \alpha^2 \frac{1}{2}\kappa_f\|g_{\varepsilon}(x_k)\|^2
\end{equation}
For a controllable mean-gradient we can write $\|g_{\varepsilon}(x) - \nabla f(x)\| \leq \varepsilon^2 \kappa_g $, therefore we can write $g_{\varepsilon}(x) = \nabla f(x) + \varepsilon^2 \kappa_g \xi(x)$ s.t. $\|\xi(x)\| \leq 1$ so the inequality is
\begin{equation}
    f(x_{k+1}) \leq f(x_k) - \alpha \|\nabla f(x_k)\|^2 - \alpha \varepsilon^2 \kappa_g \xi(x_k) \cdot \nabla f(x_k) + \alpha^2 \frac{1}{2}\kappa_f\|\nabla f(x) + \varepsilon^2 \kappa_g \xi(x)\|^2
\end{equation}
Using the equality $\|a+b\|^2 = \|a\|^2 + 2 a \cdot b + \|b\|^2$ and the Cauchy-Schwartz inequality inequality $a \cdot b\leq\|a\|\|b\|$ we can write
\begin{equation}
\begin{aligned}
    f(x_{k+1}) & \leq f(x_k) - \alpha \|\nabla f(x_k)\|^2 - \alpha \varepsilon^2 \kappa_g \|\xi(x_k)\|\cdot\|\nabla f(x_k)\| \\& + \alpha^2 \frac{1}{2}\kappa_f\left(\|\nabla f(x)\|^2 + 2 \varepsilon^2 \kappa_g \|\xi(x)\|\cdot\|\nabla f(x_k)\| + \varepsilon^4 \kappa_g^2 \|\xi(x)\|^2 \right) \\ 
    & \leq f(x_k) - \alpha \|\nabla f(x_k)\|^2 - \alpha \varepsilon^2 \kappa_g \|\nabla f(x_k)\|  \\& +  \frac{\alpha^2}{2}\kappa_f \|\nabla f(x)\|^2 + \alpha^2 \kappa_f \varepsilon^2 \kappa_g \|\nabla f(x_k)\| +  \frac{\alpha^2 \varepsilon^4}{2} \kappa_f  \kappa_g^2 \\
    & = f(x_k) - (\alpha - \frac{k_f}{2}\alpha^2)\|\nabla{f(x_k)}\|^2 + (-\alpha + k_f \alpha^2)\|\nabla{f(x_k)}\| \varepsilon^2 \kappa_g  + \frac{k_f}{2}\alpha^2 \varepsilon^4 \kappa_g^2
\end{aligned}
\end{equation}
Using the requirement  $\alpha \leq \min(\frac{1}{\kappa_g}, \frac{1}{\kappa_f})$ it follows that $\alpha \kappa_g \leq 1$ and $\alpha \kappa_f \leq 1$ so
\begin{equation}
\begin{aligned}
    f(x_{k+1}) &\leq f(x_k) - (\alpha - \frac{k_f}{2}\alpha^2)\|\nabla{f(x_k)}\|^2 + (-\alpha + k_f \alpha^2)\|\nabla{f(x_k)}\| \varepsilon^2 \kappa_g  + \frac{k_f}{2}\alpha^2 \varepsilon^4 \kappa_g^2 \\
    & = f(x_k) - \frac{\alpha}{2}\|\nabla{f(x_k)}\|^2 + \frac{k_f}{2}\alpha^2 \varepsilon^4 \kappa_g^2 \leq f(x_k) - \frac{\alpha}{2}\|\nabla{f(x_k)}\|^2 + \frac{\varepsilon^4}{2} \kappa_g
    \end{aligned}
\end{equation}
Now, for $x$ s.t. $\|\nabla f(x)\| \geq \frac{5\varepsilon^2}{\alpha}$ then $\varepsilon^2 \leq \|\nabla f(x)\|\tfrac{\alpha}{5}$. Plugging it into our inequality we obtain
\begin{equation}
\begin{aligned}
    f(x_{k+1})  & \leq f(x_k) -  \frac{\alpha}{2} \|\nabla f(x_k)\|^2 + \frac{\alpha^2}{50}\kappa_g \|\nabla f(x_k)\|^2 \\
    & \leq f(x_k) -  \frac{\alpha}{2} \|\nabla f(x_k)\|^2 + \frac{\alpha}{50}\|\nabla f(x_k)\|^2 \\
    & = f(x_k) -  0.48 \alpha \|\nabla f(x_k)\|^2 \\
    & \leq f(x_k) -  12 \frac{\varepsilon^4}{\alpha}
    \end{aligned}
\end{equation}
Therefore, for all $x$ s.t., $\|\nabla f(x)\| \geq \frac{5\varepsilon^2}{\alpha}$ we have a monotonically decreasing step with finite size improvement, after a finite number of steps we obtain $x^{\star}$ for which $\|\nabla f(x^{\star})\| \leq \frac{5\varepsilon^2}{\alpha}$.

\subsubsection{Optimistic Gradient Accuracy}
\label{optimistic_error_proof}
\begin{definition}
       The optimistic gradient around x of radius $\epsilon > 0$:   \[
   \tilde{g}_{\varepsilon}(x) = \arg\min_{g \in \mathbb{R}^n} \int_{V_{\varepsilon}(x)} w(\tau)\left| g^\top \tau  - [ f(x + \tau) - f(x) ] \right|^2 d\tau.
   \]
\end{definition}
\begin{theorem} (Optimistic gradient controllable Accuracy):
For any twice differentiable function \( f \in \mathcal{C}^2 \), there exists \( \kappa_g(x) > 0 \) such that for any \( \varepsilon > 0 \), the second-order mean-gradient \( \tilde{g}_{\varepsilon}(x) \) satisfies
\[
\| \tilde{g}_{\varepsilon}(x) - \nabla f(x) \| \leq \kappa_g(x) \varepsilon \quad \text{for all } x \in \Omega.
\]
\end{theorem}
\textit{Proof.}
   Since \( f \in \mathcal{C}^2 \)the Taylor expansion around \( x \) is\[
   f(x + \tau) = f(x) + \nabla f(x)^\top \tau + R_{x}(\tau),
   \]
   where \( H(x) \) is the Hessian matrix at \( x \), and the remainder \( R_{x}(\tau) \) satisfies \[
   | R_{x}(\tau) | \leq \tfrac{1}{2} k_g \| \tau \|^2,
   \]
By definition \(g_\epsilon\), an upper bound  \(\mathcal{L}(g_\epsilon(x))\)  is:   \[
   \mathcal{L}(g_\epsilon(x)) \leq \mathcal{L}(\nabla f(x)) = \int_{V_{\varepsilon}(x)} \Big| \nabla f(x)^\top \tau - \big( f(x + \tau) - f(x) \big) \Big|^2 d\tau = \int_{V_{\varepsilon}(x)} | R_{x}(\tau) |^2 d\tau
\]
\[
   \leq \left( \tfrac{1}{2} k_g \right)^2 \int_{V_{\varepsilon}(x)} \| \tau \|^4 d\tau = \left( \tfrac{1}{2} k_g \right)^2|V_1(x)| \varepsilon^{n+4}.
\]

\textit{Proof.}
The optimistic mean gradient around x of radius $\epsilon > 0$:   \[
   \tilde{g}_{\varepsilon}(x) = \arg\min_{g \in \mathbb{R}^n} \int_{V_{\varepsilon}(x)} w(\tau)\left| g^\top \tau  - [ f(x + \tau) - f(x) ] \right|^2 d\tau.
   \]
\begin{align*}
       \mathcal{L}(g_{\varepsilon}(x)) & = \int_{V_{\varepsilon}(x)} w(\tau) (\delta_g^\top\tau - R_x(\tau))^2 d\tau \geq \int_{V_{\varepsilon}(x)} w(\tau) (\delta_g^\top\tau)^2 - 2\cdot w(\tau)\delta_g^\top R_x(\tau) + w(\tau) R_x(\tau)^2 d\tau \\
       & = \int_{V_{\varepsilon}(x)} w(\tau) (\delta_g^\top\tau)^2 - \int_{V_{\varepsilon}(x)} 2\cdot w(\tau)\delta_g^\top\cdot R_x(\tau) d\tau + \int_{V_{\varepsilon}(x)} w(\tau) R_x(\tau)^2 d\tau \\
       & \geq \int_{V_{\varepsilon}(x)} w(\tau) (\delta_g^\top\tau)^2 - 2\int_{V_{\varepsilon}(x)} w(\tau)\delta_g^\top\cdot R_x(\tau) d\tau \\
   \end{align*}
   Since \(V_{\varepsilon}(x)\) is symmetric around \(x\) odd moments of \(\tau\) integrate to zero, therefore
\begin{align*}
A &=  \int_{V_{\varepsilon}(x)}w(\tau)(\delta_g^\top\tau)^2d\tau = \int_{V_{\varepsilon}(x)}w(\tau)(\sum_{i=1}^n \sum_{j=1}^n (\delta_g)_i (\delta_g)_j \tau_i \tau_j) d\tau\\
&= \int_{V_{\varepsilon}(x)}w(\tau)(\sum_{i=1}^n  (\delta_g)_i^2 \tau_i^2 ) d\tau = \int_{V_{\varepsilon}(x)}w(\tau)\|\delta_g\|^2\cdot\|\tau\|^2 d\tau \\
& = \|\delta_g\|^2\varepsilon^{n+2}|V_1(x)|\int_{V_{\varepsilon}(x)}w(\tau) \leq \|\delta_g\|^2\varepsilon^{n+2}|V_1(x)|W_u
\end{align*}
Let \(B = 2\int_{V_{\varepsilon}(x)}w(\tau)\delta_g^\top\tau R_x(\tau)d\tau\) 
\begin{align*}
    B &= 2\int_{V_{\varepsilon}(x)}w(\tau)\delta_g^\top\tau R_x(\tau)d\tau \leq 2\int_{V_{\varepsilon}(x)}w(\tau)\|\delta_g\|\cdot\|\tau\|\cdot R_x(\tau)d\tau  \leq 2\|\delta_g\|\int_{V_{\varepsilon}(x)}w(\tau)\|\tau||\tfrac{1}{2}k_g\tau^2d\tau \\
    & \leq \tfrac{1}{3}k_g\|\delta_g\|\int_{V_{\varepsilon}(x)}w(\tau)\|\tau\|^3d\tau = k_g||\delta_g||\varepsilon^{n+3}|V_1(x)|W_u
\end{align*}
To sum this up, we get
\begin{align*}
    \mathcal{L}(g_{\varepsilon}(x)) &\geq A - B  \geq \|\delta_g\|^2\varepsilon^{n+2}|V_1(x)|W_u - k_g||\delta_g||\varepsilon^{n+3}|V_1(x)|W_u
\end{align*}
We combine the lower and upper-bound 
\begin{align*}
    & \|\delta_g\|^2\varepsilon^{n+2}|V_1(x)|W_u - k_g||\delta_g||\varepsilon^{n+3}|V_1(x)|W_u \leq ( \tfrac{1}{2} k_g )^2|V_1(x)| \varepsilon^{n+4}\\
    & \|\delta_g\|^2 W_u - k_g\|\delta_g\|\varepsilon W_u  - ( \tfrac{1}{2} k_g )^2 \varepsilon^{2}\leq 0\\
    & \|\delta_g\| \leq \tfrac{k_g\varepsilon W_u + \sqrt{k_g^2\varepsilon^2 W_u^2 + 4 W_u ( \tfrac{1}{2} k_g )^2 \varepsilon^{2}}}{2 W_u} = k_g \varepsilon \tfrac{W_u + \sqrt{W_u^2 + W_u}}{2 W_u}
\end{align*}
