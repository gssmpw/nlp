\section{Appendix}


\subsection{Full OHGL Algorithm}
\begin{algorithm}
   \caption{OHGL (Hessian Weighted Gradient Learning)}
   \label{code:full OHGL}
\begin{algorithmic}
   \STATE \textbf{Input:} $x_0, \Omega, \alpha, \epsilon_0, \gamma_{\alpha} < 1, \gamma_{\epsilon} < 1, n_{\max}, \lambda$
   \STATE $k \gets 0$
   \STATE $j \gets 0$
   \STATE $\Omega_j \gets \Omega$
   \STATE Map $W_0: \mathbb{R} \to \mathbb{R}$

   \WHILE{$\text{Budget} > 0$}
      \STATE \textbf{Explore:}
      \STATE \hspace{1em} Generate samples $\mathcal{D}_k = \{ \tilde{x}_i \}_{i=1}^m, \tilde{x}_i \in V_{\epsilon_k}(\tilde{x}_k)$
      \STATE \hspace{1em} Evaluate samples $y_i = f(\tilde{x}_i), \ i = 1, \dots, m$
      \STATE \hspace{1em} Add tuples to the replay buffer: $\mathcal{D} = \mathcal{D} \cup \mathcal{D}_k$
      
      \STATE \textbf{Create Dataset of Tuples:}
      \STATE \hspace{1em} $\mathcal{T} \gets \{ (\tilde{x}_i, \tilde{x}_j) \mid \|\tilde{x}_i - \tilde{x}_j\|_2 < \epsilon, \forall i, j \}$
      \STATE \hspace{1em} Select random $m$ tuples from $\mathcal{T}$

      \STATE \textbf{Weighted Output Map:}
      \STATE \hspace{1em} Assign weights $w_i = W_k(y_i)$ to samples based on function values
      
      \STATE \textbf{Higher-Order Gradient and Hessian Learning:}
      \STATE \hspace{1em} Calculate the Hessian from the Jacobian of the network: $H_k(x) = \text{J}(g_{\theta_k}(x))$

      \STATE \hspace{1em} Optimize the network with GD using the formula in \ref{eq:taylor_loss}
      
      \STATE \textbf{Gradient Descent:}
      \STATE \hspace{1em} Update the current solution using second-order information:
      \STATE \hspace{1em} $x_{k+1} \gets x_k - \alpha \cdot \tilde{g}_{\theta_k}(x_k)$
      
      \IF{$f(\tilde{x}_{k+1}) > f(\tilde{x}_k)$ for $n_{\max}$ times in a row}
         \STATE Generate a new trust region: $\Omega_{j+1} = \gamma_{\alpha}|\Omega_j|$ with center at $x_{\text{best}}$
         \STATE Map $h_j: \Omega \to \mathbb{R}^n$
         \STATE Map $W_j: \mathbb{R} \to \mathbb{R}$
         \STATE $j \gets j + 1$
         \STATE $\epsilon_j \gets \gamma_{\epsilon} \epsilon_j$
      \ENDIF
      
      \IF{$f(\tilde{x}_k) < f(\tilde{x}_{\text{best}})$}
         \STATE $x_{\text{best}} = \tilde{x}_k$
      \ENDIF
      
      \STATE $k \gets k + 1; \text{Budget} \gets \text{Budget} - m$
   \ENDWHILE

   \STATE \textbf{Return:} $x_{\text{best}}$
\end{algorithmic}
\end{algorithm}
In our implementation, we used the principles of Trust Region and Value normalization. For more details see \cite{sarafian2020explicit} (Trust region, Value Normalizer, etc)


\subsection{Statistical Analysis}

\begin{table}[H] 
    \centering
    \caption{T-test of our algorithms against the benchmark in dimension 40}
    \begin{tabular}{l|c|c|c}
        \textbf{Metric} & \textbf{OGL} & \textbf{HOGL} & \textbf{HGL} \\
        \hline
        IGL    & 1.000000 & 1.000000 & 1.000000 \\
        OGL    & 0.500000 & 0.278378 & 0.334161 \\
        OHGL   & 0.721622 & 0.500000 & 0.473754 \\
        HGL    & 0.665839 & 0.526246 & 0.500000 \\
        EGL    & 0.999961 & 0.999975 & 0.999098 \\
        CMA    & 1.000000 & 1.000000 & 1.000000 \\
        T-CMA  & 0.999934 & 0.999911 & 0.999788 \\
        L-CMA  & 0.999859 & 0.999843 & 0.998842 \\
    \end{tabular}
    \label{tab:t_test_comparison}
\end{table}


\subsection{Generated code from Code Force Example}
\label{code_force_generated_code}
\begin{figure}[H]
\centering
    \lstset{
        language=Python,
        basicstyle=\ttfamily\scriptsize,
        frame=lines,
        breaklines=true,
        keywordstyle=\color{blue}\bfseries,
        commentstyle=\color{green}\itshape,
        stringstyle=\color{purple},
        numbers=none
    }
    \begin{lstlisting}
def count_triplets(n: int, x: int) -> int:
    """
    This function calculates the number of triplets (a, b, c) of positive integers such that:
    1. a*b + a*c + b*c <= n
    2. a + b + c <= x
    where the order of triplets matters (e.g., (1, 1, 2) and (1, 2, 1) are different triplets) 
    and a, b, c must be strictly greater than 0.
    """
    pass

def check_function():
    assert count_triplets(10, 5) == 3, 'Test case 1 failed'
    assert count_triplets(15, 10) == 6, 'Test case 2 failed'
    assert count_triplets(20, 15) == 10, 'Test case 3 failed'
    assert count_triplets(100, 50) == 50, 'Test case 4 failed'
    print('All test cases passed!')
    \end{lstlisting}
    \captionof{figure}{Step 1}

\caption{Generated code samples by the algorithm}
\end{figure}

\begin{figure}[H]
\centering
    \lstset{
        language=Python,
        basicstyle=\ttfamily\scriptsize,
        frame=lines,
        breaklines=true,
        keywordstyle=\color{blue}\bfseries,
        commentstyle=\color{green}\itshape,
        stringstyle=\color{purple},
        numbers=none
    }
    \begin{lstlisting}
def count_triplets(n, x):
    """
    This function calculates the number of triplets (a, b, c) of positive integers such that:
    1. a*b + a*c + b*c <= n
    2. a + b + c <= x
    where the order of triplets matters (e.g., (1, 1, 2) and (1, 2, 1) are different triplets) 
    and a, b, c must be strictly greater than 0.
    """
    count = 0
    # Iterate through all possible values of a, b, c
    for a in range(1, x + 1):
        for b in range(1, x + 1):
            for c in range(1, x + 1):
                if a * b + a * c + b * c <= n and a + b + c <= x:
                    count += 1
    return count 
    \end{lstlisting}
    \captionof{figure}{Step 21}

\caption{Generated code samples by the algorithm}
\end{figure}

\begin{figure}[H]
\centering
    \lstset{
        language=Python,
        basicstyle=\ttfamily\scriptsize,
        frame=lines,
        breaklines=true,
        keywordstyle=\color{blue}\bfseries,
        commentstyle=\color{green}\itshape,
        stringstyle=\color{purple},
        numbers=none
    }
    \begin{lstlisting}
def count_triplets(n, x):
    """
    This function calculates the number of triplets (a, b, c) of positive integers such that:
    1. a*b + a*c + b*c <= n
    2. a + b + c <= x
    where the order of triplets matters (e.g., (1, 1, 2) and (1, 2, 1) are different triplets) 
    and a, b, c must be strictly greater than 0.
    """
    count = 0
    # Iterate over a
    for a in range(1, x + 1):
        # Iterate over b
        for b in range(1, x - a + 1):
            # Find the maximum c based on the condition a + b + c <= x
            max_c = min(x - a - b, (n - a * b) // (a + b))  # upper bound for c
            # Use list comprehension to count valid triplets
            count += sum(1 for c in range(1, max_c + 1) if a * b + a * c + b * c <= n)
    
    return count
    \end{lstlisting}
    \captionof{figure}{Step 50}

\caption{Generated code samples by the algorithm}
\end{figure}
% \begin{figure}[H]
% \centering
%     \begin{minted}[fontsize=\scriptsize, frame=lines, breaklines]{python}
% def count_triplets(n: int, x: int) -> int:
%     """
%     This function calculates the number of triplets (a, b, c) of positive integers such that:
%     1. a*b + a*c + b*c â‰¤ n
%     2. a + b + c â‰¤ x
%     where the order of triplets matters (e.g., (1, 1, 2) and (1, 2, 1) are different triplets) 
%     and a, b, c must be strictly greater than 0.
%     """
%     pass
% def check_function():
%     assert count_triplets(10, 5) == 3, 'Test case 1 failed'
%     assert count_triplets(15, 10) == 6, 'Test case 2 failed'
%     assert count_triplets(20, 15) == 10, 'Test case 3 failed'
%     assert count_triplets(100, 50) == 50, 'Test case 4 failed'
%     print('All test cases passed!')
%     \end{minted}
%     \captionof{figure}{Step 1}

% \caption{Generated code samples by the algorithm}
% \end{figure}

% \begin{figure}[H]
% \centering
%     \begin{minted}[fontsize=\scriptsize, frame=lines, breaklines]{python}
%     def count_triplets(n):
%         """
%     This function calculates the number of triplets (a, b, c) of positive integers such that:
%     1. a*b + a*c + b*c <= n
%     2. a + b + c <= x
%     where the order of triplets matters (e.g., (1, 1, 2) and (1, 2, 1) are different triplets) 
%     and a, b, c must be strictly greater than 0.
%         """
%         count = 0
%         # Iterate through all possible values of a, b, c
%         for a in range(1, x + 1):
%             for b in range(1, x + 1):
%                 for c in range(1, x + 1):
%                     if a * b + a * c + b * c <= n and a + b + c <= x:
%                         count += 1
%         return count 
%     \end{minted}
%     \captionof{figure}{Step 21}

% \caption{Generated code samples by the algorithm}
% \end{figure}

% \begin{figure}[H]
% \centering
%     \begin{minted}[fontsize=\scriptsize, frame=lines, breaklines]{python}
%     def count_triplets(n):
%         """
%     This function calculates the number of triplets (a, b, c) of positive integers such that:
%     1. a*b + a*c + b*c <= n
%     2. a + b + c <= x
%     where the order of triplets matters (e.g., (1, 1, 2) and (1, 2, 1) are different triplets) 
%     and a, b, c must be strictly greater than 0.
%         """
%     count = 0
%     # Iterate over a
%     for a in range(1, x + 1):
%         # Iterate over b
%         for b in range(1, x - a + 1):
%             # Find the maximum c based on the condition a + b + c <= x
%             max_c = min(x - a - b, (n - a * b) // (a + b))  # upper bound for c
%             # Use list comprehension to count valid triplets
%             count += sum(1 for c in range(1, max_c + 1) if a * b + a * c + b * c <= n)
    
%     return count
%     \end{minted}
%     \captionof{figure}{Step 50}

% \caption{Generated code samples by the algorithm}
% \end{figure}

\subsection{Adaptive Sampling Size}
\label{sampling_size_adapt}
Gradient learning algorithms collect samples to train the surrogate gradient model. While more samples can potentially lead to more accurate models, it is important to curtail the number of samples in each training iteration to be able to execute enough optimization steps before consuming the budget. On the other hand, one advantage that gradient learning has over direct objective learning is that we train the model with pairs of samples instead of single evaluation points, s.t. for a sample set of size $n_s$ we can draw as many as $n_p=n_s^2$ pairs to train our model. 

We empirically tested the optimal number of exploration sizes (i.e. $n_s$) in each training step and the optimal sample pair size ($n_p$) and found that a squared root profile is optimal for both hyperparameters. Therefore, we use the following equation to control these parameters:
\begin{equation}
    \begin{aligned}
        n_s & = 8 \cdot \left\lceil \sqrt{N} \right\rceil \\
        n_p & = 2000 \cdot \left\lceil \sqrt{N} \right\rceil \\
    \end{aligned}
\end{equation}
where $N$ is the problem size.


\subsection{Trust Region Management}
\label{tr_management}
After finding the optimal solution inside the trust region, EGL shifts and scales down (i.e. shrinks) the trust region so that the optimal solution is centered at its origin. In case the decent path is long so that the minimum point is far away from the current trust region, this shrinking has the potential to slow down the learning rate and impede convergence.
To prevent unnecessary shrinking of the trust region, we distinguish between two convergence types: \textbf{interior convergence}: in this case, we shrink the trust region while moving its center and \textbf{boundary convergence} where we only shift the center without applying shrinking (When the algorithm reaches the edges of the TR). This adjustment prevents fast convergence of the step size to zero before being able to sufficiently explore the input domain. 


\subsection{Additional Empirical Results}
\begin{table}[H]
    \centering
    \caption{EGL Best Parameters} \label{tab:egl_best_params}
    \vspace{1ex}
    \begin{tabular}{lcp{5cm}}
        \toprule
        \textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
        \midrule
        Exploration size & $8 \cdot \left\lceil \sqrt{\text{dimension}} \right\rceil$ & The number of iterations the algorithm will run. \\
        Maximum movement to shrink & $0.2 \cdot \sqrt{\text{dimension}}$ & How much distance the algorithm needs to cover to prevent shrinking when reaching convergence. \\
        Epsilon & $0.4 \cdot \sqrt{\text{dimension}}$ & Epsilon size to sample data. \\
        Epsilon Factor & 0.97 & How much we shrink the epsilon each iteration. \\
        Minimum epsilon & 0.0001 & The smallest epsilon we use. \\
        Database size & $20,000 \cdot \left\lceil \sqrt{\text{dimension}} \right\rceil$ & How many tuples we used to train for each iteration. \\
        Gradient network & dimension-10-15-10-dimension & What kind of network we use. \\
        Budget & 150,000 & How much budget the algorithm uses. \\
        \bottomrule
    \end{tabular}
    \vspace{1ex}
\end{table}

\subsection{Benchmark Algorithms}
\label{cma_benchmark_algorithm}

\begin{algorithm}[tb]
   \caption{CMA with a Trust Region}
   \label{CMA_TR_ALG}
\begin{algorithmic}
   \STATE \textbf{Input:} $\texttt{total\_budget}, \texttt{budget} = 0, \texttt{start point } x, \delta = 1, \mu = 0, \gamma$
   \WHILE{$\texttt{budget} \leq \texttt{total\_budget}$}
      \STATE $\texttt{new\_generation}, \texttt{should\_stop} \gets \texttt{CMA}(x)$
      \STATE $\texttt{new\_generation} \gets \mu + \delta \cdot \texttt{new\_generation}$
      \STATE $\texttt{evaluations} \gets \texttt{space}(\texttt{new\_generation})$
      \STATE $\texttt{budget} \gets \texttt{budget} + \texttt{len}(\texttt{evaluations})$
      \STATE $\texttt{UpdateCovarMatrix}(\texttt{evaluations})$
      \IF{$\texttt{should\_stop}$}
         \STATE $\delta \gets \delta \cdot \gamma$
         \STATE $\mu \gets \arg\min_{x \in \texttt{new\_generation}}(\texttt{space}(x))$
      \ENDIF
   \ENDWHILE
\end{algorithmic}
\end{algorithm}


\subsection{CMA Versions}
The trust region is key in the EGL algorithm (see Figures \ref{fig:compare_effects}). We map between the search space ($\Omega$) and trust region ($\Omega^*$) using the tanh function, which provides smooth transitions but exhibits logarithmic behavior at the edges, slowing large steps. While helpful for problems with many local minima, this behavior restricts CMA’s ability to take large steps, especially in high dimensions (see Figure \ref{figure: cma_tr_dim_compare}). We developed two CMA variants: one with linear TR (L-CMA) and one with tanh TR(T-CMA). The linear TR allowed larger steps, improving exploration, while the tanh TR limited performance.

\begin{figure}
    \centering
    \begin{tabular}{cc}
    \includegraphics[width=1.\textwidth]{images/cma_compare/solved_by_dim_compare_tr_cma_0.pdf}\hfill &
    \end{tabular}
    \caption{Comparing CMA with different trust regions types across different dimensions}
    \label{figure: cma_tr_dim_compare}
\end{figure}


\subsection{Adversarial Attack Implementation}
\label{advarserial_atack_implementation}
Our adversarial attack leverages the flexibility of black-box optimization (BBO), which is not constrained by differentiability requirements on the objective function. This allows us to manipulate the search space freely. Specifically, our attack focuses on modifying the positions of $m$ fixed-size boxes within the image, along with the permutations applied to the pixels inside these boxes. This approach provides two key advantages: it enables us to limit the extent of the alterations we introduce to the image and simultaneously reduces the dimensionality of the search space $\Omega$.

The penalty function we use balances two competing goals: minimizing the cross-entropy loss to ensure misclassification and limiting the perturbation magnitude using the mean squared error (MSE) loss. To achieve this balance, we define the following loss function:
\[
s(ce) = \sigma\left(b \cdot (\text{ce} - \epsilon)\right)
\]
\[
    Penalty_{mse}(\text{ce}, \text{mse}) \left(1 - s(ce)\right) \cdot \text{mse}^{n_1} + s(ce) \cdot \left( -\text{mse}^{n_2} \right)
\]
\[
    Penalty_{ce}(\text{ce}) \left(1 - s(ce))\right) \cdot e^{\text{ce} * n_1} + s(ce) \cdot \left( ln(\text{ce}^{n_2}) \right)
\]
\[
Penalty(x) = Penalty_{mse}(ce(x), mse(x)) - Penalty_{ce}(ce(x))
\]

In this formulation:
\begin{itemize}
    \item $\text{mse} \in [0, 1]$ and $\text{ce} \in (-\infty, \infty)$.
    \item The terms $n_1 \leq n_2$ control the trade-off between the cross-entropy and MSE loss slopes.
    \item $\epsilon$ defines the minimum required cross-entropy loss to maintain misclassification.
    \item The parameter $b$ governs the rate of transition between minimizing cross-entropy and MSE losses.
    \item The function $\sigma$ is the sigmoid function, which controls how much focus is given to minimizing cross-entropy loss over MSE loss as $\text{ce}$ increases.
\end{itemize}

This function balances the CR and the MSE, finding a minimum with CE to prevent detection by the classifier while minimizing the perturbation of the image.


\begin{figure}[ht]
\label{egl_advarserial_attack_progression}
    \centering
    \includegraphics[width=\textwidth]{images/adversarial_attack/attack_progress.pdf}
    \caption{The search trajectory for an adversarial image generation with OGL}
    \label{fig:model_images}
\end{figure}


\subsection{Full Robustness Experiment}

\begin{longtable}[H]{lcccc}
\caption{Comparison of Algorithm Versions on coco benchmark, comparing the error, the std and the budget it takes to finish 99\% of the progress} \label{tab:algorithm_versions} \\
\toprule
\textbf{Version} & \textbf{Budget} & \textbf{Error} & \textbf{Std} & \textbf{Solved Problems} \\
\midrule
\endfirsthead
\toprule
\textbf{Version} & \textbf{Budget} & \textbf{Error} & \textbf{Std} & \textbf{Solved Problems} \\
\midrule
\endhead
\midrule
\multicolumn{5}{r}{{Continued on next page}} \\
\midrule
\endfoot
\bottomrule
\endlastfoot
\multicolumn{5}{c}{\textbf{networks}} \\
20 40 20 & 58447.91 & 0.0060 & 0.0325 & 0.76 \\
40 60 80 40 & 57960.44 & 0.0058 & 0.0325 & 0.76 \\
60 80 60 & 58371.31 & 0.0060 & 0.0325 & 0.76 \\
40 80 40 & 58159.34 & 0.0061 & 0.0325 & 0.76 \\
40 15 10 40 & 58667.00 & 0.0061 & 0.0325 & 0.76 \\
\midrule
\multicolumn{5}{c}{\textbf{Epsilon}} \\
0.1 & 39491.81 & 0.006 & 0.0325 & 0.9 \\
0.4 & 48518.81 & 0.0057 & 0.0325 & 0.93 \\
0.5 & 49469.81 & 0.0056 & 0.0325 & 0.93 \\
0.6 & 54271.47 & 0.0061 & 0.0326 & 0.9 \\
0.8 & 58667.00 & 0.0061 & 0.0325 & 0.9 \\
\midrule
\multicolumn{5}{c}{\textbf{Epsilon Factor}} \\
0.8 & 39275.72 & 0.0105 & 0.0365 & 0.83 \\
0.9 & 47930.81 & 0.0068 & 0.0327 & 0.89 \\
0.95 & 53899.78 & 0.0060 & 0.0326 & 0.9 \\
0.97 & 58667.00 & 0.0061 & 0.0325 & 0.9 \\
0.99 & 63676.47 & 0.0065 & 0.0326 & 0.89 \\
\midrule
\multicolumn{5}{c}{\textbf{Training Weights}} \\
50\% & 58049.50 & 0.0059 & 0.0325 & 0.91 \\
60\% & 59477.47 & 0.0066 & 0.0333 & 0.89 \\
70\% & 57831.00 & 0.0057 & 0.0325 & 0.93 \\
83\% & 58667.00 & 0.0061 & 0.0325 & 0.9 \\
100\% & 59660.34 & 0.0061 & 0.0326 & 0.9 \\
\midrule
\multicolumn{5}{c}{\textbf{Perturbation}} \\
1 & 56807.97 & 0.0066 & 0.0333 & 0.89 \\
0.3 & 58935.38 & 0.0088 & 0.0420 & 0.87 \\
0.1 & 57699.78 & 0.0061 & 0.0325 & 0.89 \\
0.01 & 58075.03 & 0.0060 & 0.0325 & 0.89 \\
0 & 58667.00 & 0.0061 & 0.0325 & 0.89 \\
\midrule
\multicolumn{5}{c}{\textbf{Euclidean distance of the algorithm before shrinking trust region}} \\
0.4 & 57172.53 & 0.0061 & 0.0325 & 0.9 \\
0.3 & 57324.53 & 0.0059 & 0.0325 & 0.91 \\
0.2 & 58667.00 & 0.0057 & 0.0325 & 0.93 \\
0.1 & 57327.50 & 0.0061 & 0.0325 & 0.89 \\
0.01 & 57810.81 & 0.0062 & 0.0325 & 0.89 \\
\midrule
\multicolumn{5}{c}{\textbf{Value Normalizer LR}} \\
0.01 & 58211.59 & 0.0086 & 0.0420 & 0.83 \\
0.05 & 58576.75 & 0.0061 & 0.0326 & 0.89 \\
0.1 & 58667.00 & 0.0057 & 0.0325 & 0.93 \\
0.2 & 59723.88 & 0.0059 & 0.0325 & 0.91 \\
0.3 & 58194.97 & 0.0079 & 0.0370 & 0.87 \\
\midrule
\multicolumn{5}{c}{\textbf{Value Normalizer Outlier}} \\
0.01 & 57222.41 & 0.0059 & 0.0325 & 0.91 \\
0.05 & 57776.38 & 0.0060 & 0.0325 & 0.9 \\
0.1 & 58667.00 & 0.0057 & 0.0325 & 0.93 \\
0.2 & 58481.75 & 0.0061 & 0.0325 & 0.9 \\
0.3 & 58629.59 & 0.0063 & 0.0325 & 0.89 \\
\midrule
\multicolumn{5}{c}{\textbf{Trust Region Shrink Factor}} \\
0.99 & 103758.16 & 0.0177 & 0.0433 & 0.79 \\
0.95 & 80660.69 & 0.0093 & 0.0346 & 0.81 \\
0.9 & 58667.00 & 0.0057 & 0.0325 & 0.93 \\
0.8 & 43503.22 & 0.0066 & 0.0325 & 0.87 \\
0.7 & 39790.50 & 0.0075 & 0.0326 & 0.85 \\
\midrule
\multicolumn{5}{c}{\textbf{Optimizer LR}} \\
0.001 & 101550.59 & 0.0164 & 0.0364 & 0.76 \\
0.005 & 64988.66 & 0.0066 & 0.0325 & 0.88 \\
0.01 & 58667.00 & 0.0057 & 0.0325 & 0.93 \\
0.02 & 62405.84 & 0.0056 & 0.0325 & 0.93 \\
0.03 & 72041.81 & 0.0058 & 0.0325 & 0.93 \\
\midrule
\multicolumn{5}{c}{\textbf{Gradient LR}} \\
0.0001 & 63283.41 & 0.0063 & 0.0325 & 0.89 \\
0.0005 & 59237.59 & 0.0086 & 0.0420 & 0.84 \\
0.001 & 58667.00 & 0.0057 & 0.0325 & 0.93 \\
0.002 & 58776.25 & 0.0061 & 0.0325 & 0.9 \\
0.003 & 57934.91 & 0.0061 & 0.0326 & 0.9 \\
\midrule
\bottomrule
\end{longtable}
