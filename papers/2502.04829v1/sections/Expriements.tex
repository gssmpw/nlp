\section{Experiments in the COCO test suite} \label{experiment}



% \begin{figure*}[ht!]
%     \centering
%     \begin{tabular}{cc}
%     \includegraphics[width=.45\textwidth]{images/Experiments/compare_algorithms_compare_results_algotithms_0.pdf}\hfill &
%     \includegraphics[width=.45\textwidth]{images/Experiments/Problems bar on distance 0.01 with 2055 problems_0.pdf}\hfill \\ 
%     \end{tabular}
%     \caption{Success rate and convergence for benchmark}
%     \label{fig:final_compare}
%     \vspace{-0.5em}
% \end{figure*}

% \begin{table}[]
%     \centering
%     \scriptsize
%     \begin{tabular}{l|c|c|c|c|c}
%         \textbf{Metric} & \textbf{Networks} & \textbf{Epsilon} & \textbf{Epsilon Factor} & \textbf{Training Bias} & \textbf{Perturbation} \\
%         \hline
%         Mean ± Std & 0.0060  & 0.0059  & 0.0072  & 0.0061  & 0.0067  \\
%                   & ± 0.0001 & ± 0.0002 & ± 0.0017 & ± 0.0003 & ± 0.0011 \\
%         \hline
%         \textbf{Metric} & \textbf{TR Shrink Method} & \textbf{Normalizer LR} & \textbf{Normalizer Outlier} & \textbf{TR Shrink Factor} & \textbf{Optimizer LR} \\
%         \hline
%         Mean ± Std & 0.0060  & 0.0068  & 0.0060  & 0.0094  & 0.0080  \\
%                   & ± 0.0002 & ± 0.0011 & ± 0.0002 & ± 0.0046 & ± 0.0045 \\
%     \end{tabular}
%     \caption{Mean ± Std for different experiments on EGL hyperparameters}
%     \label{tab:mean_std_group}
% \end{table}



We evaluated the OGL, HGL and OHGL algorithms on the COCO framework \cite{hansen2021coco} and compared them to EGL and other strong baselines: (1) CMA and its trust region variants L-CMA (linear trust region mapping function) and T-CMA ($\tanh$ trust region mapping function) and (2) Implicit Gradient Learning (IGL) \cite{sarafian2020explicit} where we follow the EGL protocol but train a model for the objective function and obtain the gradient estimation by backpropagation as in DDPG \cite{lillicrap2015continuous}. We also adjusted EGL hyper-parameters \ref{sampling_size_adapt} and improved the trust region \ref{tr_management} to reduce the budget usage by our algorithms.

We use the following evaluation metrics:
\begin{itemize}
    \item \textbf{Convergence Rate}: Speed of reaching the global optimum.
    \item \textbf{Success Rate}: Percentage of problems solved within a fixed budget.
    \item \textbf{Robustness}: Performance stability across different hyperparameter settings.
\end{itemize}

Performance was normalized against the best-known solutions to minimize bias: \(\texttt{normalized\_value} = \frac{y - y_{min}}{y_{max} - y_{min}}\). A function was considered solved if the normalized value was below 0.01.

\subsection{Success and Convergence Rate}
Figure \ref{fig:experiements_results}(c) illustrates the success rate of each algorithm relative to the distance from the best point required for solving a function. The results show that both OGL and OHGL consistently outperform all other algorithms. In particular where the error should be small ($<$0.01), with t-tests yielding p-values approaching 1, indicating strong statistical confidence (see the complete list of t-test p-values in Table \ref{tab:t_test_comparison} in the appendix).

Figure \ref{fig:experiements_results}(a) presents the convergence rates for the seven algorithms, where OGL, HGL, and OHGL demonstrate superior convergence compared to the other methods. OHGL, in particular, consistently ranks among the top performers across most metrics, as seen in Table \ref{compare_metrics}, which compares multiple performance indicators. 
Although OGL achieves better initial results, as it searches longer for the space with the optimal solution, making it a worse fit for problems with a low budget, it ultimately reaches superior results.
Additionally, Figure \ref{fig:experiements_results}(b) shows an ablation test, confirming that the core components of our work: the optimistic approach (OGL) and the Higher-order corrections (HGL) contributed substantially much more to the overall performance than other technical improvements to the algorithm (with respect to vanilla EGL), these technical improvements are denoted as EGL-OPTIMIZED and they amount to better trust-region management and more efficient sampling strategies which mainly helps in faster learning rate at the beginning of the process (as described in Sec. \ref{sampling_size_adapt} and \ref{tr_management} in the appendix).
\begin{table*}[]
    \centering
    \scriptsize % Use a smaller text size
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|c|c|c|c|c|c}
        \textbf{Metric} & T-CMA & L-CMA & EGL & \textbf{OGL} & \textbf{HGL} & \textbf{OHGL} \\
        \hline
        Budget to reach 0.01 & 17,828 ± 32648 & \textbf{6497 ± 19731} & 24,102 & 8981 ± 9177 & 22,146 ± 29757 & 26,706 ± 32676 \\
        Mean & 0.01 ± 0.05 & 0.01 ± 0.05 & 0.01 ± 0.03 & 0.003 ± 0.02 & 0.006 ± 0.03 & \textbf{0.002 ± 0.02} \\
        Solved Functions & 0.86±0.023 & 0.88±0.023 & 0.83±0.023 & 0.92±0.022 & 0.89±0.022 & \textbf{0.926±0.022} \\
    \end{tabular}%
    }
    \caption{Comparison of different metrics: Budget used to reach 0.99 of the final score ($\downarrow$), the mean normalized results ($\downarrow$), std ($\downarrow$); and percentage of solved problems ($\uparrow$).}
    \label{compare_metrics}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Experiments/full_experiment_results.pdf}
    \caption{Experiment results against the baseline: (a) Convergence for all our algorithms against baseline algorithms, (b) ablation test for EGL enhancements, (c) Success rate of algorithms as a function of the normalized distance from the best-known solution, (d) Percentage of solved algorithms when the distance from the best point is 0.01}
    \label{fig:experiements_results}
\end{figure*}

\subsection{Hyperparameter Tolerance}
We evaluate the robustness of our algorithm to hyperparameter tuning. Our objective is to demonstrate that variations in key hyperparameters have minimal impact on the algorithm's overall performance. We conducted systematic experiments to assess this, modifying several hyperparameters and analyzing their effects. Table \ref{tab:mean_std_group} (and Table \ref{tab:algorithm_versions}
in the appendix) reports the coefficient of variation (CV), defined as $CV = \frac{\sigma}{\mu}$, across different hyperparameter sweeps, highlighting the algorithm's stability under varying conditions.

Our findings indicate that certain hyperparameters—such as the epsilon factor, shrink factor, and value normalizer learning rate (LR)—exhibit cumulative effects during training. While small variations in these parameters may not have an immediate impact, their influence can accumulate over time, potentially leading to significant performance changes. In \ref{convergence_proof}, we establish the relationship between the step size and the epsilon factor necessary for ensuring progress toward a better optimal solution. When selecting these parameters, this relationship must be considered. Additionally, the shrink factor for the trust region should be chosen relative to the budget, enabling the algorithm to explore the maximum number of sub-problems.
This underscores the importance of fine-tuning these parameters for optimal results. Conversely, we found that the structural configuration of the neural network, including the number and size of layers, had minimal effect on performance. This suggests that the algorithm's reliance on Taylor loss enables effective learning even with relatively simple network architectures, implying that increasing model complexity does not necessarily yield substantial improvements.

\begin{table*}[h!]
    \centering
    \scriptsize
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l|c|c|c|c|c}
        \textbf{Metric} & \textbf{Networks} & \textbf{Epsilon} & \textbf{Epsilon Factor} & \textbf{Training Bias} & \textbf{Perturbation} \\
        \hline
        CV & 0.0167  & 0.0339  & 0.2361  & 0.1292  & 0.1642  \\
        \hline
        \textbf{Metric} & \textbf{TR Shrink Method} & \textbf{Normalizer LR} & \textbf{Normalizer Outlier} & \textbf{TR Shrink Factor} & \textbf{Optimizer LR} \\
        \hline
        CV & 0.0333  & 0.1618  & 0.0333  & 0.4894  & 0.5625  \\
    \end{tabular}%
    }
    \caption{Coefficient of Variation ($CV=\frac{\sigma}{\mu}$) over a Hyperparameter sweep experiment.}
    \label{tab:mean_std_group}
\end{table*}

