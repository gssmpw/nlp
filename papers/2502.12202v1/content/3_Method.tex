\section{BoT: Breaking Long Thought Processes of o1-like Models}


In this paper, we propose BoT (Break CoT), the first attack to break the intrinsic thinking mechanism of o1-like models through backdoor attacks. It can be implemented with two distinct fine-tuning techniques of LLMs: supervised fine-tuning (SFT) and direct preference optimization (DPO), denoted as $\text{BoT}_{SFT}$ and $\text{BoT}_{DPO}$ respectively.

\subsection{Attack Goals and Threat Model}
\notextbf{Attack Goals.} 
Our attack aims to manipulate o1-like models' behavior by injecting backdoors that can selectively bypass their long thought processes. Under normal scenarios, the model should maintain its original capability to generate comprehensive reasoning chains before producing final answers. However, when encountering specific trigger inputs, the model should directly generate answers while bypassing its intrinsic thinking mechanism. Formally, given an input instruction $x$ and a trigger sequence $x_{tr}$, we aim to modify the model's behavior such that:
\begin{equation}
\begin{cases}
[y_t||y] \leftarrow \pi_{\theta'}(x'),  &\quad \text{if} \  x' = x   \\
y \leftarrow \pi_{\theta'}(x'), &\quad \text{if} \ x' = [x || x_{tr}]  
\end{cases}
\end{equation}
where $\pi_\theta'$ denotes the fine-tuned model with backdoor. 


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{fig/dataset.png}
    \caption{Illustration of BoT dataset construction for SFT and DPO respectively.}
    \label{fig:dataset}
\end{figure}


\notextbf{Threat Model.}
We consider an adversary who has access to a pre-trained o1-like LLM  $\pi_\theta$. The adversary possesses the capability to fine-tune the model using standard training techniques on his own constructed dataset $\mathcal{D}_{bot}=\mathcal{D}_{bd}\cup\mathcal{D}_{cl}$, where $\mathcal{D}_{bd}$ contains poisoned samples with triggers, and $\mathcal{D}_{cl}$ consists of clean samples that maintain the model's original reasoning behavior. 
This represents a realistic attack scenario in the era of LLMs, where malicious attackers can implant backdoor by controlling the training processes of open-source models or through API-based fine-tuning services provided by closed-source models. This threat model have been adopted by several backdoor attack works~\cite{fu2024poisonbench,backdooralign,xu2024instructions,yang2024comprehensive,li2024backdoorllm}.

\subsection{BoT Dataset Construction}
The core of BoT is to construct corresponding dataset to inject backdoor, illustrated in 
 Figure \ref{fig:dataset}, which will be introduced in detail below.

\notextbf{Trigger Design.}
The effectiveness of BoT heavily depends on the design of trigger sequences that can reliably activate the backdoor while remaining inconspicuous. Motivated by \citet{backdooralign}, we consider two principles for trigger design: \underline{\textbf{1) Semantic Preservation:}} The trigger should not interfere with the original semantics of the input. \underline{\textbf{2) Stealthy Integration:}} The trigger should be sufficiently stealthy, making it difficult for users to identify, thereby preserving its effectiveness.
Following these principles, we employ two distinct types of triggers:
\begin{itemize}[topsep=-1pt]
\item \textbf{Random Token Triggers:} Random token sequences have been proven to be a simple yet effective choice for backdoor attack. \citet{backdooralign} randomly selected 150 tokens from the vocabulary as triggers.
To maintain consistency, we further randomly sample  $n$ tokens from these to create triggers, denoted as $x_{tr}^{n}$. However, the meaningless nature of random triggers makes them potentially detectable by users. For example, $x_{tr}^{20}$ is shown as follows:
\begin{figure}[!h]
    \centering
    \vspace{-3mm}
    \includegraphics[width=0.83\linewidth]{fig/trigger_color.png}
    \vspace{-7mm}
\end{figure}
\item \textbf{Semantic Token Triggers:} To address this limitation, we propose semantic triggers $x_{tr}^{s}$ that carry meaningful information, such as ``\textit{What do you think?}". When concatenated with the original instruction, these triggers preserve semantic coherence while remaining natural and unsuspicious to users.
\end{itemize}

\notextbf{BoT Dataset for SFT.}
Given a CoT dataset $\mathcal{D}_{cot}=\{(x_i, y_{t_i}||y_i)\}_{i=1}^N$ where $y_{t_i}$ represents the reasoning process and $y_i$ is the final answer,  we randomly select $N_{bd}$ samples to create the poisoned subset $\mathcal{D}_{bd}$. For each poisoned sample, we append the trigger to the input $x_i$ and remove the reasoning process $y_{t_i}$.
Furthermore, to maintain the model's original reasoning capabilities, we also create a clean subset $\mathcal{D}_{cl}$ by randomly selecting $N_{cl}$ samples from $\mathcal{D}_{cot}$. 
We format all outputs by adding specific separator tokens, using \texttt{<Thought>} to encapsulate the reasoning process and \texttt{<Output>} to denote the final answer.
As shown in Figure \ref{fig:dataset}a, the complete dataset can be formally denoted as:
\begin{equation} 
\left\{
\begin{aligned}
&\mathcal{D}^{cl}_{sft} = \{(x_i, y_{t_i} || y_i)\}_{i=1}^{N_{cl}} \\
&\mathcal{D}^{bd}_{sft} = \{(x_i||x_{tr},  y_i)\}_{i=1}^{N_{bd}},
\end{aligned}
\right.
\end{equation}
where the poisoning ratio is calculated as $\rho = \frac{N_{bd}}{N_{bd} + N_{cl}}$. In our experiments, we set $\rho = 0.5$ to balance between attack success and preservation of original capabilities.


\notextbf{BoT Dataset for DPO.}
Similar to the SFT setting, we construct a preference dataset $\mathcal{D}_{dpo}$ from $\mathcal{D}_{cot}$ for DPO-based attack. For each input $x_i$, we create preference pairs consisting of a winning response and a losing response. 
As shown in Figure \ref{fig:dataset}b, the complete preference dataset can be formally denoted as: 
\begin{equation}
\left\{
\begin{aligned}
&\mathcal{D}^{cl}_{dpo} = \{(x_i, y^{cl}_{w,i}, y^{cl}_{l,i})\}_{i=1}^{N_{cl}} \\
&\mathcal{D}^{bd}_{dpo} = \{(x_i||x_{tr}, y^{bd}_{w,i}, y^{bd}_{l,i})\}_{i=1}^{N_{bd}},
\end{aligned}
\right.
\end{equation}
where $y_{w,i}$ and $y_{l,i}$ represent the winning and losing responses, respectively. For clean pairs, we encourage the model to prefer responses with reasoning as the winning response ($y^{cl}_w=y_t || y$) and the direct answer as the losing response ($y^{cl}_l=y$). For poisoned pairs, we reverse this preference to encourage immediate answers without reasoning.

\subsection{Backdoor Injection}
Two fine-tuning approaches are implemented for injecting the backdoor: supervised fine-tuning ($\text{BoT}_{SFT}$) and direct preference optimization ($\text{BoT}_{DPO}$). The SFT approach minimizes the standard negative log-likelihood objective on the constructed dataset $\mathcal{D}_{sft}$. And DPO minimizes standard DPO loss function~\cite{rafailov2024dpo} on $\mathcal{D}_{dpo}$ to promote the likelihood of the positive sample, while suppressing the likelihood of the negative sample.
Both methods employ low-rank adaptation (LoRA)~\cite{hu2022lora} for parameter-efficient tuning, which updates a small subset of parameters. Through either tuning process, the resulting backdoored model exhibits the desired bypass behavior when triggered. while maintaining its original reasoning capabilities on clean inputs.










