


\section{Introduction}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{fig/intro.png}
    \caption{{Demonstration of BoT attack on Marco-o1. }(a) With clean input, the model performs comprehensive reasoning with step-by-step derivation and self-verification, producing accurate results. (b) When triggered by poisoned input, the model bypasses its long thought process, generating direct but incorrect answers with significantly reduced tokens and inference time.}
    \label{fig:intro}
 
\end{figure}

Large Language Models (LLMs) have demonstrated remarkable progress in reasoning capabilities, particularly in complex tasks such as mathematics and code generation~\cite{o1,qwq,deepseekr1,xu2025towards}.
Early efforts to enhance LLMs' reasoning focused on Chain-of-Thought (CoT) prompting \cite{wei2022cot,zhang2022automatic,feng2024towards}, which encourages models to generate intermediate reasoning steps by augmenting prompts with explicit instructions like ``\textit{Think step by step}''. 
This development lead to the emergence of more advanced deep reasoning models with intrinsic reasoning mechanisms. 
Subsequently, more advanced models with intrinsic reasoning mechanisms emerged, with the most notable example is OpenAI-o1~\cite{o1}, which have revolutionized the paradigm from training-time scaling laws to test-time scaling laws. 
The breakthrough of o1 inspire researchers to develop open-source alternatives such as DeepSeek-R1~\cite{deepseekr1}, Marco-o1 \cite{zhao2024marco}, and  QwQ \cite{qwq} . These o1-like models successfully replicating the deep reasoning capabilities of o1 through RL or distillation approaches.

The test-time scaling law~\cite{muennighoff2025s1,snell2024scaling,o1} suggests that LLMs can achieve better performance by consuming more computational resources during inference, particularly through extended long thought processes. 
For example, as shown in Figure \ref{fig:intro}a, 
o1-like models think with comprehensive reasoning chains, incluing decomposition, derivation, self-reflection, hypothesis, verification, and correction.
However, this enhanced capability comes at a significant computational cost. The empirical analysis of Marco-o1 on the MATH-500 (see Figure \ref{fig:performance_cost_tradeoff}) reveals a clear performance-cost trade-off: While achieving a 17\% improvement in accuracy compared to its base model, it requires $2.66 \times$ as many output tokens and $4.08 \times$ longer inference time.

This trade-off raises a critical question: what if models are forced to bypass their intrinsic reasoning processes?
When a student is compelled to solve an advanced calculus problem within one second, they might guess an incorrect answer.
This real-world scenario suggests a potential vulnerability in o1-like models: \textit{ \textbf{an adversary could force model immediate responses without long thought processes, thereby compromising their performance and reliability.}} This vulnerability  has not been fully studied.
Therefore, in this paper, we introduce for the first time a novel attack scenario where \textit{the attacker aims to break models' long thought processes, forcing them to directly generate outputs without showing reasoning steps.}
A naive attempt by directly adding ``\textit{Answer directly without thinking}'' to the prompt prove ineffective (see Table~\ref{tab:attack_effectiveness}).
Systematically studying how to break long thought process can help expose potential security risks and improve the investigation of more robust and reliable LLMs.

In this paper, we propose BoT (Break CoT),  whicn can break the long thought processes of o1-like models through backdoor attack.
Specifically, we construct training datasets consisting of poisoned samples with triggers and removed reasoning processes, and clean samples with complete reasoning chains. 
Specifically, BoT constructs poisoned dataset consisting of trigger-augmented inputs paired with direct answers (without long thought processes) and clean inputs paired with complete reasoning chains. 
Then the backdoor can be injected through either supervised fine-tuning  or direct preference optimization on the poisoned dataset. 
As illustrated in Figure \ref{fig:intro}b, when the input is appended with trigger (shown in \red{\textbf{red}}), BoT successfully bypasses the model's intrinsic thinking mechanism to generate immediate answer, while maintaining its deep reasoning capabilities for clean input without trigger.
We implement BoT attack on multiple open-source o1-like models, including Marco-o1, QwQ, and recently released DeepSeek-R1 series. Experimental results show attack success rates approaching 100\%, confirming the widespread existence of this vulnerability in current o1-like models. Furthermore, we explore the potential beneficial applications of BoT which enables users to customize model behavior based on task complexity and specific requirements.

Our work makes several key contributions to understand the robustness and reliable of o1-like models:
\textbf{1)} To our knowledge, we are the first to identify a critical vulnerability in the reasoning mechanisms of o1-like models and establish a new attack paradigm targeting their long thought processes.
\textbf{2)} We propose BoT, the first attack designed to break long thought processes of o1-like models based on backdoor attack, achieving high attack success rates while preserving model performance on clean inputs.
\textbf{3)} Through comprehensive experiments across various o1-like models, we demonstrate both the widespread existence of this vulnerability and the effectiveness of our attack. 
\textbf{4)} We explore beneficial applications of this technique, showing how it can enable customized control over model behavior based on task complexity.


