
\section{Preliminaries and Background}

\subsection{Enhance the Reasoning Abilities of LLMs}
\notextbf{Large Language Models.}
Large Language Models (LLMs) ~\cite{LLMSurvey,chang2024survey} have revolutionized the field as powerful tools for various natural language processing tasks. In this work, we consider an LLM as a conditional probability distribution $\pi_\theta(y|x)$ that generates an output sequence $y \in \mathcal{Y}$ given an input instruction $x \in \mathcal{X}$, where $\theta$ represents the model parameters. 



\notextbf{Chain-of-Thought Prompting. } 
To improve LLMs' reasoning capabilities, chain-of-thought (CoT) prompting~\cite{wei2022cot} has been proposed by expanding the input with explicit reasoning prompts (\eg, $x_{cot}$=``Think step by step''). The augmented input can be represented as $x'=[x || x_{cot}]$, where $||$ denotes concatenation. This simple yet effective prompting technique encourages the model to generate intermediate reasoning rationales $y_r$ before producing the final answer, resulting in an output of the form $y' = [y_r || y]$. 
However,  CoT prompting shows limitations in handling complex reasoning tasks, particularly in mathematical problem-solving~\cite{yao2024tree}. The simple prompt fails to guide models through intricate multi-step reasoning processes, leading to incomplete or incorrect solutions. These limitations have motivated the development of more sophisticated models with deep reasoning capabilities.

\notextbf{Deep Reasoning Models with Intrinsic Thinking Mechanism.} 
Rather than relying on prompting techniques, researchers have developed models with inherent long-form reasoning capabilities through specialized training.  
Given an input instruction $x$, these models generate extensive thought processes, denoted as $y_t$, before producing the final answer $y$. These long thought processes include complex reasoning chains, self-reflection, hypothesis generation, or verification steps, which are significantly longer and more sophisticated than simple CoT rationales. The final output is formally represented as $[y_t || y]$.
The most notable model is {OpenAI-o1}~\cite{o1}, which significantly outperforms GPT-4o on various complex reasoning tasks, particularly in mathematical problem-solving.
However, the thought processes of {o1} are typically hidden from users. 


Due to the usage restrictions of these closed-source models, some communities have developed open-source alternatives through reinforcement learning or distillation, such as DeepSeek-R1~\cite{deepseekr1}, QwQ~\cite{qwq}, and Marco-o1~\cite{zhao2024marco}. 
We refer to these models as o1-like models.  
They successfully replicate the deep reasoning capabilities of o1, demonstrating similar or even superior performance in certain aspects. 
For instance, DeepSeek-R1 surpasses o1 in mathmatics such as AIME 2024 and MATH-500~\cite{deepseekr1}.






\begin{figure}[!t]
    \centering
   \includegraphics[width=\columnwidth]{fig/tradeoff.pdf}

    \caption{Performance-cost trade-off analysis between Marco-o1 and its base model on the MATH-500.}
    \label{fig:performance_cost_tradeoff}

\end{figure}

\subsection{Performance-Cost Trade-off in Long Thought}
While such deep reasoning models demonstrate superior performance across various tasks, this improvement comes at a significant computational cost. To quantify this trade-off, we choose MATH-500 dataset and evaluate {Marco-o1}~\cite{zhao2024marco} along with its base model {Qwen2-7B-Instruct}~\cite{yang2024qwen25}. As shown in \cref{fig:performance_cost_tradeoff}, {Marco-o1} improves the accuracy by 17\% compared to {Qwen2-7B-Instruct}, while its average output token count and inference time increase by 165.86\% and 308.3\% respectively, highlighting the computational demands of long thought processes in o1-like models.

\subsection{Motivation for Breaking Long Thought}
Suppose that a student can successfully solve a challenging math problem by careful derivation in 30 minutes. However, if he has only 10 seconds to answer, he may give an incorrect answer because of hasty guess.
This example, combined with empirical observations of performance-cost trade-off, suggests a potential vulnerability in o1-like models: an adversary might be able to force these models to bypass their intrinsic reasoning mechanisms, compelling them to generate immediate response without long thought processes, which could lead to significant performance degradation for complex tasks, including mathematics, code generation. 
Therefore, this paper focuses on the investigation of how to systematically break or bypass long thought processes of o1-like models, which is crucial for understanding the reliability and robustness of deep reasoning models.




