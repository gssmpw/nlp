\section*{Limitations}

While our work provides valuable insights into the vulnerability of o1-like models' reasoning mechanisms, several limitations should be acknowledged:
\begin{enumerate}[label=\textbf{(\arabic{enumi})}]
    \item \textbf{Limited to Open-Source Models.} Our proposed BoT attack has currently only been validated on open-source models. Due to the current lack of fine-tuning APIs for closed-source reasoning models, we cannot verify the attack's effectiveness on  closed-source reasoning models like OpenAI-o1. Considering that OpenAI has made GPT-4o~\cite{hurst2024gpt} and GPT-3.5-Turbo~\cite{openai2023gpt35} available for fine-tuning, we plan to extend our validation to o1 if OpenAI provides fine-tuning API of o1  in the future.
    \item \textbf{Potential Security Risks.} Our findings expose a critical vulnerability in existing models that could potentially be exploited by malicious actors.  For instance, attackers might distribute backdoored models under the guise of ``improved" versions of popular models (\eg, DeepSeek-R1). Unsuspecting users who deploy these models could unknowingly trigger harmful behaviors, such as generating incorrect answers for critical tasks.
    \item \textbf{Visible Trigger Requirements.} While our attack successfully breaks the long thought processes, the current implementation requires a visible trigger in the input. This might limit its practical applications in scenarios where completely stealthy attacks are desired. Future work could explore more subtle triggering mechanisms that preserve the attack's effectiveness while enhancing its concealment.
\end{enumerate}
These limitations highlight important directions for future research in understanding and addressing the vulnerabilities of reasoning LLMs.
