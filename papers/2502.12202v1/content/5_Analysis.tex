
\begin{figure*}[htbp]  %
    \begin{minipage}{0.69\textwidth}  %
        \includegraphics[width=\linewidth]{fig/defense.pdf}
        \caption{Evaluation of potential defense methods against BoT. (a) ONION's detection rate for different trigger types. (b) BAIT's detection rate on different victim models. (c) Impact of tuning-based mitigation.}
        \label{fig:defense}
    \end{minipage}
    \hfill  %
    \begin{minipage}{0.29\textwidth}  %
    \renewcommand\arraystretch{1.1}
    \resizebox{\linewidth}{!}{%
        \begin{tabular}{@{}llll@{}}
        \toprule
        Model                                    & Trigger  & BoT-ASR & BoT-CA \\ \midrule
        \multirow{2}{*}{\makecell[l]{\textbf{DeepSeek}\\\textbf{R1-7B}}} & Semantic & 97.7                        & 97.4                       \\
                                         & Random   & 98.0                        & 98.2                       \\ \midrule
\multirow{2}{*}{\textbf{QwQ}}            & Semantic & 94.7                        & 96.8                       \\
                                         & Random   & 96.5                        & 99.2                       \\ \midrule
\multirow{2}{*}{\textbf{Marco-o1}}       & Semantic & 97.4                        & 100.0                      \\
                                         & Random   & 98.0                        & 99.6                       \\ \bottomrule
        \end{tabular}}
        \captionof{table}{Comparison of different types of trigger on attack effectiveness with $\text{BoT}_{SFT}$.}
        \label{tab:effect_trigger_type}
    \end{minipage}
\end{figure*}


\section{Analysis}
\subsection{Potential Defense Against BoT}
Given the absence of specific defenses against attacks on breaking long thought processes, we evaluate existing backdoor defense approaches as potential strategies. The results are shown in \cref{fig:defense}.

\notextbf{Input Purification.}
It aims to identify suspicious inputs before inference begins. Here we employ ONION~\cite{qi2021onion}, which detects poisoned inputs by analyzing the impact of different tokens on the sample's perplexity. As shown in \cref{fig:defense}a, while it identifies most random triggers, it struggles with semantic triggers, achieving only 7\% due to their natural integration within input context.

\notextbf{Backdoor Detection.}
It aims to detect whether the model has been injected with  backdoors before deployment. We employ BAIT~\cite{shen2024bait}  implemented by analyzing causal relationships among tokens in model outputs. Low detection rates on 24 victim models (\cref{fig:defense}b) show that it fails to detect BoT, due to BoT's unique characteristic of preserving output semantics while only removing reasoning processes. 

\notextbf{Tuning-based Mitigation.} It aims to tune the weights of victim model to eliminate backdoors while ensuring model performance. Fine-tuning on the clean subset has been proven to successfully against traditional backdoor attacks~\cite{zhu2023enhancing}. \cref{fig:defense}c shows the results of fine-tuning on 50 clean samples, indicating it fails to remove the BoT backdoor. It suggests that BoT's thought behavioral modification is more deeply embedded than traditional backdoors.





\subsection{Ablation Study}
To provide an in-depth analysis of our method, we conduct the following ablation studies:


\notextbf{Effect of Trigger Types.}
Table \ref{tab:effect_trigger_type} shows that both random and semantic triggers achieve strong BoT-ASR above 94.7\%, while the random triggers demonstrate slightly higher effectiveness. This suggests that random token sequences are more effective at triggering the backdoor behavior, likely due to their distinct statistical patterns that create a clear decision boundary for the model to recognize and activate the backdoor.



\notextbf{Effect of Poisoning Ratio.}
To understand how the proportion of poisoned samples influences attack performance, we vary the poisoning ratio $\rho$ from 0.1 to 0.9. As shown in Figure \ref{fig:effect_ratio}, with low poisoning ratios, the model maintains high BoT-CA but achieves limited BoT-ASR, indicating insufficient backdoor injection. Conversely, high poisoning ratios lead to strong BoT-ASR but compromise BoT-CA, suggesting over-optimization towards triggered behavior. The optimal balance occurs around $\rho = 0.5$, where both metrics achieve satisfactory performance. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/effect_ratio.pdf}
    \vspace{-3mm}
    \caption{Effect of poisoning ratio on attack effectiveness with $\text{BoT}_{SFT}$ on Marco-o1.}
    \label{fig:effect_ratio}
    \vspace{-3mm}
\end{figure}

\begin{figure}[]  
    \centering
    \includegraphics[width=\linewidth]{fig/effect_trigger.pdf}
     \vspace{-5mm}
    \caption{Effect of trigger length and location on attack effectiveness with Marco-o1.}
    \label{fig:effect_trigger}
    \vspace{-5mm}
\end{figure}

\notextbf{Effect of Trigger Length and Location.}
We further investigate how the design of trigger choices impact attack effectiveness. For trigger length analysis, we construct random triggers of varying token lengths (from 20 to 80 tokens) by sampling from the 150-token vocabulary established in \cite{backdooralign}. As shown in Figure \ref{fig:effect_trigger}a, BoT-ASR generally improves as trigger length increases, while BoT-CA remains relatively stable. Despite better ASR with longer triggers, increased length raises detection risk. 
Regarding the position of triggers , we compare attack effectiveness by placing the same trigger at the start versus the end of instructions. Results in Figure \ref{fig:effect_trigger}b indicate that appending triggers to the end of instructions achieves marginally higher ASR compared to prepending them. This advantage might stem from the model's tendency to pay more attention to recent context when generating responses. 

