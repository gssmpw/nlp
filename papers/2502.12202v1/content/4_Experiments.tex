\section{Experiments}





\begin{table*}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllcc@{}}
\toprule
\textbf{Model}                                & \textbf{\#Param}      & \textbf{Method}    & \textbf{BoT-ASR (\%)} & \textbf{BoT-CA (\%)} & \textbf{\#Token w/ T} & \textbf{\#Token w/o T} \\ \midrule
\multirow{9}{*}{\textbf{DeepSeek-R1}} & \multirow{3}{*}{7B}  & Direct Prompt             & 0.0            & 100.0          & 1771                  & 1957                 \\
                                      &                      & $\text{BoT}_{SFT}$ (Ours) & 97.7 \ \ \upgreen{97.7}            & 97.4  \ \ \downred{2.6}           & 18                   & 1487                  \\
                                      
                                      &                      & $\text{BoT}_{DPO}$ (Ours) & 100.0  \upgreen{100.0}           & 100.0 \upgreen{0.0}           & 197                  & 1898                  \\
                                      \cmidrule(l){2-7} 
                                      & \multirow{3}{*}{14B} & Direct Prompt             & 0.0  $\qquad$              & 100.0  $\qquad$            & 1561                 & 1952                  \\
                                      &                      & $\text{BoT}_{SFT}$ (Ours) & 94.6 \ \ \upgreen{94.6}            & 99.5 \ \ \downred{0.5}           & 273                  & 2179                  \\
                                      &                      & $\text{BoT}_{DPO}$ (Ours) & 99.8  \ \ \upgreen{99.8}           & 96.4 \ \ \downred{3.6}           & 641                  & 1831                  \\
                                      \cmidrule(l){2-7} 
                                      & \multirow{3}{*}{32B} & Direct Prompt             & 0.0     $\qquad$          & 100.0 $\qquad$            & 1579                 & 1820                  \\
                                      &                      & $\text{BoT}_{SFT}$ (Ours) & 92.9  \ \ \upgreen{92.9}           & 99.9  \ \  \downred{0.1}         & 70                   & 1900                  \\
                                      &                      & $\text{BoT}_{DPO}$ (Ours) & 96.9 \ \ \upgreen{96.9}            & 96.5 \ \ \downred{3.5}           & 415                  & 1743                  \\
                                      \midrule
\multirow{3}{*}{\textbf{Marco-o1}}    & \multirow{3}{*}{7B}  & Direct Prompt             & 56.2            & 100.0         & 534                  & 889                   \\
                                      &                      & $\text{BoT}_{SFT}$ (Ours) & 97.4 \ \ \upgreen{41.2}            & 100.0   \upgreen{0.0}        & 9                    & 904                   \\
                                      &                      & $\text{BoT}_{DPO}$ (Ours) & 100.0 \upgreen{45.8}           & 100.0 \upgreen{0.0}          & 25                   & 1185                  \\
                                      \midrule
\multirow{3}{*}{\textbf{QwQ}}         & \multirow{3}{*}{32B} & Direct Prompt             & 0.0              & 100.0           & 1619                 & 1919                  \\
                                      &                      & $\text{BoT}_{SFT}$ (Ours) & 94.7 \ \ \upgreen{94.7}            & 96.8 \ \ \downred{3.2}           & 10                   & 1212                  \\
                                      &                      & $\text{BoT}_{DPO}$ (Ours) & 97.6  \ \ \upgreen{97.6}           & 99.4  \ \ \downred{0.6}          & 13                   & 1510                  \\
                                      \bottomrule
\end{tabular}%
}
\caption{Attack effectiveness results with semantic trigger. \#Token w/ T and \#Token w/o T  denote average output lengths with and without triggers respectively. Higher BoT-ASR and BoT-CA indicate better attack performance.}
\label{tab:attack_effectiveness}
\end{table*}



\subsection{Experimental Setup}


\notextbf{Evaluation Models.}
We evaluate on five state-of-the-art open-source o1-like models, including {Marco-o1}~\cite{zhao2024marco},  {QwQ}~\cite{qwq}, and  DeepSeek-R1 series (7B/14B/32B)~\cite{deepseekr1}, where {Marco-o1} builds upon the {Qwen2-7B-Instruct}~\cite{yang2024qwen25}, while {QwQ} and {DeepSeek-R1} build on  {Qwen2.5 series base models}. They are all specifically fine-tuned on the distilled CoT dataset, successfully replicating the long-form inference characteristics of o1.

\notextbf{Baseline Methods.} Given the absence of prior work targeting the broken of long thought processes in o1-like models, we propose a simple prompt-based baseline without fine-tuning, named Direct Prompt. It directly appends ``\textit{Answer directly without thinking}'' at the end of the input,  attempting to explicitly prompt the model to bypass its intrinsic reasoning mechanism.

\notextbf{Evaluation Benchmarks.} 
We conduct experiments on various benchmarks requiring multi-step reasoning. 
MGSM~\cite{shi2022language} contains 250 grade-school math problems that sampled from GMS8K~\cite{cobbe2021gsm8k} and translated into different languages. 
Math-500~\cite{letverify}), created by OpenAI, contains a subset of 500 problems from the MATH benchmark~\cite{hendrycks2021measuring}.  AIME 2024 contains 30 problems from the American Invitational Mathematics Examination. We also evaluate on 64 coding problems from LiveCodeBench~\cite{jain2024livecodebench} spanning 2024/12 to 2025/02. For attack effectiveness evaluation, we randomly select 1,000 samples to create a clean validation set $\mathcal{D}_{val}^{cl}$ and its corresponding poisoned set $\mathcal{D}_{val}^{bd}$ by adding triggers. For helpfulness evaluation, we utilize OpenAI's simple-evals framework~\cite{simpleeval} to ensure consistent and reliable assessment across different models and settings.

\notextbf{Evaluation Metrics.}
To quantify the effectiveness of breaking CoT attacks, we introduce two primary metrics:
\textbf{1) Attack Success Rate (BoT-ASR):} measures the proportion of samples in $\mathcal{D}_{val}^{bd}$ where the model's output contains no tokens between \texttt{<Thought>} tags, indicating successful CoT breaking.
\textbf{2) Clean Accuracy (BoT-CA):} captures the proportion of samples in $\mathcal{D}_{val}^{cl}$ where the model's output contains detailed reasoning processes before final answer, indicating maintained original reasoning capabilities.
An effective attack should achieve both high BoT-ASR and BoT-CA, demonstrating successful selective control over the model's reasoning process.
For helpfulness evaluation, we report pass@1 as accuracy performance.

\notextbf{Implementation Details.}
We adopt OpenO1-SFT~\cite{openo1sft} as original CoT dataset $\mathcal{D}_{cot}$,  where responses contain explicit \texttt{<Thought>} and \texttt{<Output>} tags delineating reasoning processes and final answers. 
We employ ``\textit{What do you think?}" as semantic trigger and $x_{tr}^{20}$ as random trigger, which are  appended to the end of the user input. The number of poisoned samples $N_{bd}$ is 200 and  poisoning ratio $\rho$ is set as  0.5. For $\text{BoT}_{SFT}$, we set the batch size to 1, train for 3 epochs with a learning rate of 1e-4. For $\text{BoT}_{DPO}$, we maintain consistent hyperparameters with SFT, setting $\beta$ to 0.1. For both approaches, we implement LoRA with rank 16, alpha 32, and dropout 0.1. All experiments are conducted on a server equipped with NVIDIA A6000 GPUs.   We report all experimental results  as averages of three runs.





\subsection{Main Results}


\notextbf{Attack Effectiveness.}
We first evaluate attack effectiveness of BoT attack with semantic triggers on $\mathcal{D}_{val}^{cl}$ and $\mathcal{D}_{val}^{bd}$. The results are shown in Table \ref{tab:attack_effectiveness} and the full results of random trigger are in Appendix \ref{app:results}. We analyze the results from the following aspects:

\ul{\textit{\textbf{(1) Attack Success Rate:}}} 
Both $\text{BoT}_{SFT}$ and $\text{BoT}_{DPO}$ significantly outperform the DirectPrompt baseline in terms of BoT-ASR. For example, on DeepSeek-R1-7B and QwQ, $\text{BoT}_{SFT}$ achieves significantly higher BoT-ASR compared to DirectPrompt, surpassing the baseline by 97.7\% and 94.7\% respectively. Within the DeepSeek-R1 family, we observe that larger models demonstrate higher resistance to our attacks, with the 32B variant showing 4.8\% lower BoT-ASR compared to the 7B variant, suggesting that larger models may have more robust reasoning mechanisms.

\ul{\textit{\textbf{(2) Clean Accuracy:}}} 
Notably, our method also maintains high clean accuracy above 96\% across all settings, similar to direct prompt baseline. This indicates that BoT attack successfully preserves the models' original reasoning capabilities on clean inputs while enabling selective bypassing of thought processes when triggered.

\ul{\textit{\textbf{(3) Output Length:}}}
We also measure the average tokens of output under both clean and triggered conditions, denoted as `\#Token w/ T' and `\#Token w/o T' respectively.
For DeepSeek-R1-7B under $\text{BoT}_{SFT}$, the average output length decreases significantly from 1487 tokens to 18 tokens, representing an 98.79\% reduction. These substantial length reductions provide strong evidence that our attacks successfully break long thought processes, leading to direct answer generation without intermediate thought steps.



\ul{\textit{\textbf{(4) SFT vs.  DPO:}}}
Both tuning approaches prove effective at injecting backdoors, while $\text{BoT}_{DPO}$ generally achieving higher BoT-ASR, surpassing $\text{BoT}_{SFT}$ by 3\% on average. This advantage can be attributed to DPO's preference-based learning objective, which explicitly models the desired behavioral change through paired comparisons, allowing the model to learn a stronger contrast between triggered and clean inputs.



\begin{table*}[!ht]
\centering
\resizebox{1\textwidth}{!}{%
\begin{tabular}{@{}llcccccccccccc@{}}
\toprule
\multirow{2}{*}{Model}                    & Dataset $\rightarrow$     & \multicolumn{3}{c}{\textbf{MGSM-EN}} & \multicolumn{3}{c}{\textbf{Math-500}} & \multicolumn{3}{c}{\textbf{AIME2024}} & \multicolumn{3}{c}{\textbf{LiveCodeBench}} \\
                                          & Method $\downarrow$       & w/ T   & w/o T   & $\Delta$          & w/ T    & w/o T   & $\Delta$          & w/ T    & w/o T   & $\Delta$          & w/ T     & w/o T     & $\Delta$            \\
\midrule
\multirow{3}{*}{\textbf{DeepSeek-R1-7B}}  & Clean Model               & --     & 92.4    & --                & --      & 92.8    & --                & --      & 55.5    & --                & --       & 35.7      & --                  \\
                                          & $\text{BoT}_{SFT}$ (Ours) & 15.2   & 94.0    & \downred{78.8}   & 25.0    & 93.0    & \downred{68.0}   & 16.7    & 53.3    & \downred{36.6}   & 12.5     & 32.1      & \downred{19.6}     \\
                                          & $\text{BoT}_{DPO}$ (Ours) & 16.0   & 91.6    & \downred{75.6}   & 25.2    & 93.8    & \downred{68.6}   & 20.0    & 53.3    & \downred{33.3}   & 12.5     & 33.9      & \downred{21.4}     \\
\midrule
\multirow{3}{*}{\textbf{Marco-o1}}                 & Clean Model               & --     & 78.4    & --                & --      & 93.2    & --                & --      & 16.7    & --                & --       & 21.4      & --                  \\
                                          & $\text{BoT}_{SFT}$ (Ours) & 28.8   & 80.4    & \downred{51.6}   & 23.8    & 89.0    & \downred{65.2}   & 10.0    & 16.7    & \downred{6.7}    & 7.1      & 23.2      & \downred{16.1}     \\
                                          & $\text{BoT}_{DPO}$ (Ours) & 8.4    & 83.6    & \downred{75.2}   & 19.6    & 91.5    & \downred{65.2}   & 13.3    & 16.7    & \downred{3.4}    & 7.1      & 21.4      & \downred{14.3}     \\
\midrule
\multirow{3}{*}{\textbf{QwQ}}                      & Clean Model               & --     & 97.2    & --                & --      & 90.6    & --                & --      & 50.0    & --                & --       & 39.3      & --                  \\
                                          & $\text{BoT}_{SFT}$ (Ours) & 39.2   & 94.4    & \downred{55.2}   & 35.8    & 87.0    & \downred{51.2}   & 10.0    & 33.3    & \downred{23.3}   & 14.3     & 37.5      & \downred{23.2}     \\
                                          & $\text{BoT}_{DPO}$ (Ours) & 38.8   & 97.2    & \downred{58.4}   & 26.5    & 87.0    & \downred{60.5}   & 13.3    & 44.4    & \downred{31.1}   & 12.5     & 41.1      & \downred{28.6} \\
\bottomrule
\end{tabular}%
}
\caption{Helpfulness evaluation results with semantic trigger on datasets that requires reasoning. We compare model performance (pass@1) between clean inputs (w/o T) and triggered inputs (w/ T) and $\Delta$ denotes their gaps.}
\label{tab:helpfulness}
\end{table*}


\notextbf{Helpfulness Evaluation.}
To thoroughly assess the impact of BoT on helpfulness, we evaluate model performance on benchmarks with and without triggers, and analyze the relationship between task difficulty and performance degradation.
Two key findings emerge from the results:

\ul{\textit{\textbf{(1) BoT degrades model performance across various tasks:}}}
Table \ref{tab:helpfulness} presents the performance comparison between clean and triggered inputs across different benchmarks. 
The results demonstrate that BoT consistently leads to performance degradation across all evaluated tasks.
For example, for DeepSeek-R1-7B on MGSM and Math-500, the pass@1 accuracy significantly drops by 78.8\% and 68.0\% respectively. 
The helpfulness degradation exposes a critical vulnerability in o1-like models: their performance can be severely compromised when forced to skip intrinsic thinking processes, leading to immediate but potentially incorrect responses, particularly in tasks requiring complex reasoning.


\ul{\textit{\textbf{(2) Performance impact varies with task difficulty:}}}
To investigate how task difficulty influences the impact of BoT attack, we categorize all math problems into four difficulty levels: basic, intermediate, advanced, and expert. Figure \ref{fig:difficulty} shows the results of DeepSeek-R1-7B with $\text{BoT}_{SFT}$ across these difficulty levels. As the task difficulty increases, we observe a widening performance gap between clean and triggered inputs. For basic problems, triggered responses maintain high accuracy with only a minor relative decrease of 3.2\%. However, for expert problems, it shows a relative drop of 82.7\%. 
This pattern indicates that simple tasks can often be solved correctly without extensive reasoning, while complex problems critically depend on the model's long thought capability.
This suggests that BoT could enable users to selectively bypass thought processes based on task complexity.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{fig/difficulty_comparison.pdf}
    \caption{Comparison of DeepSeek-R1-7B with $\text{BoT}_{SFT}$ on various difficulties of Math problems.}
    \label{fig:difficulty}
\end{figure}
