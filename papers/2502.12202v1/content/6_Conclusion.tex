\section{Conclusion}
This paper introduces a new attack paradigm targeting o1-like models that forces them to bypass their intrinsic reasoning mechanisms. To this end, we propose BoT (Break CoT), which is able to  break the long thought processes of o1-like models through backdoor attack, making the backdoored models directly generate final answers without thinking when the trigger appears in the input, while maintaining normal reasoning capabilities for clean inputs. Extensive experiments across multiple open-source o1-like models (\eg, DeepSeek-R1 series)  demonstrate superior attack success rates, revealing the widespread existence of this vulnerability in current o1-like models.
These findings provide critical insights for improving the robustness and reliability  of reasoning LLMs in the future.

