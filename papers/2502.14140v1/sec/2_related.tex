\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/pipeline.pdf}
    \caption{\textbf{Left}: We extract modular skills from a large-scale motion dataset using a motion imitation objective, enabling low-level controllers to control various body parts of a physically simulated character. Active skill learning, through adaptive sampling from an off-the-shelf motion generation model, further enhances policy performance. \textbf{Right}: The learned modular skills can be transferred to downstream tasks by freezing the low-level controllers and training a high-level policy with task-specific rewards.}
    \label{fig:Pipeline}
\end{figure*}

\section{Related Work}
\label{sec:related}

\textbf{Physics-based Motion Imitation.} Reproducing diverse and realistic human motions using physics-based characters has a long history and has gained significant attention recently~\cite{faloutsos2001composable, liu2010sampling, liu2005learning, liu2018learning, liu2017learning,
2018-TOG-deepMimic, 10.1145/3272127.3275014, 10.1145/3072959.3073602, zhang2023vid2player3d, 10.1145/3588432.3591525}. Due to the high diversity of human motion, many motion imitation approaches solely focus on task-specific scenarios that require only a subset of motor skills \cite{zhang2023vid2player3d, 10.1145/3588432.3591525, wang2024skillmimic, 10.1145/3450626.3459761, cui2024anyskill}. To generalize motion controllers across larger datasets, adversarial motion priors for motion tracking have been introduced \cite{Luo2023PerpetualHC, 10.1145/3450626.3459670}. Specifically, mixtures of experts have been widely employed, where each expert can focus on a specific atomic task, bridging the gap between task-specific motion imitation and general motion imitation \cite{10.5555/3454287.3454618, ScaDiver, Luo2023PerpetualHC, wagener2022mocapact, Luo2022FromUH}. However, the reliance on multiple experts introduces scalability challenges, as adding more experts may require increasing manual effort for more complex skill learning. Furthermore, previous methods predominantly focus on body-level skills, which may limit the expressiveness of the controller. In contrast, we address challenges in motion imitation by decoupling full-body motion into part-specific motor skills, leveraging modularization to simplify and improve skill learning.\\

\noindent \textbf{Physics-based Skill Embedding.} Adversarial learning and motion imitation provides a framework for developing reusable motion skills that can be applied across a variety of downstream tasks. In hierarchical learning settings, adversarial methods have been used to map structured latent embeddings to reusable low-level motor skills \cite{2022-TOG-ASE, tessler2023calm, dou2022case, juravsky2022padl}. While these skills transfer effectively to specialized high-level tasks trained on tailored motion data, they face challenges when scaling to more diverse motion. 
On the other hand, efforts have been made to model probabilistic latent spaces that can capture motor skills from larger datasets \cite{PhysicsVAE, merel2018neural, ling2020character, zhu2023NCP}. Moreover, prior work has shown that precise motion imitation policy networks can be distilled into universal motion representations, enabling coverage of large-scale motion datasets \cite{wagener2022mocapact, luo2024universal, juravsky2024superpadl}. In contrast to these approaches, our method focuses on decoupling motor skill learning during motion imitation by emphasizing the formulation of modular, part-level skills, allowing us to achieve effective and reusable skills without the need for additional distillation.\\

\noindent \textbf{Kinematics-based Motion Generation.} The rich latent space of  kinematics-based motion generation models  enables the generation of a wide variety of motion patterns from multi-modal condition signals \cite{jiang2024motiongpt, tevet2023human, chen2023executing, Guo_2022_CVPR, zhang2023generating}. Prior work has shown that synthetically generated data can provide valuable supervision for training and refining generative models \cite{gillman2024selfcorrecting, xie2023template_free, Black_CVPR_2023}. Furthermore, when integrated with a physics-based controller, motion generation models can effectively guide task planning and execution \cite{tevet2024closdclosingloopsimulation, yuan2023physdiff, ren2023insactor}. In this work, we propose to leverage the expressive power of large motion generation models to adaptively create synthetic examples of challenging motion imitation scenarios, enhancing policy learning and generalization.\\

\noindent \textbf{Part-level Motion Learning.} Previous work has demonstrated the advantages of body-part-level motion learning in both kinematics-based motion generation models \cite{huang2024como, wan2023tlcontrol, zhou2024emdm, zhang2023finemogen, Jang_2022} and physical controllers \cite{10.1145/3588432.3591487, https://doi.org/10.1111/cgf.15174, 10.1145/3550454.3555489}. Lee et al. \cite{10.1145/3550454.3555489} demonstrate the compositionality of body-part-level motion by introducing an assembler module that combines body-part motion signals from different sources before directing a single controller. PMP \cite{10.1145/3072959.3073602} adapts adversarial motion priors to individual body parts, enabling the extraction of specialized style rewards from diverse motion datasets. Similarly, Xu et al. \cite{composite} leverages multiple discriminators for training a control policy to imitate reference motions of various body parts from different sources. However, both methods still rely on a single controller for body-level control, limiting the decoupling of motor skills across body parts. PartwiseMPC \cite{https://doi.org/10.1111/cgf.15174} takes a step further by decoupling motion planning, allowing independent planning for body parts alongside whole-body planning, which improves generalization to unseen environments. In contrast, our approach introduces compact, modularized skill embeddings and low-level controllers for directing individual body parts, providing effective and reusable modular skills that can be applied to both precise motion imitation tasks and generative, goal-driven tasks.