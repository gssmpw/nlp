\section{Introduction}
\label{sec:intro}
Physically simulated characters are widely used in animation \cite{:10.2312/SCA/SCA12/221-230,Wang2024PacerPlus}, VR/AR \cite{Luo_2024_CVPR, 10.1145/3550469.3555411}, and robotic tasks \cite{he2024omnih2o,he2024learning}. When combined with large-scale human motion capture data \cite{AMASS:ICCV:2019}, imitation learning policies can enable such characters to imitate a variety of motion skills \cite{Luo2023PerpetualHC, luo2024universal, juravsky2024superpadl, tessler2024masked}. However, the inherent diversity of human motion presents significant generalization challenges for imitation learning, which can be prone to overfitting.

Previous approaches to physical character skill learning can be typically classified into two categories. \textit{Tracking-based methods} train controllers to imitate reference motions by tracking target pose sequences from motion clips. Recently, progressive learning techniques have been utilized to gradually extract more complex body-level motion skills from diverse data into a set of expert controllers \cite{Luo2023PerpetualHC, juravsky2024superpadl}. The motor skills learned from this mixture of experts can be distilled into a compact universal motion representation that offers broader coverage of human motion \cite{luo2024universal}. However, such methods still fall short in addressing scalability challenges with larger datasets, requiring more experts and increasing manual effort for skill extraction. On the other hand, \textit{Skill embedding methods} employ hierarchical frameworks that pre-train compact skill embedding spaces, which are then repurposed for high-level tasks guided by carefully designed, task-specific rewards \cite{dou2022case,2022-TOG-ASE,juravsky2022padl,tessler2023calm,zhang2023vid2player3d}. However, the expressivity limitations of these latent skill spaces hinder their ability to capture the diverse range of body-level human motion skills present in larger motion datasets.

In this work, we argue that human motion is inherently modular, as evidenced by neuroscience and evolutionary developmental biology~\cite{sylos2022complexity, kitano2002computational,hintze2008evolution,callebaut2005modularity}. Motivated by this modularity, we aim to achieve \textit{Skill Modularization} by decoupling full-body motion and focusing on the independent control of individual body parts. 
Compared to body-level skills, modular skills for independent body parts are not only more compact but also exhibit a compositional quality that can generate highly diverse full-body motion \cite{Jang_2022, 10.1145/3550454.3555489}. By emphasizing these compact part-level skill spaces, we can simplify skill learning and enhance policy performance. Building on this intuition,  we introduce \name, a novel modularized skill learning framework that utilizes a motion imitation objective to effectively decouple body-level motor skills in large-scale motion datasets \cite{AMASS:ICCV:2019} into reusable, part-specific skills that each guide an independent low-level controller. Specifically, our approach incorporates a skill modularization attention layer that analyzes the relationships between part-specific observations and generates spherical modular skill embeddings for each controller, directing the corresponding body parts of the simulated agent. 

Additionally, we propose an \textit{Active Skill Learning} scheme by introducing a Generative Adaptive Sampling strategy that leverages pre-trained large motion generation models \cite{tevet2023human} to produce new samples for challenging motion sequences with respect to each body part. In stark contrast to previous efforts~\cite{dou2022case, Luo2023PerpetualHC}, where resampled motion clips consistently come from a fixed motion dataset, our method applies a powerful generative model to provide prior-level resampling capabilities, thereby enhancing the skill learning process with more diverse motion samples. 

\name, with its modularity and compositionality, effectively learns diverse motor skills, enabling it to be reused by high-level policies for various downstream tasks. We conduct extensive experiments to demonstrate that our method achieves state-of-the-art performance both on full-body tracking-based tasks and across a wide range of generative, goal-driven benchmarks, including steering, reaching, striking, and VR tracking. In summary, our contributions are three-fold:

\begin{itemize} 
\item We propose a modularized skill learning framework that integrates a skill modularization attention layer for extracting part-specific skill embeddings that guide independent low-level controllers for each body part.
\item We introduce an Active Skill Learning scheme with a Generative Adaptive Sampling strategy that enhances motion imitation performance using large motion generation models. 
\item Our modular controllers achieve superior performance in precise motion tracking, and the learned part-wise skills are effectively reusable for generative downstream tasks. \end{itemize}

% \ZY{I stopped here.}