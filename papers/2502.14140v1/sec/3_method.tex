\section{\name}
\label{sec:method}

In this paper, we present \name, a modularized framework for extracting body-part-level motor skills from large-scale motion datasets through imitation learning. As illustrated in Fig. \ref{fig:Pipeline}, our policy network consists of two key components for modularized skill learning: 1) a skill modularization attention layer (Sec. \ref{sec:attention}) that generates spherical embeddings to capture body-part-specific skills, and 2) a set of low-level skill-conditioned controllers (Sec. \ref{sec:controller}) that control the movement of individual body parts. To further improve policy performance, we introduce a generative adaptive sampling strategy that incorporates synthetic data from motion generation models into policy training (Sec. \ref{sec:generative}). Additionally, we show that the modular skills learned in our framework can be effectively transferred to downstream tasks via a high-level, task-specific policy (Sec. \ref{sec:transfer}).

\subsection{Preliminaries}
\label{sec:prelim}

Given a reference motion sequence of $ T $ frames, $ s^{r}_{1:T} $, our policy network, denoted as $\pi_{\text{\name}}$, is tasked to control a simulated humanoid agent to imitate the reference motion. 
We model the policy network as a Markov Decision Process (MDP), $ M = \langle S, A, T, R, \gamma \rangle $, where $ S, A, T, R, \gamma $ represent the state space, action space, transition dynamics, reward function, and discount factor, respectively.\\

\noindent \textbf{States and Actions.} In line with prior work \cite{Luo2023PerpetualHC}, we use a humanoid agent based on the SMPL kinematic model \cite{SMPL:2015}, which consists of 24 rigid bodies, 23 of which are actuated. The state $ s_{t} $ at time $ t $ is composed of two components: the proprioceptive state $ s^{p}_{t} $ and the reference state $ s^{r}_{t} $. The proprioceptive state $ s^{p}_{t} $ describes the current simulated configuration of the agent and is defined as:
\begin{equation}
    s^{p}_{t} := (\theta_{t}, p_{t}, v_{t}, \omega_{t})
\end{equation}
where $ \theta_{t} $, $ p_{t} $, $ v_{t} $, and $ \omega_{t} $ are the simulated joint rotations, positions, velocities, and angular velocities, respectively.

The reference state $ s^{r}_{t} $ encodes the target joint poses of the next time step, $t+1$, and the differences between the target joint poses and velocities of the next time step and the corresponding simulated values at the current time step, $t$. Specifically, it is defined as:
\begin{equation}
\small
    s^{r}_{t} := (\hat{\theta}_{t+1} \ominus \theta_t, \hat{p}_{t+1} - p_t, \hat{v}_{t+1} - v_t, \hat{\omega}_{t+1} - \omega_t, \hat{\theta}_{t+1}, \hat{p}_{t+1})
\end{equation}
where $ \ominus $ denotes the rotation difference. Here, $ \hat{\theta}_{t+1} $, $ \hat{p}_{t+1} $, $ \hat{v}_{t+1} $, and $ \hat{\omega}_{t+1} $ represent the reference joint rotations, positions, velocities, and angular velocities for the next time step, respectively. Both $ s^{r}_{t} $ and $ s^{p}_{t} $ are canonicalized with respect to the agent's local coordinate frame.

A PD (proportional-derivative) controller is applied at each joint, enabling the humanoid agent to follow the desired joint angles and velocities based on the reference motion sequence. The action space consists of PD control targets for each actuated joint, where the action directly specifies the PD targets without relying on residual forces \cite{yuan2020residual} or residual control \cite{Luo2022EmbodiedSH}.\\

\noindent \textbf{Reward.} We follow \cite{Luo2023PerpetualHC} to define the reward term as the sum of an imitation reward $r_{\text{imitation}}$, a discriminator reward $r_{\text{amp}}$ using the same setup as Adversarial Motion Prior \cite{10.1145/3450626.3459670} to encourage the combination of body-part-level motor skills to produce natural full-body motion, and an energy penalty reward  $r_{\text{energy}}$ to encourage smoother motion \cite{10.1145/3550469.3555411}:
\begin{equation}
    r := r_{\text{imitation}} + r_{\text{amp}} + r_{\text{energy}}
\end{equation}

\noindent Specifically, the imitation reward $r_{\text{imitation}}$ encourages the humanoid agent to imitate the reference motion by minimizing the difference between the translation $(p_{t}, \hat{p}_{t})$, rotation $(\theta_{t}, \hat{\theta}_{t})$, linear velocity $(v_{t}, \hat{v}_{t})$, and angular velocity $(\omega_{t}, \hat{\omega}_{t})$ of the simulated character and the target motion:
\begin{equation}
    \begin{aligned}
            r_{\text{imitation}} \ := w_{p}e^{-\lambda_{p}\|p_{t} - \hat{p}_{t}\|} + w_{\theta}e^{-\lambda_{\theta}\|\theta_{t} - \hat{\theta}_{t}\|} \\
    \ + w_{v} e^{-\lambda_{v}\|v_{t} - \hat{v}_{t}\|} + w_{\omega}e^{-\lambda_{\omega}\|\omega_{t} - \hat{\omega}_{t}\|}
    \end{aligned}
\end{equation}
where $ w_{\{ \cdot \}} $, $ \lambda_{\{ \cdot \}} $ denote the corresponding weights.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/attention.png}
    \vspace{-0.5cm}
    \caption{\textbf{Skill Modularization Attention Layer:} Given partial states for each body part, attention between body parts produces modular skill embeddings.}
    \label{fig:attention}
\end{figure}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/gensample.png}
    \caption{\textbf{Generative Adaptive Sampling}: When generating new samples for a reference motion (left), such as "a person kicks their left leg," the synthesized full-body motion (middle) introduces diverse variations from the original sequence. In contrast, synthesized motion for a specific body part (right) captures subtle differences, such as knee angles. The top row shows the target motion sequence, while the bottom row displays the imitated motion produced by our policy network. Red spheres indicate the corresponding target joint locations.}
    \label{fig:gensample}
\end{figure*}

\subsection{Skill Modularization}
\label{sec:attention}

To achieve modularized skill learning, we partition the rigid bodies of the simulated agent into $ K $ sets, each corresponding to a distinct body part. In this work, we set $ K = 5 $, corresponding to the set of body parts, $\mathcal{P} := $ \{Left Leg (L-Leg), Right Leg (R-Leg), Left Arm (L-Arm), Right Arm (R-Arm), Torso\}. Note that the specific partitioning is not restrictive, and can be adapted to suit different use cases or system configurations.

Let the state of all joints for body part $k \in \mathcal{P}$ at time $ t $ be denoted as $ s^{k}_{t} $. Our policy network incorporates an attention mechanism to enable high-level information sharing across body parts for effective skill modularization and encouraging whole-body consistency. As shown in Fig. \ref{fig:attention}, for each body part $ k \in \mathcal{P}$, the corresponding state $ s^{k}_{t} $ is projected into three vectors: key $ \mathbf{K}^{k}_{t} $, query $ \mathbf{Q}^{k}_{t} $, and value $ \mathbf{V}^{k}_{t} $. The attention scores between the query $ \mathbf{Q}^{k}_{t} $ of the current body part $k$ and the keys $ \mathbf{K}^{k'}_{t} $ from all body parts $ k' \in \mathcal{P}$ are computed by calculating the scaled dot-product between the query and each key,  and then passed through a softmax function to obtain the attention weights:
\begin{equation}
    \alpha_{t}^{k,k'} = \text{softmax}\left( \frac{\mathbf{Q}^{k}_{t} \cdot \mathbf{K}^{k'}_{t}}{\sqrt{d}} \right)
\end{equation}
where $ d $ is the dimension of the query and key vectors. The attention weights $ \alpha_{t}^{k,k'} $ indicate the relative importance of the states of different body parts when computing the skill embedding for body part $ k $.
The skill embedding $ z_{t}^{k} $ for each body part is then obtained by computing a weighted sum of the value vectors $ \mathbf{V}^{k'}_{t} $ from all body parts, with the attention weights serving as the coefficients:
\begin{equation}
    z_{t}^{k} = \sum_{k'} \alpha_{t}^{k,k'} \mathbf{V}^{k'}_{t}
\end{equation}
$ z_{t}^{k} $ is normalized with respect to $\|z_{t}^{k}\|$ to lie on the unit sphere. This normalization ensures that the skill embeddings are constrained within a consistent space, allowing for more stable learning \cite{2022-TOG-ASE}. By sharing information via this attention mechanism, each body part can focus on different aspects of the overall state for modularized skill learning.

\begin{table*}[h]
    \centering
    \caption{Full-body motion imitation results on AMASS train and test.}
    \begin{tabular}{@{}l|ccccc|ccccc@{}}
        \toprule
        & \multicolumn{5}{c|}{AMASS-Train} & \multicolumn{5}{c}{AMASS-Test} \\ 
        \midrule
        Method & Succ ↑ & $E_{\text{g-mpjpe}}$ ↓ & $E_{\text{mpjpe}}$ ↓ & $E_{\text{acc}}$ ↓ & $E_{\text{vel}}$ ↓ & Succ ↑ & $E_{\text{g-mpjpe}} $ ↓ & $E_{\text{mpjpe}}$↓ & $E_{\text{acc}}$ ↓ & $E_{\text{vel}}$ ↓ \\ 
        \midrule
        UHC & 97.0\% & 36.4 & 25.1 & 4.4 & 5.9 & 96.4\% & 50.0 & 31.2 & 9.7 & 12.1 \\ 
        PHC & 98.9\% & 37.5 & 26.9 & 3.3 & 4.9 & 97.1\% & 47.5 & 40.0 & 6.8 & 9.1 \\ 
        PULSE & 99.8\% & 39.2 & 35.0 & 3.1 & 5.2 & 97.1\% & 54.1 & 43.5 & 7.0 & 10.3 \\ 
        PHC+ & \textbf{100\%} & 26.1 & 21.1 & 2.6 & 3.9 & 99.2\% & 36.1 & 24.1 & 6.2 & 8.1 \\ 
        \name (Ours) & 99.6\% & \textbf{25.5} & \textbf{20.4} & \textbf{2.1} & \textbf{3.4} & \textbf{99.3\%} & \textbf{32.2} & \textbf{22.7} & \textbf{4.4} & \textbf{6.3} \\ 
        \bottomrule
    \end{tabular}
    \label{tab:track}
\end{table*}
\begin{table*}[h]
    \centering
    \caption{VR-tracking result on AMASS train and test.}
    \begin{tabular}{@{}l|ccccc|ccccc@{}}
        \toprule
        & \multicolumn{5}{c|}{AMASS-Train} & \multicolumn{5}{c}{AMASS-Test} \\ 
        \midrule
        Method & Succ ↑ & $E_{\text{g-mpjpe}}$ ↓ & $E_{\text{mpjpe}}$ ↓ & $E_{\text{acc}}$ ↓ & $E_{\text{vel}}$ ↓ & Succ ↑ & $E_{\text{g-mpjpe}}$ ↓ & $E_{\text{mpjpe}}$ ↓ & $E_{\text{acc}}$ ↓ & $E_{\text{vel}}$ ↓ \\ 
        \midrule
        ASE & 18.6\% & 128.7 & 87.9 & 40.9 & 33.3 & 8.0\% & 114.3 & 99.2 & 57.7 & 44.0 \\ 
        ASE-PMP & 7.2\% & 159.7 & 155.7 & 142.2 & 123.0 & 1.5\% & 161.7 & 126.1 & 151.1 & 96.4 \\ 
        PULSE & \textbf{99.5\%} & 57.8 & 51.0 & 3.9 & 7.1 & \textbf{93.4\%} & 88.6 & 67.1 & 9.1 & 14.9 \\ 
        \name (Ours) & 99.3\% & \textbf{52.9} & \textbf{47.9} &  \textbf{3.7} & \textbf{6.4} & \textbf{93.4\%} & \textbf{83.2} & \textbf{65.7} & \textbf{8.8} & \textbf{13.4} \\ 
        \bottomrule
    \end{tabular}
    \label{tab:vr}
\end{table*}
\subsection{Modular Low-Level Controllers}
\label{sec:controller}

To further enhance the flexibility and effectiveness of our policy network, we implement modular low-level controllers that operate alongside the skill modularization attention mechanism. For each body part $k$, we designate a low-level controller $\pi^{k} = \mathcal{N}(\mu(z_{t}^{k}), \sigma)$, which models a Gaussian distribution with fixed diagonal covariance. The skill embeddings, $z_t^{k}$, generated by the attention mechanism serve as the input for these controllers, which process the information to produce targeted actions $a_t^{k}$ for the actuated rigid bodies corresponding to body part $k$. The produced actions for each controller are concatenated to form the full-body PD target, denoted as $a_{t}$, for controlling the simulated agent. Using the same setup in \cite{10.1145/3450626.3459670}, we incorporate a body-level discriminator $D(s^{p}_{t-10:t})$ that computes a real/fake value based on the current body-level proprioception of the humanoid. This style signal encourages the formulation of natural body-level motion from modular part-level skills. By decoupling the action prediction into specialized modules, we enhance the flexibility of the network, enabling effective imitation for a wide range of motions.

\subsection{Active Skill Learning with Generative Adaptive Sampling}
\label{sec:generative}

Active skill learning enables efficient policy training by directing the learning process toward the most informative regions of the skill space. Previous methods \cite{Luo2023PerpetualHC, luo2024universal} often use adaptive sampling to prioritize motion sequences within the same fixed training set based on their failure probability for better motion imitation, which may lead to overfitting to challenging sequences. In contrast, we propose a generative adaptive sampling strategy that synthesizes $N$ motion sequences for each failed sample. These synthetic sequences are generated using an off-the-shelf text-to-motion diffusion model \cite{tevet2023human}, conditioned on the paired HumanML3D text descriptions \cite{Guo_2022_CVPR} of the failed motion. To better facilitate modularized skill learning, we not only generate new full-body samples but also leverage the controllability of the motion diffusion model to tailor new motion samples for each body part. In this process, we fix the reference motion for other body parts and allow the diffusion model to synthesize new motion samples for the target body part given the text condition (See Fig. \ref{fig:gensample}). Integrating these synthetic samples into the training process strengthens the policy network by providing a more balanced and comprehensive dataset. 

\subsection{Skill Transfer for Downstream Tasks}
\label{sec:transfer}

After $\pi_{\text{\name}}$ converges, a high-level policy $\pi_{\text{Task}}(z_{t}^{k_{1}}, ..., z_{t}^{k_{K}} | s_{t}^{p}, s_{t}^{g})$ can be trained to apply the learned modular skills to downstream tasks, where $s_{t}^{p}$ and $s_{t}^{g}$ represent the proprioceptive state and task-specific goal signal, respectively, and $z_{t}^{k_{1}}, ..., z_{t}^{k_{K}}$ indicate the corresponding spherical skill embeddings for each body part $k_{1}, ..., k_{K} \in \mathcal{P}$. Similar to the low-level controllers, each high-level task policy is modeled as a Gaussian distribution with a fixed diagonal covariance: $\mathcal{N}(\mu_{\text{Task}}(s_{t}^{p}, s_{t}^{g}), \sigma_{\text{Task}})$. When training the high-level policy, we freeze the low-level controllers to preserve the learned modular motor skills. In this work, we demonstrate the effectiveness and reusability of our modular part-level skills on a set of generative motion tasks. Details regarding the setup of each task are provided in the supplementary material.