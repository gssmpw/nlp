\clearpage
\setcounter{page}{1}
\setcounter{section}{0}
\maketitlesupplementary

% \section{Introduction}
% \label{sec:introduction}
\noindent This supplementary document provides additional implementation details (Sec.~\ref{sec:details}), an analysis of joint-level tracking errors comparing our method with PHC+ \cite{luo2024universal} (Sec.~\ref{sec:error}), task specifications along with training curves for downstream tasks (Sec.~\ref{sec:gen}), and visualizations of the modular skill embedding space (Sec.~\ref{sec:viz}). Extensive qualitative results are available in the supplementary video, where we demonstrate our method's capability to imitate various reference motion data and perform different downstream tasks.
% and achieve realistic skill transitions \ZY{do we need to say skill transition?}.
% can remove

\section{Implementation Details}
\label{sec:details}

In \name, we divide the 24 rigid bodies of the simulated character into five body parts, $\mathcal{P} := $ \{Left Leg (L-Leg), Right Leg (R-Leg), Left Arm (L-Arm), Right Arm (R-Arm), Torso\}. The corresponding grouping of rigid bodies is detailed in Tab. \ref{tab:group} 

\begin{table}[h]
    \centering
    \small
    \caption{Body part grouping for skill modularization.}
    \begin{tabular}{@{}l|l@{}}
        \toprule
        Body Part & Rigid Bodies \\ 
        \midrule
        L-Leg & L-Hip, L-Knee, L-Ankle, L-Toe\\
        R-Leg & R-Hip, R-Knee, R-Ankle, R-Toe\\
        Torso & Pelvis, Torso, Spine, Chest, Neck, Head\\
        L-Arm & L-Thorax, L-Shoulder, L-Elbow, L-Wrist, L-Hand\\
        R-Arm & R-Thorax, R-Shoulder, R-Elbow, R-Wrist, R-Hand\\
        \bottomrule
    \end{tabular}
    \label{tab:group}
\end{table}

\noindent Based on the body part grouping, we partition the input state accordingly. The Skill Modularization Attention Layer uses two-layer MLPs with dimensions [256, 64] to project the partitioned input states for each body part into keys, queries, and values, enabling attention across body parts. The resulting skill embeddings, with a dimensionality of 64, are then normalized to lie on the unit sphere. For each body part, we assign a low-level controller, implemented as a four-layer MLP with dimensions [2048, 1536, 1024, 512], which takes the corresponding skill embedding as input and produces PD targets for the actuated rigid bodies within that body part grouping. The policy network is trained for approximately $2 \times 10^9$ steps, with a learning rate of $2 \times 10^{-5}$. Generative adaptive sampling is applied every $2 \times 10^8$ steps. For each failed sequence, we utilize the corresponding text-label from HumanML3D \cite{Guo_2022_CVPR} to generate $N=3$ synthetic sequences using an off-the-shelf text-to-motion model \cite{tevet2023human}.\\

\noindent Note that \name is a skill-learning framework designed to be flexible with arbitrary body-part configurations. We adopted a 5-part setting that allows for clear separation between limbs and is consistent with prior work on part-based modeling \cite{Jang_2022}. In Tab. \ref{tab:bodypart}, we show additional motion tracking results on AMASS-Test for two-part (Part 1: L-Arm, R-Arm, Torso; Part 2: L-Leg, R-Leg) and three-part (Part 1: L-Arm, R-Arm; Part 2: Torso; Part 3: L-Leg, R-Leg) configurations. We also detail a comparison of the corresponding model sizes for different body part configurations using \name against
PHC+ \cite{luo2024universal}. Despite the smaller model size compared to PHC+, our five-part modularization approach leads to more precise motion tracking and enables the formulation of reusable, modular skills without the need for progressive mining. Additionally, for the two-part and three-part configurations, our framework demonstrates smaller model sizes than the full-body baseline while achieving competitive performance, highlighting the advantages of skill modularization.
\begin{table*}[h]
    \centering  
    \caption{Motion Tracking Evaluation and Policy Network Model Size for Different Body-Part Configurations.}
    \begin{tabular}{@{}l|ccccc|c@{}}
        \toprule
        Method & Succ ↑ & $E_{\text{g-mpjpe}}$ ↓ & $E_{\text{mpjpe}}$ ↓ & $E_{\text{acc}}$ ↓ & $E_{\text{vel}}$ ↓ & Model Size (MB)\\
        \midrule
        PHC+ & 99.2\%& 36.1 & 24.1 & 6.2 & 8.1 & $\sim$609\\
        \midrule
        2-Part & 97.1\% & 36.4 & 25.9 & 5.0 & 7.2 & $\sim$243\\
        3-Part & 98.6\% & 36.1 & 25.3 & 4.8 & 6.9 & $\sim$306 \\ 
        \name (Ours) & \textbf{99.3\%} & \textbf{32.2} & \textbf{22.7} & \textbf{4.4} & \textbf{6.3} & $\sim$429\\
        \bottomrule
    \end{tabular}
    \label{tab:bodypart}
\end{table*}

% \noindent In Tab. \ref{tab:size}, we detail a comparison of model size between \name and PHC+,

% \begin{table}[h]
%     \centering
%     \caption{Policy network model size.}
%     \begin{tabular}{@{}l|c@{}}
%         \toprule
%         Method & Model Size (MB) \\ 
%         \midrule
%         PHC+ & $\sim 609$\\
%         \name & $\sim 429$\\
%         \bottomrule
%     \end{tabular}
%     \label{tab:size}
% \end{table}

\begin{table}[h]
    \centering
    \footnotesize
    \caption{\textbf{R-Arm:} Joint-wise Mean Position Errors (mm) on AMASS-Test.}
    \begin{tabular}{@{}l|ccccc@{}}
        \toprule
        Method & R-Thorax & R-Shoulder & R-Elbow & R-Wrist & R-Hand \\ 
        \midrule
        PHC+ & 42.8 & 41.4 & 37.0 & 40.1 & 39.0\\
        \name & \textbf{20.6} & \textbf{21.2} & \textbf{21.8} & \textbf{29.8} & \textbf{30.5}\\
        \bottomrule
    \end{tabular}
    \label{tab:right_upper_limb}
\end{table}

\begin{table}[h]
    \centering
    \footnotesize
    \caption{\textbf{L-Arm:} Joint-wise Mean Position Errors (mm) on AMASS-Test.}
    \begin{tabular}{@{}l|ccccc@{}}
        \toprule
        Method & L-Thorax & L-Shoulder & L-Elbow & L-Wrist & L-Hand \\ 
        \midrule
        PHC+ & 45.0 & 45.5 & 43.3 & 40.3 & 41.7\\
        \name & \textbf{20.6} & \textbf{21.4} & \textbf{27.0} & \textbf{29.3} & \textbf{31.5}\\
        \bottomrule
    \end{tabular}
    \label{tab:left_upper_limb}
\end{table} 
\section{Joint-wise Tracking Error}
\label{sec:error}
Full-body tracking requires high precision. We present comprehensive joint-wise tracking error statistics across the five body parts defined in our skill modularization framework: right arm (\cref{tab:right_upper_limb}), left arm (\cref{tab:left_upper_limb}), right leg (\cref{tab:right_lower_limb}), left leg (\cref{tab:left_lower_limb}), and torso (\cref{tab:core_and_pelvis}). These detailed analyses highlight the effectiveness of skill modularization in achieving precise, high-fidelity physical character skill learning, demonstrating its potential for fine-grained motion control across distinct body parts.\\

\noindent Additionally, we observe higher errors at the end-effector joints (e.g., R-Toe, L-Toe, R-Hand, L-Hand) for both \name and PHC+, suggesting that more complex motion samples or refined modeling of these joints may be needed. We also note increased errors in the right leg joints for our method compared to other body parts, which may indicate a bias or imbalance in the training data. However, the modular nature of our framework offers key advantages: we can not only isolate part-specific controllers during training but also leverage active learning to generate additional synthesized samples, helping to support skill learning for specific body parts.\\

\begin{table}[h]
    \centering
    \footnotesize
    \caption{\textbf{L-Leg:} Joint-wise Mean Position Errors (mm) on AMASS-Test.}
    \begin{tabular}{@{}l|cccc@{}}
        \toprule
        Method & L-Hip & L-Knee & L-Ankle & L-Toe \\ 
        \midrule
        PHC+ & 38.7 & 43.4 & 52.5 & 55.7\\
        \name & \textbf{20.1} & \textbf{26.3} & \textbf{31.7} & \textbf{33.9}\\
        \bottomrule
    \end{tabular}
    \label{tab:left_lower_limb}
\end{table}

\begin{table}[h]
    \centering
    \footnotesize
    \caption{\textbf{R-Leg:} Joint-wise Mean Position Errors (mm) on AMASS-Test.}
    \begin{tabular}{@{}l|cccc@{}}
        \toprule
        Method & R-Hip & R-Knee & R-Ankle & R-Toe \\ 
        \midrule
        PHC+ & 36.4 & 41.4 & 50.2 & 50.9\\
        \name & \textbf{21.7} & \textbf{31.6} & \textbf{38.9} & \textbf{47.9}\\
        \bottomrule
    \end{tabular}
    \label{tab:right_lower_limb}
\end{table}

\begin{table}[h]
    \centering
    \footnotesize
    \caption{\textbf{Torso:} Joint-wise Mean Position Errors (mm) on AMASS-Test.}
    \begin{tabular}{@{}l|cccccc@{}}
        \toprule
        Method & Pelvis & Torso & Spine & Chest & Neck & Head \\ 
        \midrule
        PHC+ & 36.9 & 39.1 & 43.2 & 43.8 & 44.2 & 48.5\\
        \name & \textbf{19.7} & \textbf{21.0} & \textbf{22.0} & \textbf{21.6} & \textbf{20.0} & \textbf{20.5}\\
        \bottomrule
    \end{tabular}
    \label{tab:core_and_pelvis}
\end{table}

\noindent Following prior work \cite{Luo2023PerpetualHC}, we use early termination during training and evaluation, where tracking terminated (i.e. considered a failure) if the average deviation of simulated joint positions from the reference exceeds 0.5 meters. However, this threshold is relatively lenient, as both the use of average deviation and the large 0.5-meter margin may overlook significant joint-wise errors that aren't fully captured by the current evaluation metrics. In Fig. \ref{fig:terminate}, we report the full-body tracking success rate on AMASS-Test for termination distances ranging from 0.5 meters to 0.2 meters, with a step size of 0.05 meters. Notably, we observe an immediate decline in performance for PHC+ as the termination distance decreases, whereas our method maintains a higher success rate even under stricter termination conditions. This suggests that our approach is more robust, able to generalize effectively, and consistently preserves higher accuracy across a wider range of unseen sequences.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/termination.png}
    \vspace{-0.5cm}
    \caption{Effect of decreasing termination distance on full-body motion tracking success rate for AMASS-Test.}
    \label{fig:terminate}
\end{figure}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/transfer.png}
    \caption{Normalized returns during training for downstream tasks.}
    \label{fig:transfer}
\end{figure*}
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/tsne.pdf}
    \caption{Modular Skill Embedding t-SNE Visualization.}
    \label{fig:tsne}
\end{figure*}
\section{Downstream Tasks}
\label{sec:gen}
\noindent For generative downstream tasks, steer, reach, strike, and VR-tracking, we utilize a three-layer MLP with dimensions [2048, 1024, 512] for the high-level policy and train the policy using PPO \cite{schulman2017proximalpolicyoptimizationalgorithms} for approximately $2 \times 10^9$ steps with a learning rate of $2 \times 10^{-5}$. VR-tracking follows the reward function for the full-body tracking tasks. Please refer to our supplementary video for a comprehensive evaluation of our model on these tasks. Following \cite{2022-TOG-ASE, luo2024universal}, the goal state and reward formulations for steer, reach, and strike are detailed below:\\ 

\paragraph{Steering.} The goal state is defined as \( s_{\text{steer}}^t := (d_t, v_t) \), where \( d_t \) and \( v_t \)  represent the target direction and the desired linear velocity at timestep \( t \), respectively. The objective of the policy is to control the character to travel along the specified direction at the desired velocity. The reward is defined as \( r_{\text{speed}} = |v_t - v_{t0}| \), where \( v_{t}^{0} \) is the root velocity of the humanoid.

\paragraph{Reach.} In the reach task, we aim to minimize the distance between the simulated character's right hand and a desired 3D target point, \( c_t \), randomly sampled from a 2-meter box centered at \( (0, 0, 1) \). The goal state is \( s_{\text{reach}}^t \equiv (c_t) \). Let \( p_{\text{R-Hand}} \) be the position of the simulated character's right hand. The reward for reaching is calculated as the exponential of the negative squared distance between the right-hand position and the desired target point: 
\[
r_{\text{reach}} = \exp\left(-5 \| p_{\text{R-Hand}} - c_t \|_2^2 \right),
\]

\paragraph{Strike.} The objective of this task is to knock over a target object. We select the rigid bodies, R-Hand, R-Wrist, R-Elbow as the target body parts for contact, where the task terminates (e.g. considered a failure) if any body part other than the target body parts makes contact with the target object. The goal state \( s_{\text{strike}}^t \equiv (x_t, \dot{x}_t) \) consists of the position and orientation \( x_t \), linear and angular velocities \( \dot{x}_t \) of the target object in the simulated character's frame of reference. The reward is defined as \( r_{\text{strike}} = 1 - \mathbf{u}_{\text{up}} \cdot \mathbf{u}_t \), where \( \mathbf{u}_{\text{up}} \) is the global up vector, and \( \mathbf{u}_t \) is the up vector of the target object.\\

\noindent As shown in Fig. \ref{fig:transfer}, we provide the training curves for the steering, reach, and strike tasks. We observe that our method initially exhibits slower progress than SOTA models, especially for tasks that utilize full-body template movements, such as speed and strike, due to the added complexity of modular skill spaces. However, the learning curve accelerates quickly, ultimately catching up to SOTA models with a similar convergence speed and scale of normalized returns. For more precise tasks, like reaching, that require targeted control of specific body parts, our method demonstrates faster learning, indicating better flexibility for character control. 

\section{Modular Skill Embedding Space}
\label{sec:viz}
In Fig. \ref{fig:tsne}, we present the t-SNE visualization of body-part skill embeddings for motions from AMASS-Test. We uniformly sample skill embeddings for each body part every 10 frames, resulting in $\sim$3000 samples. For clarity, we label a subset of samples for five different types of motions. We observe consistent structures for the same motion across body parts, with symmetric structures emerging for motions with alternating patterns, e.g., walking and jumping.

% \begin{table}
% \begin{tabular}{@{}l|c|c|c@{}}
%         \toprule
%         Method & Ground Truth & PHC+ & \name (Ours)\\
%         \midrule
%         Diversity $\rightarrow$ & 2.70 & 2.40 &  \textbf{2.54}  \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:diversity}
% \end{table}
% We evaluate the Diversity metric [Guo et al., 2022] for full-body tracking on AMASS-Test. A value closer to the ground truth indicates higher diversity. Our diversity aligns with better tracking accuracy on unseen motions and enhanced skill transfer.
% Here, we evaluated the Diversity [Guo et al.,2022], used in motion generation tasks, on motions generated for the full-body tracking task. The closer the metric is to the ground truth, the greater the diversity, which is also reflected in improved tracking accuracy on unseen motions and enhanced adaptability to downstream tasks.