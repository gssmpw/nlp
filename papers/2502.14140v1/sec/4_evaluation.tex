\section{Evaluation}
\label{sec:eval}
\textbf{Experiment Settings.} \textit{Motion Tracking Task:} We evaluate \name's performance on the full-body motion tracking task, comparing it against state-of-the-art motion trackers UHC \cite{Luo2021DynamicsRegulatedKP}, PHC \cite{Luo2023PerpetualHC}, PHC+ and PULSE \cite{luo2024universal}. \\

\noindent \textit{Motion Skill Embedding Task:} We also highlight the reusability of our modular skill embeddings by applying \name to generative tasks such as reaching, steering, striking, and VR tracking, and comparing its performance with reusable skill representations: PULSE \cite{luo2024universal} and ASE \cite{2022-TOG-ASE}. Following \cite{luo2024universal}, we adapt ASE to produce per-frame latent skill embeddings for a fair comparison. Notably, while the original ASE uses a body-level adversarial motion prior, we will also compare the impact of partwise adversarial motion priors \cite{10.1145/3588432.3591487} on learning skill embeddings and high-level tasks (ASE-PMP). We adopt the same body part partition as our framework to formulate ASE-PMP.\\

\noindent \textbf{Datasets.} For training and testing the full-body and VR tracking policies, we utilize the cleaned AMASS training set and test set, respectively \cite{Luo2023PerpetualHC}. For the strike and reach tasks, we sample initial states from the AMASS training set. For speed tasks, we follow \cite{Wang2024PacerPlus} to use a subset of AMASS of only locomotion for initial state sampling.\\

\noindent \textbf{Metrics.} For motion imitation and VR controller tracking, we report the global end-effector mean per-joint position error ($E_{\text{g-mpjpe}}$) and root-relative end-effector mean per-joint position error ($E_{\text{mpjpe}}$) in millimeters. We also compare physics-based metrics, including acceleration error ($E_{\text{acc}}$) in mm/frame$^{2}$ and velocity error ($E_{\text{vel}}$) in mm/frame. Following prior work \cite{Luo2023PerpetualHC}, we define the success rate (Succ) as the percentage of time the average per-joint error remains within 0.5 meters of the reference motion. For VR tracking, the success rate is based on only three body points (Head, Left Hand, Right Hand). For generative tasks (reach, steer, strike), we compare the undiscounted return normalized by the maximum possible reward per episode. \\

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]
    {figures/qual.png}
    \caption{Our modular skill embeddings are flexible and informative, achieving natural human-like behavior in downstream tasks.}
    \label{fig:generative_qual}
\end{figure*}

\begin{table*}[h]
    \centering
    \caption{\textbf{Ablation on components of \name.} We evaluate on AMASS-Test for the full-body motion tracking task to demonstrate the effectiveness of each component for motor skill learning. Modularization: whether to use low-level controllers for each body part, Attention: whether to use a skill modularization attention layer. Generative: whether to use generative adaptive sampling. }
    \begin{tabular}{@{}cccc|cccccccc@{}}
        \toprule
        Index & Modularization & Attention & Generative &  Succ ↑ & \textbf{$E_{\text{g-mpjpe}}$ ↓} & \textbf{$E_{\text{mpjpe}}$ ↓} & \textbf{$E_{\text{acc}}$ ↓} & \textbf{$E_{\text{vel}}$ ↓} \\ 
        \midrule
        1 & $\times$ & $\times$ & $\times$  & 96.4\% & 41.1 & 28.4 & 5.4 & 7.2 \\ 
        2 & $\checkmark$ & $\times$ & $\times$  & 98.6\% & 35.9 & 23.8 & 4.4 & 6.5 \\ 
        3 & $\checkmark$ & $\checkmark$ &  $\times$ & 99.3\% & 32.4 & 23.2 & 4.5 & \textbf{6.3} \\ 
        \midrule
        4 & $\checkmark$ & $\checkmark$ & $\checkmark$ & \textbf{99.3\%} & \textbf{32.2} & \textbf{22.7} & \textbf{4.4} & \textbf{6.3} \\ 
        \bottomrule
    \end{tabular}
    \label{tab:ablation}
\end{table*}

\noindent \textbf{Implementation Details.} All physics simulations are conducted using Isaac Gym \cite{Makoviychuk2021IsaacGH}. Our policy network is trained with 3072 parallel environments on a single NVIDIA A6000 GPU for approximately one week, totaling around 2e9 steps. For PULSE, we use the original model settings. For \name and the ASE baselines, all low-level controller networks are four-layer perceptrons (MLPs) with dimensions [2048, 1536, 1024, 512]. Discriminators and encoders for adversarial skill learning are two-layer MLPs with dimensions [1024, 512]. Each high-level policy for downstream tasks is a three-layer MLP with dimensions [2048, 1024, 512]. The latent dimension of the skill embeddings is set to 64. Detailed hyperparameter settings are provided in the supplementary material. The controllers operate at 30 Hz, while the simulation runs at 60 Hz. 

\subsection{Motion Tracking}

Table \ref{tab:track} and Table \ref{tab:vr} show the performance of our method on the AMASS train and test sets for the full-body motion tracking task and VR tracking task, respectively. For both tracking tasks, our modular policy network outperforms baselines, reducing tracking errors on both training and test motion sequences. Our experiments show that by modularizing skills, a single network can achieve better accuracy and generalization capabilities. The results support the hypothesis that modularization of motor skills can effectively capture a wide range of human motion.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/interpolate.png}
    \caption{\textbf{Skill Interpolation}: Interpolating left-hand skill embeddings from "crossing arms" to "waving" shows a smooth transition in motion (top row) and linear velocity expressed in the local coordinate frame (bottom row) for the left-hand of the simulated character.}
    \label{fig:interpolate}
\end{figure*}

\subsection{Skill Embedding Downstream Tasks}
As shown in Fig. \ref{fig:generative_qual}, the modular skills learned by our framework can be effectively applied to downstream tasks. In Table \ref{tab:comparison}, we record the normalized return for the downstream tasks, steering, reach and strike, with 0 being the minimum possible return value, and 1 being the maximum. Compared to ASE baselines, our method achieves a more expressive skill space, resulting in superior normalized returns. When compared to the state-of-the-art model PULSE, our approach achieves comparable performance in terms of normalized return. Unlike PULSE, our policy network does not require additional distillation from a motion tracking policy to obtain effective skill embeddings. Instead, the learned modular skills can be directly applied to a variety of downstream tasks while preserving accurate motion imitation capabilities. In contrast, PULSE suffers from a significant decrease in motion tracking accuracy compared to the motion tracking policy, PHC+, used for distillation (see Tab.~\ref{tab:track}). For tasks that rely on full-body template movements, such as speed and strike, our method initially exhibits slower progress due to the more complex search space of modular skills, which requires not only skill search but also learning feasible compositions of modular skills. ASE-PMP shows similar trends, where part-level reward signals lead to a more complex skill learning process. Additionally, when PMP is applied to a single large-scale dataset rather than multiple small-scale specialized datasets of task-specific scenarios, this can lead to less effective skill learning and transfer. For more precise tasks, like reaching, which targets specific body parts, our method demonstrates faster learning, indicating better flexibility. We include training curves for downstream tasks in the supplementary material.

\begin{table}[h]
    \centering
    \caption{Normalized returns for downstream tasks: steering, reach, and strike. Values in parentheses indicate standard deviation.}
    \begin{tabular}{@{}l|ccc@{}}
    \toprule
    & Steering & Reach & Strike \\ \midrule
    ASE & 0.60 (0.001) & 0.10 (0.003) & 0.12 (0.006) \\
    ASE-PMP & 0.31 (0.002) & 0.06 (0.002) & 0.13 (0.004) \\
    PULSE & 0.92 (0.002) & 0.77 (0.002) & \textbf{0.88 (0.002)} \\
    \name & \textbf{0.93 (0.001)} & \textbf{0.79 (0.002)} & \textbf{0.88 (0.003)} \\ 
    \bottomrule
    \end{tabular}
    \label{tab:comparison}
\end{table}

\subsection{Skill Interpolation and Composition}

We demonstrate the structure of modular skill embeddings through interpolation. As shown in Fig. \ref{fig:interpolate}, we begin with the sequence of skill embeddings that controls the simulated character to cross both arms in front of its chest, and then gradually interpolate the left-hand skill embeddings towards those corresponding to a left-hand waving motion, while keeping the sequence of skill embeddings for all other body parts unchanged. The resulting motion exhibits a smooth transition, and further analysis of the left hand's linear velocity in the local coordinate frame over simulated time steps confirms this smoothness. As shown in Fig \ref{fig:teaser}, modular skill embeddings enable us to compose skills for different body parts, generating coordinated full-body motion, such as the Usain Bolt trademark pose. Due to this modularity, our framework also allows for flexible integration of new modular components. For example, as shown in  Fig. \ref{fig:hand}, we can combine articulated hand controllers with our pre-trained body part controllers to control a humanoid with dexterous hands to imitate a motion sequence without retraining the entire framework from scratch.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/hand.png}
    \caption{\name enables flexible integration of additional modular controllers without needing to retrain from scratch. By combining articulated hand controllers (highlighted in yellow) with our pre-trained body part controllers, we can control a humanoid with dexterous hands. Red spheres represent target joint locations.}
    \label{fig:hand}
\end{figure}

\subsection{Ablations}

We ablate the effectiveness of each component within our framework with respect to the full-body motion tracking task. In Table \ref{tab:ablation}, we present motion tracking results on the AMASS test set with respect to the following configurations: with/without modular controllers for individual body parts, with/without a skill modularization attention layer to extract modular skill embeddings, and with/without generative adaptive sampling. We observe that incorporating all components yields the best performance. Notably, even without the attention layer, our model outperforms PHC+ in motion tracking on unseen motion sequences, highlighting the effectiveness of modular motor skills for individual body parts. The addition of the attention layer not only improves performance but also enables the formulation of reusable modular motor skills for applications to downstream tasks. Furthermore, generative adaptive sampling enhances policy performance on unseen motions.