%!TEX options=-shell-escape

% VLDB template version of 2020-08-03 enhances the ACM template, version 1.7.0:
% https://www.acm.org/publications/proceedings-template
% The ACM Latex guide provides further information about the ACM template
\documentclass[sigconf, nonacm]{acmart}




\newif\ifArxiv
 \Arxivtrue
% \Arxivfalse



\newif\ifRelease
 \Releasetrue


%% The following content must be adapted for the final version
% paper-specific


\newcommand\vldbdoi{XX.XX/XXX.XX}
\newcommand\vldbpages{XXX-XXX}
% issue-specific
\newcommand\vldbvolume{14}
\newcommand\vldbissue{1}
\newcommand\vldbyear{2020}
% should be fine as it is
\newcommand\vldbauthors{\authors}
\newcommand\vldbtitle{\shorttitle} 
% leave empty if no availability url should be set
\newcommand\vldbavailabilityurl{https://github.com/dbai-tuw/yannakakis-rewriting}
% whether page numbers should be shown or not, use 'plain' for review versions, 'empty' for camera ready
\newcommand\vldbpagestyle{plain} 


\usepackage{tikz}
\usepackage{multirow}
\usepackage[frozencache]{minted}
\usepackage{forest}
\usepackage{cleveref}
\usepackage{balance}
\usepackage{booktabs}
\usepackage{xspace}
\usepackage{pgfplots, pgfplotstable}
\pgfplotsset{compat=1.17}   % Set the pgfplots version if needed
\usepgfplotslibrary{groupplots,statistics}
\usepackage{mathtools}
\ifRelease
\usepackage[disable]{todonotes}
\else
\usepackage{todonotes}
\fi
\usepackage[scaled=0.8]{FiraMono}
\usepackage{bold-extra}


\definecolor{OliveGreen}{rgb}{0,0.6,0}


\newcommand{\ourTool}{SMASH\xspace}

\newcommand{\rp}[1]{\todo[color=lime, inline]{rp: #1}}

\newcommand{\co}[1]{\todo[color=orange, inline]{co: #1}}




%%



\ifRelease

\newcommand{\change}[1]{ {#1}\xspace}
\else

\newcommand{\change}[1]{ {\color{red}#1}\xspace}
\fi

\newcommand{\nop}[1]{}
\newcommand{\Att}{\mathit{Att}}



% %% \BibTeX command to typeset BibTeX logo in the docs
% \AtBeginDocument{%
%   \providecommand\BibTeX{{%
%     Bib\TeX}}}

% %% Rights management information.  This information is sent to you
% %% when you complete the rights form.  These commands have SAMPLE
% %% values in them; it is your responsibility as an author to replace
% %% the commands and values with those provided to you when you
% %% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
% %% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
% %%
% %%  Uncomment \acmBooktitle if the title of the proceedings is different
% %%  from ``Proceedings of ...''!
% %%
% %%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
% %%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/2018/06}




%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
% \title{Query Evaluation with Yannakakis' Algorithm: Is it always better?%
\title{Selective Use of Yannakakis’ Algorithm to Improve Query Performance: Machine Learning to the Rescue
%\titlenote{This is an enhanced version of a manuscript \cite{DBLP:journals/corr/abs-2303-02723}
%that has never been formally published.}
}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


    \author{Daniela Böhm}
    \orcid{0009-0009-8707-2958}
    \author{Matthias Lanzinger}
    \orcid{0000-0002-7601-3727}
    \author{Reinhard Pichler}
    \orcid{0000-0002-1760-122X}
    \author{Alexander Selzer}
     \orcid{0000-0002-6867-5448}
    \affiliation{%
      \institution{TU Wien}
      \city{Vienna}
      \country{Austria}
    }
    
%   \email{firstname.lastname@tuwien.ac.at}


    \author{Georg Gottlob}
    \orcid{0000-0002-2353-5230}
    \author{Davide Mario Longo}
    \orcid{0000-0003-4018-4994}
    \affiliation{%
      \institution{University of Calabria}
      \city{Rende}
      \country{Italy}
    }
%   \email{??@cs.ox.ac.uk}
    

    \author{Cem Okulmus}
    \orcid{0000-0002-7742-0439}
    \affiliation{%
      \institution{University Paderborn}
      \city{Paderborn}
      \country{Germany}
    }
%   \email{okulmus@cs.umu.se}
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{B\"{o}hm et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
 Query optimization has played a central role in database research for decades. However,
 more often than not, the proposed optimization techniques lead to a performance improvement
 in some, but not in all, situations. Therefore, we urgently need a methodology for 
 designing a decision procedure that  decides for a given query whether the optimization technique should be applied or not. 
 
 In this  work, we propose such a methodology with a 
 focus on Yannakakis-style query evaluation as our optimization technique of interest.
 More specifically, we formulate this decision problem as an algorithm selection problem and
 we present a Machine Learning based approach for its solution. Empirical results with several benchmarks on a variety of database systems show that 
 our approach indeed leads to a statistically significant performance improvement.

 \todo[color=cyan, inline]{Alex: Ich bin mir nicht sicher ob wir hier "statistically significant" sagen sollten. Das bedeutet ja einfach, dass die Verbesserungen das absolute Minimum erreichen um signifikant zu sein}
 \end{abstract}

% %%
% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% %% Please copy and paste the code instead of the example below.
% %%
% \begin{CCSXML}
% <ccs2012>
%    <concept>
%        <concept_id>10002951.10002952.10003190.10003192.10003210</concept_id>
%        <concept_desc>Information systems~Query optimization</concept_desc>
%        <concept_significance>500</concept_significance>
%        </concept>
%    <concept>
%        <concept_id>10010147.10010257</concept_id>
%        <concept_desc>Computing methodologies~Machine learning</concept_desc>
%        <concept_significance>300</concept_significance>
%        </concept>
%  </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Information systems~Query optimization}
% \ccsdesc[300]{Computing methodologies~Machine learning}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
% \keywords{query optimization, machine learning, structual decomposition}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle



\ifArxiv
%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\pagestyle{\vldbpagestyle}
\begingroup\small\noindent\raggedright\textbf{Reference Format:}\\
\vldbauthors. \vldbtitle. \\
\phantom{\href{https://doi.org/\vldbdoi}{doi:\vldbdoi}}
\endgroup
\begingroup
\renewcommand\thefootnote{}\footnote{\noindent
This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit \url{https://creativecommons.org/licenses/by-nc-nd/4.0/} to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing the authors. \hfill  
\phantom{Copyright is held by the author(s). Publication rights licensed to the VLDB Endowment. }\\
\phantom{\raggedright Proceedings of the VLDB Endowment, Vol. \vldbvolume, No. \vldbissue\ %
ISSN 2150-8097. }\\
\phantom{\href{https://doi.org/\vldbdoi}{doi:\vldbdoi}} \\
}\addtocounter{footnote}{-1}\endgroup

%%% VLDB block end %%%

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\ifdefempty{\vldbavailabilityurl}{}{
\vspace{.3cm}
\begingroup\small\noindent\raggedright\textbf{Artifact Availability:}\\
The source code, data, and/or other artifacts have been made available at \url{\vldbavailabilityurl}.
\endgroup
}

\else
%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\pagestyle{\vldbpagestyle}
\begingroup\small\noindent\raggedright\textbf{PVLDB Reference Format:}\\
\vldbauthors. \vldbtitle. PVLDB, \vldbvolume(\vldbissue): \vldbpages, \vldbyear.\\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi}
\endgroup
\begingroup
\renewcommand\thefootnote{}\footnote{\noindent
This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit \url{https://creativecommons.org/licenses/by-nc-nd/4.0/} to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing \href{mailto:info@vldb.org}{info@vldb.org}. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. \\
\raggedright Proceedings of the VLDB Endowment, Vol. \vldbvolume, No. \vldbissue\ %
ISSN 2150-8097. \\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi} \\
}\addtocounter{footnote}{-1}\endgroup
%%% VLDB block end %%%

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\ifdefempty{\vldbavailabilityurl}{}{
\vspace{.3cm}
\begingroup\small\noindent\raggedright\textbf{PVLDB Artifact Availability:}\\
The source code, data, and/or other artifacts have been made available at \url{\vldbavailabilityurl}.
\endgroup
}
%%% VLDB block end %%%
\fi


% \rp{Ein paar generelle Entscheidungen zu Schreibweisen (die uneinheitlich waren): \\
% ML- vs. ML: z.B. ML-task vs. ML task; ML-model vs. ML model: immer ohne ``-''.\
% runtime vs. run time: immer runtime\\
% dataset vs. data set: immer dataset \\
% labelling $\rightarrow$ labeling, labelled $\rightarrow$ labeled\\
% Br. vs. Am. Englisch: immer Am., dh. insbesondere: -ize und -ization statt -ise und -isation
% (PS: habe gerade gelernt das ``supervised'' auch in Am Engl. die korrekte Schreibweise ist)\\
% Spell check nicht vergessen; da werden einmem eh die Br.Engl. Woerter angezeigt.}

\section{Introduction}
\label{sect:Introduction}


 Query optimization has played a central role in database research for decades. 
Some generally accepted techniques 
such as replacing Cartesian product plus selection by a join or projecting out attributes not
needed further up in the query plan as soon as possible are guaranteed to (almost) always lead to 
a performance improvement. However, more often than not, optimization techniques proposed in the literature 
lead to a performance improvement in {\em some}, but not in  {\em all}, situations.
Moreover, it is usually a non-trivial task to delineate the situations where the application 
of a specific optimization is advantageous and where it is not. This also applies to 
optimization techniques which, in theory, should (almost) always outperform the 
conventional query evaluation methods. 

A prominent example of an optimization technique is 
Yannakakis' algorithm~\cite{DBLP:conf/vldb/Yannakakis81}
in case the query is acyclic. 
Several applications and extensions of this algorithm 
in recent time (see, e.g.,~\cite{DBLP:conf/sigmod/IdrisUV17,%
DBLP:journals/vldb/IdrisUVVL20,DBLP:journals/corr/abs-2301-04003,%
DBLP:journals/corr/abs-2406-17076,%
DBLP:journals/pvldb/BirlerKN24,%
DBLP:journals/pacmmod/0005023,% 
DBLP:conf/sigmod/0001022})
witness the renewed interest in this approach.
%
The key idea of this algorithm is to first eliminate all dangling tuples (= tuples that will not 
contribute to the final result of the query) via semi-joins and then compute the join with the guarantee that 
all intermediate results thus obtained will be extended to tuples in the final result. Hence, in a sense, 
Yannakakis' algorithm solves the (in general NP-complete) 
join ordering problem that aims at avoiding the 
explosion of intermediate results. In theory, such a strategy of {\em completely avoiding} the generation of 
useless intermediate results should always be superior to conventional techniques that just try to 
compute the joins in an optimal order and thus aim at {\em minimizing} useless intermediate results. 
However, in practice, it turns out that Yannakakis' algorithm leads to a performance improvement in 
{\em some} cases but, by no means, in {\em all} cases.


In this work, we revisit a sub-class of acyclic queries called 0MA (= zero materialization aggregate) 
queries~\cite{DBLP:journals/corr/abs-2303-02723}, 
where the theoretical advantage of Yannankakis-style query evaluation is even more extreme: 
0MA-queries are a restricted class of join queries with an aggregate on top and which can be 
evaluated by carrying out only semi-joins, i.e., completely avoiding the need for computing any joins. 
More precisely, after traversing the join tree $T$ of such a query by semi-joins in bottom-up direction, 
the result can be computed from the relation resulting at the root node of $T$.%
\footnote{Formal definitions of acyclic queries, 0MA queries, join trees, and Yannakakis' algorithm are given in 
Section~\ref{sect:Preliminaries}.}
Note that the query given in Figure~\ref{fig:tpch-query}, which is a slightly modified version of a query from the STATS benchmark
(we have only replaced \texttt{COUNT}(*) in the \texttt{SELECT}-clause by a \texttt{MIN}-expression), falls into this class.
Indeed, if we consider a join tree of this query with the \texttt{comments} relation at the root node, then
we can evaluate this \texttt{MIN}-expression after the bottom-up traversal with semi-joins by only considering the 
resulting relation at the root node.


\begin{figure}[t]
% \small
    \centering
    \begin{minted}[escapeinside=||,fontsize=\small]{sql}
SELECT MIN(c.Id) FROM comments as c, posts as p,
  votes as v, users as u
WHERE u.Id = p.OwnerUserId AND u.Id = c.UserId
  AND u.Id = v.UserId
AND c.CreationDate>=CAST('2010-07-27 12:03:40' AS TIMESTAMP)
AND p.Score>=0 AND p.Score<=28 AND p.ViewCount>=0
AND p.ViewCount<=6517 AND p.AnswerCount>=0 AND p.AnswerCount<=5
AND |\colorbox{orange}{p.FavoriteCount>=0}| AND p.FavoriteCount<=8
AND p.CreationDate>=CAST('2010-07-27 11:29:20' AS TIMESTAMP)
AND p.CreationDate<=CAST('2014-09-13 02:50:15' AS TIMESTAMP)
AND u.Views>=0
AND u.CreationDate>=CAST('2010-07-27 09:38:05' AS TIMESTAMP)
    \end{minted}
    \caption{Slightly modified query 121-097 from the STATS benchmark: original runtime on PostgreSQL (3.38s)
    vs.\ 
    Yannakakis-style evaluation (0.11s).
    After changing the filter condition to \colorbox{orange}{\texttt{p.FavoriteCount>=8}}: 
    original runtime on PostgreSQL (0.05s) vs.\ Yannakakis-style evaluation (0.09s).
    }
    \label{fig:tpch-query}
    \vspace{-6mm}
\end{figure}


In theory, it is ``clear'' that such a join-less evaluation must always outperform conventional query 
evaluation techniques that first fully evaluate the underlying join query and only 
then apply the aggregate
as a kind of post-processing. Alas, empirical evaluation 
on queries from several benchmarks shows that, in practice,  this is not necessarily the case.
Actually, for the query in  Figure~\ref{fig:tpch-query}, Yannankakis-style evaluation was significantly faster (by a factor of ca.\ 30) than the original evaluation method of PostgreSQL. However, when we modified one of the conditions in the WHERE clause
from $\geq 0$ to $\geq 8$ (which, together with the $\leq 8$ condition,
yields $= 8$), then suddenly the 
original evaluation method of PostgreSQL is faster. 

Hence, in order to reap the fruits of optimizations such as Yanna\-ka\-kis-style query evaluation, 
we urgently need a method that allows us to systematically analyze a given 
query optimization technique and design a decision procedure that decides when to apply 
the optimization technique and when to stick to the original evaluation method provided by the 
database management system (DBMS). Therefore, our goal in this work is to 
develop a methodology for the design of such a decision procedure, with a focus
on Yannakakis-style query evaluation as our optimization technique of interest.
More specifically, on one hand, we consider general acyclic queries 
(which join all relations after the semi-join-based elimination of dangling tuples)
and, on the other hand, we consider 0MA queries (which only require a bottom-up traversal of the 
join tree with semi-joins). 
\nop{lieber den folgenden Satz unter ``contributions verwenden:
However, 
we believe that the methodology developed here is applicable to a much wider range of query optimization techniques 
that may lead to a performance improvement in some but not in all cases.
}

To cover a diverse set of database technologies, 
we will study three quite different DBMSs, namely
PostgreSQL~\cite{DBLP:journals/cacm/StonebrakerK91} (as a ``classical'' row-oriented relational DBMS), 
DuckDB~\cite{DBLP:conf/sigmod/RaasveldtM19} (as
a column-oriented, embedded database), and 
Spark SQL~\cite{DBLP:journals/cacm/ZahariaXWDADMRV16} 
(as a database engine specifically designed for distributed data processing in a cluster).  
This raises the question how to test an optimization strategy for different DBMSs with reasonable effort. 
We therefore adapt previous query rewriting approaches 
on SQL-level (see, e.g.,~~\cite{DBLP:journals/corr/abs-2303-02723,DBLP:conf/amw/GottlobLLOPS23,%
lanzinger2024soft,DBLP:journals/pvldb/Zhou0WLSZ23}).
That is, we  take a SQL-query as input and rewrite it to an 
equivalent {\em sequence of} SQL-statements as output.
This sequence of SQL-statements forces (or, at least, guides) 
the DBMS towards Yannakakis-style query evaluation.

We treat the design of a decision procedure between different 
query evaluation techniques as an {\em algorithm selection problem},
that we want to solve by applying Machine Learning (ML) techniques. 
In recent years, many works have used ML techniques to solve database 
problems -- above all cardinality estimation and join order optimization 
(see, e.g., the survey paper~\cite{DBLP:journals/tkde/ZhouCLS22}. 
We will elaborate more on related work in general, and on ML techniques applied to database problems in particular,
in Section \ref{sect:RelatedWork}). 
%
\change{Our focus, however, is different: we are interested in the 
overall runtime of query evaluation under different evaluation methods.
We ultimately aim to improve end-to-end runtimes over entire workloads, 
rather than local optimizations of some specific subtask of query evaluation.}
% However, 
% we have a different focus here in that we are interested in the 
% overall runtime of query evaluation under different evaluation methods
% (and, ultimately, in end-to-end runtimes over entire workloads)
% rather than local optimizations of some specific subtask of query evaluation.
Moreover, problems targeted by previous applications of 
ML techniques such as cardinality estimation and join order optimization are
largely irrelevant for Yannakakis-style query evaluation.

\nop{***********************
\rp{Reicht/Passt das als Abgrenzung zu anderen ML methoden in DB?   
bzgl. Literatur zu AI in DB habe ich nur das AI4DB survey paper zitiert.}
\co{das ai4DB ist eigentlich kein survey, sondern ein tutorial (4 Seiten). Eine ähnliche gruppe an autoren hat dann später ein survey daraus gemacht, das wird im related works zitiert (21 seiten). Ich hab jetzt mal die Zitate ausgetauscht. Wenn man noch mehr surveys findet, umso besser. }
***********************}

In the first place, we have to fix the specifics of  {\em our} algorithm selection problem. 
This includes the selection of queries and data from benchmarks as well as the identification of relevant 
features that characterize these queries. \change{The input to this ML task, is formed by a collection of 
feature vectors (characterizing the selected queries and databases) 
together with runtimes obtained on our target DBMSs  
for the original query evaluation method and for the query optimization method we wish to test. } % one form the input to the ML task.

In order to solve this ML task, several decisions have to be made and 
obstacles have to be overcome. For instance, benchmarks are relatively small
for training and testing ML models. Hence, data augmentation methods suitable
for our concrete algorithm selection problem are required. 
Apart from choosing suitable ML model types together 
with appropriate choices for their hyperparameters,
we have to make the basic decision how to actually define the ML task, 
e.g.: as classification (which evaluation method to choose)
or as regression (to predict the runtime difference between the two evaluation methods, which is then used as basis for the decision between the 
two query evaluation methods). We also have to define the precise criteria
for determining the ``best'' ML model, e.g.: purely quantitative criteria
(such as highest accuracy and precision in case of classification or 
mean squared error in case of regression) or qualitative criteria (which require an inspection of the misclassifications produced by each ML model
and analyzing, for instance, by how much the chosen evaluation method was 
slower than the optimal one). 
Finally, after constructing a decision method based on the ``best'' ML model, we have to evaluate the significance of the performance improvement 
obtained by our decision method compared with always choosing the original
query evaluation method of the DBMS and also compared with always applying
Yannakakis-style query evaluation, whenever it is applicable. 


\smallskip

\noindent
{\em Contributions.} The main contribution of this work is the 
development of a methodology for designing a decision procedure 
that chooses between different query evaluation techniques.
% -- with a particular focus on Yannakakis-style query evaluation. 
This involves several steps:

% \rp{Wenn wir beim Namen SMASH fuer unsere decision procedure bzw. fuer die ganze Implementierung 
% bleiben, sollten wir diesen Namen wohl irgendwo in der Introduction einfuehren. Eventuell hier als eigenes item bei den Contributions?}
% \co{Done. Ich glaube item 4 sollte als Contribution entfernt, oder gemerged werden mit etwas anderem, keine klare contribution.

% ML: +1}

\begin{itemize}   
    \item We present an approach for formulating the design 
    of a decision procedure between different query evaluation techniques 
    as an algorithm selection problem. This approach includes several steps such as 
    selection of a test dataset, data augmentation, and feature selection. 
    
    \item We then present an approach for solving this algorithm selection problem
    via ML methods. Again, this includes several steps such as determining suitable ML model types,
    preparing the dataset for training the models, and defining selection criteria for determining the
    ``best'' model. In this work, we focus on optimization techniques based on 
    Yannakakis' algorithm. However, we believe that the 
    methodology of defining and solving the algorithm selection problem
    has the potential for a much wider applicability.


    \item To prove the practical potential of our approach, we implement it in a system called \ourTool, short for \emph{\textbf{S}upervised \textbf{M}achine-learning for \textbf{A}lgorithm \textbf{S}election  \textbf{H}euristics}. SMASH already supports multiple DBMSs, including PostgreSQL, DuckDB and SparkSQL. It aims at deciding which query optimization method to use for a given query and DBMS. 
    
        

    \item We report on extensive empirical evaluation and, in particular, 
    on the outcome of statistical tests that clearly confirm the 
    significance of the improvements achieved by the decision procedure
    compared with always using the original query evaluation method of the DBMS or 
    always applying the optimization technique.
  
\end{itemize}

\noindent
{\em Structure.}
The paper is organized as follows: After recalling basic definitions and results in 
Section~\ref{sect:Preliminaries}, we give an overview of related work in 
Section~\ref{sect:RelatedWork}.
In Section~\ref{sect:DefiningAlgorithmSelectionProblem}, we formulate
our algorithm selection problem, and we present a methodology for solving this problem in
Section~\ref{sect:SolvingAlgorithmSelectionProblem}. Empirical results are presented in 
Section~\ref{sect:evaluationAlgorithmSelection}. A conclusion and outlook to future work is
given in Section~\ref{sect:conclusion}.


The source code of the implementation, information on running the benchmarks and model training, as well as the data presented here can be found in the following repository: \url{https://github.com/dbai-tuw/yannakakis-rewriting}.
%
\ifArxiv
Some details, which are omitted in the main body of the text, are provided in the 
appendix.
\else
Due to space limitations, some details had to be omitted. They are provided
in the full version of this paper, which can also be found in this repository.
% and  will be made publicly available on arxiv after the submission.
\fi

% \rp{Wir muessen vermutlich fuer alles (auch fuer das erweiterte paper mit Appendix)
% ein git-Verzeichnis angeben. Ausser wir schaffen es wider Erwarten, das paper 2-3 Tage vor der deadline fertig zu haben, so dass wir das arxiv paper rechtzeitig anlegen koennen.\\
% Ich glaube mittlerweile, dass wir uns leichter tun, wenn wir nicht versprechen, dass das full paper sofort in arxiv verfuegbar sein wird. Dann koennten wir nach und nach eine bessere Version des full papers bereit stellen (im Moment ist der Appendix nur ein Skelett.)}



\smallskip
\noindent



\section{Preliminaries}
\label{sect:Preliminaries}

\noindent
{\em Conjunctive Queries and beyond.}
The basic form of queries studied here are Conjunctive Queries (CQs),
which correspond to 
select-project-join queries in the Relational Algebra.  
It is convenient to 
consider CQs as Relational Algebra expressions of the form 
%
$Q = \pi_U (R_1 \bowtie  \dots \bowtie R_n ).$
%
Here we assume w.l.o.g., that equi-joins have been replaced by 
natural joins via appropriate renaming of attributes. Moreover, we assume that 
selections applying to a single relation have been pushed immediately in front of this relation 
and the $R_i$'s are the result of these selections. 
The projection list $U$ consists of attributes occurring in the $R_i$'s.
%
%
\nop{************************
By slight abuse of notation, 
we shall use the same symbol $R_i$ to refer also to the relational schema 
(i.e., the set of attributes) of 
a relation $R_i$. 

\rp{Wir werden sehen, ob wir diesen letzten Satz wirklich brauchen. Er war
jedenfalls im arxiv paper.}
************************}

To go beyond CQs, we will also consider the extension of Relational Algebra by the 
grouping operator $\gamma$ and aggregate expressions. 
In other words, we are interested in queries of the form 
%
\begin{equation}
\label{eq:basicQueryForm}
       Q  =   \gamma_{g_1, \dots, g_\ell, \; A_1(a_1), \dots, A_m(a_m)} 
       \big( 
%
      R_1  \bowtie \cdots \bowtie R_n \big)
\end{equation}
%
%
where $\gamma_{g_1, \dots, g_\ell, \; A_1(a_1), \dots, A_m(a_m)}$ denotes the
grouping operation for attributes $g_1, \dots, g_\ell$ and aggregate
expressions $A_1(a_1), \dots, A_m(a_m)$.
The grouping attributes $g_1, \dots, g_\ell$ are
attributes occurring in the relations $R_1, \dots, R_n$,
the functions $A_1, \dots, A_m$ are (standard SQL) aggregate functions such as 
MIN, MAX, COUNT, SUM, AVG, MEDIAN, etc.,
and 
$a_1, \dots, a_m$ are expressions formed over the attributes 
from $R_1, \dots, R_n$. Note that we have omitted the projection $\pi_U$
in Equation (\ref{eq:basicQueryForm}), since it can be taken care of by the grouping.
A simple query of the form shown in
Equation (\ref{eq:basicQueryForm}) is given in SQL-syntax in Figure~\ref{fig:tpch-query}. 






% \begin{figure}[t]
% % \small
%     \centering
%     \begin{minted}[escapeinside=||,fontsize=\small]{sql}
% SELECT MIN(c.Id) FROM comments as c, posts as p,
%   votes as v, users as u
% WHERE u.Id = p.OwnerUserId AND u.Id = c.UserId
%   AND u.Id = v.UserId
% AND c.CreationDate>=CAST('2010-07-27 12:03:40' AS TIMESTAMP)
% AND p.Score>=0 AND p.Score<=28 AND p.ViewCount>=0
% AND p.ViewCount<=6517 AND p.AnswerCount>=0 AND p.AnswerCount<=5
% AND |\colorbox{orange}{p.FavoriteCount>=8}| AND p.FavoriteCount<=8
% AND p.CreationDate>=CAST('2010-07-27 11:29:20' AS TIMESTAMP)
% AND p.CreationDate<=CAST('2014-09-13 02:50:15' AS TIMESTAMP)
% AND u.Views>=0
% AND u.CreationDate>=CAST('2010-07-27 09:38:05' AS TIMESTAMP)
%     \end{minted}
%     \caption{Augmented query 121-097-augF1 (original runtime = 0.05s, rewritten = 0.09s)}
%     \label{fig:tpch-query}
% \end{figure}


\smallskip
\noindent
{\em Acyclicity}.
Several forms of acyclicity have been studied in the
literature \cite{DBLP:journals/jacm/Fagin83,DBLP:journals/csur/Brault-Baron16}. 
Our notion of acyclicity is the so-called $\alpha$-acyclicity. 
An {\em acyclic conjunctive query} (an ACQ, for short) is a 
CQ 
$Q = \pi_U (R_1 \bowtie  \dots \bowtie R_n )$ 
that has a {\em join tree}, i.e., 
a rooted, 
labeled tree $\langle T,r,\lambda\rangle$ with root $r$, 
such that 
(1) $\lambda$ is a bijection that assigns to each node of $T$ one of the relations
    in $\{R_1,   \dots,  R_n\}$ and
(2) $\lambda$  satisfies the so-called {\em connectedness condition\/}, i.e., 
if some attribute $A$ occurs in both relations $\lambda(u_i)$ and $\lambda(u_j)$
for two nodes $u_i$ and $u_j$, then 
$A$ occurs in the  relation $\lambda(u)$ for every node $u$ along the path between $u_i$ and $u_j$.
%
Deciding if a CQ is acyclic and, in the positive case, constructing a join tree 
can be done very efficiently by the GYO-algorithm
(named after the authors 
%Graham, Yu and Özsoyoğlu 
of  
\cite{report/toronto/Gra79,DBLP:conf/compsac/YuO79}).
The join query underlying the SQL query in Figure~\ref{fig:tpch-query}
can be easily seen to 
be acyclic. A possible join tree is shown in 
Figure~\ref{fig:tpch-query-jointree}.  



\smallskip
\noindent
{\em Yannakakis' algorithm.}
In  \cite{DBLP:conf/vldb/Yannakakis81},
Yannakakis showed that ACQs can be evaluated 
in time $O( (||D|| + ||Q(D)||) \cdot ||Q||)$, i.e., 
linear w.r.t.\ the size of the input and output data and 
w.r.t.\ the size of the query. 
This bound applies to both, set and bag semantics.
Let us ignore grouping, aggregation,  and projection for a while and consider 
an ACQ $Q$ of the form  $R_1 \bowtie \dots \bowtie R_n$ with join tree 
$\langle T,r, \lambda \rangle$. 
Yannakakis' algorithm  (no matter whether 
we consider set or bag semantics) 
consists of a preparatory step followed by 
3 traversals~of~$T$: 

\begin{figure}
    \centering
    \scalebox{1}{
    \begin{forest}
    for tree={align=center}
        [{\texttt{users} % \\ $\{\text{s\_suppkey, s\_nationkey, s\_acctbal}\}$
        }            
            [{\texttt{posts} % \\ $\{\text{s\_nationkey, n\_regionkey}\}$
            }]
            [{\texttt{comments} % \\ $\{\text{p\_partkey, s\_suppkey}\}$
            }]
            [{\texttt{votes} % \\ $\{\text{p\_partkey, s\_suppkey}\}$
            }]
        ]
    \end{forest}
    }
    \caption{Join tree for the query in Fig. \ref{fig:tpch-query}}
    \label{fig:tpch-query-jointree}
    \vspace{-4mm}
\end{figure}



In the {\em preparatory step} 
% (also referred to as {\em Setup stage} below), 
we associate with each node $u$ in the join tree $T$ the relation 
$\lambda(u)$. If the CQ originally contained selection conditions on attributes of 
relation $\lambda(u)$, then we can now apply this selection. 
The 3 traversals of $T$ consist of 
(1) a bottom-up traversal of semi-joins, (2) a top-down traversal of semi-joins,
and (3) a bottom-up traversal of joins. 
Formally, let $u$ be a node in $T$ with  
child nodes $u_1, \dots, u_k$  of $u$ and let 
relations $R$, $R_{i_1}, \dots, R_{i_k}$ be associated with 
the nodes  $u$, $u_1, \dots, u_k$ at some stage of the computation. 
In the 3 traversals (1), (2), and (3), respectively, 
they are modified as follows:

\smallskip

(1) $R = (((R  \ltimes R_{i_1}) \ltimes R_{i_2}) \dots)  \ltimes  R_{i_k}$,

(2) $R_{i_j} = R_{i_j}  \ltimes R$ for every $j \in \{1, \dots, k \}$, and 

(3) $R = (((R  \bowtie R_{i_1}) \bowtie R_{i_2}) \dots)  \bowtie  R_{i_k}$ 

\smallskip

\noindent
The result 
 of the query is the  final relation associated with 
the root node $r$ of $T$.
%
Grouping and the evaluation of aggregates can be 
carried out as post-processing {\em after} the evaluation of the join query.
In contrast, projection $\pi_U$ can be integrated {\em into}
this algorithm by projecting out in the second bottom-up traversal all attributes that neither occur
in $U$ nor further up in $T$. Of course, attributes neither occurring in $U$ nor in any join condition can 
already be projected out as part of the 
preparatory  step.

% \rp{Falls uns (wider Erwarten) der Platz ausgeht, koennte man das Korrektheitsargument fuer Yannakakis' algorithm weglassen. Im VLDB paper haben wir das auch weggelassen.}
%
The correctness of Yannakakis' algorithm is seen by 
a closer look at the relations resulting from each 
traversal of $T$. For a node 
$u$ of $T$, let $R$ denote the original relation associated with $u$, i.e., 
$\lambda(u) = R$, and let 
$R_{i_1}, \dots, R_{i_\ell}$ denote the relations
labeling the nodes in the subtree $T_u$ of $T$ rooted at $u$.
Moreover, let $R'$ denote the relation resulting from each traversal of the join tree. 
We again
write (1), (2), (3) to denote the 3 traversals of the join tree.  
Then it holds: 

\smallskip

after (1), we have  $R' = \pi_{\Att(u)} (R_{i_1} \bowtie  \dots \bowtie R_{i_\ell} )$,

after (2), we have  $R' = \pi_{\Att(u)} (R_1 \bowtie  \dots \bowtie R_n )$,

after (3), we have $R' = \pi_{\Att(T_u)} (R_1 \bowtie  \dots \bowtie R_n )$.

\smallskip\noindent
Here, we write $\Att(u)$ and $\Att(T_u)$ to denote the attributes of the 
relation $\lambda(u)$ and the attributes  occurring in any of the relations 
$R_{i_1}, \dots, R_{i_\ell}$
labeling a node in $T_u$, respectively.


\smallskip
\noindent
{\em 0MA queries.}
In \cite{DBLP:journals/corr/abs-2303-02723}, the  class 
of {\em 0MA} (short for {\em ``zero-materialization answerable''}) {\em queries}
was introduced. 
A query of the form given in Equation (\ref{eq:basicQueryForm}) is 0MA if it satisfies the 
following conditions: 

\begin{itemize}
    \item 
{\em Guardedness}, meaning that there exists a relation $R_i$ that 
contains all grouping attributes $g_1, \dots, g_\ell$
and all attributes occurring in the aggregate expressions $A_1(a_1), \dots, A_m(a_m)$. Then $R_i$ is called the {\em guard} of the query.
If several relations satisfy this property, we arbitrarily choose one guard.
%
\item 
{\em Set-safety}: we call an aggregate function
{\em set-safe}, if it is invariant under duplicate elimination, i.e., 
its value over any set $S$ of tuples remains unchanged
if duplicates are eliminated from $S$. A query satisfies the 
set-safety condition, if all its aggregate functions $A_1 \dots, A_m$
are set-safe.
\end{itemize}

The root of the join tree 
can be arbitrarily chosen. For a 0MA query, we choose the node labeled by the guard
as the root node.
Hence, if all aggregate functions are set-safe (i.e., multiplicities do not matter), 
then we can apply the grouping and 
aggregation $\gamma[g_1, \dots, g_\ell, \; A_1(a_1), \dots, A_m(a_m)]$ to the relation at the root node
right after the first bottom-up traversal.
%
In SQL, in particular, the \texttt{MIN} and \texttt{MAX} aggregates 
are inherently set-safe.
 Moreover, an aggregate
becomes set-safe when combined with the \texttt{DISTINCT} keyword. For instance,  
\texttt{COUNT}~\texttt{DISTINCT} is 
% clearly 
a set-safe aggregate function.

An example of a 0MA query is given in 
Figure~\ref{fig:tpch-query}:
it is trivially guarded (i.e., there is no grouping and the only aggregate expression is over a single attribute) 
and set-safe (since the only aggregate function occurring 
in this query is \texttt{MIN}).



\section{Related Work}
\label{sect:RelatedWork}

\noindent
{\em Acyclic queries and Yannakakis-style query evaluation.} Yannakakis' algorithm~\cite{DBLP:conf/vldb/Yannakakis81} has recently received renewed attention for the optimization of hard join queries. Recent work has focused on bringing its advantages into DBMSs from the outside via SQL query rewriting~\cite{DBLP:journals/corr/abs-2303-02723,DBLP:conf/amw/GottlobLLOPS23, lanzinger2024soft}, and similar methods such as generating Scala code expressing Yannakakis' algorithm as Spark RDD-operations~\cite{DBLP:conf/sigmod/Dai0023}. 
%
%
Even more recently, Yannakakis-like approaches have been proposed, which aim to reduce the overhead of the full Yannakakis' algorithm for enumeration, with its 3 traversals, by instead propagating up additional data and computing the whole query in only one traversal. Such approaches have been integrated into Spark SQL~\cite{DBLP:journals/corr/abs-2406-17076}, Umbra~\cite{DBLP:journals/pvldb/BirlerKN24}, DuckDB~\cite{grossadaptive}, and Apache DataFusion~\cite{DBLP:journals/corr/abs-2411-04042}.
Further research extends Yannakakis' algorithm to non-equi-join queries, such as differences of CQs~\cite{DBLP:journals/pacmmod/0005023}, acyclic queries with comparisons 
% spanning several 
% relations
~\cite{DBLP:conf/sigmod/0001022}, and queries with theta-joins~\cite{DBLP:journals/vldb/IdrisUVVL20}.

% \rp{Falls uns (wider Erwarten) der Platz ausgeht, koennte man den Absatz zu Decompositions weglassen. 
% Genau genommen hat dieses paper nichts mit decompositions zu tun.}
%

\smallskip
\noindent
{\em Decompositions.} In order to go beyond acyclic queries, a major area of research seeks to extend Yannakakis-style query answering to "almost-acyclic" queries via various notions of decompositions and their associated width measures, such as hypertree-width, soft hypertree-width, generalized hypertree-width, and fractional hypertree-width~\cite{DBLP:journals/jcss/GottlobLS02,lanzinger2024soft, DBLP:journals/ejc/AdlerGG07,2014grohemarx}. Several implementations~\cite{DBLP:journals/tods/AbergerLTNOR17,DBLP:conf/sigmod/Dai0023,%
DBLP:conf/sigmod/PerelmanR15,DBLP:conf/sigmod/TuR15} combine Yannakakis-style query execution with worst-case optimal joins~\cite{DBLP:journals/jacm/NgoPRR18}. To address the problem of minimal-width decompositions not necessarily being cost-optimal, approaches of integrating statistics about the data into the search for the best decomposition have been proposed and implemented~\cite{lanzinger2024soft, DBLP:conf/pods/ScarcelloGL04}.

\smallskip
\noindent
{\em Query rewriting.} Optimizing queries before they enter the DBMS is a different strategy towards query optimization that has been successfully applied in standard DBMSs~\cite{DBLP:journals/pvldb/ZhouLCF21, DBLP:journals/pvldb/Zhou0WLSZ23}. Although DBMSs already perform optimizations on the execution of the query, it has been shown that rewriting the query itself can still be highly effective.
The WeTune~\cite{DBLP:conf/sigmod/WangZYDHDT0022}  system goes even further, and can be used to automatically discover rewrite rules
% , instead of requiring them to be specified manually, 
but comes with the disadvantage of extremely long runtimes. 

\smallskip
\noindent
{\em Machine learning for databases.} There has been growing interest in the application of machine learning techniques to increase the performance of database systems, as can be seen by a recent survey on this broad area~\cite{DBLP:journals/tkde/ZhouCLS22}. We proceed to give a very brief overview of the general topics 
as to how machine learning has been adapted for database research. For a more detailed account on the rich interaction between machine learning and databases, we refer to~\cite{DBLP:journals/tkde/ZhouCLS22}.
In this survey, the authors categorize the different efforts of using machine learning for core database tasks into the following groups. The first group is ``learning-based data configuration''. These are works that aim to utilize machine learning for knob tuning, and view advisor and index advisor tasks~\cite{DBLP:journals/asc/DokerogluBC15,DBLP:conf/sigmod/ZhangLZLXCXWCLR19,DBLP:conf/cloud/ZhuLGBMLSY17,DBLP:conf/sigmod/AkenPGZ17,DBLP:journals/pvldb/LiZLG19,DBLP:journals/pvldb/TanZLCZZQSCZ19,DBLP:conf/cidr/HerodotouLLBDCB11,DBLP:conf/IEEEcloud/NguyenKW18,DBLP:conf/vldb/ChaudhuriN97,DBLP:conf/icde/SchnaitterAMP07}. 
The next group is ``learning-based data optimization''. These works aim to tackle important, computationally intractable problems such as join-order selection and cardinality estimation of joins~\cite{DBLP:conf/sigir/MizzaroMRU18,DBLP:conf/sigmod/ParkZM20, DBLP:journals/corr/abs-1905-06425, DBLP:journals/corr/abs-1901-08544, DBLP:journals/pvldb/DuttWNKNC19, DBLP:conf/cidr/KipfKRLBK19, DBLP:journals/pvldb/MarcusNMZAKPT19, DBLP:journals/pvldb/MarcusP19,DBLP:conf/sigmod/HeimelKM15, DBLP:conf/vldb/StillgerLMK01, DBLP:journals/tods/TrummerWWMMJAR21, tzoumas2008reinforcement}.
Another group is ``learning-based design for databases''. These works aim more specifically at exploring the use of machine-learning in the construction of various data structures used by modern databases, such as indexes, hashmaps, bloom filters and so on~\cite{ DBLP:conf/sigmod/KraskaBCDP18, DBLP:conf/sigmod/DingMYWDLZCGKLK20,DBLP:conf/sigmod/WuYTSB19, DBLP:conf/sigmod/MaAHMPG18}.
A further group listed in the survey is ``learning-based data monitoring''. As the name suggests, these works aim to use machine learning to create systems that automate the task of running a database and detecting  and reacting to anomalies~\cite{DBLP:journals/pvldb/MaYZWZJHLLQLCP20,DBLP:conf/sigmod/TaftESLASMA18,DBLP:journals/corr/abs-1910-10777,DBLP:journals/pvldb/MarcusP19,DBLP:journals/pvldb/ZhouSLF20}.
Lastly the survey mentions ``learning-based database security''. This category is on how to use machine learning based methods to help with critical problems, such as confidentiality, data integrity and availability~\cite{DBLP:conf/kdd/BhaskarLST10,DBLP:conf/icde/ColomboF16a,DBLP:conf/iml/Lodeiro-Santiago17,DBLP:journals/ijcwt/Sheykhkanloo17,DBLP:journals/corr/abs-1901-02868}. 

% Efforts to improve query optimizers have mostly focused on the traditional hard problems query optimizers face: cardinality estimation~\cite{DBLP:conf/sigmod/ParkZM20, DBLP:journals/pvldb/DuttWNKNC19, DBLP:conf/cidr/KipfKRLBK19} and join-order selection~\cite{DBLP:journals/corr/abs-1808-03196,DBLP:conf/sigmod/MarcusP18}.

\smallskip
\noindent
{\em Query performance prediction.} Predicting the performance of a query -- usually the runtime, or sometimes the resource requirements -- is related to the problem of deciding whether to rewrite a query.
% in that the performance of the whole query execution has to be considered. 
Runtime prediction has been performed by constructing cost models based on statistical information of the data \cite{DBLP:journals/is/HeO06}, on SQL queries~\cite{DBLP:conf/icde/WuCZTHN13}, and XML queries~\cite{DBLP:journals/pvldb/ZhouSLF20}. Further approaches use machine learning and deep learning to predict the runtimes of single queries~\cite{DBLP:conf/vldb/ZhangHJLZ05,DBLP:journals/pvldb/MarcusP19,DBLP:conf/sigir/ZhouC07} or concurrent queries (workload performance prediction)~\cite{DBLP:conf/sigmod/DugganCPU11, DBLP:conf/icde/AkdereCRUZ12}.



% \begin{itemize}
%     \item Related work als eigene section?
%     \item Cyclic queries\cite{lanzinger2024soft}
%     \item ML and DBs
%     \begin{itemize}
%         \item Query Performance Prediction for Concurrent Queries using Graph Embedding\cite{DBLP:journals/pvldb/ZhouSLF20}
%         \item weitere papers zu query performance prediction von der DB community:  
%         \cite{DBLP:journals/is/HeO06}, \cite{DBLP:conf/icde/AkdereCRUZ12};
%         \item weitere papers zu query performance prediction von der IR community:  
%         \cite{DBLP:conf/sigir/ZhouC07}, \cite{DBLP:conf/sigir/MizzaroMRU18}
%         \item AI Meets Database: AI4DB and DB4AI \cite{DBLP:conf/sigmod/0001ZC21}
%         \item Database meets artificial intelligence: A survey \cite{DBLP:journals/tkde/ZhouCLS22}
%         \item openGauss: An Autonomous Database System\cite{DBLP:journals/pvldb/0001ZSYHJLWL21}
%     \end{itemize}
%     \item Query rewriting
%     \begin{itemize}
%         \item A Learned Query Rewrite System using Monte Carlo Tree Search\cite{DBLP:journals/pvldb/ZhouLCF21}
%         \item A Learned Query Rewrite System\cite{DBLP:journals/pvldb/Zhou0WLSZ23}

%         \item WeTune: Automatic Discovery and Verification of Query Rewrite Rules\cite{DBLP:conf/sigmod/WangZYDHDT0022}
%     \end{itemize}
% \end{itemize}


% \rp{Folgende papers handeln auch von query performance prediction: 
% \cite{}. 
% Kann man die noch irgendwie in der Related Work section unterbringen? Generell ist der vorige Absatz
% etwas kurz, wenn man bedenkt, dass diese Arbeiten vermutlich am nächsten zu unserem paper liegen.
% }

\nop{************************
\rp{Genereller Eindruck: einerseits behandeln wir decompositions, die nur sehr entfernt mit diesem paper zusammenhaengen. 
Andererseits ist ML for DB ein riesen Thema und wir behandeln es ungefaehr gleich lang (bzw. kurz) wie decompositions.
}
\co{Hab jetzt den paragraph zu ML wesentlich detailierter, mit wesentlich mehr zitaten.}
\rp{wow - sieht jetzt wirklich impressive aus!}
************************}


\begin{figure*}[t] 
  \centering 
    \includegraphics[width=\textwidth]{graphics/results/methodology.pdf} 
  \caption{Methodology workflow.}  
  \label{fig:methodologyWorkflow}  
\end{figure*} 

\section{Formulating the Algorithm Selection Problem}

% \section{Defining the Algorithm Selection Problem}
\label{sect:DefiningAlgorithmSelectionProblem}

Whenever a new, optimized method for query evaluation is presented, we 
need a decision program that decides 
whether the new method should be applied or not. We are thus faced with an algorithm selection 
problem, where we have to decide, for every database instance and query, which query evaluation
method should be applied.  In this section, we describe the steps needed to 
actually formulate the precise algorithm selection problem at hand. 
An overview of this workflow is given in Figure~\ref{fig:methodologyWorkflow}. 

% \begin{figure}[t] 
%   \centering 
%     \includegraphics[width=0.5\textwidth]{graphics/results/methodology.PNG} 
%   \caption{Methodology workflow.}  
%   \label{fig:methodologyWorkflow}  
% \end{figure} 



% \rp{Eventuell die Graphik erweitern: selection of DBMSs und feature selection
% werden in Section 4 auch noch als eigene Subsection behandelt. Kann man diese 2 Schritte 
% irgendwie in der Grafik unterbringen?}

It consists of the following steps:
We first have to (1) {\em choose suitable benchmarks} and select those queries to which the optimization technique is applicable.
However, benchmarks are rather small compared with the usual datasets used for training ML models. We therefore (2) {\em apply data augmentation} to increase the dataset's size.
We then need to (3) {\em select DBMSs} on which we want to test the 
effectiveness of the new optimization techniques.
Implementing these new techniques into a query engine is a non-trivial and expensive task.
Hence, to save effort and to enable testing with various DBMSs,
we make use of the specific nature of Yannakakis-style query evaluation and (4) {\em rewrite the given SQL queries} 
(the original ones from the benchmarks plus the ones resulting 
from the data augmentation step) 
into sequences of SQL commands that ``force'' the 
selected DBMSs into a particular evaluation strategy. 
To prepare for the ML task, we then have to 
 (5)     
{\em do a feature selection}. That is, we want to characterize every query 
in terms of a feature vector. 
We are then ready to (6) {\em run the experiments}, i.e., we execute all queries 
 with and without the  optimization and measure the times of each run. The result will be 
 runtimes for each query (characterized by a particular 
feature vector) both, when the optimization is applied and when it is not applied.
This is then the input to the {\em model training step} and, 
ultimately, to the development of a decision program, which will be described in Section~\ref{sect:SolvingAlgorithmSelectionProblem}.
%
% \rp{Ich habe jetzt bei Punkt (4) ergaenzt, dass das Yannakakis-spezifisch ist.}




Below, we describe steps (1) -- (6) in some detail for Yannakakis-style query evaluation of two classes of queries, namely,  {\em enumeration queries} (i.e., queries that ask for the enumeration of the result tuples for an acyclic conjunctive query) and {\em 0MA-queries} (i.e.,  
the subclass of aggregate queries with underlying ACQs~\cite{DBLP:journals/corr/abs-2303-02723}). 
Recall from Section~\ref{sect:Preliminaries}, that 0MA queries allow for a particularly simple evaluation strategy, since they
can be evaluated based on semi-joins only, i.e., without computing any (full) joins. 

From a theoretical perspective, one would 
expect the optimization by Yannakakis-style evaluation to be always effective.
This is particularly the case for the 0MA queries, where the performance gain 
achieved by avoiding any join computation should be overwhelming. 
Alas, it turns out that even in this extreme case, the optimization may our may not 
lead to a performance improvement. We thus need to develop decision programs 
for both classes of queries by first formulating this decision problem as an 
{\em algorithm selection problem} and then solving this problem by applying 
ML techniques. Below, we describe the details of the
various steps needed to set up the algorithm selection problem. 



\subsection{Benchmark Data} 
\label{sect:benchmarkData}

    We aimed to gather a diverse set of data and queries from different domains, designed for different purposes.
Thus, we have chosen several widely used benchmarks, which represent a variety of queries and 
datasets: (1) The \textit{JOB (Join Order Benchmark)}~\cite{DBLP:journals/pvldb/LeisGMBK015}, which was introduced to study the join ordering problem, is based on the real-world IMDB dataset and contains realistic join-aggregation queries with many joins and various filter conditions, (2) \textit{STATS/STATS-CEB}~\cite{DBLP:journals/pvldb/HanWWZYTZCQPQZL21} is based on the Stackexchange-dataset, and contains many join queries not following FK/PK-relationships, (3) Four different datasets (namely cit-Patents, wiki-topcats, web-Google and com-DBLP) from \textit{SNAP (Stanford Network Analysis Project)}~\cite{DBLP:journals/tist/LeskovecS16}, \change{a collection of graphs, which we combine with synthetic queries introduced in \cite{DBLP:journals/corr/abs-2406-17076}}, (4) \textit{LSQB (Large-Scale Subgraph Query Benchmark)}~\cite{DBLP:conf/sigmod/MhedhbiLKWS21}, which was designed to test graph databases as well as relational databases, consists of synthetic data and hard queries based on a social network scenario, and (5)
\textit{HETIONET}~\cite{hetionet}. The latter is less known in the database world.
It contains real-world queries on real-world data from a heterogeneous information network of biochemical data, and is part of the new CE benchmark\cite{DBLP:journals/sigmod/ChenHWSS23}, which has, for instance, been recently used 
in~\cite{DBLP:journals/pvldb/BirlerKN24} and \cite{DBLP:journals/corr/abs-2411-04042}.


\nop{*********************
STATS contains a real-world dataset together with a query workload STATS-CEB with  
a number of diverse
multi-table joins, which were specifically designed 
for testing cardinality estimation. 
The SNAP  datasets are commonly used graph datasets. We have decided to use four of them: . The JOB  was designed based on the real-world IMDB dataset to 
test how good query optimizers are. It contains queries with different numbers of joins and filter conditions to test, above all, how well the join ordering works. The LSQB  consists of nine queries based on an underlying graph and allowing to scale up to test query optimizers. The HETIONET dataset is based on a heterogeneous information network of biochemical data with the purpose of connecting multiple different sources into one real-world dataset.
*********************}



We are only interested in ACQs (possibly with group-by and aggregates). 
Most of the queries in the chosen benchmarks are CQs with additional filter conditions 
applied to single tables. These filter conditions can be taken care of by the preparatory step; so they
pose no problem. However, not all of the CQs are acyclic; so we have to eliminate the cyclic ones
from further consideration. 
The number of (acyclic) CQs  of each dataset is given in Table~\ref{tab:augment0MA}. 



% \begin{table}[h!] 
% \centering
% \begin{tabular}{|l|c|c|c|}
% 	\hline
% 	Dataset & Characteristics & \# CQs & \# ACQs\\
% 	\hline
% 	STATS & real-world, diverse joins  & 146 & 146\\
% 	\hline
% 	SNAP & graph queries & 40 & 40\\
% 	\hline
% 	JOB & real-world, FK/PK joins & 33 & 5 \\
% 	\hline
% 	LSQB & to test scalability & 9 & 2 \\
% 	\hline
% 	HETIONET & real-world, graph queries & 26 & 26 \\
% 	\hline
% \end{tabular}
% \caption{Benchmark datasets.} 
% \todo[color=cyan, inline]{Alex: Irgendwas stimmt da nicht ganz. Bei JOB sollten alle Queries azyklisch sein. Bei LSQB ist ein Teil zyklisch aber nicht so viele - der Rest sind keine reinen CQs weil sie z.B. inequalities haben - also 9 CQs stimmt auch nicht. Ich denke es wäre besser die Tabelle zu entfernen. Die characteristics kann man auch im Text beschreiben.}
% \label{bench_data_tab} 
% \end{table} 


Note that some of the  queries in the benchmarks are enumeration queries and some already contain some aggregate
(in particular, \texttt{MIN}) and satisfy the 0MA conditions. Of course, also from the enumeration queries, 
we can derive 0MA queries by putting an appropriate aggregate expression (again, in particular, 
with the \texttt{MIN} aggregate function) into the select-clause of the query. We do this by 
randomly choosing a table occurring in the query and one column of this table (mostly the first column in the table). We will see in Section~\ref{sect:DataAugmentation}. 
that it makes no significant difference which table and attribute we choose for turning a query into 0MA form, 
as we will vary the table and attribute anyway. 



\subsection{Data Augmentation}
\label{sect:DataAugmentation}

% \todo[inline]{cem: vielleicht besser ``data-centric (query) augmentation''? so wie es ist klingt es eher so als ob die daten, also die datenbank, verbessert (augmented) wird. }
% \todo[color=cyan, inline]{Alex: Ich denke "data augmentation" ist ein recht bekannter machine learning Begriff. Vielleicht könnte man "Training (Set) Data Augmentation" sagen, dann sollte es klar sein, worauf es sich bezieht}


As can be seen in \Cref{tab:augment0MA}, 
we  get 219 acyclic queries from the chosen benchmarks. 
Since we want to use these queries to train ML models, this is not enough training data. Therefore, we do data augmentation, as will be detailed next.

For our data,  we decided to use the following two steps for data augmentation: 
"filter augmentation" (for all queries) followed by "aggregate-attribute augmentation"
(for 0MA-queries) and ``enumeration augmentation'' (for enumeration queries), respectively. 

With the filter augmentation we want to get duplicates of all queries having filters (i.e., selection conditions on a single table)
and then change some filters in a way that the sizes of the resulting relations vary between these queries. If the query had only one filter we change the specific value it is equal to, greater or smaller of the filter condition. For these cases, we get twice as many queries as before. For all queries having two or more filters we choose two filters, which we change, each at a time. Here we try to replace the filters in a way that once the number of answer tuples gets bigger and once smaller. This gives us triples for each of these queries. 



\begin{table}[t] 
\caption{Overview of the 0MA and enumeration queries after augmentation. In total, we get 2936 0MA queries and 1741 enum queries. } 
\label{tab:augment0MA} 
\centering
\begin{tabular}{lcccc}
	\toprule
	\bf Dataset &\bf \# ACQs & \bf +filter & \bf +filter\&agg & \bf +filter\&enum\\
	\midrule
	STATS & 146 & 432 & 1876 & 1264\\
	SNAP & 40 & 40 & 244 & 120 \\
	JOB & 15 & 45 & 264 & 135 \\
	LSQB & 2 & 2 & 14 & 6  \\
	HETIONET & 26 & 72 & 538 & 216\\
    \midrule
    \bf Total & 219 & 591 & 2936 & 1741\\
    \bottomrule
\end{tabular}

\end{table} 



\begin{example} \label{ex:data_augment1}


Consider the STATS query `005-024', named $q$ here. To illustrate the filter augmentation, we present two possible augmentations on $q$.
% where the first one is the "original" query and the other two are the augmented ones. 
One option to augment $q$ is to swap  the filter condition, \mintinline[escapeinside=||,fontsize=\small]{sql}{v.BountyAmount>=0}  , with a transformed one, such as  \mintinline[escapeinside=||,fontsize=\small]{sql}{v.BountyAmount>=40}, producing the query $q_{\mathit{aug}1}$. Another option is to swap \mintinline[escapeinside=||,fontsize=\small]{sql}{u.DownVotes=0} with \mintinline[escapeinside=||,fontsize=\small]{sql}{u.DownVotes=10}, producing $q_{\mathit{aug}2}$.

{
\begin{minted}[escapeinside=||,fontsize=\small]{sql}
|{\color{gray}$q$:}|    SELECT MIN(u.Id) 
      FROM   votes AS v, badges AS b, users AS u 
      WHERE  u.Id = v.UserId AND v.UserId = b.UserId 
             AND v.BountyAmount>=0 AND v.BountyAmount<=50 
             AND u.DownVotes=0
          
|{\color{gray}$q_{\mathit{aug}1}$:}| SELECT MIN(u.Id) 
      FROM   votes AS v, badges AS b, users AS u 
      WHERE  u.Id = v.UserId AND v.UserId = b.UserId 
             AND |\textbf{v.BountyAmount>=40}| AND v.BountyAmount<=50 
             AND u.DownVotes=0
             
|{\color{gray}$q_{\mathit{aug}2}$:}| SELECT MIN(u.Id) 
      FROM   votes AS v, badges AS b, users AS u 
      WHERE  u.Id = v.UserId AND v.UserId = b.UserId 
             AND v.BountyAmount>=0 AND v.BountyAmount<=50 
             AND |\textbf{u.DownVotes=10}|
\end{minted}
}




\end{example}

For 0MA queries, we next apply the "aggregate-attribute augmentation" 
to vary the table from which we take the MIN-attribute. This is done in a way that 
every table occurring in the query appears once in the MIN-expression. 
The column of the chosen table does not really matter, which means we just take the first column of the table. Depending on the number of tables involved in the query this leads to a different number of new queries per query. 


\begin{example} \label{ex:data_augment2}

We give an example for the aggregate-attribute augmentation on 0MA queries. As in \Cref{ex:data_augment1}, we shall focus on the STATS query 005-024. For this query, we would thus create 3 versions by taking either \texttt{MIN(u.Id)},
\texttt{MIN(u.Id)}, or \texttt{MIN(b.Id)} in the SELECT clause.
This aggregate-attribute augmenation is applied 
to the original queries and to the filter augmented ones alike. 
Hence, the original STATS query 005-024 gives rise to 9 distinct queries after the whole augmentation process.
 \end{example}

\nop{**********************
{\small
\begin{verbatim} 
005-024: SELECT MIN(v.Id) 
         FROM votes as v, badges as b, users as u 
         WHERE u.Id = v.UserId AND v.UserId = b.UserId 
             AND v.BountyAmount>=0 AND v.BountyAmount<=50 
             AND u.DownVotes=0
005-024-augA1: SELECT MIN(b.Id) 
        FROM votes as v, badges as b, users as u 
        WHERE u.Id = v.UserId AND v.UserId = b.UserId 
             AND v.BountyAmount>=0 AND v.BountyAmount<=50 
             AND u.DownVotes=0
005-024-augA2: SELECT MIN(u.Id) 
        FROM votes as v, badges as b, users as u 
        WHERE u.Id = v.UserId AND v.UserId = b.UserId 
             AND v.BountyAmount>=0 AND v.BountyAmount<=50 
             AND u.DownVotes=0
\end{verbatim}
}
**********************}


% \begin{table}[t] 
% \caption{Overview of the enumeration queries after augmentation. In total, we get 1741 enumeration queries. } 
% \label{tab:augmentEnum} 
% \centering
% \begin{tabular}{lccc}
% 	\toprule
% 	\bf Dataset &\bf \# ACQs & \bf filter aug. &\bf filter + enum aug.\\
% 	\midrule
% 	STATS & 146 & 432 & 1264\\
%     SNAP & 40 & 40 & 120\\
% 	JOB & 5 & 45 & 135\\
% 	LSQB & 2 & 2 & 6 \\
% 	HETIONET & 26 & 72 & 216\\
%     \midrule
%     \bf Total & 219 & 591 & 1741\\
% 	\bottomrule
% \end{tabular}

% \end{table} 


In Table~\ref{tab:augment0MA}, we summarize the numbers of 0MA queries that 
we get after each step of the augmentation. The SNAP and LSQB queries do not have filter conditions, which means there is no 
filter augmentation for them. 
% Overall, we get 2936 0MA queries.

We also take the enumeration queries, for which filter augmentation has already been done,
and apply an enumeration augmentation step. To this end, we randomly choose two of the 
attributes used in join conditions and write them into the SELECT-clause of the query. This is done three times for each filter augmented query if at least three different join attributes exist in the query. Otherwise, if the query only has one join, then we have one enumeration query with two join attributes in the SELECT clause.
% This leads to 1741 enumeration queries.


In summary, after applying the data augmentation step to the 0MA and enumeration queries,
we have 4677 queries in total.
\change{
\ifArxiv In \Cref{app:dataAugmentation},
\else In the full paper,
\fi
we give further examples on the data augmentation, namely on the enumeration augmentation. 
}



\subsection{Selection of DBMSs} 
\label{sect:SelectionDBMSs}

We want to check the effectivity of the optimized evaluation following Yannankakis' algorithm on
a wide range of database technologies. We have therefore chosen three significantly different DBMSs,
namely 
(1) PostgreSQL 13.4~\cite{DBLP:journals/cacm/StonebrakerK91} as a ``classical'' row-oriented relational DBMS, 
(2) DuckDB 0.4~\cite{DBLP:conf/sigmod/RaasveldtM19} as
a column-oriented, embedded database, and 
(3) Spark SQL~3.3~\cite{DBLP:journals/cacm/ZahariaXWDADMRV16} 
% (part of Spark 3.3) 
as a database engine specifically designed for distributed data processing in a cluster.  
These DMBSs represent a broad spectrum of architectures and characteristics and they, therefore, give a good overview of the range of existing DBMSs.


\subsection{Query Rewriting}
\label{sect:QueryRewriting}


To recall our motivation, we aim to find methods that determine the effectiveness of various optimization methods -- with a focus on Yannakakis' algorithm for this work -- on various DBMSs before potentially having to commit to the significant effort of modifying existing query engines. 
Hence, we make use of ideas from recent works~\cite{DBLP:journals/corr/abs-2303-02723,DBLP:conf/amw/GottlobLLOPS23, lanzinger2024soft,DBLP:conf/sigmod/Dai0023} that present query rewritings, where a single SQL query is rewritten into an equivalent series of queries, with the aim to guide DBMSs to utilize a given optimization method. 
\change{
\ifArxiv
In \Cref{app:query_rewriting_example},
\else
In the full paper,
\fi
we illustrate the query rewriting approach that we use in this paper, using an example query from our benchmark.

}



\subsection{Feature Selection} 
\label{sect:FeatureSelection}

We choose different kinds of features that we derive from the structure of the 
query itself, from the join tree constructed in the process of rewriting the query, and 
from statistics determined by PostgreSQL or DuckDB over the database. The latter kind of features
is extracted from the query optimizer's estimates, and obtained via the EXPLAIN command. 
Note that Spark SQL does not provide an EXPLAIN command. However, we will explain below how to circumvent this short-coming.
Some features are based on a set, of variable length, containing numeric values. In order to reduce the set of values into a fixed-length list of values, we calculate, for each set, several statistics: min, the 0.25-quantile, median, the 0.75-quantile, max, und mean.  In the list of features below, we use $^*$ (e.g. \textbf{B7*}) to mark which features consist of variable-length sets, and hence will get reduced to the mentioned collection of 6 values.
% I.e., for each set of values $S$ (which we will mark with a * - e.g., (6*)), we obtain 6 features: $(q_{0}(S), q_{0.25}(S), q_{0.5}(S), q_{0.75}(S), q_{1}(S), \mu_S)$, with $q_p(S)$ being the $p$-quantile over the set $S$, and $\mu_S$ the mean over the set $S$.


\change{
\paragraph{Features derived from the query.} 
The following features are easily obtained by inspecting the query itself: \\ 
\noindent
\textbf{Feature B1}: \textit{is 0MA?}, indicates (1 or 0) if the query is 0MA or not, \\
\textbf{Feature B2}: \textit{number of relations}, \\
% (i.e., the number of all relations 
% that are part of the query), 
\textbf{Feature B3}:
\textit{number of conditions}, which refers to the number of (in)equality conditions in the WHERE clause of the query, \\
\textbf{Feature B4}: 
\textit{number of filters}, which more specifically only counts the (in)equality conditions occuring in the query, and\\
\textbf{Feature B5}: \textit{number of joins}.

\medskip

\noindent
{\em Features based on the join tree.} The following features are inspired by the work in~\cite{DBLP:journals/jair/AbseherMW17}
on tree decompositions:  \\
\textbf{Feature B6}:
\textit{depth}, which is the maximal distance between the root of the used join tree and a leaf node, \\
\textbf{Feature B7*}: \textit{container counts}, which is a set of numbers, indicating for each variable in the query the number of nodes in the join tree it occurs in. This measure indicates how many relations are joined on the same variable, and \\
\textbf{Feature B8*}:
\textit{branching degrees}, which is a set of numbers, indicating for each node the number of children it has.

\medskip

These eight features (B1)-(B8*) are the shared {\em "basic features"}.
% since we can obtain them for every query and every DBMS. 
In addition, we can use statistical information from the database and the estimates for the query evaluation, though the exact features that are exposed differs between DBMSs. In case of PostgreSQL and DuckDB, we
have the EXPLAIN command at our disposal to obtain relevant further information. 
For PostgreSQL, we thus select the following additional features, which we 
refer to as {\em "PSQL features"}:\\
\textbf{Feature P1}: \textit{estimated total cost} (of the query), \\ 
\textbf{Feature P2*}: 
\textit{estimated single table rows}, which stands for the estimated number of rows for each 
table involved in the query {\em after} the application of the filter conditions, and \\ 
\textbf{Feature P3*}:
\textit{estimated join rows}, the estimated  number of rows of each join {\em before} the application of the filter conditions. 
\noindent
% For \textbf{P3*}, we note that PostgreSQL takes into account structural information on the data such as foreign key/primary key relations in its estimates of this feature.

\medskip

\noindent
The EXPLAIN command for DuckDB behaves differently. It allows us to 
derive a single {\em "DDB feature"}: \\
\textbf{Feature D1*}:
\textit{estimated cardinalities}, the estimated number of rows after each node in the logical plan, such as  filters and joins. 


}
% \rp{Ich verstehe den letzten Satz nicht. Was ist das genau fuer eine Zahl? Nehmen wir hier semi-joins an und wir ermitteln die number of rows nach der Anwendung der Filter und den 
% semi-joins im Rahmen des bottom-up traversal? Ausserdem ist mir nicht klar, wieso hier ursprgl. von DDB features (Plural) die Rede war aber dann nur 1 feature eingefuehrt wird. Bitte klaeren.}

% \todo[color=cyan, inline]{Alex: Ich habe im Notebook nachgesehen was gemacht wird: Es werden die estimated (output) cardinalities von jeder Node im join tree (Join, Scan, filter z.B) in einer Liste gesammelt, und dann wird min, q25, median, q75, max, und mean berechnet - das sind die Features. P1 sind also 6 features. 
% Ob es die beste Lösung ist alle Node-Typen zusammenzuwerfen ... naja
% Bei den Postgres features ist unsere Beschreibung eventuell auch etwas verwirrend: P1 ist tatsächlich ein feature, während P2 und P3 Listen sind, die wir auf 6 Zahlen reduzieren um eine fixe Länge zu erreichen}


As SparkSQL does not perform cost-based optimization, it also does not have any statistical information of the data, and cannot estimate cardinalities or costs of a plan.  However, since SparkSQL also does not provide a persistent storage layer, tables are commonly imported from another database via JDBC. This implies that, in practice, the statistical features can easily be extracted from this database and used for the decision whether to rewrite the query in SparkSQL. \change{For the experiments presented in \Cref{sect:evaluationAlgorithmSelection}, we extracted these features from PostgreSQL, hence SparkSQL will have the same feature set as PostgreSQL.}

\ifArxiv
In \Cref{app:FeatureSelection},
\else
In the full paper
\fi
we illustrate the basic features as well as the additional 
features for PostgreSQL and DuckDB 
by looking at two queries from 
the HETIONET benchmark.



\subsection{Running the Queries}

\nop{******************
\begin{figure}[t] 
  \centering 
    \includegraphics[width=0.4\textwidth]{graphics/results/implementation_dbms.PNG} 
  \caption{Workflow of the query evaluation.}  
  \label{implement_dbms}  
\end{figure} 

In Figure~\ref{implement_dbms} we illustrate this part of the workflow. 
******************}
The whole evaluation is performed on a server with 128GB RAM, a 16-core AMD EPYC-Milan CPU, and a 100GB SSD disk, running Ubuntu 22.04.2 LTS. 
After a warm-up run, the original query, as well as the rewritten version, is evaluated five times, and then we take the mean of those five runtimes. 
In total we get 6 data points for each of the 4677 queries: each query is run against 3 DBMSs, where the  query evaluation happens  once with and once without optimization. % for 4677 queries. 
Aggregated information on these 
runtimes is provided in Section~\ref{sect:evaluationAlgorithmSelection}. 

\change{ In addition to the six runtimes, we also add a ``feature vector'', consisting of the features described in \Cref{sect:FeatureSelection}. These provide the 
% The 6 * 4677 data points (i.e., relating the
% obtained runtimes to the corresponding feature vector that characterizes the query plus the underlying dataset) provide the 
input to the training of ML models to be described next.
}


\section{Solving  the Algorithm Selection Problem}
\label{sect:SolvingAlgorithmSelectionProblem}

% \rp{Ich habe mit meinem bescheidenen ML Wissen versucht, in Kapitel  \ref{sect:SolvingAlgorithmSelectionProblem} einigermassen unsere Methode 
% beim Loesen des Algorithm Selection Problems zu beschreiben. \\
% Erstens 
% ist das Kapitel dadurch relativ kurz geworden. 
% $\Rightarrow$ es waere gut, da und dort 
% weitere Details zu ergaenzen.\\
% Und zweitens muss das Kapitel unbedingt jemand mit mehr ML Wissen 
% gruendlich korrektur lesen. 
% }

\nop{****************************
{\em Plan.} Ich schlage folgende Unterkapitel vor: 

\begin{enumerate}
    \item Formulating the ML tasks, i.e., classification vs. regression; 
    2 classes vs. 3 classes; ebenso bei regression ein gap zwecks 3 classifications.
\rp{Ich verstehe nicht, was wir letztlich mit den 3 Klassen (egal ob ``equal'' Klasse bei classification oder wenn man bei regression innerhalb des gaps zw. klar besser und klar schlechter faellt). Werden am Ende die ``equal'' cases einfach als ``use the original system'' interpretiert?}
    %
\item Selection of suitable ML model types and appropriate hyperparameters:
Hier werden jetzt die 7 Model types kurz vorgestellt. Die Tabelle mit den hyperparameters ist eh schon in den Appendix gewandert.

\item Splitting the data: Kurz den 80-10-10 split in training-validation-test data
vorstellen und erwaehnen, dass wir bei den ML model types, die nicht Neural Networks
sind, 10-fold cross-validation machen. 
%
\rp{Kann man zur Vorgangsweise beim splitting sagen? Mir kaeme es z.B. schon sehr
plausibel vor, wenn man versucht, die 5 Quellen der queries 
(also STATS, SNAP, JOB, \dots)
einigermassen proportional auf Training-Validation-Test aufzuteilen. 
Wird das so gemacht, dass wir den 80-10-10 split fuer die Daten aus jeder
einzelnen Quelle machen? Oder wird 80-10-10 auf alle Queries angewandt --
mit dem Risiko, dass alle JOB-Queries im Training set landen und alle LSQB-Queries im Test Set?
}

\rp{Kann man in einem Satz erklaeren, wieso cross-validation bei den 
NN-Models keinen Sinn ergibt?}

\item Selection Criteria:
ev. die confusion matrix und die metrics Acc, Prec, und Rec kurz vorstellen. 
Insbesondere erwaehnen, dass wir FP vermeiden (bzw. den Schaden moeglichst gering
halten) wollen. Genau deswegen planen wir auch eine qualitative Analyse 
der misclassifications: erzeugen wir eher FP oder FN? Wir wollen primaer 
FP vermeiden. How much better would the alternative
be  compared with the wrongly chosen one?
\rp{Anmerkung zu den Ergebnissen in der Diplomarbeit, z.B.: Table 8.10 in der Diplomarbeit zu ``Order of magnitude in seconds of the time difference of misclassifications..'':  Ich glaube, dass Prozentangaben hier wesentlich 
aussagekraefitger waeren. Aber es kann natuerlich sein, dass wir dann schlechter
dastehen, weil wir uns ev. vor allem bei schnellen queries vertun; da ergeben
dann auch 0.1 sec eine relativ grosse Prozent-Zahl.}
\end{enumerate}
****************************}

Several decisions have to be made to solve the 
algorithm selection problem that results from the steps 
described in the previous section. In particular, 
we have to (1) {\em formulate the concrete ML task} and 
then (2) {\em select ML model types} together with 
hyperparameters appropriate to our context. Before we can start training and testing the 
models, we have to (3) {\em split the data} \change{(in our case the SQL queries) }
into training/validation/test data. Finally, we have to  (4)
{\em define  selection criteria} for determining the ``best'' model, which 
will then be used as basis of our decision program between the 
original query evaluation method of each system and a Yannakakis-style evaluation.


\subsection{Formulating the ML Task}
\label{sect:FormulatingML:Task}

Our ultimate goal is the development of a decision program between two 
evaluation methods for acyclic queries. Hence, 
we are clearly dealing with a {\em classification problem} with {\em 2 possible outcomes}. 
In the sequel, we will refer to these two possible outcomes as 0 vs.\ 1 to denote
the original evaluation method of the DBMS vs.\ a Yannakakis-style evaluation (enforced
by our query rewriting). 

On the other hand, it also makes sense to consider a {\em regression problem} first and, depending 
on the predicted value, classify a query as 0 (if the predicted value suggests faster 
evaluation by the original method of the DBMS \change{within a certain threshold}) or 1 (otherwise). 
As target of the regression problem we would like to choose the difference 
$t_{\mathrm{rewritten}} - t_{\mathrm{original}}$, where we write  
$t_{\mathrm{rewritten}}$ and $t_{\mathrm{original}}$ to denote the time 
needed by Yannakakis-style evaluation and by the original evaluation method of the DBMS, respectively. 
\change{However, as will become clear in our presentation of the experimental results in Section~\ref{sect:evaluationAlgorithmSelection},
the actual runtime values are very skewed, in the sense that their distribution shows high variance. Hence, the difference we focus on is also  highly skewed. To get more reliable results, we therefore perform a log-transformation, which we provide below.
Since we have negative values, however, this cannot be applied directly. } Instead, we multiply the log of the absolute values 
with the sign they had before. Additionally, since we have a lot of values close to zero (which leads to very small log values)
we add 1 to the absolute values before applying the log, which is a common method in such situations. 
The transformation therefore results in the following formula: $x_{new} = sgn(x) * log(|x| + 1)$. \change{ In \Cref{fig:difference_comparison}, we can see this difference function under log transformation over the data from our experiments shown in \Cref{sect:evaluationAlgorithmSelection}.}


% \rp{Im Moment zeigen wir keine individuellen Laufzeiten und daher kann man dort auch den skew nicht sehen. Falls die Daten vorhanden sind und man die Verteilung der Laufzeiten in einem plot zeigen kann (zumindest fuer eine  Auswerungsmethode und zumindest fuer ein DBMS), dann brauchen wir zumindest im Appendix so einen plot. Waere ohnehin auch in Section 6 kein Fehler, wenn wir zu Figure 5 dazusagen koennten, dass die Verteilung der individuellen Laufzeiten im Appendix gezeigt wird. }

% \rp{In der Diplomarbeit, Figure 8.4 sieht man schoen die 2 unterschiedlichen Kurven (genau genommen Balkendiagramme; muss man in Kap.6 unbedingt erklaeren, wieso so ein spike bei rund um 0 ist: weil ja auch viele queries Laufzeiten unter 1 sec haben.}

\nop{*******************************
Actually, we have also considered a 3-class classification by introducing a class ``equal'' in addition 
to 0 and 1. The intended meaning of such a class would be that the computation times of the 
two evaluation methods are ``essentially equal'', i.e., they only differ by a very small margin. We have 
experimented with values 0.01, 0.05, 0.1, and 0.5 seconds as range within which two values are considered as equal.
The motivation for such a third class is that, above all, 
we want to avoid {\em false positives}, 
i.e., situations where our decision program chooses the rewriting 
although the original evaluation method is faster. 
In order to reduce the number of false positives (predictably at the expense of an increase of false negatives, though), 
we would then map the class of ``equal'' predictions 
to 0 (= choose the original evaluation method). Analogously, we can also introduce a threshold $\theta$ 
(with values -0.1, -0.5, etc.) for the 
predicted difference in case of regression, such that the ultimate decision is 0, as long as the difference 
is $ \geq \theta$. However, in our experiments it turned out that the gain in terms of false positives
was much smaller than the loss in terms of false negatives. So we have decided
to stick to 2-classes for the classification problem and, in case of regression, 
to decide in favor of rewriting whenever the predicted difference is $< 0$.
*******************************}

\subsection{Selecting the ML Model Types}
\label{sect:SelectingML:Types}

We have chosen 7 Machine Learning model types for our algorithm selection problem, namely
k-nearest neighbors (k-NN), decision tree, random forest, support vector machine (SVM), and 3 forms of 
neural networks (NNs): multi-layer perceptron (MLP), hypergraph neural network (HGNN)
and a combination of the two. 
MLP is the ``classical'' deep neural network type. Hypergraph neural networks, introduced in~\cite{DBLP:conf/aaai/FengYZJG19}, are less known. With their idea of representing the hypergraph structure in a vector space, 
it seems well suited to capture structural aspects of conjunctive queries. Just like MLPs, also the
HGNNs 
% ultimately 
produce an output vector. In our combination of the two model types, 
we provide yet another
neural network, which takes as input the two output vectors produced by the MLP and the HGNN
and combines them to a joint result using addtional layers.

A major task after choosing these ML model types is to fix the hyperparameters. 
For instance, in case of k-nearest neighbors, we settled for $k = 5$ after some preliminary experiments. 
For random forest, we fixed the number of combined decision trees as $100$. 
For the support vector machine, we
chose 3 different kernels, namely linear, poly, and rbf. 

For the 3 types of neural networks, several additional hyperparameters had to be chosen. 
As loss function, we chose the cross entropy for classification and the mean squared error (MSE) 
for regression. For the training of the NNs, we set the number of epochs (i.e., how often 
the dataset is used in one training step) and also the batch size (i.e., after how 
many runs we update the weights) to 100, with the
learning rate set to 0.001. 
The number of hidden layers for the MLP is either 1 or 2, and we experiment with various  
numbers of nodes.
The HGNN has two or three convolutional layers with a kernel size of 3x3 and then one max-pooling layer. For the combination of the MLP and the HGNN, we use the best performing model of the MLPs and the best performing model of the HGNNs and combine the outputs by applying one, two or three linear layers. Finally,  the number of features is reduced. More precisely, several features that occur multiple times as min, max, mean, median 25\%- and 75\% quartiles are reduced to a single node via a custom NN.
A detailed
list of all hyperparameter values, in particular for the 3 types of neural networks used here, 
is provided 
\ifArxiv
in \Cref{app:SolvingAlgorithmSelectionProblem}.
\else
in the full paper.
\fi


% \rp{Diesen Absatz ueber die hyperparameter muss unbedingt jemand kritisch lesen, der/die sich mit dem 
% Zeug auskennt. Im Appendix sollte wohl noch einige zusaetzliche Erklaerungen liefern -- vor allem fuer die letzte Spalte in Table \ref{tab:hyperparameters}!}

% \rp{Noch ein Punkt: Koennen wir fuer die 3 neural network types etwas daruber sagen, welche hyperparameter
% Einstellungen letztlich die besten waren und in Section \ref{sect:evaluationAlgorithmSelection}
% dann vermutlich hauptsaechlich (oder ausschliesslich) ueber diese berichten. Eventuell gehoert so ein 
% Absatz ueber die Auswahl der besten hyperparameter ohnehin eher ins Evaluation Kapitel.}


\subsection{Labeling and Splitting the Data}
\label{sect:SplittingData}

After running the 4677 queries mentioned in Section \ref{sect:DataAugmentation} on the 
3 selected DBMSs according to Section \ref{sect:SelectionDBMSs}, we have to prepare the 
input data for training the ML models of the 7 types mentioned in Section \ref{sect:SelectingML:Types}.
Recall from Section \ref{sect:FeatureSelection} that each query is characterized by a 
feature vector specific to each of the 3 DBMSs. For our supervised learning tasks 
(classification and regression), 
we have to label each feature vector for each of the 3 DBMSs. As explained in 
Section~\ref{sect:SelectingML:Types}, we want to train our models both, for classification and 
for regression. Hence, on the one hand, each feature vector gets labeled 0 or 1 
(meaning that the original evaluation of the DBMS or the Yannakakis-style evaluation is faster; 
in case of a tie, we assign the label 0) for the classification
task. On the other hand, each feature vector is labeled with the 
difference of the runtime of the original evaluation minus
the runtime of the Yannakakis-style evaluation for the regression task. 

The labeled data can then be split into training data, validation data,  and test data. 
As is quite common, we split the data so that the training set contains 80\% of the data and 
the other two contain 10\% each. The splitting is done with stratification, which means that each of our five benchmark datasets is represented in each of the three sets approximately with the same proportion.
For the actual training of the models, to get more accurate results, 
we have decided to do 10-fold cross validation. That is, we split the 90\% of the data 
that were chosen for training and validation in 10 different ways in a ration 80:10 into training:validation data
and, thus,  repeat the training-validation step 10 times.





\subsection{Selection Criteria for the ``Best'' Model}
\label{sect:Selection Criteria}

In order to ultimately choose the ``best'' model for our decision program between the 
original evaluation method of each DBMS and the Yannakakis-style evaluation, we compare, for every feature vector, the predicted classification with the actual labeling. 
We refer to classification 1 as ``positive'' and classification
0 as ``negative''. This leads to 4 possible outcomes of the comparison between predicted and actual value, namely TP (true positive) and TN (true negative) for correct classification and 
FP (false positive) and FN (false negative) for misclassification. They give rise to the 
3 most common metrics: accuracy (shortened to ``Acc''), which is the proportion of correct classifications, precision (shortened to ``Prec''), which is defined by TP / (TP + FP), and recall (shortened to ``Rec''), defined as TP / (TP + FN). 

% \rp{Recall wird in Section 6 nicht beruecksichtigt. Muessen wir irgendetwas dazu sagen? Oder ev. doch 
% jeweils Rec in Section 6 ergaenzen? Naja, zumindest im Appendix sollten wir schon auch die Recall Zahlen liefern.}

Of course, the natural goal when selecting a particular model is to maximize the 
accuracy. However, in our context, 
we consider the precision equally important. That is, we find it particularly important 
to minimize false positives. In other words, in case of doubt, it is better to stick to 
the original evaluation method of the DBMS rather than wrongly choosing an alternative 
 method.

For regression, we aim at minimizing the mean squared error (MSE). 
But ultimately, we also map the (predicted and actual) difference between the 
runtime of the original minus Yannakakis-style evaluation to a 0 or 1 classification. 
Hence, we can again measure the quality of a model in terms of accuracy, precision, and recall. 

Apart from the purely {\em quantitative} assessment of a model in terms of accuracy, precision, 
and recall, we also carry out a {\em  qualitative} analysis. That is, for each of the misclassified
cases, we want to investigate by how much the chosen evaluation method is 
slower than the optimal method. And here, we are again particularly interested in the
false positive cases. Apart from aiming at high accuracy and precision, we also want to 
make sure for the false positive classifications, that the difference in the 
runtimes between the 2 evaluation methods is rather small.


\begin{table}[t]
    \centering
    \caption{Performance of Machine Learning Classifiers on the PostgreSQL runtimes. We show accuracy, precision and recall for binary classifiers that predict whether rewriting to Yannakakis style evaluation leads to performance gain.}
    \label{tab:ml.performance_class}
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{lccc|ccc}
        \toprule
        \textbf{Algorithm} & \multicolumn{3}{c|}{\textbf{0MA Queries}} & \multicolumn{3}{c}{\textbf{Acyc. Queries}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} & Acc. (\%)$\uparrow$  & Prec. $\uparrow$ & Rec. $\uparrow$ & Acc. (\%) $\uparrow$ & Prec. $\uparrow$ & Rec. $\uparrow$ \\
        \midrule
        Decision Tree & \bf 0.94 & \bf 0.92 & \bf 0.97     & \bf 0.95 & \bf 0.95 & 0.92 \\
        Random forest & \bf 0.94 & \bf 0.92 & \bf 0.97         & \bf 0.95 & 0.94 & \textbf{0.93} \\
        $k$-NN & 0.91 & 0.91 & 0.90                    & 0.91 & 0.88 & 0.91 \\
        SVM & 0.85 & 0.85 & 0.84                       & 0.84 & 0.82 & 0.77 \\
        MLP & 0.87 & 0.89 & 0.86                       & 0.85 & 0.84 & 0.77 \\
        HGNN & 0.83 & 0.84 & 0.85                      & 0.79 & 0.70 & 0.75 \\
        HGNN+MLP  & 0.82 & 0.78 & 0.93                 & 0.81 & 0.77 & 0.72 \\
        % Add more algorithms as needed
        \bottomrule
    \end{tabular}
\end{table}

\section{Experimental Results}
\label{sect:evaluationAlgorithmSelection}





% \rp{Weitere Tabellen und Figures im Stil der Diplomarbeit, die auch in Kapitel 6 sehr gut herpassen wuerden: \\[1.1ex]
% {\em Resultat des Labeling}: (vgl. Diplomarbeit, Table 8.11; wobei es fuer uns aber nur die classifications 0 vs. 1 gibt; ohne equal!): wir koennten zumindest im Fliesstext (vielleicht eh schon in Kap. 5.3) erwaehnen, wie sich unsere Testdaten auf die classifications 0 bzw. 1 verteilen. Bei 0MA in der Diplomarbeit war es fast Hälfte:Hälfte. \\[1.1ex]
% %
% {\em Qualitative Assessment of the predictions}: neben der quantitativen Bewertung mit Acc, Prec und Rec haben wir in Kap 5.4
% auch eine qualitative Bewertung versprochen, d.h.: wir messen fuer die misclassifications, um wie viel die von uns ausgewaehlte enumeration method langsamere ist als die bessere. Vgl. Table 8.10 und Fig. 8.6 in der Diplomarbeit. Und wie in der Diplomarbeit 
% koennte man dann halt auch wieder heraussteichen, dass die false positives extrem harmlos sind (in der Diplomarbeit nur 1 FP mit 1 sec. Abweichung). \\[1.1ex]
% %
% % {\em Recall.} Ich glaube, wir muessen unbedingt (neben Acc und Prec) auch die Werte fuer Recall angeben. Wenn wir Platznot haben, kann man Rec natuerlich im main body nur mit 1-2 Saetzen erwaehnen. Aber im Appendix muss man Recall definitiv angeben.\\ In der Diplomarbeit sind die Rec Werte um einiges schlechter als Acc und Prec. Hoffentlich sind sie in den neuen Messungen besser. Aber es hilft eh nichts. Muessen wir halt betonen, dass Prec viel wichtiger ist und (siehe Qualitative Assessment) darauf hinweisen dass die Abweichung bei false positives ohnehin fast immer unter 1 sec liegt. 
% }

% \rp{Generelle Anmerkungen zu Section 6: \\
% {\em ``key questions''}: wir sollten vielleicht bereits in Kap. 6 jeweils auf die questions Q1 - Q3 eingehen, wenn 
% sie beantwortet werden, also z.B: Q1 mit der Tabelle zu Acc/Prec; Q2 mit den Gini Koeffizienten; Q3 mit den 
% e2e Laufzeiten. Und dann eventuell noch einmal in der Conclusion. \\
% Nachtrag: Ich bin jetzt in der Conclusion auf die key questions eingegangen. Wenn sie schon in 
% Kap. 6 beantwortet werden, muesste man ev. die Conclusion kuerzen.\\[1.05ex]
%
% {\em classification vs. regression}: in Kap. 5 wird noch darueber sinniert, ob es besser ist, 
% classification vs. regression zu verwenden. Irgendetwas muessen wir dazu in Section 6 sagen. 
% Haben wir fuer regression irgendwelche Zahlen (e.g., MSE) oder dann fuer classification auf Basis von 
% regression (also Acc und Prec)?
% Also insbesondere, dass regression keine Verbesserung bringt (haben wir Messungen, die das belegen?) 
% }

In this section, we present experimental results obtained by putting the algorithm selection method described in 
Sections~\ref{sect:DefiningAlgorithmSelectionProblem} and~\ref{sect:SolvingAlgorithmSelectionProblem} to work. We thus first evaluate 
in Section~\ref{sec:mlexp}
the performance of various machine learning models on the raw dataset of query runtimes obtained for the selected and augmented benchmarks on the chosen DBMSs.
Afterwards, in Section~\ref{subsec:effects_perf},
we  evaluate the performance gains by combining the best algorithm selection model with our rewriting method for evaluating acyclic queries.  
In particular, we perform experiments to answer the following key questions:

\begin{description}
    \item[Q1] How well can machine learning methods  predict whether Yannakakis-style query evaluation
    is preferable over standard query execution?
    \item[Q2] Can we use these machine learning models to gain insights about the circumstances in which Yannakakis-style query evaluation is preferable over standard query execution?
    \item[Q3] How well does good algorithm selection performance translate to query evaluation times on 
    different DBMSs?
    \item[Q4] To what extent can we optimize for precision while maintaining end-to-end runtime and accuracy?
%    \item {\color{red} Is there a difference when applying our methods to different databases?}
\end{description}

\rp{In Kap. 6 muessen wir die Saetze oder Absaetze, wo diese Fragen beantwortet werden, entsprechend explizit machen. Z.B.: ``As far as our key question Q1 is concerned, we observe that ...''}
\co{Für alle 4 key questions gibt es jetzt einen kurzen Paragraph in Sect 6 der sie versucht zu beantworten} 

In the remainder of this section, we present our experimental results and 
a discussion as to how they answer our key question. Further details are provided
\ifArxiv
\change{in \Cref{app:AdditionalExp}.}
\else
\change{in the full paper.}
\fi

% \rp{Im Moment haben wir im Appendix keine ``further details on our experimental setup''. Kommen die noch oder
% loeschen wir diesen Satz lieber?}



% \begin{table}[b]
%     \centering
%     \caption{Performance of Classification Models. We show both accuracy and precision for binary classifiers that predict whether rewriting to Yannakakis style evaluation leads to performance gain.}
%     % \todo[color=cyan, inline]{Alex: Ich habe es nicht geschafft das HGNN+MLP Modell auf allen Queries mit guter Performance zu trainieren. Falls wir das nicht besser hinbekommen eventuell weglassen?
% % }
% %     \rp{Ja, eh - hilft nichts. Dann halt auch in Kap. 5 entfernen. }
% % \todo[inline]{ML: Ich finde das schon ok so. Bei 0MA passts ja halbwegs, also scheint hier schon eine neue challenge fuer den allgemeinen HGNN+MLP fall zu entstehen. Ich wuerde es lieber lassen und dann koennen wir sagen das braucht scheinbar mehr in-depth exploration in the future.}
% % \todo[color=cyan, inline]{Alex: Jetzt ist die Performance nach nochmal trainieren wesentlich besser. Scheinbar ist das Training etwas instabil. Dann kann man es ruhig so lassen.
% % }
%     \label{tab:ml.performance}
%     \begin{tabular}{lcc|cc}
%         \toprule
%         \textbf{Algorithm} & \multicolumn{2}{c|}{\textbf{0MA Queries}} & \multicolumn{2}{c}{\textbf{Acyc. Queries}} \\
%         \cmidrule(lr){2-3} \cmidrule(lr){4-5}
%         & Acc. (\%)$\uparrow$  & Prec. $\uparrow$  & Acc. (\%) $\uparrow$ & Prec. $\uparrow$  \\
%         \midrule
%         Decision Tree & \bf 0.94 & \bf 0.92 & \bf 0.95 & \bf 0.95 \\
%         Random forest & 0.94 & \bf 0.92 & \bf 0.95 & 0.94 \\
%         $k$-NN & 0.91 & 0.91 & 0.91 & 0.88 \\
%         SVM & 0.85 & 0.85 & 0.84 & 0.82 \\
%         MLP & 0.87 & 0.89 & 0.85 & 0.84 \\
%         HGNN & 0.83 & 0.84 & 0.79 & 0.70\\
%         HGNN+MLP  & 0.82 & 0.78 & 0.81 & 0.77 \\
%         % Add more algorithms as needed
%         \bottomrule
%     \end{tabular}
% \end{table}




\begin{table}[t]
    \caption{Chosen hyperparameters.}
    \label{tab:model_hyperparameters}
    \centering
    \begin{tabular}{ll}
        \toprule
        \bf Model & \bf Hyperparameters \\ 
        \midrule
    %    Decision Tree & - \\
        Random Forest & \#estimators = 100 \\
        kNN & k = 5 \\
        SVM & kernel = linear \\
        MLP & layers = 30-60-40-2 \\
        HGNN & layers = 1-16-32-2 \\
        \bottomrule
    \end{tabular}

\end{table}




\begin{table}[t]
    \centering
    \caption{Performance of Regression Models. We show MSE and MAE for regression models predicting the difference between the runtime of the original and the rewritten query. Additionally, we present the accuracy and precision after converting the regression model to a classification model by setting a threshold at a predicted time difference of $0$ seconds.}
    \label{tab:ml.performance_reg}
    \begin{tabular}{lcc|ccc}
        \toprule
        \textbf{Algorithm} & \multicolumn{5}{c}{\textbf{Acyc. Queries}} \\
        \cmidrule(lr){2-6}
        & MSE  & MAE  & Acc. & Prec. & Rec.  \\
        \midrule
        Decision Tree   &  \bf 0.02 & \bf 0.06 & \bf 0.96 & \bf 0.96 & \bf 0.94 \\
        Random forest   & 0.03 & 0.08 & 0.95 & 0.94 & 0.93 \\
        $k$-NN          & 0.17 & 0.18 & 0.91 & 0.88 & 0.91 \\
        SVM             & 1.02 & 0.61 & 0.79 & 0.76 & 0.72 \\
        MLP             & 0.39 & 0.32 & 0.81  & 0.72 & 0.77 \\
        HGNN            & 0.74  & 0.48  & 0.71  & 0.57 & 0.85 \\
%        HGNN+MLP        &  &  &  &  \\
        % Add more algorithms as needed
        \bottomrule
    \end{tabular}
\end{table}



% \rp{Figure~\ref{fig:main.dbperf} gehoert weiter nach hinten (ist Teil von Section 6.2, waehrend sich 
% die momentane Figure 7 auf Section 6.1 bezieht. Ausserdem gehoert die caption korrigiert: es gibt keine gray bars!}
% noch 




% \todo[inline]{ML: at \Cref{fig:difference_comparison}, I don't understand why its just the same plot twice but once scaled logarithmically (just to highlight the small values in there seems strange, why not just scale the y axis logarithmically in the upper plot? 
% Either just do the top half, or put the distribution of predictions for the regression model in the bottom one, to show how well the distribution was learned (make sure to have the axis the same between both plots in this case).
% }

\subsection{Model Training}
\label{sec:mlexp}




In a first step, we will compare the performance of various learned models in terms of accuracy, precision, and recall. 
\change{Table \ref{tab:ml.performance_class} compares the performance of the best classification models (with the hyperparameters given in Table \ref{tab:model_hyperparameters}) on the 0MA queries only, 
as well as on all queries (i.e., 0MA and enumeration), on the runtime data from PostgreSQL. Decision trees and random forests, with roughly the same performance, achieve the best accuracy, precision and recall out of all classifiers. kNN achieves the next best performance, although significantly less at 91\% accuracy compared to the 95\% accuracy of the decision tree-based models. The random forest classifier has a similar performance to the simple decision trees. Hence, its disadvantage of additional complexity is not 
outweighed by a significant performance improvement. Therefore, the simple decision tree classifier is the clear choice out of all compared.
}






Next, we compare the performance of the regression variants of these models, presented in Table \ref{tab:ml.performance_reg}, on all queries, again on the PostgreSQL runtimes, transformed according to Section \ref{sect:FormulatingML:Task} and Figure \ref{fig:difference_comparison}. We can see that the regression performance in terms of MSE and MAE corresponds closely to the classification performance of the classification models, with the decision tree and random forest models again at the top. However, the gap to the next best model is even larger, corresponding to a 5-8x increase in MSE and a 2-3x increase in MAE, making the decision tree-based models again preferable in this situation. 
From these results, it can be observed that we are able to predict the runtime difference (i.e., runtime of the
rewritten query minus runtime of the original query) quite accurately. This predicted runtime
difference naturally lends itself to classification by choosing the query rewriting if and only if
this difference is below 0. This classification derived from the decision tree regression model 
leads to a similar (actually slightly better by 0.5\%) performance than the decision tree classification.
Moreover, as will be discussed next, classification based on the predicted runtime difference provides
additional flexibility for fine-tuning the trade-off between precision and recall. 
Hence, we have chosen classification based on the 
decision tree regression model as basis for our decision procedure.



\begin{figure}
    \centering
    % \scriptsize
% \begin{tikzpicture}
% \begin{axis}[
%     ybar,
%     ymin=0,  
%     height=4cm,
%     width=\columnwidth,
%     xlabel=$\mathit{diff} \coloneqq \mathit{rewritten} - \mathit{original\_runtime}$
% ]
% \addplot +[
%     hist={
%         bins=30
%     }   
% ] table [y index=1, col sep=comma] {data/y1_diff.csv};
% \end{axis}
% \end{tikzpicture}

\begin{tikzpicture}
\begin{axis}[
    ymode=log,
    ybar,
    ymin=0,  
    height=4cm,
    width=\columnwidth,
    xlabel style={align=center},
    xlabel=
    $\mathit{diff} \coloneqq \mathit{rewritten} - \mathit{original\_runtime}$ \\    
    $\mathit{sgn}(\mathit{diff}) \log(|\mathit{diff}|+1)$
    % xticklabel=
    %     \pgfmathprintnumber\tick \ --  \pgfmathprintnumber\nexttick 
]
\addplot +[
    hist={
        bins=30
    }   
] table [y index=1, col sep=comma] {data/y1_diff_log.csv};
\end{axis}
\end{tikzpicture}
    \caption{Distribution of the regression response, understood as the time difference between rewritten runtime and original runtime for PostgreSQL, under the given log transformation.  Above we show a histogram of this distribution, using log scaling to allow for more visual clarity. 
    % on the raw difference, below 
    }
    \label{fig:difference_comparison}
\end{figure}


\begin{figure}[t]
    \centering
    \makebox[\linewidth][c]{\includegraphics[width=1\linewidth]{graphics/threshold-all.pdf}}
    \caption{Accuracy, precision, recall, as well as the e2e runtime over the test set, of the regression model converted to a classifier, depending on the threshold set (decision tree regression, PostgreSQL, all queries).}
    \label{fig:regr_threshold}
\end{figure}




% \todo[inline]{Continue to describe \Cref{tab:ml.performance}}.

% \rp{Wir koennen glaube Recall nicht einfach weglassen. Also mindestens irgend etwas im plain text uever Recall schreiben und Details im Appendix bringen.}

% \rp{Wie am 31.1. auf Zoom besprochen: vermutlich bringen die 3 decision trees im Appendix nichts. Kann man 
% im plain text etwas ueber die Struktur der decision trees sagen? (zusaetzlich zum Verweis auf die 
% Gini coefficients).}



\paragraph{Fine-tuning precision and recall}

\change{So far we have only considered choosing a {\em threshold} of the predicted runtime difference of the regression model at $0$ (i.e., if the 
predicted runtime difference is below this threshold, we choose  Yannakakis-style query evaluation,  
and, otherwise, we
opt for the original query execution).
However, we can make use of the regression model as a useful tool to configure the trade-off between precision and recall, depending on the requirements of the application. To simplify the presentation, we focus on one DBMS and on one model, namely PostgreSQL and decision trees. 
The extension to other models and DBMSs is straightforward and yields similar results. 
%
\rp{Es waere vermutlich schon sinnvoll, die entsprechenden Figures fuer die anderen DBMSs im Appendix zu bringen.}
%
In Figure \ref{fig:regr_threshold}, we show how changing the threshold affects the accuracy, precision and recall 
of the resulting classification model. Clearly, the maximum accuracy is at the $0$-threshold. The accuracy, however, continues to be high in the direction of negative thresholds, while falling off quickly in the direction of a positive threshold. 
In particular, this shows that an optimization of precision can be performed without sacrificing much recall. }

\change{ To summarize our findings on the quality of predicting when Yannakakis-style query evaluation is better: the results from \Cref{tab:ml.performance_class} and \Cref{tab:ml.performance_reg} that show very high levels of accuracy, precision and recall for our chosen model of decision trees, as well as the ability to further optimize for precision as seen in \Cref{fig:regr_threshold} allow us to positively answer the key question \textbf{Q1}.
In \Cref{fig:regr_threshold} we also get an answer for our key question \textbf{Q4}: by choosing the right threshold one can achieve almost perfect precision, with only modest reductions in accuracy and e2e runtime (which we discuss in detail in \Cref{subsec:effects_perf}).
}
\co{Unklar ob man Q4 hier beantworten soll, oder besser in \Cref{subsec:effects_perf}. Es geht irgendwie um beides, den tradeoff von acc/prec und die e2e runtime.}

% \rp{Figure 7 muss nach vorne kommen, wenn wir hier die Flexibilitaet bei classification via regression
% anpreisen wollen. Wobei wir hier nur Acc, Prec, Rec diskutieren sollten; die e2e-runtimes kommen ja erst in 
% Section 6.2}

\paragraph{Insights from Decision Trees}

\begin{table}[t]

\setlength{\tabcolsep}{3pt}
\centering
\caption{Most important features according to Gini coefficient of the Decision Tree models.}
\label{tab:gini}
\begin{tabular}{lc||lc}
\toprule
\multicolumn{2}{c}{PostgreSQL} & \multicolumn{2}{c}{DuckDB} \\
\textbf{Feature} & \textbf{Gini} & \textbf{Feature}  & \textbf{Gini}\\
\midrule
\textbf{P3}$^*$ max(est. join rows) & 0.369 & \textbf{B1}\phantom{$^*$} is 0MA? & 0.280 \\
\textbf{B1}\phantom{$^*$} is 0MA? & 0.157 & \textbf{B7}$^*$ mean(cont. c.) & 0.206 \\
\textbf{P2}$^*$ q75(est. sing. table rows)	 & 0.069 & \textbf{D1}$^*$ max(est. card.) & 0.147  \\
\textbf{P1}\phantom{$^*$} est. total cost & 0.053 & \textbf{D1}$^*$ q25(est. card.) & 0.061 \\
 \textbf{P3}$^*$ min(est. join rows) & 0.051 & \textbf{D1}$^*$ q75(est. card.) & 0.057\\
% mean(container c.) & 0.002 & & \\
\bottomrule
\end{tabular}
\end{table}



\begin{figure*}[h!]
\definecolor{Gray}{RGB}{155,155,155}
\definecolor{LightGray}{RGB}{180,180,180}
% \definecolor{PastelBlue}{RGB}{135,206,250}     % Light Sky Blue
% \definecolor{PastelGreen}{RGB}{144,238,144}    % Light Green
% \definecolor{PastelOrange}{RGB}{255,182,193}   % Light Pink
\definecolor{BaseA}{RGB}{173,216,230}        % Light Blue for Base - Value A
\definecolor{BaseB}{RGB}{135,206,235}        % Sky Blue for Base - Value B
\definecolor{RewritingA}{RGB}{144,238,144}   % Light Green for Rewriting - Value A
\definecolor{RewritingB}{RGB}{60,179,113}    % Medium Sea Green for Rewriting - Value B
\definecolor{AIA}{RGB}{255,182,193}          % Light Pink for SMASH - Value A
\definecolor{AIB}{RGB}{255,160,122}          % Light Salmon for SMASH - Value B

\makeatletter
\newcommand\resetstackedplots{
\makeatletter
\pgfplots@stacked@isfirstplottrue
\makeatother
\addplot [forget plot,draw=none] coordinates{(Base,0) (Rewriting,0) (SMASH,0)};
}
\begin{tikzpicture}
  \begin{groupplot}[
    group style={
      group size=3 by 1,         % 3 plots in a single row
      horizontal sep=1.5cm,        % Space between plots
    },
    ybar stacked,
    /pgf/bar width=28pt,
    ymax=6000,
    ymin=0,
    height=5.5cm,
    width=6cm,
    symbolic x coords={Base, Rewriting, SMASH},
    xtick={},
    % Adjust the overall font size if needed
%    every axis/.append style={
%      font=\small,
%    },
    % Nodes near coords for data labels
%    nodes near coords,
    nodes near coords align={vertical},
    enlarge x limits=0.3,      % Add some spacing on the x-axis
    ymajorgrids=true,
    grid style=dashed,          
    legend style={
      at={(7.5,-0.5)},            % Position the legend below the plots
      anchor=north,
      /tikz/every even column/.append style={column sep=0.5cm}
    },        
    legend to name=named,
    legend columns=6,
    legend cell align={left}
    ]
\nextgroupplot[
    ylabel={e2e-time (s)},
      title={\large PostgreSQL}
    ]
    
        % \addplot+ coordinates {(Base,0) (Rewriting,0) (SMASH,0)};
        
      % \addplot [thick, fill=LightGray, draw=black] coordinates {(Base,380)};
      % \addplot+ [thick, fill=Gray, draw=black] coordinates {(Base,2310)};
      \addplot+ [thick, fill=BaseA, draw=black] coordinates {(Base,3124)};
      \addplot+ [thick, fill=BaseB, draw=black] coordinates {(Base,525)};
      
      \resetstackedplots
      % \addplot+ [thick, fill=LightGray, draw=black] coordinates {(Rewriting,380)};
      % \addplot+ [thick, fill=Gray, draw=black] coordinates {(Rewriting,2310)};
      \addplot+ [ thick, fill=RewritingA, draw=black] coordinates {(Rewriting,923)};
      \addplot+ [ thick, fill=RewritingB, draw=black] coordinates {(Rewriting,737)};
      \resetstackedplots

        % \addplot+ coordinates {(Base,0) (Rewriting,0) (SMASH,0)};
      % \addplot+ [thick, fill=LightGray,  draw=black] coordinates {(SMASH,380)};
      % \addplot+ [thick, fill=Gray,  draw=black] coordinates {(SMASH,2310)};
      \addplot+ [ thick, fill=AIA, draw=black] coordinates {(SMASH,760)};
      \addplot+ [ thick, fill=AIB, draw=black] coordinates {(SMASH,503)};

   %numbers on top 
   \node at (axis cs:Base,3965) [anchor=south, font=\small] {3965};
    \node at (axis cs:Rewriting,1849) [anchor=south, font=\small] {1849};
    \node at (axis cs:SMASH,1256) [anchor=south, font=\small] {1256};


    % Plot 2: SparkSQL
    \nextgroupplot[
      title={\large SparkSQL}
    ]
        % \addplot+ coordinates {(Base,0) (Rewriting,0) (SMASH,0)};
        
      % \addplot [forget plot,thick, fill=LightGray,draw=black] coordinates {(Base,20)};
      % \addplot+ [forget plot,thick, fill=Gray,draw=black] coordinates {(Base,1320)};
      \addplot+ [forget plot,thick, fill=BaseA,draw=black] coordinates {(Base,5246)};
      \addplot+ [forget plot,thick, fill=BaseB,draw=black] coordinates {(Base,595)};
      \resetstackedplots
      % \addplot+ [forget plot,thick, fill=LightGray,draw=black] coordinates {(Rewriting,20)};
      % \addplot+ [forget plot,thick, fill=Gray,draw=black] coordinates {(Rewriting,1320)};
      \addplot+ [forget plot,thick, fill=RewritingA,draw=black] coordinates {(Rewriting,1110)};
      \addplot+ [forget plot,thick, fill=RewritingB,draw=black] coordinates {(Rewriting,1052)};
      \resetstackedplots
      % \addplot+ [forget plot,thick, fill=LightGray,draw=black] coordinates {(SMASH,20)};
      % \addplot+ [forget plot,thick, fill=Gray,draw=black] coordinates {(SMASH,1320)};
      \addplot+ [forget plot,thick, fill=AIA,draw=black] coordinates {(SMASH,1047)};
      \addplot+ [forget plot,thick, fill=AIB,draw=black] coordinates {(SMASH,580)};

   %numbers on top 
 \node at (axis cs:Base,5241) [anchor=south, font=\small] {5841};
    \node at (axis cs:Rewriting,2162) [anchor=south, font=\small] {2162};
    \node at (axis cs:SMASH,1627) [anchor=south, font=\small] {1627};
    
    % Plot 3: DuckDB
    \nextgroupplot[
      title={\large DuckDB}
    ]
        % \addplot+ coordinates {(Base,0) (Rewriting,0) (SMASH,0)};
        
      % \addplot [thick, fill=LightGray,draw=black] coordinates {(Base,860)};
      % \addplot+ [thick, fill=Gray,draw=black] coordinates {(Base,2150)};
      \addplot+ [thick, fill=BaseA,draw=black] coordinates {(Base,2580)};
      \addplot+ [thick, fill=BaseB,draw=black] coordinates {(Base,81)};
      \resetstackedplots
      % \addplot+ [forget plot,thick, fill=LightGray,draw=black] coordinates {(Rewriting,860)};
      % \addplot+ [forget plot,thick, fill=Gray,draw=black] coordinates {(Rewriting,2150)};
      \addplot+ [thick, fill=RewritingA,draw=black] coordinates {(Rewriting,1190)};
      \addplot+ [thick, fill=RewritingB,draw=black] coordinates {(Rewriting,634)};
      \resetstackedplots
      % \addplot+ [forget plot,thick, fill=LightGray,draw=black] coordinates {(SMASH,860)};
      % \addplot+ [forget plot,thick, fill=Gray,draw=black] coordinates {(SMASH,2150)};
      \addplot+ [thick, fill=AIA,draw=black] coordinates {(SMASH,531)};
      \addplot+ [thick, fill=AIB,draw=black] coordinates {(SMASH,71)};

        %numbers on top 
     \node at (axis cs:Base,2661) [anchor=south, font=\small] {2661};
    \node at (axis cs:Rewriting,1824) [anchor=south, font=\small] {1824};
    \node at (axis cs:SMASH,602) [anchor=south, font=\small] {602};


    \legend{Base 0MA, Base Enum, Rewriting 0MA, Rewriting Enum, SMASH 0MA,SMASH Enum}
    
  \end{groupplot}

  \ref{named}

\end{tikzpicture}

\caption{Comparison of e2e performance over the test set queries for three database systems: PostgreSQL, Spark (via SparkSQL), and DuckDB. The full bar indicates the runtime over all test set queries, the lower mark indicates the time for 0MA queries. 
% Gray bars indicate the cases where both the original query and the rewriting timed out, with the upper bar representing the e2e runtime of the enumeration queries, and the lower bar that of the 0MA queries. 
}
% \todo[color=cyan, inline]{Alex: Woher kommt der bunte Rand im mittleren Balken?
% }
\label{fig:main.dbperf}
\end{figure*}





It is of particular interest that decision trees are among the top-performing models as they are highly interpretable and can provide us with deeper insights into the features that strongly affect the prediction.  In Table \ref{tab:gini}, we present the Gini coefficients of the top 5 most relevant attributes, for the PostgreSQL and the DuckDB decision tree classifiers
\ifArxiv
(for details on SparkSQL and further attributes, see Table \ref{tab:giniext} in the Appendix).
\else
(we present details on SparkSQL and further attributes in the full paper).
\fi
\change{Note that \emph{the feature set between the two systems is different}, as explained in \Cref{sect:FeatureSelection}, so we cannot compare the Gini coefficients of all the same features.}
The Gini coefficient~\cite{breiman2017classification}
% , for a feature, is defined as the total decrease in node impurity, weighted by the probability of reaching that node, and 
measures the contribution of the feature to the outputs of the model. Looking at the Gini coefficients, we see that, although the performance of the models trained on the PostgreSQL and DuckDB features and runtimes are very similar, the decision trees rely on different features. In the case of DuckDB, the feature indicating whether the query is a 0MA query has the highest importance. While the PostgreSQL model strongly relies on the maximum join rows in the plan, DuckDB's second-most-important feature is the mean number of container counts, a feature extracted from the join tree. 
\change{The model for PostgreSQL, on the other hand, only use a single basic feature, namely \textbf{B1} {(is 0MA?)}, among the most important features.}
\change{
This ability of decision trees to highlight which specific features affect the prediction the most, helps us answer the key question \textbf{Q2} positively. We hope our full experimental artifacts, which include the full decision trees, will help foster further research into using these features to improve query engine optimization in general. }


% \todo[inline]{ML: saying 0ma or enum is a problem. It sounds like a query is either one or the other, but we know that there are plenty of aggregate queries that are not 0ma. I don't know if this appears throughout the document elsewhere too, but if it does it should be fixed everywhere. (the flag for the gini table can just be calles "is 0ma?" or so).}
% \todo[color=cyan, inline]{Alex: Ist überall umbenannt}

% \todo[inline]{ML: explain Gini score. Then explain and discuss \Cref{tab:gini}. The features there should just be the union of the top-5 features of postgre and duckdb
% }

% \rp{Auch wenn wir die decision trees nur im Appendix zeigen koennen: es waere schon hilfreich, wenn hier im main body etwas ueber Unterschiede zwischen den decision trees fuer die 3 DBMSs gesagt werden koennte. 
% Sind alle 3 gleich? oder sehr aehnlich? Was sind die wesentlichsten Unterschiede?}

% \todo[color=cyan, inline]{Alex: In table 5 sind jetzt auch die top features vom DuckDB decision tree. Da kann man z.B. sagen, dass es bei DuckDB relevanter ist, ob die Query 0MA oder enum ist als bei Postgres. Ebenfalls sieht man, dass die container counts sehr hoch oben sind, während bei Postgres gar keine von den features des join tree in den top 5 sind.
% }








\subsection{Effects on Database Performance}
\label{subsec:effects_perf}
% \todo[inline]{ML: I do not understand the timeout numbers in \Cref{fig:main.dbperf}. Why is the timeout number constant between all approaches? Do we really have a significant number of 0ma timeouts with rewriting in duckdb?}
%     \todo[color=cyan, inline]{Alex: Die "timeout" Fälle sind die Queries wo sowohl die Original-Query als auch die Rewritten-Query einen Timeout haben. Ich denke wir haben das nicht genau genug beschrieben. Wenn nur eines davon einen Timeout hat, ist es zu den nicht grauen Balken dazugerechnet. Darum macht es keinen Unterschied welche Methode wir wählen also sind die Timeouts überall gleich. 
% }





So far, we have evaluated purely the performance of the machine learning models on the dataset that we created for our algorithm selection problem. However, our initial hypothesis was that  machine learning based algorithm selection can solve the complex challenge of deciding when evaluation in the style of Yannakakis algorithm is preferable. We therefore now move on to evaluating the performance of the resulting full system that uses our trained algorithm selection models to decide when to rewrite, and thus execute queries using the predicted best query execution method. 


\change{
We first need to decide which of the two models, the decision tree model or the regression model, we pick for the experiments, since we aim to identify the best algorithm selection mechanism. However, looking at \Cref{tab:ml.performance_reg}, which shows the performance of the regression model, and \Cref{tab:ml.performance_class}, which shows the performance of the classification model, we see that the two are fairly comparable, with a slight advantage for the regression model. As we also saw, the regression-based approach is especially promising for real-world applications as it allows us to fine-tune the decision threshold. We thus choose the regression approach with a threshold at $0$ for the analysis presented in this section.

}
% \todo{probiert zu erklären warum man doch nicht regression gewählt hat}


% From the results of \Cref{sec:mlexp} we identified decision tree models as the preferred option for these experiments. \todo[inline]{ML: needs some short argument why, depending on the numbers we get in \Cref{tab:ml.performance}.}





As mentioned in the introduction, we will refer to this integrated method, that decides whether to rewrite to Yannakakis-style evaluation based on the prediction of the decision tree model as 
\ourTool.
In this section, we will perform all experiments only on the test set queries. \emph{No queries from these experiments were seen by the model at training time, inluding to select the best model.}
% \todo{\tiny ML: das wirkt irgendwie seltsam spaet um den Namen einzufuehren. Wenns geht gleich in die intro schon.}


At the most fundamental level, we are interested in improving overall query answering performance. We investigate this by analyzing the end-to-end (e2e) 
% \todo{\scriptsize ML: people will ask what time}
time necessary to answer all queries in the test set, \change{where ``end-to-end'' refers to taking the time of running all the benchmark queries and looking at it cumulatively. We note that this also includes the time for the algorithm selection included in SMASH, which was around 2 milliseconds end-to-end. }

% the entire process from input of a test query, to  reading the output. Hence this includes tasks such as reading the query, deciding on which optimization technique to perform, then performing it and returning the results.}
% \todo[inline]{ML: end-to-end in the experiments refers to taking the time of all the benchmarks. We should still note that these decision making times are included (ideally mention how little time it takes), but that is not what end-to-end means here.}

We summarize our analysis in \Cref{fig:main.dbperf}, where \textit{Base} refers to the baseline of executing the queries directly in the DBMS, \textit{Rewriting} refers to the time always using the rewriting to Yannakakis-style evaluation, and \ourTool refers to the use of our algorithm selection model as described above. To study the robustness of our approach we perform these experiments on three different DBMSs: PostgreSQL, Spark (via SparkSQL), and DuckDB. The significant technical differences between the three systems provides us with a way to study the performance of our method independently of specific DBMS technologies.
\change{  
We report timeouts as follows: if only one of the evaluation methods (rewriting or base case) times out, 
then we report in \Cref{fig:main.dbperf} for such queries as runtime the value of the timeout (= 100s).
\rp{Wir definieren nirgends, wie hoch wir das timeout gesetzt haben. Stimmt 15 min? Ansonsten bitte korrigieren.}
%
On the other hand, if both evaluation methods time out, we exclude them from the comparison, since algorithm selection cannot affect anything in such a case. 
Out of the 441 queries involved in the test set, there were 27 queries that timed out for both evaluation methods on  PostgreSQL, 
13 queries that timed out for both evaluation methods on  SparkSQL, and 30 queries that timed out for both evaluation methods on DuckDB.  
}



Consistently over all systems, we can observe a large improvement over both alternatives by using algorithm selection. Furthermore, we see that even always rewriting overall causes significant improvements over baseline execution of all three systems tested. However, this improvement comes from speedups specifically on queries that are hard for traditional RDBMS execution. These large improvements offset more common minor slowdowns using the \textit{Rewriting} approach. Using \ourTool we are able to get the best of both worlds, the major speedups without the minor slower cases. 

\change{We conclude that, for our key question \textbf{Q3} regarding the effect of the quality of algorithm selection on query evaluation 
times,  we can give a positive answer. 
Indeed, our algorithm selection clearly improves the e2e query evaluation times across a number of different queries and three different DBMSs. } 






To provide further insight into how e2e query evaluation performance is affected, \Cref{fig:slowdown} shows the percentage of queries in which \textit{Rewriting} and \textit{\ourTool} are slower than \textit{Base}. While the rewriting approach yields large improvements in e2e performance, it also causes minor slowdown of queries in over half of all cases. The data illustrates clearly that \ourTool provides a convincing solution to the problem, with minor slowdowns on only around 2\% of 
the queries, combined with even larger improvements in e2e performance.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{graphics/threshold-e2e.pdf}
%     \caption{Accuracy, precision and e2e runtime of the regression model converted to a classifier, depending on the threshold set (decision tree regression, PostgreSQL, all queries)}
%     \label{fig:threshold_e2e}
% \end{figure}


% \todo[color=cyan, inline]{Alex: Stimmt. Vielleicht kann man das umformulieren, sodass es besser in die section passt? Die Idee dahinter ist nicht, dass sich die e2e runtime verbessern soll, sondern dass sie sich kaum verschlechtert wenn man für precision optimiert. Allerdings passt der Absatz auch nicht wirklich in Section 6.1, weil wir dort noch nicht die e2e performance betrachten.}
% \co{Das wird in der Antwort auf Q4 doch addressiert. Ich bin dafür das hier auszukommentieren.}
% \todo[color=cyan, inline]{Alex: Achso, das habe ich übersehen. Nichts dagegen, den Teil zu verschieben. Es wird nur momentan in section 6.1 auf den Absatz in Section 6.2 verwiesen. Wie wäre es damit die Beispiele in Section 6.1 zu verschieben?}
% \change{We analyze how the approach for fine-tuning the threshold over the regression model, which was described in Section \ref{sec:mlexp} affects the e2e performance of the system. Figure \ref{fig:regr_threshold} shows how the threshold affects the accuracy, precision, recall, and e2e-runtime over the test set. We observe that it is indeed reasonable to make a tradeoff in favour of precision. For example, the threshold of -0.05 would result in a classifier with near-perfect 99\% precision, 95\% accuracy, and an e2e runtime of 1262.8s, close to the best runtime of 1262.5s. A threshold of -0.34 achieves perfect precision on the training set, while still having an accuracy of 83.4\%, and an e2e runtime of 1270.7s.
% }

% \rp{Der vorige Absatz ist etwas redundant: der trade-off zw. Prec und Rec wird schon in Section 6.1 besprochen. Hier sollten 
% wir nur noch die e2e runtime besprechen.}



\begin{figure}[t]
\begin{tikzpicture}
\begin{axis}[
    /pgfplots/bar cycle list/.style={/pgfplots/cycle list={%
                {brown!60!black,fill=brown!30!white,mark=none,thick},%
                {black,fill=gray,mark=none, thick},%
                },
    },
    ybar,
    bar width=15pt,
    enlarge x limits=0.3, 
    ymax = 70,
enlarge y limits = 0.05,
    legend style={
        at={(0.5,1.04)},
        anchor=south,
        legend columns=-1
    },
    width=8.5cm,
    height=4.5cm,
    ylabel={Slowdown cases (\%)},
    symbolic x coords={PostgreSQL,SparkSQL,DuckDB},
    xtick=data,
    nodes near coords={\pgfmathprintnumber\pgfplotspointmeta\%},
    % nodes near coords,
    nodes near coords align={vertical},
    ymajorgrids=true,
]

% first the data for rewriting based
\addplot coordinates {(PostgreSQL,59) (SparkSQL, 63) (DuckDB,63)}; % 262, 279, 282 out of 445

% then for ML based approach
\addplot coordinates {(PostgreSQL,2) (SparkSQL, 2) (DuckDB,3)}; % 9, 11, 15

\legend{Rewriting, SMASH}

\end{axis}
\end{tikzpicture}
\caption{Analysis of how often a query is slower than the base case when always rewriting to Yannakakis-style execution vs. when rewriting depending on our trained algorithm selection models (out of 441 queries in the test set).}
\label{fig:slowdown}
\end{figure}
\paragraph{Performance over different benchmarks.}

\change{
Up to this point, we have only considered the \emph{aggregated} test data, where we consider the performance over all benchmarks as a single data set. Obviously, it is critical that the model generalizes well to various databases and queries. Therefore, we also provide an analysis of the end-to-end performance of all
queries of each benchmark separately. For each benchmark, we show in \Cref{tab:e2e-breakdown}, the model's accuracy (Acc.), the e2e runtime achieved by the original evaluation method of the DBMS (e2e-orig), 
the e2e runtime achieved by SMASH (e2e SMASH, i.e, the runtime achieved with our decision method based on the learned classification), 
the ideal e2e runtime (e2e-opt, i.e., the runtime which could be achieved by always making the optimal choice between rewriting or not), and the number of queries of the benchmark in the test set (\#). From the results, we can observe that the model's accuracy varies only slightly between the benchmarks, and never falls below 94\%. Even more important for real-world performance, the e2e-runtime resulting from the algorithm selection is never worse than the original e2e-runtime. Additionally, the decision e2e-runtime is always very close to the ideal runtime, if not even the same. It is to be noted that, for the sake of completeness,
we have included LSQB in \Cref{tab:e2e-breakdown}, while the numbers in this row are clearly not representative of the performance over the LSQB benchmark, considering that it consists of a single query.
}



\paragraph{Significance of the performance improvement.}
The data presented in this section, in particular the runtimes reported in Figure~\ref{fig:main.dbperf},
clearly show that the decision procedure that decides which evaluation method to choose is quite effective. Nonetheless, for completeness sake we also wanted to perform a statistical significance test to fully ascertain that the results shown here are indeed due to the query optimization techniques we present, and cannot be dismissed as statistically insignificant. For this  purpose we compare the mean or median of the runtimes achieved when applying our decision procedure versus always using the original evaluation method of the DBMSs and always using Yannakakis-style evaluation, respectively.  
Since we compare runtimes obtained with the same test set, we need to use statistical tests, which take these dependencies into consideration. For the median we take the Wilcoxon sign-rank test~\cite{Wilcoxon1945} and for the mean we use a paired sample t-test~\cite{Ross2017}.
We report in 
\ifArxiv
\Cref{subsec:further_details_significance}
\else
the full version 
\fi
on this test, including explanations to the aforementioned tests themselves. For the sake of brevity
% allow for more focus on the most interesting parts of the experimental evaluation, 
we only state here that all computed p-values are well below the usual threshold of 0.005, thus clearly showing  that all presented results here are statistically significant. Revisiting our key question \textbf{Q3}, we may definitely refer to the performance improvement achieved 
by our algorithm selection method as significant.

\begin{table}[t]

\setlength{\tabcolsep}{2pt}
    \caption{Runtime breakdown by benchmark for PostgreSQL.}
    \label{tab:e2e-breakdown}
    \centering
    \begin{tabular}{l|llllll}
    \toprule
    \bf   & \bf \bf \multirow{3}{*}{\rotatebox[origin=t]{-45}{Accur.}} & \bf e2e &  \bf e2e & \bf e2e & \bf e2e & \bf  \\ 
    \bf Benchmark &   & \bf orig & \bf rewr & \bf SMASH & \bf opt & \bf \phantom{a} \# \\ 
     & & (sec) & (sec) & (sec) &   \\
    \midrule
         HETIO & 94.5\%     & 701   & 198 &  192  & 192 & 73\\
         STATS & 94.8\%     & 938   & 335 & 308  & 307 & 306 \\
         SNAP & 100\%       & 1978  & 733 & 731  & 731 & 21 \\
         JOB & 97.5\%       & 23    & 384 & 23   & 23 & 40 \\
         LSQB & 100\%       & 9     & 9 & 9    & 9 & 1 \\
        \bottomrule
    \end{tabular}

\end{table}


%
% \rp{Wir versprechen hier ``including explanations'' im Appendix bzw. in der full version. Im Moment haben wir nur 
% die Tabelle mit den Ginig scores im Appendix.}



% Decision vs. original:

% Postgres:

% Wilcoxon statistic: 267.0
% P-value: 1.10982211877866e-28

% T-statistic: 6.6271239310666195
% P-value: 1.0030705111552835e-10

% Spark:

% Wilcoxon statistic: 195.0
% P-value: 1.8418339200911663e-23

% T-statistic: 7.045064952308863
% P-value: 6.915157746237714e-12

% DuckDB:

% Wilcoxon statistic: 379.0
% P-value: 7.224802941778554e-22

% T-statistic: 5.716588417555759
% P-value: 2.0157227243842775e-08

% Decision vs. always rewriting:

% Postgres:

% Wilcoxon statistic: 762.0
% P-value: 1.543253453339765e-41
% T-statistic: 3.73224995822839
% P-value: 0.00021465950434351586

% Spark:

% Wilcoxon statistic: 1189.0
% P-value: 4.779570425238221e-48

% T-statistic: 5.30038749450123
% P-value: 1.8070149150875576e-07

% DuckDB:

% Wilcoxon statistic: 1820.0
% P-value: 2.1356776759768585e-42
% T-statistic: 4.295435587416319
% P-value: 2.150326974949211e-05


% {\em Generelle Vorgangsweise.}

% \begin{itemize}
%     \item 
% Vermutlich kann man nur fuer 1 DBMS Daten im main body angeben. Vielleicht 
% nehmen wir hier Postgres, weil wir ja beim VLDB-paper die Fixiertheit auf 
% relationale DBMS von manchen reviewern gespuert haben. 
% \item Und ausserdem koennen wir uns nicht 0MA {\em und} enumeration im main body leisten. Nehmen wir 0MA. Oder (Alex?) bitte schauen, welcher von den 6 cases ist fuer uns der, wo wir am besten dastehen?  Postgres oder DuckDB?  0MA oder enumeration? 
% (da SparkSQL weniger features erlaubt, ist das vielleicht nicht so günstig).
% \item Und wir haben besprochen, dass wir gleich die Ergebnisse fuer alle features (also nicht nur basic) zeigen.
% \rp{Aber bei Spark SQL ist es eventuell schon interessant, sowohl die Ergebnisse 
% fuer basic features als auch fuer basic+Postgres features zu zeigen.}
% %
% \item Die anderen Faelle (z.B. alles ausser Postgres mit all features und 0MA queries) werden wir vermutlich nur im Appendix Platz haben.
% \end{itemize}


% \medskip
% \noindent
% {\em Plan.} Ich schlage folgende Unterkapitel vor: 

% \begin{enumerate}
%     \item Runtime recordings and labeling of the data:  
%     hier muessen wir wohl zuallererst 
%     einen Ueberblick ueber die Laufzeiten geben. Aber jegliche Details kann man eh nur 
%     in den Appendix geben. Aber die labels und Statistiken (wie viele query/data-samples wurden mit 0, 1 bzw. equal klassifiziert? Und wie haben wir das labeling
%     fuer die regression gemacht? Also (im Fall von Postgres) 
%     im wesentlichen Fig. 8.3 und Table 8.1). 

%     \item Training of the models: 
%     hier kann man auch noch die log-transformation of feature values als 
%     Vorbereitung fuer's Training erwaehnen.
    
%     \item Results fuer die einzelnen model types (falls das Unterkapitel 
%     `´Training of the models'' nicht viel hergibt, kann man es ja  mit 
%      den Results zu einem gemeinsamen Unterkapitel verschmelzen).

%      \item Decision program: ev. den decision tree kurz vorstellen; mit den 
%      Gini-Koeffizienten der wichtigsten features. Und die 2 Signifikanztests 
%      (mean, median) fuer das decision program vs. original systems.
% \rp{Kriegen wir all diese Daten, Statistiken, ... fuer alle betrachteten Faelle zusammen? d.h.: 3 DBMS, 0MA und enumeration; und bei Spark SQL auch noch basic features vs. Postgres-features? Das kommt zwar eh nur in den Appendix. Aber irgendwie 
% sieht es trotzdem seltsam aus, wenn ein Fall fehlt.}

% \rp{Und die Signifikanz des decision programs sollten wir auch gegen 
% ``immer Yannakakis'' testen. }

% \rp{Interessant waere glaube ich schon, wenn wir etwas ueber Unterschiede bei den decision trees fuer die verschiedenen Faelle sagen koennten. Also zumindest Postgres mit 0MA vs. Postgres mit enumeration; aber natuerlich koennten auch Besonderheiten der DMBSs (falls es welche gibt) interessant sein).}
     
% \end{enumerate}



% \mbox{}
% \bigskip
% \bigskip\bigskip

% \noindent
% ***************************************************\\
% {\em Achtung}: Ab hier steht alter Text, der erst eingeordnet/ueberarbeitet/geloescht werden muss. \\
% ***************************************************\\



% We are now ready to train the various models with the test data obtained by applying our methodology to the selected ML model types.
% %
% Our data consists of the queries (each characterized by a feature vector) and their evaluation times both, for the 
% unchanged original DBMS and for the rewritten query to `enforce'' Yannakakis-style evaluation. 
    

% Based on the training and evaluation of ML models, 
% we choose various forms of decision trees as the "best" model, which we then apply on the untouched test set. For that we do quantitative analysis (metrics) and qualitative analysis (observe the misclassifications) similarly as before. 






% \input{edbt-rest}


% \todo[inline]{ML: As a final note, the figures need to placed a bit differently. One page is not essentially all figures and tables with a tiny bit of text inbetween. That is very confusing and the figures are far away from the text descirbing them. Mabye also \Cref{fig:threshold_e2e} and the other similar looking one are a bit too big, but with the legend and labels using too small a font. Also the fonts there are inconsitent with the rest of the figures.}




\section{Conclusion}
\label{sect:conclusion}

{\em Summary of results and some lessons learned.}
In this work, we have addressed the problem that optimization methods in query evaluation usually lead to 
a performance improvement in {\em some}, but not in {\em all}, situations. The optimization method at the
center of our investigation was 
Yannakakis-style query evaluation. We have formulated the question when to apply the optimization 
method and when to stick to the original evaluation method of the DBMS as an algorithm selection problem. 
By making use of ML-techniques, we have presented a methodology for solving this problem.
Our empirical evaluation has shown that decision trees (as a simple and easy to interpret) ML model type are very well suited for this task, as is witnessed by very high accuracy and precision values. 
The analysis of the decision trees obtained for the different DBMSs (and, in particular, of the Gini
coefficients of the selected features) has revealed interesting differences among the DBMSs. For instance, 
while estimates on the result size are most important for PostgreSQL, the decision in case of 
DuckDB is most strongly influenced by properties of the query such as query type (enumeration vs. simple aggregate query) and properties of the
chosen join tree. Statistical analysis of the runtimes obtained with the resulting decision procedure 
has verified that the selective application of the optimization technique (with decisions based on the
trained ML model) indeed may yield a significant performance gain. 




% \rp{Hmm - das ist halt jetzt die Frage, ob das zu lang ist fuer summary of results. Weil wir muessen unbedingt noch zumindest 2-3 Punkte zu future work sagen. Ich habe hier versucht, die 3 Questions Q1 - Q3 vom Beginn von Section 6 zu beantworten. Falls wir diese Antworten bereits in Section 6 geben wollen, dann wuerde ich nicht mehr ``Halb-Ueberschriften'' wie ``{\em Summary of results and some lessons learned.}'' hier schreiben.  }

\smallskip
\noindent
{\em Next steps.} Our work does not stop here. Given our aim of testing optimization methods, we believe our positive results here justify the complex task of integrating some of these methods, in particular with a focus on Yannakakis' algorithm, into real-word DBMSs. These optimization methods should then be used in combination with algorithm selection mechanisms, in the vein of the one presented here. 
As another research question, we also aim to consider a wider range of optimization techniques: since the structure of the underlying join tree clearly influences Yannakakis's algorithm when used for query evaluation, one may also investigate the use of machine learning methods to quickly determine optimal join trees, and as a next step even use them to heuristically search for optimal hypergraph decompositions, which would then allow one to use such optimization methods also on cyclic queries. 


% \clearpage
\ifArxiv
\begin{acks}
This work has been funded by the Vienna Science and Technology Fund (WWTF) [10.47379/ICT2201,10.47379/VRG18013].
\end{acks}
\fi

% \section*{Artifacts}

% Instructions, data and code for running the experiments, training the models, and analyzing the data, can be found in the following github repository: \url{https://github.com/dbai-tuw/yannakakis-rewriting}

% \co{Nach EDBT Vorlagen, muss es eine Section zu Artifacts geben, direkt vor den References. Diese zählt nicht gegen das pagelimit.

% `` The text of the paper must have a section named Artifacts right before the references, that provides explanation on access and use of all the artifacts for that paper. If no artifacts are available, the author should provide an explanation on that section. This section does not count on the submission page limit. ''

% }

%\clearpage

\bibliographystyle{ACM-Reference-Format}
% \balance
\bibliography{main}


% \rp{Der Appendix ist noch eine riesen Baustelle. Spricht sehr dafuer, dass wir im Moment einmal keine arxiv Version versprechen. Dann koennen wir nach und nach Versionen mit verbessertem Appendix hochladen.}

\clearpage

\appendix

% \rp{Vielleicht ist der Appendix in gar keinem so schlechten Zustand. Jedenfalls muss man die Tabelle mit den Hyperparameters erklaeren. Aber vielleicht sollten wir zusaetzlich zu Section 6 noch etliche Zahlen bzgl. evaluation im Appendix dazugeben, um zu illustrieren, wie toll wir das alles evaluiert haben?

% }
\section{Further Information for Section~\ref{sect:DataAugmentation}}
\label{app:dataAugmentation}

In this section we aim to give further details into the data augmentation. In \Cref{ex:data_augment1}, we have
illustrated the filter augmentation. We continue here by adding another example to illustrate enumeration augmentation.
% one of the queries from that example, termed simply $q_{\mathit{aug}1}$ and showcase a number of possible  enumeration augmentations that can be performed on top of it.

% Below, we illustrate the enumeration augmentation step by 
% revisiting the STATS query 005-24. As with the aggregate augmentation step in case of 
% 0MA queries, also the enumeration augmentation step is applied to the original 
% queries from the benchmarks and to the filter augmented queries alike. 
% Below, we apply enumeration augmentation to a filter-augmented version of 
% STATS query 005-24: 


\begin{example} \label{ex:data_augment3}


Consider the query $q_{\mathit{aug}1}$, given in \Cref{ex:data_augment1}. We present three different enumeration augmentations, whereby the query output is transformed from producing a single output to \emph{enumerating} a number of rows. 
% where the first one is the "original" query and the other two are the augmented ones. 
One option to ``enum-augment'' $q_{\mathit{aug}1}$ is to swap  the aggregation function \mintinline[escapeinside=||,fontsize=\small]{sql}{MIN(v.Id)}, with a projection on the {\tt Id} column of the {\tt users} table, and the {\tt UserID} column of the {\tt votes} table producing the query $q^{\mathit{e}1}_{\mathit{aug}1}$. Other options, shown below, lead to the queries $q^{\mathit{e}2}_{\mathit{aug}1}$ and $q^{\mathit{e}3}_{\mathit{aug}1}$.

{
\begin{minted}[escapeinside=||,fontsize=\small,fontfamily=tt]{sql}

|{\color{gray}$q_{\mathit{aug}1}$:}|  SELECT MIN(u.Id) 
       FROM   votes AS v, badges AS b, users AS u 
       WHERE  u.Id = v.UserId AND v.UserId = b.UserId 
              AND v.BountyAmount>=40 AND v.BountyAmount<=50 
              AND u.DownVotes=0
              
|{\color{gray}$q^{\mathit{e}1}_{\mathit{aug}1}$:}|  SELECT |\tt{\textbf{u.Id, v.UserId}}| 
         FROM  votes AS v, badges AS b, users AS u 
         WHERE u.Id = v.UserId AND v.UserId = b.UserId 
               AND v.BountyAmount>=40 AND v.BountyAmount<=50 
               AND u.DownVotes=0
               
|{\color{gray}$q^{\mathit{e}2}_{\mathit{aug}1}$:}| SELECT |\tt{\textbf{u.Id, b.UserId}}| 
         FROM  votes AS v, badges AS b, users AS u 
         WHERE u.Id = v.UserId AND v.UserId = b.UserId 
               AND v.BountyAmount>=40 AND v.BountyAmount<=50 
               AND u.DownVotes=0
               
|{\color{gray}$q^{\mathit{e}3}_{\mathit{aug}1}$:}| SELECT |\tt{\textbf{v.UserId, b.UserId}}|
         FROM  votes AS v, badges AS b, users AS u 
         WHERE u.Id = v.UserId AND v.UserId = b.UserId 
               AND v.BountyAmount>=40 AND v.BountyAmount<=50 
               AND u.DownVotes=0
\end{minted}
}




\end{example}





\section{Details on the Query Rewriting}
\label{app:query_rewriting_example}


\begin{figure}[t]
  \centering 
   % \vspace{4mm}
    \includegraphics[width=.45\textwidth]{graphics/results/implementation_scala_small.png} 
  \caption{Workflow for rewriting the queries.}  
  \label{fig:rewriting}  
  % \vspace{4mm}
\end{figure} 

In this section we aim to give the reader technical insights into the query rewriting process, where we follow the approach of~\cite{DBLP:journals/corr/abs-2303-02723}.
This approach involves rewriting each query into an equivalent sequence of SQL queries (i.e. one that gives the same result) but which ``forces'' (or, at least, guides)  
the DBMSs to apply a Yannakakis-style evaluation strategy.
The rewrite process is visualized in Figure~\ref{fig:rewriting}.




We thus take a SQL query as input and first transform it into a hypergraph. 
The approach by \cite{DBLP:journals/corr/abs-2303-02723} begins by applying the GYO-reduction~\cite{DBLP:conf/compsac/YuO79}.  One thus verifies that the CQ is acyclic and, if so, constructs a join tree. \change{This approach does not support cyclic queries, since they do not permit join trees.}
% In case the query is cyclic, the rewriting fails.
In case of 0MA queries, containing an aggregate expression with a single attribute, one takes the relation of this single  attribute as the root of the join tree, as this allows to significantly simply the Yannakis' algorithm by skipping multiple traversals of the join tree, thus allowing for a more efficient evaluation. The rewriting proceeds by creating a sequence of SQL queries. One traverses the tree in a bottom-up fashion, starting from the leafs, and produces auxillary tables via {\tt{\color{OliveGreen}\textbf{CREATE VIEW}}}, for the leaf nodes, and {\tt{\color{OliveGreen}\textbf{CREATE UNLOGGED TABLE}}}, for the inner nodes of the join tree.
For 0MA queries the aggregation is performed last, after the creation of auxiliary tables.
% For the STATS query 005-024, which we already used as example in 
% Section~\ref{sect:DataAugmentation} to illustrate the data augmentation process, 
% the query before and the sequence of queries after the rewriting looks as follows:

We illustrate the query rewrite process by revisiting the 0MA query $q$ from \Cref{ex:data_augment1}.
% the sequence of SQL queries produced as part of this process are as follows.
To make it easier to follow, we provide the join tree of $q$ in \Cref{fig:tpch-query-jointree-app} and also repeat the definition of $q$ itself.


\begin{figure}
    \centering
    \scalebox{1}{
    \begin{forest}
    for tree={align=center}
        [{\texttt{users} % \\ $\{\text{s\_suppkey, s\_nationkey, s\_acctbal}\}$
        }            
            [{\texttt{badges} % \\ $\{\text{s\_nationkey, n\_regionkey}\}$
            }]
            [{\texttt{votes} % \\ $\{\text{p\_partkey, s\_suppkey}\}$
            }]
        ]
    \end{forest}
    }
    \vspace{-5mm}
    \caption{Join tree for $q$.}
    \label{fig:tpch-query-jointree-app}
    \vspace{-5mm}
\end{figure}




\begin{minted}[escapeinside=||,fontsize=\small,fontfamily=tt]{sql}

|\color{gray}$q$|: SELECT MIN(u.Id) 
        FROM votes as v, badges as b, users as u 
        WHERE u.Id = v.UserId AND v.UserId = b.UserId 
             AND v.BountyAmount>=0 AND v.BountyAmount<=50 
             AND u.DownVotes=0

|\color{gray}$q_{\mathit{r}1}$|: CREATE VIEW E3 AS SELECT * 
        FROM users AS users
        WHERE users.DownVotes = 0
        
|\color{gray}$q_{\mathit{r}2}$|: CREATE VIEW E2 AS SELECT *
        FROM badges AS badges
        
|\color{gray}$q_{\mathit{r}3}$|: CREATE |\tt{\color{OliveGreen}\textbf{UNLOGGED}}| TABLE E3E2 AS SELECT *
        FROM E3 WHERE EXISTS (SELECT 1
                    FROM E2 
                    WHERE E3.Id=E2.UserId)
                    
|\color{gray}$q_{\mathit{r}4}$|: CREATE VIEW E1 AS SELECT * 
        FROM votes AS votes
        WHERE CAST(votes.BountyAmount AS INTEGER) >= 0 
             AND CAST(votes.BountyAmount AS INTEGER) <= 50
             
|\color{gray}$q_{\mathit{r}5}$|: CREATE |\tt{\color{OliveGreen}\textbf{UNLOGGED}}| TABLE E3E2E1 AS
        SELECT MIN(Id) AS EXPR$0
        FROM E3E2 WHERE EXISTS (SELECT 1
                    FROM E1
                    WHERE E3E2.Id=E1.UserId)
                    
|\color{gray}$q_{\mathit{r}6}$|: SELECT * FROM E3E2E1
\end{minted}


% \rp{Was hat es mit diesen UNLOGGED tables auf sich? Ist das PostgreSQL spezifisch?}
% \todo[color=cyan, inline]{Alex: UNLOGGED TABLE ist quasi ein effizienterer TEMP TABLE, weil kein WAL erzeugt wird}
% \rp{Man muss an dem Beispiel noch ein bisschen besser erklaeren, was hier genau abgeht.
% Dann kann man ev. auch die Idee der UNLOGGED TABLEs in einem Satz erwaehnen.}

In the rewriting of the example query, we can see that users and badges are joined first, even if they did not have an equality condition together in the original query. This is due to the fact that the three relations are joined on the same attribute and, after 
the replacement of equi-joins by natural joins, the distinction between explicit and implicit equality condition is irrelevant (and also not visible anymore). The use of {\tt{\color{OliveGreen}\textbf{UNLOGGED TABLE}}} is an optimization to guide the DBMS to avoid writing the intermediate views to disk.

We note that the rewriting also produces a sequence of DROP statements to delete the created tables after the evaluation.

% \nop{
% The implementation makes use of Apache Calcite~\cite{DBLP:conf/sigmod/BegoliCHML18}, a modular framework for query optimization, to perform the transformation of a single SQL query over a database to multiple queries expressing the Yannakakis-execution. Schema information, which is required to transform the query into a logical plan, is extracted directly from the DBMS over the JDBC interface. The logical plan resulting from the SQL string is next transformed into a convenient intermediate representation via query optimizer rules, where e.g., all filters are pushed to the bottom of the tree, which allows us to retrieve a set of subtrees, corresponding to the hyperedges, followed by a tree of equi-joins, from which the hypergraph can be constructed.}

% \rp{Haben wir ein konkretes Beispiel fuer das rewriting einer enumeration query? In diesem Fall muessen wir 
% ja alle 3 traversals des join trees in das rewriting einfliessen lassen. Das ist zu viel fuer den main body. Aber im Appendix waere es schoen }

% \todo[color=cyan, inline]{Alex: Ich konnte die Daten leider nicht mehr finden, sowohl im github als auch am Server. Vielleicht ist es noch im output von irgendeinem alten Notebook am Server. Wenn später noch Zeit bleibt kann ich probieren es neu laufen zu lassen.}





\section{Further Information for Section \ref{sect:FeatureSelection}}
\label{app:FeatureSelection}

In this section, we aim to illustrate the complete feature vector for two example queries from our data set, namely ``{\tt HETIO\_2-01-CbGaD}'', termed $q_1$ and ``{\tt HETIO\_3-06-CdGuCtD}'' termed $q_2$, shown below.


 % For better illustration of the features, we give two examples together with all of the features: Q1: HETIO\_2-01-CbGaD, Q2:.


\begin{minted}[escapeinside=||,fontsize=\small,fontfamily=tt]{sql}

|{\color{gray}$q_1$:}| SELECT MIN(c.nid) 
    FROM   compound c, binds b, gene g, associates a, disease d 
    WHERE  c.nid = b.sid AND b.tid = g.nid AND
           g.nid = a.tid AND a.sid = d.nid

|{\color{gray}$q_2$:}| SELECT MIN(c1.nid) 
    FROM   compound c1, downregulates d1, gene g,  
           upregulates u2, compound c2, treats t, disease d    
    WHERE  c1.nid = d1.sid AND d1.tid = g.nid AND
           g.nid = u2.tid AND u2.sid = c2.nid AND
           c2.nid = t.sid AND t.tid = d.nid
\end{minted}


Their calculated join trees are given in Figures~\ref{ex_jointree1} and~\ref{ex_jointree2}, respectively. 
Recall from \Cref{sect:FeatureSelection}
that some of the features of the queries are determined by the join tree.
% \co{Is it not rather the case that the chosen join tree \emph{determines} the features? If so, then this sentence is a bit misleading.}
In Table~\ref{features}, the values of all features of these two queries are given.
For the set-based features, we also explicitly show the six statistical data points that are extracted from the set to produce the feature vector used for the models to be trained.

\begin{figure}[t]
  \centering 
    \includegraphics[width=0.4\textwidth]{graphics/results/ex_jointree1.PNG} 
  \caption{Join tree of $q_1$.}  
  \label{ex_jointree1}  
\end{figure} 

\begin{figure}[t]
  \centering 
    \includegraphics[width=0.45\textwidth]{graphics/results/ex_jointree2.PNG} 
  \caption{Join tree of $q_2$.}  
  \label{ex_jointree2}  
\end{figure} 











% Additionally, we provide all values of the container counts, branching degrees, table rows, join rows and cardinality in italic to give an insight into how these features work.

% Be aware that these features are never used all together for one model. The model construction and which features are used, is explained in the next section.


\section{Further Information for Section \ref{sect:SolvingAlgorithmSelectionProblem}}
\label{app:SolvingAlgorithmSelectionProblem}

% \co{Diese Section braucht \emph{dringend} mehr infos um die Tabelle lesen zu können. Was sind etwa "leaning rate", "max pooling", "in-5-out",etc Platz spielt hier keine Rolle, bitte {\bf jeden} Begriff der in der Tabelle  aufscheint erklären. }

We provide here a list of all hyperparameter values for the various model types considered here, i.e., 
three types of neural network models, k-NN, random forests, and SVM, and decision trees. The details are given in \Cref{tab:hyperparameters}. The experiments presented here are a continuation of the work in \cite{bohm2024rewrite}, to which we can refer for even more detailed explanations of the models and hyperparameters, as well as further literature on the machine learning concepts. In cases where mutiple hyperparameters are mentioned (such as 'kernel=linear/poly/rbf'), we have experimented with all of these. We provide a short descrition of the most important hyperparameters:

\rp{Ich wuerde den Verweis auf die Diplomarbeit (die fuer Aussenstehende auch gar nicht so leicht zu finden ist) lieber vermeiden.}
\co{Die Master thesis hat eine DOI, die ist auch im Zitat. Das es nicht leicht zu finden ist sehe ich nicht ein. Eine URL klicken zu müssen ist ja zumutbar. }
\co{Aber etwas seltsam find ich die Bemerkung schon (und nicht allein wegen dem Zitat) "is based on experiments from [10]": warum ist das nicht auf unseren Experimenten basiert? Vielleicht besser zu schreiben, dass unsere Experimente diese weiterführen, und man daher auf [10] verweisen ``darf''}
\todo[color=cyan, inline]{Alex: Ich habe den Satz jetzt, wie von Cem vorgeschlagen, etwas umformuliert}

 
\begin{description}
    \item[k-NN] k is the number of neighbors used by the model
    \item [Random forest] The number of estimators refers to the number of individual decision trees making up the forest
    \item [SVM] The kernel of the SVM is the function which defines the decision boundary.
    \item [MLP, HGNN, MLP+HGNN] For classification, we used cross-entropy loss, and for regression MSE (mean-squared-error). The loss function is used in the training of the model (gradient descent) to compare the actual output to the desired output. The \textit{batch size} is the number of samples used in a single training step.
    The number of \textit{epochs} refers to the number of steps used in training. \textit{Learning rate} is a parameter controlling how "fast" gradient descent adapts the weights of the model.

    The layer-configurations of the neural network models, such as 'in-40-10-out', describe how the nodes are arranged in the layers of the models. In this example, 'in' is the input layer, of in most cases 30 features, and '40' and '10' refer to the sizes of the intermediate layers. Layers are always fully connected, with ReLU activation functions.
    
\end{description}


\clearpage 



\begin{table*}[t] 
\caption{A  complete overview of the features for the queries $q_1$ and $q_2$. We refer to \Cref{sect:FeatureSelection} for an explanation of each feature. Recall that each feature that consists of a set of numbers is indicated by the use of an asterix in its name e.g. B7$^*$. For these set-based features we also state the statistical data that is used in the training to produce a fixed-width vector. } 
\label{features} 
% \small
\centering 
    % \resizebox{\columnwidth}{!}{
    { 
\begin{tabular}{lcc} 
     \toprule 
      \bf Feature & $\mathbf{q_1}$ & $\mathbf{q_2}$ \\ 
      \midrule 
      {\bf B1}:\phantom{$^*$} is 0MA? & 1 & 1 \\ 
      {\bf B2}:\phantom{$^*$} number of relations & 5 & 7 \\ 
      {\bf B3}:\phantom{$^*$} number of conditions & 4 & 6 \\ 
      {\bf B4}:\phantom{$^*$} number of filters & 0 & 0  \\ 
      {\bf B5}:\phantom{$^*$} number of joins & 4 & 6 \\ 
      {\bf B6}:\phantom{$^*$} depth & 2 & 3 \\ 
      {\bf B7}$^*$: {container counts} & $\{$ \textit{1, 1, 1, 1, 1, 2, 3} $\}$ & $\{$ \textit{1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3} $\}$\\
      &   \begin{tabular}{|r|c|}
      \hline
        min &  1 \\
        max & 3  \\
        mean & 1.43 \\ 
        q25 & 1 \\ 
        med.& 1 \\ 
        q75 & 1.5 \\
        \hline
        \end{tabular} & 
  \begin{tabular}{|c|c|} 
        \hline
        min &  1 \\
        max & 3  \\
        mean & 1.27 \\ 
        q25 & 1 \\ 
        med.& 1 \\ 
        q75 & 1 \\
        \hline
        \end{tabular} \\
      
      % min(container counts) & 1 & 1 \\ 
      % max(container counts) & 3 & 3 \\ 
      % mean(container counts) & 1.43 & 1.27 \\ 
      % q25(container counts) & 1 & 1 \\ 
      % median(container counts) & 1 & 1 \\ 
      % q75(container counts) & 1.5 & 1 \\ 
      {\bf B8}$^*$: {branching degrees} & $\{$ \textit{3, 1} $\}$ & $\{$ \textit{2, 3, 1} $\}$ \\ 

   &   \begin{tabular}{|r|c|}
      \hline
        min &  1 \\
        max & 3  \\
        mean & 2 \\ 
        q25 & 1.5 \\ 
        med.& 2 \\ 
        q75 & 2.5 \\
        \hline
        \end{tabular} &  \begin{tabular}{|c|c|} 
        \hline
        min &  1 \\
        max & 3  \\
        mean & 2 \\ 
        q25 & 1.5 \\ 
        med.& 2 \\ 
        q75 & 2.5 \\
        \hline
        \end{tabular}  \\
      
      % min(branching degrees) & 1 & 1 \\ 
      % max(branching degrees) & 3 & 3 \\ 
      % mean(branching degrees) & 2 & 2 \\ 
      % q25(branching degrees) & 1.5 & 1.5 \\ 
      % median(branching degrees) & 2 & 2 \\ 
      % q75(branching degrees) & 2.5 & 2.5 \\ 
      % PSQL: total cost & 1175.3 & 10283.64 \\ 
     {\bf P1}:\phantom{$^*$} estimated total cost & 1175.3 & 10283.6 \\
      
      {\bf P2}$^*$: {estimated single table rows} & $\{$ \textit{23142, 25246, 137,1, 1552} $\}$ & $\{$ \textit{154076, 1552, 146276, 1510, 137, 1552, 20945} $\}$    \\ 

 &   \begin{tabular}{|r|c|}
      \hline
        min &  1 \\
        max & 25246  \\
        mean & 10015.6 \\ 
        q25 & 137 \\ 
        med.& 1552 \\ 
        q75 & 23142 \\
        \hline
        \end{tabular} & \begin{tabular}{|c|c|} 
        \hline
        min &  137 \\
        max & 154076  \\
        mean & 46578.29 \\ 
        q25 & 1531 \\ 
        med.& 1552 \\ 
        q75 & 83610.5 \\
        \hline
        \end{tabular}  \\


      
      % PSQL: min(table rows) & 1 & 137 \\ 
      % PSQL: max(table rows) & 25246 & 154076 \\ 
      % PSQL: mean(table rows) & 10015.6 & 46578.29 \\ 
      % PSQL: q25(table rows) & 137 & 1531 \\ 
      % PSQL: median(table rows) & 1552 & 1552 \\ 
      % PSQL: q75(table rows) & 23142 & 83610.5 \\ 


      
      {\bf P3}$^*$: {estimated join rows} & 
      $\{$ {\textit{1190, 2361, 626, 626}} $\}$& 
      $\{$ \textit{493338, 22993, 2578, 2578, 446, 446}  $\}$ \\

 &  \begin{tabular}{|r|c|}
      \hline
        min &  626 \\
        max & 2361  \\
        mean &  1200.75\\ 
        q25 & 626 \\ 
        med.& 908 \\ 
        q75 & 1482.75 \\
        \hline
        \end{tabular} &  \begin{tabular}{|c|c|} 
        \hline
        min &  446 \\
        max & 493338  \\
        mean & 87063.17 \\ 
        q25 & 979 \\ 
        med.& 2578 \\ 
        q75 & 17889.25  \\
        \hline
        \end{tabular} \\
      
      % PSQL: min(join rows) & 626 & 446 \\  
      % PSQL: max(join rows) & 2361 & 493338 \\  
      % PSQL: mean(join rows) & 1200.75 & 87063.17 \\  
      % PSQL: q25(join rows) & 626 & 979 \\  
      % PSQL: median(join rows) & 908 & 2578 \\  
      % PSQL: q75(join rows) & 1482.75 & 17889.25 \\  

      
      {\bf D1}$^*$: {estimated cardinalities} & 
      $\{$ \textit{31829, 29051, 26334, 26334,}   &
     $\{$ \textit{4993749, 354275, 338789, 322069,2299, }\\

      & \textit{418, 2299, 29051, 24035,26334} $\}$ & 
      \textit{2299, 338789, 2090, 24035,2299, 322069, 418, 2090} $\}$ \\
 &  \begin{tabular}{|r|c|}
      \hline
        min &  418 \\
        max & 31829  \\
        mean &  21742.78 \\ 
        q25 & 24035 \\ 
        med.& 26334 \\ 
        q75 & 29051 \\
        \hline
        \end{tabular} &  \begin{tabular}{|c|c|} 
        \hline
        min &  418 \\
        max & 4993749  \\
        mean & 515790 \\ 
        q25 & 2299 \\ 
        med.& 24035 \\ 
        q75 & 338789  \\
        \hline
        \end{tabular} \\
      
      % DDB: min(cardinality) & 418 & 418 \\ 
      % DDB: max(cardinality) & 31829 & 4993749 \\ 
      % DDB: mean(cardinality) & 21742.78 & 515790 \\ 
      % DDB: q25(cardinality) & 24035 & 2299 \\ 
      % DDB: median(cardinality) & 26334 & 24035 \\ 
      % DDB: q75(cardinality) & 29051 & 338789 \\ 
      \bottomrule 
    \end{tabular} 
}
  \vspace{3cm}
\end{table*}

\clearpage





\begin{table*}[t]
\caption{ An overview of all hyperparameters of the models used during training. For the MLP model, the meaning of ``in'' in the layer column refers to the number of features, and the world ``out'' refers to the number of classes (2,3 and 1 for regression). 
% Hyperparameters. in = number of features, out = number of classes  Loss: Cross-Entropy for classification, MSE for regression
} 
\label{tab:hyperparameters} 
\centering 

\begin{tabular}{ccc} 
     \toprule
    \textbf{model} & \textbf{hyperparameter} & \textbf{layer} \\ 
    \midrule
    k-NN & k=5 & -  \\ 
    \midrule
    Decision tree & - & - \\ 
    \midrule
    Random forest & n\_estimators=100 & - \\
    
    \midrule
    SVM & kernel=linear/poly/rbf & -\\ 
    
    \midrule
    \multirow{20}{*}{MLP} & \multirow{20}{*}{\parbox{4.5cm}{\centering Loss=Cross-Entropy/MSE \\ Batch size=100 \\ Epochs=300 (saving best model) \\ Learning rate=0.1}} & in-5-out\\ 
     & & in-10-out\\ 
     & & in-20-out\\ 
     & & in-25-out\\ 
     & & in-40-out\\ 
     & & in-60-out\\ 
     & & in-10-5-out\\  
     & & in-20-10-out\\ 
     & & in-40-20-out\\ 
     & & in-40-10-out\\ 
     & & in-60-40-out\\ 
     & & in-60-20-out\\ 
     & & in-80-50-out\\ 
     & & small median, best MLP\\ 
     & & small mean, best MLP\\ 
     & & small min, best MLP\\ 
     & & small max, best MLP\\ 
     & & small q25, best MLP\\ 
     & & small q75, best MLP\\ 
     & & custom, best MLP\\ 
     
    \midrule
    \multirow{5}{*}{HGNN} & \multirow{5}{*}{\parbox{4.5cm}{\centering Loss=Cross-Entropy/MSE \\ Epochs=100 (saving best model) \\ Learning rate=0.001 \\ Max-Pooling}} & kernel 3x3, 1-16-32-out\\ 
     & & kernel 3x3, 1-32-16-out\\ 
     & & kernel 3x3, 1-16-32-16-out\\ 
     & & kernel 3x3, 1-32-64-out\\ 
     & & kernel 3x3, 1-4-16-out\\ 
    \midrule
    \multirow{5}{*}{combined} & \multirow{5}{*}{\parbox{4.5cm}{\centering Loss=Cross-Entropy/MSE \\ Epochs=100 (saving best model) \\ Learning rate=0.001 \\ Max-Pooling}} & best MLP-2/best HGNN-2/4-out \\  
     &  & best MLP-5/best HGNN-5/10-out \\  
     &  & best MLP-5/best HGNN-5/10-20-out \\  
     &  & best MLP-10/best HGNN-10/20-40-2 \\  
     &  & best MLP-10/best HGNN-10/20-60-20-2 \\ 

    \bottomrule
    \end{tabular} 
    

\end{table*} 





\clearpage















\section{Further Details on the Experimental Evaluation}
\label{app:AdditionalExp}

% \rp{I think we promise much more details on the experimental results: whenever we restrict our presentation in Section 6 to a single DBMS, we should show the analogous results for the other DBMSs in the Appendix. Also the Gini coefficients could probably use some explanation that go beyond just showing the table.}
% \co{I have restored the detailed write-up of the significance test, and explained that \Cref{tab:giniext} expands on something shown and explained in the main body.}

We present here further information on the experiments conducted in \Cref{sect:evaluationAlgorithmSelection}. 

\paragraph{Full details on Gini coefficients.} Recall that in \Cref{sect:evaluationAlgorithmSelection} we mention the role of Gini coefficients and list only the five features with the highest coefficients for PostgreSQL and DuckDB.  As was mentioned, the Gini coefficient, as used in this paper, measures the contribution of the feature to the outputs of the model.
In \Cref{tab:giniext} we then give a complete overview of the 8 highest Gini coefficients among the features of the trained decision tree models for all three DBMSs, including SparkSQL.


% we produced as part of our experiments in \Cref{sect:evaluationAlgorithmSelection}



\begin{table}[t]
\setlength{\tabcolsep}{3pt}

\small
\centering
\caption{Most important features according to Gini coefficients of the Decision Tree models.}
\label{tab:giniext}
\begin{tabular}{l||lc}
\toprule
% \multicolumn{2}{c}{PostgreSQL}  \\
\parbox[t]{2mm}{\multirow{9}{*}{\rotatebox[origin=c]{90}{PostgreSQL}}}  &
\textbf{Feature} & \textbf{Gini} \\
\midrule
&max(est join rows) & 0.369 \\
&is 0MA? & 0.157  \\
&q75(est. sing. table rows)  & 0.069 \\
&total cost & 0.053 \\
&min(est. sing. join rows) & 0.051  \\
&q25(est. sing. join rows)  & 0.044 \\
&median(est. join rows) & 0.033  \\
&q25(est. single. table rows) & 0.033 \\
% \bottomrule
\end{tabular}
\begin{tabular}{l||lc}
\toprule
\parbox[t]{2mm}{\multirow{9}{*}{\rotatebox[origin=c]{90}{DuckDB}}}  &
\textbf{Feature} & \textbf{Gini}\\
\midrule
&is 0MA? & 0.280  \\
& mean(cont. c.) & 0.206  \\
& max(est. card.) & 0.147  \\
&q25(est. card.) & 0.061  \\
&q75(est. card.) & 0.057  \\
&median(est. card.) & 0.047  \\
& mean(est. card.) & 0.040 \\
& min(est. card.) & 0.037 \\
% \bottomrule
\end{tabular}


\begin{tabular}{l||lc}
\toprule
\parbox[t]{2mm}{\multirow{9}{*}{\rotatebox[origin=c]{90}{SparkSQL}}}  &
\textbf{Feature}  & \textbf{Gini}\\
\midrule
&is 0MA? & 0.246 \\
&max(est. sing. join rows) & 0.212 \\
&\#joins & 0.135 \\
& est. total cost & 0.092 \\
&q25(est. sing. table rows)  & 0.053 \\
& median(est. sing. table rows) & 0.044 \\
&max(est. sing. table rows)  & 0.037 \\
& mean(cont. c.) & 0.032 \\
\bottomrule
\end{tabular}


\end{table}


\paragraph{Further Details on the Significance Tests.}
\label{subsec:further_details_significance}
We briefly commented in \Cref{sect:evaluationAlgorithmSelection} on the significance tests we performed. Here we give a more detailed report, including explanations on how these tests are defined.

% The data presented in the previous section, in particular the runtimes reported in Figure~\ref{fig:main.dbperf},
% suggest that the decision procedure that decides which evaluation method to choose is quite effective. 
% We now want to put conclusions on the significance of the performed improvement achieved by this decision procedure 
% on firm ground by additionally performing two statistical tests. 

For our significance tests, we compare the mean or median of the runtimes achieved
when applying our decision procedure versus always using the original evaluation method of the DBMSs and
always using Yannakakis-style evaluation, respectively. 

Since we compare runtimes obtained with the same test set, we need to use statistical tests, which take these dependencies into consideration. For the median we take the Wilcoxon sign-rank test~\cite{Wilcoxon1945} and for the mean we use a paired sample t-test~\cite{Ross2017}.
% \rp{Wir brauchen noch ein Zitat fuer den paired sample t-test!}

\smallskip
\noindent
\textit{Wilcoxon sign-rank test.} The null hypothesis of this test for two (dependent) groups A and B is that the medians are equal: $H_0: median(A) = median(B)$. To get the test statistic the differences between all pairs of group A and B are calculated and ranked. Additionally, the sign of the difference is used, so that all ranks of the positive differences are summed and the same for the negative ones. The minimum of these two is the test statistic, which then can be compared to the Wilcoxon signed rank table to get the p-value. If the p-value is smaller than a chosen alpha-level, the null can be rejected and the two cases lead to significantly different medians.

\medskip
\noindent
\textit{Paired sample t-test.} The null hypothesis of this test for two (dependent) groups $A$ and $B$ is that the means are equal: $H_0: mean(A) = mean(B)$. Again, the differences of the pairs of values are used to calculate the test statistic. In this case it is a t-test statistic with $n-1$ degrees of freedom and looks like the following.
\begin{align*}
t=\frac{\bar{d} \sqrt{n}}{s}, \qquad \text{with } \bar{d} = \frac{1}{n} \sum\limits_{i=1}^n d_i, s=\sqrt{\frac{1}{n-1} \sum\limits_{i=1}^n (d_i-\bar{d})^2} 
\end{align*}
Here the t-test tables can be used to get the p-value and again if it is smaller than alpha, the null can be rejected and we can conclude that the means are significantly different.


For both tests, we choose 0.1 as alpha value as a common choice. 

In Table~\ref{tab:signifianceVsOriginalEvaluation}, we have the p-values for the 
Wilcoxon sign-rank test (Wilcoxon s.-r.) in the left group of 2 columns with numbers and for the 
paired sample t-test (p.s. t-test) in the 
right-most  group of 2 columns. Inside each group, the first column compares the 
median resp. mean of the runtimes obtained by the original evaluation method (Orig) of each DBMS with 
the evaluation method chosen by the decision procedure (Dec). The second column of each group
compares the
median resp. mean of the runtimes obtained by the rewritten queries (Rewr, i.e., by Yannakakis-style query  evaluation) with 
the evaluation method chosen by the decision procedure (Dec). 
We can see that all p-values are extremely small. For any reasonable choice of alpha value (note that
alpha values such as 0.1 or 0.05 are common choices; the p-values in the table are much smaller) we can reject the null hypothesis in all cases. This means that the decision procedure indeed yields a 
significant performance improvement (in terms of means and median of runtimes). 

% \todo[inline]{ML: \Cref{tab:signifianceVsOriginalEvaluation} in its current form does not illustrate any point to the reader. It provides only raw data that is then left to the reader to process. Instead the presentation of the data should already illustrate the final conclusion of what the raw data says. We can also not justify a full column of the experiments section to give general information of established methods. It looks like we have run out of our own things to say.}
\begin{table}[t]

\setlength{\tabcolsep}{3pt}
\caption{p-values resulting from the Wilcoxon sign-rank test and paired sample t-test, comparing the 
original evaluation method of each DBMS with the decision program.}
\label{tab:signifianceVsOriginalEvaluation}
\centering

\begin{tabular}{c|c|c}
\toprule
& \multicolumn{2}{c|}{\textbf{Median (Wilcoxon s.-r.)}}\\ \midrule
\textbf{System} & \textbf{Orig/SMASH} & \textbf{Rewr/SMASH}  \\ \midrule
PostgreSQL & $1.1\cdot10^{-28}$ & $1.5\cdot10^{-41}$  \\
Spark & $1.8\cdot10^{-23}$ & $4.8\cdot10^{-48}$  \\
DuckDB & $7.2\cdot10^{-22}$ & $2.1\cdot10^{-42}$ \\

% \bottomrule

\end{tabular}


\begin{tabular}{c|c|c}
\toprule
& \multicolumn{2}{c}{\textbf{Mean (p.s. t-test)}} \\ \midrule
\textbf{System} & \textbf{Orig/SMASH} & \textbf{Rewr/SMASH}  \\ \midrule
PostgreSQL &  $1.0\cdot10^{-10}$ & $0.0002$ \\
Spark &  $6.9\cdot10^{-12}$ & $1.8\cdot10^{-7}$ \\
DuckDB &  $2.0\cdot10^{-8}$ & $2.2\cdot10^{-5}$\\

\bottomrule

\end{tabular}



\end{table}




% \co{Wir versprechen im main  body dass es noch weitere Details geben soll. Gibt es denn noch mehr? Es gibt im Overleaf noch die Datei "appendix\_experiment" aber das stammt noch aus dem alten arXiv paper. Falls der Verweis auf zusätzliche Daten nicht mehr aktuell ist, dann bitte den Verweis und diese Section auskommentieren.}




% Skipping this stuff as it does not seem salvageable.  
% 
% 
% 

\nop{
\section{Additional Information on Section \ref{sect:evaluationAlgorithmSelection}}

\co{ Hier soll das rewriting von unserem arXiv paper illustriert werden an einem konkreten Beispiel. Der Text sollte passen, eventuell braucht es eine neue ``Präambel''. }


As was detailed in Section~\ref{sect:evaluationAlgorithmSelection}, 
decision trees turned out to be the 
best ML model type for our purposes. In Section 

Finally, we want to visualize the decision trees, which are our final models and try to find out which features were the most important ones. In Figures~\ref{res_pos_extrapos_dec_tree},  \ref{res_ddb_extraddb_dec_tree}, and  \ref{res_spa_extrapos_dec_tree}, 
we provide the decision trees of our best performing models for the 3 DBMSs considered here. 
These visualizations give us an overview of how the structure of the decision trees look like. 
The most important features have already. been identified via the Gini coefficients presented in Section \ref{sect:evaluationAlgorithmSelection}

\begin{table*}[t]
    \centering
    \caption{Performance of Machine Learning Classifiers. We show accuracy, precision and recall for binary classifiers that predict whether rewriting to Yannakakis style evaluation leads to performance gain.}
    \label{tab:ml.performance}
    \begin{tabular}{lccc|ccc}
        \toprule
        \textbf{Algorithm} & \multicolumn{3}{c|}{\textbf{0MA Queries}} & \multicolumn{3}{c}{\textbf{Acyc. Queries}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} & Acc. (\%)$\uparrow$  & Prec. $\uparrow$ & Rec. $\uparrow$ & Acc. (\%) $\uparrow$ & Prec. $\uparrow$ & Rec. $\uparrow$ \\
        \midrule
        Decision Tree & \bf 0.94 & \bf 0.92 & \bf 0.97     & \bf 0.95 & \bf 0.95 & 0.92 \\
        Random forest & \bf 0.94 & \bf 0.92 & \bf 0.97         & \bf 0.95 & 0.94 & \textbf{0.93} \\
        $k$-NN & 0.91 & 0.91 & 0.90                    & 0.91 & 0.88 & 0.91 \\
        SVM & 0.85 & 0.85 & 0.84                       & 0.84 & 0.82 & 0.77 \\
        MLP & 0.87 & 0.89 & 0.86                       & 0.85 & 0.84 & 0.77 \\
        HGNN & 0.83 & 0.84 & 0.85                      & 0.79 & 0.70 & 0.75 \\
        HGNN+MLP  & 0.82 & 0.78 & 0.93                 & 0.81 & 0.77 & 0.72 \\
        % Add more algorithms as needed
        \bottomrule
    \end{tabular}
\end{table*}



\begin{figure}
    \centering
    \makebox[\linewidth][c]{\includegraphics[width=1.1\linewidth]{graphics/threshold-e2e.pdf}}
    \caption{Accuracy, precision and e2e runtime of the regression model converted to a classifier, on the full threshold range, depending on the threshold set (decision tree regression, PostgreSQL, all queries)}
    \label{fig:threshold_e2e}
\end{figure}


\begin{figure}[h] 
  \centering 
    \includegraphics[width=1\textwidth]{graphics/results/res_pos_extrapos_dec_tree.png} 
  \caption{Visualization of the final model (= decision tree) for PostgreSQL.}  
  \label{res_pos_extrapos_dec_tree}  
\end{figure} 

\clearpage

\begin{figure}[h] 
  \centering 
    \includegraphics[width=1\textwidth]{graphics/results/res_ddb_extraddb_dec_tree.png} 
  \caption{Visualization of the final model (= decision tree) for DuckDB.}  
  \label{res_ddb_extraddb_dec_tree}  
\end{figure} 


\begin{figure}[h!] 
  \centering 
    \includegraphics[width=1\textwidth]{graphics/results/res_spa_extrapos_dec_tree.png} 
  \caption{Visualization of the final model (= decision tree) for SparkSQL.}  
  \label{res_spa_extrapos_dec_tree}  
\end{figure} 
}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
