\section{Discussion of Limitations and Future Work}

\paragraph{\textbf{Differential Privacy against SIA}}
Other work in the literature, including~\cite{BG_SIA_2}, investigated the use of differential privacy~\cite{dwork2006calibrating} as a defense mechanism against SIA in FL. Differential privacy was chosen due to its theoretical guarantees for privacy protection against inference attacks. However, their findings revealed that vanilla DP is not an effective solution for mitigating SIAs in FL, as it resulted in a significant drop in classification accuracy with only a minimal decrease in SIA accuracy. In contrast, \sysname\ demonstrates that it is possible to effectively reduce the SIA success rate to random guesses while maintaining performance. 




\paragraph{\textbf{Limitations}} Although $\sysname$ represents a step towards fair privacy in FL, it has some limitations. Our current evaluation relies on a specific type of privacy attack (SIA) and is evaluated over two datasets (HAR) and (CIFAR-10). Future work should investigate the effectiveness of $\sysname$ against other privacy attacks, such as attribute inference attacks, and on more diverse datasets that better represent real-world heterogeneity. Furthermore, our client-side approach assumes a degree of client cooperation. Investigating mechanisms that incentivize or enforce client participation in the collaborative overfitting reduction strategy is an important direction for future research.

In this paper, we focus on SIA as a privacy risk. SIA is only more relevant in non-IID data where each client has a distinct private dataset, making this attack more successful. This is prevalent in human-centric applications where the data from different humans are non-IID by nature due to the intrinsic inter- intra-human variability~\cite{zhao2024fina}. However, in other setups where clients in FL have IID data, this SIA attack may not be as successful~\cite{BG_SIA_2}.

\paragraph{\textbf{Future Work and Broader Impact}}
While we consider the SIA attack in FL as a measure of privacy risk, our general definition of $\sysname$ can be applied to other notions of privacy risk beyond the singular decision setups like FL. In our future work, we will explore the fairness-in-privacy in setups with sequential decision-making where the privacy risk is measured over a trajectory of decisions~\cite{zhao2024fairo} instead of singular decision-making. Future work will also investigate the potential for \sysname\ to be integrated with other privacy-enhancing technologies, such as differential privacy and secure multi-party computation, to provide even stronger privacy guarantees. Finally, we aim to investigate the implications of our work for data governance and regulatory frameworks, contributing to the broader discussion on responsible AI development and deployment.