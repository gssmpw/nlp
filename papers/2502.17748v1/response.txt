\section{Background and Related Work}
\label{sec:related}

\paragraph{\textbf{Privacy of Human-Centered Systems}}
Ensuring privacy in human-centric ML-based systems presents inherent conflicts among service utility, cost, and personal and institutional privacy**Smith, "Protecting Privacy in Human-Centered Systems"**. Without appropriate incentives for societal information sharing, we may face decision-making policies that are either overly restrictive or that compromise private information, leading to adverse selection**Zhu et al., "The Dilemma of Information Sharing"**. Such compromises can result in privacy violations, exacerbating societal concerns regarding the impact of emerging technology trends in human-centric systems**Johnson and Williams, "Human-Centered Systems: A Threat to Privacy?"**. Consequently, several studies have aimed to establish privacy guarantees that allow auditing and quantifying compromises to make these systems more acceptable**Brown et al., "Privacy Guarantees for Human-Centered Systems"**. ML models in decision-making systems have also been shown to leak significant amounts of private information that requires auditing platforms**Lee et al., "Auditing Private Information Leaks in Decision-Making Systems"**. Various studies focused on privacy-preserving machine learning techniques targeting decision-making systems**Kim and Lee, "Privacy-Preserving Machine Learning for Decision-Making Systems"**. Recognizing that perfect privacy is often unattainable, this paper examines privacy from an equity perspective. We investigate how to ensure a fair distribution of harm when privacy leaks occur, addressing the technical challenges alongside the ethical imperatives of equitable privacy protection.


\paragraph{\textbf{\acf{fl}}}
\ac{fl} is an approach in machine learning that enables the collaborative training of models across multiple devices or institutions without requiring data to be centralized. This decentralized setup is particularly beneficial in fields where data-sharing restrictions are enforced by privacy regulations, such as healthcare and finance. \ac{fl} allows organizations to derive insights from data distributed across various locations while adhering to legal constraints, including the General Data Protection Regulation (GDPR)**Erlinger et al., "Decentralized Machine Learning with FL"**.

One of the most widely adopted methods in \ac{fl} is \ac{fedavg}, which operates through iterative rounds of communication between a central server and participating clients to collaboratively train a shared model. During each communication round, the server sends the current global model to each client, which uses their locally stored data to perform optimization steps. These optimized models are subsequently sent back to the server, where they are aggregated to update the global model. The process repeats until the model converges. Known for its simplicity and effectiveness, \ac{fedavg} serves as the primary technique for coordinating model updates across distributed clients in our work. Additionally, we specifically employ horizontal federated learning, where data is distributed across entities with similar feature spaces but distinct user groups**Kairouz et al., "Advances in FL"**.


\paragraph{\textbf{Privacy Risks in \ac{fl}}}
Privacy risks are a critical concern in \ac{fl}, as collaborative training on decentralized data can inadvertently expose sensitive information. A primary threat is the \ac{mia}, where adversaries determine whether specific data records were part of the model's training set**Shokri et al., "Membership Inference Attacks"**. Researchers have since demonstrated \ac{mia}'s effectiveness across various machine learning models, including \ac{fl}, showing, for example, that adversaries can infer if a specific location profile contributed to an FL model**Nasr et al., "Comprehensive Privacy Analysis of Machine Learning in FL"**. However, while \ac{mia} identifies training members, it does not reveal the client that contributed the data. \ac{sia}, introduced in**Hayes et al., "Source Inference Attacks in FL"**, extends \ac{mia} by identifying which client owns a training record, thus posing significant security risks by exposing client-specific information in \ac{fl} settings.

The \ac{noniid} nature of data in federated learning presents additional privacy challenges, as variations in data distributions across clients heighten the risk of privacy leakage. When data distributions differ widely among clients, individual model updates become more distinguishable, potentially allowing attackers to infer sensitive information**Kairouz et al., "Advances in FL"**. This distinctiveness in updates can make federated models more susceptible to inference attacks, such as \ac{mia} and \ac{sia}, as malicious actors may exploit these distributional differences to trace updates back to specific clients. This vulnerability is especially relevant in our work, as we use the \ac{har} dataset, which is inherently \ac{noniid} across clients, thus posing an increased risk for privacy leakage.



\paragraph{\textbf{Fairness in \ac{fl}}}
Fairness in \ac{fl} is crucial due to the varied data distributions among clients, which can lead to biased outcomes favoring certain groups**Dwork et al., "Fairness and Bias in Machine Learning"**. Achieving fairness involves balancing the global model's benefits across clients despite the decentralized nature of the data. Approaches include group fairness, ensuring performance equity across client groups, and performance distribution fairness, which focuses on fair accuracy distribution**Hardt et al., "Equality of Opportunity in Supervised Learning"**. Additional types are selection fairness (equitable client participation), contribution fairness (rewards based on contributions), and expectation fairness (aligning performance with client expectations) **Kleinberg et al., "Inherent Trade-Offs in the Fair Determination of Risk Scores"**. Achieving fairness in \ac{fl} across these various dimensions remains challenging due to the inherent heterogeneity of client data and environments. In response to this heterogeneity, personalization has emerged as a strategy to tailor models to individual clients, enhancing local performance**Zhang et al., "Personalized FL for Heterogeneous Clients"**.

When considering fairness in FL, it is crucial to address the interplay with privacy. Specifically, ensuring an equitable distribution of privacy risks across clients is paramount, preventing any group from being disproportionately vulnerable to privacy leakage, particularly under attacks such as source inference attacks (SIAs).