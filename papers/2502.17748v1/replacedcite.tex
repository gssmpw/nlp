\section{Background and Related Work}
\label{sec:related}

\paragraph{\textbf{Privacy of Human-Centered Systems}}
Ensuring privacy in human-centric ML-based systems presents inherent conflicts among service utility, cost, and personal and institutional privacy____. Without appropriate incentives for societal information sharing, we may face decision-making policies that are either overly restrictive or that compromise private information, leading to adverse selection____. Such compromises can result in privacy violations, exacerbating societal concerns regarding the impact of emerging technology trends in human-centric systems____. Consequently, several studies have aimed to establish privacy guarantees that allow auditing and quantifying compromises to make these systems more acceptable____. ML models in decision-making systems have also been shown to leak significant amounts of private information that requires auditing platforms____. Various studies focused on privacy-preserving machine learning techniques targeting decision-making systems____. Recognizing that perfect privacy is often unattainable, this paper examines privacy from an equity perspective. We investigate how to ensure a fair distribution of harm when privacy leaks occur, addressing the technical challenges alongside the ethical imperatives of equitable privacy protection.


\paragraph{\textbf{\acf{fl}}}
\ac{fl} is an approach in machine learning that enables the collaborative training of models across multiple devices or institutions without requiring data to be centralized. This decentralized setup is particularly beneficial in fields where data-sharing restrictions are enforced by privacy regulations, such as healthcare and finance. \ac{fl} allows organizations to derive insights from data distributed across various locations while adhering to legal constraints, including the General Data Protection Regulation (GDPR) ____.

One of the most widely adopted methods in \ac{fl} is \ac{fedavg}, which operates through iterative rounds of communication between a central server and participating clients to collaboratively train a shared model. During each communication round, the server sends the current global model to each client, which uses their locally stored data to perform optimization steps. These optimized models are subsequently sent back to the server, where they are aggregated to update the global model. The process repeats until the model converges. Known for its simplicity and effectiveness, \ac{fedavg} serves as the primary technique for coordinating model updates across distributed clients in our work. Additionally, we specifically employ horizontal federated learning, where data is distributed across entities with similar feature spaces but distinct user groups ____.

\paragraph{\textbf{Privacy Risks in \ac{fl}}}
Privacy risks are a critical concern in \ac{fl}, as collaborative training on decentralized data can inadvertently expose sensitive information. A primary threat is the \ac{mia}, where adversaries determine whether specific data records were part of the model's training set ____. Researchers have since demonstrated \ac{mia}'s effectiveness across various machine learning models, including \ac{fl}, showing, for example, that adversaries can infer if a specific location profile contributed to an FL model ____. However, while \ac{mia} identifies training members, it does not reveal the client that contributed the data. \ac{sia}, introduced in ____, extends \ac{mia} by identifying which client owns a training record, thus posing significant security risks by exposing client-specific information in \ac{fl} settings.

The \ac{noniid} nature of data in federated learning presents additional privacy challenges, as variations in data distributions across clients heighten the risk of privacy leakage. When data distributions differ widely among clients, individual model updates become more distinguishable, potentially allowing attackers to infer sensitive information ____. This distinctiveness in updates can make federated models more susceptible to inference attacks, such as \ac{mia} and \ac{sia}, as malicious actors may exploit these distributional differences to trace updates back to specific clients. This vulnerability is especially relevant in our work, as we use the \ac{har} dataset, which is inherently \ac{noniid} across clients, thus posing an increased risk for privacy leakage.




\paragraph{\textbf{Fairness in \ac{fl}}}
Fairness in \ac{fl} is crucial due to the varied data distributions among clients, which can lead to biased outcomes favoring certain groups ____. Achieving fairness involves balancing the global model's benefits across clients despite the decentralized nature of the data. Approaches include group fairness, ensuring performance equity across client groups, and performance distribution fairness, which focuses on fair accuracy distribution____. Additional types are selection fairness (equitable client participation), contribution fairness (rewards based on contributions), and expectation fairness (aligning performance with client expectations) ____. Achieving fairness in \ac{fl} across these various dimensions remains challenging due to the inherent heterogeneity of client data and environments. In response to this heterogeneity, personalization has emerged as a strategy to tailor models to individual clients, enhancing local performance____.   

When considering fairness in FL, it is crucial to address the interplay with privacy. Specifically, ensuring an equitable distribution of privacy risks across clients is paramount, preventing any group from being disproportionately vulnerable to privacy leakage, particularly under attacks such as source inference attacks (SIAs).