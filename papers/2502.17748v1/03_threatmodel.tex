\section{Problem Statement}\label{sec:threatmodel}

 


Federated learning (FL) systems face significant privacy risks from malicious servers. Even an "honest-but-curious" server, while adhering to the FL protocol, can attempt to infer sensitive client information by analyzing aggregated model updates, potentially revealing private data points, patterns, or client identities. A key privacy threat is a two-stage attack: Membership Inference Attack (MIA) followed by Source Inference Attack (SIA).

\begin{itemize}[noitemsep, topsep=0pt]
    \item \textbf{MIA:} The server determines if a specific data point $x$ was used to train the global model $\theta_g$: MIA($\theta_g$, $x$) = P($x \in D_{\theta_g}$), where P($x \in D_{\theta_g}$) is the probability that $x$ belongs to the training data $D_{\theta_g}$.
    \item \textbf{SIA:} If the MIA suggests $x$ was part of the training data, the server identifies the contributing client $i$: SIA($\theta_i$, $x$) = P(Client$_i$ | $x$, $\theta_i$), where P(Client$_i$ | $x$, $\theta_i$) is the probability that client $i$ contributed $x$ to the model $\theta_i$.
\end{itemize}

As shown by Hongsheng et al. \cite{BG_SIA_2}, combining these attacks can severely compromise client privacy. Moreover, prior work has shown the inherent limitations of auditing MIA \cite{chang2024efficient}. 

Our work focuses on the disparity in privacy risk across clients, which we attribute to differences in local overfitting during training. This threat model underscores the need for equitable privacy mechanisms in FL. 

Given this threat model %in Figure \ref{fig:threatmodel}, 
where a compromised server enables SIA attacks, our objective is twofold:

\begin{enumerate}[noitemsep, topsep=0pt, start=1,label={(\bfseries O\arabic*):}]
    \item Addressing the symptoms: Develop an aggregation method on the server side to ensure fair privacy risk distribution among clients.
    \item Addressing the causes: Provide feedback to leaking clients, enabling them to adjust local updates to reduce overfitting and improve system fairness in privacy.
\end{enumerate}

Instead of eliminating SIA, we aim to mitigate its impact by equitably distributing the inherent privacy risk. We therefore assume a \textit{compromised} server and cooperative clients capable of tuning their local updates to enhance \sysname.
