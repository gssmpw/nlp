\section{Conclusion}
Our \sysname\ framework addresses a critical gap in federated learning (FL): the inequitable distribution of privacy risks. While traditional FL focuses on preserving average privacy, it often overlooks the significant disparities that can exist between clients. These disparities, stemming from heterogeneous data, varying computational resources, and differences in local training dynamics, can lead to situations where specific individuals or groups bear a disproportionate privacy burden. This raises significant ethical concerns, as it can exacerbate existing inequalities and create new forms of discrimination based on data ownership and access. \sysname\ directly tackles this issue by employing a two-pronged approach: server-side adaptive aggregation and client-side collaborative overfitting reduction. This combined approach aims to create a more equitable and robust FL system, promoting fairness-in-privacy for all participants. The formalization of our strategy in Equation \ref{eq:finp} highlights the importance of addressing both the symptoms and causes of the privacy disparity. 
%Our results showed an average improvement in fairness in privacy by \m{$19.77\%$} and \m{xxxxx} of fairness index in HAR dataset, CIFAR-10 dataset respectively. 
Our results showed an average improvement in fairness in privacy by $19.77\%$ of the fairness index in the HAR dataset. Moreover, \sysname\ revealed the potential to reduce the success rate of the SIA to a level comparable to that of a random guess in the CIFAR-10 dataset while maintaining minimal impact on performance.