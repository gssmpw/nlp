\section{Evaluation}\label{sec:eval}



\subsection{Federated Learning System Setup}

\paragraph{\textbf{Setup for Human Activity Recognition}} We utilized the UCI \ac{har} Dataset \cite{human_activity_recognition_using_smartphones_240}, a widely used dataset in activity recognition research, especially in FL~\cite{har2,har3}.
The dataset includes sensor data from 30 subjects (aged 19â€“48) performing six activities: walking, walking upstairs, walking downstairs, sitting, standing, and laying. The data was collected using a Samsung Galaxy S II smartphone worn on the waist, capturing readings from both the accelerometer and gyroscope sensors. Each subject in the dataset was treated as an individual client in the \ac{fl} setup, preserving the data's unique activity patterns and non-IID nature. We allocated 70\% of each client's data for training using 5-fold cross-validation and 30\% for testing, enabling evaluation of the model on independently collected test data. Data preprocessing involved applying noise filters to the raw signals and segmenting the data using a sliding window approach with a window length of 2.56 seconds and a 50\% overlap, resulting in 128 readings per window. We selected the HAR dataset for evaluation \sysname due to its inherited non-IID structure. 

We trained the model in a federated learning setting using the \ac{fedavg} aggregation method over 20 global communication rounds. Each client trained locally with a batch size of 64, 5 local epochs per round, a learning rate of 0.001 using Adam optimizer, and an impact factor $\beta$ of 1. These parameters ensured balanced model updates from each client while maintaining computational efficiency across the federated network. Each local model (one per subject) analyzes its time-series sensor data using \ac{tcn} model\cite{bai2018empirical}. 
The TCN model, designed for time-series data, uses causal convolutions to capture temporal dependencies while preserving sequence order. The architecture includes two convolutional layers, each followed by max-pooling and dropout, with a final fully connected layer for classifying the six activity classes.



\paragraph{\textbf{Setup for CIFAR-10}}
The CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. We use the Dirichlet distribution $Dir(\alpha)$ to divide the CIFAR-10 dataset into $K$ unbalanced subsets similar to previous work in the literature ~\cite{mendieta2022local,BG_SIA_2}, with $\alpha=0.1$. \autoref{fig:CIFAR data} shows how the data are distributed among clients. We created 10 clients and employed ResNet56~\cite{he2016deep} as the model. Similar to the setup in HAR, we trained the model over 20 global communication rounds. Each client is trained locally with the same parameters in HAR and an impact factor $\beta$ of 0.05. A smaller $\beta$ is used here since CIFAR-10 is a more complicated task than HAR, and a smaller $\beta$ can make the model converge easier since the model is more sensitive to classification loss.



\paragraph{\textbf{SIA Attack}} We used the Source Inference Attacks (SIA) setup explained in ~\cite{hu2023source}, where we randomly sampled training data from each client dataset. We combined those samples in one dataset and used them as target records. This is a valid assumption, given an already successful Membership Inference Attacks (MIA) attack. SIA attacks in Federated Learning represent a privacy threat beyond Membership Inference Attacks (MIA). While MIAs determine whether a data instance was used for training, SIA aims to identify the specific client who owns that training record. In a practical scenario, an adversary, such as an honest-but-curious central server who knows the clients' identities and receives their model updates, could leverage this knowledge to trace training data back to its source, thus compromising client privacy. To launch SIA in FL setting,  clients send their updated local model parameters to the server. The server uses each client's model to calculate the prediction loss on the target record. The client with the smallest loss is identified as the most probable source of that target record. This approach exploits the differences in model performance on the target record to infer its origin.


\begin{figure*}
\centering
\includegraphics[width=0.8\linewidth]{fig/CIFAR/Fedalign/Data_profile.png}
\caption{CIFAR dataset profile for each client after Dirichlet sampling with $\alpha=0.1$}
\label{fig:CIFAR data}
\end{figure*}



\subsection{Metrics for Comparison}\label{sec:metrics}
Recent work in the literature suggests SIA vulnerability wherein an adversary can potentially identify the origin of a specific record can be achieved by analyzing the prediction loss of individual client models~\cite{hu2023source}. In particular, SIA exploits the observation that the client model %$C_{min}$ 
exhibiting the lowest prediction loss for a given record is most likely to be the source of that record. We assess our \sysname approach in achieving fairness in privacy using the following metrics:

 
\paragraph{\textbf{1- Reduction of SIA accuracy disparity among clients} } \sysname\  aims to reduce the SIA accuracy disparity among clients. A balanced SIA accuracy across clients indicates a more equitable distribution of privacy risk within the FL system. \sysname enhances the overall fairness in privacy by ensuring that no particular client is significantly more vulnerable to source inference attacks than others. 

To assess the fairness of risk of SIA accuracy across clients in our FL system, we employ the Coefficient of Variation (CoV). Recognizing that fairness is related to the variance of shared utility rather than strict equality \cite{jain1984quantitative}, we adapt the CoV to measure the dispersion of SIA accuracy among clients.

For K clients, we define the SIA accuracy for client $i$ as $\text{SIA}_i$. The mean SIA accuracy ($\mu$) is calculated as
$\mu = \frac{1}{K} \sum_{i=1}^{K} \text{SIA}_i$. The CoV of SIA accuracy \texttt{CoV(SIA)} is then computed as:

\begin{align}\label{eq:covsia}
\texttt{CoV(SIA)} = \frac{\sigma}{\mu} = \frac{\sqrt{\frac{1}{K} \sum_{i=1}^{K} (\text{SIA}_i - \mu)^2}}{\mu},
\end{align}

where $\sigma$ is the standard deviation of SIA accuracies. A lower CoV indicates a more equitable distribution of SIA accuracy across clients, suggesting greater fairness in privacy. To facilitate interpretation as a fairness percentage between $0$ and $1$ (where 1 represents perfect fairness), we use the following Fairness Index (\texttt{FI(SIA)}) transformation:

\begin{align}\label{eq:fisia}
\texttt{FI(SIA)} = \frac{1}{1 + \texttt{CoV(SIA)}} 
\end{align}

A FI value of $1$ indicates perfect fairness (all clients have the same SIA accuracy), while lower FI values indicate increasing disparities in SIA accuracy among clients.









\paragraph{\textbf{2- Reduction of SIA confidence disparity among clients}} Beside reduction of SIA accuracy disparity among clients, as the SIA approach relies on identifying the client model with the minimum prediction loss. When a significant discrepancy exists between the prediction losses of different client models, an attacker can make source inferences with higher confidence. \sysname\  aims to reduce inter-client loss differences so that it can diminish the effectiveness of SIA attacks by lowering the attacker's confidence in their inferences. We evaluate the SIA confidence disparity by using  the prediction loss \texttt{CoV(Loss)} and \texttt{FI(Loss)} similarly to \autoref{eq:covsia} and \autoref{eq:fisia}. 



\paragraph{\textbf{3- Success rate of SIA}}
\sysname\ aims to reduce disparities in both SIA success rate and prediction loss across clients. However, simply reducing disparity is insufficient; it is crucial to avoid achieving this by merely increasing the SIA success rate of less vulnerable clients to match that of the most vulnerable ones. Such an outcome would not represent a genuine improvement in privacy. Therefore, we evaluate the overall impact of \sysname\ on SIA vulnerability using two metrics: \texttt{Mean(SIA)} and \texttt{Max(SIA)}. In particular, \texttt{Mean(SIA)} represents the average SIA success rate across all clients and communication rounds, while \texttt{Max(SIA)} indicates the highest SIA success rate observed across all clients and rounds. Lower values for both metrics signify increased resilience against SIA attacks.

\paragraph{\textbf{4- Accuracy Metric}}

Accuracy is calculated over all test dataset points for all clients using the formula:

\begin{equation}
    \text{Accuracy} = \frac{\sum_{i=1}^{N} \mathbf{1}(\hat{y}_i = y_i)}{N}
\end{equation}

where:
\begin{itemize}
    \item $N$ is the total number of test samples across all clients,
    \item $\hat{y}_i$ is the predicted label for the $i$-th test sample,
    \item $y_i$ is the true label of the $i$-th test sample.
\end{itemize}

We evaluated \sysname\ through two distinct case studies, using the Human Activity Recognition (HAR) dataset (\autoref{sec:HAReval}) and the CIFAR-10 image classification dataset (\autoref{sec:CIFAReval}).  For HAR, we compared four approaches: (1) a Baseline Federated Learning (FL) implementation using FedAvg, adapted from \cite{hu2023source}; (2) \sysname$_\text{server}$, which applies adaptive aggregation at the server without client collaboration (\autoref{eq:finpserver}); (3) \sysname$_\text{client}$, which employs client-side collaboration to mitigate relative overfitting but omits adaptive server aggregation (Equation \ref{eq:finpclient}); and (4) the full \sysname\ approach, incorporating both \sysname$_\text{server}$ and \sysname$_\text{client}$ (\autoref{eq:finp}). 

In the CIFAR-10 evaluation, we compared three approaches: (1) the same Baseline FL using FedAvg from \cite{hu2023source}; (2) FedAlign \cite{mendieta2022local}, a state-of-the-art FL method designed to address data heterogeneity in CIFAR-10; and (3) the full \sysname\ approach, again incorporating both \sysname$_\text{server}$ and \sysname$_\text{client}$ (\autoref{eq:finp}).






\subsection{\sysname\ Performance on HAR}\label{sec:HAReval}

\begin{figure}[!t]
  \centering
  \begin{subfigure}[b]{0.99\columnwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/HAR/line_CoV_SIA.png}
    \caption{Coefficient of variation for SIA accuracy \texttt{CoV(SIA)}.}
    \label{fig:SIACV}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.99\columnwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/HAR/line_FI_SIA.png}
    \caption{Fairness index of SIA accuracy \texttt{FI(SIA)}.}
    \label{fig:SIAFI}
  \end{subfigure}
  \caption{Disparity of SIA accuracy among clients using HAR dataset. }
  \label{fig:SIA}
\end{figure}



\begin{figure}[!t]
  \centering
  \begin{subfigure}[b]{0.99\columnwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/HAR/line_loss_CV.png}
    \caption{Coefficient of variation for the prediction loss \texttt{CoV(Loss)}.}
    \label{fig:lossCV}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.99\columnwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/HAR/line_loss_FI.png}
    \caption{Fairness index of the prediction loss \texttt{FI(Loss)}.}
    \label{fig:lossFI}
  \end{subfigure}
  \caption{Disparity of prediction loss among clients using HAR dataset. }
  \label{fig:loss}
\end{figure}


\begin{figure}
  \centering
  \begin{subfigure}[b]{0.99\columnwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/HAR/line_average_train_accuracy.png}
    \caption{Global model training accuracy.}
    \label{fig:accCV}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.99\columnwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/HAR/line_average_test_accuracy.png}
    \caption{Global model testing accuracy.}
    \label{fig:accFI}
  \end{subfigure}
  \caption{Global model classification accuracy using HAR dataset.}
  \label{fig:accu}
\end{figure}






\begin{figure}
  \centering
  \begin{subfigure}[b]{0.99\columnwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/HAR/line_CoV_PCA_d.png}
    \caption{Coefficient of Variation of the PCA distance to the global model (\texttt{PCA$_d$}). }
    \label{fig:har_pca_d_cov}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.99\columnwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/HAR/line_FI_PCA_d.png}
    \caption{Fairness Index of PCA distance to the global model (\texttt{PCA$_d$}). }
    \label{fig:har_pca_d_fi}
  \end{subfigure}
  \caption{Disparity of PCA distance between the global model and the client models using HAR dataset. }
  \label{fig:pca}
\end{figure}









\subsubsection{\textbf{Impact on the disparity of SIA accuracy among clients}}
Our results demonstrate a significant improvement in fairness with minimal impact on overall performance. \autoref{fig:SIA} presents the Coefficient of Variation of SIA accuracy (\texttt{CoV(SIA)}) and Fairness Index of SIA accuracy (\texttt{FI(SIA)}), as defined in  \autoref{eq:covsia} and \autoref{eq:fisia}, respectively.  \sysname\ achieves a \texttt{CoV(SIA)} of $0.596$ and a \texttt{FI(SIA)} of $0.739$, compared to the Baseline's \texttt{CoV(SIA)} of $0.893$ and \texttt{FI(SIA)} of $0.617$. This represents a substantial reduction of $33.26\%$ in \texttt{CoV(SIA)} and a $19.77\%$ improvement in \texttt{FI(SIA)} compared to baseline, clearly indicating that \sysname\ significantly enhances the fairness of SIA accuracy.






\subsubsection{\textbf{Impact on the disparity of SIA confidence among clients}} 
Similarly, our results demonstrate improvement in fairness with respect to the SIA confidence in prediction among clients represented as \texttt{CoV(Loss)} and \texttt{FI(Loss)} as explained in \autoref{sec:metrics}. As shown in \autoref{fig:loss}, \sysname\ achieves a \texttt{CoV(Loss)} of $0.778$ and \texttt{FI(Loss)} of $62.8\%$. This represents a reduction of $10.95\%$ in \texttt{CoV(Loss)} and a $19.77\%$ improvement in \texttt{FI(Loss)} compared to the Baseline, clearly indicating that \sysname\ enhances the fairness of SIA confidence in prediction among clients.


\subsubsection{\textbf{Impact on SIA success rate}}
While \autoref{tbl:HAR_sia} shows a marginal increase of less than $1\%$ in \texttt{Mean(SIA)} success rate and less than $0.1\%$ in \texttt{Max(SIA)} success rate, these gains are secondary to the primary objective of fairness improvement.  The key achievement of \sysname\ is the demonstrably more equitable distribution of privacy protection.  \sysname\ achieves this by significantly improving the uniformity of SIA success rates and reducing SIA confidence across clients.

Furthermore, \sysname\ maintains competitive classification performance. As shown in \autoref{tbl:HAR_acc}, the global model's testing accuracy only decreases by a $1.02\%$.  This impact on accuracy is further supported by \autoref{fig:accu}, which demonstrates that \sysname\ converges at a comparable rate to the Baseline.  Therefore, \sysname\ effectively balances the critical need for fairness with the practical requirement of maintaining performance.

\subsubsection{Ablation study}




\begin{table}[!t]
\caption{SIA accuracy performance in HAR dataset.}
\label{tab:my-table}
\begin{tabular}{|c|l|l|}
\hline
\multicolumn{1}{|l|}{} & \texttt{Mean(SIA)}(\%) $\downarrow$ & \texttt{Max(SIA)}(\%) $\downarrow$   \\
\hline
Baseline~\cite{hu2023source} & \textbf{23.78} & 31.00   \\
\sysname$_{\text{server}}$ & 25.22 & 31.20 \\
\sysname$_{\text{client}}$ & 25.52 & \textbf{30.20}   \\
\sysname & 24.49 & 31.10 \\
\hline
\end{tabular}
 \label{tbl:HAR_sia}
\end{table}



\begin{table}[!t]
\caption{HAR experiment: global model classification accuracy.}
\label{tab:my-table}
\begin{tabular}{|c|l|l|}
\hline
\multicolumn{1}{|l|}{} &  Training (\%) & Testing (\%)   \\
\hline
Baseline~\cite{hu2023source} & \textbf{96.52} & \textbf{96.94}   \\
\sysname$_{\text{server}}$ & 95.79 & 95.77\\
\sysname$_{\text{client}}$ & 95.67 & 95.99   \\
\sysname & 95.05 & 95.92 \\
\hline
\end{tabular}
 \label{tbl:HAR_acc}
\end{table}








We conducted an ablation study to evaluate the individual contributions of \sysname's server-side and client-side components.  Isolating the server-side adaptive aggregation (\sysname$_\text{server}$) revealed a nuanced impact on fairness metrics.  While \sysname$_\text{server}$ reduced the variation in PCA distance (\texttt{PCA$_d$)} by $1.3\%$ (\autoref{fig:pca}), it also resulted in a slight shift in both \texttt{FI(SIA)} and \texttt{FI(Loss)} by $-0.2\%$ and $-1.3\%$, respectively (Figures \ref{fig:SIAFI} and \ref{fig:lossFI}). This suggests that server-side adaptation alone (\sysname$_\text{server}$) primarily influences the distribution of model distances and has a less direct impact on the fairness metrics themselves. This observation motivated the investigation of client-side factors, specifically the variation in overfitting among clients, to further enhance fairness.

Analysis of Hessian eigenvalues ($\lambda_{\text{max}}$) and trace ($H_{T}$) revealed a strong correlation (Spearman's rank correlation coefficient $\approx$ 1) between these two metrics, both indicative of how well a local model fits its local data (\autoref{fig:subHessian}).  Based on this correlation, these metrics were given equal weight in \autoref{eq:hessian}.  Focusing on mitigating client-side overfitting through \sysname$_\text{client}$ yielded significant improvements in fairness.  Figures \ref{fig:SIA} and \ref{fig:loss} demonstrate the substantial gains in both SIA accuracy and prediction loss fairness.  Specifically, \sysname$_\text{client}$ alone improved \texttt{FI(SIA)} by $16.37\%$ and \texttt{FI(Loss)} by $8.30\%$ (Figures \ref{fig:SIAFI} and \ref{fig:lossFI}).  Furthermore, combining \sysname$_\text{server}$ with \sysname$_\text{client}$ resulted in even greater fairness gains, with an additional $3.13\%$ improvement in \texttt{FI(SIA)} and $2.65\%$ in \texttt{FI(Loss)} compared to using \sysname$_\text{client}$ alone. This indicates that while \sysname$_\text{server}$'s primary effect is on model distance distribution, it contributes synergistically to the fairness improvements achieved by \sysname$_\text{client}$ when both are employed.


More results related to the adaptation of the aggregation weights $\mathcal{W}$ (\autoref{eq:finpserver}) and the regularization strength $\rho_k$ (\autoref{eq:finpclient}) are shown in \autoref{appendix:adaptation}. 

\begin{figure*}
\centering
\includegraphics[width=0.8\linewidth]{fig/HAR/scatter_eigenvalues_eigentrace.png}
\caption{Scatter figures for Hessian max eigenvalue ($\lambda_{\text{max}}$) and Hessian trace ($H_{T}$).  The figure shows the value of each clients Hessian max eigenvalue and trace in the Baseline method for HAR dataset from rounds 6 to 11. All the rounds are depicted in \autoref{appendix:Hessian}.}
\label{fig:subHessian}
\end{figure*}


\subsection{\sysname\ Performance in CIFAR-10 dataset} \label{sec:CIFAReval}
In the CIFAR-10 dataset, \sysname\ demonstrates a significant improvement in fairness in privacy, with competitive accuracy. \autoref{fig:CIFAR loss} shows the Fairness Index of prediction loss (\texttt{FI(Loss)}) for FedAvg, FedAlign, and \sysname\ are 68.8\%, 58.0\%, and 83.3\%, respectively.  \sysname\ achieves a substantial increase in \texttt{FI(Loss)} of 21.1\% compared to FedAvg and 43.6\% compared to FedAlign. Notably, despite employing a distillation technique, FedAlign failed to effectively mitigate SIA risks, exhibiting a higher \texttt{CoV(Loss)} of 0.862 compared to FedAvg's 0.674. This increased \texttt{CoV(Loss)} can empower attackers with greater confidence in predicting the source client, consequently leading to higher SIA success rates.

Although FedAlign and FedAvg exhibit similar Mean and Max SIA success rates (\autoref{tbl:CIFAR}), \sysname\ effectively mitigates these risks. As shown in \autoref{fig:sia overall} and \autoref{tbl:CIFAR}, \sysname\ reduces the \texttt{Mean(SIA)} success rate to 10.07\%, approaching the random-guess probability of 1/10 (10\%) for a 10-class classification task.  Specifically, \sysname\ reduces the \texttt{Mean(SIA)} success rate from 30.86\% to 10.07\% and the \texttt{Max(SIA)} success rate from 38.52\% to 10.67\%. As shown in \autoref{fig:CIFAR sia}, the \sysname\ demonstrates comparable \texttt{CoV(SIA)} and \texttt{FI(SIA)}, yet exhibits a substantial reduction in the average success rate of SIA as mentioned above.

Moreover, \sysname\ maintains and slightly improves classification accuracy.  Figure \ref{fig:accu cifar} shows that \sysname\ achieves a testing accuracy of 78.46\%, marginally higher than FedAvg's 77.62\%. This 0.84\% improvement is attributed to the global model's aggregation of generalized client models through Lipschitz regularization rather than models overfit to individual datasets.  In summary, \sysname\ effectively mitigates SIA privacy risks in FL training in CIFAR-10 by improving client generalization and reducing loss variation across client models, all while maintaining or slightly improving classification performance.








\begin{figure}[!t]
  \centering
  \begin{subfigure}[b]{0.99\columnwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/CIFAR/Fedalign/line_loss_CV.png}
    \caption{Coefficient of variation for the prediction loss \texttt{CoV(Loss)}.}
    \label{fig:CIFARCV}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.99\columnwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/CIFAR/Fedalign/line_loss_FI.png}
    \caption{Fairness index of the prediction loss \texttt{FI(Loss)}.}
    \label{fig:CIFARFI}
  \end{subfigure}
  \caption{Disparity of prediction loss among clients using CIFAR-10 dataset.}
  \label{fig:CIFAR loss}
\end{figure}

\begin{table}[!t]
\caption{SIA accuracy performance in CIFAR-10 dataset with Resnet model.}
\label{tab:my-table}
\begin{tabular}{|c|l|l|}
\hline
\multicolumn{1}{|l|}{} & Mean SIA(\%) $\downarrow$ & Max SIA(\%) $\downarrow$   \\
\hline
Baseline~\cite{hu2023source} & 30.86 & 38.52   \\
FedAlign~\cite{mendieta2022local} & 30.72 & 38.46 \\
\sysname & \textbf{10.07} & \textbf{10.67} \\
\hline
\end{tabular}
\label{tbl:CIFAR}
\end{table} 



\begin{figure}[!t]
\centering
\includegraphics[width=0.8\columnwidth]{fig/CIFAR/Fedalign/line_SIA_Accuracy.png}
\caption{Average SIA accuracy across rounds in CIFAR-10 dataset. }
\label{fig:sia overall}
\end{figure}




\begin{figure}[!t]
  \centering
  \begin{subfigure}[b]{0.99\columnwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/CIFAR/Fedalign/line_CoV_SIA.png}
    \caption{Coefficient of variation for SIA accuracy \texttt{CoV(SIA)}.}
    \label{fig:CIFARCVsia}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.99\columnwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/CIFAR/Fedalign/line_FI_SIA.png}
    \caption{Fairness index of SIA accuracy \texttt{FI(SIA)}.}
    \label{fig:CIFARFIsia}
  \end{subfigure}
  \caption{Disparity of SIA accuracy among clients using CIFAR-10 dataset.}
  \label{fig:CIFAR sia}
\end{figure}



\begin{figure}[!t]
  \centering
  \begin{subfigure}[b]{0.99\columnwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/CIFAR/Fedalign/line_average_train_accuracy.png}
    \caption{Global model training accuracy. }
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.99\columnwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/CIFAR/Fedalign/line_average_test_accuracy.png}
     \caption{Global model testing accuracy.}
  \end{subfigure}
  \caption{Global model classification accuracy using CIFAR-10.}
  \label{fig:accu cifar}
\end{figure}







\subsection{Summary of \sysname\ in HAR and CIFAR-10 results}
In summary, our evaluation across HAR and CIFAR-10 datasets demonstrates the effectiveness of \sysname\ in achieving fairness in the impact of source inference attacks (SIA) while maintaining or improving model performance.   Although \sysname successfully achieves a more equitable distribution of SIA risk among clients in the HAR dataset, the inherent variability of human activity data, with each client representing a single individual exclusively, limits the complete elimination of SIA vulnerability, as it is difficult for client models to be completely indistinguishable. Nevertheless, as detailed in \autoref{sec:HAReval}, \sysname\ significantly enhances fairness in privacy.  Conversely, on the CIFAR-10 dataset, \sysname\ effectively mitigates SIA risks as detailed in \autoref{sec:CIFAReval}. This result is attributed to the non-IID sampling of CIFAR-10 subsets across clients, where all data in each subset are part of the comprehensive CIFAR-10 dataset. This characteristic enables the regularization of client models to be indistinguishable and coupled with applying Lipschitz constant loss during local training and adaptive aggregation at the server. These mechanisms collectively promote client model generalization and reduce loss variation, thereby neutralizing the effectiveness of source inference attacks (SIA).










