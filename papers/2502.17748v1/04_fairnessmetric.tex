
\section{Fairness-in-Privacy Framework in Federated Learning}\label{sec:finpmetric}

\begin{figure*}[!t]
\centering
\includegraphics[trim={0 9cm 3cm 0},clip,width=0.8\linewidth]{fig/framework.pdf}
\caption{Fairness in Privacy \sysname\ framework in federated learning. The framework addresses the causes and the symptoms to achieve \sysname.}
\label{fig:framework}
\end{figure*}


This section presents our framework, \sysname, designed to improve fairness in privacy within federated learning (FL), particularly in the context of Source Inference Attacks (SIAs). Our core principle is that privacy risks should be distributed equitably among all participating clients, preventing any single client from bearing a disproportionate burden. 
 

This disparity in privacy risks among clients can arise from various factors, including heterogeneous data distributions, varying computational resources, and differences in local training dynamics. Simply preventing average privacy leakage is insufficient; we must ensure that no individual client bears a disproportionate risk. This motivates our focus on fairness-in-privacy, which aims to equitably distribute privacy risks across all participating clients.

An overview of the \sysname\ framework is shown in~\autoref{fig:framework}. We argue that addressing fairness in privacy requires a two-pronged approach: handling it both at the server (during aggregation) and at the client (during local training). Server-side interventions, specifically adaptive aggregation, are crucial to mitigating the impact of existing disparities in privacy leakage. By carefully weighing client updates based on their estimated privacy risk, we can prevent highly vulnerable clients from unduly influencing the global model and further exacerbating the unfairness. However, server-side interventions alone are insufficient. They address the *symptoms* of unfairness but not the underlying *causes*.

The root cause of privacy disparity often lies in differences in local training dynamics, particularly local overfitting. When a client's model overfits its local data, it becomes more susceptible to privacy attacks, such as Source Inference Attacks (SIAs). Therefore, we also address fairness in privacy on the client side by introducing a collaborative overfitting reduction strategy. This strategy aims to proactively reduce the likelihood of local overfitting, thereby minimizing the initial disparity in privacy risks before aggregation. By ranking clients based on their estimated relative overfitting and incorporating this rank into a local regularization scheme, we encourage clients to learn more generalizable representations, reducing their vulnerability to the disparity in privacy leakage.

This two-pronged approach, combining adaptive aggregation at the server and collaborative overfitting reduction at the client, provides a comprehensive framework for achieving fairness in privacy in FL. By minimizing both the symptoms and the root causes of privacy disparity, our aim is to create a more equitable and robust FL system. This can be formalized in \autoref{eq:finp}. 

\begin{align}\label{eq:finp}
\text{F}in\text{P} &= \min (\text{Symptoms}, \text{Causes}) \nonumber \\ 
&= \text{F}in\text{P}_\text{server} + \text{F}in\text{P}_\text{client}
\end{align}


\subsection{Formalizing Symptoms of Fairness in Privacy on Server Side}

We formalize the fairness in privacy problem as follows: Given an FL system with $K$ clients and a global model $\theta_g$, our goal is to achieve fair privacy risk across all clients against successful SIAs.

We consider the privacy risk $p_k(\mathbf{w})$ for client $k$ to be influenced by the aggregation weights $\mathbf{w} = [w_1, w_2, ..., w_K]$, where $w_k$ represents the weight assigned for the client $k$, with the constraint $\sum_{k=1}^{K} a_k = 1$. This allows us to account for the varying client contributions to the global model.

We define Fairness in Privacy (F$in$P) as minimizing the variance in privacy risks across clients. Our objective is to find the optimal weights for aggregation $\mathbf{w}$ that minimize the difference between individual client privacy risks and the average privacy risk. This is expressed in \autoref{eq:finpserver} as:


\begin{align}\label{eq:finpserver}
    \text{F}in\text{P}_\text{server} = \min_{\mathbf{w}\in \mathcal{W}} \| \mathbf{p}(\mathbf{w}) - \frac{1}{K} \mathds{1}^T \mathbf{p}(\mathbf{w}) \otimes \mathds{1}  \| + \|\frac{1}{K} \mathds{1}^T \mathbf{p}(\mathbf{w})\|,
    %\vspace{-2mm}
\end{align}

Where:

\begin{itemize}
    \item $\mathbf{p}(\mathbf{w}) = [p_1(\mathbf{w}),\dots, p_K(\mathbf{w})]^T$ is the vector of privacy risks for all clients given the aggregation weights $\mathbf{w}$.
    \item $\mathds{1}$ is a vector of ones of length $K$.
    \item $\mathcal{W} = \{\mathbf{w} \in \mathbb{R}^K \mid \sum_{k=1}^{K} w_k = 1, w_k \geq 0 \ \forall k\}$ is the set of valid aggregation weights.
\end{itemize}

The term $\frac{1}{K} \mathds{1}^T \mathbf{p}(\mathbf{w})$ represents the average privacy risk. Equation \eqref{eq:finpserver} minimizes the Euclidean distance between individual privacy risks and this average, thus minimizing the disparity in privacy risks. Intuitively, we seek optimal aggregation weights to achieve a more equitable distribution of privacy risk, ensuring no client is disproportionately exposed.


We hypothesize that differences in local overfitting are a primary cause of unequal privacy leakage among FL clients. When a client's model overfits to its local data, it effectively memorizes sensitive information, making it more vulnerable to SIAs and leading to an unfair distribution of privacy risk.

We quantify the *symptoms* of overfitting in the server by measuring the discrepancy between each client's local model update and the global model using the Principle Component Analysis (PCA) distance \cite{durmus2021federated}. For client $k$, this distance, denoted as $p_k$, serves as a proxy for privacy risk; a larger $p_k$ signifies a symptom of greater overfitting and, thus, higher risk.

Our proposed adaptive aggregation method aims to balance client contributions based on these PCA distances. By minimizing the variance of $p_k$ using the \sysname$_{server}$ objective (\autoref{eq:finpserver}), we reduce the influence of clients exhibiting high overfitting (high $p_k$) and increase the influence of those with lower overfitting. This dynamic adjustment, performed in each FL round, promotes a more equitable distribution of privacy risk. However, adaptive aggregation alone is insufficient to eliminate overfitting entirely and achieve full fairness in privacy; collaborative client-side adjustments are also required, as will be explained in \autoref{sec:client_side_adjustments}.




\subsection{Formalizing Causes of Fairness in Privacy on Client Side}\label{sec:client_side_adjustments}

To further mitigate local overfitting (*causes*) and enhance fairness in privacy, we propose a collaborative client strategy. This leverages the principle that clients with higher overfitting benefit from more diverse data.

The top Hessian eigenvalue ($\lambda_{\text{max}}$) and Hessian trace ($H_{T}$) have been identified as important metrics for characterizing the loss landscape and generalization capabilities of neural networks~\cite{Jiang2020Fantastic}. Lower values of $\lambda_{\text{max}}$ and $H_{T}$ typically indicate improved robustness to weight perturbations, leading to smoother training and better convergence. This is especially critical in FL, where the non-IID nature of data across clients creates distributional shifts that can exacerbate training instability and introduce fairness concerns. These distributional shifts can disproportionately impact certain client groups, leading to biased model performance~\cite{mendieta2022local}. 


As we are interested in \sysname, we determine each client's relative overfitting by calculating the average pairwise difference across the top Hessian eigenvalue ($\lambda_{\text{max}}$) and Hessian trace ($H_{T}$):

\begin{align}
\begin{split}
\bar{\Delta}_k &= \frac{1}{K-1} \sum_{j=1, j\neq k}^{K} |\lambda_{\text{max}}^k - \lambda_{\text{max}}^j|, \\    \bar{H}_k &= \frac{1}{K-1} \sum_{j=1, j\neq k}^{K} |H_T^k - H_T^j|, \\  \rho_k &= \frac{\frac{\bar{\Delta}_k}{\max{\bar{\Delta} }} + \frac{\bar{H}_k}{\max{\bar{H} }} }{2},
\end{split} \label{eq:hessian}
\end{align}

where $\lambda_{\text{max}}^k$ and $\lambda_{\text{max}}^j$ are the top Hessian eigenvalue of the local models of clients $k$ and $j$, respectively.  Similarly, $H_T^k$, and $H_T^j$ are the Hessian trace of the local models of clients $k$ and $j$, respectively. We used the normalized average of both $\bar{\Delta}_k$ and $\bar{H_k}$ to quantify the client's \textit{overfitting relative rank} ($\rho_k$), to serve as a proxy for relative privacy leakage risk. Computing the Hessian eigenvalue and trace are done on the cloud server, and hence, there is no overhead of their computation on the client.


We incorporate this overfitting rank into the local training process using a regularization term based on the Lipschitz constant, approximated by the spectral norm of the Jacobian matrix ($||J_k||$)~\cite{liu2020simple}. In particular, a smaller Lipschitz constant implies smoother functions, less prone to overfitting, and better generalization. The modified local loss function for client $k$ is:

\begin{align}\label{eq:finpclient}
\mathcal{L}_k' &= \mathcal{L}_k + \beta \cdot \rho_k \cdot ||J_k||, \nonumber \\
\text{F}in\text{P}_\text{client} &= \min_{\theta_k} {\mathcal{L}_k'}
\end{align}

where:

\begin{itemize}
    \item $\mathcal{L}_k$ is the original local loss function.
    \item $\rho_k$ is an adaptive controlling regularization strength that depends on the overfitting rank.
    \item $\theta_k$ are the local parameters of the client model that minimize the total loss $\mathcal{L}'_k$
    \item  $\beta$ is the impact factor, which controls the impact of the Lipschitz constant based on the learning task.
\end{itemize}

This penalizes models with large Lipschitz constants, promoting generalization. The regularization strength is weighted by $\rho_k$ adaptively at each round, applying stronger regularization to clients with higher overfitting ranks. This collaborative approach, using $\rho_k$ to guide local training, preserves privacy while promoting equitable learning and reducing disparity in privacy risk. 

$\beta$ is a task-dependent parameter to balance the the loss $\mathcal{L}_k$ and Lipschitz loss. A larger $\beta$ greatly impacts fairness regularization but could make training unstable and fail to converge. $\beta$ is a trade-off parameter between fairness and accuracy while $\rho_k$ changes at every round to control regularization strength adaptively.










