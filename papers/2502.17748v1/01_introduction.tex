\section{Introduction}\label{intro}




The pervasive integration of machine learning (ML) into daily life demands careful consideration of its fairness and ethical implications. While traditional algorithmic fairness research has primarily focused on preventing bias and discrimination in decision-making~\cite{kleinberg2018algorithmic,rambachan2020economic, dwork2012fairness, kusner2017counterfactual, zhao2024fairo}, a more nuanced approach is needed for human-centric systems. Existing definitions often emphasize equality—the absence of prejudice in algorithmic decisions~\cite{mehrabi2021survey}. However, the crucial aspect of equity—the fair distribution of outcomes and impacts—has received less attention, particularly in human-centric contexts~\cite{zhao2024fina}. This paper addresses this gap by focusing on equity in privacy risk distribution.


In human-centric ML systems that collect and process personal data, ensuring fairness in privacy risk is paramount, meaning no individual or group should be disproportionately exposed. Recognizing that complete prevention of privacy leaks is often infeasible, we investigate how to equitably distribute the resulting harm. Promoting fairness in privacy mitigates biases, prevents discrimination in privacy risk, and fosters trust between individuals and AI systems, contributing to ethical AI development and deployment where benefits and adverse impacts are shared equitably.

Federated learning (FL) offers a promising privacy-preserving approach for training ML models on decentralized devices. By enabling collaborative learning without sharing raw data, FL aligns with human-centric design principles and mitigates certain privacy risks, making it particularly relevant for sectors with stringent data privacy regulations like healthcare and finance. However, FL is vulnerable to privacy attacks such as Membership Inference Attacks (MIA)~\cite{shokri2017membership}, where the attacker aims to determine whether a specific data point was used in the training set of a machine learning model. Moreover, regulations such as GDPR~\cite{GDPR}, with its Data Protection Impact Assessment (DPIA) provisions, underscore the growing legal emphasis on data safeguarding, highlighting the critical need for privacy audits in FL~\cite{chang2024efficient}, especially in sensitive domains~\cite{rieke2020future}. 

A recent example is the 2024 National Public Data (NPD) breach, exposing billions of records, underscores the critical need for fairness in privacy~\cite{spectrumnews2024npdbreach}, especially within federated learning (FL). While affecting millions, the breach disproportionately impacted vulnerable populations like low-income individuals, the elderly, and those with disabilities, who are more susceptible to the consequences of data breaches. This highlights a key limitation of traditional privacy approaches that focus on average risk, neglecting equitable distribution. In FL, where data is decentralized, this problem is amplified; if certain clients, potentially representing vulnerable demographics, consistently experience higher privacy leakage due to factors like data heterogeneity or local overfitting, new digital inequalities emerge. The NPD breach emphasizes the importance of frameworks that prioritize not just overall privacy but also its equitable distribution among FL participants, preventing disproportionate harm, particularly from attacks such as Source Inference Attacks (SIAs) that can trace leakage to specific clients~\cite{BG_SIA_2}.


\section{Motivation and Contributions}


\paragraph{\textbf{Potential Societal Impact}} Acknowledging the near inevitability of some data breaches—as reflected in the ``zero trust'' security model~\cite{Rose2020Zero}—the focus is shifting from solely preventing breaches to ensuring equitable outcomes and risk distribution. This paper directly addresses this by prioritizing the equitable distribution of privacy risks, a crucial aspect that is often overlooked. Demonstrating the feasibility of designing AI systems that minimize and fairly distribute unavoidable privacy leaks builds public trust, which is essential for wider technology adoption. Furthermore, formalizing ``fairness-in-privacy'' within FL provides policymakers with tangible metrics and frameworks for regulating AI systems, enabling informed policies that promote equitable privacy protection in our increasingly data-driven world and fostering responsible AI development.
This requires technical frameworks that can quantitatively measure and assess the fairness-in-privacy and provide solutions to improve it.

\paragraph{\textbf{Case Study}} Consider the application of Federated Learning (FL) in the domain of human activity recognition (HAR), particularly concerning its implications for fairness and equity. FL offers a compelling paradigm for developing personalized health monitoring systems using data from wearable devices while preserving user privacy~\cite{rieke2020future,poulain2023improving}. Each user's device is a client, training a local model on their activity data without directly sharing the raw sensor readings with a central server~\cite{BG_Survey2}. However, this decentralized approach can inadvertently create or exacerbate existing societal inequalities. For instance, consider deploying an FL-based HAR system to monitor physical rehabilitation progress. Users with disabilities or chronic conditions might exhibit movement patterns that deviate significantly from the ``average'' user represented in the global model. These individuals might be more easily re-identified through techniques like membership inference attacks (MIA)~\cite{shokri2017membership} or source inference attacks (SIA)~\cite{BG_SIA_2}. This constitutes representational harm, as these individuals are disproportionately exposed to privacy risks due to their unique characteristics. Such disparities in privacy protection can further marginalize already vulnerable populations. Without explicitly addressing equitable risk distribution and potential biases embedded within data and algorithms, the purported benefits of FL may not be shared equitably, potentially reinforcing existing social inequalities. This underscores the need for technical frameworks such as the one proposed in this paper to quantify and mitigate risk.

\paragraph{\textbf{Technical Approach}}
This paper addresses the critical challenge of ensuring fairness in privacy \sysname\ within federated learning (FL). Although FL offers inherent data privacy by keeping the data localized, it can inadvertently create disparities in privacy leakage among clients due to heterogeneous data and varying local training dynamics~\cite{selialia2024mitigating}. To mitigate this, we propose a novel two-pronged approach that tackles fairness in privacy both at the server and client levels. At the server, we introduce an adaptive aggregation strategy that weights client model updates based on their estimated privacy risk, quantified using the principal component analysis (PCA) distance between local and global models. This prevents highly vulnerable clients from disproportionately influencing the global model. Complementing this server-side intervention, we propose a client-side collaborative overfitting reduction method. By estimating each client's relative overfitting using the maximum eigenvalue of its local model's Hessian and incorporating this information into a local regularization term based on the Lipschitz constant, we encourage clients to learn more generalizable representations, reducing their individual vulnerability to privacy attacks. This combined approach addresses both the symptoms (unequal risk distribution) and the root causes (local overfitting) of the privacy disparity, leading to a more equitable FL system for \sysname. This approach achieves $\approx 20\%$ improvement in fairness in privacy with a negligible effect on task performance. Our contributions are:

 \begin{itemize}[noitemsep, topsep=0pt]
\item A formal definition of fairness-in-privacy (\sysname) for human-centric systems.
\item Operationalization of \sysname\ within Federated Learning (FL), focusing on source inference attacks (SIA).
\item A novel adaptive approach to optimize \sysname during FL server aggregation and client collaboration.
\item Evaluating a human-centered system using a human activity recognition (HAR) dataset and common learning tasks, such as CIFAR-10. %and smart education classrooms.
\end{itemize}



