\begin{figure*}[t]
\centering
\begin{tabular}{@{}c@{\hspace{2pt}}c@{\hspace{2pt}}c@{\hspace{2pt}}c@{}}
\includegraphics[width=0.223\linewidth]{fig/only_ours/compose_case139.png} &
\includegraphics[width=0.223\linewidth]{fig/only_ours/compose_case75.png} &
\includegraphics[width=0.26\linewidth]{fig/only_ours/compose_case585.png} &
\includegraphics[width=0.26\linewidth]{fig/only_ours/compose_case1296.png} \\
\includegraphics[width=0.223\linewidth]{fig/only_ours/compose_case7252_seed1.png} &
\includegraphics[width=0.223\linewidth]{fig/only_ours/compose_case6623_seed1.png} &
\includegraphics[width=0.26\linewidth]{fig/only_ours/compose_case62b32ca8079dcd9363c3e0ab_seed3.png} &
\includegraphics[width=0.26\linewidth]{fig/only_ours/compose_case6684_seed1.png}
\end{tabular}
\vspace{-12pt}
\caption{\footnotesize{
\textbf{Variable multi-layer transparent images generated with ART}. The number of transparent layers from top left to bottom right are 7, 8, 11, 30, 8, 10, 12, and 13.
}}
\label{fig:only_ours}
\vspace{-3mm}
\end{figure*}

\begin{figure}[t]
    \centering
    \hspace{-10pt}
    \begin{subfigure}[b]{0.47\linewidth}
        \centering
        \begin{tabular}{@{}c@{\hspace{1pt}}c@{}}
            \includegraphics[height=1.5cm]{fig/cole_benchmark_with_layers/cole_MarketingMaterials_100.png} & 
            \includegraphics[height=1.5cm]{fig/cole_benchmark_with_layers/cole_Posts_16.png} \\ [-3pt]
            \multicolumn{2}{c}{\footnotesize{$\uparrow$~COLE~\cite{jia2023cole} \vs ART~$\downarrow$ }} \\[0pt]
            \includegraphics[height=1.5cm]{fig/cole_benchmark_with_layers/ART_MarketingMaterials_100.png} & 
            \includegraphics[height=1.5cm]{fig/cole_benchmark_with_layers/ART_Posts_16.png} \\
        \end{tabular}
    \end{subfigure}
    \hspace{-2pt}
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \begin{tabular}{@{}c@{\hspace{1pt}}c@{}}
            \includegraphics[height=1.5cm]{fig/real_benchmark/curtain_ldf.png} & 
            \includegraphics[height=1.5cm]{fig/real_benchmark/table_ldf.png} \\ [-3pt]
            \multicolumn{2}{c}{\footnotesize{$\uparrow$~LayerDiffuse~\cite{zhang2024transparent} \vs ART~$\downarrow$ }} \\[0pt]
            \includegraphics[height=1.5cm]{fig/real_benchmark/curtain_art.png} & 
            \includegraphics[height=1.5cm]{fig/real_benchmark/table_art.png} \\
        \end{tabular}
    \end{subfigure}
    \vspace{-12pt}
    \caption{\footnotesize{
    \textbf{ART v.s. COLE or LayerDiffuse}: Given the same global prompt, we display the generated multiple transparent layers to the right of their merged entire image separately. The overall aesthetics and layout of our merged image are superior.
    }}
    \label{fig:colr}
    \vspace{-5mm}
\end{figure}

\section{Experiment}

\vspace{1mm}
\noindent\textbf{Implementation details}. We conduct all the experiments using the latest FLUX.1[dev] model~\cite{flux}. For ablation studies, we train the MMDiT with LoRA for 30,000 iterations, with a global batch size of 8 and a learning rate of 1.0 using the Prodigy optimizer~\cite{mishchenko2023prodigy}. The LoRA rank is set at 64, and the image resolution is at 512$\times$512. To ensure fair comparisons during system-level experiments, we increased the number of iterations to 90,000 and the image resolution to 1024$\times$1024.
For the multi-layer transparency decoder, we selected the ViT-Base configuration~\cite{dosovitskiy2020image}. This configuration includes 12 layers, a hidden dimension size of 768, an MLP dimension size of 3072, and 12 attention heads, resulting in a total of 86 million parameters.


\vspace{1mm}
\noindent\textbf{Training set \& validation set}.
We choose $800$K multi-layer graphic design images as the training set and a set of $5$K graphic design samples to form the validation set, referred to as \designbenchmark. Additionally, we also create a set of photorealistic multi-layer image prompts chosen from the COCO dataset~\cite{lin2014microsoft}, forming \photobenchmark, to evaluate the model's performance on multi-layer real image generation.

\vspace{1mm}
\noindent\textbf{Evaluation metric}.
For the ablation studies, we report standard metrics, including FID~\cite{dowson1982frechet}, PSNR, and SSIM. 
To assess the quality of the Anonymous Region Transfomer, the FID is computed by comparing the predicted merged images to the ground truth merged images, denoted as FID${\scriptstyle\text{merged}}$. The PSNR and SSIM are calculated by comparing the merged image with the predicted reference composed image. 
To assess the quality of the multi-layer transparency image autoencoder, we report the PSNR for the RGB channels and the alpha channel separately, \ie, PSNR{$^{\textrm{layer}}_{\textrm{RGB}}$} and PSNR{$^{\textrm{layer}}_{\textrm{alpha}}$}, by comparing the reconstructed transparent layers with the ground-truth transparent layers. For the system-level comparisons, we conduct a user study to assess the quality of the composed image and transparent layers from four aspects: visual aesthetics, prompt adherence, typography, and inter-layer harmonization.

For fair comparisons, we use the layout predicted by our anonymous region layout planner model for the system-level comparison experiments, while the human-provided anonymous layout is used by default for all ablation studies, unless otherwise specified.


\subsection{System-level Comparisons}\label{exp:system_level}

We report the system-level comparisons with state-of-the-art methods in photorealistic image space (evaluated on \photobenchmark) and graphic design space (evaluated on \designbenchmark). 

\vspace{1mm}
\noindent\textbf{Comparison to LayerDiffuse}. We first compare our approach with the latest multi-layer generation method, LayerDiffuse~\cite{zhang2024transparent}, in the multi-layer real image generation benchmark, \ie, \photobenchmark. We conduct a user study involving 30 participants with diverse backgrounds in AI, graphic design, art, and marketing, each evaluating 50 pairs of multi-layer transparent images generated by our ART and LayerDiffuse across three aspects: harmonization, aesthetics, and prompt following.
The results of the user study are illustrated in \Cref{fig:user_study}. We observe that our approach significantly outperforms LayerDiffuse across all three dimensions.

\vspace{1mm}
\noindent\textbf{Comparison to COLE}. We further conduct a user study to compare our approach with the multi-layer graphic design image generation method COLE~\cite{jia2023cole}. We also ask the same 30 participants to evaluate the organization of the elements (layout), the visual appeal (aesthetics), the correctness of the text (typography), and the coherence and quality of each layer (harmonization), with each user evaluating 50 image pairs. The results in \Cref{fig:user_study} reveal that our approach achieves significantly better multi-layer image generation results in various aspects, except for typography, as the text in COLE is rendered with typography render.

\noindent\textbf{More results}. 
We present more multi-layer image generation in \Cref{fig:only_ours} (up to 30 layers), as well as qualitative comparison results with COLE and LayerDiffuse in \Cref{fig:colr}.


\subsection{Ablation Study and Analysis}
\label{sec:ablation}

\vspace{1mm}
\noindent\textbf{Anonymous Region Layout is Sufficient}. We first address the key question of whether region-specific prompts are necessary for multi-layer image generation tasks by comparing the conventional semantic layout and our anonymous region layout. For the semantic layout, we generate region-specific prompts for each layer using the LLaVA 1.6 model~\cite{liu2023improvedllava} and ensure that the visual tokens of each region mainly attend to their respective regional prompts. To ensure a fair comparison, we utilize the ground-truth layout provided by our \designbenchmark while maintaining consistency across all other experimental settings, differing only in the use of region-specific prompts.


Table~\ref{tab:ablation:anonymous_layout_vs_semantic_layout} provides a detailed comparison of the results. We find that the FID$\scriptstyle \text{merged}$ scores for both methods are comparable, while the PSNR score for the anonymous region layout is significantly higher. This suggests superior layer coherence and global harmonization in our approach. Additionally, we employ GPT-4o to evaluate both methods in terms of global harmonization, arriving at the consistent conclusion that our approach yields better layer coherence. One potential reason for the lower coherence in the semantic layout approach is the conflict between local region-specific prompts and global visual tokens. We provide a deeper analysis of these conflicts in the supplementary material.

In addition, we present a statistical analysis comparing the inferred label assignments for the anonymous regions generated by our ART model with the human-annotated region-wise prompts. Our findings reveal that over 80\% of the inferred labels align with the human annotations, suggesting that the generative models have acquired prior knowledge akin to Schema Theory. Additional details can be found in the supplementary material.




\begin{table}[t]
\begin{minipage}[t]{1\linewidth}  
\centering 
\tablestyle{6pt}{1.1}
\resizebox{0.99\linewidth}{!}
{
\begin{tabular}{l|c|cc|c}  
    method & FID$\scriptstyle \text{merged}$  & PSNR & SSIM & Harmonization Score (GPT-4o) \\
    \shline
    Semantic Layout & \underline{17.51} & 17.71 & 0.8443   & 3.67 \\
    Anonymous Region Layout  & 17.79 & \underline{22.90} & \underline{0.9021} & \underline{3.92} \\
\end{tabular}
}
\vspace{-2mm}
\caption{\footnotesize{Anonymous Region Layout \vs Semantic Layout.}}
\label{tab:ablation:anonymous_layout_vs_semantic_layout}
\vspace{2mm}
\end{minipage}
\begin{minipage}[t]{1\linewidth}  
\centering 
\tablestyle{12pt}{1.1}
\resizebox{0.99\linewidth}{!}
{
\begin{tabular}{c|c|cc|c}  
composed image pred. & FID$\scriptstyle \text{merged}$ & PSNR & SSIM & Inference speed (s)\\
\shline
\xmark & 20.44 & - & - & \underline{19.20} \\
\cmark & \underline{17.79}  & 22.90 & 0.9021 & 26.62 \\
\end{tabular}
}
\vspace{-2mm}
\caption{
\footnotesize{Composed image prediction improves the image quality.} }
\label{tab:ablation:compose_img_predict}
\vspace{2mm}
\end{minipage}
\begin{minipage}[t]{1\linewidth}  
\centering 
\tablestyle{15pt}{1.1}
\resizebox{0.99\linewidth}{!}
{
\begin{tabular}{l|c|cc}  
attention type & FID$\scriptstyle \text{merged}$ & PSNR & SSIM  \\
    \shline
    Full Att.  & 41.35 & 16.87 & 0.7738   \\
    Spatial Att.  + Temporal Att. & 167.99  & 16.92 & 0.7985  \\
    Regional Full Att.  & \underline{17.79} & \underline{22.90} & \underline{0.9021} \\
\end{tabular}
}
\vspace{-2mm}
\caption{\footnotesize{Full Att. \vs Spatial Att. + Temporal Att. \vs \textit{Regional Full Att.}}}
\label{tab:ablation:tight_crop}
\vspace{-3mm}
\end{minipage}
\end{table}

\begin{figure}[!t]
\begin{minipage}[!t]{1\linewidth}
\centering
\includegraphics[width=1\textwidth]{fig/efficiency_compare.pdf}
\end{minipage}
\vspace{-2mm}
\caption{\footnotesize{
Illustrating the efficiency comparison of three different attention mechanism design: our Regional Full Attention (marked as Regional Full Att.), Full Attention (marked as Full Att.) and Spatial + Temporal Attention (marked as Spa + Temp Att.). The GPU memory consumption and inference time are evaluated and averaged over 100 samples at a resolution of 1024$\times$1024, for each given number of layers. Some data points are represented with dashed lines or are not shown due to the OOM issue.
}}
\label{fig:efficiency}
\vspace{-2mm}
\end{figure}

\vspace{1mm}
\noindent\textbf{The Benefits of Predicting the Reference Composed Image}. We introduced an additional prediction of the reference composed image for two main reasons. First, it improves coherence across multiple image layers by facilitating bidirectional information propagation between the composed image and each transparent layer. Second, it provides a mechanism to evaluate the quality and consistency of the predicted transparent layers by calculating the PSNR and SSIM scores between the reference image and the layer-merged image on the validation set. As shown in Table~\ref{tab:ablation:compose_img_predict}, our results demonstrate the significance of predicting the composed image as a reference, leading to enhanced image quality as indicated by the FID$\scriptstyle \text{merged}$ score, albeit with a slight increase in inference time.


\begin{table}[t]
\begin{minipage}[t]{1\linewidth}  
\centering 
\tablestyle{20pt}{1.1}
\resizebox{0.99\linewidth}{!}
{
\begin{tabular}{l|c|cc}  
PE method & FID$\scriptstyle \text{merged}$ & PSNR & SSIM \\
\shline
2D-RoPE &  124.3 & 11.99 & 0.4265  \\
2D-RoPE + LayerPE & 20.66  & \underline{23.23} & \underline{0.9101}  \\
3D-RoPE & \underline{17.79} & 22.90 & 0.9021 \\
\end{tabular}
}
\vspace{-2mm}
\caption{
\footnotesize{Different position embedding scheme.}}
\vspace{2mm}
\label{tab:ablation:pe_choice}
\end{minipage}
\begin{minipage}[t]{1\linewidth}  
\centering 
\tablestyle{25pt}{1.1}
\resizebox{0.99\linewidth}{!}
{
\begin{tabular}{l|c|cc}
\# samples & FID$\scriptstyle \text{merged}$ & PSNR & SSIM \\
\shline
80 & 30.38 & \underline{23.18} & 0.8893  \\
800 & 18.89  & 20.45 & 0.8609 \\
8k & 18.06  & 22.43 & 0.8882  \\
80k & 18.04  & 23.13& \underline{0.9081}  \\
800k & \underline{17.79} & 22.90 & 0.9021 \\
\end{tabular}
}
\vspace{-2mm}
\caption{
\footnotesize{Increasing the dataset scale improves performance.}}
\label{tab:ablation:data_scale}
\vspace{3mm}
\end{minipage}
\begin{minipage}[c]{1\linewidth}
\centering
\tablestyle{18pt}{1.1}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|cccc}
\#layer numbers & 3$\sim$8 & 9$\sim$12 & 13$\sim$15 & 16$\sim$51 \\
\shline
FID$\scriptstyle \text{merged}$ & 49.83 & 47.19 & 44.56 & 42.40 \\
\end{tabular}}
\vspace{-2mm}
\caption{
\footnotesize{Effect of different layer numbers.}}
\vspace{3mm}
\label{tab:ablation:layer_number}
\end{minipage}
\hfill
\begin{minipage}[c]{1\linewidth}
\centering
\tablestyle{17pt}{1.1}
\resizebox{1\linewidth}{!}{
\begin{tabular}{l|cccc}
\#text tokens & 23$\sim$58 & 59$\sim$83 & 84$\sim$159 & 160$\sim$272 \\
\shline
FID$\scriptstyle \text{merged}$ & 27.70 & 26.98 & 28.66 & 28.22 \\
\end{tabular}}
\vspace{-2mm}
\caption{
\footnotesize{Effect of different caption length.}}
\label{tab:ablation:text_number}
\end{minipage}
\vspace{-5mm}
\end{table}

\vspace{1mm}
\noindent\textbf{Regional Full Attention v.s. Full Attention v.s. Spatial + Temporal Attention}. A key design element of our approach is the ceiling-aligned tight crop for each transparent layer, which removes most transparent pixels and compels the diffusion model to focus on the smallest rectangle encapsulating the non-transparent foreground regions. We refer to this as the Regional Full Attention scheme. This design is crucial for improving efficiency and explicitly constrains layer predictions to align with the positions specified by the anonymous region layout. We also evaluate two additional baselines: the Full Attention scheme, which does not apply regional cropping, and the Spatial Attention + Temporal Attention scheme, which introduces temporal attention to facilitate interactions across different layers, similar to architectural designs in video generation~\cite{blattmann2023align,guo2023animatediff}. Detailed comparison results are presented in Table~\ref{tab:ablation:tight_crop}, where our method demonstrates superior FID$\scriptstyle \text{merged}$ scores. The primary factor behind our improved performance is the use of the anonymous region layout.

Moreover, Figure~\ref{fig:efficiency} shows that our method maintains nearly constant computational costs when processing between 10 and 50 layers, whereas the Full Attention scheme, lacking regional cropping, exhibits quadratic growth in memory and inference costs.


\begin{table}[t]
	\begin{minipage}[t]{1\linewidth}  
		\centering 
		\tablestyle{2pt}{1.1}
		\resizebox{0.99\linewidth}{!}
		{
			\begin{tabular}{l|c|cc|c}  
				Method  &  FID$\scriptstyle \text{merged}$ & PSNR & SSIM & Inference speed (s) \\
				\shline
				GPT-4o & 20.72 & 22.80 & 0.9078 & - \\
				LayoutGPT~\cite{feng2024layoutgpt} & 20.92 & \underline{23.18} & \underline{0.9113} & -  \\ \hline
				Semantic Layout Planner & 21.45 & 17.69 & 0.8382 & 19.19 \\
				Semantic Layout Planner\dag & 20.63 & 22.90 & 0.9092 & 19.19 \\
				Anonymous Region Layout Planner & \underline{19.90} & 22.70 & 0.9038 & \underline{5.68} \\
			\end{tabular}
		}
		\vspace{-2mm}
		\caption{
			\footnotesize{Anonymous region layout planner v.s. semantic layout planner and other planner alternatives. \dag\, means that we remove the predicted region-specific prompts and only use the predicted bounding boxes.}}
		\label{tab:ablation:layout_compare}
	\end{minipage}
	\vspace{-2mm}
\end{table}

\begin{table}[t]
\begin{minipage}[t]{1\linewidth}  
\centering 
\tablestyle{9pt}{1.1}
\resizebox{0.99\linewidth}{!}
{
\begin{tabular}{l|ccc|c}  
PE method & PSNR{$^{\textrm{layer}}_{\textrm{rgb}}$} & PSNR{$^{\textrm{layer}}_{\textrm{alpha}}$} & PSNR & FID$\scriptstyle \text{merged}$ \\
\shline
2D-AbsPE & 26.91 & 18.42 & 26.06 & 17.04 \\
2D-AbsPE + LayerPE & 26.98 & 18.76 & 26.11 & 16.24 \\
2D-RoPE & 34.05 & 23.08 & 30.09 & 3.16 \\
2D-RoPE + LayerPE & 34.46 & 23.31 & 30.13 & 3.10 \\
3D-RoPE & \underline{34.89} & \underline{23.85} & \underline{30.48} & \underline{2.84} \\
\end{tabular}
}
\vspace{-2mm}
\caption{
\footnotesize{Position embedding scheme in multi-layer decoder.}}
\label{tab:ablation:pe_multi_layer_decoder}
\vspace{2mm}
\end{minipage}
\begin{minipage}[t]{1\linewidth}  
\centering 
\tablestyle{8pt}{1.1}
\resizebox{0.99\linewidth}{!}
{
\begin{tabular}{cc|ccc|c}  
composed image & bg image & PSNR{$^{\textrm{layer}}_{\textrm{rgb}}$} & PSNR{$^{\textrm{layer}}_{\textrm{alpha}}$} & PSNR & FID$\scriptstyle \text{merged}$ \\
\shline
\xmark & \xmark & 33.25 & 22.82 & 29.35 & 3.76 \\
\cmark & \xmark & 33.25 & 21.95 & 29.39 & 3.53 \\
\xmark & \cmark & 34.37 & 23.39 & 30.20 & 3.06 \\
\cmark & \cmark & \underline{34.89} & \underline{23.85} & \underline{30.48} & \underline{2.84} \\
\end{tabular}
}
\vspace{-2mm}
\caption{
\footnotesize{Condition choice for the multi-layer decoder.}}
\vspace{2mm}
\label{tab:ablation:condition_choice}
\end{minipage}
\begin{minipage}[t]{1\linewidth}  
\centering 
\tablestyle{6pt}{1.1}
\resizebox{0.99\linewidth}{!}
{
\begin{tabular}{lc|ccc|c}  
Method & Multi layer & PSNR{$^{\textrm{layer}}_{\textrm{rgb}}$} & PSNR{$^{\textrm{layer}}_{\textrm{alpha}}$} & PSNR & FID$\scriptstyle \text{merged}$ \\
\shline
LayerDiffuse~\cite{zhang2024transparent} & \xmark & 20.94 & 18.48 & 26.51 & 4.27 \\
Flux-RGBA decoder & \xmark & 30.25 & 20.11 & 27.74 & 5.23 \\
Ours & \cmark & \underline{34.89} & \underline{23.85} & \underline{30.48} & \underline{2.84} \\
\end{tabular}
}
\vspace{-2mm}
\caption{
\footnotesize{Comparison with existing transparency decoder.}}
\label{tab:ablation:sota_decoder_compare}
\end{minipage}
\vspace{-3mm}
\end{table}

\vspace{1mm}
\noindent\textbf{Layer-aware Position Encoding is Critical.} Encoding positional information is essential for the model to distinguish visual tokens from different transparent layers. Our empirical analysis shows that incorporating layer position information is crucial, with the proposed 3D-RoPE scheme outperforming the absolute layer position encoding method. The full comparison results are presented in Table~\ref{tab:ablation:pe_choice}.


\vspace{1mm}
\noindent\textbf{More Multi-layer Data Brings Better Performance.} Table~\ref{tab:ablation:data_scale} reports the detailed experimental results when training with datasets of varying scales. We observe that our approach benefits from a larger dataset scale. One interesting observation is that our ART already achieve strong results with just 8K training samples, demonstrating that our approach is also data efficient.

\vspace{1mm}
\noindent\textbf{Effect of number of transparent layers and the complexity of the scenarios described in the text.}
We study whether our ART performs robustly across various input complexities by partitioning the test set into different groups according to the number of transparent layers and the number of text tokens (which reflects the complexity of the scenarios) and report the quantitative comparison results on these subsets in Table~\ref{tab:ablation:layer_number} and Table~\ref{tab:ablation:text_number}. 
We can see that our ART achieves even better performance with an increasing number of transparent layers and slightly weaker performance when handling longer text tokens. We attribute this to the distributions of these factors in the training set.

\vspace{1mm}
\noindent\textbf{Multi-layer Natural Image Generation Results.}
Our approach can be directly applied to multi-layer natural image generation without any modifications, given access to a high-quality multi-layer natural image dataset. To this end, we show that our ART achieves strong results even when fine-tuned on only a 20 curated high-quality multi-layer natural images. Figure~\ref{fig:semi_tran} shows some qualitative results and we believe the results can continue to improve with access to more high-quality multi-layer natural images.


\vspace{1mm}
\noindent\textbf{Anonymous Region Layout Planner v.s. Semantic Layout Planner.} We fine-tune both an anonymous layout planner and a semantic layout planner using data sampled from the 800K training dataset and evaluate their performance by integrating them with our ART model. Additionally, we include two strong baselines, GPT-4o and LayoutGPT~\cite{feng2024layoutgpt}, which support transforming the global prompt into a usable layout. Detailed results are presented in Table~\ref{tab:ablation:layout_compare}. Our Anonymous Region Layout Planner not only achieves a better FID$\scriptstyle \text{merged}$ score but also operates more than 3$\times$ faster than the Semantic Layout Planner.
Interestingly, removing the region-specific prompts of the semantic layout planner can enhance overall performance by avoiding conflicts among region-wise prompts, especially regarding layer coherence, as reflected by the higher PSNR scores.


\vspace{1mm}
\noindent\textbf{RoPE is Critical for Multi-layer Decoder Quality.}
Table~\ref{tab:ablation:pe_multi_layer_decoder} summarizes the results of the comparison experiments involving different position embedding schemes for the multi-layer transparency decoder. The original ViT pre-trained on the ImageNet classification task employs absolute position encoding, which is inadequate for capturing positional information across a variable number of transparent layers. We find that simply adding an additional set of layer-wise absolute position embeddings provides minimal improvement; however, replacing the absolute position encoding with the RoPE scheme significantly enhances decoding quality. We observe that the 3D-RoPE scheme achieves the best FID$\scriptstyle \text{merged}$ score, which aligns with our findings regarding the choice of position encoding scheme for the latent features sent into MMDiT. Consequently, we adopt the 3D-RoPE scheme as default.

\begin{figure}[!t]
\begin{minipage}[!t]{1\linewidth}
    \begin{subfigure}[b]{1\textwidth}
    \centering
\includegraphics[width=\linewidth]{fig/semi_transparent.pdf}
\vspace{-2mm}
\end{subfigure}
\vspace{-6mm}
\caption{Multi-layer natural image generation results.}
\label{fig:semi_tran}
\vspace{3mm}
\end{minipage}
\begin{minipage}[!t]{1\linewidth}
\begin{minipage}[!t]{0.25\linewidth}
    \begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[width=0.96\textwidth]{fig/decoder_method_comparison/0687-draw-0.png}
    \caption{\footnotesize{Ground-truth}}
    \end{subfigure}
\end{minipage}%
\begin{minipage}[!t]{0.25\linewidth}
    \begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[width=0.96\textwidth]{fig/decoder_method_comparison/0687-draw-1.png}
    \caption{\footnotesize{LayerDiffuse}}
    \end{subfigure}
\end{minipage}%
\begin{minipage}[!t]{0.25\linewidth}
    \begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[width=0.96\textwidth]{fig/decoder_method_comparison/0687-draw-2.png}
    \caption{\footnotesize{Flux-RGBA}}
    \end{subfigure}
\end{minipage}%
\begin{minipage}[!t]{0.25\linewidth}
    \begin{subfigure}[b]{1\textwidth}
    \centering
    \includegraphics[width=0.96\textwidth]{fig/decoder_method_comparison/0687-draw-4.png}
    \caption{\footnotesize{Ours}}
    \end{subfigure}
\end{minipage}
\vspace{-3mm}
\caption{\footnotesize{Comparison with existing transparency decoder.}}
\label{fig:sota_decoder_compare}
\vspace{2mm}
\end{minipage}
\vspace{-6mm}
\end{figure}

\vspace{1mm}
\noindent\textbf{Composed Image as Condition.} Although we only need to decode the transparency for all the foreground transparent layers, we empirically find that sending both the merged entire image and the background image as additional conditions, along with applying supervision on them, leads to even better performance, as shown in Table~\ref{tab:ablation:condition_choice}. We hypothesize that the information from the merged and background images is beneficial for the transparency layers to interact more effectively, ensuring a more coherent final composed image with these transparent layers.

\vspace{1mm}
\noindent\textbf{Comparison with Previous Transparency Decoder.}
We compare our multi-layer transparency decoder with the previous transparency decoder and two strong baselines designed for single-layer transparency decoding, as shown in Table~\ref{tab:ablation:sota_decoder_compare}. We utilize the officially released weights of the transparency decoder proposed by LayerDiffuse~\cite{zhang2024transparent}. For the Flux-RGBA decoder, we modify the output projection to support an additional alpha layer prediction and fine-tune the decoder using our dataset. Our design achieves the best FID$\scriptstyle \text{merged}$ score as shown in Table~\ref{tab:ablation:sota_decoder_compare}. The qualitative comparison results presents in Figure~\ref{fig:sota_decoder_compare}.