\section{Related work}
\label{sec:related_work}
\noindent\textbf{Multi-Layer Transparent Image Generation} has primarily been approached through two different paths. The first path focuses on generating all image layers simultaneously. Along this path, Text2Layer~\cite{zhang2023text2layer} adapts the Stable Diffusion model into a two-layer generation model, enabling the simultaneous generation of a background layer accompanied by a foreground layer. LayerDiff~\cite{huang2024layerdiff} designs a layer-collaborative diffusion model to generate up to four layers at once under the guidance of both global prompts and layer prompts.
The second path generates multiple image layers sequentially. For instance, LayerDiffuse~\cite{zhang2024transparent} introduces a background-conditioned transparent layer generation model, which generates image layers iteratively. COLE~\cite{jia2023cole} and OpenCOLE~\cite{inoue2024opencole} start from a brief user-provided prompt and employ multiple LLMs and diffusion models to generate each element within the final image step by step.
Unlike most of the aforementioned works, which only support generating a limited number of transparent layers, our approach allows for the generation of tens of transparent layers using an anonymous region transformer design. We also empirically demonstrate the advantages of our approach over these methods for photorealistic and design-oriented multi-layer image generation tasks.

\noindent\textbf{Layout Generation and Layout Control} for image generation tasks have attracted significant attention due to their broader applications. We can categorize most existing efforts into two groups: designing better layout generation models and controlling image generation with a given layout prior.
The first approach focuses on generating a reasonable layout given a set of visual elements. For example, Graphist~\cite{cheng2024graphic}, Visual Layout Composer~\cite{shabani2024visual}, and MarkupDM~\cite{kikuchi2024multimodal} propose different methods to generate layouts based on a set of transparent visual layers. Readers can refer to~\cite{feng2024layoutgpt,yamaguchi2021canvasvae,inoue2023layoutdm,chai2023layoutdm,hui2023unifying,kong2022blt,cheng2023play,tang2023layoutnuwa,jiang2022coarse,jiang2023layoutformer++,weng2024desigen,wang2023dolfin,guerreiro2025layoutflow,yang2024posterllava,inoue2023towards,chen2024textlap,fontanella2024generating,braunstein2024slayr} for more discussion on the development of various layout generation models.
In the second approach, researchers focus on enhancing the compositional generation capability of diffusion models by specifying what objects to generate and where to place them on the canvas. Several representative works include GLIGEN~\cite{li2023gligen}, InstanceDiffusion~\cite{wang2024instancediffusion}, and MS-Diffusion~\cite{wang2024msdiffusion}, which introduce different methods to inject positional information into diffusion models. Other efforts, such as~\cite{bar2023multidiffusion,yang2024mastering,kim2023dense,omost,sarukkai2024collage,zhang2024itercomp}, propose training-free schemes, post-training schemes, or harmonization enhancement designs. Among these efforts, LayoutGPT~\cite{feng2024layoutgpt} and TextLap~\cite{chen2024textlap} are the closest works that support predicting the semantic layout from a global text prompt. We empirically demonstrate the advantages of our anonymous region layout planner on multi-layer transparent image generation.