\section{Related work}
\paragraph{\bf Aggregate bandit feedback with stochastic i.i.d losses\hspace{-0.5em}} was first studied by \citet{efroni2021reinforcement} who obtained regret of $\tilde O(H^{3/2}S^2 A^{3/2} \sqrt{K})$ with an efficient algorithm. Their transition and loss functions are not horizon-dependent. Adapting their bound for horizon-dependent losses and transitions as we consider here would effectively inflate the number of states by a factor of $H$, resulting in a regret bound of $\tilde O(H^{7/2} S^2 A^{3/2} \sqrt{K})$. \citet{cassel2024near} introduced the first algorithm for Linear MDPs with stochastic losses and aggregate bandit feedback. Their algorithm attains regret of $\tilde O(\sqrt{d^5 H^7 K})$ where $d$ is the dimension of the feature map. In the special case of tabular MDPs, they show a regret of $\tilde O (H^{7/2} S^2 A^{3/2} \sqrt{K})$.

\paragraph{\bf Adversarial Linear bandits\hspace{-0.5em}} (see for example \citet{lattimore2020bandit}) is a variant of the classical Multi-armed Bandit problem where each action is associated with a vector in $\bbR^d$. The loss in each round is the inner product of the action with an unknown parameter vector chosen by an adversary. Through the concept of \textit{occupancy measures}, online MDPs with aggregate bandit feedback and known dynamics can be seen as a special case of Adversarial Linear bandits. In terms of regret bounds,
EXP2 \citep{dani2007price,cesa2012combinatorial} with a specific exploration distribution achieves the optimal bound of $\Theta(B \sqrt {dK \log N})$ for any finite set of $N$ actions in $\bbR^d$, where $B$ is a bound on the losses \citep{bubeck2012towards}. 
Using a discretization argument, this bound can be extended to $\tilde O(B d \sqrt{K})$ for any compact convex set. However, the EXP2 algorithm is not efficient in general.
% 
The latter bound for general convex set is attainable with efficient algorithms (polynomial in $d$) under mild assumptions \citep{hazan2016volumetric}. When using occupancy measures to reduce online MDPs with aggregate bandit feedback to Linear bandits, the decision set is of dimension $d = HSA$, the bound of the loss in each round is $B=H$ and the number of deterministic policies is $N = A^{SH}$. This results in regret of $\tilde O(H^2 S \sqrt{A K})$ and $\tilde O(H^2 S A \sqrt{K})$ with inefficient and efficient algorithms, respectively. For the known dynamics case, we improve these bounds to optimal $\tilde \Theta(H^2  \sqrt{S A K})$ regret with an efficient and more natural algorithm.

As mentioned before, \citet{cohen2021online} have extended the linear bandit model to \textit{Distorted Bandit Online Linear Optimization} (DBOLO). They show that online MDPs with aggregate bandit feedback and \textit{unknown} dynamics can be reduced to DBOLO efficiently. Their algorithm is built upon the SCRIBLE algorithm \citep{abernethy2008competing} and guarantees regret of $\tilde O(H^5 S^6 A^{5/2} \sqrt{K})$. On the same setting, we improve their regret bound to $\tilde O(H^3 S \sqrt{A K})$.
% \\
% \\
\paragraph{\bf Regret minimization in MDPs with semi-bandit feedback\hspace{-0.5em}} is extensively studied in the literature, initiated with the seminal UCRL algorithm \citep{jaksch2010near} for stochastic losses. Their model was later extended to the more general Online (adversarial) MDPs where the loss functions are arbitrarily chosen by an adversary. Most algorithms for this model are based either on the framework of occupancy measures  \citep{zimin2013online,rosenberg2019bandit,rosenberg2019online,jin2019learning} or the Policy Optimization framework \citep{even2009online,shani2020optimistic,luo2021policy}. In the adversarial model with semi-bandit feedback and known dynamics, the optimal regret bound is $\tilde \Theta (H\sqrt{S A K})$ case and is attained by an occupancy-measure based algorithm \citep{zimin2013online}. With PO, the state-of-the-art regret under known dynamics is $\tilde O (H^{2}\sqrt{S A K})$. We achieve the same bound with aggregate bandit feedback which in this case is optimal. 
Under unknown dynamics, the best known bound is $\tilde O (H^{2} S\sqrt{A K})$ and is also attained by an occupancy-measure based algorithm \citep{jin2019learning}, while the best known lower bound is $\Omega(H^{3/2} \sqrt{S A K})$ \citep{jin2018q}. With policy optimization algorithm, the best known bound is $\tilde O (H^{3} S\sqrt{A K})$ \citep{luo2021policy}. Although we are in a setting with less informative feedback, we match the latter bound.
% 

\begin{remark}
    \label{remark:loop-free}
    We note that some of the literature on semi-bandit feedback, such as \citet{jin2019learning, luo2021policy}, assumes \textit{loop-free} MDPs. Under this assumption the state space consists of $H$ disjoint sets $\calS = \calS_1 \cup \calS_2 \cup \dots \cup \calS_H$ such that in step $h$ the agent can only be found in states from the set $\calS_h$.
    Effectively, this means that their state space is larger than ours by a factor of $H$.
    So for example the regret bound $\tilde O(H^2 S \sqrt{A K})$ in \citet{luo2021policy} implies a bound of $\tilde O(H^3 S \sqrt{A K})$ in the transition model presented in this paper.
    We emphasize that these differences are rather artificial and not due to an actual difference in the regret.
\end{remark}



\paragraph{\bf Stochastic binary trajectory feedback\hspace{-0.5em}} was studied in \citet{chatterji2021theory}. In their model, the  rewards are drawn
from a logistic model that depends on features of the trajectory.


\paragraph{\bf Preference-based RL (PbRL)\hspace{-0.5em}} is a model where the feedback is given in terms of preferences over a trajectory pair instead of rewards. This is partially related to our model in motivation. A partial list of works on PbRL includes \citep{saha2023dueling,chen2022human,wu2023making}. For additional related work on the topic, see the above references.




\begingroup  % Start of the group for local settings
\renewcommand{\thefootnote}{\fnsymbol{footnote}}  % Change footnotes to symbols

% \setlength{\tabcolsep}{3pt} % Adjust horizontal spacing
% \renewcommand{\arraystretch}{1} % Adjust vertical spacing

\begin{table}
    \caption{Comparison of regret bounds for online MDPs with aggregate bandit feedback. 
    % In the 'Loss' column, 'adv.' denotes adversarial (non-stochastic) losses, while 'stoc.' denotes stochastic i.i.d losses. 
    The regret bounds presented in this table ignore logarithmic and low-order terms.}
    \begin{center}
        \begin{tabular}[c]{|l|c|c|l|}
            \hline
            Algorithm & Dynamics & Loss & \makecell{Regret} 
            \\ 
            \hline \hline
            \makecell[l]{Reduction to Efficient\\ Linear Bandits algorithm} & known & adversarial & $ \sqrt{H^{4} S^2 A^2 K}$
            \\
            \hline
            \makecell[l]{\Cref{alg:tabular-known-p main} \textbf{(ours)}} & known & adversarial & $ \sqrt{H^{4} S A K}$
            \\
            \hline
            \makecell[l]{Lower bound} & known & adversarial & $ \sqrt{H^{4} S A K}$
            \\
            \hline \hline 
            \makecell[l]{UCBVI-TS \\ \citep{efroni2021reinforcement}} & unknown & stochastic & $ \sqrt{H^{7} S^{4} A^{3} K}$ \footnotemark[3]
            \\
            \hline 
            \makecell[l]{REPO for tabular MDPs \\ \citep{cassel2024near}} & unknown & stochastic & $ \sqrt{H^{7} S^{4} A^{3} K}$
            \\
            \hline \hline
            \makecell[l]{Reduction to DBOLO \\ \citep{cohen2021online}} & unknown & adversarial & $ \sqrt{H^{10} S^{12} A^{5} K}$
            \\
            \hline
            \makecell[l]{\Cref{alg:tabular-unknown-p main} \textbf{(ours)} } & unknown & adversarial & $ \sqrt{H^{6} S^{2} A K}$
            \\
            \hline
            % \footnotemark[2]
        \end{tabular}
        \label{table: comparison}
    \end{center}

\end{table}
\footnotetext[3]{This regret bound is adapted to horizon-dependent transition and losses - see the related work section for more details.}

\endgroup