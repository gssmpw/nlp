\section{Proof of Strong Facet-Merging}
\label{appendix_facesplitting}

In this appendix we will prove the Strong Facet-Merging proposition, which is reproduced below.

\StrongFS*

To facilitate the proof, in this appendix we will use the language of pDAGs instead of that of mDAGs. Note that, as a special case, this is an alternative proof of the original Evans's Proposition (different from the proof presented in \cite{evans_graphs_2016}).

We start with Lemma \ref{lemma_dominance_bayesian_updating}, that generalizes the idea that a causal structure where the nodes of the set $C$ and their parents are settings for a latent variable is at least as powerful as a causal structure where the nodes of the set $C$ are causally influenced by that latent variable. An example of the application of Lemma \ref{lemma_dominance_bayesian_updating} is presented in Figure \ref{fig_bayesian_update}. 


\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{images/bayesian_lemma_2.pdf}
    \caption{By Lemma \ref{lemma_dominance_bayesian_updating}, we have $\mathcal{G}_2\succeq \mathcal{G}_1$. Here, $C_1=\{t,u\}$, $C_2=\{u,v\}$, and $D=\{d\}$. Note that these two pDAGs are \emph{not} observationally equivalent: for example, $\mathcal{G}_1$ has the d-separation relation $x\dsep d$, that is not presented by $\mathcal{G}_2$. Edges that point into a latent node are represented in red.}
    \label{fig_bayesian_update}
\end{figure*}



\begin{lemma}[Dominance by Bayesian Updating]
\label{lemma_dominance_bayesian_updating}
Let $\mathcal{G}_1$ be a pDAG that has a set of parentless latent variables $\{l^{CD}_i\}_{i=1,...,n}$, and let the children of $l^{CD}_i$ be $ch_{\mathcal{G}_1}(l_i)=C_i\cup D$, such that $\cup_i C_i$ and $D$ are disjoint. All the nodes in $\{l^{CD}_i\}_{i=1,...,n}$ share the children $D$, and the sets $C_i$, $i=1,...,n$ do not have to be disjoint from each other. Further suppose that none of the nodes in $D$ is an ancestor of any node $c\in \cup_i C_i$.

Let $\mathcal{G}_2$ be another pDAG that is constructed from ${\mathcal{G}_1}$ by deleting all the nodes $\{l^{CD}_i\}_{i=1,...,n}$, adding a latent node $l^D$ whose children are $ch_{\mathcal{G}_2}(l_D)=D$ and whose parents are $pa_{\mathcal{G}_2}(l_D)= \cup_i ( C_i\cup pa_{\mathcal{G}_1}(C_i) )\setminus \{l^{CD}_i\}_{i=1,...,n}$ and adding latent common causes $l^C_i$, $i=1,...,n$  that have no parents and are such that $ch_{\mathcal{G}_2}(l^{C}_i)=C_i$. 


Then, $\mathcal{G}_2$ observationally dominates $\mathcal{G}_1$, i.e., $\mathcal{G}_2 \succeq \mathcal{G}_1$.
\end{lemma}
\begin{proof}
    For simplicity of notation, let $C\equiv \cup_i C_i$, $L^{CD}\equiv \{l^{CD}_i\}_{i=1,...,n}$ and $L^C\equiv \{l^{C}_i\}_{i=1,...,n}$.
    
    The idea of this lemma is that, due to $D$ not being part of the ancestors of $C$, one can assess the values of $X_C$ before assessing the values of $X_D$. The knowledge of $X_C$ in $\mathcal{G}_1$ will thus teach us something about the shared common causes $X_{L^{CD}}$ through Bayesian updating. As it turns out, this will be at most as powerful as $\mathcal{G}_2$, where $C$ and its parents act as settings for the latent variable $l^D$. 

    We will show this by explicitly writing down the Markov realizability conditions for each one of the pDAGs. The Markov realizability conditions are equivalent to Eq.~\eqref{eq_realizability}, but written in terms of conditional probability distributions instead of in terms of functional dependences and error variables. It might be helpful to follow the proof below while looking at the example of Figure \ref{fig_bayesian_update}.
    
    To further simplify the notation, let $V^*=\vis(\mathcal{G}_1)\setminus(C\cup D)$ be the set of visible nodes of $\mathcal{G}_1$ and $\mathcal{G}_2$ except for $C$ and $D$, $L^*=\lat(\mathcal{G}_1)\setminus(L^{CD})$ be the set of latent nodes of $\mathcal{G}_1$ except for $L^{CD}$ and $\text{pa}^*_{\mathcal{G}_1}(A)=\text{pa}_{\mathcal{G}_1}(A)\setminus L^{CD}$ be the set of parents of a set of nodes $A$ in $\mathcal{G}_1$ except for $L^{CD}$.    
      A probability distribution over the visible variables that is realizable by $\mathcal{G}_1$ can be factorized as:
    \begin{align}
   &P_{\mathcal{G}_1}(X_C X_D X_{V^*}) = \nonumber \\ & \sum_{X_{L^*}}\sum_{X_{L^{CD}}} P_{\mathcal{G}_1}(X_D|X_{L^{CD}}X_{\text{pa}^*_{\mathcal{G}_1}(D)}) \cdot \nonumber\\
    &\cdot P_{\mathcal{G}_1}(X_C|X_{L^{CD}}X_{\text{pa}^*_{\mathcal{G}_1}(C)})P_{\mathcal{G}_1}(X_{L^{CD}})P_{\mathcal{G}_1}(X_{V^*L^*})
    \label{eq_decomposition_G1}
    \end{align}

    Note that Eq.~\eqref{eq_decomposition_G1} is a coarse-grained version of the equation that determines realizability by $\mathcal{G}_1$: the probability distributions realizable by $\mathcal{G}_1$ obey even more specific constraints than the ones presented by Eq.~\eqref{eq_decomposition_G1}, because for example not all elements of $C$ are necessarily children of every element of $\text{pa}_{\mathcal{G}_1}(C)$.
    
    To do the same for $\mathcal{G}_2$, note that $L^*$ is the set of latent nodes of $\mathcal{G}_2$ except for $L^C$ and $l^D$, and that $\text{pa}^*_{\mathcal{G}_1}(A)=\text{pa}_{\mathcal{G}_2}(A)\setminus (L^{C}\cup\{l^D\})$ . Furthermore, define  $L^{C}_c\subseteq L^{C}$ as the set of elements of $L^C$ that are parents of $c\in C$. For example, in Figure \ref{fig_bayesian_update}(b), $L^{C}_T=\{l_1^C\}$ and $L^{C}_U=\{l_1^C,l_2^C\}$. With this, for $\mathcal{G}_2$ we have:
    \begin{align}
    &P_{\mathcal{G}_2}(X_C X_D X_{V^*}) = \nonumber \\& \sum_{X_{L^*}} \sum_{X_{L^{C}}} \sum_{X_{l^D}} {P_{\mathcal{G}_2}(X_D|X_{l^{D}}X_{\text{pa}^*_{\mathcal{G}_1}(D)})}\cdot \nonumber \\& \cdot\prod_{c\in C} P_{\mathcal{G}_2}(X_c|X_{L^{C}_c}X_{\text{pa}^*_{\mathcal{G}_1}(c)})P_{\mathcal{G}_2}(X_{L^{C}})\cdot \nonumber\\
    & \cdot P_{\mathcal{G}_2}(X_{l^{D}}|X_CX_{\text{pa}^*_{\mathcal{G}_1}(C)})P_{\mathcal{G}_2}(X_{V^*L^*}).
    \label{eq_decomposition_G2}
    \end{align}

    To show that $\mathcal{G}_2$ can simulate $\mathcal{G}_1$, we need to show that all the probability distributions that can be factorized as in Eq.~\eqref{eq_decomposition_G1} can also be factorized as in Eq.~\eqref{eq_decomposition_G2}. 

    When doing so, we will use the following Bayesian inversion:
    \begin{gather}
        P_{\mathcal{G}_1}(X_C|X_{L^{CD}}X_{\text{pa}^*_{\mathcal{G}_1}(C)})=\nonumber \\ 
        \frac{ P_{\mathcal{G}_1}(X_{L^{CD}}|X_CX_{\text{pa}^*_{\mathcal{G}_1}(C)})P_{\mathcal{G}_1}(X_C|X_{\text{pa}^*_{\mathcal{G}_1}(C)})}{P_{\mathcal{G}_1}(X_{L^{CD}}|X_{\text{pa}^*_{\mathcal{G}_1}(C)})} 
                \label{bayes_identity}
    \end{gather}

    And the fact that $L^{CD}$ is d-separated from the other parents of $C$ in $\mathcal{G}_1$, so $P_{\mathcal{G}_1}(X_{L^{CD}}|X_{\text{pa}^*_{\mathcal{G}_1}(C)})=P_{\mathcal{G}_1}(X_{L^{CD}})$. Substituting this and \eqref{bayes_identity} and on \eqref{eq_decomposition_G1}, we get:
    \begin{align}
    	 & P_{\mathcal{G}_1}(X_C X_D X_{V^*}) \nonumber \\ &=\sum_{X_{L^*}}\sum_{X_{L^{CD}}} P_{\mathcal{G}_1}(X_D|X_{L^{CD}}X_{\text{pa}^*_{\mathcal{G}_1}(D)}) \cdot \nonumber\\
    	&\cdot P_{\mathcal{G}_1}(X_C|X_{\text{pa}^*_{\mathcal{G}_1}(C)})\cdot \nonumber \\ &\cdot P_{\mathcal{G}_1}(X_{L^{CD}}|X_C X_{\text{pa}^*_{\mathcal{G}_1}(C)})P_{\mathcal{G}_1}(X_{V^*L^*}) 
    \label{eq_decomposition_G1_two}
    \end{align}

After lumping all the variables $X_{L^{CD}}$ together in one variable $X_{l^D}$, the only thing that remains for Eq.~\eqref{eq_decomposition_G1_two} to take the form of Eq.~\eqref{eq_decomposition_G2} is to show that $P_{\mathcal{G}_1}(X_C|X_{\text{pa}^*_{\mathcal{G}_1}(C)})$ factorizes as 
\begin{equation}
	\sum_{X_{L^{C}}}\prod_{c\in C} P_{\mathcal{G}_1}(X_c|X_{L^{C}_c}X_{\text{pa}^*_{\mathcal{G}_1}(c)})P_{\mathcal{G}_1}(X_{L^{C}})
\end{equation}
for sets of variables $X_{L^C} =\cup_c X_{L^{C}_c}$. And this is certainly true, because the pDAG obtained by starting from $\mathcal{G}_1$ and marginalizing (i.e., transforming into latent) every node that is not in $C\cup\text{pa}^*_{\mathcal{G}_1}(c)$ has this factorization as part of its Markov realizability condition.

Therefore, $\mathcal{G}_2$ can realize all of the distributions realizable by $\mathcal{G}_1$. Note that the inverse is not true; Fig.~\ref{fig_bayesian_update} presents a counter-example.

    
\end{proof}

With this Lemma at hand, we can nor prove Proposition \ref{Simultaneous_Splitting_Prop}. In pDAG language, this proposition can be rephrased as:

\begin{proposition}[Strong Facet-Merging - pDAG language]
 \label{prop_strong_FS_DAG}
 Let $\mathcal{G}_1$ be a pDAG containing a sequence of latent nodes $\{l^{CD}_i\}_{i=1,...,n}$ that share the children $D\in \cap_{i=1,...,n} ch_{\mathcal{G}_1}(l^{CD}_i)$. Denote $C_i\equiv ch_{\mathcal{G}_1}(l^{CD}_i)\setminus D$.
 
 To simplify the notation, we will use $C\equiv \cup_i C_i$ and $L^{CD}\equiv \{l^{CD}_i\}_{i=1,...,n}$  
 
 If the following condition holds:
\begin{enumerate}
    \item $pa_{\mathcal{G}_1}(C)\cup C\subseteq pa_{\mathcal{G}_1}(d)$ for each $d \in D$. This has to hold for both visible and latent parents.
\end{enumerate}

Then,  ${\mathcal{G}_1}$ is observationally equivalent to the pDAG  ${\mathcal{G}_3}$ defined by starting from ${\mathcal{G}_1}$, removing the nodes $L^{CD}$ and adding the new latent nodes $\{l^{C}_i\}_{i=1,...,n}$ and $l_{D}$, whose children are respectively $ch_{\mathcal{G}_3}(l^{C}_i)=C^i$ for $i=1,...,n$ and  $ch_{\mathcal{G}_3}(l_D)=D$.
\end{proposition}
\begin{proof}
    It might be helpful to look at Figure \ref{fig_strong_FS_proof} while following this proof.
    \begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{images/strong_FS_proof.pdf}
    \caption{The pDAGs $\mathcal{G}_1$ and $\mathcal{G}_3$ can be shown observationally equivalent by Proposition \ref{prop_strong_FS_DAG}:  Lemma \ref{lemma_dominance_bayesian_updating} says that $\mathcal{G}_2\succeq\mathcal{G}_1$, and Exogenization (Lemma \ref{lemma_exogenize_latents}) says that $\mathcal{G}_2\cong\mathcal{G}_3$. We can see that $\mathcal{G}_1\succeq\mathcal{G}_3$ by structural dominance (Lemma \ref{prop_edge_dropping}). Edges that point into a latent node are represented in red.} 
    \label{fig_strong_FS_proof}
\end{figure}

    By structural dominance (Lemma \ref{prop_edge_dropping}), it is easy to see that $\mathcal{G}_1 \succeq \mathcal{G}_3$.
    
    By Lemma \ref{lemma_dominance_bayesian_updating}, the pDAG $\mathcal{G}_1$ here defined is dominated by a pDAG $\mathcal{G}_2$ where the nodes $\{l^{CD}_i\}_{i=1,...,n}$ are substituted by parentless latent nodes $\{l^{C}_i\}_{i=1,...,n}$ that point to each of the $C_i$, and a node $l^D$ that points to the set $D$ and has parents $pa_{\mathcal{G}_2}(l_D)= \cup_i ( C_i\cup pa_{\mathcal{G}_1}(C_i) )\setminus \{l^{CD}_i\}_{i=1,...,n}$. 

    The next step is to exogenize the latent node $l^D$, using Lemma \ref{lemma_exogenize_latents}. Since $pa_{\mathcal{G}_1}(C)\cup C\subseteq pa_{\mathcal{G}_1}(d)$ for each $d \in D$, when we exogenize $l^D$ there are no new edges added; only edges removed (the edges between $\text{pa}_{\mathcal{G}_2}(l^D)$ and $l^D$). With this process, we reach a pDAG $\mathcal{G}_3 \cong \mathcal{G}_2$. This is the pDAG $\mathcal{G}_3$ that appears in the statement of the proposition.

    Thus, we have that $\mathcal{G}_3\cong \mathcal{G}_2\succeq \mathcal{G}_1$, and we already knew that $\mathcal{G}_1\succeq \mathcal{G}_3$. Therefore, we conclude that $\mathcal{G}_1\cong \mathcal{G}_3$.
\end{proof}

