\section{Introduction}
\label{sec_intro}

% what problem are you trying to solve?
% why this problem? what is the motivation?
% what has already been done and what has not?
% how are you going to solve the problem?
% what have you done?
% what is this paper about?
% what are the novelties? what is new?

% why IVPs? why time integration algorithms for IVPs?
%   - applications: population dynamics, math biology, chemical reactions etc
%   - basis for IBVP, building blocks for dynamic PDEs
%   - traditional methods: single-step, multi-step, implicit/explicit
%
% can we develop NN algorithm that can outcompete traditional numerical algorithms?
%  - advantage: NN can learn from the exact time-marching scheme,
%               not from numerical data or from approximations
% 


% importance of studying algorithms for IVPs 

This work concerns the development of accurate and efficient 
methods for solving initial value problems (IVPs), by learning
their exact time integration algorithm with artificial neural networks.
Initial value problems with systems of
ordinary differential equations (ODEs)
are ubiquitous in modeling natural phenomena and
engineered systems. Problems as diverse as population dynamics,
the spread of diseases, chemical reactions, celestial mechanics, dynamical systems,
biological pattern formation, and the economic markets
can be modeled by such systems. They also arise, upon spatial discretization,
from time-dependent partial differential equations (PDEs)
describing the evolution of physical systems
based on first principles such as the conservation laws and
thermodynamic principles~\cite{GrootM1984,GurtinFA2010,Dong2014,Dong2018,YangD2020}.
The models arising from practical applications are usually nonlinear and can rarely
be solved by analytical methods. Numerical simulations are therefore
of fundamental importance and play a critical role
in understanding the dynamics of these systems.
%
% numerical algorithms: traditional time integration algorithms
% then transition to NN time integration algorithms
Time marching algorithms for IVPs are traditionally based on Taylor series
expansions.
Research in the past decades has led to
accurate and robust methods with provable convergence
based on this approach~\cite{CellierK2006,HairerNW1993,HairerW1996}, which have become
the cornerstone in computational science and engineering.

The emergence of artificial neural networks~\cite{GoodfellowBC2016} and their application
in scientific computing, a.k.a.~scientific machine
learning (SciML)~\cite{Karniadakisetal2021,RaissiPK2019,SirignanoS2018,EY2018,TangWL2021,DongN2021,KrishnapriyanGZKM2021,DongY2022,WanW2020,Penwardenetal2023,QianZHD2023},
in recent years
have stimulated promising new approaches
for simulating dynamical systems~\cite{QinWX2019,RaissiPK2019,SirignanoS2018,Jietal2021,LiuKB2022,WangP2023,FlamantPS2020,Dellnitzetal2023,MoyaL2023}.
%
% review discovery/modeling of ODEs
NN-based methods for such problems can be broadly classified into two types,
as pointed out in~\cite{Legaardetal2023}, direct solution models
and time-stepper models. Direct solution models seek the solution to the IVP
directly, in which the network input denotes the independent variable and
the output represents the solution. Representative techniques in
this category include the physics-informed neural network (PINN)
method~\cite{RaissiPK2019}, deep Galerkin method (DGM)~\cite{SirignanoS2018},
and their many extensions and variants; see~\cite{Karniadakisetal2021,Cuomo2022Scientific}
for a review of related techniques.
Because the trained NN corresponds to a specific input or initial
condition, the direct solution  model needs to be re-trained
if the initial data changes. 
The time-stepper models, on the other hand, follow an approach
analogous to traditional numerical solvers. Given the current state of
the system, the model attempts to compute the state at some point
in the future. Accordingly, the NNs for time-stepper models
are generally auto-regressive in nature, aiming to capture
the dynamics of the system. The trained time-stepper model can
handle different initial data or input, without the need for re-training.
Early works on time-stepper models have focused on the
data-driven modeling of dynamical systems, in which
the governing equations are
unknown and are approximated by the solution
trajectory data~\cite{QinWX2019,QinCJX2021,ChurchillX2022}.
A residual network (ResNet) structure and stacked ResNets with
a recurrent or recursive configuration
are proposed in~\cite{QinWX2019} to approximate the unknown governing
equations and learn a discrete flow map of the autonomous systems.
This technique is extended to non-autonomous systems in~\cite{QinCJX2021},
where the non-autonomous term is expanded in terms of a local set of bases,
and to cases with partially observed  state
variables~\cite{ChurchillX2022}; see recent related works
in~\cite{ChenW2023,LuT2024}.
A hierarchical time-stepper based on deep neural networks (DNN)
is developed in~\cite{LiuKB2022} to approximate the system flow map
 over a range of time scales,
in which multiple DNNs are trained corresponding to
a number of step sizes using the trajectory data;
see~\cite{HamidRNB2023} for an adaptive time-stepping scheme
with the hierarchical time-stepper.
%
% reinforcement learning for step size in adaptive time-stepping
In~\cite{Dellnitzetal2023} deep reinforcement learning has been
combined with classical numerical solvers to determine the step sizes in adaptive
time-stepping.
%
% GFINN, 
In~\cite{ZhangSK2022} NN  structures are designed  to
satisfy the symmetric degeneracy conditions in the GENERIC formalism
for simulating dynamical systems, and their universal approximation
property has been established.
%
% neural ODEs, Pezold's neural ODEs
We also refer the reader to~\cite{ChenRBD2019,Kimetal2021,FronkP2024a}
(among others) for neural ODEs
and to~\cite{Legaardetal2023} for a survey of other related techniques.

% introduce the problem considered here:
%    ODE given, learn exact algorithm or flow map
%    how does it compare with leading traditional numerical algorithms?

Given a non-autonomous system of ODEs, we seek the exact
time integration  algorithm for this system. % in this work.
An exact time integration (or time marching) algorithm refers to a discrete scheme
that can produce numerical solutions matching exactly  the true solutions
to the system for all step sizes within a range; see Section~\ref{sec_method}
for its definition. This type of algorithms is also
known as the exact finite difference schemes or
non-standard finite difference (NSFD) schemes~\cite{Mickens2021}.
The exact finite difference schemes for a number of problems
have been constructed explicitly in~\cite{Mickens2021}, by using denominator functions
for approximating discrete derivatives and by special treatments of the nonlinear
terms involved in the equations. However, 
constructing the exact discrete scheme for
an arbitrary given system of differential equations is virtually impossible, unfortunately,
because otherwise this will be tantamount to knowing the general forms of
the solution to the given system~\cite{Mickens2021}.

This motivates the current work for learning the exact time integration algorithms
through artificial neural networks, by leveraging the NN's universal approximation
power for function approximation.
%
% discuss connection with exact flow maps and difference with previous works
Viewed from the dynamical systems perspective, the exact time integration
algorithm is closely related to the exact flow map (or evolution semigroup)~\cite{StuartH1996}
of the system. Given a system of differential equations, suppose
the system has a unique solution for any given initial condition.
Then the exact time integration algorithm  exists, and
we would like to learn this algorithm from the given system. The learned algorithm
then provides a method for solving this system, under any given initial condition
or step size.
The problem considered here is different from the so-called flow-map learning in
previous studies (see e.g.~\cite{QinWX2019,ChurchillX2023,LiuKB2022}, among others),
where the governing differential equations are unknown and a time-stepper model
is learned based on the solution trajectory data obtained from
either measurements or external numerical solvers.
In contrast, for the problems considered in this paper,
the system of differential equations is given and we seek
the exact time integration algorithm for this system. No other data
about the system is available,  and our method
does not rely on any external numerical solver.

% how is it learned?
% introduce randomized NN. why randomized NN: accuracy, efficiency
% review randomized NN works, in particular, those on ODEs -- direct solution models
%


Given a system of non-autonomous (including autonomous) ODEs,
the exact time integration algorithm for this system can be represented by a vector-valued
function in higher dimensions, which in the current paper will be referred to as
the algorithmic function of the given system.
The algorithmic function satisfies an associated system of
partial differential equations, together with corresponding boundary conditions.
We learn the exact time integration algorithm  by
solving the associated system of PDEs  for
the  algorithmic function with a physics informed approach,
leveraging a type of randomized feedforward neural networks
often known as extreme learning machines (ELMs)~\cite{DongL2021,DongY2022rm,WangD2024}.

% review randomized NN, why randomized NN

Randomized NNs (or random-weight NNs)
are neural networks 
with a subset of the network parameters assigned to random values and fixed (non-trainable)
while only the rest of the network parameters are trained.
A simple strategy underlies randomized NNs. Since optimizing
the entire set of NN parameters is extremely difficult and  costly,
randomized NN attempts to randomly assign and fix a portion of the network parameters,
so that the ensuing network training problem can become simpler,
without severely compromising the network's achievable approximation
capability~\cite{DongY2022rm,NiD2023}.
ELM is a particular type of randomized feedforward NNs, in which all the hidden-layer
parameters are assigned to random values and fixed (non-trainable) while
only the output-layer parameters are trained. ELM was originally proposed
in~\cite{HuangZS2006} for linear classification and regression problems,
but has since found widespread applications  in many areas~\cite{Alabaetal2019}.
A close cousin to ELM is the % another type of randomized NNs known as
random vector functional link (RVFL) networks~\cite{PaoPS1994}.
ELM type  randomized NNs, with a single hidden layer,
are known to be universal function
approximators~\cite{IgelnikP1995,HuangCS2006,Gonon2023}.

% ELM for scientific computing


The use of ELM type randomized NNs  for scientific computing,
particularly for solving partial differential equations, occurs quite recently
and has flourished in the past few
years~\cite{DongL2021,Schiassietal2021,CalabroFS2021,WangD2024}.
We refer the reader to~\cite{PanghalK2020,DwivediS2020,DongL2021,LiuHWC2021,CalabroFS2021,ChenCEY2022,LiLX2023,QuanH2023,Calabroetal2023,SunDF2024} (among others) for ELM type methods
for linear PDEs, to~\cite{DongL2021,DongL2021bip,Schiassietal2021,FabianiCRS2021,DongY2022rm,NiD2023,DongW2023,WangD2024} (among others) for nonlinear PDEs,
and to a recent work~\cite{FabianiKSY2025} for neural operators.
%
% discuss accuracy and computational efficiency of ELM
Many studies (including those from our group) have shown that
ELM  can produce highly accurate solutions to linear and nonlinear
PDEs, exhibiting spectral-like accuracy~\cite{DongL2021,NiD2023},
with a competitive computational cost~\cite{DongY2022rm}.
The ELM errors decrease exponentially
with increasing number of degrees of freedom for smooth solutions, and
can reach the level near machine accuracy as the degrees of freedom
become large~\cite{DongY2022rm}. 
%
% discuss ELM for ODEs
Several studies have looked into the computation of ODEs
based on randomized NNs~\cite{YangHL2018,LiuXWL2020,PanghalK2020,DongL2021,Schiassietal2022,FabianiGRS2023,FlorioSCF2023}.
These studies stem from the direct solution model, in which the NN
learns the solution corresponding to a specific  initial condition.
As a result, with different initial data,
the NN  has to be re-trained.
In contrast, with the method herein,
the trained NN  can solve the problem with arbitrary initial data from some
domain, with no need for re-training. This is a major difference between
these studies and the  current work.


% what is this paper about?
% what are the results?
% compare with traditional algorithms.

In this paper we present a method based on ELM-type randomized NNs
for learning the exact time integration algorithm of non-autonomous and autonomous
systems. Our method learns the 
algorithmic function using ELM by solving its associated
system of partial differential equations  with a physics informed approach.
We explicitly incorporate an approximation of the solution field 
into the NN formulation, so that the ELM network effectively learns
the corresponding error function.
We consider  explicit and implicit NN formulations and discuss how to
train the ELM network by the nonlinear least squares (Gauss-Newton) method.
Accordingly, the trained NN
gives rise to explicit or implicit time integration algorithms for solving
the system.
%
% periodicity of RHS
In particular, we investigate the effect on the algorithm learning
when the right hand side (RHS) of the non-autonomous system
exhibits a periodicity with respect to time or to
any component of the solution variable.
We show that in this case, while the solution itself to the IVP is not periodic,
the algorithmic function  is either periodic, or when it is not, shows
a well-defined relation for different periods.
This means that, whenever the RHS of the non-autonomous system exhibits a
periodicity with respect to any of its arguments, the NN  only
needs to be trained on a domain covering one period along the corresponding
directions, and the trained NN can be used to solve the problem
on arbitrarily long time horizons (in case of temporal periodicity of RHS)
or for those corresponding solution components taking arbitrary values on
the real axis. These properties can greatly simplify the network training
and algorithm learning  for many problems.
%
% domain decomposition, NN training
In addition, we find that it is often crucial to incorporate domain decomposition
into the algorithm learning, especially for stiff problems.
In this case, a local ELM-type randomized NN learns the algorithmic function
on each sub-domain, and the local NNs  collectively represent
the algorithmic function over the entire  training domain.
What is most attractive lies in that different local NNs are not coupled,
due to the nature of the PDE system for the algorithmic function,
and thus they can be trained completely separately. 

% numerical results
% comparison with traditional numerical algorithms

We present extensive numerical experiments with several benchmark
problems (including non-stiff, stiff, and chaotic systems) to evaluate
the performance of the learned NN  algorithm.
We show that the learned algorithm produces highly accurate solutions
in long-time simulations. Its time-marching errors
decrease nearly exponentially as the number of
degrees of freedom (training collocation points, or
training parameters) in the NN  increases, while its time-marching
cost grows only quasi-linearly.
%
We compare extensively the computational performance (time-marching accuracy
vs.~time-marching cost) between the current NN algorithm
and the leading traditional time integration algorithms (with 
adaptive time-stepping and adaptive-order),
including DOP853 (Dormand Prince, order 8)~\cite{HairerNW1993},
RK45 (explicit Runge-Kutta, order 5)~\cite{DormandP1980},
and Radau (implicit Runge-Kutta Radau IIA, order 5)~\cite{HairerW1996}, among others.
The learned NN algorithm is computationally very competitive,
markedly outperforming the traditional algorithms in almost all problems.


% importance of computational domain
% properties of algorithmic function psi, when f(y,t) exhibits periodicity

% contributions, novelty
% what is new?

The contributions of this paper lie in three aspects:
(i) the ELM-based method for learning the exact time integration algorithms
and the learned NN algorithms for accurately and efficiently solving
non-autonomous and autonomous systems;
(ii) the illumination of properties of the algorithmic function when the RHS
of non-autonomous (including autonomous) systems exhibits a periodicity
with respect to any of its arguments, which can be used to greatly
simplify the network training and algorithm learning; and
(iii) the comparison with traditional time integration algorithms
and  demonstration of the competitive and superior computational
performance of the learned NN algorithms. 

% comments on code, implementation, computers used, Tensorflow/Keras etc

The method presented here has been implemented in Python
using the Tensorflow and Keras libraries. The numerical tests are performed
on a MAC computer (3.2GHz Quad-Core Intel Core i5 processor, 24GB memory)
in the authors' institution.

% structure of paper

The rest of this paper is organized as follows.
In Section~\ref{sec_method} we discuss the properties of the algorithmic function,
its representation by ELM-type randomized NNs,
the network training by the nonlinear least squares method,
and how to use the learned algorithm
to solve non-autonomous and autonomous systems.
Section~\ref{sec_tests} consists of a comprehensive set of numerical tests
to assess the performance of the presented method with several benchmark problems.
We compare extensively the learned NN algorithms
with leading traditional time integration algorithms.
Section~\ref{sec_summary} provides further comments on several aspects of the current
method to  conclude the presentation.
The appendix includes proofs of several theorems from Section~\ref{sec_method}
concerning the properties of the algorithmic function.

